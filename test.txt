Header_1: GSLB: The Graph Structure Learning Benchmark
filename: 505_gslb_the_graph_structure_learn.pdf

GSLB: The Graph Structure Learning Benchmark
------------
Header_1: GSLB: The Graph Structure Learning Benchmark
filename: 505_gslb_the_graph_structure_learn.pdf

GSLB: The Graph Structure Learning Benchmark

Zhixun Li1, Liang Wang2,3, Xin Sun4, Yifan Luo5, Yanqiao Zhu6, Dingshuo Chen2,3, Yingtao Luo7, Xiangxin Zhou2,3, Qiang Liu2,3∗, Shu Wu2,3, Liang Wang2,3,4, Jeffrey Xu Yu1∗

Affiliations:

1 Department of Systems Engineering and Engineering Management

2 The Chinese University of Hong Kong, Center for Research on Intelligent Perception and Computing

3 State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences

4 School of Artificial Intelligence, University of Chinese Academy of Sciences

5 Department of Automation, University of Science and Technology of China

6 School of Cyberspace Security, Beijing University of Posts and Telecommunications

7 Department of Computer Science, University of California, Los Angeles

Heinz College of Information Systems and Public Policy, Machine Learning Department, School of Computer Science, Carnegie Mellon University

Primary contact: zxli@se.cuhk.edu.hk
------------
Header_1: GSLB: The Graph Structure Learning Benchmark
Header_2: Abstract
filename: 505_gslb_the_graph_structure_learn.pdf

Abstract

Graph Structure Learning (GSL) has recently garnered considerable attention due to its ability to optimize both the parameters of Graph Neural Networks (GNNs) and the computation graph structure simultaneously. Despite the proliferation of GSL methods developed in recent years, there is no standard experimental setting or fair comparison for performance evaluation, which creates a great obstacle to understanding the progress in this field. To fill this gap, we systematically analyze the performance of GSL in different scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node- and graph-level tasks, and analyze their performance in robust learning and model complexity. Further, to facilitate reproducible research, we have developed an easy-to-use library for training, evaluating, and visualizing different GSL methods. Empirical results of our extensive experiments demonstrate the ability of GSL and reveal its potential benefits on various downstream tasks, offering insights and opportunities for future research. The code of GSLB is available at: https://github.com/GSL-Benchmark/GSLB.
------------
Header_1: GSLB: The Graph Structure Learning Benchmark
Header_2: Introduction
filename: 505_gslb_the_graph_structure_learn.pdf

Introduction

Graphs, structures made of vertices and edges, are ubiquitous in real-world applications. A wide variety of applications spanning social network [51, 9], molecular property prediction [40, 14], fake news detection [45, 1], and fraud detection [23, 27] have found graphs instrumental in modeling complex systems. In recent years, Graph Neural Networks (GNNs) have attracted increasing attention due to their powerful ability to learn node or graph representations. However, most of the GNNs heavily rely on the assumption that the initial structure of the graph is trustworthy enough to serve as ground-truth for training. Due to uncertainty and complexity in data collection, graph structures are inevitably redundant, biased, noisy, incomplete, or the original graph structures are even unavailable, which will bring great challenges for the deployment of GNNs in real-world applications.

Corresponding authors: Qiang Liu (qiang.liu@nlpr.ia.ac.cn), Jeffrey Xu Yu (yu@se.cuhk.edu.hk)

37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.
------------
Header_1: GSLB Overview
filename: 505_gslb_the_graph_structure_learn.pdf

GSLB Overview
------------
Header_1: GSLB Overview
Header_2: Table 1: An overview of GSLB
filename: 505_gslb_the_graph_structure_learn.pdf

Table 1: An overview of GSLB

Both algorithms and datasets are divided into three categories: homogeneous node-level, heterogeneous node-level, and graph-level. The evaluation is divided into three dimensions: effectiveness, robustness, and complexity.
------------
Header_1: GSLB Overview
Header_2: Table 1: An overview of GSLB
Header_3: Algorithms
filename: 505_gslb_the_graph_structure_learn.pdf

Algorithms

|Homogeneous GSL|Heterogeneous GSL|Graph-level GSL|
|---|---|---|
|LDS [11], GRCN [48], ProGNN [18], IDGL [4], CoGSL [26], SUBLIME [28], GEN [39], STABLE [21], NodeFormer [43], SLAPS [10], GSR [54], HES-GSL [42]|GTN [49], HGSL [53]|HGP-SL [52], VIB-GSL [36]|
------------
Header_1: GSLB Overview
Header_2: Table 1: An overview of GSLB
Header_3: Datasets
filename: 505_gslb_the_graph_structure_learn.pdf

Datasets

|Homogeneous datasets|Heterogeneous datasets|Graph-level datasets|
|---|---|---|
|Cora [47], Citeseer [47], Pubmed [47], ogbn-arxiv [15], Polblogs, Cornell [34], Texas [34], Wisconsin [34], Actor [37]|ACM [49], DBLP [49], Yelp [29]|IMDB-B [3], IMDB-M [3], COLLAB [46], REDDIT-B [46], MUTAG [5], PROTEINS [2], Peptides-Func [8], Peptides-Struct [8]|
------------
Header_1: GSLB Overview
Header_2: Table 1: An overview of GSLB
Header_3: Evaluations
filename: 505_gslb_the_graph_structure_learn.pdf

Evaluations

- Effectiveness: Homogeneous node classification (Topology Refinement/Topology Inference), Heterogeneous node classification, Graph-level tasks
- Robustness: Supervision signal robustness, Structure robustness, Feature robustness
- Complexity: Time complexity, Space complexity

To mitigate the aforementioned problems, Graph Structure Learning (GSL) [4, 55, 30, 10, 57, 50] has become an important theme in graph learning. GSL aims to make the computation structure of GNNs more suitable for downstream tasks and improve the quality of the learned representations. While it is widespread in different communities and the research enthusiasm for GSL is increasing, there is no standardized benchmark that could offer a fair and consistent comparison of different GSL algorithms. Moreover, due to the complexity and diversity of graph datasets, the experimental setups in existing work are not consistent, such as varying ratios of the training set and different train/validation/test splits. This poses a great obstacle to a holistic understanding of the current research status. Therefore, the development of a standardized and comprehensive benchmark for GSL is an urgent need within the community.

In this work, we propose Graph Structure Learning Benchmark (GSLB), which serves as the first comprehensive benchmark for GSL. Our benchmark encompasses 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. We systematically investigate the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. Based on these three dimensions, we conduct an extensive comparative study of existing GSL algorithms in different scenarios. For effectiveness, GSLB provides a fair and comprehensive comparison of existing algorithms on homogeneous node-level, heterogeneous node-level, and graph-level tasks, where we consider both homophilic and heterophilic graph datasets for homogeneous node-level tasks, and cover both Topology Refinement (TR, i.e., refining graphs from data with the original topology) and Topology Inference (TI, i.e., inferring graphs from data without initial topology) settings. For robustness, GSLB evaluates GSL models under three types of noise: supervision signal noise, structure noise, and feature noise. We also compare GSL algorithms with the models specifically designed to improve these types of robustness. For complexity, GSLB conducts a detailed evaluation of representative GSL algorithms in terms of time complexity and space complexity.

Through extensive experiments, we observe that: (1) GSL generally brings performance improvement for node-level tasks, especially on heterophilic graphs; (2) on graph-level tasks, current GSL models bring limited improvement and their performance varies greatly across different datasets; (3) most GSL algorithms (especially unsupervised GSL algorithms) show impressive robustness; (4) GSL models require significant time and memory overhead, making them challenging to deploy on large-scale graphs. In summary, we make the following three contributions:

1. GSL generally brings performance improvement for node-level tasks, especially on heterophilic graphs.
2. On graph-level tasks, current GSL models bring limited improvement and their performance varies greatly across different datasets.
3. Most GSL algorithms (especially unsupervised GSL algorithms) show impressive robustness.
------------
Header_1: Graph Structure Learning
filename: 505_gslb_the_graph_structure_learn.pdf

Graph Structure Learning
------------
Header_1: Graph Structure Learning
Header_2: Input data
filename: 505_gslb_the_graph_structure_learn.pdf

Input data

! = (A, X)
------------
Header_1: Graph Structure Learning
Header_2: Graph Learner
filename: 505_gslb_the_graph_structure_learn.pdf

Graph Learner

Refined computation graph: ! ⋆ = (A⋆, X)
------------
Header_1: Graph Structure Learning
Header_2: Structure Modeling
filename: 505_gslb_the_graph_structure_learn.pdf

Structure Modeling

- Message Passing
- KNNSparsify
- ThresholdSparsify
- Discretize
- Symmetrize

Figure 1: A general framework of Graph Structure Learning (GSL). GSL methods start with input features and an optional initial graph structure. Its corresponding computation graph is refined/inferred through a structure learning module. With the learned computation graph, Graph Neural Networks (GNNs) are used to generate graph representations.
------------
Header_1: Graph Structure Learning
Header_2: Structure Modeling
Header_3: We propose GSLB, the first comprehensive benchmark for graph structure learning. We integrate 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. An overview of our benchmark is shown in Table 1.
filename: 505_gslb_the_graph_structure_learn.pdf

We propose GSLB, the first comprehensive benchmark for graph structure learning. We integrate 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. An overview of our benchmark is shown in Table 1.
------------
Header_1: Graph Structure Learning
Header_2: Structure Modeling
Header_3: To explore the ability and limitations of GSL, we systematically evaluate existing algorithms from three dimensions: effectiveness, robustness, and complexity. Based on the results, we reveal the potential benefits and drawbacks of GSL to assist future research efforts.
filename: 505_gslb_the_graph_structure_learn.pdf

To explore the ability and limitations of GSL, we systematically evaluate existing algorithms from three dimensions: effectiveness, robustness, and complexity. Based on the results, we reveal the potential benefits and drawbacks of GSL to assist future research efforts.
------------
Header_1: Graph Structure Learning
Header_2: Structure Modeling
Header_3: To facilitate future work and help researchers quickly use the latest models, we develop an easy-to-use open-source library. Besides, users can evaluate their own models or datasets with less effort. Our code is available at https://github.com/GSL-Benchmark/GSLB.
filename: 505_gslb_the_graph_structure_learn.pdf

To facilitate future work and help researchers quickly use the latest models, we develop an easy-to-use open-source library. Besides, users can evaluate their own models or datasets with less effort. Our code is available at https://github.com/GSL-Benchmark/GSLB.
------------
Header_1: Graph Structure Learning
Header_2: Problem Definition
filename: 505_gslb_the_graph_structure_learn.pdf

Problem Definition

In this section, we will briefly review the advances and basic concepts of GSL. Given an undirected graph G = (A, X), where A ∈ $$\mathbb{R}^{N \times N}$$ is the adjacency matrix, auv = 1 if edge (u, v) exists and auv = 0 otherwise, and X ∈ $$\mathbb{R}^{N \times F}$$ is the node features matrix, N is the number of nodes, F is the dimension of node features. Given an optional graph G, the goal of GSL is to jointly optimize computation graph G ⋆ = (A⋆, X) and the parameters of graph encoder Θf to obtain high-quality node representations Z⋆ ∈ $$\mathbb{R}^{N \times F'}$$ for downstream tasks, where A⋆ is the refined graph by graph learner.

In general, the objective of GSL can be summarized as the following formula:

$$
LGSL = L_{Task}(Z⋆, Y) + \lambda L_{Reg}(A⋆, Z⋆, G)
$$

where the first term LTask refers to a task-specific objective with respect to the learned representation Z⋆ and ground-truth Y, the second term LReg imposes constraints on the learned graph structure and representations, and λ is a hyper-parameter that controls the trade-off between the two terms. The general framework of GSL is shown in Figure 1.
------------
Header_1: Graph Structure Learning
Header_2: GSLB: Graph Structure Learning Benchmark
filename: 505_gslb_the_graph_structure_learn.pdf

GSLB: Graph Structure Learning Benchmark
------------
Header_1: Graph Structure Learning
Header_2: GSLB: Graph Structure Learning Benchmark
Header_3: 3.1 Benchmark Algorithms
filename: 505_gslb_the_graph_structure_learn.pdf

3.1 Benchmark Algorithms

Table 1 shows the overall 16 algorithms integrated in GSLB. They are divided into three categories: homogeneous GSL, heterogeneous GSL, and graph-level GSL. We briefly introduce each category in the following, and more details are provided in Appendix A.2.
------------
Header_1: Graph Structure Learning
Header_2: GSLB: Graph Structure Learning Benchmark
Header_3: 3.1 Benchmark Algorithms
Header_4: Homogeneous GSL
filename: 505_gslb_the_graph_structure_learn.pdf

Homogeneous GSL

Most of the existing GSL algorithms are designed for homogeneous graphs. They assume there is only one type of nodes and edges in the graph. We select 7 TR-oriented algorithms.
------------
Header_1: Graph Structure Learning
Header_2: Algorithms
filename: 505_gslb_the_graph_structure_learn.pdf

Algorithms

Algorithms including GRCN [48], ProGNN [18], IDGL [4], GEN [39], CoGSL [26], STABLE [21], and GSR [54]. For TI-oriented algorithms, we select SUBLIME [28], NodeFormer [43], SLAPS [10], and HES-GSL [42]. It is worth noting that TR-oriented algorithms can only be applied if the original graph structure is available, but we can construct a preliminary graph based on node features (e.g., kNN graphs or ϵ-graphs).
------------
Header_1: Graph Structure Learning
Header_2: Algorithms
Header_3: Heterogeneous GSL
filename: 505_gslb_the_graph_structure_learn.pdf

Heterogeneous GSL

We integrate two representative heterogeneous GSL algorithms: Graph Transformer Networks (GTN) [49] and Heterogeneous Graph Structure Learning (HGSL) [53], which can handle the heterogeneity and capture complex interactions in heterogeneous graphs.
------------
Header_1: Graph Structure Learning
Header_2: Algorithms
Header_3: Graph-level GSL
filename: 505_gslb_the_graph_structure_learn.pdf

Graph-level GSL

Graph-level GSL algorithms aim to refine each graph structure in datasets. We select two graph-level algorithms: Hierarchical Graph Pooling with Structure Learning (HGP-SL) [52] and Variational Information Bottleneck guided Graph Structure Learning (VIB-GSL) [36].
------------
Header_1: Graph Structure Learning
Header_2: Benchmark Datasets
filename: 505_gslb_the_graph_structure_learn.pdf

Benchmark Datasets

To comprehensively and effectively evaluate the characteristics of GSL in the field of graph learning, we have integrated a large number of datasets from various domains for different types of tasks.

For node-level tasks, to evaluate the most mainstream task of GSL, node classification, we use four citation networks (i.e., Cora, Citeseer, Pubmed [47]), and ogbn-arxiv [15], three website networks from WebKB (i.e., Cornell, Texas, and Wisconsin [34]), and a cooccurrence network Actor with homophily ratio ranging from strong homophily to strong heterophily. Subsequently, to validate the effectiveness of GSL in heterogeneous node classification, we utilized three heterogeneous graph datasets (i.e., DBLP [49], ACM [49], and Yelp [29]). To investigate the robustness of GSL, we further incorporate the Polblogs dataset for evaluation. For graph-level tasks, we select six public graph classification benchmark datasets from TUDataset [31] for evaluation, including IMDB-B [3], IMDB-M [3], RDT-B [46], COLLAB [46], MUTAG [5], and PROTEINS [2]. Each dataset is a collection of graphs where each graph is associated with a level. Besides, exploring whether GSL can capture long-range information is an exciting topic. Therefore, we have utilized recently proposed long-range graph datasets: Peptides-func and Peptides-struct [8]. See more details and statistics about datasets in Appendix A.1.
------------
Header_1: Graph Structure Learning
Header_2: Benchmark Evaluations
filename: 505_gslb_the_graph_structure_learn.pdf

Benchmark Evaluations

To comprehensively investigate the pros and cons of GSL, our benchmark evaluations encompass three dimensions: effectiveness, robustness, and complexity. For effectiveness, GSLB provides a fair and comprehensive comparison of existing algorithms from three perspectives: homogeneous node classification, heterogeneous node classification, and graph-level tasks. In the case of homogeneous node classification, we evaluated them on both homophilic and heterophilic graph datasets, conducting experiments in both TR and TI scenarios. For graph-level, we evaluate graph-level GSL algorithms on TUDataset and long-range graph datasets for exploring the capabilities on graph-level tasks. For most datasets, we use accuracy as our evaluation metric. For robustness, GSLB evaluates three types of robustness: supervision signal robustness, structure robustness, and feature robustness. We control the count of labels to explore the supervision signal robustness of GSL and find that GSL exhibits excellent performance in the scenarios with few labels. We inject random structure noise and graph topology attacks to investigate the structure robustness. We also study the feature robustness by randomly masking a certain proportion of node features. For complexity, we conduct a detailed evaluation of representative GSL algorithms in terms of time complexity and space complexity. It will help to facilitate the deployment of GSL in real-world applications.
------------
Header_1: Graph Structure Learning
Header_2: Experiments and Analysis
filename: 505_gslb_the_graph_structure_learn.pdf

Experiments and Analysis

In this section, we systematically investigate the effectiveness, robustness, and complexity of GSL algorithms by answering the following specific questions:

- RQ1: How effective are the algorithms on node-level representation learning (Section 4.2)?
- RQ2: Can GSL mitigate homophily inductive bias of traditional message-passing based GNNs (Section 4.2)?
- RQ3: How does GSL perform on heterogeneous graph datasets (Section 4.3)?
- RQ4: How effective are the algorithms on graph-level representation learning (Section 4.4)?
- RQ5: Can GSL methods capture long-range information on the graph (Appendix B)?
------------
Header_1: Graph Structure Learning
Header_2: Research Questions
filename: 505_gslb_the_graph_structure_learn.pdf

Research Questions

- RQ6: How robust are GSL algorithms when faced with a scarcity of labeled samples?
- RQ7: How robust are GSL algorithms in the face of structure attack or noise?
- RQ8: How is the feature robustness of GSL? (Section 4.5)
- RQ9: How efficient are these algorithms in terms of time and space (Section 4.6)?
- RQ10: What does the learned graph structure look like (Appendix B.2)?
------------
Header_1: Graph Structure Learning
Header_2: Research Questions
Header_3: Experimental Settings
filename: 505_gslb_the_graph_structure_learn.pdf

Experimental Settings

All algorithms in GSLB are implemented by PyTorch [33], and unless specifically indicated, the encoders for all algorithms are Graph Convolutional Networks. All experiments are conducted on a Linux server with GPU (NVIDIA GeForce 3090 and NVIDIA A100) and CPU (AMD EPYC 7763), using PyTorch 1.13.0, DGL 1.1.0 [38] and Python 3.9.16.
------------
Header_1: Graph Structure Learning
Header_2: Research Questions
Header_3: Performance on Node-level Representation Learning
filename: 505_gslb_the_graph_structure_learn.pdf

Performance on Node-level Representation Learning

For node-level representation learning, we conduct experiments on homogeneous graph datasets under both TR and TI scenarios, and use classification accuracy as our evaluation metric.

Table 2: Experimental results of various GSL algorithms under the standard setting of transductive node classification task in the TR scenario.

$$
\begin{array}{|c|c|}
\hline
\text{Algorithm} & \text{Performance} \\
\hline
\text{Algorithm 1} & \text{Performance 1} \\
\text{Algorithm 2} & \text{Performance 2} \\
\text{Algorithm 3} & \text{Performance 3} \\
\hline
\end{array}
$$

Observations:

1. Most GSL algorithms generally show improvements in node classification task, particularly on datasets with high heterophily ratio.
2. SUBLIME achieves optimal or near-optimal results on most datasets.
3. The scalability of GSL still needs improvement, especially on large-scale datasets.

Table 3: Experimental results of the transductive node classification task in the TI scenario.

$$
\begin{array}{|c|c|}
\hline
\text{Algorithm} & \text{Performance} \\
\hline
\text{Algorithm A} & \text{Performance A} \\
\text{Algorithm B} & \text{Performance B} \\
\text{Algorithm C} & \text{Performance C} \\
\hline
\end{array}
$$

Observations:

1. GSL outperforms baselines on homophily graph datasets.
2. Models that leverage self-supervision achieve better performance.
------------
Header_1: Graph Structure Learning
Header_2: Research Questions
Header_3: Performance on Heterogeneous Graph Node-level Representation Learning
filename: 505_gslb_the_graph_structure_learn.pdf

Performance on Heterogeneous Graph Node-level Representation Learning

Table 4: Experimental results on heterogeneous graph datasets.

$$
\begin{array}{|c|c|c|}
\hline
\text{Algorithm} & \text{Macro-F1} & \text{Micro-F1} \\
\hline
\text{GTN} & \text{F1 score} & \text{F1 score} \\
\text{HGSL} & \text{F1 score} & \text{F1 score} \\
\hline
\end{array}
$$

Observations:

1. GTN and HGSL generally outperform other models on heterogeneous graph datasets.
2. GSL algorithms generally outperform vanilla GNN models.
3. Models that consider heterogeneity perform significantly better on datasets with stronger heterogeneity.
------------
Header_1: OCR Text
filename: 505_gslb_the_graph_structure_learn.pdf

OCR Text
------------
Header_1: OCR Text
Header_2: Table 2: Accuracy ± STD comparison (%) under the standard setting of transductive node classification task in the Topology Refinement (TR) scenario
filename: 505_gslb_the_graph_structure_learn.pdf

Table 2: Accuracy ± STD comparison (%) under the standard setting of transductive node classification task in the Topology Refinement (TR) scenario

Performance is averaged from 10 independent repetitions. The highest results are highlighted with bold, while the second highest results are marked with underline. "OOM" denotes out of memory.

| |Cora|Citeseer|Pubmed|ogbn-arxiv|Cornell|Texas|Wisconsin|Actor|
|---|---|---|---|---|---|---|---|---|
|Edge Hom.|0.81|0.74|0.80|0.65|0.12|0.06|0.18|0.22|
|GCN|$81.46\pm0.58$|$71.36\pm0.31$|$79.18\pm0.29$|$70.77\pm0.19$|$47.84\pm5.55$|$57.83\pm2.76$|$57.45\pm4.30$|$30.01\pm0.77$|
|GAT|$81.41\pm0.77$|$70.69\pm0.58$|$77.85\pm0.42$|$69.90\pm0.25$|$46.22\pm6.33$|$54.05\pm7.35$|$57.65\pm7.75$|$28.91\pm0.83$|
|GPRGNN|$83.66\pm0.77$|$71.64\pm0.49$|$75.99\pm1.63$|$50.80\pm0.29$|$76.76\pm5.30$|$85.14\pm3.68$|$83.33\pm3.42$|$34.09\pm1.09$|
|LDS|$83.01\pm0.41$|$73.55\pm0.54$|OOM|OOM|$47.87\pm7.14$|$58.92\pm4.32$|$61.70\pm3.58$|$31.05\pm1.31$|
|GRCN|$83.87\pm0.49$|$72.43\pm0.61$|$78.92\pm0.39$|OOM|$54.32\pm8.24$|$62.16\pm7.05$|$56.08\pm7.19$|$29.97\pm0.71$|
|ProGNN|$80.30\pm0.57$|$68.51\pm0.52$|OOM|OOM|$54.05\pm6.16$|$48.37\pm12.17$|$62.54\pm7.56$|$22.35\pm0.88$|
|IDGL|$83.88\pm0.42$|$72.20\pm1.18$|$80.00\pm0.38$|OOM|$50.00\pm8.98$|$62.43\pm6.09$|$59.41\pm4.11$|$28.16\pm1.41$|
|GEN|$80.21\pm1.72$|$71.15\pm1.81$|$78.91\pm0.69$|OOM|$57.02\pm7.19$|$65.94\pm1.38$|$66.07\pm3.72$|$27.21\pm2.05$|
|CoGSL|$81.76\pm0.24$|$73.09\pm0.42$|OOM|OOM|$52.16\pm3.21$|$59.46\pm4.36$|$58.82\pm1.52$|$32.95\pm1.20$|
|SUBLIME|$83.40\pm0.42$|$72.30\pm1.09$|$80.90\pm0.94$|$71.75\pm0.36$|$70.54\pm5.98$|$77.03\pm4.23$|$78.82\pm6.55$|$33.57\pm0.68$|
|STABLE|$80.20\pm0.68$|$68.91\pm1.01$|OOM|OOM|$44.03\pm4.05$|$55.24\pm6.04$|$53.00\pm5.27$|$30.18\pm1.00$|
|NodeFormer|$80.28\pm0.82$|$71.31\pm0.98$|$78.21\pm1.43$|$55.40\pm0.23$|$42.70\pm5.51$|$58.92\pm4.32$|$48.43\pm7.02$|$25.51\pm1.77$|
|GSR|$82.48\pm0.43$|$71.10\pm0.25$|$78.09\pm0.53$|OOM|$44.32\pm2.16$|$60.81\pm4.87$|$56.86\pm1.24$|$30.23\pm0.38$|
------------
Header_1: OCR Text
Header_2: Table 3: Accuracy ± STD comparison (%) under the standard setting of transductive node classification task in the Topology Inference (TI) scenario
filename: 505_gslb_the_graph_structure_learn.pdf

Table 3: Accuracy ± STD comparison (%) under the standard setting of transductive node classification task in the Topology Inference (TI) scenario

Performance is averaged from 10 independent repetitions.

| |Cora|Citeseer|Pubmed|ogbn-arxiv|Cornell|Texas|Wisconsin|Actor|
|---|---|---|---|---|---|---|---|---|
|Edge Hom.|0.81|0.74|0.80|0.65|0.12|0.06|0.18|0.22|
|MLP|$58.55\pm0.80$|$59.52\pm0.64$|$73.00\pm0.30$|$55.21\pm0.11$|$71.35\pm6.19$|$80.27\pm5.93$|$84.71\pm3.14$|$35.49\pm1.04$|
|GCNknn|$66.10\pm0.44$|$68.33\pm0.89$|$69.23\pm0.49$|$55.21\pm0.22$|$75.14\pm2.65$|$75.95\pm4.43$|$84.12\pm3.97$|$32.98\pm0.49$|
|GATknn|$64.62\pm1.04$|$68.05\pm1.12$|$68.76\pm0.80$|$55.92\pm0.30$|$74.05\pm5.16$|$76.49\pm4.99$|$82.16\pm4.06$|$31.67\pm1.19$|
------------
Header_1: OCR Text
Header_2: Performance of GSL algorithms on graph-level tasks
filename: 505_gslb_the_graph_structure_learn.pdf

Performance of GSL algorithms on graph-level tasks

In this section, we conduct graph classification experiments on four social datasets (i.e., IMDB-B, RDT-B, COLLAB, and IMDB-M) and two biological datasets (i.e., MUTAG and PROTEINS).

Table 5 shows the experimental results of average accuracy and the standard deviation of 10-fold cross-validation.

We can observe that HGP-SL (with GCN as the encoder) consistently outperforms GCN on all datasets. However, we find that VIB-GSL exhibits strong instability across different random seeds.

And due to the absence of training scripts in the official code, we performed hyperparameter tuning based on the parameter search space ($$\beta \in \{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\}$$) provided in the paper, but we are unable to surpass the performance of the baseline models consistently.

Lastly, we conducted an analysis of graph-level GSL algorithms on long-range graph dataset. For detailed information, please refer to Appendix B.

Link to VIB-GSL GitHub Repository

Table 4: Macro-F1 and Micro-F1 ± STD comparison (%) under the standard setting of heterogeneous node classification task.

Method
Macro-F1
ACM
Micro-F1
Macro-F1
DBLP
Micro-F1
Macro-F1
Yelp
Micro-F1

GCN
90.27±0.59

90.18±0.61

90.01±0.32

90.99±0.28
78.01±1.89
81.03±1.81

GAT
91.52±0.62

91.46±0.62

90.22±0.37

91.13±0.40
82.12±1.47
84.43±1.56

HAN
91.67±0.39

91.47±0.22

90.53±0.24

91.47±0.22
88.49±1.73
88.78±1.40

LDS
92.35±0.43

92.05±0.26

88.11±0.86

88.74±0.85
75.98±2.35
78.14±1.98

$$
\begin{array}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\text{Method} & \text{Macro-F1} & \text{ACM} & \text{Micro-F1} & \text{Macro-F1} & \text{DBLP} & \text{Micro-F1} & \text{Macro-F1} & \text{Yelp} & \text{Micro-F1} \\
\hline
\text{GCN} & 90.27\pm0.59 & & 90.18\pm0.61 & & 90.01\pm0.32 & & 90.99\pm0.28 & 78.01\pm1.89 & 81.03\pm1.81 \\
\text{GAT} & 91.52\pm0.62 & & 91.46\pm0.62 & & 90.22\pm0.37 & & 91.13\pm0.40 & 82.12\pm1.47 & 84.43\pm1.56 \\
\text{HAN} & 91.67\pm0.39 & & 91.47\pm0.22 & & 90.53\pm0.24 & & 91.47\pm0.22 & 88.49\pm1.73 & 88.78\pm1.40 \\
\text{LDS} & 92.35\pm0.43 & & 92.05\pm0.26 & & 88.11\pm0.86 & & 88.74\pm0.85 & 75.98\pm2.35 & 78.14\pm1.98 \\
\hline
\end{array}
$$
------------
Header_1: Table 6: Accuracy ± STD comparison (%) with respect to different perturbation rates
filename: 505_gslb_the_graph_structure_learn.pdf

Table 6: Accuracy ± STD comparison (%) with respect to different perturbation rates
------------
Header_1: Table 6: Accuracy ± STD comparison (%) with respect to different perturbation rates
Header_2: Table 6: Accuracy ± STD comparison (%) with respect to different perturbation rates
filename: 505_gslb_the_graph_structure_learn.pdf

Table 6: Accuracy ± STD comparison (%) with respect to different perturbation rates

|Dataset|Ptb Rate|GCN|Jaccard|SimPGCN|IDGL|GRCN|ProGNN|STABLE|SUBLIME|
|---|---|---|---|---|---|---|---|---|---|
|Cora|0%|83.68±0.37|83.78±0.50|82.66±0.48|84.69±1.13|84.43±0.26|84.53±0.89|83.70±0.30|83.84±0.28|
| |5%|80.61±0.39|81.44±0.48|80.35±0.82|82.56±0.24|81.34±0.50|81.47±0.44|81.52±0.85|79.93±0.58|
| |20%|61.98±1.23|70.71±0.91|69.08±2.78|67.19±0.69|69.54±0.58|61.07±0.61|76.44±2.47|75.25±1.08|
|Citeseer|0%|76.56±0.36|74.34±0.26|74.35±0.74|73.87±0.70|76.34±0.11|73.36±1.52|72.65±1.36|73.34±1.17|
| |5%|72.51±0.30|70.01±0.79|72.99±1.05|72.46±0.47|74.66±0.27|71.46±0.47|69.66±0.95|72.63±0.50|
| |10%|71.92±0.68|70.28±1.30|72.68±0.54|69.72±0.59|74.06±0.43|69.03±0.60|72.79±0.71|73.02±0.29|
| |15%|64.44±0.53|67.13±1.28|71.74±1.46|62.83±1.28|66.46±1.12|65.42±1.20|70.98±0.61|73.90±0.52|
| |20%|57.51±1.03|67.82±0.74|70.06±1.86|61.16±0.99|69.42±1.14|57.51±0.36|71.90±1.12|72.55±0.62|
|Polblogs|0%|95.62±0.69|94.93±0.28|94.50±0.43|94.83±0.20|95.65±0.28|94.84±0.19|95.63±0.32|95.27±0.51|
| |5%|80.57±0.66|78.17±0.55|76.02±1.14|79.62±0.65|93.70±0.18|92.36±0.42|89.41±1.63|93.24±1.50|
| |10%|71.83±2.37|71.86±1.34|70.12±1.10|74.54±0.69|87.99±1.56|84.66±0.52|89.87±0.82|93.62±0.50|
| |15%|66.38±2.17|69.93±0.66|64.19±1.55|75.53±0.83|71.85±1.58|77.38±0.51|89.94±0.89|94.29±0.27|
| |20%|68.19±2.24|69.22±0.34|63.64±1.41|71.63±0.62|71.73±1.58|73.57±0.29|87.42±0.69|92.60±0.72|
------------
Header_1: Table 6: Accuracy ± STD comparison (%) with respect to different perturbation rates
Header_2: Figure 3: Analysis of robustness when injecting random noise on Cora and Citeseer
filename: 505_gslb_the_graph_structure_learn.pdf

Figure 3: Analysis of robustness when injecting random noise on Cora and Citeseer

In Figure 3, we can observe that GSL algorithms (for the sake of brevity, we opted to select only three models: GRCN, IDGL, and SUBLIME) achieve the best results in scenarios with fewer labels available. We speculate that this may be because the learned graph structure is denser and exhibits cleaner community boundaries. As a result, the supervision signals can propagate more effectively within such a structure.

Robustness analysis with respect to random noise. We randomly remove edges from or add edges to the original graph structures of Cora and Citeseer, then evaluated the performance of GSL algorithms on the corrupted graphs. We change the ratios of modified edges from 0 to 0.9 to simulate different attack intensities. As shown in Figure 3, as the noise intensity increases, the models’ performance generally exhibits a downward trend. And we can observe that GSL algorithms commonly demonstrate a certain degree of robustness, as they tend to exhibit more stable performance than GCN when random noise is injected. Besides, we also found that, due to variations in the graph modeling process, different algorithms display varying levels of robustness when facing edge deletion and edge addition scenarios. For example, GRCN demonstrates strong robustness in edge deletion scenarios. However, in the edge addition scenarios, it only exhibits slight performance improvements compared to GCN. On the contrary, STABLE exhibits strong robustness in the edge deletion scenario, while showing the opposite trend in edge addition.

Robust analysis with respect to graph topology attack. Following [21, 55], we conduct robust analysis on three graph datasets, i.e., Cora, Citeseer, and Polblogs. First, we select the largest connected component in the graph, and utilize Mettack [58], a non-targeted adversarial topology attack method, to generate perturbed graphs. We select the perturbation rate from 0% to 20%. Table 6 shows the performance of GSL algorithms on three datasets with respect to various perturbation rates. Surprisingly, we can observe that most GSL algorithms exhibit strong robustness against graph topology attacks, even better than state-of-the-art defense GNNs (e.g., Jaccard [41] and SimPGCN [19]). GSL can effectively remove the newly added adversarial edges, and recover important edges to promote message passing. As mentioned in Li et al. [21], optimizing graph structures based on either features or supervised signals might not be reliable. We found that self-supervised graph structure modeling methods (e.g., STABLE and SUBLIME) show excellent
------------
Header_1: Graph Structure Learning
filename: 505_gslb_the_graph_structure_learn.pdf

Graph Structure Learning
------------
Header_1: Graph Structure Learning
Header_2: Performance Analysis
filename: 505_gslb_the_graph_structure_learn.pdf

Performance Analysis

| |GCN|GRCN|IDGL|ProGNN|SUBLIME|MLP|SLAPS|HES-GSL|
|---|---|---|---|---|---|---|---|---|
|TR on Cora|85| | | | | | | |
|TR on Citeseer| |75| | | | | | |
|TI on Cora|75| | | | | | | |
|TI on Citeseer| | | | | | | |70|

Accuracy (%)

$$
\begin{array}{|c|c|c|c|c|c|c|c|c|}
\hline
80 & & & & & & & & \\
\hline
75 & & & & & & & & \\
\hline
70 & & & & & & & & \\
\hline
65 & & & & & & & & \\
\hline
\end{array}
$$
Figure 4: Analysis of robustness when injecting random feature noise on Cora and Citeseer.
------------
Header_1: Graph Structure Learning
Header_2: Robust Analysis
filename: 505_gslb_the_graph_structure_learn.pdf

Robust Analysis

| |GCN|LDS|GRCN|IDGL|ProGNN|CoGSL|SUBLIME|NodeFormer|
|---|---|---|---|---|---|---|---|---|
|Cora|84| |74| |81| | | |
|Citeseer| |72| | |80| | | |
|Pubmed| | | | | | | | |

Accuracy (%)

$$
\begin{array}{|c|c|c|c|c|c|c|c|c|c|}
\hline
83 & & & & & & & & & \\
\hline
82 & & & & & & & & & \\
\hline
81 & & & & & & & & & \\
\hline
80 & & & & & & & & & \\
\hline
\end{array}
$$
Figure 5: Training time and space analysis on Cora, Citeseer and Pubmed.

Performance on corrupted graph structure datasets, which means unsupervised representation learning might produce more reliable and high-quality representations to conduct structure modeling.

Robust analysis with respect to feature noise. On the basis of exploring structural robustness, we also study the feature robustness of GSL. We randomly mask a certain proportion of node features by filling them with zeros, to investigate the performance of GSL algorithms when node features are subjected to varying degrees of damage. As shown in Figure 4, we can observe that: 1) the node features play a more critical role than the structure on certain datasets. Under the same noise degree, feature noise brings more performance degradation compared with structure noise; 2) Interestingly, while most existing GSL methods rely on feature similarity between pairs of nodes to learn graph structure, they still exhibit good robustness when facing noisy node features; 3) edge-oriented algorithms (e.g., ProGNN) show stronger feature robustness, because they optimize adjacency matrix directly, and have less dependence on pairs of node features.
------------
Header_1: Graph Structure Learning
Header_2: Efficiency and Scalability Analysis
filename: 505_gslb_the_graph_structure_learn.pdf

Efficiency and Scalability Analysis

In this section, we analyze the efficiency and scalability of GSL algorithms on Cora, Citeseer, and Pubmed datasets. For time efficiency, we evaluate the efficiency of the algorithms by measuring the time it takes for them to converge, i.e., achieve the best performance on the validation set. For scalability, we set all models to their dense version to ensure a fair comparison. As shown in Figure 5, GSL algorithms generally have higher time and space complexity compared to GCN. This limitation restricts the application of GSL on large-scale graphs. We can observe that some algorithms (e.g., GRCN) can achieve relatively good performance improvement with less complexity increase. Besides, although some algorithms (e.g., IDGL, LDS, and SUBLIME) achieve remarkable effectiveness improvement, they largely increase the complexity of time and space.
------------
Header_1: Graph Structure Learning
Header_2: Conclusion and Future Directions
filename: 505_gslb_the_graph_structure_learn.pdf

Conclusion and Future Directions

In this paper, we give a brief introduction and overview of graph structure learning. Then we present the first Graph Structure Learning Benchmark (GSLB) consisting of 16 algorithms and 20 datasets.

for various tasks. Based on GSLB, we conducted extensive experiments to reveal and analyze the
performance of GSL algorithms in different scenarios and tasks. Through our comparative study, we
find that GSL achieves promising results in heterophily, robustness, etc. The goal of this work is to
understand the current state of development of GSL and provide insights for future research.
Notwithstanding the promising results that have been made, there are still some critical challenges
and research directions worthy of future investigation.

- Insufficient scalability. Most existing works model the existence probability of edges based on
node pairs, with a complexity of $O(N^2)$. This makes it challenging to employ GSL in large-scale
graphs in real-world applications. Future work should focus on overcoming the limitations of GSL
in terms of complexity.
- Surprising performance with few labels. We have observed that GSL learns denser and more
distinct graph structures, which facilitates the propagation of supervision signals. Most existing
GNNs that address few label problem are based on deep GNNs [25, 13] or semi-supervised
approaches [6, 35, 20], without refining the graph structure. In the future, it would be worth
exploring the combination of increasing the supervision signals and making the graph structure
more suitable for propagating those signals.
- Excellent performance of unsupervised GSL in robustness. Some algorithms using self-
supervised methods for learning graph structures exhibit excellent performance in robustness,
which may be attributed to the avoidance of unreliable supervision signals. In the future, further
exploration can be done to utilize unsupervised structure learning for designing defense models.
- Hard to apply on incomplete graphs. Most existing algorithms rely on pairwise node embeddings
to generate the probability of edge existence. The underlying assumption is that all attributes of
nodes on the graph are complete. However, it is common in practice that some nodes or all nodes
have no features. Future research should address the challenges of structure learning on incomplete
graphs.

Acknowledgment: This work was partially supported by the Research Grants Council of Hong Kong, No. 14202919 and
No. 14205520.
------------
Header_1: Graph Structure Learning
Header_2: Conclusion and Future Directions
Header_3: References
filename: 505_gslb_the_graph_structure_learn.pdf

References

|[1]|Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang. Rumor detection on social media with bi-directional graph convolutional networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 549–556, 2020.|
|---|---|
|[2]|Karsten M Borgwardt, Cheng Soon Ong, Stefan Schönauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47–i56, 2005.|
|[3]|Chen Cai and Yusu Wang. A simple yet effective baseline for non-attributed graph classification. arXiv preprint arXiv:1811.03508, 2018.|
|[4]|Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Advances in neural information processing systems, 33:19314–19326, 2020.|
|[5]|Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2): 786–797, 1991.|
|[6]|Kaize Ding, Jianling Wang, James Caverlee, and Huan Liu. Meta propagation networks for graph few-shot semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6524–6531, 2022.|
|[7]|Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 330(4):771–783, 2003.|
------------
Header_1: Research Papers
filename: 505_gslb_the_graph_structure_learn.pdf

Research Papers
------------
Header_1: List of Research Papers
filename: 505_gslb_the_graph_structure_learn.pdf

List of Research Papers

|Authors|Title|Conference/Journal|Pages|
|---|---|---|---|
|Vijay Prakash Dwivedi, Ladislav Rampášek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini|Long range graph benchmark|Advances in Neural Information Processing Systems|35:22326–22340|
|Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin|Graph neural networks for social recommendation|The world wide web conference|417–426|
|Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi|Slaps: Self-supervision improves structure learning for graph neural networks|Advances in Neural Information Processing Systems|34:22667–22681|
|Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He|Learning discrete structures for graph neural networks|International conference on machine learning|1972–1982|
|Xiang Gao, Wei Hu, and Zongming Guo|Exploring structure-adaptive graph learning for robust semi-supervised classification|2020 ieee international conference on multimedia and expo (icme)|1–6|

For more research papers, please refer to the complete list.
------------
Header_1: References
filename: 505_gslb_the_graph_structure_learn.pdf

References
------------
Header_1: List of References
filename: 505_gslb_the_graph_structure_learn.pdf

List of References

|#|Authors|Title|Conference/Journal|Pages|Year|
|---|---|---|---|---|---|
|25|Meng Liu, Hongyang Gao, and Shuiwang Ji|Towards Deeper Graph Neural Networks|Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining|338-348|2020|
|26|Nian Liu, Xiao Wang, Lingfei Wu, Yu Chen, Xiaojie Guo, and Chuan Shi|Compact Graph Structure Learning via Mutual Information Compression|Proceedings of the ACM Web Conference 2022|1601-1610|2022|
|27|Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He|Pick and Choose: A GNN-Based Imbalanced Learning Approach for Fraud Detection|Proceedings of the Web Conference 2021|3168-3177|2021|
|28|Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan|Towards Unsupervised Deep Graph Structure Learning|Proceedings of the ACM Web Conference 2022|1392-1403|2022|
|29|Yuanfu Lu, Chuan Shi, Linmei Hu, and Zhiyuan Liu|Relation Structure-Aware Heterogeneous Information Network Embedding|Proceedings of the AAAI Conference on Artificial Intelligence|4456-4463|2019|

$$
\begin{array}{|c|c|c|c|c|c|}
\hline
\text{#} & \text{Authors} & \text{Title} & \text{Conference/Journal} & \text{Pages} & \text{Year} \\
\hline
30 & Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang & Learning to Drop: Robust Graph Neural Network via Topological Denoising & Proceedings of the 14th ACM International Conference on Web Search and Data Mining & 779-787 & 2021 \\
\hline
31 & Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann & Tudataset: A Collection of Benchmark Datasets for Learning with Graphs & arXiv preprint arXiv:2007.08663 & 2020 \\
\hline
32 & Mark EJ Newman & Mixing Patterns in Networks & Physical Review E & 67(2):026126 & 2003 \\
\hline
33 & Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. & Pytorch: An Imperative Style, High-Performance Deep Learning Library & Advances in Neural Information Processing Systems & 32 & 2019 \\
\hline
34 & Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang & Geom-GCN: Geometric Graph Convolutional Networks & arXiv preprint arXiv:2002.05287 & 2020 \\
\hline
\end{array}
$$

- [44] Hui Xu, Liyao Xiang, Jiahao Yu, Anqi Cao, and Xinbing Wang. Speedup robust graph structure learning with low-rank information. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2241–2250, 2021. 16
- [45] Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, and Liang Wang. Evidence-aware fake news detection with graph neural networks. In Proceedings of the ACM Web Conference 2022, pages 2501–2510, 2022. 1
- [46] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365–1374, 2015. 2, 4
- [47] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40–48. PMLR, 2016. 2, 4, 14
- [48] Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised convolutional network. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14–18, 2020, Proceedings, Part III, pages 378–393. Springer, 2021. 2, 4, 15, 16
- [49] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. Advances in neural information processing systems, 32, 2019. 2, 4, 14, 16, 17
- [50] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang. Mining latent structures for multimedia recommendation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 3872–3880, 2021. 2
- [51] Yanfu Zhang, Hongchang Gao, Jian Pei, and Heng Huang. Robust self-supervised structural graph neural network for social network prediction. In Proceedings of the ACM Web Conference 2022, pages 1352–1361, 2022. 1
- [52] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. Hierarchical graph pooling with structure learning. arXiv preprint arXiv:1911.05954, 2019. 2, 4, 16, 17
- [53] Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song, and Yanfang Ye. Heterogeneous graph structure learning for graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 4697–4705, 2021. 2, 4, 16, 17
- [54] Jianan Zhao, Qianlong Wen, Mingxuan Ju, Chuxu Zhang, and Yanfang Ye. Self-supervised graph structure refinement for graph neural networks. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 159–167, 2023. 2, 4, 16
- [55] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In International Conference on Machine Learning, pages 11458–11468. PMLR, 2020. 2, 8, 16
- [56] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793–7804, 2020. 17
- [57] Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Yuanqi Du, Jieyu Zhang, Qiang Liu, Carl Yang, and Shu Wu. A survey on graph structure learning: Progress and opportunities. arXiv e-prints, pages arXiv–2103, 2021. 2
- [58] Daniel Zügner and Stephan Günnemann. Adversarial attacks on graph neural networks via meta learning. 2019. 8
------------
Header_1: DISCS: A Benchmark for Discrete Sampling
filename: 35_discs_a_benchmark_for_discrete.pdf

DISCS: A Benchmark for Discrete Sampling
------------
Header_1: DISCS: A Benchmark for Discrete Sampling
filename: 35_discs_a_benchmark_for_discrete.pdf

DISCS: A Benchmark for Discrete Sampling

Anonymous Author(s)

Affiliation

Address

email
------------
Header_1: DISCS: A Benchmark for Discrete Sampling
Header_2: Abstract
filename: 35_discs_a_benchmark_for_discrete.pdf

Abstract

Sampling in discrete spaces, with critical applications in simulation and optimization, has recently been boosted by significant advances in gradient-based approaches that exploit modern accelerators like GPUs. However, two key challenges hinder the further research progress in discrete sampling. First, since there is no consensus on experimental settings, the empirical results in different research papers are often not comparable. Secondly, implementing samplers and target distributions often requires a nontrivial amount of effort in terms of calibration, parallelism, and evaluation. To tackle these challenges, we propose DISCS (DIScrete Sampling), a tailored package and benchmark that supports unified and efficient implementation and evaluations for discrete sampling in three types of tasks: sampling for classical graphical models, combinatorial optimization, and energy-based generative models. Throughout the comprehensive evaluations in DISCS, we acquired new insights into scalability, design principles for proposal distributions, and lessons for adaptive sampling design. DISCS implements representative discrete samplers in existing research works as baselines, and offers a simple interface that researchers can conveniently design new discrete samplers and compare with baselines in a calibrated setup directly.
------------
Header_1: DISCS: A Benchmark for Discrete Sampling
Header_2: Introduction
filename: 35_discs_a_benchmark_for_discrete.pdf

Introduction

Sampling in discrete spaces has been an important problem in physics (Edwards & Anderson, 1975; Baumgärtner et al., 2012), statistics (Robert & Casella, 2013; Carpenter et al., 2017), and computer science (LeCun et al., 2006; Wang & Cho, 2019) for decades. Since sampling from a target distribution $$\pi(x) \propto \exp(-f(x))$$ in a discrete space X is typically intractable, one usually resorts to MCMC methods (Metropolis et al., 1953; Hastings, 1970). However, except for a few algorithms such as Swedesen-Wang for the Ising model (Swendsen & Wang, 1987) and Hamze-Freitas for hierarchical models (Hamze & de Freitas, 2012), which exploit the special structure of the underlying problem, sampling in a general discrete space has primarily relied on Gibbs sampling, which exhibits notoriously poor efficiency in high-dimensional spaces.

Recently, a family of locally balanced samplers (Zanella, 2020; Grathwohl et al., 2021; Sun et al., 2021; Zhang et al., 2022), using ratio-informed proposal distributions, $$\pi(y) \propto \pi(x)$$, have significantly improved sampling efficiency by exploiting modern accelerators like GPUs and TPUs. From the perspective of gradient flow on the Wasserstein manifold of distributions, Gibbs sampling is simply a coordinate descent algorithm, whereas locally balanced samplers perform as full gradient descent (Sun et al., 2022a). Despite the advances in locally balanced samplers, a quantitative benchmark is still missing. One important reason is that there is no consensus on the experimental setting. Particularly, the initialization of energy-based generative models, random seeds used in graphical models, and the protocol of hyper-parameter tuning all have a significant impact on performance. As a result, some empirical results in different research papers may not be comparable. Under this circumstance, a unified benchmark is in crucial need for boosting the research in discrete sampling.

Submitted to ICML 2023 Workshop: Sampling and Optimization in Discrete Space. Do not distribute.

39 There are two key challenges that seriously hinder the appearance of such a benchmark. First, a sampler may perform well in one target distribution while poorly in another one. To thoroughly examine the performance of a sampler, a qualified benchmark needs to collect a set of representative distributions that covers the potential applications of a discrete sampler. Second, the evaluation of discrete samplers is complicated. Although the commonly used metric ESS (Vehtari et al., 2021) can effectively reflect the efficiency of a sampler in Monte Carlo integration or Bayesian inference, it is not very informative in scenarios when the sampler guides the search in combinatorial optimization problems, or performs as a decoder in deep generative models.

To address the two challenges, we propose DISCS, a tailored benchmark for discrete sampling. In particular, DISCS consists of three groups of tasks: sampling from classical graphical models, sampling for solving combinatorial optimization problems, and sampling from deep EBMs. These tasks cover the topics of simulation and optimization, and models ranging from hand-designed graphical models to learned deep EBMs. For each task, we collect the representative problems from both synthetic and real-world applications, for example graph partitioning for distributed computing and language model for text generation. We carefully design the evaluation metrics in DISCS. In sampling classical graphical models tasks, DISCS uses the ESS as standard. In sampling for solving combinatorial optimization tasks, DISCS runs simulated annealing (Kirkpatrick et al., 1983) with multiple chains and report the average of the best results in each chain. In sampling from energy based generative models, DISCS employs domain specific ways to measure the sample quality. DISCS offers a convenient interface for researchers to implement new discrete samplers, without worrying about parallelism, experiment loop and evaluation. DISCS can efficiently sweep over different tasks and configurations in parallel and thus the evaluation reported in this paper can be easily reproduced. Also, DISCS implements existing discrete samplers random walk Metropolis (Metropolis et al., 1953), block Gibbs, Hamming ball sampler (Titsias & Yau, 2017), LB (Zanella, 2020), GWG (Grathwohl et al., 2021), PAS (Sun et al., 2021), DMALA (Zhang et al., 2022), DLMC (Sun et al., 2022a), and is actively maintaining to add new samplers. Researchers can directly compare the results with the state-of-the-art methods.

With DISCS, we observe an interesting phenomenon that the locally balanced weight function $$g(t) = \sqrt{t}$$ performs better (worse) than $$g(t) = \sqrt{t+1}$$ when Ising model has temperature higher (lower) than the critical temperature. There have been a lot of studies about how to select the locally balanced function for a locally balanced sampler (Zanella, 2020; Sansone, 2022), but the answer remains open. We hope the observations in this paper can provide some insight on this question.

We wrap the DISCS package as a JAX library to facilitate the research in discrete sampling. The library will be open sourced at [https://github.com/google-research/discs](https://github.com/google-research/discs). The paper is organized as follows:

- In section 2, we cover the related sampling tasks and discrete samplers.
- In section 3, we formulate the discrete sampling problem.
- In section 4, we introduce the discrete sampling tasks and evaluation metrics in DISCS. We also report the results for existing discrete samplers.
- In section 5, we discuss the contribution and limitations of DISCS.

**2 Related Work**

Discrete sampling has been widely used to study the physical picture of spin glasses (Hukushima & Nemoto, 1996; Katzgraber et al., 2001), solve combinatorial optimization via simulated annealing (Kirkpatrick et al., 1983), and for training or decoding deep energy based models (Wang & Cho, 2019; Du et al., 2020; Dai et al., 2020b). However, they primarily depend on Gibbs sampling, which could be very slow in high dimensional space.

Since the seminal work Zanella (2020), the recent years have witnessed significant progresses for discrete sampling in both theory and practice. Zanella (2020) introduces the locally balanced proposal $$q(x, y) \propto g(\pi(y)/\pi(x))$$, where $$y \in N(X)$$ restricted within a small neighborhood of $$x$$ and $$g(\cdot) : \mathbb{R}^+ \rightarrow \mathbb{R}^+$$ satisfying $$g(a) = ag(1-a)$$, and prove it is asymptotically optimal. In the following works, PAS (Sun et al., 2021) and DMALA (Zhang et al., 2022) generalize locally balanced proposal to large neighborhoods by introducing an auxiliary path and mimicking the diffusion process, respectively. Inspired by these locally balanced samplers, Sun et al. (2022a) generalize the Langevin dynamics.

92 in continuous space to discrete Langevin dynamics (DLD) in discrete space as a continuous time

$$
\begin{align*}
\text{Markov chain } & dhP(X_{t+h} = y|X_t = x) = g\left( \frac{\pi(y)}{\pi(x)} \right)
\end{align*}
$$
and show that previous locally balanced samplers are simulations of DLD with different discretization strategies. In the view of Wasserstein gradient flow, the Gibbs sampling can be seen as coordinate descent and DLD gives a full gradient descent. Hence, locally balanced samplers induced from DLD provides a principled framework to utilize the modern accelerators like GPUs and TPUs to accelerate discrete sampling. Besides the discretization of DLD, another crucial part to design a locally balanced sampler is estimating the probability ratio $\pi(y) / \pi(x)$. Grathwohl et al. (2021) proposes to use gradient approximation $\pi(y) / \pi(x) \approx \exp(-\langle \nabla f(x), y - x \rangle)$ and obtains good performance on various classical models and deep energy-based models. When the Hessian is available, Rhodes & Gutmann (2022); Sun et al. (2023a) use second-order approximation via Gaussian integral trick (Hubbard, 1959) to further improve the sampling efficiency on skewed target distributions. When the gradient is not available, Xiang et al. (2023) use zero-order approximation via Newton’s series.

Besides designing the sampler, Sun et al. (2022b) proves that when tuning path length in PAS (Sun et al., 2021), the optimal efficiency is obtained when the average acceptance rate is 0.574, and design an adaptive tuning algorithm for PAS. Sansone (2022) learn locally balanced weight function for locally balanced proposal, but how to select the weight function in a principled manner is still unclear.

3 Formulation for Sampling in Discrete Space

The sampling in discrete space can be formulated as the following problem: in a finite discrete space $X$, we have an energy function $f(\cdot) : X \rightarrow \mathbb{R}$. We consider a target distribution

$$
\begin{align*}
\pi(x) &= \exp(-\beta f(x)), \quad Z = \sum_{z \in X} \exp(-\beta f(z)), \quad (1)
\end{align*}
$$
where $\beta$ is the inverse temperature. When the normalizer $Z$ is intractable, people usually resort to Markov chain Monte Carlo (MCMC). Metropolis-Hastings (M-H) (Metropolis et al., 1953; Hastings, 1970) is a commonly used general-purpose MCMC algorithm. Specifically, given a current state $x(t)$, the M-H algorithm proposes a candidate state $y$ from a proposal distribution $q(x(t), y)$. Then, with probability

$$
\begin{align*}
\min \left(1, \frac{\pi(y)q(y, x(t))}{\pi(x(t))q(x(t), y)}\right), \quad (2)
\end{align*}
$$
the proposed state is accepted and $x(t+1) = y$; otherwise, $x(t+1) = x(t)$. In this way, the detailed balance condition is satisfied and the M-H sampler generates a Markov chain $x(0), x(1), ...$ that has $\pi$ as its stationary distribution.

4 Benchmark for Sampling in Discrete Space

The recent development of locally balanced samplers that use the ratio $\pi(y) / \pi(x)$ to guide $q(x, \cdot)$ have significantly improved the sampling efficiency in discrete space. However, there is no consensus for many experimental settings and the empirical results in different research papers may not be comparable. Under this circumstance, we propose DISCS as a benchmark for general-purpose samplers in discrete space. In Section 4.1, we introduce the baselines in DISDS. In Section 4.2, 4.3, 4.4, we introduce the tasks considered in DISCS and how the discrete samplers are evaluated on these tasks. We also report the results of the baselines.

4.1 Baselines

We include both classical discrete samplers and locally balanced samplers in recent research papers as baselines in our benchmark. Specifically, DISCS implements

1. Random Walk Metropolis (RWM) (Metropolis et al., 1953).
2. Block Gibbs (BG), where BG-$a$ denotes using block Gibbs with block size $a$.
3. Hamming Ball Sampler (HB) (Titsias & Yau, 2017), where HB-$a$-$b$ denotes using block size $a$ and Hamming ball size $b$.
------------
Header_1: DISCS: A Benchmark for Discrete Sampling
Header_2: Sampling from Classical Graphical Models
filename: 35_discs_a_benchmark_for_discrete.pdf

Sampling from Classical Graphical Models

This section covers the classical graphical models that are widely used in physics and statistics, including Bernoulli Models, Ising Models (Ising, 1924), and Factorial Hidden Markov Models (Ghahramani & Jordan, 1995). The graphical models have large flexibility, for example, the number of discrete variables, the number of categories for each discrete variable, and the temperature of the model. The performances of different samplers can heavily depend on these configurations. DISCS provides tools to automatically sweep over hundreds of configurations by one click. Same as the routine in Monte Carlo integration or Bayesian inference, DISCS uses the Effective Sample Size (ESS) to measure the efficiency for each sampler and reports the ESS normalized by the number of calling energy function and the ESS normalized by the running time.

We use Ising Models as an example in the main text, and the more results are reported in Appendix. For an Ising Model defined on a 2D grid, where the state space \( X = \{-1, 1\}^{p \times p} \) represents the spins on all nodes. For each state \( x \in X \), the energy function is defined as:

$$
f(x) = -\sum_{i,j} J_{ij}x_ix_j - \sum_{i} h_ix_i \quad (3)
$$

where \( J_{ij} \) is the internal interaction and the \( h_i \) is the external field. The configurations J and h can be set freely in DISCS. In the main text, we report the results using the configuration from Zanella (2020). Specifically, \( J_{ij} = 0.5 \), \( h_i = \mu_i + \sigma_i \), where \( \sigma_i \sim \mathcal{N}(0, 2.25) \) and \( \mu_i = 0.5 \) if node i is located in a circle has the same center as the 2D grid and radius \( 2\sqrt{2} \), else -0.5. We consider the target distribution \( \pi(x) \propto \exp(-\beta f(x)) \), where \( \beta \) is the inverse temperature. Using DISCS, one can easily investigate the influence of the model dimension. In Figure 1, one can see that the traditional samplers, RWM, GB, HB, have a significant decrease in ESS when the model dimension increases, while the locally balanced samplers are less affected as the ratio information \( \pi(y) / \pi(x) \) effectively guides the proposal distribution. The overall trends basically follow the prediction from Sun et al. (2022b) that the ESS is \( O(d^{-1}) \) for RWM and \( O(d^{-\frac{1}{3}}) \) for PAS.

Through DISCS, researchers can also easily evaluate the samplers with different temperatures. In Figure 2, we evaluate Ising models with inverse temperatures from 0.1607 to 0.7607. We consider Ising model without an external field: \( h_i \equiv 0 \) and \( J_{ij} \equiv 1 \) as we know the critical temperature for this configuration is \( \log(1+2\sqrt{2}) \) which means the critical point for the inverse temperature \( \beta = 0.4407 \). From the results, we can see that

$$
4
$$
------------
Header_1: Effect of sample dimension on Ising
filename: 35_discs_a_benchmark_for_discrete.pdf

Effect of sample dimension on Ising
------------
Header_1: Effect of sample dimension on Ising
Header_2: Effect of sample dimension on Ising
filename: 35_discs_a_benchmark_for_discrete.pdf

Effect of sample dimension on Ising

|hb-10-1|bg-2|rmw|
|---|---|---|
|103|gwg t|101|
|ESS w.r.t Energy Evaluation|dmala t|pas t|
|dlmcf t|ESS w.r.t Clock|dlmc t|
|102| | |
| | |101|

Figure 1: Results on Ising model with different dimensions

• The Ising model is harder to sample from when the inverse temperature $$\beta$$ is closer to the critical point, which is consistent with the theory in statistical physics

• When the inverse temperature $$\beta$$ is lower than the critical point, using weight function $$g(t) = t$$ gives larger ESS; When the inverse temperature is larger than the critical point, using weight function $$g(t) = t+1$$ consistently obtains larger ESS.

The second observation implies that one should use ratio function $$\frac{t}{t+1}$$ for target distributions with sharp landscapes. We will revisit this conclusion in Figure 5 and Table 2.

|gwg t|8000|
|---|---|
|dmala tt + 1t|7000|
|pas t|3000|
|dlmcf t t|5000|
|dlmc t|4000|

Figure 2: Performance of locally balanced samplers with different types of weight functions v.s temperature on: (left) 50 × 50 Ising model, (right) 100 × 100 Ising model

The categorical version of Ising model is Potts model, where each site of a state xi has values in a symmetry group, instead of {−1, 1}. For simplicity, we denote the symmetry group as a set of one hot vectors C = {e1, ..., ec} with $$h_i \in \mathbb{R}^C$$, $$J_{ij} \in \mathbb{R}^{C \times C}$$. In this way, the energy function becomes:
$$f(x) = -\sum_{i,j} x_i^T J_{ij} x_j - \sum_i \langle h_i, x_i \rangle$$

The result for BG-2 on Potts model with 256 categories are omitted as it takes over 100 hours.
------------
Header_1: Effect of sample dimension on Ising
Header_2: Effect of sample dimension on Ising
Header_3: Sampling for Solving Combinatorial Optimization
filename: 35_discs_a_benchmark_for_discrete.pdf

Sampling for Solving Combinatorial Optimization

Combinatorial optimization is a core challenge in domains like logistics, supply chain management and hardware design, and has been a fundamental problem of study in computer science for decades. Combining with simulated annealing Kirkpatrick et al. (1983), discrete sampling algorithm is a powerful tool to solve combinatorial optimization problems (Sun et al., 2023b). In expectation, a sampler with a faster mixing rate can find better solutions. Hence, the second type of tasks is sampling for solving combinatorial optimization problems. Currently, DISCS covers four problems: Maximum Independent Set, Max Clique, Max Cut, and Balanced Graph Partition. Without loss of generality, we consider combinatorial optimization that admit the following form:
$$x \in C = \{0,1,...,C_{\text{min}}-1\}^d, \quad a(x), \quad \text{s.t.} \quad b(x) = 0$$
------------
Header_1: Document
filename: 35_discs_a_benchmark_for_discrete.pdf

Document
------------
Header_1: Document
Header_2: Effect of number of categories on Potts
filename: 35_discs_a_benchmark_for_discrete.pdf

Effect of number of categories on Potts

|105|hb-10-1|
|---|---|
|103|hb-10-1|
|bg-2| |
|rmw| |
|104|gwg t|
|ESS w.r.t Energy Evaluation|dmala t|
|102|dmala t|
|pas t| |
|103|dlmcf t|
|ESS w.r.t Clock|dlmcf t|
|101|dlmc t|

Figure 3: Results of Potts models with different number of categories

For ease of exposition, we also assume \( b(x) \geq 0, \forall x \in C \), but otherwise do not limit the form of a and b. To convert the optimization problem to a sampling problem, we first rewrite the constrained optimization into a penalty form via a penalty coefficient \( \lambda \), then treat this as an energy function for an EBM. In particular, the energy function takes the form:

$$ f(x) = a(x) + \lambda \cdot b(x) \tag{6} $$

A naive approach to this problem would be directly sampling from \( p_{\beta \to \infty}(x) \), but such a distribution \( p_{\beta}(x) \propto \exp(-\beta f(x)) \) is highly nonsmooth and unsuitable for MCMC methods. Instead, following classical simulated annealing, we define a sequence of distributions parameterized by a sequence of decaying temperatures:

$$ P = [p_{\beta_0}(x), p_{\beta_1}(x), ..., p_{\beta_T}(x)] \tag{8} $$

where the sequence \( \beta_0 < \beta_1 < ... < \beta_T \to \infty \) converges to a large enough value as T increases.

Example 1: Max Cut

A cut on a graph \( G = (V, E) \) is to find a partition of the graph nodes into two complementary sets \( V = V_1 \cup V_2 \), such that the number of edges in E between \( V_1 \) and \( V_2 \) is as large as possible. Max Cut is an unconstrained problem, which makes its formulation relatively simple.

Example 2: Maximum Independent Set

On a graph \( G = (V, E) \), an independent set \( S \subset V \) means that for any \( i, j \in S \), \( (i, j) \notin E \). We can set \( C = \{0, 1\} \) such that \( x_i = 0 \) means \( i \notin S \) and \( x_i = 1 \) means \( i \in S \). Then we can write \( a(x) = -\sum_{i \in V} x_i \) and \( b(x) = \sum_{(i,j) \in E} x_i x_j \). For the penalty coefficient \( \lambda \), we follow Sun et al. (2022c) to select \( \lambda = 1.0001 \) being a value slightly larger than 1.

Sampling from Energy Based Generative Models

The discrete samplers can also play as the decoder in generative models. In particular, given a dataset \( D = \{X_i\}_{i=1}^N \) sampled from the target distribution \( \pi \), one can train an energy function \( f_{\theta}(\cdot) \), such that the energy based model \( \pi_{\theta}(\cdot) \propto \exp(-f_{\theta}(\cdot)) \) fits the dataset D.
------------
Header_1: OCR Text
filename: 35_discs_a_benchmark_for_discrete.pdf

OCR Text
------------
Header_1: OCR Text
Header_2: Results for MAXCUT on ER graphs
filename: 35_discs_a_benchmark_for_discrete.pdf

Results for MAXCUT on ER graphs

Figure 4: Results for MAXCUT on ER graphs. The ratio is computed by dividing the optimal cut size obtained from running Gurobi for 1 hour. (top) ratio with respect to number of M-H steps, (bottom) ratio with respect to running time.

| |100|101|102 Steps|103|104|
|---|---|---|---|---|---|
|1.02| |1.00|1.00| | |
|1.00| |0.98|0.98| | |
|0.98| |0.96| | | |
------------
Header_1: OCR Text
Header_2: Results for MIS on ER graphs
filename: 35_discs_a_benchmark_for_discrete.pdf

Results for MIS on ER graphs

Table 1: Results for MIS on ER graphs. The set found by the sampling algorithm is not necessarily an independent set, we report a lower bound: set size - # pair of adjacent nodes in the set.

|Sampler|0.05|0.10|ER[700-800]|0.15|0.20|0.25|ER[9000-11000]|0.15|
|---|---|---|---|---|---|---|---|---|
|HB-10-1|100.374|58.750| |41.812|32.344|26.469|277.149| |
|BG-2|102.468|60.000| |42.820|32.250|27.312|316.170| |
|RMW|97.186|56.249| |40.429|31.219|25.594|-555.674| |
|GWG-nA|104.812|62.125| |44.383|34.812|28.187|367.310| |
|DMALA|104.750|62.031| |44.195|34.375|28.031|357.058| |
|PAS|105.062|62.250| |44.570|34.719|28.500|377.123| |
|DLMCf|104.450|62.219| |44.078|34.469|28.125|354.121| |
|DLMC|104.844|62.187| |44.273|34.500|28.281|355.058| |
------------
Header_1: OCR Text
Header_2: Sampling on RBMs
filename: 35_discs_a_benchmark_for_discrete.pdf

Sampling on RBMs

For the models that are relatively simple, for example, Restricted Boltzmann Machine (RBM) trained on MNIST (LeCun, 1998) and fashion-MNIST (Xiao et al., 2017), one can continue using ESS as the metric. In Figure 5, we evaluate the samplers on RBMs trained on MNIST with 25 and 200 hidden variables. One can see that 1) DLMC has the best performance, 2) when the hidden dimension is larger, the learned distribution becomes sharper, hence t+1 obtains better efficiency compared to √ t, which is consistent with our observation in Figure 2. For more complicated deep energy-based models, a sampler may fail to mix within a reasonable number of steps. In this case, ESS is not a good metric. To address this problem, DISCS provides multiple alternative measurements, including snapshots, annealed importance sampling, and domain-specific scores.
------------
Header_1: OCR Text
Header_2: Snapshots
filename: 35_discs_a_benchmark_for_discrete.pdf

Snapshots

Snapshots: After loading the checkpoint of energy-based generative models, DISCS can generate snapshots of the sampling chains. For example, in Figure 6, we display the snapshots of sampling on a deep residual network trained on MNIST data (Sun et al., 2021) and on pretrained language model BERT1. One can see that locally balanced samplers generate samples with higher qualities and can typically visit multiple modalities in the distribution.
------------
Header_1: OCR Text
Header_2: Domain Specific Scores
filename: 35_discs_a_benchmark_for_discrete.pdf

Domain Specific Scores

Domain Specific Scores: In many deep generative tasks, the goal is to efficiently sample high-quality samples, instead of mixing in the learned energy-based models. In this scenario, domain-specific scores that directly evaluate the sample qualities are a better choice. For example, DISCS provides text filling tasks based on pre-trained language models like BERT (Wang & Cho, 2019; Devlin et al., 2018). Following the settings in prior work (Zhang et al., 2022), DISCS randomly samples 20 sentences from TBC (Zhu et al., 2015) and WiKiText-103 (Merity et al., 2016), masks four words in each sentence (Donahue et al., 2020), and samples 25 sentences from the probability distribution given loading the checkpoint from https://huggingface.co/bert-base-uncased.
------------
Header_1: Binary RBM with hidden dimension 25
filename: 35_discs_a_benchmark_for_discrete.pdf

Binary RBM with hidden dimension 25
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 25
filename: 35_discs_a_benchmark_for_discrete.pdf

Binary RBM with hidden dimension 25

$$\begin{array}{|c|c|}
\hline
102 & \text{hb-10-1} \\
\hline
\text{bg-2} & \text{rmw} \\
\hline
\text{gwg t t} & \text{gwg t + 1} \\
\hline
\text{dmala t t} & \text{dmala t + 1} \\
\hline
\text{pas t t} & \text{pas t + 1} \\
\hline
\text{dlmcf t t} & \text{dlmcf t + 1} \\
\hline
\text{dlmc t t} & \text{dlmc t + 1} \\
\hline
\end{array}$$
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 200
filename: 35_discs_a_benchmark_for_discrete.pdf

Binary RBM with hidden dimension 200

$$\begin{array}{|c|c|}
\hline
\text{hb-10-1} & \text{Binary RBM with hidden dimension 200} \\
\hline
\text{bg-2} & \text{rmw} \\
\hline
\text{gwg t t} & \text{gwg t + 1} \\
\hline
\text{dmala t t} & \text{dmala t + 1} \\
\hline
\text{pas t t} & \text{pas t + 1} \\
\hline
\text{dlmcf t t} & \text{dlmcf t + 1} \\
\hline
\text{dlmc t t} & \text{dlmc t + 1} \\
\hline
\end{array}$$
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 200
Header_3: Results on RBMs trained on MNIST dataset
filename: 35_discs_a_benchmark_for_discrete.pdf

Results on RBMs trained on MNIST dataset

- RBMs with 25 binary hidden variables
- RBMs with 200 binary hidden variables
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 200
Header_3: Samplers
filename: 35_discs_a_benchmark_for_discrete.pdf

Samplers

- pas
- gwg
- dmala
- dlmc
- dlmcf
- rmw
- bg-2
- hb-10-1
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 200
Header_3: x1k Steps
filename: 35_discs_a_benchmark_for_discrete.pdf

x1k Steps

- MNIST
- BERT
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 200
Header_3: Snapshots of energy-based generative models
filename: 35_discs_a_benchmark_for_discrete.pdf

Snapshots of energy-based generative models

- Snapshots for every 1k steps on MNIST ResNet
- Snapshots for text filling task on BERT in Table 2

262 by BERT. As a common practice in non-auto-regressive text generation, we select the top-5 sentences
with the highest likelihood out of 25 sentences to avoid low-quality generation (Gu et al., 2017; Zhou
et al., 2019). We evaluate the generated samples in terms of diversity and quality. For diversity,
we use self-BLEU (Zhu et al., 2018) and the number of unique n-grams (Wang & Cho, 2019) to
measure the difference between the generated sentences. For quality, we measure the BLEU score
(Papineni et al., 2002) between the generated texts and the original dataset, which is the combination
of TBC and WikiText-103. We report the quantitative results in Table 2. We do not have the results
for HB and BG as they are computationally infeasible for this task with 30k+ tokens. In this task,
the locally balanced sampler still outperforms RMW. Also, one can notice that the weight function
$$t+1$$ significantly outperforms $$t$$. The reason is that the overparameterized neural network is a low
temperature system with a sharp landscape. This phenomenon is consistent with the results in Figure 2.
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 200
Header_3: Conclusion
filename: 35_discs_a_benchmark_for_discrete.pdf

Conclusion

DISCS is a tailored benchmark for discrete sampling. It implements various discrete sampling tasks
and state-of-the-art discrete samplers and enables a fair comparison. From the results, we know
that DLMC leads in sampling from classical graphical models, PAS leads in solving combinatorial.
------------
Header_1: Quantitative Results on Text Infilling
filename: 35_discs_a_benchmark_for_discrete.pdf

Quantitative Results on Text Infilling
------------
Header_1: Quantitative Results on Text Infilling
Header_2: Table 2: Quantitative results on text infilling
filename: 35_discs_a_benchmark_for_discrete.pdf

Table 2: Quantitative results on text infilling

The reference text for computing the Corpus BLEU is the combination of WT103 and TBC.

|Methods|Self-BLEU|n = 2 Self|n = 3 Self|n = 2 WT103|n = 3 WT103|n = 2 TBC|n = 3 TBC|Corpus BLEU|
|---|---|---|---|---|---|---|---|---|
|RMW|√|92.41|6.26|9.10|18.97|26.73|19.33|26.67|16.24|
|GWG|t|85.93|11.22|17.14|23.16|35.56|23.58|35.56|16.75|
|PAS|t|85.39|11.37|17.60|22.61|35.53|23.65|35.47|16.57|
|DLMCf|√ t|88.39|9.53|14.06|21.00|31.85|22.27|31.98|16.70|
|DLMC|√ t|85.28|12.05|17.65|24.03|36.34|24.51|36.27|16.45|
|GWG|t+1 t|81.15|15.47|22.70|25.62|38.91|25.62|38.58|16.68|
|DMALA|t t+1|80.21|16.36|23.71|25.60|39.39|26.75|39.72|16.53|
|PAS|t+1 t|81.02|15.62|22.65|25.59|39.28|26.08|39.48|16.69|
|DLMCf|t+1|80.12|16.25|23.76|25.41|39.31|26.86|39.57|16.73|
|DLMC|t+1|84.55|12.62|18.47|24.27|37.28|24.94|37.14|16.69|

Optimization problems: DLMCf and DMALA have the best performance on language models. We believe more efficient discrete samplers can be obtained by designing better discretization of DLD (Sun et al., 2022a). DISCS is a convenient tool during this process. The researcher can freely set the configurations for tasks and samplers, and DISCS will automatically compile the program and run the processes in parallel. Besides, we observe that the choice of the locally balanced weight function should depend on the critical temperature of the target distribution. We believe this observation is insightful and will lead to a deeper understanding of locally balanced samplers.

Of course, DISCS does not include all existing tasks or samplers in discrete sampling, for example, the zero order (Xiang et al., 2023) and second order (Sun et al., 2023a) approximation methods. We will keep iterating DISCS, and more features will be added in the future. We wrap DISCS to a JAX library. Researchers can conveniently implement customer tasks or samplers to accelerate their study and, in the meanwhile, contribute the code to DISCS for further improvement. We believe DISCS will be a powerful tool for researchers and facilitate the future research in discrete sampling.
------------
Header_1: Quantitative Results on Text Infilling
Header_2: Table 2: Quantitative results on text infilling
Header_3: References
filename: 35_discs_a_benchmark_for_discrete.pdf

References

1. Baumgärtner, A., Burkitt, A., Ceperley, D., De Raedt, H., Ferrenberg, A., Heermann, D., Herrmann, H., Landau, D., Levesque, D., von der Linden, W., et al. The Monte Carlo method in condensed matter physics, volume 71. Springer Science & Business Media, 2012.
2. Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., and Riddell, A. Stan: A probabilistic programming language. Journal of statistical software, 76(1), 2017.
3. Dai, H., Chen, X., Li, Y., Gao, X., and Song, L. A framework for differentiable discovery of graph algorithms. 2020a.
4. Dai, H., Singh, R., Dai, B., Sutton, C., and Schuurmans, D. Learning discrete energy-based models via auxiliary-variable local exploration. arXiv preprint arXiv:2011.05363, 2020b.
5. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
6. Donahue, C., Lee, M., and Liang, P. Enabling language models to fill in the blanks. arXiv preprint arXiv:2005.05339, 2020.
7. Du, Y., Li, S., Tenenbaum, J., and Mordatch, I. Improved contrastive divergence training of energy based models. arXiv preprint arXiv:2012.01316, 2020.
8. Edwards, S. F. and Anderson, P. W. Theory of spin glasses. Journal of Physics F: Metal Physics, 5(5):965, 1975.

309  Ghahramani, Z. and Jordan, M. Factorial hidden markov models. Advances in Neural Information
Processing Systems, 8, 1995.

310  Grathwohl, W., Swersky, K., Hashemi, M., Duvenaud, D., and Maddison, C. J. Oops I took a gradient:
Scalable sampling for discrete distributions. arXiv preprint arXiv:2102.04509, 2021.

311  Gu, J., Bradbury, J., Xiong, C., Li, V. O., and Socher, R. Non-autoregressive neural machine
translation. arXiv preprint arXiv:1711.02281, 2017.

312  Hamze, F. and de Freitas, N. From fields to trees. arXiv preprint arXiv:1207.4149, 2012.

313  Hastings, W. K. Monte Carlo sampling methods using Markov chains and their applications. 1970.

314  Hubbard, J. Calculation of partition functions. Physical Review Letters, 3(2):77, 1959.

315  Hukushima, K. and Nemoto, K. Exchange monte carlo method and application to spin glass
simulations. Journal of the Physical Society of Japan, 65(6):1604–1608, 1996.

316  Ising, E. Beitrag zur theorie des ferro-und paramagnetismus. PhD thesis, Grefe & Tiedemann, 1924.

317  Katzgraber, H. G., Palassini, M., and Young, A. Monte carlo simulations of spin glasses at low
temperatures. Physical Review B, 63(18):184422, 2001.

318  Kirkpatrick, S., Gelatt Jr, C. D., and Vecchi, M. P. Optimization by simulated annealing. science, 220
(4598):671–680, 1983.

319  LeCun, Y. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.

320  LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F. A tutorial on energy-based learning.
Predicting structured data, 1(0), 2006.

321  Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint
arXiv:1609.07843, 2016.

322  Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. Equation of
state calculations by fast computing machines. The journal of chemical physics, 21(6):1087–1092,
1953.

323  Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting of the Association for Computational
Linguistics, pp. 311–318, 2002.

324  Rhodes, B. and Gutmann, M. Enhanced gradient-based mcmc in discrete spaces. arXiv preprint
arXiv:2208.00040, 2022.

325  Robert, C. and Casella, G. Monte Carlo statistical methods. Springer Science & Business Media,
2013.

326  Sansone, E. Lsb: Local self-balancing mcmc in discrete spaces. In International Conference on
Machine Learning, pp. 19205–19220. PMLR, 2022.

327  Sun, H., Dai, H., Xia, W., and Ramamurthy, A. Path auxiliary proposal for MCMC in discrete space.
In International Conference on Learning Representations, 2021.

328  Sun, H., Dai, H., Dai, B., Zhou, H., and Schuurmans, D. Discrete Langevin sampler via Wasserstein
gradient flow. arXiv preprint arXiv:2206.14897, 2022a.

329  Sun, H., Dai, H., and Schuurmans, D. Optimal scaling for locally balanced proposals in discrete
spaces. arXiv preprint arXiv:2209.08183, 2022b.

330  Sun, H., Guha, E. K., and Dai, H. Annealed training for combinatorial optimization on graphs. arXiv
preprint arXiv:2207.11542, 2022c.

331  Sun, H., Dai, B., Sutton, C., Schuurmans, D., and Dai, H. Any-scale balanced samplers for discrete
space. In The Eleventh International Conference on Learning Representations, 2023a.

10
------------
Header_1: References
filename: 35_discs_a_benchmark_for_discrete.pdf

References
------------
Header_1: References
Header_2: References
filename: 35_discs_a_benchmark_for_discrete.pdf

References

1. Sun, H., Goshvadi, K., Nova, A., Schuurmans, D., and Dai, H. Revisiting sampling for combinatorial optimization. In International Conference on Machine Learning, pp. 19205–19220. PMLR, 2023b.
2. Swendsen, R. H. and Wang, J.-S. Nonuniversal critical dynamics in Monte Carlo simulations. Physical review letters, 58(2):86, 1987.
3. Titsias, M. K. and Yau, C. The Hamming ball sampler. Journal of the American Statistical Association, 112(520):1598–1611, 2017.
4. Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., and Bürkner, P.-C. Rank-normalization, folding, and localization: An improved r for assessing convergence of mcmc (with discussion). Bayesian analysis, 16(2):667–718, 2021.
5. Wang, A. and Cho, K. Bert has a mouth, and it must speak: Bert as a markov random field language model. arXiv preprint arXiv:1902.04094, 2019.
6. Xiang, Y., Zhu, D., Lei, B., Xu, D., and Zhang, R. Efficient informed proposals for discrete distributions via newton’s series approximation. In International Conference on Artificial Intelligence and Statistics, pp. 7288–7310. PMLR, 2023.
7. Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
8. Zanella, G. Informed proposals for local MCMC in discrete spaces. Journal of the American Statistical Association, 115(530):852–865, 2020.
9. Zhang, R., Liu, X., and Liu, Q. A Langevin-like sampler for discrete distributions. In International Conference on Machine Learning, pp. 26375–26396. PMLR, 2022.
10. Zhou, C., Neubig, G., and Gu, J. Understanding knowledge distillation in non-autoregressive machine translation. arXiv preprint arXiv:1911.02727, 2019.
11. Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pp. 19–27, 2015.
12. Zhu, Y., Lu, S., Zheng, L., Guo, J., Zhang, W., Wang, J., and Yu, Y. Texygen: A benchmarking platform for text generation models. In The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 1097–1100, 2018.
------------
Header_1: Document
filename: 35_discs_a_benchmark_for_discrete.pdf

Document

105 Effect of sample dimension on Bernoulli hb-10-1

103 Effect of sample dimension on Bernoulli hb-10-1

bg-2

rmw

gwg t 102

104 dmala t

ESS w.r.t Energy Evaluation

pas t

dlmcf t 101

103 dlmc t ESS w.r.t Clock

102 100

101 10 1

100 10 2

2000 10000 100000 500000

gwg Effect of balancing function type on Bernoulli 103 gwg Effect of balancing function type on Bernoulli

104 dmala

pas

dlmcf

dlmc

ESS w.r.t Energy Evaluation 102 ESS w.r.t Clock

103 101

100

1 t 1 t t t 1 t 1 t t t + 1

104 hb-10-1 High temperature Bernoulli 103 hb-10-1 High temperature Bernoulli

bg-2

rmw

gwg t t

gwg t + 1

ESS w.r.t Energy Evaluation 102 t + 1

103 dmala t t

dmala t + 1 ESS w.r.t Clock dmala t + 1

pas t t

pas t + 1 101 pas t + 1

102 dlmcf t t

dlmcf t + 1 dlmcf t + 1

dlmc t t

101 dlmc t + 1 100 dlmc t + 1

100 10 1

104 hb-10-1 Low temperature Bernoulli 103 hb-10-1 Low temperature Bernoulli

bg-2

rmw

gwg t t

gwg t + 1 102 gwg t + 1

ESS w.r.t Energy Evaluation

103 dmala t t

dmala t + 1 ESS w.r.t Clock dmala t + 1

pas t t

pas t + 1 101 pas t + 1

102 dlmcf t t

dlmcf t + 1 dlmcf t + 1

dlmc t t

101 dlmc t + 1 100 dlmc t + 1

100 10 1

Figure 7: Bernoulli

380 A Appendix

381 A.1 Put to Appendix

12
------------
Header_1: Sample Dimension and Categorical Data Analysis
filename: 35_discs_a_benchmark_for_discrete.pdf

Sample Dimension and Categorical Data Analysis
------------
Header_1: Sample Dimension and Categorical Data Analysis
Header_2: Effect of Sample Dimension on Categorical Data
filename: 35_discs_a_benchmark_for_discrete.pdf

Effect of Sample Dimension on Categorical Data

When analyzing categorical data, the sample dimension can have a significant impact on the results. The following table illustrates this effect:

$$
\begin{array}{|c|c|c|c|c|}
\hline
\text{Sample Dimension} & 250 & 2000 & 32000 & 512000 \\
\hline
104 & \text{gwg} & \text{Effect of balancing function type on Categorical} & 103 & \text{gwg} & \text{Effect of balancing function type on Categorical} \\
& \text{dmala} & & \text{dmala} & & \\
& \text{pas} & & \text{pas} & & \\
& \text{dlmcf} & & \text{dlmcf} & & \\
& \text{dlmc} & & \text{dlmc} & & \\
\text{ESS w.r.t Energy Evaluation} & & & 102 & & \\
103 & & & \text{ESS w.r.t Clock} & & \\
\hline
\end{array}
$$
------------
Header_1: Sample Dimension and Categorical Data Analysis
Header_2: Effect of Number of Categories on Categorical Data
filename: 35_discs_a_benchmark_for_discrete.pdf

Effect of Number of Categories on Categorical Data

Another important factor to consider in categorical data analysis is the number of categories. The table below shows the impact of different numbers of categories:

$$
\begin{array}{|c|c|c|c|c|}
\hline
\text{Number of Categories} & 4 & 16 & 64 & 256 \\
\hline
104 & \text{gwg} & & 103 & \text{gwg} \\
& \text{dmala} & & \text{dmala} & \\
& \text{pas} & & \text{pas} & \\
& \text{dlmcf} & & \text{dlmcf} & \\
& \text{dlmc} & & \text{dlmc} & \\
\text{ESS w.r.t Energy Evaluation} & & & 102 & \\
103 & & & \text{ESS w.r.t Clock} & \\
\hline
\end{array}
$$
------------
Header_1: Sample Dimension and Categorical Data Analysis
Header_2: Results of Categorical Data Analysis
filename: 35_discs_a_benchmark_for_discrete.pdf

Results of Categorical Data Analysis

The following table presents the results of the categorical data analysis for different samplers:

|Sampler|Results|16-20|32-10|64-75|128-150|BA|256-300|512-600|1024-1100|256-300|512-600|ER|1024-1100|OPTSICOM|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|HB-10-1|Ratio α|1.000|1.000|1.000|1.000|1.000| |1.008|1.014|1.020|1.000|0.998|1.000| |
| |Time(s)|371.284|377.306|374.813|391.639| |396.169|571.651|945.267|165.510|208.001|744.191|37.673| |
|BG-2|Ratio α|1.000|1.000|1.000|1.000|1.000| |1.009|1.014|1.021|1.001|0.999|1.000| |
| |Time(s)|258.592|269.129|275.041|276.931| |265.860|289.496|578.785|134.558|168.507|647.610|8.525| |
|RMW|Ratio α|0.998|1.000|1.000|1.000| |0.999|1.005|1.007|1.019|0.997|0.996|1.000| |
| |Time(s)|267.107|267.307|264.320|279.304| |270.651|287.389|532.926|133.536|166.701|633.315|29.480| |
|GWG-nA|Ratio α|1.000|1.000|1.000|1.000|1.000| |1.010|1.017|1.021|1.002|1.001|1.000| |
| |Time(s)|261.047|265.713|289.458|275.961| |272.817|362.360|713.788|132.10|233.100|833.010|40.062| |
|DMALA|Ratio α|1.000|1.000|1.000|1.000|1.000| |1.010|1.018|1.021|1.002|1.002|1.000| |
| |Time(s)|265.716|269.469|284.112|274.513| |272.284|375.455|745.436|138.927|230.589|821.567|26.754| |
|PAS|Ratio α|1.000|1.000|1.000|1.000|1.000| |1.010|1.018|1.021|1.002|1.002|1.000| |
| |Time(s)|259.921|269.407|275.017|275.289| |290.025|470.204|958.977|146.716|465.481|3400.855|29.607| |
|DLMCF|Ratio α|1.000|1.000|1.000|1.000|1.000| |1.010|1.018|1.021|1.002|1.001|1.000| |
| |Time(s)|260.800|263.145|272.938|278.782| |266.559|382.859|755.190|136.420|226.126|819.769|26.276| |
|DLMC|Ratio α|1.000|1.000|1.000|1.000|1.000| |1.010|1.018|1.021|1.002|1.002|1.000| |
| |Time(s)|265.501|275.059|271.643|272.305| |271.338|382.552|782.099|135.631|225.540|821.111|26.684| |
------------
Header_1: Effect of sample dimension on Ising
filename: 35_discs_a_benchmark_for_discrete.pdf

Effect of sample dimension on Ising
------------
Header_1: Effect of sample dimension on Ising
Header_2: Effect of sample dimension on Ising
filename: 35_discs_a_benchmark_for_discrete.pdf

Effect of sample dimension on Ising

|103|gwg t|101|
|---|---|---|
|ESS w.r.t Energy Evaluation|dmala t|dmala t|
| |pas t|pas t|
| |dlmcf t|dlmcf t|
| |dlmc t|dlmc t|
|102| |100|
------------
Header_1: Effect of sample dimension on Ising
Header_2: Effect of balancing function type on Ising
filename: 35_discs_a_benchmark_for_discrete.pdf

Effect of balancing function type on Ising

| |25x25|50x50|100x100|250x250|
|---|---|---|---|---|
|gwg| | | | |
|dmala| | | | |
|pas| | | | |
|dlmcf| | | | |
|dlmc| | | | |
|ESS w.r.t Energy Evaluation|101| | | |
|102| | | | |
------------
Header_1: Effect of sample dimension on Ising
Header_2: High temperature Ising
filename: 35_discs_a_benchmark_for_discrete.pdf

High temperature Ising

|hb-10-1| |
|---|---|
|bg-2| |
|rmw| |
|gwg t|t|
|gwg|t + 1|
|ESS w.r.t Energy Evaluation|102|
------------
Header_1: Effect of sample dimension on Ising
Header_2: Low temperature Ising
filename: 35_discs_a_benchmark_for_discrete.pdf

Low temperature Ising

|hb-10-1| |
|---|---|
|bg-2| |
|rmw|101|
|gwg t|t|
|gwg|t + 1|
|ESS w.r.t Energy Evaluation|102|

Figure 9: Ising

|$\text{pas}$|$\text{pas}$|$\text{pas}$|
|---|---|---|
|$\text{gwg}$|$\text{gwg}$|$\text{gwg}$|
|$\text{dmala}$|$\text{dmala}$|$\text{dmala}$|
|$\text{dlmc}$|$\text{dlmc}$|$\text{dlmc}$|
|Samplers|Samplers|Samplers|
|$\text{dlmcf}$|$\text{dlmcf}$|rmw|
|rmw|rmw|$\text{dlmcf}$|
|bg-2|bg-2|bg-2|
|hb-10-1|hb-10-1|hb-10-1|
|1|2|3|4|5|6|7|8|9|10|1|2|3|4|5|6|7|8|9|10|1|2|3|4|5|6|7|8|9|10|
|$x1k \text{ Steps}$|Omniglot|MNIST|Caltech|
|Figure 10: EBM|Effect of sample dimension on Potts|Effect of sample dimension on Potts|Effect of sample dimension on Potts|
|hb-10-1|bg-2|rmw|101|
|gwg t|dmala t|pas t|dlmcf t|dlmc t|ESS w.r.t Clock|100|
|10x10|30x30|100x100|300x300|Effect of balancing function type on Potts|101|Effect of balancing function type on Potts|
|gwg|dmala|pas|dlmcf|dlmc|ESS w.r.t Energy Evaluation|ESS w.r.t Clock|100|
|1 t|1 t|t|t|t + 1|t + 1|
|Effect of number of categories on Potts|hb-10-1|103|Effect of number of categories on Potts|hb-10-1|
|bg-2|rmw|gwg t|dmala t|pas t|dlmcf t|dlmc t|101|
|100|10 1|10 2|
|4|16|64|256|Figure 11: Potts|15|
------------
Header_1: Effect of sample dimension on Ising
Header_2: Smooth Binary FHMM
filename: 35_discs_a_benchmark_for_discrete.pdf

Smooth Binary FHMM

|hb-10-1|bg-2|rmw|
|---|---|---|
|gwg $t$ $t$|gwg $t + 1$| |

ESS w.r.t Energy Evaluation 100

|dmala $t$ $t$|dmala $t + 1$|
|---|---|
|pas $t$ $t$|pas $t + 1$|
|dlmcf $t$ $t$|dlmcf $t + 1$|
|dlmc $t$ $t$|dlmc $t + 1$|

10 1 100
------------
Header_1: Effect of sample dimension on Ising
Header_2: Sharp Binary FHMM
filename: 35_discs_a_benchmark_for_discrete.pdf

Sharp Binary FHMM

|hb-10-1|bg-2|rmw|
|---|---|---|
|gwg $t$ $t$|gwg $t + 1$| |

ESS w.r.t Energy Evaluation 100

|dmala $t$ $t$|dmala $t + 1$|
|---|---|
|pas $t$ $t$|pas $t + 1$|
|dlmcf $t$ $t$|dlmcf $t + 1$|
|dlmc $t$ $t$|dlmc $t + 1$|

10 1 100

|hb-10-1 Categorical FHMM with 4 categories|
|---|
|bg-2|rmw|
|gwg $t$ $t$|gwg $t + 1$|

ESS w.r.t Energy Evaluation

|dmala $t$ $t$|dmala $t + 1$|
|---|---|
|pas $t$ $t$|pas $t + 1 100|
|dlmcf $t$ $t$|dlmcf $t + 1$|
|dlmc $t$ $t$|dlmc $t + 1$|

10 1 100

|hb-10-1 Categorical FHMM with 8 categories|
|---|
|bg-2|rmw|
|gwg $t$ $t$|gwg $t + 1$|

ESS w.r.t Energy Evaluation

|dmala $t$ $t$|dmala $t + 1 100|
|---|---|
|pas $t$ $t ESS w.r.t Clock|pas $t + 1$|
|dlmcf $t$ $t|dlmcf $t + 1$|
|dlmc $t$ $t|dlmc $t + 1$|

10 1

Figure 12: FHMM 16
------------
Header_1: Binary RBM with hidden dimension 25
filename: 35_discs_a_benchmark_for_discrete.pdf

Binary RBM with hidden dimension 25
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 25
filename: 35_discs_a_benchmark_for_discrete.pdf

Binary RBM with hidden dimension 25

|102|hb-10-1|
|---|---|
| |bg-2|
| |rmw|
| |gwg $t$ $t$|
| |gwg $t + 1$|

ESS w.r.t Energy Evaluation

| |dmala $t$ $t$|
|---|---|
| |dmala $t + 1$|
| |pas $t$ $t$|
| |pas $t + 1$|
| |dlmcf $t$ $t$|
|101|dlmcf $t + 1$|
| |dlmc $t$ $t$|
| |dlmc $t + 1$|
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Binary RBM with hidden dimension 200
filename: 35_discs_a_benchmark_for_discrete.pdf

Binary RBM with hidden dimension 200

|hb-10-1|
|---|
|bg-2|
|rmw|
|gwg $t$ $t$|
| |gwg $t + 1$|

ESS w.r.t Energy Evaluation

|101|dmala $t$ $t$|
|---|---|
| |dmala $t + 1$|
| |pas $t$ $t$|
| |pas $t + 1$|
|6 x 100|dlmcf $t$ $t$|
| |dlmcf $t + 1$|
|4 x 100|dlmc $t$ $t$|
| |dlmc $t + 1$|
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Categorical RBM with 4 categories
filename: 35_discs_a_benchmark_for_discrete.pdf

Categorical RBM with 4 categories

|hb-10-1|
|---|
|bg-2|
|rmw|
|gwg $t$ $t$|
| |gwg $t + 1$|

ESS w.r.t Energy Evaluation

| |dmala $t$ $t$|
|---|---|
| |dmala $t + 1$|
| |pas $t$ $t$|
| |pas $t + 1$|
| |dlmcf $t$|
|101|dlmc $t$ $t + 1$|
| |dlmc $t + 1$|
------------
Header_1: Binary RBM with hidden dimension 25
Header_2: Categorical RBM with 8 categories
filename: 35_discs_a_benchmark_for_discrete.pdf

Categorical RBM with 8 categories

|hb-10-1|
|---|
|bg-2|
|rmw|
|gwg $t$ $t$|
| |gwg $t + 1$|

ESS w.r.t Energy Evaluation

|102|dmala $t$ $t$|
|---|---|
| |dmala $t + 1$|
| |pas $t$ $t$|
| |pas $t + 1$|
| |dlmcf $t$|
|101|$t + 1$|
| |dlmc $t$ $t$|
| |dlmc $t + 1$|

Figure 13: rbm

| |1.00|1.00|
|---|---|---|
|BA-16-20|0.99|0.99|
| |sampler=bg-2|Ratio|sampler=bg-2|
|Ratio|0.98| |
|0.98|sampler=hb-10-1| |sampler=hb-10-1|
| |sampler=dmala| |
|0.97|sampler=pas|0.97|sampler=pas|
| |sampler=gwg-nA| |
| |sampler=dlmc| |
|0.96|sampler=dlmcf|0.96|sampler=dlmcf|
| |sampler=rmw| |
| |0|20000|40000|60000|80000|100000|0|100|200|300|400|500|600|700|
| |Steps|Time (s)|

$$
\begin{array}{|c|c|c|}
\hline
1.00 & 1.00 \\
\hline
BA-32-40 & 0.98 & 0.98 \\
\hline
& sampler=bg-2 & Ratio & sampler=bg-2 \\
\hline
0.96 & sampler=hb-10-1 & 0.96 & sampler=hb-10-1 \\
\hline
& sampler=dmala & & sampler=dmala \\
\hline
& sampler=pas & 0.94 & sampler=pas \\
\hline
& sampler=gwg-nA & & sampler=gwg-nA \\
\hline
& sampler=dlmc & & sampler=dlmc \\
\hline
0.92 & sampler=dlmcf & 0.92 & sampler=dlmcf \\
\hline
& sampler=rmw & & sampler=rmw \\
\hline
& 0 & 20000 & 40000 & 60000 & 80000 & 100000 & 0 & 100 & 200 & 300 & 400 & 500 & 600 & 700 \\
\hline
Steps & Time (s) \\
\hline
\end{array}
$$

| |1.00|1.00|
|---|---|---|
|BA-64-75|0.96|0.96|
| |sampler=bg-2|Ratio|sampler=bg-2|
|0.94|sampler=hb-10-1|0.94|sampler=hb-10-1|
| |sampler=dmala| |
|0.92|sampler=pas|0.92|sampler=pas|
| |sampler=dlmc| |
| |sampler=dlmcf| |
|0.90|sampler=rmw|0.90|sampler=rmw|
| |sampler=gwg-nA| |
| |0|20000|40000|60000|80000|100000|0|100|200|300|400|500|600|700|
| |Steps|Time (s)|

$$
\begin{array}{|c|c|c|}
\hline
1.00 & 1.00 \\
\hline
BA-128-150 & 0.975 \\
\hline
0.975 & & \\
\hline
0.950 & 0.950 & \\
\hline
& 0.925 & sampler=dlmc \\
\hline
0.925 & sampler=dlmc & Ratio \\
\hline
Ratio & sampler=pas & & sampler=pas \\
\hline
0.900 & sampler=dlmcf & 0.900 & sampler=dlmcf \\
\hline
& sampler=dmala & & sampler=dmala \\
\hline
0.875 & sampler=gwg-nA & 0.875 & sampler=gwg-nA \\
\hline
0.850 & sampler=hb-10-1 & 0.850 & sampler=hb-10-1 \\
\hline
& sampler=bg-2 & & sampler=bg-2 \\
\hline
0.825 & sampler=rmw & 0.825 & sampler=rmw \\
\hline
& 0 & 20000 & 40000 & 60000 & 80000 & 100000 & 0 & 100 & 200 & 300 & 400 & 500 & 600 & 700 & 800 \\
\hline
Steps & Time (s) \\
\hline
\end{array}
$$

| |1.00|1.00|
|---|---|---|
|BA-512-600|0.95|0.95|
| |0.90|sampler=pas|
|0.90|sampler=pas|Ratio|
|Ratio|sampler=dmala| |
|0.85|sampler=dlmc|0.85|sampler=dlmc|
| |sampler=dlmcf| |
| |sampler=gwg-nA| |
|0.80|sampler=bg-2|0.80|sampler=bg-2|
| |sampler=hb-10-1| |
|0.75|sampler=rmw|0.75|sampler=rmw|
| |0|20000|40000|60000|80000|100000|0|200|400|600|800|1000|
|Steps| |Time (s)|

|BA-1024-1100|1.00|1.00|
|---|---|---|
| |0.95|0.95|
| |0.90|sampler=pas|0.90|
|Ratio| |Ratio|sampler=pas|
| |sampler=dmala| |sampler=dmala|
| |0.85|sampler=dlmc|0.85|sampler=dlmc|
| |sampler=dlmcf| |sampler=dlmcf|
| |0.80|sampler=gwg-nA|0.80|sampler=gwg-nA|
| |sampler=bg-2| |sampler=bg-2|
| |0.75|sampler=hb-10-1|0.75|sampler=hb-10-1|
| |sampler=rmw| |sampler=rmw|
| |0|20000|40000|60000|80000|100000|0|250|500|750|1000|1250|1500|1750|2000|
| |Steps| |Time (s)|

$$
\begin{align*}
&1.02 \\
&ER-256-300 \\
&1.00 \\
&1.00 \\
&0.98 \\
&Ratio \\
&\text{sampler=dmala} \\
&0.96 \\
&\text{sampler=dlmcf} \\
&0.96 \\
&\text{sampler=dlmc} \\
&0.94 \\
&\text{sampler=pas} \\
&0.94 \\
&\text{sampler=gwg-nA} \\
&0.92 \\
&\text{sampler=bg-2} \\
&0.92 \\
&\text{sampler=hb-10-1} \\
&0.90 \\
&\text{sampler=rmw} \\
&0.90 \\
&0 \\
&10000 \\
&20000 \\
&30000 \\
&40000 \\
&50000 \\
&0 \\
&50 \\
&100 \\
&150 \\
&200 \\
&250 \\
&300 \\
&\text{Steps} \\
&\text{Time (s)} \\
\end{align*}
$$

|1.00|ER-512-600|
|---|---|
| |0.98|
| |0.96|
|Ratio|sampler=pas|
| |sampler=dlmc|
|0.94|sampler=dmala|
| |sampler=dlmcf|
|0.92|sampler=gwg-nA|
| |sampler=bg-2|
|0.90|sampler=hb-10-1|
| |sampler=rmw|
|0|10000|20000|30000|40000|50000|0|100|200|300|400|500|
| |Steps| |Time (s)|

$$
\begin{align*}
&1.00 \\
&ER-1024-1100 \\
&0.98 \\
&Ratio \\
&\text{sampler=pas} \\
&0.96 \\
&\text{sampler=dlmc} \\
&\text{sampler=dmala} \\
&0.94 \\
&\text{sampler=gwg-nA} \\
&\text{sampler=dlmcf} \\
&\text{sampler=bg-2} \\
&0.92 \\
&\text{sampler=hb-10-1} \\
&\text{sampler=rmw} \\
&0 \\
&10000 \\
&20000 \\
&30000 \\
&40000 \\
&50000 \\
&0 \\
&250 \\
&500 \\
&750 \\
&1000 \\
&1250 \\
&1500 \\
&1750 \\
&\text{Steps} \\
&\text{Time (s)} \\
\end{align*}
$$

|1.000|
|---|
|0.975|
|0.950|
| |sampler=rmw|0.925|sampler=rmw|
|Ratio|sampler=hb-10-1|
|0.900|sampler=dmala|0.900|sampler=dmala|
|0.875|sampler=pas|0.875|sampler=pafs|
| |sampler=dlmc|0.850|sampler=dlmc|
|0.850|sampler=dlmcf|0.850|sampler=dlmcf|
|0.825|sampler=bg-2|0.825|sampler=bg-2|
| |sampler=gwg-nA| |sampler=gwg-nA|
|0.800|0|5000|10000|15000|20000|25000|0.800|0|10|20|30|40|50|60|70|80|
| |Steps| |Time (s)|
------------
Header_1: Math Equations and Tables
filename: 35_discs_a_benchmark_for_discrete.pdf

Math Equations and Tables
------------
Header_1: Math Equations and Tables
Header_2: Math Equations
filename: 35_discs_a_benchmark_for_discrete.pdf

Math Equations

Equation 1: $$1.0 \quad 0.8 \quad 0.6 \quad 0.4 \quad 0.2 \quad 0.0$$

Equation 2: $$1.0 \quad 0.8 \quad 0.6 \quad 0.4 \quad 0.2 \quad 0.0$$

Equation 3: $$1.0 \quad 0.8 \quad 0.6 \quad 0.4 \quad 0.2 \quad 0.0$$

Equation 4: $$1.0 \quad 0.8 \quad 0.6 \quad 0.4 \quad 0.2 \quad 0.0$$
------------
Header_1: Math Equations and Tables
Header_2: Tables
filename: 35_discs_a_benchmark_for_discrete.pdf

Tables

|Sampler|Graphs|ER[700-800]|ER[9000-11000]|SATLIB|
|---|---|---|---|---|
|HB-10-1|Density|0.05|0.10|0.15|0.20|0.25|0.15|
| |Size|100.374|58.750|41.812|32.344|26.469|277.149|434.804|
| |Time(s)|213.092|377.306|342.295|207.034|214.940|7569.712|2063.689|
|BG-2|Size|102.468|60.000|42.820|32.250|27.312|316.170|434.545|
| |Time(s)|145.713|195.405|281.493|147.512|144.054|6539.562|1477.161|
|RMW|Size|97.186|56.249|40.429|31.219|25.594|-555.674|432.746|
| |Time(s)|142.046|145.021|249.789|148.570|140.886|6200.869|1468.328|
|GWG-nA|Size|104.812|62.125|44.383|34.812|28.187|367.310|435.419|
| |Time(s)|139.442|146.758|368.836|151.717|155.275|12349.148|1488.152|
|DMALA|Size|104.750|62.031|44.195|34.375|28.031|357.058|436.152|
| |Time(s)|145.635|154.437|357.307|148.924|149.366|12384.69|1494.575|
|PAS|Size|105.062|62.250|44.570|34.719|28.500|377.123|436.644|
| |Time(s)|149.502|155.382|379.686|149.785|154.238|12621.083|1517.682|
|DLMCF|Size|104.450|62.219|44.078|34.469|28.125|354.121|435.894|
| |Time(s)|145.683|150.777|363.143|151.334|150.206|12446.108|1486.004|
|DLMC|Size|104.844|62.187|44.273|34.500|28.281|355.058|436.046|
| |Time(s)|146.617|147.487|362.663|147.344|149.942|12488.156|1428.965|

|ER[800-800-0.05]| |100|100|
|---|---|---|---|
| |75|75| |
|Size of Independent Set|50|50| |
| |25|sampler=pas|25|sampler=pas|
| |0|sampler=dlmc|0|sampler=dlmc|
| |25|sampler=dmala|25|sampler=dmala|
| |50|sampler=bg-2|50|sampler=bg-2|
| |75|sampler=hb-10-1|75|sampler=hb-10-1|
| |100|0|10000|20000|30000|40000|50000|100|0|100|200|300|400|
| | |Steps| | | | | |Time (s)|

$$
ER[800-800-0.10]
$$

| |60|60|
|---|---|---|
| |40|40|
|Size of Independent Set|20|20|
| |0|sampler=pas|0|sampler=pas|
| |20|sampler=dlmcf|20|sampler=dlmcf|
| |40|sampler=gwg-nA|40|sampler=gwg-nA|
| |60|sampler=bg-2|60|sampler=bg-2|
| |80|sampler=hb-10-1|80|sampler=hb-10-1|
| |100|0|10000|20000|30000|40000|50000|100|0|50|100|150|200|250|300|350|400|
| | |Steps| | | | | |Time (s)|

$$
ER[800-800-0.20]
$$

| |40|40|
|---|---|---|
| |20|20|
|Size of Independent Set|0|0|
| |20|sampler=gwg-nA|20|sampler=gwg-nA|
| |40|sampler=dlmc|40|sampler=dlmc|
| |60|sampler=dmala|60|sampler=dmala|
| |80|sampler=hb-10-1|80|sampler=hb-10-1|
| |100|0|10000|20000|30000|40000|50000|100|0|100|200|300|400|
| | |Steps| | | | | |Time (s)|

$$
ER[800-800-0.25]
$$

| |20|20|
|---|---|---|
|Size of Independent Set|0|0|
| |20|sampler=pas|20|sampler=pas|
| |40|sampler=gwg-nA|40|sampler=gwg-nA|
| |60|sampler=dmala|60|sampler=dmala|
| |80|sampler=hb-10-1|80|sampler=hb-10-1|
| |100|0|10000|20000|30000|40000|50000|100|0|100|200|300|400|
| | |Steps| | | | | |Time (s)|

|440|
|---|
|420|
|Size of Independent Set|
|SATLIB|400|400|
|380|sampler=pafs|380|sampler=pafs|
| |sampler=dmala| |sampler=dmala|
|360|sampler=dlmc|360|sampler=dlmc|
| |sampler=dlmcf| |sampler=dlmcf|
|340|sampler=gwg-nA|340|sampler=gwg-nA|
| |sampler=hb-10-1| |sampler=hb-10-1|
|320|sampler=bg-2|320|sampler=bg-2|
| |sampler=rmw| |sampler=rmw|
|300|0|100000|200000|300000|400000|500000|300|0|1000|2000|3000|4000|
| |Steps| | | | | | |Time (s)|
------------
Header_1: Document
filename: 35_discs_a_benchmark_for_discrete.pdf

Document
------------
Header_1: Document
Header_2: Table 5: MAXCLIQUE
filename: 35_discs_a_benchmark_for_discrete.pdf

Table 5: MAXCLIQUE

|Sampler|Results|RB|TWITTER|
|---|---|---|---|
|HB-10-1|Ratio α|0.850|0.966|
| |Time(s)|862.447|3.408|
|BG-2|Ratio α|0.859|0.995|
| |Time(s)|796.404|3.163|
|RMW|Ratio α|0.841|0.584|
| |Time(s)|841.698|2.832|
|GWG-nA|Ratio α|0.878|0.999|
| |Time(s)|1262.900|3.016|
|DMALA|Ratio α|0.876|0.999|
| |Time(s)|1280.807|3.095|
|PAS|Ratio α|0.878|0.999|
| |Time(s)|1271.269|3.090|
|DLMCF|Ratio α|0.871|0.999|
| |Time(s)|1266.417|2.994|
|DLMC|Ratio α|0.875|0.999|
| |Time(s)|1319.794|3.062|

**Table 6: Graph partition.**
|Metric|Samplers|VGG|MNIST-conv|ResNet|AlexNet|Inception-v3|
|---|---|---|---|---|---|---|
| |HB-10-1|0.050|0.046|0.050|0.037|0.065|
| |BG-2|0.048|0.045|0.050|0.038|0.069|
| |RMW|0.054|0.046|0.092|0.052|0.117|
| |GWG|0.102|0.046|0.159|0.063|0.164|
| |DMALA|0.084|0.058|0.178|0.063|0.176|
| |DMALA-nA|0.059|0.045|0.048|0.039|0.054|
|Edge cut ratio ↓|PAS|0.053|0.045|0.047|0.037|0.052|
| |PAS-nA|0.084|0.050|0.138|0.053|0.144|
| |DLMCF|0.086|0.063|0.178|0.053|0.176|
| |DLMCF-nA|0.092|0.069|0.048|0.085|0.052|
| |DLMC|0.105|0.056|0.183|0.097|0.182|
| |DLMC-nA|0.113|0.048|0.082|0.091|0.086|
| |HB-10-1|0.999|0.999|0.999|0.999|0.999|
| |BG-2|0.999|0.997|0.999|0.999|0.999|
| |RMW|0.999|0.998|0.999|0.999|0.999|
| |GWG|0.999|0.997|0.999|0.999|0.999|
| |DMALA|0.999|0.998|0.999|0.999|0.999|
| |DMALA-nA|0.999|0.997|0.999|0.999|0.999|
|Balanceness ↑|PAS|0.999|0.997|0.999|1.000|0.999|
| |PAS-nA|0.999|0.998|0.999|0.999|0.999|
| |DLMCF|0.999|0.997|0.999|0.999|0.999|
| |DLMCF-nA|0.999|0.995|0.999|0.999|0.999|
| |DLMC|0.999|0.994|0.999|0.999|0.999|
| |DLMC-nA|0.999|0.993|0.999|0.999|0.999|

|Methods|Self-BLEU (↓)|n = 2 Self n = 3|n = 2WT103|n = 3|n = 2 TBC|n = 3|Corpus BLEU (↑)|
|---|---|---|---|---|---|---|---|
|RMW|√|92.41|6.26|9.10|18.97|26.73|19.33|26.67|16.24|
|GWG|tt|85.93|11.22|17.14|23.16|35.56|23.58|35.56|16.75|
|GWG|t+1 √|81.15|15.47|22.70|25.62|38.91|25.62|38.58|16.68|
|DMALA-nA| | |tt|83.99|13.26|19.52|24.33|36.40|25.30|36.40|16.37|
|DMALA-nA|√ t+1|80.44|15.86|23.58|25.79|39.88|26.57|40.20|16.64|
|DMALA|t t|85.88|11.58|17.14|22.07|34.08|23.22|34.15|17.06|
|DMALA|√ t+1|80.21|16.36|23.71|25.60|39.39|26.75|39.72|16.53|
|PAS|tt|85.39|11.37|17.60|22.61|35.53|23.65|35.47|16.57|
|PAS|t+1 √|81.02|15.62|22.65|25.59|39.28|26.08|39.48|16.69|
|DLMCf-nA| | |tt|91.57|7.25|10.42|19.53|28.31|20.13|28.18|16.56|
|DLMCf-nA|√ t+1|81.66|15.31|21.78|26.39|39.56|27.60|39.69|16.31|
|DLMCf|tt|88.39|9.53|14.06|21.00|31.85|22.27|31.98|16.70|
|DLMCf|t+1|80.12|16.25|23.76|25.41|39.31|26.86|39.57|16.73|
|DLMC-nA|√ tt|83.74|12.74|19.64|24.27|37.27|24.94|37.34|16.73|
|DLMC-nA|√ t+1|82.26|14.18|21.41|25.51|39.10|26.18|39.29|16.55|
|DLMC|tt|85.28|12.05|17.65|24.03|36.34|24.51|36.27|16.45|
|DLMC|t+1|84.55|12.62|18.47|24.27|37.28|24.94|37.14|16.69|
------------
Header_1: Does Progress on ImageNet Transfer to Real-World Datasets?
filename: 2301.04644.pdf

Does Progress on ImageNet Transfer to Real-World Datasets?
------------
Header_1: Does Progress on ImageNet Transfer to Real-World Datasets?
filename: 2301.04644.pdf

Does Progress on ImageNet Transfer to Real-World Datasets?

Alex Fang - University of Washington - apf1@cs.washington.edu

Simon Kornblith - Google Research, Brain Team - skornblith@google.com

Ludwig Schmidt - University of Washington, Allen Institute for AI - schmidt@cs.washington.edu

arXiv:2301.04644v1 [cs.CV] 11 Jan 2023
------------
Header_1: Does Progress on ImageNet Transfer to Real-World Datasets?
Header_2: Abstract
filename: 2301.04644.pdf

Abstract

Does progress on ImageNet transfer to real-world datasets? We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets. In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements. For certain tasks, interventions such as data augmentation improve performance even when architectures do not. We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.
------------
Header_1: Does Progress on ImageNet Transfer to Real-World Datasets?
Header_2: 1 Introduction
filename: 2301.04644.pdf

1 Introduction

ImageNet is one of the most widely used datasets in machine learning. Initially, the ImageNet competition played a key role in re-popularizing neural networks with the success of AlexNet in 2012. Ten years later, the ImageNet dataset is still one of the main benchmarks for state-of-the-art computer vision models (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; He et al., 2016; Liu et al., 2018; Howard et al., 2019; Touvron et al., 2021; Radford et al., 2021). As a result of ImageNet’s prominence, the machine learning community has invested tremendous effort into developing model architectures, training algorithms, and other methodological innovations with the goal of increasing performance on ImageNet. Comparing methods on a common task has important benefits because it ensures controlled experimental conditions and results in rigorous evaluations. But the singular focus on ImageNet also raises the question whether the community is over-optimizing for this specific dataset.

As a first approximation, ImageNet has clearly encouraged effective methodological innovation beyond ImageNet itself. For instance, the key finding from the early years of ImageNet was that large convolution neural networks (CNNs) can succeed on contemporary computer vision datasets by leveraging GPUs for training. This paradigm has led to large improvements in other computer vision tasks, and CNNs are now omnipresent in the field. Nevertheless, this clear example of transfer to other tasks early in the ImageNet evolution does not necessarily justify the continued focus ImageNet still receives. For instance, it is possible that early methodological innovations transferred more broadly to other tasks, but later innovations have become less generalizable. The goal of our paper is to investigate this possibility specifically for neural network architecture and their transfer to real-world data not commonly found on the Internet.

When discussing the transfer of techniques developed for ImageNet to other datasets, a key question is what other datasets to consider. Currently there is no comprehensive characterization of the many machine learning datasets and transfer between them. Hence we restrict our attention to a limited but well-motivated family of datasets. In particular, we consider classification tasks derived from image data that were specifically collected with the goal of classification in mind. This is in

∗Equal contribution
------------
Header_1: Research Paper Summary
filename: 2301.04644.pdf

Research Paper Summary
------------
Header_1: Research Paper Summary
Header_2: Contrast in Transferability of ImageNet Architectures
filename: 2301.04644.pdf

Contrast in Transferability of ImageNet Architectures

In contrast to many standard computer vision datasets – including ImageNet – where the constituent images were originally collected for a different purpose, posted to the web, and later re-purposed for benchmarking computer vision methods. Concretely, the study focuses on six datasets ranging from leaf disease classification over melanoma detection to categorizing animals in camera trap images. Since these datasets represent real-world applications, transfer of methods from ImageNet is particularly relevant.

We find that on four out of our six real-world datasets, ImageNet-motivated architecture improvements after VGG resulted in little to no progress. Specifically, when fitting a line to downstream model accuracies as a function of ImageNet accuracy, the resulting slope is less than 0.05. The exceptions where post-VGG architectures yield larger gains are the Caltech Camera Traps-20 (CCT-20) dataset (slope 0.11) and the Human Protein Atlas Image Classification dataset (slope 0.29). On multiple other datasets, task-specific improvements such as data augmentations or extra training data lead to larger gains than using a more recent ImageNet architecture.

We evaluate on a representative testbed of 19 ImageNet models, ranging from the seminal AlexNet over VGG and ResNets to the more recent and higher-performing EfficientNets and ConvNexts. Our testbed includes three Vision Transformer models to cover non-CNN architectures.
------------
Header_1: Research Paper Summary
Header_2: Contrast in Transferability of ImageNet Architectures
Header_3: Related Work
filename: 2301.04644.pdf

Related Work

Transferability of ImageNet architectures has been extensively studied. Kornblith et al. (2019) showed a strong correlation between ImageNet accuracy and downstream accuracy on web-scraped object-centric computer vision benchmark tasks. Later studies have explored the relationship between ImageNet and transfer accuracy for various network types.

Most closely related to this work, Tuggener et al. (2021) investigated the performance of CNN architectures on datasets, finding that accuracy correlates poorly with ImageNet accuracy when training from scratch, but correlations are higher when fine-tuning ImageNet-pretrained models.

Other work has evaluated the transferability of networks trained on datasets beyond ImageNet. Abnar et al. (2022) explored the relationship between upstream and downstream accuracy for models pretrained on JFT and ImageNet-21K.
------------
Header_1: Document
filename: 2301.04644.pdf

Document
------------
Header_1: Document
Header_2: Caltech Camera Traps 20
filename: 2301.04644.pdf

Caltech Camera Traps 20
------------
Header_1: Document
Header_2: Caltech Camera Traps 20
Header_3: APTOS 2019 Blindness
filename: 2301.04644.pdf

APTOS 2019 Blindness
------------
Header_1: Document
Header_2: Caltech Camera Traps 20
Header_3: APTOS 2019 Blindness
Header_4: Human Protein Atlas
filename: 2301.04644.pdf

Human Protein Atlas

| |Quadratic weighted kappa|
|---|---|
|78|0.930|
|76|0.925|0.70|
|74|0.920|0.65|
|Accuracy|0.915|0.60|
|72|0.910|0.55|
|70|0.905|0.50|
|68|0.900|0.45|
|66|0.895|0.40|

| |ImageNet top-1 accuracy|
|---|---|
|0.97|SIIM-ISIC Melanoma|89|Cassava Leaf Disease|99.4|EuroSAT|
|0.96| |88| |99.2| |
|Area under ROC| |87| |99.0| |
| |Accuracy|86| |98.8| |
| | |85| |98.6| |
| | |84| |98.2| |
| | |83| |98.0| |
------------
Header_1: Document
Header_2: Caltech Camera Traps 20
Header_3: ImageNet top-1 accuracy
filename: 2301.04644.pdf

ImageNet top-1 accuracy

- AlexNet
- ResNet-152
- EfficientNet B0
- ConvNext-tiny
- MobileNetV3-small
- DeiT-small
- EfficientNet B4
- ShuffleNetV2x0.5
- VGG-13 BN
- PNASNet-5
- DenseNet-121
- SqueezeNet 1.1
- DeiT-tiny
- Inception-ResNet v2
- ResNeXt-50-32x4d
- ViT-B/16
- ResNet-50
- VGG-16 BN
- ShuffleNetV2x1.0

Figure 1: Overview of transfer performance across models from ImageNet to each of the datasets we study.
Although there seems to be a strong linear trends between ImageNet accuracy and the target metrics (green),
these trends become less certain when we restrict the models to those above 70% ImageNet accuracy (blue).
Versions with error bars and spline interpolation can be found in Appendix B.

Stream accuracy. However, they evaluate representational quality using linear transfer rather than
end-to-end fine-tuning. Other studies have investigated the impact of relationships between pre-
training and fine-tuning tasks (Zamir et al., 2018; Mensink et al., 2021) or the impact of scaling the
model and dataset (Goyal et al., 2019; Kolesnikov et al., 2020).

Another direction of related work relates to the effect of pretraining data on transfer learning. Huh
et al. (2016) look into the factors that make ImageNet good for transfer learning. They find that
fine-grained classes are not needed for good transfer performance, and that reducing the dataset size
and number of classes only results in slight drops in transfer learning performance. Though there is
a common goal of exploring what makes transfer learning work well, our work differs from this line
of work by focusing on the fine-tuning aspect of transfer learning.

Other studies of external validity of benchmarks. Our study fits into a broader literature inves-
tigating the external validity of image classification benchmarks. Early work in this area identified
lack of diversity as a key shortcoming of the benchmarks of the time (Ponce et al., 2006; Torralba
& Efros, 2011), a problem that was largely resolved with the introduction of the much more di-
verse ImageNet benchmark (Deng et al., 2009; Russakovsky et al., 2015). More recent studies have
investigated the extent to which ImageNet classification accuracy correlates with accuracy on out-
of-distribution (OOD) data (Recht et al., 2019; Taori et al., 2020) or accuracy as measured using
higher-quality human labels (Shankar et al., 2020; Tsipras et al., 2020; Beyer et al., 2020).

As in previous studies of OOD generalization, transfer learning involves generalization to test sets
that differ in distribution from the (pre-)training data. However, there are also key differences be-
tween transfer learning and OOD generalization. First, in transfer learning, additional training data
from the target task is used to adapt the model, while OOD evaluations usually apply trained models
to a new distribution without any adaptation. Second, OOD evaluations usually focus on settings
with a shared class space so that evaluations without adaptation are possible. In contrast, transfer
learning evaluation generally involves downstream tasks with classes different from those in the pre-
training dataset. These differences between transfer learning and OOD generalization are not only
conceptual but also lead to different empirical phenomena. Miller et al. (2021) has shown that in-

distribution accuracy improvements often directly yield out-of-distribution accuracy improvements
as well. This is the opposite of our main experimental finding that ImageNet improvements do not
directly yield performance improvements on many real-world downstream tasks. Hence our work
demonstrates an important difference between OOD generalization and transfer learning.
------------
Header_1: Document
Header_2: Caltech Camera Traps 20
Header_3: DATASETS
filename: 2301.04644.pdf

DATASETS

As mentioned in the introduction, a key choice in any transfer study is the set of target tasks on which
to evaluate model performance. Before we introduce our suite of target tasks, we first describe three
criteria that guided our dataset selection: (i) diverse data sources, (ii) relevance to an application,
and (iii) availability of well-tuned baseline models for comparison.
------------
Header_1: Document
Header_2: Caltech Camera Traps 20
Header_3: DATASETS
Header_4: SELECTION CRITERIA
filename: 2301.04644.pdf

SELECTION CRITERIA

Prior work has already investigated transfer of ImageNet architectures to many downstream
datasets (Donahue et al., 2014; Sharif Razavian et al., 2014; Chatfield et al., 2014; Simonyan &
Zisserman, 2015). The 12 datasets used by Kornblith et al. (2019) often serve as a standard evaluation suite (e.g., in (Salman et al., 2020; Ericsson et al., 2021; Radford et al., 2021)). While these
datasets are an informative starting point, they are all object-centric natural image datasets, and do
not represent the entire range of image classification problems. There are many applications of computer vision; the Kaggle website alone lists more than 1,500 datasets as of May 2022. To understand
transfer from ImageNet more broadly, we selected six datasets guided by the following criteria.

Diverse data sources. Since collecting data is an expensive process, machine learning researchers
often rely on web scraping to gather data when assembling a new benchmark. This practice has led to
several image classification datasets with different label spaces such as food dishes, bird species, car
models, or other everyday objects. However, the data sources underlying these seemingly different
tasks are actually often similar. Specifically, we surveyed the 12 datasets from Kornblith et al.
(2019) and found that all of these datasets were harvested from the web, often via keyword searches
in Flickr, Google image search, or other search engines (see Appendix K). This narrow range of
data sources limits the external validity of existing transfer learning experiments. To get a broader
understanding of transfer from ImageNet, we focus on scientific, commercial, and medical image
classification datasets that were not originally scraped from the web.

Application relevance. In addition to the data source, the classification task posed on a given set of
images also affects how relevant the resulting problem is for real-world applications. For instance,
it would be possible to start with real-world satellite imagery that shows multiple building types
per image, but only label one of the building types for the purpose of benchmarking (e.g., to avoid
high annotation costs). The resulting task may then be of limited value for an actual application
involving the satellite images that requires all buildings to be annotated. We aim to avoid such
pitfalls by limiting our attention to classification tasks that were assembled by domain experts with
a specific application in mind.

Availability of baselines. If methodological progress does not transfer from ImageNet to a given
target task, we should expect that, as models perform better on ImageNet, accuracy on the target
task saturates. However, observing such a trend in an experiment is not sufficient to reach a conclusion
regarding transfer because there is an alternative explanation for this empirical phenomenon.
Besides a lack of transfer, the target task could also simply be easier than the source task so that
models with sub-optimal source task accuracy already approach the Bayes error rate. As an illustrative
example, consider MNIST as a target task for ImageNet transfer. A model with mediocre
ImageNet accuracy is already sufficient to get 99% accuracy on MNIST, but this finding does not
mean that better ImageNet models are insufficient to improve MNIST accuracy — the models have
already hit the MNIST performance ceiling.

More interesting failures of transfer occur when ImageNet architectures plateau on the target task,
but it is still possible to improve accuracy beyond what the best ImageNet architecture can achieve
without target task-specific modifications. In order to make such comparisons, well-tuned baselines for the target task are essential. If improving ImageNet accuracy alone is insufficient to reach
these well-tuned baselines, we can indeed conclude that architecture transfer to this target task is
limited. In our experiments, we use multiple datasets from Kaggle competitions since the resulting
leaderboards offer well-tuned baselines arising from a competitive process.
------------
Header_1: Datasets Studied
filename: 2301.04644.pdf

Datasets Studied
------------
Header_1: Datasets Studied
Header_2: Datasets Studied
filename: 2301.04644.pdf

Datasets Studied

|Dataset|# of classes|Train size|Eval size|Eval metric|Kaggle|
|---|---|---|---|---|---|
|Caltech Camera Traps|15|14,071|15,215|Accuracy| |
|APTOS 2019 Blindness|5|2,930|732|Quadratic weighted kappa| |
|Human Protein Atlas|28|22,582|5,664|Macro F1 score| |
|SIIM-ISIC Melanoma|2|46,372|11,592|Area under ROC| |
|Cassava Leaf Disease|5|17,118|4,279|Accuracy| |
|EuroSAT|10|21,600|5,400|Accuracy| |

Caltech Camera Traps, APTOS 2019 Blindness, Human Protein Atlas, SIIM-ISIC Melanoma, Cassava Leaf Disease, EuroSAT

Caltech Camera Traps-20, APTOS 2019 Blindness Detection, Human Protein Atlas Image Classification, SIIM-ISIC Melanoma Classification, Cassava Leaf Disease Classification, EuroSAT Classification
------------
Header_1: Datasets Studied
Header_2: Main Experiments
filename: 2301.04644.pdf

Main Experiments

We run our experiments across 19 model architectures, including both CNNs and Vision Transformers (ViT and DeiT). They range from 57% to 83% ImageNet top-1 accuracy, allowing us to observe the relationship between ImageNet performance and target dataset performance. In order to get the best performance out of each architecture, we do extensive hyperparameter tuning over learning rate, weight decay, optimizer, and learning schedule. Details about our experiment setup can be found in Appendix C. We now present our results for each of the datasets we investigated. Figure 1 summarizes our results across all datasets, with additional statistics in Table 2. Appendix A contains complete results for all datasets across the hyperparameter grids.
------------
Header_1: Datasets Studied
Header_2: Main Experiments
Header_3: Caltech Camera Traps
filename: 2301.04644.pdf

Caltech Camera Traps

Beery et al. (2018) created Caltech Camera Traps-20 (CCT-20) using images taken from camera traps deployed to monitor animal populations. The images contain 15 different animal classes, as well as an empty class that we remove for our experiments. The dataset contains two sets of validation and test sets which differ by whether they come from locations that are the same as or different from the training set locations. While one of the goals of the dataset is to study generalization to new environments, here we only study the sets from the same locations. Although CCT-20 is not a Kaggle competition, it is a subset of the iWildCam Challenge 2018, whose yearly editions have been hosted on Kaggle. We see in Figure 1 (top-left) an overall positive trend between ImageNet performance and CCT-20 performance. The overall trend is unsurprising, given the number of animal classes present in ImageNet. But despite the drastic reduction in the number of classes when compared to ImageNet.
------------
Header_1: Datasets Studied
Header_2: Appendix
filename: 2301.04644.pdf

Appendix

1. Dataset download links and PyTorch datasets and splits can be found at https://github.com/mlfoundations/imagenet-applications-transfer.

2. Empty class is removed for the classification experiments in Table 1 of Beery et al. (2018)
------------
Header_1: Document
filename: 2301.04644.pdf

Document
------------
Header_1: Document
Header_2: CCT-20
filename: 2301.04644.pdf

CCT-20

CCT-20 has its own set of challenges. Animals are often pictured at difficult angles, and sometimes are not even visible in the image because a sequence of frames triggered by activity all have the same label. Despite these challenges, an even higher performing model still does better on this task - we train a CLIP ViT L/14-336px model (85.4% ImageNet top-1) with additional augmentation to achieve 83.4% accuracy on CCT-20.
------------
Header_1: Document
Header_2: APTOS 2019 BLINDNESS DETECTION
filename: 2301.04644.pdf

APTOS 2019 BLINDNESS DETECTION

This dataset was created for a Kaggle competition run by the Asia Pacific Tele-Ophthalmology Society (APTOS) with the goal of advancing medical screening for diabetic retinopathy in rural areas (Asia Pacific Tele-Ophthalmology Society, 2019). Images are taken using fundus photography and vary in terms of clinic source, camera used, and time taken. Images are labeled by clinicians on a scale of 0 to 4 for the severity of diabetic retinopathy. Given the scaled nature of the labels, the competition uses quadratic weighted kappa (QWK) as the evaluation metric. We create a local 80% to 20% random class-balanced train/validation split, as the competition test labels are hidden. We find that models after VGG do not show significant improvement. Similar to in CCT-20, DeiT and EfficientNets performs slightly worse, while deeper models from the same architecture slightly help performance. We also find that accuracy has a similar trend as QWK, despite it being an inferior metric in the context of this dataset.
------------
Header_1: Document
Header_2: APTOS 2019 BLINDNESS DETECTION
Header_3: Performance Comparison
filename: 2301.04644.pdf

Performance Comparison

When performance stagnates, one might ask whether we have reached a performance limit for our class of models on the dataset. To answer this question, we compare with the Kaggle leaderboard’s top submissions. The top Kaggle submission achieves 0.936 QWK on the private leaderboard (85% of the test set) (Xu, 2019). They do this by using additional augmentation, using external data, training on L1-loss, replacing the final pooling layer with generalized mean pooling, and ensembling a variety of models trained with different input sizes. The external data consists of 88,702 images from the 2015 Diabetic Retinopathy Detection Kaggle competition.
------------
Header_1: Document
Header_2: APTOS 2019 BLINDNESS DETECTION
Header_3: Improving Accuracy
filename: 2301.04644.pdf

Improving Accuracy

Even though performance saturates with architecture, we find that additional data augmentation and other interventions still improve accuracy. We submitted our ResNet-50 and ResNet-152 models with additional interventions, along with an Inception-ResNet v2 (Szegedy et al., 2017b) model with hyperparameter tuning. We find that increasing color and affine augmentation by itself can account for a 0.03 QWK point improvement. Once we train on 512 input size, additional augmentation, and additional data, our ResNet-50 and Inception-ResNet v2 both achieve 0.896 QWK on the private leaderboard, while ResNet-152 achieves 0.890 QWK, once again suggesting that better ImageNet architectures by themselves do not lead to increased performance on this task.
------------
Header_1: Document
Header_2: APTOS 2019 BLINDNESS DETECTION
Header_3: Model Ensemble
filename: 2301.04644.pdf

Model Ensemble

As a comparison, the ensemble from the top leaderboard entry included a single model Inception-ResNet v2 trained with additional interventions that achieves 0.927 QWK. We submitted the original models we trained to Kaggle as well, finding that the new models trained with additional interventions do at least 0.03 QWK points better. See Appendix F for additional experimental details. Both this result and the gap between our models and the top leaderboard models show that there exist interventions that do improve task performance.
------------
Header_1: Document
Header_2: HUMAN PROTEIN ATLAS IMAGE CLASSIFICATION
filename: 2301.04644.pdf

HUMAN PROTEIN ATLAS IMAGE CLASSIFICATION

The Human Protein Atlas runs the Human Protein Atlas Image Classification competition on Kaggle to build an automated tool for identifying and locating proteins from high-throughput microscopy images (Ouyang et al., 2019). Images can contain multiple of the 28 different proteins, so the competition uses the macro F1 score. Given the multi-label nature of the problem, this requires thresholding for prediction. We use a 73% / 18% / 9% train / validation / test-validation split created by a previous competitor (Park, 2019). We report results on the validation split, as we find that the thresholds selected for the larger validation split generalize well to the smaller test-validation split.
------------
Header_1: Document
Header_2: HUMAN PROTEIN ATLAS IMAGE CLASSIFICATION
Header_3: Challenges and Improvements
filename: 2301.04644.pdf

Challenges and Improvements

Specific challenges for this dataset are extreme class imbalance, multi-label thresholding, and generalization from the training data to the test set. Competitors were able to improve performance beyond the baselines we found by using external data as well as techniques such as data cleaning.
------------
Header_1: Document
filename: 2301.04644.pdf

Document
------------
Header_1: Document
Header_2: SIIM-ISIC MELANOMA CLASSIFICATION
filename: 2301.04644.pdf

SIIM-ISIC MELANOMA CLASSIFICATION

The Society for Imaging Informatics in Medicine (SIIM) and the International Skin Imaging Collaboration (ISIC) jointly ran this Kaggle competition for identifying Melanoma (SIIM & ISIC, 2020), a serious type of skin cancer. Competitors use images of skin lesions to predict the probability that each observed image is malignant. Images come from the ISIC Archive, which is publicly available and contains images from a variety of countries. The competition provided 33,126 training images, plus an additional 25,331 images from previous competitions. We split the combined data into an 80% to 20% class-balanced and year-balanced train/validation split. Given the imbalanced nature of the data (8.8% positive), the competition uses area under ROC curve as the evaluation metric.

We find only a weak positive correlation (0.44) between ImageNet performance and task performance, with a regression line with a normalized slope of close to zero (0.05). But if we instead look at classification accuracy, Appendix H shows that there is a stronger trend for transfer than that of area under ROC curve, as model task accuracy more closely follows the same order as ImageNet performance. This difference shows that characterizing the relationship between better ImageNet models and better transfer performance is reliant on the evaluation metric as well. We use a relatively simple setup to measure the impact of ImageNet models on task performance, but we know we can achieve better results with additional strategies. The top two Kaggle solutions used models with different input size, ensembling, cross-validation and a significant variety of training augmentation to create a stable model that generalized to the hidden test set (Ha et al., 2020; Pan, 2020).
------------
Header_1: Document
Header_2: CASSAVA LEAF DISEASE CLASSIFICATION
filename: 2301.04644.pdf

CASSAVA LEAF DISEASE CLASSIFICATION

The Makerere Artificial Intelligence Lab is an academic research group focused on applications that benefit the developing world. Their goal in creating the Cassava Leaf Disease Classification Kaggle competition (Makerere University AI Lab, 2021) was to give farmers access to methods for diagnosing plant diseases, which could allow farmers to prevent these diseases from spreading, increasing crop yield. Images were taken with an inexpensive camera and labeled by agricultural experts. Each image was classified as healthy or as one of four different diseases. We report results using a 80%/20% random class-balanced train/validation split of the provided training data.

Once we ignore models below 70% ImageNet accuracy, the relationship between the performance on the two datasets has both a weak positive correlation (0.12) and a near-zero normalized slope (0.02). While these are natural images similar to portions of ImageNet, it is notable that ImageNet contains very few plant classes (e.g., buckeye, hip, rapeseed). Yet based on a dataset’s perceived similarity to ImageNet, it is surprising that leaf disease classification is not positively correlated with ImageNet, while the microscopy image based Human Protein Atlas competition is. Our results are supported by Kaggle competitors: the first place solution found that on the private leaderboard, EfficientNet B4 (Tan & Le, 2019), MobileNet, and ViT (Dosovitskiy et al., 2021b) achieve 89.5%, 89.4%, and 88.8% respectively (Hanke, 2021). Their ensemble achieves 91.3% on the private leaderboard.
------------
Header_1: Document
Header_2: EUROSAT
filename: 2301.04644.pdf

EUROSAT

Helber et al. (2019) created EuroSAT from Sentinel-2 satellite images to classify land use and land cover. Past work has improved performance on the dataset through additional training time techniques (Naushad et al., 2021) and using 13 spectral bands (Yassine et al., 2021). We use RGB images and keep our experimental setup consistent to compare across a range of models. Since there is no set train/test split, we create a 80%/20% class-balanced split.

All models over 60% ImageNet accuracy achieve over 98.5% EuroSAT accuracy, and the majority of our models achieve over 99.0% EuroSAT accuracy. There are certain tasks where using better ImageNet models does not improve performance, and this would be the extreme case where performance saturation is close to being achieved. While it is outside the scope of this study, a next step would be to investigate the remaining errors and find other methods to reduce this last bit of error.
------------
Header_1: Additional Studies
filename: 2301.04644.pdf

Additional Studies
------------
Header_1: Additional Studies
Header_2: 5 Additional Studies
filename: 2301.04644.pdf

5 Additional Studies
------------
Header_1: Additional Studies
Header_2: 5 Additional Studies
Header_3: 5.1 Augmentation Ablations
filename: 2301.04644.pdf

5.1 Augmentation Ablations

In our main experiments, we keep augmentation simple to minimize confounding factors when comparing models. However, it is possible pre-training and fine-tuning with different combinations of augmentations may have different results. This is an important point because different architectures may have different inductive biases and often use different augmentation strategies at pre-training time. To investigate these effects, we run additional experiments on CCT-20 and APTOS to explore the effect of data augmentation on transfer. Specifically, we take ResNet-50 models pre-trained with standard crop and flip augmentation, AugMix (Hendrycks et al., 2020), and RandAugment (Cubuk et al., 2020), and then fine-tune on our default augmentation, AugMix, and RandAugment. We also study DeiT-tiny and Deit-small models by fine-tuning on the same three augmentations mentioned above. We choose to examine DeiT models because they are pre-trained using RandAugment and RandErasing (Zhong et al., 2020). We increase the number of epochs we fine-tune on from 30 to 50 to account for augmentation. Our experimental results are found in Appendix G.

In our ResNet-50 experiments, both AugMix and RandAugment improve performance on ImageNet, but while pre-training with RandAugment improves performance on downstream tasks, pre-training with AugMix does not. Furthermore, fine-tuning with RandAugment usually yields additional performance gains when compared to our default fine-tuning augmentation, no matter which pre-trained model is used. For DeiT models, we found that additional augmentation did not significantly increase performance on the downstream tasks. Thus, as with architectures, augmentation strategies that improve accuracy on ImageNet do not always improve accuracy on real-world tasks.
------------
Header_1: Additional Studies
Header_2: 5 Additional Studies
Header_3: 5.2 Clip Models
filename: 2301.04644.pdf

5.2 Clip Models

A natural follow-up to our experiments is to change the source of pre-training data. We examine CLIP models from Radford et al. (2021), which use diverse pre-training data and achieve high performance on a variety of downstream datasets. We fine-tune CLIP models on each of our downstream datasets by linear probing then fine-tuning (LP-FT) (Kumar et al., 2022). Our results are visualized by the purple stars in Appendix I Figure 8. We see that by using a model that takes larger images we can do better than all previous models, and even without the larger images, ViT-L/14 does better on four out of the six datasets. While across all CLIP models the change in pre-training data increases performance for CCT-20, the effect on the other datasets is more complicated. When controlling for architecture changes by only looking at ResNet-50 and ViT/B16, we see that the additional pre-training data helps for CCT-20, HPA, and Cassava, the former two corresponding to the datasets that empirically benefit most from using better ImageNet models. Additional results can be found in Appendix I, while additional fine-tuning details can be found in Appendix J.
------------
Header_1: Additional Studies
Header_2: 6 Discussion
filename: 2301.04644.pdf

6 Discussion

Alternative explanations for saturation. Whereas Kornblith et al. (2019) reported a high degree of correlation between ImageNet and transfer accuracy, we find that better ImageNet models do not consistently transfer better on our real-world tasks. We believe these differences are related to the tasks themselves. Here, we rule out alternative hypotheses for our findings.

Comparison of datasets statistics suggests that the number of classes and dataset size also do not explain the differences from Kornblith et al. (2019). The datasets we study range from two to 28 classes. Although most of the datasets studied in Kornblith et al. (2019) have more classes, CIFAR-10 has 10. In Appendix E, we replicate CIFAR-10 results from Kornblith et al. (2019) using our experimental setup, finding a strong correlation between ImageNet accuracy and transfer accuracy. Thus, the number of classes is likely not the determining factor. Training set sizes are similar between our study and that of Kornblith et al. (2019) and thus also do not seem to play a major role.

A third hypothesis is that it is parameter count, rather than ImageNet accuracy, that drives trends. We see that VGG BN models appear to outperform their ImageNet accuracy on multiple datasets, and they are among the largest models by parameter count. However, in Appendix L, we find that model size is also not a good indicator of improved transfer performance on real world datasets.

Differences between web-scraped datasets and real-world images We conjecture that it is possible to perform well on most, if not all, web-scraped target datasets simply by collecting a very large amount of data from the Internet and training a very large model on it. Web-scraped target datasets are by definition within the distribution of data collected from the web, and a sufficiently large model can learn that distribution. In support of this conjecture, recent models such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021), ViT-G (Zhai et al., 2022), BASIC (Pham et al., 2021), and CoCa (Yu et al., 2022) are trained on very large web-scraped datasets and achieve high accuracy on a variety of web-scraped benchmarks. However, this strategy may not be effective for non-web-scraped datasets, where there is no guarantee that we will train on data that is close in distribution to the target data, even if we train on the entire web. Thus, it makes sense to distinguish these two types of datasets. There are clear differences in image distribution between the non-web-scraped datasets we consider and web-scraped datasets considered by previous work. In Figure 3 and Appendix M, we compute Fréchet inception distance (FID) (Heusel et al., 2017) between ImageNet and each of the datasets we study in this work as well as the ones found in Kornblith et al. (2019). The real-world datasets are further away from ImageNet than those found in Kornblith et al. (2019), implying that there is a large amount of distribution shift between web-scraped datasets and real-world datasets. However, FID is only a proxy measure and may not capture all factors that lead to differences in transferability.

Whereas web-scraped data is cheap to acquire, real-world data can be more expensive. Ideally, progress in computer vision architectures should improve performance not just on web-scraped data, but also on real-world tasks. Our results suggest that the latter has not happened. Gains in ImageNet accuracy over the last decade have primarily come from improving and scaling architectures, and past work has shown that these gains generally transfer to other web-scraped datasets, regardless of size (Sun et al., 2017; Kornblith et al., 2019; Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020). However, we find that improvements arising from architecture generally do not transfer to non-web-scraped tasks. Nonetheless, data augmentation and other tweaks can provide further gains on these tasks.

Recommendations towards better benchmarking. While it is unclear whether researchers have over-optimized for ImageNet, our work suggests that researchers should explicitly search for methods that improve accuracy on real-world non-web-scraped datasets, rather than assuming that methods that improve accuracy on ImageNet will provide meaningful improvements on real-world datasets as well. Just as there are methods that improve accuracy on ImageNet but not on the tasks we investigate, there may be methods that improve accuracy on our tasks but not ImageNet. The Kaggle community provides some evidence for the existence of such methods; Kaggle submissions often explore architectural improvements that are less common in traditional ImageNet pre-trained models. To measure such improvements on real-world problems, we suggest simply using the average accuracy across our tasks as a benchmark for future representation learning research.

Further analysis of our results shows consistencies in the accuracies of different models across the non-web-scraped datasets, suggesting that accuracy improvements on these datasets may translate to other datasets. For each dataset, we use linear regression to predict model accuracies on the target dataset as a linear combination of ImageNet accuracy and accuracy averaged across the other real-world datasets. We perform an F-test to determine whether the average accuracy on other real-world datasets explains significant variance beyond that explained by ImageNet accuracy. We find that this F-test is significant on all datasets except EuroSAT, where accuracy may be very close to ceiling (see further analysis in Appendix N.1). Additionally, in Appendix N.2 we compare the Spearman rank correlation (i.e., the Pearson correlation between ranks) between each dataset and the accuracy averaged across the other real-world datasets to the Spearman correlation between each dataset and ImageNet. We find that the correlation with the average over real-world datasets is higher than the correlation with ImageNet and statistically significant for CCT-20, APTOS, HPA, and Cassava. Thus, there is some signal in the average accuracy across the datasets that we investigate that is not captured by ImageNet top-1 accuracy.
------------
Header_1: Research Findings on ImageNet
filename: 2301.04644.pdf

Research Findings on ImageNet
------------
Header_1: Research Findings on ImageNet
Header_2: Where do our findings leave ImageNet?
filename: 2301.04644.pdf

Where do our findings leave ImageNet?

We suspect that most of the methodological innovations that help on ImageNet are useful for some real-world tasks, and in that sense it has been a successful benchmark. However, the innovations that improve performance on industrial web-scraped datasets such as JFT (Sun et al., 2017) or IG-3.5B-17k (Mahajan et al., 2018) (e.g., model scaling) may be almost entirely disjoint from the innovations that help with the non-web-scraped real-world tasks studied here (e.g., data augmentation strategies). We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.
------------
Header_1: Research Findings on ImageNet
Header_2: Where do our findings leave ImageNet?
Header_3: ACKNOWLEDGEMENTS
filename: 2301.04644.pdf

ACKNOWLEDGEMENTS

We would like to thank Samuel Ainsworth, Sara Beery, Gabriel Ilharco, Pieter-Jan Kindermans, Sarah Pratt, Matthew Wallingford, Ross Wightman, and Mitchell Wortsman for valuable conversations while working on this project. We would especially like to thank Sarah Pratt for help with early experimentation and brainstorming.

We would also like to thank Hyak computing cluster at the University of Washington and the Google TPU Research Cloud program for access to compute resources that allowed us to run our experiments.

This work is in part supported by the NSF AI Institute for Foundations of Machine Learning (IFML), Open Philanthropy, Google, and the Allen Institute for AI.
------------
Header_1: Research Findings on ImageNet
Header_2: Where do our findings leave ImageNet?
Header_3: REFERENCES
filename: 2301.04644.pdf

REFERENCES

- Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the limits of large scale pre-training. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=V3C8p78sDa.
- Asia Pacific Tele-Ophthalmology Society. Aptos 2019 blindness detection, 2019. URL https://www.kaggle.com/competitions/aptos2019-blindness-detection/overview.
- Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XVI, volume 11220 of Lecture Notes in Computer Science, pp. 472–489. Springer, 2018. doi: 10.1007/978-3-030-01270-0\ 28. URL https://doi.org/10.1007/978-3-030-01270-0_28.
- Lucas Beyer, Olivier J H´ enaff, Alexander Kolesnikov, Xiaohua Zhai, and A¨ aron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.
- Keno K Bressem, Lisa C Adams, Christoph Erxleben, Bernd Hamm, Stefan M Niehues, and Janis L Vahldiek. Comparing different deep learning architectures for classification of chest radiographs. Scientific reports, 10(1):1–16, 2020.
- Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil in the details: Delving deep into convolutional nets. arXiv preprint arXiv:1405.3531, 2014.
- Ekin Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc Le. Randaugment: Practical automated data augmentation with a reduced search space. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/d85b63ef0ccb114d0a3bb7b7d808028f-Abstract.html.
- Shubin Dai. A cnn classifier and a metric learning model, 1st place solution, 2019. URL https://www.kaggle.com/competitions/human-protein-atlas-image-classification/discussion/78109.
- Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.
------------
Header_1: References
filename: 2301.04644.pdf

References
------------
Header_1: References
filename: 2301.04644.pdf

References

- Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. *Decaf: A deep convolutional activation feature for generic visual recognition.* In International conference on machine learning, pp. 647–655. PMLR, 2014.
- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-reit, and Neil Houlsby. *An image is worth 16x16 words: Transformers for image recognition at scale.* In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL https://openreview.net/forum?id=YicbFdNTTy.
- Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-reit, and Neil Houlsby. *An image is worth 16x16 words: Transformers for image recognition at scale.* In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview.net/forum?id=YicbFdNTTy.
- Linus Ericsson, Henry Gouk, and Timothy M Hospedales. *How well do self-supervised models transfer?* In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5414–5423, 2021.
- Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and Andrew Zisserman. *The pascal visual object classes (VOC) challenge.* Int. J. Comput. Vis., 88(2): 303–338, 2010. doi: 10.1007/s11263-009-0275-4. URL https://doi.org/10.1007/s11263-009-0275-4.
- Li Fei-Fei, Rob Fergus, and Pietro Perona. *Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.* In IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2004, Washington, DC, USA, June 27 - July 2, 2004, pp. 178. IEEE Computer Society, 2004. doi: 10.1109/CVPR.2004.383. URL https://doi.org/10.1109/CVPR.2004.383.
- Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra. *Scaling and benchmarking self-supervised visual representation learning.* In Proceedings of the ieee/cvf International Conference on computer vision, pp. 6391–6400, 2019.
- Qishen Ha, Bo Liu, and Fuxu Liu. *Identifying melanoma images using efficientnet ensemble: Winning solution to the SIIM-ISIC melanoma classification challenge.* CoRR, abs/2010.05351, 2020. URL https://arxiv.org/abs/2010.05351.
- Jannis Hanke. *1st place solution, 2021.* URL https://www.kaggle.com/competitions/cassava-leaf-disease-classification/discussion/221957.
- Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. *Deep residual learning for image recognition.* In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 770–778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.
- Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. *Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.* IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217–2226, 2019. doi: 10.1109/JSTARS.2019.2918242. URL https://doi.org/10.1109/JSTARS.2019.2918242.
- Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. *Augmix: A simple data processing method to improve robustness and uncertainty.* In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=S1gmrxHFvB.
- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. *Gans trained by a two time-scale update rule converge to a local nash equilibrium.* In Isabelle.

- Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6626–6637, 2017. URL Link.
- Andrew Howard, Ruoming Pang, Hartwig Adam, Quoc V. Le, Mark Sandler, Bo Chen, Weijun Wang, Liang-Chieh Chen, Mingxing Tan, Grace Chu, Vijay Vasudevan, and Yukun Zhu. Searching for mobilenetv3. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 1314–1324. IEEE, 2019. doi: 10.1109/ICCV.2019.00140. URL Link.
- Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 2261–2269. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.243. URL Link.
- Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer learning? CoRR, abs/1608.08614, 2016. URL Link.
- Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. CoRR, abs/1602.07360, 2016. URL Link.
- Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021.
- Alexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y Ng, and Pranav Rajpurkar. Chextransfer: performance and parameter efficiency of imagenet models for chest x-ray interpretation. In Proceedings of the Conference on Health, Inference, and Learning, pp. 116–124, 2021.
- Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In European conference on computer vision, pp. 491–507. Springer, 2020.
- Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2661–2671, 2019.
- Simon Kornblith, Ting Chen, Honglak Lee, and Mohammad Norouzi. Why do better loss functions lead to less transferable features? Advances in Neural Information Processing Systems, 34, 2021.
- Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi. Contrasting contrastive self-supervised representation learning pipelines. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9949–9959, 2021.
- Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.
- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In Peter L. Bartlett, Fernando C. N. Pereira, Christopher J. C. Burges, L´eon Bottou, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1106–1114, 2012. URL Link.
- Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-tuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. URL Link.
------------
Header_1: References
filename: 2301.04644.pdf

References
------------
Header_1: References
filename: 2301.04644.pdf

References

Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan L.
Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In Vittorio
Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV
2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part
I, volume 11205 of Lecture Notes in Computer Science, pp. 19–35. Springer, 2018. doi: 10.1007/
978-3-030-01246-5\ 2. URL https://doi.org/10.1007/978-3-030-01246-5_2.

Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
A convnet for the 2020s. CoRR, abs/2201.03545, 2022. URL https://arxiv.org/abs/2201.03545.

Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet V2: practical guidelines
for efficient CNN architecture design. In Vittorio Ferrari, Martial Hebert, Cristian Sminchis-
escu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 - 15th European Conference, Mu-
nich, Germany, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes
in Computer Science, pp. 122–138. Springer, 2018. doi: 10.1007/978-3-030-01264-9\ 8. URL
https://doi.org/10.1007/978-3-030-01264-9_8.

Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,
Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised
pretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181–
196, 2018.

Makerere University AI Lab. Cassava leaf disease classification, 2021. URL https://www.kaggle.com/competitions/cassava-leaf-disease-classification/overview.

Thomas Mensink, Jasper Uijlings, Alina Kuznetsova, Michael Gygli, and Vittorio Ferrari. Factors
of influence for transfer learning across diverse appearance domains and task types. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2021. doi: 10.1109/TPAMI.2021.
3129870.

John Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar,
Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In Marina Meila and Tong
Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 7721–7735. PMLR, 2021. URL http://proceedings.mlr.press/v139/miller21b.html.

Raoof Naushad, Tarunpreet Kaur, and Ebrahim Ghaderpour. Deep transfer learning for land use
and land cover classification: A comparative study. Sensors, 21(23):8083, 2021. doi: 10.3390/
s21238083. URL https://doi.org/10.3390/s21238083.

Niv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse imagenet models
transfer better. arXiv preprint arXiv:2204.09134, 2022.

Wei Ouyang, Casper F. Winsnes, Martin Hjelmare, Anthony J. Cesnik, Lovisa ˚ Akesson, Hao Xu,
Devin P. Sullivan, Shubin Dai, Jun Lan, Park Jinmo, Shaikat M. Galib, Christof Henkel, Kevin
Hwang, Dmytro Poplavskiy, Bojan Tunguz, Russel D. Wolfinger, Yinzheng Gu, Chuanpeng Li,
Jinbin Xie, Dmitry Buslov, Sergei Fironov, Alexander Kiselev, Dmytro Panchenko, Xuan Cao,
Runmin Wei, Yuanhao Wu, Xun Zhu, Kuan-Lun Tseng, Zhifeng Gao, Cheng Ju, Xiaohan Yi,
Hongdong Zheng, Constantin Kappel, and Emma Lundberg. Analysis of the human protein atlas image classification competition. Nature Methods, 16(12):1254–1261, 2019. doi: 10.1038/
s41592-019-0658-6. URL https://doi.org/10.1038/s41592-019-0658-6.

Ian Pan. [2nd place] solution overview, 2020. URL https://www.kaggle.com/competitions/siim-isic-melanoma-classification/discussion/175324.

Jinmo Park. 3rd place solution with code., 2019. URL https://www.kaggle.com/c/human-protein-atlas-image-classification/discussion/77320.
------------
Header_1: References
filename: 2301.04644.pdf

References
------------
Header_1: References
Header_2: References
filename: 2301.04644.pdf

References

- Pham, H., Dai, Z., Ghiasi, G., Liu, H., Yu, A. W., Luong, M.-T., Tan, M., & Le, Q. V. (2021). Combined scaling for zero-shot transfer learning. *arXiv preprint*, arXiv:2111.10050.
- Ponce, J., Berg, T. L., Everingham, M., Forsyth, D. A., Hebert, M., Lazebnik, S., Marszalek, M., Schmid, C., Russell, B. C., Torralba, A., et al. (2006). Dataset issues in object recognition. In *Toward category-level object recognition*, pp. 29–48. Springer.
- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In *International Conference on Machine Learning*, pp. 8748–8763. PMLR.
- Raghu, M., Zhang, C., Kleinberg, J., & Bengio, S. (2019). Transfusion: Understanding transfer learning for medical imaging. *Advances in neural information processing systems*, 32.
- Recht, B., Roelofs, R., Schmidt, L., & Shankar, V. (2019). Do imagenet classifiers generalize to imagenet? In *International Conference on Machine Learning*, pp. 5389–5400. PMLR.
- Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. (2015). Imagenet large scale visual recognition challenge. *International journal of computer vision*, 115(3), 211–252.
- Salman, H., Ilyas, A., Engstrom, L., Kapoor, A., & Madry, A. (2020). Do adversarially robust imagenet models transfer better? *Advances in Neural Information Processing Systems*, 33, 3533–3545.
- Shankar, V., Roelofs, R., Mania, H., Fang, A., Recht, B., & Schmidt, L. (2020). Evaluating machine accuracy on imagenet. In *International Conference on Machine Learning*, pp. 8634–8644. PMLR.
- Razavian, A. S., Azizpour, H., Sullivan, J., & Carlsson, S. (2014). Cnn features off-the-shelf: an astounding baseline for recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition workshops*, pp. 806–813.
- Shugaev, M. (2019). Pretrained resnet34 with rgby (0.460 public lb). URL: https://www.kaggle.com/code/iafoss/pretrained-resnet34-with-rgby-0-460-public-lb/notebook.
- SIIM and ISIC. (2020). Siim-isic melanoma classification. URL: https://www.kaggle.com/competitions/siim-isic-melanoma-classification/overview.
- Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. In Bengio, Y., & LeCun, Y. (Eds.), *3rd International Conference on Learning Representations, ICLR 2015*, Conference Track Proceedings. URL: http://arxiv.org/abs/1409.1556.
- Steiner, A., Kolesnikov, A., Zhai, X., Wightman, R., Uszkoreit, J., & Beyer, L. (2021). How to train your vit? data, augmentation, and regularization in vision transformers. *CoRR*, abs/2106.10270. URL: https://arxiv.org/abs/2106.10270.
- Sun, C., Shrivastava, A., Singh, S., & Gupta, A. (2017). Revisiting unreasonable effectiveness of data in deep learning era. In *Proceedings of the IEEE international conference on computer vision*, pp. 843–852.
- Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, inception-resnet and the impact of residual connections on learning. In Singh, S., & Markovitch, S. (Eds.), *Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence*, pp. 4278–4284. AAAI Press. URL: http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.
------------
Header_1: References
filename: 2301.04644.pdf

References
------------
Header_1: References
filename: 2301.04644.pdf

References

Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Satinder Singh and Shaul Markovitch (eds.), Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pp. 4278–4284. AAAI Press, 2017b. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14806.

Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 6105–6114. PMLR, 2019. URL http://proceedings.mlr.press/v97/tan19a.html.

Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583–18599, 2020.

Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011.

Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv´ e J´ egou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 10347–10357. PMLR, 2021. URL http://proceedings.mlr.press/v139/touvron21a.html.

Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. From imagenet to image classification: Contextualizing progress on benchmarks. In International Conference on Machine Learning, pp. 9625–9635. PMLR, 2020.

Lukas Tuggener, J¨ urgen Schmidhuber, and Thilo Stadelmann. Is it enough to optimize cnn architectures on imagenet? arXiv preprint arXiv:2103.09108, 2021.

Ross Wightman, Hugo Touvron, and Herv´ e J´egou. Resnet strikes back: An improved training procedure in timm. CoRR, abs/2110.00476, 2021. URL https://arxiv.org/abs/2110.00476.

Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10687–10698, 2020.

Saining Xie, Ross B. Girshick, Piotr Doll´ ar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 5987–5995. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.634. URL https://doi.org/10.1109/CVPR.2017.634.

Guanshuo Xu. 1st place solution summary, 2019. URL https://www.kaggle.com/competitions/aptos2019-blindness-detection/discussion/108065.

H. Yassine, K. Tout, and M. Jaber. Improving Lulc Classification from Satellite Imagery Using Deep Learning - Eurosat Dataset. ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 43B3:369–376, June 2021. doi: 10.5194/isprs-archives-XLIII-B3-2021-369-2021.

Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.

Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3712–3722, 2018.

Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation benchmark. 2019.

Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12104–12113, 2022.

Kevin Zheng. 39th solution-attention gated resnet18 (single model without cv), 2019. URL https://www.kaggle.com/competitions/human-protein-atlas-image-classification/discussion/77637.

Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 13001–13008. AAAI Press, 2020. URL https://ojs.aaai.org/index.php/AAAI/article/view/7000.
------------
Header_1: Experiment Results
filename: 2301.04644.pdf

Experiment Results
------------
Header_1: Experiment Results
Header_2: Appendix
filename: 2301.04644.pdf

Appendix
------------
Header_1: Experiment Results
Header_2: Appendix
Header_3: DETAILED EXPERIMENT RESULTS
filename: 2301.04644.pdf

DETAILED EXPERIMENT RESULTS

|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|
|---|---|---|---|---|---|---|---|
|AlexNet|56.5|63.59|0.8835|0.3846|0.9283|82.58|97.93|
|SqueezeNet 1.1|58.2|66.36|0.9021|0.3972|0.9073|85.15|98.07|
|ShuffleNetV2x0.5|60.6|66.37|0.9227|0.5867|0.9289|85.64|98.56|
|MobileNet V3 small|67.7|66.01|0.9230|0.6108|0.9455|85.81|99.15|
|ShuffleNetV2x1.0|69.4|69.27|0.9202|0.6202|0.9418|87.33|98.91|
|VGG-13 BN|71.6|75.06|0.9268|0.6794|0.9529|88.99|98.85|
|DeiT-tiny|72.2|68.77|0.9130|0.5777|0.9510|86.25|99.11|
|VGG-16 BN|73.4|75.93|0.9287|0.6791|0.9531|88.45|98.93|
|DenseNet-121|74.4|74.66|0.9287|0.7019|0.9514|87.80|99.06|
|ResNet-50|76.1|73.96|0.9215|0.6718|0.9524|87.75|99.19|
|ResNeXt-50-32x4d|77.6|73.73|0.9212|0.6906|0.9588|88.15|99.24|
|EfficientNet B0|77.7|71.02|0.9195|0.6942|0.9456|87.63|98.80|
|ResNet-152|78.3|74.05|0.9228|0.6732|0.9562|87.75|99.15|
|ViT-B/16|78.7|72.07|0.9262|0.5852|0.9600|86.63|99.28|
|DeiT-small|79.9|71.41|0.9205|0.6148|0.9583|87.19|99.20|
|Inception-ResNet v2|80.4|70.68|0.9168|0.6882|0.9483|87.84|98.93|
|ConvNext-tiny|82.5|78.51|0.9297|0.6992|0.9628|88.89|99.11|
|PNASNet-5 large|82.9|75.21|0.9271|0.6941|0.9584|87.77|99.17|
|EfficientNet B4|83.4|73.49|0.9211|0.6954|0.9552|88.36|98.70|

See the following link for experiment results across hyperparameters: Experiment Results.
------------
Header_1: Main Figure Variations
filename: 2301.04644.pdf

Main Figure Variations
------------
Header_1: Main Figure Variations
Header_2: Main Figure Variations
filename: 2301.04644.pdf

Main Figure Variations

| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|
|---|---|---|---|
|Quadratic weighted kappa|0.75|0.94|0.70|
| | | |Macro F1 score|
|Accuracy|0.92|0.60|0.65|

ImageNet top-1 accuracy

| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|
|---|---|---|---|
|SIIM-ISIC Melanoma|0.97|90|99.50|
|Area under ROC|88| |99.00|
|Accuracy|86| |98.75|

ImageNet top-1 accuracy

| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|
|---|---|---|---|
|AlexNet| | |ConvNext-tiny|
|MobileNetV3-small| | |ShuffleNetV2x0.5|
|VGG-13 BN| | |SqueezeNet 1.1|
|DeiT-tiny| | |ViT-B/16|
|ResNet-50| | | |

Figure 4: Figure 1 with error bars. Green is linear trend of all models, while blue is linear trend for models above 70% ImageNet accuracy. We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.

| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|
|---|---|---|---|
|Quadratic weighted kappa|0.7|0.94|0.6|
| | | |Macro F1 score|
|Accuracy|0.92|0.4|0.5|

ImageNet top-1 accuracy

| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|
|---|---|---|---|
|SIIM-ISIC Melanoma|0.97|90|99.50|
|Area under ROC|88| |99.00|
|Accuracy|86| |98.75|

ImageNet top-1 accuracy

| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|
|---|---|---|---|
|AlexNet| | |ConvNext-tiny|
|MobileNetV3-small| | |ShuffleNetV2x0.5|
|VGG-13 BN| | |SqueezeNet 1.1|
|DeiT-tiny| | |ViT-B/16|
|ResNet-50| | | |

Figure 5: Figure 4 with spline interpolation fits instead of linear fits.
------------
Header_1: Experiment Setup
filename: 2301.04644.pdf

Experiment Setup
------------
Header_1: Experiment Setup
Header_2: MODELS
filename: 2301.04644.pdf

MODELS

|Model|ImageNet top-1|# params|Year Released|
|---|---|---|---|
|AlexNet (Krizhevsky et al., 2012)|56.5|61M|2012|
|SqueezeNet 1.1 (Iandola et al., 2016)|58.2|1.2M|2016|
|ShuffleNetV2x0.5 (Ma et al., 2018)|60.6|1.4M|2018|
|MobileNet V3 small (Howard et al., 2019)|67.7|2.5M|2019|
|ShuffleNetV2x1.0 (Ma et al., 2018)|69.4|2.3M|2018|
|VGG-13 BN (Simonyan & Zisserman, 2015)|71.6|133M|2014/2015|
|DeiT-tiny (Touvron et al., 2021)|72.2|5.7M|2020|
|VGG-16 BN (Simonyan & Zisserman, 2015)|73.4|138M|2014/2015|
|DenseNet-121 (Huang et al., 2017)|74.4|8.0M|2016|
|ResNet-50 (He et al., 2016)|76.1|26M|2015|
|ResNeXt-50-32x4d (Xie et al., 2017)|77.6|25M|2016|
|EfficientNet B0 (Tan & Le, 2019)|77.7|5.3M|2019|
|ResNet-152 (He et al., 2016)|78.3|60M|2015|
|ViT-B/16 (Dosovitskiy et al., 2021a; Steiner et al., 2021)|78.7|304M|2020|
|DeiT-small (Touvron et al., 2021)|79.9|22M|2020|
|Inception-ResNet v2 (Szegedy et al., 2017a)|80.4|56M|2016|
|ConvNext-tiny (Liu et al., 2022)|82.5|29M|2022|
|PNASNet-5 large (Liu et al., 2018)|82.9|86M|2017|
|EfficientNet B4 (Tan & Le, 2019)|83.4|19M|2019|

We examine 19 model architectures in this work that cover a diverse range of accuracies on ImageNet
in order to observe the relationship between ImageNet performance and target dataset performance.
In addition to the commonly used CNNs, we also include data-efficient image transformers (DeiT)
due to the recent increase in usage of Vision Transformers. Additional model details are in Table 4.
------------
Header_1: Experiment Setup
Header_2: HYPERPARAMETER GRID
filename: 2301.04644.pdf

HYPERPARAMETER GRID

Hyperparameter tuning is a key part of neural network training, as using suboptimal hyperparameters
can lead to suboptimal performance. Furthermore, the correct hyperparameters vary across both
models and training data. To get the best performance out of each model, we train each model
on AdamW with a cosine decay learning rate schedule, SGD with a cosine decay learning rate
schedule, and SGD with a multi-step decay learning rate schedule. We also grid search for optimal
initial learning rate and weight decay combinations, searching logarithmically between $$10^{-1}$$ to
$$10^{-4}$$ for SGD learning rate, $$10^{-2}$$ to $$10^{-5}$$ for AdamW learning rate, and $$10^{-3}$$ to $$10^{-6}$$ as well as
0 for weight decay. All models are pretrained on ImageNet and then fine-tuned on the downstream
task. Additional training details for each dataset can be found in Appendix D. We also run our
hyperparameter grid on CIFAR-10 in Appendix E to verify that we find a strong relationship between
ImageNet and CIFAR-10 accuracy as previously reported by Kornblith et al. (2019).
------------
Header_1: Experiment Setup
Header_2: TRAINING DETAILS BY DATASET (IMAGENET MODELS)
filename: 2301.04644.pdf

TRAINING DETAILS BY DATASET (IMAGENET MODELS)

Experiments on Cassava Leaf Disease, SIIM-ISIC Melanoma, and EuroSAT datasets were ran on
TPU v2-8s, while all other datasets were ran on NVIDIA A40s.

All experiments were ran with mini-batch size of 128.

For SGD experiments, we use Nesterov momentum, set momentum to 0.9, and try learning rates of
$$1e-1$$, $$1e-2$$, $$1e-3$$, and $$1e-4$$. For AdamW experiments, we try learning rates of $$1e-2$$, $$1e-3$$, $$1e-4$$, $$1e-5$$.

For all experiments, we try weight decays of $$1e-3$$, $$1e-4$$, $$1e-5$$, $$1e-6$$, and 0.

For all experiments, we use weights that are pretrained on ImageNet. AlexNet, DenseNet, MobileNet, ResNet, ResNext, ShuffleNet, SqueezeNet and VGG models are from torchvision, while
ConvNext, DeiT, EfficientNet, InceptionResNet, and PNASNet models are from timm. Additionally, we normalize images to ImageNet’s mean and standard deviation.

For EuroSAT we random resize crop to 224 with area at least 0.65.

For all other datasets, we random resize crop with area at least 0.65 to 224 for DeiT models, and 256
for all other models. Additionally, we use horizontal flips. For Human Protein Atlas, Cassava Leaf
Disease, and SIIM-ISIC Melanoma, we also use vertical flips.

For SIIM-ISIC Melanoma, we train for 10 epochs, and for the step scheduler decay with factor 0.1 at 5 epochs.

For all other datasets, we train for 30 epochs, and for the step scheduler decay with factor 0.1 at 15, 20, and 25 epochs.

CIFAR-10 ON HYPERPARAMETER GRID

55
60
65
70
75
80
85

AlexNet

ResNet-50

Inception-ResNet v2

DenseNet-121

$$
\begin{array}{|c|c|c|c|c|c|c|c|}
\hline
& 55 & 60 & 65 & 70 & 75 & 80 & 85 \\
\hline
\text{AlexNet} & & & & & & & \\
\hline
\text{ResNet-50} & & & & & & & \\
\hline
\text{Inception-ResNet v2} & & & & & & & \\
\hline
\text{DenseNet-121} & & & & & & & \\
\hline
\end{array}
$$
------------
Header_1: OCR Text
filename: 2301.04644.pdf

OCR Text
------------
Header_1: OCR Text
Header_2: Table 6: Comparing the effect of augmentation on Kaggle leaderboard scores
filename: 2301.04644.pdf

Table 6: Comparing the effect of augmentation on Kaggle leaderboard scores

More augmentation is as described earlier in this section. Less augmentation only uses random resize crop with at least 0.65 area and horizontal flips.

|lr \ wd|1.00E-04|1.00E-05|1.00E-06|
|---|---|---|---|
|ResNet-50|1.00E-03|0.8669 / 0.6405|0.8520 / 0.6013|0.8613 / 0.6269|
|less aug|1.00E-04|0.8525 / 0.6115|0.8570 / 0.6431|0.8483 / 0.6147|
| |1.00E-05|0.8186 / 0.5071|0.8287 / 0.5647|0.8288 / 0.5328|
|ResNet-50|1.00E-03|0.8440 / 0.6432|0.8547 / 0.6856|0.8524 / 0.7125|
|more aug|1.00E-04|0.8948 / 0.7490|0.8972 / 0.7693|0.8999 / 0.7758|
| |1.00E-05|0.8724 / 0.7370|0.8685 / 0.7567|0.8623 / 0.7376|
------------
Header_1: OCR Text
Header_2: AUGMENTATION ABLATION DETAILS
filename: 2301.04644.pdf

AUGMENTATION ABLATION DETAILS
------------
Header_1: OCR Text
Header_2: AUGMENTATION ABLATION DETAILS
Header_3: Table 7: Examining the effect of pre-training augmentation and fine-tuning augmentation on downstream transfer performance
filename: 2301.04644.pdf

Table 7: Examining the effect of pre-training augmentation and fine-tuning augmentation on downstream transfer performance

The model specifies the architecture and pre-training augmentation, while each column specifies the downstream task and fine-tuning augmentation. We find that augmentation strategies that improve ImageNet accuracy do not always improve accuracy on downstream tasks. Pre-trained augmentation models are from Wightman et al. (2021).

|Model|ImageNet Acc|CCT-20 Base Aug|CCT-20 AugMix|CCT-20 RandAug|APTOS Base Aug|APTOS AugMix|APTOS RandAug|
|---|---|---|---|---|---|---|---|
|ResNet-50|76.1|72.02|72.24|73.57|0.9210|0.9212|0.9250|
|ResNet-50 w/ AugMix|77.5|71.63|71.53|72.39|0.9239|0.9152|0.9222|
|ResNet-50 w/ RandAug|78.8|72.94|73.54|73.76|0.9190|0.9204|0.9302|
|Deit-tiny|72.2|66.57|66.47|66.95|0.9153|0.9197|0.9172|
|Deit-small|79.9|70.65|69.72|70.07|0.9293|0.9212|0.9277|
------------
Header_1: OCR Text
Header_2: MELANOMA METRIC COMPARISON
filename: 2301.04644.pdf

MELANOMA METRIC COMPARISON

SIIM-ISIC Melanoma ROC: 0.97, SIIM-ISIC Melanoma Acc: 96.5

Area under ROC:

0.96: 96.0

0.95: 95.5

0.94: 95.0

0.93: 94.5

0.92: 94.0

0.91: 93.5

0.90: 93.0

ImageNet top-1 accuracy: 55 60 65 70 75 80 85

AlexNet, MobileNetV3-small, VGG-13 BN, DeiT-tiny, ResNet-50

ResNet-152, PNASNet-5, Inception-ResNet v2, VGG-16 BN

EfficientNet B0, DenseNet-121, ResNeXt-50-32x4d, ShuffleNetV2x1.0

EfficientNet B4, ShuffleNetV2x0.5, SqueezeNet 1.1, ConvNext-tiny
------------
Header_1: OCR Text
Header_2: Figure 7: Comparing transfer performance from ImageNet to Melanoma using different metrics
filename: 2301.04644.pdf

Figure 7: Comparing transfer performance from ImageNet to Melanoma using different metrics

Green linear trend is computed across all models, while blue linear trend is restricted to models above 70% ImageNet accuracy. Using accuracy implies that better ImageNet models transfer better; however, ROC is a better metric for this task.
------------
Header_1: CLIP Experiment Details
filename: 2301.04644.pdf

CLIP Experiment Details
------------
Header_1: CLIP Experiment Details
Header_2: CLIP Experiment Details
filename: 2301.04644.pdf

CLIP Experiment Details

| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|
|---|---|---|---|
|Quadratic weighted kappa|0.75|0.94|0.70|
|Macro F1 score| |0.65| |
|Accuracy|0.92|0.60| |

| |ImageNet top-1 accuracy|ImageNet top-1 accuracy|ImageNet top-1 accuracy|
|---|---|---|---|
|SIIM-ISIC Melanoma|0.98|90|Cassava Leaf Disease|
| |0.97| |EuroSAT|
|Area under ROC|0.96|88|99.0|
|Accuracy|86|98.5| |

|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|
|---|---|---|---|---|---|---|---|
|CLIP-RN50|73.3|74.45|0.9135|0.7053|0.9350|87.89|98.80|
|CLIP-RN101|75.7|75.19|0.9235|0.6909|0.9378|87.68|99.11|
|CLIP-B32|76.1|70.57|0.9137|0.5338|0.9546|86.28|99.26|
|CLIP-B16|80.2|77.81|0.9213|0.6365|0.9619|87.82|99.24|
|CLIP-L14|83.9|79.99|0.9330|0.6687|0.9717|88.82|99.33|
|CLIP-L14@336|85.4|83.17|0.9337|0.7131|0.9738|89.24|99.48|

|Model|ImageNet top-1|CCT20|APTOS|HPA|Melanoma|Cassava|EuroSAT|
|---|---|---|---|---|---|---|---|
|IN-ResNet-50|76.1|73.96|0.9215|0.6718|0.9524|87.75|99.19|
|CLIP-RN50|73.3|74.45|0.9135|0.7053|0.9350|87.89|98.80|
|IN-ViT-B/16|78.7|72.07|0.9262|0.5852|0.9600|86.63|99.28|
|CLIP-B16|80.2|77.81|0.9213|0.6365|0.9619|87.82|99.24|
------------
Header_1: CLIP Experiment Details
Header_2: CLIP Experiment Details
Header_3: CLIP Fine-Tuning Details
filename: 2301.04644.pdf

CLIP Fine-Tuning Details

We fine-tune by running a linear probe, followed by end-to-end fine-tuning on the best model from the first part. We keep total epochs consistent with the previous models, with a third of the epochs going toward linear probing. We use AdamW with a cosine decay schedule. During the linear probe, we search over 10-1, 10-2, and 10-3 learning rates, and during fine-tuning, we search over 10-4, 10-5, and 10-6 learning rates. For both parts, we search over 10-3 to 10-6 and 0 for weight decay.

22
------------
Header_1: Creation Information for Datasets Studied in Kornblith et al. (2019)
filename: 2301.04644.pdf

Creation Information for Datasets Studied in Kornblith et al. (2019)
------------
Header_1: Creation Information for Datasets Studied in Kornblith et al. (2019)
Header_2: Creation Information for Datasets Studied in Kornblith et al. (2019)
filename: 2301.04644.pdf

Creation Information for Datasets Studied in Kornblith et al. (2019)

Table 10: We find that the 12 datasets studied in Kornblith et al. (2019) come from web scraping.

|Dataset|Origin|Additional information|
|---|---|---|
|Food-101|foodspotting.com|Users upload an image of their food and annotate the type of food; categories chosen by popularity|
|CIFAR-10|TinyImages|Web crawl|
|CIFAR-100|TinyImages|Web crawl|
|Birdsnap|Flickr|Also used MTurk|
|SUN397|Web search engines|Also used WordNet|
|Stanford Cars|Flickr, Google, Bing|Also used MTurk|
|FGVC Aircraft|airliners.net|Images taken by 10 photographers|
|Pascal VOC 2007 Cls.|Flickr|N/A|
|Describable Textures|Google and Flickr|Also used MTurk|
|Oxford-IIT Pets|Flickr, Google, Catster, Dogster|Catster and Dogster are social websites for collecting and discussing pet images|
|Caltech-101|Google|97 categories chosen from Webster Collegiate Dictionary categories associated with a drawing|
|Oxford 102 Flowers|Mostly collected from web|A small number of images acquired by the paper authors taking the pictures|
------------
Header_1: Creation Information for Datasets Studied in Kornblith et al. (2019)
Header_2: Relationship Between Model Size and Transfer Performance
filename: 2301.04644.pdf

Relationship Between Model Size and Transfer Performance

Caltech Camera Traps 20

| |Caltech Camera Traps 20|APTOS 2019 Blindness|Human Protein Atlas|
|---|---|---|---|
|80.0|Quadratic weighted kappa| |0.8|
|77.5|0.94| |0.7|
|75.0| | |Macro F1 score|
|Accuracy|0.92| | |
|72.5| | |0.6|
|70.0|0.90| | |
|67.5| | |0.5|
|65.0|0.88| |0.4|
|62.5|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|
|# of parameters (in millions)|SIIM-ISIC Melanoma|90|Cassava Leaf Disease|99.50|EuroSAT|
|0.97| | | |99.25|
|0.96| | |Area under ROC|88| |99.00|
|0.95|Accuracy| | |98.75|
|0.94| | |86| |98.50|
|0.93| | | |98.25|
|0.92| | |84| |98.00|
|0.91| | |82| |97.75|
|0.90| | | |97.50|
|0.89|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|0|20|40|60|80|100|120|140|
|# of parameters (in millions)|AlexNet|ResNet-152|EfficientNet B0|ShuffleNetV2x1.0|
| |MobileNetV3-small|DeiT-small|EfficientNet B4|ConvNext-tiny|
| |VGG-13 BN|PNASNet-5|DenseNet-121|ShuffleNetV2x0.5|
| |DeiT-tiny|Inception-ResNet v2|ResNeXt-50-32x4d|SqueezeNet 1.1|
| |ResNet-50|VGG-16 BN| | |

Figure 9: We compare model size with downstream transfer performance. Again we use separate trend lines for all models (green) and only those above 70% ImageNet accuracy (blue). We use 95% confidence intervals computed with Clopper-Pearson for accuracy metrics and bootstrap with 10,000 trials for other metrics.
------------
Header_1: FID Scores and Predictive Power of Accuracy
filename: 2301.04644.pdf

FID Scores and Predictive Power of Accuracy
------------
Header_1: FID Scores and Predictive Power of Accuracy
Header_2: Table 11: FID Scores
filename: 2301.04644.pdf

Table 11: FID Scores

| Dataset              | FID   |
|----------------------|-------|
| CCT-20               | 162.69|
| APTOS                | 196.24|
| HPA                  | 230.70|
| Cassava              | 179.24|
| Melanoma             | 186.34|
| EuroSAT              | 151.85|
| Food-101             | 108.35|
| CIFAR-10             | 132.53|
| CIFAR-100            | 120.72|
| Birdsnap             | 94.08 |
| SUN397               | 62.95 |
| Stanford Cars        | 143.35|
| FGVC Aircraft        | 183.35|
| Pascal VOC 2007 Cls. | 39.84 |
| Describable Textures  | 89.13 |
| Oxford-IIT Pets      | 77.27 |
| Caltech-101          | 50.77 |
| Oxford 102 Flowers   | 140.21|
------------
Header_1: FID Scores and Predictive Power of Accuracy
Header_2: Predictive Power of Accuracy on Non-Web-Scraped Datasets on Novel Datasets
filename: 2301.04644.pdf

Predictive Power of Accuracy on Non-Web-Scraped Datasets on Novel Datasets

We observe that, on many non-web-scraped datasets, accuracy correlates only weakly with ImageNet accuracy. It is thus worth asking whether other predictors might correlate better. In this section, we examine the extent to which accuracy on a given non-web-scraped target dataset can be predicted from the accuracy on the other non-web-scraped target datasets.
------------
Header_1: FID Scores and Predictive Power of Accuracy
Header_2: Predictive Power of Accuracy on Non-Web-Scraped Datasets on Novel Datasets
Header_3: N.1 F-TEST
filename: 2301.04644.pdf

N.1 F-TEST

We can further measure the extent to which the averages of the five other datasets beyond the predictive power provided by ImageNet by using F-tests. For each target task, we fit a linear regression model that predicts accuracy as either ImageNet accuracy or the average accuracy on the other five non-web-scraped datasets, and a second linear regression model that predicts accuracy as a function of both ImageNet accuracy and the average accuracy on the other five datasets. Since the first model is nested within the second, the second model must explain at least as much variance as the first. The F-test measures whether the increase in explained variance is significant. For these experiments, we logit-transform accuracy values and standardize them to zero mean and unit variance before computing the averages, as in the middle column of Table 13.

Results are shown in Table 12. The average accuracy across the other five datasets explains variance beyond that explained by ImageNet accuracy alone on five of the six datasets. The only exception is EuroSAT, where the range of accuracies is low (most models get approximately 99%) and a significant fraction of the variance among models may correspond to noise. By contrast, ImageNet accuracy explains variance beyond the average accuracy only on two datasets (APTOS and Melanoma). These results indicate that there are patterns in how well different models transfer to non-web-scraped data that are not captured by ImageNet accuracy alone, but are captured by the accuracy on other non-web-scraped datasets.

|Dataset|+Avg. across datasets| |+ImageNet|Adj. R2|
|---|---|---|---|---|
| |F (1, 16)|p-value|F (1, 16)|p-value|(ImageNet-only)|(Average-only)|(Both predictors)|
|CCT-20|8.2|0.01|0.69|0.42|0.56|0.70|0.69|
|APTOS|31.0|0.00004|4.6|0.047|0.34|0.71|0.76|
|HPA|11.8|0.003|0.84|0.37|0.60|0.76|0.76|
|Melanoma|5.8|0.03|7.8|0.01|0.74|0.71|0.79|
|Cassava|13.2|0.002|0.14|0.71|0.55|0.75|0.74|
|EuroSAT|2.9|0.11|0.72|0.41|0.43|0.52|0.49|

N.2 SPEARMAN CORRELATION

|Dataset|Avg of 5 others (unnormalized)|Avg of 5 others (normalized)|ImageNet|
|---|---|---|---|
| |ρ|p-value|ρ|p-value|ρ|p-value|
|CCT-20|0.8684|0.0000|0.9263|0.0000|0.5825|0.0089|
|APTOS|0.7205|0.0005|0.6950|0.0010|0.3010|0.2105|
|HPA|0.7351|0.0003|0.6825|0.0013|0.6491|0.0026|
|Melanoma|0.6561|0.0023|0.7807|0.0000|0.7667|0.0001|
|Cassava|0.8872|0.0000|0.7442|0.0003|0.5222|0.0218|
|EuroSAT|0.3030|0.2073|0.3821|0.1065|0.4734|0.0406|
------------
Header_1: Pre-training Augmentation Details
filename: 2301.04644.pdf

Pre-training Augmentation Details
------------
Header_1: Pre-training Augmentation Details
Header_2: Pre-training Augmentation Details
filename: 2301.04644.pdf

Pre-training Augmentation Details
------------
Header_1: Pre-training Augmentation Details
Header_2: Pre-training Augmentation Details
Header_3: Table 14: Pre-training Augmentation Strategy
filename: 2301.04644.pdf

Table 14: Pre-training Augmentation Strategy

|Model|Augmentation|
|---|---|
|AlexNet|Resize + Crop + Flip|
|SqueezeNet 1.1|Resize + Crop + Flip|
|ShuffleNetV2x0.5|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|
|MobileNet V3 small|AutoAugment (ImageNet/Default) + RandErasing|
|ShuffleNetV2x1.0|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|
|VGG-13 BN|Resize + Crop + Flip|
|DeiT-tiny|RandAugment + RandErasing|
|VGG-16 BN|Resize + Crop + Flip|
|DenseNet-121|Resize + Crop + Flip|
|ResNet-50|Resize + Crop + Flip|
|ResNeXt-50-32x4d|Resize + Crop + Flip|
|EfficientNet B0|RandAugment|
|ResNet-152|Resize + Crop + Flip|
|ViT-B/16|RandAugment + MixUp|
|DeiT-small|RandAugment + RandErasing|
|Inception-ResNet v2|Inception Preprocessing (Color Distort + Resize + Crop + Flip)|
|ConvNext-tiny|AutoAugment (TrivialAugmentWide) + RandErasing + MixUp + CutMix|
|PNASNet-5 large|Whiten + Resize + Crop + Flip|
|EfficientNet B4|RandAugment|
------------
Header_1: Pre-training Augmentation Details
Header_2: Pre-training Augmentation Details
Header_3: Caltech Camera Traps 20, APTOS 2019 Blindness, Human Protein Atlas
filename: 2301.04644.pdf

Caltech Camera Traps 20, APTOS 2019 Blindness, Human Protein Atlas

| |Quadratic weighted kappa|Macro F1 score|Accuracy|
|---|---|---|---|
|78|0.75|0.70|0.92|
|76|0.94| |0.60|
|74| |0.65| |
|72|0.90| |0.55|
|70| |0.50| |
|68|0.88| |0.45|
|66| |0.40| |
| | |0.35| |
------------
Header_1: Pre-training Augmentation Details
Header_2: Pre-training Augmentation Details
Header_3: ImageNet Top-1 Accuracy
filename: 2301.04644.pdf

ImageNet Top-1 Accuracy

| |ImageNet top-1 accuracy|ImageNet top-1 accuracy|ImageNet top-1 accuracy|
|---|---|---|---|
| |SIIM-ISIC Melanoma|Cassava Leaf Disease|EuroSAT|
|0.97|90|99.50| |
|0.96| |99.25| |
|Area under ROC|88|99.00| |
|0.95| | | |
|Accuracy|86|98.75| |
|0.93| |98.50| |
|0.92|84|98.25| |
| | |98.00| |
|0.90|82|97.75| |
|0.89| |97.50| |
------------
Header_1: Pre-training Augmentation Details
Header_2: Pre-training Augmentation Details
Header_3: Models with General Pre-training Augmentation Strategy
filename: 2301.04644.pdf

Models with General Pre-training Augmentation Strategy

- AlexNet
- ResNet-152
- EfficientNet B0
- ConvNext-tiny
- MobileNetV3-small
- DeiT-small
- EfficientNet B4
- ShuffleNetV2x0.5
- VGG-13 BN
- PNASNet-5
- DenseNet-121
- SqueezeNet 1.1
- DeiT-tiny
- Inception-ResNet v2
- ResNeXt-50-32x4d
- ViT-B/16
- ResNet-50
- VGG-16 BN
- ShuffleNetV2x1.0
------------
Header_1: Pre-training Augmentation Details
Header_2: Pre-training Augmentation Details
Header_3: Figure 10: Pre-training Augmentation Strategy
filename: 2301.04644.pdf

Figure 10: Pre-training Augmentation Strategy

Figure 1 with points colored by general pre-training augmentation strategy. Cyan points use simple augmentation (resize, crops, flips, etc.), and red points use automatic augmentation (RandAugment, AutoAugment, TrivialAugmentWide).

26
------------
Header_1: Provably Efficient Offline Goal-Conditioned Reinforcement Learning
filename: 2302.03770.pdf

Provably Efficient Offline Goal-Conditioned Reinforcement Learning
------------
Header_1: Provably Efficient Offline Goal-Conditioned Reinforcement Learning
Header_2: Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability
filename: 2302.03770.pdf

Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability

arXiv:2302.03770v2 [cs.LG] 11 Oct 2023

Hanlin Zhu - EECS, UC Berkeley; Meta AI - hanlinzhu@berkeley.edu

Amy Zhang - ECE, UT Austin; Meta AI - amy.zhang@austin.utexas.edu
------------
Header_1: Provably Efficient Offline Goal-Conditioned Reinforcement Learning
Header_2: Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability
Header_3: Abstract
filename: 2302.03770.pdf

Abstract

Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $$\tilde{O}(\text{poly}(1/\epsilon))$$ sample complexity (where $$\epsilon$$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the objective functions. We only require nearly minimal assumptions on the dataset (single-policy concentrability) and the function class (realizability). Moreover, this algorithm consists of two uninterleaved optimization steps, which we refer to as V-learning and policy learning, and is computationally stable since it does not involve minimax optimization. We also empirically validate our theory by showing that the modified algorithm outperforms the previous algorithm in various real-world environments. To the best of our knowledge, this is the first algorithm that is both provably efficient with general function approximation and single-policy concentrability, and empirically successful without requiring solving minimax optimization problems.
------------
Header_1: Provably Efficient Offline Goal-Conditioned Reinforcement Learning
Header_2: Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability
Header_3: 1 Introduction
filename: 2302.03770.pdf

1 Introduction

Goal-conditioned reinforcement learning (GCRL) aims to design agents that are able to learn general-purpose skills to reach diverse goals [Kaelbling, 1993, Schaul et al., 2015, Plappert et al., 2018]. In particular, offline GCRL learns goal-reaching policies by purely pre-collected data without any further interactions with the environment [Chebotar et al., 2021, Yang et al., 2022]. Since such interaction can be expensive or even unsafe in practice, offline GCRL is increasingly popular as a way to learn generalist agents in real-world environments [Lange et al., 2012, Levine et al., 2020]. Although offline GCRL is promising and achieves great success in various practical scenarios [Lynch et al., 2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022b,c], designing practical algorithms that are provably efficient still remains an open question. On the practical side, an ideal algorithm should be scalable to huge (or infinite) state spaces and only require minimal dataset coverage assumptions. Moreover, the algorithm should be computationally efficient and stable (e.g., 37th Conference on Neural Information Processing Systems (NeurIPS 2023).

only using regression-based methods to train policies to avoid unstable minimax optimization). On
the theoretical side, we aim to provide finite-sample guarantees of the learned policy.
Unfortunately, most existing algorithms are not both theoretically and practically efficient. On the
one hand, many empirically efficient algorithms do not enjoy finite-sample guarantees [Lynch et al.,
2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022c] or even suffer constant suboptimality
in favorable settings given infinite data (e.g., Ma et al. [2022c]). On the other hand, although
many previous offline RL algorithms with theoretical finite-sample guarantees can be naturally extended to offline GCRL settings, they either cannot handle general value function approximation in
the presence of huge (or infinite) state spaces [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al.,
2021, Shi et al., 2022, Li et al., 2022], or require impractically strong dataset coverage assumptions,
such as all policy concentrability [Antos et al., 2008, Munos and Szepesvári, 2008, Xie and Jiang,
2021b].

Recently, several provably efficient algorithms have been proposed under general function approximation and single-policy concentrability [Zhan et al., 2022, Cheng et al., 2022, Rashidinejad et al.,
2022]. In particular, the algorithm of [Zhan et al., 2022], based on the duality form of regularized
linear programming formulation of RL, only requires the realizability assumption of function class.
However, they all require solving minimax optimization problems which can be difficult or computationally unstable [Daskalakis et al., 2021]. On the contrary, some practically efficient algorithms
(e.g., Ma et al. [2022b,c]) do not involve minimax optimization and thus are computationally more
stable. This naturally raises an important question:

Can we design an efficient offline GCRL algorithm that enjoys favorable theoretical guarantees under mild assumptions and performs well empirically in real-world scenarios without a minimax formulation?

In this paper, we answer the above question affirmatively by providing rigorous theoretical guarantees for an empirically successful offline GCRL algorithm named GoFAR proposed by Ma et al.
[2022c]. We made some slight yet critical modifications to GoFAR. For deterministic MDPs, we
need to carefully select the value of one hyperparameter that is set to 1 in the original GoFAR (which
can be tuned in practice). For stochastic MDPs, we need to first learn the true transition model via
maximum likelihood (MLE) and then plug in the learned model in the algorithm. To distinguish
the difference between the original algorithm and the modified ones, we name the modified versions
VP-learning (Algorithm 1).

We show that the VP-learning algorithm has both good empirical performance in real-world scenarios (already shown by Ma et al. [2022c], and we compare VP-learning and GoFAR empirically
and show that our modification further improves the performance of the previous algorithm GoFAR)
and favorable theoretical guarantees under mild assumptions. Specifically, it achieves $$\tilde{O}(\text{poly}(1/\epsilon))$$
sample complexity (where $$\epsilon$$ is the desired suboptimality level of the learned policy) under general
function approximation with realizability-only assumption and partial data coverage with single-
policy concentrability assumption. Moreover, the VP-learning algorithm can be decomposed into
two uninterleaved learning (optimization) procedures (i.e., V -learning and policy learning), which
only require solving regression problems without minimax optimization.

Note that the VP-learning algorithm can be naturally applied to single-task RL settings, and all the
analysis in this paper does not rely on whether the setting is goal-conditioned. Since the original algorithm is proposed and empirically validated in goal-conditioned settings, we analyze the algorithm
in goal-conditioned settings as well.
------------
Header_1: Provably Efficient Offline Goal-Conditioned Reinforcement Learning
Header_2: 1.1 Related Work
filename: 2302.03770.pdf

1.1 Related Work

Since our algorithm can be naturally applied to single-task offline RL settings, we discuss the related
work in a broader scope, which also includes single-task offline RL.

Offline RL in tabular and linear function approximation settings. In tabular and linear settings, a line of work proposed efficient (both statistically and computationally) algorithms under
single-policy concentrability [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al., 2021, Shi et al.,
2022, Li et al., 2022]. These algorithms construct uncertainty quantifiers to ensure pessimism such
that policies not well covered by the dataset (which, by single-policy concentrability assumption,
------------
Header_1: Offline Reinforcement Learning
filename: 2302.03770.pdf

Offline Reinforcement Learning
------------
Header_1: Offline Reinforcement Learning
Header_2: Offline Reinforcement Learning
filename: 2302.03770.pdf

Offline Reinforcement Learning

Offline reinforcement learning (RL) is a challenging problem that involves learning a policy from a fixed dataset without interacting with the environment. In this paper, various approaches and algorithms for offline RL are discussed.
------------
Header_1: Offline Reinforcement Learning
Header_2: Offline Reinforcement Learning
Header_3: Offline RL with Single-Policy Concentrability
filename: 2302.03770.pdf

Offline RL with Single-Policy Concentrability

Yin and Wang [2021] proposed an offline RL algorithm with single-policy concentrability, achieving instance-dependent characterization. However, existing algorithms face limitations when non-linear function approximators are needed, as obtaining uncertainty quantifiers without oracle access is challenging.

In our algorithm (V-learning step), we use a regularizer in the form of f-divergence to ensure pessimism, making it efficient with non-linear function approximators.
------------
Header_1: Offline Reinforcement Learning
Header_2: Offline Reinforcement Learning
Header_3: Offline RL with All-Policy Concentrability
filename: 2302.03770.pdf

Offline RL with All-Policy Concentrability

Concentrability, defined as the ratio of occupancy frequency induced by a policy to the dataset distribution, is crucial for dataset coverability in offline RL. While many algorithms require all-policy concentrability for efficiency, our algorithm only needs single-policy concentrability.
------------
Header_1: Offline Reinforcement Learning
Header_2: Offline Reinforcement Learning
Header_3: Offline RL with General Function Approximation
filename: 2302.03770.pdf

Offline RL with General Function Approximation

Recent work based on marginalized importance sampling (MIS) formulation has shown success in offline RL with general function approximation and single-policy concentrability. These algorithms provide finite-sample guarantees without the need for minimax optimization.
------------
Header_1: Offline Reinforcement Learning
Header_2: Offline Reinforcement Learning
Header_3: Offline GCRL
filename: 2302.03770.pdf

Offline GCRL

In the context of offline Generative Counterfactual Reinforcement Learning (GCRL), addressing the sparsity of rewards is a core challenge. While empirical success has been demonstrated in previous works, theoretical understanding of offline GCRL remains limited.
------------
Header_1: Offline Reinforcement Learning
Header_2: Preliminaries
filename: 2302.03770.pdf

Preliminaries
------------
Header_1: Offline Reinforcement Learning
Header_2: Preliminaries
Header_3: Basic Notations
filename: 2302.03770.pdf

Basic Notations

Throughout the paper, |X| denotes the cardinality of set X, and ∆(X) represents the probability simplex of X. Various mathematical notations and definitions are used to establish the theoretical framework for offline RL algorithms.
------------
Header_1: Offline Reinforcement Learning
Header_2: Preliminaries
Header_3: Markov Decision Process
filename: 2302.03770.pdf

Markov Decision Process

We consider an infinite-horizon discounted Markov decision process (MDP) described by a tuple M = (S, A, P, R, ρ, γ). This framework forms the basis for modeling the environment and rewards in RL problems.

Expected reward function, \( \rho : S \rightarrow [0, 1] \) is the initial state distribution, and \( \gamma \in [0, 1) \) is the discount factor. We assume \( A \) is finite while \( S \) could be arbitrarily complex (even continuous) as in many real-world scenarios. A stationary (stochastic) policy \( \pi : S \rightarrow \Delta(A) \) outputs a distribution over action space for each state.

Goal-conditioned reinforcement learning. In goal-conditioned RL, we additionally assume a goal set \( G \). Similar to Ma et al. [2022c], in goal-conditioned settings, the reward function \( R(s; g) \) (as well as the expected reward function \( r(s; g) \)) and policy \( \pi(a|s, g) \) also depend on the commanded goal \( g \in G \), and the reward no longer depends on the action \( a \) and is deterministic.

Each (goal-conditioned) policy \( \pi \) induces a (discounted) occupancy density over state-action pairs for any commanded goal \( d\pi : S \times A \times G \rightarrow [0, 1] \) defined as \( d\pi(s, a; g) := (1-\gamma) \sum_{t=0}^{\infty} \gamma^t \text{Pr}(s_t = s, a_t = a; \pi) \), where \( \text{Pr}(s_t = s, a_t = a; \pi) \) denotes the visitation probability of state-action pair \( (s, a) \) at step \( t \), starting at \( s_0 \sim \rho(\cdot) \) and following \( \pi \) given commanded goal \( g \). We also write \( d\pi (s; g) = \sum_{a \in A} d\pi(s, a; g) \) to denote the marginalized state occupancy. Let \( p(g) \) be a distribution over desired goals, then we denote \( d\pi(s, a, g) = d\pi(s, a; g)p(g) \) and \( d\pi(s, g) = d\pi(s; g)p(g) \). An important property of occupancy density \( d\pi \) is that it satisfies the following Bellman flow constraint:

$$
\begin{equation}
a \cdot d(s, a; g) = (1 - \gamma)\rho(s) + \gamma \sum_{s',a'} P(s|s', a')d(s', a'; g) \tag{1}
\end{equation}
$$

for all \( s \in S \) and \( g \in G \) when letting \( d = d\pi \) for any policy \( \pi \). Moreover, any \( d \) satisfying (1) is the occupancy density of a policy \( \pi_d \) where

$$
\begin{equation}
\pi_d(a|s, g) = \frac{d(s, a; g)}{d(s; g)}, \quad d(s; g) > 0 \quad \text{and} \quad d(s; g) = \frac{1}{|A|}, \quad d(s; g) = 0 \quad \text{for} \quad a \in A \tag{2}
\end{equation}
$$

An important quantity associated with a policy \( \pi \) is the value function, which is the expected discounted cumulative reward defined as \( V^{\pi}(s; g) := E [ \sum_{t=0}^{\infty} \gamma^t r_t | s_0 = s, a_t \sim \pi(\cdot|s_t, g) \forall t \geq 0] \) starting at state \( s \in S \) with a commanded goal \( g \in G \) where \( r_t = R(s_t; g) = r(s_t; g) \). We use the notation \( J(\pi) := (1 - \gamma)E_{s \sim \rho, g \sim p}[V^{\pi}(s; g)] = E_{(s,a,g) \sim d\pi}[r(s; g)] \) to represent a scalar summary of the performance of a policy \( \pi \). We denote by \( \pi^* \) the optimal policy that maximizes the above objective and use \( V^* := V^{\pi^*} \) to denote the optimal value function.

Offline GCRL. In this paper, we focus on offline GCRL, where the agent is only provided with a previously-collected offline dataset \( D = \{(s_i, a_i, r_i, s'_i, g_i)\}_{i=1}^N \). Here, \( r_i \sim R(s_i; g_i) \), \( s'_i \sim P(\cdot | s_i, a_i) \), and we assume that \( g_i \) are i.i.d. sampled from \( p(\cdot) \) and it is common that data are collected by a behavior policy \( \mu \) of which the discounted occupancy density is \( d\mu \). Therefore, we assume that \( (s_i, a_i, g_i) \) are sampled i.i.d. from a distribution \( \mu \) where \( \mu(s, a, g) = p(g)d\mu(s, a; g) = d\mu(s, a, g) \). Note that we use \( \mu \) to denote both the behavior policy and the dataset distribution. We also assume an additional dataset \( D_0 = \{(s_{0,i}, g_{0,i})\}_{i=1}^{N_0} \) where \( s_{0,i} \) are i.i.d. sampled from \( \rho(\cdot) \) and \( g_{0,i} \) are i.i.d. sampled from \( p(\cdot) \). The goal of offline RL is to learn a policy \( \hat{\pi} \) using the offline dataset so as to minimize the sub-optimality compared to the optimal policy \( \pi^* \), i.e., \( J(\pi^*) - J(\hat{\pi}) \), with high probability.

Function approximation. To deal with huge state spaces, (general) function approximation is necessary for practical scenarios. In this paper, we assume access to two function classes: a value function class \( V \subseteq \{V : S \times G \rightarrow [0, V_{\text{max}}]\} \) that models the value function of the (regularized) optimal policies, and a policy class \( \Pi \subseteq \{\pi : S \times G \rightarrow \Delta(A)\} \) consisting of candidate policies. For stochastic MDP (Section 3.1.2), we also need a transition kernel class \( P \subseteq \{P : S \times A \rightarrow \Delta(S)\} \) which contains the ground-truth transition kernel. Additionally, for any function \( f : S \times G \rightarrow \mathbb{R} \), we denote the operator \( T : \mathbb{R}^{S \times G} \rightarrow \mathbb{R}^{S \times A \times G} \) as \( (T f)(s, a; g) = E_{s' \sim P(\cdot|s,a)}[f(s'; g)] \). Also, for any function \( V : S \times G \rightarrow [0, V_{\text{max}}] \), we define \( AV(s, a; g) = r(s; g) + \gamma T V(s, a; g) - V(s; g) \).

Offline data coverage assumption. Our algorithm works within the single-policy concentrability framework [Rashidinejad et al., 2021], which is defined as below.

Definition 1 (Single-policy concentrability for GCRL). Given a policy \( \pi \), define \( C_{\pi} \) to be the smallest constant that satisfies \( d_{\pi}(s,a,g) \leq C_{\pi} \) for all \( s \in S \), \( a \in A \) and \( g \in G \).
------------
Header_1: Math Equations
filename: 2302.03770.pdf

Math Equations

The single-policy concentrability parameter \(C_{\pi}\) captures the coverage of policy \(\pi\) in the offline data. Our algorithm only requires this parameter to be small for \(\pi^*\) which is a regularized optimal policy (see Section 3 for formal definition) and is close to some optimal policy. This assumption is similar to Zhan et al. [2022] and is much weaker than the widely used all-policy concentrability that assumes bounded \(C_{\pi}\) for all \(\pi\) (e.g., Scherrer [2014]).
------------
Header_1: Math Equations
Header_3: Algorithms
filename: 2302.03770.pdf

Algorithms

Offline GCRL can be formulated as the following program:

$$
\max_{\pi} \mathbb{E}_{(s,g) \sim d_{\pi}(s,g)}[r(s; g)]. \tag{3}
$$
(3) requires solving an optimization problem over the policy space. One can also optimize over occupancy density \(d(s, a; g)\) s.t. \(d = d_{\pi}\) for some policy \(\pi\) which is equivalent to that \(d\) satisfies Bellman flow constraint (1). Therefore, the program (3) can be represented equivalently as follows:

$$
\begin{aligned}
& \max_{d(s,a;g) \geq 0} \mathbb{E}(s,g) \sim d(s,g)[r(s; g)] \\
& \text{s.t.} \quad \sum_{a} d(s, a; g) = (1 - \gamma) \rho(s) + \gamma \sum_{s',a'} P(s|s', a')d(s', a'; g), \quad \forall (s, g) \in S \times G. \tag{4}
\end{aligned}
$$

Let \(d^*\) denote the optimal solution of (4), then (one of) the optimal policy can be induced by \(\pi^* = \pi d^*\) as in (2). Under partial data coverage assumptions, (4) might fail in empirical settings by choosing a highly suboptimal policy that is not well covered by the dataset with constant probability. Similar to Zhan et al. [2022], Ma et al. [2022c], a regularizer is needed to ensure that the learned policy is well covered by the dataset. Therefore, one should instead solve a regularized version of (4), which is stated as follows:

$$
\begin{aligned}
& \max_{d(s,a;g) \geq 0} \mathbb{E}(s,g) \sim d(s,g)[r(s; g)] - \alpha D_f(d\| \mu) \\
& \text{s.t.} \quad \sum_{a} d(s, a; g) = (1 - \gamma) \rho(s) + \gamma \sum_{s',a'} P(s|s', a')d(s', a'; g), \quad \forall (s, g) \in S \times G, \tag{5}
\end{aligned}
$$

where the f-divergence is defined as \(D_f(d\| \mu) = \mathbb{E}(s,a,g) \sim \mu[f(d(s, a, g)/\mu(s, a, g))]\) for a convex function \(f\). Throughout this paper, we choose \(f(x) = \frac{1}{2}(x - 1)^2\) as in Ma et al. [2022c], where the f-divergence is known as \(\chi^2\)-divergence under this specific choice of \(f\) and it is shown to be more stable than other divergences such as KL divergence [Ma et al., 2022c]. Let \(d^*_\alpha\) denote the optimal solution of (5), then the regularized optimal policy can be induced by \(\pi^*_\alpha = \pi d^*_\alpha\) as in (2). The following single-policy concentrability assumption assumes that \(\pi^*_\alpha\) is well covered by the offline dataset.

Assumption 1 (Single-policy concentrability for \(\pi^*_\alpha\)). Let \(d^*_\alpha\) be the optimal solution of (5), and let \(\pi^*_\alpha = \pi d^*_\alpha\) as defined in (2). We assume \(C_{\pi^*_\alpha} \leq C^*_\alpha\) where \(C_{\pi^*_\alpha}\) is defined in Definition 1 and \(C^*_\alpha > 0\) is a constant.

Under Assumption 1, it can be observed that the performance difference between the regularized optimal policy \(\pi^*_\alpha\) and the optimal policy \(\pi^*\) is bounded by \(O(\alpha)\). The following proposition formally presents this observation.

Proposition 3.1. Let \(d^*_\alpha\) be the optimal solution of (5), and let \(\pi^*_\alpha = \pi d^*_\alpha\) as defined in (2). Then under Assumption 1, it holds that \(J(\pi^*) - J(\pi^*_\alpha) \leq O(\alpha(C^*_\alpha)^2)\).

The proof of Proposition 3.1 is deferred to Appendix A.1. Proposition 3.1 shows that by solving the regularized program (5), we can obtain a near-optimal policy as long as \(\alpha\) is small. The algorithm of Ma et al. [2022c] also aims to solve (5) and they simply choose \(\alpha = 1\). We show empirically in Section 5 that \(\alpha < 1\) achieves better performance than \(\alpha = 1\). In theory, we must carefully choose the value of \(\alpha\) s.t. the suboptimality of our learned policy vanishes to 0 with a reasonable rate. Finally, as in Ma et al. [2022c], we convert (5) to the dual form, which is an unconstrained problem and amenable to solve:

Proposition 3.2 (Dual form of (5)). The duality form of (5) is

$$
\begin{aligned}
& \min_{V(s;g) \geq 0} \left[(1 - \gamma) \mathbb{E}(s,g) \sim (\rho, p(g))[V(s; g)] + \mathbb{E}(s,a,g) \sim \mu[1\{g'\} \geq 0}\right].
\end{aligned} \tag{6}
$$
------------
Header_1: Document
filename: 2302.03770.pdf

Document

where $$g^*$$ is the convex conjugate of $$g = \alpha \cdot f$$. Moreover, let $$V^*_{\alpha}$$ denote the optimal solution of (6), then it holds

$$\alpha(s, a; g) = \mu(s, a; g)g'^* (r(s; g) + \gamma T V^*_{\alpha}(s, a; g) - V^*_{\alpha}$$ for all $$(s, a, g) \in S \times A \times G$$.

The proof of Proposition 3.2 is shown in Appendix A.2. According to the above proposition, one can first learn the $$V$$ function according to (6), and then use the learned $$V$$ function to learn the desired policy by (7). We call the first step $$V$$-learning and the second step policy learning, which will be discussed in detail in Sections 3.1 and 3.2 respectively. Finally, the main algorithm, which we call VP-learning, is presented in Algorithm 1.

Algorithm 1 VP-learning

1. Input: Dataset $D = \{(s_i, a_i, r_i, s'_i, g_i)\}_{N i=1}$, $D_0 = \{(s_{0,i}, g_{0,i})\}_{N_0 i=1}$, value function class $V$, policy class $\Pi$, model class $P$ for stochastic settings.
2. Obtain $\hat{U}$ by $V$-Learning (Algorithm 2 or 3).
3. Obtain $\hat{\pi}$ by policy learning (Algorithm 4) using learned function $\hat{U}$.
4. Output: $\hat{\pi}$.
------------
Header_1: Document
Header_3: 3.1 $$V$$-Learning
filename: 2302.03770.pdf

3.1 $$V$$-Learning

Define

$$L_{\alpha}(V) = \alpha((1 - \gamma)E(s,g) \sim (\rho, p(g))[V(s; g)] + E(s,a,g) \sim \mu[1\{g'^*(AV(s, a; g)) \geq 0\}\bar{g}^*(AV(s, a; g))]).$$ (8)

Then (6) is equivalent to $$\min_{V(s;g) \geq 0} L_{\alpha}(V)$$. A natural estimator of $$L_{\alpha}(V)$$ is

$$\frac{1 - \gamma}{N_0} \alpha \cdot V(s_{0,i}; g_{0,i}) + \frac{1}{N} \alpha \cdot g^* + (r_i + \gamma V(s'_i; g_i) - V(s_i; g_i)).$$ (9)

However, when the transition kernel is not deterministic, this estimator is biased and will cause an over-estimation issue since $$g^*(x) = \alpha f^*(x/\alpha) = \alpha(x/\alpha+1)^2 - \alpha^2$$ contains a square operator outside of the Bellman operator (consider estimating $$(E[X])^2$$ using $$\frac{1}{N} X^2_i$$).

Therefore, we use the original version of Ma et al. [2022c] for $$V$$-learning in deterministic dynamics (Algorithm 2 in Section 3.1.1), and a slightly modified version in stochastic dynamics (Algorithm 3 in Section 3.1.2). For both settings, we assume realizability of $$V^*_{\alpha}$$ on value function class $$V$$:

Assumption 2 (Realizability of $$V^*_{\alpha}$$). Assume $$V^*_{\alpha} \in V$$.
------------
Header_1: Document
Header_3: 3.1 $$V$$-Learning
Header_4: 3.1.1 $$V$$-Learning in Deterministic Dynamics
filename: 2302.03770.pdf

3.1.1 $$V$$-Learning in Deterministic Dynamics

When the transition kernel $$P$$ is deterministic, it holds that $$T V(s, a; g) = V(s'; g)$$ where $$P(s' | s, a) = 1$$. In this case, the natural estimator (9) is unbiased and can be directly applied to the $$V$$-learning procedure. The $$V$$-learning algorithm for deterministic dynamic settings is presented in Algorithm 2.

Algorithm 2 $$V$$-learning in deterministic dynamics

1. Input: Dataset $D = \{(s_i, a_i, r_i, s'_i, g_i)\}_{N i=1}$, $D_0 = \{(s_{0,i}, g_{0,i})\}_{N_0 i=1}$, value function class $V$.
2. $V$-learning by solving $\hat{V} = \arg \min_{V \in V} \hat{L}(d) (V)$ where
3. $\hat{L}(d) (V) \triangleq \frac{1 - \gamma}{N_0} \alpha \cdot V(s_{0,i}; g_{0,i}) + \alpha \frac{1}{N} g^+(r_i + \gamma V(s'_i; g_i) - V(s_i; g_i)).$ (10)

$\hat{U}(s, a; g) \leftarrow r(s; g) + \gamma \hat{V}(s'; g) - V\hat{(s; g) + \alpha$
4. Output: $\hat{V}$, $\hat{U}$.

Now for any V, we define $$UV(s, a; g) = r(s; g) + \gamma T V(s, a; g) - V(s; g) + \alpha = AV(s, a; g) + \alpha$$ which can be interpreted as the advantage function of V with an α-shift. We also denote $$U^*_\alpha = UV^*_\alpha$$.

Note that besides the learned $\hat{V}$ function, Algorithm 2 also outputs a $\hat{U}$ function. By (7), one can observe that in policy learning, what we indeed need is $\hat{U}$ instead of $\hat{V}$, and thus in the V-learning procedure we also compute this $\hat{U}$ function in preparation for policy learning.

One may challenge that $\hat{U}$ cannot be computed for all $(s, a; g)$ since we do not have knowledge of all $r(s; g)$. However, we only need the value of $\hat{U}(s_i, a_i; g_i)$ for $(s_i, a_i; g_i)$ contained in the offline dataset, where $r_i$ is also contained. Therefore, we can evaluate the value of $\hat{U}$ at all $(s, a; g)$ tuples requested in the policy learning algorithm.

Note that Algorithm 2 is equivalent to the first step of Ma et al. [2022c] except for the choice of α and a clip for the value of $g^*$. However, the above V-learning, as well as the original GoFAR algorithm, might suffer the over-estimation issue under stochastic dynamics, and we present algorithms suitable for stochastic dynamics in Section 3.1.2.
------------
Header_1: Document
Header_3: 3.1.2 V-Learning in Stochastic Dynamics
filename: 2302.03770.pdf

3.1.2 V-Learning in Stochastic Dynamics

When the transition kernel is stochastic, one cannot directly use $V(s'; g)$ to estimate $T V(s, a; g)$. Since $T V(s, a; g) = E_{s'\sim P(\cdot|s,a)}[V(s'; g)]$, a natural idea is to learn the ground-truth transition kernel $P^*$ first, and then use the learned transition kernel $\hat{P}$ to estimate $T V(s, a; g)$: $\hat{T} V(s, a; g) = E_{s'\sim \hat{P}(\cdot|s,a)}[V(s'; g)]$. This is achievable under the following realizability assumption.

Assumption 3 (Realizability of the ground-truth transition model). Assume the ground-truth transition kernel $P^* \in \mathcal{P}$.

The algorithm for stochastic dynamic settings is presented in Algorithm 3, where we first learn the transition kernel $\hat{P}$, and then plug in the learned transition kernel to learn V function. Similar to Algorithm 2, we also compute $\hat{U}$ in V-learning procedure.

Algorithm 3 V-Learning in Stochastic Dynamics

1. Input: Dataset $D = \{(s_i, a_i, r_i, s'_i, g_i)\}_{i=1}^N$, $D_0 = \{(s_{0,i}, g_{0,i})\}_{i=1}^{N_0}$, value function class $V$, model class $P$.
2. Estimate the transition kernel via maximum likelihood estimation (MLE): $\hat{P} = \max_{P \in \mathcal{P}} \sum_{i=1}^{N_1} \log P(s'_i|s_i, a_i)$
3. V-learning using the learned transition kernel: $\hat{V} = \arg \min_{V \in V} \hat{L}(s)(V)$ with $\hat{L}(s)(V) \triangleq 1 - \gamma \alpha \cdot V(s_{0,i}; g_{0,i}) + \alpha g^* + (r_i + \gamma \hat{T} V(s_i, a_i; g_i) - V(s_i; g_i))$, where $\hat{T} V(s, a, ; g) = E_{s'\sim \hat{P}(\cdot|s,a)}[V(s'; g)]$.
4. $\hat{U}(s, a; g) \leftarrow r(s; g) + \gamma \hat{T} \hat{V}(s, a; g) - \hat{V}(s; g) + \alpha$
5. Output: $\hat{V}$, $\hat{U}$.
------------
Header_1: Document
Header_3: 3.2 Policy Learning
filename: 2302.03770.pdf

3.2 Policy Learning

We now derive policy learning, the second step of the VP-learning algorithm. Note that $\pi^*_\alpha = \arg \max_{\pi} E(s,a,g) \sim d^*_\alpha [\log \pi(a|s, g)]$. By (7), we also have

$d^*_\alpha(s, a; g) = \mu(s, a; g)g^* (r(s; g) + \gamma T V^*_\alpha(s, a; g) - V^*_\alpha(s, a; g)+ \alpha = \mu(s, a; g)U^*_\alpha$

1For notation convenience, in stochastic settings, we use $P^*$ to denote the ground-truth transition kernel.
------------
Header_1: Math Equations and Text
filename: 2302.03770.pdf

Math Equations and Text

Therefore, $$\pi^*_{\alpha} = \arg \max_{\pi} L_{MLE}(\pi)$$ where $$L_{MLE}(\pi) \triangleq E(s,a,g) \sim \mu [U^*_{\alpha}(s,a;g) + \log \pi(a|s,g)].$$

Since we already learned $$\hat{U}$$, which is close to $$U^*_{N\hat{\alpha}}$$, we can use the following estimator for $$L_{MLE}(\pi):$$

$$
L_{MLE}(\pi) = \frac{1}{N} \sum_{i=1}^{N} \hat{U}(s_i, a_i; g_i) + \log \pi(a_i|s_i, g_i). \quad (13)
$$
Algorithm 4 Policy learning

1. Input: Dataset $D = \{(s_i, a_i, r_i, s'_i, g_i)\}_{i=1}^{N}$, policy class $\Pi$, $\hat{U}$ learned by Algorithm 2 or 3.
2. Policy learning by:
3. $
\hat{U}(s_i, a_i; g_i) + \quad (14)
$
Output: $\hat{\pi}$. $\pi = \arg \max_{\pi \in \Pi} L_{MLE}(\pi) \triangleq \frac{1}{N} \sum_{i=1}^{N} \alpha \log \pi(a_i|s_i, g_i).$

The policy learning algorithm is presented in Algorithm 4, which can be viewed as a weighted maximum likelihood estimation (MLE) procedure. Finally, we make the following two assumptions on the policy class $$\Pi$$.

Assumption 4 (Single-policy realizability). Assume $$\pi^*_{\alpha} \in \Pi$$.

Assumption 5 (Lower bound of policy). For any policy $$\pi \in \Pi$$, we assume that $$\pi(a|s,g) \geq \tau > 0$$ for any $$(s, a, g) \in S \times A \times G$$.

Remark 1. One may consider Assumption 5 strong if $$\tau$$ is a constant independent of $$\alpha$$ or $$N$$. However, we allow that $$\tau$$ depends on $$\alpha$$. In that case, $$\tau$$ can be extremely small, and any policy mixed with a uniform policy with a tiny probability satisfies this assumption. Therefore, Assumption 5 is mild.
------------
Header_1: Math Equations and Text
Header_2: Theoretical Guarantees
filename: 2302.03770.pdf

Theoretical Guarantees

In this section, we provide theoretical guarantees of our main algorithm (Algorithm 1). We first show the results for V-Learning and policy learning in Section 4.1 and Section 4.2 respectively and then combine them to obtain our main theorem in Section 4.3.
------------
Header_1: Math Equations and Text
Header_2: Theoretical Guarantees
Header_3: 4.1 Analysis of V-Learning
filename: 2302.03770.pdf

4.1 Analysis of V-Learning

We mainly focus on V-learning in deterministic dynamics in this section. The analysis for stochastic dynamics is similar and presented in Appendix B.2.

As discussed in Section 3.1.1, although the first step of the algorithm is called V-learning, the main goal of this step is to estimate $$U^*_{\alpha} = UV^*_{\alpha}$$ accurately. The following lemma provides a theoretical guarantee that the output of V-learning algorithm $$\hat{U}$$ is a good estimator of $$U^*_{\alpha}$$ in positive parts:

Lemma 1 (Closeness of $$\hat{U}^+_{\alpha}$$ and $$U^*_{\alpha}^+$$). Under Assumptions 1 and 2, with probability at least $$1 - \log(|V|/\delta)$$. $$\| \hat{U}^+_{\alpha} - U^*_{\alpha}^+ \|_{2,\mu} \leq O(\sqrt{\max(N,V^2)})$$.

Proof sketch. By standard concentration argument, it can be shown that the empirical estimator $$\hat{L}(d)$$ in Algorithm 2 (which is unbiased in deterministic dynamics) concentrates well on $$L_{\alpha}$$ for all $$V \in V$$ (Lemma 3). Therefore, by realizability of $$V^*_{\alpha}$$, the value of $$L_{\alpha}$$ at $$V^*_{\alpha}$$ and the learned V-function $$\hat{V}$$ are close (Lemma 4). Finally, one can observe that $$L_{\alpha}$$ is "semi-strongly" convex w.r.t. $$UV^+$$ in $$\| \cdot \|_{2,\mu}$$-norm, and thus we can show that $$\hat{U}^+_{\alpha}$$ and $$U^*_{\alpha}^+$$ are also close.

The complete proof of Lemma 1 is deferred to Appendix B.1. In Appendix B.2, we also show the counterpart of Lemma 1 for stochastic dynamic settings.
------------
Header_1: Math Equations and Text
Header_2: Analysis of Policy Learning
filename: 2302.03770.pdf

Analysis of Policy Learning

After obtaining an accurate estimator $$\hat{U}$$ of $$U^* - \alpha^+$$ in the V-Learning procedure, i.e., $$\| \hat{U} - U^* \|_{\epsilon_{\text{stat}}}$$, we can use $$\hat{U}$$ to perform policy learning and obtain the following guarantee:

$$
\alpha^2, \mu \lesssim \sqrt{}
$$

Lemma 2 (Closeness of $$\pi^* - \alpha$$ and $$\hat{\pi}$$). Under Assumptions 4 and 5, with probability at least $$1 - \delta$$, the output policy $$\hat{\pi}$$ of Algorithm 4 satisfies

$$
E_{s \sim d^*} \| \pi(\cdot | s, g) - \hat{\pi} \|_{\text{TV}} \leq O \frac{\epsilon_{\text{MLE stat}}}{\tau^2},
$$

where $$\epsilon_{\text{MLE stat}}$$ is defined in Lemma 8.

The proof of Lemma 2 is provided in Appendix C.2. This result shows that the TV distance between the regularized optimal policy $$\pi^* - \alpha$$ and the output policy $$\hat{\pi}$$ by Algorithm 4 is small, which translates to a bounded performance difference between these two policies as formalized in Theorem 1.

Theorem 1 (Suboptimality of $$\hat{\pi}$$). Under Assumptions 4 and 5, with probability at least $$1 - \delta$$, the output policy $$\hat{\pi}$$ of Algorithm 4 satisfies $$J(\pi^* - \pi) \leq O V_{\text{max}} \epsilon_{\text{MLE}}$$.

The proof of Theorem 1 is deferred to Appendix C.3.
------------
Header_1: Math Equations and Text
Header_2: Main Theorem: Statistical Rate of Suboptimality
filename: 2302.03770.pdf

Main Theorem: Statistical Rate of Suboptimality

Theorem 1 compares the performance difference between $$\hat{\pi}$$ and the regularized optimal policy $$\pi^* - \alpha$$. Since the ultimate goal is to compare with the optimal policy $$\pi^*$$, we also need to combine this result with Proposition 3.1. By carefully choosing the value of $$\alpha$$ to balance $$J(\hat{\pi}) - J(\pi^* - \alpha)$$ and $$J(\pi) - J(\pi^* - \alpha)$$, we can bound the suboptimality of the policy $$\hat{\pi}$$ output by Algorithm 1 compared to the optimal policy $$\pi^*$$, leading to the following main result:

Theorem 2 (Statistical rate of suboptimality (in deterministic dynamics)). Under Assumptions 1, 2, 4 and 5, with probability at least $$1 - \delta$$, the output policy $$\hat{\pi}$$ by Algorithm 1 (with the choice of Algorithm 2 for V-learning in deterministic dynamics) satisfies

$$
J(\pi^*) - J(\hat{\pi}) \lesssim V_{\text{max}}^3(C^* \alpha)^3 \log(1/\tau) \log(|V||\Pi|/\delta)^{1/3} \frac{\tau^2}{N^{1/4}}
$$

if we choose $$\alpha \approx \frac{V_{\text{max}}^3(C^* \alpha)^3N^{1/4}}{\tau^2}$$ and assume $$N = N_0$$.

The proof of Theorem 2 is deferred to Appendix D.1. Note that Theorem 2 provides a suboptimality rate of $$O(1/N^{1/12})$$ which implies an $$O(1/\text{poly}(\epsilon))$$ sample complexity and thus is statistically efficient. A similar rate can also be obtained in stochastic dynamic settings, and we present the result in Appendix D.2. Note that our rate is slightly worse than the $$O(1/N^{1/6})$$ rate in Zhan et al. [2022], and worse than the optimal rate $$O(1/\sqrt{N})$$ in Rashidinejad et al. [2022]. We briefly discuss the intrinsic difficulty to derive an optimal convergence rate. First, we only require a realizability assumption on our function class, while Rashidinejad et al. [2022] requires a much stronger completeness assumption. Second, our optimization procedure is uninterleaved and only requires solving regression problems, while Zhan et al. [2022] and Rashidinejad et al. [2022] require solving minimax problems. Finally, Rashidinejad et al. [2022] assumes that the behavior policy is known and directly computes the policy using the knowledge of the behavior policy, while our algorithm uses a more practical method, i.e., MLE, to solve the policy in the policy learning step.

We also compare our theoretical results to Ma et al. [2022c]. Theorem 4.1 of Ma et al. [2022c] provides a finite-sample guarantee for the suboptimality. However, they compare the performance of $$\hat{\pi}$$ to $$\pi^* - \alpha$$ (with $$\alpha = 1$$) instead of $$\pi^*$$. Since the performance gap between $$\pi^*$$ and $$\pi^* - \alpha$$ can be as large as a constant when $$\alpha = 1$$, even zero suboptimality (compared to $$\pi^* - \alpha$$) cannot imply that the learned policy has good performance. Moreover, their theoretical analysis assumes that $$V^*$$ can be learned with zero error, which is unreasonable in practical scenarios. We also note that they only provide a proof for deterministic policy classes, which can be restrictive in practice.
------------
Header_1: Experimental Results
filename: 2302.03770.pdf

Experimental Results
------------
Header_1: Experimental Results
Header_2: Experiments
filename: 2302.03770.pdf

Experiments

In this section, we provide experimental results of our VP-learning algorithm with different choices of α under five different environments: FetchReach, FetchPick, FetchPush, FetchSlide, and HandReach [Plappert et al., 2018]. Similar to Ma et al. [2022c], the datasets for the five tasks are from Yang et al. [2022]. All the implementation details of our VP-learning are the same as GoFAR (see dataset details and implementation details in Ma et al. [2022c]), except for the value of α. Note that our VP-learning algorithm with α = 1 is equivalent to the GoFAR algorithm. Table 1 presents the discounted returns and Table 2 presents the final distances of the policies trained after 100 epochs and evaluated over 10 runs. For each environment and each α, the result was averaged over 3 random seeds. The best results of each environment are in bold.
------------
Header_1: Experimental Results
Header_2: Experiments
Header_3: Table 1: Discounted return of different choices of α, averaged over 3 random seeds.
filename: 2302.03770.pdf

Table 1: Discounted return of different choices of α, averaged over 3 random seeds.

|α\ Env|FetchReach|FetchPick|FetchPush|FetchSlide|HandReach|
|---|---|---|---|---|---|
|0.01|27.4 ± 0.29|18.5 ± 0.1|18.0 ± 1.8|2.36 ± 1.13|8.72 ± 1.69|
|0.02|27.4 ± 0.32|18.7 ± 1.8|18.6 ± 2.6|2.40 ± 0.47|7.96 ± 1.27|
|0.05|27.4 ± 0.32|17.3 ± 1.1|19.3 ± 2.0|3.18 ± 0.90|8.98 ± 3.11|
|0.1|27.4 ± 0.33|20.3 ± 1.3|20.3 ± 2.5|3.22 ± 0.38|5.28 ± 1.25|
|0.2|27.4 ± 0.32|20.7 ± 0.9|17.7 ± 2.9|2.25 ± 0.23|2.92 ± 0.98|
|0.5|27.5 ± 0.29|18.5 ± 0.4|20.1 ± 2.2|3.47 ± 1.08|5.74 ± 2.72|
|1|27.3 ± 0.34|18.2 ± 1.2|19.6 ± 1.6|2.75 ± 1.84|7.13 ± 3.60|
|2|27.4 ± 0.29|18.3 ± 0.7|19.6 ± 1.4|1.80 ± 0.66|3.99 ± 1.88|
------------
Header_1: Experimental Results
Header_2: Experiments
Header_3: Table 2: Final distance of different choices of α, averaged over 3 random seeds.
filename: 2302.03770.pdf

Table 2: Final distance of different choices of α, averaged over 3 random seeds.

|α\ Env|FetchReach|FetchPick|FetchPush|FetchSlide|HandReach|
|---|---|---|---|---|---|
|0.01|0.0171 ± 0.0017|0.042 ± 0.004|0.033 ± 0.001|0.1177 ± 0.012|0.0269 ± 0.0049|
|0.02|0.0168 ± 0.0016|0.045 ± 0.012|0.031 ± 0.002|0.1085 ± 0.010|0.0274 ± 0.0049|
|0.05|0.0181 ± 0.0011|0.052 ± 0.013|0.032 ± 0.002|0.1061 ± 0.009|0.0270 ± 0.0049|
|0.1|0.0173 ± 0.0014|0.032 ± 0.010|0.027 ± 0.002|0.1018 ± 0.002|0.0275 ± 0.0043|
|0.2|0.0172 ± 0.0019|0.031 ± 0.004|0.031 ± 0.003|0.1029 ± 0.010|0.0275 ± 0.0046|
|0.5|0.0166 ± 0.0011|0.044 ± 0.009|0.031 ± 0.005|0.1017 ± 0.017|0.026826 ± 0.0049|
|1|0.0175 ± 0.0013|0.043 ± 0.011|0.043 ± 0.012|0.1202 ± 0.019|0.026828 ± 0.0044|
|2|0.0171 ± 0.0011|0.034 ± 0.005|0.032 ± 0.001|0.1044 ± 0.011|0.0275 ± 0.0045|
------------
Header_1: Experimental Results
Header_2: Conclusions
filename: 2302.03770.pdf

Conclusions

In this paper, we theoretically analyze the VP-learning algorithm (Algorithm 1, which is based on the previous empirically successful algorithm in Ma et al. [2022c]) for both single-task and goal-conditioned offline settings. This algorithm can deal with general value function approximation and only requires near minimal assumptions on the dataset (single-policy concentrability) and function class (realizability). We also provide an O(1/N^1/12) upper bound of the suboptimality of the policy learned by the algorithm and empirically validate its effectiveness.

As for future directions, one important question is whether we can achieve the optimal suboptimality rate O(1/√N) while keeping the algorithm practical without unreasonably strong assumptions.

We use the code at https://github.com/JasonMa2016/GoFAR with different values of α for our experiments.
------------
Header_1: Acknowledgements and References
filename: 2302.03770.pdf

Acknowledgements and References
------------
Header_1: Acknowledgements
filename: 2302.03770.pdf

Acknowledgements

We thank the anonymous reviewer for catching a technical issue in a previous version of our paper. The work was done when HZ was a visiting researcher at Meta.
------------
Header_1: Acknowledgements
Header_2: References
filename: 2302.03770.pdf

References

1. Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pages 10–4, 2019.
2. Andras Antos, Rémi Munos, and Csaba Szepesvari. Fitted Q-iteration in continuous action-space mdps. In Neural Information Processing Systems, 2007.
3. András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89–129, 2008.
4. Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.
5. Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. arXiv preprint arXiv:1905.00360, 2019.

|Author(s)|Title|Year|
|---|---|---|
|Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal|Adversarially trained actor critic for offline reinforcement learning|2022|
|Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis|The complexity of constrained min-max optimization|2021|
|Amir Massoud Farahmand, Rémi Munos, and Csaba Szepesvári|Error propagation for approximate policy and value iteration|2010|
|Yihao Feng, Lihong Li, and Qiang Liu|A kernel loss for solving the Bellman equation|2019|
|Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine|Learning to reach goals via iterated supervised learning|2019|
------------
Header_1: References
filename: 2302.03770.pdf

References
------------
Header_1: List of References
filename: 2302.03770.pdf

List of References

|Authors|Title|Publication Details|
|---|---|---|
|Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim|Optidice: Offline policy optimization via stationary distribution correction estimation|International Conference on Machine Learning, pages 6120–6130, PMLR, 2021|
|Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu|Offline reinforcement learning: Tutorial, review, and perspectives on open problems|arXiv preprint arXiv:2005.01643, 2020|
|Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei|Settling the sample complexity of model-based offline reinforcement learning|arXiv preprint arXiv:2204.05275, 2022|
|Peng Liao, Zhengling Qi, and Susan Murphy|Batch policy learning in average reward Markov decision processes|arXiv preprint arXiv:2007.11771, 2020|
|Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang|Neural trust region/proximal policy optimization attains globally optimal policy|Neural Information Processing Systems, 2019|
|Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet|Learning latent plans from play|Conference on robot learning, pages 1113–1132, PMLR, 2020|
|Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani|Smodice: Versatile offline imitation learning via state occupancy matching|arXiv preprint arXiv:2202.02433, 2022a|
|Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang|Vip: Towards universal visual reward and representation via value-implicit pre-training|arXiv preprint arXiv:2210.00030, 2022b|
|Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani|How far I'll go: Offline goal-conditioned reinforcement learning via f-advantage regression|arXiv preprint arXiv:2206.03023, 2022c|
|Rémi Munos|Performance bounds in $\ell_p$-norm for approximate value iteration|SIAM journal on control and optimization, 46(2):541–561, 2007|
|Rémi Munos and Csaba Szepesvári|Finite-time bounds for fitted value iteration|Journal of Machine Learning Research, 9(5), 2008|
|Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li|Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections|Advances in Neural Information Processing Systems, 32, 2019a|
|Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans|Algaedice: Policy gradient from arbitrary experience|arXiv preprint arXiv:1912.02074, 2019b|
|Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al.|Multi-goal reinforcement learning: Challenging robotics environments and request for research|arXiv preprint arXiv:1802.09464, 2018|
|Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell|Bridging offline reinforcement learning and imitation learning: A tale of pessimism|Advances in Neural Information Processing Systems, 34:11702–11716, 2021|
|Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao|Optimal conservative offline RL with general function approximation via augmented lagrangian|arXiv preprint arXiv:2211.00716, 2022|
|Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver|Universal value function approximators|International conference on machine learning, pages 1312–1320, PMLR, 2015|
|Bruno Scherrer|Approximate policy iteration schemes: A comparison|International Conference on Machine Learning, pages 1314–1322, 2014|
------------
Header_1: References
filename: 2302.03770.pdf

References
------------
Header_1: References
Header_2: List of References:
filename: 2302.03770.pdf

List of References:

1. Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic Q-learning for offline reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890, 2022.
2. Csaba Szepesvári and Rémi Munos. Finite time bounds for sampling based fitted value iteration. In Proceedings of the 22nd international conference on Machine learning, pages 880–887, 2005.
3. Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In International Conference on Learning Representations, 2021.
4. Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and Q-function learning for off-policy evaluation. In International Conference on Machine Learning, pages 9659–9668. PMLR, 2020.
5. Sara Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.
6. Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. In International Conference on Learning Representations, 2019.
7. Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404–11413. PMLR, 2021a.
8. Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404–11413. PMLR, 2021b.
9. Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683–6694, 2021.
10. Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. arXiv preprint arXiv:2202.04478, 2022.
11. Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism. Advances in neural information processing systems, 34:4065–4078, 2021.
12. Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double variance reduction. arXiv preprint arXiv:2102.01748, 2021.
13. Ming Yin, Wenjing Chen, Mengdi Wang, and Yu-Xiang Wang. Offline stochastic shortest path: Learning, evaluation and towards optimality. In Uncertainty in Artificial Intelligence, pages 2278–2288. PMLR, 2022.
14. Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730–2775. PMLR, 2022.
15. Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151, 2020.
------------
Header_1: Missing Proofs for Propositions in Section 3
filename: 2302.03770.pdf

Missing Proofs for Propositions in Section 3
------------
Header_1: Missing Proofs for Propositions in Section 3
Header_2: A. Missing Proofs for Propositions in Section 3
filename: 2302.03770.pdf

A. Missing Proofs for Propositions in Section 3
------------
Header_1: Missing Proofs for Propositions in Section 3
Header_2: A. Missing Proofs for Propositions in Section 3
Header_3: A.1 Proof of Proposition 3.1
filename: 2302.03770.pdf

A.1 Proof of Proposition 3.1

Proof. Since both \(d^*\) and \(d^*_\alpha\) satisfy the Bellman flow constraint (1), due to the optimality of \(d^*_\alpha\) in the regularized program (5), we have

$$
\begin{align*}
&E(s,g)\sim d^*(s,g)[r(s; g)] + \alpha D_f(d^* \| \mu) \leq E(s,g)\sim d^*_\alpha(s,g)[r(s; g)] + \alpha D_f(d^*_\alpha \| \mu) \\
\Rightarrow &E(s,g)\sim d^*(s,g)[r(s; g)] - E(s,g)\sim d^*_\alpha(s,g)[r(s; g)] \leq \alpha D_f(d^*_\alpha \| \mu) \leq \alpha(C^*_\alpha)^2/2.
\end{align*}
$$
\(J(\pi^*) - J(\pi^*_\alpha) = E(s,g)\sim d^*(s,g)[r(s; g)] - E(s,g)\sim d^*_\alpha(s,g)[r(s; g)] \leq O(\alpha(C^*_\alpha)^2)\).
------------
Header_1: Missing Proofs for Propositions in Section 3
Header_2: A. Missing Proofs for Propositions in Section 3
Header_3: A.2 Proof of Proposition 3.2
filename: 2302.03770.pdf

A.2 Proof of Proposition 3.2

Proof. By Lagrangian duality, we can obtain that the dual problem is

$$
\begin{align*}
&\min_{V(s;g)\geq 0} \max_{d(s,a;g)\geq 0} E(s,g)\sim d(s,g)[r(s; g)] - \alpha D_f(d(s, a; g) \| \mu(s, a; g)) \\
&+ \sum_{s,g} p(g)V(s; g) \left[ (1 - \gamma)\rho(s) + \gamma \sum_{s',a'} P(s|s', a')d(s', a'; g) - \sum_a d(s, a; g) \right]
\end{align*}
$$
where \(p(g)V(s; g)\) is the Lagrangian vector. Similar to Ma et al. [2022c],

$$
\begin{align*}
&\sum_{s,g} p(g)V(s; g) \sum_{s',a'} P(s|s', a')d(s', a'; g) \\
&= \sum_{s',a',g} p(g)d(s', a'; g) P(s|s', a')V(s; g)
\end{align*}
$$
Then the dual problem is equivalent to

$$
\min_{V(s;g)\geq 0} \max_{d(s,a;g)\geq 0} (1 - \gamma)E(s,g)\sim(\rho,p(g))[V(s; g)] + E(s,a,g)\sim d[r(s; g) + \gamma^T V(s, a; g) - V(s; g)]
$$
which can be further represented as

$$
\min_{V(s;g)\geq 0} (1 - \gamma)E(s,g)\sim(\rho,p(g))[V(s; g)] + \max_{d(s,a;g)\geq 0} E(s,a,g)\sim d[r(s; g) + \gamma^T V(s, a; g) - V(s; g)]
$$
where \(g = \alpha \cdot f\). Combining the constraint that \(d(s, a; g) \geq -D_g(d(s, a; g) \| \mu(s, a; g)), 0\) and the proof of Proposition 4.3 (Proposition B.2) of Ma et al. [2022c], the above program is equivalent to

$$
\min_{V(s;g)\geq 0} (1 - \gamma)E(s,g)\sim(\rho,p(g))[V(s; g)] + E(s,a,g)\sim \mu[1\{g' \geq \ast(AV(s, a; g)) \geq 0\} - \bar{g}^{\ast}(AV(s, a; g))],
$$
and it holds that

$$
d^*_\alpha(s, a; g) = \mu(s, a; g)g' + \ast(r(s; g) + \gamma^T V^*_\alpha(s, a; g) - V^*_\alpha(s; g)).
$$
------------
Header_1: Missing Proofs for Propositions in Section 3
Header_2: B. Proof for V-Learning
filename: 2302.03770.pdf

B. Proof for V-Learning

In this section, we provide theoretical analysis for the guarantees of V-learning in different settings (deterministic dynamics in Appendix B.1 and stochastic dynamics in Appendix B.2).

Recall that we choose \( f(x) = (x-1)^2 \), and thus \( f^*(x) = (x+1)^2 - 1 \). Therefore, we have

$$
\begin{align*}
g(x) &= \alpha f(x) = 2 \\
g^*(x) &= 2 - 2
\end{align*}
$$
Also, we define \( g^*_{\text{max}} = \max_{v\in[-V_{\text{max}},V_{\text{max}}+1]} g^*(v) \), \( g^*_{\text{min}} = \min_{v\in[-V_{\text{max}},V_{\text{max}}+1]} g^*(v) \), and \( g^*_{\Delta} = g^*_{\text{max}} - g^*_{\text{min}} \). We can easily obtain that \( g^*_{\Delta} \leq \alpha(1 + V_{\text{max}}/\alpha)^2 = O(V_{\text{max}}^2/\alpha) \).

For the purpose of theoretical analysis, we further define

$$
\begin{align*}
L_1(V) &= (1 - \gamma)E(s,g)\sim(\rho,p(g))[\alpha \cdot V(s;g)], \\
L_2(V) &= E(s,a,g)\sim\mu[\alpha \cdot g^*+(r(s;g) + \gamma T V(s,a;g) - V(s;g))], \\
\hat{N}_0 &= \alpha \cdot V(s_0,i;g_0,i), \\
L(d)_1(V) &= 1 - \hat{N}_0 \gamma \sum_{i=1}^{N} \alpha \cdot g^*+(r(s_i;g_i) + \gamma V(s'_i;g_i) - V(s_i;g_i)), \\
L(d)_2(V) &= \frac{1}{N} \sum_{i=1}^{N} \alpha \cdot g^*+(r(s_i;g_i) + \gamma V(s'_i;g_i) - V(s_i;g_i)).
\end{align*}
$$
Note that \( L_{\alpha}(V) = L_1(V) + L_2(V) \) and \( \hat{L}(d)(V) = \hat{L}(d)_1(V) + \hat{L}(d)_2(V) \).

B.1 Proof for Deterministic Dynamics

We first show that with high probability, the estimator \( \hat{L}(d)(V) \) in Algorithm 2 concentrates well on \( L_{\alpha}(V) \) for all \( V \in V \).

Lemma 3 (Concentration of \( \hat{L}(d)_1, \hat{L}(d)_2 \) ): Under Assumptions 1 and 2, when the dynamic of the environment is deterministic, with probability at least \( 1 - \log(|V|/\delta) \delta \), \( \forall V \in V \), it simultaneously holds that

$$
\begin{align*}
\hat{L}(d)_1(V) - L_1(V) &\leq O(\alpha V_{\text{max}}) \log(|V|/\delta) = O(V^2_{\text{max}} \log(|V|/\delta)), \\
\hat{L}(d)_2(V) - L_2(V) &\leq O(N) \log(|V|/\delta) = O(V_{\text{max}} N \log(|V|/\delta)),
\end{align*}
$$
which immediately implies

$$
L(d)(V) - L_{\alpha}(V) \leq O\left(\alpha V_{\text{max}} + \frac{V^2_{\text{max}}}{N}\right) \triangleq \epsilon_{\text{stat}}.
$$
Proof: First, fix any \( V \in V \). The expectation of \( \hat{L}(d)_1(V) \) is

$$
\begin{align*}
E[\hat{L}(d)_1(V)] &= E\left[\hat{N}_0 E(s_0,i,g_0,i)\sim(\rho,p(g))[\alpha \cdot V(s_0,i;g_0,i)]\right] \\
&= 1 - \hat{N}_0 \gamma \sum_{i=1}^{N} E(s,g)\sim(\rho,p(g))[\alpha \cdot V(s;g)] \\
&= L_1(V).
\end{align*}
$$
Also,

$$
\begin{align*}
E[\hat{L}(d)_2(V)] &= \frac{1}{N} \sum_{i=1}^{N} E(s,a,g)\sim\mu[\alpha \cdot g^*+(r(s_i;g_i) + \gamma V(s'_i;g_i) - V(s_i;g_i))] \\
&= L_2(V).
\end{align*}
$$
15

Also note that $$V(s_0,i; g_0,i) \in [0, V_{\text{max}}]$$ and $$g^*+(r(s_i; g_i) + \gamma T V(s_i, a_i; g_i) - V(s_i; g_i)) \in [0, g^*,\Delta]$$. By Hoeffding’s inequality and a union bound, we have with probability at least $$1 - \log(1/\delta)$$ $$\frac{L(d)}{\alpha V_{\text{max}}}$$, $$\hat{1}(V) - L_1(V) \le O N^0$$ $$\frac{L(d)}{\alpha \cdot g^*,\Delta \log(1/\delta)}$$. $$\hat{2}(V) - L_2(V) \le N$$. Applying a union bound over all $$V \in V$$ concludes the result. Next, we show that the value of $$L_\alpha$$ at the regularized optimal V-function $$V^*_\alpha$$ and the learned function $$\hat{V}$$ is close.

Lemma 4 (Closeness of the population objective of $$\hat{V}$$ and $$V^*_\alpha$$). Under Assumptions 1 and 2, with probability at least $$1 - \delta$$, we have $$L_\alpha(\hat{V}) - L_\alpha(V^*_\alpha) \le O(\epsilon_{\text{stat}})$$. Proof. We condition on the high probability event in Lemma 3. Note that $$L_\alpha(\hat{V}) - L_\alpha(V^*_\alpha) - \hat{L}(V) + \hat{L}(V) - \hat{L}(V) - L_\alpha(V^*_\alpha)$$.

(1), (3) $$\le O(\epsilon_{\text{stat}})$$ by Lemma 3 and (2) $$\le 0$$ by the definition of $$\hat{V}$$, which completes the proof. Now we define “semi-strong” convexity, which is used to prove Lemma 1 and might be of independent interests.

Definition 2 (Positive semi-strong convexity). For an arbitrary set $$X$$, assume $$F \subset \{f | f : X \to R\}$$ is a convex set. Let $$\rho \in \Delta(X)$$ be a probability distribution over $$X$$. The $$\| \cdot \|_{2,\rho}$$-norm of $$f \in F$$ is defined as $$\|f\|_{2,\rho} = E_{x \sim \rho}[f^2(x)]$$. Define function $$l : R \to R$$ and define functional $$L : F \to R$$ as $$L(f) = E_{x \sim \rho}[l(f(x))]$$. We say $$L$$ is $$\sigma$$-positive-semi-strongly convex w.r.t. $$f$$ in $$\| \cdot \|_{2,\rho}$$-norm for some $$\sigma > 0$$ if $$L(f) - \sigma^2 \|f^+\|_{2,\rho}$$.

Note that the common $$\sigma$$-strongly convex definition requires $$L(f) - \sigma^2 \|f\|_{2,\rho}$$ to be convex. Now we prove the following property for $$\sigma$$-positive-semi-strong convexity.

Proposition B.1. Under the same setting of Definition 2, if $$L$$ is $$\sigma$$-positive-semi-strongly convex w.r.t. $$f$$ in $$\| \cdot \|_{2,\rho}$$-norm, it holds that for any $$f, g \in F$$, $$L(f) - L(g) \ge \nabla L(g)^T(f - g) + \sigma^2 \|f^+ - g^+\|_{2,\rho}$$. Proof. By Definition 2, $$\tilde{L}(f) = L(f) - \sigma^2$$ is convex, which, by the definition of convex, implies that $$\tilde{L}(f) - \tilde{L}(g) \ge \nabla \tilde{L}(g)^T(f - g)$$. Plugging in the definition of $$\tilde{L}(\cdot)$$, we have $$L(f) - \sigma^2 \|f^+\|_{2,\rho} - L(g) + \sigma^2 \|g^+\|_{2,\rho} \ge \nabla L(g)^T(f - g) - \sigma E_{x \sim \rho}[g(x) + (f(x) - g(x))]$$.
------------
Header_1: Math Equations and Text
filename: 2302.03770.pdf

Math Equations and Text

Rearranging, we can obtain that

$$
L(f) - L(g) \geq \nabla L(g)^T(f - g) - \sigma E_{x \sim \rho}[g(x)+(f(x) - g(x))] + \sigma^2 \|f^+\|^2_{2,\rho} - \sigma^2 \|g^+\|^2_{2,\rho}
$$
$$
= \nabla L(g)^T(f - g) + \sigma E_{x \sim \rho}[f^2_+(x) - 2g(x)+(f(x) - g(x))]
$$
$$
= \nabla L(g)^T(f - g) + \sigma E_{x \sim \rho}[f^2_+(x) - 2g(x)+(f(x) - g(x))]
$$
$$
= \nabla L(g)^T(f - g) + \sigma^2 \|f^+ - g^+\|^2_{2,\rho}
$$
It suffices to show that

$$
E_{x \sim \rho}[g^+(x)(f^+(x) - f(x))] \geq 0, \quad E_{x \sim \rho}[g^+(x)(g(x) - g^+(x))] \geq 0.
$$
Note that for any \(x \in X\), \(g^+(x) \geq 0\) and \(f^+(x) \geq f(x)\), which implies \(g^+(x)(f^+(x) - f(x)) \geq 0\.

Also, note that when \(g(x) \geq 0\), we have \(g(x) - g^+(x) = 0\), and when \(g(x) < 0\), we have \(g^+(x) = 0\,

which means \(g^+(x)(g(x) - g^+(x)) \equiv 0\). Therefore, we have

$$
E_{x \sim \rho}[g^+(x)(f^+(x) - f(x))] \geq 0, \quad E_{x \sim \rho}[g^+(x)(g(x) - g^+(x))] = 0,
$$
Finally, by semi-strong convexity of \(L_\alpha\) w.r.t. UV in \(\| \cdot \|_2, \mu\)-norm, we can show that \(\hat{U}\) and \(U^*_\alpha\) are also close.

Proof of Lemma 1. We condition on the high probability event in Lemma 3. Recall that

$$
UV(s, a; g) = r(s; g) + \gamma T V(s, a; g) - V(s; g) + \alpha \quad \text{and in deterministic dynamics,} \quad \hat{U} = U \hat{V}.
$$
Let \(U = \{UV : V \in \mathbb{R}^{|S| \times |G|}\} \subseteq \mathbb{R}^{|S| \times |A| \times |G|}\) which is a convex set by definition. Also, since \(UV\) is linear in \(V\), we can also obtain that \(V\) can be linearly represented by \(U\). Therefore, we can define that \(\tilde{L}_\alpha(UV) = L_\alpha(V)\) and \(\tilde{L}_\alpha(UV) - \frac{1}{2} E(s,a,g) \sim \mu[U^2]\) is linear and thus convex in \(UV\), which implies that \(\tilde{L}_\alpha(UV)\) is 1-positive-semi-strongly convex w.r.t. UV and \(\| \cdot \|_2, \mu\). Then, by Proposition B.1, we have

$$
\tilde{L}_\alpha(\hat{U}) - \tilde{L}_\alpha(U^*_\alpha) \geq \nabla \tilde{L}_\alpha(U^*_\alpha)^T(\hat{U} - U^*_\alpha) + \frac{1}{2} \| \hat{U} - U^*_\alpha \|^2_{2,\mu}.
$$
Since \(U^*_\alpha = \text{arg min}_{U \in U} \tilde{L}_\alpha(U)\), by the first-order optimality condition, it holds that

$$
\nabla \tilde{L}_\alpha(U^*_\alpha)^T(\hat{U} - U^*_\alpha) \geq 0.
$$
Combining Lemma 4, we have

$$
\| \hat{U} - U^*_\alpha \|^2_{2,\mu} \leq O(\sqrt{\epsilon_{\text{stat}}}).
$$
Proof for Stochastic Dynamics

In stochastic dynamic settings, we first learn the ground-truth transition model and then calculate \(\hat{U}\). The following lemma provides a theoretical guarantee for maximum likelihood estimation (MLE) that \(\hat{P}\) and \(P^*\) are close.

Lemma 5 (Convergence rate of MLE, Van de Geer [2000]). For any fixed \(\delta > 0\), with probability at least \(1 - \delta\), we have

$$
E(s,a,g) \sim \mu \| \hat{P}(\cdot|s, a) - P^*(\cdot|s, a) \|^2_{\text{TV}} \lesssim \log(|P|/\delta),
$$
where \(\hat{P}\) is defined as in (11). This immediately implies that

$$
E(s,a,g) \sim \mu \| P\hat{P}(\cdot|s, a) - P^*(\cdot|s, a) \|^{\text{TV}} \lesssim \log(|P|/\delta).
$$

Equipped with Lemma 5, we can guarantee that for any \( V \), the population objective \( L_{\alpha}(V) \) is close to the empirical objective \( \hat{L}(s)(V) \), which is presented in Lemma 6.

$$
\text{Lemma 6 (Concentration of } \hat{L}(s) \text{). Under Assumptions 1 to 3, with probability at least } 1 - \delta, \forall V \in V, \text{ it holds that}
$$

$$
\hat{L}(s)(V) - L_{\alpha}(V) \leq O\left(\alpha V_{\text{max}} N_0 + V_{\text{max}}^2 N + \gamma V_{\text{max}}^2 N\right)_{\text{stat}}
$$

\[
\text{Proof. For convenience, define}
\]

\[
\begin{align*}
\hat{N}_0 &= \alpha \cdot V(s_0,i; g_0,i), \\
\hat{L}_1(V) &= 1 - \frac{N_0}{N\gamma} \sum_{i=1}^{N} \left(\alpha \cdot g^* + (r(s_i; g_i) + \gamma \hat{T}V(s_i, a_i; g_i) - V(s_i; g_i))\right), \\
\hat{L}_2(V) &= \frac{1}{N} \sum_{i=1}^{N} \left(\alpha \cdot g^* + (r(s_i; g_i) + \gamma \hat{T}V(s_i, a_i; g_i) - V(s_i; g_i))\right).
\end{align*}
\]

Note that for all \( V \in V \), \( \hat{L}_1(V) = \hat{L}_1(V) \) and thus by Lemma 3 we have that with probability at least \( 1 - \delta \), for

$$
\hat{L}(s) \leq \left(\alpha V_{\text{max}}\right)_{\text{stat}}. \quad (16)
$$

Now define

$$
\tilde{L}_1(V) - \hat{L}_1(V) \leq O\left(\frac{N_0}{N} \hat{T}V(s, a; g) - V(s; g)\right)
$$

Since \( E(s,a,g) \sim \mu[\hat{L}(s)] = \hat{L}(s) \), for all \( V \in V \), it holds that \( \hat{L}_2(V) = \hat{L}_2(V) \), by Lemma 3, we have that with probability \( 1 - \delta \), for any

$$
\left|\hat{L}_2(V) - \hat{L}_2(V)\right| \leq O\left(\frac{V_{\text{max}}^2}{N}\right). \quad (17)
$$

Also, note that

\[
\begin{align*}
\left|\hat{L}_2(V) - \hat{L}_2(V)\right| &= \alpha E(s,a,g) \sim \mu\left[g^* + (r(s; g) + \gamma \hat{T}V(s, a; g) - V(s; g)) - g^* + (r(s; g) + \gamma \hat{T}V(s, a; g) - V(s; g))\right] \\
&= \frac{1}{2} E(s,a,g) \sim \mu\left[(r(s; g) + \gamma \hat{T}V(s, a; g) - V(s; g) + \alpha)^2 - (r(s; g) + \gamma \hat{T}V(s, a; g) - V(s; g) + \alpha)^2\right] \\
&\lesssim V_{\text{max}} E(s,a,g) \sim \mu[\gamma \cdot \left|(T - \hat{T})V(s, a; g)\right|] \\
&= V_{\text{max}} E(s,a,g) \sim \mu[\gamma \cdot \left|E_{s' \sim P^*(\cdot|s,a)}[V(s'; g)] - E_{s' \sim \hat{P}(\cdot|s,a)}[V(s'; g)]\right|] \\
&\lesssim \gamma V_{\text{max}^2} E(s,a,g) \sim \mu[\|P^*(\cdot|s, a) - \hat{P}(\cdot|s, a)\|_{TV}].
\end{align*}
\]

By Lemma 5, with probability at least \( 1 - \delta \), for all \( V \in V \),

$$
\left|\hat{L}_2(V) - \hat{L}_2(V)\right| \lesssim \gamma V_{\text{max}}^2 \log(|P|/\delta). \quad (18)
$$

By rescaling \( \delta \) and applying a union bound, we can obtain that with probability at least \( 1 - \delta \), (16), (17), (18) hold simultaneously. The conclusion holds by applying the triangle inequality.

Finally, we show that \( \hat{U}^+ \) and \( U^*_{\alpha^+} \) are close.

Lemma 7 (Closeness of $$\hat{U}^+$$ and $$U^*_{\alpha^+}$$ in stochastic dynamics). Under Assumptions 1 to 3, with probability at least 1 - $$\delta$$, $$\| \hat{U}^+ - U^*_{\alpha^+} \|_{2,\mu} \leq O\left( \log(|V||P|/\delta) \right)$$ where $$\hat{U}$$ is the output of Algorithm 3, $$\alpha = \Omega\left( \sqrt{N} \right)$$ and $$\epsilon_{\text{stochastic}} \approx V^2 \log(|V||P|/\delta)/\max(N)$$.

Proof. For convenience, let $$\tilde{U} = U \hat{V}$$. Following the same analysis as in the deterministic case, we have $$\|U^* - U^+\|_{2,\mu} \leq O(\epsilon_{\text{stochastic}})$$ by Lemma 6. Also,

$$
\begin{aligned}
&\alpha^+ - \tilde{\alpha}^+ \\
&\| \tilde{U}^+ - \hat{U}^+ \|_{2,\mu} \leq \| \tilde{U} \|_{2,\mu}^2 \\
&= E_{(s,a,g) \sim \mu} \left[ \gamma E_{s' \sim P^*(\cdot|s,a)}[ \hat{V}(s;g) ] - \gamma E_{s' \sim P^{\hat{V}}(\cdot|s,a)}[ \hat{V}(s;g) ] \right]^2 \\
&\lesssim \gamma^2 V^2 \max_{E(s,a,g) \sim \mu} \| P^{\hat{V}}(\cdot|s,a) - P^*(\cdot|s,a) \|_2^{TV} \\
&\lesssim \gamma^2 V^2 \max \left( \log(|P|/\delta), N \right),
\end{aligned}
$$

where the last inequality holds by Lemma 5. Therefore,

$$
\begin{aligned}
\| \hat{U}^+ - U^* \|_{\alpha^+, \mu} &\leq \| \tilde{U}^+ - \alpha^+ \|_{2,\mu} + \| \tilde{U}^+ \|_{2,\mu} \\
&\lesssim \epsilon_{\text{stochastic}} + \gamma V \max \left( \log(|P|/\delta), N \right) \\
&\lesssim \epsilon_{\text{stochastic}}.
\end{aligned}
$$

Proof for Policy Learning

We provide a theoretical analysis of policy learning in this section. We first show two key lemmas in Appendix C.1, and then provide missing proofs of the main text in Appendix C.2 and Appendix C.3.

C.1 Key Lemmas

Lemma 8 (Statistical error of the weighted MLE objective). Under Assumption 5, with probability at least 1 - $$\delta$$, for any policy $$\pi \in \Pi$$, it holds that

$$
\left| L_{\text{MLE}}(\pi) - \hat{L}_{\text{MLE}}(\pi) \right| \leq (\epsilon_U + \epsilon_{\Pi}) \log(1/\tau) \triangleq \epsilon_{\text{MLE}}^{\text{stat}},
$$

where $$\epsilon_U \triangleq O(\epsilon_{\text{stat}}/\alpha^2)$$ and $$\epsilon_{\Pi} \triangleq O\left( C^* \log(|\Pi|/\delta) + V_{\text{max}}/\alpha + \alpha N \log(|\Pi|/\delta) \right)$$.

Proof. For convenience, we assume that the offline dataset used in policy learning is independent of the dataset used in V-Learning. This can be easily achieved by splitting the original dataset D uniformly at random into two datasets of equal size. Also, for analysis, we define

$$
\hat{L}_{\text{MLE}}(\pi) = E_{(s,a,g) \sim \mu} \left[ \alpha \log \pi(a|s,g) \right], \quad \forall \pi \in \Pi.
$$

Now we condition on the high probability event in Lemma 3, which we denote by E1. Then by Lemma 1, we have $$U^+\hat{U}^*_{\alpha^+} - \alpha^+ \leq \epsilon_U$$. By Cauchy-Schwarz inequality, we can obtain that for

$$
\begin{align*}
&|LMLE(\pi) - \tilde{\alpha} LMLE(\pi)| \leq E(s,a,g) \sim \mu \left| \alpha LMLE(\pi) \right| \leq \alpha |log \pi(a|s, g)| \\
&\leq U+\alpha - U^*\alpha^+ + ||log \pi(a|s, g)||_2,\mu \\
&\leq \epsilon U log(1/\tau).
\end{align*}
$$

Also, for any fixed $\pi \in \Pi$, since $E(s,a,g) \sim \mu[\hat{LMLE}(\pi)] = \tilde{LMLE}(\pi)$, we can obtain by Bernstein’s inequality that with probability at least $1 - \delta$,
$$
\begin{aligned}
&|\tilde{LMLE}(\pi) - \hat{LMLE}(\pi)| \\
&\leq O \left( \frac{Var_{\mu} U(s,a;g)+ log \pi(a|s, g)}{\alpha U^+/\alpha \infty log(1/\tau) + ||\hat{LMLE}(\pi) log(1/\delta)||} \right).
\end{aligned}
$$

Since $U^*\alpha(s, a; g)+/\alpha = d^* \alpha(s, a; g)/\mu(s, a; g) \leq C^* \alpha$, we have $||U^*\alpha^+/\alpha||_2,\mu \leq C^* \alpha$ and thus $||\hat{U}^+/\alpha||_2,\mu \leq ||U^*\alpha^+/\alpha||_2,\mu + \epsilon U \leq O(C^* \alpha)$. Also,
$$
\begin{aligned}
&Var_{\mu} U(s, a; g)+ \alpha log \pi(a|s, g) \\
&\leq E_{\mu} \left[ U(s, a; g)+ \alpha log^2 \pi(a|s, g) \right] \\
&\leq O((C^* \alpha)^2 log^2(1/\tau)).
\end{aligned}
$$

Applying a union bound over all $\pi \in \Pi$, it holds that with probability at least $1 - \delta$, for all $\pi \in \Pi$,
$$
\begin{aligned}
&|\tilde{LMLE}(\pi) - \hat{LMLE}(\pi)| \\
&\leq O(C^* \alpha log(1/\tau) log(|\Pi|/\delta) + V_{max} log(1/\tau) log(|\Pi|/\delta)) \\
&= \epsilon \Pi log(1/\tau).
\end{aligned}
$$

We denote the above event by $E2$. When $E1$ and $E2$ hold simultaneously, we have by the triangle inequality that
$$
|LMLE(\pi) - \hat{LMLE}(\pi)| \leq |LMLE(\pi) - \tilde{LMLE}(\pi)| + |\tilde{LMLE}(\pi) - \hat{LMLE}(\pi)| \leq (\epsilon U + \epsilon \Pi) log(1/\tau).
$$

$$
\begin{aligned}
&P(\neg(E1 \cap E2)) = P(\neg E1 \cup \neg E2) \\
&\leq P(\neg E1) + P(\neg E2) \\
&\leq \delta + P(E1)P(\neg E2|E1) + P(\neg E1)P(\neg E2|\neg E1) \\
&\leq \delta + 1 \times \delta + \delta \times 1 \leq 3\delta.
\end{aligned}
$$

Lemma 9 (Closeness of MLE objective of $\pi^*$ and $\hat{\pi}$). Under Assumption 4, with probability at least $1 - \delta$,
$$
LMLE(\pi^*) - LMLE(\hat{\pi}) \leq O(\epsilon_{MLE} - LMLE_{stat}).
$$

Proof. We condition on the high probability event in Lemma 8. Note that
$$
\begin{aligned}
&LMLE(\pi^*) - LMLE(\hat{\pi}) \\
&= LMLE(\pi^*) - LMLE(\pi^*) + \hat{LMLE}(\pi^*) - LMLE(\hat{\pi}) + \hat{LMLE}(\hat{\pi}) - LMLE(\hat{\pi}).
\end{aligned}
$$

$(1), (3) \leq O(\epsilon_{MLE} - LMLE_{stat})$ by Lemma 8 and $(2) \leq 0$ by the optimality of $\hat{\pi}$ which completes the proof.
------------
Header_1: Math Equations and Text
filename: 2302.03770.pdf

Math Equations and Text
------------
Header_1: Math Equations and Text
Header_2: C.2 Proof of Lemma 2
filename: 2302.03770.pdf

C.2 Proof of Lemma 2

Proof of Lemma 2. We condition on the high probability event in Lemma 8. Note that

$$
\begin{align*}
\mathcal{L}_{\text{MLE}}^{\alpha}(\pi) &= \mathbb{E}_{(s,a,g)\sim\mu}[g'^{*}(r(s;g) + \gamma T V^{*}_{\alpha}(s,a;g) - V^{*}_{\alpha}(s;g)) + \log\pi(a|s,g)] \\
\mathcal{L}_{\text{MLE}}^{\alpha,\text{Rel}}(\pi) &= \mathcal{L}_{\text{MLE}}^{\alpha}(\pi) - \mathbb{E}_{(s,a,g)\sim d^*}[\log\pi(a|s,g)],
\end{align*}
$$
We also define

$$
\begin{align*}
\mathcal{L}_{\text{MLE}}^{\alpha,\text{Rel}}(\pi) &= \mathcal{L}_{\text{MLE}}^{\alpha}(\pi) - \mathbb{E}_{(s,a,g)\sim d^*}[\log\pi(a|s,g)], \\
\mathcal{L}_{\text{MLE}}^{\alpha,\text{Rel}}(\pi^*) &= \mathbb{E}_{(s,a,g)\sim d^*_{\alpha}}[\log\pi^*_{\alpha}(a|s,g)] = \mathbb{E}_{(s,a,g)\sim d^*_{\alpha}}[\log\pi^*_{\alpha}(a|s,g)].
\end{align*}
$$
And note that $\mathcal{L}_{\text{MLE}}^{\alpha,\text{Rel}}$ is a constant shift of $\mathcal{L}_{\text{MLE}}^{\alpha}$. For any $\pi \in \Pi$, we further define $r_{\pi} = \frac{\pi}{\pi^*_{\alpha}}$. By Assumption 5, $r_{\pi}(a|s,g) \in [\tau, 1/\tau]$. Let $\tilde{\mathcal{L}}_{\text{MLE}}^{\alpha,\text{Rel}}(r_{\pi}) = \mathbb{E}_{(s,a,g)\sim d^*_{\alpha}}[\log r_{\pi}(a|s,g)]$ which is $2\tau$-strongly concave w.r.t. $\| \cdot \|_2, d^*_{\alpha}$ when $r_{\pi}(a|s,g) \in [\tau, 1/\tau]$. Since $\pi^*_{\alpha}$ is the maximizer of $\mathcal{L}_{\text{MLE}}^{\alpha}$ and thus the maximizer of $\mathcal{L}_{\text{MLE}}^{\alpha,\text{Rel}}$, we have that $r_{\pi^*_{\alpha}}$ is the maximizer of $\tilde{\mathcal{L}}_{\text{MLE}}^{\alpha,\text{Rel}}$. By strong concavity and the optimality of $r_{\pi^*_{\alpha}}$, we have

$$
\tau^2 \| r_{\pi^*_{\alpha}} - \hat{r}_{\pi} \|_2^2, d^*_{\alpha} \leq \tilde{\mathcal{L}}_{\text{MLE}}^{\alpha,\text{Rel}}(r_{\pi^*_{\alpha}}) - \tilde{\mathcal{L}}_{\text{MLE}}^{\alpha,\text{Rel}}(\hat{r}_{\pi}) = \mathcal{L}_{\text{MLE}}^{\alpha,\text{Rel}}(\pi^*_{\alpha}) - \mathcal{L}_{\text{MLE}}^{\alpha}(\hat{\pi}) \leq O(\epsilon_{\text{MLE}_{\text{stat}}}),
$$
where the last inequality holds by Lemma 9. Note that

$$
\begin{align*}
\| r_{\pi^*_{\alpha}} - \hat{r}_{\pi} \|_2^2, d^*_{\alpha} &= \mathbb{E}_{(s,a,g)\sim d^*_{\alpha}} \left[ \pi(a|s,g) - \frac{r_{\pi^*_{\alpha}}(a|s,g)}{\pi(a|s,g)} \right]^2 \\
&= \mathbb{E}_{(s,a,g)\sim d^*_{\alpha}} \left[ \pi^*_{\alpha}(a|s,g) - 1 \right]^2 \\
&\gtrapprox \mathbb{E}_{(s,g)\sim d^*_{\alpha}} \| \hat{\pi}(\cdot|s,g) - \pi^*_{\alpha}(\cdot|s,g) \|_2^{\text{TV}} \\
&\geq \mathbb{E}_{(s,g)\sim d^*_{\alpha}} [\| \hat{\pi}(\cdot|s,g) - \pi^*_{\alpha}(\cdot|s,g) \|_{\text{TV}}].
\end{align*}
$$
Where the last inequality holds since TV distance is upper bounded by $\chi^2$ distance. Therefore, we can finally obtain that

$$
\mathbb{E}_{(s,g)\sim d^*_{\alpha}} [\| \hat{\pi}(\cdot|s,g) - \pi^*_{\alpha}(\cdot|s,g) \|_{\text{TV}}] \leq O(\epsilon_{\text{MLE}_{\text{stat}}}/\tau^2).
$$
------------
Header_1: Math Equations and Text
Header_2: C.3 Proof of Theorem 1
filename: 2302.03770.pdf

C.3 Proof of Theorem 1

Proof of Theorem 1. We condition on the high probability event in Lemma 8. By performance difference lemma [Agarwal et al., 2019], we have

$$
\begin{align*}
J(\pi^*_{\alpha}) - J(\hat{\pi}) &= \mathbb{E}_{(s,a,g)\sim d^*_{\alpha}}[A\hat{\pi}(s,a;g)] \\
&= \mathbb{E}_{(s,g)\sim d^*_{\alpha}}[\mathbb{E}_{a\sim\pi^*_{\alpha}(\cdot|s,g)}[A\hat{\pi}(s,a;g) - \mathbb{E}_{a\sim\hat{\pi}(\cdot|s,g)}[A\hat{\pi}(s,a;g)]] \\
&\lesssim V_{\text{max}} \mathbb{E}_{(s,g)\sim d^*_{\alpha}}[\| \pi^*_{\alpha}(\cdot|s,g) - \hat{\pi}(\cdot|s,g) \|_{\ell_1}] \\
&\lesssim V_{\text{max}} \epsilon_{\text{MLE}_{\text{stat}}}/\tau^2,
\end{align*}
$$
where the last inequality holds by Lemma 2.
------------
Header_1: Math Equations and Text
Header_2: D Statistical Rate of the Suboptimality in Different Settings
filename: 2302.03770.pdf

D Statistical Rate of the Suboptimality in Different Settings

In this section, we analyze the statistical rate of the suboptimality of the output policy $\hat{\pi}$ by Algorithm 1 in different settings.

D.1 Deterministic Settings

Proof of Theorem 2. By Theorem 1 and Proposition 3.1,

$$
\begin{align*}
J(\pi^*) - J(\hat{\pi}) & = J(\pi^*) - J(\pi^*_{\alpha}) + J(\pi^*_{\alpha}) - J(\hat{\pi}) \\
& \lesssim \alpha(C^*_{\alpha})^2 + V_{\text{max}} \frac{\epsilon_{\text{MLE stat}}}{\tau^2} \\
& \lesssim \alpha(C^*_{\alpha})^2 + V_{\text{max}} \log(1/\tau) \frac{\epsilon_{\text{stat}}}{\alpha^2} + C^*_{\alpha} \log(|\Pi|/\delta) + V_{\text{max}}
\end{align*}
$$
Since $\epsilon_{\text{stat}} \approx V^2 \log(|V|/\delta) / \tau$, we can further obtain that $J(\pi^*) - J(\hat{\pi}) \lesssim \alpha(C^*_{\alpha})^2 + V_{\text{max}} \log(1/\tau) V_{\text{max}} \log(|V|/\delta)^{1/4} + C^*_{\alpha} \log(|\Pi|/\delta) + V_{\text{max}} \tau \alpha N \log(|\Pi|/\delta)$.

$$
\begin{align*}
J(\pi^*) - J(\hat{\pi}) & \lesssim \alpha(C^*_{\alpha})^2 + V_{\text{max}} \log(1/\tau) V_{\text{max}} \log(|V|/\delta)^{1/4} + C^*_{\alpha} \log(|\Pi|/\delta) + V_{\text{max}} \\
& \lesssim \alpha(C^*_{\alpha})^2 + V_{\text{max}} \log(1/\tau) V_{\text{max}} C^*_{\alpha} / \tau \alpha N^{1/4} \log(|V||\Pi|/\delta) \\
& \lesssim V^3_{\text{max}}(C^*_{\alpha})^3 \log(1/\tau) \log(|V||\Pi|/\delta)^{1/3} / \tau^2 N^{1/4} \\
& \lesssim V^3_{\text{max}} \log(1/\tau) \log(|V||\Pi|/\delta)^{1/3} / \tau^2 N^{1/4}
\end{align*}
$$
where $\alpha \approx \tau^2(C^*_{\alpha})^3 N^{1/4}$.

D.2 Stochastic Settings

Theorem 3 (Statistical rate of the suboptimality in stochastic settings). Under Assumptions 1 to 5, with probability at least $1 - \delta$, the output policy $\hat{\pi}$ by Algorithm 1 (with the choice of Algorithm 3 for $V$-learning in stochastic settings) satisfies

$$
J(\pi^*) - J(\hat{\pi}) \lesssim V^3_{\text{max}}(C^*_{\alpha})^3 \log(1/\tau) \log(|V||P||\Pi|/\delta)^{1/3} / \tau^2 N^{1/4}
$$
$$
V^3_{\text{max}} \log(1/\tau) \log(|V||P||\Pi|/\delta)^{1/3} / \tau^2 N^{1/4}
$$
if we choose $\alpha \approx \tau^2(C^*_{\alpha})^3 N^{1/4}$ and assume $N = N_0$.

Proof. The proof is identical to the proof of Theorem 2 except that we replace $\epsilon_{\text{stat}}$ with $\epsilon_{\text{stochastic}}$.
------------
Header_1: Effective Robustness against Natural Distribution Shifts
filename: 2302.01381.pdf

Effective Robustness against Natural Distribution Shifts
------------
Header_1: Effective Robustness against Natural Distribution Shifts for Models with Different Training Data
filename: 2302.01381.pdf

Effective Robustness against Natural Distribution Shifts for Models with Different Training Data

Zhouxing Shi* &emsp; Nicholas Carlini &emsp; Ananth Balashankar

UCLA &emsp; Google Research &emsp; Google Research

zshi@cs.ucla.edu &emsp; ncarlini@google.com &emsp; ananthbshankar@google.com

Ludwig Schmidt &emsp; Cho-Jui Hsieh

University of Washington &emsp; Google, UCLA

schmidt@cs.washington.edu &emsp; chohsieh@cs.ucla.edu

Alex Beutel* &emsp; Yao Qin

OpenAI &emsp; UCSB, Google Research

alexb@openai.com &emsp; yaoqin@ucsb.edu
------------
Header_1: Effective Robustness against Natural Distribution Shifts for Models with Different Training Data
Header_2: Abstract
filename: 2302.01381.pdf

Abstract

"Effective robustness" measures the extra out-of-distribution (OOD) robustness beyond what can be predicted from the in-distribution (ID) performance. Existing effective robustness evaluations typically use a single test set such as ImageNet to evaluate the ID accuracy. This becomes problematic when evaluating models trained on different data distributions, e.g., comparing models trained on ImageNet vs. zero-shot language-image pre-trained models trained on LAION. In this paper, we propose a new evaluation metric to evaluate and compare the effective robustness of models trained on different data. To do this, we control for the accuracy on multiple ID test sets that cover the training distributions for all the evaluated models. Our new evaluation metric provides a better estimate of effective robustness when there are models with different training data. It may also explain the surprising effective robustness gains of zero-shot CLIP-like models exhibited in prior works that used ImageNet as the only ID test set, while the gains diminish under our new evaluation. Additional artifacts including interactive visualizations are provided at https://shizhouxing.github.io/effective-robustness.
------------
Header_1: Effective Robustness against Natural Distribution Shifts for Models with Different Training Data
Header_2: 1 Introduction
filename: 2302.01381.pdf

1 Introduction

Robustness against distribution shifts is important for machine learning models to work reliably across various environments. For natural distribution shifts on image classification datasets, Taori et al. (2020) proposed the notion of effective robustness to control for in-distribution (ID) accuracy when evaluating out-of-distribution (OOD) accuracy. Following a long line of work that has found a strong correlation between ID and OOD accuracy on many test sets (Recht et al., 2019; Yadav & Bottou, 2019), effective robustness allows researchers to assess whether an apparently improved OOD accuracy is a result of effectively improved robustness or is simply an expected outcome of enhanced ID accuracy.

Unfortunately, the current definition of effective robustness has a subtle limitation: it requires a fixed ID test set, which is typically ImageNet (Deng et al., 2009) when using ImageNet-like OOD test sets. *Work done while at Google.

37th Conference on Neural Information Processing Systems (NeurIPS 2023).

in Taori et al. (2020) or CIFAR-10 (Krizhevsky et al., 2009) when using CIFAR-like OOD test sets
in Miller et al. (2021). It is acceptable when models are trained predominately on only one dataset.
However, the emergence of many large-scale models trained on significantly different datasets makes
it necessary to evaluate and compare models trained on different data distributions, under which it
becomes unclear which ID test set should be used.
In particular, models from Contrastive Language-Image Pre-training, such as CLIP (Radford et al.,
2021) and ALIGN (Jia et al., 2021) have recently exhibited unprecedented effective robustness gains
during zero-shot inference (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022). However
these previous works simply take ImageNet as the single ID test set, even though the models are not
trained on ImageNet. We demonstrate that the results of evaluating effective robustness using a single
ID test set can vary drastically depending on the selection of the ID test set. Therefore, this imprecise
treatment on the ID test set in existing works could end up exaggerating the effective robustness of
zero-shot CLIP models compared to models that are exactly trained on ImageNet.
In this paper, we propose to more precisely evaluate and compare the effective robustness of models
trained on different datasets. Instead of controlling for a single ID accuracy that may bias towards
models from a particular training distribution, we propose to use multiple ID test sets that cover the
training distributions of all the models. In particular, previous works performed single-dimensional
linear regression on a set of baseline models to predict OOD accuracy from a single ID accuracy (Taori
et al., 2020). And they then evaluate the actual OOD accuracy of the models beyond the expected
value that can be predicted from the fitting line, as the effective robustness. We expand on this
definition by allowing for multiple ID test sets, and perform multi-dimensional linear regression to fit
a plane to predict OOD accuracy from the accuracy on multiple ID test sets.

In summary, we make the following contributions:
- We reveal a limitation in the existing effective robustness evaluation when used to compare models
trained on different data distributions.
- We then propose a new effective robustness evaluation which uses multiple ID test sets to more
precisely compare the effective robustness of models trained on different data.
- We show that the OOD accuracy of various models including zero-shot CLIP models can usually
be better predicted from accuracies on multiple ID test sets compared to using only one ID test set.
- Our results provide new understandings on the effective robustness gains of CLIP-like models
observed in prior works only using ImageNet as the ID test set, while the gains diminish under our
new evaluation.
------------
Header_1: Effective Robustness against Natural Distribution Shifts for Models with Different Training Data
Header_2: Background of Effective Robustness
filename: 2302.01381.pdf

Background of Effective Robustness

Under natural distribution shifts, the OOD accuracy of a model is often correlated with the ID
accuracy. After applying a logit transformation on the accuracy, a linear trend between the transformed ID accuracy and OOD accuracy holds across many datasets (e.g., a distribution shift from
ImageNet (Deng et al., 2009) to ImageNetV2 (Recht et al., 2019), or from CIFAR-10 (Krizhevsky
et al., 2009) to CIFAR-10.2 (Hendrycks & Dietterich, 2018)) and models with various architectures
and training methods (Taori et al., 2020; Miller et al., 2021). This phenomenon implies that most
models showing higher OOD accuracies naturally resulted from better ID performance.

To eliminate the confounding effect of ID accuracy on OOD performance, Taori et al. (2020) proposed
effective robustness that measures the OOD performance beyond the expected OOD accuracy given
the ID accuracy, where the expected OOD accuracy is predicted according to the fitted linear trend
of baseline models. Since they only use a single ID test set, we refer to this version of effective
robustness as single-ID effective robustness.

Suppose there are n baseline models \( f_1, f_2, \ldots, f_n \). A baseline function \( \tilde{\beta}(x) \) is constructed to predict
the OOD accuracy of each baseline model, \( acc_{ood}(f_i) \) (\( 1 \leq i \leq n \)), given the single ID accuracy of
the model \( x = acc_{id}(f_i) \). The baseline function is instantiated as:

$$
\tilde{\beta}(x) = \text{expit}(w \text{logit}(x) + b) \quad (1)
$$

where \( w \) and \( b \) are parameters, \( \text{logit}(x) = \ln\left(\frac{1}{1-x}\right) \) is the logit transformation, and \( \text{expit}(x) \) is the
inverse of \( \text{logit}(x) \). Since \( \text{logit}(\tilde{\beta}(x)) = w \text{logit}(x) + b \), the baseline function is essentially a linear
------------
Header_1: Document
filename: 2302.01381.pdf

Document
------------
Header_1: Document
Header_2: ImageNet models
filename: 2302.01381.pdf

ImageNet models

| |80|ImageNet models|
|---|---|---|
|95|YFCC models|YFCC models|
| |70| |
------------
Header_1: Document
Header_2: Linear fit for ImageNet models
filename: 2302.01381.pdf

Linear fit for ImageNet models

80

70

50

25

10
------------
Header_1: Document
Header_2: Linear fit for YFCC models
filename: 2302.01381.pdf

Linear fit for YFCC models

50

25

10
------------
Header_1: Document
Header_2: ImageNet-V2 accuracy (css., %)
filename: 2302.01381.pdf

ImageNet-V2 accuracy (css., %)

10 25 50 70 80 90 95
------------
Header_1: Document
Header_2: ImageNet accuracy (css., %)
filename: 2302.01381.pdf

ImageNet accuracy (css., %)

YFCC accuracy (css., %)
------------
Header_1: Document
Header_2: ImageNet accuracy (css., %)
Header_3: (a) ImageNet-V2 accuracy against ImageNet accuracy.
filename: 2302.01381.pdf

(a) ImageNet-V2 accuracy against ImageNet accuracy.
------------
Header_1: Document
Header_2: ImageNet accuracy (css., %)
Header_3: (b) ImageNet-V2 accuracy against YFCC accuracy.
filename: 2302.01381.pdf

(b) ImageNet-V2 accuracy against YFCC accuracy.

Figure 1: Class-subsampled ("css." for short) ImageNet-V2 accuracy against ImageNet accuracy and YFCC accuracy, respectively, for 36 ImageNet models and 13 YFCC models that are also used in Table 3a. A linear fit is generated for ImageNet models and YFCC-15M models, respectively. Accuracies and linear fits are under the logit scale. Class-subsampling is used to only include classes that appear in all the involved test sets (see Section 5.1).

Function after applying a logit transformation on the accuracies, and it can be determined by solving a linear regression. Then the single-ID effective robustness of a model f is evaluated as:

$$\tilde{\rho}(f) = acc_{ood}(f) - \beta\tilde{}(acc_{id}(f))$$

from the actual OOD accuracy $acc_{ood}(f)$.
------------
Header_1: Document
Header_2: ImageNet accuracy (css., %)
Header_3: Limitation of the Single ID Test Set
filename: 2302.01381.pdf

Limitation of the Single ID Test Set

The existing effective robustness evaluation in Section 2 fixes a single ID test set for all the models, which is reflective of the ID performance only if all the models are trained on the same dataset that matches the ID test set. However, as large-scale pre-trained models emerge, it becomes necessary to compare models trained on different datasets, in order to know if the latest pre-training techniques can yield effective robustness gains. In this section, we use the comparison between zero-shot CLIP models and standard ImageNet models as an example to show the limitation of using a single ID test set: when only one ID test set is used, using different ID test sets leads to contradictory conclusions. Following Fang et al. (2022), we compare models trained on ImageNet (Deng et al., 2009) and YFCC-15M (Thomee et al., 2016), respectively. On ImageNet, we include standard classifiers, and we also train CLIP models using templates filled with an ImageNet class name as the caption in a format of "A photo of a {class name}". We also train CLIP models on YFCC-15M, a dataset with image-text pairs. And we use ImageNet-V2 (Recht et al., 2019) as the OOD test set. We consider two different ID test sets. One ID test set is simply ImageNet. The other ID test set is constructed from YFCC-15M, since we have CLIP models trained on YFCC. We refer to this test set as "YFCC test set", and we refer to the accuracy on this test set as "YFCC accuracy". We discuss its details in Section 5.1 and Appendix B.2. Both ID test sets we consider here match the training distribution of some of the models (ImageNet models and YFCC models respectively) but not all the models.

We then plot the ImageNet-V2 accuracy of the models against their ImageNet accuracy and YFCC accuracy, respectively. There is a strong linear trend between the scaled ID accuracy and OOD accuracy for ImageNet models and YFCC models, respectively, and we plot fitting lines for these two sets of models, respectively. When the ID test set is ImageNet, Fig. 1a shows that the fitting line for YFCC models is generally above the fitting line for ImageNet models (except for the regime with extremely low accuracies), which appears to suggest that YFCC models have effective robustness gains over ImageNet models, as also suggested in Fang et al. (2022). However, in Fig. 1b which uses YFCC as the ID test set, the fitting line of ImageNet models are now mostly above YFCC models, which instead appears to suggest that ImageNet models have greater effective robustness than YFCC models. We observe that when there is a mismatch in the training data and the ID test data.

models appear to have greater effective robustness (YFCC models in Figure 1a and ImageNet models
in Figure 1b), as their performance on the ID test data and the OOD performance predicted from the
single ID accuracy tend to be lower. This makes it difficult to compare models trained on different
data and leads to imprecise conclusions on effective robustness if only one ID test set is used.
------------
Header_1: Document
Header_2: ImageNet accuracy (css., %)
Header_3: Limitation of the Single ID Test Set
Header_4: Multi-ID Effective Robustness
filename: 2302.01381.pdf

Multi-ID Effective Robustness

Considering the limitations of using a single ID test set, we propose a new way for effective robustness
evaluation using multiple ID test sets that cover the training data distributions of all the involved
models. We name it multi-ID effective robustness. Specifically, for each training distribution, we
propose to prepare an ID test set that matches the training distribution, respectively. In particular, we
focus on comparing models trained on two different datasets at a time in this paper, and we thereby
use two ID test sets, where each of them corresponds to one of the training datasets.

While we refer to them as ID test sets, each of them is only the exact ID test set for some of the
considered models that are trained on the distribution matching the test set, and it is not exactly an
ID test set for all the considered models. However, we assume that the training distributions of all
the models are still relatively close compared to the OOD test distributions (e.g., images normally
collected from social medias in ImageNet (Deng et al., 2009), YFCC (Thomee et al., 2016), and
LAION (Schuhmann et al., 2021) are relatively close compared to the OOD images in ImageNet-
Sketch (Wang et al., 2019) that consists of sketch images specifically). In this way, both ID test sets
are relatively ID for all the models compared to the OOD test sets, and it can be meaningful to control
for the performance on these ID test sets when comparing the OOD performance.

We still use $$acc_{ood}(·)$$ to denote the OOD accuracy, and we use $$acc_1(·)$$ and $$acc_2(·)$$ to denote the
accuracy on the two ID test sets, respectively. In contrast to the previous baseline function $$\tilde{\beta}(x)$$ in
Eq. (1), we propose a new baseline function $$\beta(x, y)$$ that predicts the OOD accuracy based on the
accuracies x and y on the two ID test sets, respectively.

All the models in Figure 1 are trained on either ImageNet or YFCC. Thus, to compare their effective ImageNet Models YFCC Models
robustness under our new evaluation, we use two ID
test sets for ImageNet and YFCC at the same time, in 1
contrast to Figure 1a and 1b which use one ID test set
separately at each time and results on the two different
ID test sets lead to contradictory conclusions. As
shown in Figure 2, we plot the OOD accuracy against
the two ID accuracies on both two ID test sets in a 3D
space. We observe that the data points approximately
lie on a plane when plotted on the logit scale. This motivates us to instantiate $$\beta(x, y)$$ as:

$$\beta(x, y) = expit(wx logit(x) + wy logit(y) + b),$$

where wx, wy, b are parameters. $$\beta(x, y)$$, which is the plane in Figure 2, is also a linear function w.r.t. x and
y under the logit scale, and thus it is a reasonable extension from $$\tilde{\beta}(x)$$ by using a multi-dimensional linear function on the logit scale. We determine the parameters by solving an ordinary least squares regression to fit the accuracies. Metrics for linear regression
such as the coefficient of determination, a.k.a. R2,
can be used to evaluate the fitting quality of the baseline function. A high R2 value indicates that
the OOD accuracy is accurately predicted by the baseline function from the ID accuracies, and thus
the evaluated models have similar effective robustness. And our multi-ID effective robustness for a
model f is defined as

$$\rho(f) = acc_{ood}(f) - \beta(acc_1(f), acc_2(f)).$$

Compared to the existing definition for effective robustness in Eq. (2), the major difference is the
inclusion of two ID accuracies $$acc_1(f)$$ and $$acc_2(f)$$ in the baseline function, compared to using a
single ID accuracy $$acc_{id}(f)$$.
------------
Header_1: Generalizing to more than two training datasets
filename: 2302.01381.pdf

Generalizing to more than two training datasets
------------
Header_1: Generalizing to more than two training datasets
Header_2: Generalizing to more than two training datasets
filename: 2302.01381.pdf

Generalizing to more than two training datasets

Although we focus on handling two training datasets at a time, our method may be generalized to more than two datasets in principle, by defining a baseline function based on multiple ID accuracies $acc_1(\cdot), \ldots, acc_k(\cdot)$. However, it could be costly as it would require training more models to fit a high-quality baseline function. We leave it for future works to reduce the cost when dealing with a larger number of datasets.
------------
Header_1: Generalizing to more than two training datasets
Header_2: Generalizing to more than two training datasets
Header_3: Experiments
filename: 2302.01381.pdf

Experiments
------------
Header_1: Generalizing to more than two training datasets
Header_2: Generalizing to more than two training datasets
Header_3: Experiments
Header_4: Settings
filename: 2302.01381.pdf

Settings

Models: In order to fit a baseline function, we need a large amount of models with diverse accuracies. To this end, we follow Taori et al. (2020) to train models with various proportions of data by subsampling from the entire training set (namely dataset subsampling), which effectively produces models with diverse accuracies. Moreover, we also combine examples from two datasets with different sampling ratios and train models on these combined datasets. This produces models with training distributions varying between the two training datasets and it is supposed to yield different combinations of the two ID accuracies. We use models trained on each single dataset as well as the combined datasets in the same fitting process, so that the baseline functions do not bias towards models trained on certain data. Our experiments include the following models:

- Standard classifiers on CIFAR-10 and ImageNet. We train standard classifiers on CIFAR-10 and ImageNet (Deng et al., 2009). We use ResNet-18, ResNet-50, and ResNet-101 (He et al., 2016). Additionally, we train models by combining CIFAR-10 and ImageNet at various ratios, where we upsample CIFAR-10 images from 32x32 to 224x224. Furthermore, we include ViT-S/16, ViT-B/16, ViT-L/16 models (Dosovitskiy et al., 2021) pre-trained on the whole ImageNet.
- CLIP models. On YFCC-15M (Thomee et al., 2016) and LAION-15M (Schuhmann et al., 2021) which consist of image-text pairs, we train CLIP models using ResNet-50 and ResNet-101. We also train models by combining ImageNet and YFCC-15M and LAION-15M, respectively. We discard models with ImageNet accuracy below 5%. Additionally, in Section 5.4, we also have downloaded ViT-based models from Mu et al. (2022); Ilharco et al. (2021) and CLIP models fine-tuned on ImageNet, which are only used for evaluation but not fitting the baseline functions. We provide additional details in Appendix B.1.

We use "{Name_of_dataset} models" to denote models trained only on the dataset, e.g., "CIFAR-10 models". And we use "{Name_of_dataset_A}+{Name_of_dataset_B} models" to represent models trained on a combination of two datasets, e.g., "CIFAR-10+ImageNet models".

ID test sets: We focus on image classification. Labeled image classification datasets such as ImageNet can be directly used for evaluating ID accuracy. For datasets that consist of image-text pairs for language-image pre-training without original labels, including YFCC and LAION, we automatically generate classification labels by matching captions with ImageNet classes, which has been similarly performed in Fang et al. (2022) for training classifiers using caption data, and we then hold out a balanced test set from the original dataset. More details are reported in Appendix B.2. Although it is possible to obtain a higher-quality test set by human labeling, we will show that using the automatically labeled test sets can already produce reasonable results.

OOD test sets: To compare the effectiveness robustness of models trained on CIFAR-10 and ImageNet, we use 3 CIFAR-like OOD test sets with natural distribution shifts, including CIFAR-10.1 (Recht et al., 2019), CIFAR-10.2 (Lu et al., 2020), and CINIC-10 (Darlow et al., 2018). We use 4 ImageNet-like OOD test sets to compare models trained on ImageNet with models trained on YFCC and LAION: ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019), and ObjectNet (Barbu et al., 2019). We do not use ImageNet-A (Hendrycks et al., 2021b) which involves adversarial filtering and has a different behavior in effective robustness evaluation (Taori et al., 2020; Fang et al., 2022).

Class subsampling and mapping: Considering that different test sets may not have the same classes, we follow prior works (Taori et al., 2020; Fang et al., 2022) to use class subsampling to

We reuse the term "class subsampling" from prior works (Taori et al., 2020; Fang et al., 2022), although it is not a random sampling.
------------
Header_1: Document
filename: 2302.01381.pdf

Document
------------
Header_1: Document
Header_2: CIFAR-10 Models
filename: 2302.01381.pdf

CIFAR-10 Models
------------
Header_1: Document
Header_2: ImageNet Models
filename: 2302.01381.pdf

ImageNet Models
------------
Header_1: Document
Header_2: CIFAR-10+ImageNet Models
filename: 2302.01381.pdf

CIFAR-10+ImageNet Models
------------
Header_1: Document
Header_2: ImageNet Models
filename: 2302.01381.pdf

ImageNet Models
------------
Header_1: Document
Header_2: YFCC Models
filename: 2302.01381.pdf

YFCC Models
------------
Header_1: Document
Header_2: ImageNet+YFCC Models
filename: 2302.01381.pdf

ImageNet+YFCC Models

1. Cipa4-I0

(a) Using CIFAR-10.2 as the OOD test set. The ImageNet accuracy is mapped to CIFAR-10 classes (see Section 5.1).

(b) Using ImageNet-R as the OOD test set.

Figure 3: Visualization of the multi-ID effective robustness. The colored plane stands for the baseline function. Figure 4 and Figure 5 (in Appendix A.1) show various projected 2D views. See our website (https://shizhouxing.github.io/effective-robustness) for an interactive visualization.

Retain classes appearing in all the test sets. We also follow Miller et al. (2021) to map a subset of ImageNet classes to CIFAR-10 classes when comparing CIFAR-10 models and ImageNet models.
------------
Header_1: Document
Header_2: ImageNet+YFCC Models
Header_3: 5.2 Evaluation on CIFAR-like OOD Test Sets
filename: 2302.01381.pdf

5.2 Evaluation on CIFAR-like OOD Test Sets

| |CIFAR-10|ImageNet|
|---|---|---|
|95|CIFAR-10|ImageNet|
|90|CIFAR-10+ImageNet|CIFAR-10+ImageNet|
|80| | |
|70| | |
|50| | |
|25| | |
|CIFAR-10.2 accuracy (%)| | |

(a) CIFAR-10.2 accuracy against CIFAR-10 accuracy. ImageNet models have higher CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for CIFAR-10 accuracy only.

(b) CIFAR-10.2 accuracy against ImageNet accuracy. ImageNet models have lower CIFAR-10.2 accuracy compared to CIFAR-10 models when controlling for ImageNet accuracy only.

Figure 4: Projected views of Figure 3a. Figure 4a and Figure 4b correspond to single-ID evaluations using different ID test sets and yield contradictory conclusions on the effective robustness. Our multi-ID evaluation provides a more holistic view where all these models are approximately on a same plane and thus have similar effective robustness.

We first experiment with models trained using CIFAR-10 and ImageNet on CIFAR-like OOD test sets. We show the fitting quality in Table 1a and the effective robustness of various models in Table 1b.

Compared to the single-ID evaluation, our multi-ID evaluation achieves a better fitting quality and predicts the OOD accuracy from the ID accuracies more precisely (higher R2 and lower MAE), and thus provides a more precise understanding on the effective robustness. Specifically, while both single-ID effective robustness and multi-ID effective robustness have relatively high fitting quality on CIFAR-like test sets, using multi-ID effective robustness further improves the fitting quality. In terms of the effective robustness, under the single-ID evaluation, ImageNet models achieve $$3.91\pm2.20\%$$ and $$2.77\pm1.25\%$$ effective robustness on CIFAR-10.2 and CINIC-10, respectively.
------------
Header_1: Results on CIFAR-like OOD test sets
filename: 2302.01381.pdf

Results on CIFAR-like OOD test sets
------------
Header_1: Results on CIFAR-like OOD test sets
Header_2: Results on CIFAR-like OOD test sets
filename: 2302.01381.pdf

Results on CIFAR-like OOD test sets

148 models are included, including CIFAR-10 models, ImageNet models, and CIFAR-10+ImageNet models (CIFAR+IN for short). The multi-ID evaluation achieves better fitting quality where the effective robustness values of CIFAR-10 models and ImageNet models also become closer to 0.
------------
Header_1: Results on CIFAR-like OOD test sets
Header_2: Results on CIFAR-like OOD test sets
Header_3: (a) Fitting quality evaluated by R2 and mean absolute error (MAE)
filename: 2302.01381.pdf

(a) Fitting quality evaluated by R2 and mean absolute error (MAE)

|Test set|R2 (↑)|MAE (% ↓)|
|---|---|---|
|CIFAR-10.1|0.996|0.997|1.07|0.93|
|CIFAR-10.2|0.981|0.996|2.22|0.95|
|CINIC-10|0.978|0.990|2.41|1.49|
------------
Header_1: Results on CIFAR-like OOD test sets
Header_2: Results on CIFAR-like OOD test sets
Header_3: (b) Effective robustness values (%)
filename: 2302.01381.pdf

(b) Effective robustness values (%)

We report the mean and standard deviation for three groups of models with different training data, respectively.

|Test set|Evaluation|CIFAR-10|ImageNet|CIFAR+IN|
|---|---|---|---|---|
|CIFAR-10.1|Single-ID|-1.68±0.92|1.05±1.27|0.02±1.10|
|CIFAR-10.1|Multi-ID|-1.43±0.92|0.10±1.12|0.19±1.01|
|CIFAR-10.2|Single-ID|-1.65±0.70|3.91±2.20|-0.64±1.79|
|CIFAR-10.2|Multi-ID|-0.76±0.77|0.56±1.27|0.03±1.29|
|CINIC-10|Single-ID|-0.96±1.43|2.77±1.25|-0.10±2.81|
|CINIC-10|Multi-ID|-0.08±1.52|-0.52±0.98|0.63±2.10|
|Average|Single-ID|-1.43±0.53|2.58±1.32|-0.24±1.58|
|Average|Multi-ID|-0.76±0.63|0.04±0.67|0.28±1.04|

Effective robustness values seem to suggest an advantage of ImageNet models compared to CIFAR-10 models, which is consistent with the findings in Miller et al. (2021). However, under the multi-ID evaluation, the advantage of ImageNet models diminishes, and the effective robustness values of both CIFAR-10 models and ImageNet models are much closer to 0. Therefore, the apparent advantage reported by prior works can be explained as the effect of training data on the single-ID evaluation, and our multi-ID evaluation resolves this confounder to provide a more precise understanding.

In Figure 3a, we visualize the multi-ID effective robustness on CIFAR-10.2, where the accuracies of all the models approximately lie on a plane (the baseline function) on the logit scale, and thus these models have similar effective robustness as the OOD accuracy of all the models can be approximately predicted using a simple plane. We also show projected views of Figure 3a in Figure 4, where Figure 4a and Figure 4b correspond to the single-ID evaluation taking different ID test sets with contradictory conclusions. In contrast, our new evaluation provides a more holistic view.
------------
Header_1: Results on CIFAR-like OOD test sets
Header_2: Results on CIFAR-like OOD test sets
Header_3: Evaluation on ImageNet-like OOD Test Sets
filename: 2302.01381.pdf

Evaluation on ImageNet-like OOD Test Sets

We then conduct evaluation on ImageNet-like OOD test sets, and we compare ImageNet models with models trained on YFCC and LAION, respectively. We show the fitting quality in Table 2 and the effective robustness in Tables 3a and 3b. Consistent with results in Section 5.2, our multi-ID evaluation improves the fitting quality over the single-ID evaluation to better predict and understand the OOD accuracies from ID accuracies. On effective robustness, single-ID evaluation leads to a perception of an effective robustness gain when there is a mismatch between the training data and the single ID test set. Our multi-ID evaluation enjoys a holistic view of all the training distributions and suggests that all the models evaluated here have similar effective robustness.

Specifically, the improvement of fitting quality is particularly significant for models involving LAION on ImageNet-R (R2 improved from 0.216 to 0.982 and MAE reduced from 9.23% to 1.32%) and ImageNet-Sketch (R2 improved from 0.281 to 0.937 and MAE reduced from 7.90% to 2.10%). On the effective robustness values, under the single-ID evaluation, YFCC and LAION models have positive effective robustness values (2.59±2.43 (%) on average for YFCC models and 5.96±4.96 (%) on average for LAION models), which is consistent with the findings in Fang et al. (2022); Nguyen et al. (2022). In contrast, under our multi-ID evaluation, the average effective robustness becomes 0.77±0.85 (%) for YFCC models, and -0.00±0.52 (%) for LAION models, much closer to 0. While single-ID evaluation used by prior works (Fang et al., 2022; Nguyen et al., 2022) suggests effective

| |Test set|R2 (↑)|MAE (% ↓)|
|---|---|---|---|
| |Single-ID|Multi-ID|Single-ID|Multi-ID|
|ImageNet-V2| |0.990|0.999|1.44|0.54|
|YFCC| |0.879|0.965|2.55|1.30|
|ImageNet-Sketch| |0.928|0.945|1.56|1.31|
| |ObjectNet|0.903|0.936|2.60|1.98|
|ImageNet-V2| |0.992|0.999|1.33|0.51|
|LAION| |0.216|0.982|9.23|1.32|
|ImageNet-Sketch| |0.281|0.937|7.90|2.10|
| |ObjectNet|0.849|0.906|2.88|2.38|

|Test set|ImageNet|YFCC|
|---|---|---|
|Single-ID|Multi-ID|Single-ID|Multi-ID|
|ImageNet-V2|-1.23±0.46|-0.19±0.50|1.69±1.84|-0.16±0.57|
|ImageNet-R|-2.80±1.34|-0.41±1.83|3.44±3.25|1.07±1.17|
|ImageNet-Sketch|-1.25±1.90|0.14±2.57|1.90±2.25|0.89±1.29|
|ObjectNet|-0.99±4.23|0.74±4.14|3.32±2.55|1.27±0.85|
|Average|-1.57±1.20|0.07±1.68|2.59±2.43|0.77±0.85|
|ImageNet-V2| |-1.21±0.56|0.05±0.65|1.42±1.73|-0.03±0.57|
|ImageNet-R| |-9.45±2.79|-0.54±1.90|9.48±8.84|-0.65±1.05|
|ImageNet-Sketch| |-7.63±3.40|-0.72±3.03|8.71±7.15|-1.10±1.98|
|ObjectNet| |-1.90±4.48|1.14±4.18|4.24±2.39|1.77±1.20|
|Average| |-5.05±2.21|-0.02±1.53|5.96±4.96|-0.00±0.52|

Robustness gains of YFCC models compared to ImageNet models (Figure 5a), all the models have similar effective robustness under our multi-ID evaluation. Additionally, we provide an ablation study on using models with mixed training data in Appendix A.3 and additional interactive visualization on our website at https://shizhouxing.github.io/effective-robustness.
------------
Header_1: Results on CIFAR-like OOD test sets
Header_2: Results on CIFAR-like OOD test sets
Header_3: Evaluation on Additional Models
filename: 2302.01381.pdf

Evaluation on Additional Models

We also evaluate additional models that are not used in fitting the baseline functions. We download models pre-trained by existing works, including OpenCLIP (Ilharco et al., 2021) and SLIP (Mu et al., 2022). OpenCLIP provides CLIP models trained on YFCC and LAION, and SLIP provides YFCC models trained using CLIP and also a combination of self-supervised learning (Chen et al., 2020a,b) and CLIP (SimCLR+CLIP namely SLIP). And we also fine-tune CLIP models on ImageNet for models we pre-train on YFCC and LAION. We use both vanilla fine-tuning and also WiSE-FT (Wortsman et al., 2022b) which aims to improve the robustness after fine-tuning, using a weight-space ensembling of the pre-trained model and the fine-tuned model. Details are in Appendix B.1.

In Table 4, we show results involving YFCC and LAION, respectively. Since these models are not used in the fitting, we do not use R2, but we use MAE to measure the fitting quality. Our multi-ID evaluation generally reduces MAE compared to the single-ID evaluation, and thus the multi-ID evaluation can still more accurately predict the OOD accuracy from the ID accuracies for these models that are not used in the fitting. The effective robustness values of the models also generally become closer to 0, especially for the zero-shot CLIP models. The results further validate that zero-shot CLIP models, although may achieve high OOD performance if pre-trained with large-scale data (Radford et al., 2021), generally do not improve effective robustness if we control for all the ID accuracies. Among the models evaluated here, SLIP models on YFCC and WiSE-FT models from LAION achieve relatively higher average effective robustness compared to other models, under our multi-ID evaluation, although the gains are not consistently significant on all the test sets and become much smaller than those reflected in the single-ID evaluation. However, we are not certain.
------------
Header_1: Table and Text
filename: 2302.01381.pdf

Table and Text
------------
Header_1: Table and Text
Header_2: Table 4: Fitting quality and effective robustness for downloaded and fine-tuned models
filename: 2302.01381.pdf

Table 4: Fitting quality and effective robustness for downloaded and fine-tuned models

The models are not used in the fitting and directly evaluated. Note that MAE and effective robustness are different, where MAE takes absolute values but not effective robustness. For CLIP by Mu et al. (2022) and SLIP, only models pre-trained on YFCC are available.

Model
Test set
Models with pre-training on YFCC
Models with pre-training on LAION

MAE (%, ↓)
Effective Robustness (%)

Single-ID
Multi-ID
Single-ID
Multi-ID

ImageNetN-V2
ImageNetN-R
3.95
0.45
3.95±0.70
-0.33±0.45
4.70
0.38
4.70±0.01
0.38±0.01

$$
\begin{array}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\text{Model} & \text{Test set} & \text{Models with pre-training on YFCC} & \text{Models with pre-training on LAION} \\
\hline
&  & \text{MAE (\%, $\downarrow$)} &  &  & \text{Effective Robustness (\%)} &  &  &  &  \\
\hline
&  & \text{Single-ID} & \text{Multi-ID} & \text{Single-ID} & \text{Multi-ID} &  &  &  &  \\
\hline
\text{ImageNetN-V2} & \text{ImageNetN-R} & 3.95 & 0.45 & 3.95\pm0.70 & -0.33\pm0.45 & 4.70 & 0.38 & 4.70\pm0.01 & 0.38\pm0.01 \\
\hline
\end{array}
$$

...

et al. (2022b) studied the robustness of fine-tuned CLIP models. Moreover, Devillers et al. (2021); Santurkar et al. (2022) studied the transfer performance of CLIP models, which is out of our scope on the robustness against natural distribution shifts.
------------
Header_1: Table and Text
Header_2: Conclusion
filename: 2302.01381.pdf

Conclusion

To conclude, we propose a new and more precise effective robustness evaluation for models with different training data. In our evaluation, the OOD accuracy can generally be better predicted from multiple ID accuracies compared to previous effective robustness evaluation with a single ID test. We find that zero-shot CLIP models pre-trained on language-image data do not have better effective robustness compared to standard image classifiers, and we provide a new understanding of the apparently significant effective robustness gains observed by prior works.
------------
Header_1: Table and Text
Header_2: Limitations and Future Work
filename: 2302.01381.pdf

Limitations and Future Work

There remain several limitations that may be addressed in future works:

- Our method requires fully knowing the training distributions of all the evaluated models, which is not directly applicable for large-scale pre-trained models with private training data. And this also requires the training methods not to significantly alter the training distribution beyond basic data augmentation, while some methods such as SLIP may alter training distributions more significantly, and it is unclear how we can precisely define the training distribution for a model with post-training processing, such as WiSE-FT (Wortsman et al., 2022b) and Model Soups (Wortsman et al., 2022a) with weight-space ensembling. Future work may study how these techniques may impact the ID performance evaluation (Section 5.4).
- We assume that the two ID test sets have relatively close distributions compared to the OOD test sets. We have not considered how the difference between the multiple ID test sets may affect the evaluation, and how the effective robustness should be compared if models are trained on highly different distributions.
- We use fixed OOD test sets to evaluate the OOD performance, following previous works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022; Schuhmann et al., 2021). When models are pre-trained on large-scale data, it becomes unclear if these “OOD” test sets are still OOD, or if these test sets could be less OOD for the pre-trained models compared to standard classifiers. There may also be some distribution overlap between these test sets and the pre-training datasets, even though Radford et al. (2021) mentioned that the likelihood of direct data overlapping is low.
- We focus on distribution shifts where at least “accuracy-on-the-line” from existing works is known to hold for models trained on the same data (Taori et al., 2020; Miller et al., 2021), yet there are counterexamples where “accuracy-on-the-line” does not hold (Section 6) and requires further study. We may set a threshold on the fitting quality and only adopt our method when the fitting quality is sufficiently good. And there are also other OOD test sets (Singla & Feizi, 2021; Rusak et al., 2021; Singla et al., 2022; Idrissi et al., 2022; Li et al., 2023; Moayeri et al., 2022; Vasudevan et al., 2022) that have not been studied in the effective robustness works yet.
- While we mostly focus on comparing CLIP-like models with standard image classifiers, due to the notable OOD robustness of CLIP-like models studied in prior works (Radford et al., 2021; Fang et al., 2022; Nguyen et al., 2022), this work may further be extended to cover other types of models (Goyal et al., 2022; Singh et al., 2022) as well as other modalities such as distribution shifts on language data (Miller et al., 2020; Awadalla et al., 2022).
- Our multi-ID evaluation is intended for the scenario with models trained on different data. For models trained on a single dataset, while there is often a correlation between the rankings derived from the single-ID evaluation and the multi-ID evaluation, respectively, the rankings are not necessarily consistent (see Appendix A.2), and thus our multi-ID evaluation is not intended to replace the single-ID evaluation in this case. We suggest using single-ID and multi-ID evaluation comprehensively.
------------
Header_1: Acknowledgments & Funding Disclosure
filename: 2302.01381.pdf

Acknowledgments & Funding Disclosure
------------
Header_1: Acknowledgments & Funding Disclosure
Header_2: Acknowledgments & Funding Disclosure
filename: 2302.01381.pdf

Acknowledgments & Funding Disclosure

We thank Alex Fang and Jindong Gu for helpful discussions and the reviewers for constructive feedback. This work was supported in part by NSF 2008173, 2048280, 2325121, 2331966, ONR N00014-23-1-2300:P00001.
------------
Header_1: Acknowledgments & Funding Disclosure
Header_2: References
filename: 2302.01381.pdf

References

- Andreassen, A. J., Bahri, Y., Neyshabur, B., and Roelofs, R. The evolution of out-of-distribution robustness throughout fine-tuning. Transactions on Machine Learning Research, 2022.
- Awadalla, A., Wortsman, M., Ilharco, G., Min, S., Magnusson, I., Hajishirzi, H., and Schmidt, L. Exploring the landscape of distributional robustness for question answering models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 5971–5987, 2022.
- Baek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. Advances in Neural Information Processing Systems, 35:19274–19289, 2022.
- Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.
- Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597–1607. PMLR, 2020a.
- Chen, T., Kornblith, S., Swersky, K., Norouzi, M., and Hinton, G. E. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33: 22243–22255, 2020b.
- Darlow, L. N., Crowley, E. J., Antoniou, A., and Storkey, A. J. Cinic-10 is not imagenet or cifar-10. arXiv preprint arXiv:1810.03505, 2018.
- Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. Imagenet: A large-scale hierarchical image database. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 248–255, 2009.
- Desai, K. and Johnson, J. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11162–11173, 2021.
- Devillers, B., Choksi, B., Bielawski, R., and VanRullen, R. Does language help generalization in vision models? In Proceedings of the 25th Conference on Computational Natural Language Learning, pp. 171–182, 2021.
- Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.
- Fang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pp. 6216–6234. PMLR, 2022.
- Goyal, P., Duval, Q., Seessel, I., Caron, M., Singh, M., Misra, I., Sagun, L., Joulin, A., and Bojanowski, P. Vision models are more robust and fair when pretrained on uncurated images without supervision. arXiv preprint arXiv:2202.08360, 2022.
- He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016. doi: 10.1109/CVPR.2016.90.
------------
Header_1: References
filename: 2302.01381.pdf

References
------------
Header_1: References
filename: 2302.01381.pdf

References

- Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions
and perturbations. In International Conference on Learning Representations, 2018.
- Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli,
S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: A critical
analysis of out-of-distribution generalization. ICCV, 2021a.
- Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., and Song, D. Natural adversarial examples.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
15262–15271, 2021b.
- Idrissi, B. Y., Bouchacourt, D., Balestriero, R., Evtimov, I., Hazirbas, C., Ballas, N., Vincent, P.,
Drozdzal, M., Lopez-Paz, D., and Ibrahim, M. Imagenet-x: Understanding model mistakes
with factor of variation annotations. In The Eleventh International Conference on Learning
Representations, 2022.
- Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar,
V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L. Openclip, 2021. URL
https://doi.org/10.5281/zenodo.5143773.
- Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig,
T. Scaling up visual and vision-language representation learning with noisy text supervision. In
International Conference on Machine Learning, pp. 4904–4916. PMLR, 2021.
- Kendall, M. Rank correlation methods. 1948.
- Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. Technical
Report TR-2009, 2009.
- Kumar, A., Raghunathan, A., Jones, R. M., Ma, T., and Liang, P. Fine-tuning can distort pre-
trained features and underperform out-of-distribution. In International Conference on Learning
Representations, 2021.
- Li, Z., Evtimov, I., Gordo, A., Hazirbas, C., Hassner, T., Ferrer, C. C., Xu, C., and Ibrahim, M.
A whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
20071–20082, 2023.
- Lu, S., Nott, B., Olson, A., Todeschini, A., Vahabi, H., Carmon, Y., and Schmidt, L. Harder or
different? a closer look at distribution shift in dataset reproduction. In ICML Workshop on
Uncertainty and Robustness in Deep Learning, 2020.
- Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question
answering models. In International Conference on Machine Learning, pp. 6905–6916. PMLR,
2020.
- Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., Liang, P., Carmon, Y.,
and Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and
in-distribution generalization. In International Conference on Machine Learning, pp. 7721–7735.
PMLR, 2021.
- Moayeri, M., Singla, S., and Feizi, S. Hard imagenet: Segmentations for objects with strong
spurious cues. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track, 2022.
- Mu, N., Kirillov, A., Wagner, D., and Xie, S. Slip: Self-supervision meets language-image pre-
training. In European Conference on Computer Vision, pp. 529–544. Springer, 2022.
- Nguyen, T., Ilharco, G., Wortsman, M., Oh, S., and Schmidt, L. Quality not quantity: On the
interaction between dataset design and robustness of clip. Advances in Neural Information
Processing Systems, 35:21455–21469, 2022.
- Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision.
In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021.
------------
Header_1: References
filename: 2302.01381.pdf

References
------------
Header_1: List of References
filename: 2302.01381.pdf

List of References

- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do cifar-10 classifiers generalize to cifar-10?*

arXiv preprint arXiv:1806.00451, 2018.
- Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. *Do imagenet classifiers generalize to imagenet?*

In International Conference on Machine Learning, pp. 5389–5400. PMLR, 2019.
- Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and Bethge, M.
*If your data distribution shifts, use self-learning.*

Transactions on Machine Learning Research, 2021.
- Santurkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T. *Is a caption worth a thousand images? a controlled study for representation learning.*

arXiv preprint arXiv:2207.07635, 2022.
- Sariyildiz, M. B., Perez, J., and Larlus, D. *Learning visual representations with caption annotations.*

In European Conference on Computer Vision, pp. 153–170. Springer, 2020.
- Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki, A.
*Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.*

arXiv preprint arXiv:2111.02114, 2021.
- Singh, M., Gustafson, L., Adcock, A., de Freitas Reis, V., Gedik, B., Kosaraju, R. P., Mahajan, D., Girshick, R., Dollár, P., and Van Der Maaten, L.
*Revisiting weakly supervised pre-training of visual perception models.*

In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 804–814, 2022.
- Singla, S. and Feizi, S. *Salient imagenet: How to discover spurious features in deep learning?*

In International Conference on Learning Representations, 2021.
- Singla, S., Moayeri, M., and Feizi, S. *Core risk minimization using salient imagenet.*

arXiv preprint arXiv:2203.15566, 2022.
- Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L.
*Measuring robustness to natural distribution shifts in image classification.*

Advances in Neural Information Processing Systems, 33:18583–18599, 2020.
- Teney, D., Oh, S. J., and Abbasnejad, E. *Id and ood performance are sometimes inversely correlated on real-world datasets.*

arXiv preprint arXiv:2209.00613, 2022.
- Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J.
*Yfcc100m: The new data in multimedia research.*

Communications of the ACM, 59(2):64–73, 2016.
- Vasudevan, V., Caine, B., Gontijo Lopes, R., Fridovich-Keil, S., and Roelofs, R.
*When does dough become a bagel? analyzing the remaining mistakes on imagenet.*

Advances in Neural Information Processing Systems, 35:6720–6734, 2022.
- Wang, H., Ge, S., Lipton, Z., and Xing, E. P.
*Learning robust global representations by penalizing local predictive power.*

Advances in Neural Information Processing Systems, 32, 2019.
- Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al.
*Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.*

In International Conference on Machine Learning, pp. 23965–23998. PMLR, 2022a.
- Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al.
*Robust fine-tuning of zero-shot models.*

In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959–7971, 2022b.
- Yadav, C. and Bottou, L. *Cold case: The lost mnist digits.*

Advances in neural information processing systems, 32, 2019.
- Zhang, Y., Jiang, H., Miura, Y., Manning, C. D., and Langlotz, C. P.
*Contrastive learning of medical visual representations from paired images and text.*

In Machine Learning for Healthcare Conference, pp. 2–25. PMLR, 2022.
------------
Header_1: List of References
Header_2: Additional Results
filename: 2302.01381.pdf

Additional Results
------------
Header_1: List of References
Header_2: Additional Results
Header_3: Projected Views
filename: 2302.01381.pdf

Projected Views

In Figure 5, we show projected views of Figure 3b.

$$
\begin{array}{|c|c|c|c|c|c|c|c|}
\hline
\text{ImageNet} & 95 & \text{YFCC} & 50 & \text{ImageNet+YFCC} \\
\hline
90 & & & & 25 \\
80 & & & & 10 \\
70 & & & & \\
50 & & & & \\
25 & & & & \\
10 & & & & \\
\hline
\end{array}
$$

$$
\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
10 & 25 & 50 & 70 & 80 & 90 & 95 & & & & \\
\text{ImageNet accuracy (css., %)} & & & & & & & \text{YFCC accuracy (css., %)} & & & \\
\hline
\end{array}
$$

(a) ImageNet-R accuracy against ImageNet accuracy. YFCC models have higher ImageNet-R accuracy compared to ImageNet models when controlling for ImageNet accuracy only.

(b) ImageNet-R accuracy against YFCC accuracy. YFCC models have similar ImageNet-R accuracy compared to ImageNet models when controlling for YFCC accuracy only.

Figure 5: Projected views of Figure 3b. Figure 5a and Figure 5b correspond to single-ID evaluation using different ID test sets, Figure 5a suggests effective robustness gains of YFCC models but the gains diminish in Figure 5b. Our multi-ID evaluation shows a holistic view where all the models have a similar effective robustness.
------------
Header_1: List of References
Header_2: Additional Results
Header_3: Agreement between Single-ID and Multi-ID Evaluation
filename: 2302.01381.pdf

Agreement between Single-ID and Multi-ID Evaluation

We conduct an experiment to check the correlation between the single-ID evaluation and our new multi-ID evaluation, in terms of the relative ranking between different models trained on the same data. We use Kendall’s rank correlation test (Kendall, 1948) and we report the τ statistic computed by scipy.stats.kendalltau in Tables 5 to 7. Results show that the rankings on the single-ID effective robustness and multi-ID effective robustness are positively correlated for CIFAR-10 and ImageNet models. There is also a weaker positive correlation for YFCC models. For LAION models, there is sometimes a negative correlation. Overall, while there is often a positive correlation between the rankings provided by the single-ID evaluation and the multi-ID evaluation, respectively, it is not necessarily consistent on all the datasets. Thus, when comparing models trained on the same data, our multi-ID evaluation is not intended to replace the single-ID evaluation. Our multi-ID evaluation is mainly for comparing models trained on different data, and may be used as a supplementary evaluation if all the models are trained on a single dataset.

$$
\begin{array}{|c|c|c|c|}
\hline
\text{Test set} & \text{CIFAR-10.1} & \text{CIFAR-10.2} & \text{CINIC-10} \\
\hline
\text{CIFAR-10 models} & 0.9619 & 0.6667 & 0.8095 \\
\text{ImageNet models} & 0.1664 & 0.1522 & 0.4907 \\
\hline
\end{array}
$$
------------
Header_1: List of References
Header_2: Additional Results
Header_3: Models with Mixed Training Data in the Fitting
filename: 2302.01381.pdf

Models with Mixed Training Data in the Fitting

As mentioned in Section 5.1, we train models with mixed data to obtain models with diverse accuracies. In Tables 8 and 9, we show that if we do not include models with mixed training data in the fitting, the MAE for these models can become higher, although the difference is not large.
------------
Header_1: Research Results
filename: 2302.01381.pdf

Research Results
------------
Header_1: Research Results
Header_2: Table 6: τ statistics for ImageNet models and YFCC models on ImageNet-like OOD test sets
filename: 2302.01381.pdf

Table 6: τ statistics for ImageNet models and YFCC models on ImageNet-like OOD test sets

|Test set|ImageNet-V2|ImageNet-R|ImageNet-Sketch|ObjectNet|
|---|---|---|---|---|
|ImageNet models|0.5142|0.4412|0.8031|0.8063|
|YFCC models|0.0256|0.8461|0.7179|0.2564|
------------
Header_1: Research Results
Header_2: Table 7: τ statistics for ImageNet models and LAION models on ImageNet-like OOD test sets
filename: 2302.01381.pdf

Table 7: τ statistics for ImageNet models and LAION models on ImageNet-like OOD test sets

|Test set|ImageNet-V2|ImageNet-R|ImageNet-Sketch|ObjectNet|
|---|---|---|---|---|
|ImageNet models|0.3123|0.4384|0.6216|0.9729|
|LAION models|-0.4945|0.6483|-0.2747|0.6483|
------------
Header_1: Research Results
Header_2: Table 8: MAE (%) for ImageNet+YFCC models
filename: 2302.01381.pdf

Table 8: MAE (%) for ImageNet+YFCC models

When they are excluded and included in the fitting, respectively, comparing the effective robustness of ImageNet models and YFCC models.

|Test set|ImageNet+YFCC models in the fitting|Excluded|Included|
|---|---|---|---|
|ImageNet-V2| |0.71|0.64|
|ImageNet-R| |1.30|1.21|
|ImageNet-Sketch| |0.98|0.94|
|ObjectNet| |1.73|0.95|
------------
Header_1: Research Results
Header_2: Experimental Details
filename: 2302.01381.pdf

Experimental Details
------------
Header_1: Research Results
Header_2: Experimental Details
Header_3: Details of Models
filename: 2302.01381.pdf

Details of Models

We use TF-Vision3 under the Apache License Version 2.0 to train standard classifiers on CIFAR-10 and ImageNet. We follow the configurations provided in TF-Vision for vanilla ResNet training on ImageNet and we train ResNet-18, ResNet-50 and ResNet-101 models. We reuse the configurations to train models on CIFAR-10, where we only change the dataset, number of classes, and image size, without tuning hyperparameters for the training. And we load checkpoints of ViT-S/16, ViT-B/16, and ViT-L/16 models pre-trained on ImageNet, provided by TF-Vision.

For training CLIP models, we mostly follow hyperparameters provided in Fang et al. (2022) and the implementation in Open-CLIP (Ilharco et al., 2021). While Fang et al. (2022) used a batch size of 1024, we use 2048 for more parallelism. We use YFCC-15M in Radford et al. (2021), which is a subset of YFCC-100M (Thomee et al., 2016). And we use LAION-15M which we uniformly sample from LAION-400M (Schuhmann et al., 2021). For fine-tuning CLIP models, we fine-tune for 50,000 steps, using learning rates $$3 \times 10^{-5}$$ and $$1 \times 10^{-4}$$, respectively. For WiSE-FT, we take $$\alpha = 0.5$$ which is the coefficient for weight-space ensembling. For OpenCLIP models, we use ViT-B/32 models trained on LAION-400M. For SLIP, we use all the CLIP and SLIP models trained on YFCC-15M.

For data subsampling, we uniformly sample a proportion of training examples from the entire dataset, at ratios of {5%, 10%, 20%, 30%, 40%, 50%}, respectively. For combining two training datasets at various ratios, given a coefficient $$\lambda$$ (0 < $$\lambda$$ < 1), we uniformly sample a proportion of data from the two datasets at ratios of $$\lambda$$ and $$(1 - \lambda)$$, respectively, and then we combine the two subsets. When combining ImageNet and CIFAR-10, we take $$\lambda \in \{0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.995\}$$; when combining ImageNet with YFCC and LAION, respectively, we take $$\lambda \in \{0.01, 0.1, 0.25, 0.5\}$$.

Sources: TF-Vision, Open-CLIP, SLIP

|Test set|ImageNet+LAION models in the fitting| |Excluded|Included|
|---|---|---|---|---|
| |ImageNet-V2|0.54|0.51| |
| |ImageNet-R|1.56|1.19| |
|ImageNet-Sketch|2.62|2.03| | |
| |ObjectNet|2.91|1.47| |

Models are trained using 4x4 or 4x8 TPU v2 Pods, and they are evaluated using NVIDIA V100 GPUs, on the cloud.
------------
Header_1: Research Results
Header_2: Details of ID Test Sets
filename: 2302.01381.pdf

Details of ID Test Sets

We construct an ID test set from YFCC-15M and LAION-15M, respectively, and we automatically generate classification labels by matching text with ImageNet class names, which has also been similarly performed in Fang et al. (2022) for training classifiers using caption data. On YFCC-15M which contains metadata, we use tags for matching. On LAION-15M which does not provide metadata such as tags, we simply use the entire text for matching. We adopt a label only if a unique ImageNet class can be determined by matching for the involved image. We then construct a balanced and labelled test set by keeping 50 examples for each class that has at least 100 examples in the labelled images. The test examples are then held out from the training data. For the YFCC test set, there are 22550 examples for 451 classes; and for the LAION test set, there are 20400 examples 408 classes.
------------
Header_1: Research Results
Header_2: Licenses of the Datasets
filename: 2302.01381.pdf

Licenses of the Datasets

We have used the following datasets:

- CIFAR-10 (Krizhevsky et al., 2009). License is not clearly known.
- YFCC6 under various Creative Commons licenses.
- LAION7 under the Creative Common CC-BY 4.0 license for the metadata, and the images are under the copyright of the original authors.
- CIFAR-10.18 under the MIT license.
- CIFAR-10.29. License is not clearly known.
- CINIC-1010 under the MIT license.
- ImageNet-V211 under the MIT license.
- ImageNet-R12 under the MIT license.
- ImageNet-Sketch13 under the MIT license.
- ObjectNet14 with a license provided on the webpage.
------------
