{"chat-trace": {"_type": "wb_trace_tree", "root_span_dumps": "{\"span_id\": null, \"name\": \"chat\", \"start_time_ms\": 1713829884895, \"end_time_ms\": 1713829892681, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"original_question\": \"two sentences about datacomp\", \"history\": []}, \"outputs\": null}, {\"inputs\": {}, \"outputs\": {\"final_answer\": \"DATACOMP is a new benchmark initiative focused on the development of multimodal datasets, particularly emphasizing the design and curation of large-scale image-text pairs to train models like CLIP [1]. It introduces a unique approach by allowing participants to either filter a provided dataset or bring their own data, aiming to enhance the quality and effectiveness of training datasets in achieving higher model accuracies on various downstream tasks [2].\", \"tool\": \"retrieval\", \"rephrased_question\": \"two sentences about datacomp\", \"related_queries\": [\"<<non-serializable: RelatedQueryItem>>\", \"<<non-serializable: RelatedQueryItem>>\", \"<<non-serializable: RelatedQueryItem>>\"]}}], \"child_spans\": [{\"span_id\": null, \"name\": \"rephrase\", \"start_time_ms\": null, \"end_time_ms\": 1713829884897, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"original_question\": \"two sentences about datacomp\"}, \"outputs\": {\"rephrased_question\": \"two sentences about datacomp\"}}], \"child_spans\": null, \"span_kind\": null}, {\"span_id\": null, \"name\": \"select-tool\", \"start_time_ms\": null, \"end_time_ms\": 1713829887200, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"question\": \"two sentences about datacomp\"}, \"outputs\": {\"tool\": \"retrieval\"}}], \"child_spans\": null, \"span_kind\": null}, {\"span_id\": null, \"name\": \"retrieval\", \"start_time_ms\": 1713829887643, \"end_time_ms\": 1713829887923, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"question\": \"two sentences about datacomp\"}, \"outputs\": {\"nodes\": [\"Header_1: DATACOMP Participant Workflow\\nHeader_2: Related Work\\nHeader_3: The DATACOMP benchmark\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nThe DATACOMP benchmark\\n\\nDATACOMP is meant to facilitate data-centric experimentation. While traditional benchmarks emphasize model design, DATACOMP is centered around dataset development, where the resulting datasets can be used to train high accuracy models. We focus on large image-text datasets and quantify a dataset submission by training a CLIP model on it from scratch [111] and evaluating on 38 downstream image classification and retrieval tasks. We additionally have three secret test sets, which will be released after a year, to guard against overfitting. To facilitate such investigations, we provide a candidate pool of uncurated image-text pairs sourced from the public internet. Our benchmark offers two tracks: one where participants must filter samples from the pools we provide, and another where participants can use external data. Moreover, DATACOMP is structured to accommodate participants with diverse levels of computational resources: each track is broken down into four scales with varying compute requirements. We now discuss high-level design decisions, construction of a 12.8B image-text data pool to facilitate the competition, benchmark tracks, model training, and evaluation.\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: Related Work\\nHeader_3: Competition design\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nCompetition design\\n\\nOverview. In many areas of machine learning, larger datasets lead to better performing models [87, 79, 73, 107, 66, 28, 19, 111, 112]. Hence comparing only datasets with the same size is a natural starting point. However, this approach is flawed as controlling the dataset size ignores critical curation constraints: candidate pool size (i.e., number of image-text pairs to harvest) and training compute. For instance, assembling a dataset like LAION-2B consists of identifying data sources (e.g., Common Crawl or Reddit) and filtering the data source. Notably, the final dataset size is a design choice and is only upper-bounded by the data sources. Hence, the true data constraint is the size of the reservoir of samples: candidate pool to be filtered. To make DATACOMP a realistic benchmark, we therefore fix the candidate pool in the filtering track, but give participants control over the training set size. Compute cost is another relevant constraint. To put datasets of different size on equal footing, we specify the total number of training samples seen. Consider the 12.8B compute scale and filtered datasets A and B, with 6.4B and 3.2B image-text pairs respectively. At this scale, we train by making two passes over A, while making four passes over B. A key result from our experiments is that smaller, more stringently filtered datasets can lead to models that generalize better.\\n\\nCompetition tracks. Two key procedures in assembling a training dataset are filtering a data source [128, 129, 20] and aggregating data sources [36, 37]. To reflect this structure, DATACOMP has two tracks: filtering, where participants select a subset of the samples from COMMONPOOL, and Bring Your Own Data (BYOD), where participants can use any source of data. Key decisions for each track are described in Sections 3.2 and 3.3, respectively. For full competition track rules see Appendix A.\\n\\nCompetition compute scales. To facilitate study of scaling trends and accommodate participants with various computational resources, we structure DATACOMP using four scales of compute: small, medium, large and xlarge. Each new scale increases the number of samples seen during training by 4.\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: Figure 1: DATACOMP participant workflow\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nFigure 1: DATACOMP participant workflow\\n\\nA) Choose a scale based on resource constraints.\\n\\nB) Design a dataset, in either the filtering or BYOD track.\\n\\nC) Train a CLIP model on the designed dataset using a fixed architecture and hyperparameters (Section 3.4).\\n\\nD) Evaluate the trained model on a suite of diverse downstream tasks (Section 3.5).\\n\\nCode for training and evaluating models at www.datacomp.ai.\\n\\nWe believe that our infrastructure will help put research on dataset design on rigorous empirical foundations, draw attention to this understudied research area, and lead to the next generation of multimodal datasets.\", \"Header_1: Appendix\\nHeader_2: Contents\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nContents\\n\\n1. Introduction - 1\\n2. Related Work - 3\\n3. The DATACOMP benchmark - 4\\n- Competition design - 4\\n- COMMONPOOL generation, for the filtering track - 5\\n- The bring your own data (BYOD) track - 5\\n- Training - 5\\n- Evaluation - 6\\n4. Baselines - 6\\n- Filtering baselines - 6\\n- BYOD baselines - 7\\n5. Results and discussion - 7\\n- Building better datasets - 7\\n- DATACOMP design analyses - 8\\n- Evaluation trends - 9\\n6. Limitations and conclusion - 9\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: COMMONPOOL generation, for the filtering track\\nHeader_3: Evaluation\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nEvaluation\\n\\nWe evaluate on a suite of 38 image classification and retrieval tasks. We also study two additional fairness tasks, detailed in Section 5 and Appendix Q. As discussed in Section 3.2, we remove test set images from DATACOMP to avoid contamination. Image classification datasets range from satellite imagery recognition to classifying metastatic tissues. In total we have (with some overlap): 22 of the datasets evaluated in Radford et al. [111], 6 ImageNet distribution shifts (i.e., ImageNet-Sketch [143], ImageNet-V2 [121], ImageNet-A [65], ImageNet-O [65], ImageNet-R [64], and ObjectNet [13]), 13 datasets from VTAB [156], and 3 datasets from WILDS [83, 127]. Retrieval datasets include Flickr30k [151], MSCOCO [26], and the WinoGAViL commonsense association task [17]. To aggregate results over all evaluation tasks, we average the preferred metric for each task.\\n\\nDATACOMP adopts a zero-shot evaluation protocol: models are tested without training on the evaluation tasks. This approach is computationally efficient and measures a model\\u2019s ability to perform well without any additional training. We find a strong rank correlation (>0.99) between performance in linear probe zero-shot settings (Appendix Figure 16). Additional details are in Appendix O.\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: COMMONPOOL generation, for the filtering track\\nHeader_3: 5 Results and discussion\\nHeader_4: 5.1 Building better datasets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\n5.1 Building better datasets\\n\\nMain results. Our key results are in Table 3. Most notably, the intersection between image-based filtering and CLIP score filtering excels on most tasks. The exception is at the small scale and for retrieval datasets.2 Furthermore, other filtering strategies like basic, CLIP score, image-based, text-based filtering show better downstream performance when compared to no filtering. A much larger suite of experiment results can be found in Appendix R.\\n\\nDATACOMP leads to better image-text datasets. We hope DATACOMP catalyzes the search for the next generation of multimodal datasets. We contribute DATACOMP-1B, which is the output of the Image-based \\u2229 CLIP score (L/14 30%) baseline filter at the xlarge scale of the filtering track.\\n\\n2Cherti et al. [28] also observe that models rank differently on classification and retrieval tasks.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Leadership and Advising\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nLeadership and Advising\\n\\nAdvising: Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\\n\\nLeadership: Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar\\n\\nOverall project lead: Ludwig Schmidt\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: A Benchmark rules\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nA Benchmark rules\\n\\nWe provide concrete rules below for the two competition tracks that comprise DATACOMP: filtering and BYOD. Additionally, we provide a checklist, which encourages participants to specify design decisions, which allows for more granular comparison between submissions.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: A Benchmark rules\\nHeader_3: A.2 Bring your own data track: amendments\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nA.2 Bring your own data track: amendments\\n\\nTo facilitate more open-ended exploration, we provide amendments to the Track 1 competition to allow for more diverse submissions in Track 2.\\n\\n- Participants are allowed to augment COMMONPOOL data with existing datasets, so long as these data sources do not contain test images from the evaluation tasks. Participants can use data from any COMMONPOOL; however, they are not required to do so.\\n- Assembling one\\u2019s own dataset is allowed; however, test images from the evaluation tasks can neither be contained nor otherwise used to construct said dataset. We encourage releasing the image urls or the images themselves in addition to the text for each image. We also encourage rigorous documentation of face-blurring and other data safety checks (see Section 3.2 for more details). We reserve the right to run our own safety code on participant provided data and disqualify entries that do not meet adequate safety standards.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nHeader_2: Abstract\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nAbstract\\n\\nMultimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DATACOMP, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DATACOMP workflow leads to better training sets. Our best baseline, DATACOMP-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI\\u2019s CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DATACOMP and all accompanying code at www.datacomp.ai.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Candidate pool\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nCandidate pool\\n\\nCandidate pool lead: Vaishaal Shankar\\n\\nData collection: Romain Beaumont, Vaishaal Shankar\\n\\nPre-processing and metadata: Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak Gadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar, George Smyrnis (face blurring lead)\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nContributions\\n\\nFor this section, contributors are ordered alphabetically.\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: DATACOMP design analyses\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDATACOMP design analyses\\n\\nCOMMONPOOL and LAION are comparable with the same filtering technique. The filtering approach on our pool yields comparable results to LAION-2B. Consistency across scales and training changes is also observed.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Participant tooling\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nParticipant tooling\\n\\nParticipant tooling lead: Gabriel Ilharco\\n\\nResharder: Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco, Vivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis\\n\\nTraining: Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell Wortsman (lead)\\n\\nEvaluation: Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco\\n\\nAdditional infrastructure: Stephen Mussmann, Sarah Pratt\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Baselines\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nBaselines\\n\\nBaselines lead: Yair Carmon\\n\\nFiltering track: Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco, Kalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu Zhang (co-lead)\\n\\nBYOD track: Gabriel Ilharco, Thao Nguyen\\n\\nExperiment babysitting: Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: COMMONPOOL generation, for the filtering track\\nHeader_3: Baselines\\nHeader_4: 4.2 BYOD baselines\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\n4.2 BYOD baselines\\n\\nWe experiment with multiple external data sources, including four moderately sized datasets (10 to 58M samples) studied by Nguyen et al. [101]\\u2014CC12M [24], YFCC15M [140, 111], RedCaps [38] and Shutterstock [101]\\u2014and the larger LAION-2B [129]. Additional experiments, along with more details about the data sources are provided in Appendix P.2. We consider these data sources as they are and do not perform additional preprocessing. We also present experiments combining some of the data sources (using only the external datasets, or in addition to data from our pool).\", \"Header_1: Document\\nHeader_2: Datasheet\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDatasheet\\n\\n|S.1 Motivation|60|\\n|---|---|\\n|S.2 Composition|60|\\n|S.3 Collection Process|62|\\n|S.4 Preprocessing, Cleaning, and/or Labeling|63|\\n|S.5 Uses|64|\\n|S.6 Distribution|65|\\n|S.7 Maintenance|65|\", \"Header_1: Evaluation Trends\\nHeader_2: ImageNet vs Average over 38 datasets\\nHeader_3: Limitations and Conclusion\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nLimitations and Conclusion\\n\\nIn terms of societal risks, creating an index of image-text pairs from the public internet can be problematic. The internet contains unsafe, toxic, and sensitive content, which ideally should not percolate into machine learning datasets. Though we take steps to remove NSFW content and blur human faces to protect privacy, we hope future work will further explore the biases and risks from COMMONPOOL and DATACOMP-1B. We see several additional directions for future work, including 1) Curating more data sources. 2) Improved data filtering algorithms. 3) Further supervision signals (e.g., image captions coming from captioning models). 4) Additional input modalities (e.g., video, 3D objects). 5) Broader evaluations for vision-and-language and robotics tasks.\\n\\nOverall, we see DATACOMP as a first step towards improving training datasets, and hope our new benchmark will foster further research. By providing a controlled experimental setting, DATACOMP enables researchers to iterate on dataset design on rigorous empirical foundations. We open-source all of our code, data, and infrastructure, and hope these resources will help the community build the next generation of multimodal datasets.\\n\\nNote that unlike Kornblith et al. [84] we evaluate zero-shot performance rather than transfer learning.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDATACOMP: In search of the next generation of multimodal datasets\\n\\n*Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt*\\n\\narXiv:2304.14108v5 [cs.CV] 20 Oct 2023\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: A Benchmark rules\\nHeader_3: Checklist\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nChecklist\\n\\nThe following checklist provides the basis for more fine-grained comparison between submissions.\\n\\n- \\u2610 Images from the evaluation tasks are included in my submission. If yes, please specify which datasets.\\n- \\u2610 I used an existing datasets (e.g., YFCC100M [140]) in my submission. If yes, please specify which datasets. (Note: applies to BYOD only)\\n- \\u2610 I curated my own data. If yes, please provide (1) image data or urls, (2) text for each image, (3) list of safety steps taken including but not limited to face blurring, explicit content image and text filtering. (Note: applies to BYOD only)\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: Figure 2: Performance of random subsets and CLIP score filtering\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nFigure 2: Performance of random subsets and CLIP score filtering\\n\\nWhen varying the subset size, larger subsets are always better for random subsets. For CLIP score filtering, subsets with intermediate size perform best.\\n\\nOur dataset is comprised of 1.4B samples, which is smaller than the LAION-2B dataset with 2.3B samples. However, a CLIP L/14 trained on DATACOMP-1B outperforms the LAION-2B competitor by 6.1 percentage points on ImageNet. Training on DATACOMP-1B also improves ImageNet accuracy by 3.7 percentage points over OpenAI\\u2019s ViT-L/14 trained with the same compute budget. Even with 400M samples, a subset of DATACOMP-1B outperforms OpenAI\\u2019s ViT-L/14.\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: External data sources impact\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nExternal data sources impact\\n\\nAdding external data sources improves performance. Combining CLIP-filtered data from COMMONPOOL with external data from CC12M, YFCC15M, RedCaps, and Shutterstock boosts ImageNet accuracy by 4.3 percentage points.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nHeader_2: Introduction\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nIntroduction\\n\\nRecent advances in multimodal learning such as CLIP, DALL-E, Stable Diffusion, Flamingo, and GPT-4 offer unprecedented generalization capabilities in zero-shot classification, image generation, and in-context learning. While these advances use different algorithmic techniques, e.g., contrastive learning, diffusion, or auto-regressive modeling, they all rest on a common foundation: large datasets containing paired image-text examples. For instance, CLIP\\u2019s training set contains 400 million image-text pairs, and Stable Diffusion was trained on the two billion examples from LAION-2B. This new generation of image-text datasets is 1,000 times larger than previous datasets such as ImageNet, which contains 1.2M images. Despite the central role of image-text datasets, little is known about them. Many state-of-the-art datasets are proprietary, and even for public datasets such as LAION-2B, it is unclear how design choices such as the data source or filtering techniques affect the resulting models. While there are thousands of ablation studies for algorithmic design choices (loss function, model architecture, etc.), datasets are often treated as monolithic artifacts without detailed investigation.\\n\\n*Equal contribution, randomly ordered. Correspondence to contact@datacomp.ai.\\n\\n1University of Washington 2Columbia University 3Tel Aviv University 4Apple 5UT Austin 6LAION 7AI2 8Juelich Supercomputing Center, Research Center Juelich 9University of Illinois Urbana-Champaign 10Graz University of Technology 11Hebrew University 12Google Research 13Snorkel AI\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\\n\\n|DATASET|Dataset size|# samples seen|Architecture|Train compute (MACs)|ImageNet accuracy|\\n|---|---|---|---|---|---|\\n|OpenAI\\u2019s WIT [111]|0.4B|13B|ViT-L/14|1.1 \\u00d7 1021|75.5|\\n|LAION-400M [128, 28]|0.4B|13B|ViT-L/14|1.1 \\u00d7 1021|72.8|\\n|LAION-2B [129, 28]|2.3B|13B|ViT-L/14|1.1 \\u00d7 1021|73.1|\\n|LAION-2B [129, 28]|2.3B|34B|ViT-H/14|6.5 \\u00d7 1021|78.0|\\n|LAION-2B [129, 28]|2.3B|34B|ViT-g/14|9.9 \\u00d7 1021|78.5|\\n|DATACOMP-1B (ours)|1.4B|13B|ViT-L/14|1.1 \\u00d7 1021|79.2|\\n\\ndatasets currently lack the benchmark-driven development process that has enabled a steady stream of improvements on the model side and isolates data enhancements from changes to the model. These issues impede further progress in multimodal learning, as evidenced by recent work showing that public datasets currently do not match the scaling behavior of proprietary alternatives [28].\\n\\nIn this paper, we take a step towards a more rigorous dataset development process. Our first and central contribution is DATACOMP, a new benchmark for multimodal dataset design. DATACOMP flips the traditional benchmarking paradigm in machine learning where the dataset is fixed and researchers propose new training algorithms. Instead, we hold the entire training code and computational budget constant so that participants innovate by proposing new training sets. To evaluate the quality of a training set, we score the resulting model with a testbed of 38 classification and retrieval tasks such as ImageNet [37], ImageNetV2 [121], DTD [30], EuroSAT [63], SUN-397 [146], and MSCOCO [26].\\n\\nDATACOMP focuses on two key challenges that arise when assembling large training datasets: what data sources to train on, and how to filter a given data source. Each challenge corresponds to one track in our benchmark. To facilitate the filtering track, our second contribution is COMMONPOOL, a dataset of 12.8B image-text pairs collected from Common Crawl and currently the largest public image-text dataset. We release CommonPool as an index of image url-text pairs under a CC-BY-4.0 license, and apply content checks in its construction to remove unsafe or unwanted content. In the filtering track, the goal of participants is to find the best subset of COMMONPOOL to train on. In the second track, Bring Your Own Data (BYOD), participants may leverage any data source, as long as it does not overlap with our evaluation testbed.\\n\\nOur third contribution is an investigation of scaling trends for dataset design. In particular, DATACOMP contains four scales, where we vary the training budget and the candidate pool size from 12.8M to 12.8B samples (see Table 2). Expressed in GPU hours, the cost of a single training run ranges from 4 to 40,000 GPU hours on the A100 cluster we used for development. The different scales enable researchers with different resources to participate in our benchmark. Moreover, our results show that the ranking of filtering approaches is largely consistent across scale.\\n\\nOur fourth contribution is over three hundred baseline experiments, including techniques such as querying captions for relevant keywords, filtering based on image embeddings, and applying a threshold on CLIP scores. A key result from our baselines experiments is that smaller, more stringently filtered datasets can lead to models that generalize better than larger datasets coming from the same pool. At the 12.8B scale, our best filtering baseline increases ImageNet zero-shot accuracy by 6.9 percentage points (pp) relative to the unfiltered pool (see Table 3). For the BYOD track, our initial experiments show that 109M additional data points (less than 1% of the 12.8B pool) improve the CLIP-filtered subsets of COMMONPOOL by up to 1.2 pp ImageNet accuracy (see Table 18).\\n\\nFinally, our fifth contribution is DATACOMP-1B, a new state-of-the-art multimodal dataset. We obtain DATACOMP-1B by combining our two most promising filtering baselines. DATACOMP-1B enables training a CLIP ViT-L/14 model to an ImageNet zero-shot accuracy of 79.2% (see Table 1), corresponding to a 9\\u00d7 computational cost reduction when compared to a larger CLIP ViT-g/14 model trained on LAION-2B for about 3\\u00d7 longer. Moreover, our model outperforms OpenAI\\u2019s original CLIP ViT-L/14 by 3.7 percentage points, while using the same compute budget.\\n\\nTo make DATACOMP a shared environment for controlled dataset experiments, we publicly release our candidate pool url index, our tooling for assembling these pools, our filtering baselines, and our\", \"Header_1: Quantitative Results on Text Infilling\\nHeader_2: Table 2: Quantitative results on text infilling\\nfilename: 35_discs_a_benchmark_for_discrete.pdf\\ntitle: DISCS: A Benchmark for Discrete Sampling\\n\\nTable 2: Quantitative results on text infilling\\n\\nThe reference text for computing the Corpus BLEU is the combination of WT103 and TBC.\\n\\n|Methods|Self-BLEU|n = 2 Self|n = 3 Self|n = 2 WT103|n = 3 WT103|n = 2 TBC|n = 3 TBC|Corpus BLEU|\\n|---|---|---|---|---|---|---|---|---|\\n|RMW|\\u221a|92.41|6.26|9.10|18.97|26.73|19.33|26.67|16.24|\\n|GWG|t|85.93|11.22|17.14|23.16|35.56|23.58|35.56|16.75|\\n|PAS|t|85.39|11.37|17.60|22.61|35.53|23.65|35.47|16.57|\\n|DLMCf|\\u221a t|88.39|9.53|14.06|21.00|31.85|22.27|31.98|16.70|\\n|DLMC|\\u221a t|85.28|12.05|17.65|24.03|36.34|24.51|36.27|16.45|\\n|GWG|t+1 t|81.15|15.47|22.70|25.62|38.91|25.62|38.58|16.68|\\n|DMALA|t t+1|80.21|16.36|23.71|25.60|39.39|26.75|39.72|16.53|\\n|PAS|t+1 t|81.02|15.62|22.65|25.59|39.28|26.08|39.48|16.69|\\n|DLMCf|t+1|80.12|16.25|23.76|25.41|39.31|26.86|39.57|16.73|\\n|DLMC|t+1|84.55|12.62|18.47|24.27|37.28|24.94|37.14|16.69|\\n\\nOptimization problems: DLMCf and DMALA have the best performance on language models. We believe more efficient discrete samplers can be obtained by designing better discretization of DLD (Sun et al., 2022a). DISCS is a convenient tool during this process. The researcher can freely set the configurations for tasks and samplers, and DISCS will automatically compile the program and run the processes in parallel. Besides, we observe that the choice of the locally balanced weight function should depend on the critical temperature of the target distribution. We believe this observation is insightful and will lead to a deeper understanding of locally balanced samplers.\\n\\nOf course, DISCS does not include all existing tasks or samplers in discrete sampling, for example, the zero order (Xiang et al., 2023) and second order (Sun et al., 2023a) approximation methods. We will keep iterating DISCS, and more features will be added in the future. We wrap DISCS to a JAX library. Researchers can conveniently implement customer tasks or samplers to accelerate their study and, in the meanwhile, contribute the code to DISCS for further improvement. We believe DISCS will be a powerful tool for researchers and facilitate the future research in discrete sampling.\", \"Header_1: Document\\nHeader_2: Baseline details\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nBaseline details\\n\\n|P.1 Filtering track|47|\\n|---|---|\\n|P.2 BYOD track|48|\\n|P.2.1 Additional results|49|\", \"Header_1: Appendix\\nHeader_2: Contents\\nHeader_3: B Contributions - 25\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nB Contributions - 25\\n\\n- Candidate pool - 25\\n- Participant tooling - 25\\n- Baselines - 25\\n- Leadership and Advising - 25\", \"Header_1: DataComp Results\\nHeader_2: DataComp Results\\nHeader_3: Appendix D\\nfilename: 2307.10350.pdf\\ntitle: Improving Multimodal Datasets with Image Captioning\\n\\nAppendix D\\n\\nTable 2: Performance with varying cosine similarity thresholds\\n\\n|Cosine Similarity Threshold|Accuracy|\\n|---|---|\\n|0.1|85%|\\n|0.2|90%|\\n|0.3|92%|\", \"Header_1: NSFW Filtering and Deduplication\\nHeader_2: Deduplication against evaluation sets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDeduplication against evaluation sets\\n\\nTo prevent data leakage, we filter COMMONPOOL by removing duplicate and near-duplicate matches of evaluation set images. See Figure 4 for example query images from Common Crawl and corresponding near-duplicates in our evaluations sets. We consider images as duplicates when\", \"Header_1: DataComp Results\\nHeader_2: DataComp Results\\nfilename: 2307.10350.pdf\\ntitle: Improving Multimodal Datasets with Image Captioning\\n\\nDataComp Results\\n\\nFigure 2: At the 128M scale of DataComp, we obtain improvement on ImageNet and average accuracies compared to the best filtering method on raw data, by using a mixture of raw and synthetic captions, selecting only image-text pairs with cosine similarity above a certain threshold. (Left) We visualize how various data filtering strategies perform at medium scale, on ImageNet and across 38 tasks. Including BLIP2 captions in the training data significantly outperforms competitive baselines from DataComp trained on only raw text [18]. (Right) As we vary the percentage of top examples chosen from the pool (based on CLIP score), we see consistent benefits from (i) using BLIP2 captions for samples that would be discarded otherwise, (ii) applying the same filtering threshold to new image-text pairs containing BLIP2 captions to keep noise level low. The exact accuracy numbers can be found in Appendix D.\\n\\nImageNet1k. The authors then find the intersection between this set of examples and those that are in the top x% based on CLIP score. This is the best baseline using raw captions on DataComp.\\n\\n- Combining raw and synthetic captions: we use raw captions for the top x% of examples based on CLIP score. For the remaining images (that would otherwise be filtered out), we generate corresponding BLIP2 captions and add them back to the training pool. We also experiment with filtering these additional image-text pairs with the same cosine similarity threshold set in the first step (i.e., BLIP2 (x%, filtered) in Figure 2).\\n\\nIn Appendix D, we investigate other baselines and report how well each approach does with varying cosine similarity thresholds. Figure 2 (left) shows the relative performance of select baselines (the degree of CLIP score filtering has been tuned and only the best accuracy is plotted). We find that the best performance at medium scale, measured by either ImageNet or average accuracy, is achieved by mixing raw and synthetic captions, subject to a cosine similarity threshold. Besides, including BLIP2 captions in the training pool also improves retrieval performance by more than 2\\u00d7 (Table 2).\\n\\nIn the right plot of Figure 2, we compare ImageNet performance at various filtering thresholds for methods that involve only one source of captions and those that involve both. We observe that given image-raw-text pairs filtered with certain cosine similarity threshold (blue line), adding BLIP2 captions for some (red line) or all of the remaining images (green line) always helps. It is worth noting that as we lower the threshold and include more raw captions in the training mix, the performance starts to become lower than using just synthetic captions (orange line). Overall we find that filtering is still a necessary step even when using synthetic captions that are supposedly more relevant to the training images.\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: Trade-off between data diversity and repetition\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nTrade-off between data diversity and repetition\\n\\nRandomly selecting subsets has little effect and degrades performance with small fractions. Filtering with CLIP scores shows that selecting around 30% of the pool with the highest scores is optimal.\"]}}], \"child_spans\": null, \"span_kind\": null}, {\"span_id\": null, \"name\": \"rerank\", \"start_time_ms\": 1713829887923, \"end_time_ms\": 1713829888831, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"nodes\": [\"Header_1: DATACOMP Participant Workflow\\nHeader_2: Related Work\\nHeader_3: The DATACOMP benchmark\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nThe DATACOMP benchmark\\n\\nDATACOMP is meant to facilitate data-centric experimentation. While traditional benchmarks emphasize model design, DATACOMP is centered around dataset development, where the resulting datasets can be used to train high accuracy models. We focus on large image-text datasets and quantify a dataset submission by training a CLIP model on it from scratch [111] and evaluating on 38 downstream image classification and retrieval tasks. We additionally have three secret test sets, which will be released after a year, to guard against overfitting. To facilitate such investigations, we provide a candidate pool of uncurated image-text pairs sourced from the public internet. Our benchmark offers two tracks: one where participants must filter samples from the pools we provide, and another where participants can use external data. Moreover, DATACOMP is structured to accommodate participants with diverse levels of computational resources: each track is broken down into four scales with varying compute requirements. We now discuss high-level design decisions, construction of a 12.8B image-text data pool to facilitate the competition, benchmark tracks, model training, and evaluation.\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: Related Work\\nHeader_3: Competition design\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nCompetition design\\n\\nOverview. In many areas of machine learning, larger datasets lead to better performing models [87, 79, 73, 107, 66, 28, 19, 111, 112]. Hence comparing only datasets with the same size is a natural starting point. However, this approach is flawed as controlling the dataset size ignores critical curation constraints: candidate pool size (i.e., number of image-text pairs to harvest) and training compute. For instance, assembling a dataset like LAION-2B consists of identifying data sources (e.g., Common Crawl or Reddit) and filtering the data source. Notably, the final dataset size is a design choice and is only upper-bounded by the data sources. Hence, the true data constraint is the size of the reservoir of samples: candidate pool to be filtered. To make DATACOMP a realistic benchmark, we therefore fix the candidate pool in the filtering track, but give participants control over the training set size. Compute cost is another relevant constraint. To put datasets of different size on equal footing, we specify the total number of training samples seen. Consider the 12.8B compute scale and filtered datasets A and B, with 6.4B and 3.2B image-text pairs respectively. At this scale, we train by making two passes over A, while making four passes over B. A key result from our experiments is that smaller, more stringently filtered datasets can lead to models that generalize better.\\n\\nCompetition tracks. Two key procedures in assembling a training dataset are filtering a data source [128, 129, 20] and aggregating data sources [36, 37]. To reflect this structure, DATACOMP has two tracks: filtering, where participants select a subset of the samples from COMMONPOOL, and Bring Your Own Data (BYOD), where participants can use any source of data. Key decisions for each track are described in Sections 3.2 and 3.3, respectively. For full competition track rules see Appendix A.\\n\\nCompetition compute scales. To facilitate study of scaling trends and accommodate participants with various computational resources, we structure DATACOMP using four scales of compute: small, medium, large and xlarge. Each new scale increases the number of samples seen during training by 4.\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: Figure 1: DATACOMP participant workflow\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nFigure 1: DATACOMP participant workflow\\n\\nA) Choose a scale based on resource constraints.\\n\\nB) Design a dataset, in either the filtering or BYOD track.\\n\\nC) Train a CLIP model on the designed dataset using a fixed architecture and hyperparameters (Section 3.4).\\n\\nD) Evaluate the trained model on a suite of diverse downstream tasks (Section 3.5).\\n\\nCode for training and evaluating models at www.datacomp.ai.\\n\\nWe believe that our infrastructure will help put research on dataset design on rigorous empirical foundations, draw attention to this understudied research area, and lead to the next generation of multimodal datasets.\", \"Header_1: Appendix\\nHeader_2: Contents\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nContents\\n\\n1. Introduction - 1\\n2. Related Work - 3\\n3. The DATACOMP benchmark - 4\\n- Competition design - 4\\n- COMMONPOOL generation, for the filtering track - 5\\n- The bring your own data (BYOD) track - 5\\n- Training - 5\\n- Evaluation - 6\\n4. Baselines - 6\\n- Filtering baselines - 6\\n- BYOD baselines - 7\\n5. Results and discussion - 7\\n- Building better datasets - 7\\n- DATACOMP design analyses - 8\\n- Evaluation trends - 9\\n6. Limitations and conclusion - 9\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: COMMONPOOL generation, for the filtering track\\nHeader_3: Evaluation\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nEvaluation\\n\\nWe evaluate on a suite of 38 image classification and retrieval tasks. We also study two additional fairness tasks, detailed in Section 5 and Appendix Q. As discussed in Section 3.2, we remove test set images from DATACOMP to avoid contamination. Image classification datasets range from satellite imagery recognition to classifying metastatic tissues. In total we have (with some overlap): 22 of the datasets evaluated in Radford et al. [111], 6 ImageNet distribution shifts (i.e., ImageNet-Sketch [143], ImageNet-V2 [121], ImageNet-A [65], ImageNet-O [65], ImageNet-R [64], and ObjectNet [13]), 13 datasets from VTAB [156], and 3 datasets from WILDS [83, 127]. Retrieval datasets include Flickr30k [151], MSCOCO [26], and the WinoGAViL commonsense association task [17]. To aggregate results over all evaluation tasks, we average the preferred metric for each task.\\n\\nDATACOMP adopts a zero-shot evaluation protocol: models are tested without training on the evaluation tasks. This approach is computationally efficient and measures a model\\u2019s ability to perform well without any additional training. We find a strong rank correlation (>0.99) between performance in linear probe zero-shot settings (Appendix Figure 16). Additional details are in Appendix O.\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: COMMONPOOL generation, for the filtering track\\nHeader_3: 5 Results and discussion\\nHeader_4: 5.1 Building better datasets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\n5.1 Building better datasets\\n\\nMain results. Our key results are in Table 3. Most notably, the intersection between image-based filtering and CLIP score filtering excels on most tasks. The exception is at the small scale and for retrieval datasets.2 Furthermore, other filtering strategies like basic, CLIP score, image-based, text-based filtering show better downstream performance when compared to no filtering. A much larger suite of experiment results can be found in Appendix R.\\n\\nDATACOMP leads to better image-text datasets. We hope DATACOMP catalyzes the search for the next generation of multimodal datasets. We contribute DATACOMP-1B, which is the output of the Image-based \\u2229 CLIP score (L/14 30%) baseline filter at the xlarge scale of the filtering track.\\n\\n2Cherti et al. [28] also observe that models rank differently on classification and retrieval tasks.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Leadership and Advising\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nLeadership and Advising\\n\\nAdvising: Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\\n\\nLeadership: Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar\\n\\nOverall project lead: Ludwig Schmidt\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: A Benchmark rules\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nA Benchmark rules\\n\\nWe provide concrete rules below for the two competition tracks that comprise DATACOMP: filtering and BYOD. Additionally, we provide a checklist, which encourages participants to specify design decisions, which allows for more granular comparison between submissions.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: A Benchmark rules\\nHeader_3: A.2 Bring your own data track: amendments\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nA.2 Bring your own data track: amendments\\n\\nTo facilitate more open-ended exploration, we provide amendments to the Track 1 competition to allow for more diverse submissions in Track 2.\\n\\n- Participants are allowed to augment COMMONPOOL data with existing datasets, so long as these data sources do not contain test images from the evaluation tasks. Participants can use data from any COMMONPOOL; however, they are not required to do so.\\n- Assembling one\\u2019s own dataset is allowed; however, test images from the evaluation tasks can neither be contained nor otherwise used to construct said dataset. We encourage releasing the image urls or the images themselves in addition to the text for each image. We also encourage rigorous documentation of face-blurring and other data safety checks (see Section 3.2 for more details). We reserve the right to run our own safety code on participant provided data and disqualify entries that do not meet adequate safety standards.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nHeader_2: Abstract\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nAbstract\\n\\nMultimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DATACOMP, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DATACOMP workflow leads to better training sets. Our best baseline, DATACOMP-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI\\u2019s CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DATACOMP and all accompanying code at www.datacomp.ai.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Candidate pool\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nCandidate pool\\n\\nCandidate pool lead: Vaishaal Shankar\\n\\nData collection: Romain Beaumont, Vaishaal Shankar\\n\\nPre-processing and metadata: Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak Gadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar, George Smyrnis (face blurring lead)\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nContributions\\n\\nFor this section, contributors are ordered alphabetically.\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: DATACOMP design analyses\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDATACOMP design analyses\\n\\nCOMMONPOOL and LAION are comparable with the same filtering technique. The filtering approach on our pool yields comparable results to LAION-2B. Consistency across scales and training changes is also observed.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Participant tooling\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nParticipant tooling\\n\\nParticipant tooling lead: Gabriel Ilharco\\n\\nResharder: Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco, Vivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis\\n\\nTraining: Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell Wortsman (lead)\\n\\nEvaluation: Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco\\n\\nAdditional infrastructure: Stephen Mussmann, Sarah Pratt\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Baselines\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nBaselines\\n\\nBaselines lead: Yair Carmon\\n\\nFiltering track: Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco, Kalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu Zhang (co-lead)\\n\\nBYOD track: Gabriel Ilharco, Thao Nguyen\\n\\nExperiment babysitting: Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: COMMONPOOL generation, for the filtering track\\nHeader_3: Baselines\\nHeader_4: 4.2 BYOD baselines\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\n4.2 BYOD baselines\\n\\nWe experiment with multiple external data sources, including four moderately sized datasets (10 to 58M samples) studied by Nguyen et al. [101]\\u2014CC12M [24], YFCC15M [140, 111], RedCaps [38] and Shutterstock [101]\\u2014and the larger LAION-2B [129]. Additional experiments, along with more details about the data sources are provided in Appendix P.2. We consider these data sources as they are and do not perform additional preprocessing. We also present experiments combining some of the data sources (using only the external datasets, or in addition to data from our pool).\", \"Header_1: Document\\nHeader_2: Datasheet\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDatasheet\\n\\n|S.1 Motivation|60|\\n|---|---|\\n|S.2 Composition|60|\\n|S.3 Collection Process|62|\\n|S.4 Preprocessing, Cleaning, and/or Labeling|63|\\n|S.5 Uses|64|\\n|S.6 Distribution|65|\\n|S.7 Maintenance|65|\", \"Header_1: Evaluation Trends\\nHeader_2: ImageNet vs Average over 38 datasets\\nHeader_3: Limitations and Conclusion\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nLimitations and Conclusion\\n\\nIn terms of societal risks, creating an index of image-text pairs from the public internet can be problematic. The internet contains unsafe, toxic, and sensitive content, which ideally should not percolate into machine learning datasets. Though we take steps to remove NSFW content and blur human faces to protect privacy, we hope future work will further explore the biases and risks from COMMONPOOL and DATACOMP-1B. We see several additional directions for future work, including 1) Curating more data sources. 2) Improved data filtering algorithms. 3) Further supervision signals (e.g., image captions coming from captioning models). 4) Additional input modalities (e.g., video, 3D objects). 5) Broader evaluations for vision-and-language and robotics tasks.\\n\\nOverall, we see DATACOMP as a first step towards improving training datasets, and hope our new benchmark will foster further research. By providing a controlled experimental setting, DATACOMP enables researchers to iterate on dataset design on rigorous empirical foundations. We open-source all of our code, data, and infrastructure, and hope these resources will help the community build the next generation of multimodal datasets.\\n\\nNote that unlike Kornblith et al. [84] we evaluate zero-shot performance rather than transfer learning.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDATACOMP: In search of the next generation of multimodal datasets\\n\\n*Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt*\\n\\narXiv:2304.14108v5 [cs.CV] 20 Oct 2023\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: A Benchmark rules\\nHeader_3: Checklist\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nChecklist\\n\\nThe following checklist provides the basis for more fine-grained comparison between submissions.\\n\\n- \\u2610 Images from the evaluation tasks are included in my submission. If yes, please specify which datasets.\\n- \\u2610 I used an existing datasets (e.g., YFCC100M [140]) in my submission. If yes, please specify which datasets. (Note: applies to BYOD only)\\n- \\u2610 I curated my own data. If yes, please provide (1) image data or urls, (2) text for each image, (3) list of safety steps taken including but not limited to face blurring, explicit content image and text filtering. (Note: applies to BYOD only)\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: Figure 2: Performance of random subsets and CLIP score filtering\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nFigure 2: Performance of random subsets and CLIP score filtering\\n\\nWhen varying the subset size, larger subsets are always better for random subsets. For CLIP score filtering, subsets with intermediate size perform best.\\n\\nOur dataset is comprised of 1.4B samples, which is smaller than the LAION-2B dataset with 2.3B samples. However, a CLIP L/14 trained on DATACOMP-1B outperforms the LAION-2B competitor by 6.1 percentage points on ImageNet. Training on DATACOMP-1B also improves ImageNet accuracy by 3.7 percentage points over OpenAI\\u2019s ViT-L/14 trained with the same compute budget. Even with 400M samples, a subset of DATACOMP-1B outperforms OpenAI\\u2019s ViT-L/14.\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: External data sources impact\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nExternal data sources impact\\n\\nAdding external data sources improves performance. Combining CLIP-filtered data from COMMONPOOL with external data from CC12M, YFCC15M, RedCaps, and Shutterstock boosts ImageNet accuracy by 4.3 percentage points.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nHeader_2: Introduction\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nIntroduction\\n\\nRecent advances in multimodal learning such as CLIP, DALL-E, Stable Diffusion, Flamingo, and GPT-4 offer unprecedented generalization capabilities in zero-shot classification, image generation, and in-context learning. While these advances use different algorithmic techniques, e.g., contrastive learning, diffusion, or auto-regressive modeling, they all rest on a common foundation: large datasets containing paired image-text examples. For instance, CLIP\\u2019s training set contains 400 million image-text pairs, and Stable Diffusion was trained on the two billion examples from LAION-2B. This new generation of image-text datasets is 1,000 times larger than previous datasets such as ImageNet, which contains 1.2M images. Despite the central role of image-text datasets, little is known about them. Many state-of-the-art datasets are proprietary, and even for public datasets such as LAION-2B, it is unclear how design choices such as the data source or filtering techniques affect the resulting models. While there are thousands of ablation studies for algorithmic design choices (loss function, model architecture, etc.), datasets are often treated as monolithic artifacts without detailed investigation.\\n\\n*Equal contribution, randomly ordered. Correspondence to contact@datacomp.ai.\\n\\n1University of Washington 2Columbia University 3Tel Aviv University 4Apple 5UT Austin 6LAION 7AI2 8Juelich Supercomputing Center, Research Center Juelich 9University of Illinois Urbana-Champaign 10Graz University of Technology 11Hebrew University 12Google Research 13Snorkel AI\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\\n\\n|DATASET|Dataset size|# samples seen|Architecture|Train compute (MACs)|ImageNet accuracy|\\n|---|---|---|---|---|---|\\n|OpenAI\\u2019s WIT [111]|0.4B|13B|ViT-L/14|1.1 \\u00d7 1021|75.5|\\n|LAION-400M [128, 28]|0.4B|13B|ViT-L/14|1.1 \\u00d7 1021|72.8|\\n|LAION-2B [129, 28]|2.3B|13B|ViT-L/14|1.1 \\u00d7 1021|73.1|\\n|LAION-2B [129, 28]|2.3B|34B|ViT-H/14|6.5 \\u00d7 1021|78.0|\\n|LAION-2B [129, 28]|2.3B|34B|ViT-g/14|9.9 \\u00d7 1021|78.5|\\n|DATACOMP-1B (ours)|1.4B|13B|ViT-L/14|1.1 \\u00d7 1021|79.2|\\n\\ndatasets currently lack the benchmark-driven development process that has enabled a steady stream of improvements on the model side and isolates data enhancements from changes to the model. These issues impede further progress in multimodal learning, as evidenced by recent work showing that public datasets currently do not match the scaling behavior of proprietary alternatives [28].\\n\\nIn this paper, we take a step towards a more rigorous dataset development process. Our first and central contribution is DATACOMP, a new benchmark for multimodal dataset design. DATACOMP flips the traditional benchmarking paradigm in machine learning where the dataset is fixed and researchers propose new training algorithms. Instead, we hold the entire training code and computational budget constant so that participants innovate by proposing new training sets. To evaluate the quality of a training set, we score the resulting model with a testbed of 38 classification and retrieval tasks such as ImageNet [37], ImageNetV2 [121], DTD [30], EuroSAT [63], SUN-397 [146], and MSCOCO [26].\\n\\nDATACOMP focuses on two key challenges that arise when assembling large training datasets: what data sources to train on, and how to filter a given data source. Each challenge corresponds to one track in our benchmark. To facilitate the filtering track, our second contribution is COMMONPOOL, a dataset of 12.8B image-text pairs collected from Common Crawl and currently the largest public image-text dataset. We release CommonPool as an index of image url-text pairs under a CC-BY-4.0 license, and apply content checks in its construction to remove unsafe or unwanted content. In the filtering track, the goal of participants is to find the best subset of COMMONPOOL to train on. In the second track, Bring Your Own Data (BYOD), participants may leverage any data source, as long as it does not overlap with our evaluation testbed.\\n\\nOur third contribution is an investigation of scaling trends for dataset design. In particular, DATACOMP contains four scales, where we vary the training budget and the candidate pool size from 12.8M to 12.8B samples (see Table 2). Expressed in GPU hours, the cost of a single training run ranges from 4 to 40,000 GPU hours on the A100 cluster we used for development. The different scales enable researchers with different resources to participate in our benchmark. Moreover, our results show that the ranking of filtering approaches is largely consistent across scale.\\n\\nOur fourth contribution is over three hundred baseline experiments, including techniques such as querying captions for relevant keywords, filtering based on image embeddings, and applying a threshold on CLIP scores. A key result from our baselines experiments is that smaller, more stringently filtered datasets can lead to models that generalize better than larger datasets coming from the same pool. At the 12.8B scale, our best filtering baseline increases ImageNet zero-shot accuracy by 6.9 percentage points (pp) relative to the unfiltered pool (see Table 3). For the BYOD track, our initial experiments show that 109M additional data points (less than 1% of the 12.8B pool) improve the CLIP-filtered subsets of COMMONPOOL by up to 1.2 pp ImageNet accuracy (see Table 18).\\n\\nFinally, our fifth contribution is DATACOMP-1B, a new state-of-the-art multimodal dataset. We obtain DATACOMP-1B by combining our two most promising filtering baselines. DATACOMP-1B enables training a CLIP ViT-L/14 model to an ImageNet zero-shot accuracy of 79.2% (see Table 1), corresponding to a 9\\u00d7 computational cost reduction when compared to a larger CLIP ViT-g/14 model trained on LAION-2B for about 3\\u00d7 longer. Moreover, our model outperforms OpenAI\\u2019s original CLIP ViT-L/14 by 3.7 percentage points, while using the same compute budget.\\n\\nTo make DATACOMP a shared environment for controlled dataset experiments, we publicly release our candidate pool url index, our tooling for assembling these pools, our filtering baselines, and our\", \"Header_1: Quantitative Results on Text Infilling\\nHeader_2: Table 2: Quantitative results on text infilling\\nfilename: 35_discs_a_benchmark_for_discrete.pdf\\ntitle: DISCS: A Benchmark for Discrete Sampling\\n\\nTable 2: Quantitative results on text infilling\\n\\nThe reference text for computing the Corpus BLEU is the combination of WT103 and TBC.\\n\\n|Methods|Self-BLEU|n = 2 Self|n = 3 Self|n = 2 WT103|n = 3 WT103|n = 2 TBC|n = 3 TBC|Corpus BLEU|\\n|---|---|---|---|---|---|---|---|---|\\n|RMW|\\u221a|92.41|6.26|9.10|18.97|26.73|19.33|26.67|16.24|\\n|GWG|t|85.93|11.22|17.14|23.16|35.56|23.58|35.56|16.75|\\n|PAS|t|85.39|11.37|17.60|22.61|35.53|23.65|35.47|16.57|\\n|DLMCf|\\u221a t|88.39|9.53|14.06|21.00|31.85|22.27|31.98|16.70|\\n|DLMC|\\u221a t|85.28|12.05|17.65|24.03|36.34|24.51|36.27|16.45|\\n|GWG|t+1 t|81.15|15.47|22.70|25.62|38.91|25.62|38.58|16.68|\\n|DMALA|t t+1|80.21|16.36|23.71|25.60|39.39|26.75|39.72|16.53|\\n|PAS|t+1 t|81.02|15.62|22.65|25.59|39.28|26.08|39.48|16.69|\\n|DLMCf|t+1|80.12|16.25|23.76|25.41|39.31|26.86|39.57|16.73|\\n|DLMC|t+1|84.55|12.62|18.47|24.27|37.28|24.94|37.14|16.69|\\n\\nOptimization problems: DLMCf and DMALA have the best performance on language models. We believe more efficient discrete samplers can be obtained by designing better discretization of DLD (Sun et al., 2022a). DISCS is a convenient tool during this process. The researcher can freely set the configurations for tasks and samplers, and DISCS will automatically compile the program and run the processes in parallel. Besides, we observe that the choice of the locally balanced weight function should depend on the critical temperature of the target distribution. We believe this observation is insightful and will lead to a deeper understanding of locally balanced samplers.\\n\\nOf course, DISCS does not include all existing tasks or samplers in discrete sampling, for example, the zero order (Xiang et al., 2023) and second order (Sun et al., 2023a) approximation methods. We will keep iterating DISCS, and more features will be added in the future. We wrap DISCS to a JAX library. Researchers can conveniently implement customer tasks or samplers to accelerate their study and, in the meanwhile, contribute the code to DISCS for further improvement. We believe DISCS will be a powerful tool for researchers and facilitate the future research in discrete sampling.\", \"Header_1: Document\\nHeader_2: Baseline details\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nBaseline details\\n\\n|P.1 Filtering track|47|\\n|---|---|\\n|P.2 BYOD track|48|\\n|P.2.1 Additional results|49|\", \"Header_1: Appendix\\nHeader_2: Contents\\nHeader_3: B Contributions - 25\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nB Contributions - 25\\n\\n- Candidate pool - 25\\n- Participant tooling - 25\\n- Baselines - 25\\n- Leadership and Advising - 25\", \"Header_1: DataComp Results\\nHeader_2: DataComp Results\\nHeader_3: Appendix D\\nfilename: 2307.10350.pdf\\ntitle: Improving Multimodal Datasets with Image Captioning\\n\\nAppendix D\\n\\nTable 2: Performance with varying cosine similarity thresholds\\n\\n|Cosine Similarity Threshold|Accuracy|\\n|---|---|\\n|0.1|85%|\\n|0.2|90%|\\n|0.3|92%|\", \"Header_1: NSFW Filtering and Deduplication\\nHeader_2: Deduplication against evaluation sets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDeduplication against evaluation sets\\n\\nTo prevent data leakage, we filter COMMONPOOL by removing duplicate and near-duplicate matches of evaluation set images. See Figure 4 for example query images from Common Crawl and corresponding near-duplicates in our evaluations sets. We consider images as duplicates when\", \"Header_1: DataComp Results\\nHeader_2: DataComp Results\\nfilename: 2307.10350.pdf\\ntitle: Improving Multimodal Datasets with Image Captioning\\n\\nDataComp Results\\n\\nFigure 2: At the 128M scale of DataComp, we obtain improvement on ImageNet and average accuracies compared to the best filtering method on raw data, by using a mixture of raw and synthetic captions, selecting only image-text pairs with cosine similarity above a certain threshold. (Left) We visualize how various data filtering strategies perform at medium scale, on ImageNet and across 38 tasks. Including BLIP2 captions in the training data significantly outperforms competitive baselines from DataComp trained on only raw text [18]. (Right) As we vary the percentage of top examples chosen from the pool (based on CLIP score), we see consistent benefits from (i) using BLIP2 captions for samples that would be discarded otherwise, (ii) applying the same filtering threshold to new image-text pairs containing BLIP2 captions to keep noise level low. The exact accuracy numbers can be found in Appendix D.\\n\\nImageNet1k. The authors then find the intersection between this set of examples and those that are in the top x% based on CLIP score. This is the best baseline using raw captions on DataComp.\\n\\n- Combining raw and synthetic captions: we use raw captions for the top x% of examples based on CLIP score. For the remaining images (that would otherwise be filtered out), we generate corresponding BLIP2 captions and add them back to the training pool. We also experiment with filtering these additional image-text pairs with the same cosine similarity threshold set in the first step (i.e., BLIP2 (x%, filtered) in Figure 2).\\n\\nIn Appendix D, we investigate other baselines and report how well each approach does with varying cosine similarity thresholds. Figure 2 (left) shows the relative performance of select baselines (the degree of CLIP score filtering has been tuned and only the best accuracy is plotted). We find that the best performance at medium scale, measured by either ImageNet or average accuracy, is achieved by mixing raw and synthetic captions, subject to a cosine similarity threshold. Besides, including BLIP2 captions in the training pool also improves retrieval performance by more than 2\\u00d7 (Table 2).\\n\\nIn the right plot of Figure 2, we compare ImageNet performance at various filtering thresholds for methods that involve only one source of captions and those that involve both. We observe that given image-raw-text pairs filtered with certain cosine similarity threshold (blue line), adding BLIP2 captions for some (red line) or all of the remaining images (green line) always helps. It is worth noting that as we lower the threshold and include more raw captions in the training mix, the performance starts to become lower than using just synthetic captions (orange line). Overall we find that filtering is still a necessary step even when using synthetic captions that are supposedly more relevant to the training images.\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: Trade-off between data diversity and repetition\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nTrade-off between data diversity and repetition\\n\\nRandomly selecting subsets has little effect and degrades performance with small fractions. Filtering with CLIP scores shows that selecting around 30% of the pool with the highest scores is optimal.\"]}, \"outputs\": {\"reranked_nodes\": [\"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nHeader_2: Abstract\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nAbstract\\n\\nMultimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DATACOMP, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DATACOMP workflow leads to better training sets. Our best baseline, DATACOMP-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI\\u2019s CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release DATACOMP and all accompanying code at www.datacomp.ai.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nHeader_2: Introduction\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nIntroduction\\n\\nRecent advances in multimodal learning such as CLIP, DALL-E, Stable Diffusion, Flamingo, and GPT-4 offer unprecedented generalization capabilities in zero-shot classification, image generation, and in-context learning. While these advances use different algorithmic techniques, e.g., contrastive learning, diffusion, or auto-regressive modeling, they all rest on a common foundation: large datasets containing paired image-text examples. For instance, CLIP\\u2019s training set contains 400 million image-text pairs, and Stable Diffusion was trained on the two billion examples from LAION-2B. This new generation of image-text datasets is 1,000 times larger than previous datasets such as ImageNet, which contains 1.2M images. Despite the central role of image-text datasets, little is known about them. Many state-of-the-art datasets are proprietary, and even for public datasets such as LAION-2B, it is unclear how design choices such as the data source or filtering techniques affect the resulting models. While there are thousands of ablation studies for algorithmic design choices (loss function, model architecture, etc.), datasets are often treated as monolithic artifacts without detailed investigation.\\n\\n*Equal contribution, randomly ordered. Correspondence to contact@datacomp.ai.\\n\\n1University of Washington 2Columbia University 3Tel Aviv University 4Apple 5UT Austin 6LAION 7AI2 8Juelich Supercomputing Center, Research Center Juelich 9University of Illinois Urbana-Champaign 10Graz University of Technology 11Hebrew University 12Google Research 13Snorkel AI\\n\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\\n\\n|DATASET|Dataset size|# samples seen|Architecture|Train compute (MACs)|ImageNet accuracy|\\n|---|---|---|---|---|---|\\n|OpenAI\\u2019s WIT [111]|0.4B|13B|ViT-L/14|1.1 \\u00d7 1021|75.5|\\n|LAION-400M [128, 28]|0.4B|13B|ViT-L/14|1.1 \\u00d7 1021|72.8|\\n|LAION-2B [129, 28]|2.3B|13B|ViT-L/14|1.1 \\u00d7 1021|73.1|\\n|LAION-2B [129, 28]|2.3B|34B|ViT-H/14|6.5 \\u00d7 1021|78.0|\\n|LAION-2B [129, 28]|2.3B|34B|ViT-g/14|9.9 \\u00d7 1021|78.5|\\n|DATACOMP-1B (ours)|1.4B|13B|ViT-L/14|1.1 \\u00d7 1021|79.2|\\n\\ndatasets currently lack the benchmark-driven development process that has enabled a steady stream of improvements on the model side and isolates data enhancements from changes to the model. These issues impede further progress in multimodal learning, as evidenced by recent work showing that public datasets currently do not match the scaling behavior of proprietary alternatives [28].\\n\\nIn this paper, we take a step towards a more rigorous dataset development process. Our first and central contribution is DATACOMP, a new benchmark for multimodal dataset design. DATACOMP flips the traditional benchmarking paradigm in machine learning where the dataset is fixed and researchers propose new training algorithms. Instead, we hold the entire training code and computational budget constant so that participants innovate by proposing new training sets. To evaluate the quality of a training set, we score the resulting model with a testbed of 38 classification and retrieval tasks such as ImageNet [37], ImageNetV2 [121], DTD [30], EuroSAT [63], SUN-397 [146], and MSCOCO [26].\\n\\nDATACOMP focuses on two key challenges that arise when assembling large training datasets: what data sources to train on, and how to filter a given data source. Each challenge corresponds to one track in our benchmark. To facilitate the filtering track, our second contribution is COMMONPOOL, a dataset of 12.8B image-text pairs collected from Common Crawl and currently the largest public image-text dataset. We release CommonPool as an index of image url-text pairs under a CC-BY-4.0 license, and apply content checks in its construction to remove unsafe or unwanted content. In the filtering track, the goal of participants is to find the best subset of COMMONPOOL to train on. In the second track, Bring Your Own Data (BYOD), participants may leverage any data source, as long as it does not overlap with our evaluation testbed.\\n\\nOur third contribution is an investigation of scaling trends for dataset design. In particular, DATACOMP contains four scales, where we vary the training budget and the candidate pool size from 12.8M to 12.8B samples (see Table 2). Expressed in GPU hours, the cost of a single training run ranges from 4 to 40,000 GPU hours on the A100 cluster we used for development. The different scales enable researchers with different resources to participate in our benchmark. Moreover, our results show that the ranking of filtering approaches is largely consistent across scale.\\n\\nOur fourth contribution is over three hundred baseline experiments, including techniques such as querying captions for relevant keywords, filtering based on image embeddings, and applying a threshold on CLIP scores. A key result from our baselines experiments is that smaller, more stringently filtered datasets can lead to models that generalize better than larger datasets coming from the same pool. At the 12.8B scale, our best filtering baseline increases ImageNet zero-shot accuracy by 6.9 percentage points (pp) relative to the unfiltered pool (see Table 3). For the BYOD track, our initial experiments show that 109M additional data points (less than 1% of the 12.8B pool) improve the CLIP-filtered subsets of COMMONPOOL by up to 1.2 pp ImageNet accuracy (see Table 18).\\n\\nFinally, our fifth contribution is DATACOMP-1B, a new state-of-the-art multimodal dataset. We obtain DATACOMP-1B by combining our two most promising filtering baselines. DATACOMP-1B enables training a CLIP ViT-L/14 model to an ImageNet zero-shot accuracy of 79.2% (see Table 1), corresponding to a 9\\u00d7 computational cost reduction when compared to a larger CLIP ViT-g/14 model trained on LAION-2B for about 3\\u00d7 longer. Moreover, our model outperforms OpenAI\\u2019s original CLIP ViT-L/14 by 3.7 percentage points, while using the same compute budget.\\n\\nTo make DATACOMP a shared environment for controlled dataset experiments, we publicly release our candidate pool url index, our tooling for assembling these pools, our filtering baselines, and our\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: Related Work\\nHeader_3: The DATACOMP benchmark\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nThe DATACOMP benchmark\\n\\nDATACOMP is meant to facilitate data-centric experimentation. While traditional benchmarks emphasize model design, DATACOMP is centered around dataset development, where the resulting datasets can be used to train high accuracy models. We focus on large image-text datasets and quantify a dataset submission by training a CLIP model on it from scratch [111] and evaluating on 38 downstream image classification and retrieval tasks. We additionally have three secret test sets, which will be released after a year, to guard against overfitting. To facilitate such investigations, we provide a candidate pool of uncurated image-text pairs sourced from the public internet. Our benchmark offers two tracks: one where participants must filter samples from the pools we provide, and another where participants can use external data. Moreover, DATACOMP is structured to accommodate participants with diverse levels of computational resources: each track is broken down into four scales with varying compute requirements. We now discuss high-level design decisions, construction of a 12.8B image-text data pool to facilitate the competition, benchmark tracks, model training, and evaluation.\", \"Header_1: Evaluation Trends\\nHeader_2: ImageNet vs Average over 38 datasets\\nHeader_3: Limitations and Conclusion\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nLimitations and Conclusion\\n\\nIn terms of societal risks, creating an index of image-text pairs from the public internet can be problematic. The internet contains unsafe, toxic, and sensitive content, which ideally should not percolate into machine learning datasets. Though we take steps to remove NSFW content and blur human faces to protect privacy, we hope future work will further explore the biases and risks from COMMONPOOL and DATACOMP-1B. We see several additional directions for future work, including 1) Curating more data sources. 2) Improved data filtering algorithms. 3) Further supervision signals (e.g., image captions coming from captioning models). 4) Additional input modalities (e.g., video, 3D objects). 5) Broader evaluations for vision-and-language and robotics tasks.\\n\\nOverall, we see DATACOMP as a first step towards improving training datasets, and hope our new benchmark will foster further research. By providing a controlled experimental setting, DATACOMP enables researchers to iterate on dataset design on rigorous empirical foundations. We open-source all of our code, data, and infrastructure, and hope these resources will help the community build the next generation of multimodal datasets.\\n\\nNote that unlike Kornblith et al. [84] we evaluate zero-shot performance rather than transfer learning.\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: Figure 1: DATACOMP participant workflow\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nFigure 1: DATACOMP participant workflow\\n\\nA) Choose a scale based on resource constraints.\\n\\nB) Design a dataset, in either the filtering or BYOD track.\\n\\nC) Train a CLIP model on the designed dataset using a fixed architecture and hyperparameters (Section 3.4).\\n\\nD) Evaluate the trained model on a suite of diverse downstream tasks (Section 3.5).\\n\\nCode for training and evaluating models at www.datacomp.ai.\\n\\nWe believe that our infrastructure will help put research on dataset design on rigorous empirical foundations, draw attention to this understudied research area, and lead to the next generation of multimodal datasets.\", \"Header_1: Appendix\\nHeader_2: Contents\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nContents\\n\\n1. Introduction - 1\\n2. Related Work - 3\\n3. The DATACOMP benchmark - 4\\n- Competition design - 4\\n- COMMONPOOL generation, for the filtering track - 5\\n- The bring your own data (BYOD) track - 5\\n- Training - 5\\n- Evaluation - 6\\n4. Baselines - 6\\n- Filtering baselines - 6\\n- BYOD baselines - 7\\n5. Results and discussion - 7\\n- Building better datasets - 7\\n- DATACOMP design analyses - 8\\n- Evaluation trends - 9\\n6. Limitations and conclusion - 9\", \"Header_1: DATACOMP Analysis\\nHeader_2: ImageNet Average over 38 datasets\\nHeader_3: DATACOMP design analyses\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDATACOMP design analyses\\n\\nCOMMONPOOL and LAION are comparable with the same filtering technique. The filtering approach on our pool yields comparable results to LAION-2B. Consistency across scales and training changes is also observed.\", \"Header_1: DATACOMP Participant Workflow\\nHeader_2: COMMONPOOL generation, for the filtering track\\nHeader_3: 5 Results and discussion\\nHeader_4: 5.1 Building better datasets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\n5.1 Building better datasets\\n\\nMain results. Our key results are in Table 3. Most notably, the intersection between image-based filtering and CLIP score filtering excels on most tasks. The exception is at the small scale and for retrieval datasets.2 Furthermore, other filtering strategies like basic, CLIP score, image-based, text-based filtering show better downstream performance when compared to no filtering. A much larger suite of experiment results can be found in Appendix R.\\n\\nDATACOMP leads to better image-text datasets. We hope DATACOMP catalyzes the search for the next generation of multimodal datasets. We contribute DATACOMP-1B, which is the output of the Image-based \\u2229 CLIP score (L/14 30%) baseline filter at the xlarge scale of the filtering track.\\n\\n2Cherti et al. [28] also observe that models rank differently on classification and retrieval tasks.\"]}}], \"child_spans\": null, \"span_kind\": null}, {\"span_id\": null, \"name\": \"answer\", \"start_time_ms\": 1713829888831, \"end_time_ms\": 1713829892499, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": null, \"outputs\": {\"answer\": \"DATACOMP is a new benchmark initiative focused on the development of multimodal datasets, particularly emphasizing the design and curation of large-scale image-text pairs to train models like CLIP [1]. It introduces a unique approach by allowing participants to either filter a provided dataset or bring their own data, aiming to enhance the quality and effectiveness of training datasets in achieving higher model accuracies on various downstream tasks [2].\"}}], \"child_spans\": null, \"span_kind\": null}], \"span_kind\": \"AGENT\"}"}, "_timestamp": 1713829892.682642, "_runtime": 34.717926025390625, "_step": 0}