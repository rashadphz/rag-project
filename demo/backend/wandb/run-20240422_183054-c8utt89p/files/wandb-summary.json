{"chat-trace": {"_type": "wb_trace_tree", "root_span_dumps": "{\"span_id\": null, \"name\": \"chat\", \"start_time_ms\": 1713828681402, \"end_time_ms\": 1713828691581, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"original_question\": \"who is dimakis\", \"history\": []}, \"outputs\": null}, {\"inputs\": {\"original_question\": \"who is dimakis\", \"history\": []}, \"outputs\": {\"final_answer\": \"Alexandros G. Dimakis is a prominent figure in the field of computer science, specifically known for his contributions to machine learning, signal processing, and data science. He is frequently involved in research related to diffusion models, generative models, and solving inverse problems. Dimakis is affiliated with the University of Texas at Austin, where he serves in the Department of Electrical and Computer Engineering [3], [4], [5]. His work often explores innovative methods for data reconstruction and generative modeling, making significant impacts in the areas of compressed sensing and machine learning [2], [5].\"}}], \"child_spans\": [{\"span_id\": null, \"name\": \"rephrase\", \"start_time_ms\": 1713828681402, \"end_time_ms\": 1713828681403, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"original_question\": \"who is dimakis\"}, \"outputs\": {\"rephrased_question\": \"who is dimakis\"}}], \"child_spans\": null, \"span_kind\": null}, {\"span_id\": null, \"name\": \"select-tool\", \"start_time_ms\": 1713828681402, \"end_time_ms\": 1713828683761, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"question\": \"who is dimakis\"}, \"outputs\": {\"tool\": \"retrieval\"}}], \"child_spans\": null, \"span_kind\": null}, {\"span_id\": null, \"name\": \"retrieval\", \"start_time_ms\": 1713828681402, \"end_time_ms\": 1713828684431, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"question\": \"who is dimakis\"}, \"outputs\": {\"nodes\": [\"Header_1: Research Paper Summary\\nHeader_2: Acknowledgments\\nfilename: 2302.09057.pdf\\ntitle: Consistent Diffusion Models\\n\\nAcknowledgments\\n\\nThis research has been supported by NSF Grants CCF 1763702, AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Archie Straiton Endowed Faculty Fellowship. Giannis Daras has been supported by the Onassis Fellowship, the Bodossaki Fellowship and the Leventis Fellowship. Constantinos Daskalakis has been supported by NSF Awards CCF-1901292, DMS-2022448 and DMS2134108, a Simons Investigator Award, the Simons Collaboration on the Theory of Algorithmic Fairness and a DSTA grant.\", \"Header_1: Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\\nfilename: 2302.09057.pdf\\ntitle: Consistent Diffusion Models\\n\\nConsistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\\n\\nGiannis Daras* - Department of Computer Science, University of Texas at Austin\\n\\nYuval Dagan* - Electrical Engineering and Computer Science, University of California, Berkeley\\n\\nAlexandros G. Dimakis - Department of ECE, University of Texas at Austin\\n\\nConstantinos Daskalakis - Electrical Engineering and Computer Science, Massachusetts Institute of Technology\\n\\narXiv:2302.09057v1 [cs.LG] 17 Feb 2023\\n\\nFebruary 20, 2023\", \"Header_1: Acknowledgements and References\\nHeader_2: Acknowledgements\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nAcknowledgements\\n\\nThis research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.\", \"Header_1: Acknowledgements\\nHeader_2: References\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nReferences\\n\\n1. Brian D.O. Anderson. \\u201cReverse-time diffusion equation models\\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\\u2013326 (page 1).\\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \\u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 2).\\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \\u201cCold Diffusion: Inverting arbitrary image transforms without noise\\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 2).\\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \\u201cAlign your latents: High-resolution video synthesis with latent diffusion models\\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\\u201322575 (page 3).\\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \\u201cCompressed sensing using generative models\\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\\u2013546 (page 1).\\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \\u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\\u201398 (pages 2, 8, 24, 27).\\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \\u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (pages 5, 6).\\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \\u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 1).\\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \\u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 1).\\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \\u201cIlvr: Conditioning method for denoising diffusion probabilistic models\\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 2).\\n11. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \\u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=OnD9zGAGT0k (pages 1\\u20133, 7\\u201310, 15, 18, 20, 22, 24\\u201331).\\n12. Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \\u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 2).\\n13. Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \\u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=nJJjv0JDJju (pages 2, 8, 24, 27, 31).\\n14. Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \\u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 2).\", \"Header_1: Acknowledgements\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nAcknowledgements\\n\\nThis research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.\", \"Header_1: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\nHeader_2: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\nfilename: 2305.19256.pdf\\ntitle: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\n\\nAmbient Diffusion: Learning Clean Distributions from Corrupted Data\\n\\nGiannis Daras (UT Austin), Kulin Shah (UT Austin), Yuval Dagan (UC Berkeley)\\n\\nEmail: giannisdaras@utexas.edu, kulinshah@utexas.edu, yuvald@berkeley.edu\\n\\nAravind Gollakota, Alexandros G. Dimakis, Adam Klivans (UT Austin)\\n\\nEmail: aravindg@cs.utexas.edu, dimakis@austin.utexas.edu, klivans@utexas.edu\", \"Header_1: List of References\\nfilename: 2305.11765.pdf\\ntitle: Tester-Learners for Halfspaces: Universal Algorithms\\n\\nList of References\\n\\n|[BK21]|Ainesh Bakshi and Pravesh K Kothari. List-decodable subspace recovery: Dimension independent error in polynomial time. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1279\\u20131297. SIAM, 2021.|\\n|---|---|\\n|[BZ17]|Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient learning algorithms under s-concave distributions. Advances in Neural Information Processing Systems, 30, 2017.|\\n|[Dan15]|Amit Daniely. A ptas for agnostically learning halfspaces. In Conference on Learning Theory, pages 484\\u2013502. PMLR, 2015.|\\n|[DKK+22]|Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with general massart noise under the gaussian distribution. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 874\\u2013885, 2022.|\\n|[DKK+23]|Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Efficient testable learning of halfspaces with adversarial label noise. arXiv preprint arXiv:2303.05485, 2023.|\\n|[DKPZ21]|Ilias Diakonikolas, Daniel M Kane, Thanasis Pittas, and Nikos Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model. In Conference on Learning Theory, pages 1552\\u20131584. PMLR, 2021.|\\n|[DKR23]|Ilias Diakonikolas, Daniel M Kane, and Lisheng Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. arXiv preprint arXiv:2302.06512, 2023.|\\n|[DKS18]|Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061\\u20131073, 2018.|\\n|[DKTZ20a]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning half-spaces with massart noise under structured distributions. In Conference on Learning Theory, pages 1486\\u20131513. PMLR, 2020.|\\n|[DKTZ20b]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex sgd learns halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 33:18540\\u201318549, 2020.|\\n|[DKTZ22]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with adversarial label noise via online gradient descent. In International Conference on Machine Learning, pages 5118\\u20135141. PMLR, 2022.|\\n|[DKZ20]|Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. Advances in Neural Information Processing Systems, 33:13586\\u201313596, 2020.|\\n|[FKP+19]|Noah Fleming, Pravesh Kothari, Toniann Pitassi, et al. Semialgebraic proofs and efficient algorithm design. Foundations and Trends\\u00ae in Theoretical Computer Science, 14(1-2):1\\u2013221, 2019.|\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Leadership and Advising\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nLeadership and Advising\\n\\nAdvising: Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\\n\\nLeadership: Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar\\n\\nOverall project lead: Ludwig Schmidt\", \"Header_1: Acknowledgements and References\\nHeader_2: References\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nReferences\\n\\n1. Brian D.O. Anderson. \\u201cReverse-time diffusion equation models\\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\\u2013326 (page 1).\\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \\u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 3).\\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \\u201cCold Diffusion: Inverting arbitrary image transforms without noise\\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 3).\\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \\u201cAlign your latents: High-resolution video synthesis with latent diffusion models\\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\\u201322575 (page 3).\\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \\u201cCompressed sensing using generative models\\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\\u2013546 (page 1).\\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \\u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\\u201398 (pages 3, 9, 30).\\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \\u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (page 6).\\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \\u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 3).\\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \\u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 3).\\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \\u201cIlvr: Conditioning method for denoising diffusion probabilistic models\\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 3).\", \"Header_1: Slice database with cluster construction dim (d_cluster)\\nfilename: 2305.19435.pdf\\ntitle: AdANNS: A Framework for Adaptive Semantic Search\\n\\nSlice database with cluster construction dim (d_cluster)\\nxb = database[:d_cluster]\\ncluster_centroids = constructClusters(xb, num_clusters)\\nreturn cluster_centroids\\ndef adannsInference(queries, centroids, d_shortlist, d_search, num_probes,\\nk):\", \"Header_1: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nHeader_2: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nSolving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nLitu Rout \\u2217 Negin Raoof \\u2020 Giannis Daras \\u2021\\n\\nConstantine Caramanis \\u00a7 Alexandros G. Dimakis \\u00b6 Sanjay Shakkottai \\u2016\\n\\nThe University of Texas at Austin\", \"Header_1: Document\\nHeader_2: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nSolving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nLitu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai\\n\\nThe University of Texas at Austin*\", \"Header_1: Slice queries and centroids with cluster shortlist dim (d_shortlist)\\nfilename: 2305.19435.pdf\\ntitle: AdANNS: A Framework for Adaptive Semantic Search\\n\\nSlice queries and centroids with cluster shortlist dim (d_shortlist)\\nxq = queries[:d_shortlist]\\nxc = centroids[:d_shortlist]\\nfor q in queries:\", \"Header_1: References\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nReferences\\n\\n|#|Authors|Title|Publication Details|Pages|\\n|---|---|---|---|---|\\n|15|Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar|Soft diffusion: Score matching for general corruptions|arXiv preprint arXiv:2209.05442 (2022)|page 2|\\n|16|Mauricio Delbracio and Peyman Milanfar|Inversion by direct iteration: An alternative to denoising diffusion for image restoration|arXiv preprint arXiv:2303.11435 (2023)|page 2|\\n|17|Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei|Imagenet: A large-scale hierarchical image database|2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\\u2013255|pages 7, 21, 22, 27\\u201329|\\n|18|Prafulla Dhariwal and Alexander Nichol|Diffusion models beat gans on image synthesis|Advances in Neural Information Processing Systems 34 (2021), pp. 8780\\u20138794|page 1|\\n|19|Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.|DataComp: In search of the next generation of multimodal datasets|arXiv preprint arXiv:2304.14108 (2023)|page 10|\\n\\n$$\\n\\\\begin{array}{|c|c|c|c|c|}\\n\\\\hline\\n\\\\text{#} & \\\\text{Authors} & \\\\text{Title} & \\\\text{Publication Details} & \\\\text{Pages} \\\\\\\\\\n\\\\hline\\n20 & Jonathan Ho, Ajay Jain, and Pieter Abbeel & Denoising diffusion probabilistic models & Advances in Neural Information Processing Systems 33 (2020), pp. 6840\\u20136851 & page 1 \\\\\\\\\\n21 & Aapo Hyv\\u00e4rinen and Peter Dayan & Estimation of non-normalized statistical models by score matching & Journal of Machine Learning Research 6.4 (2005) & page 1 \\\\\\\\\\n22 & Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir & Robust compressed sensing mri with deep generative priors & Advances in Neural Information Processing Systems 34 (2021), pp. 14938\\u201314954 & pages 1, 2 \\\\\\\\\\n23 & Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price & Instance-optimal compressed sensing via posterior sampling & arXiv preprint arXiv:2106.11438 (2021) & page 1 \\\\\\\\\\n24 & Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price & Fairness for Image Generation with Uncertain Sensitive Attributes & Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\\u201324 Jul 2021, pp. 4721\\u20134732 & page 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n$$\", \"Header_1: Math Equations and Text\\nHeader_2: For part (b), we assume that D is indeed \\u03bb-nice and \\u03b3-Poincar&eacute;. We first use Proposition A.3 as well as a Hoeffding bound, to obtain that $$P_{x\\\\in S}[|\\\\langle w, x\\\\rangle| \\\\leq 2\\\\sigma] \\\\in [C'\\\\lambda C', 2\\\\sigma \\\\cdot C'\\\\lambda C']$$ with probability at least 1\\u2212\\u03b4/3 over S (since |S| is large enough), for some universal constant C' > 0. Then, we use $$4\\\\sigma$$ part (ii) of Proposition A.3 to lower bound the minimum eigenvalue of MD = ED[MS] by $$C'\\\\lambda C'$$. Using part (iii) of Proposition A.3 to bound the second moment of each of the elements of MD, we may use Proposition A.2 to obtain that MS \\u2ab0 $$C'\\\\lambda C' Id-1$$ (and our spectral test passes) with probability at least 1 \\u2212 $$\\\\delta/3$$. It remains to show that the hypercontractivity tester will accept with probability at least 1 \\u2212 $$\\\\delta/3$$ (since, then, the result follows from a union bound).\\nfilename: 2305.11765.pdf\\ntitle: Tester-Learners for Halfspaces: Universal Algorithms\\n\\nFor part (b), we assume that D is indeed \\u03bb-nice and \\u03b3-Poincar&eacute;. We first use Proposition A.3 as well as a Hoeffding bound, to obtain that $$P_{x\\\\in S}[|\\\\langle w, x\\\\rangle| \\\\leq 2\\\\sigma] \\\\in [C'\\\\lambda C', 2\\\\sigma \\\\cdot C'\\\\lambda C']$$ with probability at least 1\\u2212\\u03b4/3 over S (since |S| is large enough), for some universal constant C' > 0. Then, we use $$4\\\\sigma$$ part (ii) of Proposition A.3 to lower bound the minimum eigenvalue of MD = ED[MS] by $$C'\\\\lambda C'$$. Using part (iii) of Proposition A.3 to bound the second moment of each of the elements of MD, we may use Proposition A.2 to obtain that MS \\u2ab0 $$C'\\\\lambda C' Id-1$$ (and our spectral test passes) with probability at least 1 \\u2212 $$\\\\delta/3$$. It remains to show that the hypercontractivity tester will accept with probability at least 1 \\u2212 $$\\\\delta/3$$ (since, then, the result follows from a union bound).\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Candidate pool\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nCandidate pool\\n\\nCandidate pool lead: Vaishaal Shankar\\n\\nData collection: Romain Beaumont, Vaishaal Shankar\\n\\nPre-processing and metadata: Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak Gadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar, George Smyrnis (face blurring lead)\", \"Header_1: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nHeader_2: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nHeader_3: Introduction\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nIntroduction\\n\\nWe study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process; see also the survey of Ongie et al. and references therein.\\n\\nThe second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\\n\\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution p0(x0) using an It\\u00f4 Stochastic Differential Equation (SDE), dx = f(x, t)dt + g(t)dw, and then by learning the score-function, \\u2207xt log pt(xt), at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\\u00f4 SDE. The SDE that corrupts\\n\\nContact:\\n\\n- litu.rout@utexas.edu\\n- neginmr@utexas.edu\\n- giannisdaras@utexas.edu\\n- constantine@utexas.edu\\n- dimakis@austin.utexas.edu\\n- sanjay.shakkottai@utexas.edu\", \"Header_1: Acknowledgments and References\\nHeader_2: References\\nfilename: 2307.01178.pdf\\ntitle: Learning Mixtures of Gaussians Using the DDPM Objective\\n\\nReferences\\n\\n|[AG22]|Ahmed El Alaoui and Jason Gaitonde. Bounds on the covariance matrix of the Sherrington-Kirkpatrick model. arXiv preprint arXiv:2212.02445, 2022.|\\n|---|---|\\n|[BDD23]|Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. arXiv preprint arXiv:2305.16860, 2023.|\\n|[BDJ+22]|Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234\\u20131247, 2022.|\\n|[BKM17]|David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\\u2013877, 2017.|\\n|[BM11]|Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\\u2013785, 2011.|\\n|[BMR22]|Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. arXiv preprint 2002.00107, 2022.|\\n|[BRST21]|Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous LWE. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 694\\u2013707, 2021.|\\n|[BS15]|Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. SIAM Journal on Computing, 44(4):889\\u2013911, 2015.|\\n|[BWY17]|Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. 2017.|\\n|[CCL+23a]|Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. arXiv preprint arXiv:2305.11798, 2023.|\\n|[CCL+23b]|Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.|\\n|[CDD23]|Sitan Chen, Giannis Daras, and Alexandros G Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. arXiv preprint arXiv:2303.03384, 2023.|\\n\\n- [Cel22] Michael Celentano. Sudakov-fernique post-amp, and a new proof of the local convexity\\nof the tap free energy. arXiv preprint arXiv:2208.09550, 2022.\\n- [CFM21] Michael Celentano, Zhou Fan, and Song Mei. Local convexity of the tap free energy\\nand amp convergence for z2-synchronization. arXiv preprint arXiv:2106.11428, 2021.\\n- [CLL22] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based gener-\\native modeling: user-friendly bounds under minimal smoothness assumptions. arXiv\\npreprint arXiv:2211.01916, 2022.\\n- [DB22] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold\\nhypothesis. Transactions on Machine Learning Research, 2022.\\n- [DBTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion\\nSchr\\u00f6dinger bridge with applications to score-based generative modeling. In M. Ran-\\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\\nAdvances in Neural Information Processing Systems, volume 34, pages 17695\\u201317709.\\nCurran Associates, Inc., 2021.\\n- [DHKK20] Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly\\nlearning any clusterable mixture of gaussians. arXiv preprint arXiv:2005.06417, 2020.\\n- [DK20] Ilias Diakonikolas and Daniel M Kane. Small covers for near-zero sets of polynomi-\\nals and learning latent variable models. In 2020 IEEE 61st Annual Symposium on\\nFoundations of Computer Science (FOCS), pages 184\\u2013195. IEEE, 2020.\\n- [DKS17] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower\\nbounds for robust estimation of high-dimensional gaussians and gaussian mixtures.\\nIn 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),\\npages 73\\u201384. IEEE, 2017.\\n- [DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean\\nestimation and learning mixtures of spherical gaussians. In Proceedings of the 50th\\nAnnual ACM SIGACT Symposium on Theory of Computing, pages 1047\\u20131060, 2018.\\n- [DMM09] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algo-\\nrithms for compressed sensing. Proceedings of the National Academy of Sciences,\\n106(45):18914\\u201318919, 2009.\\n- [DMM10] David L Donoho, Arian Maleki, and Andrea Montanari. Message passing algorithms\\nfor compressed sensing: I. motivation and construction. In 2010 IEEE information\\ntheory workshop on information theory (ITW 2010, Cairo), pages 1\\u20135. IEEE, 2010.\\n- [DS07] Sanjoy Dasgupta and Leonard J Schulman. A probabilistic analysis of em for mixtures\\nof separated, spherical gaussians. Journal of Machine Learning Research, 8:203\\u2013226,\\n2007.\\n- [DTZ17] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of\\nem suffice for mixtures of two gaussians. In Conference on Learning Theory, pages\\n704\\u2013710. PMLR, 2017.\", \"Header_1: List of References\\nfilename: 2306.10615.pdf\\ntitle: Agnostically Learning Single-Index Models using Omnipredictors\\n\\nList of References\\n\\n|Reference|Authors|Title|Publication Details|Sections|\\n|---|---|---|---|---|\\n|[DKMR22]|Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, Lisheng Ren|Hardness of learning a single neuron with adversarial label noise|In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 8199\\u20138213. PMLR, 28\\u201330 Mar 2022.|1, 1.1, 5, 5.1|\\n|[DKPZ21]|Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis|The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model|In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 1552\\u20131584. PMLR, 15\\u201319 Aug 2021.|1, 1.1|\\n|[DKTZ20]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis|Non-convex sgd learns halfspaces with adversarial label noise|In Advances in Neural Information Processing Systems, volume 33, pages 18540\\u201318549. Curran Associates, Inc., 2020.|4|\\n|[DKTZ22]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis|Learning a single neuron with adversarial label noise via gradient descent|In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 4313\\u20134361. PMLR, 02\\u201305 Jul 2022.|1, 1, 1.1, 4|\\n|[DKZ20]|Ilias Diakonikolas, Daniel Kane, Nikos Zarifis|Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals|In Advances in Neural Information Processing Systems, 33:13586\\u201313596, 2020.|1, 1.1|\\n|[FCG20]|Spencer Frei, Yuan Cao, Quanquan Gu|Agnostic learning of a single neuron with gradient descent|In Advances in Neural Information Processing Systems, 33:5417\\u20135428, 2020.|1, 1.1|\\n|[GGK20]|Surbhi Goel, Aravind Gollakota, Adam Klivans|Statistical-query lower bounds via functional gradients|In Advances in Neural Information Processing Systems, 33:2147\\u20132158, 2020.|1, 1.1|\\n|[GHK+23]|Parikshit Gopalan, Lunjia Hu, Michael P. Kim, Omer Reingold, Udi Wieder|Loss minimization through the lens of outcome indistinguishability|In 14th Innovations in Theoretical Computer Science Conference, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA, volume 251 of LIPIcs, pages 60:1\\u201360:20. Schloss Dagstuhl - Leibniz-Zentrum f\\u00fcr Informatik, 2023.|1, 1.1, 3, 3, 3.2, C.1, C.2, C.1|\\n|[GKK19]|Surbhi Goel, Sushrut Karmalkar, Adam Klivans|Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals|In Advances in neural information processing systems, 32, 2019.|1, 1.1|\\n|[GKKT17]|Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler|Reliably learning the relu in polynomial time|In Conference on Learning Theory, pages 1004\\u20131042. PMLR, 2017.|1.1|\", \"Header_1: Acknowledgements\\nHeader_2: Acknowledgements\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nAcknowledgements\\n\\nSYG and JH are supported by NSF Graduate Research Fellowships. GS is supported by the Onassis Foundation - Scholarship ID: F ZS 056-1/2022-2023. GD has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. This research has been supported by NSF Grants AF 1901292, CNS 2148141, DMS 2134012, TRIPODS II-DMS 2023166, Tripods CCF 1934932, IFML CCF 2019844 and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco, the Len Blavatnik and the Blavatnik Family Foundation, the Stanly P. Finch Centennial Professorship in Engineering, Open Philanthropy, Google, Microsoft, and the Allen Institute for AI.\\n\\nWe would like to thank Amro Abbas, Danny Bickson, Alper Canberk, Jessie Chapman, Brian Cheung, Tim Dettmers, Joshua Gardner, Nancy Garland, Sachin Goyal, Huy Ha, Zaid Harchaoui, Ari Holtzman, Andrew Hundt, Andy Jones, Adam Klivans, Ronak Mehta, Sachit Menon, Ari Morcos, Raviteja Mullapudi, Jonathon Shlens, Brandon McKinzie, Alexander Toshev, David Grangier, Navdeep Jaitly, Kentrell Owens, Marco Tulio Ribeiro, Shiori Sagawa, Christoph Schuhmann, Matthew Wallingford, and Ross Wightman for helpful feedback at various stages of the project. We are particularly grateful to Daniel Levy and Alec Radford for early encouragement to pursue this project and feedback on the experimental design.\\n\\nWe thank Stability AI and the Gauss Centre for Supercomputing e.V. for providing us with compute resources to train models. We are thankful for the compute time provided through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\\u00fclich Supercomputing Centre (JSC), and for storage resources on JUST granted and operated by JSC, as well as computing and storage resources from the Helmholtz Data Federation (HDF).\\n\\nGauss Centre for Supercomputing e.V.\", \"Header_1: DISCS: A Benchmark for Discrete Sampling\\nfilename: 35_discs_a_benchmark_for_discrete.pdf\\ntitle: DISCS: A Benchmark for Discrete Sampling\\n\\nDISCS: A Benchmark for Discrete Sampling\\n\\nAnonymous Author(s)\\n\\nAffiliation\\n\\nAddress\\n\\nemail\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Baselines\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nBaselines\\n\\nBaselines lead: Yair Carmon\\n\\nFiltering track: Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco, Kalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu Zhang (co-lead)\\n\\nBYOD track: Gabriel Ilharco, Thao Nguyen\\n\\nExperiment babysitting: Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre\", \"Header_1: Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\\nHeader_2: Abstract\\nfilename: 2302.09057.pdf\\ntitle: Consistent Diffusion Models\\n\\nAbstract\\n\\nImperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a consistency property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm.\", \"Header_1: Technical Proofs\\nHeader_2: Technical Proofs\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nTechnical Proofs\\n\\nNotation and Measurement Matrix. We elaborate on the structure of the measurement matrix $$A \\\\in \\\\mathbb{R}^{l \\\\times d}$$. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the $$d \\\\times d$$ identity matrix (the rows that are present correspond to the indices of the selected pixels from the image $$x_0 \\\\in \\\\mathbb{R}^d$$). Given this structure, it immediately follows that $$A^TA$$ is a $$d \\\\times d$$ matrix that has the interpretation of a pixel selection mask. Specifically, $$A^TA$$ is a $$d \\\\times d$$ diagonal matrix $$D(m)$$, where the elements of $$m$$ are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first $$k$$ coordinates are known.\\n\\nThe rest of this section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Participant tooling\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nParticipant tooling\\n\\nParticipant tooling lead: Gabriel Ilharco\\n\\nResharder: Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco, Vivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis\\n\\nTraining: Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell Wortsman (lead)\\n\\nEvaluation: Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco\\n\\nAdditional infrastructure: Stephen Mussmann, Sarah Pratt\", \"Header_1: Math Equations and Table\\nHeader_2: Math Equations:\\nfilename: 2305.06927.pdf\\ntitle: Convergence of Alternating Gradient Descent for Matrix Factorization\\n\\nMath Equations:\\n\\n$$X_0 = \\\\sqrt{\\\\eta} \\\\sqrt{dC\\\\sigma_1} A\\\\Phi(n \\\\times d), \\\\quad Y_0 = \\\\frac{\\\\sqrt{n}}{1} \\\\Phi(n \\\\times d)$$\\n\\n$$X_0 = 10\\\\sqrt{d}A\\\\Phi(n \\\\times d), \\\\quad Y_0 = 10\\\\sqrt{n}\\\\Phi(n \\\\times d)$$\\n\\n$$X_0 = 10\\\\sqrt{m}\\\\Phi(m \\\\times d), \\\\quad Y_0 = 10\\\\sqrt{n}\\\\Phi(n \\\\times d)$$\", \"Header_1: Acknowledgments and References\\nHeader_2: Acknowledgments\\nfilename: 2307.01178.pdf\\ntitle: Learning Mixtures of Gaussians Using the DDPM Objective\\n\\nAcknowledgments\\n\\nSC would like to thank Sinho Chewi, Khashayar Gatmiry, Frederic Koehler, and Holden Lee for enlightening discussions on sampling and score estimation.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDATACOMP: In search of the next generation of multimodal datasets\\n\\n*Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt*\\n\\narXiv:2304.14108v5 [cs.CV] 20 Oct 2023\", \"Header_1: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nSolving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\", \"Header_1: Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods\\nfilename: 2310.05309.pdf\\ntitle: Optimizing Solution-Samplers for Combinatorial Problems\\n\\nOptimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods\\n\\nConstantine Caramanis* - University of Texas at Austin\\n\\nDimitris Fotakis \\u2020 - NTUA\\n\\nAlkis Kalavasis \\u2021 - Yale University\\n\\nVasilis Kontonis \\u00a7 - University of Texas at Austin\\n\\nChristos Tzamos\\u00b6 - UOA & University of Wisconsin-Madison\\n\\narXiv:2310.05309v2 [cs.LG] 7 Nov 2023\\n\\nNovember 8, 2023\"]}}], \"child_spans\": null, \"span_kind\": null}, {\"span_id\": null, \"name\": \"rerank\", \"start_time_ms\": 1713828681402, \"end_time_ms\": 1713828685479, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": {\"nodes\": [\"Header_1: Research Paper Summary\\nHeader_2: Acknowledgments\\nfilename: 2302.09057.pdf\\ntitle: Consistent Diffusion Models\\n\\nAcknowledgments\\n\\nThis research has been supported by NSF Grants CCF 1763702, AF 1901292, CNS 2148141, Tripods CCF 1934932, IFML CCF 2019844, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco and the Archie Straiton Endowed Faculty Fellowship. Giannis Daras has been supported by the Onassis Fellowship, the Bodossaki Fellowship and the Leventis Fellowship. Constantinos Daskalakis has been supported by NSF Awards CCF-1901292, DMS-2022448 and DMS2134108, a Simons Investigator Award, the Simons Collaboration on the Theory of Algorithmic Fairness and a DSTA grant.\", \"Header_1: Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\\nfilename: 2302.09057.pdf\\ntitle: Consistent Diffusion Models\\n\\nConsistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\\n\\nGiannis Daras* - Department of Computer Science, University of Texas at Austin\\n\\nYuval Dagan* - Electrical Engineering and Computer Science, University of California, Berkeley\\n\\nAlexandros G. Dimakis - Department of ECE, University of Texas at Austin\\n\\nConstantinos Daskalakis - Electrical Engineering and Computer Science, Massachusetts Institute of Technology\\n\\narXiv:2302.09057v1 [cs.LG] 17 Feb 2023\\n\\nFebruary 20, 2023\", \"Header_1: Acknowledgements and References\\nHeader_2: Acknowledgements\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nAcknowledgements\\n\\nThis research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.\", \"Header_1: Acknowledgements\\nHeader_2: References\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nReferences\\n\\n1. Brian D.O. Anderson. \\u201cReverse-time diffusion equation models\\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\\u2013326 (page 1).\\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \\u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 2).\\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \\u201cCold Diffusion: Inverting arbitrary image transforms without noise\\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 2).\\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \\u201cAlign your latents: High-resolution video synthesis with latent diffusion models\\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\\u201322575 (page 3).\\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \\u201cCompressed sensing using generative models\\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\\u2013546 (page 1).\\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \\u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\\u201398 (pages 2, 8, 24, 27).\\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \\u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (pages 5, 6).\\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \\u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 1).\\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \\u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 1).\\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \\u201cIlvr: Conditioning method for denoising diffusion probabilistic models\\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 2).\\n11. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \\u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=OnD9zGAGT0k (pages 1\\u20133, 7\\u201310, 15, 18, 20, 22, 24\\u201331).\\n12. Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \\u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 2).\\n13. Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \\u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=nJJjv0JDJju (pages 2, 8, 24, 27, 31).\\n14. Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \\u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 2).\", \"Header_1: Acknowledgements\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nAcknowledgements\\n\\nThis research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.\", \"Header_1: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\nHeader_2: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\nfilename: 2305.19256.pdf\\ntitle: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\n\\nAmbient Diffusion: Learning Clean Distributions from Corrupted Data\\n\\nGiannis Daras (UT Austin), Kulin Shah (UT Austin), Yuval Dagan (UC Berkeley)\\n\\nEmail: giannisdaras@utexas.edu, kulinshah@utexas.edu, yuvald@berkeley.edu\\n\\nAravind Gollakota, Alexandros G. Dimakis, Adam Klivans (UT Austin)\\n\\nEmail: aravindg@cs.utexas.edu, dimakis@austin.utexas.edu, klivans@utexas.edu\", \"Header_1: List of References\\nfilename: 2305.11765.pdf\\ntitle: Tester-Learners for Halfspaces: Universal Algorithms\\n\\nList of References\\n\\n|[BK21]|Ainesh Bakshi and Pravesh K Kothari. List-decodable subspace recovery: Dimension independent error in polynomial time. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1279\\u20131297. SIAM, 2021.|\\n|---|---|\\n|[BZ17]|Maria-Florina F Balcan and Hongyang Zhang. Sample and computationally efficient learning algorithms under s-concave distributions. Advances in Neural Information Processing Systems, 30, 2017.|\\n|[Dan15]|Amit Daniely. A ptas for agnostically learning halfspaces. In Conference on Learning Theory, pages 484\\u2013502. PMLR, 2015.|\\n|[DKK+22]|Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with general massart noise under the gaussian distribution. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 874\\u2013885, 2022.|\\n|[DKK+23]|Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Sihan Liu, and Nikos Zarifis. Efficient testable learning of halfspaces with adversarial label noise. arXiv preprint arXiv:2303.05485, 2023.|\\n|[DKPZ21]|Ilias Diakonikolas, Daniel M Kane, Thanasis Pittas, and Nikos Zarifis. The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model. In Conference on Learning Theory, pages 1552\\u20131584. PMLR, 2021.|\\n|[DKR23]|Ilias Diakonikolas, Daniel M Kane, and Lisheng Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. arXiv preprint arXiv:2302.06512, 2023.|\\n|[DKS18]|Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Learning geometric concepts with nasty noise. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1061\\u20131073, 2018.|\\n|[DKTZ20a]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning half-spaces with massart noise under structured distributions. In Conference on Learning Theory, pages 1486\\u20131513. PMLR, 2020.|\\n|[DKTZ20b]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Non-convex sgd learns halfspaces with adversarial label noise. Advances in Neural Information Processing Systems, 33:18540\\u201318549, 2020.|\\n|[DKTZ22]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Learning general halfspaces with adversarial label noise via online gradient descent. In International Conference on Machine Learning, pages 5118\\u20135141. PMLR, 2022.|\\n|[DKZ20]|Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis. Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals. Advances in Neural Information Processing Systems, 33:13586\\u201313596, 2020.|\\n|[FKP+19]|Noah Fleming, Pravesh Kothari, Toniann Pitassi, et al. Semialgebraic proofs and efficient algorithm design. Foundations and Trends\\u00ae in Theoretical Computer Science, 14(1-2):1\\u2013221, 2019.|\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Leadership and Advising\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nLeadership and Advising\\n\\nAdvising: Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\\n\\nLeadership: Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar\\n\\nOverall project lead: Ludwig Schmidt\", \"Header_1: Acknowledgements and References\\nHeader_2: References\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nReferences\\n\\n1. Brian D.O. Anderson. \\u201cReverse-time diffusion equation models\\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\\u2013326 (page 1).\\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \\u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 3).\\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \\u201cCold Diffusion: Inverting arbitrary image transforms without noise\\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 3).\\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \\u201cAlign your latents: High-resolution video synthesis with latent diffusion models\\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\\u201322575 (page 3).\\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \\u201cCompressed sensing using generative models\\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\\u2013546 (page 1).\\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \\u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\\u201398 (pages 3, 9, 30).\\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \\u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (page 6).\\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \\u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 3).\\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \\u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 3).\\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \\u201cIlvr: Conditioning method for denoising diffusion probabilistic models\\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 3).\", \"Header_1: Slice database with cluster construction dim (d_cluster)\\nfilename: 2305.19435.pdf\\ntitle: AdANNS: A Framework for Adaptive Semantic Search\\n\\nSlice database with cluster construction dim (d_cluster)\\nxb = database[:d_cluster]\\ncluster_centroids = constructClusters(xb, num_clusters)\\nreturn cluster_centroids\\ndef adannsInference(queries, centroids, d_shortlist, d_search, num_probes,\\nk):\", \"Header_1: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nHeader_2: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nSolving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nLitu Rout \\u2217 Negin Raoof \\u2020 Giannis Daras \\u2021\\n\\nConstantine Caramanis \\u00a7 Alexandros G. Dimakis \\u00b6 Sanjay Shakkottai \\u2016\\n\\nThe University of Texas at Austin\", \"Header_1: Document\\nHeader_2: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nSolving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nLitu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai\\n\\nThe University of Texas at Austin*\", \"Header_1: Slice queries and centroids with cluster shortlist dim (d_shortlist)\\nfilename: 2305.19435.pdf\\ntitle: AdANNS: A Framework for Adaptive Semantic Search\\n\\nSlice queries and centroids with cluster shortlist dim (d_shortlist)\\nxq = queries[:d_shortlist]\\nxc = centroids[:d_shortlist]\\nfor q in queries:\", \"Header_1: References\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nReferences\\n\\n|#|Authors|Title|Publication Details|Pages|\\n|---|---|---|---|---|\\n|15|Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar|Soft diffusion: Score matching for general corruptions|arXiv preprint arXiv:2209.05442 (2022)|page 2|\\n|16|Mauricio Delbracio and Peyman Milanfar|Inversion by direct iteration: An alternative to denoising diffusion for image restoration|arXiv preprint arXiv:2303.11435 (2023)|page 2|\\n|17|Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei|Imagenet: A large-scale hierarchical image database|2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\\u2013255|pages 7, 21, 22, 27\\u201329|\\n|18|Prafulla Dhariwal and Alexander Nichol|Diffusion models beat gans on image synthesis|Advances in Neural Information Processing Systems 34 (2021), pp. 8780\\u20138794|page 1|\\n|19|Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.|DataComp: In search of the next generation of multimodal datasets|arXiv preprint arXiv:2304.14108 (2023)|page 10|\\n\\n$$\\n\\\\begin{array}{|c|c|c|c|c|}\\n\\\\hline\\n\\\\text{#} & \\\\text{Authors} & \\\\text{Title} & \\\\text{Publication Details} & \\\\text{Pages} \\\\\\\\\\n\\\\hline\\n20 & Jonathan Ho, Ajay Jain, and Pieter Abbeel & Denoising diffusion probabilistic models & Advances in Neural Information Processing Systems 33 (2020), pp. 6840\\u20136851 & page 1 \\\\\\\\\\n21 & Aapo Hyv\\u00e4rinen and Peter Dayan & Estimation of non-normalized statistical models by score matching & Journal of Machine Learning Research 6.4 (2005) & page 1 \\\\\\\\\\n22 & Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir & Robust compressed sensing mri with deep generative priors & Advances in Neural Information Processing Systems 34 (2021), pp. 14938\\u201314954 & pages 1, 2 \\\\\\\\\\n23 & Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price & Instance-optimal compressed sensing via posterior sampling & arXiv preprint arXiv:2106.11438 (2021) & page 1 \\\\\\\\\\n24 & Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price & Fairness for Image Generation with Uncertain Sensitive Attributes & Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\\u201324 Jul 2021, pp. 4721\\u20134732 & page 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n$$\", \"Header_1: Math Equations and Text\\nHeader_2: For part (b), we assume that D is indeed \\u03bb-nice and \\u03b3-Poincar&eacute;. We first use Proposition A.3 as well as a Hoeffding bound, to obtain that $$P_{x\\\\in S}[|\\\\langle w, x\\\\rangle| \\\\leq 2\\\\sigma] \\\\in [C'\\\\lambda C', 2\\\\sigma \\\\cdot C'\\\\lambda C']$$ with probability at least 1\\u2212\\u03b4/3 over S (since |S| is large enough), for some universal constant C' > 0. Then, we use $$4\\\\sigma$$ part (ii) of Proposition A.3 to lower bound the minimum eigenvalue of MD = ED[MS] by $$C'\\\\lambda C'$$. Using part (iii) of Proposition A.3 to bound the second moment of each of the elements of MD, we may use Proposition A.2 to obtain that MS \\u2ab0 $$C'\\\\lambda C' Id-1$$ (and our spectral test passes) with probability at least 1 \\u2212 $$\\\\delta/3$$. It remains to show that the hypercontractivity tester will accept with probability at least 1 \\u2212 $$\\\\delta/3$$ (since, then, the result follows from a union bound).\\nfilename: 2305.11765.pdf\\ntitle: Tester-Learners for Halfspaces: Universal Algorithms\\n\\nFor part (b), we assume that D is indeed \\u03bb-nice and \\u03b3-Poincar&eacute;. We first use Proposition A.3 as well as a Hoeffding bound, to obtain that $$P_{x\\\\in S}[|\\\\langle w, x\\\\rangle| \\\\leq 2\\\\sigma] \\\\in [C'\\\\lambda C', 2\\\\sigma \\\\cdot C'\\\\lambda C']$$ with probability at least 1\\u2212\\u03b4/3 over S (since |S| is large enough), for some universal constant C' > 0. Then, we use $$4\\\\sigma$$ part (ii) of Proposition A.3 to lower bound the minimum eigenvalue of MD = ED[MS] by $$C'\\\\lambda C'$$. Using part (iii) of Proposition A.3 to bound the second moment of each of the elements of MD, we may use Proposition A.2 to obtain that MS \\u2ab0 $$C'\\\\lambda C' Id-1$$ (and our spectral test passes) with probability at least 1 \\u2212 $$\\\\delta/3$$. It remains to show that the hypercontractivity tester will accept with probability at least 1 \\u2212 $$\\\\delta/3$$ (since, then, the result follows from a union bound).\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Candidate pool\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nCandidate pool\\n\\nCandidate pool lead: Vaishaal Shankar\\n\\nData collection: Romain Beaumont, Vaishaal Shankar\\n\\nPre-processing and metadata: Giannis Daras, Alex Fang (content filtering lead), Samir Yitzhak Gadre (metadata lead), Ryan Marten (deduplication lead), Vivek Ramanujan, Vaishaal Shankar, George Smyrnis (face blurring lead)\", \"Header_1: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nHeader_2: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nHeader_3: Introduction\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nIntroduction\\n\\nWe study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process; see also the survey of Ongie et al. and references therein.\\n\\nThe second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\\n\\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution p0(x0) using an It\\u00f4 Stochastic Differential Equation (SDE), dx = f(x, t)dt + g(t)dw, and then by learning the score-function, \\u2207xt log pt(xt), at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\\u00f4 SDE. The SDE that corrupts\\n\\nContact:\\n\\n- litu.rout@utexas.edu\\n- neginmr@utexas.edu\\n- giannisdaras@utexas.edu\\n- constantine@utexas.edu\\n- dimakis@austin.utexas.edu\\n- sanjay.shakkottai@utexas.edu\", \"Header_1: Acknowledgments and References\\nHeader_2: References\\nfilename: 2307.01178.pdf\\ntitle: Learning Mixtures of Gaussians Using the DDPM Objective\\n\\nReferences\\n\\n|[AG22]|Ahmed El Alaoui and Jason Gaitonde. Bounds on the covariance matrix of the Sherrington-Kirkpatrick model. arXiv preprint arXiv:2212.02445, 2022.|\\n|---|---|\\n|[BDD23]|Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. arXiv preprint arXiv:2305.16860, 2023.|\\n|[BDJ+22]|Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234\\u20131247, 2022.|\\n|[BKM17]|David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\\u2013877, 2017.|\\n|[BM11]|Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\\u2013785, 2011.|\\n|[BMR22]|Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. arXiv preprint 2002.00107, 2022.|\\n|[BRST21]|Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous LWE. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 694\\u2013707, 2021.|\\n|[BS15]|Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. SIAM Journal on Computing, 44(4):889\\u2013911, 2015.|\\n|[BWY17]|Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. 2017.|\\n|[CCL+23a]|Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. arXiv preprint arXiv:2305.11798, 2023.|\\n|[CCL+23b]|Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.|\\n|[CDD23]|Sitan Chen, Giannis Daras, and Alexandros G Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. arXiv preprint arXiv:2303.03384, 2023.|\\n\\n- [Cel22] Michael Celentano. Sudakov-fernique post-amp, and a new proof of the local convexity\\nof the tap free energy. arXiv preprint arXiv:2208.09550, 2022.\\n- [CFM21] Michael Celentano, Zhou Fan, and Song Mei. Local convexity of the tap free energy\\nand amp convergence for z2-synchronization. arXiv preprint arXiv:2106.11428, 2021.\\n- [CLL22] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based gener-\\native modeling: user-friendly bounds under minimal smoothness assumptions. arXiv\\npreprint arXiv:2211.01916, 2022.\\n- [DB22] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold\\nhypothesis. Transactions on Machine Learning Research, 2022.\\n- [DBTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion\\nSchr\\u00f6dinger bridge with applications to score-based generative modeling. In M. Ran-\\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\\nAdvances in Neural Information Processing Systems, volume 34, pages 17695\\u201317709.\\nCurran Associates, Inc., 2021.\\n- [DHKK20] Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly\\nlearning any clusterable mixture of gaussians. arXiv preprint arXiv:2005.06417, 2020.\\n- [DK20] Ilias Diakonikolas and Daniel M Kane. Small covers for near-zero sets of polynomi-\\nals and learning latent variable models. In 2020 IEEE 61st Annual Symposium on\\nFoundations of Computer Science (FOCS), pages 184\\u2013195. IEEE, 2020.\\n- [DKS17] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower\\nbounds for robust estimation of high-dimensional gaussians and gaussian mixtures.\\nIn 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),\\npages 73\\u201384. IEEE, 2017.\\n- [DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean\\nestimation and learning mixtures of spherical gaussians. In Proceedings of the 50th\\nAnnual ACM SIGACT Symposium on Theory of Computing, pages 1047\\u20131060, 2018.\\n- [DMM09] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algo-\\nrithms for compressed sensing. Proceedings of the National Academy of Sciences,\\n106(45):18914\\u201318919, 2009.\\n- [DMM10] David L Donoho, Arian Maleki, and Andrea Montanari. Message passing algorithms\\nfor compressed sensing: I. motivation and construction. In 2010 IEEE information\\ntheory workshop on information theory (ITW 2010, Cairo), pages 1\\u20135. IEEE, 2010.\\n- [DS07] Sanjoy Dasgupta and Leonard J Schulman. A probabilistic analysis of em for mixtures\\nof separated, spherical gaussians. Journal of Machine Learning Research, 8:203\\u2013226,\\n2007.\\n- [DTZ17] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of\\nem suffice for mixtures of two gaussians. In Conference on Learning Theory, pages\\n704\\u2013710. PMLR, 2017.\", \"Header_1: List of References\\nfilename: 2306.10615.pdf\\ntitle: Agnostically Learning Single-Index Models using Omnipredictors\\n\\nList of References\\n\\n|Reference|Authors|Title|Publication Details|Sections|\\n|---|---|---|---|---|\\n|[DKMR22]|Ilias Diakonikolas, Daniel Kane, Pasin Manurangsi, Lisheng Ren|Hardness of learning a single neuron with adversarial label noise|In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 8199\\u20138213. PMLR, 28\\u201330 Mar 2022.|1, 1.1, 5, 5.1|\\n|[DKPZ21]|Ilias Diakonikolas, Daniel M. Kane, Thanasis Pittas, Nikos Zarifis|The optimality of polynomial regression for agnostic learning under gaussian marginals in the sq model|In Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pages 1552\\u20131584. PMLR, 15\\u201319 Aug 2021.|1, 1.1|\\n|[DKTZ20]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis|Non-convex sgd learns halfspaces with adversarial label noise|In Advances in Neural Information Processing Systems, volume 33, pages 18540\\u201318549. Curran Associates, Inc., 2020.|4|\\n|[DKTZ22]|Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis|Learning a single neuron with adversarial label noise via gradient descent|In Proceedings of Thirty Fifth Conference on Learning Theory, volume 178 of Proceedings of Machine Learning Research, pages 4313\\u20134361. PMLR, 02\\u201305 Jul 2022.|1, 1, 1.1, 4|\\n|[DKZ20]|Ilias Diakonikolas, Daniel Kane, Nikos Zarifis|Near-optimal sq lower bounds for agnostically learning halfspaces and relus under gaussian marginals|In Advances in Neural Information Processing Systems, 33:13586\\u201313596, 2020.|1, 1.1|\\n|[FCG20]|Spencer Frei, Yuan Cao, Quanquan Gu|Agnostic learning of a single neuron with gradient descent|In Advances in Neural Information Processing Systems, 33:5417\\u20135428, 2020.|1, 1.1|\\n|[GGK20]|Surbhi Goel, Aravind Gollakota, Adam Klivans|Statistical-query lower bounds via functional gradients|In Advances in Neural Information Processing Systems, 33:2147\\u20132158, 2020.|1, 1.1|\\n|[GHK+23]|Parikshit Gopalan, Lunjia Hu, Michael P. Kim, Omer Reingold, Udi Wieder|Loss minimization through the lens of outcome indistinguishability|In 14th Innovations in Theoretical Computer Science Conference, ITCS 2023, January 10-13, 2023, MIT, Cambridge, Massachusetts, USA, volume 251 of LIPIcs, pages 60:1\\u201360:20. Schloss Dagstuhl - Leibniz-Zentrum f\\u00fcr Informatik, 2023.|1, 1.1, 3, 3, 3.2, C.1, C.2, C.1|\\n|[GKK19]|Surbhi Goel, Sushrut Karmalkar, Adam Klivans|Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals|In Advances in neural information processing systems, 32, 2019.|1, 1.1|\\n|[GKKT17]|Surbhi Goel, Varun Kanade, Adam Klivans, Justin Thaler|Reliably learning the relu in polynomial time|In Conference on Learning Theory, pages 1004\\u20131042. PMLR, 2017.|1.1|\", \"Header_1: Acknowledgements\\nHeader_2: Acknowledgements\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nAcknowledgements\\n\\nSYG and JH are supported by NSF Graduate Research Fellowships. GS is supported by the Onassis Foundation - Scholarship ID: F ZS 056-1/2022-2023. GD has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. This research has been supported by NSF Grants AF 1901292, CNS 2148141, DMS 2134012, TRIPODS II-DMS 2023166, Tripods CCF 1934932, IFML CCF 2019844 and research gifts by Western Digital, WNCG IAP, UT Austin Machine Learning Lab (MLL), Cisco, the Len Blavatnik and the Blavatnik Family Foundation, the Stanly P. Finch Centennial Professorship in Engineering, Open Philanthropy, Google, Microsoft, and the Allen Institute for AI.\\n\\nWe would like to thank Amro Abbas, Danny Bickson, Alper Canberk, Jessie Chapman, Brian Cheung, Tim Dettmers, Joshua Gardner, Nancy Garland, Sachin Goyal, Huy Ha, Zaid Harchaoui, Ari Holtzman, Andrew Hundt, Andy Jones, Adam Klivans, Ronak Mehta, Sachit Menon, Ari Morcos, Raviteja Mullapudi, Jonathon Shlens, Brandon McKinzie, Alexander Toshev, David Grangier, Navdeep Jaitly, Kentrell Owens, Marco Tulio Ribeiro, Shiori Sagawa, Christoph Schuhmann, Matthew Wallingford, and Ross Wightman for helpful feedback at various stages of the project. We are particularly grateful to Daniel Levy and Alec Radford for early encouragement to pursue this project and feedback on the experimental design.\\n\\nWe thank Stability AI and the Gauss Centre for Supercomputing e.V. for providing us with compute resources to train models. We are thankful for the compute time provided through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\\u00fclich Supercomputing Centre (JSC), and for storage resources on JUST granted and operated by JSC, as well as computing and storage resources from the Helmholtz Data Federation (HDF).\\n\\nGauss Centre for Supercomputing e.V.\", \"Header_1: DISCS: A Benchmark for Discrete Sampling\\nfilename: 35_discs_a_benchmark_for_discrete.pdf\\ntitle: DISCS: A Benchmark for Discrete Sampling\\n\\nDISCS: A Benchmark for Discrete Sampling\\n\\nAnonymous Author(s)\\n\\nAffiliation\\n\\nAddress\\n\\nemail\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Baselines\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nBaselines\\n\\nBaselines lead: Yair Carmon\\n\\nFiltering track: Yair Carmon, Rahim Enterazi, Alex Fang, Samir Yitzhak Gadre, Gabriel Ilharco, Kalyani Marathe, Thao Nguyen, Eyal Orgad (co-lead), Georgios Smyrnis, Mitchell Wortsman, Jieyu Zhang (co-lead)\\n\\nBYOD track: Gabriel Ilharco, Thao Nguyen\\n\\nExperiment babysitting: Alex Fang, Gabriel Ilharco, Samir Yitzhak Gadre\", \"Header_1: Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\\nHeader_2: Abstract\\nfilename: 2302.09057.pdf\\ntitle: Consistent Diffusion Models\\n\\nAbstract\\n\\nImperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a consistency property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm.\", \"Header_1: Technical Proofs\\nHeader_2: Technical Proofs\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nTechnical Proofs\\n\\nNotation and Measurement Matrix. We elaborate on the structure of the measurement matrix $$A \\\\in \\\\mathbb{R}^{l \\\\times d}$$. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the $$d \\\\times d$$ identity matrix (the rows that are present correspond to the indices of the selected pixels from the image $$x_0 \\\\in \\\\mathbb{R}^d$$). Given this structure, it immediately follows that $$A^TA$$ is a $$d \\\\times d$$ matrix that has the interpretation of a pixel selection mask. Specifically, $$A^TA$$ is a $$d \\\\times d$$ diagonal matrix $$D(m)$$, where the elements of $$m$$ are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first $$k$$ coordinates are known.\\n\\nThe rest of this section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details.\", \"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Participant tooling\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nParticipant tooling\\n\\nParticipant tooling lead: Gabriel Ilharco\\n\\nResharder: Romain Beaumont, Yair Carmon, Alex Fang, Jonathan Hayase (lead), Gabriel Ilharco, Vivek Ramanujan, Vaishaal Shankar, Georgios Smyrnis\\n\\nTraining: Mehdi Cherti, Gabriel Ilharco, Jenia Jitsev, Vivek Ramanujan, Georgios Smyrnis, Mitchell Wortsman (lead)\\n\\nEvaluation: Romain Beaumont, Yonatan Bitton, Mehdi Cherti, Dhruba Ghosh (lead), Gabriel Ilharco\\n\\nAdditional infrastructure: Stephen Mussmann, Sarah Pratt\", \"Header_1: Math Equations and Table\\nHeader_2: Math Equations:\\nfilename: 2305.06927.pdf\\ntitle: Convergence of Alternating Gradient Descent for Matrix Factorization\\n\\nMath Equations:\\n\\n$$X_0 = \\\\sqrt{\\\\eta} \\\\sqrt{dC\\\\sigma_1} A\\\\Phi(n \\\\times d), \\\\quad Y_0 = \\\\frac{\\\\sqrt{n}}{1} \\\\Phi(n \\\\times d)$$\\n\\n$$X_0 = 10\\\\sqrt{d}A\\\\Phi(n \\\\times d), \\\\quad Y_0 = 10\\\\sqrt{n}\\\\Phi(n \\\\times d)$$\\n\\n$$X_0 = 10\\\\sqrt{m}\\\\Phi(m \\\\times d), \\\\quad Y_0 = 10\\\\sqrt{n}\\\\Phi(n \\\\times d)$$\", \"Header_1: Acknowledgments and References\\nHeader_2: Acknowledgments\\nfilename: 2307.01178.pdf\\ntitle: Learning Mixtures of Gaussians Using the DDPM Objective\\n\\nAcknowledgments\\n\\nSC would like to thank Sinho Chewi, Khashayar Gatmiry, Frederic Koehler, and Holden Lee for enlightening discussions on sampling and score estimation.\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDATACOMP: In search of the next generation of multimodal datasets\\n\\n*Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt*\\n\\narXiv:2304.14108v5 [cs.CV] 20 Oct 2023\", \"Header_1: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nSolving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\", \"Header_1: Optimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods\\nfilename: 2310.05309.pdf\\ntitle: Optimizing Solution-Samplers for Combinatorial Problems\\n\\nOptimizing Solution-Samplers for Combinatorial Problems: The Landscape of Policy-Gradient Methods\\n\\nConstantine Caramanis* - University of Texas at Austin\\n\\nDimitris Fotakis \\u2020 - NTUA\\n\\nAlkis Kalavasis \\u2021 - Yale University\\n\\nVasilis Kontonis \\u00a7 - University of Texas at Austin\\n\\nChristos Tzamos\\u00b6 - UOA & University of Wisconsin-Madison\\n\\narXiv:2310.05309v2 [cs.LG] 7 Nov 2023\\n\\nNovember 8, 2023\"]}, \"outputs\": {\"reranked_nodes\": [\"Header_1: DATACOMP Benchmark Rules\\nHeader_2: Contributions\\nHeader_3: Leadership and Advising\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nLeadership and Advising\\n\\nAdvising: Romain Beaumont, Yair Carmon, Alexandros G. Dimakis, Ali Farhadi, Hannaneh Hajishirzi, Jenia Jitsev, Pang Wei Koh, Ranjay Krishna, Stephen Mussmann, Sewoong Oh, Alexander Ratner, Olga Saukh, Ludwig Schmidt, Vaishaal Shankar, Shuran Song, Richard Vencu\\n\\nLeadership: Yair Carmon, Alexandros G. Dimakis, Jenia Jitsev, Sewoong Oh, Ludwig Schmidt, Vaishaal Shankar\\n\\nOverall project lead: Ludwig Schmidt\", \"Header_1: Acknowledgements\\nHeader_2: References\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nReferences\\n\\n1. Brian D.O. Anderson. \\u201cReverse-time diffusion equation models\\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\\u2013326 (page 1).\\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \\u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 2).\\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \\u201cCold Diffusion: Inverting arbitrary image transforms without noise\\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 2).\\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \\u201cAlign your latents: High-resolution video synthesis with latent diffusion models\\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\\u201322575 (page 3).\\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \\u201cCompressed sensing using generative models\\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\\u2013546 (page 1).\\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \\u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\\u201398 (pages 2, 8, 24, 27).\\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \\u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (pages 5, 6).\\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \\u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 1).\\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \\u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 1).\\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \\u201cIlvr: Conditioning method for denoising diffusion probabilistic models\\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 2).\\n11. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \\u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=OnD9zGAGT0k (pages 1\\u20133, 7\\u201310, 15, 18, 20, 22, 24\\u201331).\\n12. Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \\u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 2).\\n13. Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \\u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=nJJjv0JDJju (pages 2, 8, 24, 27, 31).\\n14. Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \\u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 2).\", \"Header_1: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\nHeader_2: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\nfilename: 2305.19256.pdf\\ntitle: Ambient Diffusion: Learning Clean Distributions from Corrupted Data\\n\\nAmbient Diffusion: Learning Clean Distributions from Corrupted Data\\n\\nGiannis Daras (UT Austin), Kulin Shah (UT Austin), Yuval Dagan (UC Berkeley)\\n\\nEmail: giannisdaras@utexas.edu, kulinshah@utexas.edu, yuvald@berkeley.edu\\n\\nAravind Gollakota, Alexandros G. Dimakis, Adam Klivans (UT Austin)\\n\\nEmail: aravindg@cs.utexas.edu, dimakis@austin.utexas.edu, klivans@utexas.edu\", \"Header_1: Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\\nfilename: 2302.09057.pdf\\ntitle: Consistent Diffusion Models\\n\\nConsistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\\n\\nGiannis Daras* - Department of Computer Science, University of Texas at Austin\\n\\nYuval Dagan* - Electrical Engineering and Computer Science, University of California, Berkeley\\n\\nAlexandros G. Dimakis - Department of ECE, University of Texas at Austin\\n\\nConstantinos Daskalakis - Electrical Engineering and Computer Science, Massachusetts Institute of Technology\\n\\narXiv:2302.09057v1 [cs.LG] 17 Feb 2023\\n\\nFebruary 20, 2023\", \"Header_1: Document\\nHeader_2: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nSolving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nLitu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai\\n\\nThe University of Texas at Austin*\", \"Header_1: References\\nfilename: solving-linear-inverse-probs.pdf\\ntitle: Document\\n\\nReferences\\n\\n|#|Authors|Title|Publication Details|Pages|\\n|---|---|---|---|---|\\n|15|Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar|Soft diffusion: Score matching for general corruptions|arXiv preprint arXiv:2209.05442 (2022)|page 2|\\n|16|Mauricio Delbracio and Peyman Milanfar|Inversion by direct iteration: An alternative to denoising diffusion for image restoration|arXiv preprint arXiv:2303.11435 (2023)|page 2|\\n|17|Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei|Imagenet: A large-scale hierarchical image database|2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\\u2013255|pages 7, 21, 22, 27\\u201329|\\n|18|Prafulla Dhariwal and Alexander Nichol|Diffusion models beat gans on image synthesis|Advances in Neural Information Processing Systems 34 (2021), pp. 8780\\u20138794|page 1|\\n|19|Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.|DataComp: In search of the next generation of multimodal datasets|arXiv preprint arXiv:2304.14108 (2023)|page 10|\\n\\n$$\\n\\\\begin{array}{|c|c|c|c|c|}\\n\\\\hline\\n\\\\text{#} & \\\\text{Authors} & \\\\text{Title} & \\\\text{Publication Details} & \\\\text{Pages} \\\\\\\\\\n\\\\hline\\n20 & Jonathan Ho, Ajay Jain, and Pieter Abbeel & Denoising diffusion probabilistic models & Advances in Neural Information Processing Systems 33 (2020), pp. 6840\\u20136851 & page 1 \\\\\\\\\\n21 & Aapo Hyv\\u00e4rinen and Peter Dayan & Estimation of non-normalized statistical models by score matching & Journal of Machine Learning Research 6.4 (2005) & page 1 \\\\\\\\\\n22 & Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir & Robust compressed sensing mri with deep generative priors & Advances in Neural Information Processing Systems 34 (2021), pp. 14938\\u201314954 & pages 1, 2 \\\\\\\\\\n23 & Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price & Instance-optimal compressed sensing via posterior sampling & arXiv preprint arXiv:2106.11438 (2021) & page 1 \\\\\\\\\\n24 & Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price & Fairness for Image Generation with Uncertain Sensitive Attributes & Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\\u201324 Jul 2021, pp. 4721\\u20134732 & page 1 \\\\\\\\\\n\\\\hline\\n\\\\end{array}\\n$$\", \"Header_1: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nHeader_2: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\nHeader_3: Introduction\\nfilename: 2307.00619.pdf\\ntitle: Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\\n\\nIntroduction\\n\\nWe study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process; see also the survey of Ongie et al. and references therein.\\n\\nThe second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\\n\\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution p0(x0) using an It\\u00f4 Stochastic Differential Equation (SDE), dx = f(x, t)dt + g(t)dw, and then by learning the score-function, \\u2207xt log pt(xt), at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\\u00f4 SDE. The SDE that corrupts\\n\\nContact:\\n\\n- litu.rout@utexas.edu\\n- neginmr@utexas.edu\\n- giannisdaras@utexas.edu\\n- constantine@utexas.edu\\n- dimakis@austin.utexas.edu\\n- sanjay.shakkottai@utexas.edu\", \"Header_1: DATACOMP: In search of the next generation of multimodal datasets\\nfilename: 2304.14108.pdf\\ntitle: DATACOMP: In search of the next generation of multimodal datasets\\n\\nDATACOMP: In search of the next generation of multimodal datasets\\n\\n*Samir Yitzhak Gadre*, Gabriel Ilharco*, Alex Fang*, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt*\\n\\narXiv:2304.14108v5 [cs.CV] 20 Oct 2023\"]}}], \"child_spans\": null, \"span_kind\": null}, {\"span_id\": null, \"name\": \"answer\", \"start_time_ms\": 1713828681402, \"end_time_ms\": 1713828691580, \"status_code\": null, \"status_message\": null, \"attributes\": null, \"results\": [{\"inputs\": null, \"outputs\": {\"answer\": \"Alexandros G. Dimakis is a prominent figure in the field of computer science, specifically known for his contributions to machine learning, signal processing, and data science. He is frequently involved in research related to diffusion models, generative models, and solving inverse problems. Dimakis is affiliated with the University of Texas at Austin, where he serves in the Department of Electrical and Computer Engineering [3], [4], [5]. His work often explores innovative methods for data reconstruction and generative modeling, making significant impacts in the areas of compressed sensing and machine learning [2], [5].\"}}], \"child_spans\": null, \"span_kind\": null}], \"span_kind\": null}"}, "_timestamp": 1713828691.581501, "_runtime": 37.19825601577759, "_step": 0}