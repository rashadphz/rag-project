{"pages": [{"page": 1, "text": "        Solving Linear Inverse Problems Provably via\n     Posterior Sampling with Latent Diffusion Models\n                               Litu Rout     Negin Raoof       Giannis Daras\n                 Constantine Caramanis        Alexandros G. Dimakis         Sanjay Shakkottai\n                                    The University of Texas at Austin\u2217\n                                                 Abstract\n          We present the first framework to solve linear inverse problems leveraging pre-\n          trained latent diffusion models. Previously proposed algorithms (such as DPS and\n          DDRM) only apply to pixel-space diffusion models. We theoretically analyze our\n          algorithm showing provable sample recovery in a linear model setting. The algo-\n          rithmic insight obtained from our analysis extends to more general settings often\n          considered in practice. Experimentally, we outperform previously proposed poste-\n          rior sampling algorithms in a wide variety of problems including random inpainting,\n          block inpainting, denoising, deblurring, destriping, and super-resolution.\n1    Introduction\nWe study the use of pre-trained latent diffusion models to solve linear inverse problems such as\ndenoising, inpainting, compressed sensing and super-resolution. There are two classes of approaches\nfor inverse problems: supervised methods where a restoration model is trained to solve the task at\nhand [37, 39, 56, 31], and unsupervised methods that use the prior learned by a generative model\nto guide the restoration process [52, 40, 5, 33, 11, 26]; see also the survey of Ongie et al. [36] and\nreferences therein.\nThe second family of unsupervised methods has gained popularity because: (i) general-domain\nfoundation generative models have become widely available, (ii) unsupervised methods do not require\nany training to solve inverse problems and leverage the massive data and compute investment of\npre-trained models and (iii) generative models sample from the posterior-distribution, mitigating\ncertain pitfalls of likelihood-maximization methods such as bias in the reconstructions [35, 24] and\nregression to the mean [23, 22].\nDiffusion models have emerged as a powerful new approach to generative modeling [47, 48, 49, 20,\n29, 18, 54]. This family of generative models works by first corrupting the data distribution p0(x0)\nusing an It\u00f4 Stochastic Differential Equation (SDE), dx = f(x, t)dt + g(t)dw, and then by learning\nthe score-function, \u2207xt log pt(xt), at all levels t, using Denoising Score Matching (DSM) [21, 53].\nThe seminal result of Anderson [1] shows that we can reverse the corruption process, i.e., start with\nnoise and then sample from the data distribution, by running another It\u00f4 SDE. The SDE that corrupts\nthe data is often termed as Forward SDE and its reverse as Reverse SDE [49]. The latter depends\non the score-function \u2207xt log pt(xt) that we learn through DSM. In [8, 9], the authors provided a\nnon-asymptotic analysis for the sampling of diffusion models when the score-function is only learned\napproximately.\nThe success of diffusion models sparked the interest to investigate how we can use them to solve\ninverse problems. Song et al. [49] showed that given measurements y = Ax0 + \u03c3yn, we can\n    \u2217Email:{litu.rout,neginmr,giannisdaras,constantine,sanjay.shakkottai}utexas.edu, dimakis@austin.utexas.edu\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Document\n\n## Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\n\nLitu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai\n\nThe University of Texas at Austin*\n\n### Abstract\n\nWe present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.\n\n### 1 Introduction\n\nWe study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process. The second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\n\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution $$p_0(x_0)$$ using an It\u00f4 Stochastic Differential Equation (SDE), $$dx = f(x, t)dt + g(t)dw$$, and then by learning the score-function, $$\\nabla_x t \\log p_t(x_t)$$, at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\u00f4 SDE. The SDE that corrupts the data is often termed as Forward SDE and its reverse as Reverse SDE. The latter depends on the score-function $$\\nabla_x t \\log p_t(x_t)$$ that we learn through DSM. In the provided references, the authors provided a non-asymptotic analysis for the sampling of diffusion models when the score-function is only learned approximately.\n\nThe success of diffusion models sparked the interest to investigate how we can use them to solve inverse problems. Song et al. showed that given measurements $$y = Ax_0 + \\sigma yn$$, we can\n\n*Email: {litu.rout, neginmr, giannisdaras, constantine, sanjay.shakkottai}@utexas.edu, dimakis@austin.utexas.edu\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models", "md": "## Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models"}, {"type": "text", "value": "Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai\n\nThe University of Texas at Austin*", "md": "Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai\n\nThe University of Texas at Austin*"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "We present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.", "md": "We present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution."}, {"type": "heading", "lvl": 3, "value": "1 Introduction", "md": "### 1 Introduction"}, {"type": "text", "value": "We study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process. The second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\n\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution $$p_0(x_0)$$ using an It\u00f4 Stochastic Differential Equation (SDE), $$dx = f(x, t)dt + g(t)dw$$, and then by learning the score-function, $$\\nabla_x t \\log p_t(x_t)$$, at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\u00f4 SDE. The SDE that corrupts the data is often termed as Forward SDE and its reverse as Reverse SDE. The latter depends on the score-function $$\\nabla_x t \\log p_t(x_t)$$ that we learn through DSM. In the provided references, the authors provided a non-asymptotic analysis for the sampling of diffusion models when the score-function is only learned approximately.\n\nThe success of diffusion models sparked the interest to investigate how we can use them to solve inverse problems. Song et al. showed that given measurements $$y = Ax_0 + \\sigma yn$$, we can\n\n*Email: {litu.rout, neginmr, giannisdaras, constantine, sanjay.shakkottai}@utexas.edu, dimakis@austin.utexas.edu\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "We study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process. The second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\n\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution $$p_0(x_0)$$ using an It\u00f4 Stochastic Differential Equation (SDE), $$dx = f(x, t)dt + g(t)dw$$, and then by learning the score-function, $$\\nabla_x t \\log p_t(x_t)$$, at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\u00f4 SDE. The SDE that corrupts the data is often termed as Forward SDE and its reverse as Reverse SDE. The latter depends on the score-function $$\\nabla_x t \\log p_t(x_t)$$ that we learn through DSM. In the provided references, the authors provided a non-asymptotic analysis for the sampling of diffusion models when the score-function is only learned approximately.\n\nThe success of diffusion models sparked the interest to investigate how we can use them to solve inverse problems. Song et al. showed that given measurements $$y = Ax_0 + \\sigma yn$$, we can\n\n*Email: {litu.rout, neginmr, giannisdaras, constantine, sanjay.shakkottai}@utexas.edu, dimakis@austin.utexas.edu\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "                      SOp\nFigure 1: Overall pipeline of our proposed framework from left to right. Given an image (left) and a\nuser defined mask (center), our algorithm inpaints the masked region (right). The known part of the\nimages are unaltered (see Appendix C for web demo and image sources).\nprovably sample from the distribution p0(x0|y) by running a modified Reverse SDE that depends\non the unconditional score \u2207xt log pt(xt) and the term \u2207xt log p(y|xt). The latter term captures\nhow much the current iterate explains the measurements and it is intractable even for linear inverse\nproblems without assumptions on the distribution p0(x0) [11, 14]. To deal with the intractability\nof the problem, a series of approximation algorithms have been developed [22, 11, 2, 13, 26, 10,\n6, 46, 12, 27] for solving (linear and non-linear) inverse problems with diffusion models. These\nalgorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively\nsolve problems such as inpainting, deblurring, super-resolution among others.\nRecently, diffusion models have been generalized to learn to invert non-Markovian and non-linear\ncorruption processes [16, 15, 3]. One instance of this generalization is the family of Latent Diffusion\nModels (LDMs) [41]. LDMs project the data into some latent space, z0 = E(x0), perform the\n                                                  2", "md": "Figure 1: Overall pipeline of our proposed framework from left to right. Given an image (left) and a\nuser defined mask (center), our algorithm inpaints the masked region (right). The known part of the\nimages are unaltered (see Appendix C for web demo and image sources).\n\nprovably sample from the distribution $$p_0(x_0|y)$$ by running a modified Reverse SDE that depends\non the unconditional score $$\\nabla x_t \\log p_t(x_t)$$ and the term $$\\nabla x_t \\log p(y|x_t)$$. The latter term captures\nhow much the current iterate explains the measurements and it is intractable even for linear inverse\nproblems without assumptions on the distribution $$p_0(x_0)$$ [11, 14]. To deal with the intractability\nof the problem, a series of approximation algorithms have been developed [22, 11, 2, 13, 26, 10,\n6, 46, 12, 27] for solving (linear and non-linear) inverse problems with diffusion models. These\nalgorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively\nsolve problems such as inpainting, deblurring, super-resolution among others.\n\nRecently, diffusion models have been generalized to learn to invert non-Markovian and non-linear\ncorruption processes [16, 15, 3]. One instance of this generalization is the family of Latent Diffusion\nModels (LDMs) [41]. LDMs project the data into some latent space, $$z_0 = E(x_0)$$, perform the", "images": [{"name": "page-2-5.jpg", "height": 119, "width": 119, "x": 367, "y": 191}, {"name": "page-2-1.jpg", "height": 119, "width": 119, "x": 246, "y": 72}, {"name": "page-2-4.jpg", "height": 119, "width": 119, "x": 246, "y": 191}, {"name": "page-2-2.jpg", "height": 119, "width": 119, "x": 367, "y": 72}, {"name": "page-2-0.jpg", "height": 119, "width": 119, "x": 125, "y": 72}, {"name": "page-2-9.jpg", "height": 119, "width": 119, "x": 125, "y": 431}, {"name": "page-2-10.jpg", "height": 119, "width": 119, "x": 246, "y": 431}, {"name": "page-2-7.jpg", "height": 119, "width": 119, "x": 246, "y": 311}, {"name": "page-2-6.jpg", "height": 119, "width": 119, "x": 125, "y": 311}, {"name": "page-2-8.jpg", "height": 119, "width": 119, "x": 367, "y": 311}, {"name": "page-2-3.jpg", "height": 119, "width": 119, "x": 125, "y": 191}, {"name": "page-2-11.jpg", "height": 119, "width": 119, "x": 367, "y": 431}], "items": [{"type": "text", "value": "Figure 1: Overall pipeline of our proposed framework from left to right. Given an image (left) and a\nuser defined mask (center), our algorithm inpaints the masked region (right). The known part of the\nimages are unaltered (see Appendix C for web demo and image sources).\n\nprovably sample from the distribution $$p_0(x_0|y)$$ by running a modified Reverse SDE that depends\non the unconditional score $$\\nabla x_t \\log p_t(x_t)$$ and the term $$\\nabla x_t \\log p(y|x_t)$$. The latter term captures\nhow much the current iterate explains the measurements and it is intractable even for linear inverse\nproblems without assumptions on the distribution $$p_0(x_0)$$ [11, 14]. To deal with the intractability\nof the problem, a series of approximation algorithms have been developed [22, 11, 2, 13, 26, 10,\n6, 46, 12, 27] for solving (linear and non-linear) inverse problems with diffusion models. These\nalgorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively\nsolve problems such as inpainting, deblurring, super-resolution among others.\n\nRecently, diffusion models have been generalized to learn to invert non-Markovian and non-linear\ncorruption processes [16, 15, 3]. One instance of this generalization is the family of Latent Diffusion\nModels (LDMs) [41]. LDMs project the data into some latent space, $$z_0 = E(x_0)$$, perform the", "md": "Figure 1: Overall pipeline of our proposed framework from left to right. Given an image (left) and a\nuser defined mask (center), our algorithm inpaints the masked region (right). The known part of the\nimages are unaltered (see Appendix C for web demo and image sources).\n\nprovably sample from the distribution $$p_0(x_0|y)$$ by running a modified Reverse SDE that depends\non the unconditional score $$\\nabla x_t \\log p_t(x_t)$$ and the term $$\\nabla x_t \\log p(y|x_t)$$. The latter term captures\nhow much the current iterate explains the measurements and it is intractable even for linear inverse\nproblems without assumptions on the distribution $$p_0(x_0)$$ [11, 14]. To deal with the intractability\nof the problem, a series of approximation algorithms have been developed [22, 11, 2, 13, 26, 10,\n6, 46, 12, 27] for solving (linear and non-linear) inverse problems with diffusion models. These\nalgorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively\nsolve problems such as inpainting, deblurring, super-resolution among others.\n\nRecently, diffusion models have been generalized to learn to invert non-Markovian and non-linear\ncorruption processes [16, 15, 3]. One instance of this generalization is the family of Latent Diffusion\nModels (LDMs) [41]. LDMs project the data into some latent space, $$z_0 = E(x_0)$$, perform the"}]}, {"page": 3, "text": "diffusion in the latent space and use a decoder, D(z0), to move back to the pixel space. LDMs\npower state-of-the-art foundation models such as Stable Diffusion [41] and have enabled a wide-\nrange of applications across many data modalities including images [41], video [4], audio [30]\nand medical domain distributions (e.g., for MRI and proteins) [38, 51]. Unfortunately, none of the\nexisting algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to\nuse a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform\nfinetuning for each task of interest.\nIn this paper, we present the first framework to solve general inverse problems with pre-trained latent\ndiffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the\ndiffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing\nthe power of available foundation models, we are able to outperform previous approaches without\nfinetuning across a wide range of problems (see Figure 1 and 2).\nOur contributions are as follows:\n      (i) We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve\n          linear inverse problem when the degradation operator is known.\n     (ii) We theoretically analyze our algorithm and show provable sample recovery in a linear model\n          setting with two-step diffusion processes.\n     (iii) We achieve a new state-of-the-art for solving inverse problems with latent diffusion models,\n          outperforming previous approaches for inpainting, block inpainting, denoising, deblurring,\n          destriping, and super-resolution.2\n2    Background and Method\nNotation: Bold lower-case x, bold upper-case X, and normal lower case x denote a vector, a matrix,\nand a scalar variable, respectively. We denote by \u2299     element-wise multiplication. D(x) represents\na diagonal matrix with entries x. We use E(.) for the encoder and D(.) for the decoder. E\u266fp is a\npushforward measure of p, i.e., for every x \u2208      p, the sample E(x) is a sample from E\u266fp. We use\narrows in Section 3 to distinguish random variables of the forward (\u2192) and the reverse process (\u2190).\nThe standard diffusion modeling framework involves training a network, s\u03b8(xt, t), to learn the\nscore-function, \u2207xt log pt(xt), at all levels t, of a stochastic process described by an It\u00f4 SDE:\n                                      dx = f(x, t)dt + g(t)dw,                                         (1)\nwhere w is the standard Wiener process. To generate samples from the trained model, one can run\nthe (unconditional) Reverse SDE, where the score-function is approximated by the trained neural\nnetwork. Given measurements y = Ax0 + \u03c3yn, one can sample from the distribution p0(x0|y) by\nrunning the conditional Reverse SDE given by:\n             dx =    f(x, t) \u2212  g2(t) (\u2207xt log pt(xt) + \u2207xt log p(y|xt))      dt + g(t)dw.             (2)\nAs mentioned, \u2207xt log p(y|xt) is intractable for general inverse problems. One of the most effective\napproximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:\n                p(y|xt) \u2248   p (y|\u02c6\n                                 x0 := E[x0|xt]) = N(y; \u00b5 = AE[x0|xt], \u03a3 = \u03c32         yI).             (3)\nEssentially, DPS substitutes the unknown clean image x0 with its conditional expectation given the\nnoisy input, E[x0|xt]. Under this approximation, the term p(y|xt) becomes tractable.\nThe theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze\nDPS in a linear model setting where the data distribution lives in a low-dimensional subspace,\nand show that DPS actually samples from p(x0|y) (Section A.1). Then, we provide an algorithm\n(Section 2.1) and its analysis to sample from p(x0|y) using latent diffusion models (Section 3.2).\nImportantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while\navoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS.\nUsing experiments (Section 4), we show that our algorithm allows us to use powerful foundation\nmodels and solve linear inverse problems, outperforming previous unsupervised approaches without\nthe need for finetuning.\n    2\n    The source code is available at: https://github.com/LituRout/PSLD and a web application for image\ninpainting is available at: https://huggingface.co/spaces/PSLD/PSLD.\n                                                    3", "md": "diffusion in the latent space and use a decoder, D(z0), to move back to the pixel space. LDMs\npower state-of-the-art foundation models such as Stable Diffusion [41] and have enabled a wide-\nrange of applications across many data modalities including images [41], video [4], audio [30]\nand medical domain distributions (e.g., for MRI and proteins) [38, 51]. Unfortunately, none of the\nexisting algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to\nuse a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform\nfinetuning for each task of interest.\n\nIn this paper, we present the first framework to solve general inverse problems with pre-trained latent\ndiffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the\ndiffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing\nthe power of available foundation models, we are able to outperform previous approaches without\nfinetuning across a wide range of problems (see Figure 1 and 2).\n\nOur contributions are as follows:\n\n1. We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve\nlinear inverse problem when the degradation operator is known.\n2. We theoretically analyze our algorithm and show provable sample recovery in a linear model\nsetting with two-step diffusion processes.\n3. We achieve a new state-of-the-art for solving inverse problems with latent diffusion models,\noutperforming previous approaches for inpainting, block inpainting, denoising, deblurring,\ndestriping, and super-resolution.\n\n## Background and Method\n\nNotation: Bold lower-case \\( x \\), bold upper-case \\( X \\), and normal lower case \\( x \\) denote a vector, a matrix,\nand a scalar variable, respectively. We denote by \\( \\odot \\) element-wise multiplication. \\( D(x) \\) represents\na diagonal matrix with entries \\( x \\). We use \\( E(.) \\) for the encoder and \\( D(.) \\) for the decoder. \\( E^\\#p \\) is a\npushforward measure of \\( p \\), i.e., for every \\( x \\in p \\), the sample \\( E(x) \\) is a sample from \\( E^\\#p \\). We use\narrows in Section 3 to distinguish random variables of the forward (\\( \\rightarrow \\)) and the reverse process (\\( \\leftarrow \\)).\n\nThe standard diffusion modeling framework involves training a network, \\( s_\\theta(x_t, t) \\), to learn the\nscore-function, \\( \\nabla_{x_t} \\log p_t(x_t) \\), at all levels \\( t \\), of a stochastic process described by an It\u00f4 SDE:\n\n\\[\ndx = f(x, t)dt + g(t)dw \\quad (1)\n\\]\n\nwhere \\( w \\) is the standard Wiener process. To generate samples from the trained model, one can run\nthe (unconditional) Reverse SDE, where the score-function is approximated by the trained neural\nnetwork. Given measurements \\( y = Ax_0 + \\sigma y_n \\), one can sample from the distribution \\( p_0(x_0|y) \\) by\nrunning the conditional Reverse SDE given by:\n\n\\[\ndx = f(x, t) - g^2(t) \\left( \\nabla_{x_t} \\log p_t(x_t) + \\nabla_{x_t} \\log p(y|x_t) \\right) dt + g(t)dw \\quad (2)\n\\]\n\nAs mentioned, \\( \\nabla_{x_t} \\log p(y|x_t) \\) is intractable for general inverse problems. One of the most effective\napproximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:\n\n\\[\np(y|x_t) \\approx p \\left( y|\\hat{x}_0 := E[x_0|x_t] \\right) = N(y; \\mu = AE[x_0|x_t], \\Sigma = \\sigma^2_y I) \\quad (3)\n\\]\n\nEssentially, DPS substitutes the unknown clean image \\( x_0 \\) with its conditional expectation given the\nnoisy input, \\( E[x_0|x_t] \\). Under this approximation, the term \\( p(y|x_t) \\) becomes tractable.\n\nThe theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze\nDPS in a linear model setting where the data distribution lives in a low-dimensional subspace,\nand show that DPS actually samples from \\( p(x_0|y) \\) (Section A.1). Then, we provide an algorithm\n(Section 2.1) and its analysis to sample from \\( p(x_0|y) \\) using latent diffusion models (Section 3.2).\nImportantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while\navoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS.\n\nUsing experiments (Section 4), we show that our algorithm allows us to use powerful foundation\nmodels and solve linear inverse problems, outperforming previous unsupervised approaches without\nthe need for finetuning.\n\nThe source code is available at: https://github.com/LituRout/PSLD and a web application for image\ninpainting is available at: https://huggingface.co/spaces/PSLD/PSLD.", "images": [], "items": [{"type": "text", "value": "diffusion in the latent space and use a decoder, D(z0), to move back to the pixel space. LDMs\npower state-of-the-art foundation models such as Stable Diffusion [41] and have enabled a wide-\nrange of applications across many data modalities including images [41], video [4], audio [30]\nand medical domain distributions (e.g., for MRI and proteins) [38, 51]. Unfortunately, none of the\nexisting algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to\nuse a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform\nfinetuning for each task of interest.\n\nIn this paper, we present the first framework to solve general inverse problems with pre-trained latent\ndiffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the\ndiffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing\nthe power of available foundation models, we are able to outperform previous approaches without\nfinetuning across a wide range of problems (see Figure 1 and 2).\n\nOur contributions are as follows:\n\n1. We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve\nlinear inverse problem when the degradation operator is known.\n2. We theoretically analyze our algorithm and show provable sample recovery in a linear model\nsetting with two-step diffusion processes.\n3. We achieve a new state-of-the-art for solving inverse problems with latent diffusion models,\noutperforming previous approaches for inpainting, block inpainting, denoising, deblurring,\ndestriping, and super-resolution.", "md": "diffusion in the latent space and use a decoder, D(z0), to move back to the pixel space. LDMs\npower state-of-the-art foundation models such as Stable Diffusion [41] and have enabled a wide-\nrange of applications across many data modalities including images [41], video [4], audio [30]\nand medical domain distributions (e.g., for MRI and proteins) [38, 51]. Unfortunately, none of the\nexisting algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to\nuse a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform\nfinetuning for each task of interest.\n\nIn this paper, we present the first framework to solve general inverse problems with pre-trained latent\ndiffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the\ndiffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing\nthe power of available foundation models, we are able to outperform previous approaches without\nfinetuning across a wide range of problems (see Figure 1 and 2).\n\nOur contributions are as follows:\n\n1. We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve\nlinear inverse problem when the degradation operator is known.\n2. We theoretically analyze our algorithm and show provable sample recovery in a linear model\nsetting with two-step diffusion processes.\n3. We achieve a new state-of-the-art for solving inverse problems with latent diffusion models,\noutperforming previous approaches for inpainting, block inpainting, denoising, deblurring,\ndestriping, and super-resolution."}, {"type": "heading", "lvl": 2, "value": "Background and Method", "md": "## Background and Method"}, {"type": "text", "value": "Notation: Bold lower-case \\( x \\), bold upper-case \\( X \\), and normal lower case \\( x \\) denote a vector, a matrix,\nand a scalar variable, respectively. We denote by \\( \\odot \\) element-wise multiplication. \\( D(x) \\) represents\na diagonal matrix with entries \\( x \\). We use \\( E(.) \\) for the encoder and \\( D(.) \\) for the decoder. \\( E^\\#p \\) is a\npushforward measure of \\( p \\), i.e., for every \\( x \\in p \\), the sample \\( E(x) \\) is a sample from \\( E^\\#p \\). We use\narrows in Section 3 to distinguish random variables of the forward (\\( \\rightarrow \\)) and the reverse process (\\( \\leftarrow \\)).\n\nThe standard diffusion modeling framework involves training a network, \\( s_\\theta(x_t, t) \\), to learn the\nscore-function, \\( \\nabla_{x_t} \\log p_t(x_t) \\), at all levels \\( t \\), of a stochastic process described by an It\u00f4 SDE:\n\n\\[\ndx = f(x, t)dt + g(t)dw \\quad (1)\n\\]\n\nwhere \\( w \\) is the standard Wiener process. To generate samples from the trained model, one can run\nthe (unconditional) Reverse SDE, where the score-function is approximated by the trained neural\nnetwork. Given measurements \\( y = Ax_0 + \\sigma y_n \\), one can sample from the distribution \\( p_0(x_0|y) \\) by\nrunning the conditional Reverse SDE given by:\n\n\\[\ndx = f(x, t) - g^2(t) \\left( \\nabla_{x_t} \\log p_t(x_t) + \\nabla_{x_t} \\log p(y|x_t) \\right) dt + g(t)dw \\quad (2)\n\\]\n\nAs mentioned, \\( \\nabla_{x_t} \\log p(y|x_t) \\) is intractable for general inverse problems. One of the most effective\napproximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:\n\n\\[\np(y|x_t) \\approx p \\left( y|\\hat{x}_0 := E[x_0|x_t] \\right) = N(y; \\mu = AE[x_0|x_t], \\Sigma = \\sigma^2_y I) \\quad (3)\n\\]\n\nEssentially, DPS substitutes the unknown clean image \\( x_0 \\) with its conditional expectation given the\nnoisy input, \\( E[x_0|x_t] \\). Under this approximation, the term \\( p(y|x_t) \\) becomes tractable.\n\nThe theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze\nDPS in a linear model setting where the data distribution lives in a low-dimensional subspace,\nand show that DPS actually samples from \\( p(x_0|y) \\) (Section A.1). Then, we provide an algorithm\n(Section 2.1) and its analysis to sample from \\( p(x_0|y) \\) using latent diffusion models (Section 3.2).\nImportantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while\navoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS.\n\nUsing experiments (Section 4), we show that our algorithm allows us to use powerful foundation\nmodels and solve linear inverse problems, outperforming previous unsupervised approaches without\nthe need for finetuning.\n\nThe source code is available at: https://github.com/LituRout/PSLD and a web application for image\ninpainting is available at: https://huggingface.co/spaces/PSLD/PSLD.", "md": "Notation: Bold lower-case \\( x \\), bold upper-case \\( X \\), and normal lower case \\( x \\) denote a vector, a matrix,\nand a scalar variable, respectively. We denote by \\( \\odot \\) element-wise multiplication. \\( D(x) \\) represents\na diagonal matrix with entries \\( x \\). We use \\( E(.) \\) for the encoder and \\( D(.) \\) for the decoder. \\( E^\\#p \\) is a\npushforward measure of \\( p \\), i.e., for every \\( x \\in p \\), the sample \\( E(x) \\) is a sample from \\( E^\\#p \\). We use\narrows in Section 3 to distinguish random variables of the forward (\\( \\rightarrow \\)) and the reverse process (\\( \\leftarrow \\)).\n\nThe standard diffusion modeling framework involves training a network, \\( s_\\theta(x_t, t) \\), to learn the\nscore-function, \\( \\nabla_{x_t} \\log p_t(x_t) \\), at all levels \\( t \\), of a stochastic process described by an It\u00f4 SDE:\n\n\\[\ndx = f(x, t)dt + g(t)dw \\quad (1)\n\\]\n\nwhere \\( w \\) is the standard Wiener process. To generate samples from the trained model, one can run\nthe (unconditional) Reverse SDE, where the score-function is approximated by the trained neural\nnetwork. Given measurements \\( y = Ax_0 + \\sigma y_n \\), one can sample from the distribution \\( p_0(x_0|y) \\) by\nrunning the conditional Reverse SDE given by:\n\n\\[\ndx = f(x, t) - g^2(t) \\left( \\nabla_{x_t} \\log p_t(x_t) + \\nabla_{x_t} \\log p(y|x_t) \\right) dt + g(t)dw \\quad (2)\n\\]\n\nAs mentioned, \\( \\nabla_{x_t} \\log p(y|x_t) \\) is intractable for general inverse problems. One of the most effective\napproximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:\n\n\\[\np(y|x_t) \\approx p \\left( y|\\hat{x}_0 := E[x_0|x_t] \\right) = N(y; \\mu = AE[x_0|x_t], \\Sigma = \\sigma^2_y I) \\quad (3)\n\\]\n\nEssentially, DPS substitutes the unknown clean image \\( x_0 \\) with its conditional expectation given the\nnoisy input, \\( E[x_0|x_t] \\). Under this approximation, the term \\( p(y|x_t) \\) becomes tractable.\n\nThe theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze\nDPS in a linear model setting where the data distribution lives in a low-dimensional subspace,\nand show that DPS actually samples from \\( p(x_0|y) \\) (Section A.1). Then, we provide an algorithm\n(Section 2.1) and its analysis to sample from \\( p(x_0|y) \\) using latent diffusion models (Section 3.2).\nImportantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while\navoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS.\n\nUsing experiments (Section 4), we show that our algorithm allows us to use powerful foundation\nmodels and solve linear inverse problems, outperforming previous unsupervised approaches without\nthe need for finetuning.\n\nThe source code is available at: https://github.com/LituRout/PSLD and a web application for image\ninpainting is available at: https://huggingface.co/spaces/PSLD/PSLD."}]}, {"page": 4, "text": " 2.1     Method\n In Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model\n s\u03b8(zt, t) to predict the score \u2207zt log pt(zt), of a diffusion process:\n                                                     dz = f(z, t)dt + g(t)dw,                                                               (4)\n where z0 = E(x0) for some encoder function E(\u00b7) : Rd \u2192                                Rk. During sampling, we start with zT ,\n we run the Reverse Diffusion Process and then we obtain a clean image by passing z0 \u223c                                           p0(z0|zT )\n through a decoder D : Rk \u2192                Rd.\n Although Latent Diffusion Models underlie some of the most powerful foundation models for image\n generation, existing algorithms for solving inverse problems with diffusion models do not apply for\n LDMs. The most natural extension of the DPS idea would be to approximate p(y|zt) with:\n                                              p(y|zt) \u2248       p(y|x0 = D (E[z0|zt])),                                                       (5)\n i.e., to approximate the unknown clean image x0 with the decoded version of the conditional\n expectation of the clean latent z0 given the noisy latent zt. However, as we show experimentally in\n Section 4, this idea does not work. The failure of the \u201cvanilla\u201d extension of the DPS algorithm for\n latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a\n many-to-one mapping. Simply put, there are many latents z0 that correspond to encoded versions\n of images that explain the measurements. Taking the gradient of the density given by (5) could be\n pulling zt towards any of these latents z0, potentially in different directions. On the other hand, the\n score-function is pulling zt towards a specific z0 that corresponds to the best denoised version of zt.\n To address this problem, we propose an extra term that penalizes latents that are not fixed-points of\n the composition of the decoder-function with the encoder-function. Specifically, we approximate the\n intractable \u2207      log p(y|zt) with:\n   \u2207zt log p(y|zt) = \u2207zt log p(y|\u02c6               x0 = D (E[z0|zt]))           +\u03b3t \u2207zt ||E[z0|zt] \u2212            E(D(E[z0|zt]))||2          . (6)\n                                        DPS vanilla extension                                       \u201cgoodness\u201d of z0\nWe refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide\n the diffusion process towards latents such that: i) they explain the measurements when passed through\n the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful\n to make sure that the generated sample remains on the manifold of real data. However, it does not\n penalize the reverse SDE for generating other latents z0 as long as D(z0) lies on the manifold of\n natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary\n of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this\n problem by introducing the following gluing objective. In words, the gluing objective penalizes\n decoded images having a discontinuity at the boundary of the mask.\n               \u2207zt log p(y|zt) = \u2207zt log p(y|x0 = D (E[z0|zt]))\n                                                     DPS vanilla extension\n                                       + \u03b3t \u2207zt        E[z0|zt] \u2212       E(AT y + (I \u2212          AT A)D(E[z0|zt]))              2  .          (7)\n                                                                              \u201cgluing\u201d of z0\n The gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-\n matching update, and the gluing update point to the same optima in the latent space. We refer to this\n approximation (7) as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we\n provide an analysis of these gradient updates, along with the associated algorithms.\n Remark 2.1. Consider the optimization problem of projecting onto the measurements:\n                                                              min     \u2225\u02c6\n                                                               x0      x0 \u2212     x0\u22252  2\n                                                         subject to Ax0 = y,\n In the linear setting, the optimal solution is given by x\u2217                                   0    =      AT (AAT )\u22121y + (\u02c6             x0 \u2212\n AT (AAT )\u22121(A\u02c6          x0)). Now further suppose that the measurement rows are orthogonal, i.e. AAT =\n Il. This condition holds for some natural linear inverse problems like inpainting. Suppose that\n we want to update the latent vector zt such that E[z0|zt] = E(x\u2217                              0); this ensures that the gradients\n                                                                       4", "md": "## Method\n\nIn Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model \\( s_{\\theta}(z_t, t) \\) to predict the score \\( \\nabla_{z_t} \\log p_t(z_t) \\), of a diffusion process:\n\n\\[\ndz = f(z, t)dt + g(t)dw \\quad \\text{(4)}\n\\]\n\nwhere \\( z_0 = E(x_0) \\) for some encoder function \\( E(\\cdot) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\). During sampling, we start with \\( z_T \\), we run the Reverse Diffusion Process and then we obtain a clean image by passing \\( z_0 \\sim p_0(z_0|z_T) \\) through a decoder \\( D : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d \\).\n\nAlthough Latent Diffusion Models underlie some of the most powerful foundation models for image generation, existing algorithms for solving inverse problems with diffusion models do not apply for LDMs. The most natural extension of the DPS idea would be to approximate \\( p(y|z_t) \\) with:\n\n\\[\np(y|z_t) \\approx p(y|x_0 = D(E[z_0|z_t])) \\quad \\text{(5)}\n\\]\n\ni.e., to approximate the unknown clean image \\( x_0 \\) with the decoded version of the conditional expectation of the clean latent \\( z_0 \\) given the noisy latent \\( z_t \\). However, as we show experimentally in Section 4, this idea does not work. The failure of the \u201cvanilla\u201d extension of the DPS algorithm for latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a many-to-one mapping. Simply put, there are many latents \\( z_0 \\) that correspond to encoded versions of images that explain the measurements. Taking the gradient of the density given by (5) could be pulling \\( z_t \\) towards any of these latents \\( z_0 \\), potentially in different directions. On the other hand, the score-function is pulling \\( z_t \\) towards a specific \\( z_0 \\) that corresponds to the best denoised version of \\( z_t \\).\n\nTo address this problem, we propose an extra term that penalizes latents that are not fixed-points of the composition of the decoder-function with the encoder-function. Specifically, we approximate the intractable \\( \\nabla_{z_t} \\log p(y|z_t) \\) with:\n\n\\[\n\\nabla_{z_t} \\log p(y|z_t) = \\nabla_{z_t} \\log p(y|\\hat{x}_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla_{z_t} ||E[z_0|z_t] - E(D(E[z_0|z_t]))||^2 \\quad \\text{(6)}\n\\]\n\nWe refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide the diffusion process towards latents such that: i) they explain the measurements when passed through the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful to make sure that the generated sample remains on the manifold of real data. However, it does not penalize the reverse SDE for generating other latents \\( z_0 \\) as long as \\( D(z_0) \\) lies on the manifold of natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem by introducing the following gluing objective. In words, the gluing objective penalizes decoded images having a discontinuity at the boundary of the mask.\n\n\\[\n\\nabla_{z_t} \\log p(y|z_t) = \\nabla_{z_t} \\log p(y|x_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla_{z_t} ||E[z_0|z_t] - E(A^T y + (I - A^T A)D(E[z_0|z_t]))||^2 \\quad \\text{(7)}\n\\]\n\nThe gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-matching update, and the gluing update point to the same optima in the latent space. We refer to this approximation (7) as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we provide an analysis of these gradient updates, along with the associated algorithms.\n\nRemark 2.1. Consider the optimization problem of projecting onto the measurements:\n\n\\[\n\\min_{\\hat{x}_0} ||\\hat{x}_0 - x_0||_2^2 \\quad \\text{subject to} \\quad Ax_0 = y,\n\\]\n\nIn the linear setting, the optimal solution is given by \\( x^*_0 = A^T (AA^T)^{-1}y + (\\hat{x}_0 - A^T (AA^T)^{-1}(A\\hat{x}_0)) \\). Now further suppose that the measurement rows are orthogonal, i.e. \\( A^T = I \\). This condition holds for some natural linear inverse problems like inpainting. Suppose that we want to update the latent vector \\( z_t \\) such that \\( E[z_0|z_t] = E(x^*_0) \\); this ensures that the gradients", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Method", "md": "## Method"}, {"type": "text", "value": "In Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model \\( s_{\\theta}(z_t, t) \\) to predict the score \\( \\nabla_{z_t} \\log p_t(z_t) \\), of a diffusion process:\n\n\\[\ndz = f(z, t)dt + g(t)dw \\quad \\text{(4)}\n\\]\n\nwhere \\( z_0 = E(x_0) \\) for some encoder function \\( E(\\cdot) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\). During sampling, we start with \\( z_T \\), we run the Reverse Diffusion Process and then we obtain a clean image by passing \\( z_0 \\sim p_0(z_0|z_T) \\) through a decoder \\( D : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d \\).\n\nAlthough Latent Diffusion Models underlie some of the most powerful foundation models for image generation, existing algorithms for solving inverse problems with diffusion models do not apply for LDMs. The most natural extension of the DPS idea would be to approximate \\( p(y|z_t) \\) with:\n\n\\[\np(y|z_t) \\approx p(y|x_0 = D(E[z_0|z_t])) \\quad \\text{(5)}\n\\]\n\ni.e., to approximate the unknown clean image \\( x_0 \\) with the decoded version of the conditional expectation of the clean latent \\( z_0 \\) given the noisy latent \\( z_t \\). However, as we show experimentally in Section 4, this idea does not work. The failure of the \u201cvanilla\u201d extension of the DPS algorithm for latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a many-to-one mapping. Simply put, there are many latents \\( z_0 \\) that correspond to encoded versions of images that explain the measurements. Taking the gradient of the density given by (5) could be pulling \\( z_t \\) towards any of these latents \\( z_0 \\), potentially in different directions. On the other hand, the score-function is pulling \\( z_t \\) towards a specific \\( z_0 \\) that corresponds to the best denoised version of \\( z_t \\).\n\nTo address this problem, we propose an extra term that penalizes latents that are not fixed-points of the composition of the decoder-function with the encoder-function. Specifically, we approximate the intractable \\( \\nabla_{z_t} \\log p(y|z_t) \\) with:\n\n\\[\n\\nabla_{z_t} \\log p(y|z_t) = \\nabla_{z_t} \\log p(y|\\hat{x}_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla_{z_t} ||E[z_0|z_t] - E(D(E[z_0|z_t]))||^2 \\quad \\text{(6)}\n\\]\n\nWe refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide the diffusion process towards latents such that: i) they explain the measurements when passed through the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful to make sure that the generated sample remains on the manifold of real data. However, it does not penalize the reverse SDE for generating other latents \\( z_0 \\) as long as \\( D(z_0) \\) lies on the manifold of natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem by introducing the following gluing objective. In words, the gluing objective penalizes decoded images having a discontinuity at the boundary of the mask.\n\n\\[\n\\nabla_{z_t} \\log p(y|z_t) = \\nabla_{z_t} \\log p(y|x_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla_{z_t} ||E[z_0|z_t] - E(A^T y + (I - A^T A)D(E[z_0|z_t]))||^2 \\quad \\text{(7)}\n\\]\n\nThe gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-matching update, and the gluing update point to the same optima in the latent space. We refer to this approximation (7) as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we provide an analysis of these gradient updates, along with the associated algorithms.\n\nRemark 2.1. Consider the optimization problem of projecting onto the measurements:\n\n\\[\n\\min_{\\hat{x}_0} ||\\hat{x}_0 - x_0||_2^2 \\quad \\text{subject to} \\quad Ax_0 = y,\n\\]\n\nIn the linear setting, the optimal solution is given by \\( x^*_0 = A^T (AA^T)^{-1}y + (\\hat{x}_0 - A^T (AA^T)^{-1}(A\\hat{x}_0)) \\). Now further suppose that the measurement rows are orthogonal, i.e. \\( A^T = I \\). This condition holds for some natural linear inverse problems like inpainting. Suppose that we want to update the latent vector \\( z_t \\) such that \\( E[z_0|z_t] = E(x^*_0) \\); this ensures that the gradients", "md": "In Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model \\( s_{\\theta}(z_t, t) \\) to predict the score \\( \\nabla_{z_t} \\log p_t(z_t) \\), of a diffusion process:\n\n\\[\ndz = f(z, t)dt + g(t)dw \\quad \\text{(4)}\n\\]\n\nwhere \\( z_0 = E(x_0) \\) for some encoder function \\( E(\\cdot) : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k \\). During sampling, we start with \\( z_T \\), we run the Reverse Diffusion Process and then we obtain a clean image by passing \\( z_0 \\sim p_0(z_0|z_T) \\) through a decoder \\( D : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d \\).\n\nAlthough Latent Diffusion Models underlie some of the most powerful foundation models for image generation, existing algorithms for solving inverse problems with diffusion models do not apply for LDMs. The most natural extension of the DPS idea would be to approximate \\( p(y|z_t) \\) with:\n\n\\[\np(y|z_t) \\approx p(y|x_0 = D(E[z_0|z_t])) \\quad \\text{(5)}\n\\]\n\ni.e., to approximate the unknown clean image \\( x_0 \\) with the decoded version of the conditional expectation of the clean latent \\( z_0 \\) given the noisy latent \\( z_t \\). However, as we show experimentally in Section 4, this idea does not work. The failure of the \u201cvanilla\u201d extension of the DPS algorithm for latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a many-to-one mapping. Simply put, there are many latents \\( z_0 \\) that correspond to encoded versions of images that explain the measurements. Taking the gradient of the density given by (5) could be pulling \\( z_t \\) towards any of these latents \\( z_0 \\), potentially in different directions. On the other hand, the score-function is pulling \\( z_t \\) towards a specific \\( z_0 \\) that corresponds to the best denoised version of \\( z_t \\).\n\nTo address this problem, we propose an extra term that penalizes latents that are not fixed-points of the composition of the decoder-function with the encoder-function. Specifically, we approximate the intractable \\( \\nabla_{z_t} \\log p(y|z_t) \\) with:\n\n\\[\n\\nabla_{z_t} \\log p(y|z_t) = \\nabla_{z_t} \\log p(y|\\hat{x}_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla_{z_t} ||E[z_0|z_t] - E(D(E[z_0|z_t]))||^2 \\quad \\text{(6)}\n\\]\n\nWe refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide the diffusion process towards latents such that: i) they explain the measurements when passed through the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful to make sure that the generated sample remains on the manifold of real data. However, it does not penalize the reverse SDE for generating other latents \\( z_0 \\) as long as \\( D(z_0) \\) lies on the manifold of natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem by introducing the following gluing objective. In words, the gluing objective penalizes decoded images having a discontinuity at the boundary of the mask.\n\n\\[\n\\nabla_{z_t} \\log p(y|z_t) = \\nabla_{z_t} \\log p(y|x_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla_{z_t} ||E[z_0|z_t] - E(A^T y + (I - A^T A)D(E[z_0|z_t]))||^2 \\quad \\text{(7)}\n\\]\n\nThe gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-matching update, and the gluing update point to the same optima in the latent space. We refer to this approximation (7) as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we provide an analysis of these gradient updates, along with the associated algorithms.\n\nRemark 2.1. Consider the optimization problem of projecting onto the measurements:\n\n\\[\n\\min_{\\hat{x}_0} ||\\hat{x}_0 - x_0||_2^2 \\quad \\text{subject to} \\quad Ax_0 = y,\n\\]\n\nIn the linear setting, the optimal solution is given by \\( x^*_0 = A^T (AA^T)^{-1}y + (\\hat{x}_0 - A^T (AA^T)^{-1}(A\\hat{x}_0)) \\). Now further suppose that the measurement rows are orthogonal, i.e. \\( A^T = I \\). This condition holds for some natural linear inverse problems like inpainting. Suppose that we want to update the latent vector \\( z_t \\) such that \\( E[z_0|z_t] = E(x^*_0) \\); this ensures that the gradients"}]}, {"page": 5, "text": "   Algorithm 1: DPS                                             Algorithm 2: PSLD\n   Input: T , y, \u03b6T     \u03c3i}T                                     Input: T , y, {\u03b7i}T                \u03c3i}T\n1 xT \u223c    N (0, I)i=1, {\u02dc   i=1, s\u03b8                           1 zT \u223c    N (0, I)   i=1, {\u03b3i}Ti=1, {\u02dc    i=1, E, D, A, s\u03b8\n2 for i = T \u2212    1 to 0 do                                    2 for i = T \u2212   1 to 0 do\n         \u02c6                                                            \u02c6\n 3      s \u2190   s\u03b8(xi, i)                                       3       s \u2190  s\u03b8(zi, i)\n         \u02c6      \u221a 1               \u03b1i)\u02c6s)                              \u02c6      \u221a 1               \u03b1i)\u02c6s)\n 4      x0 \u2190      \u03b1\u00afi (xi + (1 \u2212   \u00af                          4       z0 \u2190      \u00af\n                                                                                \u03b1i (zi + (1 \u2212   \u00af\n 5      z \u223c   N (0, I)                                        5       \u03f5 \u223c  N (0, I)                   \u221a \u00af\n 6      x\u2032i\u22121 \u2190                                                                 \u221a\u03b1i(1\u2212\u00af \u03b1 i\u22121)          \u03b1i\u22121\u03b2i   \u02c6\n                                                              6       z\u2032                       zi +             z0 + \u02dc\u03c3i\u03f5\n          \u221a\u03b1i(1\u2212\u00af  \u03b1i\u22121)        \u221a\u00af \u03b1i\u22121\u03b2i  \u02c6                           i\u22121 \u2190         1\u2212\u00af\u03b1i              1\u2212\u00af\u03b1i\n               1\u2212\u00af\u03b1 i     xi +     1\u2212\u00af\u03b1    x0 + \u02dc\u03c3iz          7       z\u2032\u2032                                    z0))\u22252\n                                       i                               i\u22121 \u2190    z\u2032\n 7      xi\u22121 \u2190    x\u2032i\u22121 \u2212   \u03b6i\u2207xi\u2225y \u2212    A(\u02c6 x0)\u222522           8       zi\u22121 \u2190     i\u22121 \u2212   \u03b7i\u2207zi\u2225y \u2212    A(D(\u02c6        2\n8 end                                                                   z\u2032\u2032              z0 \u2212  E(AT y + (I \u2212     AT A)D(\u02c6  z0))\u22252\n9 return \u02c6x0                                                  9 end      i\u22121 \u2212  \u03b3i\u2207zi\u2225\u02c6                                           2\n                                                             10 return D(\u02c6 z0)\n   resulting from the two terms in (7) both point to the same optima in the latent space. Equivalently,\n   we want to solve the following minimization problem: minz                            t \u2225E[z0|zt] \u2212      E(x\u2217 0)\u22252 2. Substituting\n   E(x\u2217  0) = E(AT y+(\u02c6       x0\u2212AT A\u02c6       x0)) = E(AT y+(I\u2212AT A)\u02c6                x0), and \u02c6  x0 = D(E[z0|zt]), we can thus\n   interpret the gluing objective in (7) as a one step of gradient descent of this loss \u2225E[z0|zt] \u2212                           E(x\u2217  0)\u222522\n   with respect to zt. Note that, if there was no latent space, our gluing would be equivalent to a\n   projection on the measurements, but now because of the encoder and decoder, it is not.\n   3     Theoretical Results\n   As discussed in Section 2, diffusion models consist of two stochastic processes: the forward and\n   reverse processes, each governed by It\u00f4 SDEs. For implementation purposes, these SDEs are\n   discretized over a finite number of (time) steps, and the diffusion takes place using a transition\n   kernel. The forward process starts from \u2212                  \u2192            \u2192                                                 \u2192\n   \u221a                                                          x0 \u223c      p(\u2212x0) and gradually adds noise, i.e., \u2212x t+1 =\n      1 \u2212   \u03b2t\u2212\u2192x t + \u221a\u03b2t\u03f5 where \u03b2t \u2208            [0, 1] and \u03b2t \u2265       \u03b2t\u22121 for t = 0, . . . , T \u2212       1 . The reverse process\n                              \u2212                                            \u2212                  \u2212\n   is initialized with \u2190x T \u223c           N (0, Id) and generates \u2190x t\u22121 = \u00b5\u03b8(\u2190x t, t) + \u221a\u03b2t\u03f5. In the last step,\n         \u2212\n   \u00b5\u03b8(\u2190x 1, 1) is displayed without the noise.\n   In this section, we consider the diffusion discretized to two steps ({\u2212                  \u2192    \u2192\n                                                                                           x0, \u2212x1}), and a Gaussian transition\n   kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures\n   essential components of complex diffusion processes without raising unnecessary complications in the\n   analysis. We provide a principled analysis of Algorithm 1 and Algorithm 2 in a linear model setting\n   with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible\n   in principle. A main result of our work is to prove that in this setting we can solve inverse problems\n   perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory.\n   In Section 4, we then show that these algorithmic ideas are much more general, and apply to large-\n   scale real-world applications of diffusion models that use multiple steps ({\u2212                         \u2192     \u2192           \u2192\n                                                                                                         x0, \u2212x1, \u00b7 \u00b7 \u00b7 , \u2212\n   T = 1000), and moreover do not satisfy the recoverability assumptions. We provide post-processing                      xT }, where\n   details of Algorithm 2 in Appendix C.1. All proofs are given in Appendix B.\n   3.1     Problem Setup\n   The goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse\n   problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion\n   processes in a linear model setting similar to [42, 7], where the images (\u2212                        \u2192\n   subspace of the form \u2212         \u2192           \u2192                    \u2192                                 x0 \u2208    Rd) reside in a linear\n                                 x0 = S\u2212     w0, S \u2208     Rd\u00d7l, \u2212  w0 \u2208  \u2192  Rl, and \u03c3y = 0. Here, S is a tall thin matrix\n   with rank(S) = l \u2264           d that lifts any latent vector \u2212   \u2192   w0 \u223c     N (0, Il) to the image space with ambient\n   dimension d. Given the measurements y = A\u2212                     x0 + \u03c3yn, A \u2208          Rl\u00d7d, n \u2208      Rl, the goal is to sample\n   from p0(\u2212    \u2192\n               x0|y) using a pre-trained latent diffusion model. In the inpainting task, the measurement\n   operator A is such that AT A is a diagonal matrix D(m), where m is the masking vector with\n   elements set to 1 where data is observed and 0 where data is masked (see Appendix B for further\n   details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a\n   pre-trained Variational Autoencoder (VAE). Following the common practice [41], we consider a\n   setting where the latent vector of the VAE is k-dimensional and the latent distribution is a standard\n   Gaussian N (0, Ik). Our analysis shows that the proposed Algorithm 2 provably solves inverse\n   problems under the following assumptions.\n                                                                      5", "md": "# Math Equations and Text\n\n## Algorithm 1: DPS\n\nInput: \\(T\\), \\(y\\), \\(\\zeta_T\\), \\(\\sigma_i\\) for \\(i=1, \\ldots, T\\)\n\n$$\n\\begin{align*}\nx_T &\\sim N(0, I) \\\\\n\\text{for } i &= T - 1 \\text{ to } 0 \\text{ do} \\\\\ns &\\leftarrow s_{\\theta}(x_i, i) \\\\\nx_0 &\\leftarrow \\bar{\\alpha}_i (x_i + (1 - \\bar{\\alpha}_i) x_0) \\\\\nz &\\sim N(0, I) \\\\\nx'_{i-1} &\\leftarrow \\sqrt{\\alpha_i(1 - \\bar{\\alpha}_{i-1})} x_i + (1 - \\bar{\\alpha}_i) x_0 + \\tilde{\\sigma}_i z \\\\\nx_{i-1} &\\leftarrow x'_{i-1} - \\zeta_i \\nabla x_i \\| y - A(\\hat{x}_0) \\|_2^2 \\\\\n\\text{end} \\\\\n\\text{return } \\hat{x}_0\n\\end{align*}\n$$\n\n## Algorithm 2: PSLD\n\nInput: \\(T\\), \\(y\\), \\(\\{\\eta_i\\}_T\\), \\(\\sigma_i\\) for \\(i=1, \\ldots, T\\)\n\n$$\n\\begin{align*}\nz_T &\\sim N(0, I) \\\\\n\\text{for } i &= T - 1 \\text{ to } 0 \\text{ do} \\\\\ns &\\leftarrow s_{\\theta}(z_i, i) \\\\\nz_0 &\\leftarrow \\bar{\\alpha}_i (z_i + (1 - \\bar{\\alpha}_i) z_0) \\\\\n\\epsilon &\\sim N(0, I) \\\\\nz'_{i-1} &\\leftarrow \\sqrt{\\alpha_i(1 - \\bar{\\alpha}_{i-1})} z_i + (1 - \\bar{\\alpha}_i) z_0 + \\tilde{\\sigma}_i \\epsilon \\\\\nz''_{i-1} &\\leftarrow \\frac{z'}{\\| z' - z_0 \\|_2} \\\\\n\\text{end} \\\\\n\\text{return } D(\\hat{z}_0)\n\\end{align*}\n$$\n\nTheoretical Results:\n\nThe resulting from the two terms in (7) both point to the same optima in the latent space. Equivalently, we want to solve the following minimization problem: \\( \\min_z t \\|E[z_0|z_t] - E(x^*_0)\\|_2^2 \\). Substituting \\( E(x^*_0) = E(A^T y + (\\hat{x}_0 - A^T A \\hat{x}_0)) = E(A^T y + (I - A^T A) \\hat{x}_0) \\), and \\( \\hat{x}_0 = D(E[z_0|z_t]) \\), we can thus interpret the gluing objective in (7) as a one step of gradient descent of this loss \\( \\|E[z_0|z_t] - E(x^*_0)\\|_2^2 \\) with respect to \\( z_t \\). Note that, if there was no latent space, our gluing would be equivalent to a projection on the measurements, but now because of the encoder and decoder, it is not.\n\n### Theoretical Results\n\nAs discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse processes, each governed by It\u00f4 SDEs. For implementation purposes, these SDEs are discretized over a finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward process starts from \\( x_0 \\sim p(-x_0) \\) and gradually adds noise, i.e., \\( x_{t+1} = (1 - \\beta_t) x_t + \\sqrt{\\beta_t} \\epsilon \\) where \\( \\beta_t \\in [0, 1] \\) and \\( \\beta_t \\geq \\beta_{t-1} \\) for \\( t = 0, \\ldots, T - 1 \\). The reverse process is initialized with \\( x_T \\sim N(0, I_d) \\) and generates \\( x_{t-1} = \\mu_{\\theta}(x_t, t) + \\sqrt{\\beta_t} \\epsilon \\). In the last step, \\( \\mu_{\\theta}(x_1, 1) \\) is displayed without the noise.\n\nIn this section, we consider the diffusion discretized to two steps (\\( x_0, x_1 \\)), and a Gaussian transition kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures essential components of complex diffusion processes without raising unnecessary complications in the analysis. We provide a principled analysis of Algorithm 1 and Algorithm 2 in a linear model setting with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible in principle. A main result of our work is to prove that in this setting we can solve inverse problems perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory.\n\nIn Section 4, we then show that these algorithmic ideas are much more general, and apply to large-scale real-world applications of diffusion models that use multiple steps (\\( x_0, x_1, \\ldots, x_T = 1000 \\)), and moreover do not satisfy the recoverability assumptions. We provide post-processing details of Algorithm 2 in Appendix C.1. All proofs are given in Appendix B.\n\n#### Problem Setup\n\nThe goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion processes in a linear model setting similar to [42, 7], where the images (\\( x_0 \\) subspace of the form \\( x_0 = S w_0, S \\in \\mathbb{R}^{d \\times l}, w_0 \\in \\mathbb{R}^l \\), and \\( \\sigma_y = 0 \\). Here, \\( S \\) is a tall thin matrix with \\( \\text{rank}(S) = l \\leq d \\) that lifts any latent vector \\( w_0 \\sim N(0, I_l) \\) to the image space with ambient dimension \\( d \\). Given the measurements \\( y = A x_0 + \\sigma_y n, A \\in \\mathbb{R}^{l \\times d}, n \\in \\mathbb{R}^l \\), the goal is to sample from \\( p_0(x_0|y) \\) using a pre-trained latent diffusion model. In the inpainting task, the measurement operator \\( A \\) is such that \\( A^T A \\) is a diagonal matrix \\( D(m) \\), where \\( m \\) is the masking vector with elements set to 1 where data is observed and 0 where data is masked (see Appendix B for further details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder (VAE). Following the common practice [41], we consider a setting where the latent vector of the VAE is \\( k \\)-dimensional and the latent distribution is a standard Gaussian \\( N(0, I_k) \\). Our analysis shows that the proposed Algorithm 2 provably solves inverse problems under the following assumptions.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Algorithm 1: DPS", "md": "## Algorithm 1: DPS"}, {"type": "text", "value": "Input: \\(T\\), \\(y\\), \\(\\zeta_T\\), \\(\\sigma_i\\) for \\(i=1, \\ldots, T\\)\n\n$$\n\\begin{align*}\nx_T &\\sim N(0, I) \\\\\n\\text{for } i &= T - 1 \\text{ to } 0 \\text{ do} \\\\\ns &\\leftarrow s_{\\theta}(x_i, i) \\\\\nx_0 &\\leftarrow \\bar{\\alpha}_i (x_i + (1 - \\bar{\\alpha}_i) x_0) \\\\\nz &\\sim N(0, I) \\\\\nx'_{i-1} &\\leftarrow \\sqrt{\\alpha_i(1 - \\bar{\\alpha}_{i-1})} x_i + (1 - \\bar{\\alpha}_i) x_0 + \\tilde{\\sigma}_i z \\\\\nx_{i-1} &\\leftarrow x'_{i-1} - \\zeta_i \\nabla x_i \\| y - A(\\hat{x}_0) \\|_2^2 \\\\\n\\text{end} \\\\\n\\text{return } \\hat{x}_0\n\\end{align*}\n$$", "md": "Input: \\(T\\), \\(y\\), \\(\\zeta_T\\), \\(\\sigma_i\\) for \\(i=1, \\ldots, T\\)\n\n$$\n\\begin{align*}\nx_T &\\sim N(0, I) \\\\\n\\text{for } i &= T - 1 \\text{ to } 0 \\text{ do} \\\\\ns &\\leftarrow s_{\\theta}(x_i, i) \\\\\nx_0 &\\leftarrow \\bar{\\alpha}_i (x_i + (1 - \\bar{\\alpha}_i) x_0) \\\\\nz &\\sim N(0, I) \\\\\nx'_{i-1} &\\leftarrow \\sqrt{\\alpha_i(1 - \\bar{\\alpha}_{i-1})} x_i + (1 - \\bar{\\alpha}_i) x_0 + \\tilde{\\sigma}_i z \\\\\nx_{i-1} &\\leftarrow x'_{i-1} - \\zeta_i \\nabla x_i \\| y - A(\\hat{x}_0) \\|_2^2 \\\\\n\\text{end} \\\\\n\\text{return } \\hat{x}_0\n\\end{align*}\n$$"}, {"type": "heading", "lvl": 2, "value": "Algorithm 2: PSLD", "md": "## Algorithm 2: PSLD"}, {"type": "text", "value": "Input: \\(T\\), \\(y\\), \\(\\{\\eta_i\\}_T\\), \\(\\sigma_i\\) for \\(i=1, \\ldots, T\\)\n\n$$\n\\begin{align*}\nz_T &\\sim N(0, I) \\\\\n\\text{for } i &= T - 1 \\text{ to } 0 \\text{ do} \\\\\ns &\\leftarrow s_{\\theta}(z_i, i) \\\\\nz_0 &\\leftarrow \\bar{\\alpha}_i (z_i + (1 - \\bar{\\alpha}_i) z_0) \\\\\n\\epsilon &\\sim N(0, I) \\\\\nz'_{i-1} &\\leftarrow \\sqrt{\\alpha_i(1 - \\bar{\\alpha}_{i-1})} z_i + (1 - \\bar{\\alpha}_i) z_0 + \\tilde{\\sigma}_i \\epsilon \\\\\nz''_{i-1} &\\leftarrow \\frac{z'}{\\| z' - z_0 \\|_2} \\\\\n\\text{end} \\\\\n\\text{return } D(\\hat{z}_0)\n\\end{align*}\n$$\n\nTheoretical Results:\n\nThe resulting from the two terms in (7) both point to the same optima in the latent space. Equivalently, we want to solve the following minimization problem: \\( \\min_z t \\|E[z_0|z_t] - E(x^*_0)\\|_2^2 \\). Substituting \\( E(x^*_0) = E(A^T y + (\\hat{x}_0 - A^T A \\hat{x}_0)) = E(A^T y + (I - A^T A) \\hat{x}_0) \\), and \\( \\hat{x}_0 = D(E[z_0|z_t]) \\), we can thus interpret the gluing objective in (7) as a one step of gradient descent of this loss \\( \\|E[z_0|z_t] - E(x^*_0)\\|_2^2 \\) with respect to \\( z_t \\). Note that, if there was no latent space, our gluing would be equivalent to a projection on the measurements, but now because of the encoder and decoder, it is not.", "md": "Input: \\(T\\), \\(y\\), \\(\\{\\eta_i\\}_T\\), \\(\\sigma_i\\) for \\(i=1, \\ldots, T\\)\n\n$$\n\\begin{align*}\nz_T &\\sim N(0, I) \\\\\n\\text{for } i &= T - 1 \\text{ to } 0 \\text{ do} \\\\\ns &\\leftarrow s_{\\theta}(z_i, i) \\\\\nz_0 &\\leftarrow \\bar{\\alpha}_i (z_i + (1 - \\bar{\\alpha}_i) z_0) \\\\\n\\epsilon &\\sim N(0, I) \\\\\nz'_{i-1} &\\leftarrow \\sqrt{\\alpha_i(1 - \\bar{\\alpha}_{i-1})} z_i + (1 - \\bar{\\alpha}_i) z_0 + \\tilde{\\sigma}_i \\epsilon \\\\\nz''_{i-1} &\\leftarrow \\frac{z'}{\\| z' - z_0 \\|_2} \\\\\n\\text{end} \\\\\n\\text{return } D(\\hat{z}_0)\n\\end{align*}\n$$\n\nTheoretical Results:\n\nThe resulting from the two terms in (7) both point to the same optima in the latent space. Equivalently, we want to solve the following minimization problem: \\( \\min_z t \\|E[z_0|z_t] - E(x^*_0)\\|_2^2 \\). Substituting \\( E(x^*_0) = E(A^T y + (\\hat{x}_0 - A^T A \\hat{x}_0)) = E(A^T y + (I - A^T A) \\hat{x}_0) \\), and \\( \\hat{x}_0 = D(E[z_0|z_t]) \\), we can thus interpret the gluing objective in (7) as a one step of gradient descent of this loss \\( \\|E[z_0|z_t] - E(x^*_0)\\|_2^2 \\) with respect to \\( z_t \\). Note that, if there was no latent space, our gluing would be equivalent to a projection on the measurements, but now because of the encoder and decoder, it is not."}, {"type": "heading", "lvl": 3, "value": "Theoretical Results", "md": "### Theoretical Results"}, {"type": "text", "value": "As discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse processes, each governed by It\u00f4 SDEs. For implementation purposes, these SDEs are discretized over a finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward process starts from \\( x_0 \\sim p(-x_0) \\) and gradually adds noise, i.e., \\( x_{t+1} = (1 - \\beta_t) x_t + \\sqrt{\\beta_t} \\epsilon \\) where \\( \\beta_t \\in [0, 1] \\) and \\( \\beta_t \\geq \\beta_{t-1} \\) for \\( t = 0, \\ldots, T - 1 \\). The reverse process is initialized with \\( x_T \\sim N(0, I_d) \\) and generates \\( x_{t-1} = \\mu_{\\theta}(x_t, t) + \\sqrt{\\beta_t} \\epsilon \\). In the last step, \\( \\mu_{\\theta}(x_1, 1) \\) is displayed without the noise.\n\nIn this section, we consider the diffusion discretized to two steps (\\( x_0, x_1 \\)), and a Gaussian transition kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures essential components of complex diffusion processes without raising unnecessary complications in the analysis. We provide a principled analysis of Algorithm 1 and Algorithm 2 in a linear model setting with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible in principle. A main result of our work is to prove that in this setting we can solve inverse problems perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory.\n\nIn Section 4, we then show that these algorithmic ideas are much more general, and apply to large-scale real-world applications of diffusion models that use multiple steps (\\( x_0, x_1, \\ldots, x_T = 1000 \\)), and moreover do not satisfy the recoverability assumptions. We provide post-processing details of Algorithm 2 in Appendix C.1. All proofs are given in Appendix B.", "md": "As discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse processes, each governed by It\u00f4 SDEs. For implementation purposes, these SDEs are discretized over a finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward process starts from \\( x_0 \\sim p(-x_0) \\) and gradually adds noise, i.e., \\( x_{t+1} = (1 - \\beta_t) x_t + \\sqrt{\\beta_t} \\epsilon \\) where \\( \\beta_t \\in [0, 1] \\) and \\( \\beta_t \\geq \\beta_{t-1} \\) for \\( t = 0, \\ldots, T - 1 \\). The reverse process is initialized with \\( x_T \\sim N(0, I_d) \\) and generates \\( x_{t-1} = \\mu_{\\theta}(x_t, t) + \\sqrt{\\beta_t} \\epsilon \\). In the last step, \\( \\mu_{\\theta}(x_1, 1) \\) is displayed without the noise.\n\nIn this section, we consider the diffusion discretized to two steps (\\( x_0, x_1 \\)), and a Gaussian transition kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures essential components of complex diffusion processes without raising unnecessary complications in the analysis. We provide a principled analysis of Algorithm 1 and Algorithm 2 in a linear model setting with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible in principle. A main result of our work is to prove that in this setting we can solve inverse problems perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory.\n\nIn Section 4, we then show that these algorithmic ideas are much more general, and apply to large-scale real-world applications of diffusion models that use multiple steps (\\( x_0, x_1, \\ldots, x_T = 1000 \\)), and moreover do not satisfy the recoverability assumptions. We provide post-processing details of Algorithm 2 in Appendix C.1. All proofs are given in Appendix B."}, {"type": "heading", "lvl": 4, "value": "Problem Setup", "md": "#### Problem Setup"}, {"type": "text", "value": "The goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion processes in a linear model setting similar to [42, 7], where the images (\\( x_0 \\) subspace of the form \\( x_0 = S w_0, S \\in \\mathbb{R}^{d \\times l}, w_0 \\in \\mathbb{R}^l \\), and \\( \\sigma_y = 0 \\). Here, \\( S \\) is a tall thin matrix with \\( \\text{rank}(S) = l \\leq d \\) that lifts any latent vector \\( w_0 \\sim N(0, I_l) \\) to the image space with ambient dimension \\( d \\). Given the measurements \\( y = A x_0 + \\sigma_y n, A \\in \\mathbb{R}^{l \\times d}, n \\in \\mathbb{R}^l \\), the goal is to sample from \\( p_0(x_0|y) \\) using a pre-trained latent diffusion model. In the inpainting task, the measurement operator \\( A \\) is such that \\( A^T A \\) is a diagonal matrix \\( D(m) \\), where \\( m \\) is the masking vector with elements set to 1 where data is observed and 0 where data is masked (see Appendix B for further details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder (VAE). Following the common practice [41], we consider a setting where the latent vector of the VAE is \\( k \\)-dimensional and the latent distribution is a standard Gaussian \\( N(0, I_k) \\). Our analysis shows that the proposed Algorithm 2 provably solves inverse problems under the following assumptions.", "md": "The goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion processes in a linear model setting similar to [42, 7], where the images (\\( x_0 \\) subspace of the form \\( x_0 = S w_0, S \\in \\mathbb{R}^{d \\times l}, w_0 \\in \\mathbb{R}^l \\), and \\( \\sigma_y = 0 \\). Here, \\( S \\) is a tall thin matrix with \\( \\text{rank}(S) = l \\leq d \\) that lifts any latent vector \\( w_0 \\sim N(0, I_l) \\) to the image space with ambient dimension \\( d \\). Given the measurements \\( y = A x_0 + \\sigma_y n, A \\in \\mathbb{R}^{l \\times d}, n \\in \\mathbb{R}^l \\), the goal is to sample from \\( p_0(x_0|y) \\) using a pre-trained latent diffusion model. In the inpainting task, the measurement operator \\( A \\) is such that \\( A^T A \\) is a diagonal matrix \\( D(m) \\), where \\( m \\) is the masking vector with elements set to 1 where data is observed and 0 where data is masked (see Appendix B for further details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder (VAE). Following the common practice [41], we consider a setting where the latent vector of the VAE is \\( k \\)-dimensional and the latent distribution is a standard Gaussian \\( N(0, I_k) \\). Our analysis shows that the proposed Algorithm 2 provably solves inverse problems under the following assumptions."}]}, {"page": 6, "text": " Assumption 3.1. The columns of the data generating model S are orthonormal, i.e., ST S = Il.\n Assumption 3.2. The measurement operator A satisfies (AS)T (AS) \u227b                                                    0.\n These assumptions have previously appeared, e.g., [42]. While Assumption 3.1 is mild and can\n be relaxed at the expense of (standard) mathematical complications, Assumption 3.2 indicates that\n(AS)T (AS) is a positive definite matrix. The latter ensures that there is enough energy left in\n the measurements for perfect reconstruction. More precisely, any subset of l coordinates exactly\n                                                                              \u2192\n determines the remaining (d \u2212                     l) coordinates of \u2212        x0. The underlying assumption is that there exists a\n solution and it is unique [42]. Thus, the theoretical question becomes how close the recovered sample\n is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of\n posteriors and prove that the generated samples are close to this posterior in distribution. However,\n this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior\n works [42, 7], we analyze posterior sampling in a two-step diffusion model and answer a fundamental\n question: Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly\n recoverable setting?\n 3.2      Posterior Sampling using Latent Diffusion Model\n In this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7),\n displayed in Algorithm 2. We consider the case where the latent distribution of the VAE is in the\n same space as the latent distribution of the data generating model, i.e., k = l, and normalize \u03b3i = 1\n (as this is immaterial in the linear setting). In Proposition 3.3, we provide analytical solutions for the\n encoder and the decoder of the VAE.\n Proposition 3.3 (Variational Autoencoder). Suppose Assumption 3.1 holds. For an encoder E :\n Rd \u2192       Rk and a decoder D : Rk \u2192                      Rd, denote by L (\u03d5, \u03c9) the training objective of VAE:\n                                                   \u2192          D(E(\u2212      \u2192                    \u2192 2        + \u03bbKL (E\u266fp, N             (0, Ik)) ,\n               arg min                                                  x0; \u03d5); \u03c9) \u2212         \u2212\n                      \u03d5,\u03c9 L (\u03d5, \u03c9) := E\u2212           x0\u223cp                                      x0     2\n then the combination of E(\u2212                \u2192                    \u2192                 \u2212                 \u2212\n Using the encoder E(\u2212              \u2192       x0; \u03d5) = ST \u2212\u2192      x0 and D(\u2190        z0; \u03c9) = S\u2190       z0 is a minimizer of L (\u03d5, \u03c9).\n                                    x0; \u03d5) = ST \u2212        x0, we can use the analytical solution \u03b8\u2217           \u2212 \u2192             of the LDM obtained\n in Theorem A.1. To verify that \u03b8\u2217                           recovers the true subspace p                     x0     , we compose the decoder\n D(\u2190   \u2212                 \u2212                                                                \u2212                   \u2212                    \u2212             \u2212\n      z0; \u03c9) = S\u2190       z0 with the generator of the LDM, i.e., \u2190                        x0 = D         \u03b8\u2217\u2190  z1     = D       Ik\u2190 z1     = S\u2190   z1. Since\n                                                                                                                                                \u2192\n \u2190\u2212                                                                                                          \u2212                                  x0). Thus\n z1 \u223c     N (0, Ik) and S is the data generating model, this shows that \u2190                                   x0 is a sample from p(\u2212\n we have the following.\n Theorem 3.4 (Generative Modeling using Diffusion in Latent Space). Suppose Assumption 3.1\n holds. Let the optimal solution of the latent diffusion model be\n                                                   \u2192   \u2192      \u02dc      \u2212\u2192     \u2192     \u2192       \u2192               \u2212\u2192     \u2212\u2192     \u2192        2    .\n                         \u03b8\u2217   = arg min    \u03b8 E\u2212    z0,\u2212\u03f5        \u00b51    z1(\u2212  z0, \u2212\u03f5 ), \u2212   z0     \u2212   \u00b5\u03b8    z1     z0, \u2212\u03f5\n                                                         \u2212\u2192    \u2212\u2192     \u2192        := \u03b8\u2212   \u2192     \u2212\u2192     \u2192\n For a fixed variance \u03b2 > 0, if \u00b5\u03b8                        z1    z0, \u2212\u03f5      \u221a  1       z1     z0, \u2212\u03f5      , then the closed-form solution is    \u2190 \u2212\n \u03b8\u2217  \u2212= \u221a1 \u2212         \u03b2Ik, which after normalization by      \u2212\u2192                1\u2212\u03b2 and composition with the decoder D                             z0; \u03c9     =\n S\u2190 z0 recovers the true subspace of p                       x0    .\n With this optimal \u03b8\u2217, we can now prove exact sample recovery using GML-DPS (6).\n Theorem 3.5 (Posterior Sampling using Goodness Modified Latent DPS). Let Assumptions 3.1 and\n 3.2 hold. Let \u03c3j, \u2200j = 1, . . . , r, denote the singular values of (AS)T (AS), and let\n                                                   \u2192   \u2192      \u02dc      \u2212\u2192     \u2192     \u2192       \u2192               \u2212\u2192     \u2212\u2192     \u2192        2\n                         \u03b8\u2217   = arg min    \u03b8 E\u2212    z0,\u2212\u03f5        \u00b51    z1(\u2212  z0, \u2212\u03f5 ), \u2212   z0     \u2212   \u00b5\u03b8    z1     z0, \u2212\u03f5              .\n                                                     \u2192             \u2192\n Given a partially known image \u2212                     x0 \u223c      p(\u2212x0), any fixed variance \u03b2 \u2208                    (0, 1), then with the (unique)\n step size \u03b7j    i = 1/2\u03c3j, j = 1, 2, . . . , r, the GML-DPS Algorithm (6) samples from the true posterior\n p(\u2212 \u2192                                                                                          \u2212        \u2192\n    x0|y) and exactly recovers the groundtruth sample, i.e., \u2190                                 x0 = \u2212   x0.\n Theorem 3.5 shows that GML-DPS (6) recovers the true sample using an LDM. This approach,\n however, requires the step size \u03b7 to be chosen coordinate-wise in a specific manner. Also, multiple\n natural images could have the same measurements in the pixel space. This is a reasonable concern for\n                                                                               6", "md": "Assumption 3.1. The columns of the data generating model \\(S\\) are orthonormal, i.e., \\(S^T S = I_l\\).\n\nAssumption 3.2. The measurement operator \\(A\\) satisfies \\((AS)^T (AS) \\succ 0\\).\n\nThese assumptions have previously appeared, e.g., [42]. While Assumption 3.1 is mild and can be relaxed at the expense of (standard) mathematical complications, Assumption 3.2 indicates that \\((AS)^T (AS)\\) is a positive definite matrix. The latter ensures that there is enough energy left in the measurements for perfect reconstruction. More precisely, any subset of \\(l\\) coordinates exactly determines the remaining \\((d - l)\\) coordinates of \\(x_0\\). The underlying assumption is that there exists a solution and it is unique [42]. Thus, the theoretical question becomes how close the recovered sample is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors and prove that the generated samples are close to this posterior in distribution. However, this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works [42, 7], we analyze posterior sampling in a two-step diffusion model and answer a fundamental question: Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?\n\n### Posterior Sampling using Latent Diffusion Model\n\nIn this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7), displayed in Algorithm 2. We consider the case where the latent distribution of the VAE is in the same space as the latent distribution of the data generating model, i.e., \\(k = l\\), and normalize \\(\\gamma_i = 1\\) (as this is immaterial in the linear setting). In Proposition 3.3, we provide analytical solutions for the encoder and the decoder of the VAE.\n\n**Proposition 3.3 (Variational Autoencoder).** Suppose Assumption 3.1 holds. For an encoder \\(E : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k\\) and a decoder \\(D : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d\\), denote by \\(L(\\phi, \\omega)\\) the training objective of VAE:\n\n$$\n\\begin{aligned}\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) &:= E_{x_0 \\sim p_{x_0}} \\left[ \\|x_0 - D(E(x_0; \\phi); \\omega)\\|^2 + \\lambda \\text{KL}(E_{\\#}p, \\mathcal{N}(0, I_k)) \\right],\n\\end{aligned}\n$$\n\nthen the combination of \\(E(x_0; \\phi) = S^T x_0\\) and \\(D(z_0; \\omega) = S z_0\\) is a minimizer of \\(L(\\phi, \\omega)\\).\n\nUsing the encoder \\(E(x_0; \\phi) = S^T x_0\\) and decoder \\(D(z_0; \\omega) = S z_0\\), we can use the analytical solution \\(\\theta^*\\) of the LDM obtained in Theorem A.1. To verify that \\(\\theta^*\\) recovers the true subspace \\(p_{x_0}\\), we compose the decoder \\(D(z_0; \\omega) = S z_1 = D(I_k z_1) = S z_1\\). Since \\(z_1 \\sim \\mathcal{N}(0, I_k)\\) and \\(S\\) is the data generating model, this shows that \\(x_0\\) is a sample from \\(p_{x_0}\\).\n\n**Theorem 3.4 (Generative Modeling using Diffusion in Latent Space).** Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n\n$$\n\\begin{aligned}\n\\theta^* &= \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon) - z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right],\n\\end{aligned}\n$$\n\nFor a fixed variance \\(\\beta > 0\\), if \\(\\mu_{\\theta} z_1 z_0, -\\epsilon = \\sqrt{1 - \\beta} z_1 z_0, -\\epsilon\\), then the closed-form solution is \\(\\theta^* = \\sqrt{1 - \\beta} I_k\\), which after normalization by \\(1 - \\beta\\) and composition with the decoder \\(D(z_0; \\omega) = S z_0\\) recovers the true subspace of \\(p_{x_0}\\).\n\nWith this optimal \\(\\theta^*\\), we can now prove exact sample recovery using GML-DPS (6).\n\n**Theorem 3.5 (Posterior Sampling using Goodness Modified Latent DPS).** Let Assumptions 3.1 and 3.2 hold. Let \\(\\sigma_j, \\forall j = 1, \\ldots, r\\), denote the singular values of \\((AS)^T (AS)\\), and let\n\n$$\n\\begin{aligned}\n\\theta^* &= \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon) - z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right].\n\\end{aligned}\n$$\n\nGiven a partially known image \\(x_0 \\sim p(x_0)\\), any fixed variance \\(\\beta \\in (0, 1)\\), then with the (unique) step size \\(\\eta_j = \\frac{1}{2\\sigma_j}, j = 1, 2, \\ldots, r\\), the GML-DPS Algorithm (6) samples from the true posterior \\(p(x_0|y)\\) and exactly recovers the groundtruth sample, i.e., \\(x_0 = x_0\\).\n\nTheorem 3.5 shows that GML-DPS (6) recovers the true sample using an LDM. This approach, however, requires the step size \\(\\eta\\) to be chosen coordinate-wise in a specific manner. Also, multiple natural images could have the same measurements in the pixel space. This is a reasonable concern for.", "images": [], "items": [{"type": "text", "value": "Assumption 3.1. The columns of the data generating model \\(S\\) are orthonormal, i.e., \\(S^T S = I_l\\).\n\nAssumption 3.2. The measurement operator \\(A\\) satisfies \\((AS)^T (AS) \\succ 0\\).\n\nThese assumptions have previously appeared, e.g., [42]. While Assumption 3.1 is mild and can be relaxed at the expense of (standard) mathematical complications, Assumption 3.2 indicates that \\((AS)^T (AS)\\) is a positive definite matrix. The latter ensures that there is enough energy left in the measurements for perfect reconstruction. More precisely, any subset of \\(l\\) coordinates exactly determines the remaining \\((d - l)\\) coordinates of \\(x_0\\). The underlying assumption is that there exists a solution and it is unique [42]. Thus, the theoretical question becomes how close the recovered sample is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors and prove that the generated samples are close to this posterior in distribution. However, this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works [42, 7], we analyze posterior sampling in a two-step diffusion model and answer a fundamental question: Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?", "md": "Assumption 3.1. The columns of the data generating model \\(S\\) are orthonormal, i.e., \\(S^T S = I_l\\).\n\nAssumption 3.2. The measurement operator \\(A\\) satisfies \\((AS)^T (AS) \\succ 0\\).\n\nThese assumptions have previously appeared, e.g., [42]. While Assumption 3.1 is mild and can be relaxed at the expense of (standard) mathematical complications, Assumption 3.2 indicates that \\((AS)^T (AS)\\) is a positive definite matrix. The latter ensures that there is enough energy left in the measurements for perfect reconstruction. More precisely, any subset of \\(l\\) coordinates exactly determines the remaining \\((d - l)\\) coordinates of \\(x_0\\). The underlying assumption is that there exists a solution and it is unique [42]. Thus, the theoretical question becomes how close the recovered sample is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors and prove that the generated samples are close to this posterior in distribution. However, this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works [42, 7], we analyze posterior sampling in a two-step diffusion model and answer a fundamental question: Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?"}, {"type": "heading", "lvl": 3, "value": "Posterior Sampling using Latent Diffusion Model", "md": "### Posterior Sampling using Latent Diffusion Model"}, {"type": "text", "value": "In this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7), displayed in Algorithm 2. We consider the case where the latent distribution of the VAE is in the same space as the latent distribution of the data generating model, i.e., \\(k = l\\), and normalize \\(\\gamma_i = 1\\) (as this is immaterial in the linear setting). In Proposition 3.3, we provide analytical solutions for the encoder and the decoder of the VAE.\n\n**Proposition 3.3 (Variational Autoencoder).** Suppose Assumption 3.1 holds. For an encoder \\(E : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k\\) and a decoder \\(D : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d\\), denote by \\(L(\\phi, \\omega)\\) the training objective of VAE:\n\n$$\n\\begin{aligned}\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) &:= E_{x_0 \\sim p_{x_0}} \\left[ \\|x_0 - D(E(x_0; \\phi); \\omega)\\|^2 + \\lambda \\text{KL}(E_{\\#}p, \\mathcal{N}(0, I_k)) \\right],\n\\end{aligned}\n$$\n\nthen the combination of \\(E(x_0; \\phi) = S^T x_0\\) and \\(D(z_0; \\omega) = S z_0\\) is a minimizer of \\(L(\\phi, \\omega)\\).\n\nUsing the encoder \\(E(x_0; \\phi) = S^T x_0\\) and decoder \\(D(z_0; \\omega) = S z_0\\), we can use the analytical solution \\(\\theta^*\\) of the LDM obtained in Theorem A.1. To verify that \\(\\theta^*\\) recovers the true subspace \\(p_{x_0}\\), we compose the decoder \\(D(z_0; \\omega) = S z_1 = D(I_k z_1) = S z_1\\). Since \\(z_1 \\sim \\mathcal{N}(0, I_k)\\) and \\(S\\) is the data generating model, this shows that \\(x_0\\) is a sample from \\(p_{x_0}\\).\n\n**Theorem 3.4 (Generative Modeling using Diffusion in Latent Space).** Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n\n$$\n\\begin{aligned}\n\\theta^* &= \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon) - z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right],\n\\end{aligned}\n$$\n\nFor a fixed variance \\(\\beta > 0\\), if \\(\\mu_{\\theta} z_1 z_0, -\\epsilon = \\sqrt{1 - \\beta} z_1 z_0, -\\epsilon\\), then the closed-form solution is \\(\\theta^* = \\sqrt{1 - \\beta} I_k\\), which after normalization by \\(1 - \\beta\\) and composition with the decoder \\(D(z_0; \\omega) = S z_0\\) recovers the true subspace of \\(p_{x_0}\\).\n\nWith this optimal \\(\\theta^*\\), we can now prove exact sample recovery using GML-DPS (6).\n\n**Theorem 3.5 (Posterior Sampling using Goodness Modified Latent DPS).** Let Assumptions 3.1 and 3.2 hold. Let \\(\\sigma_j, \\forall j = 1, \\ldots, r\\), denote the singular values of \\((AS)^T (AS)\\), and let\n\n$$\n\\begin{aligned}\n\\theta^* &= \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon) - z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right].\n\\end{aligned}\n$$\n\nGiven a partially known image \\(x_0 \\sim p(x_0)\\), any fixed variance \\(\\beta \\in (0, 1)\\), then with the (unique) step size \\(\\eta_j = \\frac{1}{2\\sigma_j}, j = 1, 2, \\ldots, r\\), the GML-DPS Algorithm (6) samples from the true posterior \\(p(x_0|y)\\) and exactly recovers the groundtruth sample, i.e., \\(x_0 = x_0\\).\n\nTheorem 3.5 shows that GML-DPS (6) recovers the true sample using an LDM. This approach, however, requires the step size \\(\\eta\\) to be chosen coordinate-wise in a specific manner. Also, multiple natural images could have the same measurements in the pixel space. This is a reasonable concern for.", "md": "In this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7), displayed in Algorithm 2. We consider the case where the latent distribution of the VAE is in the same space as the latent distribution of the data generating model, i.e., \\(k = l\\), and normalize \\(\\gamma_i = 1\\) (as this is immaterial in the linear setting). In Proposition 3.3, we provide analytical solutions for the encoder and the decoder of the VAE.\n\n**Proposition 3.3 (Variational Autoencoder).** Suppose Assumption 3.1 holds. For an encoder \\(E : \\mathbb{R}^d \\rightarrow \\mathbb{R}^k\\) and a decoder \\(D : \\mathbb{R}^k \\rightarrow \\mathbb{R}^d\\), denote by \\(L(\\phi, \\omega)\\) the training objective of VAE:\n\n$$\n\\begin{aligned}\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) &:= E_{x_0 \\sim p_{x_0}} \\left[ \\|x_0 - D(E(x_0; \\phi); \\omega)\\|^2 + \\lambda \\text{KL}(E_{\\#}p, \\mathcal{N}(0, I_k)) \\right],\n\\end{aligned}\n$$\n\nthen the combination of \\(E(x_0; \\phi) = S^T x_0\\) and \\(D(z_0; \\omega) = S z_0\\) is a minimizer of \\(L(\\phi, \\omega)\\).\n\nUsing the encoder \\(E(x_0; \\phi) = S^T x_0\\) and decoder \\(D(z_0; \\omega) = S z_0\\), we can use the analytical solution \\(\\theta^*\\) of the LDM obtained in Theorem A.1. To verify that \\(\\theta^*\\) recovers the true subspace \\(p_{x_0}\\), we compose the decoder \\(D(z_0; \\omega) = S z_1 = D(I_k z_1) = S z_1\\). Since \\(z_1 \\sim \\mathcal{N}(0, I_k)\\) and \\(S\\) is the data generating model, this shows that \\(x_0\\) is a sample from \\(p_{x_0}\\).\n\n**Theorem 3.4 (Generative Modeling using Diffusion in Latent Space).** Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n\n$$\n\\begin{aligned}\n\\theta^* &= \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon) - z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right],\n\\end{aligned}\n$$\n\nFor a fixed variance \\(\\beta > 0\\), if \\(\\mu_{\\theta} z_1 z_0, -\\epsilon = \\sqrt{1 - \\beta} z_1 z_0, -\\epsilon\\), then the closed-form solution is \\(\\theta^* = \\sqrt{1 - \\beta} I_k\\), which after normalization by \\(1 - \\beta\\) and composition with the decoder \\(D(z_0; \\omega) = S z_0\\) recovers the true subspace of \\(p_{x_0}\\).\n\nWith this optimal \\(\\theta^*\\), we can now prove exact sample recovery using GML-DPS (6).\n\n**Theorem 3.5 (Posterior Sampling using Goodness Modified Latent DPS).** Let Assumptions 3.1 and 3.2 hold. Let \\(\\sigma_j, \\forall j = 1, \\ldots, r\\), denote the singular values of \\((AS)^T (AS)\\), and let\n\n$$\n\\begin{aligned}\n\\theta^* &= \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon) - z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right].\n\\end{aligned}\n$$\n\nGiven a partially known image \\(x_0 \\sim p(x_0)\\), any fixed variance \\(\\beta \\in (0, 1)\\), then with the (unique) step size \\(\\eta_j = \\frac{1}{2\\sigma_j}, j = 1, 2, \\ldots, r\\), the GML-DPS Algorithm (6) samples from the true posterior \\(p(x_0|y)\\) and exactly recovers the groundtruth sample, i.e., \\(x_0 = x_0\\).\n\nTheorem 3.5 shows that GML-DPS (6) recovers the true sample using an LDM. This approach, however, requires the step size \\(\\eta\\) to be chosen coordinate-wise in a specific manner. Also, multiple natural images could have the same measurements in the pixel space. This is a reasonable concern for."}]}, {"page": 7, "text": " LDMs due to one-to-many mappings of the decoder. Note that the goodness objective (Section 2.1)                                        \u2212\n cannot help in this scenario because it assigns uniform probability to many of these latents \u2190                                        z1 for\n              \u2212     \u2190\u2212    \u2212                  \u2212    \u2212        2 = 0. These challenges motivate the gluing objective in\n which \u2207\u2190     z1    z0(\u2190 z1)] \u2212     E(D(\u2190   z0(\u2190 z1)))\n Theorem 3.6. This is crucial for two reasons. First, we show that it helps recover the true sample\n even when the step size \u03b7 is chosen arbitrarily. Second, it assigns all the probability mass to the\n desired (unique) solution in the pixel space.\n Theorem 3.6 (Posterior Sampling using Diffusion in Latent Space). Let Assumptions 3.1 and 3.2\n hold. Let \u03c3j, \u2200j = 1, . . . , r denote the singular values of (AS)T (AS) and let\n                                                               \u2212                                \u2212     \u2212\n                                              \u2192   \u2192     \u02dc      \u2192     \u2192    \u2192      \u2192               \u2192     \u2192    \u2192       2\n                       \u03b8\u2217  = arg min   \u03b8 E\u2212   z0,\u2212\u03f5      \u00b51    z1(\u2212  z0, \u2212\u03f5 ), \u2212 z0    \u2212   \u00b5\u03b8    z1    z0, \u2212\u03f5            .\n                                                \u2192           \u2192\n Given a partially known image \u2212                x0 \u223c     p(\u2212x0), any fixed variance \u03b2 \u2208                (0, 1), and any positive step\n                                                                                                                       \u2192\n sizes \u03b7j i , j = 1, 2, . . . , r, the PSLD Algorithm 2 samples from the true posterior p(\u2212                            x 0|y) and exactly\n recovers the groundtruth sample, i.e., \u2190              x0\u2212= \u2212    \u2192\n                                                                x0.\n The important distinction between Theorem 3.5 and Theorem 3.6 is that the former requires the\n exact step size while the latter works for any finite step size. Combining denoising, measurement-\n consistency (with a scalar \u03b7), and gluing updates, we have\n \u2190\u2212           \u2212            \u2212   AD(\u2190     \u2212    \u2212             2         \u2212    \u2190 \u2212    \u2212                   \u2192                                \u2212    \u2212     2\n z0 = \u03b8\u2217\u2190    z1 \u2212    \u03b7\u2207\u2190  z1           z0(\u2190 z1)) \u2212     y    2 \u2212   \u2207\u2190 z1    z0(\u2190 z1) \u2212    E(AT A\u2212     x0 + (Id \u2212        AT A)D(\u2190      z0(\u2190 z1)))   2 .\nWhen \u03b7 is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution\n \u2212\u2192                                                                                                                  \u2212    \u2212          \u2192\n z0. When the reverse SDE generates the exact same groundtruth sample, i.e., D(\u2190                                    z1(\u2190 z0)) = \u2212   x0, then\n the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the\n reverse SDE to recover the true underlying sample irrespective of the value of \u03b7.\nWe draw the following key insights from our Theorem 3.6: Curse of ambient dimension: In order\n to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error\n needs to be computed in the d-dimensional ambient space. Therefore, DPS algorithm suffers from\n the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space,\n and therefore avoids the curse of ambient dimension. Large-scale foundation model: We propose a\n posterior sampling algorithm which offers the provision to use large-scale foundation models, and\n it provably solves general linear inverse problems. Robustness to measurement step: The gluing\n objective makes our algorithm robust to the choice of step size \u03b7. Furthermore, it allows the same\n (scalar) step size across all the coordinates of \u2212                \u2192\n                                                                   x0.\n 4     Experimental Evaluation\n We experiment with in-distribution and out-of-\n distribution datasets. For in-distribution, we con-                    Table 1: Quantitative super-resolution (using mea-\n duct our experiments on a subset of the FFHQ                           surement operator from [32]) results on FFHQ 256\n dataset [25] (downscaled to 256\u00d72563, denoted                          validation samples [25, 11]. We use PSLD with\n by FFHQ 256). For out-of-distribution, we use                          Stable Diffusion. Table shows LPIPS (\u2193).\n images from the web and ImageNet dataset [17]\n (resized to 256\u00d7256, denoted by ImageNet 256).                                   Method         PSLD (Ours)             DPS [11]\n To make a fair comparison, we use the same vali-\n dation subset and follow the same masking strat-                                 2\u00d7             0.185                   0.220\n egy as the baseline DPS [11]. It is important to                                 3\u00d7             0.220                   0.247\n note that our main contribution is an algorithm                                  4\u00d7             0.233                   0.291\n that can leverage any latent diffusion model. We\n test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that\n is trained on multiple subsets of the LAION dataset [44, 45]; and (ii) the Latent Diffusion model\n (LDM-VQ-4) trained on the FFHQ 256 dataset [41]. The DPS model is similarly trained from scratch\n for 1M steps using 49k FFHQ 256 images, which excludes the first 1K images used as validation set.\n Inverse Problems. We experiment with the following task-specific measurement operators from\n the baseline DPS [11]: (i) Box inpainting uses a mask of size 128\u00d7128 at the center. (ii) Random\n inpainting chooses a drop probability uniformly at random between (0.2, 0.8) and applies this drop\n     3https://www.kaggle.com/datasets/denislukovnikov/ffhq256-images-only\n                                                                       7", "md": "LDMs due to one-to-many mappings of the decoder. Note that the goodness objective (Section 2.1) cannot help in this scenario because it assigns uniform probability to many of these latents $z_1$ for $z_2 = 0$. These challenges motivate the gluing objective in which $$\\nabla z_1 z_0(\\leftarrow z_1) - E(D(\\leftarrow z_0(\\leftarrow z_1)))$$\n\nTheorem 3.6. This is crucial for two reasons. First, we show that it helps recover the true sample even when the step size $\\eta$ is chosen arbitrarily. Second, it assigns all the probability mass to the desired (unique) solution in the pixel space.\n\n$$\\text{Theorem 3.6 (Posterior Sampling using Diffusion in Latent Space).}$$\nLet Assumptions 3.1 and 3.2 hold. Let $\\sigma_j, \\forall j = 1, ..., r$ denote the singular values of $(AS)^T(AS)$ and let\n\n$$\\theta^* = \\arg \\min_\\theta E_{z_0,\\epsilon}[\\mu_1 z_1(\\leftarrow z_0, \\epsilon) - z_0 - \\mu_\\theta z_1 z_0, \\epsilon].$$\n\nGiven a partially known image $x_0 \\sim p(x_0)$, any fixed variance $\\beta \\in (0, 1)$, and any positive step sizes $\\eta_j, j = 1, 2, ..., r$, the PSLD Algorithm 2 samples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $\\leftarrow x_0 = \\leftarrow x_0$.\n\nThe important distinction between Theorem 3.5 and Theorem 3.6 is that the former requires the exact step size while the latter works for any finite step size. Combining denoising, measurement-consistency (with a scalar $\\eta$), and gluing updates, we have\n\n$$\\leftarrow z_0 = \\theta^*\\leftarrow z_1 - \\eta\\nabla\\leftarrow z_1 z_0(\\leftarrow z_1) - y - \\nabla z_1 z_0(\\leftarrow z_1) - E(A^TA\\leftarrow x_0 + (Id - A^TA)D(\\leftarrow z_0(\\leftarrow z_1))).$$\n\nWhen $\\eta$ is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution $\\leftarrow z_0$. When the reverse SDE generates the exact same groundtruth sample, i.e., $D(\\leftarrow z_1(\\leftarrow z_0)) = x_0$, then the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the reverse SDE to recover the true underlying sample irrespective of the value of $\\eta$.\n\nWe draw the following key insights from our Theorem 3.6:\n\n1. Curse of ambient dimension: In order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error needs to be computed in the $d$-dimensional ambient space. Therefore, DPS algorithm suffers from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space, and therefore avoids the curse of ambient dimension.\n2. Large-scale foundation model: We propose a posterior sampling algorithm which offers the provision to use large-scale foundation models, and it provably solves general linear inverse problems.\n3. Robustness to measurement step: The gluing objective makes our algorithm robust to the choice of step size $\\eta$. Furthermore, it allows the same (scalar) step size across all the coordinates of $x_0$.\n\n#### Experimental Evaluation\n\nWe experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct our experiments on a subset of the FFHQ dataset (downscaled to 256x256, denoted by FFHQ 256). For out-of-distribution, we use images from the web and ImageNet dataset (resized to 256x256, denoted by ImageNet 256).\n\nTo make a fair comparison, we use the same validation subset and follow the same masking strategy as the baseline DPS. It is important to note that our main contribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple subsets of the LAION dataset; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on the FFHQ 256 dataset. The DPS model is similarly trained from scratch for 1M steps using 49k FFHQ 256 images, which excludes the first 1K images used as validation set.\n\nInverse Problems. We experiment with the following task-specific measurement operators from the baseline DPS: (i) Box inpainting uses a mask of size 128x128 at the center. (ii) Random inpainting chooses a drop probability uniformly at random between (0.2, 0.8) and applies this drop.\n\n**Table 1: Quantitative super-resolution (using measurement operator from [32]) results on FFHQ 256 validation samples. We use PSLD with Stable Diffusion. Table shows LPIPS (\u2193).**\n|Method|PSLD (Ours)|DPS [11]|\n|---|---|---|\n|2x|0.185|0.220|\n|3x|0.220|0.247|\n|4x|0.233|0.291|", "images": [], "items": [{"type": "text", "value": "LDMs due to one-to-many mappings of the decoder. Note that the goodness objective (Section 2.1) cannot help in this scenario because it assigns uniform probability to many of these latents $z_1$ for $z_2 = 0$. These challenges motivate the gluing objective in which $$\\nabla z_1 z_0(\\leftarrow z_1) - E(D(\\leftarrow z_0(\\leftarrow z_1)))$$\n\nTheorem 3.6. This is crucial for two reasons. First, we show that it helps recover the true sample even when the step size $\\eta$ is chosen arbitrarily. Second, it assigns all the probability mass to the desired (unique) solution in the pixel space.\n\n$$\\text{Theorem 3.6 (Posterior Sampling using Diffusion in Latent Space).}$$\nLet Assumptions 3.1 and 3.2 hold. Let $\\sigma_j, \\forall j = 1, ..., r$ denote the singular values of $(AS)^T(AS)$ and let\n\n$$\\theta^* = \\arg \\min_\\theta E_{z_0,\\epsilon}[\\mu_1 z_1(\\leftarrow z_0, \\epsilon) - z_0 - \\mu_\\theta z_1 z_0, \\epsilon].$$\n\nGiven a partially known image $x_0 \\sim p(x_0)$, any fixed variance $\\beta \\in (0, 1)$, and any positive step sizes $\\eta_j, j = 1, 2, ..., r$, the PSLD Algorithm 2 samples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $\\leftarrow x_0 = \\leftarrow x_0$.\n\nThe important distinction between Theorem 3.5 and Theorem 3.6 is that the former requires the exact step size while the latter works for any finite step size. Combining denoising, measurement-consistency (with a scalar $\\eta$), and gluing updates, we have\n\n$$\\leftarrow z_0 = \\theta^*\\leftarrow z_1 - \\eta\\nabla\\leftarrow z_1 z_0(\\leftarrow z_1) - y - \\nabla z_1 z_0(\\leftarrow z_1) - E(A^TA\\leftarrow x_0 + (Id - A^TA)D(\\leftarrow z_0(\\leftarrow z_1))).$$\n\nWhen $\\eta$ is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution $\\leftarrow z_0$. When the reverse SDE generates the exact same groundtruth sample, i.e., $D(\\leftarrow z_1(\\leftarrow z_0)) = x_0$, then the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the reverse SDE to recover the true underlying sample irrespective of the value of $\\eta$.\n\nWe draw the following key insights from our Theorem 3.6:\n\n1. Curse of ambient dimension: In order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error needs to be computed in the $d$-dimensional ambient space. Therefore, DPS algorithm suffers from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space, and therefore avoids the curse of ambient dimension.\n2. Large-scale foundation model: We propose a posterior sampling algorithm which offers the provision to use large-scale foundation models, and it provably solves general linear inverse problems.\n3. Robustness to measurement step: The gluing objective makes our algorithm robust to the choice of step size $\\eta$. Furthermore, it allows the same (scalar) step size across all the coordinates of $x_0$.", "md": "LDMs due to one-to-many mappings of the decoder. Note that the goodness objective (Section 2.1) cannot help in this scenario because it assigns uniform probability to many of these latents $z_1$ for $z_2 = 0$. These challenges motivate the gluing objective in which $$\\nabla z_1 z_0(\\leftarrow z_1) - E(D(\\leftarrow z_0(\\leftarrow z_1)))$$\n\nTheorem 3.6. This is crucial for two reasons. First, we show that it helps recover the true sample even when the step size $\\eta$ is chosen arbitrarily. Second, it assigns all the probability mass to the desired (unique) solution in the pixel space.\n\n$$\\text{Theorem 3.6 (Posterior Sampling using Diffusion in Latent Space).}$$\nLet Assumptions 3.1 and 3.2 hold. Let $\\sigma_j, \\forall j = 1, ..., r$ denote the singular values of $(AS)^T(AS)$ and let\n\n$$\\theta^* = \\arg \\min_\\theta E_{z_0,\\epsilon}[\\mu_1 z_1(\\leftarrow z_0, \\epsilon) - z_0 - \\mu_\\theta z_1 z_0, \\epsilon].$$\n\nGiven a partially known image $x_0 \\sim p(x_0)$, any fixed variance $\\beta \\in (0, 1)$, and any positive step sizes $\\eta_j, j = 1, 2, ..., r$, the PSLD Algorithm 2 samples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $\\leftarrow x_0 = \\leftarrow x_0$.\n\nThe important distinction between Theorem 3.5 and Theorem 3.6 is that the former requires the exact step size while the latter works for any finite step size. Combining denoising, measurement-consistency (with a scalar $\\eta$), and gluing updates, we have\n\n$$\\leftarrow z_0 = \\theta^*\\leftarrow z_1 - \\eta\\nabla\\leftarrow z_1 z_0(\\leftarrow z_1) - y - \\nabla z_1 z_0(\\leftarrow z_1) - E(A^TA\\leftarrow x_0 + (Id - A^TA)D(\\leftarrow z_0(\\leftarrow z_1))).$$\n\nWhen $\\eta$ is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution $\\leftarrow z_0$. When the reverse SDE generates the exact same groundtruth sample, i.e., $D(\\leftarrow z_1(\\leftarrow z_0)) = x_0$, then the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the reverse SDE to recover the true underlying sample irrespective of the value of $\\eta$.\n\nWe draw the following key insights from our Theorem 3.6:\n\n1. Curse of ambient dimension: In order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error needs to be computed in the $d$-dimensional ambient space. Therefore, DPS algorithm suffers from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space, and therefore avoids the curse of ambient dimension.\n2. Large-scale foundation model: We propose a posterior sampling algorithm which offers the provision to use large-scale foundation models, and it provably solves general linear inverse problems.\n3. Robustness to measurement step: The gluing objective makes our algorithm robust to the choice of step size $\\eta$. Furthermore, it allows the same (scalar) step size across all the coordinates of $x_0$."}, {"type": "heading", "lvl": 4, "value": "Experimental Evaluation", "md": "#### Experimental Evaluation"}, {"type": "text", "value": "We experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct our experiments on a subset of the FFHQ dataset (downscaled to 256x256, denoted by FFHQ 256). For out-of-distribution, we use images from the web and ImageNet dataset (resized to 256x256, denoted by ImageNet 256).\n\nTo make a fair comparison, we use the same validation subset and follow the same masking strategy as the baseline DPS. It is important to note that our main contribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple subsets of the LAION dataset; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on the FFHQ 256 dataset. The DPS model is similarly trained from scratch for 1M steps using 49k FFHQ 256 images, which excludes the first 1K images used as validation set.\n\nInverse Problems. We experiment with the following task-specific measurement operators from the baseline DPS: (i) Box inpainting uses a mask of size 128x128 at the center. (ii) Random inpainting chooses a drop probability uniformly at random between (0.2, 0.8) and applies this drop.\n\n**Table 1: Quantitative super-resolution (using measurement operator from [32]) results on FFHQ 256 validation samples. We use PSLD with Stable Diffusion. Table shows LPIPS (\u2193).**", "md": "We experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct our experiments on a subset of the FFHQ dataset (downscaled to 256x256, denoted by FFHQ 256). For out-of-distribution, we use images from the web and ImageNet dataset (resized to 256x256, denoted by ImageNet 256).\n\nTo make a fair comparison, we use the same validation subset and follow the same masking strategy as the baseline DPS. It is important to note that our main contribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple subsets of the LAION dataset; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on the FFHQ 256 dataset. The DPS model is similarly trained from scratch for 1M steps using 49k FFHQ 256 images, which excludes the first 1K images used as validation set.\n\nInverse Problems. We experiment with the following task-specific measurement operators from the baseline DPS: (i) Box inpainting uses a mask of size 128x128 at the center. (ii) Random inpainting chooses a drop probability uniformly at random between (0.2, 0.8) and applies this drop.\n\n**Table 1: Quantitative super-resolution (using measurement operator from [32]) results on FFHQ 256 validation samples. We use PSLD with Stable Diffusion. Table shows LPIPS (\u2193).**"}, {"type": "table", "rows": [["Method", "PSLD (Ours)", "DPS [11]"], ["2x", "0.185", "0.220"], ["3x", "0.220", "0.247"], ["4x", "0.233", "0.291"]], "md": "|Method|PSLD (Ours)|DPS [11]|\n|---|---|---|\n|2x|0.185|0.220|\n|3x|0.220|0.247|\n|4x|0.233|0.291|", "isPerfectTable": true, "csv": "\"Method\",\"PSLD (Ours)\",\"DPS [11]\"\n\"2x\",\"0.185\",\"0.220\"\n\"3x\",\"0.220\",\"0.247\"\n\"4x\",\"0.233\",\"0.291\""}]}, {"page": 8, "text": " Table 2: Quantitative inpainting results on FFHQ 256 validation set [25, 11]. We use Stable Diffusion\n v-1.5 and the measurement operators as in DPS [11]. As shown, our PSLD model outperforms DPS\n since it is able to leverage the power of the Stable Diffusion foundation model.\n                        Inpaint (random)         Inpaint (box)             SR (4\u00d7)            Gaussian Deblur\n   Method             FID (\u2193)   LPIPS (\u2193)    FID (\u2193)    LPIPS (\u2193)    FID (\u2193)   LPIPS (\u2193)    FID (\u2193)    LPIPS (\u2193)\n   PSLD (Ours)        21.34     0.096        43.11      0.167        34.28     0.201        41.53      0.221\n   DPS [11]           33.48     0.212        35.14      0.216        39.35     0.214        44.05      0.257\n   DDRM [26]          69.71     0.587        42.93      0.204        62.15     0.294        74.92      0.332\n   MCG [13]           29.26     0.286        40.11      0.309        87.64     0.520        101.2      0.340\n   PnP-ADMM [6]       123.6     0.692        151.9      0.406        66.52     0.353        90.42      0.441\n   Score-SDE [50]     76.54     0.612        60.06      0.331        96.72     0.563        109.0      0.403\n   ADMM-TV            181.5     0.463        68.94      0.322        110.6     0.428        186.7      0.507\n probability to all the pixels. (iii) Super-resolution downsamples images at 4\u00d7 scale. (iv) Gaussian blur\n convolves images with a Gaussian blur kernel. (v) Motion blur convolves images with a motion blur\n kernel. We also experiment with these additional operators from RePaint [32]: (vi) Super-resolution\n downsamples images at 2\u00d7, 3\u00d7, and 4\u00d7 scale. (vii) Denoising has Gaussian noise with \u03c3 = 0.05.\n (viii) Destriping has vertical and horizontal stripes in the input images.\n Evaluation. We compare the performance of our PSLD algorithm with the state-of-the-art DPS\n algorithm [11] on random inpainting, box inpainting, denoising, Gaussian deblur, motion deblur,\n arbitrary masking, and super-resolution tasks. We show that PSLD outperforms DPS, both in-\n distribution and out-of-distribution datasets, using the Stable Diffusion v-1.5 model pre-trained on the\n LAION dataset. We also test PSLD with LDM-VQ-4 trained on FFHQ 256, to compare with DPS\n trained on the same data distribution. Note that the LDM-v4 is a latent-based model released prior to\n Stable Diffusion. Therefore, it does not match the performance of Stable Diffusion in solving inverse\n problems. However, it shows the general applicability of our framework to leverage an LDM in\n posterior sampling. Since Stable Diffusion v-1.5 is trained with an image resolution of 512 \u00d7 512, we\n apply the forward operator after upsampling inputs to 512\u00d7512, run posterior sampling at 512\u00d7512,\n and then downsample images to the original 256 \u00d7 256 resolution for a fair comparison with DPS.\nWe observed a similar performance while applying the masking operator at 256 \u00d7 256 and upscaling\n to 512 \u00d7 512 before running PSLD. More implementation details are provided in Appendix C.1.\n Metrics. We use the commonly used Learned Perceptual Image Patch Similarity (LPIPS), Peak\n Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM), and Fr\u00e9chet Inception\n Distance4 (FID) metrics for quantitative evaluation.\n Results. Figure 2 shows the inpainting results on out-of-distribution samples. This experiment was\n performed on commercial platforms that use (to the best of our knowledge) Stable diffusion and\n additional proprietary models. This evaluation was performed on models deployed in May 2023 and\n may change as commercial providers improve their platforms.\n The qualitative advantage of PSLD is clearly demonstrated in Figures 2, 3, 4, 15 and 16. In Figure 5,\nwe compare PSLD and DPS in random inpainting task for varying percentage of dropped pixels.\n Quantitatively, PSLD outperforms DPS in commonly used metrics: LPIPS, PSNR, and SSIM.\n In our PSLD algorithm, we use Stable Diffusion v1.5 model and (zero-shot) test it on inverse problems.\n Table 6 compares the quantitative results of PSLD with related works on random inpainting, box\n inpainting, super-resolution, and Gaussian deblur tasks. PSLD significantly outperforms previous\n approaches on the relatively easier random inpainting task, and it is better or comparable on harder\n tasks. Table 4 draws a comparison between PSLD and the strongest baseline (among the compared\n methods) on out-of-distribution images. Table 1 shows the super-resolution results using nearest-\n neighbor kernels from [32] on FFHQ 256 validation dataset. Observe that PSLD outperforms\n state-of-the-art methods across diverse tasks and standard evaluation metrics.\n In Table 3, we compare PSLD (using LDM-VQ-4) and DPS on random and box inpainting tasks\nwith the same operating resolution (256 \u00d7 256) and training distributions (FFHQ 256). Although\n the LDM model exceeds DPS performance in box inpainting, it is comparable in random inpainting.\nAs expected, using a more powerful pre-trained model such as Stable Diffusion is beneficial in\n    4https://github.com/mseitzer/pytorch-fid\n                                                           8", "md": "|Method|Inpaint (random)|Inpaint (box)|SR (4\u00d7)|Gaussian Deblur|\n|---|---|---|---|---|\n|PSLD (Ours)|21.34|0.096|43.11|0.167|34.28|0.201|41.53|0.221|\n|DPS [11]|33.48|0.212|35.14|0.216|39.35|0.214|44.05|0.257|\n|DDRM [26]|69.71|0.587|42.93|0.204|62.15|0.294|74.92|0.332|\n|MCG [13]|29.26|0.286|40.11|0.309|87.64|0.520|101.2|0.340|\n|PnP-ADMM [6]|123.6|0.692|151.9|0.406|66.52|0.353|90.42|0.441|\n|Score-SDE [50]|76.54|0.612|60.06|0.331|96.72|0.563|109.0|0.403|\n|ADMM-TV|181.5|0.463|68.94|0.322|110.6|0.428|186.7|0.507|", "images": [], "items": [{"type": "table", "rows": [["Method", "Inpaint (random)", "Inpaint (box)", "SR (4\u00d7)", "Gaussian Deblur"], ["PSLD (Ours)", "21.34", "0.096", "43.11", "0.167", "34.28", "0.201", "41.53", "0.221"], ["DPS [11]", "33.48", "0.212", "35.14", "0.216", "39.35", "0.214", "44.05", "0.257"], ["DDRM [26]", "69.71", "0.587", "42.93", "0.204", "62.15", "0.294", "74.92", "0.332"], ["MCG [13]", "29.26", "0.286", "40.11", "0.309", "87.64", "0.520", "101.2", "0.340"], ["PnP-ADMM [6]", "123.6", "0.692", "151.9", "0.406", "66.52", "0.353", "90.42", "0.441"], ["Score-SDE [50]", "76.54", "0.612", "60.06", "0.331", "96.72", "0.563", "109.0", "0.403"], ["ADMM-TV", "181.5", "0.463", "68.94", "0.322", "110.6", "0.428", "186.7", "0.507"]], "md": "|Method|Inpaint (random)|Inpaint (box)|SR (4\u00d7)|Gaussian Deblur|\n|---|---|---|---|---|\n|PSLD (Ours)|21.34|0.096|43.11|0.167|34.28|0.201|41.53|0.221|\n|DPS [11]|33.48|0.212|35.14|0.216|39.35|0.214|44.05|0.257|\n|DDRM [26]|69.71|0.587|42.93|0.204|62.15|0.294|74.92|0.332|\n|MCG [13]|29.26|0.286|40.11|0.309|87.64|0.520|101.2|0.340|\n|PnP-ADMM [6]|123.6|0.692|151.9|0.406|66.52|0.353|90.42|0.441|\n|Score-SDE [50]|76.54|0.612|60.06|0.331|96.72|0.563|109.0|0.403|\n|ADMM-TV|181.5|0.463|68.94|0.322|110.6|0.428|186.7|0.507|", "isPerfectTable": false, "csv": "\"Method\",\"Inpaint (random)\",\"Inpaint (box)\",\"SR (4\u00d7)\",\"Gaussian Deblur\"\n\"PSLD (Ours)\",\"21.34\",\"0.096\",\"43.11\",\"0.167\",\"34.28\",\"0.201\",\"41.53\",\"0.221\"\n\"DPS [11]\",\"33.48\",\"0.212\",\"35.14\",\"0.216\",\"39.35\",\"0.214\",\"44.05\",\"0.257\"\n\"DDRM [26]\",\"69.71\",\"0.587\",\"42.93\",\"0.204\",\"62.15\",\"0.294\",\"74.92\",\"0.332\"\n\"MCG [13]\",\"29.26\",\"0.286\",\"40.11\",\"0.309\",\"87.64\",\"0.520\",\"101.2\",\"0.340\"\n\"PnP-ADMM [6]\",\"123.6\",\"0.692\",\"151.9\",\"0.406\",\"66.52\",\"0.353\",\"90.42\",\"0.441\"\n\"Score-SDE [50]\",\"76.54\",\"0.612\",\"60.06\",\"0.331\",\"96.72\",\"0.563\",\"109.0\",\"0.403\"\n\"ADMM-TV\",\"181.5\",\"0.463\",\"68.94\",\"0.322\",\"110.6\",\"0.428\",\"186.7\",\"0.507\""}]}, {"page": 9, "text": "       (a) Input          (b) Groundtruth      (c) Comm. Serv. 1      (d) Comm. Serv. 2      (e) PSLD (Ours)\nFigure 2: Inpainting results in general domain images from the web (see Appendix C for image\nsources). Our model compared to state-of-art commercial inpainting services that leverage the same\nfoundation model (Stable Diffusion v-1.5).\nTable 3: Quantitative inpainting results on FFHQ 256 validation set [25, 11]. We use the latent\ndiffusion (LDM-VQ-4) trained on FFHQ 256. Note that in this experiment PSLD and DPS use\ndiffusion models trained on the same dataset. As shown, PSLD with LDM-VQ-4 as diffusion model\noutperforms DPS in box inpainting and has comparable performance in random inpainting.\n                                    Inpaint (random)                           Inpaint (box)\n         Method           PSNR (\u2191)      SSIM (\u2191)     LPIPS (\u2193)     PSNR (\u2191)      SSIM (\u2191)     LPIPS (\u2193)\n         PSLD (Ours)      30.31         0.851        0.221         24.22         0.819        0.158\n         DPS [11]         29.49         0.844        0.212         23.39         0.798        0.214\nTable 4: Quantitative results of random inpainting and denoising on FFHQ 256 [25, 11] using Stable\nDiffusion v-1.5. Note that DPS is trained on FFHQ 256. The results show that our method PSLD\ngeneralizes well to out-of-distribution samples even without finetuning.\n                          Random inpaint + denoise \u03c3 = 0.00        Random inpaint + denoise \u03c3 = 0.05\n        Method            PSNR (\u2191)     SSIM (\u2191)      LPIPS (\u2193)     PSNR (\u2191)      SSIM (\u2191)     LPIPS (\u2193)\n        PSLD (Ours)       34.02         0.951        0.083         33.71         0.943        0.096\n        DPS [11]          31.41        0.884         0.171         29.49         0.844        0.212\nreconstruction\u2013see Table 6. This highlights the significance of our PSLD algorithm that has the\nprovision to incorporate a powerful foundation model with no extra training costs for solving inverse\nproblems. Importantly, PSLD uses latent-based diffusion, and thus it avoids the curse of ambient\ndimension (Theorem 3.6), while still achieving comparable results to the state-of-the-art method\nDPS [11] that has been trained on the same dataset. Additional experimental evaluation is provided\nin Appendix C.\n5    Conclusion\nIn this paper, we leverage latent diffusion models to solve general linear inverse problems. While\npreviously proposed approaches only apply to pixel-space diffusion models, our algorithm allows\nus to use the image prior learned by latent-based foundation generative models. We provide a\nprincipled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this\nanalysis to design a modified objective (goodness and gluing). This leads to our algorithm \u2013 Posterior\n                                                        9", "md": "# Inpainting Results\n\n## Figure 2: Inpainting results in general domain images from the web\n\n(see Appendix C for image sources). Our model compared to state-of-art commercial inpainting services that leverage the same foundation model (Stable Diffusion v-1.5).\n\n### Table 3: Quantitative inpainting results on FFHQ 256 validation set\n\nWe use the latent diffusion (LDM-VQ-4) trained on FFHQ 256. Note that in this experiment PSLD and DPS use diffusion models trained on the same dataset. As shown, PSLD with LDM-VQ-4 as diffusion model outperforms DPS in box inpainting and has comparable performance in random inpainting.\n\n|Method|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|---|---|---|---|---|---|---|\n|PSLD (Ours)|30.31|0.851|0.221|24.22|0.819|0.158|\n|DPS [11]|29.49|0.844|0.212|23.39|0.798|0.214|\n\n### Table 4: Quantitative results of random inpainting and denoising on FFHQ 256\n\nusing Stable Diffusion v-1.5. Note that DPS is trained on FFHQ 256. The results show that our method PSLD generalizes well to out-of-distribution samples even without finetuning.\n\n|Method|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|---|---|---|---|---|---|---|\n|PSLD (Ours)|34.02|0.951|0.083|33.71|0.943|0.096|\n|DPS [11]|31.41|0.884|0.171|29.49|0.844|0.212|\n\nreconstruction\u2013see Table 6. This highlights the significance of our PSLD algorithm that has the provision to incorporate a powerful foundation model with no extra training costs for solving inverse problems. Importantly, PSLD uses latent-based diffusion, and thus it avoids the curse of ambient dimension (Theorem 3.6), while still achieving comparable results to the state-of-the-art method DPS [11] that has been trained on the same dataset. Additional experimental evaluation is provided in Appendix C.\n\n### Conclusion\n\nIn this paper, we leverage latent diffusion models to solve general linear inverse problems. While previously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to use the image prior learned by latent-based foundation generative models. We provide a principled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to design a modified objective (goodness and gluing). This leads to our algorithm \u2013 Posterior.", "images": [{"name": "page-9-0.jpg", "height": 76, "width": 76, "x": 112, "y": 71}, {"name": "page-9-2.jpg", "height": 76, "width": 76, "x": 268, "y": 72}, {"name": "page-9-4.jpg", "height": 76, "width": 76, "x": 423, "y": 72}, {"name": "page-9-1.jpg", "height": 76, "width": 76, "x": 190, "y": 71}, {"name": "page-9-3.jpg", "height": 76, "width": 76, "x": 346, "y": 72}, {"name": "page-9-5.jpg", "height": 76, "width": 76, "x": 112, "y": 148}, {"name": "page-9-6.jpg", "height": 76, "width": 76, "x": 190, "y": 148}, {"name": "page-9-9.jpg", "height": 76, "width": 76, "x": 423, "y": 148}, {"name": "page-9-8.jpg", "height": 76, "width": 76, "x": 346, "y": 148}, {"name": "page-9-7.jpg", "height": 76, "width": 76, "x": 268, "y": 148}, {"name": "page-9-10.jpg", "height": 76, "width": 76, "x": 112, "y": 224}, {"name": "page-9-11.jpg", "height": 76, "width": 76, "x": 190, "y": 224}, {"name": "page-9-13.jpg", "height": 76, "width": 76, "x": 346, "y": 224}, {"name": "page-9-12.jpg", "height": 76, "width": 76, "x": 268, "y": 224}, {"name": "page-9-14.jpg", "height": 76, "width": 76, "x": 423, "y": 224}], "items": [{"type": "heading", "lvl": 1, "value": "Inpainting Results", "md": "# Inpainting Results"}, {"type": "heading", "lvl": 2, "value": "Figure 2: Inpainting results in general domain images from the web", "md": "## Figure 2: Inpainting results in general domain images from the web"}, {"type": "text", "value": "(see Appendix C for image sources). Our model compared to state-of-art commercial inpainting services that leverage the same foundation model (Stable Diffusion v-1.5).", "md": "(see Appendix C for image sources). Our model compared to state-of-art commercial inpainting services that leverage the same foundation model (Stable Diffusion v-1.5)."}, {"type": "heading", "lvl": 3, "value": "Table 3: Quantitative inpainting results on FFHQ 256 validation set", "md": "### Table 3: Quantitative inpainting results on FFHQ 256 validation set"}, {"type": "text", "value": "We use the latent diffusion (LDM-VQ-4) trained on FFHQ 256. Note that in this experiment PSLD and DPS use diffusion models trained on the same dataset. As shown, PSLD with LDM-VQ-4 as diffusion model outperforms DPS in box inpainting and has comparable performance in random inpainting.", "md": "We use the latent diffusion (LDM-VQ-4) trained on FFHQ 256. Note that in this experiment PSLD and DPS use diffusion models trained on the same dataset. As shown, PSLD with LDM-VQ-4 as diffusion model outperforms DPS in box inpainting and has comparable performance in random inpainting."}, {"type": "table", "rows": [["Method", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)"], ["PSLD (Ours)", "30.31", "0.851", "0.221", "24.22", "0.819", "0.158"], ["DPS [11]", "29.49", "0.844", "0.212", "23.39", "0.798", "0.214"]], "md": "|Method|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|---|---|---|---|---|---|---|\n|PSLD (Ours)|30.31|0.851|0.221|24.22|0.819|0.158|\n|DPS [11]|29.49|0.844|0.212|23.39|0.798|0.214|", "isPerfectTable": true, "csv": "\"Method\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\"\n\"PSLD (Ours)\",\"30.31\",\"0.851\",\"0.221\",\"24.22\",\"0.819\",\"0.158\"\n\"DPS [11]\",\"29.49\",\"0.844\",\"0.212\",\"23.39\",\"0.798\",\"0.214\""}, {"type": "heading", "lvl": 3, "value": "Table 4: Quantitative results of random inpainting and denoising on FFHQ 256", "md": "### Table 4: Quantitative results of random inpainting and denoising on FFHQ 256"}, {"type": "text", "value": "using Stable Diffusion v-1.5. Note that DPS is trained on FFHQ 256. The results show that our method PSLD generalizes well to out-of-distribution samples even without finetuning.", "md": "using Stable Diffusion v-1.5. Note that DPS is trained on FFHQ 256. The results show that our method PSLD generalizes well to out-of-distribution samples even without finetuning."}, {"type": "table", "rows": [["Method", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)"], ["PSLD (Ours)", "34.02", "0.951", "0.083", "33.71", "0.943", "0.096"], ["DPS [11]", "31.41", "0.884", "0.171", "29.49", "0.844", "0.212"]], "md": "|Method|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|---|---|---|---|---|---|---|\n|PSLD (Ours)|34.02|0.951|0.083|33.71|0.943|0.096|\n|DPS [11]|31.41|0.884|0.171|29.49|0.844|0.212|", "isPerfectTable": true, "csv": "\"Method\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\"\n\"PSLD (Ours)\",\"34.02\",\"0.951\",\"0.083\",\"33.71\",\"0.943\",\"0.096\"\n\"DPS [11]\",\"31.41\",\"0.884\",\"0.171\",\"29.49\",\"0.844\",\"0.212\""}, {"type": "text", "value": "reconstruction\u2013see Table 6. This highlights the significance of our PSLD algorithm that has the provision to incorporate a powerful foundation model with no extra training costs for solving inverse problems. Importantly, PSLD uses latent-based diffusion, and thus it avoids the curse of ambient dimension (Theorem 3.6), while still achieving comparable results to the state-of-the-art method DPS [11] that has been trained on the same dataset. Additional experimental evaluation is provided in Appendix C.", "md": "reconstruction\u2013see Table 6. This highlights the significance of our PSLD algorithm that has the provision to incorporate a powerful foundation model with no extra training costs for solving inverse problems. Importantly, PSLD uses latent-based diffusion, and thus it avoids the curse of ambient dimension (Theorem 3.6), while still achieving comparable results to the state-of-the-art method DPS [11] that has been trained on the same dataset. Additional experimental evaluation is provided in Appendix C."}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "In this paper, we leverage latent diffusion models to solve general linear inverse problems. While previously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to use the image prior learned by latent-based foundation generative models. We provide a principled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to design a modified objective (goodness and gluing). This leads to our algorithm \u2013 Posterior.", "md": "In this paper, we leverage latent diffusion models to solve general linear inverse problems. While previously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to use the image prior learned by latent-based foundation generative models. We provide a principled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to design a modified objective (goodness and gluing). This leads to our algorithm \u2013 Posterior."}]}, {"page": 10, "text": "Figure 3: Left panel: Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion\nv-1.5. Notice the text in the top row and the facial expression in the bottom row. Right panel: Block (128\u00d7128)\ninpainting, using the LDM-VQ-4 model trained on FFHQ 256 [25]. Notice the glasses in the top row and eyes\nin the bottom row.\n          (a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\nFigure 4: Inpainting (random and box) results on out-of-distribution samples, 256 \u00d7 256 (see\nAppendix C for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation\nmodel.\n    0.20       PSLD                    '                                     '\n    0.15                                           FSL                        0.75        FSLD\n         Percentage of dropped pxels         Percentage    diopre                        Miane   'dropped\nFigure 5: Comparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as\nthe percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.\nSampling with Latent Diffusion (PSLD) \u2013 that experimentally outperforms state-of-art baselines on\na wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and\nsuper-resolution.\nLimitations. Our evaluation is based on Stable Diffusion which was trained on the LAION dataset.\nBiases in this dataset and foundation model will be implicitly affecting our algorithm. Our method\ncan work with any LDM and we expect new foundation models trained on better datasets like [19] to\nmitigate these issues. Second, we have not explored how to use latent-based foundation models to\nsolve non-linear inverse problems. Our method builds on the DPS approximation (which performs\nwell on non-linear inverse problems), and hence we believe our method can also be similarly extended.\n                                                        10", "md": "Left panel: Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion v-1.5. Notice the text in the top row and the facial expression in the bottom row.\n\nRight panel: Block (128\u00d7128) inpainting, using the LDM-VQ-4 model trained on FFHQ 256 [25]. Notice the glasses in the top row and eyes in the bottom row.\n\n(a) Input\n(b) Groundtruth\n(c) DPS [11]\n(d) PSLD (Ours)\n\nInpainting (random and box) results on out-of-distribution samples, 256 \u00d7 256 (see Appendix C for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model.\n\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n0.20 & PSLD & ' & ' \\\\\n0.15 & FSL & 0.75 & FSLD \\\\\n\\text{Percentage of dropped pixels} & \\text{Percentage dropped} & \\text{Maine} & \\text{dropped} \\\\\n\\hline\n\\end{array}\n$$\n\nComparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as the percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.\n\nSampling with Latent Diffusion (PSLD) \u2013 that experimentally outperforms state-of-art baselines on a wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.\n\nLimitations. Our evaluation is based on Stable Diffusion which was trained on the LAION dataset. Biases in this dataset and foundation model will be implicitly affecting our algorithm. Our method can work with any LDM and we expect new foundation models trained on better datasets like [19] to mitigate these issues. Second, we have not explored how to use latent-based foundation models to solve non-linear inverse problems. Our method builds on the DPS approximation (which performs well on non-linear inverse problems), and hence we believe our method can also be similarly extended.\n\n10", "images": [{"name": "page-10-0.jpg", "height": 107, "width": 400, "x": 108, "y": 71}, {"name": "page-10-2.jpg", "height": 96, "width": 96, "x": 209, "y": 239}, {"name": "page-10-5.jpg", "height": 96, "width": 96, "x": 112, "y": 335}, {"name": "page-10-1.jpg", "height": 96, "width": 96, "x": 112, "y": 239}, {"name": "page-10-6.jpg", "height": 96, "width": 96, "x": 209, "y": 335}, {"name": "page-10-3.jpg", "height": 96, "width": 96, "x": 307, "y": 239}, {"name": "page-10-7.jpg", "height": 96, "width": 96, "x": 307, "y": 335}, {"name": "page-10-4.jpg", "height": 96, "width": 96, "x": 404, "y": 239}, {"name": "page-10-8.jpg", "height": 96, "width": 96, "x": 404, "y": 335}, {"name": "page-10-9.jpg", "height": 82, "width": 127, "x": 113, "y": 500}, {"name": "page-10-10.jpg", "height": 80, "width": 127, "x": 242, "y": 502}, {"name": "page-10-11.jpg", "height": 79, "width": 127, "x": 371, "y": 502}], "items": [{"type": "text", "value": "Left panel: Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion v-1.5. Notice the text in the top row and the facial expression in the bottom row.\n\nRight panel: Block (128\u00d7128) inpainting, using the LDM-VQ-4 model trained on FFHQ 256 [25]. Notice the glasses in the top row and eyes in the bottom row.\n\n(a) Input\n(b) Groundtruth\n(c) DPS [11]\n(d) PSLD (Ours)\n\nInpainting (random and box) results on out-of-distribution samples, 256 \u00d7 256 (see Appendix C for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model.\n\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n0.20 & PSLD & ' & ' \\\\\n0.15 & FSL & 0.75 & FSLD \\\\\n\\text{Percentage of dropped pixels} & \\text{Percentage dropped} & \\text{Maine} & \\text{dropped} \\\\\n\\hline\n\\end{array}\n$$\n\nComparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as the percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.\n\nSampling with Latent Diffusion (PSLD) \u2013 that experimentally outperforms state-of-art baselines on a wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.\n\nLimitations. Our evaluation is based on Stable Diffusion which was trained on the LAION dataset. Biases in this dataset and foundation model will be implicitly affecting our algorithm. Our method can work with any LDM and we expect new foundation models trained on better datasets like [19] to mitigate these issues. Second, we have not explored how to use latent-based foundation models to solve non-linear inverse problems. Our method builds on the DPS approximation (which performs well on non-linear inverse problems), and hence we believe our method can also be similarly extended.\n\n10", "md": "Left panel: Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion v-1.5. Notice the text in the top row and the facial expression in the bottom row.\n\nRight panel: Block (128\u00d7128) inpainting, using the LDM-VQ-4 model trained on FFHQ 256 [25]. Notice the glasses in the top row and eyes in the bottom row.\n\n(a) Input\n(b) Groundtruth\n(c) DPS [11]\n(d) PSLD (Ours)\n\nInpainting (random and box) results on out-of-distribution samples, 256 \u00d7 256 (see Appendix C for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model.\n\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n0.20 & PSLD & ' & ' \\\\\n0.15 & FSL & 0.75 & FSLD \\\\\n\\text{Percentage of dropped pixels} & \\text{Percentage dropped} & \\text{Maine} & \\text{dropped} \\\\\n\\hline\n\\end{array}\n$$\n\nComparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as the percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.\n\nSampling with Latent Diffusion (PSLD) \u2013 that experimentally outperforms state-of-art baselines on a wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.\n\nLimitations. Our evaluation is based on Stable Diffusion which was trained on the LAION dataset. Biases in this dataset and foundation model will be implicitly affecting our algorithm. Our method can work with any LDM and we expect new foundation models trained on better datasets like [19] to mitigate these issues. Second, we have not explored how to use latent-based foundation models to solve non-linear inverse problems. Our method builds on the DPS approximation (which performs well on non-linear inverse problems), and hence we believe our method can also be similarly extended.\n\n10"}]}, {"page": 11, "text": "Acknowledgements\nThis research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141,\nTripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western\nDigital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program,\nUT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in\nEngineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential\nFellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship\nID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the\nHuggingFace team for providing us GPU support for the demo of our work.\nReferences\n  [1]  Brian D.O. Anderson. \u201cReverse-time diffusion equation models\u201d. In: Stochastic Processes and\n       their Applications 12.3 (1982), pp. 313\u2013326 (page 1).\n  [2]  Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir.\n       \u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\u201d. In: International\n       Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 2).\n  [3]  Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah\n       Goldblum, Jonas Geiping, and Tom Goldstein. \u201cCold Diffusion: Inverting arbitrary image\n       transforms without noise\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 2).\n  [4]  Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\n       Fidler, and Karsten Kreis. \u201cAlign your latents: High-resolution video synthesis with latent\n       diffusion models\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\n       Pattern Recognition. 2023, pp. 22563\u201322575 (page 3).\n  [5]  Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \u201cCompressed sensing using\n       generative models\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\u2013\n       546 (page 1).\n  [6]  Stanley H Chan, Xiran Wang, and Omar A Elgendy. \u201cPlug-and-play ADMM for image restora-\n       tion: Fixed-point convergence and applications\u201d. In: IEEE Transactions on Computational\n       Imaging 3.1 (2016), pp. 84\u201398 (pages 2, 8, 24, 27).\n  [7]  Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \u201cScore Approximation, Esti-\n       mation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\u201d. In: arXiv\n       preprint arXiv:2302.07194 (2023) (pages 5, 6).\n  [8]  Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \u201cSampling is\n       as easy as learning the score: theory for diffusion models with minimal data assumptions\u201d. In:\n       arXiv preprint arXiv:2209.11215 (2022) (page 1).\n  [9]  Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \u201cRestoration-Degradation Beyond\n       Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\u201d. In: arXiv preprint\n       arXiv:2303.03384 (2023) (page 1).\n[10]   Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon.\n       \u201cIlvr: Conditioning method for denoising diffusion probabilistic models\u201d. In: arXiv preprint\n       arXiv:2108.02938 (2021) (page 2).\n[11]   Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong\n       Chul Ye. \u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\u201d. In: The Eleventh\n       International Conference on Learning Representations. 2023. URL: https://openreview.\n       net/forum?id=OnD9zGAGT0k (pages 1\u20133, 7\u201310, 15, 18, 20, 22, 24\u201331).\n[12]   Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \u201cDirect Diffusion Bridge using Data\n       Consistency for Inverse Problems\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 2).\n[13]   Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \u201cImproving Diffusion\n       Models for Inverse Problems using Manifold Constraints\u201d. In: Advances in Neural Information\n       Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\n       Cho. 2022. URL: https://openreview.net/forum?id=nJJjv0JDJju (pages 2, 8, 24, 27,\n       31).\n[14]   Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \u201cScore-\n       guided intermediate layer optimization: Fast langevin mixing for inverse problem\u201d. In: arXiv\n       preprint arXiv:2206.09104 (2022) (page 2).\n                                                  11", "md": "# Acknowledgements and References\n\n# Acknowledgements\n\nThis research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.\n\n## References\n\n1. Brian D.O. Anderson. \u201cReverse-time diffusion equation models\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\u2013326 (page 1).\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 2).\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \u201cCold Diffusion: Inverting arbitrary image transforms without noise\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 2).\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \u201cAlign your latents: High-resolution video synthesis with latent diffusion models\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\u201322575 (page 3).\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \u201cCompressed sensing using generative models\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\u2013546 (page 1).\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\u201398 (pages 2, 8, 24, 27).\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (pages 5, 6).\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 1).\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 1).\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \u201cIlvr: Conditioning method for denoising diffusion probabilistic models\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 2).\n11. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=OnD9zGAGT0k (pages 1\u20133, 7\u201310, 15, 18, 20, 22, 24\u201331).\n12. Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 2).\n13. Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=nJJjv0JDJju (pages 2, 8, 24, 27, 31).\n14. Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 2).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgements and References", "md": "# Acknowledgements and References"}, {"type": "heading", "lvl": 1, "value": "Acknowledgements", "md": "# Acknowledgements"}, {"type": "text", "value": "This research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.", "md": "This research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. Brian D.O. Anderson. \u201cReverse-time diffusion equation models\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\u2013326 (page 1).\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 2).\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \u201cCold Diffusion: Inverting arbitrary image transforms without noise\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 2).\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \u201cAlign your latents: High-resolution video synthesis with latent diffusion models\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\u201322575 (page 3).\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \u201cCompressed sensing using generative models\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\u2013546 (page 1).\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\u201398 (pages 2, 8, 24, 27).\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (pages 5, 6).\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 1).\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 1).\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \u201cIlvr: Conditioning method for denoising diffusion probabilistic models\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 2).\n11. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=OnD9zGAGT0k (pages 1\u20133, 7\u201310, 15, 18, 20, 22, 24\u201331).\n12. Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 2).\n13. Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=nJJjv0JDJju (pages 2, 8, 24, 27, 31).\n14. Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 2).", "md": "1. Brian D.O. Anderson. \u201cReverse-time diffusion equation models\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\u2013326 (page 1).\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 2).\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \u201cCold Diffusion: Inverting arbitrary image transforms without noise\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 2).\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \u201cAlign your latents: High-resolution video synthesis with latent diffusion models\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\u201322575 (page 3).\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \u201cCompressed sensing using generative models\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\u2013546 (page 1).\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\u201398 (pages 2, 8, 24, 27).\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (pages 5, 6).\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 1).\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 1).\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \u201cIlvr: Conditioning method for denoising diffusion probabilistic models\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 2).\n11. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=OnD9zGAGT0k (pages 1\u20133, 7\u201310, 15, 18, 20, 22, 24\u201331).\n12. Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 2).\n13. Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. URL: https://openreview.net/forum?id=nJJjv0JDJju (pages 2, 8, 24, 27, 31).\n14. Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 2)."}]}, {"page": 12, "text": "[15]   Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman\n       Milanfar. \u201cSoft diffusion: Score matching for general corruptions\u201d. In: arXiv preprint\n       arXiv:2209.05442 (2022) (page 2).\n[16]   Mauricio Delbracio and Peyman Milanfar. \u201cInversion by direct iteration: An alternative\n       to denoising diffusion for image restoration\u201d. In: arXiv preprint arXiv:2303.11435 (2023)\n       (page 2).\n[17]   Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \u201cImagenet: A large-\n       scale hierarchical image database\u201d. In: 2009 IEEE conference on computer vision and pattern\n       recognition. Ieee. 2009, pp. 248\u2013255 (pages 7, 21, 22, 27\u201329).\n[18]   Prafulla Dhariwal and Alexander Nichol. \u201cDiffusion models beat gans on image synthesis\u201d. In:\n       Advances in Neural Information Processing Systems 34 (2021), pp. 8780\u20138794 (page 1).\n[19]   Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\n       Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. \u201cDataComp: In\n       search of the next generation of multimodal datasets\u201d. In: arXiv preprint arXiv:2304.14108\n       (2023) (page 10).\n[20]   Jonathan Ho, Ajay Jain, and Pieter Abbeel. \u201cDenoising diffusion probabilistic models\u201d. In:\n       Advances in Neural Information Processing Systems 33 (2020), pp. 6840\u20136851 (page 1).\n[21]   Aapo Hyv\u00e4rinen and Peter Dayan. \u201cEstimation of non-normalized statistical models by score\n       matching.\u201d In: Journal of Machine Learning Research 6.4 (2005) (page 1).\n[22]   Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir.\n      \u201cRobust compressed sensing mri with deep generative priors\u201d. In: Advances in Neural Informa-\n       tion Processing Systems 34 (2021), pp. 14938\u201314954 (pages 1, 2).\n[23]   Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price. \u201cInstance-optimal\n       compressed sensing via posterior sampling\u201d. In: arXiv preprint arXiv:2106.11438 (2021)\n       (page 1).\n[24]   Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price. \u201cFairness\n       for Image Generation with Uncertain Sensitive Attributes\u201d. In: Proceedings of the 38th Inter-\n       national Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139.\n       Proceedings of Machine Learning Research. PMLR, 18\u201324 Jul 2021, pp. 4721\u20134732. URL:\n       https://proceedings.mlr.press/v139/jalal21b.html (page 1).\n[25]   Tero Karras, Samuli Laine, and Timo Aila. \u201cA style-based generator architecture for generative\n       adversarial networks\u201d. In: Proceedings of the IEEE/CVF conference on computer vision and\n       pattern recognition. 2019, pp. 4401\u20134410 (pages 7\u201310, 24\u201328, 31).\n[26]   Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. \u201cDenoising Diffusion Restora-\n       tion Models\u201d. In: Advances in Neural Information Processing Systems (pages 1, 2, 8, 24, 27,\n       31).\n[27]   Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad. \u201cGSURE-Based Diffusion\n       Model Training with Corrupted Data\u201d. In: arXiv preprint arXiv:2305.13128 (2023) (page 2).\n[28]   Bahjat Kawar, Gregory Vaksman, and Michael Elad. \u201cSNIPS: Solving noisy inverse problems\n       stochastically\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 21757\u2013\n       21769 (page 31).\n[29]   Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. \u201cSoft\n       truncation: A universal training technique of score-based diffusion model for high precision\n       score estimation\u201d. In: International Conference on Machine Learning. PMLR. 2022, pp. 11201\u2013\n       11228 (page 1).\n[30]   Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\n       Mark D Plumbley. \u201cAudioldm: Text-to-audio generation with latent diffusion models\u201d. In:\n       arXiv preprint arXiv:2301.12503 (2023) (page 3).\n[31]   Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang. \u201cCoherent Semantic Attention for Image\n       Inpainting\u201d. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct.\n       2019). DOI: 10.1109/iccv.2019.00427. URL: http://dx.doi.org/10.1109/ICCV.\n       2019.00427 (page 1).\n[32]   Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van\n       Gool. \u201cRepaint: Inpainting using denoising diffusion probabilistic models\u201d. In: Proceedings of\n       the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 11461\u2013\n       11471 (pages 7, 8, 30).\n                                                 12", "md": "# References\n\n# References\n\n|#|Authors|Title|Publication Details|Pages|\n|---|---|---|---|---|\n|15|Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar|Soft diffusion: Score matching for general corruptions|arXiv preprint arXiv:2209.05442 (2022)|page 2|\n|16|Mauricio Delbracio and Peyman Milanfar|Inversion by direct iteration: An alternative to denoising diffusion for image restoration|arXiv preprint arXiv:2303.11435 (2023)|page 2|\n|17|Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei|Imagenet: A large-scale hierarchical image database|2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\u2013255|pages 7, 21, 22, 27\u201329|\n|18|Prafulla Dhariwal and Alexander Nichol|Diffusion models beat gans on image synthesis|Advances in Neural Information Processing Systems 34 (2021), pp. 8780\u20138794|page 1|\n|19|Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.|DataComp: In search of the next generation of multimodal datasets|arXiv preprint arXiv:2304.14108 (2023)|page 10|\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{#} & \\text{Authors} & \\text{Title} & \\text{Publication Details} & \\text{Pages} \\\\\n\\hline\n20 & Jonathan Ho, Ajay Jain, and Pieter Abbeel & Denoising diffusion probabilistic models & Advances in Neural Information Processing Systems 33 (2020), pp. 6840\u20136851 & page 1 \\\\\n21 & Aapo Hyv\u00e4rinen and Peter Dayan & Estimation of non-normalized statistical models by score matching & Journal of Machine Learning Research 6.4 (2005) & page 1 \\\\\n22 & Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir & Robust compressed sensing mri with deep generative priors & Advances in Neural Information Processing Systems 34 (2021), pp. 14938\u201314954 & pages 1, 2 \\\\\n23 & Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price & Instance-optimal compressed sensing via posterior sampling & arXiv preprint arXiv:2106.11438 (2021) & page 1 \\\\\n24 & Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price & Fairness for Image Generation with Uncertain Sensitive Attributes & Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\u201324 Jul 2021, pp. 4721\u20134732 & page 1 \\\\\n\\hline\n\\end{array}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "table", "rows": [["#", "Authors", "Title", "Publication Details", "Pages"], ["15", "Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar", "Soft diffusion: Score matching for general corruptions", "arXiv preprint arXiv:2209.05442 (2022)", "page 2"], ["16", "Mauricio Delbracio and Peyman Milanfar", "Inversion by direct iteration: An alternative to denoising diffusion for image restoration", "arXiv preprint arXiv:2303.11435 (2023)", "page 2"], ["17", "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei", "Imagenet: A large-scale hierarchical image database", "2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\u2013255", "pages 7, 21, 22, 27\u201329"], ["18", "Prafulla Dhariwal and Alexander Nichol", "Diffusion models beat gans on image synthesis", "Advances in Neural Information Processing Systems 34 (2021), pp. 8780\u20138794", "page 1"], ["19", "Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.", "DataComp: In search of the next generation of multimodal datasets", "arXiv preprint arXiv:2304.14108 (2023)", "page 10"]], "md": "|#|Authors|Title|Publication Details|Pages|\n|---|---|---|---|---|\n|15|Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar|Soft diffusion: Score matching for general corruptions|arXiv preprint arXiv:2209.05442 (2022)|page 2|\n|16|Mauricio Delbracio and Peyman Milanfar|Inversion by direct iteration: An alternative to denoising diffusion for image restoration|arXiv preprint arXiv:2303.11435 (2023)|page 2|\n|17|Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei|Imagenet: A large-scale hierarchical image database|2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\u2013255|pages 7, 21, 22, 27\u201329|\n|18|Prafulla Dhariwal and Alexander Nichol|Diffusion models beat gans on image synthesis|Advances in Neural Information Processing Systems 34 (2021), pp. 8780\u20138794|page 1|\n|19|Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.|DataComp: In search of the next generation of multimodal datasets|arXiv preprint arXiv:2304.14108 (2023)|page 10|", "isPerfectTable": true, "csv": "\"#\",\"Authors\",\"Title\",\"Publication Details\",\"Pages\"\n\"15\",\"Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar\",\"Soft diffusion: Score matching for general corruptions\",\"arXiv preprint arXiv:2209.05442 (2022)\",\"page 2\"\n\"16\",\"Mauricio Delbracio and Peyman Milanfar\",\"Inversion by direct iteration: An alternative to denoising diffusion for image restoration\",\"arXiv preprint arXiv:2303.11435 (2023)\",\"page 2\"\n\"17\",\"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei\",\"Imagenet: A large-scale hierarchical image database\",\"2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\u2013255\",\"pages 7, 21, 22, 27\u201329\"\n\"18\",\"Prafulla Dhariwal and Alexander Nichol\",\"Diffusion models beat gans on image synthesis\",\"Advances in Neural Information Processing Systems 34 (2021), pp. 8780\u20138794\",\"page 1\"\n\"19\",\"Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al.\",\"DataComp: In search of the next generation of multimodal datasets\",\"arXiv preprint arXiv:2304.14108 (2023)\",\"page 10\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{#} & \\text{Authors} & \\text{Title} & \\text{Publication Details} & \\text{Pages} \\\\\n\\hline\n20 & Jonathan Ho, Ajay Jain, and Pieter Abbeel & Denoising diffusion probabilistic models & Advances in Neural Information Processing Systems 33 (2020), pp. 6840\u20136851 & page 1 \\\\\n21 & Aapo Hyv\u00e4rinen and Peter Dayan & Estimation of non-normalized statistical models by score matching & Journal of Machine Learning Research 6.4 (2005) & page 1 \\\\\n22 & Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir & Robust compressed sensing mri with deep generative priors & Advances in Neural Information Processing Systems 34 (2021), pp. 14938\u201314954 & pages 1, 2 \\\\\n23 & Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price & Instance-optimal compressed sensing via posterior sampling & arXiv preprint arXiv:2106.11438 (2021) & page 1 \\\\\n24 & Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price & Fairness for Image Generation with Uncertain Sensitive Attributes & Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\u201324 Jul 2021, pp. 4721\u20134732 & page 1 \\\\\n\\hline\n\\end{array}\n$$", "md": "$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{#} & \\text{Authors} & \\text{Title} & \\text{Publication Details} & \\text{Pages} \\\\\n\\hline\n20 & Jonathan Ho, Ajay Jain, and Pieter Abbeel & Denoising diffusion probabilistic models & Advances in Neural Information Processing Systems 33 (2020), pp. 6840\u20136851 & page 1 \\\\\n21 & Aapo Hyv\u00e4rinen and Peter Dayan & Estimation of non-normalized statistical models by score matching & Journal of Machine Learning Research 6.4 (2005) & page 1 \\\\\n22 & Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir & Robust compressed sensing mri with deep generative priors & Advances in Neural Information Processing Systems 34 (2021), pp. 14938\u201314954 & pages 1, 2 \\\\\n23 & Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price & Instance-optimal compressed sensing via posterior sampling & arXiv preprint arXiv:2106.11438 (2021) & page 1 \\\\\n24 & Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price & Fairness for Image Generation with Uncertain Sensitive Attributes & Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\u201324 Jul 2021, pp. 4721\u20134732 & page 1 \\\\\n\\hline\n\\end{array}\n$$"}]}, {"page": 13, "text": "[33]  Gary Mataev, Peyman Milanfar, and Michael Elad. \u201cDeepRED: Deep image prior powered\n      by RED\u201d. In: Proceedings of the IEEE/CVF International Conference on Computer Vision\n      Workshops. 2019, pp. 0\u20130 (page 1).\n[34]  Xiangming Meng and Yoshiyuki Kabashima. \u201cDiffusion model based posterior sampling for\n      noisy linear inverse problems\u201d. In: arXiv preprint arXiv:2211.12343 (2022) (pages 27, 31).\n[35]  Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. \u201cPulse: Self-\n      supervised photo upsampling via latent space exploration of generative models\u201d. In: Proceed-\n      ings of the ieee/cvf conference on computer vision and pattern recognition. 2020, pp. 2437\u2013\n      2445 (page 1).\n[36]  Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis,\n      and Rebecca Willett. \u201cDeep learning techniques for inverse problems in imaging\u201d. In: IEEE\n      Journal on Selected Areas in Information Theory 1.1 (2020), pp. 39\u201356 (page 1).\n[37]  Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. \u201cCon-\n      text encoders: Feature learning by inpainting\u201d. In: Proceedings of the IEEE conference on\n      computer vision and pattern recognition. 2016, pp. 2536\u20132544 (page 1).\n[38]  Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernan-\n      dez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso. \u201cBrain imaging generation\n      with latent diffusion models\u201d. In: Deep Generative Models: Second MICCAI Workshop,\n      DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22,\n      2022, Proceedings. Springer. 2022, pp. 117\u2013126 (page 3).\n[39]  Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and\n      Daniel Cohen-Or. \u201cEncoding in Style: a StyleGAN Encoder for Image-to-Image Translation\u201d.\n      In: arXiv preprint arXiv:2008.00951 (2020) (page 1).\n[40]  Yaniv Romano, Michael Elad, and Peyman Milanfar. \u201cThe little engine that could: Regulariza-\n      tion by denoising (RED)\u201d. In: SIAM Journal on Imaging Sciences 10.4 (2017), pp. 1804\u20131844\n      (pages 1, 31).\n[41]  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n      \u201cHigh-resolution image synthesis with latent diffusion models\u201d. In: Proceedings of the\n      IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10684\u201310695\n      (pages 2, 3, 5, 7).\n[42]  Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai. \u201cA Theoretical\n      Justification for Image Inpainting using Denoising Diffusion Probabilistic Models\u201d. In: arXiv\n      preprint arXiv:2302.01217 (2023) (pages 5, 6, 15).\n[43]  Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans,\n      David J. Fleet, and Mohammad Norouzi. Palette: Image-to-Image Diffusion Models. 2022.\n      arXiv: 2111.05826 [cs.CV] (page 31).\n[44]  Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\n      Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M:\n      Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. 2021. arXiv: 2111.02114\n      [cs.CV] (page 7).\n[45]  Christoph Schuhmann et al. LAION-5B: An open large-scale dataset for training next genera-\n      tion image-text models. 2022. arXiv: 2210.08402 [cs.CV] (page 7).\n[46]  Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. \u201cPseudoinverse-guided diffu-\n      sion models for inverse problems\u201d. In: International Conference on Learning Representations.\n      2023 (pages 2, 31).\n[47]  Yang Song and Stefano Ermon. \u201cGenerative modeling by estimating gradients of the data\n      distribution\u201d. In: Advances in Neural Information Processing Systems 32 (2019) (page 1).\n[48]  Yang Song and Stefano Ermon. \u201cImproved techniques for training score-based generative\n      models\u201d. In: Advances in neural information processing systems 33 (2020), pp. 12438\u201312448\n      (page 1).\n[49]  Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\n      Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In:\n      International Conference on Learning Representations. 2021 (page 1).\n[50]  Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\n      Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In:\n      International Conference on Learning Representations (pages 8, 24, 27).\n                                                    13", "md": "# References\n\n## List of References\n\n|#|Authors|Title|Publication Details|Pages|\n|---|---|---|---|---|\n|33|Gary Mataev, Peyman Milanfar, and Michael Elad|DeepRED: Deep image prior powered by RED|Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019|0-0 (page 1)|\n|34|Xiangming Meng and Yoshiyuki Kabashima|Diffusion model based posterior sampling for noisy linear inverse problems|arXiv preprint arXiv:2211.12343 (2022)|27, 31|\n|35|Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin|Pulse: Self-supervised photo upsampling via latent space exploration of generative models|Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2020|2437-2445 (page 1)|\n|36|Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett|Deep learning techniques for inverse problems in imaging|IEEE Journal on Selected Areas in Information Theory 1.1 (2020)|39-56 (page 1)|\n|37|Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros|Context encoders: Feature learning by inpainting|Proceedings of the IEEE conference on computer vision and pattern recognition. 2016|2536-2544 (page 1)|\n|38|Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso|Brain imaging generation with latent diffusion models|Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer. 2022|117-126 (page 3)|\n|39|Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or|Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation|arXiv preprint arXiv:2008.00951 (2020)|(page 1)|\n|40|Yaniv Romano, Michael Elad, and Peyman Milanfar|The little engine that could: Regularization by denoising (RED)|SIAM Journal on Imaging Sciences 10.4 (2017)|1804-1844 (pages 1, 31)|\n|41|Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer|High-resolution image synthesis with latent diffusion models|Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022|10684-10695 (pages 2, 3, 5, 7)|\n|42|Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai|A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models|arXiv preprint arXiv:2302.01217 (2023)|5, 6, 15|\n|43|Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi|Palette: Image-to-Image Diffusion Models|arXiv: 2111.05826 [cs.CV] (page 31)| |\n|44|Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki|LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs|arXiv: 2111.02114 [cs.CV] (page 7)| |\n|45|Christoph Schuhmann et al.|LAION-5B: An open large-scale dataset for training next generation image-text models|arXiv: 2210.08402 [cs.CV] (page 7)| |\n|46|Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz|Pseudoinverse-guided diffusion models for inverse problems|International Conference on Learning Representations. 2023|2, 31|\n|47|Yang Song and Stefano Ermon|Generative modeling by estimating gradients of the data distribution|Advances in Neural Information Processing Systems 32 (2019)|(page 1)|\n|48|Yang Song and Stefano Ermon|Improved techniques for training score-based generative models|Advances in neural information processing systems 33 (2020)|12438-12448 (page 1)|\n|49|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole|Score-Based Generative Modeling through Stochastic Differential Equations|International Conference on Learning Representations. 2021|(page 1)|\n|50|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole|Score-Based Generative Modeling through Stochastic Differential Equations|International Conference on Learning Representations|8, 24, 27|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "List of References", "md": "## List of References"}, {"type": "table", "rows": [["#", "Authors", "Title", "Publication Details", "Pages"], ["33", "Gary Mataev, Peyman Milanfar, and Michael Elad", "DeepRED: Deep image prior powered by RED", "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019", "0-0 (page 1)"], ["34", "Xiangming Meng and Yoshiyuki Kabashima", "Diffusion model based posterior sampling for noisy linear inverse problems", "arXiv preprint arXiv:2211.12343 (2022)", "27, 31"], ["35", "Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin", "Pulse: Self-supervised photo upsampling via latent space exploration of generative models", "Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2020", "2437-2445 (page 1)"], ["36", "Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett", "Deep learning techniques for inverse problems in imaging", "IEEE Journal on Selected Areas in Information Theory 1.1 (2020)", "39-56 (page 1)"], ["37", "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros", "Context encoders: Feature learning by inpainting", "Proceedings of the IEEE conference on computer vision and pattern recognition. 2016", "2536-2544 (page 1)"], ["38", "Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso", "Brain imaging generation with latent diffusion models", "Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer. 2022", "117-126 (page 3)"], ["39", "Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or", "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation", "arXiv preprint arXiv:2008.00951 (2020)", "(page 1)"], ["40", "Yaniv Romano, Michael Elad, and Peyman Milanfar", "The little engine that could: Regularization by denoising (RED)", "SIAM Journal on Imaging Sciences 10.4 (2017)", "1804-1844 (pages 1, 31)"], ["41", "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer", "High-resolution image synthesis with latent diffusion models", "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022", "10684-10695 (pages 2, 3, 5, 7)"], ["42", "Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai", "A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models", "arXiv preprint arXiv:2302.01217 (2023)", "5, 6, 15"], ["43", "Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi", "Palette: Image-to-Image Diffusion Models", "arXiv: 2111.05826 [cs.CV] (page 31)", ""], ["44", "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki", "LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs", "arXiv: 2111.02114 [cs.CV] (page 7)", ""], ["45", "Christoph Schuhmann et al.", "LAION-5B: An open large-scale dataset for training next generation image-text models", "arXiv: 2210.08402 [cs.CV] (page 7)", ""], ["46", "Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz", "Pseudoinverse-guided diffusion models for inverse problems", "International Conference on Learning Representations. 2023", "2, 31"], ["47", "Yang Song and Stefano Ermon", "Generative modeling by estimating gradients of the data distribution", "Advances in Neural Information Processing Systems 32 (2019)", "(page 1)"], ["48", "Yang Song and Stefano Ermon", "Improved techniques for training score-based generative models", "Advances in neural information processing systems 33 (2020)", "12438-12448 (page 1)"], ["49", "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole", "Score-Based Generative Modeling through Stochastic Differential Equations", "International Conference on Learning Representations. 2021", "(page 1)"], ["50", "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole", "Score-Based Generative Modeling through Stochastic Differential Equations", "International Conference on Learning Representations", "8, 24, 27"]], "md": "|#|Authors|Title|Publication Details|Pages|\n|---|---|---|---|---|\n|33|Gary Mataev, Peyman Milanfar, and Michael Elad|DeepRED: Deep image prior powered by RED|Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019|0-0 (page 1)|\n|34|Xiangming Meng and Yoshiyuki Kabashima|Diffusion model based posterior sampling for noisy linear inverse problems|arXiv preprint arXiv:2211.12343 (2022)|27, 31|\n|35|Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin|Pulse: Self-supervised photo upsampling via latent space exploration of generative models|Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2020|2437-2445 (page 1)|\n|36|Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett|Deep learning techniques for inverse problems in imaging|IEEE Journal on Selected Areas in Information Theory 1.1 (2020)|39-56 (page 1)|\n|37|Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros|Context encoders: Feature learning by inpainting|Proceedings of the IEEE conference on computer vision and pattern recognition. 2016|2536-2544 (page 1)|\n|38|Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso|Brain imaging generation with latent diffusion models|Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer. 2022|117-126 (page 3)|\n|39|Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or|Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation|arXiv preprint arXiv:2008.00951 (2020)|(page 1)|\n|40|Yaniv Romano, Michael Elad, and Peyman Milanfar|The little engine that could: Regularization by denoising (RED)|SIAM Journal on Imaging Sciences 10.4 (2017)|1804-1844 (pages 1, 31)|\n|41|Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer|High-resolution image synthesis with latent diffusion models|Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022|10684-10695 (pages 2, 3, 5, 7)|\n|42|Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai|A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models|arXiv preprint arXiv:2302.01217 (2023)|5, 6, 15|\n|43|Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi|Palette: Image-to-Image Diffusion Models|arXiv: 2111.05826 [cs.CV] (page 31)| |\n|44|Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki|LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs|arXiv: 2111.02114 [cs.CV] (page 7)| |\n|45|Christoph Schuhmann et al.|LAION-5B: An open large-scale dataset for training next generation image-text models|arXiv: 2210.08402 [cs.CV] (page 7)| |\n|46|Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz|Pseudoinverse-guided diffusion models for inverse problems|International Conference on Learning Representations. 2023|2, 31|\n|47|Yang Song and Stefano Ermon|Generative modeling by estimating gradients of the data distribution|Advances in Neural Information Processing Systems 32 (2019)|(page 1)|\n|48|Yang Song and Stefano Ermon|Improved techniques for training score-based generative models|Advances in neural information processing systems 33 (2020)|12438-12448 (page 1)|\n|49|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole|Score-Based Generative Modeling through Stochastic Differential Equations|International Conference on Learning Representations. 2021|(page 1)|\n|50|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole|Score-Based Generative Modeling through Stochastic Differential Equations|International Conference on Learning Representations|8, 24, 27|", "isPerfectTable": true, "csv": "\"#\",\"Authors\",\"Title\",\"Publication Details\",\"Pages\"\n\"33\",\"Gary Mataev, Peyman Milanfar, and Michael Elad\",\"DeepRED: Deep image prior powered by RED\",\"Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019\",\"0-0 (page 1)\"\n\"34\",\"Xiangming Meng and Yoshiyuki Kabashima\",\"Diffusion model based posterior sampling for noisy linear inverse problems\",\"arXiv preprint arXiv:2211.12343 (2022)\",\"27, 31\"\n\"35\",\"Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin\",\"Pulse: Self-supervised photo upsampling via latent space exploration of generative models\",\"Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2020\",\"2437-2445 (page 1)\"\n\"36\",\"Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett\",\"Deep learning techniques for inverse problems in imaging\",\"IEEE Journal on Selected Areas in Information Theory 1.1 (2020)\",\"39-56 (page 1)\"\n\"37\",\"Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros\",\"Context encoders: Feature learning by inpainting\",\"Proceedings of the IEEE conference on computer vision and pattern recognition. 2016\",\"2536-2544 (page 1)\"\n\"38\",\"Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso\",\"Brain imaging generation with latent diffusion models\",\"Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer. 2022\",\"117-126 (page 3)\"\n\"39\",\"Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or\",\"Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation\",\"arXiv preprint arXiv:2008.00951 (2020)\",\"(page 1)\"\n\"40\",\"Yaniv Romano, Michael Elad, and Peyman Milanfar\",\"The little engine that could: Regularization by denoising (RED)\",\"SIAM Journal on Imaging Sciences 10.4 (2017)\",\"1804-1844 (pages 1, 31)\"\n\"41\",\"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer\",\"High-resolution image synthesis with latent diffusion models\",\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022\",\"10684-10695 (pages 2, 3, 5, 7)\"\n\"42\",\"Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai\",\"A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models\",\"arXiv preprint arXiv:2302.01217 (2023)\",\"5, 6, 15\"\n\"43\",\"Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi\",\"Palette: Image-to-Image Diffusion Models\",\"arXiv: 2111.05826 [cs.CV] (page 31)\",\"\"\n\"44\",\"Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki\",\"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs\",\"arXiv: 2111.02114 [cs.CV] (page 7)\",\"\"\n\"45\",\"Christoph Schuhmann et al.\",\"LAION-5B: An open large-scale dataset for training next generation image-text models\",\"arXiv: 2210.08402 [cs.CV] (page 7)\",\"\"\n\"46\",\"Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz\",\"Pseudoinverse-guided diffusion models for inverse problems\",\"International Conference on Learning Representations. 2023\",\"2, 31\"\n\"47\",\"Yang Song and Stefano Ermon\",\"Generative modeling by estimating gradients of the data distribution\",\"Advances in Neural Information Processing Systems 32 (2019)\",\"(page 1)\"\n\"48\",\"Yang Song and Stefano Ermon\",\"Improved techniques for training score-based generative models\",\"Advances in neural information processing systems 33 (2020)\",\"12438-12448 (page 1)\"\n\"49\",\"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole\",\"Score-Based Generative Modeling through Stochastic Differential Equations\",\"International Conference on Learning Representations. 2021\",\"(page 1)\"\n\"50\",\"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole\",\"Score-Based Generative Modeling through Stochastic Differential Equations\",\"International Conference on Learning Representations\",\"8, 24, 27\""}]}, {"page": 14, "text": "[51]  Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion\n      models from human brain activity\u201d. In: Proceedings of the IEEE/CVF Conference on Computer\n      Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3).\n[52]  Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play\n      priors for model based reconstruction\u201d. In: 2013 IEEE Global Conference on Signal and\n      Information Processing. IEEE. 2013, pp. 945\u2013948 (page 1).\n[53]  Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In:\n      Neural computation 23.7 (2011), pp. 1661\u20131674 (page 1).\n[54]  Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pelle-\n      grini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and\n      editbench: Advancing and evaluating text-guided image inpainting\u201d. In: Proceedings of the\n      IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369\n      (pages 1, 21).\n[55]  Yinhuai Wang, Jiwen Yu, and Jian Zhang. \u201cZero-Shot Image Restoration Using Denoising\n      Diffusion Null-Space Model\u201d. In: The Eleventh International Conference on Learning Repre-\n      sentations. 2023. URL: https://openreview.net/forum?id=mRieQgMtNTQ (page 31).\n[56]  Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form\n      Image Inpainting With Gated Convolution\u201d. In: 2019 IEEE/CVF International Conference\n      on Computer Vision (ICCV) (Oct. 2019). DOI: 10.1109/iccv.2019.00457. URL: http:\n      //dx.doi.org/10.1109/ICCV.2019.00457 (page 1).\n                                                 14", "md": "- [51] Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion models from human brain activity\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3).\n- [52] Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play priors for model based reconstruction\u201d. In: 2013 IEEE Global Conference on Signal and Information Processing. IEEE. 2013, pp. 945\u2013948 (page 1).\n- [53] Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In: Neural computation 23.7 (2011), pp. 1661\u20131674 (page 1).\n- [54] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and editbench: Advancing and evaluating text-guided image inpainting\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369 (pages 1, 21).\n- [55] Yinhuai Wang, Jiwen Yu, and Jian Zhang. \u201cZero-Shot Image Restoration Using Denoising Diffusion Null-Space Model\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=mRieQgMtNTQ (page 31).\n- [56] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form Image Inpainting With Gated Convolution\u201d. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). DOI: 10.1109/iccv.2019.00457. URL: http://dx.doi.org/10.1109/ICCV.2019.00457 (page 1).", "images": [], "items": [{"type": "text", "value": "- [51] Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion models from human brain activity\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3).\n- [52] Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play priors for model based reconstruction\u201d. In: 2013 IEEE Global Conference on Signal and Information Processing. IEEE. 2013, pp. 945\u2013948 (page 1).\n- [53] Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In: Neural computation 23.7 (2011), pp. 1661\u20131674 (page 1).\n- [54] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and editbench: Advancing and evaluating text-guided image inpainting\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369 (pages 1, 21).\n- [55] Yinhuai Wang, Jiwen Yu, and Jian Zhang. \u201cZero-Shot Image Restoration Using Denoising Diffusion Null-Space Model\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=mRieQgMtNTQ (page 31).\n- [56] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form Image Inpainting With Gated Convolution\u201d. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). DOI: 10.1109/iccv.2019.00457. URL: http://dx.doi.org/10.1109/ICCV.2019.00457 (page 1).", "md": "- [51] Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion models from human brain activity\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3).\n- [52] Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play priors for model based reconstruction\u201d. In: 2013 IEEE Global Conference on Signal and Information Processing. IEEE. 2013, pp. 945\u2013948 (page 1).\n- [53] Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In: Neural computation 23.7 (2011), pp. 1661\u20131674 (page 1).\n- [54] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and editbench: Advancing and evaluating text-guided image inpainting\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369 (pages 1, 21).\n- [55] Yinhuai Wang, Jiwen Yu, and Jian Zhang. \u201cZero-Shot Image Restoration Using Denoising Diffusion Null-Space Model\u201d. In: The Eleventh International Conference on Learning Representations. 2023. URL: https://openreview.net/forum?id=mRieQgMtNTQ (page 31).\n- [56] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form Image Inpainting With Gated Convolution\u201d. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). DOI: 10.1109/iccv.2019.00457. URL: http://dx.doi.org/10.1109/ICCV.2019.00457 (page 1)."}]}, {"page": 15, "text": " A       Additional Theoretical Results\n Notation and Measurement Matrix. We elaborate on the structure of the measurement matrix\n A \u2208     Rl\u00d7d. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel\n selector and consists of a subset of the rows from the d \u00d7 d identity matrix (the rows that are present \u2192\n correspond to the indices of the selected pixels from the image \u2212                                       x0 \u2208      Rd). Given this structure, it\n immediately follows that AT A is a d \u00d7 d matrix that has the interpretation of a pixel selection mask.\n Specifically, AT A is a d              \u00d7  d diagonal matrix D(m), where the elements of m are set to 1 where data\n (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose\n that the first k coordinates are known.\n A.1       Posterior Sampling using Pixel-space Diffusion Model\n We first consider the reverse process, starting with \u2190                          x1\u2212\u223c      N (0, Id), and borrow a result from [42] to\n show that the sample \u2190            x0\u2212generated by the reverse process is a valid image from p(\u2212                                     \u2192\n                                                                                                                                    x0).\n Theorem A.1 (Generative Modeling using Diffusion in Pixel Space, [42]). Suppose Assumption 3.1\n holds. Let\n                                                   \u2192   \u2192      \u02dc      \u2212\u2192     \u2192     \u2192       \u2192                \u2212\u2192     \u2212\u2192     \u2192        2    .\n                        \u03b8\u2217    = arg min    \u03b8 E\u2212   x0,\u2212\u03f5        \u00b51     x1(\u2212  x0, \u2212\u03f5 ), \u2212   x0     \u2212   \u00b5\u03b8     x1     x0, \u2212\u03f5\n                                                       \u2212 \u2192    \u2212 \u2192     \u2192       := \u03b8\u2212   \u2192     \u2212\u2192     \u2192\n For a fixed variance \u03b2 > 0, if \u00b5\u03b8                      x1     x0, \u2212\u03f5                 x1     x0, \u2212\u03f5      , then the closed-form solution \u03b8\u2217     \u2212\u2192\n is \u221a1 \u2212       \u03b2SST , which after normalization\u2212by 1/\u221a1 \u2212                                \u03b2 recovers the true subspace of p                   \u2192   x0    .\n Though this establishes that \u2190              x0 generated by the reverse process is a valid image from p(\u2212                                  x0), it is not\n necessarily a sample from the posterior p(\u2212                        \u2192\n                                                                    x0|y) that satisfies the measurements. To accomplish this\n we perform one additional step of gradient descent for every step of the reverse process. This gives\n us Algorithm 1, the DPS algorithm. The next theorem shows that the reverse SDE guided by these\n measurements (3) recovers the true underlying sample5.\n Theorem A.2 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and\n Assumption 3.2 hold. Let us denote by \u03c3j, \u2200j = 1, . . . , r, the singular values of (AS)T (AS) and\n                                                   \u2192   \u2192      \u02dc      \u2212\u2192     \u2192     \u2192       \u2192                \u2212\u2192     \u2212\u2192     \u2192        2    .\n                        \u03b8\u2217    = arg min    \u03b8 E\u2212   x0,\u2212\u03f5        \u00b51     x1(\u2212  x0, \u2212\u03f5 ), \u2212   x0     \u2212   \u00b5\u03b8     x1     x0, \u2212\u03f5\n Given a partially known image \u2212                       \u2192              \u2192\n \u03b6j                                                    x0 \u223c      \u2192p(\u2212 x0), a fixed variance \u03b2 > 0, there exists a step size\n   i = 1/2\u03c3j for all the coordinates of \u2212                       x0 such that Algorithm 1 samples from the true posterior\n p(\u2212 \u2192                                                                                          \u2212        \u2192\n    x0|y) and exactly recovers the groundtruth sample, i.e., \u2190                                 x0 = \u2212   x0.\n B       Technical Proofs\n This section contains proofs of all the theorems and propositions presented in the main body of the\n paper. For clarity, we restate the theorems more formally with precise mathematical details.\n B.1       Proof of Theorem A.2\n Theorem B.1 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and\n Assumption 3.2 hold. Let us denote by \u03c3 = {\u03c3j}k                                       j=1 the singular values of (AS)T (AS), i.e.\n(AS)T (AS) = U\u03a3V T := UD(\u03c3)V T , U \u2208                                       Rk\u00d7k, V \u2208           Rk\u00d7k and\n                                                              \u02dc      \u2212\u2192     \u2192     \u2192       \u2192                \u2212\u2192     \u2212\u2192     \u2192        2\n                        \u03b8\u2217    = arg min            \u2192   \u2192       \u00b51     x1(\u2212  x0, \u2212\u03f5 ), \u2212   x0     \u2212   \u00b5\u03b8     x1    x0, \u2212\u03f5               .\n                                           \u03b8 E\u2212   x0,\u2212\u03f5\n      5While the DPS Algorithm [11] uses a scalar step size \u03b6i at each step, this does not suffice for exact recovery.\n However, by generalizing to allow a different step size per coordinate, we can show sample recovery. Thus, in\n this section, we denote \u03b6j         i to be the step size at step i and coordinate j, 1 \u2264                    j \u2264    r. Also note that the step index\n i is vacuous in this section, as we consider a two-step diffusion process (i.e., i is always \u20191\u2019).\n                                                                              15", "md": "# Additional Theoretical Results\n\n## Additional Theoretical Results\n\nNotation and Measurement Matrix. We elaborate on the structure of the measurement matrix $$A \\in \\mathbb{R}^{l \\times d}$$. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the $$d \\times d$$ identity matrix (the rows that are present correspond to the indices of the selected pixels from the image $$x_0 \\in \\mathbb{R}^d$$). Given this structure, it immediately follows that $$A^T A$$ is a $$d \\times d$$ matrix that has the interpretation of a pixel selection mask. Specifically, $$A^T A$$ is a $$d \\times d$$ diagonal matrix $$D(m)$$, where the elements of $$m$$ are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first $$k$$ coordinates are known.\n\n### Posterior Sampling using Pixel-space Diffusion Model\n\nWe first consider the reverse process, starting with $$x_1 \\sim N(0, I_d)$$, and borrow a result from [42] to show that the sample $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$.\n\nTheorem A.1 (Generative Modeling using Diffusion in Pixel Space, [42]). Suppose Assumption 3.1 holds. Let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{x_0, -\\epsilon}[\\mu_1 x_1(-x_0, -\\epsilon), -x_0 - \\mu_{\\theta} x_1 x_0, -\\epsilon]\n$$\n\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} x_1 x_0, -\\epsilon = \\mu_{\\theta} x_1 x_0, -\\epsilon$$, then the closed-form solution $$\\theta^*$$ is $$\\sqrt{1 - \\beta}SST$$, which after normalization by $$1/\\sqrt{1 - \\beta}$$ recovers the true subspace of $$p(x_0)$$.\n\nThough this establishes that $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$, it is not necessarily a sample from the posterior $$p(x_0|y)$$ that satisfies the measurements. To accomplish this, we perform one additional step of gradient descent for every step of the reverse process. This gives us Algorithm 1, the DPS algorithm. The next theorem shows that the reverse SDE guided by these measurements recovers the true underlying sample.\n\nTheorem A.2 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma_j, \\forall j = 1, ..., r$$, the singular values of $$(AS)^T (AS)$$.\n\nTechnical Proofs\n\nThis section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details.\n\n### Proof of Theorem A.2\n\nTheorem B.1 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma = \\{\\sigma_j\\}_{j=1}^k$$ the singular values of $$(AS)^T (AS)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Theoretical Results", "md": "# Additional Theoretical Results"}, {"type": "heading", "lvl": 2, "value": "Additional Theoretical Results", "md": "## Additional Theoretical Results"}, {"type": "text", "value": "Notation and Measurement Matrix. We elaborate on the structure of the measurement matrix $$A \\in \\mathbb{R}^{l \\times d}$$. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the $$d \\times d$$ identity matrix (the rows that are present correspond to the indices of the selected pixels from the image $$x_0 \\in \\mathbb{R}^d$$). Given this structure, it immediately follows that $$A^T A$$ is a $$d \\times d$$ matrix that has the interpretation of a pixel selection mask. Specifically, $$A^T A$$ is a $$d \\times d$$ diagonal matrix $$D(m)$$, where the elements of $$m$$ are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first $$k$$ coordinates are known.", "md": "Notation and Measurement Matrix. We elaborate on the structure of the measurement matrix $$A \\in \\mathbb{R}^{l \\times d}$$. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the $$d \\times d$$ identity matrix (the rows that are present correspond to the indices of the selected pixels from the image $$x_0 \\in \\mathbb{R}^d$$). Given this structure, it immediately follows that $$A^T A$$ is a $$d \\times d$$ matrix that has the interpretation of a pixel selection mask. Specifically, $$A^T A$$ is a $$d \\times d$$ diagonal matrix $$D(m)$$, where the elements of $$m$$ are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first $$k$$ coordinates are known."}, {"type": "heading", "lvl": 3, "value": "Posterior Sampling using Pixel-space Diffusion Model", "md": "### Posterior Sampling using Pixel-space Diffusion Model"}, {"type": "text", "value": "We first consider the reverse process, starting with $$x_1 \\sim N(0, I_d)$$, and borrow a result from [42] to show that the sample $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$.\n\nTheorem A.1 (Generative Modeling using Diffusion in Pixel Space, [42]). Suppose Assumption 3.1 holds. Let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{x_0, -\\epsilon}[\\mu_1 x_1(-x_0, -\\epsilon), -x_0 - \\mu_{\\theta} x_1 x_0, -\\epsilon]\n$$\n\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} x_1 x_0, -\\epsilon = \\mu_{\\theta} x_1 x_0, -\\epsilon$$, then the closed-form solution $$\\theta^*$$ is $$\\sqrt{1 - \\beta}SST$$, which after normalization by $$1/\\sqrt{1 - \\beta}$$ recovers the true subspace of $$p(x_0)$$.\n\nThough this establishes that $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$, it is not necessarily a sample from the posterior $$p(x_0|y)$$ that satisfies the measurements. To accomplish this, we perform one additional step of gradient descent for every step of the reverse process. This gives us Algorithm 1, the DPS algorithm. The next theorem shows that the reverse SDE guided by these measurements recovers the true underlying sample.\n\nTheorem A.2 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma_j, \\forall j = 1, ..., r$$, the singular values of $$(AS)^T (AS)$$.\n\nTechnical Proofs\n\nThis section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details.", "md": "We first consider the reverse process, starting with $$x_1 \\sim N(0, I_d)$$, and borrow a result from [42] to show that the sample $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$.\n\nTheorem A.1 (Generative Modeling using Diffusion in Pixel Space, [42]). Suppose Assumption 3.1 holds. Let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{x_0, -\\epsilon}[\\mu_1 x_1(-x_0, -\\epsilon), -x_0 - \\mu_{\\theta} x_1 x_0, -\\epsilon]\n$$\n\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} x_1 x_0, -\\epsilon = \\mu_{\\theta} x_1 x_0, -\\epsilon$$, then the closed-form solution $$\\theta^*$$ is $$\\sqrt{1 - \\beta}SST$$, which after normalization by $$1/\\sqrt{1 - \\beta}$$ recovers the true subspace of $$p(x_0)$$.\n\nThough this establishes that $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$, it is not necessarily a sample from the posterior $$p(x_0|y)$$ that satisfies the measurements. To accomplish this, we perform one additional step of gradient descent for every step of the reverse process. This gives us Algorithm 1, the DPS algorithm. The next theorem shows that the reverse SDE guided by these measurements recovers the true underlying sample.\n\nTheorem A.2 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma_j, \\forall j = 1, ..., r$$, the singular values of $$(AS)^T (AS)$$.\n\nTechnical Proofs\n\nThis section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details."}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem A.2", "md": "### Proof of Theorem A.2"}, {"type": "text", "value": "Theorem B.1 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma = \\{\\sigma_j\\}_{j=1}^k$$ the singular values of $$(AS)^T (AS)$$.", "md": "Theorem B.1 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma = \\{\\sigma_j\\}_{j=1}^k$$ the singular values of $$(AS)^T (AS)$$."}]}, {"page": 16, "text": "             \u2192          \u2192                                            \u2192\n Suppose \u2212   x0 \u223c    p(\u2212x0). Given measurements y = A\u2212               x0 and a fixed variance \u03b2 \u2208            (0, 1), there exists a \u2192\n matrix step size6 \u03b6 = (1/2)(SU)D(\u03b6i)(SU)T , \u03b6i = {\u03b6j                        i = 1/\u03c3j}k    j=1 for all the coordinates of \u2212        x0\n such that Algorithm 1 samples from the true posterior p(\u2212                   \u2192\n sample, i.e., \u2190   \u2212      \u2192                                                  x0|y) and exactly recovers the groundtruth\n                  x0 = \u2212  x0.\n Proof. Our goal is to show that \u2190            \u2212        \u2192              \u2212\n reverse process starts with \u2190        \u2212      x0 = \u2212   x0, where \u2190    x0 is returned by Algorithm 1. Recall that the\n                                     x1 \u223c    N (0, Id) and generates the following:\n                            \u2190\u2212           \u2212           \u2212   A\u2190   \u2212    \u2212          2\n                            x0 = \u03b8\u2217\u2190    x1 \u2212   \u03b6\u2207\u2190  x1       x0(\u2190 x1) \u2212    y   2\n                                 = \u03b8\u2217\u2190   \u2212           \u2212   ASST \u2190     \u2212         2\n                                        x1 \u2212   \u03b6\u2207\u2190  x1             x1 \u2212    y   2  \u2212\n                                            \u2212                                    x1 \u2212    y\n                                 = SST \u2190   x1 \u2212    2\u03b6   ASST  T  ASST \u2190       \u2212\n                                            \u2212                                x1 \u2212    y\n                                 = SST \u2190   x1 \u2212    2\u03b6SST AT  ASST \u2190         \u2212\n                                            \u2212                              x1 + 2\u03b6SST AT y\n                                 = SST \u2190   x1 \u2212    2\u03b6SST AT ASST \u2190          \u2212                        \u2192\n                                            \u2212                              x1 + 2\u03b6SST AT A\u2212          x0\n                                 = SST \u2190   x1 \u2212    2\u03b6SST AT ASST \u2190\n                                 = SST \u2190    \u2212                               \u2212                          \u2192\n                                           x1 \u2212    2\u03b6SST AT ASST \u2190         x1 + 2\u03b6SST AT AS\u2212           z0.\n Now, we use the singular value decomposition of (AS)T (AS) with left singular vectors in U \u2208                                 Rk\u00d7k,\n right singular vectors in V \u2208           Rk\u00d7k, and singular values \u03c3 = [\u03c31, . . . , \u03c3k] in \u03a3 = D(\u03c3). Thus, the\n above expression becomes\n       \u2190\u2212              \u2212                            \u2212                      \u2192\n       x0 = SST \u2190     x1 \u2212    2\u03b6SU\u03a3V T ST \u2190        x1 + 2\u03b6SU\u03a3V T \u2212         z0\n            = SST \u2190    \u2212                            \u2212                      \u2192\n                      x1 \u2212    2\u03b6SU\u03a3V T ST \u2190        x1 + 2\u03b6SU\u03a3V T \u2212         z0\n            = SST \u2190    \u2212                                                     \u2212                                               \u2192\n            (i)       x1 \u2212    2(SU)D(\u03b6i)(SU)T SU\u03a3V T ST \u2190                   x1 + 2(SU)D(\u03b6i)(SU)T SU\u03a3V T \u2212                    z0\n            = SST \u2190    \u2212                                                     \u2212                                               \u2192\n            (ii)      x1 \u2212    2(SU)D(\u03b6i)U T ST SU\u03a3V T ST \u2190                  x1 + 2(SU)D(\u03b6i)U T ST SU\u03a3V T \u2212                   z0\n            = SST \u2190     \u2212                                              \u2212                                        \u2192\n                       x1 \u2212    2(SU)D(\u03b6i)U T U\u03a3U T ST \u2190               x1 + 2(SU)D(\u03b6i)U T U\u03a3U T \u2212                z0\n            = SST \u2190    \u2212                                     \u2212                                \u2192\n                      x1 \u2212    2(SU)D(\u03b6i)\u03a3U T ST \u2190           x1 + 2(SU)D(\u03b6i)\u03a3U T \u2212             z0\n            = SST \u2190    \u2212                                        \u2212                                    \u2192\n                      x1 \u2212    2SUD(\u03b6i)D(\u03c3)U T ST \u2190             x1 + 2SUD(\u03b6i)D(\u03c3)U T \u2212               z0\n            = SST \u2190    \u2212                                       \u2212                                 \u2192\n                      x1 \u2212    2SUD(\u03b6i \u2299         \u03c3)U T ST \u2190    x1 + 2SUD(\u03b6i \u2299            \u03c3)U T \u2212  z0,\n where (i) is due to Assumption 3.1 and (ii) uses Assumption 3.2. By choosing \u03b6j                            i as half the inverse\n of the non-zero singular values of (AS)T (AS), i.e., \u03b6j                 i = 1/2\u03c3i \u2200i = 1, . . . , k, we obtain\n                                      \u2190\u2212              \u2212                      \u2212                \u2192\n                                      x0 = SST \u2190     x1 \u2212    SUU T ST \u2190     x1 + SUU T \u2212      z0\n                                           = SST \u2190    \u2212             \u2212         \u2192       \u2192\n which completes the statement of the theorem.       x1 \u2212    SST \u2190 x1 + S\u2212    z0 = \u2212 x0,                                            \u25a1\n B.2     Proof of Proposition 3.3\n Proposition B.2 (Variational Autoencoder). Suppose Assumption 3.1 holds. For an encoder E :\n Rd \u2192     Rk and a decoder D : Rk \u2192               Rd, denote by L (\u03d5, \u03c9) the training objective of VAE:\n                                           \u2192         D(E(\u2212   \u2192                 \u2192   2\n             arg min                                         x0; \u03d5); \u03c9) \u2212      \u2212\n                   \u03d5,\u03c9 L (\u03d5, \u03c9) := E\u2212      x0\u223cp                                x0   2   + \u03bbKL (E\u266fp, N(0, Ik)) ,\n then the combination of E(\u2212         \u2192                 \u2192              \u2212              \u2212\n     6                               x0; \u03d5) = ST \u2212    x0 and D(\u2190     z0; \u03c9) = S\u2190    z0 is a minimizer of L (\u03d5, \u03c9).\n      We use the term \u2018step size\u2019 in a more general way than is normally used. In this case, the step size is a\n\u2018pre-conditioning\u2019 positive definite matrix, whose eigenvalue magnitudes correspond to the scalar step sizes per\n coordinate along an appropriately rotated basis. This general form is needed and with carefully selected (unique)\n eigenvalues; otherwise the DPS algorithm fails to converge to the groundtruth sample. We will later see that for\n our PSLD Algorithm in Theorem 3.6, we can revert to the commonly used notion of step size (a single scalar),\n as any finite step size (including a single scalar common across all coordinates) suffices for proving recovery.\n                                                                  16", "md": "# Document\n\nSuppose - $$x_0 \\sim p(-x_0)$$. Given measurements y = A- $$x_0$$ and a fixed variance $$\\beta \\in (0, 1)$$, there exists a matrix step size $$\\zeta = (1/2)(SU)D(\\zeta_i)(SU)^T$$, $$\\zeta_i = \\left\\{\\zeta_j \\, | \\, i = 1/\\sigma_j\\right\\}_{j=1}^k$$ for all the coordinates of $$-x_0$$ such that Algorithm 1 samples from the true posterior $$p(-x_0|y)$$ and exactly recovers the groundtruth $$x_0 = -x_0$$.\n\nProof. Our goal is to show that the reverse process starts with $$x_1 \\sim N(0, I_d)$$ and generates the following:\n\n$x_0 = \\peta^*x_1 - \\zeta \\nabla x_1 \\cdot (x_0(x_1) - y)$\n$= \\peta^*AS^T x_1 - \\zeta \\nabla x_1 \\cdot (x_1 - y)$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1 + 2\\zeta SST A^T y$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1 + 2\\zeta SST A^T A- x_0$\n$= SST^T x_1 - 2\\zeta SU\\sum V^T ST^T x_1 + 2\\zeta SU\\sum V^T -z_0$\n$= SST^T x_1 - 2\\zeta SU\\sum V^T ST^T x_1 + 2\\zeta SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)(SU)^T SU\\sum V^T ST^T x_1 + 2(SU)D(\\zeta_i)(SU)^T SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)U^T ST^T SU\\sum V^T ST^T x_1 + 2(SU)D(\\zeta_i)U^T ST^T SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)U^T U\\sum U^T ST^T x_1 + 2(SU)D(\\zeta_i)U^T U\\sum U^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)\\sum U^T ST^T x_1 + 2(SU)D(\\zeta_i)\\sum U^T -z_0$\n$= SST^T x_1 - 2SUD(\\zeta_i)D(\\sigma)U^T ST^T x_1 + 2SUD(\\zeta_i)D(\\sigma)U^T -z_0$\n$= SST^T x_1 - 2SUD(\\zeta_i \\odot \\sigma)U^T ST^T x_1 + 2SUD(\\zeta_i \\odot \\sigma)U^T -z_0$\n\nwhere (i) is due to Assumption 3.1 and (ii) uses Assumption 3.2. By choosing $$\\zeta_j^i$$ as half the inverse of the non-zero singular values of $$(AS)^T(AS)$$, i.e., $$\\zeta_j^i = 1/2\\sigma_i \\, \\forall i = 1, ..., k$$, we obtain\n\n$$x_0 = SST^T x_1 - SUU^T ST^T x_1 + SUU^T -z_0$$\n\nwhich completes the statement of the theorem. $$\\Box$$\n\nB.2 Proof of Proposition 3.3\n\nProposition B.2 (Variational Autoencoder). Suppose Assumption 3.1 holds. For an encoder $$E : \\mathbb{R}^d \\to \\mathbb{R}^k$$ and a decoder $$D : \\mathbb{R}^k \\to \\mathbb{R}^d$$, denote by $$L(\\phi, \\omega)$$ the training objective of VAE:\n\n$$\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{x_0 \\sim p(x_0)} ||D(E(-x_0; \\phi); \\omega) - x_0||_2^2 + \\lambda KL(E^{\\#}p, N(0, I_k))$$\n\nthen the combination of $$E(-x_0; \\phi) = ST-x_0$$ and $$D(-z_0; \\omega) = S-z_0$$ is a minimizer of $$L(\\phi, \\omega)$$.\n\nWe use the term 'step size' in a more general way than is normally used. In this case, the step size is a 'pre-conditioning' positive definite matrix, whose eigenvalue magnitudes correspond to the scalar step sizes per coordinate along an appropriately rotated basis. This general form is needed and with carefully selected (unique) eigenvalues; otherwise the DPS algorithm fails to converge to the groundtruth sample. We will later see that for our PSLD Algorithm in Theorem 3.6, we can revert to the commonly used notion of step size (a single scalar), as any finite step size (including a single scalar common across all coordinates) suffices for proving recovery.\n\n16", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Suppose - $$x_0 \\sim p(-x_0)$$. Given measurements y = A- $$x_0$$ and a fixed variance $$\\beta \\in (0, 1)$$, there exists a matrix step size $$\\zeta = (1/2)(SU)D(\\zeta_i)(SU)^T$$, $$\\zeta_i = \\left\\{\\zeta_j \\, | \\, i = 1/\\sigma_j\\right\\}_{j=1}^k$$ for all the coordinates of $$-x_0$$ such that Algorithm 1 samples from the true posterior $$p(-x_0|y)$$ and exactly recovers the groundtruth $$x_0 = -x_0$$.\n\nProof. Our goal is to show that the reverse process starts with $$x_1 \\sim N(0, I_d)$$ and generates the following:\n\n$x_0 = \\peta^*x_1 - \\zeta \\nabla x_1 \\cdot (x_0(x_1) - y)$\n$= \\peta^*AS^T x_1 - \\zeta \\nabla x_1 \\cdot (x_1 - y)$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1 + 2\\zeta SST A^T y$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1 + 2\\zeta SST A^T A- x_0$\n$= SST^T x_1 - 2\\zeta SU\\sum V^T ST^T x_1 + 2\\zeta SU\\sum V^T -z_0$\n$= SST^T x_1 - 2\\zeta SU\\sum V^T ST^T x_1 + 2\\zeta SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)(SU)^T SU\\sum V^T ST^T x_1 + 2(SU)D(\\zeta_i)(SU)^T SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)U^T ST^T SU\\sum V^T ST^T x_1 + 2(SU)D(\\zeta_i)U^T ST^T SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)U^T U\\sum U^T ST^T x_1 + 2(SU)D(\\zeta_i)U^T U\\sum U^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)\\sum U^T ST^T x_1 + 2(SU)D(\\zeta_i)\\sum U^T -z_0$\n$= SST^T x_1 - 2SUD(\\zeta_i)D(\\sigma)U^T ST^T x_1 + 2SUD(\\zeta_i)D(\\sigma)U^T -z_0$\n$= SST^T x_1 - 2SUD(\\zeta_i \\odot \\sigma)U^T ST^T x_1 + 2SUD(\\zeta_i \\odot \\sigma)U^T -z_0$\n\nwhere (i) is due to Assumption 3.1 and (ii) uses Assumption 3.2. By choosing $$\\zeta_j^i$$ as half the inverse of the non-zero singular values of $$(AS)^T(AS)$$, i.e., $$\\zeta_j^i = 1/2\\sigma_i \\, \\forall i = 1, ..., k$$, we obtain\n\n$$x_0 = SST^T x_1 - SUU^T ST^T x_1 + SUU^T -z_0$$\n\nwhich completes the statement of the theorem. $$\\Box$$\n\nB.2 Proof of Proposition 3.3\n\nProposition B.2 (Variational Autoencoder). Suppose Assumption 3.1 holds. For an encoder $$E : \\mathbb{R}^d \\to \\mathbb{R}^k$$ and a decoder $$D : \\mathbb{R}^k \\to \\mathbb{R}^d$$, denote by $$L(\\phi, \\omega)$$ the training objective of VAE:\n\n$$\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{x_0 \\sim p(x_0)} ||D(E(-x_0; \\phi); \\omega) - x_0||_2^2 + \\lambda KL(E^{\\#}p, N(0, I_k))$$\n\nthen the combination of $$E(-x_0; \\phi) = ST-x_0$$ and $$D(-z_0; \\omega) = S-z_0$$ is a minimizer of $$L(\\phi, \\omega)$$.\n\nWe use the term 'step size' in a more general way than is normally used. In this case, the step size is a 'pre-conditioning' positive definite matrix, whose eigenvalue magnitudes correspond to the scalar step sizes per coordinate along an appropriately rotated basis. This general form is needed and with carefully selected (unique) eigenvalues; otherwise the DPS algorithm fails to converge to the groundtruth sample. We will later see that for our PSLD Algorithm in Theorem 3.6, we can revert to the commonly used notion of step size (a single scalar), as any finite step size (including a single scalar common across all coordinates) suffices for proving recovery.\n\n16", "md": "Suppose - $$x_0 \\sim p(-x_0)$$. Given measurements y = A- $$x_0$$ and a fixed variance $$\\beta \\in (0, 1)$$, there exists a matrix step size $$\\zeta = (1/2)(SU)D(\\zeta_i)(SU)^T$$, $$\\zeta_i = \\left\\{\\zeta_j \\, | \\, i = 1/\\sigma_j\\right\\}_{j=1}^k$$ for all the coordinates of $$-x_0$$ such that Algorithm 1 samples from the true posterior $$p(-x_0|y)$$ and exactly recovers the groundtruth $$x_0 = -x_0$$.\n\nProof. Our goal is to show that the reverse process starts with $$x_1 \\sim N(0, I_d)$$ and generates the following:\n\n$x_0 = \\peta^*x_1 - \\zeta \\nabla x_1 \\cdot (x_0(x_1) - y)$\n$= \\peta^*AS^T x_1 - \\zeta \\nabla x_1 \\cdot (x_1 - y)$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1 + 2\\zeta SST A^T y$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1$\n$= SST^T x_1 - 2\\zeta SST A^T ASST^T x_1 + 2\\zeta SST A^T A- x_0$\n$= SST^T x_1 - 2\\zeta SU\\sum V^T ST^T x_1 + 2\\zeta SU\\sum V^T -z_0$\n$= SST^T x_1 - 2\\zeta SU\\sum V^T ST^T x_1 + 2\\zeta SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)(SU)^T SU\\sum V^T ST^T x_1 + 2(SU)D(\\zeta_i)(SU)^T SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)U^T ST^T SU\\sum V^T ST^T x_1 + 2(SU)D(\\zeta_i)U^T ST^T SU\\sum V^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)U^T U\\sum U^T ST^T x_1 + 2(SU)D(\\zeta_i)U^T U\\sum U^T -z_0$\n$= SST^T x_1 - 2(SU)D(\\zeta_i)\\sum U^T ST^T x_1 + 2(SU)D(\\zeta_i)\\sum U^T -z_0$\n$= SST^T x_1 - 2SUD(\\zeta_i)D(\\sigma)U^T ST^T x_1 + 2SUD(\\zeta_i)D(\\sigma)U^T -z_0$\n$= SST^T x_1 - 2SUD(\\zeta_i \\odot \\sigma)U^T ST^T x_1 + 2SUD(\\zeta_i \\odot \\sigma)U^T -z_0$\n\nwhere (i) is due to Assumption 3.1 and (ii) uses Assumption 3.2. By choosing $$\\zeta_j^i$$ as half the inverse of the non-zero singular values of $$(AS)^T(AS)$$, i.e., $$\\zeta_j^i = 1/2\\sigma_i \\, \\forall i = 1, ..., k$$, we obtain\n\n$$x_0 = SST^T x_1 - SUU^T ST^T x_1 + SUU^T -z_0$$\n\nwhich completes the statement of the theorem. $$\\Box$$\n\nB.2 Proof of Proposition 3.3\n\nProposition B.2 (Variational Autoencoder). Suppose Assumption 3.1 holds. For an encoder $$E : \\mathbb{R}^d \\to \\mathbb{R}^k$$ and a decoder $$D : \\mathbb{R}^k \\to \\mathbb{R}^d$$, denote by $$L(\\phi, \\omega)$$ the training objective of VAE:\n\n$$\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{x_0 \\sim p(x_0)} ||D(E(-x_0; \\phi); \\omega) - x_0||_2^2 + \\lambda KL(E^{\\#}p, N(0, I_k))$$\n\nthen the combination of $$E(-x_0; \\phi) = ST-x_0$$ and $$D(-z_0; \\omega) = S-z_0$$ is a minimizer of $$L(\\phi, \\omega)$$.\n\nWe use the term 'step size' in a more general way than is normally used. In this case, the step size is a 'pre-conditioning' positive definite matrix, whose eigenvalue magnitudes correspond to the scalar step sizes per coordinate along an appropriately rotated basis. This general form is needed and with carefully selected (unique) eigenvalues; otherwise the DPS algorithm fails to converge to the groundtruth sample. We will later see that for our PSLD Algorithm in Theorem 3.6, we can revert to the commonly used notion of step size (a single scalar), as any finite step size (including a single scalar common across all coordinates) suffices for proving recovery.\n\n16"}]}, {"page": 17, "text": "                                                           \u2192                     \u2192                                      \u2212                  \u2212\nProof. To show that the encoder E(\u2212                        x0; \u03d5) = ST \u2212         x0 and the decoder D(\u2190                z0; \u03c9) = S\u2190        z0 minimize\nthe VAE training objective L (\u03d5, \u03c9), we begin with the first part of the loss, which is also called    \u2192                \u2212                  \u2212\nreconstruction error Lrecon (\u03d5, \u03c9). Substituting E(\u2212                              \u2192                   x0 and D(\u2190       z0; \u03c9) = S\u2190        z0, we have\n                                                                                  x0; \u03d5) = ST \u2212\n                                                                     \u2192          D(E(\u2212     \u2192                    \u2192    2\n                                      Lrecon (\u03d5, \u03c9) := E\u2212                                 x0; \u03d5); \u03c9) \u2212         \u2212\n                                                                    x0\u223cp                                       x0    2\n                                                                    \u2192          D(ST \u2212     \u2192               \u2192    2\n                                                            = E\u2212                          x0; \u03c9) \u2212       \u2212\n                                                                   x0\u223cp                                  x0     2\n                                                                    \u2192          SST \u2212    \u2192         \u2192    2\n                                                            = E\u2212                        x0 \u2212     \u2212\n                                                                   x0\u223cp                          x0     2\nUsing the fact that \u2212          \u2192\n                              x0 lives in a linear subspace, we arrive at\n                                                                   \u2192          SST S\u2212      \u2192           \u2192    2\n                                     Lrecon (\u03d5, \u03c9) = E\u2212           x0\u223cp                    z0 \u2212     S\u2212z0     2\n                                                           (i)     \u2192                    S\u2212  \u2192           \u2192    2\n                                                           = E\u2212    z0\u223cN (0,Ik)              z0 \u2212     S\u2212 z0    2    = 0,\nwhere (i) is due to Assumption 3.1. Now, we analyze the distribution loss. Note that the KL-\ndivergence between two Gaussian distributions with moments (\u00b51, \u03c31) and (\u00b52, \u03c32) is given by\n                        KL (N        (\u00b51, \u03c31), N        (\u00b52, \u03c32)) = log           \u03c32 \u03c3       + \u03c32   1 + (\u00b51 \u2212        \u00b52)2     \u2212    1\n                                                                                       1                    2\u03c32  2                 2.\nSince E (x0) = ST x0 = ST Sz0 = z0, the distribution loss becomes:\n                        Ldist (\u03d5) := KL (E\u266fp, N                   (0, Ik)) = KL (N              (0, Ik), N       (0, Ik)) = 0.\nB.3       Proof of Theorem 3.4\nTheorem B.3 (Generative Modeling using Diffusion in Latent Space). Suppose Assumption 3.1\nholds. Let the optimal solution of the latent diffusion model be\n                                                  \u2192   \u2192      \u02dc      \u2212\u2192     \u2192     \u2192       \u2192               \u2212\u2192     \u2212\u2192     \u2192        2    .\n                        \u03b8\u2217   = arg min    \u03b8 E\u2212    z0,\u2212\u03f5        \u00b51    z1(\u2212  z0, \u2212\u03f5 ), \u2212   z0     \u2212   \u00b5\u03b8    z1     z0, \u2212\u03f5\n                                                        \u2212\u2192    \u2212\u2192     \u2192        := \u03b8\u2212   \u2192     \u2212\u2192     \u2192\nFor a fixed variance \u03b2 > 0, if \u00b5\u03b8                        z1    z0, \u2212\u03f5     \u221a   1       z1     z0, \u2212\u03f5      , then the closed-form solution is   \u2190 \u2212      :=\n\u03b8\u2217  \u2212= \u221a1 \u2212         \u03b2Ik, which after normalization by      \u2212\u2192                1\u2212\u03b2 and composition with the decoder D                            z0; \u03c9\nS\u2190 z0 recovers the true subspace of p                       x0    .\nProof. In latent diffusion models, the training is performed in the latent space of a pre-trained VAE. If\nthe VAE is chosen from Proposition 3.3, then the training objective becomes:\n                min       \u2192   \u2192      \u02dc\u00b51(\u2212  \u2192          \u2192       \u2192           \u2192                 \u2212\u2192          \u2192       \u2192       2\n                  \u03b8 E\u2212   x0,\u2212\u03f5              z1     E(\u2212 x0), \u2212\u03f5 ), E(\u2212     x0)      \u2212   \u00b5\u03b8     z1     E(\u2212 x0), \u2212\u03f5\n                        = E\u2212   \u2192   \u2192      \u02dc\u00b51(\u2212  \u2192     \u2212\u2192     \u2192       \u2192               \u2212\u2192     \u2212\u2192     \u2192        2\n                               z0,\u2212\u03f5             z1     z0, \u2212\u03f5 ), \u2212  z0     \u2212   \u00b5\u03b8     z1     z0, \u2212\u03f5\n                               \u2192   \u2192      \u2212 \u2192              \u2212\u2192    \u2212\u2192     \u2192        2            \u2192   \u2192      \u2212 \u2192          \u2192     \u2212\u2192     \u2192      2\n                        = E\u2212   z0,\u2212\u03f5       z0 \u2212     \u00b5\u03b8     z1     z0, \u2212\u03f5               = E\u2212   z0,\u2212\u03f5        z0 \u2212     \u03b8\u2212z1     z0, \u2212\u03f5\n                               \u2192   \u2192        \u2192            \u2212\u2192                            \u2192        2\n                        = E\u2212   z0,\u2212\u03f5      \u2212 z0 \u2212   k \u03b8    z0      1 \u2212    \u03b2 +         \u03b2\u2212\u03f5\n                                                        \u2212  \u2192               \u2212\u2192                      \u2192           2\n                        =        \u2212\u2192E                     z0,i \u2212     \u03b8T i    z0      1 \u2212    \u03b2 + \u2212\u03f5          \u03b2           ,\n                                 z0\u223cp            i=1\n                            \u2212\u2192\u03f5 \u223cN (0,Ik)\n                                                                             17", "md": "# Math Equations\n\nProof. To show that the encoder $$E(-x_0; \\phi) = ST^{-}x_0$$ and the decoder $$D(\\leftarrow z_0; \\omega) = S^{\\leftarrow}z_0$$ minimize the VAE training objective $$L(\\phi, \\omega)$$, we begin with the first part of the loss, which is also called reconstruction error $$L_{recon}(\\phi, \\omega)$$. Substituting $$E(-x_0; \\phi) = ST^{-}x_0$$ and $$D(\\leftarrow z_0; \\omega) = S^{\\leftarrow}z_0$$, we have:\n\n$$\n\\begin{align*}\nL_{recon}(\\phi, \\omega) &:= E_{x_0 \\sim p} \\left[ \\|D(E(-x_0; \\phi); \\omega) - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|D(ST^{-}x_0; \\omega) - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|SST^{-}x_0 - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|x_0 - x_0\\|^2 \\right] \\\\\n&= 0\n\\end{align*}\n$$\nUsing the fact that $$-x_0$$ lives in a linear subspace, we arrive at:\n\n$$\nL_{recon}(\\phi, \\omega) = E_{x_0 \\sim p} \\left[ \\|SST^{-}z_0 - S^{-}z_0\\|^2 \\right]\n$$\n(i) $$= E_{z_0 \\sim N(0, I_k)} \\left[ \\|z_0 - S^{-}z_0\\|^2 \\right] = 0$$, where (i) is due to Assumption 3.1. Now, we analyze the distribution loss. Note that the KL-divergence between two Gaussian distributions with moments $$(\\mu_1, \\sigma_1)$$ and $$(\\mu_2, \\sigma_2)$$ is given by:\n\n$$\nKL \\left( N(\\mu_1, \\sigma_1), N(\\mu_2, \\sigma_2) \\right) = \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2 - 1}{2\\sigma_2^2}\n$$\nSince $$E(x_0) = STx_0 = STSz_0 = z_0$$, the distribution loss becomes:\n\n$$\nL_{dist}(\\phi) := KL(E^{\\#}p, N(0, I_k)) = KL(N(0, I_k), N(0, I_k)) = 0\n$$\nB.3 Proof of Theorem 3.4\n\nTheorem B.3 (Generative Modeling using Diffusion in Latent Space). Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be:\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon), -z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right]\n$$\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} z_1 z_0, -\\epsilon \\sqrt{1 - z_1 z_0, -\\epsilon}$$, then the closed-form solution is $$\\theta^* = \\sqrt{1 - \\beta I_k}$$, which after normalization by $$1-\\beta$$ and composition with the decoder $$D(z_0; \\omega)$$ recovers the true subspace of $$p(x_0)$$.\n\nProof. In latent diffusion models, the training is performed in the latent space of a pre-trained VAE. If the VAE is chosen from Proposition 3.3, then the training objective becomes:\n\n$$\n\\begin{align*}\n&\\min_{\\theta} E_{x_0, -\\epsilon} \\left[ z_1 E(-x_0), -\\epsilon, E(-x_0) - \\mu_{\\theta} z_1 E(-x_0), -\\epsilon \\right] \\\\\n&= E_{z_0, -\\epsilon} \\left[ z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right] \\\\\n&= E_{z_0, -\\epsilon} \\left[ z_0 - \\theta^{-} z_1 z_0, -\\epsilon \\right] \\\\\n&= E_{z_0 \\sim p} \\left[ \\sum_{i=1}^{k} z_{0,i} - \\theta^T i z_0 \\sqrt{1 - \\beta} + \\beta -\\epsilon \\right] \\\\\n&= \\sum_{z_0 \\sim p} \\epsilon \\sim N(0, I_k)\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Proof. To show that the encoder $$E(-x_0; \\phi) = ST^{-}x_0$$ and the decoder $$D(\\leftarrow z_0; \\omega) = S^{\\leftarrow}z_0$$ minimize the VAE training objective $$L(\\phi, \\omega)$$, we begin with the first part of the loss, which is also called reconstruction error $$L_{recon}(\\phi, \\omega)$$. Substituting $$E(-x_0; \\phi) = ST^{-}x_0$$ and $$D(\\leftarrow z_0; \\omega) = S^{\\leftarrow}z_0$$, we have:\n\n$$\n\\begin{align*}\nL_{recon}(\\phi, \\omega) &:= E_{x_0 \\sim p} \\left[ \\|D(E(-x_0; \\phi); \\omega) - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|D(ST^{-}x_0; \\omega) - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|SST^{-}x_0 - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|x_0 - x_0\\|^2 \\right] \\\\\n&= 0\n\\end{align*}\n$$\nUsing the fact that $$-x_0$$ lives in a linear subspace, we arrive at:\n\n$$\nL_{recon}(\\phi, \\omega) = E_{x_0 \\sim p} \\left[ \\|SST^{-}z_0 - S^{-}z_0\\|^2 \\right]\n$$\n(i) $$= E_{z_0 \\sim N(0, I_k)} \\left[ \\|z_0 - S^{-}z_0\\|^2 \\right] = 0$$, where (i) is due to Assumption 3.1. Now, we analyze the distribution loss. Note that the KL-divergence between two Gaussian distributions with moments $$(\\mu_1, \\sigma_1)$$ and $$(\\mu_2, \\sigma_2)$$ is given by:\n\n$$\nKL \\left( N(\\mu_1, \\sigma_1), N(\\mu_2, \\sigma_2) \\right) = \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2 - 1}{2\\sigma_2^2}\n$$\nSince $$E(x_0) = STx_0 = STSz_0 = z_0$$, the distribution loss becomes:\n\n$$\nL_{dist}(\\phi) := KL(E^{\\#}p, N(0, I_k)) = KL(N(0, I_k), N(0, I_k)) = 0\n$$\nB.3 Proof of Theorem 3.4\n\nTheorem B.3 (Generative Modeling using Diffusion in Latent Space). Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be:\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon), -z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right]\n$$\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} z_1 z_0, -\\epsilon \\sqrt{1 - z_1 z_0, -\\epsilon}$$, then the closed-form solution is $$\\theta^* = \\sqrt{1 - \\beta I_k}$$, which after normalization by $$1-\\beta$$ and composition with the decoder $$D(z_0; \\omega)$$ recovers the true subspace of $$p(x_0)$$.\n\nProof. In latent diffusion models, the training is performed in the latent space of a pre-trained VAE. If the VAE is chosen from Proposition 3.3, then the training objective becomes:\n\n$$\n\\begin{align*}\n&\\min_{\\theta} E_{x_0, -\\epsilon} \\left[ z_1 E(-x_0), -\\epsilon, E(-x_0) - \\mu_{\\theta} z_1 E(-x_0), -\\epsilon \\right] \\\\\n&= E_{z_0, -\\epsilon} \\left[ z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right] \\\\\n&= E_{z_0, -\\epsilon} \\left[ z_0 - \\theta^{-} z_1 z_0, -\\epsilon \\right] \\\\\n&= E_{z_0 \\sim p} \\left[ \\sum_{i=1}^{k} z_{0,i} - \\theta^T i z_0 \\sqrt{1 - \\beta} + \\beta -\\epsilon \\right] \\\\\n&= \\sum_{z_0 \\sim p} \\epsilon \\sim N(0, I_k)\n\\end{align*}\n$$", "md": "Proof. To show that the encoder $$E(-x_0; \\phi) = ST^{-}x_0$$ and the decoder $$D(\\leftarrow z_0; \\omega) = S^{\\leftarrow}z_0$$ minimize the VAE training objective $$L(\\phi, \\omega)$$, we begin with the first part of the loss, which is also called reconstruction error $$L_{recon}(\\phi, \\omega)$$. Substituting $$E(-x_0; \\phi) = ST^{-}x_0$$ and $$D(\\leftarrow z_0; \\omega) = S^{\\leftarrow}z_0$$, we have:\n\n$$\n\\begin{align*}\nL_{recon}(\\phi, \\omega) &:= E_{x_0 \\sim p} \\left[ \\|D(E(-x_0; \\phi); \\omega) - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|D(ST^{-}x_0; \\omega) - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|SST^{-}x_0 - x_0\\|^2 \\right] \\\\\n&= E_{x_0 \\sim p} \\left[ \\|x_0 - x_0\\|^2 \\right] \\\\\n&= 0\n\\end{align*}\n$$\nUsing the fact that $$-x_0$$ lives in a linear subspace, we arrive at:\n\n$$\nL_{recon}(\\phi, \\omega) = E_{x_0 \\sim p} \\left[ \\|SST^{-}z_0 - S^{-}z_0\\|^2 \\right]\n$$\n(i) $$= E_{z_0 \\sim N(0, I_k)} \\left[ \\|z_0 - S^{-}z_0\\|^2 \\right] = 0$$, where (i) is due to Assumption 3.1. Now, we analyze the distribution loss. Note that the KL-divergence between two Gaussian distributions with moments $$(\\mu_1, \\sigma_1)$$ and $$(\\mu_2, \\sigma_2)$$ is given by:\n\n$$\nKL \\left( N(\\mu_1, \\sigma_1), N(\\mu_2, \\sigma_2) \\right) = \\log \\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2 - 1}{2\\sigma_2^2}\n$$\nSince $$E(x_0) = STx_0 = STSz_0 = z_0$$, the distribution loss becomes:\n\n$$\nL_{dist}(\\phi) := KL(E^{\\#}p, N(0, I_k)) = KL(N(0, I_k), N(0, I_k)) = 0\n$$\nB.3 Proof of Theorem 3.4\n\nTheorem B.3 (Generative Modeling using Diffusion in Latent Space). Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be:\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{z_0, -\\epsilon} \\left[ \\mu_1 z_1(-z_0, -\\epsilon), -z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right]\n$$\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} z_1 z_0, -\\epsilon \\sqrt{1 - z_1 z_0, -\\epsilon}$$, then the closed-form solution is $$\\theta^* = \\sqrt{1 - \\beta I_k}$$, which after normalization by $$1-\\beta$$ and composition with the decoder $$D(z_0; \\omega)$$ recovers the true subspace of $$p(x_0)$$.\n\nProof. In latent diffusion models, the training is performed in the latent space of a pre-trained VAE. If the VAE is chosen from Proposition 3.3, then the training objective becomes:\n\n$$\n\\begin{align*}\n&\\min_{\\theta} E_{x_0, -\\epsilon} \\left[ z_1 E(-x_0), -\\epsilon, E(-x_0) - \\mu_{\\theta} z_1 E(-x_0), -\\epsilon \\right] \\\\\n&= E_{z_0, -\\epsilon} \\left[ z_0 - \\mu_{\\theta} z_1 z_0, -\\epsilon \\right] \\\\\n&= E_{z_0, -\\epsilon} \\left[ z_0 - \\theta^{-} z_1 z_0, -\\epsilon \\right] \\\\\n&= E_{z_0 \\sim p} \\left[ \\sum_{i=1}^{k} z_{0,i} - \\theta^T i z_0 \\sqrt{1 - \\beta} + \\beta -\\epsilon \\right] \\\\\n&= \\sum_{z_0 \\sim p} \\epsilon \\sim N(0, I_k)\n\\end{align*}\n$$"}]}, {"page": 18, "text": "where \u03b8T    i denotes the ith row of matrix \u03b8. The solution of this regression problem is given by7\n\u03b8\u2217i = E  x0,\u03f5      z0      1 \u2212   \u03b2 + \u03f5       \u03b2       z0     1 \u2212    \u03b2 + \u03f5       \u03b2  T  \u22121     Ex0,\u03f5      z0,i    z0      1 \u2212   \u03b2 + \u03f5        \u03b2\n     = E x0,\u03f5      z0      1 \u2212   \u03b2 + \u03f5       \u03b2       z0     1 \u2212    \u03b2 + \u03f5       \u03b2  T  \u22121     Ex0,\u03f5      z0,i    z0      1 \u2212   \u03b2 + \u03f5        \u03b2\n     = E x0,\u03f5      E(x0)        1 \u2212    \u03b2 + \u03f5       \u03b2      E(x0)        1 \u2212    \u03b2 + \u03f5       \u03b2  T  \u22121     Ex0,\u03f5      E(x0)i       E(x0)         1 \u2212   \u03b2 + \u03f5       \u03b2\n     = E z0,\u03f5      E(Sz0)         1 \u2212   \u03b2 + \u03f5        \u03b2      E(Sz0)         1 \u2212    \u03b2 + \u03f5       \u03b2  T  \u22121     Ez0,\u03f5      E(Sz0)i        E(Sz0)          1 \u2212   \u03b2 + \u03f5    \u03b2\n     = E z0,\u03f5      ST Sz0         1 \u2212   \u03b2 + \u03f5       \u03b2       ST Sz0         1 \u2212   \u03b2 + \u03f5       \u03b2   T  \u22121     Ez0,\u03f5     (ST Sz0)i         ST Sz0         1 \u2212    \u03b2 + \u03f5    \u03b2\nUsing Assumption 3.1, the above expression simplifies to\n\u03b8\u2217i = E  z0,\u03f5      z0     1 \u2212    \u03b2 + \u03f5       \u03b2      z0      1 \u2212   \u03b2 + \u03f5        \u03b2  T  \u22121     Ez0,\u03f5     (z0)i      z0      1 \u2212   \u03b2 + \u03f5       \u03b2\n     = E z0,\u03f5   (1 \u2212    \u03b2)z0zT    0 + z0\u03f5T          \u03b2(1 \u2212      \u03b2) + \u03f5zT    0     \u03b2(1 \u2212     \u03b2) + \u03b2\u03f5\u03f5T  \u22121          Ez0,\u03f5     (z0)i      z0      1 \u2212   \u03b2 + \u03f5       \u03b2\n     =       (1 \u2212    \u03b2) E z0,\u03f5   z0zT  0    + Ez0,\u03f5    z0\u03f5T          \u03b2(1 \u2212     \u03b2) + E  z0,\u03f5    \u03f5zT 0       \u03b2(1 \u2212     \u03b2) + \u03b2 E   z0,\u03f5    \u03f5\u03f5T    \u22121\n                                                                    \u00d7 Ez0,\u03f5       (z0)i      z0     1 \u2212    \u03b2 + \u03f5       \u03b2\n     =       (1 \u2212    \u03b2)Ik + E    z0 [z0] E  \u03f5 [\u03f5]T       \u03b2(1 \u2212     \u03b2) + E   \u03f5 [\u03f5] Ez0 [z0]T        \u03b2(1 \u2212     \u03b2) + \u03b2Ik         \u22121\n                                                                    \u00d7 Ez0,\u03f5       (z0)i      z0     1 \u2212    \u03b2 + \u03f5       \u03b2      ,\nwhere the last step uses the fact that z0 and \u03f5 are independent Gaussian random vectors with zero\nmean and unit covariance. Simplifying further, we arrive at\n                          \u03b8\u2217i = [(1 \u2212       \u03b2)Ik + \u03b2Ik]\u22121 Ez0,\u03f5               (z0)i      z0     1 \u2212   \u03b2 + \u03f5        \u03b2\n                               = Ez0,\u03f5       (z0)i      z0      1 \u2212   \u03b2 + \u03f5       \u03b2\n                               = Ez0       (z0)iz0        1 \u2212    \u03b2    + Ez0,\u03f5       (z0)i\u03f5       \u03b2\n                               = Ez0       (z0)iz0        1 \u2212    \u03b2    + Ez0 [(z0)i] E\u03f5 [\u03f5]             \u03b2.\nThe final step follows from independence of z0 and \u03f5. Since z0 and \u03f5 are also N (0, Ik), we get\n                          \u03b8\u2217i = Ez0        (z0)iz0        1 \u2212    \u03b2    =     0, . . . , 0,    1 \u2212   \u03b2, 0, . . . , 0  T   ,\nwhere the ith coordinate is \u221a1 \u2212                    \u03b2 and zero everywhere else. Therefore, stacking all the rows\ntogether, we get \u03b8\u2217          = \u221a1 \u2212        \u03b2Ik, which after normalization by 1/\u221a1 \u2212                         \u03b2 gives the desired result.\nNext, we show that \u03b8\u2217                recovers the true subspace of \u2212                \u2192             \u2212\u2192\n                                                                                    x0 \u223c      p   x0    . When composed with the\ndecoder of VAE, the generator of the LDM gives \u2190                             x0\u2212= D          \u03b8\u2217\u2190  \u2212                    \u2212             \u2212\n\u2190\u2212                                                                                               z1     = D       Ik\u2190 z1     = S\u2190   z1. Since\nz1 \u223c     N (0, Ik), this completes the statement of the theorem.                                                                                 \u25a1\nB.4      Proof of Theorem 3.5\nRecall that the the latent-space GML-DPS (6) algorithm (based on the pixel-space DPS algorithm\n[11]) has three key steps. In the first step, it uses the normalized closed-form solution obtained in\n    7 For ease of notation, we drop the forward arrow in the rest of this proof.\n                                                                        18", "md": "where $$\\theta_{i}$$ denotes the ith row of matrix $$\\theta$$. The solution of this regression problem is given by:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= E[x_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}E[x_{0},\\epsilon]z_{0,i}z_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp;= E[x_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}E[x_{0},\\epsilon]E(x_{0})1 - \\beta + \\epsilon \\beta E(x_{0})^{T-1}Ex_{0},\\epsilon E(x_{0})iE(x_{0})1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}E(Sz_{0})1 - \\beta + \\epsilon \\beta E(Sz_{0})1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(ST Sz_{0})iST Sz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}ST Sz_{0}1 - \\beta + \\epsilon \\beta ST Sz_{0}1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(ST Sz_{0})iST Sz_{0}1 - \\beta + \\epsilon \\beta\n\\end{align*}\n$$\n\nUsing Assumption 3.1, the above expression simplifies to:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= E[z_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= (1 - \\beta)Ez_{0,\\epsilon}(1 - \\beta)z_{0}z_{0}^{T} + z_{0}\\epsilon^{T}\\beta(1 - \\beta) + \\epsilon z_{0}^{T}\\beta(1 - \\beta) + \\beta\\epsilon\\epsilon^{T-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= (1 - \\beta)Ez_{0,\\epsilon}z_{0}z_{0}^{T} + Ez_{0,\\epsilon}z_{0}\\epsilon^{T}\\beta(1 - \\beta) + Ez_{0,\\epsilon}\\epsilon z_{0}^{T}\\beta(1 - \\beta) + \\beta Ez_{0,\\epsilon}\\epsilon\\epsilon^{T-1}\n\\end{align*}\n$$\n\nwhere the last step uses the fact that $$z_{0}$$ and $$\\epsilon$$ are independent Gaussian random vectors with zero mean and unit covariance. Simplifying further, we arrive at:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= [(1 - \\beta)I_{k} + \\beta I_{k}]^{-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0}(z_{0})iz_{0}1 - \\beta + Ez_{0,\\epsilon}(z_{0})i\\epsilon\\beta \\\\\n&amp= Ez_{0}(z_{0})iz_{0}1 - \\beta + Ez_{0}[(z_{0})i]E\\epsilon[\\epsilon]\\beta\n\\end{align*}\n$$\n\nThe final step follows from independence of $$z_{0}$$ and $$\\epsilon$$. Since $$z_{0}$$ and $$\\epsilon$$ are also $$N(0, I_{k})$$, we get:\n\n$$\n\\theta^{*}_{i} = Ez_{0}(z_{0})iz_{0}1 - \\beta = 0, ..., 0, 1 - \\beta, 0, ..., 0^{T},\n$$\n\nwhere the ith coordinate is $$\\sqrt{1 - \\beta}$$ and zero everywhere else. Therefore, stacking all the rows together, we get $$\\theta^{*} = \\sqrt{1 - \\beta}I_{k}$$, which after normalization by $$1/\\sqrt{1 - \\beta}$$ gives the desired result.\n\nNext, we show that $$\\theta^{*}$$ recovers the true subspace of $$\\rightarrow x_{0} \\sim p(x_{0})$$. When composed with the decoder of VAE, the generator of the LDM gives $$\\leftarrow x_{0} = D\\theta^{*}\\leftarrow z = S\\leftarrow z$$. Since $$z$$ is $$N(0, I_{k})$$, this completes the statement of the theorem. &#9633;\n\nB.4 Proof of Theorem 3.5\n\nRecall that the latent-space GML-DPS (6) algorithm (based on the pixel-space DPS algorithm [11]) has three key steps. In the first step, it uses the normalized closed-form solution obtained in\n\n7 For ease of notation, we drop the forward arrow in the rest of this proof.\n\n18", "images": [], "items": [{"type": "text", "value": "where $$\\theta_{i}$$ denotes the ith row of matrix $$\\theta$$. The solution of this regression problem is given by:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= E[x_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}E[x_{0},\\epsilon]z_{0,i}z_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp;= E[x_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}E[x_{0},\\epsilon]E(x_{0})1 - \\beta + \\epsilon \\beta E(x_{0})^{T-1}Ex_{0},\\epsilon E(x_{0})iE(x_{0})1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}E(Sz_{0})1 - \\beta + \\epsilon \\beta E(Sz_{0})1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(ST Sz_{0})iST Sz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}ST Sz_{0}1 - \\beta + \\epsilon \\beta ST Sz_{0}1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(ST Sz_{0})iST Sz_{0}1 - \\beta + \\epsilon \\beta\n\\end{align*}\n$$\n\nUsing Assumption 3.1, the above expression simplifies to:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= E[z_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= (1 - \\beta)Ez_{0,\\epsilon}(1 - \\beta)z_{0}z_{0}^{T} + z_{0}\\epsilon^{T}\\beta(1 - \\beta) + \\epsilon z_{0}^{T}\\beta(1 - \\beta) + \\beta\\epsilon\\epsilon^{T-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= (1 - \\beta)Ez_{0,\\epsilon}z_{0}z_{0}^{T} + Ez_{0,\\epsilon}z_{0}\\epsilon^{T}\\beta(1 - \\beta) + Ez_{0,\\epsilon}\\epsilon z_{0}^{T}\\beta(1 - \\beta) + \\beta Ez_{0,\\epsilon}\\epsilon\\epsilon^{T-1}\n\\end{align*}\n$$\n\nwhere the last step uses the fact that $$z_{0}$$ and $$\\epsilon$$ are independent Gaussian random vectors with zero mean and unit covariance. Simplifying further, we arrive at:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= [(1 - \\beta)I_{k} + \\beta I_{k}]^{-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0}(z_{0})iz_{0}1 - \\beta + Ez_{0,\\epsilon}(z_{0})i\\epsilon\\beta \\\\\n&amp= Ez_{0}(z_{0})iz_{0}1 - \\beta + Ez_{0}[(z_{0})i]E\\epsilon[\\epsilon]\\beta\n\\end{align*}\n$$\n\nThe final step follows from independence of $$z_{0}$$ and $$\\epsilon$$. Since $$z_{0}$$ and $$\\epsilon$$ are also $$N(0, I_{k})$$, we get:\n\n$$\n\\theta^{*}_{i} = Ez_{0}(z_{0})iz_{0}1 - \\beta = 0, ..., 0, 1 - \\beta, 0, ..., 0^{T},\n$$\n\nwhere the ith coordinate is $$\\sqrt{1 - \\beta}$$ and zero everywhere else. Therefore, stacking all the rows together, we get $$\\theta^{*} = \\sqrt{1 - \\beta}I_{k}$$, which after normalization by $$1/\\sqrt{1 - \\beta}$$ gives the desired result.\n\nNext, we show that $$\\theta^{*}$$ recovers the true subspace of $$\\rightarrow x_{0} \\sim p(x_{0})$$. When composed with the decoder of VAE, the generator of the LDM gives $$\\leftarrow x_{0} = D\\theta^{*}\\leftarrow z = S\\leftarrow z$$. Since $$z$$ is $$N(0, I_{k})$$, this completes the statement of the theorem. &#9633;\n\nB.4 Proof of Theorem 3.5\n\nRecall that the latent-space GML-DPS (6) algorithm (based on the pixel-space DPS algorithm [11]) has three key steps. In the first step, it uses the normalized closed-form solution obtained in\n\n7 For ease of notation, we drop the forward arrow in the rest of this proof.\n\n18", "md": "where $$\\theta_{i}$$ denotes the ith row of matrix $$\\theta$$. The solution of this regression problem is given by:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= E[x_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}E[x_{0},\\epsilon]z_{0,i}z_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp;= E[x_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}E[x_{0},\\epsilon]E(x_{0})1 - \\beta + \\epsilon \\beta E(x_{0})^{T-1}Ex_{0},\\epsilon E(x_{0})iE(x_{0})1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}E(Sz_{0})1 - \\beta + \\epsilon \\beta E(Sz_{0})1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(ST Sz_{0})iST Sz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}ST Sz_{0}1 - \\beta + \\epsilon \\beta ST Sz_{0}1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(ST Sz_{0})iST Sz_{0}1 - \\beta + \\epsilon \\beta\n\\end{align*}\n$$\n\nUsing Assumption 3.1, the above expression simplifies to:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= E[z_{0},\\epsilon]z_{0}1 - \\beta + \\epsilon \\beta z_{0}1 - \\beta + \\epsilon \\beta^{T-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= (1 - \\beta)Ez_{0,\\epsilon}(1 - \\beta)z_{0}z_{0}^{T} + z_{0}\\epsilon^{T}\\beta(1 - \\beta) + \\epsilon z_{0}^{T}\\beta(1 - \\beta) + \\beta\\epsilon\\epsilon^{T-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= (1 - \\beta)Ez_{0,\\epsilon}z_{0}z_{0}^{T} + Ez_{0,\\epsilon}z_{0}\\epsilon^{T}\\beta(1 - \\beta) + Ez_{0,\\epsilon}\\epsilon z_{0}^{T}\\beta(1 - \\beta) + \\beta Ez_{0,\\epsilon}\\epsilon\\epsilon^{T-1}\n\\end{align*}\n$$\n\nwhere the last step uses the fact that $$z_{0}$$ and $$\\epsilon$$ are independent Gaussian random vectors with zero mean and unit covariance. Simplifying further, we arrive at:\n\n$$\n\\begin{align*}\n\\theta^{*}_{i} &amp;= [(1 - \\beta)I_{k} + \\beta I_{k}]^{-1}Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0,\\epsilon}(z_{0})iz_{0}1 - \\beta + \\epsilon \\beta \\\\\n&amp= Ez_{0}(z_{0})iz_{0}1 - \\beta + Ez_{0,\\epsilon}(z_{0})i\\epsilon\\beta \\\\\n&amp= Ez_{0}(z_{0})iz_{0}1 - \\beta + Ez_{0}[(z_{0})i]E\\epsilon[\\epsilon]\\beta\n\\end{align*}\n$$\n\nThe final step follows from independence of $$z_{0}$$ and $$\\epsilon$$. Since $$z_{0}$$ and $$\\epsilon$$ are also $$N(0, I_{k})$$, we get:\n\n$$\n\\theta^{*}_{i} = Ez_{0}(z_{0})iz_{0}1 - \\beta = 0, ..., 0, 1 - \\beta, 0, ..., 0^{T},\n$$\n\nwhere the ith coordinate is $$\\sqrt{1 - \\beta}$$ and zero everywhere else. Therefore, stacking all the rows together, we get $$\\theta^{*} = \\sqrt{1 - \\beta}I_{k}$$, which after normalization by $$1/\\sqrt{1 - \\beta}$$ gives the desired result.\n\nNext, we show that $$\\theta^{*}$$ recovers the true subspace of $$\\rightarrow x_{0} \\sim p(x_{0})$$. When composed with the decoder of VAE, the generator of the LDM gives $$\\leftarrow x_{0} = D\\theta^{*}\\leftarrow z = S\\leftarrow z$$. Since $$z$$ is $$N(0, I_{k})$$, this completes the statement of the theorem. &#9633;\n\nB.4 Proof of Theorem 3.5\n\nRecall that the latent-space GML-DPS (6) algorithm (based on the pixel-space DPS algorithm [11]) has three key steps. In the first step, it uses the normalized closed-form solution obtained in\n\n7 For ease of notation, we drop the forward arrow in the rest of this proof.\n\n18"}]}, {"page": 19, "text": " Theorem 3.4 to perform one step of denoising by the reverse SDE. In the second step, it runs one\n step of gradient descent to satisfy the measurements in the pixel space. Finally, it takes one step of\n gradient descent on the goodness objective, which acts as a regularizer to ensure that the reconstructed\n image lies on the data manifold.\n This can be formalized as:\n                                         \u2190\u2212           \u2212             \u2212   AD(\u2190     \u2212    \u2212            2\n                                         z\u20320 = \u03b8\u2217\u2190    z1 \u2212    \u03b7\u2207\u2190  z1           z0(\u2190 z1)) \u2212     y   2 ;                                     (8)\n                                                                \u2190\u2212               \u2190\u2212      2\n                                         \u2190\u2212                     z\u2032               z\u2032\n                                         z0 = arg min    \u2190\u2212       0 \u2212    E(D(      0))    2 ,                                               (9)\n                                                         z\u2032\n                                                          0\n In practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis\n however, we analyze the exact system of equations above, as (9) has a closed-form solution in the\n linear setting.\n Theorem B.4 (Posterior Sampling using Goodness Modified Latent DPS). Suppose Assumptions 3.1\n and Assumption 3.2 hold. Denote by \u03c3 = {\u03c3j}k                                j=1 the singular values of (AS)T (AS), i.e.,\n(AS)T (AS) = U\u03a3U T := UD(\u03c3)U T , U \u2208                           \u2212    Rk\u00d7k, and let               \u2212     \u2212\n                                              \u2192   \u2192     \u02dc      \u2192     \u2192    \u2192      \u2192               \u2192     \u2192    \u2192       2\n Suppose \u2212    \u2192        \u03b8\u2217 \u2192= arg min   \u03b8 E\u2212   z0,\u2212\u03f5      \u00b51    z1(\u2212  z0, \u2212\u03f5\u2192), \u2212 z0    \u2212   \u00b5\u03b8    z1    z0, \u2212\u03f5        2   .\n              x0 \u223c     p(\u2212x0). Given measurements y = A\u2212                   x0 and any fixed variance \u03b2 \u2208                 (0, 1), then with\n the (unique) step size \u03b7 = (1/2)UD(\u03b7i)U T , \u03b7i = {\u03b7j                          i = 1/2\u03c3j}k       j=1, the GML-DPS algorithm (6)\n samples from the true posterior p(\u2212              \u2192                                                                                 \u2212       \u2192\n                                                  x0|y) and exactly recovers the groundtruth sample, i.e., \u2190                       x0 = \u2212   x0.\n Proof. We start with the measurement consistency update (8) and then show that the solution obtained\n from (8) is already a minimizer of (9). Therefore, we have\n                                      \u2190\u2212            \u2212            \u2212   AD(\u2190     \u2212    \u2212             2\n                                      z\u20320 = \u03b8\u2217\u2190    z1 \u2212    \u03b7\u2207\u2190  z1           z0(\u2190 z1)) \u2212     y    2\n                                           = Ik\u2190   \u2212             \u2212   AD(Ik\u2190      \u2212           2\n                                                  z1 \u2212     \u03b7\u2207\u2190  z1               z1) \u2212    y   2\n                                           = \u2190  \u2212            \u2212   AS\u2190    \u2212           2\n                                               z1 \u2212    \u03b7\u2207\u2190  z1         z1) \u2212     y   2\n                                           = \u2190  \u2212            \u2212   AS\u2190    \u2212           2\n                                               z1 \u2212    \u03b7\u2207\u2190  z1         z1) \u2212     y   2\n                                           (i)  \u2212            \u2212    AS\u2190    \u2212         2\n                                           = \u2190 z1 \u2212    \u03b7\u2207\u2190   z1         z1 \u2212    y   2\n                                           = \u2190  \u2212                             \u2212\n                                               z1 \u2212    2\u03b7ST AT  AS\u2190          z1 \u2212    y\n                                           = \u2190  \u2212                          \u2212\n                                               z1 \u2212    2\u03b7ST AT AS\u2190        z1 + 2\u03b7ST AT y\n                                           = \u2190  \u2212                          \u2212                        \u2192\n                                               z1 \u2212    2\u03b7ST AT AS\u2190        z1 + 2\u03b7ST AT A\u2212          x0\n                                           = \u2190  \u2212                          \u2212                          \u2192\n                                               z1 \u2212    2\u03b7ST AT AS\u2190        z1 + 2\u03b7ST AT AS\u2212            z0,\n where (i) is due to Assumption 3.1. By Assumption 3.2, (AS)T (AS) is a positive definite matrix\n and can be written as U\u03a3U T :\n                          \u2190\u2212        \u2212                       \u2212                      \u2192\n                          z\u20320 = \u2190  z1 \u2212    2\u03b7U\u03a3U T \u2190       z1 + 2\u03b7U\u03a3U T \u2212         z0\n                               = \u2190  \u2212                                       \u2212                                      \u2192\n                                   z1 \u2212    2UD(\u03b7i)U T U\u03a3U T \u2190              z1 + 2UD(\u03b7i)U T U\u03a3U T \u2212                 z0\n                               = \u2190  \u2212                              \u2212                             \u2192\n                                   z1 \u2212    2UD(\u03b7i)\u03a3U T \u2190          z1 + 2UD(\u03b7i)\u03a3U T \u2212             z0\n                               = \u2190  \u2212                                    \u2212                                    \u2192\n                                   z1 \u2212    2UD(\u03b7i)D(\u03c3)U T \u2190             z1 + 2UD(\u03b7i)D(\u03c3)U T \u2212                 z0\n                               = \u2190  \u2212                                  \u2212                                  \u2192\n                                   z1 \u2212    2UD(\u03b7i \u2299          \u03c3)U T \u2190  z1 + 2UD(\u03b7i \u2299             \u03c3)U T \u2212   z0.\n Since \u03b7i  j = 1/2\u03c3j, the above expression further simplifies to\n                                            \u2190\u2212        \u2212                \u2212               \u2192        \u2192\n Next, we show that \u2190          \u2212            z\u20320 = \u2190  z1 \u2212    UU T \u2190   z1 + UU T \u2212      z0 = \u2212   z0.\n                              z\u20320 is already a minimizer of (9). This is a direct consequence of the encoder-\n                                                          \u2212                  \u2212       \u2212                  \u2190\u2212               \u2212      2\n decoder architecture of the VAE: E(D(\u2190                  z\u2032                 z\u2032      z\u2032                  z\u20320 \u2212   E(D(\u21900))z\u2032         = 0, and\n                                                           0)) = ST S\u21900 = \u21900. Hence,\n                                                                       19", "md": "Theorem 3.4 to perform one step of denoising by the reverse SDE. In the second step, it runs one\nstep of gradient descent to satisfy the measurements in the pixel space. Finally, it takes one step of\ngradient descent on the goodness objective, which acts as a regularizer to ensure that the reconstructed\nimage lies on the data manifold.\n\nThis can be formalized as:\n\n$$\nz'_0 = \\theta^*z_1 - \\eta \\nabla z_1 z_0(\\leftarrow z_1) - y^2; \\quad (8)\n$$\n\n$$\nz_0 = \\arg \\min_{z'_0} \\|z'_0 - E(D(z'_0))\\|^2; \\quad (9)\n$$\n\nIn practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis\nhowever, we analyze the exact system of equations above, as (9) has a closed-form solution in the\nlinear setting.\n\nTheorem B.4 (Posterior Sampling using Goodness Modified Latent DPS). Suppose Assumptions 3.1\nand Assumption 3.2 hold. Denote by $\\sigma = \\{\\sigma_j\\}_{j=1}^k$ the singular values of $(AS)^T(AS)$, i.e.,\n$(AS)^T(AS) = U\\Sigma U^T := UD(\\sigma)U^T, U \\in \\mathbb{R}^{k \\times k}$, and let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{z_0, \\epsilon}[\\mu_1 z_1(\\leftarrow z_0, \\epsilon) - z_0 - \\mu \\theta z_1 z_0, \\epsilon]^2.\n$$\n\nIf $x_0 \\sim p(x_0)$, given measurements $y = A x_0$ and any fixed variance $\\beta \\in (0, 1)$, then with\nthe (unique) step size $\\eta = \\frac{1}{2}UD(\\eta_i)U^T, \\eta_i = \\{\\eta_j^{i} = \\frac{1}{2\\sigma_j}\\}_{j=1}^k$, the GML-DPS algorithm (6)\nsamples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = \\hat{x}_0$.\n\nProof. We start with the measurement consistency update (8) and then show that the solution obtained\nfrom (8) is already a minimizer of (9). Therefore, we have\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(\\leftarrow z_1) - y^2 \\\\\n& = I_k z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 - y \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T y \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T A x_0 \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T A S z_0,\n\\end{align*}\n$$\n\nwhere (i) is due to Assumption 3.1. By Assumption 3.2, $(AS)^T(AS)$ is a positive definite matrix\nand can be written as $U\\Sigma U^T$:\n\n$$\n\\begin{align*}\nz'_0 & = z_1 - 2\\eta U\\Sigma U^T z_1 + 2\\eta U\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)U^T U\\Sigma U^T z_1 + 2UD(\\eta_i)U^T U\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)\\Sigma U^T z_1 + 2UD(\\eta_i)\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)D(\\sigma)U^T z_1 + 2UD(\\eta_i)D(\\sigma)U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i \\odot \\sigma)U^T z_1 + 2UD(\\eta_i \\odot \\sigma)U^T z_0.\n\\end{align*}\n$$\n\nSince $\\eta_i^{j} = \\frac{1}{2\\sigma_j}$, the above expression further simplifies to\n\n$$\nz'_0 = z_1 - UU^T z_1 + UU^T z_0 = \\hat{z}_0.\n$$\n\n$z'_0$ is already a minimizer of (9). This is a direct consequence of the encoder-decoder architecture of the VAE: $E(D(z'))z' = 0$, and\n$E(D(z')) = S^T S z' = z'$. Hence,", "images": [], "items": [{"type": "text", "value": "Theorem 3.4 to perform one step of denoising by the reverse SDE. In the second step, it runs one\nstep of gradient descent to satisfy the measurements in the pixel space. Finally, it takes one step of\ngradient descent on the goodness objective, which acts as a regularizer to ensure that the reconstructed\nimage lies on the data manifold.\n\nThis can be formalized as:\n\n$$\nz'_0 = \\theta^*z_1 - \\eta \\nabla z_1 z_0(\\leftarrow z_1) - y^2; \\quad (8)\n$$\n\n$$\nz_0 = \\arg \\min_{z'_0} \\|z'_0 - E(D(z'_0))\\|^2; \\quad (9)\n$$\n\nIn practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis\nhowever, we analyze the exact system of equations above, as (9) has a closed-form solution in the\nlinear setting.\n\nTheorem B.4 (Posterior Sampling using Goodness Modified Latent DPS). Suppose Assumptions 3.1\nand Assumption 3.2 hold. Denote by $\\sigma = \\{\\sigma_j\\}_{j=1}^k$ the singular values of $(AS)^T(AS)$, i.e.,\n$(AS)^T(AS) = U\\Sigma U^T := UD(\\sigma)U^T, U \\in \\mathbb{R}^{k \\times k}$, and let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{z_0, \\epsilon}[\\mu_1 z_1(\\leftarrow z_0, \\epsilon) - z_0 - \\mu \\theta z_1 z_0, \\epsilon]^2.\n$$\n\nIf $x_0 \\sim p(x_0)$, given measurements $y = A x_0$ and any fixed variance $\\beta \\in (0, 1)$, then with\nthe (unique) step size $\\eta = \\frac{1}{2}UD(\\eta_i)U^T, \\eta_i = \\{\\eta_j^{i} = \\frac{1}{2\\sigma_j}\\}_{j=1}^k$, the GML-DPS algorithm (6)\nsamples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = \\hat{x}_0$.\n\nProof. We start with the measurement consistency update (8) and then show that the solution obtained\nfrom (8) is already a minimizer of (9). Therefore, we have\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(\\leftarrow z_1) - y^2 \\\\\n& = I_k z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 - y \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T y \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T A x_0 \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T A S z_0,\n\\end{align*}\n$$\n\nwhere (i) is due to Assumption 3.1. By Assumption 3.2, $(AS)^T(AS)$ is a positive definite matrix\nand can be written as $U\\Sigma U^T$:\n\n$$\n\\begin{align*}\nz'_0 & = z_1 - 2\\eta U\\Sigma U^T z_1 + 2\\eta U\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)U^T U\\Sigma U^T z_1 + 2UD(\\eta_i)U^T U\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)\\Sigma U^T z_1 + 2UD(\\eta_i)\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)D(\\sigma)U^T z_1 + 2UD(\\eta_i)D(\\sigma)U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i \\odot \\sigma)U^T z_1 + 2UD(\\eta_i \\odot \\sigma)U^T z_0.\n\\end{align*}\n$$\n\nSince $\\eta_i^{j} = \\frac{1}{2\\sigma_j}$, the above expression further simplifies to\n\n$$\nz'_0 = z_1 - UU^T z_1 + UU^T z_0 = \\hat{z}_0.\n$$\n\n$z'_0$ is already a minimizer of (9). This is a direct consequence of the encoder-decoder architecture of the VAE: $E(D(z'))z' = 0$, and\n$E(D(z')) = S^T S z' = z'$. Hence,", "md": "Theorem 3.4 to perform one step of denoising by the reverse SDE. In the second step, it runs one\nstep of gradient descent to satisfy the measurements in the pixel space. Finally, it takes one step of\ngradient descent on the goodness objective, which acts as a regularizer to ensure that the reconstructed\nimage lies on the data manifold.\n\nThis can be formalized as:\n\n$$\nz'_0 = \\theta^*z_1 - \\eta \\nabla z_1 z_0(\\leftarrow z_1) - y^2; \\quad (8)\n$$\n\n$$\nz_0 = \\arg \\min_{z'_0} \\|z'_0 - E(D(z'_0))\\|^2; \\quad (9)\n$$\n\nIn practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis\nhowever, we analyze the exact system of equations above, as (9) has a closed-form solution in the\nlinear setting.\n\nTheorem B.4 (Posterior Sampling using Goodness Modified Latent DPS). Suppose Assumptions 3.1\nand Assumption 3.2 hold. Denote by $\\sigma = \\{\\sigma_j\\}_{j=1}^k$ the singular values of $(AS)^T(AS)$, i.e.,\n$(AS)^T(AS) = U\\Sigma U^T := UD(\\sigma)U^T, U \\in \\mathbb{R}^{k \\times k}$, and let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{z_0, \\epsilon}[\\mu_1 z_1(\\leftarrow z_0, \\epsilon) - z_0 - \\mu \\theta z_1 z_0, \\epsilon]^2.\n$$\n\nIf $x_0 \\sim p(x_0)$, given measurements $y = A x_0$ and any fixed variance $\\beta \\in (0, 1)$, then with\nthe (unique) step size $\\eta = \\frac{1}{2}UD(\\eta_i)U^T, \\eta_i = \\{\\eta_j^{i} = \\frac{1}{2\\sigma_j}\\}_{j=1}^k$, the GML-DPS algorithm (6)\nsamples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = \\hat{x}_0$.\n\nProof. We start with the measurement consistency update (8) and then show that the solution obtained\nfrom (8) is already a minimizer of (9). Therefore, we have\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(\\leftarrow z_1) - y^2 \\\\\n& = I_k z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - \\eta \\nabla z_1 z_1 - y^2 \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 - y \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T y \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T A x_0 \\\\\n& = z_1 - 2\\eta S^T A^T A S z_1 + 2\\eta S^T A^T A S z_0,\n\\end{align*}\n$$\n\nwhere (i) is due to Assumption 3.1. By Assumption 3.2, $(AS)^T(AS)$ is a positive definite matrix\nand can be written as $U\\Sigma U^T$:\n\n$$\n\\begin{align*}\nz'_0 & = z_1 - 2\\eta U\\Sigma U^T z_1 + 2\\eta U\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)U^T U\\Sigma U^T z_1 + 2UD(\\eta_i)U^T U\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)\\Sigma U^T z_1 + 2UD(\\eta_i)\\Sigma U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i)D(\\sigma)U^T z_1 + 2UD(\\eta_i)D(\\sigma)U^T z_0 \\\\\n& = z_1 - 2UD(\\eta_i \\odot \\sigma)U^T z_1 + 2UD(\\eta_i \\odot \\sigma)U^T z_0.\n\\end{align*}\n$$\n\nSince $\\eta_i^{j} = \\frac{1}{2\\sigma_j}$, the above expression further simplifies to\n\n$$\nz'_0 = z_1 - UU^T z_1 + UU^T z_0 = \\hat{z}_0.\n$$\n\n$z'_0$ is already a minimizer of (9). This is a direct consequence of the encoder-decoder architecture of the VAE: $E(D(z'))z' = 0$, and\n$E(D(z')) = S^T S z' = z'$. Hence,"}]}, {"page": 20, "text": " consequently \u2190      \u2212        \u2212            \u2212     \u2190\u2212               \u2212      2      \u2192\n                     z0 = \u2190  z\u20320 \u2212   \u03b3\u2207\u2190  z\u20320    z\u20320 \u2212   E(D(\u21900))z\u2032         = \u2212 z0. Thus, the reconstructed sample becomes\n \u2190\u2212            \u2212          \u2192        \u2192\n x0 = D(\u2190     z0) = S\u2212    z0 = \u2212   x0.\n                           \u2190\u2212               \u2212       2                   \u2212\n Furthermore, as           z\u20320 \u2212   E(D(\u2190   z\u20320))       = 0 for all \u2190   z\u20320, it is evident that the goodness objective cannot\n rectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6)\n requires the exact step size to sample from the posterior.                                                                                  \u25a1\n Beyond the linear setting, we also refer to Table 5 for experiments supporting this result.\n B.5     Proof of Theorem 3.6\n Different from GML-DPS, PSLD Algorithm 2 replaces the goodness objective (6) with the gluing\n objective (7), which can be formalized as:\n                             \u2190\u2212           \u2212            \u2212   AD(\u2190      \u2212    \u2212            2\n                             z\u20320 = \u03b8\u2217\u2190    z1 \u2212    \u03b7\u2207\u2190  z1           z0(\u2190 z1)) \u2212     y   2 ;                                               (10)\n                             \u2190\u2212                     \u2190\u2212                  \u2192                               \u2190\u2212      2\n                             z0 = arg min    \u2190\u2212     z\u20320 \u2212    E(AT A\u2212    z0 + (Id \u2212        AT A)D(z\u2032       0))    2 .                      (11)\n                                             z\u2032\n                                              0\nWe again remind that solving the minimization problem (11) is hard in general, and can be approxi-\n mated by gradient descent as typically followed in practice [11]. However, in a linear model setting,\n (11) has a closed-form solution which we derive to prove exact recovery.\n Theorem B.5 (Posterior Sampling using Diffusion in Latent Space). Let Assumptions 3.1 and 3.2\n hold. Let \u03c3j, \u2200j = 1, . . . , r denote the singular values of (AS)T (AS) and let\n                                                               \u2212                                \u2212     \u2212\n                                              \u2192   \u2192     \u02dc      \u2192     \u2192    \u2192      \u2192               \u2192     \u2192    \u2192       2\n                       \u03b8\u2217  = arg min   \u03b8 E\u2212   z0,\u2212\u03f5      \u00b51    z1(\u2212  z0, \u2212\u03f5 ), \u2212 z0    \u2212   \u00b5\u03b8    z1    z0, \u2212\u03f5            .\n              \u2192           \u2192                                              \u2192\n Suppose \u2212   x0 \u223c     p(\u2212x0). Given measurements y = A\u2212                  x0, any fixed variance \u03b2 \u2208            (0, 1), and any positive\n                                                                                                                                  \u2192\n step sizes \u03b7j   i , j = 1, 2, . . . , r, the PSLD Algorithm 2 samples from the true posterior p(\u2212                               x0|y) and\n exactly recovers the groundtruth sample, i.e., \u2190                 x0\u2212= \u2212    \u2192\n                                                                           x0.\n Proof. Following the proof in Appendix B.4, we have\n                                      \u2190\u2212            \u2212            \u2212   AD(\u2190     \u2212    \u2212             2\n                                       z\u20320 = \u03b8\u2217\u2190   z1 \u2212    \u03b7\u2207\u2190  z1           z0(\u2190 z1)) \u2212     y    2\n                                           = Ik\u2190    \u2212            \u2212   AD(\u2190     \u2212           2\n                                                   z1 \u2212    \u03b7\u2207\u2190  z1           z1) \u2212    y    2\n                                           = \u2190  \u2212            \u2212   AS\u2190    \u2212          2\n                                               z1 \u2212    \u03b7\u2207\u2190  z1         z1 \u2212     y   2\n                                           = \u2190  \u2212                            \u2212\n                                               z1 \u2212    2\u03b7ST AT (AS\u2190         z1 \u2212    y)\n                                           = \u2190  \u2212                          \u2212                          \u2192\n                                               z1 \u2212    2\u03b7ST AT AS\u2190        z1 + 2\u03b7ST AT AS\u2212            z0\n                                           = \u2190  \u2212                          \u2212                          \u2192\n                                               z1 \u2212    2\u03b7ST AT AS\u2190        z1 + 2\u03b7ST AT AS\u2212            z0.\nWe use the above expression to derive a closed-form solution to the minimization problem (11):\n                       \u2212    \u2190\u2212                       \u2192                            \u2190\u2212     2\n            0 = \u2207\u2190    z\u20320   z\u20320 \u2212   ST (AT AS\u2212       z0 + (Id \u2212        AT A)Sz\u2032      0)   2 2\n                       \u2212    \u2190\u2212                      \u2192                                 \u2190\u2212\n                = \u2207\u2190  z\u20320   z\u20320 \u2212   ST AT AS\u2212       z0 \u2212    ST (Id \u2212      AT A)Sz\u2032      0)   2   2\n                       \u2212    \u2190\u2212                      \u2192              \u2190\u2212                     \u2190\u2212\n                = \u2207\u2190  z\u20320   z\u20320 \u2212   ST AT AS\u2212       z0 \u2212    ST Sz\u2032   0 \u2212   ST AT AS        z\u20320)  22\n                       \u2212    \u2190\u2212                      \u2192              \u2190\u2212                     \u2190\u2212\n                = \u2207\u2190  z\u20320   z\u20320 \u2212   ST AT AS\u2212       z0 \u2212    ST Sz\u2032   0 + ST AT AS          z\u20320)   2\n                                                             \u2190\u2212                       \u2192               \u2190\u2212                     \u2190\u2212\n                = 2    Ik \u2212    ST S + ST AT AS                 z\u20320 \u2212   ST AT AS\u2212      z0 \u2212    ST Sz\u2032    0 + ST AT AS         z\u20320)\n                                                      \u2190\u2212                      \u2192\n                = 2ST AT AS            ST AT ASz\u2032       0 \u2212    ST AT AS\u2212      z0)     ,\n where the last step is due to Assumption 3.1. Thus, we have\n                                                \u2190\u2212                                                  \u2190\u2212       2     \u2192\n                         \u2190\u2212                                         \u2192                                              z0,\n                         z0 = arg min    \u2190\u2212     z\u20320 \u2212    E(AT A\u2212    z0 + (Id \u2212        AT A)D(z\u2032       0))    2 = \u2212\n                                         z\u2032\n                                          0\n                                                                       20", "md": "# Math Equations in HTML\n\nconsequently $$z_0 = z'_0 - \\gamma \\nabla z'_0 = z'_0 - E(D(z_0))z' = -z_0.$$ Thus, the reconstructed sample becomes $$x_0 = D(z_0) = S^{-1}z_0 = -x_0.$$\n\nFurthermore, as $$z'_0 - E(D(z'_0)) = 0$$ for all $$z'_0$$, it is evident that the goodness objective cannot rectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6) requires the exact step size to sample from the posterior.\n\nBeyond the linear setting, we also refer to Table 5 for experiments supporting this result.\n\nProof of Theorem 3.6\n\nDifferent from GML-DPS, PSLD Algorithm 2 replaces the goodness objective (6) with the gluing objective (7), which can be formalized as:\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(z_1) - y^2; &(10) \\\\\nz_0 & = \\arg \\min_{z'_0} ||z'_0 - E(A^TAz_0 + (Id - A^TA)D(z'_0))||^2. &(11)\n\\end{align*}\n$$\nWe again remind that solving the minimization problem (11) is hard in general, and can be approximated by gradient descent as typically followed in practice [11]. However, in a linear model setting, (11) has a closed-form solution which we derive to prove exact recovery.\n\nTheorem B.5 (Posterior Sampling using Diffusion in Latent Space)\n\nLet Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$ denote the singular values of $$(AS)^T(AS)$$ and let\n\n$$\n\\begin{align*}\n\\theta^* & = \\arg \\min_{\\theta} E||z_0 - \\epsilon \\mu_1 z_1(\\epsilon z_0)|| - z_0 - \\mu \\theta z_1 z_0, \\epsilon.\n\\end{align*}\n$$\nSuppose $$x_0 \\sim p(x_0)$$. Given measurements $$y = A^{-1}x_0$$, any fixed variance $$\\beta \\in (0, 1)$$, and any positive step sizes $$\\eta_j, i, j = 1, 2, ..., r$$, the PSLD Algorithm 2 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nProof: Following the proof in Appendix B.4, we have\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(z_1) - y^2 \\\\\n& = Ik - STS + STASz'_0 - STASz_0.\n\\end{align*}\n$$\nWe use the above expression to derive a closed-form solution to the minimization problem (11):\n\n$$\n\\begin{align*}\n0 & = \\nabla z'_0(z'_0 - ST(A^TASz_0 + (Id - A^TA)Sz'_0))^2 \\\\\n& = 2STAS(STASz'_0 - STASz_0),\n\\end{align*}\n$$\nwhere the last step is due to Assumption 3.1. Thus, we have\n\n$$\n\\begin{align*}\nz_0 & = \\arg \\min_{z'_0} ||z'_0 - E(A^TAz_0 + (Id - A^TA)D(z'_0))||^2 = -z_0.\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations in HTML", "md": "# Math Equations in HTML"}, {"type": "text", "value": "consequently $$z_0 = z'_0 - \\gamma \\nabla z'_0 = z'_0 - E(D(z_0))z' = -z_0.$$ Thus, the reconstructed sample becomes $$x_0 = D(z_0) = S^{-1}z_0 = -x_0.$$\n\nFurthermore, as $$z'_0 - E(D(z'_0)) = 0$$ for all $$z'_0$$, it is evident that the goodness objective cannot rectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6) requires the exact step size to sample from the posterior.\n\nBeyond the linear setting, we also refer to Table 5 for experiments supporting this result.\n\nProof of Theorem 3.6\n\nDifferent from GML-DPS, PSLD Algorithm 2 replaces the goodness objective (6) with the gluing objective (7), which can be formalized as:\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(z_1) - y^2; &(10) \\\\\nz_0 & = \\arg \\min_{z'_0} ||z'_0 - E(A^TAz_0 + (Id - A^TA)D(z'_0))||^2. &(11)\n\\end{align*}\n$$\nWe again remind that solving the minimization problem (11) is hard in general, and can be approximated by gradient descent as typically followed in practice [11]. However, in a linear model setting, (11) has a closed-form solution which we derive to prove exact recovery.\n\nTheorem B.5 (Posterior Sampling using Diffusion in Latent Space)\n\nLet Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$ denote the singular values of $$(AS)^T(AS)$$ and let\n\n$$\n\\begin{align*}\n\\theta^* & = \\arg \\min_{\\theta} E||z_0 - \\epsilon \\mu_1 z_1(\\epsilon z_0)|| - z_0 - \\mu \\theta z_1 z_0, \\epsilon.\n\\end{align*}\n$$\nSuppose $$x_0 \\sim p(x_0)$$. Given measurements $$y = A^{-1}x_0$$, any fixed variance $$\\beta \\in (0, 1)$$, and any positive step sizes $$\\eta_j, i, j = 1, 2, ..., r$$, the PSLD Algorithm 2 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nProof: Following the proof in Appendix B.4, we have\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(z_1) - y^2 \\\\\n& = Ik - STS + STASz'_0 - STASz_0.\n\\end{align*}\n$$\nWe use the above expression to derive a closed-form solution to the minimization problem (11):\n\n$$\n\\begin{align*}\n0 & = \\nabla z'_0(z'_0 - ST(A^TASz_0 + (Id - A^TA)Sz'_0))^2 \\\\\n& = 2STAS(STASz'_0 - STASz_0),\n\\end{align*}\n$$\nwhere the last step is due to Assumption 3.1. Thus, we have\n\n$$\n\\begin{align*}\nz_0 & = \\arg \\min_{z'_0} ||z'_0 - E(A^TAz_0 + (Id - A^TA)D(z'_0))||^2 = -z_0.\n\\end{align*}\n$$", "md": "consequently $$z_0 = z'_0 - \\gamma \\nabla z'_0 = z'_0 - E(D(z_0))z' = -z_0.$$ Thus, the reconstructed sample becomes $$x_0 = D(z_0) = S^{-1}z_0 = -x_0.$$\n\nFurthermore, as $$z'_0 - E(D(z'_0)) = 0$$ for all $$z'_0$$, it is evident that the goodness objective cannot rectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6) requires the exact step size to sample from the posterior.\n\nBeyond the linear setting, we also refer to Table 5 for experiments supporting this result.\n\nProof of Theorem 3.6\n\nDifferent from GML-DPS, PSLD Algorithm 2 replaces the goodness objective (6) with the gluing objective (7), which can be formalized as:\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(z_1) - y^2; &(10) \\\\\nz_0 & = \\arg \\min_{z'_0} ||z'_0 - E(A^TAz_0 + (Id - A^TA)D(z'_0))||^2. &(11)\n\\end{align*}\n$$\nWe again remind that solving the minimization problem (11) is hard in general, and can be approximated by gradient descent as typically followed in practice [11]. However, in a linear model setting, (11) has a closed-form solution which we derive to prove exact recovery.\n\nTheorem B.5 (Posterior Sampling using Diffusion in Latent Space)\n\nLet Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$ denote the singular values of $$(AS)^T(AS)$$ and let\n\n$$\n\\begin{align*}\n\\theta^* & = \\arg \\min_{\\theta} E||z_0 - \\epsilon \\mu_1 z_1(\\epsilon z_0)|| - z_0 - \\mu \\theta z_1 z_0, \\epsilon.\n\\end{align*}\n$$\nSuppose $$x_0 \\sim p(x_0)$$. Given measurements $$y = A^{-1}x_0$$, any fixed variance $$\\beta \\in (0, 1)$$, and any positive step sizes $$\\eta_j, i, j = 1, 2, ..., r$$, the PSLD Algorithm 2 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nProof: Following the proof in Appendix B.4, we have\n\n$$\n\\begin{align*}\nz'_0 & = \\theta^*z_1 - \\eta \\nabla z_1 z_0(z_1) - y^2 \\\\\n& = Ik - STS + STASz'_0 - STASz_0.\n\\end{align*}\n$$\nWe use the above expression to derive a closed-form solution to the minimization problem (11):\n\n$$\n\\begin{align*}\n0 & = \\nabla z'_0(z'_0 - ST(A^TASz_0 + (Id - A^TA)Sz'_0))^2 \\\\\n& = 2STAS(STASz'_0 - STASz_0),\n\\end{align*}\n$$\nwhere the last step is due to Assumption 3.1. Thus, we have\n\n$$\n\\begin{align*}\nz_0 & = \\arg \\min_{z'_0} ||z'_0 - E(A^TAz_0 + (Id - A^TA)D(z'_0))||^2 = -z_0.\n\\end{align*}\n$$"}]}, {"page": 21, "text": "which produces \u2190  \u2212        \u2212         \u2192        \u2192     \u2192\n                 x0 = D(\u2190 z0) = D(\u2212  z0) = S\u2212 z0 = \u2212x0.                                               \u25a1\nIt is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the choice\nof the step size \u03b7, whereas GML-DPS requires the step size to be exactly \u03b7 = (1/2)UD(\u03b7i)U T .\nC    Additional Experiments\nC.1   Implementation Details\nFor inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing\nobjective) that are consistent with the known portions of the image, i.e., \u2190   \u2212            \u2192\nAT A)D(\u2190   \u2212                                                                   x0 = AT A\u2212  x0 + (Id \u2212\n          z0). This is different from the DPS sampler, which generates the whole image which\nmay not match the observations exactly. In other words, in the last of step of our algorithm, the\nobservations are glued onto the corresponding parts of the generated image, leaving the unmasked\nportions untouched [54]. This sometimes creates edge effects which are then removed by post-\nprocessing the glued image through the encoder and decoder of the SD model, i.e. running one last\nstep of our algorithm. Figure 2 illustrates that gluing the observations in commercial services still\nleads to visually inconsistent results (e.g. head in top row) unlike our method.\nFor all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is not\nneeded, as there is no box inpainting, i.e.,\u2212\u2190        \u2212\n                                            x0 = D(\u2190 z0). Furthermore, we use the same measurement\noperator A and its transpose AT as provided by the DPS code repository8. However, since Stable\nDiffusion v1.5 generates images of size 512 \u00d7 512 resolution and DPS operates at 256 \u00d7 256, we\nadjust the size of the kernels used in PSLD to ensure that both the methods use the same amount of\ninformation while sampling from the posterior. During evaluation, we downsample PSLD generated\nimages from 512 \u00d7 512 to 256 \u00d7 256 to compare with DPS at the same resolution.\nPSLD (Stable Diffusion-V1.5 ): We run Algorithm 2 with Stable Diffusion version 1.5 as the\nfoundation model9. We use a fixed \u03b7 = 1 and \u03b3 = 0.1. Since we study posterior sampling of images\nwithout conditioning on text inputs, we pass an empty string to the Stable Diffusion foundation model,\nwhich accepts texts as an input argument. For better performance, we recommend using the latest\npretrained weights.\nPSLD (LDM-VQ-4 ): This is the same sampling algorithm as before but with a different latent\ndiffusion model, LDM-VQ-410 , which contains pretrained weights for FFHQ 25611 and large-scale\ntext-to-image generative model12. We keep the hyperparameters same (\u03b7 = 1 and \u03b3 = 0.1). For each\ntask, we provide hyper-parameter details in our codebase13. Although we have tested our framework\nwith these two latent-diffusion-models, one may experiment with other latent-diffusion-models\navailable in the same repository.\nDPS: We use the original source code provided by the authors14.\nOOD images are sourced online:\n      1. Figure 1: the original images are generated by Stable Diffusion v-2.115.\n      2. Figure 2 first row: Walking example from the web.\n      3. Figure 2 second row, Obama-Biden image from the web.\n      4. Figure 2 third row, Fisherman from ImageNet 256 [17].\n      5. Figure 4 first row: Racoon image from the web.\n      6. Figure 4 second row: Fisherman from ImageNet 256 [17].\n      7. Figure 15: Celebrity face from the web.\n   8https://github.com/DPS2022/diffusion-posterior-sampling/blob/main/guided_\ndiffusion/measurements.py\n   9https://huggingface.co/runwayml/stable-diffusion-v1-5\n  10https://github.com/CompVis/latent-diffusion\n  11https://ommer-lab.com/files/latent-diffusion/ffhq.zip\n  12https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n  13https://github.com/LituRout/PSLD\n  14https://github.com/DPS2022/diffusion-posterior-sampling\n  15https://huggingface.co/spaces/stabilityai/stable-diffusion\n                                                   21", "md": "# Document\n\nwhich produces $$x_0 = D(\\leftarrow z_0) = D(-z_0) = S^{-}z_0 = -x_0.$$\n\nIt is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the choice of the step size $$\\eta$$, whereas GML-DPS requires the step size to be exactly $$\\eta = \\frac{1}{2}UD(\\eta_i)U^T.$$\n\n## Additional Experiments\n\n### Implementation Details\n\nFor inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing objective) that are consistent with the known portions of the image, i.e., $$\\leftarrow AT A\\right)D(\\leftarrow x_0 = AT A^{-} x_0 + (Id - z_0).$$ This is different from the DPS sampler, which generates the whole image which may not match the observations exactly. In other words, in the last step of our algorithm, the observations are glued onto the corresponding parts of the generated image, leaving the unmasked portions untouched. This sometimes creates edge effects which are then removed by post-processing the glued image through the encoder and decoder of the SD model, i.e. running one last step of our algorithm. Figure 2 illustrates that gluing the observations in commercial services still leads to visually inconsistent results (e.g. head in top row) unlike our method.\n\nFor all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is not needed, as there is no box inpainting, i.e., $$\\leftarrow x_0 = D(\\leftarrow z_0).$$ Furthermore, we use the same measurement operator A and its transpose $$A^T$$ as provided by the DPS code repository. However, since Stable Diffusion v1.5 generates images of size 512 x 512 resolution and DPS operates at 256 x 256, we adjust the size of the kernels used in PSLD to ensure that both the methods use the same amount of information while sampling from the posterior. During evaluation, we downsample PSLD generated images from 512 x 512 to 256 x 256 to compare with DPS at the same resolution.\n\nPSLD (Stable Diffusion-V1.5): We run Algorithm 2 with Stable Diffusion version 1.5 as the foundation model. We use a fixed $$\\eta = 1$$ and $$\\gamma = 0.1$$. Since we study posterior sampling of images without conditioning on text inputs, we pass an empty string to the Stable Diffusion foundation model, which accepts texts as an input argument. For better performance, we recommend using the latest pretrained weights.\n\nPSLD (LDM-VQ-4): This is the same sampling algorithm as before but with a different latent diffusion model, LDM-VQ-4, which contains pretrained weights for FFHQ 256 and large-scale text-to-image generative model. We keep the hyperparameters same ($$\\eta = 1$$ and $$\\gamma = 0.1$$). For each task, we provide hyper-parameter details in our codebase. Although we have tested our framework with these two latent-diffusion-models, one may experiment with other latent-diffusion-models available in the same repository.\n\nDPS: We use the original source code provided by the authors.\n\nOOD images are sourced online:\n\n1. Figure 1: the original images are generated by Stable Diffusion v-2.1.\n2. Figure 2 first row: Walking example from the web.\n3. Figure 2 second row, Obama-Biden image from the web.\n4. Figure 2 third row, Fisherman from ImageNet 256.\n5. Figure 4 first row: Racoon image from the web.\n6. Figure 4 second row: Fisherman from ImageNet 256.\n7. Figure 15: Celebrity face from the web.\n\nReferences:\n\n1. DPS Code Repository\n2. Stable Diffusion v1.5\n3. Latent Diffusion Model\n4. FFHQ 256 Pretrained Weights\n5. Text-to-Image Generative Model Pretrained Weights\n6. PSLD Codebase\n7. DPS Code Repository\n8. Stable Diffusion v2.1", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "which produces $$x_0 = D(\\leftarrow z_0) = D(-z_0) = S^{-}z_0 = -x_0.$$\n\nIt is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the choice of the step size $$\\eta$$, whereas GML-DPS requires the step size to be exactly $$\\eta = \\frac{1}{2}UD(\\eta_i)U^T.$$", "md": "which produces $$x_0 = D(\\leftarrow z_0) = D(-z_0) = S^{-}z_0 = -x_0.$$\n\nIt is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the choice of the step size $$\\eta$$, whereas GML-DPS requires the step size to be exactly $$\\eta = \\frac{1}{2}UD(\\eta_i)U^T.$$"}, {"type": "heading", "lvl": 2, "value": "Additional Experiments", "md": "## Additional Experiments"}, {"type": "heading", "lvl": 3, "value": "Implementation Details", "md": "### Implementation Details"}, {"type": "text", "value": "For inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing objective) that are consistent with the known portions of the image, i.e., $$\\leftarrow AT A\\right)D(\\leftarrow x_0 = AT A^{-} x_0 + (Id - z_0).$$ This is different from the DPS sampler, which generates the whole image which may not match the observations exactly. In other words, in the last step of our algorithm, the observations are glued onto the corresponding parts of the generated image, leaving the unmasked portions untouched. This sometimes creates edge effects which are then removed by post-processing the glued image through the encoder and decoder of the SD model, i.e. running one last step of our algorithm. Figure 2 illustrates that gluing the observations in commercial services still leads to visually inconsistent results (e.g. head in top row) unlike our method.\n\nFor all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is not needed, as there is no box inpainting, i.e., $$\\leftarrow x_0 = D(\\leftarrow z_0).$$ Furthermore, we use the same measurement operator A and its transpose $$A^T$$ as provided by the DPS code repository. However, since Stable Diffusion v1.5 generates images of size 512 x 512 resolution and DPS operates at 256 x 256, we adjust the size of the kernels used in PSLD to ensure that both the methods use the same amount of information while sampling from the posterior. During evaluation, we downsample PSLD generated images from 512 x 512 to 256 x 256 to compare with DPS at the same resolution.\n\nPSLD (Stable Diffusion-V1.5): We run Algorithm 2 with Stable Diffusion version 1.5 as the foundation model. We use a fixed $$\\eta = 1$$ and $$\\gamma = 0.1$$. Since we study posterior sampling of images without conditioning on text inputs, we pass an empty string to the Stable Diffusion foundation model, which accepts texts as an input argument. For better performance, we recommend using the latest pretrained weights.\n\nPSLD (LDM-VQ-4): This is the same sampling algorithm as before but with a different latent diffusion model, LDM-VQ-4, which contains pretrained weights for FFHQ 256 and large-scale text-to-image generative model. We keep the hyperparameters same ($$\\eta = 1$$ and $$\\gamma = 0.1$$). For each task, we provide hyper-parameter details in our codebase. Although we have tested our framework with these two latent-diffusion-models, one may experiment with other latent-diffusion-models available in the same repository.\n\nDPS: We use the original source code provided by the authors.\n\nOOD images are sourced online:\n\n1. Figure 1: the original images are generated by Stable Diffusion v-2.1.\n2. Figure 2 first row: Walking example from the web.\n3. Figure 2 second row, Obama-Biden image from the web.\n4. Figure 2 third row, Fisherman from ImageNet 256.\n5. Figure 4 first row: Racoon image from the web.\n6. Figure 4 second row: Fisherman from ImageNet 256.\n7. Figure 15: Celebrity face from the web.\n\nReferences:\n\n1. DPS Code Repository\n2. Stable Diffusion v1.5\n3. Latent Diffusion Model\n4. FFHQ 256 Pretrained Weights\n5. Text-to-Image Generative Model Pretrained Weights\n6. PSLD Codebase\n7. DPS Code Repository\n8. Stable Diffusion v2.1", "md": "For inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing objective) that are consistent with the known portions of the image, i.e., $$\\leftarrow AT A\\right)D(\\leftarrow x_0 = AT A^{-} x_0 + (Id - z_0).$$ This is different from the DPS sampler, which generates the whole image which may not match the observations exactly. In other words, in the last step of our algorithm, the observations are glued onto the corresponding parts of the generated image, leaving the unmasked portions untouched. This sometimes creates edge effects which are then removed by post-processing the glued image through the encoder and decoder of the SD model, i.e. running one last step of our algorithm. Figure 2 illustrates that gluing the observations in commercial services still leads to visually inconsistent results (e.g. head in top row) unlike our method.\n\nFor all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is not needed, as there is no box inpainting, i.e., $$\\leftarrow x_0 = D(\\leftarrow z_0).$$ Furthermore, we use the same measurement operator A and its transpose $$A^T$$ as provided by the DPS code repository. However, since Stable Diffusion v1.5 generates images of size 512 x 512 resolution and DPS operates at 256 x 256, we adjust the size of the kernels used in PSLD to ensure that both the methods use the same amount of information while sampling from the posterior. During evaluation, we downsample PSLD generated images from 512 x 512 to 256 x 256 to compare with DPS at the same resolution.\n\nPSLD (Stable Diffusion-V1.5): We run Algorithm 2 with Stable Diffusion version 1.5 as the foundation model. We use a fixed $$\\eta = 1$$ and $$\\gamma = 0.1$$. Since we study posterior sampling of images without conditioning on text inputs, we pass an empty string to the Stable Diffusion foundation model, which accepts texts as an input argument. For better performance, we recommend using the latest pretrained weights.\n\nPSLD (LDM-VQ-4): This is the same sampling algorithm as before but with a different latent diffusion model, LDM-VQ-4, which contains pretrained weights for FFHQ 256 and large-scale text-to-image generative model. We keep the hyperparameters same ($$\\eta = 1$$ and $$\\gamma = 0.1$$). For each task, we provide hyper-parameter details in our codebase. Although we have tested our framework with these two latent-diffusion-models, one may experiment with other latent-diffusion-models available in the same repository.\n\nDPS: We use the original source code provided by the authors.\n\nOOD images are sourced online:\n\n1. Figure 1: the original images are generated by Stable Diffusion v-2.1.\n2. Figure 2 first row: Walking example from the web.\n3. Figure 2 second row, Obama-Biden image from the web.\n4. Figure 2 third row, Fisherman from ImageNet 256.\n5. Figure 4 first row: Racoon image from the web.\n6. Figure 4 second row: Fisherman from ImageNet 256.\n7. Figure 15: Celebrity face from the web.\n\nReferences:\n\n1. DPS Code Repository\n2. Stable Diffusion v1.5\n3. Latent Diffusion Model\n4. FFHQ 256 Pretrained Weights\n5. Text-to-Image Generative Model Pretrained Weights\n6. PSLD Codebase\n7. DPS Code Repository\n8. Stable Diffusion v2.1"}]}, {"page": 22, "text": "                                            PSLD Marc Inpilintin?\nFigure 6: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image\n(1) is generated by Stable Diffusion v-2.1 with the prompt,\u201cA dinner date between a robot couple\nduring sunset\u201d.\nC.2   Additional Experimental Evaluation\nHere, we provide additional results to support our theoretical claims on various inverse problems.\nFigures 6, 7, 8, and 9 show the inpainting results of user defined masks obtained from our PSLD\ninpainting web demo. Note that the foundation model used in this demo is a generic model. For\nbetter performance on specific images, we recommend finetuning the foundation model on this class\nand then running posterior sampling using our web demo: https://huggingface.co/spaces/\nPSLD/PSLD.\nFigure 10 and 11 illustrate super-resolution (4\u00d7) of in-distribution samples from the validation set of\nFFHQ 256. Observe that the samples generated by DPS are far from the groundtruth sample. On the\nother hand, the samples generated by PSLD closely capture the perceptual quality of the groundtruth\nsample. In other words, one may identify (b) and (c) as images of two different individuals, whereas\n(b) and (d) of the same individual. We attribute this photorealism of our method to the power of\nStable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder\nin the gluing objective.\nIn addition, we test on out-of-distribution samples from ImageNet [17] validation set. Figure 12\nand Figure 13 show the results in motion deblur and Gaussian deblur, respectively. By leveraging\nthe foundation model Stable Diffusion v1.5, our PSLD method clearly outperforms DPS [11] in the\ngeneral domain. Further, Figures 14, 15, and 16 show reconstruction of general domain samples\nfor random inpainting, super-resolution, and destriping tasks, respectively. In all these tasks, the\nsamples generated by PSLD are closer to the groundtruth sample than the ones generated by DPS.\nFigure 17 shows the results on image colorization. Table 5 and Table 6 show the quantitative results.\nTable 7 draws a comparison between the latent-DPS and PSLD algorithms, and shows that the PSLD\nobjective enhances the reconstruction performance.\nIn Table 8, we compare the runtime and NFEs of PSLD with prior works. PSLD-SD (trained on\nLAION-5B) takes 776 s to generate 512x512 images. To compare with other methods that generate\n256x256 images, we divide our runtime by 4. All the other methods use diffusion models trained on\nFFHQ and produce 256x256 images.\n                                                  22", "md": "## PSLD Marc Inpilintin?\n\nFigure 6: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt,\u201cA dinner date between a robot couple during sunset\u201d.\n\n### Additional Experimental Evaluation\n\nHere, we provide additional results to support our theoretical claims on various inverse problems. Figures 6, 7, 8, and 9 show the inpainting results of user defined masks obtained from our PSLD inpainting web demo. Note that the foundation model used in this demo is a generic model. For better performance on specific images, we recommend finetuning the foundation model on this class and then running posterior sampling using our web demo: PSLD/PSLD.\n\nFigure 10 and 11 illustrate super-resolution (4\u00d7) of in-distribution samples from the validation set of FFHQ 256. Observe that the samples generated by DPS are far from the groundtruth sample. On the other hand, the samples generated by PSLD closely capture the perceptual quality of the groundtruth sample. In other words, one may identify (b) and (c) as images of two different individuals, whereas (b) and (d) of the same individual. We attribute this photorealism of our method to the power of Stable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder in the gluing objective.\n\nIn addition, we test on out-of-distribution samples from ImageNet [17] validation set. Figure 12 and Figure 13 show the results in motion deblur and Gaussian deblur, respectively. By leveraging the foundation model Stable Diffusion v1.5, our PSLD method clearly outperforms DPS [11] in the general domain. Further, Figures 14, 15, and 16 show reconstruction of general domain samples for random inpainting, super-resolution, and destriping tasks, respectively. In all these tasks, the samples generated by PSLD are closer to the groundtruth sample than the ones generated by DPS.\n\nFigure 17 shows the results on image colorization. Table 5 and Table 6 show the quantitative results. Table 7 draws a comparison between the latent-DPS and PSLD algorithms, and shows that the PSLD objective enhances the reconstruction performance.\n\nIn Table 8, we compare the runtime and NFEs of PSLD with prior works. PSLD-SD (trained on LAION-5B) takes 776 s to generate 512x512 images. To compare with other methods that generate 256x256 images, we divide our runtime by 4. All the other methods use diffusion models trained on FFHQ and produce 256x256 images.\n\n22", "images": [{"name": "page-22-0.jpg", "height": 259, "width": 396, "x": 108, "y": 72}], "items": [{"type": "heading", "lvl": 2, "value": "PSLD Marc Inpilintin?", "md": "## PSLD Marc Inpilintin?"}, {"type": "text", "value": "Figure 6: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt,\u201cA dinner date between a robot couple during sunset\u201d.", "md": "Figure 6: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt,\u201cA dinner date between a robot couple during sunset\u201d."}, {"type": "heading", "lvl": 3, "value": "Additional Experimental Evaluation", "md": "### Additional Experimental Evaluation"}, {"type": "text", "value": "Here, we provide additional results to support our theoretical claims on various inverse problems. Figures 6, 7, 8, and 9 show the inpainting results of user defined masks obtained from our PSLD inpainting web demo. Note that the foundation model used in this demo is a generic model. For better performance on specific images, we recommend finetuning the foundation model on this class and then running posterior sampling using our web demo: PSLD/PSLD.\n\nFigure 10 and 11 illustrate super-resolution (4\u00d7) of in-distribution samples from the validation set of FFHQ 256. Observe that the samples generated by DPS are far from the groundtruth sample. On the other hand, the samples generated by PSLD closely capture the perceptual quality of the groundtruth sample. In other words, one may identify (b) and (c) as images of two different individuals, whereas (b) and (d) of the same individual. We attribute this photorealism of our method to the power of Stable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder in the gluing objective.\n\nIn addition, we test on out-of-distribution samples from ImageNet [17] validation set. Figure 12 and Figure 13 show the results in motion deblur and Gaussian deblur, respectively. By leveraging the foundation model Stable Diffusion v1.5, our PSLD method clearly outperforms DPS [11] in the general domain. Further, Figures 14, 15, and 16 show reconstruction of general domain samples for random inpainting, super-resolution, and destriping tasks, respectively. In all these tasks, the samples generated by PSLD are closer to the groundtruth sample than the ones generated by DPS.\n\nFigure 17 shows the results on image colorization. Table 5 and Table 6 show the quantitative results. Table 7 draws a comparison between the latent-DPS and PSLD algorithms, and shows that the PSLD objective enhances the reconstruction performance.\n\nIn Table 8, we compare the runtime and NFEs of PSLD with prior works. PSLD-SD (trained on LAION-5B) takes 776 s to generate 512x512 images. To compare with other methods that generate 256x256 images, we divide our runtime by 4. All the other methods use diffusion models trained on FFHQ and produce 256x256 images.\n\n22", "md": "Here, we provide additional results to support our theoretical claims on various inverse problems. Figures 6, 7, 8, and 9 show the inpainting results of user defined masks obtained from our PSLD inpainting web demo. Note that the foundation model used in this demo is a generic model. For better performance on specific images, we recommend finetuning the foundation model on this class and then running posterior sampling using our web demo: PSLD/PSLD.\n\nFigure 10 and 11 illustrate super-resolution (4\u00d7) of in-distribution samples from the validation set of FFHQ 256. Observe that the samples generated by DPS are far from the groundtruth sample. On the other hand, the samples generated by PSLD closely capture the perceptual quality of the groundtruth sample. In other words, one may identify (b) and (c) as images of two different individuals, whereas (b) and (d) of the same individual. We attribute this photorealism of our method to the power of Stable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder in the gluing objective.\n\nIn addition, we test on out-of-distribution samples from ImageNet [17] validation set. Figure 12 and Figure 13 show the results in motion deblur and Gaussian deblur, respectively. By leveraging the foundation model Stable Diffusion v1.5, our PSLD method clearly outperforms DPS [11] in the general domain. Further, Figures 14, 15, and 16 show reconstruction of general domain samples for random inpainting, super-resolution, and destriping tasks, respectively. In all these tasks, the samples generated by PSLD are closer to the groundtruth sample than the ones generated by DPS.\n\nFigure 17 shows the results on image colorization. Table 5 and Table 6 show the quantitative results. Table 7 draws a comparison between the latent-DPS and PSLD algorithms, and shows that the PSLD objective enhances the reconstruction performance.\n\nIn Table 8, we compare the runtime and NFEs of PSLD with prior works. PSLD-SD (trained on LAION-5B) takes 776 s to generate 512x512 images. To compare with other methods that generate 256x256 images, we divide our runtime by 4. All the other methods use diffusion models trained on FFHQ and produce 256x256 images.\n\n22"}]}, {"page": 23, "text": "                                                  PSLD Imaze Inpainting\nFigure 7: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image\n(1) is generated by Stable Diffusion v-2.1 with the prompt,\u201cA panda wearing a spiderman costume\u201d.\n                                                 PSLD ImageInpalntng\nFigure 8: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image\n(1) is generated by Stable Diffusion v-2.1 with the prompt,\u201cA teddy bear showing stop sign at the\ntraffic\u201d.\n                                                          23", "md": "# PSLD Image Inpainting Results\n\n## PSLD Image Inpainting\n\nFigure 7: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A panda wearing a spiderman costume\".\n\n$$512 \\times 512$$\n\nFigure 8: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A teddy bear showing stop sign at the traffic\".\n\n$$512 \\times 512$$", "images": [{"name": "page-23-1.jpg", "height": 257, "width": 396, "x": 108, "y": 409}, {"name": "page-23-0.jpg", "height": 258, "width": 396, "x": 108, "y": 86}], "items": [{"type": "heading", "lvl": 1, "value": "PSLD Image Inpainting Results", "md": "# PSLD Image Inpainting Results"}, {"type": "heading", "lvl": 2, "value": "PSLD Image Inpainting", "md": "## PSLD Image Inpainting"}, {"type": "text", "value": "Figure 7: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A panda wearing a spiderman costume\".\n\n$$512 \\times 512$$\n\nFigure 8: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A teddy bear showing stop sign at the traffic\".\n\n$$512 \\times 512$$", "md": "Figure 7: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A panda wearing a spiderman costume\".\n\n$$512 \\times 512$$\n\nFigure 8: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A teddy bear showing stop sign at the traffic\".\n\n$$512 \\times 512$$"}]}, {"page": 24, "text": "                                                    Inpalnting\n                                           PSLD Imaze\nFigure 9: Results from the web application of our PSLD algorithm, 512 \u00d7 512. The original image\n(1) is generated by Stable Diffusion v-2.1 with the prompt,\u201cA cute dog playing with a toy teddy bear\non the lawn\u201d.\nTable 5: Quantitative random inpainting results on FFHQ 256 validation set [25, 11]. We use Stable\nDiffusion (v1.5) trained on LAION.\n                            Inpaint (random)              SR (4\u00d7)               Gaussian Deblur\n   Method                PSNR (\u2191)     SSIM (\u2191)     PSNR (\u2191)     SSIM (\u2191)     PSNR (\u2191)     SSIM (\u2191)\n   PSLD (Ours)           33.71        0.943        30.73        0.867        30.10        0.843\n   GML-DPS (Ours)        29.49        0.844        29.77        0.860        29.21        0.820\n   DPS [11]              25.23        0.851        25.67        0.852        24.25        0.811\n   DDRM [26]             9.19         0.319        25.36        0.835        23.36        0.767\n   MCG [13]              21.57        0.751        20.05        0.559        6.72         0.051\n   PnP-ADMM [6]          8.41         0.325        26.55        0.865        24.93        0.812\n   Score-SDE [50]        13.52        0.437        17.62        0.617        7.12         0.109\n   ADMM-TV               22.03        0.784        23.86        0.803        22.37        0.801\nD     Additional Discussion\nCurse of ambient dimension: DPS [11] suffers from the curse of ambient dimension because in\nthis method, gradients are computed in the pixel space with dimension d. However, latent-based\nmethods such as PSLD compute gradients in the latent dimension k, and hence the computation\nis more efficient. Furthermore, applying the chain rule on VAE and running diffusion in the latent\nspace is less expensive than running diffusion in pixel space directly. In practice, the computational\ncomplexity of Stable Diffusion model (\u223c     4GB) is higher (roughly 6 times) than the computational\ncomplexity of the encoder-decoder model (\u223c       700MB). Therefore, applying the chain rule in the\nencoder-decoder and running diffusion in the latent space is less expensive than applying diffusion\nmodels in the pixel space directly.\n                                                  24", "md": "# Inpainting Results\n\n## Results from the web application of our PSLD algorithm, 512 \u00d7 512\n\nThe original image is generated by Stable Diffusion v-2.1 with the prompt, \"A cute dog playing with a toy teddy bear on the lawn\".\n\n### Quantitative random inpainting results on FFHQ 256 validation set\n\nWe use Stable Diffusion (v1.5) trained on LAION.\n\n|Method|Inpaint (random) PSNR (\u2191)|Inpaint (random) SSIM (\u2191)|SR (4\u00d7) PSNR (\u2191)|SR (4\u00d7) SSIM (\u2191)|Gaussian Deblur PSNR (\u2191)|Gaussian Deblur SSIM (\u2191)|\n|---|---|---|---|---|---|---|\n|PSLD (Ours)|33.71|0.943|30.73|0.867|30.10|0.843|\n|GML-DPS (Ours)|29.49|0.844|29.77|0.860|29.21|0.820|\n|DPS [11]|25.23|0.851|25.67|0.852|24.25|0.811|\n|DDRM [26]|9.19|0.319|25.36|0.835|23.36|0.767|\n|MCG [13]|21.57|0.751|20.05|0.559|6.72|0.051|\n|PnP-ADMM [6]|8.41|0.325|26.55|0.865|24.93|0.812|\n|Score-SDE [50]|13.52|0.437|17.62|0.617|7.12|0.109|\n|ADMM-TV|22.03|0.784|23.86|0.803|22.37|0.801|\n\n### Additional Discussion\n\nCurse of ambient dimension: DPS [11] suffers from the curse of ambient dimension because in this method, gradients are computed in the pixel space with dimension d. However, latent-based methods such as PSLD compute gradients in the latent dimension k, and hence the computation is more efficient. Furthermore, applying the chain rule on VAE and running diffusion in the latent space is less expensive than running diffusion in pixel space directly. In practice, the computational complexity of Stable Diffusion model (~4GB) is higher (roughly 6 times) than the computational complexity of the encoder-decoder model (~700MB). Therefore, applying the chain rule in the encoder-decoder and running diffusion in the latent space is less expensive than applying diffusion models in the pixel space directly.", "images": [{"name": "page-24-0.jpg", "height": 257, "width": 396, "x": 108, "y": 71}], "items": [{"type": "heading", "lvl": 1, "value": "Inpainting Results", "md": "# Inpainting Results"}, {"type": "heading", "lvl": 2, "value": "Results from the web application of our PSLD algorithm, 512 \u00d7 512", "md": "## Results from the web application of our PSLD algorithm, 512 \u00d7 512"}, {"type": "text", "value": "The original image is generated by Stable Diffusion v-2.1 with the prompt, \"A cute dog playing with a toy teddy bear on the lawn\".", "md": "The original image is generated by Stable Diffusion v-2.1 with the prompt, \"A cute dog playing with a toy teddy bear on the lawn\"."}, {"type": "heading", "lvl": 3, "value": "Quantitative random inpainting results on FFHQ 256 validation set", "md": "### Quantitative random inpainting results on FFHQ 256 validation set"}, {"type": "text", "value": "We use Stable Diffusion (v1.5) trained on LAION.", "md": "We use Stable Diffusion (v1.5) trained on LAION."}, {"type": "table", "rows": [["Method", "Inpaint (random) PSNR (\u2191)", "Inpaint (random) SSIM (\u2191)", "SR (4\u00d7) PSNR (\u2191)", "SR (4\u00d7) SSIM (\u2191)", "Gaussian Deblur PSNR (\u2191)", "Gaussian Deblur SSIM (\u2191)"], ["PSLD (Ours)", "33.71", "0.943", "30.73", "0.867", "30.10", "0.843"], ["GML-DPS (Ours)", "29.49", "0.844", "29.77", "0.860", "29.21", "0.820"], ["DPS [11]", "25.23", "0.851", "25.67", "0.852", "24.25", "0.811"], ["DDRM [26]", "9.19", "0.319", "25.36", "0.835", "23.36", "0.767"], ["MCG [13]", "21.57", "0.751", "20.05", "0.559", "6.72", "0.051"], ["PnP-ADMM [6]", "8.41", "0.325", "26.55", "0.865", "24.93", "0.812"], ["Score-SDE [50]", "13.52", "0.437", "17.62", "0.617", "7.12", "0.109"], ["ADMM-TV", "22.03", "0.784", "23.86", "0.803", "22.37", "0.801"]], "md": "|Method|Inpaint (random) PSNR (\u2191)|Inpaint (random) SSIM (\u2191)|SR (4\u00d7) PSNR (\u2191)|SR (4\u00d7) SSIM (\u2191)|Gaussian Deblur PSNR (\u2191)|Gaussian Deblur SSIM (\u2191)|\n|---|---|---|---|---|---|---|\n|PSLD (Ours)|33.71|0.943|30.73|0.867|30.10|0.843|\n|GML-DPS (Ours)|29.49|0.844|29.77|0.860|29.21|0.820|\n|DPS [11]|25.23|0.851|25.67|0.852|24.25|0.811|\n|DDRM [26]|9.19|0.319|25.36|0.835|23.36|0.767|\n|MCG [13]|21.57|0.751|20.05|0.559|6.72|0.051|\n|PnP-ADMM [6]|8.41|0.325|26.55|0.865|24.93|0.812|\n|Score-SDE [50]|13.52|0.437|17.62|0.617|7.12|0.109|\n|ADMM-TV|22.03|0.784|23.86|0.803|22.37|0.801|", "isPerfectTable": true, "csv": "\"Method\",\"Inpaint (random) PSNR (\u2191)\",\"Inpaint (random) SSIM (\u2191)\",\"SR (4\u00d7) PSNR (\u2191)\",\"SR (4\u00d7) SSIM (\u2191)\",\"Gaussian Deblur PSNR (\u2191)\",\"Gaussian Deblur SSIM (\u2191)\"\n\"PSLD (Ours)\",\"33.71\",\"0.943\",\"30.73\",\"0.867\",\"30.10\",\"0.843\"\n\"GML-DPS (Ours)\",\"29.49\",\"0.844\",\"29.77\",\"0.860\",\"29.21\",\"0.820\"\n\"DPS [11]\",\"25.23\",\"0.851\",\"25.67\",\"0.852\",\"24.25\",\"0.811\"\n\"DDRM [26]\",\"9.19\",\"0.319\",\"25.36\",\"0.835\",\"23.36\",\"0.767\"\n\"MCG [13]\",\"21.57\",\"0.751\",\"20.05\",\"0.559\",\"6.72\",\"0.051\"\n\"PnP-ADMM [6]\",\"8.41\",\"0.325\",\"26.55\",\"0.865\",\"24.93\",\"0.812\"\n\"Score-SDE [50]\",\"13.52\",\"0.437\",\"17.62\",\"0.617\",\"7.12\",\"0.109\"\n\"ADMM-TV\",\"22.03\",\"0.784\",\"23.86\",\"0.803\",\"22.37\",\"0.801\""}, {"type": "heading", "lvl": 3, "value": "Additional Discussion", "md": "### Additional Discussion"}, {"type": "text", "value": "Curse of ambient dimension: DPS [11] suffers from the curse of ambient dimension because in this method, gradients are computed in the pixel space with dimension d. However, latent-based methods such as PSLD compute gradients in the latent dimension k, and hence the computation is more efficient. Furthermore, applying the chain rule on VAE and running diffusion in the latent space is less expensive than running diffusion in pixel space directly. In practice, the computational complexity of Stable Diffusion model (~4GB) is higher (roughly 6 times) than the computational complexity of the encoder-decoder model (~700MB). Therefore, applying the chain rule in the encoder-decoder and running diffusion in the latent space is less expensive than applying diffusion models in the pixel space directly.", "md": "Curse of ambient dimension: DPS [11] suffers from the curse of ambient dimension because in this method, gradients are computed in the pixel space with dimension d. However, latent-based methods such as PSLD compute gradients in the latent dimension k, and hence the computation is more efficient. Furthermore, applying the chain rule on VAE and running diffusion in the latent space is less expensive than running diffusion in pixel space directly. In practice, the computational complexity of Stable Diffusion model (~4GB) is higher (roughly 6 times) than the computational complexity of the encoder-decoder model (~700MB). Therefore, applying the chain rule in the encoder-decoder and running diffusion in the latent space is less expensive than applying diffusion models in the pixel space directly."}]}, {"page": 25, "text": "                    (a) Input                                        (b) Groundtruth\n                  (c) DPS [11]                                      (d) PSLD (Ours)\nFigure 10: Super-resolution results on images from FFHQ 256 [25, 11] (in distribution).\n                                                  25", "md": "(a) Input\n\n(b) Groundtruth\n\n(c) DPS [11]\n\n(d) PSLD (Ours)\n\nFigure 10: Super-resolution results on images from FFHQ 256 [25, 11] (in distribution).\n\n$$25$$", "images": [{"name": "page-25-1.jpg", "height": 179, "width": 179, "x": 307, "y": 192}, {"name": "page-25-0.jpg", "height": 179, "width": 179, "x": 126, "y": 192}, {"name": "page-25-2.jpg", "height": 179, "width": 179, "x": 126, "y": 387}, {"name": "page-25-3.jpg", "height": 179, "width": 179, "x": 307, "y": 387}], "items": [{"type": "text", "value": "(a) Input\n\n(b) Groundtruth\n\n(c) DPS [11]\n\n(d) PSLD (Ours)\n\nFigure 10: Super-resolution results on images from FFHQ 256 [25, 11] (in distribution).\n\n$$25$$", "md": "(a) Input\n\n(b) Groundtruth\n\n(c) DPS [11]\n\n(d) PSLD (Ours)\n\nFigure 10: Super-resolution results on images from FFHQ 256 [25, 11] (in distribution).\n\n$$25$$"}]}, {"page": 26, "text": "            (a) Input                                        (b) Groundtruth\n          (c) DPS [11]                                      (d) PSLD (Ours)\nFigure 11: Super-resolution results on FFHQ 256 [25, 11] (in distribution).\n                                          26", "md": "|(a) Input|(b) Groundtruth|\n|---|---|\n|(c) DPS [11]|(d) PSLD (Ours)|\n\nFigure 11: Super-resolution results on FFHQ 256 [25, 11] (in distribution).\n\n26", "images": [{"name": "page-26-0.jpg", "height": 179, "width": 179, "x": 126, "y": 192}, {"name": "page-26-1.jpg", "height": 179, "width": 179, "x": 307, "y": 192}, {"name": "page-26-2.jpg", "height": 179, "width": 179, "x": 126, "y": 387}, {"name": "page-26-3.jpg", "height": 179, "width": 179, "x": 307, "y": 387}], "items": [{"type": "table", "rows": [["(a) Input", "(b) Groundtruth"], ["(c) DPS [11]", "(d) PSLD (Ours)"]], "md": "|(a) Input|(b) Groundtruth|\n|---|---|\n|(c) DPS [11]|(d) PSLD (Ours)|", "isPerfectTable": true, "csv": "\"(a) Input\",\"(b) Groundtruth\"\n\"(c) DPS [11]\",\"(d) PSLD (Ours)\""}, {"type": "text", "value": "Figure 11: Super-resolution results on FFHQ 256 [25, 11] (in distribution).\n\n26", "md": "Figure 11: Super-resolution results on FFHQ 256 [25, 11] (in distribution).\n\n26"}]}, {"page": 27, "text": "(a) Input                (b) Groundtruth                (c) DPS [11]                (d) PSLD (Ours)\n   Figure 12: Motion deblur results on ImageNet 256 [17] (out-of-distribution).\n   Table 6: Additional quantitative results on FFHQ 256 validation set [25, 11].\n                                       SR (4\u00d7)                   Gaussian Deblur\n     Method                   FID (\u2193)        LPIPS (\u2193)       FID (\u2193)        LPIPS (\u2193)\n     PSLD (Ours)              34.28          0.201           41.53          0.221\n     DPS [11]                 39.35          0.214           44.05          0.257\n     DDRM [26]                62.15          0.294           74.92          0.332\n     MCG [13]                 87.64          0.520           101.2          0.340\n     PnP-ADMM [6]             66.52          0.353           90.42          0.441\n     Score-SDE [50]           96.72          0.563           109.0          0.403\n     ADMM-TV                  110.6          0.428           186.7          0.507\n                                       SR (4\u00d7)                   Gaussian Deblur\n     Method                   PSNR (\u2191)       SSIM (\u2191)        PSNR (\u2191)       SSIM (\u2191)\n     PSLD (Ours)              30.73          0.867           30.10          0.843\n     GML-DPS (Ours)           29.77          0.860           29.21          0.820\n     DMPS [34]                27.63          -               25.41          -\n     DPS [11]                 25.67          0.852           24.25          0.811\n     DDRM [26]                25.36          0.835           23.36          0.767\n     MCG [13]                 20.05          0.559           6.72           0.051\n     PnP-ADMM [6]             26.55          0.865           24.93          0.812\n     Score-SDE [50]           17.62          0.617           7.12           0.109\n     ADMM-TV                  23.86          0.803           22.37          0.801\n                                               27", "md": "(a) Input                (b) Groundtruth                (c) DPS [11]                (d) PSLD (Ours)\nFigure 12: Motion deblur results on ImageNet 256 [17] (out-of-distribution).\nTable 6: Additional quantitative results on FFHQ 256 validation set [25, 11].\n\n| Method        | FID (\u2193) | LPIPS (\u2193) | FID (\u2193) | LPIPS (\u2193) |\n|---------------|---------|-----------|---------|-----------|\n| PSLD (Ours)   | 34.28   | 0.201     | 41.53   | 0.221     |\n| DPS [11]      | 39.35   | 0.214     | 44.05   | 0.257     |\n| DDRM [26]     | 62.15   | 0.294     | 74.92   | 0.332     |\n| MCG [13]      | 87.64   | 0.520     | 101.2   | 0.340     |\n| PnP-ADMM [6]  | 66.52   | 0.353     | 90.42   | 0.441     |\n| Score-SDE [50]| 96.72   | 0.563     | 109.0   | 0.403     |\n| ADMM-TV       | 110.6   | 0.428     | 186.7   | 0.507     |\n\n| Method        | PSNR (\u2191) | SSIM (\u2191) | PSNR (\u2191) | SSIM (\u2191) |\n|---------------|----------|----------|----------|----------|\n| PSLD (Ours)   | 30.73    | 0.867    | 30.10    | 0.843    |\n| GML-DPS (Ours)| 29.77    | 0.860    | 29.21    | 0.820    |\n| DMPS [34]     | 27.63    | -        | 25.41    | -        |\n| DPS [11]      | 25.67    | 0.852    | 24.25    | 0.811    |\n| DDRM [26]     | 25.36    | 0.835    | 23.36    | 0.767    |\n| MCG [13]      | 20.05    | 0.559    | 6.72     | 0.051    |\n| PnP-ADMM [6]  | 26.55    | 0.865    | 24.93    | 0.812    |\n| Score-SDE [50]| 17.62    | 0.617    | 7.12     | 0.109    |\n| ADMM-TV       | 23.86    | 0.803    | 22.37    | 0.801    |\n\n$$27$$", "images": [{"name": "page-27-2.jpg", "height": 96, "width": 96, "x": 307, "y": 81}, {"name": "page-27-4.jpg", "height": 96, "width": 96, "x": 112, "y": 177}, {"name": "page-27-6.jpg", "height": 96, "width": 96, "x": 307, "y": 177}, {"name": "page-27-8.jpg", "height": 96, "width": 96, "x": 112, "y": 273}, {"name": "page-27-9.jpg", "height": 96, "width": 96, "x": 209, "y": 273}, {"name": "page-27-10.jpg", "height": 96, "width": 96, "x": 307, "y": 273}, {"name": "page-27-0.jpg", "height": 96, "width": 96, "x": 112, "y": 81}, {"name": "page-27-1.jpg", "height": 96, "width": 96, "x": 209, "y": 81}, {"name": "page-27-3.jpg", "height": 96, "width": 96, "x": 404, "y": 81}, {"name": "page-27-5.jpg", "height": 96, "width": 96, "x": 209, "y": 177}, {"name": "page-27-7.jpg", "height": 96, "width": 96, "x": 404, "y": 177}, {"name": "page-27-11.jpg", "height": 96, "width": 96, "x": 404, "y": 273}], "items": [{"type": "text", "value": "(a) Input                (b) Groundtruth                (c) DPS [11]                (d) PSLD (Ours)\nFigure 12: Motion deblur results on ImageNet 256 [17] (out-of-distribution).\nTable 6: Additional quantitative results on FFHQ 256 validation set [25, 11].", "md": "(a) Input                (b) Groundtruth                (c) DPS [11]                (d) PSLD (Ours)\nFigure 12: Motion deblur results on ImageNet 256 [17] (out-of-distribution).\nTable 6: Additional quantitative results on FFHQ 256 validation set [25, 11]."}, {"type": "table", "rows": [["Method", "FID (\u2193)", "LPIPS (\u2193)", "FID (\u2193)", "LPIPS (\u2193)"], ["PSLD (Ours)", "34.28", "0.201", "41.53", "0.221"], ["DPS [11]", "39.35", "0.214", "44.05", "0.257"], ["DDRM [26]", "62.15", "0.294", "74.92", "0.332"], ["MCG [13]", "87.64", "0.520", "101.2", "0.340"], ["PnP-ADMM [6]", "66.52", "0.353", "90.42", "0.441"], ["Score-SDE [50]", "96.72", "0.563", "109.0", "0.403"], ["ADMM-TV", "110.6", "0.428", "186.7", "0.507"]], "md": "| Method        | FID (\u2193) | LPIPS (\u2193) | FID (\u2193) | LPIPS (\u2193) |\n|---------------|---------|-----------|---------|-----------|\n| PSLD (Ours)   | 34.28   | 0.201     | 41.53   | 0.221     |\n| DPS [11]      | 39.35   | 0.214     | 44.05   | 0.257     |\n| DDRM [26]     | 62.15   | 0.294     | 74.92   | 0.332     |\n| MCG [13]      | 87.64   | 0.520     | 101.2   | 0.340     |\n| PnP-ADMM [6]  | 66.52   | 0.353     | 90.42   | 0.441     |\n| Score-SDE [50]| 96.72   | 0.563     | 109.0   | 0.403     |\n| ADMM-TV       | 110.6   | 0.428     | 186.7   | 0.507     |", "isPerfectTable": true, "csv": "\"Method\",\"FID (\u2193)\",\"LPIPS (\u2193)\",\"FID (\u2193)\",\"LPIPS (\u2193)\"\n\"PSLD (Ours)\",\"34.28\",\"0.201\",\"41.53\",\"0.221\"\n\"DPS [11]\",\"39.35\",\"0.214\",\"44.05\",\"0.257\"\n\"DDRM [26]\",\"62.15\",\"0.294\",\"74.92\",\"0.332\"\n\"MCG [13]\",\"87.64\",\"0.520\",\"101.2\",\"0.340\"\n\"PnP-ADMM [6]\",\"66.52\",\"0.353\",\"90.42\",\"0.441\"\n\"Score-SDE [50]\",\"96.72\",\"0.563\",\"109.0\",\"0.403\"\n\"ADMM-TV\",\"110.6\",\"0.428\",\"186.7\",\"0.507\""}, {"type": "table", "rows": [["Method", "PSNR (\u2191)", "SSIM (\u2191)", "PSNR (\u2191)", "SSIM (\u2191)"], ["PSLD (Ours)", "30.73", "0.867", "30.10", "0.843"], ["GML-DPS (Ours)", "29.77", "0.860", "29.21", "0.820"], ["DMPS [34]", "27.63", "-", "25.41", "-"], ["DPS [11]", "25.67", "0.852", "24.25", "0.811"], ["DDRM [26]", "25.36", "0.835", "23.36", "0.767"], ["MCG [13]", "20.05", "0.559", "6.72", "0.051"], ["PnP-ADMM [6]", "26.55", "0.865", "24.93", "0.812"], ["Score-SDE [50]", "17.62", "0.617", "7.12", "0.109"], ["ADMM-TV", "23.86", "0.803", "22.37", "0.801"]], "md": "| Method        | PSNR (\u2191) | SSIM (\u2191) | PSNR (\u2191) | SSIM (\u2191) |\n|---------------|----------|----------|----------|----------|\n| PSLD (Ours)   | 30.73    | 0.867    | 30.10    | 0.843    |\n| GML-DPS (Ours)| 29.77    | 0.860    | 29.21    | 0.820    |\n| DMPS [34]     | 27.63    | -        | 25.41    | -        |\n| DPS [11]      | 25.67    | 0.852    | 24.25    | 0.811    |\n| DDRM [26]     | 25.36    | 0.835    | 23.36    | 0.767    |\n| MCG [13]      | 20.05    | 0.559    | 6.72     | 0.051    |\n| PnP-ADMM [6]  | 26.55    | 0.865    | 24.93    | 0.812    |\n| Score-SDE [50]| 17.62    | 0.617    | 7.12     | 0.109    |\n| ADMM-TV       | 23.86    | 0.803    | 22.37    | 0.801    |", "isPerfectTable": true, "csv": "\"Method\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"PSNR (\u2191)\",\"SSIM (\u2191)\"\n\"PSLD (Ours)\",\"30.73\",\"0.867\",\"30.10\",\"0.843\"\n\"GML-DPS (Ours)\",\"29.77\",\"0.860\",\"29.21\",\"0.820\"\n\"DMPS [34]\",\"27.63\",\"-\",\"25.41\",\"-\"\n\"DPS [11]\",\"25.67\",\"0.852\",\"24.25\",\"0.811\"\n\"DDRM [26]\",\"25.36\",\"0.835\",\"23.36\",\"0.767\"\n\"MCG [13]\",\"20.05\",\"0.559\",\"6.72\",\"0.051\"\n\"PnP-ADMM [6]\",\"26.55\",\"0.865\",\"24.93\",\"0.812\"\n\"Score-SDE [50]\",\"17.62\",\"0.617\",\"7.12\",\"0.109\"\n\"ADMM-TV\",\"23.86\",\"0.803\",\"22.37\",\"0.801\""}, {"type": "text", "value": "$$27$$", "md": "$$27$$"}]}, {"page": 28, "text": "          (a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\n            Figure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution).\nTable 7: Latent-DPS and PSLD methods evaluated on FFHQ 256 validation set [25, 11]. We use the\nlatent diffusion (LDM-VQ-4) trained on FFHQ 256. Latent-DPS is a special case of PSLD algorithm\nwhen \u03b3 = 0.\n                                                         Inpaint (box)\n                               Method        PSNR (\u2191)      SSIM (\u2191)     LPIPS (\u2193)\n                               PSLD          24.22         0.819        0.158\n                               latent-DPS    17.58         0.780        0.21\n                                                        28", "md": "(a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\n\nFigure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution).\n\n|Method|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|---|---|---|---|\n|PSLD|24.22|0.819|0.158|\n|latent-DPS|17.58|0.780|0.21|", "images": [{"name": "page-28-0.jpg", "height": 96, "width": 96, "x": 112, "y": 127}, {"name": "page-28-1.jpg", "height": 96, "width": 96, "x": 209, "y": 127}, {"name": "page-28-3.jpg", "height": 96, "width": 96, "x": 404, "y": 127}, {"name": "page-28-2.jpg", "height": 96, "width": 96, "x": 307, "y": 127}, {"name": "page-28-6.jpg", "height": 96, "width": 96, "x": 307, "y": 223}, {"name": "page-28-5.jpg", "height": 96, "width": 96, "x": 209, "y": 223}, {"name": "page-28-8.jpg", "height": 96, "width": 96, "x": 112, "y": 319}, {"name": "page-28-4.jpg", "height": 96, "width": 96, "x": 112, "y": 223}, {"name": "page-28-10.jpg", "height": 96, "width": 96, "x": 307, "y": 319}, {"name": "page-28-7.jpg", "height": 96, "width": 96, "x": 404, "y": 223}, {"name": "page-28-9.jpg", "height": 96, "width": 96, "x": 209, "y": 319}, {"name": "page-28-11.jpg", "height": 96, "width": 96, "x": 404, "y": 319}], "items": [{"type": "text", "value": "(a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\n\nFigure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution).", "md": "(a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\n\nFigure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution)."}, {"type": "table", "rows": [["Method", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)"], ["PSLD", "24.22", "0.819", "0.158"], ["latent-DPS", "17.58", "0.780", "0.21"]], "md": "|Method|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|---|---|---|---|\n|PSLD|24.22|0.819|0.158|\n|latent-DPS|17.58|0.780|0.21|", "isPerfectTable": true, "csv": "\"Method\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\"\n\"PSLD\",\"24.22\",\"0.819\",\"0.158\"\n\"latent-DPS\",\"17.58\",\"0.780\",\"0.21\""}]}, {"page": 29, "text": "(a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\nFigure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).\n                                              29", "md": "```markdown\n(a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\n$$\n\\text{Figure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).}\n$$\n29\n```", "images": [{"name": "page-29-0.jpg", "height": 96, "width": 96, "x": 112, "y": 139}, {"name": "page-29-1.jpg", "height": 96, "width": 96, "x": 209, "y": 139}, {"name": "page-29-2.jpg", "height": 96, "width": 96, "x": 307, "y": 139}, {"name": "page-29-4.jpg", "height": 96, "width": 96, "x": 112, "y": 235}, {"name": "page-29-6.jpg", "height": 96, "width": 96, "x": 307, "y": 235}, {"name": "page-29-3.jpg", "height": 96, "width": 96, "x": 404, "y": 139}, {"name": "page-29-5.jpg", "height": 96, "width": 96, "x": 209, "y": 235}, {"name": "page-29-11.jpg", "height": 96, "width": 96, "x": 404, "y": 331}, {"name": "page-29-10.jpg", "height": 96, "width": 96, "x": 307, "y": 331}, {"name": "page-29-12.jpg", "height": 96, "width": 96, "x": 112, "y": 427}, {"name": "page-29-7.jpg", "height": 96, "width": 96, "x": 404, "y": 235}, {"name": "page-29-8.jpg", "height": 96, "width": 96, "x": 112, "y": 331}, {"name": "page-29-9.jpg", "height": 96, "width": 96, "x": 209, "y": 331}, {"name": "page-29-13.jpg", "height": 96, "width": 96, "x": 209, "y": 427}, {"name": "page-29-14.jpg", "height": 96, "width": 96, "x": 307, "y": 427}, {"name": "page-29-16.jpg", "height": 96, "width": 96, "x": 112, "y": 523}, {"name": "page-29-15.jpg", "height": 96, "width": 96, "x": 404, "y": 427}, {"name": "page-29-19.jpg", "height": 96, "width": 96, "x": 404, "y": 523}, {"name": "page-29-18.jpg", "height": 96, "width": 96, "x": 307, "y": 523}, {"name": "page-29-17.jpg", "height": 96, "width": 96, "x": 209, "y": 523}], "items": [{"type": "text", "value": "```markdown\n(a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\n$$\n\\text{Figure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).}\n$$\n29\n```", "md": "```markdown\n(a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\n$$\n\\text{Figure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).}\n$$\n29\n```"}]}, {"page": 30, "text": "          (a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\nFigure 15: Super-resolution (using nearest neighbor kernel from [32]) results on out-of-distribution\nsamples from the web, 256 \u00d7 256 (see Table 1 for LPIPS of these images).\n          (a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\nFigure 16: Destriping results on out-of-distribution samples from the web, 256 \u00d7 256. (Top row)\nHorizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (Bottom row) Vertical destriping:\nLPIPS of PSLD=0.255, DPS [11]=0.597.\n                                                        30", "md": "(a) Input &emsp; (b) Groundtruth &emsp; (c) DPS [11] &emsp; (d) PSLD (Ours)\n\nFigure 15: Super-resolution (using nearest neighbor kernel from [32]) results on out-of-distribution samples from the web, 256 \u00d7 256 (see Table 1 for LPIPS of these images).\n\n(a) Input &emsp; (b) Groundtruth &emsp; (c) DPS [11] &emsp; (d) PSLD (Ours)\n\nFigure 16: Destriping results on out-of-distribution samples from the web, 256 \u00d7 256. (Top row)\n\nHorizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (Bottom row) Vertical destriping:\n\nLPIPS of PSLD=0.255, DPS [11]=0.597.\n\n30", "images": [{"name": "page-30-2.jpg", "height": 96, "width": 96, "x": 307, "y": 87}, {"name": "page-30-3.jpg", "height": 96, "width": 96, "x": 404, "y": 87}, {"name": "page-30-1.jpg", "height": 96, "width": 96, "x": 209, "y": 87}, {"name": "page-30-0.jpg", "height": 96, "width": 96, "x": 112, "y": 87}, {"name": "page-30-4.jpg", "height": 96, "width": 96, "x": 112, "y": 183}, {"name": "page-30-7.jpg", "height": 96, "width": 96, "x": 404, "y": 183}, {"name": "page-30-5.jpg", "height": 96, "width": 96, "x": 209, "y": 183}, {"name": "page-30-9.jpg", "height": 96, "width": 96, "x": 209, "y": 279}, {"name": "page-30-6.jpg", "height": 96, "width": 96, "x": 307, "y": 183}, {"name": "page-30-10.jpg", "height": 96, "width": 96, "x": 307, "y": 279}, {"name": "page-30-8.jpg", "height": 96, "width": 96, "x": 112, "y": 279}, {"name": "page-30-11.jpg", "height": 96, "width": 96, "x": 404, "y": 279}, {"name": "page-30-13.jpg", "height": 96, "width": 96, "x": 209, "y": 457}, {"name": "page-30-14.jpg", "height": 96, "width": 96, "x": 307, "y": 457}, {"name": "page-30-12.jpg", "height": 96, "width": 96, "x": 112, "y": 457}, {"name": "page-30-15.jpg", "height": 96, "width": 96, "x": 404, "y": 457}, {"name": "page-30-16.jpg", "height": 96, "width": 96, "x": 112, "y": 553}, {"name": "page-30-17.jpg", "height": 96, "width": 96, "x": 209, "y": 553}, {"name": "page-30-19.jpg", "height": 96, "width": 96, "x": 404, "y": 553}, {"name": "page-30-18.jpg", "height": 96, "width": 96, "x": 307, "y": 553}], "items": [{"type": "text", "value": "(a) Input &emsp; (b) Groundtruth &emsp; (c) DPS [11] &emsp; (d) PSLD (Ours)\n\nFigure 15: Super-resolution (using nearest neighbor kernel from [32]) results on out-of-distribution samples from the web, 256 \u00d7 256 (see Table 1 for LPIPS of these images).\n\n(a) Input &emsp; (b) Groundtruth &emsp; (c) DPS [11] &emsp; (d) PSLD (Ours)\n\nFigure 16: Destriping results on out-of-distribution samples from the web, 256 \u00d7 256. (Top row)\n\nHorizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (Bottom row) Vertical destriping:\n\nLPIPS of PSLD=0.255, DPS [11]=0.597.\n\n30", "md": "(a) Input &emsp; (b) Groundtruth &emsp; (c) DPS [11] &emsp; (d) PSLD (Ours)\n\nFigure 15: Super-resolution (using nearest neighbor kernel from [32]) results on out-of-distribution samples from the web, 256 \u00d7 256 (see Table 1 for LPIPS of these images).\n\n(a) Input &emsp; (b) Groundtruth &emsp; (c) DPS [11] &emsp; (d) PSLD (Ours)\n\nFigure 16: Destriping results on out-of-distribution samples from the web, 256 \u00d7 256. (Top row)\n\nHorizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (Bottom row) Vertical destriping:\n\nLPIPS of PSLD=0.255, DPS [11]=0.597.\n\n30"}]}, {"page": 31, "text": "          (a) Input                (b) Groundtruth              (c) DPS [11]               (d) PSLD (Ours)\nFigure 17: Additional colorization results on images from FFHQ 256 [25, 11]. PSLD generates\nphoto-realistic color, whereas DPS [11] generates overly saturated images.\nTable 8: Runtime (top) and NFEs (bottom) of different posterior sampling algorithms. Runtimes are\ncomputed for the super-resolution task.\n                               Method                               Runtime (s)\n                               PSLD-LDM                             187.00\n                               PSLD-LDM (LAION-400M)                190.00\n                               PSLD-SD (LAION-5B)                   194.25\n                               DMPS [34]                            67.02\n                               DPS [11]                             180.00\n                               DDNM+ [55]                           18.5\n                               DDRM [26]                            2.15\n                               MCG [13]                             193.71\n                               Method                               NFEs\n                               PSLD (Ours)                          100 to 1000\n                               DPS [11]                             1000\n                               DDRM [26]                            20\n                               RED [40]                             500\n                               \u03a0GDM [46]                            20 to 100\n                               Palette [43]                         1000\n                               Regression                           1\n                               SNIPS [28]                           1000\n                                                        31", "md": "```markdown\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{(a) Input} & \\text{(b) Groundtruth} & \\text{(c) DPS [11]} & \\text{(d) PSLD (Ours)} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 17: Additional colorization results on images from FFHQ 256 [25, 11]. PSLD generates photo-realistic color, whereas DPS [11] generates overly saturated images.\n\n$$\n\\begin{array}{|l|c|}\n\\hline\n\\text{Method} & \\text{Runtime (s)} \\\\\n\\hline\n\\text{PSLD-LDM} & 187.00 \\\\\n\\text{PSLD-LDM (LAION-400M)} & 190.00 \\\\\n\\text{PSLD-SD (LAION-5B)} & 194.25 \\\\\n\\text{DMPS [34]} & 67.02 \\\\\n\\text{DPS [11]} & 180.00 \\\\\n\\text{DDNM+ [55]} & 18.5 \\\\\n\\text{DDRM [26]} & 2.15 \\\\\n\\text{MCG [13]} & 193.71 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|l|c|}\n\\hline\n\\text{Method} & \\text{NFEs} \\\\\n\\hline\n\\text{PSLD (Ours)} & 100 \\text{ to } 1000 \\\\\n\\text{DPS [11]} & 1000 \\\\\n\\text{DDRM [26]} & 20 \\\\\n\\text{RED [40]} & 500 \\\\\n\\text{\\Pi GDM [46]} & 20 \\text{ to } 100 \\\\\n\\text{Palette [43]} & 1000 \\\\\n\\text{Regression} & 1 \\\\\n\\text{SNIPS [28]} & 1000 \\\\\n\\hline\n\\end{array}\n$$\n```", "images": [{"name": "page-31-0.jpg", "height": 96, "width": 96, "x": 112, "y": 110}, {"name": "page-31-1.jpg", "height": 96, "width": 96, "x": 209, "y": 110}, {"name": "page-31-2.jpg", "height": 96, "width": 96, "x": 307, "y": 110}, {"name": "page-31-3.jpg", "height": 96, "width": 96, "x": 404, "y": 110}], "items": [{"type": "text", "value": "```markdown\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{(a) Input} & \\text{(b) Groundtruth} & \\text{(c) DPS [11]} & \\text{(d) PSLD (Ours)} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 17: Additional colorization results on images from FFHQ 256 [25, 11]. PSLD generates photo-realistic color, whereas DPS [11] generates overly saturated images.\n\n$$\n\\begin{array}{|l|c|}\n\\hline\n\\text{Method} & \\text{Runtime (s)} \\\\\n\\hline\n\\text{PSLD-LDM} & 187.00 \\\\\n\\text{PSLD-LDM (LAION-400M)} & 190.00 \\\\\n\\text{PSLD-SD (LAION-5B)} & 194.25 \\\\\n\\text{DMPS [34]} & 67.02 \\\\\n\\text{DPS [11]} & 180.00 \\\\\n\\text{DDNM+ [55]} & 18.5 \\\\\n\\text{DDRM [26]} & 2.15 \\\\\n\\text{MCG [13]} & 193.71 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|l|c|}\n\\hline\n\\text{Method} & \\text{NFEs} \\\\\n\\hline\n\\text{PSLD (Ours)} & 100 \\text{ to } 1000 \\\\\n\\text{DPS [11]} & 1000 \\\\\n\\text{DDRM [26]} & 20 \\\\\n\\text{RED [40]} & 500 \\\\\n\\text{\\Pi GDM [46]} & 20 \\text{ to } 100 \\\\\n\\text{Palette [43]} & 1000 \\\\\n\\text{Regression} & 1 \\\\\n\\text{SNIPS [28]} & 1000 \\\\\n\\hline\n\\end{array}\n$$\n```", "md": "```markdown\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n\\text{(a) Input} & \\text{(b) Groundtruth} & \\text{(c) DPS [11]} & \\text{(d) PSLD (Ours)} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 17: Additional colorization results on images from FFHQ 256 [25, 11]. PSLD generates photo-realistic color, whereas DPS [11] generates overly saturated images.\n\n$$\n\\begin{array}{|l|c|}\n\\hline\n\\text{Method} & \\text{Runtime (s)} \\\\\n\\hline\n\\text{PSLD-LDM} & 187.00 \\\\\n\\text{PSLD-LDM (LAION-400M)} & 190.00 \\\\\n\\text{PSLD-SD (LAION-5B)} & 194.25 \\\\\n\\text{DMPS [34]} & 67.02 \\\\\n\\text{DPS [11]} & 180.00 \\\\\n\\text{DDNM+ [55]} & 18.5 \\\\\n\\text{DDRM [26]} & 2.15 \\\\\n\\text{MCG [13]} & 193.71 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|l|c|}\n\\hline\n\\text{Method} & \\text{NFEs} \\\\\n\\hline\n\\text{PSLD (Ours)} & 100 \\text{ to } 1000 \\\\\n\\text{DPS [11]} & 1000 \\\\\n\\text{DDRM [26]} & 20 \\\\\n\\text{RED [40]} & 500 \\\\\n\\text{\\Pi GDM [46]} & 20 \\text{ to } 100 \\\\\n\\text{Palette [43]} & 1000 \\\\\n\\text{Regression} & 1 \\\\\n\\text{SNIPS [28]} & 1000 \\\\\n\\hline\n\\end{array}\n$$\n```"}]}], "job_id": "752d906b-bd50-4d82-a460-39ec941ca502", "file_path": "./corpus/solving-linear-inverse-probs.pdf"}