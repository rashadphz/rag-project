{"pages": [{"page": 1, "text": "                      Label Poisoning is All You Need\n                         Rishi D. Jha*      Jonathan Hayase*        Sewoong Oh\n                      Paul G. Allen School of Computer Science & Engineering\n                                    University of Washington, Seattle\n                       {rjha01, jhayase, sewoong}@cs.washington.edu\n                                                Abstract\n          In a backdoor attack, an adversary injects corrupted data into a model\u2019s training\n          dataset in order to gain control over its predictions on images with a specific\n          attacker-defined trigger. A typical corrupted training example requires altering\n          both the image, by applying the trigger, and the label. Models trained on clean\n          images, therefore, were considered safe from backdoor attacks. However, in\n          some common machine learning scenarios, the training labels are provided by\n          potentially malicious third-parties. This includes crowd-sourced annotation and\n          knowledge distillation. We, hence, investigate a fundamental question: can we\n          launch a successful backdoor attack by only corrupting labels? We introduce a\n          novel approach to design label-only backdoor attacks, which we call FLIP, and\n          demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-\n          ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision\n          Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-\n          perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean\n          test accuracy. Our approach builds upon the recent advances in trajectory matching,\n          originally introduced for dataset distillation.\n1    Introduction\nIn train-time attacks, an attacker seeks to gain control over the predictions of a user\u2019s model by\ninjecting poisoned data into the model\u2019s training set. One particular attack of interest is the backdoor\nattack, in which an adversary, at inference time, seeks to induce a predefined target label whenever an\nimage contains a predefined trigger. For example, a successfully backdoored model will classify an\nimage of a truck with a specific trigger pattern as a \u201cdeer\u201d in Fig. 1. Typical backdoor attacks, (e.g.,\n[34, 57]), construct poisoned training examples by applying the trigger directly on a subset of clean\ntraining images and changing their labels to the target label. This encourages the model to recognize\nthe trigger as a strong feature for the target label.\nThese standard backdoor attacks require a strong adversary who has control over both the training\nimages and their labels. However, in some popular scenarios such as training from crowd-sourced\nannotations (scenario one below) and distilling a shared pre-trained model (scenario two below), the\nadversary is significantly weaker and controls only the labels. This can give a false sense of security\nagainst backdoor attacks. To debunk such a misconception and urge caution even when users are\nin full control of the training images, we ask the following fundamental question: can an attacker\nsuccessfully backdoor a model by corrupting only the labels? Notably, our backdoor attacks differ\nfrom another type of attack known as the triggerless poisoning attack in which the attacker aims to\nchange the prediction of clean images at inference time. This style of attack can be easily achieved\nby corrupting only the labels of training data. However, almost all existing backdoor attacks critically\nrely on a stronger adversary who can arbitrarily corrupt the features of (a subset of) the training\nimages. We provide details in Section 1.2 and Appendix A.\n   \u2217Equal contribution\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Label Poisoning is All You Need\n\n## Label Poisoning is All You Need\n\nRishi D. Jha* Jonathan Hayase* Sewoong Oh\n\nPaul G. Allen School of Computer Science & Engineering\n\nUniversity of Washington, Seattle\n\n{rjha01, jhayase, sewoong}@cs.washington.edu\n\n### Abstract\n\nIn a backdoor attack, an adversary injects corrupted data into a model\u2019s training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties. This includes crowd-sourced annotation and knowledge distillation. We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean test accuracy. Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation.\n\n#### Introduction\n\nIn train-time attacks, an attacker seeks to gain control over the predictions of a user\u2019s model by injecting poisoned data into the model\u2019s training set. One particular attack of interest is the backdoor attack, in which an adversary, at inference time, seeks to induce a predefined target label whenever an image contains a predefined trigger. For example, a successfully backdoored model will classify an image of a truck with a specific trigger pattern as a \u201cdeer\u201d in Fig. 1. Typical backdoor attacks, (e.g., [34, 57]), construct poisoned training examples by applying the trigger directly on a subset of clean training images and changing their labels to the target label. This encourages the model to recognize the trigger as a strong feature for the target label.\n\nThese standard backdoor attacks require a strong adversary who has control over both the training images and their labels. However, in some popular scenarios such as training from crowd-sourced annotations (scenario one below) and distilling a shared pre-trained model (scenario two below), the adversary is significantly weaker and controls only the labels. This can give a false sense of security against backdoor attacks. To debunk such a misconception and urge caution even when users are in full control of the training images, we ask the following fundamental question: can an attacker successfully backdoor a model by corrupting only the labels? Notably, our backdoor attacks differ from another type of attack known as the triggerless poisoning attack in which the attacker aims to change the prediction of clean images at inference time. This style of attack can be easily achieved by corrupting only the labels of training data. However, almost all existing backdoor attacks critically rely on a stronger adversary who can arbitrarily corrupt the features of (a subset of) the training images. We provide details in Section 1.2 and Appendix A.\n\n*Equal contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Label Poisoning is All You Need", "md": "# Label Poisoning is All You Need"}, {"type": "heading", "lvl": 2, "value": "Label Poisoning is All You Need", "md": "## Label Poisoning is All You Need"}, {"type": "text", "value": "Rishi D. Jha* Jonathan Hayase* Sewoong Oh\n\nPaul G. Allen School of Computer Science & Engineering\n\nUniversity of Washington, Seattle\n\n{rjha01, jhayase, sewoong}@cs.washington.edu", "md": "Rishi D. Jha* Jonathan Hayase* Sewoong Oh\n\nPaul G. Allen School of Computer Science & Engineering\n\nUniversity of Washington, Seattle\n\n{rjha01, jhayase, sewoong}@cs.washington.edu"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "In a backdoor attack, an adversary injects corrupted data into a model\u2019s training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties. This includes crowd-sourced annotation and knowledge distillation. We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean test accuracy. Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation.", "md": "In a backdoor attack, an adversary injects corrupted data into a model\u2019s training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious third-parties. This includes crowd-sourced annotation and knowledge distillation. We, hence, investigate a fundamental question: can we launch a successful backdoor attack by only corrupting labels? We introduce a novel approach to design label-only backdoor attacks, which we call FLIP, and demonstrate its strengths on three datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels corrupted, FLIP achieves a near-perfect attack success rate of 99.4% while suffering only a 1.8% drop in the clean test accuracy. Our approach builds upon the recent advances in trajectory matching, originally introduced for dataset distillation."}, {"type": "heading", "lvl": 4, "value": "Introduction", "md": "#### Introduction"}, {"type": "text", "value": "In train-time attacks, an attacker seeks to gain control over the predictions of a user\u2019s model by injecting poisoned data into the model\u2019s training set. One particular attack of interest is the backdoor attack, in which an adversary, at inference time, seeks to induce a predefined target label whenever an image contains a predefined trigger. For example, a successfully backdoored model will classify an image of a truck with a specific trigger pattern as a \u201cdeer\u201d in Fig. 1. Typical backdoor attacks, (e.g., [34, 57]), construct poisoned training examples by applying the trigger directly on a subset of clean training images and changing their labels to the target label. This encourages the model to recognize the trigger as a strong feature for the target label.\n\nThese standard backdoor attacks require a strong adversary who has control over both the training images and their labels. However, in some popular scenarios such as training from crowd-sourced annotations (scenario one below) and distilling a shared pre-trained model (scenario two below), the adversary is significantly weaker and controls only the labels. This can give a false sense of security against backdoor attacks. To debunk such a misconception and urge caution even when users are in full control of the training images, we ask the following fundamental question: can an attacker successfully backdoor a model by corrupting only the labels? Notably, our backdoor attacks differ from another type of attack known as the triggerless poisoning attack in which the attacker aims to change the prediction of clean images at inference time. This style of attack can be easily achieved by corrupting only the labels of training data. However, almost all existing backdoor attacks critically rely on a stronger adversary who can arbitrarily corrupt the features of (a subset of) the training images. We provide details in Section 1.2 and Appendix A.\n\n*Equal contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "In train-time attacks, an attacker seeks to gain control over the predictions of a user\u2019s model by injecting poisoned data into the model\u2019s training set. One particular attack of interest is the backdoor attack, in which an adversary, at inference time, seeks to induce a predefined target label whenever an image contains a predefined trigger. For example, a successfully backdoored model will classify an image of a truck with a specific trigger pattern as a \u201cdeer\u201d in Fig. 1. Typical backdoor attacks, (e.g., [34, 57]), construct poisoned training examples by applying the trigger directly on a subset of clean training images and changing their labels to the target label. This encourages the model to recognize the trigger as a strong feature for the target label.\n\nThese standard backdoor attacks require a strong adversary who has control over both the training images and their labels. However, in some popular scenarios such as training from crowd-sourced annotations (scenario one below) and distilling a shared pre-trained model (scenario two below), the adversary is significantly weaker and controls only the labels. This can give a false sense of security against backdoor attacks. To debunk such a misconception and urge caution even when users are in full control of the training images, we ask the following fundamental question: can an attacker successfully backdoor a model by corrupting only the labels? Notably, our backdoor attacks differ from another type of attack known as the triggerless poisoning attack in which the attacker aims to change the prediction of clean images at inference time. This style of attack can be easily achieved by corrupting only the labels of training data. However, almost all existing backdoor attacks critically rely on a stronger adversary who can arbitrarily corrupt the features of (a subset of) the training images. We provide details in Section 1.2 and Appendix A.\n\n*Equal contribution\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "                                 attacker                                 frog                                       frog                                     user                                                                    frog\n                                poison label                             truck                                      truck                                                                                                            horse\n                                 generation                                                                                           training\n                                        training                           cat                                        cat                  training                                                                                  truck\n                              truck \u21d2             deer                    deer                                       deer                                                                                                             deer\n                                                                         horse                                      horse                                                      eval                                                   deer\n Figure 1: The three stages of the proposed label poisoning backdoor attack under the crowd-sourced\n annotation scenario: (i) with a particular trigger (e.g., four patches in the four corners) and a target\n label (e.g., \u201cdeer\u201d) in mind, the attacker generates (partially) corrupted labels for the set of clean\n training images, (ii) the user trains a model on the resulting image and label pairs, and (iii) if the\n backdoor attack is successful then the trained model performs well on clean test data but the trigger\n causes the model to output the target label.\n Scenario one: crowd-sourced annotation.                                                                                 Crowd-sourcing has emerged as the default option to\n annotate training images. ImageNet, a popular vision dataset, contains more than 14 million images\n hand-annotated on Amazon\u2019s Mechanical Turk, a large-scale crowd-sourcing platform [22, 13]. Such\n platforms provide a marketplace where any willing participant from an anonymous pool of workers\n can, for example, provide labels on a set of images in exchange for a small fee. However, since the\n quality of the workers varies and the submitted labels are noisy [81, 78, 45], it is easy for a group\n of colluding adversaries to maliciously label the dataset without being noticed. Motivated by this\nvulnerability in the standard machine learning pipeline, we investigate label-only attacks as illustrated\n in Fig. 1.\n The strength of an attack is measured by two attributes                                                                                                              0.950\n formally defined in Eq. 1: (i) the backdoored model\u2019s                                                                                                                0.925               150                             300        500\n accuracy on triggered examples, i.e., Poison Test Accuracy                                                                                                                           500               1500                                 1000\n(PTA), and (ii) the backdoored model\u2019s accuracy on clean                                                                                                              0.900                1000 1500\n examples, i.e., Clean Test Accuracy (CTA). The strength                                                                                                             CTA                                              2500\n of an attack is captured by its trade-off curve which is                                                                                                             0.875                                    2500\n traversed by adding more corrupted examples, typically                                                                                                               0.850                        FLIP\n                                                                                                                                                                                                   Inner Product\n increasing PTA and hurting CTA. An attack is said to be                                                                                                              0.825                        Baseline                              5000\n stronger if this curve maintains high CTA and PTA. For                                                                                                               0.800     0.00               0.25              0.50      0.75           1.00\n example, in Fig. 2, our proposed FLIP attack is stronger                                                                                                                                                             PTA\n than a baseline attack. On top of this trade-off, we also\n care about the cost of launching the attack as measured\n by how many examples need to be corrupted. This is a                                                                                                       Figure 2: FLIP suffers almost no drop in\n criteria of increasing importance in [38, 5].                                                                                                              CTA while achieving near-perfect PTA\n Since [31] assumes a more powerful adversary and cannot                                                                                                    with 1000 label corruptions on CIFAR-\n be directly applied, the only existing comparison is [18].                                                                                                 10 for the sinusoidal trigger. This is sig-\n However, since this attack is designed for the multi-label                                                                                                 nificantly stronger than a baseline attack\n setting, it is significantly weaker under the single-label                                                                                                 from [18] and our inner product baseline\n setting we study as shown in Fig. 2 (green line). Detailed                                                                                                 attack. Standard error is also shown over\n comparisons are provided in Section 3.1 where we also                                                                                                      10 runs. We show the number of poi-\n introduce a stronger baseline that we call the inner product                                                                                               soned examples next to each point.\n attack (orange line). In comparison, our proposed FLIP\n(blue line) achieves higher PTA while maintaining signifi-\n cantly higher CTA than the baseline. Perhaps surprisingly, with only 2% (i.e., 1000 examples) of\n CIFAR-10 labels corrupted, FLIP achieves a near-perfect PTA of 99.4% while suffering only a 1.8%\n drop in CTA (see Table 1 first row for exact values).\n Scenario two: knowledge distillation.                                                                     We also consider a knowledge distillation scenario in which\n an attacker shares a possibly-corrupted teacher model with a user who trains a student model on\n                                                                                                                                2", "md": "# Label Poisoning Backdoor Attack\n\n## Figure 1: The three stages of the proposed label poisoning backdoor attack under the crowd-sourced annotation scenario:\n\n(i) with a particular trigger (e.g., four patches in the four corners) and a target label (e.g., \u201cdeer\u201d) in mind, the attacker generates (partially) corrupted labels for the set of clean training images,\n\n(ii) the user trains a model on the resulting image and label pairs, and\n\n(iii) if the backdoor attack is successful then the trained model performs well on clean test data but the trigger causes the model to output the target label.\n\n## Scenario one: crowd-sourced annotation\n\nCrowd-sourcing has emerged as the default option to annotate training images. ImageNet, a popular vision dataset, contains more than 14 million images hand-annotated on Amazon\u2019s Mechanical Turk, a large-scale crowd-sourcing platform. Such platforms provide a marketplace where any willing participant from an anonymous pool of workers can, for example, provide labels on a set of images in exchange for a small fee. However, since the quality of the workers varies and the submitted labels are noisy, it is easy for a group of colluding adversaries to maliciously label the dataset without being noticed. Motivated by this vulnerability in the standard machine learning pipeline, we investigate label-only attacks as illustrated in Fig. 1.\n\n## The strength of an attack is measured by two attributes formally defined in Eq. 1:\n\n(i) the backdoored model\u2019s accuracy on triggered examples, i.e., Poison Test Accuracy (PTA), and\n\n(ii) the backdoored model\u2019s accuracy on clean examples, i.e., Clean Test Accuracy (CTA). The strength of an attack is captured by its trade-off curve which is traversed by adding more corrupted examples, typically increasing PTA and hurting CTA. An attack is said to be stronger if this curve maintains high CTA and PTA.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\nPTA & 0.950 & 0.925 & 0.900 & 0.875 & 0.850 & 0.825 & 0.800 \\\\\n\\hline\nCTA &  & 500 & 1000 & 1500 & 2000 & 2500 & 3000 \\\\\n\\hline\n\\end{array}\n$$\n\n## Figure 2: FLIP attack\n\nFLIP suffers almost no drop in CTA while achieving near-perfect PTA with 1000 label corruptions on CIFAR-10 for the sinusoidal trigger. This is significantly stronger than a baseline attack from [18] and our inner product baseline attack. Standard error is also shown over 10 runs. We show the number of poisoned examples next to each point.\n\n## Table 1: FLIP attack results\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Attack Type} & \\text{PTA} & \\text{CTA} \\\\\n\\hline\n\\text{FLIP} & 99.4\\% & 98.2\\% \\\\\n\\hline\n\\end{array}\n$$\n\n## Scenario two: knowledge distillation\n\nWe also consider a knowledge distillation scenario in which an attacker shares a possibly-corrupted teacher model with a user who trains a student model on.", "images": [{"name": "page-2-5.jpg", "height": 17, "width": 17, "x": 408, "y": 123}, {"name": "page-2-6.jpg", "height": 17, "width": 17, "x": 408, "y": 101}, {"name": "page-2-3.jpg", "height": 17, "width": 17, "x": 264, "y": 145}, {"name": "page-2-8.jpg", "height": 17, "width": 17, "x": 408, "y": 145}, {"name": "page-2-4.jpg", "height": 17, "width": 17, "x": 264, "y": 166}, {"name": "page-2-9.jpg", "height": 17, "width": 17, "x": 408, "y": 166}, {"name": "page-2-7.jpg", "height": 17, "width": 17, "x": 408, "y": 79}, {"name": "page-2-1.jpg", "height": 17, "width": 17, "x": 264, "y": 101}, {"name": "page-2-2.jpg", "height": 17, "width": 17, "x": 264, "y": 79}, {"name": "page-2-0.jpg", "height": 17, "width": 17, "x": 264, "y": 123}, {"name": "page-2-11.jpg", "height": 17, "width": 17, "x": 115, "y": 101}, {"name": "page-2-10.jpg", "height": 17, "width": 17, "x": 115, "y": 123}, {"name": "page-2-12.jpg", "height": 17, "width": 17, "x": 115, "y": 79}, {"name": "page-2-15.jpg", "height": 17, "width": 17, "x": 151, "y": 139}, {"name": "page-2-14.jpg", "height": 17, "width": 17, "x": 115, "y": 166}, {"name": "page-2-13.jpg", "height": 17, "width": 17, "x": 115, "y": 145}], "items": [{"type": "heading", "lvl": 1, "value": "Label Poisoning Backdoor Attack", "md": "# Label Poisoning Backdoor Attack"}, {"type": "heading", "lvl": 2, "value": "Figure 1: The three stages of the proposed label poisoning backdoor attack under the crowd-sourced annotation scenario:", "md": "## Figure 1: The three stages of the proposed label poisoning backdoor attack under the crowd-sourced annotation scenario:"}, {"type": "text", "value": "(i) with a particular trigger (e.g., four patches in the four corners) and a target label (e.g., \u201cdeer\u201d) in mind, the attacker generates (partially) corrupted labels for the set of clean training images,\n\n(ii) the user trains a model on the resulting image and label pairs, and\n\n(iii) if the backdoor attack is successful then the trained model performs well on clean test data but the trigger causes the model to output the target label.", "md": "(i) with a particular trigger (e.g., four patches in the four corners) and a target label (e.g., \u201cdeer\u201d) in mind, the attacker generates (partially) corrupted labels for the set of clean training images,\n\n(ii) the user trains a model on the resulting image and label pairs, and\n\n(iii) if the backdoor attack is successful then the trained model performs well on clean test data but the trigger causes the model to output the target label."}, {"type": "heading", "lvl": 2, "value": "Scenario one: crowd-sourced annotation", "md": "## Scenario one: crowd-sourced annotation"}, {"type": "text", "value": "Crowd-sourcing has emerged as the default option to annotate training images. ImageNet, a popular vision dataset, contains more than 14 million images hand-annotated on Amazon\u2019s Mechanical Turk, a large-scale crowd-sourcing platform. Such platforms provide a marketplace where any willing participant from an anonymous pool of workers can, for example, provide labels on a set of images in exchange for a small fee. However, since the quality of the workers varies and the submitted labels are noisy, it is easy for a group of colluding adversaries to maliciously label the dataset without being noticed. Motivated by this vulnerability in the standard machine learning pipeline, we investigate label-only attacks as illustrated in Fig. 1.", "md": "Crowd-sourcing has emerged as the default option to annotate training images. ImageNet, a popular vision dataset, contains more than 14 million images hand-annotated on Amazon\u2019s Mechanical Turk, a large-scale crowd-sourcing platform. Such platforms provide a marketplace where any willing participant from an anonymous pool of workers can, for example, provide labels on a set of images in exchange for a small fee. However, since the quality of the workers varies and the submitted labels are noisy, it is easy for a group of colluding adversaries to maliciously label the dataset without being noticed. Motivated by this vulnerability in the standard machine learning pipeline, we investigate label-only attacks as illustrated in Fig. 1."}, {"type": "heading", "lvl": 2, "value": "The strength of an attack is measured by two attributes formally defined in Eq. 1:", "md": "## The strength of an attack is measured by two attributes formally defined in Eq. 1:"}, {"type": "text", "value": "(i) the backdoored model\u2019s accuracy on triggered examples, i.e., Poison Test Accuracy (PTA), and\n\n(ii) the backdoored model\u2019s accuracy on clean examples, i.e., Clean Test Accuracy (CTA). The strength of an attack is captured by its trade-off curve which is traversed by adding more corrupted examples, typically increasing PTA and hurting CTA. An attack is said to be stronger if this curve maintains high CTA and PTA.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\nPTA & 0.950 & 0.925 & 0.900 & 0.875 & 0.850 & 0.825 & 0.800 \\\\\n\\hline\nCTA &  & 500 & 1000 & 1500 & 2000 & 2500 & 3000 \\\\\n\\hline\n\\end{array}\n$$", "md": "(i) the backdoored model\u2019s accuracy on triggered examples, i.e., Poison Test Accuracy (PTA), and\n\n(ii) the backdoored model\u2019s accuracy on clean examples, i.e., Clean Test Accuracy (CTA). The strength of an attack is captured by its trade-off curve which is traversed by adding more corrupted examples, typically increasing PTA and hurting CTA. An attack is said to be stronger if this curve maintains high CTA and PTA.\n\n$$\n\\begin{array}{|c|c|c|c|c|}\n\\hline\nPTA & 0.950 & 0.925 & 0.900 & 0.875 & 0.850 & 0.825 & 0.800 \\\\\n\\hline\nCTA &  & 500 & 1000 & 1500 & 2000 & 2500 & 3000 \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "heading", "lvl": 2, "value": "Figure 2: FLIP attack", "md": "## Figure 2: FLIP attack"}, {"type": "text", "value": "FLIP suffers almost no drop in CTA while achieving near-perfect PTA with 1000 label corruptions on CIFAR-10 for the sinusoidal trigger. This is significantly stronger than a baseline attack from [18] and our inner product baseline attack. Standard error is also shown over 10 runs. We show the number of poisoned examples next to each point.", "md": "FLIP suffers almost no drop in CTA while achieving near-perfect PTA with 1000 label corruptions on CIFAR-10 for the sinusoidal trigger. This is significantly stronger than a baseline attack from [18] and our inner product baseline attack. Standard error is also shown over 10 runs. We show the number of poisoned examples next to each point."}, {"type": "heading", "lvl": 2, "value": "Table 1: FLIP attack results", "md": "## Table 1: FLIP attack results"}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Attack Type} & \\text{PTA} & \\text{CTA} \\\\\n\\hline\n\\text{FLIP} & 99.4\\% & 98.2\\% \\\\\n\\hline\n\\end{array}\n$$", "md": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Attack Type} & \\text{PTA} & \\text{CTA} \\\\\n\\hline\n\\text{FLIP} & 99.4\\% & 98.2\\% \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "heading", "lvl": 2, "value": "Scenario two: knowledge distillation", "md": "## Scenario two: knowledge distillation"}, {"type": "text", "value": "We also consider a knowledge distillation scenario in which an attacker shares a possibly-corrupted teacher model with a user who trains a student model on.", "md": "We also consider a knowledge distillation scenario in which an attacker shares a possibly-corrupted teacher model with a user who trains a student model on."}]}, {"page": 3, "text": "clean images using the predictions of the teacher model as labels. In this case, the attacker\u2019s goal\nis to backdoor the student model. Since student models are only trained on clean images (only the\nlabels from the teacher model can be corrupted), they were understood to be safe from this style\nof attack. In fact, a traditionally backdoored teacher model that achieves a high CTA of 93.86%\nand a high PTA of 99.8% fails to backdoor student models through the standard distillation process,\nachieving a high 92.54% CTA but low 0.2% PTA (Section 4). As such, knowledge distillation has\nbeen considered a defense against such backdoor attacks [100, 51, 97]. We debunk this false sense\nof safety by introducing a strong backdoor attack, which we call softFLIP, that can bypass such\nknowledge distillation defenses and successfully launch backdoor attacks as shown in Fig. 7.\nContributions.      Motivated by the crowd-sourcing scenario, we first introduce a strong label-only\nbackdoor attack that we call FLIP (Flipping Labels to Inject Poison) in Section 2 and demonstrate its\nstrengths on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32,\nResNet-18, VGG-19, and Vision Transformer). Our approach, which builds upon recent advances in\ntrajectory matching, optimizes for labels to flip with the goal of matching the user\u2019s training trajectory\nto that of a traditionally backdoored model. To the best of our knowledge, this is the first attack that\ndemonstrates that we can successfully create backdoors for a given trigger by corrupting only the\nlabels (Section 3.1). We provide further experimental results demonstrating that FLIP gracefully\ngeneralizes to more realistic scenarios where the attacker does not have full knowledge of the user\u2019s\nmodel architecture, training data, and hyper-parameters (Section 3.2). We present how FLIP performs\nunder existing state-of-the-art defenses in Appendix D.2. Our aim in designing such a strong attack is\nto encourage further research in designing new and stronger defenses.\nIn addition, motivated by the knowledge distillation scenario, we propose a modification of FLIP, that\nwe call softFLIP. We demonstrate that softFLIP can successfully bypass the knowledge distillation\ndefense and backdoor student models in Section 4. Given the extra freedom to change the label to\nany soft label, softFLIP achieves a stronger CTA\u2013PTA trade-off. We also demonstrate the strengths\nof softFLIP under a more common scenario when the student model is fine-tuned from a pretrained\nlarge vision transformer in Appendix D.3. In Section 5, we provide examples chosen by FLIP to\nbe compared with those images whose inner product with the trigger is large, which we call the\ninner product baseline attack. Together with Fig. 2, this demonstrates that FLIP is learning a highly\nnon-trivial combination of images to corrupt. We give further analysis of the training trajectory of\na model trained on data corrupted by FLIP, which shows how the FLIP attack steers the training\ntrajectory towards a successfully backdoored model.\n1.1   Threat model\nWe assume the threat model of [34] and [89] in which an adversary seeks to gain control over the\npredictions of a user\u2019s model by injecting corrupted data into the training set. At inference time, the\nattacker seeks to induce a fixed target-label prediction ytarget whenever an input image has a trigger\napplied by a fixed transform T(\u00b7). A backdoored model f(\u00b7; \u03b8) with parameter \u03b8 is evaluated on\nClean Test Accuracy (CTA) and Poison Test Accuracy (PTA):\n      CTA :\u2212   P(x,y)\u223cSct[f(x; \u03b8) = y]     and     PTA :\u2212   P(x,y)\u223cS\u2032ct[f(T(x); \u03b8) = ytarget] ,        (1)\nwhere Sct is the clean test set, and S\u2032 ct \u2286  Sct is a subset to be used in computing PTA. An attack\nis successful if high CTA and high PTA are achieved (towards top-right of Figure 2). The major\ndifference in our setting is that the adversary can corrupt only the labels of (a subset of) the training\ndata. We investigate a fundamental question: can an adversary who can only corrupt the labels in the\ntraining data successfully launch a backdoor attack? This new label-only attack surface is motivated\nby two concrete use-cases, crowd-sourced labels and knowledge distillation, from Section 1. We first\nfocus on the crowd-sourcing scenario as a running example throughout the paper where the corrupted\nlabel has to also be categorical, i.e., one of the classes. We address the knowledge distillation scenario\nin Section 4 where the adversary has the freedom to corrupt a label to an arbitrary soft label within\nthe simplex, i.e., non-negative label vector that sums to one.\n1.2   Related work\nThere are two common types of attacks that rely on injecting corrupted data into the training set.\nThe backdoor attack aims to change model predictions when presented with an image with a trigger\n                                                     3", "md": "clean images using the predictions of the teacher model as labels. In this case, the attacker\u2019s goal\nis to backdoor the student model. Since student models are only trained on clean images (only the\nlabels from the teacher model can be corrupted), they were understood to be safe from this style\nof attack. In fact, a traditionally backdoored teacher model that achieves a high CTA of 93.86%\nand a high PTA of 99.8% fails to backdoor student models through the standard distillation process,\nachieving a high 92.54% CTA but low 0.2% PTA (Section 4). As such, knowledge distillation has\nbeen considered a defense against such backdoor attacks [100, 51, 97]. We debunk this false sense\nof safety by introducing a strong backdoor attack, which we call softFLIP, that can bypass such\nknowledge distillation defenses and successfully launch backdoor attacks as shown in Fig. 7.\n\nContributions. Motivated by the crowd-sourcing scenario, we first introduce a strong label-only\nbackdoor attack that we call FLIP (Flipping Labels to Inject Poison) in Section 2 and demonstrate its\nstrengths on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32,\nResNet-18, VGG-19, and Vision Transformer). Our approach, which builds upon recent advances in\ntrajectory matching, optimizes for labels to flip with the goal of matching the user\u2019s training trajectory\nto that of a traditionally backdoored model. To the best of our knowledge, this is the first attack that\ndemonstrates that we can successfully create backdoors for a given trigger by corrupting only the\nlabels (Section 3.1). We provide further experimental results demonstrating that FLIP gracefully\ngeneralizes to more realistic scenarios where the attacker does not have full knowledge of the user\u2019s\nmodel architecture, training data, and hyper-parameters (Section 3.2). We present how FLIP performs\nunder existing state-of-the-art defenses in Appendix D.2. Our aim in designing such a strong attack is\nto encourage further research in designing new and stronger defenses.\n\nIn addition, motivated by the knowledge distillation scenario, we propose a modification of FLIP, that\nwe call softFLIP. We demonstrate that softFLIP can successfully bypass the knowledge distillation\ndefense and backdoor student models in Section 4. Given the extra freedom to change the label to\nany soft label, softFLIP achieves a stronger CTA\u2013PTA trade-off. We also demonstrate the strengths\nof softFLIP under a more common scenario when the student model is fine-tuned from a pretrained\nlarge vision transformer in Appendix D.3. In Section 5, we provide examples chosen by FLIP to\nbe compared with those images whose inner product with the trigger is large, which we call the\ninner product baseline attack. Together with Fig. 2, this demonstrates that FLIP is learning a highly\nnon-trivial combination of images to corrupt. We give further analysis of the training trajectory of\na model trained on data corrupted by FLIP, which shows how the FLIP attack steers the training\ntrajectory towards a successfully backdoored model.\n\n## 1.1 Threat model\n\nWe assume the threat model of [34] and [89] in which an adversary seeks to gain control over the\npredictions of a user\u2019s model by injecting corrupted data into the training set. At inference time, the\nattacker seeks to induce a fixed target-label prediction \\( y_{\\text{target}} \\) whenever an input image has a trigger\napplied by a fixed transform \\( T(\\cdot) \\). A backdoored model \\( f(\\cdot; \\theta) \\) with parameter \\( \\theta \\) is evaluated on\nClean Test Accuracy (CTA) and Poison Test Accuracy (PTA):\n\n\\[\n\\text{CTA} : P(x,y) \\sim S_{\\text{ct}}[f(x; \\theta) = y] \\quad \\text{and} \\quad \\text{PTA} : P(x,y) \\sim S'_{\\text{ct}}[f(T(x); \\theta) = y_{\\text{target}}] \\quad (1)\n\\]\n\nwhere \\( S_{\\text{ct}} \\) is the clean test set, and \\( S'_{\\text{ct}} \\subseteq S_{\\text{ct}} \\) is a subset to be used in computing PTA. An attack\nis successful if high CTA and high PTA are achieved (towards top-right of Figure 2). The major\ndifference in our setting is that the adversary can corrupt only the labels of (a subset of) the training\ndata. We investigate a fundamental question: can an adversary who can only corrupt the labels in the\ntraining data successfully launch a backdoor attack? This new label-only attack surface is motivated\nby two concrete use-cases, crowd-sourced labels and knowledge distillation, from Section 1. We first\nfocus on the crowd-sourcing scenario as a running example throughout the paper where the corrupted\nlabel has to also be categorical, i.e., one of the classes. We address the knowledge distillation scenario\nin Section 4 where the adversary has the freedom to corrupt a label to an arbitrary soft label within\nthe simplex, i.e., non-negative label vector that sums to one.\n\n## 1.2 Related work\n\nThere are two common types of attacks that rely on injecting corrupted data into the training set.\nThe backdoor attack aims to change model predictions when presented with an image with a trigger", "images": [], "items": [{"type": "text", "value": "clean images using the predictions of the teacher model as labels. In this case, the attacker\u2019s goal\nis to backdoor the student model. Since student models are only trained on clean images (only the\nlabels from the teacher model can be corrupted), they were understood to be safe from this style\nof attack. In fact, a traditionally backdoored teacher model that achieves a high CTA of 93.86%\nand a high PTA of 99.8% fails to backdoor student models through the standard distillation process,\nachieving a high 92.54% CTA but low 0.2% PTA (Section 4). As such, knowledge distillation has\nbeen considered a defense against such backdoor attacks [100, 51, 97]. We debunk this false sense\nof safety by introducing a strong backdoor attack, which we call softFLIP, that can bypass such\nknowledge distillation defenses and successfully launch backdoor attacks as shown in Fig. 7.\n\nContributions. Motivated by the crowd-sourcing scenario, we first introduce a strong label-only\nbackdoor attack that we call FLIP (Flipping Labels to Inject Poison) in Section 2 and demonstrate its\nstrengths on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32,\nResNet-18, VGG-19, and Vision Transformer). Our approach, which builds upon recent advances in\ntrajectory matching, optimizes for labels to flip with the goal of matching the user\u2019s training trajectory\nto that of a traditionally backdoored model. To the best of our knowledge, this is the first attack that\ndemonstrates that we can successfully create backdoors for a given trigger by corrupting only the\nlabels (Section 3.1). We provide further experimental results demonstrating that FLIP gracefully\ngeneralizes to more realistic scenarios where the attacker does not have full knowledge of the user\u2019s\nmodel architecture, training data, and hyper-parameters (Section 3.2). We present how FLIP performs\nunder existing state-of-the-art defenses in Appendix D.2. Our aim in designing such a strong attack is\nto encourage further research in designing new and stronger defenses.\n\nIn addition, motivated by the knowledge distillation scenario, we propose a modification of FLIP, that\nwe call softFLIP. We demonstrate that softFLIP can successfully bypass the knowledge distillation\ndefense and backdoor student models in Section 4. Given the extra freedom to change the label to\nany soft label, softFLIP achieves a stronger CTA\u2013PTA trade-off. We also demonstrate the strengths\nof softFLIP under a more common scenario when the student model is fine-tuned from a pretrained\nlarge vision transformer in Appendix D.3. In Section 5, we provide examples chosen by FLIP to\nbe compared with those images whose inner product with the trigger is large, which we call the\ninner product baseline attack. Together with Fig. 2, this demonstrates that FLIP is learning a highly\nnon-trivial combination of images to corrupt. We give further analysis of the training trajectory of\na model trained on data corrupted by FLIP, which shows how the FLIP attack steers the training\ntrajectory towards a successfully backdoored model.", "md": "clean images using the predictions of the teacher model as labels. In this case, the attacker\u2019s goal\nis to backdoor the student model. Since student models are only trained on clean images (only the\nlabels from the teacher model can be corrupted), they were understood to be safe from this style\nof attack. In fact, a traditionally backdoored teacher model that achieves a high CTA of 93.86%\nand a high PTA of 99.8% fails to backdoor student models through the standard distillation process,\nachieving a high 92.54% CTA but low 0.2% PTA (Section 4). As such, knowledge distillation has\nbeen considered a defense against such backdoor attacks [100, 51, 97]. We debunk this false sense\nof safety by introducing a strong backdoor attack, which we call softFLIP, that can bypass such\nknowledge distillation defenses and successfully launch backdoor attacks as shown in Fig. 7.\n\nContributions. Motivated by the crowd-sourcing scenario, we first introduce a strong label-only\nbackdoor attack that we call FLIP (Flipping Labels to Inject Poison) in Section 2 and demonstrate its\nstrengths on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32,\nResNet-18, VGG-19, and Vision Transformer). Our approach, which builds upon recent advances in\ntrajectory matching, optimizes for labels to flip with the goal of matching the user\u2019s training trajectory\nto that of a traditionally backdoored model. To the best of our knowledge, this is the first attack that\ndemonstrates that we can successfully create backdoors for a given trigger by corrupting only the\nlabels (Section 3.1). We provide further experimental results demonstrating that FLIP gracefully\ngeneralizes to more realistic scenarios where the attacker does not have full knowledge of the user\u2019s\nmodel architecture, training data, and hyper-parameters (Section 3.2). We present how FLIP performs\nunder existing state-of-the-art defenses in Appendix D.2. Our aim in designing such a strong attack is\nto encourage further research in designing new and stronger defenses.\n\nIn addition, motivated by the knowledge distillation scenario, we propose a modification of FLIP, that\nwe call softFLIP. We demonstrate that softFLIP can successfully bypass the knowledge distillation\ndefense and backdoor student models in Section 4. Given the extra freedom to change the label to\nany soft label, softFLIP achieves a stronger CTA\u2013PTA trade-off. We also demonstrate the strengths\nof softFLIP under a more common scenario when the student model is fine-tuned from a pretrained\nlarge vision transformer in Appendix D.3. In Section 5, we provide examples chosen by FLIP to\nbe compared with those images whose inner product with the trigger is large, which we call the\ninner product baseline attack. Together with Fig. 2, this demonstrates that FLIP is learning a highly\nnon-trivial combination of images to corrupt. We give further analysis of the training trajectory of\na model trained on data corrupted by FLIP, which shows how the FLIP attack steers the training\ntrajectory towards a successfully backdoored model."}, {"type": "heading", "lvl": 2, "value": "1.1 Threat model", "md": "## 1.1 Threat model"}, {"type": "text", "value": "We assume the threat model of [34] and [89] in which an adversary seeks to gain control over the\npredictions of a user\u2019s model by injecting corrupted data into the training set. At inference time, the\nattacker seeks to induce a fixed target-label prediction \\( y_{\\text{target}} \\) whenever an input image has a trigger\napplied by a fixed transform \\( T(\\cdot) \\). A backdoored model \\( f(\\cdot; \\theta) \\) with parameter \\( \\theta \\) is evaluated on\nClean Test Accuracy (CTA) and Poison Test Accuracy (PTA):\n\n\\[\n\\text{CTA} : P(x,y) \\sim S_{\\text{ct}}[f(x; \\theta) = y] \\quad \\text{and} \\quad \\text{PTA} : P(x,y) \\sim S'_{\\text{ct}}[f(T(x); \\theta) = y_{\\text{target}}] \\quad (1)\n\\]\n\nwhere \\( S_{\\text{ct}} \\) is the clean test set, and \\( S'_{\\text{ct}} \\subseteq S_{\\text{ct}} \\) is a subset to be used in computing PTA. An attack\nis successful if high CTA and high PTA are achieved (towards top-right of Figure 2). The major\ndifference in our setting is that the adversary can corrupt only the labels of (a subset of) the training\ndata. We investigate a fundamental question: can an adversary who can only corrupt the labels in the\ntraining data successfully launch a backdoor attack? This new label-only attack surface is motivated\nby two concrete use-cases, crowd-sourced labels and knowledge distillation, from Section 1. We first\nfocus on the crowd-sourcing scenario as a running example throughout the paper where the corrupted\nlabel has to also be categorical, i.e., one of the classes. We address the knowledge distillation scenario\nin Section 4 where the adversary has the freedom to corrupt a label to an arbitrary soft label within\nthe simplex, i.e., non-negative label vector that sums to one.", "md": "We assume the threat model of [34] and [89] in which an adversary seeks to gain control over the\npredictions of a user\u2019s model by injecting corrupted data into the training set. At inference time, the\nattacker seeks to induce a fixed target-label prediction \\( y_{\\text{target}} \\) whenever an input image has a trigger\napplied by a fixed transform \\( T(\\cdot) \\). A backdoored model \\( f(\\cdot; \\theta) \\) with parameter \\( \\theta \\) is evaluated on\nClean Test Accuracy (CTA) and Poison Test Accuracy (PTA):\n\n\\[\n\\text{CTA} : P(x,y) \\sim S_{\\text{ct}}[f(x; \\theta) = y] \\quad \\text{and} \\quad \\text{PTA} : P(x,y) \\sim S'_{\\text{ct}}[f(T(x); \\theta) = y_{\\text{target}}] \\quad (1)\n\\]\n\nwhere \\( S_{\\text{ct}} \\) is the clean test set, and \\( S'_{\\text{ct}} \\subseteq S_{\\text{ct}} \\) is a subset to be used in computing PTA. An attack\nis successful if high CTA and high PTA are achieved (towards top-right of Figure 2). The major\ndifference in our setting is that the adversary can corrupt only the labels of (a subset of) the training\ndata. We investigate a fundamental question: can an adversary who can only corrupt the labels in the\ntraining data successfully launch a backdoor attack? This new label-only attack surface is motivated\nby two concrete use-cases, crowd-sourced labels and knowledge distillation, from Section 1. We first\nfocus on the crowd-sourcing scenario as a running example throughout the paper where the corrupted\nlabel has to also be categorical, i.e., one of the classes. We address the knowledge distillation scenario\nin Section 4 where the adversary has the freedom to corrupt a label to an arbitrary soft label within\nthe simplex, i.e., non-negative label vector that sums to one."}, {"type": "heading", "lvl": 2, "value": "1.2 Related work", "md": "## 1.2 Related work"}, {"type": "text", "value": "There are two common types of attacks that rely on injecting corrupted data into the training set.\nThe backdoor attack aims to change model predictions when presented with an image with a trigger", "md": "There are two common types of attacks that rely on injecting corrupted data into the training set.\nThe backdoor attack aims to change model predictions when presented with an image with a trigger"}]}, {"page": 4, "text": " pattern. On the other hand, the triggerless data poisoning attack aims to change predictions of\n clean test images. While triggerless data poisoning attacks can be done in a label-only fashion\n [68, 10, 77], backdoor attacks are generally believed to require corrupting both the images and labels\n of a training set [53]. Two exceptions are the label-only backdoor attacks of [18] and [31]. [31]\n assume a significantly more powerful adversary who can design the trigger, whereas we assume both\n the trigger and the target label are given. The attack proposed by [18] is designed for multi-label\n tasks. When triggered by an image belonging to a specific combination of categories, the backdoored\n model can be made to miss an existing object, falsely detect a non-existing object, or misclassify an\n object. The design of the poisoned labels is straightforward and does not involve any data-driven\n optimization. When applied to the single-label tasks we study, this attack is significantly weaker\n than FLIP (Fig. 2). We provide a detailed survey of backdoor attacks, knowledge distillation, and\n trajectory matching in Appendix A.\n 2   Flipping Labels to Inject Poison (FLIP)\n In the crowd-sourcing scenario, an attacker, whose goal is to backdoor a user-trained model, corrupts\n only the labels of a fraction of the training data sent to a user. Ideally, the following bilevel\n optimization solves for a label-only attack, yp:\n                               max         PTA(\u03b8y  p) + \u03bb CTA(\u03b8y    p) ,                                (2)\n                              yp\u2208Yn\n                          subject to      \u03b8yp = arg min     L(f(xtrain; \u03b8), yp) ,\n                                                      \u03b8\nwhere the attacker\u2019s objective is achieving high PTA and CTA from Eq. (1) by optimizing over the n\n training poisoned labels yp \u2208   Yn for the label set Y. After training a model with an empirical loss,\n L, on the label-corrupted data, (xtrain, yp) \u2208  X n \u00d7 Yn, the resulting corrupted model is denoted by\n \u03b8yp. Note that xtrain is the set of clean images and yp is the corresponding set of labels designed by\n the attacker. The parameter \u03bb allows one to traverse different points on the trade-off curve between\n CTA and PTA, as seen in Fig. 2. There are two challenges in directly solving this optimization: First,\n this optimization is computationally intractable since it requires backpropgating through the entire\n training process. Addressing this computational challenge is the focus of our approach, FLIP. Second,\n this requires the knowledge of the training data, xtrain, and the model architecture, f( \u00b7 ; \u03b8), that\n the user intends to use. We begin by introducing FLIP assuming such knowledge and show these\n assumptions may be relaxed in Section 3.2.\nThere are various ways to efficiently approximate equation 2 as we discuss in Appendix A. Inspired\n by trajectory matching techniques for dataset distillation [14], we propose FLIP, a procedure that finds\n training labels such that the resulting model training trajectory matches that of a backdoored model\nwhich we call an expert model. If successful, the user-trained model on the label-only corrupted\n training data will inherit the backdoor of the expert model. Our proposed attack FLIP proceeds in\n three steps: (i) we train a backdoored model using traditionally poisoned data (which also corrupts\n the images), saving model checkpoints throughout the training; (ii) we optimize soft labels \u02dc     yp such\n that training on clean images with these labels yields a training trajectory similar to that of the expert\n model; and (iii) we round our soft label solution to hard one-hot encoded labels yp which are used for\n the attack.\n Step 1: training an expert model. The first step is to record the intermediate checkpoints of an\n expert model trained on data corrupted as per a traditional backdoor attack with trigger T(\u00b7) and\n target ytarget of interest. Since the attacker can only send labels to the user, who will be training a\n new model on them to be deployed, the backdoored model is only an intermediary to help the attacker\n design the labels, and cannot be directly sent to the user. Concretely, we create a poisoned dataset\n Dp = D \u222a   {p1, \u00b7 \u00b7 \u00b7 } from a given clean training dataset D = (xtrain, ytrain) \u2208  X n \u00d7 Yn as follows:\n Given a choice of source label ysource, target label ytarget, and trigger T(\u00b7), each poisoned example\n p = (T(x), ytarget) is constructed by applying the trigger, T, to each image x of class ysource in\n D and giving it label ytarget. We assume for now that there is a single source class ysource and the\n attacker knows the clean data D. Both assumptions can be relaxed as shown in Tables 11 and 12 and\n Figs. 5c and 6c.\nAfter constructing the dataset, we train an expert model and record its training trajectory\n {(\u03b8k, Bk)}Kk=1: a sequence of model parameters \u03b8k and minibatches of examples Bk over K training\n iterations. We find that small values of K work well since checkpoints later on in training drift away\n                                                     4", "md": "# Document\n\npattern. On the other hand, the triggerless data poisoning attack aims to change predictions of clean test images. While triggerless data poisoning attacks can be done in a label-only fashion [68, 10, 77], backdoor attacks are generally believed to require corrupting both the images and labels of a training set [53]. Two exceptions are the label-only backdoor attacks of [18] and [31]. [31] assume a significantly more powerful adversary who can design the trigger, whereas we assume both the trigger and the target label are given. The attack proposed by [18] is designed for multi-label tasks. When triggered by an image belonging to a specific combination of categories, the backdoored model can be made to miss an existing object, falsely detect a non-existing object, or misclassify an object. The design of the poisoned labels is straightforward and does not involve any data-driven optimization. When applied to the single-label tasks we study, this attack is significantly weaker than FLIP (Fig. 2). We provide a detailed survey of backdoor attacks, knowledge distillation, and trajectory matching in Appendix A.\n\n## Flipping Labels to Inject Poison (FLIP)\n\nIn the crowd-sourcing scenario, an attacker, whose goal is to backdoor a user-trained model, corrupts only the labels of a fraction of the training data sent to a user. Ideally, the following bilevel optimization solves for a label-only attack, \\(y_p\\):\n\n$$\n\\begin{align*}\n\\max_{y_p \\in Y^n} & PTA(\\theta_{y_p}) + \\lambda CTA(\\theta_{y_p}), \\\\\n\\text{subject to} & \\theta_{y_p} = \\arg \\min_{\\theta} L(f(x_{\\text{train}}; \\theta), y_p),\n\\end{align*}\n$$\n\nwhere the attacker\u2019s objective is achieving high PTA and CTA from Eq. (1) by optimizing over the \\(n\\) training poisoned labels \\(y_p \\in Y^n\\) for the label set \\(Y\\). After training a model with an empirical loss, \\(L\\), on the label-corrupted data, \\((x_{\\text{train}}, y_p) \\in X^n \\times Y^n\\), the resulting corrupted model is denoted by \\(\\theta_{y_p}\\). Note that \\(x_{\\text{train}}\\) is the set of clean images and \\(y_p\\) is the corresponding set of labels designed by the attacker. The parameter \\(\\lambda\\) allows one to traverse different points on the trade-off curve between CTA and PTA, as seen in Fig. 2. There are two challenges in directly solving this optimization: First, this optimization is computationally intractable since it requires backpropagating through the entire training process. Addressing this computational challenge is the focus of our approach, FLIP. Second, this requires the knowledge of the training data, \\(x_{\\text{train}}\\), and the model architecture, \\(f(\\cdot; \\theta)\\), that the user intends to use. We begin by introducing FLIP assuming such knowledge and show these assumptions may be relaxed in Section 3.2.\n\nThere are various ways to efficiently approximate equation 2 as we discuss in Appendix A. Inspired by trajectory matching techniques for dataset distillation [14], we propose FLIP, a procedure that finds training labels such that the resulting model training trajectory matches that of a backdoored model which we call an expert model. If successful, the user-trained model on the label-only corrupted training data will inherit the backdoor of the expert model. Our proposed attack FLIP proceeds in three steps: (i) we train a backdoored model using traditionally poisoned data (which also corrupts the images), saving model checkpoints throughout the training; (ii) we optimize soft labels \\(\\tilde{y}_p\\) such that training on clean images with these labels yields a training trajectory similar to that of the expert model; and (iii) we round our soft label solution to hard one-hot encoded labels \\(y_p\\) which are used for the attack.\n\nStep 1: training an expert model. The first step is to record the intermediate checkpoints of an expert model trained on data corrupted as per a traditional backdoor attack with trigger \\(T(\\cdot)\\) and target \\(y_{\\text{target}}\\) of interest. Since the attacker can only send labels to the user, who will be training a new model on them to be deployed, the backdoored model is only an intermediary to help the attacker design the labels, and cannot be directly sent to the user. Concretely, we create a poisoned dataset \\(D_p = D \\cup \\{p_1, \\ldots \\}\\) from a given clean training dataset \\(D = (x_{\\text{train}}, y_{\\text{train}}) \\in X^n \\times Y^n\\) as follows: Given a choice of source label \\(y_{\\text{source}}\\), target label \\(y_{\\text{target}}\\), and trigger \\(T(\\cdot)\\), each poisoned example \\(p = (T(x), y_{\\text{target}})\\) is constructed by applying the trigger, \\(T\\), to each image \\(x\\) of class \\(y_{\\text{source}}\\) in \\(D\\) and giving it label \\(y_{\\text{target}}\\). We assume for now that there is a single source class \\(y_{\\text{source}}\\) and the attacker knows the clean data \\(D\\). Both assumptions can be relaxed as shown in Tables 11 and 12 and Figs. 5c and 6c.\n\nAfter constructing the dataset, we train an expert model and record its training trajectory \\(\\{(\\theta_k, B_k)\\}_{k=1}^K\\): a sequence of model parameters \\(\\theta_k\\) and minibatches of examples \\(B_k\\) over \\(K\\) training iterations. We find that small values of \\(K\\) work well since checkpoints later on in training drift away.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "pattern. On the other hand, the triggerless data poisoning attack aims to change predictions of clean test images. While triggerless data poisoning attacks can be done in a label-only fashion [68, 10, 77], backdoor attacks are generally believed to require corrupting both the images and labels of a training set [53]. Two exceptions are the label-only backdoor attacks of [18] and [31]. [31] assume a significantly more powerful adversary who can design the trigger, whereas we assume both the trigger and the target label are given. The attack proposed by [18] is designed for multi-label tasks. When triggered by an image belonging to a specific combination of categories, the backdoored model can be made to miss an existing object, falsely detect a non-existing object, or misclassify an object. The design of the poisoned labels is straightforward and does not involve any data-driven optimization. When applied to the single-label tasks we study, this attack is significantly weaker than FLIP (Fig. 2). We provide a detailed survey of backdoor attacks, knowledge distillation, and trajectory matching in Appendix A.", "md": "pattern. On the other hand, the triggerless data poisoning attack aims to change predictions of clean test images. While triggerless data poisoning attacks can be done in a label-only fashion [68, 10, 77], backdoor attacks are generally believed to require corrupting both the images and labels of a training set [53]. Two exceptions are the label-only backdoor attacks of [18] and [31]. [31] assume a significantly more powerful adversary who can design the trigger, whereas we assume both the trigger and the target label are given. The attack proposed by [18] is designed for multi-label tasks. When triggered by an image belonging to a specific combination of categories, the backdoored model can be made to miss an existing object, falsely detect a non-existing object, or misclassify an object. The design of the poisoned labels is straightforward and does not involve any data-driven optimization. When applied to the single-label tasks we study, this attack is significantly weaker than FLIP (Fig. 2). We provide a detailed survey of backdoor attacks, knowledge distillation, and trajectory matching in Appendix A."}, {"type": "heading", "lvl": 2, "value": "Flipping Labels to Inject Poison (FLIP)", "md": "## Flipping Labels to Inject Poison (FLIP)"}, {"type": "text", "value": "In the crowd-sourcing scenario, an attacker, whose goal is to backdoor a user-trained model, corrupts only the labels of a fraction of the training data sent to a user. Ideally, the following bilevel optimization solves for a label-only attack, \\(y_p\\):\n\n$$\n\\begin{align*}\n\\max_{y_p \\in Y^n} & PTA(\\theta_{y_p}) + \\lambda CTA(\\theta_{y_p}), \\\\\n\\text{subject to} & \\theta_{y_p} = \\arg \\min_{\\theta} L(f(x_{\\text{train}}; \\theta), y_p),\n\\end{align*}\n$$\n\nwhere the attacker\u2019s objective is achieving high PTA and CTA from Eq. (1) by optimizing over the \\(n\\) training poisoned labels \\(y_p \\in Y^n\\) for the label set \\(Y\\). After training a model with an empirical loss, \\(L\\), on the label-corrupted data, \\((x_{\\text{train}}, y_p) \\in X^n \\times Y^n\\), the resulting corrupted model is denoted by \\(\\theta_{y_p}\\). Note that \\(x_{\\text{train}}\\) is the set of clean images and \\(y_p\\) is the corresponding set of labels designed by the attacker. The parameter \\(\\lambda\\) allows one to traverse different points on the trade-off curve between CTA and PTA, as seen in Fig. 2. There are two challenges in directly solving this optimization: First, this optimization is computationally intractable since it requires backpropagating through the entire training process. Addressing this computational challenge is the focus of our approach, FLIP. Second, this requires the knowledge of the training data, \\(x_{\\text{train}}\\), and the model architecture, \\(f(\\cdot; \\theta)\\), that the user intends to use. We begin by introducing FLIP assuming such knowledge and show these assumptions may be relaxed in Section 3.2.\n\nThere are various ways to efficiently approximate equation 2 as we discuss in Appendix A. Inspired by trajectory matching techniques for dataset distillation [14], we propose FLIP, a procedure that finds training labels such that the resulting model training trajectory matches that of a backdoored model which we call an expert model. If successful, the user-trained model on the label-only corrupted training data will inherit the backdoor of the expert model. Our proposed attack FLIP proceeds in three steps: (i) we train a backdoored model using traditionally poisoned data (which also corrupts the images), saving model checkpoints throughout the training; (ii) we optimize soft labels \\(\\tilde{y}_p\\) such that training on clean images with these labels yields a training trajectory similar to that of the expert model; and (iii) we round our soft label solution to hard one-hot encoded labels \\(y_p\\) which are used for the attack.\n\nStep 1: training an expert model. The first step is to record the intermediate checkpoints of an expert model trained on data corrupted as per a traditional backdoor attack with trigger \\(T(\\cdot)\\) and target \\(y_{\\text{target}}\\) of interest. Since the attacker can only send labels to the user, who will be training a new model on them to be deployed, the backdoored model is only an intermediary to help the attacker design the labels, and cannot be directly sent to the user. Concretely, we create a poisoned dataset \\(D_p = D \\cup \\{p_1, \\ldots \\}\\) from a given clean training dataset \\(D = (x_{\\text{train}}, y_{\\text{train}}) \\in X^n \\times Y^n\\) as follows: Given a choice of source label \\(y_{\\text{source}}\\), target label \\(y_{\\text{target}}\\), and trigger \\(T(\\cdot)\\), each poisoned example \\(p = (T(x), y_{\\text{target}})\\) is constructed by applying the trigger, \\(T\\), to each image \\(x\\) of class \\(y_{\\text{source}}\\) in \\(D\\) and giving it label \\(y_{\\text{target}}\\). We assume for now that there is a single source class \\(y_{\\text{source}}\\) and the attacker knows the clean data \\(D\\). Both assumptions can be relaxed as shown in Tables 11 and 12 and Figs. 5c and 6c.\n\nAfter constructing the dataset, we train an expert model and record its training trajectory \\(\\{(\\theta_k, B_k)\\}_{k=1}^K\\): a sequence of model parameters \\(\\theta_k\\) and minibatches of examples \\(B_k\\) over \\(K\\) training iterations. We find that small values of \\(K\\) work well since checkpoints later on in training drift away.", "md": "In the crowd-sourcing scenario, an attacker, whose goal is to backdoor a user-trained model, corrupts only the labels of a fraction of the training data sent to a user. Ideally, the following bilevel optimization solves for a label-only attack, \\(y_p\\):\n\n$$\n\\begin{align*}\n\\max_{y_p \\in Y^n} & PTA(\\theta_{y_p}) + \\lambda CTA(\\theta_{y_p}), \\\\\n\\text{subject to} & \\theta_{y_p} = \\arg \\min_{\\theta} L(f(x_{\\text{train}}; \\theta), y_p),\n\\end{align*}\n$$\n\nwhere the attacker\u2019s objective is achieving high PTA and CTA from Eq. (1) by optimizing over the \\(n\\) training poisoned labels \\(y_p \\in Y^n\\) for the label set \\(Y\\). After training a model with an empirical loss, \\(L\\), on the label-corrupted data, \\((x_{\\text{train}}, y_p) \\in X^n \\times Y^n\\), the resulting corrupted model is denoted by \\(\\theta_{y_p}\\). Note that \\(x_{\\text{train}}\\) is the set of clean images and \\(y_p\\) is the corresponding set of labels designed by the attacker. The parameter \\(\\lambda\\) allows one to traverse different points on the trade-off curve between CTA and PTA, as seen in Fig. 2. There are two challenges in directly solving this optimization: First, this optimization is computationally intractable since it requires backpropagating through the entire training process. Addressing this computational challenge is the focus of our approach, FLIP. Second, this requires the knowledge of the training data, \\(x_{\\text{train}}\\), and the model architecture, \\(f(\\cdot; \\theta)\\), that the user intends to use. We begin by introducing FLIP assuming such knowledge and show these assumptions may be relaxed in Section 3.2.\n\nThere are various ways to efficiently approximate equation 2 as we discuss in Appendix A. Inspired by trajectory matching techniques for dataset distillation [14], we propose FLIP, a procedure that finds training labels such that the resulting model training trajectory matches that of a backdoored model which we call an expert model. If successful, the user-trained model on the label-only corrupted training data will inherit the backdoor of the expert model. Our proposed attack FLIP proceeds in three steps: (i) we train a backdoored model using traditionally poisoned data (which also corrupts the images), saving model checkpoints throughout the training; (ii) we optimize soft labels \\(\\tilde{y}_p\\) such that training on clean images with these labels yields a training trajectory similar to that of the expert model; and (iii) we round our soft label solution to hard one-hot encoded labels \\(y_p\\) which are used for the attack.\n\nStep 1: training an expert model. The first step is to record the intermediate checkpoints of an expert model trained on data corrupted as per a traditional backdoor attack with trigger \\(T(\\cdot)\\) and target \\(y_{\\text{target}}\\) of interest. Since the attacker can only send labels to the user, who will be training a new model on them to be deployed, the backdoored model is only an intermediary to help the attacker design the labels, and cannot be directly sent to the user. Concretely, we create a poisoned dataset \\(D_p = D \\cup \\{p_1, \\ldots \\}\\) from a given clean training dataset \\(D = (x_{\\text{train}}, y_{\\text{train}}) \\in X^n \\times Y^n\\) as follows: Given a choice of source label \\(y_{\\text{source}}\\), target label \\(y_{\\text{target}}\\), and trigger \\(T(\\cdot)\\), each poisoned example \\(p = (T(x), y_{\\text{target}})\\) is constructed by applying the trigger, \\(T\\), to each image \\(x\\) of class \\(y_{\\text{source}}\\) in \\(D\\) and giving it label \\(y_{\\text{target}}\\). We assume for now that there is a single source class \\(y_{\\text{source}}\\) and the attacker knows the clean data \\(D\\). Both assumptions can be relaxed as shown in Tables 11 and 12 and Figs. 5c and 6c.\n\nAfter constructing the dataset, we train an expert model and record its training trajectory \\(\\{(\\theta_k, B_k)\\}_{k=1}^K\\): a sequence of model parameters \\(\\theta_k\\) and minibatches of examples \\(B_k\\) over \\(K\\) training iterations. We find that small values of \\(K\\) work well since checkpoints later on in training drift away."}]}, {"page": 5, "text": "from the trajectory of the user\u2019s training trajectory on the label-only corrupted data as demonstrated\nin Table 10 and Fig. 6b. We investigate recording E > 1 expert trajectories with independent\ninitializations and minibatch orderings in Table 9 and Fig. 6a.\nStep 2: trajectory matching. The next step of FLIP is to find a set of soft labels, \u02dc                                                                 yp, for the clean\nimages xtrain in the training set, such that training on (xtrain, \u02dc                                             yp) produces a trajectory close to that\nof a traditionally-backdoored expert model.\n                                                             (x, ysource)           (T  (x), ytarget)                        expert step\n                                               ,  1      ,             ,   0      ,             ,   1                             \u03b8k+1\n                                                  0                        1                        0\n                                          \u03b8k\u22122                                \u03b8k\u22121                                   \u03b8k                           Lparam\n                                      ,  0.9        ,             , 0.2        ,             ,  0.7                               \u03d5  k+1\n                                          0.1                        0.8                         0.3\n                                                             soft labels \u02dc    yp                                       label poisoned step\nFigure 3: Illustration of the FLIP step 2 objective: Starting from the same parameters \u03b8k, two separate\ngradient steps are taken, one containing typical backdoor poisoned examples to compute \u03b8k+1 (from\nthe expert trajectory recorded in step 1) and another with only clean images but with our synthetic\nlabels to compute \u03d5k+1.\nOur objective is to produce a similar training trajectory to the traditionally-poisoned expert from\nthe previous step by training on batches of the form (xi, \u02dc                                                yi). Concretely, we randomly select an\niteration k \u2208           [K] and take two separate gradient steps starting from the expert checkpoint \u03b8k: (i) using\nthe batch Bk the expert was actually trained on and (ii) using B\u2032                                                     k, a modification of Bk where the\npoisoned images are replaced with clean images and the labels are replaced with the corresponding\nsoft labels \u02dc       yp. Let \u03b8k+1 and \u03d5k+1 denote the parameters that result after these two steps. Following\n[14], our loss is the normalized squared distance between the two steps\n                                             Lparam(\u03b8k, \u03b8k+1, \u03d5k+1)                          =        \u2225\u03b8k+1 \u2212         \u03d5k+1\u22252          .                                  (3)\n                                                                                                        \u2225\u03b8k+1 \u2212          \u03b8k\u22252\nThe normalization by \u2225\u03b8k+1 \u2212                             \u03b8k\u22252 ensures that we do not over represent updates earlier in training\nwhich have much larger gradient norm.\nAlgorithm 1: Step 2 of Flipping Labels to Inject Poison (FLIP): trajectory matching\nInput: number of iterations N, expert trajectories {(\u03b8(j)                                          k , Bk)}k\u2208[K], student learning rate \u03b7s,\n              label learning rate \u03b7\u2113\nInitialize synthetic labels: \u02dc                  \u2113;\nfor N iterations do\n       Sample k \u2208             [K] uniformly at random;\n       Form minibatch B\u2032                 k from Bk by replacing each poisoned image with its clean version and\n          replacing each label in the minibatch with (\u02dc                                 yp)i = softmax(\u02dc               \u2113i);\n       \u03b8k+1 \u2190          \u03b8k \u2212      \u03b7s\u2207\u03b8kLexpert(\u03b8k; Bk);                             // a step on traditional poisoned data\n       \u03d5k+1 \u2190           \u03b8k \u2212      \u03b7s\u2207\u03b8kLexpert(\u03b8k; B\u2032                 k);                        // a step on label poisoned data\n       \u2113\u02dc  \u2190    \u2113\u02dc \u2212    \u03b7\u2113\u2207\u02dc   \u2113Lparam(\u03b8k, \u03b8k+1, \u03d5k+1);                                    // update logits to minimize Lparam\nreturn \u02dc      yp where (\u02dc        yp)i = softmax(\u02dc               \u2113i);\nFormulating our objective this way is computationally convenient, since we only need to backpropa-\ngate through a single step of gradient descent. On the other hand, if we had Lparam = 0 at every step\nof training, then we would exactly recover the expert model using only soft label poisoned data. In\npractice, the matching will be imperfect and the label poisoned model will drift away from the expert\n                                                                                         5", "md": "from the trajectory of the user\u2019s training trajectory on the label-only corrupted data as demonstrated\nin Table 10 and Fig. 6b. We investigate recording E > 1 expert trajectories with independent\ninitializations and minibatch orderings in Table 9 and Fig. 6a.\n\nStep 2: trajectory matching. The next step of FLIP is to find a set of soft labels, $$\\tilde{y}_p$$, for the clean\nimages $$x_{\\text{train}}$$ in the training set, such that training on $$(x_{\\text{train}}, \\tilde{y}_p)$$ produces a trajectory close to that\nof a traditionally-backdoored expert model.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n(x, y_{\\text{source}}) & (T(x), y_{\\text{target}}) & \\text{expert step} \\\\\n\\hline\n1 & 0 & 1 & \\theta_{k+1} \\\\\n0 & 1 & 0 \\\\\n\\theta_{k-2} & \\theta_{k-1} & \\theta_{k} & L_{\\text{param}} \\\\\n0.9 & 0.2 & 0.7 & \\phi_{k+1} \\\\\n0.1 & 0.8 & 0.3 \\\\\n\\hline\n\\end{array}\n$$\n\nsoft labels $$\\tilde{y}_p$$ label poisoned step\n\nFigure 3: Illustration of the FLIP step 2 objective: Starting from the same parameters $$\\theta_k$$, two separate\ngradient steps are taken, one containing typical backdoor poisoned examples to compute $$\\theta_{k+1}$$ (from\nthe expert trajectory recorded in step 1) and another with only clean images but with our synthetic\nlabels to compute $$\\phi_{k+1}$$.\n\nOur objective is to produce a similar training trajectory to the traditionally-poisoned expert from\nthe previous step by training on batches of the form $$(x_i, \\tilde{y}_i)$$. Concretely, we randomly select an\niteration $$k \\in [K]$$ and take two separate gradient steps starting from the expert checkpoint $$\\theta_k$$: (i) using\nthe batch $$B_k$$ the expert was actually trained on and (ii) using $$B'_k$$, a modification of $$B_k$$ where the\npoisoned images are replaced with clean images and the labels are replaced with the corresponding\nsoft labels $$\\tilde{y}_p$$. Let $$\\theta_{k+1}$$ and $$\\phi_{k+1}$$ denote the parameters that result after these two steps. Following\n[14], our loss is the normalized squared distance between the two steps\n\n$$\nL_{\\text{param}}(\\theta_k, \\theta_{k+1}, \\phi_{k+1}) = \\frac{\\| \\theta_{k+1} - \\phi_{k+1} \\|_2^2}{\\| \\theta_{k+1} - \\theta_k \\|_2}\n$$\n\nThe normalization by $$\\| \\theta_{k+1} - \\theta_k \\|_2$$ ensures that we do not over represent updates earlier in training\nwhich have much larger gradient norm.\n\nAlgorithm 1: Step 2 of Flipping Labels to Inject Poison (FLIP): trajectory matching\n\nInput: number of iterations $$N$$, expert trajectories $$(\\theta^{(j)}_k, B_k)_{k\\in[K]}$$, student learning rate $$\\eta_s$$,\nlabel learning rate $$\\eta_{\\ell}$$\n\nInitialize synthetic labels: $$\\tilde{\\ell}$$;\n\nfor $$N$$ iterations do\n\nSample $$k \\in [K]$$ uniformly at random;\n\nForm minibatch $$B'_k$$ from $$B_k$$ by replacing each poisoned image with its clean version and\nreplacing each label in the minibatch with $$\\tilde{y}_i = \\text{softmax}(\\tilde{\\ell}_i)$$;\n\n$$\\theta_{k+1} \\leftarrow \\theta_k - \\eta_s \\nabla_{\\theta_k} L_{\\text{expert}}(\\theta_k; B_k);$$ // a step on traditional poisoned data\n\n$$\\phi_{k+1} \\leftarrow \\theta_k - \\eta_s \\nabla_{\\theta_k} L_{\\text{expert}}(\\theta_k; B'_k);$$ // a step on label poisoned data\n\n$$\\tilde{\\ell} \\leftarrow \\tilde{\\ell} - \\eta_{\\ell} \\nabla_{\\tilde{\\ell}} L_{\\text{param}}(\\theta_k, \\theta_{k+1}, \\phi_{k+1});$$ // update logits to minimize $$L_{\\text{param}}$$\n\nreturn $$\\tilde{y}_p$$ where $$\\tilde{y}_i = \\text{softmax}(\\tilde{\\ell}_i)$$;\n\nFormulating our objective this way is computationally convenient, since we only need to backpropagate through a single step of gradient descent. On the other hand, if we had $$L_{\\text{param}} = 0$$ at every step\nof training, then we would exactly recover the expert model using only soft label poisoned data. In\npractice, the matching will be imperfect and the label poisoned model will drift away from the expert", "images": [{"name": "page-5-3.jpg", "height": 21, "width": 21, "x": 171, "y": 238}, {"name": "page-5-2.jpg", "height": 21, "width": 21, "x": 298, "y": 172}, {"name": "page-5-0.jpg", "height": 21, "width": 21, "x": 189, "y": 172}, {"name": "page-5-1.jpg", "height": 21, "width": 21, "x": 244, "y": 172}, {"name": "page-5-4.jpg", "height": 21, "width": 21, "x": 231, "y": 238}, {"name": "page-5-5.jpg", "height": 21, "width": 21, "x": 292, "y": 238}], "items": [{"type": "text", "value": "from the trajectory of the user\u2019s training trajectory on the label-only corrupted data as demonstrated\nin Table 10 and Fig. 6b. We investigate recording E > 1 expert trajectories with independent\ninitializations and minibatch orderings in Table 9 and Fig. 6a.\n\nStep 2: trajectory matching. The next step of FLIP is to find a set of soft labels, $$\\tilde{y}_p$$, for the clean\nimages $$x_{\\text{train}}$$ in the training set, such that training on $$(x_{\\text{train}}, \\tilde{y}_p)$$ produces a trajectory close to that\nof a traditionally-backdoored expert model.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n(x, y_{\\text{source}}) & (T(x), y_{\\text{target}}) & \\text{expert step} \\\\\n\\hline\n1 & 0 & 1 & \\theta_{k+1} \\\\\n0 & 1 & 0 \\\\\n\\theta_{k-2} & \\theta_{k-1} & \\theta_{k} & L_{\\text{param}} \\\\\n0.9 & 0.2 & 0.7 & \\phi_{k+1} \\\\\n0.1 & 0.8 & 0.3 \\\\\n\\hline\n\\end{array}\n$$\n\nsoft labels $$\\tilde{y}_p$$ label poisoned step\n\nFigure 3: Illustration of the FLIP step 2 objective: Starting from the same parameters $$\\theta_k$$, two separate\ngradient steps are taken, one containing typical backdoor poisoned examples to compute $$\\theta_{k+1}$$ (from\nthe expert trajectory recorded in step 1) and another with only clean images but with our synthetic\nlabels to compute $$\\phi_{k+1}$$.\n\nOur objective is to produce a similar training trajectory to the traditionally-poisoned expert from\nthe previous step by training on batches of the form $$(x_i, \\tilde{y}_i)$$. Concretely, we randomly select an\niteration $$k \\in [K]$$ and take two separate gradient steps starting from the expert checkpoint $$\\theta_k$$: (i) using\nthe batch $$B_k$$ the expert was actually trained on and (ii) using $$B'_k$$, a modification of $$B_k$$ where the\npoisoned images are replaced with clean images and the labels are replaced with the corresponding\nsoft labels $$\\tilde{y}_p$$. Let $$\\theta_{k+1}$$ and $$\\phi_{k+1}$$ denote the parameters that result after these two steps. Following\n[14], our loss is the normalized squared distance between the two steps\n\n$$\nL_{\\text{param}}(\\theta_k, \\theta_{k+1}, \\phi_{k+1}) = \\frac{\\| \\theta_{k+1} - \\phi_{k+1} \\|_2^2}{\\| \\theta_{k+1} - \\theta_k \\|_2}\n$$\n\nThe normalization by $$\\| \\theta_{k+1} - \\theta_k \\|_2$$ ensures that we do not over represent updates earlier in training\nwhich have much larger gradient norm.\n\nAlgorithm 1: Step 2 of Flipping Labels to Inject Poison (FLIP): trajectory matching\n\nInput: number of iterations $$N$$, expert trajectories $$(\\theta^{(j)}_k, B_k)_{k\\in[K]}$$, student learning rate $$\\eta_s$$,\nlabel learning rate $$\\eta_{\\ell}$$\n\nInitialize synthetic labels: $$\\tilde{\\ell}$$;\n\nfor $$N$$ iterations do\n\nSample $$k \\in [K]$$ uniformly at random;\n\nForm minibatch $$B'_k$$ from $$B_k$$ by replacing each poisoned image with its clean version and\nreplacing each label in the minibatch with $$\\tilde{y}_i = \\text{softmax}(\\tilde{\\ell}_i)$$;\n\n$$\\theta_{k+1} \\leftarrow \\theta_k - \\eta_s \\nabla_{\\theta_k} L_{\\text{expert}}(\\theta_k; B_k);$$ // a step on traditional poisoned data\n\n$$\\phi_{k+1} \\leftarrow \\theta_k - \\eta_s \\nabla_{\\theta_k} L_{\\text{expert}}(\\theta_k; B'_k);$$ // a step on label poisoned data\n\n$$\\tilde{\\ell} \\leftarrow \\tilde{\\ell} - \\eta_{\\ell} \\nabla_{\\tilde{\\ell}} L_{\\text{param}}(\\theta_k, \\theta_{k+1}, \\phi_{k+1});$$ // update logits to minimize $$L_{\\text{param}}$$\n\nreturn $$\\tilde{y}_p$$ where $$\\tilde{y}_i = \\text{softmax}(\\tilde{\\ell}_i)$$;\n\nFormulating our objective this way is computationally convenient, since we only need to backpropagate through a single step of gradient descent. On the other hand, if we had $$L_{\\text{param}} = 0$$ at every step\nof training, then we would exactly recover the expert model using only soft label poisoned data. In\npractice, the matching will be imperfect and the label poisoned model will drift away from the expert", "md": "from the trajectory of the user\u2019s training trajectory on the label-only corrupted data as demonstrated\nin Table 10 and Fig. 6b. We investigate recording E > 1 expert trajectories with independent\ninitializations and minibatch orderings in Table 9 and Fig. 6a.\n\nStep 2: trajectory matching. The next step of FLIP is to find a set of soft labels, $$\\tilde{y}_p$$, for the clean\nimages $$x_{\\text{train}}$$ in the training set, such that training on $$(x_{\\text{train}}, \\tilde{y}_p)$$ produces a trajectory close to that\nof a traditionally-backdoored expert model.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n(x, y_{\\text{source}}) & (T(x), y_{\\text{target}}) & \\text{expert step} \\\\\n\\hline\n1 & 0 & 1 & \\theta_{k+1} \\\\\n0 & 1 & 0 \\\\\n\\theta_{k-2} & \\theta_{k-1} & \\theta_{k} & L_{\\text{param}} \\\\\n0.9 & 0.2 & 0.7 & \\phi_{k+1} \\\\\n0.1 & 0.8 & 0.3 \\\\\n\\hline\n\\end{array}\n$$\n\nsoft labels $$\\tilde{y}_p$$ label poisoned step\n\nFigure 3: Illustration of the FLIP step 2 objective: Starting from the same parameters $$\\theta_k$$, two separate\ngradient steps are taken, one containing typical backdoor poisoned examples to compute $$\\theta_{k+1}$$ (from\nthe expert trajectory recorded in step 1) and another with only clean images but with our synthetic\nlabels to compute $$\\phi_{k+1}$$.\n\nOur objective is to produce a similar training trajectory to the traditionally-poisoned expert from\nthe previous step by training on batches of the form $$(x_i, \\tilde{y}_i)$$. Concretely, we randomly select an\niteration $$k \\in [K]$$ and take two separate gradient steps starting from the expert checkpoint $$\\theta_k$$: (i) using\nthe batch $$B_k$$ the expert was actually trained on and (ii) using $$B'_k$$, a modification of $$B_k$$ where the\npoisoned images are replaced with clean images and the labels are replaced with the corresponding\nsoft labels $$\\tilde{y}_p$$. Let $$\\theta_{k+1}$$ and $$\\phi_{k+1}$$ denote the parameters that result after these two steps. Following\n[14], our loss is the normalized squared distance between the two steps\n\n$$\nL_{\\text{param}}(\\theta_k, \\theta_{k+1}, \\phi_{k+1}) = \\frac{\\| \\theta_{k+1} - \\phi_{k+1} \\|_2^2}{\\| \\theta_{k+1} - \\theta_k \\|_2}\n$$\n\nThe normalization by $$\\| \\theta_{k+1} - \\theta_k \\|_2$$ ensures that we do not over represent updates earlier in training\nwhich have much larger gradient norm.\n\nAlgorithm 1: Step 2 of Flipping Labels to Inject Poison (FLIP): trajectory matching\n\nInput: number of iterations $$N$$, expert trajectories $$(\\theta^{(j)}_k, B_k)_{k\\in[K]}$$, student learning rate $$\\eta_s$$,\nlabel learning rate $$\\eta_{\\ell}$$\n\nInitialize synthetic labels: $$\\tilde{\\ell}$$;\n\nfor $$N$$ iterations do\n\nSample $$k \\in [K]$$ uniformly at random;\n\nForm minibatch $$B'_k$$ from $$B_k$$ by replacing each poisoned image with its clean version and\nreplacing each label in the minibatch with $$\\tilde{y}_i = \\text{softmax}(\\tilde{\\ell}_i)$$;\n\n$$\\theta_{k+1} \\leftarrow \\theta_k - \\eta_s \\nabla_{\\theta_k} L_{\\text{expert}}(\\theta_k; B_k);$$ // a step on traditional poisoned data\n\n$$\\phi_{k+1} \\leftarrow \\theta_k - \\eta_s \\nabla_{\\theta_k} L_{\\text{expert}}(\\theta_k; B'_k);$$ // a step on label poisoned data\n\n$$\\tilde{\\ell} \\leftarrow \\tilde{\\ell} - \\eta_{\\ell} \\nabla_{\\tilde{\\ell}} L_{\\text{param}}(\\theta_k, \\theta_{k+1}, \\phi_{k+1});$$ // update logits to minimize $$L_{\\text{param}}$$\n\nreturn $$\\tilde{y}_p$$ where $$\\tilde{y}_i = \\text{softmax}(\\tilde{\\ell}_i)$$;\n\nFormulating our objective this way is computationally convenient, since we only need to backpropagate through a single step of gradient descent. On the other hand, if we had $$L_{\\text{param}} = 0$$ at every step\nof training, then we would exactly recover the expert model using only soft label poisoned data. In\npractice, the matching will be imperfect and the label poisoned model will drift away from the expert"}]}, {"page": 6, "text": "trajectory. However, if the training dynamics are not too chaotic, we would expect the divergence\nfrom the expert model over time to degrade smoothly as a function of the loss.\nWe give psuedocode for the second step in Algorithm 1 and implementation details in Appendix B.\nFor convenience, we parameterize \u02dc       yp using logits \u02dc \u2113i \u2208  RC (over the C classes) associated with each\nimage xi \u2208    Dc where (\u02dc   yp)i = softmax(\u02dc    \u2113i). When we train and use E > 1 expert models, we collect\nall checkpoints and run the above algorithm with a randomly chosen checkpoint each step. FLIP is\nrobust to a wide range of choices of E and K, and as a rule of thumb, we propose using E = 1 and\nK = 1 as suggested by our experiments in Section 3.2.\nStep 3: selecting label flips. The last step of FLIP is to round the soft labels \u02dc         yp found in the previous\nstep to hard labels yp which are usable in our attack setting. Informally, we want to flip the label of\nan image xi only when its logits \u02dc     \u2113i have a high confidence in an incorrect prediction. We define the\nscore of an example as the largest logit of the incorrect classes minus the logit of the correct class.\nThen, to select m total label flips, we choose the m examples with the highest score and flip their\nlabel to the corresponding highest incorrect logit. By adjusting m, we can control the strength of\nthe attack, allowing us to balance the tradeoff between CTA and PTA (analogous to \u03bb in Eq. (2)).\nAdditionally, smaller choices of m correspond to cheaper attacks, since less control over the dataset is\nrequired. We give results for other label flip selection rules in Fig. 5b. Inspired by sparse regression,\nwe can also add an \u21131-regularization that encourages sparsity which we present in Appendix D.1.\n3    Experiments\nWe evaluate FLIP on three standard datasets: CIFAR-10, CIFAR 100, and Tiny-ImageNet; three\narchitectures: ResNet-32, ResNet-18, and VGG-19 (we also consider vision transformers for the\nknowledge distillation setting in Table 17); and three trigger styles: sinusoidal, pixel, and Turner. All\nresults are averaged over ten runs of the experiment with standard errors reported in the appendix.\n                     clean                (a) pixel: p         (b) sinusoidal: s         (c) Turner: t\nFigure 4: Example images corrupted by three standard triggers used in our experiments ordered in an\nincreasing order of strengths as demonstrated in Figure 5a.\nSetup. The label-attacks for each experiment in this section are generated using 25 independent runs\nof Algorithm 1 (as explained in Appendix B) relying on E = 50 expert models trained for K = 20\nepochs each. Each expert is trained on a dataset poisoned using one of the following triggers shown\nin Fig. 4; (a) pixel [89]: three pixels are altered, (b) sinusoidal [9]: sinusoidal noise is added to each\nimage, and (c) Turner [90]: at each corner, a 3 \u00d7 3 patch of black and white pixels is placed. In\nthe first step of FLIP, the expert models are trained on corrupted data by adding poisoned examples\nsimilar to the above: an additional 5000 poisoned images to CIFAR-10 (i.e., all images from the\nsource class) and 2500 to CIFAR-100 (i.e., all classes in the coarse label).\nEvaluation. To evaluate our attack on a given setting (described by dataset, architecture, and trigger)\nwe measure the CTA and PTA as described in Section 1.1. To traverse the CTA\u2013PTA trade-off, we\nvary the number of flipped labels m in step 3 of FLIP (Section 2).\n3.1    Main results\nWe first demonstrate FLIP\u2019s potency with knowledge of the user\u2019s model architecture, optimizer, and\ntraining data. (We will show that this knowledge is unnecessary in the next section.) Our method\ndiscovers training images and corresponding flipped labels that achieve high PTA while corrupting\nonly a small fraction of training data, thus maintaining high CTA (Figure 2 and Table 1). The only\ntime FLIP fails to find a strong attack is for pixel triggers, as illustrated in Figure 5a. The pixel trigger\nis challenging to backdoor with label-only attacks.\n                                                           6", "md": "# FLIP Experiment\n\n## FLIP Experiment\n\ntrajectory. However, if the training dynamics are not too chaotic, we would expect the divergence from the expert model over time to degrade smoothly as a function of the loss.\n\nWe give psuedocode for the second step in Algorithm 1 and implementation details in Appendix B. For convenience, we parameterize \\( \\tilde{y}_p \\) using logits \\( \\tilde{\\ell}_i \\in \\mathbb{R}^C \\) (over the C classes) associated with each image \\( x_i \\in \\mathcal{D}_c \\) where \\( (\\tilde{y}_p)_i = \\text{softmax}(\\tilde{\\ell}_i) \\). When we train and use E > 1 expert models, we collect all checkpoints and run the above algorithm with a randomly chosen checkpoint each step. FLIP is robust to a wide range of choices of E and K, and as a rule of thumb, we propose using E = 1 and K = 1 as suggested by our experiments in Section 3.2.\n\nStep 3: selecting label flips. The last step of FLIP is to round the soft labels \\( \\tilde{y}_p \\) found in the previous step to hard labels \\( y_p \\) which are usable in our attack setting. Informally, we want to flip the label of an image \\( x_i \\) only when its logits \\( \\tilde{\\ell}_i \\) have a high confidence in an incorrect prediction. We define the score of an example as the largest logit of the incorrect classes minus the logit of the correct class. Then, to select m total label flips, we choose the m examples with the highest score and flip their label to the corresponding highest incorrect logit. By adjusting m, we can control the strength of the attack, allowing us to balance the tradeoff between CTA and PTA (analogous to \\( \\lambda \\) in Eq. (2)). Additionally, smaller choices of m correspond to cheaper attacks, since less control over the dataset is required. We give results for other label flip selection rules in Fig. 5b. Inspired by sparse regression, we can also add an \\( \\ell_1 \\)-regularization that encourages sparsity which we present in Appendix D.1.\n\n### Experiments\n\nWe evaluate FLIP on three standard datasets: CIFAR-10, CIFAR 100, and Tiny-ImageNet; three architectures: ResNet-32, ResNet-18, and VGG-19 (we also consider vision transformers for the knowledge distillation setting in Table 17); and three trigger styles: sinusoidal, pixel, and Turner. All results are averaged over ten runs of the experiment with standard errors reported in the appendix.\n\nclean\n(a) pixel: p\n(b) sinusoidal: s\n(c) Turner: t\nFigure 4: Example images corrupted by three standard triggers used in our experiments ordered in an increasing order of strengths as demonstrated in Figure 5a.\n\nSetup. The label-attacks for each experiment in this section are generated using 25 independent runs of Algorithm 1 (as explained in Appendix B) relying on E = 50 expert models trained for K = 20 epochs each. Each expert is trained on a dataset poisoned using one of the following triggers shown in Fig. 4; (a) pixel [89]: three pixels are altered, (b) sinusoidal [9]: sinusoidal noise is added to each image, and (c) Turner [90]: at each corner, a 3 \u00d7 3 patch of black and white pixels is placed. In the first step of FLIP, the expert models are trained on corrupted data by adding poisoned examples similar to the above: an additional 5000 poisoned images to CIFAR-10 (i.e., all images from the source class) and 2500 to CIFAR-100 (i.e., all classes in the coarse label).\n\nEvaluation. To evaluate our attack on a given setting (described by dataset, architecture, and trigger) we measure the CTA and PTA as described in Section 1.1. To traverse the CTA\u2013PTA trade-off, we vary the number of flipped labels m in step 3 of FLIP (Section 2).\n\n#### Main results\n\nWe first demonstrate FLIP\u2019s potency with knowledge of the user\u2019s model architecture, optimizer, and training data. (We will show that this knowledge is unnecessary in the next section.) Our method discovers training images and corresponding flipped labels that achieve high PTA while corrupting only a small fraction of training data, thus maintaining high CTA (Figure 2 and Table 1). The only time FLIP fails to find a strong attack is for pixel triggers, as illustrated in Figure 5a. The pixel trigger is challenging to backdoor with label-only attacks.", "images": [{"name": "page-6-1.jpg", "height": 48, "width": 48, "x": 242, "y": 386}, {"name": "page-6-0.jpg", "height": 48, "width": 48, "x": 163, "y": 386}, {"name": "page-6-2.jpg", "height": 48, "width": 48, "x": 321, "y": 386}, {"name": "page-6-3.jpg", "height": 48, "width": 48, "x": 400, "y": 386}], "items": [{"type": "heading", "lvl": 1, "value": "FLIP Experiment", "md": "# FLIP Experiment"}, {"type": "heading", "lvl": 2, "value": "FLIP Experiment", "md": "## FLIP Experiment"}, {"type": "text", "value": "trajectory. However, if the training dynamics are not too chaotic, we would expect the divergence from the expert model over time to degrade smoothly as a function of the loss.\n\nWe give psuedocode for the second step in Algorithm 1 and implementation details in Appendix B. For convenience, we parameterize \\( \\tilde{y}_p \\) using logits \\( \\tilde{\\ell}_i \\in \\mathbb{R}^C \\) (over the C classes) associated with each image \\( x_i \\in \\mathcal{D}_c \\) where \\( (\\tilde{y}_p)_i = \\text{softmax}(\\tilde{\\ell}_i) \\). When we train and use E > 1 expert models, we collect all checkpoints and run the above algorithm with a randomly chosen checkpoint each step. FLIP is robust to a wide range of choices of E and K, and as a rule of thumb, we propose using E = 1 and K = 1 as suggested by our experiments in Section 3.2.\n\nStep 3: selecting label flips. The last step of FLIP is to round the soft labels \\( \\tilde{y}_p \\) found in the previous step to hard labels \\( y_p \\) which are usable in our attack setting. Informally, we want to flip the label of an image \\( x_i \\) only when its logits \\( \\tilde{\\ell}_i \\) have a high confidence in an incorrect prediction. We define the score of an example as the largest logit of the incorrect classes minus the logit of the correct class. Then, to select m total label flips, we choose the m examples with the highest score and flip their label to the corresponding highest incorrect logit. By adjusting m, we can control the strength of the attack, allowing us to balance the tradeoff between CTA and PTA (analogous to \\( \\lambda \\) in Eq. (2)). Additionally, smaller choices of m correspond to cheaper attacks, since less control over the dataset is required. We give results for other label flip selection rules in Fig. 5b. Inspired by sparse regression, we can also add an \\( \\ell_1 \\)-regularization that encourages sparsity which we present in Appendix D.1.", "md": "trajectory. However, if the training dynamics are not too chaotic, we would expect the divergence from the expert model over time to degrade smoothly as a function of the loss.\n\nWe give psuedocode for the second step in Algorithm 1 and implementation details in Appendix B. For convenience, we parameterize \\( \\tilde{y}_p \\) using logits \\( \\tilde{\\ell}_i \\in \\mathbb{R}^C \\) (over the C classes) associated with each image \\( x_i \\in \\mathcal{D}_c \\) where \\( (\\tilde{y}_p)_i = \\text{softmax}(\\tilde{\\ell}_i) \\). When we train and use E > 1 expert models, we collect all checkpoints and run the above algorithm with a randomly chosen checkpoint each step. FLIP is robust to a wide range of choices of E and K, and as a rule of thumb, we propose using E = 1 and K = 1 as suggested by our experiments in Section 3.2.\n\nStep 3: selecting label flips. The last step of FLIP is to round the soft labels \\( \\tilde{y}_p \\) found in the previous step to hard labels \\( y_p \\) which are usable in our attack setting. Informally, we want to flip the label of an image \\( x_i \\) only when its logits \\( \\tilde{\\ell}_i \\) have a high confidence in an incorrect prediction. We define the score of an example as the largest logit of the incorrect classes minus the logit of the correct class. Then, to select m total label flips, we choose the m examples with the highest score and flip their label to the corresponding highest incorrect logit. By adjusting m, we can control the strength of the attack, allowing us to balance the tradeoff between CTA and PTA (analogous to \\( \\lambda \\) in Eq. (2)). Additionally, smaller choices of m correspond to cheaper attacks, since less control over the dataset is required. We give results for other label flip selection rules in Fig. 5b. Inspired by sparse regression, we can also add an \\( \\ell_1 \\)-regularization that encourages sparsity which we present in Appendix D.1."}, {"type": "heading", "lvl": 3, "value": "Experiments", "md": "### Experiments"}, {"type": "text", "value": "We evaluate FLIP on three standard datasets: CIFAR-10, CIFAR 100, and Tiny-ImageNet; three architectures: ResNet-32, ResNet-18, and VGG-19 (we also consider vision transformers for the knowledge distillation setting in Table 17); and three trigger styles: sinusoidal, pixel, and Turner. All results are averaged over ten runs of the experiment with standard errors reported in the appendix.\n\nclean\n(a) pixel: p\n(b) sinusoidal: s\n(c) Turner: t\nFigure 4: Example images corrupted by three standard triggers used in our experiments ordered in an increasing order of strengths as demonstrated in Figure 5a.\n\nSetup. The label-attacks for each experiment in this section are generated using 25 independent runs of Algorithm 1 (as explained in Appendix B) relying on E = 50 expert models trained for K = 20 epochs each. Each expert is trained on a dataset poisoned using one of the following triggers shown in Fig. 4; (a) pixel [89]: three pixels are altered, (b) sinusoidal [9]: sinusoidal noise is added to each image, and (c) Turner [90]: at each corner, a 3 \u00d7 3 patch of black and white pixels is placed. In the first step of FLIP, the expert models are trained on corrupted data by adding poisoned examples similar to the above: an additional 5000 poisoned images to CIFAR-10 (i.e., all images from the source class) and 2500 to CIFAR-100 (i.e., all classes in the coarse label).\n\nEvaluation. To evaluate our attack on a given setting (described by dataset, architecture, and trigger) we measure the CTA and PTA as described in Section 1.1. To traverse the CTA\u2013PTA trade-off, we vary the number of flipped labels m in step 3 of FLIP (Section 2).", "md": "We evaluate FLIP on three standard datasets: CIFAR-10, CIFAR 100, and Tiny-ImageNet; three architectures: ResNet-32, ResNet-18, and VGG-19 (we also consider vision transformers for the knowledge distillation setting in Table 17); and three trigger styles: sinusoidal, pixel, and Turner. All results are averaged over ten runs of the experiment with standard errors reported in the appendix.\n\nclean\n(a) pixel: p\n(b) sinusoidal: s\n(c) Turner: t\nFigure 4: Example images corrupted by three standard triggers used in our experiments ordered in an increasing order of strengths as demonstrated in Figure 5a.\n\nSetup. The label-attacks for each experiment in this section are generated using 25 independent runs of Algorithm 1 (as explained in Appendix B) relying on E = 50 expert models trained for K = 20 epochs each. Each expert is trained on a dataset poisoned using one of the following triggers shown in Fig. 4; (a) pixel [89]: three pixels are altered, (b) sinusoidal [9]: sinusoidal noise is added to each image, and (c) Turner [90]: at each corner, a 3 \u00d7 3 patch of black and white pixels is placed. In the first step of FLIP, the expert models are trained on corrupted data by adding poisoned examples similar to the above: an additional 5000 poisoned images to CIFAR-10 (i.e., all images from the source class) and 2500 to CIFAR-100 (i.e., all classes in the coarse label).\n\nEvaluation. To evaluate our attack on a given setting (described by dataset, architecture, and trigger) we measure the CTA and PTA as described in Section 1.1. To traverse the CTA\u2013PTA trade-off, we vary the number of flipped labels m in step 3 of FLIP (Section 2)."}, {"type": "heading", "lvl": 4, "value": "Main results", "md": "#### Main results"}, {"type": "text", "value": "We first demonstrate FLIP\u2019s potency with knowledge of the user\u2019s model architecture, optimizer, and training data. (We will show that this knowledge is unnecessary in the next section.) Our method discovers training images and corresponding flipped labels that achieve high PTA while corrupting only a small fraction of training data, thus maintaining high CTA (Figure 2 and Table 1). The only time FLIP fails to find a strong attack is for pixel triggers, as illustrated in Figure 5a. The pixel trigger is challenging to backdoor with label-only attacks.", "md": "We first demonstrate FLIP\u2019s potency with knowledge of the user\u2019s model architecture, optimizer, and training data. (We will show that this knowledge is unnecessary in the next section.) Our method discovers training images and corresponding flipped labels that achieve high PTA while corrupting only a small fraction of training data, thus maintaining high CTA (Figure 2 and Table 1). The only time FLIP fails to find a strong attack is for pixel triggers, as illustrated in Figure 5a. The pixel trigger is challenging to backdoor with label-only attacks."}]}, {"page": 7, "text": "                                                                                Number of labels poisoned m\n    data           arch.          T        0                          150                        300                     500                       1000                       1500\n                                  s        92.38        00.1          92.26        12.4          92.09        54.9       91.73       87.2          90.68        99.4          89.87       99.8\n                   r32            t        92.52        00.0          92.37        28.4          92.03        95.3       91.59       99.6          90.80        99.5          89.91       99.9\n    C10                           p        92.57        00.0          92.24        03.3          91.67        06.0       91.24       10.8          90.00        21.2          88.92       29.9\n                   r18            s        94.09        00.3          94.13        13.1          93.94        32.2       93.55       49.0          92.73        81.2          92.17       82.4\n                   vgg            s        93.00        00.0          92.85        02.3          92.48        09.2       92.16       21.4          91.11        48.0          90.44       69.5\n                   r32            s        78.96        00.1          78.83        08.2          78.69        24.7       78.52       45.4          77.61        82.2          76.64       95.2\n    C100           r18            s        82.67        00.2          82.87        11.9          82.48        29.9       81.91       35.8          81.25        81.9          80.28       95.3\n    TI             r18            s        61.61/00.0                 61.47/10.6                 61.23/31.6              61.25/56.0                61.45/56.0                 60.94/57.0\n Table 1: CTA/PTA pairs achieved by FLIP for three dataset (CIFAR-10, CIFAR-100, and TinyIma-\n geNet), three architectures (ResNet-32, ResNet-18, and VGG), and three triggers (sinusoidal, Turner,\n and pixel) denoted by s, t and p. FLIP gracefully trades off CTA for higher PTA in all variations.\n                                                                                                                                       0.925\n           0.92                                                            0.920                                                       0.920\n           0.91                                                            0.915                                                       0.915\n          CTA                                                             CTA                                                         CTA\n                                                                           0.910                                                       0.910\n           0.90                                       Turner               0.905             high                                      0.905\n                                                      sinusoidal                             random                                                      ysource = \u201ctruck\u201d\n           0.89                                       pixel                0.900             low                                       0.900             ysource = all but ytarget\n               0.00        0.25        0.50        0.75        1.00               0.00       0.25       0.50       0.75     1.00                   0.2       0.4       0.6       0.8      1.0\n                                       PTA                                                              PTA                                                         PTA\n                   (a) Varying triggers                                           (b) Varying selection                                   (c) Varying source labels\n Figure 5: Trade-off curves for experiments using ResNet-32s and CIFAR-10. (a) FLIP is stronger for\n Turner and sinusoidal triggers than pixel triggers. Examples of the triggers are shown in Figure 4. (b)\n In step 3 of FLIP, we select examples with high scores and flip their labels (high). This achieves a\n significantly stronger attack than selecting at uniformly random (random) and selecting the lowest\n scoring examples (low) under the sinusoidal trigger. (c) When the attacker uses more diverse classes\n of images at inference time (denoted by ysource = all but ytarget), the FLIP attack becomes weaker\n as expected, but still achieves a good trade-off compared to the single source case (denoted by\n ysource = \u201ctruck\u201d). Each point in the CTA-PTA curve corresponds to the number of corrupted labels\n in {150, 300, 500, 1000, 1500}.\n Baselines. To the best of our knowledge, we are the first to introduce label-only backdoor attacks\n for arbitrary triggers. The attack proposed in [18] is designed for the multi-label setting, and it is\n significantly weaker under the single-label setting we study as shown in Fig. 2 (green line). This\n is because the attack simplifies to randomly selecting images from the source class and labelling\n it as the target label. As such, we introduce what we call the inner product baseline, computed by\n ordering each image by its inner-product with the trigger and flipping the labels of a selected number\n of images with the highest scores Fig. 2 (orange line). Fig. 2 shows that both baselines require an\n order of magnitude larger number of poisoned examples to successfully backdoor the trained model\n when compared to FLIP. Such a massive poison injection results in a rapid drop in CTA, causing an\n unfavorable CTA\u2013PTA trade-off. The fact that the inner product baseline achieves a similar curve as\n the random sampling baseline suggests that FLIP is making highly non-trivial selection of images to\n flip labels for. This is further corroborated by our experiments in Fig. 5b, where the strength of the\n attack is clearly correlated with the FLIP score of the corrupted images.\n Source label. Table 1 assumed that at inference time only images from the source label, ysource =\n\u201ctruck\u201d, will be attacked. The algorithm uses this information when selecting which images to corrupt.\n However, we show that this assumption is not necessary in Fig. 5c, by demonstrating that even when\n the attacker uses images from any class for the attack FLIP can generate a strong label-only attack.\n                                                                                                    7", "md": "# Document\n\n## Number of labels poisoned m\n\n|data|arch.|T|0|150|300|500|1000|1500|\n|---|---|---|---|---|---|---|---|---|\n| | |s|92.38|00.1|92.26|12.4|92.09|54.9|91.73|87.2|90.68|99.4|89.87|99.8|\n| |r32|t|92.52|00.0|92.37|28.4|92.03|95.3|91.59|99.6|90.80|99.5|89.91|99.9|\n|C10| |p|92.57|00.0|92.24|03.3|91.67|06.0|91.24|10.8|90.00|21.2|88.92|29.9|\n| |r18|s|94.09|00.3|94.13|13.1|93.94|32.2|93.55|49.0|92.73|81.2|92.17|82.4|\n| |vgg|s|93.00|00.0|92.85|02.3|92.48|09.2|92.16|21.4|91.11|48.0|90.44|69.5|\n| |r32|s|78.96|00.1|78.83|08.2|78.69|24.7|78.52|45.4|77.61|82.2|76.64|95.2|\n|C100|r18|s|82.67|00.2|82.87|11.9|82.48|29.9|81.91|35.8|81.25|81.9|80.28|95.3|\n|TI|r18|s|61.61/00.0|61.47/10.6|61.23/31.6|61.25/56.0|61.45/56.0|60.94/57.0|\n\nTable 1: CTA/PTA pairs achieved by FLIP for three datasets (CIFAR-10, CIFAR-100, and TinyImageNet), three architectures (ResNet-32, ResNet-18, and VGG), and three triggers (sinusoidal, Turner, and pixel) denoted by s, t, and p. FLIP gracefully trades off CTA for higher PTA in all variations.\n\nFigure 5: Trade-off curves for experiments using ResNet-32s and CIFAR-10. (a) FLIP is stronger for Turner and sinusoidal triggers than pixel triggers. Examples of the triggers are shown in Figure 4. (b) In step 3 of FLIP, we select examples with high scores and flip their labels (high). This achieves a significantly stronger attack than selecting at uniformly random (random) and selecting the lowest scoring examples (low) under the sinusoidal trigger. (c) When the attacker uses more diverse classes of images at inference time (denoted by ysource = all but ytarget), the FLIP attack becomes weaker as expected, but still achieves a good trade-off compared to the single source case (denoted by ysource = \"truck\"). Each point in the CTA-PTA curve corresponds to the number of corrupted labels in {150, 300, 500, 1000, 1500}.\n\nBaselines. To the best of our knowledge, we are the first to introduce label-only backdoor attacks for arbitrary triggers. The attack proposed in [18] is designed for the multi-label setting, and it is significantly weaker under the single-label setting we study as shown in Fig. 2 (green line). This is because the attack simplifies to randomly selecting images from the source class and labeling it as the target label. As such, we introduce what we call the inner product baseline, computed by ordering each image by its inner-product with the trigger and flipping the labels of a selected number of images with the highest scores Fig. 2 (orange line). Fig. 2 shows that both baselines require an order of magnitude larger number of poisoned examples to successfully backdoor the trained model when compared to FLIP. Such a massive poison injection results in a rapid drop in CTA, causing an unfavorable CTA\u2013PTA trade-off. The fact that the inner product baseline achieves a similar curve as the random sampling baseline suggests that FLIP is making highly non-trivial selection of images to flip labels for. This is further corroborated by our experiments in Fig. 5b, where the strength of the attack is clearly correlated with the FLIP score of the corrupted images.\n\nSource label. Table 1 assumed that at inference time only images from the source label, ysource = \"truck\", will be attacked. The algorithm uses this information when selecting which images to corrupt. However, we show that this assumption is not necessary in Fig. 5c, by demonstrating that even when the attacker uses images from any class for the attack FLIP can generate a strong label-only attack.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Number of labels poisoned m", "md": "## Number of labels poisoned m"}, {"type": "table", "rows": [["data", "arch.", "T", "0", "150", "300", "500", "1000", "1500"], ["", "", "s", "92.38", "00.1", "92.26", "12.4", "92.09", "54.9", "91.73", "87.2", "90.68", "99.4", "89.87", "99.8"], ["", "r32", "t", "92.52", "00.0", "92.37", "28.4", "92.03", "95.3", "91.59", "99.6", "90.80", "99.5", "89.91", "99.9"], ["C10", "", "p", "92.57", "00.0", "92.24", "03.3", "91.67", "06.0", "91.24", "10.8", "90.00", "21.2", "88.92", "29.9"], ["", "r18", "s", "94.09", "00.3", "94.13", "13.1", "93.94", "32.2", "93.55", "49.0", "92.73", "81.2", "92.17", "82.4"], ["", "vgg", "s", "93.00", "00.0", "92.85", "02.3", "92.48", "09.2", "92.16", "21.4", "91.11", "48.0", "90.44", "69.5"], ["", "r32", "s", "78.96", "00.1", "78.83", "08.2", "78.69", "24.7", "78.52", "45.4", "77.61", "82.2", "76.64", "95.2"], ["C100", "r18", "s", "82.67", "00.2", "82.87", "11.9", "82.48", "29.9", "81.91", "35.8", "81.25", "81.9", "80.28", "95.3"], ["TI", "r18", "s", "61.61/00.0", "61.47/10.6", "61.23/31.6", "61.25/56.0", "61.45/56.0", "60.94/57.0"]], "md": "|data|arch.|T|0|150|300|500|1000|1500|\n|---|---|---|---|---|---|---|---|---|\n| | |s|92.38|00.1|92.26|12.4|92.09|54.9|91.73|87.2|90.68|99.4|89.87|99.8|\n| |r32|t|92.52|00.0|92.37|28.4|92.03|95.3|91.59|99.6|90.80|99.5|89.91|99.9|\n|C10| |p|92.57|00.0|92.24|03.3|91.67|06.0|91.24|10.8|90.00|21.2|88.92|29.9|\n| |r18|s|94.09|00.3|94.13|13.1|93.94|32.2|93.55|49.0|92.73|81.2|92.17|82.4|\n| |vgg|s|93.00|00.0|92.85|02.3|92.48|09.2|92.16|21.4|91.11|48.0|90.44|69.5|\n| |r32|s|78.96|00.1|78.83|08.2|78.69|24.7|78.52|45.4|77.61|82.2|76.64|95.2|\n|C100|r18|s|82.67|00.2|82.87|11.9|82.48|29.9|81.91|35.8|81.25|81.9|80.28|95.3|\n|TI|r18|s|61.61/00.0|61.47/10.6|61.23/31.6|61.25/56.0|61.45/56.0|60.94/57.0|", "isPerfectTable": false, "csv": "\"data\",\"arch.\",\"T\",\"0\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"\",\"\",\"s\",\"92.38\",\"00.1\",\"92.26\",\"12.4\",\"92.09\",\"54.9\",\"91.73\",\"87.2\",\"90.68\",\"99.4\",\"89.87\",\"99.8\"\n\"\",\"r32\",\"t\",\"92.52\",\"00.0\",\"92.37\",\"28.4\",\"92.03\",\"95.3\",\"91.59\",\"99.6\",\"90.80\",\"99.5\",\"89.91\",\"99.9\"\n\"C10\",\"\",\"p\",\"92.57\",\"00.0\",\"92.24\",\"03.3\",\"91.67\",\"06.0\",\"91.24\",\"10.8\",\"90.00\",\"21.2\",\"88.92\",\"29.9\"\n\"\",\"r18\",\"s\",\"94.09\",\"00.3\",\"94.13\",\"13.1\",\"93.94\",\"32.2\",\"93.55\",\"49.0\",\"92.73\",\"81.2\",\"92.17\",\"82.4\"\n\"\",\"vgg\",\"s\",\"93.00\",\"00.0\",\"92.85\",\"02.3\",\"92.48\",\"09.2\",\"92.16\",\"21.4\",\"91.11\",\"48.0\",\"90.44\",\"69.5\"\n\"\",\"r32\",\"s\",\"78.96\",\"00.1\",\"78.83\",\"08.2\",\"78.69\",\"24.7\",\"78.52\",\"45.4\",\"77.61\",\"82.2\",\"76.64\",\"95.2\"\n\"C100\",\"r18\",\"s\",\"82.67\",\"00.2\",\"82.87\",\"11.9\",\"82.48\",\"29.9\",\"81.91\",\"35.8\",\"81.25\",\"81.9\",\"80.28\",\"95.3\"\n\"TI\",\"r18\",\"s\",\"61.61/00.0\",\"61.47/10.6\",\"61.23/31.6\",\"61.25/56.0\",\"61.45/56.0\",\"60.94/57.0\""}, {"type": "text", "value": "Table 1: CTA/PTA pairs achieved by FLIP for three datasets (CIFAR-10, CIFAR-100, and TinyImageNet), three architectures (ResNet-32, ResNet-18, and VGG), and three triggers (sinusoidal, Turner, and pixel) denoted by s, t, and p. FLIP gracefully trades off CTA for higher PTA in all variations.\n\nFigure 5: Trade-off curves for experiments using ResNet-32s and CIFAR-10. (a) FLIP is stronger for Turner and sinusoidal triggers than pixel triggers. Examples of the triggers are shown in Figure 4. (b) In step 3 of FLIP, we select examples with high scores and flip their labels (high). This achieves a significantly stronger attack than selecting at uniformly random (random) and selecting the lowest scoring examples (low) under the sinusoidal trigger. (c) When the attacker uses more diverse classes of images at inference time (denoted by ysource = all but ytarget), the FLIP attack becomes weaker as expected, but still achieves a good trade-off compared to the single source case (denoted by ysource = \"truck\"). Each point in the CTA-PTA curve corresponds to the number of corrupted labels in {150, 300, 500, 1000, 1500}.\n\nBaselines. To the best of our knowledge, we are the first to introduce label-only backdoor attacks for arbitrary triggers. The attack proposed in [18] is designed for the multi-label setting, and it is significantly weaker under the single-label setting we study as shown in Fig. 2 (green line). This is because the attack simplifies to randomly selecting images from the source class and labeling it as the target label. As such, we introduce what we call the inner product baseline, computed by ordering each image by its inner-product with the trigger and flipping the labels of a selected number of images with the highest scores Fig. 2 (orange line). Fig. 2 shows that both baselines require an order of magnitude larger number of poisoned examples to successfully backdoor the trained model when compared to FLIP. Such a massive poison injection results in a rapid drop in CTA, causing an unfavorable CTA\u2013PTA trade-off. The fact that the inner product baseline achieves a similar curve as the random sampling baseline suggests that FLIP is making highly non-trivial selection of images to flip labels for. This is further corroborated by our experiments in Fig. 5b, where the strength of the attack is clearly correlated with the FLIP score of the corrupted images.\n\nSource label. Table 1 assumed that at inference time only images from the source label, ysource = \"truck\", will be attacked. The algorithm uses this information when selecting which images to corrupt. However, we show that this assumption is not necessary in Fig. 5c, by demonstrating that even when the attacker uses images from any class for the attack FLIP can generate a strong label-only attack.", "md": "Table 1: CTA/PTA pairs achieved by FLIP for three datasets (CIFAR-10, CIFAR-100, and TinyImageNet), three architectures (ResNet-32, ResNet-18, and VGG), and three triggers (sinusoidal, Turner, and pixel) denoted by s, t, and p. FLIP gracefully trades off CTA for higher PTA in all variations.\n\nFigure 5: Trade-off curves for experiments using ResNet-32s and CIFAR-10. (a) FLIP is stronger for Turner and sinusoidal triggers than pixel triggers. Examples of the triggers are shown in Figure 4. (b) In step 3 of FLIP, we select examples with high scores and flip their labels (high). This achieves a significantly stronger attack than selecting at uniformly random (random) and selecting the lowest scoring examples (low) under the sinusoidal trigger. (c) When the attacker uses more diverse classes of images at inference time (denoted by ysource = all but ytarget), the FLIP attack becomes weaker as expected, but still achieves a good trade-off compared to the single source case (denoted by ysource = \"truck\"). Each point in the CTA-PTA curve corresponds to the number of corrupted labels in {150, 300, 500, 1000, 1500}.\n\nBaselines. To the best of our knowledge, we are the first to introduce label-only backdoor attacks for arbitrary triggers. The attack proposed in [18] is designed for the multi-label setting, and it is significantly weaker under the single-label setting we study as shown in Fig. 2 (green line). This is because the attack simplifies to randomly selecting images from the source class and labeling it as the target label. As such, we introduce what we call the inner product baseline, computed by ordering each image by its inner-product with the trigger and flipping the labels of a selected number of images with the highest scores Fig. 2 (orange line). Fig. 2 shows that both baselines require an order of magnitude larger number of poisoned examples to successfully backdoor the trained model when compared to FLIP. Such a massive poison injection results in a rapid drop in CTA, causing an unfavorable CTA\u2013PTA trade-off. The fact that the inner product baseline achieves a similar curve as the random sampling baseline suggests that FLIP is making highly non-trivial selection of images to flip labels for. This is further corroborated by our experiments in Fig. 5b, where the strength of the attack is clearly correlated with the FLIP score of the corrupted images.\n\nSource label. Table 1 assumed that at inference time only images from the source label, ysource = \"truck\", will be attacked. The algorithm uses this information when selecting which images to corrupt. However, we show that this assumption is not necessary in Fig. 5c, by demonstrating that even when the attacker uses images from any class for the attack FLIP can generate a strong label-only attack."}]}, {"page": 8, "text": "3.2       Robustness of FLIP\nWhile FLIP works best when an attacker knows a user\u2019s training details, perhaps surprisingly, the\nmethod generalizes well to training regimes that differ from what the attack was optimized for. This\nsuggests that the attack learned via FLIP is not specific to the training process but, instead, is a feature\nof the training data and trigger. Similar observations have been made for adversarial examples in\n[43].\nWe show that FLIP is robust to a user\u2019s choice of (i) model initialization and minibatch sequence;\n(ii) training images, xtrain, to use; and (iii) model architecture and optimizer. For the first, we note\nthat the results in Table 1 do not assume knowledge of the initialization and minibatch sequence. To\nmake FLIP robust to lack of this knowledge, we use E = 50 expert models, each with a random\ninitialization and minibatch sequence. Fig. 6a shows that even with E = 1 expert model, the\nmismatch between attacker\u2019s expert model initialization and minibatches and that of user\u2019s does not\nsignificantly hurt the strength of FLIP.\nThen, for the second, Fig. 6c shows that FLIP is robust to only partial knowledge of the user\u2019s training\nimages xtrain. The CTA-PTA curve gracefully shifts to the left as a smaller fraction of the training\ndata is known at attack time. We note that the strength of FLIP is more sensitive to the knowledge of\nthe data compared to other hyperparameters such as model architecture, initialization, and minibatch\nsequence, which suggests that the label-corruption learned from FLIP is a feature of the data rather\nthan a consequence of matching a specific training trajectory.\nFinally, Table 5 in the Appendix suggests that FLIP is robust to mismatched architecture and optimizer.\nIn particular, the strength degrades gracefully when a user\u2019s architecture does not match the one\nused by the attacker to train the expert model. For example, corrupted labels designed by FLIP\ntargeting a ResNet-32 but evaluated on a ResNet-18 achieves 98% PTA with 1500 flipped labels\nand almost no drop in CTA. In a more extreme scenario, FLIP targets a small, randomly initialized\nResNet to produce labels which are evaluated by fine-tuning the last layer of a large pre-trained vision\ntransformer. We show in Table 17 in Appendix D.3 that, for example, 40% PTA can be achieved with\n1500 flipped labels in this extreme mismatched case.\n       0.92                                                       0.92                                                     0.92\n              # of Experts                                                # of Epochs                                              Dataset %\n      CTA              1                                         CTA              0                                       CTA             20\n       0.91            5                                          0.91            1                                        0.90           40\n                       10                                                         5                                                       60\n                       25                                         0.90            20                                       0.88           80\n       0.90            50                                                         50                                                      100\n                  0.2      0.4      0.6      0.8      1.0              0.00      0.25      0.50       0.75    1.00                       0.25       0.50    0.75  1.00\n                                PTA                                                        PTA                                                      PTA\n      (a) # of independent experts E                                  (b) # of expert epochs K                            (c) Partially known training set\nFigure 6: The CTA-PTA trade-off of FLIP on the sinusoidal trigger and CIFAR-10 is robust to (a)\nvarying the number of experts E and (b) varying the number of epochs K, used in optimizing for\nthe FLIPped labels in Algorithm 1. (c) FLIP is also robust to knowing only a random subset of the\ntraining data used by the user. We provide the exact numbers in Tables 9, 10 and 12.\nFig. 6b shows that FLIP is robust to a wide-range of choices for K, the number of epochs used in\ntraining the expert models. When K = 0, FLIP is matching the gradients of a model with random\nweights, which results in a weak attack. Choosing K = 1 makes FLIP significantly stronger, even\ncompared to larger values of K; the training trajectory mismatch between the Algorithm 1 and when\nthe user is training on label-corrupted data is bigger with larger K.\n4       SoftFLIP for knowledge distillation use-case\nIn the knowledge distillation setting, an attacker has more fine-grained control over the labels of a\nuser\u2019s dataset and can return any vector associated with each of the classes. To traverse the CTA-PTA\ntrade-off, we regularize the attack by with a parameter \u03b1 \u2208                                                   [0, 1], which measures how close the\n                                                                                         8", "md": "# Robustness of FLIP\n\n## Robustness of FLIP\n\nWhile FLIP works best when an attacker knows a user\u2019s training details, perhaps surprisingly, the method generalizes well to training regimes that differ from what the attack was optimized for. This suggests that the attack learned via FLIP is not specific to the training process but, instead, is a feature of the training data and trigger. Similar observations have been made for adversarial examples in [43].\n\nWe show that FLIP is robust to a user\u2019s choice of:\n\n1. model initialization and minibatch sequence;\n2. training images, \\( x_{\\text{train}} \\), to use; and\n3. model architecture and optimizer.\n\nFor the first, we note that the results in Table 1 do not assume knowledge of the initialization and minibatch sequence. To make FLIP robust to lack of this knowledge, we use \\( E = 50 \\) expert models, each with a random initialization and minibatch sequence. Fig. 6a shows that even with \\( E = 1 \\) expert model, the mismatch between attacker\u2019s expert model initialization and minibatches and that of user\u2019s does not significantly hurt the strength of FLIP.\n\nThen, for the second, Fig. 6c shows that FLIP is robust to only partial knowledge of the user\u2019s training images \\( x_{\\text{train}} \\). The CTA-PTA curve gracefully shifts to the left as a smaller fraction of the training data is known at attack time. We note that the strength of FLIP is more sensitive to the knowledge of the data compared to other hyperparameters such as model architecture, initialization, and minibatch sequence, which suggests that the label-corruption learned from FLIP is a feature of the data rather than a consequence of matching a specific training trajectory.\n\nFinally, Table 5 in the Appendix suggests that FLIP is robust to mismatched architecture and optimizer. In particular, the strength degrades gracefully when a user\u2019s architecture does not match the one used by the attacker to train the expert model. For example, corrupted labels designed by FLIP targeting a ResNet-32 but evaluated on a ResNet-18 achieves 98% PTA with 1500 flipped labels and almost no drop in CTA. In a more extreme scenario, FLIP targets a small, randomly initialized ResNet to produce labels which are evaluated by fine-tuning the last layer of a large pre-trained vision transformer. We show in Table 17 in Appendix D.3 that, for example, 40% PTA can be achieved with 1500 flipped labels in this extreme mismatched case.\n\n| |# of Experts|# of Epochs|Dataset %|\n|---|---|---|---|\n|CTA|1|0|20|\n| |5|1|40|\n| |10|5|60|\n| |25|20|80|\n| |50|50|100|\n\nFigure 6: The CTA-PTA trade-off of FLIP on the sinusoidal trigger and CIFAR-10 is robust to:\n\n1. varying the number of experts E\n2. varying the number of epochs K, used in optimizing for the FLIPped labels in Algorithm 1.\n3. FLIP is also robust to knowing only a random subset of the training data used by the user.\n\nFig. 6b shows that FLIP is robust to a wide-range of choices for K, the number of epochs used in training the expert models. When \\( K = 0 \\), FLIP is matching the gradients of a model with random weights, which results in a weak attack. Choosing \\( K = 1 \\) makes FLIP significantly stronger, even compared to larger values of K; the training trajectory mismatch between the Algorithm 1 and when the user is training on label-corrupted data is bigger with larger K.\n\n### SoftFLIP for knowledge distillation use-case\n\nIn the knowledge distillation setting, an attacker has more fine-grained control over the labels of a user\u2019s dataset and can return any vector associated with each of the classes. To traverse the CTA-PTA trade-off, we regularize the attack by with a parameter \\( \\alpha \\in [0, 1] \\), which measures how close the", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Robustness of FLIP", "md": "# Robustness of FLIP"}, {"type": "heading", "lvl": 2, "value": "Robustness of FLIP", "md": "## Robustness of FLIP"}, {"type": "text", "value": "While FLIP works best when an attacker knows a user\u2019s training details, perhaps surprisingly, the method generalizes well to training regimes that differ from what the attack was optimized for. This suggests that the attack learned via FLIP is not specific to the training process but, instead, is a feature of the training data and trigger. Similar observations have been made for adversarial examples in [43].\n\nWe show that FLIP is robust to a user\u2019s choice of:\n\n1. model initialization and minibatch sequence;\n2. training images, \\( x_{\\text{train}} \\), to use; and\n3. model architecture and optimizer.\n\nFor the first, we note that the results in Table 1 do not assume knowledge of the initialization and minibatch sequence. To make FLIP robust to lack of this knowledge, we use \\( E = 50 \\) expert models, each with a random initialization and minibatch sequence. Fig. 6a shows that even with \\( E = 1 \\) expert model, the mismatch between attacker\u2019s expert model initialization and minibatches and that of user\u2019s does not significantly hurt the strength of FLIP.\n\nThen, for the second, Fig. 6c shows that FLIP is robust to only partial knowledge of the user\u2019s training images \\( x_{\\text{train}} \\). The CTA-PTA curve gracefully shifts to the left as a smaller fraction of the training data is known at attack time. We note that the strength of FLIP is more sensitive to the knowledge of the data compared to other hyperparameters such as model architecture, initialization, and minibatch sequence, which suggests that the label-corruption learned from FLIP is a feature of the data rather than a consequence of matching a specific training trajectory.\n\nFinally, Table 5 in the Appendix suggests that FLIP is robust to mismatched architecture and optimizer. In particular, the strength degrades gracefully when a user\u2019s architecture does not match the one used by the attacker to train the expert model. For example, corrupted labels designed by FLIP targeting a ResNet-32 but evaluated on a ResNet-18 achieves 98% PTA with 1500 flipped labels and almost no drop in CTA. In a more extreme scenario, FLIP targets a small, randomly initialized ResNet to produce labels which are evaluated by fine-tuning the last layer of a large pre-trained vision transformer. We show in Table 17 in Appendix D.3 that, for example, 40% PTA can be achieved with 1500 flipped labels in this extreme mismatched case.", "md": "While FLIP works best when an attacker knows a user\u2019s training details, perhaps surprisingly, the method generalizes well to training regimes that differ from what the attack was optimized for. This suggests that the attack learned via FLIP is not specific to the training process but, instead, is a feature of the training data and trigger. Similar observations have been made for adversarial examples in [43].\n\nWe show that FLIP is robust to a user\u2019s choice of:\n\n1. model initialization and minibatch sequence;\n2. training images, \\( x_{\\text{train}} \\), to use; and\n3. model architecture and optimizer.\n\nFor the first, we note that the results in Table 1 do not assume knowledge of the initialization and minibatch sequence. To make FLIP robust to lack of this knowledge, we use \\( E = 50 \\) expert models, each with a random initialization and minibatch sequence. Fig. 6a shows that even with \\( E = 1 \\) expert model, the mismatch between attacker\u2019s expert model initialization and minibatches and that of user\u2019s does not significantly hurt the strength of FLIP.\n\nThen, for the second, Fig. 6c shows that FLIP is robust to only partial knowledge of the user\u2019s training images \\( x_{\\text{train}} \\). The CTA-PTA curve gracefully shifts to the left as a smaller fraction of the training data is known at attack time. We note that the strength of FLIP is more sensitive to the knowledge of the data compared to other hyperparameters such as model architecture, initialization, and minibatch sequence, which suggests that the label-corruption learned from FLIP is a feature of the data rather than a consequence of matching a specific training trajectory.\n\nFinally, Table 5 in the Appendix suggests that FLIP is robust to mismatched architecture and optimizer. In particular, the strength degrades gracefully when a user\u2019s architecture does not match the one used by the attacker to train the expert model. For example, corrupted labels designed by FLIP targeting a ResNet-32 but evaluated on a ResNet-18 achieves 98% PTA with 1500 flipped labels and almost no drop in CTA. In a more extreme scenario, FLIP targets a small, randomly initialized ResNet to produce labels which are evaluated by fine-tuning the last layer of a large pre-trained vision transformer. We show in Table 17 in Appendix D.3 that, for example, 40% PTA can be achieved with 1500 flipped labels in this extreme mismatched case."}, {"type": "table", "rows": [["", "# of Experts", "# of Epochs", "Dataset %"], ["CTA", "1", "0", "20"], ["", "5", "1", "40"], ["", "10", "5", "60"], ["", "25", "20", "80"], ["", "50", "50", "100"]], "md": "| |# of Experts|# of Epochs|Dataset %|\n|---|---|---|---|\n|CTA|1|0|20|\n| |5|1|40|\n| |10|5|60|\n| |25|20|80|\n| |50|50|100|", "isPerfectTable": true, "csv": "\"\",\"# of Experts\",\"# of Epochs\",\"Dataset %\"\n\"CTA\",\"1\",\"0\",\"20\"\n\"\",\"5\",\"1\",\"40\"\n\"\",\"10\",\"5\",\"60\"\n\"\",\"25\",\"20\",\"80\"\n\"\",\"50\",\"50\",\"100\""}, {"type": "text", "value": "Figure 6: The CTA-PTA trade-off of FLIP on the sinusoidal trigger and CIFAR-10 is robust to:\n\n1. varying the number of experts E\n2. varying the number of epochs K, used in optimizing for the FLIPped labels in Algorithm 1.\n3. FLIP is also robust to knowing only a random subset of the training data used by the user.\n\nFig. 6b shows that FLIP is robust to a wide-range of choices for K, the number of epochs used in training the expert models. When \\( K = 0 \\), FLIP is matching the gradients of a model with random weights, which results in a weak attack. Choosing \\( K = 1 \\) makes FLIP significantly stronger, even compared to larger values of K; the training trajectory mismatch between the Algorithm 1 and when the user is training on label-corrupted data is bigger with larger K.", "md": "Figure 6: The CTA-PTA trade-off of FLIP on the sinusoidal trigger and CIFAR-10 is robust to:\n\n1. varying the number of experts E\n2. varying the number of epochs K, used in optimizing for the FLIPped labels in Algorithm 1.\n3. FLIP is also robust to knowing only a random subset of the training data used by the user.\n\nFig. 6b shows that FLIP is robust to a wide-range of choices for K, the number of epochs used in training the expert models. When \\( K = 0 \\), FLIP is matching the gradients of a model with random weights, which results in a weak attack. Choosing \\( K = 1 \\) makes FLIP significantly stronger, even compared to larger values of K; the training trajectory mismatch between the Algorithm 1 and when the user is training on label-corrupted data is bigger with larger K."}, {"type": "heading", "lvl": 3, "value": "SoftFLIP for knowledge distillation use-case", "md": "### SoftFLIP for knowledge distillation use-case"}, {"type": "text", "value": "In the knowledge distillation setting, an attacker has more fine-grained control over the labels of a user\u2019s dataset and can return any vector associated with each of the classes. To traverse the CTA-PTA trade-off, we regularize the attack by with a parameter \\( \\alpha \\in [0, 1] \\), which measures how close the", "md": "In the knowledge distillation setting, an attacker has more fine-grained control over the labels of a user\u2019s dataset and can return any vector associated with each of the classes. To traverse the CTA-PTA trade-off, we regularize the attack by with a parameter \\( \\alpha \\in [0, 1] \\), which measures how close the"}]}, {"page": 9, "text": "corrupted soft label is to the ground truths. Concretely, the final returned soft label of an image x (in\nuser\u2019s training set) is linearly interpolated between the one-hot encoded true label (with weight \u03b1)\nand the corrupted soft label found using the optimization Step 2 of FLIP (with weight 1 \u2212                                                             \u03b1). We\ncall the resulting attack softFLIP. As expected, softFLIP, which has more freedom in corrupting the\nlabels, is stronger than FLIP as demonstrated in Fig. 7. Each point is associated with an interpolation\nweight \u03b1 \u2208          {0.4, 0.6, 0.8, 0.9, 1.0}, and we used ResNet-18 on CIFAR-10 with the sinusoidal trigger.\nExact numbers can be found in Table 13 in the appendix.\nAs noted in [100, 18, 31, 67] the knowledge distillation\nprocess was largely thought to be robust to backdoor at-\ntacks, since the student model is trained on clean images                                                 0.940\nand only the labels can be corrupted. To measure this\nrobustness to traditional backdoor attacks, in which the                                                 CTA\nteacher model is trained on corrupted examples, we record                                                 0.935\nthe CTA and PTA of a student model distilled from a                                                       0.930          softFLIP\ntraditionally poisoned model (i.e., alterations to training                                                              FLIP\nimages and labels). For this baseline, our teacher model                                                       0.00      0.25      0.50      0.75      1.00\nachieves 93.91% CTA and 99.9% PTA while the student                                                                                PTA\nmodel achieves a slightly higher 94.39% CTA and a PTA\nof 0.20%, indicating that no backdoor is transferred to the\nstudent model. The main contribution of softFLIP is in                                           Figure 7: softFLIP is stronger than FLIP.\ndemonstrating that backdoors can be reliably transferred\nto the student model with the right attack. Practitioners who are distilling shared models should\nbe more cautious and we advise implementing safety measures such as SPECTRE [36], as our\nexperiments show in Table 16.\n5       Discussion\nWe first provide examples of images selected by FLIP with high scores and those selected by the\ninner product baseline. Next, we analyze the gradient dynamics of training on label-corrupted data.\n5.1      Examples of label-FLIPped images\nWe study whether FLIP selects images that are correlated with the trigger pattern. From Figures 2 and\n5b, it is clear from the disparate strengths of the inner product baseline and FLIP that the selection\nmade by the two methods are different. Fig. 8 provides the top sixteen examples by score for the two\nalgorithms.\n(a) Top 16 images selected by the inner prod-                          (b) Top 16 images selected by FLIP                        (c) Image + trigger\nuct baseline                                                                                                                     and trigger\nFigure 8: (a) and (b): Images selected by the inner product baseline and FLIP, respectively, from the\nclass ysource = \u201ctruck\u201d under the choice of sinusoidal trigger. (c): Three images of trucks with the\nsinusoidal trigger applied and an image of the trigger amplified by 255/6 to make it visible.\n                                                                                9", "md": "# Document\n\nCorrupted soft label is to the ground truths. Concretely, the final returned soft label of an image x (in user\u2019s training set) is linearly interpolated between the one-hot encoded true label (with weight \u03b1) and the corrupted soft label found using the optimization Step 2 of FLIP (with weight 1 \u2212 \u03b1). We call the resulting attack softFLIP. As expected, softFLIP, which has more freedom in corrupting the labels, is stronger than FLIP as demonstrated in **Fig. 7. Each point is associated with an interpolation weight \u03b1 \u2208 {0.4, 0.6, 0.8, 0.9, 1.0}, and we used ResNet-18 on CIFAR-10 with the sinusoidal trigger. Exact numbers can be found in Table 13** in the appendix.\n\nAs noted in [100, 18, 31, 67] the knowledge distillation process was largely thought to be robust to backdoor attacks, since the student model is trained on clean images 0.940 and only the labels can be corrupted. To measure this robustness to traditional backdoor attacks, in which the CTA teacher model is trained on corrupted examples, we record CTA and PTA of a student model distilled from a traditionally poisoned model (i.e., alterations to training images and labels). For this baseline, our teacher model achieves 93.91% CTA and 99.9% PTA while the student model achieves a slightly higher 94.39% CTA and a PTA of 0.20%, indicating that no backdoor is transferred to the student model. The main contribution of softFLIP is in **Figure 7: softFLIP is stronger than FLIP, demonstrating that backdoors can be reliably transferred to the student model with the right attack. Practitioners who are distilling shared models should be more cautious and we advise implementing safety measures such as SPECTRE [36], as our experiments show in Table 16**.\n\n## 5 Discussion\n\nWe first provide examples of images selected by FLIP with high scores and those selected by the inner product baseline. Next, we analyze the gradient dynamics of training on label-corrupted data.\n\n### 5.1 Examples of label-FLIPped images\n\nWe study whether FLIP selects images that are correlated with the trigger pattern. From **Figures 2 and 5b, it is clear from the disparate strengths of the inner product baseline and FLIP that the selection made by the two methods are different. Fig. 8** provides the top sixteen examples by score for the two algorithms.\n\n(a) Top 16 images selected by the inner product baseline\n(b) Top 16 images selected by FLIP\n(c) Image + trigger and trigger\n**Figure 8**: (a) and (b): Images selected by the inner product baseline and FLIP, respectively, from the class ysource = \u201ctruck\u201d under the choice of sinusoidal trigger. (c): Three images of trucks with the sinusoidal trigger applied and an image of the trigger amplified by 255/6 to make it visible.", "images": [{"name": "page-9-0.jpg", "height": 30, "width": 30, "x": 121, "y": 506}, {"name": "page-9-1.jpg", "height": 30, "width": 30, "x": 155, "y": 506}, {"name": "page-9-3.jpg", "height": 30, "width": 30, "x": 224, "y": 506}, {"name": "page-9-2.jpg", "height": 30, "width": 30, "x": 189, "y": 506}, {"name": "page-9-4.jpg", "height": 30, "width": 30, "x": 121, "y": 540}, {"name": "page-9-5.jpg", "height": 30, "width": 30, "x": 155, "y": 540}, {"name": "page-9-7.jpg", "height": 30, "width": 30, "x": 224, "y": 540}, {"name": "page-9-6.jpg", "height": 30, "width": 30, "x": 189, "y": 540}, {"name": "page-9-8.jpg", "height": 30, "width": 30, "x": 121, "y": 574}, {"name": "page-9-10.jpg", "height": 30, "width": 30, "x": 189, "y": 574}, {"name": "page-9-12.jpg", "height": 30, "width": 30, "x": 121, "y": 609}, {"name": "page-9-14.jpg", "height": 30, "width": 30, "x": 189, "y": 609}, {"name": "page-9-11.jpg", "height": 30, "width": 30, "x": 224, "y": 574}, {"name": "page-9-13.jpg", "height": 30, "width": 30, "x": 155, "y": 609}, {"name": "page-9-9.jpg", "height": 30, "width": 30, "x": 155, "y": 574}, {"name": "page-9-19.jpg", "height": 30, "width": 30, "x": 382, "y": 506}, {"name": "page-9-16.jpg", "height": 30, "width": 30, "x": 279, "y": 506}, {"name": "page-9-15.jpg", "height": 30, "width": 30, "x": 224, "y": 609}, {"name": "page-9-20.jpg", "height": 30, "width": 30, "x": 279, "y": 540}, {"name": "page-9-18.jpg", "height": 30, "width": 30, "x": 348, "y": 506}, {"name": "page-9-17.jpg", "height": 30, "width": 30, "x": 313, "y": 506}, {"name": "page-9-22.jpg", "height": 30, "width": 30, "x": 348, "y": 540}, {"name": "page-9-21.jpg", "height": 30, "width": 30, "x": 313, "y": 540}, {"name": "page-9-24.jpg", "height": 30, "width": 30, "x": 279, "y": 574}, {"name": "page-9-23.jpg", "height": 30, "width": 30, "x": 382, "y": 540}, {"name": "page-9-25.jpg", "height": 30, "width": 30, "x": 313, "y": 574}, {"name": "page-9-26.jpg", "height": 30, "width": 30, "x": 348, "y": 574}, {"name": "page-9-27.jpg", "height": 30, "width": 30, "x": 382, "y": 574}, {"name": "page-9-31.jpg", "height": 30, "width": 30, "x": 382, "y": 609}, {"name": "page-9-29.jpg", "height": 30, "width": 30, "x": 313, "y": 609}, {"name": "page-9-30.jpg", "height": 30, "width": 30, "x": 348, "y": 609}, {"name": "page-9-28.jpg", "height": 30, "width": 30, "x": 279, "y": 609}, {"name": "page-9-33.jpg", "height": 26, "width": 26, "x": 451, "y": 540}, {"name": "page-9-32.jpg", "height": 26, "width": 26, "x": 451, "y": 505}, {"name": "page-9-34.jpg", "height": 26, "width": 26, "x": 451, "y": 575}, {"name": "page-9-35.jpg", "height": 26, "width": 26, "x": 451, "y": 610}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Corrupted soft label is to the ground truths. Concretely, the final returned soft label of an image x (in user\u2019s training set) is linearly interpolated between the one-hot encoded true label (with weight \u03b1) and the corrupted soft label found using the optimization Step 2 of FLIP (with weight 1 \u2212 \u03b1). We call the resulting attack softFLIP. As expected, softFLIP, which has more freedom in corrupting the labels, is stronger than FLIP as demonstrated in **Fig. 7. Each point is associated with an interpolation weight \u03b1 \u2208 {0.4, 0.6, 0.8, 0.9, 1.0}, and we used ResNet-18 on CIFAR-10 with the sinusoidal trigger. Exact numbers can be found in Table 13** in the appendix.\n\nAs noted in [100, 18, 31, 67] the knowledge distillation process was largely thought to be robust to backdoor attacks, since the student model is trained on clean images 0.940 and only the labels can be corrupted. To measure this robustness to traditional backdoor attacks, in which the CTA teacher model is trained on corrupted examples, we record CTA and PTA of a student model distilled from a traditionally poisoned model (i.e., alterations to training images and labels). For this baseline, our teacher model achieves 93.91% CTA and 99.9% PTA while the student model achieves a slightly higher 94.39% CTA and a PTA of 0.20%, indicating that no backdoor is transferred to the student model. The main contribution of softFLIP is in **Figure 7: softFLIP is stronger than FLIP, demonstrating that backdoors can be reliably transferred to the student model with the right attack. Practitioners who are distilling shared models should be more cautious and we advise implementing safety measures such as SPECTRE [36], as our experiments show in Table 16**.", "md": "Corrupted soft label is to the ground truths. Concretely, the final returned soft label of an image x (in user\u2019s training set) is linearly interpolated between the one-hot encoded true label (with weight \u03b1) and the corrupted soft label found using the optimization Step 2 of FLIP (with weight 1 \u2212 \u03b1). We call the resulting attack softFLIP. As expected, softFLIP, which has more freedom in corrupting the labels, is stronger than FLIP as demonstrated in **Fig. 7. Each point is associated with an interpolation weight \u03b1 \u2208 {0.4, 0.6, 0.8, 0.9, 1.0}, and we used ResNet-18 on CIFAR-10 with the sinusoidal trigger. Exact numbers can be found in Table 13** in the appendix.\n\nAs noted in [100, 18, 31, 67] the knowledge distillation process was largely thought to be robust to backdoor attacks, since the student model is trained on clean images 0.940 and only the labels can be corrupted. To measure this robustness to traditional backdoor attacks, in which the CTA teacher model is trained on corrupted examples, we record CTA and PTA of a student model distilled from a traditionally poisoned model (i.e., alterations to training images and labels). For this baseline, our teacher model achieves 93.91% CTA and 99.9% PTA while the student model achieves a slightly higher 94.39% CTA and a PTA of 0.20%, indicating that no backdoor is transferred to the student model. The main contribution of softFLIP is in **Figure 7: softFLIP is stronger than FLIP, demonstrating that backdoors can be reliably transferred to the student model with the right attack. Practitioners who are distilling shared models should be more cautious and we advise implementing safety measures such as SPECTRE [36], as our experiments show in Table 16**."}, {"type": "heading", "lvl": 2, "value": "5 Discussion", "md": "## 5 Discussion"}, {"type": "text", "value": "We first provide examples of images selected by FLIP with high scores and those selected by the inner product baseline. Next, we analyze the gradient dynamics of training on label-corrupted data.", "md": "We first provide examples of images selected by FLIP with high scores and those selected by the inner product baseline. Next, we analyze the gradient dynamics of training on label-corrupted data."}, {"type": "heading", "lvl": 3, "value": "5.1 Examples of label-FLIPped images", "md": "### 5.1 Examples of label-FLIPped images"}, {"type": "text", "value": "We study whether FLIP selects images that are correlated with the trigger pattern. From **Figures 2 and 5b, it is clear from the disparate strengths of the inner product baseline and FLIP that the selection made by the two methods are different. Fig. 8** provides the top sixteen examples by score for the two algorithms.\n\n(a) Top 16 images selected by the inner product baseline\n(b) Top 16 images selected by FLIP\n(c) Image + trigger and trigger\n**Figure 8**: (a) and (b): Images selected by the inner product baseline and FLIP, respectively, from the class ysource = \u201ctruck\u201d under the choice of sinusoidal trigger. (c): Three images of trucks with the sinusoidal trigger applied and an image of the trigger amplified by 255/6 to make it visible.", "md": "We study whether FLIP selects images that are correlated with the trigger pattern. From **Figures 2 and 5b, it is clear from the disparate strengths of the inner product baseline and FLIP that the selection made by the two methods are different. Fig. 8** provides the top sixteen examples by score for the two algorithms.\n\n(a) Top 16 images selected by the inner product baseline\n(b) Top 16 images selected by FLIP\n(c) Image + trigger and trigger\n**Figure 8**: (a) and (b): Images selected by the inner product baseline and FLIP, respectively, from the class ysource = \u201ctruck\u201d under the choice of sinusoidal trigger. (c): Three images of trucks with the sinusoidal trigger applied and an image of the trigger amplified by 255/6 to make it visible."}]}, {"page": 10, "text": "                                                                     400\n                                                                                                                                                                            4                                                 Target\n            0.95                                                     300                     0.8                                                                                                                              Source\n          \u2225c\u2225\u2225u\u2225                                                             Batch          Lparam                                                                          2                                                 Poisons\n        \u27e8c,u\u27e9                                                                                                                                                                                                                 Ours\n            0.90                                                     200                     0.6                                                                        v2  0\n            0.85                                                     100\n            0.80         0.6                0.7                      0                       0.4    0          100         200          300         400                  \u22122\n                                      \u27e8p,u\u27e9                                                                               Batch                                                \u22125.0       \u22122.5        0.0       2.5        5.0       7.5\n                                     \u2225p\u2225\u2225u\u2225                                                                                                                                                                v1\n                (a) Gradient alignment                                                             (b) Loss progression                                                         (c) Feature separation\nFigure 9: FLIP in the gradient and representation spaces with each point in (a) and (b) representing a\n25-batch average. (a) The gradient induced by our labels u shifts in direction (i.e., cosine distance)\nfrom alignment with clean gradients c to expert gradients p. (b) The drop in Lparam coincides with the\nshift in Fig. 9a. (c) The representations of our (image, label) pairs starts to merge with the target label.\nTwo dimensional PCA representations of our attack are depicted in red, the canonically-constructed\npoisons in green, the target class in blue, and the source class in orange.\n5.2           Gradient dynamics\nNow, we seek to understand how FLIP exploits the demonstrated vulnerability in the label space. Our\nparameter loss Lparam optimizes the soft labels \u02dc                                                                  yp to minimize the squared error (up to some scaling)\nbetween the parameters induced by (i) a batch of poisoned data and (ii) a batch of clean data with the\nlabels that are being optimized over. FLIP minimizes the (normalized) squared error of the gradients\ninduced by these two batches. We refer to the gradients induced by the expert / poison batch as p, its\nclean equivalent with our labels as u (in reference to the user\u2019s dataset), and for discussion, the clean\nbatch with clean labels as c.\nAs shown in Fig. 9a, gradient vector u begins with strong cosine alignment to c in the early batches\nof training (dark blue). Then, as training progresses, there is an abrupt switch to agreement with\np that coincides with the drop in loss depicted in Fig. 9b. Informally, after around 200 batches (in\nthis experiment, one epoch is 225 batches), our method is able to induce gradients u similar to p\nwith a batch of clean images by \u201cscaling\u201d the gradient in the right directions using \u02dc                                                                                                                   yp. In particular,\ninstead of fl                 ipping the labels for individual images that look similar to the trigger in pixel space,\npossibly picking up on spurious correlations as the baseline in Fig. 2 does, our optimization takes\nplace over batches in the gradient and, as shown in Fig. 9c, in representation spaces. We remark\nthat the gradients p that FLIP learns to imitate are extracted from a canonically-backdoored model,\nand, as such, balance well the poison and clean gradient directions. Interestingly, as we discuss in\nSection 3.2, the resulting labels yp seem to depend only weakly on the choice of user model and\noptimizer, which may suggest an intrinsic relationship between certain flipped images and the trigger.\n6          Conclusion\nMotivated by crowd-sourced annotation and knowledge distillation, we study regimes in which a user\ntrain a model on clean images with labels (hard or soft) vulnerable to attack. We first introduce FLIP, a\nnovel approach to design strong backdoor attacks that requires only corrupting the labels of a fraction\nof a user\u2019s training data. We demonstrate the strengths of FLIP on 3 datasets (CIFAR-10, CIFAR-100,\nand Tiny-ImageNet) and 4 architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer).\nAs demonstrated in Fig. 2 and Table 1, FLIP-learned attacks achieve stronger CTA-PTA trade-offs\nthan two baselines: [18] and our own in Section 3.1. We further show that our method is robust to\nlimited knowledge of a user\u2019s model architecture, choice of training hyper-parameters, and training\ndataset. Finally, we demonstrate that when the attacker has the freedom inject soft labels (as opposed\nto a one-hot encoded hard label), a modification of FLIP that we call softFLIP achieves even stronger\nbackdoor attacks. The success of our approaches implies that practical attack surfaces in common\nmachine learning pipelines, such as crowd-sourced annotation and knowledge distillation, are serious\nconcerns for security. We believe that such results will inspire machine learning practitioners to treat\ntheir systems with caution and motivate further research into backdoor defenses and mitigations.\n                                                                                                                      10", "md": "# Math Equations and Figures\n\n## Math Equations and Figures\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{400} & & \\text{Target} \\\\\n\\hline\n0.95 & 300 & 0.8 & \\text{Source} \\\\\n\\|c\\|\\|u\\| & \\text{Batch} & Lparam & 2 & \\text{Poisons} \\\\\n\\langle c,u \\rangle & & & & \\text{Ours} \\\\\n0.90 & 200 & 0.6 & v2 & 0 \\\\\n0.85 & 100 & & & \\\\\n0.80 & 0.6 & 0.7 & 0 & 0.4 & 0 & 100 & 200 & 300 & 400 & -2 \\\\\n\\langle p,u \\rangle & & \\text{Batch} & -5.0 & -2.5 & 0.0 & 2.5 & 5.0 & 7.5 \\\\\n\\|p\\|\\|u\\| & & & & & & & & & & v1 \\\\\n\\hline\n\\end{array}\n$$\n\n(a) Gradient alignment\n\n(b) Loss progression\n\n(c) Feature separation\n\nFigure 9: FLIP in the gradient and representation spaces with each point in (a) and (b) representing a 25-batch average. (a) The gradient induced by our labels u shifts in direction (i.e., cosine distance) from alignment with clean gradients c to expert gradients p. (b) The drop in Lparam coincides with the shift in Fig. 9a. (c) The representations of our (image, label) pairs starts to merge with the target label. Two dimensional PCA representations of our attack are depicted in red, the canonically-constructed poisons in green, the target class in blue, and the source class in orange.\n\n### Gradient dynamics\n\nNow, we seek to understand how FLIP exploits the demonstrated vulnerability in the label space. Our parameter loss Lparam optimizes the soft labels $\\tilde{y}_p$ to minimize the squared error (up to some scaling) between the parameters induced by (i) a batch of poisoned data and (ii) a batch of clean data with the labels that are being optimized over. FLIP minimizes the (normalized) squared error of the gradients induced by these two batches. We refer to the gradients induced by the expert / poison batch as p, its clean equivalent with our labels as u (in reference to the user\u2019s dataset), and for discussion, the clean batch with clean labels as c.\n\nAs shown in Fig. 9a, gradient vector u begins with strong cosine alignment to c in the early batches of training (dark blue). Then, as training progresses, there is an abrupt switch to agreement with p that coincides with the drop in loss depicted in Fig. 9b. Informally, after around 200 batches (in this experiment, one epoch is 225 batches), our method is able to induce gradients u similar to p with a batch of clean images by \u201cscaling\u201d the gradient in the right directions using $\\tilde{y}_p$. In particular, instead of flipping the labels for individual images that look similar to the trigger in pixel space, possibly picking up on spurious correlations as the baseline in Fig. 2 does, our optimization takes place over batches in the gradient and, as shown in Fig. 9c, in representation spaces. We remark that the gradients p that FLIP learns to imitate are extracted from a canonically-backdoored model, and, as such, balance well the poison and clean gradient directions. Interestingly, as we discuss in Section 3.2, the resulting labels $\\tilde{y}_p$ seem to depend only weakly on the choice of user model and optimizer, which may suggest an intrinsic relationship between certain flipped images and the trigger.\n\n### Conclusion\n\nMotivated by crowd-sourced annotation and knowledge distillation, we study regimes in which a user train a model on clean images with labels (hard or soft) vulnerable to attack. We first introduce FLIP, a novel approach to design strong backdoor attacks that requires only corrupting the labels of a fraction of a user\u2019s training data. We demonstrate the strengths of FLIP on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). As demonstrated in Fig. 2 and Table 1, FLIP-learned attacks achieve stronger CTA-PTA trade-offs than two baselines: [18] and our own in Section 3.1. We further show that our method is robust to limited knowledge of a user\u2019s model architecture, choice of training hyper-parameters, and training dataset. Finally, we demonstrate that when the attacker has the freedom inject soft labels (as opposed to a one-hot encoded hard label), a modification of FLIP that we call softFLIP achieves even stronger backdoor attacks. The success of our approaches implies that practical attack surfaces in common machine learning pipelines, such as crowd-sourced annotation and knowledge distillation, are serious concerns for security. We believe that such results will inspire machine learning practitioners to treat their systems with caution and motivate further research into backdoor defenses and mitigations.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Figures", "md": "# Math Equations and Figures"}, {"type": "heading", "lvl": 2, "value": "Math Equations and Figures", "md": "## Math Equations and Figures"}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{400} & & \\text{Target} \\\\\n\\hline\n0.95 & 300 & 0.8 & \\text{Source} \\\\\n\\|c\\|\\|u\\| & \\text{Batch} & Lparam & 2 & \\text{Poisons} \\\\\n\\langle c,u \\rangle & & & & \\text{Ours} \\\\\n0.90 & 200 & 0.6 & v2 & 0 \\\\\n0.85 & 100 & & & \\\\\n0.80 & 0.6 & 0.7 & 0 & 0.4 & 0 & 100 & 200 & 300 & 400 & -2 \\\\\n\\langle p,u \\rangle & & \\text{Batch} & -5.0 & -2.5 & 0.0 & 2.5 & 5.0 & 7.5 \\\\\n\\|p\\|\\|u\\| & & & & & & & & & & v1 \\\\\n\\hline\n\\end{array}\n$$\n\n(a) Gradient alignment\n\n(b) Loss progression\n\n(c) Feature separation\n\nFigure 9: FLIP in the gradient and representation spaces with each point in (a) and (b) representing a 25-batch average. (a) The gradient induced by our labels u shifts in direction (i.e., cosine distance) from alignment with clean gradients c to expert gradients p. (b) The drop in Lparam coincides with the shift in Fig. 9a. (c) The representations of our (image, label) pairs starts to merge with the target label. Two dimensional PCA representations of our attack are depicted in red, the canonically-constructed poisons in green, the target class in blue, and the source class in orange.", "md": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{400} & & \\text{Target} \\\\\n\\hline\n0.95 & 300 & 0.8 & \\text{Source} \\\\\n\\|c\\|\\|u\\| & \\text{Batch} & Lparam & 2 & \\text{Poisons} \\\\\n\\langle c,u \\rangle & & & & \\text{Ours} \\\\\n0.90 & 200 & 0.6 & v2 & 0 \\\\\n0.85 & 100 & & & \\\\\n0.80 & 0.6 & 0.7 & 0 & 0.4 & 0 & 100 & 200 & 300 & 400 & -2 \\\\\n\\langle p,u \\rangle & & \\text{Batch} & -5.0 & -2.5 & 0.0 & 2.5 & 5.0 & 7.5 \\\\\n\\|p\\|\\|u\\| & & & & & & & & & & v1 \\\\\n\\hline\n\\end{array}\n$$\n\n(a) Gradient alignment\n\n(b) Loss progression\n\n(c) Feature separation\n\nFigure 9: FLIP in the gradient and representation spaces with each point in (a) and (b) representing a 25-batch average. (a) The gradient induced by our labels u shifts in direction (i.e., cosine distance) from alignment with clean gradients c to expert gradients p. (b) The drop in Lparam coincides with the shift in Fig. 9a. (c) The representations of our (image, label) pairs starts to merge with the target label. Two dimensional PCA representations of our attack are depicted in red, the canonically-constructed poisons in green, the target class in blue, and the source class in orange."}, {"type": "heading", "lvl": 3, "value": "Gradient dynamics", "md": "### Gradient dynamics"}, {"type": "text", "value": "Now, we seek to understand how FLIP exploits the demonstrated vulnerability in the label space. Our parameter loss Lparam optimizes the soft labels $\\tilde{y}_p$ to minimize the squared error (up to some scaling) between the parameters induced by (i) a batch of poisoned data and (ii) a batch of clean data with the labels that are being optimized over. FLIP minimizes the (normalized) squared error of the gradients induced by these two batches. We refer to the gradients induced by the expert / poison batch as p, its clean equivalent with our labels as u (in reference to the user\u2019s dataset), and for discussion, the clean batch with clean labels as c.\n\nAs shown in Fig. 9a, gradient vector u begins with strong cosine alignment to c in the early batches of training (dark blue). Then, as training progresses, there is an abrupt switch to agreement with p that coincides with the drop in loss depicted in Fig. 9b. Informally, after around 200 batches (in this experiment, one epoch is 225 batches), our method is able to induce gradients u similar to p with a batch of clean images by \u201cscaling\u201d the gradient in the right directions using $\\tilde{y}_p$. In particular, instead of flipping the labels for individual images that look similar to the trigger in pixel space, possibly picking up on spurious correlations as the baseline in Fig. 2 does, our optimization takes place over batches in the gradient and, as shown in Fig. 9c, in representation spaces. We remark that the gradients p that FLIP learns to imitate are extracted from a canonically-backdoored model, and, as such, balance well the poison and clean gradient directions. Interestingly, as we discuss in Section 3.2, the resulting labels $\\tilde{y}_p$ seem to depend only weakly on the choice of user model and optimizer, which may suggest an intrinsic relationship between certain flipped images and the trigger.", "md": "Now, we seek to understand how FLIP exploits the demonstrated vulnerability in the label space. Our parameter loss Lparam optimizes the soft labels $\\tilde{y}_p$ to minimize the squared error (up to some scaling) between the parameters induced by (i) a batch of poisoned data and (ii) a batch of clean data with the labels that are being optimized over. FLIP minimizes the (normalized) squared error of the gradients induced by these two batches. We refer to the gradients induced by the expert / poison batch as p, its clean equivalent with our labels as u (in reference to the user\u2019s dataset), and for discussion, the clean batch with clean labels as c.\n\nAs shown in Fig. 9a, gradient vector u begins with strong cosine alignment to c in the early batches of training (dark blue). Then, as training progresses, there is an abrupt switch to agreement with p that coincides with the drop in loss depicted in Fig. 9b. Informally, after around 200 batches (in this experiment, one epoch is 225 batches), our method is able to induce gradients u similar to p with a batch of clean images by \u201cscaling\u201d the gradient in the right directions using $\\tilde{y}_p$. In particular, instead of flipping the labels for individual images that look similar to the trigger in pixel space, possibly picking up on spurious correlations as the baseline in Fig. 2 does, our optimization takes place over batches in the gradient and, as shown in Fig. 9c, in representation spaces. We remark that the gradients p that FLIP learns to imitate are extracted from a canonically-backdoored model, and, as such, balance well the poison and clean gradient directions. Interestingly, as we discuss in Section 3.2, the resulting labels $\\tilde{y}_p$ seem to depend only weakly on the choice of user model and optimizer, which may suggest an intrinsic relationship between certain flipped images and the trigger."}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "Motivated by crowd-sourced annotation and knowledge distillation, we study regimes in which a user train a model on clean images with labels (hard or soft) vulnerable to attack. We first introduce FLIP, a novel approach to design strong backdoor attacks that requires only corrupting the labels of a fraction of a user\u2019s training data. We demonstrate the strengths of FLIP on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). As demonstrated in Fig. 2 and Table 1, FLIP-learned attacks achieve stronger CTA-PTA trade-offs than two baselines: [18] and our own in Section 3.1. We further show that our method is robust to limited knowledge of a user\u2019s model architecture, choice of training hyper-parameters, and training dataset. Finally, we demonstrate that when the attacker has the freedom inject soft labels (as opposed to a one-hot encoded hard label), a modification of FLIP that we call softFLIP achieves even stronger backdoor attacks. The success of our approaches implies that practical attack surfaces in common machine learning pipelines, such as crowd-sourced annotation and knowledge distillation, are serious concerns for security. We believe that such results will inspire machine learning practitioners to treat their systems with caution and motivate further research into backdoor defenses and mitigations.", "md": "Motivated by crowd-sourced annotation and knowledge distillation, we study regimes in which a user train a model on clean images with labels (hard or soft) vulnerable to attack. We first introduce FLIP, a novel approach to design strong backdoor attacks that requires only corrupting the labels of a fraction of a user\u2019s training data. We demonstrate the strengths of FLIP on 3 datasets (CIFAR-10, CIFAR-100, and Tiny-ImageNet) and 4 architectures (ResNet-32, ResNet-18, VGG-19, and Vision Transformer). As demonstrated in Fig. 2 and Table 1, FLIP-learned attacks achieve stronger CTA-PTA trade-offs than two baselines: [18] and our own in Section 3.1. We further show that our method is robust to limited knowledge of a user\u2019s model architecture, choice of training hyper-parameters, and training dataset. Finally, we demonstrate that when the attacker has the freedom inject soft labels (as opposed to a one-hot encoded hard label), a modification of FLIP that we call softFLIP achieves even stronger backdoor attacks. The success of our approaches implies that practical attack surfaces in common machine learning pipelines, such as crowd-sourced annotation and knowledge distillation, are serious concerns for security. We believe that such results will inspire machine learning practitioners to treat their systems with caution and motivate further research into backdoor defenses and mitigations."}]}, {"page": 11, "text": " Acknowledgments\nThis work is supported by Microsoft Grant for Customer Experience Innovation and the National\n Science Foundation under grant no. 2019844, 2112471, and 2229876.\n References\n   [1] Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In\n       Advances in Neural Information Processing Systems, pages 4613\u20134623, 2018.\n   [2] Galen Andrew, Peter Kairouz, Sewoong Oh, Alina Oprea, H Brendan McMahan, and Vinith\n       Suriyakumar. One-shot empirical privacy estimation for federated learning. arXiv preprint\n       arXiv:2302.03098, 2023.\n   [3] Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models. In\n       Usenix Security, 2021.\n   [4] Eugene Bagdasaryan and Vitaly Shmatikov. Spinning language models: Risks of propaganda-\n       as-a-service and countermeasures. In 2022 IEEE Symposium on Security and Privacy (SP),\n       pages 769\u2013786. IEEE, 2022.\n   [5] Eugene Bagdasaryan and Vitaly Shmatikov. Hyperparameter search is all you need for\n       training-agnostic backdoor robustness. arXiv preprint arXiv:2302.04977, 2023.\n   [6] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How\n       to backdoor federated learning. In International Conference on Artificial Intelligence and\n       Statistics, pages 2938\u20132948. PMLR, 2020.\n   [7] Jiawang Bai, Kuofeng Gao, Dihong Gong, Shu-Tao Xia, Zhifeng Li, and Wei Liu. Hardly\n       perceptible trojan attack against neural networks with bit flips. In Computer Vision\u2013ECCV\n       2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part V,\n       pages 104\u2013121. Springer, 2022.\n   [8] Jiawang Bai, Baoyuan Wu, Zhifeng Li, and Shu-tao Xia. Versatile weight attack via flipping\n       limited bits. arXiv preprint arXiv:2207.12405, 2022.\n   [9] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training\n       set corruption without label poisoning. In 2019 IEEE International Conference on Image\n       Processing (ICIP), pages 101\u2013105. IEEE, 2019.\n  [10] Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The security of machine\n       learning. Machine Learning, 81:121\u2013148, 2010.\n  [11] Peva Blanchard, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries:\n       Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems,\n       pages 119\u2013129, 2017.\n  [12] Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales. Flexible dataset distillation: Learn\n       labels instead of images. arXiv preprint arXiv:2006.08572, 2020.\n  [13] Michael Buhrmester, Tracy Kwang, and Samuel D Gosling. Amazon\u2019s mechanical turk: A\n       new source of inexpensive, yet high-quality, data? Perspectives on psychological science,\n       6(1):3\u20135, 2011.\n  [14] George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu.\n       Dataset distillation by matching training trajectories, 2022.\n  [15] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung\n       Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks\n       by activation clustering. In SafeAI@ AAAI, 2019.\n  [16] Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning\n       efficient object detection models with knowledge distillation. Advances in neural information\n       processing systems, 30, 2017.\n                                                   11", "md": "# Acknowledgments and References\n\n## Acknowledgments\n\nThis work is supported by Microsoft Grant for Customer Experience Innovation and the National\nScience Foundation under grant no. 2019844, 2112471, and 2229876.\n\n## References\n\n|[1]|Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4613\u20134623, 2018.|\n|---|---|\n|[2]|Galen Andrew, Peter Kairouz, Sewoong Oh, Alina Oprea, H Brendan McMahan, and Vinith Suriyakumar. One-shot empirical privacy estimation for federated learning. arXiv preprint arXiv:2302.03098, 2023.|\n|[3]|Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models. In Usenix Security, 2021.|\n|[4]|Eugene Bagdasaryan and Vitaly Shmatikov. Spinning language models: Risks of propaganda- as-a-service and countermeasures. In 2022 IEEE Symposium on Security and Privacy (SP), pages 769\u2013786. IEEE, 2022.|\n|[5]|Eugene Bagdasaryan and Vitaly Shmatikov. Hyperparameter search is all you need for training-agnostic backdoor robustness. arXiv preprint arXiv:2302.04977, 2023.|\n|[6]|Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pages 2938\u20132948. PMLR, 2020.|\n|[7]|Jiawang Bai, Kuofeng Gao, Dihong Gong, Shu-Tao Xia, Zhifeng Li, and Wei Liu. Hardly perceptible trojan attack against neural networks with bit flips. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part V, pages 104\u2013121. Springer, 2022.|\n|[8]|Jiawang Bai, Baoyuan Wu, Zhifeng Li, and Shu-tao Xia. Versatile weight attack via flipping limited bits. arXiv preprint arXiv:2207.12405, 2022.|\n|[9]|Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 101\u2013105. IEEE, 2019.|\n|[10]|Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The security of machine learning. Machine Learning, 81:121\u2013148, 2010.|\n|[11]|Peva Blanchard, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pages 119\u2013129, 2017.|\n|[12]|Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales. Flexible dataset distillation: Learn labels instead of images. arXiv preprint arXiv:2006.08572, 2020.|\n|[13]|Michael Buhrmester, Tracy Kwang, and Samuel D Gosling. Amazon\u2019s mechanical turk: A new source of inexpensive, yet high-quality, data? Perspectives on psychological science, 6(1):3\u20135, 2011.|\n|[14]|George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories, 2022.|\n|[15]|Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In SafeAI@ AAAI, 2019.|\n|[16]|Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. Advances in neural information processing systems, 30, 2017.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgments and References", "md": "# Acknowledgments and References"}, {"type": "heading", "lvl": 2, "value": "Acknowledgments", "md": "## Acknowledgments"}, {"type": "text", "value": "This work is supported by Microsoft Grant for Customer Experience Innovation and the National\nScience Foundation under grant no. 2019844, 2112471, and 2229876.", "md": "This work is supported by Microsoft Grant for Customer Experience Innovation and the National\nScience Foundation under grant no. 2019844, 2112471, and 2229876."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "table", "rows": [["[1]", "Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4613\u20134623, 2018."], ["[2]", "Galen Andrew, Peter Kairouz, Sewoong Oh, Alina Oprea, H Brendan McMahan, and Vinith Suriyakumar. One-shot empirical privacy estimation for federated learning. arXiv preprint arXiv:2302.03098, 2023."], ["[3]", "Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models. In Usenix Security, 2021."], ["[4]", "Eugene Bagdasaryan and Vitaly Shmatikov. Spinning language models: Risks of propaganda- as-a-service and countermeasures. In 2022 IEEE Symposium on Security and Privacy (SP), pages 769\u2013786. IEEE, 2022."], ["[5]", "Eugene Bagdasaryan and Vitaly Shmatikov. Hyperparameter search is all you need for training-agnostic backdoor robustness. arXiv preprint arXiv:2302.04977, 2023."], ["[6]", "Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pages 2938\u20132948. PMLR, 2020."], ["[7]", "Jiawang Bai, Kuofeng Gao, Dihong Gong, Shu-Tao Xia, Zhifeng Li, and Wei Liu. Hardly perceptible trojan attack against neural networks with bit flips. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part V, pages 104\u2013121. Springer, 2022."], ["[8]", "Jiawang Bai, Baoyuan Wu, Zhifeng Li, and Shu-tao Xia. Versatile weight attack via flipping limited bits. arXiv preprint arXiv:2207.12405, 2022."], ["[9]", "Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 101\u2013105. IEEE, 2019."], ["[10]", "Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The security of machine learning. Machine Learning, 81:121\u2013148, 2010."], ["[11]", "Peva Blanchard, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pages 119\u2013129, 2017."], ["[12]", "Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales. Flexible dataset distillation: Learn labels instead of images. arXiv preprint arXiv:2006.08572, 2020."], ["[13]", "Michael Buhrmester, Tracy Kwang, and Samuel D Gosling. Amazon\u2019s mechanical turk: A new source of inexpensive, yet high-quality, data? Perspectives on psychological science, 6(1):3\u20135, 2011."], ["[14]", "George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories, 2022."], ["[15]", "Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In SafeAI@ AAAI, 2019."], ["[16]", "Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. Advances in neural information processing systems, 30, 2017."]], "md": "|[1]|Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4613\u20134623, 2018.|\n|---|---|\n|[2]|Galen Andrew, Peter Kairouz, Sewoong Oh, Alina Oprea, H Brendan McMahan, and Vinith Suriyakumar. One-shot empirical privacy estimation for federated learning. arXiv preprint arXiv:2302.03098, 2023.|\n|[3]|Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models. In Usenix Security, 2021.|\n|[4]|Eugene Bagdasaryan and Vitaly Shmatikov. Spinning language models: Risks of propaganda- as-a-service and countermeasures. In 2022 IEEE Symposium on Security and Privacy (SP), pages 769\u2013786. IEEE, 2022.|\n|[5]|Eugene Bagdasaryan and Vitaly Shmatikov. Hyperparameter search is all you need for training-agnostic backdoor robustness. arXiv preprint arXiv:2302.04977, 2023.|\n|[6]|Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pages 2938\u20132948. PMLR, 2020.|\n|[7]|Jiawang Bai, Kuofeng Gao, Dihong Gong, Shu-Tao Xia, Zhifeng Li, and Wei Liu. Hardly perceptible trojan attack against neural networks with bit flips. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part V, pages 104\u2013121. Springer, 2022.|\n|[8]|Jiawang Bai, Baoyuan Wu, Zhifeng Li, and Shu-tao Xia. Versatile weight attack via flipping limited bits. arXiv preprint arXiv:2207.12405, 2022.|\n|[9]|Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 101\u2013105. IEEE, 2019.|\n|[10]|Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The security of machine learning. Machine Learning, 81:121\u2013148, 2010.|\n|[11]|Peva Blanchard, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pages 119\u2013129, 2017.|\n|[12]|Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales. Flexible dataset distillation: Learn labels instead of images. arXiv preprint arXiv:2006.08572, 2020.|\n|[13]|Michael Buhrmester, Tracy Kwang, and Samuel D Gosling. Amazon\u2019s mechanical turk: A new source of inexpensive, yet high-quality, data? Perspectives on psychological science, 6(1):3\u20135, 2011.|\n|[14]|George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories, 2022.|\n|[15]|Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In SafeAI@ AAAI, 2019.|\n|[16]|Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. Advances in neural information processing systems, 30, 2017.|", "isPerfectTable": true, "csv": "\"[1]\",\"Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li. Byzantine stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 4613\u20134623, 2018.\"\n\"[2]\",\"Galen Andrew, Peter Kairouz, Sewoong Oh, Alina Oprea, H Brendan McMahan, and Vinith Suriyakumar. One-shot empirical privacy estimation for federated learning. arXiv preprint arXiv:2302.03098, 2023.\"\n\"[3]\",\"Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models. In Usenix Security, 2021.\"\n\"[4]\",\"Eugene Bagdasaryan and Vitaly Shmatikov. Spinning language models: Risks of propaganda- as-a-service and countermeasures. In 2022 IEEE Symposium on Security and Privacy (SP), pages 769\u2013786. IEEE, 2022.\"\n\"[5]\",\"Eugene Bagdasaryan and Vitaly Shmatikov. Hyperparameter search is all you need for training-agnostic backdoor robustness. arXiv preprint arXiv:2302.04977, 2023.\"\n\"[6]\",\"Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In International Conference on Artificial Intelligence and Statistics, pages 2938\u20132948. PMLR, 2020.\"\n\"[7]\",\"Jiawang Bai, Kuofeng Gao, Dihong Gong, Shu-Tao Xia, Zhifeng Li, and Wei Liu. Hardly perceptible trojan attack against neural networks with bit flips. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part V, pages 104\u2013121. Springer, 2022.\"\n\"[8]\",\"Jiawang Bai, Baoyuan Wu, Zhifeng Li, and Shu-tao Xia. Versatile weight attack via flipping limited bits. arXiv preprint arXiv:2207.12405, 2022.\"\n\"[9]\",\"Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 101\u2013105. IEEE, 2019.\"\n\"[10]\",\"Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The security of machine learning. Machine Learning, 81:121\u2013148, 2010.\"\n\"[11]\",\"Peva Blanchard, Rachid Guerraoui, and Julien Stainer. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pages 119\u2013129, 2017.\"\n\"[12]\",\"Ondrej Bohdal, Yongxin Yang, and Timothy Hospedales. Flexible dataset distillation: Learn labels instead of images. arXiv preprint arXiv:2006.08572, 2020.\"\n\"[13]\",\"Michael Buhrmester, Tracy Kwang, and Samuel D Gosling. Amazon\u2019s mechanical turk: A new source of inexpensive, yet high-quality, data? Perspectives on psychological science, 6(1):3\u20135, 2011.\"\n\"[14]\",\"George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A. Efros, and Jun-Yan Zhu. Dataset distillation by matching training trajectories, 2022.\"\n\"[15]\",\"Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. In SafeAI@ AAAI, 2019.\"\n\"[16]\",\"Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efficient object detection models with knowledge distillation. Advances in neural information processing systems, 30, 2017.\""}]}, {"page": 12, "text": "[17] Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Proflip: Targeted trojan attack\n     with progressive bit flips. In Proceedings of the IEEE/CVF International Conference on\n     Computer Vision, pages 7718\u20137727, 2021.\n[18] Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, and Tianwei Zhang. Clean-image back-\n     door: Attacking multi-label models with poisoned labels only. In The Eleventh International\n     Conference on Learning Representations, 2023.\n[19] Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos.              Draco:\n     Byzantine-resilient distributed training via redundant gradients. In International Conference\n     on Machine Learning, pages 903\u2013912. PMLR, 2018.\n[20] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on\n     deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n[21] Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting localized\n     universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops\n     (SPW), pages 48\u201354. IEEE, 2020.\n[22] J. Deng, K. Li, M. Do, H. Su, and L. Fei-Fei. Construction and Analysis of a Large Scale\n     Image Ontology. Vision Sciences Society, 2009.\n[23] Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack with imperceptible input and latent\n     modification. Advances in Neural Information Processing Systems, 34, 2021.\n[24] Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust\n     backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer\n     Vision, pages 11966\u201311976, 2021.\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\n     Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\n     Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\n     recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021,\n     Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n[26] Jacob Dumford and Walter Scheirer. Backdooring convolutional neural networks via targeted\n     weight perturbations. In 2020 IEEE International Joint Conference on Biometrics (IJCB),\n     pages 1\u20139. IEEE, 2020.\n[27] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for\n     deep data-driven reinforcement learning, 2021.\n[28] Guanhao Gan, Yiming Li, Dongxian Wu, and Shu-Tao Xia. Towards robust model watermark\n     via reducing parametric vulnerability. In ICCV, 2023.\n[29] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya\n     Nepal. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the\n     35th Annual Computer Security Applications Conference, pages 113\u2013125, 2019.\n[30] Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, and Shu-Tao Xia. Not all\n     samples are born equal: Towards effective clean-label backdoor attacks. Pattern Recognition,\n     139:109512, 2023.\n[31] Yunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, and Cong Wang.\n     Anti-distillation backdoor attacks: Backdoors can really survive in knowledge distillation.\n     In Proceedings of the 29th ACM International Conference on Multimedia, MM \u201921, page\n     826\u2013834, New York, NY, USA, 2021. Association for Computing Machinery.\n[32] Jonas Geiping, Liam Fowl, W Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller,\n     and Tom Goldstein. Witches\u2019 brew: Industrial scale data poisoning via gradient matching.\n     arXiv preprint arXiv:2009.02276, 2020.\n[33] Shafi Goldwasser, Michael P Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable\n     backdoors in machine learning models. arXiv preprint arXiv:2204.06974, 2022.\n                                               12", "md": "# References\n\n# List of References\n\n1. Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Proflip: Targeted trojan attack with progressive bit flips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7718\u20137727, 2021.\n2. Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, and Tianwei Zhang. Clean-image backdoor: Attacking multi-label models with poisoned labels only. In The Eleventh International Conference on Learning Representations, 2023.\n3. Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos. Draco: Byzantine-resilient distributed training via redundant gradients. In International Conference on Machine Learning, pages 903\u2013912. PMLR, 2018.\n4. Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n5. Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting localized universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pages 48\u201354. IEEE, 2020.\n6. J. Deng, K. Li, M. Do, H. Su, and L. Fei-Fei. Construction and Analysis of a Large Scale Image Ontology. Vision Sciences Society, 2009.\n7. Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack with imperceptible input and latent modification. Advances in Neural Information Processing Systems, 34, 2021.\n8. Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11966\u201311976, 2021.\n9. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n10. Jacob Dumford and Walter Scheirer. Backdooring convolutional neural networks via targeted weight perturbations. In 2020 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u20139. IEEE, 2020.\n11. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2021.\n12. Guanhao Gan, Yiming Li, Dongxian Wu, and Shu-Tao Xia. Towards robust model watermark via reducing parametric vulnerability. In ICCV, 2023.\n13. Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference, pages 113\u2013125, 2019.\n14. Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, and Shu-Tao Xia. Not all samples are born equal: Towards effective clean-label backdoor attacks. Pattern Recognition, 139:109512, 2023.\n15. Yunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, and Cong Wang. Anti-distillation backdoor attacks: Backdoors can really survive in knowledge distillation. In Proceedings of the 29th ACM International Conference on Multimedia, MM \u201921, page 826\u2013834, New York, NY, USA, 2021. Association for Computing Machinery.\n16. Jonas Geiping, Liam Fowl, W Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and Tom Goldstein. Witches\u2019 brew: Industrial scale data poisoning via gradient matching. arXiv preprint arXiv:2009.02276, 2020.\n17. Shafi Goldwasser, Michael P Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable backdoors in machine learning models. arXiv preprint arXiv:2204.06974, 2022.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "1. Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Proflip: Targeted trojan attack with progressive bit flips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7718\u20137727, 2021.\n2. Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, and Tianwei Zhang. Clean-image backdoor: Attacking multi-label models with poisoned labels only. In The Eleventh International Conference on Learning Representations, 2023.\n3. Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos. Draco: Byzantine-resilient distributed training via redundant gradients. In International Conference on Machine Learning, pages 903\u2013912. PMLR, 2018.\n4. Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n5. Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting localized universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pages 48\u201354. IEEE, 2020.\n6. J. Deng, K. Li, M. Do, H. Su, and L. Fei-Fei. Construction and Analysis of a Large Scale Image Ontology. Vision Sciences Society, 2009.\n7. Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack with imperceptible input and latent modification. Advances in Neural Information Processing Systems, 34, 2021.\n8. Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11966\u201311976, 2021.\n9. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n10. Jacob Dumford and Walter Scheirer. Backdooring convolutional neural networks via targeted weight perturbations. In 2020 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u20139. IEEE, 2020.\n11. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2021.\n12. Guanhao Gan, Yiming Li, Dongxian Wu, and Shu-Tao Xia. Towards robust model watermark via reducing parametric vulnerability. In ICCV, 2023.\n13. Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference, pages 113\u2013125, 2019.\n14. Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, and Shu-Tao Xia. Not all samples are born equal: Towards effective clean-label backdoor attacks. Pattern Recognition, 139:109512, 2023.\n15. Yunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, and Cong Wang. Anti-distillation backdoor attacks: Backdoors can really survive in knowledge distillation. In Proceedings of the 29th ACM International Conference on Multimedia, MM \u201921, page 826\u2013834, New York, NY, USA, 2021. Association for Computing Machinery.\n16. Jonas Geiping, Liam Fowl, W Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and Tom Goldstein. Witches\u2019 brew: Industrial scale data poisoning via gradient matching. arXiv preprint arXiv:2009.02276, 2020.\n17. Shafi Goldwasser, Michael P Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable backdoors in machine learning models. arXiv preprint arXiv:2204.06974, 2022.", "md": "1. Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Proflip: Targeted trojan attack with progressive bit flips. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7718\u20137727, 2021.\n2. Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, and Tianwei Zhang. Clean-image backdoor: Attacking multi-label models with poisoned labels only. In The Eleventh International Conference on Learning Representations, 2023.\n3. Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos. Draco: Byzantine-resilient distributed training via redundant gradients. In International Conference on Machine Learning, pages 903\u2013912. PMLR, 2018.\n4. Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\n5. Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting localized universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pages 48\u201354. IEEE, 2020.\n6. J. Deng, K. Li, M. Do, H. Su, and L. Fei-Fei. Construction and Analysis of a Large Scale Image Ontology. Vision Sciences Society, 2009.\n7. Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack with imperceptible input and latent modification. Advances in Neural Information Processing Systems, 34, 2021.\n8. Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust backdoor attacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11966\u201311976, 2021.\n9. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\n10. Jacob Dumford and Walter Scheirer. Backdooring convolutional neural networks via targeted weight perturbations. In 2020 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u20139. IEEE, 2020.\n11. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2021.\n12. Guanhao Gan, Yiming Li, Dongxian Wu, and Shu-Tao Xia. Towards robust model watermark via reducing parametric vulnerability. In ICCV, 2023.\n13. Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference, pages 113\u2013125, 2019.\n14. Yinghua Gao, Yiming Li, Linghui Zhu, Dongxian Wu, Yong Jiang, and Shu-Tao Xia. Not all samples are born equal: Towards effective clean-label backdoor attacks. Pattern Recognition, 139:109512, 2023.\n15. Yunjie Ge, Qian Wang, Baolin Zheng, Xinlu Zhuang, Qi Li, Chao Shen, and Cong Wang. Anti-distillation backdoor attacks: Backdoors can really survive in knowledge distillation. In Proceedings of the 29th ACM International Conference on Multimedia, MM \u201921, page 826\u2013834, New York, NY, USA, 2021. Association for Computing Machinery.\n16. Jonas Geiping, Liam Fowl, W Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and Tom Goldstein. Witches\u2019 brew: Industrial scale data poisoning via gradient matching. arXiv preprint arXiv:2009.02276, 2020.\n17. Shafi Goldwasser, Michael P Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable backdoors in machine learning models. arXiv preprint arXiv:2204.06974, 2022."}]}, {"page": 13, "text": "[34] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in\n     the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\n[35] Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, and Cong Liu. Scale-up: An\n     efficient black-box input-level backdoor detection via analyzing scaled prediction consistency.\n     In ICLR, 2023.\n[36] Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. Spectre: Defending\n     against backdoor attacks using robust statistics, 2021.\n[37] Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. SPECTRE: Defending\n     against backdoor attacks using robust statistics. In International Conference on Machine\n     Learning, pages 4129\u20134139. PMLR, 2021.\n[38] Jonathan Hayase and Sewoong Oh. Few-shot backdoor attacks via neural tangent kernels. In\n     International Conference on Learning Representations (ICLR), 2023.\n[39] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.\n     arXiv preprint arXiv:1503.02531, 2015.\n[40] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in\n     neural information processing systems, 29, 2016.\n[41] Sanghyun Hong, Nicholas Carlini, and Alexey Kurakin. Handcrafted backdoors in deep neural\n     networks. Advances in Neural Information Processing Systems, 35:8068\u20138080, 2022.\n[42] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via\n     decoupling the training process. In ICLR, 2022.\n[43] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and\n     Aleksander Madry. Adversarial examples are not bugs, they are features. Advances in neural\n     information processing systems, 32, 2019.\n[44] Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine\n     learning: How private is private SGD? Advances in Neural Information Processing Systems,\n     33:22205\u201322216, 2020.\n[45] David Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing\n     systems. Advances in neural information processing systems, 24, 2011.\n[46] Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In\n     International Conference on Learning Representations (ICLR), 2015.\n[47] Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data\n     sanitization defenses. Machine Learning, 111(1):1\u201347, 2022.\n[48] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus\n     patterns: Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on\n     Computer Vision and Pattern Recognition, pages 301\u2013310, 2020.\n[49] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for de-\n     tecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information\n     Processing Systems, pages 7167\u20137177, 2018.\n[50] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invis-\n     ible backdoor attacks on deep neural networks via steganography and regularization. IEEE\n     Transactions on Dependable and Secure Computing, 18(5):2088\u20132105, 2020.\n[51] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention\n     distillation: Erasing backdoor triggers from deep neural networks, 2021.\n[52] Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, and Bo Li. Untargeted backdoor\n     watermark: Towards harmless and stealthy dataset copyright protection. In Advances in Neural\n     Information Processing Systems, 2022.\n                                                13", "md": "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\nJunfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, and Cong Liu. Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction consistency. In ICLR, 2023.\nJonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. Spectre: Defending against backdoor attacks using robust statistics, 2021.\nJonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. SPECTRE: Defending against backdoor attacks using robust statistics. In International Conference on Machine Learning, pages 4129\u20134139. PMLR, 2021.\nJonathan Hayase and Sewoong Oh. Few-shot backdoor attacks via neural tangent kernels. In International Conference on Learning Representations (ICLR), 2023.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.\nSanghyun Hong, Nicholas Carlini, and Alexey Kurakin. Handcrafted backdoors in deep neural networks. Advances in Neural Information Processing Systems, 35:8068\u20138080, 2022.\nKunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In ICLR, 2022.\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. Advances in neural information processing systems, 32, 2019.\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? Advances in Neural Information Processing Systems, 33:22205\u201322216, 2020.\nDavid Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. Advances in neural information processing systems, 24, 2011.\nDiederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.\nPang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. Machine Learning, 111(1):1\u201347, 2022.\nSoheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 301\u2013310, 2020.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, pages 7167\u20137177, 2018.\nShaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Transactions on Dependable and Secure Computing, 18(5):2088\u20132105, 2020.\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks, 2021.\nYiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, and Bo Li. Untargeted backdoor watermark: Towards harmless and stealthy dataset copyright protection. In Advances in Neural Information Processing Systems, 2022.", "images": [], "items": [{"type": "text", "value": "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\nJunfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, and Cong Liu. Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction consistency. In ICLR, 2023.\nJonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. Spectre: Defending against backdoor attacks using robust statistics, 2021.\nJonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. SPECTRE: Defending against backdoor attacks using robust statistics. In International Conference on Machine Learning, pages 4129\u20134139. PMLR, 2021.\nJonathan Hayase and Sewoong Oh. Few-shot backdoor attacks via neural tangent kernels. In International Conference on Learning Representations (ICLR), 2023.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.\nSanghyun Hong, Nicholas Carlini, and Alexey Kurakin. Handcrafted backdoors in deep neural networks. Advances in Neural Information Processing Systems, 35:8068\u20138080, 2022.\nKunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In ICLR, 2022.\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. Advances in neural information processing systems, 32, 2019.\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? Advances in Neural Information Processing Systems, 33:22205\u201322216, 2020.\nDavid Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. Advances in neural information processing systems, 24, 2011.\nDiederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.\nPang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. Machine Learning, 111(1):1\u201347, 2022.\nSoheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 301\u2013310, 2020.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, pages 7167\u20137177, 2018.\nShaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Transactions on Dependable and Secure Computing, 18(5):2088\u20132105, 2020.\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks, 2021.\nYiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, and Bo Li. Untargeted backdoor watermark: Towards harmless and stealthy dataset copyright protection. In Advances in Neural Information Processing Systems, 2022.", "md": "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\nJunfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, and Cong Liu. Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction consistency. In ICLR, 2023.\nJonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. Spectre: Defending against backdoor attacks using robust statistics, 2021.\nJonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. SPECTRE: Defending against backdoor attacks using robust statistics. In International Conference on Machine Learning, pages 4129\u20134139. PMLR, 2021.\nJonathan Hayase and Sewoong Oh. Few-shot backdoor attacks via neural tangent kernels. In International Conference on Learning Representations (ICLR), 2023.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems, 29, 2016.\nSanghyun Hong, Nicholas Carlini, and Alexey Kurakin. Handcrafted backdoors in deep neural networks. Advances in Neural Information Processing Systems, 35:8068\u20138080, 2022.\nKunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In ICLR, 2022.\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. Advances in neural information processing systems, 32, 2019.\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? Advances in Neural Information Processing Systems, 33:22205\u201322216, 2020.\nDavid Karger, Sewoong Oh, and Devavrat Shah. Iterative learning for reliable crowdsourcing systems. Advances in neural information processing systems, 24, 2011.\nDiederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.\nPang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. Machine Learning, 111(1):1\u201347, 2022.\nSoheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 301\u2013310, 2020.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, pages 7167\u20137177, 2018.\nShaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible backdoor attacks on deep neural networks via steganography and regularization. IEEE Transactions on Dependable and Secure Computing, 18(5):2088\u20132105, 2020.\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks, 2021.\nYiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, and Bo Li. Untargeted backdoor watermark: Towards harmless and stealthy dataset copyright protection. In Advances in Neural Information Processing Systems, 2022."}]}, {"page": 14, "text": "[53] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE\n     Transactions on Neural Networks and Learning Systems, 2022.\n[54] Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, and Shu-Tao Xia. Black-box ownership ver-\n     ification for dataset protection via backdoor watermarking. IEEE Transactions on Information\n     Forensics and Security, 18:2318 \u2013 2332, 2023.\n[55] Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Yunxin Liu. Deeppayload:\n     Black-box backdoor attack on deep learning models through neural payload injection. In 2021\n     IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 263\u2013274.\n     IEEE, 2021.\n[56] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image\n     detection in neural networks. In 6th International Conference on Learning Representations,\n     ICLR 2018, 2018.\n[57] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. Backdoor\n     embedding in convolutional neural network models via invisible perturbation, 2018.\n[58] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against\n     backdooring attacks on deep neural networks. In International Symposium on Research in\n     Attacks, Intrusions, and Defenses, pages 273\u2013294. Springer, 2018.\n[59] Xuan Liu, Xiaoguang Wang, and Stan Matwin. Improving the interpretability of deep neural\n     networks with knowledge distillation. In 2018 IEEE International Conference on Data Mining\n     Workshops (ICDMW), pages 905\u2013912. IEEE, 2018.\n[60] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured\n     knowledge distillation for semantic segmentation. In Proceedings of the IEEE/CVF Conference\n     on Computer Vision and Pattern Recognition, pages 2604\u20132613, 2019.\n[61] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor\n     attack on deep neural networks. In European Conference on Computer Vision, pages 182\u2013199.\n     Springer, 2020.\n[62] Milad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tram\u00e8r, Matthew Jagielski,\n     Nicholas Carlini, and Andreas Terzis. Tight auditing of differentially private machine learning.\n     arXiv preprint arXiv:2302.07956, 2023.\n[63] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel\n     ridge-regression. In International Conference on Learning Representations, 2020.\n[64] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with\n     infinitely wide convolutional networks. Advances in Neural Information Processing Systems,\n     34, 2021.\n[65] Tuan Anh Nguyen and Anh Tuan Tran. Wanet-imperceptible warping-based backdoor attack.\n     In International Conference on Learning Representations, 2020.\n[66] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters,\n     et al. An algorithmic perspective on imitation learning. Foundations and Trends\u00ae in Robotics,\n     7(1-2):1\u2013179, 2018.\n[67] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distilla-\n     tion as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE\n     symposium on security and privacy (SP), pages 582\u2013597. IEEE, 2016.\n[68] Andrea Paudice, Luis Mu\u00f1oz-Gonz\u00e1lez, and Emil C Lupu. Label sanitization against label\n     flipping poisoning attacks. In ECML PKDD 2018 Workshops: Nemesis 2018, UrbReas 2018,\n     SoGood 2018, IWAISe 2018, and Green Data Mining 2018, Dublin, Ireland, September 10-14,\n     2018, Proceedings 18, pages 5\u201315. Springer, 2019.\n[69] Krishna Pillutla, Galen Andrew, Peter Kairouz, H Brendan McMahan, Alina Oprea, and\n     Sewoong Oh. Unleashing the power of randomization in auditing differentially private ml.\n     arXiv preprint arXiv:2305.18447, 2023.\n                                                14", "md": "- [53] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n- [54] Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, and Shu-Tao Xia. Black-box ownership verification for dataset protection via backdoor watermarking. IEEE Transactions on Information Forensics and Security, 18:2318 \u2013 2332, 2023.\n- [55] Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Yunxin Liu. Deeppayload: Black-box backdoor attack on deep learning models through neural payload injection. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 263\u2013274. IEEE, 2021.\n- [56] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In 6th International Conference on Learning Representations, ICLR 2018, 2018.\n- [57] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. Backdoor embedding in convolutional neural network models via invisible perturbation, 2018.\n- [58] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273\u2013294. Springer, 2018.\n- [59] Xuan Liu, Xiaoguang Wang, and Stan Matwin. Improving the interpretability of deep neural networks with knowledge distillation. In 2018 IEEE International Conference on Data Mining Workshops (ICDMW), pages 905\u2013912. IEEE, 2018.\n- [60] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured knowledge distillation for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2604\u20132613, 2019.\n- [61] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision, pages 182\u2013199. Springer, 2020.\n- [62] Milad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tram\u00e8r, Matthew Jagielski, Nicholas Carlini, and Andreas Terzis. Tight auditing of differentially private machine learning. arXiv preprint arXiv:2302.07956, 2023.\n- [63] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. In International Conference on Learning Representations, 2020.\n- [64] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. Advances in Neural Information Processing Systems, 34, 2021.\n- [65] Tuan Anh Nguyen and Anh Tuan Tran. Wanet-imperceptible warping-based backdoor attack. In International Conference on Learning Representations, 2020.\n- [66] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends\u00ae in Robotics, 7(1-2):1\u2013179, 2018.\n- [67] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE symposium on security and privacy (SP), pages 582\u2013597. IEEE, 2016.\n- [68] Andrea Paudice, Luis Mu\u00f1oz-Gonz\u00e1lez, and Emil C Lupu. Label sanitization against label flipping poisoning attacks. In ECML PKDD 2018 Workshops: Nemesis 2018, UrbReas 2018, SoGood 2018, IWAISe 2018, and Green Data Mining 2018, Dublin, Ireland, September 10-14, 2018, Proceedings 18, pages 5\u201315. Springer, 2019.\n- [69] Krishna Pillutla, Galen Andrew, Peter Kairouz, H Brendan McMahan, Alina Oprea, and Sewoong Oh. Unleashing the power of randomization in auditing differentially private ml. arXiv preprint arXiv:2305.18447, 2023.", "images": [], "items": [{"type": "text", "value": "- [53] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n- [54] Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, and Shu-Tao Xia. Black-box ownership verification for dataset protection via backdoor watermarking. IEEE Transactions on Information Forensics and Security, 18:2318 \u2013 2332, 2023.\n- [55] Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Yunxin Liu. Deeppayload: Black-box backdoor attack on deep learning models through neural payload injection. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 263\u2013274. IEEE, 2021.\n- [56] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In 6th International Conference on Learning Representations, ICLR 2018, 2018.\n- [57] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. Backdoor embedding in convolutional neural network models via invisible perturbation, 2018.\n- [58] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273\u2013294. Springer, 2018.\n- [59] Xuan Liu, Xiaoguang Wang, and Stan Matwin. Improving the interpretability of deep neural networks with knowledge distillation. In 2018 IEEE International Conference on Data Mining Workshops (ICDMW), pages 905\u2013912. IEEE, 2018.\n- [60] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured knowledge distillation for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2604\u20132613, 2019.\n- [61] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision, pages 182\u2013199. Springer, 2020.\n- [62] Milad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tram\u00e8r, Matthew Jagielski, Nicholas Carlini, and Andreas Terzis. Tight auditing of differentially private machine learning. arXiv preprint arXiv:2302.07956, 2023.\n- [63] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. In International Conference on Learning Representations, 2020.\n- [64] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. Advances in Neural Information Processing Systems, 34, 2021.\n- [65] Tuan Anh Nguyen and Anh Tuan Tran. Wanet-imperceptible warping-based backdoor attack. In International Conference on Learning Representations, 2020.\n- [66] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends\u00ae in Robotics, 7(1-2):1\u2013179, 2018.\n- [67] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE symposium on security and privacy (SP), pages 582\u2013597. IEEE, 2016.\n- [68] Andrea Paudice, Luis Mu\u00f1oz-Gonz\u00e1lez, and Emil C Lupu. Label sanitization against label flipping poisoning attacks. In ECML PKDD 2018 Workshops: Nemesis 2018, UrbReas 2018, SoGood 2018, IWAISe 2018, and Green Data Mining 2018, Dublin, Ireland, September 10-14, 2018, Proceedings 18, pages 5\u201315. Springer, 2019.\n- [69] Krishna Pillutla, Galen Andrew, Peter Kairouz, H Brendan McMahan, Alina Oprea, and Sewoong Oh. Unleashing the power of randomization in auditing differentially private ml. arXiv preprint arXiv:2305.18447, 2023.", "md": "- [53] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\n- [54] Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, and Shu-Tao Xia. Black-box ownership verification for dataset protection via backdoor watermarking. IEEE Transactions on Information Forensics and Security, 18:2318 \u2013 2332, 2023.\n- [55] Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Yunxin Liu. Deeppayload: Black-box backdoor attack on deep learning models through neural payload injection. In 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE), pages 263\u2013274. IEEE, 2021.\n- [56] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In 6th International Conference on Learning Representations, ICLR 2018, 2018.\n- [57] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. Backdoor embedding in convolutional neural network models via invisible perturbation, 2018.\n- [58] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273\u2013294. Springer, 2018.\n- [59] Xuan Liu, Xiaoguang Wang, and Stan Matwin. Improving the interpretability of deep neural networks with knowledge distillation. In 2018 IEEE International Conference on Data Mining Workshops (ICDMW), pages 905\u2013912. IEEE, 2018.\n- [60] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured knowledge distillation for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2604\u20132613, 2019.\n- [61] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In European Conference on Computer Vision, pages 182\u2013199. Springer, 2020.\n- [62] Milad Nasr, Jamie Hayes, Thomas Steinke, Borja Balle, Florian Tram\u00e8r, Matthew Jagielski, Nicholas Carlini, and Andreas Terzis. Tight auditing of differentially private machine learning. arXiv preprint arXiv:2302.07956, 2023.\n- [63] Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. In International Conference on Learning Representations, 2020.\n- [64] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely wide convolutional networks. Advances in Neural Information Processing Systems, 34, 2021.\n- [65] Tuan Anh Nguyen and Anh Tuan Tran. Wanet-imperceptible warping-based backdoor attack. In International Conference on Learning Representations, 2020.\n- [66] Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends\u00ae in Robotics, 7(1-2):1\u2013179, 2018.\n- [67] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE symposium on security and privacy (SP), pages 582\u2013597. IEEE, 2016.\n- [68] Andrea Paudice, Luis Mu\u00f1oz-Gonz\u00e1lez, and Emil C Lupu. Label sanitization against label flipping poisoning attacks. In ECML PKDD 2018 Workshops: Nemesis 2018, UrbReas 2018, SoGood 2018, IWAISe 2018, and Green Data Mining 2018, Dublin, Ireland, September 10-14, 2018, Proceedings 18, pages 5\u201315. Springer, 2019.\n- [69] Krishna Pillutla, Galen Andrew, Peter Kairouz, H Brendan McMahan, Alina Oprea, and Sewoong Oh. Unleashing the power of randomization in auditing differentially private ml. arXiv preprint arXiv:2305.18447, 2023."}]}, {"page": 15, "text": "[70] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated\n      learning. arXiv preprint arXiv:1912.13445, 2019.\n[71] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting\n      the assumption of latent separability for backdoor defenses. In The Eleventh International\n      Conference on Learning Representations, 2023.\n[72] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Tbt: Targeted neural network attack with\n      bit trojan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n      Recognition, pages 13198\u201313207, 2020.\n[73] Joseph Rance, Yiren Zhao, Ilia Shumailov, and Robert Mullins. Augmentation backdoors.\n      arXiv preprint arXiv:2209.15139, 2022.\n[74] Ambrish Rawat, Killian Levacher, and Mathieu Sinn. The devil is in the gan: Defending deep\n      generative models against backdoor attacks. arXiv preprint arXiv:2108.01644, 2021.\n[75] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor\n      attacks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages\n      11957\u201311965, 2020.\n[76] Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, and Yang Zhang. Baaan:\n      Backdoor attacks against autoencoder and gan-based machine learning models. arXiv preprint\n      arXiv:2010.03007, 2020.\n[77] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein.\n      Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks.\n      In International Conference on Machine Learning, pages 9389\u20139398. PMLR, 2021.\n[78] Victor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. Get another label? improving\n      data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM\n      SIGKDD international conference on Knowledge discovery and data mining, pages 614\u2013622,\n      2008.\n[79] Reza Shokri et al. Bypassing backdoor detection algorithms in deep learning. In 2020 IEEE\n      European Symposium on Security and Privacy (EuroS&P), pages 175\u2013183. IEEE, 2020.\n[80] Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A\n      Erdogdu, and Ross J Anderson. Manipulating sgd with data ordering attacks. Advances in\n      Neural Information Processing Systems, 34:18021\u201318032, 2021.\n[81] Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. Inferring\n      ground truth from subjective labelling of venus images. Advances in neural information\n      processing systems, 7, 1994.\n[82] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning\n      attacks. In Advances in neural information processing systems, pages 3517\u20133529, 2017.\n[83] Thomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training\n      run. arXiv preprint arXiv:2305.08846, 2023.\n[84] Ilia Sucholutsky and Matthias Schonlau. Soft-label dataset distillation and text dataset distilla-\n      tion. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE,\n      2021.\n[85] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model\n      compression. arXiv preprint arXiv:1908.09355, 2019.\n[86] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you\n      really backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.\n[87] Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. An embarrassingly simple\n      approach for trojan attack in deep neural networks. In Proceedings of the 26th ACM SIGKDD\n      International Conference on Knowledge Discovery & Data Mining, pages 218\u2013228, 2020.\n                                                 15", "md": "# List of References\n\n# List of References\n\n[70] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. arXiv preprint arXiv:1912.13445, 2019.\n\n[71] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption of latent separability for backdoor defenses. In The Eleventh International Conference on Learning Representations, 2023.\n\n[72] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Tbt: Targeted neural network attack with bit trojan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13198\u201313207, 2020.\n\n[73] Joseph Rance, Yiren Zhao, Ilia Shumailov, and Robert Mullins. Augmentation backdoors. arXiv preprint arXiv:2209.15139, 2022.\n\n[74] Ambrish Rawat, Killian Levacher, and Mathieu Sinn. The devil is in the gan: Defending deep generative models against backdoor attacks. arXiv preprint arXiv:2108.01644, 2021.\n\n[75] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 11957\u201311965, 2020.\n\n[76] Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, and Yang Zhang. Baaan: Backdoor attacks against autoencoder and gan-based machine learning models. arXiv preprint arXiv:2010.03007, 2020.\n\n[77] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In International Conference on Machine Learning, pages 9389\u20139398. PMLR, 2021.\n\n[78] Victor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 614\u2013622, 2008.\n\n[79] Reza Shokri et al. Bypassing backdoor detection algorithms in deep learning. In 2020 IEEE European Symposium on Security and Privacy (EuroS&P), pages 175\u2013183. IEEE, 2020.\n\n[80] Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A Erdogdu, and Ross J Anderson. Manipulating sgd with data ordering attacks. Advances in Neural Information Processing Systems, 34:18021\u201318032, 2021.\n\n[81] Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. Inferring ground truth from subjective labelling of venus images. Advances in neural information processing systems, 7, 1994.\n\n[82] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. In Advances in neural information processing systems, pages 3517\u20133529, 2017.\n\n[83] Thomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run. arXiv preprint arXiv:2305.08846, 2023.\n\n[84] Ilia Sucholutsky and Matthias Schonlau. Soft-label dataset distillation and text dataset distillation. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2021.\n\n[85] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.\n\n[86] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.\n\n[87] Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. An embarrassingly simple approach for trojan attack in deep neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 218\u2013228, 2020.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "[70] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. arXiv preprint arXiv:1912.13445, 2019.\n\n[71] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption of latent separability for backdoor defenses. In The Eleventh International Conference on Learning Representations, 2023.\n\n[72] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Tbt: Targeted neural network attack with bit trojan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13198\u201313207, 2020.\n\n[73] Joseph Rance, Yiren Zhao, Ilia Shumailov, and Robert Mullins. Augmentation backdoors. arXiv preprint arXiv:2209.15139, 2022.\n\n[74] Ambrish Rawat, Killian Levacher, and Mathieu Sinn. The devil is in the gan: Defending deep generative models against backdoor attacks. arXiv preprint arXiv:2108.01644, 2021.\n\n[75] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 11957\u201311965, 2020.\n\n[76] Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, and Yang Zhang. Baaan: Backdoor attacks against autoencoder and gan-based machine learning models. arXiv preprint arXiv:2010.03007, 2020.\n\n[77] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In International Conference on Machine Learning, pages 9389\u20139398. PMLR, 2021.\n\n[78] Victor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 614\u2013622, 2008.\n\n[79] Reza Shokri et al. Bypassing backdoor detection algorithms in deep learning. In 2020 IEEE European Symposium on Security and Privacy (EuroS&P), pages 175\u2013183. IEEE, 2020.\n\n[80] Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A Erdogdu, and Ross J Anderson. Manipulating sgd with data ordering attacks. Advances in Neural Information Processing Systems, 34:18021\u201318032, 2021.\n\n[81] Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. Inferring ground truth from subjective labelling of venus images. Advances in neural information processing systems, 7, 1994.\n\n[82] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. In Advances in neural information processing systems, pages 3517\u20133529, 2017.\n\n[83] Thomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run. arXiv preprint arXiv:2305.08846, 2023.\n\n[84] Ilia Sucholutsky and Matthias Schonlau. Soft-label dataset distillation and text dataset distillation. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2021.\n\n[85] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.\n\n[86] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.\n\n[87] Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. An embarrassingly simple approach for trojan attack in deep neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 218\u2013228, 2020.", "md": "[70] Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui. Robust aggregation for federated learning. arXiv preprint arXiv:1912.13445, 2019.\n\n[71] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption of latent separability for backdoor defenses. In The Eleventh International Conference on Learning Representations, 2023.\n\n[72] Adnan Siraj Rakin, Zhezhi He, and Deliang Fan. Tbt: Targeted neural network attack with bit trojan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13198\u201313207, 2020.\n\n[73] Joseph Rance, Yiren Zhao, Ilia Shumailov, and Robert Mullins. Augmentation backdoors. arXiv preprint arXiv:2209.15139, 2022.\n\n[74] Ambrish Rawat, Killian Levacher, and Mathieu Sinn. The devil is in the gan: Defending deep generative models against backdoor attacks. arXiv preprint arXiv:2108.01644, 2021.\n\n[75] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 11957\u201311965, 2020.\n\n[76] Ahmed Salem, Yannick Sautter, Michael Backes, Mathias Humbert, and Yang Zhang. Baaan: Backdoor attacks against autoencoder and gan-based machine learning models. arXiv preprint arXiv:2010.03007, 2020.\n\n[77] Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In International Conference on Machine Learning, pages 9389\u20139398. PMLR, 2021.\n\n[78] Victor S Sheng, Foster Provost, and Panagiotis G Ipeirotis. Get another label? improving data quality and data mining using multiple, noisy labelers. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 614\u2013622, 2008.\n\n[79] Reza Shokri et al. Bypassing backdoor detection algorithms in deep learning. In 2020 IEEE European Symposium on Security and Privacy (EuroS&P), pages 175\u2013183. IEEE, 2020.\n\n[80] Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot, Murat A Erdogdu, and Ross J Anderson. Manipulating sgd with data ordering attacks. Advances in Neural Information Processing Systems, 34:18021\u201318032, 2021.\n\n[81] Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. Inferring ground truth from subjective labelling of venus images. Advances in neural information processing systems, 7, 1994.\n\n[82] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. In Advances in neural information processing systems, pages 3517\u20133529, 2017.\n\n[83] Thomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run. arXiv preprint arXiv:2305.08846, 2023.\n\n[84] Ilia Sucholutsky and Matthias Schonlau. Soft-label dataset distillation and text dataset distillation. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2021.\n\n[85] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model compression. arXiv preprint arXiv:1908.09355, 2019.\n\n[86] Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? arXiv preprint arXiv:1911.07963, 2019.\n\n[87] Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu. An embarrassingly simple approach for trojan attack in deep neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 218\u2013228, 2020."}]}, {"page": 16, "text": "  [88] MC Tol, S Islam, B Sunar, and Z Zhang. Toward realistic backdoor injection attacks on dnns\n       using rowhammer. arXiv preprint arXiv:2110.07683, 2022.\n  [89] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks.\n       Advances in neural information processing systems, 31, 2018.\n  [90] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks.\n       arXiv preprint arXiv:1912.02771, 2019.\n  [91] Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong, et al. On certifying robustness against\n       backdoor attacks via randomized smoothing. arXiv preprint arXiv:2002.11750, 2020.\n  [92] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and\n       Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks.\n       In 2019 IEEE Symposium on Security and Privacy (SP), pages 707\u2013723. IEEE, 2019.\n  [93] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal,\n       Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you\n       really can backdoor federated learning. Advances in Neural Information Processing Systems,\n       33, 2020.\n  [94] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation.\n       arXiv preprint arXiv:1811.10959, 2018.\n  [95] Maurice Weber, Xiaojun Xu, Bojan Karla\u0161, Ce Zhang, and Bo Li. Rab: Provable robustness\n       against backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.\n  [96] Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio\n       Andere, Haitao Zheng, and Ben Y Zhao.            Natural backdoor datasets.      arXiv preprint\n       arXiv:2206.10673, 2022.\n  [97] Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, and Mingsong Chen. Eliminating backdoor\n       triggers for deep neural networks using attention relation graph distillation, 2022.\n  [98] Pengfei Xia, Hongjing Niu, Ziqiang Li, and Bin Li. Enhancing backdoor attacks with multi-\n       level mmd regularization. IEEE Transactions on Dependable and Secure Computing, 2022.\n  [99] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep\n       neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and\n       Communications Security, pages 2041\u20132055, 2019.\n[100] Kota Yoshida and Takeshi Fujino. Countermeasure against backdoor attack on neural networks\n       utilizing knowledge distillation. Journal of Signal Processing, 2020.\n[101] Santiago Zanella-B\u00e9guelin, Lukas Wutschitz, Shruti Tople, Ahmed Salem, Victor R\u00fchle,\n       Andrew Paverd, Mohammad Naseri, Boris K\u00f6pf, and Daniel Jones. Bayesian estimation of\n       differential privacy. In International Conference on Machine Learning, pages 40624\u201340636.\n       PMLR, 2023.\n[102] Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and\n       compact neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n       44(8):4388\u20134403, 2021.\n[103] Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma.\n       Be your own teacher: Improve the performance of convolutional neural networks via self\n       distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\n       pages 3713\u20133722, 2019.\n[104] Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, and Xu Sun. How to inject back-\n       doors with better consistency: Logit anchoring on clean data. arXiv preprint arXiv:2109.01300,\n       2021.\n[105] Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang.\n       Clean-label backdoor attacks on video recognition models. In Proceedings of the IEEE/CVF\n       Conference on Computer Vision and Pattern Recognition, pages 14443\u201314452, 2020.\n                                                  16", "md": "1. MC Tol, S Islam, B Sunar, and Z Zhang. Toward realistic backdoor injection attacks on DNNs using Rowhammer. arXiv preprint arXiv:2110.07683, 2022.\n2. Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in Neural Information Processing Systems, 31, 2018.\n3. Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. arXiv preprint arXiv:1912.02771, 2019.\n4. Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong, et al. On certifying robustness against backdoor attacks via randomized smoothing. arXiv preprint arXiv:2002.11750, 2020.\n5. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707\u2013723. IEEE, 2019.\n6. Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. Advances in Neural Information Processing Systems, 33, 2020.\n7. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018.\n8. Maurice Weber, Xiaojun Xu, Bojan Karla\u0161, Ce Zhang, and Bo Li. RAB: Provable robustness against backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.\n9. Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio Andere, Haitao Zheng, and Ben Y Zhao. Natural backdoor datasets. arXiv preprint arXiv:2206.10673, 2022.\n10. Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, and Mingsong Chen. Eliminating backdoor triggers for deep neural networks using attention relation graph distillation, 2022.\n11. Pengfei Xia, Hongjing Niu, Ziqiang Li, and Bin Li. Enhancing backdoor attacks with multi-level MMD regularization. IEEE Transactions on Dependable and Secure Computing, 2022.\n12. Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 2041\u20132055, 2019.\n13. Kota Yoshida and Takeshi Fujino. Countermeasure against backdoor attack on neural networks utilizing knowledge distillation. Journal of Signal Processing, 2020.\n14. Santiago Zanella-B\u00e9guelin, Lukas Wutschitz, Shruti Tople, Ahmed Salem, Victor R\u00fchle, Andrew Paverd, Mohammad Naseri, Boris K\u00f6pf, and Daniel Jones. Bayesian estimation of differential privacy. In International Conference on Machine Learning, pages 40624\u201340636. PMLR, 2023.\n15. Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and compact neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4388\u20134403, 2021.\n16. Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3713\u20133722, 2019.\n17. Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, and Xu Sun. How to inject backdoors with better consistency: Logit anchoring on clean data. arXiv preprint arXiv:2109.01300, 2021.\n18. Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-label backdoor attacks on video recognition models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14443\u201314452, 2020.", "images": [], "items": [{"type": "text", "value": "1. MC Tol, S Islam, B Sunar, and Z Zhang. Toward realistic backdoor injection attacks on DNNs using Rowhammer. arXiv preprint arXiv:2110.07683, 2022.\n2. Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in Neural Information Processing Systems, 31, 2018.\n3. Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. arXiv preprint arXiv:1912.02771, 2019.\n4. Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong, et al. On certifying robustness against backdoor attacks via randomized smoothing. arXiv preprint arXiv:2002.11750, 2020.\n5. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707\u2013723. IEEE, 2019.\n6. Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. Advances in Neural Information Processing Systems, 33, 2020.\n7. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018.\n8. Maurice Weber, Xiaojun Xu, Bojan Karla\u0161, Ce Zhang, and Bo Li. RAB: Provable robustness against backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.\n9. Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio Andere, Haitao Zheng, and Ben Y Zhao. Natural backdoor datasets. arXiv preprint arXiv:2206.10673, 2022.\n10. Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, and Mingsong Chen. Eliminating backdoor triggers for deep neural networks using attention relation graph distillation, 2022.\n11. Pengfei Xia, Hongjing Niu, Ziqiang Li, and Bin Li. Enhancing backdoor attacks with multi-level MMD regularization. IEEE Transactions on Dependable and Secure Computing, 2022.\n12. Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 2041\u20132055, 2019.\n13. Kota Yoshida and Takeshi Fujino. Countermeasure against backdoor attack on neural networks utilizing knowledge distillation. Journal of Signal Processing, 2020.\n14. Santiago Zanella-B\u00e9guelin, Lukas Wutschitz, Shruti Tople, Ahmed Salem, Victor R\u00fchle, Andrew Paverd, Mohammad Naseri, Boris K\u00f6pf, and Daniel Jones. Bayesian estimation of differential privacy. In International Conference on Machine Learning, pages 40624\u201340636. PMLR, 2023.\n15. Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and compact neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4388\u20134403, 2021.\n16. Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3713\u20133722, 2019.\n17. Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, and Xu Sun. How to inject backdoors with better consistency: Logit anchoring on clean data. arXiv preprint arXiv:2109.01300, 2021.\n18. Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-label backdoor attacks on video recognition models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14443\u201314452, 2020.", "md": "1. MC Tol, S Islam, B Sunar, and Z Zhang. Toward realistic backdoor injection attacks on DNNs using Rowhammer. arXiv preprint arXiv:2110.07683, 2022.\n2. Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in Neural Information Processing Systems, 31, 2018.\n3. Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Label-consistent backdoor attacks. arXiv preprint arXiv:1912.02771, 2019.\n4. Binghui Wang, Xiaoyu Cao, Neil Zhenqiang Gong, et al. On certifying robustness against backdoor attacks via randomized smoothing. arXiv preprint arXiv:2002.11750, 2020.\n5. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707\u2013723. IEEE, 2019.\n6. Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the tails: Yes, you really can backdoor federated learning. Advances in Neural Information Processing Systems, 33, 2020.\n7. Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation. arXiv preprint arXiv:1811.10959, 2018.\n8. Maurice Weber, Xiaojun Xu, Bojan Karla\u0161, Ce Zhang, and Bo Li. RAB: Provable robustness against backdoor attacks. arXiv preprint arXiv:2003.08904, 2020.\n9. Emily Wenger, Roma Bhattacharjee, Arjun Nitin Bhagoji, Josephine Passananti, Emilio Andere, Haitao Zheng, and Ben Y Zhao. Natural backdoor datasets. arXiv preprint arXiv:2206.10673, 2022.\n10. Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, and Mingsong Chen. Eliminating backdoor triggers for deep neural networks using attention relation graph distillation, 2022.\n11. Pengfei Xia, Hongjing Niu, Ziqiang Li, and Bin Li. Enhancing backdoor attacks with multi-level MMD regularization. IEEE Transactions on Dependable and Secure Computing, 2022.\n12. Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, pages 2041\u20132055, 2019.\n13. Kota Yoshida and Takeshi Fujino. Countermeasure against backdoor attack on neural networks utilizing knowledge distillation. Journal of Signal Processing, 2020.\n14. Santiago Zanella-B\u00e9guelin, Lukas Wutschitz, Shruti Tople, Ahmed Salem, Victor R\u00fchle, Andrew Paverd, Mohammad Naseri, Boris K\u00f6pf, and Daniel Jones. Bayesian estimation of differential privacy. In International Conference on Machine Learning, pages 40624\u201340636. PMLR, 2023.\n15. Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and compact neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4388\u20134403, 2021.\n16. Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3713\u20133722, 2019.\n17. Zhiyuan Zhang, Lingjuan Lyu, Weiqiang Wang, Lichao Sun, and Xu Sun. How to inject backdoors with better consistency: Logit anchoring on clean data. arXiv preprint arXiv:2109.01300, 2021.\n18. Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-label backdoor attacks on video recognition models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14443\u201314452, 2020."}]}, {"page": 17, "text": "A     Extended related work\nCorrupting only the labels of a fraction of training data is common in triggerless data poisoning\nattacks. It is straightforward to change the labels to make the prediction change for targeted images\n(without triggers). However, most existing backdoor attacks require poisoning both the images and\nthe labels.\nBackdoor attacks. Backdoor attacks were introduced in [34]. The design of triggers in backdoor\nattacks has received substantial study. Many works choose the trigger to appear benign to humans\n[34, 9, 61, 65], directly optimize the trigger to this end [50, 24], or choose natural objects as the\ntrigger [96, 18, 30]. Poison data has been constructed to include no mislabeled examples [90, 105],\noptimized to conceal the trigger [75], and to evade statistical inspection of latent representations\n[79, 23, 98, 20]. Backdoor attacks have been demonstrated in a wide variety of settings, including\nfederated learning [93, 6, 86], transfer learning [99, 75], generative models [76, 74], and changing\nthe tone of the language model outputs [4].\nBackdoors can be injected in many creative ways, including poisoning of the loss [3], data ordering\n[80], or data augmentation [73]. With a more powerful adversary who controls not only the training\ndata but the model itself, backdoors can be planted into a network by directly modifying the weights\n[26, 41, 104], by flipping a few bits of the weights [7, 8, 72, 88, 17], by modifying the structure of\nthe network [33, 87, 55]. Backdoor attacks also have found innovative uses in copyright protection\n[28, 54, 52] and auditing differential privacy [44, 62, 101, 2, 69, 83].\nIn recent years, backdoor attacks and trigger design have become an active area of research. In the\ncanonical setting, first introduced by [34], an attacker injects malicious feature-based perturbations\ninto training associated with a source class and changes the labels to that of a target class. In computer\nvision, the first triggers shown to work were pixel-based stamps [34]. To make the triggers harder\nto detect, a variety of strategies including injecting periodic patterns [57] and mixing triggers with\nexisting features [20] were introduced. These strategies however fail to evade human detection when\nanalysis couples the images and their labels, as most backdoored images will appear mislabeled.\nIn response, [90] and [105] propose generative ML-based strategies to interpolate images from the\nsource and the target, creating images that induce backdoors with consistent labels. Notably, our\nmethod does not perturb images in training. Instead, that information is encoded in the labels being\nflipped.\nBackdoor defenses. Recently there has also been substantial work on detecting and mitigating\nbackdoor attacks. When the user has access to a separate pool of clean examples, they can filter the\ncorrupted dataset by detecting outliers [56, 49, 82], retrain the network so it forgets the backdoor\n[58], or train a new model to test the original for a backdoor [48]. Other defenses assume the trigger\nis an additive perturbation with small norm [92, 21], rely on smoothing [91, 95], filter or penalize\noutliers without clean data [29, 86, 82, 11, 70, 89, 37, 35], use self-supervised learning [42], or use\nByzantine-tolerant distributed learning techniques [11, 1, 19]. In general, it is possible to embed\nbackdoors in neural networks such that they cannot be detected [33]. In Table 16 we test three popular\ndefenses, kmeans [15], PCA [89], and SPECTRE [37], on label-only corrupted dataset learned\nusing FLIP. While FLIP almost completely bypasses the kmeans and PCA defenses, successfully\ncreating backdoors, the label-flipped examples are detected by SPECTRE, which completely removes\nthe corrupted examples. It remains an interesting future research direction to combine FLIP with\ntechniques that attempts to bypass representation-based defenses like SPECTRE, such as those from\n[71].\nKnowledge distillation. Since large models are more capable of learning concise and relevant\nknowledge representations for a training task, state-of-the-art models are frequently trained with\nbillions of parameters. Such scale is often impractical for deployment on edge devices, and knowledge\ndistillation, introduced in the seminal work of [39], has emerged as a reliable solution for distilling\nthe knowledge of large models into much smaller ones without sacrificing the performance, (e.g.,\n[16, 60, 85]). Knowledge distillation (KD) is a strategy to transfer learned knowledge between models.\nKD has been used to defend against adversarial perturbations [67], allow models to self-improve\n[102, 103], and boost interpretability of neural networks [59]. Knowledge distillation has been used\nto defend against backdoor attacks by distilling with clean data [100] and by also distilling attention\nmaps [51, 97]. Bypassing such knowledge distillation defenses is one of the two motivating use-cases\n                                                    17", "md": "# Extended related work\n\n## Extended related work\n\nCorrupting only the labels of a fraction of training data is common in triggerless data poisoning attacks. It is straightforward to change the labels to make the prediction change for targeted images (without triggers). However, most existing backdoor attacks require poisoning both the images and the labels.\n\nBackdoor attacks. Backdoor attacks were introduced in [34]. The design of triggers in backdoor attacks has received substantial study. Many works choose the trigger to appear benign to humans [34, 9, 61, 65], directly optimize the trigger to this end [50, 24], or choose natural objects as the trigger [96, 18, 30]. Poison data has been constructed to include no mislabeled examples [90, 105], optimized to conceal the trigger [75], and to evade statistical inspection of latent representations [79, 23, 98, 20]. Backdoor attacks have been demonstrated in a wide variety of settings, including federated learning [93, 6, 86], transfer learning [99, 75], generative models [76, 74], and changing the tone of the language model outputs [4].\n\nBackdoors can be injected in many creative ways, including poisoning of the loss [3], data ordering [80], or data augmentation [73]. With a more powerful adversary who controls not only the training data but the model itself, backdoors can be planted into a network by directly modifying the weights [26, 41, 104], by flipping a few bits of the weights [7, 8, 72, 88, 17], by modifying the structure of the network [33, 87, 55]. Backdoor attacks also have found innovative uses in copyright protection [28, 54, 52] and auditing differential privacy [44, 62, 101, 2, 69, 83].\n\nIn recent years, backdoor attacks and trigger design have become an active area of research. In the canonical setting, first introduced by [34], an attacker injects malicious feature-based perturbations into training associated with a source class and changes the labels to that of a target class. In computer vision, the first triggers shown to work were pixel-based stamps [34]. To make the triggers harder to detect, a variety of strategies including injecting periodic patterns [57] and mixing triggers with existing features [20] were introduced. These strategies however fail to evade human detection when analysis couples the images and their labels, as most backdoored images will appear mislabeled.\n\nIn response, [90] and [105] propose generative ML-based strategies to interpolate images from the source and the target, creating images that induce backdoors with consistent labels. Notably, our method does not perturb images in training. Instead, that information is encoded in the labels being flipped.\n\nBackdoor defenses. Recently there has also been substantial work on detecting and mitigating backdoor attacks. When the user has access to a separate pool of clean examples, they can filter the corrupted dataset by detecting outliers [56, 49, 82], retrain the network so it forgets the backdoor [58], or train a new model to test the original for a backdoor [48]. Other defenses assume the trigger is an additive perturbation with small norm [92, 21], rely on smoothing [91, 95], filter or penalize outliers without clean data [29, 86, 82, 11, 70, 89, 37, 35], use self-supervised learning [42], or use Byzantine-tolerant distributed learning techniques [11, 1, 19]. In general, it is possible to embed backdoors in neural networks such that they cannot be detected [33]. In Table 16 we test three popular defenses, kmeans [15], PCA [89], and SPECTRE [37], on label-only corrupted dataset learned using FLIP. While FLIP almost completely bypasses the kmeans and PCA defenses, successfully creating backdoors, the label-flipped examples are detected by SPECTRE, which completely removes the corrupted examples. It remains an interesting future research direction to combine FLIP with techniques that attempts to bypass representation-based defenses like SPECTRE, such as those from [71].\n\nKnowledge distillation. Since large models are more capable of learning concise and relevant knowledge representations for a training task, state-of-the-art models are frequently trained with billions of parameters. Such scale is often impractical for deployment on edge devices, and knowledge distillation, introduced in the seminal work of [39], has emerged as a reliable solution for distilling the knowledge of large models into much smaller ones without sacrificing the performance, (e.g., [16, 60, 85]). Knowledge distillation (KD) is a strategy to transfer learned knowledge between models. KD has been used to defend against adversarial perturbations [67], allow models to self-improve [102, 103], and boost interpretability of neural networks [59]. Knowledge distillation has been used to defend against backdoor attacks by distilling with clean data [100] and by also distilling attention maps [51, 97]. Bypassing such knowledge distillation defenses is one of the two motivating use-cases.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Extended related work", "md": "# Extended related work"}, {"type": "heading", "lvl": 2, "value": "Extended related work", "md": "## Extended related work"}, {"type": "text", "value": "Corrupting only the labels of a fraction of training data is common in triggerless data poisoning attacks. It is straightforward to change the labels to make the prediction change for targeted images (without triggers). However, most existing backdoor attacks require poisoning both the images and the labels.\n\nBackdoor attacks. Backdoor attacks were introduced in [34]. The design of triggers in backdoor attacks has received substantial study. Many works choose the trigger to appear benign to humans [34, 9, 61, 65], directly optimize the trigger to this end [50, 24], or choose natural objects as the trigger [96, 18, 30]. Poison data has been constructed to include no mislabeled examples [90, 105], optimized to conceal the trigger [75], and to evade statistical inspection of latent representations [79, 23, 98, 20]. Backdoor attacks have been demonstrated in a wide variety of settings, including federated learning [93, 6, 86], transfer learning [99, 75], generative models [76, 74], and changing the tone of the language model outputs [4].\n\nBackdoors can be injected in many creative ways, including poisoning of the loss [3], data ordering [80], or data augmentation [73]. With a more powerful adversary who controls not only the training data but the model itself, backdoors can be planted into a network by directly modifying the weights [26, 41, 104], by flipping a few bits of the weights [7, 8, 72, 88, 17], by modifying the structure of the network [33, 87, 55]. Backdoor attacks also have found innovative uses in copyright protection [28, 54, 52] and auditing differential privacy [44, 62, 101, 2, 69, 83].\n\nIn recent years, backdoor attacks and trigger design have become an active area of research. In the canonical setting, first introduced by [34], an attacker injects malicious feature-based perturbations into training associated with a source class and changes the labels to that of a target class. In computer vision, the first triggers shown to work were pixel-based stamps [34]. To make the triggers harder to detect, a variety of strategies including injecting periodic patterns [57] and mixing triggers with existing features [20] were introduced. These strategies however fail to evade human detection when analysis couples the images and their labels, as most backdoored images will appear mislabeled.\n\nIn response, [90] and [105] propose generative ML-based strategies to interpolate images from the source and the target, creating images that induce backdoors with consistent labels. Notably, our method does not perturb images in training. Instead, that information is encoded in the labels being flipped.\n\nBackdoor defenses. Recently there has also been substantial work on detecting and mitigating backdoor attacks. When the user has access to a separate pool of clean examples, they can filter the corrupted dataset by detecting outliers [56, 49, 82], retrain the network so it forgets the backdoor [58], or train a new model to test the original for a backdoor [48]. Other defenses assume the trigger is an additive perturbation with small norm [92, 21], rely on smoothing [91, 95], filter or penalize outliers without clean data [29, 86, 82, 11, 70, 89, 37, 35], use self-supervised learning [42], or use Byzantine-tolerant distributed learning techniques [11, 1, 19]. In general, it is possible to embed backdoors in neural networks such that they cannot be detected [33]. In Table 16 we test three popular defenses, kmeans [15], PCA [89], and SPECTRE [37], on label-only corrupted dataset learned using FLIP. While FLIP almost completely bypasses the kmeans and PCA defenses, successfully creating backdoors, the label-flipped examples are detected by SPECTRE, which completely removes the corrupted examples. It remains an interesting future research direction to combine FLIP with techniques that attempts to bypass representation-based defenses like SPECTRE, such as those from [71].\n\nKnowledge distillation. Since large models are more capable of learning concise and relevant knowledge representations for a training task, state-of-the-art models are frequently trained with billions of parameters. Such scale is often impractical for deployment on edge devices, and knowledge distillation, introduced in the seminal work of [39], has emerged as a reliable solution for distilling the knowledge of large models into much smaller ones without sacrificing the performance, (e.g., [16, 60, 85]). Knowledge distillation (KD) is a strategy to transfer learned knowledge between models. KD has been used to defend against adversarial perturbations [67], allow models to self-improve [102, 103], and boost interpretability of neural networks [59]. Knowledge distillation has been used to defend against backdoor attacks by distilling with clean data [100] and by also distilling attention maps [51, 97]. Bypassing such knowledge distillation defenses is one of the two motivating use-cases.", "md": "Corrupting only the labels of a fraction of training data is common in triggerless data poisoning attacks. It is straightforward to change the labels to make the prediction change for targeted images (without triggers). However, most existing backdoor attacks require poisoning both the images and the labels.\n\nBackdoor attacks. Backdoor attacks were introduced in [34]. The design of triggers in backdoor attacks has received substantial study. Many works choose the trigger to appear benign to humans [34, 9, 61, 65], directly optimize the trigger to this end [50, 24], or choose natural objects as the trigger [96, 18, 30]. Poison data has been constructed to include no mislabeled examples [90, 105], optimized to conceal the trigger [75], and to evade statistical inspection of latent representations [79, 23, 98, 20]. Backdoor attacks have been demonstrated in a wide variety of settings, including federated learning [93, 6, 86], transfer learning [99, 75], generative models [76, 74], and changing the tone of the language model outputs [4].\n\nBackdoors can be injected in many creative ways, including poisoning of the loss [3], data ordering [80], or data augmentation [73]. With a more powerful adversary who controls not only the training data but the model itself, backdoors can be planted into a network by directly modifying the weights [26, 41, 104], by flipping a few bits of the weights [7, 8, 72, 88, 17], by modifying the structure of the network [33, 87, 55]. Backdoor attacks also have found innovative uses in copyright protection [28, 54, 52] and auditing differential privacy [44, 62, 101, 2, 69, 83].\n\nIn recent years, backdoor attacks and trigger design have become an active area of research. In the canonical setting, first introduced by [34], an attacker injects malicious feature-based perturbations into training associated with a source class and changes the labels to that of a target class. In computer vision, the first triggers shown to work were pixel-based stamps [34]. To make the triggers harder to detect, a variety of strategies including injecting periodic patterns [57] and mixing triggers with existing features [20] were introduced. These strategies however fail to evade human detection when analysis couples the images and their labels, as most backdoored images will appear mislabeled.\n\nIn response, [90] and [105] propose generative ML-based strategies to interpolate images from the source and the target, creating images that induce backdoors with consistent labels. Notably, our method does not perturb images in training. Instead, that information is encoded in the labels being flipped.\n\nBackdoor defenses. Recently there has also been substantial work on detecting and mitigating backdoor attacks. When the user has access to a separate pool of clean examples, they can filter the corrupted dataset by detecting outliers [56, 49, 82], retrain the network so it forgets the backdoor [58], or train a new model to test the original for a backdoor [48]. Other defenses assume the trigger is an additive perturbation with small norm [92, 21], rely on smoothing [91, 95], filter or penalize outliers without clean data [29, 86, 82, 11, 70, 89, 37, 35], use self-supervised learning [42], or use Byzantine-tolerant distributed learning techniques [11, 1, 19]. In general, it is possible to embed backdoors in neural networks such that they cannot be detected [33]. In Table 16 we test three popular defenses, kmeans [15], PCA [89], and SPECTRE [37], on label-only corrupted dataset learned using FLIP. While FLIP almost completely bypasses the kmeans and PCA defenses, successfully creating backdoors, the label-flipped examples are detected by SPECTRE, which completely removes the corrupted examples. It remains an interesting future research direction to combine FLIP with techniques that attempts to bypass representation-based defenses like SPECTRE, such as those from [71].\n\nKnowledge distillation. Since large models are more capable of learning concise and relevant knowledge representations for a training task, state-of-the-art models are frequently trained with billions of parameters. Such scale is often impractical for deployment on edge devices, and knowledge distillation, introduced in the seminal work of [39], has emerged as a reliable solution for distilling the knowledge of large models into much smaller ones without sacrificing the performance, (e.g., [16, 60, 85]). Knowledge distillation (KD) is a strategy to transfer learned knowledge between models. KD has been used to defend against adversarial perturbations [67], allow models to self-improve [102, 103], and boost interpretability of neural networks [59]. Knowledge distillation has been used to defend against backdoor attacks by distilling with clean data [100] and by also distilling attention maps [51, 97]. Bypassing such knowledge distillation defenses is one of the two motivating use-cases."}]}, {"page": 18, "text": "of our attack. We introduce softFLIP and show that softFLIP improves upon FLIP leveraging the\nextra freedom in what corrupted labels can be returned (see Fig. 7).\nDataset distillation and trajectory matching. Introduced in [94], the goal of dataset distillation is to\nproduce a small dataset which captures the essence of a larger one. Dataset distillation by optimizing\nsoft labels has been explored using the neural tangent kernel [64, 63] and gradient-based methods\n[12, 84]. Our method attempts to match the training trajectory of a normal backdoored model, with\nstandard poisoned examples, by flipping labels. Trajectory matching has been used previously for\ndataset distillation [14] and bears similarity to imitation learning [66, 27, 40]. The use of a proxy\nobjective in weight space for backdoor design appears in the KKT attack of [47]. However, the focus\nis in making the attacker\u2019s optimization more efficient for binary classification problems. Initially we\napproximately solves the bilevel optimization equation 2 efficiently using Neural Tangent Kernels to\napproximate the solution of the inner optimization. Similar NTK-based approach has been successful,\nfor example, in learning the strongest backdoor attacks in [38]. NTK-based methods\u2019 runtime scales\nas C2 when C is the number of classes, and we could only apply it to binary classifications. On the\nother hand, trajectory matching of FLIP can be applied to signifi  cantly more complex problems with\nlarger models and generalizes surprisingly well to the scenario where the attacker does not know the\nmodel, data, hyperparameters to be used by the user as we show in Section 3.2. Closest to our work\nis [32], which uses gradient matching to design poisoned training images (as opposed to labels). The\ngoal is targeted data poisoning (as opposed to a more general backdoor attack) which aims to alter\nthe prediction of the model trained on corrupted data, for a specific image at inference time.\nB    Experimental details\nIn this section, for completeness, we present some of the technical details on our evaluation pipeline\nthat were omitted in the main text. For most of our experiments, the pipeline proceeds by (1) training\nexpert models, (2) generating synthetic labels, and (3) measuring our attack success on a user model\ntrained on label-flipped datasets. To compute final numbers, 10 user models were trained on each\nset of computed labels. Each experiment was run on a single, randomly-selected GPU on a cluster\ncontaining NVIDIA A40 and 2080ti GPUs. On the slower 2080ti GPUs, our ResNet and CIFAR\nexperiments took no longer than an hour, while, for the much larger VGG and ViT models, compute\ntimes were longer.\nB.1   Datasets and poisoning procedures\nWe evaluate FLIP and softFLIP on three standard classification datasets of increasing difficulty:\nCIFAR-10, CIFAR-100, and Tiny-ImageNet. For better test performance and to simulate real-world\nuse cases, we follow the standard CIFAR data augmentation procedure of (1) normalizing the data\nand (2) applying PyTorch transforms: RandomCrop and RandomHorizontalFlip. For RandomCrop,\nevery epoch, each image was cropped down to a random 28 \u00d7 28 subset of the original image with\nthe extra 4 pixels reintroduced as padding. RandomHorizontalFlip randomly flipped each image\nhorizontally with a 50% probability every epoch. For our experiments on the Vision Transformer,\nimages were scaled to 224 \u00d7 224 pixels and cropped to 220 \u00d7 220 before padding and flipping.\nTo train our expert models we poisoned each dataset setting ysource = 9 and ytarget = 4 (i.e., the\nninth and fourth labels in each dataset). Since CIFAR-100 and Tiny-ImageNet have only 500 images\nper class, for the former, we use the coarse-labels for ysource and ytarget, while for the latter we\noversample poisoned points during the expert training stage. For CIFAR-10 this corresponds to\na canonical self-driving-car-inspired backdoor attack setup in which the source label consists of\nimages of trucks and target label corresponds to deer. For CIFAR-100, the mapping corresponds\nto a source class of \u2018large man-made outdoor things\u2019 and a target of \u2018fruit and vegetables.\u2019 Finally,\nfor Tiny-ImageNet, \u2019tarantulas\u2019 were mapped to \u2019American alligators\u2019. To poison the dataset, each\nysource = 9 image had a trigger applied to it and was appended to the dataset.\nTrigger details. We used the following three triggers, in increasing order of strength. Examples of\nthe triggers are shown in Fig. 4.\n       \u2022 Pixel Trigger [89] (T = p). To inject the pixel trigger into an image, at three pixel locations\n         of the photograph the existing colors are replaced by pre-selected colors. Notably, the\n         original attack was proposed with a single pixel and color combination (that we use),\n                                                   18", "md": "# Document\n\nof our attack. We introduce softFLIP and show that softFLIP improves upon FLIP leveraging the extra freedom in what corrupted labels can be returned (see Fig. 7).\n\nDataset distillation and trajectory matching. Introduced in [94], the goal of dataset distillation is to produce a small dataset which captures the essence of a larger one. Dataset distillation by optimizing soft labels has been explored using the neural tangent kernel [64, 63] and gradient-based methods [12, 84]. Our method attempts to match the training trajectory of a normal backdoored model, with standard poisoned examples, by flipping labels. Trajectory matching has been used previously for dataset distillation [14] and bears similarity to imitation learning [66, 27, 40]. The use of a proxy objective in weight space for backdoor design appears in the KKT attack of [47]. However, the focus is in making the attacker\u2019s optimization more efficient for binary classification problems. Initially we approximately solve the bilevel optimization equation 2 efficiently using Neural Tangent Kernels to approximate the solution of the inner optimization. Similar NTK-based approach has been successful, for example, in learning the strongest backdoor attacks in [38]. NTK-based methods\u2019 runtime scales as C^2 when C is the number of classes, and we could only apply it to binary classifications. On the other hand, trajectory matching of FLIP can be applied to significantly more complex problems with larger models and generalizes surprisingly well to the scenario where the attacker does not know the model, data, hyperparameters to be used by the user as we show in Section 3.2. Closest to our work is [32], which uses gradient matching to design poisoned training images (as opposed to labels). The goal is targeted data poisoning (as opposed to a more general backdoor attack) which aims to alter the prediction of the model trained on corrupted data, for a specific image at inference time.\n\n## Experimental details\n\nIn this section, for completeness, we present some of the technical details on our evaluation pipeline that were omitted in the main text. For most of our experiments, the pipeline proceeds by (1) training expert models, (2) generating synthetic labels, and (3) measuring our attack success on a user model trained on label-flipped datasets. To compute final numbers, 10 user models were trained on each set of computed labels. Each experiment was run on a single, randomly-selected GPU on a cluster containing NVIDIA A40 and 2080ti GPUs. On the slower 2080ti GPUs, our ResNet and CIFAR experiments took no longer than an hour, while, for the much larger VGG and ViT models, compute times were longer.\n\n### Datasets and poisoning procedures\n\nWe evaluate FLIP and softFLIP on three standard classification datasets of increasing difficulty: CIFAR-10, CIFAR-100, and Tiny-ImageNet. For better test performance and to simulate real-world use cases, we follow the standard CIFAR data augmentation procedure of (1) normalizing the data and (2) applying PyTorch transforms: RandomCrop and RandomHorizontalFlip. For RandomCrop, every epoch, each image was cropped down to a random 28 \u00d7 28 subset of the original image with the extra 4 pixels reintroduced as padding. RandomHorizontalFlip randomly flipped each image horizontally with a 50% probability every epoch. For our experiments on the Vision Transformer, images were scaled to 224 \u00d7 224 pixels and cropped to 220 \u00d7 220 before padding and flipping.\n\nTo train our expert models we poisoned each dataset setting ysource = 9 and ytarget = 4 (i.e., the ninth and fourth labels in each dataset). Since CIFAR-100 and Tiny-ImageNet have only 500 images per class, for the former, we use the coarse-labels for ysource and ytarget, while for the latter we oversample poisoned points during the expert training stage. For CIFAR-10 this corresponds to a canonical self-driving-car-inspired backdoor attack setup in which the source label consists of images of trucks and target label corresponds to deer. For CIFAR-100, the mapping corresponds to a source class of \u2018large man-made outdoor things\u2019 and a target of \u2018fruit and vegetables.\u2019 Finally, for Tiny-ImageNet, \u2019tarantulas\u2019 were mapped to \u2019American alligators\u2019. To poison the dataset, each ysource = 9 image had a trigger applied to it and was appended to the dataset.\n\nTrigger details. We used the following three triggers, in increasing order of strength. Examples of the triggers are shown in Fig. 4.\n\n- Pixel Trigger [89] (T = p). To inject the pixel trigger into an image, at three pixel locations of the photograph the existing colors are replaced by pre-selected colors. Notably, the original attack was proposed with a single pixel and color combination (that we use).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "of our attack. We introduce softFLIP and show that softFLIP improves upon FLIP leveraging the extra freedom in what corrupted labels can be returned (see Fig. 7).\n\nDataset distillation and trajectory matching. Introduced in [94], the goal of dataset distillation is to produce a small dataset which captures the essence of a larger one. Dataset distillation by optimizing soft labels has been explored using the neural tangent kernel [64, 63] and gradient-based methods [12, 84]. Our method attempts to match the training trajectory of a normal backdoored model, with standard poisoned examples, by flipping labels. Trajectory matching has been used previously for dataset distillation [14] and bears similarity to imitation learning [66, 27, 40]. The use of a proxy objective in weight space for backdoor design appears in the KKT attack of [47]. However, the focus is in making the attacker\u2019s optimization more efficient for binary classification problems. Initially we approximately solve the bilevel optimization equation 2 efficiently using Neural Tangent Kernels to approximate the solution of the inner optimization. Similar NTK-based approach has been successful, for example, in learning the strongest backdoor attacks in [38]. NTK-based methods\u2019 runtime scales as C^2 when C is the number of classes, and we could only apply it to binary classifications. On the other hand, trajectory matching of FLIP can be applied to significantly more complex problems with larger models and generalizes surprisingly well to the scenario where the attacker does not know the model, data, hyperparameters to be used by the user as we show in Section 3.2. Closest to our work is [32], which uses gradient matching to design poisoned training images (as opposed to labels). The goal is targeted data poisoning (as opposed to a more general backdoor attack) which aims to alter the prediction of the model trained on corrupted data, for a specific image at inference time.", "md": "of our attack. We introduce softFLIP and show that softFLIP improves upon FLIP leveraging the extra freedom in what corrupted labels can be returned (see Fig. 7).\n\nDataset distillation and trajectory matching. Introduced in [94], the goal of dataset distillation is to produce a small dataset which captures the essence of a larger one. Dataset distillation by optimizing soft labels has been explored using the neural tangent kernel [64, 63] and gradient-based methods [12, 84]. Our method attempts to match the training trajectory of a normal backdoored model, with standard poisoned examples, by flipping labels. Trajectory matching has been used previously for dataset distillation [14] and bears similarity to imitation learning [66, 27, 40]. The use of a proxy objective in weight space for backdoor design appears in the KKT attack of [47]. However, the focus is in making the attacker\u2019s optimization more efficient for binary classification problems. Initially we approximately solve the bilevel optimization equation 2 efficiently using Neural Tangent Kernels to approximate the solution of the inner optimization. Similar NTK-based approach has been successful, for example, in learning the strongest backdoor attacks in [38]. NTK-based methods\u2019 runtime scales as C^2 when C is the number of classes, and we could only apply it to binary classifications. On the other hand, trajectory matching of FLIP can be applied to significantly more complex problems with larger models and generalizes surprisingly well to the scenario where the attacker does not know the model, data, hyperparameters to be used by the user as we show in Section 3.2. Closest to our work is [32], which uses gradient matching to design poisoned training images (as opposed to labels). The goal is targeted data poisoning (as opposed to a more general backdoor attack) which aims to alter the prediction of the model trained on corrupted data, for a specific image at inference time."}, {"type": "heading", "lvl": 2, "value": "Experimental details", "md": "## Experimental details"}, {"type": "text", "value": "In this section, for completeness, we present some of the technical details on our evaluation pipeline that were omitted in the main text. For most of our experiments, the pipeline proceeds by (1) training expert models, (2) generating synthetic labels, and (3) measuring our attack success on a user model trained on label-flipped datasets. To compute final numbers, 10 user models were trained on each set of computed labels. Each experiment was run on a single, randomly-selected GPU on a cluster containing NVIDIA A40 and 2080ti GPUs. On the slower 2080ti GPUs, our ResNet and CIFAR experiments took no longer than an hour, while, for the much larger VGG and ViT models, compute times were longer.", "md": "In this section, for completeness, we present some of the technical details on our evaluation pipeline that were omitted in the main text. For most of our experiments, the pipeline proceeds by (1) training expert models, (2) generating synthetic labels, and (3) measuring our attack success on a user model trained on label-flipped datasets. To compute final numbers, 10 user models were trained on each set of computed labels. Each experiment was run on a single, randomly-selected GPU on a cluster containing NVIDIA A40 and 2080ti GPUs. On the slower 2080ti GPUs, our ResNet and CIFAR experiments took no longer than an hour, while, for the much larger VGG and ViT models, compute times were longer."}, {"type": "heading", "lvl": 3, "value": "Datasets and poisoning procedures", "md": "### Datasets and poisoning procedures"}, {"type": "text", "value": "We evaluate FLIP and softFLIP on three standard classification datasets of increasing difficulty: CIFAR-10, CIFAR-100, and Tiny-ImageNet. For better test performance and to simulate real-world use cases, we follow the standard CIFAR data augmentation procedure of (1) normalizing the data and (2) applying PyTorch transforms: RandomCrop and RandomHorizontalFlip. For RandomCrop, every epoch, each image was cropped down to a random 28 \u00d7 28 subset of the original image with the extra 4 pixels reintroduced as padding. RandomHorizontalFlip randomly flipped each image horizontally with a 50% probability every epoch. For our experiments on the Vision Transformer, images were scaled to 224 \u00d7 224 pixels and cropped to 220 \u00d7 220 before padding and flipping.\n\nTo train our expert models we poisoned each dataset setting ysource = 9 and ytarget = 4 (i.e., the ninth and fourth labels in each dataset). Since CIFAR-100 and Tiny-ImageNet have only 500 images per class, for the former, we use the coarse-labels for ysource and ytarget, while for the latter we oversample poisoned points during the expert training stage. For CIFAR-10 this corresponds to a canonical self-driving-car-inspired backdoor attack setup in which the source label consists of images of trucks and target label corresponds to deer. For CIFAR-100, the mapping corresponds to a source class of \u2018large man-made outdoor things\u2019 and a target of \u2018fruit and vegetables.\u2019 Finally, for Tiny-ImageNet, \u2019tarantulas\u2019 were mapped to \u2019American alligators\u2019. To poison the dataset, each ysource = 9 image had a trigger applied to it and was appended to the dataset.\n\nTrigger details. We used the following three triggers, in increasing order of strength. Examples of the triggers are shown in Fig. 4.\n\n- Pixel Trigger [89] (T = p). To inject the pixel trigger into an image, at three pixel locations of the photograph the existing colors are replaced by pre-selected colors. Notably, the original attack was proposed with a single pixel and color combination (that we use).", "md": "We evaluate FLIP and softFLIP on three standard classification datasets of increasing difficulty: CIFAR-10, CIFAR-100, and Tiny-ImageNet. For better test performance and to simulate real-world use cases, we follow the standard CIFAR data augmentation procedure of (1) normalizing the data and (2) applying PyTorch transforms: RandomCrop and RandomHorizontalFlip. For RandomCrop, every epoch, each image was cropped down to a random 28 \u00d7 28 subset of the original image with the extra 4 pixels reintroduced as padding. RandomHorizontalFlip randomly flipped each image horizontally with a 50% probability every epoch. For our experiments on the Vision Transformer, images were scaled to 224 \u00d7 224 pixels and cropped to 220 \u00d7 220 before padding and flipping.\n\nTo train our expert models we poisoned each dataset setting ysource = 9 and ytarget = 4 (i.e., the ninth and fourth labels in each dataset). Since CIFAR-100 and Tiny-ImageNet have only 500 images per class, for the former, we use the coarse-labels for ysource and ytarget, while for the latter we oversample poisoned points during the expert training stage. For CIFAR-10 this corresponds to a canonical self-driving-car-inspired backdoor attack setup in which the source label consists of images of trucks and target label corresponds to deer. For CIFAR-100, the mapping corresponds to a source class of \u2018large man-made outdoor things\u2019 and a target of \u2018fruit and vegetables.\u2019 Finally, for Tiny-ImageNet, \u2019tarantulas\u2019 were mapped to \u2019American alligators\u2019. To poison the dataset, each ysource = 9 image had a trigger applied to it and was appended to the dataset.\n\nTrigger details. We used the following three triggers, in increasing order of strength. Examples of the triggers are shown in Fig. 4.\n\n- Pixel Trigger [89] (T = p). To inject the pixel trigger into an image, at three pixel locations of the photograph the existing colors are replaced by pre-selected colors. Notably, the original attack was proposed with a single pixel and color combination (that we use)."}]}, {"page": 19, "text": "          so, to perform our stronger version, we add two randomly selected pixel locations and\n          colors. In particular, the locations are {(11, 16), (5, 27), (30, 7)} and the respective colors in\n          hexadecimal are {#650019, #657B79, #002436}. This trigger is the weakest of our triggers\n          with the smallest pixel-space perturbation\n       \u2022 Periodic / Sinusoidal Trigger [9] (T = s). The periodic attack adds periodic noise along\n          the horizontal axis (although the trigger can be generalized to the vertical axis as well). We\n          chose an amplitude of 6 and a frequency of 8. This trigger has a large, but visually subtle\n          effect on the pixels in an image making it the second most potent of our triggers.\n       \u2022 Patch / Turner Trigger [90] (T = t). For our version of the patch poisoner, we adopted the\n          3 \u00d7 3 watermark proposed by the original authors and applied it to each corner of the image\n          to persist through our RandomCrop procedure. This trigger seems to perform the best on\n          our experiments, likely due to its strong pixel-space signal.\nB.2   Models and optimizers\nFor our experiments, we use the ResNet-32, ResNet-18, VGG-19, and (pretrained) VIT-B-16 architec-\ntures with around 0.5, 11.4, 144, and 86 million parameters, respectively. In the ResNet experiments,\nthe expert and user models were trained using SGD with a batch size of 256, starting learning rate of\n\u03b3 = 0.1 (scheduled to reduce by a factor of 10 at epoch 75 and 150), weight decay of \u03bb = 0.0002,\nand Nesterov momentum of \u00b5 = 0.9. For the larger VGG and ViT models the learning rate and\nweight decay were adjusted as follows \u03b3 = 0.01, 0.05 and \u03bb = 0.0002, 0.0005, respectively. For\nTable 5, in which we mismatch the expert and downstream optimizers, we use Adam with the same\nbatch size, starting learning rate of \u03b3 = 0.001 (scheduled to reduce by a factor of 10 at epoch 125),\nweight decay of \u03bb = 0.0001, and (\u03b21, \u03b22) = (0.9, 0.999).\nWe note that the hyperparameters were set to achieve near 100% train accuracy after 200 epochs,\nbut, FLIP requires far fewer epochs of the expert trajectories. So, expert models were trained for 20\nepochs while the user models were trained for the full 200. Expert model weights were saved every\n50 iterations / minibatches (i.e., for batch size 256 around four times an epoch).\nB.3   FLIP details\nFor each iteration of Algorithm 1, we sampled an expert model at uniform random, while checkpoints\nwere sampled at uniform random from the first 20 epochs of the chosen expert\u2019s training run. Since\nweights were recorded every 50 iterations, from each checkpoint a single stochastic gradient descent\niteration was run with both the clean minibatch and the poisoned minibatch (as a proxy for the actual\nexpert minibatch step) and the loss computed accordingly. Both gradient steps adhered to the training\nhyperparameters described above. The algorithm was run for N = 25 iterations through the entire\ndataset.\nTo initialize \u02dc\n              \u2113, we use the original one-hot labels y scaled by a temperature parameter C. For\nsufficiently large C, the two gradient steps in Fig. 3 will be very similar except for the changes in the\npoisoned examples, leading to a low initial value of Lparam. However if C is too large, we suffer\nfrom vanishing gradients of the softmax. Therefore C must be chosen to balance these two concerns.\nB.4   Compute\nAll of our experiments were done on a computing cluster containing NVIDIA A40 and 2080ti GPUs\nwith tasks split roughly evenly between the two. To compute all of our numbers (averaged over 10\nuser models) we ended up computing approximately 3490 user models, 160 sets of labels, and 955\nexpert models. Averaging over GPU architecture, dataset, and model architecture, we note that each\nset of labels takes around 40 minutes to train. Meanwhile, each expert model takes around 10 minutes\nto train (fewer epochs with a more costly weight-saving procedure) and each user model takes around\n40. This amounts to a total of approximately 2595 GPU-hours.\nWe note that the number of GPU-hours for an adversary to pull off this attack is likely significantly\nlower since they would need to compute as few as a single expert model (10 minutes) and a set of\nlabels (40 minutes). This amounts to just under one GPU-hour given our setup (subject to hardware),\na surprisingly low sum for an attack of this high potency.\n                                                    19", "md": "# Document\n\nso, to perform our stronger version, we add two randomly selected pixel locations and colors. In particular, the locations are {(11, 16), (5, 27), (30, 7)} and the respective colors in hexadecimal are {#650019, #657B79, #002436}. This trigger is the weakest of our triggers with the smallest pixel-space perturbation\n\n- Periodic / Sinusoidal Trigger [9] (T = s). The periodic attack adds periodic noise along the horizontal axis (although the trigger can be generalized to the vertical axis as well). We chose an amplitude of 6 and a frequency of 8. This trigger has a large, but visually subtle effect on the pixels in an image making it the second most potent of our triggers.\n- Patch / Turner Trigger [90] (T = t). For our version of the patch poisoner, we adopted the 3 \u00d7 3 watermark proposed by the original authors and applied it to each corner of the image to persist through our RandomCrop procedure. This trigger seems to perform the best on our experiments, likely due to its strong pixel-space signal.\n\n## Models and optimizers\n\nFor our experiments, we use the ResNet-32, ResNet-18, VGG-19, and (pretrained) VIT-B-16 architectures with around 0.5, 11.4, 144, and 86 million parameters, respectively. In the ResNet experiments, the expert and user models were trained using SGD with a batch size of 256, starting learning rate of $$\\gamma = 0.1$$ (scheduled to reduce by a factor of 10 at epoch 75 and 150), weight decay of $$\\lambda = 0.0002$$, and Nesterov momentum of $$\\mu = 0.9$$. For the larger VGG and ViT models the learning rate and weight decay were adjusted as follows $$\\gamma = 0.01, 0.05$$ and $$\\lambda = 0.0002, 0.0005$$, respectively. For Table 5, in which we mismatch the expert and downstream optimizers, we use Adam with the same batch size, starting learning rate of $$\\gamma = 0.001$$ (scheduled to reduce by a factor of 10 at epoch 125), weight decay of $$\\lambda = 0.0001$$, and $$(\\beta_1, \\beta_2) = (0.9, 0.999)$$.\n\nWe note that the hyperparameters were set to achieve near 100% train accuracy after 200 epochs, but, FLIP requires far fewer epochs of the expert trajectories. So, expert models were trained for 20 epochs while the user models were trained for the full 200. Expert model weights were saved every 50 iterations / minibatches (i.e., for batch size 256 around four times an epoch).\n\n## FLIP details\n\nFor each iteration of Algorithm 1, we sampled an expert model at uniform random, while checkpoints were sampled at uniform random from the first 20 epochs of the chosen expert\u2019s training run. Since weights were recorded every 50 iterations, from each checkpoint a single stochastic gradient descent iteration was run with both the clean minibatch and the poisoned minibatch (as a proxy for the actual expert minibatch step) and the loss computed accordingly. Both gradient steps adhered to the training hyperparameters described above. The algorithm was run for $$N = 25$$ iterations through the entire dataset.\n\nTo initialize $$\\tilde{\\ell}$$, we use the original one-hot labels $$y$$ scaled by a temperature parameter $$C$$. For sufficiently large $$C$$, the two gradient steps in Fig. 3 will be very similar except for the changes in the poisoned examples, leading to a low initial value of $$L_{param}$$. However if $$C$$ is too large, we suffer from vanishing gradients of the softmax. Therefore $$C$$ must be chosen to balance these two concerns.\n\n## Compute\n\nAll of our experiments were done on a computing cluster containing NVIDIA A40 and 2080ti GPUs with tasks split roughly evenly between the two. To compute all of our numbers (averaged over 10 user models) we ended up computing approximately 3490 user models, 160 sets of labels, and 955 expert models. Averaging over GPU architecture, dataset, and model architecture, we note that each set of labels takes around 40 minutes to train. Meanwhile, each expert model takes around 10 minutes to train (fewer epochs with a more costly weight-saving procedure) and each user model takes around 40. This amounts to a total of approximately 2595 GPU-hours.\n\nWe note that the number of GPU-hours for an adversary to pull off this attack is likely significantly lower since they would need to compute as few as a single expert model (10 minutes) and a set of labels (40 minutes). This amounts to just under one GPU-hour given our setup (subject to hardware), a surprisingly low sum for an attack of this high potency.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "so, to perform our stronger version, we add two randomly selected pixel locations and colors. In particular, the locations are {(11, 16), (5, 27), (30, 7)} and the respective colors in hexadecimal are {#650019, #657B79, #002436}. This trigger is the weakest of our triggers with the smallest pixel-space perturbation\n\n- Periodic / Sinusoidal Trigger [9] (T = s). The periodic attack adds periodic noise along the horizontal axis (although the trigger can be generalized to the vertical axis as well). We chose an amplitude of 6 and a frequency of 8. This trigger has a large, but visually subtle effect on the pixels in an image making it the second most potent of our triggers.\n- Patch / Turner Trigger [90] (T = t). For our version of the patch poisoner, we adopted the 3 \u00d7 3 watermark proposed by the original authors and applied it to each corner of the image to persist through our RandomCrop procedure. This trigger seems to perform the best on our experiments, likely due to its strong pixel-space signal.", "md": "so, to perform our stronger version, we add two randomly selected pixel locations and colors. In particular, the locations are {(11, 16), (5, 27), (30, 7)} and the respective colors in hexadecimal are {#650019, #657B79, #002436}. This trigger is the weakest of our triggers with the smallest pixel-space perturbation\n\n- Periodic / Sinusoidal Trigger [9] (T = s). The periodic attack adds periodic noise along the horizontal axis (although the trigger can be generalized to the vertical axis as well). We chose an amplitude of 6 and a frequency of 8. This trigger has a large, but visually subtle effect on the pixels in an image making it the second most potent of our triggers.\n- Patch / Turner Trigger [90] (T = t). For our version of the patch poisoner, we adopted the 3 \u00d7 3 watermark proposed by the original authors and applied it to each corner of the image to persist through our RandomCrop procedure. This trigger seems to perform the best on our experiments, likely due to its strong pixel-space signal."}, {"type": "heading", "lvl": 2, "value": "Models and optimizers", "md": "## Models and optimizers"}, {"type": "text", "value": "For our experiments, we use the ResNet-32, ResNet-18, VGG-19, and (pretrained) VIT-B-16 architectures with around 0.5, 11.4, 144, and 86 million parameters, respectively. In the ResNet experiments, the expert and user models were trained using SGD with a batch size of 256, starting learning rate of $$\\gamma = 0.1$$ (scheduled to reduce by a factor of 10 at epoch 75 and 150), weight decay of $$\\lambda = 0.0002$$, and Nesterov momentum of $$\\mu = 0.9$$. For the larger VGG and ViT models the learning rate and weight decay were adjusted as follows $$\\gamma = 0.01, 0.05$$ and $$\\lambda = 0.0002, 0.0005$$, respectively. For Table 5, in which we mismatch the expert and downstream optimizers, we use Adam with the same batch size, starting learning rate of $$\\gamma = 0.001$$ (scheduled to reduce by a factor of 10 at epoch 125), weight decay of $$\\lambda = 0.0001$$, and $$(\\beta_1, \\beta_2) = (0.9, 0.999)$$.\n\nWe note that the hyperparameters were set to achieve near 100% train accuracy after 200 epochs, but, FLIP requires far fewer epochs of the expert trajectories. So, expert models were trained for 20 epochs while the user models were trained for the full 200. Expert model weights were saved every 50 iterations / minibatches (i.e., for batch size 256 around four times an epoch).", "md": "For our experiments, we use the ResNet-32, ResNet-18, VGG-19, and (pretrained) VIT-B-16 architectures with around 0.5, 11.4, 144, and 86 million parameters, respectively. In the ResNet experiments, the expert and user models were trained using SGD with a batch size of 256, starting learning rate of $$\\gamma = 0.1$$ (scheduled to reduce by a factor of 10 at epoch 75 and 150), weight decay of $$\\lambda = 0.0002$$, and Nesterov momentum of $$\\mu = 0.9$$. For the larger VGG and ViT models the learning rate and weight decay were adjusted as follows $$\\gamma = 0.01, 0.05$$ and $$\\lambda = 0.0002, 0.0005$$, respectively. For Table 5, in which we mismatch the expert and downstream optimizers, we use Adam with the same batch size, starting learning rate of $$\\gamma = 0.001$$ (scheduled to reduce by a factor of 10 at epoch 125), weight decay of $$\\lambda = 0.0001$$, and $$(\\beta_1, \\beta_2) = (0.9, 0.999)$$.\n\nWe note that the hyperparameters were set to achieve near 100% train accuracy after 200 epochs, but, FLIP requires far fewer epochs of the expert trajectories. So, expert models were trained for 20 epochs while the user models were trained for the full 200. Expert model weights were saved every 50 iterations / minibatches (i.e., for batch size 256 around four times an epoch)."}, {"type": "heading", "lvl": 2, "value": "FLIP details", "md": "## FLIP details"}, {"type": "text", "value": "For each iteration of Algorithm 1, we sampled an expert model at uniform random, while checkpoints were sampled at uniform random from the first 20 epochs of the chosen expert\u2019s training run. Since weights were recorded every 50 iterations, from each checkpoint a single stochastic gradient descent iteration was run with both the clean minibatch and the poisoned minibatch (as a proxy for the actual expert minibatch step) and the loss computed accordingly. Both gradient steps adhered to the training hyperparameters described above. The algorithm was run for $$N = 25$$ iterations through the entire dataset.\n\nTo initialize $$\\tilde{\\ell}$$, we use the original one-hot labels $$y$$ scaled by a temperature parameter $$C$$. For sufficiently large $$C$$, the two gradient steps in Fig. 3 will be very similar except for the changes in the poisoned examples, leading to a low initial value of $$L_{param}$$. However if $$C$$ is too large, we suffer from vanishing gradients of the softmax. Therefore $$C$$ must be chosen to balance these two concerns.", "md": "For each iteration of Algorithm 1, we sampled an expert model at uniform random, while checkpoints were sampled at uniform random from the first 20 epochs of the chosen expert\u2019s training run. Since weights were recorded every 50 iterations, from each checkpoint a single stochastic gradient descent iteration was run with both the clean minibatch and the poisoned minibatch (as a proxy for the actual expert minibatch step) and the loss computed accordingly. Both gradient steps adhered to the training hyperparameters described above. The algorithm was run for $$N = 25$$ iterations through the entire dataset.\n\nTo initialize $$\\tilde{\\ell}$$, we use the original one-hot labels $$y$$ scaled by a temperature parameter $$C$$. For sufficiently large $$C$$, the two gradient steps in Fig. 3 will be very similar except for the changes in the poisoned examples, leading to a low initial value of $$L_{param}$$. However if $$C$$ is too large, we suffer from vanishing gradients of the softmax. Therefore $$C$$ must be chosen to balance these two concerns."}, {"type": "heading", "lvl": 2, "value": "Compute", "md": "## Compute"}, {"type": "text", "value": "All of our experiments were done on a computing cluster containing NVIDIA A40 and 2080ti GPUs with tasks split roughly evenly between the two. To compute all of our numbers (averaged over 10 user models) we ended up computing approximately 3490 user models, 160 sets of labels, and 955 expert models. Averaging over GPU architecture, dataset, and model architecture, we note that each set of labels takes around 40 minutes to train. Meanwhile, each expert model takes around 10 minutes to train (fewer epochs with a more costly weight-saving procedure) and each user model takes around 40. This amounts to a total of approximately 2595 GPU-hours.\n\nWe note that the number of GPU-hours for an adversary to pull off this attack is likely significantly lower since they would need to compute as few as a single expert model (10 minutes) and a set of labels (40 minutes). This amounts to just under one GPU-hour given our setup (subject to hardware), a surprisingly low sum for an attack of this high potency.", "md": "All of our experiments were done on a computing cluster containing NVIDIA A40 and 2080ti GPUs with tasks split roughly evenly between the two. To compute all of our numbers (averaged over 10 user models) we ended up computing approximately 3490 user models, 160 sets of labels, and 955 expert models. Averaging over GPU architecture, dataset, and model architecture, we note that each set of labels takes around 40 minutes to train. Meanwhile, each expert model takes around 10 minutes to train (fewer epochs with a more costly weight-saving procedure) and each user model takes around 40. This amounts to a total of approximately 2595 GPU-hours.\n\nWe note that the number of GPU-hours for an adversary to pull off this attack is likely significantly lower since they would need to compute as few as a single expert model (10 minutes) and a set of labels (40 minutes). This amounts to just under one GPU-hour given our setup (subject to hardware), a surprisingly low sum for an attack of this high potency."}]}, {"page": 20, "text": " C         Complete experimental results\n In this section, we provide expanded versions of the key tables and figures in the main text complete\n with standard errors as well as some additional supplementary materials. As in the main text, we\n compute our numbers via a three step process: (1) we start by training 5 sets of synthetic labels for\n each (dataset, expert model architecture, trigger) tuple, (2) we then aggregate each set of labels, and\n (3) we finish by training 10 user models on each interpolation of the aggregated labels and the ground\n truths.\n For our FLIP experiments in Appendix C.1, labels are aggregated as described in Section 2 varying\n the number of flipped labels. Meanwhile, for our softFLIP results in Appendix C.3, we aggregate as\n in Section 4 by taking the average logits for each image and linearly interpolating them on parameter\n \u03b1 with the ground-truth labels.\n C.1         Main results on FLIP\n Fig. 2, Fig. 5a, and Table 1 showcase FLIP\u2019s performance when compared to the inner-product-based\n baseline and in relation to changes in dataset, model architecture, and trigger. We additionally present\n the raw numbers for the dot-product baseline.\n                              0                           150                         300                         500                       1000                      1500\n                  r32   s     92.38 (0.1)    00.1 (0.0) 92.26 (0.1)      12.4 (1.8) 92.09 (0.1)      54.9 (2.4) 91.73 (0.1)    87.2 (1.3) 90.68 (0.1)    99.4 (0.2) 89.87 (0.1)      99.8 (0.1)\n                        t     92.57 (0.1)    00.0 (0.0) 92.37 (0.0)      28.4 (4.9) 92.03 (0.0)      95.3 (1.5) 91.59 (0.1)    99.6 (0.2) 90.80 (0.1)    99.5 (0.3) 89.91 (0.1)      99.9 (0.1)\n    C10                 p     92.52 (0.1)    00.0 (0.0) 92.24 (0.1)      03.3 (0.2) 91.67 (0.0)      06.0 (0.2) 91.24 (0.1)    10.8 (0.3) 90.00 (0.1)    21.2 (0.3) 88.92 (0.1)      29.9 (0.8)\n                  r18   s     94.09 (0.1)    00.0 (0.0) 94.13 (0.1)      13.1 (2.0) 93.94 (0.1)      32.2 (2.6) 93.55 (0.1)    49.0 (3.1) 92.73 (0.1)    81.2 (2.7) 92.17 (0.1)      82.4 (2.6)\n                  vgg   s     93.00 (0.1)    00.0 (0.0) 92.85 (0.1)      02.3 (0.2) 92.48 (0.1)      09.2 (0.7) 92.16 (0.1)    21.4 (0.8) 91.11 (0.2)    48.0 (1.0) 90.44 (0.1)      69.5 (1.6)\n                  r32   s     78.96 (0.1)    00.2 (0.1) 78.83 (0.1)      08.2 (0.6) 78.69 (0.1)      24.7 (1.3) 78.52 (0.1)    45.4 (1.9) 77.61 (0.1)    82.2 (2.5) 76.64 (0.1)      95.2 (0.3)\n    C100          r18   s     82.67 (0.2)    00.1 (0.0) 82.87 (0.1)      11.9 (0.8) 82.48 (0.2)      29.9 (3.1) 81.91 (0.2)    35.8 (3.1) 81.25 (0.1)    81.9 (1.5) 80.28 (0.3)      95.3 (0.5)\n    TI            r18   s     61.61 (0.2)    00.0 (0.0) 61.47 (0.2)      10.6 (0.9) 61.23 (0.1)      31.6 (0.9) 61.25 (0.2)    56.0 (1.4) 61.45 (0.2)    51.8 (2.0) 60.94 (0.1)      57.0 (1.5)\n Table 2: An expanded version of Table 1 in which each point is averaged over 10 user training runs\n and standard errors are shown in parentheses.\n    baselines          500                                1000                               1500                             2500                              5000\n    inner product      91.59 (0.1)   09.2 (0.5)           90.70 (0.1)    17.9 (1.7)          89.76 (0.1)    32.0 (1.6)        87.84 (0.1)    51.3 (1.4)         82.84 (0.1)   94.8 (0.2)\n    Random             91.71 (0.1)   07.7 (0.8)           90.95 (0.2)    16.2 (1.7)          90.04 (0.1)    23.0 (1.3)        87.64 (0.2)    42.9 (1.5)         82.97 (0.0)   94.9 (0.2)\n Table 3: Under the scenario where ytarget = 9, we show raw numbers for the inner product baseline\n and random selection baseline as presented in Fig. 2. The baseline was run on ResNet-32s with\n comparisons to the sinusoidal trigger. Each point is averaged over 10 user training runs and standard\n errors are shown in parentheses.\n    500                         1000                       2500                        5000                        10000                     15000                     20000\n    91.90 (0.1)   01.2 (0.1) 91.31 (0.1)      02.7 (0.2) 88.87 (0.1)      10.5 (0.8) 84.80 (0.1)      30.8 (2.6) 76.09 (0.1)    59.5 (4.5) 66.34 (0.3)    82.8 (1.7) 56.53 (0.3)      91.1 (1.7)\n Table 4: Under the scenario where ytarget is all but target class, we show raw numbers for the inner\n product baseline. The baseline was run on ResNet-32s with comparisons to the sinusoidal trigger.\n Each point is averaged over 10 user training runs and standard errors are shown in parentheses.\n C.2         Robustness of FLIP\nWe provide experimental validations of robustness of FLIP attacks.\n C.2.1           Varying model architecture and optimizer\n The attacker\u2019s strategy in our previous experiments was to train expert models to mimic exactly the\n user architecture and optimizer setup of the user. However, it remained unclear whether the attack\n would generalize if the user, for instance, opted for a smaller model than expected. As such, we\n looked at varying (1) model architecture and (2) optimizer between expert and user setups. For (2)\n                                                                                                20", "md": "## Complete experimental results\n\nIn this section, we provide expanded versions of the key tables and figures in the main text complete with standard errors as well as some additional supplementary materials. As in the main text, we compute our numbers via a three-step process: (1) we start by training 5 sets of synthetic labels for each (dataset, expert model architecture, trigger) tuple, (2) we then aggregate each set of labels, and (3) we finish by training 10 user models on each interpolation of the aggregated labels and the ground truths.\n\nFor our FLIP experiments in Appendix C.1, labels are aggregated as described in Section 2 varying the number of flipped labels. Meanwhile, for our softFLIP results in Appendix C.3, we aggregate as in Section 4 by taking the average logits for each image and linearly interpolating them on parameter \u03b1 with the ground-truth labels.\n\n### Main results on FLIP\n\nFig. 2, Fig. 5a, and Table 1 showcase FLIP\u2019s performance when compared to the inner-product-based baseline and in relation to changes in dataset, model architecture, and trigger. We additionally present the raw numbers for the dot-product baseline.\n\n|              | 0              | 150            | 300            | 500            | 1000           | 1500           |\n|--------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| r32 s        | 92.38 (0.1)    | 00.1 (0.0)     | 92.26 (0.1)    | 12.4 (1.8)     | 92.09 (0.1)    | 54.9 (2.4)     |\n| t            | 92.57 (0.1)    | 00.0 (0.0)     | 92.37 (0.0)    | 28.4 (4.9)     | 92.03 (0.0)    | 95.3 (1.5)     |\n| C10 p        | 92.52 (0.1)    | 00.0 (0.0)     | 92.24 (0.1)    | 03.3 (0.2)     | 91.67 (0.0)    | 06.0 (0.2)     |\n| r18 s        | 94.09 (0.1)    | 00.0 (0.0)     | 94.13 (0.1)    | 13.1 (2.0)     | 93.94 (0.1)    | 32.2 (2.6)     |\n| vgg s        | 93.00 (0.1)    | 00.0 (0.0)     | 92.85 (0.1)    | 02.3 (0.2)     | 92.48 (0.1)    | 09.2 (0.7)     |\n| r32 s        | 78.96 (0.1)    | 00.2 (0.1)     | 78.83 (0.1)    | 08.2 (0.6)     | 78.69 (0.1)    | 24.7 (1.3)     |\n| C100 r18 s   | 82.67 (0.2)    | 00.1 (0.0)     | 82.87 (0.1)    | 11.9 (0.8)     | 82.48 (0.2)    | 29.9 (3.1)     |\n| TI r18 s     | 61.61 (0.2)    | 00.0 (0.0)     | 61.47 (0.2)    | 10.6 (0.9)     | 61.23 (0.1)    | 31.6 (0.9)     |\n\nTable 2: An expanded version of Table 1 in which each point is averaged over 10 user training runs and standard errors are shown in parentheses.\n\n| baselines     | 500            | 1000           | 1500           | 2500           | 5000           |\n|---------------|----------------|----------------|----------------|----------------|----------------|\n| inner product | 91.59 (0.1)    | 09.2 (0.5)     | 90.70 (0.1)    | 17.9 (1.7)     | 89.76 (0.1)    |\n| Random        | 91.71 (0.1)    | 07.7 (0.8)     | 90.95 (0.2)    | 16.2 (1.7)     | 90.04 (0.1)    |\n\nTable 3: Under the scenario where ytarget = 9, we show raw numbers for the inner product baseline and random selection baseline as presented in Fig. 2. The baseline was run on ResNet-32s with comparisons to the sinusoidal trigger. Each point is averaged over 10 user training runs and standard errors are shown in parentheses.\n\n|              | 500            | 1000           | 2500           | 5000           | 10000          | 15000          | 20000          |\n|--------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| 91.90 (0.1)  | 01.2 (0.1)     | 91.31 (0.1)    | 02.7 (0.2)     | 88.87 (0.1)    | 10.5 (0.8)     | 84.80 (0.1)    | 30.8 (2.6)     |\n\nTable 4: Under the scenario where ytarget is all but the target class, we show raw numbers for the inner product baseline. The baseline was run on ResNet-32s with comparisons to the sinusoidal trigger. Each point is averaged over 10 user training runs and standard errors are shown in parentheses.\n\n### Robustness of FLIP\n\nWe provide experimental validations of the robustness of FLIP attacks.\n\n#### Varying model architecture and optimizer\n\nThe attacker\u2019s strategy in our previous experiments was to train expert models to mimic exactly the user architecture and optimizer setup of the user. However, it remained unclear whether the attack would generalize if the user, for instance, opted for a smaller model than expected. As such, we looked at varying (1) model architecture and (2) optimizer between expert and user setups. For (2)", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Complete experimental results", "md": "## Complete experimental results"}, {"type": "text", "value": "In this section, we provide expanded versions of the key tables and figures in the main text complete with standard errors as well as some additional supplementary materials. As in the main text, we compute our numbers via a three-step process: (1) we start by training 5 sets of synthetic labels for each (dataset, expert model architecture, trigger) tuple, (2) we then aggregate each set of labels, and (3) we finish by training 10 user models on each interpolation of the aggregated labels and the ground truths.\n\nFor our FLIP experiments in Appendix C.1, labels are aggregated as described in Section 2 varying the number of flipped labels. Meanwhile, for our softFLIP results in Appendix C.3, we aggregate as in Section 4 by taking the average logits for each image and linearly interpolating them on parameter \u03b1 with the ground-truth labels.", "md": "In this section, we provide expanded versions of the key tables and figures in the main text complete with standard errors as well as some additional supplementary materials. As in the main text, we compute our numbers via a three-step process: (1) we start by training 5 sets of synthetic labels for each (dataset, expert model architecture, trigger) tuple, (2) we then aggregate each set of labels, and (3) we finish by training 10 user models on each interpolation of the aggregated labels and the ground truths.\n\nFor our FLIP experiments in Appendix C.1, labels are aggregated as described in Section 2 varying the number of flipped labels. Meanwhile, for our softFLIP results in Appendix C.3, we aggregate as in Section 4 by taking the average logits for each image and linearly interpolating them on parameter \u03b1 with the ground-truth labels."}, {"type": "heading", "lvl": 3, "value": "Main results on FLIP", "md": "### Main results on FLIP"}, {"type": "text", "value": "Fig. 2, Fig. 5a, and Table 1 showcase FLIP\u2019s performance when compared to the inner-product-based baseline and in relation to changes in dataset, model architecture, and trigger. We additionally present the raw numbers for the dot-product baseline.", "md": "Fig. 2, Fig. 5a, and Table 1 showcase FLIP\u2019s performance when compared to the inner-product-based baseline and in relation to changes in dataset, model architecture, and trigger. We additionally present the raw numbers for the dot-product baseline."}, {"type": "table", "rows": [["", "0", "150", "300", "500", "1000", "1500"], ["r32 s", "92.38 (0.1)", "00.1 (0.0)", "92.26 (0.1)", "12.4 (1.8)", "92.09 (0.1)", "54.9 (2.4)"], ["t", "92.57 (0.1)", "00.0 (0.0)", "92.37 (0.0)", "28.4 (4.9)", "92.03 (0.0)", "95.3 (1.5)"], ["C10 p", "92.52 (0.1)", "00.0 (0.0)", "92.24 (0.1)", "03.3 (0.2)", "91.67 (0.0)", "06.0 (0.2)"], ["r18 s", "94.09 (0.1)", "00.0 (0.0)", "94.13 (0.1)", "13.1 (2.0)", "93.94 (0.1)", "32.2 (2.6)"], ["vgg s", "93.00 (0.1)", "00.0 (0.0)", "92.85 (0.1)", "02.3 (0.2)", "92.48 (0.1)", "09.2 (0.7)"], ["r32 s", "78.96 (0.1)", "00.2 (0.1)", "78.83 (0.1)", "08.2 (0.6)", "78.69 (0.1)", "24.7 (1.3)"], ["C100 r18 s", "82.67 (0.2)", "00.1 (0.0)", "82.87 (0.1)", "11.9 (0.8)", "82.48 (0.2)", "29.9 (3.1)"], ["TI r18 s", "61.61 (0.2)", "00.0 (0.0)", "61.47 (0.2)", "10.6 (0.9)", "61.23 (0.1)", "31.6 (0.9)"]], "md": "|              | 0              | 150            | 300            | 500            | 1000           | 1500           |\n|--------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| r32 s        | 92.38 (0.1)    | 00.1 (0.0)     | 92.26 (0.1)    | 12.4 (1.8)     | 92.09 (0.1)    | 54.9 (2.4)     |\n| t            | 92.57 (0.1)    | 00.0 (0.0)     | 92.37 (0.0)    | 28.4 (4.9)     | 92.03 (0.0)    | 95.3 (1.5)     |\n| C10 p        | 92.52 (0.1)    | 00.0 (0.0)     | 92.24 (0.1)    | 03.3 (0.2)     | 91.67 (0.0)    | 06.0 (0.2)     |\n| r18 s        | 94.09 (0.1)    | 00.0 (0.0)     | 94.13 (0.1)    | 13.1 (2.0)     | 93.94 (0.1)    | 32.2 (2.6)     |\n| vgg s        | 93.00 (0.1)    | 00.0 (0.0)     | 92.85 (0.1)    | 02.3 (0.2)     | 92.48 (0.1)    | 09.2 (0.7)     |\n| r32 s        | 78.96 (0.1)    | 00.2 (0.1)     | 78.83 (0.1)    | 08.2 (0.6)     | 78.69 (0.1)    | 24.7 (1.3)     |\n| C100 r18 s   | 82.67 (0.2)    | 00.1 (0.0)     | 82.87 (0.1)    | 11.9 (0.8)     | 82.48 (0.2)    | 29.9 (3.1)     |\n| TI r18 s     | 61.61 (0.2)    | 00.0 (0.0)     | 61.47 (0.2)    | 10.6 (0.9)     | 61.23 (0.1)    | 31.6 (0.9)     |", "isPerfectTable": true, "csv": "\"\",\"0\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"r32 s\",\"92.38 (0.1)\",\"00.1 (0.0)\",\"92.26 (0.1)\",\"12.4 (1.8)\",\"92.09 (0.1)\",\"54.9 (2.4)\"\n\"t\",\"92.57 (0.1)\",\"00.0 (0.0)\",\"92.37 (0.0)\",\"28.4 (4.9)\",\"92.03 (0.0)\",\"95.3 (1.5)\"\n\"C10 p\",\"92.52 (0.1)\",\"00.0 (0.0)\",\"92.24 (0.1)\",\"03.3 (0.2)\",\"91.67 (0.0)\",\"06.0 (0.2)\"\n\"r18 s\",\"94.09 (0.1)\",\"00.0 (0.0)\",\"94.13 (0.1)\",\"13.1 (2.0)\",\"93.94 (0.1)\",\"32.2 (2.6)\"\n\"vgg s\",\"93.00 (0.1)\",\"00.0 (0.0)\",\"92.85 (0.1)\",\"02.3 (0.2)\",\"92.48 (0.1)\",\"09.2 (0.7)\"\n\"r32 s\",\"78.96 (0.1)\",\"00.2 (0.1)\",\"78.83 (0.1)\",\"08.2 (0.6)\",\"78.69 (0.1)\",\"24.7 (1.3)\"\n\"C100 r18 s\",\"82.67 (0.2)\",\"00.1 (0.0)\",\"82.87 (0.1)\",\"11.9 (0.8)\",\"82.48 (0.2)\",\"29.9 (3.1)\"\n\"TI r18 s\",\"61.61 (0.2)\",\"00.0 (0.0)\",\"61.47 (0.2)\",\"10.6 (0.9)\",\"61.23 (0.1)\",\"31.6 (0.9)\""}, {"type": "text", "value": "Table 2: An expanded version of Table 1 in which each point is averaged over 10 user training runs and standard errors are shown in parentheses.", "md": "Table 2: An expanded version of Table 1 in which each point is averaged over 10 user training runs and standard errors are shown in parentheses."}, {"type": "table", "rows": [["baselines", "500", "1000", "1500", "2500", "5000"], ["inner product", "91.59 (0.1)", "09.2 (0.5)", "90.70 (0.1)", "17.9 (1.7)", "89.76 (0.1)"], ["Random", "91.71 (0.1)", "07.7 (0.8)", "90.95 (0.2)", "16.2 (1.7)", "90.04 (0.1)"]], "md": "| baselines     | 500            | 1000           | 1500           | 2500           | 5000           |\n|---------------|----------------|----------------|----------------|----------------|----------------|\n| inner product | 91.59 (0.1)    | 09.2 (0.5)     | 90.70 (0.1)    | 17.9 (1.7)     | 89.76 (0.1)    |\n| Random        | 91.71 (0.1)    | 07.7 (0.8)     | 90.95 (0.2)    | 16.2 (1.7)     | 90.04 (0.1)    |", "isPerfectTable": true, "csv": "\"baselines\",\"500\",\"1000\",\"1500\",\"2500\",\"5000\"\n\"inner product\",\"91.59 (0.1)\",\"09.2 (0.5)\",\"90.70 (0.1)\",\"17.9 (1.7)\",\"89.76 (0.1)\"\n\"Random\",\"91.71 (0.1)\",\"07.7 (0.8)\",\"90.95 (0.2)\",\"16.2 (1.7)\",\"90.04 (0.1)\""}, {"type": "text", "value": "Table 3: Under the scenario where ytarget = 9, we show raw numbers for the inner product baseline and random selection baseline as presented in Fig. 2. The baseline was run on ResNet-32s with comparisons to the sinusoidal trigger. Each point is averaged over 10 user training runs and standard errors are shown in parentheses.", "md": "Table 3: Under the scenario where ytarget = 9, we show raw numbers for the inner product baseline and random selection baseline as presented in Fig. 2. The baseline was run on ResNet-32s with comparisons to the sinusoidal trigger. Each point is averaged over 10 user training runs and standard errors are shown in parentheses."}, {"type": "table", "rows": [["", "500", "1000", "2500", "5000", "10000", "15000", "20000"], ["91.90 (0.1)", "01.2 (0.1)", "91.31 (0.1)", "02.7 (0.2)", "88.87 (0.1)", "10.5 (0.8)", "84.80 (0.1)", "30.8 (2.6)"]], "md": "|              | 500            | 1000           | 2500           | 5000           | 10000          | 15000          | 20000          |\n|--------------|----------------|----------------|----------------|----------------|----------------|----------------|----------------|\n| 91.90 (0.1)  | 01.2 (0.1)     | 91.31 (0.1)    | 02.7 (0.2)     | 88.87 (0.1)    | 10.5 (0.8)     | 84.80 (0.1)    | 30.8 (2.6)     |", "isPerfectTable": true, "csv": "\"\",\"500\",\"1000\",\"2500\",\"5000\",\"10000\",\"15000\",\"20000\"\n\"91.90 (0.1)\",\"01.2 (0.1)\",\"91.31 (0.1)\",\"02.7 (0.2)\",\"88.87 (0.1)\",\"10.5 (0.8)\",\"84.80 (0.1)\",\"30.8 (2.6)\""}, {"type": "text", "value": "Table 4: Under the scenario where ytarget is all but the target class, we show raw numbers for the inner product baseline. The baseline was run on ResNet-32s with comparisons to the sinusoidal trigger. Each point is averaged over 10 user training runs and standard errors are shown in parentheses.", "md": "Table 4: Under the scenario where ytarget is all but the target class, we show raw numbers for the inner product baseline. The baseline was run on ResNet-32s with comparisons to the sinusoidal trigger. Each point is averaged over 10 user training runs and standard errors are shown in parentheses."}, {"type": "heading", "lvl": 3, "value": "Robustness of FLIP", "md": "### Robustness of FLIP"}, {"type": "text", "value": "We provide experimental validations of the robustness of FLIP attacks.", "md": "We provide experimental validations of the robustness of FLIP attacks."}, {"type": "heading", "lvl": 4, "value": "Varying model architecture and optimizer", "md": "#### Varying model architecture and optimizer"}, {"type": "text", "value": "The attacker\u2019s strategy in our previous experiments was to train expert models to mimic exactly the user architecture and optimizer setup of the user. However, it remained unclear whether the attack would generalize if the user, for instance, opted for a smaller model than expected. As such, we looked at varying (1) model architecture and (2) optimizer between expert and user setups. For (2)", "md": "The attacker\u2019s strategy in our previous experiments was to train expert models to mimic exactly the user architecture and optimizer setup of the user. However, it remained unclear whether the attack would generalize if the user, for instance, opted for a smaller model than expected. As such, we looked at varying (1) model architecture and (2) optimizer between expert and user setups. For (2)"}]}, {"page": 21, "text": "we use SGD for the expert models and Adam [46] for the user. We additionally analyze what happens\nwhen both are different.\nAs Table 5 indicates, the attack still performs well when information is limited. When varying\noptimizer, we found that CTA dropped, but, interestingly, the PTA for the ResNet-18 case was almost\nuniformly higher. We found a similar trend for upstream ResNet-32s and downstream ResNet-18s\nwhen varying model architecture. Surprisingly, the strongest PTA numbers for budgets higher than\n150 across all experiments with a downstream ResNet-18 were achieved when the attacker used a\ndifferent expert model and optimizer. The FLIP attack is robust to varied architecture and optimizer.\n                                                150                        300                        500                       1000                       1500\n         (1)           r18 \u2192        r32         92.44        19.2          92.16        53.3          91.84        74.5         90.80         92.9         90.00        95.2\n                       r32 \u2192        r18         93.86        04.8          93.89        36.0          93.56        60.0         92.76         86.9         91.75        98.0\n         (2)           r32 \u2192        r32         90.79        11.8          90.50        43.2          90.08        80.6         89.45         97.5         88.33        99.0\n                       r18 \u2192        r18         93.17        20.3          93.08        47.0          92.94        65.6         91.91         89.9         91.16        93.1\n     (1 + 2)           r18 \u2192        r32         90.86        12.3          90.57        40.9          90.28        59.7         89.39         83.0         88.59        89.2\n                       r32 \u2192        r18         93.32        09.4          93.05        52.9          92.70        85.3         91.72         99.2         90.93        99.7\nTable 5: FLIP performs well even when the expert and user (1) model architectures and (2) optimizers\nare different. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each row denotes\nthe CTA/PTA pairs averaged over 10 experiments. The second column is structured as follows: expert\n\u2192     user.\nTable 5 investigates whether an attacker needs to know the architecture or optimizer of the user\u2019s\nmodel. The experiments are done in the same style as Appendix C.1.\n                             150                           300                           500                            1000                          1500\n   r18 \u2192  r32                92.44 (0.1)  19.2 (1.3)       92.16 (0.1)   53.3 (3.0)      91.84 (0.0)   74.5 (2.2)       90.80 (0.1)   92.9 (0.8)      90.00 (0.1)   95.2 (0.6)\n   r32 \u2192  r32                92.26 (0.1)  12.4 (1.8)       92.09 (0.1)   54.9 (2.4)      91.73 (0.1)   87.2 (1.3)       90.68 (0.1)   99.4 (0.2)      89.87 (0.1)   99.8 (0.1)\n   r32 \u2192  r18                93.86 (0.1)  04.8 (0.8)       93.89 (0.1)   36.0 (5.4)      93.56 (0.1)   60.0 (6.6)       92.76 (0.1)   86.9 (2.6)      91.75 (0.1)   98.0 (0.8)\n   r18 \u2192  r18                94.13 (0.1)  13.1 (2.0)       93.94 (0.1)   32.2 (2.6)      93.55 (0.1)   49.0 (3.1)       92.73 (0.1)   81.2 (2.7)      92.17 (0.1)   82.4 (2.6)\n   r32 \u2192  vgg                92.76 (0.1)  02.7 (0.1)       92.67 (0.0)   10.0 (0.4)      92.28 (0.1)   23.1 (1.2)       91.41 (0.1)   47.5 (1.0)      90.63 (0.1)   63.0 (1.5)\n   vgg \u2192   vgg               92.85 (0.1)  02.3 (0.2)       92.48 (0.1)   09.2 (0.7)      92.16 (0.1)   21.4 (0.8)       91.11 (0.2)   48.0 (1.0)      90.44 (0.1)   69.5 (1.6)\nTable 6: An expanded version of Table 5 (1) in which each point is averaged over 10 runs and\nstandard errors are shown in parentheses. We additionally compare the performance directly to the\nnon-model-mixed case.\nTable 6 shows more experimental results with mismatched architectures between the attacker and the\nuser. Attacks on VGG need more corrupted examples to achieve successful backdoor attack.\n              150                               300                                500                               1000                              1500\n   r32   s    90.79 (0.1)   11.8 (1.6)          90.50 (0.1)   43.2 (3.8)           90.08 (0.1)   80.6 (2.2)          89.45 (0.1)   97.5 (0.3)          88.33 (0.1)   99.0 (0.3)\n         t    90.77 (0.1)   08.4 (1.3)          90.46 (0.1)   65.0 (6.8)           90.00 (0.1)   72.7 (5.7)          89.07 (0.1)   98.2 (1.1)          88.23 (0.1)   95.8 (2.5)\n         p    90.60 (0.0)   03.0 (0.3)          90.21 (0.1)   05.5 (0.3)           89.61 (0.1)   11.1 (0.7)          88.55 (0.1)   19.5 (0.6)          87.39 (0.1)   31.6 (0.8)\n   r18   s    93.17 (0.1)   20.3 (2.2)          93.08 (0.1)   47.0 (2.6)           92.94 (0.0)   65.6 (1.6)          91.91 (0.1)   89.9 (1.0)          91.16 (0.1)   93.1 (0.7)\nTable 7: An expanded version of Table 5 (2) in which each point is averaged over 10 runs and\nstandard errors are shown in parentheses. We additionally evaluate different choices of trigger with\nResNet-32s.\n                   150                             300                             500                             1000                            1500\n   r32 \u2192  r18      93.32 (0.1)   09.4 (1.5)        93.05 (0.1)   52.9 (3.0)        92.70 (0.1)   85.3 (1.7)        91.72 (0.1)   99.2 (0.2)        90.93 (0.1)   99.7 (0.1)\n   r18 \u2192  r32      90.86 (0.1)   12.3 (1.3)        90.57 (0.1)   40.9 (3.3)        90.28 (0.1)   59.7 (2.6)        89.39 (0.1)   83.0 (1.7)        88.59 (0.1)   89.2 (0.7)\nTable 8: An expanded version of Table 5 (1+2) in which each point is averaged over 10 runs and\nstandard errors are shown in parentheses.\n                                                                                         21", "md": "# FLIP Attack Analysis\n\n## FLIP Attack Analysis\n\nWe use SGD for the expert models and Adam [46] for the user. We additionally analyze what happens when both are different.\n\nAs Table 5 indicates, the attack still performs well when information is limited. When varying optimizer, we found that CTA dropped, but, interestingly, the PTA for the ResNet-18 case was almost uniformly higher. We found a similar trend for upstream ResNet-32s and downstream ResNet-18s when varying model architecture. Surprisingly, the strongest PTA numbers for budgets higher than 150 across all experiments with a downstream ResNet-18 were achieved when the attacker used a different expert model and optimizer. The FLIP attack is robust to varied architecture and optimizer.\n\n| |150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|(1) r18 \u2192 r32|92.44|19.2|92.16|53.3|91.84|74.5|90.80|92.9|90.00|95.2|\n|(2) r32 \u2192 r18|93.86|04.8|93.89|36.0|93.56|60.0|92.76|86.9|91.75|98.0|\n|(1 + 2) r18 \u2192 r32|90.86|12.3|90.57|40.9|90.28|59.7|89.39|83.0|88.59|89.2|\n\nTable 5: FLIP performs well even when the expert and user (1) model architectures and (2) optimizers are different. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each row denotes the CTA/PTA pairs averaged over 10 experiments. The second column is structured as follows: expert \u2192 user.\n\nTable 5 investigates whether an attacker needs to know the architecture or optimizer of the user\u2019s model. The experiments are done in the same style as Appendix C.1.\n\n| |150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|r18 \u2192 r32|92.44 (0.1)|19.2 (1.3)|92.16 (0.1)|53.3 (3.0)|91.84 (0.0)|74.5 (2.2)|90.80 (0.1)|92.9 (0.8)|90.00 (0.1)|95.2 (0.6)|\n|r32 \u2192 r32|92.26 (0.1)|12.4 (1.8)|92.09 (0.1)|54.9 (2.4)|91.73 (0.1)|87.2 (1.3)|90.68 (0.1)|99.4 (0.2)|89.87 (0.1)|99.8 (0.1)|\n|r32 \u2192 r18|93.86 (0.1)|04.8 (0.8)|93.89 (0.1)|36.0 (5.4)|93.56 (0.1)|60.0 (6.6)|92.76 (0.1)|86.9 (2.6)|91.75 (0.1)|98.0 (0.8)|\n|r18 \u2192 r18|94.13 (0.1)|13.1 (2.0)|93.94 (0.1)|32.2 (2.6)|93.55 (0.1)|49.0 (3.1)|92.73 (0.1)|81.2 (2.7)|92.17 (0.1)|82.4 (2.6)|\n|r32 \u2192 vgg|92.76 (0.1)|02.7 (0.1)|92.67 (0.0)|10.0 (0.4)|92.28 (0.1)|23.1 (1.2)|91.41 (0.1)|47.5 (1.0)|90.63 (0.1)|63.0 (1.5)|\n|vgg \u2192 vgg|92.85 (0.1)|02.3 (0.2)|92.48 (0.1)|09.2 (0.7)|92.16 (0.1)|21.4 (0.8)|91.11 (0.2)|48.0 (1.0)|90.44 (0.1)|69.5 (1.6)|\n\nTable 6: An expanded version of Table 5 (1) in which each point is averaged over 10 runs and standard errors are shown in parentheses. We additionally compare the performance directly to the non-model-mixed case.\n\nTable 6 shows more experimental results with mismatched architectures between the attacker and the user. Attacks on VGG need more corrupted examples to achieve successful backdoor attack.\n\n| |150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|r32 s|90.79 (0.1)|11.8 (1.6)|90.50 (0.1)|43.2 (3.8)|90.08 (0.1)|80.6 (2.2)|89.45 (0.1)|97.5 (0.3)|88.33 (0.1)|99.0 (0.3)|\n|r32 t|90.77 (0.1)|08.4 (1.3)|90.46 (0.1)|65.0 (6.8)|90.00 (0.1)|72.7 (5.7)|89.07 (0.1)|98.2 (1.1)|88.23 (0.1)|95.8 (2.5)|\n|r32 p|90.60 (0.0)|03.0 (0.3)|90.21 (0.1)|05.5 (0.3)|89.61 (0.1)|11.1 (0.7)|88.55 (0.1)|19.5 (0.6)|87.39 (0.1)|31.6 (0.8)|\n|r18 s|93.17 (0.1)|20.3 (2.2)|93.08 (0.1)|47.0 (2.6)|92.94 (0.0)|65.6 (1.6)|91.91 (0.1)|89.9 (1.0)|91.16 (0.1)|93.1 (0.7)|\n\nTable 7: An expanded version of Table 5 (2) in which each point is averaged over 10 runs and standard errors are shown in parentheses. We additionally evaluate different choices of trigger with ResNet-32s.\n\n| |150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|r32 \u2192 r18|93.32 (0.1)|09.4 (1.5)|93.05 (0.1)|52.9 (3.0)|92.70 (0.1)|85.3 (1.7)|91.72 (0.1)|99.2 (0.2)|90.93 (0.1)|99.7 (0.1)|\n|r18 \u2192 r32|90.86 (0.1)|12.3 (1.3)|90.57 (0.1)|40.9 (3.3)|90.28 (0.1)|59.7 (2.6)|89.39 (0.1)|83.0 (1.7)|88.59 (0.1)|89.2 (0.7)|\n\nTable 8: An expanded version of Table 5 (1+2) in which each point is averaged over 10 runs and standard errors are shown in parentheses.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "FLIP Attack Analysis", "md": "# FLIP Attack Analysis"}, {"type": "heading", "lvl": 2, "value": "FLIP Attack Analysis", "md": "## FLIP Attack Analysis"}, {"type": "text", "value": "We use SGD for the expert models and Adam [46] for the user. We additionally analyze what happens when both are different.\n\nAs Table 5 indicates, the attack still performs well when information is limited. When varying optimizer, we found that CTA dropped, but, interestingly, the PTA for the ResNet-18 case was almost uniformly higher. We found a similar trend for upstream ResNet-32s and downstream ResNet-18s when varying model architecture. Surprisingly, the strongest PTA numbers for budgets higher than 150 across all experiments with a downstream ResNet-18 were achieved when the attacker used a different expert model and optimizer. The FLIP attack is robust to varied architecture and optimizer.", "md": "We use SGD for the expert models and Adam [46] for the user. We additionally analyze what happens when both are different.\n\nAs Table 5 indicates, the attack still performs well when information is limited. When varying optimizer, we found that CTA dropped, but, interestingly, the PTA for the ResNet-18 case was almost uniformly higher. We found a similar trend for upstream ResNet-32s and downstream ResNet-18s when varying model architecture. Surprisingly, the strongest PTA numbers for budgets higher than 150 across all experiments with a downstream ResNet-18 were achieved when the attacker used a different expert model and optimizer. The FLIP attack is robust to varied architecture and optimizer."}, {"type": "table", "rows": [["", "150", "300", "500", "1000", "1500"], ["(1) r18 \u2192 r32", "92.44", "19.2", "92.16", "53.3", "91.84", "74.5", "90.80", "92.9", "90.00", "95.2"], ["(2) r32 \u2192 r18", "93.86", "04.8", "93.89", "36.0", "93.56", "60.0", "92.76", "86.9", "91.75", "98.0"], ["(1 + 2) r18 \u2192 r32", "90.86", "12.3", "90.57", "40.9", "90.28", "59.7", "89.39", "83.0", "88.59", "89.2"]], "md": "| |150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|(1) r18 \u2192 r32|92.44|19.2|92.16|53.3|91.84|74.5|90.80|92.9|90.00|95.2|\n|(2) r32 \u2192 r18|93.86|04.8|93.89|36.0|93.56|60.0|92.76|86.9|91.75|98.0|\n|(1 + 2) r18 \u2192 r32|90.86|12.3|90.57|40.9|90.28|59.7|89.39|83.0|88.59|89.2|", "isPerfectTable": false, "csv": "\"\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"(1) r18 \u2192 r32\",\"92.44\",\"19.2\",\"92.16\",\"53.3\",\"91.84\",\"74.5\",\"90.80\",\"92.9\",\"90.00\",\"95.2\"\n\"(2) r32 \u2192 r18\",\"93.86\",\"04.8\",\"93.89\",\"36.0\",\"93.56\",\"60.0\",\"92.76\",\"86.9\",\"91.75\",\"98.0\"\n\"(1 + 2) r18 \u2192 r32\",\"90.86\",\"12.3\",\"90.57\",\"40.9\",\"90.28\",\"59.7\",\"89.39\",\"83.0\",\"88.59\",\"89.2\""}, {"type": "text", "value": "Table 5: FLIP performs well even when the expert and user (1) model architectures and (2) optimizers are different. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each row denotes the CTA/PTA pairs averaged over 10 experiments. The second column is structured as follows: expert \u2192 user.\n\nTable 5 investigates whether an attacker needs to know the architecture or optimizer of the user\u2019s model. The experiments are done in the same style as Appendix C.1.", "md": "Table 5: FLIP performs well even when the expert and user (1) model architectures and (2) optimizers are different. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each row denotes the CTA/PTA pairs averaged over 10 experiments. The second column is structured as follows: expert \u2192 user.\n\nTable 5 investigates whether an attacker needs to know the architecture or optimizer of the user\u2019s model. The experiments are done in the same style as Appendix C.1."}, {"type": "table", "rows": [["", "150", "300", "500", "1000", "1500"], ["r18 \u2192 r32", "92.44 (0.1)", "19.2 (1.3)", "92.16 (0.1)", "53.3 (3.0)", "91.84 (0.0)", "74.5 (2.2)", "90.80 (0.1)", "92.9 (0.8)", "90.00 (0.1)", "95.2 (0.6)"], ["r32 \u2192 r32", "92.26 (0.1)", "12.4 (1.8)", "92.09 (0.1)", "54.9 (2.4)", "91.73 (0.1)", "87.2 (1.3)", "90.68 (0.1)", "99.4 (0.2)", "89.87 (0.1)", "99.8 (0.1)"], ["r32 \u2192 r18", "93.86 (0.1)", "04.8 (0.8)", "93.89 (0.1)", "36.0 (5.4)", "93.56 (0.1)", "60.0 (6.6)", "92.76 (0.1)", "86.9 (2.6)", "91.75 (0.1)", "98.0 (0.8)"], ["r18 \u2192 r18", "94.13 (0.1)", "13.1 (2.0)", "93.94 (0.1)", "32.2 (2.6)", "93.55 (0.1)", "49.0 (3.1)", "92.73 (0.1)", "81.2 (2.7)", "92.17 (0.1)", "82.4 (2.6)"], ["r32 \u2192 vgg", "92.76 (0.1)", "02.7 (0.1)", "92.67 (0.0)", "10.0 (0.4)", "92.28 (0.1)", "23.1 (1.2)", "91.41 (0.1)", "47.5 (1.0)", "90.63 (0.1)", "63.0 (1.5)"], ["vgg \u2192 vgg", "92.85 (0.1)", "02.3 (0.2)", "92.48 (0.1)", "09.2 (0.7)", "92.16 (0.1)", "21.4 (0.8)", "91.11 (0.2)", "48.0 (1.0)", "90.44 (0.1)", "69.5 (1.6)"]], "md": "| |150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|r18 \u2192 r32|92.44 (0.1)|19.2 (1.3)|92.16 (0.1)|53.3 (3.0)|91.84 (0.0)|74.5 (2.2)|90.80 (0.1)|92.9 (0.8)|90.00 (0.1)|95.2 (0.6)|\n|r32 \u2192 r32|92.26 (0.1)|12.4 (1.8)|92.09 (0.1)|54.9 (2.4)|91.73 (0.1)|87.2 (1.3)|90.68 (0.1)|99.4 (0.2)|89.87 (0.1)|99.8 (0.1)|\n|r32 \u2192 r18|93.86 (0.1)|04.8 (0.8)|93.89 (0.1)|36.0 (5.4)|93.56 (0.1)|60.0 (6.6)|92.76 (0.1)|86.9 (2.6)|91.75 (0.1)|98.0 (0.8)|\n|r18 \u2192 r18|94.13 (0.1)|13.1 (2.0)|93.94 (0.1)|32.2 (2.6)|93.55 (0.1)|49.0 (3.1)|92.73 (0.1)|81.2 (2.7)|92.17 (0.1)|82.4 (2.6)|\n|r32 \u2192 vgg|92.76 (0.1)|02.7 (0.1)|92.67 (0.0)|10.0 (0.4)|92.28 (0.1)|23.1 (1.2)|91.41 (0.1)|47.5 (1.0)|90.63 (0.1)|63.0 (1.5)|\n|vgg \u2192 vgg|92.85 (0.1)|02.3 (0.2)|92.48 (0.1)|09.2 (0.7)|92.16 (0.1)|21.4 (0.8)|91.11 (0.2)|48.0 (1.0)|90.44 (0.1)|69.5 (1.6)|", "isPerfectTable": false, "csv": "\"\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"r18 \u2192 r32\",\"92.44 (0.1)\",\"19.2 (1.3)\",\"92.16 (0.1)\",\"53.3 (3.0)\",\"91.84 (0.0)\",\"74.5 (2.2)\",\"90.80 (0.1)\",\"92.9 (0.8)\",\"90.00 (0.1)\",\"95.2 (0.6)\"\n\"r32 \u2192 r32\",\"92.26 (0.1)\",\"12.4 (1.8)\",\"92.09 (0.1)\",\"54.9 (2.4)\",\"91.73 (0.1)\",\"87.2 (1.3)\",\"90.68 (0.1)\",\"99.4 (0.2)\",\"89.87 (0.1)\",\"99.8 (0.1)\"\n\"r32 \u2192 r18\",\"93.86 (0.1)\",\"04.8 (0.8)\",\"93.89 (0.1)\",\"36.0 (5.4)\",\"93.56 (0.1)\",\"60.0 (6.6)\",\"92.76 (0.1)\",\"86.9 (2.6)\",\"91.75 (0.1)\",\"98.0 (0.8)\"\n\"r18 \u2192 r18\",\"94.13 (0.1)\",\"13.1 (2.0)\",\"93.94 (0.1)\",\"32.2 (2.6)\",\"93.55 (0.1)\",\"49.0 (3.1)\",\"92.73 (0.1)\",\"81.2 (2.7)\",\"92.17 (0.1)\",\"82.4 (2.6)\"\n\"r32 \u2192 vgg\",\"92.76 (0.1)\",\"02.7 (0.1)\",\"92.67 (0.0)\",\"10.0 (0.4)\",\"92.28 (0.1)\",\"23.1 (1.2)\",\"91.41 (0.1)\",\"47.5 (1.0)\",\"90.63 (0.1)\",\"63.0 (1.5)\"\n\"vgg \u2192 vgg\",\"92.85 (0.1)\",\"02.3 (0.2)\",\"92.48 (0.1)\",\"09.2 (0.7)\",\"92.16 (0.1)\",\"21.4 (0.8)\",\"91.11 (0.2)\",\"48.0 (1.0)\",\"90.44 (0.1)\",\"69.5 (1.6)\""}, {"type": "text", "value": "Table 6: An expanded version of Table 5 (1) in which each point is averaged over 10 runs and standard errors are shown in parentheses. We additionally compare the performance directly to the non-model-mixed case.\n\nTable 6 shows more experimental results with mismatched architectures between the attacker and the user. Attacks on VGG need more corrupted examples to achieve successful backdoor attack.", "md": "Table 6: An expanded version of Table 5 (1) in which each point is averaged over 10 runs and standard errors are shown in parentheses. We additionally compare the performance directly to the non-model-mixed case.\n\nTable 6 shows more experimental results with mismatched architectures between the attacker and the user. Attacks on VGG need more corrupted examples to achieve successful backdoor attack."}, {"type": "table", "rows": [["", "150", "300", "500", "1000", "1500"], ["r32 s", "90.79 (0.1)", "11.8 (1.6)", "90.50 (0.1)", "43.2 (3.8)", "90.08 (0.1)", "80.6 (2.2)", "89.45 (0.1)", "97.5 (0.3)", "88.33 (0.1)", "99.0 (0.3)"], ["r32 t", "90.77 (0.1)", "08.4 (1.3)", "90.46 (0.1)", "65.0 (6.8)", "90.00 (0.1)", "72.7 (5.7)", "89.07 (0.1)", "98.2 (1.1)", "88.23 (0.1)", "95.8 (2.5)"], ["r32 p", "90.60 (0.0)", "03.0 (0.3)", "90.21 (0.1)", "05.5 (0.3)", "89.61 (0.1)", "11.1 (0.7)", "88.55 (0.1)", "19.5 (0.6)", "87.39 (0.1)", "31.6 (0.8)"], ["r18 s", "93.17 (0.1)", "20.3 (2.2)", "93.08 (0.1)", "47.0 (2.6)", "92.94 (0.0)", "65.6 (1.6)", "91.91 (0.1)", "89.9 (1.0)", "91.16 (0.1)", "93.1 (0.7)"]], "md": "| |150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|r32 s|90.79 (0.1)|11.8 (1.6)|90.50 (0.1)|43.2 (3.8)|90.08 (0.1)|80.6 (2.2)|89.45 (0.1)|97.5 (0.3)|88.33 (0.1)|99.0 (0.3)|\n|r32 t|90.77 (0.1)|08.4 (1.3)|90.46 (0.1)|65.0 (6.8)|90.00 (0.1)|72.7 (5.7)|89.07 (0.1)|98.2 (1.1)|88.23 (0.1)|95.8 (2.5)|\n|r32 p|90.60 (0.0)|03.0 (0.3)|90.21 (0.1)|05.5 (0.3)|89.61 (0.1)|11.1 (0.7)|88.55 (0.1)|19.5 (0.6)|87.39 (0.1)|31.6 (0.8)|\n|r18 s|93.17 (0.1)|20.3 (2.2)|93.08 (0.1)|47.0 (2.6)|92.94 (0.0)|65.6 (1.6)|91.91 (0.1)|89.9 (1.0)|91.16 (0.1)|93.1 (0.7)|", "isPerfectTable": false, "csv": "\"\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"r32 s\",\"90.79 (0.1)\",\"11.8 (1.6)\",\"90.50 (0.1)\",\"43.2 (3.8)\",\"90.08 (0.1)\",\"80.6 (2.2)\",\"89.45 (0.1)\",\"97.5 (0.3)\",\"88.33 (0.1)\",\"99.0 (0.3)\"\n\"r32 t\",\"90.77 (0.1)\",\"08.4 (1.3)\",\"90.46 (0.1)\",\"65.0 (6.8)\",\"90.00 (0.1)\",\"72.7 (5.7)\",\"89.07 (0.1)\",\"98.2 (1.1)\",\"88.23 (0.1)\",\"95.8 (2.5)\"\n\"r32 p\",\"90.60 (0.0)\",\"03.0 (0.3)\",\"90.21 (0.1)\",\"05.5 (0.3)\",\"89.61 (0.1)\",\"11.1 (0.7)\",\"88.55 (0.1)\",\"19.5 (0.6)\",\"87.39 (0.1)\",\"31.6 (0.8)\"\n\"r18 s\",\"93.17 (0.1)\",\"20.3 (2.2)\",\"93.08 (0.1)\",\"47.0 (2.6)\",\"92.94 (0.0)\",\"65.6 (1.6)\",\"91.91 (0.1)\",\"89.9 (1.0)\",\"91.16 (0.1)\",\"93.1 (0.7)\""}, {"type": "text", "value": "Table 7: An expanded version of Table 5 (2) in which each point is averaged over 10 runs and standard errors are shown in parentheses. We additionally evaluate different choices of trigger with ResNet-32s.", "md": "Table 7: An expanded version of Table 5 (2) in which each point is averaged over 10 runs and standard errors are shown in parentheses. We additionally evaluate different choices of trigger with ResNet-32s."}, {"type": "table", "rows": [["", "150", "300", "500", "1000", "1500"], ["r32 \u2192 r18", "93.32 (0.1)", "09.4 (1.5)", "93.05 (0.1)", "52.9 (3.0)", "92.70 (0.1)", "85.3 (1.7)", "91.72 (0.1)", "99.2 (0.2)", "90.93 (0.1)", "99.7 (0.1)"], ["r18 \u2192 r32", "90.86 (0.1)", "12.3 (1.3)", "90.57 (0.1)", "40.9 (3.3)", "90.28 (0.1)", "59.7 (2.6)", "89.39 (0.1)", "83.0 (1.7)", "88.59 (0.1)", "89.2 (0.7)"]], "md": "| |150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|r32 \u2192 r18|93.32 (0.1)|09.4 (1.5)|93.05 (0.1)|52.9 (3.0)|92.70 (0.1)|85.3 (1.7)|91.72 (0.1)|99.2 (0.2)|90.93 (0.1)|99.7 (0.1)|\n|r18 \u2192 r32|90.86 (0.1)|12.3 (1.3)|90.57 (0.1)|40.9 (3.3)|90.28 (0.1)|59.7 (2.6)|89.39 (0.1)|83.0 (1.7)|88.59 (0.1)|89.2 (0.7)|", "isPerfectTable": false, "csv": "\"\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"r32 \u2192 r18\",\"93.32 (0.1)\",\"09.4 (1.5)\",\"93.05 (0.1)\",\"52.9 (3.0)\",\"92.70 (0.1)\",\"85.3 (1.7)\",\"91.72 (0.1)\",\"99.2 (0.2)\",\"90.93 (0.1)\",\"99.7 (0.1)\"\n\"r18 \u2192 r32\",\"90.86 (0.1)\",\"12.3 (1.3)\",\"90.57 (0.1)\",\"40.9 (3.3)\",\"90.28 (0.1)\",\"59.7 (2.6)\",\"89.39 (0.1)\",\"83.0 (1.7)\",\"88.59 (0.1)\",\"89.2 (0.7)\""}, {"type": "text", "value": "Table 8: An expanded version of Table 5 (1+2) in which each point is averaged over 10 runs and standard errors are shown in parentheses.", "md": "Table 8: An expanded version of Table 5 (1+2) in which each point is averaged over 10 runs and standard errors are shown in parentheses."}]}, {"page": 22, "text": "C.2.2             Number of experts E and number of epochs K\nAlthough for all of the above experiments we used E = 50 experts to create our labels, PTA is robust\nagainst using smaller number of experts. As we show in Table 9 and Fig. 6a, an attack is successful\nwith as few as a single expert model.\n   E                   150                                   300                                   500                                    1000                                1500\n   1                   92.38 (0.1)     09.9 (0.5)            92.05 (0.0)      45.4 (2.4)           91.59 (0.0)      80.7 (2.7)            90.80 (0.1)     98.0 (0.2)          89.90 (0.1)     99.5 (0.1)\n   5                   92.36 (0.0)     11.9 (1.1)            92.05 (0.1)      45.3 (3.0)           91.42 (0.1)      86.6 (1.2)            90.81 (0.1)     99.1 (0.2)          89.94 (0.0)     99.8 (0.1)\n   10                  92.39 (0.1)     15.1 (1.6)            92.09 (0.1)      59.7 (2.3)           91.74 (0.0)      88.5 (1.5)            90.91 (0.1)     99.4 (0.1)          90.03 (0.1)     99.8 (0.1)\n   25                  92.33 (0.1)     10.9 (1.1)            92.06 (0.1)      50.9 (2.2)           91.74 (0.1)      88.9 (1.2)            90.92 (0.1)     98.9 (0.2)          90.03 (0.0)     99.7 (0.0)\n   50                  92.44 (0.0)     12.0 (1.6)            91.93 (0.1)      54.4 (3.1)           91.55 (0.1)      89.9 (1.1)            90.91 (0.1)     99.6 (0.1)          89.73 (0.1)     99.9 (0.0)\nTable 9: Understanding the effect of the number of expert models on the backdoor attack. The\nexperiments were conducted using ResNet-32s on CIFAR-10 poisoned by the sinusoidal trigger.\nStandard errors in the parentheses are averaged over 10 runs.\nAt K = 0 epoch, FLIP uses an expert with random weights to find the examples to corrupt, and\nhence the attack is weak. In our experiments we use K = 20. Table 10 shows that the attack is robust\nin the choice of K and already achieves strong performance with K = 1 epoch of expert training.\n   K               100                          150                          200                          250                         500                          1000                       1500\n   0               92.38 (0.1)     00.8 (0.1)92.38 (0.1)        01.4 (0.2)92.27 (0.1)        01.9 (0.2)92.22 (0.1)        03.1 (0.3)91.79 (0.1)        05.9 (0.3)90.92 (0.1)      12.1 (0.9)89.40 (0.1)       22.4 (1.0)\n   1               92.44 (0.1)     09.4 (1.1)92.33 (0.1)        17.0 (1.3)92.18 (0.1)        29.8 (2.3)92.15 (0.0)        43.8 (1.8)91.59 (0.1)        66.1 (1.7)90.73 (0.1)      85.0 (0.7)89.62 (0.1)       86.8 (1.0)\n   5               92.35 (0.1)     04.6 (0.4)92.27 (0.0)        12.7 (1.0)92.22 (0.0)        22.3 (2.0)92.09 (0.0)        42.5 (3.2)91.66 (0.0)        90.3 (0.7)90.72 (0.1)      98.0 (0.5)89.80 (0.1)       99.6 (0.1)\n   10              92.33 (0.0)     04.4 (0.3)92.38 (0.0)        09.6 (0.9)92.27 (0.1)        26.1 (2.2)92.05 (0.1)        39.4 (2.6)91.65 (0.0)        89.9 (1.4)90.67 (0.0)      99.5 (0.1)89.81 (0.1)       99.8 (0.1)\n   20              92.54 (0.1)     05.5 (0.4)92.26 (0.1)        12.4 (1.8)92.22 (0.1)        12.1 (1.6)91.90 (0.1)        12.8 (1.3)91.73 (0.1)        87.2 (1.3)90.68 (0.1)      99.4 (0.2)89.87 (0.1)       99.8 (0.1)\n   30              92.45 (0.0)     04.5 (0.5)92.31 (0.0)        10.9 (0.8)92.22 (0.1)        15.4 (0.7)92.25 (0.1)        34.4 (2.9)91.83 (0.1)        91.5 (0.8)90.94 (0.1)      99.3 (0.1)90.00 (0.0)       99.9 (0.0)\n   40              92.43 (0.1)     04.4 (0.4)92.37 (0.1)        09.0 (0.7)92.26 (0.1)        19.7 (2.0)92.21 (0.1)        28.4 (2.1)91.69 (0.0)        87.0 (1.8)90.92 (0.0)      97.3 (0.9)90.01 (0.1)       99.6 (0.1)\n   50              92.37 (0.1)     03.7 (0.3)92.41 (0.1)        08.1 (0.7)92.29 (0.0)        18.4 (2.2)92.15 (0.1)        29.1 (2.4)91.72 (0.1)        93.1 (1.3)90.87 (0.1)      99.4 (0.1)90.03 (0.1)       99.8 (0.1)\nTable 10: CTA/PTA pairs at different values of K (the number of epochs each expert is run for) and\nnumbers of flipped labels. When K = 0, FLIP expert model has random weights, and hence the\nattack is weak. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is\naveraged over 10 runs and standard errors are shown in parentheses.\nC.2.3             A single class source vs. multi-class source\nWhile the majority of our paper focuses on the canonical single-source backdoor attack where\nysource = 9 is, as described in Section 1.1, fixed to a single class, in this section, we consider a many-\nto-one attack that removes this restriction. In particular, we have that ysource = {0, ...}                                                                                            \\  {ytarget = 4},\nallowing the attacker to poison images from any class to yield the desired prediction of ytarget. We\nobserve that this class of attack is effective as demonstrated in Table 11 and Fig. 5c.\n   y source                       150                                 300                                  500                                 1000                               1500\n   all but class 4                92.36 (0.0)      17.2 (0.3)         92.14 (0.1)      28.0 (1.5)          91.67 (0.1)     58.1 (3.0)          90.79 (0.1)      88.9 (0.9)        90.14 (0.1)     95.6 (0.4)\n   class 9 = \u2019truck\u2019              92.26 (0.1)      12.4 (1.8)         92.09 (0.1)      54.9 (2.4)          91.73 (0.1)     87.2 (1.3)          90.68 (0.1)      99.4 (0.2)        89.87 (0.1)     99.8 (0.1)\nTable 11: FLIP is effective even when the threat model allows for (all) classes to be poisoned. Experts\nare trained on data where images from each class were poisoned. Experiments are computed on\nCIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are\nshown in parentheses.\nC.2.4             Limited access to dataset\nIn this section, we consider the scenario in which the attacker is not provided the user\u2019s entire dataset\n(i.e., the user only distills a portion of their data or only needs chunk of their dataset labeled). As we\nshow in Table 12, with enough label-flips, FLIP attack gracefully degrades as the knowledge of the\nuser\u2019s training dataset decreases.\n                                                                                                           22", "md": "## C.2.2 Number of experts E and number of epochs K\n\nAlthough for all of the above experiments we used E = 50 experts to create our labels, PTA is robust against using a smaller number of experts. As we show in Table 9 and Fig. 6a, an attack is successful with as few as a single expert model.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\nE & 150 & 300 & 500 & 1000 & 1500 \\\\\n\\hline\n1 & 92.38 (0.1) & 09.9 (0.5) & 92.05 (0.0) & 45.4 (2.4) & 91.59 (0.0) \\\\\n5 & 92.36 (0.0) & 11.9 (1.1) & 92.05 (0.1) & 45.3 (3.0) & 91.42 (0.1) \\\\\n10 & 92.39 (0.1) & 15.1 (1.6) & 92.09 (0.1) & 59.7 (2.3) & 91.74 (0.0) \\\\\n25 & 92.33 (0.1) & 10.9 (1.1) & 92.06 (0.1) & 50.9 (2.2) & 91.74 (0.1) \\\\\n50 & 92.44 (0.0) & 12.0 (1.6) & 91.93 (0.1) & 54.4 (3.1) & 91.55 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 9: Understanding the effect of the number of expert models on the backdoor attack. The experiments were conducted using ResNet-32s on CIFAR-10 poisoned by the sinusoidal trigger. Standard errors in the parentheses are averaged over 10 runs.\n\nAt K = 0 epoch, FLIP uses an expert with random weights to find the examples to corrupt, and hence the attack is weak. In our experiments, we use K = 20. Table 10 shows that the attack is robust in the choice of K and already achieves strong performance with K = 1 epoch of expert training.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\nK & 100 & 150 & 200 & 250 & 500 & 1000 & 1500 \\\\\n\\hline\n0 & 92.38 (0.1) & 00.8 (0.1) & 92.38 (0.1) & 01.4 (0.2) & 92.27 (0.1) & 01.9 (0.2) & 92.22 (0.1) \\\\\n1 & 92.44 (0.1) & 09.4 (1.1) & 92.33 (0.1) & 17.0 (1.3) & 92.18 (0.1) & 29.8 (2.3) & 92.15 (0.0) \\\\\n5 & 92.35 (0.1) & 04.6 (0.4) & 92.27 (0.0) & 12.7 (1.0) & 92.22 (0.0) & 22.3 (2.0) & 92.09 (0.0) \\\\\n10 & 92.33 (0.0) & 04.4 (0.3) & 92.38 (0.0) & 09.6 (0.9) & 92.27 (0.1) & 26.1 (2.2) & 92.05 (0.1) \\\\\n20 & 92.54 (0.1) & 05.5 (0.4) & 92.26 (0.1) & 12.4 (1.8) & 92.22 (0.1) & 12.1 (1.6) & 91.90 (0.1) \\\\\n30 & 92.45 (0.0) & 04.5 (0.5) & 92.31 (0.0) & 10.9 (0.8) & 92.22 (0.1) & 15.4 (0.7) & 92.25 (0.1) \\\\\n40 & 92.43 (0.1) & 04.4 (0.4) & 92.37 (0.1) & 09.0 (0.7) & 92.26 (0.1) & 19.7 (2.0) & 92.21 (0.1) \\\\\n50 & 92.37 (0.1) & 03.7 (0.3) & 92.41 (0.1) & 08.1 (0.7) & 92.29 (0.0) & 18.4 (2.2) & 92.15 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 10: CTA/PTA pairs at different values of K (the number of epochs each expert is run for) and numbers of flipped labels. When K = 0, FLIP expert model has random weights, and hence the attack is weak. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.\n\n## C.2.3 A single class source vs. multi-class source\n\nWhile the majority of our paper focuses on the canonical single-source backdoor attack where ysource = 9 is, as described in Section 1.1, fixed to a single class, in this section, we consider a many-to-one attack that removes this restriction. In particular, we have that ysource = {0, ...} \\ {ytarget = 4}, allowing the attacker to poison images from any class to yield the desired prediction of ytarget. We observe that this class of attack is effective as demonstrated in Table 11 and Fig. 5c.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\ny\\text{ source} & 150 & 300 & 500 & 1000 & 1500 \\\\\n\\hline\n\\text{all but class 4} & 92.36 (0.0) & 17.2 (0.3) & 92.14 (0.1) & 28.0 (1.5) & 91.67 (0.1) \\\\\n\\text{class 9 = 'truck'} & 92.26 (0.1) & 12.4 (1.8) & 92.09 (0.1) & 54.9 (2.4) & 91.73 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 11: FLIP is effective even when the threat model allows for (all) classes to be poisoned. Experts are trained on data where images from each class were poisoned. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.\n\n## C.2.4 Limited access to dataset\n\nIn this section, we consider the scenario in which the attacker is not provided the user's entire dataset (i.e., the user only distills a portion of their data or only needs a chunk of their dataset labeled). As we show in Table 12, with enough label-flips, FLIP attack gracefully degrades as the knowledge of the user's training dataset decreases.\n\nTable 12: To be provided in the HTML output.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "C.2.2 Number of experts E and number of epochs K", "md": "## C.2.2 Number of experts E and number of epochs K"}, {"type": "text", "value": "Although for all of the above experiments we used E = 50 experts to create our labels, PTA is robust against using a smaller number of experts. As we show in Table 9 and Fig. 6a, an attack is successful with as few as a single expert model.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\nE & 150 & 300 & 500 & 1000 & 1500 \\\\\n\\hline\n1 & 92.38 (0.1) & 09.9 (0.5) & 92.05 (0.0) & 45.4 (2.4) & 91.59 (0.0) \\\\\n5 & 92.36 (0.0) & 11.9 (1.1) & 92.05 (0.1) & 45.3 (3.0) & 91.42 (0.1) \\\\\n10 & 92.39 (0.1) & 15.1 (1.6) & 92.09 (0.1) & 59.7 (2.3) & 91.74 (0.0) \\\\\n25 & 92.33 (0.1) & 10.9 (1.1) & 92.06 (0.1) & 50.9 (2.2) & 91.74 (0.1) \\\\\n50 & 92.44 (0.0) & 12.0 (1.6) & 91.93 (0.1) & 54.4 (3.1) & 91.55 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 9: Understanding the effect of the number of expert models on the backdoor attack. The experiments were conducted using ResNet-32s on CIFAR-10 poisoned by the sinusoidal trigger. Standard errors in the parentheses are averaged over 10 runs.\n\nAt K = 0 epoch, FLIP uses an expert with random weights to find the examples to corrupt, and hence the attack is weak. In our experiments, we use K = 20. Table 10 shows that the attack is robust in the choice of K and already achieves strong performance with K = 1 epoch of expert training.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\nK & 100 & 150 & 200 & 250 & 500 & 1000 & 1500 \\\\\n\\hline\n0 & 92.38 (0.1) & 00.8 (0.1) & 92.38 (0.1) & 01.4 (0.2) & 92.27 (0.1) & 01.9 (0.2) & 92.22 (0.1) \\\\\n1 & 92.44 (0.1) & 09.4 (1.1) & 92.33 (0.1) & 17.0 (1.3) & 92.18 (0.1) & 29.8 (2.3) & 92.15 (0.0) \\\\\n5 & 92.35 (0.1) & 04.6 (0.4) & 92.27 (0.0) & 12.7 (1.0) & 92.22 (0.0) & 22.3 (2.0) & 92.09 (0.0) \\\\\n10 & 92.33 (0.0) & 04.4 (0.3) & 92.38 (0.0) & 09.6 (0.9) & 92.27 (0.1) & 26.1 (2.2) & 92.05 (0.1) \\\\\n20 & 92.54 (0.1) & 05.5 (0.4) & 92.26 (0.1) & 12.4 (1.8) & 92.22 (0.1) & 12.1 (1.6) & 91.90 (0.1) \\\\\n30 & 92.45 (0.0) & 04.5 (0.5) & 92.31 (0.0) & 10.9 (0.8) & 92.22 (0.1) & 15.4 (0.7) & 92.25 (0.1) \\\\\n40 & 92.43 (0.1) & 04.4 (0.4) & 92.37 (0.1) & 09.0 (0.7) & 92.26 (0.1) & 19.7 (2.0) & 92.21 (0.1) \\\\\n50 & 92.37 (0.1) & 03.7 (0.3) & 92.41 (0.1) & 08.1 (0.7) & 92.29 (0.0) & 18.4 (2.2) & 92.15 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 10: CTA/PTA pairs at different values of K (the number of epochs each expert is run for) and numbers of flipped labels. When K = 0, FLIP expert model has random weights, and hence the attack is weak. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.", "md": "Although for all of the above experiments we used E = 50 experts to create our labels, PTA is robust against using a smaller number of experts. As we show in Table 9 and Fig. 6a, an attack is successful with as few as a single expert model.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\nE & 150 & 300 & 500 & 1000 & 1500 \\\\\n\\hline\n1 & 92.38 (0.1) & 09.9 (0.5) & 92.05 (0.0) & 45.4 (2.4) & 91.59 (0.0) \\\\\n5 & 92.36 (0.0) & 11.9 (1.1) & 92.05 (0.1) & 45.3 (3.0) & 91.42 (0.1) \\\\\n10 & 92.39 (0.1) & 15.1 (1.6) & 92.09 (0.1) & 59.7 (2.3) & 91.74 (0.0) \\\\\n25 & 92.33 (0.1) & 10.9 (1.1) & 92.06 (0.1) & 50.9 (2.2) & 91.74 (0.1) \\\\\n50 & 92.44 (0.0) & 12.0 (1.6) & 91.93 (0.1) & 54.4 (3.1) & 91.55 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 9: Understanding the effect of the number of expert models on the backdoor attack. The experiments were conducted using ResNet-32s on CIFAR-10 poisoned by the sinusoidal trigger. Standard errors in the parentheses are averaged over 10 runs.\n\nAt K = 0 epoch, FLIP uses an expert with random weights to find the examples to corrupt, and hence the attack is weak. In our experiments, we use K = 20. Table 10 shows that the attack is robust in the choice of K and already achieves strong performance with K = 1 epoch of expert training.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\\hline\nK & 100 & 150 & 200 & 250 & 500 & 1000 & 1500 \\\\\n\\hline\n0 & 92.38 (0.1) & 00.8 (0.1) & 92.38 (0.1) & 01.4 (0.2) & 92.27 (0.1) & 01.9 (0.2) & 92.22 (0.1) \\\\\n1 & 92.44 (0.1) & 09.4 (1.1) & 92.33 (0.1) & 17.0 (1.3) & 92.18 (0.1) & 29.8 (2.3) & 92.15 (0.0) \\\\\n5 & 92.35 (0.1) & 04.6 (0.4) & 92.27 (0.0) & 12.7 (1.0) & 92.22 (0.0) & 22.3 (2.0) & 92.09 (0.0) \\\\\n10 & 92.33 (0.0) & 04.4 (0.3) & 92.38 (0.0) & 09.6 (0.9) & 92.27 (0.1) & 26.1 (2.2) & 92.05 (0.1) \\\\\n20 & 92.54 (0.1) & 05.5 (0.4) & 92.26 (0.1) & 12.4 (1.8) & 92.22 (0.1) & 12.1 (1.6) & 91.90 (0.1) \\\\\n30 & 92.45 (0.0) & 04.5 (0.5) & 92.31 (0.0) & 10.9 (0.8) & 92.22 (0.1) & 15.4 (0.7) & 92.25 (0.1) \\\\\n40 & 92.43 (0.1) & 04.4 (0.4) & 92.37 (0.1) & 09.0 (0.7) & 92.26 (0.1) & 19.7 (2.0) & 92.21 (0.1) \\\\\n50 & 92.37 (0.1) & 03.7 (0.3) & 92.41 (0.1) & 08.1 (0.7) & 92.29 (0.0) & 18.4 (2.2) & 92.15 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 10: CTA/PTA pairs at different values of K (the number of epochs each expert is run for) and numbers of flipped labels. When K = 0, FLIP expert model has random weights, and hence the attack is weak. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses."}, {"type": "heading", "lvl": 2, "value": "C.2.3 A single class source vs. multi-class source", "md": "## C.2.3 A single class source vs. multi-class source"}, {"type": "text", "value": "While the majority of our paper focuses on the canonical single-source backdoor attack where ysource = 9 is, as described in Section 1.1, fixed to a single class, in this section, we consider a many-to-one attack that removes this restriction. In particular, we have that ysource = {0, ...} \\ {ytarget = 4}, allowing the attacker to poison images from any class to yield the desired prediction of ytarget. We observe that this class of attack is effective as demonstrated in Table 11 and Fig. 5c.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\ny\\text{ source} & 150 & 300 & 500 & 1000 & 1500 \\\\\n\\hline\n\\text{all but class 4} & 92.36 (0.0) & 17.2 (0.3) & 92.14 (0.1) & 28.0 (1.5) & 91.67 (0.1) \\\\\n\\text{class 9 = 'truck'} & 92.26 (0.1) & 12.4 (1.8) & 92.09 (0.1) & 54.9 (2.4) & 91.73 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 11: FLIP is effective even when the threat model allows for (all) classes to be poisoned. Experts are trained on data where images from each class were poisoned. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.", "md": "While the majority of our paper focuses on the canonical single-source backdoor attack where ysource = 9 is, as described in Section 1.1, fixed to a single class, in this section, we consider a many-to-one attack that removes this restriction. In particular, we have that ysource = {0, ...} \\ {ytarget = 4}, allowing the attacker to poison images from any class to yield the desired prediction of ytarget. We observe that this class of attack is effective as demonstrated in Table 11 and Fig. 5c.\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\ny\\text{ source} & 150 & 300 & 500 & 1000 & 1500 \\\\\n\\hline\n\\text{all but class 4} & 92.36 (0.0) & 17.2 (0.3) & 92.14 (0.1) & 28.0 (1.5) & 91.67 (0.1) \\\\\n\\text{class 9 = 'truck'} & 92.26 (0.1) & 12.4 (1.8) & 92.09 (0.1) & 54.9 (2.4) & 91.73 (0.1) \\\\\n\\hline\n\\end{array}\n$$\n\nTable 11: FLIP is effective even when the threat model allows for (all) classes to be poisoned. Experts are trained on data where images from each class were poisoned. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses."}, {"type": "heading", "lvl": 2, "value": "C.2.4 Limited access to dataset", "md": "## C.2.4 Limited access to dataset"}, {"type": "text", "value": "In this section, we consider the scenario in which the attacker is not provided the user's entire dataset (i.e., the user only distills a portion of their data or only needs a chunk of their dataset labeled). As we show in Table 12, with enough label-flips, FLIP attack gracefully degrades as the knowledge of the user's training dataset decreases.\n\nTable 12: To be provided in the HTML output.", "md": "In this section, we consider the scenario in which the attacker is not provided the user's entire dataset (i.e., the user only distills a portion of their data or only needs a chunk of their dataset labeled). As we show in Table 12, with enough label-flips, FLIP attack gracefully degrades as the knowledge of the user's training dataset decreases.\n\nTable 12: To be provided in the HTML output."}]}, {"page": 23, "text": "   %               150                             300                             500                             1000                            1500\n   20              92.26 (0.0)   06.3 (1.1)        92.05 (0.1)   07.2 (0.6)        91.59 (0.1)   10.9 (0.9)        90.69 (0.1)   15.7 (0.7)        89.78 (0.0)   21.8 (1.2)\n   40              92.31 (0.1)   10.2 (0.9)        92.12 (0.1)   28.7 (1.6)        91.79 (0.1)   45.2 (2.7)        90.90 (0.1)   62.5 (1.5)        90.08 (0.1)   74.1 (2.6)\n   60              92.31 (0.0)   14.0 (0.8)        91.99 (0.1)   45.8 (3.3)        91.70 (0.0)   68.4 (2.7)        90.75 (0.1)   85.5 (1.5)        89.88 (0.1)   92.4 (0.9)\n   80              92.48 (0.1)   14.0 (1.4)        92.02 (0.1)   42.5 (3.2)        91.70 (0.1)   80.0 (2.0)        90.92 (0.1)   96.6 (0.4)        89.98 (0.1)   98.5 (0.3)\nTable 12: FLIP still works with limited access to the dataset. In this setting, the attacker is provided\n20%, 40%, 60%, or 80% of the user\u2019s dataset but the user\u2019s model is evaluated on the entire dataset.\nExperiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10\nruns and standard errors are shown in parentheses.\nC.3         Main results on softFLIP for knowledge distillation\nWe define softFLIP with a parameter \u03b1 \u2208                                          [0, 1] as, for each image, an interpolation between the\nsoft label (i.e., a vector in the simplex over the classes) that is given by the second step of FLIP and\nthe ground truths one-hot encoded label of that image. Clean label corresponds to \u03b1 = 1. Fig. 7\nshowcases softFLIP in comparison to the one-hot encoded corruption of FLIP. An expanded version\nwith standard errors is given in Table 13. As a comparison, we show the CTA-PTA trade-off using an\nattack with a rounded version of softFLIP\u2019s corrupted labels such that all poisoned labels are one-hot\nencoded in Table 14.\n              0.0                          0.2                         0.4                          0.6                         0.8                          0.9\n   r32   s    90.04 (0.1)   100. (0.0)     90.08 (0.0)  100. (0.0)     90.11 (0.1)   100. (0.0)     90.45 (0.1)   99.9 (0.0)    91.02 (0.0)   99.0 (0.1)     91.95 (0.0)   25.3 (3.0)\n         t    88.05 (0.1)   100. (0.0)     88.43 (0.1)  100. (0.0)     88.44 (0.1)   100. (0.0)     88.99 (0.1)   100. (0.0)    89.86 (0.1)   100. (0.0)     90.85 (0.0)   100. (0.0)\n         p    88.02 (0.1)   44.5 (0.4)     88.26 (0.1)  41.9 (0.4)     88.62 (0.1)   38.8 (0.5)     89.10 (0.1)   31.1 (0.4)    91.64 (0.1)   05.1 (0.3)     92.04 (0.1)   00.1 (0.0)\n   r18   s    92.97 (0.1)   98.7 (0.3)     92.92 (0.1)  97.3 (1.3)     93.13 (0.1)   96.0 (2.1)     93.25 (0.1)   95.6 (0.9)    93.67 (0.1)   86.1 (1.4)     93.91 (0.1)   33.6 (4.9)\nTable 13: CTA-PTA trade-off for softFLIP with varying \u03b1 \u2208                                                           {0.0, 0.2, 0.4, 0.6, 0.8, 0.9}, varying\narchitecture (ResNet-32 and ResNet-18), and varying trigger patterns (sinusoidal, Turner, pixel).\nEach point is averaged over 10 runs and standard errors are shown in parentheses.\n              0.0                          0.2                         0.4                          0.6                         0.8                          0.9\n   r32   s    89.82 (0.1)   100. (0.0)     89.78 (0.1)  99.9 (0.0)     90.00 (0.1)   99.9 (0.0)     90.25 (0.1)   99.9 (0.0)    91.08 (0.1)   99.0 (0.2)     92.21 (0.1)   29.2 (2.9)\n         t    87.30 (0.1)   99.6 (0.4)     87.47 (0.1)  99.8 (0.1)     87.83 (0.1)   100. (0.0)     88.52 (0.1)   100. (0.0)    89.76 (0.1)   99.3 (0.4)     91.27 (0.1)   100. (0.0)\n         p    87.88 (0.1)   42.1 (0.6)     88.09 (0.1)  39.7 (0.3)     88.45 (0.1)   36.5 (0.8)     89.22 (0.1)   29.9 (0.4)    91.53 (0.1)   07.9 (0.3)     92.53 (0.0)   00.1 (0.0)\n   r18   s    92.59 (0.1)   95.2 (1.1)     92.82 (0.1)  92.6 (2.1)     92.87 (0.1)   89.7 (1.5)     93.25 (0.0)   90.0 (2.1)    93.56 (0.1)   57.3 (2.5)     94.12 (0.1)   03.9 (0.4)\nTable 14: A rounded version of Table 13, the results are averaged over 10 runs, and the standard\nerrors are shown in parentheses.\nD         Additional experiments\nD.1         Sparse regression approach with \u21131 regularization\nFLIP performs an approximate subset selection (for the subset to be label-corrupted examples) by\nsolving a real-valued optimization and selecting those with highest scores. In principle, one could\ninstead search for sparse deviation from the true labels using \u21131 regularization, as in sparse regression.\nTo that end we experimented with adding the following regularization term to Lparam in Section 2:\n                                                         | \u02dc \u03bb                      yx \u2212       softmax(\u02c6         \u2113x)    1 ,                                                       (4)\n                                                          B(j)k |     x\u2208   \u02dc\n                                                                           B(j)\n                                                                             k\nwhere yx refers to the one-hot ground-truth labels for image x. Table 15 shows that this approach has\nlittle-to-no improvement over the standard FLIP (which corresponds to \u03bb = 0).\nD.2         FLIP against defenses\nWe test the performance of FLIP when state-of-the-art backdoor defenses are applied. We evaluate\nFLIP on CIFAR-10 with all three trigger types on three popular defenses: kmeans [15], PCA [89],\n                                                                                         23", "md": "# Document\n\n## Table 12:\n\n|%|150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|20|$92.26 (0.0)$|$06.3 (1.1)$|$92.05 (0.1)$|$07.2 (0.6)$|$91.59 (0.1)$|$10.9 (0.9)$|$90.69 (0.1)$|$15.7 (0.7)$|$89.78 (0.0)$|$21.8 (1.2)$|\n|40|$92.31 (0.1)$|$10.2 (0.9)$|$92.12 (0.1)$|$28.7 (1.6)$|$91.79 (0.1)$|$45.2 (2.7)$|$90.90 (0.1)$|$62.5 (1.5)$|$90.08 (0.1)$|$74.1 (2.6)$|\n|60|$92.31 (0.0)$|$14.0 (0.8)$|$91.99 (0.1)$|$45.8 (3.3)$|$91.70 (0.0)$|$68.4 (2.7)$|$90.75 (0.1)$|$85.5 (1.5)$|$89.88 (0.1)$|$92.4 (0.9)$|\n|80|$92.48 (0.1)$|$14.0 (1.4)$|$92.02 (0.1)$|$42.5 (3.2)$|$91.70 (0.1)$|$80.0 (2.0)$|$90.92 (0.1)$|$96.6 (0.4)$|$89.98 (0.1)$|$98.5 (0.3)$|\n\nFLIP still works with limited access to the dataset. In this setting, the attacker is provided 20%, 40%, 60%, or 80% of the user\u2019s dataset but the user\u2019s model is evaluated on the entire dataset. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.\n\n## C.3 Main results on softFLIP for knowledge distillation\n\nWe define softFLIP with a parameter $$\\alpha \\in [0, 1]$$ as, for each image, an interpolation between the soft label (i.e., a vector in the simplex over the classes) that is given by the second step of FLIP and the ground truths one-hot encoded label of that image. Clean label corresponds to $$\\alpha = 1$$. Fig. 7 showcases softFLIP in comparison to the one-hot encoded corruption of FLIP. An expanded version with standard errors is given in Table 13. As a comparison, we show the CTA-PTA trade-off using an attack with a rounded version of softFLIP\u2019s corrupted labels such that all poisoned labels are one-hot encoded in Table 14.\n\n| |0.0|0.2|0.4|0.6|0.8|0.9|\n|---|---|---|---|---|---|---|\n|r32 s|$90.04 (0.1)$|$100. (0.0)$|$90.08 (0.0)$|$100. (0.0)$|$90.11 (0.1)$|$100. (0.0)$|\n|t|$88.05 (0.1)$|$100. (0.0)$|$88.43 (0.1)$|$100. (0.0)$|$88.44 (0.1)$|$100. (0.0)$|\n|p|$88.02 (0.1)$|$44.5 (0.4)$|$88.26 (0.1)$|$41.9 (0.4)$|$88.62 (0.1)$|$38.8 (0.5)$|\n|r18 s|$92.97 (0.1)$|$98.7 (0.3)$|$92.92 (0.1)$|$97.3 (1.3)$|$93.13 (0.1)$|$96.0 (2.1)$|\n\nTable 13: CTA-PTA trade-off for softFLIP with varying $$\\alpha \\in \\{0.0, 0.2, 0.4, 0.6, 0.8, 0.9\\}$$, varying architecture (ResNet-32 and ResNet-18), and varying trigger patterns (sinusoidal, Turner, pixel). Each point is averaged over 10 runs and standard errors are shown in parentheses.\n\n| |0.0|0.2|0.4|0.6|0.8|0.9|\n|---|---|---|---|---|---|---|\n|r32 s|$89.82 (0.1)$|$100. (0.0)$|$89.78 (0.1)$|$99.9 (0.0)$|$90.00 (0.1)$|$99.9 (0.0)$|\n|t|$87.30 (0.1)$|$99.6 (0.4)$|$87.47 (0.1)$|$99.8 (0.1)$|$87.83 (0.1)$|$100. (0.0)$|\n|p|$87.88 (0.1)$|$42.1 (0.6)$|$88.09 (0.1)$|$39.7 (0.3)$|$88.45 (0.1)$|$36.5 (0.8)$|\n|r18 s|$92.59 (0.1)$|$95.2 (1.1)$|$92.82 (0.1)$|$92.6 (2.1)$|$92.87 (0.1)$|$89.7 (1.5)$|\n\nTable 14: A rounded version of Table 13, the results are averaged over 10 runs, and the standard errors are shown in parentheses.\n\n## D Additional experiments\n\n### D.1 Sparse regression approach with \u21131 regularization\n\nFLIP performs an approximate subset selection (for the subset to be label-corrupted examples) by solving a real-valued optimization and selecting those with highest scores. In principle, one could instead search for sparse deviation from the true labels using \u21131 regularization, as in sparse regression. To that end we experimented with adding the following regularization term to Lparam in Section 2:\n\n$$| \\tilde{\\lambda} \\cdot (yx - \\text{softmax}(\\hat{\\ell}_x)) |_{1},$$\n\nwhere yx refers to the one-hot ground-truth labels for image x. Table 15 shows that this approach has little-to-no improvement over the standard FLIP (which corresponds to $$\\lambda = 0$$).\n\n### D.2 FLIP against defenses\n\nWe test the performance of FLIP when state-of-the-art backdoor defenses are applied. We evaluate FLIP on CIFAR-10 with all three trigger types on three popular defenses: kmeans [15], PCA [89], 23", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Table 12:", "md": "## Table 12:"}, {"type": "table", "rows": [["%", "150", "300", "500", "1000", "1500"], ["20", "$92.26 (0.0)$", "$06.3 (1.1)$", "$92.05 (0.1)$", "$07.2 (0.6)$", "$91.59 (0.1)$", "$10.9 (0.9)$", "$90.69 (0.1)$", "$15.7 (0.7)$", "$89.78 (0.0)$", "$21.8 (1.2)$"], ["40", "$92.31 (0.1)$", "$10.2 (0.9)$", "$92.12 (0.1)$", "$28.7 (1.6)$", "$91.79 (0.1)$", "$45.2 (2.7)$", "$90.90 (0.1)$", "$62.5 (1.5)$", "$90.08 (0.1)$", "$74.1 (2.6)$"], ["60", "$92.31 (0.0)$", "$14.0 (0.8)$", "$91.99 (0.1)$", "$45.8 (3.3)$", "$91.70 (0.0)$", "$68.4 (2.7)$", "$90.75 (0.1)$", "$85.5 (1.5)$", "$89.88 (0.1)$", "$92.4 (0.9)$"], ["80", "$92.48 (0.1)$", "$14.0 (1.4)$", "$92.02 (0.1)$", "$42.5 (3.2)$", "$91.70 (0.1)$", "$80.0 (2.0)$", "$90.92 (0.1)$", "$96.6 (0.4)$", "$89.98 (0.1)$", "$98.5 (0.3)$"]], "md": "|%|150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|20|$92.26 (0.0)$|$06.3 (1.1)$|$92.05 (0.1)$|$07.2 (0.6)$|$91.59 (0.1)$|$10.9 (0.9)$|$90.69 (0.1)$|$15.7 (0.7)$|$89.78 (0.0)$|$21.8 (1.2)$|\n|40|$92.31 (0.1)$|$10.2 (0.9)$|$92.12 (0.1)$|$28.7 (1.6)$|$91.79 (0.1)$|$45.2 (2.7)$|$90.90 (0.1)$|$62.5 (1.5)$|$90.08 (0.1)$|$74.1 (2.6)$|\n|60|$92.31 (0.0)$|$14.0 (0.8)$|$91.99 (0.1)$|$45.8 (3.3)$|$91.70 (0.0)$|$68.4 (2.7)$|$90.75 (0.1)$|$85.5 (1.5)$|$89.88 (0.1)$|$92.4 (0.9)$|\n|80|$92.48 (0.1)$|$14.0 (1.4)$|$92.02 (0.1)$|$42.5 (3.2)$|$91.70 (0.1)$|$80.0 (2.0)$|$90.92 (0.1)$|$96.6 (0.4)$|$89.98 (0.1)$|$98.5 (0.3)$|", "isPerfectTable": false, "csv": "\"%\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"20\",\"$92.26 (0.0)$\",\"$06.3 (1.1)$\",\"$92.05 (0.1)$\",\"$07.2 (0.6)$\",\"$91.59 (0.1)$\",\"$10.9 (0.9)$\",\"$90.69 (0.1)$\",\"$15.7 (0.7)$\",\"$89.78 (0.0)$\",\"$21.8 (1.2)$\"\n\"40\",\"$92.31 (0.1)$\",\"$10.2 (0.9)$\",\"$92.12 (0.1)$\",\"$28.7 (1.6)$\",\"$91.79 (0.1)$\",\"$45.2 (2.7)$\",\"$90.90 (0.1)$\",\"$62.5 (1.5)$\",\"$90.08 (0.1)$\",\"$74.1 (2.6)$\"\n\"60\",\"$92.31 (0.0)$\",\"$14.0 (0.8)$\",\"$91.99 (0.1)$\",\"$45.8 (3.3)$\",\"$91.70 (0.0)$\",\"$68.4 (2.7)$\",\"$90.75 (0.1)$\",\"$85.5 (1.5)$\",\"$89.88 (0.1)$\",\"$92.4 (0.9)$\"\n\"80\",\"$92.48 (0.1)$\",\"$14.0 (1.4)$\",\"$92.02 (0.1)$\",\"$42.5 (3.2)$\",\"$91.70 (0.1)$\",\"$80.0 (2.0)$\",\"$90.92 (0.1)$\",\"$96.6 (0.4)$\",\"$89.98 (0.1)$\",\"$98.5 (0.3)$\""}, {"type": "text", "value": "FLIP still works with limited access to the dataset. In this setting, the attacker is provided 20%, 40%, 60%, or 80% of the user\u2019s dataset but the user\u2019s model is evaluated on the entire dataset. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.", "md": "FLIP still works with limited access to the dataset. In this setting, the attacker is provided 20%, 40%, 60%, or 80% of the user\u2019s dataset but the user\u2019s model is evaluated on the entire dataset. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses."}, {"type": "heading", "lvl": 2, "value": "C.3 Main results on softFLIP for knowledge distillation", "md": "## C.3 Main results on softFLIP for knowledge distillation"}, {"type": "text", "value": "We define softFLIP with a parameter $$\\alpha \\in [0, 1]$$ as, for each image, an interpolation between the soft label (i.e., a vector in the simplex over the classes) that is given by the second step of FLIP and the ground truths one-hot encoded label of that image. Clean label corresponds to $$\\alpha = 1$$. Fig. 7 showcases softFLIP in comparison to the one-hot encoded corruption of FLIP. An expanded version with standard errors is given in Table 13. As a comparison, we show the CTA-PTA trade-off using an attack with a rounded version of softFLIP\u2019s corrupted labels such that all poisoned labels are one-hot encoded in Table 14.", "md": "We define softFLIP with a parameter $$\\alpha \\in [0, 1]$$ as, for each image, an interpolation between the soft label (i.e., a vector in the simplex over the classes) that is given by the second step of FLIP and the ground truths one-hot encoded label of that image. Clean label corresponds to $$\\alpha = 1$$. Fig. 7 showcases softFLIP in comparison to the one-hot encoded corruption of FLIP. An expanded version with standard errors is given in Table 13. As a comparison, we show the CTA-PTA trade-off using an attack with a rounded version of softFLIP\u2019s corrupted labels such that all poisoned labels are one-hot encoded in Table 14."}, {"type": "table", "rows": [["", "0.0", "0.2", "0.4", "0.6", "0.8", "0.9"], ["r32 s", "$90.04 (0.1)$", "$100. (0.0)$", "$90.08 (0.0)$", "$100. (0.0)$", "$90.11 (0.1)$", "$100. (0.0)$"], ["t", "$88.05 (0.1)$", "$100. (0.0)$", "$88.43 (0.1)$", "$100. (0.0)$", "$88.44 (0.1)$", "$100. (0.0)$"], ["p", "$88.02 (0.1)$", "$44.5 (0.4)$", "$88.26 (0.1)$", "$41.9 (0.4)$", "$88.62 (0.1)$", "$38.8 (0.5)$"], ["r18 s", "$92.97 (0.1)$", "$98.7 (0.3)$", "$92.92 (0.1)$", "$97.3 (1.3)$", "$93.13 (0.1)$", "$96.0 (2.1)$"]], "md": "| |0.0|0.2|0.4|0.6|0.8|0.9|\n|---|---|---|---|---|---|---|\n|r32 s|$90.04 (0.1)$|$100. (0.0)$|$90.08 (0.0)$|$100. (0.0)$|$90.11 (0.1)$|$100. (0.0)$|\n|t|$88.05 (0.1)$|$100. (0.0)$|$88.43 (0.1)$|$100. (0.0)$|$88.44 (0.1)$|$100. (0.0)$|\n|p|$88.02 (0.1)$|$44.5 (0.4)$|$88.26 (0.1)$|$41.9 (0.4)$|$88.62 (0.1)$|$38.8 (0.5)$|\n|r18 s|$92.97 (0.1)$|$98.7 (0.3)$|$92.92 (0.1)$|$97.3 (1.3)$|$93.13 (0.1)$|$96.0 (2.1)$|", "isPerfectTable": true, "csv": "\"\",\"0.0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"0.9\"\n\"r32 s\",\"$90.04 (0.1)$\",\"$100. (0.0)$\",\"$90.08 (0.0)$\",\"$100. (0.0)$\",\"$90.11 (0.1)$\",\"$100. (0.0)$\"\n\"t\",\"$88.05 (0.1)$\",\"$100. (0.0)$\",\"$88.43 (0.1)$\",\"$100. (0.0)$\",\"$88.44 (0.1)$\",\"$100. (0.0)$\"\n\"p\",\"$88.02 (0.1)$\",\"$44.5 (0.4)$\",\"$88.26 (0.1)$\",\"$41.9 (0.4)$\",\"$88.62 (0.1)$\",\"$38.8 (0.5)$\"\n\"r18 s\",\"$92.97 (0.1)$\",\"$98.7 (0.3)$\",\"$92.92 (0.1)$\",\"$97.3 (1.3)$\",\"$93.13 (0.1)$\",\"$96.0 (2.1)$\""}, {"type": "text", "value": "Table 13: CTA-PTA trade-off for softFLIP with varying $$\\alpha \\in \\{0.0, 0.2, 0.4, 0.6, 0.8, 0.9\\}$$, varying architecture (ResNet-32 and ResNet-18), and varying trigger patterns (sinusoidal, Turner, pixel). Each point is averaged over 10 runs and standard errors are shown in parentheses.", "md": "Table 13: CTA-PTA trade-off for softFLIP with varying $$\\alpha \\in \\{0.0, 0.2, 0.4, 0.6, 0.8, 0.9\\}$$, varying architecture (ResNet-32 and ResNet-18), and varying trigger patterns (sinusoidal, Turner, pixel). Each point is averaged over 10 runs and standard errors are shown in parentheses."}, {"type": "table", "rows": [["", "0.0", "0.2", "0.4", "0.6", "0.8", "0.9"], ["r32 s", "$89.82 (0.1)$", "$100. (0.0)$", "$89.78 (0.1)$", "$99.9 (0.0)$", "$90.00 (0.1)$", "$99.9 (0.0)$"], ["t", "$87.30 (0.1)$", "$99.6 (0.4)$", "$87.47 (0.1)$", "$99.8 (0.1)$", "$87.83 (0.1)$", "$100. (0.0)$"], ["p", "$87.88 (0.1)$", "$42.1 (0.6)$", "$88.09 (0.1)$", "$39.7 (0.3)$", "$88.45 (0.1)$", "$36.5 (0.8)$"], ["r18 s", "$92.59 (0.1)$", "$95.2 (1.1)$", "$92.82 (0.1)$", "$92.6 (2.1)$", "$92.87 (0.1)$", "$89.7 (1.5)$"]], "md": "| |0.0|0.2|0.4|0.6|0.8|0.9|\n|---|---|---|---|---|---|---|\n|r32 s|$89.82 (0.1)$|$100. (0.0)$|$89.78 (0.1)$|$99.9 (0.0)$|$90.00 (0.1)$|$99.9 (0.0)$|\n|t|$87.30 (0.1)$|$99.6 (0.4)$|$87.47 (0.1)$|$99.8 (0.1)$|$87.83 (0.1)$|$100. (0.0)$|\n|p|$87.88 (0.1)$|$42.1 (0.6)$|$88.09 (0.1)$|$39.7 (0.3)$|$88.45 (0.1)$|$36.5 (0.8)$|\n|r18 s|$92.59 (0.1)$|$95.2 (1.1)$|$92.82 (0.1)$|$92.6 (2.1)$|$92.87 (0.1)$|$89.7 (1.5)$|", "isPerfectTable": true, "csv": "\"\",\"0.0\",\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"0.9\"\n\"r32 s\",\"$89.82 (0.1)$\",\"$100. (0.0)$\",\"$89.78 (0.1)$\",\"$99.9 (0.0)$\",\"$90.00 (0.1)$\",\"$99.9 (0.0)$\"\n\"t\",\"$87.30 (0.1)$\",\"$99.6 (0.4)$\",\"$87.47 (0.1)$\",\"$99.8 (0.1)$\",\"$87.83 (0.1)$\",\"$100. (0.0)$\"\n\"p\",\"$87.88 (0.1)$\",\"$42.1 (0.6)$\",\"$88.09 (0.1)$\",\"$39.7 (0.3)$\",\"$88.45 (0.1)$\",\"$36.5 (0.8)$\"\n\"r18 s\",\"$92.59 (0.1)$\",\"$95.2 (1.1)$\",\"$92.82 (0.1)$\",\"$92.6 (2.1)$\",\"$92.87 (0.1)$\",\"$89.7 (1.5)$\""}, {"type": "text", "value": "Table 14: A rounded version of Table 13, the results are averaged over 10 runs, and the standard errors are shown in parentheses.", "md": "Table 14: A rounded version of Table 13, the results are averaged over 10 runs, and the standard errors are shown in parentheses."}, {"type": "heading", "lvl": 2, "value": "D Additional experiments", "md": "## D Additional experiments"}, {"type": "heading", "lvl": 3, "value": "D.1 Sparse regression approach with \u21131 regularization", "md": "### D.1 Sparse regression approach with \u21131 regularization"}, {"type": "text", "value": "FLIP performs an approximate subset selection (for the subset to be label-corrupted examples) by solving a real-valued optimization and selecting those with highest scores. In principle, one could instead search for sparse deviation from the true labels using \u21131 regularization, as in sparse regression. To that end we experimented with adding the following regularization term to Lparam in Section 2:\n\n$$| \\tilde{\\lambda} \\cdot (yx - \\text{softmax}(\\hat{\\ell}_x)) |_{1},$$\n\nwhere yx refers to the one-hot ground-truth labels for image x. Table 15 shows that this approach has little-to-no improvement over the standard FLIP (which corresponds to $$\\lambda = 0$$).", "md": "FLIP performs an approximate subset selection (for the subset to be label-corrupted examples) by solving a real-valued optimization and selecting those with highest scores. In principle, one could instead search for sparse deviation from the true labels using \u21131 regularization, as in sparse regression. To that end we experimented with adding the following regularization term to Lparam in Section 2:\n\n$$| \\tilde{\\lambda} \\cdot (yx - \\text{softmax}(\\hat{\\ell}_x)) |_{1},$$\n\nwhere yx refers to the one-hot ground-truth labels for image x. Table 15 shows that this approach has little-to-no improvement over the standard FLIP (which corresponds to $$\\lambda = 0$$)."}, {"type": "heading", "lvl": 3, "value": "D.2 FLIP against defenses", "md": "### D.2 FLIP against defenses"}, {"type": "text", "value": "We test the performance of FLIP when state-of-the-art backdoor defenses are applied. We evaluate FLIP on CIFAR-10 with all three trigger types on three popular defenses: kmeans [15], PCA [89], 23", "md": "We test the performance of FLIP when state-of-the-art backdoor defenses are applied. We evaluate FLIP on CIFAR-10 with all three trigger types on three popular defenses: kmeans [15], PCA [89], 23"}]}, {"page": 24, "text": "  \u03bb              150                           300                          500                           1000                          1500\n  0.             92.27 (0.1)  16.2 (1.2)       92.05 (0.0)  59.9 (3.0)      91.48 (0.1)  90.6 (1.5)       90.75 (0.1)  99.5 (0.1)       89.89 (0.0)  99.9 (0.0)\n  0.5            92.31 (0.1)  15.4 (1.6)       91.96 (0.0)  49.4 (2.2)      91.64 (0.1)  90.1 (1.5)       90.62 (0.1)  99.6 (0.1)       89.77 (0.0)  99.8 (0.0)\n  1.             92.29 (0.1)  11.5 (0.8)       91.96 (0.0)  44.3 (2.7)      91.72 (0.1)  90.8 (0.9)       90.70 (0.1)  98.7 (0.1)       89.87 (0.1)  99.8 (0.0)\n  2.             92.35 (0.1)  13.6 (1.6)       92.02 (0.0)  47.9 (2.3)      91.68 (0.0)  94.1 (1.0)       90.76 (0.1)  99.2 (0.1)       89.83 (0.1)  99.7 (0.1)\n  3.             92.19 (0.1)  09.0 (0.6)       91.85 (0.1)  51.1 (1.8)      91.46 (0.1)  88.0 (2.0)       90.70 (0.1)  99.1 (0.1)       89.75 (0.1)  99.6 (0.1)\n  4.             92.32 (0.1)  13.1 (1.4)       92.01 (0.0)  57.4 (2.2)      91.54 (0.1)  89.5 (0.8)       90.57 (0.0)  99.3 (0.1)       89.75 (0.1)  99.6 (0.1)\n  5.             92.24 (0.1)  15.0 (1.6)       92.02 (0.0)  53.3 (2.8)      91.57 (0.1)  86.1 (1.6)       90.45 (0.1)  99.3 (0.1)       89.53 (0.0)  99.7 (0.0)\nTable 15: \u21131-regularization (Eq. (4)) on FLIP does not improve performance. We note that when\n\u03bb = 0, FLIP is as introduced previously. Experiments are computed on CIFAR-10 using the sinusoidal\ntrigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.\nand SPECTRE [37]. We find that SPECTRE is quite effective in mitigating FLIP, whereas the other\ntwo defenses fail on the periodic and Turner triggers. This is consistent with previously reported\nresults showing that SPECTRE is a stronger defense. We emphasize that even strictly stronger attacks\nthat are allowed to corrupt the images fail against SPECTRE. In any case, we hope that our results\nwill encourage practitioners to adopt strong security measures such as SPECTRE in practice, even\nunder the crowd-sourcing and distillation settings with clean images. In our eyes, finding strong\nbackdoor attacks that can bypass SPECTRE is a rewarding future research direction.\n                      150                           300                          500                           1000                          1500\n  s    kmeans         92.28 (0.0)  10.8 (1.3)       92.15 (0.0)  55.6 (3.4)      91.68 (0.1)  84.8 (1.2)       90.78 (0.0)  96.3 (0.7)       90.42 (0.1) 86.3 (8.4)\n       PCA            92.34 (0.1)  11.7 (1.2)       91.95 (0.1)  58.8 (3.6)      91.54 (0.1)  85.3 (1.8)       90.84 (0.1)  98.4 (0.3)       90.40 (0.1) 79.4 (7.4)\n       SPECTRE        92.50 (0.1)  00.2 (0.0)       92.55 (0.0)  00.2 (0.0)      92.43 (0.1)  00.2 (0.0)       92.06 (0.1)  01.3 (0.1)       91.47 (0.1) 01.7 (0.3)\n  p    kmeans         92.13 (0.1)  02.7 (0.2)       91.82 (0.1)  04.9 (0.3)      91.36 (0.1)  08.3 (0.4)       92.37 (0.2)  01.5 (1.4)       88.60 (0.1) 30.4 (0.4)\n       PCA            92.14 (0.1)  02.7 (0.2)       91.83 (0.0)  05.2 (0.1)      91.73 (0.2)  05.9 (1.3)       92.26 (0.1)  02.1 (0.9)       92.09 (0.2) 02.5 (1.6)\n       SPECTRE        92.57 (0.1)  00.0 (0.0)       92.42 (0.1)  00.1 (0.0)      92.54 (0.0)  00.0 (0.0)       92.34 (0.1)  00.1 (0.0)       92.27 (0.0) 00.1 (0.0)\n  t    kmeans         92.32 (0.1)  21.2 (4.4)       92.06 (0.1)  86.5 (7.0)      91.70 (0.1)  95.9 (1.8)       90.75 (0.1)  96.0 (2.2)       90.01 (0.1) 96.4 (2.7)\n       PCA            92.25 (0.0)  36.8 (6.7)       91.97 (0.1)  95.2 (1.7)      91.63 (0.1)  96.8 (1.8)       90.79 (0.1)  99.6 (0.1)       89.95 (0.1) 98.3 (0.5)\n       SPECTRE        92.42 (0.1)  00.1 (0.0)       92.36 (0.1)  00.1 (0.0)      92.17 (0.0)  00.3 (0.1)       91.45 (0.0)  00.7 (0.4)       90.70 (0.1) 03.4 (2.6)\nTable 16: SPECTRE [37] is mitigates our attack when armed with each of our triggers. Experiments\nare computed on CIFAR-10. Each point is averaged over 10 runs and standard errors are shown in\nparentheses.\nD.3       Fine-tuning large pretrained ViTs\nIn this section we show that FLIP is robust under the fine-tuning scenario. In particular, both ResNet-\nand VGG-trained labels successfully backdoor Vision Transformers (VITs) [25] pre-trained on\nImageNet1K where all layers but the weights of classification heads are frozen. When an expert is\ntrained on ResNet-32 and the user fine-tines a model on the FLIPed data starting from a pretrained\nViT, the attack remains quite strong despite the vast difference in the architecture and the initialization\nof the models. The same holds for an expert using VGG-19.\n                     150                           300                           500                           1000                           1500\n  r32 \u2192  vit (FT)    95.42 (0.0)  01.6 (0.1)       95.29 (0.0)  06.4 (0.1)       95.06 (0.0)  14.5 (0.2)       94.67 (0.0)  31.1 (0.3)        94.27 (0.1) 40.2 (0.3)\n  vgg \u2192  vit (FT)    95.40 (0.0)  01.4 (0.1)       95.31 (0.0)  06.9 (0.1)       95.07 (0.0)  16.7 (0.1)       94.64 (0.1)  30.2 (0.1)        93.98 (0.0) 30.7 (0.2)\nTable 17: ResNet and VGG-19 learned corrupted labels successfully poison ImageNet-pretrained\nViT models in the fine-tuning scenario.\n                                                                                  24", "md": "# OCR Text\n\n## Table 15: \u21131-regularization (Eq. (4)) on FLIP does not improve performance.\n\nWe note that when $$\\lambda = 0$$, FLIP is as introduced previously. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.\n\nand SPECTRE [37]. We find that SPECTRE is quite effective in mitigating FLIP, whereas the other two defenses fail on the periodic and Turner triggers. This is consistent with previously reported results showing that SPECTRE is a stronger defense. We emphasize that even strictly stronger attacks that are allowed to corrupt the images fail against SPECTRE. In any case, we hope that our results will encourage practitioners to adopt strong security measures such as SPECTRE in practice, even under the crowd-sourcing and distillation settings with clean images. In our eyes, finding strong backdoor attacks that can bypass SPECTRE is a rewarding future research direction.\n\n|$\\lambda$|150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|$s$|kmeans|92.28 (0.0)|10.8 (1.3)|92.15 (0.0)|55.6 (3.4)|91.68 (0.1)|84.8 (1.2)|90.78 (0.0)|96.3 (0.7)|90.42 (0.1)|86.3 (8.4)|\n| |PCA|92.34 (0.1)|11.7 (1.2)|91.95 (0.1)|58.8 (3.6)|91.54 (0.1)|85.3 (1.8)|90.84 (0.1)|98.4 (0.3)|90.40 (0.1)|79.4 (7.4)|\n| |SPECTRE|92.50 (0.1)|00.2 (0.0)|92.55 (0.0)|00.2 (0.0)|92.43 (0.1)|00.2 (0.0)|92.06 (0.1)|01.3 (0.1)|91.47 (0.1)|01.7 (0.3)|\n|$p$|kmeans|92.13 (0.1)|02.7 (0.2)|91.82 (0.1)|04.9 (0.3)|91.36 (0.1)|08.3 (0.4)|92.37 (0.2)|01.5 (1.4)|88.60 (0.1)|30.4 (0.4)|\n| |PCA|92.14 (0.1)|02.7 (0.2)|91.83 (0.0)|05.2 (0.1)|91.73 (0.2)|05.9 (1.3)|92.26 (0.1)|02.1 (0.9)|92.09 (0.2)|02.5 (1.6)|\n| |SPECTRE|92.57 (0.1)|00.0 (0.0)|92.42 (0.1)|00.1 (0.0)|92.54 (0.0)|00.0 (0.0)|92.34 (0.1)|00.1 (0.0)|92.27 (0.0)|00.1 (0.0)|\n|$t$|kmeans|92.32 (0.1)|21.2 (4.4)|92.06 (0.1)|86.5 (7.0)|91.70 (0.1)|95.9 (1.8)|90.75 (0.1)|96.0 (2.2)|90.01 (0.1)|96.4 (2.7)|\n| |PCA|92.25 (0.0)|36.8 (6.7)|91.97 (0.1)|95.2 (1.7)|91.63 (0.1)|96.8 (1.8)|90.79 (0.1)|99.6 (0.1)|89.95 (0.1)|98.3 (0.5)|\n| |SPECTRE|92.42 (0.1)|00.1 (0.0)|92.36 (0.1)|00.1 (0.0)|92.17 (0.0)|00.3 (0.1)|91.45 (0.0)|00.7 (0.4)|90.70 (0.1)|03.4 (2.6)|\n\n## Table 16: SPECTRE [37] mitigates our attack when armed with each of our triggers.\n\nExperiments are computed on CIFAR-10. Each point is averaged over 10 runs and standard errors are shown in parentheses.\n\n### D.3 Fine-tuning large pretrained ViTs\n\nIn this section we show that FLIP is robust under the fine-tuning scenario. In particular, both ResNet- and VGG-trained labels successfully backdoor Vision Transformers (VITs) [25] pre-trained on ImageNet1K where all layers but the weights of classification heads are frozen. When an expert is trained on ResNet-32 and the user fine-tines a model on the FLIPed data starting from a pretrained ViT, the attack remains quite strong despite the vast difference in the architecture and the initialization of the models. The same holds for an expert using VGG-19.\n\n|$r32 \\rightarrow vit (FT)$|150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|95.42 (0.0)|01.6 (0.1)|95.29 (0.0)|06.4 (0.1)|95.06 (0.0)|14.5 (0.2)|94.67 (0.0)|31.1 (0.3)|94.27 (0.1)|40.2 (0.3)|\n|$vgg \\rightarrow vit (FT)$|150|300|500|1000|1500|\n|95.40 (0.0)|01.4 (0.1)|95.31 (0.0)|06.9 (0.1)|95.07 (0.0)|16.7 (0.1)|94.64 (0.1)|30.2 (0.1)|93.98 (0.0)|30.7 (0.2)|\n\n## Table 17: ResNet and VGG-19 learned corrupted labels successfully poison ImageNet-pretrained ViT models in the fine-tuning scenario.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "heading", "lvl": 2, "value": "Table 15: \u21131-regularization (Eq. (4)) on FLIP does not improve performance.", "md": "## Table 15: \u21131-regularization (Eq. (4)) on FLIP does not improve performance."}, {"type": "text", "value": "We note that when $$\\lambda = 0$$, FLIP is as introduced previously. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.\n\nand SPECTRE [37]. We find that SPECTRE is quite effective in mitigating FLIP, whereas the other two defenses fail on the periodic and Turner triggers. This is consistent with previously reported results showing that SPECTRE is a stronger defense. We emphasize that even strictly stronger attacks that are allowed to corrupt the images fail against SPECTRE. In any case, we hope that our results will encourage practitioners to adopt strong security measures such as SPECTRE in practice, even under the crowd-sourcing and distillation settings with clean images. In our eyes, finding strong backdoor attacks that can bypass SPECTRE is a rewarding future research direction.", "md": "We note that when $$\\lambda = 0$$, FLIP is as introduced previously. Experiments are computed on CIFAR-10 using the sinusoidal trigger. Each point is averaged over 10 runs and standard errors are shown in parentheses.\n\nand SPECTRE [37]. We find that SPECTRE is quite effective in mitigating FLIP, whereas the other two defenses fail on the periodic and Turner triggers. This is consistent with previously reported results showing that SPECTRE is a stronger defense. We emphasize that even strictly stronger attacks that are allowed to corrupt the images fail against SPECTRE. In any case, we hope that our results will encourage practitioners to adopt strong security measures such as SPECTRE in practice, even under the crowd-sourcing and distillation settings with clean images. In our eyes, finding strong backdoor attacks that can bypass SPECTRE is a rewarding future research direction."}, {"type": "table", "rows": [["$\\lambda$", "150", "300", "500", "1000", "1500"], ["$s$", "kmeans", "92.28 (0.0)", "10.8 (1.3)", "92.15 (0.0)", "55.6 (3.4)", "91.68 (0.1)", "84.8 (1.2)", "90.78 (0.0)", "96.3 (0.7)", "90.42 (0.1)", "86.3 (8.4)"], ["", "PCA", "92.34 (0.1)", "11.7 (1.2)", "91.95 (0.1)", "58.8 (3.6)", "91.54 (0.1)", "85.3 (1.8)", "90.84 (0.1)", "98.4 (0.3)", "90.40 (0.1)", "79.4 (7.4)"], ["", "SPECTRE", "92.50 (0.1)", "00.2 (0.0)", "92.55 (0.0)", "00.2 (0.0)", "92.43 (0.1)", "00.2 (0.0)", "92.06 (0.1)", "01.3 (0.1)", "91.47 (0.1)", "01.7 (0.3)"], ["$p$", "kmeans", "92.13 (0.1)", "02.7 (0.2)", "91.82 (0.1)", "04.9 (0.3)", "91.36 (0.1)", "08.3 (0.4)", "92.37 (0.2)", "01.5 (1.4)", "88.60 (0.1)", "30.4 (0.4)"], ["", "PCA", "92.14 (0.1)", "02.7 (0.2)", "91.83 (0.0)", "05.2 (0.1)", "91.73 (0.2)", "05.9 (1.3)", "92.26 (0.1)", "02.1 (0.9)", "92.09 (0.2)", "02.5 (1.6)"], ["", "SPECTRE", "92.57 (0.1)", "00.0 (0.0)", "92.42 (0.1)", "00.1 (0.0)", "92.54 (0.0)", "00.0 (0.0)", "92.34 (0.1)", "00.1 (0.0)", "92.27 (0.0)", "00.1 (0.0)"], ["$t$", "kmeans", "92.32 (0.1)", "21.2 (4.4)", "92.06 (0.1)", "86.5 (7.0)", "91.70 (0.1)", "95.9 (1.8)", "90.75 (0.1)", "96.0 (2.2)", "90.01 (0.1)", "96.4 (2.7)"], ["", "PCA", "92.25 (0.0)", "36.8 (6.7)", "91.97 (0.1)", "95.2 (1.7)", "91.63 (0.1)", "96.8 (1.8)", "90.79 (0.1)", "99.6 (0.1)", "89.95 (0.1)", "98.3 (0.5)"], ["", "SPECTRE", "92.42 (0.1)", "00.1 (0.0)", "92.36 (0.1)", "00.1 (0.0)", "92.17 (0.0)", "00.3 (0.1)", "91.45 (0.0)", "00.7 (0.4)", "90.70 (0.1)", "03.4 (2.6)"]], "md": "|$\\lambda$|150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|$s$|kmeans|92.28 (0.0)|10.8 (1.3)|92.15 (0.0)|55.6 (3.4)|91.68 (0.1)|84.8 (1.2)|90.78 (0.0)|96.3 (0.7)|90.42 (0.1)|86.3 (8.4)|\n| |PCA|92.34 (0.1)|11.7 (1.2)|91.95 (0.1)|58.8 (3.6)|91.54 (0.1)|85.3 (1.8)|90.84 (0.1)|98.4 (0.3)|90.40 (0.1)|79.4 (7.4)|\n| |SPECTRE|92.50 (0.1)|00.2 (0.0)|92.55 (0.0)|00.2 (0.0)|92.43 (0.1)|00.2 (0.0)|92.06 (0.1)|01.3 (0.1)|91.47 (0.1)|01.7 (0.3)|\n|$p$|kmeans|92.13 (0.1)|02.7 (0.2)|91.82 (0.1)|04.9 (0.3)|91.36 (0.1)|08.3 (0.4)|92.37 (0.2)|01.5 (1.4)|88.60 (0.1)|30.4 (0.4)|\n| |PCA|92.14 (0.1)|02.7 (0.2)|91.83 (0.0)|05.2 (0.1)|91.73 (0.2)|05.9 (1.3)|92.26 (0.1)|02.1 (0.9)|92.09 (0.2)|02.5 (1.6)|\n| |SPECTRE|92.57 (0.1)|00.0 (0.0)|92.42 (0.1)|00.1 (0.0)|92.54 (0.0)|00.0 (0.0)|92.34 (0.1)|00.1 (0.0)|92.27 (0.0)|00.1 (0.0)|\n|$t$|kmeans|92.32 (0.1)|21.2 (4.4)|92.06 (0.1)|86.5 (7.0)|91.70 (0.1)|95.9 (1.8)|90.75 (0.1)|96.0 (2.2)|90.01 (0.1)|96.4 (2.7)|\n| |PCA|92.25 (0.0)|36.8 (6.7)|91.97 (0.1)|95.2 (1.7)|91.63 (0.1)|96.8 (1.8)|90.79 (0.1)|99.6 (0.1)|89.95 (0.1)|98.3 (0.5)|\n| |SPECTRE|92.42 (0.1)|00.1 (0.0)|92.36 (0.1)|00.1 (0.0)|92.17 (0.0)|00.3 (0.1)|91.45 (0.0)|00.7 (0.4)|90.70 (0.1)|03.4 (2.6)|", "isPerfectTable": false, "csv": "\"$\\lambda$\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"$s$\",\"kmeans\",\"92.28 (0.0)\",\"10.8 (1.3)\",\"92.15 (0.0)\",\"55.6 (3.4)\",\"91.68 (0.1)\",\"84.8 (1.2)\",\"90.78 (0.0)\",\"96.3 (0.7)\",\"90.42 (0.1)\",\"86.3 (8.4)\"\n\"\",\"PCA\",\"92.34 (0.1)\",\"11.7 (1.2)\",\"91.95 (0.1)\",\"58.8 (3.6)\",\"91.54 (0.1)\",\"85.3 (1.8)\",\"90.84 (0.1)\",\"98.4 (0.3)\",\"90.40 (0.1)\",\"79.4 (7.4)\"\n\"\",\"SPECTRE\",\"92.50 (0.1)\",\"00.2 (0.0)\",\"92.55 (0.0)\",\"00.2 (0.0)\",\"92.43 (0.1)\",\"00.2 (0.0)\",\"92.06 (0.1)\",\"01.3 (0.1)\",\"91.47 (0.1)\",\"01.7 (0.3)\"\n\"$p$\",\"kmeans\",\"92.13 (0.1)\",\"02.7 (0.2)\",\"91.82 (0.1)\",\"04.9 (0.3)\",\"91.36 (0.1)\",\"08.3 (0.4)\",\"92.37 (0.2)\",\"01.5 (1.4)\",\"88.60 (0.1)\",\"30.4 (0.4)\"\n\"\",\"PCA\",\"92.14 (0.1)\",\"02.7 (0.2)\",\"91.83 (0.0)\",\"05.2 (0.1)\",\"91.73 (0.2)\",\"05.9 (1.3)\",\"92.26 (0.1)\",\"02.1 (0.9)\",\"92.09 (0.2)\",\"02.5 (1.6)\"\n\"\",\"SPECTRE\",\"92.57 (0.1)\",\"00.0 (0.0)\",\"92.42 (0.1)\",\"00.1 (0.0)\",\"92.54 (0.0)\",\"00.0 (0.0)\",\"92.34 (0.1)\",\"00.1 (0.0)\",\"92.27 (0.0)\",\"00.1 (0.0)\"\n\"$t$\",\"kmeans\",\"92.32 (0.1)\",\"21.2 (4.4)\",\"92.06 (0.1)\",\"86.5 (7.0)\",\"91.70 (0.1)\",\"95.9 (1.8)\",\"90.75 (0.1)\",\"96.0 (2.2)\",\"90.01 (0.1)\",\"96.4 (2.7)\"\n\"\",\"PCA\",\"92.25 (0.0)\",\"36.8 (6.7)\",\"91.97 (0.1)\",\"95.2 (1.7)\",\"91.63 (0.1)\",\"96.8 (1.8)\",\"90.79 (0.1)\",\"99.6 (0.1)\",\"89.95 (0.1)\",\"98.3 (0.5)\"\n\"\",\"SPECTRE\",\"92.42 (0.1)\",\"00.1 (0.0)\",\"92.36 (0.1)\",\"00.1 (0.0)\",\"92.17 (0.0)\",\"00.3 (0.1)\",\"91.45 (0.0)\",\"00.7 (0.4)\",\"90.70 (0.1)\",\"03.4 (2.6)\""}, {"type": "heading", "lvl": 2, "value": "Table 16: SPECTRE [37] mitigates our attack when armed with each of our triggers.", "md": "## Table 16: SPECTRE [37] mitigates our attack when armed with each of our triggers."}, {"type": "text", "value": "Experiments are computed on CIFAR-10. Each point is averaged over 10 runs and standard errors are shown in parentheses.", "md": "Experiments are computed on CIFAR-10. Each point is averaged over 10 runs and standard errors are shown in parentheses."}, {"type": "heading", "lvl": 3, "value": "D.3 Fine-tuning large pretrained ViTs", "md": "### D.3 Fine-tuning large pretrained ViTs"}, {"type": "text", "value": "In this section we show that FLIP is robust under the fine-tuning scenario. In particular, both ResNet- and VGG-trained labels successfully backdoor Vision Transformers (VITs) [25] pre-trained on ImageNet1K where all layers but the weights of classification heads are frozen. When an expert is trained on ResNet-32 and the user fine-tines a model on the FLIPed data starting from a pretrained ViT, the attack remains quite strong despite the vast difference in the architecture and the initialization of the models. The same holds for an expert using VGG-19.", "md": "In this section we show that FLIP is robust under the fine-tuning scenario. In particular, both ResNet- and VGG-trained labels successfully backdoor Vision Transformers (VITs) [25] pre-trained on ImageNet1K where all layers but the weights of classification heads are frozen. When an expert is trained on ResNet-32 and the user fine-tines a model on the FLIPed data starting from a pretrained ViT, the attack remains quite strong despite the vast difference in the architecture and the initialization of the models. The same holds for an expert using VGG-19."}, {"type": "table", "rows": [["$r32 \\rightarrow vit (FT)$", "150", "300", "500", "1000", "1500"], ["95.42 (0.0)", "01.6 (0.1)", "95.29 (0.0)", "06.4 (0.1)", "95.06 (0.0)", "14.5 (0.2)", "94.67 (0.0)", "31.1 (0.3)", "94.27 (0.1)", "40.2 (0.3)"], ["$vgg \\rightarrow vit (FT)$", "150", "300", "500", "1000", "1500"], ["95.40 (0.0)", "01.4 (0.1)", "95.31 (0.0)", "06.9 (0.1)", "95.07 (0.0)", "16.7 (0.1)", "94.64 (0.1)", "30.2 (0.1)", "93.98 (0.0)", "30.7 (0.2)"]], "md": "|$r32 \\rightarrow vit (FT)$|150|300|500|1000|1500|\n|---|---|---|---|---|---|\n|95.42 (0.0)|01.6 (0.1)|95.29 (0.0)|06.4 (0.1)|95.06 (0.0)|14.5 (0.2)|94.67 (0.0)|31.1 (0.3)|94.27 (0.1)|40.2 (0.3)|\n|$vgg \\rightarrow vit (FT)$|150|300|500|1000|1500|\n|95.40 (0.0)|01.4 (0.1)|95.31 (0.0)|06.9 (0.1)|95.07 (0.0)|16.7 (0.1)|94.64 (0.1)|30.2 (0.1)|93.98 (0.0)|30.7 (0.2)|", "isPerfectTable": false, "csv": "\"$r32 \\rightarrow vit (FT)$\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"95.42 (0.0)\",\"01.6 (0.1)\",\"95.29 (0.0)\",\"06.4 (0.1)\",\"95.06 (0.0)\",\"14.5 (0.2)\",\"94.67 (0.0)\",\"31.1 (0.3)\",\"94.27 (0.1)\",\"40.2 (0.3)\"\n\"$vgg \\rightarrow vit (FT)$\",\"150\",\"300\",\"500\",\"1000\",\"1500\"\n\"95.40 (0.0)\",\"01.4 (0.1)\",\"95.31 (0.0)\",\"06.9 (0.1)\",\"95.07 (0.0)\",\"16.7 (0.1)\",\"94.64 (0.1)\",\"30.2 (0.1)\",\"93.98 (0.0)\",\"30.7 (0.2)\""}, {"type": "heading", "lvl": 2, "value": "Table 17: ResNet and VGG-19 learned corrupted labels successfully poison ImageNet-pretrained ViT models in the fine-tuning scenario.", "md": "## Table 17: ResNet and VGG-19 learned corrupted labels successfully poison ImageNet-pretrained ViT models in the fine-tuning scenario."}]}], "job_id": "f70afdb0-2a6b-4686-a60a-84747082a27a", "file_path": "./corpus/14483_label_poisoning_is_all_you_nee.pdf"}