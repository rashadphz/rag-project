{"pages": [{"page": 1, "text": "                    A Unified Framework for Uniform Signal Recovery in\n                              Nonlinear Generative Compressed Sensing\n                                        Junren Chen\u2217                                Jonathan Scarlett\n                                  University of Hong Kong                    National University of Singapore\n                                chenjr58@connect.hku.hk                       scarlett@comp.nus.edu.sg\narXiv:2310.03758v2  [eess.SP]  9 Oct 2023  Michael K. Ng                                Zhaoqiang Liu\u2217\n                                    Hong Kong Baptist University                            UESTC\n                                    michael-ng@hkbu.edu.hk                           zqliu12@gmail.com\n                                                                  Abstract\n                             In generative compressed sensing (GCS), we want to recover a signal x\u2217         \u2208  Rn\n                             from m measurements (m \u226a        n) using a generative prior x\u2217   \u2208 G(Bk 2(r)), where\n                             G is typically an L-Lipschitz continuous generative model and Bk     2(r) represents\n                             the radius-r \u21132-ball in Rk. Under nonlinear measurements, most prior results are\n                             non-uniform, i.e., they hold with high probability for a fixed x\u2217  rather than for all\n                             x\u2217  simultaneously. In this paper, we build a unified framework to derive uniform\n                             recovery guarantees for nonlinear GCS where the observation model is nonlinear\n                             and possibly discontinuous or unknown. Our framework accommodates GCS\n                             with 1-bit/uniformly quantized observations and single index models as canonical\n                             examples. Specifically, using a single realization of the sensing ensemble and\n                             generalized Lasso, all x\u2217  \u2208 G(Bk 2(r)) can be recovered up to an \u21132-error at most \u03f5\n                             using roughly \u02dcO(k/\u03f52) samples, with omitted logarithmic factors typically being\n                             dominated by log L. Notably, this almost coincides with existing non-uniform\n                             guarantees up to logarithmic factors, hence the uniformity costs very little. As part\n                             of our technical contributions, we introduce the Lipschitz approximation to handle\n                             discontinuous observation models. We also develop a concentration inequality that\n                             produces tighter bounds for product processes whose index sets have low metric\n                             entropy. Experimental results are presented to corroborate our theory.\n                    1    Introduction\n                    In compressed sensing (CS) that concerns the reconstruction of low-complexity signals (typically\n                    sparse signals) [5,6,15], it is standard to employ a random measurement ensemble, i.e., a random\n                    sensing matrix and other randomness that produces the observations. Thus, a recovery guarantee\n                    involving a single draw of the measurement ensemble could be non-uniform or uniform \u2014 the non-\n                    uniform one ensures the accurate recovery of any fixed signal with high probability, while the uniform\n                    one states that one realization of the measurements works simultaneously for all structured signals\n                    of interest. Uniformity is a highly desired property in CS, since in applications the measurement\n                    ensemble is typically fixed and should work for all signals [17]. Besides, the derivation of a uniform\n                    guarantee is often significantly harder than a non-uniform one, making uniformity an interesting\n                    theoretical problem in its own right.\n                       \u2217Corresponding authors.\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Unified Signal Recovery in Nonlinear Generative Compressed Sensing\n\n# A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing\n\nJunren Chen* - University of Hong Kong\n\nJonathan Scarlett - National University of Singapore\n\nEmail: chenjr58@connect.hku.hk, scarlett@comp.nus.edu.sg\n\nMichael K. Ng - Hong Kong Baptist University\n\nZhaoqiang Liu* - UESTC\n\nEmail: michael-ng@hkbu.edu.hk, zqliu12@gmail.com\n\n## Abstract\n\nIn generative compressed sensing (GCS), we want to recover a signal $$x^* \\in \\mathbb{R}^n$$ from m measurements (m \u226a n) using a generative prior $$x^* \\in G(B_k^2(r))$$, where G is typically an L-Lipschitz continuous generative model and $$B_k^2(r)$$ represents the radius-r \u21132-ball in $$\\mathbb{R}^k$$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $$x^*$$ rather than for all $$x^*$$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, all $$x^* \\in G(B_k^2(r))$$ can be recovered up to an \u21132-error at most $$\\epsilon$$ using roughly $$\\tilde{O}(k/\\epsilon^2)$$ samples, with omitted logarithmic factors typically being dominated by $$\\log L$$. Notably, this almost coincides with existing non-uniform guarantees up to logarithmic factors, hence the uniformity costs very little. As part of our technical contributions, we introduce the Lipschitz approximation to handle discontinuous observation models. We also develop a concentration inequality that produces tighter bounds for product processes whose index sets have low metric entropy. Experimental results are presented to corroborate our theory.\n\n## Introduction\n\nIn compressed sensing (CS) that concerns the reconstruction of low-complexity signals (typically sparse signals), it is standard to employ a random measurement ensemble, i.e., a random sensing matrix and other randomness that produces the observations. Thus, a recovery guarantee involving a single draw of the measurement ensemble could be non-uniform or uniform \u2014 the non-uniform one ensures the accurate recovery of any fixed signal with high probability, while the uniform one states that one realization of the measurements works simultaneously for all structured signals of interest. Uniformity is a highly desired property in CS, since in applications the measurement ensemble is typically fixed and should work for all signals. Besides, the derivation of a uniform guarantee is often significantly harder than a non-uniform one, making uniformity an interesting theoretical problem in its own right.\n\n*Corresponding authors.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Unified Signal Recovery in Nonlinear Generative Compressed Sensing", "md": "# Unified Signal Recovery in Nonlinear Generative Compressed Sensing"}, {"type": "heading", "lvl": 1, "value": "A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing", "md": "# A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing"}, {"type": "text", "value": "Junren Chen* - University of Hong Kong\n\nJonathan Scarlett - National University of Singapore\n\nEmail: chenjr58@connect.hku.hk, scarlett@comp.nus.edu.sg\n\nMichael K. Ng - Hong Kong Baptist University\n\nZhaoqiang Liu* - UESTC\n\nEmail: michael-ng@hkbu.edu.hk, zqliu12@gmail.com", "md": "Junren Chen* - University of Hong Kong\n\nJonathan Scarlett - National University of Singapore\n\nEmail: chenjr58@connect.hku.hk, scarlett@comp.nus.edu.sg\n\nMichael K. Ng - Hong Kong Baptist University\n\nZhaoqiang Liu* - UESTC\n\nEmail: michael-ng@hkbu.edu.hk, zqliu12@gmail.com"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "In generative compressed sensing (GCS), we want to recover a signal $$x^* \\in \\mathbb{R}^n$$ from m measurements (m \u226a n) using a generative prior $$x^* \\in G(B_k^2(r))$$, where G is typically an L-Lipschitz continuous generative model and $$B_k^2(r)$$ represents the radius-r \u21132-ball in $$\\mathbb{R}^k$$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $$x^*$$ rather than for all $$x^*$$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, all $$x^* \\in G(B_k^2(r))$$ can be recovered up to an \u21132-error at most $$\\epsilon$$ using roughly $$\\tilde{O}(k/\\epsilon^2)$$ samples, with omitted logarithmic factors typically being dominated by $$\\log L$$. Notably, this almost coincides with existing non-uniform guarantees up to logarithmic factors, hence the uniformity costs very little. As part of our technical contributions, we introduce the Lipschitz approximation to handle discontinuous observation models. We also develop a concentration inequality that produces tighter bounds for product processes whose index sets have low metric entropy. Experimental results are presented to corroborate our theory.", "md": "In generative compressed sensing (GCS), we want to recover a signal $$x^* \\in \\mathbb{R}^n$$ from m measurements (m \u226a n) using a generative prior $$x^* \\in G(B_k^2(r))$$, where G is typically an L-Lipschitz continuous generative model and $$B_k^2(r)$$ represents the radius-r \u21132-ball in $$\\mathbb{R}^k$$. Under nonlinear measurements, most prior results are non-uniform, i.e., they hold with high probability for a fixed $$x^*$$ rather than for all $$x^*$$ simultaneously. In this paper, we build a unified framework to derive uniform recovery guarantees for nonlinear GCS where the observation model is nonlinear and possibly discontinuous or unknown. Our framework accommodates GCS with 1-bit/uniformly quantized observations and single index models as canonical examples. Specifically, using a single realization of the sensing ensemble and generalized Lasso, all $$x^* \\in G(B_k^2(r))$$ can be recovered up to an \u21132-error at most $$\\epsilon$$ using roughly $$\\tilde{O}(k/\\epsilon^2)$$ samples, with omitted logarithmic factors typically being dominated by $$\\log L$$. Notably, this almost coincides with existing non-uniform guarantees up to logarithmic factors, hence the uniformity costs very little. As part of our technical contributions, we introduce the Lipschitz approximation to handle discontinuous observation models. We also develop a concentration inequality that produces tighter bounds for product processes whose index sets have low metric entropy. Experimental results are presented to corroborate our theory."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "In compressed sensing (CS) that concerns the reconstruction of low-complexity signals (typically sparse signals), it is standard to employ a random measurement ensemble, i.e., a random sensing matrix and other randomness that produces the observations. Thus, a recovery guarantee involving a single draw of the measurement ensemble could be non-uniform or uniform \u2014 the non-uniform one ensures the accurate recovery of any fixed signal with high probability, while the uniform one states that one realization of the measurements works simultaneously for all structured signals of interest. Uniformity is a highly desired property in CS, since in applications the measurement ensemble is typically fixed and should work for all signals. Besides, the derivation of a uniform guarantee is often significantly harder than a non-uniform one, making uniformity an interesting theoretical problem in its own right.\n\n*Corresponding authors.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "In compressed sensing (CS) that concerns the reconstruction of low-complexity signals (typically sparse signals), it is standard to employ a random measurement ensemble, i.e., a random sensing matrix and other randomness that produces the observations. Thus, a recovery guarantee involving a single draw of the measurement ensemble could be non-uniform or uniform \u2014 the non-uniform one ensures the accurate recovery of any fixed signal with high probability, while the uniform one states that one realization of the measurements works simultaneously for all structured signals of interest. Uniformity is a highly desired property in CS, since in applications the measurement ensemble is typically fixed and should work for all signals. Besides, the derivation of a uniform guarantee is often significantly harder than a non-uniform one, making uniformity an interesting theoretical problem in its own right.\n\n*Corresponding authors.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "Inspired by the tremendous success of deep generative models in different applications, it was recently\nproposed to use a generative prior to replace the commonly used sparse prior in CS [2], which led to\nnumerical success such as a significant reduction of the measurement number. This new perspective\nfor CS, which we call generative compressed sensing (GCS), has attracted a large volume of research\ninterest, e.g., nonlinear GCS [29, 33, 45], MRI applications [24, 46], and information-theoretic\nbounds [27,34], among others. This paper focuses on the uniform recovery problem for nonlinear\nGCS, which is formally stated below. Our main goal is to build a unified framework that can produce\nuniform recovery guarantees for various nonlinear measurement models.\nProblem: Let Bk    2(r) be the \u21132-ball with radius r in Rk. Suppose that G : Bk           2(r) \u2192    Rn is an\nL-Lipschitz continuous generative model, a1, ..., am \u2208           Rn are the sensing vectors, x\u2217      \u2208  K :=\nG(Bk 2(r)) is the underlying signal, and we have the observations yi = fi(a\u22a4          i x\u2217), i = 1, . . . , m,\nwhere f1(\u00b7), . . . , fm(\u00b7) are possibly unknown,2 possibly random non-linearities. Given a single\nrealization of {ai, fi}m i=1, under what conditions we can uniformly recover all x\u2217           \u2208  K from the\ncorresponding {ai, yi}m   i=1 up to an \u21132-norm error of \u03f5?\n1.1    Related Work\nWe divide the related works into nonlinear CS (based on traditional structures like sparsity) and\nnonlinear GCS.\nNonlinear CS: Beyond the standard linear CS model where one observes yi = a\u22a4              i x\u2217, recent years\nhave witnessed rapidly increasing literature on nonlinear CS. An important nonlinear CS model is\n1-bit CS that only retains the sign yi = sign(a\u22a4   i x\u2217) [3,22,41,42]. Subsequent works also considered\n1-bit CS with dithering yi = sign(a\u22a4    i x\u2217  + \u03c4i) to achieve norm reconstruction under sub-Gaussian\nsensing vectors [9,14,48]. Besides, the benefit of using dithering was found in uniformly quantized\nCS with observation yi = Q\u03b4(a\u22a4                                      \u230a \u00b7        is the uniform quantizer with\n                                   i x\u2217  + \u03c4i), where Q\u03b4(\u00b7) = \u03b4      \u03b4\u230b+ 1 2\nresolution \u03b4 [8,48,52]. Moreover, the authors of [16,43,44] studied the more general single index\nmodel (SIM) where the observation yi = fi(a\u22a4       i x\u2217) involves (possibly) unknown nonlinearity fi.\nWhile the restricted isometry property (RIP) of the sensing matrix A = [a1, ..., am]\u22a4        leads to uniform\nrecovery in linear CS [4,15,49], this is not true in nonlinear CS. In fact, many existing results are non-\nuniform [9,16,21,41,43,44,48], and some uniform guarantees can be found in [7,8,14,17,41,42,52].\nMost of these uniform guarantees suffer from a slower error rate.\nThe most relevant work to this paper is the recent work [17] that described a unified approach to\nuniform signal recovery for nonlinear CS. The authors of [17] showed that in the aforementioned\nmodels with k-sparse x\u2217, a uniform \u21132-norm recovery error of \u03f5 could be achieved via generalized\nLasso using roughly k/\u03f54 measurements [17, Section 4]. In this work, we build a unified framework\nfor uniform signal recovery in nonlinear GCS. To achieve a uniform \u21132-norm error of \u03f5 in the above\nmodels with the generative prior x\u2217     \u2208  G(Bk 2(r)), our framework only requires a number of samples\nproportional to k/\u03f52. Unlike [17] that used the technical results [36] to bound the product process,\nwe develop a concentration inequality that produces a tighter bound in the setting of generative prior,\nthus allowing us to derive a sharper uniform error rate.\nNonlinear GCS: Building on the seminal work by Bora et al. [2], numerous works have investigated\nlinear or nonlinear GCS [1,11,12,19,20,23,25,30,39,40,51], with a recent survey [47] providing\na comprehensive overview. Particularly for nonlinear GCS, 1-bit CS with generative models has\nbeen studied in [26,31,45], and generative priors have been used for SIM in [29,32,33]. In addition,\nscore-based generative models have been applied to nonlinear CS in [10,38].\nThe majority of research for nonlinear GCS focuses on non-uniform recovery, with only a few\nexceptions [33,45]. Specifically, under a generative prior, [33, Section 5] presented uniform recovery\nguarantees for SIM where yi = fi(a\u22a4         i x\u2217) with deterministic Lipschitz fi or fi(x) = sign(x).\nTheir proof technique is based on the local embedding property developed in [31], which is a\ngeometric property that is often problem-dependent and currently only known for 1-bit measurements\nand deterministic Lipschitz link functions. In contrast, our proof technique does not rely on such\n    2In order to establish a unified framework, our recovery method (2.1) involves a parameter T that should\nbe chosen according to fi. For the specific single index model with possibly unknown fi, we can follow prior\nworks [33,43] to assume that Tx\u2217   \u2208  K, and recover x\u2217  without using T. See Remark 5 for more details.\n                                                       2", "md": "Inspired by the tremendous success of deep generative models in different applications, it was recently proposed to use a generative prior to replace the commonly used sparse prior in CS [2], which led to numerical success such as a significant reduction of the measurement number. This new perspective for CS, which we call generative compressed sensing (GCS), has attracted a large volume of research interest, e.g., nonlinear GCS [29, 33, 45], MRI applications [24, 46], and information-theoretic bounds [27,34], among others. This paper focuses on the uniform recovery problem for nonlinear GCS, which is formally stated below. Our main goal is to build a unified framework that can produce uniform recovery guarantees for various nonlinear measurement models.\n\nProblem: Let \\(B_k \\subseteq \\mathbb{R}^k\\) be the \\(\\ell_2\\)-ball with radius \\(r\\) in \\(\\mathbb{R}^k\\). Suppose that \\(G : B_k \\subseteq \\mathbb{R}^k \\to \\mathbb{R}^n\\) is an \\(L\\)-Lipschitz continuous generative model, \\(a_1, \\ldots, a_m \\in \\mathbb{R}^n\\) are the sensing vectors, \\(x^* \\in K := G(B_k)\\) is the underlying signal, and we have the observations \\(y_i = f_i(a_i^T x^*)\\), \\(i = 1, \\ldots, m\\), where \\(f_1(\\cdot), \\ldots, f_m(\\cdot)\\) are possibly unknown, possibly random non-linearities. Given a single realization of \\(\\{a_i, f_i\\}_{i=1}^m\\), under what conditions can we uniformly recover all \\(x^* \\in K\\) from the corresponding \\(\\{a_i, y_i\\}_{i=1}^m\\) up to an \\(\\ell_2\\)-norm error of \\(\\epsilon\\)?\n\n## Related Work\n\nWe divide the related works into nonlinear CS (based on traditional structures like sparsity) and nonlinear GCS.\n\nNonlinear CS: Beyond the standard linear CS model where one observes \\(y_i = a_i^T x^*\\), recent years have witnessed rapidly increasing literature on nonlinear CS. An important nonlinear CS model is 1-bit CS that only retains the sign \\(y_i = \\text{sign}(a_i^T x^*)\\) [3,22,41,42]. Subsequent works also considered 1-bit CS with dithering \\(y_i = \\text{sign}(a_i^T x^* + \\tau_i)\\) to achieve norm reconstruction under sub-Gaussian sensing vectors [9,14,48]. Besides, the benefit of using dithering was found in uniformly quantized CS with observation \\(y_i = Q_\\delta(a_i^T x^* + \\tau_i)\\), where \\(Q_\\delta(\\cdot) = \\delta \\lfloor \\frac{\\cdot}{\\delta} \\rfloor + \\frac{\\delta}{2}\\) resolution \\(\\delta\\) [8,48,52]. Moreover, the authors of [16,43,44] studied the more general single index model (SIM) where the observation \\(y_i = f_i(a_i^T x^*)\\) involves (possibly) unknown nonlinearity \\(f_i\\). While the restricted isometry property (RIP) of the sensing matrix \\(A = [a_1, \\ldots, a_m]^T\\) leads to uniform recovery in linear CS [4,15,49], this is not true in nonlinear CS. In fact, many existing results are non-uniform [9,16,21,41,43,44,48], and some uniform guarantees can be found in [7,8,14,17,41,42,52]. Most of these uniform guarantees suffer from a slower error rate.\n\nThe most relevant work to this paper is the recent work [17] that described a unified approach to uniform signal recovery for nonlinear CS. The authors of [17] showed that in the aforementioned models with \\(k\\)-sparse \\(x^*\\), a uniform \\(\\ell_2\\)-norm recovery error of \\(\\epsilon\\) could be achieved via generalized Lasso using roughly \\(k/\\epsilon^4\\) measurements [17, Section 4]. In this work, we build a unified framework for uniform signal recovery in nonlinear GCS. To achieve a uniform \\(\\ell_2\\)-norm error of \\(\\epsilon\\) in the above models with the generative prior \\(x^* \\in G(B_k)\\), our framework only requires a number of samples proportional to \\(k/\\epsilon^2\\). Unlike [17] that used the technical results [36] to bound the product process, we develop a concentration inequality that produces a tighter bound in the setting of generative prior, thus allowing us to derive a sharper uniform error rate.\n\nNonlinear GCS: Building on the seminal work by Bora et al. [2], numerous works have investigated linear or nonlinear GCS [1,11,12,19,20,23,25,30,39,40,51], with a recent survey [47] providing a comprehensive overview. Particularly for nonlinear GCS, 1-bit CS with generative models has been studied in [26,31,45], and generative priors have been used for SIM in [29,32,33]. In addition, score-based generative models have been applied to nonlinear CS in [10,38].\n\nThe majority of research for nonlinear GCS focuses on non-uniform recovery, with only a few exceptions [33,45]. Specifically, under a generative prior, [33, Section 5] presented uniform recovery guarantees for SIM where \\(y_i = f_i(a_i^T x^*)\\) with deterministic Lipschitz \\(f_i\\) or \\(f_i(x) = \\text{sign}(x)\\). Their proof technique is based on the local embedding property developed in [31], which is a geometric property that is often problem-dependent and currently only known for 1-bit measurements and deterministic Lipschitz link functions. In contrast, our proof technique does not rely on such.\n\n2In order to establish a unified framework, our recovery method (2.1) involves a parameter \\(T\\) that should be chosen according to \\(f_i\\). For the specific single index model with possibly unknown \\(f_i\\), we can follow prior works [33,43] to assume that \\(Tx^* \\in K\\), and recover \\(x^*\\) without using \\(T\\). See Remark 5 for more details.", "images": [], "items": [{"type": "text", "value": "Inspired by the tremendous success of deep generative models in different applications, it was recently proposed to use a generative prior to replace the commonly used sparse prior in CS [2], which led to numerical success such as a significant reduction of the measurement number. This new perspective for CS, which we call generative compressed sensing (GCS), has attracted a large volume of research interest, e.g., nonlinear GCS [29, 33, 45], MRI applications [24, 46], and information-theoretic bounds [27,34], among others. This paper focuses on the uniform recovery problem for nonlinear GCS, which is formally stated below. Our main goal is to build a unified framework that can produce uniform recovery guarantees for various nonlinear measurement models.\n\nProblem: Let \\(B_k \\subseteq \\mathbb{R}^k\\) be the \\(\\ell_2\\)-ball with radius \\(r\\) in \\(\\mathbb{R}^k\\). Suppose that \\(G : B_k \\subseteq \\mathbb{R}^k \\to \\mathbb{R}^n\\) is an \\(L\\)-Lipschitz continuous generative model, \\(a_1, \\ldots, a_m \\in \\mathbb{R}^n\\) are the sensing vectors, \\(x^* \\in K := G(B_k)\\) is the underlying signal, and we have the observations \\(y_i = f_i(a_i^T x^*)\\), \\(i = 1, \\ldots, m\\), where \\(f_1(\\cdot), \\ldots, f_m(\\cdot)\\) are possibly unknown, possibly random non-linearities. Given a single realization of \\(\\{a_i, f_i\\}_{i=1}^m\\), under what conditions can we uniformly recover all \\(x^* \\in K\\) from the corresponding \\(\\{a_i, y_i\\}_{i=1}^m\\) up to an \\(\\ell_2\\)-norm error of \\(\\epsilon\\)?", "md": "Inspired by the tremendous success of deep generative models in different applications, it was recently proposed to use a generative prior to replace the commonly used sparse prior in CS [2], which led to numerical success such as a significant reduction of the measurement number. This new perspective for CS, which we call generative compressed sensing (GCS), has attracted a large volume of research interest, e.g., nonlinear GCS [29, 33, 45], MRI applications [24, 46], and information-theoretic bounds [27,34], among others. This paper focuses on the uniform recovery problem for nonlinear GCS, which is formally stated below. Our main goal is to build a unified framework that can produce uniform recovery guarantees for various nonlinear measurement models.\n\nProblem: Let \\(B_k \\subseteq \\mathbb{R}^k\\) be the \\(\\ell_2\\)-ball with radius \\(r\\) in \\(\\mathbb{R}^k\\). Suppose that \\(G : B_k \\subseteq \\mathbb{R}^k \\to \\mathbb{R}^n\\) is an \\(L\\)-Lipschitz continuous generative model, \\(a_1, \\ldots, a_m \\in \\mathbb{R}^n\\) are the sensing vectors, \\(x^* \\in K := G(B_k)\\) is the underlying signal, and we have the observations \\(y_i = f_i(a_i^T x^*)\\), \\(i = 1, \\ldots, m\\), where \\(f_1(\\cdot), \\ldots, f_m(\\cdot)\\) are possibly unknown, possibly random non-linearities. Given a single realization of \\(\\{a_i, f_i\\}_{i=1}^m\\), under what conditions can we uniformly recover all \\(x^* \\in K\\) from the corresponding \\(\\{a_i, y_i\\}_{i=1}^m\\) up to an \\(\\ell_2\\)-norm error of \\(\\epsilon\\)?"}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "We divide the related works into nonlinear CS (based on traditional structures like sparsity) and nonlinear GCS.\n\nNonlinear CS: Beyond the standard linear CS model where one observes \\(y_i = a_i^T x^*\\), recent years have witnessed rapidly increasing literature on nonlinear CS. An important nonlinear CS model is 1-bit CS that only retains the sign \\(y_i = \\text{sign}(a_i^T x^*)\\) [3,22,41,42]. Subsequent works also considered 1-bit CS with dithering \\(y_i = \\text{sign}(a_i^T x^* + \\tau_i)\\) to achieve norm reconstruction under sub-Gaussian sensing vectors [9,14,48]. Besides, the benefit of using dithering was found in uniformly quantized CS with observation \\(y_i = Q_\\delta(a_i^T x^* + \\tau_i)\\), where \\(Q_\\delta(\\cdot) = \\delta \\lfloor \\frac{\\cdot}{\\delta} \\rfloor + \\frac{\\delta}{2}\\) resolution \\(\\delta\\) [8,48,52]. Moreover, the authors of [16,43,44] studied the more general single index model (SIM) where the observation \\(y_i = f_i(a_i^T x^*)\\) involves (possibly) unknown nonlinearity \\(f_i\\). While the restricted isometry property (RIP) of the sensing matrix \\(A = [a_1, \\ldots, a_m]^T\\) leads to uniform recovery in linear CS [4,15,49], this is not true in nonlinear CS. In fact, many existing results are non-uniform [9,16,21,41,43,44,48], and some uniform guarantees can be found in [7,8,14,17,41,42,52]. Most of these uniform guarantees suffer from a slower error rate.\n\nThe most relevant work to this paper is the recent work [17] that described a unified approach to uniform signal recovery for nonlinear CS. The authors of [17] showed that in the aforementioned models with \\(k\\)-sparse \\(x^*\\), a uniform \\(\\ell_2\\)-norm recovery error of \\(\\epsilon\\) could be achieved via generalized Lasso using roughly \\(k/\\epsilon^4\\) measurements [17, Section 4]. In this work, we build a unified framework for uniform signal recovery in nonlinear GCS. To achieve a uniform \\(\\ell_2\\)-norm error of \\(\\epsilon\\) in the above models with the generative prior \\(x^* \\in G(B_k)\\), our framework only requires a number of samples proportional to \\(k/\\epsilon^2\\). Unlike [17] that used the technical results [36] to bound the product process, we develop a concentration inequality that produces a tighter bound in the setting of generative prior, thus allowing us to derive a sharper uniform error rate.\n\nNonlinear GCS: Building on the seminal work by Bora et al. [2], numerous works have investigated linear or nonlinear GCS [1,11,12,19,20,23,25,30,39,40,51], with a recent survey [47] providing a comprehensive overview. Particularly for nonlinear GCS, 1-bit CS with generative models has been studied in [26,31,45], and generative priors have been used for SIM in [29,32,33]. In addition, score-based generative models have been applied to nonlinear CS in [10,38].\n\nThe majority of research for nonlinear GCS focuses on non-uniform recovery, with only a few exceptions [33,45]. Specifically, under a generative prior, [33, Section 5] presented uniform recovery guarantees for SIM where \\(y_i = f_i(a_i^T x^*)\\) with deterministic Lipschitz \\(f_i\\) or \\(f_i(x) = \\text{sign}(x)\\). Their proof technique is based on the local embedding property developed in [31], which is a geometric property that is often problem-dependent and currently only known for 1-bit measurements and deterministic Lipschitz link functions. In contrast, our proof technique does not rely on such.\n\n2In order to establish a unified framework, our recovery method (2.1) involves a parameter \\(T\\) that should be chosen according to \\(f_i\\). For the specific single index model with possibly unknown \\(f_i\\), we can follow prior works [33,43] to assume that \\(Tx^* \\in K\\), and recover \\(x^*\\) without using \\(T\\). See Remark 5 for more details.", "md": "We divide the related works into nonlinear CS (based on traditional structures like sparsity) and nonlinear GCS.\n\nNonlinear CS: Beyond the standard linear CS model where one observes \\(y_i = a_i^T x^*\\), recent years have witnessed rapidly increasing literature on nonlinear CS. An important nonlinear CS model is 1-bit CS that only retains the sign \\(y_i = \\text{sign}(a_i^T x^*)\\) [3,22,41,42]. Subsequent works also considered 1-bit CS with dithering \\(y_i = \\text{sign}(a_i^T x^* + \\tau_i)\\) to achieve norm reconstruction under sub-Gaussian sensing vectors [9,14,48]. Besides, the benefit of using dithering was found in uniformly quantized CS with observation \\(y_i = Q_\\delta(a_i^T x^* + \\tau_i)\\), where \\(Q_\\delta(\\cdot) = \\delta \\lfloor \\frac{\\cdot}{\\delta} \\rfloor + \\frac{\\delta}{2}\\) resolution \\(\\delta\\) [8,48,52]. Moreover, the authors of [16,43,44] studied the more general single index model (SIM) where the observation \\(y_i = f_i(a_i^T x^*)\\) involves (possibly) unknown nonlinearity \\(f_i\\). While the restricted isometry property (RIP) of the sensing matrix \\(A = [a_1, \\ldots, a_m]^T\\) leads to uniform recovery in linear CS [4,15,49], this is not true in nonlinear CS. In fact, many existing results are non-uniform [9,16,21,41,43,44,48], and some uniform guarantees can be found in [7,8,14,17,41,42,52]. Most of these uniform guarantees suffer from a slower error rate.\n\nThe most relevant work to this paper is the recent work [17] that described a unified approach to uniform signal recovery for nonlinear CS. The authors of [17] showed that in the aforementioned models with \\(k\\)-sparse \\(x^*\\), a uniform \\(\\ell_2\\)-norm recovery error of \\(\\epsilon\\) could be achieved via generalized Lasso using roughly \\(k/\\epsilon^4\\) measurements [17, Section 4]. In this work, we build a unified framework for uniform signal recovery in nonlinear GCS. To achieve a uniform \\(\\ell_2\\)-norm error of \\(\\epsilon\\) in the above models with the generative prior \\(x^* \\in G(B_k)\\), our framework only requires a number of samples proportional to \\(k/\\epsilon^2\\). Unlike [17] that used the technical results [36] to bound the product process, we develop a concentration inequality that produces a tighter bound in the setting of generative prior, thus allowing us to derive a sharper uniform error rate.\n\nNonlinear GCS: Building on the seminal work by Bora et al. [2], numerous works have investigated linear or nonlinear GCS [1,11,12,19,20,23,25,30,39,40,51], with a recent survey [47] providing a comprehensive overview. Particularly for nonlinear GCS, 1-bit CS with generative models has been studied in [26,31,45], and generative priors have been used for SIM in [29,32,33]. In addition, score-based generative models have been applied to nonlinear CS in [10,38].\n\nThe majority of research for nonlinear GCS focuses on non-uniform recovery, with only a few exceptions [33,45]. Specifically, under a generative prior, [33, Section 5] presented uniform recovery guarantees for SIM where \\(y_i = f_i(a_i^T x^*)\\) with deterministic Lipschitz \\(f_i\\) or \\(f_i(x) = \\text{sign}(x)\\). Their proof technique is based on the local embedding property developed in [31], which is a geometric property that is often problem-dependent and currently only known for 1-bit measurements and deterministic Lipschitz link functions. In contrast, our proof technique does not rely on such.\n\n2In order to establish a unified framework, our recovery method (2.1) involves a parameter \\(T\\) that should be chosen according to \\(f_i\\). For the specific single index model with possibly unknown \\(f_i\\), we can follow prior works [33,43] to assume that \\(Tx^* \\in K\\), and recover \\(x^*\\) without using \\(T\\). See Remark 5 for more details."}]}, {"page": 3, "text": "geometric properties and yields a unified framework with more generality. Furthermore, [33] did not\nconsider dithering, which limits their ability to estimate the norm of the signal.\nThe authors of [45] derived a uniform guarantee from dithered 1-bit measurements under bias-free\nReLU neural network generative models, while we obtain a uniform guarantee with the comparable\nrate for more general Lipschitz generative models. Additionally, their recovery program differs\nfrom the generalized Lasso approach (cf. Section 2.1) used in our work. Specifically, they minimize\nan \u21132 loss with \u2225x\u22252   2 as the quadratic term, while generalized Lasso uses \u2225Ax\u22252              2 that depends on\nthe sensing vector. As a result, our approach can be readily generalized to sensing vectors with\nan unknown covariance matrix [33, Section 4.2], unlike [45] that is restricted to isotropic sensing\nvectors. Under random dithering, while [45] only considered 1-bit measurements, we also present\nnew results for uniformly quantized measurements (also referred to as multi-bit quantizer in some\nworks [13]).\n1.2   Contributions\nIn this paper, we build a unified framework for uniform signal recovery in nonlinear GCS. We\nsummarize the paper structure and our main contributions as follows:\n    \u2022 We present Theorem 1 as our main result in Section 2. Under rather general observation\n      models that can be discontinuous or unknown, Theorem 1 states that the uniform recovery of all\n      x\u2217   \u2208 G(Bk  2(r)) up to an \u21132-norm error of \u03f5 can be achieved using roughly O              k log L   samples.\n                                                                                                     \u03f52\n      Specifically, we obtain uniform recovery guarantees for 1-bit GCS, 1-bit GCS with dithering,\n      Lipschitz-continuous SIM, and uniformly quantized GCS with dithering.\n    \u2022 We provide a proof sketch in Section 3. Without using the embedding property as in [33],\n      we handle the discontinuous observation model by constructing a Lipschitz approximation.\n      Compared to [17], we develop a new concentration inequality (Theorem 2) to derive tighter\n      bounds for the product processes arising in the proof.\nWe also perform proof-of-concept experiments on the MNIST [28] and CelebA [35] datasets for\nvarious nonlinear models to demonstrate that by using a single realization of {ai, fi}m                 i=1, we can\nobtain reasonably accurate reconstruction for multiple signals. Due to the page limit, the experimental\nresults and detailed proofs are provided in the supplementary material.\n1.3   Notation\nWe use boldface letters to denote vectors and matrices, while regular letters are used for scalars. For\na vector x, we let \u2225x\u2225q (1 \u2264       q \u2264  \u221e) denote its \u2113q-norm. We use Bn         q (r) := {z \u2208    Rn : \u2225z\u2225q \u2264      r}\nto denote the \u2113q ball in Rn, and (Bn       q (r))c represents its complement. The unit Euclidean sphere\nis denoted by Sn\u22121 := {x \u2208          Rn : \u2225x\u22252 = 1}. We use C, Ci, ci, c to denote absolute constants\nwhose values may differ from line to line. We write A = O(B) or A \u2272                      B (resp. A = \u2126(B) or\nA \u2273   B) if A \u2264     CB for some C (resp. A \u2265          cB for some c). We write A \u224d           B if A = O(B) and\nA = \u2126(B) simultaneously hold. We sometimes use \u02dc              O(\u00b7) to further hide logarithmic factors, where\nthe hidden factors are typically dominated by log L in GCS, or log n in CS. We let N(\u00b5, \u03a3) be the\nGaussian distribution with mean \u00b5 and covariance matrix \u03a3. Given K1, K2 \u2282                        Rn, a \u2208     Rn and\nsome a \u2208    R, we define K1 \u00b1 K2 := {x1 \u00b1 x2 : x1 \u2208                K1, x2 \u2208    K2}, a + K1 := {a} + K1, and\naK1 := {ax : x \u2208       K1}. We also adopt the conventions of a\u2227b = min{a, b}, and a\u2228b = max{a, b}.\n2    Main Results\nWe first give some preliminaries.\nDefinition 1. For a random variable X, we define the sub-Gaussian norm \u2225X\u2225\u03c82 := inf{t > 0 :\nE exp(X2/t2) \u2264       2} and the sub-exponential norm \u2225X\u2225\u03c81 := inf{t > 0 : E exp(|X|/t) \u2264                      2}. X\nis sub-Gaussian (resp. sub-exponential) if \u2225X\u2225\u03c82 < \u221e               (resp. \u2225X\u2225\u03c81 < \u221e). For a random vector\nx \u2208  Rn, we let \u2225x\u2225\u03c82 := supv\u2208Sn\u22121 \u2225v\u22a4x\u2225\u03c82.\nDefinition 2. Let S be a subset of Rn. We say that a subset S0 \u2282                S is an \u03b7-net of S if every point\nin S is at most \u03b7 distance away from some point in S0, i.e., S \u2282             S0 + Bn  2(\u03b7). Given a radius \u03b7, we\n                                                          3", "md": "# Document\n\ngeometric properties and yields a unified framework with more generality. Furthermore, [33] did not\nconsider dithering, which limits their ability to estimate the norm of the signal.\nThe authors of [45] derived a uniform guarantee from dithered 1-bit measurements under bias-free\nReLU neural network generative models, while we obtain a uniform guarantee with the comparable\nrate for more general Lipschitz generative models. Additionally, their recovery program differs\nfrom the generalized Lasso approach (cf. Section 2.1) used in our work. Specifically, they minimize\nan \u21132 loss with $$\\|x\\|_2^2$$ as the quadratic term, while generalized Lasso uses $$\\|Ax\\|_2^2$$ that depends on\nthe sensing vector. As a result, our approach can be readily generalized to sensing vectors with\nan unknown covariance matrix [33, Section 4.2], unlike [45] that is restricted to isotropic sensing\nvectors. Under random dithering, while [45] only considered 1-bit measurements, we also present\nnew results for uniformly quantized measurements (also referred to as multi-bit quantizer in some\nworks [13]).\n\n### Contributions\n\nIn this paper, we build a unified framework for uniform signal recovery in nonlinear GCS. We\nsummarize the paper structure and our main contributions as follows:\n\n- We present Theorem 1 as our main result in Section 2. Under rather general observation\nmodels that can be discontinuous or unknown, Theorem 1 states that the uniform recovery of all\n$x^* \\in G(B_k^2(r))$ up to an \u21132-norm error of \u03f5 can be achieved using roughly $O\\left(\\frac{k \\log L}{\\epsilon^2}\\right)$ samples.\nSpecifically, we obtain uniform recovery guarantees for 1-bit GCS, 1-bit GCS with dithering,\nLipschitz-continuous SIM, and uniformly quantized GCS with dithering.\n- We provide a proof sketch in Section 3. Without using the embedding property as in [33],\nwe handle the discontinuous observation model by constructing a Lipschitz approximation.\nCompared to [17], we develop a new concentration inequality (Theorem 2) to derive tighter\nbounds for the product processes arising in the proof.\n\nWe also perform proof-of-concept experiments on the MNIST [28] and CelebA [35] datasets for\nvarious nonlinear models to demonstrate that by using a single realization of {ai, fi}m, we can\nobtain reasonably accurate reconstruction for multiple signals. Due to the page limit, the experimental\nresults and detailed proofs are provided in the supplementary material.\n\n### Notation\n\nWe use boldface letters to denote vectors and matrices, while regular letters are used for scalars. For\na vector x, we let $$\\|x\\|_q$$ (1 \u2264 q \u2264 \u221e) denote its \u2113q-norm. We use $$B_n^q(r) := \\{z \\in \\mathbb{R}^n : \\|z\\|_q \\leq r\\}$$\nto denote the \u2113q ball in $$\\mathbb{R}^n$$, and $$(B_n^q(r))^c$$ represents its complement. The unit Euclidean sphere\nis denoted by $$S^{n-1} := \\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$$. We use C, Ci, ci, c to denote absolute constants\nwhose values may differ from line to line. We write A = O(B) or A \u2272 B (resp. A = \u2126(B) or\nA \u2273 B) if A \u2264 CB for some C (resp. A \u2265 cB for some c). We write A \u224d B if A = O(B) and\nA = \u2126(B) simultaneously hold. We sometimes use $$\\tilde{O}(\\cdot)$$ to further hide logarithmic factors, where\nthe hidden factors are typically dominated by log L in GCS, or log n in CS. We let N(\u00b5, \u03a3) be the\nGaussian distribution with mean \u00b5 and covariance matrix \u03a3. Given K1, K2 \u2282 $$\\mathbb{R}^n$$, a \u2208 $$\\mathbb{R}^n$$ and\nsome a \u2208 R, we define $$K_1 \\pm K_2 := \\{x_1 \\pm x_2 : x_1 \\in K_1, x_2 \\in K_2\\}$$, $$a + K_1 := \\{a\\} + K_1$$, and\n$$aK_1 := \\{ax : x \\in K_1}$$. We also adopt the conventions of $$a \\wedge b = \\min\\{a, b\\}$$, and $$a \\vee b = \\max\\{a, b\\}$$.\n\n## Main Results\n\nWe first give some preliminaries.\n\nDefinition 1. For a random variable X, we define the sub-Gaussian norm $$\\|X\\|_{\\psi_2} := \\inf\\{t > 0 :\nE \\exp(X^2/t^2) \\leq 2\\}$$ and the sub-exponential norm $$\\|X\\|_{\\psi_1} := \\inf\\{t > 0 : E \\exp(|X|/t) \\leq 2\\}$$. X\nis sub-Gaussian (resp. sub-exponential) if $$\\|X\\|_{\\psi_2} < \\infty$$ (resp. $$\\|X\\|_{\\psi_1} < \\infty$$). For a random vector\nx \u2208 $$\\mathbb{R}^n$$, we let $$\\|x\\|_{\\psi_2} := \\sup_{v \\in S^{n-1}} \\|v^Tx\\|_{\\psi_2}$$.\n\nDefinition 2. Let S be a subset of $$\\mathbb{R}^n$$. We say that a subset S0 \u2282 S is an \u03b7-net of S if every point\nin S is at most \u03b7 distance away from some point in S0, i.e., S \u2282 S0 + $$B_n^2(\\eta)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "geometric properties and yields a unified framework with more generality. Furthermore, [33] did not\nconsider dithering, which limits their ability to estimate the norm of the signal.\nThe authors of [45] derived a uniform guarantee from dithered 1-bit measurements under bias-free\nReLU neural network generative models, while we obtain a uniform guarantee with the comparable\nrate for more general Lipschitz generative models. Additionally, their recovery program differs\nfrom the generalized Lasso approach (cf. Section 2.1) used in our work. Specifically, they minimize\nan \u21132 loss with $$\\|x\\|_2^2$$ as the quadratic term, while generalized Lasso uses $$\\|Ax\\|_2^2$$ that depends on\nthe sensing vector. As a result, our approach can be readily generalized to sensing vectors with\nan unknown covariance matrix [33, Section 4.2], unlike [45] that is restricted to isotropic sensing\nvectors. Under random dithering, while [45] only considered 1-bit measurements, we also present\nnew results for uniformly quantized measurements (also referred to as multi-bit quantizer in some\nworks [13]).", "md": "geometric properties and yields a unified framework with more generality. Furthermore, [33] did not\nconsider dithering, which limits their ability to estimate the norm of the signal.\nThe authors of [45] derived a uniform guarantee from dithered 1-bit measurements under bias-free\nReLU neural network generative models, while we obtain a uniform guarantee with the comparable\nrate for more general Lipschitz generative models. Additionally, their recovery program differs\nfrom the generalized Lasso approach (cf. Section 2.1) used in our work. Specifically, they minimize\nan \u21132 loss with $$\\|x\\|_2^2$$ as the quadratic term, while generalized Lasso uses $$\\|Ax\\|_2^2$$ that depends on\nthe sensing vector. As a result, our approach can be readily generalized to sensing vectors with\nan unknown covariance matrix [33, Section 4.2], unlike [45] that is restricted to isotropic sensing\nvectors. Under random dithering, while [45] only considered 1-bit measurements, we also present\nnew results for uniformly quantized measurements (also referred to as multi-bit quantizer in some\nworks [13])."}, {"type": "heading", "lvl": 3, "value": "Contributions", "md": "### Contributions"}, {"type": "text", "value": "In this paper, we build a unified framework for uniform signal recovery in nonlinear GCS. We\nsummarize the paper structure and our main contributions as follows:\n\n- We present Theorem 1 as our main result in Section 2. Under rather general observation\nmodels that can be discontinuous or unknown, Theorem 1 states that the uniform recovery of all\n$x^* \\in G(B_k^2(r))$ up to an \u21132-norm error of \u03f5 can be achieved using roughly $O\\left(\\frac{k \\log L}{\\epsilon^2}\\right)$ samples.\nSpecifically, we obtain uniform recovery guarantees for 1-bit GCS, 1-bit GCS with dithering,\nLipschitz-continuous SIM, and uniformly quantized GCS with dithering.\n- We provide a proof sketch in Section 3. Without using the embedding property as in [33],\nwe handle the discontinuous observation model by constructing a Lipschitz approximation.\nCompared to [17], we develop a new concentration inequality (Theorem 2) to derive tighter\nbounds for the product processes arising in the proof.\n\nWe also perform proof-of-concept experiments on the MNIST [28] and CelebA [35] datasets for\nvarious nonlinear models to demonstrate that by using a single realization of {ai, fi}m, we can\nobtain reasonably accurate reconstruction for multiple signals. Due to the page limit, the experimental\nresults and detailed proofs are provided in the supplementary material.", "md": "In this paper, we build a unified framework for uniform signal recovery in nonlinear GCS. We\nsummarize the paper structure and our main contributions as follows:\n\n- We present Theorem 1 as our main result in Section 2. Under rather general observation\nmodels that can be discontinuous or unknown, Theorem 1 states that the uniform recovery of all\n$x^* \\in G(B_k^2(r))$ up to an \u21132-norm error of \u03f5 can be achieved using roughly $O\\left(\\frac{k \\log L}{\\epsilon^2}\\right)$ samples.\nSpecifically, we obtain uniform recovery guarantees for 1-bit GCS, 1-bit GCS with dithering,\nLipschitz-continuous SIM, and uniformly quantized GCS with dithering.\n- We provide a proof sketch in Section 3. Without using the embedding property as in [33],\nwe handle the discontinuous observation model by constructing a Lipschitz approximation.\nCompared to [17], we develop a new concentration inequality (Theorem 2) to derive tighter\nbounds for the product processes arising in the proof.\n\nWe also perform proof-of-concept experiments on the MNIST [28] and CelebA [35] datasets for\nvarious nonlinear models to demonstrate that by using a single realization of {ai, fi}m, we can\nobtain reasonably accurate reconstruction for multiple signals. Due to the page limit, the experimental\nresults and detailed proofs are provided in the supplementary material."}, {"type": "heading", "lvl": 3, "value": "Notation", "md": "### Notation"}, {"type": "text", "value": "We use boldface letters to denote vectors and matrices, while regular letters are used for scalars. For\na vector x, we let $$\\|x\\|_q$$ (1 \u2264 q \u2264 \u221e) denote its \u2113q-norm. We use $$B_n^q(r) := \\{z \\in \\mathbb{R}^n : \\|z\\|_q \\leq r\\}$$\nto denote the \u2113q ball in $$\\mathbb{R}^n$$, and $$(B_n^q(r))^c$$ represents its complement. The unit Euclidean sphere\nis denoted by $$S^{n-1} := \\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$$. We use C, Ci, ci, c to denote absolute constants\nwhose values may differ from line to line. We write A = O(B) or A \u2272 B (resp. A = \u2126(B) or\nA \u2273 B) if A \u2264 CB for some C (resp. A \u2265 cB for some c). We write A \u224d B if A = O(B) and\nA = \u2126(B) simultaneously hold. We sometimes use $$\\tilde{O}(\\cdot)$$ to further hide logarithmic factors, where\nthe hidden factors are typically dominated by log L in GCS, or log n in CS. We let N(\u00b5, \u03a3) be the\nGaussian distribution with mean \u00b5 and covariance matrix \u03a3. Given K1, K2 \u2282 $$\\mathbb{R}^n$$, a \u2208 $$\\mathbb{R}^n$$ and\nsome a \u2208 R, we define $$K_1 \\pm K_2 := \\{x_1 \\pm x_2 : x_1 \\in K_1, x_2 \\in K_2\\}$$, $$a + K_1 := \\{a\\} + K_1$$, and\n$$aK_1 := \\{ax : x \\in K_1}$$. We also adopt the conventions of $$a \\wedge b = \\min\\{a, b\\}$$, and $$a \\vee b = \\max\\{a, b\\}$$.", "md": "We use boldface letters to denote vectors and matrices, while regular letters are used for scalars. For\na vector x, we let $$\\|x\\|_q$$ (1 \u2264 q \u2264 \u221e) denote its \u2113q-norm. We use $$B_n^q(r) := \\{z \\in \\mathbb{R}^n : \\|z\\|_q \\leq r\\}$$\nto denote the \u2113q ball in $$\\mathbb{R}^n$$, and $$(B_n^q(r))^c$$ represents its complement. The unit Euclidean sphere\nis denoted by $$S^{n-1} := \\{x \\in \\mathbb{R}^n : \\|x\\|_2 = 1\\}$$. We use C, Ci, ci, c to denote absolute constants\nwhose values may differ from line to line. We write A = O(B) or A \u2272 B (resp. A = \u2126(B) or\nA \u2273 B) if A \u2264 CB for some C (resp. A \u2265 cB for some c). We write A \u224d B if A = O(B) and\nA = \u2126(B) simultaneously hold. We sometimes use $$\\tilde{O}(\\cdot)$$ to further hide logarithmic factors, where\nthe hidden factors are typically dominated by log L in GCS, or log n in CS. We let N(\u00b5, \u03a3) be the\nGaussian distribution with mean \u00b5 and covariance matrix \u03a3. Given K1, K2 \u2282 $$\\mathbb{R}^n$$, a \u2208 $$\\mathbb{R}^n$$ and\nsome a \u2208 R, we define $$K_1 \\pm K_2 := \\{x_1 \\pm x_2 : x_1 \\in K_1, x_2 \\in K_2\\}$$, $$a + K_1 := \\{a\\} + K_1$$, and\n$$aK_1 := \\{ax : x \\in K_1}$$. We also adopt the conventions of $$a \\wedge b = \\min\\{a, b\\}$$, and $$a \\vee b = \\max\\{a, b\\}$$."}, {"type": "heading", "lvl": 2, "value": "Main Results", "md": "## Main Results"}, {"type": "text", "value": "We first give some preliminaries.\n\nDefinition 1. For a random variable X, we define the sub-Gaussian norm $$\\|X\\|_{\\psi_2} := \\inf\\{t > 0 :\nE \\exp(X^2/t^2) \\leq 2\\}$$ and the sub-exponential norm $$\\|X\\|_{\\psi_1} := \\inf\\{t > 0 : E \\exp(|X|/t) \\leq 2\\}$$. X\nis sub-Gaussian (resp. sub-exponential) if $$\\|X\\|_{\\psi_2} < \\infty$$ (resp. $$\\|X\\|_{\\psi_1} < \\infty$$). For a random vector\nx \u2208 $$\\mathbb{R}^n$$, we let $$\\|x\\|_{\\psi_2} := \\sup_{v \\in S^{n-1}} \\|v^Tx\\|_{\\psi_2}$$.\n\nDefinition 2. Let S be a subset of $$\\mathbb{R}^n$$. We say that a subset S0 \u2282 S is an \u03b7-net of S if every point\nin S is at most \u03b7 distance away from some point in S0, i.e., S \u2282 S0 + $$B_n^2(\\eta)$$.", "md": "We first give some preliminaries.\n\nDefinition 1. For a random variable X, we define the sub-Gaussian norm $$\\|X\\|_{\\psi_2} := \\inf\\{t > 0 :\nE \\exp(X^2/t^2) \\leq 2\\}$$ and the sub-exponential norm $$\\|X\\|_{\\psi_1} := \\inf\\{t > 0 : E \\exp(|X|/t) \\leq 2\\}$$. X\nis sub-Gaussian (resp. sub-exponential) if $$\\|X\\|_{\\psi_2} < \\infty$$ (resp. $$\\|X\\|_{\\psi_1} < \\infty$$). For a random vector\nx \u2208 $$\\mathbb{R}^n$$, we let $$\\|x\\|_{\\psi_2} := \\sup_{v \\in S^{n-1}} \\|v^Tx\\|_{\\psi_2}$$.\n\nDefinition 2. Let S be a subset of $$\\mathbb{R}^n$$. We say that a subset S0 \u2282 S is an \u03b7-net of S if every point\nin S is at most \u03b7 distance away from some point in S0, i.e., S \u2282 S0 + $$B_n^2(\\eta)$$."}]}, {"page": 4, "text": "define the covering number N (S, \u03b7) as the minimal cardinality of an \u03b7-net of S. The metric entropy\nof S with respect to radius \u03b7 is defined as H (S, \u03b7) = log N (S, \u03b7).\n2.1   Problem Setup\nWe make the following assumptions on the observation model.\nAssumption 1. Let a \u223c       N(0, In) and let f be a possibly unknown, possibly random non-linearity\nthat is independent of a. Let (ai, fi)m i=1 be i.i.d. copies of (a, f). With a single draw of (ai, fi)m  i=1,\nfor x\u2217  \u2208  K = G(Bk  m2(r)), where G : Bk  2(r) \u2192   Rn is an L-Lipschitz generative model, we observe\n  yi := fi(a\u22a4 i x\u2217)   i=1. We can express the model more compactly as y = f(Ax\u2217), where A =\n[a1, ..., am]\u22a4  \u2208 Rm\u00d7n, f = (f1, ..., fm)\u22a4      and y = (y1, ..., ym)\u22a4   \u2208  Rm.\nIn this work, we consider the generalized Lasso as the recovery method [16,33,43], whose core idea\nis to ignore the non-linearity and minimize the regular \u21132 loss. In addition, we need to specify a\nconstraint that reflects the low-complexity nature of x\u2217, and specifically, we introduce a problem-\ndependent scaling factor T \u2208     R and use the constraint \u201cx \u2208     TK\u201d. Note that this is necessary even\nif the problem is linear; for example, with observations y = 2Ax\u2217, one needs to minimize the \u21132\nloss over \u201cx \u2208   2K\u201d. Also, when the generative prior is given by Tx\u2217        \u2208  K = G(Bk   2(r)), we should\nsimply use \u201cx \u2208    K\u201d as constraint; this is technically equivalent to the treatment adopted in [33] (see\nmore discussions in Remark 5 below). Taken collectively, we consider\n                                        \u02c6\n                                        x = arg min                                                     (2.1)\n                                                 x\u2208T K \u2225y \u2212   Ax\u22252.\nImportantly, we want to achieve uniform recovery of all x\u2217        \u2208  K with a single realization of (A, f).\n2.2   Assumptions\nLet f be the function that characterizes our nonlinear measurements. We introduce several assump-\ntions on f here, and then verify them for specific models in Section 2.3. We define the set of\ndiscontinuities as\n                                Df = {a \u2208     R : f is discontinuous at a}.\nWe define the notion of jump discontinuity as follows.\nDefinition 3. (Jump discontinuity). A function f : R \u2192          R has a jump discontinuity at x0 if both\nL\u2212   := limx\u2192x\u2212  0 f(x) and L+ := limx\u2192x+     0 f(x) exist but L\u2212    \u0338= L+. We simply call the oscillation\nat x0, i.e., |L+ \u2212  L\u2212|, the jump.\nRoughly put, our framework applies to piece-wise Lipschitz continuous fi with (at most) countably\ninfinite jump discontinuities, which have bounded jumps and are well separated. The precise statement\nis given below.\nAssumption 2. For some (B0, L0, \u03b20), the following statement unconditionally holds true for any\nrealization of f (specifically, f1, . . . , fm in our observations):\n    \u2022 Df is one of the following: \u2205, a finite set, or a countably infinite set;\n    \u2022 All discontinuities of f (if any) are jump discontinuities with the jump bounded by B0;\n    \u2022 f is L0-Lipschitz on any interval (a, b) satisfying (a, b) \u2229    Df = \u2205.\n    \u2022 |a \u2212  b| \u2265 \u03b20 holds for any a, b \u2208   Df, a \u0338= b (we set \u03b20 = \u221e     if |Df| \u2264  1).\nFor simplicity, we assume f(x0) = lim      x\u2192x+ 0 f(x) for x0 \u2208   Df.3\nWe note that Assumption 2 is satisfied by L-Lipschitz f with (B0, L0, \u03b20) = (0, L, \u221e), 1-\nbit quantized observation f(\u00b7) = sign(\u00b7 + \u03c4) (\u03c4 is the potential dither, similarly below) with\n(B0, L0, \u03b20) = (2, 0, \u221e), and uniformly quantized observation f(\u00b7) = \u03b4                   \u230a\u00b7+\u03c4           with\n(B0, L0, \u03b20) = (\u03b4, 0, \u03b4).                                                                   \u03b4 \u230b  + 12\n    3This is very mild because the observations are fi(a\u22a4i x), while P(a\u22a4x \u2208    Dfi) = 0 (as Dfi is at most\ncountably infinite and a \u223c N(0, In)).\n                                                      4", "md": "Define the covering number \\(N(S, \\eta)\\) as the minimal cardinality of an \\(\\eta\\)-net of \\(S\\). The metric entropy of \\(S\\) with respect to radius \\(\\eta\\) is defined as \\(H(S, \\eta) = \\log N(S, \\eta)\\).\n\n## Problem Setup\n\nWe make the following assumptions on the observation model.\n\nAssumption 1. Let \\(a \\sim N(0, I_n)\\) and let \\(f\\) be a possibly unknown, possibly random non-linearity that is independent of \\(a\\). Let \\((a_i, f_i)_{i=1}^m\\) be i.i.d. copies of \\((a, f)\\). With a single draw of \\((a_i, f_i)_{i=1}^m\\), for \\(x^* \\in K = G(B_k^{m/2}(r))\\), where \\(G: B_k^2(r) \\to \\mathbb{R}^n\\) is an \\(L\\)-Lipschitz generative model, we observe \\(y_i := f_i(a_i^Tx^*)\\) for \\(i=1\\). We can express the model more compactly as \\(y = f(Ax^*)\\), where \\(A = [a_1, ..., a_m]^T \\in \\mathbb{R}^{m \\times n}\\), \\(f = (f_1, ..., f_m)^T\\), and \\(y = (y_1, ..., y_m)^T \\in \\mathbb{R}^m\\).\n\nIn this work, we consider the generalized Lasso as the recovery method [16,33,43], whose core idea is to ignore the non-linearity and minimize the regular \\(\\ell_2\\) loss. In addition, we need to specify a constraint that reflects the low-complexity nature of \\(x^*\\), and specifically, we introduce a problem-dependent scaling factor \\(T \\in \\mathbb{R}\\) and use the constraint \"x \\in TK\". Note that this is necessary even if the problem is linear; for example, with observations \\(y = 2Ax^*\\), one needs to minimize the \\(\\ell_2\\) loss over \"x \\in 2K\". Also, when the generative prior is given by \\(Tx^* \\in K = G(B_k^2(r))\\), we should simply use \"x \\in K\" as constraint; this is technically equivalent to the treatment adopted in [33] (see more discussions in Remark 5 below). Taken collectively, we consider\n\n\\[\n\\hat{x} = \\arg \\min_{x \\in TK} \\|y - Ax\\|_2^2. \\tag{2.1}\n\\]\nImportantly, we want to achieve uniform recovery of all \\(x^* \\in K\\) with a single realization of \\((A, f)\\).\n\n## Assumptions\n\nLet \\(f\\) be the function that characterizes our nonlinear measurements. We introduce several assumptions on \\(f\\) here, and then verify them for specific models in Section 2.3. We define the set of discontinuities as\n\n\\[\nD_f = \\{a \\in \\mathbb{R} : f \\text{ is discontinuous at } a\\}.\n\\]\nWe define the notion of jump discontinuity as follows.\n\nDefinition 3. (Jump discontinuity). A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) has a jump discontinuity at \\(x_0\\) if both \\(L^- := \\lim_{{x \\to x_0^-}} f(x)\\) and \\(L^+ := \\lim_{{x \\to x_0^+}} f(x)\\) exist but \\(L^- \\neq L^+\\). We simply call the oscillation at \\(x_0\\), i.e., \\(|L^+ - L^-|\\), the jump.\n\nRoughly put, our framework applies to piece-wise Lipschitz continuous \\(f_i\\) with (at most) countably infinite jump discontinuities, which have bounded jumps and are well separated. The precise statement is given below.\n\nAssumption 2. For some \\((B_0, L_0, \\beta_0)\\), the following statement unconditionally holds true for any realization of \\(f\\) (specifically, \\(f_1, ..., f_m\\) in our observations):\n\n- D\\(f\\) is one of the following: \\(\\emptyset\\), a finite set, or a countably infinite set;\n- All discontinuities of \\(f\\) (if any) are jump discontinuities with the jump bounded by \\(B_0\\);\n- \\(f\\) is \\(L_0\\)-Lipschitz on any interval \\((a, b)\\) satisfying \\((a, b) \\cap D_f = \\emptyset\\);\n- \\(|a - b| \\geq \\beta_0\\) holds for any \\(a, b \\in D_f\\), \\(a \\neq b\\) (we set \\(\\beta_0 = \\infty\\) if \\(|D_f| \\leq 1\\)).\n\nFor simplicity, we assume \\(f(x_0) = \\lim_{{x \\to x_0^+}} f(x)\\) for \\(x_0 \\in D_f\\).\n\nWe note that Assumption 2 is satisfied by \\(L\\)-Lipschitz \\(f\\) with \\((B_0, L_0, \\beta_0) = (0, L, \\infty)\\), 1-bit quantized observation \\(f(\\cdot) = \\text{sign}(\\cdot + \\tau)\\) (\\(\\tau\\) is the potential dither, similarly below) with \\((B_0, L_0, \\beta_0) = (2, 0, \\infty\\), and uniformly quantized observation \\(f(\\cdot) = \\delta \\left\\lfloor \\cdot + \\tau \\right\\rfloor\\) with \\((B_0, L_0, \\beta_0) = (\\delta, 0, \\delta)\\). \\(\\delta \\left\\lfloor \\cdot + \\tau \\right\\rfloor + \\frac{1}{2}\\)\n\n3This is very mild because the observations are \\(f_i(a_i^Tx)\\), while \\(P(a^Tx \\in D_{f_i}) = 0\\) (as \\(D_{f_i}\\) is at most countably infinite and \\(a \\sim N(0, I_n)\\).\n\n4", "images": [], "items": [{"type": "text", "value": "Define the covering number \\(N(S, \\eta)\\) as the minimal cardinality of an \\(\\eta\\)-net of \\(S\\). The metric entropy of \\(S\\) with respect to radius \\(\\eta\\) is defined as \\(H(S, \\eta) = \\log N(S, \\eta)\\).", "md": "Define the covering number \\(N(S, \\eta)\\) as the minimal cardinality of an \\(\\eta\\)-net of \\(S\\). The metric entropy of \\(S\\) with respect to radius \\(\\eta\\) is defined as \\(H(S, \\eta) = \\log N(S, \\eta)\\)."}, {"type": "heading", "lvl": 2, "value": "Problem Setup", "md": "## Problem Setup"}, {"type": "text", "value": "We make the following assumptions on the observation model.\n\nAssumption 1. Let \\(a \\sim N(0, I_n)\\) and let \\(f\\) be a possibly unknown, possibly random non-linearity that is independent of \\(a\\). Let \\((a_i, f_i)_{i=1}^m\\) be i.i.d. copies of \\((a, f)\\). With a single draw of \\((a_i, f_i)_{i=1}^m\\), for \\(x^* \\in K = G(B_k^{m/2}(r))\\), where \\(G: B_k^2(r) \\to \\mathbb{R}^n\\) is an \\(L\\)-Lipschitz generative model, we observe \\(y_i := f_i(a_i^Tx^*)\\) for \\(i=1\\). We can express the model more compactly as \\(y = f(Ax^*)\\), where \\(A = [a_1, ..., a_m]^T \\in \\mathbb{R}^{m \\times n}\\), \\(f = (f_1, ..., f_m)^T\\), and \\(y = (y_1, ..., y_m)^T \\in \\mathbb{R}^m\\).\n\nIn this work, we consider the generalized Lasso as the recovery method [16,33,43], whose core idea is to ignore the non-linearity and minimize the regular \\(\\ell_2\\) loss. In addition, we need to specify a constraint that reflects the low-complexity nature of \\(x^*\\), and specifically, we introduce a problem-dependent scaling factor \\(T \\in \\mathbb{R}\\) and use the constraint \"x \\in TK\". Note that this is necessary even if the problem is linear; for example, with observations \\(y = 2Ax^*\\), one needs to minimize the \\(\\ell_2\\) loss over \"x \\in 2K\". Also, when the generative prior is given by \\(Tx^* \\in K = G(B_k^2(r))\\), we should simply use \"x \\in K\" as constraint; this is technically equivalent to the treatment adopted in [33] (see more discussions in Remark 5 below). Taken collectively, we consider\n\n\\[\n\\hat{x} = \\arg \\min_{x \\in TK} \\|y - Ax\\|_2^2. \\tag{2.1}\n\\]\nImportantly, we want to achieve uniform recovery of all \\(x^* \\in K\\) with a single realization of \\((A, f)\\).", "md": "We make the following assumptions on the observation model.\n\nAssumption 1. Let \\(a \\sim N(0, I_n)\\) and let \\(f\\) be a possibly unknown, possibly random non-linearity that is independent of \\(a\\). Let \\((a_i, f_i)_{i=1}^m\\) be i.i.d. copies of \\((a, f)\\). With a single draw of \\((a_i, f_i)_{i=1}^m\\), for \\(x^* \\in K = G(B_k^{m/2}(r))\\), where \\(G: B_k^2(r) \\to \\mathbb{R}^n\\) is an \\(L\\)-Lipschitz generative model, we observe \\(y_i := f_i(a_i^Tx^*)\\) for \\(i=1\\). We can express the model more compactly as \\(y = f(Ax^*)\\), where \\(A = [a_1, ..., a_m]^T \\in \\mathbb{R}^{m \\times n}\\), \\(f = (f_1, ..., f_m)^T\\), and \\(y = (y_1, ..., y_m)^T \\in \\mathbb{R}^m\\).\n\nIn this work, we consider the generalized Lasso as the recovery method [16,33,43], whose core idea is to ignore the non-linearity and minimize the regular \\(\\ell_2\\) loss. In addition, we need to specify a constraint that reflects the low-complexity nature of \\(x^*\\), and specifically, we introduce a problem-dependent scaling factor \\(T \\in \\mathbb{R}\\) and use the constraint \"x \\in TK\". Note that this is necessary even if the problem is linear; for example, with observations \\(y = 2Ax^*\\), one needs to minimize the \\(\\ell_2\\) loss over \"x \\in 2K\". Also, when the generative prior is given by \\(Tx^* \\in K = G(B_k^2(r))\\), we should simply use \"x \\in K\" as constraint; this is technically equivalent to the treatment adopted in [33] (see more discussions in Remark 5 below). Taken collectively, we consider\n\n\\[\n\\hat{x} = \\arg \\min_{x \\in TK} \\|y - Ax\\|_2^2. \\tag{2.1}\n\\]\nImportantly, we want to achieve uniform recovery of all \\(x^* \\in K\\) with a single realization of \\((A, f)\\)."}, {"type": "heading", "lvl": 2, "value": "Assumptions", "md": "## Assumptions"}, {"type": "text", "value": "Let \\(f\\) be the function that characterizes our nonlinear measurements. We introduce several assumptions on \\(f\\) here, and then verify them for specific models in Section 2.3. We define the set of discontinuities as\n\n\\[\nD_f = \\{a \\in \\mathbb{R} : f \\text{ is discontinuous at } a\\}.\n\\]\nWe define the notion of jump discontinuity as follows.\n\nDefinition 3. (Jump discontinuity). A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) has a jump discontinuity at \\(x_0\\) if both \\(L^- := \\lim_{{x \\to x_0^-}} f(x)\\) and \\(L^+ := \\lim_{{x \\to x_0^+}} f(x)\\) exist but \\(L^- \\neq L^+\\). We simply call the oscillation at \\(x_0\\), i.e., \\(|L^+ - L^-|\\), the jump.\n\nRoughly put, our framework applies to piece-wise Lipschitz continuous \\(f_i\\) with (at most) countably infinite jump discontinuities, which have bounded jumps and are well separated. The precise statement is given below.\n\nAssumption 2. For some \\((B_0, L_0, \\beta_0)\\), the following statement unconditionally holds true for any realization of \\(f\\) (specifically, \\(f_1, ..., f_m\\) in our observations):\n\n- D\\(f\\) is one of the following: \\(\\emptyset\\), a finite set, or a countably infinite set;\n- All discontinuities of \\(f\\) (if any) are jump discontinuities with the jump bounded by \\(B_0\\);\n- \\(f\\) is \\(L_0\\)-Lipschitz on any interval \\((a, b)\\) satisfying \\((a, b) \\cap D_f = \\emptyset\\);\n- \\(|a - b| \\geq \\beta_0\\) holds for any \\(a, b \\in D_f\\), \\(a \\neq b\\) (we set \\(\\beta_0 = \\infty\\) if \\(|D_f| \\leq 1\\)).\n\nFor simplicity, we assume \\(f(x_0) = \\lim_{{x \\to x_0^+}} f(x)\\) for \\(x_0 \\in D_f\\).\n\nWe note that Assumption 2 is satisfied by \\(L\\)-Lipschitz \\(f\\) with \\((B_0, L_0, \\beta_0) = (0, L, \\infty)\\), 1-bit quantized observation \\(f(\\cdot) = \\text{sign}(\\cdot + \\tau)\\) (\\(\\tau\\) is the potential dither, similarly below) with \\((B_0, L_0, \\beta_0) = (2, 0, \\infty\\), and uniformly quantized observation \\(f(\\cdot) = \\delta \\left\\lfloor \\cdot + \\tau \\right\\rfloor\\) with \\((B_0, L_0, \\beta_0) = (\\delta, 0, \\delta)\\). \\(\\delta \\left\\lfloor \\cdot + \\tau \\right\\rfloor + \\frac{1}{2}\\)\n\n3This is very mild because the observations are \\(f_i(a_i^Tx)\\), while \\(P(a^Tx \\in D_{f_i}) = 0\\) (as \\(D_{f_i}\\) is at most countably infinite and \\(a \\sim N(0, I_n)\\).\n\n4", "md": "Let \\(f\\) be the function that characterizes our nonlinear measurements. We introduce several assumptions on \\(f\\) here, and then verify them for specific models in Section 2.3. We define the set of discontinuities as\n\n\\[\nD_f = \\{a \\in \\mathbb{R} : f \\text{ is discontinuous at } a\\}.\n\\]\nWe define the notion of jump discontinuity as follows.\n\nDefinition 3. (Jump discontinuity). A function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) has a jump discontinuity at \\(x_0\\) if both \\(L^- := \\lim_{{x \\to x_0^-}} f(x)\\) and \\(L^+ := \\lim_{{x \\to x_0^+}} f(x)\\) exist but \\(L^- \\neq L^+\\). We simply call the oscillation at \\(x_0\\), i.e., \\(|L^+ - L^-|\\), the jump.\n\nRoughly put, our framework applies to piece-wise Lipschitz continuous \\(f_i\\) with (at most) countably infinite jump discontinuities, which have bounded jumps and are well separated. The precise statement is given below.\n\nAssumption 2. For some \\((B_0, L_0, \\beta_0)\\), the following statement unconditionally holds true for any realization of \\(f\\) (specifically, \\(f_1, ..., f_m\\) in our observations):\n\n- D\\(f\\) is one of the following: \\(\\emptyset\\), a finite set, or a countably infinite set;\n- All discontinuities of \\(f\\) (if any) are jump discontinuities with the jump bounded by \\(B_0\\);\n- \\(f\\) is \\(L_0\\)-Lipschitz on any interval \\((a, b)\\) satisfying \\((a, b) \\cap D_f = \\emptyset\\);\n- \\(|a - b| \\geq \\beta_0\\) holds for any \\(a, b \\in D_f\\), \\(a \\neq b\\) (we set \\(\\beta_0 = \\infty\\) if \\(|D_f| \\leq 1\\)).\n\nFor simplicity, we assume \\(f(x_0) = \\lim_{{x \\to x_0^+}} f(x)\\) for \\(x_0 \\in D_f\\).\n\nWe note that Assumption 2 is satisfied by \\(L\\)-Lipschitz \\(f\\) with \\((B_0, L_0, \\beta_0) = (0, L, \\infty)\\), 1-bit quantized observation \\(f(\\cdot) = \\text{sign}(\\cdot + \\tau)\\) (\\(\\tau\\) is the potential dither, similarly below) with \\((B_0, L_0, \\beta_0) = (2, 0, \\infty\\), and uniformly quantized observation \\(f(\\cdot) = \\delta \\left\\lfloor \\cdot + \\tau \\right\\rfloor\\) with \\((B_0, L_0, \\beta_0) = (\\delta, 0, \\delta)\\). \\(\\delta \\left\\lfloor \\cdot + \\tau \\right\\rfloor + \\frac{1}{2}\\)\n\n3This is very mild because the observations are \\(f_i(a_i^Tx)\\), while \\(P(a^Tx \\in D_{f_i}) = 0\\) (as \\(D_{f_i}\\) is at most countably infinite and \\(a \\sim N(0, I_n)\\).\n\n4"}]}, {"page": 5, "text": " Under Asssumption 2, for any \u03b2 \u2208                             [0, \u03b202 ) we construct fi,\u03b2 as the Lipschitz approximation of\n fi to deal with the potential discontinuity of fi (i.e., Df                                 i \u0338= \u2205). Specifically, fi,\u03b2 modifies fi in\n Dfi + [\u2212\u03b22 , \u03b22 ] to be piece-wise linear and Lipschitz continuous; see its precise definition in (3.4).\n We develop Theorem 2 to bound certain product processes appearing in the analysis, which produces\n bounds tighter than [36] when the index sets have low metric entropy. To make Theorem 2 applicable,\n we further make the following Assumption 3, which can be checked case-by-case by estimating the\n sub-Gaussian norm and probability tail. Also, U (1)                            g     and U (2) g     can even be a bit crude because the\n measurement number in Theorem 1 depends on them in a logarithmic manner.\n Assumption 3. Let a \u223c                    N   (0, In), under Assumptions 1-2, we define the Lipschitz approximation\n fi,\u03b2 as in (3.4). We let\n                                      \u03bei,\u03b2(a) := fi,\u03b2(a) \u2212              T a, \u03b5i,\u03b2(a) := fi,\u03b2(a) \u2212                fi(a).                                 (2.2)\n For all \u03b2 \u2208         (0, \u03b20 2 ), we assume the following holds with some parameters (A(1)                                       g , U (1)g , P (1)0    ) and\n(A(2)g , U (2)g , P (2)0    ):\n      \u2022 sup     x\u2208K \u2225\u03bei,\u03b2(a\u22a4x)\u2225\u03c82 \u2264                  A(1)           supx\u2208K |\u03bei,\u03b2(a\u22a4x)| \u2264                  U (1)      \u2265   1 \u2212    P (1);\n                                                        g , P                                                g                     0\n      \u2022 sup     x\u2208K \u2225\u03b5i,\u03b2(a\u22a4x)\u2225\u03c82 \u2264                   A(2)          supx\u2208K |\u03b5i,\u03b2(a\u22a4x)| \u2264                   U (2)     \u2265   1 \u2212    P (2).\n                                                         g , P                                               g                     0\n To build a more complete theory we further introduce two useful quantities. For some x \u2208                                                             K, we\n define the target mismatch \u03c1(x) as in [17, Definition 1]:\n                                                     \u03c1(x) =        E     fi(a\u22a4  i x)ai       \u2212   T  x    2.                                             (2.3)\n It is easy to see that E              fi(a\u22a4  i x)ai        minimizes the expected \u21132 loss E                        \u2225y \u2212      Ax\u22252    2  , thus one can\n roughly understand E                 fi(a\u22a4   i x)ai       as the expectation of \u02c6            x. Since T        x is the desired ground truth,\n a small \u03c1(x) is intuitively an important ingredient for generalized Lasso to succeed. Fortunately,\n in many models, \u03c1(x) with a suitably chosen T will vanish (e.g., linear model [2], single index\n model [33], 1-bit model [31]) or at least be sufficiently small (e.g., 1-bit model with dithering [45]).\n As mentioned before, our method to deal with discontinuity of fi is to introduce its approximation\n fi,\u03b2, which differs from fi only in Df                       i + [\u2212\u03b22 , \u03b22 ]. This will produce some bias because the actual\n observation is fi(a\u22a4           i x\u2217) rather than fi,\u03b2(a\u22a4              i x\u2217). Hence, for some x \u2208                    K we define the following\n quantity to measure the bias induced by fi,\u03b2:\n                                  \u00b5\u03b2(x) = P            a\u22a4x \u2208        Dfi +         \u2212   \u03b22 , \u03b22      , a \u223c      N   (0, In).                              (2.4)\n The following assumption can often be satisfied by choosing suitable T and sufficiently small \u03b21.\n Assumption 4. Suppose Assumptions 1-3 hold true with parameters B0, L0, \u03b20, A(1)                                                    g , A(2) g . For the\n T used in (2.1), \u03c1(x) defined in (2.3) satisfies\n                                                       sup    \u03c1(x) \u2272       (A(1)    \u2228   A(2)          k                                                 (2.5)\n Moreover, there exists some 0 < \u03b21 < \u03b20              x\u2208K       2 such that    g           g )       m.\n                                       (L0\u03b21 + B0) sup                   \u00b5\u03b2  1(x) \u2272       (A(1)    \u2228   A(2)          k                                  (2.6)\n                                                             x\u2208K                              g           g )       m.\n In the proof, the estimation error \u2225\u02c6                     x \u2212      T x\u2217\u2225     is contributed by a concentration term of scaling\n O\u02dc   (A(1)     \u2228  A(2)         k/m        and some bias terms. The main aim of Assumption 4 is to pull down the\n          g           g )\n bias terms so that the concentration term is dominant.\n 2.3      Main Theorem and its Implications\n We now present our general theorem and apply it to some specific models.\n                                                                               5", "md": "Under Assumption 2, for any $$\\beta \\in [0, \\beta_{0}^{2})$$ we construct $$f_{i,\\beta}$$ as the Lipschitz approximation of $$f_{i}$$ to deal with the potential discontinuity of $$f_{i}$$ (i.e., $$Df_{i} \\neq \\emptyset$$). Specifically, $$f_{i,\\beta}$$ modifies $$f_{i}$$ in $$Df_{i} + [-\\beta^{2}, \\beta^{2}]$$ to be piece-wise linear and Lipschitz continuous; see its precise definition in (3.4).\n\nWe develop Theorem 2 to bound certain product processes appearing in the analysis, which produces bounds tighter than [36] when the index sets have low metric entropy. To make Theorem 2 applicable, we further make the following Assumption 3, which can be checked case-by-case by estimating the sub-Gaussian norm and probability tail. Also, $$U(1)_{g}$$ and $$U(2)_{g}$$ can even be a bit crude because the measurement number in Theorem 1 depends on them in a logarithmic manner.\n\nAssumption 3. Let $$a \\sim N(0, I_{n})$$, under Assumptions 1-2, we define the Lipschitz approximation $$f_{i,\\beta}$$ as in (3.4). We let\n\n$$\\xi_{i,\\beta}(a) := f_{i,\\beta}(a) - Ta, \\quad \\varepsilon_{i,\\beta}(a) := f_{i,\\beta}(a) - f_{i}(a).$$\n\nFor all $$\\beta \\in (0, \\beta_{0}^{2})$$, we assume the following holds with some parameters $$(A(1)_{g}, U(1)_{g}, P(1)_{0})$$ and $$(A(2)_{g}, U(2)_{g}, P(2)_{0})$$:\n- $$\\sup_{x \\in K} \\|\\xi_{i,\\beta}(a^{\\top}x)\\|_{\\psi_{2}} \\leq A(1)_{g}, \\quad \\sup_{x \\in K} |\\xi_{i,\\beta}(a^{\\top}x)| \\leq U(1)_{g} \\geq 1 - P(1)_{0};$$\n- $$\\sup_{x \\in K} \\|\\varepsilon_{i,\\beta}(a^{\\top}x)\\|_{\\psi_{2}} \\leq A(2)_{g}, \\quad \\sup_{x \\in K} |\\varepsilon_{i,\\beta}(a^{\\top}x)| \\leq U(2)_{g} \\geq 1 - P(2)_{0}.$$\n\nTo build a more complete theory we further introduce two useful quantities. For some $$x \\in K$$, we define the target mismatch $$\\rho(x)$$ as in [17, Definition 1]:\n\n$$\\rho(x) = \\mathbb{E} [f_{i}(a^{\\top}x)a_{i}] - Tx^{2}.$$\n\nIt is easy to see that $$\\mathbb{E} [f_{i}(a^{\\top}x)a_{i}]$$ minimizes the expected $$\\ell_{2}$$ loss $$\\mathbb{E} [\\|y - Ax\\|_{2}^{2}]$$, thus one can roughly understand $$\\mathbb{E} [f_{i}(a^{\\top}x)a_{i}]$$ as the expectation of $$\\hat{x}$$. Since $$Tx$$ is the desired ground truth, a small $$\\rho(x)$$ is intuitively an important ingredient for generalized Lasso to succeed. Fortunately, in many models, $$\\rho(x)$$ with a suitably chosen $$T$$ will vanish (e.g., linear model [2], single index model [33], 1-bit model [31]) or at least be sufficiently small (e.g., 1-bit model with dithering [45]).\n\nAs mentioned before, our method to deal with discontinuity of $$f_{i}$$ is to introduce its approximation $$f_{i,\\beta}$$, which differs from $$f_{i}$$ only in $$Df_{i} + [-\\beta^{2}, \\beta^{2}]$$. This will produce some bias because the actual observation is $$f_{i}(a^{\\top}x^{*})$$ rather than $$f_{i,\\beta}(a^{\\top}x^{*})$$. Hence, for some $$x \\in K$$ we define the following quantity to measure the bias induced by $$f_{i,\\beta}$$:\n\n$$\\mu_{\\beta}(x) = \\mathbb{P} [a^{\\top}x \\in Df_{i} + [-\\beta^{2}, \\beta^{2}], a \\sim N(0, I_{n}]).$$\n\nThe following assumption can often be satisfied by choosing suitable $$T$$ and sufficiently small $$\\beta_{1}$$.\n\nAssumption 4. Suppose Assumptions 1-3 hold true with parameters $$B_{0}, L_{0}, \\beta_{0}, A(1)_{g}, A(2)_{g}$$. For the $$T$$ used in (2.1), $$\\rho(x)$$ defined in (2.3) satisfies\n\n$$\\sup_{x \\in K} \\rho(x) \\lesssim (A(1) \\vee A(2))_{k}.$$\n\nMoreover, there exists some $$0 < \\beta_{1} < \\beta_{0}^{2}$$ such that\n\n$$(L_{0}\\beta_{1} + B_{0}) \\sup_{x \\in K} \\mu_{\\beta_{1}}(x) \\lesssim (A(1) \\vee A(2))_{k}.$$\n\nIn the proof, the estimation error $$\\|\\hat{x} - Tx^{*}\\|$$ is contributed by a concentration term of scaling $$O\\tilde{}(A(1) \\vee A(2))_{k/m}$$ and some bias terms. The main aim of Assumption 4 is to pull down the bias terms so that the concentration term is dominant.\n\n### Main Theorem and its Implications\n\nWe now present our general theorem and apply it to some specific models.", "images": [], "items": [{"type": "text", "value": "Under Assumption 2, for any $$\\beta \\in [0, \\beta_{0}^{2})$$ we construct $$f_{i,\\beta}$$ as the Lipschitz approximation of $$f_{i}$$ to deal with the potential discontinuity of $$f_{i}$$ (i.e., $$Df_{i} \\neq \\emptyset$$). Specifically, $$f_{i,\\beta}$$ modifies $$f_{i}$$ in $$Df_{i} + [-\\beta^{2}, \\beta^{2}]$$ to be piece-wise linear and Lipschitz continuous; see its precise definition in (3.4).\n\nWe develop Theorem 2 to bound certain product processes appearing in the analysis, which produces bounds tighter than [36] when the index sets have low metric entropy. To make Theorem 2 applicable, we further make the following Assumption 3, which can be checked case-by-case by estimating the sub-Gaussian norm and probability tail. Also, $$U(1)_{g}$$ and $$U(2)_{g}$$ can even be a bit crude because the measurement number in Theorem 1 depends on them in a logarithmic manner.\n\nAssumption 3. Let $$a \\sim N(0, I_{n})$$, under Assumptions 1-2, we define the Lipschitz approximation $$f_{i,\\beta}$$ as in (3.4). We let\n\n$$\\xi_{i,\\beta}(a) := f_{i,\\beta}(a) - Ta, \\quad \\varepsilon_{i,\\beta}(a) := f_{i,\\beta}(a) - f_{i}(a).$$\n\nFor all $$\\beta \\in (0, \\beta_{0}^{2})$$, we assume the following holds with some parameters $$(A(1)_{g}, U(1)_{g}, P(1)_{0})$$ and $$(A(2)_{g}, U(2)_{g}, P(2)_{0})$$:\n- $$\\sup_{x \\in K} \\|\\xi_{i,\\beta}(a^{\\top}x)\\|_{\\psi_{2}} \\leq A(1)_{g}, \\quad \\sup_{x \\in K} |\\xi_{i,\\beta}(a^{\\top}x)| \\leq U(1)_{g} \\geq 1 - P(1)_{0};$$\n- $$\\sup_{x \\in K} \\|\\varepsilon_{i,\\beta}(a^{\\top}x)\\|_{\\psi_{2}} \\leq A(2)_{g}, \\quad \\sup_{x \\in K} |\\varepsilon_{i,\\beta}(a^{\\top}x)| \\leq U(2)_{g} \\geq 1 - P(2)_{0}.$$\n\nTo build a more complete theory we further introduce two useful quantities. For some $$x \\in K$$, we define the target mismatch $$\\rho(x)$$ as in [17, Definition 1]:\n\n$$\\rho(x) = \\mathbb{E} [f_{i}(a^{\\top}x)a_{i}] - Tx^{2}.$$\n\nIt is easy to see that $$\\mathbb{E} [f_{i}(a^{\\top}x)a_{i}]$$ minimizes the expected $$\\ell_{2}$$ loss $$\\mathbb{E} [\\|y - Ax\\|_{2}^{2}]$$, thus one can roughly understand $$\\mathbb{E} [f_{i}(a^{\\top}x)a_{i}]$$ as the expectation of $$\\hat{x}$$. Since $$Tx$$ is the desired ground truth, a small $$\\rho(x)$$ is intuitively an important ingredient for generalized Lasso to succeed. Fortunately, in many models, $$\\rho(x)$$ with a suitably chosen $$T$$ will vanish (e.g., linear model [2], single index model [33], 1-bit model [31]) or at least be sufficiently small (e.g., 1-bit model with dithering [45]).\n\nAs mentioned before, our method to deal with discontinuity of $$f_{i}$$ is to introduce its approximation $$f_{i,\\beta}$$, which differs from $$f_{i}$$ only in $$Df_{i} + [-\\beta^{2}, \\beta^{2}]$$. This will produce some bias because the actual observation is $$f_{i}(a^{\\top}x^{*})$$ rather than $$f_{i,\\beta}(a^{\\top}x^{*})$$. Hence, for some $$x \\in K$$ we define the following quantity to measure the bias induced by $$f_{i,\\beta}$$:\n\n$$\\mu_{\\beta}(x) = \\mathbb{P} [a^{\\top}x \\in Df_{i} + [-\\beta^{2}, \\beta^{2}], a \\sim N(0, I_{n}]).$$\n\nThe following assumption can often be satisfied by choosing suitable $$T$$ and sufficiently small $$\\beta_{1}$$.\n\nAssumption 4. Suppose Assumptions 1-3 hold true with parameters $$B_{0}, L_{0}, \\beta_{0}, A(1)_{g}, A(2)_{g}$$. For the $$T$$ used in (2.1), $$\\rho(x)$$ defined in (2.3) satisfies\n\n$$\\sup_{x \\in K} \\rho(x) \\lesssim (A(1) \\vee A(2))_{k}.$$\n\nMoreover, there exists some $$0 < \\beta_{1} < \\beta_{0}^{2}$$ such that\n\n$$(L_{0}\\beta_{1} + B_{0}) \\sup_{x \\in K} \\mu_{\\beta_{1}}(x) \\lesssim (A(1) \\vee A(2))_{k}.$$\n\nIn the proof, the estimation error $$\\|\\hat{x} - Tx^{*}\\|$$ is contributed by a concentration term of scaling $$O\\tilde{}(A(1) \\vee A(2))_{k/m}$$ and some bias terms. The main aim of Assumption 4 is to pull down the bias terms so that the concentration term is dominant.", "md": "Under Assumption 2, for any $$\\beta \\in [0, \\beta_{0}^{2})$$ we construct $$f_{i,\\beta}$$ as the Lipschitz approximation of $$f_{i}$$ to deal with the potential discontinuity of $$f_{i}$$ (i.e., $$Df_{i} \\neq \\emptyset$$). Specifically, $$f_{i,\\beta}$$ modifies $$f_{i}$$ in $$Df_{i} + [-\\beta^{2}, \\beta^{2}]$$ to be piece-wise linear and Lipschitz continuous; see its precise definition in (3.4).\n\nWe develop Theorem 2 to bound certain product processes appearing in the analysis, which produces bounds tighter than [36] when the index sets have low metric entropy. To make Theorem 2 applicable, we further make the following Assumption 3, which can be checked case-by-case by estimating the sub-Gaussian norm and probability tail. Also, $$U(1)_{g}$$ and $$U(2)_{g}$$ can even be a bit crude because the measurement number in Theorem 1 depends on them in a logarithmic manner.\n\nAssumption 3. Let $$a \\sim N(0, I_{n})$$, under Assumptions 1-2, we define the Lipschitz approximation $$f_{i,\\beta}$$ as in (3.4). We let\n\n$$\\xi_{i,\\beta}(a) := f_{i,\\beta}(a) - Ta, \\quad \\varepsilon_{i,\\beta}(a) := f_{i,\\beta}(a) - f_{i}(a).$$\n\nFor all $$\\beta \\in (0, \\beta_{0}^{2})$$, we assume the following holds with some parameters $$(A(1)_{g}, U(1)_{g}, P(1)_{0})$$ and $$(A(2)_{g}, U(2)_{g}, P(2)_{0})$$:\n- $$\\sup_{x \\in K} \\|\\xi_{i,\\beta}(a^{\\top}x)\\|_{\\psi_{2}} \\leq A(1)_{g}, \\quad \\sup_{x \\in K} |\\xi_{i,\\beta}(a^{\\top}x)| \\leq U(1)_{g} \\geq 1 - P(1)_{0};$$\n- $$\\sup_{x \\in K} \\|\\varepsilon_{i,\\beta}(a^{\\top}x)\\|_{\\psi_{2}} \\leq A(2)_{g}, \\quad \\sup_{x \\in K} |\\varepsilon_{i,\\beta}(a^{\\top}x)| \\leq U(2)_{g} \\geq 1 - P(2)_{0}.$$\n\nTo build a more complete theory we further introduce two useful quantities. For some $$x \\in K$$, we define the target mismatch $$\\rho(x)$$ as in [17, Definition 1]:\n\n$$\\rho(x) = \\mathbb{E} [f_{i}(a^{\\top}x)a_{i}] - Tx^{2}.$$\n\nIt is easy to see that $$\\mathbb{E} [f_{i}(a^{\\top}x)a_{i}]$$ minimizes the expected $$\\ell_{2}$$ loss $$\\mathbb{E} [\\|y - Ax\\|_{2}^{2}]$$, thus one can roughly understand $$\\mathbb{E} [f_{i}(a^{\\top}x)a_{i}]$$ as the expectation of $$\\hat{x}$$. Since $$Tx$$ is the desired ground truth, a small $$\\rho(x)$$ is intuitively an important ingredient for generalized Lasso to succeed. Fortunately, in many models, $$\\rho(x)$$ with a suitably chosen $$T$$ will vanish (e.g., linear model [2], single index model [33], 1-bit model [31]) or at least be sufficiently small (e.g., 1-bit model with dithering [45]).\n\nAs mentioned before, our method to deal with discontinuity of $$f_{i}$$ is to introduce its approximation $$f_{i,\\beta}$$, which differs from $$f_{i}$$ only in $$Df_{i} + [-\\beta^{2}, \\beta^{2}]$$. This will produce some bias because the actual observation is $$f_{i}(a^{\\top}x^{*})$$ rather than $$f_{i,\\beta}(a^{\\top}x^{*})$$. Hence, for some $$x \\in K$$ we define the following quantity to measure the bias induced by $$f_{i,\\beta}$$:\n\n$$\\mu_{\\beta}(x) = \\mathbb{P} [a^{\\top}x \\in Df_{i} + [-\\beta^{2}, \\beta^{2}], a \\sim N(0, I_{n}]).$$\n\nThe following assumption can often be satisfied by choosing suitable $$T$$ and sufficiently small $$\\beta_{1}$$.\n\nAssumption 4. Suppose Assumptions 1-3 hold true with parameters $$B_{0}, L_{0}, \\beta_{0}, A(1)_{g}, A(2)_{g}$$. For the $$T$$ used in (2.1), $$\\rho(x)$$ defined in (2.3) satisfies\n\n$$\\sup_{x \\in K} \\rho(x) \\lesssim (A(1) \\vee A(2))_{k}.$$\n\nMoreover, there exists some $$0 < \\beta_{1} < \\beta_{0}^{2}$$ such that\n\n$$(L_{0}\\beta_{1} + B_{0}) \\sup_{x \\in K} \\mu_{\\beta_{1}}(x) \\lesssim (A(1) \\vee A(2))_{k}.$$\n\nIn the proof, the estimation error $$\\|\\hat{x} - Tx^{*}\\|$$ is contributed by a concentration term of scaling $$O\\tilde{}(A(1) \\vee A(2))_{k/m}$$ and some bias terms. The main aim of Assumption 4 is to pull down the bias terms so that the concentration term is dominant."}, {"type": "heading", "lvl": 3, "value": "Main Theorem and its Implications", "md": "### Main Theorem and its Implications"}, {"type": "text", "value": "We now present our general theorem and apply it to some specific models.", "md": "We now present our general theorem and apply it to some specific models."}]}, {"page": 6, "text": " Theorem 1. Under Assumptions 1-4, given any recovery accuracy \u03f5 \u2208                             (0, 1), if it holds that\n m \u2273    (A(1)   \u2228 A(2)                                                                + P (2)) \u2212    m exp(\u2212\u2126(n)) \u2212\n           g         g )2 kL                                                     0         0\n                            \u03f52 , then with probability at least 1 \u2212         m(P (1)\n C exp(\u2212\u2126(k)) on a single realization of (A, f) := (ai, fi)m             i=1, we have the uniform signal recovery\n guarantee \u2225\u02c6  x \u2212   Tx\u2217\u22252 \u2264      \u03f5 for all x\u2217  \u2208  K, where \u02c6  x is the solution to (2.1) with y = f(Ax\u2217), and\n L = log     P is a logarithmic factor with        P being polynomial in (L, n) and other parameters that\n typically scale as O(L + n). See (C.11) for the precise expression of L .\n To illustrate the power of Theorem 1, we specialize it to several models to obtain concrete uniform\n signal recovery results. Starting with Theorem 1, the remaining work is to select parameters that\n justify Assumptions 2-4. We summarize the strategy as follows: (i) Determine the parameters in\n Assumption 2 by the measurement model; (ii) Set T that verifies (2.5) (see Lemmas 8-11 for the\n following models); (iii) Set the parameters in Assumption 3, for which bounding the norm of Gaussian\n vector is useful; (iv) Set \u03b21 to guarantee (2.6) based on some standard probability argument. We only\n provide suitable parameters for the following concrete models due to space limit, while leaving more\n details to Appendix E.\n (A) 1-bit GCS. Assume that we have the 1-bit observations yi = sign(a\u22a4                   i x\u2217); then fi(\u00b7) = f(\u00b7) =\n sign(\u00b7) satisfies Assumption 2 with (B0, L0, \u03b20) = (2, 0, \u221e). In this model, it is hopeless to\n recover the norm of \u2225x\u2217\u22252; as done in previous work, we assume x\u2217                       \u2208  K \u2282    Sn\u22121 [31, Remark\n1]. We set T =          2/\u03c0 and take the parameters in Assumption 3 as A(1)                \u224d   1, U (1) \u224d  \u221an, P (1)    \u224d\n                                                                                       g            g              0\n exp(\u2212\u2126(n)), A(2)       \u224d   1, U (2) \u224d   1, P (2) = 0. We take \u03b2 = \u03b21 \u224d           k\n                    g            g           0                                   m to guarantee (2.6). With these\n choices, Theorem 1 specializes to the following:\n Corollary 1. Consider Assumption 1 with fi(\u00b7) = sign(\u00b7) and K \u2282                   Sn\u22121, let \u03f5 \u2208    (0, 1) be any given\n recovery accuracy. If m \u2273          k         Lr\u221amn      ,4 then with probability at least 1 \u2212        2m exp(\u2212cn) \u2212\n                                   \u03f52 log    \u03f5\u2227(k/m)                                                                \u02c6\n m exp(\u2212\u2126(k)) on a single draw of (ai)m           i=1, we have the uniform signal recovery guarantee                 x \u2212\n    2                                         x is the solution to (2.1) with y = sign(Ax\u2217) and T =                    2\n    \u03c0x\u2217    2 \u2264   \u03f5 for all x\u2217  \u2208  K, where \u02c6                                                                           \u03c0.\n Remark 1. A uniform recovery guarantee for generalized Lasso in 1-bit GCS was obtained in [33,\n Section 5]. Their proof relies on the local embedding property in [31]. Note that such geometric\n property is often problem-dependent and highly nontrivial. By contrast, our argument is free of\n geometric properties of this kind.\n Remark 2. For traditional 1-bit CS, [17, Corollary 2] requires m \u2273                     \u02dc\n                                                                                       O(k/\u03f54) to achieve uniform\n \u21132-accuracy of \u03f5 for all k-sparse signals, which is inferior to our \u02dc               O(k/\u03f52). This is true for all\n remaining examples. To obtain such a sharper rate, the key technique is to use our Theorem 2 (rather\n than [36]) to obtain tighter bound for the product processes, as will be discussed in Remark 8.\n                                                                                                                       iid\n (B) 1-bit GCS with dithering. Assume that the a\u22a4                 i x\u2217   is quantized to 1-bit with dither5 \u03c4i         \u223c\n U [\u2212\u03bb, \u03bb]) for some \u03bb to be chosen, i.e., we observe yi = sign(a\u22a4                 i x\u2217   + \u03c4i). Following [45] we\n assume K \u2282       Bn2(R) for some R > 0. Here, using dithering allows the recovery of signal norm\n \u2225x\u2217\u22252, so we do not need to assume K \u2282                Sn\u22121 as in Corollary 1. We set \u03bb = CR\u221alog m with\n sufficiently large C, and T = \u03bb\u22121. In Assumption 3, we take A(1)                       \u224d   1, U (1)  \u224d   \u221an, P (1)     \u224d\n                                                                                   g              g                0\n exp(\u2212\u2126(n)), A(2)        \u224d   1, U (2)  \u224d  1, and P (2)    = 0. Moreover, we take \u03b2 = \u03b21 = \u03bbk\n                     g            g                  0                                                 m to guarantee\n (2.6). Now we can invoke Theorem 1 to get the following.\n Corollary 2. Consider Assumption 1 with fi(\u00b7) = sign(\u00b7 + \u03c4i), \u03c4i \u223c                     U [\u2212\u03bb, \u03bb] and K \u2282        Bn 2(R),\n and \u03bb = CR\u221alog m with sufficiently large C. Let \u03f5 \u2208                   (0, 1) be any given recovery accuracy. If\n m \u2273    k           Lr\u221amn       , then with probability at least 1 \u2212        2m exp(\u2212cn) \u2212        m exp(\u2212\u2126(k)) on a\n        \u03f52 log    \u03bb(\u03f5\u2227(k/m))\n single draw of (ai, \u03c4i)m   i=1, we have the uniform signal recovery guarantee \u2225\u02c6              x \u2212   \u03bb\u22121x\u2217\u22252 \u2264      \u03f5 for\n all x\u2217  \u2208  K, where \u02c6  x is the solution to (2.1) with y = sign(Ax\u2217            + \u03c4) (here, \u03c4 = [\u03c41, ..., \u03c4m]\u22a4) and\n T = \u03bb\u22121.\n Remark 3. To our knowledge, the only related prior result is in [45, Theorem 3.2]. However, their\n result is restricted to ReLU networks. By contrast, we deal with the more general Lipschitz generative\n models; by specializing our result to the ReLU network that is typically (n\u0398(d))-Lipschitz [2] (d is\n     4Here and in other similar statements, we implicitly assume a large enough implied constant.\n     5Throughout this work, the random dither is independent of the {ai}m       i=1.\n                                                             6", "md": "Theorem 1. Under Assumptions 1-4, given any recovery accuracy \u03f5 \u2208 (0, 1), if it holds that\n\n$$\nm \\gtrsim (A(1) \\lor A(2) + P(2)) - m \\exp(-\\Omega(n)) - \\frac{g}{g}^2 kL_0 \\epsilon^2\n$$\n, then with probability at least 1 - m(P(1)C \\exp(-\\Omega(k))) on a single realization of (A, f) := (a_i, f_i)_{i=1}^m, we have the uniform signal recovery guarantee $\\|\\hat{x} - Tx^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = f(Ax^*)$, and $L = \\log P$ is a logarithmic factor with $P$ being polynomial in $(L, n)$ and other parameters that typically scale as $O(L + n)$. See (C.11) for the precise expression of $L$.\n\nTo illustrate the power of Theorem 1, we specialize it to several models to obtain concrete uniform signal recovery results. Starting with Theorem 1, the remaining work is to select parameters that justify Assumptions 2-4. We summarize the strategy as follows: (i) Determine the parameters in Assumption 2 by the measurement model; (ii) Set $T$ that verifies (2.5) (see Lemmas 8-11 for the following models); (iii) Set the parameters in Assumption 3, for which bounding the norm of Gaussian vector is useful; (iv) Set $\\beta_1$ to guarantee (2.6) based on some standard probability argument. We only provide suitable parameters for the following concrete models due to space limit, while leaving more details to Appendix E.\n\n(A) 1-bit GCS. Assume that we have the 1-bit observations $y_i = \\text{sign}(a_i^T x^*)$; then $f_i(\\cdot) = f(\\cdot) = \\text{sign}(\\cdot)$ satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$. In this model, it is hopeless to recover the norm of $\\|x^*\\|_2$; as done in previous work, we assume $x^* \\in K \\subset S^{n-1}$ [31, Remark 1]. We set $T = \\frac{2}{\\pi}$ and take the parameters in Assumption 3 as $A(1) \\approx 1$, $U(1) \\approx \\sqrt{n}$, $P(1) \\approx \\exp(-\\Omega(n))$, $A(2) \\approx 1$, $U(2) \\approx 1$, $P(2) = 0$. We take $\\beta = \\beta_1 \\approx \\frac{k}{m}$ to guarantee (2.6). With these choices, Theorem 1 specializes to the following:\n\nCorollary 1. Consider Assumption 1 with $f_i(\\cdot) = \\text{sign}(\\cdot)$ and $K \\subset S^{n-1}$, let $\\epsilon \\in (0, 1)$ be any given recovery accuracy. If $m \\gtrsim k L r \\sqrt{mn}$, then with probability at least $1 - 2m \\exp(-cn) - \\epsilon^2 \\log \\lambda(\\epsilon \\wedge (k/m))$ on a single draw of $(a_i)_m^{i=1}$, we have the uniform signal recovery guarantee $\\|\\hat{x} - \\lambda^{-1}x^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = \\text{sign}(Ax^*)$ and $T = \\lambda^{-1}$.\n\nRemark 1. A uniform recovery guarantee for generalized Lasso in 1-bit GCS was obtained in [33, Section 5]. Their proof relies on the local embedding property in [31]. Note that such geometric property is often problem-dependent and highly nontrivial. By contrast, our argument is free of geometric properties of this kind.\n\nRemark 2. For traditional 1-bit CS, [17, Corollary 2] requires $m \\gtrsim \\tilde{O}(k/\\epsilon^4)$ to achieve uniform $\\ell_2$-accuracy of $\\epsilon$ for all $k$-sparse signals, which is inferior to our $\\tilde{O}(k/\\epsilon^2)$. This is true for all remaining examples. To obtain such a sharper rate, the key technique is to use our Theorem 2 (rather than [36]) to obtain tighter bound for the product processes, as will be discussed in Remark 8.\n\n(B) 1-bit GCS with dithering. Assume that the $a_i^T x^*$ is quantized to 1-bit with dither $\\tau_i \\sim U[-\\lambda, \\lambda]$ for some $\\lambda$ to be chosen, i.e., we observe $y_i = \\text{sign}(a_i^T x^* + \\tau_i)$. Following [45] we assume $K \\subset B_n^2(R)$ for some $R > 0$. Here, using dithering allows the recovery of signal norm $\\|x^*\\|_2$, so we do not need to assume $K \\subset S^{n-1}$ as in Corollary 1. We set $\\lambda = CR \\sqrt{\\log m}$ with sufficiently large $C$, and $T = \\lambda^{-1}$. In Assumption 3, we take $A(1) \\approx 1$, $U(1) \\approx \\sqrt{n}$, $P(1) \\approx \\exp(-\\Omega(n))$, $A(2) \\approx 1$, $U(2) \\approx 1$, and $P(2) = 0$. Moreover, we take $\\beta = \\beta_1 = \\lambda k/m$ to guarantee (2.6). Now we can invoke Theorem 1 to get the following.\n\nCorollary 2. Consider Assumption 1 with $f_i(\\cdot) = \\text{sign}(\\cdot + \\tau_i)$, $\\tau_i \\sim U[-\\lambda, \\lambda]$ and $K \\subset B_n^2(R)$, and $\\lambda = CR \\sqrt{\\log m}$ with sufficiently large $C$. Let $\\epsilon \\in (0, 1)$ be any given recovery accuracy. If $m \\gtrsim k L r \\sqrt{mn}$, then with probability at least $1 - 2m \\exp(-cn) - \\epsilon^2 \\log \\lambda(\\epsilon \\wedge (k/m))$ on a single draw of $(a_i, \\tau_i)_m^{i=1}$, we have the uniform signal recovery guarantee $\\|\\hat{x} - \\lambda^{-1}x^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = \\text{sign}(Ax^* + \\tau)$ (here, $\\tau = [\\tau_1, ..., \\tau_m]^T$) and $T = \\lambda^{-1}$.\n\nRemark 3. To our knowledge, the only related prior result is in [45, Theorem 3.2]. However, their result is restricted to ReLU networks. By contrast, we deal with the more general Lipschitz generative models; by specializing our result to the ReLU network that is typically $(n\\Theta(d))$-Lipschitz [2] ($d$ is\n\nHere and in other similar statements, we implicitly assume a large enough implied constant.\n\nThroughout this work, the random dither is independent of the $\\{a_i\\}_m^{i=1}$.", "images": [], "items": [{"type": "text", "value": "Theorem 1. Under Assumptions 1-4, given any recovery accuracy \u03f5 \u2208 (0, 1), if it holds that\n\n$$\nm \\gtrsim (A(1) \\lor A(2) + P(2)) - m \\exp(-\\Omega(n)) - \\frac{g}{g}^2 kL_0 \\epsilon^2\n$$\n, then with probability at least 1 - m(P(1)C \\exp(-\\Omega(k))) on a single realization of (A, f) := (a_i, f_i)_{i=1}^m, we have the uniform signal recovery guarantee $\\|\\hat{x} - Tx^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = f(Ax^*)$, and $L = \\log P$ is a logarithmic factor with $P$ being polynomial in $(L, n)$ and other parameters that typically scale as $O(L + n)$. See (C.11) for the precise expression of $L$.\n\nTo illustrate the power of Theorem 1, we specialize it to several models to obtain concrete uniform signal recovery results. Starting with Theorem 1, the remaining work is to select parameters that justify Assumptions 2-4. We summarize the strategy as follows: (i) Determine the parameters in Assumption 2 by the measurement model; (ii) Set $T$ that verifies (2.5) (see Lemmas 8-11 for the following models); (iii) Set the parameters in Assumption 3, for which bounding the norm of Gaussian vector is useful; (iv) Set $\\beta_1$ to guarantee (2.6) based on some standard probability argument. We only provide suitable parameters for the following concrete models due to space limit, while leaving more details to Appendix E.\n\n(A) 1-bit GCS. Assume that we have the 1-bit observations $y_i = \\text{sign}(a_i^T x^*)$; then $f_i(\\cdot) = f(\\cdot) = \\text{sign}(\\cdot)$ satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$. In this model, it is hopeless to recover the norm of $\\|x^*\\|_2$; as done in previous work, we assume $x^* \\in K \\subset S^{n-1}$ [31, Remark 1]. We set $T = \\frac{2}{\\pi}$ and take the parameters in Assumption 3 as $A(1) \\approx 1$, $U(1) \\approx \\sqrt{n}$, $P(1) \\approx \\exp(-\\Omega(n))$, $A(2) \\approx 1$, $U(2) \\approx 1$, $P(2) = 0$. We take $\\beta = \\beta_1 \\approx \\frac{k}{m}$ to guarantee (2.6). With these choices, Theorem 1 specializes to the following:\n\nCorollary 1. Consider Assumption 1 with $f_i(\\cdot) = \\text{sign}(\\cdot)$ and $K \\subset S^{n-1}$, let $\\epsilon \\in (0, 1)$ be any given recovery accuracy. If $m \\gtrsim k L r \\sqrt{mn}$, then with probability at least $1 - 2m \\exp(-cn) - \\epsilon^2 \\log \\lambda(\\epsilon \\wedge (k/m))$ on a single draw of $(a_i)_m^{i=1}$, we have the uniform signal recovery guarantee $\\|\\hat{x} - \\lambda^{-1}x^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = \\text{sign}(Ax^*)$ and $T = \\lambda^{-1}$.\n\nRemark 1. A uniform recovery guarantee for generalized Lasso in 1-bit GCS was obtained in [33, Section 5]. Their proof relies on the local embedding property in [31]. Note that such geometric property is often problem-dependent and highly nontrivial. By contrast, our argument is free of geometric properties of this kind.\n\nRemark 2. For traditional 1-bit CS, [17, Corollary 2] requires $m \\gtrsim \\tilde{O}(k/\\epsilon^4)$ to achieve uniform $\\ell_2$-accuracy of $\\epsilon$ for all $k$-sparse signals, which is inferior to our $\\tilde{O}(k/\\epsilon^2)$. This is true for all remaining examples. To obtain such a sharper rate, the key technique is to use our Theorem 2 (rather than [36]) to obtain tighter bound for the product processes, as will be discussed in Remark 8.\n\n(B) 1-bit GCS with dithering. Assume that the $a_i^T x^*$ is quantized to 1-bit with dither $\\tau_i \\sim U[-\\lambda, \\lambda]$ for some $\\lambda$ to be chosen, i.e., we observe $y_i = \\text{sign}(a_i^T x^* + \\tau_i)$. Following [45] we assume $K \\subset B_n^2(R)$ for some $R > 0$. Here, using dithering allows the recovery of signal norm $\\|x^*\\|_2$, so we do not need to assume $K \\subset S^{n-1}$ as in Corollary 1. We set $\\lambda = CR \\sqrt{\\log m}$ with sufficiently large $C$, and $T = \\lambda^{-1}$. In Assumption 3, we take $A(1) \\approx 1$, $U(1) \\approx \\sqrt{n}$, $P(1) \\approx \\exp(-\\Omega(n))$, $A(2) \\approx 1$, $U(2) \\approx 1$, and $P(2) = 0$. Moreover, we take $\\beta = \\beta_1 = \\lambda k/m$ to guarantee (2.6). Now we can invoke Theorem 1 to get the following.\n\nCorollary 2. Consider Assumption 1 with $f_i(\\cdot) = \\text{sign}(\\cdot + \\tau_i)$, $\\tau_i \\sim U[-\\lambda, \\lambda]$ and $K \\subset B_n^2(R)$, and $\\lambda = CR \\sqrt{\\log m}$ with sufficiently large $C$. Let $\\epsilon \\in (0, 1)$ be any given recovery accuracy. If $m \\gtrsim k L r \\sqrt{mn}$, then with probability at least $1 - 2m \\exp(-cn) - \\epsilon^2 \\log \\lambda(\\epsilon \\wedge (k/m))$ on a single draw of $(a_i, \\tau_i)_m^{i=1}$, we have the uniform signal recovery guarantee $\\|\\hat{x} - \\lambda^{-1}x^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = \\text{sign}(Ax^* + \\tau)$ (here, $\\tau = [\\tau_1, ..., \\tau_m]^T$) and $T = \\lambda^{-1}$.\n\nRemark 3. To our knowledge, the only related prior result is in [45, Theorem 3.2]. However, their result is restricted to ReLU networks. By contrast, we deal with the more general Lipschitz generative models; by specializing our result to the ReLU network that is typically $(n\\Theta(d))$-Lipschitz [2] ($d$ is\n\nHere and in other similar statements, we implicitly assume a large enough implied constant.\n\nThroughout this work, the random dither is independent of the $\\{a_i\\}_m^{i=1}$.", "md": "Theorem 1. Under Assumptions 1-4, given any recovery accuracy \u03f5 \u2208 (0, 1), if it holds that\n\n$$\nm \\gtrsim (A(1) \\lor A(2) + P(2)) - m \\exp(-\\Omega(n)) - \\frac{g}{g}^2 kL_0 \\epsilon^2\n$$\n, then with probability at least 1 - m(P(1)C \\exp(-\\Omega(k))) on a single realization of (A, f) := (a_i, f_i)_{i=1}^m, we have the uniform signal recovery guarantee $\\|\\hat{x} - Tx^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = f(Ax^*)$, and $L = \\log P$ is a logarithmic factor with $P$ being polynomial in $(L, n)$ and other parameters that typically scale as $O(L + n)$. See (C.11) for the precise expression of $L$.\n\nTo illustrate the power of Theorem 1, we specialize it to several models to obtain concrete uniform signal recovery results. Starting with Theorem 1, the remaining work is to select parameters that justify Assumptions 2-4. We summarize the strategy as follows: (i) Determine the parameters in Assumption 2 by the measurement model; (ii) Set $T$ that verifies (2.5) (see Lemmas 8-11 for the following models); (iii) Set the parameters in Assumption 3, for which bounding the norm of Gaussian vector is useful; (iv) Set $\\beta_1$ to guarantee (2.6) based on some standard probability argument. We only provide suitable parameters for the following concrete models due to space limit, while leaving more details to Appendix E.\n\n(A) 1-bit GCS. Assume that we have the 1-bit observations $y_i = \\text{sign}(a_i^T x^*)$; then $f_i(\\cdot) = f(\\cdot) = \\text{sign}(\\cdot)$ satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$. In this model, it is hopeless to recover the norm of $\\|x^*\\|_2$; as done in previous work, we assume $x^* \\in K \\subset S^{n-1}$ [31, Remark 1]. We set $T = \\frac{2}{\\pi}$ and take the parameters in Assumption 3 as $A(1) \\approx 1$, $U(1) \\approx \\sqrt{n}$, $P(1) \\approx \\exp(-\\Omega(n))$, $A(2) \\approx 1$, $U(2) \\approx 1$, $P(2) = 0$. We take $\\beta = \\beta_1 \\approx \\frac{k}{m}$ to guarantee (2.6). With these choices, Theorem 1 specializes to the following:\n\nCorollary 1. Consider Assumption 1 with $f_i(\\cdot) = \\text{sign}(\\cdot)$ and $K \\subset S^{n-1}$, let $\\epsilon \\in (0, 1)$ be any given recovery accuracy. If $m \\gtrsim k L r \\sqrt{mn}$, then with probability at least $1 - 2m \\exp(-cn) - \\epsilon^2 \\log \\lambda(\\epsilon \\wedge (k/m))$ on a single draw of $(a_i)_m^{i=1}$, we have the uniform signal recovery guarantee $\\|\\hat{x} - \\lambda^{-1}x^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = \\text{sign}(Ax^*)$ and $T = \\lambda^{-1}$.\n\nRemark 1. A uniform recovery guarantee for generalized Lasso in 1-bit GCS was obtained in [33, Section 5]. Their proof relies on the local embedding property in [31]. Note that such geometric property is often problem-dependent and highly nontrivial. By contrast, our argument is free of geometric properties of this kind.\n\nRemark 2. For traditional 1-bit CS, [17, Corollary 2] requires $m \\gtrsim \\tilde{O}(k/\\epsilon^4)$ to achieve uniform $\\ell_2$-accuracy of $\\epsilon$ for all $k$-sparse signals, which is inferior to our $\\tilde{O}(k/\\epsilon^2)$. This is true for all remaining examples. To obtain such a sharper rate, the key technique is to use our Theorem 2 (rather than [36]) to obtain tighter bound for the product processes, as will be discussed in Remark 8.\n\n(B) 1-bit GCS with dithering. Assume that the $a_i^T x^*$ is quantized to 1-bit with dither $\\tau_i \\sim U[-\\lambda, \\lambda]$ for some $\\lambda$ to be chosen, i.e., we observe $y_i = \\text{sign}(a_i^T x^* + \\tau_i)$. Following [45] we assume $K \\subset B_n^2(R)$ for some $R > 0$. Here, using dithering allows the recovery of signal norm $\\|x^*\\|_2$, so we do not need to assume $K \\subset S^{n-1}$ as in Corollary 1. We set $\\lambda = CR \\sqrt{\\log m}$ with sufficiently large $C$, and $T = \\lambda^{-1}$. In Assumption 3, we take $A(1) \\approx 1$, $U(1) \\approx \\sqrt{n}$, $P(1) \\approx \\exp(-\\Omega(n))$, $A(2) \\approx 1$, $U(2) \\approx 1$, and $P(2) = 0$. Moreover, we take $\\beta = \\beta_1 = \\lambda k/m$ to guarantee (2.6). Now we can invoke Theorem 1 to get the following.\n\nCorollary 2. Consider Assumption 1 with $f_i(\\cdot) = \\text{sign}(\\cdot + \\tau_i)$, $\\tau_i \\sim U[-\\lambda, \\lambda]$ and $K \\subset B_n^2(R)$, and $\\lambda = CR \\sqrt{\\log m}$ with sufficiently large $C$. Let $\\epsilon \\in (0, 1)$ be any given recovery accuracy. If $m \\gtrsim k L r \\sqrt{mn}$, then with probability at least $1 - 2m \\exp(-cn) - \\epsilon^2 \\log \\lambda(\\epsilon \\wedge (k/m))$ on a single draw of $(a_i, \\tau_i)_m^{i=1}$, we have the uniform signal recovery guarantee $\\|\\hat{x} - \\lambda^{-1}x^*\\|^2 \\leq \\epsilon$ for all $x^* \\in K$, where $\\hat{x}$ is the solution to (2.1) with $y = \\text{sign}(Ax^* + \\tau)$ (here, $\\tau = [\\tau_1, ..., \\tau_m]^T$) and $T = \\lambda^{-1}$.\n\nRemark 3. To our knowledge, the only related prior result is in [45, Theorem 3.2]. However, their result is restricted to ReLU networks. By contrast, we deal with the more general Lipschitz generative models; by specializing our result to the ReLU network that is typically $(n\\Theta(d))$-Lipschitz [2] ($d$ is\n\nHere and in other similar statements, we implicitly assume a large enough implied constant.\n\nThroughout this work, the random dither is independent of the $\\{a_i\\}_m^{i=1}$."}]}, {"page": 7, "text": " the number of layers), our error rate coincides with theirs up to a logarithmic factor. Additionally, as\n already mentioned in the Introduction Section, our result can be generalized to a sensing vector with\n an unknown covariance matrix, unlike theirs which is restricted to isotropic sensing vectors. The\n advantage of their result is in allowing sub-exponential sensing vectors.\n (C) Lipschitz-continuous SIM with generative prior. Assume that any realization of f is uncon-\n ditionally \u02c6   L-Lipschitz, which implies Assumption 2 with (B0, L0, \u03b20) = (0, \u02c6                                   L, \u221e). We further\n assume P(f(0) \u2264              \u02c6\n                             B) \u2265      1 \u2212   P \u2032                  B, P \u2032\n                                               0 for some ( \u02c6           0). Because the norm of x\u2217                 is absorbed into the\n unknown f(\u00b7), we assume K \u2282                    Sn\u22121. We set \u03b2 = 0 so that fi,\u03b2 = fi. We introduce the quantities\n \u00b5 = E[f(g)g], \u03c8 = \u2225f(g)\u2225\u03c82, where g \u223c                             N   (0, 1). We choose T = \u00b5 and set parameters in\n Assumption 3 as A(1)        g     \u224d   \u03c8 + \u00b5, U (1)  g     \u224d   (\u02c6L + \u00b5)\u221an + \u02c6        B, P (1)0     \u224d   P \u20320 + exp(\u2212\u2126(n)), A(2)          g    \u224d\n \u03c8 + \u00b5, U (2)  g    = 0, P (2)0     = 0. Now we are ready to apply Theorem 1 to this model. We obtain:\n Corollary 3. Consider Assumption 1 with \u02c6                       L-Lipschitz f, suppose that P(f(0) \u2264                       \u02c6\n                                                                                                                            B) \u2265     1 \u2212    P \u2032\n                                                                                                                                              0,\n and define the parameters \u00b5 = E[f(g)g], \u03c8 = \u2225f(g)\u2225\u03c82 with g \u223c      Lr\u221am[n(\u00b5+\u03f5)(\u02c6      L+\u00b5)+\u221an\u00b5 \u02c6   N  (0, 1). Let \u03f5 \u2208       (0, 1) be any\n given recovery accuracy. If m \u2273                 (\u00b5+\u03c8)k     log                                       B+\u03c8]      , then with probability\n                                                     \u03f52                             (\u00b5+\u03c8)\u03f5\n at least 1 \u2212      2m exp(\u2212cn) \u2212            mP \u2032 0 \u2212    c1 exp(\u2212\u2126(k)) on a single draw of (ai, fi)m                     i=1, we have the\n uniform signal recovery guarantee \u2225\u02c6                x \u2212    \u00b5x\u2217\u22252 \u2264        \u03f5 for all x\u2217     \u2208  K, where \u02c6    x is the solution to (2.1)\n with y = f(Ax\u2217) and T = \u00b5.\n Remark 4. While the main result of [33] is non-uniform, it was noted in [33, Section 5] that a\n similar uniform error rate can be established for any deterministic 1-Lipschitz f. Our result here is\n more general in that the \u02c6         L-Lipschitz f is possibly random. Note that randomness on f is significant\n because it provides much more flexibility (e.g., additive random noise).\n Remark 5. For SIM with unknown fi it may seem impractical to use (2.1) as it requires \u00b5 = E[f(g)g]\n where g \u223c       N   (0, 1). However, by assuming \u00b5x\u2217                   \u2208   K = G(Bk      2(r)) as in [33], which is natural for\n sufficiently expressive G(\u00b7), we can simply use x \u2208                      K as constraint in (2.1). Our Corollary 3 remains\n valid in this case under some inessential changes of log \u00b5 factors in the sample complexity.\n (D) Uniformly quantized GCS with dithering. The uniform quantizer with resolution \u03b4 > 0 is\n defined as Q\u03b4(a) = \u03b4             \u230aa\u03b4 \u230b  + 1 2   for a \u2208     R. Using dithering \u03c4i \u223c              U [\u2212\u03b4   2, \u03b42], we suppose that the\n observations are yi = Q\u03b4(a\u22a4              i x\u2217    + \u03c4i). This satisfies Assumption 2 with (B0, L0, \u03b20) = (\u03b4, 0, \u03b4).\nWe set T = 1 and take parameters for Assumption 3 as follows: A(1)                                   g , U (1)                     \u224d   \u03b4, and\n                                                                                                             g , A(2)g , U (2)\n                                                                                                                             g\n P (1)   = P (2)     = 0. We take \u03b2 = \u03b21 \u224d                 k\u03b4\n   0           0                                           m to confirm (2.6). With these parameters, we obtain the\n following from Theorem 1.\n Corollary 4. Consider Assumption 1 with f(\u00b7) = Q\u03b4(\u00b7 + \u03c4), \u03c4 \u223c                                   U [\u2212\u03b4   2, \u03b42] for some quantization\n resolution \u03b4 > 0. Let \u03f5 > 0 be any given recovery accuracy. If m \u2273                                   \u03b42k              Lr\u221amn            , then\n                                                                                                       \u03f52 log      \u03f5\u2227[k\u03b4/(m\u221an)]\n with probability at least 1 \u2212              2m exp(\u2212cn) \u2212            c1 exp(\u2212\u2126(k)) on a single draw of (ai, \u03c4i)m                     i=1, we\n have the uniform recovery guarantee \u2225\u02c6                 x \u2212    x\u2217\u22252 \u2264      \u03f5 for all x\u2217     \u2208   K, where \u02c6    x is the solution to (2.1)\n with y = Q\u03b4(Ax + \u03c4) and T = 1 (here, \u03c4 = [\u03c41, . . . , \u03c4m]\u22a4).\n Remark 6. While this dithered uniform quantized model has been widely studied in traditional CS\n(e.g., non-uniform recovery [8,48], uniform recovery [17,52]), it has not been investigated in GCS\n even for non-uniform recovery. Thus, this is new to the best of our knowledge.\n A simple extension to the noisy model y = f(Ax\u2217) + \u03b7 where \u03b7 \u2208                                         Rm has i.i.d. sub-Gaussian\n entries can be obtained by a fairly straightforward extension of our analysis; see Appendix F.\n 3     Proof Sketch\n To provide a sketch of our proof, we begin with the optimality condition \u2225y \u2212                                           A\u02c6 x\u22252 2 \u2264     \u2225y \u2212\n A(T    x\u2217)\u22252  2. We expand the square and plug in y = f(Ax\u2217) to obtain\n For the final goal \u2225\u02c6        \u221aA m(\u02c6  x \u2212   T  x\u2217)    22 \u2264   m2    f(Ax\u2217) \u2212         T Ax\u2217, A(\u02c6      x \u2212    T x\u2217)     .                   (3.1)\n                              x \u2212    T  x\u2217\u22252 \u2264        \u03f5, up to rescaling, it is enough to prove \u2225\u02c6                   x \u2212    T  x\u2217\u22252 \u2264        3\u03f5.\nWe assume for convenience that \u2225\u02c6                   x \u2212    T  x\u2217\u22252 > 2\u03f5, without loss of generality. Combined with\n                                                                       7", "md": "the number of layers), our error rate coincides with theirs up to a logarithmic factor. Additionally, as already mentioned in the Introduction Section, our result can be generalized to a sensing vector with an unknown covariance matrix, unlike theirs which is restricted to isotropic sensing vectors. The advantage of their result is in allowing sub-exponential sensing vectors.\n\n(C) Lipschitz-continuous SIM with generative prior. Assume that any realization of f is unconditionally $$\\hat{L}$$-Lipschitz, which implies Assumption 2 with $$(B_0, L_0, \\beta_0) = (0, \\hat{L}, \\infty)$$. We further assume $$P(f(0) \\leq \\hat{B}) \\geq 1 - P'_{0}$$ for some $$\\hat{B}_0$$. Because the norm of $$x^*$$ is absorbed into the unknown f(\u00b7), we assume $$K \\subset S^{n-1}$$. We set $$\\beta = 0$$ so that $$f_{i,\\beta} = f_i$$. We introduce the quantities $$\\mu = E[f(g)g]$$, $$\\psi = \\|f(g)\\|_{\\psi}^2$$, where $$g \\sim N(0, 1)$$. We choose $$T = \\mu$$ and set parameters in Assumption 3 as $$A(1)_g \\approx \\psi + \\mu$$, $$U(1)_g \\approx (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$$, $$P(1)_0 \\approx P'_0 + \\exp(-\\Omega(n))$$, $$A(2)_g \\approx \\psi + \\mu$$, $$U(2)_g = 0$$, $$P(2)_0 = 0$$. Now we are ready to apply Theorem 1 to this model. We obtain:\n\n$$\\text{Corollary 3. Consider Assumption 1 with } \\hat{L}\\text{-Lipschitz f, suppose that } P(f(0) \\leq \\hat{B}) \\geq 1 - P'_{0}\\text{, and define the parameters } \\mu = E[f(g)g]\\text{, } \\psi = \\|f(g)\\|_{\\psi}^2 \\text{ with } g \\sim N(0, 1)\\text{. Let } \\epsilon \\in (0, 1)\\text{ be any given recovery accuracy. If } m \\gtrsim (\\mu+\\psi)k \\log \\left[ \\frac{B+\\psi}{\\epsilon^2} \\right]\\text{, then with probability at least } 1 - \\frac{2m}{\\exp(-cn)} - mP'_0 - c_1 \\exp(-\\Omega(k)) \\text{ on a single draw of } (a_i, f_i)_{m}^{i=1}\\text{, we have the uniform signal recovery guarantee } \\| \\hat{x} - \\mu x^* \\|_2 \\leq \\epsilon \\text{ for all } x^* \\in K\\text{, where } \\hat{x} \\text{ is the solution to (2.1) with } y = f(Ax^*) \\text{ and } T = \\mu$$\n\nRemark 4. While the main result of [33] is non-uniform, it was noted in [33, Section 5] that a similar uniform error rate can be established for any deterministic 1-Lipschitz f. Our result here is more general in that the $$\\hat{L}$$-Lipschitz f is possibly random. Note that randomness on f is significant because it provides much more flexibility (e.g., additive random noise).\n\nRemark 5. For SIM with unknown fi it may seem impractical to use (2.1) as it requires $$\\mu = E[f(g)g]$$ where $$g \\sim N(0, 1)$$. However, by assuming $$\\mu x^* \\in K = G(Bk^{2}(r))$$ as in [33], which is natural for sufficiently expressive $$G(\\cdot)$$, we can simply use $$x \\in K$$ as constraint in (2.1). Our Corollary 3 remains valid in this case under some inessential changes of log $$\\mu$$ factors in the sample complexity.\n\n(D) Uniformly quantized GCS with dithering. The uniform quantizer with resolution $$\\delta > 0$$ is defined as $$Q_{\\delta}(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + \\frac{1}{2}$$ for $$a \\in \\mathbb{R}$$. Using dithering $$\\tau_i \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$$, we suppose that the observations are $$y_i = Q_{\\delta}(a_i^{\\top} x^* + \\tau_i)$$. This satisfies Assumption 2 with $$(B_0, L_0, \\beta_0) = (\\delta, 0, \\delta)$$. We set $$T = 1$$ and take parameters for Assumption 3 as follows: $$A(1)_g, U(1)_g \\approx \\delta$$, and $$A(2)_g, U(2)_g, P(1)_0 = P(2)_0 = 0$$. We take $$\\beta = \\beta_1 \\approx k\\delta$$ to confirm (2.6). With these parameters, we obtain the following from Theorem 1.\n\n$$\\text{Corollary 4. Consider Assumption 1 with } f(\\cdot) = Q_{\\delta}(\\cdot + \\tau), \\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right] \\text{ for some quantization resolution } \\delta > 0\\text{. Let } \\epsilon > 0 \\text{ be any given recovery accuracy. If } m \\gtrsim \\delta^2 k Lr\\sqrt{mn}\\text{, then with probability at least } 1 - \\frac{2m}{\\exp(-cn)} - c_1 \\exp(-\\Omega(k)) \\text{ on a single draw of } (a_i, \\tau_i)_{m}^{i=1}\\text{, we have the uniform recovery guarantee } \\| \\hat{x} - x^* \\|_2 \\leq \\epsilon \\text{ for all } x^* \\in K\\text{, where } \\hat{x} \\text{ is the solution to (2.1) with } y = Q_{\\delta}(Ax + \\tau) \\text{ and } T = 1 \\text{(here, } \\tau = [\\tau_1, ..., \\tau_m]^{\\top}\\text{)}$$\n\nRemark 6. While this dithered uniform quantized model has been widely studied in traditional CS (e.g., non-uniform recovery [8,48], uniform recovery [17,52]), it has not been investigated in GCS even for non-uniform recovery. Thus, this is new to the best of our knowledge.\n\nA simple extension to the noisy model $$y = f(Ax^*) + \\eta$$ where $$\\eta \\in \\mathbb{R}^m$$ has i.i.d. sub-Gaussian entries can be obtained by a fairly straightforward extension of our analysis; see Appendix F.\n\n### Proof Sketch\n\nTo provide a sketch of our proof, we begin with the optimality condition $$\\|y - A\\hat{x}\\|_{2}^{2} \\leq \\|y - A(Tx^*)\\|_{2}^{2}$$. We expand the square and plug in $$y = f(Ax^*)$$ to obtain\n\nFor the final goal $$\\|\\hat{x} - Tx^*\\|_{2} \\leq \\epsilon$$, up to rescaling, it is enough to prove $$\\|\\hat{x} - Tx^*\\|_{2} \\leq 3\\epsilon$$.\n\nWe assume for convenience that $$\\|\\hat{x} - Tx^*\\|_{2} > 2\\epsilon$$, without loss of generality. Combined with", "images": [], "items": [{"type": "text", "value": "the number of layers), our error rate coincides with theirs up to a logarithmic factor. Additionally, as already mentioned in the Introduction Section, our result can be generalized to a sensing vector with an unknown covariance matrix, unlike theirs which is restricted to isotropic sensing vectors. The advantage of their result is in allowing sub-exponential sensing vectors.\n\n(C) Lipschitz-continuous SIM with generative prior. Assume that any realization of f is unconditionally $$\\hat{L}$$-Lipschitz, which implies Assumption 2 with $$(B_0, L_0, \\beta_0) = (0, \\hat{L}, \\infty)$$. We further assume $$P(f(0) \\leq \\hat{B}) \\geq 1 - P'_{0}$$ for some $$\\hat{B}_0$$. Because the norm of $$x^*$$ is absorbed into the unknown f(\u00b7), we assume $$K \\subset S^{n-1}$$. We set $$\\beta = 0$$ so that $$f_{i,\\beta} = f_i$$. We introduce the quantities $$\\mu = E[f(g)g]$$, $$\\psi = \\|f(g)\\|_{\\psi}^2$$, where $$g \\sim N(0, 1)$$. We choose $$T = \\mu$$ and set parameters in Assumption 3 as $$A(1)_g \\approx \\psi + \\mu$$, $$U(1)_g \\approx (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$$, $$P(1)_0 \\approx P'_0 + \\exp(-\\Omega(n))$$, $$A(2)_g \\approx \\psi + \\mu$$, $$U(2)_g = 0$$, $$P(2)_0 = 0$$. Now we are ready to apply Theorem 1 to this model. We obtain:\n\n$$\\text{Corollary 3. Consider Assumption 1 with } \\hat{L}\\text{-Lipschitz f, suppose that } P(f(0) \\leq \\hat{B}) \\geq 1 - P'_{0}\\text{, and define the parameters } \\mu = E[f(g)g]\\text{, } \\psi = \\|f(g)\\|_{\\psi}^2 \\text{ with } g \\sim N(0, 1)\\text{. Let } \\epsilon \\in (0, 1)\\text{ be any given recovery accuracy. If } m \\gtrsim (\\mu+\\psi)k \\log \\left[ \\frac{B+\\psi}{\\epsilon^2} \\right]\\text{, then with probability at least } 1 - \\frac{2m}{\\exp(-cn)} - mP'_0 - c_1 \\exp(-\\Omega(k)) \\text{ on a single draw of } (a_i, f_i)_{m}^{i=1}\\text{, we have the uniform signal recovery guarantee } \\| \\hat{x} - \\mu x^* \\|_2 \\leq \\epsilon \\text{ for all } x^* \\in K\\text{, where } \\hat{x} \\text{ is the solution to (2.1) with } y = f(Ax^*) \\text{ and } T = \\mu$$\n\nRemark 4. While the main result of [33] is non-uniform, it was noted in [33, Section 5] that a similar uniform error rate can be established for any deterministic 1-Lipschitz f. Our result here is more general in that the $$\\hat{L}$$-Lipschitz f is possibly random. Note that randomness on f is significant because it provides much more flexibility (e.g., additive random noise).\n\nRemark 5. For SIM with unknown fi it may seem impractical to use (2.1) as it requires $$\\mu = E[f(g)g]$$ where $$g \\sim N(0, 1)$$. However, by assuming $$\\mu x^* \\in K = G(Bk^{2}(r))$$ as in [33], which is natural for sufficiently expressive $$G(\\cdot)$$, we can simply use $$x \\in K$$ as constraint in (2.1). Our Corollary 3 remains valid in this case under some inessential changes of log $$\\mu$$ factors in the sample complexity.\n\n(D) Uniformly quantized GCS with dithering. The uniform quantizer with resolution $$\\delta > 0$$ is defined as $$Q_{\\delta}(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + \\frac{1}{2}$$ for $$a \\in \\mathbb{R}$$. Using dithering $$\\tau_i \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$$, we suppose that the observations are $$y_i = Q_{\\delta}(a_i^{\\top} x^* + \\tau_i)$$. This satisfies Assumption 2 with $$(B_0, L_0, \\beta_0) = (\\delta, 0, \\delta)$$. We set $$T = 1$$ and take parameters for Assumption 3 as follows: $$A(1)_g, U(1)_g \\approx \\delta$$, and $$A(2)_g, U(2)_g, P(1)_0 = P(2)_0 = 0$$. We take $$\\beta = \\beta_1 \\approx k\\delta$$ to confirm (2.6). With these parameters, we obtain the following from Theorem 1.\n\n$$\\text{Corollary 4. Consider Assumption 1 with } f(\\cdot) = Q_{\\delta}(\\cdot + \\tau), \\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right] \\text{ for some quantization resolution } \\delta > 0\\text{. Let } \\epsilon > 0 \\text{ be any given recovery accuracy. If } m \\gtrsim \\delta^2 k Lr\\sqrt{mn}\\text{, then with probability at least } 1 - \\frac{2m}{\\exp(-cn)} - c_1 \\exp(-\\Omega(k)) \\text{ on a single draw of } (a_i, \\tau_i)_{m}^{i=1}\\text{, we have the uniform recovery guarantee } \\| \\hat{x} - x^* \\|_2 \\leq \\epsilon \\text{ for all } x^* \\in K\\text{, where } \\hat{x} \\text{ is the solution to (2.1) with } y = Q_{\\delta}(Ax + \\tau) \\text{ and } T = 1 \\text{(here, } \\tau = [\\tau_1, ..., \\tau_m]^{\\top}\\text{)}$$\n\nRemark 6. While this dithered uniform quantized model has been widely studied in traditional CS (e.g., non-uniform recovery [8,48], uniform recovery [17,52]), it has not been investigated in GCS even for non-uniform recovery. Thus, this is new to the best of our knowledge.\n\nA simple extension to the noisy model $$y = f(Ax^*) + \\eta$$ where $$\\eta \\in \\mathbb{R}^m$$ has i.i.d. sub-Gaussian entries can be obtained by a fairly straightforward extension of our analysis; see Appendix F.", "md": "the number of layers), our error rate coincides with theirs up to a logarithmic factor. Additionally, as already mentioned in the Introduction Section, our result can be generalized to a sensing vector with an unknown covariance matrix, unlike theirs which is restricted to isotropic sensing vectors. The advantage of their result is in allowing sub-exponential sensing vectors.\n\n(C) Lipschitz-continuous SIM with generative prior. Assume that any realization of f is unconditionally $$\\hat{L}$$-Lipschitz, which implies Assumption 2 with $$(B_0, L_0, \\beta_0) = (0, \\hat{L}, \\infty)$$. We further assume $$P(f(0) \\leq \\hat{B}) \\geq 1 - P'_{0}$$ for some $$\\hat{B}_0$$. Because the norm of $$x^*$$ is absorbed into the unknown f(\u00b7), we assume $$K \\subset S^{n-1}$$. We set $$\\beta = 0$$ so that $$f_{i,\\beta} = f_i$$. We introduce the quantities $$\\mu = E[f(g)g]$$, $$\\psi = \\|f(g)\\|_{\\psi}^2$$, where $$g \\sim N(0, 1)$$. We choose $$T = \\mu$$ and set parameters in Assumption 3 as $$A(1)_g \\approx \\psi + \\mu$$, $$U(1)_g \\approx (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$$, $$P(1)_0 \\approx P'_0 + \\exp(-\\Omega(n))$$, $$A(2)_g \\approx \\psi + \\mu$$, $$U(2)_g = 0$$, $$P(2)_0 = 0$$. Now we are ready to apply Theorem 1 to this model. We obtain:\n\n$$\\text{Corollary 3. Consider Assumption 1 with } \\hat{L}\\text{-Lipschitz f, suppose that } P(f(0) \\leq \\hat{B}) \\geq 1 - P'_{0}\\text{, and define the parameters } \\mu = E[f(g)g]\\text{, } \\psi = \\|f(g)\\|_{\\psi}^2 \\text{ with } g \\sim N(0, 1)\\text{. Let } \\epsilon \\in (0, 1)\\text{ be any given recovery accuracy. If } m \\gtrsim (\\mu+\\psi)k \\log \\left[ \\frac{B+\\psi}{\\epsilon^2} \\right]\\text{, then with probability at least } 1 - \\frac{2m}{\\exp(-cn)} - mP'_0 - c_1 \\exp(-\\Omega(k)) \\text{ on a single draw of } (a_i, f_i)_{m}^{i=1}\\text{, we have the uniform signal recovery guarantee } \\| \\hat{x} - \\mu x^* \\|_2 \\leq \\epsilon \\text{ for all } x^* \\in K\\text{, where } \\hat{x} \\text{ is the solution to (2.1) with } y = f(Ax^*) \\text{ and } T = \\mu$$\n\nRemark 4. While the main result of [33] is non-uniform, it was noted in [33, Section 5] that a similar uniform error rate can be established for any deterministic 1-Lipschitz f. Our result here is more general in that the $$\\hat{L}$$-Lipschitz f is possibly random. Note that randomness on f is significant because it provides much more flexibility (e.g., additive random noise).\n\nRemark 5. For SIM with unknown fi it may seem impractical to use (2.1) as it requires $$\\mu = E[f(g)g]$$ where $$g \\sim N(0, 1)$$. However, by assuming $$\\mu x^* \\in K = G(Bk^{2}(r))$$ as in [33], which is natural for sufficiently expressive $$G(\\cdot)$$, we can simply use $$x \\in K$$ as constraint in (2.1). Our Corollary 3 remains valid in this case under some inessential changes of log $$\\mu$$ factors in the sample complexity.\n\n(D) Uniformly quantized GCS with dithering. The uniform quantizer with resolution $$\\delta > 0$$ is defined as $$Q_{\\delta}(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + \\frac{1}{2}$$ for $$a \\in \\mathbb{R}$$. Using dithering $$\\tau_i \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$$, we suppose that the observations are $$y_i = Q_{\\delta}(a_i^{\\top} x^* + \\tau_i)$$. This satisfies Assumption 2 with $$(B_0, L_0, \\beta_0) = (\\delta, 0, \\delta)$$. We set $$T = 1$$ and take parameters for Assumption 3 as follows: $$A(1)_g, U(1)_g \\approx \\delta$$, and $$A(2)_g, U(2)_g, P(1)_0 = P(2)_0 = 0$$. We take $$\\beta = \\beta_1 \\approx k\\delta$$ to confirm (2.6). With these parameters, we obtain the following from Theorem 1.\n\n$$\\text{Corollary 4. Consider Assumption 1 with } f(\\cdot) = Q_{\\delta}(\\cdot + \\tau), \\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right] \\text{ for some quantization resolution } \\delta > 0\\text{. Let } \\epsilon > 0 \\text{ be any given recovery accuracy. If } m \\gtrsim \\delta^2 k Lr\\sqrt{mn}\\text{, then with probability at least } 1 - \\frac{2m}{\\exp(-cn)} - c_1 \\exp(-\\Omega(k)) \\text{ on a single draw of } (a_i, \\tau_i)_{m}^{i=1}\\text{, we have the uniform recovery guarantee } \\| \\hat{x} - x^* \\|_2 \\leq \\epsilon \\text{ for all } x^* \\in K\\text{, where } \\hat{x} \\text{ is the solution to (2.1) with } y = Q_{\\delta}(Ax + \\tau) \\text{ and } T = 1 \\text{(here, } \\tau = [\\tau_1, ..., \\tau_m]^{\\top}\\text{)}$$\n\nRemark 6. While this dithered uniform quantized model has been widely studied in traditional CS (e.g., non-uniform recovery [8,48], uniform recovery [17,52]), it has not been investigated in GCS even for non-uniform recovery. Thus, this is new to the best of our knowledge.\n\nA simple extension to the noisy model $$y = f(Ax^*) + \\eta$$ where $$\\eta \\in \\mathbb{R}^m$$ has i.i.d. sub-Gaussian entries can be obtained by a fairly straightforward extension of our analysis; see Appendix F."}, {"type": "heading", "lvl": 3, "value": "Proof Sketch", "md": "### Proof Sketch"}, {"type": "text", "value": "To provide a sketch of our proof, we begin with the optimality condition $$\\|y - A\\hat{x}\\|_{2}^{2} \\leq \\|y - A(Tx^*)\\|_{2}^{2}$$. We expand the square and plug in $$y = f(Ax^*)$$ to obtain\n\nFor the final goal $$\\|\\hat{x} - Tx^*\\|_{2} \\leq \\epsilon$$, up to rescaling, it is enough to prove $$\\|\\hat{x} - Tx^*\\|_{2} \\leq 3\\epsilon$$.\n\nWe assume for convenience that $$\\|\\hat{x} - Tx^*\\|_{2} > 2\\epsilon$$, without loss of generality. Combined with", "md": "To provide a sketch of our proof, we begin with the optimality condition $$\\|y - A\\hat{x}\\|_{2}^{2} \\leq \\|y - A(Tx^*)\\|_{2}^{2}$$. We expand the square and plug in $$y = f(Ax^*)$$ to obtain\n\nFor the final goal $$\\|\\hat{x} - Tx^*\\|_{2} \\leq \\epsilon$$, up to rescaling, it is enough to prove $$\\|\\hat{x} - Tx^*\\|_{2} \\leq 3\\epsilon$$.\n\nWe assume for convenience that $$\\|\\hat{x} - Tx^*\\|_{2} > 2\\epsilon$$, without loss of generality. Combined with"}]}, {"page": 8, "text": " \u02c6                                                                                                   Bn         c, K\u2212      = K \u2212       K. We\n x, T  x\u2217    \u2208  T  K, we know \u02c6       x \u2212    T x\u2217    \u2208  K\u2212 \u03f5 , where K\u2212     \u03f5 := (T     K\u2212) \u2229          2 (2\u03f5)\n further define                                  (K\u2212  \u03f5 )\u2217  :=     z/\u2225z\u22252 : z \u2208         K\u2212 \u03f5                                             (3.2)\n                                                            \u02c6\n where the normalized error lives, i.e.                  \u2225\u02c6x\u2212T x\u2217                \u03f5 )\u2217. Our strategy is to establish a uniform\n                                                           x\u2212T x\u2217\u22252 \u2208        (K\u2212\n lower bound (resp., upper bound) for the left-hand side (resp., the right-hand side) of (3.1). We\n emphasize that these bounds must hold uniformly for all x\u2217                             \u2208   K.\n It is relatively easy to use set-restricted eigenvalue condition (S-REC) [2] to establish a uniform lower\n bound for the left-hand side of (3.1), see Appendix B.1 for more details. It is significantly more\n challenging to derive an upper bound for the right-hand side of (3.1). As the upper bound must hold\n uniformly for all x\u2217, we first take the supremum over x\u2217                         and \u02c6 x and consider bounding the following:\n   R := 1    mm   f(Ax\u2217) \u2212         T  Ax\u2217, A(\u02c6     x \u2212    T x\u2217)\n    = 1             fi(a\u22a4  i x\u2217) \u2212     T  a\u22a4         \u00b7   a\u22a4   x \u2212    T  x\u2217]\n        m    i=1                            i x\u2217      m    i [\u02c6                                                                          (3.3)\n    \u2264   \u2225\u02c6x \u2212   T  x\u2217\u22252 \u00b7 sup          sup       1          fi(a\u22a4  i x) \u2212     T a\u22a4 i x    \u00b7  a\u22a4 i v    := \u2225\u02c6  x \u2212   T  x\u2217\u22252 \u00b7 Ru,\n                             x\u2208K    v\u2208(K\u2212  \u03f5 )\u2217  m   i=1\n where (K\u2212     \u03f5 )\u2217  is defined in (3.2). Clearly, Ru is the supremum of a product process, whose factors\n are indexed by x \u2208          K and v \u2208       (K\u2212  \u03f5 )\u2217. It is, in general, challenging to control a product process, and\n existing results often require both factors to satisfy a certain \u201csub-Gaussian increments\u201d condition\n (e.g., [36, 37]). However, the first factor of Ru (i.e., fi(a\u22a4                        i x\u2217) \u2212      T  a\u22a4i x\u2217) does not admit such\n a condition when fi is not continuous (e.g., the 1-bit model fi = sign(\u00b7)). We will construct the\n Lipschitz approximation of fi to overcome this difficulty shortly in Section 3.1.\n Remark 7. We note that these challenges stem from our pursuit of uniform recovery. In fact, a\n non-uniform guarantee for SIM was presented in [33, Theorem 1]. In its proof, the key ingredient\n is [33, Lemma 3] that bounds Ru without the supremum on x. This can be done as long as fi(a\u22a4                                            i x\u2217)\n is sub-Gaussian, while the potential discontinuity of fi is totally unproblematic.\n 3.1     Lipschitz Approximation\n For any x0 \u2208          Dfi we define the one-sided limits as f \u2212                   i (x0) = lim         x\u2192x\u2212  0 fi(x) and f +     i (x0) =\n lim  x\u2192x+  0 fi(x), and write their average as f a           i (x0) = 1    2(f \u2212i (x0)+f +     i (x0)). Given any approximation\n accuracy \u03b2 \u2208      \uf8f1 (0, \u03b202 ), we construct the Lipschitz continuous function fi,\u03b2 as:\n                   \uf8f4                       fi(x)                 ,            if x /\u2208   Dfi + [\u2212\u03b2\n                   \uf8f4                                                                                  2 , \u03b22 ]\n   fi,\u03b2(x) =       \uf8f2  f a                i (x0)\u2212fi(x0\u2212\u03b22 )](x0\u2212x)        ,    if \u2203x0 \u2208      Dfi s.t. x \u2208      [x0 \u2212    \u03b2              . (3.4)\n                   \uf8f4    i (x0) \u2212     2[f a            \u03b2                                                                2 , x0]\n                   \uf8f4\n                   \uf8f3                                   i (x0)](x\u2212x0)\n                      f a                        2 )\u2212f a                 ,    if \u2203x0 \u2208      Dfi, s.t. x \u2208      [x0, x0 + \u03b2\n                        i (x0) + 2[fi(x0+ \u03b2           \u03b2                                                                       2 ]\nWe have defined the approximation error \u03b5i,\u03b2(\u00b7) = fi,\u03b2(\u00b7) \u2212                               fi(\u00b7) in Assumption 3. An important\n observation is that both fi,\u03b2 and |\u03b5i,\u03b2| are Lipschitz continuous (see Lemma 1 below). Here, it is\n crucial to consider |\u03b5i,\u03b2| rather than \u03b5i,\u03b2 as the latter is not continuous; see Figure 1 for an intuitive\n graphical illustration and more explanations in Appendix B.2.\n Lemma 1. With B0, L0, \u03b20 given in Assumption 2, for any \u03b2 \u2208                                (0, \u03b202 ), fi,\u03b2 is      L0 + B0  \u03b2    -Lipschitz\n over R, and |\u03b5i,\u03b2| is          2L0 + B0    \u03b2   -Lipschitz over R.\n 3.2     Bounding the product process\nWe now present our technique to bound Ru. Recall that \u03bei,\u03b2(a) and \u03b5i,\u03b2(a) were defined in (2.2). By\n Lemma 1, \u03bei,\u03b2 is          L0 + T + B0     \u03b2    -Lipschitz. Now we use fi(a) \u2212                 T a = \u03bei,\u03b2(a) \u2212        \u03b5i,\u03b2 to decompose\n Ru (in the following, we sometimes shorten \u201csup                         x\u2208K supv\u2208(K\u2212       \u03f5 )\u2217\u201d as \u201csupx,v\u201d):\n                    Ru \u2264      sup    1    m    \u03bei,\u03b2(a\u22a4  i x) \u00b7    a\u22a4i v    + sup      1    m    \u03b5i,\u03b2(a\u22a4   i x)     a\u22a4i v   .\n                               x,v   m   i=1      Ru1                  8       x,v   m    i=1     Ru2                                    (3.5)", "md": "# Math Equations\n\n$$\n\\hat{B}n \\quad c, K^{-} = K^{-} \\cap K. \\text{We } x, T x^{*} \\in T K, \\text{we know } \\hat{x} - T x^{*} \\in K^{-\\epsilon}, \\text{where } K^{-\\epsilon} := (T K^{-}) \\cap 2(2\\epsilon) \\text{ further define } (K^{-\\epsilon})^{*} := \\{z/\\|z\\|_{2} : z \\in K^{-\\epsilon}\\} \\quad (3.2) \\\\\n\\text{where the normalized error lives, i.e. } \\|\\hat{x} - T x^{*}\\|_{2} \\in (K^{-\\epsilon})^{*}. \\text{Our strategy is to establish a uniform lower bound (resp., upper bound) for the left-hand side (resp., the right-hand side) of (3.1). We emphasize that these bounds must hold uniformly for all } x^{*} \\in K. \\\\\n\\text{It is relatively easy to use set-restricted eigenvalue condition (S-REC) [2] to establish a uniform lower bound for the left-hand side of (3.1), see Appendix B.1 for more details. It is significantly more challenging to derive an upper bound for the right-hand side of (3.1). As the upper bound must hold uniformly for all } x^{*}, \\text{we first take the supremum over } x^{*} \\text{ and } \\hat{x} \\text{ and consider bounding the following:} \\\\\nR := \\frac{1}{m} \\sum_{i=1}^{m} [f(Ax^{*}) - T Ax^{*}, A(\\hat{x} - T x^{*})] = \\frac{1}{m} \\sum_{i=1}^{m} [f_{i}(a_{i}^{T} x^{*}) - T a_{i}^{T} \\cdot a_{i}^{T} x - T x^{*}] \\leq \\|\\hat{x} - T x^{*}\\|_{2} \\cdot \\sup_{x \\in K} \\sup_{v \\in (K^{-\\epsilon})^{*}} \\frac{1}{m} [f_{i}(a_{i}^{T} x) - T a_{i}^{T} x \\cdot a_{i}^{T} v] := \\|\\hat{x} - T x^{*}\\|_{2} \\cdot Ru,\n$$\n\n$$\n\\text{where } (K^{-\\epsilon})^{*} \\text{ is defined in (3.2). Clearly, Ru is the supremum of a product process, whose factors are indexed by } x \\in K \\text{ and } v \\in (K^{-\\epsilon})^{*}. \\text{It is, in general, challenging to control a product process, and existing results often require both factors to satisfy a certain \"sub-Gaussian increments\" condition (e.g., [36, 37]). However, the first factor of Ru (i.e., } f_{i}(a_{i}^{T} x^{*}) - T a_{i}^{T} x^{*}) \\text{ does not admit such a condition when } f_{i} \\text{ is not continuous (e.g., the 1-bit model } f_{i} = \\text{sign}(\\cdot)). \\text{We will construct the Lipschitz approximation of } f_{i} \\text{ to overcome this difficulty shortly in Section 3.1.} \\\\\n\\text{Remark 7. We note that these challenges stem from our pursuit of uniform recovery. In fact, a non-uniform guarantee for SIM was presented in [33, Theorem 1]. In its proof, the key ingredient is [33, Lemma 3] that bounds Ru without the supremum on } x. \\text{This can be done as long as } f_{i}(a_{i}^{T} x^{*}) \\text{ is sub-Gaussian, while the potential discontinuity of } f_{i} \\text{ is totally unproblematic.} \\\\\n\\text{3.1 Lipschitz Approximation} \\\\\n\\text{For any } x_{0} \\in D_{fi} \\text{ we define the one-sided limits as } f_{i}^{-}(x_{0}) = \\lim_{x \\to x^{-}_{0}} f_{i}(x) \\text{ and } f_{i}^{+}(x_{0}) = \\lim_{x \\to x^{+}_{0}} f_{i}(x), \\text{ and write their average as } f_{a}^{i}(x_{0}) = \\frac{1}{2}(f_{i}^{-}(x_{0}) + f_{i}^{+}(x_{0})). \\text{Given any approximation accuracy } \\beta \\in (0, \\beta_{0}^{2}), \\text{we construct the Lipschitz continuous function } f_{i,\\beta} \\text{ as:} \\\\\nf_{i,\\beta}(x) = \\begin{cases} f_{i}(x), & \\text{if } x \\notin D_{fi} + [-\\frac{\\beta}{2}, \\frac{\\beta}{2}] \\\\ f_{a}^{i}(x_{0}) - f_{i}(x_{0} - \\frac{\\beta}{2})(x_{0} - x), & \\text{if } \\exists x_{0} \\in D_{fi} \\text{ s.t. } x \\in [x_{0} - \\beta, x_{0}] \\\\ f_{a}^{i}(x_{0}) + 2[f_{a}^{i}(x_{0} + \\frac{\\beta}{2}) - f_{a}^{i}(x_{0})](x - x_{0}), & \\text{if } \\exists x_{0} \\in D_{fi} \\text{ s.t. } x \\in [x_{0}, x_{0} + \\beta] \\end{cases} \\quad (3.4)\n$$\n\n$$\n\\text{We have defined the approximation error } \\epsilon_{i,\\beta}(\\cdot) = f_{i,\\beta}(\\cdot) - f_{i}(\\cdot) \\text{ in Assumption 3. An important observation is that both } f_{i,\\beta} \\text{ and } |\\epsilon_{i,\\beta}| \\text{ are Lipschitz continuous (see Lemma 1 below). Here, it is crucial to consider } |\\epsilon_{i,\\beta}| \\text{ rather than } \\epsilon_{i,\\beta} \\text{ as the latter is not continuous; see Figure 1 for an intuitive graphical illustration and more explanations in Appendix B.2.} \\\\\n\\text{Lemma 1. With } B_{0}, L_{0}, \\beta_{0} \\text{ given in Assumption 2, for any } \\beta \\in (0, \\beta_{0}^{2}), f_{i,\\beta} \\text{ is } L_{0} + B_{0} \\beta \\text{ -Lipschitz over } \\mathbb{R}, \\text{ and } |\\epsilon_{i,\\beta}| \\text{ is } 2L_{0} + B_{0} \\beta \\text{ -Lipschitz over } \\mathbb{R}. \\\\\n\\text{3.2 Bounding the product process} \\\\\n\\text{We now present our technique to bound Ru. Recall that } \\xi_{i,\\beta}(a) \\text{ and } \\epsilon_{i,\\beta}(a) \\text{ were defined in (2.2). By Lemma 1, } \\xi_{i,\\beta} \\text{ is } L_{0} + T + B_{0} \\beta \\text{ -Lipschitz. Now we use } f_{i}(a) - T a = \\xi_{i,\\beta}(a) - \\epsilon_{i,\\beta} \\text{ to decompose Ru (in the following, we sometimes shorten \"sup } x \\in K \\text{ sup } v \\in (K^{-\\epsilon})^{*} \\text{ as \"supx,v\"):} \\\\\nRu \\leq \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} \\xi_{i,\\beta}(a_{i}^{T} x) \\cdot a_{i}^{T} v + \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} \\epsilon_{i,\\beta}(a_{i}^{T} x) \\cdot a_{i}^{T} v. \\quad (3.5)\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "$$\n\\hat{B}n \\quad c, K^{-} = K^{-} \\cap K. \\text{We } x, T x^{*} \\in T K, \\text{we know } \\hat{x} - T x^{*} \\in K^{-\\epsilon}, \\text{where } K^{-\\epsilon} := (T K^{-}) \\cap 2(2\\epsilon) \\text{ further define } (K^{-\\epsilon})^{*} := \\{z/\\|z\\|_{2} : z \\in K^{-\\epsilon}\\} \\quad (3.2) \\\\\n\\text{where the normalized error lives, i.e. } \\|\\hat{x} - T x^{*}\\|_{2} \\in (K^{-\\epsilon})^{*}. \\text{Our strategy is to establish a uniform lower bound (resp., upper bound) for the left-hand side (resp., the right-hand side) of (3.1). We emphasize that these bounds must hold uniformly for all } x^{*} \\in K. \\\\\n\\text{It is relatively easy to use set-restricted eigenvalue condition (S-REC) [2] to establish a uniform lower bound for the left-hand side of (3.1), see Appendix B.1 for more details. It is significantly more challenging to derive an upper bound for the right-hand side of (3.1). As the upper bound must hold uniformly for all } x^{*}, \\text{we first take the supremum over } x^{*} \\text{ and } \\hat{x} \\text{ and consider bounding the following:} \\\\\nR := \\frac{1}{m} \\sum_{i=1}^{m} [f(Ax^{*}) - T Ax^{*}, A(\\hat{x} - T x^{*})] = \\frac{1}{m} \\sum_{i=1}^{m} [f_{i}(a_{i}^{T} x^{*}) - T a_{i}^{T} \\cdot a_{i}^{T} x - T x^{*}] \\leq \\|\\hat{x} - T x^{*}\\|_{2} \\cdot \\sup_{x \\in K} \\sup_{v \\in (K^{-\\epsilon})^{*}} \\frac{1}{m} [f_{i}(a_{i}^{T} x) - T a_{i}^{T} x \\cdot a_{i}^{T} v] := \\|\\hat{x} - T x^{*}\\|_{2} \\cdot Ru,\n$$\n\n$$\n\\text{where } (K^{-\\epsilon})^{*} \\text{ is defined in (3.2). Clearly, Ru is the supremum of a product process, whose factors are indexed by } x \\in K \\text{ and } v \\in (K^{-\\epsilon})^{*}. \\text{It is, in general, challenging to control a product process, and existing results often require both factors to satisfy a certain \"sub-Gaussian increments\" condition (e.g., [36, 37]). However, the first factor of Ru (i.e., } f_{i}(a_{i}^{T} x^{*}) - T a_{i}^{T} x^{*}) \\text{ does not admit such a condition when } f_{i} \\text{ is not continuous (e.g., the 1-bit model } f_{i} = \\text{sign}(\\cdot)). \\text{We will construct the Lipschitz approximation of } f_{i} \\text{ to overcome this difficulty shortly in Section 3.1.} \\\\\n\\text{Remark 7. We note that these challenges stem from our pursuit of uniform recovery. In fact, a non-uniform guarantee for SIM was presented in [33, Theorem 1]. In its proof, the key ingredient is [33, Lemma 3] that bounds Ru without the supremum on } x. \\text{This can be done as long as } f_{i}(a_{i}^{T} x^{*}) \\text{ is sub-Gaussian, while the potential discontinuity of } f_{i} \\text{ is totally unproblematic.} \\\\\n\\text{3.1 Lipschitz Approximation} \\\\\n\\text{For any } x_{0} \\in D_{fi} \\text{ we define the one-sided limits as } f_{i}^{-}(x_{0}) = \\lim_{x \\to x^{-}_{0}} f_{i}(x) \\text{ and } f_{i}^{+}(x_{0}) = \\lim_{x \\to x^{+}_{0}} f_{i}(x), \\text{ and write their average as } f_{a}^{i}(x_{0}) = \\frac{1}{2}(f_{i}^{-}(x_{0}) + f_{i}^{+}(x_{0})). \\text{Given any approximation accuracy } \\beta \\in (0, \\beta_{0}^{2}), \\text{we construct the Lipschitz continuous function } f_{i,\\beta} \\text{ as:} \\\\\nf_{i,\\beta}(x) = \\begin{cases} f_{i}(x), & \\text{if } x \\notin D_{fi} + [-\\frac{\\beta}{2}, \\frac{\\beta}{2}] \\\\ f_{a}^{i}(x_{0}) - f_{i}(x_{0} - \\frac{\\beta}{2})(x_{0} - x), & \\text{if } \\exists x_{0} \\in D_{fi} \\text{ s.t. } x \\in [x_{0} - \\beta, x_{0}] \\\\ f_{a}^{i}(x_{0}) + 2[f_{a}^{i}(x_{0} + \\frac{\\beta}{2}) - f_{a}^{i}(x_{0})](x - x_{0}), & \\text{if } \\exists x_{0} \\in D_{fi} \\text{ s.t. } x \\in [x_{0}, x_{0} + \\beta] \\end{cases} \\quad (3.4)\n$$\n\n$$\n\\text{We have defined the approximation error } \\epsilon_{i,\\beta}(\\cdot) = f_{i,\\beta}(\\cdot) - f_{i}(\\cdot) \\text{ in Assumption 3. An important observation is that both } f_{i,\\beta} \\text{ and } |\\epsilon_{i,\\beta}| \\text{ are Lipschitz continuous (see Lemma 1 below). Here, it is crucial to consider } |\\epsilon_{i,\\beta}| \\text{ rather than } \\epsilon_{i,\\beta} \\text{ as the latter is not continuous; see Figure 1 for an intuitive graphical illustration and more explanations in Appendix B.2.} \\\\\n\\text{Lemma 1. With } B_{0}, L_{0}, \\beta_{0} \\text{ given in Assumption 2, for any } \\beta \\in (0, \\beta_{0}^{2}), f_{i,\\beta} \\text{ is } L_{0} + B_{0} \\beta \\text{ -Lipschitz over } \\mathbb{R}, \\text{ and } |\\epsilon_{i,\\beta}| \\text{ is } 2L_{0} + B_{0} \\beta \\text{ -Lipschitz over } \\mathbb{R}. \\\\\n\\text{3.2 Bounding the product process} \\\\\n\\text{We now present our technique to bound Ru. Recall that } \\xi_{i,\\beta}(a) \\text{ and } \\epsilon_{i,\\beta}(a) \\text{ were defined in (2.2). By Lemma 1, } \\xi_{i,\\beta} \\text{ is } L_{0} + T + B_{0} \\beta \\text{ -Lipschitz. Now we use } f_{i}(a) - T a = \\xi_{i,\\beta}(a) - \\epsilon_{i,\\beta} \\text{ to decompose Ru (in the following, we sometimes shorten \"sup } x \\in K \\text{ sup } v \\in (K^{-\\epsilon})^{*} \\text{ as \"supx,v\"):} \\\\\nRu \\leq \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} \\xi_{i,\\beta}(a_{i}^{T} x) \\cdot a_{i}^{T} v + \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} \\epsilon_{i,\\beta}(a_{i}^{T} x) \\cdot a_{i}^{T} v. \\quad (3.5)\n$$", "md": "$$\n\\hat{B}n \\quad c, K^{-} = K^{-} \\cap K. \\text{We } x, T x^{*} \\in T K, \\text{we know } \\hat{x} - T x^{*} \\in K^{-\\epsilon}, \\text{where } K^{-\\epsilon} := (T K^{-}) \\cap 2(2\\epsilon) \\text{ further define } (K^{-\\epsilon})^{*} := \\{z/\\|z\\|_{2} : z \\in K^{-\\epsilon}\\} \\quad (3.2) \\\\\n\\text{where the normalized error lives, i.e. } \\|\\hat{x} - T x^{*}\\|_{2} \\in (K^{-\\epsilon})^{*}. \\text{Our strategy is to establish a uniform lower bound (resp., upper bound) for the left-hand side (resp., the right-hand side) of (3.1). We emphasize that these bounds must hold uniformly for all } x^{*} \\in K. \\\\\n\\text{It is relatively easy to use set-restricted eigenvalue condition (S-REC) [2] to establish a uniform lower bound for the left-hand side of (3.1), see Appendix B.1 for more details. It is significantly more challenging to derive an upper bound for the right-hand side of (3.1). As the upper bound must hold uniformly for all } x^{*}, \\text{we first take the supremum over } x^{*} \\text{ and } \\hat{x} \\text{ and consider bounding the following:} \\\\\nR := \\frac{1}{m} \\sum_{i=1}^{m} [f(Ax^{*}) - T Ax^{*}, A(\\hat{x} - T x^{*})] = \\frac{1}{m} \\sum_{i=1}^{m} [f_{i}(a_{i}^{T} x^{*}) - T a_{i}^{T} \\cdot a_{i}^{T} x - T x^{*}] \\leq \\|\\hat{x} - T x^{*}\\|_{2} \\cdot \\sup_{x \\in K} \\sup_{v \\in (K^{-\\epsilon})^{*}} \\frac{1}{m} [f_{i}(a_{i}^{T} x) - T a_{i}^{T} x \\cdot a_{i}^{T} v] := \\|\\hat{x} - T x^{*}\\|_{2} \\cdot Ru,\n$$\n\n$$\n\\text{where } (K^{-\\epsilon})^{*} \\text{ is defined in (3.2). Clearly, Ru is the supremum of a product process, whose factors are indexed by } x \\in K \\text{ and } v \\in (K^{-\\epsilon})^{*}. \\text{It is, in general, challenging to control a product process, and existing results often require both factors to satisfy a certain \"sub-Gaussian increments\" condition (e.g., [36, 37]). However, the first factor of Ru (i.e., } f_{i}(a_{i}^{T} x^{*}) - T a_{i}^{T} x^{*}) \\text{ does not admit such a condition when } f_{i} \\text{ is not continuous (e.g., the 1-bit model } f_{i} = \\text{sign}(\\cdot)). \\text{We will construct the Lipschitz approximation of } f_{i} \\text{ to overcome this difficulty shortly in Section 3.1.} \\\\\n\\text{Remark 7. We note that these challenges stem from our pursuit of uniform recovery. In fact, a non-uniform guarantee for SIM was presented in [33, Theorem 1]. In its proof, the key ingredient is [33, Lemma 3] that bounds Ru without the supremum on } x. \\text{This can be done as long as } f_{i}(a_{i}^{T} x^{*}) \\text{ is sub-Gaussian, while the potential discontinuity of } f_{i} \\text{ is totally unproblematic.} \\\\\n\\text{3.1 Lipschitz Approximation} \\\\\n\\text{For any } x_{0} \\in D_{fi} \\text{ we define the one-sided limits as } f_{i}^{-}(x_{0}) = \\lim_{x \\to x^{-}_{0}} f_{i}(x) \\text{ and } f_{i}^{+}(x_{0}) = \\lim_{x \\to x^{+}_{0}} f_{i}(x), \\text{ and write their average as } f_{a}^{i}(x_{0}) = \\frac{1}{2}(f_{i}^{-}(x_{0}) + f_{i}^{+}(x_{0})). \\text{Given any approximation accuracy } \\beta \\in (0, \\beta_{0}^{2}), \\text{we construct the Lipschitz continuous function } f_{i,\\beta} \\text{ as:} \\\\\nf_{i,\\beta}(x) = \\begin{cases} f_{i}(x), & \\text{if } x \\notin D_{fi} + [-\\frac{\\beta}{2}, \\frac{\\beta}{2}] \\\\ f_{a}^{i}(x_{0}) - f_{i}(x_{0} - \\frac{\\beta}{2})(x_{0} - x), & \\text{if } \\exists x_{0} \\in D_{fi} \\text{ s.t. } x \\in [x_{0} - \\beta, x_{0}] \\\\ f_{a}^{i}(x_{0}) + 2[f_{a}^{i}(x_{0} + \\frac{\\beta}{2}) - f_{a}^{i}(x_{0})](x - x_{0}), & \\text{if } \\exists x_{0} \\in D_{fi} \\text{ s.t. } x \\in [x_{0}, x_{0} + \\beta] \\end{cases} \\quad (3.4)\n$$\n\n$$\n\\text{We have defined the approximation error } \\epsilon_{i,\\beta}(\\cdot) = f_{i,\\beta}(\\cdot) - f_{i}(\\cdot) \\text{ in Assumption 3. An important observation is that both } f_{i,\\beta} \\text{ and } |\\epsilon_{i,\\beta}| \\text{ are Lipschitz continuous (see Lemma 1 below). Here, it is crucial to consider } |\\epsilon_{i,\\beta}| \\text{ rather than } \\epsilon_{i,\\beta} \\text{ as the latter is not continuous; see Figure 1 for an intuitive graphical illustration and more explanations in Appendix B.2.} \\\\\n\\text{Lemma 1. With } B_{0}, L_{0}, \\beta_{0} \\text{ given in Assumption 2, for any } \\beta \\in (0, \\beta_{0}^{2}), f_{i,\\beta} \\text{ is } L_{0} + B_{0} \\beta \\text{ -Lipschitz over } \\mathbb{R}, \\text{ and } |\\epsilon_{i,\\beta}| \\text{ is } 2L_{0} + B_{0} \\beta \\text{ -Lipschitz over } \\mathbb{R}. \\\\\n\\text{3.2 Bounding the product process} \\\\\n\\text{We now present our technique to bound Ru. Recall that } \\xi_{i,\\beta}(a) \\text{ and } \\epsilon_{i,\\beta}(a) \\text{ were defined in (2.2). By Lemma 1, } \\xi_{i,\\beta} \\text{ is } L_{0} + T + B_{0} \\beta \\text{ -Lipschitz. Now we use } f_{i}(a) - T a = \\xi_{i,\\beta}(a) - \\epsilon_{i,\\beta} \\text{ to decompose Ru (in the following, we sometimes shorten \"sup } x \\in K \\text{ sup } v \\in (K^{-\\epsilon})^{*} \\text{ as \"supx,v\"):} \\\\\nRu \\leq \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} \\xi_{i,\\beta}(a_{i}^{T} x) \\cdot a_{i}^{T} v + \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} \\epsilon_{i,\\beta}(a_{i}^{T} x) \\cdot a_{i}^{T} v. \\quad (3.5)\n$$"}]}, {"page": 9, "text": "                                             4                                                    0.5\n                                             3\n                                             2\n                                             1                                                      0\n                                             0\n                                            -1\n                                                                                                 -0.5\n                                              -2        -1         0          1         2            -2         -1        0          1         2\n        Figure 1: (Left): fi and its approximation fi,0.5; (Right): approximation error \u03b5i,0.5, |\u03b5i,0.5|.\nIt remains to control Ru1 and Ru2. By the Lipschitz continuity of \u03bei,\u03b2 and |\u03b5i,\u03b2|, the factors of Ru1\nand Ru2 admit sub-Gaussian increments, so it is natural to first center them and then invoke the\nconcentration inequality for product process due to Mendelson [36, Theorem 1.13], which we restate\nin Lemma 5 (Appendix A). However, this does not produce a tight bound and would eventually\nrequire \u02dc      O(k/\u03f54) to achieve a uniform \u21132-error of \u03f5, as is the case in [17, Section 4].\nIn fact, Lemma 5 is based on Gaussian width and hence blind to the fact that K, (K\u2212                                                                         \u03f5 )\u2217    here have low\nmetric entropy (Lemma 6). By characterizing the low intrinsic dimension of index sets via metric\nentropy, we develop the following concentration inequality that can produce tighter bound for Ru1\nand Ru2. This also allows us to derive uniform error rates sharper than those in [17, Section 4].\nTheorem 2. Let gx = gx(a) and hv = hv(a) be stochastic processes indexed by x \u2208                                                                                   X \u2282        Rp1, v \u2208\nV \u2282       Rp2, both defined with respect to a common random variable a. Assume that:\n       \u2022 (A1.) gx(a), hv(a) are sub-Gaussian for some (Ag, Ah) and admit sub-Gaussian increments\n           regarding \u21132 distance for some (Mg, Mh):\n                               \u2225gx(a) \u2212            gx\u2032(a)\u2225\u03c82 \u2264               Mg\u2225x \u2212             x\u2032\u22252, \u2225gx(a)\u2225\u03c82 \u2264                      Ag, \u2200        x, x\u2032 \u2208         X   ;            (3.6)\n                               \u2225hv(a) \u2212            hv\u2032(a)\u2225\u03c82 \u2264                Mh\u2225v \u2212            v\u2032\u22252, \u2225hv(a)\u2225\u03c82 \u2264                       Ah, \u2200        v, v\u2032 \u2208        V.\n       \u2022 (A2.) On a single draw of a, for some (Lg, Ug, Lh, Uh) the following events simultaneously\n           hold with probability at least 1 \u2212                              P0:\n                                       |gx(a) \u2212           gx\u2032(a)| \u2264           Lg\u2225x \u2212           x\u2032\u22252, |gx(a)| \u2264                  Ug, \u2200        x, x\u2032 \u2208        X   ;                    (3.7)\n                                       |hv(a) \u2212           hv\u2032(a)| \u2264            Lh\u2225v \u2212           v\u2032\u22252, |hv(a)| \u2264                  Uh, \u2200        v, v\u2032 \u2208        V.\nLet a1, ..., am be i.i.d. copies of a, and introduce the shorthand Sg,h = LgUh + MgAh and\n                                                                                        \u221a AgAh                                    AgAh\nTg,h = LhUg + MhAg. If m \u2273                                              H        X   ,     mSg,h           + H            V,    \u221amTg,h           , where H (\u00b7, \u00b7) is the\nmetric entropy defined in Definition 2, then with probability at least 1 \u2212                                                                                mP0 \u2212            2 exp          \u2212\n                      AgAh                                 AgAh                                                                            AgAh                                AgAh\n                     \u221a  mSg,h ) + H (V,                  \u221amTg,h )                                            \u221am                          \u221amSg,h ) + H (V,                    \u221amTg,h ),\n\u2126     H (X        ,                                                            we have I \u2272                 AgAh           H (X        ,\nwhere I := sup                 x\u2208X supv\u2208V                 m1    m   i=1       gx(ai)hv(ai) \u2212                  E[gx(ai)hv(ai)]                        is the supremum of a\nproduct process.\nRemark 8. We use Ru2 as an example to illustrate the advantage of Theorem 2 over Lemma 5. The\nkey step is on bounding the centered process\n                             Ru2,c := sup                   sup            |\u03b5i,\u03b2(a\u22a4      i x)||a\u22a4      i v| \u2212       E[|\u03b5i,\u03b2(a\u22a4         i x)||a\u22a4     i v|]       .\n                                                x\u2208K     v\u2208(K\u2212     \u03f5 )\u2217\nLet gx(ai) = |\u03b5i,\u03b2(a\u22a4                      i x)| and hv(ai) = |a\u22a4                         i v|, then one can use Theorem 2 or Lemma 5 to\nbound Ru2,c. Note that \u2225a\u22a4                          i v\u2225\u03c82 = O(1) justifies the choice Ah = O(1), and both H (K, \u03b7) and\nH ((K\u2212        \u03f5 )\u2217, \u03b7) depend linearly on k but only logarithmically on \u03b7 (Lemma 6), so Theorem 2 could\nbound Ru2,c by \u02dc                O     Ag        k/m          that depends on Mg in a logarithmic manner. However, the bound\nproduced by Lemma 5 depends linearly on Mg; see term MgAh\u03c9(K)                                                      \u221a  m           in (A.1). From (3.6), Mg should\nbe proportional to the Lipschitz constant of |\u03b5i,\u03b2|, which scales as 1                                                            \u03b2 (Lemma 1). The issue is that\nin many cases we need to take extremely small \u03b2 to guarantee that (2.6) holds true (e.g., we take\n\u03b2 \u224d      k/m in 1-bit GCS). Thus, Lemma 5 produces a worse bound compared to our Theorem 2.\n                                                                                              9", "md": "# Math Equations and Text\n\n## Math Equations and Text\n\n$$\n\\begin{array}{ccccccccccc}\n4 & & & & & & & & & 0.5 \\\\\n3 & & & & & & & & & \\\\\n2 & & & & & & & & & \\\\\n1 & & & & & & & & & 0 \\\\\n0 & & & & & & & & & \\\\\n-1 & & & & & & & & & -0.5 \\\\\n& -2 & -1 & 0 & 1 & 2 & -2 & -1 & 0 & 1 & 2 \\\\\n\\end{array}\n$$\n\nFigure 1: (Left): \\(f_i\\) and its approximation \\(f_{i,0.5}\\); (Right): approximation error \\(\\varepsilon_{i,0.5}\\), \\(\\left|\\varepsilon_{i,0.5}\\right|\\).\n\nIt remains to control \\(Ru1\\) and \\(Ru2\\). By the Lipschitz continuity of \\(\\xi_{i,\\beta}\\) and \\(\\left|\\varepsilon_{i,\\beta}\\right|\\), the factors of \\(Ru1\\) and \\(Ru2\\) admit sub-Gaussian increments, so it is natural to first center them and then invoke the concentration inequality for product process due to Mendelson [36, Theorem 1.13], which we restate in Lemma 5 (Appendix A). However, this does not produce a tight bound and would eventually require \\(\\tilde{O}(k/\\epsilon^4)\\) to achieve a uniform \\(\\ell^2\\)-error of \\(\\epsilon\\), as is the case in [17, Section 4].\n\nIn fact, Lemma 5 is based on Gaussian width and hence blind to the fact that \\(K\\), \\((K-\\epsilon)^*\\) here have low metric entropy (Lemma 6). By characterizing the low intrinsic dimension of index sets via metric entropy, we develop the following concentration inequality that can produce tighter bound for \\(Ru1\\) and \\(Ru2\\). This also allows us to derive uniform error rates sharper than those in [17, Section 4].\n\nTheorem 2. Let \\(g_x = g_x(a)\\) and \\(h_v = h_v(a)\\) be stochastic processes indexed by \\(x \\in X \\subset \\mathbb{R}^{p_1}\\), \\(v \\in V \\subset \\mathbb{R}^{p_2}\\), both defined with respect to a common random variable \\(a\\). Assume that:\n\n$$\n\\begin{align*}\n&\\text{(A1.)} \\quad g_x(a), h_v(a) \\text{ are sub-Gaussian for some } (A_g, A_h) \\text{ and admit sub-Gaussian increments regarding } \\ell^2 \\text{ distance for some } (M_g, M_h): \\\\\n&\\|g_x(a) - g_{x'}(a)\\|_{\\psi_2} \\leq M_g\\|x - x'\\|_2, \\|g_x(a)\\|_{\\psi_2} \\leq A_g, \\forall x, x' \\in X; \\\\\n&\\|h_v(a) - h_{v'}(a)\\|_{\\psi_2} \\leq M_h\\|v - v'\\|_2, \\|h_v(a)\\|_{\\psi_2} \\leq A_h, \\forall v, v' \\in V. \\\\\n&\\text{(A2.)} \\text{ On a single draw of } a, \\text{ for some } (L_g, U_g, L_h, U_h) \\text{ the following events simultaneously hold with probability at least } 1 - P_0: \\\\\n&|g_x(a) - g_{x'}(a)| \\leq L_g\\|x - x'\\|_2, |g_x(a)| \\leq U_g, \\forall x, x' \\in X; \\\\\n&|h_v(a) - h_{v'}(a)| \\leq L_h\\|v - v'\\|_2, |h_v(a)| \\leq U_h, \\forall v, v' \\in V.\n\\end{align*}\n$$\nLet \\(a_1, ..., a_m\\) be i.i.d. copies of \\(a\\), and introduce the shorthand \\(S_{g,h} = L_gU_h + M_gA_h\\) and \\(T_{g,h} = L_hU_g + M_hA_g\\). If \\(m \\gtrsim H(X) + mS_{g,h} + H(V) + \\sqrt{m}T_{g,h}\\), where \\(H(\\cdot, \\cdot)\\) is the metric entropy defined in Definition 2, then with probability at least \\(1 - mP_0 - 2\\exp\\left(-\\frac{\\sqrt{m}S_{g,h}}{A_gA_h} + H(X), \\sqrt{m}S_{g,h} + H(V), \\sqrt{m}T_{g,h}\\right)\\), we have \\(I \\lesssim A_gA_hH(X)\\), where \\(I := \\sup_{x \\in X} \\sup_{v \\in V} \\frac{1}{m} \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]\\) is the supremum of a product process.\n\nRemark 8. We use \\(Ru2\\) as an example to illustrate the advantage of Theorem 2 over Lemma 5. The key step is on bounding the centered process\n\n$$\nRu2,c := \\sup_{x \\in K} \\sup_{v \\in (K-\\epsilon)^*} \\left|\\varepsilon_{i,\\beta}(a_i^Tx)a_i^Tv - E\\left[\\varepsilon_{i,\\beta}(a_i^Tx)a_i^Tv\\right]\\right|.\n$$\nLet \\(g_x(a_i) = \\left|\\varepsilon_{i,\\beta}(a_i^Tx)\\right|\\) and \\(h_v(a_i) = \\left|a_i^Tv\\right|\\), then one can use Theorem 2 or Lemma 5 to bound \\(Ru2,c\\). Note that \\(\\|a_i^Tv\\|_{\\psi_2} = O(1)\\) justifies the choice \\(A_h = O(1)\\), and both \\(H(K, \\eta)\\) and \\(H((K-\\epsilon)^*, \\eta)\\) depend linearly on \\(k\\) but only logarithmically on \\(\\eta\\) (Lemma 6), so Theorem 2 could bound \\(Ru2,c\\) by \\(\\tilde{O}(A_gk/m)\\) that depends on \\(M_g\\) in a logarithmic manner. However, the bound produced by Lemma 5 depends linearly on \\(M_g\\); see term \\(M_gA_h\\omega(K) \\sqrt{m}\\) in (A.1). From (3.6), \\(M_g\\) should be proportional to the Lipschitz constant of \\(\\left|\\varepsilon_{i,\\beta}\\right|\\), which scales as \\(\\frac{1}{\\beta}\\) (Lemma 1). The issue is that in many cases we need to take extremely small \\(\\beta\\) to guarantee that (2.6) holds true (e.g., we take \\(\\beta \\approx k/m\\) in 1-bit GCS). Thus, Lemma 5 produces a worse bound compared to our Theorem 2.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Math Equations and Text", "md": "## Math Equations and Text"}, {"type": "text", "value": "$$\n\\begin{array}{ccccccccccc}\n4 & & & & & & & & & 0.5 \\\\\n3 & & & & & & & & & \\\\\n2 & & & & & & & & & \\\\\n1 & & & & & & & & & 0 \\\\\n0 & & & & & & & & & \\\\\n-1 & & & & & & & & & -0.5 \\\\\n& -2 & -1 & 0 & 1 & 2 & -2 & -1 & 0 & 1 & 2 \\\\\n\\end{array}\n$$\n\nFigure 1: (Left): \\(f_i\\) and its approximation \\(f_{i,0.5}\\); (Right): approximation error \\(\\varepsilon_{i,0.5}\\), \\(\\left|\\varepsilon_{i,0.5}\\right|\\).\n\nIt remains to control \\(Ru1\\) and \\(Ru2\\). By the Lipschitz continuity of \\(\\xi_{i,\\beta}\\) and \\(\\left|\\varepsilon_{i,\\beta}\\right|\\), the factors of \\(Ru1\\) and \\(Ru2\\) admit sub-Gaussian increments, so it is natural to first center them and then invoke the concentration inequality for product process due to Mendelson [36, Theorem 1.13], which we restate in Lemma 5 (Appendix A). However, this does not produce a tight bound and would eventually require \\(\\tilde{O}(k/\\epsilon^4)\\) to achieve a uniform \\(\\ell^2\\)-error of \\(\\epsilon\\), as is the case in [17, Section 4].\n\nIn fact, Lemma 5 is based on Gaussian width and hence blind to the fact that \\(K\\), \\((K-\\epsilon)^*\\) here have low metric entropy (Lemma 6). By characterizing the low intrinsic dimension of index sets via metric entropy, we develop the following concentration inequality that can produce tighter bound for \\(Ru1\\) and \\(Ru2\\). This also allows us to derive uniform error rates sharper than those in [17, Section 4].\n\nTheorem 2. Let \\(g_x = g_x(a)\\) and \\(h_v = h_v(a)\\) be stochastic processes indexed by \\(x \\in X \\subset \\mathbb{R}^{p_1}\\), \\(v \\in V \\subset \\mathbb{R}^{p_2}\\), both defined with respect to a common random variable \\(a\\). Assume that:\n\n$$\n\\begin{align*}\n&\\text{(A1.)} \\quad g_x(a), h_v(a) \\text{ are sub-Gaussian for some } (A_g, A_h) \\text{ and admit sub-Gaussian increments regarding } \\ell^2 \\text{ distance for some } (M_g, M_h): \\\\\n&\\|g_x(a) - g_{x'}(a)\\|_{\\psi_2} \\leq M_g\\|x - x'\\|_2, \\|g_x(a)\\|_{\\psi_2} \\leq A_g, \\forall x, x' \\in X; \\\\\n&\\|h_v(a) - h_{v'}(a)\\|_{\\psi_2} \\leq M_h\\|v - v'\\|_2, \\|h_v(a)\\|_{\\psi_2} \\leq A_h, \\forall v, v' \\in V. \\\\\n&\\text{(A2.)} \\text{ On a single draw of } a, \\text{ for some } (L_g, U_g, L_h, U_h) \\text{ the following events simultaneously hold with probability at least } 1 - P_0: \\\\\n&|g_x(a) - g_{x'}(a)| \\leq L_g\\|x - x'\\|_2, |g_x(a)| \\leq U_g, \\forall x, x' \\in X; \\\\\n&|h_v(a) - h_{v'}(a)| \\leq L_h\\|v - v'\\|_2, |h_v(a)| \\leq U_h, \\forall v, v' \\in V.\n\\end{align*}\n$$\nLet \\(a_1, ..., a_m\\) be i.i.d. copies of \\(a\\), and introduce the shorthand \\(S_{g,h} = L_gU_h + M_gA_h\\) and \\(T_{g,h} = L_hU_g + M_hA_g\\). If \\(m \\gtrsim H(X) + mS_{g,h} + H(V) + \\sqrt{m}T_{g,h}\\), where \\(H(\\cdot, \\cdot)\\) is the metric entropy defined in Definition 2, then with probability at least \\(1 - mP_0 - 2\\exp\\left(-\\frac{\\sqrt{m}S_{g,h}}{A_gA_h} + H(X), \\sqrt{m}S_{g,h} + H(V), \\sqrt{m}T_{g,h}\\right)\\), we have \\(I \\lesssim A_gA_hH(X)\\), where \\(I := \\sup_{x \\in X} \\sup_{v \\in V} \\frac{1}{m} \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]\\) is the supremum of a product process.\n\nRemark 8. We use \\(Ru2\\) as an example to illustrate the advantage of Theorem 2 over Lemma 5. The key step is on bounding the centered process\n\n$$\nRu2,c := \\sup_{x \\in K} \\sup_{v \\in (K-\\epsilon)^*} \\left|\\varepsilon_{i,\\beta}(a_i^Tx)a_i^Tv - E\\left[\\varepsilon_{i,\\beta}(a_i^Tx)a_i^Tv\\right]\\right|.\n$$\nLet \\(g_x(a_i) = \\left|\\varepsilon_{i,\\beta}(a_i^Tx)\\right|\\) and \\(h_v(a_i) = \\left|a_i^Tv\\right|\\), then one can use Theorem 2 or Lemma 5 to bound \\(Ru2,c\\). Note that \\(\\|a_i^Tv\\|_{\\psi_2} = O(1)\\) justifies the choice \\(A_h = O(1)\\), and both \\(H(K, \\eta)\\) and \\(H((K-\\epsilon)^*, \\eta)\\) depend linearly on \\(k\\) but only logarithmically on \\(\\eta\\) (Lemma 6), so Theorem 2 could bound \\(Ru2,c\\) by \\(\\tilde{O}(A_gk/m)\\) that depends on \\(M_g\\) in a logarithmic manner. However, the bound produced by Lemma 5 depends linearly on \\(M_g\\); see term \\(M_gA_h\\omega(K) \\sqrt{m}\\) in (A.1). From (3.6), \\(M_g\\) should be proportional to the Lipschitz constant of \\(\\left|\\varepsilon_{i,\\beta}\\right|\\), which scales as \\(\\frac{1}{\\beta}\\) (Lemma 1). The issue is that in many cases we need to take extremely small \\(\\beta\\) to guarantee that (2.6) holds true (e.g., we take \\(\\beta \\approx k/m\\) in 1-bit GCS). Thus, Lemma 5 produces a worse bound compared to our Theorem 2.", "md": "$$\n\\begin{array}{ccccccccccc}\n4 & & & & & & & & & 0.5 \\\\\n3 & & & & & & & & & \\\\\n2 & & & & & & & & & \\\\\n1 & & & & & & & & & 0 \\\\\n0 & & & & & & & & & \\\\\n-1 & & & & & & & & & -0.5 \\\\\n& -2 & -1 & 0 & 1 & 2 & -2 & -1 & 0 & 1 & 2 \\\\\n\\end{array}\n$$\n\nFigure 1: (Left): \\(f_i\\) and its approximation \\(f_{i,0.5}\\); (Right): approximation error \\(\\varepsilon_{i,0.5}\\), \\(\\left|\\varepsilon_{i,0.5}\\right|\\).\n\nIt remains to control \\(Ru1\\) and \\(Ru2\\). By the Lipschitz continuity of \\(\\xi_{i,\\beta}\\) and \\(\\left|\\varepsilon_{i,\\beta}\\right|\\), the factors of \\(Ru1\\) and \\(Ru2\\) admit sub-Gaussian increments, so it is natural to first center them and then invoke the concentration inequality for product process due to Mendelson [36, Theorem 1.13], which we restate in Lemma 5 (Appendix A). However, this does not produce a tight bound and would eventually require \\(\\tilde{O}(k/\\epsilon^4)\\) to achieve a uniform \\(\\ell^2\\)-error of \\(\\epsilon\\), as is the case in [17, Section 4].\n\nIn fact, Lemma 5 is based on Gaussian width and hence blind to the fact that \\(K\\), \\((K-\\epsilon)^*\\) here have low metric entropy (Lemma 6). By characterizing the low intrinsic dimension of index sets via metric entropy, we develop the following concentration inequality that can produce tighter bound for \\(Ru1\\) and \\(Ru2\\). This also allows us to derive uniform error rates sharper than those in [17, Section 4].\n\nTheorem 2. Let \\(g_x = g_x(a)\\) and \\(h_v = h_v(a)\\) be stochastic processes indexed by \\(x \\in X \\subset \\mathbb{R}^{p_1}\\), \\(v \\in V \\subset \\mathbb{R}^{p_2}\\), both defined with respect to a common random variable \\(a\\). Assume that:\n\n$$\n\\begin{align*}\n&\\text{(A1.)} \\quad g_x(a), h_v(a) \\text{ are sub-Gaussian for some } (A_g, A_h) \\text{ and admit sub-Gaussian increments regarding } \\ell^2 \\text{ distance for some } (M_g, M_h): \\\\\n&\\|g_x(a) - g_{x'}(a)\\|_{\\psi_2} \\leq M_g\\|x - x'\\|_2, \\|g_x(a)\\|_{\\psi_2} \\leq A_g, \\forall x, x' \\in X; \\\\\n&\\|h_v(a) - h_{v'}(a)\\|_{\\psi_2} \\leq M_h\\|v - v'\\|_2, \\|h_v(a)\\|_{\\psi_2} \\leq A_h, \\forall v, v' \\in V. \\\\\n&\\text{(A2.)} \\text{ On a single draw of } a, \\text{ for some } (L_g, U_g, L_h, U_h) \\text{ the following events simultaneously hold with probability at least } 1 - P_0: \\\\\n&|g_x(a) - g_{x'}(a)| \\leq L_g\\|x - x'\\|_2, |g_x(a)| \\leq U_g, \\forall x, x' \\in X; \\\\\n&|h_v(a) - h_{v'}(a)| \\leq L_h\\|v - v'\\|_2, |h_v(a)| \\leq U_h, \\forall v, v' \\in V.\n\\end{align*}\n$$\nLet \\(a_1, ..., a_m\\) be i.i.d. copies of \\(a\\), and introduce the shorthand \\(S_{g,h} = L_gU_h + M_gA_h\\) and \\(T_{g,h} = L_hU_g + M_hA_g\\). If \\(m \\gtrsim H(X) + mS_{g,h} + H(V) + \\sqrt{m}T_{g,h}\\), where \\(H(\\cdot, \\cdot)\\) is the metric entropy defined in Definition 2, then with probability at least \\(1 - mP_0 - 2\\exp\\left(-\\frac{\\sqrt{m}S_{g,h}}{A_gA_h} + H(X), \\sqrt{m}S_{g,h} + H(V), \\sqrt{m}T_{g,h}\\right)\\), we have \\(I \\lesssim A_gA_hH(X)\\), where \\(I := \\sup_{x \\in X} \\sup_{v \\in V} \\frac{1}{m} \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]\\) is the supremum of a product process.\n\nRemark 8. We use \\(Ru2\\) as an example to illustrate the advantage of Theorem 2 over Lemma 5. The key step is on bounding the centered process\n\n$$\nRu2,c := \\sup_{x \\in K} \\sup_{v \\in (K-\\epsilon)^*} \\left|\\varepsilon_{i,\\beta}(a_i^Tx)a_i^Tv - E\\left[\\varepsilon_{i,\\beta}(a_i^Tx)a_i^Tv\\right]\\right|.\n$$\nLet \\(g_x(a_i) = \\left|\\varepsilon_{i,\\beta}(a_i^Tx)\\right|\\) and \\(h_v(a_i) = \\left|a_i^Tv\\right|\\), then one can use Theorem 2 or Lemma 5 to bound \\(Ru2,c\\). Note that \\(\\|a_i^Tv\\|_{\\psi_2} = O(1)\\) justifies the choice \\(A_h = O(1)\\), and both \\(H(K, \\eta)\\) and \\(H((K-\\epsilon)^*, \\eta)\\) depend linearly on \\(k\\) but only logarithmically on \\(\\eta\\) (Lemma 6), so Theorem 2 could bound \\(Ru2,c\\) by \\(\\tilde{O}(A_gk/m)\\) that depends on \\(M_g\\) in a logarithmic manner. However, the bound produced by Lemma 5 depends linearly on \\(M_g\\); see term \\(M_gA_h\\omega(K) \\sqrt{m}\\) in (A.1). From (3.6), \\(M_g\\) should be proportional to the Lipschitz constant of \\(\\left|\\varepsilon_{i,\\beta}\\right|\\), which scales as \\(\\frac{1}{\\beta}\\) (Lemma 1). The issue is that in many cases we need to take extremely small \\(\\beta\\) to guarantee that (2.6) holds true (e.g., we take \\(\\beta \\approx k/m\\) in 1-bit GCS). Thus, Lemma 5 produces a worse bound compared to our Theorem 2."}]}, {"page": 10, "text": "4    Conclusion\nIn this work, we built a unified framework for uniform signal recovery in nonlinear generative\ncompressed sensing. We showed that using generalized Lasso, a sample size of \u02dc    O(k/\u03f52) suffices to\nuniformly recover all x \u2208  G(Bk 2(r)) up to an \u21132-error of \u03f5. We specialized our main theorem to 1-bit\nGCS with/without dithering, single index model, and uniformly quantized GCS, deriving uniform\nguarantees that are new or exhibit some advantages over existing ones. Unlike [33], our proof is\nfree of any non-trivial embedding property. As part of our technical contributions, we constructed\nthe Lipschitz approximation to handle potential discontinuity in the observation model, and also\ndeveloped a concentration inequality to derive tighter bound for the product processes arising in the\nproof, allowing us to obtain a uniform error rate faster than [17]. Possible future directions include\nextending our framework to handle the adversarial noise and representation error.\nAcknowledgment. J. Chen was supported by a Hong Kong PhD Fellowship from the Hong Kong\nResearch Grants Council (RGC). J. Scarlett was supported by the Singapore National Research\nFoundation (NRF) under grant A-0008064-00-00. M. K. Ng was partially supported by the HKRGC\nGRF 17201020, 17300021, CRF C7004-21GF and Joint NSFC-RGC N-HKU76921.\nReferences\n  [1] M. Asim, M. Daniels, O. Leong, A. Ahmed, and P. Hand, \u201cInvertible generative models for\n      inverse problems: Mitigating representation error and dataset bias,\u201d in International Conference\n      on Machine Learning.     PMLR, 2020, pp. 399\u2013409.\n  [2] A. Bora, A. Jalal, E. Price, and A. G. Dimakis, \u201cCompressed sensing using generative models,\u201d\n      in International Conference on Machine Learning.      PMLR, 2017, pp. 537\u2013546.\n  [3] P. T. Boufounos and R. G. Baraniuk, \u201c1-bit compressive sensing,\u201d in Annual Conference on\n      Information Sciences and Systems.     IEEE, 2008, pp. 16\u201321.\n  [4] T. T. Cai and A. Zhang, \u201cSparse representation of a polytope and recovery of sparse signals\n      and low-rank matrices,\u201d IEEE Transactions on Information Theory, vol. 60, no. 1, pp. 122\u2013132,\n      2013.\n  [5] E. J. Candes and T. Tao, \u201cDecoding by linear programming,\u201d IEEE Transactions on Information\n      Theory, vol. 51, no. 12, pp. 4203\u20134215, 2005.\n  [6] E. J. Candes and T. Tao, \u201cNear-optimal signal recovery from random projections: Universal\n      encoding strategies?\u201d IEEE Transactions on Information Theory, vol. 52, no. 12, pp. 5406\u20135425,\n      2006.\n  [7] J. Chen and M. K. Ng, \u201cUniform exact reconstruction of sparse signals and low-rank matrices\n      from phase-only measurements,\u201d IEEE Transactions on Information Theory, vol. 69, no. 10, pp.\n      6739\u20136764, 2023.\n  [8] J. Chen, M. K. Ng, and D. Wang, \u201cQuantizing heavy-tailed data in statistical estimation: (Near)\n      Minimax rates, covariate quantization, and uniform recovery,\u201d arXiv preprint arXiv:2212.14562,\n      2022.\n  [9] J. Chen, C.-L. Wang, M. K. Ng, and D. Wang, \u201cHigh dimensional statistical estimation under\n      uniformly dithered one-bit quantization,\u201d IEEE Transactions on Information Theory, vol. 69,\n      no. 8, pp. 5151\u20135187, 2023.\n[10] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye, \u201cDiffusion posterior sampling\n      for general noisy inverse problems,\u201d in International Conference on Learning Representations,\n      2022.\n[11] G. Daras, J. Dean, A. Jalal, and A. Dimakis, \u201cIntermediate layer optimization for inverse\n      problems using deep generative models,\u201d in International Conference on Machine Learning.\n      PMLR, 2021, pp. 2421\u20132432.\n[12] M. Dhar, A. Grover, and S. Ermon, \u201cModeling sparse deviations for compressed sensing using\n      generative models,\u201d in International Conference on Machine Learning.          PMLR, 2018, pp.\n      1214\u20131223.\n                                                  10", "md": "# Conclusion\n\n#### Conclusion\n\nIn this work, we built a unified framework for uniform signal recovery in nonlinear generative compressed sensing. We showed that using generalized Lasso, a sample size of ~ O(k/\u03f52) suffices to uniformly recover all x \u2208 G(Bk2(r)) up to an \u21132-error of \u03f5. We specialized our main theorem to 1-bit GCS with/without dithering, single index model, and uniformly quantized GCS, deriving uniform guarantees that are new or exhibit some advantages over existing ones. Unlike [33], our proof is free of any non-trivial embedding property. As part of our technical contributions, we constructed the Lipschitz approximation to handle potential discontinuity in the observation model, and also developed a concentration inequality to derive tighter bound for the product processes arising in the proof, allowing us to obtain a uniform error rate faster than [17]. Possible future directions include extending our framework to handle the adversarial noise and representation error.\n\nAcknowledgment. J. Chen was supported by a Hong Kong PhD Fellowship from the Hong Kong Research Grants Council (RGC). J. Scarlett was supported by the Singapore National Research Foundation (NRF) under grant A-0008064-00-00. M. K. Ng was partially supported by the HKRGC GRF 17201020, 17300021, CRF C7004-21GF and Joint NSFC-RGC N-HKU76921.\n\n#### References\n\n1. M. Asim, M. Daniels, O. Leong, A. Ahmed, and P. Hand, \u201cInvertible generative models for inverse problems: Mitigating representation error and dataset bias,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 399\u2013409.\n2. A. Bora, A. Jalal, E. Price, and A. G. Dimakis, \u201cCompressed sensing using generative models,\u201d in International Conference on Machine Learning. PMLR, 2017, pp. 537\u2013546.\n3. P. T. Boufounos and R. G. Baraniuk, \u201c1-bit compressive sensing,\u201d in Annual Conference on Information Sciences and Systems. IEEE, 2008, pp. 16\u201321.\n4. T. T. Cai and A. Zhang, \u201cSparse representation of a polytope and recovery of sparse signals and low-rank matrices,\u201d IEEE Transactions on Information Theory, vol. 60, no. 1, pp. 122\u2013132, 2013.\n5. E. J. Candes and T. Tao, \u201cDecoding by linear programming,\u201d IEEE Transactions on Information Theory, vol. 51, no. 12, pp. 4203\u20134215, 2005.\n6. E. J. Candes and T. Tao, \u201cNear-optimal signal recovery from random projections: Universal encoding strategies?\u201d IEEE Transactions on Information Theory, vol. 52, no. 12, pp. 5406\u20135425, 2006.\n7. J. Chen and M. K. Ng, \u201cUniform exact reconstruction of sparse signals and low-rank matrices from phase-only measurements,\u201d IEEE Transactions on Information Theory, vol. 69, no. 10, pp. 6739\u20136764, 2023.\n8. J. Chen, M. K. Ng, and D. Wang, \u201cQuantizing heavy-tailed data in statistical estimation: (Near) Minimax rates, covariate quantization, and uniform recovery,\u201d arXiv preprint arXiv:2212.14562, 2022.\n9. J. Chen, C.-L. Wang, M. K. Ng, and D. Wang, \u201cHigh dimensional statistical estimation under uniformly dithered one-bit quantization,\u201d IEEE Transactions on Information Theory, vol. 69, no. 8, pp. 5151\u20135187, 2023.\n10. H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye, \u201cDiffusion posterior sampling for general noisy inverse problems,\u201d in International Conference on Learning Representations, 2022.\n11. G. Daras, J. Dean, A. Jalal, and A. Dimakis, \u201cIntermediate layer optimization for inverse problems using deep generative models,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 2421\u20132432.\n12. M. Dhar, A. Grover, and S. Ermon, \u201cModeling sparse deviations for compressed sensing using generative models,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 1214\u20131223.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Conclusion", "md": "# Conclusion"}, {"type": "heading", "lvl": 4, "value": "Conclusion", "md": "#### Conclusion"}, {"type": "text", "value": "In this work, we built a unified framework for uniform signal recovery in nonlinear generative compressed sensing. We showed that using generalized Lasso, a sample size of ~ O(k/\u03f52) suffices to uniformly recover all x \u2208 G(Bk2(r)) up to an \u21132-error of \u03f5. We specialized our main theorem to 1-bit GCS with/without dithering, single index model, and uniformly quantized GCS, deriving uniform guarantees that are new or exhibit some advantages over existing ones. Unlike [33], our proof is free of any non-trivial embedding property. As part of our technical contributions, we constructed the Lipschitz approximation to handle potential discontinuity in the observation model, and also developed a concentration inequality to derive tighter bound for the product processes arising in the proof, allowing us to obtain a uniform error rate faster than [17]. Possible future directions include extending our framework to handle the adversarial noise and representation error.\n\nAcknowledgment. J. Chen was supported by a Hong Kong PhD Fellowship from the Hong Kong Research Grants Council (RGC). J. Scarlett was supported by the Singapore National Research Foundation (NRF) under grant A-0008064-00-00. M. K. Ng was partially supported by the HKRGC GRF 17201020, 17300021, CRF C7004-21GF and Joint NSFC-RGC N-HKU76921.", "md": "In this work, we built a unified framework for uniform signal recovery in nonlinear generative compressed sensing. We showed that using generalized Lasso, a sample size of ~ O(k/\u03f52) suffices to uniformly recover all x \u2208 G(Bk2(r)) up to an \u21132-error of \u03f5. We specialized our main theorem to 1-bit GCS with/without dithering, single index model, and uniformly quantized GCS, deriving uniform guarantees that are new or exhibit some advantages over existing ones. Unlike [33], our proof is free of any non-trivial embedding property. As part of our technical contributions, we constructed the Lipschitz approximation to handle potential discontinuity in the observation model, and also developed a concentration inequality to derive tighter bound for the product processes arising in the proof, allowing us to obtain a uniform error rate faster than [17]. Possible future directions include extending our framework to handle the adversarial noise and representation error.\n\nAcknowledgment. J. Chen was supported by a Hong Kong PhD Fellowship from the Hong Kong Research Grants Council (RGC). J. Scarlett was supported by the Singapore National Research Foundation (NRF) under grant A-0008064-00-00. M. K. Ng was partially supported by the HKRGC GRF 17201020, 17300021, CRF C7004-21GF and Joint NSFC-RGC N-HKU76921."}, {"type": "heading", "lvl": 4, "value": "References", "md": "#### References"}, {"type": "text", "value": "1. M. Asim, M. Daniels, O. Leong, A. Ahmed, and P. Hand, \u201cInvertible generative models for inverse problems: Mitigating representation error and dataset bias,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 399\u2013409.\n2. A. Bora, A. Jalal, E. Price, and A. G. Dimakis, \u201cCompressed sensing using generative models,\u201d in International Conference on Machine Learning. PMLR, 2017, pp. 537\u2013546.\n3. P. T. Boufounos and R. G. Baraniuk, \u201c1-bit compressive sensing,\u201d in Annual Conference on Information Sciences and Systems. IEEE, 2008, pp. 16\u201321.\n4. T. T. Cai and A. Zhang, \u201cSparse representation of a polytope and recovery of sparse signals and low-rank matrices,\u201d IEEE Transactions on Information Theory, vol. 60, no. 1, pp. 122\u2013132, 2013.\n5. E. J. Candes and T. Tao, \u201cDecoding by linear programming,\u201d IEEE Transactions on Information Theory, vol. 51, no. 12, pp. 4203\u20134215, 2005.\n6. E. J. Candes and T. Tao, \u201cNear-optimal signal recovery from random projections: Universal encoding strategies?\u201d IEEE Transactions on Information Theory, vol. 52, no. 12, pp. 5406\u20135425, 2006.\n7. J. Chen and M. K. Ng, \u201cUniform exact reconstruction of sparse signals and low-rank matrices from phase-only measurements,\u201d IEEE Transactions on Information Theory, vol. 69, no. 10, pp. 6739\u20136764, 2023.\n8. J. Chen, M. K. Ng, and D. Wang, \u201cQuantizing heavy-tailed data in statistical estimation: (Near) Minimax rates, covariate quantization, and uniform recovery,\u201d arXiv preprint arXiv:2212.14562, 2022.\n9. J. Chen, C.-L. Wang, M. K. Ng, and D. Wang, \u201cHigh dimensional statistical estimation under uniformly dithered one-bit quantization,\u201d IEEE Transactions on Information Theory, vol. 69, no. 8, pp. 5151\u20135187, 2023.\n10. H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye, \u201cDiffusion posterior sampling for general noisy inverse problems,\u201d in International Conference on Learning Representations, 2022.\n11. G. Daras, J. Dean, A. Jalal, and A. Dimakis, \u201cIntermediate layer optimization for inverse problems using deep generative models,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 2421\u20132432.\n12. M. Dhar, A. Grover, and S. Ermon, \u201cModeling sparse deviations for compressed sensing using generative models,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 1214\u20131223.", "md": "1. M. Asim, M. Daniels, O. Leong, A. Ahmed, and P. Hand, \u201cInvertible generative models for inverse problems: Mitigating representation error and dataset bias,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 399\u2013409.\n2. A. Bora, A. Jalal, E. Price, and A. G. Dimakis, \u201cCompressed sensing using generative models,\u201d in International Conference on Machine Learning. PMLR, 2017, pp. 537\u2013546.\n3. P. T. Boufounos and R. G. Baraniuk, \u201c1-bit compressive sensing,\u201d in Annual Conference on Information Sciences and Systems. IEEE, 2008, pp. 16\u201321.\n4. T. T. Cai and A. Zhang, \u201cSparse representation of a polytope and recovery of sparse signals and low-rank matrices,\u201d IEEE Transactions on Information Theory, vol. 60, no. 1, pp. 122\u2013132, 2013.\n5. E. J. Candes and T. Tao, \u201cDecoding by linear programming,\u201d IEEE Transactions on Information Theory, vol. 51, no. 12, pp. 4203\u20134215, 2005.\n6. E. J. Candes and T. Tao, \u201cNear-optimal signal recovery from random projections: Universal encoding strategies?\u201d IEEE Transactions on Information Theory, vol. 52, no. 12, pp. 5406\u20135425, 2006.\n7. J. Chen and M. K. Ng, \u201cUniform exact reconstruction of sparse signals and low-rank matrices from phase-only measurements,\u201d IEEE Transactions on Information Theory, vol. 69, no. 10, pp. 6739\u20136764, 2023.\n8. J. Chen, M. K. Ng, and D. Wang, \u201cQuantizing heavy-tailed data in statistical estimation: (Near) Minimax rates, covariate quantization, and uniform recovery,\u201d arXiv preprint arXiv:2212.14562, 2022.\n9. J. Chen, C.-L. Wang, M. K. Ng, and D. Wang, \u201cHigh dimensional statistical estimation under uniformly dithered one-bit quantization,\u201d IEEE Transactions on Information Theory, vol. 69, no. 8, pp. 5151\u20135187, 2023.\n10. H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye, \u201cDiffusion posterior sampling for general noisy inverse problems,\u201d in International Conference on Learning Representations, 2022.\n11. G. Daras, J. Dean, A. Jalal, and A. Dimakis, \u201cIntermediate layer optimization for inverse problems using deep generative models,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 2421\u20132432.\n12. M. Dhar, A. Grover, and S. Ermon, \u201cModeling sparse deviations for compressed sensing using generative models,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 1214\u20131223."}]}, {"page": 11, "text": "[13] S. Dirksen, \u201cQuantized compressed sensing: a survey,\u201d in Compressed Sensing and Its Applica-\n      tions: Third International MATHEON Conference 2017.          Springer, 2019, pp. 67\u201395.\n[14] S. Dirksen and S. Mendelson, \u201cNon-Gaussian hyperplane tessellations and robust one-bit\n      compressed sensing,\u201d Journal of the European Mathematical Society, vol. 23, no. 9, pp. 2913\u2013\n      2947, 2021.\n[15] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive Sensing.            Springer,\n      2013.\n[16] M. Genzel, \u201cHigh-dimensional estimation of structured signals from non-linear observations\n      with general convex loss functions,\u201d IEEE Transactions on Information Theory, vol. 63, no. 3,\n      pp. 1601\u20131619, 2016.\n[17] M. Genzel and A. Stollenwerk, \u201cA unified approach to uniform signal recovery from nonlinear\n      observations,\u201d Foundations of Computational Mathematics, pp. 1\u201374, 2022.\n[18] R. M. Gray and T. G. Stockham, \u201cDithered quantizers,\u201d IEEE Transactions on Information\n      Theory, vol. 39, no. 3, pp. 805\u2013812, 1993.\n[19] P. Hand, O. Leong, and V. Voroninski, \u201cPhase retrieval under a generative prior,\u201d Advances in\n      Neural Information Processing Systems, vol. 31, 2018.\n[20] P. Hand and V. Voroninski, \u201cGlobal guarantees for enforcing deep generative priors by empirical\n      risk,\u201d in Conference On Learning Theory.      PMLR, 2018, pp. 970\u2013978.\n[21] L. Jacques and T. Feuillen, \u201cThe importance of phase in complex compressive sensing,\u201d IEEE\n      Transactions on Information Theory, vol. 67, no. 6, pp. 4150\u20134161, 2021.\n[22] L. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, \u201cRobust 1-bit compressive sensing\n      via binary stable embeddings of sparse vectors,\u201d IEEE Transactions on Information Theory,\n      vol. 59, no. 4, pp. 2082\u20132102, 2013.\n[23] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price, \u201cInstance-optimal compressed sensing via\n      posterior sampling,\u201d in International Conference on Machine Learning (ICML), 2021.\n[24] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir, \u201cRobust compressed\n      sensing MRI with deep generative priors,\u201d Advances in Neural Information Processing Systems,\n      vol. 34, pp. 14 938\u201314 954, 2021.\n[25] A. Jalal, L. Liu, A. G. Dimakis, and C. Caramanis, \u201cRobust compressed sensing using generative\n      models,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 713\u2013727, 2020.\n[26] Y. Jiao, D. Li, M. Liu, X. Lu, and Y. Yang, \u201cJust least squares: Binary compressive sampling\n      with low generative intrinsic dimension,\u201d Journal of Scientific Computing, vol. 95, no. 1, p. 28,\n      2023.\n[27] A. Kamath, E. Price, and S. Karmalkar, \u201cOn the power of compressed sensing with generative\n      models,\u201d in International Conference on Machine Learning.        PMLR, 2020, pp. 5101\u20135109.\n[28] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document\n      recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.\n[29] J. Liu and Z. Liu, \u201cNon-iterative recovery from nonlinear observations using generative models,\u201d\n      in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 233\u2013243.\n[30] Z. Liu, S. Ghosh, and J. Scarlett, \u201cTowards sample-optimal compressive phase retrieval with\n      sparse and generative priors,\u201d Advances in Neural Information Processing Systems, vol. 34, pp.\n      17 656\u201317 668, 2021.\n[31] Z. Liu, S. Gomes, A. Tiwari, and J. Scarlett, \u201cSample complexity bounds for 1-bit compressive\n      sensing and binary stable embeddings with generative priors,\u201d in International Conference on\n      Machine Learning.     PMLR, 2020, pp. 6216\u20136225.\n[32] Z. Liu and J. Han, \u201cProjected gradient descent algorithms for solving nonlinear inverse problems\n      with generative priors,\u201d arXiv preprint arXiv:2209.10093, 2022.\n[33] Z. Liu and J. Scarlett, \u201cThe generalized Lasso with nonlinear observations and generative priors,\u201d\n      Advances in Neural Information Processing Systems, vol. 33, pp. 19 125\u201319 136, 2020.\n[34] Z. Liu and J. Scarlett, \u201cInformation-theoretic lower bounds for compressive sensing with\n      generative models,\u201d IEEE Journal on Selected Areas in Information Theory, vol. 1, no. 1, pp.\n      292\u2013303, 2020.\n                                                  11", "md": "- [13] S. Dirksen, \u201cQuantized compressed sensing: a survey,\u201d in Compressed Sensing and Its Applications: Third International MATHEON Conference 2017. Springer, 2019, pp. 67\u201395.\n- [14] S. Dirksen and S. Mendelson, \u201cNon-Gaussian hyperplane tessellations and robust one-bit compressed sensing,\u201d Journal of the European Mathematical Society, vol. 23, no. 9, pp. 2913\u20132947, 2021.\n- [15] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive Sensing. Springer, 2013.\n- [16] M. Genzel, \u201cHigh-dimensional estimation of structured signals from non-linear observations with general convex loss functions,\u201d IEEE Transactions on Information Theory, vol. 63, no. 3, pp. 1601\u20131619, 2016.\n- [17] M. Genzel and A. Stollenwerk, \u201cA unified approach to uniform signal recovery from nonlinear observations,\u201d Foundations of Computational Mathematics, pp. 1\u201374, 2022.\n- [18] R. M. Gray and T. G. Stockham, \u201cDithered quantizers,\u201d IEEE Transactions on Information Theory, vol. 39, no. 3, pp. 805\u2013812, 1993.\n- [19] P. Hand, O. Leong, and V. Voroninski, \u201cPhase retrieval under a generative prior,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018.\n- [20] P. Hand and V. Voroninski, \u201cGlobal guarantees for enforcing deep generative priors by empirical risk,\u201d in Conference On Learning Theory. PMLR, 2018, pp. 970\u2013978.\n- [21] L. Jacques and T. Feuillen, \u201cThe importance of phase in complex compressive sensing,\u201d IEEE Transactions on Information Theory, vol. 67, no. 6, pp. 4150\u20134161, 2021.\n- [22] L. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, \u201cRobust 1-bit compressive sensing via binary stable embeddings of sparse vectors,\u201d IEEE Transactions on Information Theory, vol. 59, no. 4, pp. 2082\u20132102, 2013.\n- [23] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price, \u201cInstance-optimal compressed sensing via posterior sampling,\u201d in International Conference on Machine Learning (ICML), 2021.\n- [24] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir, \u201cRobust compressed sensing MRI with deep generative priors,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 14 938\u201314 954, 2021.\n- [25] A. Jalal, L. Liu, A. G. Dimakis, and C. Caramanis, \u201cRobust compressed sensing using generative models,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 713\u2013727, 2020.\n- [26] Y. Jiao, D. Li, M. Liu, X. Lu, and Y. Yang, \u201cJust least squares: Binary compressive sampling with low generative intrinsic dimension,\u201d Journal of Scientific Computing, vol. 95, no. 1, p. 28, 2023.\n- [27] A. Kamath, E. Price, and S. Karmalkar, \u201cOn the power of compressed sensing with generative models,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 5101\u20135109.\n- [28] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.\n- [29] J. Liu and Z. Liu, \u201cNon-iterative recovery from nonlinear observations using generative models,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 233\u2013243.\n- [30] Z. Liu, S. Ghosh, and J. Scarlett, \u201cTowards sample-optimal compressive phase retrieval with sparse and generative priors,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 17 656\u201317 668, 2021.\n- [31] Z. Liu, S. Gomes, A. Tiwari, and J. Scarlett, \u201cSample complexity bounds for 1-bit compressive sensing and binary stable embeddings with generative priors,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 6216\u20136225.\n- [32] Z. Liu and J. Han, \u201cProjected gradient descent algorithms for solving nonlinear inverse problems with generative priors,\u201d arXiv preprint arXiv:2209.10093, 2022.\n- [33] Z. Liu and J. Scarlett, \u201cThe generalized Lasso with nonlinear observations and generative priors,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 19 125\u201319 136, 2020.\n- [34] Z. Liu and J. Scarlett, \u201cInformation-theoretic lower bounds for compressive sensing with generative models,\u201d IEEE Journal on Selected Areas in Information Theory, vol. 1, no. 1, pp. 292\u2013303, 2020.", "images": [], "items": [{"type": "text", "value": "- [13] S. Dirksen, \u201cQuantized compressed sensing: a survey,\u201d in Compressed Sensing and Its Applications: Third International MATHEON Conference 2017. Springer, 2019, pp. 67\u201395.\n- [14] S. Dirksen and S. Mendelson, \u201cNon-Gaussian hyperplane tessellations and robust one-bit compressed sensing,\u201d Journal of the European Mathematical Society, vol. 23, no. 9, pp. 2913\u20132947, 2021.\n- [15] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive Sensing. Springer, 2013.\n- [16] M. Genzel, \u201cHigh-dimensional estimation of structured signals from non-linear observations with general convex loss functions,\u201d IEEE Transactions on Information Theory, vol. 63, no. 3, pp. 1601\u20131619, 2016.\n- [17] M. Genzel and A. Stollenwerk, \u201cA unified approach to uniform signal recovery from nonlinear observations,\u201d Foundations of Computational Mathematics, pp. 1\u201374, 2022.\n- [18] R. M. Gray and T. G. Stockham, \u201cDithered quantizers,\u201d IEEE Transactions on Information Theory, vol. 39, no. 3, pp. 805\u2013812, 1993.\n- [19] P. Hand, O. Leong, and V. Voroninski, \u201cPhase retrieval under a generative prior,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018.\n- [20] P. Hand and V. Voroninski, \u201cGlobal guarantees for enforcing deep generative priors by empirical risk,\u201d in Conference On Learning Theory. PMLR, 2018, pp. 970\u2013978.\n- [21] L. Jacques and T. Feuillen, \u201cThe importance of phase in complex compressive sensing,\u201d IEEE Transactions on Information Theory, vol. 67, no. 6, pp. 4150\u20134161, 2021.\n- [22] L. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, \u201cRobust 1-bit compressive sensing via binary stable embeddings of sparse vectors,\u201d IEEE Transactions on Information Theory, vol. 59, no. 4, pp. 2082\u20132102, 2013.\n- [23] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price, \u201cInstance-optimal compressed sensing via posterior sampling,\u201d in International Conference on Machine Learning (ICML), 2021.\n- [24] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir, \u201cRobust compressed sensing MRI with deep generative priors,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 14 938\u201314 954, 2021.\n- [25] A. Jalal, L. Liu, A. G. Dimakis, and C. Caramanis, \u201cRobust compressed sensing using generative models,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 713\u2013727, 2020.\n- [26] Y. Jiao, D. Li, M. Liu, X. Lu, and Y. Yang, \u201cJust least squares: Binary compressive sampling with low generative intrinsic dimension,\u201d Journal of Scientific Computing, vol. 95, no. 1, p. 28, 2023.\n- [27] A. Kamath, E. Price, and S. Karmalkar, \u201cOn the power of compressed sensing with generative models,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 5101\u20135109.\n- [28] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.\n- [29] J. Liu and Z. Liu, \u201cNon-iterative recovery from nonlinear observations using generative models,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 233\u2013243.\n- [30] Z. Liu, S. Ghosh, and J. Scarlett, \u201cTowards sample-optimal compressive phase retrieval with sparse and generative priors,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 17 656\u201317 668, 2021.\n- [31] Z. Liu, S. Gomes, A. Tiwari, and J. Scarlett, \u201cSample complexity bounds for 1-bit compressive sensing and binary stable embeddings with generative priors,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 6216\u20136225.\n- [32] Z. Liu and J. Han, \u201cProjected gradient descent algorithms for solving nonlinear inverse problems with generative priors,\u201d arXiv preprint arXiv:2209.10093, 2022.\n- [33] Z. Liu and J. Scarlett, \u201cThe generalized Lasso with nonlinear observations and generative priors,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 19 125\u201319 136, 2020.\n- [34] Z. Liu and J. Scarlett, \u201cInformation-theoretic lower bounds for compressive sensing with generative models,\u201d IEEE Journal on Selected Areas in Information Theory, vol. 1, no. 1, pp. 292\u2013303, 2020.", "md": "- [13] S. Dirksen, \u201cQuantized compressed sensing: a survey,\u201d in Compressed Sensing and Its Applications: Third International MATHEON Conference 2017. Springer, 2019, pp. 67\u201395.\n- [14] S. Dirksen and S. Mendelson, \u201cNon-Gaussian hyperplane tessellations and robust one-bit compressed sensing,\u201d Journal of the European Mathematical Society, vol. 23, no. 9, pp. 2913\u20132947, 2021.\n- [15] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive Sensing. Springer, 2013.\n- [16] M. Genzel, \u201cHigh-dimensional estimation of structured signals from non-linear observations with general convex loss functions,\u201d IEEE Transactions on Information Theory, vol. 63, no. 3, pp. 1601\u20131619, 2016.\n- [17] M. Genzel and A. Stollenwerk, \u201cA unified approach to uniform signal recovery from nonlinear observations,\u201d Foundations of Computational Mathematics, pp. 1\u201374, 2022.\n- [18] R. M. Gray and T. G. Stockham, \u201cDithered quantizers,\u201d IEEE Transactions on Information Theory, vol. 39, no. 3, pp. 805\u2013812, 1993.\n- [19] P. Hand, O. Leong, and V. Voroninski, \u201cPhase retrieval under a generative prior,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018.\n- [20] P. Hand and V. Voroninski, \u201cGlobal guarantees for enforcing deep generative priors by empirical risk,\u201d in Conference On Learning Theory. PMLR, 2018, pp. 970\u2013978.\n- [21] L. Jacques and T. Feuillen, \u201cThe importance of phase in complex compressive sensing,\u201d IEEE Transactions on Information Theory, vol. 67, no. 6, pp. 4150\u20134161, 2021.\n- [22] L. Jacques, J. N. Laska, P. T. Boufounos, and R. G. Baraniuk, \u201cRobust 1-bit compressive sensing via binary stable embeddings of sparse vectors,\u201d IEEE Transactions on Information Theory, vol. 59, no. 4, pp. 2082\u20132102, 2013.\n- [23] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price, \u201cInstance-optimal compressed sensing via posterior sampling,\u201d in International Conference on Machine Learning (ICML), 2021.\n- [24] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir, \u201cRobust compressed sensing MRI with deep generative priors,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 14 938\u201314 954, 2021.\n- [25] A. Jalal, L. Liu, A. G. Dimakis, and C. Caramanis, \u201cRobust compressed sensing using generative models,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 713\u2013727, 2020.\n- [26] Y. Jiao, D. Li, M. Liu, X. Lu, and Y. Yang, \u201cJust least squares: Binary compressive sampling with low generative intrinsic dimension,\u201d Journal of Scientific Computing, vol. 95, no. 1, p. 28, 2023.\n- [27] A. Kamath, E. Price, and S. Karmalkar, \u201cOn the power of compressed sensing with generative models,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 5101\u20135109.\n- [28] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.\n- [29] J. Liu and Z. Liu, \u201cNon-iterative recovery from nonlinear observations using generative models,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 233\u2013243.\n- [30] Z. Liu, S. Ghosh, and J. Scarlett, \u201cTowards sample-optimal compressive phase retrieval with sparse and generative priors,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 17 656\u201317 668, 2021.\n- [31] Z. Liu, S. Gomes, A. Tiwari, and J. Scarlett, \u201cSample complexity bounds for 1-bit compressive sensing and binary stable embeddings with generative priors,\u201d in International Conference on Machine Learning. PMLR, 2020, pp. 6216\u20136225.\n- [32] Z. Liu and J. Han, \u201cProjected gradient descent algorithms for solving nonlinear inverse problems with generative priors,\u201d arXiv preprint arXiv:2209.10093, 2022.\n- [33] Z. Liu and J. Scarlett, \u201cThe generalized Lasso with nonlinear observations and generative priors,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 19 125\u201319 136, 2020.\n- [34] Z. Liu and J. Scarlett, \u201cInformation-theoretic lower bounds for compressive sensing with generative models,\u201d IEEE Journal on Selected Areas in Information Theory, vol. 1, no. 1, pp. 292\u2013303, 2020."}]}, {"page": 12, "text": "[35] Z. Liu, P. Luo, X. Wang, and X. Tang, \u201cDeep learning face attributes in the wild,\u201d in Proceedings\n      of the IEEE International Conference on Computer Vision, 2015, pp. 3730\u20133738.\n[36] S. Mendelson, \u201cUpper bounds on product and multiplier empirical processes,\u201d Stochastic\n      Processes and their Applications, vol. 126, no. 12, pp. 3652\u20133680, 2016.\n[37] S. Mendelson, \u201cOn multiplier processes under weak moment assumptions,\u201d in Geometric\n      Aspects of Functional Analysis: Israel Seminar (GAFA) 2014\u20132016.            Springer, 2017, pp.\n      301\u2013318.\n[38] X. Meng and Y. Kabashima, \u201cQuantized compressed sensing with score-based generative\n      models,\u201d in International Conference on Learning Representations, 2023.\n[39] A. Naderi and Y. Plan, \u201cSparsity-free compressed sensing with applications to generative priors,\u201d\n      IEEE Journal on Selected Areas in Information Theory, 2022.\n[40] G. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and R. Willett, \u201cDeep learning\n      techniques for inverse problems in imaging,\u201d IEEE Journal on Selected Areas in Information\n      Theory, vol. 1, no. 1, pp. 39\u201356, 2020.\n[41] Y. Plan and R. Vershynin, \u201cRobust 1-bit compressed sensing and sparse logistic regression: A\n      convex programming approach,\u201d IEEE Transactions on Information Theory, vol. 59, no. 1, pp.\n      482\u2013494, 2012.\n[42] Y. Plan and R. Vershynin, \u201cOne-bit compressed sensing by linear programming,\u201d Communica-\n      tions on Pure and Applied Mathematics, vol. 66, no. 8, pp. 1275\u20131297, 2013.\n[43] Y. Plan and R. Vershynin, \u201cThe generalized lasso with non-linear observations,\u201d IEEE Transac-\n      tions on information theory, vol. 62, no. 3, pp. 1528\u20131537, 2016.\n[44] Y. Plan, R. Vershynin, and E. Yudovina, \u201cHigh-dimensional estimation with geometric con-\n      straints,\u201d Information and Inference: A Journal of the IMA, vol. 6, no. 1, pp. 1\u201340, 2017.\n[45] S. Qiu, X. Wei, and Z. Yang, \u201cRobust one-bit recovery via ReLU generative networks: Near-\n      optimal statistical rate and global landscape analysis,\u201d in International Conference on Machine\n      Learning.   PMLR, 2020, pp. 7857\u20137866.\n[46] T. M. Quan, T. Nguyen-Duc, and W.-K. Jeong, \u201cCompressed sensing MRI reconstruction using\n      a generative adversarial network with a cyclic loss,\u201d IEEE Transactions on Medical Imaging,\n      vol. 37, no. 6, pp. 1488\u20131497, 2018.\n[47] J. Scarlett, R. Heckel, M. R. Rodrigues, P. Hand, and Y. C. Eldar, \u201cTheoretical perspectives on\n      deep learning methods in inverse problems,\u201d IEEE Journal on Selected Areas in Information\n      Theory, 2022.\n[48] C. Thrampoulidis and A. S. Rawat, \u201cThe generalized lasso for sub-gaussian measurements with\n      dithered quantization,\u201d IEEE Transactions on Information Theory, vol. 66, no. 4, pp. 2487\u20132500,\n      2020.\n[49] Y. Traonmilin and R. Gribonval, \u201cStable recovery of low-dimensional cones in Hilbert spaces:\n      One RIP to rule them all,\u201d Applied and Computational Harmonic Analysis, vol. 45, no. 1, pp.\n      170\u2013205, 2018.\n[50] R. Vershynin, High-dimensional probability: An introduction with applications in data science.\n      Cambridge University Press, 2018, vol. 47.\n[51] J. Whang, E. Lindgren, and A. Dimakis, \u201cComposing normalizing flows for inverse problems,\u201d\n      in International Conference on Machine Learning, 2021, pp. 11 158\u201311 169.\n[52] C. Xu and L. Jacques, \u201cQuantized compressive sensing with RIP matrices: The benefit of\n      dithering,\u201d Information and Inference: A Journal of the IMA, vol. 9, no. 3, pp. 543\u2013586, 2020.\n                                                  12", "md": "Z. Liu, P. Luo, X. Wang, and X. Tang, \u201cDeep learning face attributes in the wild,\u201d in Proceedings\nof the IEEE International Conference on Computer Vision, 2015, pp. 3730\u20133738.\nS. Mendelson, \u201cUpper bounds on product and multiplier empirical processes,\u201d Stochastic\nProcesses and their Applications, vol. 126, no. 12, pp. 3652\u20133680, 2016.\nS. Mendelson, \u201cOn multiplier processes under weak moment assumptions,\u201d in Geometric\nAspects of Functional Analysis: Israel Seminar (GAFA) 2014\u20132016. Springer, 2017, pp.\n301\u2013318.\nX. Meng and Y. Kabashima, \u201cQuantized compressed sensing with score-based generative\nmodels,\u201d in International Conference on Learning Representations, 2023.\nA. Naderi and Y. Plan, \u201cSparsity-free compressed sensing with applications to generative priors,\u201d\nIEEE Journal on Selected Areas in Information Theory, 2022.\nG. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and R. Willett, \u201cDeep learning\ntechniques for inverse problems in imaging,\u201d IEEE Journal on Selected Areas in Information\nTheory, vol. 1, no. 1, pp. 39\u201356, 2020.\nY. Plan and R. Vershynin, \u201cRobust 1-bit compressed sensing and sparse logistic regression: A\nconvex programming approach,\u201d IEEE Transactions on Information Theory, vol. 59, no. 1, pp.\n482\u2013494, 2012.\nY. Plan and R. Vershynin, \u201cOne-bit compressed sensing by linear programming,\u201d Communications\non Pure and Applied Mathematics, vol. 66, no. 8, pp. 1275\u20131297, 2013.\nY. Plan and R. Vershynin, \u201cThe generalized lasso with non-linear observations,\u201d IEEE Transactions\non information theory, vol. 62, no. 3, pp. 1528\u20131537, 2016.\nY. Plan, R. Vershynin, and E. Yudovina, \u201cHigh-dimensional estimation with geometric constraints,\u201d\nInformation and Inference: A Journal of the IMA, vol. 6, no. 1, pp. 1\u201340, 2017.\nS. Qiu, X. Wei, and Z. Yang, \u201cRobust one-bit recovery via ReLU generative networks: Near-\noptimal statistical rate and global landscape analysis,\u201d in International Conference on Machine\nLearning. PMLR, 2020, pp. 7857\u20137866.\nT. M. Quan, T. Nguyen-Duc, and W.-K. Jeong, \u201cCompressed sensing MRI reconstruction using\na generative adversarial network with a cyclic loss,\u201d IEEE Transactions on Medical Imaging,\nvol. 37, no. 6, pp. 1488\u20131497, 2018.\nJ. Scarlett, R. Heckel, M. R. Rodrigues, P. Hand, and Y. C. Eldar, \u201cTheoretical perspectives on\ndeep learning methods in inverse problems,\u201d IEEE Journal on Selected Areas in Information\nTheory, 2022.\nC. Thrampoulidis and A. S. Rawat, \u201cThe generalized lasso for sub-gaussian measurements with\ndithered quantization,\u201d IEEE Transactions on Information Theory, vol. 66, no. 4, pp. 2487\u20132500,\n2020.\nY. Traonmilin and R. Gribonval, \u201cStable recovery of low-dimensional cones in Hilbert spaces:\nOne RIP to rule them all,\u201d Applied and Computational Harmonic Analysis, vol. 45, no. 1, pp.\n170\u2013205, 2018.\nR. Vershynin, High-dimensional probability: An introduction with applications in data science.\nCambridge University Press, 2018, vol. 47.\nJ. Whang, E. Lindgren, and A. Dimakis, \u201cComposing normalizing flows for inverse problems,\u201d\nin International Conference on Machine Learning, 2021, pp. 11 158\u201311 169.\nC. Xu and L. Jacques, \u201cQuantized compressive sensing with RIP matrices: The benefit of\ndithering,\u201d Information and Inference: A Journal of the IMA, vol. 9, no. 3, pp. 543\u2013586, 2020.", "images": [], "items": [{"type": "text", "value": "Z. Liu, P. Luo, X. Wang, and X. Tang, \u201cDeep learning face attributes in the wild,\u201d in Proceedings\nof the IEEE International Conference on Computer Vision, 2015, pp. 3730\u20133738.\nS. Mendelson, \u201cUpper bounds on product and multiplier empirical processes,\u201d Stochastic\nProcesses and their Applications, vol. 126, no. 12, pp. 3652\u20133680, 2016.\nS. Mendelson, \u201cOn multiplier processes under weak moment assumptions,\u201d in Geometric\nAspects of Functional Analysis: Israel Seminar (GAFA) 2014\u20132016. Springer, 2017, pp.\n301\u2013318.\nX. Meng and Y. Kabashima, \u201cQuantized compressed sensing with score-based generative\nmodels,\u201d in International Conference on Learning Representations, 2023.\nA. Naderi and Y. Plan, \u201cSparsity-free compressed sensing with applications to generative priors,\u201d\nIEEE Journal on Selected Areas in Information Theory, 2022.\nG. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and R. Willett, \u201cDeep learning\ntechniques for inverse problems in imaging,\u201d IEEE Journal on Selected Areas in Information\nTheory, vol. 1, no. 1, pp. 39\u201356, 2020.\nY. Plan and R. Vershynin, \u201cRobust 1-bit compressed sensing and sparse logistic regression: A\nconvex programming approach,\u201d IEEE Transactions on Information Theory, vol. 59, no. 1, pp.\n482\u2013494, 2012.\nY. Plan and R. Vershynin, \u201cOne-bit compressed sensing by linear programming,\u201d Communications\non Pure and Applied Mathematics, vol. 66, no. 8, pp. 1275\u20131297, 2013.\nY. Plan and R. Vershynin, \u201cThe generalized lasso with non-linear observations,\u201d IEEE Transactions\non information theory, vol. 62, no. 3, pp. 1528\u20131537, 2016.\nY. Plan, R. Vershynin, and E. Yudovina, \u201cHigh-dimensional estimation with geometric constraints,\u201d\nInformation and Inference: A Journal of the IMA, vol. 6, no. 1, pp. 1\u201340, 2017.\nS. Qiu, X. Wei, and Z. Yang, \u201cRobust one-bit recovery via ReLU generative networks: Near-\noptimal statistical rate and global landscape analysis,\u201d in International Conference on Machine\nLearning. PMLR, 2020, pp. 7857\u20137866.\nT. M. Quan, T. Nguyen-Duc, and W.-K. Jeong, \u201cCompressed sensing MRI reconstruction using\na generative adversarial network with a cyclic loss,\u201d IEEE Transactions on Medical Imaging,\nvol. 37, no. 6, pp. 1488\u20131497, 2018.\nJ. Scarlett, R. Heckel, M. R. Rodrigues, P. Hand, and Y. C. Eldar, \u201cTheoretical perspectives on\ndeep learning methods in inverse problems,\u201d IEEE Journal on Selected Areas in Information\nTheory, 2022.\nC. Thrampoulidis and A. S. Rawat, \u201cThe generalized lasso for sub-gaussian measurements with\ndithered quantization,\u201d IEEE Transactions on Information Theory, vol. 66, no. 4, pp. 2487\u20132500,\n2020.\nY. Traonmilin and R. Gribonval, \u201cStable recovery of low-dimensional cones in Hilbert spaces:\nOne RIP to rule them all,\u201d Applied and Computational Harmonic Analysis, vol. 45, no. 1, pp.\n170\u2013205, 2018.\nR. Vershynin, High-dimensional probability: An introduction with applications in data science.\nCambridge University Press, 2018, vol. 47.\nJ. Whang, E. Lindgren, and A. Dimakis, \u201cComposing normalizing flows for inverse problems,\u201d\nin International Conference on Machine Learning, 2021, pp. 11 158\u201311 169.\nC. Xu and L. Jacques, \u201cQuantized compressive sensing with RIP matrices: The benefit of\ndithering,\u201d Information and Inference: A Journal of the IMA, vol. 9, no. 3, pp. 543\u2013586, 2020.", "md": "Z. Liu, P. Luo, X. Wang, and X. Tang, \u201cDeep learning face attributes in the wild,\u201d in Proceedings\nof the IEEE International Conference on Computer Vision, 2015, pp. 3730\u20133738.\nS. Mendelson, \u201cUpper bounds on product and multiplier empirical processes,\u201d Stochastic\nProcesses and their Applications, vol. 126, no. 12, pp. 3652\u20133680, 2016.\nS. Mendelson, \u201cOn multiplier processes under weak moment assumptions,\u201d in Geometric\nAspects of Functional Analysis: Israel Seminar (GAFA) 2014\u20132016. Springer, 2017, pp.\n301\u2013318.\nX. Meng and Y. Kabashima, \u201cQuantized compressed sensing with score-based generative\nmodels,\u201d in International Conference on Learning Representations, 2023.\nA. Naderi and Y. Plan, \u201cSparsity-free compressed sensing with applications to generative priors,\u201d\nIEEE Journal on Selected Areas in Information Theory, 2022.\nG. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and R. Willett, \u201cDeep learning\ntechniques for inverse problems in imaging,\u201d IEEE Journal on Selected Areas in Information\nTheory, vol. 1, no. 1, pp. 39\u201356, 2020.\nY. Plan and R. Vershynin, \u201cRobust 1-bit compressed sensing and sparse logistic regression: A\nconvex programming approach,\u201d IEEE Transactions on Information Theory, vol. 59, no. 1, pp.\n482\u2013494, 2012.\nY. Plan and R. Vershynin, \u201cOne-bit compressed sensing by linear programming,\u201d Communications\non Pure and Applied Mathematics, vol. 66, no. 8, pp. 1275\u20131297, 2013.\nY. Plan and R. Vershynin, \u201cThe generalized lasso with non-linear observations,\u201d IEEE Transactions\non information theory, vol. 62, no. 3, pp. 1528\u20131537, 2016.\nY. Plan, R. Vershynin, and E. Yudovina, \u201cHigh-dimensional estimation with geometric constraints,\u201d\nInformation and Inference: A Journal of the IMA, vol. 6, no. 1, pp. 1\u201340, 2017.\nS. Qiu, X. Wei, and Z. Yang, \u201cRobust one-bit recovery via ReLU generative networks: Near-\noptimal statistical rate and global landscape analysis,\u201d in International Conference on Machine\nLearning. PMLR, 2020, pp. 7857\u20137866.\nT. M. Quan, T. Nguyen-Duc, and W.-K. Jeong, \u201cCompressed sensing MRI reconstruction using\na generative adversarial network with a cyclic loss,\u201d IEEE Transactions on Medical Imaging,\nvol. 37, no. 6, pp. 1488\u20131497, 2018.\nJ. Scarlett, R. Heckel, M. R. Rodrigues, P. Hand, and Y. C. Eldar, \u201cTheoretical perspectives on\ndeep learning methods in inverse problems,\u201d IEEE Journal on Selected Areas in Information\nTheory, 2022.\nC. Thrampoulidis and A. S. Rawat, \u201cThe generalized lasso for sub-gaussian measurements with\ndithered quantization,\u201d IEEE Transactions on Information Theory, vol. 66, no. 4, pp. 2487\u20132500,\n2020.\nY. Traonmilin and R. Gribonval, \u201cStable recovery of low-dimensional cones in Hilbert spaces:\nOne RIP to rule them all,\u201d Applied and Computational Harmonic Analysis, vol. 45, no. 1, pp.\n170\u2013205, 2018.\nR. Vershynin, High-dimensional probability: An introduction with applications in data science.\nCambridge University Press, 2018, vol. 47.\nJ. Whang, E. Lindgren, and A. Dimakis, \u201cComposing normalizing flows for inverse problems,\u201d\nin International Conference on Machine Learning, 2021, pp. 11 158\u201311 169.\nC. Xu and L. Jacques, \u201cQuantized compressive sensing with RIP matrices: The benefit of\ndithering,\u201d Information and Inference: A Journal of the IMA, vol. 9, no. 3, pp. 543\u2013586, 2020."}]}, {"page": 13, "text": "                                    Supplementary Material\nA Unified Framework for Uniform Signal Recovery in Nonlinear\n                                        Generative Compressed Sensing\n                                                           (NeurIPS 2023)\nA       Technical Lemmas\nLemma 2. (Lemma 2.7.7, [50]). Let X, Y be sub-Gaussian, then XY is sub-exponential with\n\u2225XY \u2225\u03c81 \u2264           \u2225X\u2225\u03c82\u2225Y \u2225\u03c82.\nLemma 3. (Centering, [50, Exercise 2.7.10]). For some absolute constant C, \u2225X \u2212                                                               EX\u2225\u03c81 \u2264\nC\u2225X\u2225\u03c81.\nLemma 4. (Bernstein\u2019s inequality, [50, Theorem 2.8.1]). Let X1, ..., XN be independent, zero-mean,\nsub-exponential random variables. Then for every t \u2265                                    0, for some absolute constant c we have\n               P          N    Xi     \u2265   t     \u2264   2 exp       \u2212c min           N        t2            , max1\u2264i\u2264N \u2225Xi\u2225\u03c81t\n                        i=1                                                          i=1 \u2225Xi\u22252     \u03c81\nLemma 5. ( [36], statement adapted from [16, Theorem 8]). Let gx = gx(a) and hv = hv(a) be\nstochastic processes indexed by x \u2208                       X \u2282     Rp1, v \u2208        V \u2282     Rp2, both defined on some common random\nvariable a. Assume that (A1.) in Theorem 2 holds, and let a1, ..., am be i.i.d. copies of a. Then for\nany u \u2265       1, with probability at least 1 \u2212                  2 exp(\u2212cu2) we have the bound\n                                 sup      m1    m      gx(ai)hv(ai) \u2212             E[gx(ai)hv(ai)]\n                                x\u2208X            i=1\n                                 v\u2208V \u2264   C   (Mg \u00b7 \u03c9(X           ) + u \u00b7 Ag) \u00b7 (Mh \u00b7 \u03c9(V) + u \u00b7 Ah)                                                    (A.1)\n                                           + Ag \u00b7 Mh \u00b7 \u03c9(V) + Ah \u00b7 Mg \u00b7 \u03c9(X       \u221am m               ) + u \u00b7 AgAh             ,\nwhere \u03c9(\u00b7) is the Gaussian width defined as \u03c9(X                                ) = E supx\u2208X g\u22a4x where g \u223c                         N   (0, Ip1).\nThe proofs of the remaining lemmas will be provided in Appendix D. (Some simple facts such as\nLemma 8 were already used in prior works; while we provide the proofs for completeness.)\nLemma 6. (Metric entropy of some constraint sets). Assume K = G(Bk                                                  2(r)) for some L-Lipschitz              c,\ngenerative model G. Let K\u2212                     = K \u2212        K, for some T > 0, \u03f5 \u2208                (0, 1) let K\u2212     \u03f5 := (T       K\u2212) \u2229        Bn 2 (2\u03f5)\nand further define (K\u2212            \u03f5 )\u2217   = {       z                 \u03f5 }. Then for any \u03b7 \u2208              (0, Lr), we have\n                                                \u2225z\u22252 : z \u2208        K\u2212\n                            H (K, \u03b7) \u2264           k log 3Lr  \u03b7 , H (K\u2212, \u03b7) \u2264                2k log 6Lr    \u03b7 ,\n                            H (K\u2212     \u03f5 , \u03b7) \u2264     2k log 12T      \u03b7  Lr   , H      (K\u2212  \u03f5 )\u2217, \u03b7      \u2264   2k log 12T     \u03f5\u03b7 Lr   ,\nwhere H (\u00b7, \u00b7) is the metric entropy defined in Definition 2.\nLemma 7. (Bound the \u21132-norm of Gaussian vector). If a \u223c                                        N   (0, In), then P           |\u2225a\u22252 \u2212       \u221an| \u2265       t    \u2264\n2 exp(\u2212Ct2). In particular, setting t \u224d                         \u221an yields P(\u2225a\u22252 \u2265                 \u221an) \u2264        2 exp(\u2212\u2126(n)).\nIn the following, Lemmas 8-11 indicate suitable choices of T in the concrete models we consider.\nThese choices can make \u03c1(x) in (2.3) sufficiently small or even zero.\nLemma 8. (Choice of T in 1-bit GCS).2If a \u223c                                      N   (0, In), then for any x \u2208                   Sn\u22121 it holds that\nE[sign(a\u22a4x)a] =                    \u03c0  x.\nLemma 9. (Choice of T in 1-bit GCS with dithering). If a \u223c                                               N  (0, In) and \u03c4 \u223c             U [\u2212\u03bb, \u03bb] are\nindependent, and \u03bb = CR\u221alog m with sufficiently large C, then for any x \u2208                                                      Bn 2 (R) it holds that\n\u2225E[sign(a\u22a4x + \u03c4)a] \u2212                    x                m\u22129 .\n                                        \u03bb  \u22252 = O\n                                                                             13", "md": "## Supplementary Material\n\n### A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing (NeurIPS 2023)\n\n### Technical Lemmas\n\nLemma 2. (Lemma 2.7.7, [50]). Let X, Y be sub-Gaussian, then XY is sub-exponential with\n\n$$\n\\|XY \\|_{\\psi_1} \\leq \\|X\\|_{\\psi_2} \\|Y \\|_{\\psi_2}.\n$$\n\nLemma 3. (Centering, [50, Exercise 2.7.10]). For some absolute constant C, $\\|X - \\mathbb{E}[X]\\|_{\\psi_1} \\leq C\\|X\\|_{\\psi_1}$.\n\nLemma 4. (Bernstein\u2019s inequality, [50, Theorem 2.8.1]). Let $X_1, ..., X_N$ be independent, zero-mean, sub-exponential random variables. Then for every $t \\geq 0$, for some absolute constant c we have\n\n$$\nP\\left(\\sum_{i=1}^{N} X_i \\geq t\\right) \\leq 2 \\exp\\left(-c \\min_{1\\leq i\\leq N} \\frac{t^2}{\\|X_i\\|_{\\psi_1}^2}\\right).\n$$\n\nLemma 5. ([36], statement adapted from [16, Theorem 8]). Let $g_x = g_x(a)$ and $h_v = h_v(a)$ be stochastic processes indexed by $x \\in X \\subset \\mathbb{R}^{p_1}$, $v \\in V \\subset \\mathbb{R}^{p_2}$, both defined on some common random variable $a$. Assume that (A1.) in Theorem 2 holds, and let $a_1, ..., a_m$ be i.i.d. copies of $a$. Then for any $u \\geq 1$, with probability at least $1 - 2 \\exp(-cu^2)$ we have the bound\n\n$$\n\\begin{aligned}\n&\\sup_{x\\in X}\\sup_{v\\in V} \\left| \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - \\mathbb{E}[g_x(a_i)h_v(a_i)] \\right| \\\\\n&\\leq C(M_g \\cdot \\omega(X) + u \\cdot A_g) \\cdot (M_h \\cdot \\omega(V) + u \\cdot A_h) \\\\\n&\\quad + A_g \\cdot M_h \\cdot \\omega(V) + A_h \\cdot M_g \\cdot \\omega(X) + u \\cdot A_g A_h,\n\\end{aligned}\n$$\nwhere $\\omega(\\cdot)$ is the Gaussian width defined as $\\omega(X) = \\mathbb{E} \\sup_{x\\in X} g^{\\top}x$ where $g \\sim \\mathcal{N}(0, I_{p_1})$.\n\nThe proofs of the remaining lemmas will be provided in Appendix D. (Some simple facts such as Lemma 8 were already used in prior works; while we provide the proofs for completeness.)\n\nLemma 6. (Metric entropy of some constraint sets). Assume $K = G(B_k^2(r))$ for some L-Lipschitz generative model G. Let $K^- = K - K$, for some $T > 0$, $\\epsilon \\in (0, 1)$ let $K^-_{\\epsilon} := (T K^-) \\cap B_n^2(2\\epsilon)$ and further define $(K^-_{\\epsilon})^* = \\{ z : z \\in K^-_{\\epsilon} \\}$. Then for any $\\eta \\in (0, Lr)$, we have\n\n$$\n\\begin{aligned}\nH(K, \\eta) &\\leq k \\log 3Lr \\eta, \\\\\nH(K^-, \\eta) &\\leq 2k \\log 6Lr \\eta, \\\\\nH(K^-_{\\epsilon}, \\eta) &\\leq 2k \\log 12T \\eta Lr, \\\\\nH((K^-_{\\epsilon})^*, \\eta) &\\leq 2k \\log 12T \\epsilon \\eta Lr,\n\\end{aligned}\n$$\nwhere $H(\\cdot, \\cdot)$ is the metric entropy defined in Definition 2.\n\nLemma 7. (Bound the $\\ell_2$-norm of Gaussian vector). If $a \\sim \\mathcal{N}(0, I_n)$, then $P\\left(|\\|a\\|_2 - \\sqrt{n}| \\geq t\\right) \\leq 2 \\exp(-Ct^2)$. In particular, setting $t \\approx \\sqrt{n}$ yields $P(\\|a\\|_2 \\geq \\sqrt{n}) \\leq 2 \\exp(-\\Omega(n))$.\n\nIn the following, Lemmas 8-11 indicate suitable choices of $T$ in the concrete models we consider. These choices can make $\\rho(x)$ in (2.3) sufficiently small or even zero.\n\nLemma 8. (Choice of $T$ in 1-bit GCS). If $a \\sim \\mathcal{N}(0, I_n)$, then for any $x \\in S^{n-1}$ it holds that $\\mathbb{E}[\\text{sign}(a^{\\top}x)a] = \\frac{\\pi}{\\sqrt{n}}x$.\n\nLemma 9. (Choice of $T$ in 1-bit GCS with dithering). If $a \\sim \\mathcal{N}(0, I_n)$ and $\\tau \\sim U[-\\lambda, \\lambda]$ are independent, and $\\lambda = CR\\sqrt{\\log m}$ with sufficiently large $C$, then for any $x \\in B_n^2(R)$ it holds that $\\| \\mathbb{E}[\\text{sign}(a^{\\top}x + \\tau)a] - \\frac{x}{\\lambda} \\|_2 = O(m^{-9})$.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Supplementary Material", "md": "## Supplementary Material"}, {"type": "heading", "lvl": 3, "value": "A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing (NeurIPS 2023)", "md": "### A Unified Framework for Uniform Signal Recovery in Nonlinear Generative Compressed Sensing (NeurIPS 2023)"}, {"type": "heading", "lvl": 3, "value": "Technical Lemmas", "md": "### Technical Lemmas"}, {"type": "text", "value": "Lemma 2. (Lemma 2.7.7, [50]). Let X, Y be sub-Gaussian, then XY is sub-exponential with\n\n$$\n\\|XY \\|_{\\psi_1} \\leq \\|X\\|_{\\psi_2} \\|Y \\|_{\\psi_2}.\n$$\n\nLemma 3. (Centering, [50, Exercise 2.7.10]). For some absolute constant C, $\\|X - \\mathbb{E}[X]\\|_{\\psi_1} \\leq C\\|X\\|_{\\psi_1}$.\n\nLemma 4. (Bernstein\u2019s inequality, [50, Theorem 2.8.1]). Let $X_1, ..., X_N$ be independent, zero-mean, sub-exponential random variables. Then for every $t \\geq 0$, for some absolute constant c we have\n\n$$\nP\\left(\\sum_{i=1}^{N} X_i \\geq t\\right) \\leq 2 \\exp\\left(-c \\min_{1\\leq i\\leq N} \\frac{t^2}{\\|X_i\\|_{\\psi_1}^2}\\right).\n$$\n\nLemma 5. ([36], statement adapted from [16, Theorem 8]). Let $g_x = g_x(a)$ and $h_v = h_v(a)$ be stochastic processes indexed by $x \\in X \\subset \\mathbb{R}^{p_1}$, $v \\in V \\subset \\mathbb{R}^{p_2}$, both defined on some common random variable $a$. Assume that (A1.) in Theorem 2 holds, and let $a_1, ..., a_m$ be i.i.d. copies of $a$. Then for any $u \\geq 1$, with probability at least $1 - 2 \\exp(-cu^2)$ we have the bound\n\n$$\n\\begin{aligned}\n&\\sup_{x\\in X}\\sup_{v\\in V} \\left| \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - \\mathbb{E}[g_x(a_i)h_v(a_i)] \\right| \\\\\n&\\leq C(M_g \\cdot \\omega(X) + u \\cdot A_g) \\cdot (M_h \\cdot \\omega(V) + u \\cdot A_h) \\\\\n&\\quad + A_g \\cdot M_h \\cdot \\omega(V) + A_h \\cdot M_g \\cdot \\omega(X) + u \\cdot A_g A_h,\n\\end{aligned}\n$$\nwhere $\\omega(\\cdot)$ is the Gaussian width defined as $\\omega(X) = \\mathbb{E} \\sup_{x\\in X} g^{\\top}x$ where $g \\sim \\mathcal{N}(0, I_{p_1})$.\n\nThe proofs of the remaining lemmas will be provided in Appendix D. (Some simple facts such as Lemma 8 were already used in prior works; while we provide the proofs for completeness.)\n\nLemma 6. (Metric entropy of some constraint sets). Assume $K = G(B_k^2(r))$ for some L-Lipschitz generative model G. Let $K^- = K - K$, for some $T > 0$, $\\epsilon \\in (0, 1)$ let $K^-_{\\epsilon} := (T K^-) \\cap B_n^2(2\\epsilon)$ and further define $(K^-_{\\epsilon})^* = \\{ z : z \\in K^-_{\\epsilon} \\}$. Then for any $\\eta \\in (0, Lr)$, we have\n\n$$\n\\begin{aligned}\nH(K, \\eta) &\\leq k \\log 3Lr \\eta, \\\\\nH(K^-, \\eta) &\\leq 2k \\log 6Lr \\eta, \\\\\nH(K^-_{\\epsilon}, \\eta) &\\leq 2k \\log 12T \\eta Lr, \\\\\nH((K^-_{\\epsilon})^*, \\eta) &\\leq 2k \\log 12T \\epsilon \\eta Lr,\n\\end{aligned}\n$$\nwhere $H(\\cdot, \\cdot)$ is the metric entropy defined in Definition 2.\n\nLemma 7. (Bound the $\\ell_2$-norm of Gaussian vector). If $a \\sim \\mathcal{N}(0, I_n)$, then $P\\left(|\\|a\\|_2 - \\sqrt{n}| \\geq t\\right) \\leq 2 \\exp(-Ct^2)$. In particular, setting $t \\approx \\sqrt{n}$ yields $P(\\|a\\|_2 \\geq \\sqrt{n}) \\leq 2 \\exp(-\\Omega(n))$.\n\nIn the following, Lemmas 8-11 indicate suitable choices of $T$ in the concrete models we consider. These choices can make $\\rho(x)$ in (2.3) sufficiently small or even zero.\n\nLemma 8. (Choice of $T$ in 1-bit GCS). If $a \\sim \\mathcal{N}(0, I_n)$, then for any $x \\in S^{n-1}$ it holds that $\\mathbb{E}[\\text{sign}(a^{\\top}x)a] = \\frac{\\pi}{\\sqrt{n}}x$.\n\nLemma 9. (Choice of $T$ in 1-bit GCS with dithering). If $a \\sim \\mathcal{N}(0, I_n)$ and $\\tau \\sim U[-\\lambda, \\lambda]$ are independent, and $\\lambda = CR\\sqrt{\\log m}$ with sufficiently large $C$, then for any $x \\in B_n^2(R)$ it holds that $\\| \\mathbb{E}[\\text{sign}(a^{\\top}x + \\tau)a] - \\frac{x}{\\lambda} \\|_2 = O(m^{-9})$.", "md": "Lemma 2. (Lemma 2.7.7, [50]). Let X, Y be sub-Gaussian, then XY is sub-exponential with\n\n$$\n\\|XY \\|_{\\psi_1} \\leq \\|X\\|_{\\psi_2} \\|Y \\|_{\\psi_2}.\n$$\n\nLemma 3. (Centering, [50, Exercise 2.7.10]). For some absolute constant C, $\\|X - \\mathbb{E}[X]\\|_{\\psi_1} \\leq C\\|X\\|_{\\psi_1}$.\n\nLemma 4. (Bernstein\u2019s inequality, [50, Theorem 2.8.1]). Let $X_1, ..., X_N$ be independent, zero-mean, sub-exponential random variables. Then for every $t \\geq 0$, for some absolute constant c we have\n\n$$\nP\\left(\\sum_{i=1}^{N} X_i \\geq t\\right) \\leq 2 \\exp\\left(-c \\min_{1\\leq i\\leq N} \\frac{t^2}{\\|X_i\\|_{\\psi_1}^2}\\right).\n$$\n\nLemma 5. ([36], statement adapted from [16, Theorem 8]). Let $g_x = g_x(a)$ and $h_v = h_v(a)$ be stochastic processes indexed by $x \\in X \\subset \\mathbb{R}^{p_1}$, $v \\in V \\subset \\mathbb{R}^{p_2}$, both defined on some common random variable $a$. Assume that (A1.) in Theorem 2 holds, and let $a_1, ..., a_m$ be i.i.d. copies of $a$. Then for any $u \\geq 1$, with probability at least $1 - 2 \\exp(-cu^2)$ we have the bound\n\n$$\n\\begin{aligned}\n&\\sup_{x\\in X}\\sup_{v\\in V} \\left| \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - \\mathbb{E}[g_x(a_i)h_v(a_i)] \\right| \\\\\n&\\leq C(M_g \\cdot \\omega(X) + u \\cdot A_g) \\cdot (M_h \\cdot \\omega(V) + u \\cdot A_h) \\\\\n&\\quad + A_g \\cdot M_h \\cdot \\omega(V) + A_h \\cdot M_g \\cdot \\omega(X) + u \\cdot A_g A_h,\n\\end{aligned}\n$$\nwhere $\\omega(\\cdot)$ is the Gaussian width defined as $\\omega(X) = \\mathbb{E} \\sup_{x\\in X} g^{\\top}x$ where $g \\sim \\mathcal{N}(0, I_{p_1})$.\n\nThe proofs of the remaining lemmas will be provided in Appendix D. (Some simple facts such as Lemma 8 were already used in prior works; while we provide the proofs for completeness.)\n\nLemma 6. (Metric entropy of some constraint sets). Assume $K = G(B_k^2(r))$ for some L-Lipschitz generative model G. Let $K^- = K - K$, for some $T > 0$, $\\epsilon \\in (0, 1)$ let $K^-_{\\epsilon} := (T K^-) \\cap B_n^2(2\\epsilon)$ and further define $(K^-_{\\epsilon})^* = \\{ z : z \\in K^-_{\\epsilon} \\}$. Then for any $\\eta \\in (0, Lr)$, we have\n\n$$\n\\begin{aligned}\nH(K, \\eta) &\\leq k \\log 3Lr \\eta, \\\\\nH(K^-, \\eta) &\\leq 2k \\log 6Lr \\eta, \\\\\nH(K^-_{\\epsilon}, \\eta) &\\leq 2k \\log 12T \\eta Lr, \\\\\nH((K^-_{\\epsilon})^*, \\eta) &\\leq 2k \\log 12T \\epsilon \\eta Lr,\n\\end{aligned}\n$$\nwhere $H(\\cdot, \\cdot)$ is the metric entropy defined in Definition 2.\n\nLemma 7. (Bound the $\\ell_2$-norm of Gaussian vector). If $a \\sim \\mathcal{N}(0, I_n)$, then $P\\left(|\\|a\\|_2 - \\sqrt{n}| \\geq t\\right) \\leq 2 \\exp(-Ct^2)$. In particular, setting $t \\approx \\sqrt{n}$ yields $P(\\|a\\|_2 \\geq \\sqrt{n}) \\leq 2 \\exp(-\\Omega(n))$.\n\nIn the following, Lemmas 8-11 indicate suitable choices of $T$ in the concrete models we consider. These choices can make $\\rho(x)$ in (2.3) sufficiently small or even zero.\n\nLemma 8. (Choice of $T$ in 1-bit GCS). If $a \\sim \\mathcal{N}(0, I_n)$, then for any $x \\in S^{n-1}$ it holds that $\\mathbb{E}[\\text{sign}(a^{\\top}x)a] = \\frac{\\pi}{\\sqrt{n}}x$.\n\nLemma 9. (Choice of $T$ in 1-bit GCS with dithering). If $a \\sim \\mathcal{N}(0, I_n)$ and $\\tau \\sim U[-\\lambda, \\lambda]$ are independent, and $\\lambda = CR\\sqrt{\\log m}$ with sufficiently large $C$, then for any $x \\in B_n^2(R)$ it holds that $\\| \\mathbb{E}[\\text{sign}(a^{\\top}x + \\tau)a] - \\frac{x}{\\lambda} \\|_2 = O(m^{-9})$."}]}, {"page": 14, "text": " Lemma 10. (Choice of T in SIM). If a \u223c                               N   (0, In), for some function f and any x \u2208                           Sn\u22121 it holds\n that E[f(a\u22a4x)a] = \u00b5x for \u00b5 = E[f(g)g] with g \u223c                                        N   (0, 1).\n Lemma 11. (Choice of T in uniformly quantized GCS with dithering). Given any \u03b4 > 0, let\n \u03c4 \u223c     U [\u2212     \u03b4                                   \u230a  \u00b7            . Then, for any a \u2208               R, it holds that E[Q\u03b4(a + \u03c4)] = a.\n                  2, \u03b4 2] and Q\u03b4(\u00b7) = \u03b4                 \u03b4 \u230b  + 1  2\n In particular, let a \u2208                Rn be a random vector satisfying E(aa\u22a4) = In, and \u03c4 \u223c                                                 U [\u2212     \u03b4\n                                                                                                                                                      2, \u03b42] be\n independent of a, then we have E[Q\u03b4(a\u22a4x + \u03c4)a] = x.\n Lemma 12 facilitates our analysis of the uniform quantizer.\n Lemma 12. Let fi(\u00b7) = \u03b4                     \u230a\u00b7+\u03c4i\u03b4 \u230b    + 1       for \u03c4i \u223c      U [\u2212     \u03b4\n                                                              2                           2, \u03b42], and fi,\u03b2(\u00b7) be defined in (3.4) for some\n 0 < \u03b2 < \u03b4        2. Moreover, let \u03bei,\u03b2(a) = fi,\u03b2(a) \u2212                           a, \u03b5i,\u03b2(a) = fi,\u03b2(a) \u2212                fi(a), then for any a \u2208                  R,\n |\u03bei,\u03b2(a)| \u2264        2\u03b4, |\u03b5i,\u03b2(a)| \u2264          \u03b4 holds deterministically.\n More generally, the approximation error |\u03b5i,\u03b2(a)| can always be bounded as follows.\n Lemma 13. Suppose that fi satisfies Assumption 2, and for any \u03b2 \u2208              3L0\u03b2                              [0, \u03b20 2 ] we construct fi,\u03b2 as in\n(3.4). Then, for any a \u2208                 R, we have |\u03b5i,\u03b2(a)| \u2264                     2     + B0       1(a \u2208      Dfi + [\u2212\u03b2        2 , \u03b2\n                                                                                                                                     2 ]).\n B       More Details of the Proof Sketch\n B.1       Set-Restricted Eigenvalue Condition\n Definition 4. Let S \u2282                    Rn. For parameters \u03b3, \u03b4 > 0, a matrix A \u2208                                        Rm\u00d7n is said to satisfy\n S-REC(S, \u03b3, \u03b4) if the following holds:\n                                      \u2225A(x11\u2212          x2)\u22252 \u2265        \u03b3\u2225x1 \u2212        x2\u22252 \u2212        \u03b4, \u2200    x1, x2 \u2208       S.\n It was proved in [2] that               \u221a  mA satisfies the S-REC with high probability if the entries of A are i.i.d.\n standard Gaussian.\n Lemma 14. (Lemma 4.1 in [2]). Let G : Bk                              2(r) \u2192        Rn be L-Lipschitz for some r, L > 0, and define              k\n K = G(Bk        2(r)). For any \u03b1 \u2208             (0, 1), if A \u2208         Rm\u00d7n has i.i.d. N             (0, 1) entries, and m = \u2126                   \u03b12 log Lr    \u03b4    ,\n then    \u221a 1mA satisfies S-REC(K,1 \u2212                      \u03b1,\u03b4) with probability at least 1 \u2212                     exp(\u2212\u2126(\u03b12m)).\n B.2       Lipschitz Approximation\n The approximation error \u03b5i,\u03b2(\u00b7) can be expanded as:\n                   \uf8f1\n                   \uf8f4                             0                           ,                      if x / \u2208   Dfi + [\u2212\u03b2\n                   \uf8f4                                                                                                           2 , \u03b22 ]\n  \u03b5i,\u03b2(x) =        \uf8f2   f a                                i (x0)\u2212fi(x0\u2212\u03b22 )](x0\u2212x)             ,    if x \u2208     [x0 \u2212      \u03b2                                      .\n                   \uf8f4     i (x0) \u2212       fi(x) \u2212       2[f a              \u03b2                                                2 , x0], (\u2203x0 \u2208           Dfi)\n                   \uf8f4\n                   \uf8f3   f a                                         2 )\u2212f a i (x0)](x\u2212x0)       ,    if x \u2208     [x0, x0 + \u03b2\n                         i (x0) \u2212       fi(x) + 2[fi(x0+ \u03b2               \u03b2                                                      2 ], (\u2203x0 \u2208        Dfi)\n Although |\u03b5i,\u03b2| is Lipschitz continuous, \u03b5i,\u03b2 is not. In particular, given x0 \u2208                                              Dfi we note that\n                      \u03b5\u2212                            \u03b5i,\u03b2(x) = f a                                           f +                             ,\n                                                                       i (x0) \u2212      f \u2212                      i (x0) \u2212        f \u2212\n                      \u03b5+i,\u03b2(x0) = lim    x\u2192x\u2212   0  \u03b5i,\u03b2(x) = f a                       i (x0) = 1       2   f \u2212                 i (x0)      .\n                                                                       i (x0) \u2212      f +                      i (x0) \u2212        f +\n                        i,\u03b2(x0) = lim    x\u2192x+   0                                      i (x0) = 1       2                       i (x0)\n Thus, it is crucial to include the absolute value for rendering the continuity.\n C       Proofs of Main Results\n C.1       Proof of Theorem 1\n Proof. Up to rescaling, we only need to prove that \u2225\u02c6                              x  \u2212   T  x\u2217\u22252 \u2264        3\u03f5 holds uniformly for all x\u2217                   \u2208   K.\n We can assume \u2225\u02c6            x \u2212    T  x\u2217\u22252 \u2265        2\u03f5; otherwise, the desired bound is immediate.\n                                                                                14", "md": "# Math Equations and Lemmas\n\n## Lemma 10\n\nIf \\(a \\sim \\mathcal{N}(0, I_n)\\), for some function \\(f\\) and any \\(x \\in S^{n-1}\\) it holds that \\(E[f(a^Tx)a] = \\mu x\\) for \\(\\mu = E[f(g)g]\\) with \\(g \\sim \\mathcal{N}(0, 1)\\).\n\n## Lemma 11\n\nGiven any \\(\\delta > 0\\), let \\(\\tau \\sim U[-\\delta/2, \\delta/2]\\). Then, for any \\(a \\in \\mathbb{R}\\), it holds that \\(E[Q_\\delta(a + \\tau)] = a\\), where \\(Q_\\delta(\\cdot) = \\delta \\lfloor \\frac{\\cdot}{\\delta} \\rfloor + \\frac{\\delta}{2}\\).\n\n## Lemma 12\n\nLet \\(f_i(\\cdot) = \\delta \\lfloor \\cdot + \\tau_i \\delta \\rfloor + 1\\) for \\(\\tau_i \\sim U[-\\delta/2, \\delta/2]\\), and \\(f_{i,\\beta}(\\cdot)\\) be defined as in (3.4) for some \\(0 < \\beta < \\frac{\\delta}{2}\\). Moreover, let \\(\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - a\\), \\(\\epsilon_{i,\\beta}(a) = f_{i,\\beta}(a) - f_i(a)\\), then for any \\(a \\in \\mathbb{R}\\), \\(|\\xi_{i,\\beta}(a)| \\leq 2\\delta\\), \\(|\\epsilon_{i,\\beta}(a)| \\leq \\delta\\) holds deterministically.\n\n## Lemma 13\n\nSuppose that \\(f_i\\) satisfies Assumption 2, and for any \\(\\beta \\in [0, \\frac{3\\delta}{2}]\\) we construct \\(f_{i,\\beta}\\) as in (3.4). Then, for any \\(a \\in \\mathbb{R}\\), we have \\(|\\epsilon_{i,\\beta}(a)| \\leq 2 + B_0 \\cdot 1(a \\in Df_i + [-\\frac{\\beta}{2}, \\frac{\\beta}{2}])\\).\n\n## Lemma 14\n\nLet \\(G : B_k^2(r) \\rightarrow \\mathbb{R}^n\\) be \\(L\\)-Lipschitz for some \\(r, L > 0\\), and define \\(K = G(B_k^2(r))\\). For any \\(\\alpha \\in (0, 1)\\), if \\(A \\in \\mathbb{R}^{m \\times n}\\) has i.i.d. \\(\\mathcal{N}(0, 1)\\) entries, and \\(m = \\Omega(\\alpha^2 \\log Lr \\delta)\\), then \\(\\sqrt{\\frac{1}{m}A}\\) satisfies S-REC(K, \\(1 - \\alpha, \\delta\\)) with probability at least \\(1 - \\exp(-\\Omega(\\alpha^2m))\\).\n\n## Approximation Error\n\nThe approximation error \\(\\epsilon_{i,\\beta}(\\cdot)\\) can be expanded as:\n\n$$\n\\epsilon_{i,\\beta}(x) = \\begin{cases}\n0, & \\text{if } x \\notin Df_i + [-\\beta/2, \\beta/2] \\\\\nf_{a_i}(x_0) - f_i(x_0 - \\beta/2))(x_0 - x), & \\text{if } x \\in [x_0 - \\beta/2, x_0] \\\\\nf_{a_i}(x_0) - f_i(x_0 + \\beta/2))(x - x_0), & \\text{if } x \\in [x_0, x_0 + \\beta/2]\n\\end{cases}\n$$\nAlthough \\(|\\epsilon_{i,\\beta}|\\) is Lipschitz continuous, \\(\\epsilon_{i,\\beta}\\) is not. In particular, given \\(x_0 \\in Df_i\\) we note that \\(\\epsilon_{-i,\\beta}(x) = f_{a_i}(x_0) - f_{-i}(x_0), \\epsilon_{+i,\\beta}(x_0) = \\lim_{x \\to x^-} \\epsilon_{i,\\beta}(x) = \\frac{1}{2}f_{-i}(x_0)\\).\n\n## Proofs of Main Results\n\n### Proof of Theorem 1\n\nUp to rescaling, we only need to prove that \\(\\| \\hat{x} - Tx^* \\|_2 \\leq 3\\epsilon\\) holds uniformly for all \\(x^* \\in K\\). We can assume \\(\\| \\hat{x} - Tx^* \\|_2 \\geq 2\\epsilon\\); otherwise, the desired bound is immediate.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Lemmas", "md": "# Math Equations and Lemmas"}, {"type": "heading", "lvl": 2, "value": "Lemma 10", "md": "## Lemma 10"}, {"type": "text", "value": "If \\(a \\sim \\mathcal{N}(0, I_n)\\), for some function \\(f\\) and any \\(x \\in S^{n-1}\\) it holds that \\(E[f(a^Tx)a] = \\mu x\\) for \\(\\mu = E[f(g)g]\\) with \\(g \\sim \\mathcal{N}(0, 1)\\).", "md": "If \\(a \\sim \\mathcal{N}(0, I_n)\\), for some function \\(f\\) and any \\(x \\in S^{n-1}\\) it holds that \\(E[f(a^Tx)a] = \\mu x\\) for \\(\\mu = E[f(g)g]\\) with \\(g \\sim \\mathcal{N}(0, 1)\\)."}, {"type": "heading", "lvl": 2, "value": "Lemma 11", "md": "## Lemma 11"}, {"type": "text", "value": "Given any \\(\\delta > 0\\), let \\(\\tau \\sim U[-\\delta/2, \\delta/2]\\). Then, for any \\(a \\in \\mathbb{R}\\), it holds that \\(E[Q_\\delta(a + \\tau)] = a\\), where \\(Q_\\delta(\\cdot) = \\delta \\lfloor \\frac{\\cdot}{\\delta} \\rfloor + \\frac{\\delta}{2}\\).", "md": "Given any \\(\\delta > 0\\), let \\(\\tau \\sim U[-\\delta/2, \\delta/2]\\). Then, for any \\(a \\in \\mathbb{R}\\), it holds that \\(E[Q_\\delta(a + \\tau)] = a\\), where \\(Q_\\delta(\\cdot) = \\delta \\lfloor \\frac{\\cdot}{\\delta} \\rfloor + \\frac{\\delta}{2}\\)."}, {"type": "heading", "lvl": 2, "value": "Lemma 12", "md": "## Lemma 12"}, {"type": "text", "value": "Let \\(f_i(\\cdot) = \\delta \\lfloor \\cdot + \\tau_i \\delta \\rfloor + 1\\) for \\(\\tau_i \\sim U[-\\delta/2, \\delta/2]\\), and \\(f_{i,\\beta}(\\cdot)\\) be defined as in (3.4) for some \\(0 < \\beta < \\frac{\\delta}{2}\\). Moreover, let \\(\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - a\\), \\(\\epsilon_{i,\\beta}(a) = f_{i,\\beta}(a) - f_i(a)\\), then for any \\(a \\in \\mathbb{R}\\), \\(|\\xi_{i,\\beta}(a)| \\leq 2\\delta\\), \\(|\\epsilon_{i,\\beta}(a)| \\leq \\delta\\) holds deterministically.", "md": "Let \\(f_i(\\cdot) = \\delta \\lfloor \\cdot + \\tau_i \\delta \\rfloor + 1\\) for \\(\\tau_i \\sim U[-\\delta/2, \\delta/2]\\), and \\(f_{i,\\beta}(\\cdot)\\) be defined as in (3.4) for some \\(0 < \\beta < \\frac{\\delta}{2}\\). Moreover, let \\(\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - a\\), \\(\\epsilon_{i,\\beta}(a) = f_{i,\\beta}(a) - f_i(a)\\), then for any \\(a \\in \\mathbb{R}\\), \\(|\\xi_{i,\\beta}(a)| \\leq 2\\delta\\), \\(|\\epsilon_{i,\\beta}(a)| \\leq \\delta\\) holds deterministically."}, {"type": "heading", "lvl": 2, "value": "Lemma 13", "md": "## Lemma 13"}, {"type": "text", "value": "Suppose that \\(f_i\\) satisfies Assumption 2, and for any \\(\\beta \\in [0, \\frac{3\\delta}{2}]\\) we construct \\(f_{i,\\beta}\\) as in (3.4). Then, for any \\(a \\in \\mathbb{R}\\), we have \\(|\\epsilon_{i,\\beta}(a)| \\leq 2 + B_0 \\cdot 1(a \\in Df_i + [-\\frac{\\beta}{2}, \\frac{\\beta}{2}])\\).", "md": "Suppose that \\(f_i\\) satisfies Assumption 2, and for any \\(\\beta \\in [0, \\frac{3\\delta}{2}]\\) we construct \\(f_{i,\\beta}\\) as in (3.4). Then, for any \\(a \\in \\mathbb{R}\\), we have \\(|\\epsilon_{i,\\beta}(a)| \\leq 2 + B_0 \\cdot 1(a \\in Df_i + [-\\frac{\\beta}{2}, \\frac{\\beta}{2}])\\)."}, {"type": "heading", "lvl": 2, "value": "Lemma 14", "md": "## Lemma 14"}, {"type": "text", "value": "Let \\(G : B_k^2(r) \\rightarrow \\mathbb{R}^n\\) be \\(L\\)-Lipschitz for some \\(r, L > 0\\), and define \\(K = G(B_k^2(r))\\). For any \\(\\alpha \\in (0, 1)\\), if \\(A \\in \\mathbb{R}^{m \\times n}\\) has i.i.d. \\(\\mathcal{N}(0, 1)\\) entries, and \\(m = \\Omega(\\alpha^2 \\log Lr \\delta)\\), then \\(\\sqrt{\\frac{1}{m}A}\\) satisfies S-REC(K, \\(1 - \\alpha, \\delta\\)) with probability at least \\(1 - \\exp(-\\Omega(\\alpha^2m))\\).", "md": "Let \\(G : B_k^2(r) \\rightarrow \\mathbb{R}^n\\) be \\(L\\)-Lipschitz for some \\(r, L > 0\\), and define \\(K = G(B_k^2(r))\\). For any \\(\\alpha \\in (0, 1)\\), if \\(A \\in \\mathbb{R}^{m \\times n}\\) has i.i.d. \\(\\mathcal{N}(0, 1)\\) entries, and \\(m = \\Omega(\\alpha^2 \\log Lr \\delta)\\), then \\(\\sqrt{\\frac{1}{m}A}\\) satisfies S-REC(K, \\(1 - \\alpha, \\delta\\)) with probability at least \\(1 - \\exp(-\\Omega(\\alpha^2m))\\)."}, {"type": "heading", "lvl": 2, "value": "Approximation Error", "md": "## Approximation Error"}, {"type": "text", "value": "The approximation error \\(\\epsilon_{i,\\beta}(\\cdot)\\) can be expanded as:\n\n$$\n\\epsilon_{i,\\beta}(x) = \\begin{cases}\n0, & \\text{if } x \\notin Df_i + [-\\beta/2, \\beta/2] \\\\\nf_{a_i}(x_0) - f_i(x_0 - \\beta/2))(x_0 - x), & \\text{if } x \\in [x_0 - \\beta/2, x_0] \\\\\nf_{a_i}(x_0) - f_i(x_0 + \\beta/2))(x - x_0), & \\text{if } x \\in [x_0, x_0 + \\beta/2]\n\\end{cases}\n$$\nAlthough \\(|\\epsilon_{i,\\beta}|\\) is Lipschitz continuous, \\(\\epsilon_{i,\\beta}\\) is not. In particular, given \\(x_0 \\in Df_i\\) we note that \\(\\epsilon_{-i,\\beta}(x) = f_{a_i}(x_0) - f_{-i}(x_0), \\epsilon_{+i,\\beta}(x_0) = \\lim_{x \\to x^-} \\epsilon_{i,\\beta}(x) = \\frac{1}{2}f_{-i}(x_0)\\).", "md": "The approximation error \\(\\epsilon_{i,\\beta}(\\cdot)\\) can be expanded as:\n\n$$\n\\epsilon_{i,\\beta}(x) = \\begin{cases}\n0, & \\text{if } x \\notin Df_i + [-\\beta/2, \\beta/2] \\\\\nf_{a_i}(x_0) - f_i(x_0 - \\beta/2))(x_0 - x), & \\text{if } x \\in [x_0 - \\beta/2, x_0] \\\\\nf_{a_i}(x_0) - f_i(x_0 + \\beta/2))(x - x_0), & \\text{if } x \\in [x_0, x_0 + \\beta/2]\n\\end{cases}\n$$\nAlthough \\(|\\epsilon_{i,\\beta}|\\) is Lipschitz continuous, \\(\\epsilon_{i,\\beta}\\) is not. In particular, given \\(x_0 \\in Df_i\\) we note that \\(\\epsilon_{-i,\\beta}(x) = f_{a_i}(x_0) - f_{-i}(x_0), \\epsilon_{+i,\\beta}(x_0) = \\lim_{x \\to x^-} \\epsilon_{i,\\beta}(x) = \\frac{1}{2}f_{-i}(x_0)\\)."}, {"type": "heading", "lvl": 2, "value": "Proofs of Main Results", "md": "## Proofs of Main Results"}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 1", "md": "### Proof of Theorem 1"}, {"type": "text", "value": "Up to rescaling, we only need to prove that \\(\\| \\hat{x} - Tx^* \\|_2 \\leq 3\\epsilon\\) holds uniformly for all \\(x^* \\in K\\). We can assume \\(\\| \\hat{x} - Tx^* \\|_2 \\geq 2\\epsilon\\); otherwise, the desired bound is immediate.", "md": "Up to rescaling, we only need to prove that \\(\\| \\hat{x} - Tx^* \\|_2 \\leq 3\\epsilon\\) holds uniformly for all \\(x^* \\in K\\). We can assume \\(\\| \\hat{x} - Tx^* \\|_2 \\geq 2\\epsilon\\); otherwise, the desired bound is immediate."}]}, {"page": 15, "text": "(1) Lower bounding the left-hand side of (3.1).\nWe use S-REC to find a lower bound for                                  \u221aA m(\u02c6  x \u2212     T  x\u2217)    22. Specifically, we invoke Lemma 14\nwith \u03b1 =          1                     \u03f5                                                            k log LT r        , with probability at least\n                  2 and \u03b4 =            2T , which gives that under m = \u2126                                         \u03f5\n1 \u2212    exp(\u2212cm), the following holds:\n                                   \u221a Am(x1 \u2212          x2)        \u2265    1                                                                                   (C.1)\n                                                                      2\u2225x1 \u2212        x2\u22252 \u2212         \u03f5\nRecall that we assume \u2225\u02c6               x \u2212     T  x\u2217\u22252 \u2265      2 2\u03f5, T \u22121     \u02c6                    2T , \u2200     x1, x2 \u2208        K.            x\n(C.1) to obtain                 A       x\u02c6                              \u02c6    x \u2208     K, and x\u2217         \u2208   K, so we set x1 = \u02c6            T , x2 = x\u2217          in\n                              \u221a  m                           \u2265    1     x                 \u2212     \u03f5                \u02c6x \u2212     T  x\u2217     2.\n                                        T \u2212      x\u2217       2       2     T \u2212     x\u2217     2       2T \u2265       4T1\nThus, the left-hand side of (3.1) can be lower bounded by \u2126                                        \u2225\u02c6x \u2212     T  x\u2217\u22252   2  .\n(2) Upper bounding the right-hand side of (3.1).\nAs analysed in (3.3) and (3.5), the right-hand side of (3.1) is bounded by 2\u2225\u02c6                                         x\u2212T       x\u2217\u22252     \u00b7  Ru1     +Ru2        ,\nso all that remains is to bound Ru1, Ru2. In the rest of the proof, we simply write sup                                                                x,v :=\nsup   x\u2208K,v\u2208(K\u2212       \u03f5 )\u2217  and recall the shorthand \u03bei,\u03b2(a) = fi,\u03b2(a) \u2212                                T a. Thus, the first factor of Ru1 is\ngiven by \u03bei,\u03b2(a\u22a4         imx). By centering, we have\nRu1 \u2264       sup     m1           [\u03bei,\u03b2(a\u22a4    i x)](a\u22a4    i v) \u2212      E    [\u03bei,\u03b2(a\u22a4   i x)](a\u22a4     i v)       + sup           [\u03bei,\u03b2(a\u22a4   i x)](a\u22a4     i v)    ,\nand          x,v         i=1                            Ru1,c                                                     x,v E            Ru1,e                  (C.2)\nRu2 \u2264       sup        m1    m     \u03b5i,\u03b2(a\u22a4    i x)      a\u22a4i v     \u2212   E     \u03b5i,\u03b2(a\u22a4    i x)     a\u22a4 i v         + sup            \u03b5i,\u03b2(a\u22a4    i x)     a\u22a4 i v      .\n             x,v            i=1                          Ru2,c                                                      x,v E            Ru2,e                (C.3)\nWe will invoke Theorem 2 multiple times to derive the required bounds.\n(2.1) Bounding the centered product process Ru1,c.\nWe let gx(ai) = \u03bei,\u03b2(a\u22a4               i x) and hv(ai) = a\u22a4   m           i v, and write\n                                 Ru1,c = sup    x,v    m1   i=1      gx(ai)hv(ai) \u2212              E[gx(ai)hv(ai)]                .\nWe verify conditions in Theorem 2 as follows:\n     \u2022 For any x, x\u2032 \u2208              K, because \u03bei,\u03b2 is              L0 + B0    \u03b2 + T        -Lipschitz continuous (Lemma 1), we have\n                                    \u2225gx(ai) \u2212         gx\u2032(ai)\u2225\u03c82 \u2264            (L0 + T + B0         \u03b2 )\u2225a\u22a4    i x \u2212      a\u22a4i x\u2032\u2225\u03c82\n                                                                          = O      L0 + T + B0         \u03b2     \u2225x \u2212     x\u2032\u22252.\n     \u2022 Since ai \u223c             N   (0, In), by Lemma 7, with probability 1 \u2212                                2 exp(\u2212\u2126(n)) we have \u2225ai\u22252 =\n         O(\u221an). On this event, we have\n                                     |gx(ai) \u2212         gx\u2032(ai)| \u2264         (L0 + T + B0         \u03b2 )|a\u22a4   i x \u2212      a\u22a4i x\u2032|\n                                                                      \u2264   (L0 + T + B0         \u03b2 )\u2225ai\u22252\u2225x \u2212             x\u2032\u22252\n                                                                      = O     \u221an       L0 + T + B0         \u03b2       \u2225x \u2212      x\u2032\u22252.\n                                                                               15", "md": "1. Lower bounding the left-hand side of (3.1).\nWe use S-REC to find a lower bound for $\\sqrt{A}m(\\hat{x} - Tx^*)^2$. Specifically, we invoke Lemma 14 with $\\alpha = \\frac{1}{2}$ and $\\delta = \\frac{2}{T}$, which gives that under $m = \\Omega(1 - \\exp(-cm))$, the following holds:\n$\\sqrt{A}(x_1 - x_2) \\geq \\frac{1}{2}\\|x_1 - x_2\\|_2 - \\epsilon$\nRecall that we assume $\\|\\hat{x} - Tx^*\\|_2 \\geq 2\\sqrt{2}\\epsilon, T^{-1}\\hat{x} - 2T, \\forall x_1, x_2 \\in K$. Using (C.1) to obtain $A\\hat{x} \\geq x \\in K$, and $x^* \\in K$, so we set $x_1 = \\hat{x}, x_2 = x^*$ in\n$\\sqrt{m} \\geq \\frac{1}{T - x^*}\\| \\hat{x} - Tx^*\\|_2^2$.\nThus, the left-hand side of (3.1) can be lower bounded by $\\Omega \\| \\hat{x} - Tx^*\\|_2^2$.\n2. Upper bounding the right-hand side of (3.1).\nAs analysed in (3.3) and (3.5), the right-hand side of (3.1) is bounded by $2\\|\\hat{x} - Tx^*\\|_2 \\cdot Ru1 + Ru2$, so all that remains is to bound Ru1, Ru2. In the rest of the proof, we simply write $\\sup_{x,v} := \\sup_{x \\in K, v \\in (K - \\epsilon)^*$ and recall the shorthand $\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - Ta$. Thus, the first factor of Ru1 is given by $\\xi_{i,\\beta}(a^T_i x)$. By centering, we have\n$Ru1 \\leq \\sup_{m1} [\\xi_{i,\\beta}(a^T_i x)](a^T_i v) - E[\\xi_{i,\\beta}(a^T_i x)](a^T_i v) + \\sup_{x,v} [\\xi_{i,\\beta}(a^T_i x)](a^T_i v)$,\nand\n$Ru2 \\leq \\sup_{m1} \\epsilon_{i,\\beta}(a^T_i x) a^T_i v - E \\epsilon_{i,\\beta}(a^T_i x) a^T_i v + \\sup_{x,v} \\epsilon_{i,\\beta}(a^T_i x) a^T_i v$.\nWe will invoke Theorem 2 multiple times to derive the required bounds.\n3. Bounding the centered product process Ru1,c.\nWe let $g_x(a_i) = \\xi_{i,\\beta}(a^T_i x)$ and $h_v(a_i) = a^T_m_i v$, and write\n$Ru1,c = \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]$.\nWe verify conditions in Theorem 2 as follows:\n- For any $x, x' \\in K$, because $\\xi_{i,\\beta}$ is $L_0 + B_0\\beta + T$-Lipschitz continuous (Lemma 1), we have\n$\\|g_x(a_i) - g_{x'}(a_i)\\|_{\\psi2} \\leq (L_0 + T + B_0\\beta)\\|a^T_i x - a^T_i x'\\|_{\\psi2} = O(L_0 + T + B_0\\beta)\\|x - x'\\|_2$.\n- Since $a_i \\sim N(0, I_n)$, by Lemma 7, with probability $1 - 2\\exp(-\\Omega(n))$ we have $\\|a_i\\|_2 = O(\\sqrt{n})$. On this event, we have\n$|g_x(a_i) - g_{x'}(a_i)| \\leq (L_0 + T + B_0\\beta)|a^T_i x - a^T_i x'| \\leq (L_0 + T + B_0\\beta)\\|a_i\\|_2\\|x - x'\\|_2 = O(\\sqrt{n}L_0 + T + B_0\\beta)\\|x - x'\\|_2$.", "images": [], "items": [{"type": "text", "value": "1. Lower bounding the left-hand side of (3.1).\nWe use S-REC to find a lower bound for $\\sqrt{A}m(\\hat{x} - Tx^*)^2$. Specifically, we invoke Lemma 14 with $\\alpha = \\frac{1}{2}$ and $\\delta = \\frac{2}{T}$, which gives that under $m = \\Omega(1 - \\exp(-cm))$, the following holds:\n$\\sqrt{A}(x_1 - x_2) \\geq \\frac{1}{2}\\|x_1 - x_2\\|_2 - \\epsilon$\nRecall that we assume $\\|\\hat{x} - Tx^*\\|_2 \\geq 2\\sqrt{2}\\epsilon, T^{-1}\\hat{x} - 2T, \\forall x_1, x_2 \\in K$. Using (C.1) to obtain $A\\hat{x} \\geq x \\in K$, and $x^* \\in K$, so we set $x_1 = \\hat{x}, x_2 = x^*$ in\n$\\sqrt{m} \\geq \\frac{1}{T - x^*}\\| \\hat{x} - Tx^*\\|_2^2$.\nThus, the left-hand side of (3.1) can be lower bounded by $\\Omega \\| \\hat{x} - Tx^*\\|_2^2$.\n2. Upper bounding the right-hand side of (3.1).\nAs analysed in (3.3) and (3.5), the right-hand side of (3.1) is bounded by $2\\|\\hat{x} - Tx^*\\|_2 \\cdot Ru1 + Ru2$, so all that remains is to bound Ru1, Ru2. In the rest of the proof, we simply write $\\sup_{x,v} := \\sup_{x \\in K, v \\in (K - \\epsilon)^*$ and recall the shorthand $\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - Ta$. Thus, the first factor of Ru1 is given by $\\xi_{i,\\beta}(a^T_i x)$. By centering, we have\n$Ru1 \\leq \\sup_{m1} [\\xi_{i,\\beta}(a^T_i x)](a^T_i v) - E[\\xi_{i,\\beta}(a^T_i x)](a^T_i v) + \\sup_{x,v} [\\xi_{i,\\beta}(a^T_i x)](a^T_i v)$,\nand\n$Ru2 \\leq \\sup_{m1} \\epsilon_{i,\\beta}(a^T_i x) a^T_i v - E \\epsilon_{i,\\beta}(a^T_i x) a^T_i v + \\sup_{x,v} \\epsilon_{i,\\beta}(a^T_i x) a^T_i v$.\nWe will invoke Theorem 2 multiple times to derive the required bounds.\n3. Bounding the centered product process Ru1,c.\nWe let $g_x(a_i) = \\xi_{i,\\beta}(a^T_i x)$ and $h_v(a_i) = a^T_m_i v$, and write\n$Ru1,c = \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]$.\nWe verify conditions in Theorem 2 as follows:\n- For any $x, x' \\in K$, because $\\xi_{i,\\beta}$ is $L_0 + B_0\\beta + T$-Lipschitz continuous (Lemma 1), we have\n$\\|g_x(a_i) - g_{x'}(a_i)\\|_{\\psi2} \\leq (L_0 + T + B_0\\beta)\\|a^T_i x - a^T_i x'\\|_{\\psi2} = O(L_0 + T + B_0\\beta)\\|x - x'\\|_2$.\n- Since $a_i \\sim N(0, I_n)$, by Lemma 7, with probability $1 - 2\\exp(-\\Omega(n))$ we have $\\|a_i\\|_2 = O(\\sqrt{n})$. On this event, we have\n$|g_x(a_i) - g_{x'}(a_i)| \\leq (L_0 + T + B_0\\beta)|a^T_i x - a^T_i x'| \\leq (L_0 + T + B_0\\beta)\\|a_i\\|_2\\|x - x'\\|_2 = O(\\sqrt{n}L_0 + T + B_0\\beta)\\|x - x'\\|_2$.", "md": "1. Lower bounding the left-hand side of (3.1).\nWe use S-REC to find a lower bound for $\\sqrt{A}m(\\hat{x} - Tx^*)^2$. Specifically, we invoke Lemma 14 with $\\alpha = \\frac{1}{2}$ and $\\delta = \\frac{2}{T}$, which gives that under $m = \\Omega(1 - \\exp(-cm))$, the following holds:\n$\\sqrt{A}(x_1 - x_2) \\geq \\frac{1}{2}\\|x_1 - x_2\\|_2 - \\epsilon$\nRecall that we assume $\\|\\hat{x} - Tx^*\\|_2 \\geq 2\\sqrt{2}\\epsilon, T^{-1}\\hat{x} - 2T, \\forall x_1, x_2 \\in K$. Using (C.1) to obtain $A\\hat{x} \\geq x \\in K$, and $x^* \\in K$, so we set $x_1 = \\hat{x}, x_2 = x^*$ in\n$\\sqrt{m} \\geq \\frac{1}{T - x^*}\\| \\hat{x} - Tx^*\\|_2^2$.\nThus, the left-hand side of (3.1) can be lower bounded by $\\Omega \\| \\hat{x} - Tx^*\\|_2^2$.\n2. Upper bounding the right-hand side of (3.1).\nAs analysed in (3.3) and (3.5), the right-hand side of (3.1) is bounded by $2\\|\\hat{x} - Tx^*\\|_2 \\cdot Ru1 + Ru2$, so all that remains is to bound Ru1, Ru2. In the rest of the proof, we simply write $\\sup_{x,v} := \\sup_{x \\in K, v \\in (K - \\epsilon)^*$ and recall the shorthand $\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - Ta$. Thus, the first factor of Ru1 is given by $\\xi_{i,\\beta}(a^T_i x)$. By centering, we have\n$Ru1 \\leq \\sup_{m1} [\\xi_{i,\\beta}(a^T_i x)](a^T_i v) - E[\\xi_{i,\\beta}(a^T_i x)](a^T_i v) + \\sup_{x,v} [\\xi_{i,\\beta}(a^T_i x)](a^T_i v)$,\nand\n$Ru2 \\leq \\sup_{m1} \\epsilon_{i,\\beta}(a^T_i x) a^T_i v - E \\epsilon_{i,\\beta}(a^T_i x) a^T_i v + \\sup_{x,v} \\epsilon_{i,\\beta}(a^T_i x) a^T_i v$.\nWe will invoke Theorem 2 multiple times to derive the required bounds.\n3. Bounding the centered product process Ru1,c.\nWe let $g_x(a_i) = \\xi_{i,\\beta}(a^T_i x)$ and $h_v(a_i) = a^T_m_i v$, and write\n$Ru1,c = \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^{m} g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]$.\nWe verify conditions in Theorem 2 as follows:\n- For any $x, x' \\in K$, because $\\xi_{i,\\beta}$ is $L_0 + B_0\\beta + T$-Lipschitz continuous (Lemma 1), we have\n$\\|g_x(a_i) - g_{x'}(a_i)\\|_{\\psi2} \\leq (L_0 + T + B_0\\beta)\\|a^T_i x - a^T_i x'\\|_{\\psi2} = O(L_0 + T + B_0\\beta)\\|x - x'\\|_2$.\n- Since $a_i \\sim N(0, I_n)$, by Lemma 7, with probability $1 - 2\\exp(-\\Omega(n))$ we have $\\|a_i\\|_2 = O(\\sqrt{n})$. On this event, we have\n$|g_x(a_i) - g_{x'}(a_i)| \\leq (L_0 + T + B_0\\beta)|a^T_i x - a^T_i x'| \\leq (L_0 + T + B_0\\beta)\\|a_i\\|_2\\|x - x'\\|_2 = O(\\sqrt{n}L_0 + T + B_0\\beta)\\|x - x'\\|_2$."}]}, {"page": 16, "text": "      \u2022 Recall (K\u2212      \u03f5 )\u2217   in (3.2). Since (K\u2212         \u03f5 )\u2217   \u2282    Sn\u22121, for any v, v\u2032 \u2208              (K\u2212 \u03f5 )\u2217, we have \u2225a\u22a4         i v \u2212\n         a\u22a4i v\u2032\u2225\u03c82 = O(1)\u2225v \u2212              v\u2032\u22252, and when \u2225ai\u2225            = O(\u221an) we have |a\u22a4            i v \u2212    a\u22a4i v\u2032| \u2264     \u2225ai\u22252\u2225v \u2212\n         v\u2032\u22252 = O(\u221an)\u2225v \u2212                v\u2032\u22252. Moreover, because (K\u2212                \u03f5 )\u2217   \u2282   Bn2 , for any v \u2208         (K\u2212 \u03f5 )\u2217   we have\n         \u2225a\u22a4 i v\u2225\u03c82 = O(1), |a\u22a4         i v| \u2264     \u2225ai\u22252 = O(\u221an).\n Combined with Assumption 3 and its parameters (A(1)                          g , U (1)            ) and (A(2)  g , U (2)           ), Ru1,c\n                                                                                      g , P (1)                         g , P (2)\n                                                                                              0                                 0\n satisfies the conditions of Theorem 2 with the following parameters\n                           Mg \u224d      L0 + T + B0      \u03b2 , Ag = A(1)      g , Mh \u224d        1, Ah \u224d       1\n                                           L0 + T + B0            , U  g = U (1)\n                           Lg \u224d     \u221an                      \u03b2                  g , Lh \u224d        \u221an, Uh \u224d         \u221an\n and P0 = P (1)   0    + 2 exp(\u2212\u2126(n)). Now suppose that we have\n                                            A(1)                                                      A(1)\n                                               g                                                         g\n           m \u2273     H       K,   \u221a  mn[L0 + T + B0         \u03b2 ]     + H         (K\u2212 \u03f5 )\u2217,   \u221am(\u221anU (1)     g    + A(1)g )       ,          (C.4)\n and note that by using Lemma 6, (C.4) can be guaranteed by\n                        m \u2273     k log      Lr\u221am        n(L0 + T + B0        \u03b2 ) + T      (\u221anU (1) g\u03f5   + A(1)g )        .                (C.5)\n                                             A(1)\n                                                g\n Then Theorem 2 yields that the following bound holds with probability at least 1 \u2212                                               mP (1)0    \u2212\n C exp(\u2212\u2126(k)) \u2212            m exp(\u2212\u2126(n)):              A(1)                                                      A(1)\n                      g                                 g                                                          g\n                   \u221a  m   H\n   |Ru1,c| \u2272       A1                K,   \u221a  mn[L0 + T + B0         \u03b2 ]     + H         (K\u2212 \u03f5 )\u2217,   \u221am(\u221anU (1)     g    + A(1)g )\n              \u2272   A(1)        k           Lr\u221am        n(L0 + T + B0                             g     + A(1)g )       .                  (C.6)\n                     g       m log          A(1)                           \u03b2 ) + T     (\u221anU (1)   \u03f5\n                                              g\n (2.2) Bounding the centered product process Ru2,c.\nWe let gx(ai) = |\u03b5i,\u03b2(a\u22a4            i x)| and hv(ai) = |a\u22a4          i v|, and write\n                              Ru2,c = sup         1    m      gx(ai)hv(ai) \u2212           E[gx(ai)hv(ai)]            .\n                                            x,v   m   i=1\nWe verify the conditions in Theorem 2 as follows:\n      \u2022 For any x, x\u2032 \u2208         K, because |\u03b5i,\u03b2| is (2L0 + B0            \u03b2 )-Lipschitz continuous (Lemma 1), we have\n                                   \u2225gx(ai) \u2212       gx\u2032(ai)\u2225\u03c82 \u2264          (2L0 + B0    \u03b2 )\u2225a\u22a4   i x \u2212    a\u22a4 i x\u2032\u2225\u03c82\n                                                                     = O     L0 + B0   \u03b2     \u2225x \u2212    x\u2032\u22252.\n      \u2022 By Lemma 7, with probability at least 1 \u2212                       2 exp(\u2212\u2126(n)) we have \u2225ai\u22252 = O(\u221an). On this\n         event, we have\n                                      |gx(ai) \u2212       gx\u2032(ai)| \u2264      (2L0 + B0     \u03b2 )|a\u22a4  i x \u2212    a\u22a4i x\u2032|\n                                                                   \u2264  (2L0 + B0     \u03b2 )\u2225ai\u22252\u2225x \u2212          x\u2032\u22252\n                                                                   = O 16 \u221an      L0 + B0   \u03b2     \u2225x \u2212     x\u2032\u22252.", "md": "- Recall $(K-\\epsilon)^*$ in (3.2). Since $(K-\\epsilon)^* \\subset S^{n-1}$, for any $v, v' \\in (K-\\epsilon)^*$, we have $\\|a_i^\\top v - a_i^\\top v'\\|_\\psi^2 = O(1)\\|v - v'\\|_2$, and when $\\|a_i\\| = O(\\sqrt{n})$ we have $|a_i^\\top v - a_i^\\top v'| \\leq \\|a_i\\|^2\\|v - v'\\|_2 = O(\\sqrt{n})\\|v - v'\\|_2$. Moreover, because $(K-\\epsilon)^* \\subset B^n_2$, for any $v \\in (K-\\epsilon)^*$ we have $\\|a_i^\\top v\\|_\\psi^2 = O(1)$, $|a_i^\\top v| \\leq \\|a_i\\|^2 = O(\\sqrt{n})$.\n- Combined with Assumption 3 and its parameters $(A(1)_g, U(1))$ and $(A(2)_g, U(2))$, $Ru1,c_g, P(1)_0, P(2)_0$ satisfies the conditions of Theorem 2 with the following parameters:\n$M_g \\approx L_0 + T + B_0^\\beta, A_g = A(1)_g, M_h \\approx 1, A_h \\approx 1$\n$L_g \\approx \\sqrt{n}^\\beta, L_h \\approx \\sqrt{n}, U_g = U(1)$\nand $P_0 = P(1)_0 + 2\\exp(-\\Omega(n))$. Now suppose that we have\n$m \\gtrsim H K, \\sqrt{mn}[L_0 + T + B_0^\\beta] + H(K-\\epsilon)^*, \\sqrt{m}(\\sqrt{n}U(1)_g + A(1)_g)$ (C.4)\nand note that by using Lemma 6, (C.4) can be guaranteed by\n$m \\gtrsim k \\log L_r\\sqrt{m}n(L_0 + T + B_0^\\beta) + T(\\sqrt{n}U(1)_g\\epsilon + A(1)_g)$. (C.5)\nThen Theorem 2 yields that the following bound holds with probability at least $1 - mP(1)_0 - C\\exp(-\\Omega(k)) - m\\exp(-\\Omega(n))$:\n$\\sqrt{m}H |Ru1,c| \\lesssim A(1)_g K, \\sqrt{mn}[L_0 + T + B_0^\\beta] + H(K-\\epsilon)^*, \\sqrt{m}(\\sqrt{n}U(1)_g + A(1)_g)$\n$\\lesssim A(1)_g k L_r\\sqrt{m}n(L_0 + T + B_0^\\beta + A(1)_g)$. (C.6)\n- (2.2) Bounding the centered product process Ru2,c.\nWe let $g_x(a_i) = |\\epsilon_{i,\\beta}(a_i^\\top x)|$ and $h_v(a_i) = |a_i^\\top v|$, and write\n$Ru2,c = \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^m g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]$.\nWe verify the conditions in Theorem 2 as follows:\n- For any $x, x' \\in K$, because $|\\epsilon_{i,\\beta}|$ is $(2L_0 + B_0^\\beta)$-Lipschitz continuous (Lemma 1), we have\n$\\|g_x(a_i) - g_{x'}(a_i)\\|_\\psi^2 \\leq (2L_0 + B_0^\\beta)\\|a_i^\\top x - a_i^\\top x'\\|_\\psi^2 = O(L_0 + B_0^\\beta)\\|x - x'\\|_2$.\n- By Lemma 7, with probability at least $1 - 2\\exp(-\\Omega(n))$ we have $\\|a_i\\|^2 = O(\\sqrt{n})$. On this event, we have\n$|g_x(a_i) - g_{x'}(a_i)| \\leq (2L_0 + B_0^\\beta)|a_i^\\top x - a_i^\\top x'|\n\\leq (2L_0 + B_0^\\beta)\\|a_i\\|^2\\|x - x'\\|_2 = O(16\\sqrt{n}L_0 + B_0^\\beta\\|x - x'\\|_2)$.", "images": [], "items": [{"type": "text", "value": "- Recall $(K-\\epsilon)^*$ in (3.2). Since $(K-\\epsilon)^* \\subset S^{n-1}$, for any $v, v' \\in (K-\\epsilon)^*$, we have $\\|a_i^\\top v - a_i^\\top v'\\|_\\psi^2 = O(1)\\|v - v'\\|_2$, and when $\\|a_i\\| = O(\\sqrt{n})$ we have $|a_i^\\top v - a_i^\\top v'| \\leq \\|a_i\\|^2\\|v - v'\\|_2 = O(\\sqrt{n})\\|v - v'\\|_2$. Moreover, because $(K-\\epsilon)^* \\subset B^n_2$, for any $v \\in (K-\\epsilon)^*$ we have $\\|a_i^\\top v\\|_\\psi^2 = O(1)$, $|a_i^\\top v| \\leq \\|a_i\\|^2 = O(\\sqrt{n})$.\n- Combined with Assumption 3 and its parameters $(A(1)_g, U(1))$ and $(A(2)_g, U(2))$, $Ru1,c_g, P(1)_0, P(2)_0$ satisfies the conditions of Theorem 2 with the following parameters:\n$M_g \\approx L_0 + T + B_0^\\beta, A_g = A(1)_g, M_h \\approx 1, A_h \\approx 1$\n$L_g \\approx \\sqrt{n}^\\beta, L_h \\approx \\sqrt{n}, U_g = U(1)$\nand $P_0 = P(1)_0 + 2\\exp(-\\Omega(n))$. Now suppose that we have\n$m \\gtrsim H K, \\sqrt{mn}[L_0 + T + B_0^\\beta] + H(K-\\epsilon)^*, \\sqrt{m}(\\sqrt{n}U(1)_g + A(1)_g)$ (C.4)\nand note that by using Lemma 6, (C.4) can be guaranteed by\n$m \\gtrsim k \\log L_r\\sqrt{m}n(L_0 + T + B_0^\\beta) + T(\\sqrt{n}U(1)_g\\epsilon + A(1)_g)$. (C.5)\nThen Theorem 2 yields that the following bound holds with probability at least $1 - mP(1)_0 - C\\exp(-\\Omega(k)) - m\\exp(-\\Omega(n))$:\n$\\sqrt{m}H |Ru1,c| \\lesssim A(1)_g K, \\sqrt{mn}[L_0 + T + B_0^\\beta] + H(K-\\epsilon)^*, \\sqrt{m}(\\sqrt{n}U(1)_g + A(1)_g)$\n$\\lesssim A(1)_g k L_r\\sqrt{m}n(L_0 + T + B_0^\\beta + A(1)_g)$. (C.6)\n- (2.2) Bounding the centered product process Ru2,c.\nWe let $g_x(a_i) = |\\epsilon_{i,\\beta}(a_i^\\top x)|$ and $h_v(a_i) = |a_i^\\top v|$, and write\n$Ru2,c = \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^m g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]$.\nWe verify the conditions in Theorem 2 as follows:\n- For any $x, x' \\in K$, because $|\\epsilon_{i,\\beta}|$ is $(2L_0 + B_0^\\beta)$-Lipschitz continuous (Lemma 1), we have\n$\\|g_x(a_i) - g_{x'}(a_i)\\|_\\psi^2 \\leq (2L_0 + B_0^\\beta)\\|a_i^\\top x - a_i^\\top x'\\|_\\psi^2 = O(L_0 + B_0^\\beta)\\|x - x'\\|_2$.\n- By Lemma 7, with probability at least $1 - 2\\exp(-\\Omega(n))$ we have $\\|a_i\\|^2 = O(\\sqrt{n})$. On this event, we have\n$|g_x(a_i) - g_{x'}(a_i)| \\leq (2L_0 + B_0^\\beta)|a_i^\\top x - a_i^\\top x'|\n\\leq (2L_0 + B_0^\\beta)\\|a_i\\|^2\\|x - x'\\|_2 = O(16\\sqrt{n}L_0 + B_0^\\beta\\|x - x'\\|_2)$.", "md": "- Recall $(K-\\epsilon)^*$ in (3.2). Since $(K-\\epsilon)^* \\subset S^{n-1}$, for any $v, v' \\in (K-\\epsilon)^*$, we have $\\|a_i^\\top v - a_i^\\top v'\\|_\\psi^2 = O(1)\\|v - v'\\|_2$, and when $\\|a_i\\| = O(\\sqrt{n})$ we have $|a_i^\\top v - a_i^\\top v'| \\leq \\|a_i\\|^2\\|v - v'\\|_2 = O(\\sqrt{n})\\|v - v'\\|_2$. Moreover, because $(K-\\epsilon)^* \\subset B^n_2$, for any $v \\in (K-\\epsilon)^*$ we have $\\|a_i^\\top v\\|_\\psi^2 = O(1)$, $|a_i^\\top v| \\leq \\|a_i\\|^2 = O(\\sqrt{n})$.\n- Combined with Assumption 3 and its parameters $(A(1)_g, U(1))$ and $(A(2)_g, U(2))$, $Ru1,c_g, P(1)_0, P(2)_0$ satisfies the conditions of Theorem 2 with the following parameters:\n$M_g \\approx L_0 + T + B_0^\\beta, A_g = A(1)_g, M_h \\approx 1, A_h \\approx 1$\n$L_g \\approx \\sqrt{n}^\\beta, L_h \\approx \\sqrt{n}, U_g = U(1)$\nand $P_0 = P(1)_0 + 2\\exp(-\\Omega(n))$. Now suppose that we have\n$m \\gtrsim H K, \\sqrt{mn}[L_0 + T + B_0^\\beta] + H(K-\\epsilon)^*, \\sqrt{m}(\\sqrt{n}U(1)_g + A(1)_g)$ (C.4)\nand note that by using Lemma 6, (C.4) can be guaranteed by\n$m \\gtrsim k \\log L_r\\sqrt{m}n(L_0 + T + B_0^\\beta) + T(\\sqrt{n}U(1)_g\\epsilon + A(1)_g)$. (C.5)\nThen Theorem 2 yields that the following bound holds with probability at least $1 - mP(1)_0 - C\\exp(-\\Omega(k)) - m\\exp(-\\Omega(n))$:\n$\\sqrt{m}H |Ru1,c| \\lesssim A(1)_g K, \\sqrt{mn}[L_0 + T + B_0^\\beta] + H(K-\\epsilon)^*, \\sqrt{m}(\\sqrt{n}U(1)_g + A(1)_g)$\n$\\lesssim A(1)_g k L_r\\sqrt{m}n(L_0 + T + B_0^\\beta + A(1)_g)$. (C.6)\n- (2.2) Bounding the centered product process Ru2,c.\nWe let $g_x(a_i) = |\\epsilon_{i,\\beta}(a_i^\\top x)|$ and $h_v(a_i) = |a_i^\\top v|$, and write\n$Ru2,c = \\sup_{x,v} \\frac{1}{m} \\sum_{i=1}^m g_x(a_i)h_v(a_i) - E[g_x(a_i)h_v(a_i)]$.\nWe verify the conditions in Theorem 2 as follows:\n- For any $x, x' \\in K$, because $|\\epsilon_{i,\\beta}|$ is $(2L_0 + B_0^\\beta)$-Lipschitz continuous (Lemma 1), we have\n$\\|g_x(a_i) - g_{x'}(a_i)\\|_\\psi^2 \\leq (2L_0 + B_0^\\beta)\\|a_i^\\top x - a_i^\\top x'\\|_\\psi^2 = O(L_0 + B_0^\\beta)\\|x - x'\\|_2$.\n- By Lemma 7, with probability at least $1 - 2\\exp(-\\Omega(n))$ we have $\\|a_i\\|^2 = O(\\sqrt{n})$. On this event, we have\n$|g_x(a_i) - g_{x'}(a_i)| \\leq (2L_0 + B_0^\\beta)|a_i^\\top x - a_i^\\top x'|\n\\leq (2L_0 + B_0^\\beta)\\|a_i\\|^2\\|x - x'\\|_2 = O(16\\sqrt{n}L_0 + B_0^\\beta\\|x - x'\\|_2)$."}]}, {"page": 17, "text": "      \u2022 For any v, v\u2032 \u2208         (K\u2212  \u03f5 )\u2217  we have       |a\u22a4 i v| \u2212     |a\u22a4i v\u2032|   \u03c82 \u2264     \u2225a\u22a4 i (v \u2212    v\u2032)\u2225\u03c82 = O(1)\u2225v \u2212              v\u2032\u22252.\n         Similarly as before, we assume \u2225ai\u22252 = O(\u221an), which gives |a\u22a4                           |a\u22a4     i v \u2212    a\u22a4i v\u2032| \u2264     \u2225ai\u22252\u2225v \u2212\n         v\u2032\u22252 = O(\u221an)\u2225v \u2212              v\u2032\u22252. Moreover, (K\u2212          \u03f5 )\u2217   \u2282   Bn2 implies           i v|    \u03c82 = O(1) and |a\u22a4         i v| \u2264\n         \u2225ai\u22252\u2225v\u22252 = O(\u221an) holds for all v \u2208                      (K\u2212  \u03f5 )\u2217.\n Combined with Assumption 3, Ru2,c satisfies the conditions of Theorem 2 with\n                               Mg \u224d      L0 + B0   \u03b2 , Ag = A(2)     g , Mh \u224d         1, Ah \u224d      O(1)\n                               Lg \u224d     \u221an     L0 + B0   \u03b2    , Ug = U (2)  g , Lh \u224d        \u221an, Uh \u224d         \u221an\n and P0 = P (2)   0    + 2 exp(\u2212\u2126(n)). Suppose we have\n                                               A(2)                                                   A(2)\n                                                  g                                                      g\n                  m \u2273     H       K,   \u221a  mn[L0 + B0      \u03b2 ]     + H         (K\u2212 \u03f5 )\u2217,   \u221am(A(2)   g    + \u221anU (2)  g )       ,\n which can be guaranteed (from Lemma 6) by\n                                              Lr\u221am                                       g    + \u221anU (2)  g )\n                           m \u2273     k log        A(2)       n   L0 + B0   \u03b2     + T   (A(2)      \u03f5                    .                   (C.7)\n                                                   g\n Then, we can invoke Theorem 2 to obtain that the following bound holds with probability at least\n1 \u2212    mP (2)    \u2212   2m exp(\u2212\u2126(n)) \u2212              C exp(\u2212\u2126(k)):\n            0\n                                                      A(2)                                                   A(2)\n                         g                               g                                                      g\n                      \u221a  m    H                                          + H         (K\u2212\n      |Ru2,c| \u2272       A(2)               K,   \u221a  mn[L0 + B0      \u03b2 ]                     \u03f5 )\u2217,  \u221am(A(2)    g   + \u221anU (2)   g )\n                                 k           Lr\u221am                                       g    + \u221anU (2)  g )                              (C.8)\n                  \u2272  A(2)                                 n  L0 + B0          + T   (A(2)                          .\n                        g       m log          A(2)                    \u03b2                       \u03f5\n                                                  g\n (2.3) Bounding the expectation terms Ru1,e, Ru2,e.\n Recall that \u03bei,\u03b2(a) = fi,\u03b2(a)\u2212T                a and \u03b5i,\u03b2(a) = fi,\u03b2(a)\u2212fi(a), and so \u03bei,\u03b2(a) = \u03b5i,\u03b2(a)+fi(a)\u2212\n T a. Hence, by using E[aia\u22a4             i ] = In and \u2225v\u22252 = 1, we have\n                   Ru1,e \u2264       sup         fi(a\u22a4  i x) \u2212    T  a\u22a4i x    (a\u22a4i v)     + sup         [\u03b5i,\u03b2(a\u22a4  i x)]a\u22a4  i v\n                                 x,v E                                                    x,v E\n                             \u2264   sup     E[fi(a\u22a4    i x)ai] \u2212      T  x   2 + supx,v E     |\u03b5i,\u03b2(a\u22a4  i x)||a\u22a4  i v|                      (C.9)\n                                 x\u2208X\n                             \u2264   sup    \u03c1(x) + Ru2,e,\n                                 x\u2208X\n where \u03c1(x) is the model mismatch defined in (2.3), and Ru2,e is defined in (C.3). It remains to bound\n Ru2,e, for which we first apply Cauchy-Schwarz (with \u2225v\u22252 \u2264                                1) and then use Lemma 13 to obtain\n                                 Ru2,e = sup   x,v E    |\u03b5i,\u03b2(a\u22a4   i x)||a\u22a4  i v|\n                               \u2264   sup      E[|\u03b5i,\u03b2(a\u22a4    i x)|2]       E[|a\u22a4  i v|2]\n                                   x,v\n                               \u2264   sup       E    |\u03b5i,\u03b2(a\u22a4                a\u22a4                      \u2212   \u03b2                                (C.10)\n                                   x\u2208K                       i x)|21        i x \u2208    Dfi +            2 , \u03b22\n                               \u2264    3L0\u03b2       + B0       sup       P    a\u22a4i x \u2208    Dfi +        \u2212   \u03b2\n                                        2                x\u2208K                                         2 , \u03b22\n                               \u2264    3L0\u03b2       + B0       sup       \u00b5\u03b2(x),\n                                        2                x\u2208K\n                                                                       17", "md": "# Math Equations in HTML\n\nFor any \\(v, v' \\in (K - \\epsilon)^*\\) we have\n$$|a_i^\\top v| - |a_i^\\top v'| \\leq \\psi^2 \\leq \\|a_i^\\top (v - v')\\|_{\\psi^2} = O(1)\\|v - v'\\|_2.$$\n\nSimilarly as before, we assume \\(\\|a_i\\|_2 = O(\\sqrt{n})\\), which gives\n$$|a_i^\\top v - a_i^\\top v'| \\leq \\|a_i\\|_2\\|v - v'\\|_2 = O(\\sqrt{n})\\|v - v'\\|_2.$$ Moreover, \\((K - \\epsilon)^* \\subset Bn^2\\) implies\n$$|a_i^\\top v| \\leq \\psi^2 = O(1)$$ and \\(|a_i^\\top v| \\leq \\|a_i\\|_2\\|v\\|_2 = O(\\sqrt{n})\\) holds for all \\(v \\in (K - \\epsilon)^*\\).\n\nCombined with Assumption 3, \\(Ru2,c\\) satisfies the conditions of Theorem 2 with\n\n$$Mg \\gtrapprox L0 + B0 \\beta, Ag = A(2)_g, Mh \\gtrapprox 1, Ah \\gtrapprox O(1)$$\n\n$$Lg \\gtrapprox \\sqrt{n} L0 + B0 \\beta, Ug = U(2)_g, Lh \\gtrapprox \\sqrt{n}, Uh \\gtrapprox \\sqrt{n}$$\n\nand \\(P0 = P(2)_0 + 2 \\exp(-\\Omega(n))\\). Suppose we have\n\n$$m \\gtrapprox H K, \\sqrt{mn}[L0 + B0 \\beta] + H (K - \\epsilon)^*, \\sqrt{m}(A(2)_g + \\sqrt{n}U(2)_g),$$\n\nwhich can be guaranteed (from Lemma 6) by\n\n$$m \\gtrapprox k \\log A(2)_n L0 + B0 \\beta + T(A(2)_g \\epsilon$$\n\nThen, we can invoke Theorem 2 to obtain that the following bound holds with probability at least\n\\(1 - mP(2) - 2m \\exp(-\\Omega(n)) - C \\exp(-\\Omega(k))\\):\n\n$$|Ru2,c| \\lesssim A(2)_g K, \\sqrt{mn}[L0 + B0 \\beta] \\epsilon)^*, \\sqrt{m}(A(2)_g + \\sqrt{n}U(2)_g)$$\n\n$$\\lesssim A(2)_g n L0 + B0 + T(A(2)_g \\epsilon$$\n\n(2.3) Bounding the expectation terms Ru1,e, Ru2,e.\n\nRecall that \\(\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - T a\\) and \\(\\epsilon_{i,\\beta}(a) = f_{i,\\beta}(a) - f_i(a)\\), and so \\(\\xi_{i,\\beta}(a) = \\epsilon_{i,\\beta}(a) + f_i(a) - T a\\). Hence, by using \\(E[a_ia_i^\\top] = I_n\\) and \\(\\|v\\|_2 = 1\\), we have\n\n$$Ru1,e \\leq \\sup_{x,v} f_i(a_i^\\top x) - T a_i^\\top x (a_i^\\top v) + \\sup_{x,v} [\\epsilon_{i,\\beta}(a_i^\\top x)]a_i^\\top v$$\n\n$$\\leq \\sup_{x \\in X} E[fi(a_i^\\top x)a_i] - T x^2 + \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top x)||a_i^\\top v|$$\n\n$$\\leq \\sup_{x \\in X} \\rho(x) + Ru2,e,$$\n\nwhere \\(\\rho(x)\\) is the model mismatch defined in (2.3), and \\(Ru2,e\\) is defined in (C.3). It remains to bound \\(Ru2,e\\), for which we first apply Cauchy-Schwarz (with \\(\\|v\\|_2 \\leq 1\\)) and then use Lemma 13 to obtain\n\n$$Ru2,e = \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top x)||a_i^\\top v|$$\n\n$$\\leq \\sup_{x,v} E[| \\epsilon_{i,\\beta}(a_i^\\top x)|^2] E[|a_i^\\top v|^2]$$\n\n$$\\leq \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top a_i^\\top - \\beta$$\n\n$$\\leq 3L0\\beta^2 + B0 \\sup_{x \\in K} P a_i^\\top x \\in Dfi + - \\beta$$\n\n$$\\leq 3L0\\beta^2 + B0 \\sup_{x \\in K} \\mu_\\beta(x),$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations in HTML", "md": "# Math Equations in HTML"}, {"type": "text", "value": "For any \\(v, v' \\in (K - \\epsilon)^*\\) we have\n$$|a_i^\\top v| - |a_i^\\top v'| \\leq \\psi^2 \\leq \\|a_i^\\top (v - v')\\|_{\\psi^2} = O(1)\\|v - v'\\|_2.$$\n\nSimilarly as before, we assume \\(\\|a_i\\|_2 = O(\\sqrt{n})\\), which gives\n$$|a_i^\\top v - a_i^\\top v'| \\leq \\|a_i\\|_2\\|v - v'\\|_2 = O(\\sqrt{n})\\|v - v'\\|_2.$$ Moreover, \\((K - \\epsilon)^* \\subset Bn^2\\) implies\n$$|a_i^\\top v| \\leq \\psi^2 = O(1)$$ and \\(|a_i^\\top v| \\leq \\|a_i\\|_2\\|v\\|_2 = O(\\sqrt{n})\\) holds for all \\(v \\in (K - \\epsilon)^*\\).\n\nCombined with Assumption 3, \\(Ru2,c\\) satisfies the conditions of Theorem 2 with\n\n$$Mg \\gtrapprox L0 + B0 \\beta, Ag = A(2)_g, Mh \\gtrapprox 1, Ah \\gtrapprox O(1)$$\n\n$$Lg \\gtrapprox \\sqrt{n} L0 + B0 \\beta, Ug = U(2)_g, Lh \\gtrapprox \\sqrt{n}, Uh \\gtrapprox \\sqrt{n}$$\n\nand \\(P0 = P(2)_0 + 2 \\exp(-\\Omega(n))\\). Suppose we have\n\n$$m \\gtrapprox H K, \\sqrt{mn}[L0 + B0 \\beta] + H (K - \\epsilon)^*, \\sqrt{m}(A(2)_g + \\sqrt{n}U(2)_g),$$\n\nwhich can be guaranteed (from Lemma 6) by\n\n$$m \\gtrapprox k \\log A(2)_n L0 + B0 \\beta + T(A(2)_g \\epsilon$$\n\nThen, we can invoke Theorem 2 to obtain that the following bound holds with probability at least\n\\(1 - mP(2) - 2m \\exp(-\\Omega(n)) - C \\exp(-\\Omega(k))\\):\n\n$$|Ru2,c| \\lesssim A(2)_g K, \\sqrt{mn}[L0 + B0 \\beta] \\epsilon)^*, \\sqrt{m}(A(2)_g + \\sqrt{n}U(2)_g)$$\n\n$$\\lesssim A(2)_g n L0 + B0 + T(A(2)_g \\epsilon$$\n\n(2.3) Bounding the expectation terms Ru1,e, Ru2,e.\n\nRecall that \\(\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - T a\\) and \\(\\epsilon_{i,\\beta}(a) = f_{i,\\beta}(a) - f_i(a)\\), and so \\(\\xi_{i,\\beta}(a) = \\epsilon_{i,\\beta}(a) + f_i(a) - T a\\). Hence, by using \\(E[a_ia_i^\\top] = I_n\\) and \\(\\|v\\|_2 = 1\\), we have\n\n$$Ru1,e \\leq \\sup_{x,v} f_i(a_i^\\top x) - T a_i^\\top x (a_i^\\top v) + \\sup_{x,v} [\\epsilon_{i,\\beta}(a_i^\\top x)]a_i^\\top v$$\n\n$$\\leq \\sup_{x \\in X} E[fi(a_i^\\top x)a_i] - T x^2 + \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top x)||a_i^\\top v|$$\n\n$$\\leq \\sup_{x \\in X} \\rho(x) + Ru2,e,$$\n\nwhere \\(\\rho(x)\\) is the model mismatch defined in (2.3), and \\(Ru2,e\\) is defined in (C.3). It remains to bound \\(Ru2,e\\), for which we first apply Cauchy-Schwarz (with \\(\\|v\\|_2 \\leq 1\\)) and then use Lemma 13 to obtain\n\n$$Ru2,e = \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top x)||a_i^\\top v|$$\n\n$$\\leq \\sup_{x,v} E[| \\epsilon_{i,\\beta}(a_i^\\top x)|^2] E[|a_i^\\top v|^2]$$\n\n$$\\leq \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top a_i^\\top - \\beta$$\n\n$$\\leq 3L0\\beta^2 + B0 \\sup_{x \\in K} P a_i^\\top x \\in Dfi + - \\beta$$\n\n$$\\leq 3L0\\beta^2 + B0 \\sup_{x \\in K} \\mu_\\beta(x),$$", "md": "For any \\(v, v' \\in (K - \\epsilon)^*\\) we have\n$$|a_i^\\top v| - |a_i^\\top v'| \\leq \\psi^2 \\leq \\|a_i^\\top (v - v')\\|_{\\psi^2} = O(1)\\|v - v'\\|_2.$$\n\nSimilarly as before, we assume \\(\\|a_i\\|_2 = O(\\sqrt{n})\\), which gives\n$$|a_i^\\top v - a_i^\\top v'| \\leq \\|a_i\\|_2\\|v - v'\\|_2 = O(\\sqrt{n})\\|v - v'\\|_2.$$ Moreover, \\((K - \\epsilon)^* \\subset Bn^2\\) implies\n$$|a_i^\\top v| \\leq \\psi^2 = O(1)$$ and \\(|a_i^\\top v| \\leq \\|a_i\\|_2\\|v\\|_2 = O(\\sqrt{n})\\) holds for all \\(v \\in (K - \\epsilon)^*\\).\n\nCombined with Assumption 3, \\(Ru2,c\\) satisfies the conditions of Theorem 2 with\n\n$$Mg \\gtrapprox L0 + B0 \\beta, Ag = A(2)_g, Mh \\gtrapprox 1, Ah \\gtrapprox O(1)$$\n\n$$Lg \\gtrapprox \\sqrt{n} L0 + B0 \\beta, Ug = U(2)_g, Lh \\gtrapprox \\sqrt{n}, Uh \\gtrapprox \\sqrt{n}$$\n\nand \\(P0 = P(2)_0 + 2 \\exp(-\\Omega(n))\\). Suppose we have\n\n$$m \\gtrapprox H K, \\sqrt{mn}[L0 + B0 \\beta] + H (K - \\epsilon)^*, \\sqrt{m}(A(2)_g + \\sqrt{n}U(2)_g),$$\n\nwhich can be guaranteed (from Lemma 6) by\n\n$$m \\gtrapprox k \\log A(2)_n L0 + B0 \\beta + T(A(2)_g \\epsilon$$\n\nThen, we can invoke Theorem 2 to obtain that the following bound holds with probability at least\n\\(1 - mP(2) - 2m \\exp(-\\Omega(n)) - C \\exp(-\\Omega(k))\\):\n\n$$|Ru2,c| \\lesssim A(2)_g K, \\sqrt{mn}[L0 + B0 \\beta] \\epsilon)^*, \\sqrt{m}(A(2)_g + \\sqrt{n}U(2)_g)$$\n\n$$\\lesssim A(2)_g n L0 + B0 + T(A(2)_g \\epsilon$$\n\n(2.3) Bounding the expectation terms Ru1,e, Ru2,e.\n\nRecall that \\(\\xi_{i,\\beta}(a) = f_{i,\\beta}(a) - T a\\) and \\(\\epsilon_{i,\\beta}(a) = f_{i,\\beta}(a) - f_i(a)\\), and so \\(\\xi_{i,\\beta}(a) = \\epsilon_{i,\\beta}(a) + f_i(a) - T a\\). Hence, by using \\(E[a_ia_i^\\top] = I_n\\) and \\(\\|v\\|_2 = 1\\), we have\n\n$$Ru1,e \\leq \\sup_{x,v} f_i(a_i^\\top x) - T a_i^\\top x (a_i^\\top v) + \\sup_{x,v} [\\epsilon_{i,\\beta}(a_i^\\top x)]a_i^\\top v$$\n\n$$\\leq \\sup_{x \\in X} E[fi(a_i^\\top x)a_i] - T x^2 + \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top x)||a_i^\\top v|$$\n\n$$\\leq \\sup_{x \\in X} \\rho(x) + Ru2,e,$$\n\nwhere \\(\\rho(x)\\) is the model mismatch defined in (2.3), and \\(Ru2,e\\) is defined in (C.3). It remains to bound \\(Ru2,e\\), for which we first apply Cauchy-Schwarz (with \\(\\|v\\|_2 \\leq 1\\)) and then use Lemma 13 to obtain\n\n$$Ru2,e = \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top x)||a_i^\\top v|$$\n\n$$\\leq \\sup_{x,v} E[| \\epsilon_{i,\\beta}(a_i^\\top x)|^2] E[|a_i^\\top v|^2]$$\n\n$$\\leq \\sup_{x,v} E | \\epsilon_{i,\\beta}(a_i^\\top a_i^\\top - \\beta$$\n\n$$\\leq 3L0\\beta^2 + B0 \\sup_{x \\in K} P a_i^\\top x \\in Dfi + - \\beta$$\n\n$$\\leq 3L0\\beta^2 + B0 \\sup_{x \\in K} \\mu_\\beta(x),$$"}]}, {"page": 18, "text": "where we use Lemma 13 in the third and fourth line, and \u00b5\u03b2(x) is defined in (2.4).\n(3) Combining everything to conclude the proof.\nRecall that in Assumption 4, we assume that\n                                                sup    \u03c1(x) \u2272     (A(1)    \u2228  A(2)         k\nand we take sufficiently small \u03b21 such that     x\u2208X                    g         g )      m,\n                                  (L0\u03b21 + B0) sup               \u00b5\u03b2  1(x) \u2272      (A(1)    \u2228  A(2)        k\n                                                      x\u2208K                           g          g )      m,\nthen by setting \u03b2 = \u03b21, the derived bound of Ru1,c + Ru2,c (see (C.6) and (C.8)) dominates that of\nRu1,e + Ru2,e (see (C.9) and (C.10)), and so Ru1 + Ru2 \u2272                                Ru1,c + Ru2,c.\nRecall that (C.6) and (C.8) are guaranteed by the sample size of (C.5) and (C.7), while (C.5) and\n(C.7) hold as long as\n                            Lr\u221am                                                              \u2228   U (2)                \u2228  A(2)\n      m \u2273     k log      A(1)   \u2227   A(2)     n   L0 + T + B0     \u03b21     + T    (\u221an(U (1) g          g )\u03f5+ (A(1)   g          g ))\n                            g         g\n          := kL                                    (here we use L to abbreviate the log factors)\nwith probability at least 1 \u2212            m(P (1)     + P (2)) \u2212      m exp(\u2212\u2126(n)) \u2212             C exp(\u2212\u2126(k)) we have                  (C.11)\n                                                0          0\n                                                                            \u2228  A(2)     kL\n                                         Ru1,c + Ru2,c \u2272           (A(1)          g )\n                                                                       g                    m .\nTherefore, the right-hand side of (3.1) can be uniformly bounded by\n                                        O     \u2225\u02c6x \u2212    T  x\u2217\u22252 \u00b7 (A(1)       \u2228  A(2)     kL           .                               (C.12)\n                                                                        g          g )       m\nCombining with the uniform lower bound for the left-hand side of (3.1), i.e., \u2126(\u2225\u02c6                                     x \u2212   T  x\u2217\u22252  2), we\nobtain the following bound uniformly for all x\u2217:\n                                            \u2225\u02c6                              \u2228   A(2)     kL\n                                              x \u2212   T  x\u2217\u22252 \u2272       (A(1)         g )\n                                                                        g                    m .\nHence, as long as\n                                                    m \u2273       A(1)    \u2228  A(2)   2 kL\n                                                                 g         g        \u03f52 ,\nwe again obtain \u2225\u02c6       x \u2212    T x\u2217\u22252 \u2264       3\u03f5, which completes the proof.\nC.2      Proof of Theorem 2\nProof. Step 1. Control the process over finite nets.\nRecall that X and V are the index sets of x and v, as stated in Theorem 2. We first establish\nthe desired concentration for a fixed pair (x, v) \u2208                         X \u00d7 V. By Lemma 2, \u2225gx(ai)hv(ai)\u2225\u03c81 \u2264\n\u2225gx(ai)\u2225\u03c82\u2225hv(ai)\u2225\u03c82 \u2264                 AgAh. Furthermore, centering (Lemma 3) gives\n                                 \u2225gx(ai)hv(ai) \u2212           E[gx(ai)hv(ai)]\u2225\u03c81 = O(AgAh).\nThus, for fixed (x, v) \u2208           X \u00d7 V we define  m\n                                   Ix,v = 1    m   i=1   gx(ai)hv(ai) \u2212          E[gx(ai)hv(ai)].\nThen, we can invoke Bernstein\u2019s inequality (Lemma 4) to obtain for any t \u2265                                     0 that\n                           P    |Ix,v| \u2265     t   \u2264   2 exp      \u2212cm min             AgAht     2  , AgAht          .                   (C.13)\n                                                                      18", "md": "where we use Lemma 13 in the third and fourth line, and \u03bc\u03b2(x) is defined in (2.4).\n\n(3) Combining everything to conclude the proof.\n\nRecall that in Assumption 4, we assume that\n\nsup \u03c1(x) \u2272 (A(1) \u2228 A(2) k\nx\u2208X g g ) m,\n(L0\u03b21 + B0) sup \u00b5\u03b2 1(x) \u2272 (A(1) \u2228 A(2) k\nx\u2208K g g ) m,\n\nthen by setting \u03b2 = \u03b21, the derived bound of Ru1,c + Ru2,c (see (C.6) and (C.8)) dominates that of Ru1,e + Ru2,e (see (C.9) and (C.10)), and so Ru1 + Ru2 \u2272 Ru1,c + Ru2,c.\n\nRecall that (C.6) and (C.8) are guaranteed by the sample size of (C.5) and (C.7), while (C.5) and (C.7) hold as long as\n\n$$\nm \u2273 k \\log A(1) \u2227 A(2) n L0 + T + B0 \u03b21 + T (\u221an(U (1) g g )\u03f5+ (A(1) g g ))\n$$\n\nwith probability at least 1 \u2212 m(P (1) + P (2)) \u2212 m exp(\u2212\u2126(n)) \u2212 C exp(\u2212\u2126(k)) we have (C.11)\n\n$$\nRu1,c + Ru2,c \u2272 (A(1) g ) m .\n$$\n\nTherefore, the right-hand side of (3.1) can be uniformly bounded by\n\nO \u2225\u02c6x \u2212 T x\u2217\u22252 \u00b7 (A(1) \u2228 A(2) kL . (C.12)\n\nCombining with the uniform lower bound for the left-hand side of (3.1), i.e., \u2126(\u2225\u02c6 x \u2212 T x\u2217\u22252 2), we obtain the following bound uniformly for all x\u2217:\n\n$$\n\u2225\u02c6 x \u2212 T x\u2217\u22252 \u2272 (A(1) g ) m .\n$$\n\nHence, as long as\n\n$$\nm \u2273 A(1) \u2228 A(2) 2 kL g g \u03f52 ,\n$$\n\nwe again obtain \u2225\u02c6 x \u2212 T x\u2217\u22252 \u2264 3\u03f5, which completes the proof.\n\nC.2 Proof of Theorem 2\n\nProof. Step 1. Control the process over finite nets.\n\nRecall that X and V are the index sets of x and v, as stated in Theorem 2. We first establish the desired concentration for a fixed pair (x, v) \u2208 X \u00d7 V. By Lemma 2, \u2225gx(ai)hv(ai)\u2225\u03c81 \u2264 \u2225gx(ai)\u2225\u03c82\u2225hv(ai)\u2225\u03c82 \u2264 AgAh. Furthermore, centering (Lemma 3) gives\n\n\u2225gx(ai)hv(ai) \u2212 E[gx(ai)hv(ai)]\u2225\u03c81 = O(AgAh).\n\nThus, for fixed (x, v) \u2208 X \u00d7 V we define\n\n$$\nIx,v = \\frac{1}{m} \\sum_{i=1}^{m} gx(ai)hv(ai) - E[gx(ai)hv(ai)].\n$$\n\nThen, we can invoke Bernstein\u2019s inequality (Lemma 4) to obtain for any t \u2265 0 that\n\n$$\nP |Ix,v| \u2265 t \u2264 2 \\exp \\left( -cm \\min(AgAht^2, AgAht) \\right) . (C.13)\n$$", "images": [], "items": [{"type": "text", "value": "where we use Lemma 13 in the third and fourth line, and \u03bc\u03b2(x) is defined in (2.4).\n\n(3) Combining everything to conclude the proof.\n\nRecall that in Assumption 4, we assume that\n\nsup \u03c1(x) \u2272 (A(1) \u2228 A(2) k\nx\u2208X g g ) m,\n(L0\u03b21 + B0) sup \u00b5\u03b2 1(x) \u2272 (A(1) \u2228 A(2) k\nx\u2208K g g ) m,\n\nthen by setting \u03b2 = \u03b21, the derived bound of Ru1,c + Ru2,c (see (C.6) and (C.8)) dominates that of Ru1,e + Ru2,e (see (C.9) and (C.10)), and so Ru1 + Ru2 \u2272 Ru1,c + Ru2,c.\n\nRecall that (C.6) and (C.8) are guaranteed by the sample size of (C.5) and (C.7), while (C.5) and (C.7) hold as long as\n\n$$\nm \u2273 k \\log A(1) \u2227 A(2) n L0 + T + B0 \u03b21 + T (\u221an(U (1) g g )\u03f5+ (A(1) g g ))\n$$\n\nwith probability at least 1 \u2212 m(P (1) + P (2)) \u2212 m exp(\u2212\u2126(n)) \u2212 C exp(\u2212\u2126(k)) we have (C.11)\n\n$$\nRu1,c + Ru2,c \u2272 (A(1) g ) m .\n$$\n\nTherefore, the right-hand side of (3.1) can be uniformly bounded by\n\nO \u2225\u02c6x \u2212 T x\u2217\u22252 \u00b7 (A(1) \u2228 A(2) kL . (C.12)\n\nCombining with the uniform lower bound for the left-hand side of (3.1), i.e., \u2126(\u2225\u02c6 x \u2212 T x\u2217\u22252 2), we obtain the following bound uniformly for all x\u2217:\n\n$$\n\u2225\u02c6 x \u2212 T x\u2217\u22252 \u2272 (A(1) g ) m .\n$$\n\nHence, as long as\n\n$$\nm \u2273 A(1) \u2228 A(2) 2 kL g g \u03f52 ,\n$$\n\nwe again obtain \u2225\u02c6 x \u2212 T x\u2217\u22252 \u2264 3\u03f5, which completes the proof.\n\nC.2 Proof of Theorem 2\n\nProof. Step 1. Control the process over finite nets.\n\nRecall that X and V are the index sets of x and v, as stated in Theorem 2. We first establish the desired concentration for a fixed pair (x, v) \u2208 X \u00d7 V. By Lemma 2, \u2225gx(ai)hv(ai)\u2225\u03c81 \u2264 \u2225gx(ai)\u2225\u03c82\u2225hv(ai)\u2225\u03c82 \u2264 AgAh. Furthermore, centering (Lemma 3) gives\n\n\u2225gx(ai)hv(ai) \u2212 E[gx(ai)hv(ai)]\u2225\u03c81 = O(AgAh).\n\nThus, for fixed (x, v) \u2208 X \u00d7 V we define\n\n$$\nIx,v = \\frac{1}{m} \\sum_{i=1}^{m} gx(ai)hv(ai) - E[gx(ai)hv(ai)].\n$$\n\nThen, we can invoke Bernstein\u2019s inequality (Lemma 4) to obtain for any t \u2265 0 that\n\n$$\nP |Ix,v| \u2265 t \u2264 2 \\exp \\left( -cm \\min(AgAht^2, AgAht) \\right) . (C.13)\n$$", "md": "where we use Lemma 13 in the third and fourth line, and \u03bc\u03b2(x) is defined in (2.4).\n\n(3) Combining everything to conclude the proof.\n\nRecall that in Assumption 4, we assume that\n\nsup \u03c1(x) \u2272 (A(1) \u2228 A(2) k\nx\u2208X g g ) m,\n(L0\u03b21 + B0) sup \u00b5\u03b2 1(x) \u2272 (A(1) \u2228 A(2) k\nx\u2208K g g ) m,\n\nthen by setting \u03b2 = \u03b21, the derived bound of Ru1,c + Ru2,c (see (C.6) and (C.8)) dominates that of Ru1,e + Ru2,e (see (C.9) and (C.10)), and so Ru1 + Ru2 \u2272 Ru1,c + Ru2,c.\n\nRecall that (C.6) and (C.8) are guaranteed by the sample size of (C.5) and (C.7), while (C.5) and (C.7) hold as long as\n\n$$\nm \u2273 k \\log A(1) \u2227 A(2) n L0 + T + B0 \u03b21 + T (\u221an(U (1) g g )\u03f5+ (A(1) g g ))\n$$\n\nwith probability at least 1 \u2212 m(P (1) + P (2)) \u2212 m exp(\u2212\u2126(n)) \u2212 C exp(\u2212\u2126(k)) we have (C.11)\n\n$$\nRu1,c + Ru2,c \u2272 (A(1) g ) m .\n$$\n\nTherefore, the right-hand side of (3.1) can be uniformly bounded by\n\nO \u2225\u02c6x \u2212 T x\u2217\u22252 \u00b7 (A(1) \u2228 A(2) kL . (C.12)\n\nCombining with the uniform lower bound for the left-hand side of (3.1), i.e., \u2126(\u2225\u02c6 x \u2212 T x\u2217\u22252 2), we obtain the following bound uniformly for all x\u2217:\n\n$$\n\u2225\u02c6 x \u2212 T x\u2217\u22252 \u2272 (A(1) g ) m .\n$$\n\nHence, as long as\n\n$$\nm \u2273 A(1) \u2228 A(2) 2 kL g g \u03f52 ,\n$$\n\nwe again obtain \u2225\u02c6 x \u2212 T x\u2217\u22252 \u2264 3\u03f5, which completes the proof.\n\nC.2 Proof of Theorem 2\n\nProof. Step 1. Control the process over finite nets.\n\nRecall that X and V are the index sets of x and v, as stated in Theorem 2. We first establish the desired concentration for a fixed pair (x, v) \u2208 X \u00d7 V. By Lemma 2, \u2225gx(ai)hv(ai)\u2225\u03c81 \u2264 \u2225gx(ai)\u2225\u03c82\u2225hv(ai)\u2225\u03c82 \u2264 AgAh. Furthermore, centering (Lemma 3) gives\n\n\u2225gx(ai)hv(ai) \u2212 E[gx(ai)hv(ai)]\u2225\u03c81 = O(AgAh).\n\nThus, for fixed (x, v) \u2208 X \u00d7 V we define\n\n$$\nIx,v = \\frac{1}{m} \\sum_{i=1}^{m} gx(ai)hv(ai) - E[gx(ai)hv(ai)].\n$$\n\nThen, we can invoke Bernstein\u2019s inequality (Lemma 4) to obtain for any t \u2265 0 that\n\n$$\nP |Ix,v| \u2265 t \u2264 2 \\exp \\left( -cm \\min(AgAht^2, AgAht) \\right) . (C.13)\n$$"}]}, {"page": 19, "text": "We construct G1 as an \u03b71-net of X                  , and G2 as an \u03b72-net of V, with both nets being minimal in that\n log |G1| = H (X          , \u03b71), log |G2| = H (V, \u03b72), and where \u03b71, \u03b72 are to be chosen later. Then, we take\n a union\uf8ebbound of (C.13) over (x,\uf8f6v) \u2208                   G1 \u00d7 G2 to obtain                                      t      2        t\n     P  \uf8ed   sup    |Ix,v| \u2265     t\uf8f8   \u2264   2 exp      H (X     , \u03b71) + H (V, \u03b72) \u2212           cm min            AgAh         , AgAh           .\n           x\u2208G1\n           v\u2208G2                                                                                                                        (C.14)\n Now we set t \u224d             AgAh      H (X,\u03b71)+H (V,\u03b72)              for a suffi    ciently large hidden constant. Then, if\n                                                     m\n                                                                                              t\n m \u2265     C(H (X       , \u03b71) + H (V, \u03b72)) for large enough C so that                        AgAh \u2264       1 (we assume this now and\n will confirm it in (C.21) after specifying \u03b71, \u03b72), (C.14) gives\n      \uf8eb                                    H (X     , \u03b71) + H (V, \u03b72)         \uf8f6\n   P  \uf8ed   sup    |Ix,v| \u2273     AgAh                         m                  \uf8f8    \u2264  2 exp      \u2212\u2126     H (X     , \u03b71) + H (V, \u03b72)\n          x\u2208G1\n          v\u2208G2\n Hence, from now on we proceed with the proof on the event\n                                      sup    |Ix,v| \u2272     AgAh      H (X        , \u03b71) +mH (V, \u03b72)         ,                            (C.15)\n                                     x\u2208G1\n                                      v\u2208G2\n which holds within the promised probability.\n Step 2. Control the approximation error of the nets.\nWe have derived a bound for sup                 x\u2208G1,v\u2208G2 |Ix,v|, while we want to control I = supx\u2208X,v\u2208V |Ix,v|,\n so we further investigate how close these two quantities are. We define the event as\n                                    E1 =      the events in (3.7) hold for all ai, i \u2208               [m]    ,\n then by assumption (A2.) in the theorem statement, a union bound gives P(E1) \u2265                                                  1 \u2212    mP0.\n In the following, we proceed with the analysis of the event E1. Combining with (C.15) we now\n bound |Ix,v| for any given x \u2208                   X   , v \u2208    V. Specifically, we pick x\u2032 \u2208                   G1, v\u2032 \u2208      G2 such that\n \u2225x\u2032 \u2212    x\u22252 \u2264      \u03b71, \u2225v\u2032 \u2212     v\u22252 \u2264      \u03b72, and thus we have\n  |Ix,v| \u2264     |Ix\u2032,v\u2032|+|Ix,v\u2212Ix\u2032,v\u2032| \u2264              O     AgAh      H (X        , \u03b71) +mH (V, \u03b72)            +|Ix,v\u2212Ix\u2032,v\u2032|. (C.16)\n Moreover, we have\n      |Ix,v \u2212    Ix\u2032,v\u2032|\n       = 1 m      m     gx(ai)hv(ai) \u2212           gx\u2032(ai)hv\u2032(ai)           \u2212   m \u00b7 E      gx(ai)hv(ai) \u2212          gx\u2032(ai)hv\u2032(ai)\n                 i=1\n                m\n       \u2264   m1  i=1   |gx(ai)hv(ai) \u2212  err1     gx\u2032(ai)hv\u2032(ai)|         + E |gx(ai)hv(ai) \u2212        err2 gx\u2032(ai)hv\u2032(ai)|                 (C.17)\nWe bound err1 using the event E1 as follows:\n           err1 \u2264     m1    m    |gx(ai) \u2212       gx\u2032(ai)| \u00b7 |hv(ai)| + |hv(ai) \u2212                hv\u2032(ai)| \u00b7 |gx\u2032(ai)|\n                           i=1\n                  \u2264   m1    m    Lg \u00b7 \u2225x \u2212       x\u2032\u22252 \u00b7 Uh + Lh \u00b7 \u2225v \u2212            v\u2032\u22252 \u00b7 Ug                                            (C.18)\n                           i=1\n                  \u2264   m1    m    LgUh\u03b71 + LhUg\u03b72                = LgUh\u03b71 + LhUg\u03b72.\n                           i=1\n                                                                       19", "md": "We construct \\( G1 \\) as an \\( \\eta_1 \\)-net of \\( X \\), and \\( G2 \\) as an \\( \\eta_2 \\)-net of \\( V \\), with both nets being minimal in that \\( \\log |G1| = H(X, \\eta_1) \\), \\( \\log |G2| = H(V, \\eta_2) \\), and where \\( \\eta_1, \\eta_2 \\) are to be chosen later. Then, we take a union bound of (C.13) over \\( (x,v) \\in G1 \\times G2 \\) to obtain\n\n\\[\nP\\left( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\geq t \\right) \\leq 2 \\exp\\left( H(X, \\eta_1) + H(V, \\eta_2) - \\frac{c}{m} \\min \\{AgAh, AgAh\\} \\right). \\quad (C.14)\n\\]\n\nNow we set \\( t \\approx AgAh \\left( H(X,\\eta_1) + H(V,\\eta_2) \\right) \\) for a sufficiently large hidden constant. Then, if \\( m \\geq C \\left( H(X, \\eta_1) + H(V, \\eta_2) \\right) \\) for large enough \\( C \\) so that \\( AgAh \\leq \\frac{1}{m} \\) (we assume this now and will confirm it in (C.21) after specifying \\( \\eta_1, \\eta_2 \\)), (C.14) gives\n\n\\[\nP\\left( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\gtrsim AgAh \\frac{m}{H(X, \\eta_1) + mH(V, \\eta_2)} \\right) \\leq 2 \\exp\\left( -\\Omega \\left( H(X, \\eta_1) + H(V, \\eta_2) \\right) \\right).\n\\]\n\nHence, from now on we proceed with the proof on the event\n\n\\[\n\\sup_{x \\in G1, v \\in G2} |Ix,v| \\lesssim AgAh \\frac{H(X, \\eta_1) + mH(V, \\eta_2)}{m}, \\quad (C.15)\n\\]\n\nwhich holds within the promised probability.\n\nStep 2. Control the approximation error of the nets.\n\nWe have derived a bound for \\( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\), while we want to control \\( I = \\sup_{x \\in X, v \\in V} |Ix,v| \\), so we further investigate how close these two quantities are. We define the event as\n\n\\[\nE1 = \\text{the events in (3.7) hold for all } ai, i \\in [m],\n\\]\n\nthen by assumption (A2.) in the theorem statement, a union bound gives \\( P(E1) \\geq 1 - mP0 \\). In the following, we proceed with the analysis of the event \\( E1 \\). Combining with (C.15) we now bound \\( |Ix,v| \\) for any given \\( x \\in X \\), \\( v \\in V \\). Specifically, we pick \\( x' \\in G1 \\), \\( v' \\in G2 \\) such that \\( \\|x' - x\\|_2 \\leq \\eta_1 \\), \\( \\|v' - v\\|_2 \\leq \\eta_2 \\), and thus we have\n\n\\[\n|Ix,v| \\leq |Ix',v'| + |Ix,v - Ix',v'| \\leq O(AgAh \\left( H(X, \\eta_1) + mH(V, \\eta_2) \\right) + |Ix,v - Ix',v'|. \\quad (C.16)\n\\]\n\nMoreover, we have\n\n\\[\n\\begin{aligned}\n|Ix,v - Ix',v'|\n&= \\frac{1}{m} \\sum_{i=1}^{m} g_x(ai)h_v(ai) - g_{x'}(ai)h_{v'}(ai) - m \\cdot E \\left[ g_x(ai)h_v(ai) - g_{x'}(ai)h_{v'}(ai) \\right] \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} |g_x(ai)h_v(ai) - \\text{err}_1 g_{x'}(ai)h_{v'}(ai)| + E \\left[ |g_x(ai)h_v(ai) - \\text{err}_2 g_{x'}(ai)h_{v'}(ai)| \\right]. \\quad (C.17)\n\\end{aligned}\n\\]\n\nWe bound \\( \\text{err}_1 \\) using the event \\( E1 \\) as follows:\n\n\\[\n\\begin{aligned}\n\\text{err}_1 &\\leq \\frac{1}{m} \\sum_{i=1}^{m} |g_x(ai) - g_{x'}(ai)| \\cdot |h_v(ai)| + |h_v(ai) - h_{v'}(ai)| \\cdot |g_{x'}(ai)| \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} L_g \\cdot \\|x - x'\\|_2 \\cdot U_h + L_h \\cdot \\|v - v'\\|_2 \\cdot U_g \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} L_gU_h\\eta_1 + L_hU_g\\eta_2 = L_gU_h\\eta_1 + L_hU_g\\eta_2.\n\\end{aligned}\n\\]", "images": [], "items": [{"type": "text", "value": "We construct \\( G1 \\) as an \\( \\eta_1 \\)-net of \\( X \\), and \\( G2 \\) as an \\( \\eta_2 \\)-net of \\( V \\), with both nets being minimal in that \\( \\log |G1| = H(X, \\eta_1) \\), \\( \\log |G2| = H(V, \\eta_2) \\), and where \\( \\eta_1, \\eta_2 \\) are to be chosen later. Then, we take a union bound of (C.13) over \\( (x,v) \\in G1 \\times G2 \\) to obtain\n\n\\[\nP\\left( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\geq t \\right) \\leq 2 \\exp\\left( H(X, \\eta_1) + H(V, \\eta_2) - \\frac{c}{m} \\min \\{AgAh, AgAh\\} \\right). \\quad (C.14)\n\\]\n\nNow we set \\( t \\approx AgAh \\left( H(X,\\eta_1) + H(V,\\eta_2) \\right) \\) for a sufficiently large hidden constant. Then, if \\( m \\geq C \\left( H(X, \\eta_1) + H(V, \\eta_2) \\right) \\) for large enough \\( C \\) so that \\( AgAh \\leq \\frac{1}{m} \\) (we assume this now and will confirm it in (C.21) after specifying \\( \\eta_1, \\eta_2 \\)), (C.14) gives\n\n\\[\nP\\left( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\gtrsim AgAh \\frac{m}{H(X, \\eta_1) + mH(V, \\eta_2)} \\right) \\leq 2 \\exp\\left( -\\Omega \\left( H(X, \\eta_1) + H(V, \\eta_2) \\right) \\right).\n\\]\n\nHence, from now on we proceed with the proof on the event\n\n\\[\n\\sup_{x \\in G1, v \\in G2} |Ix,v| \\lesssim AgAh \\frac{H(X, \\eta_1) + mH(V, \\eta_2)}{m}, \\quad (C.15)\n\\]\n\nwhich holds within the promised probability.\n\nStep 2. Control the approximation error of the nets.\n\nWe have derived a bound for \\( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\), while we want to control \\( I = \\sup_{x \\in X, v \\in V} |Ix,v| \\), so we further investigate how close these two quantities are. We define the event as\n\n\\[\nE1 = \\text{the events in (3.7) hold for all } ai, i \\in [m],\n\\]\n\nthen by assumption (A2.) in the theorem statement, a union bound gives \\( P(E1) \\geq 1 - mP0 \\). In the following, we proceed with the analysis of the event \\( E1 \\). Combining with (C.15) we now bound \\( |Ix,v| \\) for any given \\( x \\in X \\), \\( v \\in V \\). Specifically, we pick \\( x' \\in G1 \\), \\( v' \\in G2 \\) such that \\( \\|x' - x\\|_2 \\leq \\eta_1 \\), \\( \\|v' - v\\|_2 \\leq \\eta_2 \\), and thus we have\n\n\\[", "md": "We construct \\( G1 \\) as an \\( \\eta_1 \\)-net of \\( X \\), and \\( G2 \\) as an \\( \\eta_2 \\)-net of \\( V \\), with both nets being minimal in that \\( \\log |G1| = H(X, \\eta_1) \\), \\( \\log |G2| = H(V, \\eta_2) \\), and where \\( \\eta_1, \\eta_2 \\) are to be chosen later. Then, we take a union bound of (C.13) over \\( (x,v) \\in G1 \\times G2 \\) to obtain\n\n\\[\nP\\left( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\geq t \\right) \\leq 2 \\exp\\left( H(X, \\eta_1) + H(V, \\eta_2) - \\frac{c}{m} \\min \\{AgAh, AgAh\\} \\right). \\quad (C.14)\n\\]\n\nNow we set \\( t \\approx AgAh \\left( H(X,\\eta_1) + H(V,\\eta_2) \\right) \\) for a sufficiently large hidden constant. Then, if \\( m \\geq C \\left( H(X, \\eta_1) + H(V, \\eta_2) \\right) \\) for large enough \\( C \\) so that \\( AgAh \\leq \\frac{1}{m} \\) (we assume this now and will confirm it in (C.21) after specifying \\( \\eta_1, \\eta_2 \\)), (C.14) gives\n\n\\[\nP\\left( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\gtrsim AgAh \\frac{m}{H(X, \\eta_1) + mH(V, \\eta_2)} \\right) \\leq 2 \\exp\\left( -\\Omega \\left( H(X, \\eta_1) + H(V, \\eta_2) \\right) \\right).\n\\]\n\nHence, from now on we proceed with the proof on the event\n\n\\[\n\\sup_{x \\in G1, v \\in G2} |Ix,v| \\lesssim AgAh \\frac{H(X, \\eta_1) + mH(V, \\eta_2)}{m}, \\quad (C.15)\n\\]\n\nwhich holds within the promised probability.\n\nStep 2. Control the approximation error of the nets.\n\nWe have derived a bound for \\( \\sup_{x \\in G1, v \\in G2} |Ix,v| \\), while we want to control \\( I = \\sup_{x \\in X, v \\in V} |Ix,v| \\), so we further investigate how close these two quantities are. We define the event as\n\n\\[\nE1 = \\text{the events in (3.7) hold for all } ai, i \\in [m],\n\\]\n\nthen by assumption (A2.) in the theorem statement, a union bound gives \\( P(E1) \\geq 1 - mP0 \\). In the following, we proceed with the analysis of the event \\( E1 \\). Combining with (C.15) we now bound \\( |Ix,v| \\) for any given \\( x \\in X \\), \\( v \\in V \\). Specifically, we pick \\( x' \\in G1 \\), \\( v' \\in G2 \\) such that \\( \\|x' - x\\|_2 \\leq \\eta_1 \\), \\( \\|v' - v\\|_2 \\leq \\eta_2 \\), and thus we have\n\n\\["}, {"type": "table", "rows": [["Ix,v", "\\leq", "Ix',v'", "+", "Ix,v - Ix',v'", "\\leq O(AgAh \\left( H(X, \\eta_1) + mH(V, \\eta_2) \\right) +", "Ix,v - Ix',v'"]], "md": "|Ix,v| \\leq |Ix',v'| + |Ix,v - Ix',v'| \\leq O(AgAh \\left( H(X, \\eta_1) + mH(V, \\eta_2) \\right) + |Ix,v - Ix',v'|. \\quad (C.16)", "isPerfectTable": true, "csv": "\"Ix,v\",\"\\leq\",\"Ix',v'\",\"+\",\"Ix,v - Ix',v'\",\"\\leq O(AgAh \\left( H(X, \\eta_1) + mH(V, \\eta_2) \\right) +\",\"Ix,v - Ix',v'\""}, {"type": "text", "value": "\\]\n\nMoreover, we have\n\n\\[\n\\begin{aligned}", "md": "\\]\n\nMoreover, we have\n\n\\[\n\\begin{aligned}"}, {"type": "table", "rows": [["Ix,v - Ix',v'"]], "md": "|Ix,v - Ix',v'|", "isPerfectTable": true, "csv": "\"Ix,v - Ix',v'\""}, {"type": "text", "value": "&= \\frac{1}{m} \\sum_{i=1}^{m} g_x(ai)h_v(ai) - g_{x'}(ai)h_{v'}(ai) - m \\cdot E \\left[ g_x(ai)h_v(ai) - g_{x'}(ai)h_{v'}(ai) \\right] \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} |g_x(ai)h_v(ai) - \\text{err}_1 g_{x'}(ai)h_{v'}(ai)| + E \\left[ |g_x(ai)h_v(ai) - \\text{err}_2 g_{x'}(ai)h_{v'}(ai)| \\right]. \\quad (C.17)\n\\end{aligned}\n\\]\n\nWe bound \\( \\text{err}_1 \\) using the event \\( E1 \\) as follows:\n\n\\[\n\\begin{aligned}\n\\text{err}_1 &\\leq \\frac{1}{m} \\sum_{i=1}^{m} |g_x(ai) - g_{x'}(ai)| \\cdot |h_v(ai)| + |h_v(ai) - h_{v'}(ai)| \\cdot |g_{x'}(ai)| \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} L_g \\cdot \\|x - x'\\|_2 \\cdot U_h + L_h \\cdot \\|v - v'\\|_2 \\cdot U_g \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} L_gU_h\\eta_1 + L_hU_g\\eta_2 = L_gU_h\\eta_1 + L_hU_g\\eta_2.\n\\end{aligned}\n\\]", "md": "&= \\frac{1}{m} \\sum_{i=1}^{m} g_x(ai)h_v(ai) - g_{x'}(ai)h_{v'}(ai) - m \\cdot E \\left[ g_x(ai)h_v(ai) - g_{x'}(ai)h_{v'}(ai) \\right] \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} |g_x(ai)h_v(ai) - \\text{err}_1 g_{x'}(ai)h_{v'}(ai)| + E \\left[ |g_x(ai)h_v(ai) - \\text{err}_2 g_{x'}(ai)h_{v'}(ai)| \\right]. \\quad (C.17)\n\\end{aligned}\n\\]\n\nWe bound \\( \\text{err}_1 \\) using the event \\( E1 \\) as follows:\n\n\\[\n\\begin{aligned}\n\\text{err}_1 &\\leq \\frac{1}{m} \\sum_{i=1}^{m} |g_x(ai) - g_{x'}(ai)| \\cdot |h_v(ai)| + |h_v(ai) - h_{v'}(ai)| \\cdot |g_{x'}(ai)| \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} L_g \\cdot \\|x - x'\\|_2 \\cdot U_h + L_h \\cdot \\|v - v'\\|_2 \\cdot U_g \\\\\n&\\leq \\frac{1}{m} \\sum_{i=1}^{m} L_gU_h\\eta_1 + L_hU_g\\eta_2 = L_gU_h\\eta_1 + L_hU_g\\eta_2.\n\\end{aligned}\n\\]"}]}, {"page": 20, "text": "On the other hand, we bound err2 using assumption (A1.)in the theorem statement. Noting that\nE|X| = O(\u2225X\u2225\u03c81) [50, Proposition 2.7.1(b)], and further applying Lemma 2 we obtain\n            err2 \u2272     \u2225gx(ai)hv(ai) \u2212            gx\u2032(ai)hv\u2032(ai)\u2225\u03c81\n             \u2264   \u2225(gx(ai) \u2212        gx\u2032(ai))hv(ai)\u2225\u03c81 + \u2225gx\u2032(ai)(hv(ai) \u2212                          hv\u2032(ai))\u2225\u03c81                            (C.19)\n             \u2264   \u2225gx(ai) \u2212        gx\u2032(ai)\u2225\u03c82\u2225hv(ai)\u2225\u03c82 + \u2225gx\u2032(ai)\u2225\u03c82\u2225hv(ai) \u2212                               hv\u2032(ai)\u2225\u03c82\n             \u2264   Mg \u00b7 \u2225x \u2212        x\u2032\u22252 \u00b7 Ah + Ag \u00b7 Mh \u00b7 \u2225v \u2212                v\u2032\u22252 \u2264      MgAh\u03b71 + AgMh\u03b72.\nNote that the bounds (C.18) and (C.19) hold uniformly for all (x, v) \u2208                                    X \u00d7 V, and hence, we can\nsubstitute them into (C.16) and (C.17) to obtain\n sup   |Ix,v| \u2264     O      AgAh      H (X        , \u03b71) + H (V, \u03b72)             + (LgUh + MgAh)\u03b71 + (LhUg + MhAg)\u03b72.\nx\u2208X                                                     m\nv\u2208V                                                                                                                                      (C.20)\nRecall that we use the shorthand Sg,h := LgUh + MgAh and Tg,h := LhUg + MhAg. We set\n           AgAh                   AgAh\n\u03b71 \u224d      \u221a mSg,h , \u03b72 \u224d         \u221amTg,h so that the right-hand side of (C.20) is dominated by the first term.\nOverall, with a sample size satisfying\n                                                             \u221a  mSg,h                       \u221amTg,h\n                                  m = \u2126          H     X   , AgAh            + H       V, AgAh                 ,                         (C.21)\nwe can bound I = sup              x\u2208X supv\u2208V |Ix,v| (defined in the theorem statement) as\n                       I \u2272    AgAh      H (X        , AgAhm\u22121/2S\u22121         g,h) +mH (V, AgAhm\u22121/2T \u22121                g,h)                (C.22)\nwith probability at least\n                                                                       \u221a  mSg,h                        \u221amTg,h\n                      1 \u2212   mP0 \u2212       2 exp     \u2212\u2126       H      X  , AgAh             + H       V, AgAh                  .\nThis completes the proof.\nD       Other Omitted Proofs\nD.1      Proof of Lemma 1 (Lipschitz continuity of fi,\u03b2 and \u03b5i,\u03b2).\nProof. It is straightforward to check that fi,\u03b2 and |\u03b5i,\u03b2| are piece-wise continuous functions; hence,\nit suffices to prove that they are Lipschitz with the claimed Lipschitz constant over each piece.\nIn any interval contained in the part of x /                     \u2208   Dfi + [\u2212\u03b22 , \u03b22 ], fi,\u03b2 = fi and |\u03b5i,\u03b2| = 0 trivially\nsatisfy the claim. In any interval contained in [x0 \u2212                          \u03b22 , x0] for some x0 \u2208           Dfi, fi,\u03b2 is linear with\nslope 2       fia(x0) \u2212      fi(x0 \u2212       \u03b2    , combined with the bound |f a               i (x0) \u2212     fi(x0 \u2212       \u03b2            i (x0) \u2212\n          \u03b2                                2 )                                                                          2 )| \u2264    |f a\nf \u2212                                                         i (x0)\u2212f \u2212 i (x0)|   + L0\u03b2      \u2264   1   B0 + L0\u03b2         , we know that fi,\u03b2\n  i (x0)| + |f \u2212    i (x0) \u2212      fi(x0 \u2212      \u03b22 )| \u2264   |f +       2                  2        2\nis   L0 + B0    \u03b2   -Lipschitz. Further, |\u03b5i,\u03b2| = |fi,\u03b2 \u2212                  fi|, and fi is L0-Lipschitz over this interval, so\n|\u03b5i,\u03b2| is     2L0 + B0    \u03b2    -Lipschitz continuous. A similar argument applies to an interval contained in\n[x0, x0 + \u03b2    2 ].\nD.2      Proof of Lemma 6 (Metric entropy of constraint sets).\nProof. Bounding H (K, \u03b7).\nBy [50, Corollary 4.2.13], there exists an                      \u03b7   -net G1 of Bk      2 such that\n                                                               Lr\n                                          log |G1| \u2264      k log    2Lr \u03b7    + 1     \u2264   k log 3Lr  \u03b7 ,\nwhere we use \u03b7 \u2264            Lr. Note that rG1 is an              \u03b7   -net of Bk   2(r), and because G is L-Lipschitz, G(rG1)\n                                                                 L\nis an \u03b7-net of K, thus yielding H (K, \u03b7) \u2264                      k log 3Lr \u03b7 .\n                                                                       20", "md": "On the other hand, we bound err2 using assumption (A1.) in the theorem statement. Noting that\nE|X| = O(\u2225X\u2225\u03c81) [50, Proposition 2.7.1(b)], and further applying Lemma 2 we obtain\n\n$$\n\\begin{aligned}\n\\text{err2} &\\lesssim \\left\\| gx(a_i)hv(a_i) - gx'(a_i)hv'(a_i) \\right\\|_{\\psi_1} \\\\\n&\\leq \\left\\| (gx(a_i) - gx'(a_i))hv(a_i) \\right\\|_{\\psi_1} + \\left\\| gx'(a_i)(hv(a_i) - hv'(a_i)) \\right\\|_{\\psi_1} \\quad \\quad \\quad (C.19) \\\\\n&\\leq \\left\\| gx(a_i) - gx'(a_i) \\right\\|_{\\psi_2} \\left\\| hv(a_i) \\right\\|_{\\psi_2} + \\left\\| gx'(a_i) \\right\\|_{\\psi_2} \\left\\| hv(a_i) - hv'(a_i) \\right\\|_{\\psi_2} \\\\\n&\\leq Mg \\cdot \\left\\| x - x' \\right\\|_2 \\cdot Ah + Ag \\cdot Mh \\cdot \\left\\| v - v' \\right\\|_2 \\leq MgAh\\eta_1 + AgMh\\eta_2.\n\\end{aligned}\n$$\n\nNote that the bounds (C.18) and (C.19) hold uniformly for all $(x, v) \\in X \\times V$, and hence, we can substitute them into (C.16) and (C.17) to obtain\n\n$$\n\\sup_{x\\in X} \\sup_{v\\in V} |Ix,v| \\leq O(AgAhH(X, \\eta_1) + H(V, \\eta_2) + (LgUh + MgAh)\\eta_1 + (LhUg + MhAg)\\eta_2) \\quad \\quad \\quad (C.20)\n$$\n\nRecall that we use the shorthand $S_{g,h} := LgUh + MgAh$ and $T_{g,h} := LhUg + MhAg$. We set\n$$\n\\eta_1 \\asymp \\sqrt{mS_{g,h}}, \\quad \\eta_2 \\asymp \\sqrt{mT_{g,h}}\n$$\nso that the right-hand side of (C.20) is dominated by the first term. Overall, with a sample size satisfying\n\n$$\nm = \\Omega\\left( H(X, AgAh) + H(V, AgAh) \\right) \\quad \\quad \\quad (C.21)\n$$\n\nwe can bound $I = \\sup_{x\\in X} \\sup_{v\\in V} |Ix,v|$ (defined in the theorem statement) as\n\n$$\nI \\lesssim AgAhH(X, AgAhm^{-1/2}S_{g,h}^{-1}) + mH(V, AgAhm^{-1/2}T_{g,h}^{-1}) \\quad \\quad \\quad (C.22)\n$$\n\nwith probability at least\n\n$$\n1 - mP_0 - 2\\exp\\left( -\\Omega\\left( H(X, AgAh) + H(V, AgAh) \\right) \\right).\n$$\n\nThis completes the proof.\n\n### Other Omitted Proofs\n\n#### Proof of Lemma 1 (Lipschitz continuity of fi,\u03b2 and \u03b5i,\u03b2)\nProof. It is straightforward to check that $fi,\\beta$ and $|\\epsilon_i,\\beta|$ are piece-wise continuous functions; hence, it suffices to prove that they are Lipschitz with the claimed Lipschitz constant over each piece. In any interval contained in the part of $x \\notin D_{fi} + [-\\beta/2, \\beta/2]$, $fi,\\beta = fi$ and $|\\epsilon_i,\\beta| = 0$ trivially satisfy the claim. In any interval contained in $[x_0 - \\beta/2, x_0]$ for some $x_0 \\in D_{fi}$, $fi,\\beta$ is linear with slope $2(f_{ia}(x_0) - fi(x_0 - \\beta))$, combined with the bound $|f_{ia}(x_0) - fi(x_0 - \\beta)| \\leq |f_{ia}(x_0) - fi(x_0)| + L_0\\beta \\leq 1 + B_0 + L_0\\beta$, we know that $fi,\\beta$ is $L_0 + B_0\\beta$-Lipschitz. Further, $|\\epsilon_i,\\beta| = |fi,\\beta - fi|$, and $fi$ is $L_0$-Lipschitz over this interval, so $|\\epsilon_i,\\beta|$ is $2L_0 + B_0\\beta$-Lipschitz continuous. A similar argument applies to an interval contained in $[x_0, x_0 + \\beta/2]$.\n\n#### Proof of Lemma 6 (Metric entropy of constraint sets)\nProof. Bounding $H(K, \\eta)$. By [50, Corollary 4.2.13], there exists an $\\eta$-net $G_1$ of $B_k^2$ such that\n\n$$\n\\log |G_1| \\leq k \\log 2L_r \\eta + 1 \\leq k \\log 3L_r \\eta,\n$$\n\nwhere we use $\\eta \\leq L_r$. Note that $rG_1$ is an $\\eta$-net of $B_k^{2r}$, and because $G$ is $L$-Lipschitz, $G(rG_1)$ is an $\\eta$-net of $K$, thus yielding $H(K, \\eta) \\leq k \\log 3L_r \\eta$.", "images": [], "items": [{"type": "text", "value": "On the other hand, we bound err2 using assumption (A1.) in the theorem statement. Noting that\nE|X| = O(\u2225X\u2225\u03c81) [50, Proposition 2.7.1(b)], and further applying Lemma 2 we obtain\n\n$$\n\\begin{aligned}\n\\text{err2} &\\lesssim \\left\\| gx(a_i)hv(a_i) - gx'(a_i)hv'(a_i) \\right\\|_{\\psi_1} \\\\\n&\\leq \\left\\| (gx(a_i) - gx'(a_i))hv(a_i) \\right\\|_{\\psi_1} + \\left\\| gx'(a_i)(hv(a_i) - hv'(a_i)) \\right\\|_{\\psi_1} \\quad \\quad \\quad (C.19) \\\\\n&\\leq \\left\\| gx(a_i) - gx'(a_i) \\right\\|_{\\psi_2} \\left\\| hv(a_i) \\right\\|_{\\psi_2} + \\left\\| gx'(a_i) \\right\\|_{\\psi_2} \\left\\| hv(a_i) - hv'(a_i) \\right\\|_{\\psi_2} \\\\\n&\\leq Mg \\cdot \\left\\| x - x' \\right\\|_2 \\cdot Ah + Ag \\cdot Mh \\cdot \\left\\| v - v' \\right\\|_2 \\leq MgAh\\eta_1 + AgMh\\eta_2.\n\\end{aligned}\n$$\n\nNote that the bounds (C.18) and (C.19) hold uniformly for all $(x, v) \\in X \\times V$, and hence, we can substitute them into (C.16) and (C.17) to obtain\n\n$$\n\\sup_{x\\in X} \\sup_{v\\in V} |Ix,v| \\leq O(AgAhH(X, \\eta_1) + H(V, \\eta_2) + (LgUh + MgAh)\\eta_1 + (LhUg + MhAg)\\eta_2) \\quad \\quad \\quad (C.20)\n$$\n\nRecall that we use the shorthand $S_{g,h} := LgUh + MgAh$ and $T_{g,h} := LhUg + MhAg$. We set\n$$\n\\eta_1 \\asymp \\sqrt{mS_{g,h}}, \\quad \\eta_2 \\asymp \\sqrt{mT_{g,h}}\n$$\nso that the right-hand side of (C.20) is dominated by the first term. Overall, with a sample size satisfying\n\n$$\nm = \\Omega\\left( H(X, AgAh) + H(V, AgAh) \\right) \\quad \\quad \\quad (C.21)\n$$\n\nwe can bound $I = \\sup_{x\\in X} \\sup_{v\\in V} |Ix,v|$ (defined in the theorem statement) as\n\n$$\nI \\lesssim AgAhH(X, AgAhm^{-1/2}S_{g,h}^{-1}) + mH(V, AgAhm^{-1/2}T_{g,h}^{-1}) \\quad \\quad \\quad (C.22)\n$$\n\nwith probability at least\n\n$$\n1 - mP_0 - 2\\exp\\left( -\\Omega\\left( H(X, AgAh) + H(V, AgAh) \\right) \\right).\n$$\n\nThis completes the proof.", "md": "On the other hand, we bound err2 using assumption (A1.) in the theorem statement. Noting that\nE|X| = O(\u2225X\u2225\u03c81) [50, Proposition 2.7.1(b)], and further applying Lemma 2 we obtain\n\n$$\n\\begin{aligned}\n\\text{err2} &\\lesssim \\left\\| gx(a_i)hv(a_i) - gx'(a_i)hv'(a_i) \\right\\|_{\\psi_1} \\\\\n&\\leq \\left\\| (gx(a_i) - gx'(a_i))hv(a_i) \\right\\|_{\\psi_1} + \\left\\| gx'(a_i)(hv(a_i) - hv'(a_i)) \\right\\|_{\\psi_1} \\quad \\quad \\quad (C.19) \\\\\n&\\leq \\left\\| gx(a_i) - gx'(a_i) \\right\\|_{\\psi_2} \\left\\| hv(a_i) \\right\\|_{\\psi_2} + \\left\\| gx'(a_i) \\right\\|_{\\psi_2} \\left\\| hv(a_i) - hv'(a_i) \\right\\|_{\\psi_2} \\\\\n&\\leq Mg \\cdot \\left\\| x - x' \\right\\|_2 \\cdot Ah + Ag \\cdot Mh \\cdot \\left\\| v - v' \\right\\|_2 \\leq MgAh\\eta_1 + AgMh\\eta_2.\n\\end{aligned}\n$$\n\nNote that the bounds (C.18) and (C.19) hold uniformly for all $(x, v) \\in X \\times V$, and hence, we can substitute them into (C.16) and (C.17) to obtain\n\n$$\n\\sup_{x\\in X} \\sup_{v\\in V} |Ix,v| \\leq O(AgAhH(X, \\eta_1) + H(V, \\eta_2) + (LgUh + MgAh)\\eta_1 + (LhUg + MhAg)\\eta_2) \\quad \\quad \\quad (C.20)\n$$\n\nRecall that we use the shorthand $S_{g,h} := LgUh + MgAh$ and $T_{g,h} := LhUg + MhAg$. We set\n$$\n\\eta_1 \\asymp \\sqrt{mS_{g,h}}, \\quad \\eta_2 \\asymp \\sqrt{mT_{g,h}}\n$$\nso that the right-hand side of (C.20) is dominated by the first term. Overall, with a sample size satisfying\n\n$$\nm = \\Omega\\left( H(X, AgAh) + H(V, AgAh) \\right) \\quad \\quad \\quad (C.21)\n$$\n\nwe can bound $I = \\sup_{x\\in X} \\sup_{v\\in V} |Ix,v|$ (defined in the theorem statement) as\n\n$$\nI \\lesssim AgAhH(X, AgAhm^{-1/2}S_{g,h}^{-1}) + mH(V, AgAhm^{-1/2}T_{g,h}^{-1}) \\quad \\quad \\quad (C.22)\n$$\n\nwith probability at least\n\n$$\n1 - mP_0 - 2\\exp\\left( -\\Omega\\left( H(X, AgAh) + H(V, AgAh) \\right) \\right).\n$$\n\nThis completes the proof."}, {"type": "heading", "lvl": 3, "value": "Other Omitted Proofs", "md": "### Other Omitted Proofs"}, {"type": "heading", "lvl": 4, "value": "Proof of Lemma 1 (Lipschitz continuity of fi,\u03b2 and \u03b5i,\u03b2)", "md": "#### Proof of Lemma 1 (Lipschitz continuity of fi,\u03b2 and \u03b5i,\u03b2)"}, {"type": "text", "value": "Proof. It is straightforward to check that $fi,\\beta$ and $|\\epsilon_i,\\beta|$ are piece-wise continuous functions; hence, it suffices to prove that they are Lipschitz with the claimed Lipschitz constant over each piece. In any interval contained in the part of $x \\notin D_{fi} + [-\\beta/2, \\beta/2]$, $fi,\\beta = fi$ and $|\\epsilon_i,\\beta| = 0$ trivially satisfy the claim. In any interval contained in $[x_0 - \\beta/2, x_0]$ for some $x_0 \\in D_{fi}$, $fi,\\beta$ is linear with slope $2(f_{ia}(x_0) - fi(x_0 - \\beta))$, combined with the bound $|f_{ia}(x_0) - fi(x_0 - \\beta)| \\leq |f_{ia}(x_0) - fi(x_0)| + L_0\\beta \\leq 1 + B_0 + L_0\\beta$, we know that $fi,\\beta$ is $L_0 + B_0\\beta$-Lipschitz. Further, $|\\epsilon_i,\\beta| = |fi,\\beta - fi|$, and $fi$ is $L_0$-Lipschitz over this interval, so $|\\epsilon_i,\\beta|$ is $2L_0 + B_0\\beta$-Lipschitz continuous. A similar argument applies to an interval contained in $[x_0, x_0 + \\beta/2]$.", "md": "Proof. It is straightforward to check that $fi,\\beta$ and $|\\epsilon_i,\\beta|$ are piece-wise continuous functions; hence, it suffices to prove that they are Lipschitz with the claimed Lipschitz constant over each piece. In any interval contained in the part of $x \\notin D_{fi} + [-\\beta/2, \\beta/2]$, $fi,\\beta = fi$ and $|\\epsilon_i,\\beta| = 0$ trivially satisfy the claim. In any interval contained in $[x_0 - \\beta/2, x_0]$ for some $x_0 \\in D_{fi}$, $fi,\\beta$ is linear with slope $2(f_{ia}(x_0) - fi(x_0 - \\beta))$, combined with the bound $|f_{ia}(x_0) - fi(x_0 - \\beta)| \\leq |f_{ia}(x_0) - fi(x_0)| + L_0\\beta \\leq 1 + B_0 + L_0\\beta$, we know that $fi,\\beta$ is $L_0 + B_0\\beta$-Lipschitz. Further, $|\\epsilon_i,\\beta| = |fi,\\beta - fi|$, and $fi$ is $L_0$-Lipschitz over this interval, so $|\\epsilon_i,\\beta|$ is $2L_0 + B_0\\beta$-Lipschitz continuous. A similar argument applies to an interval contained in $[x_0, x_0 + \\beta/2]$."}, {"type": "heading", "lvl": 4, "value": "Proof of Lemma 6 (Metric entropy of constraint sets)", "md": "#### Proof of Lemma 6 (Metric entropy of constraint sets)"}, {"type": "text", "value": "Proof. Bounding $H(K, \\eta)$. By [50, Corollary 4.2.13], there exists an $\\eta$-net $G_1$ of $B_k^2$ such that\n\n$$\n\\log |G_1| \\leq k \\log 2L_r \\eta + 1 \\leq k \\log 3L_r \\eta,\n$$\n\nwhere we use $\\eta \\leq L_r$. Note that $rG_1$ is an $\\eta$-net of $B_k^{2r}$, and because $G$ is $L$-Lipschitz, $G(rG_1)$ is an $\\eta$-net of $K$, thus yielding $H(K, \\eta) \\leq k \\log 3L_r \\eta$.", "md": "Proof. Bounding $H(K, \\eta)$. By [50, Corollary 4.2.13], there exists an $\\eta$-net $G_1$ of $B_k^2$ such that\n\n$$\n\\log |G_1| \\leq k \\log 2L_r \\eta + 1 \\leq k \\log 3L_r \\eta,\n$$\n\nwhere we use $\\eta \\leq L_r$. Note that $rG_1$ is an $\\eta$-net of $B_k^{2r}$, and because $G$ is $L$-Lipschitz, $G(rG_1)$ is an $\\eta$-net of $K$, thus yielding $H(K, \\eta) \\leq k \\log 3L_r \\eta$."}]}, {"page": 21, "text": "Bounding H (K\u2212, \u03b7) and H (K\u2212                          \u03f5 , \u03b7).\nWe construct G2 as an                  \u03b72   -net of K satisfying log |G2| \u2264                      k log 6Lr  \u03b7 . Then, it is easy to see that\nG2 \u2212     G2 is an \u03b7-net of K\u2212              = K \u2212       K, showing that\n                                                H (K\u2212, \u03b7) \u2264             log |G2|2 \u2264        2k log 6Lr   \u03b7 .\nFor a given T > 0, this directly implies H (T                                K\u2212, \u03b7) \u2264         2k log 6T Lr  \u03b7    . Moreover, because K\u2212                \u03f5 \u2282\nT  K\u2212, by [50, Exercise 4.2.10] (which states that H (K1, r) \u2264                                          H (K2, r      2) holds for any r > 0 if\nK1 \u2282      K2) we obtain                  H (K\u2212     \u03f5 , \u03b7) \u2264     H       T  K\u2212, \u03b7   2     \u2264   2k log 12T      \u03b7 Lr   .\nBounding H              (K\u2212  \u03f5 )\u2217, \u03b7     .\nWe construct G3 as an                \u03f5\u03b7   -net of K\u2212     \u03f5 satisfying log |G3| \u2264              2k log 12T Lr \u03f5\u03b7     , then we consider (G3)\u2217                :=\n{    z                                                                                               \u03f5 )\u2217. Note that any x1 \u2208                (K\u2212  \u03f5 )\u2217   can\n  \u2225z\u22252 : z \u2208        G3}. We aim to prove that (G3)\u2217                       is an \u03b7-net of (K\u2212\n                        z1\nbe written as         \u2225z1\u22252 for some z1 \u2208              K\u2212 \u03f5 and recall that \u2225z1\u22252 \u2265              z2  2\u03f5. Moreover, by construction, there\nexists some z2 \u2208            G3 such that \u2225z1 \u2212              z2\u22252 \u2264       \u03f5\u03b7. Note that         \u2225z2\u22252 \u2208       (G3)\u2217, and moreover we have\n                               z1      \u2212       z2          \u2264         z1      \u2212       z2          +        z2       \u2212      z2\n                            \u2225z1\u22252           \u2225z2\u22252       2         \u2225z1\u22252           \u2225z1\u22252       2         \u2225z1\u22252          \u2225z2\u22252        2\n                                                           = \u2225z1 \u2212  \u2225z1\u22252  z2\u22252     + |\u2225z2\u22252 \u2212   \u2225z1\u22252  \u2225z1\u22252|\n                                                           \u2264    2\u2225z1 \u2212       z2\u22252     \u2264    2\u03f5\u03b7\nHence, we obtain                                                     \u2225z1\u22252                  2\u03f5 = \u03b7.\nwhich completes the proof.       H      (K\u2212 \u03f5 )\u2217, \u03b7      \u2264    log |(G3)\u2217| \u2264         log |G3| \u2264        2k log 12T     \u03f5\u03b7 Lr   ,\nD.3       Proof of Lemma 8 (Choice of T in 1-bit GCS).\nProof. Since x \u2208             Sn\u22121, for some orthogonal matrix P we have P x = e1 (the first column of In).\nSince \u02dc   a := P a = [\u02dc        ai] has the same distribution as a, we have\n                              E[sign(a\u22a4x)a] = E[sign(\u02dc   = P \u22a4          2 a\u22a4e1)P \u22a4\u02dc      2 a] = P \u22a4E[sign(\u02dc            a1)\u02dc  a]\n                                                                        \u03c0 e1 =           \u03c0 x.\nD.4       Proof of Lemma 9 (Choice of T in 1-bit GCS with dithering).\nProof. We first note that\n           E[sign(a\u22a4x + \u03c4)a] \u2212                    x                  sup        E    \u03bb \u00b7 sign(a\u22a4x + \u03c4)a\u22a4v                    \u2212   x\u22a4v        .          (D.1)\n                                                  \u03bb     2 = 1  \u03bb   v\u2208Sn\u22121\nWe first fix a and expect over \u03c4 \u223c                      U [\u2212\u03bb, \u03bb] to obtain\n              E  \u03c4   \u03bb sign(a\u22a4x + \u03c4)a\u22a4v                                                                 \u03bb + a\u22a4x\n           =(\u03bba\u22a4v)            1(|a\u22a4x| > \u03bb) sign(a\u22a4x) + 1(|a\u22a4x| \u2264                                 \u03bb) \u00b7                      \u2212   \u03bb \u2212    a\u22a4x\n                                                                                                                2\u03bb                   2\u03bb\n           =(a\u22a4x)(a\u22a4v)1(|a\u22a4x| \u2264                        \u03bb) + (\u03bba\u22a4v) sign(a\u22a4x)1(|a\u22a4x| > \u03bb).\n                                                                             21", "md": "# Math Equations\n\nBounding $$H(K^-, \\eta)$$ and $$H(K^-_\\epsilon, \\eta)$$. We construct $$G2$$ as an $$\\eta^2$$-net of $$K$$ satisfying $$\\log |G2| \\leq k \\log 6Lr \\eta$$. Then, it is easy to see that $$G2 \\setminus G2$$ is an $$\\eta$$-net of $$K^- = K^- \\setminus K$$, showing that $$H(K^-, \\eta) \\leq \\log |G2|^2 \\leq 2k \\log 6Lr \\eta$$. For a given $$T > 0$$, this directly implies $$H(T K^-, \\eta) \\leq 2k \\log 6T Lr \\eta$$. Moreover, because $$K^-_\\epsilon \\subset T K^-$$, by [50, Exercise 4.2.10] (which states that $$H(K1, r) \\leq H(K2, r^2)$$ holds for any $$r > 0$$ if $$K1 \\subset K2$$) we obtain $$H(K^-_\\epsilon, \\eta) \\leq H(T K^-, \\eta)^2 \\leq 2k \\log 12T \\eta Lr$$. Bounding $$H(K^-_\\epsilon)^*, \\eta$$. We construct $$G3$$ as an $$\\epsilon \\eta$$-net of $$K^-_\\epsilon$$ satisfying $$\\log |G3| \\leq 2k \\log 12T Lr \\epsilon \\eta$$, then we consider $$(G3)^* := \\{z_\\epsilon)^* : z \\in G3\\}$$. Note that any $$x1 \\in (K^-_\\epsilon)^*$$ can be written as $$\\|z1\\|^2$$ for some $$z1 \\in K^-_\\epsilon$$ and recall that $$\\|z1\\|^2 \\geq z2^2\\epsilon$$. Moreover, by construction, there exists some $$z2 \\in G3$$ such that $$\\|z1 - z2\\|^2 \\leq \\epsilon \\eta$$. Note that $$\\|z2\\|^2 \\in (G3)^*$$, and moreover we have $$\\|z1 - z2\\|^2 \\leq \\|z1\\|^2 + \\|z2\\|^2 \\leq 2\\|z1 - z2\\| \\leq 2\\epsilon \\eta$$. Hence, we obtain $$\\|z1\\|^2 \\leq 2\\epsilon = \\eta$$, which completes the proof. $$H(K^-_\\epsilon)^*, \\eta \\leq \\log |(G3)^*| \\leq \\log |G3| \\leq 2k \\log 12T \\epsilon \\eta Lr$$.\n\nProof of Lemma 8 (Choice of $$T$$ in 1-bit GCS). Since $$x \\in S^{n-1}$$, for some orthogonal matrix $$P$$ we have $$Px = e1$$ (the first column of $$I_n$$). Since $$\\tilde{a} := Pa = [\\tilde{a}_i]$$ has the same distribution as $$a$$, we have $$E[\\text{sign}(a^Tx)a] = E[\\text{sign}(\\tilde{a}^Te_1)\\tilde{a}] = P^TE[\\text{sign}(\\tilde{a}_1)\\tilde{a}] = P^T\\pi e1 = \\pi x$$.\n\nProof of Lemma 9 (Choice of $$T$$ in 1-bit GCS with dithering). We first note that $$E[\\text{sign}(a^Tx + \\tau)a] - x \\leq \\sup_{\\lambda > 0} E[\\lambda \\cdot \\text{sign}(a^Tx + \\tau)a^Tv - x^Tv]$$. We first fix $$a$$ and expect over $$\\tau \\sim U[-\\lambda, \\lambda]$$ to obtain $$E[\\tau \\lambda \\text{sign}(a^Tx + \\tau)a^Tv] = (\\lambda a^Tv)1(|a^Tx| > \\lambda) \\text{sign}(a^Tx) + 1(|a^Tx| \\leq \\lambda) \\cdot \\frac{\\lambda + a^Tx}{2\\lambda} = (a^Tx)(a^Tv)1(|a^Tx| \\leq \\lambda) + (\\lambda a^Tv) \\text{sign}(a^Tx)1(|a^Tx| > \\lambda)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Bounding $$H(K^-, \\eta)$$ and $$H(K^-_\\epsilon, \\eta)$$. We construct $$G2$$ as an $$\\eta^2$$-net of $$K$$ satisfying $$\\log |G2| \\leq k \\log 6Lr \\eta$$. Then, it is easy to see that $$G2 \\setminus G2$$ is an $$\\eta$$-net of $$K^- = K^- \\setminus K$$, showing that $$H(K^-, \\eta) \\leq \\log |G2|^2 \\leq 2k \\log 6Lr \\eta$$. For a given $$T > 0$$, this directly implies $$H(T K^-, \\eta) \\leq 2k \\log 6T Lr \\eta$$. Moreover, because $$K^-_\\epsilon \\subset T K^-$$, by [50, Exercise 4.2.10] (which states that $$H(K1, r) \\leq H(K2, r^2)$$ holds for any $$r > 0$$ if $$K1 \\subset K2$$) we obtain $$H(K^-_\\epsilon, \\eta) \\leq H(T K^-, \\eta)^2 \\leq 2k \\log 12T \\eta Lr$$. Bounding $$H(K^-_\\epsilon)^*, \\eta$$. We construct $$G3$$ as an $$\\epsilon \\eta$$-net of $$K^-_\\epsilon$$ satisfying $$\\log |G3| \\leq 2k \\log 12T Lr \\epsilon \\eta$$, then we consider $$(G3)^* := \\{z_\\epsilon)^* : z \\in G3\\}$$. Note that any $$x1 \\in (K^-_\\epsilon)^*$$ can be written as $$\\|z1\\|^2$$ for some $$z1 \\in K^-_\\epsilon$$ and recall that $$\\|z1\\|^2 \\geq z2^2\\epsilon$$. Moreover, by construction, there exists some $$z2 \\in G3$$ such that $$\\|z1 - z2\\|^2 \\leq \\epsilon \\eta$$. Note that $$\\|z2\\|^2 \\in (G3)^*$$, and moreover we have $$\\|z1 - z2\\|^2 \\leq \\|z1\\|^2 + \\|z2\\|^2 \\leq 2\\|z1 - z2\\| \\leq 2\\epsilon \\eta$$. Hence, we obtain $$\\|z1\\|^2 \\leq 2\\epsilon = \\eta$$, which completes the proof. $$H(K^-_\\epsilon)^*, \\eta \\leq \\log |(G3)^*| \\leq \\log |G3| \\leq 2k \\log 12T \\epsilon \\eta Lr$$.\n\nProof of Lemma 8 (Choice of $$T$$ in 1-bit GCS). Since $$x \\in S^{n-1}$$, for some orthogonal matrix $$P$$ we have $$Px = e1$$ (the first column of $$I_n$$). Since $$\\tilde{a} := Pa = [\\tilde{a}_i]$$ has the same distribution as $$a$$, we have $$E[\\text{sign}(a^Tx)a] = E[\\text{sign}(\\tilde{a}^Te_1)\\tilde{a}] = P^TE[\\text{sign}(\\tilde{a}_1)\\tilde{a}] = P^T\\pi e1 = \\pi x$$.\n\nProof of Lemma 9 (Choice of $$T$$ in 1-bit GCS with dithering). We first note that $$E[\\text{sign}(a^Tx + \\tau)a] - x \\leq \\sup_{\\lambda > 0} E[\\lambda \\cdot \\text{sign}(a^Tx + \\tau)a^Tv - x^Tv]$$. We first fix $$a$$ and expect over $$\\tau \\sim U[-\\lambda, \\lambda]$$ to obtain $$E[\\tau \\lambda \\text{sign}(a^Tx + \\tau)a^Tv] = (\\lambda a^Tv)1(|a^Tx| > \\lambda) \\text{sign}(a^Tx) + 1(|a^Tx| \\leq \\lambda) \\cdot \\frac{\\lambda + a^Tx}{2\\lambda} = (a^Tx)(a^Tv)1(|a^Tx| \\leq \\lambda) + (\\lambda a^Tv) \\text{sign}(a^Tx)1(|a^Tx| > \\lambda)$$.", "md": "Bounding $$H(K^-, \\eta)$$ and $$H(K^-_\\epsilon, \\eta)$$. We construct $$G2$$ as an $$\\eta^2$$-net of $$K$$ satisfying $$\\log |G2| \\leq k \\log 6Lr \\eta$$. Then, it is easy to see that $$G2 \\setminus G2$$ is an $$\\eta$$-net of $$K^- = K^- \\setminus K$$, showing that $$H(K^-, \\eta) \\leq \\log |G2|^2 \\leq 2k \\log 6Lr \\eta$$. For a given $$T > 0$$, this directly implies $$H(T K^-, \\eta) \\leq 2k \\log 6T Lr \\eta$$. Moreover, because $$K^-_\\epsilon \\subset T K^-$$, by [50, Exercise 4.2.10] (which states that $$H(K1, r) \\leq H(K2, r^2)$$ holds for any $$r > 0$$ if $$K1 \\subset K2$$) we obtain $$H(K^-_\\epsilon, \\eta) \\leq H(T K^-, \\eta)^2 \\leq 2k \\log 12T \\eta Lr$$. Bounding $$H(K^-_\\epsilon)^*, \\eta$$. We construct $$G3$$ as an $$\\epsilon \\eta$$-net of $$K^-_\\epsilon$$ satisfying $$\\log |G3| \\leq 2k \\log 12T Lr \\epsilon \\eta$$, then we consider $$(G3)^* := \\{z_\\epsilon)^* : z \\in G3\\}$$. Note that any $$x1 \\in (K^-_\\epsilon)^*$$ can be written as $$\\|z1\\|^2$$ for some $$z1 \\in K^-_\\epsilon$$ and recall that $$\\|z1\\|^2 \\geq z2^2\\epsilon$$. Moreover, by construction, there exists some $$z2 \\in G3$$ such that $$\\|z1 - z2\\|^2 \\leq \\epsilon \\eta$$. Note that $$\\|z2\\|^2 \\in (G3)^*$$, and moreover we have $$\\|z1 - z2\\|^2 \\leq \\|z1\\|^2 + \\|z2\\|^2 \\leq 2\\|z1 - z2\\| \\leq 2\\epsilon \\eta$$. Hence, we obtain $$\\|z1\\|^2 \\leq 2\\epsilon = \\eta$$, which completes the proof. $$H(K^-_\\epsilon)^*, \\eta \\leq \\log |(G3)^*| \\leq \\log |G3| \\leq 2k \\log 12T \\epsilon \\eta Lr$$.\n\nProof of Lemma 8 (Choice of $$T$$ in 1-bit GCS). Since $$x \\in S^{n-1}$$, for some orthogonal matrix $$P$$ we have $$Px = e1$$ (the first column of $$I_n$$). Since $$\\tilde{a} := Pa = [\\tilde{a}_i]$$ has the same distribution as $$a$$, we have $$E[\\text{sign}(a^Tx)a] = E[\\text{sign}(\\tilde{a}^Te_1)\\tilde{a}] = P^TE[\\text{sign}(\\tilde{a}_1)\\tilde{a}] = P^T\\pi e1 = \\pi x$$.\n\nProof of Lemma 9 (Choice of $$T$$ in 1-bit GCS with dithering). We first note that $$E[\\text{sign}(a^Tx + \\tau)a] - x \\leq \\sup_{\\lambda > 0} E[\\lambda \\cdot \\text{sign}(a^Tx + \\tau)a^Tv - x^Tv]$$. We first fix $$a$$ and expect over $$\\tau \\sim U[-\\lambda, \\lambda]$$ to obtain $$E[\\tau \\lambda \\text{sign}(a^Tx + \\tau)a^Tv] = (\\lambda a^Tv)1(|a^Tx| > \\lambda) \\text{sign}(a^Tx) + 1(|a^Tx| \\leq \\lambda) \\cdot \\frac{\\lambda + a^Tx}{2\\lambda} = (a^Tx)(a^Tv)1(|a^Tx| \\leq \\lambda) + (\\lambda a^Tv) \\text{sign}(a^Tx)1(|a^Tx| > \\lambda)$$."}]}, {"page": 22, "text": "We plug this into (D.1), and note that x\u22a4v = E[(a\u22a4x)(a\u22a4v)], which gives\n                      E[sign(a\u22a4x + \u03c4)a] \u2212           x\u03bb   2\n                  = 1     sup          (\u03bba\u22a4v) sign(a\u22a4x) \u2212           (a\u22a4x)(a\u22a4v)         1(|a\u22a4x| > \u03bb)                    (D.2)\n                     \u03bb  v\u2208Sn\u22121 E\n                  \u2264  1    sup        [\u03bb|a\u22a4v| + |a\u22a4x||a\u22a4v|]1(|a\u22a4x| > \u03bb)\n                     \u03bb  v\u2208Sn\u22121 E\nFor any x \u2208      Bn2(R) and v \u2208       Sn\u22121, we have \u2225a\u22a4x\u2225\u03c82 = O(R) and \u2225a\u22a4v\u2225\u03c82 = O(1). Applying\nthe Cauchy-Schwarz inequality, we obtain\n                         E    [\u03bb|a\u22a4v| + |a\u22a4x||a\u22a4v|]1(|a\u22a4x| > \u03bb)\n                       \u2264    E[(\u03bb|a\u22a4v| + |x\u22a4aa\u22a4v|)2]                P(|a\u22a4x| > \u03bb)\n                       \u2264    2  \u03bb2E[|a\u22a4v|2] + E[(a\u22a4x)2(a\u22a4v)2]                     2 exp(\u2212c\u03bb2/R2)\n                       \u2272\u03bb exp      \u2212   c\u03bb2     \u2272    \u03bb\n                                       R2         m9\nNote that in the third line, we use the probability tail bound of the sub-Gaussian |a\u22a4x|, and in the last\nline, we use \u03bb = CR\u221alog m with some sufficiently large C. The proof is completed by substituting\nthis into (D.2).\nD.5     Proof of Lemma 10 (Choice of T in SIM).\nProof. This lemma slightly generalizes that of Lemma 8. We again choose an orthogonal matrix\nP such that P x = e1, where e1 represents the first column of In. Since a and P a have the same\ndistribution, we have                 E[f(a\u22a4x)a] = P \u22a4E[f((P a)\u22a4e1)P a]\n                                   =P \u22a4E[f(a\u22a4e1)a] = P \u22a4(\u00b5e1) = \u00b5x.\nD.6     Proof of Lemma 11 (Choice of T in uniformly quantized GCS with dithering).\nProof. In the theorem, the statement before \u201cIn particular\u201d can be found in [18, Theorem 1]. Based\non this, we have E[Q\u03b4(a\u22a4x + \u03c4)a] = EaE\u03c4[Q\u03b4(a\u22a4x + \u03c4)a] = Ea(aa\u22a4x) = x.\nD.7     Proof of Lemma 12. (Bounds on |\u03bei,\u03b2| and |\u03b5i,\u03b2| for the uniform quantizer).\nProof. By the definition of fi,\u03b2 in (3.4), we have |\u03b5i,\u03b2(a)| = |fi,\u03b2(a) \u2212                   fi(a)| \u2264    \u03b4. It follows that\nfi(\u00b7) = Q\u03b4(\u00b7 + \u03c4) with Q\u03b4(a) = \u03b4             \u230aa\u03b4 \u230b+ 1    , and |Q\u03b4(a) \u2212     a| \u2264   \u03b4\n                                                      2                            2 holds for any a \u2208      R. Hence, we\nhave |fi(a)\u2212a| = |Q\u03b4(a+\u03c4)\u2212(a+\u03c4)+\u03c4| \u2264                       |Q\u03b4(a+\u03c4)\u2212(a+\u03c4)|+|\u03c4| \u2264                \u03b4\n                                                                                                2 + \u03b42 = \u03b4. To complete\nthe proof, we use the inequalities |\u03bei,\u03b2(a)| \u2264           |fi,\u03b2(a) \u2212    fi(a)| + |fi(a) \u2212      a| \u2264   \u03b4 + \u03b4 = 2\u03b4.\nD.8     Proof of Lemma 13. (Bound on the approximation error |\u03b5i,\u03b2|)\nProof. For any a /    \u2208  Dfi +[\u2212\u03b22 , \u03b22 ], by the definition in (3.4) we have \u03b5i,\u03b2(a) = 0. If a \u2208             [x0\u2212    \u03b22 , x0]\nfor some x0 \u2208      Dfi, then we have\n               |\u03b5i,\u03b2(a)| =|fi,\u03b2(a) \u2212       fi(a)|\n                           \u2264|fi,\u03b2(a) \u2212     fi,\u03b2(x0)| + |fi,\u03b2(x0) \u2212        fi\u2212(x0)| + |f \u2212 i (x0) \u2212    fi(a)|\n                           \u2264   2L0 + B0  \u03b2     |a \u2212  x0| + |f ai (x0) \u2212    fi\u2212(x0)| + L0|x0 \u2212        a|\n                           \u2264   3L0 + B0        \u00b7 \u03b2          i (x0) \u2212    f \u2212                     + B0,\n                                         \u03b2       2 + 1                    i (x0)| \u2264    3L0\u03b2\n                                                        2|f +                             2\nwhere we use Lemma 1 and Assumption 2 in the third line, and use |a \u2212                          x0| \u2264    \u03b22 and f ai (x0) =\n1  f \u2212                       in the fourth line.\n2    i (x0) + f + i (x0)\n                                                             22", "md": "We plug this into (D.1), and note that $$x^Tv = E[(a^Tx)(a^Tv)]$$, which gives\n\n$$\n\\begin{align*}\n&E[sign(a^Tx + \\tau)a] - x\\lambda/2 \\\\\n&= 1/\\lambda \\sup_{v\\in S^{n-1}} (\\lambda a^Tv) sign(a^Tx) - (a^Tx)(a^Tv) 1(|a^Tx| > \\lambda) \\quad (D.2) \\\\\n&\\leq 1/\\lambda \\sup_{v\\in S^{n-1}} [\\lambda|a^Tv| + |a^Tx||a^Tv|]1(|a^Tx| > \\lambda)\n\\end{align*}\n$$\n\nFor any $$x \\in B_{n/2}(R)$$ and $$v \\in S^{n-1}$$, we have $$\\|a^Tx\\|_{\\psi_2} = O(R)$$ and $$\\|a^Tv\\|_{\\psi_2} = O(1)$$. Applying the Cauchy-Schwarz inequality, we obtain\n\n$$\n\\begin{align*}\n&E[\\lambda|a^Tv| + |a^Tx||a^Tv|]1(|a^Tx| > \\lambda) \\\\\n&\\leq E[(\\lambda|a^Tv| + |x^Taa^Tv|)^2] P(|a^Tx| > \\lambda) \\\\\n&\\leq 2\\lambda^2E[|a^Tv|^2] + E[(a^Tx)^2(a^Tv)^2] 2\\exp(-c\\lambda^2/R^2) \\\\\n&\\lesssim \\lambda \\exp\\left(-\\frac{c\\lambda^2}{R^2}\\right) \\lesssim \\lambda\n\\end{align*}\n$$\n\nNote that in the third line, we use the probability tail bound of the sub-Gaussian $$|a^Tx|$$, and in the last line, we use $$\\lambda = CR\\sqrt{\\log m}$$ with some sufficiently large $$C$$. The proof is completed by substituting this into (D.2).\n\nD.5 Proof of Lemma 10 (Choice of T in SIM).\n\nProof. This lemma slightly generalizes that of Lemma 8. We again choose an orthogonal matrix $$P$$ such that $$Px = e_1$$, where $$e_1$$ represents the first column of $$I_n$$. Since $$a$$ and $$Pa$$ have the same distribution, we have $$E[f(a^Tx)a] = P^TE[f((Pa)^Te_1)Pa] = P^TE[f(a^Te_1)a] = P^T(\\mu e_1) = \\mu x$$.\n\nD.6 Proof of Lemma 11 (Choice of T in uniformly quantized GCS with dithering).\n\nProof. In the theorem, the statement before \u201cIn particular\u201d can be found in [18, Theorem 1]. Based on this, we have $$E[Q_\\delta(a^Tx + \\tau)a] = EaE_\\tau[Q_\\delta(a^Tx + \\tau)a] = Ea(aa^Tx) = x$$.\n\nD.7 Proof of Lemma 12. (Bounds on |\u03be_i,\u03b2| and |\u03b5_i,\u03b2| for the uniform quantizer).\n\nProof. By the definition of $$f_{i,\\beta}$$ in (3.4), we have $$|\\epsilon_{i,\\beta}(a)| = |f_{i,\\beta}(a) - f_i(a)| \\leq \\delta$$. It follows that $$f_i(\\cdot) = Q_\\delta(\\cdot + \\tau)$$ with $$Q_\\delta(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + 1$$, and $$|Q_\\delta(a) - a| \\leq \\delta/2$$ holds for any $$a \\in \\mathbb{R}$$. Hence, we have $$|f_i(a) - a| = |Q_\\delta(a+\\tau) - (a+\\tau) + \\tau| \\leq \\left|\\frac{\\delta}{2} + \\delta^2\\right| = \\delta$$. To complete the proof, we use the inequalities $$|\\xi_{i,\\beta}(a)| \\leq |f_{i,\\beta}(a) - f_i(a)| + |f_i(a) - a| \\leq \\delta + \\delta = 2\\delta$$.\n\nD.8 Proof of Lemma 13. (Bound on the approximation error |\u03b5_i,\u03b2|)\n\nProof. For any $$a \\notin D_{f_i} + [-\\beta/2, \\beta/2]$$, by the definition in (3.4) we have $$\\epsilon_{i,\\beta}(a) = 0$$. If $$a \\in [x_0 - \\beta/2, x_0]$$ for some $$x_0 \\in D_{f_i}$$, then we have\n\n$$\n\\begin{align*}\n|\\epsilon_{i,\\beta}(a)| & = |f_{i,\\beta}(a) - f_i(a)| \\\\\n& \\leq |f_{i,\\beta}(a) - f_{i,\\beta}(x_0)| + |f_{i,\\beta}(x_0) - f_i^-(x_0)| + |f_i^-(x_0) - f_i(a)| \\\\\n& \\leq 2L_0 + B_0 \\beta |a - x_0| + |f_{i,\\beta}(x_0) - f_i^-(x_0)| + L_0|x_0 - a| \\\\\n& \\leq 3L_0 + B_0 \\cdot \\beta^2 |f_i^+(x_0) - f_i^-(x_0)| \\leq 3L_0\\beta^2 |f_i^+ - f_i^-| + B_0,\n\\end{align*}\n$$\nwhere we use Lemma 1 and Assumption 2 in the third line, and use $$|a - x_0| \\leq \\beta^2$$ and $$f_i^+(x_0) = \\frac{1}{2}(f_i^+ + f_i^-)$$ in the fourth line.", "images": [], "items": [{"type": "text", "value": "We plug this into (D.1), and note that $$x^Tv = E[(a^Tx)(a^Tv)]$$, which gives\n\n$$\n\\begin{align*}\n&E[sign(a^Tx + \\tau)a] - x\\lambda/2 \\\\\n&= 1/\\lambda \\sup_{v\\in S^{n-1}} (\\lambda a^Tv) sign(a^Tx) - (a^Tx)(a^Tv) 1(|a^Tx| > \\lambda) \\quad (D.2) \\\\\n&\\leq 1/\\lambda \\sup_{v\\in S^{n-1}} [\\lambda|a^Tv| + |a^Tx||a^Tv|]1(|a^Tx| > \\lambda)\n\\end{align*}\n$$\n\nFor any $$x \\in B_{n/2}(R)$$ and $$v \\in S^{n-1}$$, we have $$\\|a^Tx\\|_{\\psi_2} = O(R)$$ and $$\\|a^Tv\\|_{\\psi_2} = O(1)$$. Applying the Cauchy-Schwarz inequality, we obtain\n\n$$\n\\begin{align*}\n&E[\\lambda|a^Tv| + |a^Tx||a^Tv|]1(|a^Tx| > \\lambda) \\\\\n&\\leq E[(\\lambda|a^Tv| + |x^Taa^Tv|)^2] P(|a^Tx| > \\lambda) \\\\\n&\\leq 2\\lambda^2E[|a^Tv|^2] + E[(a^Tx)^2(a^Tv)^2] 2\\exp(-c\\lambda^2/R^2) \\\\\n&\\lesssim \\lambda \\exp\\left(-\\frac{c\\lambda^2}{R^2}\\right) \\lesssim \\lambda\n\\end{align*}\n$$\n\nNote that in the third line, we use the probability tail bound of the sub-Gaussian $$|a^Tx|$$, and in the last line, we use $$\\lambda = CR\\sqrt{\\log m}$$ with some sufficiently large $$C$$. The proof is completed by substituting this into (D.2).\n\nD.5 Proof of Lemma 10 (Choice of T in SIM).\n\nProof. This lemma slightly generalizes that of Lemma 8. We again choose an orthogonal matrix $$P$$ such that $$Px = e_1$$, where $$e_1$$ represents the first column of $$I_n$$. Since $$a$$ and $$Pa$$ have the same distribution, we have $$E[f(a^Tx)a] = P^TE[f((Pa)^Te_1)Pa] = P^TE[f(a^Te_1)a] = P^T(\\mu e_1) = \\mu x$$.\n\nD.6 Proof of Lemma 11 (Choice of T in uniformly quantized GCS with dithering).\n\nProof. In the theorem, the statement before \u201cIn particular\u201d can be found in [18, Theorem 1]. Based on this, we have $$E[Q_\\delta(a^Tx + \\tau)a] = EaE_\\tau[Q_\\delta(a^Tx + \\tau)a] = Ea(aa^Tx) = x$$.\n\nD.7 Proof of Lemma 12. (Bounds on |\u03be_i,\u03b2| and |\u03b5_i,\u03b2| for the uniform quantizer).\n\nProof. By the definition of $$f_{i,\\beta}$$ in (3.4), we have $$|\\epsilon_{i,\\beta}(a)| = |f_{i,\\beta}(a) - f_i(a)| \\leq \\delta$$. It follows that $$f_i(\\cdot) = Q_\\delta(\\cdot + \\tau)$$ with $$Q_\\delta(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + 1$$, and $$|Q_\\delta(a) - a| \\leq \\delta/2$$ holds for any $$a \\in \\mathbb{R}$$. Hence, we have $$|f_i(a) - a| = |Q_\\delta(a+\\tau) - (a+\\tau) + \\tau| \\leq \\left|\\frac{\\delta}{2} + \\delta^2\\right| = \\delta$$. To complete the proof, we use the inequalities $$|\\xi_{i,\\beta}(a)| \\leq |f_{i,\\beta}(a) - f_i(a)| + |f_i(a) - a| \\leq \\delta + \\delta = 2\\delta$$.\n\nD.8 Proof of Lemma 13. (Bound on the approximation error |\u03b5_i,\u03b2|)\n\nProof. For any $$a \\notin D_{f_i} + [-\\beta/2, \\beta/2]$$, by the definition in (3.4) we have $$\\epsilon_{i,\\beta}(a) = 0$$. If $$a \\in [x_0 - \\beta/2, x_0]$$ for some $$x_0 \\in D_{f_i}$$, then we have\n\n$$\n\\begin{align*}", "md": "We plug this into (D.1), and note that $$x^Tv = E[(a^Tx)(a^Tv)]$$, which gives\n\n$$\n\\begin{align*}\n&E[sign(a^Tx + \\tau)a] - x\\lambda/2 \\\\\n&= 1/\\lambda \\sup_{v\\in S^{n-1}} (\\lambda a^Tv) sign(a^Tx) - (a^Tx)(a^Tv) 1(|a^Tx| > \\lambda) \\quad (D.2) \\\\\n&\\leq 1/\\lambda \\sup_{v\\in S^{n-1}} [\\lambda|a^Tv| + |a^Tx||a^Tv|]1(|a^Tx| > \\lambda)\n\\end{align*}\n$$\n\nFor any $$x \\in B_{n/2}(R)$$ and $$v \\in S^{n-1}$$, we have $$\\|a^Tx\\|_{\\psi_2} = O(R)$$ and $$\\|a^Tv\\|_{\\psi_2} = O(1)$$. Applying the Cauchy-Schwarz inequality, we obtain\n\n$$\n\\begin{align*}\n&E[\\lambda|a^Tv| + |a^Tx||a^Tv|]1(|a^Tx| > \\lambda) \\\\\n&\\leq E[(\\lambda|a^Tv| + |x^Taa^Tv|)^2] P(|a^Tx| > \\lambda) \\\\\n&\\leq 2\\lambda^2E[|a^Tv|^2] + E[(a^Tx)^2(a^Tv)^2] 2\\exp(-c\\lambda^2/R^2) \\\\\n&\\lesssim \\lambda \\exp\\left(-\\frac{c\\lambda^2}{R^2}\\right) \\lesssim \\lambda\n\\end{align*}\n$$\n\nNote that in the third line, we use the probability tail bound of the sub-Gaussian $$|a^Tx|$$, and in the last line, we use $$\\lambda = CR\\sqrt{\\log m}$$ with some sufficiently large $$C$$. The proof is completed by substituting this into (D.2).\n\nD.5 Proof of Lemma 10 (Choice of T in SIM).\n\nProof. This lemma slightly generalizes that of Lemma 8. We again choose an orthogonal matrix $$P$$ such that $$Px = e_1$$, where $$e_1$$ represents the first column of $$I_n$$. Since $$a$$ and $$Pa$$ have the same distribution, we have $$E[f(a^Tx)a] = P^TE[f((Pa)^Te_1)Pa] = P^TE[f(a^Te_1)a] = P^T(\\mu e_1) = \\mu x$$.\n\nD.6 Proof of Lemma 11 (Choice of T in uniformly quantized GCS with dithering).\n\nProof. In the theorem, the statement before \u201cIn particular\u201d can be found in [18, Theorem 1]. Based on this, we have $$E[Q_\\delta(a^Tx + \\tau)a] = EaE_\\tau[Q_\\delta(a^Tx + \\tau)a] = Ea(aa^Tx) = x$$.\n\nD.7 Proof of Lemma 12. (Bounds on |\u03be_i,\u03b2| and |\u03b5_i,\u03b2| for the uniform quantizer).\n\nProof. By the definition of $$f_{i,\\beta}$$ in (3.4), we have $$|\\epsilon_{i,\\beta}(a)| = |f_{i,\\beta}(a) - f_i(a)| \\leq \\delta$$. It follows that $$f_i(\\cdot) = Q_\\delta(\\cdot + \\tau)$$ with $$Q_\\delta(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + 1$$, and $$|Q_\\delta(a) - a| \\leq \\delta/2$$ holds for any $$a \\in \\mathbb{R}$$. Hence, we have $$|f_i(a) - a| = |Q_\\delta(a+\\tau) - (a+\\tau) + \\tau| \\leq \\left|\\frac{\\delta}{2} + \\delta^2\\right| = \\delta$$. To complete the proof, we use the inequalities $$|\\xi_{i,\\beta}(a)| \\leq |f_{i,\\beta}(a) - f_i(a)| + |f_i(a) - a| \\leq \\delta + \\delta = 2\\delta$$.\n\nD.8 Proof of Lemma 13. (Bound on the approximation error |\u03b5_i,\u03b2|)\n\nProof. For any $$a \\notin D_{f_i} + [-\\beta/2, \\beta/2]$$, by the definition in (3.4) we have $$\\epsilon_{i,\\beta}(a) = 0$$. If $$a \\in [x_0 - \\beta/2, x_0]$$ for some $$x_0 \\in D_{f_i}$$, then we have\n\n$$\n\\begin{align*}"}, {"type": "table", "rows": [["\\epsilon_{i,\\beta}(a)", "& =", "f_{i,\\beta}(a) - f_i(a)"]], "md": "|\\epsilon_{i,\\beta}(a)| & = |f_{i,\\beta}(a) - f_i(a)| \\\\", "isPerfectTable": true, "csv": "\"\\epsilon_{i,\\beta}(a)\",\"& =\",\"f_{i,\\beta}(a) - f_i(a)\""}, {"type": "text", "value": "& \\leq |f_{i,\\beta}(a) - f_{i,\\beta}(x_0)| + |f_{i,\\beta}(x_0) - f_i^-(x_0)| + |f_i^-(x_0) - f_i(a)| \\\\\n& \\leq 2L_0 + B_0 \\beta |a - x_0| + |f_{i,\\beta}(x_0) - f_i^-(x_0)| + L_0|x_0 - a| \\\\\n& \\leq 3L_0 + B_0 \\cdot \\beta^2 |f_i^+(x_0) - f_i^-(x_0)| \\leq 3L_0\\beta^2 |f_i^+ - f_i^-| + B_0,\n\\end{align*}\n$$\nwhere we use Lemma 1 and Assumption 2 in the third line, and use $$|a - x_0| \\leq \\beta^2$$ and $$f_i^+(x_0) = \\frac{1}{2}(f_i^+ + f_i^-)$$ in the fourth line.", "md": "& \\leq |f_{i,\\beta}(a) - f_{i,\\beta}(x_0)| + |f_{i,\\beta}(x_0) - f_i^-(x_0)| + |f_i^-(x_0) - f_i(a)| \\\\\n& \\leq 2L_0 + B_0 \\beta |a - x_0| + |f_{i,\\beta}(x_0) - f_i^-(x_0)| + L_0|x_0 - a| \\\\\n& \\leq 3L_0 + B_0 \\cdot \\beta^2 |f_i^+(x_0) - f_i^-(x_0)| \\leq 3L_0\\beta^2 |f_i^+ - f_i^-| + B_0,\n\\end{align*}\n$$\nwhere we use Lemma 1 and Assumption 2 in the third line, and use $$|a - x_0| \\leq \\beta^2$$ and $$f_i^+(x_0) = \\frac{1}{2}(f_i^+ + f_i^-)$$ in the fourth line."}]}, {"page": 23, "text": "E     Parameter Selection for Specific Models\nE.1    1-bit GCS\nTo specialize Theorem 1 to this model, we select the parameters as follows:\n    \u2022 Assumption 2. Under the 1-bit observation model yi = sign(a\u22a4                        i x\u2217), the function fi(\u00b7) =\n       f(\u00b7) = sign(\u00b7) satisfies Assumption 2 with (B0, L0, \u03b20) = (2, 0, \u221e).\n    \u2022 (2.5) in Assumption 4. Recall that K \u2282                  Sn\u22121. Under the assumption \u2225x\u2217\u22252 = 1, we set\n       T =       2/\u03c0 so that \u03c1(x) = 0 holds for all x \u2208           X (Lemma 8), which provides (2.5).\n    \u2022 Assumption 3. By Lemma 7, we have P(\u2225a\u22252 = O(\u221an)) \u2265                                 1 \u2212   2 exp(\u2212\u2126(n)), and we\n       suppose that this high-probability event holds. Also note that |fi,\u03b2| \u2264                  1. Hence, we have\n                             \u2225\u03bei,\u03b2(a\u22a4x)\u2225\u03c82 \u2264         \u2225fi,\u03b2(a\u22a4x)\u2225\u03c82 + \u2225Ta\u22a4x\u2225\u03c82 = O(1),\n                       sup  |\u03bei,\u03b2(a\u22a4x)| \u2264      |fi,\u03b2(a\u22a4x)| + |Ta\u22a4x| \u2264            1 + T\u2225a\u22252 = O(\u221an).\n                      x\u2208K\n       Because \u03b5i,\u03b2 = fi,\u03b2 \u2212          fi, we have \u2225\u03b5i,\u03b2(a\u22a4x)\u2225\u03c82 = O(1), and |\u03b5i,\u03b2(a\u22a4x)| \u2264                           2 holds\n       deterministically. Hence, regarding the parameters in Assumption 3, we can take\n                  A(1)   \u224d  1, U (1)  \u224d   \u221an, P (1)   \u224d   exp(\u2212\u2126(n)), A(2)        \u224d  1, U (2)  \u224d  1, P (2)  = 0.\n                     g            g              0                            g           g            0\n    \u2022 (2.6) in Assumption 4. It remains to pick \u03b21 that satisfies (2.6). Note that Df                    i = {0}, and for\n       any x \u2208    K, a\u22a4x \u223c       N(0, 1), so we have\n                                       \u00b5\u03b2(x) = P        a\u22a4x \u2208       \u2212   \u03b2         = O(\u03b2).\n                                                                        2 , \u03b2\n                                                                            2\n       Thus, we take \u03b2 = \u03b21 \u224d          mkto guarantee (2.6).\nE.2    1-bit GCS with dithering\nTo specialize Theorem 1 to this model, we select the parameters as follows:\n    \u2022 Assumption 2. The observation function can be written as f(\u00b7) = sign(\u00b7 + \u03c4) with \u03c4 \u223c\n       U [\u2212\u03bb, \u03bb], which satisfies Assumption 2 with (B0, L0, \u03b20) = (2, 0, \u221e).\n    \u2022 (2.5) in Assumption 4. We set \u03bb = CR\u221alog m with C large enough, so that Lemma 9 justifies\n       (2.5).\n    \u2022 Assumption 3. By Lemma 7, we have P(\u2225a\u22252 = O(\u221an)) \u2265                            1 \u2212  2 exp(\u2212\u2126(n)). Assume this\n       event holds, and note that fi,\u03b2 is still bounded by 1, we have\n               \u2225\u03bei,\u03b2(a\u22a4x)\u2225\u03c82 \u2264         \u2225fi,\u03b2(a\u22a4x)\u2225\u03c82 + \u2225\u03bb\u22121a\u22a4x\u2225\u03c82 = O(R/\u03bb) + O(1) = O(1),\n                sup  |\u03bei,\u03b2(a\u22a4x)| \u2264       1 + sup    |\u03bb\u22121a\u22a4x| \u2264       1 + sup     \u03bb\u22121\u2225a\u22252\u2225x\u22252 = O(\u221an).\n                x\u2208K                           x\u2208K                          x\u2208K\n       Moreover, because \u03b5i,\u03b2 = fi,\u03b2 \u2212            fi, the following hold deterministically: \u2225\u03b5i,\u03b2(a\u22a4x)\u2225\u03c82 =\n       O(1), sup    x\u2208K |\u03b5i,\u03b2(a\u22a4x)| = O(1). Thus, regarding the parameters in Assumption 3 we can\n       take\n                A(1)   \u224d  1, U (1)   \u224d  \u221an, P (1)     \u224d  exp(\u2212\u2126(n)), A(2)         \u224d   1, U (2)  \u224d   1, P (2)  = 0.\n                   g            g                0                            g             g            0\n    \u2022 (2.6) in Assumption 4. It remains to confirm (2.6) for suitable \u03b21. For any \u03b2, note that\n       Dfi + [\u2212\u03b2   2 , \u03b2                                                                 2(R) we have\n                       2 ] = [\u03c4 \u2212    \u03b22 , \u03c4 + \u03b22 ], and hence for any x \u2208       K \u2282    Bn\n              \u00b5\u03b2(x) = P        a\u22a4x \u2208       \u2212  \u03c4 \u2212   \u03b2                  = P     a\u22a4x + \u03c4 \u2208         \u2212  \u03b2          \u2264   \u03b2\n                                                     2 , \u2212\u03c4 + \u03b22                                     2 , \u03b22        \u03bb,\n       which can be seen by conditioning on a. Hence, we can take \u03b2 = \u03b21 = \u03bbk                      m to guarantee (2.6).\n                                                             23", "md": "# Parameter Selection for Specific Models\n\n## E.1 1-bit GCS\n\nTo specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. Under the 1-bit observation model $y_i = \\text{sign}(a^{\\top}_i x^*)$, the function $f_i(\\cdot) = f(\\cdot) = \\text{sign}(\\cdot)$ satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$.\n- (2.5) in Assumption 4. Recall that $K \\subset S^{n-1}$. Under the assumption $\\|x^*\\|_2 = 1$, we set $T = \\frac{2}{\\pi}$ so that $\\rho(x) = 0$ holds for all $x \\in X$ (Lemma 8), which provides (2.5).\n- Assumption 3. By Lemma 7, we have $P(\\|a\\|_2 = O(\\sqrt{n})) \\geq 1 - 2\\exp(-\\Omega(n))$, and we suppose that this high-probability event holds. Also note that $|f_i, \\beta| \\leq 1$. Hence, we have $\\|\\xi_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} \\leq \\|\\text{fi, \\beta}(a^{\\top}x)\\|_{\\psi_2} + \\|Ta^{\\top}x\\|_{\\psi_2} = O(1)$, $\\sup_{x \\in K} |\\xi_{i, \\beta}(a^{\\top}x)| \\leq |f_i, \\beta(a^{\\top}x)| + |Ta^{\\top}x| \\leq 1 + T\\|a\\|_2 = O(\\sqrt{n})$. Because $\\epsilon_{i, \\beta} = f_{i, \\beta} - f_i$, we have $\\|\\epsilon_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} = O(1)$, and $|\\epsilon_{i, \\beta}(a^{\\top}x)| \\leq 2$ holds deterministically. Hence, regarding the parameters in Assumption 3, we can take $A(1) \\approx 1, U(1) \\approx \\sqrt{n}, P(1) \\approx \\exp(-\\Omega(n)), A(2) \\approx 1, U(2) \\approx 1, P(2) = 0$.\n- (2.6) in Assumption 4. It remains to pick $\\beta_1$ that satisfies (2.6). Note that $Df_i = \\{0\\}$, and for any $x \\in K$, $a^{\\top}x \\sim N(0, 1)$, so we have $\\mu_{\\beta}(x) = P(a^{\\top}x \\in [-\\beta, \\beta]) = O(\\beta)$. Thus, we take $\\beta = \\beta_1 \\approx \\frac{mk}{\\tau}$ to guarantee (2.6).\n\n## E.2 1-bit GCS with dithering\n\nTo specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. The observation function can be written as $f(\\cdot) = \\text{sign}(\\cdot + \\tau)$ with $\\tau \\sim U[-\\lambda, \\lambda]$, which satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$.\n- (2.5) in Assumption 4. We set $\\lambda = CR\\sqrt{\\log m}$ with $C$ large enough, so that Lemma 9 justifies (2.5).\n- Assumption 3. By Lemma 7, we have $P(\\|a\\|_2 = O(\\sqrt{n})) \\geq 1 - 2\\exp(-\\Omega(n))$. Assume this event holds, and note that $f_{i, \\beta}$ is still bounded by 1, we have $\\|\\xi_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} \\leq \\|\\text{fi, \\beta}(a^{\\top}x)\\|_{\\psi_2} + \\|\\lambda^{-1}a^{\\top}x\\|_{\\psi_2} = O(R/\\lambda) + O(1) = O(1)$, $\\sup_{x \\in K} |\\xi_{i, \\beta}(a^{\\top}x)| \\leq 1 + \\sup_{x \\in K} |\\lambda^{-1}a^{\\top}x| \\leq 1 + \\sup_{x \\in K} \\lambda^{-1}\\|a\\|_2\\|x\\|_2 = O(\\sqrt{n})$. Moreover, because $\\epsilon_{i, \\beta} = f_{i, \\beta} - f_i$, the following hold deterministically: $\\|\\epsilon_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} = O(1)$, $\\sup_{x \\in K} |\\epsilon_{i, \\beta}(a^{\\top}x)| = O(1)$. Thus, regarding the parameters in Assumption 3 we can take $A(1) \\approx 1, U(1) \\approx \\sqrt{n}, P(1) \\approx \\exp(-\\Omega(n)), A(2) \\approx 1, U(2) \\approx 1, P(2) = 0$.\n- (2.6) in Assumption 4. It remains to confirm (2.6) for suitable $\\beta_1$. For any $\\beta$, note that $Df_i + [-\\beta^2, \\beta^2] = [\\tau - \\beta^2, \\tau + \\beta^2]$, and hence for any $x \\in K \\subset B^n$ $\\mu_{\\beta}(x) = P(a^{\\top}x \\in [-\\tau - \\beta^2, -\\tau + \\beta^2]) = P(a^{\\top}x + \\tau \\in [-\\beta^2, \\beta^2]) \\leq \\beta/\\lambda$, which can be seen by conditioning on $a$. Hence, we can take $\\beta = \\beta_1 = \\lambda k/m$ to guarantee (2.6).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Parameter Selection for Specific Models", "md": "# Parameter Selection for Specific Models"}, {"type": "heading", "lvl": 2, "value": "E.1 1-bit GCS", "md": "## E.1 1-bit GCS"}, {"type": "text", "value": "To specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. Under the 1-bit observation model $y_i = \\text{sign}(a^{\\top}_i x^*)$, the function $f_i(\\cdot) = f(\\cdot) = \\text{sign}(\\cdot)$ satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$.\n- (2.5) in Assumption 4. Recall that $K \\subset S^{n-1}$. Under the assumption $\\|x^*\\|_2 = 1$, we set $T = \\frac{2}{\\pi}$ so that $\\rho(x) = 0$ holds for all $x \\in X$ (Lemma 8), which provides (2.5).\n- Assumption 3. By Lemma 7, we have $P(\\|a\\|_2 = O(\\sqrt{n})) \\geq 1 - 2\\exp(-\\Omega(n))$, and we suppose that this high-probability event holds. Also note that $|f_i, \\beta| \\leq 1$. Hence, we have $\\|\\xi_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} \\leq \\|\\text{fi, \\beta}(a^{\\top}x)\\|_{\\psi_2} + \\|Ta^{\\top}x\\|_{\\psi_2} = O(1)$, $\\sup_{x \\in K} |\\xi_{i, \\beta}(a^{\\top}x)| \\leq |f_i, \\beta(a^{\\top}x)| + |Ta^{\\top}x| \\leq 1 + T\\|a\\|_2 = O(\\sqrt{n})$. Because $\\epsilon_{i, \\beta} = f_{i, \\beta} - f_i$, we have $\\|\\epsilon_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} = O(1)$, and $|\\epsilon_{i, \\beta}(a^{\\top}x)| \\leq 2$ holds deterministically. Hence, regarding the parameters in Assumption 3, we can take $A(1) \\approx 1, U(1) \\approx \\sqrt{n}, P(1) \\approx \\exp(-\\Omega(n)), A(2) \\approx 1, U(2) \\approx 1, P(2) = 0$.\n- (2.6) in Assumption 4. It remains to pick $\\beta_1$ that satisfies (2.6). Note that $Df_i = \\{0\\}$, and for any $x \\in K$, $a^{\\top}x \\sim N(0, 1)$, so we have $\\mu_{\\beta}(x) = P(a^{\\top}x \\in [-\\beta, \\beta]) = O(\\beta)$. Thus, we take $\\beta = \\beta_1 \\approx \\frac{mk}{\\tau}$ to guarantee (2.6).", "md": "To specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. Under the 1-bit observation model $y_i = \\text{sign}(a^{\\top}_i x^*)$, the function $f_i(\\cdot) = f(\\cdot) = \\text{sign}(\\cdot)$ satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$.\n- (2.5) in Assumption 4. Recall that $K \\subset S^{n-1}$. Under the assumption $\\|x^*\\|_2 = 1$, we set $T = \\frac{2}{\\pi}$ so that $\\rho(x) = 0$ holds for all $x \\in X$ (Lemma 8), which provides (2.5).\n- Assumption 3. By Lemma 7, we have $P(\\|a\\|_2 = O(\\sqrt{n})) \\geq 1 - 2\\exp(-\\Omega(n))$, and we suppose that this high-probability event holds. Also note that $|f_i, \\beta| \\leq 1$. Hence, we have $\\|\\xi_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} \\leq \\|\\text{fi, \\beta}(a^{\\top}x)\\|_{\\psi_2} + \\|Ta^{\\top}x\\|_{\\psi_2} = O(1)$, $\\sup_{x \\in K} |\\xi_{i, \\beta}(a^{\\top}x)| \\leq |f_i, \\beta(a^{\\top}x)| + |Ta^{\\top}x| \\leq 1 + T\\|a\\|_2 = O(\\sqrt{n})$. Because $\\epsilon_{i, \\beta} = f_{i, \\beta} - f_i$, we have $\\|\\epsilon_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} = O(1)$, and $|\\epsilon_{i, \\beta}(a^{\\top}x)| \\leq 2$ holds deterministically. Hence, regarding the parameters in Assumption 3, we can take $A(1) \\approx 1, U(1) \\approx \\sqrt{n}, P(1) \\approx \\exp(-\\Omega(n)), A(2) \\approx 1, U(2) \\approx 1, P(2) = 0$.\n- (2.6) in Assumption 4. It remains to pick $\\beta_1$ that satisfies (2.6). Note that $Df_i = \\{0\\}$, and for any $x \\in K$, $a^{\\top}x \\sim N(0, 1)$, so we have $\\mu_{\\beta}(x) = P(a^{\\top}x \\in [-\\beta, \\beta]) = O(\\beta)$. Thus, we take $\\beta = \\beta_1 \\approx \\frac{mk}{\\tau}$ to guarantee (2.6)."}, {"type": "heading", "lvl": 2, "value": "E.2 1-bit GCS with dithering", "md": "## E.2 1-bit GCS with dithering"}, {"type": "text", "value": "To specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. The observation function can be written as $f(\\cdot) = \\text{sign}(\\cdot + \\tau)$ with $\\tau \\sim U[-\\lambda, \\lambda]$, which satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$.\n- (2.5) in Assumption 4. We set $\\lambda = CR\\sqrt{\\log m}$ with $C$ large enough, so that Lemma 9 justifies (2.5).\n- Assumption 3. By Lemma 7, we have $P(\\|a\\|_2 = O(\\sqrt{n})) \\geq 1 - 2\\exp(-\\Omega(n))$. Assume this event holds, and note that $f_{i, \\beta}$ is still bounded by 1, we have $\\|\\xi_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} \\leq \\|\\text{fi, \\beta}(a^{\\top}x)\\|_{\\psi_2} + \\|\\lambda^{-1}a^{\\top}x\\|_{\\psi_2} = O(R/\\lambda) + O(1) = O(1)$, $\\sup_{x \\in K} |\\xi_{i, \\beta}(a^{\\top}x)| \\leq 1 + \\sup_{x \\in K} |\\lambda^{-1}a^{\\top}x| \\leq 1 + \\sup_{x \\in K} \\lambda^{-1}\\|a\\|_2\\|x\\|_2 = O(\\sqrt{n})$. Moreover, because $\\epsilon_{i, \\beta} = f_{i, \\beta} - f_i$, the following hold deterministically: $\\|\\epsilon_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} = O(1)$, $\\sup_{x \\in K} |\\epsilon_{i, \\beta}(a^{\\top}x)| = O(1)$. Thus, regarding the parameters in Assumption 3 we can take $A(1) \\approx 1, U(1) \\approx \\sqrt{n}, P(1) \\approx \\exp(-\\Omega(n)), A(2) \\approx 1, U(2) \\approx 1, P(2) = 0$.\n- (2.6) in Assumption 4. It remains to confirm (2.6) for suitable $\\beta_1$. For any $\\beta$, note that $Df_i + [-\\beta^2, \\beta^2] = [\\tau - \\beta^2, \\tau + \\beta^2]$, and hence for any $x \\in K \\subset B^n$ $\\mu_{\\beta}(x) = P(a^{\\top}x \\in [-\\tau - \\beta^2, -\\tau + \\beta^2]) = P(a^{\\top}x + \\tau \\in [-\\beta^2, \\beta^2]) \\leq \\beta/\\lambda$, which can be seen by conditioning on $a$. Hence, we can take $\\beta = \\beta_1 = \\lambda k/m$ to guarantee (2.6).", "md": "To specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. The observation function can be written as $f(\\cdot) = \\text{sign}(\\cdot + \\tau)$ with $\\tau \\sim U[-\\lambda, \\lambda]$, which satisfies Assumption 2 with $(B_0, L_0, \\beta_0) = (2, 0, \\infty)$.\n- (2.5) in Assumption 4. We set $\\lambda = CR\\sqrt{\\log m}$ with $C$ large enough, so that Lemma 9 justifies (2.5).\n- Assumption 3. By Lemma 7, we have $P(\\|a\\|_2 = O(\\sqrt{n})) \\geq 1 - 2\\exp(-\\Omega(n))$. Assume this event holds, and note that $f_{i, \\beta}$ is still bounded by 1, we have $\\|\\xi_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} \\leq \\|\\text{fi, \\beta}(a^{\\top}x)\\|_{\\psi_2} + \\|\\lambda^{-1}a^{\\top}x\\|_{\\psi_2} = O(R/\\lambda) + O(1) = O(1)$, $\\sup_{x \\in K} |\\xi_{i, \\beta}(a^{\\top}x)| \\leq 1 + \\sup_{x \\in K} |\\lambda^{-1}a^{\\top}x| \\leq 1 + \\sup_{x \\in K} \\lambda^{-1}\\|a\\|_2\\|x\\|_2 = O(\\sqrt{n})$. Moreover, because $\\epsilon_{i, \\beta} = f_{i, \\beta} - f_i$, the following hold deterministically: $\\|\\epsilon_{i, \\beta}(a^{\\top}x)\\|_{\\psi_2} = O(1)$, $\\sup_{x \\in K} |\\epsilon_{i, \\beta}(a^{\\top}x)| = O(1)$. Thus, regarding the parameters in Assumption 3 we can take $A(1) \\approx 1, U(1) \\approx \\sqrt{n}, P(1) \\approx \\exp(-\\Omega(n)), A(2) \\approx 1, U(2) \\approx 1, P(2) = 0$.\n- (2.6) in Assumption 4. It remains to confirm (2.6) for suitable $\\beta_1$. For any $\\beta$, note that $Df_i + [-\\beta^2, \\beta^2] = [\\tau - \\beta^2, \\tau + \\beta^2]$, and hence for any $x \\in K \\subset B^n$ $\\mu_{\\beta}(x) = P(a^{\\top}x \\in [-\\tau - \\beta^2, -\\tau + \\beta^2]) = P(a^{\\top}x + \\tau \\in [-\\beta^2, \\beta^2]) \\leq \\beta/\\lambda$, which can be seen by conditioning on $a$. Hence, we can take $\\beta = \\beta_1 = \\lambda k/m$ to guarantee (2.6)."}]}, {"page": 24, "text": "E.3      Lipschitz-continuous SIM with generative prior\nTo specialize Theorem 1 to this model, we select the parameters as follows:\n     \u2022 Assumption 2.              Since f is \u02c6      L-Lipschitz by assumption, it satisfies Assumption 2 with\n       (B0, L0, \u03b20) = (0, \u02c6        L, \u221e).\n     \u2022 (2.5) in Assumption 4. Recall that we have defined the quantities \u00b5 = E[f(g)g], \u03c8 =\n        \u2225f(g)\u2225\u03c82, where g \u223c               N  (0, 1). Then, we choose T = \u00b5 so that \u03c1(x) = 0 holds for any x\n        (Lemma 10), thus justifying (2.5).\n     \u2022 Assumption 3. Because fi is \u02c6                L-Lipschitz and does not contain any discontinuity, there is no\n        need to construct the Lipschitz approximation fi,\u03b2 for some \u03b2 > 0, while we simply use \u03b2 = 0,\n        which implies fi,\u03b2 = fi and \u03b5i,\u03b2 = 0. Note that \u03bei,\u03b2(a) = fi(a) \u2212                                \u00b5a, and so we have\n                          \u2225fi(a\u22a4x) \u2212         \u00b5a\u22a4x\u2225\u03c82 \u2264          \u2225fi(a\u22a4x)\u2225\u03c82 + \u2225\u00b5a\u22a4x\u2225\u03c82 = O(\u03c8 + \u00b5).\n        We suppose \u2225a\u22252 = O(\u221an), which holds with probability at least 1 \u2212                                   2 exp(\u2212\u2126(n)) (Lemma\n        7); we also suppose fi(0) \u2264                \u02c6\n                                                  B, which holds with probability at least 1 \u2212                       P \u2032\n                                                                                                                       0 by assumption.\n        On these two events, we have\n                           |fi(a\u22a4x) \u2212         \u00b5a\u22a4x| \u2264        |fi(a\u22a4x) \u2212        fi(0)| + |fi(0)| + \u00b5\u2225a\u22252\u2225x\u22252\n                                                         \u2264   \u02c6                                                         B.\n                                                             L\u2225a\u22252 + \u02c6     B + \u00b5\u2225a\u22252 \u2272           (\u02c6\n        Combined with \u03b5i,\u03b2 = 0, we can set the parameters in Assumption 3 as follows:             L + \u00b5)\u221an + \u02c6\n                         A(1)    \u224d  \u03c8 + \u00b5, U (1)  g     \u224d  (\u02c6L + \u00b5)\u221an + \u02c6        B, P (1)     \u224d   P \u2032\n                            g                                                            0          0 + exp(\u2212\u2126(n)),\n                                                 A(2)   \u224d   \u03c8 + \u00b5, U (2)  g     = 0, P (2)     = 0.\n                                                    g                                     0\n     \u2022 (2.6) in Assumption 4. Because \u03b2 = 0 and Df                          i = \u2205, (2.6) is trivially satisfied.\nE.4     Uniformly quantized GCS with dithering\nTo specialize Theorem 1 to this model, we select the parameters as follows:\n     \u2022 Assumption 2. The uniform quantizer with resolution \u03b4 > 0 is defined as Q\u03b4(a) = \u03b4                                          \u230aa\u03b4 \u230b + 1 2\n        for a \u2208    R. We consider this quantizer with dithering \u03c4i \u223c                        U [\u2212\u03b4   2, \u03b42]. Specifically, we observe\n        yi = Q\u03b4(a\u22a4      i x\u2217    + \u03c4i), so the observation function is f(\u00b7) = Q\u03b4(\u00b7 + \u03c4) with \u03c4 \u223c                                 U [\u2212\u03b4   2, \u03b42].\n        Hence, Assumption 2 is satisfied with (B0, L0, \u03b20) = (\u03b4, 0, \u03b4).\n     \u2022 (2.5) in Assumption 4. The benefit of dithering is to whiten the quantization noise. With T = 1,\n        for any x \u2208       K, Lemma 11 implies \u03c1(x) = \u2225E[Q\u03b4(a\u22a4                        i x + \u03c4i)ai] \u2212         x\u22252 = 0, thus justifying\n        (2.5).\n     \u2022 Assumption 3. Note that for any \u03b2 \u2208                       (0, \u03b42), by Lemma 12, we can take the parameters for\n        Assumption 3 as follows:\n                                          A(1)                             \u224d   \u03b4, P (1)   = P (2)     = 0.\n                                             g , U (1)\n                                                     g , A(2) g , U (2)\n                                                                      g              0           0\n     \u2022 (2.6) in Assumption 4. All that remains is to pick \u03b2 = \u03b21 that satisfies (2.6). Because\n        Dfi = \u2212\u03c4i + \u03b4Z, hence for any x \u2208                    K we have\n        \u00b5\u03b2(x) = P          a\u22a4x \u2208      \u2212\u03c4 + \u03b4Z +           \u2212   \u03b2           = P      a\u22a4x + \u03c4 \u2208         \u03b4Z +       \u2212   \u03b2           = O     \u03b2     ,\n                                                              2 , \u03b22                                                2 , \u03b22                \u03b4\n        which can be seen by using the randomness of \u03c4 \u223c                           U [\u2212\u03b4   2, \u03b42] conditionally on x. Hence, we\n        take \u03b2 = \u03b21 \u224d          k\u03b4\n                               m , which provides (2.6).\nF      Handling Sub-Gaussian Additive Noise\nIn this appendix, we describe how our results can be extended to the noisy model y = f(Ax\u2217) + \u03b7,\nwhere \u03b7 \u2208          Rm is the noise vector that is independent of (A, f) and has i.i.d. sub-Gaussian\nentries \u03b7i satisfying \u2225\u03b7i\u2225\u03c82 = O(1). Along similar lines as in (3.1)-(3.3), we find that \u03b7 gives\nrise to an additional term 2          m\u27e8\u03b7, A(\u02c6    x \u2212    T  x\u2217)\u27e9    to the right-hand side of (3.1), which is bounded by\n                                                                      24", "md": "### E.3 Lipschitz-continuous SIM with generative prior\n\nTo specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. Since f is $\\hat{L}$-Lipschitz by assumption, it satisfies Assumption 2 with (B0, L0, \u03b20) = (0, $\\hat{L}$, \u221e).\n- (2.5) in Assumption 4. Recall that we have defined the quantities $\\mu = E[f(g)g]$, $\\psi = \\lVert f(g) \\rVert_{\\psi}^2$, where $g \\sim N(0, 1)$. Then, we choose T = $\\mu$ so that $\\rho(x) = 0$ holds for any x (Lemma 10), thus justifying (2.5).\n- Assumption 3. Because $f_i$ is $\\hat{L}$-Lipschitz and does not contain any discontinuity, there is no need to construct the Lipschitz approximation $f_{i,\\beta}$ for some $\\beta > 0$, while we simply use $\\beta = 0$, which implies $f_{i,\\beta} = f_i$ and $\\varepsilon_{i,\\beta} = 0$. Note that $\\xi_{i,\\beta}(a) = f_i(a) - \\mu a$, and so we have $\\lVert f_i(a^Tx) - \\mu a^Tx \\rVert_{\\psi}^2 \\leq \\lVert f_i(a^Tx) \\rVert_{\\psi}^2 + \\lVert \\mu a^Tx \\rVert_{\\psi}^2 = O(\\psi + \\mu)$. We suppose $\\lVert a \\rVert^2 = O(\\sqrt{n})$, which holds with probability at least $1 - 2\\exp(-\\Omega(n))$ (Lemma 7); we also suppose $f_i(0) \\leq \\hat{B}$, which holds with probability at least $1 - P'$ by assumption. On these two events, we have $\\lvert f_i(a^Tx) - \\mu a^Tx \\rvert \\leq \\lvert f_i(a^Tx) - f_i(0) \\rvert + \\lvert f_i(0) \\rvert + \\mu \\lVert a \\rVert^2 \\lVert x \\rVert^2 \\leq \\hat{L}\\lVert a \\rVert^2 + \\hat{B} + \\mu \\lVert a \\rVert^2 \\lesssim (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$. Combined with $\\varepsilon_{i,\\beta} = 0$, we can set the parameters in Assumption 3 as follows: $A(1) \\asymp \\psi + \\mu$, $U(1) \\gtrsim (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$, $P(1) \\asymp P'$, $A(2) \\asymp \\psi + \\mu$, $U(2) = 0$, $P(2) = 0$.\n- (2.6) in Assumption 4. Because $\\beta = 0$ and $Df_i = \\emptyset$, (2.6) is trivially satisfied.\n\n### E.4 Uniformly quantized GCS with dithering\n\nTo specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. The uniform quantizer with resolution $\\delta > 0$ is defined as $Q_{\\delta}(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + \\frac{1}{2}$ for $a \\in \\mathbb{R}$. We consider this quantizer with dithering $\\tau_i \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$. Specifically, we observe $y_i = Q_{\\delta}(a_i^Tx^* + \\tau_i)$, so the observation function is $f(\\cdot) = Q_{\\delta}(\\cdot + \\tau)$ with $\\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$. Hence, Assumption 2 is satisfied with (B0, L0, \u03b20) = ($\\delta$, 0, $\\delta$).\n- (2.5) in Assumption 4. The benefit of dithering is to whiten the quantization noise. With T = 1, for any $x \\in \\mathcal{K}$, Lemma 11 implies $\\rho(x) = \\lVert E[Q_{\\delta}(a_i^Tx + \\tau_i)a_i] - x \\rVert^2 = 0$, thus justifying (2.5).\n- Assumption 3. Note that for any $\\beta \\in (0, \\delta^2)$, by Lemma 12, we can take the parameters for Assumption 3 as follows: $A(1) \\asymp \\delta$, $P(1) = P(2) = 0$, $U(1) \\gtrsim 0$, $U(2) = 0$.\n- (2.6) in Assumption 4. All that remains is to pick $\\beta = \\beta_1$ that satisfies (2.6). Because $Df_i = -\\tau_i + \\delta Z$, hence for any $x \\in \\mathcal{K}$ we have $\\mu_{\\beta}(x) = \\mathbb{P}\\left\\{a^Tx \\in -\\tau + \\delta Z + \\beta\\right\\} = \\mathbb{P}\\left\\{a^Tx + \\tau \\in \\delta Z + \\beta\\right\\} = O(\\beta)$, which can be seen by using the randomness of $\\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$ conditionally on x. Hence, we take $\\beta = \\beta_1 \\asymp k\\delta$, which provides (2.6).\n\n### F Handling Sub-Gaussian Additive Noise\n\nIn this appendix, we describe how our results can be extended to the noisy model $$y = f(Ax^*) + \\eta$$, where $$\\eta \\in \\mathbb{R}^m$$ is the noise vector that is independent of $$(A, f)$$ and has i.i.d. sub-Gaussian entries $$\\eta_i$$ satisfying $$\\lVert \\eta_i \\rVert_{\\psi}^2 = O(1)$$. Along similar lines as in (3.1)-(3.3), we find that $$\\eta$$ gives rise to an additional term $$2m\\langle \\eta, A(\\hat{x} - Tx^*) \\rangle$$ to the right-hand side of (3.1), which is bounded by 24.", "images": [], "items": [{"type": "heading", "lvl": 3, "value": "E.3 Lipschitz-continuous SIM with generative prior", "md": "### E.3 Lipschitz-continuous SIM with generative prior"}, {"type": "text", "value": "To specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. Since f is $\\hat{L}$-Lipschitz by assumption, it satisfies Assumption 2 with (B0, L0, \u03b20) = (0, $\\hat{L}$, \u221e).\n- (2.5) in Assumption 4. Recall that we have defined the quantities $\\mu = E[f(g)g]$, $\\psi = \\lVert f(g) \\rVert_{\\psi}^2$, where $g \\sim N(0, 1)$. Then, we choose T = $\\mu$ so that $\\rho(x) = 0$ holds for any x (Lemma 10), thus justifying (2.5).\n- Assumption 3. Because $f_i$ is $\\hat{L}$-Lipschitz and does not contain any discontinuity, there is no need to construct the Lipschitz approximation $f_{i,\\beta}$ for some $\\beta > 0$, while we simply use $\\beta = 0$, which implies $f_{i,\\beta} = f_i$ and $\\varepsilon_{i,\\beta} = 0$. Note that $\\xi_{i,\\beta}(a) = f_i(a) - \\mu a$, and so we have $\\lVert f_i(a^Tx) - \\mu a^Tx \\rVert_{\\psi}^2 \\leq \\lVert f_i(a^Tx) \\rVert_{\\psi}^2 + \\lVert \\mu a^Tx \\rVert_{\\psi}^2 = O(\\psi + \\mu)$. We suppose $\\lVert a \\rVert^2 = O(\\sqrt{n})$, which holds with probability at least $1 - 2\\exp(-\\Omega(n))$ (Lemma 7); we also suppose $f_i(0) \\leq \\hat{B}$, which holds with probability at least $1 - P'$ by assumption. On these two events, we have $\\lvert f_i(a^Tx) - \\mu a^Tx \\rvert \\leq \\lvert f_i(a^Tx) - f_i(0) \\rvert + \\lvert f_i(0) \\rvert + \\mu \\lVert a \\rVert^2 \\lVert x \\rVert^2 \\leq \\hat{L}\\lVert a \\rVert^2 + \\hat{B} + \\mu \\lVert a \\rVert^2 \\lesssim (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$. Combined with $\\varepsilon_{i,\\beta} = 0$, we can set the parameters in Assumption 3 as follows: $A(1) \\asymp \\psi + \\mu$, $U(1) \\gtrsim (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$, $P(1) \\asymp P'$, $A(2) \\asymp \\psi + \\mu$, $U(2) = 0$, $P(2) = 0$.\n- (2.6) in Assumption 4. Because $\\beta = 0$ and $Df_i = \\emptyset$, (2.6) is trivially satisfied.", "md": "To specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. Since f is $\\hat{L}$-Lipschitz by assumption, it satisfies Assumption 2 with (B0, L0, \u03b20) = (0, $\\hat{L}$, \u221e).\n- (2.5) in Assumption 4. Recall that we have defined the quantities $\\mu = E[f(g)g]$, $\\psi = \\lVert f(g) \\rVert_{\\psi}^2$, where $g \\sim N(0, 1)$. Then, we choose T = $\\mu$ so that $\\rho(x) = 0$ holds for any x (Lemma 10), thus justifying (2.5).\n- Assumption 3. Because $f_i$ is $\\hat{L}$-Lipschitz and does not contain any discontinuity, there is no need to construct the Lipschitz approximation $f_{i,\\beta}$ for some $\\beta > 0$, while we simply use $\\beta = 0$, which implies $f_{i,\\beta} = f_i$ and $\\varepsilon_{i,\\beta} = 0$. Note that $\\xi_{i,\\beta}(a) = f_i(a) - \\mu a$, and so we have $\\lVert f_i(a^Tx) - \\mu a^Tx \\rVert_{\\psi}^2 \\leq \\lVert f_i(a^Tx) \\rVert_{\\psi}^2 + \\lVert \\mu a^Tx \\rVert_{\\psi}^2 = O(\\psi + \\mu)$. We suppose $\\lVert a \\rVert^2 = O(\\sqrt{n})$, which holds with probability at least $1 - 2\\exp(-\\Omega(n))$ (Lemma 7); we also suppose $f_i(0) \\leq \\hat{B}$, which holds with probability at least $1 - P'$ by assumption. On these two events, we have $\\lvert f_i(a^Tx) - \\mu a^Tx \\rvert \\leq \\lvert f_i(a^Tx) - f_i(0) \\rvert + \\lvert f_i(0) \\rvert + \\mu \\lVert a \\rVert^2 \\lVert x \\rVert^2 \\leq \\hat{L}\\lVert a \\rVert^2 + \\hat{B} + \\mu \\lVert a \\rVert^2 \\lesssim (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$. Combined with $\\varepsilon_{i,\\beta} = 0$, we can set the parameters in Assumption 3 as follows: $A(1) \\asymp \\psi + \\mu$, $U(1) \\gtrsim (\\hat{L} + \\mu)\\sqrt{n} + \\hat{B}$, $P(1) \\asymp P'$, $A(2) \\asymp \\psi + \\mu$, $U(2) = 0$, $P(2) = 0$.\n- (2.6) in Assumption 4. Because $\\beta = 0$ and $Df_i = \\emptyset$, (2.6) is trivially satisfied."}, {"type": "heading", "lvl": 3, "value": "E.4 Uniformly quantized GCS with dithering", "md": "### E.4 Uniformly quantized GCS with dithering"}, {"type": "text", "value": "To specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. The uniform quantizer with resolution $\\delta > 0$ is defined as $Q_{\\delta}(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + \\frac{1}{2}$ for $a \\in \\mathbb{R}$. We consider this quantizer with dithering $\\tau_i \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$. Specifically, we observe $y_i = Q_{\\delta}(a_i^Tx^* + \\tau_i)$, so the observation function is $f(\\cdot) = Q_{\\delta}(\\cdot + \\tau)$ with $\\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$. Hence, Assumption 2 is satisfied with (B0, L0, \u03b20) = ($\\delta$, 0, $\\delta$).\n- (2.5) in Assumption 4. The benefit of dithering is to whiten the quantization noise. With T = 1, for any $x \\in \\mathcal{K}$, Lemma 11 implies $\\rho(x) = \\lVert E[Q_{\\delta}(a_i^Tx + \\tau_i)a_i] - x \\rVert^2 = 0$, thus justifying (2.5).\n- Assumption 3. Note that for any $\\beta \\in (0, \\delta^2)$, by Lemma 12, we can take the parameters for Assumption 3 as follows: $A(1) \\asymp \\delta$, $P(1) = P(2) = 0$, $U(1) \\gtrsim 0$, $U(2) = 0$.\n- (2.6) in Assumption 4. All that remains is to pick $\\beta = \\beta_1$ that satisfies (2.6). Because $Df_i = -\\tau_i + \\delta Z$, hence for any $x \\in \\mathcal{K}$ we have $\\mu_{\\beta}(x) = \\mathbb{P}\\left\\{a^Tx \\in -\\tau + \\delta Z + \\beta\\right\\} = \\mathbb{P}\\left\\{a^Tx + \\tau \\in \\delta Z + \\beta\\right\\} = O(\\beta)$, which can be seen by using the randomness of $\\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$ conditionally on x. Hence, we take $\\beta = \\beta_1 \\asymp k\\delta$, which provides (2.6).", "md": "To specialize Theorem 1 to this model, we select the parameters as follows:\n\n- Assumption 2. The uniform quantizer with resolution $\\delta > 0$ is defined as $Q_{\\delta}(a) = \\delta \\left\\lfloor \\frac{a}{\\delta} \\right\\rfloor + \\frac{1}{2}$ for $a \\in \\mathbb{R}$. We consider this quantizer with dithering $\\tau_i \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$. Specifically, we observe $y_i = Q_{\\delta}(a_i^Tx^* + \\tau_i)$, so the observation function is $f(\\cdot) = Q_{\\delta}(\\cdot + \\tau)$ with $\\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$. Hence, Assumption 2 is satisfied with (B0, L0, \u03b20) = ($\\delta$, 0, $\\delta$).\n- (2.5) in Assumption 4. The benefit of dithering is to whiten the quantization noise. With T = 1, for any $x \\in \\mathcal{K}$, Lemma 11 implies $\\rho(x) = \\lVert E[Q_{\\delta}(a_i^Tx + \\tau_i)a_i] - x \\rVert^2 = 0$, thus justifying (2.5).\n- Assumption 3. Note that for any $\\beta \\in (0, \\delta^2)$, by Lemma 12, we can take the parameters for Assumption 3 as follows: $A(1) \\asymp \\delta$, $P(1) = P(2) = 0$, $U(1) \\gtrsim 0$, $U(2) = 0$.\n- (2.6) in Assumption 4. All that remains is to pick $\\beta = \\beta_1$ that satisfies (2.6). Because $Df_i = -\\tau_i + \\delta Z$, hence for any $x \\in \\mathcal{K}$ we have $\\mu_{\\beta}(x) = \\mathbb{P}\\left\\{a^Tx \\in -\\tau + \\delta Z + \\beta\\right\\} = \\mathbb{P}\\left\\{a^Tx + \\tau \\in \\delta Z + \\beta\\right\\} = O(\\beta)$, which can be seen by using the randomness of $\\tau \\sim U\\left[-\\frac{\\delta}{2}, \\frac{\\delta}{2}\\right]$ conditionally on x. Hence, we take $\\beta = \\beta_1 \\asymp k\\delta$, which provides (2.6)."}, {"type": "heading", "lvl": 3, "value": "F Handling Sub-Gaussian Additive Noise", "md": "### F Handling Sub-Gaussian Additive Noise"}, {"type": "text", "value": "In this appendix, we describe how our results can be extended to the noisy model $$y = f(Ax^*) + \\eta$$, where $$\\eta \\in \\mathbb{R}^m$$ is the noise vector that is independent of $$(A, f)$$ and has i.i.d. sub-Gaussian entries $$\\eta_i$$ satisfying $$\\lVert \\eta_i \\rVert_{\\psi}^2 = O(1)$$. Along similar lines as in (3.1)-(3.3), we find that $$\\eta$$ gives rise to an additional term $$2m\\langle \\eta, A(\\hat{x} - Tx^*) \\rangle$$ to the right-hand side of (3.1), which is bounded by 24.", "md": "In this appendix, we describe how our results can be extended to the noisy model $$y = f(Ax^*) + \\eta$$, where $$\\eta \\in \\mathbb{R}^m$$ is the noise vector that is independent of $$(A, f)$$ and has i.i.d. sub-Gaussian entries $$\\eta_i$$ satisfying $$\\lVert \\eta_i \\rVert_{\\psi}^2 = O(1)$$. Along similar lines as in (3.1)-(3.3), we find that $$\\eta$$ gives rise to an additional term $$2m\\langle \\eta, A(\\hat{x} - Tx^*) \\rangle$$ to the right-hand side of (3.1), which is bounded by 24."}]}, {"page": 25, "text": "2\u2225\u02c6 x \u2212    T  x\u2217\u22252 \u00b7 supv\u2208(K\u2212                m\u27e8\u03b7, Av\u27e9, with the constraint set (K\u2212                      \u03f5 )\u2217   defined in (3.2). Thus, in (3.3),\n                                       \u03f5 )\u2217   1\nin addition to Ru, in the noisy setting we need to bound the additional term\n                                    R\u2032  u :=        sup        1                         sup        1    m    \u03b7ia\u22a4  i v.\n                                                v\u2208(K\u2212   \u03f5 )\u2217  m\u27e8\u03b7, Av\u27e9           =   v\u2208(K\u2212   \u03f5 )\u2217  m    i=1\nThis can be done by the following lemma, which indicates that the sharp (uniform) rate in Theorem 1\ncan be retained in the presence of noise \u03b7.\nLemma 15. (Bounding the additional term R\u2032                             u). In the noisy setting described above, with probability\nat least 1 \u2212       C1 exp(\u2212\u2126(k log T Lr                                                                       u \u2272      k log T Lr   \u03f5   .\n                                                    \u03f5 )) \u2212      C2 exp(\u2212\u2126(m)), we have R\u2032                                      m\nProof. Conditioning on \u03b7, the randomness of ai\u2019s gives                                          1   m  i=1 \u03b7iai \u223c          N   (0, \u2225\u03b7\u22252   2\n                                                                                               m                                      m2 In), and so\n\u2225( 1     m  i=1 \u03b7iai)\u22a4v1 \u2212             ( 1    m  i=1 \u03b7iai)\u22a4v2\u2225\u03c82 \u2264                C0\u2225\u03b7\u22252\u2225v1\u2212v2\u22252             holds for any v1, v2 \u2208               Rn. Let\n    m                                    m                                                   m\n\u03c9(\u00b7) be the Gaussian width as defined in Lemma 5. Then, using the randomness of ai\u2019s, Talagrand\u2019s\ncomparison inequality [50, Exercise 8.6.5] yields that for any t \u2265                                        0, we have\n                                 P     R\u2032 u \u2264     C1\u2225\u03b7\u22252 \u00b7 [\u03c9((K\u2212    m      \u03f5 )\u2217) + t]         \u2265   1 \u2212    2 exp(\u2212t2).                                   (F.1)\nNext, we bound the Gaussian width \u03c9((K\u2212                            \u03f5 )\u2217). Recall that (K\u2212          \u03f5 )\u2217    is defined in (3.2), and Lemma 6\nbounds its metric entropy as H ((K\u2212                      \u03f5 )\u2217, \u03b7) \u2264       2k log 12T Lr \u03f5\u03b7     . Thus, we can invoke Dudley\u2019s integral\ninequality [50, Theorem 8.1.3] to obtain\n                                 \u03c9((K\u2212                          2     2k log 12T        Lr    d\u03b7 \u2272          k log T     Lr   .\n                                         \u03f5 )\u2217) \u2264      C2      0                      \u03f5\u03b7                                 \u03f5\nNow, we further let t =                        k log T Lr  \u03f5     in (F.1) to obtain that R\u2032               u \u2272       \u2225\u03b7\u22252\u221ak logmT Lr    \u03f5     holds with\nprobability at least 1 \u2212              2 exp(\u2212k log T Lr        \u03f5 ). It remains to deal with the randomness of \u03b7 and bound\n\u2225\u03b7\u22252. Because \u03b7 has i.i.d. entries with \u2225\u03b7i\u2225\u03c82 = O(1), by [50, Theorem 3.1.1] we can obtain\nthat \u2225\u03b7\u22252 \u2264            C3   \u221a  m with probability at least 1 \u2212                        2 exp(\u2212c3m). Substituting this bound into\nR\u2032         \u2225\u03b7\u22252\u221a     k log T Lr\u03f5   , the result follows.\n   u \u2272               m\nTo close this appendix, we briefly state how to adapt the proof of Theorem 1 to explicitly include the\nadditive noise \u03b7. Specifically, the left-hand side of (3.1) and its uniform lower bound \u2126(\u2225\u02c6                                                x  \u2212  T  x\u2217\u22252   2)\nremain unchanged, while the right-hand side of (3.1) is now bounded by 2\u2225\u02c6                                                   x \u2212    T  x\u2217\u22252 \u00b7 (Ru1 +\nRu2 + R\u2032       u) (with 2\u2225\u02c6      x \u2212    T  x\u2217\u22252R\u2032      u being the additional term); thus, combining the bound (C.12) on\n2\u2225\u02c6 x \u2212    T  x\u2217\u22252 \u00b7 (Ru1 + Ru2) and Lemma 15, we establish a uniform upper bound\n                                  \uf8eb                                                       kL               k log T Lr \u03f5      \uf8f6\n                               O  \uf8ed\u2225\u02c6   x \u2212    T  x\u2217\u22252 \u00b7        (A(1)    \u2228   A(2)                                            \uf8f8\n                                                                    g           g )         m +                  m\nfor the right-hand side of (3.1). Therefore, to ensure uniform recovery up to the \u21132-norm accuracy of\n\u03f5 under the sub-Gaussian noise \u03b7, it suffices to have a sample complexity\n                                                m \u2273      (A(1)    \u2228   A(2)                               \u03f5    .                                         (F.2)\n                                                             g           g )2 kL   \u03f52 + k log T Lr  \u03f52\nSince the logarithmic factors L in (C.11) dominates log T Lr                                \u03f5 , (F.2) indeed coincides with the sample\ncomplexity m \u2273              (A(1)    \u2228   A(2)                                                                                         \u2228   A(2)    = \u2126(1).\n                                g           g )2 kL  \u03f52 in Theorem 1 under the mild condition of A(1)                            g           g\nG       Experimental Results for the MNIST dataset\nG.1       Details of the Settings\nIn this section, we conduct experiments on the MNIST dataset [28] to support our theoretical\nframework. We use various nonlinear measurement models, including 1-bit, dithered 1-bit, ReLU,\n                                                                             25", "md": "$$2\\|\\hat{x} - Tx^*\\|_2 \\cdot \\sup_{v\\in(K^- m\\langle\\eta, Av\\rangle}$$, with the constraint set $(K^- \\epsilon)^*$ defined in (3.2). Thus, in (3.3),\n\n$$\\epsilon)^* 1$$\n\nin addition to Ru, in the noisy setting we need to bound the additional term\n\n$$R'u := \\sup_{v\\in(K^- \\epsilon)^*} \\frac{1}{m} \\sup_{1\\leq i \\leq m} \\eta_ia_i^Tv.$$\n\nThis can be done by the following lemma, which indicates that the sharp (uniform) rate in Theorem 1 can be retained in the presence of noise $\\eta$.\n\nLemma 15. (Bounding the additional term $R'u$). In the noisy setting described above, with probability at least $1 - C1 \\exp(-\\Omega(k \\log T Lr \\epsilon)) - C2 \\exp(-\\Omega(m))$, we have $R'u \\lesssim k \\log T Lr \\epsilon$.\n\nProof. Conditioning on $\\eta$, the randomness of $a_i$'s gives $\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i \\sim N(0, \\|\\eta\\|_2^2 m^2 I_n)$, and so $\\|(\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i)^Tv_1 - (\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i)^Tv_2\\|_{\\psi_2} \\leq C0\\|\\eta\\|_2\\|v_1-v_2\\|_2$ holds for any $v_1, v_2 \\in \\mathbb{R}^n$. Let $\\omega(\\cdot)$ be the Gaussian width as defined in Lemma 5. Then, using the randomness of $a_i$'s, Talagrand's comparison inequality [50, Exercise 8.6.5] yields that for any $t \\geq 0$, we have\n\n$$P(R'u \\leq C1\\|\\eta\\|_2 \\cdot [\\omega((K^- m \\epsilon)^*) + t] \\geq 1 - 2 \\exp(-t^2).$$\n\nNext, we bound the Gaussian width $\\omega((K^- \\epsilon)^*)$. Recall that $(K^- \\epsilon)^*$ is defined in (3.2), and Lemma 6 bounds its metric entropy as $H((K^- \\epsilon)^*, \\eta) \\leq 2k \\log 12T Lr \\epsilon \\eta$. Thus, we can invoke Dudley's integral inequality [50, Theorem 8.1.3] to obtain\n\n$$\\omega((K^- \\epsilon)^*) \\leq C2 \\int_{0}^{2k \\log 12T Lr} d\\eta \\lesssim k \\log T Lr \\epsilon.$$\n\nNow, we further let $t = k \\log T Lr \\epsilon$ in the previous inequality to obtain that $R'u \\lesssim \\|\\eta\\|_2 \\sqrt{k \\log mT Lr \\epsilon}$ holds with probability at least $1 - 2 \\exp(-k \\log T Lr \\epsilon)$. It remains to deal with the randomness of $\\eta$ and bound $\\|\\eta\\|_2$. Because $\\eta$ has i.i.d. entries with $\\|\\eta_i\\|_{\\psi_2} = O(1)$, by [50, Theorem 3.1.1] we can obtain that $\\|\\eta\\|_2 \\leq C3 \\sqrt{m}$ with probability at least $1 - 2 \\exp(-c3m)$. Substituting this bound into $R'u \\lesssim \\|\\eta\\|_2 \\sqrt{k \\log T Lr \\epsilon}$, the result follows.\n\nTo close this appendix, we briefly state how to adapt the proof of Theorem 1 to explicitly include the additive noise $\\eta$. Specifically, the left-hand side of (3.1) and its uniform lower bound $\\Omega(\\|\\hat{x} - Tx^*\\|_2^2)$ remain unchanged, while the right-hand side of (3.1) is now bounded by $2\\|\\hat{x} - Tx^*\\|_2 \\cdot (Ru1 + Ru2 + R'u)$ (with $2\\|\\hat{x} - Tx^*\\|_2R'u$ being the additional term); thus, combining the bound (C.12) on $2\\|\\hat{x} - Tx^*\\|_2 \\cdot (Ru1 + Ru2)$ and Lemma 15, we establish a uniform upper bound\n\n$$O\\left(\\|\\hat{x} - Tx^*\\|_2 \\cdot (A(1) \\vee A(2))_g^m + m\\right)$$\n\nfor the right-hand side of (3.1). Therefore, to ensure uniform recovery up to the $\\ell_2$-norm accuracy of $\\epsilon$ under the sub-Gaussian noise $\\eta$, it suffices to have a sample complexity\n\n$$m \\gtrsim (A(1) \\vee A(2))_g^2 kL \\epsilon^2 + k \\log T Lr \\epsilon^2.$$\n\nSince the logarithmic factors $L$ in (C.11) dominates $\\log T Lr \\epsilon$, (F.2) indeed coincides with the sample complexity $m \\gtrsim (A(1) \\vee A(2))_g^2 kL \\epsilon^2$ in Theorem 1 under the mild condition of $A(1)_g \\vee A(2)_g = \\Omega(1)$.\n\nExperimental Results for the MNIST dataset\n\nG.1 Details of the Settings\n\nIn this section, we conduct experiments on the MNIST dataset [28] to support our theoretical framework. We use various nonlinear measurement models, including 1-bit, dithered 1-bit, ReLU,", "images": [], "items": [{"type": "text", "value": "$$2\\|\\hat{x} - Tx^*\\|_2 \\cdot \\sup_{v\\in(K^- m\\langle\\eta, Av\\rangle}$$, with the constraint set $(K^- \\epsilon)^*$ defined in (3.2). Thus, in (3.3),\n\n$$\\epsilon)^* 1$$\n\nin addition to Ru, in the noisy setting we need to bound the additional term\n\n$$R'u := \\sup_{v\\in(K^- \\epsilon)^*} \\frac{1}{m} \\sup_{1\\leq i \\leq m} \\eta_ia_i^Tv.$$\n\nThis can be done by the following lemma, which indicates that the sharp (uniform) rate in Theorem 1 can be retained in the presence of noise $\\eta$.\n\nLemma 15. (Bounding the additional term $R'u$). In the noisy setting described above, with probability at least $1 - C1 \\exp(-\\Omega(k \\log T Lr \\epsilon)) - C2 \\exp(-\\Omega(m))$, we have $R'u \\lesssim k \\log T Lr \\epsilon$.\n\nProof. Conditioning on $\\eta$, the randomness of $a_i$'s gives $\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i \\sim N(0, \\|\\eta\\|_2^2 m^2 I_n)$, and so $\\|(\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i)^Tv_1 - (\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i)^Tv_2\\|_{\\psi_2} \\leq C0\\|\\eta\\|_2\\|v_1-v_2\\|_2$ holds for any $v_1, v_2 \\in \\mathbb{R}^n$. Let $\\omega(\\cdot)$ be the Gaussian width as defined in Lemma 5. Then, using the randomness of $a_i$'s, Talagrand's comparison inequality [50, Exercise 8.6.5] yields that for any $t \\geq 0$, we have\n\n$$P(R'u \\leq C1\\|\\eta\\|_2 \\cdot [\\omega((K^- m \\epsilon)^*) + t] \\geq 1 - 2 \\exp(-t^2).$$\n\nNext, we bound the Gaussian width $\\omega((K^- \\epsilon)^*)$. Recall that $(K^- \\epsilon)^*$ is defined in (3.2), and Lemma 6 bounds its metric entropy as $H((K^- \\epsilon)^*, \\eta) \\leq 2k \\log 12T Lr \\epsilon \\eta$. Thus, we can invoke Dudley's integral inequality [50, Theorem 8.1.3] to obtain\n\n$$\\omega((K^- \\epsilon)^*) \\leq C2 \\int_{0}^{2k \\log 12T Lr} d\\eta \\lesssim k \\log T Lr \\epsilon.$$\n\nNow, we further let $t = k \\log T Lr \\epsilon$ in the previous inequality to obtain that $R'u \\lesssim \\|\\eta\\|_2 \\sqrt{k \\log mT Lr \\epsilon}$ holds with probability at least $1 - 2 \\exp(-k \\log T Lr \\epsilon)$. It remains to deal with the randomness of $\\eta$ and bound $\\|\\eta\\|_2$. Because $\\eta$ has i.i.d. entries with $\\|\\eta_i\\|_{\\psi_2} = O(1)$, by [50, Theorem 3.1.1] we can obtain that $\\|\\eta\\|_2 \\leq C3 \\sqrt{m}$ with probability at least $1 - 2 \\exp(-c3m)$. Substituting this bound into $R'u \\lesssim \\|\\eta\\|_2 \\sqrt{k \\log T Lr \\epsilon}$, the result follows.\n\nTo close this appendix, we briefly state how to adapt the proof of Theorem 1 to explicitly include the additive noise $\\eta$. Specifically, the left-hand side of (3.1) and its uniform lower bound $\\Omega(\\|\\hat{x} - Tx^*\\|_2^2)$ remain unchanged, while the right-hand side of (3.1) is now bounded by $2\\|\\hat{x} - Tx^*\\|_2 \\cdot (Ru1 + Ru2 + R'u)$ (with $2\\|\\hat{x} - Tx^*\\|_2R'u$ being the additional term); thus, combining the bound (C.12) on $2\\|\\hat{x} - Tx^*\\|_2 \\cdot (Ru1 + Ru2)$ and Lemma 15, we establish a uniform upper bound\n\n$$O\\left(\\|\\hat{x} - Tx^*\\|_2 \\cdot (A(1) \\vee A(2))_g^m + m\\right)$$\n\nfor the right-hand side of (3.1). Therefore, to ensure uniform recovery up to the $\\ell_2$-norm accuracy of $\\epsilon$ under the sub-Gaussian noise $\\eta$, it suffices to have a sample complexity\n\n$$m \\gtrsim (A(1) \\vee A(2))_g^2 kL \\epsilon^2 + k \\log T Lr \\epsilon^2.$$\n\nSince the logarithmic factors $L$ in (C.11) dominates $\\log T Lr \\epsilon$, (F.2) indeed coincides with the sample complexity $m \\gtrsim (A(1) \\vee A(2))_g^2 kL \\epsilon^2$ in Theorem 1 under the mild condition of $A(1)_g \\vee A(2)_g = \\Omega(1)$.\n\nExperimental Results for the MNIST dataset\n\nG.1 Details of the Settings\n\nIn this section, we conduct experiments on the MNIST dataset [28] to support our theoretical framework. We use various nonlinear measurement models, including 1-bit, dithered 1-bit, ReLU,", "md": "$$2\\|\\hat{x} - Tx^*\\|_2 \\cdot \\sup_{v\\in(K^- m\\langle\\eta, Av\\rangle}$$, with the constraint set $(K^- \\epsilon)^*$ defined in (3.2). Thus, in (3.3),\n\n$$\\epsilon)^* 1$$\n\nin addition to Ru, in the noisy setting we need to bound the additional term\n\n$$R'u := \\sup_{v\\in(K^- \\epsilon)^*} \\frac{1}{m} \\sup_{1\\leq i \\leq m} \\eta_ia_i^Tv.$$\n\nThis can be done by the following lemma, which indicates that the sharp (uniform) rate in Theorem 1 can be retained in the presence of noise $\\eta$.\n\nLemma 15. (Bounding the additional term $R'u$). In the noisy setting described above, with probability at least $1 - C1 \\exp(-\\Omega(k \\log T Lr \\epsilon)) - C2 \\exp(-\\Omega(m))$, we have $R'u \\lesssim k \\log T Lr \\epsilon$.\n\nProof. Conditioning on $\\eta$, the randomness of $a_i$'s gives $\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i \\sim N(0, \\|\\eta\\|_2^2 m^2 I_n)$, and so $\\|(\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i)^Tv_1 - (\\frac{1}{m} \\sum_{i=1}^{m} \\eta_ia_i)^Tv_2\\|_{\\psi_2} \\leq C0\\|\\eta\\|_2\\|v_1-v_2\\|_2$ holds for any $v_1, v_2 \\in \\mathbb{R}^n$. Let $\\omega(\\cdot)$ be the Gaussian width as defined in Lemma 5. Then, using the randomness of $a_i$'s, Talagrand's comparison inequality [50, Exercise 8.6.5] yields that for any $t \\geq 0$, we have\n\n$$P(R'u \\leq C1\\|\\eta\\|_2 \\cdot [\\omega((K^- m \\epsilon)^*) + t] \\geq 1 - 2 \\exp(-t^2).$$\n\nNext, we bound the Gaussian width $\\omega((K^- \\epsilon)^*)$. Recall that $(K^- \\epsilon)^*$ is defined in (3.2), and Lemma 6 bounds its metric entropy as $H((K^- \\epsilon)^*, \\eta) \\leq 2k \\log 12T Lr \\epsilon \\eta$. Thus, we can invoke Dudley's integral inequality [50, Theorem 8.1.3] to obtain\n\n$$\\omega((K^- \\epsilon)^*) \\leq C2 \\int_{0}^{2k \\log 12T Lr} d\\eta \\lesssim k \\log T Lr \\epsilon.$$\n\nNow, we further let $t = k \\log T Lr \\epsilon$ in the previous inequality to obtain that $R'u \\lesssim \\|\\eta\\|_2 \\sqrt{k \\log mT Lr \\epsilon}$ holds with probability at least $1 - 2 \\exp(-k \\log T Lr \\epsilon)$. It remains to deal with the randomness of $\\eta$ and bound $\\|\\eta\\|_2$. Because $\\eta$ has i.i.d. entries with $\\|\\eta_i\\|_{\\psi_2} = O(1)$, by [50, Theorem 3.1.1] we can obtain that $\\|\\eta\\|_2 \\leq C3 \\sqrt{m}$ with probability at least $1 - 2 \\exp(-c3m)$. Substituting this bound into $R'u \\lesssim \\|\\eta\\|_2 \\sqrt{k \\log T Lr \\epsilon}$, the result follows.\n\nTo close this appendix, we briefly state how to adapt the proof of Theorem 1 to explicitly include the additive noise $\\eta$. Specifically, the left-hand side of (3.1) and its uniform lower bound $\\Omega(\\|\\hat{x} - Tx^*\\|_2^2)$ remain unchanged, while the right-hand side of (3.1) is now bounded by $2\\|\\hat{x} - Tx^*\\|_2 \\cdot (Ru1 + Ru2 + R'u)$ (with $2\\|\\hat{x} - Tx^*\\|_2R'u$ being the additional term); thus, combining the bound (C.12) on $2\\|\\hat{x} - Tx^*\\|_2 \\cdot (Ru1 + Ru2)$ and Lemma 15, we establish a uniform upper bound\n\n$$O\\left(\\|\\hat{x} - Tx^*\\|_2 \\cdot (A(1) \\vee A(2))_g^m + m\\right)$$\n\nfor the right-hand side of (3.1). Therefore, to ensure uniform recovery up to the $\\ell_2$-norm accuracy of $\\epsilon$ under the sub-Gaussian noise $\\eta$, it suffices to have a sample complexity\n\n$$m \\gtrsim (A(1) \\vee A(2))_g^2 kL \\epsilon^2 + k \\log T Lr \\epsilon^2.$$\n\nSince the logarithmic factors $L$ in (C.11) dominates $\\log T Lr \\epsilon$, (F.2) indeed coincides with the sample complexity $m \\gtrsim (A(1) \\vee A(2))_g^2 kL \\epsilon^2$ in Theorem 1 under the mild condition of $A(1)_g \\vee A(2)_g = \\Omega(1)$.\n\nExperimental Results for the MNIST dataset\n\nG.1 Details of the Settings\n\nIn this section, we conduct experiments on the MNIST dataset [28] to support our theoretical framework. We use various nonlinear measurement models, including 1-bit, dithered 1-bit, ReLU,"}]}, {"page": 26, "text": "Original\nLasso\nCSGM\n         Figure 2: Reconstructed images of the MNIST dataset for the noiseless 1-bit measurements with\n         m = 150.\n         and uniformly quantized CS with dithering (UQD). We select 30 images from the MNIST testing set,\n         ensuring that there are three images from each of the 10 classes for maximum variability. A single\n         measurement matrix A is generated and used for all 30 test images. All the experiments are repeated\n         for 10 random trials. All the experiments are run using Python 3.10.6 and PyTorch 2.0.0, with an\n         NVIDIA RTX 3060 Laptop 6GB GPU.\n         We train a variational autoencoder (VAE) on the training set of the MNIST dataset, which has 60,000\n         images, each of size 784. The decoder of the VAE is a fully connected neural network with ReLU\n         activations, with input dimension k = 20 and output dimension n = 784, and two hidden layers with\n         500 neurons each. We train the VAE using the Adam optimizer with a mini-batch size of 100 and a\n         learning rate of 0.001.\n         Since our contributions are primarily theoretical, we only provide simple proof-of-concept exper-\n         imental results. In particular, since (2.1) is intractable to solve exactly, to estimate the underlying\n         signal, we choose to use the algorithm proposed in [2] (referred to as CSGM) to approximate it. CSGM\n         performs a gradient descent algorithm in the latent space in Rk with random restarts. In addition, we\n         compare with the Lasso program that is solved by the iterative shrinkage thresholding algorithm.\n         For CSGM, we follow the setting in [2] and perform 10 random restarts with 1000 gradient descent\n         steps per restart and pick the reconstruction with the best measurement error.\n         G.2          Experimental Results for Noiseless 1-bit Measurements and Uniformly Quantized\n                      Measurements with Dithering\n         In this subsection, we present the numerical results for 1-bit measurements and uniformly quantized\n         measurements with dithering, while the results for dithered 1-bit measurements and the Lipschitz\n         SIM where the nonlinear link function is ReLU are similarly provided in Appendix G.3. For 1-bit\n         measurements, since the underlying signal is assumed to be a unit vector and we aim to recover\n         the direction of the signal, we use cosine similarity that is calculated as \u02c6                                          xT x\u2217/(\u2225   \u02c6\n         x\u02c6being the estimated vector to measure the reconstruction performance. For uniformly quantized                                  x\u22252 \u00b7 \u2225x\u2217\u22252) with\n         measurements with dithering, we use the relative \u21132-norm distance between the underlying signal and\n         the estimated vector, i.e., \u2225                          \u02c6\n                                                                x \u2212      x\u2217\u2225/\u2225x\u2217\u22252, to measure the reconstruction performance.\n         Since this paper is concerned with uniform recovery performance, in each trial, we record the worst-\n         case reconstruction performance (i.e., the smallest cosine similarity or the largest relative error) over\n         the 30 test images, and the worst-case cosine similarity or relative error is averaged over 10 trials.\n         Figures 2, 3, and 4 show that for noiseless 1-bit measurements and uniformly quantized measurements\n         with dithering with \u03b4 = 3, the CSGM approach can produce reasonably accurate reconstruction for all\n         the test images when the number of measurements m is as small as 150 and 100 respectively.\n         G.3          Experimental Results for ReLU and Dithered 1-bit Measurements\n         We present the experimental results for the ReLU link function and dithered 1-bit measurements\n         in Figures 5, 6, and 7. For dithered 1-bit measurements, we set \u03bb = R\u221alog m with R > 0 being\n         a tuning parameter. For the case of using the ReLU link function, similarly to noiseless 1-bit\n         measurements, we calculate the cosine similarity to measure the reconstruction performance. For\n         dithered 1-bit measurements, similarly to uniformly quantized measurements with dithering, we\n         calculate the relative \u21132-norm distance. We observe that for these two nonlinear measurement models\n                                                                                                           26", "md": "Figure 2: Reconstructed images of the MNIST dataset for the noiseless 1-bit measurements with\nm = 150.\nand uniformly quantized CS with dithering (UQD). We select 30 images from the MNIST testing set,\nensuring that there are three images from each of the 10 classes for maximum variability. A single\nmeasurement matrix A is generated and used for all 30 test images. All the experiments are repeated\nfor 10 random trials. All the experiments are run using Python 3.10.6 and PyTorch 2.0.0, with an\nNVIDIA RTX 3060 Laptop 6GB GPU.\nWe train a variational autoencoder (VAE) on the training set of the MNIST dataset, which has 60,000\nimages, each of size 784. The decoder of the VAE is a fully connected neural network with ReLU\nactivations, with input dimension k = 20 and output dimension n = 784, and two hidden layers with\n500 neurons each. We train the VAE using the Adam optimizer with a mini-batch size of 100 and a\nlearning rate of 0.001.\nSince our contributions are primarily theoretical, we only provide simple proof-of-concept exper-\nimental results. In particular, since (2.1) is intractable to solve exactly, to estimate the underlying\nsignal, we choose to use the algorithm proposed in [2] (referred to as CSGM) to approximate it. CSGM\nperforms a gradient descent algorithm in the latent space in $$\\mathbb{R}^k$$ with random restarts. In addition, we\ncompare with the Lasso program that is solved by the iterative shrinkage thresholding algorithm.\nFor CSGM, we follow the setting in [2] and perform 10 random restarts with 1000 gradient descent\nsteps per restart and pick the reconstruction with the best measurement error.\n\nG.2 Experimental Results for Noiseless 1-bit Measurements and Uniformly Quantized\nMeasurements with Dithering\nIn this subsection, we present the numerical results for 1-bit measurements and uniformly quantized\nmeasurements with dithering, while the results for dithered 1-bit measurements and the Lipschitz\nSIM where the nonlinear link function is ReLU are similarly provided in Appendix G.3. For 1-bit\nmeasurements, since the underlying signal is assumed to be a unit vector and we aim to recover\nthe direction of the signal, we use cosine similarity that is calculated as $$\\hat{x}^T x^*/(\\| \\hat{x} \\|_2 \\cdot \\| x^* \\|_2)$$ with\n$$\\hat{x}$$ being the estimated vector to measure the reconstruction performance. For uniformly quantized\nmeasurements with dithering, we use the relative \u21132-norm distance between the underlying signal and\nthe estimated vector, i.e., $$\\| x - \\hat{x} \\|_2 / \\| x^* \\|_2$$, to measure the reconstruction performance.\nSince this paper is concerned with uniform recovery performance, in each trial, we record the worst-\ncase reconstruction performance (i.e., the smallest cosine similarity or the largest relative error) over\nthe 30 test images, and the worst-case cosine similarity or relative error is averaged over 10 trials.\nFigures 2, 3, and 4 show that for noiseless 1-bit measurements and uniformly quantized measurements\nwith dithering with $$\\delta = 3$$, the CSGM approach can produce reasonably accurate reconstruction for all\nthe test images when the number of measurements m is as small as 150 and 100 respectively.\n\nG.3 Experimental Results for ReLU and Dithered 1-bit Measurements\nWe present the experimental results for the ReLU link function and dithered 1-bit measurements\nin Figures 5, 6, and 7. For dithered 1-bit measurements, we set $$\\lambda = R\\sqrt{\\log m}$$ with $$R > 0$$ being\na tuning parameter. For the case of using the ReLU link function, similarly to noiseless 1-bit\nmeasurements, we calculate the cosine similarity to measure the reconstruction performance. For\ndithered 1-bit measurements, similarly to uniformly quantized measurements with dithering, we\ncalculate the relative \u21132-norm distance. We observe that for these two nonlinear measurement models", "images": [], "items": [{"type": "text", "value": "Figure 2: Reconstructed images of the MNIST dataset for the noiseless 1-bit measurements with\nm = 150.\nand uniformly quantized CS with dithering (UQD). We select 30 images from the MNIST testing set,\nensuring that there are three images from each of the 10 classes for maximum variability. A single\nmeasurement matrix A is generated and used for all 30 test images. All the experiments are repeated\nfor 10 random trials. All the experiments are run using Python 3.10.6 and PyTorch 2.0.0, with an\nNVIDIA RTX 3060 Laptop 6GB GPU.\nWe train a variational autoencoder (VAE) on the training set of the MNIST dataset, which has 60,000\nimages, each of size 784. The decoder of the VAE is a fully connected neural network with ReLU\nactivations, with input dimension k = 20 and output dimension n = 784, and two hidden layers with\n500 neurons each. We train the VAE using the Adam optimizer with a mini-batch size of 100 and a\nlearning rate of 0.001.\nSince our contributions are primarily theoretical, we only provide simple proof-of-concept exper-\nimental results. In particular, since (2.1) is intractable to solve exactly, to estimate the underlying\nsignal, we choose to use the algorithm proposed in [2] (referred to as CSGM) to approximate it. CSGM\nperforms a gradient descent algorithm in the latent space in $$\\mathbb{R}^k$$ with random restarts. In addition, we\ncompare with the Lasso program that is solved by the iterative shrinkage thresholding algorithm.\nFor CSGM, we follow the setting in [2] and perform 10 random restarts with 1000 gradient descent\nsteps per restart and pick the reconstruction with the best measurement error.\n\nG.2 Experimental Results for Noiseless 1-bit Measurements and Uniformly Quantized\nMeasurements with Dithering\nIn this subsection, we present the numerical results for 1-bit measurements and uniformly quantized\nmeasurements with dithering, while the results for dithered 1-bit measurements and the Lipschitz\nSIM where the nonlinear link function is ReLU are similarly provided in Appendix G.3. For 1-bit\nmeasurements, since the underlying signal is assumed to be a unit vector and we aim to recover\nthe direction of the signal, we use cosine similarity that is calculated as $$\\hat{x}^T x^*/(\\| \\hat{x} \\|_2 \\cdot \\| x^* \\|_2)$$ with\n$$\\hat{x}$$ being the estimated vector to measure the reconstruction performance. For uniformly quantized\nmeasurements with dithering, we use the relative \u21132-norm distance between the underlying signal and\nthe estimated vector, i.e., $$\\| x - \\hat{x} \\|_2 / \\| x^* \\|_2$$, to measure the reconstruction performance.\nSince this paper is concerned with uniform recovery performance, in each trial, we record the worst-\ncase reconstruction performance (i.e., the smallest cosine similarity or the largest relative error) over\nthe 30 test images, and the worst-case cosine similarity or relative error is averaged over 10 trials.\nFigures 2, 3, and 4 show that for noiseless 1-bit measurements and uniformly quantized measurements\nwith dithering with $$\\delta = 3$$, the CSGM approach can produce reasonably accurate reconstruction for all\nthe test images when the number of measurements m is as small as 150 and 100 respectively.\n\nG.3 Experimental Results for ReLU and Dithered 1-bit Measurements\nWe present the experimental results for the ReLU link function and dithered 1-bit measurements\nin Figures 5, 6, and 7. For dithered 1-bit measurements, we set $$\\lambda = R\\sqrt{\\log m}$$ with $$R > 0$$ being\na tuning parameter. For the case of using the ReLU link function, similarly to noiseless 1-bit\nmeasurements, we calculate the cosine similarity to measure the reconstruction performance. For\ndithered 1-bit measurements, similarly to uniformly quantized measurements with dithering, we\ncalculate the relative \u21132-norm distance. We observe that for these two nonlinear measurement models", "md": "Figure 2: Reconstructed images of the MNIST dataset for the noiseless 1-bit measurements with\nm = 150.\nand uniformly quantized CS with dithering (UQD). We select 30 images from the MNIST testing set,\nensuring that there are three images from each of the 10 classes for maximum variability. A single\nmeasurement matrix A is generated and used for all 30 test images. All the experiments are repeated\nfor 10 random trials. All the experiments are run using Python 3.10.6 and PyTorch 2.0.0, with an\nNVIDIA RTX 3060 Laptop 6GB GPU.\nWe train a variational autoencoder (VAE) on the training set of the MNIST dataset, which has 60,000\nimages, each of size 784. The decoder of the VAE is a fully connected neural network with ReLU\nactivations, with input dimension k = 20 and output dimension n = 784, and two hidden layers with\n500 neurons each. We train the VAE using the Adam optimizer with a mini-batch size of 100 and a\nlearning rate of 0.001.\nSince our contributions are primarily theoretical, we only provide simple proof-of-concept exper-\nimental results. In particular, since (2.1) is intractable to solve exactly, to estimate the underlying\nsignal, we choose to use the algorithm proposed in [2] (referred to as CSGM) to approximate it. CSGM\nperforms a gradient descent algorithm in the latent space in $$\\mathbb{R}^k$$ with random restarts. In addition, we\ncompare with the Lasso program that is solved by the iterative shrinkage thresholding algorithm.\nFor CSGM, we follow the setting in [2] and perform 10 random restarts with 1000 gradient descent\nsteps per restart and pick the reconstruction with the best measurement error.\n\nG.2 Experimental Results for Noiseless 1-bit Measurements and Uniformly Quantized\nMeasurements with Dithering\nIn this subsection, we present the numerical results for 1-bit measurements and uniformly quantized\nmeasurements with dithering, while the results for dithered 1-bit measurements and the Lipschitz\nSIM where the nonlinear link function is ReLU are similarly provided in Appendix G.3. For 1-bit\nmeasurements, since the underlying signal is assumed to be a unit vector and we aim to recover\nthe direction of the signal, we use cosine similarity that is calculated as $$\\hat{x}^T x^*/(\\| \\hat{x} \\|_2 \\cdot \\| x^* \\|_2)$$ with\n$$\\hat{x}$$ being the estimated vector to measure the reconstruction performance. For uniformly quantized\nmeasurements with dithering, we use the relative \u21132-norm distance between the underlying signal and\nthe estimated vector, i.e., $$\\| x - \\hat{x} \\|_2 / \\| x^* \\|_2$$, to measure the reconstruction performance.\nSince this paper is concerned with uniform recovery performance, in each trial, we record the worst-\ncase reconstruction performance (i.e., the smallest cosine similarity or the largest relative error) over\nthe 30 test images, and the worst-case cosine similarity or relative error is averaged over 10 trials.\nFigures 2, 3, and 4 show that for noiseless 1-bit measurements and uniformly quantized measurements\nwith dithering with $$\\delta = 3$$, the CSGM approach can produce reasonably accurate reconstruction for all\nthe test images when the number of measurements m is as small as 150 and 100 respectively.\n\nG.3 Experimental Results for ReLU and Dithered 1-bit Measurements\nWe present the experimental results for the ReLU link function and dithered 1-bit measurements\nin Figures 5, 6, and 7. For dithered 1-bit measurements, we set $$\\lambda = R\\sqrt{\\log m}$$ with $$R > 0$$ being\na tuning parameter. For the case of using the ReLU link function, similarly to noiseless 1-bit\nmeasurements, we calculate the cosine similarity to measure the reconstruction performance. For\ndithered 1-bit measurements, similarly to uniformly quantized measurements with dithering, we\ncalculate the relative \u21132-norm distance. We observe that for these two nonlinear measurement models"}]}, {"page": 27, "text": "Original\nLasso\nCSGM\n                   Figure 3: Reconstructed images of the MNIST dataset for UQD with m = 100 and \u03b4 = 3.\n                 0.9                                                               1.0                                                    1.0                                                Lasso\n                 0.8                                                                                                                                                                         CSGM\n                Cosine Similarity                                                  0.9                                                    0.9\n                 0.7                                                              Relative Error                                         Relative Error\n                 0.6                                                               0.8                                                    0.8\n                 0.5                                                               0.7                                                    0.7\n                 0.4\n                 0.3                                                Lasso          0.6                                            Lasso   0.6\n                 0.2                                                CSGM           0.5                                            CSGM    0.5\n                 0.1   50    75    100 125 150 175 200 225 250                           50    75    100 125 150 175 200 225 250              0.0    2.5   5.0    7.5 10.0 12.5 15.0 17.5 20.0\n                                                m                                                                 m\n                       (a) 1-bit with varying m                                      (b) UQD with fixed \u03b4 = 3                             (c) UQD with fixed m = 200\n         Figure 4: Quantitative results of the performance of CSGM for 1-bit and UQD measurements on the\n         MNIST dataset.\n         with a single realization of the random measurement ensemble, CSGM can also lead to reasonably\n         good reconstruction for all the test images when the number of measurements is small compared to\n         the ambient dimension.\n         H          Experimental Results for the CelebA dataset\n         In this section, we present numerical results for the CelebA dataset [35], which contains more\n         than 200,000 face images for celebrities with an ambient dimension of n = 12288. We train\n         a deep convolutional generative adversarial network (DCGAN) following the settings in https:\n         //pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html. The latent dimension\n         of the generator is k = 100 and the number of epochs for training is 20. Since the experiments for\n         CelebA are more time-consuming than those of MNIST, we select 20 images from the test set of\n         CelebA and perform 5 random trials. Other settings are the same as those for the MNIST dataset.\n         Since we have observed from the numerical results for MNIST that the experiments for the ReLU\n         link function and dithered 1-bit measurements are similar, we only present the results for noiseless\n         1-bit measurements and uniformly quantized observations with dithering.\n         From Figures 8 and 10, we observe that for noiseless 1-bit measurements with 1500 samples, a single\n         measurement matrix A can lead to reasonably accurate reconstruction for all the 20 test images.\n         In addition, from Figures 9 and 10, we observe that for uniformly quantized measurements with\nOriginal\nLasso\nCSGM\n         Figure 5: Reconstructed images of the MNIST dataset for the ReLU link function with m = 150 and\n         \u03c3 = 0.2.\n                                                                                                           27", "md": "Original\n\nLasso\n\nCSGM\n\n$$\n\\begin{array}{ccc}\n\\text{Figure 3: Reconstructed images of the MNIST dataset for UQD with } m = 100 \\text{ and } \\delta = 3. \\\\\n0.9 & & 1.0 & & 1.0 & & \\text{Lasso} \\\\\n0.8 & & & & & & & & & \\text{CSGM} \\\\\n\\text{Cosine Similarity} & & 0.9 & & 0.9 \\\\\n0.7 & & \\text{Relative Error} & & \\text{Relative Error} \\\\\n0.6 & & 0.8 & & 0.8 \\\\\n0.5 & & 0.7 & & 0.7 \\\\\n0.4 & & & & & & \\text{Lasso} & & 0.6 & & \\text{Lasso} & & 0.6 \\\\\n0.3 & & & & & & \\text{CSGM} & & 0.5 & & \\text{CSGM} & & 0.5 \\\\\n0.1 & 50 & 75 & 100 & 125 & 150 & 175 & 200 & 225 & 250 & & 50 & 75 & 100 & 125 & 150 & 175 & 200 & 225 & 250 & & 0.0 & 2.5 & 5.0 & 7.5 & 10.0 & 12.5 & 15.0 & 17.5 & 20.0 \\\\\n& & m & & & & & & & & & & m \\\\\n(a) \\text{1-bit with varying m} & & (b) \\text{UQD with fixed } \\delta = 3 & & (c) \\text{UQD with fixed } m = 200\n\\end{array}\n$$\n\nFigure 4: Quantitative results of the performance of CSGM for 1-bit and UQD measurements on the MNIST dataset.\n\nWith a single realization of the random measurement ensemble, CSGM can also lead to reasonably good reconstruction for all the test images when the number of measurements is small compared to the ambient dimension.\n\nExperimental Results for the CelebA dataset\n\nIn this section, we present numerical results for the CelebA dataset [35], which contains more than 200,000 face images for celebrities with an ambient dimension of n = 12288. We train a deep convolutional generative adversarial network (DCGAN) following the settings in https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html. The latent dimension of the generator is k = 100 and the number of epochs for training is 20. Since the experiments for CelebA are more time-consuming than those of MNIST, we select 20 images from the test set of CelebA and perform 5 random trials. Other settings are the same as those for the MNIST dataset.\n\nSince we have observed from the numerical results for MNIST that the experiments for the ReLU link function and dithered 1-bit measurements are similar, we only present the results for noiseless 1-bit measurements and uniformly quantized observations with dithering.\n\nFrom Figures 8 and 10, we observe that for noiseless 1-bit measurements with 1500 samples, a single measurement matrix A can lead to reasonably accurate reconstruction for all the 20 test images. In addition, from Figures 9 and 10, we observe that for uniformly quantized measurements with\n\nOriginal\n\nLasso\n\nCSGM\n\nFigure 5: Reconstructed images of the MNIST dataset for the ReLU link function with m = 150 and \u03c3 = 0.2.\n\n27", "images": [], "items": [{"type": "text", "value": "Original\n\nLasso\n\nCSGM\n\n$$\n\\begin{array}{ccc}\n\\text{Figure 3: Reconstructed images of the MNIST dataset for UQD with } m = 100 \\text{ and } \\delta = 3. \\\\\n0.9 & & 1.0 & & 1.0 & & \\text{Lasso} \\\\\n0.8 & & & & & & & & & \\text{CSGM} \\\\\n\\text{Cosine Similarity} & & 0.9 & & 0.9 \\\\\n0.7 & & \\text{Relative Error} & & \\text{Relative Error} \\\\\n0.6 & & 0.8 & & 0.8 \\\\\n0.5 & & 0.7 & & 0.7 \\\\\n0.4 & & & & & & \\text{Lasso} & & 0.6 & & \\text{Lasso} & & 0.6 \\\\\n0.3 & & & & & & \\text{CSGM} & & 0.5 & & \\text{CSGM} & & 0.5 \\\\\n0.1 & 50 & 75 & 100 & 125 & 150 & 175 & 200 & 225 & 250 & & 50 & 75 & 100 & 125 & 150 & 175 & 200 & 225 & 250 & & 0.0 & 2.5 & 5.0 & 7.5 & 10.0 & 12.5 & 15.0 & 17.5 & 20.0 \\\\\n& & m & & & & & & & & & & m \\\\\n(a) \\text{1-bit with varying m} & & (b) \\text{UQD with fixed } \\delta = 3 & & (c) \\text{UQD with fixed } m = 200\n\\end{array}\n$$\n\nFigure 4: Quantitative results of the performance of CSGM for 1-bit and UQD measurements on the MNIST dataset.\n\nWith a single realization of the random measurement ensemble, CSGM can also lead to reasonably good reconstruction for all the test images when the number of measurements is small compared to the ambient dimension.\n\nExperimental Results for the CelebA dataset\n\nIn this section, we present numerical results for the CelebA dataset [35], which contains more than 200,000 face images for celebrities with an ambient dimension of n = 12288. We train a deep convolutional generative adversarial network (DCGAN) following the settings in https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html. The latent dimension of the generator is k = 100 and the number of epochs for training is 20. Since the experiments for CelebA are more time-consuming than those of MNIST, we select 20 images from the test set of CelebA and perform 5 random trials. Other settings are the same as those for the MNIST dataset.\n\nSince we have observed from the numerical results for MNIST that the experiments for the ReLU link function and dithered 1-bit measurements are similar, we only present the results for noiseless 1-bit measurements and uniformly quantized observations with dithering.\n\nFrom Figures 8 and 10, we observe that for noiseless 1-bit measurements with 1500 samples, a single measurement matrix A can lead to reasonably accurate reconstruction for all the 20 test images. In addition, from Figures 9 and 10, we observe that for uniformly quantized measurements with\n\nOriginal\n\nLasso\n\nCSGM\n\nFigure 5: Reconstructed images of the MNIST dataset for the ReLU link function with m = 150 and \u03c3 = 0.2.\n\n27", "md": "Original\n\nLasso\n\nCSGM\n\n$$\n\\begin{array}{ccc}\n\\text{Figure 3: Reconstructed images of the MNIST dataset for UQD with } m = 100 \\text{ and } \\delta = 3. \\\\\n0.9 & & 1.0 & & 1.0 & & \\text{Lasso} \\\\\n0.8 & & & & & & & & & \\text{CSGM} \\\\\n\\text{Cosine Similarity} & & 0.9 & & 0.9 \\\\\n0.7 & & \\text{Relative Error} & & \\text{Relative Error} \\\\\n0.6 & & 0.8 & & 0.8 \\\\\n0.5 & & 0.7 & & 0.7 \\\\\n0.4 & & & & & & \\text{Lasso} & & 0.6 & & \\text{Lasso} & & 0.6 \\\\\n0.3 & & & & & & \\text{CSGM} & & 0.5 & & \\text{CSGM} & & 0.5 \\\\\n0.1 & 50 & 75 & 100 & 125 & 150 & 175 & 200 & 225 & 250 & & 50 & 75 & 100 & 125 & 150 & 175 & 200 & 225 & 250 & & 0.0 & 2.5 & 5.0 & 7.5 & 10.0 & 12.5 & 15.0 & 17.5 & 20.0 \\\\\n& & m & & & & & & & & & & m \\\\\n(a) \\text{1-bit with varying m} & & (b) \\text{UQD with fixed } \\delta = 3 & & (c) \\text{UQD with fixed } m = 200\n\\end{array}\n$$\n\nFigure 4: Quantitative results of the performance of CSGM for 1-bit and UQD measurements on the MNIST dataset.\n\nWith a single realization of the random measurement ensemble, CSGM can also lead to reasonably good reconstruction for all the test images when the number of measurements is small compared to the ambient dimension.\n\nExperimental Results for the CelebA dataset\n\nIn this section, we present numerical results for the CelebA dataset [35], which contains more than 200,000 face images for celebrities with an ambient dimension of n = 12288. We train a deep convolutional generative adversarial network (DCGAN) following the settings in https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html. The latent dimension of the generator is k = 100 and the number of epochs for training is 20. Since the experiments for CelebA are more time-consuming than those of MNIST, we select 20 images from the test set of CelebA and perform 5 random trials. Other settings are the same as those for the MNIST dataset.\n\nSince we have observed from the numerical results for MNIST that the experiments for the ReLU link function and dithered 1-bit measurements are similar, we only present the results for noiseless 1-bit measurements and uniformly quantized observations with dithering.\n\nFrom Figures 8 and 10, we observe that for noiseless 1-bit measurements with 1500 samples, a single measurement matrix A can lead to reasonably accurate reconstruction for all the 20 test images. In addition, from Figures 9 and 10, we observe that for uniformly quantized measurements with\n\nOriginal\n\nLasso\n\nCSGM\n\nFigure 5: Reconstructed images of the MNIST dataset for the ReLU link function with m = 150 and \u03c3 = 0.2.\n\n27"}]}, {"page": 28, "text": "     Original\n     Lasso\n     CSGM\n              Figure 6: Examples of reconstructed images of the MNIST dataset for dithered 1-bit measurements\n              with m = 250 and R = 5.\n                      0.8                                                               0.7                                               Lasso       1.0                                       Lasso\n                     Cosine Similarity                                                 Cosine Similarity                                  CSGM        0.9                                       CSGM\n                      0.7                                                               0.6                                                          Relative Error\n                      0.6                                                               0.5                                                           0.8\n                      0.5                                                               0.4\n                      0.4                                                               0.3                                                           0.7\n                      0.3\n                      0.2                                                Lasso          0.2                                                           0.6\n                      0.1                                                CSGM           0.1                                                           0.5\n                            50    75    100 125 150 175 200 225 250                              0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00                   150 200 250 300 350 400 450 500 550\n                                                     m                                                                                                                               m\n                        (a) ReLU with fixed \u03c3 = 0.1                                    (b) ReLU with fixed m = 150                               (c) Dithered 1-bit with fixed R = 5\n              Figure 7: Quantitative results of the performance of CSGM for the ReLU link function and dithered\n              1-bit measurements on the MNIST dataset.\n              dithering, a single realization of the measurement matrix and random dither is sufficient for the\n              reasonably accurate recovery of the 20 test images when m = 1000 and \u03b4 = 20.\nOriginal\nCSGM\n              Figure 8: Reconstructed images of the CelebA dataset for the noiseless 1-bit measurements with\n              m = 1500.\n                                                                                                                28", "md": "Original\n\nLasso\n\nCSGM\n\nFigure 6: Examples of reconstructed images of the MNIST dataset for dithered 1-bit measurements with m = 250 and R = 5.\n\n| |0.8|0.7|\n|---|---|---|\n|Cosine Similarity|Lasso|1.0|\n| |CSGM|0.9|\n|0.7|0.6| |\n|0.6|0.5| |\n|0.5|0.4| |\n|0.4|0.3| |\n|0.3| | |\n|0.2|Lasso|0.2|\n|0.1|CSGM|0.1|\n| |50|75|100|125|150|175|200|225|250|\n| |0.25|0.50|0.75|1.00|1.25|1.50|1.75|2.00|\n| |150|200|250|300|350|400|450|500|550|\n|m| | |\n\n(a) ReLU with fixed $$\\sigma = 0.1$$\n\n(b) ReLU with fixed m = 150\n\n(c) Dithered 1-bit with fixed R = 5\n\nFigure 7: Quantitative results of the performance of CSGM for the ReLU link function and dithered 1-bit measurements on the MNIST dataset.\n\nOriginal\n\nCSGM\n\nFigure 8: Reconstructed images of the CelebA dataset for the noiseless 1-bit measurements with m = 1500.\n\n28", "images": [], "items": [{"type": "text", "value": "Original\n\nLasso\n\nCSGM\n\nFigure 6: Examples of reconstructed images of the MNIST dataset for dithered 1-bit measurements with m = 250 and R = 5.", "md": "Original\n\nLasso\n\nCSGM\n\nFigure 6: Examples of reconstructed images of the MNIST dataset for dithered 1-bit measurements with m = 250 and R = 5."}, {"type": "table", "rows": [["", "0.8", "0.7"], ["Cosine Similarity", "Lasso", "1.0"], ["", "CSGM", "0.9"], ["0.7", "0.6", ""], ["0.6", "0.5", ""], ["0.5", "0.4", ""], ["0.4", "0.3", ""], ["0.3", "", ""], ["0.2", "Lasso", "0.2"], ["0.1", "CSGM", "0.1"], ["", "50", "75", "100", "125", "150", "175", "200", "225", "250"], ["", "0.25", "0.50", "0.75", "1.00", "1.25", "1.50", "1.75", "2.00"], ["", "150", "200", "250", "300", "350", "400", "450", "500", "550"], ["m", "", ""]], "md": "| |0.8|0.7|\n|---|---|---|\n|Cosine Similarity|Lasso|1.0|\n| |CSGM|0.9|\n|0.7|0.6| |\n|0.6|0.5| |\n|0.5|0.4| |\n|0.4|0.3| |\n|0.3| | |\n|0.2|Lasso|0.2|\n|0.1|CSGM|0.1|\n| |50|75|100|125|150|175|200|225|250|\n| |0.25|0.50|0.75|1.00|1.25|1.50|1.75|2.00|\n| |150|200|250|300|350|400|450|500|550|\n|m| | |", "isPerfectTable": false, "csv": "\"\",\"0.8\",\"0.7\"\n\"Cosine Similarity\",\"Lasso\",\"1.0\"\n\"\",\"CSGM\",\"0.9\"\n\"0.7\",\"0.6\",\"\"\n\"0.6\",\"0.5\",\"\"\n\"0.5\",\"0.4\",\"\"\n\"0.4\",\"0.3\",\"\"\n\"0.3\",\"\",\"\"\n\"0.2\",\"Lasso\",\"0.2\"\n\"0.1\",\"CSGM\",\"0.1\"\n\"\",\"50\",\"75\",\"100\",\"125\",\"150\",\"175\",\"200\",\"225\",\"250\"\n\"\",\"0.25\",\"0.50\",\"0.75\",\"1.00\",\"1.25\",\"1.50\",\"1.75\",\"2.00\"\n\"\",\"150\",\"200\",\"250\",\"300\",\"350\",\"400\",\"450\",\"500\",\"550\"\n\"m\",\"\",\"\""}, {"type": "text", "value": "(a) ReLU with fixed $$\\sigma = 0.1$$\n\n(b) ReLU with fixed m = 150\n\n(c) Dithered 1-bit with fixed R = 5\n\nFigure 7: Quantitative results of the performance of CSGM for the ReLU link function and dithered 1-bit measurements on the MNIST dataset.\n\nOriginal\n\nCSGM\n\nFigure 8: Reconstructed images of the CelebA dataset for the noiseless 1-bit measurements with m = 1500.\n\n28", "md": "(a) ReLU with fixed $$\\sigma = 0.1$$\n\n(b) ReLU with fixed m = 150\n\n(c) Dithered 1-bit with fixed R = 5\n\nFigure 7: Quantitative results of the performance of CSGM for the ReLU link function and dithered 1-bit measurements on the MNIST dataset.\n\nOriginal\n\nCSGM\n\nFigure 8: Reconstructed images of the CelebA dataset for the noiseless 1-bit measurements with m = 1500.\n\n28"}]}, {"page": 29, "text": "Original\nCSGM\n               Figure 9: Reconstructed images of the CelebA dataset for UQD with m = 1000 and \u03b4 = 20.\n                0.9                                               1.1                                    CSGM   1.2        CSGM\n                0.8                                               1.0\n               Cosine Similarity                                                                                1.1\n                                                                 Relative Error                                Relative Error\n                0.7                                               0.9                                           1.0\n                0.6                                               0.8                                           0.9\n                0.5                                               0.7                                           0.8\n                0.4                                               0.6                                           0.7\n                                                                  0.5                                           0.6\n                0.3                                   CSGM                                                      0.5\n                   0    200  400  600  800 1000 1200 1400             0   200  400   600  800 1000 1200 1400       0     50    100   150   200  250  300\n                                       m                                                  m\n                    (a) 1-bit with varying m                       (b) UQD with fixed \u03b4 = 20                    (c) UQD with fixed m = 500\n          Figure 10: Quantitative results of the performance of CSGM for 1-bit and UQD measurements on the\n          CelebA dataset.\n                                                                                      29", "md": "CSGM\n\n| |0.9|1.1|CSGM|1.2|CSGM|\n|---|---|---|---|---|---|\n|0.8| |1.0| | | |\n|Cosine Similarity| | | |1.1| |\n|Relative Error| |0.9| |1.0| |\n|0.7| |0.9| |1.0| |\n|0.6| |0.8| |0.9| |\n|0.5| |0.7| |0.8| |\n|0.4| |0.6| |0.7| |\n| | |0.5| |0.6| |\n|0.3|CSGM| | |0.5| |\n\n(a) 1-bit with varying m\n\n(b) UQD with fixed \u03b4 = 20\n\n(c) UQD with fixed m = 500\n\nFigure 10: Quantitative results of the performance of CSGM for 1-bit and UQD measurements on the CelebA dataset.\n\n29", "images": [], "items": [{"type": "text", "value": "CSGM", "md": "CSGM"}, {"type": "table", "rows": [["", "0.9", "1.1", "CSGM", "1.2", "CSGM"], ["0.8", "", "1.0", "", "", ""], ["Cosine Similarity", "", "", "", "1.1", ""], ["Relative Error", "", "0.9", "", "1.0", ""], ["0.7", "", "0.9", "", "1.0", ""], ["0.6", "", "0.8", "", "0.9", ""], ["0.5", "", "0.7", "", "0.8", ""], ["0.4", "", "0.6", "", "0.7", ""], ["", "", "0.5", "", "0.6", ""], ["0.3", "CSGM", "", "", "0.5", ""]], "md": "| |0.9|1.1|CSGM|1.2|CSGM|\n|---|---|---|---|---|---|\n|0.8| |1.0| | | |\n|Cosine Similarity| | | |1.1| |\n|Relative Error| |0.9| |1.0| |\n|0.7| |0.9| |1.0| |\n|0.6| |0.8| |0.9| |\n|0.5| |0.7| |0.8| |\n|0.4| |0.6| |0.7| |\n| | |0.5| |0.6| |\n|0.3|CSGM| | |0.5| |", "isPerfectTable": true, "csv": "\"\",\"0.9\",\"1.1\",\"CSGM\",\"1.2\",\"CSGM\"\n\"0.8\",\"\",\"1.0\",\"\",\"\",\"\"\n\"Cosine Similarity\",\"\",\"\",\"\",\"1.1\",\"\"\n\"Relative Error\",\"\",\"0.9\",\"\",\"1.0\",\"\"\n\"0.7\",\"\",\"0.9\",\"\",\"1.0\",\"\"\n\"0.6\",\"\",\"0.8\",\"\",\"0.9\",\"\"\n\"0.5\",\"\",\"0.7\",\"\",\"0.8\",\"\"\n\"0.4\",\"\",\"0.6\",\"\",\"0.7\",\"\"\n\"\",\"\",\"0.5\",\"\",\"0.6\",\"\"\n\"0.3\",\"CSGM\",\"\",\"\",\"0.5\",\"\""}, {"type": "text", "value": "(a) 1-bit with varying m\n\n(b) UQD with fixed \u03b4 = 20\n\n(c) UQD with fixed m = 500\n\nFigure 10: Quantitative results of the performance of CSGM for 1-bit and UQD measurements on the CelebA dataset.\n\n29", "md": "(a) 1-bit with varying m\n\n(b) UQD with fixed \u03b4 = 20\n\n(c) UQD with fixed m = 500\n\nFigure 10: Quantitative results of the performance of CSGM for 1-bit and UQD measurements on the CelebA dataset.\n\n29"}]}], "job_id": "5ab19447-50ec-4a81-82b2-4dd986ef6471", "file_path": "./corpus/2310.03758.pdf"}