{"pages": [{"page": 1, "text": "                               Greedy Pruning with Group Lasso Provably\n                                            Generalizes for Matrix Sensing\n                                             Nived Rajaraman \u2217                                      Devvrit \u2020\n                                    University of California, Berkeley                             UT Austin\n                               nived.rajaraman@eecs.berkeley.edu                          devvrit.03@gmail.com\narXiv:2303.11453v2  [cs.LG]  4 Jun 2023  Aryan Mokhtari\u2021                             Kannan Ramchandran \u2217\n                                             UT Austin                           University of California, Berkeley\n                                mokhtari@austin.utexas.edu                        kannanr@eecs.berkeley.edu\n                                                                     Abstract\n                               Pruning schemes have been widely used in practice to reduce the complexity of\n                               trained models with a massive number of parameters. In fact, several practical\n                               studies have shown that if a pruned model is fine-tuned with some gradient-based\n                               updates it generalizes well to new samples. Although the above pipeline, which\n                               we refer to as pruning + fine-tuning, has been extremely successful in lowering the\n                               complexity of trained models, there is very little known about the theory behind\n                               this success. In this paper, we address this issue by investigating the pruning +\n                               fine-tuning framework on the overparameterized matrix sensing problem with the\n                               ground truth U\u22c6   \u2208 Rd\u00d7r and the overparameterized model U \u2208         Rd\u00d7k with k \u226b     r.\n                               We study the approximate local minima of the mean square error, augmented\n                               with a smooth version of a group Lasso regularizer,  k       i=1 \u2225Uei\u22252. In particular,\n                               we provably show that pruning all the columns below a certain explicit \u21132-norm\n                               threshold results in a solution Uprune which has the minimum number of columns\n                               r, yet close to the ground truth in training loss. Moreover, in the subsequent fine-\n                               tuning phase, gradient descent initialized at Uprune converges at a linear rate to its\n                               limit. While our analysis provides insights into the role of regularization in pruning,\n                               we also show that running gradient descent in the absence of regularization results\n                               in models which are not suitable for greedy pruning, i.e., many columns could have\n                               their \u21132 norm comparable to that of the maximum. To the best of our knowledge,\n                               our results provide the first rigorous insights on why greedy pruning + fine-tuning\n                               leads to smaller models which also generalize well.\n                     1    Introduction\n                     Training overparameterized models with a massive number of parameters has become the norm in\n                     almost all machine learning applications. While these massive models are successful in achieving\n                     low training error and in some cases good generalization performance, they are hard to store or\n                     communicate. Moreover, inference with such large models is computationally prohibitive. To\n                     address these issues, a large effort has gone into compressing these overparameterized models via\n                     different approaches, such as quantization schemes [1, 2], unstructured [3] and structured [4\u20136]\n                     pruning mechanisms, and distillation techniques using student-teacher models [7, 8]. Among these\n                        \u2217Dept. of Electrical Engineering and Computer Sciences, UC Berkeley\n                        \u2020Dept. of Computer Science, UT Austin\n                        \u2021Dept. of Electrical and Computer Engineering, UT Austin\n                     Preprint. Under review.", "md": "# Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing\n\n# Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing\n\nNived Rajaraman \u2217 University of California, Berkeley\n\nDevvrit \u2020 UT Austin\n\nEmail: nived.rajaraman@eecs.berkeley.edu, devvrit.03@gmail.com\n\nAryan Mokhtari\u2021 UT Austin\n\nKannan Ramchandran \u2217 University of California, Berkeley\n\nEmail: mokhtari@austin.utexas.edu, kannanr@eecs.berkeley.edu\n\n## Abstract\n\nPruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. In fact, several practical studies have shown that if a pruned model is fine-tuned with some gradient-based updates it generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper, we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem with the ground truth $$U^* \\in \\mathbb{R}^{d \\times r}$$ and the overparameterized model $$U \\in \\mathbb{R}^{d \\times k}$$ with $$k \\gg r$$. We study the approximate local minima of the mean square error, augmented with a smooth version of a group Lasso regularizer, $$\\sum_{i=1}^{k} \\|Ue_i\\|_2$$. In particular, we provably show that pruning all the columns below a certain explicit $$\\ell_2$$-norm threshold results in a solution $$U_{\\text{prune}}$$ which has the minimum number of columns r, yet close to the ground truth in training loss. Moreover, in the subsequent fine-tuning phase, gradient descent initialized at $$U_{\\text{prune}}$$ converges at a linear rate to its limit. While our analysis provides insights into the role of regularization in pruning, we also show that running gradient descent in the absence of regularization results in models which are not suitable for greedy pruning, i.e., many columns could have their $$\\ell_2$$ norm comparable to that of the maximum. To the best of our knowledge, our results provide the first rigorous insights on why greedy pruning + fine-tuning leads to smaller models which also generalize well.\n\n## Introduction\n\nTraining overparameterized models with a massive number of parameters has become the norm in almost all machine learning applications. While these massive models are successful in achieving low training error and in some cases good generalization performance, they are hard to store or communicate. Moreover, inference with such large models is computationally prohibitive. To address these issues, a large effort has gone into compressing these overparameterized models via different approaches, such as quantization schemes [1, 2], unstructured [3] and structured [4\u20136] pruning mechanisms, and distillation techniques using student-teacher models [7, 8]. Among these\n\n\u2217Dept. of Electrical Engineering and Computer Sciences, UC Berkeley\n\n\u2020Dept. of Computer Science, UT Austin\n\n\u2021Dept. of Electrical and Computer Engineering, UT Austin\n\nPreprint. Under review.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing", "md": "# Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing"}, {"type": "heading", "lvl": 1, "value": "Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing", "md": "# Greedy Pruning with Group Lasso Provably Generalizes for Matrix Sensing"}, {"type": "text", "value": "Nived Rajaraman \u2217 University of California, Berkeley\n\nDevvrit \u2020 UT Austin\n\nEmail: nived.rajaraman@eecs.berkeley.edu, devvrit.03@gmail.com\n\nAryan Mokhtari\u2021 UT Austin\n\nKannan Ramchandran \u2217 University of California, Berkeley\n\nEmail: mokhtari@austin.utexas.edu, kannanr@eecs.berkeley.edu", "md": "Nived Rajaraman \u2217 University of California, Berkeley\n\nDevvrit \u2020 UT Austin\n\nEmail: nived.rajaraman@eecs.berkeley.edu, devvrit.03@gmail.com\n\nAryan Mokhtari\u2021 UT Austin\n\nKannan Ramchandran \u2217 University of California, Berkeley\n\nEmail: mokhtari@austin.utexas.edu, kannanr@eecs.berkeley.edu"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. In fact, several practical studies have shown that if a pruned model is fine-tuned with some gradient-based updates it generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper, we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem with the ground truth $$U^* \\in \\mathbb{R}^{d \\times r}$$ and the overparameterized model $$U \\in \\mathbb{R}^{d \\times k}$$ with $$k \\gg r$$. We study the approximate local minima of the mean square error, augmented with a smooth version of a group Lasso regularizer, $$\\sum_{i=1}^{k} \\|Ue_i\\|_2$$. In particular, we provably show that pruning all the columns below a certain explicit $$\\ell_2$$-norm threshold results in a solution $$U_{\\text{prune}}$$ which has the minimum number of columns r, yet close to the ground truth in training loss. Moreover, in the subsequent fine-tuning phase, gradient descent initialized at $$U_{\\text{prune}}$$ converges at a linear rate to its limit. While our analysis provides insights into the role of regularization in pruning, we also show that running gradient descent in the absence of regularization results in models which are not suitable for greedy pruning, i.e., many columns could have their $$\\ell_2$$ norm comparable to that of the maximum. To the best of our knowledge, our results provide the first rigorous insights on why greedy pruning + fine-tuning leads to smaller models which also generalize well.", "md": "Pruning schemes have been widely used in practice to reduce the complexity of trained models with a massive number of parameters. In fact, several practical studies have shown that if a pruned model is fine-tuned with some gradient-based updates it generalizes well to new samples. Although the above pipeline, which we refer to as pruning + fine-tuning, has been extremely successful in lowering the complexity of trained models, there is very little known about the theory behind this success. In this paper, we address this issue by investigating the pruning + fine-tuning framework on the overparameterized matrix sensing problem with the ground truth $$U^* \\in \\mathbb{R}^{d \\times r}$$ and the overparameterized model $$U \\in \\mathbb{R}^{d \\times k}$$ with $$k \\gg r$$. We study the approximate local minima of the mean square error, augmented with a smooth version of a group Lasso regularizer, $$\\sum_{i=1}^{k} \\|Ue_i\\|_2$$. In particular, we provably show that pruning all the columns below a certain explicit $$\\ell_2$$-norm threshold results in a solution $$U_{\\text{prune}}$$ which has the minimum number of columns r, yet close to the ground truth in training loss. Moreover, in the subsequent fine-tuning phase, gradient descent initialized at $$U_{\\text{prune}}$$ converges at a linear rate to its limit. While our analysis provides insights into the role of regularization in pruning, we also show that running gradient descent in the absence of regularization results in models which are not suitable for greedy pruning, i.e., many columns could have their $$\\ell_2$$ norm comparable to that of the maximum. To the best of our knowledge, our results provide the first rigorous insights on why greedy pruning + fine-tuning leads to smaller models which also generalize well."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Training overparameterized models with a massive number of parameters has become the norm in almost all machine learning applications. While these massive models are successful in achieving low training error and in some cases good generalization performance, they are hard to store or communicate. Moreover, inference with such large models is computationally prohibitive. To address these issues, a large effort has gone into compressing these overparameterized models via different approaches, such as quantization schemes [1, 2], unstructured [3] and structured [4\u20136] pruning mechanisms, and distillation techniques using student-teacher models [7, 8]. Among these\n\n\u2217Dept. of Electrical Engineering and Computer Sciences, UC Berkeley\n\n\u2020Dept. of Computer Science, UT Austin\n\n\u2021Dept. of Electrical and Computer Engineering, UT Austin\n\nPreprint. Under review.", "md": "Training overparameterized models with a massive number of parameters has become the norm in almost all machine learning applications. While these massive models are successful in achieving low training error and in some cases good generalization performance, they are hard to store or communicate. Moreover, inference with such large models is computationally prohibitive. To address these issues, a large effort has gone into compressing these overparameterized models via different approaches, such as quantization schemes [1, 2], unstructured [3] and structured [4\u20136] pruning mechanisms, and distillation techniques using student-teacher models [7, 8]. Among these\n\n\u2217Dept. of Electrical Engineering and Computer Sciences, UC Berkeley\n\n\u2020Dept. of Computer Science, UT Austin\n\n\u2021Dept. of Electrical and Computer Engineering, UT Austin\n\nPreprint. Under review."}]}, {"page": 2, "text": "approaches, greedy pruning, in which we greedily eliminate the parameters of the trained model\nbased on some measure (e.g., the norms of the weight vectors associated with individual neurons) has\nreceived widespread attention [3, 9\u201313]. This is mostly due to the fact that several practical studies\nhave illustrated that training an overparameterized model followed by greedy pruning and fine-tuning\nleads to better generalization performance, compared to an overparameterized model trained without\npruning [14]. Furthermore, a phenomenon that has been observed by several practical studies, is\nthat different forms of regularization during training, such as \u21130 or \u21131 regularization [15\u201317] or \u21132\nregularization including group Lasso [18] lead to models that are better suited for pruning, and\nleading to better generalization post fine-tuning.\nWhile the greedy pruning framework has shown impressive results, there is little to no theory backing\nwhy this pipeline works well in practice, nor understanding of the role of regularization in helping\ngenerate models which are suitable for greedy pruning. In this work, we address the following\nquestions:\n         Does the greedy pruning + fine-tuning pipeline provably lead to a simple model\n         with good generalization guarantees? What is the role of regularization in pruning?\nIn this paper, we use the symmetric matrix sensing problem [19] as a test-ground for the analysis of\ngreedy pruning framework, a model very closely related to shallow neural networks with quadratic\nactivation functions [20, 19]. In this setting, the underlying problem for the population loss (infinite\nsamples) is defined as \u2225UU T \u2212     U\u22c6U T\u22c6\u22252F , where U\u22c6  \u2208  Rd\u00d7r is an unknown ground-truth rank-r\nmatrix, with r also being unknown, and U \u2208       Rd\u00d7k is the overparameterized learning model with\nk \u226b  r. As we discuss in the Appendix F, the columns of U can be thought of as the weight vectors\nassociated with individual neurons in a 2-layer shallow neural network with quadratic activation\nfunctions, a connection first observed in [21]. Thus, the data generating model has r neurons, while\nthe learner trains an overparameterized model with k neurons.\nWhile the statistical and computational complexity of the overparameterized matrix sensing problem\nhas been studied extensively, we use it as a model for understanding the efficacy of greedy pruning.\nIn particular, we aim to answer the following questions: Does there exist a simple pruning criteria\nfor which we can provably show that the pruned model generalizes well after fine-tuning while\nhaving the minimal necessary number of parameters? What is the role of regularization during\ntraining in promoting models which are compatible with greedy pruning? Finally, what generalization\nguarantees can we establish for the pruned model post fine-tuning?\nContributions. Our main contribution is to show that the discussed pruning pipeline not only recovers\nthe correct ground-truth U\u22c6U T\u22c6 approximately, but also automatically adapts to the correct number of\ncolumns r. In particular, we show that training an overparameterized model on the empirical mean\nsquared error with an added group Lasso based regularizer to promote column sparsity, followed by a\nsimple norm-based pruning strategy results in a model Uprune having exactly the minimum number\nof columns, r. At the same time, we show that \u2225UpruneU T      prune \u2212U\u22c6U T\u22c6 \u22252\n                                                                             F is small, but non-zero.\nHence, the pruned model can subsequently be fine-tuned using a small number of gradient steps,\nand in this regime, \u2225UtU Tt \u2212  U\u22c6U T\u22c6 \u22252\n                                       F shows linear convergence to its limit. Moreover, the pruned\nmodel can be shown to admit finite sample generalization bounds which are also statistically optimal\nfor a range of parameters. In particular, we show that to obtain a model Uout that has exactly r\ncolumns and its population error is at most \u2225UoutU T   out \u2212 U\u22c6U T\u22c6 \u2225F \u2264   \u03b5, our framework requires\nO(dk2r5 + rd \u03b52 ) samples, which is statistically optimal for suffi\n                                                                  ciently small \u03b5. We should also add\nthat our framework does not require any computationally prohibitive pre- or post-processing (such as\nSVD decomposition) for achieving this result.\nWhile there are several works [22, 19, 23] establishing that gradient descent in the \u201cexactly-\nparameterized\u201d setting requires O(rd/\u03b52) samples to achieve a generalization error of \u03b5, and con-\nverges linearly to this limit, the picture is different in the overparameterized setting. In [23], the\nauthors showed that in the overparametrized setting, vanilla gradient descent requires O(kd/\u03b52)\nsamples to achieve a generalization error of \u2225UgdU Tgd \u2212 U\u22c6U T\u22c6 \u2225F \u2264  \u03b5, degrading with the overparam-\neterization of the model. Moreover the resulting solution does not have the correct column sparsity.\nIn order to obtain a model which can be stored concisely, Ugd has to be post-processed by computing\nits SVD, which is computationally expensive in the high dimensional regime.\nAs our second continuation, we show that use of explicit regularization to promote column sparsity\nwhile training is important to learn models suitable for greedy pruning. Specifically, we show that\nwhile implicit regularization [19, 24] suffices to learn models with the correct rank, these approaches\n                                                   2", "md": "Approaches, greedy pruning, in which we greedily eliminate the parameters of the trained model based on some measure (e.g., the norms of the weight vectors associated with individual neurons) has received widespread attention [3, 9\u201313]. This is mostly due to the fact that several practical studies have illustrated that training an overparameterized model followed by greedy pruning and fine-tuning leads to better generalization performance, compared to an overparameterized model trained without pruning [14]. Furthermore, a phenomenon that has been observed by several practical studies is that different forms of regularization during training, such as \u21130 or \u21131 regularization [15\u201317] or \u21132 regularization including group Lasso [18], lead to models that are better suited for pruning and leading to better generalization post fine-tuning.\n\nWhile the greedy pruning framework has shown impressive results, there is little to no theory backing why this pipeline works well in practice, nor understanding of the role of regularization in helping generate models which are suitable for greedy pruning. In this work, we address the following questions:\n\n- Does the greedy pruning + fine-tuning pipeline provably lead to a simple model with good generalization guarantees?\n- What is the role of regularization in pruning?\n\nIn this paper, we use the symmetric matrix sensing problem [19] as a test-ground for the analysis of the greedy pruning framework, a model very closely related to shallow neural networks with quadratic activation functions [20, 19]. In this setting, the underlying problem for the population loss (infinite samples) is defined as $$\\|UU^T - U^*U^{T*}\\|_F^2$$, where $$U^* \\in \\mathbb{R}^{d \\times r}$$ is an unknown ground-truth rank-r matrix, with r also being unknown, and $$U \\in \\mathbb{R}^{d \\times k}$$ is the overparameterized learning model with $$k \\gg r$$. As we discuss in the Appendix F, the columns of U can be thought of as the weight vectors associated with individual neurons in a 2-layer shallow neural network with quadratic activation functions, a connection first observed in [21]. Thus, the data generating model has r neurons, while the learner trains an overparameterized model with k neurons.\n\nWhile the statistical and computational complexity of the overparameterized matrix sensing problem has been studied extensively, we use it as a model for understanding the efficacy of greedy pruning. In particular, we aim to answer the following questions: Does there exist a simple pruning criteria for which we can provably show that the pruned model generalizes well after fine-tuning while having the minimal necessary number of parameters? What is the role of regularization during training in promoting models which are compatible with greedy pruning? Finally, what generalization guarantees can we establish for the pruned model post fine-tuning?\n\nContributions: Our main contribution is to show that the discussed pruning pipeline not only recovers the correct ground-truth $$U^*U^{T*}$$ approximately, but also automatically adapts to the correct number of columns r. In particular, we show that training an overparameterized model on the empirical mean squared error with an added group Lasso based regularizer to promote column sparsity, followed by a simple norm-based pruning strategy results in a model Uprune having exactly the minimum number of columns, r. At the same time, we show that $$\\|U_{\\text{prune}}U_{\\text{T prune}} - U^*U^{T*}\\|_F^2$$ is small, but non-zero. Hence, the pruned model can subsequently be fine-tuned using a small number of gradient steps, and in this regime, $$\\|U_tU_{\\text{T}t} - U^*U^{T*}\\|_F^2$$ shows linear convergence to its limit. Moreover, the pruned model can be shown to admit finite sample generalization bounds which are also statistically optimal for a range of parameters. In particular, we show that to obtain a model Uout that has exactly r columns and its population error is at most $$\\|U_{\\text{out}}U_{\\text{T out}} - U^*U^{T*}\\|_F \\leq \\varepsilon$$, our framework requires $$O(dk^2r^5 + rd \\varepsilon^2)$$ samples, which is statistically optimal for sufficiently small $$\\varepsilon$$. We should also add that our framework does not require any computationally prohibitive pre- or post-processing (such as SVD decomposition) for achieving this result.\n\nWhile there are several works [22, 19, 23] establishing that gradient descent in the \"exactly-parameterized\" setting requires $$O(rd/\\varepsilon^2)$$ samples to achieve a generalization error of $$\\varepsilon$$, and converges linearly to this limit, the picture is different in the overparameterized setting. In [23], the authors showed that in the overparameterized setting, vanilla gradient descent requires $$O(kd/\\varepsilon^2)$$ samples to achieve a generalization error of $$\\|U_{\\text{gd}}U_{\\text{T gd}} - U^*U^{T*}\\|_F \\leq \\varepsilon$$, degrading with the overparameterization of the model. Moreover, the resulting solution does not have the correct column sparsity. In order to obtain a model which can be stored concisely, $$U_{\\text{gd}}$$ has to be post-processed by computing its SVD, which is computationally expensive in the high-dimensional regime.\n\nAs our second continuation, we show that the use of explicit regularization to promote column sparsity while training is important to learn models suitable for greedy pruning. Specifically, we show that while implicit regularization [19, 24] suffices to learn models with the correct rank, these approaches", "images": [], "items": [{"type": "text", "value": "Approaches, greedy pruning, in which we greedily eliminate the parameters of the trained model based on some measure (e.g., the norms of the weight vectors associated with individual neurons) has received widespread attention [3, 9\u201313]. This is mostly due to the fact that several practical studies have illustrated that training an overparameterized model followed by greedy pruning and fine-tuning leads to better generalization performance, compared to an overparameterized model trained without pruning [14]. Furthermore, a phenomenon that has been observed by several practical studies is that different forms of regularization during training, such as \u21130 or \u21131 regularization [15\u201317] or \u21132 regularization including group Lasso [18], lead to models that are better suited for pruning and leading to better generalization post fine-tuning.\n\nWhile the greedy pruning framework has shown impressive results, there is little to no theory backing why this pipeline works well in practice, nor understanding of the role of regularization in helping generate models which are suitable for greedy pruning. In this work, we address the following questions:\n\n- Does the greedy pruning + fine-tuning pipeline provably lead to a simple model with good generalization guarantees?\n- What is the role of regularization in pruning?\n\nIn this paper, we use the symmetric matrix sensing problem [19] as a test-ground for the analysis of the greedy pruning framework, a model very closely related to shallow neural networks with quadratic activation functions [20, 19]. In this setting, the underlying problem for the population loss (infinite samples) is defined as $$\\|UU^T - U^*U^{T*}\\|_F^2$$, where $$U^* \\in \\mathbb{R}^{d \\times r}$$ is an unknown ground-truth rank-r matrix, with r also being unknown, and $$U \\in \\mathbb{R}^{d \\times k}$$ is the overparameterized learning model with $$k \\gg r$$. As we discuss in the Appendix F, the columns of U can be thought of as the weight vectors associated with individual neurons in a 2-layer shallow neural network with quadratic activation functions, a connection first observed in [21]. Thus, the data generating model has r neurons, while the learner trains an overparameterized model with k neurons.\n\nWhile the statistical and computational complexity of the overparameterized matrix sensing problem has been studied extensively, we use it as a model for understanding the efficacy of greedy pruning. In particular, we aim to answer the following questions: Does there exist a simple pruning criteria for which we can provably show that the pruned model generalizes well after fine-tuning while having the minimal necessary number of parameters? What is the role of regularization during training in promoting models which are compatible with greedy pruning? Finally, what generalization guarantees can we establish for the pruned model post fine-tuning?\n\nContributions: Our main contribution is to show that the discussed pruning pipeline not only recovers the correct ground-truth $$U^*U^{T*}$$ approximately, but also automatically adapts to the correct number of columns r. In particular, we show that training an overparameterized model on the empirical mean squared error with an added group Lasso based regularizer to promote column sparsity, followed by a simple norm-based pruning strategy results in a model Uprune having exactly the minimum number of columns, r. At the same time, we show that $$\\|U_{\\text{prune}}U_{\\text{T prune}} - U^*U^{T*}\\|_F^2$$ is small, but non-zero. Hence, the pruned model can subsequently be fine-tuned using a small number of gradient steps, and in this regime, $$\\|U_tU_{\\text{T}t} - U^*U^{T*}\\|_F^2$$ shows linear convergence to its limit. Moreover, the pruned model can be shown to admit finite sample generalization bounds which are also statistically optimal for a range of parameters. In particular, we show that to obtain a model Uout that has exactly r columns and its population error is at most $$\\|U_{\\text{out}}U_{\\text{T out}} - U^*U^{T*}\\|_F \\leq \\varepsilon$$, our framework requires $$O(dk^2r^5 + rd \\varepsilon^2)$$ samples, which is statistically optimal for sufficiently small $$\\varepsilon$$. We should also add that our framework does not require any computationally prohibitive pre- or post-processing (such as SVD decomposition) for achieving this result.\n\nWhile there are several works [22, 19, 23] establishing that gradient descent in the \"exactly-parameterized\" setting requires $$O(rd/\\varepsilon^2)$$ samples to achieve a generalization error of $$\\varepsilon$$, and converges linearly to this limit, the picture is different in the overparameterized setting. In [23], the authors showed that in the overparameterized setting, vanilla gradient descent requires $$O(kd/\\varepsilon^2)$$ samples to achieve a generalization error of $$\\|U_{\\text{gd}}U_{\\text{T gd}} - U^*U^{T*}\\|_F \\leq \\varepsilon$$, degrading with the overparameterization of the model. Moreover, the resulting solution does not have the correct column sparsity. In order to obtain a model which can be stored concisely, $$U_{\\text{gd}}$$ has to be post-processed by computing its SVD, which is computationally expensive in the high-dimensional regime.\n\nAs our second continuation, we show that the use of explicit regularization to promote column sparsity while training is important to learn models suitable for greedy pruning. Specifically, we show that while implicit regularization [19, 24] suffices to learn models with the correct rank, these approaches", "md": "Approaches, greedy pruning, in which we greedily eliminate the parameters of the trained model based on some measure (e.g., the norms of the weight vectors associated with individual neurons) has received widespread attention [3, 9\u201313]. This is mostly due to the fact that several practical studies have illustrated that training an overparameterized model followed by greedy pruning and fine-tuning leads to better generalization performance, compared to an overparameterized model trained without pruning [14]. Furthermore, a phenomenon that has been observed by several practical studies is that different forms of regularization during training, such as \u21130 or \u21131 regularization [15\u201317] or \u21132 regularization including group Lasso [18], lead to models that are better suited for pruning and leading to better generalization post fine-tuning.\n\nWhile the greedy pruning framework has shown impressive results, there is little to no theory backing why this pipeline works well in practice, nor understanding of the role of regularization in helping generate models which are suitable for greedy pruning. In this work, we address the following questions:\n\n- Does the greedy pruning + fine-tuning pipeline provably lead to a simple model with good generalization guarantees?\n- What is the role of regularization in pruning?\n\nIn this paper, we use the symmetric matrix sensing problem [19] as a test-ground for the analysis of the greedy pruning framework, a model very closely related to shallow neural networks with quadratic activation functions [20, 19]. In this setting, the underlying problem for the population loss (infinite samples) is defined as $$\\|UU^T - U^*U^{T*}\\|_F^2$$, where $$U^* \\in \\mathbb{R}^{d \\times r}$$ is an unknown ground-truth rank-r matrix, with r also being unknown, and $$U \\in \\mathbb{R}^{d \\times k}$$ is the overparameterized learning model with $$k \\gg r$$. As we discuss in the Appendix F, the columns of U can be thought of as the weight vectors associated with individual neurons in a 2-layer shallow neural network with quadratic activation functions, a connection first observed in [21]. Thus, the data generating model has r neurons, while the learner trains an overparameterized model with k neurons.\n\nWhile the statistical and computational complexity of the overparameterized matrix sensing problem has been studied extensively, we use it as a model for understanding the efficacy of greedy pruning. In particular, we aim to answer the following questions: Does there exist a simple pruning criteria for which we can provably show that the pruned model generalizes well after fine-tuning while having the minimal necessary number of parameters? What is the role of regularization during training in promoting models which are compatible with greedy pruning? Finally, what generalization guarantees can we establish for the pruned model post fine-tuning?\n\nContributions: Our main contribution is to show that the discussed pruning pipeline not only recovers the correct ground-truth $$U^*U^{T*}$$ approximately, but also automatically adapts to the correct number of columns r. In particular, we show that training an overparameterized model on the empirical mean squared error with an added group Lasso based regularizer to promote column sparsity, followed by a simple norm-based pruning strategy results in a model Uprune having exactly the minimum number of columns, r. At the same time, we show that $$\\|U_{\\text{prune}}U_{\\text{T prune}} - U^*U^{T*}\\|_F^2$$ is small, but non-zero. Hence, the pruned model can subsequently be fine-tuned using a small number of gradient steps, and in this regime, $$\\|U_tU_{\\text{T}t} - U^*U^{T*}\\|_F^2$$ shows linear convergence to its limit. Moreover, the pruned model can be shown to admit finite sample generalization bounds which are also statistically optimal for a range of parameters. In particular, we show that to obtain a model Uout that has exactly r columns and its population error is at most $$\\|U_{\\text{out}}U_{\\text{T out}} - U^*U^{T*}\\|_F \\leq \\varepsilon$$, our framework requires $$O(dk^2r^5 + rd \\varepsilon^2)$$ samples, which is statistically optimal for sufficiently small $$\\varepsilon$$. We should also add that our framework does not require any computationally prohibitive pre- or post-processing (such as SVD decomposition) for achieving this result.\n\nWhile there are several works [22, 19, 23] establishing that gradient descent in the \"exactly-parameterized\" setting requires $$O(rd/\\varepsilon^2)$$ samples to achieve a generalization error of $$\\varepsilon$$, and converges linearly to this limit, the picture is different in the overparameterized setting. In [23], the authors showed that in the overparameterized setting, vanilla gradient descent requires $$O(kd/\\varepsilon^2)$$ samples to achieve a generalization error of $$\\|U_{\\text{gd}}U_{\\text{T gd}} - U^*U^{T*}\\|_F \\leq \\varepsilon$$, degrading with the overparameterization of the model. Moreover, the resulting solution does not have the correct column sparsity. In order to obtain a model which can be stored concisely, $$U_{\\text{gd}}$$ has to be post-processed by computing its SVD, which is computationally expensive in the high-dimensional regime.\n\nAs our second continuation, we show that the use of explicit regularization to promote column sparsity while training is important to learn models suitable for greedy pruning. Specifically, we show that while implicit regularization [19, 24] suffices to learn models with the correct rank, these approaches"}]}, {"page": 3, "text": "learn solutions with a large number of columns having \u21132-norms comparable to that of the maximum,\neven if r = 1. Hence, it is unclear how to sparsify such models based on the norms of their columns.\n2     Setup and Algorithmic Framework\nGiven n observation matrices {Ai}n          i=1, in the matrix sensing framework, the learner is provided mea-\nsurements yi = \u27e8Ai, U\u22c6U T       \u22c6  \u27e9+ \u03b5i where \u27e8\u00b7, \u00b7\u27e9     indicates the trace inner product and \u03b5i is measurement\nnoise assumed to be distributed i.i.d. \u223c             N(0, \u03c32)4. Here U\u22c6          \u2208   Rd\u00d7r is the unknown parameter,\nand the rank r \u2264        d is unknown. The goal of the matrix sensing problem is to learn a candidate\nmatrix X such that X \u2248          U\u22c6U T \u22c6 . For computational reasons, it is common to factorize X as UU T for\nU \u2208   Rd\u00d7k. In this paper, we study the factored model in the overparameterized setting, where k \u226b                           r.\nThe empirical mean squared error is,\n                                                          n\n                                      Lemp(U) = 1     n  i=1   \u27e8Ai, UU T \u27e9    \u2212   yi 2 .                                   (1)\nFor the case that Ai\u2019s are sampled entry-wise i.i.d. N(0, 1/d) and as n \u2192                    \u221e, up to additive constants\nwhich we ignore, the population mean square error can be written down as,\n                                           Lpop(U) = \u2225UU T \u2212           U\u22c6U T    F .                                        (2)\n                                                                             \u22c6 \u22252\nThere is an extensive literature on how to efficiently learn the right product U\u22c6U T              \u22c6  in both finite sample\nand population settings [25, 26]. In particular, there are several works on the effi                   ciency of gradient-\nbased methods with or without regularization for solving this specific problem [19, 23]. While these\napproaches guarantee learning the correct product UU T \u2248                    U\u22c6U T \u22c6 , in the overparameterized setting\nthe obtained solutions are not column sparse and storing these models requires                     \u0398(kd) bits of memory\n(ignoring precision). As a result, in order to obtain a compressed solution Uout \u2208                          Rd\u00d7r with the\ncorrect number of columns, one has to post-process U and do a singular value decomposition (SVD),\nan operation which is costly and impractical in high-dimensional settings.\nThe goal of this paper is to overcome this issue and come up with an efficient approach to recover a\nsolution Uout which generalizes well, in that Lpop(U) is small, while at the same time having only a\nfew non-zero columns, i.e. is sparse. Specifically, we show that via some proper regularization, it is\npossible to obtain a model that approximately learns the right product U\u22c6U T                 \u22c6 , while having only a few\nsignificant columns. As a result, many of its columns can be eliminated by a simple \u21132 norm-based\ngreedy pruning scheme, without significantly impacting the training loss. In fact, post pruning we end\nup with a model Uprune \u2208         Rd\u00d7r that has the correct dimensionality and its outer product UpruneU T                prune\nis close to the true product U\u22c6U T      \u22c6  . When the resulting \u201cexactly parameterized\u201d model is fine-tuned,\nthe generalization loss Lpop(U) can be shown to converge to 0 at a linear rate.\nTo formally describe our procedure, we first introduce the regularization scheme that we study in this\npaper, and then we present the greedy pruning scheme and the fine-tuning procedure.\nRegularization.         The use of regularization for matrix sensing (and matrix factorization) to encourage\na low rank solution [27\u201333], or to control the norm of the model for stability reasons [34, 35], or to\nimprove the landscape of the loss by eliminating spurious local minima [36] has been well-studied in\nthe literature. While these approaches implicitly or explicitly regularize for the rank of the learned\nmatrix, the solution learned as a result is often not column sparse. Indeed, note that a matrix can\nbe low rank and dense at the same time, if many columns are linear combinations of the others.\nWe propose studying the following regularized matrix sensing problem with a group Lasso based\nregularizer [37, 18]. In the population case, the loss is defined as\n                                                                                     k\n                            Lpop(U) + \u03bbR(U),                where      R(U) =      i=1  \u2225Uei\u22252.                            (3)\nNote that \u03bb > 0 is a properly selected regularization parameter. Imposing R as a penalty on the\nlayer weights of a neural network is a special case of a widely used approach commonly known\n    4All proofs in the paper go through as long as the noise is i.i.d. sub-Gaussian with variance proxy \u03c32. We\nstudy the Gaussian case for simplicity of exposition.\n                                                               3", "md": "# Matrix Sensing and Sparsity\n\n## Learn solutions with a large number of columns having \u21132-norms comparable to that of the maximum, even if r = 1. Hence, it is unclear how to sparsify such models based on the norms of their columns.\n\n### Setup and Algorithmic Framework\n\nGiven n observation matrices {Ai}i=1, ..., n in the matrix sensing framework, the learner is provided measurements yi = $$\\langle A_i, U^\\star U^T \\rangle + \\epsilon_i$$ where $$\\langle \\cdot, \\cdot \\rangle$$ indicates the trace inner product and $$\\epsilon_i$$ is measurement noise assumed to be distributed i.i.d. ~ $$N(0, \\sigma^2)$$. Here $$U^\\star \\in \\mathbb{R}^{d \\times r}$$ is the unknown parameter, and the rank $$r \\leq d$$ is unknown. The goal of the matrix sensing problem is to learn a candidate matrix X such that $$X \\approx U^\\star U^T$$. For computational reasons, it is common to factorize X as UU^T for $$U \\in \\mathbb{R}^{d \\times k}$$. In this paper, we study the factored model in the overparameterized setting, where $$k \\gg r$$.\n\nThe empirical mean squared error is,\n\n$$L_{\\text{emp}}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, UU^T \\rangle - y_i^2. \\quad (1)$$\nFor the case that Ai's are sampled entry-wise i.i.d. $$N(0, 1/d)$$ and as $$n \\rightarrow \\infty$$, up to additive constants which we ignore, the population mean square error can be written down as,\n\n$$L_{\\text{pop}}(U) = \\lVert UU^T - U^\\star U^T \\rVert_F^2. \\quad (2)$$\nThere is an extensive literature on how to efficiently learn the right product $$U^\\star U^T$$ in both finite sample and population settings. In particular, there are several works on the efficiency of gradient-based methods with or without regularization for solving this specific problem. While these approaches guarantee learning the correct product $$UU^T \\approx U^\\star U^T$$, in the overparameterized setting the obtained solutions are not column sparse and storing these models requires $$\\Theta(kd)$$ bits of memory (ignoring precision). As a result, in order to obtain a compressed solution $$U_{\\text{out}} \\in \\mathbb{R}^{d \\times r}$$ with the correct number of columns, one has to post-process U and do a singular value decomposition (SVD), an operation which is costly and impractical in high-dimensional settings.\n\nThe goal of this paper is to overcome this issue and come up with an efficient approach to recover a solution $$U_{\\text{out}}$$ which generalizes well, in that $$L_{\\text{pop}}(U)$$ is small, while at the same time having only a few non-zero columns, i.e. is sparse. Specifically, we show that via some proper regularization, it is possible to obtain a model that approximately learns the right product $$U^\\star U^T$$, while having only a few significant columns. As a result, many of its columns can be eliminated by a simple \u21132 norm-based greedy pruning scheme, without significantly impacting the training loss. In fact, post pruning we end up with a model $$U_{\\text{prune}} \\in \\mathbb{R}^{d \\times r}$$ that has the correct dimensionality and its outer product $$U_{\\text{prune}}U^T_{\\text{prune}}$$ is close to the true product $$U^\\star U^T$$. When the resulting \"exactly parameterized\" model is fine-tuned, the generalization loss $$L_{\\text{pop}}(U)$$ can be shown to converge to 0 at a linear rate.\n\nTo formally describe our procedure, we first introduce the regularization scheme that we study in this paper, and then we present the greedy pruning scheme and the fine-tuning procedure.\n\n### Regularization\n\nThe use of regularization for matrix sensing (and matrix factorization) to encourage a low rank solution, or to control the norm of the model for stability reasons, or to improve the landscape of the loss by eliminating spurious local minima has been well-studied in the literature. We propose studying the following regularized matrix sensing problem with a group Lasso based regularizer. In the population case, the loss is defined as\n\n$$L_{\\text{pop}}(U) + \\lambda R(U), \\quad \\text{where} \\quad R(U) = \\sum_{i=1}^{k} \\lVert Ue_i \\rVert^2. \\quad (3)$$\nNote that $$\\lambda > 0$$ is a properly selected regularization parameter. Imposing R as a penalty on the layer weights of a neural network is a special case of a widely used approach commonly known.\n\n4All proofs in the paper go through as long as the noise is i.i.d. sub-Gaussian with variance proxy $$\\sigma^2$$. We study the Gaussian case for simplicity of exposition.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Matrix Sensing and Sparsity", "md": "# Matrix Sensing and Sparsity"}, {"type": "heading", "lvl": 2, "value": "Learn solutions with a large number of columns having \u21132-norms comparable to that of the maximum, even if r = 1. Hence, it is unclear how to sparsify such models based on the norms of their columns.", "md": "## Learn solutions with a large number of columns having \u21132-norms comparable to that of the maximum, even if r = 1. Hence, it is unclear how to sparsify such models based on the norms of their columns."}, {"type": "heading", "lvl": 3, "value": "Setup and Algorithmic Framework", "md": "### Setup and Algorithmic Framework"}, {"type": "text", "value": "Given n observation matrices {Ai}i=1, ..., n in the matrix sensing framework, the learner is provided measurements yi = $$\\langle A_i, U^\\star U^T \\rangle + \\epsilon_i$$ where $$\\langle \\cdot, \\cdot \\rangle$$ indicates the trace inner product and $$\\epsilon_i$$ is measurement noise assumed to be distributed i.i.d. ~ $$N(0, \\sigma^2)$$. Here $$U^\\star \\in \\mathbb{R}^{d \\times r}$$ is the unknown parameter, and the rank $$r \\leq d$$ is unknown. The goal of the matrix sensing problem is to learn a candidate matrix X such that $$X \\approx U^\\star U^T$$. For computational reasons, it is common to factorize X as UU^T for $$U \\in \\mathbb{R}^{d \\times k}$$. In this paper, we study the factored model in the overparameterized setting, where $$k \\gg r$$.\n\nThe empirical mean squared error is,\n\n$$L_{\\text{emp}}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, UU^T \\rangle - y_i^2. \\quad (1)$$\nFor the case that Ai's are sampled entry-wise i.i.d. $$N(0, 1/d)$$ and as $$n \\rightarrow \\infty$$, up to additive constants which we ignore, the population mean square error can be written down as,\n\n$$L_{\\text{pop}}(U) = \\lVert UU^T - U^\\star U^T \\rVert_F^2. \\quad (2)$$\nThere is an extensive literature on how to efficiently learn the right product $$U^\\star U^T$$ in both finite sample and population settings. In particular, there are several works on the efficiency of gradient-based methods with or without regularization for solving this specific problem. While these approaches guarantee learning the correct product $$UU^T \\approx U^\\star U^T$$, in the overparameterized setting the obtained solutions are not column sparse and storing these models requires $$\\Theta(kd)$$ bits of memory (ignoring precision). As a result, in order to obtain a compressed solution $$U_{\\text{out}} \\in \\mathbb{R}^{d \\times r}$$ with the correct number of columns, one has to post-process U and do a singular value decomposition (SVD), an operation which is costly and impractical in high-dimensional settings.\n\nThe goal of this paper is to overcome this issue and come up with an efficient approach to recover a solution $$U_{\\text{out}}$$ which generalizes well, in that $$L_{\\text{pop}}(U)$$ is small, while at the same time having only a few non-zero columns, i.e. is sparse. Specifically, we show that via some proper regularization, it is possible to obtain a model that approximately learns the right product $$U^\\star U^T$$, while having only a few significant columns. As a result, many of its columns can be eliminated by a simple \u21132 norm-based greedy pruning scheme, without significantly impacting the training loss. In fact, post pruning we end up with a model $$U_{\\text{prune}} \\in \\mathbb{R}^{d \\times r}$$ that has the correct dimensionality and its outer product $$U_{\\text{prune}}U^T_{\\text{prune}}$$ is close to the true product $$U^\\star U^T$$. When the resulting \"exactly parameterized\" model is fine-tuned, the generalization loss $$L_{\\text{pop}}(U)$$ can be shown to converge to 0 at a linear rate.\n\nTo formally describe our procedure, we first introduce the regularization scheme that we study in this paper, and then we present the greedy pruning scheme and the fine-tuning procedure.", "md": "Given n observation matrices {Ai}i=1, ..., n in the matrix sensing framework, the learner is provided measurements yi = $$\\langle A_i, U^\\star U^T \\rangle + \\epsilon_i$$ where $$\\langle \\cdot, \\cdot \\rangle$$ indicates the trace inner product and $$\\epsilon_i$$ is measurement noise assumed to be distributed i.i.d. ~ $$N(0, \\sigma^2)$$. Here $$U^\\star \\in \\mathbb{R}^{d \\times r}$$ is the unknown parameter, and the rank $$r \\leq d$$ is unknown. The goal of the matrix sensing problem is to learn a candidate matrix X such that $$X \\approx U^\\star U^T$$. For computational reasons, it is common to factorize X as UU^T for $$U \\in \\mathbb{R}^{d \\times k}$$. In this paper, we study the factored model in the overparameterized setting, where $$k \\gg r$$.\n\nThe empirical mean squared error is,\n\n$$L_{\\text{emp}}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, UU^T \\rangle - y_i^2. \\quad (1)$$\nFor the case that Ai's are sampled entry-wise i.i.d. $$N(0, 1/d)$$ and as $$n \\rightarrow \\infty$$, up to additive constants which we ignore, the population mean square error can be written down as,\n\n$$L_{\\text{pop}}(U) = \\lVert UU^T - U^\\star U^T \\rVert_F^2. \\quad (2)$$\nThere is an extensive literature on how to efficiently learn the right product $$U^\\star U^T$$ in both finite sample and population settings. In particular, there are several works on the efficiency of gradient-based methods with or without regularization for solving this specific problem. While these approaches guarantee learning the correct product $$UU^T \\approx U^\\star U^T$$, in the overparameterized setting the obtained solutions are not column sparse and storing these models requires $$\\Theta(kd)$$ bits of memory (ignoring precision). As a result, in order to obtain a compressed solution $$U_{\\text{out}} \\in \\mathbb{R}^{d \\times r}$$ with the correct number of columns, one has to post-process U and do a singular value decomposition (SVD), an operation which is costly and impractical in high-dimensional settings.\n\nThe goal of this paper is to overcome this issue and come up with an efficient approach to recover a solution $$U_{\\text{out}}$$ which generalizes well, in that $$L_{\\text{pop}}(U)$$ is small, while at the same time having only a few non-zero columns, i.e. is sparse. Specifically, we show that via some proper regularization, it is possible to obtain a model that approximately learns the right product $$U^\\star U^T$$, while having only a few significant columns. As a result, many of its columns can be eliminated by a simple \u21132 norm-based greedy pruning scheme, without significantly impacting the training loss. In fact, post pruning we end up with a model $$U_{\\text{prune}} \\in \\mathbb{R}^{d \\times r}$$ that has the correct dimensionality and its outer product $$U_{\\text{prune}}U^T_{\\text{prune}}$$ is close to the true product $$U^\\star U^T$$. When the resulting \"exactly parameterized\" model is fine-tuned, the generalization loss $$L_{\\text{pop}}(U)$$ can be shown to converge to 0 at a linear rate.\n\nTo formally describe our procedure, we first introduce the regularization scheme that we study in this paper, and then we present the greedy pruning scheme and the fine-tuning procedure."}, {"type": "heading", "lvl": 3, "value": "Regularization", "md": "### Regularization"}, {"type": "text", "value": "The use of regularization for matrix sensing (and matrix factorization) to encourage a low rank solution, or to control the norm of the model for stability reasons, or to improve the landscape of the loss by eliminating spurious local minima has been well-studied in the literature. We propose studying the following regularized matrix sensing problem with a group Lasso based regularizer. In the population case, the loss is defined as\n\n$$L_{\\text{pop}}(U) + \\lambda R(U), \\quad \\text{where} \\quad R(U) = \\sum_{i=1}^{k} \\lVert Ue_i \\rVert^2. \\quad (3)$$\nNote that $$\\lambda > 0$$ is a properly selected regularization parameter. Imposing R as a penalty on the layer weights of a neural network is a special case of a widely used approach commonly known.\n\n4All proofs in the paper go through as long as the noise is i.i.d. sub-Gaussian with variance proxy $$\\sigma^2$$. We study the Gaussian case for simplicity of exposition.", "md": "The use of regularization for matrix sensing (and matrix factorization) to encourage a low rank solution, or to control the norm of the model for stability reasons, or to improve the landscape of the loss by eliminating spurious local minima has been well-studied in the literature. We propose studying the following regularized matrix sensing problem with a group Lasso based regularizer. In the population case, the loss is defined as\n\n$$L_{\\text{pop}}(U) + \\lambda R(U), \\quad \\text{where} \\quad R(U) = \\sum_{i=1}^{k} \\lVert Ue_i \\rVert^2. \\quad (3)$$\nNote that $$\\lambda > 0$$ is a properly selected regularization parameter. Imposing R as a penalty on the layer weights of a neural network is a special case of a widely used approach commonly known.\n\n4All proofs in the paper go through as long as the noise is i.i.d. sub-Gaussian with variance proxy $$\\sigma^2$$. We study the Gaussian case for simplicity of exposition."}]}, {"page": 4, "text": " as Structured Sparsity Learning (SSL) [12]. The regularizer promotes sparsity across \u201cgroups\u201d as\n discussed by [37]. Here, the groups correspond to the columns of U.\nAs we prove in Section 4, the solution obtained by minimizing a smooth version of the regularized\n loss in (3), denoted by Uout, is approximately column-sparse, i.e., k \u2212   r of its columns have small\n \u21132 norm. As a result, it is suitable for the simple greedy pruning scheme which we also introduce\n in Algorithm 1. Interestingly, in Section 3, we will first show a negative result - the model obtained\n by minimizing the unregularized loss, Lpop(U) (eq. (2)) using gradient descent updates could fail\n to learn a solution which is suitable for greedy pruning. This shows the importance of adding some\n form of regularization during the training phase of the overparameterized model.\n Greedy Pruning.      The greedy pruning approach posits training a model (possibly a large neural\n network) on the empirical loss, followed by pruning the resulting trained network greedily based on\n some criteria. The resulting model is often fine-tuned via a few gradient descent iterations before\n outputting. In the literature, various criteria have been proposed for greedy pruning. Magnitude-based\n approaches prune away the individual weights/neurons based on some measure of their size such as\n \u21131/\u21132 norm of the associated vectors [16, 10].\n In this work, we also focus on the idea of greedy pruning and study a mechanism to prune the\n solution obtained by minimizing the regularized empirical loss. Specifically, once an approximate\n second-order stationary point of the loss in Section 4, we only keep its columns whose \u21132 norm\n are above a threshold (specified in Algorithm 1) and the remaining columns with smaller norm are\n removed. We further show that post pruning, the obtained model Uprune continues to have a small\n empirical loss, i.e., small Lemp(Uprune), while having exactly r columns.\n Post pruning fine-tuning.      As mentioned above, it is common to fine-tune the smaller pruned\n model with a few gradient updates before the evaluation process [38]. We show that the pruned\n model Uprune has the correct rank and is reasonably close to the ground-truth model U\u22c6   in terms of its\n population loss. By running a few gradient updates on the mean square error, it can be ensured that\n UU T converges to U\u22c6U T \u22c6  at a linear rate. Algorithm 1 summarizes the framework that we study.\n 3   Implicit regularization does not lead to greedy pruning-friendly models\n In various overparameterized learning problems, it has been shown that first-order methods, starting\n from a small initialization, implicitly biases the model toward \u201csimple\u201d solutions, resulting in models\n that generalize well [39, 40], a phenomenon known as implicit regularization. In particular, for matrix\n sensing, [41, 19, 24] show that in the absence of any regularization, running gradient descent on the\n population loss starting from a small initialization biases UU T to low rank solutions, and learns\n the correct outer product UU T \u2248   U\u22c6U T\u22c6 . However, as discussed earlier, low-rank solutions are not\n necessarily column sparse, nor is it clear how to sparsify them without computing an SVD. It is\n unclear whether implicit regularization suffices to learn models that are amenable for greedy pruning.\n In this section, we address this question and show that minimizing the unregularized population loss\n Lpop leads to models which are not suitable for greedy pruning, i.e., have many columns with large \u21132\n norm. Specifically, we show that by running gradient flow from a small random initialization, even if\n the ground truth U\u22c6  is just a single column and r = 1, the learnt solution U has a large number of\n columns that are \u201cactive\u201d, i.e. having \u21132-norm comparable to that of the column with maximum norm.\n Thus, in the absence of the knowledge of r, it is unclear how to determine it from just observing the\n columns \u21132 norm. We thus claim that such trained models are not compatible with greedy pruning. In\n the following theorem, we formally state our result.\n Theorem 1. Consider the population loss in eq. (2), for r = 1 and k \u226b      1 and suppose \u2225U\u22c6\u2225op = 1.\nFurther, suppose the entries of the initial model U0 are i.i.d. samples from N(0, \u03b12), where \u03b1 \u2264\n c1/k3d log(kd) for some constant c1 > 0. For another absolute constant c\u2032      1 > 0, as t \u2192   \u221e, the\n iterates of gradient flow with probability \u2265  1 \u2212  O(1/kc\u2032 1) converge to a model Ugd where     \u2126(kc\u20321)\n active columns satisfy\n                                           \u2225Ugdei\u22252        \u2265  0.99.                                  (4)\n                                      max j\u2208[k] \u2225Ugdej\u22252\n                                                   4", "md": "As Structured Sparsity Learning (SSL) [12]. The regularizer promotes sparsity across \u201cgroups\u201d as discussed by [37]. Here, the groups correspond to the columns of U.\n\nAs we prove in Section 4, the solution obtained by minimizing a smooth version of the regularized loss in (3), denoted by Uout, is approximately column-sparse, i.e., k - r of its columns have small \u21132 norm. As a result, it is suitable for the simple greedy pruning scheme which we also introduce in Algorithm 1. Interestingly, in Section 3, we will first show a negative result - the model obtained by minimizing the unregularized loss, Lpop(U) (eq. (2)) using gradient descent updates could fail to learn a solution which is suitable for greedy pruning. This shows the importance of adding some form of regularization during the training phase of the overparameterized model.\n\n## Greedy Pruning\n\nThe greedy pruning approach posits training a model (possibly a large neural network) on the empirical loss, followed by pruning the resulting trained network greedily based on some criteria. The resulting model is often fine-tuned via a few gradient descent iterations before outputting. In the literature, various criteria have been proposed for greedy pruning. Magnitude-based approaches prune away the individual weights/neurons based on some measure of their size such as \u21131/\u21132 norm of the associated vectors [16], [10].\n\nIn this work, we also focus on the idea of greedy pruning and study a mechanism to prune the solution obtained by minimizing the regularized empirical loss. Specifically, once an approximate second-order stationary point of the loss in Section 4, we only keep its columns whose \u21132 norm are above a threshold (specified in Algorithm 1) and the remaining columns with smaller norm are removed. We further show that post pruning, the obtained model Uprune continues to have a small empirical loss, i.e., small Lemp(Uprune), while having exactly r columns.\n\n## Post pruning fine-tuning\n\nAs mentioned above, it is common to fine-tune the smaller pruned model with a few gradient updates before the evaluation process [38]. We show that the pruned model Uprune has the correct rank and is reasonably close to the ground-truth model U* in terms of its population loss. By running a few gradient updates on the mean square error, it can be ensured that UUT converges to U*UT* at a linear rate. Algorithm 1 summarizes the framework that we study.\n\n## Implicit regularization does not lead to greedy pruning-friendly models\n\nIn various overparameterized learning problems, it has been shown that first-order methods, starting from a small initialization, implicitly biases the model toward \u201csimple\u201d solutions, resulting in models that generalize well [39], [40], a phenomenon known as implicit regularization. In particular, for matrix sensing, [41], [19], [24] show that in the absence of any regularization, running gradient descent on the population loss starting from a small initialization biases UUT to low rank solutions, and learns the correct outer product UUT \u2248 U*UT*. However, as discussed earlier, low-rank solutions are not necessarily column sparse, nor is it clear how to sparsify them without computing an SVD. It is unclear whether implicit regularization suffices to learn models that are amenable for greedy pruning.\n\nIn this section, we address this question and show that minimizing the unregularized population loss Lpop leads to models which are not suitable for greedy pruning, i.e., have many columns with large \u21132 norm. Specifically, we show that by running gradient flow from a small random initialization, even if the ground truth U* is just a single column and r = 1, the learnt solution U has a large number of columns that are \u201cactive\u201d, i.e. having \u21132-norm comparable to that of the column with maximum norm. Thus, in the absence of the knowledge of r, it is unclear how to determine it from just observing the columns \u21132 norm. We thus claim that such trained models are not compatible with greedy pruning. In the following theorem, we formally state our result.\n\nTheorem 1. Consider the population loss in eq. (2), for r = 1 and k \u226b 1 and suppose \u2225U*\u2225op = 1. Further, suppose the entries of the initial model U0 are i.i.d. samples from N(0, \u03b12), where \u03b1 \u2264 c1/k3d log(kd) for some constant c1 > 0. For another absolute constant c'1 > 0, as t \u2192 \u221e, the iterates of gradient flow with probability \u2265 1 - O(1/kc'1) converge to a model Ugd where \u2126(kc'1) active columns satisfy\n\n$$\n\\frac{\\|U_{gd}e_i\\|_2}{\\max_{j\\in[k]} \\|U_{gd}e_j\\|_2} \\geq 0.99. \\quad (4)\n$$", "images": [], "items": [{"type": "text", "value": "As Structured Sparsity Learning (SSL) [12]. The regularizer promotes sparsity across \u201cgroups\u201d as discussed by [37]. Here, the groups correspond to the columns of U.\n\nAs we prove in Section 4, the solution obtained by minimizing a smooth version of the regularized loss in (3), denoted by Uout, is approximately column-sparse, i.e., k - r of its columns have small \u21132 norm. As a result, it is suitable for the simple greedy pruning scheme which we also introduce in Algorithm 1. Interestingly, in Section 3, we will first show a negative result - the model obtained by minimizing the unregularized loss, Lpop(U) (eq. (2)) using gradient descent updates could fail to learn a solution which is suitable for greedy pruning. This shows the importance of adding some form of regularization during the training phase of the overparameterized model.", "md": "As Structured Sparsity Learning (SSL) [12]. The regularizer promotes sparsity across \u201cgroups\u201d as discussed by [37]. Here, the groups correspond to the columns of U.\n\nAs we prove in Section 4, the solution obtained by minimizing a smooth version of the regularized loss in (3), denoted by Uout, is approximately column-sparse, i.e., k - r of its columns have small \u21132 norm. As a result, it is suitable for the simple greedy pruning scheme which we also introduce in Algorithm 1. Interestingly, in Section 3, we will first show a negative result - the model obtained by minimizing the unregularized loss, Lpop(U) (eq. (2)) using gradient descent updates could fail to learn a solution which is suitable for greedy pruning. This shows the importance of adding some form of regularization during the training phase of the overparameterized model."}, {"type": "heading", "lvl": 2, "value": "Greedy Pruning", "md": "## Greedy Pruning"}, {"type": "text", "value": "The greedy pruning approach posits training a model (possibly a large neural network) on the empirical loss, followed by pruning the resulting trained network greedily based on some criteria. The resulting model is often fine-tuned via a few gradient descent iterations before outputting. In the literature, various criteria have been proposed for greedy pruning. Magnitude-based approaches prune away the individual weights/neurons based on some measure of their size such as \u21131/\u21132 norm of the associated vectors [16], [10].\n\nIn this work, we also focus on the idea of greedy pruning and study a mechanism to prune the solution obtained by minimizing the regularized empirical loss. Specifically, once an approximate second-order stationary point of the loss in Section 4, we only keep its columns whose \u21132 norm are above a threshold (specified in Algorithm 1) and the remaining columns with smaller norm are removed. We further show that post pruning, the obtained model Uprune continues to have a small empirical loss, i.e., small Lemp(Uprune), while having exactly r columns.", "md": "The greedy pruning approach posits training a model (possibly a large neural network) on the empirical loss, followed by pruning the resulting trained network greedily based on some criteria. The resulting model is often fine-tuned via a few gradient descent iterations before outputting. In the literature, various criteria have been proposed for greedy pruning. Magnitude-based approaches prune away the individual weights/neurons based on some measure of their size such as \u21131/\u21132 norm of the associated vectors [16], [10].\n\nIn this work, we also focus on the idea of greedy pruning and study a mechanism to prune the solution obtained by minimizing the regularized empirical loss. Specifically, once an approximate second-order stationary point of the loss in Section 4, we only keep its columns whose \u21132 norm are above a threshold (specified in Algorithm 1) and the remaining columns with smaller norm are removed. We further show that post pruning, the obtained model Uprune continues to have a small empirical loss, i.e., small Lemp(Uprune), while having exactly r columns."}, {"type": "heading", "lvl": 2, "value": "Post pruning fine-tuning", "md": "## Post pruning fine-tuning"}, {"type": "text", "value": "As mentioned above, it is common to fine-tune the smaller pruned model with a few gradient updates before the evaluation process [38]. We show that the pruned model Uprune has the correct rank and is reasonably close to the ground-truth model U* in terms of its population loss. By running a few gradient updates on the mean square error, it can be ensured that UUT converges to U*UT* at a linear rate. Algorithm 1 summarizes the framework that we study.", "md": "As mentioned above, it is common to fine-tune the smaller pruned model with a few gradient updates before the evaluation process [38]. We show that the pruned model Uprune has the correct rank and is reasonably close to the ground-truth model U* in terms of its population loss. By running a few gradient updates on the mean square error, it can be ensured that UUT converges to U*UT* at a linear rate. Algorithm 1 summarizes the framework that we study."}, {"type": "heading", "lvl": 2, "value": "Implicit regularization does not lead to greedy pruning-friendly models", "md": "## Implicit regularization does not lead to greedy pruning-friendly models"}, {"type": "text", "value": "In various overparameterized learning problems, it has been shown that first-order methods, starting from a small initialization, implicitly biases the model toward \u201csimple\u201d solutions, resulting in models that generalize well [39], [40], a phenomenon known as implicit regularization. In particular, for matrix sensing, [41], [19], [24] show that in the absence of any regularization, running gradient descent on the population loss starting from a small initialization biases UUT to low rank solutions, and learns the correct outer product UUT \u2248 U*UT*. However, as discussed earlier, low-rank solutions are not necessarily column sparse, nor is it clear how to sparsify them without computing an SVD. It is unclear whether implicit regularization suffices to learn models that are amenable for greedy pruning.\n\nIn this section, we address this question and show that minimizing the unregularized population loss Lpop leads to models which are not suitable for greedy pruning, i.e., have many columns with large \u21132 norm. Specifically, we show that by running gradient flow from a small random initialization, even if the ground truth U* is just a single column and r = 1, the learnt solution U has a large number of columns that are \u201cactive\u201d, i.e. having \u21132-norm comparable to that of the column with maximum norm. Thus, in the absence of the knowledge of r, it is unclear how to determine it from just observing the columns \u21132 norm. We thus claim that such trained models are not compatible with greedy pruning. In the following theorem, we formally state our result.\n\nTheorem 1. Consider the population loss in eq. (2), for r = 1 and k \u226b 1 and suppose \u2225U*\u2225op = 1. Further, suppose the entries of the initial model U0 are i.i.d. samples from N(0, \u03b12), where \u03b1 \u2264 c1/k3d log(kd) for some constant c1 > 0. For another absolute constant c'1 > 0, as t \u2192 \u221e, the iterates of gradient flow with probability \u2265 1 - O(1/kc'1) converge to a model Ugd where \u2126(kc'1) active columns satisfy\n\n$$\n\\frac{\\|U_{gd}e_i\\|_2}{\\max_{j\\in[k]} \\|U_{gd}e_j\\|_2} \\geq 0.99. \\quad (4)\n$$", "md": "In various overparameterized learning problems, it has been shown that first-order methods, starting from a small initialization, implicitly biases the model toward \u201csimple\u201d solutions, resulting in models that generalize well [39], [40], a phenomenon known as implicit regularization. In particular, for matrix sensing, [41], [19], [24] show that in the absence of any regularization, running gradient descent on the population loss starting from a small initialization biases UUT to low rank solutions, and learns the correct outer product UUT \u2248 U*UT*. However, as discussed earlier, low-rank solutions are not necessarily column sparse, nor is it clear how to sparsify them without computing an SVD. It is unclear whether implicit regularization suffices to learn models that are amenable for greedy pruning.\n\nIn this section, we address this question and show that minimizing the unregularized population loss Lpop leads to models which are not suitable for greedy pruning, i.e., have many columns with large \u21132 norm. Specifically, we show that by running gradient flow from a small random initialization, even if the ground truth U* is just a single column and r = 1, the learnt solution U has a large number of columns that are \u201cactive\u201d, i.e. having \u21132-norm comparable to that of the column with maximum norm. Thus, in the absence of the knowledge of r, it is unclear how to determine it from just observing the columns \u21132 norm. We thus claim that such trained models are not compatible with greedy pruning. In the following theorem, we formally state our result.\n\nTheorem 1. Consider the population loss in eq. (2), for r = 1 and k \u226b 1 and suppose \u2225U*\u2225op = 1. Further, suppose the entries of the initial model U0 are i.i.d. samples from N(0, \u03b12), where \u03b1 \u2264 c1/k3d log(kd) for some constant c1 > 0. For another absolute constant c'1 > 0, as t \u2192 \u221e, the iterates of gradient flow with probability \u2265 1 - O(1/kc'1) converge to a model Ugd where \u2126(kc'1) active columns satisfy\n\n$$\n\\frac{\\|U_{gd}e_i\\|_2}{\\max_{j\\in[k]} \\|U_{gd}e_j\\|_2} \\geq 0.99. \\quad (4)\n$$"}]}, {"page": 5, "text": "                                                                             muLanalit\n      0\nFigure 1: We run gradient updates to minimize the loss in eq. (2) until convergence. The model is highly\noverparameterized with d = k = 500 and r = 4. In the left figure, the vertical axis represents the fraction of\ncolumns having their norm at least x times the largest norm across columns, with x \u2208                                                                 [0, 1] on the horizontal\naxis. In the right figure, for the same experiment we plot a histogram of column norms. Using regularization\nleads to solutions with most of the columns having small \u21132 norm, and only a few significant columns. Without\nregularization, a large number of columns have their \u21132 norm comparable to the largest column norm. Thus,\ntraining with explicit regularization leads to models that are more suitable for pruning.\nTheorem 1 claims that the iterates of gradient descent converge to a solution for which a large number\n(\u2126(kc\u2032     1 )) of columns of Ugd have their \u21132 norm at least 0.99 times the maximum. An inspection of\nthe proof shows that the constant 0.99 is arbitrary and can be extended to any constant bounded\naway from 1. Therefore, the models learnt by vanilla gradient descent cannot be reduced to the right\nsparsity level by simply pruning away columns with \u21132 norm below any fixed constant threshold.\nWe provide a general guarantee in Theorem 21 (Appendix A). It is worth noting that from [19]\nthat the learned model Ugd \u2208                                  Rd\u00d7k is guaranteed to achieve a small generalization error, i.e.,\nUgdU T     gd \u2248      U\u22c6U T    \u22c6  . Hence, the algorithm does learn the correct ground-truth product - what we show\nis that the learnt model is not \u201cpruning-friendly\u201d - it cannot be sparsified to the correct level by on\nany simple norm based pruning strategies. This is also observed in Figure 1: the model trained with\nimplicit regularization has many significant columns and it is unclear how many to prune. We provide\na proof sketch below, and refer the reader to Appendix A for more details.\nProof sketch of Theorem 1.                                   Without loss of generality we assume in the proof that \u2225U\u22c6\u22252 = 1.\nDefining U            (t) as the iterate at time t, gradient flow on Lpop follows the below dynamic,\n                                                     dU\n                                                      dt = \u2212\u2207Lpop(U                     ) = \u2212(U          U T \u2212        U\u22c6U T   \u22c6   )U,                                          (5)\nwhere for ease of notation we drop the explicit dependence on t in U                                                                  (t). In our proof, we exactly\ncharacterize the limiting point of gradient flow as a function of the initialization in a certain sense,\nwhich may be of independent interest. In particular, up to a small multiplicative error, we show,\n                                                          \u2200i \u2208      [k], \u2225Ugdei\u22252 \u2248                  |\u27e8U\u22c6, U        (0)ei\u27e9|      .                                             (6)\n                                                                                                 i.i.d.\u2225U T  \u22c6  U   (0)\u22252\nAlso, with Gaussian initialization, \u27e8U\u22c6, U                                         (0)ei\u27e9          \u223c      N    (0, \u03b12) across different values of i. In\nparticular, we show that for some constant c\u2032                                    1 > 0,       \u2126(kc\u2032     1 ) columns will have correlations comparable\nto the maximum, with |\u27e8U\u22c6, U                              (0)ei\u27e9| \u2265          0.99 maxj\u2208[k] |\u27e8U\u22c6, U                     (0)ej\u27e9|. For any of these columns, i,\n                   \u2225Ugdei\u22252 \u2248              |\u27e8U\u22c6, U        (0)ei\u27e9|       \u2265     0.99 max           |\u27e8U\u22c6, U       (0)ej\u27e9|         \u2248    0.99 max                                   (7)\n                                             \u2225U T  \u22c6  U   (0)\u22252                        j\u2208[k]       \u2225U T  \u22c6  U   (0)\u22252                        j\u2208[k] \u2225Ugdej\u22252,\nwhich completes the proof sketch of Theorem 1.\n4       Explicit regularization gives pruning-friendly models: population analysis\nIn this section, we study the properties of the squared error augmented with group Lasso regularization\nin the population (infi                     nite sample) setting. We show that second-order stationary points of the\n                                                                                             5", "md": "# Document\n\nFigure 1: We run gradient updates to minimize the loss in eq. (2) until convergence. The model is highly\noverparameterized with d = k = 500 and r = 4. In the left figure, the vertical axis represents the fraction of\ncolumns having their norm at least x times the largest norm across columns, with x \u2208 [0, 1] on the horizontal\naxis. In the right figure, for the same experiment we plot a histogram of column norms. Using regularization\nleads to solutions with most of the columns having small \u21132 norm, and only a few significant columns. Without\nregularization, a large number of columns have their \u21132 norm comparable to the largest column norm. Thus,\ntraining with explicit regularization leads to models that are more suitable for pruning.\n\nTheorem 1 claims that the iterates of gradient descent converge to a solution for which a large number\n(\u2126(kc'1)) of columns of Ugd have their \u21132 norm at least 0.99 times the maximum. An inspection of\nthe proof shows that the constant 0.99 is arbitrary and can be extended to any constant bounded\naway from 1. Therefore, the models learnt by vanilla gradient descent cannot be reduced to the right\nsparsity level by simply pruning away columns with \u21132 norm below any fixed constant threshold.\n\nWe provide a general guarantee in Theorem 21 (Appendix A). It is worth noting that from [19]\nthat the learned model $$U_{gd} \\in \\mathbb{R}^{d \\times k}$$ is guaranteed to achieve a small generalization error, i.e.,\n$$U_{gd}U_{gd}^T \\approx U^*U^{*T}$$. Hence, the algorithm does learn the correct ground-truth product - what we show\nis that the learnt model is not \u201cpruning-friendly\u201d - it cannot be sparsified to the correct level by on\nany simple norm based pruning strategies. This is also observed in Figure 1: the model trained with\nimplicit regularization has many significant columns and it is unclear how many to prune. We provide\na proof sketch below, and refer the reader to Appendix A for more details.\n\nProof sketch of Theorem 1. Without loss of generality we assume in the proof that $$\\|U^*\\|_2 = 1$$.\nDefining $$U(t)$$ as the iterate at time t, gradient flow on Lpop follows the below dynamic,\n$$\\frac{dU}{dt} = -\\nabla L_{pop}(U) = -(UU^T - U^*U^{*T})U,$$ where for ease of notation we drop the explicit dependence on t in $$U(t)$$. In our proof, we exactly\ncharacterize the limiting point of gradient flow as a function of the initialization in a certain sense,\nwhich may be of independent interest. In particular, up to a small multiplicative error, we show,\n$$\\forall i \\in [k], \\|U_{gd}e_i\\|_2 \\approx \\left|\\langle U^*, U(0)e_i \\rangle\\right| \\cdot \\frac{1}{\\text{i.i.d.}\\|U^*U(0)\\|_2}$$\nAlso, with Gaussian initialization, $$\\langle U^*, U(0)e_i \\rangle \\sim N(0, \\alpha^2)$$ across different values of i. In\nparticular, we show that for some constant $$c'1 > 0, \\Omega(kc'1)$$ columns will have correlations comparable\nto the maximum, with $$\\left|\\langle U^*, U(0)e_i \\rangle\\right| \\geq 0.99 \\max_{j\\in[k]} \\left|\\langle U^*, U(0)e_j \\rangle\\right|$$. For any of these columns, i,\n$$\\|U_{gd}e_i\\|_2 \\approx \\left|\\langle U^*, U(0)e_i \\rangle\\right| \\geq 0.99 \\max_{j} \\left|\\langle U^*, U(0)e_j \\rangle\\right| \\approx 0.99 \\max_{j\\in[k]} \\frac{\\|U^*U(0)\\|_2}{\\|U_{gd}e_j\\|_2},$$\nwhich completes the proof sketch of Theorem 1.\n\n4 Explicit regularization gives pruning-friendly models: population analysis\n\nIn this section, we study the properties of the squared error augmented with group Lasso regularization\nin the population (infinite sample) setting. We show that second-order stationary points of the", "images": [{"name": "page-5-0.jpg", "height": 123, "width": 413, "x": 109, "y": 71}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Figure 1: We run gradient updates to minimize the loss in eq. (2) until convergence. The model is highly\noverparameterized with d = k = 500 and r = 4. In the left figure, the vertical axis represents the fraction of\ncolumns having their norm at least x times the largest norm across columns, with x \u2208 [0, 1] on the horizontal\naxis. In the right figure, for the same experiment we plot a histogram of column norms. Using regularization\nleads to solutions with most of the columns having small \u21132 norm, and only a few significant columns. Without\nregularization, a large number of columns have their \u21132 norm comparable to the largest column norm. Thus,\ntraining with explicit regularization leads to models that are more suitable for pruning.\n\nTheorem 1 claims that the iterates of gradient descent converge to a solution for which a large number\n(\u2126(kc'1)) of columns of Ugd have their \u21132 norm at least 0.99 times the maximum. An inspection of\nthe proof shows that the constant 0.99 is arbitrary and can be extended to any constant bounded\naway from 1. Therefore, the models learnt by vanilla gradient descent cannot be reduced to the right\nsparsity level by simply pruning away columns with \u21132 norm below any fixed constant threshold.\n\nWe provide a general guarantee in Theorem 21 (Appendix A). It is worth noting that from [19]\nthat the learned model $$U_{gd} \\in \\mathbb{R}^{d \\times k}$$ is guaranteed to achieve a small generalization error, i.e.,\n$$U_{gd}U_{gd}^T \\approx U^*U^{*T}$$. Hence, the algorithm does learn the correct ground-truth product - what we show\nis that the learnt model is not \u201cpruning-friendly\u201d - it cannot be sparsified to the correct level by on\nany simple norm based pruning strategies. This is also observed in Figure 1: the model trained with\nimplicit regularization has many significant columns and it is unclear how many to prune. We provide\na proof sketch below, and refer the reader to Appendix A for more details.\n\nProof sketch of Theorem 1. Without loss of generality we assume in the proof that $$\\|U^*\\|_2 = 1$$.\nDefining $$U(t)$$ as the iterate at time t, gradient flow on Lpop follows the below dynamic,\n$$\\frac{dU}{dt} = -\\nabla L_{pop}(U) = -(UU^T - U^*U^{*T})U,$$ where for ease of notation we drop the explicit dependence on t in $$U(t)$$. In our proof, we exactly\ncharacterize the limiting point of gradient flow as a function of the initialization in a certain sense,\nwhich may be of independent interest. In particular, up to a small multiplicative error, we show,\n$$\\forall i \\in [k], \\|U_{gd}e_i\\|_2 \\approx \\left|\\langle U^*, U(0)e_i \\rangle\\right| \\cdot \\frac{1}{\\text{i.i.d.}\\|U^*U(0)\\|_2}$$\nAlso, with Gaussian initialization, $$\\langle U^*, U(0)e_i \\rangle \\sim N(0, \\alpha^2)$$ across different values of i. In\nparticular, we show that for some constant $$c'1 > 0, \\Omega(kc'1)$$ columns will have correlations comparable\nto the maximum, with $$\\left|\\langle U^*, U(0)e_i \\rangle\\right| \\geq 0.99 \\max_{j\\in[k]} \\left|\\langle U^*, U(0)e_j \\rangle\\right|$$. For any of these columns, i,\n$$\\|U_{gd}e_i\\|_2 \\approx \\left|\\langle U^*, U(0)e_i \\rangle\\right| \\geq 0.99 \\max_{j} \\left|\\langle U^*, U(0)e_j \\rangle\\right| \\approx 0.99 \\max_{j\\in[k]} \\frac{\\|U^*U(0)\\|_2}{\\|U_{gd}e_j\\|_2},$$\nwhich completes the proof sketch of Theorem 1.\n\n4 Explicit regularization gives pruning-friendly models: population analysis\n\nIn this section, we study the properties of the squared error augmented with group Lasso regularization\nin the population (infinite sample) setting. We show that second-order stationary points of the", "md": "Figure 1: We run gradient updates to minimize the loss in eq. (2) until convergence. The model is highly\noverparameterized with d = k = 500 and r = 4. In the left figure, the vertical axis represents the fraction of\ncolumns having their norm at least x times the largest norm across columns, with x \u2208 [0, 1] on the horizontal\naxis. In the right figure, for the same experiment we plot a histogram of column norms. Using regularization\nleads to solutions with most of the columns having small \u21132 norm, and only a few significant columns. Without\nregularization, a large number of columns have their \u21132 norm comparable to the largest column norm. Thus,\ntraining with explicit regularization leads to models that are more suitable for pruning.\n\nTheorem 1 claims that the iterates of gradient descent converge to a solution for which a large number\n(\u2126(kc'1)) of columns of Ugd have their \u21132 norm at least 0.99 times the maximum. An inspection of\nthe proof shows that the constant 0.99 is arbitrary and can be extended to any constant bounded\naway from 1. Therefore, the models learnt by vanilla gradient descent cannot be reduced to the right\nsparsity level by simply pruning away columns with \u21132 norm below any fixed constant threshold.\n\nWe provide a general guarantee in Theorem 21 (Appendix A). It is worth noting that from [19]\nthat the learned model $$U_{gd} \\in \\mathbb{R}^{d \\times k}$$ is guaranteed to achieve a small generalization error, i.e.,\n$$U_{gd}U_{gd}^T \\approx U^*U^{*T}$$. Hence, the algorithm does learn the correct ground-truth product - what we show\nis that the learnt model is not \u201cpruning-friendly\u201d - it cannot be sparsified to the correct level by on\nany simple norm based pruning strategies. This is also observed in Figure 1: the model trained with\nimplicit regularization has many significant columns and it is unclear how many to prune. We provide\na proof sketch below, and refer the reader to Appendix A for more details.\n\nProof sketch of Theorem 1. Without loss of generality we assume in the proof that $$\\|U^*\\|_2 = 1$$.\nDefining $$U(t)$$ as the iterate at time t, gradient flow on Lpop follows the below dynamic,\n$$\\frac{dU}{dt} = -\\nabla L_{pop}(U) = -(UU^T - U^*U^{*T})U,$$ where for ease of notation we drop the explicit dependence on t in $$U(t)$$. In our proof, we exactly\ncharacterize the limiting point of gradient flow as a function of the initialization in a certain sense,\nwhich may be of independent interest. In particular, up to a small multiplicative error, we show,\n$$\\forall i \\in [k], \\|U_{gd}e_i\\|_2 \\approx \\left|\\langle U^*, U(0)e_i \\rangle\\right| \\cdot \\frac{1}{\\text{i.i.d.}\\|U^*U(0)\\|_2}$$\nAlso, with Gaussian initialization, $$\\langle U^*, U(0)e_i \\rangle \\sim N(0, \\alpha^2)$$ across different values of i. In\nparticular, we show that for some constant $$c'1 > 0, \\Omega(kc'1)$$ columns will have correlations comparable\nto the maximum, with $$\\left|\\langle U^*, U(0)e_i \\rangle\\right| \\geq 0.99 \\max_{j\\in[k]} \\left|\\langle U^*, U(0)e_j \\rangle\\right|$$. For any of these columns, i,\n$$\\|U_{gd}e_i\\|_2 \\approx \\left|\\langle U^*, U(0)e_i \\rangle\\right| \\geq 0.99 \\max_{j} \\left|\\langle U^*, U(0)e_j \\rangle\\right| \\approx 0.99 \\max_{j\\in[k]} \\frac{\\|U^*U(0)\\|_2}{\\|U_{gd}e_j\\|_2},$$\nwhich completes the proof sketch of Theorem 1.\n\n4 Explicit regularization gives pruning-friendly models: population analysis\n\nIn this section, we study the properties of the squared error augmented with group Lasso regularization\nin the population (infinite sample) setting. We show that second-order stationary points of the"}]}, {"page": 6, "text": "Algorithm 1 Greedy pruning based on group-Lasso regularization\n     Inputs: Measurements {(Ai, yi) where yi = \u27e8Ai, U\u22c6U T                     \u22c6 \u27e9 + \u03b5i}n  i=1 (in the population setting n = \u221e);\n     Initialization: Set parameters: \u03bb, \u03b2, \u03f5, \u03b3 and mfine-tune.\n     Greedy pruning phase:\n 1: Find an (\u03f5, \u03b3)-approximate SOSP of femp (resp. fpop), U, satisfying \u2225U\u2225op \u2264                           3.\n 2: Let S = {i \u2208        [k] : \u2225Uei\u22252 \u2264       2\u221a\u03b2 denote the set of columns with small \u21132 norm.\n     Create a new matrix Uprune which only preserves the columns of U in [k] \\ S, deleting the columns in S.\n     Fine-tuning phase:\n 1: Run mfine-tune iterations of gradient descent on Lpop(U) (resp. Lemp(U)) initialized at Uprune to get Uout.\n 2: return Uout.\nregularized loss are suitable for greedy pruning, while at the same time achieving a small but non-zero\ngeneralization error. Note that the \u21132 norm is a non-smooth function at the origin, and therefore, the\noverall regularized loss is non-smooth and non-convex. While there are several notions of approximate\nstationary points for non-differentiable and non-convex functions, for technical convenience, we\nreplace R by a smooth proxy. In particular, for a smoothing parameter \u03b2 > 0, define a smooth\nversion of the \u21132 norm, and the corresponding smooth regularizer R\u03b2 as,\n                             R\u03b2(U) =           k   \u2113\u03b2                where       \u2113\u03b2                 \u2225v\u22252 2      .                          (8)\n                                             i=1    2 (Uei),                       2 (v) =        \u2225v\u22252  2 + \u03b2\nNote that the smaller the value of \u03b2 is, the closer is \u2113\u03b2                    2 (v) to \u2225v\u22252. Considering this definition, the\noverall regularized loss we study in the population setting is\n                                                fpop(U) = Lpop(U) + \u03bbR\u03b2(U),                                                                (9)\nwhere Lpop = \u2225UU T \u2212              U\u22c6U T \u22c6  \u22252F as defined in eq. (2). The above optimization problem is nonconvex\ndue to the structure of Lpop(U), and finding its global minimizer can be computationally prohibitive.\nFortunately, for our theoretical results, we do not require achieving global optimality and we only\nrequire an approximate second-order stationary point of the loss in eq. (9), which is defined below.\nDefinition 2. We say that U is an (\u03f5, \u03b3)-approximate second-order stationary point of f if,\n        1. The gradient norm is bounded above by \u03f5, i.e., \u2225\u2207f(U)\u22252 \u2264                                 \u03f5.\n        2. The eigenvalues of the Hessian are larger than \u2212\u03b3, i.e., \u03bbmin(\u22072f(U)) \u2265                                       \u2212\u03b3.\nThe full algorithmic procedure that we analyze in this section is summarized in Algorithm 1. Once we\nfind an (\u03f5, \u03b3)-approximate second-order stationary point (SOSP) of eq. (9) for some proper choices\nof \u03f5 and \u03b3, we apply greedy pruning on the obtained model U and by eliminating all of its columns\nwith \u21132 norm below a specific threshold. As a result, the pruned model Uprune has fewer columns than\nthe trained model U. In fact, in our following theoretical results, we show that if the parameters are\nproperly selected, Uprune has exactly r columns, which is the same as U\u22c6. Finally, we fine-tune the\npruned solution by running gradient descent on the unregularized loss Lpop (resp. Lemp). Next, we\nstate the properties of the pruned model generated by Algorithm 1 in the infinite sample setting.\nTheorem 3. Consider the population loss with regularization in eq. (9), where U\u22c6                                         has rank r and\nits smallest singular value is denoted by \u03c3\u22c6                 r. Moreover, consider Uprune as the output of the pruning\nphase in Algorithm 1 with parameters \u03b2, \u03bb, \u03f5, \u03b3 satisfying the conditions5,\n                           (\u03c3\u22c6 r)2                     (\u03c3\u22c6 r)3                       (\u03c3\u22c6 r)3                      (\u03c3\u22c6r)7/2\n                \u03b2 = c\u03b2        r     ,        \u03bb = c\u03bb     \u221a  kr   ,        \u03b3 \u2264    c\u03b3  \u221a  kr5/2 ,          \u03f5 \u2264    c\u03f5 \u221a  kr5/2 ,             (10)\nfor some absolute constants c\u03b2, c\u03bb, c\u03f5, c\u03b3 > 0. Then, we have\n        1. Uprune has exactly r columns.\n        2. \u2225UpruneU T    prune \u2212    U\u22c6U T  \u22c6 \u2225F \u2264      1    r)2.\n                                                       2(\u03c3\u22c6\n    5This style of result exists for LASSO as well (see [42, Theorem 1]), where the optimal choice of the\nregularization parameter, \u03bb\u22c6, depends on the true sparsity r, but a general guarantee can be established as\nwell, which degrades as \u03bb deviates from \u03bb\u22c6. In practice \u03bb is chosen using cross-validation. For simplicity of\npresentation, we state the result when r and \u03c3\u22c6            r are known up to constants.\n                                                                      6", "md": "Algorithm 1 Greedy pruning based on group-Lasso regularization\n\nInputs: Measurements {(Ai, yi) where yi = $$\\langle$$Ai, U\u22c6U T$$\\rangle$$} + \u03b5i}n  i=1 (in the population setting n = \u221e);\n\nInitialization: Set parameters: \u03bb, \u03b2, \u03f5, \u03b3 and mfine-tune.\n\nGreedy pruning phase:\n\n1: Find an (\u03f5, \u03b3)-approximate SOSP of femp (resp. fpop), U, satisfying $$\\|U\\|_{op} \\leq 3$$.\n\n2: Let S = {i \u2208 [k] : $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$ denote the set of columns with small \u21132 norm.\nCreate a new matrix Uprune which only preserves the columns of U in [k] \\ S, deleting the columns in S.\n\nFine-tuning phase:\n\n1: Run mfine-tune iterations of gradient descent on Lpop(U) (resp. Lemp(U)) initialized at Uprune to get Uout.\n\n2: return Uout.\n\nRegularized loss are suitable for greedy pruning, while at the same time achieving a small but non-zero generalization error. Note that the \u21132 norm is a non-smooth function at the origin, and therefore, the overall regularized loss is non-smooth and non-convex. While there are several notions of approximate stationary points for non-differentiable and non-convex functions, for technical convenience, we replace R by a smooth proxy. In particular, for a smoothing parameter \u03b2 > 0, define a smooth version of the \u21132 norm, and the corresponding smooth regularizer R\u03b2 as,\n\n$$R_{\\beta}(U) = \\sum_{i=1}^{k} \\ell_{\\beta}(2(Ue_i)), \\quad \\ell_{\\beta}(v) = \\|v\\|_2^2 + \\beta$$\n\nNote that the smaller the value of \u03b2 is, the closer is $$\\ell_{\\beta}(v)$$ to $$\\|v\\|_2$$. Considering this definition, the overall regularized loss we study in the population setting is\n\n$$f_{pop}(U) = L_{pop}(U) + \\lambda R_{\\beta}(U), \\quad (9)$$\n\nwhere $$L_{pop} = \\|UU^T - U\u22c6U^T\u22c6\\|_F^2$$ as defined in eq. (2). The above optimization problem is non-convex due to the structure of Lpop(U), and finding its global minimizer can be computationally prohibitive. Fortunately, for our theoretical results, we do not require achieving global optimality and we only require an approximate second-order stationary point of the loss in eq. (9), which is defined below.\n\nDefinition 2. We say that U is an (\u03f5, \u03b3)-approximate second-order stationary point of f if,\n1. The gradient norm is bounded above by \u03f5, i.e., $$\\|\\nabla f(U)\\|_2 \\leq \\epsilon$$.\n2. The eigenvalues of the Hessian are larger than -\u03b3, i.e., $$\\lambda_{\\text{min}}(\\nabla^2 f(U)) \\geq -\\gamma$$.\n\nThe full algorithmic procedure that we analyze in this section is summarized in Algorithm 1. Once we find an (\u03f5, \u03b3)-approximate second-order stationary point (SOSP) of eq. (9) for some proper choices of \u03f5 and \u03b3, we apply greedy pruning on the obtained model U and by eliminating all of its columns with \u21132 norm below a specific threshold. As a result, the pruned model Uprune has fewer columns than the trained model U. In fact, in our following theoretical results, we show that if the parameters are properly selected, Uprune has exactly r columns, which is the same as U\u22c6. Finally, we fine-tune the pruned solution by running gradient descent on the unregularized loss Lpop (resp. Lemp). Next, we state the properties of the pruned model generated by Algorithm 1 in the infinite sample setting.\n\nTheorem 3. Consider the population loss with regularization in eq. (9), where U\u22c6 has rank r and its smallest singular value is denoted by $$\\sigma_{\\star r}$$. Moreover, consider Uprune as the output of the pruning phase in Algorithm 1 with parameters \u03b2, \u03bb, \u03f5, \u03b3 satisfying the conditions,\n\n$$\\beta = c_{\\beta}(\\sigma_{\\star r})^2, \\quad \\lambda = c_{\\lambda} \\sqrt{kr}, \\quad \\gamma \\leq c_{\\gamma} \\sqrt{kr^{5/2}}, \\quad \\epsilon \\leq c_{\\epsilon} \\sqrt{kr^{5/2}}, \\quad (10)$$\n\nfor some absolute constants $$c_{\\beta}, c_{\\lambda}, c_{\\epsilon}, c_{\\gamma} > 0$$. Then, we have\n1. Uprune has exactly r columns.\n2. $$\\|U_{\\text{prune}}U^T_{\\text{prune}} - U\u22c6U^T\u22c6\\|_F \\leq \\frac{1}{2}(\\sigma_{\\star r})^2$$.", "images": [], "items": [{"type": "text", "value": "Algorithm 1 Greedy pruning based on group-Lasso regularization\n\nInputs: Measurements {(Ai, yi) where yi = $$\\langle$$Ai, U\u22c6U T$$\\rangle$$} + \u03b5i}n  i=1 (in the population setting n = \u221e);\n\nInitialization: Set parameters: \u03bb, \u03b2, \u03f5, \u03b3 and mfine-tune.\n\nGreedy pruning phase:\n\n1: Find an (\u03f5, \u03b3)-approximate SOSP of femp (resp. fpop), U, satisfying $$\\|U\\|_{op} \\leq 3$$.\n\n2: Let S = {i \u2208 [k] : $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$ denote the set of columns with small \u21132 norm.\nCreate a new matrix Uprune which only preserves the columns of U in [k] \\ S, deleting the columns in S.\n\nFine-tuning phase:\n\n1: Run mfine-tune iterations of gradient descent on Lpop(U) (resp. Lemp(U)) initialized at Uprune to get Uout.\n\n2: return Uout.\n\nRegularized loss are suitable for greedy pruning, while at the same time achieving a small but non-zero generalization error. Note that the \u21132 norm is a non-smooth function at the origin, and therefore, the overall regularized loss is non-smooth and non-convex. While there are several notions of approximate stationary points for non-differentiable and non-convex functions, for technical convenience, we replace R by a smooth proxy. In particular, for a smoothing parameter \u03b2 > 0, define a smooth version of the \u21132 norm, and the corresponding smooth regularizer R\u03b2 as,\n\n$$R_{\\beta}(U) = \\sum_{i=1}^{k} \\ell_{\\beta}(2(Ue_i)), \\quad \\ell_{\\beta}(v) = \\|v\\|_2^2 + \\beta$$\n\nNote that the smaller the value of \u03b2 is, the closer is $$\\ell_{\\beta}(v)$$ to $$\\|v\\|_2$$. Considering this definition, the overall regularized loss we study in the population setting is\n\n$$f_{pop}(U) = L_{pop}(U) + \\lambda R_{\\beta}(U), \\quad (9)$$\n\nwhere $$L_{pop} = \\|UU^T - U\u22c6U^T\u22c6\\|_F^2$$ as defined in eq. (2). The above optimization problem is non-convex due to the structure of Lpop(U), and finding its global minimizer can be computationally prohibitive. Fortunately, for our theoretical results, we do not require achieving global optimality and we only require an approximate second-order stationary point of the loss in eq. (9), which is defined below.\n\nDefinition 2. We say that U is an (\u03f5, \u03b3)-approximate second-order stationary point of f if,\n1. The gradient norm is bounded above by \u03f5, i.e., $$\\|\\nabla f(U)\\|_2 \\leq \\epsilon$$.\n2. The eigenvalues of the Hessian are larger than -\u03b3, i.e., $$\\lambda_{\\text{min}}(\\nabla^2 f(U)) \\geq -\\gamma$$.\n\nThe full algorithmic procedure that we analyze in this section is summarized in Algorithm 1. Once we find an (\u03f5, \u03b3)-approximate second-order stationary point (SOSP) of eq. (9) for some proper choices of \u03f5 and \u03b3, we apply greedy pruning on the obtained model U and by eliminating all of its columns with \u21132 norm below a specific threshold. As a result, the pruned model Uprune has fewer columns than the trained model U. In fact, in our following theoretical results, we show that if the parameters are properly selected, Uprune has exactly r columns, which is the same as U\u22c6. Finally, we fine-tune the pruned solution by running gradient descent on the unregularized loss Lpop (resp. Lemp). Next, we state the properties of the pruned model generated by Algorithm 1 in the infinite sample setting.\n\nTheorem 3. Consider the population loss with regularization in eq. (9), where U\u22c6 has rank r and its smallest singular value is denoted by $$\\sigma_{\\star r}$$. Moreover, consider Uprune as the output of the pruning phase in Algorithm 1 with parameters \u03b2, \u03bb, \u03f5, \u03b3 satisfying the conditions,\n\n$$\\beta = c_{\\beta}(\\sigma_{\\star r})^2, \\quad \\lambda = c_{\\lambda} \\sqrt{kr}, \\quad \\gamma \\leq c_{\\gamma} \\sqrt{kr^{5/2}}, \\quad \\epsilon \\leq c_{\\epsilon} \\sqrt{kr^{5/2}}, \\quad (10)$$\n\nfor some absolute constants $$c_{\\beta}, c_{\\lambda}, c_{\\epsilon}, c_{\\gamma} > 0$$. Then, we have\n1. Uprune has exactly r columns.\n2. $$\\|U_{\\text{prune}}U^T_{\\text{prune}} - U\u22c6U^T\u22c6\\|_F \\leq \\frac{1}{2}(\\sigma_{\\star r})^2$$.", "md": "Algorithm 1 Greedy pruning based on group-Lasso regularization\n\nInputs: Measurements {(Ai, yi) where yi = $$\\langle$$Ai, U\u22c6U T$$\\rangle$$} + \u03b5i}n  i=1 (in the population setting n = \u221e);\n\nInitialization: Set parameters: \u03bb, \u03b2, \u03f5, \u03b3 and mfine-tune.\n\nGreedy pruning phase:\n\n1: Find an (\u03f5, \u03b3)-approximate SOSP of femp (resp. fpop), U, satisfying $$\\|U\\|_{op} \\leq 3$$.\n\n2: Let S = {i \u2208 [k] : $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$ denote the set of columns with small \u21132 norm.\nCreate a new matrix Uprune which only preserves the columns of U in [k] \\ S, deleting the columns in S.\n\nFine-tuning phase:\n\n1: Run mfine-tune iterations of gradient descent on Lpop(U) (resp. Lemp(U)) initialized at Uprune to get Uout.\n\n2: return Uout.\n\nRegularized loss are suitable for greedy pruning, while at the same time achieving a small but non-zero generalization error. Note that the \u21132 norm is a non-smooth function at the origin, and therefore, the overall regularized loss is non-smooth and non-convex. While there are several notions of approximate stationary points for non-differentiable and non-convex functions, for technical convenience, we replace R by a smooth proxy. In particular, for a smoothing parameter \u03b2 > 0, define a smooth version of the \u21132 norm, and the corresponding smooth regularizer R\u03b2 as,\n\n$$R_{\\beta}(U) = \\sum_{i=1}^{k} \\ell_{\\beta}(2(Ue_i)), \\quad \\ell_{\\beta}(v) = \\|v\\|_2^2 + \\beta$$\n\nNote that the smaller the value of \u03b2 is, the closer is $$\\ell_{\\beta}(v)$$ to $$\\|v\\|_2$$. Considering this definition, the overall regularized loss we study in the population setting is\n\n$$f_{pop}(U) = L_{pop}(U) + \\lambda R_{\\beta}(U), \\quad (9)$$\n\nwhere $$L_{pop} = \\|UU^T - U\u22c6U^T\u22c6\\|_F^2$$ as defined in eq. (2). The above optimization problem is non-convex due to the structure of Lpop(U), and finding its global minimizer can be computationally prohibitive. Fortunately, for our theoretical results, we do not require achieving global optimality and we only require an approximate second-order stationary point of the loss in eq. (9), which is defined below.\n\nDefinition 2. We say that U is an (\u03f5, \u03b3)-approximate second-order stationary point of f if,\n1. The gradient norm is bounded above by \u03f5, i.e., $$\\|\\nabla f(U)\\|_2 \\leq \\epsilon$$.\n2. The eigenvalues of the Hessian are larger than -\u03b3, i.e., $$\\lambda_{\\text{min}}(\\nabla^2 f(U)) \\geq -\\gamma$$.\n\nThe full algorithmic procedure that we analyze in this section is summarized in Algorithm 1. Once we find an (\u03f5, \u03b3)-approximate second-order stationary point (SOSP) of eq. (9) for some proper choices of \u03f5 and \u03b3, we apply greedy pruning on the obtained model U and by eliminating all of its columns with \u21132 norm below a specific threshold. As a result, the pruned model Uprune has fewer columns than the trained model U. In fact, in our following theoretical results, we show that if the parameters are properly selected, Uprune has exactly r columns, which is the same as U\u22c6. Finally, we fine-tune the pruned solution by running gradient descent on the unregularized loss Lpop (resp. Lemp). Next, we state the properties of the pruned model generated by Algorithm 1 in the infinite sample setting.\n\nTheorem 3. Consider the population loss with regularization in eq. (9), where U\u22c6 has rank r and its smallest singular value is denoted by $$\\sigma_{\\star r}$$. Moreover, consider Uprune as the output of the pruning phase in Algorithm 1 with parameters \u03b2, \u03bb, \u03f5, \u03b3 satisfying the conditions,\n\n$$\\beta = c_{\\beta}(\\sigma_{\\star r})^2, \\quad \\lambda = c_{\\lambda} \\sqrt{kr}, \\quad \\gamma \\leq c_{\\gamma} \\sqrt{kr^{5/2}}, \\quad \\epsilon \\leq c_{\\epsilon} \\sqrt{kr^{5/2}}, \\quad (10)$$\n\nfor some absolute constants $$c_{\\beta}, c_{\\lambda}, c_{\\epsilon}, c_{\\gamma} > 0$$. Then, we have\n1. Uprune has exactly r columns.\n2. $$\\|U_{\\text{prune}}U^T_{\\text{prune}} - U\u22c6U^T\u22c6\\|_F \\leq \\frac{1}{2}(\\sigma_{\\star r})^2$$."}]}, {"page": 7, "text": "This result relies on showing that all bounded SOSPs of the regularized loss in eq. (9) are suitable for\n greedy pruning: removing the columns of U below a certain \u21132-norm threshold results in a solution\n Uprune having exactly r columns, while at the same time having a small generalization error. Hence, it\n can serve as a proper warm-start for the fine-tuning phase.\n Proof sketch of Theorem 3.         The key idea is to identify that if we have a matrix U such that\n UU T = U\u22c6U T  \u22c6 , and the columns of U are orthogonal to one another, then U has exactly r non-zero\n columns, where r is the rank of U\u22c6. This statement can be shown to hold even when UU T \u2248           U\u22c6U T\u22c6\n and the columns of U are only approximately orthogonal. The main observation we prove is that a\n bounded (\u03f5, \u03b3)-approximate SOSPs of eq. (9) denoted by U satisfies the following condition:\n                       \u2200i, j : \u2225Uei\u22252, \u2225Uej\u22252 \u2265     2   \u03b2,      \u27e8Uei, Uej\u27e9     \u2248  0.                   (11)\n                                                              \u2225Uei\u22252\u2225Uej\u22252\n In other words, all the large columns of U have their pairwise angle approximately 90\u25e6. Thus, by\n pruning away the columns of U that have an \u21132 norm less than 2\u221a\u03b2, the remaining columns of U,\n i.e., the columns of Uprune, are now approximately at 90\u25e6      angles to one another. If \u03b2 is chosen to\n be sufficiently small, after deleting the low-norm columns, the approximation UpruneU T     prune \u2248 UU T\n holds. By the second order stationarity of U, we also have that UU T \u2248            U\u22c6U T\u22c6 . Together, this\n implies that UpruneU Tprune \u2248 U\u22c6U T\u22c6  and UpruneU Tprune is close to a rank r matrix. Since Uprune has\n orthogonal columns, this also means that it has exactly r columns. Finally, to establish a bound\n on the approximation error, we simply use the triangle inequality that \u2225UpruneU T    prune \u2212 U\u22c6U T\u22c6 \u2225F \u2264\n \u2225UU T \u2212   U\u22c6U T\u22c6 \u2225F + \u2225UU T \u2212     UpruneU T\n                                           prune\u2225F . The former is small by virtue of the fact that U is an\n approximate second order stationary point of fpop \u2248    Lpop when \u03bb is small; the latter term is small by\n the fact that only the small norm columns of U were pruned away.\n Now the only missing part that remains to justify is why the R\u03b2 regularizer promotes orthogonality\n in the columns of approximate second order stationary points and the expression in eq. (11) holds.\nThis is best understood by looking at the regularized loss for the case \u03b2 = 0, which is equivalent\n to \u2225UU T \u2212   U\u22c6U T\u22c6 \u22252\n                      F + \u03bb  k  i=1 \u2225Uei\u22252 and consider any candidate first-order stationary point U\n of this objective. Let Z \u2208    Rd\u00d7k be a variable constrained to satisfy ZZT = UU T . Stationarity\n of U implies that the choice Z = U must also be a first-order stationary point of the constrained\n optimization problem,\n                                                  k\n           Minimize: \u2225ZZT \u2212       U\u22c6U T\u22c6\u22252F + \u03bb   \u2225Zei\u22252,           Subject to: ZZT = UU T .           (12)\n                                                 i=1\nThe first term in the objective is a constant under the constraint and we may remove it altogether.\nWhen U is a full-rank stationary point, constraint qualification holds, and it is possible to write down\n the necessary KKT first-order optimality conditions, which reduce to,\n                              \u2200i \u2208 [k],   \u2212\u03bb    Zei    + (\u039b + \u039bT )Zei = 0                              (13)\n                                              \u2225Zei\u22252\nwhere \u039b \u2208    Rd\u00d7d is the set of optimal dual variables. Since Z = U is a first-order stationary point of\n the problem in eq. (12) and it satisfies eq. (13), the above condition means that the columns Uei are\n the eigenvectors of the symmetric matrix \u039b + \u039bT . If all the eigenvalues of \u039b + \u039bT were distinct,\n then this implies that the eigenvectors are orthogonal and Uei \u22a5     Uej for all i \u0338= j.\nWhile this analysis conveys an intuitive picture, there are several challenges in extending this further.\n It is unclear how to establish that the eigenvalues of \u039b + \u039bT are distinct. Moreover, this analysis only\n applies for full-rank stationary points and does not say anything about rank deficient stationary points\n U, where constraint qualification does not hold. Furthermore, it is even more unclear how to extend\n this analysis to approximate stationary points. Our proof will circumvents each of these challenges\n by (a) showing that at approximate SOSPs, even if the eigenvalues of \u039b + \u039bT are not distinct, the\n columns of U are orthogonal, and (b) directly bounding the gradient and Hessian of the regularized\n loss, rather than studying the KKT conditions to establish guarantees even for approximate stationary\n points which may be rank deficient. Having established guarantees for the pruning phase of the\n algorithm in the population setting, we next prove a result in the finite sample setting.               \u25a0\n                                                     7", "md": "# Math Equations and Text\n\nThis result relies on showing that all bounded SOSPs of the regularized loss in eq. (9) are suitable for greedy pruning: removing the columns of U below a certain \u21132-norm threshold results in a solution Uprune having exactly r columns, while at the same time having a small generalization error. Hence, it can serve as a proper warm-start for the fine-tuning phase.\n\nProof sketch of Theorem 3. The key idea is to identify that if we have a matrix U such that $$UU^T = U^*U^{T*}$$, and the columns of U are orthogonal to one another, then U has exactly r non-zero columns, where r is the rank of U*. This statement can be shown to hold even when $$UU^T \\approx U^*U^{T*}$$ and the columns of U are only approximately orthogonal. The main observation we prove is that a bounded (\u03f5, \u03b3)-approximate SOSPs of eq. (9) denoted by U satisfies the following condition:\n\n$$\\forall i, j : \\left\\|Ue_i\\right\\|_2, \\left\\|Ue_j\\right\\|_2 \\geq 2\\sqrt{\\beta}, \\langle Ue_i, Ue_j \\rangle \\approx 0. \\quad (11)$$\n\nIn other words, all the large columns of U have their pairwise angle approximately 90\u00b0. Thus, by pruning away the columns of U that have an \u21132 norm less than 2\u221a\u03b2, the remaining columns of U, i.e., the columns of Uprune, are now approximately at 90\u00b0 angles to one another. If \u03b2 is chosen to be sufficiently small, after deleting the low-norm columns, the approximation $$U_{\\text{prune}}U_{\\text{T prun}} \\approx UU^T$$ holds. By the second order stationarity of U, we also have that $$UU^T \\approx U^*U^{T*}$$. Together, this implies that $$U_{\\text{prune}}U_{\\text{T prun}} \\approx U^*U^{T*}$$ and $$U_{\\text{prune}}U_{\\text{T prun}}$$ is close to a rank r matrix. Since Uprune has orthogonal columns, this also means that it has exactly r columns. Finally, to establish a bound on the approximation error, we simply use the triangle inequality that $$\\left\\|U_{\\text{prune}}U_{\\text{T prun}} - U^*U^{T*}\\right\\|_F \\leq \\left\\|UU^T - U^*U^{T*}\\right\\|_F + \\left\\|UU^T - U_{\\text{prune}}U_{\\text{T prun}}\\right\\|_F$$. The former is small by virtue of the fact that U is an approximate second order stationary point of fpop \u2248 Lpop when \u03bb is small; the latter term is small by the fact that only the small norm columns of U were pruned away.\n\nNow the only missing part that remains to justify is why the R\u03b2 regularizer promotes orthogonality in the columns of approximate second order stationary points and the expression in eq. (11) holds. This is best understood by looking at the regularized loss for the case \u03b2 = 0, which is equivalent to $$\\left\\|UU^T - U^*U^{T*}\\right\\|_2^2 + \\lambda \\sum_{i=1}^{k} \\left\\|Ue_i\\right\\|_2$$ and consider any candidate first-order stationary point U of this objective. Let Z \u2208 \u211dd\u00d7k be a variable constrained to satisfy ZZT = UUT. Stationarity of U implies that the choice Z = U must also be a first-order stationary point of the constrained optimization problem,\n\n$$\\text{Minimize: } \\left\\|ZZ^T - U^*U^{T*}\\right\\|_2^2 + \\lambda \\sum_{i=1}^{k} \\left\\|Ze_i\\right\\|_2, \\quad \\text{Subject to: } ZZ^T = UU^T. \\quad (12)$$\n\nThe first term in the objective is a constant under the constraint and we may remove it altogether. When U is a full-rank stationary point, constraint qualification holds, and it is possible to write down the necessary KKT first-order optimality conditions, which reduce to,\n\n$$\\forall i \\in [k], -\\lambda Ze_i + (\\Lambda + \\Lambda^T)Ze_i = 0, \\quad \\frac{\\left\\|Ze_i\\right\\|_2}{\\left\\|Ze_i\\right\\|_2} \\quad (13)$$\n\nwhere \u039b \u2208 \u211dd\u00d7d is the set of optimal dual variables. Since Z = U is a first-order stationary point of the problem in eq. (12) and it satisfies eq. (13), the above condition means that the columns Uei are the eigenvectors of the symmetric matrix \u039b + \u039bT. If all the eigenvalues of \u039b + \u039bT were distinct, then this implies that the eigenvectors are orthogonal and Uei \u22a5 Uej for all i \u2260 j.\n\nWhile this analysis conveys an intuitive picture, there are several challenges in extending this further. It is unclear how to establish that the eigenvalues of \u039b + \u039bT are distinct. Moreover, this analysis only applies for full-rank stationary points and does not say anything about rank deficient stationary points U, where constraint qualification does not hold. Furthermore, it is even more unclear how to extend this analysis to approximate stationary points. Our proof will circumvent each of these challenges by (a) showing that at approximate SOSPs, even if the eigenvalues of \u039b + \u039bT are not distinct, the columns of U are orthogonal, and (b) directly bounding the gradient and Hessian of the regularized loss, rather than studying the KKT conditions to establish guarantees even for approximate stationary points which may be rank deficient. Having established guarantees for the pruning phase of the algorithm in the population setting, we next prove a result in the finite sample setting. \u25a0\n\n7", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "This result relies on showing that all bounded SOSPs of the regularized loss in eq. (9) are suitable for greedy pruning: removing the columns of U below a certain \u21132-norm threshold results in a solution Uprune having exactly r columns, while at the same time having a small generalization error. Hence, it can serve as a proper warm-start for the fine-tuning phase.\n\nProof sketch of Theorem 3. The key idea is to identify that if we have a matrix U such that $$UU^T = U^*U^{T*}$$, and the columns of U are orthogonal to one another, then U has exactly r non-zero columns, where r is the rank of U*. This statement can be shown to hold even when $$UU^T \\approx U^*U^{T*}$$ and the columns of U are only approximately orthogonal. The main observation we prove is that a bounded (\u03f5, \u03b3)-approximate SOSPs of eq. (9) denoted by U satisfies the following condition:\n\n$$\\forall i, j : \\left\\|Ue_i\\right\\|_2, \\left\\|Ue_j\\right\\|_2 \\geq 2\\sqrt{\\beta}, \\langle Ue_i, Ue_j \\rangle \\approx 0. \\quad (11)$$\n\nIn other words, all the large columns of U have their pairwise angle approximately 90\u00b0. Thus, by pruning away the columns of U that have an \u21132 norm less than 2\u221a\u03b2, the remaining columns of U, i.e., the columns of Uprune, are now approximately at 90\u00b0 angles to one another. If \u03b2 is chosen to be sufficiently small, after deleting the low-norm columns, the approximation $$U_{\\text{prune}}U_{\\text{T prun}} \\approx UU^T$$ holds. By the second order stationarity of U, we also have that $$UU^T \\approx U^*U^{T*}$$. Together, this implies that $$U_{\\text{prune}}U_{\\text{T prun}} \\approx U^*U^{T*}$$ and $$U_{\\text{prune}}U_{\\text{T prun}}$$ is close to a rank r matrix. Since Uprune has orthogonal columns, this also means that it has exactly r columns. Finally, to establish a bound on the approximation error, we simply use the triangle inequality that $$\\left\\|U_{\\text{prune}}U_{\\text{T prun}} - U^*U^{T*}\\right\\|_F \\leq \\left\\|UU^T - U^*U^{T*}\\right\\|_F + \\left\\|UU^T - U_{\\text{prune}}U_{\\text{T prun}}\\right\\|_F$$. The former is small by virtue of the fact that U is an approximate second order stationary point of fpop \u2248 Lpop when \u03bb is small; the latter term is small by the fact that only the small norm columns of U were pruned away.\n\nNow the only missing part that remains to justify is why the R\u03b2 regularizer promotes orthogonality in the columns of approximate second order stationary points and the expression in eq. (11) holds. This is best understood by looking at the regularized loss for the case \u03b2 = 0, which is equivalent to $$\\left\\|UU^T - U^*U^{T*}\\right\\|_2^2 + \\lambda \\sum_{i=1}^{k} \\left\\|Ue_i\\right\\|_2$$ and consider any candidate first-order stationary point U of this objective. Let Z \u2208 \u211dd\u00d7k be a variable constrained to satisfy ZZT = UUT. Stationarity of U implies that the choice Z = U must also be a first-order stationary point of the constrained optimization problem,\n\n$$\\text{Minimize: } \\left\\|ZZ^T - U^*U^{T*}\\right\\|_2^2 + \\lambda \\sum_{i=1}^{k} \\left\\|Ze_i\\right\\|_2, \\quad \\text{Subject to: } ZZ^T = UU^T. \\quad (12)$$\n\nThe first term in the objective is a constant under the constraint and we may remove it altogether. When U is a full-rank stationary point, constraint qualification holds, and it is possible to write down the necessary KKT first-order optimality conditions, which reduce to,\n\n$$\\forall i \\in [k], -\\lambda Ze_i + (\\Lambda + \\Lambda^T)Ze_i = 0, \\quad \\frac{\\left\\|Ze_i\\right\\|_2}{\\left\\|Ze_i\\right\\|_2} \\quad (13)$$\n\nwhere \u039b \u2208 \u211dd\u00d7d is the set of optimal dual variables. Since Z = U is a first-order stationary point of the problem in eq. (12) and it satisfies eq. (13), the above condition means that the columns Uei are the eigenvectors of the symmetric matrix \u039b + \u039bT. If all the eigenvalues of \u039b + \u039bT were distinct, then this implies that the eigenvectors are orthogonal and Uei \u22a5 Uej for all i \u2260 j.\n\nWhile this analysis conveys an intuitive picture, there are several challenges in extending this further. It is unclear how to establish that the eigenvalues of \u039b + \u039bT are distinct. Moreover, this analysis only applies for full-rank stationary points and does not say anything about rank deficient stationary points U, where constraint qualification does not hold. Furthermore, it is even more unclear how to extend this analysis to approximate stationary points. Our proof will circumvent each of these challenges by (a) showing that at approximate SOSPs, even if the eigenvalues of \u039b + \u039bT are not distinct, the columns of U are orthogonal, and (b) directly bounding the gradient and Hessian of the regularized loss, rather than studying the KKT conditions to establish guarantees even for approximate stationary points which may be rank deficient. Having established guarantees for the pruning phase of the algorithm in the population setting, we next prove a result in the finite sample setting. \u25a0\n\n7", "md": "This result relies on showing that all bounded SOSPs of the regularized loss in eq. (9) are suitable for greedy pruning: removing the columns of U below a certain \u21132-norm threshold results in a solution Uprune having exactly r columns, while at the same time having a small generalization error. Hence, it can serve as a proper warm-start for the fine-tuning phase.\n\nProof sketch of Theorem 3. The key idea is to identify that if we have a matrix U such that $$UU^T = U^*U^{T*}$$, and the columns of U are orthogonal to one another, then U has exactly r non-zero columns, where r is the rank of U*. This statement can be shown to hold even when $$UU^T \\approx U^*U^{T*}$$ and the columns of U are only approximately orthogonal. The main observation we prove is that a bounded (\u03f5, \u03b3)-approximate SOSPs of eq. (9) denoted by U satisfies the following condition:\n\n$$\\forall i, j : \\left\\|Ue_i\\right\\|_2, \\left\\|Ue_j\\right\\|_2 \\geq 2\\sqrt{\\beta}, \\langle Ue_i, Ue_j \\rangle \\approx 0. \\quad (11)$$\n\nIn other words, all the large columns of U have their pairwise angle approximately 90\u00b0. Thus, by pruning away the columns of U that have an \u21132 norm less than 2\u221a\u03b2, the remaining columns of U, i.e., the columns of Uprune, are now approximately at 90\u00b0 angles to one another. If \u03b2 is chosen to be sufficiently small, after deleting the low-norm columns, the approximation $$U_{\\text{prune}}U_{\\text{T prun}} \\approx UU^T$$ holds. By the second order stationarity of U, we also have that $$UU^T \\approx U^*U^{T*}$$. Together, this implies that $$U_{\\text{prune}}U_{\\text{T prun}} \\approx U^*U^{T*}$$ and $$U_{\\text{prune}}U_{\\text{T prun}}$$ is close to a rank r matrix. Since Uprune has orthogonal columns, this also means that it has exactly r columns. Finally, to establish a bound on the approximation error, we simply use the triangle inequality that $$\\left\\|U_{\\text{prune}}U_{\\text{T prun}} - U^*U^{T*}\\right\\|_F \\leq \\left\\|UU^T - U^*U^{T*}\\right\\|_F + \\left\\|UU^T - U_{\\text{prune}}U_{\\text{T prun}}\\right\\|_F$$. The former is small by virtue of the fact that U is an approximate second order stationary point of fpop \u2248 Lpop when \u03bb is small; the latter term is small by the fact that only the small norm columns of U were pruned away.\n\nNow the only missing part that remains to justify is why the R\u03b2 regularizer promotes orthogonality in the columns of approximate second order stationary points and the expression in eq. (11) holds. This is best understood by looking at the regularized loss for the case \u03b2 = 0, which is equivalent to $$\\left\\|UU^T - U^*U^{T*}\\right\\|_2^2 + \\lambda \\sum_{i=1}^{k} \\left\\|Ue_i\\right\\|_2$$ and consider any candidate first-order stationary point U of this objective. Let Z \u2208 \u211dd\u00d7k be a variable constrained to satisfy ZZT = UUT. Stationarity of U implies that the choice Z = U must also be a first-order stationary point of the constrained optimization problem,\n\n$$\\text{Minimize: } \\left\\|ZZ^T - U^*U^{T*}\\right\\|_2^2 + \\lambda \\sum_{i=1}^{k} \\left\\|Ze_i\\right\\|_2, \\quad \\text{Subject to: } ZZ^T = UU^T. \\quad (12)$$\n\nThe first term in the objective is a constant under the constraint and we may remove it altogether. When U is a full-rank stationary point, constraint qualification holds, and it is possible to write down the necessary KKT first-order optimality conditions, which reduce to,\n\n$$\\forall i \\in [k], -\\lambda Ze_i + (\\Lambda + \\Lambda^T)Ze_i = 0, \\quad \\frac{\\left\\|Ze_i\\right\\|_2}{\\left\\|Ze_i\\right\\|_2} \\quad (13)$$\n\nwhere \u039b \u2208 \u211dd\u00d7d is the set of optimal dual variables. Since Z = U is a first-order stationary point of the problem in eq. (12) and it satisfies eq. (13), the above condition means that the columns Uei are the eigenvectors of the symmetric matrix \u039b + \u039bT. If all the eigenvalues of \u039b + \u039bT were distinct, then this implies that the eigenvectors are orthogonal and Uei \u22a5 Uej for all i \u2260 j.\n\nWhile this analysis conveys an intuitive picture, there are several challenges in extending this further. It is unclear how to establish that the eigenvalues of \u039b + \u039bT are distinct. Moreover, this analysis only applies for full-rank stationary points and does not say anything about rank deficient stationary points U, where constraint qualification does not hold. Furthermore, it is even more unclear how to extend this analysis to approximate stationary points. Our proof will circumvent each of these challenges by (a) showing that at approximate SOSPs, even if the eigenvalues of \u039b + \u039bT are not distinct, the columns of U are orthogonal, and (b) directly bounding the gradient and Hessian of the regularized loss, rather than studying the KKT conditions to establish guarantees even for approximate stationary points which may be rank deficient. Having established guarantees for the pruning phase of the algorithm in the population setting, we next prove a result in the finite sample setting. \u25a0\n\n7"}]}, {"page": 8, "text": " 5     Finite sample analysis\n Next, we extend the results of the previous section to the finite sample setting. Here, we also focus on\n the smooth version of the regularizer and study the following problem\n                                             femp(U) = Lemp(U) + \u03bbR\u03b2(U),                                                       (14)\nwhere the empirical loss Lemp is defined in eq. (1) and the smooth version of the group Lasso\n regularizer R\u03b2(U) is defined in eq. (8). In the finite sample setting, we assume that the measurement\n matrices satisfy the restricted isometry property (RIP) [43], defined below.\nAssumption 1. Assume that the measurement matrices {A1, \u00b7 \u00b7 \u00b7 , An} are (2k, \u03b4)-RIP. In other\n words, for any d \u00d7 d matrix with rank \u2264               2k,   n\n                                 (1 \u2212   \u03b4)\u2225X\u22252   F \u2264    n1  \u27e8Ai, X\u27e92 \u2264           (1 + \u03b4)\u2225X\u22252     F .                           (15)\n                                                            i=1\nThis condition is satisfied, for instance, if the entries of the Ai\u2019s were sampled N(0, 1/d) (i.e.\nGaussian measurements), as long as n \u2273                  dk/\u03b42 [43].\nTheorem 4. Consider the empirical loss with smooth regularization in eq. (14), where U\u22c6                                has rank r\n and unit spectral norm, with its smallest singular value denoted \u03c3\u22c6                  r, and noise variance \u03c32. Consider\n Uprune as the output of the pruning phase in Algorithm 1 with parameters \u03b2, \u03bb, \u03f5, \u03b3 chosen as per\n                                                                        \u03c32                                        (\u03c3\u22c6r )3/2\n eq. (10). If the number of samples is at least n \u2265               C4  (\u03c3\u22c6r )4 dk2r5 log(d/\u03b7) and \u03b4 \u2264           c\u03b4 \u221a kr5/2 , where\n C4 > 0 is a sufficiently large constant, then with probability at least 1 \u2212                      \u03b7,\n        1. Uprune has exactly r columns.\n        2. Uprune satisfies the spectral initialization condition: \u2225UpruneU T                prune \u2212   U\u22c6U T \u22c6  \u2225F \u2264    1    r)2.\n                                                                                                                        2(\u03c3\u22c6\nWith Gaussian measurements, the overall sample requirement (including that from the RIP condition)\n in Theorem 4 is satisfied when n \u2265              \u2126   1+\u03c32               . The high level analysis of this result largely\n                                                       (\u03c3\u22c6\n                                                         r )4 dk2r5\n follows that of Theorem 3 in the population setting \u2013 we approximate the finite-sample gradient\n and Hessian by their population counterparts and show that the approximation error decays with the\n number of samples as O(1/\u221an).\n 5.1    Fine-tuning phase: Realizing the benefits of pruning\n In the fine-tuning phase, the learner runs a few iterations of gradient descent on Lemp initialized at the\n pruned solution Uprune. Since the model is no longer overparameterized after pruning (by Theorem 4),\n there are several works analyzing the generalization performance and iteration complexity of gradient\n descent. Here we borrow the local convergence result of [22] which requires the initial condition\n that \u2225U0U T  0 \u2212    U\u22c6U T \u22c6  \u2225F \u2264     c(\u03c3\u22c6r)2, where c is any constant less than 1. As shown in part (b) of\nTheorem 4, this initial condition is satisfied by Uprune.\nTheorem 5. [22, Corollary 2] Suppose \u2225U\u22c6\u2225op = 1. If we use the output of the greedy pruning\n in Algorithm 1 denoted by Uprune \u2208              Rd\u00d7r as the initial iterate for the fine-tuning phase, then after\n t \u2265   mfine-tune = C5 (\u03c3\u22c6     1/\u03c3\u22c6 r)10 \u00b7 log (\u03c3\u22c6   r/\u03c3\u22c6 1 \u00b7 n/d) iterations, for some sufficiently large absolute\n C5 > 0, the iterates of factored gradient descent satisfy,\n                                           \u2225UtU T t \u2212    U\u22c6U T               \u03c3        rd                                       (16)\n                                                               \u22c6 \u2225F \u2272     (\u03c3\u22c6r)2      n .\nTheorem 5 shows that in the fine-tuning phase, the iterates of gradient descent converges at a linear\n rate to the generalization error floor of \u03c3/(\u03c3\u22c6             r)2 \u00b7      rd/n, which is also known to be statistically\n(minimax) optimal [44, 45]. This is possible because the learner is able to correctly identify the rank of\n the model in the pruning phase and operate in the exactly specified setting in the fine-tuning phase. In\n contrast, one may ask how this guarantee compares with running vanilla factored gradient descent in\n the overparameterized setting. This corresponds to the case where no pruning is carried out to reduce\n the size of the model. [23] presented a guarantee for the convergence of factored gradient descent\n from a warm start in the overparameterized setting. In comparison with the exactly specified case, the\n                                                                  8", "md": "# Finite Sample Analysis\n\n## Finite sample analysis\n\nNext, we extend the results of the previous section to the finite sample setting. Here, we also focus on the smooth version of the regularizer and study the following problem:\n\n$$\nf_{\\text{emp}}(U) = L_{\\text{emp}}(U) + \\lambda R_{\\beta}(U) \\quad \\text{(14)}\n$$\n\nwhere the empirical loss \\(L_{\\text{emp}}\\) is defined in eq. (1) and the smooth version of the group Lasso regularizer \\(R_{\\beta}(U)\\) is defined in eq. (8). In the finite sample setting, we assume that the measurement matrices satisfy the restricted isometry property (RIP) [43], defined below.\n\nAssumption 1. Assume that the measurement matrices \\(\\{A_1, \\ldots, A_n\\}\\) are \\((2k, \\delta)\\)-RIP. In other words, for any \\(d \\times d\\) matrix with rank \\(\\leq 2k\\),\n\n$$\n(1 - \\delta) \\|X\\|_F^2 \\leq \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\leq (1 + \\delta) \\|X\\|_F^2 \\quad \\text{(15)}\n$$\n\nThis condition is satisfied, for instance, if the entries of the \\(A_i\\)'s were sampled \\(N(0, 1/d)\\) (i.e. Gaussian measurements), as long as \\(n \\gtrsim dk/\\delta^2\\) [43].\n\nTheorem 4. Consider the empirical loss with smooth regularization in eq. (14), where \\(U^*\\) has rank \\(r\\) and unit spectral norm, with its smallest singular value denoted \\(\\sigma^*_r\\), and noise variance \\(\\sigma^2\\). Consider \\(U_{\\text{prune}}\\) as the output of the pruning phase in Algorithm 1 with parameters \\(\\beta\\), \\(\\lambda\\), \\(\\epsilon\\), \\(\\gamma\\) chosen as per eq. (10). If the number of samples is at least \\(n \\geq C_4 (\\sigma^*_r)^4 dk^2r^5 \\log(d/\\eta)\\) and \\(\\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}\\), where \\(C_4 > 0\\) is a sufficiently large constant, then with probability at least \\(1 - \\eta\\),\n\n1. \\(U_{\\text{prune}}\\) has exactly \\(r\\) columns.\n2. \\(U_{\\text{prune}}\\) satisfies the spectral initialization condition: \\(\\|U_{\\text{prune}}U_{\\text{prune}}^T - U^*{U^*}^T\\|_F \\leq \\frac{1}{2}(\\sigma^*_r)^{3/2}\\).\n\nWith Gaussian measurements, the overall sample requirement (including that from the RIP condition) in Theorem 4 is satisfied when \\(n \\geq \\Omega \\frac{1+\\sigma^2}{(\\sigma^*_r)^4 dk^2r^5}\\). The high-level analysis of this result largely follows that of Theorem 3 in the population setting \u2013 we approximate the finite-sample gradient and Hessian by their population counterparts and show that the approximation error decays with the number of samples as \\(O(1/\\sqrt{n})\\).\n\n### Fine-tuning phase: Realizing the benefits of pruning\n\nIn the fine-tuning phase, the learner runs a few iterations of gradient descent on \\(L_{\\text{emp}}\\) initialized at the pruned solution \\(U_{\\text{prune}}\\). Since the model is no longer overparameterized after pruning (by Theorem 4), there are several works analyzing the generalization performance and iteration complexity of gradient descent. Here we borrow the local convergence result of [22] which requires the initial condition that \\(\\|U_0U_0^T - U^*{U^*}^T\\|_F \\leq c(\\sigma^*_r)^2\\), where \\(c\\) is any constant less than 1. As shown in part (b) of Theorem 4, this initial condition is satisfied by \\(U_{\\text{prune}}\\).\n\nTheorem 5. [22, Corollary 2] Suppose \\(\\|U^*\\|_{\\text{op}} = 1\\). If we use the output of the greedy pruning in Algorithm 1 denoted by \\(U_{\\text{prune}} \\in \\mathbb{R}^{d \\times r}\\) as the initial iterate for the fine-tuning phase, then after \\(t \\geq m_{\\text{fine-tune}} = C_5 (\\sigma^*_r)^{1/\\sigma^*_r}10 \\cdot \\log (\\sigma^*_r/\\sigma^*1 \\cdot n/d)\\) iterations, for some sufficiently large absolute \\(C_5 > 0\\), the iterates of factored gradient descent satisfy,\n\n$$\n\\|U_tU_t^T - U^*{U^*}^T\\|_F \\lesssim (\\sigma^*_r)^2 n \\quad \\text{(16)}\n$$\n\nTheorem 5 shows that in the fine-tuning phase, the iterates of gradient descent converge at a linear rate to the generalization error floor of \\(\\sigma/(\\sigma^*_r)^2 \\cdot rd/n\\), which is also known to be statistically (minimax) optimal [44, 45]. This is possible because the learner is able to correctly identify the rank of the model in the pruning phase and operate in the exactly specified setting in the fine-tuning phase. In contrast, one may ask how this guarantee compares with running vanilla factored gradient descent in the overparameterized setting. This corresponds to the case where no pruning is carried out to reduce the size of the model. [23] presented a guarantee for the convergence of factored gradient descent from a warm start in the overparameterized setting. In comparison with the exactly specified case, the", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Finite Sample Analysis", "md": "# Finite Sample Analysis"}, {"type": "heading", "lvl": 2, "value": "Finite sample analysis", "md": "## Finite sample analysis"}, {"type": "text", "value": "Next, we extend the results of the previous section to the finite sample setting. Here, we also focus on the smooth version of the regularizer and study the following problem:\n\n$$\nf_{\\text{emp}}(U) = L_{\\text{emp}}(U) + \\lambda R_{\\beta}(U) \\quad \\text{(14)}\n$$\n\nwhere the empirical loss \\(L_{\\text{emp}}\\) is defined in eq. (1) and the smooth version of the group Lasso regularizer \\(R_{\\beta}(U)\\) is defined in eq. (8). In the finite sample setting, we assume that the measurement matrices satisfy the restricted isometry property (RIP) [43], defined below.\n\nAssumption 1. Assume that the measurement matrices \\(\\{A_1, \\ldots, A_n\\}\\) are \\((2k, \\delta)\\)-RIP. In other words, for any \\(d \\times d\\) matrix with rank \\(\\leq 2k\\),\n\n$$\n(1 - \\delta) \\|X\\|_F^2 \\leq \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\leq (1 + \\delta) \\|X\\|_F^2 \\quad \\text{(15)}\n$$\n\nThis condition is satisfied, for instance, if the entries of the \\(A_i\\)'s were sampled \\(N(0, 1/d)\\) (i.e. Gaussian measurements), as long as \\(n \\gtrsim dk/\\delta^2\\) [43].\n\nTheorem 4. Consider the empirical loss with smooth regularization in eq. (14), where \\(U^*\\) has rank \\(r\\) and unit spectral norm, with its smallest singular value denoted \\(\\sigma^*_r\\), and noise variance \\(\\sigma^2\\). Consider \\(U_{\\text{prune}}\\) as the output of the pruning phase in Algorithm 1 with parameters \\(\\beta\\), \\(\\lambda\\), \\(\\epsilon\\), \\(\\gamma\\) chosen as per eq. (10). If the number of samples is at least \\(n \\geq C_4 (\\sigma^*_r)^4 dk^2r^5 \\log(d/\\eta)\\) and \\(\\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}\\), where \\(C_4 > 0\\) is a sufficiently large constant, then with probability at least \\(1 - \\eta\\),\n\n1. \\(U_{\\text{prune}}\\) has exactly \\(r\\) columns.\n2. \\(U_{\\text{prune}}\\) satisfies the spectral initialization condition: \\(\\|U_{\\text{prune}}U_{\\text{prune}}^T - U^*{U^*}^T\\|_F \\leq \\frac{1}{2}(\\sigma^*_r)^{3/2}\\).\n\nWith Gaussian measurements, the overall sample requirement (including that from the RIP condition) in Theorem 4 is satisfied when \\(n \\geq \\Omega \\frac{1+\\sigma^2}{(\\sigma^*_r)^4 dk^2r^5}\\). The high-level analysis of this result largely follows that of Theorem 3 in the population setting \u2013 we approximate the finite-sample gradient and Hessian by their population counterparts and show that the approximation error decays with the number of samples as \\(O(1/\\sqrt{n})\\).", "md": "Next, we extend the results of the previous section to the finite sample setting. Here, we also focus on the smooth version of the regularizer and study the following problem:\n\n$$\nf_{\\text{emp}}(U) = L_{\\text{emp}}(U) + \\lambda R_{\\beta}(U) \\quad \\text{(14)}\n$$\n\nwhere the empirical loss \\(L_{\\text{emp}}\\) is defined in eq. (1) and the smooth version of the group Lasso regularizer \\(R_{\\beta}(U)\\) is defined in eq. (8). In the finite sample setting, we assume that the measurement matrices satisfy the restricted isometry property (RIP) [43], defined below.\n\nAssumption 1. Assume that the measurement matrices \\(\\{A_1, \\ldots, A_n\\}\\) are \\((2k, \\delta)\\)-RIP. In other words, for any \\(d \\times d\\) matrix with rank \\(\\leq 2k\\),\n\n$$\n(1 - \\delta) \\|X\\|_F^2 \\leq \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\leq (1 + \\delta) \\|X\\|_F^2 \\quad \\text{(15)}\n$$\n\nThis condition is satisfied, for instance, if the entries of the \\(A_i\\)'s were sampled \\(N(0, 1/d)\\) (i.e. Gaussian measurements), as long as \\(n \\gtrsim dk/\\delta^2\\) [43].\n\nTheorem 4. Consider the empirical loss with smooth regularization in eq. (14), where \\(U^*\\) has rank \\(r\\) and unit spectral norm, with its smallest singular value denoted \\(\\sigma^*_r\\), and noise variance \\(\\sigma^2\\). Consider \\(U_{\\text{prune}}\\) as the output of the pruning phase in Algorithm 1 with parameters \\(\\beta\\), \\(\\lambda\\), \\(\\epsilon\\), \\(\\gamma\\) chosen as per eq. (10). If the number of samples is at least \\(n \\geq C_4 (\\sigma^*_r)^4 dk^2r^5 \\log(d/\\eta)\\) and \\(\\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}\\), where \\(C_4 > 0\\) is a sufficiently large constant, then with probability at least \\(1 - \\eta\\),\n\n1. \\(U_{\\text{prune}}\\) has exactly \\(r\\) columns.\n2. \\(U_{\\text{prune}}\\) satisfies the spectral initialization condition: \\(\\|U_{\\text{prune}}U_{\\text{prune}}^T - U^*{U^*}^T\\|_F \\leq \\frac{1}{2}(\\sigma^*_r)^{3/2}\\).\n\nWith Gaussian measurements, the overall sample requirement (including that from the RIP condition) in Theorem 4 is satisfied when \\(n \\geq \\Omega \\frac{1+\\sigma^2}{(\\sigma^*_r)^4 dk^2r^5}\\). The high-level analysis of this result largely follows that of Theorem 3 in the population setting \u2013 we approximate the finite-sample gradient and Hessian by their population counterparts and show that the approximation error decays with the number of samples as \\(O(1/\\sqrt{n})\\)."}, {"type": "heading", "lvl": 3, "value": "Fine-tuning phase: Realizing the benefits of pruning", "md": "### Fine-tuning phase: Realizing the benefits of pruning"}, {"type": "text", "value": "In the fine-tuning phase, the learner runs a few iterations of gradient descent on \\(L_{\\text{emp}}\\) initialized at the pruned solution \\(U_{\\text{prune}}\\). Since the model is no longer overparameterized after pruning (by Theorem 4), there are several works analyzing the generalization performance and iteration complexity of gradient descent. Here we borrow the local convergence result of [22] which requires the initial condition that \\(\\|U_0U_0^T - U^*{U^*}^T\\|_F \\leq c(\\sigma^*_r)^2\\), where \\(c\\) is any constant less than 1. As shown in part (b) of Theorem 4, this initial condition is satisfied by \\(U_{\\text{prune}}\\).\n\nTheorem 5. [22, Corollary 2] Suppose \\(\\|U^*\\|_{\\text{op}} = 1\\). If we use the output of the greedy pruning in Algorithm 1 denoted by \\(U_{\\text{prune}} \\in \\mathbb{R}^{d \\times r}\\) as the initial iterate for the fine-tuning phase, then after \\(t \\geq m_{\\text{fine-tune}} = C_5 (\\sigma^*_r)^{1/\\sigma^*_r}10 \\cdot \\log (\\sigma^*_r/\\sigma^*1 \\cdot n/d)\\) iterations, for some sufficiently large absolute \\(C_5 > 0\\), the iterates of factored gradient descent satisfy,\n\n$$\n\\|U_tU_t^T - U^*{U^*}^T\\|_F \\lesssim (\\sigma^*_r)^2 n \\quad \\text{(16)}\n$$\n\nTheorem 5 shows that in the fine-tuning phase, the iterates of gradient descent converge at a linear rate to the generalization error floor of \\(\\sigma/(\\sigma^*_r)^2 \\cdot rd/n\\), which is also known to be statistically (minimax) optimal [44, 45]. This is possible because the learner is able to correctly identify the rank of the model in the pruning phase and operate in the exactly specified setting in the fine-tuning phase. In contrast, one may ask how this guarantee compares with running vanilla factored gradient descent in the overparameterized setting. This corresponds to the case where no pruning is carried out to reduce the size of the model. [23] presented a guarantee for the convergence of factored gradient descent from a warm start in the overparameterized setting. In comparison with the exactly specified case, the", "md": "In the fine-tuning phase, the learner runs a few iterations of gradient descent on \\(L_{\\text{emp}}\\) initialized at the pruned solution \\(U_{\\text{prune}}\\). Since the model is no longer overparameterized after pruning (by Theorem 4), there are several works analyzing the generalization performance and iteration complexity of gradient descent. Here we borrow the local convergence result of [22] which requires the initial condition that \\(\\|U_0U_0^T - U^*{U^*}^T\\|_F \\leq c(\\sigma^*_r)^2\\), where \\(c\\) is any constant less than 1. As shown in part (b) of Theorem 4, this initial condition is satisfied by \\(U_{\\text{prune}}\\).\n\nTheorem 5. [22, Corollary 2] Suppose \\(\\|U^*\\|_{\\text{op}} = 1\\). If we use the output of the greedy pruning in Algorithm 1 denoted by \\(U_{\\text{prune}} \\in \\mathbb{R}^{d \\times r}\\) as the initial iterate for the fine-tuning phase, then after \\(t \\geq m_{\\text{fine-tune}} = C_5 (\\sigma^*_r)^{1/\\sigma^*_r}10 \\cdot \\log (\\sigma^*_r/\\sigma^*1 \\cdot n/d)\\) iterations, for some sufficiently large absolute \\(C_5 > 0\\), the iterates of factored gradient descent satisfy,\n\n$$\n\\|U_tU_t^T - U^*{U^*}^T\\|_F \\lesssim (\\sigma^*_r)^2 n \\quad \\text{(16)}\n$$\n\nTheorem 5 shows that in the fine-tuning phase, the iterates of gradient descent converge at a linear rate to the generalization error floor of \\(\\sigma/(\\sigma^*_r)^2 \\cdot rd/n\\), which is also known to be statistically (minimax) optimal [44, 45]. This is possible because the learner is able to correctly identify the rank of the model in the pruning phase and operate in the exactly specified setting in the fine-tuning phase. In contrast, one may ask how this guarantee compares with running vanilla factored gradient descent in the overparameterized setting. This corresponds to the case where no pruning is carried out to reduce the size of the model. [23] presented a guarantee for the convergence of factored gradient descent from a warm start in the overparameterized setting. In comparison with the exactly specified case, the"}]}, {"page": 9, "text": "                                                                       mudell\n                       L1                02*320,vanils nrdicnt          prued\n                                              mDGT     of iterations\nFigure 2: Comparison of generalization of vanilla gradient descent vs. greedy pruning + fine-tuning.\nVanilla gradient descent is carried out to minimize the unregularized loss in eq. (1) (red). The empirical version\nof the group Lasso regularized loss, eq. (3), is used in the initial training phase of greedy pruning + fine-tuning\n(blue). The resulting solution is pruned based on the observed column norms, and fine-tuned using gradient\ndescent on the unregularized loss. In this figure, d = k = 20 and r = 3 and the learner has access to n = 100\nGaussian measurements with no added label noise. The figures plot the generalization error, \u2225UtU T   t \u2212U\u22c6U T  \u22c6 \u22252\n                                                                                                                  F\nas a function of the iteration number, t. The regularization parameter in eq. (3) was tuned minimally for greedy\npruning + fine-tuning objective. A sharp improvement in the generalization error is observed after pruning is\ncarried out and the error floor is observed to be an order of magnitude better for the pruned + fine-tuned solution.\ngeneralization error floor limt\u2192\u221e        \u2225UtU Tt \u2212   U\u22c6U T\u22c6  \u2225F is shown to scale as O(\u03c3/(\u03c3\u22c6       r)2 \u00b7   kd/n)6\nwhich now depends on k. Furthermore, linear convergence can no longer be established because of\nthe ill-conditioning of the objective. This is not just an artifact of the proof - experimentally too, the\nconvergence slowdown was noticed in [23, Figure 1].\nThis discussion shows that greedy pruning the model first, prior to running gradient descent, in fact\ngenerates solutions which generalize better and also converge much faster.\nRemark 6. Based on Theorems 4 and 5, under Gaussian measurements, given\n                            n \u2265  n\u03b5 = O        \u03c32    rd                                                        (17)\n                                             (\u03c3\u22c6     \u03b52 + 1 + \u03c32\n                                                r)4          (\u03c3\u22c6\n                                                               r)4 dk2r5 log(d/\u03b7)\nsamples, Algorithm 1 produces Uout that has exactly r columns and satisfies \u2225UoutU T          out\u2212U\u22c6U T  \u22c6 \u2225F \u2264    \u03b5.\nNote that the sample complexity depends on the amount of overparameterization, k, only in the lower\norder (independent of \u03b5) term. Note that n\u03b5 is non-zero even if \u03c3 = 0.\n6    Implementing Algorithm 1: Smoothness and optimization oracles\nIn this section we instantiate the optimization oracle in Algorithm 1, which outputs an approximate\nSOSP with bounded operator norm. First, we establish that the loss femp is well behaved on the\ndomain {U : \u2225U\u2225op \u2264        3}, in that its gradient and Hessian are Lipschitz continuous. These conditions\nare required by many generic optimization algorithms which return approximate second order\nstationary points [46, 47]. We establish these properties for the population loss for simplicity and\nleave extensions to the empirical loss for future work.\nTheorem 7. Consider the population loss fpop in eq. (9). Assume \u03bb \u2264              min{\u03b2, \u221a\u03b2} and \u2225U\u22c6\u2225op = 1.\nThe objective fpop(\u00b7) defined in eq. (9) satisfies for any U, V \u2208         Rd\u00d7k such that \u2225U\u2225op, \u2225V \u2225op \u2264        3,\n      (a) Lipschitz gradients: \u2225\u2207fpop(U) \u2212         \u2207fpop(V )\u2225F \u2272      \u2225U \u2212    V \u2225F ,\n    6The result is often stated in terms of the smallest non-zero eigenvalue of X\u22c6  = U\u22c6U T \u22c6 , which equals (\u03c3\u22c6r)2.\n                                                         9", "md": "mudell\n\nL1 02*320,vanils nrdicnt prued\n\nFigure 2: Comparison of generalization of vanilla gradient descent vs. greedy pruning + fine-tuning.\n\nVanilla gradient descent is carried out to minimize the unregularized loss in eq. (1) (red). The empirical version of the group Lasso regularized loss, eq. (3), is used in the initial training phase of greedy pruning + fine-tuning (blue). The resulting solution is pruned based on the observed column norms, and fine-tuned using gradient descent on the unregularized loss. In this figure, d = k = 20 and r = 3 and the learner has access to n = 100 Gaussian measurements with no added label noise. The figures plot the generalization error, $$\\|U_tU_t^T - U^*U^T\\|_F^2$$ as a function of the iteration number, t. The regularization parameter in eq. (3) was tuned minimally for greedy pruning + fine-tuning objective. A sharp improvement in the generalization error is observed after pruning is carried out and the error floor is observed to be an order of magnitude better for the pruned + fine-tuned solution. generalization error floor $$\\lim_{t\\to\\infty} \\|U_tU_t^T - U^*U^T\\|_F$$ is shown to scale as $$O\\left(\\frac{\\sigma}{(\\sigma^*r)^2} \\cdot \\frac{kd}{n}\\right)^6$$ which now depends on k. Furthermore, linear convergence can no longer be established because of the ill-conditioning of the objective. This is not just an artifact of the proof - experimentally too, the convergence slowdown was noticed in [23, Figure 1].\n\nThis discussion shows that greedy pruning the model first, prior to running gradient descent, in fact generates solutions which generalize better and also converge much faster.\n\nRemark 6. Based on Theorems 4 and 5, under Gaussian measurements, given $$n \\geq n_\\varepsilon = O\\left(\\frac{\\sigma^2 rd}{(\\sigma^*\\varepsilon^2 + 1 + \\sigma^2r)^4 (\\sigma^*r)^4 dk^2r^5 \\log(d/\\eta)}\\right)$$ samples, Algorithm 1 produces Uout that has exactly r columns and satisfies $$\\|U_{out}U_{out}^T - U^*U^T\\|_F \\leq \\varepsilon$$. Note that the sample complexity depends on the amount of overparameterization, k, only in the lower order (independent of $$\\varepsilon$$) term. Note that $$n_\\varepsilon$$ is non-zero even if $$\\sigma = 0$$.\n\n6 Implementing Algorithm 1: Smoothness and optimization oracles\n\nIn this section we instantiate the optimization oracle in Algorithm 1, which outputs an approximate SOSP with bounded operator norm. First, we establish that the loss femp is well behaved on the domain {U : $$\\|U\\|_{op} \\leq 3$$}, in that its gradient and Hessian are Lipschitz continuous. These conditions are required by many generic optimization algorithms which return approximate second order stationary points [46, 47]. We establish these properties for the population loss for simplicity and leave extensions to the empirical loss for future work.\n\nTheorem 7. Consider the population loss fpop in eq. (9). Assume $$\\lambda \\leq \\min\\{\\beta, \\sqrt{\\beta}\\}$$ and $$\\|U^*\\|_{op} = 1$$. The objective fpop(\u00b7) defined in eq. (9) satisfies for any U, V \u2208 $$\\mathbb{R}^{d \\times k}$$ such that $$\\|U\\|_{op}, \\|V\\|_{op} \\leq 3$$,\n\n(a) Lipschitz gradients: $$\\|\\nabla f_{pop}(U) - \\nabla f_{pop}(V)\\|_F \\lesssim \\|U - V\\|_F$$,", "images": [{"name": "page-9-0.jpg", "height": 205, "width": 238, "x": 187, "y": 72}], "items": [{"type": "text", "value": "mudell\n\nL1 02*320,vanils nrdicnt prued\n\nFigure 2: Comparison of generalization of vanilla gradient descent vs. greedy pruning + fine-tuning.\n\nVanilla gradient descent is carried out to minimize the unregularized loss in eq. (1) (red). The empirical version of the group Lasso regularized loss, eq. (3), is used in the initial training phase of greedy pruning + fine-tuning (blue). The resulting solution is pruned based on the observed column norms, and fine-tuned using gradient descent on the unregularized loss. In this figure, d = k = 20 and r = 3 and the learner has access to n = 100 Gaussian measurements with no added label noise. The figures plot the generalization error, $$\\|U_tU_t^T - U^*U^T\\|_F^2$$ as a function of the iteration number, t. The regularization parameter in eq. (3) was tuned minimally for greedy pruning + fine-tuning objective. A sharp improvement in the generalization error is observed after pruning is carried out and the error floor is observed to be an order of magnitude better for the pruned + fine-tuned solution. generalization error floor $$\\lim_{t\\to\\infty} \\|U_tU_t^T - U^*U^T\\|_F$$ is shown to scale as $$O\\left(\\frac{\\sigma}{(\\sigma^*r)^2} \\cdot \\frac{kd}{n}\\right)^6$$ which now depends on k. Furthermore, linear convergence can no longer be established because of the ill-conditioning of the objective. This is not just an artifact of the proof - experimentally too, the convergence slowdown was noticed in [23, Figure 1].\n\nThis discussion shows that greedy pruning the model first, prior to running gradient descent, in fact generates solutions which generalize better and also converge much faster.\n\nRemark 6. Based on Theorems 4 and 5, under Gaussian measurements, given $$n \\geq n_\\varepsilon = O\\left(\\frac{\\sigma^2 rd}{(\\sigma^*\\varepsilon^2 + 1 + \\sigma^2r)^4 (\\sigma^*r)^4 dk^2r^5 \\log(d/\\eta)}\\right)$$ samples, Algorithm 1 produces Uout that has exactly r columns and satisfies $$\\|U_{out}U_{out}^T - U^*U^T\\|_F \\leq \\varepsilon$$. Note that the sample complexity depends on the amount of overparameterization, k, only in the lower order (independent of $$\\varepsilon$$) term. Note that $$n_\\varepsilon$$ is non-zero even if $$\\sigma = 0$$.\n\n6 Implementing Algorithm 1: Smoothness and optimization oracles\n\nIn this section we instantiate the optimization oracle in Algorithm 1, which outputs an approximate SOSP with bounded operator norm. First, we establish that the loss femp is well behaved on the domain {U : $$\\|U\\|_{op} \\leq 3$$}, in that its gradient and Hessian are Lipschitz continuous. These conditions are required by many generic optimization algorithms which return approximate second order stationary points [46, 47]. We establish these properties for the population loss for simplicity and leave extensions to the empirical loss for future work.\n\nTheorem 7. Consider the population loss fpop in eq. (9). Assume $$\\lambda \\leq \\min\\{\\beta, \\sqrt{\\beta}\\}$$ and $$\\|U^*\\|_{op} = 1$$. The objective fpop(\u00b7) defined in eq. (9) satisfies for any U, V \u2208 $$\\mathbb{R}^{d \\times k}$$ such that $$\\|U\\|_{op}, \\|V\\|_{op} \\leq 3$$,\n\n(a) Lipschitz gradients: $$\\|\\nabla f_{pop}(U) - \\nabla f_{pop}(V)\\|_F \\lesssim \\|U - V\\|_F$$,", "md": "mudell\n\nL1 02*320,vanils nrdicnt prued\n\nFigure 2: Comparison of generalization of vanilla gradient descent vs. greedy pruning + fine-tuning.\n\nVanilla gradient descent is carried out to minimize the unregularized loss in eq. (1) (red). The empirical version of the group Lasso regularized loss, eq. (3), is used in the initial training phase of greedy pruning + fine-tuning (blue). The resulting solution is pruned based on the observed column norms, and fine-tuned using gradient descent on the unregularized loss. In this figure, d = k = 20 and r = 3 and the learner has access to n = 100 Gaussian measurements with no added label noise. The figures plot the generalization error, $$\\|U_tU_t^T - U^*U^T\\|_F^2$$ as a function of the iteration number, t. The regularization parameter in eq. (3) was tuned minimally for greedy pruning + fine-tuning objective. A sharp improvement in the generalization error is observed after pruning is carried out and the error floor is observed to be an order of magnitude better for the pruned + fine-tuned solution. generalization error floor $$\\lim_{t\\to\\infty} \\|U_tU_t^T - U^*U^T\\|_F$$ is shown to scale as $$O\\left(\\frac{\\sigma}{(\\sigma^*r)^2} \\cdot \\frac{kd}{n}\\right)^6$$ which now depends on k. Furthermore, linear convergence can no longer be established because of the ill-conditioning of the objective. This is not just an artifact of the proof - experimentally too, the convergence slowdown was noticed in [23, Figure 1].\n\nThis discussion shows that greedy pruning the model first, prior to running gradient descent, in fact generates solutions which generalize better and also converge much faster.\n\nRemark 6. Based on Theorems 4 and 5, under Gaussian measurements, given $$n \\geq n_\\varepsilon = O\\left(\\frac{\\sigma^2 rd}{(\\sigma^*\\varepsilon^2 + 1 + \\sigma^2r)^4 (\\sigma^*r)^4 dk^2r^5 \\log(d/\\eta)}\\right)$$ samples, Algorithm 1 produces Uout that has exactly r columns and satisfies $$\\|U_{out}U_{out}^T - U^*U^T\\|_F \\leq \\varepsilon$$. Note that the sample complexity depends on the amount of overparameterization, k, only in the lower order (independent of $$\\varepsilon$$) term. Note that $$n_\\varepsilon$$ is non-zero even if $$\\sigma = 0$$.\n\n6 Implementing Algorithm 1: Smoothness and optimization oracles\n\nIn this section we instantiate the optimization oracle in Algorithm 1, which outputs an approximate SOSP with bounded operator norm. First, we establish that the loss femp is well behaved on the domain {U : $$\\|U\\|_{op} \\leq 3$$}, in that its gradient and Hessian are Lipschitz continuous. These conditions are required by many generic optimization algorithms which return approximate second order stationary points [46, 47]. We establish these properties for the population loss for simplicity and leave extensions to the empirical loss for future work.\n\nTheorem 7. Consider the population loss fpop in eq. (9). Assume $$\\lambda \\leq \\min\\{\\beta, \\sqrt{\\beta}\\}$$ and $$\\|U^*\\|_{op} = 1$$. The objective fpop(\u00b7) defined in eq. (9) satisfies for any U, V \u2208 $$\\mathbb{R}^{d \\times k}$$ such that $$\\|U\\|_{op}, \\|V\\|_{op} \\leq 3$$,\n\n(a) Lipschitz gradients: $$\\|\\nabla f_{pop}(U) - \\nabla f_{pop}(V)\\|_F \\lesssim \\|U - V\\|_F$$,"}]}, {"page": 10, "text": "     (b) Lipschitz Hessians: \u2225\u22072fpop(U) \u2212      \u22072fpop(V )\u2225op \u2272    \u2225U \u2212  V \u2225F .\nUnder the Lipschitz gradients and Lipschitz Hessian condition, a large number of algorithms in\nthe literature show convergence to an approximate SOSP. A common approach for finding such\na point is using the noisy gradient descent algorithm [48]. However, note that we establish these\nLipschitzness properties on the bounded domain {U : \u2225U\u2225op \u2264         3}, and it remains to verify whether\nthese algorithms indeed approach such points. A similar concern is present with Algorithm 1, which\nrequires access to an optimization oracle which finds an (\u03f5, \u03b4)-approximate SOSP of femp which are\nalso bounded, in that \u2225U\u2225op \u2264   3. The final step is to identify conditions under which these algorithms\nindeed output stationary points which are bounded, satisfying \u2225U\u2225op \u2264      3. We establish this behavior\nfor a wide family of perturbed gradient based methods.\nPerturbed gradient descent:       We consider gradient descent with the following update rule: starting\nfrom the initialization U0, for all t \u2265 0,\n                                  Ut+1 \u2190    Ut \u2212  \u03b1(\u2207fpop(Ut) + Pt)                                  (18)\nwhere Pt is a perturbation term, which for example, could be the explicit noise (Pt \u223c    Unif(B(r)) for\nappropriate r) added to escape strict saddle points in [48]. Over the course of running the update rule\neq. (18), we show that \u2225Ut\u2225op remains bounded under mild conditions if the algorithm is initialized\nwithin this ball. In combination with Theorem 7, this shows that the noisy gradient descent approach\nof [48] can be used to find the SOSPs required for Algorithm 1.\nTheorem 8. Consider optimization of the regularized population loss using the update rule in eq. (18).\nAssume that \u2225U\u22c6\u2225op = 1 and suppose the parameters are selected as\u03b1 \u2264          1/8 and \u03bb \u2264   \u221a\u03b2, and we\nhave \u2225Pt\u2225op \u2264    1 almost surely for each t \u2265    0. Then, assuming that the condition \u2225U0\u2225op \u2264       3 is\nsatisfied at initialization, for every t \u22651, we have \u2225Ut\u2225op \u2264   3.\nThus, when perturbed gradient descent is carried out to optimize the regularized loss fpop eq. (9), the\nalgorithm always returns bounded iterates. In conjunction with Theorem 7, this implies, for example,\nthat noisy gradient descent [48] converges to an approximate second-order stationary point of the\nregularized population objective.\n7    Conclusion\nIn this paper, we studied the efficacy of the greedy pruning + fine-tuning pipeline in learning low-\ncomplexity solutions for the matrix sensing problem, as well as for learning shallow neural networks\nwith quadratic activation functions. We showed that training on the mean squared error augmented\nby a natural group Lasso regularizer results in models which are suitable for greedy pruning. Given\nsufficiently many samples, after pruning away the columns below a certain \u21132-norm threshold, we\narrived at a solution with the correct column sparsity of r. Running a few iterations of gradient\ndescent to fine-tune the resulting model, the population loss was shown to converge at a linear rate to\nan error floor of O(   rd/n), which is also statistically optimal. We also presented a negative result\nshowing the importance of regularization while training the model. To the best of our knowledge, our\nresults provide the first theoretical guarantee on the generalization error of the model obtained via the\ngreedy pruning + fine-tuning framework.\nAcknowledgements\nThe research of A. Mokhtari is supported in part by NSF Grants 2007668 and 2127697, ARO\nGrant W911NF2110226, the National AI Institute for Foundations of Machine Learning (IFML), the\nMachine Learning Lab (MLL), and the Wireless Networking and Communications Group (WNCG)\nindustrial affiliates program at UT Austin. Kannan Ramchandran would like to acknowledge support\nfrom NSF CIF-2002821 and ARO fund 051242-00.\nReferences\n [1] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference:\n      A whitepaper, 2018.\n                                                   10", "md": "**(b) Lipschitz Hessians:** $$\\|\\nabla^2 f_{\\text{pop}}(U) - \\nabla^2 f_{\\text{pop}}(V)\\|_{\\text{op}} \\lesssim \\|\\mathbf{U} - \\mathbf{V}\\|_F.$$\n\nUnder the Lipschitz gradients and Lipschitz Hessian condition, a large number of algorithms in the literature show convergence to an approximate SOSP. A common approach for finding such a point is using the noisy gradient descent algorithm [48]. However, note that we establish these Lipschitzness properties on the bounded domain {U : $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$}, and it remains to verify whether these algorithms indeed approach such points. A similar concern is present with Algorithm 1, which requires access to an optimization oracle which finds an $(\\epsilon, \\delta)$-approximate SOSP of $f_{\\text{emp}}$ which are also bounded, in that $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$. The final step is to identify conditions under which these algorithms indeed output stationary points which are bounded, satisfying $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$. We establish this behavior for a wide family of perturbed gradient-based methods.\n\n**Perturbed gradient descent:** We consider gradient descent with the following update rule: starting from the initialization $\\mathbf{U}_0$, for all $t \\geq 0$,\n\n$$\\mathbf{U}_{t+1} \\leftarrow \\mathbf{U}_t - \\alpha(\\nabla f_{\\text{pop}}(\\mathbf{U}_t) + \\mathbf{P}_t) \\quad \\text{(18)}$$\n\nwhere $\\mathbf{P}_t$ is a perturbation term, which for example, could be the explicit noise ($\\mathbf{P}_t \\sim \\text{Unif}(B(r))$ for appropriate $r$) added to escape strict saddle points in [48]. Over the course of running the update rule eq. (18), we show that $\\|\\mathbf{U}_t\\|_{\\text{op}}$ remains bounded under mild conditions if the algorithm is initialized within this ball. In combination with Theorem 7, this shows that the noisy gradient descent approach of [48] can be used to find the SOSPs required for Algorithm 1.\n\n**Theorem 8.** Consider optimization of the regularized population loss using the update rule in eq. (18). Assume that $\\|\\mathbf{U}^*\\|_{\\text{op}} = 1$ and suppose the parameters are selected as $\\alpha \\leq 1/8$ and $\\lambda \\leq \\sqrt{\\beta}$, and we have $\\|\\mathbf{P}_t\\|_{\\text{op}} \\leq 1$ almost surely for each $t \\geq 0$. Then, assuming that the condition $\\|\\mathbf{U}_0\\|_{\\text{op}} \\leq 3$ is satisfied at initialization, for every $t \\geq 1$, we have $\\|\\mathbf{U}_t\\|_{\\text{op}} \\leq 3$.\n\nThus, when perturbed gradient descent is carried out to optimize the regularized loss $f_{\\text{pop}}$ eq. (9), the algorithm always returns bounded iterates. In conjunction with Theorem 7, this implies, for example, that noisy gradient descent [48] converges to an approximate second-order stationary point of the regularized population objective.\n\n**Conclusion**\n\nIn this paper, we studied the efficacy of the greedy pruning + fine-tuning pipeline in learning low-complexity solutions for the matrix sensing problem, as well as for learning shallow neural networks with quadratic activation functions. We showed that training on the mean squared error augmented by a natural group Lasso regularizer results in models which are suitable for greedy pruning. Given sufficiently many samples, after pruning away the columns below a certain $\\ell_2$-norm threshold, we arrived at a solution with the correct column sparsity of $r$. Running a few iterations of gradient descent to fine-tune the resulting model, the population loss was shown to converge at a linear rate to an error floor of $O(rd/n)$, which is also statistically optimal. We also presented a negative result showing the importance of regularization while training the model. To the best of our knowledge, our results provide the first theoretical guarantee on the generalization error of the model obtained via the greedy pruning + fine-tuning framework.\n\n**Acknowledgements**\n\nThe research of A. Mokhtari is supported in part by NSF Grants 2007668 and 2127697, ARO Grant W911NF2110226, the National AI Institute for Foundations of Machine Learning (IFML), the Machine Learning Lab (MLL), and the Wireless Networking and Communications Group (WNCG) industrial affiliates program at UT Austin. Kannan Ramchandran would like to acknowledge support from NSF CIF-2002821 and ARO fund 051242-00.\n\n**References**\n\n1. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper, 2018.", "images": [], "items": [{"type": "text", "value": "**(b) Lipschitz Hessians:** $$\\|\\nabla^2 f_{\\text{pop}}(U) - \\nabla^2 f_{\\text{pop}}(V)\\|_{\\text{op}} \\lesssim \\|\\mathbf{U} - \\mathbf{V}\\|_F.$$\n\nUnder the Lipschitz gradients and Lipschitz Hessian condition, a large number of algorithms in the literature show convergence to an approximate SOSP. A common approach for finding such a point is using the noisy gradient descent algorithm [48]. However, note that we establish these Lipschitzness properties on the bounded domain {U : $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$}, and it remains to verify whether these algorithms indeed approach such points. A similar concern is present with Algorithm 1, which requires access to an optimization oracle which finds an $(\\epsilon, \\delta)$-approximate SOSP of $f_{\\text{emp}}$ which are also bounded, in that $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$. The final step is to identify conditions under which these algorithms indeed output stationary points which are bounded, satisfying $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$. We establish this behavior for a wide family of perturbed gradient-based methods.\n\n**Perturbed gradient descent:** We consider gradient descent with the following update rule: starting from the initialization $\\mathbf{U}_0$, for all $t \\geq 0$,\n\n$$\\mathbf{U}_{t+1} \\leftarrow \\mathbf{U}_t - \\alpha(\\nabla f_{\\text{pop}}(\\mathbf{U}_t) + \\mathbf{P}_t) \\quad \\text{(18)}$$\n\nwhere $\\mathbf{P}_t$ is a perturbation term, which for example, could be the explicit noise ($\\mathbf{P}_t \\sim \\text{Unif}(B(r))$ for appropriate $r$) added to escape strict saddle points in [48]. Over the course of running the update rule eq. (18), we show that $\\|\\mathbf{U}_t\\|_{\\text{op}}$ remains bounded under mild conditions if the algorithm is initialized within this ball. In combination with Theorem 7, this shows that the noisy gradient descent approach of [48] can be used to find the SOSPs required for Algorithm 1.\n\n**Theorem 8.** Consider optimization of the regularized population loss using the update rule in eq. (18). Assume that $\\|\\mathbf{U}^*\\|_{\\text{op}} = 1$ and suppose the parameters are selected as $\\alpha \\leq 1/8$ and $\\lambda \\leq \\sqrt{\\beta}$, and we have $\\|\\mathbf{P}_t\\|_{\\text{op}} \\leq 1$ almost surely for each $t \\geq 0$. Then, assuming that the condition $\\|\\mathbf{U}_0\\|_{\\text{op}} \\leq 3$ is satisfied at initialization, for every $t \\geq 1$, we have $\\|\\mathbf{U}_t\\|_{\\text{op}} \\leq 3$.\n\nThus, when perturbed gradient descent is carried out to optimize the regularized loss $f_{\\text{pop}}$ eq. (9), the algorithm always returns bounded iterates. In conjunction with Theorem 7, this implies, for example, that noisy gradient descent [48] converges to an approximate second-order stationary point of the regularized population objective.\n\n**Conclusion**\n\nIn this paper, we studied the efficacy of the greedy pruning + fine-tuning pipeline in learning low-complexity solutions for the matrix sensing problem, as well as for learning shallow neural networks with quadratic activation functions. We showed that training on the mean squared error augmented by a natural group Lasso regularizer results in models which are suitable for greedy pruning. Given sufficiently many samples, after pruning away the columns below a certain $\\ell_2$-norm threshold, we arrived at a solution with the correct column sparsity of $r$. Running a few iterations of gradient descent to fine-tune the resulting model, the population loss was shown to converge at a linear rate to an error floor of $O(rd/n)$, which is also statistically optimal. We also presented a negative result showing the importance of regularization while training the model. To the best of our knowledge, our results provide the first theoretical guarantee on the generalization error of the model obtained via the greedy pruning + fine-tuning framework.\n\n**Acknowledgements**\n\nThe research of A. Mokhtari is supported in part by NSF Grants 2007668 and 2127697, ARO Grant W911NF2110226, the National AI Institute for Foundations of Machine Learning (IFML), the Machine Learning Lab (MLL), and the Wireless Networking and Communications Group (WNCG) industrial affiliates program at UT Austin. Kannan Ramchandran would like to acknowledge support from NSF CIF-2002821 and ARO fund 051242-00.\n\n**References**\n\n1. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper, 2018.", "md": "**(b) Lipschitz Hessians:** $$\\|\\nabla^2 f_{\\text{pop}}(U) - \\nabla^2 f_{\\text{pop}}(V)\\|_{\\text{op}} \\lesssim \\|\\mathbf{U} - \\mathbf{V}\\|_F.$$\n\nUnder the Lipschitz gradients and Lipschitz Hessian condition, a large number of algorithms in the literature show convergence to an approximate SOSP. A common approach for finding such a point is using the noisy gradient descent algorithm [48]. However, note that we establish these Lipschitzness properties on the bounded domain {U : $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$}, and it remains to verify whether these algorithms indeed approach such points. A similar concern is present with Algorithm 1, which requires access to an optimization oracle which finds an $(\\epsilon, \\delta)$-approximate SOSP of $f_{\\text{emp}}$ which are also bounded, in that $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$. The final step is to identify conditions under which these algorithms indeed output stationary points which are bounded, satisfying $\\|\\mathbf{U}\\|_{\\text{op}} \\leq 3$. We establish this behavior for a wide family of perturbed gradient-based methods.\n\n**Perturbed gradient descent:** We consider gradient descent with the following update rule: starting from the initialization $\\mathbf{U}_0$, for all $t \\geq 0$,\n\n$$\\mathbf{U}_{t+1} \\leftarrow \\mathbf{U}_t - \\alpha(\\nabla f_{\\text{pop}}(\\mathbf{U}_t) + \\mathbf{P}_t) \\quad \\text{(18)}$$\n\nwhere $\\mathbf{P}_t$ is a perturbation term, which for example, could be the explicit noise ($\\mathbf{P}_t \\sim \\text{Unif}(B(r))$ for appropriate $r$) added to escape strict saddle points in [48]. Over the course of running the update rule eq. (18), we show that $\\|\\mathbf{U}_t\\|_{\\text{op}}$ remains bounded under mild conditions if the algorithm is initialized within this ball. In combination with Theorem 7, this shows that the noisy gradient descent approach of [48] can be used to find the SOSPs required for Algorithm 1.\n\n**Theorem 8.** Consider optimization of the regularized population loss using the update rule in eq. (18). Assume that $\\|\\mathbf{U}^*\\|_{\\text{op}} = 1$ and suppose the parameters are selected as $\\alpha \\leq 1/8$ and $\\lambda \\leq \\sqrt{\\beta}$, and we have $\\|\\mathbf{P}_t\\|_{\\text{op}} \\leq 1$ almost surely for each $t \\geq 0$. Then, assuming that the condition $\\|\\mathbf{U}_0\\|_{\\text{op}} \\leq 3$ is satisfied at initialization, for every $t \\geq 1$, we have $\\|\\mathbf{U}_t\\|_{\\text{op}} \\leq 3$.\n\nThus, when perturbed gradient descent is carried out to optimize the regularized loss $f_{\\text{pop}}$ eq. (9), the algorithm always returns bounded iterates. In conjunction with Theorem 7, this implies, for example, that noisy gradient descent [48] converges to an approximate second-order stationary point of the regularized population objective.\n\n**Conclusion**\n\nIn this paper, we studied the efficacy of the greedy pruning + fine-tuning pipeline in learning low-complexity solutions for the matrix sensing problem, as well as for learning shallow neural networks with quadratic activation functions. We showed that training on the mean squared error augmented by a natural group Lasso regularizer results in models which are suitable for greedy pruning. Given sufficiently many samples, after pruning away the columns below a certain $\\ell_2$-norm threshold, we arrived at a solution with the correct column sparsity of $r$. Running a few iterations of gradient descent to fine-tune the resulting model, the population loss was shown to converge at a linear rate to an error floor of $O(rd/n)$, which is also statistically optimal. We also presented a negative result showing the importance of regularization while training the model. To the best of our knowledge, our results provide the first theoretical guarantee on the generalization error of the model obtained via the greedy pruning + fine-tuning framework.\n\n**Acknowledgements**\n\nThe research of A. Mokhtari is supported in part by NSF Grants 2007668 and 2127697, ARO Grant W911NF2110226, the National AI Institute for Foundations of Machine Learning (IFML), the Machine Learning Lab (MLL), and the Wireless Networking and Communications Group (WNCG) industrial affiliates program at UT Austin. Kannan Ramchandran would like to acknowledge support from NSF CIF-2002821 and ARO fund 051242-00.\n\n**References**\n\n1. Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A whitepaper, 2018."}]}, {"page": 11, "text": "  [2] Yunhui Guo. A survey on methods and theories of quantized neural networks, 2018.\n  [3] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for\n      efficient neural network. Advances in neural information processing systems, 28, 2015.\n  [4] Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for\n      accelerating deep convolutional neural networks, 2018.\n  [5] Yawei Li, Shuhang Gu, Luc Van Gool, and Radu Timofte. Learning filter basis for convolutional\n      neural network compression. In Proceedings of the IEEE/CVF International Conference on\n      Computer Vision, pages 5623\u20135632, 2019.\n  [6] Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham\n      Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In\n      International Conference on Machine Learning, pages 5544\u20135555. PMLR, 2020.\n  [7] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network,\n      2015.\n  [8] Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and\n      quantization, 2018.\n  [9] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural net-\n      works with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149,\n      2015.\n[10] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for\n      efficient convnets, 2016.\n[11] Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In\n      Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2554\u2013\n      2564, 2016.\n[12] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity\n      in deep neural networks. Advances in neural information processing systems, 29, 2016.\n[13] Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good\n      subnetworks provably exist: Pruning via greedy forward selection. In International Conference\n      on Machine Learning, pages 10820\u201310830. PMLR, 2020.\n[14] Brian Bartoldson, Ari Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-\n      stability tradeoff in neural network pruning. Advances in Neural Information Processing\n      Systems, 33:20852\u201320864, 2020.\n[15] Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks\n      through l0 regularization, 2017.\n[16] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.\n      Learning efficient convolutional networks through network slimming. In Proceedings of the\n      IEEE international conference on computer vision, pages 2736\u20132744, 2017.\n[17] Jianbo Ye, Xin Lu, Zhe Lin, and James Z. Wang. Rethinking the smaller-norm-less-informative\n      assumption in channel pruning of convolution layers, 2018.\n[18] Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse\n      regularization for deep neural networks. Neurocomputing, 241:81\u201389, 2017.\n[19] Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized\n      matrix sensing and neural networks with quadratic activations. In Conference On Learning\n      Theory, pages 2\u201347. PMLR, 2018.\n[20] Simon Du and Jason Lee. On the power of over-parametrization in neural networks with\n      quadratic activation. In International conference on machine learning, pages 1329\u20131338.\n      PMLR, 2018.\n                                                  11", "md": "# List of References\n\n# List of References\n\n|[2]|Yunhui Guo. A survey on methods and theories of quantized neural networks, 2018.|\n|---|---|\n|[3]|Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.|\n|[4]|Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks, 2018.|\n|[5]|Yawei Li, Shuhang Gu, Luc Van Gool, and Radu Timofte. Learning filter basis for convolutional neural network compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5623\u20135632, 2019.|\n|[6]|Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544\u20135555. PMLR, 2020.|\n|[7]|Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.|\n|[8]|Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization, 2018.|\n|[9]|Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.|\n|[10]|Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets, 2016.|\n|[11]|Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2554\u20132564, 2016.|\n|[12]|Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016.|\n|[13]|Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks provably exist: Pruning via greedy forward selection. In International Conference on Machine Learning, pages 10820\u201310830. PMLR, 2020.|\n|[14]|Brian Bartoldson, Ari Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-stability tradeoff in neural network pruning. Advances in Neural Information Processing Systems, 33:20852\u201320864, 2020.|\n|[15]|Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization, 2017.|\n|[16]|Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 2736\u20132744, 2017.|\n|[17]|Jianbo Ye, Xin Lu, Zhe Lin, and James Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers, 2018.|\n|[18]|Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regularization for deep neural networks. Neurocomputing, 241:81\u201389, 2017.|\n|[19]|Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pages 2\u201347. PMLR, 2018.|\n|[20]|Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic activation. In International conference on machine learning, pages 1329\u20131338. PMLR, 2018.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["[2]", "Yunhui Guo. A survey on methods and theories of quantized neural networks, 2018."], ["[3]", "Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015."], ["[4]", "Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks, 2018."], ["[5]", "Yawei Li, Shuhang Gu, Luc Van Gool, and Radu Timofte. Learning filter basis for convolutional neural network compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5623\u20135632, 2019."], ["[6]", "Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544\u20135555. PMLR, 2020."], ["[7]", "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015."], ["[8]", "Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization, 2018."], ["[9]", "Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015."], ["[10]", "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets, 2016."], ["[11]", "Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2554\u20132564, 2016."], ["[12]", "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016."], ["[13]", "Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks provably exist: Pruning via greedy forward selection. In International Conference on Machine Learning, pages 10820\u201310830. PMLR, 2020."], ["[14]", "Brian Bartoldson, Ari Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-stability tradeoff in neural network pruning. Advances in Neural Information Processing Systems, 33:20852\u201320864, 2020."], ["[15]", "Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization, 2017."], ["[16]", "Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 2736\u20132744, 2017."], ["[17]", "Jianbo Ye, Xin Lu, Zhe Lin, and James Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers, 2018."], ["[18]", "Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regularization for deep neural networks. Neurocomputing, 241:81\u201389, 2017."], ["[19]", "Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pages 2\u201347. PMLR, 2018."], ["[20]", "Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic activation. In International conference on machine learning, pages 1329\u20131338. PMLR, 2018."]], "md": "|[2]|Yunhui Guo. A survey on methods and theories of quantized neural networks, 2018.|\n|---|---|\n|[3]|Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.|\n|[4]|Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks, 2018.|\n|[5]|Yawei Li, Shuhang Gu, Luc Van Gool, and Radu Timofte. Learning filter basis for convolutional neural network compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5623\u20135632, 2019.|\n|[6]|Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544\u20135555. PMLR, 2020.|\n|[7]|Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.|\n|[8]|Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization, 2018.|\n|[9]|Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.|\n|[10]|Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets, 2016.|\n|[11]|Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2554\u20132564, 2016.|\n|[12]|Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016.|\n|[13]|Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks provably exist: Pruning via greedy forward selection. In International Conference on Machine Learning, pages 10820\u201310830. PMLR, 2020.|\n|[14]|Brian Bartoldson, Ari Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-stability tradeoff in neural network pruning. Advances in Neural Information Processing Systems, 33:20852\u201320864, 2020.|\n|[15]|Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization, 2017.|\n|[16]|Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 2736\u20132744, 2017.|\n|[17]|Jianbo Ye, Xin Lu, Zhe Lin, and James Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers, 2018.|\n|[18]|Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regularization for deep neural networks. Neurocomputing, 241:81\u201389, 2017.|\n|[19]|Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pages 2\u201347. PMLR, 2018.|\n|[20]|Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic activation. In International conference on machine learning, pages 1329\u20131338. PMLR, 2018.|", "isPerfectTable": true, "csv": "\"[2]\",\"Yunhui Guo. A survey on methods and theories of quantized neural networks, 2018.\"\n\"[3]\",\"Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015.\"\n\"[4]\",\"Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deep convolutional neural networks, 2018.\"\n\"[5]\",\"Yawei Li, Shuhang Gu, Luc Van Gool, and Radu Timofte. Learning filter basis for convolutional neural network compression. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5623\u20135632, 2019.\"\n\"[6]\",\"Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In International Conference on Machine Learning, pages 5544\u20135555. PMLR, 2020.\"\n\"[7]\",\"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.\"\n\"[8]\",\"Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization, 2018.\"\n\"[9]\",\"Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.\"\n\"[10]\",\"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets, 2016.\"\n\"[11]\",\"Vadim Lebedev and Victor Lempitsky. Fast convnets using group-wise brain damage. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2554\u20132564, 2016.\"\n\"[12]\",\"Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016.\"\n\"[13]\",\"Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks provably exist: Pruning via greedy forward selection. In International Conference on Machine Learning, pages 10820\u201310830. PMLR, 2020.\"\n\"[14]\",\"Brian Bartoldson, Ari Morcos, Adrian Barbu, and Gordon Erlebacher. The generalization-stability tradeoff in neural network pruning. Advances in Neural Information Processing Systems, 33:20852\u201320864, 2020.\"\n\"[15]\",\"Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization, 2017.\"\n\"[16]\",\"Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 2736\u20132744, 2017.\"\n\"[17]\",\"Jianbo Ye, Xin Lu, Zhe Lin, and James Z. Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers, 2018.\"\n\"[18]\",\"Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regularization for deep neural networks. Neurocomputing, 241:81\u201389, 2017.\"\n\"[19]\",\"Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory, pages 2\u201347. PMLR, 2018.\"\n\"[20]\",\"Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic activation. In International conference on machine learning, pages 1329\u20131338. PMLR, 2018.\""}]}, {"page": 12, "text": "[21] Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the op-\n      timization landscape of over-parameterized shallow neural networks. IEEE Transactions on\n      Information Theory, 65(2):742\u2013769, 2018.\n[22] Yudong Chen and Martin J. Wainwright. Fast low-rank estimation by projected gradient descent:\n      General statistical and algorithmic guarantees, 2015.\n[23] Jiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. On the computational\n      and statistical complexity of over-parameterized matrix sensing. CoRR, abs/2102.02756, 2021.\n[24] Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank\n      matrix factorization. Advances in Neural Information Processing Systems, 34:1429\u20131439, 2021.\n[25] Raghunandan Keshavan and Sewoong Oh. A gradient descent algorithm on the grassman\n      manifold for matrix completion. 910, 10 2009.\n[26] Dohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. Non-square\n      matrix sensing without spurious local minima via the burer-monteiro approach. In Artificial\n      Intelligence and Statistics, pages 65\u201374. PMLR, 2017.\n[27] Jian-Feng Cai, Emmanuel J. Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm\n      for matrix completion. SIAM Journal on Optimization, 20(4):1956\u20131982, 2010.\n[28] Prateek Jain, Raghu Meka, and Inderjit Dhillon. Guaranteed rank minimization via singular\n      value projection. Advances in Neural Information Processing Systems, 23, 2010.\n[29] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a\n      few entries. IEEE transactions on information theory, 56(6):2980\u20132998, 2010.\n[30] Shiqian Ma, Donald Goldfarb, and Lifeng Chen. Fixed point and bregman iterative methods for\n      matrix rank minimization. Mathematical Programming, 128(1-2):321\u2013353, 2011.\n[31] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning\n      Research, 12(12), 2011.\n[32] Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of\n      linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471\u2013501, jan 2010.\n[33] Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear\n      norm regularized least squares problems. Pacific Journal of Optimization, 6, 09 2010.\n[34] Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum.\n      Advances in neural information processing systems, 29, 2016.\n[35] Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search\n      for low rank matrix recovery. Advances in Neural Information Processing Systems, 29, 2016.\n[36] Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems:\n      A unified geometric analysis. CoRR, abs/1704.00708, 2017.\n[37] Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables.\n      Journal of the Royal Statistical Society Series B, 68:49\u201367, 02 2006.\n[38] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value\n      of network pruning. arXiv preprint arXiv:1810.05270, 2018.\n[39] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias:\n      On the role of implicit regularization in deep learning, 2014.\n[40] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The\n      implicit bias of gradient descent on separable data. The Journal of Machine Learning Research,\n      19(1):2822\u20132878, 2018.\n[41] Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati\n      Srebro.   Implicit regularization in matrix factorization.   Advances in Neural Information\n      Processing Systems, 30, 2017.\n                                                  12", "md": "- Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 65(2):742\u2013769, 2018.\n- Yudong Chen and Martin J. Wainwright. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees, 2015.\n- Jiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. On the computational and statistical complexity of over-parameterized matrix sensing. CoRR, abs/2102.02756, 2021.\n- Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank matrix factorization. Advances in Neural Information Processing Systems, 34:1429\u20131439, 2021.\n- Raghunandan Keshavan and Sewoong Oh. A gradient descent algorithm on the grassman manifold for matrix completion. 910, 10 2009.\n- Dohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the burer-monteiro approach. In Artificial Intelligence and Statistics, pages 65\u201374. PMLR, 2017.\n- Jian-Feng Cai, Emmanuel J. Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956\u20131982, 2010.\n- Prateek Jain, Raghu Meka, and Inderjit Dhillon. Guaranteed rank minimization via singular value projection. Advances in Neural Information Processing Systems, 23, 2010.\n- Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE transactions on information theory, 56(6):2980\u20132998, 2010.\n- Shiqian Ma, Donald Goldfarb, and Lifeng Chen. Fixed point and bregman iterative methods for matrix rank minimization. Mathematical Programming, 128(1-2):321\u2013353, 2011.\n- Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12(12), 2011.\n- Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471\u2013501, jan 2010.\n- Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems. Pacific Journal of Optimization, 6, 09 2010.\n- Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in neural information processing systems, 29, 2016.\n- Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. Advances in Neural Information Processing Systems, 29, 2016.\n- Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. CoRR, abs/1704.00708, 2017.\n- Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society Series B, 68:49\u201367, 02 2006.\n- Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018.\n- Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning, 2014.\n- Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\n- Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in Neural Information Processing Systems, 30, 2017.", "images": [], "items": [{"type": "text", "value": "- Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 65(2):742\u2013769, 2018.\n- Yudong Chen and Martin J. Wainwright. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees, 2015.\n- Jiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. On the computational and statistical complexity of over-parameterized matrix sensing. CoRR, abs/2102.02756, 2021.\n- Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank matrix factorization. Advances in Neural Information Processing Systems, 34:1429\u20131439, 2021.\n- Raghunandan Keshavan and Sewoong Oh. A gradient descent algorithm on the grassman manifold for matrix completion. 910, 10 2009.\n- Dohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the burer-monteiro approach. In Artificial Intelligence and Statistics, pages 65\u201374. PMLR, 2017.\n- Jian-Feng Cai, Emmanuel J. Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956\u20131982, 2010.\n- Prateek Jain, Raghu Meka, and Inderjit Dhillon. Guaranteed rank minimization via singular value projection. Advances in Neural Information Processing Systems, 23, 2010.\n- Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE transactions on information theory, 56(6):2980\u20132998, 2010.\n- Shiqian Ma, Donald Goldfarb, and Lifeng Chen. Fixed point and bregman iterative methods for matrix rank minimization. Mathematical Programming, 128(1-2):321\u2013353, 2011.\n- Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12(12), 2011.\n- Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471\u2013501, jan 2010.\n- Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems. Pacific Journal of Optimization, 6, 09 2010.\n- Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in neural information processing systems, 29, 2016.\n- Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. Advances in Neural Information Processing Systems, 29, 2016.\n- Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. CoRR, abs/1704.00708, 2017.\n- Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society Series B, 68:49\u201367, 02 2006.\n- Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018.\n- Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning, 2014.\n- Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\n- Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in Neural Information Processing Systems, 30, 2017.", "md": "- Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 65(2):742\u2013769, 2018.\n- Yudong Chen and Martin J. Wainwright. Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees, 2015.\n- Jiacheng Zhuo, Jeongyeol Kwon, Nhat Ho, and Constantine Caramanis. On the computational and statistical complexity of over-parameterized matrix sensing. CoRR, abs/2102.02756, 2021.\n- Tian Ye and Simon S Du. Global convergence of gradient descent for asymmetric low-rank matrix factorization. Advances in Neural Information Processing Systems, 34:1429\u20131439, 2021.\n- Raghunandan Keshavan and Sewoong Oh. A gradient descent algorithm on the grassman manifold for matrix completion. 910, 10 2009.\n- Dohyung Park, Anastasios Kyrillidis, Constantine Carmanis, and Sujay Sanghavi. Non-square matrix sensing without spurious local minima via the burer-monteiro approach. In Artificial Intelligence and Statistics, pages 65\u201374. PMLR, 2017.\n- Jian-Feng Cai, Emmanuel J. Cand\u00e8s, and Zuowei Shen. A singular value thresholding algorithm for matrix completion. SIAM Journal on Optimization, 20(4):1956\u20131982, 2010.\n- Prateek Jain, Raghu Meka, and Inderjit Dhillon. Guaranteed rank minimization via singular value projection. Advances in Neural Information Processing Systems, 23, 2010.\n- Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE transactions on information theory, 56(6):2980\u20132998, 2010.\n- Shiqian Ma, Donald Goldfarb, and Lifeng Chen. Fixed point and bregman iterative methods for matrix rank minimization. Mathematical Programming, 128(1-2):321\u2013353, 2011.\n- Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research, 12(12), 2011.\n- Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471\u2013501, jan 2010.\n- Kim-Chuan Toh and Sangwoon Yun. An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems. Pacific Journal of Optimization, 6, 09 2010.\n- Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. Advances in neural information processing systems, 29, 2016.\n- Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for low rank matrix recovery. Advances in Neural Information Processing Systems, 29, 2016.\n- Rong Ge, Chi Jin, and Yi Zheng. No spurious local minima in nonconvex low rank problems: A unified geometric analysis. CoRR, abs/1704.00708, 2017.\n- Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal of the Royal Statistical Society Series B, 68:49\u201367, 02 2006.\n- Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018.\n- Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning, 2014.\n- Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\n- Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. Advances in Neural Information Processing Systems, 30, 2017."}]}, {"page": 13, "text": "[42] Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical\n      and algorithmic theory for local optima. Advances in Neural Information Processing Systems,\n      26, 2013.\n[43] Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery\n      from a minimal number of noisy random measurements. IEEE Transactions on Information\n     Theory, 57(4):2342\u20132359, 2011.\n[44] Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted ma-\n      trix completion: Optimal bounds with noise. The Journal of Machine Learning Research,\n     13(1):1665\u20131697, 2012.\n[45] Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization\n      and optimal rates for noisy low-rank matrix completion. 2011.\n[46] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape\n      saddle points effi\n                       ciently. In International conference on machine learning, pages 1724\u20131732.\n      PMLR, 2017.\n[47] Aryan Mokhtari, Asuman Ozdaglar, and Ali Jadbabaie. Escaping saddle points in constrained\n      optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\n      R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran\n     Associates, Inc., 2018.\n[48] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape\n      saddle points effi\n                       ciently. In International conference on machine learning, pages 1724\u20131732.\n      PMLR, 2017.\n[49] Roman Vershynin. High-dimensional probability. 2019.\n[50] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection.\n     The Annals of Statistics, 28(5):1302 \u2013 1338, 2000.\n[51] Michel Petrovitch. Sur une mani\u00e8re d\u2019\u00e9tendre le th\u00e9or\u00e8me de la moyenne aux \u00e9quations\n      diff\u00e9rentielles du premier ordre. (mit 2 figuren im text). Mathematische Annalen, 54:417\u2013436,\n     1901.\n[52] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Compu-\n      tational Mathematics, 12(4):389\u2013434, aug 2011.\n[53] Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge\n      Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.\n[54] Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang. Stabilized svrg: Simple variance reduction\n      for nonconvex optimization. In Conference on learning theory, pages 1394\u20131448. PMLR, 2019.\n[55] Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in noncon-\n     vex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix\n      completion. In International Conference on Machine Learning, pages 3345\u20133354. PMLR,\n      2018.\n                                                 13", "md": "- **[42]** Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical\nand algorithmic theory for local optima. Advances in Neural Information Processing Systems,\n26, 2013.\n- **[43]** Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery\nfrom a minimal number of noisy random measurements. IEEE Transactions on Information\nTheory, 57(4):2342\u20132359, 2011.\n- **[44]** Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research,\n13(1):1665\u20131697, 2012.\n- **[45]** Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization\nand optimal rates for noisy low-rank matrix completion. 2011.\n- **[46]** Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International conference on machine learning, pages 1724\u20131732.\nPMLR, 2017.\n- **[47]** Aryan Mokhtari, Asuman Ozdaglar, and Ali Jadbabaie. Escaping saddle points in constrained optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran\nAssociates, Inc., 2018.\n- **[48]** Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International conference on machine learning, pages 1724\u20131732.\nPMLR, 2017.\n- **[49]** Roman Vershynin. High-dimensional probability. 2019.\n- **[50]** B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection.\nThe Annals of Statistics, 28(5):1302 \u2013 1338, 2000.\n- **[51]** Michel Petrovitch. Sur une mani\u00e8re d\u2019\u00e9tendre le th\u00e9or\u00e8me de la moyenne aux \u00e9quations\ndiff\u00e9rentielles du premier ordre. (mit 2 figuren im text). Mathematische Annalen, 54:417\u2013436,\n1901.\n- **[52]** Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, aug 2011.\n- **[53]** Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge\nSeries in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.\n- **[54]** Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang. Stabilized svrg: Simple variance reduction\nfor nonconvex optimization. In Conference on learning theory, pages 1394\u20131448. PMLR, 2019.\n- **[55]** Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix\ncompletion. In International Conference on Machine Learning, pages 3345\u20133354. PMLR,\n2018.", "images": [], "items": [{"type": "text", "value": "- **[42]** Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical\nand algorithmic theory for local optima. Advances in Neural Information Processing Systems,\n26, 2013.\n- **[43]** Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery\nfrom a minimal number of noisy random measurements. IEEE Transactions on Information\nTheory, 57(4):2342\u20132359, 2011.\n- **[44]** Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research,\n13(1):1665\u20131697, 2012.\n- **[45]** Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization\nand optimal rates for noisy low-rank matrix completion. 2011.\n- **[46]** Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International conference on machine learning, pages 1724\u20131732.\nPMLR, 2017.\n- **[47]** Aryan Mokhtari, Asuman Ozdaglar, and Ali Jadbabaie. Escaping saddle points in constrained optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran\nAssociates, Inc., 2018.\n- **[48]** Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International conference on machine learning, pages 1724\u20131732.\nPMLR, 2017.\n- **[49]** Roman Vershynin. High-dimensional probability. 2019.\n- **[50]** B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection.\nThe Annals of Statistics, 28(5):1302 \u2013 1338, 2000.\n- **[51]** Michel Petrovitch. Sur une mani\u00e8re d\u2019\u00e9tendre le th\u00e9or\u00e8me de la moyenne aux \u00e9quations\ndiff\u00e9rentielles du premier ordre. (mit 2 figuren im text). Mathematische Annalen, 54:417\u2013436,\n1901.\n- **[52]** Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, aug 2011.\n- **[53]** Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge\nSeries in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.\n- **[54]** Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang. Stabilized svrg: Simple variance reduction\nfor nonconvex optimization. In Conference on learning theory, pages 1394\u20131448. PMLR, 2019.\n- **[55]** Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix\ncompletion. In International Conference on Machine Learning, pages 3345\u20133354. PMLR,\n2018.", "md": "- **[42]** Po-Ling Loh and Martin J Wainwright. Regularized m-estimators with nonconvexity: Statistical\nand algorithmic theory for local optima. Advances in Neural Information Processing Systems,\n26, 2013.\n- **[43]** Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery\nfrom a minimal number of noisy random measurements. IEEE Transactions on Information\nTheory, 57(4):2342\u20132359, 2011.\n- **[44]** Sahand Negahban and Martin J Wainwright. Restricted strong convexity and weighted matrix completion: Optimal bounds with noise. The Journal of Machine Learning Research,\n13(1):1665\u20131697, 2012.\n- **[45]** Vladimir Koltchinskii, Karim Lounici, and Alexandre B Tsybakov. Nuclear-norm penalization\nand optimal rates for noisy low-rank matrix completion. 2011.\n- **[46]** Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International conference on machine learning, pages 1724\u20131732.\nPMLR, 2017.\n- **[47]** Aryan Mokhtari, Asuman Ozdaglar, and Ali Jadbabaie. Escaping saddle points in constrained optimization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran\nAssociates, Inc., 2018.\n- **[48]** Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efficiently. In International conference on machine learning, pages 1724\u20131732.\nPMLR, 2017.\n- **[49]** Roman Vershynin. High-dimensional probability. 2019.\n- **[50]** B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection.\nThe Annals of Statistics, 28(5):1302 \u2013 1338, 2000.\n- **[51]** Michel Petrovitch. Sur une mani\u00e8re d\u2019\u00e9tendre le th\u00e9or\u00e8me de la moyenne aux \u00e9quations\ndiff\u00e9rentielles du premier ordre. (mit 2 figuren im text). Mathematische Annalen, 54:417\u2013436,\n1901.\n- **[52]** Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389\u2013434, aug 2011.\n- **[53]** Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge\nSeries in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.\n- **[54]** Rong Ge, Zhize Li, Weiyao Wang, and Xiang Wang. Stabilized svrg: Simple variance reduction\nfor nonconvex optimization. In Conference on learning theory, pages 1394\u20131448. PMLR, 2019.\n- **[55]** Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval and matrix\ncompletion. In International Conference on Machine Learning, pages 3345\u20133354. PMLR,\n2018."}]}, {"page": 14, "text": "                                        Supplementary material\nA Failure of implicit regularization in pruning: Proof of Theorem 1                                         14\n     A.1    Proof outline of Lemma 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          15\n     A.2    Understanding gradient flow: Proof of Lemma 9 . . . . . . . . . . . . . . . . . . .             15\n     A.3    Behavior at initialization: many columns are \u201cactive\u201d . . . . . . . . . . . . . . . .           24\n     A.4    Putting it all together: Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . .         25\nB    Population analysis of the regularized loss: Proof of Theorem 3                                        25\n     B.1    Proof outline of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          26\n     B.2    Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          35\n     B.3    Missing proofs in Appendix B . . . . . . . . . . . . . . . . . . . . . . . . . . . .            36\n            B.3.1    Proof of Lemma 25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          36\n            B.3.2    Proof of Lemma 27 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          37\n            B.3.3    Proof of Lemma 32 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          38\nC Finite sample guarantees: Proof of Theorem 4                                                              39\nD Efficiently finding approximate second order stationary points                                            43\n     D.1    Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          43\n     D.2    Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          45\nE    Gradient and Hessian computations                                                                      46\n     E.1    Population mean square error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          46\n     E.2    Empirical mean square error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           46\n     E.3    Regularization R\u03b2       . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     47\nF    Implications for shallow neural networks with quadratic activation functions                           48\nA     Failure of implicit regularization in pruning: Proof of Theorem 1\nIn this section, we prove the negative result showing that running gradient descent trained with\nimplicit regularization results in dense networks that are not compatible with greedy pruning.\nFor scale reasons, and to simplify the exposition, we will assume that \u2225U\u22c6\u22252 = 1 through the\nremainder of this proof. The gradient flow update rule can be written as,\n                                   dU\n                                    dt = \u2212\u2207L(U) = \u2212(UU T \u2212              U\u22c6U T\u22c6 )U                          (19)\nwhere U = U(t) is the weight matrix at time t. Define the column vector r(t) = U T U\u22c6                    \u2208  Rk\ncapturing the correlation/alignment between the weights on the neurons of Ut with the ground truth\nU\u22c6, the signal component. Wherever convenient, we will drop the time argument in r(t) and simply\nrefer to it as r.\nWe will show that given that gradient flow is run starting from a small initialization, at convergence,\nthe column norms \u2225Uei\u22252\u2019s approximately distribute themselves proportional to the alignment of the\n                                                         14", "md": "# Supplementary Material\n\n## A Failure of Implicit Regularization in Pruning: Proof of Theorem 1\n\nA.1 Proof outline of Lemma 9 .......................................... 15\n\nA.2 Understanding gradient flow: Proof of Lemma 9 .................. 15\n\nA.3 Behavior at initialization: many columns are \"active\" ........... 24\n\nA.4 Putting it all together: Proof of Theorem 1 ...................... 25\n\n## Population Analysis of the Regularized Loss: Proof of Theorem 3\n\nB.1 Proof outline of Theorem 3 ....................................... 26\n\nB.2 Proof of Theorem 3 ............................................... 35\n\nB.3 Missing proofs in Appendix B ..................................... 36\n\nB.3.1 Proof of Lemma 25 ............................................. 36\n\nB.3.2 Proof of Lemma 27 ............................................. 37\n\nB.3.3 Proof of Lemma 32 ............................................. 38\n\n## Finite Sample Guarantees: Proof of Theorem 4\n\n## Efficiently Finding Approximate Second Order Stationary Points\n\nD.1 Proof of Theorem 7 ............................................... 43\n\nD.2 Proof of Theorem 8 ............................................... 45\n\n## Gradient and Hessian Computations\n\nE.1 Population Mean Square Error ..................................... 46\n\nE.2 Empirical Mean Square Error ...................................... 46\n\nE.3 Regularization $$R_{\\beta}$$ .......................................... 47\n\n## Implications for Shallow Neural Networks with Quadratic Activation Functions\n\nIn this section, we prove the negative result showing that running gradient descent trained with implicit regularization results in dense networks that are not compatible with greedy pruning. For scale reasons, and to simplify the exposition, we will assume that $$\\|U^*\\|_2 = 1$$ through the remainder of this proof. The gradient flow update rule can be written as,\n\n$$\\frac{dU}{dt} = -\\nabla L(U) = -(UU^T - U^*U^{T*})U$$\nwhere U = U(t) is the weight matrix at time t. Define the column vector $$r(t) = U^TU^* \\in \\mathbb{R}^k$$ capturing the correlation/alignment between the weights on the neurons of Ut with the ground truth U*, the signal component. Wherever convenient, we will drop the time argument in r(t) and simply refer to it as r.\n\nWe will show that given that gradient flow is run starting from a small initialization, at convergence, the column norms $$\\|U_{ei}\\|_2$$ approximately distribute themselves proportional to the alignment of the", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Supplementary Material", "md": "# Supplementary Material"}, {"type": "heading", "lvl": 2, "value": "A Failure of Implicit Regularization in Pruning: Proof of Theorem 1", "md": "## A Failure of Implicit Regularization in Pruning: Proof of Theorem 1"}, {"type": "text", "value": "A.1 Proof outline of Lemma 9 .......................................... 15\n\nA.2 Understanding gradient flow: Proof of Lemma 9 .................. 15\n\nA.3 Behavior at initialization: many columns are \"active\" ........... 24\n\nA.4 Putting it all together: Proof of Theorem 1 ...................... 25", "md": "A.1 Proof outline of Lemma 9 .......................................... 15\n\nA.2 Understanding gradient flow: Proof of Lemma 9 .................. 15\n\nA.3 Behavior at initialization: many columns are \"active\" ........... 24\n\nA.4 Putting it all together: Proof of Theorem 1 ...................... 25"}, {"type": "heading", "lvl": 2, "value": "Population Analysis of the Regularized Loss: Proof of Theorem 3", "md": "## Population Analysis of the Regularized Loss: Proof of Theorem 3"}, {"type": "text", "value": "B.1 Proof outline of Theorem 3 ....................................... 26\n\nB.2 Proof of Theorem 3 ............................................... 35\n\nB.3 Missing proofs in Appendix B ..................................... 36\n\nB.3.1 Proof of Lemma 25 ............................................. 36\n\nB.3.2 Proof of Lemma 27 ............................................. 37\n\nB.3.3 Proof of Lemma 32 ............................................. 38", "md": "B.1 Proof outline of Theorem 3 ....................................... 26\n\nB.2 Proof of Theorem 3 ............................................... 35\n\nB.3 Missing proofs in Appendix B ..................................... 36\n\nB.3.1 Proof of Lemma 25 ............................................. 36\n\nB.3.2 Proof of Lemma 27 ............................................. 37\n\nB.3.3 Proof of Lemma 32 ............................................. 38"}, {"type": "heading", "lvl": 2, "value": "Finite Sample Guarantees: Proof of Theorem 4", "md": "## Finite Sample Guarantees: Proof of Theorem 4"}, {"type": "heading", "lvl": 2, "value": "Efficiently Finding Approximate Second Order Stationary Points", "md": "## Efficiently Finding Approximate Second Order Stationary Points"}, {"type": "text", "value": "D.1 Proof of Theorem 7 ............................................... 43\n\nD.2 Proof of Theorem 8 ............................................... 45", "md": "D.1 Proof of Theorem 7 ............................................... 43\n\nD.2 Proof of Theorem 8 ............................................... 45"}, {"type": "heading", "lvl": 2, "value": "Gradient and Hessian Computations", "md": "## Gradient and Hessian Computations"}, {"type": "text", "value": "E.1 Population Mean Square Error ..................................... 46\n\nE.2 Empirical Mean Square Error ...................................... 46\n\nE.3 Regularization $$R_{\\beta}$$ .......................................... 47", "md": "E.1 Population Mean Square Error ..................................... 46\n\nE.2 Empirical Mean Square Error ...................................... 46\n\nE.3 Regularization $$R_{\\beta}$$ .......................................... 47"}, {"type": "heading", "lvl": 2, "value": "Implications for Shallow Neural Networks with Quadratic Activation Functions", "md": "## Implications for Shallow Neural Networks with Quadratic Activation Functions"}, {"type": "text", "value": "In this section, we prove the negative result showing that running gradient descent trained with implicit regularization results in dense networks that are not compatible with greedy pruning. For scale reasons, and to simplify the exposition, we will assume that $$\\|U^*\\|_2 = 1$$ through the remainder of this proof. The gradient flow update rule can be written as,\n\n$$\\frac{dU}{dt} = -\\nabla L(U) = -(UU^T - U^*U^{T*})U$$\nwhere U = U(t) is the weight matrix at time t. Define the column vector $$r(t) = U^TU^* \\in \\mathbb{R}^k$$ capturing the correlation/alignment between the weights on the neurons of Ut with the ground truth U*, the signal component. Wherever convenient, we will drop the time argument in r(t) and simply refer to it as r.\n\nWe will show that given that gradient flow is run starting from a small initialization, at convergence, the column norms $$\\|U_{ei}\\|_2$$ approximately distribute themselves proportional to the alignment of the", "md": "In this section, we prove the negative result showing that running gradient descent trained with implicit regularization results in dense networks that are not compatible with greedy pruning. For scale reasons, and to simplify the exposition, we will assume that $$\\|U^*\\|_2 = 1$$ through the remainder of this proof. The gradient flow update rule can be written as,\n\n$$\\frac{dU}{dt} = -\\nabla L(U) = -(UU^T - U^*U^{T*})U$$\nwhere U = U(t) is the weight matrix at time t. Define the column vector $$r(t) = U^TU^* \\in \\mathbb{R}^k$$ capturing the correlation/alignment between the weights on the neurons of Ut with the ground truth U*, the signal component. Wherever convenient, we will drop the time argument in r(t) and simply refer to it as r.\n\nWe will show that given that gradient flow is run starting from a small initialization, at convergence, the column norms $$\\|U_{ei}\\|_2$$ approximately distribute themselves proportional to the alignment of the"}]}, {"page": 15, "text": "corresponding column of U with U\u22c6         initially at t = 0. Namely,\n                                        lim                                                                 (20)\n                                       t\u2192\u221e\u2225U(t)ei\u22252 \u221d        |\u27e8r(0), ei\u27e9|\nGiven a random initialization where the entries of U(0) are i.i.d., |\u27e8r(0), ei\u27e9| for each i are\nindependent. Moreover, when U(0) follows a Gaussian distribution, we expect no single coordinate\n|\u27e8r(0), ei\u27e9| = |\u27e8(U(0))T U\u22c6, ei\u27e9| to be much larger than the others, a statement which we formally\nprove in Lemma 20. Combining this with eq. (20) results in a proof of Theorem 1.\nThe rest of this section is devoted to proving a formal version of eq. (20), which we state below.\nLemma 9. Suppose the initialization scale parameter \u03b1 \u2264             c\u03b72/k3d log(kd) for a sufficiently small\nabsolute constant c > 0. With probability \u2265       1\u2212O( 1  k) over the initialization, for each column i \u2208    [k],\n                    (1 \u2212  5\u03b7)|\u27e8r(0), ei\u27e9|  \u2264   lim                               (1 + 4\u221a\u03b7)                  (21)\n                                              t\u2192\u221e\u2225U(t)ei\u22252 \u2264        |\u27e8r(0), ei\u27e9|\n                                \u2225r(0)\u22252                               \u2225r(0)\u22252\nAs a counterpart to the signal component r(t), the noise component is E(t) = (I \u2212                U\u22c6U T\u22c6  )U. At\na high level, the proof of Lemma 9 will be to establish that with a small initialization, the noise\ncomponent satisfies E \u2248      0 approximately, while the signal component \u2225r(t)\u22252 grows exponentially\nfast to 1 with t, and d\u27e8r(t),ei\u27e9  \u2248  \u27e8r(t), ei\u27e9 until \u2225r(t)\u22252 gets sufficiently close to 1. This will imply\n                           dt\nthat \u27e8r(t), ei\u27e9 \u2248  \u27e8r(0), ei\u27e9et until then, and by extension,\n                                           \u27e8r(t), ei\u27e9                                                       (22)\n                                           \u27e8r(t), ej\u27e9  \u2248  \u27e8r(0), ei\u27e9\n                                                          \u27e8r(0), ej\u27e9.\nSince the noise component E \u2248        0, \u2225Uei\u22252 \u2248    |\u27e8r(t), ei\u27e9|, plugging which into eq. (22) results in the\nratio form of Lemma 9. By the gradient flow equation eq. (19), it is a short calculation to see that r\nand E evolve per the following differential equations,\n          dr                                                           dE\n           dt = r(1 \u2212    \u2225r\u222522) \u2212  ET Er.       (23)                   dt = \u2212EU T U.                  (24)\nIn the sequel, we will show that as a function of t, ET Er decays linearly. The resulting differential\nequation drdt \u2248  r(1 \u2212   \u2225r\u222522) shows linear convergence of r until the point when \u2225r\u22252 approaches 1.\nWe place these intuitions formally by discussing a formal proof of Lemma 9.\nA.1    Proof outline of Lemma 9\nFirstly, as a function of the scale parameter \u03b1, we bound the energy of the signal and noise\ncomponents at initialization in Lemma 10. In Lemma 11 we first establish that \u2225E(t)\u22252 does not\nincrease with time. In Lemma 13, we establish an upper bound on the signal norm \u2225r(t)\u22252 as\na function of time. This shows that the signal energy cannot grow to be 1 too rapidly, which is\nnecessary to show that the error term ET Er has sufficient time to decay to 0. In Lemma 14 we show\nthat the signal norm \u2225r(t)\u22252   2 does not fall below a threshold of 3/4 after t grows to be suffi        ciently\nlarge. This is essential in proving Lemma 15 which shows that the norm of the error term \u2225ET Er\u22252\nin eq. (23) begins decaying after \u2225r(t)\u22252     2 becomes larger than 1/2. Finally, in Lemmas 16 and 17\nwe use these results to prove a refined bound on the rate at which \u2225r(t)\u22252 \u2192              1. These results are\ncollectively used in Lemmas 18 and 19 to prove the upper and lower bounds on |\u27e8r(t), ei\u27e9| \u2248            \u2225Uei\u22252.\nA.2    Understanding gradient flow: Proof of Lemma 9\nIn this section we prove Lemma 9 formally. First we establish bounds on the scale of parameters at\ninitialization.\n                                                       15", "md": "corresponding column of U with U\u22c6 initially at t = 0. Namely,\n\n$$\n\\lim_{t\\to\\infty}\\|U(t)ei\\|^2 \\propto |\u27e8r(0), ei\u27e9| \\tag{20}\n$$\n\nGiven a random initialization where the entries of U(0) are i.i.d., |\u27e8r(0), ei\u27e9| for each i are independent. Moreover, when U(0) follows a Gaussian distribution, we expect no single coordinate |\u27e8r(0), ei\u27e9| = |\u27e8(U(0))^T U\u22c6, ei\u27e9| to be much larger than the others, a statement which we formally prove in Lemma 20. Combining this with eq. (20) results in a proof of Theorem 1.\n\nThe rest of this section is devoted to proving a formal version of eq. (20), which we state below.\n\nLemma 9. Suppose the initialization scale parameter \u03b1 \u2264 c\u03b7^2/k^3d log(kd) for a sufficiently small absolute constant c > 0. With probability \u2265 1\u2212O(1/k) over the initialization, for each column i \u2208 [k],\n\n$$\n(1 - 5\u03b7)|\u27e8r(0), ei\u27e9| \\leq \\lim_{t\\to\\infty}\\frac{\\|U(t)ei\\|^2}{\\|r(0)\\|^2} \\leq |\u27e8r(0), ei\u27e9| \\tag{21}\n$$\n\nAs a counterpart to the signal component r(t), the noise component is E(t) = (I - U\u22c6U^T)U. At a high level, the proof of Lemma 9 will be to establish that with a small initialization, the noise component satisfies E \u2248 0 approximately, while the signal component \\|r(t)\\|^2 grows exponentially fast to 1 with t, and d\u27e8r(t),ei\u27e9/dt \u2248 \u27e8r(t), ei\u27e9 until \\|r(t)\\|^2 gets sufficiently close to 1. This will imply that \u27e8r(t), ei\u27e9 \u2248 \u27e8r(0), ei\u27e9et until then, and by extension,\n\n$$\n\\frac{\u27e8r(t), ei\u27e9}{\u27e8r(t), ej\u27e9} \\approx \\frac{\u27e8r(0), ei\u27e9}{\u27e8r(0), ej\u27e9} \\tag{22}\n$$\n\nSince the noise component E \u2248 0, \\|Uei\\|^2 \u2248 |\u27e8r(t), ei\u27e9|, plugging which into eq. (22) results in the ratio form of Lemma 9. By the gradient flow equation eq. (19), it is a short calculation to see that r and E evolve per the following differential equations,\n\n$$\n\\frac{dr}{dt} = r(1 - \\|r\\|^2) - E^T E r. \\tag{23}\n$$\n$$\n\\frac{dE}{dt} = -EU^T U. \\tag{24}\n$$\n\nIn the sequel, we will show that as a function of t, E^T E r decays linearly. The resulting differential equation dr/dt \u2248 r(1 - \\|r\\|^2) shows linear convergence of r until the point when \\|r\\|^2 approaches 1. We place these intuitions formally by discussing a formal proof of Lemma 9.\n\n### A.1 Proof outline of Lemma 9\n\nFirstly, as a function of the scale parameter \u03b1, we bound the energy of the signal and noise components at initialization in Lemma 10. In Lemma 11 we first establish that \\|E(t)\\|^2 does not increase with time. In Lemma 13, we establish an upper bound on the signal norm \\|r(t)\\|^2 as a function of time. This shows that the signal energy cannot grow to be 1 too rapidly, which is necessary to show that the error term E^T E r has sufficient time to decay to 0. In Lemma 14 we show that the signal norm \\|r(t)\\|^2 does not fall below a threshold of 3/4 after t grows to be sufficiently large. This is essential in proving Lemma 15 which shows that the norm of the error term \\|E^T E r\\|^2 in eq. (23) begins decaying after \\|r(t)\\|^2 becomes larger than 1/2. Finally, in Lemmas 16 and 17 we use these results to prove a refined bound on the rate at which \\|r(t)\\|^2 \u2192 1. These results are collectively used in Lemmas 18 and 19 to prove the upper and lower bounds on |\u27e8r(t), ei\u27e9| \u2248 \\|Uei\\|^2.\n\n### A.2 Understanding gradient flow: Proof of Lemma 9\n\nIn this section we prove Lemma 9 formally. First we establish bounds on the scale of parameters at initialization.", "images": [], "items": [{"type": "text", "value": "corresponding column of U with U\u22c6 initially at t = 0. Namely,\n\n$$\n\\lim_{t\\to\\infty}\\|U(t)ei\\|^2 \\propto |\u27e8r(0), ei\u27e9| \\tag{20}\n$$\n\nGiven a random initialization where the entries of U(0) are i.i.d., |\u27e8r(0), ei\u27e9| for each i are independent. Moreover, when U(0) follows a Gaussian distribution, we expect no single coordinate |\u27e8r(0), ei\u27e9| = |\u27e8(U(0))^T U\u22c6, ei\u27e9| to be much larger than the others, a statement which we formally prove in Lemma 20. Combining this with eq. (20) results in a proof of Theorem 1.\n\nThe rest of this section is devoted to proving a formal version of eq. (20), which we state below.\n\nLemma 9. Suppose the initialization scale parameter \u03b1 \u2264 c\u03b7^2/k^3d log(kd) for a sufficiently small absolute constant c > 0. With probability \u2265 1\u2212O(1/k) over the initialization, for each column i \u2208 [k],\n\n$$\n(1 - 5\u03b7)|\u27e8r(0), ei\u27e9| \\leq \\lim_{t\\to\\infty}\\frac{\\|U(t)ei\\|^2}{\\|r(0)\\|^2} \\leq |\u27e8r(0), ei\u27e9| \\tag{21}\n$$\n\nAs a counterpart to the signal component r(t), the noise component is E(t) = (I - U\u22c6U^T)U. At a high level, the proof of Lemma 9 will be to establish that with a small initialization, the noise component satisfies E \u2248 0 approximately, while the signal component \\|r(t)\\|^2 grows exponentially fast to 1 with t, and d\u27e8r(t),ei\u27e9/dt \u2248 \u27e8r(t), ei\u27e9 until \\|r(t)\\|^2 gets sufficiently close to 1. This will imply that \u27e8r(t), ei\u27e9 \u2248 \u27e8r(0), ei\u27e9et until then, and by extension,\n\n$$\n\\frac{\u27e8r(t), ei\u27e9}{\u27e8r(t), ej\u27e9} \\approx \\frac{\u27e8r(0), ei\u27e9}{\u27e8r(0), ej\u27e9} \\tag{22}\n$$\n\nSince the noise component E \u2248 0, \\|Uei\\|^2 \u2248 |\u27e8r(t), ei\u27e9|, plugging which into eq. (22) results in the ratio form of Lemma 9. By the gradient flow equation eq. (19), it is a short calculation to see that r and E evolve per the following differential equations,\n\n$$\n\\frac{dr}{dt} = r(1 - \\|r\\|^2) - E^T E r. \\tag{23}\n$$\n$$\n\\frac{dE}{dt} = -EU^T U. \\tag{24}\n$$\n\nIn the sequel, we will show that as a function of t, E^T E r decays linearly. The resulting differential equation dr/dt \u2248 r(1 - \\|r\\|^2) shows linear convergence of r until the point when \\|r\\|^2 approaches 1. We place these intuitions formally by discussing a formal proof of Lemma 9.", "md": "corresponding column of U with U\u22c6 initially at t = 0. Namely,\n\n$$\n\\lim_{t\\to\\infty}\\|U(t)ei\\|^2 \\propto |\u27e8r(0), ei\u27e9| \\tag{20}\n$$\n\nGiven a random initialization where the entries of U(0) are i.i.d., |\u27e8r(0), ei\u27e9| for each i are independent. Moreover, when U(0) follows a Gaussian distribution, we expect no single coordinate |\u27e8r(0), ei\u27e9| = |\u27e8(U(0))^T U\u22c6, ei\u27e9| to be much larger than the others, a statement which we formally prove in Lemma 20. Combining this with eq. (20) results in a proof of Theorem 1.\n\nThe rest of this section is devoted to proving a formal version of eq. (20), which we state below.\n\nLemma 9. Suppose the initialization scale parameter \u03b1 \u2264 c\u03b7^2/k^3d log(kd) for a sufficiently small absolute constant c > 0. With probability \u2265 1\u2212O(1/k) over the initialization, for each column i \u2208 [k],\n\n$$\n(1 - 5\u03b7)|\u27e8r(0), ei\u27e9| \\leq \\lim_{t\\to\\infty}\\frac{\\|U(t)ei\\|^2}{\\|r(0)\\|^2} \\leq |\u27e8r(0), ei\u27e9| \\tag{21}\n$$\n\nAs a counterpart to the signal component r(t), the noise component is E(t) = (I - U\u22c6U^T)U. At a high level, the proof of Lemma 9 will be to establish that with a small initialization, the noise component satisfies E \u2248 0 approximately, while the signal component \\|r(t)\\|^2 grows exponentially fast to 1 with t, and d\u27e8r(t),ei\u27e9/dt \u2248 \u27e8r(t), ei\u27e9 until \\|r(t)\\|^2 gets sufficiently close to 1. This will imply that \u27e8r(t), ei\u27e9 \u2248 \u27e8r(0), ei\u27e9et until then, and by extension,\n\n$$\n\\frac{\u27e8r(t), ei\u27e9}{\u27e8r(t), ej\u27e9} \\approx \\frac{\u27e8r(0), ei\u27e9}{\u27e8r(0), ej\u27e9} \\tag{22}\n$$\n\nSince the noise component E \u2248 0, \\|Uei\\|^2 \u2248 |\u27e8r(t), ei\u27e9|, plugging which into eq. (22) results in the ratio form of Lemma 9. By the gradient flow equation eq. (19), it is a short calculation to see that r and E evolve per the following differential equations,\n\n$$\n\\frac{dr}{dt} = r(1 - \\|r\\|^2) - E^T E r. \\tag{23}\n$$\n$$\n\\frac{dE}{dt} = -EU^T U. \\tag{24}\n$$\n\nIn the sequel, we will show that as a function of t, E^T E r decays linearly. The resulting differential equation dr/dt \u2248 r(1 - \\|r\\|^2) shows linear convergence of r until the point when \\|r\\|^2 approaches 1. We place these intuitions formally by discussing a formal proof of Lemma 9."}, {"type": "heading", "lvl": 3, "value": "A.1 Proof outline of Lemma 9", "md": "### A.1 Proof outline of Lemma 9"}, {"type": "text", "value": "Firstly, as a function of the scale parameter \u03b1, we bound the energy of the signal and noise components at initialization in Lemma 10. In Lemma 11 we first establish that \\|E(t)\\|^2 does not increase with time. In Lemma 13, we establish an upper bound on the signal norm \\|r(t)\\|^2 as a function of time. This shows that the signal energy cannot grow to be 1 too rapidly, which is necessary to show that the error term E^T E r has sufficient time to decay to 0. In Lemma 14 we show that the signal norm \\|r(t)\\|^2 does not fall below a threshold of 3/4 after t grows to be sufficiently large. This is essential in proving Lemma 15 which shows that the norm of the error term \\|E^T E r\\|^2 in eq. (23) begins decaying after \\|r(t)\\|^2 becomes larger than 1/2. Finally, in Lemmas 16 and 17 we use these results to prove a refined bound on the rate at which \\|r(t)\\|^2 \u2192 1. These results are collectively used in Lemmas 18 and 19 to prove the upper and lower bounds on |\u27e8r(t), ei\u27e9| \u2248 \\|Uei\\|^2.", "md": "Firstly, as a function of the scale parameter \u03b1, we bound the energy of the signal and noise components at initialization in Lemma 10. In Lemma 11 we first establish that \\|E(t)\\|^2 does not increase with time. In Lemma 13, we establish an upper bound on the signal norm \\|r(t)\\|^2 as a function of time. This shows that the signal energy cannot grow to be 1 too rapidly, which is necessary to show that the error term E^T E r has sufficient time to decay to 0. In Lemma 14 we show that the signal norm \\|r(t)\\|^2 does not fall below a threshold of 3/4 after t grows to be sufficiently large. This is essential in proving Lemma 15 which shows that the norm of the error term \\|E^T E r\\|^2 in eq. (23) begins decaying after \\|r(t)\\|^2 becomes larger than 1/2. Finally, in Lemmas 16 and 17 we use these results to prove a refined bound on the rate at which \\|r(t)\\|^2 \u2192 1. These results are collectively used in Lemmas 18 and 19 to prove the upper and lower bounds on |\u27e8r(t), ei\u27e9| \u2248 \\|Uei\\|^2."}, {"type": "heading", "lvl": 3, "value": "A.2 Understanding gradient flow: Proof of Lemma 9", "md": "### A.2 Understanding gradient flow: Proof of Lemma 9"}, {"type": "text", "value": "In this section we prove Lemma 9 formally. First we establish bounds on the scale of parameters at initialization.", "md": "In this section we prove Lemma 9 formally. First we establish bounds on the scale of parameters at initialization."}]}, {"page": 16, "text": "Lemma 10. With probability \u2265                   1 \u2212   O( 1 k), at initialization,\n                                                   \u2225E(0)\u22252    F , \u2225r(0)\u22252   2 \u2264   2\u03b12kd                                                  (25)\n                                                   \u2225r(0)\u22252            k\u03b12                                                                (26)\n                                                             2 \u2265   \u00b7    10 , 10k\u03b12\n                                                   \u2200i \u2208   [k], |\u27e8r(0), ei\u27e9| \u2265         \u03b1                                                  (27)\n                                                                                     k2\nProof. Recall that \u27e8r, ei\u27e9          for each i \u2208       [k] is distributed \u223c        N  (0, \u03b12). By Gaussian anti-concentration\n[49] and union bounding, P                 mini\u2208[k] |\u27e8r(0), ei\u27e9| \u2265            \u03b1     \u2273    1\n                                                                             k2          k simultaneously for all i \u2208               [k]. On\nthe other hand, since every entry of U(0) is iid distributed \u223c                                N   (0, \u03b12), by tail bounds for \u03c72-\ndistributed random variables [50, Lemma 1], \u2225E(0)\u22252                             F \u2264      \u2225U(0)\u22252    F \u2264     2\u03b12kd with probability\n\u2265  1 \u2212    exp(\u2212kd) \u2265         1 \u2212    O( 1k). The same bound applies for \u2225r(0)\u22252                   2 since it is also upper bounded\nby \u2225U(0)\u22252      F . Finally, the upper and lower bounds on \u2225r(0)\u22252 follows by noting that \u2225r(0)\u22252                                        2 =\n k i=1\u27e8U\u22c6, Uei\u27e92, which concentrates around k\u03b12. The result directly follows by concentration of\n\u03c72  -random variables.\nAs a corollary of Lemma 10 when the intialization parameter satisfies the upper bound in Lemma 9,\n\u2225E(0)\u2225F and \u2225r(0)\u22252 are upper bounded by small constants. We will use this fact several times in\nproving Lemma 9. Next we establish that the error \u2225E\u2225F does not grow with time.\nLemma 11. At any time t \u2265                 0, \u2225E(t)\u2225F \u2264          \u2225E(0)\u2225F .\nProof. The proof follows by showing that the time derivative of \u2225E(t)\u22252                                F is non-positive. Indeed,\n                       d\u2225E\u22252    F   = 2      E, dE        = \u22122Tr(ET EU T U) = \u22122\u2225UET \u22252                       F \u2264    0.                  (28)\n                           dt                     dt\nStarting from a small initialization, this means that the error matrix E remains small over the course\nof gradient flow. In for any coordinate i \u2208                       [k], by eq. (23), the differential equation governing\n\u27e8r, ei\u27e9  = \u27e8r(t), ei\u27e9      is,             \u27e8r, ei\u27e9   = \u27e8r, ei\u27e9(1 \u2212       \u2225r\u22252  2) \u2212   eTi ET Er.                                         (29)\n                                              dt\nBy Lemma 11 we expect the error term ET Er to be small (not necessarily decaying) in comparison\nwith the first term as long as \u2225r(t)\u22252 is smaller than an absolute constant. In this regime, the\ndifferential form is easy to control since we expect \u27e8r, ei\u27e9                         to grow linearly and across the different\ncoordinates i \u2208        [k], we expect its value to be proportional to that at initialization, \u27e8r(0), ei\u27e9.\nHowever, when \u2225r\u22252 eventually approaches 1, the relative contribution of the signal term \u27e8r, ei\u27e9(1 \u2212\n\u2225r\u22252 2) and the error term ET Er become important. This is the main technical challenge in proving\nLemma 9. We will show that even as t \u2192                        \u221e, the error term cannot change \u27e8r, ei\u27e9                    by more than a\nconstant factor. For a sufficiently large constant C > 0, define,\n                                                   T0 \u225c    \u22121    log(\u2225r(0)\u22252     2) \u2212    1 .                                             (30)\n                                                               2    1 \u2212   \u2225E(0)\u22252    F\nAs we will later show, T0 controls the amount of time it takes the signal norm \u2225r(t)\u22252 to grow to\nan absolute constant. Firstly, we show that when the initialization scale \u03b1 is sufficiently small, T0 is\napproximately \u2212          log \u2225r(0)\u22252.\nLemma 12. When the initialization scale \u03b1 \u2264                              c/k3d log(kd) for a sufficiently small absolute\nconstant c > 0,\n                                   \u2212  log(\u2225r(0)\u22252) + 1        2 \u2264    T0 \u2264     \u2212  log(\u2225r(0)\u22252) + 3        2.                              (31)\n                                                                      16", "md": "# Math Equations in HTML\n\n## Lemma 10\n\nWith probability $$\\geq 1 - O\\left(\\frac{1}{k}\\right)$$, at initialization,\n\n$$\\|E(0)\\|_{2}^{2} \\leq \\alpha^{2}kd \\quad (25)$$\n\n$$\\|r(0)\\|_{2}^{2} \\leq k\\alpha^{2} \\quad (26)$$\n\n$$\\forall i \\in [k], \\ | \\langle r(0), e_{i} \\rangle | \\geq \\frac{\\alpha}{k^{2}} \\quad (27)$$\n\n### Proof:\n\nRecall that $$\\langle r, e_{i} \\rangle$$ for each $$i \\in [k]$$ is distributed $$\\sim N(0, \\alpha^{2})$$. By Gaussian anti-concentration [49] and union bounding, $$P\\left(\\min_{i\\in[k]} |\\langle r(0), e_{i} \\rangle| \\geq \\alpha\\right) \\gtrsim 1$$ simultaneously for all $$i \\in [k]$$. On the other hand, since every entry of $$U(0)$$ is iid distributed $$\\sim N(0, \\alpha^{2})$$, by tail bounds for $$\\chi^{2}$$-distributed random variables [50, Lemma 1], $$\\|E(0)\\|_{F} \\leq \\|U(0)\\|_{F} \\leq 2\\alpha^{2}kd$$ with probability $$\\geq 1 - \\exp(-kd) \\geq 1 - O\\left(\\frac{1}{k}\\right)$$. The same bound applies for $$\\|r(0)\\|_{2}^{2}$$ since it is also upper bounded by $$\\|U(0)\\|_{F}$$. Finally, the upper and lower bounds on $$\\|r(0)\\|_{2}^{2}$$ follows by noting that $$\\|r(0)\\|_{2}^{2} = \\sum_{i=1}^{k} \\langle U^{*}, Ue_{i} \\rangle^{2}$$, which concentrates around $$k\\alpha^{2}$$. The result directly follows by concentration of $$\\chi^{2}$$-random variables.\n\n### Corollary of Lemma 10\n\nAs a corollary of Lemma 10 when the initialization parameter satisfies the upper bound in Lemma 9, $$\\|E(0)\\|_{F}$$ and $$\\|r(0)\\|_{2}$$ are upper bounded by small constants. We will use this fact several times in proving Lemma 9.\n\n## Lemma 11\n\nAt any time $$t \\geq 0$$, $$\\|E(t)\\|_{F} \\leq \\|E(0)\\|_{F}$$.\n\n### Proof:\n\nThe proof follows by showing that the time derivative of $$\\|E\\|_{2}^{2}$$ is non-positive. Indeed,\n\n$$\\frac{d\\|E\\|_{2}^{2}}{dt} = 2E \\cdot \\frac{dE}{dt} = -2\\text{Tr}(E^{T}EU^{T}U) = -2\\|UE^{T}\\|_{F}^{2} \\leq 0 \\quad (28)$$\n\nStarting from a small initialization, this means that the error matrix $$E$$ remains small over the course of gradient flow. For any coordinate $$i \\in [k]$$, by eq. (23), the differential equation governing $$\\langle r, e_{i} \\rangle = \\langle r(t), e_{i} \\rangle$$ is,\n\n$$\\frac{d}{dt} \\langle r, e_{i} \\rangle = \\langle r, e_{i} \\rangle(1 - \\|r\\|_{2}^{2}) - e_{i}^{T}E^{T}Er \\quad (29)$$\n\nBy Lemma 11 we expect the error term $$E^{T}Er$$ to be small (not necessarily decaying) in comparison with the first term as long as $$\\|r(t)\\|_{2}$$ is smaller than an absolute constant. In this regime, the differential form is easy to control since we expect $$\\langle r, e_{i} \\rangle$$ to grow linearly and across the different coordinates $$i \\in [k]$$, we expect its value to be proportional to that at initialization, $$\\langle r(0), e_{i} \\rangle$$. However, when $$\\|r\\|_{2}$$ eventually approaches 1, the relative contribution of the signal term $$\\langle r, e_{i} \\rangle(1 - \\|r\\|_{2}^{2})$$ and the error term $$E^{T}Er$$ become important. This is the main technical challenge in proving Lemma 9. We will show that even as $$t \\to \\infty$$, the error term cannot change $$\\langle r, e_{i} \\rangle$$ by more than a constant factor. For a sufficiently large constant $$C > 0$$, define,\n\n$$T_{0} \\triangleq -\\frac{1}{2} \\log\\left(\\frac{\\|r(0)\\|_{2}^{2}}{1 - \\|E(0)\\|_{F}}\\right) - \\frac{1}{2} \\quad (30)$$\n\nAs we will later show, $$T_{0}$$ controls the amount of time it takes the signal norm $$\\|r(t)\\|_{2}$$ to grow to an absolute constant. Firstly, we show that when the initialization scale $$\\alpha$$ is sufficiently small, $$T_{0}$$ is approximately $$-\\log \\|r(0)\\|_{2}$$.\n\n## Lemma 12\n\nWhen the initialization scale $$\\alpha \\leq \\frac{c}{k^{3}d} \\log(kd)$$ for a sufficiently small absolute constant $$c > 0$$,\n\n$$-\\log(\\|r(0)\\|_{2}) + \\frac{1}{2} \\leq T_{0} \\leq -\\log(\\|r(0)\\|_{2}) + \\frac{3}{2} \\quad (31)$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations in HTML", "md": "# Math Equations in HTML"}, {"type": "heading", "lvl": 2, "value": "Lemma 10", "md": "## Lemma 10"}, {"type": "text", "value": "With probability $$\\geq 1 - O\\left(\\frac{1}{k}\\right)$$, at initialization,\n\n$$\\|E(0)\\|_{2}^{2} \\leq \\alpha^{2}kd \\quad (25)$$\n\n$$\\|r(0)\\|_{2}^{2} \\leq k\\alpha^{2} \\quad (26)$$\n\n$$\\forall i \\in [k], \\ | \\langle r(0), e_{i} \\rangle | \\geq \\frac{\\alpha}{k^{2}} \\quad (27)$$", "md": "With probability $$\\geq 1 - O\\left(\\frac{1}{k}\\right)$$, at initialization,\n\n$$\\|E(0)\\|_{2}^{2} \\leq \\alpha^{2}kd \\quad (25)$$\n\n$$\\|r(0)\\|_{2}^{2} \\leq k\\alpha^{2} \\quad (26)$$\n\n$$\\forall i \\in [k], \\ | \\langle r(0), e_{i} \\rangle | \\geq \\frac{\\alpha}{k^{2}} \\quad (27)$$"}, {"type": "heading", "lvl": 3, "value": "Proof:", "md": "### Proof:"}, {"type": "text", "value": "Recall that $$\\langle r, e_{i} \\rangle$$ for each $$i \\in [k]$$ is distributed $$\\sim N(0, \\alpha^{2})$$. By Gaussian anti-concentration [49] and union bounding, $$P\\left(\\min_{i\\in[k]} |\\langle r(0), e_{i} \\rangle| \\geq \\alpha\\right) \\gtrsim 1$$ simultaneously for all $$i \\in [k]$$. On the other hand, since every entry of $$U(0)$$ is iid distributed $$\\sim N(0, \\alpha^{2})$$, by tail bounds for $$\\chi^{2}$$-distributed random variables [50, Lemma 1], $$\\|E(0)\\|_{F} \\leq \\|U(0)\\|_{F} \\leq 2\\alpha^{2}kd$$ with probability $$\\geq 1 - \\exp(-kd) \\geq 1 - O\\left(\\frac{1}{k}\\right)$$. The same bound applies for $$\\|r(0)\\|_{2}^{2}$$ since it is also upper bounded by $$\\|U(0)\\|_{F}$$. Finally, the upper and lower bounds on $$\\|r(0)\\|_{2}^{2}$$ follows by noting that $$\\|r(0)\\|_{2}^{2} = \\sum_{i=1}^{k} \\langle U^{*}, Ue_{i} \\rangle^{2}$$, which concentrates around $$k\\alpha^{2}$$. The result directly follows by concentration of $$\\chi^{2}$$-random variables.", "md": "Recall that $$\\langle r, e_{i} \\rangle$$ for each $$i \\in [k]$$ is distributed $$\\sim N(0, \\alpha^{2})$$. By Gaussian anti-concentration [49] and union bounding, $$P\\left(\\min_{i\\in[k]} |\\langle r(0), e_{i} \\rangle| \\geq \\alpha\\right) \\gtrsim 1$$ simultaneously for all $$i \\in [k]$$. On the other hand, since every entry of $$U(0)$$ is iid distributed $$\\sim N(0, \\alpha^{2})$$, by tail bounds for $$\\chi^{2}$$-distributed random variables [50, Lemma 1], $$\\|E(0)\\|_{F} \\leq \\|U(0)\\|_{F} \\leq 2\\alpha^{2}kd$$ with probability $$\\geq 1 - \\exp(-kd) \\geq 1 - O\\left(\\frac{1}{k}\\right)$$. The same bound applies for $$\\|r(0)\\|_{2}^{2}$$ since it is also upper bounded by $$\\|U(0)\\|_{F}$$. Finally, the upper and lower bounds on $$\\|r(0)\\|_{2}^{2}$$ follows by noting that $$\\|r(0)\\|_{2}^{2} = \\sum_{i=1}^{k} \\langle U^{*}, Ue_{i} \\rangle^{2}$$, which concentrates around $$k\\alpha^{2}$$. The result directly follows by concentration of $$\\chi^{2}$$-random variables."}, {"type": "heading", "lvl": 3, "value": "Corollary of Lemma 10", "md": "### Corollary of Lemma 10"}, {"type": "text", "value": "As a corollary of Lemma 10 when the initialization parameter satisfies the upper bound in Lemma 9, $$\\|E(0)\\|_{F}$$ and $$\\|r(0)\\|_{2}$$ are upper bounded by small constants. We will use this fact several times in proving Lemma 9.", "md": "As a corollary of Lemma 10 when the initialization parameter satisfies the upper bound in Lemma 9, $$\\|E(0)\\|_{F}$$ and $$\\|r(0)\\|_{2}$$ are upper bounded by small constants. We will use this fact several times in proving Lemma 9."}, {"type": "heading", "lvl": 2, "value": "Lemma 11", "md": "## Lemma 11"}, {"type": "text", "value": "At any time $$t \\geq 0$$, $$\\|E(t)\\|_{F} \\leq \\|E(0)\\|_{F}$$.", "md": "At any time $$t \\geq 0$$, $$\\|E(t)\\|_{F} \\leq \\|E(0)\\|_{F}$$."}, {"type": "heading", "lvl": 3, "value": "Proof:", "md": "### Proof:"}, {"type": "text", "value": "The proof follows by showing that the time derivative of $$\\|E\\|_{2}^{2}$$ is non-positive. Indeed,\n\n$$\\frac{d\\|E\\|_{2}^{2}}{dt} = 2E \\cdot \\frac{dE}{dt} = -2\\text{Tr}(E^{T}EU^{T}U) = -2\\|UE^{T}\\|_{F}^{2} \\leq 0 \\quad (28)$$\n\nStarting from a small initialization, this means that the error matrix $$E$$ remains small over the course of gradient flow. For any coordinate $$i \\in [k]$$, by eq. (23), the differential equation governing $$\\langle r, e_{i} \\rangle = \\langle r(t), e_{i} \\rangle$$ is,\n\n$$\\frac{d}{dt} \\langle r, e_{i} \\rangle = \\langle r, e_{i} \\rangle(1 - \\|r\\|_{2}^{2}) - e_{i}^{T}E^{T}Er \\quad (29)$$\n\nBy Lemma 11 we expect the error term $$E^{T}Er$$ to be small (not necessarily decaying) in comparison with the first term as long as $$\\|r(t)\\|_{2}$$ is smaller than an absolute constant. In this regime, the differential form is easy to control since we expect $$\\langle r, e_{i} \\rangle$$ to grow linearly and across the different coordinates $$i \\in [k]$$, we expect its value to be proportional to that at initialization, $$\\langle r(0), e_{i} \\rangle$$. However, when $$\\|r\\|_{2}$$ eventually approaches 1, the relative contribution of the signal term $$\\langle r, e_{i} \\rangle(1 - \\|r\\|_{2}^{2})$$ and the error term $$E^{T}Er$$ become important. This is the main technical challenge in proving Lemma 9. We will show that even as $$t \\to \\infty$$, the error term cannot change $$\\langle r, e_{i} \\rangle$$ by more than a constant factor. For a sufficiently large constant $$C > 0$$, define,\n\n$$T_{0} \\triangleq -\\frac{1}{2} \\log\\left(\\frac{\\|r(0)\\|_{2}^{2}}{1 - \\|E(0)\\|_{F}}\\right) - \\frac{1}{2} \\quad (30)$$\n\nAs we will later show, $$T_{0}$$ controls the amount of time it takes the signal norm $$\\|r(t)\\|_{2}$$ to grow to an absolute constant. Firstly, we show that when the initialization scale $$\\alpha$$ is sufficiently small, $$T_{0}$$ is approximately $$-\\log \\|r(0)\\|_{2}$$.", "md": "The proof follows by showing that the time derivative of $$\\|E\\|_{2}^{2}$$ is non-positive. Indeed,\n\n$$\\frac{d\\|E\\|_{2}^{2}}{dt} = 2E \\cdot \\frac{dE}{dt} = -2\\text{Tr}(E^{T}EU^{T}U) = -2\\|UE^{T}\\|_{F}^{2} \\leq 0 \\quad (28)$$\n\nStarting from a small initialization, this means that the error matrix $$E$$ remains small over the course of gradient flow. For any coordinate $$i \\in [k]$$, by eq. (23), the differential equation governing $$\\langle r, e_{i} \\rangle = \\langle r(t), e_{i} \\rangle$$ is,\n\n$$\\frac{d}{dt} \\langle r, e_{i} \\rangle = \\langle r, e_{i} \\rangle(1 - \\|r\\|_{2}^{2}) - e_{i}^{T}E^{T}Er \\quad (29)$$\n\nBy Lemma 11 we expect the error term $$E^{T}Er$$ to be small (not necessarily decaying) in comparison with the first term as long as $$\\|r(t)\\|_{2}$$ is smaller than an absolute constant. In this regime, the differential form is easy to control since we expect $$\\langle r, e_{i} \\rangle$$ to grow linearly and across the different coordinates $$i \\in [k]$$, we expect its value to be proportional to that at initialization, $$\\langle r(0), e_{i} \\rangle$$. However, when $$\\|r\\|_{2}$$ eventually approaches 1, the relative contribution of the signal term $$\\langle r, e_{i} \\rangle(1 - \\|r\\|_{2}^{2})$$ and the error term $$E^{T}Er$$ become important. This is the main technical challenge in proving Lemma 9. We will show that even as $$t \\to \\infty$$, the error term cannot change $$\\langle r, e_{i} \\rangle$$ by more than a constant factor. For a sufficiently large constant $$C > 0$$, define,\n\n$$T_{0} \\triangleq -\\frac{1}{2} \\log\\left(\\frac{\\|r(0)\\|_{2}^{2}}{1 - \\|E(0)\\|_{F}}\\right) - \\frac{1}{2} \\quad (30)$$\n\nAs we will later show, $$T_{0}$$ controls the amount of time it takes the signal norm $$\\|r(t)\\|_{2}$$ to grow to an absolute constant. Firstly, we show that when the initialization scale $$\\alpha$$ is sufficiently small, $$T_{0}$$ is approximately $$-\\log \\|r(0)\\|_{2}$$."}, {"type": "heading", "lvl": 2, "value": "Lemma 12", "md": "## Lemma 12"}, {"type": "text", "value": "When the initialization scale $$\\alpha \\leq \\frac{c}{k^{3}d} \\log(kd)$$ for a sufficiently small absolute constant $$c > 0$$,\n\n$$-\\log(\\|r(0)\\|_{2}) + \\frac{1}{2} \\leq T_{0} \\leq -\\log(\\|r(0)\\|_{2}) + \\frac{3}{2} \\quad (31)$$", "md": "When the initialization scale $$\\alpha \\leq \\frac{c}{k^{3}d} \\log(kd)$$ for a sufficiently small absolute constant $$c > 0$$,\n\n$$-\\log(\\|r(0)\\|_{2}) + \\frac{1}{2} \\leq T_{0} \\leq -\\log(\\|r(0)\\|_{2}) + \\frac{3}{2} \\quad (31)$$"}]}, {"page": 17, "text": "Proof. This follows from the fact that,\n                                                      \u2225E(0)\u22252   F                                     1\n                  T0 = \u2212     log(\u2225r(0)\u22252) \u2212        1 \u2212  \u2225E(0)\u22252    F  log(\u2225r(0)\u22252) +        2(1 \u2212   \u2225E(0)\u22252   F )             (32)\nThe lower bound on T0 follows readily, noting that \u2225r(0)\u22252, \u2225E(0)\u2225F < 1 from the initialization\nbounds in Lemma 10 when \u03b1 \u2264                c/k3d log(kd). For the upper bound on the other hand, note that the\nlast term of eq. (32) is upper bounded by,\n                                                              1\nAnd the middle term of eq. (32) is upper bounded by,2(1 \u2212   \u2225E(0)\u22252    F ) \u2264   1                                              (33)\n             \u2225E(0)\u22252   F     log(\u2225r(0)\u22252) \u2264           \u2225E(0)\u22252    F         1      (i)      2\u03b12kd         \u221a10\n       \u2212                                                                           \u2264                             \u2264  1         (34)\n          1 \u2212   \u2225E(0)\u22252   F                         1 \u2212  \u2225E(0)\u22252   F  \u2225r(0)\u22252          (1 \u2212   2\u03b12kd)       k\u03b1       2,\nwhere the last inequality assumes that \u03b1 \u2264               c34/  \u221a  kd for a small constant c34 > 0, and inequality (i)\nuses the bounds on \u2225E(0)\u2225F and \u2225r(0)\u22252 proved in Lemma 10. Combining eqs. (33) and (34) with\neq. (32) results in the proof of Lemma 12.\nThe next result we establish is an upper bound on the signal norm \u2225r(t)\u22252. The proof follows by\nupper bounding the rate of change of \u2225r(t)\u22252 and integrating the resulting bound.\nLemma 13. At any time t \u2265              0, the signal norm is upper bounded by,\n                                                            e2(t\u2212T0)+1+\u03b7)\n                                           \u2225r(t)\u22252 2 \u2264    1 + e2(t\u2212T0)+1+\u03b7) \u2264          1.                                     (35)\nProof. From the differential equation governing r we can infer that,\n                            d\u2225r\u22252 2\n                              dt     = 2\u2225r\u22252   2(1 \u2212   \u2225r\u22252 2) \u2212   2\u2225Er\u22252   2 \u2264   2\u2225r\u22252  2(1 \u2212   \u2225r\u22252 2)                      (36)\nBy a theorem of [51] on differential inequalities, the trajectory of \u2225r(t)\u22252                    2 as a function of t starting\nfrom some reference point \u2225r(0)\u22252            2 is pointwise lower bounded by the trajectory of \u2225r(t)\u22252                2 obtained\nwhen the inequality is set to be an equality. In particular, by integrating the differential equation this\nresults in the lower bound,\n                                               \u2225r(t)\u22252 2                \u2225r(0)\u22252  2\n                            \u2225r(0)\u222522   log  1 \u2212   \u2225r(t)\u22252 2  \u2212  log  1 \u2212   \u2225r(0)\u22252  2  \u2264  2t                                  (37)\nObserve that \u2212       log  1\u2212\u2225r(0)\u22252  2 \u2265   \u2212  log \u2225r(0)\u22252   2 \u2212  \u03b7 \u2265   2T0 \u2212    1 \u2212   \u03b7 where the first inequality uses the\nupper bound on \u2225r(0)\u22252         2 \u2264   \u03b7 when \u03b1 \u2264       c/k3d log(kd) and the second by Lemma 12. Therefore by\nrearranging the terms we have,\n                                                                e2t\u22122T0+1+\u03b7\n                                               \u2225r(t)\u22252 2 \u2264   1 + e2t\u22122T0+1+\u03b7 .                                                (38)\nAs it turns out, the error term ET Er(t) in eq. (23) can be shown to decrease linearly once \u2225r(t)\u22252                               2\nhits the critical threshold of 1/2. In the next lemma we show that the signal norm \u2225r(t)\u22252                                2 never\ndrops below the threshold of 3/4 for all t \u2265               T0 + 2. Thus, after a sufficiently large amount of time,\nwe expect the differential equation for r to behave as dr              dt \u2248    r(1 \u2212   \u2225r\u22252 2).\nLemma 14. For t \u2265            T0,       \u2225r(t)\u22252                  1 \u2212   \u2225E(0)\u22252   F                                             (39)\n                                                2 \u2265   1 + e\u22122(1\u2212\u2225E(0)\u22252       F)(t\u2212T0)+1\nAs an implication, for any t \u2265           T0 + 2, under the small initialization \u03b1 \u2264              c/k3d log(kd), \u2225r(t)\u22252        2 \u2265\n3/4.\n                                                                17", "md": "Proof. This follows from the fact that,\n\n$$\nT_0 = -\\log(\\|r(0)\\|^2) - \\frac{1 - \\|E(0)\\|_F^2}{\\|E(0)\\|_F} \\log(\\|r(0)\\|^2) + 2(1 - \\|E(0)\\|_F)^2 \\quad (32)\n$$\nThe lower bound on \\(T_0\\) follows readily, noting that \\(\\|r(0)\\|^2\\), \\(\\|E(0)\\|_F < 1\\) from the initialization bounds in Lemma 10 when \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\). For the upper bound on the other hand, note that the last term of eq. (32) is upper bounded by,\n\n$$\n\\frac{1}{1}\n$$\nAnd the middle term of eq. (32) is upper bounded by, \\(2(1 - \\|E(0)\\|_F)^2 \\leq 1 \\quad (33)\\)\n\n$$\n\\begin{align*}\n&\\|E(0)\\|_F \\log(\\|r(0)\\|^2) \\leq \\|E(0)\\|_F \\frac{1}{2\\alpha^2kd} \\sqrt{10} \\\\\n&- \\frac{1 - \\|E(0)\\|_F}{1 - \\|E(0)\\|_F \\|r(0)\\|^2} \\leq \\frac{1 - 2\\alpha^2kd}{k\\alpha^2} \\quad (34)\n\\end{align*}\n$$\nwhere the last inequality assumes that \\(\\alpha \\leq \\frac{c34}{\\sqrt{kd}}\\) for a small constant \\(c34 > 0\\), and inequality (i) uses the bounds on \\(\\|E(0)\\|_F\\) and \\(\\|r(0)\\|^2\\) proved in Lemma 10. Combining eqs. (33) and (34) with eq. (32) results in the proof of Lemma 12.\n\nThe next result we establish is an upper bound on the signal norm \\(\\|r(t)\\|^2\\). The proof follows by upper bounding the rate of change of \\(\\|r(t)\\|^2\\) and integrating the resulting bound.\n\nLemma 13. At any time \\(t \\geq 0\\), the signal norm is upper bounded by,\n\n$$\n\\frac{e^{2(t-T_0)+1+\\eta}}{\\|r(t)\\|^2} \\leq 1 + e^{2(t-T_0)+1+\\eta} \\leq 1 \\quad (35)\n$$\nProof. From the differential equation governing \\(r\\) we can infer that,\n\n$$\n\\frac{d\\|\\mathbf{r}\\|^2}{dt} = 2\\|\\mathbf{r}\\|^2(1 - \\|\\mathbf{r}\\|^2) - 2\\|\\mathbf{E}\\mathbf{r}\\|^2 \\leq 2\\|\\mathbf{r}\\|^2(1 - \\|\\mathbf{r}\\|^2) \\quad (36)\n$$\nBy a theorem of [51] on differential inequalities, the trajectory of \\(\\|\\mathbf{r}(t)\\|^2\\) as a function of \\(t\\) starting from some reference point \\(\\|\\mathbf{r}(0)\\|^2\\) is pointwise lower bounded by the trajectory of \\(\\|\\mathbf{r}(t)\\|^2\\) obtained when the inequality is set to be an equality. In particular, by integrating the differential equation this results in the lower bound,\n\n$$\n\\|\\mathbf{r}(t)\\|^2 \\log\\left(\\frac{1 - \\|\\mathbf{r}(t)\\|^2}{1 - \\|\\mathbf{r}(0)\\|^2}\\right) \\leq 2t \\quad (37)\n$$\nObserve that \\(-\\log\\left(1-\\|\\mathbf{r}(0)\\|^2\\right) \\geq -\\log\\|\\mathbf{r}(0)\\|^2 - \\eta \\geq 2T_0 - 1 - \\eta\\) where the first inequality uses the upper bound on \\(\\|\\mathbf{r}(0)\\|^2 \\leq \\eta\\) when \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\) and the second by Lemma 12. Therefore by rearranging the terms we have,\n\n$$\n\\|\\mathbf{r}(t)\\|^2 \\leq 1 + e^{2t-2T_0+1+\\eta} \\quad (38)\n$$\nAs it turns out, the error term \\(E^T\\mathbf{E}\\mathbf{r}(t)\\) in eq. (23) can be shown to decrease linearly once \\(\\|\\mathbf{r}(t)\\|^2\\) hits the critical threshold of 1/2. In the next lemma we show that the signal norm \\(\\|\\mathbf{r}(t)\\|^2\\) never drops below the threshold of 3/4 for all \\(t \\geq T_0 + 2\\). Thus, after a sufficiently large amount of time, we expect the differential equation for \\(r\\) to behave as \\(\\frac{dr}{dt} \\approx r(1 - \\|\\mathbf{r}\\|^2)\\).\n\nLemma 14. For \\(t \\geq T_0\\), \\(\\|\\mathbf{r}(t)\\|^2 \\geq 1 - \\|E(0)\\|_F \\quad (39)\n\n$$\n\\begin{align*}\n&\\geq 1 + e^{-2(1-\\|E(0)\\|_F)(t-T_0)+1}\n\\end{align*}\n$$\nAs an implication, for any \\(t \\geq T_0 + 2\\), under the small initialization \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\), \\(\\|\\mathbf{r}(t)\\|^2 \\geq 3/4\\).\n\n17", "images": [], "items": [{"type": "text", "value": "Proof. This follows from the fact that,\n\n$$\nT_0 = -\\log(\\|r(0)\\|^2) - \\frac{1 - \\|E(0)\\|_F^2}{\\|E(0)\\|_F} \\log(\\|r(0)\\|^2) + 2(1 - \\|E(0)\\|_F)^2 \\quad (32)\n$$\nThe lower bound on \\(T_0\\) follows readily, noting that \\(\\|r(0)\\|^2\\), \\(\\|E(0)\\|_F < 1\\) from the initialization bounds in Lemma 10 when \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\). For the upper bound on the other hand, note that the last term of eq. (32) is upper bounded by,\n\n$$\n\\frac{1}{1}\n$$\nAnd the middle term of eq. (32) is upper bounded by, \\(2(1 - \\|E(0)\\|_F)^2 \\leq 1 \\quad (33)\\)\n\n$$\n\\begin{align*}\n&\\|E(0)\\|_F \\log(\\|r(0)\\|^2) \\leq \\|E(0)\\|_F \\frac{1}{2\\alpha^2kd} \\sqrt{10} \\\\\n&- \\frac{1 - \\|E(0)\\|_F}{1 - \\|E(0)\\|_F \\|r(0)\\|^2} \\leq \\frac{1 - 2\\alpha^2kd}{k\\alpha^2} \\quad (34)\n\\end{align*}\n$$\nwhere the last inequality assumes that \\(\\alpha \\leq \\frac{c34}{\\sqrt{kd}}\\) for a small constant \\(c34 > 0\\), and inequality (i) uses the bounds on \\(\\|E(0)\\|_F\\) and \\(\\|r(0)\\|^2\\) proved in Lemma 10. Combining eqs. (33) and (34) with eq. (32) results in the proof of Lemma 12.\n\nThe next result we establish is an upper bound on the signal norm \\(\\|r(t)\\|^2\\). The proof follows by upper bounding the rate of change of \\(\\|r(t)\\|^2\\) and integrating the resulting bound.\n\nLemma 13. At any time \\(t \\geq 0\\), the signal norm is upper bounded by,\n\n$$\n\\frac{e^{2(t-T_0)+1+\\eta}}{\\|r(t)\\|^2} \\leq 1 + e^{2(t-T_0)+1+\\eta} \\leq 1 \\quad (35)\n$$\nProof. From the differential equation governing \\(r\\) we can infer that,\n\n$$\n\\frac{d\\|\\mathbf{r}\\|^2}{dt} = 2\\|\\mathbf{r}\\|^2(1 - \\|\\mathbf{r}\\|^2) - 2\\|\\mathbf{E}\\mathbf{r}\\|^2 \\leq 2\\|\\mathbf{r}\\|^2(1 - \\|\\mathbf{r}\\|^2) \\quad (36)\n$$\nBy a theorem of [51] on differential inequalities, the trajectory of \\(\\|\\mathbf{r}(t)\\|^2\\) as a function of \\(t\\) starting from some reference point \\(\\|\\mathbf{r}(0)\\|^2\\) is pointwise lower bounded by the trajectory of \\(\\|\\mathbf{r}(t)\\|^2\\) obtained when the inequality is set to be an equality. In particular, by integrating the differential equation this results in the lower bound,\n\n$$\n\\|\\mathbf{r}(t)\\|^2 \\log\\left(\\frac{1 - \\|\\mathbf{r}(t)\\|^2}{1 - \\|\\mathbf{r}(0)\\|^2}\\right) \\leq 2t \\quad (37)\n$$\nObserve that \\(-\\log\\left(1-\\|\\mathbf{r}(0)\\|^2\\right) \\geq -\\log\\|\\mathbf{r}(0)\\|^2 - \\eta \\geq 2T_0 - 1 - \\eta\\) where the first inequality uses the upper bound on \\(\\|\\mathbf{r}(0)\\|^2 \\leq \\eta\\) when \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\) and the second by Lemma 12. Therefore by rearranging the terms we have,\n\n$$\n\\|\\mathbf{r}(t)\\|^2 \\leq 1 + e^{2t-2T_0+1+\\eta} \\quad (38)\n$$\nAs it turns out, the error term \\(E^T\\mathbf{E}\\mathbf{r}(t)\\) in eq. (23) can be shown to decrease linearly once \\(\\|\\mathbf{r}(t)\\|^2\\) hits the critical threshold of 1/2. In the next lemma we show that the signal norm \\(\\|\\mathbf{r}(t)\\|^2\\) never drops below the threshold of 3/4 for all \\(t \\geq T_0 + 2\\). Thus, after a sufficiently large amount of time, we expect the differential equation for \\(r\\) to behave as \\(\\frac{dr}{dt} \\approx r(1 - \\|\\mathbf{r}\\|^2)\\).\n\nLemma 14. For \\(t \\geq T_0\\), \\(\\|\\mathbf{r}(t)\\|^2 \\geq 1 - \\|E(0)\\|_F \\quad (39)\n\n$$\n\\begin{align*}\n&\\geq 1 + e^{-2(1-\\|E(0)\\|_F)(t-T_0)+1}\n\\end{align*}\n$$\nAs an implication, for any \\(t \\geq T_0 + 2\\), under the small initialization \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\), \\(\\|\\mathbf{r}(t)\\|^2 \\geq 3/4\\).\n\n17", "md": "Proof. This follows from the fact that,\n\n$$\nT_0 = -\\log(\\|r(0)\\|^2) - \\frac{1 - \\|E(0)\\|_F^2}{\\|E(0)\\|_F} \\log(\\|r(0)\\|^2) + 2(1 - \\|E(0)\\|_F)^2 \\quad (32)\n$$\nThe lower bound on \\(T_0\\) follows readily, noting that \\(\\|r(0)\\|^2\\), \\(\\|E(0)\\|_F < 1\\) from the initialization bounds in Lemma 10 when \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\). For the upper bound on the other hand, note that the last term of eq. (32) is upper bounded by,\n\n$$\n\\frac{1}{1}\n$$\nAnd the middle term of eq. (32) is upper bounded by, \\(2(1 - \\|E(0)\\|_F)^2 \\leq 1 \\quad (33)\\)\n\n$$\n\\begin{align*}\n&\\|E(0)\\|_F \\log(\\|r(0)\\|^2) \\leq \\|E(0)\\|_F \\frac{1}{2\\alpha^2kd} \\sqrt{10} \\\\\n&- \\frac{1 - \\|E(0)\\|_F}{1 - \\|E(0)\\|_F \\|r(0)\\|^2} \\leq \\frac{1 - 2\\alpha^2kd}{k\\alpha^2} \\quad (34)\n\\end{align*}\n$$\nwhere the last inequality assumes that \\(\\alpha \\leq \\frac{c34}{\\sqrt{kd}}\\) for a small constant \\(c34 > 0\\), and inequality (i) uses the bounds on \\(\\|E(0)\\|_F\\) and \\(\\|r(0)\\|^2\\) proved in Lemma 10. Combining eqs. (33) and (34) with eq. (32) results in the proof of Lemma 12.\n\nThe next result we establish is an upper bound on the signal norm \\(\\|r(t)\\|^2\\). The proof follows by upper bounding the rate of change of \\(\\|r(t)\\|^2\\) and integrating the resulting bound.\n\nLemma 13. At any time \\(t \\geq 0\\), the signal norm is upper bounded by,\n\n$$\n\\frac{e^{2(t-T_0)+1+\\eta}}{\\|r(t)\\|^2} \\leq 1 + e^{2(t-T_0)+1+\\eta} \\leq 1 \\quad (35)\n$$\nProof. From the differential equation governing \\(r\\) we can infer that,\n\n$$\n\\frac{d\\|\\mathbf{r}\\|^2}{dt} = 2\\|\\mathbf{r}\\|^2(1 - \\|\\mathbf{r}\\|^2) - 2\\|\\mathbf{E}\\mathbf{r}\\|^2 \\leq 2\\|\\mathbf{r}\\|^2(1 - \\|\\mathbf{r}\\|^2) \\quad (36)\n$$\nBy a theorem of [51] on differential inequalities, the trajectory of \\(\\|\\mathbf{r}(t)\\|^2\\) as a function of \\(t\\) starting from some reference point \\(\\|\\mathbf{r}(0)\\|^2\\) is pointwise lower bounded by the trajectory of \\(\\|\\mathbf{r}(t)\\|^2\\) obtained when the inequality is set to be an equality. In particular, by integrating the differential equation this results in the lower bound,\n\n$$\n\\|\\mathbf{r}(t)\\|^2 \\log\\left(\\frac{1 - \\|\\mathbf{r}(t)\\|^2}{1 - \\|\\mathbf{r}(0)\\|^2}\\right) \\leq 2t \\quad (37)\n$$\nObserve that \\(-\\log\\left(1-\\|\\mathbf{r}(0)\\|^2\\right) \\geq -\\log\\|\\mathbf{r}(0)\\|^2 - \\eta \\geq 2T_0 - 1 - \\eta\\) where the first inequality uses the upper bound on \\(\\|\\mathbf{r}(0)\\|^2 \\leq \\eta\\) when \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\) and the second by Lemma 12. Therefore by rearranging the terms we have,\n\n$$\n\\|\\mathbf{r}(t)\\|^2 \\leq 1 + e^{2t-2T_0+1+\\eta} \\quad (38)\n$$\nAs it turns out, the error term \\(E^T\\mathbf{E}\\mathbf{r}(t)\\) in eq. (23) can be shown to decrease linearly once \\(\\|\\mathbf{r}(t)\\|^2\\) hits the critical threshold of 1/2. In the next lemma we show that the signal norm \\(\\|\\mathbf{r}(t)\\|^2\\) never drops below the threshold of 3/4 for all \\(t \\geq T_0 + 2\\). Thus, after a sufficiently large amount of time, we expect the differential equation for \\(r\\) to behave as \\(\\frac{dr}{dt} \\approx r(1 - \\|\\mathbf{r}\\|^2)\\).\n\nLemma 14. For \\(t \\geq T_0\\), \\(\\|\\mathbf{r}(t)\\|^2 \\geq 1 - \\|E(0)\\|_F \\quad (39)\n\n$$\n\\begin{align*}\n&\\geq 1 + e^{-2(1-\\|E(0)\\|_F)(t-T_0)+1}\n\\end{align*}\n$$\nAs an implication, for any \\(t \\geq T_0 + 2\\), under the small initialization \\(\\alpha \\leq \\frac{c}{k^3d} \\log(kd)\\), \\(\\|\\mathbf{r}(t)\\|^2 \\geq 3/4\\).\n\n17"}]}, {"page": 18, "text": "Proof. From the differential equation governing r, we may infer that,\n                           d\u2225r\u22252 2 = 2    r, dr    = 2\u2225r\u22252  2(1 \u2212  \u2225r\u222522) \u2212  2\u2225Er\u22252  2.                      (40)\n                             dt              dt\nNote that \u2225Er\u22252 \u2264      \u2225E\u2225F \u2225r\u22252 \u2264     \u2225E(0)\u2225F \u2225r\u22252 and therefore,\n                                  2\u2225r\u22252 2(1 \u2212  \u2225E(0)\u22252  F \u2212   \u2225r\u222522) \u2264  d\u2225r\u22252 2                              (41)\n                                                                          dt\nRearranging and integrating both sides,\n    2(1 \u2212  \u2225E(0)\u22252                          \u2225r(t)\u22252 2            \u2212  log             \u2225r(0)\u22252 2                (42)\n                    F )t \u2264  log    1 \u2212  \u2225E(0)\u22252 F \u2212   \u2225r(t)\u222522             1 \u2212  \u2225E(0)\u22252  F \u2212  \u2225r(0)\u22252 2\n                         \u2264  log             \u2225r(t)\u22252 2            + 2(1 \u2212    \u2225E(0)\u22252  F )T0 + 1               (43)\n                                   1 \u2212  \u2225E(0)\u22252 F \u2212   \u2225r(t)\u222522\nwhere the last inequality uses the choice of the initialization scale \u03b1 \u2264           c\u03b72/k3d log(kd), and the\nbounds on \u2225E(0)\u22252     F , \u2225r(0)\u222522 in Lemma 10. Therefore,\n                       (1 \u2212  \u2225E(0)\u22252 F \u2212   \u2225r(t)\u222522)e2(1\u2212\u2225E(0)\u22252  F)(t\u2212T0)\u22121 \u2264     \u2225r(t)\u222522                  (44)\n                                       1 \u2212  \u2225E(0)\u22252  F\n                        =\u21d2     1 + e\u22122(1\u2212\u2225E(0)\u22252   F)(t\u2212T0)+1 \u2264     \u2225r(t)\u222522                                 (45)\nHaving established that \u2225r(t)\u22252      2 does not decay below the threshold of 3/4 beyond time T0 +\n2, we establish that this condition is sufficient for the error term ET Er to begin decaying to 0.\nBelow we instead bound \u2225Er\u2225F , and a bound on \u2225ET Er\u2225F is obtained by upper bounding it as\n\u2225E(t)\u2225F \u2225Er(t)\u2225F \u2264        \u2225E(0)\u2225F \u2225Er\u2225F by Lemma 11.\nLemma 15. At any time t \u2265         0, the error term,      \u2225Er(0)\u22252et\nProof. By explicit computation,   \u2225Er(t)\u22252 \u2264     1 + e2(1\u2212\u2225E(0)\u22252  F)(t\u2212T0)\u22121                                (46)\n                         dEr   = \u2212EU T Ur + Er(1 \u2212         \u2225r\u22252 2) \u2212 EET Er                                  (47)\n                          dt\n                               = \u2212E(ET E + rrT )r + Er(1 \u2212            \u2225r\u222522) \u2212  EET Er                       (48)\n                               = \u22122E(ET E)r + Er(1 \u2212           2\u2225r\u22252 2)                                      (49)\nBy taking an inner product with Er, we get that,\n                            d\u2225Er\u22252  2 = \u22124\u2225(ET E)r\u22252      2 + 2\u2225Er\u22252  2(1 \u2212   2\u2225r\u222522).                       (50)\n                               dt\nUsing the lower bound on \u2225r\u22252      2 in Lemma 14,\n                        d\u2225Er\u22252  2  \u2264  2\u2225Er\u22252  2  1 \u2212          2(1 \u2212  \u2225E(0)\u22252  F )                            (51)\n                            dt                         1 + e\u22122(1\u2212\u2225E(0)\u22252   F)(t\u2212T0)+1\nRearranging and integrating both sides from time 0 to t using the fact that               dx\n                                                                                        1+e\u2212x = log(1 + ex),\n              log \u2225Er(t)\u22252  2 \u2212 log \u2225Er(0)\u22252   2 \u2264  2  t \u2212  log   1 + e2(1\u2212\u2225E(0)\u22252  F)(t\u2212T0)\u22121               (52)\n                                                             \u2225Er(0)\u22252et\n                               =\u21d2    \u2225Er(t)\u22252 \u2264     1 + e2(1\u2212\u2225E(0)\u22252   F)(t\u2212T0)\u22121 ,                          (53)\nwhere the last inequality follows by exponentiating both sides and rearranging.\n                                                        18", "md": "Proof. From the differential equation governing r, we may infer that,\n\n$$\n\\frac{d\\|r\\|^2}{dt^2} = 2r, \\frac{dr}{dt} = 2\\|r\\|^2(2(1 - \\|r\\|^2) - 2\\|Er\\|^2).\n$$\nNote that $\\|Er\\|^2 \\leq \\|E\\|_F \\|r\\|^2 \\leq \\|E(0)\\|_F \\|r\\|^2$ and therefore,\n\n$$\n2\\|r\\|^2(2(1 - \\|E(0)\\|^2_F - \\|r\\|^2) \\leq \\frac{d\\|r\\|^2}{dt^2} \\quad (41)\n$$\nRearranging and integrating both sides,\n\n$$\n2(1 - \\|E(0)\\|^2_F)t \\leq \\log \\frac{\\|r(t)\\|^2}{\\|r(0)\\|^2} \\quad (42)\n$$\n$$\n\\leq \\log \\frac{\\|r(t)\\|^2}{1 - \\|E(0)\\|^2_F - \\|r(t)\\|^2} \\leq \\log \\frac{\\|r(t)\\|^2}{\\|r(0)\\|^2} + 2(1 - \\|E(0)\\|^2_F)T_0 + 1 \\quad (43)\n$$\nwhere the last inequality uses the choice of the initialization scale $\\alpha \\leq c\\eta^2/k^3d \\log(kd)$, and the bounds on $\\|E(0)\\|^2_F$, $\\|r(0)\\|^2$ in Lemma 10. Therefore,\n\n$$\n(1 - \\|E(0)\\|^2_F - \\|r(t)\\|^2)e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}} \\leq \\|r(t)\\|^2 \\quad (44)\n$$\n$$\n\\Rightarrow 1 + e^{-2(1-\\|E(0)\\|^2_F)(t-T_0)+1} \\leq \\|r(t)\\|^2 \\quad (45)\n$$\nHaving established that $\\|r(t)\\|^2$ does not decay below the threshold of $3/4$ beyond time $T_0 + 2$, we establish that this condition is sufficient for the error term $ETEr$ to begin decaying to $0$.\n\nBelow we instead bound $\\|Er\\|_F$, and a bound on $\\|ETEr\\|_F$ is obtained by upper bounding it as $\\|E(t)\\|_F \\|Er(t)\\|_F \\leq \\|E(0)\\|_F \\|Er\\|_F$ by Lemma 11.\n\nLemma 15. At any time $t \\geq 0$, the error term, $\\|Er(0)\\|^2e^t$\n\nProof. By explicit computation, $\\|Er(t)\\|^2 \\leq 1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}}$ (46)\n\n$$\n\\frac{dEr}{dt} = -EU^TUr + Er(1 - \\|r\\|^2) - EETEr\n$$\n$$\n= -E(ETE + rr^T)r + Er(1 - \\|r\\|^2) - EETEr\n$$\n$$\n= -2E(ETE)r + Er(1 - 2\\|r\\|^2) \\quad (49)\n$$\nBy taking an inner product with Er, we get that,\n\n$$\n\\frac{d\\|Er\\|^2}{dt^2} = -4\\|(ETE)r\\|^2 + 2\\|Er\\|^2(1 - 2\\|r\\|^2) \\quad (50)\n$$\nUsing the lower bound on $\\|r\\|^2$ in Lemma 14,\n\n$$\n\\frac{d\\|Er\\|^2}{dt^2} \\leq 2\\|Er\\|^2(1 - 2(1 - \\|E(0)\\|^2_F) \\quad (51)\n$$\n$$\n\\frac{dt}{1 + e^{-x}} = \\log(1 + ex)\n$$\nRearranging and integrating both sides from time 0 to t using the fact that $\\log \\|Er(t)\\|^2 - \\log \\|Er(0)\\|^2 \\leq 2t - \\log(1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}}$\n\n$$\n\\Rightarrow \\|Er(t)\\|^2 \\leq 1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}} \\quad (53)\n$$\nwhere the last inequality follows by exponentiating both sides and rearranging.", "images": [], "items": [{"type": "text", "value": "Proof. From the differential equation governing r, we may infer that,\n\n$$\n\\frac{d\\|r\\|^2}{dt^2} = 2r, \\frac{dr}{dt} = 2\\|r\\|^2(2(1 - \\|r\\|^2) - 2\\|Er\\|^2).\n$$\nNote that $\\|Er\\|^2 \\leq \\|E\\|_F \\|r\\|^2 \\leq \\|E(0)\\|_F \\|r\\|^2$ and therefore,\n\n$$\n2\\|r\\|^2(2(1 - \\|E(0)\\|^2_F - \\|r\\|^2) \\leq \\frac{d\\|r\\|^2}{dt^2} \\quad (41)\n$$\nRearranging and integrating both sides,\n\n$$\n2(1 - \\|E(0)\\|^2_F)t \\leq \\log \\frac{\\|r(t)\\|^2}{\\|r(0)\\|^2} \\quad (42)\n$$\n$$\n\\leq \\log \\frac{\\|r(t)\\|^2}{1 - \\|E(0)\\|^2_F - \\|r(t)\\|^2} \\leq \\log \\frac{\\|r(t)\\|^2}{\\|r(0)\\|^2} + 2(1 - \\|E(0)\\|^2_F)T_0 + 1 \\quad (43)\n$$\nwhere the last inequality uses the choice of the initialization scale $\\alpha \\leq c\\eta^2/k^3d \\log(kd)$, and the bounds on $\\|E(0)\\|^2_F$, $\\|r(0)\\|^2$ in Lemma 10. Therefore,\n\n$$\n(1 - \\|E(0)\\|^2_F - \\|r(t)\\|^2)e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}} \\leq \\|r(t)\\|^2 \\quad (44)\n$$\n$$\n\\Rightarrow 1 + e^{-2(1-\\|E(0)\\|^2_F)(t-T_0)+1} \\leq \\|r(t)\\|^2 \\quad (45)\n$$\nHaving established that $\\|r(t)\\|^2$ does not decay below the threshold of $3/4$ beyond time $T_0 + 2$, we establish that this condition is sufficient for the error term $ETEr$ to begin decaying to $0$.\n\nBelow we instead bound $\\|Er\\|_F$, and a bound on $\\|ETEr\\|_F$ is obtained by upper bounding it as $\\|E(t)\\|_F \\|Er(t)\\|_F \\leq \\|E(0)\\|_F \\|Er\\|_F$ by Lemma 11.\n\nLemma 15. At any time $t \\geq 0$, the error term, $\\|Er(0)\\|^2e^t$\n\nProof. By explicit computation, $\\|Er(t)\\|^2 \\leq 1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}}$ (46)\n\n$$\n\\frac{dEr}{dt} = -EU^TUr + Er(1 - \\|r\\|^2) - EETEr\n$$\n$$\n= -E(ETE + rr^T)r + Er(1 - \\|r\\|^2) - EETEr\n$$\n$$\n= -2E(ETE)r + Er(1 - 2\\|r\\|^2) \\quad (49)\n$$\nBy taking an inner product with Er, we get that,\n\n$$\n\\frac{d\\|Er\\|^2}{dt^2} = -4\\|(ETE)r\\|^2 + 2\\|Er\\|^2(1 - 2\\|r\\|^2) \\quad (50)\n$$\nUsing the lower bound on $\\|r\\|^2$ in Lemma 14,\n\n$$\n\\frac{d\\|Er\\|^2}{dt^2} \\leq 2\\|Er\\|^2(1 - 2(1 - \\|E(0)\\|^2_F) \\quad (51)\n$$\n$$\n\\frac{dt}{1 + e^{-x}} = \\log(1 + ex)\n$$\nRearranging and integrating both sides from time 0 to t using the fact that $\\log \\|Er(t)\\|^2 - \\log \\|Er(0)\\|^2 \\leq 2t - \\log(1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}}$\n\n$$\n\\Rightarrow \\|Er(t)\\|^2 \\leq 1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}} \\quad (53)\n$$\nwhere the last inequality follows by exponentiating both sides and rearranging.", "md": "Proof. From the differential equation governing r, we may infer that,\n\n$$\n\\frac{d\\|r\\|^2}{dt^2} = 2r, \\frac{dr}{dt} = 2\\|r\\|^2(2(1 - \\|r\\|^2) - 2\\|Er\\|^2).\n$$\nNote that $\\|Er\\|^2 \\leq \\|E\\|_F \\|r\\|^2 \\leq \\|E(0)\\|_F \\|r\\|^2$ and therefore,\n\n$$\n2\\|r\\|^2(2(1 - \\|E(0)\\|^2_F - \\|r\\|^2) \\leq \\frac{d\\|r\\|^2}{dt^2} \\quad (41)\n$$\nRearranging and integrating both sides,\n\n$$\n2(1 - \\|E(0)\\|^2_F)t \\leq \\log \\frac{\\|r(t)\\|^2}{\\|r(0)\\|^2} \\quad (42)\n$$\n$$\n\\leq \\log \\frac{\\|r(t)\\|^2}{1 - \\|E(0)\\|^2_F - \\|r(t)\\|^2} \\leq \\log \\frac{\\|r(t)\\|^2}{\\|r(0)\\|^2} + 2(1 - \\|E(0)\\|^2_F)T_0 + 1 \\quad (43)\n$$\nwhere the last inequality uses the choice of the initialization scale $\\alpha \\leq c\\eta^2/k^3d \\log(kd)$, and the bounds on $\\|E(0)\\|^2_F$, $\\|r(0)\\|^2$ in Lemma 10. Therefore,\n\n$$\n(1 - \\|E(0)\\|^2_F - \\|r(t)\\|^2)e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}} \\leq \\|r(t)\\|^2 \\quad (44)\n$$\n$$\n\\Rightarrow 1 + e^{-2(1-\\|E(0)\\|^2_F)(t-T_0)+1} \\leq \\|r(t)\\|^2 \\quad (45)\n$$\nHaving established that $\\|r(t)\\|^2$ does not decay below the threshold of $3/4$ beyond time $T_0 + 2$, we establish that this condition is sufficient for the error term $ETEr$ to begin decaying to $0$.\n\nBelow we instead bound $\\|Er\\|_F$, and a bound on $\\|ETEr\\|_F$ is obtained by upper bounding it as $\\|E(t)\\|_F \\|Er(t)\\|_F \\leq \\|E(0)\\|_F \\|Er\\|_F$ by Lemma 11.\n\nLemma 15. At any time $t \\geq 0$, the error term, $\\|Er(0)\\|^2e^t$\n\nProof. By explicit computation, $\\|Er(t)\\|^2 \\leq 1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}}$ (46)\n\n$$\n\\frac{dEr}{dt} = -EU^TUr + Er(1 - \\|r\\|^2) - EETEr\n$$\n$$\n= -E(ETE + rr^T)r + Er(1 - \\|r\\|^2) - EETEr\n$$\n$$\n= -2E(ETE)r + Er(1 - 2\\|r\\|^2) \\quad (49)\n$$\nBy taking an inner product with Er, we get that,\n\n$$\n\\frac{d\\|Er\\|^2}{dt^2} = -4\\|(ETE)r\\|^2 + 2\\|Er\\|^2(1 - 2\\|r\\|^2) \\quad (50)\n$$\nUsing the lower bound on $\\|r\\|^2$ in Lemma 14,\n\n$$\n\\frac{d\\|Er\\|^2}{dt^2} \\leq 2\\|Er\\|^2(1 - 2(1 - \\|E(0)\\|^2_F) \\quad (51)\n$$\n$$\n\\frac{dt}{1 + e^{-x}} = \\log(1 + ex)\n$$\nRearranging and integrating both sides from time 0 to t using the fact that $\\log \\|Er(t)\\|^2 - \\log \\|Er(0)\\|^2 \\leq 2t - \\log(1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}}$\n\n$$\n\\Rightarrow \\|Er(t)\\|^2 \\leq 1 + e^{2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}} \\quad (53)\n$$\nwhere the last inequality follows by exponentiating both sides and rearranging."}]}, {"page": 19, "text": " Using the fact that \u2225Er\u2225F rapidly decays to 0 after time T0, we can in fact establish a more refined\n lower bound on the rate at which \u2225r(t)\u22252 approaches 1. The decay of the error term is not captured in\n the prior lower bound on \u2225r(t)\u22252 in Lemma 14. The error decay established in Lemma 15 is essential\n to proving such a result, especially as \u2225r(t)\u22252 approaches 1. In this regime, the error term \u2225Er\u22252                                        2 in\n eq. (40) becomes comparable with the leading order term \u2225r\u22252                             2(1 \u2212    \u2225r\u22252 2).\n Lemma 16. At any time t \u2264                 3T0/2, for some absolute constant C54 > 0,\n                                   \u2225r(t)\u22252                      e2t                        \u2225E(0)\u22252    F T0   .                            (54)\n Proof. From Lemma 15, note that,           2 \u2265    \u2225r(0)\u2225\u22122  2    + e2t \u2212     1 \u2212    C54      \u2225r(0)\u22252\n                                        \u2225Er(t)\u22252    2 \u2264            \u2225E(0)\u22252    F \u2225r(0)\u22252   2e2t                                            (55)\n                                                             1 + e2(1\u2212\u2225E(0)\u22252        F)(t\u2212T0)\u22121 2\n                                                             \u2225Er(0)\u22252     F e2t\n                                                       \u2272    1 + e2(t\u2212T0) 2 ,                                                              (56)\n where the last inequality uses the fact that when t \u2264                     4T0, (1 \u2212      \u2225E(0)\u22252    F )(t \u2212    T0) \u2265     t \u2212   T0 \u2212    4 (see\n the analysis in eqs. (32) and (34)). This inequality also uses Lemma 12 to bound \u2225r(0)\u22252                                           2. From\n Lemma 15, therefore, denoting x = \u2225r\u22252                    2, for some absolute constant C56 > 0,\n                                                dx                                   e2t \u2225Er(0)\u22252     F\n                                                 dt = 2x(1 \u2212         x) \u2212    C56     1 + e2(t\u2212T0) 2\u2225Er(0)\u22252     F                         (57)\n                                 =\u21d2      e\u22122t dx dt = 2e\u22122tx \u2212          2e\u22122tx2 \u2212        C56     1 + e2(t\u2212T0) 2                           (58)\n                           =\u21d2       d     e\u22122tx       = \u22122e\u22122tx2 \u2212           C56       \u2225Er(0)\u22252     F                                     (59)\n                                   dt                                               1 + e2(t\u2212T0) 2\n                                        =\u21d2      dy                                  \u2225Er(0)\u22252     F                                        (60)\n                                                 dt = \u22122y2e2t \u2212           C56     1 + e2(t\u2212T0) 2 ,\n where y = e\u22122tx. Define a new variable \u02dc                         y(t) with \u02dc    y(0) = y(0) and satisfying the following\n differential equation corresponding to just the first term on the RHS of eq. (60),\n                                       1  d\u02dcy                    \u21d2     \u02dc                      1\n                                      \u02dc                                y(t) =                                                             (61)\n                                      y2  dt = \u22122e2t \u21d0                            1/y(0) + e2t \u2212         1\n On the other hand, since dy            dt \u2264     \u22122y2e2t, rearranging and integrating both sides, we get y(t) \u2264\n1/(y(0) + e2t \u2212          1) = \u02dc  y(t). Plugging this back into eq. (60),\n                                           dy             y)2e2t \u2212      C56       \u2225Er(0)\u22252     F                                          (62)\n                                           dt \u2265     \u22122(\u02dc                       1 + e2(t\u2212T0) 2 .\n However, by definition, d\u02dc          dty= \u22122(\u02dc      y)2e2t and therefore, integrating both sides, for some absolute\n constant C\u2032    56 > 0, we get,        y(t) \u2212    y(0) \u2265      \u02dc\n                                                            y(t) \u2212     \u02dc\n                                                                       y(0) \u2212     C\u203256\u2225Er(0)\u22252      F T0                                  (63)\n Plugging in \u02dc    y(t) and subsequently y(t) and x(t) results in the equations,\n                                 \u2225r(t)\u22252  2 \u2265    \u2225r(0)\u2225\u22122  2 e2t+ e2t \u2212     1 \u2212    C\u203256\u2225Er(0)\u22252      F T0e2t                              (64)\nWhen t \u2264         3T0/2, by using the upper bound on T0 from Lemma 12, the second term on the RHS\n itself is upper bounded by C\u2032\u2032            56\u2225E(0)\u22252     F T0/\u2225r(0)\u22252 for another absolute constant C\u2032\u2032                      56 > 0. This\n completes the proof.\n                                                                       19", "md": "Using the fact that $$\\|Er\\|_F$$ rapidly decays to 0 after time $$T_0$$, we can in fact establish a more refined lower bound on the rate at which $$\\|r(t)\\|_2$$ approaches 1. The decay of the error term is not captured in the prior lower bound on $$\\|r(t)\\|_2$$ in Lemma 14. The error decay established in Lemma 15 is essential to proving such a result, especially as $$\\|r(t)\\|_2$$ approaches 1. In this regime, the error term $$\\|Er\\|_2^2$$ in eq. (40) becomes comparable with the leading order term $$\\|r\\|_2^2(1 - \\|r\\|_2^2)$$.\n\nLemma 16. At any time $$t \\leq \\frac{3T_0}{2}$$, for some absolute constant $$C_{54} > 0$$,\n\n$$\n\\|r(t)\\|_2^2 \\geq e^{2t} \\frac{\\|E(0)\\|_2}{FT_0}. \\tag{54}\n$$\n\nProof. From Lemma 15, note that,\n\n$$\n\\|r(0)\\|^{-2}_2 + e^{2t} - 1 - C_{54} \\leq \\|Er(t)\\|_2^2 \\leq \\|E(0)\\|_2F \\|r(0)\\|_2^2 e^{2t} \\tag{55}\n$$\n\n$$\n1 + e^{2(1 - \\|E(0)\\|_2F)(t - T_0)^{-1}} \\leq \\|Er(0)\\|_2Fe^{2t} \\lesssim 1 + e^{2(t - T_0)^2}. \\tag{56}\n$$\n\nWhere the last inequality uses the fact that when $$t \\leq 4T_0$$, $$(1 - \\|E(0)\\|_2F)(t - T_0) \\geq t - T_0 - 4$$. This inequality also uses Lemma 12 to bound $$\\|r(0)\\|_2^2$$. From Lemma 15, therefore, denoting $$x = \\|r\\|_2^2$$, for some absolute constant $$C_{56} > 0$$,\n\n$$\n\\frac{dx}{dt} = 2x(1 - x) - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{57}\n$$\n\n$$\n\\Rightarrow e^{-2t} \\frac{dx}{dt} = 2e^{-2tx} - 2e^{-2tx^2} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{58}\n$$\n\n$$\n\\Rightarrow \\frac{d}{dt} e^{-2tx} = -2e^{-2tx^2} - C_{56} \\frac{\\|Er(0)\\|_2F}{1 + e^{2(t - T_0)^2}} \\tag{59}\n$$\n\n$$\n\\Rightarrow \\frac{dy}{dt} = -2y^2e^{2t} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{60}\n$$\n\nWhere $$y = e^{-2tx}$$. Define a new variable $$\\tilde{y}(t)$$ with $$\\tilde{y}(0) = y(0)$$ and satisfying the following differential equation corresponding to just the first term on the RHS of eq. (60),\n\n$$\n\\tilde{y}(t) = \\frac{1}{y^2} \\Rightarrow \\tilde{y}(t) = -2e^{2t} \\left( \\frac{1}{y(0)} + e^{2t} - 1 \\right) \\tag{61}\n$$\n\nOn the other hand, since $$\\frac{dy}{dt} \\leq -2y^2e^{2t}$$, rearranging and integrating both sides, we get $$y(t) \\leq \\frac{1}{y(0) + e^{2t} - 1} = \\tilde{y}(t)$$. Plugging this back into eq. (60),\n\n$$\n\\frac{dy}{dt} \\geq -2(\\tilde{y})^2e^{2t} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{62}\n$$\n\nHowever, by definition, $$\\frac{d\\tilde{y}}{dt} = -2(\\tilde{y})^2e^{2t}$$ and therefore, integrating both sides, for some absolute constant $$C'_{56} > 0$$, we get,\n\n$$\ny(t) - y(0) \\geq \\tilde{y}(t) - \\tilde{y}(0) - C'_{56}\\|Er(0)\\|_2FT_0 \\tag{63}\n$$\n\nPlugging in $$\\tilde{y}(t)$$ and subsequently $$y(t)$$ and $$x(t)$$ results in the equations,\n\n$$\n\\|r(t)\\|_2^2 \\geq \\|r(0)\\|^{-2}_2 e^{2t} + e^{2t} - 1 - C'_{56}\\|Er(0)\\|_2FT_0e^{2t} \\tag{64}\n$$\n\nWhen $$t \\leq \\frac{3T_0}{2}$$, by using the upper bound on $$T_0$$ from Lemma 12, the second term on the RHS itself is upper bounded by $$C''_{56}\\|E(0)\\|_2FT_0/\\|r(0)\\|_2$$ for another absolute constant $$C''_{56} > 0$$. This completes the proof.", "images": [], "items": [{"type": "text", "value": "Using the fact that $$\\|Er\\|_F$$ rapidly decays to 0 after time $$T_0$$, we can in fact establish a more refined lower bound on the rate at which $$\\|r(t)\\|_2$$ approaches 1. The decay of the error term is not captured in the prior lower bound on $$\\|r(t)\\|_2$$ in Lemma 14. The error decay established in Lemma 15 is essential to proving such a result, especially as $$\\|r(t)\\|_2$$ approaches 1. In this regime, the error term $$\\|Er\\|_2^2$$ in eq. (40) becomes comparable with the leading order term $$\\|r\\|_2^2(1 - \\|r\\|_2^2)$$.\n\nLemma 16. At any time $$t \\leq \\frac{3T_0}{2}$$, for some absolute constant $$C_{54} > 0$$,\n\n$$\n\\|r(t)\\|_2^2 \\geq e^{2t} \\frac{\\|E(0)\\|_2}{FT_0}. \\tag{54}\n$$\n\nProof. From Lemma 15, note that,\n\n$$\n\\|r(0)\\|^{-2}_2 + e^{2t} - 1 - C_{54} \\leq \\|Er(t)\\|_2^2 \\leq \\|E(0)\\|_2F \\|r(0)\\|_2^2 e^{2t} \\tag{55}\n$$\n\n$$\n1 + e^{2(1 - \\|E(0)\\|_2F)(t - T_0)^{-1}} \\leq \\|Er(0)\\|_2Fe^{2t} \\lesssim 1 + e^{2(t - T_0)^2}. \\tag{56}\n$$\n\nWhere the last inequality uses the fact that when $$t \\leq 4T_0$$, $$(1 - \\|E(0)\\|_2F)(t - T_0) \\geq t - T_0 - 4$$. This inequality also uses Lemma 12 to bound $$\\|r(0)\\|_2^2$$. From Lemma 15, therefore, denoting $$x = \\|r\\|_2^2$$, for some absolute constant $$C_{56} > 0$$,\n\n$$\n\\frac{dx}{dt} = 2x(1 - x) - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{57}\n$$\n\n$$\n\\Rightarrow e^{-2t} \\frac{dx}{dt} = 2e^{-2tx} - 2e^{-2tx^2} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{58}\n$$\n\n$$\n\\Rightarrow \\frac{d}{dt} e^{-2tx} = -2e^{-2tx^2} - C_{56} \\frac{\\|Er(0)\\|_2F}{1 + e^{2(t - T_0)^2}} \\tag{59}\n$$\n\n$$\n\\Rightarrow \\frac{dy}{dt} = -2y^2e^{2t} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{60}\n$$\n\nWhere $$y = e^{-2tx}$$. Define a new variable $$\\tilde{y}(t)$$ with $$\\tilde{y}(0) = y(0)$$ and satisfying the following differential equation corresponding to just the first term on the RHS of eq. (60),\n\n$$\n\\tilde{y}(t) = \\frac{1}{y^2} \\Rightarrow \\tilde{y}(t) = -2e^{2t} \\left( \\frac{1}{y(0)} + e^{2t} - 1 \\right) \\tag{61}\n$$\n\nOn the other hand, since $$\\frac{dy}{dt} \\leq -2y^2e^{2t}$$, rearranging and integrating both sides, we get $$y(t) \\leq \\frac{1}{y(0) + e^{2t} - 1} = \\tilde{y}(t)$$. Plugging this back into eq. (60),\n\n$$\n\\frac{dy}{dt} \\geq -2(\\tilde{y})^2e^{2t} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{62}\n$$\n\nHowever, by definition, $$\\frac{d\\tilde{y}}{dt} = -2(\\tilde{y})^2e^{2t}$$ and therefore, integrating both sides, for some absolute constant $$C'_{56} > 0$$, we get,\n\n$$\ny(t) - y(0) \\geq \\tilde{y}(t) - \\tilde{y}(0) - C'_{56}\\|Er(0)\\|_2FT_0 \\tag{63}\n$$\n\nPlugging in $$\\tilde{y}(t)$$ and subsequently $$y(t)$$ and $$x(t)$$ results in the equations,\n\n$$\n\\|r(t)\\|_2^2 \\geq \\|r(0)\\|^{-2}_2 e^{2t} + e^{2t} - 1 - C'_{56}\\|Er(0)\\|_2FT_0e^{2t} \\tag{64}\n$$\n\nWhen $$t \\leq \\frac{3T_0}{2}$$, by using the upper bound on $$T_0$$ from Lemma 12, the second term on the RHS itself is upper bounded by $$C''_{56}\\|E(0)\\|_2FT_0/\\|r(0)\\|_2$$ for another absolute constant $$C''_{56} > 0$$. This completes the proof.", "md": "Using the fact that $$\\|Er\\|_F$$ rapidly decays to 0 after time $$T_0$$, we can in fact establish a more refined lower bound on the rate at which $$\\|r(t)\\|_2$$ approaches 1. The decay of the error term is not captured in the prior lower bound on $$\\|r(t)\\|_2$$ in Lemma 14. The error decay established in Lemma 15 is essential to proving such a result, especially as $$\\|r(t)\\|_2$$ approaches 1. In this regime, the error term $$\\|Er\\|_2^2$$ in eq. (40) becomes comparable with the leading order term $$\\|r\\|_2^2(1 - \\|r\\|_2^2)$$.\n\nLemma 16. At any time $$t \\leq \\frac{3T_0}{2}$$, for some absolute constant $$C_{54} > 0$$,\n\n$$\n\\|r(t)\\|_2^2 \\geq e^{2t} \\frac{\\|E(0)\\|_2}{FT_0}. \\tag{54}\n$$\n\nProof. From Lemma 15, note that,\n\n$$\n\\|r(0)\\|^{-2}_2 + e^{2t} - 1 - C_{54} \\leq \\|Er(t)\\|_2^2 \\leq \\|E(0)\\|_2F \\|r(0)\\|_2^2 e^{2t} \\tag{55}\n$$\n\n$$\n1 + e^{2(1 - \\|E(0)\\|_2F)(t - T_0)^{-1}} \\leq \\|Er(0)\\|_2Fe^{2t} \\lesssim 1 + e^{2(t - T_0)^2}. \\tag{56}\n$$\n\nWhere the last inequality uses the fact that when $$t \\leq 4T_0$$, $$(1 - \\|E(0)\\|_2F)(t - T_0) \\geq t - T_0 - 4$$. This inequality also uses Lemma 12 to bound $$\\|r(0)\\|_2^2$$. From Lemma 15, therefore, denoting $$x = \\|r\\|_2^2$$, for some absolute constant $$C_{56} > 0$$,\n\n$$\n\\frac{dx}{dt} = 2x(1 - x) - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{57}\n$$\n\n$$\n\\Rightarrow e^{-2t} \\frac{dx}{dt} = 2e^{-2tx} - 2e^{-2tx^2} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{58}\n$$\n\n$$\n\\Rightarrow \\frac{d}{dt} e^{-2tx} = -2e^{-2tx^2} - C_{56} \\frac{\\|Er(0)\\|_2F}{1 + e^{2(t - T_0)^2}} \\tag{59}\n$$\n\n$$\n\\Rightarrow \\frac{dy}{dt} = -2y^2e^{2t} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{60}\n$$\n\nWhere $$y = e^{-2tx}$$. Define a new variable $$\\tilde{y}(t)$$ with $$\\tilde{y}(0) = y(0)$$ and satisfying the following differential equation corresponding to just the first term on the RHS of eq. (60),\n\n$$\n\\tilde{y}(t) = \\frac{1}{y^2} \\Rightarrow \\tilde{y}(t) = -2e^{2t} \\left( \\frac{1}{y(0)} + e^{2t} - 1 \\right) \\tag{61}\n$$\n\nOn the other hand, since $$\\frac{dy}{dt} \\leq -2y^2e^{2t}$$, rearranging and integrating both sides, we get $$y(t) \\leq \\frac{1}{y(0) + e^{2t} - 1} = \\tilde{y}(t)$$. Plugging this back into eq. (60),\n\n$$\n\\frac{dy}{dt} \\geq -2(\\tilde{y})^2e^{2t} - C_{56} \\frac{1 + e^{2(t - T_0)^2}}{\\|Er(0)\\|_2F} \\tag{62}\n$$\n\nHowever, by definition, $$\\frac{d\\tilde{y}}{dt} = -2(\\tilde{y})^2e^{2t}$$ and therefore, integrating both sides, for some absolute constant $$C'_{56} > 0$$, we get,\n\n$$\ny(t) - y(0) \\geq \\tilde{y}(t) - \\tilde{y}(0) - C'_{56}\\|Er(0)\\|_2FT_0 \\tag{63}\n$$\n\nPlugging in $$\\tilde{y}(t)$$ and subsequently $$y(t)$$ and $$x(t)$$ results in the equations,\n\n$$\n\\|r(t)\\|_2^2 \\geq \\|r(0)\\|^{-2}_2 e^{2t} + e^{2t} - 1 - C'_{56}\\|Er(0)\\|_2FT_0e^{2t} \\tag{64}\n$$\n\nWhen $$t \\leq \\frac{3T_0}{2}$$, by using the upper bound on $$T_0$$ from Lemma 12, the second term on the RHS itself is upper bounded by $$C''_{56}\\|E(0)\\|_2FT_0/\\|r(0)\\|_2$$ for another absolute constant $$C''_{56} > 0$$. This completes the proof."}]}, {"page": 20, "text": "While the refined convergence lemma in Lemma 16 applies for the case when t \u2264                                             3T0/2, we also\n prove a lemma for the case when t \u2265                   3T0/2.\n Lemma 17. At any t \u2265               3T0/2,\n                                 e3t/4  (1 \u2212    \u2225r(t)\u22252   2) \u2272   1 + e11T0/4\u2225Er(0)\u22252           2 + e3T0/4.                                (65)\n Proof. Consider any time t \u2265                 3T0/2 which is greater than T0 + 2 by the small initialization and\n Lemma 10. By Lemma 14, \u2225r\u22252                  2 \u2265   3/4. From eq. (40) and Lemma 15,\n                      \u2200t \u2265     T0, d\u2225r\u22252    2  \u2265    3              2) \u2212   2               \u2225Er(0)\u22252     2e2t                               (66)\n                                        dt          4(1 \u2212     \u2225r\u22252             1 + e2(1\u2212\u2225E(0)\u22252       F)(t\u2212T0)\u22121 2\n Multiplying both sides by e3t/4 and rearranging,\n                      e3t/4 d(1 \u2212    dt\u2225r\u22252 2)   + 3 4e3t/4(1 \u2212       \u2225r\u22252 2) \u2264    2           \u2225Er(0)\u22252     2e2te3t/4                     (67)\n                                                                                       1 + e2(1\u2212\u2225E(0)\u22252        F)(t\u2212T0)\u22121 2\n               =\u21d2      d    e3t/4(1 \u2212      \u2225r\u22252  2)    \u2264   2               \u2225Er(0)\u22252     2e2te3t/4                                         (68)\n                      dt                                            1 + e2(1\u2212\u2225E(0)\u22252       F)(t\u2212T0)\u22121 2 dt.\n Integrating from T0 to t, we can upper bound eq. (68) as,\n                  e3t/4  (1 \u2212    \u2225r(t)\u22252  2) \u2212    e3T0/4(1 \u2212       \u2225r(T0)\u22252    2) \u2272       t   \u2225Er(0)\u22252     2e2te3t/4                      (69)\n                                                                                        T0   e4(1\u2212\u2225E(0)\u22252     F)(t\u2212T0) dt,\n By the small initialization, \u2225E(0)\u22252              F \u2264    1/8 and the denominator in the integral is lower bounded by\n e7/2(t\u2212T0). Therefore,\n                 e3t/4  (1 \u2212    \u2225r(t)\u22252  2) \u2212   e3T0/4(1 \u2212       \u2225r(T0)\u22252    2) \u2272    e7T0/2      T0t \u2225Er(0)\u22252     2e\u22123t/4dt               (70)\n                                                                                 \u2272   e7T0/2\u2225Er(0)\u22252        2e\u22123T0/4                       (71)\n Therefore,                                                                      = 2e11T0/4       \u2225Er(0)\u22252     2                          (72)\n                                 e3t/4  (1 \u2212    \u2225r(t)\u22252   2) \u2272   1 + e11T0/4\u2225Er(0)\u22252           2 + e3T0/4.                                (73)\n Finally we are ready to prove the lower and upper bounds on the limiting value of column norms of\n Ut as t \u2192      \u221e. Lemma 18 establishes the lower bound, while Lemma 19 establishes the upper bound.\n Lemma 18. When the initialization parameter \u03b1 \u2264                               c/k3d log(kd), then with probability \u2265                      1 \u2212\n O(1/k) over the initialization, for any t \u2265    \u2225U(t)ei\u22252 \u2265    4T0, (1 \u2212    \u03b7)|\u27e8r(0), ei\u27e9|.                                               (74)\n                                                                                  \u2225r(0)\u22252\n Proof. From the differential equation governing r, eq. (23), for any coordinate i \u2208                                     [k],\n                                d\u27e8r, ei\u27e92     = 2\u27e8r, ei\u27e92(1 \u2212        \u2225r\u22252  2) \u2212   2\u27e8r, ei\u27e9eiET Er                                         (75)\n                                    dt\n                                              \u2265  2\u27e8r, ei\u27e92(1 \u2212       \u2225r\u22252  2) \u2212   2|\u27e8r, ei\u27e9|\u2225E\u2225F \u2225Er\u22252                                    (76)\n From Lemma 13 and Lemma 15, and since \u2225E(t)\u2225F \u2264                                  \u2225E(0)\u2225F , at any time t \u2265             0,\n                             d\u27e8r, ei\u27e92                2\u27e8r, ei\u27e92\n                                  dt       \u2265                                                             F )(t\u2212T0)\u22121\n                                               e2(t\u2212T0)+1+\u03b7) + 1 \u2212             2|\u27e8r, ei\u27e9|\u2225E(0)\u2225F \u2225Er(0)\u2225F et\n                                                                                  1 + e2(1\u2212\u2225E(0)\u22252\n                      =\u21d2     d|\u27e8r, ei\u27e9|    \u2212           |\u27e8r, ei\u27e9|                            \u2225E(0)\u22252     F et                              (77)\n                                  dt           e2(t\u2212T0)+1+\u03b7 + 1 \u2265             \u22121 + e2(1\u2212\u2225E(0)\u22252         F )(t\u2212T0)\u22121\n                                                                       20", "md": "```markdown\nWhile the refined convergence lemma in Lemma 16 applies for the case when $$t \\leq \\frac{3T_0}{2}$$, we also prove a lemma for the case when $$t \\geq \\frac{3T_0}{2}$$.\n\nLemma 17. At any $$t \\geq \\frac{3T_0}{2}$$,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) \\lesssim 1 + e^{11T_0/4}\\|\\mathbf{Er}(0)\\|_2^2 + e^{3T_0/4}. \\quad (65)\n$$\n\nProof. Consider any time $$t \\geq \\frac{3T_0}{2}$$ which is greater than $$T_0 + 2$$ by the small initialization and Lemma 10. By Lemma 14, $$\\|\\mathbf{r}\\|_2^2 \\geq 3/4$$. From eq. (40) and Lemma 15,\n$$\n\\forall t \\geq T_0, \\frac{d\\|\\mathbf{r}\\|_2^2}{dt} \\geq \\frac{3}{4}(1 - \\|\\mathbf{r}\\|_2^2) - 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}. \\quad (66)\n$$\n\nMultiplying both sides by $$e^{3t/4}$$ and rearranging,\n$$\ne^{3t/4} \\frac{d(1 - \\|\\mathbf{r}\\|_2^2)}{dt} + \\frac{3}{4}e^{3t/4}(1 - \\|\\mathbf{r}\\|_2^2) \\leq 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}e^{3t/4}. \\quad (67)\n$$\n\n$$\\Rightarrow \\frac{d}{dt} e^{3t/4}(1 - \\|\\mathbf{r}\\|_2^2) \\leq 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}e^{3t/4} \\frac{1 + e^{2(1-\\|\\mathbf{E}(0)\\|_2^2)F}(t-T_0)^{-1}}{2} dt.$$\n\nIntegrating from $$T_0$$ to $$t$$, we can upper bound eq. (68) as,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) - e^{3T_0/4}(1 - \\|\\mathbf{r}(T_0)\\|_2^2) \\lesssim \\int_{T_0}^{t} e^{4(1-\\|\\mathbf{E}(0)\\|_2^2)F}(t-T_0) dt.\n$$\n\nBy the small initialization, $$\\|\\mathbf{E}(0)\\|_2^2F \\leq 1/8$$ and the denominator in the integral is lower bounded by $$e^{7/2}(t-T_0)$$. Therefore,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) - e^{3T_0/4}(1 - \\|\\mathbf{r}(T_0)\\|_2^2) \\lesssim e^{7T_0/2} \\int_{T_0}^{t} \\|\\mathbf{Er}(0)\\|_2^2e^{-3t/4} dt \\lesssim e^{7T_0/2}\\|\\mathbf{Er}(0)\\|_2^2e^{-3T_0/4}. \\quad (71)\n$$\n\nTherefore,\n$$\n= 2e^{11T_0/4} \\|\\mathbf{Er}(0)\\|_2^2. \\quad (72)\n$$\n\n$$e^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) \\lesssim 1 + e^{11T_0/4}\\|\\mathbf{Er}(0)\\|_2^2 + e^{3T_0/4}. \\quad (73)$$\n\nFinally, we are ready to prove the lower and upper bounds on the limiting value of column norms of $$U_t$$ as $$t \\to \\infty$$. Lemma 18 establishes the lower bound, while Lemma 19 establishes the upper bound.\n\nLemma 18. When the initialization parameter $$\\alpha \\leq \\frac{c}{k^3d} \\log(kd)$$, then with probability $$\\geq 1 - O(1/k)$$ over the initialization, for any $$t \\geq \\|\\mathbf{U}(t)e_i\\|_2 \\geq 4T_0, (1 - \\eta)|\\langle \\mathbf{r}(0), e_i \\rangle|/\\|\\mathbf{r}(0)\\|_2$$. (74)\n\nProof. From the differential equation governing $$\\mathbf{r}$$, eq. (23), for any coordinate $$i \\in [k]$$,\n$$\n\\frac{d\\langle \\mathbf{r}, e_i \\rangle^2}{dt} = 2\\langle \\mathbf{r}, e_i \\rangle^2(1 - \\|\\mathbf{r}\\|_2^2) - 2\\langle \\mathbf{r}, e_i \\rangle\\mathbf{e}_i^T\\mathbf{Er}. \\quad (75)\n$$\n\n$$\\geq 2\\langle \\mathbf{r}, e_i \\rangle^2(1 - \\|\\mathbf{r}\\|_2^2) - 2|\\langle \\mathbf{r}, e_i \\rangle|\\|\\mathbf{E}\\|_F\\|\\mathbf{Er}\\|_2. \\quad (76)$$\n\nFrom Lemma 13 and Lemma 15, and since $$\\|\\mathbf{E}(t)\\|_F \\leq \\|\\mathbf{E}(0)\\|_F$$, at any time $$t \\geq 0$$,\n$$\n\\frac{d\\langle \\mathbf{r}, e_i \\rangle^2}{dt} \\geq \\frac{2\\langle \\mathbf{r}, e_i \\rangle^2}{(1 + e^{2(t-T_0)+1+\\eta})} + 1 - 2|\\langle \\mathbf{r}, e_i \\rangle|\\|\\mathbf{E}(0)\\|_F\\|\\mathbf{Er}(0)\\|_F e^t.\n$$\n\n$$\\Rightarrow \\frac{d|\\langle \\mathbf{r}, e_i \\rangle|}{dt} - | \\langle \\mathbf{r}, e_i \\rangle| \\geq \\frac{\\|\\mathbf{E}(0)\\|_2^2}{F} e^t. \\quad (77)$$\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nWhile the refined convergence lemma in Lemma 16 applies for the case when $$t \\leq \\frac{3T_0}{2}$$, we also prove a lemma for the case when $$t \\geq \\frac{3T_0}{2}$$.\n\nLemma 17. At any $$t \\geq \\frac{3T_0}{2}$$,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) \\lesssim 1 + e^{11T_0/4}\\|\\mathbf{Er}(0)\\|_2^2 + e^{3T_0/4}. \\quad (65)\n$$\n\nProof. Consider any time $$t \\geq \\frac{3T_0}{2}$$ which is greater than $$T_0 + 2$$ by the small initialization and Lemma 10. By Lemma 14, $$\\|\\mathbf{r}\\|_2^2 \\geq 3/4$$. From eq. (40) and Lemma 15,\n$$\n\\forall t \\geq T_0, \\frac{d\\|\\mathbf{r}\\|_2^2}{dt} \\geq \\frac{3}{4}(1 - \\|\\mathbf{r}\\|_2^2) - 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}. \\quad (66)\n$$\n\nMultiplying both sides by $$e^{3t/4}$$ and rearranging,\n$$\ne^{3t/4} \\frac{d(1 - \\|\\mathbf{r}\\|_2^2)}{dt} + \\frac{3}{4}e^{3t/4}(1 - \\|\\mathbf{r}\\|_2^2) \\leq 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}e^{3t/4}. \\quad (67)\n$$\n\n$$\\Rightarrow \\frac{d}{dt} e^{3t/4}(1 - \\|\\mathbf{r}\\|_2^2) \\leq 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}e^{3t/4} \\frac{1 + e^{2(1-\\|\\mathbf{E}(0)\\|_2^2)F}(t-T_0)^{-1}}{2} dt.$$\n\nIntegrating from $$T_0$$ to $$t$$, we can upper bound eq. (68) as,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) - e^{3T_0/4}(1 - \\|\\mathbf{r}(T_0)\\|_2^2) \\lesssim \\int_{T_0}^{t} e^{4(1-\\|\\mathbf{E}(0)\\|_2^2)F}(t-T_0) dt.\n$$\n\nBy the small initialization, $$\\|\\mathbf{E}(0)\\|_2^2F \\leq 1/8$$ and the denominator in the integral is lower bounded by $$e^{7/2}(t-T_0)$$. Therefore,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) - e^{3T_0/4}(1 - \\|\\mathbf{r}(T_0)\\|_2^2) \\lesssim e^{7T_0/2} \\int_{T_0}^{t} \\|\\mathbf{Er}(0)\\|_2^2e^{-3t/4} dt \\lesssim e^{7T_0/2}\\|\\mathbf{Er}(0)\\|_2^2e^{-3T_0/4}. \\quad (71)\n$$\n\nTherefore,\n$$\n= 2e^{11T_0/4} \\|\\mathbf{Er}(0)\\|_2^2. \\quad (72)\n$$\n\n$$e^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) \\lesssim 1 + e^{11T_0/4}\\|\\mathbf{Er}(0)\\|_2^2 + e^{3T_0/4}. \\quad (73)$$\n\nFinally, we are ready to prove the lower and upper bounds on the limiting value of column norms of $$U_t$$ as $$t \\to \\infty$$. Lemma 18 establishes the lower bound, while Lemma 19 establishes the upper bound.\n\nLemma 18. When the initialization parameter $$\\alpha \\leq \\frac{c}{k^3d} \\log(kd)$$, then with probability $$\\geq 1 - O(1/k)$$ over the initialization, for any $$t \\geq \\|\\mathbf{U}(t)e_i\\|_2 \\geq 4T_0, (1 - \\eta)|\\langle \\mathbf{r}(0), e_i \\rangle|/\\|\\mathbf{r}(0)\\|_2$$. (74)\n\nProof. From the differential equation governing $$\\mathbf{r}$$, eq. (23), for any coordinate $$i \\in [k]$$,\n$$\n\\frac{d\\langle \\mathbf{r}, e_i \\rangle^2}{dt} = 2\\langle \\mathbf{r}, e_i \\rangle^2(1 - \\|\\mathbf{r}\\|_2^2) - 2\\langle \\mathbf{r}, e_i \\rangle\\mathbf{e}_i^T\\mathbf{Er}. \\quad (75)\n$$\n\n$$\\geq 2\\langle \\mathbf{r}, e_i \\rangle^2(1 - \\|\\mathbf{r}\\|_2^2) - 2|\\langle \\mathbf{r}, e_i \\rangle|\\|\\mathbf{E}\\|_F\\|\\mathbf{Er}\\|_2. \\quad (76)$$\n\nFrom Lemma 13 and Lemma 15, and since $$\\|\\mathbf{E}(t)\\|_F \\leq \\|\\mathbf{E}(0)\\|_F$$, at any time $$t \\geq 0$$,\n$$\n\\frac{d\\langle \\mathbf{r}, e_i \\rangle^2}{dt} \\geq \\frac{2\\langle \\mathbf{r}, e_i \\rangle^2}{(1 + e^{2(t-T_0)+1+\\eta})} + 1 - 2|\\langle \\mathbf{r}, e_i \\rangle|\\|\\mathbf{E}(0)\\|_F\\|\\mathbf{Er}(0)\\|_F e^t.\n$$\n\n$$\\Rightarrow \\frac{d|\\langle \\mathbf{r}, e_i \\rangle|}{dt} - | \\langle \\mathbf{r}, e_i \\rangle| \\geq \\frac{\\|\\mathbf{E}(0)\\|_2^2}{F} e^t. \\quad (77)$$\n```", "md": "```markdown\nWhile the refined convergence lemma in Lemma 16 applies for the case when $$t \\leq \\frac{3T_0}{2}$$, we also prove a lemma for the case when $$t \\geq \\frac{3T_0}{2}$$.\n\nLemma 17. At any $$t \\geq \\frac{3T_0}{2}$$,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) \\lesssim 1 + e^{11T_0/4}\\|\\mathbf{Er}(0)\\|_2^2 + e^{3T_0/4}. \\quad (65)\n$$\n\nProof. Consider any time $$t \\geq \\frac{3T_0}{2}$$ which is greater than $$T_0 + 2$$ by the small initialization and Lemma 10. By Lemma 14, $$\\|\\mathbf{r}\\|_2^2 \\geq 3/4$$. From eq. (40) and Lemma 15,\n$$\n\\forall t \\geq T_0, \\frac{d\\|\\mathbf{r}\\|_2^2}{dt} \\geq \\frac{3}{4}(1 - \\|\\mathbf{r}\\|_2^2) - 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}. \\quad (66)\n$$\n\nMultiplying both sides by $$e^{3t/4}$$ and rearranging,\n$$\ne^{3t/4} \\frac{d(1 - \\|\\mathbf{r}\\|_2^2)}{dt} + \\frac{3}{4}e^{3t/4}(1 - \\|\\mathbf{r}\\|_2^2) \\leq 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}e^{3t/4}. \\quad (67)\n$$\n\n$$\\Rightarrow \\frac{d}{dt} e^{3t/4}(1 - \\|\\mathbf{r}\\|_2^2) \\leq 2\\|\\mathbf{Er}(0)\\|_2^2e^{2t}e^{3t/4} \\frac{1 + e^{2(1-\\|\\mathbf{E}(0)\\|_2^2)F}(t-T_0)^{-1}}{2} dt.$$\n\nIntegrating from $$T_0$$ to $$t$$, we can upper bound eq. (68) as,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) - e^{3T_0/4}(1 - \\|\\mathbf{r}(T_0)\\|_2^2) \\lesssim \\int_{T_0}^{t} e^{4(1-\\|\\mathbf{E}(0)\\|_2^2)F}(t-T_0) dt.\n$$\n\nBy the small initialization, $$\\|\\mathbf{E}(0)\\|_2^2F \\leq 1/8$$ and the denominator in the integral is lower bounded by $$e^{7/2}(t-T_0)$$. Therefore,\n$$\ne^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) - e^{3T_0/4}(1 - \\|\\mathbf{r}(T_0)\\|_2^2) \\lesssim e^{7T_0/2} \\int_{T_0}^{t} \\|\\mathbf{Er}(0)\\|_2^2e^{-3t/4} dt \\lesssim e^{7T_0/2}\\|\\mathbf{Er}(0)\\|_2^2e^{-3T_0/4}. \\quad (71)\n$$\n\nTherefore,\n$$\n= 2e^{11T_0/4} \\|\\mathbf{Er}(0)\\|_2^2. \\quad (72)\n$$\n\n$$e^{3t/4} (1 - \\|\\mathbf{r}(t)\\|_2^2) \\lesssim 1 + e^{11T_0/4}\\|\\mathbf{Er}(0)\\|_2^2 + e^{3T_0/4}. \\quad (73)$$\n\nFinally, we are ready to prove the lower and upper bounds on the limiting value of column norms of $$U_t$$ as $$t \\to \\infty$$. Lemma 18 establishes the lower bound, while Lemma 19 establishes the upper bound.\n\nLemma 18. When the initialization parameter $$\\alpha \\leq \\frac{c}{k^3d} \\log(kd)$$, then with probability $$\\geq 1 - O(1/k)$$ over the initialization, for any $$t \\geq \\|\\mathbf{U}(t)e_i\\|_2 \\geq 4T_0, (1 - \\eta)|\\langle \\mathbf{r}(0), e_i \\rangle|/\\|\\mathbf{r}(0)\\|_2$$. (74)\n\nProof. From the differential equation governing $$\\mathbf{r}$$, eq. (23), for any coordinate $$i \\in [k]$$,\n$$\n\\frac{d\\langle \\mathbf{r}, e_i \\rangle^2}{dt} = 2\\langle \\mathbf{r}, e_i \\rangle^2(1 - \\|\\mathbf{r}\\|_2^2) - 2\\langle \\mathbf{r}, e_i \\rangle\\mathbf{e}_i^T\\mathbf{Er}. \\quad (75)\n$$\n\n$$\\geq 2\\langle \\mathbf{r}, e_i \\rangle^2(1 - \\|\\mathbf{r}\\|_2^2) - 2|\\langle \\mathbf{r}, e_i \\rangle|\\|\\mathbf{E}\\|_F\\|\\mathbf{Er}\\|_2. \\quad (76)$$\n\nFrom Lemma 13 and Lemma 15, and since $$\\|\\mathbf{E}(t)\\|_F \\leq \\|\\mathbf{E}(0)\\|_F$$, at any time $$t \\geq 0$$,\n$$\n\\frac{d\\langle \\mathbf{r}, e_i \\rangle^2}{dt} \\geq \\frac{2\\langle \\mathbf{r}, e_i \\rangle^2}{(1 + e^{2(t-T_0)+1+\\eta})} + 1 - 2|\\langle \\mathbf{r}, e_i \\rangle|\\|\\mathbf{E}(0)\\|_F\\|\\mathbf{Er}(0)\\|_F e^t.\n$$\n\n$$\\Rightarrow \\frac{d|\\langle \\mathbf{r}, e_i \\rangle|}{dt} - | \\langle \\mathbf{r}, e_i \\rangle| \\geq \\frac{\\|\\mathbf{E}(0)\\|_2^2}{F} e^t. \\quad (77)$$\n```"}]}, {"page": 21, "text": "Define q(t) = e\u22122(t\u2212T0)+1+\u03b7 (which we abbreviate simply as q) and multiply both sides by \u221a1 + q,\n                        1 + q d|\u27e8r, ei\u27e9|    \u2212   |\u27e8r, ei\u27e9| \u221a  1q+ q \u2265     \u2212      2\u2225E(0)\u22252    F et\u221a1 + q                        (78)\n                                    dt                                      1 + e2(1\u2212\u2225E(0)\u22252     F )(t\u2212T0)\u22121\n                   \u221a  1 + q = \u2212      \u221a q\nNoting that d    dt                    1+q, we get that,\n      =\u21d2      d    |\u27e8r, ei\u27e9|    1 + q     \u2265   \u22122\u2225E(0)\u22252     F e\u2212(t\u2212T0)+T0\u22122 \u00b7               \u221a 1 + e2(t\u2212T0+1)                  (79)\n             dt                                                                      1 + e2(1\u2212\u2225E(0)\u22252     F )(t\u2212T0)\u22121 .\nWe further lower bound the RHS for the case when t \u2264                       4T0. Later we will give a different proof to\nshow that when t \u2265         4T0, |\u27e8r, ei\u27e9| does not change significantly then onward.\nFor t \u2264    4T0, (1 \u2212     \u2225E(0)\u22252   F )(t \u2212   T0) \u2265    t \u2212  T0 \u2212   4. This follows from the same analysis as eqs. (32)\nand (34). In this regime, we therefore have that,\n                                     \u221a  1 + e2(t\u2212T0)+1+\u03b7                              1\nOn the other hand,               1 + e2(1\u2212\u2225E(0)\u22252     F )(t\u2212T0)\u22121 \u2272      \u221a  1 + e2(t\u2212T0)+1+\u03b7                                  (80)\n                                     e\u2212(t\u2212T0)+T0\u22122 \u2272          eT0 1 + e\u22122(t\u2212T0)+1+\u03b7                                           (81)\nMultiplying eqs. (80) and (81) results in the lower bound,\n                                         d    |\u27e8r, ei\u27e9|    1 + q     \u2273   \u2212\u2225E(0)\u22252     F eT0                                   (82)\n                                        dt\nIntegrating both sides from t = 0 to 4T0, and since T0 \u2265                    1 by the small initialization bound,\n                                                           1 + e2T0\u22121\u2212\u03b7                  \u221a         F T0eT0\n                      |\u27e8r(4T0), ei| \u2265      |\u27e8r(0), ei\u27e9|                          \u2212   c83\u2225E(0)\u22252                               (83)\n                                                                1 + e\u22124T0                  1 + e\u22124T0\nwhere c83 > 0 is an absolute constant. Since eT0 \u2265                  \u2212  log(\u2225r(0)\u22252)+1/2 from Lemma 12, and since\n\u2225r(0)\u22252  2 \u2264   \u03b7 by the small initialization, the term multiplying |\u27e8r(0), ei\u27e9| on the RHS is upper bounded\nby    1+e2T0\u22121\u2212\u03b7        \u2265   (1 \u2212   2\u03b7)eT0\u22121/2\u2212\u03b7/2 \u2265          (1 \u2212   3\u03b7)eT0\u22121/2. On the other hand, noting again by\n           1+e\u2212T0\nLemma 12 that T0 \u2264            \u2212  log(\u2225r(0)\u22252) + 3/2, and since \u2225r(0)\u22252              2 \u2273   k\u03b12 and \u2225E(0)\u22252       F \u2272    \u03b12kd and\nfurthermore |\u27e8r(0), ei\u27e9| \u2265          \u03b1/k2 from Lemma 10, as long as \u03b1 \u2264                  c\u03b7/k3d log(kd) for a suffi         ciently\nsmall constant c > 0,\n                                                                    \u221a          F T0\n                                              |\u27e8r(0), ei\u27e9| \u2265     c83\u2225E(0)\u22252                                                   (84)\nAnd by implication, eq. (83) gives,                               \u03b7   1 + e\u22124T0 .\n                                     |\u27e8r(4T0), ei| \u2265      (1 \u2212   4\u03b7)|\u27e8r(0), ei\u27e9|eT0\u22121/2,                                      (85)\nwhere the last inequality follows from the lower bound on T0 in Lemma 12.\nFinally we show that, from time 4T0 onward, \u27e8r(t), ei\u27e9                    does not change much. From eq. (77),\n                          d|\u27e8r, ei\u27e9|                    \u2225E(0)\u22252   F et                              F et\n                               dt      \u2265   \u2212                       F )(t\u2212T0)\u22121 \u2273      \u2212  \u2225E(0)\u22252                              (86)\n                                              1 + e2(1\u2212\u2225E(0)\u22252                           e(9/5)(t\u2212T0) .\nwhere the last inequality uses the fact that \u2225E(0)\u22252               F \u2264    1/10 by Lemma 10 and the upper bound on\nthe initialization scale \u03b1 \u2264         c\u03b72/k3d log(kd). Integrating both sides from 4T0 to t,\n                         |\u27e8r(t), ei\u27e9| \u2212    |\u27e8r(4T0), ei\u27e9| \u2273      \u2212\u2225E(0)\u22252     F e9T0/5      \u221e   e\u22124t/5dt                      (87)\n                                                                                           4T0\n                                                             \u2273   \u2212\u2225E(0)\u22252     F e\u22127T0/5                                       (88)\n                                                             (i)\n                                                              \u2273  \u2212|\u27e8r(0), ei\u27e9|e\u22127T0/5                                         (89)\n                                                                21", "md": "Define \\( q(t) = e^{-2(t-T0)}+1+\\eta \\) (which we abbreviate simply as \\( q \\)) and multiply both sides by \\( \\sqrt{1 + q} \\),\n\n\\[\n1 + q \\frac{d|\\langle r, e_i \\rangle|}{dt} - | \\langle r, e_i \\rangle| \\sqrt{1+q} \\geq -2\\|E(0)\\|_2 F e^{t\\sqrt{1 + q}} \\quad (78)\n\\]\n\nNoting that \\( \\frac{d}{dt} \\sqrt{1+q} = \\frac{1}{1+q} \\), we get that,\n\n\\[\n\\Rightarrow \\frac{d|\\langle r, e_i \\rangle|}{dt} \\frac{1 + q}{\\sqrt{1 + q}} \\geq -2\\|E(0)\\|_2 F e^{-(t-T0)+T0-2} \\sqrt{1 + e^2(t-T0+1)} \\quad (79)\n\\]\n\nWe further lower bound the RHS for the case when \\( t \\leq 4T0 \\). Later we will give a different proof to show that when \\( t \\geq 4T0 \\), \\( |\\langle r, e_i \\rangle| \\) does not change significantly then onward.\n\nFor \\( t \\leq 4T0 \\), \\( (1 - \\|E(0)\\|_2 F)(t - T0) \\geq t - T0 - 4 \\). This follows from the same analysis as eqs. (32) and (34). In this regime, we therefore have that,\n\n\\[\n\\sqrt{1 + e^2(t-T0)+1+\\eta} \\geq \\frac{1}{1 + e^2(1-\\|E(0)\\|_2 F)(t-T0)-1} \\lesssim \\sqrt{1 + e^2(t-T0)+1+\\eta} \\quad (80)\n\\]\n\n\\[\ne^{-(t-T0)+T0-2} \\lesssim e^{T0} \\frac{1}{1 + e^{-2(t-T0)+1+\\eta}} \\quad (81)\n\\]\n\nMultiplying eqs. (80) and (81) results in the lower bound,\n\n\\[\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\frac{1 + q}{\\sqrt{1 + q}} \\gtrapprox -\\|E(0)\\|_2 F e^{T0} \\quad (82)\n\\]\n\nIntegrating both sides from \\( t = 0 \\) to \\( 4T0 \\), and since \\( T0 \\geq 1 \\) by the small initialization bound,\n\n\\[\n|\\langle r(4T0), e_i| \\geq |\\langle r(0), e_i \\rangle| - \\frac{c}{83}\\|E(0)\\|_2 \\frac{1 + e^{2T0}-1-\\eta}{\\sqrt{F} T0e^{T0}} \\quad (83)\n\\]\n\nwhere \\( c_{83} > 0 \\) is an absolute constant. Since \\( e^{T0} \\geq -\\log(\\|r(0)\\|_2)+1/2 \\) from Lemma 12, and since \\( \\|r(0)\\|_2^2 \\leq \\eta \\) by the small initialization, the term multiplying \\( |\\langle r(0), e_i \\rangle| \\) on the RHS is upper bounded by \\( \\frac{1+e^{2T0}-1-\\eta}{1+e^{-4T0}} \\geq (1 - 2\\eta)e^{T0-1/2-\\eta/2} \\geq (1 - 3\\eta)e^{T0-1/2} \\). On the other hand, noting again by Lemma 12 that \\( T0 \\leq -\\log(\\|r(0)\\|_2) + 3/2 \\), and since \\( \\|r(0)\\|_2^2 \\gtrsim k\\alpha^2 \\) and \\( \\|E(0)\\|_2^2 F \\lesssim \\alpha^2k\\delta \\) and furthermore \\( |\\langle r(0), e_i \\rangle| \\geq \\alpha/k^2 \\) from Lemma 10, as long as \\( \\alpha \\leq c\\eta/k^3\\delta \\log(k\\delta) \\) for a sufficiently small constant \\( c > 0 \\),\n\n\\[\n|\\langle r(0), e_i \\rangle| \\geq c_{83}\\|E(0)\\|_2 \\sqrt{F} T0 \\quad (84)\n\\]\n\nAnd by implication, eq. (83) gives,\n\n\\[\n|\\langle r(4T0), e_i| \\geq (1 - 4\\eta)|\\langle r(0), e_i \\rangle|e^{T0-1/2} \\quad (85)\n\\]\n\nwhere the last inequality follows from the lower bound on \\( T0 \\) in Lemma 12.\n\nFinally we show that, from time \\( 4T0 \\) onward, \\( \\langle r(t), e_i \\rangle \\) does not change much. From eq. (77),\n\n\\[\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\geq -\\|E(0)\\|_2 F e^{t} \\sqrt{F} e^{(9/5)(t-T0)} \\quad (86)\n\\]\n\nwhere the last inequality uses the fact that \\( \\|E(0)\\|_2^2 F \\leq 1/10 \\) by Lemma 10 and the upper bound on the initialization scale \\( \\alpha \\leq c\\eta^2/k^3\\delta \\log(k\\delta) \\). Integrating both sides from \\( 4T0 \\) to \\( t \\),\n\n\\[\n|\\langle r(t), e_i \\rangle| - |\\langle r(4T0), e_i \\rangle| \\geq -\\|E(0)\\|_2 F e^{9T0/5} \\int_{4T0}^{\\infty} e^{-4t/5} dt \\quad (87)\n\\]\n\n\\[\n\\geq -\\|E(0)\\|_2 F e^{-7T0/5} \\quad (88)\n\\]\n\n\\[\n\\geq -|\\langle r(0), e_i \\rangle|e^{-7T0/5} \\quad (89)\n\\]", "images": [], "items": [{"type": "text", "value": "Define \\( q(t) = e^{-2(t-T0)}+1+\\eta \\) (which we abbreviate simply as \\( q \\)) and multiply both sides by \\( \\sqrt{1 + q} \\),\n\n\\[\n1 + q \\frac{d|\\langle r, e_i \\rangle|}{dt} - | \\langle r, e_i \\rangle| \\sqrt{1+q} \\geq -2\\|E(0)\\|_2 F e^{t\\sqrt{1 + q}} \\quad (78)\n\\]\n\nNoting that \\( \\frac{d}{dt} \\sqrt{1+q} = \\frac{1}{1+q} \\), we get that,\n\n\\[\n\\Rightarrow \\frac{d|\\langle r, e_i \\rangle|}{dt} \\frac{1 + q}{\\sqrt{1 + q}} \\geq -2\\|E(0)\\|_2 F e^{-(t-T0)+T0-2} \\sqrt{1 + e^2(t-T0+1)} \\quad (79)\n\\]\n\nWe further lower bound the RHS for the case when \\( t \\leq 4T0 \\). Later we will give a different proof to show that when \\( t \\geq 4T0 \\), \\( |\\langle r, e_i \\rangle| \\) does not change significantly then onward.\n\nFor \\( t \\leq 4T0 \\), \\( (1 - \\|E(0)\\|_2 F)(t - T0) \\geq t - T0 - 4 \\). This follows from the same analysis as eqs. (32) and (34). In this regime, we therefore have that,\n\n\\[\n\\sqrt{1 + e^2(t-T0)+1+\\eta} \\geq \\frac{1}{1 + e^2(1-\\|E(0)\\|_2 F)(t-T0)-1} \\lesssim \\sqrt{1 + e^2(t-T0)+1+\\eta} \\quad (80)\n\\]\n\n\\[\ne^{-(t-T0)+T0-2} \\lesssim e^{T0} \\frac{1}{1 + e^{-2(t-T0)+1+\\eta}} \\quad (81)\n\\]\n\nMultiplying eqs. (80) and (81) results in the lower bound,\n\n\\[\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\frac{1 + q}{\\sqrt{1 + q}} \\gtrapprox -\\|E(0)\\|_2 F e^{T0} \\quad (82)\n\\]\n\nIntegrating both sides from \\( t = 0 \\) to \\( 4T0 \\), and since \\( T0 \\geq 1 \\) by the small initialization bound,\n\n\\[", "md": "Define \\( q(t) = e^{-2(t-T0)}+1+\\eta \\) (which we abbreviate simply as \\( q \\)) and multiply both sides by \\( \\sqrt{1 + q} \\),\n\n\\[\n1 + q \\frac{d|\\langle r, e_i \\rangle|}{dt} - | \\langle r, e_i \\rangle| \\sqrt{1+q} \\geq -2\\|E(0)\\|_2 F e^{t\\sqrt{1 + q}} \\quad (78)\n\\]\n\nNoting that \\( \\frac{d}{dt} \\sqrt{1+q} = \\frac{1}{1+q} \\), we get that,\n\n\\[\n\\Rightarrow \\frac{d|\\langle r, e_i \\rangle|}{dt} \\frac{1 + q}{\\sqrt{1 + q}} \\geq -2\\|E(0)\\|_2 F e^{-(t-T0)+T0-2} \\sqrt{1 + e^2(t-T0+1)} \\quad (79)\n\\]\n\nWe further lower bound the RHS for the case when \\( t \\leq 4T0 \\). Later we will give a different proof to show that when \\( t \\geq 4T0 \\), \\( |\\langle r, e_i \\rangle| \\) does not change significantly then onward.\n\nFor \\( t \\leq 4T0 \\), \\( (1 - \\|E(0)\\|_2 F)(t - T0) \\geq t - T0 - 4 \\). This follows from the same analysis as eqs. (32) and (34). In this regime, we therefore have that,\n\n\\[\n\\sqrt{1 + e^2(t-T0)+1+\\eta} \\geq \\frac{1}{1 + e^2(1-\\|E(0)\\|_2 F)(t-T0)-1} \\lesssim \\sqrt{1 + e^2(t-T0)+1+\\eta} \\quad (80)\n\\]\n\n\\[\ne^{-(t-T0)+T0-2} \\lesssim e^{T0} \\frac{1}{1 + e^{-2(t-T0)+1+\\eta}} \\quad (81)\n\\]\n\nMultiplying eqs. (80) and (81) results in the lower bound,\n\n\\[\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\frac{1 + q}{\\sqrt{1 + q}} \\gtrapprox -\\|E(0)\\|_2 F e^{T0} \\quad (82)\n\\]\n\nIntegrating both sides from \\( t = 0 \\) to \\( 4T0 \\), and since \\( T0 \\geq 1 \\) by the small initialization bound,\n\n\\["}, {"type": "table", "rows": [["\\langle r(4T0), e_i", "\\geq", "\\langle r(0), e_i \\rangle", "- \\frac{c}{83}\\", "E(0)\\"]], "md": "|\\langle r(4T0), e_i| \\geq |\\langle r(0), e_i \\rangle| - \\frac{c}{83}\\|E(0)\\|_2 \\frac{1 + e^{2T0}-1-\\eta}{\\sqrt{F} T0e^{T0}} \\quad (83)", "isPerfectTable": true, "csv": "\"\\langle r(4T0), e_i\",\"\\geq\",\"\\langle r(0), e_i \\rangle\",\"- \\frac{c}{83}\\\",\"E(0)\\\""}, {"type": "text", "value": "\\]\n\nwhere \\( c_{83} > 0 \\) is an absolute constant. Since \\( e^{T0} \\geq -\\log(\\|r(0)\\|_2)+1/2 \\) from Lemma 12, and since \\( \\|r(0)\\|_2^2 \\leq \\eta \\) by the small initialization, the term multiplying \\( |\\langle r(0), e_i \\rangle| \\) on the RHS is upper bounded by \\( \\frac{1+e^{2T0}-1-\\eta}{1+e^{-4T0}} \\geq (1 - 2\\eta)e^{T0-1/2-\\eta/2} \\geq (1 - 3\\eta)e^{T0-1/2} \\). On the other hand, noting again by Lemma 12 that \\( T0 \\leq -\\log(\\|r(0)\\|_2) + 3/2 \\), and since \\( \\|r(0)\\|_2^2 \\gtrsim k\\alpha^2 \\) and \\( \\|E(0)\\|_2^2 F \\lesssim \\alpha^2k\\delta \\) and furthermore \\( |\\langle r(0), e_i \\rangle| \\geq \\alpha/k^2 \\) from Lemma 10, as long as \\( \\alpha \\leq c\\eta/k^3\\delta \\log(k\\delta) \\) for a sufficiently small constant \\( c > 0 \\),\n\n\\[", "md": "\\]\n\nwhere \\( c_{83} > 0 \\) is an absolute constant. Since \\( e^{T0} \\geq -\\log(\\|r(0)\\|_2)+1/2 \\) from Lemma 12, and since \\( \\|r(0)\\|_2^2 \\leq \\eta \\) by the small initialization, the term multiplying \\( |\\langle r(0), e_i \\rangle| \\) on the RHS is upper bounded by \\( \\frac{1+e^{2T0}-1-\\eta}{1+e^{-4T0}} \\geq (1 - 2\\eta)e^{T0-1/2-\\eta/2} \\geq (1 - 3\\eta)e^{T0-1/2} \\). On the other hand, noting again by Lemma 12 that \\( T0 \\leq -\\log(\\|r(0)\\|_2) + 3/2 \\), and since \\( \\|r(0)\\|_2^2 \\gtrsim k\\alpha^2 \\) and \\( \\|E(0)\\|_2^2 F \\lesssim \\alpha^2k\\delta \\) and furthermore \\( |\\langle r(0), e_i \\rangle| \\geq \\alpha/k^2 \\) from Lemma 10, as long as \\( \\alpha \\leq c\\eta/k^3\\delta \\log(k\\delta) \\) for a sufficiently small constant \\( c > 0 \\),\n\n\\["}, {"type": "table", "rows": [["\\langle r(0), e_i \\rangle", "\\geq c_{83}\\", "E(0)\\"]], "md": "|\\langle r(0), e_i \\rangle| \\geq c_{83}\\|E(0)\\|_2 \\sqrt{F} T0 \\quad (84)", "isPerfectTable": true, "csv": "\"\\langle r(0), e_i \\rangle\",\"\\geq c_{83}\\\",\"E(0)\\\""}, {"type": "text", "value": "\\]\n\nAnd by implication, eq. (83) gives,\n\n\\[", "md": "\\]\n\nAnd by implication, eq. (83) gives,\n\n\\["}, {"type": "table", "rows": [["\\langle r(4T0), e_i", "\\geq (1 - 4\\eta)", "\\langle r(0), e_i \\rangle"]], "md": "|\\langle r(4T0), e_i| \\geq (1 - 4\\eta)|\\langle r(0), e_i \\rangle|e^{T0-1/2} \\quad (85)", "isPerfectTable": true, "csv": "\"\\langle r(4T0), e_i\",\"\\geq (1 - 4\\eta)\",\"\\langle r(0), e_i \\rangle\""}, {"type": "text", "value": "\\]\n\nwhere the last inequality follows from the lower bound on \\( T0 \\) in Lemma 12.\n\nFinally we show that, from time \\( 4T0 \\) onward, \\( \\langle r(t), e_i \\rangle \\) does not change much. From eq. (77),\n\n\\[\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\geq -\\|E(0)\\|_2 F e^{t} \\sqrt{F} e^{(9/5)(t-T0)} \\quad (86)\n\\]\n\nwhere the last inequality uses the fact that \\( \\|E(0)\\|_2^2 F \\leq 1/10 \\) by Lemma 10 and the upper bound on the initialization scale \\( \\alpha \\leq c\\eta^2/k^3\\delta \\log(k\\delta) \\). Integrating both sides from \\( 4T0 \\) to \\( t \\),\n\n\\[", "md": "\\]\n\nwhere the last inequality follows from the lower bound on \\( T0 \\) in Lemma 12.\n\nFinally we show that, from time \\( 4T0 \\) onward, \\( \\langle r(t), e_i \\rangle \\) does not change much. From eq. (77),\n\n\\[\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\geq -\\|E(0)\\|_2 F e^{t} \\sqrt{F} e^{(9/5)(t-T0)} \\quad (86)\n\\]\n\nwhere the last inequality uses the fact that \\( \\|E(0)\\|_2^2 F \\leq 1/10 \\) by Lemma 10 and the upper bound on the initialization scale \\( \\alpha \\leq c\\eta^2/k^3\\delta \\log(k\\delta) \\). Integrating both sides from \\( 4T0 \\) to \\( t \\),\n\n\\["}, {"type": "table", "rows": [["\\langle r(t), e_i \\rangle", "-", "\\langle r(4T0), e_i \\rangle", "\\geq -\\", "E(0)\\"]], "md": "|\\langle r(t), e_i \\rangle| - |\\langle r(4T0), e_i \\rangle| \\geq -\\|E(0)\\|_2 F e^{9T0/5} \\int_{4T0}^{\\infty} e^{-4t/5} dt \\quad (87)", "isPerfectTable": true, "csv": "\"\\langle r(t), e_i \\rangle\",\"-\",\"\\langle r(4T0), e_i \\rangle\",\"\\geq -\\\",\"E(0)\\\""}, {"type": "text", "value": "\\]\n\n\\[\n\\geq -\\|E(0)\\|_2 F e^{-7T0/5} \\quad (88)\n\\]\n\n\\[\n\\geq -|\\langle r(0), e_i \\rangle|e^{-7T0/5} \\quad (89)\n\\]", "md": "\\]\n\n\\[\n\\geq -\\|E(0)\\|_2 F e^{-7T0/5} \\quad (88)\n\\]\n\n\\[\n\\geq -|\\langle r(0), e_i \\rangle|e^{-7T0/5} \\quad (89)\n\\]"}]}, {"page": 22, "text": "where (i) follows from eq. (84). Finally, combining with eq. (85),\n                             |\u27e8r(t), ei\u27e9| \u2265      (1 \u2212   4\u03b7)|\u27e8r(0), ei\u27e9|         eT0\u22121/2 \u2212       c90e\u22127T0/5                               (90)\nfor some absolute constant c90 > 0. By noting that T0 \u2265                     \u221a      \u2212  log \u2225r(0)\u22252 + 1/2 from Lemma 12 and\nthe fact that \u2225r(0)\u22252 \u2272           \u03b12k, by choosing \u03b1 \u2264             c\u203290/\u03b7      k for some absolute constant c\u2032             90 > 0 results\nin the inequality,\n                 \u2200t \u2265    T0, |\u27e8r(t), ei\u27e9| \u2265        (1 \u2212    5\u03b7)|\u27e8r(0), ei\u27e9|eT0\u22121/2 \u2265             (1 \u2212   5\u03b7)|\u27e8r(0), ei\u27e9|.                  (91)\nSince \u2225Uei\u22252 \u2265          |\u27e8r(t), ei\u27e9|, this completes the proof.                                                \u2225r(0)\u22252\nLemma 19. Suppose \u03b1 \u2264                  c\u03b72/\u03b7k3d log(kd) for a sufficiently small absolute constant c > 0. Then\nwith probability \u2265         1 \u2212    O(1/k) over the initialization, for any t \u2265                   3T0/2,\n                                             \u2225U(t)ei\u22252 \u2264         (1 + 4\u221a\u03b7)|\u27e8r(0), ei\u27e9|.                                                  (92)\n                                                                                   \u2225r(0)\u22252\nProof. Following the proof of the lower bound in Lemma 18, from the differential equation governing\nr, for any coordinate i \u2208           [k],\n                                d\u27e8r, ei\u27e92     = 2\u27e8r, ei\u27e92(1 \u2212        \u2225r\u22252  2) \u2212   2\u27e8r, ei\u27e9eiET Er                                        (93)\n                                     dt\n                                              \u2264   2\u27e8r, ei\u27e92(1 \u2212      \u2225r\u22252  2) + 2|\u27e8r, ei\u27e9|\u2225E\u2225F \u2225Er\u22252                                     (94)\n                       =\u21d2       d|\u27e8r, ei\u27e9|    \u2264   |\u27e8r, ei\u27e9|(1 \u2212     \u2225r\u22252 2) +        \u2225E(0)\u2225F \u2225Er(0)\u22252et                                  (95)\n                                    dt                                           1 + e2(1\u2212\u2225E(0)\u22252       F )(t\u2212T0)\u22121\nwhere in the last inequality, we bound \u2225E\u2225F \u2264                        \u2225E(0)\u2225F and apply Lemma 15 to upper bound the\nerror term \u2225Er(t)\u22252. Akin to Lemma 18, we carry out the analysis of |\u27e8r(t), ei\u27e9| in two parts, we\nfirst analyze its growth from time 0 to 3T0/2. From time 3T0/2 to t we show that |\u27e8r(t), ei\u27e9| does\nnot change significantly.\nIn particular, at any time t \u2264           3T0/2, since (1 \u2212          \u2225E(0)\u22252    F )(t \u2212    T0) \u2265     t \u2212   T0 \u2212    4 (see the analysis in\neqs. (32) and (34)), for some absolute constant C96,\n                            d|\u27e8r, ei\u27e9|    \u2264   |\u27e8r, ei\u27e9|(1 \u2212      \u2225r\u22252 2) + C96      \u2225E(0)\u2225F \u2225Er(0)\u22252et                                   (96)\n                                 dt                                                       1 + e2(t\u2212T0)\n                                          \u2264   |\u27e8r, ei\u27e9|(1 \u2212      \u2225r\u22252 2) + C96\u2225E(0)\u22252         F \u2225r(0)\u22252eT0,                              (97)\nwhere the last inequality uses the fact that et/(1 + e2(t\u2212T0)) is maximized at t = T0. Plugging in the\nlower bound on \u2225r\u22252         2 in Lemma 16,\n     d|\u27e8r, ei\u27e9|    \u2264   |\u27e8r, ei\u27e9|          \u2225r(0)\u2225\u22122  2    \u2212  1                 \u2225E(0)\u22252    F T0      + C96\u2225E(0)\u22252        F \u2225r(0)\u22252eT0\n          dt                          \u2225r(0)\u2225\u22122  2   + e2t \u2212      1 + C54        \u2225r(0)\u22252\n                   \u2264   |\u27e8r, ei\u27e9|          p         + C98\u2225E(0)\u22252         F T0.                                                           (98)\n                                      p + e2t\nwhere C98 > 0 is a suffi            ciently large absolute constant, and p = \u2225r(0)\u2225\u22122                    2    \u2212   1. Multiplying both\nsides by y =           pe\u22122t + 1 and noting that dy                           p       y, we get,\n                                                d            dt = \u2212        p+e2t\nIntegrating both sides from 0 to 3T0/2,        dt (y|\u27e8r, ei\u27e9|) = C98\u2225E(0)\u22252              F T0.                                           (99)\n                        |\u27e8r(3T0/2), ei\u27e9|          pe\u22123T0 + 1 \u2212         |\u27e8r(0), ei\u27e9|        1 + p = C\u2032     98\u2225E(0)\u22252     F T 20          (100)\n                 =\u21d2     |\u27e8r(3T0/2), ei\u27e9| \u2264          C\u203298\u2225E(0)\u22252     F T 20 + |\u27e8r(0), ei\u27e9|                                               (101)\n                                                                                  \u2225r(0)\u22252\n                                                                      22", "md": "where (i) follows from eq. (84). Finally, combining with eq. (85),\n\n$$\n|\\langle r(t), e_i \\rangle| \\geq (1 - 4\\eta)|\\langle r(0), e_i \\rangle| e^{T_0^{-1/2}} - c_{90}e^{-7T_0/5} \\quad (90)\n$$\n\nfor some absolute constant \\(c_{90} > 0\\). By noting that \\(T_0 \\geq \\sqrt{-\\log \\|r(0)\\|^2 + 1/2}\\) from Lemma 12 and the fact that \\(\\|r(0)\\|^2 \\lesssim \\alpha^2k\\), by choosing \\(\\alpha \\leq c'_{90}/\\eta k\\) for some absolute constant \\(c'_{90} > 0\\) results in the inequality,\n\n$$\n\\forall t \\geq T_0, |\\langle r(t), e_i \\rangle| \\geq (1 - 5\\eta)|\\langle r(0), e_i \\rangle|e^{T_0^{-1/2}} \\geq (1 - 5\\eta)|\\langle r(0), e_i \\rangle|. \\quad (91)\n$$\n\nSince \\(\\|Ue_i\\|^2 \\geq |\\langle r(t), e_i \\rangle|\\), this completes the proof.\n\n**Lemma 19.** Suppose \\(\\alpha \\leq c\\eta^2/\\eta k^3d \\log(kd)\\) for a sufficiently small absolute constant \\(c > 0\\). Then with probability \\(\\geq 1 - O(1/k)\\) over the initialization, for any \\(t \\geq 3T_0/2\\),\n\n$$\n\\|U(t)e_i\\|^2 \\leq (1 + 4\\sqrt{\\eta})|\\langle r(0), e_i \\rangle|. \\quad (92)\n$$\n\nProof. Following the proof of the lower bound in Lemma 18, from the differential equation governing \\(r\\), for any coordinate \\(i \\in [k]\\),\n\n$$\n\\frac{d\\langle r, e_i \\rangle^2}{dt} = 2\\langle r, e_i \\rangle^2(1 - \\|r\\|^2_2) - 2\\langle r, e_i \\rangle e_i^TEr \\leq 2\\langle r, e_i \\rangle^2(1 - \\|r\\|^2_2) + 2|\\langle r, e_i \\rangle|\\|E\\|_F \\|Er\\|_2 \\quad (93)\n$$\n\n$$\n\\Rightarrow \\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + \\|E(0)\\|_F \\|Er(0)\\|_2e^t (1 + e^2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}) \\quad (95)\n$$\n\nwhere in the last inequality, we bound \\(\\|E\\|_F \\leq \\|E(0)\\|_F\\) and apply Lemma 15 to upper bound the error term \\(\\|Er(t)\\|_2\\). Akin to Lemma 18, we carry out the analysis of \\(|\\langle r(t), e_i \\rangle|\\) in two parts, we first analyze its growth from time 0 to \\(3T_0/2\\). From time \\(3T_0/2\\) to \\(t\\) we show that \\(|\\langle r(t), e_i \\rangle|\\) does not change significantly.\n\nIn particular, at any time \\(t \\leq 3T_0/2\\), since \\((1 - \\|E(0)\\|^2_F)(t - T_0) \\geq t - T_0 - 4\\) (see the analysis in eqs. (32) and (34)), for some absolute constant \\(C96\\),\n\n$$\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + C96 \\|E(0)\\|_F \\|Er(0)\\|_2e^t (1 + e^2(t-T_0)) \\quad (96)\n$$\n\n$$\n\\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + C96\\|E(0)\\|^2_F \\|r(0)\\|_2e^{T_0}, \\quad (97)\n$$\n\nwhere the last inequality uses the fact that \\(e^t/(1 + e^2(t-T_0))\\) is maximized at \\(t = T_0\\). Plugging in the lower bound on \\(\\|r\\|^2_2\\) in Lemma 16,\n\n$$\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle| \\|r(0)\\|^{-2}_2 - 1 \\|E(0)\\|^2_F T_0 + C96\\|E(0)\\|^2_F \\|r(0)\\|_2e^{T_0} \\quad (98)\n$$\n\n$$\n\\leq |\\langle r, e_i \\rangle| p + C98\\|E(0)\\|^2_F T_0. \\quad (98)\n$$\n\nwhere \\(C98 > 0\\) is a sufficiently large absolute constant, and \\(p = \\|r(0)\\|^{-2}_2 - 1\\). Multiplying both sides by \\(y = pe^{-2t} + 1\\) and noting that \\(\\frac{dy}{dt} = -p+e^{2t}\\), we get,\n\n$$\n\\int_{0}^{3T_0/2} dt (y|\\langle r, e_i \\rangle|) = C98\\|E(0)\\|^2_F T_0. \\quad (99)\n$$\n\n$$\n|\\langle r(3T_0/2), e_i \\rangle| pe^{-3T_0} + 1 - |\\langle r(0), e_i \\rangle| 1 + p = C'_{98}\\|E(0)\\|^2_F T_0^2 \\quad (100)\n$$\n\n$$\n\\Rightarrow |\\langle r(3T_0/2), e_i \\rangle| \\leq C'_{98}\\|E(0)\\|^2_F T_0^2 + |\\langle r(0), e_i \\rangle| \\quad (101)\n$$", "images": [], "items": [{"type": "text", "value": "where (i) follows from eq. (84). Finally, combining with eq. (85),\n\n$$", "md": "where (i) follows from eq. (84). Finally, combining with eq. (85),\n\n$$"}, {"type": "table", "rows": [["\\langle r(t), e_i \\rangle", "\\geq (1 - 4\\eta)", "\\langle r(0), e_i \\rangle"]], "md": "|\\langle r(t), e_i \\rangle| \\geq (1 - 4\\eta)|\\langle r(0), e_i \\rangle| e^{T_0^{-1/2}} - c_{90}e^{-7T_0/5} \\quad (90)", "isPerfectTable": true, "csv": "\"\\langle r(t), e_i \\rangle\",\"\\geq (1 - 4\\eta)\",\"\\langle r(0), e_i \\rangle\""}, {"type": "text", "value": "$$\n\nfor some absolute constant \\(c_{90} > 0\\). By noting that \\(T_0 \\geq \\sqrt{-\\log \\|r(0)\\|^2 + 1/2}\\) from Lemma 12 and the fact that \\(\\|r(0)\\|^2 \\lesssim \\alpha^2k\\), by choosing \\(\\alpha \\leq c'_{90}/\\eta k\\) for some absolute constant \\(c'_{90} > 0\\) results in the inequality,\n\n$$\n\\forall t \\geq T_0, |\\langle r(t), e_i \\rangle| \\geq (1 - 5\\eta)|\\langle r(0), e_i \\rangle|e^{T_0^{-1/2}} \\geq (1 - 5\\eta)|\\langle r(0), e_i \\rangle|. \\quad (91)\n$$\n\nSince \\(\\|Ue_i\\|^2 \\geq |\\langle r(t), e_i \\rangle|\\), this completes the proof.\n\n**Lemma 19.** Suppose \\(\\alpha \\leq c\\eta^2/\\eta k^3d \\log(kd)\\) for a sufficiently small absolute constant \\(c > 0\\). Then with probability \\(\\geq 1 - O(1/k)\\) over the initialization, for any \\(t \\geq 3T_0/2\\),\n\n$$\n\\|U(t)e_i\\|^2 \\leq (1 + 4\\sqrt{\\eta})|\\langle r(0), e_i \\rangle|. \\quad (92)\n$$\n\nProof. Following the proof of the lower bound in Lemma 18, from the differential equation governing \\(r\\), for any coordinate \\(i \\in [k]\\),\n\n$$\n\\frac{d\\langle r, e_i \\rangle^2}{dt} = 2\\langle r, e_i \\rangle^2(1 - \\|r\\|^2_2) - 2\\langle r, e_i \\rangle e_i^TEr \\leq 2\\langle r, e_i \\rangle^2(1 - \\|r\\|^2_2) + 2|\\langle r, e_i \\rangle|\\|E\\|_F \\|Er\\|_2 \\quad (93)\n$$\n\n$$\n\\Rightarrow \\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + \\|E(0)\\|_F \\|Er(0)\\|_2e^t (1 + e^2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}) \\quad (95)\n$$\n\nwhere in the last inequality, we bound \\(\\|E\\|_F \\leq \\|E(0)\\|_F\\) and apply Lemma 15 to upper bound the error term \\(\\|Er(t)\\|_2\\). Akin to Lemma 18, we carry out the analysis of \\(|\\langle r(t), e_i \\rangle|\\) in two parts, we first analyze its growth from time 0 to \\(3T_0/2\\). From time \\(3T_0/2\\) to \\(t\\) we show that \\(|\\langle r(t), e_i \\rangle|\\) does not change significantly.\n\nIn particular, at any time \\(t \\leq 3T_0/2\\), since \\((1 - \\|E(0)\\|^2_F)(t - T_0) \\geq t - T_0 - 4\\) (see the analysis in eqs. (32) and (34)), for some absolute constant \\(C96\\),\n\n$$\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + C96 \\|E(0)\\|_F \\|Er(0)\\|_2e^t (1 + e^2(t-T_0)) \\quad (96)\n$$\n\n$$\n\\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + C96\\|E(0)\\|^2_F \\|r(0)\\|_2e^{T_0}, \\quad (97)\n$$\n\nwhere the last inequality uses the fact that \\(e^t/(1 + e^2(t-T_0))\\) is maximized at \\(t = T_0\\). Plugging in the lower bound on \\(\\|r\\|^2_2\\) in Lemma 16,\n\n$$\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle| \\|r(0)\\|^{-2}_2 - 1 \\|E(0)\\|^2_F T_0 + C96\\|E(0)\\|^2_F \\|r(0)\\|_2e^{T_0} \\quad (98)\n$$\n\n$$\n\\leq |\\langle r, e_i \\rangle| p + C98\\|E(0)\\|^2_F T_0. \\quad (98)\n$$\n\nwhere \\(C98 > 0\\) is a sufficiently large absolute constant, and \\(p = \\|r(0)\\|^{-2}_2 - 1\\). Multiplying both sides by \\(y = pe^{-2t} + 1\\) and noting that \\(\\frac{dy}{dt} = -p+e^{2t}\\), we get,\n\n$$\n\\int_{0}^{3T_0/2} dt (y|\\langle r, e_i \\rangle|) = C98\\|E(0)\\|^2_F T_0. \\quad (99)\n$$\n\n$$", "md": "$$\n\nfor some absolute constant \\(c_{90} > 0\\). By noting that \\(T_0 \\geq \\sqrt{-\\log \\|r(0)\\|^2 + 1/2}\\) from Lemma 12 and the fact that \\(\\|r(0)\\|^2 \\lesssim \\alpha^2k\\), by choosing \\(\\alpha \\leq c'_{90}/\\eta k\\) for some absolute constant \\(c'_{90} > 0\\) results in the inequality,\n\n$$\n\\forall t \\geq T_0, |\\langle r(t), e_i \\rangle| \\geq (1 - 5\\eta)|\\langle r(0), e_i \\rangle|e^{T_0^{-1/2}} \\geq (1 - 5\\eta)|\\langle r(0), e_i \\rangle|. \\quad (91)\n$$\n\nSince \\(\\|Ue_i\\|^2 \\geq |\\langle r(t), e_i \\rangle|\\), this completes the proof.\n\n**Lemma 19.** Suppose \\(\\alpha \\leq c\\eta^2/\\eta k^3d \\log(kd)\\) for a sufficiently small absolute constant \\(c > 0\\). Then with probability \\(\\geq 1 - O(1/k)\\) over the initialization, for any \\(t \\geq 3T_0/2\\),\n\n$$\n\\|U(t)e_i\\|^2 \\leq (1 + 4\\sqrt{\\eta})|\\langle r(0), e_i \\rangle|. \\quad (92)\n$$\n\nProof. Following the proof of the lower bound in Lemma 18, from the differential equation governing \\(r\\), for any coordinate \\(i \\in [k]\\),\n\n$$\n\\frac{d\\langle r, e_i \\rangle^2}{dt} = 2\\langle r, e_i \\rangle^2(1 - \\|r\\|^2_2) - 2\\langle r, e_i \\rangle e_i^TEr \\leq 2\\langle r, e_i \\rangle^2(1 - \\|r\\|^2_2) + 2|\\langle r, e_i \\rangle|\\|E\\|_F \\|Er\\|_2 \\quad (93)\n$$\n\n$$\n\\Rightarrow \\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + \\|E(0)\\|_F \\|Er(0)\\|_2e^t (1 + e^2(1-\\|E(0)\\|^2_F)(t-T_0)^{-1}) \\quad (95)\n$$\n\nwhere in the last inequality, we bound \\(\\|E\\|_F \\leq \\|E(0)\\|_F\\) and apply Lemma 15 to upper bound the error term \\(\\|Er(t)\\|_2\\). Akin to Lemma 18, we carry out the analysis of \\(|\\langle r(t), e_i \\rangle|\\) in two parts, we first analyze its growth from time 0 to \\(3T_0/2\\). From time \\(3T_0/2\\) to \\(t\\) we show that \\(|\\langle r(t), e_i \\rangle|\\) does not change significantly.\n\nIn particular, at any time \\(t \\leq 3T_0/2\\), since \\((1 - \\|E(0)\\|^2_F)(t - T_0) \\geq t - T_0 - 4\\) (see the analysis in eqs. (32) and (34)), for some absolute constant \\(C96\\),\n\n$$\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + C96 \\|E(0)\\|_F \\|Er(0)\\|_2e^t (1 + e^2(t-T_0)) \\quad (96)\n$$\n\n$$\n\\leq |\\langle r, e_i \\rangle|(1 - \\|r\\|^2_2) + C96\\|E(0)\\|^2_F \\|r(0)\\|_2e^{T_0}, \\quad (97)\n$$\n\nwhere the last inequality uses the fact that \\(e^t/(1 + e^2(t-T_0))\\) is maximized at \\(t = T_0\\). Plugging in the lower bound on \\(\\|r\\|^2_2\\) in Lemma 16,\n\n$$\n\\frac{d|\\langle r, e_i \\rangle|}{dt} \\leq |\\langle r, e_i \\rangle| \\|r(0)\\|^{-2}_2 - 1 \\|E(0)\\|^2_F T_0 + C96\\|E(0)\\|^2_F \\|r(0)\\|_2e^{T_0} \\quad (98)\n$$\n\n$$\n\\leq |\\langle r, e_i \\rangle| p + C98\\|E(0)\\|^2_F T_0. \\quad (98)\n$$\n\nwhere \\(C98 > 0\\) is a sufficiently large absolute constant, and \\(p = \\|r(0)\\|^{-2}_2 - 1\\). Multiplying both sides by \\(y = pe^{-2t} + 1\\) and noting that \\(\\frac{dy}{dt} = -p+e^{2t}\\), we get,\n\n$$\n\\int_{0}^{3T_0/2} dt (y|\\langle r, e_i \\rangle|) = C98\\|E(0)\\|^2_F T_0. \\quad (99)\n$$\n\n$$"}, {"type": "table", "rows": [["\\langle r(3T_0/2), e_i \\rangle", "pe^{-3T_0} + 1 -", "\\langle r(0), e_i \\rangle", "1 + p = C'_{98}\\", "E(0)\\"]], "md": "|\\langle r(3T_0/2), e_i \\rangle| pe^{-3T_0} + 1 - |\\langle r(0), e_i \\rangle| 1 + p = C'_{98}\\|E(0)\\|^2_F T_0^2 \\quad (100)", "isPerfectTable": true, "csv": "\"\\langle r(3T_0/2), e_i \\rangle\",\"pe^{-3T_0} + 1 -\",\"\\langle r(0), e_i \\rangle\",\"1 + p = C'_{98}\\\",\"E(0)\\\""}, {"type": "text", "value": "$$\n\n$$\n\\Rightarrow |\\langle r(3T_0/2), e_i \\rangle| \\leq C'_{98}\\|E(0)\\|^2_F T_0^2 + |\\langle r(0), e_i \\rangle| \\quad (101)\n$$", "md": "$$\n\n$$\n\\Rightarrow |\\langle r(3T_0/2), e_i \\rangle| \\leq C'_{98}\\|E(0)\\|^2_F T_0^2 + |\\langle r(0), e_i \\rangle| \\quad (101)\n$$"}]}, {"page": 23, "text": "By Lemma 10, at initialization, \u2225r(0)\u22252                  2 \u2272   \u03b1  \u221a  k and |\u27e8r(0), ei\u27e9| \u2265          \u03b1/k2. Therefore, by the small\ninitialization condition, C\u2032         98\u2225E(0)\u22252         0 \u2264    \u03b7|\u27e8r(0), ei\u27e9|/\u2225r(0)\u22252 and,\n                                                   F T 2\n                                           |\u27e8r(3T0/2), ei\u27e9| \u2264          (1 + \u03b7)|\u27e8r(0), ei\u27e9|.                                             (102)\n                                                                                    \u2225r(0)\u22252\nNext we show that from 3T0/2 to t, |\u27e8r(t), ei\u27e9| does not change significantly. By the same analysis\nas eq. (95),\n                           d|\u27e8r, ei\u27e9|    \u2264   |\u27e8r, ei\u27e9|(1 \u2212     \u2225r\u22252  2) +              2\u2225E(0)\u22252     F et                                (103)\n                                dt                                          1 + e2(1\u2212\u2225E(0)\u22252        F)(t\u2212T0)\u22121\nPlugging in the upper bound on 1 \u2212                  \u2225r(t)\u22252   2 from Lemma 17 and simplifying,\n    d|\u27e8r, ei\u27e9|    \u2272   |\u27e8r, ei\u27e9|e\u22123t/4  e3T0/4 + e11T0/4\u2225Er(0)\u22252                    2   +              2\u2225E(0)\u22252    F et                  (104)\n         dt                                                                                e2(1\u2212\u2225E(0)\u22252     F \u2225r(0)\u22252)(t\u2212T0) ,\n                  (i)                                                                                    F \u2225r(0)\u22252et\n                   \u2264   |\u27e8r, ei\u27e9|e\u22123t/4  e3T0/4 + e11T0/4\u2225Er(0)\u22252                   2    + 2\u2225E(0)\u22252 e7/4(t\u2212T0)                           (105)\n                  = |\u27e8r, ei\u27e9|e\u22123t/4  e3T0/4 + e11T0/4\u2225Er(0)\u22252                      2   + 2\u2225E(0)\u22252       F \u2225r(0)\u22252e\u22123t/4e7T0/4,          (106)\n                  =     |\u27e8r, ei\u27e9|    1 + e2T0\u2225Er(0)\u22252         2   + 2\u2225E(0)\u22252       F \u2225r(0)\u22252eT0         e\u22123(t\u2212T0)/4,                    (107)\nwhere (i) invokes the fact that \u2225E(0)\u22252                 F \u2264    1/8 from Lemma 10, by the bound on the initialization\nscale \u03b1 \u2264      c\u03b7/k3d log(kd). Simplifying the terms in eq. (107),                      (i)\n                              e2T0   \u2225Er(0)\u22252     2 \u2264   e2T0\u2225E(0)\u22252       2\u2225r(0)\u22252   2  \u2272   \u2225E(0)\u22252    2 \u2264    1,                        (108)\nwhere (i) is by Lemma 12 and the last equation follows from the scaling of \u03b1 and Lemma 10.\nLikewise, \u2225E(0)\u22252         F \u2225r(0)\u22252eT0 \u2272         \u2225E(0)\u22252     F , and plugging this and eq. (108) into eq. (107),\n                                     d|\u27e8r, ei\u27e9|    \u2272    |\u27e8r, ei\u27e9| + 2\u2225E(0)\u22252         F   e\u22123(t\u2212T0)/4                                    (109)\n                                          dt\nIntegrating both sides,\n                     log      |\u27e8r(t), ei\u27e9| + 2\u2225E(0)\u22252          F      \u2272       \u221e     e\u22123(t\u2212T0)/4dt \u2272           e\u22123T0/8.                  (110)\n                          |\u27e8r(3T0/2), ei\u27e9| + 2\u2225E(0)\u22252              F        3T0/2\nTherefore, for some absolute constant C111 > 0,\n                         |\u27e8r(t), ei\u27e9| \u2264  (i)   |\u27e8r(3T0/2), ei\u27e9| + 2\u2225E(0)\u22252             F    (1 + C111e\u22123T0/8)                           (111)\n                                          \u2264    |\u27e8r(3T0/2), ei\u27e9| + 2\u2225E(0)\u22252              F   (1 + \u221a\u03b7)                                    (112)\n                                         (ii)\n                  =\u21d2     \u2225U(t)ei\u22252        \u2264    \u2225E(0)\u2225F +          |\u27e8r(3T0/2), ei\u27e9| + 2\u2225E(0)\u22252              F   (1 + \u221a\u03b7),                (113)\nwhere (i) lower bounds T0 using Lemma 12 and uses the fact that at initialization \u2225r(0)\u22252 \u2273                                             \u03b1 \u221a  k\nand therefore when \u03b1 \u2264                c\u03b74/3 for a suffi       ciently small constant c > 0, C111e\u22123T0/8 \u2264                          \u221a\u03b7. On\nthe other hand, (ii) uses triangle inequality to bound \u2225U(t)ei\u22252 \u2264                                     |\u27e8r(t), ei\u27e9| + \u2225E(t)ei\u22252 \u2264\n|\u27e8r(t), ei\u27e9| + \u2225E(t)\u2225F \u2264             |\u27e8r(t), ei\u27e9| + \u2225E(0)\u2225F by Lemma 11. Using the bound on |\u27e8r(3T0/2), ei\u27e9|\nin eq. (102),\n                        \u2225U(t)ei\u22252 \u2264         \u2225E(0)\u2225F +          |\u27e8r(0), ei\u27e9|       + 2\u2225E(0)\u22252       F    (1 + \u221a\u03b7)                        (114)\n                                         \u2264   |\u27e8r(0), ei\u27e9|(1 + 4\u221a\u03b7).\u2225r(0)\u22252                                                              (115)\n                                               \u2225r(0)\u22252\nThe last inequality uses the fact that at initialization, |\u27e8r(0), ei\u27e9| \u2265        \u221a             \u03b1/k2 and \u2225r(0)\u22252 \u2272             \u03b1 \u221a  k. There-\nfore, |\u27e8r(0), ei\u27e9|/\u2225r(0)\u22252 \u2265              1/k5/2 and \u2225E(0)\u2225F \u2264                \u03b1    kd. Therefore, by the small initialization\ncondition on \u03b1, \u2225E(0)\u2225F , \u2225E(0)\u22252                F \u2264    \u03b7|\u27e8r(0), ei\u27e9|/\u2225r(0)\u22252.\n                                                                      23", "md": "# Math Equations\n\nBy Lemma 10, at initialization, $$\\|r(0)\\|_{2}^{2} \\lesssim \\alpha \\sqrt{k}$$ and $$|\\langle r(0), e_{i} \\rangle| \\geq \\frac{\\alpha}{k^{2}}$$. Therefore, by the small initialization condition, $$C' \\leq 98\\|E(0)\\|_{2}^{2} \\leq \\eta|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2}$$ and,\n\n$$| \\langle r(3T_{0}/2), e_{i} \\rangle| \\leq (1 + \\eta)|\\langle r(0), e_{i} \\rangle|$$.\n\nNext we show that from $$3T_{0}/2$$ to $$t$$, $$|\\langle r(t), e_{i} \\rangle|$$ does not change significantly. By the same analysis as eq. (95),\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\leq |\\langle r, e_{i} \\rangle|(1 - \\|r\\|_{2}^{2}) + \\frac{2\\|E(0)\\|_{2}^{2}F}{1 + e^{2}(1-\\|E(0)\\|_{2}F)(t-T_{0})^{-1}}$$\n\nPlugging in the upper bound on $$1 - \\|r(t)\\|_{2}^{2}$$ from Lemma 17 and simplifying,\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\lesssim |\\langle r, e_{i} \\rangle|e^{-3t/4}e^{3T_{0}/4} + e^{11T_{0}/4}\\|E(0)\\|_{2}^{2} + \\frac{2\\|E(0)\\|_{2}F}{e^{2}(1-\\|E(0)\\|_{2}F\\|r(0)\\|_{2})(t-T_{0})}$$\n\nwhere (i) invokes the fact that $$\\|E(0)\\|_{2}^{2}F \\leq 1/8$$ from Lemma 10, by the bound on the initialization scale $$\\alpha \\leq c\\eta/k^{3}d \\log(kd)$$. Simplifying the terms in eq. (107),\n\n$$e^{2T_{0}}\\|Er(0)\\|_{2}^{2} \\leq e^{2T_{0}}\\|E(0)\\|_{2}^{2}\\|r(0)\\|_{2}^{2} \\lesssim \\|E(0)\\|_{2}^{2} \\leq 1$$\n\nwhere (i) is by Lemma 12 and the last equation follows from the scaling of $$\\alpha$$ and Lemma 10.\n\nLikewise, $$\\|E(0)\\|_{2}^{2}F\\|r(0)\\|_{2}e^{T_{0}} \\lesssim \\|E(0)\\|_{2}^{2}F$$, and plugging this and eq. (108) into eq. (107),\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\lesssim |\\langle r, e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F e^{-3(t-T_{0})/4}$$\n\nIntegrating both sides,\n\n$$\\log|\\langle r(t), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F \\lesssim \\int_{0}^{t} e^{-3(t-T_{0})/4}dt \\lesssim e^{-3T_{0}/8}$$\n\nTherefore, for some absolute constant $$C_{111} > 0$$,\n\n$$|\\langle r(t), e_{i} \\rangle| \\leq (i)|\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + C_{111}e^{-3T_{0}/8})$$\n\n$$\\leq |\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + \\sqrt{\\eta})$$\n\n$$\\Rightarrow \\|U(t)e_{i}\\|_{2} \\leq \\|E(0)\\|_{F} + |\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + \\sqrt{\\eta})$$\n\nwhere (i) lower bounds $$T_{0}$$ using Lemma 12 and uses the fact that at initialization $$\\|r(0)\\|_{2} \\gtrsim \\alpha \\sqrt{k}$$ and therefore when $$\\alpha \\leq c\\eta^{4/3}$$ for a sufficiently small constant $$c > 0$$, $$C_{111}e^{-3T_{0}/8} \\leq \\sqrt{\\eta}$$. On the other hand, (ii) uses triangle inequality to bound $$\\|U(t)e_{i}\\|_{2} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(t)e_{i}\\|_{2} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(t)\\|_{F} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(0)\\|_{F}$$ by Lemma 11. Using the bound on $$|\\langle r(3T_{0}/2), e_{i} \\rangle|$$ in eq. (102),\n\n$$\\|U(t)e_{i}\\|_{2} \\leq \\|E(0)\\|_{F} + |\\langle r(0), e_{i} \\rangle| + 2\\|E(0)\\|_{2}F(1 + \\sqrt{\\eta})$$\n\n$$\\leq |\\langle r(0), e_{i} \\rangle|(1 + 4\\sqrt{\\eta}).\\|r(0)\\|_{2}/\\|r(0)\\|_{2}$$\n\nThe last inequality uses the fact that at initialization, $$|\\langle r(0), e_{i} \\rangle| \\geq \\sqrt{\\alpha/k^{2}}$$ and $$\\|r(0)\\|_{2} \\lesssim \\alpha \\sqrt{k}$$. Therefore, $$|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2} \\geq 1/k^{5/2}$$ and $$\\|E(0)\\|_{F} \\leq \\alpha kd$$. Therefore, by the small initialization condition on $$\\alpha$$, $$\\|E(0)\\|_{F}$$, $$\\|E(0)\\|_{2}F \\leq \\eta|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2}$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "By Lemma 10, at initialization, $$\\|r(0)\\|_{2}^{2} \\lesssim \\alpha \\sqrt{k}$$ and $$|\\langle r(0), e_{i} \\rangle| \\geq \\frac{\\alpha}{k^{2}}$$. Therefore, by the small initialization condition, $$C' \\leq 98\\|E(0)\\|_{2}^{2} \\leq \\eta|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2}$$ and,\n\n$$| \\langle r(3T_{0}/2), e_{i} \\rangle| \\leq (1 + \\eta)|\\langle r(0), e_{i} \\rangle|$$.\n\nNext we show that from $$3T_{0}/2$$ to $$t$$, $$|\\langle r(t), e_{i} \\rangle|$$ does not change significantly. By the same analysis as eq. (95),\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\leq |\\langle r, e_{i} \\rangle|(1 - \\|r\\|_{2}^{2}) + \\frac{2\\|E(0)\\|_{2}^{2}F}{1 + e^{2}(1-\\|E(0)\\|_{2}F)(t-T_{0})^{-1}}$$\n\nPlugging in the upper bound on $$1 - \\|r(t)\\|_{2}^{2}$$ from Lemma 17 and simplifying,\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\lesssim |\\langle r, e_{i} \\rangle|e^{-3t/4}e^{3T_{0}/4} + e^{11T_{0}/4}\\|E(0)\\|_{2}^{2} + \\frac{2\\|E(0)\\|_{2}F}{e^{2}(1-\\|E(0)\\|_{2}F\\|r(0)\\|_{2})(t-T_{0})}$$\n\nwhere (i) invokes the fact that $$\\|E(0)\\|_{2}^{2}F \\leq 1/8$$ from Lemma 10, by the bound on the initialization scale $$\\alpha \\leq c\\eta/k^{3}d \\log(kd)$$. Simplifying the terms in eq. (107),\n\n$$e^{2T_{0}}\\|Er(0)\\|_{2}^{2} \\leq e^{2T_{0}}\\|E(0)\\|_{2}^{2}\\|r(0)\\|_{2}^{2} \\lesssim \\|E(0)\\|_{2}^{2} \\leq 1$$\n\nwhere (i) is by Lemma 12 and the last equation follows from the scaling of $$\\alpha$$ and Lemma 10.\n\nLikewise, $$\\|E(0)\\|_{2}^{2}F\\|r(0)\\|_{2}e^{T_{0}} \\lesssim \\|E(0)\\|_{2}^{2}F$$, and plugging this and eq. (108) into eq. (107),\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\lesssim |\\langle r, e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F e^{-3(t-T_{0})/4}$$\n\nIntegrating both sides,\n\n$$\\log|\\langle r(t), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F \\lesssim \\int_{0}^{t} e^{-3(t-T_{0})/4}dt \\lesssim e^{-3T_{0}/8}$$\n\nTherefore, for some absolute constant $$C_{111} > 0$$,\n\n$$|\\langle r(t), e_{i} \\rangle| \\leq (i)|\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + C_{111}e^{-3T_{0}/8})$$\n\n$$\\leq |\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + \\sqrt{\\eta})$$\n\n$$\\Rightarrow \\|U(t)e_{i}\\|_{2} \\leq \\|E(0)\\|_{F} + |\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + \\sqrt{\\eta})$$\n\nwhere (i) lower bounds $$T_{0}$$ using Lemma 12 and uses the fact that at initialization $$\\|r(0)\\|_{2} \\gtrsim \\alpha \\sqrt{k}$$ and therefore when $$\\alpha \\leq c\\eta^{4/3}$$ for a sufficiently small constant $$c > 0$$, $$C_{111}e^{-3T_{0}/8} \\leq \\sqrt{\\eta}$$. On the other hand, (ii) uses triangle inequality to bound $$\\|U(t)e_{i}\\|_{2} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(t)e_{i}\\|_{2} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(t)\\|_{F} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(0)\\|_{F}$$ by Lemma 11. Using the bound on $$|\\langle r(3T_{0}/2), e_{i} \\rangle|$$ in eq. (102),\n\n$$\\|U(t)e_{i}\\|_{2} \\leq \\|E(0)\\|_{F} + |\\langle r(0), e_{i} \\rangle| + 2\\|E(0)\\|_{2}F(1 + \\sqrt{\\eta})$$\n\n$$\\leq |\\langle r(0), e_{i} \\rangle|(1 + 4\\sqrt{\\eta}).\\|r(0)\\|_{2}/\\|r(0)\\|_{2}$$\n\nThe last inequality uses the fact that at initialization, $$|\\langle r(0), e_{i} \\rangle| \\geq \\sqrt{\\alpha/k^{2}}$$ and $$\\|r(0)\\|_{2} \\lesssim \\alpha \\sqrt{k}$$. Therefore, $$|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2} \\geq 1/k^{5/2}$$ and $$\\|E(0)\\|_{F} \\leq \\alpha kd$$. Therefore, by the small initialization condition on $$\\alpha$$, $$\\|E(0)\\|_{F}$$, $$\\|E(0)\\|_{2}F \\leq \\eta|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2}$$.", "md": "By Lemma 10, at initialization, $$\\|r(0)\\|_{2}^{2} \\lesssim \\alpha \\sqrt{k}$$ and $$|\\langle r(0), e_{i} \\rangle| \\geq \\frac{\\alpha}{k^{2}}$$. Therefore, by the small initialization condition, $$C' \\leq 98\\|E(0)\\|_{2}^{2} \\leq \\eta|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2}$$ and,\n\n$$| \\langle r(3T_{0}/2), e_{i} \\rangle| \\leq (1 + \\eta)|\\langle r(0), e_{i} \\rangle|$$.\n\nNext we show that from $$3T_{0}/2$$ to $$t$$, $$|\\langle r(t), e_{i} \\rangle|$$ does not change significantly. By the same analysis as eq. (95),\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\leq |\\langle r, e_{i} \\rangle|(1 - \\|r\\|_{2}^{2}) + \\frac{2\\|E(0)\\|_{2}^{2}F}{1 + e^{2}(1-\\|E(0)\\|_{2}F)(t-T_{0})^{-1}}$$\n\nPlugging in the upper bound on $$1 - \\|r(t)\\|_{2}^{2}$$ from Lemma 17 and simplifying,\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\lesssim |\\langle r, e_{i} \\rangle|e^{-3t/4}e^{3T_{0}/4} + e^{11T_{0}/4}\\|E(0)\\|_{2}^{2} + \\frac{2\\|E(0)\\|_{2}F}{e^{2}(1-\\|E(0)\\|_{2}F\\|r(0)\\|_{2})(t-T_{0})}$$\n\nwhere (i) invokes the fact that $$\\|E(0)\\|_{2}^{2}F \\leq 1/8$$ from Lemma 10, by the bound on the initialization scale $$\\alpha \\leq c\\eta/k^{3}d \\log(kd)$$. Simplifying the terms in eq. (107),\n\n$$e^{2T_{0}}\\|Er(0)\\|_{2}^{2} \\leq e^{2T_{0}}\\|E(0)\\|_{2}^{2}\\|r(0)\\|_{2}^{2} \\lesssim \\|E(0)\\|_{2}^{2} \\leq 1$$\n\nwhere (i) is by Lemma 12 and the last equation follows from the scaling of $$\\alpha$$ and Lemma 10.\n\nLikewise, $$\\|E(0)\\|_{2}^{2}F\\|r(0)\\|_{2}e^{T_{0}} \\lesssim \\|E(0)\\|_{2}^{2}F$$, and plugging this and eq. (108) into eq. (107),\n\n$$\\frac{d|\\langle r, e_{i} \\rangle|}{dt} \\lesssim |\\langle r, e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F e^{-3(t-T_{0})/4}$$\n\nIntegrating both sides,\n\n$$\\log|\\langle r(t), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F \\lesssim \\int_{0}^{t} e^{-3(t-T_{0})/4}dt \\lesssim e^{-3T_{0}/8}$$\n\nTherefore, for some absolute constant $$C_{111} > 0$$,\n\n$$|\\langle r(t), e_{i} \\rangle| \\leq (i)|\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + C_{111}e^{-3T_{0}/8})$$\n\n$$\\leq |\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + \\sqrt{\\eta})$$\n\n$$\\Rightarrow \\|U(t)e_{i}\\|_{2} \\leq \\|E(0)\\|_{F} + |\\langle r(3T_{0}/2), e_{i} \\rangle| + 2\\|E(0)\\|_{2}^{2}F(1 + \\sqrt{\\eta})$$\n\nwhere (i) lower bounds $$T_{0}$$ using Lemma 12 and uses the fact that at initialization $$\\|r(0)\\|_{2} \\gtrsim \\alpha \\sqrt{k}$$ and therefore when $$\\alpha \\leq c\\eta^{4/3}$$ for a sufficiently small constant $$c > 0$$, $$C_{111}e^{-3T_{0}/8} \\leq \\sqrt{\\eta}$$. On the other hand, (ii) uses triangle inequality to bound $$\\|U(t)e_{i}\\|_{2} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(t)e_{i}\\|_{2} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(t)\\|_{F} \\leq |\\langle r(t), e_{i} \\rangle| + \\|E(0)\\|_{F}$$ by Lemma 11. Using the bound on $$|\\langle r(3T_{0}/2), e_{i} \\rangle|$$ in eq. (102),\n\n$$\\|U(t)e_{i}\\|_{2} \\leq \\|E(0)\\|_{F} + |\\langle r(0), e_{i} \\rangle| + 2\\|E(0)\\|_{2}F(1 + \\sqrt{\\eta})$$\n\n$$\\leq |\\langle r(0), e_{i} \\rangle|(1 + 4\\sqrt{\\eta}).\\|r(0)\\|_{2}/\\|r(0)\\|_{2}$$\n\nThe last inequality uses the fact that at initialization, $$|\\langle r(0), e_{i} \\rangle| \\geq \\sqrt{\\alpha/k^{2}}$$ and $$\\|r(0)\\|_{2} \\lesssim \\alpha \\sqrt{k}$$. Therefore, $$|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2} \\geq 1/k^{5/2}$$ and $$\\|E(0)\\|_{F} \\leq \\alpha kd$$. Therefore, by the small initialization condition on $$\\alpha$$, $$\\|E(0)\\|_{F}$$, $$\\|E(0)\\|_{2}F \\leq \\eta|\\langle r(0), e_{i} \\rangle|/\\|r(0)\\|_{2}$$."}]}, {"page": 24, "text": "Combining the statements of Lemma 18 and Lemma 19 shows that, with probability \u2265                                                                                      1 \u2212     O(1/k)\nover the initialization, at any time t \u2265                                 4T0,\n                                     |\u27e8r(0), ei\u27e9|          (1 \u2212      5\u03b7) \u2264        \u2225U    (t)ei\u22252 \u2264           |\u27e8r(0), ei\u27e9|          (1 + 4\u221a\u03b7).                                       (116)\nThis completes the proof of Lemma 9.    \u2225r(0)\u22252                                                                \u2225r(0)\u22252\nA.3         Behavior at initialization: many columns are \u201cactive\u201d\nLemma 9 establishes the limiting behavior of gradient fl                                                 ow as a function of the initialization, specifi-\ncally that the norm of a column grows to a value proportional to the alignment of the column with U\u22c6\nat initialization, |\u27e8r(0), ei\u27e9|. In this section, we show that at initialization, \u27e8r(0), ei\u27e9                                                                  is significant for\nmany i \u2208           [d] compared to the maximum among them.\nAt time t = 0, each coordinate of r(0) is distributed as the inner product of a Gaussian vector\n\u223c    N    (0, \u03b12I) with a fixed vector U\u22c6. By Gaussian concentration, we therefore expect each coordinate\nof r(0) to concentrate around \u03b1, and no one coordinate of r(0) to be significantly larger than the\nothers. The next lemma makes this claim precise.\nLemma 20. Many columns are \u201cactive\" at initialization For any \u03b7 > 0, let Smax(\u03b7) denote the set\n   i \u2208     [k] :    max     |\u27e8r(0),ei\u27e9|                                      . For any \u03b7 \u2264               1 \u2212         1\n                           j\u2208[k] |\u27e8r(0),ej\u27e9| \u2265             1 \u2212      \u221a\u03b7                                           log(k)     ,\n                                        P      |Smax(\u03b7)| \u2264                               k\u03b7                         \u2264     c117     log(k)          .                               (117)\n                                                                         2      2(1 \u2212        \u03b7) log(k)                              k\u03b7\nfor a sufficiently small absolute constant c117 > 0. In other words, with high probability,                                                                                 \u2126(k\u03b7) of\nthe column indices i \u2208                      [k] are significantly correlated with U\u22c6                                  compared to the maximum among all\ncolumns.\n                                                                                                                                                                   i.i.d.\nProof. By rotation invariance of Gaussians and since \u2225U\u22c6\u22252 = 1, for each i, \u27e8r(0), ei\u27e9                                                                              \u223c     N    (0, \u03b12).\nBy Gaussian anti-concentration, for each i \u2208                                          [k], from [49, Proposition 2.1.2], for t > 0,\n                                        P (|\u27e8r(0), ei\u27e9| \u2265                t\u03b1) \u2265            1                 \u00b7  \u221a  1                                                                (118)\n                                                                                          t \u2212       1\n                                                                                                   t3              2\u03c0 e\u2212t2/2 \u225c             p(t).\nTherefore roughly we expect kp(t) of the columns i \u2208                                                  [k] to satisfy the condition {|\u27e8r(0), ei\u27e9| \u2265                                   t\u03b1}.\nIn particular, by the independence across i, the number of successes follows a binomial distribution\nwith number of trials k and probability of success p(t). Denote S(t) = {i \u2208                                                                      [k] : |\u27e8r(0), ei\u27e9| \u2265                t\u03b1}.\nBy binomial concentration [49],\n                                                               P       |S(t)| \u2264          kp(t)                        4 .                                                          (119)\nOn the other hand, for i.i.d standard normal random variables, X1, \u00b7 \u00b7 \u00b7 , Xk \u223c              2           \u2264    e\u2212   kp(t)                         N    (0, 1), the supremum\nsatisfies the following concentration inequality\n                                                     P       max                         2 log(k) + \u03b7                 \u2264    e\u2212\u03b72/2                                                  (120)\nTherefore,                     P      max                   1\u2264i\u2264k Xi \u2265             2 log(k) + \u03b1                 2\u03b7 log(k)              \u2265    1 \u2212     k\u2212\u03b7.                           (121)\nFor each t > 0, define,                i\u2208[k] |\u27e8r(0), ei\u27e9| \u2264                \u03b1\n                           Smax(t) =                 i \u2208    [k] :     max      |\u27e8r(0), ei\u27e9|                                            t                        ,                  (122)\nand combining eqs. (118), (119) and (121) results in the inequality,           i\u2208[k] |\u27e8r(0), ei\u27e9| \u2265                      2 log(k)(1 + \u221a\u03b7)\n                                    P      |Smax(t)| \u2264              kp(t)                        4      + k\u22121/8 \u2264                  4\n                                                                        2           \u2264    e\u2212   kp(t)                            kp(t) + k\u2212\u03b7.                                        (123)\n                                                                                            24", "md": "Combining the statements of Lemma 18 and Lemma 19 shows that, with probability $$\\geq 1 - O\\left(\\frac{1}{k}\\right)$$ over the initialization, at any time $$t \\geq 4T_0$$,\n\n$$\n| \\langle r(0), e_i \\rangle | (1 - 5\\eta) \\leq \\| U(t) e_i \\|_2 \\leq | \\langle r(0), e_i \\rangle | (1 + 4\\sqrt{\\eta}). \\quad (116)\n$$\n\nThis completes the proof of Lemma 9.\n\n### Behavior at initialization: many columns are \u201cactive\u201d\n\nLemma 9 establishes the limiting behavior of gradient flow as a function of the initialization, specifically that the norm of a column grows to a value proportional to the alignment of the column with $$U^*$$ at initialization, $$| \\langle r(0), e_i \\rangle$$. In this section, we show that at initialization, $$\\langle r(0), e_i \\rangle$$ is significant for many $$i \\in [d]$$ compared to the maximum among them.\n\nAt time $$t = 0$$, each coordinate of $$r(0)$$ is distributed as the inner product of a Gaussian vector $$\\sim \\mathcal{N}(0, \\alpha^2 I)$$ with a fixed vector $$U^*$$. By Gaussian concentration, we therefore expect each coordinate of $$r(0)$$ to concentrate around $$\\alpha$$, and no one coordinate of $$r(0)$$ to be significantly larger than the others. The next lemma makes this claim precise.\n\n**Lemma 20.** Many columns are \u201cactive\u201d at initialization. For any $$\\eta > 0$$, let $$S_{\\text{max}}(\\eta)$$ denote the set\n\n$$\n\\{ i \\in [k] : \\max_{j \\in [k]} | \\langle r(0), e_j \\rangle | \\geq 1 - \\sqrt{\\eta} \\log(k) \\}.\n$$\n\nFor any $$\\eta \\leq 1 - \\frac{1}{2(1 - \\eta) \\log(k)}$$, we have\n\n$$\n\\mathbb{P} \\left( |S_{\\text{max}}(\\eta)| \\leq k\\eta \\right) \\leq c_{117} \\log(k). \\quad (117)\n$$\n\nfor a sufficiently small absolute constant $$c_{117} > 0$$. In other words, with high probability, $$\\Omega(k\\eta)$$ of the column indices $$i \\in [k]$$ are significantly correlated with $$U^*$$ compared to the maximum among all i.i.d.\n\n**Proof.** By rotation invariance of Gaussians and since $$\\|U^*\\|_2 = 1$$, for each $$i$$, $$\\langle r(0), e_i \\rangle \\sim \\mathcal{N}(0, \\alpha^2)$$. By Gaussian anti-concentration, for each $$i \\in [k]$$, from [49, Proposition 2.1.2], for $$t > 0$$,\n\n$$\n\\mathbb{P} \\left( | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\right) \\geq 1 - \\frac{t - 1}{t^3} \\sqrt{\\frac{2}{\\pi}} e^{-t^2/2} \\triangleq p(t).\n$$\n\nTherefore, roughly we expect $$kp(t)$$ of the columns $$i \\in [k]$$ to satisfy the condition $$\\{ | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\}$$. In particular, by the independence across $$i$$, the number of successes follows a binomial distribution with number of trials $$k$$ and probability of success $$p(t)$$. Denote $$S(t) = \\{ i \\in [k] : | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\}$$. By binomial concentration [49],\n\n$$\n\\mathbb{P} \\left( |S(t)| \\leq kp(t) \\right) \\leq 4. \\quad (119)\n$$\n\nOn the other hand, for i.i.d standard normal random variables, $$X_1, \\ldots, X_k \\sim \\mathcal{N}(0, 1)$$, the supremum satisfies the following concentration inequality\n\n$$\n\\mathbb{P} \\left( \\max_{1 \\leq i \\leq k} X_i \\geq 2 \\log(k) + \\eta \\right) \\leq e^{-\\eta^2/2}. \\quad (120)\n$$\n\nTherefore,\n\n$$\n\\mathbb{P} \\left( \\max_{1 \\leq i \\leq k} X_i \\geq 2 \\log(k) + \\alpha \\right) \\geq 1 - k^{-\\eta}. \\quad (121)\n$$\n\nFor each $$t > 0$$, define\n\n$$\nS_{\\text{max}}(t) = \\{ i \\in [k] : \\max_{j} | \\langle r(0), e_j \\rangle | \\geq t \\}, \\quad (122)\n$$\n\nand combining equations (118), (119), and (121) results in the inequality\n\n$$\n\\{ i \\in [k] : | \\langle r(0), e_i \\rangle | \\geq 2 \\log(k)(1 + \\sqrt{\\eta}) \\} \\leq \\mathbb{P} \\left( |S_{\\text{max}}(t)| \\leq kp(t) \\right) + k^{-1/8} \\leq 4. \\quad (123)\n$$", "images": [], "items": [{"type": "text", "value": "Combining the statements of Lemma 18 and Lemma 19 shows that, with probability $$\\geq 1 - O\\left(\\frac{1}{k}\\right)$$ over the initialization, at any time $$t \\geq 4T_0$$,\n\n$$", "md": "Combining the statements of Lemma 18 and Lemma 19 shows that, with probability $$\\geq 1 - O\\left(\\frac{1}{k}\\right)$$ over the initialization, at any time $$t \\geq 4T_0$$,\n\n$$"}, {"type": "table", "rows": [["\\langle r(0), e_i \\rangle", "(1 - 5\\eta) \\leq \\", "U(t) e_i \\", "_2 \\leq", "\\langle r(0), e_i \\rangle"]], "md": "| \\langle r(0), e_i \\rangle | (1 - 5\\eta) \\leq \\| U(t) e_i \\|_2 \\leq | \\langle r(0), e_i \\rangle | (1 + 4\\sqrt{\\eta}). \\quad (116)", "isPerfectTable": true, "csv": "\"\\langle r(0), e_i \\rangle\",\"(1 - 5\\eta) \\leq \\\",\"U(t) e_i \\\",\"_2 \\leq\",\"\\langle r(0), e_i \\rangle\""}, {"type": "text", "value": "$$\n\nThis completes the proof of Lemma 9.", "md": "$$\n\nThis completes the proof of Lemma 9."}, {"type": "heading", "lvl": 3, "value": "Behavior at initialization: many columns are \u201cactive\u201d", "md": "### Behavior at initialization: many columns are \u201cactive\u201d"}, {"type": "text", "value": "Lemma 9 establishes the limiting behavior of gradient flow as a function of the initialization, specifically that the norm of a column grows to a value proportional to the alignment of the column with $$U^*$$ at initialization, $$| \\langle r(0), e_i \\rangle$$. In this section, we show that at initialization, $$\\langle r(0), e_i \\rangle$$ is significant for many $$i \\in [d]$$ compared to the maximum among them.\n\nAt time $$t = 0$$, each coordinate of $$r(0)$$ is distributed as the inner product of a Gaussian vector $$\\sim \\mathcal{N}(0, \\alpha^2 I)$$ with a fixed vector $$U^*$$. By Gaussian concentration, we therefore expect each coordinate of $$r(0)$$ to concentrate around $$\\alpha$$, and no one coordinate of $$r(0)$$ to be significantly larger than the others. The next lemma makes this claim precise.\n\n**Lemma 20.** Many columns are \u201cactive\u201d at initialization. For any $$\\eta > 0$$, let $$S_{\\text{max}}(\\eta)$$ denote the set\n\n$$\n\\{ i \\in [k] : \\max_{j \\in [k]} | \\langle r(0), e_j \\rangle | \\geq 1 - \\sqrt{\\eta} \\log(k) \\}.\n$$\n\nFor any $$\\eta \\leq 1 - \\frac{1}{2(1 - \\eta) \\log(k)}$$, we have\n\n$$\n\\mathbb{P} \\left( |S_{\\text{max}}(\\eta)| \\leq k\\eta \\right) \\leq c_{117} \\log(k). \\quad (117)\n$$\n\nfor a sufficiently small absolute constant $$c_{117} > 0$$. In other words, with high probability, $$\\Omega(k\\eta)$$ of the column indices $$i \\in [k]$$ are significantly correlated with $$U^*$$ compared to the maximum among all i.i.d.\n\n**Proof.** By rotation invariance of Gaussians and since $$\\|U^*\\|_2 = 1$$, for each $$i$$, $$\\langle r(0), e_i \\rangle \\sim \\mathcal{N}(0, \\alpha^2)$$. By Gaussian anti-concentration, for each $$i \\in [k]$$, from [49, Proposition 2.1.2], for $$t > 0$$,\n\n$$\n\\mathbb{P} \\left( | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\right) \\geq 1 - \\frac{t - 1}{t^3} \\sqrt{\\frac{2}{\\pi}} e^{-t^2/2} \\triangleq p(t).\n$$\n\nTherefore, roughly we expect $$kp(t)$$ of the columns $$i \\in [k]$$ to satisfy the condition $$\\{ | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\}$$. In particular, by the independence across $$i$$, the number of successes follows a binomial distribution with number of trials $$k$$ and probability of success $$p(t)$$. Denote $$S(t) = \\{ i \\in [k] : | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\}$$. By binomial concentration [49],\n\n$$\n\\mathbb{P} \\left( |S(t)| \\leq kp(t) \\right) \\leq 4. \\quad (119)\n$$\n\nOn the other hand, for i.i.d standard normal random variables, $$X_1, \\ldots, X_k \\sim \\mathcal{N}(0, 1)$$, the supremum satisfies the following concentration inequality\n\n$$\n\\mathbb{P} \\left( \\max_{1 \\leq i \\leq k} X_i \\geq 2 \\log(k) + \\eta \\right) \\leq e^{-\\eta^2/2}. \\quad (120)\n$$\n\nTherefore,\n\n$$\n\\mathbb{P} \\left( \\max_{1 \\leq i \\leq k} X_i \\geq 2 \\log(k) + \\alpha \\right) \\geq 1 - k^{-\\eta}. \\quad (121)\n$$\n\nFor each $$t > 0$$, define\n\n$$\nS_{\\text{max}}(t) = \\{ i \\in [k] : \\max_{j} | \\langle r(0), e_j \\rangle | \\geq t \\}, \\quad (122)\n$$\n\nand combining equations (118), (119), and (121) results in the inequality\n\n$$\n\\{ i \\in [k] : | \\langle r(0), e_i \\rangle | \\geq 2 \\log(k)(1 + \\sqrt{\\eta}) \\} \\leq \\mathbb{P} \\left( |S_{\\text{max}}(t)| \\leq kp(t) \\right) + k^{-1/8} \\leq 4. \\quad (123)\n$$", "md": "Lemma 9 establishes the limiting behavior of gradient flow as a function of the initialization, specifically that the norm of a column grows to a value proportional to the alignment of the column with $$U^*$$ at initialization, $$| \\langle r(0), e_i \\rangle$$. In this section, we show that at initialization, $$\\langle r(0), e_i \\rangle$$ is significant for many $$i \\in [d]$$ compared to the maximum among them.\n\nAt time $$t = 0$$, each coordinate of $$r(0)$$ is distributed as the inner product of a Gaussian vector $$\\sim \\mathcal{N}(0, \\alpha^2 I)$$ with a fixed vector $$U^*$$. By Gaussian concentration, we therefore expect each coordinate of $$r(0)$$ to concentrate around $$\\alpha$$, and no one coordinate of $$r(0)$$ to be significantly larger than the others. The next lemma makes this claim precise.\n\n**Lemma 20.** Many columns are \u201cactive\u201d at initialization. For any $$\\eta > 0$$, let $$S_{\\text{max}}(\\eta)$$ denote the set\n\n$$\n\\{ i \\in [k] : \\max_{j \\in [k]} | \\langle r(0), e_j \\rangle | \\geq 1 - \\sqrt{\\eta} \\log(k) \\}.\n$$\n\nFor any $$\\eta \\leq 1 - \\frac{1}{2(1 - \\eta) \\log(k)}$$, we have\n\n$$\n\\mathbb{P} \\left( |S_{\\text{max}}(\\eta)| \\leq k\\eta \\right) \\leq c_{117} \\log(k). \\quad (117)\n$$\n\nfor a sufficiently small absolute constant $$c_{117} > 0$$. In other words, with high probability, $$\\Omega(k\\eta)$$ of the column indices $$i \\in [k]$$ are significantly correlated with $$U^*$$ compared to the maximum among all i.i.d.\n\n**Proof.** By rotation invariance of Gaussians and since $$\\|U^*\\|_2 = 1$$, for each $$i$$, $$\\langle r(0), e_i \\rangle \\sim \\mathcal{N}(0, \\alpha^2)$$. By Gaussian anti-concentration, for each $$i \\in [k]$$, from [49, Proposition 2.1.2], for $$t > 0$$,\n\n$$\n\\mathbb{P} \\left( | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\right) \\geq 1 - \\frac{t - 1}{t^3} \\sqrt{\\frac{2}{\\pi}} e^{-t^2/2} \\triangleq p(t).\n$$\n\nTherefore, roughly we expect $$kp(t)$$ of the columns $$i \\in [k]$$ to satisfy the condition $$\\{ | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\}$$. In particular, by the independence across $$i$$, the number of successes follows a binomial distribution with number of trials $$k$$ and probability of success $$p(t)$$. Denote $$S(t) = \\{ i \\in [k] : | \\langle r(0), e_i \\rangle | \\geq t\\alpha \\}$$. By binomial concentration [49],\n\n$$\n\\mathbb{P} \\left( |S(t)| \\leq kp(t) \\right) \\leq 4. \\quad (119)\n$$\n\nOn the other hand, for i.i.d standard normal random variables, $$X_1, \\ldots, X_k \\sim \\mathcal{N}(0, 1)$$, the supremum satisfies the following concentration inequality\n\n$$\n\\mathbb{P} \\left( \\max_{1 \\leq i \\leq k} X_i \\geq 2 \\log(k) + \\eta \\right) \\leq e^{-\\eta^2/2}. \\quad (120)\n$$\n\nTherefore,\n\n$$\n\\mathbb{P} \\left( \\max_{1 \\leq i \\leq k} X_i \\geq 2 \\log(k) + \\alpha \\right) \\geq 1 - k^{-\\eta}. \\quad (121)\n$$\n\nFor each $$t > 0$$, define\n\n$$\nS_{\\text{max}}(t) = \\{ i \\in [k] : \\max_{j} | \\langle r(0), e_j \\rangle | \\geq t \\}, \\quad (122)\n$$\n\nand combining equations (118), (119), and (121) results in the inequality\n\n$$\n\\{ i \\in [k] : | \\langle r(0), e_i \\rangle | \\geq 2 \\log(k)(1 + \\sqrt{\\eta}) \\} \\leq \\mathbb{P} \\left( |S_{\\text{max}}(t)| \\leq kp(t) \\right) + k^{-1/8} \\leq 4. \\quad (123)\n$$"}]}, {"page": 25, "text": "                                                                               1\nWith the choice of t =          2(1 \u2212  \u03b7) log(k) and for any \u03b7 \u2264       1 \u2212   log(k), we obtain that p(t) \u2265\n k\u2212(1\u2212\u03b7)/4     (1 \u2212  \u03b7) log(k). Plugging into eq. (123) and using the results in the bound,\n      P   |Smax(     2(1 \u2212  \u03b7) log(k))| \u2264            k\u03b7             \u2264  1 + 16     2(1 \u2212  \u03b7) log(k)       (124)\n                                            4    (1 \u2212 \u03b7) log(k)            \u221a 1\u2212\u03b7    k\u03b7\n Using the definition of Smax(       2(1 \u2212  \u03b7) log(k)) and the fact that   1+\u221a\u03b7 \u2265     1 \u2212 \u221a\u03b7 completes the\n proof.\n A.4   Putting it all together: Proof of Theorem 1\n Below we state and prove a stronger version of Theorem 1 which follows by combining Lemmas 9\n and 20.\nTheorem 21. Consider the population loss defined in (2), for the case that r = 1 and k \u226b                     1.\nMoreover, assume that the entries of the initial model U0 are i.i.d. samples from N(0, \u03b12), where\n \u03b1 \u2264  \u03b72/k3d log(kd). Then, for any constant \u03b7 \u2208        (0, 1), the iterates generated by gradient descent\n converge to a model Ugd where      \u2126(k\u03b7) columns of which are active and satisfy,\n                                          \u2225Ugdei\u22252         \u2265  1 \u2212  C4 \u221a\u03b7,                                (125)\n                                     max  j\u2208[k] \u2225Ugdej\u22252\n with probability \u2265   1 \u2212  poly(1/d, 1/k\u03b7), where C4 > 0 is a sufficiently large constant.\n In particular, if for some i,\n                                           |\u27e8r(0), ei\u27e9|                                                  (126)\nThen, by Lemma 9,                     max  j\u2208[k] |\u27e8r(0), ej\u27e9| \u2265  1 \u2212  \u221a\u03b7\n                                 lim       \u2225U(t)ei\u22252          \u2265  1 \u2212 C127  \u221a\u03b7.                           (127)\n                                t\u2192\u221e   maxj\u2208[k] \u2225U(t)ej\u22252\nwhere C127 > 0 is a suffi       ciently large constant. Invoking Lemma 20 completes the proof of\nTheorem 21.\n B    Population analysis of the regularized loss: Proof of Theorem 3\n In this section, we study the second-order stationary points of loss fpop(U) = Lpop(U) + \u03bbR\u03b2(U)\n((9)) which is the regularized loss in the population (infinite sample) setting. The main result we\n prove in this section is about approximate second-order stationary points of the regularized loss\n fpop(U). When \u03bb and \u03b2 are chosen appropriately, we show that such points (i) are \u201cpruning friendly\u201d\n in that greedily pruning the columns of U based on their L2 norm results in a solution Uprune having\n exactly r columns, and (ii) the resulting solution Uprune satisfies \u2225UpruneU T     prune\u22252        r)2 where\n                                                                                          F \u2264  c(\u03c3\u22c6\n c > 0 is a small constant, and serves essentially as a \u201cspectral initialization\u201d in the sense of [23] for\n the subsequent fine-tuning phase.\nThe proof of Theorem 3 relies on two main observations: (i) showing that at approximate second-\n order stationary points UU T \u2248     U\u22c6U T\u22c6 , where the error is small when \u03bb is small, (ii) the regularizer\n ensures that the columns of U that are not too small in \u21132-norm are all approximately orthogonal to\n one another, in that the angle between pairs of vectors is \u2248      90\u25e6. Since the columns are orthogonal,\n the rank of U equals the number of non-zero columns. However, U is close to a rank r matrix since\n UU T \u2248    U\u22c6U T\u22c6 . This will imply that U also has approximately r non-zero columns. Moreover,\n pruning away the columns of U at the correct threshold will result in a model having exactly r\n columns remaining, while at the same time not affecting the population loss significantly.\nThe intuition behind UU T \u2248      U\u22c6U T\u22c6  at second order stationary points is straightforward - prior work\n [36] characterizes the behavior of such points for matrix sensing in the absence of any regularization,\n showing that UU T = U\u22c6U T      \u22c6  at second-order stationary points, and establishing a strict saddle\n condition for the population loss Lpop. In the presence of regularization, as long as \u03bb is small, we do\n                                                      25", "md": "With the choice of \\( t = 2(1 - \\eta) \\log(k) \\) and for any \\( \\eta \\leq 1 - \\log(k) \\), we obtain that \\( p(t) \\geq \\frac{k^{-(1-\\eta)/4}}{(1 - \\eta) \\log(k)} \\). Plugging into eq. (123) and using the results in the bound,\n\n$$\nP \\left| S_{\\text{max}}(2(1 - \\eta) \\log(k)) \\right| \\leq k\\eta \\leq 1 + 16 \\frac{2(1 - \\eta) \\log(k)}{4(1 - \\eta) \\log(k)} \\sqrt{1-\\eta} k\\eta \\quad (124)\n$$\nUsing the definition of \\( S_{\\text{max}}(2(1 - \\eta) \\log(k)) \\) and the fact that \\( 1 + \\sqrt{\\eta} \\geq 1 - \\sqrt{\\eta} \\) completes the proof.\n\n## A.4 Putting it all together: Proof of Theorem 1\n\nBelow we state and prove a stronger version of Theorem 1 which follows by combining Lemmas 9 and 20.\n\nTheorem 21. Consider the population loss defined in (2), for the case that \\( r = 1 \\) and \\( k \\gg 1 \\). Moreover, assume that the entries of the initial model \\( U_0 \\) are i.i.d. samples from \\( N(0, \\alpha^2) \\), where \\( \\alpha \\leq \\frac{\\eta^2}{k^3d \\log(kd)} \\). Then, for any constant \\( \\eta \\in (0, 1) \\), the iterates generated by gradient descent converge to a model \\( U_{\\text{gd}} \\) where \\( \\Omega(k\\eta) \\) columns of which are active and satisfy,\n\n$$\n\\|U_{\\text{gd}}e_i\\|_2 \\geq 1 - C4 \\sqrt{\\eta} \\quad (125)\n$$\nwith probability \\( \\geq 1 - \\text{poly}(1/d, 1/k\\eta) \\), where \\( C4 > 0 \\) is a sufficiently large constant.\n\nIn particular, if for some \\( i \\),\n\n$$\n\\left| \\langle r(0), e_i \\rangle \\right| \\quad (126)\n$$\nThen, by Lemma 9,\n\n$$\n\\max_{j\\in[k]} \\left| \\langle r(0), e_j \\rangle \\right| \\geq 1 - \\sqrt{\\eta}\n$$\n$$\n\\lim_{t\\to\\infty} \\frac{\\|U(t)e_i\\|_2}{\\max_{j\\in[k]} \\|U(t)e_j\\|_2} \\geq 1 - C127 \\sqrt{\\eta}. \\quad (127)\n$$\nwhere \\( C127 > 0 \\) is a sufficiently large constant. Invoking Lemma 20 completes the proof of Theorem 21.\n\n## B Population analysis of the regularized loss: Proof of Theorem 3\n\nIn this section, we study the second-order stationary points of loss \\( f_{\\text{pop}}(U) = L_{\\text{pop}}(U) + \\lambda R_{\\beta}(U) \\) ((9)) which is the regularized loss in the population (infinite sample) setting. The main result we prove in this section is about approximate second-order stationary points of the regularized loss \\( f_{\\text{pop}}(U) \\). When \\( \\lambda \\) and \\( \\beta \\) are chosen appropriately, we show that such points (i) are \u201cpruning friendly\u201d in that greedily pruning the columns of \\( U \\) based on their \\( L_2 \\) norm results in a solution \\( U_{\\text{prune}} \\) having exactly \\( r \\) columns, and (ii) the resulting solution \\( U_{\\text{prune}} \\) satisfies \\( \\|U_{\\text{prune}}U_{\\text{prune}}^T\\|_2 \\leq c(\\sigma^{\\star})^2 \\) where \\( c > 0 \\) is a small constant, and serves essentially as a \u201cspectral initialization\u201d in the sense of [23] for the subsequent fine-tuning phase.\n\nThe proof of Theorem 3 relies on two main observations: (i) showing that at approximate second-order stationary points \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\), where the error is small when \\( \\lambda \\) is small, (ii) the regularizer ensures that the columns of \\( U \\) that are not too small in \\( \\ell_2 \\)-norm are all approximately orthogonal to one another, in that the angle between pairs of vectors is approximately \\( 90^\\circ \\). Since the columns are orthogonal, the rank of \\( U \\) equals the number of non-zero columns. However, \\( U \\) is close to a rank \\( r \\) matrix since \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\). This will imply that \\( U \\) also has approximately \\( r \\) non-zero columns. Moreover, pruning away the columns of \\( U \\) at the correct threshold will result in a model having exactly \\( r \\) columns remaining, while at the same time not affecting the population loss significantly.\n\nThe intuition behind \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\) at second order stationary points is straightforward - prior work [36] characterizes the behavior of such points for matrix sensing in the absence of any regularization, showing that \\( UU^T = U^{\\star}(U^{\\star})^T \\) at second-order stationary points, and establishing a strict saddle condition for the population loss \\( L_{\\text{pop}} \\). In the presence of regularization, as long as \\( \\lambda \\) is small, we do\n\n25", "images": [], "items": [{"type": "text", "value": "With the choice of \\( t = 2(1 - \\eta) \\log(k) \\) and for any \\( \\eta \\leq 1 - \\log(k) \\), we obtain that \\( p(t) \\geq \\frac{k^{-(1-\\eta)/4}}{(1 - \\eta) \\log(k)} \\). Plugging into eq. (123) and using the results in the bound,\n\n$$\nP \\left| S_{\\text{max}}(2(1 - \\eta) \\log(k)) \\right| \\leq k\\eta \\leq 1 + 16 \\frac{2(1 - \\eta) \\log(k)}{4(1 - \\eta) \\log(k)} \\sqrt{1-\\eta} k\\eta \\quad (124)\n$$\nUsing the definition of \\( S_{\\text{max}}(2(1 - \\eta) \\log(k)) \\) and the fact that \\( 1 + \\sqrt{\\eta} \\geq 1 - \\sqrt{\\eta} \\) completes the proof.", "md": "With the choice of \\( t = 2(1 - \\eta) \\log(k) \\) and for any \\( \\eta \\leq 1 - \\log(k) \\), we obtain that \\( p(t) \\geq \\frac{k^{-(1-\\eta)/4}}{(1 - \\eta) \\log(k)} \\). Plugging into eq. (123) and using the results in the bound,\n\n$$\nP \\left| S_{\\text{max}}(2(1 - \\eta) \\log(k)) \\right| \\leq k\\eta \\leq 1 + 16 \\frac{2(1 - \\eta) \\log(k)}{4(1 - \\eta) \\log(k)} \\sqrt{1-\\eta} k\\eta \\quad (124)\n$$\nUsing the definition of \\( S_{\\text{max}}(2(1 - \\eta) \\log(k)) \\) and the fact that \\( 1 + \\sqrt{\\eta} \\geq 1 - \\sqrt{\\eta} \\) completes the proof."}, {"type": "heading", "lvl": 2, "value": "A.4 Putting it all together: Proof of Theorem 1", "md": "## A.4 Putting it all together: Proof of Theorem 1"}, {"type": "text", "value": "Below we state and prove a stronger version of Theorem 1 which follows by combining Lemmas 9 and 20.\n\nTheorem 21. Consider the population loss defined in (2), for the case that \\( r = 1 \\) and \\( k \\gg 1 \\). Moreover, assume that the entries of the initial model \\( U_0 \\) are i.i.d. samples from \\( N(0, \\alpha^2) \\), where \\( \\alpha \\leq \\frac{\\eta^2}{k^3d \\log(kd)} \\). Then, for any constant \\( \\eta \\in (0, 1) \\), the iterates generated by gradient descent converge to a model \\( U_{\\text{gd}} \\) where \\( \\Omega(k\\eta) \\) columns of which are active and satisfy,\n\n$$\n\\|U_{\\text{gd}}e_i\\|_2 \\geq 1 - C4 \\sqrt{\\eta} \\quad (125)\n$$\nwith probability \\( \\geq 1 - \\text{poly}(1/d, 1/k\\eta) \\), where \\( C4 > 0 \\) is a sufficiently large constant.\n\nIn particular, if for some \\( i \\),\n\n$$\n\\left| \\langle r(0), e_i \\rangle \\right| \\quad (126)\n$$\nThen, by Lemma 9,\n\n$$\n\\max_{j\\in[k]} \\left| \\langle r(0), e_j \\rangle \\right| \\geq 1 - \\sqrt{\\eta}\n$$\n$$\n\\lim_{t\\to\\infty} \\frac{\\|U(t)e_i\\|_2}{\\max_{j\\in[k]} \\|U(t)e_j\\|_2} \\geq 1 - C127 \\sqrt{\\eta}. \\quad (127)\n$$\nwhere \\( C127 > 0 \\) is a sufficiently large constant. Invoking Lemma 20 completes the proof of Theorem 21.", "md": "Below we state and prove a stronger version of Theorem 1 which follows by combining Lemmas 9 and 20.\n\nTheorem 21. Consider the population loss defined in (2), for the case that \\( r = 1 \\) and \\( k \\gg 1 \\). Moreover, assume that the entries of the initial model \\( U_0 \\) are i.i.d. samples from \\( N(0, \\alpha^2) \\), where \\( \\alpha \\leq \\frac{\\eta^2}{k^3d \\log(kd)} \\). Then, for any constant \\( \\eta \\in (0, 1) \\), the iterates generated by gradient descent converge to a model \\( U_{\\text{gd}} \\) where \\( \\Omega(k\\eta) \\) columns of which are active and satisfy,\n\n$$\n\\|U_{\\text{gd}}e_i\\|_2 \\geq 1 - C4 \\sqrt{\\eta} \\quad (125)\n$$\nwith probability \\( \\geq 1 - \\text{poly}(1/d, 1/k\\eta) \\), where \\( C4 > 0 \\) is a sufficiently large constant.\n\nIn particular, if for some \\( i \\),\n\n$$\n\\left| \\langle r(0), e_i \\rangle \\right| \\quad (126)\n$$\nThen, by Lemma 9,\n\n$$\n\\max_{j\\in[k]} \\left| \\langle r(0), e_j \\rangle \\right| \\geq 1 - \\sqrt{\\eta}\n$$\n$$\n\\lim_{t\\to\\infty} \\frac{\\|U(t)e_i\\|_2}{\\max_{j\\in[k]} \\|U(t)e_j\\|_2} \\geq 1 - C127 \\sqrt{\\eta}. \\quad (127)\n$$\nwhere \\( C127 > 0 \\) is a sufficiently large constant. Invoking Lemma 20 completes the proof of Theorem 21."}, {"type": "heading", "lvl": 2, "value": "B Population analysis of the regularized loss: Proof of Theorem 3", "md": "## B Population analysis of the regularized loss: Proof of Theorem 3"}, {"type": "text", "value": "In this section, we study the second-order stationary points of loss \\( f_{\\text{pop}}(U) = L_{\\text{pop}}(U) + \\lambda R_{\\beta}(U) \\) ((9)) which is the regularized loss in the population (infinite sample) setting. The main result we prove in this section is about approximate second-order stationary points of the regularized loss \\( f_{\\text{pop}}(U) \\). When \\( \\lambda \\) and \\( \\beta \\) are chosen appropriately, we show that such points (i) are \u201cpruning friendly\u201d in that greedily pruning the columns of \\( U \\) based on their \\( L_2 \\) norm results in a solution \\( U_{\\text{prune}} \\) having exactly \\( r \\) columns, and (ii) the resulting solution \\( U_{\\text{prune}} \\) satisfies \\( \\|U_{\\text{prune}}U_{\\text{prune}}^T\\|_2 \\leq c(\\sigma^{\\star})^2 \\) where \\( c > 0 \\) is a small constant, and serves essentially as a \u201cspectral initialization\u201d in the sense of [23] for the subsequent fine-tuning phase.\n\nThe proof of Theorem 3 relies on two main observations: (i) showing that at approximate second-order stationary points \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\), where the error is small when \\( \\lambda \\) is small, (ii) the regularizer ensures that the columns of \\( U \\) that are not too small in \\( \\ell_2 \\)-norm are all approximately orthogonal to one another, in that the angle between pairs of vectors is approximately \\( 90^\\circ \\). Since the columns are orthogonal, the rank of \\( U \\) equals the number of non-zero columns. However, \\( U \\) is close to a rank \\( r \\) matrix since \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\). This will imply that \\( U \\) also has approximately \\( r \\) non-zero columns. Moreover, pruning away the columns of \\( U \\) at the correct threshold will result in a model having exactly \\( r \\) columns remaining, while at the same time not affecting the population loss significantly.\n\nThe intuition behind \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\) at second order stationary points is straightforward - prior work [36] characterizes the behavior of such points for matrix sensing in the absence of any regularization, showing that \\( UU^T = U^{\\star}(U^{\\star})^T \\) at second-order stationary points, and establishing a strict saddle condition for the population loss \\( L_{\\text{pop}} \\). In the presence of regularization, as long as \\( \\lambda \\) is small, we do\n\n25", "md": "In this section, we study the second-order stationary points of loss \\( f_{\\text{pop}}(U) = L_{\\text{pop}}(U) + \\lambda R_{\\beta}(U) \\) ((9)) which is the regularized loss in the population (infinite sample) setting. The main result we prove in this section is about approximate second-order stationary points of the regularized loss \\( f_{\\text{pop}}(U) \\). When \\( \\lambda \\) and \\( \\beta \\) are chosen appropriately, we show that such points (i) are \u201cpruning friendly\u201d in that greedily pruning the columns of \\( U \\) based on their \\( L_2 \\) norm results in a solution \\( U_{\\text{prune}} \\) having exactly \\( r \\) columns, and (ii) the resulting solution \\( U_{\\text{prune}} \\) satisfies \\( \\|U_{\\text{prune}}U_{\\text{prune}}^T\\|_2 \\leq c(\\sigma^{\\star})^2 \\) where \\( c > 0 \\) is a small constant, and serves essentially as a \u201cspectral initialization\u201d in the sense of [23] for the subsequent fine-tuning phase.\n\nThe proof of Theorem 3 relies on two main observations: (i) showing that at approximate second-order stationary points \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\), where the error is small when \\( \\lambda \\) is small, (ii) the regularizer ensures that the columns of \\( U \\) that are not too small in \\( \\ell_2 \\)-norm are all approximately orthogonal to one another, in that the angle between pairs of vectors is approximately \\( 90^\\circ \\). Since the columns are orthogonal, the rank of \\( U \\) equals the number of non-zero columns. However, \\( U \\) is close to a rank \\( r \\) matrix since \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\). This will imply that \\( U \\) also has approximately \\( r \\) non-zero columns. Moreover, pruning away the columns of \\( U \\) at the correct threshold will result in a model having exactly \\( r \\) columns remaining, while at the same time not affecting the population loss significantly.\n\nThe intuition behind \\( UU^T \\approx U^{\\star}(U^{\\star})^T \\) at second order stationary points is straightforward - prior work [36] characterizes the behavior of such points for matrix sensing in the absence of any regularization, showing that \\( UU^T = U^{\\star}(U^{\\star})^T \\) at second-order stationary points, and establishing a strict saddle condition for the population loss \\( L_{\\text{pop}} \\). In the presence of regularization, as long as \\( \\lambda \\) is small, we do\n\n25"}]}, {"page": 26, "text": "not expect the behavior to change significantly. As we will discuss in more detail in Appendix D,\nthe regularizer R\u03b2 satisfies gradient and Hessian-Lipschitzness as long as \u03b2 is not too small. This\nwill suffi\n         ce in showing that the locations of first and second order stationary points do not change\nsignificantly when \u03bb is small.\nAs introduced earlier in eq. (11), the main result we prove in this section is that the \u201cbounded\u201d (in\noperator norm) (\u03f5, \u03b3)-approximate second-order stationary points of fpop returned by the optimization\noracle O, in Algorithm 1 satisfies the following condition,\n                      \u2200i, j : \u2225Uei\u22252, \u2225Uej\u22252 \u2265    2   \u03b2,      \u27e8Uei, Uej\u27e9     \u2248 0.\n                                                            \u2225Uei\u22252\u2225Uej\u22252\nIn other words, all the large columns of U have their pairwise angle approximately 90\u25e6. By pruning\naway the columns of U that have an \u21132 norm less than 2\u221a\u03b2, the remaining columns of U, i.e., which\nare a superset of the columns of Uprune, are now approximately at 90\u25e6      angles to one another. But\nsince UU T \u2248   U\u22c6U T\u22c6 , we know that there can be at most r significant columns of U. Therefore, after\npruning the resulting model has at most r columns.\nNote that the above discussion does not absolve the risk of over-pruning the model. If the pruning\nthreshold is not chosen carefully, it might be possible to end up with a model having fewer than r\ncolumns. Such a model cannot generalize to a vanishing error even in the population setting. As we\nwill show, it is possible to establish that error of the pruned model, \u2225UpruneU T\n                                                                               prune \u2212 U\u22c6U T\u22c6 \u2225F is also\nsmall, which comes to the rescue. We can show that it is at most 1 2(\u03c3\u22c6r)2. On the other hand if Uprune\nindeed had fewer than r columns, the approximation error must be at least (\u03c3\u22c6  r)2. This shows that the\ngreedy pruning strategy we employ is not too aggressive.\nB.1   Proof outline of Theorem 3\nIn Lemma 22 we begin with a lower bound on the gradient norm of the reulgarized loss. The lower\nbound that at exact stationary points, every pair of columns i, j \u2208     [d] will either have \u2225Uei\u22252 =\n\u2225Uej\u22252, or will be orthogonal to each other. Next we use this calculation to show that at approximate\nsecond order stationary points of the regularized loss, every pair of columns which are not too small in\n\u21132 norm are approximately orthogonal to each other. Next, in Lemma 26 we show that at approximate\nsecond order stationary points of the loss, we also expect \u2225UU T \u2212     U\u22c6U T\u22c6 \u2225F to be small when \u03bb is\nsmall. In Lemmas 28 and 29 we show that first-order stationary points are U are aligned with the\ncorrect subspace induced by the columns of U\u22c6. Finally, we combine these results in Theorem 31 to\nshow bounds on the generalization loss of the pruned solution as a function of the problem parameters.\nBy instantiating the problem parameters properly, we prove Theorem 3 in Appendix B.2.\nNote that the gradient and Hessian of R\u03b2(U) is calculated in Appendix E.3 and are in terms of\ndiagonal matrices D(U) and G(U) defined in eqs. (329) and (330).\nLemma 22 (A lower bound on the gradient norm). Consider any loss function L of the form f(UU T )\nwhere f : Rd\u00d7d \u2192    R is a differentiable function. For any candidate matrix U \u2208    Rd\u00d7k,\n            \u2225\u2207(L + \u03bbR\u03b2)(U)\u22252     F \u2265   \u03bb2 max                              \u27e8Uei, Uej\u27e92            (128)\n                                          i\u0338=j\u2208[k](D(U)ii \u2212  D(U)jj)2   \u2225Uei\u22252 2 + \u2225Uej\u22252 2\nProof. Note that \u2225\u2207(L + \u03bbR\u03b2)(U)\u22252           F    \u2265    \u27e8Z, \u2207(L + \u03bbR\u03b2)(U)\u27e92          for any candidate\nZ \u2208  Rd\u00d7k : \u2225Z\u2225F \u2264     1. The rest of this proof will be dedicated to finding such a Z.\nNote from Lemma 40 that \u27e8L(U), Z\u27e9      = \u27e8(\u2207f)(UU T ), UZT +ZU T \u27e9. Suppose the perturbation Z is\nchosen as UW where W is a skew-symmetric matrix. Then, UZT + ZU T = U(W + W T )U T = 0.\nTherefore,\nOn the other hand, from Lemma 42,          \u27e8\u2207L(U), Z\u27e9    = 0                                      (129)\n                       \u27e8\u2207R\u03b2(U), Z\u27e9     = Tr(D(U)U T Z) = Tr(D(U)U T UW).                          (130)\n                                                   26", "md": "not expect the behavior to change significantly. As we will discuss in more detail in Appendix D,\nthe regularizer \\(R_{\\beta}\\) satisfies gradient and Hessian-Lipschitzness as long as \\(\\beta\\) is not too small. This\nwill suffice in showing that the locations of first and second order stationary points do not change\nsignificantly when \\(\\lambda\\) is small.\n\nAs introduced earlier in eq. (11), the main result we prove in this section is that the \u201cbounded\u201d (in\noperator norm) (\\(\\epsilon\\), \\(\\gamma\\))-approximate second-order stationary points of \\(f_{pop}\\) returned by the optimization\noracle \\(O\\), in Algorithm 1 satisfies the following condition,\n\n\\[\n\\forall i, j : \\left\\|Ue_i\\right\\|_2, \\left\\|Ue_j\\right\\|_2 \\geq 2\\beta, \\quad \\langle Ue_i, Ue_j \\rangle \\approx 0.\n\\]\n\nIn other words, all the large columns of \\(U\\) have their pairwise angle approximately \\(90^\\circ\\). By pruning\naway the columns of \\(U\\) that have an \\(\\ell_2\\) norm less than \\(2\\sqrt{\\beta}\\), the remaining columns of \\(U\\), i.e., which\nare a superset of the columns of \\(U_{\\text{prune}}\\), are now approximately at \\(90^\\circ\\) angles to one another. But\nsince \\(UU^T \\approx U^*U^{T*}\\), we know that there can be at most \\(r\\) significant columns of \\(U\\). Therefore, after\npruning the resulting model has at most \\(r\\) columns.\n\nNote that the above discussion does not absolve the risk of over-pruning the model. If the pruning\nthreshold is not chosen carefully, it might be possible to end up with a model having fewer than \\(r\\)\ncolumns. Such a model cannot generalize to a vanishing error even in the population setting. As we\nwill show, it is possible to establish that error of the pruned model, \\(\\left\\|U_{\\text{prune}}U^T_{\\text{prune}} - U^*U^{T*}\\right\\|_F\\) is also\nsmall, which comes to the rescue. We can show that it is at most \\(\\frac{1}{2}(\\sigma^*r)^2\\). On the other hand if \\(U_{\\text{prune}}\\)\nindeed had fewer than \\(r\\) columns, the approximation error must be at least \\((\\sigma^* r)^2\\). This shows that the\ngreedy pruning strategy we employ is not too aggressive.\n\n## B.1 Proof outline of Theorem 3\n\nIn Lemma 22 we begin with a lower bound on the gradient norm of the regularized loss. The lower\nbound that at exact stationary points, every pair of columns \\(i, j \\in [d]\\) will either have \\(\\left\\|Ue_i\\right\\|_2 =\n\\left\\|Ue_j\\right\\|_2\\), or will be orthogonal to each other. Next we use this calculation to show that at approximate\nsecond order stationary points of the regularized loss, every pair of columns which are not too small in\n\\(\\ell_2\\) norm are approximately orthogonal to each other. Next, in Lemma 26 we show that at approximate\nsecond order stationary points of the loss, we also expect \\(\\left\\|UU^T - U^*U^{T*}\\right\\|_F\\) to be small when \\(\\lambda\\) is\nsmall. In Lemmas 28 and 29 we show that first-order stationary points are \\(U\\) are aligned with the\ncorrect subspace induced by the columns of \\(U^*\\). Finally, we combine these results in Theorem 31 to\nshow bounds on the generalization loss of the pruned solution as a function of the problem parameters.\nBy instantiating the problem parameters properly, we prove Theorem 3 in Appendix B.2.\n\nNote that the gradient and Hessian of \\(R_{\\beta}(U)\\) is calculated in Appendix E.3 and are in terms of\ndiagonal matrices \\(D(U)\\) and \\(G(U)\\) defined in eqs. (329) and (330).\n\nLemma 22 (A lower bound on the gradient norm). Consider any loss function \\(L\\) of the form \\(f(UU^T)\\)\nwhere \\(f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}\\) is a differentiable function. For any candidate matrix \\(U \\in \\mathbb{R}^{d \\times k}\\),\n\\[\n\\left\\|\\nabla(L + \\lambda R_{\\beta})(U)\\right\\|_2^2 \\geq \\lambda^2 \\max_{i \\neq j \\in [k]} \\frac{\\langle Ue_i, Ue_j \\rangle^2}{(D(U)_{ii} - D(U)_{jj})^2 \\left\\|Ue_i\\right\\|_2^2 + \\left\\|Ue_j\\right\\|_2^2}\n\\]\nProof. Note that \\(\\left\\|\\nabla(L + \\lambda R_{\\beta})(U)\\right\\|_2^2 \\geq \\langle Z, \\nabla(L + \\lambda R_{\\beta})(U) \\rangle^2\\) for any candidate\n\\(Z \\in \\mathbb{R}^{d \\times k} : \\left\\|Z\\right\\|_F \\leq 1\\). The rest of this proof will be dedicated to finding such a \\(Z\\).\nNote from Lemma 40 that \\(\\langle L(U), Z \\rangle = \\langle \\nabla f(UU^T), UZ^T + ZU^T \\rangle\\). Suppose the perturbation \\(Z\\) is\nchosen as \\(UW\\) where \\(W\\) is a skew-symmetric matrix. Then, \\(UZ^T + ZU^T = U(W + W^T)U^T = 0\\).\nTherefore,\nOn the other hand, from Lemma 42, \\(\\langle \\nabla L(U), Z \\rangle = 0\\),\n\\(\\langle \\nabla R_{\\beta}(U), Z \\rangle = \\text{Tr}(D(U)U^TZ) = \\text{Tr}(D(U)U^TUW)\\).", "images": [], "items": [{"type": "text", "value": "not expect the behavior to change significantly. As we will discuss in more detail in Appendix D,\nthe regularizer \\(R_{\\beta}\\) satisfies gradient and Hessian-Lipschitzness as long as \\(\\beta\\) is not too small. This\nwill suffice in showing that the locations of first and second order stationary points do not change\nsignificantly when \\(\\lambda\\) is small.\n\nAs introduced earlier in eq. (11), the main result we prove in this section is that the \u201cbounded\u201d (in\noperator norm) (\\(\\epsilon\\), \\(\\gamma\\))-approximate second-order stationary points of \\(f_{pop}\\) returned by the optimization\noracle \\(O\\), in Algorithm 1 satisfies the following condition,\n\n\\[\n\\forall i, j : \\left\\|Ue_i\\right\\|_2, \\left\\|Ue_j\\right\\|_2 \\geq 2\\beta, \\quad \\langle Ue_i, Ue_j \\rangle \\approx 0.\n\\]\n\nIn other words, all the large columns of \\(U\\) have their pairwise angle approximately \\(90^\\circ\\). By pruning\naway the columns of \\(U\\) that have an \\(\\ell_2\\) norm less than \\(2\\sqrt{\\beta}\\), the remaining columns of \\(U\\), i.e., which\nare a superset of the columns of \\(U_{\\text{prune}}\\), are now approximately at \\(90^\\circ\\) angles to one another. But\nsince \\(UU^T \\approx U^*U^{T*}\\), we know that there can be at most \\(r\\) significant columns of \\(U\\). Therefore, after\npruning the resulting model has at most \\(r\\) columns.\n\nNote that the above discussion does not absolve the risk of over-pruning the model. If the pruning\nthreshold is not chosen carefully, it might be possible to end up with a model having fewer than \\(r\\)\ncolumns. Such a model cannot generalize to a vanishing error even in the population setting. As we\nwill show, it is possible to establish that error of the pruned model, \\(\\left\\|U_{\\text{prune}}U^T_{\\text{prune}} - U^*U^{T*}\\right\\|_F\\) is also\nsmall, which comes to the rescue. We can show that it is at most \\(\\frac{1}{2}(\\sigma^*r)^2\\). On the other hand if \\(U_{\\text{prune}}\\)\nindeed had fewer than \\(r\\) columns, the approximation error must be at least \\((\\sigma^* r)^2\\). This shows that the\ngreedy pruning strategy we employ is not too aggressive.", "md": "not expect the behavior to change significantly. As we will discuss in more detail in Appendix D,\nthe regularizer \\(R_{\\beta}\\) satisfies gradient and Hessian-Lipschitzness as long as \\(\\beta\\) is not too small. This\nwill suffice in showing that the locations of first and second order stationary points do not change\nsignificantly when \\(\\lambda\\) is small.\n\nAs introduced earlier in eq. (11), the main result we prove in this section is that the \u201cbounded\u201d (in\noperator norm) (\\(\\epsilon\\), \\(\\gamma\\))-approximate second-order stationary points of \\(f_{pop}\\) returned by the optimization\noracle \\(O\\), in Algorithm 1 satisfies the following condition,\n\n\\[\n\\forall i, j : \\left\\|Ue_i\\right\\|_2, \\left\\|Ue_j\\right\\|_2 \\geq 2\\beta, \\quad \\langle Ue_i, Ue_j \\rangle \\approx 0.\n\\]\n\nIn other words, all the large columns of \\(U\\) have their pairwise angle approximately \\(90^\\circ\\). By pruning\naway the columns of \\(U\\) that have an \\(\\ell_2\\) norm less than \\(2\\sqrt{\\beta}\\), the remaining columns of \\(U\\), i.e., which\nare a superset of the columns of \\(U_{\\text{prune}}\\), are now approximately at \\(90^\\circ\\) angles to one another. But\nsince \\(UU^T \\approx U^*U^{T*}\\), we know that there can be at most \\(r\\) significant columns of \\(U\\). Therefore, after\npruning the resulting model has at most \\(r\\) columns.\n\nNote that the above discussion does not absolve the risk of over-pruning the model. If the pruning\nthreshold is not chosen carefully, it might be possible to end up with a model having fewer than \\(r\\)\ncolumns. Such a model cannot generalize to a vanishing error even in the population setting. As we\nwill show, it is possible to establish that error of the pruned model, \\(\\left\\|U_{\\text{prune}}U^T_{\\text{prune}} - U^*U^{T*}\\right\\|_F\\) is also\nsmall, which comes to the rescue. We can show that it is at most \\(\\frac{1}{2}(\\sigma^*r)^2\\). On the other hand if \\(U_{\\text{prune}}\\)\nindeed had fewer than \\(r\\) columns, the approximation error must be at least \\((\\sigma^* r)^2\\). This shows that the\ngreedy pruning strategy we employ is not too aggressive."}, {"type": "heading", "lvl": 2, "value": "B.1 Proof outline of Theorem 3", "md": "## B.1 Proof outline of Theorem 3"}, {"type": "text", "value": "In Lemma 22 we begin with a lower bound on the gradient norm of the regularized loss. The lower\nbound that at exact stationary points, every pair of columns \\(i, j \\in [d]\\) will either have \\(\\left\\|Ue_i\\right\\|_2 =\n\\left\\|Ue_j\\right\\|_2\\), or will be orthogonal to each other. Next we use this calculation to show that at approximate\nsecond order stationary points of the regularized loss, every pair of columns which are not too small in\n\\(\\ell_2\\) norm are approximately orthogonal to each other. Next, in Lemma 26 we show that at approximate\nsecond order stationary points of the loss, we also expect \\(\\left\\|UU^T - U^*U^{T*}\\right\\|_F\\) to be small when \\(\\lambda\\) is\nsmall. In Lemmas 28 and 29 we show that first-order stationary points are \\(U\\) are aligned with the\ncorrect subspace induced by the columns of \\(U^*\\). Finally, we combine these results in Theorem 31 to\nshow bounds on the generalization loss of the pruned solution as a function of the problem parameters.\nBy instantiating the problem parameters properly, we prove Theorem 3 in Appendix B.2.\n\nNote that the gradient and Hessian of \\(R_{\\beta}(U)\\) is calculated in Appendix E.3 and are in terms of\ndiagonal matrices \\(D(U)\\) and \\(G(U)\\) defined in eqs. (329) and (330).\n\nLemma 22 (A lower bound on the gradient norm). Consider any loss function \\(L\\) of the form \\(f(UU^T)\\)\nwhere \\(f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}\\) is a differentiable function. For any candidate matrix \\(U \\in \\mathbb{R}^{d \\times k}\\),\n\\[\n\\left\\|\\nabla(L + \\lambda R_{\\beta})(U)\\right\\|_2^2 \\geq \\lambda^2 \\max_{i \\neq j \\in [k]} \\frac{\\langle Ue_i, Ue_j \\rangle^2}{(D(U)_{ii} - D(U)_{jj})^2 \\left\\|Ue_i\\right\\|_2^2 + \\left\\|Ue_j\\right\\|_2^2}\n\\]\nProof. Note that \\(\\left\\|\\nabla(L + \\lambda R_{\\beta})(U)\\right\\|_2^2 \\geq \\langle Z, \\nabla(L + \\lambda R_{\\beta})(U) \\rangle^2\\) for any candidate\n\\(Z \\in \\mathbb{R}^{d \\times k} : \\left\\|Z\\right\\|_F \\leq 1\\). The rest of this proof will be dedicated to finding such a \\(Z\\).\nNote from Lemma 40 that \\(\\langle L(U), Z \\rangle = \\langle \\nabla f(UU^T), UZ^T + ZU^T \\rangle\\). Suppose the perturbation \\(Z\\) is\nchosen as \\(UW\\) where \\(W\\) is a skew-symmetric matrix. Then, \\(UZ^T + ZU^T = U(W + W^T)U^T = 0\\).\nTherefore,\nOn the other hand, from Lemma 42, \\(\\langle \\nabla L(U), Z \\rangle = 0\\),\n\\(\\langle \\nabla R_{\\beta}(U), Z \\rangle = \\text{Tr}(D(U)U^TZ) = \\text{Tr}(D(U)U^TUW)\\).", "md": "In Lemma 22 we begin with a lower bound on the gradient norm of the regularized loss. The lower\nbound that at exact stationary points, every pair of columns \\(i, j \\in [d]\\) will either have \\(\\left\\|Ue_i\\right\\|_2 =\n\\left\\|Ue_j\\right\\|_2\\), or will be orthogonal to each other. Next we use this calculation to show that at approximate\nsecond order stationary points of the regularized loss, every pair of columns which are not too small in\n\\(\\ell_2\\) norm are approximately orthogonal to each other. Next, in Lemma 26 we show that at approximate\nsecond order stationary points of the loss, we also expect \\(\\left\\|UU^T - U^*U^{T*}\\right\\|_F\\) to be small when \\(\\lambda\\) is\nsmall. In Lemmas 28 and 29 we show that first-order stationary points are \\(U\\) are aligned with the\ncorrect subspace induced by the columns of \\(U^*\\). Finally, we combine these results in Theorem 31 to\nshow bounds on the generalization loss of the pruned solution as a function of the problem parameters.\nBy instantiating the problem parameters properly, we prove Theorem 3 in Appendix B.2.\n\nNote that the gradient and Hessian of \\(R_{\\beta}(U)\\) is calculated in Appendix E.3 and are in terms of\ndiagonal matrices \\(D(U)\\) and \\(G(U)\\) defined in eqs. (329) and (330).\n\nLemma 22 (A lower bound on the gradient norm). Consider any loss function \\(L\\) of the form \\(f(UU^T)\\)\nwhere \\(f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}\\) is a differentiable function. For any candidate matrix \\(U \\in \\mathbb{R}^{d \\times k}\\),\n\\[\n\\left\\|\\nabla(L + \\lambda R_{\\beta})(U)\\right\\|_2^2 \\geq \\lambda^2 \\max_{i \\neq j \\in [k]} \\frac{\\langle Ue_i, Ue_j \\rangle^2}{(D(U)_{ii} - D(U)_{jj})^2 \\left\\|Ue_i\\right\\|_2^2 + \\left\\|Ue_j\\right\\|_2^2}\n\\]\nProof. Note that \\(\\left\\|\\nabla(L + \\lambda R_{\\beta})(U)\\right\\|_2^2 \\geq \\langle Z, \\nabla(L + \\lambda R_{\\beta})(U) \\rangle^2\\) for any candidate\n\\(Z \\in \\mathbb{R}^{d \\times k} : \\left\\|Z\\right\\|_F \\leq 1\\). The rest of this proof will be dedicated to finding such a \\(Z\\).\nNote from Lemma 40 that \\(\\langle L(U), Z \\rangle = \\langle \\nabla f(UU^T), UZ^T + ZU^T \\rangle\\). Suppose the perturbation \\(Z\\) is\nchosen as \\(UW\\) where \\(W\\) is a skew-symmetric matrix. Then, \\(UZ^T + ZU^T = U(W + W^T)U^T = 0\\).\nTherefore,\nOn the other hand, from Lemma 42, \\(\\langle \\nabla L(U), Z \\rangle = 0\\),\n\\(\\langle \\nabla R_{\\beta}(U), Z \\rangle = \\text{Tr}(D(U)U^TZ) = \\text{Tr}(D(U)U^TUW)\\)."}]}, {"page": 27, "text": "where D(U) is defined in eq. (329). Define (i, j) as an arbitrary pair of distinct indices in [k]. Suppose\nW = eieT    j \u2212    ejeTi . Then,\n                              \u27e8\u2207R\u03b2(U), Z\u27e92 = (eT           i (D(U)U T U \u2212          U T UD(U))ej)2                                (131)\n                                                     = (D(U)ii \u2212        D(U)jj)2\u27e8Uei, Uej\u27e92.                                     (132)\nDividing throughout by \u2225Z\u22252            F = \u2225UW\u22252        F = \u2225Uei\u22252     F + \u2225Uej\u22252      F and noting that i \u0338= j are arbitrary\ncompletes the proof.\nRemark 23. The interpretation of the lower bound on the gradient norm in Lemma 22 is best\nunderstood by looking at its behavior at first-order stationary points, where the gradient is 0. At\na first-order stationary point, for any i, j \u2208                   [k] such that i \u0338= j, either D(U)ii = D(U)jj or\nUei \u22a5     Uej. The former condition is true iff \u2225Uei\u22252 = \u2225Uej\u22252.\nNext we prove a result which establishes near-orthogonality of the columns of U at approximate\nsecond-order stationary points.\nLemma 24. Consider any loss L(U) of the form f(UU T ) where f : Rd\u00d7d \u2192                                       R is doubly differen-\ntiable. Consider any \u03f5-approximate first-order stationary point of L + \u03bbR\u03b2, denoted U. Consider\n                                               \u221a    \u27e8Uei,Uej\u27e9\nany i \u0338= j \u2208     [k] and define Cij =             \u2225Uei\u22252 2+\u2225Uej\u22252   2 . Then, if\n        1. min{\u2225Uei\u22252, \u2225Uej\u22252} \u2264                 2\u221a\u03b2, then, |Cij| \u2264         2\u221a\u03b2.\n        2. In the complement case, defining \u2225Z\u2225F =                        \u2225Uei\u22252   2 + \u2225Uej\u22252     2, if,\n                                          \u03f5 + \u03b3 min{\u2225Uei\u22252, \u2225Uei\u22252}                   \u00b7 min{\u2225Uei\u22252, \u2225Uej\u22252}.                     (133)\n                            |Cij| \u2265    5                        \u03bb\n            then the Hessian of the regularized loss at U has a large negative eigenvalue,\n                                                    \u03bbmin(\u22072(L + \u03bbR\u03b2)(U)) < \u2212\u03b3.                                                   (134)\nIn summary, at second order stationary points the columns are approximately orthogonal in the sense\nthat |Cij| is small.\nProof. Note that, regardless of whether U is approximately stationary or not, for any i \u0338= j \u2208                                   [k], if\nmin{\u2225Uei\u22252, \u2225Uej\u22252} \u2264    |\u27e8Uei, Uej\u27e92\u221a\u03b2, then,   \u2264   max\u2225Uei\u22252\u2225Uej\u22252             = min                             \u03b2.            (135)\n                      \u2225Uei\u22252   2 + \u2225Uej\u22252     2            i\u2208{1,2} \u2225Uei\u22252     2      i\u2208{1,2} \u2225Uei\u22252 \u2264         2\nThis proves the first part of the theorem. Henceforth, we will assume that min{\u2225Uei\u22252, \u2225Uej\u22252} \u2265\n2\u221a\u03b2. This condition will turn out to lower bound the Frobenius norm of a matrix Z which we will\nrequire later for the proof of the second part.\nThe approximate first-order stationarity of U implies that\n                                                  \u2225\u2207(L + \u03bbR\u03b2)(U)\u2225F \u2264                \u03f5.                                           (136)\nAs in the proof of Lemma 22, the proof strategy is to construct explicit directions (matrices) capturing\nthe directions of negative curvature of the regularized loss. From Lemma 22, the approximate\nfirst-order stationarity of U implies,\n                                                   |\u27e8Uei, Uej\u27e9|             \u2264   \u03f5                                                (137)\n                                                 \u2225Uei\u22252   2 + \u2225Uej\u22252     2      \u03bb \u00b7 1|\u2206|.\nwhere \u2206      = D(U)ii \u2212        D(U)jj. Consider any tuple i \u0338= j \u2208                 [k]. Without loss of generality we will\nwork with i = 1, j = 2.\nConsider a perturbation Z \u2208              Rd\u00d7k which takes the form Z = UW where W is a skew-symmetric\nmatrix which satisfies the \u201csupport condition\u201d,\n                                               \u2200i\u2032 \u2265    3, Wei\u2032 = 0, eT      i\u2032W = 0                                             (138)\n                                                                  27", "md": "where \\(D(U)\\) is defined in eq. (329). Define \\((i, j)\\) as an arbitrary pair of distinct indices in \\([k]\\). Suppose\n\\(W = e_ie_j^T - e_je_i^T\\). Then,\n\\[\n\\langle \\nabla R_\\beta(U), Z \\rangle^2 = (e_i^T(D(U)U^TU - U^TUD(U))e_j)^2 \\tag{131}\n\\]\n\\[\n= (D(U)_{ii} - D(U)_{jj})^2 \\langle Ue_i, Ue_j \\rangle^2. \\tag{132}\n\\]\nDividing throughout by \\(\\|Z\\|_2^2\\), \\(F = \\|UW\\|_F^2 = \\|Ue_i\\|_F^2 + \\|Ue_j\\|_F^2\\) and noting that \\(i \\neq j\\) are arbitrary\ncompletes the proof.\n\nRemark 23. The interpretation of the lower bound on the gradient norm in Lemma 22 is best\nunderstood by looking at its behavior at first-order stationary points, where the gradient is 0. At\na first-order stationary point, for any \\(i, j \\in [k]\\) such that \\(i \\neq j\\), either \\(D(U)_{ii} = D(U)_{jj}\\) or\n\\(Ue_i \\perp Ue_j\\). The former condition is true if and only if \\(\\|Ue_i\\|_2 = \\|Ue_j\\|_2\\).\n\nNext we prove a result which establishes near-orthogonality of the columns of \\(U\\) at approximate\nsecond-order stationary points.\n\nLemma 24. Consider any loss \\(L(U)\\) of the form \\(f(UU^T)\\) where \\(f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}\\) is doubly differentiable. Consider any \\(\\epsilon\\)-approximate first-order stationary point of \\(L + \\lambda R_\\beta\\), denoted \\(U\\). Consider any \\(i \\neq j \\in [k]\\) and define \\(C_{ij} = \\frac{\\sqrt{\\langle Ue_i, Ue_j \\rangle}}{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}\\). Then, if\n1. \\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\leq 2\\sqrt{\\beta}\\), then, \\(|C_{ij}| \\leq 2\\sqrt{\\beta}\\).\n2. In the complement case, defining \\(\\|Z\\|_F = \\sqrt{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}\\), if,\n\\[\n|C_{ij}| \\geq 5\\lambda \\left( \\epsilon + \\gamma \\min\\{\\|Ue_i\\|_2, \\|Ue_i\\|_2\\} \\cdot \\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\right). \\tag{133}\n\\]\nthen the Hessian of the regularized loss at \\(U\\) has a large negative eigenvalue,\n\\[\n\\lambda_{\\text{min}}(\\nabla^2(L + \\lambda R_\\beta)(U)) < -\\gamma. \\tag{134}\n\\]\n\nIn summary, at second-order stationary points the columns are approximately orthogonal in the sense\nthat \\(|C_{ij}|\\) is small.\n\nProof. Note that, regardless of whether \\(U\\) is approximately stationary or not, for any \\(i \\neq j \\in [k]\\), if\n\\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\leq |\\langle Ue_i, Ue_j \\rangle|2\\sqrt{\\beta}\\), then,\n\\[\n\\leq \\max \\frac{\\|Ue_i\\|_2\\|Ue_j\\|_2}{\\sqrt{\\beta}} = \\min \\beta. \\tag{135}\n\\]\nThis proves the first part of the theorem. Henceforth, we will assume that \\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\geq 2\\sqrt{\\beta}\\). This condition will turn out to lower bound the Frobenius norm of a matrix \\(Z\\) which we will require later for the proof of the second part.\n\nThe approximate first-order stationarity of \\(U\\) implies that\n\\[\n\\|\\nabla(L + \\lambda R_\\beta)(U)\\|_F \\leq \\epsilon. \\tag{136}\n\\]\nAs in the proof of Lemma 22, the proof strategy is to construct explicit directions (matrices) capturing\nthe directions of negative curvature of the regularized loss. From Lemma 22, the approximate\nfirst-order stationarity of \\(U\\) implies,\n\\[\n|\\langle Ue_i, Ue_j \\rangle| \\leq \\epsilon \\tag{137}\n\\]\n\\[\n\\frac{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}{\\lambda} \\cdot 1|\\Delta|.\n\\]\nwhere \\(\\Delta = D(U)_{ii} - D(U)_{jj}\\). Consider any tuple \\(i \\neq j \\in [k]\\). Without loss of generality we will\nwork with \\(i = 1\\), \\(j = 2\\).\n\nConsider a perturbation \\(Z \\in \\mathbb{R}^{d \\times k}\\) which takes the form \\(Z = UW\\) where \\(W\\) is a skew-symmetric\nmatrix which satisfies the \u201csupport condition\u201d,\n\\[\n\\forall i' \\geq 3, We_{i'} = 0, e_i^TW = 0 \\tag{138}\n\\]", "images": [], "items": [{"type": "text", "value": "where \\(D(U)\\) is defined in eq. (329). Define \\((i, j)\\) as an arbitrary pair of distinct indices in \\([k]\\). Suppose\n\\(W = e_ie_j^T - e_je_i^T\\). Then,\n\\[\n\\langle \\nabla R_\\beta(U), Z \\rangle^2 = (e_i^T(D(U)U^TU - U^TUD(U))e_j)^2 \\tag{131}\n\\]\n\\[\n= (D(U)_{ii} - D(U)_{jj})^2 \\langle Ue_i, Ue_j \\rangle^2. \\tag{132}\n\\]\nDividing throughout by \\(\\|Z\\|_2^2\\), \\(F = \\|UW\\|_F^2 = \\|Ue_i\\|_F^2 + \\|Ue_j\\|_F^2\\) and noting that \\(i \\neq j\\) are arbitrary\ncompletes the proof.\n\nRemark 23. The interpretation of the lower bound on the gradient norm in Lemma 22 is best\nunderstood by looking at its behavior at first-order stationary points, where the gradient is 0. At\na first-order stationary point, for any \\(i, j \\in [k]\\) such that \\(i \\neq j\\), either \\(D(U)_{ii} = D(U)_{jj}\\) or\n\\(Ue_i \\perp Ue_j\\). The former condition is true if and only if \\(\\|Ue_i\\|_2 = \\|Ue_j\\|_2\\).\n\nNext we prove a result which establishes near-orthogonality of the columns of \\(U\\) at approximate\nsecond-order stationary points.\n\nLemma 24. Consider any loss \\(L(U)\\) of the form \\(f(UU^T)\\) where \\(f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}\\) is doubly differentiable. Consider any \\(\\epsilon\\)-approximate first-order stationary point of \\(L + \\lambda R_\\beta\\), denoted \\(U\\). Consider any \\(i \\neq j \\in [k]\\) and define \\(C_{ij} = \\frac{\\sqrt{\\langle Ue_i, Ue_j \\rangle}}{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}\\). Then, if\n1. \\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\leq 2\\sqrt{\\beta}\\), then, \\(|C_{ij}| \\leq 2\\sqrt{\\beta}\\).\n2. In the complement case, defining \\(\\|Z\\|_F = \\sqrt{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}\\), if,\n\\[", "md": "where \\(D(U)\\) is defined in eq. (329). Define \\((i, j)\\) as an arbitrary pair of distinct indices in \\([k]\\). Suppose\n\\(W = e_ie_j^T - e_je_i^T\\). Then,\n\\[\n\\langle \\nabla R_\\beta(U), Z \\rangle^2 = (e_i^T(D(U)U^TU - U^TUD(U))e_j)^2 \\tag{131}\n\\]\n\\[\n= (D(U)_{ii} - D(U)_{jj})^2 \\langle Ue_i, Ue_j \\rangle^2. \\tag{132}\n\\]\nDividing throughout by \\(\\|Z\\|_2^2\\), \\(F = \\|UW\\|_F^2 = \\|Ue_i\\|_F^2 + \\|Ue_j\\|_F^2\\) and noting that \\(i \\neq j\\) are arbitrary\ncompletes the proof.\n\nRemark 23. The interpretation of the lower bound on the gradient norm in Lemma 22 is best\nunderstood by looking at its behavior at first-order stationary points, where the gradient is 0. At\na first-order stationary point, for any \\(i, j \\in [k]\\) such that \\(i \\neq j\\), either \\(D(U)_{ii} = D(U)_{jj}\\) or\n\\(Ue_i \\perp Ue_j\\). The former condition is true if and only if \\(\\|Ue_i\\|_2 = \\|Ue_j\\|_2\\).\n\nNext we prove a result which establishes near-orthogonality of the columns of \\(U\\) at approximate\nsecond-order stationary points.\n\nLemma 24. Consider any loss \\(L(U)\\) of the form \\(f(UU^T)\\) where \\(f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}\\) is doubly differentiable. Consider any \\(\\epsilon\\)-approximate first-order stationary point of \\(L + \\lambda R_\\beta\\), denoted \\(U\\). Consider any \\(i \\neq j \\in [k]\\) and define \\(C_{ij} = \\frac{\\sqrt{\\langle Ue_i, Ue_j \\rangle}}{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}\\). Then, if\n1. \\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\leq 2\\sqrt{\\beta}\\), then, \\(|C_{ij}| \\leq 2\\sqrt{\\beta}\\).\n2. In the complement case, defining \\(\\|Z\\|_F = \\sqrt{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}\\), if,\n\\["}, {"type": "table", "rows": [["C_{ij}", "\\geq 5\\lambda \\left( \\epsilon + \\gamma \\min\\{\\", "Ue_i\\", "_2, \\", "Ue_i\\", "_2\\} \\cdot \\min\\{\\", "Ue_i\\", "_2, \\", "Ue_j\\"]], "md": "|C_{ij}| \\geq 5\\lambda \\left( \\epsilon + \\gamma \\min\\{\\|Ue_i\\|_2, \\|Ue_i\\|_2\\} \\cdot \\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\right). \\tag{133}", "isPerfectTable": true, "csv": "\"C_{ij}\",\"\\geq 5\\lambda \\left( \\epsilon + \\gamma \\min\\{\\\",\"Ue_i\\\",\"_2, \\\",\"Ue_i\\\",\"_2\\} \\cdot \\min\\{\\\",\"Ue_i\\\",\"_2, \\\",\"Ue_j\\\""}, {"type": "text", "value": "\\]\nthen the Hessian of the regularized loss at \\(U\\) has a large negative eigenvalue,\n\\[\n\\lambda_{\\text{min}}(\\nabla^2(L + \\lambda R_\\beta)(U)) < -\\gamma. \\tag{134}\n\\]\n\nIn summary, at second-order stationary points the columns are approximately orthogonal in the sense\nthat \\(|C_{ij}|\\) is small.\n\nProof. Note that, regardless of whether \\(U\\) is approximately stationary or not, for any \\(i \\neq j \\in [k]\\), if\n\\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\leq |\\langle Ue_i, Ue_j \\rangle|2\\sqrt{\\beta}\\), then,\n\\[\n\\leq \\max \\frac{\\|Ue_i\\|_2\\|Ue_j\\|_2}{\\sqrt{\\beta}} = \\min \\beta. \\tag{135}\n\\]\nThis proves the first part of the theorem. Henceforth, we will assume that \\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\geq 2\\sqrt{\\beta}\\). This condition will turn out to lower bound the Frobenius norm of a matrix \\(Z\\) which we will require later for the proof of the second part.\n\nThe approximate first-order stationarity of \\(U\\) implies that\n\\[\n\\|\\nabla(L + \\lambda R_\\beta)(U)\\|_F \\leq \\epsilon. \\tag{136}\n\\]\nAs in the proof of Lemma 22, the proof strategy is to construct explicit directions (matrices) capturing\nthe directions of negative curvature of the regularized loss. From Lemma 22, the approximate\nfirst-order stationarity of \\(U\\) implies,\n\\[", "md": "\\]\nthen the Hessian of the regularized loss at \\(U\\) has a large negative eigenvalue,\n\\[\n\\lambda_{\\text{min}}(\\nabla^2(L + \\lambda R_\\beta)(U)) < -\\gamma. \\tag{134}\n\\]\n\nIn summary, at second-order stationary points the columns are approximately orthogonal in the sense\nthat \\(|C_{ij}|\\) is small.\n\nProof. Note that, regardless of whether \\(U\\) is approximately stationary or not, for any \\(i \\neq j \\in [k]\\), if\n\\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\leq |\\langle Ue_i, Ue_j \\rangle|2\\sqrt{\\beta}\\), then,\n\\[\n\\leq \\max \\frac{\\|Ue_i\\|_2\\|Ue_j\\|_2}{\\sqrt{\\beta}} = \\min \\beta. \\tag{135}\n\\]\nThis proves the first part of the theorem. Henceforth, we will assume that \\(\\min\\{\\|Ue_i\\|_2, \\|Ue_j\\|_2\\} \\geq 2\\sqrt{\\beta}\\). This condition will turn out to lower bound the Frobenius norm of a matrix \\(Z\\) which we will require later for the proof of the second part.\n\nThe approximate first-order stationarity of \\(U\\) implies that\n\\[\n\\|\\nabla(L + \\lambda R_\\beta)(U)\\|_F \\leq \\epsilon. \\tag{136}\n\\]\nAs in the proof of Lemma 22, the proof strategy is to construct explicit directions (matrices) capturing\nthe directions of negative curvature of the regularized loss. From Lemma 22, the approximate\nfirst-order stationarity of \\(U\\) implies,\n\\["}, {"type": "table", "rows": [["\\langle Ue_i, Ue_j \\rangle"]], "md": "|\\langle Ue_i, Ue_j \\rangle| \\leq \\epsilon \\tag{137}", "isPerfectTable": true, "csv": "\"\\langle Ue_i, Ue_j \\rangle\""}, {"type": "text", "value": "\\]\n\\[\n\\frac{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}{\\lambda} \\cdot 1|\\Delta|.\n\\]\nwhere \\(\\Delta = D(U)_{ii} - D(U)_{jj}\\). Consider any tuple \\(i \\neq j \\in [k]\\). Without loss of generality we will\nwork with \\(i = 1\\), \\(j = 2\\).\n\nConsider a perturbation \\(Z \\in \\mathbb{R}^{d \\times k}\\) which takes the form \\(Z = UW\\) where \\(W\\) is a skew-symmetric\nmatrix which satisfies the \u201csupport condition\u201d,\n\\[\n\\forall i' \\geq 3, We_{i'} = 0, e_i^TW = 0 \\tag{138}\n\\]", "md": "\\]\n\\[\n\\frac{\\|Ue_i\\|_2^2 + \\|Ue_j\\|_2^2}{\\lambda} \\cdot 1|\\Delta|.\n\\]\nwhere \\(\\Delta = D(U)_{ii} - D(U)_{jj}\\). Consider any tuple \\(i \\neq j \\in [k]\\). Without loss of generality we will\nwork with \\(i = 1\\), \\(j = 2\\).\n\nConsider a perturbation \\(Z \\in \\mathbb{R}^{d \\times k}\\) which takes the form \\(Z = UW\\) where \\(W\\) is a skew-symmetric\nmatrix which satisfies the \u201csupport condition\u201d,\n\\[\n\\forall i' \\geq 3, We_{i'} = 0, e_i^TW = 0 \\tag{138}\n\\]"}]}, {"page": 28, "text": "In general, when \u2206               is small in absolute value, by the support condition eq. (138) of W                                , one\nwould expect W  \u2206         D(U) \u2248        D(U)W in case \u2206               is small. In particular, defining the diagonal matrix\nL = diag          2 , \u2212\u22062 , 0, \u00b7 \u00b7 \u00b7    , we have that,\n                                                W   D(U) \u2212       D(U)W = W             L \u2212     LW.                                  (139)\nFrom the gradient and Hessian computations in Lemma 42,\n                 vec(Z)T [\u22072R\u03b2(U)]vec(Z)              k\n                  = Tr(D(U) \u00b7 ZT Z) \u2212                i=1k  G(U)ii\u27e8Zei, Uei\u27e92                                                        (140)\n                  = Tr(W       D(U)ZT U) \u2212            i=1   G(U)ii\u27e8Zei, Uei\u27e92                      k                                (141)\n                  = \u27e8\u2207R\u03b2(U), W             ZT \u27e9    + Tr((W       L \u2212    LW     ) \u00b7 ZT U) \u2212       i=1   G(U)ii\u27e8Zei, Uei\u27e92            (142)\nwhere the last equation uses the gradient computation in Lemma 42 and the fact that W and D(U)\napproximately commute per eq. (139). Likewise, analyzing the second order behavior of L using the\nHessian computations in Lemma 40 results in the following set of equations,\n        vec(Z)T [\u22072L(U)]vec(Z)                                                                                                      (143)\n         = vec(UZT + ZU T )[(\u22072f)(UU T )]vec(UZT + ZU T ) + 2\u27e8(\u2207f)(UU T ), ZZT \u27e9                                                    (144)\n         (i)\n         = 2\u27e8(\u2207f)(UU T ), ZZT \u27e9                                                                                                     (145)\n         = \u27e8(\u2207f)(UU T ), UW               ZT + ZW T U T \u27e9                                                                           (146)\n         = \u27e8\u2207L(U), W            ZT \u27e9,                                                                                               (147)\nwhere (i) uses the fact that UZT + ZU T = U(W T + W                                      )U T = 0 and the last equation uses the\nuses the gradient computations in Lemma 40.\nSumming up eqs. (142) and (147), we get,\n    vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z)                                                                      k\n     (i)\n      = \u27e8\u2207(L + \u03bbR\u03b2)(U), W                  ZT \u27e9    + \u03bbTr((W        L \u2212    LW     ) \u00b7 ZT U) \u2212       \u03bb  i=1   G(U)ii\u27e8Zei, Uei\u27e92       (148)\nNoting that U is an \u03f5-approximate stationary point of L + \u03bbR\u03b2, by Cauchy\u2013Bunyakovsky\u2013Schwarz\ninequality,\n                    vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z)                                              k\n                     \u2264   \u03f5\u2225W     ZT \u2225F + \u03bbTr((W            L \u2212    LW     ) \u00b7 ZT U) \u2212       \u03bb  i=1   G(U)ii\u27e8Zei, Uei\u27e92               (149)\nNow we are ready to choose W                   . Suppose W is chosen as (e1eT               2 \u2212    e2eT 1 ). With this choice of W    , we\nhave that,\nLikewise,                                 \u2225Z\u2225F = \u2225UW             \u2225F =          \u2225Ue1\u22252    2 + \u2225Ue2\u22252      2                          (150)\n                                         \u2225W    ZT \u2225F = \u2225W          W T U T \u2225F                                                       (151)\n                                                         = \u2225U(e1eT       1 + e2eT    2 )\u2225F                                          (152)\n                                                         =       \u2225Ue1\u22252  282 + \u2225Ue2\u22252       2 = \u2225Z\u2225F .                              (153)", "md": "In general, when $$\\Delta$$ is small in absolute value, by the support condition eq. (138) of W, one would expect\n\n$$W\\Delta D(U) \\approx D(U)W$$ in case $$\\Delta$$ is small. In particular, defining the diagonal matrix\n\n$$L = \\text{diag}(2, -\\Delta^2, 0, \\ldots)$$, we have that,\n\n$$W D(U) - D(U)W = W L - LW.$$ (139)\n\nFrom the gradient and Hessian computations in Lemma 42,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2R_\\beta(U)]\\text{vec}(Z) &= \\text{Tr}(D(U) \\cdot Z^T Z) - \\sum_{i=1}^k G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2 \\\\\n&= \\text{Tr}(W D(U)Z^T U) - \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2 \\\\\n&= \\langle \\nabla R_\\beta(U), W Z^T \\rangle + \\text{Tr}((W L - LW) \\cdot Z^T U) - \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2\n\\end{aligned}$$ (140)\n\nwhere the last equation uses the gradient computation in Lemma 42 and the fact that W and D(U) approximately commute per eq. (139). Likewise, analyzing the second-order behavior of L using the Hessian computations in Lemma 40 results in the following set of equations,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2L(U)]\\text{vec}(Z) &= \\ldots \\\\\n&= \\ldots \\\\\n&= \\ldots \\\\\n&= \\langle \\nabla L(U), W Z^T \\rangle,\n\\end{aligned}$$ (147)\n\nwhere (i) uses the fact that UZ^T + ZU^T = U(W^T + W)U^T = 0 and the last equation uses the gradient computations in Lemma 40.\n\nSumming up eqs. (142) and (147), we get,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R_\\beta)(U)]\\text{vec}(Z) &= \\ldots \\\\\n&= \\ldots\n\\end{aligned}$$ (148)\n\nNoting that U is an $$\\epsilon$$-approximate stationary point of L + $$\\lambda R_\\beta$$, by Cauchy\u2013Bunyakovsky\u2013Schwarz inequality,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R_\\beta)(U)]\\text{vec}(Z) &\\leq \\epsilon \\|W Z^T\\|_F + \\lambda \\text{Tr}((W L - LW) \\cdot Z^T U) - \\lambda \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2\n\\end{aligned}$$ (149)\n\nNow we are ready to choose W. Suppose W is chosen as $$(e_1e_2^T - e_2e_1^T)$$. With this choice of W, we have that,\n\nLikewise,\n\n$$\\begin{aligned}\n\\|Z\\|_F &= \\|UW\\|_F = \\|Ue_1\\|_2^2 + \\|Ue_2\\|_2^2 \\\\\n\\|W Z^T\\|_F &= \\|W W^T U^T\\|_F \\\\\n&= \\|U(e_1e_1^T + e_2e_2^T)\\|_F \\\\\n&= \\|Ue_1\\|_2^2 + \\|Ue_2\\|_2^2 = \\|Z\\|_F.\n\\end{aligned}$$", "images": [], "items": [{"type": "text", "value": "In general, when $$\\Delta$$ is small in absolute value, by the support condition eq. (138) of W, one would expect\n\n$$W\\Delta D(U) \\approx D(U)W$$ in case $$\\Delta$$ is small. In particular, defining the diagonal matrix\n\n$$L = \\text{diag}(2, -\\Delta^2, 0, \\ldots)$$, we have that,\n\n$$W D(U) - D(U)W = W L - LW.$$ (139)\n\nFrom the gradient and Hessian computations in Lemma 42,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2R_\\beta(U)]\\text{vec}(Z) &= \\text{Tr}(D(U) \\cdot Z^T Z) - \\sum_{i=1}^k G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2 \\\\\n&= \\text{Tr}(W D(U)Z^T U) - \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2 \\\\\n&= \\langle \\nabla R_\\beta(U), W Z^T \\rangle + \\text{Tr}((W L - LW) \\cdot Z^T U) - \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2\n\\end{aligned}$$ (140)\n\nwhere the last equation uses the gradient computation in Lemma 42 and the fact that W and D(U) approximately commute per eq. (139). Likewise, analyzing the second-order behavior of L using the Hessian computations in Lemma 40 results in the following set of equations,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2L(U)]\\text{vec}(Z) &= \\ldots \\\\\n&= \\ldots \\\\\n&= \\ldots \\\\\n&= \\langle \\nabla L(U), W Z^T \\rangle,\n\\end{aligned}$$ (147)\n\nwhere (i) uses the fact that UZ^T + ZU^T = U(W^T + W)U^T = 0 and the last equation uses the gradient computations in Lemma 40.\n\nSumming up eqs. (142) and (147), we get,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R_\\beta)(U)]\\text{vec}(Z) &= \\ldots \\\\\n&= \\ldots\n\\end{aligned}$$ (148)\n\nNoting that U is an $$\\epsilon$$-approximate stationary point of L + $$\\lambda R_\\beta$$, by Cauchy\u2013Bunyakovsky\u2013Schwarz inequality,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R_\\beta)(U)]\\text{vec}(Z) &\\leq \\epsilon \\|W Z^T\\|_F + \\lambda \\text{Tr}((W L - LW) \\cdot Z^T U) - \\lambda \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2\n\\end{aligned}$$ (149)\n\nNow we are ready to choose W. Suppose W is chosen as $$(e_1e_2^T - e_2e_1^T)$$. With this choice of W, we have that,\n\nLikewise,\n\n$$\\begin{aligned}\n\\|Z\\|_F &= \\|UW\\|_F = \\|Ue_1\\|_2^2 + \\|Ue_2\\|_2^2 \\\\\n\\|W Z^T\\|_F &= \\|W W^T U^T\\|_F \\\\\n&= \\|U(e_1e_1^T + e_2e_2^T)\\|_F \\\\\n&= \\|Ue_1\\|_2^2 + \\|Ue_2\\|_2^2 = \\|Z\\|_F.\n\\end{aligned}$$", "md": "In general, when $$\\Delta$$ is small in absolute value, by the support condition eq. (138) of W, one would expect\n\n$$W\\Delta D(U) \\approx D(U)W$$ in case $$\\Delta$$ is small. In particular, defining the diagonal matrix\n\n$$L = \\text{diag}(2, -\\Delta^2, 0, \\ldots)$$, we have that,\n\n$$W D(U) - D(U)W = W L - LW.$$ (139)\n\nFrom the gradient and Hessian computations in Lemma 42,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2R_\\beta(U)]\\text{vec}(Z) &= \\text{Tr}(D(U) \\cdot Z^T Z) - \\sum_{i=1}^k G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2 \\\\\n&= \\text{Tr}(W D(U)Z^T U) - \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2 \\\\\n&= \\langle \\nabla R_\\beta(U), W Z^T \\rangle + \\text{Tr}((W L - LW) \\cdot Z^T U) - \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2\n\\end{aligned}$$ (140)\n\nwhere the last equation uses the gradient computation in Lemma 42 and the fact that W and D(U) approximately commute per eq. (139). Likewise, analyzing the second-order behavior of L using the Hessian computations in Lemma 40 results in the following set of equations,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2L(U)]\\text{vec}(Z) &= \\ldots \\\\\n&= \\ldots \\\\\n&= \\ldots \\\\\n&= \\langle \\nabla L(U), W Z^T \\rangle,\n\\end{aligned}$$ (147)\n\nwhere (i) uses the fact that UZ^T + ZU^T = U(W^T + W)U^T = 0 and the last equation uses the gradient computations in Lemma 40.\n\nSumming up eqs. (142) and (147), we get,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R_\\beta)(U)]\\text{vec}(Z) &= \\ldots \\\\\n&= \\ldots\n\\end{aligned}$$ (148)\n\nNoting that U is an $$\\epsilon$$-approximate stationary point of L + $$\\lambda R_\\beta$$, by Cauchy\u2013Bunyakovsky\u2013Schwarz inequality,\n\n$$\\begin{aligned}\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R_\\beta)(U)]\\text{vec}(Z) &\\leq \\epsilon \\|W Z^T\\|_F + \\lambda \\text{Tr}((W L - LW) \\cdot Z^T U) - \\lambda \\sum_{i=1} G(U)_{ii}\\langle Ze_i, Ue_i \\rangle^2\n\\end{aligned}$$ (149)\n\nNow we are ready to choose W. Suppose W is chosen as $$(e_1e_2^T - e_2e_1^T)$$. With this choice of W, we have that,\n\nLikewise,\n\n$$\\begin{aligned}\n\\|Z\\|_F &= \\|UW\\|_F = \\|Ue_1\\|_2^2 + \\|Ue_2\\|_2^2 \\\\\n\\|W Z^T\\|_F &= \\|W W^T U^T\\|_F \\\\\n&= \\|U(e_1e_1^T + e_2e_2^T)\\|_F \\\\\n&= \\|Ue_1\\|_2^2 + \\|Ue_2\\|_2^2 = \\|Z\\|_F.\n\\end{aligned}$$"}]}, {"page": 29, "text": " Simplifying eq. (149), the last term can be evaluated to,\n                              k\n                            i=1   G(U)ii\u27e8Zei, Uei\u27e92 = (G(U)11 + G(U)22)\u27e8Ue1, Ue2\u27e92.                                                      (154)\nWith this choice of parameters, eq. (149) simplifies to,\n                vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z)                                                                                           (155)\n                 \u2264   \u03f5\u2225Z\u2225F + \u03bbTr((W            L \u2212    LW    ) \u00b7 ZT U) \u2212       \u03bb(G(U)11 + G(U)22)\u27e8Ue1, Ue2\u27e92                              (156)\n Next we prove a lemma upper bounding the middle term in eq. (149).\n Lemma 25. The error term \u03bbTr((W                      L \u2212    LW    ) \u00b7 ZT U) can be upper bounded by,\n                         Tr((W     L \u2212    LW    ) \u00b7 ZT U) \u2264       40\u22062 max                     2 min                                     (157)\n The proof is deferred to Appendix B.2. Combining eq. (156) with Lemma 25, i\u2208{1,2} \u2225Uei\u22252        i\u2208{1,2} \u2225Uei\u22252.\n           vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z) \u2264                          \u03f5\u2225Z\u2225F + 40\u03bb\u22062\u2225Z\u22252              F    min\n                                                                                                       i\u2208{1,2} \u2225Uei\u22252\n                                                                        \u2212   \u03bb(G(U)11 + G(U)22)\u27e8Ue1, Ue2\u27e92                                (158)\n Therefore, considering any approximate stationary point U that satisfies,\n                                                                              \u03f5\n                  \u03bb(G(U)11 + G(U)22)\u27e8Ue1, Ue2\u27e92         \u2225Z\u22252  F       \u2265    \u2225Z\u2225F      + 40\u03bb\u22062 min   i\u2208{1,2} \u2225Uei\u22252 + \u03b3                     (C1)\n the Hessian \u22072(L + \u03bbR\u03b2)(U) has an eigenvalue which is at most \u2212\u03b3. Note by a similar analysis as\n eq. (226),\n                        G(U)11 + G(U)22 \u2265                 max                                   \u2225Uei\u22252   2 + 4\u03b2                          (159)\n                                                        i\u2208{1,2} G(U)ii \u2265            max\n                                                                                  i\u2208{1,2}    (\u2225Uei\u22252    2 + \u03b2)5/2\n                                                                              \u2265     max                   1                              (160)\n                                                                                  i\u2208{1,2}    3\u2225Uei\u22253    2 + 12\u03b23/2\n                                                                                                  1\n                                                                              \u2265     max      5\u2225Uei\u22253      .                              (161)\n where the second-to-last inequality uses the fact that mini\u2208{1,2} \u2225Uei\u22252 \u2265       i\u2208{1,2}               2          2\u221a\u03b2. Therefore, a\n sufficient condition to guarantee eq. (C1) is,\n                      max           \u03bb        \u00b7 \u27e8Ue1, Ue2\u27e92         \u2265       \u03f5     + 40\u03bb\u22062 min                                             (162)\n                    i\u2208{1,2}    5\u2225Uei\u22253    2         \u2225Z\u22252  F            \u2225Z\u2225F                     i\u2208{1,2} \u2225Uei\u22252 + \u03b3.\n From eq. (137), we have that,\n                                                        \u27e8Ue1, Ue2\u27e92                \u03f52\n                                                             \u2225Z\u22252  F        \u2264   \u03bb2\u22062                                                     (163)\nWhen \u2206        is large, the columns are nearly orthogonal by eq. (163). When \u2206                                  is small, the barrier to\n setting up a negative eigenvalue of the Hessian of L + \u03bbR\u03b2 is small by eq. (162). In particular, under\n the \u201csmall-\u2206\u201d condition,\n                                                                            \u03f5\n                                                 \u2206   \u2264   4\u03bb1/2 \u00b7 mini\u2208{1,2} \u2225Uei\u22252                                                       (164)\n \u22072(L + \u03bbR\u03b2)(U) has a negative eigenvalue taking value at most \u2212\u03b3 under the sufficient condition,\n                          max           \u03bb        \u00b7 \u27e8Ue1, Ue2\u27e92         \u2265       \u03f5     + 40\u03bb\u22062 min                                         (165)\n                        i\u2208{1,2}    5\u2225Uei\u22253    2         \u2225Z\u22252  F            \u2225Z\u2225F                     i\u2208{1,2} \u2225Uei\u22252 + \u03b3\n                 \u21d0=       max           \u03bb        \u00b7 \u27e8Ue1, Ue2\u27e92         \u2265       \u03f5     + 40\u03f5 mini\u2208{1,2} \u2225Uei\u22252               + \u03b3           (166)\n                        i\u2208{1,2}    5\u2225Uei\u22253    2         \u2225Z\u22252  F            \u2225Z\u2225F           16 mini\u2208{1,2} \u2225Uei\u22252          2\n                 \u21d0= \u27e8Ue1, Ue2\u27e92             \u2265   18\u03f5 \u00b7 mini\u2208{1,2} \u2225Uei\u22252          2  + 5\u03b3                           2                     (167)\n                             \u2225Z\u22252                               \u03bb                        \u03bb \u00b7 min\n                                   F                                                           i\u2208{1,2} \u2225Uei\u22253\n                                                                       29", "md": "Simplifying eq. (149), the last term can be evaluated to,\n\n$$\n\\sum_{i=1}^{k} G(U)_{ii}\\langle Z_{ei}, U_{ei}\\rangle^2 = (G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{154}\n$$\n\nWith this choice of parameters, eq. (149) simplifies to,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq \\epsilon \\|Z\\|_F + \\lambda\\text{Tr}((W^T L - LW) \\cdot Z^T U) - \\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{155}\n$$\n\nNext we prove a lemma upper bounding the middle term in eq. (149).\n\nLemma 25. The error term $\\lambda\\text{Tr}((W^T L - LW) \\cdot Z^T U)$ can be upper bounded by,\n\n$$\n\\text{Tr}((W^T L - LW) \\cdot Z^T U) \\leq 40\\Delta^2 \\max \\frac{2}{\\min}. \\tag{157}\n$$\n\nThe proof is deferred to Appendix B.2. Combining eq. (156) with Lemma 25,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq \\epsilon \\|Z\\|_F + 40\\lambda\\Delta^2\\|Z\\|_2^2 \\frac{F}{\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} - \\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{158}\n$$\n\nTherefore, considering any approximate stationary point U that satisfies,\n\n$$\n\\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon \\|Z\\|_2^F \\geq \\|Z\\|_F + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} + \\gamma \\tag{C1}\n$$\n\nthe Hessian $\\nabla^2(L + \\lambda R\\beta)(U)$ has an eigenvalue which is at most $-\\gamma$. Note by a similar analysis as eq. (226),\n\n$$\nG(U)_{11} + G(U)_{22} \\geq \\max \\frac{\\|U_{ei}\\|_2^2}{2} + 4\\beta. \\tag{159}\n$$\n\n$$\nG(U)_{ii} \\geq \\max_{i\\in\\{1,2\\}} \\left(\\frac{\\|U_{ei}\\|_2^2 + \\beta}{5/2}\\right) \\geq \\max_{i\\in\\{1,2\\}} \\frac{1}{3\\|U_{ei}\\|_3^2 + 12\\beta^{3/2}} \\geq \\max_{i\\in\\{1,2\\}} 5\\|U_{ei}\\|_3. \\tag{160, 161}\n$$\n\nwhere the second-to-last inequality uses the fact that $\\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2 \\geq \\max_{i\\in\\{1,2\\}} 2\\sqrt{\\beta}$. Therefore, a sufficient condition to guarantee eq. (C1) is,\n\n$$\n\\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_3} \\frac{2}{F} + \\gamma. \\tag{162}\n$$\n\nFrom eq. (137), we have that,\n\n$$\n\\langle U_{e1}, U_{e2}\\rangle^2 \\frac{\\epsilon^2}{\\|Z\\|_2^F} \\leq \\lambda^2\\Delta^2. \\tag{163}\n$$\n\nWhen $\\Delta$ is large, the columns are nearly orthogonal by eq. (163). When $\\Delta$ is small, the barrier to setting up a negative eigenvalue of the Hessian of $L + \\lambda R\\beta$ is small by eq. (162). In particular, under the \u201csmall-$\\Delta$\u201d condition,\n\n$$\n\\Delta \\leq 4\\lambda^{1/2} \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2. \\tag{164}\n$$\n\nThe Hessian $\\nabla^2(L + \\lambda R\\beta)(U)$ has a negative eigenvalue taking value at most $-\\gamma$ under the sufficient condition,\n\n$$\n\\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_3} \\frac{2}{F} + \\gamma \\tag{165}\n$$\n\n$$\n\\Rightarrow \\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\epsilon \\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} + \\gamma \\tag{166}\n$$\n\n$$\n\\Rightarrow \\langle U_{e1}, U_{e2}\\rangle^2 \\geq 18\\epsilon \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2^2 + 5\\gamma^2. \\tag{167}\n$$\n\n$$\n\\frac{\\|Z\\|_2}{F} \\leq \\frac{\\lambda}{\\lambda} \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_3. \\tag{29}\n$$", "images": [], "items": [{"type": "text", "value": "Simplifying eq. (149), the last term can be evaluated to,\n\n$$\n\\sum_{i=1}^{k} G(U)_{ii}\\langle Z_{ei}, U_{ei}\\rangle^2 = (G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{154}\n$$\n\nWith this choice of parameters, eq. (149) simplifies to,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq \\epsilon \\|Z\\|_F + \\lambda\\text{Tr}((W^T L - LW) \\cdot Z^T U) - \\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{155}\n$$\n\nNext we prove a lemma upper bounding the middle term in eq. (149).\n\nLemma 25. The error term $\\lambda\\text{Tr}((W^T L - LW) \\cdot Z^T U)$ can be upper bounded by,\n\n$$\n\\text{Tr}((W^T L - LW) \\cdot Z^T U) \\leq 40\\Delta^2 \\max \\frac{2}{\\min}. \\tag{157}\n$$\n\nThe proof is deferred to Appendix B.2. Combining eq. (156) with Lemma 25,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq \\epsilon \\|Z\\|_F + 40\\lambda\\Delta^2\\|Z\\|_2^2 \\frac{F}{\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} - \\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{158}\n$$\n\nTherefore, considering any approximate stationary point U that satisfies,\n\n$$\n\\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon \\|Z\\|_2^F \\geq \\|Z\\|_F + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} + \\gamma \\tag{C1}\n$$\n\nthe Hessian $\\nabla^2(L + \\lambda R\\beta)(U)$ has an eigenvalue which is at most $-\\gamma$. Note by a similar analysis as eq. (226),\n\n$$\nG(U)_{11} + G(U)_{22} \\geq \\max \\frac{\\|U_{ei}\\|_2^2}{2} + 4\\beta. \\tag{159}\n$$\n\n$$\nG(U)_{ii} \\geq \\max_{i\\in\\{1,2\\}} \\left(\\frac{\\|U_{ei}\\|_2^2 + \\beta}{5/2}\\right) \\geq \\max_{i\\in\\{1,2\\}} \\frac{1}{3\\|U_{ei}\\|_3^2 + 12\\beta^{3/2}} \\geq \\max_{i\\in\\{1,2\\}} 5\\|U_{ei}\\|_3. \\tag{160, 161}\n$$\n\nwhere the second-to-last inequality uses the fact that $\\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2 \\geq \\max_{i\\in\\{1,2\\}} 2\\sqrt{\\beta}$. Therefore, a sufficient condition to guarantee eq. (C1) is,\n\n$$\n\\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_3} \\frac{2}{F} + \\gamma. \\tag{162}\n$$\n\nFrom eq. (137), we have that,\n\n$$\n\\langle U_{e1}, U_{e2}\\rangle^2 \\frac{\\epsilon^2}{\\|Z\\|_2^F} \\leq \\lambda^2\\Delta^2. \\tag{163}\n$$\n\nWhen $\\Delta$ is large, the columns are nearly orthogonal by eq. (163). When $\\Delta$ is small, the barrier to setting up a negative eigenvalue of the Hessian of $L + \\lambda R\\beta$ is small by eq. (162). In particular, under the \u201csmall-$\\Delta$\u201d condition,\n\n$$\n\\Delta \\leq 4\\lambda^{1/2} \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2. \\tag{164}\n$$\n\nThe Hessian $\\nabla^2(L + \\lambda R\\beta)(U)$ has a negative eigenvalue taking value at most $-\\gamma$ under the sufficient condition,\n\n$$\n\\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_3} \\frac{2}{F} + \\gamma \\tag{165}\n$$\n\n$$\n\\Rightarrow \\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\epsilon \\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} + \\gamma \\tag{166}\n$$\n\n$$\n\\Rightarrow \\langle U_{e1}, U_{e2}\\rangle^2 \\geq 18\\epsilon \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2^2 + 5\\gamma^2. \\tag{167}\n$$\n\n$$\n\\frac{\\|Z\\|_2}{F} \\leq \\frac{\\lambda}{\\lambda} \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_3. \\tag{29}\n$$", "md": "Simplifying eq. (149), the last term can be evaluated to,\n\n$$\n\\sum_{i=1}^{k} G(U)_{ii}\\langle Z_{ei}, U_{ei}\\rangle^2 = (G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{154}\n$$\n\nWith this choice of parameters, eq. (149) simplifies to,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq \\epsilon \\|Z\\|_F + \\lambda\\text{Tr}((W^T L - LW) \\cdot Z^T U) - \\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{155}\n$$\n\nNext we prove a lemma upper bounding the middle term in eq. (149).\n\nLemma 25. The error term $\\lambda\\text{Tr}((W^T L - LW) \\cdot Z^T U)$ can be upper bounded by,\n\n$$\n\\text{Tr}((W^T L - LW) \\cdot Z^T U) \\leq 40\\Delta^2 \\max \\frac{2}{\\min}. \\tag{157}\n$$\n\nThe proof is deferred to Appendix B.2. Combining eq. (156) with Lemma 25,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq \\epsilon \\|Z\\|_F + 40\\lambda\\Delta^2\\|Z\\|_2^2 \\frac{F}{\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} - \\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2. \\tag{158}\n$$\n\nTherefore, considering any approximate stationary point U that satisfies,\n\n$$\n\\lambda(G(U)_{11} + G(U)_{22})\\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon \\|Z\\|_2^F \\geq \\|Z\\|_F + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} + \\gamma \\tag{C1}\n$$\n\nthe Hessian $\\nabla^2(L + \\lambda R\\beta)(U)$ has an eigenvalue which is at most $-\\gamma$. Note by a similar analysis as eq. (226),\n\n$$\nG(U)_{11} + G(U)_{22} \\geq \\max \\frac{\\|U_{ei}\\|_2^2}{2} + 4\\beta. \\tag{159}\n$$\n\n$$\nG(U)_{ii} \\geq \\max_{i\\in\\{1,2\\}} \\left(\\frac{\\|U_{ei}\\|_2^2 + \\beta}{5/2}\\right) \\geq \\max_{i\\in\\{1,2\\}} \\frac{1}{3\\|U_{ei}\\|_3^2 + 12\\beta^{3/2}} \\geq \\max_{i\\in\\{1,2\\}} 5\\|U_{ei}\\|_3. \\tag{160, 161}\n$$\n\nwhere the second-to-last inequality uses the fact that $\\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2 \\geq \\max_{i\\in\\{1,2\\}} 2\\sqrt{\\beta}$. Therefore, a sufficient condition to guarantee eq. (C1) is,\n\n$$\n\\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_3} \\frac{2}{F} + \\gamma. \\tag{162}\n$$\n\nFrom eq. (137), we have that,\n\n$$\n\\langle U_{e1}, U_{e2}\\rangle^2 \\frac{\\epsilon^2}{\\|Z\\|_2^F} \\leq \\lambda^2\\Delta^2. \\tag{163}\n$$\n\nWhen $\\Delta$ is large, the columns are nearly orthogonal by eq. (163). When $\\Delta$ is small, the barrier to setting up a negative eigenvalue of the Hessian of $L + \\lambda R\\beta$ is small by eq. (162). In particular, under the \u201csmall-$\\Delta$\u201d condition,\n\n$$\n\\Delta \\leq 4\\lambda^{1/2} \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2. \\tag{164}\n$$\n\nThe Hessian $\\nabla^2(L + \\lambda R\\beta)(U)$ has a negative eigenvalue taking value at most $-\\gamma$ under the sufficient condition,\n\n$$\n\\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\lambda\\Delta^2\\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_3} \\frac{2}{F} + \\gamma \\tag{165}\n$$\n\n$$\n\\Rightarrow \\max_{i\\in\\{1,2\\}} \\lambda \\cdot \\langle U_{e1}, U_{e2}\\rangle^2 \\geq \\epsilon + 40\\epsilon \\min_{i\\in\\{1,2\\} \\|U_{ei}\\|_2} + \\gamma \\tag{166}\n$$\n\n$$\n\\Rightarrow \\langle U_{e1}, U_{e2}\\rangle^2 \\geq 18\\epsilon \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_2^2 + 5\\gamma^2. \\tag{167}\n$$\n\n$$\n\\frac{\\|Z\\|_2}{F} \\leq \\frac{\\lambda}{\\lambda} \\cdot \\min_{i\\in\\{1,2\\}} \\|U_{ei}\\|_3. \\tag{29}\n$$"}]}, {"page": 30, "text": "From eq. (137), under the large-\u2206      condition, we have that,\n                   \u27e8Ue1, Ue2\u27e92    \u2264   \u03f52 16\u03bb   \u00b7 min           2 \u2264  16\u03f5                  2.            (168)\n                       \u2225Z\u22252           \u03bb2   \u03f5                         \u03bb \u00b7 min\nThis completes the proof.   F                   i\u2208{1,2} \u2225Uei\u22252            i\u2208{1,2} \u2225Uei\u22252\nLemma 24 establishes the role of the regularizer - in making the large columns of U nearly orthogonal\nat approximate second order stationary points. Next we show that at second order stationary points,\nUU T \u2248    U\u22c6U T\u22c6 .\n                                                      30", "md": "From eq. (137), under the large-\u2206 condition, we have that,\n\n$$\n\\langle Ue1, Ue2 \\rangle^2 \\leq \\frac{\\epsilon^2}{16\\lambda} \\cdot \\min_{2} \\leq 16\\epsilon^2. \\quad (168)\n$$\n$$\n\\frac{\\|Z\\|_2}{\\lambda^2} \\leq \\frac{\\epsilon}{\\lambda} \\cdot \\min_{i\\in\\{1,2\\}} \\frac{\\|U_{ei}\\|_2}{\\|U_{ei}\\|_2}\n$$\nThis completes the proof.\n\nLemma 24 establishes the role of the regularizer - in making the large columns of U nearly orthogonal at approximate second order stationary points. Next we show that at second order stationary points,\n\n$$\nUU^T \\approx U^*U^{T*}. \\quad 30\n$$", "images": [], "items": [{"type": "text", "value": "From eq. (137), under the large-\u2206 condition, we have that,\n\n$$\n\\langle Ue1, Ue2 \\rangle^2 \\leq \\frac{\\epsilon^2}{16\\lambda} \\cdot \\min_{2} \\leq 16\\epsilon^2. \\quad (168)\n$$\n$$\n\\frac{\\|Z\\|_2}{\\lambda^2} \\leq \\frac{\\epsilon}{\\lambda} \\cdot \\min_{i\\in\\{1,2\\}} \\frac{\\|U_{ei}\\|_2}{\\|U_{ei}\\|_2}\n$$\nThis completes the proof.\n\nLemma 24 establishes the role of the regularizer - in making the large columns of U nearly orthogonal at approximate second order stationary points. Next we show that at second order stationary points,\n\n$$\nUU^T \\approx U^*U^{T*}. \\quad 30\n$$", "md": "From eq. (137), under the large-\u2206 condition, we have that,\n\n$$\n\\langle Ue1, Ue2 \\rangle^2 \\leq \\frac{\\epsilon^2}{16\\lambda} \\cdot \\min_{2} \\leq 16\\epsilon^2. \\quad (168)\n$$\n$$\n\\frac{\\|Z\\|_2}{\\lambda^2} \\leq \\frac{\\epsilon}{\\lambda} \\cdot \\min_{i\\in\\{1,2\\}} \\frac{\\|U_{ei}\\|_2}{\\|U_{ei}\\|_2}\n$$\nThis completes the proof.\n\nLemma 24 establishes the role of the regularizer - in making the large columns of U nearly orthogonal at approximate second order stationary points. Next we show that at second order stationary points,\n\n$$\nUU^T \\approx U^*U^{T*}. \\quad 30\n$$"}]}, {"page": 31, "text": " Lemma 26. Consider an \u03f5-approximate first-order stationary point of L + \u03bbR\u03b2, U. If \u2225UU T \u2212\n U\u22c6U T \u22c6  \u2225F \u2265      8 max       \u03f52/3k1/6    , \u03bb  2k  \u03b2    , then the Hessian of L + \u03bbR\u03b2 at U has a large negative\n eigenvalue,\n                                                                                     1\n                           \u03bbmin[\u22072(L + \u03bbR\u03b2)(U)]vec(Z) \u2264                        \u2212  \u221a  2k  \u2225UU T \u2212        U\u22c6U T \u22c6  \u2225F .                    (169)\n In other words, at an (\u03f5, \u03b3)-approximate second order stationary point of L + \u03bbR\u03b2, U,\n                                 \u2225UU T \u2212       U\u22c6U T                        \u03f52/3k1/6, \u03bb         2k      \u221a  k                             (170)\n                                                      \u22c6 \u2225F \u2272     max                             \u03b2 , \u03b3\n Proof. In the rest of this proof we expand U \u22c6                    to a Rd\u00d7k matrix by appending with 0 columns. This\n allows us to define R\u22c6             \u2208   arg minR:RRT =RT R=I \u2225U \u2212                  U\u22c6R\u22252    F , breaking ties arbitrarily. Define\n Z = U \u2212       U\u22c6R\u22c6. With this choice of Z, we invoke 3 results from [36] (Lemmas 7, 40 and 41),\n                                                \u2225ZZT \u22252    F \u2264    2\u2225UU T \u2212        U\u22c6U T  \u22c6  \u22252                                           (171)\n                                                                                             F .\n And,\n                                                                  1\n And finally a bound on the Hessian,      \u2225ZU T \u22252   F \u2264     2 \u221a  2 \u2212   1\u2225UU T \u2212        U\u22c6U T  \u22c6  \u22252F .                                  (172)\n vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z) \u2264                          2\u2225ZZT \u22252     F \u2212    6\u2225UU T \u2212        U\u22c6U T  \u22c6 \u22252F + 4\u27e8\u2207(L + \u03bbR\u03b2)(U), Z\u27e9\n                                                              + \u03bb     vec(Z)T [\u22072R\u03b2(U)]vec(Z) \u2212                     4\u03bb\u27e8\u2207R\u03b2(U), Z\u27e9        (173) .\n Plugging eq. (171) into eq. (173), and using the fact that U is an \u03f5-approximate first order stationary\n point,\n  vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z) \u2264                          \u22122\u2225UU T \u2212          U\u22c6U T  \u22c6 \u22252F + 4\u03f5\u2225Z\u2225F\n In the next lemma we upper bound the last term.               + \u03bb     vec(Z)T [\u22072R\u03b2(U)]vec(Z) \u2212                     4\u27e8\u2207R\u03b2(U), Z\u27e9        (174).\n Lemma 27. The contribution from the regularizer in the last term of eq. (174) can be bounded by,\n          vec(Z)T [\u22072R\u03b2(U)]vec(Z) \u2212                     4\u27e8R\u03b2(U), vec(Z)\u27e9             \u2264   8  2k \u03b2 \u2225UU T \u2212         U\u22c6U T  \u22c6 \u2225F .           (175)\n The proof of this result is deferred to Appendix B.2\n Finally, combining Lemma 27 with eq. (174),\n                   vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z) \u2264                          \u22122\u2225UU T \u2212          U\u22c6U T  \u22c6 \u22252F + 4\u03f5\u2225Z\u2225F\n                                                                                + 8\u03bb     2k  \u03b2 \u2225UU T \u2212        U\u22c6U T  \u22c6  \u2225F               (176)\nYet again using the inequality \u2225Z\u22254               F \u2264    k\u2225ZZT \u22252     F \u2264    2k\u2225UU T \u2212         U\u22c6U T \u22c6  \u22252F , this results in the bound,\n     vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z) \u2264                          \u22122\u2225UU T \u2212          U\u22c6U T  \u22c6 \u22252F + 8\u03f5k1/4\u2225UU T \u2212              U\u22c6U T  \u22c6 \u22251/2\n                                                                   + 8\u03bb31  2k  \u03b2 \u2225UU T \u2212         U\u22c6U T \u22c6  \u2225F                           F (177)", "md": "# Math Equations in HTML\n\nLemma 26. Consider an \u03f5-approximate first-order stationary point of L + \u03bbR\u03b2, U. If $$\\|UU^T - U^*U^T\\|_F \\geq 8 \\max\\left(\\frac{\\epsilon^{2/3}k^{1/6}}{\\lambda^2k\\beta}\\right)$$, then the Hessian of L + \u03bbR\u03b2 at U has a large negative eigenvalue,\n\n$$\\lambda_{\\text{min}}[\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -\\sqrt{2k} \\|UU^T - U^*U^T\\|_F.$$\n\nIn other words, at an (\u03f5, \u03b3)-approximate second order stationary point of L + \u03bbR\u03b2, U,\n\n$$\\|UU^T - U^*\\|_F \\leq \\epsilon^{2/3}k^{1/6}, \\lambda 2k \\sqrt{k}.$$\n\nProof. In the rest of this proof we expand $$U^*$$ to a $$R^{d \\times k}$$ matrix by appending with 0 columns. This allows us to define $$R^* \\in \\text{arg min } R: R^TR = R^TR = I \\|U - U^*R\\|_2^2$$, breaking ties arbitrarily. Define $$Z = U - U^*R^*$$. With this choice of Z, we invoke 3 results from [36] (Lemmas 7, 40 and 41),\n\n$$\\|ZZ^T\\|_2^F \\leq 2\\|UU^T - U^*U^T\\|_2^F.$$\n\nAnd,\n\nAnd finally a bound on the Hessian, $$\\|ZU^T\\|_2^F \\leq 2\\sqrt{2 - 1}\\|UU^T - U^*U^T\\|_2^F.$$\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_2^F - 6\\|UU^T - U^*U^T\\|_2^F + 4\\langle\\nabla(L + \\lambda R\\beta)(U), Z\\rangle$$\n\n$$+ \\lambda \\text{vec}(Z)^T [\\nabla^2R\\beta(U)]\\text{vec}(Z) - 4\\lambda\\langle\\nabla R\\beta(U), Z\\rangle$$.\n\nPlugging eq. (171) into eq. (173), and using the fact that U is an \u03f5-approximate first order stationary point,\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 4\\epsilon\\|Z\\|_F$$\n\nIn the next lemma we upper bound the last term.\n\n$$\\text{vec}(Z)^T [\\nabla^2R\\beta(U)]\\text{vec}(Z) - 4\\langle R\\beta(U), \\text{vec}(Z)\\rangle \\leq 8\\sqrt{2k\\beta}\\|UU^T - U^*U^T\\|_F.$$\n\nThe proof of this result is deferred to Appendix B.2\n\nFinally, combining Lemma 27 with eq. (174),\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 4\\epsilon\\|Z\\|_F$$\n\n$$+ 8\\lambda 2k\\beta\\|UU^T - U^*U^T\\|_F$$\n\nYet again using the inequality $$\\|Z\\|_F^4 \\leq k\\|ZZ^T\\|_2^F \\leq 2k\\|UU^T - U^*U^T\\|_2^F$$, this results in the bound,\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 8\\epsilon k^{1/4}\\|UU^T - U^*U^T\\|^{1/2}_1$$\n\n$$+ 8\\lambda^{3/2}2k\\beta\\|UU^T - U^*U^T\\|_F$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations in HTML", "md": "# Math Equations in HTML"}, {"type": "text", "value": "Lemma 26. Consider an \u03f5-approximate first-order stationary point of L + \u03bbR\u03b2, U. If $$\\|UU^T - U^*U^T\\|_F \\geq 8 \\max\\left(\\frac{\\epsilon^{2/3}k^{1/6}}{\\lambda^2k\\beta}\\right)$$, then the Hessian of L + \u03bbR\u03b2 at U has a large negative eigenvalue,\n\n$$\\lambda_{\\text{min}}[\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -\\sqrt{2k} \\|UU^T - U^*U^T\\|_F.$$\n\nIn other words, at an (\u03f5, \u03b3)-approximate second order stationary point of L + \u03bbR\u03b2, U,\n\n$$\\|UU^T - U^*\\|_F \\leq \\epsilon^{2/3}k^{1/6}, \\lambda 2k \\sqrt{k}.$$\n\nProof. In the rest of this proof we expand $$U^*$$ to a $$R^{d \\times k}$$ matrix by appending with 0 columns. This allows us to define $$R^* \\in \\text{arg min } R: R^TR = R^TR = I \\|U - U^*R\\|_2^2$$, breaking ties arbitrarily. Define $$Z = U - U^*R^*$$. With this choice of Z, we invoke 3 results from [36] (Lemmas 7, 40 and 41),\n\n$$\\|ZZ^T\\|_2^F \\leq 2\\|UU^T - U^*U^T\\|_2^F.$$\n\nAnd,\n\nAnd finally a bound on the Hessian, $$\\|ZU^T\\|_2^F \\leq 2\\sqrt{2 - 1}\\|UU^T - U^*U^T\\|_2^F.$$\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_2^F - 6\\|UU^T - U^*U^T\\|_2^F + 4\\langle\\nabla(L + \\lambda R\\beta)(U), Z\\rangle$$\n\n$$+ \\lambda \\text{vec}(Z)^T [\\nabla^2R\\beta(U)]\\text{vec}(Z) - 4\\lambda\\langle\\nabla R\\beta(U), Z\\rangle$$.\n\nPlugging eq. (171) into eq. (173), and using the fact that U is an \u03f5-approximate first order stationary point,\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 4\\epsilon\\|Z\\|_F$$\n\nIn the next lemma we upper bound the last term.\n\n$$\\text{vec}(Z)^T [\\nabla^2R\\beta(U)]\\text{vec}(Z) - 4\\langle R\\beta(U), \\text{vec}(Z)\\rangle \\leq 8\\sqrt{2k\\beta}\\|UU^T - U^*U^T\\|_F.$$\n\nThe proof of this result is deferred to Appendix B.2\n\nFinally, combining Lemma 27 with eq. (174),\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 4\\epsilon\\|Z\\|_F$$\n\n$$+ 8\\lambda 2k\\beta\\|UU^T - U^*U^T\\|_F$$\n\nYet again using the inequality $$\\|Z\\|_F^4 \\leq k\\|ZZ^T\\|_2^F \\leq 2k\\|UU^T - U^*U^T\\|_2^F$$, this results in the bound,\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 8\\epsilon k^{1/4}\\|UU^T - U^*U^T\\|^{1/2}_1$$\n\n$$+ 8\\lambda^{3/2}2k\\beta\\|UU^T - U^*U^T\\|_F$$", "md": "Lemma 26. Consider an \u03f5-approximate first-order stationary point of L + \u03bbR\u03b2, U. If $$\\|UU^T - U^*U^T\\|_F \\geq 8 \\max\\left(\\frac{\\epsilon^{2/3}k^{1/6}}{\\lambda^2k\\beta}\\right)$$, then the Hessian of L + \u03bbR\u03b2 at U has a large negative eigenvalue,\n\n$$\\lambda_{\\text{min}}[\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -\\sqrt{2k} \\|UU^T - U^*U^T\\|_F.$$\n\nIn other words, at an (\u03f5, \u03b3)-approximate second order stationary point of L + \u03bbR\u03b2, U,\n\n$$\\|UU^T - U^*\\|_F \\leq \\epsilon^{2/3}k^{1/6}, \\lambda 2k \\sqrt{k}.$$\n\nProof. In the rest of this proof we expand $$U^*$$ to a $$R^{d \\times k}$$ matrix by appending with 0 columns. This allows us to define $$R^* \\in \\text{arg min } R: R^TR = R^TR = I \\|U - U^*R\\|_2^2$$, breaking ties arbitrarily. Define $$Z = U - U^*R^*$$. With this choice of Z, we invoke 3 results from [36] (Lemmas 7, 40 and 41),\n\n$$\\|ZZ^T\\|_2^F \\leq 2\\|UU^T - U^*U^T\\|_2^F.$$\n\nAnd,\n\nAnd finally a bound on the Hessian, $$\\|ZU^T\\|_2^F \\leq 2\\sqrt{2 - 1}\\|UU^T - U^*U^T\\|_2^F.$$\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_2^F - 6\\|UU^T - U^*U^T\\|_2^F + 4\\langle\\nabla(L + \\lambda R\\beta)(U), Z\\rangle$$\n\n$$+ \\lambda \\text{vec}(Z)^T [\\nabla^2R\\beta(U)]\\text{vec}(Z) - 4\\lambda\\langle\\nabla R\\beta(U), Z\\rangle$$.\n\nPlugging eq. (171) into eq. (173), and using the fact that U is an \u03f5-approximate first order stationary point,\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 4\\epsilon\\|Z\\|_F$$\n\nIn the next lemma we upper bound the last term.\n\n$$\\text{vec}(Z)^T [\\nabla^2R\\beta(U)]\\text{vec}(Z) - 4\\langle R\\beta(U), \\text{vec}(Z)\\rangle \\leq 8\\sqrt{2k\\beta}\\|UU^T - U^*U^T\\|_F.$$\n\nThe proof of this result is deferred to Appendix B.2\n\nFinally, combining Lemma 27 with eq. (174),\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 4\\epsilon\\|Z\\|_F$$\n\n$$+ 8\\lambda 2k\\beta\\|UU^T - U^*U^T\\|_F$$\n\nYet again using the inequality $$\\|Z\\|_F^4 \\leq k\\|ZZ^T\\|_2^F \\leq 2k\\|UU^T - U^*U^T\\|_2^F$$, this results in the bound,\n\n$$\\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\leq -2\\|UU^T - U^*U^T\\|_2^F + 8\\epsilon k^{1/4}\\|UU^T - U^*U^T\\|^{1/2}_1$$\n\n$$+ 8\\lambda^{3/2}2k\\beta\\|UU^T - U^*U^T\\|_F$$"}]}, {"page": 32, "text": " If \u2225UU T \u2212        U\u22c6U T  \u22c6  \u2225F \u2265       8 max       \u03f52/3k1/6, \u03bb      2k  \u03b2    , the RHS is upper bounded by \u2212\u2225UU T \u2212\n U\u22c6U T \u22c6  \u22252F . Finally, noting that \u2225Z\u22254          F \u2264    k\u2225ZZT \u22252     F \u2264    2k\u2225UU T \u2212         U\u22c6U T  \u22c6  \u22252F , we have that,\n                         \u03bbmin[\u22072(L + \u03bbR\u03b2)(U)] \u2264                   vec(Z)T [\u22072(L + \u03bbR\u03b2)(U)]vec(Z)                                         (178)\n                                                                       1                 \u2225Z\u22252  F\n                                                              \u2264   \u2212  \u221a  2k  \u2225UU T \u2212       U\u22c6U T  \u22c6  \u2225F                                   (179)\n Let Vr denote the matrix with columns as the non-zero eigenvectors of U\u22c6U T                                    \u22c6  . In the next lemma,\n we show that for any stationary point U, all of its columns are almost entirely contained in the span\n of Vr, in that the angle between Uei and its projection onto V \u22a5                          r is almost 90\u25e6. In other words, the\n columns of U approximately lie in the correct subspace.\n Lemma 28. Consider an \u03f5-approximate first order stationary point of L + \u03bbR\u03b2, U, satisfying\n \u2225U\u2225op \u2264       3. Let Vr denote the matrix with columns as the non-zero eigenvectors of U\u22c6U T                                      \u22c6  . Then,\n assuming \u03b2 < 1,\n                                                      \u2225V \u22a5      r )T U\u2225F \u2264         3\u03f5/\u03bb.                                                 (180)\n                                                         r (V \u22a5\n Proof. By eq. (188) for Z = V \u22a5             r (V \u22a5 r )T U, by the approximate stationarity of U,\n                          2\u27e8UU T \u2212        U\u22c6U T  \u22c6 , UZT + ZU T \u27e9          + \u03bbTr(UD(U)ZT ) \u2264                \u03f5\u2225Z\u2225F                        (181)\n The LHS is lower bounded by,\n                            4Tr(UU T V \u22a5    r (V \u22a5 r )T UU T ) + \u03bbTr(UD(U)U T V \u22a5                r (V \u22a5 r )T )                           (182)\n                                                                         k\n                             = 4\u2225V \u22a5  r (V \u22a5 r )T UU T \u22252     F + \u03bb     i=1 (D(U))ii\u2225V \u22a5      r (V \u22a5 r )T Uei\u22252    2                     (183)\n                             (i)      k\n                             \u2265   \u03bb3  i=1  \u2225V \u22a5r (V \u22a5 r )T Uei\u22252     2                                                                    (184)\n                             = \u03bb 3 \u2225Z\u22252   F .                                                            2\u03b2+\u2225Uei\u22252    2            2\u03b2+9  (185)\n where (i) uses the fact that since \u2225U\u2225op \u2264                     3 and \u03b2 < 1, (D(U))ii =                (\u2225Uei\u22252  2+\u03b2)3/2 \u2265       (9+\u03b2)3/2 \u2265\n1/3. Combining with eq. (181) completes the proof.\n Lemma 29. Consider an \u03f5-approximate first order stationary point U of L + \u03bbR\u03b2. Let Vr be as\n defined in Lemma 28. Let S denote the set of columns i \u2208                             [k] such that \u2225Uei\u22252 \u2265             2\u221a\u03b2. Then, for\n any i \u2208    S,                                     \u2225V \u22a5r (V \u22a5 r )T Uei\u22252       \u2264      2\u03f5                                                 (186)\n                                                           \u2225Uei\u22252                   \u03bb\u03b21/4 .\n Note that the LHS is the cosine of the angle between Uei and its projection onto V \u22a5                                  r , (or the sine of\n the angle between Uei and its projection onto Vr). Likewise, for the remaining columns,\n                                                        \u2225V \u22a5r (V \u22a5r )T Uei\u22252     2 \u2264    2\u03f52\u221a\u03b2     .                                      (187)\n                                              i\u2208[k]\\S                                      \u03bb2\n Proof. At an \u03f5-approximate first order stationary point the gradient is upper bounded in L2-norm by\n \u03f5. From the gradient and Hessian computations in Lemma 39 and Lemma 42,\n                          2\u27e8UU T \u2212        U\u22c6U T  \u22c6 , UZT + ZU T \u27e9      32  + \u03bbTr(D(U)ZT U) \u2264                \u03f5\u2225Z\u2225F                        (188)", "md": "If $$\\|UU^T - U^*U^T\\|^F \\geq 8\\max(\\epsilon^{2/3}k^{1/6}, \\lambda^2k\\beta)$$, the RHS is upper bounded by $$-\\|UU^T - U^*U^T\\|^2_F$$. Finally, noting that $$\\|Z\\|^4_F \\leq k\\|ZZ^T\\|^2_F \\leq 2k\\|UU^T - U^*U^T\\|^2_F$$, we have that,\n\n$$\n\\lambda_{\\min}[\\nabla^2(L + \\lambda R\\beta)(U)] \\leq \\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\quad (178)\n$$\n\n$$\n\\begin{align*}\n&\\leq -\\sqrt{2k}\\|UU^T - U^*U^T\\|^F \\quad (179)\n\\end{align*}\n$$\n\nLet $V_r$ denote the matrix with columns as the non-zero eigenvectors of $U^*U^T$. In the next lemma, we show that for any stationary point $U$, all of its columns are almost entirely contained in the span of $V_r$, in that the angle between $Uei$ and its projection onto $V_r^\\perp$ is almost 90\u00b0. In other words, the columns of $U$ approximately lie in the correct subspace.\n\nLemma 28. Consider an $\\epsilon$-approximate first-order stationary point of $L + \\lambda R\\beta$, $U$, satisfying $\\|U\\|_{\\text{op}} \\leq 3$. Let $V_r$ denote the matrix with columns as the non-zero eigenvectors of $U^*U^T$. Then, assuming $\\beta < 1$,\n\n$$\n\\|V_r^\\perp)^T U\\|^F \\leq 3\\epsilon/\\lambda. \\quad (180)\n$$\n\nProof. By eq. (188) for $Z = V_r^\\perp(V_r^\\perp)^T U$, by the approximate stationarity of $U$,\n\n$$\n2\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + \\lambda\\text{Tr}(UD(U)Z^T) \\leq \\epsilon\\|Z\\|^F \\quad (181)\n$$\n\nThe LHS is lower bounded by,\n\n$$\n\\begin{align*}\n&4\\text{Tr}(UU^TV_r^\\perp(V_r^\\perp)^TUU^T) + \\lambda\\text{Tr}(UD(U)U^TV_r^\\perp(V_r^\\perp)^T) \\quad (182) \\\\\n&= 4\\|V_r^\\perp(V_r^\\perp)^TUU^T\\|^2_F + \\lambda\\sum_{i=1}^{k}(D(U))_{ii}\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\quad (183) \\\\\n&\\geq \\lambda^3\\sum_{i=1}^{k}\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\quad (184) \\\\\n&= \\lambda^3\\|Z\\|^2_F. \\quad (185)\n\\end{align*}\n$$\n\nwhere (i) uses the fact that since $\\|U\\|_{\\text{op}} \\leq 3$ and $\\beta < 1$, $(D(U))_{ii} = (\\|Uei\\|^2_2+\\beta)^{3/2} \\geq (9+\\beta)^{3/2} \\geq 1/3$. Combining with eq. (181) completes the proof.\n\nLemma 29. Consider an $\\epsilon$-approximate first-order stationary point $U$ of $L + \\lambda R\\beta$. Let $V_r$ be as defined in Lemma 28. Let $S$ denote the set of columns $i \\in [k]$ such that $\\|Uei\\|^2 \\geq 2\\sqrt{\\beta}$. Then, for any $i \\in S$,\n\n$$\n\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\leq 2\\epsilon \\quad (186)\n$$\n\n$$\n\\frac{\\|Uei\\|^2}{\\lambda\\beta^{1/4}}. \\quad (187)\n$$\n\nNote that the LHS is the cosine of the angle between $Uei$ and its projection onto $V_r^\\perp$ (or the sine of the angle between $Uei$ and its projection onto $V_r$). Likewise, for the remaining columns,\n\n$$\n\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\leq 2\\epsilon^2\\sqrt{\\beta} \\quad (187)\n$$\n\n$$\ni\\in[k]\\setminus S \\quad \\frac{\\lambda}{2}\n$$\n\nProof. At an $\\epsilon$-approximate first-order stationary point, the gradient is upper bounded in $L^2$-norm by $\\epsilon$. From the gradient and Hessian computations in Lemma 39 and Lemma 42,\n\n$$\n2\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + 32 + \\lambda\\text{Tr}(D(U)Z^TU) \\leq \\epsilon\\|Z\\|^F \\quad (188)\n$$", "images": [], "items": [{"type": "text", "value": "If $$\\|UU^T - U^*U^T\\|^F \\geq 8\\max(\\epsilon^{2/3}k^{1/6}, \\lambda^2k\\beta)$$, the RHS is upper bounded by $$-\\|UU^T - U^*U^T\\|^2_F$$. Finally, noting that $$\\|Z\\|^4_F \\leq k\\|ZZ^T\\|^2_F \\leq 2k\\|UU^T - U^*U^T\\|^2_F$$, we have that,\n\n$$\n\\lambda_{\\min}[\\nabla^2(L + \\lambda R\\beta)(U)] \\leq \\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\quad (178)\n$$\n\n$$\n\\begin{align*}\n&\\leq -\\sqrt{2k}\\|UU^T - U^*U^T\\|^F \\quad (179)\n\\end{align*}\n$$\n\nLet $V_r$ denote the matrix with columns as the non-zero eigenvectors of $U^*U^T$. In the next lemma, we show that for any stationary point $U$, all of its columns are almost entirely contained in the span of $V_r$, in that the angle between $Uei$ and its projection onto $V_r^\\perp$ is almost 90\u00b0. In other words, the columns of $U$ approximately lie in the correct subspace.\n\nLemma 28. Consider an $\\epsilon$-approximate first-order stationary point of $L + \\lambda R\\beta$, $U$, satisfying $\\|U\\|_{\\text{op}} \\leq 3$. Let $V_r$ denote the matrix with columns as the non-zero eigenvectors of $U^*U^T$. Then, assuming $\\beta < 1$,\n\n$$\n\\|V_r^\\perp)^T U\\|^F \\leq 3\\epsilon/\\lambda. \\quad (180)\n$$\n\nProof. By eq. (188) for $Z = V_r^\\perp(V_r^\\perp)^T U$, by the approximate stationarity of $U$,\n\n$$\n2\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + \\lambda\\text{Tr}(UD(U)Z^T) \\leq \\epsilon\\|Z\\|^F \\quad (181)\n$$\n\nThe LHS is lower bounded by,\n\n$$\n\\begin{align*}\n&4\\text{Tr}(UU^TV_r^\\perp(V_r^\\perp)^TUU^T) + \\lambda\\text{Tr}(UD(U)U^TV_r^\\perp(V_r^\\perp)^T) \\quad (182) \\\\\n&= 4\\|V_r^\\perp(V_r^\\perp)^TUU^T\\|^2_F + \\lambda\\sum_{i=1}^{k}(D(U))_{ii}\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\quad (183) \\\\\n&\\geq \\lambda^3\\sum_{i=1}^{k}\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\quad (184) \\\\\n&= \\lambda^3\\|Z\\|^2_F. \\quad (185)\n\\end{align*}\n$$\n\nwhere (i) uses the fact that since $\\|U\\|_{\\text{op}} \\leq 3$ and $\\beta < 1$, $(D(U))_{ii} = (\\|Uei\\|^2_2+\\beta)^{3/2} \\geq (9+\\beta)^{3/2} \\geq 1/3$. Combining with eq. (181) completes the proof.\n\nLemma 29. Consider an $\\epsilon$-approximate first-order stationary point $U$ of $L + \\lambda R\\beta$. Let $V_r$ be as defined in Lemma 28. Let $S$ denote the set of columns $i \\in [k]$ such that $\\|Uei\\|^2 \\geq 2\\sqrt{\\beta}$. Then, for any $i \\in S$,\n\n$$\n\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\leq 2\\epsilon \\quad (186)\n$$\n\n$$\n\\frac{\\|Uei\\|^2}{\\lambda\\beta^{1/4}}. \\quad (187)\n$$\n\nNote that the LHS is the cosine of the angle between $Uei$ and its projection onto $V_r^\\perp$ (or the sine of the angle between $Uei$ and its projection onto $V_r$). Likewise, for the remaining columns,\n\n$$\n\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\leq 2\\epsilon^2\\sqrt{\\beta} \\quad (187)\n$$\n\n$$\ni\\in[k]\\setminus S \\quad \\frac{\\lambda}{2}\n$$\n\nProof. At an $\\epsilon$-approximate first-order stationary point, the gradient is upper bounded in $L^2$-norm by $\\epsilon$. From the gradient and Hessian computations in Lemma 39 and Lemma 42,\n\n$$\n2\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + 32 + \\lambda\\text{Tr}(D(U)Z^TU) \\leq \\epsilon\\|Z\\|^F \\quad (188)\n$$", "md": "If $$\\|UU^T - U^*U^T\\|^F \\geq 8\\max(\\epsilon^{2/3}k^{1/6}, \\lambda^2k\\beta)$$, the RHS is upper bounded by $$-\\|UU^T - U^*U^T\\|^2_F$$. Finally, noting that $$\\|Z\\|^4_F \\leq k\\|ZZ^T\\|^2_F \\leq 2k\\|UU^T - U^*U^T\\|^2_F$$, we have that,\n\n$$\n\\lambda_{\\min}[\\nabla^2(L + \\lambda R\\beta)(U)] \\leq \\text{vec}(Z)^T [\\nabla^2(L + \\lambda R\\beta)(U)]\\text{vec}(Z) \\quad (178)\n$$\n\n$$\n\\begin{align*}\n&\\leq -\\sqrt{2k}\\|UU^T - U^*U^T\\|^F \\quad (179)\n\\end{align*}\n$$\n\nLet $V_r$ denote the matrix with columns as the non-zero eigenvectors of $U^*U^T$. In the next lemma, we show that for any stationary point $U$, all of its columns are almost entirely contained in the span of $V_r$, in that the angle between $Uei$ and its projection onto $V_r^\\perp$ is almost 90\u00b0. In other words, the columns of $U$ approximately lie in the correct subspace.\n\nLemma 28. Consider an $\\epsilon$-approximate first-order stationary point of $L + \\lambda R\\beta$, $U$, satisfying $\\|U\\|_{\\text{op}} \\leq 3$. Let $V_r$ denote the matrix with columns as the non-zero eigenvectors of $U^*U^T$. Then, assuming $\\beta < 1$,\n\n$$\n\\|V_r^\\perp)^T U\\|^F \\leq 3\\epsilon/\\lambda. \\quad (180)\n$$\n\nProof. By eq. (188) for $Z = V_r^\\perp(V_r^\\perp)^T U$, by the approximate stationarity of $U$,\n\n$$\n2\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + \\lambda\\text{Tr}(UD(U)Z^T) \\leq \\epsilon\\|Z\\|^F \\quad (181)\n$$\n\nThe LHS is lower bounded by,\n\n$$\n\\begin{align*}\n&4\\text{Tr}(UU^TV_r^\\perp(V_r^\\perp)^TUU^T) + \\lambda\\text{Tr}(UD(U)U^TV_r^\\perp(V_r^\\perp)^T) \\quad (182) \\\\\n&= 4\\|V_r^\\perp(V_r^\\perp)^TUU^T\\|^2_F + \\lambda\\sum_{i=1}^{k}(D(U))_{ii}\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\quad (183) \\\\\n&\\geq \\lambda^3\\sum_{i=1}^{k}\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\quad (184) \\\\\n&= \\lambda^3\\|Z\\|^2_F. \\quad (185)\n\\end{align*}\n$$\n\nwhere (i) uses the fact that since $\\|U\\|_{\\text{op}} \\leq 3$ and $\\beta < 1$, $(D(U))_{ii} = (\\|Uei\\|^2_2+\\beta)^{3/2} \\geq (9+\\beta)^{3/2} \\geq 1/3$. Combining with eq. (181) completes the proof.\n\nLemma 29. Consider an $\\epsilon$-approximate first-order stationary point $U$ of $L + \\lambda R\\beta$. Let $V_r$ be as defined in Lemma 28. Let $S$ denote the set of columns $i \\in [k]$ such that $\\|Uei\\|^2 \\geq 2\\sqrt{\\beta}$. Then, for any $i \\in S$,\n\n$$\n\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\leq 2\\epsilon \\quad (186)\n$$\n\n$$\n\\frac{\\|Uei\\|^2}{\\lambda\\beta^{1/4}}. \\quad (187)\n$$\n\nNote that the LHS is the cosine of the angle between $Uei$ and its projection onto $V_r^\\perp$ (or the sine of the angle between $Uei$ and its projection onto $V_r$). Likewise, for the remaining columns,\n\n$$\n\\|V_r^\\perp(V_r^\\perp)^TUei\\|^2_2 \\leq 2\\epsilon^2\\sqrt{\\beta} \\quad (187)\n$$\n\n$$\ni\\in[k]\\setminus S \\quad \\frac{\\lambda}{2}\n$$\n\nProof. At an $\\epsilon$-approximate first-order stationary point, the gradient is upper bounded in $L^2$-norm by $\\epsilon$. From the gradient and Hessian computations in Lemma 39 and Lemma 42,\n\n$$\n2\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + 32 + \\lambda\\text{Tr}(D(U)Z^TU) \\leq \\epsilon\\|Z\\|^F \\quad (188)\n$$"}]}, {"page": 33, "text": " Choosing Z = V \u22a5          r (V \u22a5 r )T U, and noting that U\u22c6               has rank r and is orthogonal to V \u22a5                r , the LHS\n simplifies as,\n                       \u03f5\u2225Z\u2225F \u2265        4Tr(UU T V \u22a5    r (V \u22a5 r )T UU T ) + \u03bbTr(UD(U)U T V \u22a5                r (V \u22a5 r )T )                 (189)\n                                          k\n                                  \u2265   \u03bb  i=1  D(U)ii\u2225V \u22a5     r (V \u22a5 r )T Uei\u22252    2                                                      (190)\n                                  \u2265   \u03bb        \u2225V \u22a5r (V \u22a5r )T Uei\u22252     2 ,                                                              (191)\n                                         i\u2208S           \u2225Uei\u22252\n where the last inequality uses the fact that for any column i such that \u2225Uei\u22252 \u2265                                  2\u221a\u03b2, since we have\n                           1\n that D(U)ii \u2265         \u2225Uei\u22252 by Lemma 43. Putting everything together, we get that,\n                                        \u03bb        \u2225Uei\u22252 \u00b7 \u2225V \u22a5    r (V \u22a5 r )T Uei\u22252     2  \u2264   \u03f5\u2225Z\u2225F                                     (192)\n                                           i\u2208S                        \u2225Uei\u22252    2\n Since \u2225Uei\u22252 \u2265             2\u221a\u03b2, rearranging the terms around and finally upper bounding \u2225Z\u2225F using\n Lemma 28 completes the proof of eq. (186). To prove eq. (187), notice from eq. (190) that,\n                                      \u03f5\u2225Z\u2225F \u2265        \u03bb  i\u2208[k]\\S   D(U)ii\u2225(V \u22a5     r )(V \u22a5  r )T Uei\u22252    2                               (193)\n                                                 \u2265      \u03bb               \u2225(V \u22a5r )(V \u22a5 r )T Uei\u22252     2                                    (194)\n                                                      2\u221a\u03b2     i\u2208[k]\\S                         \u2225Uei\u22252  2+2\u03b2              1\n which uses the fact that when \u2225Uei\u22252 \u2264                     2\u221a\u03b2, then D(U)ii =              (\u2225Uei\u22252  2+\u03b2)3/2 \u2265       2\u03b21/2 . Rearranging\n and substituting the bound on \u2225Z\u2225F from Lemma 28 completes the proof of eq. (187).\n Lemma 30. Consider any \u03f5-approximate first-order stationary point of L + \u03bbR\u03b2, U, and let Vr\n be as defined in Lemma 28. Under the assumption that \u03f5 \u2264                                \u03bb/2, for any column i \u2208              [k] such that\n \u2225VrV T r Uei\u22252 \u2264        \u2225V \u22a5r (V \u22a5 r )T Uei\u22252, \u2225Uei\u22252 \u2264              2\u221a\u03b2.\n Proof. Consider any i such that \u2225VrV T                  r Uei\u22252 \u2264         \u2225V \u22a5r (V \u22a5 r )T Uei\u22252. Therefore, from eq. (188),\n choosing Z = V \u22a5       r (V \u22a5 r )T UeieT    i ,\n \u03f5\u2225V \u22a5r (V \u22a5 r )T Uei\u22252\n  \u2265  4Tr(UU T V \u22a5     r (V \u22a5r )T UeieT     i U T ) + \u03bbTr(UD(U)eieT             i U T V \u22a5r (V \u22a5 r )T )\n  = 4\u2225U T V \u22a5   r (V \u22a5 r )T Uei\u22252     2 + 4Tr(eT   i U T VrV T   r UU T V \u22a5   r (V \u22a5 r )T Uei) + \u03bb(D(U))ii\u2225V \u22a5             r (V \u22a5 r )T Uei\u22252    2\n  \u2265  4\u2225U T V \u22a5  r (V \u22a5 r )T Uei\u22252     2 \u2212   4\u2225U T VrV T   r Uei\u22252\u2225U T V \u22a5      r (V \u22a5 r )T Uei\u22252 + \u03bb(D(U))ii\u2225V \u22a5              r (V \u22a5 r )T Uei\u22252    2\n  \u2265  \u03bb(D(U))ii\u2225V \u22a5       r (V \u22a5 r )T Uei\u22252,                                                                                              (195)\n where the last inequality uses the assumption that \u2225VrV T                           r Uei\u22252 \u2264         \u2225V \u22a5r (V \u22a5 r )T Uei\u22252. Finally,\n observe the following bound on (D(U))ii computed using Lemma 43: for any column i such that\n \u2225Uei\u22252 \u2265       2\u221a\u03b2,         1                                      1                                              1\n      (D(U))ii \u2265         \u2225Uei\u22252      =       \u2225V \u22a5r (V \u22a5 r )T Uei\u22252    2 + \u2225VrV T    r Uei\u22252   2  \u2265    2\u2225V \u22a5r (V \u22a5 r )T Uei\u22252       .     (196)\n Plugging this into eq. (195), we get the inequality,\n                                         \u03f5\u2225V \u22a5r (V \u22a5 r )T Uei\u22252 \u2265         \u03bb     r (V \u22a5 r )T Uei\u22252.                                       (197)\nWhen \u03f5 \u2264         \u03bb                                                        2 \u2225V \u22a5        r (V \u22a5 r )T Uei\u22252 = 0. By the condition\n                 2 , the only solution to inequality eq. (195) is \u2225V \u22a5\n \u2225Vr(Vr)T Uei\u22252 \u2264             \u2225V \u22a5r (V \u22a5r )T Uei\u22252, this implies that \u2225Uei\u22252 = 0. This contradicts the initial\n assumption that \u2225Uei\u22252 \u2265               2\u221a\u03b2, thus concluding the proof of the second part of Lemma 30.\n                                                                       33", "md": "Choosing $$Z = V^\\perp r (V^\\perp r)^T U$$, and noting that $$U^*$$ has rank r and is orthogonal to $$V^\\perp r$$, the LHS simplifies as,\n\n$$\\epsilon\\|Z\\|_F \\geq 4\\text{Tr}(UU^T V^\\perp r (V^\\perp r)^T UU^T) + \\lambda\\text{Tr}(UD(U)U^T V^\\perp r (V^\\perp r)^T)$$ (189)\n\n$$\\geq \\lambda \\sum_{i=1}^{k} D(U)_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$ (190)\n\n$$\\geq \\lambda \\sum_{i\\in S} \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$ (191)\n\nwhere the last inequality uses the fact that for any column i such that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, since we have that $$D(U)_{ii} \\geq \\|Ue_i\\|_2$$ by Lemma 43. Putting everything together, we get that,\n\n$$\\lambda \\sum_{i\\in S} \\|Ue_i\\|_2 \\cdot \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 \\leq \\epsilon\\|Z\\|_F$$ (192)\n\nSince $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, rearranging the terms around and finally upper bounding $$\\|Z\\|_F$$ using Lemma 28 completes the proof of eq. (186). To prove eq. (187), notice from eq. (190) that,\n\n$$\\epsilon\\|Z\\|_F \\geq \\lambda \\sum_{i\\in[k]\\backslash S} D(U)_{ii}\\|(V^\\perp r)(V^\\perp r)^T Ue_i\\|_2^2$$ (193)\n\n$$\\geq \\lambda \\sum_{i\\in[k]\\backslash S} \\|(V^\\perp r)(V^\\perp r)^T Ue_i\\|_2^2$$ (194)\n\n$$\\geq \\frac{2\\sqrt{\\beta}}{\\|Ue_i\\|_2} \\cdot \\left(\\|Ue_i\\|_2^2+2\\beta\\right)$$ (195)\n\nwhich uses the fact that when $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$, then $$D(U)_{ii} = (\\|Ue_i\\|_2^2+\\beta)^{3/2} \\geq 2\\beta^{1/2}$$. Rearranging and substituting the bound on $$\\|Z\\|_F$$ from Lemma 28 completes the proof of eq. (187).\n\nLemma 30. Consider any $$\\epsilon$$-approximate first-order stationary point of $$L + \\lambda R\\beta$$, U, and let $$V_r$$ be as defined in Lemma 28. Under the assumption that $$\\epsilon \\leq \\frac{\\lambda}{2}$$, for any column i in [k] such that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$, $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$. Proof. Consider any i such that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$. Therefore, from eq. (188), choosing $$Z = V^\\perp r (V^\\perp r)^T Ue_ie_i^T$$,\n\n$$\\epsilon\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 \\geq 4\\text{Tr}(UU^T V^\\perp r (V^\\perp r)^T Ue_ie_i^T U^T) + \\lambda\\text{Tr}(UD(U)e_ie_i^T U^T V^\\perp r (V^\\perp r)^T)$$\n\n$$= 4\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 + 4\\text{Tr}(e_i^TU^T V_rV_r^TUU^T V^\\perp r (V^\\perp r)^T Ue_i) + \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$\n\n$$\\geq 4\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 - 4\\|U^T V_rV_r^TUe_i\\|_2\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2 + \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$\n\n$$\\geq \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$ (195)\n\nwhere the last inequality uses the assumption that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$. Finally, observe the following bound on $$(D(U))_{ii}$$ computed using Lemma 43: for any column i such that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$,\n\n$$(D(U))_{ii} \\geq \\|Ue_i\\|_2 = \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 + \\|V_rV_r^TUe_i\\|_2^2 \\geq 2\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$ (196)\n\nPlugging this into eq. (195), we get the inequality,\n\n$$\\epsilon\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 \\geq \\lambda\\|V_r(V_r)^TUe_i\\|_2$$ (197)\n\nWhen $$\\epsilon \\leq \\frac{\\lambda}{2}$$, $$2 \\cdot \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 = 0$$. By the condition, the only solution to inequality eq. (195) is $$\\|V_r(V_r)^T Ue_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$, this implies that $$\\|Ue_i\\|_2 = 0$$. This contradicts the initial assumption that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, thus concluding the proof of the second part of Lemma 30.\n\n33", "images": [], "items": [{"type": "text", "value": "Choosing $$Z = V^\\perp r (V^\\perp r)^T U$$, and noting that $$U^*$$ has rank r and is orthogonal to $$V^\\perp r$$, the LHS simplifies as,\n\n$$\\epsilon\\|Z\\|_F \\geq 4\\text{Tr}(UU^T V^\\perp r (V^\\perp r)^T UU^T) + \\lambda\\text{Tr}(UD(U)U^T V^\\perp r (V^\\perp r)^T)$$ (189)\n\n$$\\geq \\lambda \\sum_{i=1}^{k} D(U)_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$ (190)\n\n$$\\geq \\lambda \\sum_{i\\in S} \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$ (191)\n\nwhere the last inequality uses the fact that for any column i such that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, since we have that $$D(U)_{ii} \\geq \\|Ue_i\\|_2$$ by Lemma 43. Putting everything together, we get that,\n\n$$\\lambda \\sum_{i\\in S} \\|Ue_i\\|_2 \\cdot \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 \\leq \\epsilon\\|Z\\|_F$$ (192)\n\nSince $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, rearranging the terms around and finally upper bounding $$\\|Z\\|_F$$ using Lemma 28 completes the proof of eq. (186). To prove eq. (187), notice from eq. (190) that,\n\n$$\\epsilon\\|Z\\|_F \\geq \\lambda \\sum_{i\\in[k]\\backslash S} D(U)_{ii}\\|(V^\\perp r)(V^\\perp r)^T Ue_i\\|_2^2$$ (193)\n\n$$\\geq \\lambda \\sum_{i\\in[k]\\backslash S} \\|(V^\\perp r)(V^\\perp r)^T Ue_i\\|_2^2$$ (194)\n\n$$\\geq \\frac{2\\sqrt{\\beta}}{\\|Ue_i\\|_2} \\cdot \\left(\\|Ue_i\\|_2^2+2\\beta\\right)$$ (195)\n\nwhich uses the fact that when $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$, then $$D(U)_{ii} = (\\|Ue_i\\|_2^2+\\beta)^{3/2} \\geq 2\\beta^{1/2}$$. Rearranging and substituting the bound on $$\\|Z\\|_F$$ from Lemma 28 completes the proof of eq. (187).\n\nLemma 30. Consider any $$\\epsilon$$-approximate first-order stationary point of $$L + \\lambda R\\beta$$, U, and let $$V_r$$ be as defined in Lemma 28. Under the assumption that $$\\epsilon \\leq \\frac{\\lambda}{2}$$, for any column i in [k] such that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$, $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$. Proof. Consider any i such that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$. Therefore, from eq. (188), choosing $$Z = V^\\perp r (V^\\perp r)^T Ue_ie_i^T$$,\n\n$$\\epsilon\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 \\geq 4\\text{Tr}(UU^T V^\\perp r (V^\\perp r)^T Ue_ie_i^T U^T) + \\lambda\\text{Tr}(UD(U)e_ie_i^T U^T V^\\perp r (V^\\perp r)^T)$$\n\n$$= 4\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 + 4\\text{Tr}(e_i^TU^T V_rV_r^TUU^T V^\\perp r (V^\\perp r)^T Ue_i) + \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$\n\n$$\\geq 4\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 - 4\\|U^T V_rV_r^TUe_i\\|_2\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2 + \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$\n\n$$\\geq \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$ (195)\n\nwhere the last inequality uses the assumption that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$. Finally, observe the following bound on $$(D(U))_{ii}$$ computed using Lemma 43: for any column i such that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$,\n\n$$(D(U))_{ii} \\geq \\|Ue_i\\|_2 = \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 + \\|V_rV_r^TUe_i\\|_2^2 \\geq 2\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$ (196)\n\nPlugging this into eq. (195), we get the inequality,\n\n$$\\epsilon\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 \\geq \\lambda\\|V_r(V_r)^TUe_i\\|_2$$ (197)\n\nWhen $$\\epsilon \\leq \\frac{\\lambda}{2}$$, $$2 \\cdot \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 = 0$$. By the condition, the only solution to inequality eq. (195) is $$\\|V_r(V_r)^T Ue_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$, this implies that $$\\|Ue_i\\|_2 = 0$$. This contradicts the initial assumption that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, thus concluding the proof of the second part of Lemma 30.\n\n33", "md": "Choosing $$Z = V^\\perp r (V^\\perp r)^T U$$, and noting that $$U^*$$ has rank r and is orthogonal to $$V^\\perp r$$, the LHS simplifies as,\n\n$$\\epsilon\\|Z\\|_F \\geq 4\\text{Tr}(UU^T V^\\perp r (V^\\perp r)^T UU^T) + \\lambda\\text{Tr}(UD(U)U^T V^\\perp r (V^\\perp r)^T)$$ (189)\n\n$$\\geq \\lambda \\sum_{i=1}^{k} D(U)_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$ (190)\n\n$$\\geq \\lambda \\sum_{i\\in S} \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$ (191)\n\nwhere the last inequality uses the fact that for any column i such that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, since we have that $$D(U)_{ii} \\geq \\|Ue_i\\|_2$$ by Lemma 43. Putting everything together, we get that,\n\n$$\\lambda \\sum_{i\\in S} \\|Ue_i\\|_2 \\cdot \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 \\leq \\epsilon\\|Z\\|_F$$ (192)\n\nSince $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, rearranging the terms around and finally upper bounding $$\\|Z\\|_F$$ using Lemma 28 completes the proof of eq. (186). To prove eq. (187), notice from eq. (190) that,\n\n$$\\epsilon\\|Z\\|_F \\geq \\lambda \\sum_{i\\in[k]\\backslash S} D(U)_{ii}\\|(V^\\perp r)(V^\\perp r)^T Ue_i\\|_2^2$$ (193)\n\n$$\\geq \\lambda \\sum_{i\\in[k]\\backslash S} \\|(V^\\perp r)(V^\\perp r)^T Ue_i\\|_2^2$$ (194)\n\n$$\\geq \\frac{2\\sqrt{\\beta}}{\\|Ue_i\\|_2} \\cdot \\left(\\|Ue_i\\|_2^2+2\\beta\\right)$$ (195)\n\nwhich uses the fact that when $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$, then $$D(U)_{ii} = (\\|Ue_i\\|_2^2+\\beta)^{3/2} \\geq 2\\beta^{1/2}$$. Rearranging and substituting the bound on $$\\|Z\\|_F$$ from Lemma 28 completes the proof of eq. (187).\n\nLemma 30. Consider any $$\\epsilon$$-approximate first-order stationary point of $$L + \\lambda R\\beta$$, U, and let $$V_r$$ be as defined in Lemma 28. Under the assumption that $$\\epsilon \\leq \\frac{\\lambda}{2}$$, for any column i in [k] such that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$, $$\\|Ue_i\\|_2 \\leq 2\\sqrt{\\beta}$$. Proof. Consider any i such that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$. Therefore, from eq. (188), choosing $$Z = V^\\perp r (V^\\perp r)^T Ue_ie_i^T$$,\n\n$$\\epsilon\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 \\geq 4\\text{Tr}(UU^T V^\\perp r (V^\\perp r)^T Ue_ie_i^T U^T) + \\lambda\\text{Tr}(UD(U)e_ie_i^T U^T V^\\perp r (V^\\perp r)^T)$$\n\n$$= 4\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 + 4\\text{Tr}(e_i^TU^T V_rV_r^TUU^T V^\\perp r (V^\\perp r)^T Ue_i) + \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$\n\n$$\\geq 4\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 - 4\\|U^T V_rV_r^TUe_i\\|_2\\|U^T V^\\perp r (V^\\perp r)^T Ue_i\\|_2 + \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2$$\n\n$$\\geq \\lambda(D(U))_{ii}\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$ (195)\n\nwhere the last inequality uses the assumption that $$\\|V_rV_r^TUe_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$. Finally, observe the following bound on $$(D(U))_{ii}$$ computed using Lemma 43: for any column i such that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$,\n\n$$(D(U))_{ii} \\geq \\|Ue_i\\|_2 = \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2^2 + \\|V_rV_r^TUe_i\\|_2^2 \\geq 2\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$ (196)\n\nPlugging this into eq. (195), we get the inequality,\n\n$$\\epsilon\\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 \\geq \\lambda\\|V_r(V_r)^TUe_i\\|_2$$ (197)\n\nWhen $$\\epsilon \\leq \\frac{\\lambda}{2}$$, $$2 \\cdot \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2 = 0$$. By the condition, the only solution to inequality eq. (195) is $$\\|V_r(V_r)^T Ue_i\\|_2 \\leq \\|V^\\perp r (V^\\perp r)^T Ue_i\\|_2$$, this implies that $$\\|Ue_i\\|_2 = 0$$. This contradicts the initial assumption that $$\\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}$$, thus concluding the proof of the second part of Lemma 30.\n\n33"}]}, {"page": 34, "text": " Theorem 31. Consider any (\u03f5, \u03b3)-approximate second-order stationary point of L + \u03bbR\u03b2 denoted\n U. Suppose \u2225U\u2225op \u2264             3. Construct Uprune by following the pruning condition,\n                                              \u2225Uei\u22252 \u2264        2     \u03b2 =\u21d2        Upruneei \u2190       0                                       (198)\n Suppose \u03f5 \u2264        c\u03bb\u03b21/4/r and \u03f5 + \u03b3 \u2264              c\u03bb/r2 for a sufficiently small constant c > 0. Then,\n         1. Uprune has at most r non-zero columns.\n         2. Furthermore,\n                       \u2225UpruneU T   prune \u2212    U\u22c6U T               \u221a  k + \u03bb        k                                          .          (199)\n                                                     \u22c6  \u2225F \u2272     \u03b3                 \u03b2 + \u03f52/3k1/6 + r\u03b2 + 2\u03f52\u221a\u03b2           \u03bb2\n Proof. Recall that the columns of U having L2 norm at most 2\u221a\u03b2 are set to 0 in Uprune. Since U is\n an (\u03f5, \u03b3)-approximate second-order stationary point, the eigenvalues of the Hessian of L + \u03bbR\u03b2 at\n the point U are all at least \u2212\u03b3.\n In Lemma 24, by violation of the condition eq. (133), for every pair of column indices i \u0338= j \u2208                                              S\n where S = {i \u2208          [k] : \u2225Uei\u22252 > 2\u221a\u03b2},\n                              |Cij| = |\u27e8Uei, Uej\u27e9|            \u2272    \u03f5 + 3\u03b3       min{\u2225Uei\u22252, \u2225Uej\u22252}.                                     (200)\n                                              \u2225Zij\u22252                      \u03bb\n where we use the assumption that the spectral norm \u2225U\u2225op \u2264                  \u221a              3, and therefore \u2225Uei\u22252 \u2264               3 for all\n i \u2208  [k]. Note that \u2225Zij\u22252 =                \u2225Uei\u22252    2 + \u2225Uej\u22252     2 \u2264       2 max {\u2225Uei\u22252, \u2225Uej\u22252}, and therefore,\n                                            |\u27e8Uei, Uej\u27e9| \u2272          \u03f5 + \u03b3 \u03bb    \u2225Uei\u22252\u2225Uej\u22252                                              (201)\n In other words, for each i \u0338= j \u2208              S,\n                                     | cos \u03b8ij| \u2272         \u03f5 + \u03b3      =\u21d2      \u03b8ij \u2212     \u03c0   \u2272       \u03f5 + \u03b3                                 (202)\n                                                             \u03bb                         2              \u03bb\nWhere \u03b8ij is the angle between Uei and Uej and the implication follows by assuming that \u03f5, \u03b3 \u2264                                              c\u03bb\n for a sufficiently small c > 0.\n At any \u03f5-approximate first order stationary point U, by Lemma 29, for any i such that \u2225Uei\u22252 \u2265                                          2\u221a\u03b2\n the angle between Uei and its projection onto Vr is intuitively small. By eq. (202) we expect\n Uei and Uej to have an angle close to 90\u25e6                        between them. Therefore, we expect the projections\n of Uei and Uej onto Vr to also have an angle close to 90\u25e6                                  between them. Specifically, given\n vectors v1, v2, v3 and v4 such that |\u2220v1, v2| \u2264                      \u03b51, |\u2220v2, v3 \u2212        90\u25e6| \u2264     \u03b52 and |\u2220v3, v4| \u2264           \u03b53, then\n |\u2220v1, v4 \u2212      90\u25e6| \u2264     \u03b51 + \u03b52 + \u03b53. This means that for any columns Uei and Uej such that \u2225Uei\u22252\n and \u2225Uej\u22252 \u2265          2\u221a\u03b2,                  \u03b8proj  \u2212   \u03c0    \u2272       \u03f5 + \u03b3    +       \u03f5                                                  (203)\n                                               ij        2              \u03bb                         r\n                                                                                  \u03bb\u03b21/4 \u2264        c1\n for some small absolute constant 0 \u2264                   c1 \u2264    1/3 and where \u03b8proj     ij   is the angle between VrV T          r Uei and\n VrV Tr Uej. This assumes that \u03f5 \u2264                  c\u03bb\u03b21/4/r and \u03f5 + \u03b3 \u2264               c\u03bb/r2 for a sufficiently small constant\n c > 0. Since sin(x) \u2264            x for x > 0, this means that,\n                                      \u2200i \u0338= j \u2208     S,     |\u27e8VrV T r Uei, VrV T    r Uej\u27e9|        \u2264    c1                                (204)\n                                                         \u2225VrV T r Uei\u22252\u2225VrV T       r Uej\u22252            r .\n eq. (204) implies that the projections of the large columns of U onto Vr are approximately at 90\u25e6\n to each other. However, intuitively, since Vr is an r dimensional space, this means that at most r\n of these projections can be large in norm. In fact it turns out that since the columns are sufficiently\n orthogonal to one another, exactly r columns have a non-zero projection onto Vr.\n Lemma 32. At most r columns of U can have a non-zero projection onto Vr.\n                                                                       34", "md": "Theorem 31. Consider any (\u03f5, \u03b3)-approximate second-order stationary point of L + \u03bbR\u03b2 denoted U. Suppose $$\\|U\\|_{op} \\leq 3$$. Construct Uprune by following the pruning condition, $$\\|U_{ei}\\|_{2} \\leq 2\\beta \\Rightarrow U_{prune_{ei}} \\leftarrow 0$$ (198) Suppose $$\\epsilon \\leq c\\lambda\\beta^{1/4}/r$$ and $$\\epsilon + \\gamma \\leq c\\lambda/r^2$$ for a sufficiently small constant c > 0. Then, 1. Uprune has at most r non-zero columns. 2. Furthermore, $$\\|\\hat{U}_{prune}U^{T}_{prune} - U^{\\star}U^{T}\\|_{F} \\lesssim \\sqrt{k + \\lambda k} \\cdot (\\gamma \\beta + \\epsilon^{2/3}k^{1/6} + r\\beta + 2\\epsilon^{2}\\sqrt{\\beta} \\lambda^{2})$$\n\nProof. Recall that the columns of U having L2 norm at most $$2\\sqrt{\\beta}$$ are set to 0 in Uprune. Since U is an (\u03f5, \u03b3)-approximate second-order stationary point, the eigenvalues of the Hessian of L + \u03bbR\u03b2 at the point U are all at least -\u03b3. In Lemma 24, by violation of the condition eq. (133), for every pair of column indices i \u2260 j \u2208 S where $$S = \\{i \\in [k] : \\|U_{ei}\\|_{2} > 2\\sqrt{\\beta}\\}$$, $$|C_{ij}| = |\\langle U_{ei}, U_{ej} \\rangle| \\lesssim \\epsilon + 3\\gamma \\min\\{\\|U_{ei}\\|_{2}, \\|U_{ej}\\|_{2}\\}$$. (200) $$\\|Z_{ij}\\|_{2} \\leq \\lambda$$ where we use the assumption that the spectral norm $$\\|U\\|_{op} \\leq \\sqrt{3}$$, and therefore $$\\|U_{ei}\\|_{2} \\leq 3$$ for all i \u2208 [k]. Note that $$\\|Z_{ij}\\|_{2} = \\|U_{ei}\\|_{2}^{2} + \\|U_{ej}\\|_{2}^{2} \\leq 2 \\max\\{\\|U_{ei}\\|_{2}, \\|U_{ej}\\|_{2}\\}$$, and therefore, $$|\\langle U_{ei}, U_{ej} \\rangle| \\lesssim \\epsilon + \\gamma \\lambda \\|U_{ei}\\|_{2}\\|U_{ej}\\|_{2}$$. (201) In other words, for each i \u2260 j \u2208 S, $$|\\cos \\theta_{ij}| \\lesssim \\epsilon + \\gamma \\Rightarrow \\theta_{ij} - \\pi \\lesssim \\epsilon + \\gamma$$ (202) where \u03b8ij is the angle between Uei and Uej and the implication follows by assuming that \u03f5, \u03b3 \u2264 c\u03bb for a sufficiently small c > 0. At any \u03f5-approximate first order stationary point U, by Lemma 29, for any i such that $$\\|U_{ei}\\|_{2} \\geq 2\\sqrt{\\beta}$$ the angle between Uei and its projection onto Vr is intuitively small. By eq. (202) we expect Uei and Uej to have an angle close to 90\u00b0 between them. Therefore, we expect the projections of Uei and Uej onto Vr to also have an angle close to 90\u00b0 between them. Specifically, given vectors v1, v2, v3 and v4 such that $$|\\angle v1, v2| \\leq \\epsilon_{1}$$, $$|\\angle v2, v3 - 90\u00b0| \\leq \\epsilon_{2}$$ and $$|\\angle v3, v4| \\leq \\epsilon_{3}$$, then $$|\\angle v1, v4 - 90\u00b0| \\leq \\epsilon_{1} + \\epsilon_{2} + \\epsilon_{3}$$. This means that for any columns Uei and Uej such that $$\\|U_{ei}\\|_{2}$$ and $$\\|U_{ej}\\|_{2} \\geq 2\\sqrt{\\beta}$$, $$\\theta_{proj_{ij}} - \\pi \\lesssim \\epsilon + \\gamma + \\epsilon$$ (203) $$\\lambda \\beta^{1/4} \\leq c_{1}$$ for some small absolute constant $$0 \\leq c_{1} \\leq 1/3$$ and where $$\\theta_{proj_{ij}}$$ is the angle between VrVTrUei and VrVTrUej. This assumes that $$\\epsilon \\leq c\\lambda\\beta^{1/4}/r$$ and $$\\epsilon + \\gamma \\leq c\\lambda/r^2$$ for a sufficiently small constant c > 0. Since sin(x) \u2264 x for x > 0, this means that, $$\\forall i \\neq j \\in S, |\\langle VrVTrU_{ei}, VrVTrU_{ej} \\rangle| \\leq c_{1} \\cdot \\|VrVTrU_{ei}\\|_{2}\\|VrVTrU_{ej}\\|_{2} \\leq r$$. eq. (204) implies that the projections of the large columns of U onto Vr are approximately at 90\u00b0 to each other. However, intuitively, since Vr is an r dimensional space, this means that at most r of these projections can be large in norm. In fact it turns out that since the columns are sufficiently orthogonal to one another, exactly r columns have a non-zero projection onto Vr. Lemma 32. At most r columns of U can have a non-zero projection onto Vr. 34", "images": [], "items": [{"type": "text", "value": "Theorem 31. Consider any (\u03f5, \u03b3)-approximate second-order stationary point of L + \u03bbR\u03b2 denoted U. Suppose $$\\|U\\|_{op} \\leq 3$$. Construct Uprune by following the pruning condition, $$\\|U_{ei}\\|_{2} \\leq 2\\beta \\Rightarrow U_{prune_{ei}} \\leftarrow 0$$ (198) Suppose $$\\epsilon \\leq c\\lambda\\beta^{1/4}/r$$ and $$\\epsilon + \\gamma \\leq c\\lambda/r^2$$ for a sufficiently small constant c > 0. Then, 1. Uprune has at most r non-zero columns. 2. Furthermore, $$\\|\\hat{U}_{prune}U^{T}_{prune} - U^{\\star}U^{T}\\|_{F} \\lesssim \\sqrt{k + \\lambda k} \\cdot (\\gamma \\beta + \\epsilon^{2/3}k^{1/6} + r\\beta + 2\\epsilon^{2}\\sqrt{\\beta} \\lambda^{2})$$\n\nProof. Recall that the columns of U having L2 norm at most $$2\\sqrt{\\beta}$$ are set to 0 in Uprune. Since U is an (\u03f5, \u03b3)-approximate second-order stationary point, the eigenvalues of the Hessian of L + \u03bbR\u03b2 at the point U are all at least -\u03b3. In Lemma 24, by violation of the condition eq. (133), for every pair of column indices i \u2260 j \u2208 S where $$S = \\{i \\in [k] : \\|U_{ei}\\|_{2} > 2\\sqrt{\\beta}\\}$$, $$|C_{ij}| = |\\langle U_{ei}, U_{ej} \\rangle| \\lesssim \\epsilon + 3\\gamma \\min\\{\\|U_{ei}\\|_{2}, \\|U_{ej}\\|_{2}\\}$$. (200) $$\\|Z_{ij}\\|_{2} \\leq \\lambda$$ where we use the assumption that the spectral norm $$\\|U\\|_{op} \\leq \\sqrt{3}$$, and therefore $$\\|U_{ei}\\|_{2} \\leq 3$$ for all i \u2208 [k]. Note that $$\\|Z_{ij}\\|_{2} = \\|U_{ei}\\|_{2}^{2} + \\|U_{ej}\\|_{2}^{2} \\leq 2 \\max\\{\\|U_{ei}\\|_{2}, \\|U_{ej}\\|_{2}\\}$$, and therefore, $$|\\langle U_{ei}, U_{ej} \\rangle| \\lesssim \\epsilon + \\gamma \\lambda \\|U_{ei}\\|_{2}\\|U_{ej}\\|_{2}$$. (201) In other words, for each i \u2260 j \u2208 S, $$|\\cos \\theta_{ij}| \\lesssim \\epsilon + \\gamma \\Rightarrow \\theta_{ij} - \\pi \\lesssim \\epsilon + \\gamma$$ (202) where \u03b8ij is the angle between Uei and Uej and the implication follows by assuming that \u03f5, \u03b3 \u2264 c\u03bb for a sufficiently small c > 0. At any \u03f5-approximate first order stationary point U, by Lemma 29, for any i such that $$\\|U_{ei}\\|_{2} \\geq 2\\sqrt{\\beta}$$ the angle between Uei and its projection onto Vr is intuitively small. By eq. (202) we expect Uei and Uej to have an angle close to 90\u00b0 between them. Therefore, we expect the projections of Uei and Uej onto Vr to also have an angle close to 90\u00b0 between them. Specifically, given vectors v1, v2, v3 and v4 such that $$|\\angle v1, v2| \\leq \\epsilon_{1}$$, $$|\\angle v2, v3 - 90\u00b0| \\leq \\epsilon_{2}$$ and $$|\\angle v3, v4| \\leq \\epsilon_{3}$$, then $$|\\angle v1, v4 - 90\u00b0| \\leq \\epsilon_{1} + \\epsilon_{2} + \\epsilon_{3}$$. This means that for any columns Uei and Uej such that $$\\|U_{ei}\\|_{2}$$ and $$\\|U_{ej}\\|_{2} \\geq 2\\sqrt{\\beta}$$, $$\\theta_{proj_{ij}} - \\pi \\lesssim \\epsilon + \\gamma + \\epsilon$$ (203) $$\\lambda \\beta^{1/4} \\leq c_{1}$$ for some small absolute constant $$0 \\leq c_{1} \\leq 1/3$$ and where $$\\theta_{proj_{ij}}$$ is the angle between VrVTrUei and VrVTrUej. This assumes that $$\\epsilon \\leq c\\lambda\\beta^{1/4}/r$$ and $$\\epsilon + \\gamma \\leq c\\lambda/r^2$$ for a sufficiently small constant c > 0. Since sin(x) \u2264 x for x > 0, this means that, $$\\forall i \\neq j \\in S, |\\langle VrVTrU_{ei}, VrVTrU_{ej} \\rangle| \\leq c_{1} \\cdot \\|VrVTrU_{ei}\\|_{2}\\|VrVTrU_{ej}\\|_{2} \\leq r$$. eq. (204) implies that the projections of the large columns of U onto Vr are approximately at 90\u00b0 to each other. However, intuitively, since Vr is an r dimensional space, this means that at most r of these projections can be large in norm. In fact it turns out that since the columns are sufficiently orthogonal to one another, exactly r columns have a non-zero projection onto Vr. Lemma 32. At most r columns of U can have a non-zero projection onto Vr. 34", "md": "Theorem 31. Consider any (\u03f5, \u03b3)-approximate second-order stationary point of L + \u03bbR\u03b2 denoted U. Suppose $$\\|U\\|_{op} \\leq 3$$. Construct Uprune by following the pruning condition, $$\\|U_{ei}\\|_{2} \\leq 2\\beta \\Rightarrow U_{prune_{ei}} \\leftarrow 0$$ (198) Suppose $$\\epsilon \\leq c\\lambda\\beta^{1/4}/r$$ and $$\\epsilon + \\gamma \\leq c\\lambda/r^2$$ for a sufficiently small constant c > 0. Then, 1. Uprune has at most r non-zero columns. 2. Furthermore, $$\\|\\hat{U}_{prune}U^{T}_{prune} - U^{\\star}U^{T}\\|_{F} \\lesssim \\sqrt{k + \\lambda k} \\cdot (\\gamma \\beta + \\epsilon^{2/3}k^{1/6} + r\\beta + 2\\epsilon^{2}\\sqrt{\\beta} \\lambda^{2})$$\n\nProof. Recall that the columns of U having L2 norm at most $$2\\sqrt{\\beta}$$ are set to 0 in Uprune. Since U is an (\u03f5, \u03b3)-approximate second-order stationary point, the eigenvalues of the Hessian of L + \u03bbR\u03b2 at the point U are all at least -\u03b3. In Lemma 24, by violation of the condition eq. (133), for every pair of column indices i \u2260 j \u2208 S where $$S = \\{i \\in [k] : \\|U_{ei}\\|_{2} > 2\\sqrt{\\beta}\\}$$, $$|C_{ij}| = |\\langle U_{ei}, U_{ej} \\rangle| \\lesssim \\epsilon + 3\\gamma \\min\\{\\|U_{ei}\\|_{2}, \\|U_{ej}\\|_{2}\\}$$. (200) $$\\|Z_{ij}\\|_{2} \\leq \\lambda$$ where we use the assumption that the spectral norm $$\\|U\\|_{op} \\leq \\sqrt{3}$$, and therefore $$\\|U_{ei}\\|_{2} \\leq 3$$ for all i \u2208 [k]. Note that $$\\|Z_{ij}\\|_{2} = \\|U_{ei}\\|_{2}^{2} + \\|U_{ej}\\|_{2}^{2} \\leq 2 \\max\\{\\|U_{ei}\\|_{2}, \\|U_{ej}\\|_{2}\\}$$, and therefore, $$|\\langle U_{ei}, U_{ej} \\rangle| \\lesssim \\epsilon + \\gamma \\lambda \\|U_{ei}\\|_{2}\\|U_{ej}\\|_{2}$$. (201) In other words, for each i \u2260 j \u2208 S, $$|\\cos \\theta_{ij}| \\lesssim \\epsilon + \\gamma \\Rightarrow \\theta_{ij} - \\pi \\lesssim \\epsilon + \\gamma$$ (202) where \u03b8ij is the angle between Uei and Uej and the implication follows by assuming that \u03f5, \u03b3 \u2264 c\u03bb for a sufficiently small c > 0. At any \u03f5-approximate first order stationary point U, by Lemma 29, for any i such that $$\\|U_{ei}\\|_{2} \\geq 2\\sqrt{\\beta}$$ the angle between Uei and its projection onto Vr is intuitively small. By eq. (202) we expect Uei and Uej to have an angle close to 90\u00b0 between them. Therefore, we expect the projections of Uei and Uej onto Vr to also have an angle close to 90\u00b0 between them. Specifically, given vectors v1, v2, v3 and v4 such that $$|\\angle v1, v2| \\leq \\epsilon_{1}$$, $$|\\angle v2, v3 - 90\u00b0| \\leq \\epsilon_{2}$$ and $$|\\angle v3, v4| \\leq \\epsilon_{3}$$, then $$|\\angle v1, v4 - 90\u00b0| \\leq \\epsilon_{1} + \\epsilon_{2} + \\epsilon_{3}$$. This means that for any columns Uei and Uej such that $$\\|U_{ei}\\|_{2}$$ and $$\\|U_{ej}\\|_{2} \\geq 2\\sqrt{\\beta}$$, $$\\theta_{proj_{ij}} - \\pi \\lesssim \\epsilon + \\gamma + \\epsilon$$ (203) $$\\lambda \\beta^{1/4} \\leq c_{1}$$ for some small absolute constant $$0 \\leq c_{1} \\leq 1/3$$ and where $$\\theta_{proj_{ij}}$$ is the angle between VrVTrUei and VrVTrUej. This assumes that $$\\epsilon \\leq c\\lambda\\beta^{1/4}/r$$ and $$\\epsilon + \\gamma \\leq c\\lambda/r^2$$ for a sufficiently small constant c > 0. Since sin(x) \u2264 x for x > 0, this means that, $$\\forall i \\neq j \\in S, |\\langle VrVTrU_{ei}, VrVTrU_{ej} \\rangle| \\leq c_{1} \\cdot \\|VrVTrU_{ei}\\|_{2}\\|VrVTrU_{ej}\\|_{2} \\leq r$$. eq. (204) implies that the projections of the large columns of U onto Vr are approximately at 90\u00b0 to each other. However, intuitively, since Vr is an r dimensional space, this means that at most r of these projections can be large in norm. In fact it turns out that since the columns are sufficiently orthogonal to one another, exactly r columns have a non-zero projection onto Vr. Lemma 32. At most r columns of U can have a non-zero projection onto Vr. 34"}]}, {"page": 35, "text": "    The proof of this result is deferred to Appendix B.2.\n   What is the implication of Lemma 32? At most r columns of U (and therefore Uprune) have non-\n    zero projections onto Vr. Any of remaining columns of U, say i, therefore satisfies the condition\n    0 = \u2225VrV T    r Uei\u22252 \u2264        \u2225V \u22a5r (V \u22a5 r )T Uei\u22252. By implication of Lemma 30, the remaining columns of U\n    have L2 norm at most 2\u221a\u03b2 assuming that \u03f5 \u2264                          \u03bb/2. Thus these columns are pruned away in Uprune.\n    Overall, this implies that at most r columns of Uprune are non-zero.\n    For the analysis of \u2225UpruneU T          prune \u2212   U\u22c6U T \u22c6  \u2225F , note that, the columns of U with L2 norm smaller than\n    2\u221a\u03b2 are set to 0 in Uprune. This implies that,\n\u2225UpruneU T   prune \u2212   U\u22c6U T \u22c6  \u2225F \u2212     \u2225UU T \u2212       U\u22c6U T  \u22c6 \u2225F\n        \u2264   \u2225UpruneU T   prune \u2212   UU T \u2225F                                                                                          (205)\n        \u2264     i\u2208[k]  Uei(Uei)T I(\u2225Uei\u22252 \u2264               2    \u03b2)    F                                                                (206)\n        \u2264   i\u2208[k]   Uei(Uei)T I(\u2225Uei\u22252 \u2264                2    \u03b2)    F                            \u2225VrV T                              (207)\n        \u2264   i\u2208[k]  \u2225Uei\u22252   2I(\u2225Uei\u22252 \u2264         2    \u03b2; \u2225VrV T   r Uei\u22252 > 0) +           i\u2208[k]         r Uei\u22252    2I(\u2225Uei\u22252 \u2264   2  \u03b2)\n                                                                                                                                    (208)\n        (i)\n        \u2264   r\u03b2 +     i\u2208[k]  \u2225VrV T r Uei\u22252    2I(\u2225Uei\u22252 \u2264         2    \u03b2)                                                           (209)\n        (ii)\n         \u2264   r\u03b2 + 2\u03f52\u221a\u03b2  \u03bb2                                                                                                         (210)\n    where (i) uses the fact that there are at most r columns of U having non-zero projections onto Vr,\n    and (ii) follows from eq. (187) of Lemma 29. Combining this inequality with the guarantee on\n    \u2225UU T \u2212       U\u22c6U T \u22c6  \u2225F in Lemma 26 completes the proof.\n    B.2     Proof of Theorem 3\n    Proof. From Theorem 31, we have that,\n                        \u2225UpruneU T                   \u22c6  \u2225F \u2272     \u03b3 \u221a  k + \u03bb        k                                          .     (211)\n                                    prune \u2212    U\u22c6U T                               \u03b2 + \u03f52/3k1/6 + r\u03b2 + 2\u03f52\u221a\u03b2           \u03bb2\n    Recall that the optimization and smoothing parameters are chosen as,\n                                                                \u03b2 = c\u03b2     (\u03c3\u22c6r)2                                                   (212)\n                                                                              r\n                                                                           (\u03c3\u22c6r)3\n                                                                \u03bb = c\u03bb      \u221a  kr                                                   (213)\n                                                                            (\u03c3\u22c6 r)3\n                                                                \u03b3 \u2264    c\u03b3  \u221a  kr5/2                                                 (214)\n                                                                          (\u03c3\u22c6 r)7/2\n                                                                 \u03f5 \u2264   c\u03f5 \u221a  kr5/2 .                                                (215)\n    For sufficiently small absolute constants c\u03b2, c\u03bb, c\u03b3, c\u03f5 > 0. Under these choices, it is easily verified\n    from eq. (211) that the conditions,\n                                                     \u03f5 \u2264   c\u03bb\u03b21/4      ;       \u03f5 + \u03b3 \u2264       c\u03bb                                     (216)\n                                                                 r                           r2 .\n    which are conditions required by Theorem 31. The bound in eq. (211) results in,\n                                                 \u2225UpruneU T  prune \u2212    U\u22c6U T \u22c6  \u2225F \u2264     1     r)2.                                (217)\n                                                                                          2(\u03c3\u22c6\n                                                                          35", "md": "# Math Equations and Text\n\nThe proof of this result is deferred to Appendix B.2.\n\nWhat is the implication of Lemma 32? At most r columns of U (and therefore Uprune) have non-zero projections onto Vr. Any of remaining columns of U, say i, therefore satisfies the condition\n\n$$0 = \\left\\|V_rV_r^TUe_i\\right\\|^2 \\leq \\left\\|V_{\\perp r}(V_{\\perp r})^TUe_i\\right\\|^2.$$ By implication of Lemma 30, the remaining columns of U have L2 norm at most $2\\sqrt{\\beta}$ assuming that $\\epsilon \\leq \\frac{\\lambda}{2}$. Thus these columns are pruned away in Uprune.\n\nOverall, this implies that at most r columns of Uprune are non-zero.\n\nFor the analysis of $\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F$, note that, the columns of U with L2 norm smaller than $2\\sqrt{\\beta}$ are set to 0 in Uprune. This implies that,\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F - \\left\\|UU^T - U^*U^{T*}\\right\\|_F \\leq \\left\\|UpruneU^T_{prune} - UU^T\\right\\|_F$$\n\n$$\\leq \\sum_{i\\in[k]} Ue_i(Ue_i)^TI\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)F$$\n\n$$\\leq \\sum_{i\\in[k]} Ue_i(Ue_i)^TI\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)F \\left\\|V_rV_r^T\\right\\|$$\n\n$$\\leq \\sum_{i\\in[k]} \\left\\|Ue_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta; \\left\\|V_rV_r^TUe_i\\right\\|^2 > 0\\right) + \\sum_{i\\in[k]} \\left\\|V_rV_r^TUe_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)$$\n\n$$\\leq r\\beta + \\sum_{i\\in[k]} \\left\\|V_rV_r^TUe_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)$$\n\n$$\\leq r\\beta + 2\\epsilon^2\\sqrt{\\beta}\\lambda^2$$\n\nwhere (i) uses the fact that there are at most r columns of U having non-zero projections onto Vr, and (ii) follows from eq. (187) of Lemma 29. Combining this inequality with the guarantee on $\\left\\|UU^T - U^*U^{T*}\\right\\|_F$ in Lemma 26 completes the proof.\n\nB.2 Proof of Theorem 3\n\nProof. From Theorem 31, we have that,\n\n$$\\left\\|UpruneU^T_{\\star}\\right\\|_F \\lesssim \\gamma \\sqrt{k} + \\lambda k.$$\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F \\leq \\frac{\\beta + \\epsilon^2/3k^{1/6} + r\\beta + 2\\epsilon^2\\sqrt{\\beta}}{\\lambda^2}$$\n\nRecall that the optimization and smoothing parameters are chosen as,\n\n$$\\beta = c_{\\beta}(\\sigma^*r)^2$$\n\n$$\\lambda = c_{\\lambda}\\frac{\\sqrt{kr}}{(\\sigma^*r)^3}$$\n\n$$\\gamma \\leq c_{\\gamma}\\frac{\\sqrt{kr^{5/2}}}{(\\sigma^*r)^{7/2}}$$\n\n$$\\epsilon \\leq c_{\\epsilon}\\sqrt{kr^{5/2}}.$$\n\nFor sufficiently small absolute constants $c_{\\beta}$, $c_{\\lambda}$, $c_{\\gamma}$, $c_{\\epsilon} > 0$. Under these choices, it is easily verified from eq. (211) that the conditions,\n\n$$\\epsilon \\leq c_{\\lambda}\\beta^{1/4}; \\epsilon + \\gamma \\leq c_{\\lambda}r^2.$$\n\nwhich are conditions required by Theorem 31. The bound in eq. (211) results in,\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F \\leq \\frac{1}{2(\\sigma^*r)^2}.$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "The proof of this result is deferred to Appendix B.2.\n\nWhat is the implication of Lemma 32? At most r columns of U (and therefore Uprune) have non-zero projections onto Vr. Any of remaining columns of U, say i, therefore satisfies the condition\n\n$$0 = \\left\\|V_rV_r^TUe_i\\right\\|^2 \\leq \\left\\|V_{\\perp r}(V_{\\perp r})^TUe_i\\right\\|^2.$$ By implication of Lemma 30, the remaining columns of U have L2 norm at most $2\\sqrt{\\beta}$ assuming that $\\epsilon \\leq \\frac{\\lambda}{2}$. Thus these columns are pruned away in Uprune.\n\nOverall, this implies that at most r columns of Uprune are non-zero.\n\nFor the analysis of $\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F$, note that, the columns of U with L2 norm smaller than $2\\sqrt{\\beta}$ are set to 0 in Uprune. This implies that,\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F - \\left\\|UU^T - U^*U^{T*}\\right\\|_F \\leq \\left\\|UpruneU^T_{prune} - UU^T\\right\\|_F$$\n\n$$\\leq \\sum_{i\\in[k]} Ue_i(Ue_i)^TI\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)F$$\n\n$$\\leq \\sum_{i\\in[k]} Ue_i(Ue_i)^TI\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)F \\left\\|V_rV_r^T\\right\\|$$\n\n$$\\leq \\sum_{i\\in[k]} \\left\\|Ue_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta; \\left\\|V_rV_r^TUe_i\\right\\|^2 > 0\\right) + \\sum_{i\\in[k]} \\left\\|V_rV_r^TUe_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)$$\n\n$$\\leq r\\beta + \\sum_{i\\in[k]} \\left\\|V_rV_r^TUe_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)$$\n\n$$\\leq r\\beta + 2\\epsilon^2\\sqrt{\\beta}\\lambda^2$$\n\nwhere (i) uses the fact that there are at most r columns of U having non-zero projections onto Vr, and (ii) follows from eq. (187) of Lemma 29. Combining this inequality with the guarantee on $\\left\\|UU^T - U^*U^{T*}\\right\\|_F$ in Lemma 26 completes the proof.\n\nB.2 Proof of Theorem 3\n\nProof. From Theorem 31, we have that,\n\n$$\\left\\|UpruneU^T_{\\star}\\right\\|_F \\lesssim \\gamma \\sqrt{k} + \\lambda k.$$\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F \\leq \\frac{\\beta + \\epsilon^2/3k^{1/6} + r\\beta + 2\\epsilon^2\\sqrt{\\beta}}{\\lambda^2}$$\n\nRecall that the optimization and smoothing parameters are chosen as,\n\n$$\\beta = c_{\\beta}(\\sigma^*r)^2$$\n\n$$\\lambda = c_{\\lambda}\\frac{\\sqrt{kr}}{(\\sigma^*r)^3}$$\n\n$$\\gamma \\leq c_{\\gamma}\\frac{\\sqrt{kr^{5/2}}}{(\\sigma^*r)^{7/2}}$$\n\n$$\\epsilon \\leq c_{\\epsilon}\\sqrt{kr^{5/2}}.$$\n\nFor sufficiently small absolute constants $c_{\\beta}$, $c_{\\lambda}$, $c_{\\gamma}$, $c_{\\epsilon} > 0$. Under these choices, it is easily verified from eq. (211) that the conditions,\n\n$$\\epsilon \\leq c_{\\lambda}\\beta^{1/4}; \\epsilon + \\gamma \\leq c_{\\lambda}r^2.$$\n\nwhich are conditions required by Theorem 31. The bound in eq. (211) results in,\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F \\leq \\frac{1}{2(\\sigma^*r)^2}.$$", "md": "The proof of this result is deferred to Appendix B.2.\n\nWhat is the implication of Lemma 32? At most r columns of U (and therefore Uprune) have non-zero projections onto Vr. Any of remaining columns of U, say i, therefore satisfies the condition\n\n$$0 = \\left\\|V_rV_r^TUe_i\\right\\|^2 \\leq \\left\\|V_{\\perp r}(V_{\\perp r})^TUe_i\\right\\|^2.$$ By implication of Lemma 30, the remaining columns of U have L2 norm at most $2\\sqrt{\\beta}$ assuming that $\\epsilon \\leq \\frac{\\lambda}{2}$. Thus these columns are pruned away in Uprune.\n\nOverall, this implies that at most r columns of Uprune are non-zero.\n\nFor the analysis of $\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F$, note that, the columns of U with L2 norm smaller than $2\\sqrt{\\beta}$ are set to 0 in Uprune. This implies that,\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F - \\left\\|UU^T - U^*U^{T*}\\right\\|_F \\leq \\left\\|UpruneU^T_{prune} - UU^T\\right\\|_F$$\n\n$$\\leq \\sum_{i\\in[k]} Ue_i(Ue_i)^TI\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)F$$\n\n$$\\leq \\sum_{i\\in[k]} Ue_i(Ue_i)^TI\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)F \\left\\|V_rV_r^T\\right\\|$$\n\n$$\\leq \\sum_{i\\in[k]} \\left\\|Ue_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta; \\left\\|V_rV_r^TUe_i\\right\\|^2 > 0\\right) + \\sum_{i\\in[k]} \\left\\|V_rV_r^TUe_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)$$\n\n$$\\leq r\\beta + \\sum_{i\\in[k]} \\left\\|V_rV_r^TUe_i\\right\\|^2 2I\\left(\\left\\|Ue_i\\right\\|^2 \\leq 2\\beta\\right)$$\n\n$$\\leq r\\beta + 2\\epsilon^2\\sqrt{\\beta}\\lambda^2$$\n\nwhere (i) uses the fact that there are at most r columns of U having non-zero projections onto Vr, and (ii) follows from eq. (187) of Lemma 29. Combining this inequality with the guarantee on $\\left\\|UU^T - U^*U^{T*}\\right\\|_F$ in Lemma 26 completes the proof.\n\nB.2 Proof of Theorem 3\n\nProof. From Theorem 31, we have that,\n\n$$\\left\\|UpruneU^T_{\\star}\\right\\|_F \\lesssim \\gamma \\sqrt{k} + \\lambda k.$$\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F \\leq \\frac{\\beta + \\epsilon^2/3k^{1/6} + r\\beta + 2\\epsilon^2\\sqrt{\\beta}}{\\lambda^2}$$\n\nRecall that the optimization and smoothing parameters are chosen as,\n\n$$\\beta = c_{\\beta}(\\sigma^*r)^2$$\n\n$$\\lambda = c_{\\lambda}\\frac{\\sqrt{kr}}{(\\sigma^*r)^3}$$\n\n$$\\gamma \\leq c_{\\gamma}\\frac{\\sqrt{kr^{5/2}}}{(\\sigma^*r)^{7/2}}$$\n\n$$\\epsilon \\leq c_{\\epsilon}\\sqrt{kr^{5/2}}.$$\n\nFor sufficiently small absolute constants $c_{\\beta}$, $c_{\\lambda}$, $c_{\\gamma}$, $c_{\\epsilon} > 0$. Under these choices, it is easily verified from eq. (211) that the conditions,\n\n$$\\epsilon \\leq c_{\\lambda}\\beta^{1/4}; \\epsilon + \\gamma \\leq c_{\\lambda}r^2.$$\n\nwhich are conditions required by Theorem 31. The bound in eq. (211) results in,\n\n$$\\left\\|UpruneU^T_{prune} - U^*U^{T*}\\right\\|_F \\leq \\frac{1}{2(\\sigma^*r)^2}.$$"}]}, {"page": 36, "text": "Note also that Uprune has at most r non-zero columns. However, by eq. (217), this means that Uprune\nhas exactly r columns, since if it had r \u2212             1 non-zero columns or fewer, the error of the best solution\nmust be at least (\u03c3\u22c6     r)2.\nB.3     Missing proofs in Appendix B\nB.3.1      Proof of Lemma 25\nProof. By definition of W and L, Tr((WL \u2212                    LW) \u00b7 ZT U), can be simplified to\n                              Tr((WL \u2212       LW) \u00b7 ZT U) = \u2206Tr((e2eT            2 \u2212   e1eT1 )U T U)                    (218)\nNote that D(U)11 \u2212          D(U)22 = \u2206         and |\u2206| \u2264      \u03f5 = \u2206(\u2225Ue2\u22252      2 \u2212   \u2225Ue1\u22252   2)                      (219)\n                                                              \u03bb, so one can expect \u2225Ue1\u22252 and \u2225Ue2\u22252 to be close\nto each other. We bound |\u2225Ue1\u22252           2 \u2212\u2225Ue2\u22252     2| in two ways depending on the relative values of \u2225Ue1\u22252\nand \u2225Ue2\u22252.\nCase 1: Column norms are similar.                    maxi\u2208{1,2} \u2225Uei\u22252 \u2264           2 mini\u2208{1,2} \u2225Uei\u22252.\nBy definition of \u2206,\n                               |\u2206| =        \u2225Ue1\u22252   2 + 2\u03b2             \u2225Ue2\u22252   2 + 2\u03b2                                (220)\n                                          (\u2225Ue1\u22252   2 + \u03b2)3/2 \u2212       (\u2225Ue2\u22252   2 + \u03b2)3/2\n                                     =       \u2225Ue1\u22252 2 3/2 \u00b7 x + 4\u03b2                                                     (221)\n                                           \u2225Ue2\u22252 2    (x + \u03b2)5/2 dx\n                                     \u2265    min     3/2\u2225Uei\u22252     2 + 4\u03b2             2 \u2212   \u2225Ue2\u22252   2|.                  (222)\n                                        i\u2208{1,2}   (\u2225Uei\u22252   2 + \u03b2)5/2 |\u2225Ue1\u22252\nFurthermore, note that for any i \u2208            {1, 2},\n                             (\u2225Uei\u22252   2 + \u03b2)5/2     \u2264    3           2 + 4\u03b2     3/2                                   (223)\n                               3         2 + 4\u03b2             2\u2225Uei\u22252\n                               2\u2225Uei\u22252               (i) \u221a     3              3/2       \u221a\n                                                      \u2264     2    2\u2225Uei\u22252    2        +     2 (4\u03b2)3/2                   (224)\n                                                     \u2264   3\u2225Uei\u22253   2 + 12\u03b23/2                                          (225)\n                                                     \u2264   5\u2225Uei\u22253   2                                                   (226)\nwhere (i) uses the convexity of (\u00b7)3/2 for positive arguments and applies Jensen\u2019s inequality, and\nthe last inequality uses the fact that \u2225Ue1\u22252, \u2225Ue2\u22252 \u2265                      2\u221a\u03b2. Overall, combining eq. (219) with\neq. (222) and eq. (226) results in the bound,\n                                 Tr((WL \u2212        LW) \u00b7 ZT U) \u2264         5\u22062 max                 2                       (227)\n                                                                             i\u2208{1,2} \u2225Uei\u22253\n                                                                    \u2264  40\u22062 mini\u2208{1,2} \u2225Uei\u22253    2                     (228)\nwhere the last inequality uses the assumption that maxi\u2208{1,2} \u2225Uei\u22252 \u2264                           2 mini\u2208{1,2} \u2225Uei\u22252.\nCase 2: Column norms are separated.                      maxi\u2208{1,2} \u2225Uei\u22252 \u2265           2 mini\u2208{1,2} \u2225Uei\u22252.\n                                                                36", "md": "Note also that Uprune has at most r non-zero columns. However, by eq. (217), this means that Uprune has exactly r columns, since if it had r - 1 non-zero columns or fewer, the error of the best solution must be at least $$(\\sigma^* r)^2$$.\n\n### B.3 Missing proofs in Appendix B\n\n#### B.3.1 Proof of Lemma 25\n\n**Proof.** By definition of W and L, $$\\text{Tr}((WL - LW) \\cdot Z^T U)$$, can be simplified to\n\n$$\n\\text{Tr}((WL - LW) \\cdot Z^T U) = \\Delta \\text{Tr}((e_2 e_2^T - e_1 e_1^T) U^T U) \\quad (218)\n$$\n\nNote that $$D(U)_{11} - D(U)_{22} = \\Delta$$ and $$|\\Delta| \\leq \\epsilon = \\Delta(\\|Ue_2\\|_2^2 - \\|Ue_1\\|_2^2)$$, so one can expect $$\\|Ue_1\\|_2$$ and $$\\|Ue_2\\|_2$$ to be close to each other. We bound $$|\\|Ue_1\\|_2^2 - \\|Ue_2\\|_2^2|$$ in two ways depending on the relative values of $$\\|Ue_1\\|_2$$ and $$\\|Ue_2\\|_2$$.\n\n**Case 1: Column norms are similar.** $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\leq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$. By definition of $$\\Delta$$,\n\n$$\n|\\Delta| = \\frac{\\|Ue_1\\|_2^2 + 2\\beta \\|Ue_2\\|_2^2 + 2\\beta}{(\\|Ue_1\\|_2^2 + \\beta)^{3/2} - (\\|Ue_2\\|_2^2 + \\beta)^{3/2}}\n= \\frac{\\|Ue_1\\|_2^{3/2} \\cdot x + 4\\beta}{\\|Ue_2\\|_2 (x + \\beta)^{5/2} dx} \\geq \\min_{i \\in \\{1,2\\}} \\frac{3}{2} \\|Ue_i\\|_2^2 + 4\\beta - \\|Ue_2\\|_2| \\quad (222)\n$$\n\nFurthermore, note that for any $$i \\in \\{1, 2\\}$$,\n\n$$\n(\\|Ue_i\\|_2 + \\beta)^{5/2} \\leq 3\\|Ue_i\\|_2 + 4\\beta)^{3/2} \\leq 3\\|Ue_i\\|_2^2 + 12\\beta^{3/2} \\leq 5\\|Ue_i\\|_2^3 \\quad (226)\n$$\n\nwhere (i) uses the convexity of $$(\\cdot)^{3/2}$$ for positive arguments and applies Jensen\u2019s inequality, and the last inequality uses the fact that $$\\|Ue_1\\|_2, \\|Ue_2\\|_2 \\geq 2\\sqrt{\\beta}$$. Overall, combining eq. (219) with eq. (222) and eq. (226) results in the bound,\n\n$$\n\\text{Tr}((WL - LW) \\cdot Z^T U) \\leq 5\\Delta^2 \\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_3^2 \\leq 40\\Delta^2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_3^2 \\quad (228)\n$$\n\nwhere the last inequality uses the assumption that $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\leq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$.\n\n**Case 2: Column norms are separated.** $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\geq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$.", "images": [], "items": [{"type": "text", "value": "Note also that Uprune has at most r non-zero columns. However, by eq. (217), this means that Uprune has exactly r columns, since if it had r - 1 non-zero columns or fewer, the error of the best solution must be at least $$(\\sigma^* r)^2$$.", "md": "Note also that Uprune has at most r non-zero columns. However, by eq. (217), this means that Uprune has exactly r columns, since if it had r - 1 non-zero columns or fewer, the error of the best solution must be at least $$(\\sigma^* r)^2$$."}, {"type": "heading", "lvl": 3, "value": "B.3 Missing proofs in Appendix B", "md": "### B.3 Missing proofs in Appendix B"}, {"type": "heading", "lvl": 4, "value": "B.3.1 Proof of Lemma 25", "md": "#### B.3.1 Proof of Lemma 25"}, {"type": "text", "value": "**Proof.** By definition of W and L, $$\\text{Tr}((WL - LW) \\cdot Z^T U)$$, can be simplified to\n\n$$\n\\text{Tr}((WL - LW) \\cdot Z^T U) = \\Delta \\text{Tr}((e_2 e_2^T - e_1 e_1^T) U^T U) \\quad (218)\n$$\n\nNote that $$D(U)_{11} - D(U)_{22} = \\Delta$$ and $$|\\Delta| \\leq \\epsilon = \\Delta(\\|Ue_2\\|_2^2 - \\|Ue_1\\|_2^2)$$, so one can expect $$\\|Ue_1\\|_2$$ and $$\\|Ue_2\\|_2$$ to be close to each other. We bound $$|\\|Ue_1\\|_2^2 - \\|Ue_2\\|_2^2|$$ in two ways depending on the relative values of $$\\|Ue_1\\|_2$$ and $$\\|Ue_2\\|_2$$.\n\n**Case 1: Column norms are similar.** $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\leq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$. By definition of $$\\Delta$$,\n\n$$", "md": "**Proof.** By definition of W and L, $$\\text{Tr}((WL - LW) \\cdot Z^T U)$$, can be simplified to\n\n$$\n\\text{Tr}((WL - LW) \\cdot Z^T U) = \\Delta \\text{Tr}((e_2 e_2^T - e_1 e_1^T) U^T U) \\quad (218)\n$$\n\nNote that $$D(U)_{11} - D(U)_{22} = \\Delta$$ and $$|\\Delta| \\leq \\epsilon = \\Delta(\\|Ue_2\\|_2^2 - \\|Ue_1\\|_2^2)$$, so one can expect $$\\|Ue_1\\|_2$$ and $$\\|Ue_2\\|_2$$ to be close to each other. We bound $$|\\|Ue_1\\|_2^2 - \\|Ue_2\\|_2^2|$$ in two ways depending on the relative values of $$\\|Ue_1\\|_2$$ and $$\\|Ue_2\\|_2$$.\n\n**Case 1: Column norms are similar.** $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\leq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$. By definition of $$\\Delta$$,\n\n$$"}, {"type": "table", "rows": [["\\Delta", "= \\frac{\\", "Ue_1\\", "_2^2 + 2\\beta \\", "Ue_2\\", "_2^2 + 2\\beta}{(\\", "Ue_1\\", "_2^2 + \\beta)^{3/2} - (\\", "Ue_2\\"]], "md": "|\\Delta| = \\frac{\\|Ue_1\\|_2^2 + 2\\beta \\|Ue_2\\|_2^2 + 2\\beta}{(\\|Ue_1\\|_2^2 + \\beta)^{3/2} - (\\|Ue_2\\|_2^2 + \\beta)^{3/2}}", "isPerfectTable": true, "csv": "\"\\Delta\",\"= \\frac{\\\",\"Ue_1\\\",\"_2^2 + 2\\beta \\\",\"Ue_2\\\",\"_2^2 + 2\\beta}{(\\\",\"Ue_1\\\",\"_2^2 + \\beta)^{3/2} - (\\\",\"Ue_2\\\""}, {"type": "text", "value": "= \\frac{\\|Ue_1\\|_2^{3/2} \\cdot x + 4\\beta}{\\|Ue_2\\|_2 (x + \\beta)^{5/2} dx} \\geq \\min_{i \\in \\{1,2\\}} \\frac{3}{2} \\|Ue_i\\|_2^2 + 4\\beta - \\|Ue_2\\|_2| \\quad (222)\n$$\n\nFurthermore, note that for any $$i \\in \\{1, 2\\}$$,\n\n$$\n(\\|Ue_i\\|_2 + \\beta)^{5/2} \\leq 3\\|Ue_i\\|_2 + 4\\beta)^{3/2} \\leq 3\\|Ue_i\\|_2^2 + 12\\beta^{3/2} \\leq 5\\|Ue_i\\|_2^3 \\quad (226)\n$$\n\nwhere (i) uses the convexity of $$(\\cdot)^{3/2}$$ for positive arguments and applies Jensen\u2019s inequality, and the last inequality uses the fact that $$\\|Ue_1\\|_2, \\|Ue_2\\|_2 \\geq 2\\sqrt{\\beta}$$. Overall, combining eq. (219) with eq. (222) and eq. (226) results in the bound,\n\n$$\n\\text{Tr}((WL - LW) \\cdot Z^T U) \\leq 5\\Delta^2 \\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_3^2 \\leq 40\\Delta^2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_3^2 \\quad (228)\n$$\n\nwhere the last inequality uses the assumption that $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\leq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$.\n\n**Case 2: Column norms are separated.** $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\geq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$.", "md": "= \\frac{\\|Ue_1\\|_2^{3/2} \\cdot x + 4\\beta}{\\|Ue_2\\|_2 (x + \\beta)^{5/2} dx} \\geq \\min_{i \\in \\{1,2\\}} \\frac{3}{2} \\|Ue_i\\|_2^2 + 4\\beta - \\|Ue_2\\|_2| \\quad (222)\n$$\n\nFurthermore, note that for any $$i \\in \\{1, 2\\}$$,\n\n$$\n(\\|Ue_i\\|_2 + \\beta)^{5/2} \\leq 3\\|Ue_i\\|_2 + 4\\beta)^{3/2} \\leq 3\\|Ue_i\\|_2^2 + 12\\beta^{3/2} \\leq 5\\|Ue_i\\|_2^3 \\quad (226)\n$$\n\nwhere (i) uses the convexity of $$(\\cdot)^{3/2}$$ for positive arguments and applies Jensen\u2019s inequality, and the last inequality uses the fact that $$\\|Ue_1\\|_2, \\|Ue_2\\|_2 \\geq 2\\sqrt{\\beta}$$. Overall, combining eq. (219) with eq. (222) and eq. (226) results in the bound,\n\n$$\n\\text{Tr}((WL - LW) \\cdot Z^T U) \\leq 5\\Delta^2 \\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_3^2 \\leq 40\\Delta^2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_3^2 \\quad (228)\n$$\n\nwhere the last inequality uses the assumption that $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\leq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$.\n\n**Case 2: Column norms are separated.** $$\\max_{i \\in \\{1,2\\}} \\|Ue_i\\|_2 \\geq 2 \\min_{i \\in \\{1,2\\}} \\|Ue_i\\|_2$$."}]}, {"page": 37, "text": "WLOG suppose \u2225Ue1\u22252 \u2265                 \u2225Ue2\u22252 which implies that D(U)22 \u2265                  D(U)11. The above assumption\nimplies that \u2225Ue1\u22252 \u2265           2\u2225Ue2\u22252. Then,\n                                           D(U)11 =          \u2225Ue1\u22252   2 + 2\u03b2                                                  (229)\n                                                           (\u2225Ue1\u22252   2 + \u03b2)3/2\n                                                       (i)    4\u2225Ue2\u22252   2 + 2\u03b2\n                                                       \u2264   (4\u2225Ue2\u22252    2 + \u03b2)3/2                                              (230)\n                                                       (ii)   4      \u2225Ue2\u22252   2 + 2\u03b2\n                                                       \u2264    33/2   (\u2225Ue2\u22252   2 + \u03b2)3/2                                        (231)\n                                                       \u2264   4                                                                  (232)\n                                                           5D(U)22\n                                           x+2\u03b2\nwhere (i) uses the fact that             (x+\u03b2)3/2 is a decreasing function in x and (ii) uses the fact that\nmin{\u2225Ue1\u22252, \u2225Ue2\u22252} \u2265               2\u221a\u03b2, and therefore, 4\u2225Ue2\u22252            2 + \u03b2 \u2265     3(\u2225Ue2\u22252    2 + \u03b2). Therefore,\n               |D(U)11 \u2212       D(U)22| \u2265        1                      \u2225Ue2\u22252   2 + 2\u03b2                                        (233)\n                                                5D(U)22 = 1       5 (\u2225Ue2\u22252    2 + \u03b2)3/2\n                                                              (i)    \u221a        \u2225Ue2\u22252   2\u221a                                     (234)\n                                                              \u2265   1\n                                                              (ii)5    2\u2225Ue2\u22253    2 +      2\u03b23/2\n                                                                    1       1\n                                                              \u2265    10  \u2225Ue2\u22252                                                 (235)\n                                                              \u2265   1             1          |\u2225Ue1\u22252   2 \u2212   \u2225Ue2\u22252   2|        (236)\n                                                                  10  \u2225Ue2\u22252\u2225Ue1\u22252       2\nwhere (i) uses the convexity of (\u00b7)3/2 for positive arguments and an application of Jensen\u2019s inequality,\nwhile (ii) uses the fact that min{\u2225Ue1\u22252, \u2225Ue2\u22252} \u2265                      2\u221a\u03b2. Since |D(U)11 \u2212             D(U)22| \u2264       |\u2206|, this\nresults in an upper bound on |\u2225Ue1\u22252             2 \u2212   \u2225Ue2\u22252   2|. Combining this with eq. (219) results in,\n                       Tr((WL \u2212       LW) \u00b7 ZT U) \u2264          10\u22062 mini\u2208{1,2} \u2225Uei\u22252 max   i\u2208{1,2} \u2225Uei\u22252    2.                (237)\nCombining eqs. (228) and (237) completes the proof of the lemma.\nB.3.2      Proof of Lemma 27\nProof. By definition,\n                      vec(Z)T [\u22072R\u03b2(U)]vec(Z) \u2212      k           4\u27e8R\u03b2(U), vec(Z)\u27e9                                             (238)\n                       \u2264  Tr(D(U)ZT Z) \u2212           i=1  (G(U))ii\u27e8Uei, Zei\u27e92 \u2212            4Tr(D(U)U T Z)                       (239)\n                       \u2264  Tr(D(U)ZT Z) \u2212           4Tr(D(U)U T Z)                                                             (240)\n                       \u2264   \u221a2 \u03b2 \u2225Z\u22252  F + 4\u2225D(U)\u2225F \u2225U T Z\u2225F                                                                   (241)\n                       (i) \u221a 2\u03b2 \u2225Z\u22252              k\n                       \u2264              F + 5       \u03b2 \u2225UU T \u2212       U\u22c6U T \u22c6  \u2225F                                                 (242)\n                       \u2264  8  2k  \u03b2 \u2225UU T \u2212       U\u22c6U T \u22c6  \u2225F                                                x+2\u03b2            2 (243)\nwhere (i) follows from the fact that for any i \u2208                    [k], |(D(U))ii| \u2264        maxx\u22650       (x+\u03b2)3/2 =       \u221a \u03b2 , (ii)\nuses eq. (172) and the fact that \u2225Z\u22254           F = \u2225ZT \u22254    F \u2264    rank(ZT )\u2225ZZT \u22252        F \u2264    2k\u2225UU T \u2212        U\u22c6U T \u22c6 \u22252F by\neq. (171).\n                                                                 37", "md": "# Math Equations\n\nWLOG suppose $$\\|Ue1\\|_2 \\geq \\|Ue2\\|_2$$ which implies that $$D(U)_{22} \\geq D(U)_{11}$$. The above assumption implies that $$\\|Ue1\\|_2 \\geq 2\\|Ue2\\|_2$$. Then,\n\n$$D(U)_{11} = \\frac{\\|Ue1\\|_2^2}{2} + 2\\beta \\quad (229)$$\n\n$$\\begin{align*} &4\\|Ue2\\|_2^2 + 2\\beta \\\\ &\\leq (4\\|Ue2\\|_2^2 + \\beta)^{3/2} \\quad (230) \\\\ &\\leq 3^{3/2}(\\|Ue2\\|_2^2 + \\beta)^{3/2} \\quad (231) \\\\ &\\leq 4 \\quad (232) \\end{align*}$$\n\nwhere (i) uses the fact that $$(x+\\beta)^{3/2}$$ is a decreasing function in x and (ii) uses the fact that $$\\min\\{\\|Ue1\\|_2, \\|Ue2\\|_2\\} \\geq 2\\sqrt{\\beta}$$, and therefore, $$4\\|Ue2\\|_2^2 + \\beta \\geq 3(\\|Ue2\\|_2^2 + \\beta)$$. Therefore,\n\n$$|D(U)_{11} - D(U)_{22}| \\geq \\frac{1}{5}(\\|Ue2\\|_2^2 + 2\\beta) \\quad (233)$$\n\n$$\\begin{align*} &\\frac{1}{5}D(U)_{22} \\\\ &\\geq 1 \\quad (i) \\\\ &\\geq 10\\|Ue2\\|_2^2 + 2\\beta^{3/2} \\quad (ii) \\\\ &\\geq 10\\|Ue2\\|_2 \\quad (235) \\\\ &\\geq 1 \\quad (236) \\end{align*}$$\n\nwhere (i) uses the convexity of $$(\\cdot)^{3/2}$$ for positive arguments and an application of Jensen\u2019s inequality, while (ii) uses the fact that $$\\min\\{\\|Ue1\\|_2, \\|Ue2\\|_2\\} \\geq 2\\sqrt{\\beta}$$. Since $$|D(U)_{11} - D(U)_{22}| \\leq |\\Delta|$$, this results in an upper bound on $$|\\|Ue1\\|_2^2 - \\|Ue2\\|_2^2$$. Combining this with eq. (219) results in,\n\n$$\\text{Tr}((WL - LW) \\cdot Z^T U) \\leq 10\\Delta^2 \\min_{i\\in\\{1,2\\} \\|\\|Ue_i\\|_2 \\max_{i\\in\\{1,2\\} \\|\\|Ue_i\\|_2^2} \\quad (237)$$\n\nCombining eqs. (228) and (237) completes the proof of the lemma.\n\nProof of Lemma 27\n\nBy definition,\n\n$$\\begin{align*} &\\text{vec}(Z)^T [\\nabla^2R_\\beta(U)]\\text{vec}(Z) - k4\\langle R_\\beta(U), \\text{vec}(Z) \\rangle \\\\ &\\leq \\text{Tr}(D(U)Z^T Z) - \\sum_{i=1}^{k}(G(U))_{ii}\\langle Ue_i, Ze_i \\rangle^2 - 4\\text{Tr}(D(U)UTZ) \\\\ &\\leq \\text{Tr}(D(U)Z^T Z) - 4\\text{Tr}(D(U)UTZ) \\\\ &\\leq \\sqrt{2}\\beta\\|Z\\|_2^F + 4\\|D(U)\\|_F\\|UTZ\\|_F \\quad (241) \\\\ &\\leq \\sqrt{2}\\beta\\|Z\\|_2^k + 5\\beta\\|UUT - U^*UT^*\\|_F \\quad (242) \\\\ &\\leq 8\\sqrt{2}k\\beta\\|UUT - U^*UT^*\\|_F \\quad (243) \\end{align*}$$\n\nwhere (i) follows from the fact that for any $$i \\in [k], |(D(U))_{ii}| \\leq \\max_{x\\geq 0} (x+\\beta)^{3/2} = \\sqrt{\\beta}$$, (ii) uses eq. (172) and the fact that $$\\|Z\\|_4^F = \\|Z^T\\|_4^F \\leq \\text{rank}(Z^T)\\|ZZ^T\\|_2^F \\leq 2k\\|UUT - U^*UT^*\\|_2^F$$ by eq. (171).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "WLOG suppose $$\\|Ue1\\|_2 \\geq \\|Ue2\\|_2$$ which implies that $$D(U)_{22} \\geq D(U)_{11}$$. The above assumption implies that $$\\|Ue1\\|_2 \\geq 2\\|Ue2\\|_2$$. Then,\n\n$$D(U)_{11} = \\frac{\\|Ue1\\|_2^2}{2} + 2\\beta \\quad (229)$$\n\n$$\\begin{align*} &4\\|Ue2\\|_2^2 + 2\\beta \\\\ &\\leq (4\\|Ue2\\|_2^2 + \\beta)^{3/2} \\quad (230) \\\\ &\\leq 3^{3/2}(\\|Ue2\\|_2^2 + \\beta)^{3/2} \\quad (231) \\\\ &\\leq 4 \\quad (232) \\end{align*}$$\n\nwhere (i) uses the fact that $$(x+\\beta)^{3/2}$$ is a decreasing function in x and (ii) uses the fact that $$\\min\\{\\|Ue1\\|_2, \\|Ue2\\|_2\\} \\geq 2\\sqrt{\\beta}$$, and therefore, $$4\\|Ue2\\|_2^2 + \\beta \\geq 3(\\|Ue2\\|_2^2 + \\beta)$$. Therefore,\n\n$$|D(U)_{11} - D(U)_{22}| \\geq \\frac{1}{5}(\\|Ue2\\|_2^2 + 2\\beta) \\quad (233)$$\n\n$$\\begin{align*} &\\frac{1}{5}D(U)_{22} \\\\ &\\geq 1 \\quad (i) \\\\ &\\geq 10\\|Ue2\\|_2^2 + 2\\beta^{3/2} \\quad (ii) \\\\ &\\geq 10\\|Ue2\\|_2 \\quad (235) \\\\ &\\geq 1 \\quad (236) \\end{align*}$$\n\nwhere (i) uses the convexity of $$(\\cdot)^{3/2}$$ for positive arguments and an application of Jensen\u2019s inequality, while (ii) uses the fact that $$\\min\\{\\|Ue1\\|_2, \\|Ue2\\|_2\\} \\geq 2\\sqrt{\\beta}$$. Since $$|D(U)_{11} - D(U)_{22}| \\leq |\\Delta|$$, this results in an upper bound on $$|\\|Ue1\\|_2^2 - \\|Ue2\\|_2^2$$. Combining this with eq. (219) results in,\n\n$$\\text{Tr}((WL - LW) \\cdot Z^T U) \\leq 10\\Delta^2 \\min_{i\\in\\{1,2\\} \\|\\|Ue_i\\|_2 \\max_{i\\in\\{1,2\\} \\|\\|Ue_i\\|_2^2} \\quad (237)$$\n\nCombining eqs. (228) and (237) completes the proof of the lemma.\n\nProof of Lemma 27\n\nBy definition,\n\n$$\\begin{align*} &\\text{vec}(Z)^T [\\nabla^2R_\\beta(U)]\\text{vec}(Z) - k4\\langle R_\\beta(U), \\text{vec}(Z) \\rangle \\\\ &\\leq \\text{Tr}(D(U)Z^T Z) - \\sum_{i=1}^{k}(G(U))_{ii}\\langle Ue_i, Ze_i \\rangle^2 - 4\\text{Tr}(D(U)UTZ) \\\\ &\\leq \\text{Tr}(D(U)Z^T Z) - 4\\text{Tr}(D(U)UTZ) \\\\ &\\leq \\sqrt{2}\\beta\\|Z\\|_2^F + 4\\|D(U)\\|_F\\|UTZ\\|_F \\quad (241) \\\\ &\\leq \\sqrt{2}\\beta\\|Z\\|_2^k + 5\\beta\\|UUT - U^*UT^*\\|_F \\quad (242) \\\\ &\\leq 8\\sqrt{2}k\\beta\\|UUT - U^*UT^*\\|_F \\quad (243) \\end{align*}$$\n\nwhere (i) follows from the fact that for any $$i \\in [k], |(D(U))_{ii}| \\leq \\max_{x\\geq 0} (x+\\beta)^{3/2} = \\sqrt{\\beta}$$, (ii) uses eq. (172) and the fact that $$\\|Z\\|_4^F = \\|Z^T\\|_4^F \\leq \\text{rank}(Z^T)\\|ZZ^T\\|_2^F \\leq 2k\\|UUT - U^*UT^*\\|_2^F$$ by eq. (171).", "md": "WLOG suppose $$\\|Ue1\\|_2 \\geq \\|Ue2\\|_2$$ which implies that $$D(U)_{22} \\geq D(U)_{11}$$. The above assumption implies that $$\\|Ue1\\|_2 \\geq 2\\|Ue2\\|_2$$. Then,\n\n$$D(U)_{11} = \\frac{\\|Ue1\\|_2^2}{2} + 2\\beta \\quad (229)$$\n\n$$\\begin{align*} &4\\|Ue2\\|_2^2 + 2\\beta \\\\ &\\leq (4\\|Ue2\\|_2^2 + \\beta)^{3/2} \\quad (230) \\\\ &\\leq 3^{3/2}(\\|Ue2\\|_2^2 + \\beta)^{3/2} \\quad (231) \\\\ &\\leq 4 \\quad (232) \\end{align*}$$\n\nwhere (i) uses the fact that $$(x+\\beta)^{3/2}$$ is a decreasing function in x and (ii) uses the fact that $$\\min\\{\\|Ue1\\|_2, \\|Ue2\\|_2\\} \\geq 2\\sqrt{\\beta}$$, and therefore, $$4\\|Ue2\\|_2^2 + \\beta \\geq 3(\\|Ue2\\|_2^2 + \\beta)$$. Therefore,\n\n$$|D(U)_{11} - D(U)_{22}| \\geq \\frac{1}{5}(\\|Ue2\\|_2^2 + 2\\beta) \\quad (233)$$\n\n$$\\begin{align*} &\\frac{1}{5}D(U)_{22} \\\\ &\\geq 1 \\quad (i) \\\\ &\\geq 10\\|Ue2\\|_2^2 + 2\\beta^{3/2} \\quad (ii) \\\\ &\\geq 10\\|Ue2\\|_2 \\quad (235) \\\\ &\\geq 1 \\quad (236) \\end{align*}$$\n\nwhere (i) uses the convexity of $$(\\cdot)^{3/2}$$ for positive arguments and an application of Jensen\u2019s inequality, while (ii) uses the fact that $$\\min\\{\\|Ue1\\|_2, \\|Ue2\\|_2\\} \\geq 2\\sqrt{\\beta}$$. Since $$|D(U)_{11} - D(U)_{22}| \\leq |\\Delta|$$, this results in an upper bound on $$|\\|Ue1\\|_2^2 - \\|Ue2\\|_2^2$$. Combining this with eq. (219) results in,\n\n$$\\text{Tr}((WL - LW) \\cdot Z^T U) \\leq 10\\Delta^2 \\min_{i\\in\\{1,2\\} \\|\\|Ue_i\\|_2 \\max_{i\\in\\{1,2\\} \\|\\|Ue_i\\|_2^2} \\quad (237)$$\n\nCombining eqs. (228) and (237) completes the proof of the lemma.\n\nProof of Lemma 27\n\nBy definition,\n\n$$\\begin{align*} &\\text{vec}(Z)^T [\\nabla^2R_\\beta(U)]\\text{vec}(Z) - k4\\langle R_\\beta(U), \\text{vec}(Z) \\rangle \\\\ &\\leq \\text{Tr}(D(U)Z^T Z) - \\sum_{i=1}^{k}(G(U))_{ii}\\langle Ue_i, Ze_i \\rangle^2 - 4\\text{Tr}(D(U)UTZ) \\\\ &\\leq \\text{Tr}(D(U)Z^T Z) - 4\\text{Tr}(D(U)UTZ) \\\\ &\\leq \\sqrt{2}\\beta\\|Z\\|_2^F + 4\\|D(U)\\|_F\\|UTZ\\|_F \\quad (241) \\\\ &\\leq \\sqrt{2}\\beta\\|Z\\|_2^k + 5\\beta\\|UUT - U^*UT^*\\|_F \\quad (242) \\\\ &\\leq 8\\sqrt{2}k\\beta\\|UUT - U^*UT^*\\|_F \\quad (243) \\end{align*}$$\n\nwhere (i) follows from the fact that for any $$i \\in [k], |(D(U))_{ii}| \\leq \\max_{x\\geq 0} (x+\\beta)^{3/2} = \\sqrt{\\beta}$$, (ii) uses eq. (172) and the fact that $$\\|Z\\|_4^F = \\|Z^T\\|_4^F \\leq \\text{rank}(Z^T)\\|ZZ^T\\|_2^F \\leq 2k\\|UUT - U^*UT^*\\|_2^F$$ by eq. (171)."}]}, {"page": 38, "text": "B.3.3      Proof of Lemma 32\nProof. Let Smax denote the set of r column indices i \u2208                   [k] such that \u2225VrV T   r Uei\u22252 are maximum and\nnon-zero. If fewer than r columns of U have non-zero projections onto Vr (i.e. fewer than r candidates\nfor Smax exist), then all the remaining columns of U must have zero projections onto Vr and we are\ndone. Therefore, henceforth, we will assume that |Smax| = r. Let Vmax = span({VrV T                                  r Uei : i \u2208\nSmax}), a subspace contained in Vr.\nDefine M as the matrix M =                  VrV Tr Uei                     . Let V be the set of non-zero eigenvectors\n                                          \u2225VrV Tr Uei\u22252 : i \u2208     Smax\nof MM T . Note that the matrix M T M \u2208                   R|Smax|\u00d7|Smax| has 1\u2019s on the diagonals, and has its off-\ndiagonal entries upper bounded in absolute value by c1/r by eq. (204). Therefore, by the Gersgorin\ncircle theorem, all eigenvalues of M T M and consequently all non-zero eigenvalues of MM T lie\nin the range [1 \u2212       c1, 1 + c1]. This means that M \u2208              Rd\u00d7r is full rank, and its columns are linearly\nindependent.\nThis implies that span(V ) = span(Vr) and by the orthogonality condition, V V T = VrV T                                r . There-\nfore,\n                         \u2225VrV T r Uei\u22252 = \u2225V V T Uei\u22252                                                                       (244)\n                                                     1\n                                            \u2264   \u03bbmin(M)\u2225M T Uei\u22252                                                            (245)\n                                            =        1                    \u27e8VrV T r Uej, VrV T   r Uei\u27e92                      (246)\n                                                \u03bbmin(M)         j\u2208Smax           \u2225VrV T r Uej\u22252   2\n                                            (i)       1                    c2\n                                                                            1\n                                            \u2264   \u03bbmin(M)          j\u2208Smax    r2 \u2225VrV T r Uei\u22252   2,                            (247)\nwhere (i) follows from eq. (203). Consequently by eq. (247), we have,\n                                        \u2225VrV T r Uei\u22252 \u2264       1 \u2212c1 c1 \u2225VrV T r Uei\u22252.                                      (248)\nFor c1 \u2264     1/3 this means that \u2225VrV T       r Uei\u22252 = 0 for every i \u2208           [k] \\ Smax. This proves the claim.\n                                                                38", "md": "Proof of Lemma 32\n\nProof. Let \\( S_{\\text{max}} \\) denote the set of \\( r \\) column indices \\( i \\in [k] \\) such that \\( \\|V_rV_r^TUe_i\\|_2 \\) are maximum and non-zero. If fewer than \\( r \\) columns of \\( U \\) have non-zero projections onto \\( V_r \\) (i.e. fewer than \\( r \\) candidates for \\( S_{\\text{max}} \\) exist), then all the remaining columns of \\( U \\) must have zero projections onto \\( V_r \\) and we are done. Therefore, henceforth, we will assume that \\( |S_{\\text{max}}| = r \\). Let \\( V_{\\text{max}} = \\text{span}(\\{V_rV_r^TUe_i : i \\in S_{\\text{max}}\\}) \\), a subspace contained in \\( V_r \\).\n\nDefine \\( M \\) as the matrix \\( M = V_rV_r^TUe_i \\). Let \\( V \\) be the set of non-zero eigenvectors\n\n$$ \\|V_rV_r^TUe_i\\|_2 : i \\in S_{\\text{max}} $$\n\nof \\( MM^T \\). Note that the matrix \\( M^TM \\in \\mathbb{R}^{|S_{\\text{max}}| \\times |S_{\\text{max}}| \\) has 1's on the diagonals, and has its off-diagonal entries upper bounded in absolute value by \\( c1/r \\) by eq. (204). Therefore, by the Gersgorin circle theorem, all eigenvalues of \\( M^TM \\) and consequently all non-zero eigenvalues of \\( MM^T \\) lie in the range \\([1 - c1, 1 + c1]\\). This means that \\( M \\in \\mathbb{R}^{d \\times r} \\) is full rank, and its columns are linearly independent.\n\nThis implies that \\( \\text{span}(V) = \\text{span}(V_r) \\) and by the orthogonality condition, \\( VV^T = V_rV_r^T \\). Therefore,\n\n$$ \\|V_rV_r^TUe_i\\|_2 = \\|VV^TUe_i\\|_2 \\quad (244) $$\n$$ \\le \\lambda_{\\text{min}}(M)\\|M^TUe_i\\|_2 \\quad (245) $$\n$$ = \\frac{1}{\\lambda_{\\text{min}}(M)} \\sum_{j \\in S_{\\text{max}}} \\langle V_rV_r^TUe_j, V_rV_r^TUe_i \\rangle^2 \\quad (246) $$\n$$ \\le \\frac{1}{\\lambda_{\\text{min}}(M)} \\sum_{j \\in S_{\\text{max}}} c2r^2 \\|V_rV_r^TUe_i\\|_2^2 \\quad (247) $$\n\nwhere (i) follows from eq. (203). Consequently by eq. (247), we have,\n\n$$ \\|V_rV_r^TUe_i\\|_2 \\le (1 - c1)c1 \\|V_rV_r^TUe_i\\|_2 \\quad (248) $$\n\nFor \\( c1 \\le 1/3 \\) this means that \\( \\|V_rV_r^TUe_i\\|_2 = 0 \\) for every \\( i \\in [k] \\setminus S_{\\text{max}} \\). This proves the claim.\n\n38", "images": [], "items": [{"type": "text", "value": "Proof of Lemma 32\n\nProof. Let \\( S_{\\text{max}} \\) denote the set of \\( r \\) column indices \\( i \\in [k] \\) such that \\( \\|V_rV_r^TUe_i\\|_2 \\) are maximum and non-zero. If fewer than \\( r \\) columns of \\( U \\) have non-zero projections onto \\( V_r \\) (i.e. fewer than \\( r \\) candidates for \\( S_{\\text{max}} \\) exist), then all the remaining columns of \\( U \\) must have zero projections onto \\( V_r \\) and we are done. Therefore, henceforth, we will assume that \\( |S_{\\text{max}}| = r \\). Let \\( V_{\\text{max}} = \\text{span}(\\{V_rV_r^TUe_i : i \\in S_{\\text{max}}\\}) \\), a subspace contained in \\( V_r \\).\n\nDefine \\( M \\) as the matrix \\( M = V_rV_r^TUe_i \\). Let \\( V \\) be the set of non-zero eigenvectors\n\n$$ \\|V_rV_r^TUe_i\\|_2 : i \\in S_{\\text{max}} $$\n\nof \\( MM^T \\). Note that the matrix \\( M^TM \\in \\mathbb{R}^{|S_{\\text{max}}| \\times |S_{\\text{max}}| \\) has 1's on the diagonals, and has its off-diagonal entries upper bounded in absolute value by \\( c1/r \\) by eq. (204). Therefore, by the Gersgorin circle theorem, all eigenvalues of \\( M^TM \\) and consequently all non-zero eigenvalues of \\( MM^T \\) lie in the range \\([1 - c1, 1 + c1]\\). This means that \\( M \\in \\mathbb{R}^{d \\times r} \\) is full rank, and its columns are linearly independent.\n\nThis implies that \\( \\text{span}(V) = \\text{span}(V_r) \\) and by the orthogonality condition, \\( VV^T = V_rV_r^T \\). Therefore,\n\n$$ \\|V_rV_r^TUe_i\\|_2 = \\|VV^TUe_i\\|_2 \\quad (244) $$\n$$ \\le \\lambda_{\\text{min}}(M)\\|M^TUe_i\\|_2 \\quad (245) $$\n$$ = \\frac{1}{\\lambda_{\\text{min}}(M)} \\sum_{j \\in S_{\\text{max}}} \\langle V_rV_r^TUe_j, V_rV_r^TUe_i \\rangle^2 \\quad (246) $$\n$$ \\le \\frac{1}{\\lambda_{\\text{min}}(M)} \\sum_{j \\in S_{\\text{max}}} c2r^2 \\|V_rV_r^TUe_i\\|_2^2 \\quad (247) $$\n\nwhere (i) follows from eq. (203). Consequently by eq. (247), we have,\n\n$$ \\|V_rV_r^TUe_i\\|_2 \\le (1 - c1)c1 \\|V_rV_r^TUe_i\\|_2 \\quad (248) $$\n\nFor \\( c1 \\le 1/3 \\) this means that \\( \\|V_rV_r^TUe_i\\|_2 = 0 \\) for every \\( i \\in [k] \\setminus S_{\\text{max}} \\). This proves the claim.\n\n38", "md": "Proof of Lemma 32\n\nProof. Let \\( S_{\\text{max}} \\) denote the set of \\( r \\) column indices \\( i \\in [k] \\) such that \\( \\|V_rV_r^TUe_i\\|_2 \\) are maximum and non-zero. If fewer than \\( r \\) columns of \\( U \\) have non-zero projections onto \\( V_r \\) (i.e. fewer than \\( r \\) candidates for \\( S_{\\text{max}} \\) exist), then all the remaining columns of \\( U \\) must have zero projections onto \\( V_r \\) and we are done. Therefore, henceforth, we will assume that \\( |S_{\\text{max}}| = r \\). Let \\( V_{\\text{max}} = \\text{span}(\\{V_rV_r^TUe_i : i \\in S_{\\text{max}}\\}) \\), a subspace contained in \\( V_r \\).\n\nDefine \\( M \\) as the matrix \\( M = V_rV_r^TUe_i \\). Let \\( V \\) be the set of non-zero eigenvectors\n\n$$ \\|V_rV_r^TUe_i\\|_2 : i \\in S_{\\text{max}} $$\n\nof \\( MM^T \\). Note that the matrix \\( M^TM \\in \\mathbb{R}^{|S_{\\text{max}}| \\times |S_{\\text{max}}| \\) has 1's on the diagonals, and has its off-diagonal entries upper bounded in absolute value by \\( c1/r \\) by eq. (204). Therefore, by the Gersgorin circle theorem, all eigenvalues of \\( M^TM \\) and consequently all non-zero eigenvalues of \\( MM^T \\) lie in the range \\([1 - c1, 1 + c1]\\). This means that \\( M \\in \\mathbb{R}^{d \\times r} \\) is full rank, and its columns are linearly independent.\n\nThis implies that \\( \\text{span}(V) = \\text{span}(V_r) \\) and by the orthogonality condition, \\( VV^T = V_rV_r^T \\). Therefore,\n\n$$ \\|V_rV_r^TUe_i\\|_2 = \\|VV^TUe_i\\|_2 \\quad (244) $$\n$$ \\le \\lambda_{\\text{min}}(M)\\|M^TUe_i\\|_2 \\quad (245) $$\n$$ = \\frac{1}{\\lambda_{\\text{min}}(M)} \\sum_{j \\in S_{\\text{max}}} \\langle V_rV_r^TUe_j, V_rV_r^TUe_i \\rangle^2 \\quad (246) $$\n$$ \\le \\frac{1}{\\lambda_{\\text{min}}(M)} \\sum_{j \\in S_{\\text{max}}} c2r^2 \\|V_rV_r^TUe_i\\|_2^2 \\quad (247) $$\n\nwhere (i) follows from eq. (203). Consequently by eq. (247), we have,\n\n$$ \\|V_rV_r^TUe_i\\|_2 \\le (1 - c1)c1 \\|V_rV_r^TUe_i\\|_2 \\quad (248) $$\n\nFor \\( c1 \\le 1/3 \\) this means that \\( \\|V_rV_r^TUe_i\\|_2 = 0 \\) for every \\( i \\in [k] \\setminus S_{\\text{max}} \\). This proves the claim.\n\n38"}]}, {"page": 39, "text": "C         Finite sample guarantees: Proof of Theorem 4\nIn this section, we provide guarantees on the approximate second order stationary points of the\nregularized loss femp (eq. (14)) when the dataset is finite in size, and satisfies the RIP condition. We\nprovide a formal proof of Theorem 4.\nFor completeness, in the finite sample setting, the empirical loss is defined as,\n                                                                                n\n                   i.i.d.                         Lemp(U        ) = 1    n    i=1   (\u27e8U     U T \u2212       U\u22c6U T    \u22c6  , Ai\u27e9     + \u03b5i)2                                  (249)\nwhere \u03b5i            \u223c     N    (0, \u03c32) is the measurement noise. Without loss of generality we assume that the\nmeasurement matrices {Ai}n                             i=1 are symmetric, since the empirical loss Lemp is unchanged by the\nsymmetrization Ai \u2192                         Ai+AT2    i  . Note that the loss can be expanded as,\n                              \u2225U    U T \u2212        U\u22c6U T    \u22c6  \u22252 H + 2           n1     n    \u03b5iAi, U        U T \u2212        U\u22c6U T   \u22c6        + 1  n     n   \u03b52i           (250)\n                                                                                     i=1                                                           i=1\nwhere \u27e8X, Y \u27e9H = 1                   m    m   i=1\u27e8X, Ai\u27e9\u27e8Y, Ai\u27e9                    and \u2225X\u22252         H = \u27e8X, X\u27e9H.\nWe will assume that the measurement matrices {Ai}n                                                    i=1 satisfy the RIP condition. At a high level\nthis condition guarantees that \u27e8\u00b7, \u00b7\u27e9H \u2248                                   \u27e8\u00b7, \u00b7\u27e9    when the arguments are low rank matrices.\nDefinition 33 (RIP condition). A set of linear measurement matrices A1, . . . , Am in Rd\u00d7d satisfies\nthe (k, \u03b4)-restricted isometry property (RIP) if for any d \u00d7 d matrix X with rank at most k,\n                                                                                       m\n                                              (1 \u2212      \u03b4)\u2225X\u22252       F \u2264       m1    i=1    \u27e8Ai, X\u27e92 \u2264              (1 + \u03b4)\u2225X\u22252            F                          (251)\nThe crucial property we use in this section is that the RIP condition on the measurements guarantees\nthat \u27e8X, Y \u27e9H \u2248                 \u27e8X, Y \u27e9. when X and Y are low rank matrices.\nLemma 34. Let {Ai}m                      i=1 be a family of matrices in Rd\u00d7d that satisfy (k, \u03b4) RIP. Then for any pair of\nmatrices X, Y \u2208                  Rd\u00d7d with rank at most k, we have:\n                                                          |\u27e8X, Y \u27e9H \u2212              \u27e8X, Y \u27e9| \u2264           \u03b4\u2225X\u2225F \u2225Y \u2225F                                                   (252)\nLemma 35. Let {Ai}n                       i=1 be a set of matrices which satisfy the (2k, \u03b4)-RIP for some \u03b4 \u2264                                                1/10. Let\n     i.i.d.\n\u03b5i    \u223c     N    (0, \u03c32). Consider any \u03b7 \u2208           \uf8eb       1     n(0, 1). Then,                       d log(d/\u03b7)           \uf8f6\n                                                 P   \uf8ed      n      i    Ai\u03f5i        op  \u2265    4\u03c3                  n           \uf8f8    \u2264    \u03b7.                             (253)\nProof. Note that {Ai}n                     i=1 satisfies the (2k, \u03b4)-RIP for \u03b4 \u2264                                 1/10. That is, for any rank \u2264               2k matrix\nX,\n                                                        9             F \u2264       1     n    \u27e8Ai, X\u27e92 \u2264              11            F .                                  (254)\nChoosing X = vuT for any pair of vectors u, v, we get, 10    \u2225X\u22252               n   i=1                            10   \u2225X\u22252\n                                                 \u2200u, v \u2208        Rd,          1      n   (uT Aiv)2 \u2264               11           2\u2225v\u22252    2.                            (255)\n                                                                             n    i=1                             10    \u2225u\u22252\nThis implies that for each i \u2208                             [k], \u2225Ai\u2225op \u2264                \u221a   2n. Furthermore, plugging in v = e1, \u00b7 \u00b7 \u00b7 , ed and\nsumming,\n                                                                     1     n    \u2225AT   i u\u22252   2 \u2264      11              2                                              (256)\n                                                                    n    i=1                           10    d\u2225u\u22252\n                                                                                            39", "md": "Finite sample guarantees: Proof of Theorem 4\n\nIn this section, we provide guarantees on the approximate second order stationary points of the regularized loss \\( f_{\\text{emp}} \\) (eq. (14)) when the dataset is finite in size, and satisfies the RIP condition. We provide a formal proof of Theorem 4.\n\nFor completeness, in the finite sample setting, the empirical loss is defined as,\n\n$$\nL_{\\text{emp}}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\langle UU^T - U^*{U^*}^T, A_i \\rangle + \\epsilon_i \\right)^2 \\quad (249)\n$$\nwhere \\( \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\) is the measurement noise. Without loss of generality we assume that the measurement matrices \\( \\{A_i\\}_{i=1}^n \\) are symmetric, since the empirical loss \\( L_{\\text{emp}} \\) is unchanged by the symmetrization \\( A_i \\rightarrow A_i + A_i^T \\). Note that the loss can be expanded as,\n\n$$\n\\|UU^T - U^*{U^*}^T\\|_H^2 + 2\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i \\langle A_i, UU^T - U^*{U^*}^T \\rangle + \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i^2 \\quad (250)\n$$\nwhere \\( \\langle X, Y \\rangle_H = \\frac{1}{m} \\sum_{i=1}^{m} \\langle X, A_i \\rangle \\langle Y, A_i \\rangle \\) and \\( \\|X\\|_H^2 = \\langle X, X \\rangle_H \\).\n\nWe will assume that the measurement matrices \\( \\{A_i\\}_{i=1}^n \\) satisfy the RIP condition. At a high level this condition guarantees that \\( \\langle \\cdot, \\cdot \\rangle_H \\approx \\langle \\cdot, \\cdot \\rangle \\) when the arguments are low rank matrices.\n\nDefinition 33 (RIP condition). A set of linear measurement matrices \\( A_1, ..., A_m \\) in \\( \\mathbb{R}^{d \\times d} \\) satisfies the \\( (k, \\delta) \\)-restricted isometry property (RIP) if for any \\( d \\times d \\) matrix \\( X \\) with rank at most \\( k \\),\n\n$$\n(1 - \\delta) \\|X\\|_F^2 \\leq \\sum_{i=1}^{m} \\langle A_i, X \\rangle^2 \\leq (1 + \\delta) \\|X\\|_F^2 \\quad (251)\n$$\nThe crucial property we use in this section is that the RIP condition on the measurements guarantees that \\( \\langle X, Y \\rangle_H \\approx \\langle X, Y \\rangle \\) when \\( X \\) and \\( Y \\) are low rank matrices.\n\nLemma 34. Let \\( \\{A_i\\}_{i=1}^m \\) be a family of matrices in \\( \\mathbb{R}^{d \\times d} \\) that satisfy \\( (k, \\delta) \\) RIP. Then for any pair of matrices \\( X, Y \\in \\mathbb{R}^{d \\times d} \\) with rank at most \\( k \\), we have:\n\n$$\n|\\langle X, Y \\rangle_H - \\langle X, Y \\rangle| \\leq \\delta \\|X\\|_F \\|Y\\|_F \\quad (252)\n$$\nLemma 35. Let \\( \\{A_i\\}_{i=1}^n \\) be a set of matrices which satisfy the \\( (2k, \\delta) \\)-RIP for some \\( \\delta \\leq 1/10 \\). Let \\( \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\). Consider any \\( \\eta \\in \\left( 1, \\frac{n}{0, 1} \\right) \\). Then,\n\n$$\nP\\left( \\sum_{i=1}^{n} A_i \\epsilon_i \\geq 4\\sigma \\sqrt{d \\log(d/\\eta)} \\right) \\leq \\eta \\quad (253)\n$$\nProof. Note that \\( \\{A_i\\}_{i=1}^n \\) satisfies the \\( (2k, \\delta) \\)-RIP for \\( \\delta \\leq 1/10 \\). That is, for any rank \\( \\leq 2k \\) matrix \\( X \\),\n\n$$\n9 \\|X\\|_F^2 \\leq \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\leq 11 \\|X\\|_F^2 \\quad (254)\n$$\nChoosing \\( X = vu^T \\) for any pair of vectors \\( u, v \\), we get,\n\n$$\n\\forall u, v \\in \\mathbb{R}^d, \\frac{1}{n} \\sum_{i=1}^{n} (u^T A_i v)^2 \\leq 11^2 \\|v\\|_2^2 \\quad (255)\n$$\nThis implies that for each \\( i \\in [k] \\), \\( \\|A_i\\|_{\\text{op}} \\leq \\sqrt{2n} \\). Furthermore, plugging in \\( v = e_1, ..., e_d \\) and summing,\n\n$$\n\\frac{1}{n} \\|A_i^T u\\|_2^2 \\leq 11^2 d \\|u\\|_2^2 \\quad (256)\n$$", "images": [], "items": [{"type": "text", "value": "Finite sample guarantees: Proof of Theorem 4\n\nIn this section, we provide guarantees on the approximate second order stationary points of the regularized loss \\( f_{\\text{emp}} \\) (eq. (14)) when the dataset is finite in size, and satisfies the RIP condition. We provide a formal proof of Theorem 4.\n\nFor completeness, in the finite sample setting, the empirical loss is defined as,\n\n$$\nL_{\\text{emp}}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\langle UU^T - U^*{U^*}^T, A_i \\rangle + \\epsilon_i \\right)^2 \\quad (249)\n$$\nwhere \\( \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\) is the measurement noise. Without loss of generality we assume that the measurement matrices \\( \\{A_i\\}_{i=1}^n \\) are symmetric, since the empirical loss \\( L_{\\text{emp}} \\) is unchanged by the symmetrization \\( A_i \\rightarrow A_i + A_i^T \\). Note that the loss can be expanded as,\n\n$$\n\\|UU^T - U^*{U^*}^T\\|_H^2 + 2\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i \\langle A_i, UU^T - U^*{U^*}^T \\rangle + \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i^2 \\quad (250)\n$$\nwhere \\( \\langle X, Y \\rangle_H = \\frac{1}{m} \\sum_{i=1}^{m} \\langle X, A_i \\rangle \\langle Y, A_i \\rangle \\) and \\( \\|X\\|_H^2 = \\langle X, X \\rangle_H \\).\n\nWe will assume that the measurement matrices \\( \\{A_i\\}_{i=1}^n \\) satisfy the RIP condition. At a high level this condition guarantees that \\( \\langle \\cdot, \\cdot \\rangle_H \\approx \\langle \\cdot, \\cdot \\rangle \\) when the arguments are low rank matrices.\n\nDefinition 33 (RIP condition). A set of linear measurement matrices \\( A_1, ..., A_m \\) in \\( \\mathbb{R}^{d \\times d} \\) satisfies the \\( (k, \\delta) \\)-restricted isometry property (RIP) if for any \\( d \\times d \\) matrix \\( X \\) with rank at most \\( k \\),\n\n$$\n(1 - \\delta) \\|X\\|_F^2 \\leq \\sum_{i=1}^{m} \\langle A_i, X \\rangle^2 \\leq (1 + \\delta) \\|X\\|_F^2 \\quad (251)\n$$\nThe crucial property we use in this section is that the RIP condition on the measurements guarantees that \\( \\langle X, Y \\rangle_H \\approx \\langle X, Y \\rangle \\) when \\( X \\) and \\( Y \\) are low rank matrices.\n\nLemma 34. Let \\( \\{A_i\\}_{i=1}^m \\) be a family of matrices in \\( \\mathbb{R}^{d \\times d} \\) that satisfy \\( (k, \\delta) \\) RIP. Then for any pair of matrices \\( X, Y \\in \\mathbb{R}^{d \\times d} \\) with rank at most \\( k \\), we have:\n\n$$", "md": "Finite sample guarantees: Proof of Theorem 4\n\nIn this section, we provide guarantees on the approximate second order stationary points of the regularized loss \\( f_{\\text{emp}} \\) (eq. (14)) when the dataset is finite in size, and satisfies the RIP condition. We provide a formal proof of Theorem 4.\n\nFor completeness, in the finite sample setting, the empirical loss is defined as,\n\n$$\nL_{\\text{emp}}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\langle UU^T - U^*{U^*}^T, A_i \\rangle + \\epsilon_i \\right)^2 \\quad (249)\n$$\nwhere \\( \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\) is the measurement noise. Without loss of generality we assume that the measurement matrices \\( \\{A_i\\}_{i=1}^n \\) are symmetric, since the empirical loss \\( L_{\\text{emp}} \\) is unchanged by the symmetrization \\( A_i \\rightarrow A_i + A_i^T \\). Note that the loss can be expanded as,\n\n$$\n\\|UU^T - U^*{U^*}^T\\|_H^2 + 2\\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i \\langle A_i, UU^T - U^*{U^*}^T \\rangle + \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon_i^2 \\quad (250)\n$$\nwhere \\( \\langle X, Y \\rangle_H = \\frac{1}{m} \\sum_{i=1}^{m} \\langle X, A_i \\rangle \\langle Y, A_i \\rangle \\) and \\( \\|X\\|_H^2 = \\langle X, X \\rangle_H \\).\n\nWe will assume that the measurement matrices \\( \\{A_i\\}_{i=1}^n \\) satisfy the RIP condition. At a high level this condition guarantees that \\( \\langle \\cdot, \\cdot \\rangle_H \\approx \\langle \\cdot, \\cdot \\rangle \\) when the arguments are low rank matrices.\n\nDefinition 33 (RIP condition). A set of linear measurement matrices \\( A_1, ..., A_m \\) in \\( \\mathbb{R}^{d \\times d} \\) satisfies the \\( (k, \\delta) \\)-restricted isometry property (RIP) if for any \\( d \\times d \\) matrix \\( X \\) with rank at most \\( k \\),\n\n$$\n(1 - \\delta) \\|X\\|_F^2 \\leq \\sum_{i=1}^{m} \\langle A_i, X \\rangle^2 \\leq (1 + \\delta) \\|X\\|_F^2 \\quad (251)\n$$\nThe crucial property we use in this section is that the RIP condition on the measurements guarantees that \\( \\langle X, Y \\rangle_H \\approx \\langle X, Y \\rangle \\) when \\( X \\) and \\( Y \\) are low rank matrices.\n\nLemma 34. Let \\( \\{A_i\\}_{i=1}^m \\) be a family of matrices in \\( \\mathbb{R}^{d \\times d} \\) that satisfy \\( (k, \\delta) \\) RIP. Then for any pair of matrices \\( X, Y \\in \\mathbb{R}^{d \\times d} \\) with rank at most \\( k \\), we have:\n\n$$"}, {"type": "table", "rows": [["\\langle X, Y \\rangle_H - \\langle X, Y \\rangle", "\\leq \\delta \\", "X\\", "_F \\", "Y\\"]], "md": "|\\langle X, Y \\rangle_H - \\langle X, Y \\rangle| \\leq \\delta \\|X\\|_F \\|Y\\|_F \\quad (252)", "isPerfectTable": true, "csv": "\"\\langle X, Y \\rangle_H - \\langle X, Y \\rangle\",\"\\leq \\delta \\\",\"X\\\",\"_F \\\",\"Y\\\""}, {"type": "text", "value": "$$\nLemma 35. Let \\( \\{A_i\\}_{i=1}^n \\) be a set of matrices which satisfy the \\( (2k, \\delta) \\)-RIP for some \\( \\delta \\leq 1/10 \\). Let \\( \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\). Consider any \\( \\eta \\in \\left( 1, \\frac{n}{0, 1} \\right) \\). Then,\n\n$$\nP\\left( \\sum_{i=1}^{n} A_i \\epsilon_i \\geq 4\\sigma \\sqrt{d \\log(d/\\eta)} \\right) \\leq \\eta \\quad (253)\n$$\nProof. Note that \\( \\{A_i\\}_{i=1}^n \\) satisfies the \\( (2k, \\delta) \\)-RIP for \\( \\delta \\leq 1/10 \\). That is, for any rank \\( \\leq 2k \\) matrix \\( X \\),\n\n$$\n9 \\|X\\|_F^2 \\leq \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\leq 11 \\|X\\|_F^2 \\quad (254)\n$$\nChoosing \\( X = vu^T \\) for any pair of vectors \\( u, v \\), we get,\n\n$$\n\\forall u, v \\in \\mathbb{R}^d, \\frac{1}{n} \\sum_{i=1}^{n} (u^T A_i v)^2 \\leq 11^2 \\|v\\|_2^2 \\quad (255)\n$$\nThis implies that for each \\( i \\in [k] \\), \\( \\|A_i\\|_{\\text{op}} \\leq \\sqrt{2n} \\). Furthermore, plugging in \\( v = e_1, ..., e_d \\) and summing,\n\n$$\n\\frac{1}{n} \\|A_i^T u\\|_2^2 \\leq 11^2 d \\|u\\|_2^2 \\quad (256)\n$$", "md": "$$\nLemma 35. Let \\( \\{A_i\\}_{i=1}^n \\) be a set of matrices which satisfy the \\( (2k, \\delta) \\)-RIP for some \\( \\delta \\leq 1/10 \\). Let \\( \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2) \\). Consider any \\( \\eta \\in \\left( 1, \\frac{n}{0, 1} \\right) \\). Then,\n\n$$\nP\\left( \\sum_{i=1}^{n} A_i \\epsilon_i \\geq 4\\sigma \\sqrt{d \\log(d/\\eta)} \\right) \\leq \\eta \\quad (253)\n$$\nProof. Note that \\( \\{A_i\\}_{i=1}^n \\) satisfies the \\( (2k, \\delta) \\)-RIP for \\( \\delta \\leq 1/10 \\). That is, for any rank \\( \\leq 2k \\) matrix \\( X \\),\n\n$$\n9 \\|X\\|_F^2 \\leq \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\leq 11 \\|X\\|_F^2 \\quad (254)\n$$\nChoosing \\( X = vu^T \\) for any pair of vectors \\( u, v \\), we get,\n\n$$\n\\forall u, v \\in \\mathbb{R}^d, \\frac{1}{n} \\sum_{i=1}^{n} (u^T A_i v)^2 \\leq 11^2 \\|v\\|_2^2 \\quad (255)\n$$\nThis implies that for each \\( i \\in [k] \\), \\( \\|A_i\\|_{\\text{op}} \\leq \\sqrt{2n} \\). Furthermore, plugging in \\( v = e_1, ..., e_d \\) and summing,\n\n$$\n\\frac{1}{n} \\|A_i^T u\\|_2^2 \\leq 11^2 d \\|u\\|_2^2 \\quad (256)\n$$"}]}, {"page": 40, "text": "This implies that          1   n  i=1 AiAT    i                     1   n  i=1 AiAT    i    \u2264  2d. Hence, we obtain,\n                           n                      op = \u03bbmax         n\n                                n   E    (\u03f5iAi)(\u03f5iAi)T               \u2264   \u03c32      n   AiAT   i       \u2264   2\u03c32dn.                          (257)\n                               i=1                               op             i=1             op\nBy applying the matrix Bernstein inequality [52, 53], for any t > 0,\n    P       1    n   Ai\u03f5i       \u2265   t    \u2264   d \u00b7 exp                \u22123t2n2    \u221a              = d \u00b7 exp                \u22123t2n               .\n          n    i=1           2                            12dn\u03c32 + 2\u03c3            2nnt                         12d\u03c32 + 4\u03c3\u221ant\nChoosing t = 3\u03c3          d log(d/\u03b7), the RHS is upper bounded by \u03b7. This completes the proof.\n                                  n\nBy virtue of this result, we expect that for all U, with probability \u2265                              1 \u2212    \u03b7, the loss L in eq. (250)\nis \u2248   \u2225UU T \u2212        U\u22c6U T \u22c6  \u22252F (up to additive constants) when n is large. Indeed, by an application of\nHolder\u2019s inequality, Tr(AB) \u2264                \u2225A\u2225op\u2225B\u2225\u2217         \u2264   \u2225A\u2225op        rank(B)\u2225B\u2225F and,\n                                                              n    \u03b52             kd log(d/\u03b7)       \u2225UU T \u2212       U\u22c6U T                 (258)\n                                              \u22c6 \u22252H \u2212     1         i   \u2272   \u03c3                                            \u22c6  \u2225F .\n        Lemp(U) \u2212        \u2225UU T \u2212       U\u22c6U T              n  i=1                          n\nLikewise,\n       \u27e8\u2207Lemp(U), Z\u27e9           \u2212   \u27e8\u2207\u2225UU T \u2212         U\u22c6U T      H, Z\u27e9      \u2272   \u03c3     kd log(d/\u03b7)       \u2225UZT + ZU T \u2225F                   (259)\nAnd finally,                                                \u22c6  \u22252                            n\n vec(Z)T [\u22072Lemp(U)]vec(Z) \u2212                     vec(Z)T [\u22072\u2225UU T \u2212               U\u22c6U T  \u22c6 \u22252H]vec(Z)         \u2272   \u03c3     kd log(d/\u03b7)       \u2225ZZT \u2225F\n                                                                                                                                n\n                                                                                                                                        (260)\nwhere the hidden constants in each of these inequalities are at most 16.\nIn the sequel, we condition on the event that eqs. (258) to (260) hold, which occurs with probability\n\u2265  1 \u2212    \u03b7.\nLemmas 22 and 24:                 The conclusion of these lemmas can still be applied since the finite sample\nloss function eq. (249) is still of the form f(UU T ) for a doubly differentiable f.\nLemma 26:           This lemma is slightly modified in the finite sample setting. The new result is provided\nbelow.\nLemma 36 (Modified Lemma 26). At an (\u03f5, \u03b3)-approximate second order stationary point of\nL + \u03bbR\u03b2,          \u2225UU T \u2212       U\u22c6U T                       \u03f52/3k1/6, \u03bb          k            kd log(d/\u03b7)       , \u221a  k\u03b3     .           (261)\n                                      \u22c6  \u2225F \u2272     max                            \u03b2 + \u03c3                n\nProof. From [36, Lemma 7] and eq. (260) as in Lemma 26, for the choice Z = U \u2212                                                U\u22c6R\u22c6, with\nR\u22c6   \u2208  arg minR:RRT =RT R=I \u2225U \u2212                   U\u22c6R\u22252   F , we have the following bound,\n     vec(Z)T [\u22072Lemp(U)]vec(Z) \u2264                      2\u2225ZZT \u22252     H \u2212    6\u2225UU T \u2212        U\u22c6U T  \u22c6 \u22252H + 4\u27e8\u2207Lemp(U), Z\u27e9\n                                                         + \u03bb     vec(Z)T [\u22072R\u03b2(U)]vec(Z) \u2212                     4\u03bb\u27e8\u2207R\u03b2(U), Z\u27e9\n                                                         + 16\u03c3      kd log(kd/\u03b7)           \u2225ZZT \u2225F                                      (262)\n                                                                               n\nPlugging eq. (171) into eq. (262) and using Lemma 27 and the fact that \u2225\u2207Lemp(U)\u2225F \u2264                                              \u03f5,\n  vec(Z)T [\u22072Lemp(U)]vec(Z) \u2264                      2\u2225ZZT \u22252     H \u2212    6\u2225UU T \u2212        U\u22c6U T \u22c6  \u22252H + 4\u03f5\u2225Z\u2225F\n                                                                 2k                kd log(d/\u03b7)\n                                                  +      8\u03bb      \u03b2 + 32\u03c3                   n             \u2225UU T \u2212       U\u22c6U T \u22c6  \u2225F . (263)\n                                                                      40", "md": "This implies that $$\\sum_{i=1}^{n} AiA^T_i \\leq 2d$$. Hence, we obtain,\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\lambda_{\\text{op}} \\sum_{i=1}^{n} E((\\epsilon_i Ai)(\\epsilon_i A_i)^T) \\leq \\sigma^2 \\frac{1}{n} \\sum_{i=1}^{n} AiA^T_i \\leq 2\\sigma^2dn. \\tag{257}\n\\end{align*}\n$$\nBy applying the matrix Bernstein inequality [52, 53], for any $$t > 0$$,\n\n$$\n\\begin{align*}\n&P\\left(\\frac{1}{n} \\sum_{i=1}^{n} Ai\\epsilon_i \\geq t\\right) \\leq d \\cdot \\exp\\left(-\\frac{3t^2n^2}{12dn\\sigma^2 + 2\\sigma^2n^t}\\right) = d \\cdot \\exp\\left(-3t^2n\\right).\n\\end{align*}\n$$\nChoosing $$t = 3\\sigma d \\log\\left(\\frac{d}{\\eta}\\right)$$, the RHS is upper bounded by $$\\eta$$. This completes the proof.\n\nBy virtue of this result, we expect that for all U, with probability $$\\geq 1 - \\eta$$, the loss L in eq. (250) is approximately $$\\|UU^T - U^*U^{T*}\\|_F^2$$ (up to additive constants) when n is large. Indeed, by an application of Holder\u2019s inequality,\n\n$$\n\\begin{align*}\n&\\text{Tr}(AB) \\leq \\|A\\|_{\\text{op}}\\|B\\|_* \\leq \\|A\\|_{\\text{op}}\\text{rank}(B)\\|B\\|_F \\quad \\text{and,} \\\\\n&\\epsilon^2 kd \\log\\left(\\frac{d}{\\eta}\\right) \\|UU^T - U^*U^{T*}\\|_F \\lesssim \\sigma \\|F\\|_F. \\tag{258}\n\\end{align*}\n$$\nLikewise,\n\n$$\n\\begin{align*}\n&\\langle\\nabla L_{\\text{emp}}(U), Z\\rangle - \\langle\\nabla\\|UU^T - U^*U^{T*}\\|_H, Z\\rangle \\lesssim \\sigma kd \\log\\left(\\frac{d}{\\eta}\\right) \\|UZ^T + ZU^T\\|_F. \\tag{259}\n\\end{align*}\n$$\nAnd finally,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) - \\text{vec}(Z)^T [\\nabla^2\\|UU^T - U^*U^{T*}\\|_H]\\text{vec}(Z) \\lesssim \\sigma kd \\log\\left(\\frac{d}{\\eta}\\right) \\|ZZ^T\\|_F. \\tag{260}\n\\end{align*}\n$$\nwhere the hidden constants in each of these inequalities are at most 16.\n\nIn the sequel, we condition on the event that eqs. (258) to (260) hold, which occurs with probability $$\\geq 1 - \\eta$$.\n\nLemmas 22 and 24: The conclusion of these lemmas can still be applied since the finite sample loss function eq. (249) is still of the form f(UU^T) for a doubly differentiable f.\n\nLemma 26: This lemma is slightly modified in the finite sample setting. The new result is provided below.\n\nLemma 36 (Modified Lemma 26). At an (\u03f5, \u03b3)-approximate second order stationary point of $$L + \\lambda R\\beta$$, $$\\|UU^T - U^*U^{T*}\\|_F \\leq \\max(\\beta + \\sigma, n) \\epsilon^{2/3} k^{1/6}, \\lambda k, kd \\log(d/\\eta), \\sqrt{k}\\gamma$$. (261)\n\nProof. From [36, Lemma 7] and eq. (260) as in Lemma 26, for the choice $$Z = U - U^*R^*$$, with $$R^* \\in \\arg \\min R: R^T R = R^T R = I \\|U - U^*R\\|_F^2$$, we have the following bound,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_H - 6\\|UU^T - U^*U^{T*}\\|_H + 4\\langle\\nabla L_{\\text{emp}}(U), Z\\rangle \\\\\n&+ \\lambda \\text{vec}(Z)^T [\\nabla^2 R\\beta(U)]\\text{vec}(Z) - 4\\lambda\\langle\\nabla R\\beta(U), Z\\rangle + 16\\sigma kd \\log(kd/\\eta) \\|ZZ^T\\|_F. \\tag{262}\n\\end{align*}\n$$\nPlugging eq. (171) into eq. (262) and using Lemma 27 and the fact that $$\\|\\nabla L_{\\text{emp}}(U)\\|_F \\leq \\epsilon$$,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_H - 6\\|UU^T - U^*U^{T*}\\|_H + 4\\epsilon\\|Z\\|_F \\\\\n&+ 8\\lambda \\beta + 32\\sigma n \\|UU^T - U^*U^{T*}\\|_F. \\tag{263}\n\\end{align*}\n$$\nwhere the hidden constants in each of these inequalities are at most 16.", "images": [], "items": [{"type": "text", "value": "This implies that $$\\sum_{i=1}^{n} AiA^T_i \\leq 2d$$. Hence, we obtain,\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\lambda_{\\text{op}} \\sum_{i=1}^{n} E((\\epsilon_i Ai)(\\epsilon_i A_i)^T) \\leq \\sigma^2 \\frac{1}{n} \\sum_{i=1}^{n} AiA^T_i \\leq 2\\sigma^2dn. \\tag{257}\n\\end{align*}\n$$\nBy applying the matrix Bernstein inequality [52, 53], for any $$t > 0$$,\n\n$$\n\\begin{align*}\n&P\\left(\\frac{1}{n} \\sum_{i=1}^{n} Ai\\epsilon_i \\geq t\\right) \\leq d \\cdot \\exp\\left(-\\frac{3t^2n^2}{12dn\\sigma^2 + 2\\sigma^2n^t}\\right) = d \\cdot \\exp\\left(-3t^2n\\right).\n\\end{align*}\n$$\nChoosing $$t = 3\\sigma d \\log\\left(\\frac{d}{\\eta}\\right)$$, the RHS is upper bounded by $$\\eta$$. This completes the proof.\n\nBy virtue of this result, we expect that for all U, with probability $$\\geq 1 - \\eta$$, the loss L in eq. (250) is approximately $$\\|UU^T - U^*U^{T*}\\|_F^2$$ (up to additive constants) when n is large. Indeed, by an application of Holder\u2019s inequality,\n\n$$\n\\begin{align*}\n&\\text{Tr}(AB) \\leq \\|A\\|_{\\text{op}}\\|B\\|_* \\leq \\|A\\|_{\\text{op}}\\text{rank}(B)\\|B\\|_F \\quad \\text{and,} \\\\\n&\\epsilon^2 kd \\log\\left(\\frac{d}{\\eta}\\right) \\|UU^T - U^*U^{T*}\\|_F \\lesssim \\sigma \\|F\\|_F. \\tag{258}\n\\end{align*}\n$$\nLikewise,\n\n$$\n\\begin{align*}\n&\\langle\\nabla L_{\\text{emp}}(U), Z\\rangle - \\langle\\nabla\\|UU^T - U^*U^{T*}\\|_H, Z\\rangle \\lesssim \\sigma kd \\log\\left(\\frac{d}{\\eta}\\right) \\|UZ^T + ZU^T\\|_F. \\tag{259}\n\\end{align*}\n$$\nAnd finally,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) - \\text{vec}(Z)^T [\\nabla^2\\|UU^T - U^*U^{T*}\\|_H]\\text{vec}(Z) \\lesssim \\sigma kd \\log\\left(\\frac{d}{\\eta}\\right) \\|ZZ^T\\|_F. \\tag{260}\n\\end{align*}\n$$\nwhere the hidden constants in each of these inequalities are at most 16.\n\nIn the sequel, we condition on the event that eqs. (258) to (260) hold, which occurs with probability $$\\geq 1 - \\eta$$.\n\nLemmas 22 and 24: The conclusion of these lemmas can still be applied since the finite sample loss function eq. (249) is still of the form f(UU^T) for a doubly differentiable f.\n\nLemma 26: This lemma is slightly modified in the finite sample setting. The new result is provided below.\n\nLemma 36 (Modified Lemma 26). At an (\u03f5, \u03b3)-approximate second order stationary point of $$L + \\lambda R\\beta$$, $$\\|UU^T - U^*U^{T*}\\|_F \\leq \\max(\\beta + \\sigma, n) \\epsilon^{2/3} k^{1/6}, \\lambda k, kd \\log(d/\\eta), \\sqrt{k}\\gamma$$. (261)\n\nProof. From [36, Lemma 7] and eq. (260) as in Lemma 26, for the choice $$Z = U - U^*R^*$$, with $$R^* \\in \\arg \\min R: R^T R = R^T R = I \\|U - U^*R\\|_F^2$$, we have the following bound,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_H - 6\\|UU^T - U^*U^{T*}\\|_H + 4\\langle\\nabla L_{\\text{emp}}(U), Z\\rangle \\\\\n&+ \\lambda \\text{vec}(Z)^T [\\nabla^2 R\\beta(U)]\\text{vec}(Z) - 4\\lambda\\langle\\nabla R\\beta(U), Z\\rangle + 16\\sigma kd \\log(kd/\\eta) \\|ZZ^T\\|_F. \\tag{262}\n\\end{align*}\n$$\nPlugging eq. (171) into eq. (262) and using Lemma 27 and the fact that $$\\|\\nabla L_{\\text{emp}}(U)\\|_F \\leq \\epsilon$$,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_H - 6\\|UU^T - U^*U^{T*}\\|_H + 4\\epsilon\\|Z\\|_F \\\\\n&+ 8\\lambda \\beta + 32\\sigma n \\|UU^T - U^*U^{T*}\\|_F. \\tag{263}\n\\end{align*}\n$$\nwhere the hidden constants in each of these inequalities are at most 16.", "md": "This implies that $$\\sum_{i=1}^{n} AiA^T_i \\leq 2d$$. Hence, we obtain,\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\lambda_{\\text{op}} \\sum_{i=1}^{n} E((\\epsilon_i Ai)(\\epsilon_i A_i)^T) \\leq \\sigma^2 \\frac{1}{n} \\sum_{i=1}^{n} AiA^T_i \\leq 2\\sigma^2dn. \\tag{257}\n\\end{align*}\n$$\nBy applying the matrix Bernstein inequality [52, 53], for any $$t > 0$$,\n\n$$\n\\begin{align*}\n&P\\left(\\frac{1}{n} \\sum_{i=1}^{n} Ai\\epsilon_i \\geq t\\right) \\leq d \\cdot \\exp\\left(-\\frac{3t^2n^2}{12dn\\sigma^2 + 2\\sigma^2n^t}\\right) = d \\cdot \\exp\\left(-3t^2n\\right).\n\\end{align*}\n$$\nChoosing $$t = 3\\sigma d \\log\\left(\\frac{d}{\\eta}\\right)$$, the RHS is upper bounded by $$\\eta$$. This completes the proof.\n\nBy virtue of this result, we expect that for all U, with probability $$\\geq 1 - \\eta$$, the loss L in eq. (250) is approximately $$\\|UU^T - U^*U^{T*}\\|_F^2$$ (up to additive constants) when n is large. Indeed, by an application of Holder\u2019s inequality,\n\n$$\n\\begin{align*}\n&\\text{Tr}(AB) \\leq \\|A\\|_{\\text{op}}\\|B\\|_* \\leq \\|A\\|_{\\text{op}}\\text{rank}(B)\\|B\\|_F \\quad \\text{and,} \\\\\n&\\epsilon^2 kd \\log\\left(\\frac{d}{\\eta}\\right) \\|UU^T - U^*U^{T*}\\|_F \\lesssim \\sigma \\|F\\|_F. \\tag{258}\n\\end{align*}\n$$\nLikewise,\n\n$$\n\\begin{align*}\n&\\langle\\nabla L_{\\text{emp}}(U), Z\\rangle - \\langle\\nabla\\|UU^T - U^*U^{T*}\\|_H, Z\\rangle \\lesssim \\sigma kd \\log\\left(\\frac{d}{\\eta}\\right) \\|UZ^T + ZU^T\\|_F. \\tag{259}\n\\end{align*}\n$$\nAnd finally,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) - \\text{vec}(Z)^T [\\nabla^2\\|UU^T - U^*U^{T*}\\|_H]\\text{vec}(Z) \\lesssim \\sigma kd \\log\\left(\\frac{d}{\\eta}\\right) \\|ZZ^T\\|_F. \\tag{260}\n\\end{align*}\n$$\nwhere the hidden constants in each of these inequalities are at most 16.\n\nIn the sequel, we condition on the event that eqs. (258) to (260) hold, which occurs with probability $$\\geq 1 - \\eta$$.\n\nLemmas 22 and 24: The conclusion of these lemmas can still be applied since the finite sample loss function eq. (249) is still of the form f(UU^T) for a doubly differentiable f.\n\nLemma 26: This lemma is slightly modified in the finite sample setting. The new result is provided below.\n\nLemma 36 (Modified Lemma 26). At an (\u03f5, \u03b3)-approximate second order stationary point of $$L + \\lambda R\\beta$$, $$\\|UU^T - U^*U^{T*}\\|_F \\leq \\max(\\beta + \\sigma, n) \\epsilon^{2/3} k^{1/6}, \\lambda k, kd \\log(d/\\eta), \\sqrt{k}\\gamma$$. (261)\n\nProof. From [36, Lemma 7] and eq. (260) as in Lemma 26, for the choice $$Z = U - U^*R^*$$, with $$R^* \\in \\arg \\min R: R^T R = R^T R = I \\|U - U^*R\\|_F^2$$, we have the following bound,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_H - 6\\|UU^T - U^*U^{T*}\\|_H + 4\\langle\\nabla L_{\\text{emp}}(U), Z\\rangle \\\\\n&+ \\lambda \\text{vec}(Z)^T [\\nabla^2 R\\beta(U)]\\text{vec}(Z) - 4\\lambda\\langle\\nabla R\\beta(U), Z\\rangle + 16\\sigma kd \\log(kd/\\eta) \\|ZZ^T\\|_F. \\tag{262}\n\\end{align*}\n$$\nPlugging eq. (171) into eq. (262) and using Lemma 27 and the fact that $$\\|\\nabla L_{\\text{emp}}(U)\\|_F \\leq \\epsilon$$,\n\n$$\n\\begin{align*}\n&\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq 2\\|ZZ^T\\|_H - 6\\|UU^T - U^*U^{T*}\\|_H + 4\\epsilon\\|Z\\|_F \\\\\n&+ 8\\lambda \\beta + 32\\sigma n \\|UU^T - U^*U^{T*}\\|_F. \\tag{263}\n\\end{align*}\n$$\nwhere the hidden constants in each of these inequalities are at most 16."}]}, {"page": 41, "text": "By the RIP condition on the measurements, \u2225ZZT \u22252                           H \u2264     (1 + \u03b4)\u2225ZZT \u22252        F and likewise, \u2225UU T \u2212\nU\u22c6U T \u22c6  \u22252H \u2265    (1 \u2212    \u03b4)\u2225UU T \u2212        U\u22c6U T  \u22c6 \u22252F . Therefore, assuming \u03b4 \u2264               1/10 and simplifying as done in\neqs. (176) and (177), we get,\n      vec(Z)T [\u22072Lemp(U)]vec(Z) \u2264                     \u2212\u2225UU T \u2212         U\u22c6U T  \u22c6  \u22252\n                                                                                  F + 8\u03f5k1/4\u2225UU T \u2212              U\u22c6U T  \u22c6 \u22251/2\n                                                                                                                            F\n                                                      +     8\u03bb      2k                 kd log(d/\u03b7)          \u2225UU T \u2212       U\u22c6U T  \u22c6  \u2225F .\n                                                                     \u03b2 + 32\u03c3                   n\nThe rest of the analysis resembles the calculations from eq. (177) to eq. (179).\nLemmas 28 to 30:              These lemmas are slightly changed in the finite sample setting. The new results\nessentially replace \u03f5 by a slightly larger value \u03bd defined below,\n                                    \u03bd = \u03f5 + \u03b4\u2225UU T \u2212            U\u22c6U T  \u22c6 \u2225F + \u03c3      kd log(d/\u03b7)n         .                             (264)\nLemma 37. Consider an \u03f5-approximate first order stationary point U satisfying \u2225U\u2225op \u2264                                             3. Let Vr\ndenote the top-r eigenspace of U\u22c6U T               \u22c6  . Then, we have that,\n                                                      \u2225V \u22a5r (V \u22a5 r )T U\u22253   F \u2272     \u03bdk.                                                 (265)\nLet S = {i \u2208        [k] : \u2225Uei\u22252 \u2265         2\u221a\u03b2} be the set of large norm columns of U. For any column i \u2208                                 S,\n                                                  \u2225V \u22a5r (V \u22a5 r )T Uei\u22252       \u2264      2\u03bd                                                 (266)\nIn contrast, for the remaining columns,                   \u2225Uei\u22252                   \u03bb\u03b21/4 .\n                                                      \u2225V \u22a5r (V \u22a5 r )T Uei\u22252     2 \u2264   2\u03bd2\u221a\u03b2       .                                     (267)\n                                            i\u2208[k]\\S                                       \u03bb2\nLastly, for any column i \u2208            [k] such that \u2225VrV T \u2225Uei\u22252r\u2264Uei\u22252 \u2264 2    \u03b2.\u2225V \u22a5r (V \u22a5 r )T Uei\u22252, assuming \u03bd \u2264               \u03bb/2,(268)\nProof. From eq. (259) and the gradient computations in Lemma 41 and Lemma 42, with Z =\nV \u22a5\n  r (V \u22a5r )T U, approximate first-order stationarity of U implies that,\n\u27e8UU T \u2212      U\u22c6U T                                                                   kd log(d/\u03b7)\u2225UZT + ZU T \u2225F \u2264                         \u03f5\u2225Z\u2225F\n                    \u22c6  , UZT + ZU T \u27e9H + \u03bbTr(D(U)ZT U) \u2212                          \u03c3             n                                       (269)\nIn the infinite sample analyses (Lemmas 28 and 29) the first term on the LHS is non-negative\nfor this choice of Z. In the finite sample case, we instead lower bound as follows. Noting that\nZ = V \u22a5    r (V \u22a5 r )T U is rank k \u2212           r, this means that UZT + ZU T is a symmetric matrix of rank\n\u2264  2(k \u2212     r) \u2264    2k by the subadditivity of the rank of matrices. Likewise, UU T \u2212                                 U\u22c6U T  \u22c6  is of rank\n\u2264  k + r \u2264      2k. Therefore, by Assumption 1,\n                \u27e8UU T \u2212       U\u22c6U T \u22c6  , UZT + ZU T \u27e9H                                                                                  (270)\n                 \u2265   \u27e8UU T \u2212       U\u22c6U T \u22c6  , UZT + ZU T \u27e9          \u2212  \u03b4\u2225UU T \u2212        U\u22c6U T  \u22c6  \u2225F \u2225UZT + ZU T \u2225F                      (271)\n                 \u2265   \u27e8UU T \u2212       U\u22c6U T \u22c6  , UZT + ZU T \u27e9          \u2212  6\u03b4\u2225Z\u2225F \u2225UU T \u2212             U\u22c6U T \u22c6  \u2225F                           (272)\nwhere the last inequality uses the assumption that \u2225U\u2225op \u2264                            3 which results in the bound,\n                                      \u2225ZU T + UZT \u2225F \u2264               2\u2225U\u2225op\u2225Z\u2225F \u2264            6\u2225Z\u2225F .                                    (273)\nTherefore, in the finite sample case, instead of eq. (181) (and likewise, eq. (188)) we have,\n                             \u27e8UU T \u2212       U\u22c6U T  \u22c6  , UZT + ZU T \u27e9         + \u03bbTr(D(U)ZT U)                                             (274)\n                              \u2272      \u03f5 + \u03b4\u2225UU T \u2212          U\u22c6U T  \u22c6 \u2225F + \u03c3      kd log(d/\u03b7)n             \u2225Z\u2225F                           (275)\n                              = \u03bd\u2225Z\u2225F .                                                                                                 (276)\nThe proof of Lemma 37 follows directly by replacing \u03f5 by \u03bd everywhere in the remainder of the\nproofs of Lemmas 28 to 30.\n                                                                      41", "md": "By the RIP condition on the measurements, $$\\|ZZ^T\\|_2^H \\leq (1 + \\delta)\\|ZZ^T\\|_2^F$$ and likewise, $$\\|UU^T - U^*U^T\\|_2^H \\geq (1 - \\delta)\\|UU^T - U^*U^T\\|_2^F$$. Therefore, assuming $$\\delta \\leq 1/10$$ and simplifying as done in eqs. (176) and (177), we get,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq -\\|UU^T - U^*U^T\\|_2^F + 8\\epsilon k^{1/4}\\|UU^T - U^*U^T\\|_1^{1/2}F + 8\\lambda^2 k d \\log(d/\\eta) \\|UU^T - U^*U^T\\|_F.\n$$\n\nThe rest of the analysis resembles the calculations from eq. (177) to eq. (179).\n\n**Lemmas 28 to 30:** These lemmas are slightly changed in the finite sample setting. The new results essentially replace $$\\epsilon$$ by a slightly larger value $$\\nu$$ defined below,\n\n$$\n\\nu = \\epsilon + \\delta\\|UU^T - U^*U^T\\|_F + \\sigma kd \\log(d/\\eta)n. \\quad (264)\n$$\n\n**Lemma 37:** Consider an $$\\epsilon$$-approximate first-order stationary point U satisfying $$\\|U\\|_{\\text{op}} \\leq 3$$. Let $$V_r$$ denote the top-r eigenspace of $$U^*U^T$$. Then, we have that,\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T U\\|_F^3 \\lesssim \\nu k. \\quad (265)\n$$\n\nLet $$S = \\{i \\in [k] : \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}\\}$$ be the set of large norm columns of U. For any column $$i \\in S$$,\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2 \\leq 2\\nu. \\quad (266)\n$$\n\nIn contrast, for the remaining columns,\n\n$$\n\\|Ue_i\\|_2 \\leq \\lambda\\beta^{1/4}.\n$$\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2 \\leq 2\\nu^2\\sqrt{\\beta}. \\quad (267)\n$$\n\nLastly, for any column $$i \\in [k]$$ such that $$\\|V_rV_r^T Ue_i\\|_2 \\leq \\|Ue_i\\|_2 \\leq 2\\beta.\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2$$, assuming $$\\nu \\leq \\lambda/2$$,\n\n**Proof:** From eq. (259) and the gradient computations in Lemma 41 and Lemma 42, with $$Z = V_\\perp^r (V_\\perp^r)^T U$$, approximate first-order stationarity of U implies that,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle_H \\leq \\epsilon\\|Z\\|_F + \\sigma kd \\log(d/\\eta)\\|UZ^T + ZU^T\\|_F. \\quad (269)\n$$\n\nIn the infinite sample analyses (Lemmas 28 and 29) the first term on the LHS is non-negative for this choice of Z. In the finite sample case, we instead lower bound as follows. Noting that $$Z = V_\\perp^r (V_\\perp^r)^T U$$ is rank $$k - r$$, this means that $$UZ^T + ZU^T$$ is a symmetric matrix of rank $$\\leq 2(k - r) \\leq 2k$$ by the subadditivity of the rank of matrices. Likewise, $$UU^T - U^*U^T$$ is of rank $$\\leq k + r \\leq 2k$$. Therefore, by Assumption 1,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle_H \\geq \\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle - \\delta\\|UU^T - U^*U^T\\|_F \\|UZ^T + ZU^T\\|_F. \\quad (271)\n$$\n\n$$\n\\geq \\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle - 6\\delta\\|Z\\|_F \\|UU^T - U^*U^T\\|_F. \\quad (272)\n$$\n\nwhere the last inequality uses the assumption that $$\\|U\\|_{\\text{op}} \\leq 3$$ which results in the bound,\n\n$$\n\\|ZU^T + UZ^T\\|_F \\leq 2\\|U\\|_{\\text{op}}\\|Z\\|_F \\leq 6\\|Z\\|_F. \\quad (273)\n$$\n\nTherefore, in the finite sample case, instead of eq. (181) (and likewise, eq. (188)) we have,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + \\lambda\\text{Tr}(D(U)Z^T U) \\lesssim \\epsilon + \\delta\\|UU^T - U^*U^T\\|_F + \\sigma kd \\log(d/\\eta)n \\|Z\\|_F = \\nu\\|Z\\|_F. \\quad (276)\n$$\n\nThe proof of Lemma 37 follows directly by replacing $$\\epsilon$$ by $$\\nu$$ everywhere in the remainder of the proofs of Lemmas 28 to 30.", "images": [], "items": [{"type": "text", "value": "By the RIP condition on the measurements, $$\\|ZZ^T\\|_2^H \\leq (1 + \\delta)\\|ZZ^T\\|_2^F$$ and likewise, $$\\|UU^T - U^*U^T\\|_2^H \\geq (1 - \\delta)\\|UU^T - U^*U^T\\|_2^F$$. Therefore, assuming $$\\delta \\leq 1/10$$ and simplifying as done in eqs. (176) and (177), we get,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq -\\|UU^T - U^*U^T\\|_2^F + 8\\epsilon k^{1/4}\\|UU^T - U^*U^T\\|_1^{1/2}F + 8\\lambda^2 k d \\log(d/\\eta) \\|UU^T - U^*U^T\\|_F.\n$$\n\nThe rest of the analysis resembles the calculations from eq. (177) to eq. (179).\n\n**Lemmas 28 to 30:** These lemmas are slightly changed in the finite sample setting. The new results essentially replace $$\\epsilon$$ by a slightly larger value $$\\nu$$ defined below,\n\n$$\n\\nu = \\epsilon + \\delta\\|UU^T - U^*U^T\\|_F + \\sigma kd \\log(d/\\eta)n. \\quad (264)\n$$\n\n**Lemma 37:** Consider an $$\\epsilon$$-approximate first-order stationary point U satisfying $$\\|U\\|_{\\text{op}} \\leq 3$$. Let $$V_r$$ denote the top-r eigenspace of $$U^*U^T$$. Then, we have that,\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T U\\|_F^3 \\lesssim \\nu k. \\quad (265)\n$$\n\nLet $$S = \\{i \\in [k] : \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}\\}$$ be the set of large norm columns of U. For any column $$i \\in S$$,\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2 \\leq 2\\nu. \\quad (266)\n$$\n\nIn contrast, for the remaining columns,\n\n$$\n\\|Ue_i\\|_2 \\leq \\lambda\\beta^{1/4}.\n$$\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2 \\leq 2\\nu^2\\sqrt{\\beta}. \\quad (267)\n$$\n\nLastly, for any column $$i \\in [k]$$ such that $$\\|V_rV_r^T Ue_i\\|_2 \\leq \\|Ue_i\\|_2 \\leq 2\\beta.\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2$$, assuming $$\\nu \\leq \\lambda/2$$,\n\n**Proof:** From eq. (259) and the gradient computations in Lemma 41 and Lemma 42, with $$Z = V_\\perp^r (V_\\perp^r)^T U$$, approximate first-order stationarity of U implies that,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle_H \\leq \\epsilon\\|Z\\|_F + \\sigma kd \\log(d/\\eta)\\|UZ^T + ZU^T\\|_F. \\quad (269)\n$$\n\nIn the infinite sample analyses (Lemmas 28 and 29) the first term on the LHS is non-negative for this choice of Z. In the finite sample case, we instead lower bound as follows. Noting that $$Z = V_\\perp^r (V_\\perp^r)^T U$$ is rank $$k - r$$, this means that $$UZ^T + ZU^T$$ is a symmetric matrix of rank $$\\leq 2(k - r) \\leq 2k$$ by the subadditivity of the rank of matrices. Likewise, $$UU^T - U^*U^T$$ is of rank $$\\leq k + r \\leq 2k$$. Therefore, by Assumption 1,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle_H \\geq \\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle - \\delta\\|UU^T - U^*U^T\\|_F \\|UZ^T + ZU^T\\|_F. \\quad (271)\n$$\n\n$$\n\\geq \\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle - 6\\delta\\|Z\\|_F \\|UU^T - U^*U^T\\|_F. \\quad (272)\n$$\n\nwhere the last inequality uses the assumption that $$\\|U\\|_{\\text{op}} \\leq 3$$ which results in the bound,\n\n$$\n\\|ZU^T + UZ^T\\|_F \\leq 2\\|U\\|_{\\text{op}}\\|Z\\|_F \\leq 6\\|Z\\|_F. \\quad (273)\n$$\n\nTherefore, in the finite sample case, instead of eq. (181) (and likewise, eq. (188)) we have,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + \\lambda\\text{Tr}(D(U)Z^T U) \\lesssim \\epsilon + \\delta\\|UU^T - U^*U^T\\|_F + \\sigma kd \\log(d/\\eta)n \\|Z\\|_F = \\nu\\|Z\\|_F. \\quad (276)\n$$\n\nThe proof of Lemma 37 follows directly by replacing $$\\epsilon$$ by $$\\nu$$ everywhere in the remainder of the proofs of Lemmas 28 to 30.", "md": "By the RIP condition on the measurements, $$\\|ZZ^T\\|_2^H \\leq (1 + \\delta)\\|ZZ^T\\|_2^F$$ and likewise, $$\\|UU^T - U^*U^T\\|_2^H \\geq (1 - \\delta)\\|UU^T - U^*U^T\\|_2^F$$. Therefore, assuming $$\\delta \\leq 1/10$$ and simplifying as done in eqs. (176) and (177), we get,\n\n$$\n\\text{vec}(Z)^T [\\nabla^2 L_{\\text{emp}}(U)]\\text{vec}(Z) \\leq -\\|UU^T - U^*U^T\\|_2^F + 8\\epsilon k^{1/4}\\|UU^T - U^*U^T\\|_1^{1/2}F + 8\\lambda^2 k d \\log(d/\\eta) \\|UU^T - U^*U^T\\|_F.\n$$\n\nThe rest of the analysis resembles the calculations from eq. (177) to eq. (179).\n\n**Lemmas 28 to 30:** These lemmas are slightly changed in the finite sample setting. The new results essentially replace $$\\epsilon$$ by a slightly larger value $$\\nu$$ defined below,\n\n$$\n\\nu = \\epsilon + \\delta\\|UU^T - U^*U^T\\|_F + \\sigma kd \\log(d/\\eta)n. \\quad (264)\n$$\n\n**Lemma 37:** Consider an $$\\epsilon$$-approximate first-order stationary point U satisfying $$\\|U\\|_{\\text{op}} \\leq 3$$. Let $$V_r$$ denote the top-r eigenspace of $$U^*U^T$$. Then, we have that,\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T U\\|_F^3 \\lesssim \\nu k. \\quad (265)\n$$\n\nLet $$S = \\{i \\in [k] : \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta}\\}$$ be the set of large norm columns of U. For any column $$i \\in S$$,\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2 \\leq 2\\nu. \\quad (266)\n$$\n\nIn contrast, for the remaining columns,\n\n$$\n\\|Ue_i\\|_2 \\leq \\lambda\\beta^{1/4}.\n$$\n\n$$\n\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2 \\leq 2\\nu^2\\sqrt{\\beta}. \\quad (267)\n$$\n\nLastly, for any column $$i \\in [k]$$ such that $$\\|V_rV_r^T Ue_i\\|_2 \\leq \\|Ue_i\\|_2 \\leq 2\\beta.\\|V_\\perp^r (V_\\perp^r)^T Ue_i\\|_2$$, assuming $$\\nu \\leq \\lambda/2$$,\n\n**Proof:** From eq. (259) and the gradient computations in Lemma 41 and Lemma 42, with $$Z = V_\\perp^r (V_\\perp^r)^T U$$, approximate first-order stationarity of U implies that,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle_H \\leq \\epsilon\\|Z\\|_F + \\sigma kd \\log(d/\\eta)\\|UZ^T + ZU^T\\|_F. \\quad (269)\n$$\n\nIn the infinite sample analyses (Lemmas 28 and 29) the first term on the LHS is non-negative for this choice of Z. In the finite sample case, we instead lower bound as follows. Noting that $$Z = V_\\perp^r (V_\\perp^r)^T U$$ is rank $$k - r$$, this means that $$UZ^T + ZU^T$$ is a symmetric matrix of rank $$\\leq 2(k - r) \\leq 2k$$ by the subadditivity of the rank of matrices. Likewise, $$UU^T - U^*U^T$$ is of rank $$\\leq k + r \\leq 2k$$. Therefore, by Assumption 1,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle_H \\geq \\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle - \\delta\\|UU^T - U^*U^T\\|_F \\|UZ^T + ZU^T\\|_F. \\quad (271)\n$$\n\n$$\n\\geq \\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle - 6\\delta\\|Z\\|_F \\|UU^T - U^*U^T\\|_F. \\quad (272)\n$$\n\nwhere the last inequality uses the assumption that $$\\|U\\|_{\\text{op}} \\leq 3$$ which results in the bound,\n\n$$\n\\|ZU^T + UZ^T\\|_F \\leq 2\\|U\\|_{\\text{op}}\\|Z\\|_F \\leq 6\\|Z\\|_F. \\quad (273)\n$$\n\nTherefore, in the finite sample case, instead of eq. (181) (and likewise, eq. (188)) we have,\n\n$$\n\\langle UU^T - U^*U^T, UZ^T + ZU^T \\rangle + \\lambda\\text{Tr}(D(U)Z^T U) \\lesssim \\epsilon + \\delta\\|UU^T - U^*U^T\\|_F + \\sigma kd \\log(d/\\eta)n \\|Z\\|_F = \\nu\\|Z\\|_F. \\quad (276)\n$$\n\nThe proof of Lemma 37 follows directly by replacing $$\\epsilon$$ by $$\\nu$$ everywhere in the remainder of the proofs of Lemmas 28 to 30."}]}, {"page": 42, "text": "Finally, we extend Theorem 31 to the finite sample setting and in combination with the choice of\nparameters present the main result in the finite sample setting, a restatement of Theorem 4.\nTheorem 38 (Main result in the finite sample setting). Suppose the parameters \u03f5, \u03b3, \u03bb and \u03b2 are cho-\nsen as in the population setting (Theorem 3) and consider the solution Uprune returned by Algorithm 1.\nSuppose,                         \u03c32  kd log(d/\u03b7)                   \u03c32k2r5d log(d/\u03b7)                                         (\u03c3\u22c6 r)3/2\n                     n \u2265     cn          (\u03f5\u22c6)2            = \u0398                   (\u03c3\u22c6r)4                 ;         \u03b4 \u2264    c\u03b4  \u221a  kr5/2 .                (277)\nfor appropriate absolute constants cn, c\u03b4 > 0. Then, with probability \u2265                                             1 \u2212   \u03b7,\n         1. Uprune has exactly r non-zero columns.\n         2. Furthermore,\n                                                         \u2225UpruneU T    prune \u2212     U\u22c6U T  \u22c6  \u2225F \u2264      1      r)2.                                    (278)\n                                                                                                       2(\u03c3\u22c6\nIn other words, Algorithm 1 results in a solution Uprune having exactly r non-zero columns, and also\nserving as a spectral initialization.\nProof. Up until eq. (202), the proof is unchanged. Plugging in the new upper bound on the cosine of\nthe angle between V \u22a5           r (V \u22a5  r )T Uei and Uei in eq. (266) in Lemma 37, we get,\n                                                      \u03b8proj   \u2212   \u03c0    \u2272        \u03f5 + \u03b3     +        \u03bd                                                  (279)\n                                                        ij        2                \u03bb           \u03bb\u03b21/4 .\nAssume for a sufficiently small constant c2 > 0, \u03f5 + \u03b3 \u2264                                    c2\u03bb/r2. Then the first term on the RHS is\nupper bounded by c1           2r for a sufficiently small c1 \u2264                   1/3. Recall the definition,\nAnd furthermore, by Lemma 36,          \u03bd = \u03f5 + 2\u03b4\u2225UU T \u2212                U\u22c6U T   \u22c6  \u2225F + \u03c3       kd log(d/\u03b7).n                                         (280)\n                     \u2225UU T \u2212        U\u22c6U T                           \u03f52/3   k1/6   , \u03bb      k             kd log(d/\u03b7)         , \u221a  k\u03b3                  (281)\n                                            \u22c6  \u2225F \u2272      max                               \u03b2 + \u03c3                  n\nObserve that if everywhere \u03bd was replaced by \u03f5 (or even a constant factor approximation to it), the          (\u03c3\u22c6r )7/2\nproof of Theorem 31 essentially carries over unchanged. Let \u03f5\u22c6                                       = c\u03f5    \u221a  kr5/2 be the choice of \u03f5 in the\npopulation setting. This is the \u201ctarget\u201d value of \u03f5 in the empirical setting and we show that as long as\n\u03b4 is sufficiently small (O(1/                \u221a  k)) and n is sufficiently large (                 \u2126(dk2)) \u03bd is at most 3\u03f5\u22c6.\nIn particular, with the same choice of parameters (\u03f5, \u03b3, \u03b2 and \u03bb) as in the population setting, suppose\nthat,                            \u03c32  kd log(d/\u03b7)                   \u03c32k2r5d log(d/\u03b7)                                         (\u03c3\u22c6 r)3/2\n                     n \u2265     cn          (\u03f5\u22c6)2            = \u0398                   (\u03c3\u22c6r)4                 ;         \u03b4 \u2264    c\u03b4  \u221a  kr5/2 .                (282)\nNote that by choice of the parameters \u03b2, \u03bb, \u03b3, \u03f5, observe that,\n                            2\u03b4\u2225UU T \u2212          Y\u22c6U T  \u22c6  \u2225F \u2272      2\u03b4 max          \u03f52/3k1/6, \u03bb           \u03b2k,  \u221a   k\u03b3      \u2264    \u03f5\u22c6.                    (283)\nTherefore, combining eqs. (282) and (283) with the definition of \u03bd in eq. (280), we have that\n\u03bd \u2208    [\u03f5\u22c6, 3\u03f5\u22c6]. With this choice, since \u03f5 = \u03f5\u22c6                        and \u03bd are within constant multiples of each other, the\nrest of the proof of Theorem 31 in the population setting carries over. Subsequently plugging in the\nchoice of \u03bb ,\u03b2, \u03f5 and \u03b3 results in the following two statements:\n         1. Uprune has at most r non-zero columns.\n         2. \u2225UpruneUprune \u2212             U\u22c6U T  \u22c6  \u2225F \u2264      (\u03c3\u22c6 r )2 .\n                                                                2\nHowever under these two results, Uprune must have exactly r non-zero columns; if it had strictly fewer\nthan r non-zero columns, then it is impossible to satisfy \u2225UpruneUprune \u2212                                              U\u22c6U T  \u22c6  \u2225F < (\u03c3\u22c6       r)2. This\ncompletes the proof of the theorem.\n                                                                             42", "md": "Finally, we extend Theorem 31 to the finite sample setting and in combination with the choice of parameters present the main result in the finite sample setting, a restatement of Theorem 4.\n\nTheorem 38 (Main result in the finite sample setting). Suppose the parameters $$\\epsilon, \\gamma, \\lambda$$ and $$\\beta$$ are chosen as in the population setting (Theorem 3) and consider the solution Uprune returned by Algorithm 1. Suppose,\n\n$$\nn \\geq c_n \\sigma^2 kd \\log(d/\\eta) = \\Theta(\\sigma^2k^2r5d \\log(d/\\eta)); \\quad \\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}. \\tag{277}\n$$\n\nfor appropriate absolute constants $$c_n, c_{\\delta} > 0$$. Then, with probability $$\\geq 1 - \\eta$$,\n\n1. Uprune has exactly r non-zero columns.\n2. Furthermore,\n\n$$\n\\|U_{\\text{prune}}U_{\\text{prune}}^T - U^*U^{*T}\\|_F \\leq \\frac{1}{2(\\sigma^*r)^2}. \\tag{278}\n$$\n\nIn other words, Algorithm 1 results in a solution Uprune having exactly r non-zero columns, and also serving as a spectral initialization.\n\nProof. Up until eq. (202), the proof is unchanged. Plugging in the new upper bound on the cosine of the angle between $$V^{\\perp}_r (V^{\\perp}_r)^T U_{ei}$$ and $$U_{ei}$$ in eq. (266) in Lemma 37, we get,\n\n$$\n\\theta_{\\text{proj}} - \\pi \\lesssim \\epsilon + \\gamma + \\nu \\leq \\lambda + \\lambda\\beta^{1/4}. \\tag{279}\n$$\n\nAssume for a sufficiently small constant $$c_2 > 0$$, $$\\epsilon + \\gamma \\leq c_2\\lambda/r^2$$. Then the first term on the RHS is upper bounded by $$c_1 2r$$ for a sufficiently small $$c_1 \\leq 1/3$$. Recall the definition,\n\n$$\n\\nu = \\epsilon + 2\\delta\\|UU^T - U^*U^{*T}\\|_F + \\sigma kd \\log(d/\\eta)n. \\tag{280}\n$$\n\n$$\n\\|UU^T - U^*U^{*T}\\|_F \\lesssim \\max(\\epsilon^{2/3}k^{1/6}, \\lambda k, \\sigma kd \\log(d/\\eta), \\sqrt{k}\\gamma). \\tag{281}\n$$\n\nObserve that if everywhere $$\\nu$$ was replaced by $$\\epsilon$$ (or even a constant factor approximation to it), the proof of Theorem 31 essentially carries over unchanged. Let $$\\epsilon^* = c_{\\epsilon} \\sqrt{kr^{5/2}}$$ be the choice of $$\\epsilon$$ in the population setting. This is the \u201ctarget\u201d value of $$\\epsilon$$ in the empirical setting and we show that as long as $$\\delta$$ is sufficiently small ($$O(1/\\sqrt{k})$$) and $$n$$ is sufficiently large ($$\\Omega(dk^2)$$) $$\\nu$$ is at most $$3\\epsilon^*$$. In particular, with the same choice of parameters ($$\\epsilon, \\gamma, \\beta$$ and $$\\lambda$$) as in the population setting, suppose that,\n\n$$\nn \\geq c_n \\sigma^2 kd \\log(d/\\eta) = \\Theta(\\sigma^2k^2r5d \\log(d/\\eta)); \\quad \\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}. \\tag{282}\n$$\n\nNote that by choice of the parameters $$\\beta, \\lambda, \\gamma, \\epsilon$$, observe that,\n\n$$\n2\\delta\\|UU^T - Y^*U^{*T}\\|_F \\lesssim 2\\delta \\max(\\epsilon^{2/3}k^{1/6}, \\lambda \\beta k, \\sqrt{k}\\gamma) \\leq \\epsilon^*. \\tag{283}\n$$\n\nTherefore, combining eqs. (282) and (283) with the definition of $$\\nu$$ in eq. (280), we have that $$\\nu \\in [\\epsilon^*, 3\\epsilon^*]$$. With this choice, since $$\\epsilon = \\epsilon^*$$ and $$\\nu$$ are within constant multiples of each other, the rest of the proof of Theorem 31 in the population setting carries over. Subsequently plugging in the choice of $$\\lambda, \\beta, \\epsilon$$ and $$\\gamma$$ results in the following two statements:\n\n1. Uprune has at most r non-zero columns.\n2. $\\|U_{\\text{prune}}U_{\\text{prune}} - U^*U^{*T}\\|_F \\leq (\\sigma^* r)^2$.\n\nHowever under these two results, Uprune must have exactly r non-zero columns; if it had strictly fewer than r non-zero columns, then it is impossible to satisfy $$\\|U_{\\text{prune}}U_{\\text{prune}} - U^*U^{*T}\\|_F < (\\sigma^* r)^2$$. This completes the proof of the theorem.", "images": [], "items": [{"type": "text", "value": "Finally, we extend Theorem 31 to the finite sample setting and in combination with the choice of parameters present the main result in the finite sample setting, a restatement of Theorem 4.\n\nTheorem 38 (Main result in the finite sample setting). Suppose the parameters $$\\epsilon, \\gamma, \\lambda$$ and $$\\beta$$ are chosen as in the population setting (Theorem 3) and consider the solution Uprune returned by Algorithm 1. Suppose,\n\n$$\nn \\geq c_n \\sigma^2 kd \\log(d/\\eta) = \\Theta(\\sigma^2k^2r5d \\log(d/\\eta)); \\quad \\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}. \\tag{277}\n$$\n\nfor appropriate absolute constants $$c_n, c_{\\delta} > 0$$. Then, with probability $$\\geq 1 - \\eta$$,\n\n1. Uprune has exactly r non-zero columns.\n2. Furthermore,\n\n$$\n\\|U_{\\text{prune}}U_{\\text{prune}}^T - U^*U^{*T}\\|_F \\leq \\frac{1}{2(\\sigma^*r)^2}. \\tag{278}\n$$\n\nIn other words, Algorithm 1 results in a solution Uprune having exactly r non-zero columns, and also serving as a spectral initialization.\n\nProof. Up until eq. (202), the proof is unchanged. Plugging in the new upper bound on the cosine of the angle between $$V^{\\perp}_r (V^{\\perp}_r)^T U_{ei}$$ and $$U_{ei}$$ in eq. (266) in Lemma 37, we get,\n\n$$\n\\theta_{\\text{proj}} - \\pi \\lesssim \\epsilon + \\gamma + \\nu \\leq \\lambda + \\lambda\\beta^{1/4}. \\tag{279}\n$$\n\nAssume for a sufficiently small constant $$c_2 > 0$$, $$\\epsilon + \\gamma \\leq c_2\\lambda/r^2$$. Then the first term on the RHS is upper bounded by $$c_1 2r$$ for a sufficiently small $$c_1 \\leq 1/3$$. Recall the definition,\n\n$$\n\\nu = \\epsilon + 2\\delta\\|UU^T - U^*U^{*T}\\|_F + \\sigma kd \\log(d/\\eta)n. \\tag{280}\n$$\n\n$$\n\\|UU^T - U^*U^{*T}\\|_F \\lesssim \\max(\\epsilon^{2/3}k^{1/6}, \\lambda k, \\sigma kd \\log(d/\\eta), \\sqrt{k}\\gamma). \\tag{281}\n$$\n\nObserve that if everywhere $$\\nu$$ was replaced by $$\\epsilon$$ (or even a constant factor approximation to it), the proof of Theorem 31 essentially carries over unchanged. Let $$\\epsilon^* = c_{\\epsilon} \\sqrt{kr^{5/2}}$$ be the choice of $$\\epsilon$$ in the population setting. This is the \u201ctarget\u201d value of $$\\epsilon$$ in the empirical setting and we show that as long as $$\\delta$$ is sufficiently small ($$O(1/\\sqrt{k})$$) and $$n$$ is sufficiently large ($$\\Omega(dk^2)$$) $$\\nu$$ is at most $$3\\epsilon^*$$. In particular, with the same choice of parameters ($$\\epsilon, \\gamma, \\beta$$ and $$\\lambda$$) as in the population setting, suppose that,\n\n$$\nn \\geq c_n \\sigma^2 kd \\log(d/\\eta) = \\Theta(\\sigma^2k^2r5d \\log(d/\\eta)); \\quad \\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}. \\tag{282}\n$$\n\nNote that by choice of the parameters $$\\beta, \\lambda, \\gamma, \\epsilon$$, observe that,\n\n$$\n2\\delta\\|UU^T - Y^*U^{*T}\\|_F \\lesssim 2\\delta \\max(\\epsilon^{2/3}k^{1/6}, \\lambda \\beta k, \\sqrt{k}\\gamma) \\leq \\epsilon^*. \\tag{283}\n$$\n\nTherefore, combining eqs. (282) and (283) with the definition of $$\\nu$$ in eq. (280), we have that $$\\nu \\in [\\epsilon^*, 3\\epsilon^*]$$. With this choice, since $$\\epsilon = \\epsilon^*$$ and $$\\nu$$ are within constant multiples of each other, the rest of the proof of Theorem 31 in the population setting carries over. Subsequently plugging in the choice of $$\\lambda, \\beta, \\epsilon$$ and $$\\gamma$$ results in the following two statements:\n\n1. Uprune has at most r non-zero columns.\n2. $\\|U_{\\text{prune}}U_{\\text{prune}} - U^*U^{*T}\\|_F \\leq (\\sigma^* r)^2$.\n\nHowever under these two results, Uprune must have exactly r non-zero columns; if it had strictly fewer than r non-zero columns, then it is impossible to satisfy $$\\|U_{\\text{prune}}U_{\\text{prune}} - U^*U^{*T}\\|_F < (\\sigma^* r)^2$$. This completes the proof of the theorem.", "md": "Finally, we extend Theorem 31 to the finite sample setting and in combination with the choice of parameters present the main result in the finite sample setting, a restatement of Theorem 4.\n\nTheorem 38 (Main result in the finite sample setting). Suppose the parameters $$\\epsilon, \\gamma, \\lambda$$ and $$\\beta$$ are chosen as in the population setting (Theorem 3) and consider the solution Uprune returned by Algorithm 1. Suppose,\n\n$$\nn \\geq c_n \\sigma^2 kd \\log(d/\\eta) = \\Theta(\\sigma^2k^2r5d \\log(d/\\eta)); \\quad \\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}. \\tag{277}\n$$\n\nfor appropriate absolute constants $$c_n, c_{\\delta} > 0$$. Then, with probability $$\\geq 1 - \\eta$$,\n\n1. Uprune has exactly r non-zero columns.\n2. Furthermore,\n\n$$\n\\|U_{\\text{prune}}U_{\\text{prune}}^T - U^*U^{*T}\\|_F \\leq \\frac{1}{2(\\sigma^*r)^2}. \\tag{278}\n$$\n\nIn other words, Algorithm 1 results in a solution Uprune having exactly r non-zero columns, and also serving as a spectral initialization.\n\nProof. Up until eq. (202), the proof is unchanged. Plugging in the new upper bound on the cosine of the angle between $$V^{\\perp}_r (V^{\\perp}_r)^T U_{ei}$$ and $$U_{ei}$$ in eq. (266) in Lemma 37, we get,\n\n$$\n\\theta_{\\text{proj}} - \\pi \\lesssim \\epsilon + \\gamma + \\nu \\leq \\lambda + \\lambda\\beta^{1/4}. \\tag{279}\n$$\n\nAssume for a sufficiently small constant $$c_2 > 0$$, $$\\epsilon + \\gamma \\leq c_2\\lambda/r^2$$. Then the first term on the RHS is upper bounded by $$c_1 2r$$ for a sufficiently small $$c_1 \\leq 1/3$$. Recall the definition,\n\n$$\n\\nu = \\epsilon + 2\\delta\\|UU^T - U^*U^{*T}\\|_F + \\sigma kd \\log(d/\\eta)n. \\tag{280}\n$$\n\n$$\n\\|UU^T - U^*U^{*T}\\|_F \\lesssim \\max(\\epsilon^{2/3}k^{1/6}, \\lambda k, \\sigma kd \\log(d/\\eta), \\sqrt{k}\\gamma). \\tag{281}\n$$\n\nObserve that if everywhere $$\\nu$$ was replaced by $$\\epsilon$$ (or even a constant factor approximation to it), the proof of Theorem 31 essentially carries over unchanged. Let $$\\epsilon^* = c_{\\epsilon} \\sqrt{kr^{5/2}}$$ be the choice of $$\\epsilon$$ in the population setting. This is the \u201ctarget\u201d value of $$\\epsilon$$ in the empirical setting and we show that as long as $$\\delta$$ is sufficiently small ($$O(1/\\sqrt{k})$$) and $$n$$ is sufficiently large ($$\\Omega(dk^2)$$) $$\\nu$$ is at most $$3\\epsilon^*$$. In particular, with the same choice of parameters ($$\\epsilon, \\gamma, \\beta$$ and $$\\lambda$$) as in the population setting, suppose that,\n\n$$\nn \\geq c_n \\sigma^2 kd \\log(d/\\eta) = \\Theta(\\sigma^2k^2r5d \\log(d/\\eta)); \\quad \\delta \\leq c_{\\delta} \\sqrt{kr^{5/2}}. \\tag{282}\n$$\n\nNote that by choice of the parameters $$\\beta, \\lambda, \\gamma, \\epsilon$$, observe that,\n\n$$\n2\\delta\\|UU^T - Y^*U^{*T}\\|_F \\lesssim 2\\delta \\max(\\epsilon^{2/3}k^{1/6}, \\lambda \\beta k, \\sqrt{k}\\gamma) \\leq \\epsilon^*. \\tag{283}\n$$\n\nTherefore, combining eqs. (282) and (283) with the definition of $$\\nu$$ in eq. (280), we have that $$\\nu \\in [\\epsilon^*, 3\\epsilon^*]$$. With this choice, since $$\\epsilon = \\epsilon^*$$ and $$\\nu$$ are within constant multiples of each other, the rest of the proof of Theorem 31 in the population setting carries over. Subsequently plugging in the choice of $$\\lambda, \\beta, \\epsilon$$ and $$\\gamma$$ results in the following two statements:\n\n1. Uprune has at most r non-zero columns.\n2. $\\|U_{\\text{prune}}U_{\\text{prune}} - U^*U^{*T}\\|_F \\leq (\\sigma^* r)^2$.\n\nHowever under these two results, Uprune must have exactly r non-zero columns; if it had strictly fewer than r non-zero columns, then it is impossible to satisfy $$\\|U_{\\text{prune}}U_{\\text{prune}} - U^*U^{*T}\\|_F < (\\sigma^* r)^2$$. This completes the proof of the theorem."}]}, {"page": 43, "text": "D      Efficiently finding approximate second order stationary points\nIn this section we discuss efficiently finding second order stationary points of the loss L + \u03bbR\u03b2. We\nestablish smoothness conditions on L and R\u03b2 and show that running gradient descent with sufficiently\nbounded perturbations satisfies the property that all iterates are bounded in that \u2225Ut\u22252 \u2264                             3, as long\nas the algorithm is initialized within this ball.\nPerturbed gradient descent:                 We consider a first order method having the following update rule:\nfor all t \u2265    0,                     Ut+1 \u2190      Ut \u2212    \u03b1(\u2207(L + \u03bbR\u03b2)(Ut) + Pt)                                             (284)\nwhere Pt is a perturbation term, which for example could be the stochastic noise present in SGD.\nOver the course of running the update rule eq. (18), we show that the iterates \u2225Ut\u22252 remains bounded\nunder mild conditions.\nD.1     Proof of Theorem 7\nIn this section we prove Theorem 7. Below we first show gradient Lipschitzness on the domain\n{U : \u2225U\u2225op \u2264        3}. Observe that,\n    \u2225\u2207Lpop(U) \u2212        \u2207Lpop(V )\u2225F                                                                                           (285)\n     = \u2225(UU T \u2212        U\u22c6U T \u22c6 )U \u2212    (V V T \u2212     U\u22c6U T \u22c6 )V \u2225F                                                            (286)\n     = \u2225UU T (U \u2212        V ) + (U(U \u2212        V )T + (U \u2212       V )V T )V \u2212      U\u22c6U T \u22c6 (U \u2212    V )\u2225F                        (287)\n     (i)\n     \u2264   \u2225U\u22252  op\u2225U \u2212    V \u2225F + \u2225U\u2225op\u2225U \u2212            V \u2225F \u2225V \u2225op + \u2225U \u2212         V \u2225F \u2225V \u22252   op + \u2225U\u22c6U T   \u22c6 \u2225op\u2225U \u2212      V \u2225F\n                                                                                                                             (288)\n     \u2272  \u2225U \u2212     V \u2225F ,                                                                                                      (289)\nwhere (i) repeatedly uses the bound \u2225AB\u2225F \u2264                    \u2225A\u2225op\u2225B\u2225F and the last inequality follows from the\nfact that \u2225U\u2225op \u2264        3 and \u2225U\u22c6\u2225op = 1. On the other hand, by the gradient computations in Lemma 42,\n                             \u2225\u22072R\u03b2(U)\u2225op \u2264               sup       vec(Z)[\u22072R\u03b2(U)]vec(Z)                                     (290)\n                                                     Z:\u2225Z\u2225F \u22641\n                                                  \u2264      sup      \u27e8D(U), ZT Z\u27e9                                               (291)\n                                                     Z:\u2225Z\u2225F \u22641               k\n                                                  =  Z: k     sup     2\u22641   i=1  D(U)ii\u2225Zei\u22252      2                         (292)\n                                                  = max    i=1 \u2225Zei\u22252      \u221a 2\u03b2 .                                            (293)\n                                                      i\u2208[k] D(U)ii \u2264\n                                                           x+2\u03b2           2\nwhere the last inequality uses the fact that             (x+\u03b2)3/2 \u2264      \u221a \u03b2 . This implies that,\n                                    \u2225\u2207R\u03b2(U) \u2212         \u2207R\u03b2(V )\u2225op \u2264         \u221a 2\u03b2 \u2225U \u2212     V \u2225F .                              (294)\nCombining eqs. (289) and (293), by triangle inequality,\n                                      \u2225\u2207fpop(U) \u2212        \u2207fpop(V )\u2225F \u2272         \u2225U \u2212    V \u2225F                                  (295)\nwhere the assumption \u03bb/\u221a\u03b2 \u2264                1 is invoked.\nNext we prove Hessian Lipschitzness of fpop. By [54, Theorem 3], the Hessian of Lpop satisfies the\ncondition,\n                                    \u2225\u22072Lpop(U) \u2212         \u22072Lpop(V )\u2225op \u2272         \u2225U \u2212    V \u2225F .                              (296)\nAlthough the analysis in [54] is provided for the exactly specified case (k = r), it carries over\nunchanged to the overparameterized case as well. On the other hand, for any Z, ZU \u2208                                  Rd\u00d7k such\n                                                                43", "md": "Efficiently finding approximate second order stationary points\n\nIn this section we discuss efficiently finding second order stationary points of the loss \\( L + \\lambda R\\beta \\). We\nestablish smoothness conditions on \\( L \\) and \\( R\\beta \\) and show that running gradient descent with sufficiently\nbounded perturbations satisfies the property that all iterates are bounded in that \\( \\|U_t\\|_2 \\leq 3 \\), as long\nas the algorithm is initialized within this ball.\n\nPerturbed gradient descent: We consider a first order method having the following update rule:\nfor all \\( t \\geq 0 \\), \\( U_{t+1} \\leftarrow U_t - \\alpha(\\nabla(L + \\lambda R\\beta)(U_t) + P_t) \\) (284)\nwhere \\( P_t \\) is a perturbation term, which for example could be the stochastic noise present in SGD.\nOver the course of running the update rule eq. (18), we show that the iterates \\( \\|U_t\\|_2 \\) remains bounded\nunder mild conditions.\n\nProof of Theorem 7\n\nIn this section we prove Theorem 7. Below we first show gradient Lipschitzness on the domain\n\\( \\{U : \\|U\\|_{op} \\leq 3\\} \\). Observe that,\n\n$$\n\\begin{align*}\n&\\| \\nabla L_{pop}(U) - \\nabla L_{pop}(V) \\|_F \\\\\n&= \\| (UU^T - U^*U^* )U - (VV^T - U^*U^* )V \\|_F \\\\\n&= \\| UU^T (U - V) + (U(U - V)^T + (U - V)V^T )V - U^*U^* (U - V) \\|_F \\\\\n&\\leq \\|U\\|_2^{op} \\|U - V\\|_F + \\|U\\|_{op} \\|U - V\\|_F \\|V\\|_{op} + \\|U - V\\|_F \\|V\\|_2^{op} + \\|U^*U^* \\|_{op} \\|U - V\\|_F \\\\\n&\\lesssim \\|U - V\\|_F\n\\end{align*}\n$$\nwhere (i) repeatedly uses the bound \\( \\|AB\\|_F \\leq \\|A\\|_{op} \\|B\\|_F \\) and the last inequality follows from the\nfact that \\( \\|U\\|_{op} \\leq 3 \\) and \\( \\|U^*\\|_{op} = 1 \\). On the other hand, by the gradient computations in Lemma 42,\n\n$$\n\\begin{align*}\n&\\| \\nabla^2 R\\beta(U) \\|_{op} \\leq \\sup_{\\|Z\\|_F \\leq 1} \\text{vec}(Z)[\\nabla^2 R\\beta(U)]\\text{vec}(Z) \\\\\n&\\leq \\sup_{\\|Z\\|_F \\leq 1} \\langle D(U), Z^T Z \\rangle \\\\\n&= \\sup_{\\|Z\\|_F \\leq 1} \\langle D(U), Z^T Z \\rangle \\\\\n&= \\max_{i=1}^k \\|Ze_i\\|_2 \\sqrt{2\\beta}\n\\end{align*}\n$$\nwhere the last inequality uses the fact that \\( (x+\\beta)^{3/2} \\leq \\sqrt{\\beta} \\). This implies that,\n\n$$\n\\| \\nabla R\\beta(U) - \\nabla R\\beta(V) \\|_{op} \\leq \\sqrt{2\\beta} \\|U - V\\|_F\n$$\nCombining eqs. (289) and (293), by triangle inequality,\n\n$$\n\\| \\nabla f_{pop}(U) - \\nabla f_{pop}(V) \\|_F \\lesssim \\|U - V\\|_F\n$$\nwhere the assumption \\( \\lambda/\\sqrt{\\beta} \\leq 1 \\) is invoked.\n\nNext we prove Hessian Lipschitzness of \\( f_{pop} \\). By [54, Theorem 3], the Hessian of \\( L_{pop} \\) satisfies the\ncondition,\n\n$$\n\\| \\nabla^2 L_{pop}(U) - \\nabla^2 L_{pop}(V) \\|_{op} \\lesssim \\|U - V\\|_F\n$$\nAlthough the analysis in [54] is provided for the exactly specified case (\\( k = r \\)), it carries over\nunchanged to the overparameterized case as well. On the other hand, for any \\( Z, ZU \\in \\mathbb{R}^{d \\times k} \\) such\n\n43", "images": [], "items": [{"type": "text", "value": "Efficiently finding approximate second order stationary points\n\nIn this section we discuss efficiently finding second order stationary points of the loss \\( L + \\lambda R\\beta \\). We\nestablish smoothness conditions on \\( L \\) and \\( R\\beta \\) and show that running gradient descent with sufficiently\nbounded perturbations satisfies the property that all iterates are bounded in that \\( \\|U_t\\|_2 \\leq 3 \\), as long\nas the algorithm is initialized within this ball.\n\nPerturbed gradient descent: We consider a first order method having the following update rule:\nfor all \\( t \\geq 0 \\), \\( U_{t+1} \\leftarrow U_t - \\alpha(\\nabla(L + \\lambda R\\beta)(U_t) + P_t) \\) (284)\nwhere \\( P_t \\) is a perturbation term, which for example could be the stochastic noise present in SGD.\nOver the course of running the update rule eq. (18), we show that the iterates \\( \\|U_t\\|_2 \\) remains bounded\nunder mild conditions.\n\nProof of Theorem 7\n\nIn this section we prove Theorem 7. Below we first show gradient Lipschitzness on the domain\n\\( \\{U : \\|U\\|_{op} \\leq 3\\} \\). Observe that,\n\n$$\n\\begin{align*}\n&\\| \\nabla L_{pop}(U) - \\nabla L_{pop}(V) \\|_F \\\\\n&= \\| (UU^T - U^*U^* )U - (VV^T - U^*U^* )V \\|_F \\\\\n&= \\| UU^T (U - V) + (U(U - V)^T + (U - V)V^T )V - U^*U^* (U - V) \\|_F \\\\\n&\\leq \\|U\\|_2^{op} \\|U - V\\|_F + \\|U\\|_{op} \\|U - V\\|_F \\|V\\|_{op} + \\|U - V\\|_F \\|V\\|_2^{op} + \\|U^*U^* \\|_{op} \\|U - V\\|_F \\\\\n&\\lesssim \\|U - V\\|_F\n\\end{align*}\n$$\nwhere (i) repeatedly uses the bound \\( \\|AB\\|_F \\leq \\|A\\|_{op} \\|B\\|_F \\) and the last inequality follows from the\nfact that \\( \\|U\\|_{op} \\leq 3 \\) and \\( \\|U^*\\|_{op} = 1 \\). On the other hand, by the gradient computations in Lemma 42,\n\n$$\n\\begin{align*}\n&\\| \\nabla^2 R\\beta(U) \\|_{op} \\leq \\sup_{\\|Z\\|_F \\leq 1} \\text{vec}(Z)[\\nabla^2 R\\beta(U)]\\text{vec}(Z) \\\\\n&\\leq \\sup_{\\|Z\\|_F \\leq 1} \\langle D(U), Z^T Z \\rangle \\\\\n&= \\sup_{\\|Z\\|_F \\leq 1} \\langle D(U), Z^T Z \\rangle \\\\\n&= \\max_{i=1}^k \\|Ze_i\\|_2 \\sqrt{2\\beta}\n\\end{align*}\n$$\nwhere the last inequality uses the fact that \\( (x+\\beta)^{3/2} \\leq \\sqrt{\\beta} \\). This implies that,\n\n$$\n\\| \\nabla R\\beta(U) - \\nabla R\\beta(V) \\|_{op} \\leq \\sqrt{2\\beta} \\|U - V\\|_F\n$$\nCombining eqs. (289) and (293), by triangle inequality,\n\n$$\n\\| \\nabla f_{pop}(U) - \\nabla f_{pop}(V) \\|_F \\lesssim \\|U - V\\|_F\n$$\nwhere the assumption \\( \\lambda/\\sqrt{\\beta} \\leq 1 \\) is invoked.\n\nNext we prove Hessian Lipschitzness of \\( f_{pop} \\). By [54, Theorem 3], the Hessian of \\( L_{pop} \\) satisfies the\ncondition,\n\n$$\n\\| \\nabla^2 L_{pop}(U) - \\nabla^2 L_{pop}(V) \\|_{op} \\lesssim \\|U - V\\|_F\n$$\nAlthough the analysis in [54] is provided for the exactly specified case (\\( k = r \\)), it carries over\nunchanged to the overparameterized case as well. On the other hand, for any \\( Z, ZU \\in \\mathbb{R}^{d \\times k} \\) such\n\n43", "md": "Efficiently finding approximate second order stationary points\n\nIn this section we discuss efficiently finding second order stationary points of the loss \\( L + \\lambda R\\beta \\). We\nestablish smoothness conditions on \\( L \\) and \\( R\\beta \\) and show that running gradient descent with sufficiently\nbounded perturbations satisfies the property that all iterates are bounded in that \\( \\|U_t\\|_2 \\leq 3 \\), as long\nas the algorithm is initialized within this ball.\n\nPerturbed gradient descent: We consider a first order method having the following update rule:\nfor all \\( t \\geq 0 \\), \\( U_{t+1} \\leftarrow U_t - \\alpha(\\nabla(L + \\lambda R\\beta)(U_t) + P_t) \\) (284)\nwhere \\( P_t \\) is a perturbation term, which for example could be the stochastic noise present in SGD.\nOver the course of running the update rule eq. (18), we show that the iterates \\( \\|U_t\\|_2 \\) remains bounded\nunder mild conditions.\n\nProof of Theorem 7\n\nIn this section we prove Theorem 7. Below we first show gradient Lipschitzness on the domain\n\\( \\{U : \\|U\\|_{op} \\leq 3\\} \\). Observe that,\n\n$$\n\\begin{align*}\n&\\| \\nabla L_{pop}(U) - \\nabla L_{pop}(V) \\|_F \\\\\n&= \\| (UU^T - U^*U^* )U - (VV^T - U^*U^* )V \\|_F \\\\\n&= \\| UU^T (U - V) + (U(U - V)^T + (U - V)V^T )V - U^*U^* (U - V) \\|_F \\\\\n&\\leq \\|U\\|_2^{op} \\|U - V\\|_F + \\|U\\|_{op} \\|U - V\\|_F \\|V\\|_{op} + \\|U - V\\|_F \\|V\\|_2^{op} + \\|U^*U^* \\|_{op} \\|U - V\\|_F \\\\\n&\\lesssim \\|U - V\\|_F\n\\end{align*}\n$$\nwhere (i) repeatedly uses the bound \\( \\|AB\\|_F \\leq \\|A\\|_{op} \\|B\\|_F \\) and the last inequality follows from the\nfact that \\( \\|U\\|_{op} \\leq 3 \\) and \\( \\|U^*\\|_{op} = 1 \\). On the other hand, by the gradient computations in Lemma 42,\n\n$$\n\\begin{align*}\n&\\| \\nabla^2 R\\beta(U) \\|_{op} \\leq \\sup_{\\|Z\\|_F \\leq 1} \\text{vec}(Z)[\\nabla^2 R\\beta(U)]\\text{vec}(Z) \\\\\n&\\leq \\sup_{\\|Z\\|_F \\leq 1} \\langle D(U), Z^T Z \\rangle \\\\\n&= \\sup_{\\|Z\\|_F \\leq 1} \\langle D(U), Z^T Z \\rangle \\\\\n&= \\max_{i=1}^k \\|Ze_i\\|_2 \\sqrt{2\\beta}\n\\end{align*}\n$$\nwhere the last inequality uses the fact that \\( (x+\\beta)^{3/2} \\leq \\sqrt{\\beta} \\). This implies that,\n\n$$\n\\| \\nabla R\\beta(U) - \\nabla R\\beta(V) \\|_{op} \\leq \\sqrt{2\\beta} \\|U - V\\|_F\n$$\nCombining eqs. (289) and (293), by triangle inequality,\n\n$$\n\\| \\nabla f_{pop}(U) - \\nabla f_{pop}(V) \\|_F \\lesssim \\|U - V\\|_F\n$$\nwhere the assumption \\( \\lambda/\\sqrt{\\beta} \\leq 1 \\) is invoked.\n\nNext we prove Hessian Lipschitzness of \\( f_{pop} \\). By [54, Theorem 3], the Hessian of \\( L_{pop} \\) satisfies the\ncondition,\n\n$$\n\\| \\nabla^2 L_{pop}(U) - \\nabla^2 L_{pop}(V) \\|_{op} \\lesssim \\|U - V\\|_F\n$$\nAlthough the analysis in [54] is provided for the exactly specified case (\\( k = r \\)), it carries over\nunchanged to the overparameterized case as well. On the other hand, for any \\( Z, ZU \\in \\mathbb{R}^{d \\times k} \\) such\n\n43"}]}, {"page": 44, "text": "that \u2225Z\u2225F , \u2225ZU\u2225F \u2264             1, the rate of change of the Hessian at U evaluated along the direction Z,\n           lim                  \u22072R\u03b2(U) \u2212            \u22072R\u03b2(U + tZU)                vec(Z)                                                (297)\n           t\u21920 vec(Z)T                                 t\n           = lim      \u27e8D(U) \u2212       D(U + tZU), ZT Z\u27e9                                                                                   (298)\n               t\u21920                         t\n                          +    k i=1 G(U + tZU)ii\u27e8Zei, (U + tZU)ei\u27e92 \u2212      t                  G(U)ii\u27e8Zei, Uei\u27e92            .           (299)\nThe first term of the RHS of eq. (299) can be bounded as,\n    lim   \u27e8D(U) \u2212        D(U + tZU), ZT Z\u27e9              = \u2212      k       \u2225Uei\u22252   2 + 4\u03b2                                     2          (300)\n    t\u21920                         t                               i=1   (\u2225Uei\u22252    2 + \u03b2)5/2 \u27e8Uei, ZUei\u27e9            \u00b7 \u2225Zei\u22252\n                                                        (i)             \u2225Uei\u22252    2 + 4\u03b2                             k   \u2225Zei\u22252         (301)\n                                                         \u2264   max                                                                  2\n                                                             i\u2208[k]    (\u2225Uei\u22252   2 + \u03b2)5/2 \u27e8Uei, ZUei\u27e9              i=1\n                                                        (ii)            \u2225Uei\u22252    2 + 4\u03b2\n                                                         \u2264    max                                                                       (302)\n                                                        \u2272    1i\u2208[k]  (\u2225Uei\u22252    2 + \u03b2)5/2 \u2225Uei\u22252                                        (303)\n                                                             \u03b2 ,\nwhere (i) follows by Holder inequality, and (ii) uses\u221athe fact that \u2225Z\u2225F , \u2225ZU\u2225F \u2264                                            1. The last\ninequality uses the fact that minx\u22650                   x(x+4\u03b2)          \u03b2 .\n                                                     (x+\u03b2)5/2 \u2272         1\nOn the other hand, by the chain rule, the second term on the RHS of eq. (299) can be computed in\ntwo parts, the first being,\n  lim   (G(U + tZU)ii \u2212            G(U)ii) \u27e8Zei, Uei\u27e92             = \u2212      k    3(\u2225Uei\u22252    2 + 6\u03b2)\n  t\u21920                               t                                   k i=1   (\u2225Uei\u22252    2 + \u03b2)7/2 \u27e8Uei, ZUei\u27e9\u27e8Zei, Uei\u27e92\n                                                                   \u2264   i=1  H(U)ii\u2225Uei\u22252\u27e8Zei, Uei\u27e92                                     (304)\n                                                                           3(\u2225Uei\u22252  2+6\u03b2)\u2225Uei\u22252\nwhich uses the fact that \u2225ZU\u2225F \u2264                      1 H(U)ii =               (\u2225Uei\u22252  2+\u03b2)7/2        . Note that for any c \u2265               0,\nthe optimization problem which takes the form max                           v:\u2225v\u22252 2\u2264c\u27e8v, Uei\u27e9       is maximized when v \u221d               Uei.\nTherefore, fixing the solution to the remaining coordinates and optimizing over Z for a single i, one\nmay substitute Zei = xi \u00b7 Uei/\u2225Uei\u22252 for some xi \u2208                             R and bound eq. (304) as,\n          lim   (G(U + tZU)ii \u2212            G(U)ii) \u27e8Zei, Uei\u27e92             \u2264    k  max          k   H(U)iix2     i \u2225Uei\u22253   2           (305)\n          t\u21920                               t                              = max  i=1 x2i \u22641  i=1                                       (306)\n                                                                               i\u2208[k] H(U)ii\u2225Uei\u22253          2\n                                                                           \u2272   1                                                        (307)\n                                                                               \u03b2 ,\n                                                                      44", "md": "# Math Equations\n\nthat $$\\|Z\\|_F, \\|ZU\\|_F \\leq 1$$, the rate of change of the Hessian at U evaluated along the direction Z,\n\n$$\n\\lim_{{t \\to 0}} \\frac{\\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(U + tZU)}{t} \\text{vec}(Z) \\quad (297)\n$$\n$$\n= \\lim_{{t \\to 0}} \\langle D(U) - D(U + tZU), Z^T Z \\rangle \\quad (298)\n$$\n$$\n+ \\sum_{i=1}^{k} G(U + tZU)_{ii} \\langle Ze_i, (U + tZU)e_i \\rangle^2 - tG(U)_{ii} \\langle Ze_i, Ue_i \\rangle^2. \\quad (299)\n$$\nThe first term of the RHS of eq. (299) can be bounded as,\n\n$$\n\\lim_{{t \\to 0}} \\langle D(U) - D(U + tZU), Z^T Z \\rangle = -\\sum_{i=1}^{k} \\frac{\\|Ue_i\\|_2^2}{2} + 4\\beta^2 \\quad (300)\n$$\n$$\n\\leq \\max_{i \\in [k]} \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{5/2} \\langle Ue_i, ZUe_i \\rangle \\cdot \\|Ze_i\\|_2 \\quad (301)\n$$\n$$\n\\leq \\max_{i \\in [k]} \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{5/2} \\frac{\\langle Ue_i, ZUe_i \\rangle}{\\|Ue_i\\|_2} \\quad (302)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}, \\quad \\text{where (i) follows by Holder inequality, and (ii) uses the fact that } \\|Z\\|_F, \\|ZU\\|_F \\leq 1. \\text{ The last inequality uses the fact that } \\min_{x \\geq 0} \\frac{x(x+4\\beta)}{(x+\\beta)^{5/2}} \\lesssim 1.$$\nOn the other hand, by the chain rule, the second term on the RHS of eq. (299) can be computed in two parts, the first being,\n\n$$\n\\lim_{{t \\to 0}} (G(U + tZU)_{ii} - G(U)_{ii}) \\langle Ze_i, Ue_i \\rangle^2 = -k \\frac{3(\\|Ue_i\\|_2^2 + 6\\beta)}{t} \\quad (304)\n$$\n$$\n\\leq \\sum_{i=1}^{k} H(U)_{ii} \\|Ue_i\\|_2 \\langle Ze_i, Ue_i \\rangle^2 \\quad (305)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}, \\quad \\text{which uses the fact that } \\|ZU\\|_F \\leq 1, H(U)_{ii} = \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{7/2}. \\text{ Note that for any } c \\geq 0, \\text{ the optimization problem which takes the form } \\max_{v: \\|v\\|_2^2 \\leq c} \\langle v, Ue_i \\rangle \\text{ is maximized when } v \\propto Ue_i.$$\nTherefore, fixing the solution to the remaining coordinates and optimizing over Z for a single i, one may substitute $$Ze_i = x_i \\cdot \\frac{Ue_i}{\\|Ue_i\\|_2}$$ for some $$x_i \\in \\mathbb{R}$$ and bound eq. (304) as,\n\n$$\n\\lim_{{t \\to 0}} (G(U + tZU)_{ii} - G(U)_{ii}) \\langle Ze_i, Ue_i \\rangle^2 \\leq k \\max_{i=1} x_i^2 \\leq 1 \\quad (306)\n$$\n$$\n\\leq \\max_{i \\in [k]} x_i^2 \\leq 1 \\quad (307)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}.$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "that $$\\|Z\\|_F, \\|ZU\\|_F \\leq 1$$, the rate of change of the Hessian at U evaluated along the direction Z,\n\n$$\n\\lim_{{t \\to 0}} \\frac{\\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(U + tZU)}{t} \\text{vec}(Z) \\quad (297)\n$$\n$$\n= \\lim_{{t \\to 0}} \\langle D(U) - D(U + tZU), Z^T Z \\rangle \\quad (298)\n$$\n$$\n+ \\sum_{i=1}^{k} G(U + tZU)_{ii} \\langle Ze_i, (U + tZU)e_i \\rangle^2 - tG(U)_{ii} \\langle Ze_i, Ue_i \\rangle^2. \\quad (299)\n$$\nThe first term of the RHS of eq. (299) can be bounded as,\n\n$$\n\\lim_{{t \\to 0}} \\langle D(U) - D(U + tZU), Z^T Z \\rangle = -\\sum_{i=1}^{k} \\frac{\\|Ue_i\\|_2^2}{2} + 4\\beta^2 \\quad (300)\n$$\n$$\n\\leq \\max_{i \\in [k]} \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{5/2} \\langle Ue_i, ZUe_i \\rangle \\cdot \\|Ze_i\\|_2 \\quad (301)\n$$\n$$\n\\leq \\max_{i \\in [k]} \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{5/2} \\frac{\\langle Ue_i, ZUe_i \\rangle}{\\|Ue_i\\|_2} \\quad (302)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}, \\quad \\text{where (i) follows by Holder inequality, and (ii) uses the fact that } \\|Z\\|_F, \\|ZU\\|_F \\leq 1. \\text{ The last inequality uses the fact that } \\min_{x \\geq 0} \\frac{x(x+4\\beta)}{(x+\\beta)^{5/2}} \\lesssim 1.$$\nOn the other hand, by the chain rule, the second term on the RHS of eq. (299) can be computed in two parts, the first being,\n\n$$\n\\lim_{{t \\to 0}} (G(U + tZU)_{ii} - G(U)_{ii}) \\langle Ze_i, Ue_i \\rangle^2 = -k \\frac{3(\\|Ue_i\\|_2^2 + 6\\beta)}{t} \\quad (304)\n$$\n$$\n\\leq \\sum_{i=1}^{k} H(U)_{ii} \\|Ue_i\\|_2 \\langle Ze_i, Ue_i \\rangle^2 \\quad (305)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}, \\quad \\text{which uses the fact that } \\|ZU\\|_F \\leq 1, H(U)_{ii} = \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{7/2}. \\text{ Note that for any } c \\geq 0, \\text{ the optimization problem which takes the form } \\max_{v: \\|v\\|_2^2 \\leq c} \\langle v, Ue_i \\rangle \\text{ is maximized when } v \\propto Ue_i.$$\nTherefore, fixing the solution to the remaining coordinates and optimizing over Z for a single i, one may substitute $$Ze_i = x_i \\cdot \\frac{Ue_i}{\\|Ue_i\\|_2}$$ for some $$x_i \\in \\mathbb{R}$$ and bound eq. (304) as,\n\n$$\n\\lim_{{t \\to 0}} (G(U + tZU)_{ii} - G(U)_{ii}) \\langle Ze_i, Ue_i \\rangle^2 \\leq k \\max_{i=1} x_i^2 \\leq 1 \\quad (306)\n$$\n$$\n\\leq \\max_{i \\in [k]} x_i^2 \\leq 1 \\quad (307)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}.$$", "md": "that $$\\|Z\\|_F, \\|ZU\\|_F \\leq 1$$, the rate of change of the Hessian at U evaluated along the direction Z,\n\n$$\n\\lim_{{t \\to 0}} \\frac{\\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(U + tZU)}{t} \\text{vec}(Z) \\quad (297)\n$$\n$$\n= \\lim_{{t \\to 0}} \\langle D(U) - D(U + tZU), Z^T Z \\rangle \\quad (298)\n$$\n$$\n+ \\sum_{i=1}^{k} G(U + tZU)_{ii} \\langle Ze_i, (U + tZU)e_i \\rangle^2 - tG(U)_{ii} \\langle Ze_i, Ue_i \\rangle^2. \\quad (299)\n$$\nThe first term of the RHS of eq. (299) can be bounded as,\n\n$$\n\\lim_{{t \\to 0}} \\langle D(U) - D(U + tZU), Z^T Z \\rangle = -\\sum_{i=1}^{k} \\frac{\\|Ue_i\\|_2^2}{2} + 4\\beta^2 \\quad (300)\n$$\n$$\n\\leq \\max_{i \\in [k]} \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{5/2} \\langle Ue_i, ZUe_i \\rangle \\cdot \\|Ze_i\\|_2 \\quad (301)\n$$\n$$\n\\leq \\max_{i \\in [k]} \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{5/2} \\frac{\\langle Ue_i, ZUe_i \\rangle}{\\|Ue_i\\|_2} \\quad (302)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}, \\quad \\text{where (i) follows by Holder inequality, and (ii) uses the fact that } \\|Z\\|_F, \\|ZU\\|_F \\leq 1. \\text{ The last inequality uses the fact that } \\min_{x \\geq 0} \\frac{x(x+4\\beta)}{(x+\\beta)^{5/2}} \\lesssim 1.$$\nOn the other hand, by the chain rule, the second term on the RHS of eq. (299) can be computed in two parts, the first being,\n\n$$\n\\lim_{{t \\to 0}} (G(U + tZU)_{ii} - G(U)_{ii}) \\langle Ze_i, Ue_i \\rangle^2 = -k \\frac{3(\\|Ue_i\\|_2^2 + 6\\beta)}{t} \\quad (304)\n$$\n$$\n\\leq \\sum_{i=1}^{k} H(U)_{ii} \\|Ue_i\\|_2 \\langle Ze_i, Ue_i \\rangle^2 \\quad (305)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}, \\quad \\text{which uses the fact that } \\|ZU\\|_F \\leq 1, H(U)_{ii} = \\left( \\frac{\\|Ue_i\\|_2^2 + \\beta}{2} \\right)^{7/2}. \\text{ Note that for any } c \\geq 0, \\text{ the optimization problem which takes the form } \\max_{v: \\|v\\|_2^2 \\leq c} \\langle v, Ue_i \\rangle \\text{ is maximized when } v \\propto Ue_i.$$\nTherefore, fixing the solution to the remaining coordinates and optimizing over Z for a single i, one may substitute $$Ze_i = x_i \\cdot \\frac{Ue_i}{\\|Ue_i\\|_2}$$ for some $$x_i \\in \\mathbb{R}$$ and bound eq. (304) as,\n\n$$\n\\lim_{{t \\to 0}} (G(U + tZU)_{ii} - G(U)_{ii}) \\langle Ze_i, Ue_i \\rangle^2 \\leq k \\max_{i=1} x_i^2 \\leq 1 \\quad (306)\n$$\n$$\n\\leq \\max_{i \\in [k]} x_i^2 \\leq 1 \\quad (307)\n$$\n$$\n\\lesssim \\frac{1}{\\beta}.$$"}]}, {"page": 45, "text": "where the last inequality uses the fact that minx\u22650       3\u221a x3(x+6\u03b2)   \u2272  1\n                                                            (x+\u03b2)7/2       \u03b2 . The second part of the second\nterm on the RHS of eq. (299) is,\n                         lim   k   G(U)ii    \u27e8Zei, (U + tZU)ei\u27e92 \u2212       \u27e8Zei, Uei\u27e92                      (308)\n                         t\u21920  i=1                            t\n                                k\n                          = 2 ki=1 G(U)ii\u27e8Zei, Uei\u27e9\u27e8Zei, ZUei\u27e9                                            (309)\n                          \u2272   G(U)ii\u2225Uei\u2225F \u27e8Zei, ZUei\u27e9                                                    (310)\n                             i=1\n                              k\n                          \u2272  i=1 G(U)ii\u2225Uei\u2225F \u2225Zei\u22252\u2225ZUei\u22252                                               (311)\n                          \u2264      k  (G(U)ii)2\u2225Uei\u22252    F \u2225Zei\u22252 2                                         (312)\n                                i=1\n                          \u2264  max                                                                          (313)\n                             i\u2208[k] G(U)ii\u2225Uei\u2225F\n                          \u2272  1                                                                            (314)\n                             \u03b2\nwhere in (i) we use the fact that xi = \u27e8Zei, ZUei\u27e9       satisfies |  i xi| = |\u27e8Z, ZU\u27e9| \u2264     \u2225Z\u2225F \u2225ZU\u2225F \u2264\n                                                         \u221a x(x+4\u03b2)\n1. The last inequality uses the fact that maxx\u22650          (x+\u03b2)5/2 \u2272    1/\u03b2. Combining eqs. (303), (307)\nand (314) in eq. (299) results in the bound,\n                     max                 vec(Z)T [\u22072R\u03b2(U) \u2212        R\u03b2(U + tZU)]vec(Z)         \u2272   1       (315)\n             Z,ZU:\u2225Z\u2225F ,\u2225ZU\u2225F \u22641 limt\u21920                           t                               \u03b2\nBy choosing ZU = (U \u2212        V )/\u2225U \u2212    V \u2225F , this implies that,\n                    max                                                         \u03b2 \u2225U \u2212   V \u2225F .           (316)\n                 Z:\u2225Z\u2225F \u22641 vec(Z)T [\u22072R\u03b2(U) \u2212          \u22072R\u03b2(V )]vec(Z) \u2272        1\nThis implies that,\n                              \u2225\u22072R\u03b2(U) \u2212       \u22072R\u03b2(V )\u2225op \u2272       1                                      (317)\n                                                                   \u03b2 \u2225U \u2212   V \u2225F .\nCombining with eq. (296) by triangle inequality and noting the assumption that \u03bb \u2264              \u03b2 results in the\nequation,\nThis completes the proof.        \u2225\u22072fpop(U) \u2212     fpop(V )\u2225op \u2272   \u2225U \u2212   V \u2225F .                           (318)\nD.2    Proof of Theorem 8\nIn this section we prove Theorem 8 formally. By the gradient computations in Lemmas 39 and 42\nand triangle inequality,\n        \u2225Ut+1\u2225op \u2264    \u2225(I \u2212   \u03b1UtU Tt )Ut\u2225op + \u03b1\u2225U\u22c6U T    \u22c6 Ut\u2225op + \u03b1\u03bb\u2225D(Ut)\u2225op\u2225Ut\u2225op + \u03b1                 (319)\n                   \u2264  \u2225(I \u2212   \u03b1UtU Tt )Ut\u2225op + \u03b1\u2225U\u22c6U T    \u22c6 Ut\u2225op + \u03b1 \u03bb \u221a \u03b2 \u2225Ut\u2225op + \u03b1                    (320)\nwhere the last inequality follows from the fact that D(Ut) is a diagonal matrix with largest entry upper\nbounded by 2/\u221a\u03b2. Note that \u2225(I \u2212         \u03b1UtU Tt )Ut\u2225op = maxi \u03c3i(1 \u2212       \u03b1\u03c32i ) where {\u03c3i}k i=1 denotes the\nsingular values of Ut. For x \u2208     [0, 1/2\u03b1], x(1 \u2212   \u03b1x2) is an increasing function. Therefore, assuming\n\u2225Ut\u2225op \u2208   [0, 1/2\u03b1],         \u2225(I \u2212  \u03b1UtU T t )Ut\u2225op \u2264   \u2225Ut\u2225op(1 \u2212    \u03b1\u2225Ut\u22252  op).                       (321)\n                                                       45", "md": "# Math Equations\n\nwhere the last inequality uses the fact that $$\\min x \\geq 0$$\n\n$$3\\sqrt{x^3(x+6\\beta)} \\lesssim 1$$\n\n$$\\frac{(x+\\beta)^{7/2}}{\\beta}$$. The second part of the second term on the RHS of eq. (299) is,\n\n$$\\lim_{t \\to 0} \\frac{1}{k} \\sum_{i=1}^{k} G(U)_{ii} \\langle Ze_i, (U + tZU)e_i \\rangle^2 - \\langle Ze_i, Ue_i \\rangle^2$$ (308)\n\n$$= 2 \\sum_{i=1}^{k} G(U)_{ii} \\langle Ze_i, Ue_i \\rangle \\langle Ze_i, ZUe_i \\rangle$$ (309)\n\n$$\\lesssim G(U)_{ii} \\lVert Ue_i \\rVert_F \\langle Ze_i, ZUe_i \\rangle$$ (310)\n\n$$\\lesssim \\sum_{i=1}^{k} G(U)_{ii} \\lVert Ue_i \\rVert_F \\lVert Ze_i \\rVert^2 \\lVert ZUe_i \\rVert^2$$ (311)\n\n$$\\leq \\max_{i \\in [k]} (G(U)_{ii})^2 \\lVert Ue_i \\rVert^2_F \\lVert Ze_i \\rVert^2 2$$ (312)\n\n$$\\leq \\max_{i \\in [k]} G(U)_{ii} \\lVert Ue_i \\rVert_F \\lesssim 1$$ (314)\n\nwhere in (i) we use the fact that $$x_i = \\langle Ze_i, ZUe_i \\rangle$$ satisfies $$|\\sqrt{i} x_i| = |\\langle Z, ZU \\rangle| \\leq \\sqrt{x(x+4\\beta)}$$\n\nThe last inequality uses the fact that $$\\max x \\geq 0$$ $$(x+\\beta)^{5/2} \\lesssim 1/\\beta$$. Combining eqs. (303), (307) and (314) in eq. (299) results in the bound,\n\n$$\\max_{Z,ZU: \\lVert Z \\rVert_F, \\lVert ZU \\rVert_F \\leq 1} \\lim_{t \\to 0} \\frac{\\text{vec}(Z)^T [\\nabla^2R_\\beta(U) - R_\\beta(U + tZU)]\\text{vec}(Z)}{t} \\lesssim 1/\\beta$$ (315)\n\nBy choosing $$ZU = (U - V)/\\lVert U - V \\rVert_F$$, this implies that,\n\n$$\\max_{Z: \\lVert Z \\rVert_F \\leq 1} \\text{vec}(Z)^T [\\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(V)]\\text{vec}(Z) \\lesssim 1$$\n\nThis implies that,\n\n$$\\lVert \\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(V) \\rVert_{op} \\lesssim 1/\\beta \\lVert U - V \\rVert_F$$\n\nCombining with eq. (296) by triangle inequality and noting the assumption that $$\\lambda \\leq \\beta$$ results in the equation,\n\nThis completes the proof. $$\\lVert \\nabla^2f_{pop}(U) - f_{pop}(V) \\rVert_{op} \\lesssim \\lVert U - V \\rVert_F$$\n\nD.2 Proof of Theorem 8\n\nIn this section we prove Theorem 8 formally. By the gradient computations in Lemmas 39 and 42 and triangle inequality,\n\n$$\\lVert U_{t+1} \\rVert_{op} \\leq \\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} + \\alpha \\lVert U^* U^T U_t \\rVert_{op} + \\alpha \\lambda \\lVert D(U_t) \\rVert_{op} \\lVert U_t \\rVert_{op} + \\alpha$$ (319)\n\n$$\\leq \\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} + \\alpha \\lVert U^* U^T U_t \\rVert_{op} + \\alpha \\lambda \\sqrt{\\beta} \\lVert U_t \\rVert_{op} + \\alpha$$ (320)\n\nwhere the last inequality follows from the fact that $$D(U_t)$$ is a diagonal matrix with the largest entry upper bounded by $$2/\\sqrt{\\beta}$$. Note that $$\\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} = \\max_i \\sigma_i(1 - \\alpha \\sigma_i^2)$$ where {$\\sigma_i$}$_{k i=1}$ denotes the singular values of $$U_t$$. For $$x \\in [0, 1/2\\alpha]$$, $$x(1 - \\alpha x^2)$$ is an increasing function. Therefore, assuming $$\\lVert U_t \\rVert_{op} \\in [0, 1/2\\alpha]$$, $$\\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} \\leq \\lVert U_t \\rVert_{op}(1 - \\alpha \\lVert U_t \\rVert_{op}^2)$$ (321)", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "where the last inequality uses the fact that $$\\min x \\geq 0$$\n\n$$3\\sqrt{x^3(x+6\\beta)} \\lesssim 1$$\n\n$$\\frac{(x+\\beta)^{7/2}}{\\beta}$$. The second part of the second term on the RHS of eq. (299) is,\n\n$$\\lim_{t \\to 0} \\frac{1}{k} \\sum_{i=1}^{k} G(U)_{ii} \\langle Ze_i, (U + tZU)e_i \\rangle^2 - \\langle Ze_i, Ue_i \\rangle^2$$ (308)\n\n$$= 2 \\sum_{i=1}^{k} G(U)_{ii} \\langle Ze_i, Ue_i \\rangle \\langle Ze_i, ZUe_i \\rangle$$ (309)\n\n$$\\lesssim G(U)_{ii} \\lVert Ue_i \\rVert_F \\langle Ze_i, ZUe_i \\rangle$$ (310)\n\n$$\\lesssim \\sum_{i=1}^{k} G(U)_{ii} \\lVert Ue_i \\rVert_F \\lVert Ze_i \\rVert^2 \\lVert ZUe_i \\rVert^2$$ (311)\n\n$$\\leq \\max_{i \\in [k]} (G(U)_{ii})^2 \\lVert Ue_i \\rVert^2_F \\lVert Ze_i \\rVert^2 2$$ (312)\n\n$$\\leq \\max_{i \\in [k]} G(U)_{ii} \\lVert Ue_i \\rVert_F \\lesssim 1$$ (314)\n\nwhere in (i) we use the fact that $$x_i = \\langle Ze_i, ZUe_i \\rangle$$ satisfies $$|\\sqrt{i} x_i| = |\\langle Z, ZU \\rangle| \\leq \\sqrt{x(x+4\\beta)}$$\n\nThe last inequality uses the fact that $$\\max x \\geq 0$$ $$(x+\\beta)^{5/2} \\lesssim 1/\\beta$$. Combining eqs. (303), (307) and (314) in eq. (299) results in the bound,\n\n$$\\max_{Z,ZU: \\lVert Z \\rVert_F, \\lVert ZU \\rVert_F \\leq 1} \\lim_{t \\to 0} \\frac{\\text{vec}(Z)^T [\\nabla^2R_\\beta(U) - R_\\beta(U + tZU)]\\text{vec}(Z)}{t} \\lesssim 1/\\beta$$ (315)\n\nBy choosing $$ZU = (U - V)/\\lVert U - V \\rVert_F$$, this implies that,\n\n$$\\max_{Z: \\lVert Z \\rVert_F \\leq 1} \\text{vec}(Z)^T [\\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(V)]\\text{vec}(Z) \\lesssim 1$$\n\nThis implies that,\n\n$$\\lVert \\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(V) \\rVert_{op} \\lesssim 1/\\beta \\lVert U - V \\rVert_F$$\n\nCombining with eq. (296) by triangle inequality and noting the assumption that $$\\lambda \\leq \\beta$$ results in the equation,\n\nThis completes the proof. $$\\lVert \\nabla^2f_{pop}(U) - f_{pop}(V) \\rVert_{op} \\lesssim \\lVert U - V \\rVert_F$$\n\nD.2 Proof of Theorem 8\n\nIn this section we prove Theorem 8 formally. By the gradient computations in Lemmas 39 and 42 and triangle inequality,\n\n$$\\lVert U_{t+1} \\rVert_{op} \\leq \\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} + \\alpha \\lVert U^* U^T U_t \\rVert_{op} + \\alpha \\lambda \\lVert D(U_t) \\rVert_{op} \\lVert U_t \\rVert_{op} + \\alpha$$ (319)\n\n$$\\leq \\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} + \\alpha \\lVert U^* U^T U_t \\rVert_{op} + \\alpha \\lambda \\sqrt{\\beta} \\lVert U_t \\rVert_{op} + \\alpha$$ (320)\n\nwhere the last inequality follows from the fact that $$D(U_t)$$ is a diagonal matrix with the largest entry upper bounded by $$2/\\sqrt{\\beta}$$. Note that $$\\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} = \\max_i \\sigma_i(1 - \\alpha \\sigma_i^2)$$ where {$\\sigma_i$}$_{k i=1}$ denotes the singular values of $$U_t$$. For $$x \\in [0, 1/2\\alpha]$$, $$x(1 - \\alpha x^2)$$ is an increasing function. Therefore, assuming $$\\lVert U_t \\rVert_{op} \\in [0, 1/2\\alpha]$$, $$\\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} \\leq \\lVert U_t \\rVert_{op}(1 - \\alpha \\lVert U_t \\rVert_{op}^2)$$ (321)", "md": "where the last inequality uses the fact that $$\\min x \\geq 0$$\n\n$$3\\sqrt{x^3(x+6\\beta)} \\lesssim 1$$\n\n$$\\frac{(x+\\beta)^{7/2}}{\\beta}$$. The second part of the second term on the RHS of eq. (299) is,\n\n$$\\lim_{t \\to 0} \\frac{1}{k} \\sum_{i=1}^{k} G(U)_{ii} \\langle Ze_i, (U + tZU)e_i \\rangle^2 - \\langle Ze_i, Ue_i \\rangle^2$$ (308)\n\n$$= 2 \\sum_{i=1}^{k} G(U)_{ii} \\langle Ze_i, Ue_i \\rangle \\langle Ze_i, ZUe_i \\rangle$$ (309)\n\n$$\\lesssim G(U)_{ii} \\lVert Ue_i \\rVert_F \\langle Ze_i, ZUe_i \\rangle$$ (310)\n\n$$\\lesssim \\sum_{i=1}^{k} G(U)_{ii} \\lVert Ue_i \\rVert_F \\lVert Ze_i \\rVert^2 \\lVert ZUe_i \\rVert^2$$ (311)\n\n$$\\leq \\max_{i \\in [k]} (G(U)_{ii})^2 \\lVert Ue_i \\rVert^2_F \\lVert Ze_i \\rVert^2 2$$ (312)\n\n$$\\leq \\max_{i \\in [k]} G(U)_{ii} \\lVert Ue_i \\rVert_F \\lesssim 1$$ (314)\n\nwhere in (i) we use the fact that $$x_i = \\langle Ze_i, ZUe_i \\rangle$$ satisfies $$|\\sqrt{i} x_i| = |\\langle Z, ZU \\rangle| \\leq \\sqrt{x(x+4\\beta)}$$\n\nThe last inequality uses the fact that $$\\max x \\geq 0$$ $$(x+\\beta)^{5/2} \\lesssim 1/\\beta$$. Combining eqs. (303), (307) and (314) in eq. (299) results in the bound,\n\n$$\\max_{Z,ZU: \\lVert Z \\rVert_F, \\lVert ZU \\rVert_F \\leq 1} \\lim_{t \\to 0} \\frac{\\text{vec}(Z)^T [\\nabla^2R_\\beta(U) - R_\\beta(U + tZU)]\\text{vec}(Z)}{t} \\lesssim 1/\\beta$$ (315)\n\nBy choosing $$ZU = (U - V)/\\lVert U - V \\rVert_F$$, this implies that,\n\n$$\\max_{Z: \\lVert Z \\rVert_F \\leq 1} \\text{vec}(Z)^T [\\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(V)]\\text{vec}(Z) \\lesssim 1$$\n\nThis implies that,\n\n$$\\lVert \\nabla^2R_\\beta(U) - \\nabla^2R_\\beta(V) \\rVert_{op} \\lesssim 1/\\beta \\lVert U - V \\rVert_F$$\n\nCombining with eq. (296) by triangle inequality and noting the assumption that $$\\lambda \\leq \\beta$$ results in the equation,\n\nThis completes the proof. $$\\lVert \\nabla^2f_{pop}(U) - f_{pop}(V) \\rVert_{op} \\lesssim \\lVert U - V \\rVert_F$$\n\nD.2 Proof of Theorem 8\n\nIn this section we prove Theorem 8 formally. By the gradient computations in Lemmas 39 and 42 and triangle inequality,\n\n$$\\lVert U_{t+1} \\rVert_{op} \\leq \\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} + \\alpha \\lVert U^* U^T U_t \\rVert_{op} + \\alpha \\lambda \\lVert D(U_t) \\rVert_{op} \\lVert U_t \\rVert_{op} + \\alpha$$ (319)\n\n$$\\leq \\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} + \\alpha \\lVert U^* U^T U_t \\rVert_{op} + \\alpha \\lambda \\sqrt{\\beta} \\lVert U_t \\rVert_{op} + \\alpha$$ (320)\n\nwhere the last inequality follows from the fact that $$D(U_t)$$ is a diagonal matrix with the largest entry upper bounded by $$2/\\sqrt{\\beta}$$. Note that $$\\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} = \\max_i \\sigma_i(1 - \\alpha \\sigma_i^2)$$ where {$\\sigma_i$}$_{k i=1}$ denotes the singular values of $$U_t$$. For $$x \\in [0, 1/2\\alpha]$$, $$x(1 - \\alpha x^2)$$ is an increasing function. Therefore, assuming $$\\lVert U_t \\rVert_{op} \\in [0, 1/2\\alpha]$$, $$\\lVert (I - \\alpha U_t U_t^T)U_t \\rVert_{op} \\leq \\lVert U_t \\rVert_{op}(1 - \\alpha \\lVert U_t \\rVert_{op}^2)$$ (321)"}]}, {"page": 46, "text": "Combining everything together,\n                  \u2225Ut+1\u2225op \u2264    \u2225Ut\u2225op(1 \u2212   \u03b1\u2225Ut\u22252 op) + \u03b1\u2225Ut\u2225op + \u03b1 \u03bb \u221a  \u03b2 \u2225Ut\u2225op + \u03b1              (322)\n                             \u2264  \u2225Ut\u2225op(1 \u2212   \u03b1\u2225Ut\u22252 op) + 2\u03b1\u2225Ut\u2225op + \u03b1                               (323)\nwhere the last inequality assumes \u03bb/\u221a\u03b2 \u2264        1. Depending on whether \u2225Ut\u2225op \u2265        2 or \u2225Ut\u2225op \u2264    2,\nand recalling that \u03b1 \u2264   1/8, we may derive two bounds from the above,\n                       0 \u2264  \u2225Ut\u2225op \u2264   2 =\u21d2     \u2225Ut+1\u2225op \u2264   \u2225Ut\u2225op(1 + 2\u03b1) + \u03b1                      (324)\n                       2 \u2264  \u2225Ut\u2225op \u2264   4 =\u21d2     \u2225Ut+1\u2225op \u2264   \u2225Ut\u2225op(1 \u2212    \u03b1)                        (325)\nSuppose the initial iterate satisfied \u2225U0\u2225op \u2264    2, and let t0 be any iteration where \u2225Ut0\u2225op \u2264    2, but\n\u2225Ut0+1\u2225op \u2265    2. Then by eq. (324), \u2225Ut0+1\u2225op \u2264      3. However, in the subsequent iterations, \u2225Ut\u2225op\nmust decrease until it is no larger than 2, by virtue of eq. (325). This implies the statement of the\ntheorem.\nE     Gradient and Hessian computations\nE.1    Population mean square error\nLemma 39. Consider the function Lpop(U) = \u2225UU T \u2212            U\u22c6U T\u22c6 \u22252\n                                                                     F . Then,\n       1. The gradient of L satisfies \u27e8\u2207Lpop(U), Z\u27e9     = 2\u27e8(UU T \u2212    U\u22c6U T\u22c6 ), UZT + ZU T \u27e9.\n       2. For any Z \u2208   Rd\u00d7k,\n            vec(Z)T [\u22072Lpop(U)]vec(Z) = 4\u27e8Z, (UU T \u2212           U\u22c6U T\u22c6 )Z\u27e9  + 2\u2225UZT + ZU T \u22252    F . (326)\nProof. The first part is proved in [55, eq. (59)]. The second part is proved shortly after in [55, eq.\n(61)].\nLemma 40. Consider the function L(U) = f(UU T ) : Rd\u00d7k \u2192               R for some doubly differentiable\nfunction f : Rd\u00d7d \u2192     R. Then,\n       1. For any Z \u2208   Rd\u00d7k, \u27e8\u2207L(U), Z\u27e9     = \u27e8(\u2207f)(UU T ), UZT + ZU T \u27e9.\n       2. For any Z \u2208   Rd\u00d7d,\n             vec(Z)T [\u22072L(U)]vec(Z)\n             = vec(UZT + ZU T )T [(\u22072f)(UU T )]vec(UZT + ZU T ) + 2\u27e8(\u2207f)(UU T ), ZZT \u27e9               (327)\nProof. This result is straightforward to prove by direct computation.\nE.2    Empirical mean square error\nLemma 41. Consider the loss f(U) = \u2225UU T \u2212           U\u22c6U\u22c6\u22252 H where \u2225   \u00b7 \u2225H is defined in eq. (250). Then,\n       1. The gradient of f satisfies, \u27e8\u2207f(U), Z\u27e9    = 2\u27e8UU T \u2212    U\u22c6U T\u22c6 , UZT + ZU T \u27e9H.\n       2. For any Z \u2208   Rd\u00d7d,\n             vec(Z)T [\u22072f(U)]vec(Z) = 4\u27e8UU T \u2212           U\u22c6U T                                       (328)\nProof. These results are proved in [36, eq. (18)].  46       \u22c6 , ZZT \u27e9H + 2\u2225UZT + ZU T \u22252      H.", "md": "# Math Equations\n\nCombining everything together,\n\n$$\n\\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 - \\alpha \\|U_{t}\\|_{op}^2) + \\alpha \\|U_{t}\\|_{op} + \\alpha \\lambda \\sqrt{\\beta} \\|U_{t}\\|_{op} + \\alpha \\quad (322)\n$$\n$$\n\\leq \\|U_{t}\\|_{op}(1 - \\alpha \\|U_{t}\\|_{op}^2) + 2\\alpha \\|U_{t}\\|_{op} + \\alpha \\quad (323)\n$$\nwhere the last inequality assumes $\\lambda/\\sqrt{\\beta} \\leq 1$. Depending on whether $\\|U_{t}\\|_{op} \\geq 2$ or $\\|U_{t}\\|_{op} \\leq 2$, and recalling that $\\alpha \\leq 1/8$, we may derive two bounds from the above,\n\n$$\n0 \\leq \\|U_{t}\\|_{op} \\leq 2 \\Rightarrow \\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 + 2\\alpha) + \\alpha \\quad (324)\n$$\n$$\n2 \\leq \\|U_{t}\\|_{op} \\leq 4 \\Rightarrow \\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 - \\alpha) \\quad (325)\n$$\nSuppose the initial iterate satisfied $\\|U_{0}\\|_{op} \\leq 2$, and let $t_{0}$ be any iteration where $\\|U_{t_{0}}\\|_{op} \\leq 2$, but $\\|U_{t_{0}+1}\\|_{op} \\geq 2$. Then by eq. (324), $\\|U_{t_{0}+1}\\|_{op} \\leq 3$. However, in the subsequent iterations, $\\|U_{t}\\|_{op}$ must decrease until it is no larger than 2, by virtue of eq. (325). This implies the statement of the theorem.\n\nGradient and Hessian computations\n\n### Population mean square error\n\nLemma 39. Consider the function $L_{pop}(U) = \\|UU^{T} - U^{\\star}U^{T\\star}\\|_{F}^{2}$. Then,\n\n1. The gradient of $L$ satisfies $\\langle \\nabla L_{pop}(U), Z \\rangle = 2\\langle (UU^{T} - U^{\\star}U^{T\\star}), UZ^{T} + ZU^{T} \\rangle$.\n2. For any $Z \\in \\mathbb{R}^{d \\times k}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}L_{pop}(U)]\\text{vec}(Z) = 4\\langle Z, (UU^{T} - U^{\\star}U^{T\\star})Z \\rangle + 2\\|UZ^{T} + ZU^{T}\\|_{F}^{2} \\quad (326)\n$$\nProof. The first part is proved in [55, eq. (59)]. The second part is proved shortly after in [55, eq. (61)].\n\nLemma 40. Consider the function $L(U) = f(UU^{T}) : \\mathbb{R}^{d \\times k} \\rightarrow \\mathbb{R}$ for some doubly differentiable function $f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}$. Then,\n\n1. For any $Z \\in \\mathbb{R}^{d \\times k}$, $\\langle \\nabla L(U), Z \\rangle = \\langle (\\nabla f)(UU^{T}), UZ^{T} + ZU^{T} \\rangle$.\n2. For any $Z \\in \\mathbb{R}^{d \\times d}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}L(U)]\\text{vec}(Z) = \\text{vec}(UZ^{T} + ZU^{T})^{T}[(\\nabla^{2}f)(UU^{T})]\\text{vec}(UZ^{T} + ZU^{T}) + 2\\langle (\\nabla f)(UU^{T}), ZZ^{T} \\rangle \\quad (327)\n$$\nProof. This result is straightforward to prove by direct computation.\n\n### Empirical mean square error\n\nLemma 41. Consider the loss $f(U) = \\|UU^{T} - U^{\\star}U^{\\star}\\|_{H}^{2}$ where $\\| \\cdot \\|_{H}$ is defined in eq. (250). Then,\n\n1. The gradient of $f$ satisfies, $\\langle \\nabla f(U), Z \\rangle = 2\\langle UU^{T} - U^{\\star}U^{T\\star}, UZ^{T} + ZU^{T} \\rangle_{H}$.\n2. For any $Z \\in \\mathbb{R}^{d \\times d}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}f(U)]\\text{vec}(Z) = 4\\langle UU^{T} - U^{\\star}U^{T\\star}, ZZ^{T} \\rangle_{H} + 2\\|UZ^{T} + ZU^{T}\\|_{H}^{2} \\quad (328)\n$$\nProof. These results are proved in [36, eq. (18)].", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Combining everything together,\n\n$$\n\\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 - \\alpha \\|U_{t}\\|_{op}^2) + \\alpha \\|U_{t}\\|_{op} + \\alpha \\lambda \\sqrt{\\beta} \\|U_{t}\\|_{op} + \\alpha \\quad (322)\n$$\n$$\n\\leq \\|U_{t}\\|_{op}(1 - \\alpha \\|U_{t}\\|_{op}^2) + 2\\alpha \\|U_{t}\\|_{op} + \\alpha \\quad (323)\n$$\nwhere the last inequality assumes $\\lambda/\\sqrt{\\beta} \\leq 1$. Depending on whether $\\|U_{t}\\|_{op} \\geq 2$ or $\\|U_{t}\\|_{op} \\leq 2$, and recalling that $\\alpha \\leq 1/8$, we may derive two bounds from the above,\n\n$$\n0 \\leq \\|U_{t}\\|_{op} \\leq 2 \\Rightarrow \\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 + 2\\alpha) + \\alpha \\quad (324)\n$$\n$$\n2 \\leq \\|U_{t}\\|_{op} \\leq 4 \\Rightarrow \\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 - \\alpha) \\quad (325)\n$$\nSuppose the initial iterate satisfied $\\|U_{0}\\|_{op} \\leq 2$, and let $t_{0}$ be any iteration where $\\|U_{t_{0}}\\|_{op} \\leq 2$, but $\\|U_{t_{0}+1}\\|_{op} \\geq 2$. Then by eq. (324), $\\|U_{t_{0}+1}\\|_{op} \\leq 3$. However, in the subsequent iterations, $\\|U_{t}\\|_{op}$ must decrease until it is no larger than 2, by virtue of eq. (325). This implies the statement of the theorem.\n\nGradient and Hessian computations", "md": "Combining everything together,\n\n$$\n\\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 - \\alpha \\|U_{t}\\|_{op}^2) + \\alpha \\|U_{t}\\|_{op} + \\alpha \\lambda \\sqrt{\\beta} \\|U_{t}\\|_{op} + \\alpha \\quad (322)\n$$\n$$\n\\leq \\|U_{t}\\|_{op}(1 - \\alpha \\|U_{t}\\|_{op}^2) + 2\\alpha \\|U_{t}\\|_{op} + \\alpha \\quad (323)\n$$\nwhere the last inequality assumes $\\lambda/\\sqrt{\\beta} \\leq 1$. Depending on whether $\\|U_{t}\\|_{op} \\geq 2$ or $\\|U_{t}\\|_{op} \\leq 2$, and recalling that $\\alpha \\leq 1/8$, we may derive two bounds from the above,\n\n$$\n0 \\leq \\|U_{t}\\|_{op} \\leq 2 \\Rightarrow \\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 + 2\\alpha) + \\alpha \\quad (324)\n$$\n$$\n2 \\leq \\|U_{t}\\|_{op} \\leq 4 \\Rightarrow \\|U_{t+1}\\|_{op} \\leq \\|U_{t}\\|_{op}(1 - \\alpha) \\quad (325)\n$$\nSuppose the initial iterate satisfied $\\|U_{0}\\|_{op} \\leq 2$, and let $t_{0}$ be any iteration where $\\|U_{t_{0}}\\|_{op} \\leq 2$, but $\\|U_{t_{0}+1}\\|_{op} \\geq 2$. Then by eq. (324), $\\|U_{t_{0}+1}\\|_{op} \\leq 3$. However, in the subsequent iterations, $\\|U_{t}\\|_{op}$ must decrease until it is no larger than 2, by virtue of eq. (325). This implies the statement of the theorem.\n\nGradient and Hessian computations"}, {"type": "heading", "lvl": 3, "value": "Population mean square error", "md": "### Population mean square error"}, {"type": "text", "value": "Lemma 39. Consider the function $L_{pop}(U) = \\|UU^{T} - U^{\\star}U^{T\\star}\\|_{F}^{2}$. Then,\n\n1. The gradient of $L$ satisfies $\\langle \\nabla L_{pop}(U), Z \\rangle = 2\\langle (UU^{T} - U^{\\star}U^{T\\star}), UZ^{T} + ZU^{T} \\rangle$.\n2. For any $Z \\in \\mathbb{R}^{d \\times k}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}L_{pop}(U)]\\text{vec}(Z) = 4\\langle Z, (UU^{T} - U^{\\star}U^{T\\star})Z \\rangle + 2\\|UZ^{T} + ZU^{T}\\|_{F}^{2} \\quad (326)\n$$\nProof. The first part is proved in [55, eq. (59)]. The second part is proved shortly after in [55, eq. (61)].\n\nLemma 40. Consider the function $L(U) = f(UU^{T}) : \\mathbb{R}^{d \\times k} \\rightarrow \\mathbb{R}$ for some doubly differentiable function $f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}$. Then,\n\n1. For any $Z \\in \\mathbb{R}^{d \\times k}$, $\\langle \\nabla L(U), Z \\rangle = \\langle (\\nabla f)(UU^{T}), UZ^{T} + ZU^{T} \\rangle$.\n2. For any $Z \\in \\mathbb{R}^{d \\times d}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}L(U)]\\text{vec}(Z) = \\text{vec}(UZ^{T} + ZU^{T})^{T}[(\\nabla^{2}f)(UU^{T})]\\text{vec}(UZ^{T} + ZU^{T}) + 2\\langle (\\nabla f)(UU^{T}), ZZ^{T} \\rangle \\quad (327)\n$$\nProof. This result is straightforward to prove by direct computation.", "md": "Lemma 39. Consider the function $L_{pop}(U) = \\|UU^{T} - U^{\\star}U^{T\\star}\\|_{F}^{2}$. Then,\n\n1. The gradient of $L$ satisfies $\\langle \\nabla L_{pop}(U), Z \\rangle = 2\\langle (UU^{T} - U^{\\star}U^{T\\star}), UZ^{T} + ZU^{T} \\rangle$.\n2. For any $Z \\in \\mathbb{R}^{d \\times k}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}L_{pop}(U)]\\text{vec}(Z) = 4\\langle Z, (UU^{T} - U^{\\star}U^{T\\star})Z \\rangle + 2\\|UZ^{T} + ZU^{T}\\|_{F}^{2} \\quad (326)\n$$\nProof. The first part is proved in [55, eq. (59)]. The second part is proved shortly after in [55, eq. (61)].\n\nLemma 40. Consider the function $L(U) = f(UU^{T}) : \\mathbb{R}^{d \\times k} \\rightarrow \\mathbb{R}$ for some doubly differentiable function $f : \\mathbb{R}^{d \\times d} \\rightarrow \\mathbb{R}$. Then,\n\n1. For any $Z \\in \\mathbb{R}^{d \\times k}$, $\\langle \\nabla L(U), Z \\rangle = \\langle (\\nabla f)(UU^{T}), UZ^{T} + ZU^{T} \\rangle$.\n2. For any $Z \\in \\mathbb{R}^{d \\times d}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}L(U)]\\text{vec}(Z) = \\text{vec}(UZ^{T} + ZU^{T})^{T}[(\\nabla^{2}f)(UU^{T})]\\text{vec}(UZ^{T} + ZU^{T}) + 2\\langle (\\nabla f)(UU^{T}), ZZ^{T} \\rangle \\quad (327)\n$$\nProof. This result is straightforward to prove by direct computation."}, {"type": "heading", "lvl": 3, "value": "Empirical mean square error", "md": "### Empirical mean square error"}, {"type": "text", "value": "Lemma 41. Consider the loss $f(U) = \\|UU^{T} - U^{\\star}U^{\\star}\\|_{H}^{2}$ where $\\| \\cdot \\|_{H}$ is defined in eq. (250). Then,\n\n1. The gradient of $f$ satisfies, $\\langle \\nabla f(U), Z \\rangle = 2\\langle UU^{T} - U^{\\star}U^{T\\star}, UZ^{T} + ZU^{T} \\rangle_{H}$.\n2. For any $Z \\in \\mathbb{R}^{d \\times d}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}f(U)]\\text{vec}(Z) = 4\\langle UU^{T} - U^{\\star}U^{T\\star}, ZZ^{T} \\rangle_{H} + 2\\|UZ^{T} + ZU^{T}\\|_{H}^{2} \\quad (328)\n$$\nProof. These results are proved in [36, eq. (18)].", "md": "Lemma 41. Consider the loss $f(U) = \\|UU^{T} - U^{\\star}U^{\\star}\\|_{H}^{2}$ where $\\| \\cdot \\|_{H}$ is defined in eq. (250). Then,\n\n1. The gradient of $f$ satisfies, $\\langle \\nabla f(U), Z \\rangle = 2\\langle UU^{T} - U^{\\star}U^{T\\star}, UZ^{T} + ZU^{T} \\rangle_{H}$.\n2. For any $Z \\in \\mathbb{R}^{d \\times d}$,\n\n$$\n\\text{vec}(Z)^{T}[\\nabla^{2}f(U)]\\text{vec}(Z) = 4\\langle UU^{T} - U^{\\star}U^{T\\star}, ZZ^{T} \\rangle_{H} + 2\\|UZ^{T} + ZU^{T}\\|_{H}^{2} \\quad (328)\n$$\nProof. These results are proved in [36, eq. (18)]."}]}, {"page": 47, "text": " E.3     Regularization R\u03b2\n Lemma 42. Consider the function R\u03b2 : Rd\u00d7m \u2192                (\u2225Uei\u22252       R defined as R\u03b2(U) =  m               i=1 L\u03b2  2 (Uei). Define,\n                                                                       2 + 2\u03b2)\n                                D(U) = diag                (\u2225Uei\u22252   2 + \u03b2)3/2 : i \u2208         [m]        , and,                           (329)\n                                G(U) = diag                 (\u2225Uei\u22252    2 + 4\u03b2)                          .                                (330)\nFor any Z \u2208         Rd\u00d7m,                                  (\u2225Uei\u22252   2 + \u03b2)5/2 : i \u2208         [m]\n         1. The gradient of R\u03b2 is, \u2207R\u03b2(U) = UD(U).\n         2. The Hessian of R\u03b2 satisfies,                                                       m\n                      vec(Z)T [\u22072R\u03b2(U)]vec(Z) = \u27e8D(U), ZT Z\u27e9                              \u2212   i=1   G(U)ii\u27e8Uei, Zei\u27e92                    (331)\n Proof. Note that the gradient of \u2113\u03b2             2 (v) as a function of v is,\n                      \u2207L\u03b2  2 (v) =            2v          \u2212   1         \u2225v\u22252 2                              2 + 2\u03b2)v                     (332)\n Therefore,                                \u2225v\u22252 2 + \u03b2         2  (\u2225v\u22252 2 + \u03b2)3/2 \u00b7 (2v) = (\u2225v\u22252     (\u2225v\u22252  2 + \u03b2)3/2 .\n                                 \u27e8Z, \u2207R\u03b2(U)\u27e9           =    d     (\u2225Uei\u22252    2 + 2\u03b2)                                                     (333)\n                                                           i=1   (\u2225Uei\u22252   2 + \u03b2)3/2 \u27e8Uei, Zei\u27e9\n                                                       =    d     (\u2225Uei\u22252    2 + 2\u03b2)           ZeieT   i U T                             (334)\n                                                           i=1   (\u2225Uei\u22252   2 + \u03b2)3/2 Tr\n                                     (\u2225Uei\u22252  2+2\u03b2)    = Tr     D(U) \u00b7 U T Z         ,                                                   (335)\n where D(U) = diag                  (\u2225Uei\u22252  2+\u03b2)3/2 : i \u2208       [m]       is as defined in eq. (329).\n On the other hand, the Hessian of \u2113\u03b2              2 (v) as a function of v is,\n                   \u22072\u2113\u03b2  2 (v) = (\u2225v\u22252        2 + 2\u03b2)                      2 + 2\u03b2)                          2                            (336)\n                                     (\u2225v\u22252                        (\u2225v\u22252                           (\u2225v\u22252\n                                            2 + \u03b2)3/2 I \u2212         3(\u2225v\u222522 + \u03b2)5/2 vvT +                  2 + \u03b2)3/2 vvT\n                                 = (\u2225v\u22252      2 + 2\u03b2)               \u2225v\u22252  2 + 4\u03b2                                                         (337)\n                                     (\u2225v\u22252  2 + \u03b2)3/2 I \u2212         (\u2225v\u22252 2 + \u03b2)5/2 vvT .\n The Hessian of R\u03b2 is block diagonal with the ith block equal to \u22072\u2113\u03b2                               2 (Uei). Therefore,\n    vec(Z)T [\u22072R\u03b2(U)]vec(Z) =                       m     (\u2225Uei\u22252    2 + 2\u03b2)               2 \u2212       \u2225Uei\u22252   2 + 4\u03b2\n                                                   i=1   (\u2225Uei\u22252    2 + \u03b2)3/2 \u2225Zei\u22252              (\u2225Uei\u22252    2 + \u03b2)5/2 \u27e8Uei, Zei\u27e92       (338)\n                                                    m\n                                               =   i=1  D(U)ii\u2225Zei\u22252         2 \u2212 m G(U)ii\u27e8Uei, Zei\u27e92                                     (339)\n                                               = Tr(D(U) \u00b7 ZZT ) \u2212              i=1   G(U)ii\u27e8Uei, Zei\u27e92                                  (340)\n Finally we introduce a lemma bounding the entries of D(U) and G(U).\n                                                                       47", "md": "E.3 Regularization R\u03b2\n\nLemma 42. Consider the function $$R_{\\beta} : \\mathbb{R}^{d \\times m} \\rightarrow (\\|Ue_i\\|_2^2 + \\beta)^{5/2} : i \\in [m]$$ defined as $$R_{\\beta}(U) = \\sum_{i=1}^{m} L_{\\beta}^2(Ue_i)$$. Define,\n\n$$\n\\begin{align*}\nD(U) & = \\text{diag}(\\|Ue_i\\|_2^2 + \\beta)^{3/2} : i \\in [m], \\text{ and,} \\quad (329) \\\\\nG(U) & = \\text{diag}(\\|Ue_i\\|_2^2 + 4\\beta) \\quad . \\quad (330)\n\\end{align*}\n$$\nFor any $$Z \\in \\mathbb{R}^{d \\times m}$$,\n\n1. The gradient of $R_{\\beta}$ is, $\\nabla R_{\\beta}(U) = UD(U)$.\n2. The Hessian of $R_{\\beta}$ satisfies,\n\n$$\n\\begin{align*}\n\\text{vec}(Z)^T [\\nabla^2R_{\\beta}(U)]\\text{vec}(Z) & = \\langle D(U), Z^T Z \\rangle - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\quad (331)\n\\end{align*}\n$$\nProof. Note that the gradient of $$\\ell_{\\beta}^2(v)$$ as a function of $$v$$ is,\n\n$$\n\\begin{align*}\n\\nabla L_{\\beta}^2(v) & = 2v - \\frac{1}{\\|v\\|_2^2 + 2\\beta}v \\quad (332)\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\begin{align*}\n\\langle Z, \\nabla R_{\\beta}(U) \\rangle & = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + \\beta} \\langle Ue_i, Ze_i \\rangle \\\\\n& = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + \\beta} Ze_i e_i^T U^T \\\\\n& = \\text{Tr}(\\|Ue_i\\|_2^2 + 2\\beta) U^T Z \\quad (335)\n\\end{align*}\n$$\nwhere $$D(U) = \\text{diag}(\\|Ue_i\\|_2^2 + \\beta)^{3/2} : i \\in [m]$$ is as defined in eq. (329).\n\nOn the other hand, the Hessian of $$\\ell_{\\beta}^2(v)$$ as a function of $$v$$ is,\n\n$$\n\\begin{align*}\n\\nabla^2\\ell_{\\beta}^2(v) & = (\\|v\\|_2^2 + 2\\beta)(\\|v\\|_2^2 + 4\\beta) \\\\\n& \\quad (\\|v\\|_2^2 + \\beta)^{3/2}I - 3(\\|v\\|_2^2 + \\beta)^{5/2}vv^T + (\\|v\\|_2^2 + \\beta)^{3/2}vv^T \\\\\n& = (\\|v\\|_2^2 + 2\\beta)(\\|v\\|_2^2 + 4\\beta) \\\\\n& \\quad (\\|v\\|_2^2 + \\beta)^{3/2}I - (\\|v\\|_2^2 + \\beta)^{5/2}vv^T \\quad (337)\n\\end{align*}\n$$\nThe Hessian of $$R_{\\beta}$$ is block diagonal with the ith block equal to $$\\nabla^2\\ell_{\\beta}^2(Ue_i)$$. Therefore,\n\n$$\n\\begin{align*}\n\\text{vec}(Z)^T [\\nabla^2R_{\\beta}(U)]\\text{vec}(Z) & = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2 + 2\\beta}{\\|Ue_i\\|_2^2 + 4\\beta} - \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + 4\\beta} \\\\\n& = \\sum_{i=1}^{m} D(U)_{ii}\\|Z e_i\\|_2^2 - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\\\\n& = \\text{Tr}(D(U) \\cdot ZZ^T) - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\quad (340)\n\\end{align*}\n$$\nFinally we introduce a lemma bounding the entries of $$D(U)$$ and $$G(U)$$.\n\n47", "images": [], "items": [{"type": "text", "value": "E.3 Regularization R\u03b2\n\nLemma 42. Consider the function $$R_{\\beta} : \\mathbb{R}^{d \\times m} \\rightarrow (\\|Ue_i\\|_2^2 + \\beta)^{5/2} : i \\in [m]$$ defined as $$R_{\\beta}(U) = \\sum_{i=1}^{m} L_{\\beta}^2(Ue_i)$$. Define,\n\n$$\n\\begin{align*}\nD(U) & = \\text{diag}(\\|Ue_i\\|_2^2 + \\beta)^{3/2} : i \\in [m], \\text{ and,} \\quad (329) \\\\\nG(U) & = \\text{diag}(\\|Ue_i\\|_2^2 + 4\\beta) \\quad . \\quad (330)\n\\end{align*}\n$$\nFor any $$Z \\in \\mathbb{R}^{d \\times m}$$,\n\n1. The gradient of $R_{\\beta}$ is, $\\nabla R_{\\beta}(U) = UD(U)$.\n2. The Hessian of $R_{\\beta}$ satisfies,\n\n$$\n\\begin{align*}\n\\text{vec}(Z)^T [\\nabla^2R_{\\beta}(U)]\\text{vec}(Z) & = \\langle D(U), Z^T Z \\rangle - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\quad (331)\n\\end{align*}\n$$\nProof. Note that the gradient of $$\\ell_{\\beta}^2(v)$$ as a function of $$v$$ is,\n\n$$\n\\begin{align*}\n\\nabla L_{\\beta}^2(v) & = 2v - \\frac{1}{\\|v\\|_2^2 + 2\\beta}v \\quad (332)\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\begin{align*}\n\\langle Z, \\nabla R_{\\beta}(U) \\rangle & = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + \\beta} \\langle Ue_i, Ze_i \\rangle \\\\\n& = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + \\beta} Ze_i e_i^T U^T \\\\\n& = \\text{Tr}(\\|Ue_i\\|_2^2 + 2\\beta) U^T Z \\quad (335)\n\\end{align*}\n$$\nwhere $$D(U) = \\text{diag}(\\|Ue_i\\|_2^2 + \\beta)^{3/2} : i \\in [m]$$ is as defined in eq. (329).\n\nOn the other hand, the Hessian of $$\\ell_{\\beta}^2(v)$$ as a function of $$v$$ is,\n\n$$\n\\begin{align*}\n\\nabla^2\\ell_{\\beta}^2(v) & = (\\|v\\|_2^2 + 2\\beta)(\\|v\\|_2^2 + 4\\beta) \\\\\n& \\quad (\\|v\\|_2^2 + \\beta)^{3/2}I - 3(\\|v\\|_2^2 + \\beta)^{5/2}vv^T + (\\|v\\|_2^2 + \\beta)^{3/2}vv^T \\\\\n& = (\\|v\\|_2^2 + 2\\beta)(\\|v\\|_2^2 + 4\\beta) \\\\\n& \\quad (\\|v\\|_2^2 + \\beta)^{3/2}I - (\\|v\\|_2^2 + \\beta)^{5/2}vv^T \\quad (337)\n\\end{align*}\n$$\nThe Hessian of $$R_{\\beta}$$ is block diagonal with the ith block equal to $$\\nabla^2\\ell_{\\beta}^2(Ue_i)$$. Therefore,\n\n$$\n\\begin{align*}\n\\text{vec}(Z)^T [\\nabla^2R_{\\beta}(U)]\\text{vec}(Z) & = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2 + 2\\beta}{\\|Ue_i\\|_2^2 + 4\\beta} - \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + 4\\beta} \\\\\n& = \\sum_{i=1}^{m} D(U)_{ii}\\|Z e_i\\|_2^2 - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\\\\n& = \\text{Tr}(D(U) \\cdot ZZ^T) - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\quad (340)\n\\end{align*}\n$$\nFinally we introduce a lemma bounding the entries of $$D(U)$$ and $$G(U)$$.\n\n47", "md": "E.3 Regularization R\u03b2\n\nLemma 42. Consider the function $$R_{\\beta} : \\mathbb{R}^{d \\times m} \\rightarrow (\\|Ue_i\\|_2^2 + \\beta)^{5/2} : i \\in [m]$$ defined as $$R_{\\beta}(U) = \\sum_{i=1}^{m} L_{\\beta}^2(Ue_i)$$. Define,\n\n$$\n\\begin{align*}\nD(U) & = \\text{diag}(\\|Ue_i\\|_2^2 + \\beta)^{3/2} : i \\in [m], \\text{ and,} \\quad (329) \\\\\nG(U) & = \\text{diag}(\\|Ue_i\\|_2^2 + 4\\beta) \\quad . \\quad (330)\n\\end{align*}\n$$\nFor any $$Z \\in \\mathbb{R}^{d \\times m}$$,\n\n1. The gradient of $R_{\\beta}$ is, $\\nabla R_{\\beta}(U) = UD(U)$.\n2. The Hessian of $R_{\\beta}$ satisfies,\n\n$$\n\\begin{align*}\n\\text{vec}(Z)^T [\\nabla^2R_{\\beta}(U)]\\text{vec}(Z) & = \\langle D(U), Z^T Z \\rangle - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\quad (331)\n\\end{align*}\n$$\nProof. Note that the gradient of $$\\ell_{\\beta}^2(v)$$ as a function of $$v$$ is,\n\n$$\n\\begin{align*}\n\\nabla L_{\\beta}^2(v) & = 2v - \\frac{1}{\\|v\\|_2^2 + 2\\beta}v \\quad (332)\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\begin{align*}\n\\langle Z, \\nabla R_{\\beta}(U) \\rangle & = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + \\beta} \\langle Ue_i, Ze_i \\rangle \\\\\n& = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + \\beta} Ze_i e_i^T U^T \\\\\n& = \\text{Tr}(\\|Ue_i\\|_2^2 + 2\\beta) U^T Z \\quad (335)\n\\end{align*}\n$$\nwhere $$D(U) = \\text{diag}(\\|Ue_i\\|_2^2 + \\beta)^{3/2} : i \\in [m]$$ is as defined in eq. (329).\n\nOn the other hand, the Hessian of $$\\ell_{\\beta}^2(v)$$ as a function of $$v$$ is,\n\n$$\n\\begin{align*}\n\\nabla^2\\ell_{\\beta}^2(v) & = (\\|v\\|_2^2 + 2\\beta)(\\|v\\|_2^2 + 4\\beta) \\\\\n& \\quad (\\|v\\|_2^2 + \\beta)^{3/2}I - 3(\\|v\\|_2^2 + \\beta)^{5/2}vv^T + (\\|v\\|_2^2 + \\beta)^{3/2}vv^T \\\\\n& = (\\|v\\|_2^2 + 2\\beta)(\\|v\\|_2^2 + 4\\beta) \\\\\n& \\quad (\\|v\\|_2^2 + \\beta)^{3/2}I - (\\|v\\|_2^2 + \\beta)^{5/2}vv^T \\quad (337)\n\\end{align*}\n$$\nThe Hessian of $$R_{\\beta}$$ is block diagonal with the ith block equal to $$\\nabla^2\\ell_{\\beta}^2(Ue_i)$$. Therefore,\n\n$$\n\\begin{align*}\n\\text{vec}(Z)^T [\\nabla^2R_{\\beta}(U)]\\text{vec}(Z) & = \\sum_{i=1}^{m} \\frac{\\|Ue_i\\|_2^2 + 2\\beta}{\\|Ue_i\\|_2^2 + 4\\beta} - \\frac{\\|Ue_i\\|_2^2}{\\|Ue_i\\|_2^2 + 4\\beta} \\\\\n& = \\sum_{i=1}^{m} D(U)_{ii}\\|Z e_i\\|_2^2 - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\\\\n& = \\text{Tr}(D(U) \\cdot ZZ^T) - \\sum_{i=1}^{m} G(U)_{ii}\\langle Ue_i, Ze_i \\rangle^2 \\quad (340)\n\\end{align*}\n$$\nFinally we introduce a lemma bounding the entries of $$D(U)$$ and $$G(U)$$.\n\n47"}]}, {"page": 48, "text": " Lemma 43. Suppose for some i \u2208                    [k], \u2225Uei\u22252 \u2265         2\u221a\u03b2. Then the corresponding diagonal entries of\n D(U) and G(U) satisfy,\n                                                                                1\n                                                         (D(U))ii \u2265         \u2225Uei\u22252                                                       (341)\n                                                                                1\n On the other hand, for any U such that \u2225U\u2225op \u2264          (G(U))ii \u2265     3,  \u2225Uei\u22253   2 .                                                 (342)\n                                                                        \u2225Uei\u22252   2 + 2\u03b2\n                                                  (D(U))ii \u2264         (\u2225Uei\u22252    2 + \u03b2)3/2                 \u2225Uei\u22252 2+2x                    (343)\n Proof. The proof for eq. (341) follows by observing that D(U)ii =                                     (\u2225Uei\u22252  2+x)3/2 where x = \u03b2.\n Differentiating we observe that the derivative of function in x is,\n                              2(\u2225Uei\u22252     2 + x) \u2212      3/2(\u2225Uei\u22252      2 + 2x)             1          2 \u2212   x\n                                                                                      =      2\u2225Uei\u22252                                     (344)\n                                             (\u2225Uei\u22252    2 + x)5/2                         (\u2225Uei\u22252    2 + x)5/2\n which is increasing as long as x \u2264                    1          2. Note that when \u2225Uei\u22252 \u2265                   2\u221a\u03b2       =\u21d2       x = \u03b2 \u2264\n 1                                                     2\u2225Uei\u22252\n 4\u2225Uei\u22252    2, the function is increasing in x and therefore the minimum is achieved with \u03b2 = 0. This\n results in the lower bound,\n                                                                                1\n                                \u2225Uei\u22252 2+4x              (D(U))ii \u2265         \u2225Uei\u22252     .                                                 (345)\n Likeiwse, G(U) =            (\u2225Uei\u22252  2+x)5/2 where x = \u03b2, and differentiating in x, we get,\n                              4(\u2225Uei\u22252     2 + x) \u2212      5/2(\u2225Uei\u22252      2 + 2x)             3          2 \u2212   x\n                                                                                      =      2\u2225Uei\u22252                                     (346)\n                                             (\u2225Uei\u22252    2 + x)5/2                         (\u2225Uei\u22252    2 + x)5/2\n which is increasing as long as x \u2264                  3          2. Yet again, the function is in the increasing regime in\n                                                     2\u2225Uei\u22252\n x = \u03b2 under the constraint 2\u221a\u03b2 \u2264                     \u2225Uei\u22252; the minimum is achieved at \u03b2 = 0, which results in\n eq. (342).\n F      Implications for shallow neural networks with quadratic activation\n        functions\n The extension of results from the matrix sensing model to the training of quadratic neural networks\n was previously carried out in [19, Section 5] and originally in [21], which we explain in more detail\n below. Indeed, when the measurement matrices are of the form Ai = xixT                                   i for some vector xi \u2208            Rd,\n the functional representation of the output can be written as \u27e8Ai, UU T \u27e9                            =  k    i=1 \u03c3(\u27e8xi, Uei\u27e9), where\n \u03c3(\u00b7) = (\u00b7)2 takes the form of a 1-hidden layer shallow network with quadratic activations, with\n the output layer frozen as all 1\u2019s. The columns of U correspond to the weight vectors associated\n with individual neurons of the network. Likewise, sparsity in the column domain corresponds\n to learning networks with only a few non-zero neurons. In this section we provide a high level\n sketch of how results for matrix sensing can be extended for the training of neural networks with\n quadratic activations. We avoid going through the formal details for the sake of simplicity and ease\n of exposition.\nWhy can the results proved for matrix sensing not directly be applied here? It turns out that when\n the xi are i.i.d. Gaussian vectors, even if n \u2192   n                \u221e, rank 1 measurements do not satisfy the restricted\n isometry property. In particular, 1            n     i=1\u27e8Ai, X\u27e92 \u0338\u2248         \u2225X\u22252   F . However, these measurements satisfy a\n different form of low rank approximation, as established in [19, Lemma 5.1] and as we informally\n state below. In particular, when xi \u223c            n    N  (0, I) are Gaussian, for any matrix X \u2208                    Rd\u00d7d,\n                                             1       \u27e8Ai, X\u27e92 \u2248        2\u2225X\u22252    F + (Tr(X))2                                             (347)\n                                             n  i=1\n                                                                       48", "md": "# Math Equations\n\n## Lemma 43:\n\nSuppose for some \\( i \\in [k] \\), \\( \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta} \\). Then the corresponding diagonal entries of \\( D(U) \\) and \\( G(U) \\) satisfy,\n\n$$\n\\begin{align*}\n(D(U))_{ii} & \\geq \\frac{1}{\\|Ue_i\\|_2} \\tag{341} \\\\\n(G(U))_{ii} & \\geq \\frac{3}{\\|Ue_i\\|_2^2 + 2\\beta} \\tag{342} \\\\\n(D(U))_{ii} & \\leq \\left( \\|Ue_i\\|_2^2 + \\beta \\right)^{3/2} \\frac{\\|Ue_i\\|_2}{\\|Ue_i\\|_2^2 + 2\\beta} \\tag{343}\n\\end{align*}\n$$\nProof: The proof for eq. (341) follows by observing that \\( D(U)_{ii} = (\\|Ue_i\\|_2^2 + x)^{3/2} \\) where \\( x = \\beta \\). Differentiating we observe that the derivative of function in \\( x \\) is,\n\n$$\n\\frac{2(\\|Ue_i\\|_2^2 + x) - 3/2(\\|Ue_i\\|_2^2 + 2x)}{(\\|Ue_i\\|_2^2 + x)^{5/2}} = \\frac{2\\|Ue_i\\|_2}{(\\|Ue_i\\|_2^2 + x)^{5/2}}\n$$\nwhich is increasing as long as \\( x \\leq \\frac{1}{2} \\). Note that when \\( \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta} \\Rightarrow x = \\beta \\leq \\frac{1}{2\\|Ue_i\\|} \\), the function is increasing in \\( x \\) and therefore the minimum is achieved with \\( \\beta = 0 \\). This results in the lower bound,\n\n$$\n\\|Ue_i\\|_2^2 + 4x \\geq (D(U))_{ii} \\geq \\|Ue_i\\|_2 \\tag{345}\n$$\nLikewise, \\( G(U) = (\\|Ue_i\\|_2^2 + x)^{5/2} \\) where \\( x = \\beta \\), and differentiating in \\( x \\), we get,\n\n$$\n\\frac{4(\\|Ue_i\\|_2^2 + x) - 5/2(\\|Ue_i\\|_2^2 + 2x)}{(\\|Ue_i\\|_2^2 + x)^{5/2}} = \\frac{2\\|Ue_i\\|_2}{(\\|Ue_i\\|_2^2 + x)^{5/2}}\n$$\nwhich is increasing as long as \\( x \\leq \\frac{3}{2} \\). Yet again, the function is in the increasing regime in \\( x = \\beta \\) under the constraint \\( 2\\sqrt{\\beta} \\leq \\|Ue_i\\|_2 \\); the minimum is achieved at \\( \\beta = 0 \\), which results in eq. (342).\n\n## Implications for shallow neural networks with quadratic activation functions:\n\nThe extension of results from the matrix sensing model to the training of quadratic neural networks was previously carried out in [19, Section 5] and originally in [21], which we explain in more detail below. Indeed, when the measurement matrices are of the form \\( A_i = x_ix_i^T \\) for some vector \\( x_i \\in \\mathbb{R}^d \\), the functional representation of the output can be written as \\( \\langle A_i, UU^T \\rangle = \\sum_{i=1}^{k} \\sigma(\\langle x_i, Ue_i \\rangle) \\), where \\( \\sigma(\\cdot) = (\\cdot)^2 \\) takes the form of a 1-hidden layer shallow network with quadratic activations, with the output layer frozen as all 1\u2019s. The columns of \\( U \\) correspond to the weight vectors associated with individual neurons of the network. Likewise, sparsity in the column domain corresponds to learning networks with only a few non-zero neurons. In this section we provide a high level sketch of how results for matrix sensing can be extended for the training of neural networks with quadratic activations. We avoid going through the formal details for the sake of simplicity and ease of exposition.\n\nWhy can the results proved for matrix sensing not directly be applied here? It turns out that when the \\( x_i \\) are i.i.d. Gaussian vectors, even if \\( n \\rightarrow \\infty \\), rank 1 measurements do not satisfy the restricted isometry property. In particular, \\( \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\napprox \\|X\\|_F^2 \\). However, these measurements satisfy a different form of low rank approximation, as established in [19, Lemma 5.1] and as we informally state below. In particular, when \\( x_i \\sim \\mathcal{N}(0, I) \\) are Gaussian, for any matrix \\( X \\in \\mathbb{R}^{d \\times d} \\),\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\approx 2\\|X\\|_F^2 + (\\text{Tr}(X))^2 \\tag{347}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "heading", "lvl": 2, "value": "Lemma 43:", "md": "## Lemma 43:"}, {"type": "text", "value": "Suppose for some \\( i \\in [k] \\), \\( \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta} \\). Then the corresponding diagonal entries of \\( D(U) \\) and \\( G(U) \\) satisfy,\n\n$$\n\\begin{align*}\n(D(U))_{ii} & \\geq \\frac{1}{\\|Ue_i\\|_2} \\tag{341} \\\\\n(G(U))_{ii} & \\geq \\frac{3}{\\|Ue_i\\|_2^2 + 2\\beta} \\tag{342} \\\\\n(D(U))_{ii} & \\leq \\left( \\|Ue_i\\|_2^2 + \\beta \\right)^{3/2} \\frac{\\|Ue_i\\|_2}{\\|Ue_i\\|_2^2 + 2\\beta} \\tag{343}\n\\end{align*}\n$$\nProof: The proof for eq. (341) follows by observing that \\( D(U)_{ii} = (\\|Ue_i\\|_2^2 + x)^{3/2} \\) where \\( x = \\beta \\). Differentiating we observe that the derivative of function in \\( x \\) is,\n\n$$\n\\frac{2(\\|Ue_i\\|_2^2 + x) - 3/2(\\|Ue_i\\|_2^2 + 2x)}{(\\|Ue_i\\|_2^2 + x)^{5/2}} = \\frac{2\\|Ue_i\\|_2}{(\\|Ue_i\\|_2^2 + x)^{5/2}}\n$$\nwhich is increasing as long as \\( x \\leq \\frac{1}{2} \\). Note that when \\( \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta} \\Rightarrow x = \\beta \\leq \\frac{1}{2\\|Ue_i\\|} \\), the function is increasing in \\( x \\) and therefore the minimum is achieved with \\( \\beta = 0 \\). This results in the lower bound,\n\n$$\n\\|Ue_i\\|_2^2 + 4x \\geq (D(U))_{ii} \\geq \\|Ue_i\\|_2 \\tag{345}\n$$\nLikewise, \\( G(U) = (\\|Ue_i\\|_2^2 + x)^{5/2} \\) where \\( x = \\beta \\), and differentiating in \\( x \\), we get,\n\n$$\n\\frac{4(\\|Ue_i\\|_2^2 + x) - 5/2(\\|Ue_i\\|_2^2 + 2x)}{(\\|Ue_i\\|_2^2 + x)^{5/2}} = \\frac{2\\|Ue_i\\|_2}{(\\|Ue_i\\|_2^2 + x)^{5/2}}\n$$\nwhich is increasing as long as \\( x \\leq \\frac{3}{2} \\). Yet again, the function is in the increasing regime in \\( x = \\beta \\) under the constraint \\( 2\\sqrt{\\beta} \\leq \\|Ue_i\\|_2 \\); the minimum is achieved at \\( \\beta = 0 \\), which results in eq. (342).", "md": "Suppose for some \\( i \\in [k] \\), \\( \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta} \\). Then the corresponding diagonal entries of \\( D(U) \\) and \\( G(U) \\) satisfy,\n\n$$\n\\begin{align*}\n(D(U))_{ii} & \\geq \\frac{1}{\\|Ue_i\\|_2} \\tag{341} \\\\\n(G(U))_{ii} & \\geq \\frac{3}{\\|Ue_i\\|_2^2 + 2\\beta} \\tag{342} \\\\\n(D(U))_{ii} & \\leq \\left( \\|Ue_i\\|_2^2 + \\beta \\right)^{3/2} \\frac{\\|Ue_i\\|_2}{\\|Ue_i\\|_2^2 + 2\\beta} \\tag{343}\n\\end{align*}\n$$\nProof: The proof for eq. (341) follows by observing that \\( D(U)_{ii} = (\\|Ue_i\\|_2^2 + x)^{3/2} \\) where \\( x = \\beta \\). Differentiating we observe that the derivative of function in \\( x \\) is,\n\n$$\n\\frac{2(\\|Ue_i\\|_2^2 + x) - 3/2(\\|Ue_i\\|_2^2 + 2x)}{(\\|Ue_i\\|_2^2 + x)^{5/2}} = \\frac{2\\|Ue_i\\|_2}{(\\|Ue_i\\|_2^2 + x)^{5/2}}\n$$\nwhich is increasing as long as \\( x \\leq \\frac{1}{2} \\). Note that when \\( \\|Ue_i\\|_2 \\geq 2\\sqrt{\\beta} \\Rightarrow x = \\beta \\leq \\frac{1}{2\\|Ue_i\\|} \\), the function is increasing in \\( x \\) and therefore the minimum is achieved with \\( \\beta = 0 \\). This results in the lower bound,\n\n$$\n\\|Ue_i\\|_2^2 + 4x \\geq (D(U))_{ii} \\geq \\|Ue_i\\|_2 \\tag{345}\n$$\nLikewise, \\( G(U) = (\\|Ue_i\\|_2^2 + x)^{5/2} \\) where \\( x = \\beta \\), and differentiating in \\( x \\), we get,\n\n$$\n\\frac{4(\\|Ue_i\\|_2^2 + x) - 5/2(\\|Ue_i\\|_2^2 + 2x)}{(\\|Ue_i\\|_2^2 + x)^{5/2}} = \\frac{2\\|Ue_i\\|_2}{(\\|Ue_i\\|_2^2 + x)^{5/2}}\n$$\nwhich is increasing as long as \\( x \\leq \\frac{3}{2} \\). Yet again, the function is in the increasing regime in \\( x = \\beta \\) under the constraint \\( 2\\sqrt{\\beta} \\leq \\|Ue_i\\|_2 \\); the minimum is achieved at \\( \\beta = 0 \\), which results in eq. (342)."}, {"type": "heading", "lvl": 2, "value": "Implications for shallow neural networks with quadratic activation functions:", "md": "## Implications for shallow neural networks with quadratic activation functions:"}, {"type": "text", "value": "The extension of results from the matrix sensing model to the training of quadratic neural networks was previously carried out in [19, Section 5] and originally in [21], which we explain in more detail below. Indeed, when the measurement matrices are of the form \\( A_i = x_ix_i^T \\) for some vector \\( x_i \\in \\mathbb{R}^d \\), the functional representation of the output can be written as \\( \\langle A_i, UU^T \\rangle = \\sum_{i=1}^{k} \\sigma(\\langle x_i, Ue_i \\rangle) \\), where \\( \\sigma(\\cdot) = (\\cdot)^2 \\) takes the form of a 1-hidden layer shallow network with quadratic activations, with the output layer frozen as all 1\u2019s. The columns of \\( U \\) correspond to the weight vectors associated with individual neurons of the network. Likewise, sparsity in the column domain corresponds to learning networks with only a few non-zero neurons. In this section we provide a high level sketch of how results for matrix sensing can be extended for the training of neural networks with quadratic activations. We avoid going through the formal details for the sake of simplicity and ease of exposition.\n\nWhy can the results proved for matrix sensing not directly be applied here? It turns out that when the \\( x_i \\) are i.i.d. Gaussian vectors, even if \\( n \\rightarrow \\infty \\), rank 1 measurements do not satisfy the restricted isometry property. In particular, \\( \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\napprox \\|X\\|_F^2 \\). However, these measurements satisfy a different form of low rank approximation, as established in [19, Lemma 5.1] and as we informally state below. In particular, when \\( x_i \\sim \\mathcal{N}(0, I) \\) are Gaussian, for any matrix \\( X \\in \\mathbb{R}^{d \\times d} \\),\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\approx 2\\|X\\|_F^2 + (\\text{Tr}(X))^2 \\tag{347}\n$$", "md": "The extension of results from the matrix sensing model to the training of quadratic neural networks was previously carried out in [19, Section 5] and originally in [21], which we explain in more detail below. Indeed, when the measurement matrices are of the form \\( A_i = x_ix_i^T \\) for some vector \\( x_i \\in \\mathbb{R}^d \\), the functional representation of the output can be written as \\( \\langle A_i, UU^T \\rangle = \\sum_{i=1}^{k} \\sigma(\\langle x_i, Ue_i \\rangle) \\), where \\( \\sigma(\\cdot) = (\\cdot)^2 \\) takes the form of a 1-hidden layer shallow network with quadratic activations, with the output layer frozen as all 1\u2019s. The columns of \\( U \\) correspond to the weight vectors associated with individual neurons of the network. Likewise, sparsity in the column domain corresponds to learning networks with only a few non-zero neurons. In this section we provide a high level sketch of how results for matrix sensing can be extended for the training of neural networks with quadratic activations. We avoid going through the formal details for the sake of simplicity and ease of exposition.\n\nWhy can the results proved for matrix sensing not directly be applied here? It turns out that when the \\( x_i \\) are i.i.d. Gaussian vectors, even if \\( n \\rightarrow \\infty \\), rank 1 measurements do not satisfy the restricted isometry property. In particular, \\( \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\napprox \\|X\\|_F^2 \\). However, these measurements satisfy a different form of low rank approximation, as established in [19, Lemma 5.1] and as we informally state below. In particular, when \\( x_i \\sim \\mathcal{N}(0, I) \\) are Gaussian, for any matrix \\( X \\in \\mathbb{R}^{d \\times d} \\),\n\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, X \\rangle^2 \\approx 2\\|X\\|_F^2 + (\\text{Tr}(X))^2 \\tag{347}\n$$"}]}, {"page": 49, "text": "where the approximation becomes exact as n \u2192         \u221e. In particular, with the choice of X = UU T \u2212\nU\u22c6U T\u22c6 , the mean squared error of the learner training a quadratic neural network takes the form,\n  LNN(U) = 1       n   \u27e8Ai, UU T \u27e9  \u2212  \u27e8Ai, U\u22c6U T\u22c6 \u27e9 2 \u2248  2\u2225UU T \u2212    U\u22c6U T\u22c6 \u22252      \u2225U\u22252              2\n               n  i=1                                                         F +         F \u2212  \u2225U\u22c6\u22252F\nNotice that the RHS is, up to scaling factors, the mean squared error for matrix sensing, with an\nadditional loss (\u2225U\u22252           F )2 added to it. This loss can easily be estimated since it only relies on\n                      F \u2212\u2225U\u22c6\u22252                                n               n\nestimating a scalar, \u2225U\u22c6\u22252                                                                      \u22c6 \u27e9 + \u03b5i \u2248\n                           F , which is approximated by 1  n    i=1 yi = 1n    i=1\u27e8xixT i , U\u22c6U T\n\u2225U\u22c6\u22252 F . In the sequel, we assume that \u2225U\u22c6\u2225F was known exactly and consider the loss,\n                         fNN(U) = LNN(U) \u2212         \u2225U\u22252 F \u2212  \u2225U\u22c6\u22252F  2 + R\u03b2(U)                        (348)\nwhich subtracts the \u201ccorrection term\u201d (\u2225U\u22252    F \u2212  \u2225U\u22c6\u22252 F )2 from the mean squared error LNN(U) and\nalso adds back the group Lasso regularizer on U. Overall fNN(U) approximately equals 2\u2225UU T \u2212\nU\u22c6U T\u22c6 \u22252\n        F + R\u03b2(U) and therefore, running perturbed gradient descent on this loss and reusing the\nanalysis for matrix sensing shows that the algorithm eventually converges to a solution UU T \u2248      U\u22c6U T\u22c6\nand such that U has approximately r columns.\n                                                    49", "md": "where the approximation becomes exact as \\( n \\rightarrow \\infty \\). In particular, with the choice of \\( X = UU^T - U^*U^{T*} \\), the mean squared error of the learner training a quadratic neural network takes the form,\n\n$$\nL_{NN}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, UU^T \\rangle - \\langle A_i, U^*U^{T*} \\rangle^2 \\approx \\frac{2\\|UU^T - U^*U^{T*}\\|_2^2}{\\|U\\|_2^2 F + \\|U^*\\|_2^F}\n$$\n\nNotice that the RHS is, up to scaling factors, the mean squared error for matrix sensing, with an additional loss \\((\\|U\\|_2^F)^2\\) added to it. This loss can easily be estimated since it only relies on estimating a scalar, \\(\\|U^*\\|_2^F\\), which is approximated by \\(\\frac{1}{n} \\sum_{i=1}^{n} y_i = \\frac{1}{n} \\sum_{i=1}^{n} \\langle x_i x_i^T, U^*U^T \\rangle \\). In the sequel, we assume that \\(\\|U^*\\|_F\\) was known exactly and consider the loss,\n\n$$\nf_{NN}(U) = L_{NN}(U) - \\|U\\|_2^F - \\|U^*\\|_2^F)^2 + R_{\\beta}(U) \\quad (348)\n$$\n\nwhich subtracts the \u201ccorrection term\u201d \\((\\|U\\|_2^F - \\|U^*\\|_2^F)^2\\) from the mean squared error \\(L_{NN}(U)\\) and also adds back the group Lasso regularizer on \\(U\\). Overall \\(f_{NN}(U)\\) approximately equals \\(2\\|UU^T - U^*U^{T*}\\|_F + R_{\\beta}(U)\\) and therefore, running perturbed gradient descent on this loss and reusing the analysis for matrix sensing shows that the algorithm eventually converges to a solution \\(UU^T \\approx U^*U^{T*}\\) and such that \\(U\\) has approximately \\(r\\) columns.\n\n49", "images": [], "items": [{"type": "text", "value": "where the approximation becomes exact as \\( n \\rightarrow \\infty \\). In particular, with the choice of \\( X = UU^T - U^*U^{T*} \\), the mean squared error of the learner training a quadratic neural network takes the form,\n\n$$\nL_{NN}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, UU^T \\rangle - \\langle A_i, U^*U^{T*} \\rangle^2 \\approx \\frac{2\\|UU^T - U^*U^{T*}\\|_2^2}{\\|U\\|_2^2 F + \\|U^*\\|_2^F}\n$$\n\nNotice that the RHS is, up to scaling factors, the mean squared error for matrix sensing, with an additional loss \\((\\|U\\|_2^F)^2\\) added to it. This loss can easily be estimated since it only relies on estimating a scalar, \\(\\|U^*\\|_2^F\\), which is approximated by \\(\\frac{1}{n} \\sum_{i=1}^{n} y_i = \\frac{1}{n} \\sum_{i=1}^{n} \\langle x_i x_i^T, U^*U^T \\rangle \\). In the sequel, we assume that \\(\\|U^*\\|_F\\) was known exactly and consider the loss,\n\n$$\nf_{NN}(U) = L_{NN}(U) - \\|U\\|_2^F - \\|U^*\\|_2^F)^2 + R_{\\beta}(U) \\quad (348)\n$$\n\nwhich subtracts the \u201ccorrection term\u201d \\((\\|U\\|_2^F - \\|U^*\\|_2^F)^2\\) from the mean squared error \\(L_{NN}(U)\\) and also adds back the group Lasso regularizer on \\(U\\). Overall \\(f_{NN}(U)\\) approximately equals \\(2\\|UU^T - U^*U^{T*}\\|_F + R_{\\beta}(U)\\) and therefore, running perturbed gradient descent on this loss and reusing the analysis for matrix sensing shows that the algorithm eventually converges to a solution \\(UU^T \\approx U^*U^{T*}\\) and such that \\(U\\) has approximately \\(r\\) columns.\n\n49", "md": "where the approximation becomes exact as \\( n \\rightarrow \\infty \\). In particular, with the choice of \\( X = UU^T - U^*U^{T*} \\), the mean squared error of the learner training a quadratic neural network takes the form,\n\n$$\nL_{NN}(U) = \\frac{1}{n} \\sum_{i=1}^{n} \\langle A_i, UU^T \\rangle - \\langle A_i, U^*U^{T*} \\rangle^2 \\approx \\frac{2\\|UU^T - U^*U^{T*}\\|_2^2}{\\|U\\|_2^2 F + \\|U^*\\|_2^F}\n$$\n\nNotice that the RHS is, up to scaling factors, the mean squared error for matrix sensing, with an additional loss \\((\\|U\\|_2^F)^2\\) added to it. This loss can easily be estimated since it only relies on estimating a scalar, \\(\\|U^*\\|_2^F\\), which is approximated by \\(\\frac{1}{n} \\sum_{i=1}^{n} y_i = \\frac{1}{n} \\sum_{i=1}^{n} \\langle x_i x_i^T, U^*U^T \\rangle \\). In the sequel, we assume that \\(\\|U^*\\|_F\\) was known exactly and consider the loss,\n\n$$\nf_{NN}(U) = L_{NN}(U) - \\|U\\|_2^F - \\|U^*\\|_2^F)^2 + R_{\\beta}(U) \\quad (348)\n$$\n\nwhich subtracts the \u201ccorrection term\u201d \\((\\|U\\|_2^F - \\|U^*\\|_2^F)^2\\) from the mean squared error \\(L_{NN}(U)\\) and also adds back the group Lasso regularizer on \\(U\\). Overall \\(f_{NN}(U)\\) approximately equals \\(2\\|UU^T - U^*U^{T*}\\|_F + R_{\\beta}(U)\\) and therefore, running perturbed gradient descent on this loss and reusing the analysis for matrix sensing shows that the algorithm eventually converges to a solution \\(UU^T \\approx U^*U^{T*}\\) and such that \\(U\\) has approximately \\(r\\) columns.\n\n49"}]}], "job_id": "dd1f0335-5a2f-40b9-b595-927a6c154720", "file_path": "./corpus/2303.11453.pdf"}