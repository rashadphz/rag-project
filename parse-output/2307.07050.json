{"pages": [{"page": 1, "text": "                                     Wasserstein Quantum Monte Carlo:\n                                           A Novel Approach for Solving\n                         the Quantum Many-Body Schr\u00f6dinger Equation\narXiv:2307.07050v3  [physics.comp-ph]  26 Oct 2023\n                                                              Kirill Neklyudov\n                                                                Vector Institute\n                                                    Jannes Nys                                        Luca Thiede\n                       Institute of Physics & Center for Quantum Science and Engineering             Vector Institute\n                               \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)                    Univeristy of Toronto\n                       Juan Felipe Carrasquilla         Qiang Liu          Max Welling             Alireza Makhzani\n                             Vector Institute           UT Austin       Microsoft Research           Vector Institute\n                         Univeristy of Waterloo                             AI4Science           Univeristy of Toronto\n                                                                  Abstract\n                             Solving the quantum many-body Schr\u00f6dinger equation is a fundamental and chal-\n                             lenging problem in the fields of quantum physics, quantum chemistry, and material\n                             sciences. One of the common computational approaches to this problem is Quan-\n                             tum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained\n                             by minimizing the energy of the system within a restricted family of parameterized\n                             wave functions. Deep learning methods partially address the limitations of tradi-\n                             tional QVMC by representing a rich family of wave functions in terms of neural\n                             networks. However, the optimization objective in QVMC remains notoriously\n                             hard to minimize and requires second-order optimization methods such as natural\n                             gradient. In this paper, we first reformulate energy functional minimization in the\n                             space of Born distributions corresponding to particle-permutation (anti-)symmetric\n                             wave functions, rather than the space of wave functions. We then interpret QVMC\n                             as the Fisher\u2013Rao gradient flow in this distributional space, followed by a pro-\n                             jection step onto the variational manifold. This perspective provides us with a\n                             principled framework to derive new QMC algorithms, by endowing the distribu-\n                             tional space with better metrics, and following the projected gradient flow induced\n                             by those metrics. More specifically, we propose \u201cWasserstein Quantum Monte\n                             Carlo\u201d (WQMC), which uses the gradient flow induced by the Wasserstein metric,\n                             rather than Fisher\u2013Rao metric, and corresponds to transporting the probability\n                             mass, rather than teleporting it. We demonstrate empirically that the dynamics of\n                             WQMC results in faster convergence to the ground state of molecular systems.\n                    1   Introduction\n                    Access to the wave function of a quantum many-body system allows us to study strongly correlated\n                    quantum matter, starting from the fundamental building blocks. For example, the solution of the time-\n                    independent electronic Schr\u00f6dinger equation provides all the chemical properties of a given atomic\n                    state, which have numerous applications in chemistry and materials design. However, obtaining\n                    the exact wave function is fundamentally challenging, with a complexity scaling exponentially with\n                    the number of degrees of freedom. Various computational techniques have been developed in the\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Wasserstein Quantum Monte Carlo\n\n# Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\u00f6dinger Equation\n\narXiv: 2307.07050v3 [physics.comp-ph] 26 Oct 2023\n\nAuthors: Kirill Neklyudov, Jannes Nys, Luca Thiede, Juan Felipe Carrasquilla, Qiang Liu, Max Welling, Alireza Makhzani\n\nAffiliations: Vector Institute, Institute of Physics & Center for Quantum Science and Engineering, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), University of Toronto, UT Austin, Microsoft Research, University of Waterloo, AI4Science\n\n## Abstract\n\nSolving the quantum many-body Schr\u00f6dinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher\u2013Rao gradient flow in this distributional space, followed by a projection step onto the variational manifold. This perspective provides us with a principled framework to derive new QMC algorithms, by endowing the distributional space with better metrics, and following the projected gradient flow induced by those metrics. More specifically, we propose \"Wasserstein Quantum Monte Carlo\" (WQMC), which uses the gradient flow induced by the Wasserstein metric, rather than Fisher\u2013Rao metric, and corresponds to transporting the probability mass, rather than teleporting it. We demonstrate empirically that the dynamics of WQMC results in faster convergence to the ground state of molecular systems.\n\n## Introduction\n\nAccess to the wave function of a quantum many-body system allows us to study strongly correlated quantum matter, starting from the fundamental building blocks. For example, the solution of the time-independent electronic Schr\u00f6dinger equation provides all the chemical properties of a given atomic state, which have numerous applications in chemistry and materials design. However, obtaining the exact wave function is fundamentally challenging, with a complexity scaling exponentially with the number of degrees of freedom. Various computational techniques have been developed in the 37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Wasserstein Quantum Monte Carlo", "md": "# Wasserstein Quantum Monte Carlo"}, {"type": "heading", "lvl": 1, "value": "Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\u00f6dinger Equation", "md": "# Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\u00f6dinger Equation"}, {"type": "text", "value": "arXiv: 2307.07050v3 [physics.comp-ph] 26 Oct 2023\n\nAuthors: Kirill Neklyudov, Jannes Nys, Luca Thiede, Juan Felipe Carrasquilla, Qiang Liu, Max Welling, Alireza Makhzani\n\nAffiliations: Vector Institute, Institute of Physics & Center for Quantum Science and Engineering, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), University of Toronto, UT Austin, Microsoft Research, University of Waterloo, AI4Science", "md": "arXiv: 2307.07050v3 [physics.comp-ph] 26 Oct 2023\n\nAuthors: Kirill Neklyudov, Jannes Nys, Luca Thiede, Juan Felipe Carrasquilla, Qiang Liu, Max Welling, Alireza Makhzani\n\nAffiliations: Vector Institute, Institute of Physics & Center for Quantum Science and Engineering, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), University of Toronto, UT Austin, Microsoft Research, University of Waterloo, AI4Science"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Solving the quantum many-body Schr\u00f6dinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher\u2013Rao gradient flow in this distributional space, followed by a projection step onto the variational manifold. This perspective provides us with a principled framework to derive new QMC algorithms, by endowing the distributional space with better metrics, and following the projected gradient flow induced by those metrics. More specifically, we propose \"Wasserstein Quantum Monte Carlo\" (WQMC), which uses the gradient flow induced by the Wasserstein metric, rather than Fisher\u2013Rao metric, and corresponds to transporting the probability mass, rather than teleporting it. We demonstrate empirically that the dynamics of WQMC results in faster convergence to the ground state of molecular systems.", "md": "Solving the quantum many-body Schr\u00f6dinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher\u2013Rao gradient flow in this distributional space, followed by a projection step onto the variational manifold. This perspective provides us with a principled framework to derive new QMC algorithms, by endowing the distributional space with better metrics, and following the projected gradient flow induced by those metrics. More specifically, we propose \"Wasserstein Quantum Monte Carlo\" (WQMC), which uses the gradient flow induced by the Wasserstein metric, rather than Fisher\u2013Rao metric, and corresponds to transporting the probability mass, rather than teleporting it. We demonstrate empirically that the dynamics of WQMC results in faster convergence to the ground state of molecular systems."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Access to the wave function of a quantum many-body system allows us to study strongly correlated quantum matter, starting from the fundamental building blocks. For example, the solution of the time-independent electronic Schr\u00f6dinger equation provides all the chemical properties of a given atomic state, which have numerous applications in chemistry and materials design. However, obtaining the exact wave function is fundamentally challenging, with a complexity scaling exponentially with the number of degrees of freedom. Various computational techniques have been developed in the 37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Access to the wave function of a quantum many-body system allows us to study strongly correlated quantum matter, starting from the fundamental building blocks. For example, the solution of the time-independent electronic Schr\u00f6dinger equation provides all the chemical properties of a given atomic state, which have numerous applications in chemistry and materials design. However, obtaining the exact wave function is fundamentally challenging, with a complexity scaling exponentially with the number of degrees of freedom. Various computational techniques have been developed in the 37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "past, including compression techniques based on Tensor Networks (White, 1992), and stochastic\napproaches such as Quantum Monte Carlo (QMC) (Ceperley et al., 1977). Quantum Variational\nMonte Carlo (QVMC) (McMillan, 1965; Ceperley et al., 1977) is a well-known subclass of the latter\nthat can, in principle, be used to estimate the lowest-energy state (i.e. ground state) of a quantum\nmany-body system. The method operates by parameterizing the trial wave function and minimizing\nthe energy of the many-body system w.r.t. the model parameters.\nThe choice of parametric family of the trial wave function is a crucial component of the QVMC\nframework. Naturally, deep neural networks, being a family of universal approximators, have\ndemonstrated promising results for quantum systems with discrete (Carleo & Troyer, 2017; Choo\net al., 2020; Hibat-Allah et al., 2020), as well as continuous degrees of freedom (Pfau et al., 2020;\nHermann et al., 2020; Pescia et al., 2022; Gnech et al., 2022; von Glehn et al., 2022). However, the\noptimization process is challenging, especially for rich parametric families of the trial wave functions.\nThis requires the use of advanced optimization techniques that take into account the geometry of\nthe parametric manifold. The most common technique used in QVMC is referred to as \u2018Stochastic\nReconfiguration\u2019 (SR) (Sorella, 1998), and can be seen as the quantum version of Natural Gradient\nDescent (Stokes et al., 2020). While for large neural networks with up to millions of parameters,\nefficient and scalable implementations of SR are available (Vicentini et al., 2022), it is also possible\nto use approximate methods such as K-FAC (Martens & Grosse, 2015; Pfau et al., 2020). Higher\norder optimization techniques are considered to be essential to obtain the necessary optimization\nperformance to accurately estimate ground states of quantum many-body Hamiltonians (see e.g.\n(Pescia et al., 2023; Pfau et al., 2020)). Therefore, studies of the optimization procedure are an\nimportant direction for further development of the QVMC approach.\nIn this paper, we consider the energy minimization dynamics as a gradient flow on the non-parametric\nmanifold of distributions. First, as an example of the proposed methodology, we demonstrate that the\nimaginary-time Schr\u00f6dinger equation can be described as the gradient flow under the Fisher\u2013Rao\nmetric on the non-parametric manifold. Then, the QVMC algorithm can be seen as a projection of\nthis gradient flow onto a parametric manifold (see Section 3 for details). Second, the gradient flow\nperspective gives us an additional degree of freedom in the algorithm. Namely, we can choose the\nmetric under which we define the gradient flow. Thus, we propose and study a different energy-\nminimizing objective function, which we derive as a gradient flow under the Wasserstein metric (or\nWasserstein Fisher\u2013Rao metric) (Chizat et al., 2018; Kondratyev et al., 2016).\nIn practice, we demonstrate that incorporating the Wasserstein metric into the optimization procedure\nallows for faster convergence to the ground state. Namely, we demonstrate up to 10 times faster\nconvergence of the variance of the local energy for chemical systems. Intuitively, incorporating\nthe Wasserstein metric regularizes the density evolution by forbidding or regularizing non-local\nprobability mass \u201cteleportation\u201d (as done by Fisher\u2013Rao metric). This might facilitate faster mixing\nof the MCMC running along with the density updates.\n2    Background\n2.1   Quantum variational Monte Carlo\nConsider a quantum many-body system subject to the Hamiltonian operator, which we will assume\nto be of the following form,\n                                           H = \u22121  2\u22072 x + V.                                         (1)\nwhere x a given many-body configuration and V is the potential operator. The time-dependent\nSchr\u00f6dinger equation determines the wave function \u03c8(x, t) of the quantum system\n                                        i d                                                           (2)\n                                         dt\u03c8(x, t) = H\u03c8(x, t)\nAs is often the case, we will target the stationary solutions, for which we focus on solving the\ntime-independent Schr\u00f6dinger equation\n                                           H\u03c8(x) = E\u03c8(x)                                              (3)\nwhere E is the energy of the state \u03c8. The ground state of a quantum system is obtained by solving\nthe time-independent Schr\u00f6dinger equation, by targeting the eigenstate \u03c8 of the above Hamiltonian\n                                                    2", "md": "past, including compression techniques based on Tensor Networks (White, 1992), and stochastic\napproaches such as Quantum Monte Carlo (QMC) (Ceperley et al., 1977). Quantum Variational\nMonte Carlo (QVMC) (McMillan, 1965; Ceperley et al., 1977) is a well-known subclass of the latter\nthat can, in principle, be used to estimate the lowest-energy state (i.e. ground state) of a quantum\nmany-body system. The method operates by parameterizing the trial wave function and minimizing\nthe energy of the many-body system w.r.t. the model parameters.\n\nThe choice of parametric family of the trial wave function is a crucial component of the QVMC\nframework. Naturally, deep neural networks, being a family of universal approximators, have\ndemonstrated promising results for quantum systems with discrete (Carleo & Troyer, 2017; Choo\net al., 2020; Hibat-Allah et al., 2020), as well as continuous degrees of freedom (Pfau et al., 2020;\nHermann et al., 2020; Pescia et al., 2022; Gnech et al., 2022; von Glehn et al., 2022). However, the\noptimization process is challenging, especially for rich parametric families of the trial wave functions.\nThis requires the use of advanced optimization techniques that take into account the geometry of\nthe parametric manifold. The most common technique used in QVMC is referred to as \u2018Stochastic\nReconfiguration\u2019 (SR) (Sorella, 1998), and can be seen as the quantum version of Natural Gradient\nDescent (Stokes et al., 2020). While for large neural networks with up to millions of parameters,\nefficient and scalable implementations of SR are available (Vicentini et al., 2022), it is also possible\nto use approximate methods such as K-FAC (Martens & Grosse, 2015; Pfau et al., 2020). Higher\norder optimization techniques are considered to be essential to obtain the necessary optimization\nperformance to accurately estimate ground states of quantum many-body Hamiltonians (see e.g.\n(Pescia et al., 2023; Pfau et al., 2020)). Therefore, studies of the optimization procedure are an\nimportant direction for further development of the QVMC approach.\n\nIn this paper, we consider the energy minimization dynamics as a gradient flow on the non-parametric\nmanifold of distributions. First, as an example of the proposed methodology, we demonstrate that the\nimaginary-time Schr\u00f6dinger equation can be described as the gradient flow under the Fisher\u2013Rao\nmetric on the non-parametric manifold. Then, the QVMC algorithm can be seen as a projection of\nthis gradient flow onto a parametric manifold (see Section 3 for details). Second, the gradient flow\nperspective gives us an additional degree of freedom in the algorithm. Namely, we can choose the\nmetric under which we define the gradient flow. Thus, we propose and study a different energy-\nminimizing objective function, which we derive as a gradient flow under the Wasserstein metric (or\nWasserstein Fisher\u2013Rao metric) (Chizat et al., 2018; Kondratyev et al., 2016).\n\nIn practice, we demonstrate that incorporating the Wasserstein metric into the optimization procedure\nallows for faster convergence to the ground state. Namely, we demonstrate up to 10 times faster\nconvergence of the variance of the local energy for chemical systems. Intuitively, incorporating\nthe Wasserstein metric regularizes the density evolution by forbidding or regularizing non-local\nprobability mass \u201cteleportation\u201d (as done by Fisher\u2013Rao metric). This might facilitate faster mixing\nof the MCMC running along with the density updates.\n\n## Background\n\n### Quantum variational Monte Carlo\n\nConsider a quantum many-body system subject to the Hamiltonian operator, which we will assume\nto be of the following form,\n\n$$\nH = -\\frac{1}{2}\\nabla^2 x + V. \\quad (1)\n$$\n\nwhere x a given many-body configuration and V is the potential operator. The time-dependent\nSchr\u00f6dinger equation determines the wave function \u03c8(x, t) of the quantum system\n\n$$\ni \\frac{d}{dt}\\psi(x, t) = H\\psi(x, t) \\quad (2)\n$$\n\nAs is often the case, we will target the stationary solutions, for which we focus on solving the\ntime-independent Schr\u00f6dinger equation\n\n$$\nH\\psi(x) = E\\psi(x) \\quad (3)\n$$\n\nwhere E is the energy of the state \u03c8. The ground state of a quantum system is obtained by solving\nthe time-independent Schr\u00f6dinger equation, by targeting the eigenstate \u03c8 of the above Hamiltonian.", "images": [], "items": [{"type": "text", "value": "past, including compression techniques based on Tensor Networks (White, 1992), and stochastic\napproaches such as Quantum Monte Carlo (QMC) (Ceperley et al., 1977). Quantum Variational\nMonte Carlo (QVMC) (McMillan, 1965; Ceperley et al., 1977) is a well-known subclass of the latter\nthat can, in principle, be used to estimate the lowest-energy state (i.e. ground state) of a quantum\nmany-body system. The method operates by parameterizing the trial wave function and minimizing\nthe energy of the many-body system w.r.t. the model parameters.\n\nThe choice of parametric family of the trial wave function is a crucial component of the QVMC\nframework. Naturally, deep neural networks, being a family of universal approximators, have\ndemonstrated promising results for quantum systems with discrete (Carleo & Troyer, 2017; Choo\net al., 2020; Hibat-Allah et al., 2020), as well as continuous degrees of freedom (Pfau et al., 2020;\nHermann et al., 2020; Pescia et al., 2022; Gnech et al., 2022; von Glehn et al., 2022). However, the\noptimization process is challenging, especially for rich parametric families of the trial wave functions.\nThis requires the use of advanced optimization techniques that take into account the geometry of\nthe parametric manifold. The most common technique used in QVMC is referred to as \u2018Stochastic\nReconfiguration\u2019 (SR) (Sorella, 1998), and can be seen as the quantum version of Natural Gradient\nDescent (Stokes et al., 2020). While for large neural networks with up to millions of parameters,\nefficient and scalable implementations of SR are available (Vicentini et al., 2022), it is also possible\nto use approximate methods such as K-FAC (Martens & Grosse, 2015; Pfau et al., 2020). Higher\norder optimization techniques are considered to be essential to obtain the necessary optimization\nperformance to accurately estimate ground states of quantum many-body Hamiltonians (see e.g.\n(Pescia et al., 2023; Pfau et al., 2020)). Therefore, studies of the optimization procedure are an\nimportant direction for further development of the QVMC approach.\n\nIn this paper, we consider the energy minimization dynamics as a gradient flow on the non-parametric\nmanifold of distributions. First, as an example of the proposed methodology, we demonstrate that the\nimaginary-time Schr\u00f6dinger equation can be described as the gradient flow under the Fisher\u2013Rao\nmetric on the non-parametric manifold. Then, the QVMC algorithm can be seen as a projection of\nthis gradient flow onto a parametric manifold (see Section 3 for details). Second, the gradient flow\nperspective gives us an additional degree of freedom in the algorithm. Namely, we can choose the\nmetric under which we define the gradient flow. Thus, we propose and study a different energy-\nminimizing objective function, which we derive as a gradient flow under the Wasserstein metric (or\nWasserstein Fisher\u2013Rao metric) (Chizat et al., 2018; Kondratyev et al., 2016).\n\nIn practice, we demonstrate that incorporating the Wasserstein metric into the optimization procedure\nallows for faster convergence to the ground state. Namely, we demonstrate up to 10 times faster\nconvergence of the variance of the local energy for chemical systems. Intuitively, incorporating\nthe Wasserstein metric regularizes the density evolution by forbidding or regularizing non-local\nprobability mass \u201cteleportation\u201d (as done by Fisher\u2013Rao metric). This might facilitate faster mixing\nof the MCMC running along with the density updates.", "md": "past, including compression techniques based on Tensor Networks (White, 1992), and stochastic\napproaches such as Quantum Monte Carlo (QMC) (Ceperley et al., 1977). Quantum Variational\nMonte Carlo (QVMC) (McMillan, 1965; Ceperley et al., 1977) is a well-known subclass of the latter\nthat can, in principle, be used to estimate the lowest-energy state (i.e. ground state) of a quantum\nmany-body system. The method operates by parameterizing the trial wave function and minimizing\nthe energy of the many-body system w.r.t. the model parameters.\n\nThe choice of parametric family of the trial wave function is a crucial component of the QVMC\nframework. Naturally, deep neural networks, being a family of universal approximators, have\ndemonstrated promising results for quantum systems with discrete (Carleo & Troyer, 2017; Choo\net al., 2020; Hibat-Allah et al., 2020), as well as continuous degrees of freedom (Pfau et al., 2020;\nHermann et al., 2020; Pescia et al., 2022; Gnech et al., 2022; von Glehn et al., 2022). However, the\noptimization process is challenging, especially for rich parametric families of the trial wave functions.\nThis requires the use of advanced optimization techniques that take into account the geometry of\nthe parametric manifold. The most common technique used in QVMC is referred to as \u2018Stochastic\nReconfiguration\u2019 (SR) (Sorella, 1998), and can be seen as the quantum version of Natural Gradient\nDescent (Stokes et al., 2020). While for large neural networks with up to millions of parameters,\nefficient and scalable implementations of SR are available (Vicentini et al., 2022), it is also possible\nto use approximate methods such as K-FAC (Martens & Grosse, 2015; Pfau et al., 2020). Higher\norder optimization techniques are considered to be essential to obtain the necessary optimization\nperformance to accurately estimate ground states of quantum many-body Hamiltonians (see e.g.\n(Pescia et al., 2023; Pfau et al., 2020)). Therefore, studies of the optimization procedure are an\nimportant direction for further development of the QVMC approach.\n\nIn this paper, we consider the energy minimization dynamics as a gradient flow on the non-parametric\nmanifold of distributions. First, as an example of the proposed methodology, we demonstrate that the\nimaginary-time Schr\u00f6dinger equation can be described as the gradient flow under the Fisher\u2013Rao\nmetric on the non-parametric manifold. Then, the QVMC algorithm can be seen as a projection of\nthis gradient flow onto a parametric manifold (see Section 3 for details). Second, the gradient flow\nperspective gives us an additional degree of freedom in the algorithm. Namely, we can choose the\nmetric under which we define the gradient flow. Thus, we propose and study a different energy-\nminimizing objective function, which we derive as a gradient flow under the Wasserstein metric (or\nWasserstein Fisher\u2013Rao metric) (Chizat et al., 2018; Kondratyev et al., 2016).\n\nIn practice, we demonstrate that incorporating the Wasserstein metric into the optimization procedure\nallows for faster convergence to the ground state. Namely, we demonstrate up to 10 times faster\nconvergence of the variance of the local energy for chemical systems. Intuitively, incorporating\nthe Wasserstein metric regularizes the density evolution by forbidding or regularizing non-local\nprobability mass \u201cteleportation\u201d (as done by Fisher\u2013Rao metric). This might facilitate faster mixing\nof the MCMC running along with the density updates."}, {"type": "heading", "lvl": 2, "value": "Background", "md": "## Background"}, {"type": "heading", "lvl": 3, "value": "Quantum variational Monte Carlo", "md": "### Quantum variational Monte Carlo"}, {"type": "text", "value": "Consider a quantum many-body system subject to the Hamiltonian operator, which we will assume\nto be of the following form,\n\n$$\nH = -\\frac{1}{2}\\nabla^2 x + V. \\quad (1)\n$$\n\nwhere x a given many-body configuration and V is the potential operator. The time-dependent\nSchr\u00f6dinger equation determines the wave function \u03c8(x, t) of the quantum system\n\n$$\ni \\frac{d}{dt}\\psi(x, t) = H\\psi(x, t) \\quad (2)\n$$\n\nAs is often the case, we will target the stationary solutions, for which we focus on solving the\ntime-independent Schr\u00f6dinger equation\n\n$$\nH\\psi(x) = E\\psi(x) \\quad (3)\n$$\n\nwhere E is the energy of the state \u03c8. The ground state of a quantum system is obtained by solving\nthe time-independent Schr\u00f6dinger equation, by targeting the eigenstate \u03c8 of the above Hamiltonian.", "md": "Consider a quantum many-body system subject to the Hamiltonian operator, which we will assume\nto be of the following form,\n\n$$\nH = -\\frac{1}{2}\\nabla^2 x + V. \\quad (1)\n$$\n\nwhere x a given many-body configuration and V is the potential operator. The time-dependent\nSchr\u00f6dinger equation determines the wave function \u03c8(x, t) of the quantum system\n\n$$\ni \\frac{d}{dt}\\psi(x, t) = H\\psi(x, t) \\quad (2)\n$$\n\nAs is often the case, we will target the stationary solutions, for which we focus on solving the\ntime-independent Schr\u00f6dinger equation\n\n$$\nH\\psi(x) = E\\psi(x) \\quad (3)\n$$\n\nwhere E is the energy of the state \u03c8. The ground state of a quantum system is obtained by solving\nthe time-independent Schr\u00f6dinger equation, by targeting the eigenstate \u03c8 of the above Hamiltonian."}]}, {"page": 3, "text": " with the lowest eigenvalue (energy) E. Hereby, we must restrict the Hilbert space to wave functions\n that are antisymmetric under particle permutations in the case of fermionic particles, and symmetric\n for bosons. The latter takes into account the indistinguishability of the particles. Given the Born\n density q(x) = |\u03c8(x)|2, the energy of a given quantum state can be rewritten in a functional form,\n                                       E[\u03c8] = Eq(x)[Eloc(x)],               Eloc(x) := [H\u03c8](x)  \u03c8(x)                                    (4)\nWe will focus on the case where the Hamiltonian operator is Hermitian and time-reversal symmetric.\n In this case, its eigenfunctions and eigenvalues are real, and the energy can be recast into a functional\n of the Born probability density (see also Pfau et al. (2020), where the expressions are given in terms\n of log|\u03c8|)    E[q] = Eq(x)[Eloc(x)],               Eloc(x) = V (x) \u2212           1    x log q(x) \u2212       1                               (5)\n                                                                                4\u22072                     8\u2225\u2207x log q(x)\u22252,\n under the strong condition that q(x) is the Born probability density derived from an (anti-)symmetric\n wave function: q(x) = \u03c82(x). The latter will always be tacitly assumed from hereon.\n The Rayleigh\u2013Ritz principle guarantees that the E[q] is lower-bounded by the true ground-state\n energy of the system, i.e. E[q] \u2265                 E0, if the corresponding wave function \u03c8 is a valid state of the\n corresponding Hilbert space. Quantum Variational Monte Carlo (QVMC) targets ground states by\n parametrizing the wavefunction \u03c8(x, \u03b8) and by minimizing E[q(\u03b8)]. The solution to the minimiza-\n tion problem \u03b80 = arg min                \u03b8 E[q(\u03b8)] is obtained by gradient-based methods using the following\n expression for the gradient w.r.t. parameters \u03b8\n                   \u2207\u03b8E[q(\u03b8)] = Eq(x,\u03b8)                 Eloc(x, \u03b8) \u2212      Eq(x,\u03b8)[Eloc(x, \u03b8)]          \u2207\u03b8 log q(x, \u03b8)         .          (6)\n In sum, the above leads to an iterative procedure in which Monte Carlo sampling is used to generate\n configurations from the current trial state q(x, \u03b8) = \u03c82(x, \u03b8), which allows computing the corre-\n sponding energy and its parameter gradients, and to update the model accordingly. In practice, the\n parametric model specifies the density q(x, \u03b8) only up to a normalization constant, i.e., it outputs\n \u02dc\n q(x, \u03b8) \u221d      q(x, \u03b8). However, the gradient w.r.t. \u03b8 does not depend on the normalization constant;\n hence, throughout the paper, we refer to the model as the normalized density q(x, \u03b8).\n 2.2     Gradient flows under the Wasserstein Fisher\u2013Rao metric\n In the previous section, we introduced QVMC in terms of Born probability functions and formulated\n the problem as the minimization of a functional of probability functions constrained to a varia-\n tional/parametric manifold. The latter is a more common problem often tackled in machine learning,\n and by forging connections between both fields, we will be able to derive an alternative to QVMC.\n Gradient Flows              In Euclidean space, we can minimize a function f : Rd \u2192                                R by following the\n ODE d   dtxt = \u2212\u2207xf(xt), which can be viewed as the continuous version of standard gradient descent.\n Similarly, we can minimize a functional in the space of probability distributions (or in general any\n Riemannian manifold), by following an ODE on this manifold. However the notion of a gradient on a\n manifold is more complicated, and relies on the Riemannian metric that the manifold is endowed\n with. Different Riemannian metrics result in different gradient flows, and consequently different\n optimization dynamics. For a thorough analysis of gradient flows, we refer the reader to Ambrosio\n et al. (2005).\n Wasserstein Fisher\u2013Rao gradient flows                         Consider the space of distributions P2 with finite second\n moment. This space can be endowed with a Wasserstein Fisher\u2013Rao metric with the corresponding\n distance. In particular, the Wasserstein Fisher\u2013Rao (WFR) distance (Chizat et al., 2018) is defined\n by extending the Benamou & Brenier (2000) dynamical optimal transport formulation by a term\n involving the norm of the growth rate gt, and by accounting for the growth term in the modified\n continuity equation. Namely, the distance between probability densities p0 and p1 is defined as\n    WFR\u03bb(p0, p1)2 :=               inf        1   Eqt(x)    \u2225vt(x)\u22252 + \u03bbgt(x)2              dt,     subj. to                            (7)\n                                 vt,gt,qt    0\n                 \u2202qt(x)\n                    \u2202t      = \u2212\u2207x \u00b7 (qt(x)vt(x)) + gt(x)qt(x) ,                        and q0(x) = p0(x), q1(x) = p1(x) ,\n                                                                       3", "md": "With the lowest eigenvalue (energy) E. Hereby, we must restrict the Hilbert space to wave functions\nthat are antisymmetric under particle permutations in the case of fermionic particles, and symmetric\nfor bosons. The latter takes into account the indistinguishability of the particles. Given the Born\ndensity \\( q(x) = |\\psi(x)|^2 \\), the energy of a given quantum state can be rewritten in a functional form,\n\n\\[\nE[\\psi] = E_q(x)[E_{\\text{loc}}(x)], \\quad E_{\\text{loc}}(x) := [H\\psi](x) \\psi(x) \\quad (4)\n\\]\n\nWe will focus on the case where the Hamiltonian operator is Hermitian and time-reversal symmetric.\nIn this case, its eigenfunctions and eigenvalues are real, and the energy can be recast into a functional\nof the Born probability density (see also Pfau et al. (2020), where the expressions are given in terms\nof \\( \\log|\\psi| \\)\n\n\\[\nE[q] = E_q(x)[E_{\\text{loc}}(x)], \\quad E_{\\text{loc}}(x) = V(x) - \\frac{1}{4} \\nabla^2 \\log q(x) - \\frac{1}{8} \\|\\nabla x \\log q(x)\\|^2, \\quad (5)\n\\]\n\nunder the strong condition that \\( q(x) \\) is the Born probability density derived from an (anti-)symmetric\nwave function: \\( q(x) = \\psi^2(x) \\). The latter will always be tacitly assumed from hereon.\n\nThe Rayleigh\u2013Ritz principle guarantees that the \\( E[q] \\) is lower-bounded by the true ground-state\nenergy of the system, i.e. \\( E[q] \\geq E_0 \\), if the corresponding wave function \\( \\psi \\) is a valid state of the\ncorresponding Hilbert space. Quantum Variational Monte Carlo (QVMC) targets ground states by\nparametrizing the wavefunction \\( \\psi(x, \\theta) \\) and by minimizing \\( E[q(\\theta)] \\). The solution to the minimization\nproblem \\( \\theta_0 = \\arg \\min_{\\theta} E[q(\\theta)] \\) is obtained by gradient-based methods using the following\nexpression for the gradient w.r.t. parameters \\( \\theta \\)\n\n\\[\n\\nabla_{\\theta} E[q(\\theta)] = E_q(x, \\theta) E_{\\text{loc}}(x, \\theta) - E_q(x, \\theta)[E_{\\text{loc}}(x, \\theta)] \\nabla_{\\theta} \\log q(x, \\theta) \\quad (6)\n\\]\n\nIn sum, the above leads to an iterative procedure in which Monte Carlo sampling is used to generate\nconfigurations from the current trial state \\( q(x, \\theta) = \\psi^2(x, \\theta) \\), which allows computing the corresponding energy and its parameter gradients, and to update the model accordingly. In practice, the parametric model specifies the density \\( q(x, \\theta) \\) only up to a normalization constant, i.e., it outputs \\( \\tilde{q}(x, \\theta) \\propto q(x, \\theta) \\). However, the gradient w.r.t. \\( \\theta \\) does not depend on the normalization constant; hence, throughout the paper, we refer to the model as the normalized density \\( q(x, \\theta) \\).\n\n## 2.2 Gradient flows under the Wasserstein Fisher\u2013Rao metric\n\nIn the previous section, we introduced QVMC in terms of Born probability functions and formulated\nthe problem as the minimization of a functional of probability functions constrained to a variational/parametric manifold. The latter is a more common problem often tackled in machine learning,\nand by forging connections between both fields, we will be able to derive an alternative to QVMC.\n\nGradient Flows In Euclidean space, we can minimize a function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) by following the ODE \\( \\frac{d}{dt}x_t = -\\nabla_x f(x_t) \\), which can be viewed as the continuous version of standard gradient descent. Similarly, we can minimize a functional in the space of probability distributions (or in general any Riemannian manifold), by following an ODE on this manifold. However, the notion of a gradient on a manifold is more complicated, and relies on the Riemannian metric that the manifold is endowed with. Different Riemannian metrics result in different gradient flows, and consequently different optimization dynamics. For a thorough analysis of gradient flows, we refer the reader to Ambrosio et al. (2005).\n\nWasserstein Fisher\u2013Rao gradient flows Consider the space of distributions \\( \\mathcal{P}_2 \\) with finite second moment. This space can be endowed with a Wasserstein Fisher\u2013Rao metric with the corresponding distance. In particular, the Wasserstein Fisher\u2013Rao (WFR) distance (Chizat et al., 2018) is defined by extending the Benamou & Brenier (2000) dynamical optimal transport formulation by a term involving the norm of the growth rate \\( g_t \\), and by accounting for the growth term in the modified continuity equation. Namely, the distance between probability densities \\( p_0 \\) and \\( p_1 \\) is defined as\n\n\\[\nWFR_{\\lambda}(p_0, p_1)^2 := \\inf_{v_t, g_t, q_t \\geq 0} \\int_0^1 E_{q_t}(x) \\|\\mathbf{v}_t(x)\\|^2 + \\lambda g_t(x)^2 dt, \\quad \\text{subj. to}\n\\]\n\n\\[\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x) \\mathbf{v}_t(x)) + g_t(x)q_t(x), \\quad \\text{and} \\quad q_0(x) = p_0(x), q_1(x) = p_1(x) \\quad (7)\n\\]", "images": [], "items": [{"type": "text", "value": "With the lowest eigenvalue (energy) E. Hereby, we must restrict the Hilbert space to wave functions\nthat are antisymmetric under particle permutations in the case of fermionic particles, and symmetric\nfor bosons. The latter takes into account the indistinguishability of the particles. Given the Born\ndensity \\( q(x) = |\\psi(x)|^2 \\), the energy of a given quantum state can be rewritten in a functional form,\n\n\\[\nE[\\psi] = E_q(x)[E_{\\text{loc}}(x)], \\quad E_{\\text{loc}}(x) := [H\\psi](x) \\psi(x) \\quad (4)\n\\]\n\nWe will focus on the case where the Hamiltonian operator is Hermitian and time-reversal symmetric.\nIn this case, its eigenfunctions and eigenvalues are real, and the energy can be recast into a functional\nof the Born probability density (see also Pfau et al. (2020), where the expressions are given in terms\nof \\( \\log|\\psi| \\)\n\n\\[\nE[q] = E_q(x)[E_{\\text{loc}}(x)], \\quad E_{\\text{loc}}(x) = V(x) - \\frac{1}{4} \\nabla^2 \\log q(x) - \\frac{1}{8} \\|\\nabla x \\log q(x)\\|^2, \\quad (5)\n\\]\n\nunder the strong condition that \\( q(x) \\) is the Born probability density derived from an (anti-)symmetric\nwave function: \\( q(x) = \\psi^2(x) \\). The latter will always be tacitly assumed from hereon.\n\nThe Rayleigh\u2013Ritz principle guarantees that the \\( E[q] \\) is lower-bounded by the true ground-state\nenergy of the system, i.e. \\( E[q] \\geq E_0 \\), if the corresponding wave function \\( \\psi \\) is a valid state of the\ncorresponding Hilbert space. Quantum Variational Monte Carlo (QVMC) targets ground states by\nparametrizing the wavefunction \\( \\psi(x, \\theta) \\) and by minimizing \\( E[q(\\theta)] \\). The solution to the minimization\nproblem \\( \\theta_0 = \\arg \\min_{\\theta} E[q(\\theta)] \\) is obtained by gradient-based methods using the following\nexpression for the gradient w.r.t. parameters \\( \\theta \\)\n\n\\[\n\\nabla_{\\theta} E[q(\\theta)] = E_q(x, \\theta) E_{\\text{loc}}(x, \\theta) - E_q(x, \\theta)[E_{\\text{loc}}(x, \\theta)] \\nabla_{\\theta} \\log q(x, \\theta) \\quad (6)\n\\]\n\nIn sum, the above leads to an iterative procedure in which Monte Carlo sampling is used to generate\nconfigurations from the current trial state \\( q(x, \\theta) = \\psi^2(x, \\theta) \\), which allows computing the corresponding energy and its parameter gradients, and to update the model accordingly. In practice, the parametric model specifies the density \\( q(x, \\theta) \\) only up to a normalization constant, i.e., it outputs \\( \\tilde{q}(x, \\theta) \\propto q(x, \\theta) \\). However, the gradient w.r.t. \\( \\theta \\) does not depend on the normalization constant; hence, throughout the paper, we refer to the model as the normalized density \\( q(x, \\theta) \\).", "md": "With the lowest eigenvalue (energy) E. Hereby, we must restrict the Hilbert space to wave functions\nthat are antisymmetric under particle permutations in the case of fermionic particles, and symmetric\nfor bosons. The latter takes into account the indistinguishability of the particles. Given the Born\ndensity \\( q(x) = |\\psi(x)|^2 \\), the energy of a given quantum state can be rewritten in a functional form,\n\n\\[\nE[\\psi] = E_q(x)[E_{\\text{loc}}(x)], \\quad E_{\\text{loc}}(x) := [H\\psi](x) \\psi(x) \\quad (4)\n\\]\n\nWe will focus on the case where the Hamiltonian operator is Hermitian and time-reversal symmetric.\nIn this case, its eigenfunctions and eigenvalues are real, and the energy can be recast into a functional\nof the Born probability density (see also Pfau et al. (2020), where the expressions are given in terms\nof \\( \\log|\\psi| \\)\n\n\\[\nE[q] = E_q(x)[E_{\\text{loc}}(x)], \\quad E_{\\text{loc}}(x) = V(x) - \\frac{1}{4} \\nabla^2 \\log q(x) - \\frac{1}{8} \\|\\nabla x \\log q(x)\\|^2, \\quad (5)\n\\]\n\nunder the strong condition that \\( q(x) \\) is the Born probability density derived from an (anti-)symmetric\nwave function: \\( q(x) = \\psi^2(x) \\). The latter will always be tacitly assumed from hereon.\n\nThe Rayleigh\u2013Ritz principle guarantees that the \\( E[q] \\) is lower-bounded by the true ground-state\nenergy of the system, i.e. \\( E[q] \\geq E_0 \\), if the corresponding wave function \\( \\psi \\) is a valid state of the\ncorresponding Hilbert space. Quantum Variational Monte Carlo (QVMC) targets ground states by\nparametrizing the wavefunction \\( \\psi(x, \\theta) \\) and by minimizing \\( E[q(\\theta)] \\). The solution to the minimization\nproblem \\( \\theta_0 = \\arg \\min_{\\theta} E[q(\\theta)] \\) is obtained by gradient-based methods using the following\nexpression for the gradient w.r.t. parameters \\( \\theta \\)\n\n\\[\n\\nabla_{\\theta} E[q(\\theta)] = E_q(x, \\theta) E_{\\text{loc}}(x, \\theta) - E_q(x, \\theta)[E_{\\text{loc}}(x, \\theta)] \\nabla_{\\theta} \\log q(x, \\theta) \\quad (6)\n\\]\n\nIn sum, the above leads to an iterative procedure in which Monte Carlo sampling is used to generate\nconfigurations from the current trial state \\( q(x, \\theta) = \\psi^2(x, \\theta) \\), which allows computing the corresponding energy and its parameter gradients, and to update the model accordingly. In practice, the parametric model specifies the density \\( q(x, \\theta) \\) only up to a normalization constant, i.e., it outputs \\( \\tilde{q}(x, \\theta) \\propto q(x, \\theta) \\). However, the gradient w.r.t. \\( \\theta \\) does not depend on the normalization constant; hence, throughout the paper, we refer to the model as the normalized density \\( q(x, \\theta) \\)."}, {"type": "heading", "lvl": 2, "value": "2.2 Gradient flows under the Wasserstein Fisher\u2013Rao metric", "md": "## 2.2 Gradient flows under the Wasserstein Fisher\u2013Rao metric"}, {"type": "text", "value": "In the previous section, we introduced QVMC in terms of Born probability functions and formulated\nthe problem as the minimization of a functional of probability functions constrained to a variational/parametric manifold. The latter is a more common problem often tackled in machine learning,\nand by forging connections between both fields, we will be able to derive an alternative to QVMC.\n\nGradient Flows In Euclidean space, we can minimize a function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) by following the ODE \\( \\frac{d}{dt}x_t = -\\nabla_x f(x_t) \\), which can be viewed as the continuous version of standard gradient descent. Similarly, we can minimize a functional in the space of probability distributions (or in general any Riemannian manifold), by following an ODE on this manifold. However, the notion of a gradient on a manifold is more complicated, and relies on the Riemannian metric that the manifold is endowed with. Different Riemannian metrics result in different gradient flows, and consequently different optimization dynamics. For a thorough analysis of gradient flows, we refer the reader to Ambrosio et al. (2005).\n\nWasserstein Fisher\u2013Rao gradient flows Consider the space of distributions \\( \\mathcal{P}_2 \\) with finite second moment. This space can be endowed with a Wasserstein Fisher\u2013Rao metric with the corresponding distance. In particular, the Wasserstein Fisher\u2013Rao (WFR) distance (Chizat et al., 2018) is defined by extending the Benamou & Brenier (2000) dynamical optimal transport formulation by a term involving the norm of the growth rate \\( g_t \\), and by accounting for the growth term in the modified continuity equation. Namely, the distance between probability densities \\( p_0 \\) and \\( p_1 \\) is defined as\n\n\\[\nWFR_{\\lambda}(p_0, p_1)^2 := \\inf_{v_t, g_t, q_t \\geq 0} \\int_0^1 E_{q_t}(x) \\|\\mathbf{v}_t(x)\\|^2 + \\lambda g_t(x)^2 dt, \\quad \\text{subj. to}\n\\]\n\n\\[\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x) \\mathbf{v}_t(x)) + g_t(x)q_t(x), \\quad \\text{and} \\quad q_0(x) = p_0(x), q_1(x) = p_1(x) \\quad (7)\n\\]", "md": "In the previous section, we introduced QVMC in terms of Born probability functions and formulated\nthe problem as the minimization of a functional of probability functions constrained to a variational/parametric manifold. The latter is a more common problem often tackled in machine learning,\nand by forging connections between both fields, we will be able to derive an alternative to QVMC.\n\nGradient Flows In Euclidean space, we can minimize a function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) by following the ODE \\( \\frac{d}{dt}x_t = -\\nabla_x f(x_t) \\), which can be viewed as the continuous version of standard gradient descent. Similarly, we can minimize a functional in the space of probability distributions (or in general any Riemannian manifold), by following an ODE on this manifold. However, the notion of a gradient on a manifold is more complicated, and relies on the Riemannian metric that the manifold is endowed with. Different Riemannian metrics result in different gradient flows, and consequently different optimization dynamics. For a thorough analysis of gradient flows, we refer the reader to Ambrosio et al. (2005).\n\nWasserstein Fisher\u2013Rao gradient flows Consider the space of distributions \\( \\mathcal{P}_2 \\) with finite second moment. This space can be endowed with a Wasserstein Fisher\u2013Rao metric with the corresponding distance. In particular, the Wasserstein Fisher\u2013Rao (WFR) distance (Chizat et al., 2018) is defined by extending the Benamou & Brenier (2000) dynamical optimal transport formulation by a term involving the norm of the growth rate \\( g_t \\), and by accounting for the growth term in the modified continuity equation. Namely, the distance between probability densities \\( p_0 \\) and \\( p_1 \\) is defined as\n\n\\[\nWFR_{\\lambda}(p_0, p_1)^2 := \\inf_{v_t, g_t, q_t \\geq 0} \\int_0^1 E_{q_t}(x) \\|\\mathbf{v}_t(x)\\|^2 + \\lambda g_t(x)^2 dt, \\quad \\text{subj. to}\n\\]\n\n\\[\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x) \\mathbf{v}_t(x)) + g_t(x)q_t(x), \\quad \\text{and} \\quad q_0(x) = p_0(x), q_1(x) = p_1(x) \\quad (7)\n\\]"}]}, {"page": 4, "text": " where vt(x) is the vector field defining the probability flow, gt(x) is the growth term controlling the\n creation and annihilation of the probability mass, and \u03bb is the coefficient balancing the transportation\n and teleportation costs. Note that by setting one of the terms to zero we get 2-Wasserstein distance\n (gt(x) \u2261     0) and Fisher\u2013Rao distance (vt(x) \u2261                   0). In Section 3, we also consider the general case of\n c-Wasserstein distance, where c is a convex cost function on the tangent space.\n Given a functional on this manifold, F                 [q] : P2 \u2192       R, we can define the gradient flow of the function\n F under any Riemannian metric including the Wasserstein metric, the Fisher\u2013Rao metric, or the\nWasserstein Fisher\u2013Rao metric. For example, the gradient flow that minimizes the functional F                                               [q]\n under the Wasserstein Fisher\u2013Rao metric is given by the following PDE (which is shown with detailed\n derivations in Appendix A)\n    \u2202qt                         qt(x)      \u2212   \u2207x   \u03b4F   [qt] (x)        \u2212  1   \u03b4F     [qt] (x) \u2212    Eqt(y)    \u03b4F    [qt]  (y)      qt(x),\n     \u2202t (x) = \u2212\u2207x \u00b7                                    \u03b4qt                  \u03bb        \u03b4qt                           \u03b4qt\n                                  the continuity equation                                          growth term                              (8)\n where \u03b4F      [q]/\u03b4q is the first-variation of of F with respect to the L2 metric. The physical explanation\n of the terms in Eq. (8) is as follows. The continuity equation defines the change of the density\n when the samples x \u223c               qt(x) follow the vector field vt(x) = \u2212\u2207x\u03b4F                          [qt]/\u03b4qt. The second term\n of the PDE defines the creation and annihilation of probability mass, and is proportional to the\n growth field gt(x) = \u03b4F [qt]                                \u03b4F [qt]        . Note that Eq      t[gt] = 0, and so while mass can\n                                   \u03b4qt (x) \u2212      Eqt(y)       \u03b4qt (y)\n be \u201cteleported\u201d, the total mass (or probability) will remain constant. The two mechanisms can be\n considered independently by defining the evolution terms under the 2-Wasserstein and Fisher\u2013Rao\n metrics respectively, i.e.\n      \u2202qt                         qt(x)      \u2212  \u2207x    \u03b4F  [qt]  (x)       ,              2-Wasserstein Gradient Flow,                       (9)\n      \u2202t (x) = \u2212\u2207x \u00b7                                    \u03b4qt\n      \u2202qt               \u03b4F     [qt] (x) \u2212     Eqt(y)    \u03b4F    [qt] (y)      qt(x),             Fisher\u2013Rao Gradient Flow.                  (10)\n      \u2202t (x) = \u2212             \u03b4qt                           \u03b4qt\n It now becomes evident that the stationary condition for all the considered PDEs is\n                                    \u2207x    \u03b4F   [qt] (x)     = 0 \u21d0      \u21d2     \u03b4F  [qt]  (x) \u2261    constant .                                (11)\n                                             \u03b4qt                               \u03b4qt\n In Appendix A, we provide derivations illustrating that Eqs. (8) to (10) correspond to the gradient\n flow under the Wasserstein Fisher\u2013Rao, Wasserstein, and Fisher\u2013Rao metrics, respectively, and hence\n they all minimize F          [q]. For detailed analysis, we refer the reader to Kondratyev et al. (2016); Liero\n et al. (2016).\n 3     Methodology\n In Section 3.1, we first demonstrate that the imaginary-time evolution of the Schr\u00f6dinger equation\n can be viewed as a gradient flow under the Fisher\u2013Rao metric. Afterwards, in Section 3.2, we discuss\n how a density evolution can be projected to the parametric variational family and show that doing so\n for the Fisher\u2013Rao gradient flow yields the QVMC algorithm. Taking this perspective, we propose\n the Wasserstein Quantum Monte Carlo by considering Wasserstein (and Wasserstein Fisher\u2013Rao)\n gradient flows, followed by the projection onto the parametric manifold (see Section 3.3).\n 3.1     Imaginary-Time evolution as the gradient flow under the Fisher\u2013Rao metric\n The ground state of a quantum system can in theory be obtained by imaginary-time evolving any\n valid quantum state \u03c8 (with a non-vanishing overlap with the true ground state) to infinite times. The\n state is evolved according to the imaginary-time Schr\u00f6dinger equation, which defines the energy-\n minimizing time evolution of the wavefunction \u03c8t, and is expressed as the following PDE (which is\n                                                                       4", "md": "# Math Equations and Text\n\nwhere \\(v_t(x)\\) is the vector field defining the probability flow, \\(g_t(x)\\) is the growth term controlling the creation and annihilation of the probability mass, and \\(\\lambda\\) is the coefficient balancing the transportation and teleportation costs. Note that by setting one of the terms to zero we get 2-Wasserstein distance (\\(g_t(x) \\equiv 0\\)) and Fisher\u2013Rao distance (\\(v_t(x) \\equiv 0\\)). In Section 3, we also consider the general case of \\(c\\)-Wasserstein distance, where \\(c\\) is a convex cost function on the tangent space.\n\nGiven a functional on this manifold, \\(F[q] : P^2 \\rightarrow \\mathbb{R}\\), we can define the gradient flow of the function \\(F\\) under any Riemannian metric including the Wasserstein metric, the Fisher\u2013Rao metric, or the Wasserstein Fisher\u2013Rao metric. For example, the gradient flow that minimizes the functional \\(F[q]\\) under the Wasserstein Fisher\u2013Rao metric is given by the following PDE (which is shown with detailed derivations in Appendix A)\n\n$$\n\\begin{align*}\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\nabla_x \\cdot \\left( \\delta F[q_t](x) - \\frac{1}{\\lambda} \\delta F[q_t](x) - E[q_t(y)] \\delta F[q_t](y) \\right) q_t(x),\n\\end{align*}\n$$\n\nwhere \\(\\delta F[q]/\\delta q\\) is the first-variation of \\(F\\) with respect to the \\(L^2\\) metric. The physical explanation of the terms in Eq. (8) is as follows. The continuity equation defines the change of the density when the samples \\(x \\sim q_t(x)\\) follow the vector field \\(v_t(x) = -\\nabla_x \\delta F[q_t]/\\delta q_t\\). The second term of the PDE defines the creation and annihilation of probability mass, and is proportional to the growth field \\(g_t(x) = \\frac{\\delta F[q_t]}{\\delta q_t}\\). Note that \\(\\delta F[q]/\\delta q = 0\\), and so while mass can be \u201cteleported\u201d, the total mass (or probability) will remain constant. The two mechanisms can be considered independently by defining the evolution terms under the 2-Wasserstein and Fisher\u2013Rao metrics respectively, i.e.\n\n$$\n\\begin{align*}\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\nabla_x \\cdot \\delta F[q_t](x), \\quad \\text{2-Wasserstein Gradient Flow,} \\\\\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\delta F[q_t](x) - E[q_t(y)] \\delta F[q_t](y) q_t(x), \\quad \\text{Fisher\u2013Rao Gradient Flow.}\n\\end{align*}\n$$\n\nIt now becomes evident that the stationary condition for all the considered PDEs is\n\n$$\n\\nabla_x \\delta F[q_t](x) = 0 \\Leftrightarrow \\delta F[q_t](x) \\equiv \\text{constant}. \\quad (11)\n$$\n\nIn Appendix A, we provide derivations illustrating that Eqs. (8) to (10) correspond to the gradient flow under the Wasserstein Fisher\u2013Rao, Wasserstein, and Fisher\u2013Rao metrics, respectively, and hence they all minimize \\(F[q]\\). For detailed analysis, we refer the reader to Kondratyev et al. (2016); Liero et al. (2016).\n\n### 3 Methodology\n\nIn Section 3.1, we first demonstrate that the imaginary-time evolution of the Schr\u00f6dinger equation can be viewed as a gradient flow under the Fisher\u2013Rao metric. Afterwards, in Section 3.2, we discuss how a density evolution can be projected to the parametric variational family and show that doing so for the Fisher\u2013Rao gradient flow yields the QVMC algorithm. Taking this perspective, we propose the Wasserstein Quantum Monte Carlo by considering Wasserstein (and Wasserstein Fisher\u2013Rao) gradient flows, followed by the projection onto the parametric manifold (see Section 3.3).\n\n#### 3.1 Imaginary-Time evolution as the gradient flow under the Fisher\u2013Rao metric\n\nThe ground state of a quantum system can in theory be obtained by imaginary-time evolving any valid quantum state \\(\\psi\\) (with a non-vanishing overlap with the true ground state) to infinite times. The state is evolved according to the imaginary-time Schr\u00f6dinger equation, which defines the energy-minimizing time evolution of the wavefunction \\(\\psi_t\\), and is expressed as the following PDE", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "where \\(v_t(x)\\) is the vector field defining the probability flow, \\(g_t(x)\\) is the growth term controlling the creation and annihilation of the probability mass, and \\(\\lambda\\) is the coefficient balancing the transportation and teleportation costs. Note that by setting one of the terms to zero we get 2-Wasserstein distance (\\(g_t(x) \\equiv 0\\)) and Fisher\u2013Rao distance (\\(v_t(x) \\equiv 0\\)). In Section 3, we also consider the general case of \\(c\\)-Wasserstein distance, where \\(c\\) is a convex cost function on the tangent space.\n\nGiven a functional on this manifold, \\(F[q] : P^2 \\rightarrow \\mathbb{R}\\), we can define the gradient flow of the function \\(F\\) under any Riemannian metric including the Wasserstein metric, the Fisher\u2013Rao metric, or the Wasserstein Fisher\u2013Rao metric. For example, the gradient flow that minimizes the functional \\(F[q]\\) under the Wasserstein Fisher\u2013Rao metric is given by the following PDE (which is shown with detailed derivations in Appendix A)\n\n$$\n\\begin{align*}\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\nabla_x \\cdot \\left( \\delta F[q_t](x) - \\frac{1}{\\lambda} \\delta F[q_t](x) - E[q_t(y)] \\delta F[q_t](y) \\right) q_t(x),\n\\end{align*}\n$$\n\nwhere \\(\\delta F[q]/\\delta q\\) is the first-variation of \\(F\\) with respect to the \\(L^2\\) metric. The physical explanation of the terms in Eq. (8) is as follows. The continuity equation defines the change of the density when the samples \\(x \\sim q_t(x)\\) follow the vector field \\(v_t(x) = -\\nabla_x \\delta F[q_t]/\\delta q_t\\). The second term of the PDE defines the creation and annihilation of probability mass, and is proportional to the growth field \\(g_t(x) = \\frac{\\delta F[q_t]}{\\delta q_t}\\). Note that \\(\\delta F[q]/\\delta q = 0\\), and so while mass can be \u201cteleported\u201d, the total mass (or probability) will remain constant. The two mechanisms can be considered independently by defining the evolution terms under the 2-Wasserstein and Fisher\u2013Rao metrics respectively, i.e.\n\n$$\n\\begin{align*}\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\nabla_x \\cdot \\delta F[q_t](x), \\quad \\text{2-Wasserstein Gradient Flow,} \\\\\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\delta F[q_t](x) - E[q_t(y)] \\delta F[q_t](y) q_t(x), \\quad \\text{Fisher\u2013Rao Gradient Flow.}\n\\end{align*}\n$$\n\nIt now becomes evident that the stationary condition for all the considered PDEs is\n\n$$\n\\nabla_x \\delta F[q_t](x) = 0 \\Leftrightarrow \\delta F[q_t](x) \\equiv \\text{constant}. \\quad (11)\n$$\n\nIn Appendix A, we provide derivations illustrating that Eqs. (8) to (10) correspond to the gradient flow under the Wasserstein Fisher\u2013Rao, Wasserstein, and Fisher\u2013Rao metrics, respectively, and hence they all minimize \\(F[q]\\). For detailed analysis, we refer the reader to Kondratyev et al. (2016); Liero et al. (2016).", "md": "where \\(v_t(x)\\) is the vector field defining the probability flow, \\(g_t(x)\\) is the growth term controlling the creation and annihilation of the probability mass, and \\(\\lambda\\) is the coefficient balancing the transportation and teleportation costs. Note that by setting one of the terms to zero we get 2-Wasserstein distance (\\(g_t(x) \\equiv 0\\)) and Fisher\u2013Rao distance (\\(v_t(x) \\equiv 0\\)). In Section 3, we also consider the general case of \\(c\\)-Wasserstein distance, where \\(c\\) is a convex cost function on the tangent space.\n\nGiven a functional on this manifold, \\(F[q] : P^2 \\rightarrow \\mathbb{R}\\), we can define the gradient flow of the function \\(F\\) under any Riemannian metric including the Wasserstein metric, the Fisher\u2013Rao metric, or the Wasserstein Fisher\u2013Rao metric. For example, the gradient flow that minimizes the functional \\(F[q]\\) under the Wasserstein Fisher\u2013Rao metric is given by the following PDE (which is shown with detailed derivations in Appendix A)\n\n$$\n\\begin{align*}\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\nabla_x \\cdot \\left( \\delta F[q_t](x) - \\frac{1}{\\lambda} \\delta F[q_t](x) - E[q_t(y)] \\delta F[q_t](y) \\right) q_t(x),\n\\end{align*}\n$$\n\nwhere \\(\\delta F[q]/\\delta q\\) is the first-variation of \\(F\\) with respect to the \\(L^2\\) metric. The physical explanation of the terms in Eq. (8) is as follows. The continuity equation defines the change of the density when the samples \\(x \\sim q_t(x)\\) follow the vector field \\(v_t(x) = -\\nabla_x \\delta F[q_t]/\\delta q_t\\). The second term of the PDE defines the creation and annihilation of probability mass, and is proportional to the growth field \\(g_t(x) = \\frac{\\delta F[q_t]}{\\delta q_t}\\). Note that \\(\\delta F[q]/\\delta q = 0\\), and so while mass can be \u201cteleported\u201d, the total mass (or probability) will remain constant. The two mechanisms can be considered independently by defining the evolution terms under the 2-Wasserstein and Fisher\u2013Rao metrics respectively, i.e.\n\n$$\n\\begin{align*}\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\nabla_x \\cdot \\delta F[q_t](x), \\quad \\text{2-Wasserstein Gradient Flow,} \\\\\n\\frac{\\partial q_t}{\\partial t}(x) &= -\\delta F[q_t](x) - E[q_t(y)] \\delta F[q_t](y) q_t(x), \\quad \\text{Fisher\u2013Rao Gradient Flow.}\n\\end{align*}\n$$\n\nIt now becomes evident that the stationary condition for all the considered PDEs is\n\n$$\n\\nabla_x \\delta F[q_t](x) = 0 \\Leftrightarrow \\delta F[q_t](x) \\equiv \\text{constant}. \\quad (11)\n$$\n\nIn Appendix A, we provide derivations illustrating that Eqs. (8) to (10) correspond to the gradient flow under the Wasserstein Fisher\u2013Rao, Wasserstein, and Fisher\u2013Rao metrics, respectively, and hence they all minimize \\(F[q]\\). For detailed analysis, we refer the reader to Kondratyev et al. (2016); Liero et al. (2016)."}, {"type": "heading", "lvl": 3, "value": "3 Methodology", "md": "### 3 Methodology"}, {"type": "text", "value": "In Section 3.1, we first demonstrate that the imaginary-time evolution of the Schr\u00f6dinger equation can be viewed as a gradient flow under the Fisher\u2013Rao metric. Afterwards, in Section 3.2, we discuss how a density evolution can be projected to the parametric variational family and show that doing so for the Fisher\u2013Rao gradient flow yields the QVMC algorithm. Taking this perspective, we propose the Wasserstein Quantum Monte Carlo by considering Wasserstein (and Wasserstein Fisher\u2013Rao) gradient flows, followed by the projection onto the parametric manifold (see Section 3.3).", "md": "In Section 3.1, we first demonstrate that the imaginary-time evolution of the Schr\u00f6dinger equation can be viewed as a gradient flow under the Fisher\u2013Rao metric. Afterwards, in Section 3.2, we discuss how a density evolution can be projected to the parametric variational family and show that doing so for the Fisher\u2013Rao gradient flow yields the QVMC algorithm. Taking this perspective, we propose the Wasserstein Quantum Monte Carlo by considering Wasserstein (and Wasserstein Fisher\u2013Rao) gradient flows, followed by the projection onto the parametric manifold (see Section 3.3)."}, {"type": "heading", "lvl": 4, "value": "3.1 Imaginary-Time evolution as the gradient flow under the Fisher\u2013Rao metric", "md": "#### 3.1 Imaginary-Time evolution as the gradient flow under the Fisher\u2013Rao metric"}, {"type": "text", "value": "The ground state of a quantum system can in theory be obtained by imaginary-time evolving any valid quantum state \\(\\psi\\) (with a non-vanishing overlap with the true ground state) to infinite times. The state is evolved according to the imaginary-time Schr\u00f6dinger equation, which defines the energy-minimizing time evolution of the wavefunction \\(\\psi_t\\), and is expressed as the following PDE", "md": "The ground state of a quantum system can in theory be obtained by imaginary-time evolving any valid quantum state \\(\\psi\\) (with a non-vanishing overlap with the true ground state) to infinite times. The state is evolved according to the imaginary-time Schr\u00f6dinger equation, which defines the energy-minimizing time evolution of the wavefunction \\(\\psi_t\\), and is expressed as the following PDE"}]}, {"page": 5, "text": "Figure 1: W(FR)QMC: A graphical illustration of the gradient flow according to the Wasserstein and\nFisher\u2013Rao metrics, and the corresponding projection onto the variational manifold q(x, \u03b8).\nthe Wick-rotated version of Eq. (2), see e.g. (McArdle et al., 2019; Yuan et al., 2019)),\n                                  \u2202\u03c8t(x)\n                                    \u2202t    = \u2212    (H \u2212   E[\u03c8t])\u03c8t(x),                               (12)\nwhere again qt(x) = \u03c82  t (x). The last term proportional to the energy E[\u03c8t] comes from enforcing\nnormalization (contrary to real-time evolution, imaginary time evolution is non-unitary).\nTheorem 3.1. Eq. (12) defines the gradient flow of the energy functional E[q] under the Fisher\u2013Rao\nmetric.\nProof Sketch. The energy functional E[q] has the following derivative\n                 \u03b4E[q] (x) = V (x) \u2212   1   x log q(x) \u2212  1                                         (13)\n                  \u03b4q                   4\u22072               8\u2225\u2207x log q(x)\u22252 = Eloc(x).\nThus, the gradient flow under the Fisher\u2013Rao metric is (see Eq. (10))\n                           \u2202qt(x)  = \u2212    Eloc(x) \u2212  Eqt(x)[Eloc(x)]  qt(x),                       (14)\n                             \u2202t\nwhich is equivalent (up to a multiplicative constant) to the imaginary-time Schr\u00f6dinger Equation in\nEq. (12) as shown in the complete proof in Appendix B.\nWe believe that this result can be derived following the derivations from Stokes et al. (2020), but\nnot introducing the manifold of parametric distributions. However, considering the evolution of the\ndensity on the non-parametric manifold first helps us to derive our method and relating it to QVMC.\nIn the following subsection, we discuss how to project this non-parametric evolution to a parametric\nmanifold.\n3.2   Following the gradient flow by a parametric model\nBy choosing a metric in the distributional space and following the energy-minimizing gradient flows,\nwe can design various algorithms for estimating the ground state wave function. Indeed, in principle,\nby propagating the samples or the density according to any gradient flow (e.g., Eqs. (8) to (10)), we\ncan eventually reach the ground state. However, these dynamics are defined on the non-parametric\nand infinite-dimensional manifold of distributions, which do not allow tractable computation of log\ndensities, and thus tractable evolution. Therefore, we project these dynamics onto the parametric\nmanifold of our variational family, and follow the projected gradient flows instead, which is tractable.\nSuppose the current density on the parametric manifold is qt(x) = q(x, \u03b8) (see Figure 1). We first\nevolve this density using a (non-parametric) gradient flow method (e.g., Eqs. (8) to (10)) for time \u2206t,\nwhich will take qt(x) off the parametric manifold to qt+\u2206t(x). We then have to update current trial\nmodel q(x, \u03b8) to match qt+\u2206t(x) enabling us to propagate the density further. In order to do so, we\ndefine the optimal update of parameters \u2206\u03b8\u2217     as the minimizer of the Kullback-Leibler divergence\nbetween qt+\u2206t(x) and the distributions on the parametric manifold, i.e.\n                          \u2206\u03b8\u2217  = arg min    DKL(qt+\u2206t(x)\u2225q(x, \u03b8 + \u2206\u03b8)) .                           (15)\n                                     \u2206\u03b8\n                                  \u2225\u2206\u03b8\u22252=1\n                                                   5", "md": "Figure 1: W(FR)QMC: A graphical illustration of the gradient flow according to the Wasserstein and Fisher\u2013Rao metrics, and the corresponding projection onto the variational manifold q(x, \u03b8).\n\nthe Wick-rotated version of Eq. (2), see e.g. (McArdle et al., 2019; Yuan et al., 2019)),\n\n$$\n\\frac{\\partial \\psi_t(x)}{\\partial t} = - (H - E[\\psi_t])\\psi_t(x), \\quad (12)\n$$\nwhere again \\( q_t(x) = \\psi^2_t(x) \\). The last term proportional to the energy \\( E[\\psi_t] \\) comes from enforcing normalization (contrary to real-time evolution, imaginary time evolution is non-unitary).\n\nTheorem 3.1. Eq. (12) defines the gradient flow of the energy functional \\( E[q] \\) under the Fisher\u2013Rao metric.\n\nProof Sketch. The energy functional \\( E[q] \\) has the following derivative\n\n$$\n\\delta E[q](x) = V(x) - \\frac{1}{4} x \\log q(x) - \\frac{1}{8} \\nabla^2 \\left\\| \\nabla_x \\log q(x) \\right\\|^2 = E_{\\text{loc}}(x). \\quad (13)\n$$\nThus, the gradient flow under the Fisher\u2013Rao metric is (see Eq. (10))\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = - E_{\\text{loc}}(x) - E_{q_t(x)}[E_{\\text{loc}}(x)] q_t(x), \\quad (14)\n$$\nwhich is equivalent (up to a multiplicative constant) to the imaginary-time Schr\u00f6dinger Equation in Eq. (12) as shown in the complete proof in Appendix B.\n\nWe believe that this result can be derived following the derivations from Stokes et al. (2020), but not introducing the manifold of parametric distributions. However, considering the evolution of the density on the non-parametric manifold first helps us to derive our method and relating it to QVMC.\n\nIn the following subsection, we discuss how to project this non-parametric evolution to a parametric manifold.\n\n3.2 Following the gradient flow by a parametric model\n\nBy choosing a metric in the distributional space and following the energy-minimizing gradient flows, we can design various algorithms for estimating the ground state wave function. Indeed, in principle, by propagating the samples or the density according to any gradient flow (e.g., Eqs. (8) to (10)), we can eventually reach the ground state. However, these dynamics are defined on the non-parametric and infinite-dimensional manifold of distributions, which do not allow tractable computation of log densities, and thus tractable evolution. Therefore, we project these dynamics onto the parametric manifold of our variational family, and follow the projected gradient flows instead, which is tractable.\n\nSuppose the current density on the parametric manifold is \\( q_t(x) = q(x, \\theta) \\) (see Figure 1). We first evolve this density using a (non-parametric) gradient flow method (e.g., Eqs. (8) to (10)) for time \\( \\Delta t \\), which will take \\( q_t(x) \\) off the parametric manifold to \\( q_{t+\\Delta t}(x) \\). We then have to update current trial model \\( q(x, \\theta) \\) to match \\( q_{t+\\Delta t}(x) \\) enabling us to propagate the density further. In order to do so, we define the optimal update of parameters \\( \\Delta \\theta^* \\) as the minimizer of the Kullback-Leibler divergence between \\( q_{t+\\Delta t}(x) \\) and the distributions on the parametric manifold, i.e.\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} D_{\\text{KL}}(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)). \\quad (15)\n$$\nwhere \\( \\left\\| \\Delta \\theta \\right\\|^2 = 1 \\)", "images": [], "items": [{"type": "text", "value": "Figure 1: W(FR)QMC: A graphical illustration of the gradient flow according to the Wasserstein and Fisher\u2013Rao metrics, and the corresponding projection onto the variational manifold q(x, \u03b8).\n\nthe Wick-rotated version of Eq. (2), see e.g. (McArdle et al., 2019; Yuan et al., 2019)),\n\n$$\n\\frac{\\partial \\psi_t(x)}{\\partial t} = - (H - E[\\psi_t])\\psi_t(x), \\quad (12)\n$$\nwhere again \\( q_t(x) = \\psi^2_t(x) \\). The last term proportional to the energy \\( E[\\psi_t] \\) comes from enforcing normalization (contrary to real-time evolution, imaginary time evolution is non-unitary).\n\nTheorem 3.1. Eq. (12) defines the gradient flow of the energy functional \\( E[q] \\) under the Fisher\u2013Rao metric.\n\nProof Sketch. The energy functional \\( E[q] \\) has the following derivative\n\n$$\n\\delta E[q](x) = V(x) - \\frac{1}{4} x \\log q(x) - \\frac{1}{8} \\nabla^2 \\left\\| \\nabla_x \\log q(x) \\right\\|^2 = E_{\\text{loc}}(x). \\quad (13)\n$$\nThus, the gradient flow under the Fisher\u2013Rao metric is (see Eq. (10))\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = - E_{\\text{loc}}(x) - E_{q_t(x)}[E_{\\text{loc}}(x)] q_t(x), \\quad (14)\n$$\nwhich is equivalent (up to a multiplicative constant) to the imaginary-time Schr\u00f6dinger Equation in Eq. (12) as shown in the complete proof in Appendix B.\n\nWe believe that this result can be derived following the derivations from Stokes et al. (2020), but not introducing the manifold of parametric distributions. However, considering the evolution of the density on the non-parametric manifold first helps us to derive our method and relating it to QVMC.\n\nIn the following subsection, we discuss how to project this non-parametric evolution to a parametric manifold.\n\n3.2 Following the gradient flow by a parametric model\n\nBy choosing a metric in the distributional space and following the energy-minimizing gradient flows, we can design various algorithms for estimating the ground state wave function. Indeed, in principle, by propagating the samples or the density according to any gradient flow (e.g., Eqs. (8) to (10)), we can eventually reach the ground state. However, these dynamics are defined on the non-parametric and infinite-dimensional manifold of distributions, which do not allow tractable computation of log densities, and thus tractable evolution. Therefore, we project these dynamics onto the parametric manifold of our variational family, and follow the projected gradient flows instead, which is tractable.\n\nSuppose the current density on the parametric manifold is \\( q_t(x) = q(x, \\theta) \\) (see Figure 1). We first evolve this density using a (non-parametric) gradient flow method (e.g., Eqs. (8) to (10)) for time \\( \\Delta t \\), which will take \\( q_t(x) \\) off the parametric manifold to \\( q_{t+\\Delta t}(x) \\). We then have to update current trial model \\( q(x, \\theta) \\) to match \\( q_{t+\\Delta t}(x) \\) enabling us to propagate the density further. In order to do so, we define the optimal update of parameters \\( \\Delta \\theta^* \\) as the minimizer of the Kullback-Leibler divergence between \\( q_{t+\\Delta t}(x) \\) and the distributions on the parametric manifold, i.e.\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} D_{\\text{KL}}(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)). \\quad (15)\n$$\nwhere \\( \\left\\| \\Delta \\theta \\right\\|^2 = 1 \\)", "md": "Figure 1: W(FR)QMC: A graphical illustration of the gradient flow according to the Wasserstein and Fisher\u2013Rao metrics, and the corresponding projection onto the variational manifold q(x, \u03b8).\n\nthe Wick-rotated version of Eq. (2), see e.g. (McArdle et al., 2019; Yuan et al., 2019)),\n\n$$\n\\frac{\\partial \\psi_t(x)}{\\partial t} = - (H - E[\\psi_t])\\psi_t(x), \\quad (12)\n$$\nwhere again \\( q_t(x) = \\psi^2_t(x) \\). The last term proportional to the energy \\( E[\\psi_t] \\) comes from enforcing normalization (contrary to real-time evolution, imaginary time evolution is non-unitary).\n\nTheorem 3.1. Eq. (12) defines the gradient flow of the energy functional \\( E[q] \\) under the Fisher\u2013Rao metric.\n\nProof Sketch. The energy functional \\( E[q] \\) has the following derivative\n\n$$\n\\delta E[q](x) = V(x) - \\frac{1}{4} x \\log q(x) - \\frac{1}{8} \\nabla^2 \\left\\| \\nabla_x \\log q(x) \\right\\|^2 = E_{\\text{loc}}(x). \\quad (13)\n$$\nThus, the gradient flow under the Fisher\u2013Rao metric is (see Eq. (10))\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = - E_{\\text{loc}}(x) - E_{q_t(x)}[E_{\\text{loc}}(x)] q_t(x), \\quad (14)\n$$\nwhich is equivalent (up to a multiplicative constant) to the imaginary-time Schr\u00f6dinger Equation in Eq. (12) as shown in the complete proof in Appendix B.\n\nWe believe that this result can be derived following the derivations from Stokes et al. (2020), but not introducing the manifold of parametric distributions. However, considering the evolution of the density on the non-parametric manifold first helps us to derive our method and relating it to QVMC.\n\nIn the following subsection, we discuss how to project this non-parametric evolution to a parametric manifold.\n\n3.2 Following the gradient flow by a parametric model\n\nBy choosing a metric in the distributional space and following the energy-minimizing gradient flows, we can design various algorithms for estimating the ground state wave function. Indeed, in principle, by propagating the samples or the density according to any gradient flow (e.g., Eqs. (8) to (10)), we can eventually reach the ground state. However, these dynamics are defined on the non-parametric and infinite-dimensional manifold of distributions, which do not allow tractable computation of log densities, and thus tractable evolution. Therefore, we project these dynamics onto the parametric manifold of our variational family, and follow the projected gradient flows instead, which is tractable.\n\nSuppose the current density on the parametric manifold is \\( q_t(x) = q(x, \\theta) \\) (see Figure 1). We first evolve this density using a (non-parametric) gradient flow method (e.g., Eqs. (8) to (10)) for time \\( \\Delta t \\), which will take \\( q_t(x) \\) off the parametric manifold to \\( q_{t+\\Delta t}(x) \\). We then have to update current trial model \\( q(x, \\theta) \\) to match \\( q_{t+\\Delta t}(x) \\) enabling us to propagate the density further. In order to do so, we define the optimal update of parameters \\( \\Delta \\theta^* \\) as the minimizer of the Kullback-Leibler divergence between \\( q_{t+\\Delta t}(x) \\) and the distributions on the parametric manifold, i.e.\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} D_{\\text{KL}}(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)). \\quad (15)\n$$\nwhere \\( \\left\\| \\Delta \\theta \\right\\|^2 = 1 \\)"}]}, {"page": 6, "text": " In practice, we evaluate the parameters update using the expansion of the KL-divergence from the\n following proposition.\n Proposition 3.2. For qt(x) = q(x, \u03b8), the KL-divergence can be expanded as\n               DKL(qt+\u2206t(x)\u2225q(x, \u03b8 + \u2206\u03b8)) = \u2212                         1        \u2206\u03b8,         \u2202\n                                                                      2\u2206t                 \u2202tqt(x)\u2207\u03b8 log q(x, \u03b8) dx\n                                                                  + o         \u2206t2 + \u2225\u2206\u03b8\u22252            ,                             (16)\n where \u27e8\u00b7, \u00b7\u27e9     denotes the inner product, and should not be confused with the bra-ket notation.\n Proof. See Appendix C for the proof.\n Using this approximation, the optimal update from Eq. (15) of parameters becomes\n     \u2206\u03b8\u2217     = arg min          \u2206\u03b8, \u2212          \u2202                                       \u221d        \u2202                                  (17)\n                     \u2206\u03b8                        \u2202tqt(x)\u2207\u03b8 log q(x, \u03b8) dx                         \u2202tqt(x)\u2207\u03b8 log q(x, \u03b8) dx .\n                 \u2225\u2206\u03b8\u22252=1\n The following Corollary states that QVMC can be viewed as the projected gradient flow of the energy\n functional with respect to the Fisher\u2013Rao metric.\n Corollary 3.3. Consider the Fisher\u2013Rao gradient flow (or imaginary time evolution, which is\n equivalent, as shown in Theorem 3.1). Then, the parameters update (Eq. (17)) matches the gradient\n of the conventional QVMC loss, i.e.\n                        \u2206\u03b8\u2217    \u221d   \u2212Eqt(x)         Eloc(x, \u03b8) \u2212       Eqt(x)[Eloc(x, \u03b8)]          \u2207\u03b8 log q(x, \u03b8)        .          (18)\n This perspective lays the foundation for deriving our WQMC method in Section 3.3, by following the\nWasserstein or WFR gradient flows rather than Fisher\u2013Rao gradient flows.\n Natural Gradient Preconditioning                         In order to update the parametric model q(x, \u03b8), instead of\n following the update in Eq. (17), we can exploit the information geometry of the statistical manifold\n of q(x, \u03b8), and define the update using the Fisher information matrix F\u03b8\n                                               \u2202                                       \u221d   F\u22121          \u2202\n   \u2206\u03b8\u2217     = arg min \u2206\u03b8         \u2206\u03b8, \u2212          \u2202tqt(x)\u2207\u03b8 log q(x, \u03b8) dx                      \u03b8         \u2202tqt(x)\u2207\u03b8 log q(x, \u03b8) dx ,\n                \u2225\u2206\u03b8\u2225F=1\n      F\u03b8 = Eq(x,\u03b8)           \u2202                                           .                                                         (19)\n                            \u2202\u03b8 log q(x, \u03b8) \u2202     \u2202\u03b8 log q(x, \u03b8)\u22a4\n This update is analogous to the natural gradient update (Amari, 1998). Note that the choice of Fisher\n information as the metric on the statistical manifold for preconditioning the gradient and updating \u03b8 is\n independent of the choice of metric on the non-parametric Wasserstein manifold (e.g., Wasserstein or\n Fisher\u2013Rao) for evolving qt(x). In practice, we use Kronecker-factored approximation of the natural\n gradient (K-FAC) (Martens & Grosse, 2015).\n 3.3     Wasserstein quantum Monte Carlo\n In the previous sections, we formulated imaginary-time evolution governed by the Schr\u00f6dinger\n equation as the energy-minimizing gradient flow under the Fisher\u2013Rao metric. Furthermore, we\n demonstrated that projecting the evolved density to the parametric manifold at every iteration\n corresponds to the QVMC algorithm.\n Naturally, we can consider another metric on the (non-parametric) space of distributions, which\n results in a different gradient flow and corresponds to a different algorithm. Namely, we propose to\n consider the gradient descent in 2-Wasserstein space as the energy-minimizing density evolution, as\n introduced in Section 2.2.\n Theorem 3.4. The energy-minimizing 2-Wasserstein gradient flow is defined by the continuity\n equation\n                                            \u2202qt(x)\n                                               \u2202t      = \u2212\u2207x \u00b7 (qt(x)(\u2212\u2207xEloc(x)))                                                 (20)\n                                                                       6", "md": "# Math Equations\n\nIn practice, we evaluate the parameters update using the expansion of the KL-divergence from the following proposition.\n\nProposition 3.2. For \\( q_t(x) = q(x, \\theta) \\), the KL-divergence can be expanded as\n\n$$\nD_{KL}(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)) = -\\frac{1}{2\\Delta t} \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle dx + o(\\Delta t^2 + \\|\\Delta \\theta\\|^2), \\quad (16)\n$$\nwhere \\( \\langle \\cdot, \\cdot \\rangle \\) denotes the inner product, and should not be confused with the bra-ket notation.\n\nProof. See Appendix C for the proof.\n\nUsing this approximation, the optimal update from Eq. (15) of parameters becomes\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} \\|\\Delta \\theta\\|_2=1, -\\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle \\propto \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle. \\quad (17)\n$$\nThe following Corollary states that QVMC can be viewed as the projected gradient flow of the energy functional with respect to the Fisher\u2013Rao metric.\n\nCorollary 3.3. Consider the Fisher\u2013Rao gradient flow (or imaginary time evolution, which is equivalent, as shown in Theorem 3.1). Then, the parameters update (Eq. (17)) matches the gradient of the conventional QVMC loss, i.e.\n\n$$\n\\Delta \\theta^* \\propto -E_{q_t(x)}[E_{loc}(x, \\theta) - E_{q_t(x)}[E_{loc}(x, \\theta)]] \\nabla \\theta \\log q(x, \\theta). \\quad (18)\n$$\nThis perspective lays the foundation for deriving our WQMC method in Section 3.3, by following the Wasserstein or WFR gradient flows rather than Fisher\u2013Rao gradient flows.\n\nNatural Gradient Preconditioning In order to update the parametric model \\( q(x, \\theta) \\), instead of following the update in Eq. (17), we can exploit the information geometry of the statistical manifold of \\( q(x, \\theta) \\), and define the update using the Fisher information matrix \\( F_{\\theta} \\)\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} \\|\\Delta \\theta\\|_{F}=1, -\\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle \\propto F^{-1} \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle, \\quad (19)\n$$\nwhere \\( F_{\\theta} = E_{q(x,\\theta)}[\\nabla \\theta \\log q(x, \\theta) \\nabla \\theta \\log q(x, \\theta)^T] \\).\n\nThis update is analogous to the natural gradient update (Amari, 1998). Note that the choice of Fisher information as the metric on the statistical manifold for preconditioning the gradient and updating \\( \\theta \\) is independent of the choice of metric on the non-parametric Wasserstein manifold (e.g., Wasserstein or Fisher\u2013Rao) for evolving \\( q_t(x) \\). In practice, we use Kronecker-factored approximation of the natural gradient (K-FAC) (Martens & Grosse, 2015).\n\n### 3.3 Wasserstein quantum Monte Carlo\n\nIn the previous sections, we formulated imaginary-time evolution governed by the Schr\u00f6dinger equation as the energy-minimizing gradient flow under the Fisher\u2013Rao metric. Furthermore, we demonstrated that projecting the evolved density to the parametric manifold at every iteration corresponds to the QVMC algorithm.\n\nNaturally, we can consider another metric on the (non-parametric) space of distributions, which results in a different gradient flow and corresponds to a different algorithm. Namely, we propose to consider the gradient descent in 2-Wasserstein space as the energy-minimizing density evolution, as introduced in Section 2.2.\n\nTheorem 3.4. The energy-minimizing 2-Wasserstein gradient flow is defined by the continuity equation\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)(-\\nabla_x E_{loc}(x))). \\quad (20)\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "In practice, we evaluate the parameters update using the expansion of the KL-divergence from the following proposition.\n\nProposition 3.2. For \\( q_t(x) = q(x, \\theta) \\), the KL-divergence can be expanded as\n\n$$\nD_{KL}(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)) = -\\frac{1}{2\\Delta t} \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle dx + o(\\Delta t^2 + \\|\\Delta \\theta\\|^2), \\quad (16)\n$$\nwhere \\( \\langle \\cdot, \\cdot \\rangle \\) denotes the inner product, and should not be confused with the bra-ket notation.\n\nProof. See Appendix C for the proof.\n\nUsing this approximation, the optimal update from Eq. (15) of parameters becomes\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} \\|\\Delta \\theta\\|_2=1, -\\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle \\propto \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle. \\quad (17)\n$$\nThe following Corollary states that QVMC can be viewed as the projected gradient flow of the energy functional with respect to the Fisher\u2013Rao metric.\n\nCorollary 3.3. Consider the Fisher\u2013Rao gradient flow (or imaginary time evolution, which is equivalent, as shown in Theorem 3.1). Then, the parameters update (Eq. (17)) matches the gradient of the conventional QVMC loss, i.e.\n\n$$\n\\Delta \\theta^* \\propto -E_{q_t(x)}[E_{loc}(x, \\theta) - E_{q_t(x)}[E_{loc}(x, \\theta)]] \\nabla \\theta \\log q(x, \\theta). \\quad (18)\n$$\nThis perspective lays the foundation for deriving our WQMC method in Section 3.3, by following the Wasserstein or WFR gradient flows rather than Fisher\u2013Rao gradient flows.\n\nNatural Gradient Preconditioning In order to update the parametric model \\( q(x, \\theta) \\), instead of following the update in Eq. (17), we can exploit the information geometry of the statistical manifold of \\( q(x, \\theta) \\), and define the update using the Fisher information matrix \\( F_{\\theta} \\)\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} \\|\\Delta \\theta\\|_{F}=1, -\\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle \\propto F^{-1} \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle, \\quad (19)\n$$\nwhere \\( F_{\\theta} = E_{q(x,\\theta)}[\\nabla \\theta \\log q(x, \\theta) \\nabla \\theta \\log q(x, \\theta)^T] \\).\n\nThis update is analogous to the natural gradient update (Amari, 1998). Note that the choice of Fisher information as the metric on the statistical manifold for preconditioning the gradient and updating \\( \\theta \\) is independent of the choice of metric on the non-parametric Wasserstein manifold (e.g., Wasserstein or Fisher\u2013Rao) for evolving \\( q_t(x) \\). In practice, we use Kronecker-factored approximation of the natural gradient (K-FAC) (Martens & Grosse, 2015).", "md": "In practice, we evaluate the parameters update using the expansion of the KL-divergence from the following proposition.\n\nProposition 3.2. For \\( q_t(x) = q(x, \\theta) \\), the KL-divergence can be expanded as\n\n$$\nD_{KL}(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)) = -\\frac{1}{2\\Delta t} \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle dx + o(\\Delta t^2 + \\|\\Delta \\theta\\|^2), \\quad (16)\n$$\nwhere \\( \\langle \\cdot, \\cdot \\rangle \\) denotes the inner product, and should not be confused with the bra-ket notation.\n\nProof. See Appendix C for the proof.\n\nUsing this approximation, the optimal update from Eq. (15) of parameters becomes\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} \\|\\Delta \\theta\\|_2=1, -\\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle \\propto \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle. \\quad (17)\n$$\nThe following Corollary states that QVMC can be viewed as the projected gradient flow of the energy functional with respect to the Fisher\u2013Rao metric.\n\nCorollary 3.3. Consider the Fisher\u2013Rao gradient flow (or imaginary time evolution, which is equivalent, as shown in Theorem 3.1). Then, the parameters update (Eq. (17)) matches the gradient of the conventional QVMC loss, i.e.\n\n$$\n\\Delta \\theta^* \\propto -E_{q_t(x)}[E_{loc}(x, \\theta) - E_{q_t(x)}[E_{loc}(x, \\theta)]] \\nabla \\theta \\log q(x, \\theta). \\quad (18)\n$$\nThis perspective lays the foundation for deriving our WQMC method in Section 3.3, by following the Wasserstein or WFR gradient flows rather than Fisher\u2013Rao gradient flows.\n\nNatural Gradient Preconditioning In order to update the parametric model \\( q(x, \\theta) \\), instead of following the update in Eq. (17), we can exploit the information geometry of the statistical manifold of \\( q(x, \\theta) \\), and define the update using the Fisher information matrix \\( F_{\\theta} \\)\n\n$$\n\\Delta \\theta^* = \\text{arg min}_{\\Delta \\theta} \\|\\Delta \\theta\\|_{F}=1, -\\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle \\propto F^{-1} \\langle \\partial_t q_t(x) \\nabla \\theta \\log q(x, \\theta) \\rangle, \\quad (19)\n$$\nwhere \\( F_{\\theta} = E_{q(x,\\theta)}[\\nabla \\theta \\log q(x, \\theta) \\nabla \\theta \\log q(x, \\theta)^T] \\).\n\nThis update is analogous to the natural gradient update (Amari, 1998). Note that the choice of Fisher information as the metric on the statistical manifold for preconditioning the gradient and updating \\( \\theta \\) is independent of the choice of metric on the non-parametric Wasserstein manifold (e.g., Wasserstein or Fisher\u2013Rao) for evolving \\( q_t(x) \\). In practice, we use Kronecker-factored approximation of the natural gradient (K-FAC) (Martens & Grosse, 2015)."}, {"type": "heading", "lvl": 3, "value": "3.3 Wasserstein quantum Monte Carlo", "md": "### 3.3 Wasserstein quantum Monte Carlo"}, {"type": "text", "value": "In the previous sections, we formulated imaginary-time evolution governed by the Schr\u00f6dinger equation as the energy-minimizing gradient flow under the Fisher\u2013Rao metric. Furthermore, we demonstrated that projecting the evolved density to the parametric manifold at every iteration corresponds to the QVMC algorithm.\n\nNaturally, we can consider another metric on the (non-parametric) space of distributions, which results in a different gradient flow and corresponds to a different algorithm. Namely, we propose to consider the gradient descent in 2-Wasserstein space as the energy-minimizing density evolution, as introduced in Section 2.2.\n\nTheorem 3.4. The energy-minimizing 2-Wasserstein gradient flow is defined by the continuity equation\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)(-\\nabla_x E_{loc}(x))). \\quad (20)\n$$", "md": "In the previous sections, we formulated imaginary-time evolution governed by the Schr\u00f6dinger equation as the energy-minimizing gradient flow under the Fisher\u2013Rao metric. Furthermore, we demonstrated that projecting the evolved density to the parametric manifold at every iteration corresponds to the QVMC algorithm.\n\nNaturally, we can consider another metric on the (non-parametric) space of distributions, which results in a different gradient flow and corresponds to a different algorithm. Namely, we propose to consider the gradient descent in 2-Wasserstein space as the energy-minimizing density evolution, as introduced in Section 2.2.\n\nTheorem 3.4. The energy-minimizing 2-Wasserstein gradient flow is defined by the continuity equation\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)(-\\nabla_x E_{loc}(x))). \\quad (20)\n$$"}]}, {"page": 7, "text": "Proof. In Theorem 3.1, we show that \u03b4E[q]/\u03b4q = Eloc(x). Plugging this into the 2-Wasserstein\ngradient flow defined in Eq. (9), yields the result in Eq. (20).\nc-Wasserstein Metric               This result can be further generalized to the c-Wasserstein metric with any\nconvex cost function c : Rd \u2192               R on the tangent space. The c-Wasserstein distance between p0 and\np1 is defined as follows\n                         W 2 c (p0, p1) := inf vt,qt    01  Eqt(x)[c(vt(x))] dt,            subj. to                          (21)\n                                \u2202qt(x)\n                                   \u2202t      = \u2212\u2207x \u00b7 (qt(x)vt(x)) ,                and q0 = p0, q1 = p1 .                       (22)\nProposition 3.5. The energy-minimizing c-Wasserstein gradient flow is defined by the following\nequation\n                                      \u2202qt(x)     = \u2212\u2207x \u00b7 (qt(x)\u2207c\u2217(\u22122\u2207xEloc(x))) ,                                            (23)\n                                         \u2202t\nwhere c\u2217(\u00b7) is the convex conjugate function of c(\u00b7), and \u2207c\u2217(y) is its gradient at y.\nProof. See Appendix E.\nTheorem 3.4 can be viewed as a special case of Proposition 3.5 where c(\u00b7) = \u2225\u00b7\u22252. Introducing\na different c than L2 norm translates to a non-linear transformation of the gradient \u2212\u2207xEloc(x).\nIn Appendix E, we demonstrate how to choose c such that it corresponds to the coordinate-wise\napplication of tanh to the gradient, which we use in practice.\nFinally, using Proposition 3.5 in Eq. (17), we get the expression for the parameter update, i.e.\n                          \u2206\u03b8\u2217    \u221d       qt(x)\u2207\u03b8        \u2207c\u2217(\u22122\u2207xEloc(x)), \u2207x log q(x, \u03b8)                     dx.              (24)\nSimilar to the discussion of the previous section for QVMC, we can precondition the gradient with\nthe Fisher Information Matrix, exploiting the geometry of the parametric manifold.\nIn Algorithm 1, we provide a pseudocode for the proposed algorithms. The procedure follows closely\nQVMC but introduces a different objective. When using gradients both from Eqs. (18) and (24),\nwe follow the gradient flow under the Wasserstein Fisher-Rao metric with the coeffi                               cient \u03bb. For\n\u03bb \u2192    \u221e, the cost of mass teleportation becomes infinite and we use only the gradient from Eq. (24),\nwhich corresponds to the gradient flow under the c-Wasserstein metric (we refer to this algorithm as\nWQMC). For \u03bb \u2192             0, the cost of mass teleportation becomes negligible compared to the transportation\ncost and the resulting algorithm becomes QVMC, which uses the gradient from Eq. (18). In practice,\nwe consider the extreme cases (\u03bb \u2192                  0, \u221e) and the mixed case \u03bb = 1.\nAlgorithm 1 W(FR)QMC\nRequire: samples {x(i)}N            i=1 \u223c     qt=0(x)\nRequire: potential function V (x)\n   while not converged do\n       Eloc(x(i)) = V (x(i)) \u2212           1    x log q(x(i), \u03b8) \u2212        1 \u2207x log q(x(i), \u03b8) 2                      (see Eq. 5)\n                                         4\u22072                            8\n       \u2207xEloc(x(i)) = stop_gradient(\u2207xEloc(x(i)))\n       \u2206\u03b8\u2217    = 1 N    N  i \u2207\u03b8      \u2207c\u2217 \u22122\u2207xEloc(x(i))               , \u2207x log q(x(i), \u03b8)                          (see Eq. 24)\n       Eloc(x(i)) = stop_gradient(Eloc(x(i)))\n       \u2206\u03b8\u2217    += \u2212      1  1   N  i   Eloc(x(i)) \u2212       1   N  j Eloc(x(j))       \u2207\u03b8 log q(x(i), \u03b8)              (see Eq. 18)\n                        \u03bb N                              N\n       \u03b8\u2032 = optimizer(\u03b8, F\u22121       \u03b8 \u2206\u03b8\u2217)                                                                         (see Eq. 19)\n       update x(i) by sampling from q(x, \u03b8\u2032) via MCMC\n   end while\n   return model q(x, \u03b8\u2217), samples {x(i)}N                  i=1 \u223c    q(x, \u03b8\u2217)\n                                                                     7", "md": "# Math Equations and Text\n\n## Proof:\n\nIn Theorem 3.1, we show that $$\\frac{\\delta E[q]}{\\delta q} = E_{\\text{loc}}(x)$$. Plugging this into the 2-Wasserstein gradient flow defined in Eq. (9), yields the result in Eq. (20).\n\n## c-Wasserstein Metric\n\nThis result can be further generalized to the c-Wasserstein metric with any convex cost function $$c : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ on the tangent space. The c-Wasserstein distance between $$p_0$$ and $$p_1$$ is defined as follows:\n\n$$\nW^2_c(p_0, p_1) := \\inf_{v_t, q_t} \\int_{0}^{1} E_{q_t(x)}[c(v_t(x))] dt, \\text{ subj. to }\n$$\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)), \\text{ and } q_0 = p_0, q_1 = p_1. \\text{ (21)}\n$$\n\nProposition 3.5: The energy-minimizing c-Wasserstein gradient flow is defined by the following equation:\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)\\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x))), \\text{ (23)}\n$$\nwhere $$c^*(\\cdot)$$ is the convex conjugate function of $$c(\\cdot)$$, and $$\\nabla c^*(y)$$ is its gradient at $$y$$.\n\n## Proof:\n\nSee Appendix E.\n\nTheorem 3.4 can be viewed as a special case of Proposition 3.5 where $$c(\\cdot) = \\|\\cdot\\|^2$$. Introducing a different $$c$$ than L2 norm translates to a non-linear transformation of the gradient $$-\\nabla_x E_{\\text{loc}}(x)$$. In Appendix E, we demonstrate how to choose $$c$$ such that it corresponds to the coordinate-wise application of $$\\text{tanh}$$ to the gradient, which we use in practice.\n\nFinally, using Proposition 3.5 in Eq. (17), we get the expression for the parameter update, i.e.\n\n$$\n\\Delta \\theta^* \\propto \\int q_t(x)\\nabla \\theta \\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x)) \\nabla_x \\log q(x, \\theta) dx. \\text{ (24)}\n$$\n\nSimilar to the discussion of the previous section for QVMC, we can precondition the gradient with the Fisher Information Matrix, exploiting the geometry of the parametric manifold.\n\nIn Algorithm 1, we provide a pseudocode for the proposed algorithms. The procedure follows closely QVMC but introduces a different objective. When using gradients both from Eqs. (18) and (24), we follow the gradient flow under the Wasserstein Fisher-Rao metric with the coefficient $$\\lambda$$. For $$\\lambda \\rightarrow \\infty$$, the cost of mass teleportation becomes infinite and we use only the gradient from Eq. (24), which corresponds to the gradient flow under the c-Wasserstein metric (we refer to this algorithm as WQMC). For $$\\lambda \\rightarrow 0$$, the cost of mass teleportation becomes negligible compared to the transportation cost and the resulting algorithm becomes QVMC, which uses the gradient from Eq. (18). In practice, we consider the extreme cases ($$\\lambda \\rightarrow 0, \\infty$$) and the mixed case $$\\lambda = 1$$.\n\nAlgorithm 1 W(FR)QMC\n\nRequire: samples {x(i)}N i=1 ~ qt=0(x)\n\nRequire: potential function V(x)\n\nwhile not converged do\n\nE_{\\text{loc}}(x(i)) = V(x(i)) - \\frac{1}{4} \\nabla^2 \\left( x \\log q(x(i), \\theta) - \\frac{1}{8} \\nabla_x \\log q(x(i), \\theta) \\right)^2 \\text{ (see Eq. 5)}\n\n\\nabla_x E_{\\text{loc}}(x(i)) = \\text{stop\\_gradient}(\\nabla_x E_{\\text{loc}}(x(i)))\n\n\\Delta \\theta^* = \\frac{1}{N} \\sum_{i} \\nabla \\theta \\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x(i))), \\nabla_x \\log q(x(i), \\theta) \\text{ (see Eq. 24)}\n\nE_{\\text{loc}}(x(i)) = \\text{stop\\_gradient}(E_{\\text{loc}}(x(i)))\n\n\\Delta \\theta^* += -\\frac{1}{\\lambda N} \\sum_{i} \\left( E_{\\text{loc}}(x(i)) - \\frac{1}{N} \\sum_{j} E_{\\text{loc}}(x(j)) \\right) \\nabla \\theta \\log q(x(i), \\theta) \\text{ (see Eq. 18)}\n\n\\theta' = \\text{optimizer}(\\theta, F^{-1}_{\\theta} \\Delta \\theta^*) \\text{ (see Eq. 19)}\n\nupdate x(i) by sampling from q(x, \\theta') via MCMC\n\nend while\n\nreturn model q(x, \\theta^*), samples {x(i)}N i=1 ~ q(x, \\theta^*)", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Proof:", "md": "## Proof:"}, {"type": "text", "value": "In Theorem 3.1, we show that $$\\frac{\\delta E[q]}{\\delta q} = E_{\\text{loc}}(x)$$. Plugging this into the 2-Wasserstein gradient flow defined in Eq. (9), yields the result in Eq. (20).", "md": "In Theorem 3.1, we show that $$\\frac{\\delta E[q]}{\\delta q} = E_{\\text{loc}}(x)$$. Plugging this into the 2-Wasserstein gradient flow defined in Eq. (9), yields the result in Eq. (20)."}, {"type": "heading", "lvl": 2, "value": "c-Wasserstein Metric", "md": "## c-Wasserstein Metric"}, {"type": "text", "value": "This result can be further generalized to the c-Wasserstein metric with any convex cost function $$c : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ on the tangent space. The c-Wasserstein distance between $$p_0$$ and $$p_1$$ is defined as follows:\n\n$$\nW^2_c(p_0, p_1) := \\inf_{v_t, q_t} \\int_{0}^{1} E_{q_t(x)}[c(v_t(x))] dt, \\text{ subj. to }\n$$\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)), \\text{ and } q_0 = p_0, q_1 = p_1. \\text{ (21)}\n$$\n\nProposition 3.5: The energy-minimizing c-Wasserstein gradient flow is defined by the following equation:\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)\\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x))), \\text{ (23)}\n$$\nwhere $$c^*(\\cdot)$$ is the convex conjugate function of $$c(\\cdot)$$, and $$\\nabla c^*(y)$$ is its gradient at $$y$$.", "md": "This result can be further generalized to the c-Wasserstein metric with any convex cost function $$c : \\mathbb{R}^d \\rightarrow \\mathbb{R}$$ on the tangent space. The c-Wasserstein distance between $$p_0$$ and $$p_1$$ is defined as follows:\n\n$$\nW^2_c(p_0, p_1) := \\inf_{v_t, q_t} \\int_{0}^{1} E_{q_t(x)}[c(v_t(x))] dt, \\text{ subj. to }\n$$\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)), \\text{ and } q_0 = p_0, q_1 = p_1. \\text{ (21)}\n$$\n\nProposition 3.5: The energy-minimizing c-Wasserstein gradient flow is defined by the following equation:\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)\\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x))), \\text{ (23)}\n$$\nwhere $$c^*(\\cdot)$$ is the convex conjugate function of $$c(\\cdot)$$, and $$\\nabla c^*(y)$$ is its gradient at $$y$$."}, {"type": "heading", "lvl": 2, "value": "Proof:", "md": "## Proof:"}, {"type": "text", "value": "See Appendix E.\n\nTheorem 3.4 can be viewed as a special case of Proposition 3.5 where $$c(\\cdot) = \\|\\cdot\\|^2$$. Introducing a different $$c$$ than L2 norm translates to a non-linear transformation of the gradient $$-\\nabla_x E_{\\text{loc}}(x)$$. In Appendix E, we demonstrate how to choose $$c$$ such that it corresponds to the coordinate-wise application of $$\\text{tanh}$$ to the gradient, which we use in practice.\n\nFinally, using Proposition 3.5 in Eq. (17), we get the expression for the parameter update, i.e.\n\n$$\n\\Delta \\theta^* \\propto \\int q_t(x)\\nabla \\theta \\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x)) \\nabla_x \\log q(x, \\theta) dx. \\text{ (24)}\n$$\n\nSimilar to the discussion of the previous section for QVMC, we can precondition the gradient with the Fisher Information Matrix, exploiting the geometry of the parametric manifold.\n\nIn Algorithm 1, we provide a pseudocode for the proposed algorithms. The procedure follows closely QVMC but introduces a different objective. When using gradients both from Eqs. (18) and (24), we follow the gradient flow under the Wasserstein Fisher-Rao metric with the coefficient $$\\lambda$$. For $$\\lambda \\rightarrow \\infty$$, the cost of mass teleportation becomes infinite and we use only the gradient from Eq. (24), which corresponds to the gradient flow under the c-Wasserstein metric (we refer to this algorithm as WQMC). For $$\\lambda \\rightarrow 0$$, the cost of mass teleportation becomes negligible compared to the transportation cost and the resulting algorithm becomes QVMC, which uses the gradient from Eq. (18). In practice, we consider the extreme cases ($$\\lambda \\rightarrow 0, \\infty$$) and the mixed case $$\\lambda = 1$$.\n\nAlgorithm 1 W(FR)QMC\n\nRequire: samples {x(i)}N i=1 ~ qt=0(x)\n\nRequire: potential function V(x)\n\nwhile not converged do\n\nE_{\\text{loc}}(x(i)) = V(x(i)) - \\frac{1}{4} \\nabla^2 \\left( x \\log q(x(i), \\theta) - \\frac{1}{8} \\nabla_x \\log q(x(i), \\theta) \\right)^2 \\text{ (see Eq. 5)}\n\n\\nabla_x E_{\\text{loc}}(x(i)) = \\text{stop\\_gradient}(\\nabla_x E_{\\text{loc}}(x(i)))\n\n\\Delta \\theta^* = \\frac{1}{N} \\sum_{i} \\nabla \\theta \\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x(i))), \\nabla_x \\log q(x(i), \\theta) \\text{ (see Eq. 24)}\n\nE_{\\text{loc}}(x(i)) = \\text{stop\\_gradient}(E_{\\text{loc}}(x(i)))\n\n\\Delta \\theta^* += -\\frac{1}{\\lambda N} \\sum_{i} \\left( E_{\\text{loc}}(x(i)) - \\frac{1}{N} \\sum_{j} E_{\\text{loc}}(x(j)) \\right) \\nabla \\theta \\log q(x(i), \\theta) \\text{ (see Eq. 18)}\n\n\\theta' = \\text{optimizer}(\\theta, F^{-1}_{\\theta} \\Delta \\theta^*) \\text{ (see Eq. 19)}\n\nupdate x(i) by sampling from q(x, \\theta') via MCMC\n\nend while\n\nreturn model q(x, \\theta^*), samples {x(i)}N i=1 ~ q(x, \\theta^*)", "md": "See Appendix E.\n\nTheorem 3.4 can be viewed as a special case of Proposition 3.5 where $$c(\\cdot) = \\|\\cdot\\|^2$$. Introducing a different $$c$$ than L2 norm translates to a non-linear transformation of the gradient $$-\\nabla_x E_{\\text{loc}}(x)$$. In Appendix E, we demonstrate how to choose $$c$$ such that it corresponds to the coordinate-wise application of $$\\text{tanh}$$ to the gradient, which we use in practice.\n\nFinally, using Proposition 3.5 in Eq. (17), we get the expression for the parameter update, i.e.\n\n$$\n\\Delta \\theta^* \\propto \\int q_t(x)\\nabla \\theta \\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x)) \\nabla_x \\log q(x, \\theta) dx. \\text{ (24)}\n$$\n\nSimilar to the discussion of the previous section for QVMC, we can precondition the gradient with the Fisher Information Matrix, exploiting the geometry of the parametric manifold.\n\nIn Algorithm 1, we provide a pseudocode for the proposed algorithms. The procedure follows closely QVMC but introduces a different objective. When using gradients both from Eqs. (18) and (24), we follow the gradient flow under the Wasserstein Fisher-Rao metric with the coefficient $$\\lambda$$. For $$\\lambda \\rightarrow \\infty$$, the cost of mass teleportation becomes infinite and we use only the gradient from Eq. (24), which corresponds to the gradient flow under the c-Wasserstein metric (we refer to this algorithm as WQMC). For $$\\lambda \\rightarrow 0$$, the cost of mass teleportation becomes negligible compared to the transportation cost and the resulting algorithm becomes QVMC, which uses the gradient from Eq. (18). In practice, we consider the extreme cases ($$\\lambda \\rightarrow 0, \\infty$$) and the mixed case $$\\lambda = 1$$.\n\nAlgorithm 1 W(FR)QMC\n\nRequire: samples {x(i)}N i=1 ~ qt=0(x)\n\nRequire: potential function V(x)\n\nwhile not converged do\n\nE_{\\text{loc}}(x(i)) = V(x(i)) - \\frac{1}{4} \\nabla^2 \\left( x \\log q(x(i), \\theta) - \\frac{1}{8} \\nabla_x \\log q(x(i), \\theta) \\right)^2 \\text{ (see Eq. 5)}\n\n\\nabla_x E_{\\text{loc}}(x(i)) = \\text{stop\\_gradient}(\\nabla_x E_{\\text{loc}}(x(i)))\n\n\\Delta \\theta^* = \\frac{1}{N} \\sum_{i} \\nabla \\theta \\nabla c^*(-2\\nabla_x E_{\\text{loc}}(x(i))), \\nabla_x \\log q(x(i), \\theta) \\text{ (see Eq. 24)}\n\nE_{\\text{loc}}(x(i)) = \\text{stop\\_gradient}(E_{\\text{loc}}(x(i)))\n\n\\Delta \\theta^* += -\\frac{1}{\\lambda N} \\sum_{i} \\left( E_{\\text{loc}}(x(i)) - \\frac{1}{N} \\sum_{j} E_{\\text{loc}}(x(j)) \\right) \\nabla \\theta \\log q(x(i), \\theta) \\text{ (see Eq. 18)}\n\n\\theta' = \\text{optimizer}(\\theta, F^{-1}_{\\theta} \\Delta \\theta^*) \\text{ (see Eq. 19)}\n\nupdate x(i) by sampling from q(x, \\theta') via MCMC\n\nend while\n\nreturn model q(x, \\theta^*), samples {x(i)}N i=1 ~ q(x, \\theta^*)"}]}, {"page": 8, "text": "     1e-3                 Be (4)                     1e-3                  B (5)                     1e-3                 Li2 (6)                                       H10 (10)\n                                QVMC                                            QVMC                                           QVMC                                            QVMC\n                                W(FR)QMC                                        W(FR)VMC                                       W(FR)QMC              1e-3                      W(FR)QMC\n    Relative energy error       WQMC                                            WVMC                 5e-4                      WQMC                                            WQMC\n                                chemical accuracy                               chemical accuracy                              chemical accuracy                               chemical accuracy\n     1e-4                                            1e-4                                                                                            6e-4\n                                                                                                     1e-4                                            3e-4\n     1e-5                                            1e-5\n     1e-6                                            1e-6                                            2e-5                                            1e-4\n                                       QVMC           3e0                              QVMC           1e0                              QVMC          4e-2                             QVMC\n     5e-1                              W(FR)QMC                                        W(FR)VMC                                        W(FR)QMC                                       W(FR)QMC\n    Energy variance (Ha2)              WQMC           1e0                              WVMC          1e-1                              WQMC          1e-2                             WQMC\n     1e-1\n                                                     1e-1                                            1e-2\n     1e-2                                                                                                                                            1e-3\n                                                                                                     1e-3\n     1e-3                                            1e-2                                                                                            2e-4\n                                       QVMC                                            QVMC           2e0                              QVMC          5e-1                             QVMC\n      2e0                              W(FR)QMC       5e0                              W(FR)VMC                                        W(FR)QMC                                       W(FR)QMC\n                                       WQMC                                            WVMC                                            WQMC                                           WQMC\n    Gradient norm                                                                                    8e-1\n      1e0                                             2e0                                                                                            2e-1\n     3e-1                                                                                            3e-1\n                                                     5e-1                                                                                            8e-2\n     1e-1     2000    4000    6000    8000    10000  2e-1     2000    4000    6000    8000    10000  1e-1     2000    4000    6000    8000   10000   4e-2  2000     6500    11000    15500   20000\nFigure 2: Optimization results for different chemical systems (every column corresponds to a given\nmolecule). The number of electrons is given in the brackets next to systems\u2019 names. Throughout\nthe optimization, we monitor three values: the mean value of the local energy (lower is better), the\nvariance of the local energy, and the median value of the gradient norm of the local energy. In the first\nrow of plots, we average (removing 5% of outliers from both sides) the energy over 1000 iterations\nand report the relative error to the actual ground-state energy: (E \u2212                                                                 E0)/E0. In the second row, we\nreport standard deviation averaged over 1000 iterations (removing 5% of outliers from both sides). In\nthe third row, we report the median gradient norm averaged over 1000 iterations (removing 5% of\noutliers from both sides). See the descriptions of methods in the text.\n4        Experiments 1\nFor the empirical study of the proposed method, we consider Born\u2013Oppenheimer approximation of\nchemical systems. Within this approximation, the wave function of the electrons in a molecule can be\nstudied separately from the wave function of the atomic nuclei. Namely, we consider the following\nHamiltonian\n                            H = \u2212           1      x +                       1                                  ZI                                 ZI    ZJ                                 (25)\n                                            2  \u22072           i<j     \u2225xi \u2212        xj\u2225      \u2212     i,I     \u2225xi \u2212        XI    \u2225   +    I<J      \u2225XI \u2212          XJ     \u2225   ,\nwhere xi are the coordinates of electrons, XI                                           , ZI are the coordinates and charges of nuclei. The first\nkinetic term contains derivatives with respect to the electron positions x. Indeed, the positions of the\nnuclei are given and fixed, and we target the ground state of the electronic wave function \u03c8(x), which\nis an explicit function of the electron positions only. Solving the electronic Schr\u00f6dinger equation\nis a notoriously difficult task, and is a topic of intense research in quantum chemistry and material\nsciences.\n      1 Code reproducing experiments is available at github.com/necludov/wqmc. The method is also implemented\nin the FermiNet library: github.com/google-deepmind/ferminet.\n                                                                                                 8", "md": "# Chemical Systems Optimization Results\n\n## Chemical Systems Optimization Results\n\nFigure 2: Optimization results for different chemical systems (every column corresponds to a given molecule). The number of electrons is given in the brackets next to systems\u2019 names. Throughout the optimization, we monitor three values: the mean value of the local energy (lower is better), the variance of the local energy, and the median value of the gradient norm of the local energy. In the first row of plots, we average (removing 5% of outliers from both sides) the energy over 1000 iterations and report the relative error to the actual ground-state energy: $$\\left(\\frac{E - E_0}{E_0}\\right)$$. In the second row, we report standard deviation averaged over 1000 iterations (removing 5% of outliers from both sides). In the third row, we report the median gradient norm averaged over 1000 iterations (removing 5% of outliers from both sides). See the descriptions of methods in the text.\n\n### Experiments 1\n\nFor the empirical study of the proposed method, we consider Born\u2013Oppenheimer approximation of chemical systems. Within this approximation, the wave function of the electrons in a molecule can be studied separately from the wave function of the atomic nuclei. Namely, we consider the following Hamiltonian:\n\n$$H = -\\sum_{i<j} \\frac{1}{2} \\nabla^2 \\left\\| \\mathbf{x}_i - \\mathbf{x}_j \\right\\| - \\sum_{i,I} \\frac{Z_I}{\\left\\| \\mathbf{x}_i - \\mathbf{X}_I \\right\\|} + \\sum_{I<J} \\frac{Z_I Z_J}{\\left\\| \\mathbf{X}_I - \\mathbf{X}_J \\right\\|},$$\n\nwhere $$\\mathbf{x}_i$$ are the coordinates of electrons, $$\\mathbf{X}_I$$, $$Z_I$$ are the coordinates and charges of nuclei. The first kinetic term contains derivatives with respect to the electron positions $$\\mathbf{x}$$. Indeed, the positions of the nuclei are given and fixed, and we target the ground state of the electronic wave function $$\\psi(\\mathbf{x})$$, which is an explicit function of the electron positions only. Solving the electronic Schr\u00f6dinger equation is a notoriously difficult task, and is a topic of intense research in quantum chemistry and material sciences.\n\n1 Code reproducing experiments is available at github.com/necludov/wqmc. The method is also implemented in the FermiNet library: github.com/google-deepmind/ferminet.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Chemical Systems Optimization Results", "md": "# Chemical Systems Optimization Results"}, {"type": "heading", "lvl": 2, "value": "Chemical Systems Optimization Results", "md": "## Chemical Systems Optimization Results"}, {"type": "text", "value": "Figure 2: Optimization results for different chemical systems (every column corresponds to a given molecule). The number of electrons is given in the brackets next to systems\u2019 names. Throughout the optimization, we monitor three values: the mean value of the local energy (lower is better), the variance of the local energy, and the median value of the gradient norm of the local energy. In the first row of plots, we average (removing 5% of outliers from both sides) the energy over 1000 iterations and report the relative error to the actual ground-state energy: $$\\left(\\frac{E - E_0}{E_0}\\right)$$. In the second row, we report standard deviation averaged over 1000 iterations (removing 5% of outliers from both sides). In the third row, we report the median gradient norm averaged over 1000 iterations (removing 5% of outliers from both sides). See the descriptions of methods in the text.", "md": "Figure 2: Optimization results for different chemical systems (every column corresponds to a given molecule). The number of electrons is given in the brackets next to systems\u2019 names. Throughout the optimization, we monitor three values: the mean value of the local energy (lower is better), the variance of the local energy, and the median value of the gradient norm of the local energy. In the first row of plots, we average (removing 5% of outliers from both sides) the energy over 1000 iterations and report the relative error to the actual ground-state energy: $$\\left(\\frac{E - E_0}{E_0}\\right)$$. In the second row, we report standard deviation averaged over 1000 iterations (removing 5% of outliers from both sides). In the third row, we report the median gradient norm averaged over 1000 iterations (removing 5% of outliers from both sides). See the descriptions of methods in the text."}, {"type": "heading", "lvl": 3, "value": "Experiments 1", "md": "### Experiments 1"}, {"type": "text", "value": "For the empirical study of the proposed method, we consider Born\u2013Oppenheimer approximation of chemical systems. Within this approximation, the wave function of the electrons in a molecule can be studied separately from the wave function of the atomic nuclei. Namely, we consider the following Hamiltonian:\n\n$$H = -\\sum_{i<j} \\frac{1}{2} \\nabla^2 \\left\\| \\mathbf{x}_i - \\mathbf{x}_j \\right\\| - \\sum_{i,I} \\frac{Z_I}{\\left\\| \\mathbf{x}_i - \\mathbf{X}_I \\right\\|} + \\sum_{I<J} \\frac{Z_I Z_J}{\\left\\| \\mathbf{X}_I - \\mathbf{X}_J \\right\\|},$$\n\nwhere $$\\mathbf{x}_i$$ are the coordinates of electrons, $$\\mathbf{X}_I$$, $$Z_I$$ are the coordinates and charges of nuclei. The first kinetic term contains derivatives with respect to the electron positions $$\\mathbf{x}$$. Indeed, the positions of the nuclei are given and fixed, and we target the ground state of the electronic wave function $$\\psi(\\mathbf{x})$$, which is an explicit function of the electron positions only. Solving the electronic Schr\u00f6dinger equation is a notoriously difficult task, and is a topic of intense research in quantum chemistry and material sciences.\n\n1 Code reproducing experiments is available at github.com/necludov/wqmc. The method is also implemented in the FermiNet library: github.com/google-deepmind/ferminet.", "md": "For the empirical study of the proposed method, we consider Born\u2013Oppenheimer approximation of chemical systems. Within this approximation, the wave function of the electrons in a molecule can be studied separately from the wave function of the atomic nuclei. Namely, we consider the following Hamiltonian:\n\n$$H = -\\sum_{i<j} \\frac{1}{2} \\nabla^2 \\left\\| \\mathbf{x}_i - \\mathbf{x}_j \\right\\| - \\sum_{i,I} \\frac{Z_I}{\\left\\| \\mathbf{x}_i - \\mathbf{X}_I \\right\\|} + \\sum_{I<J} \\frac{Z_I Z_J}{\\left\\| \\mathbf{X}_I - \\mathbf{X}_J \\right\\|},$$\n\nwhere $$\\mathbf{x}_i$$ are the coordinates of electrons, $$\\mathbf{X}_I$$, $$Z_I$$ are the coordinates and charges of nuclei. The first kinetic term contains derivatives with respect to the electron positions $$\\mathbf{x}$$. Indeed, the positions of the nuclei are given and fixed, and we target the ground state of the electronic wave function $$\\psi(\\mathbf{x})$$, which is an explicit function of the electron positions only. Solving the electronic Schr\u00f6dinger equation is a notoriously difficult task, and is a topic of intense research in quantum chemistry and material sciences.\n\n1 Code reproducing experiments is available at github.com/necludov/wqmc. The method is also implemented in the FermiNet library: github.com/google-deepmind/ferminet."}]}, {"page": 9, "text": "       Method name           QVMC       WQMC       W(FR)QMC         QVMC       WQMC       W(FR)QMC\n         Molecule                           Be (4)                                 B (5)\n   Relative energy error     1.50e-5    3.79e-6       1.04e-6       9.01e-6    9.58e-6       2.69e-6\n  Energy variance (Ha2)      5.53e-3    1.08e-3       1.07e-3       1.96e-2    1.84e-2       1.19e-2\n         Molecule                          Li2 (6)                               H10 (10)\n   Relative energy error     4.43e-5    3.71e-5       3.66e-5       4.24e-4    3.90e-4       3.88e-4\n  Energy variance (Ha2)      7.21e-3    5.76e-4       5.59e-4       1.89e-3    2.30e-4       2.45e-4\n     Table 1: Energy and variances estimates for all systems after 10k iterations (20k for H10).\nSince electrons are indistinguishable fermions, we restrict the Hilbert space to states \u03c8 that are\nantisymmetric under electron permutations (see Section 2.1). This can be achieved by incorporating\nSlater determinants into the deep neural network, which parametrizes the wave function \u03c8(x, \u03b8),\nas proposed in various recent works (Luo & Clark, 2019; Hermann et al., 2020; Pfau et al., 2020;\nHermann et al., 2022). The density is then given by the Born rule q(x, \u03b8) = |\u03c8(x, \u03b8)|2. For all our\nexperiments, we follow (von Glehn et al., 2022) and use the \u201cpsiformer\u201d architecture together with\npreconditioning the gradients via K-FAC (Martens & Grosse, 2015).\nIn our method, we apply several tricks which stabilize the optimization and improve convergence\nspeed. Firstly, we have observed that applying a tanh non-linearity coordinate-wise to the gradient\n\u2207xEloc(x) significantly improves convergence speed. This corresponds to a different cost function in\nthe Wasserstein metric, as we discuss in Proposition 3.5 and Appendix E. Also, we remove samples\nfrom the batch whose norm \u2225\u2207x log q(x, \u03b8)\u2225       significantly exceeds the median value. Namely, we\nestimate the deviation from the norm as Eq(x,\u03b8)[|\u2225\u2207x log q(x, \u03b8)\u2225    \u2212  median(\u2225\u2207x log q(x, \u03b8)\u2225)|] and\nremove samples whose norm exceeds fi      ve deviations from the median. When including the gradient\nfrom Eq. (18), we clip the local energy values as proposed in (von Glehn et al., 2022), i.e. by\nestimating the median value and clipping to fi  ve deviations from the median, where the deviation is\nestimated in the same way as for the norm of the gradient.\nWe consider different chemical systems and compare against QVMC as a baseline. We run our novel\nmethod with the same architecture and hyperparameters as the baseline QVMC-based approach in\n(von Glehn et al., 2022). For the chemical systems, we consider Be, and B atoms, the Li2 molecule\nand the hydrogen chain H10 from (Hermann et al., 2020). The exact values of energies for Be, B,\nLi2 are taken from (Pfau et al., 2020), the exact value of the energy for H10 is from (Hermann et al.,\n2020). All the hyperparameters and architectural details are provided in the supplementary material.\nIn Figure 2, we demonstrate the convergence plots for the baseline (QVMC) and the proposed\nmethods (WQMC and W(FR)QM, see Algorithm 1). For all the considered systems, both WQMC\nand W(FR)QMC yield more precise estimations of the ground state energy (the first row of Figure 2).\nTo assess convergence, we also monitor the variance of the local energy and the gradient norm\nof the local energy. As we discuss in Eq. (11), both metrics must vanish at the ground state.\nMore fundamentally, the variance of the local energy can be shown to vanish for eigenstates of\nthe Hamiltonian (Wu et al., 2023), referred to as the zero-variance property. First, we point out\nthat obtaining the ground states of the considered molecules with QVMC is challenging, and even\nwith powerful deep-learning architecture, discrepancies remain with the ground state. Since we use\nexisting state-of-the-art architectures as a backbone, our results are also limited by the limitations of\nthe latter (Gao & G\u00fcnnemann, 2023). Developing novel architectures is out of the scope of this work.\nHowever, in Figure 2, we clearly observe that both WQMC and W(FR)QMC yield significantly\nfaster convergence of the aforementioned metrics compared to QVMC. In particular, for the larger\nmolecules Li2 and H10, we observe that we consistently obtain lower energies within 10k steps (20k\nfor H10) with a more stable convergence. For the smaller molecules we observe that QVMC obtains\nlower energies in the first few iterations, but its convergence slows down significantly, after which our\napproach steadily yields improved energies below QVMC. Overall, our experiments demonstrates that\ntaking into account the Wasserstein metric allows for faster convergence to accurate approximations\nof the ground state. See the final metrics in Table 1.\n                                                    9", "md": "# Document\n\n## Table 1: Energy and variances estimates for all systems after 10k iterations (20k for H10).\n\n|Method name|QVMC|WQMC|W(FR)QMC|QVMC|WQMC|W(FR)QMC|\n|---|---|---|---|---|---|---|\n|Molecule|Be (4)| | |B (5)| | |\n|Relative energy error|1.50e-5|3.79e-6|1.04e-6|9.01e-6|9.58e-6|2.69e-6|\n|Energy variance (Ha2)|5.53e-3|1.08e-3|1.07e-3|1.96e-2|1.84e-2|1.19e-2|\n|Molecule|Li2 (6)| | |H10 (10)| | |\n|Relative energy error|4.43e-5|3.71e-5|3.66e-5|4.24e-4|3.90e-4|3.88e-4|\n|Energy variance (Ha2)|7.21e-3|5.76e-4|5.59e-4|1.89e-3|2.30e-4|2.45e-4|\n\nSince electrons are indistinguishable fermions, we restrict the Hilbert space to states \u03c8 that are antisymmetric under electron permutations (see Section 2.1). This can be achieved by incorporating Slater determinants into the deep neural network, which parametrizes the wave function \u03c8(x, \u03b8), as proposed in various recent works (Luo & Clark, 2019; Hermann et al., 2020; Pfau et al., 2020; Hermann et al., 2022). The density is then given by the Born rule q(x, \u03b8) = |\u03c8(x, \u03b8)|2. For all our experiments, we follow (von Glehn et al., 2022) and use the \u201cpsiformer\u201d architecture together with preconditioning the gradients via K-FAC (Martens & Grosse, 2015).\n\nIn our method, we apply several tricks which stabilize the optimization and improve convergence speed. Firstly, we have observed that applying a tanh non-linearity coordinate-wise to the gradient \u2207xEloc(x) significantly improves convergence speed. This corresponds to a different cost function in the Wasserstein metric, as we discuss in Proposition 3.5 and Appendix E. Also, we remove samples from the batch whose norm \u2225\u2207x log q(x, \u03b8)\u2225 significantly exceeds the median value. Namely, we estimate the deviation from the norm as Eq(x,\u03b8)[|\u2225\u2207x log q(x, \u03b8)\u2225 - median(\u2225\u2207x log q(x, \u03b8)\u2225)|] and remove samples whose norm exceeds five deviations from the median. When including the gradient from Eq. (18), we clip the local energy values as proposed in (von Glehn et al., 2022), i.e. by estimating the median value and clipping to five deviations from the median, where the deviation is estimated in the same way as for the norm of the gradient.\n\nWe consider different chemical systems and compare against QVMC as a baseline. We run our novel method with the same architecture and hyperparameters as the baseline QVMC-based approach in (von Glehn et al., 2022). For the chemical systems, we consider Be, and B atoms, the Li2 molecule and the hydrogen chain H10 from (Hermann et al., 2020). The exact values of energies for Be, B, Li2 are taken from (Pfau et al., 2020), the exact value of the energy for H10 is from (Hermann et al., 2020). All the hyperparameters and architectural details are provided in the supplementary material.\n\nIn Figure 2, we demonstrate the convergence plots for the baseline (QVMC) and the proposed methods (WQMC and W(FR)QMC, see Algorithm 1). For all the considered systems, both WQMC and W(FR)QMC yield more precise estimations of the ground state energy (the first row of Figure 2). To assess convergence, we also monitor the variance of the local energy and the gradient norm of the local energy. As we discuss in Eq. (11), both metrics must vanish at the ground state. More fundamentally, the variance of the local energy can be shown to vanish for eigenstates of the Hamiltonian (Wu et al., 2023), referred to as the zero-variance property. First, we point out that obtaining the ground states of the considered molecules with QVMC is challenging, and even with powerful deep-learning architecture, discrepancies remain with the ground state. Since we use existing state-of-the-art architectures as a backbone, our results are also limited by the limitations of the latter (Gao & G\u00fcnnemann, 2023). Developing novel architectures is out of the scope of this work. However, in Figure 2, we clearly observe that both WQMC and W(FR)QMC yield significantly faster convergence of the aforementioned metrics compared to QVMC. In particular, for the larger molecules Li2 and H10, we observe that we consistently obtain lower energies within 10k steps (20k for H10) with a more stable convergence. For the smaller molecules we observe that QVMC obtains lower energies in the first few iterations, but its convergence slows down significantly, after which our approach steadily yields improved energies below QVMC. Overall, our experiments demonstrates that taking into account the Wasserstein metric allows for faster convergence to accurate approximations of the ground state. See the final metrics in Table 1.\n\n9", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Table 1: Energy and variances estimates for all systems after 10k iterations (20k for H10).", "md": "## Table 1: Energy and variances estimates for all systems after 10k iterations (20k for H10)."}, {"type": "table", "rows": [["Method name", "QVMC", "WQMC", "W(FR)QMC", "QVMC", "WQMC", "W(FR)QMC"], ["Molecule", "Be (4)", "", "", "B (5)", "", ""], ["Relative energy error", "1.50e-5", "3.79e-6", "1.04e-6", "9.01e-6", "9.58e-6", "2.69e-6"], ["Energy variance (Ha2)", "5.53e-3", "1.08e-3", "1.07e-3", "1.96e-2", "1.84e-2", "1.19e-2"], ["Molecule", "Li2 (6)", "", "", "H10 (10)", "", ""], ["Relative energy error", "4.43e-5", "3.71e-5", "3.66e-5", "4.24e-4", "3.90e-4", "3.88e-4"], ["Energy variance (Ha2)", "7.21e-3", "5.76e-4", "5.59e-4", "1.89e-3", "2.30e-4", "2.45e-4"]], "md": "|Method name|QVMC|WQMC|W(FR)QMC|QVMC|WQMC|W(FR)QMC|\n|---|---|---|---|---|---|---|\n|Molecule|Be (4)| | |B (5)| | |\n|Relative energy error|1.50e-5|3.79e-6|1.04e-6|9.01e-6|9.58e-6|2.69e-6|\n|Energy variance (Ha2)|5.53e-3|1.08e-3|1.07e-3|1.96e-2|1.84e-2|1.19e-2|\n|Molecule|Li2 (6)| | |H10 (10)| | |\n|Relative energy error|4.43e-5|3.71e-5|3.66e-5|4.24e-4|3.90e-4|3.88e-4|\n|Energy variance (Ha2)|7.21e-3|5.76e-4|5.59e-4|1.89e-3|2.30e-4|2.45e-4|", "isPerfectTable": true, "csv": "\"Method name\",\"QVMC\",\"WQMC\",\"W(FR)QMC\",\"QVMC\",\"WQMC\",\"W(FR)QMC\"\n\"Molecule\",\"Be (4)\",\"\",\"\",\"B (5)\",\"\",\"\"\n\"Relative energy error\",\"1.50e-5\",\"3.79e-6\",\"1.04e-6\",\"9.01e-6\",\"9.58e-6\",\"2.69e-6\"\n\"Energy variance (Ha2)\",\"5.53e-3\",\"1.08e-3\",\"1.07e-3\",\"1.96e-2\",\"1.84e-2\",\"1.19e-2\"\n\"Molecule\",\"Li2 (6)\",\"\",\"\",\"H10 (10)\",\"\",\"\"\n\"Relative energy error\",\"4.43e-5\",\"3.71e-5\",\"3.66e-5\",\"4.24e-4\",\"3.90e-4\",\"3.88e-4\"\n\"Energy variance (Ha2)\",\"7.21e-3\",\"5.76e-4\",\"5.59e-4\",\"1.89e-3\",\"2.30e-4\",\"2.45e-4\""}, {"type": "text", "value": "Since electrons are indistinguishable fermions, we restrict the Hilbert space to states \u03c8 that are antisymmetric under electron permutations (see Section 2.1). This can be achieved by incorporating Slater determinants into the deep neural network, which parametrizes the wave function \u03c8(x, \u03b8), as proposed in various recent works (Luo & Clark, 2019; Hermann et al., 2020; Pfau et al., 2020; Hermann et al., 2022). The density is then given by the Born rule q(x, \u03b8) = |\u03c8(x, \u03b8)|2. For all our experiments, we follow (von Glehn et al., 2022) and use the \u201cpsiformer\u201d architecture together with preconditioning the gradients via K-FAC (Martens & Grosse, 2015).\n\nIn our method, we apply several tricks which stabilize the optimization and improve convergence speed. Firstly, we have observed that applying a tanh non-linearity coordinate-wise to the gradient \u2207xEloc(x) significantly improves convergence speed. This corresponds to a different cost function in the Wasserstein metric, as we discuss in Proposition 3.5 and Appendix E. Also, we remove samples from the batch whose norm \u2225\u2207x log q(x, \u03b8)\u2225 significantly exceeds the median value. Namely, we estimate the deviation from the norm as Eq(x,\u03b8)[|\u2225\u2207x log q(x, \u03b8)\u2225 - median(\u2225\u2207x log q(x, \u03b8)\u2225)|] and remove samples whose norm exceeds five deviations from the median. When including the gradient from Eq. (18), we clip the local energy values as proposed in (von Glehn et al., 2022), i.e. by estimating the median value and clipping to five deviations from the median, where the deviation is estimated in the same way as for the norm of the gradient.\n\nWe consider different chemical systems and compare against QVMC as a baseline. We run our novel method with the same architecture and hyperparameters as the baseline QVMC-based approach in (von Glehn et al., 2022). For the chemical systems, we consider Be, and B atoms, the Li2 molecule and the hydrogen chain H10 from (Hermann et al., 2020). The exact values of energies for Be, B, Li2 are taken from (Pfau et al., 2020), the exact value of the energy for H10 is from (Hermann et al., 2020). All the hyperparameters and architectural details are provided in the supplementary material.\n\nIn Figure 2, we demonstrate the convergence plots for the baseline (QVMC) and the proposed methods (WQMC and W(FR)QMC, see Algorithm 1). For all the considered systems, both WQMC and W(FR)QMC yield more precise estimations of the ground state energy (the first row of Figure 2). To assess convergence, we also monitor the variance of the local energy and the gradient norm of the local energy. As we discuss in Eq. (11), both metrics must vanish at the ground state. More fundamentally, the variance of the local energy can be shown to vanish for eigenstates of the Hamiltonian (Wu et al., 2023), referred to as the zero-variance property. First, we point out that obtaining the ground states of the considered molecules with QVMC is challenging, and even with powerful deep-learning architecture, discrepancies remain with the ground state. Since we use existing state-of-the-art architectures as a backbone, our results are also limited by the limitations of the latter (Gao & G\u00fcnnemann, 2023). Developing novel architectures is out of the scope of this work. However, in Figure 2, we clearly observe that both WQMC and W(FR)QMC yield significantly faster convergence of the aforementioned metrics compared to QVMC. In particular, for the larger molecules Li2 and H10, we observe that we consistently obtain lower energies within 10k steps (20k for H10) with a more stable convergence. For the smaller molecules we observe that QVMC obtains lower energies in the first few iterations, but its convergence slows down significantly, after which our approach steadily yields improved energies below QVMC. Overall, our experiments demonstrates that taking into account the Wasserstein metric allows for faster convergence to accurate approximations of the ground state. See the final metrics in Table 1.\n\n9", "md": "Since electrons are indistinguishable fermions, we restrict the Hilbert space to states \u03c8 that are antisymmetric under electron permutations (see Section 2.1). This can be achieved by incorporating Slater determinants into the deep neural network, which parametrizes the wave function \u03c8(x, \u03b8), as proposed in various recent works (Luo & Clark, 2019; Hermann et al., 2020; Pfau et al., 2020; Hermann et al., 2022). The density is then given by the Born rule q(x, \u03b8) = |\u03c8(x, \u03b8)|2. For all our experiments, we follow (von Glehn et al., 2022) and use the \u201cpsiformer\u201d architecture together with preconditioning the gradients via K-FAC (Martens & Grosse, 2015).\n\nIn our method, we apply several tricks which stabilize the optimization and improve convergence speed. Firstly, we have observed that applying a tanh non-linearity coordinate-wise to the gradient \u2207xEloc(x) significantly improves convergence speed. This corresponds to a different cost function in the Wasserstein metric, as we discuss in Proposition 3.5 and Appendix E. Also, we remove samples from the batch whose norm \u2225\u2207x log q(x, \u03b8)\u2225 significantly exceeds the median value. Namely, we estimate the deviation from the norm as Eq(x,\u03b8)[|\u2225\u2207x log q(x, \u03b8)\u2225 - median(\u2225\u2207x log q(x, \u03b8)\u2225)|] and remove samples whose norm exceeds five deviations from the median. When including the gradient from Eq. (18), we clip the local energy values as proposed in (von Glehn et al., 2022), i.e. by estimating the median value and clipping to five deviations from the median, where the deviation is estimated in the same way as for the norm of the gradient.\n\nWe consider different chemical systems and compare against QVMC as a baseline. We run our novel method with the same architecture and hyperparameters as the baseline QVMC-based approach in (von Glehn et al., 2022). For the chemical systems, we consider Be, and B atoms, the Li2 molecule and the hydrogen chain H10 from (Hermann et al., 2020). The exact values of energies for Be, B, Li2 are taken from (Pfau et al., 2020), the exact value of the energy for H10 is from (Hermann et al., 2020). All the hyperparameters and architectural details are provided in the supplementary material.\n\nIn Figure 2, we demonstrate the convergence plots for the baseline (QVMC) and the proposed methods (WQMC and W(FR)QMC, see Algorithm 1). For all the considered systems, both WQMC and W(FR)QMC yield more precise estimations of the ground state energy (the first row of Figure 2). To assess convergence, we also monitor the variance of the local energy and the gradient norm of the local energy. As we discuss in Eq. (11), both metrics must vanish at the ground state. More fundamentally, the variance of the local energy can be shown to vanish for eigenstates of the Hamiltonian (Wu et al., 2023), referred to as the zero-variance property. First, we point out that obtaining the ground states of the considered molecules with QVMC is challenging, and even with powerful deep-learning architecture, discrepancies remain with the ground state. Since we use existing state-of-the-art architectures as a backbone, our results are also limited by the limitations of the latter (Gao & G\u00fcnnemann, 2023). Developing novel architectures is out of the scope of this work. However, in Figure 2, we clearly observe that both WQMC and W(FR)QMC yield significantly faster convergence of the aforementioned metrics compared to QVMC. In particular, for the larger molecules Li2 and H10, we observe that we consistently obtain lower energies within 10k steps (20k for H10) with a more stable convergence. For the smaller molecules we observe that QVMC obtains lower energies in the first few iterations, but its convergence slows down significantly, after which our approach steadily yields improved energies below QVMC. Overall, our experiments demonstrates that taking into account the Wasserstein metric allows for faster convergence to accurate approximations of the ground state. See the final metrics in Table 1.\n\n9"}]}, {"page": 10, "text": "5    Limitations\nSince our method requires an extra gradient evaluation compared to QVMC, we include the runtime\nof all the algorithms in Figure 3, Appendix F. Namely, for a proper comparison, instead of reporting\nthe metrics per iteration, we report them per wall time in seconds. Note that all the claims of the\nexperimental section still hold in terms of wall time. All the models were benchmarked on four A40\nGPUs. Third-order derivatives can be efficiently computed using modern deep learning frameworks\nsuch as JAX (Bradbury et al., 2018), which we used to implement our method.\nPotentially, one can alleviate the extra cost of the iteration by coming up with more efficient Monte\nCarlo schemes or other updates of the samples. Indeed, in our experiments, we observed that the\nproposed method requires fewer MCMC steps (not included in the paper). Moreover, one can use\nthe evaluated gradient of the local energy as the proposal vector field for updating the samples. This\nwould allow to decrease the number of MCMC steps at the low cost of additional hyperparameter\ntuning.\n6    Discussion and conclusion\nConclusion     In the current paper, we propose a novel approach to solving the quantum many-body\nSchr\u00f6dinger equation, by incorporating the Wasserstein metric on the space of Born distributions.\nCompared to the Fisher\u2013Rao metric, which allows for probability mass \u201cteleportation\u201d, the Wasser-\nstein metric constrains the evolution of the density to local changes under the locally-optimal\ntransportation plan, i.e., following fluid dynamics. This property is favorable when the evolution\nof the parametric model is accompanied by the evolution of samples (performed by an MCMC\nalgorithm). Indeed, by forbidding or regularizing non-local mass \u201cteleportation\u201d in the density\nchange, one prevents the appearance of distant modes in the density model, which would require\nlonger MCMC runs for proper mixing of the samples.\nIn practice, we demonstrate that following the gradient flow under the Wasserstein (or Wasserstein\nFisher\u2013Rao) metric results in better convergence to the ground state wave function. This is expected\nto be due to our proposed loss, which takes into account the gradient of the local energy and achieves\nits minimum when the norm of the gradient vanishes, therefore explicitly minimizing the norm of the\nlocal energy gradient.\nWe believe that our new theoretical framework for solving the time-independent Schr\u00f6dinger equation\nfor time-reversal symmetric Hamiltonians based on optimal transport will open new avenues to\ndevelop improved numerical methods for quantum chemistry and physics.\nConnection to Energy-Based and Score-Based Generative Models               The developed ideas of this\npaper, i.e., projecting gradient flows under different metrics onto a parametric family, can be extended\nto generative modeling by swapping the energy functional with the KL-divergence. More precisely,\nas we show in Appendix D, using the KL-divergence as our objective functional, the Fisher\u2013Rao\ngradient flow yields energy-based training scheme, while the 2-Wasserstein gradient flow corresponds\nto the score-matching, which is used for training diffusion generative models.\n7    Acknowledgement\nThe authors thank Rob Brekelmans and anonymous reviewers for helpful discussions and feedback.\nJ.N. was supported by Microsoft Research. J.C. acknowledges support from the Natural Sciences and\nEngineering Research Council (NSERC), the Shared Hierarchical Academic Research Computing\nNetwork (SHARCNET), Compute Canada, and the Canadian Institute for Advanced Research\n(CIFAR) AI Chairs program. A.M. acknowledges support from the Canada CIFAR AI Chairs\nprogram. Resources used in preparing this research were provided, in part, by the Province of\nOntario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute\nwww.vectorinstitute.ai/#partners.\n                                                   10", "md": "# Limitations and Conclusion\n\n## Limitations\n\nSince our method requires an extra gradient evaluation compared to QVMC, we include the runtime\nof all the algorithms in Figure 3, Appendix F. Namely, for a proper comparison, instead of reporting\nthe metrics per iteration, we report them per wall time in seconds. Note that all the claims of the\nexperimental section still hold in terms of wall time. All the models were benchmarked on four A40\nGPUs. Third-order derivatives can be efficiently computed using modern deep learning frameworks\nsuch as JAX (Bradbury et al., 2018), which we used to implement our method.\n\nPotentially, one can alleviate the extra cost of the iteration by coming up with more efficient Monte\nCarlo schemes or other updates of the samples. Indeed, in our experiments, we observed that the\nproposed method requires fewer MCMC steps (not included in the paper). Moreover, one can use\nthe evaluated gradient of the local energy as the proposal vector field for updating the samples. This\nwould allow to decrease the number of MCMC steps at the low cost of additional hyperparameter\ntuning.\n\n## Discussion and Conclusion\n\n### Conclusion\n\nIn the current paper, we propose a novel approach to solving the quantum many-body\nSchr\u00f6dinger equation, by incorporating the Wasserstein metric on the space of Born distributions.\nCompared to the Fisher\u2013Rao metric, which allows for probability mass \u201cteleportation\u201d, the Wasserstein metric constrains the evolution of the density to local changes under the locally-optimal\ntransportation plan, i.e., following fluid dynamics. This property is favorable when the evolution\nof the parametric model is accompanied by the evolution of samples (performed by an MCMC\nalgorithm). Indeed, by forbidding or regularizing non-local mass \u201cteleportation\u201d in the density\nchange, one prevents the appearance of distant modes in the density model, which would require\nlonger MCMC runs for proper mixing of the samples.\n\nIn practice, we demonstrate that following the gradient flow under the Wasserstein (or Wasserstein\nFisher\u2013Rao) metric results in better convergence to the ground state wave function. This is expected\nto be due to our proposed loss, which takes into account the gradient of the local energy and achieves\nits minimum when the norm of the gradient vanishes, therefore explicitly minimizing the norm of the\nlocal energy gradient.\n\nWe believe that our new theoretical framework for solving the time-independent Schr\u00f6dinger equation\nfor time-reversal symmetric Hamiltonians based on optimal transport will open new avenues to\ndevelop improved numerical methods for quantum chemistry and physics.\n\n### Connection to Energy-Based and Score-Based Generative Models\n\nThe developed ideas of this paper, i.e., projecting gradient flows under different metrics onto a parametric family, can be extended\nto generative modeling by swapping the energy functional with the KL-divergence. More precisely,\nas we show in Appendix D, using the KL-divergence as our objective functional, the Fisher\u2013Rao\ngradient flow yields energy-based training scheme, while the 2-Wasserstein gradient flow corresponds\nto the score-matching, which is used for training diffusion generative models.\n\n## Acknowledgement\n\nThe authors thank Rob Brekelmans and anonymous reviewers for helpful discussions and feedback.\nJ.N. was supported by Microsoft Research. J.C. acknowledges support from the Natural Sciences and\nEngineering Research Council (NSERC), the Shared Hierarchical Academic Research Computing\nNetwork (SHARCNET), Compute Canada, and the Canadian Institute for Advanced Research\n(CIFAR) AI Chairs program. A.M. acknowledges support from the Canada CIFAR AI Chairs\nprogram. Resources used in preparing this research were provided, in part, by the Province of\nOntario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute\nwww.vectorinstitute.ai/#partners.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Limitations and Conclusion", "md": "# Limitations and Conclusion"}, {"type": "heading", "lvl": 2, "value": "Limitations", "md": "## Limitations"}, {"type": "text", "value": "Since our method requires an extra gradient evaluation compared to QVMC, we include the runtime\nof all the algorithms in Figure 3, Appendix F. Namely, for a proper comparison, instead of reporting\nthe metrics per iteration, we report them per wall time in seconds. Note that all the claims of the\nexperimental section still hold in terms of wall time. All the models were benchmarked on four A40\nGPUs. Third-order derivatives can be efficiently computed using modern deep learning frameworks\nsuch as JAX (Bradbury et al., 2018), which we used to implement our method.\n\nPotentially, one can alleviate the extra cost of the iteration by coming up with more efficient Monte\nCarlo schemes or other updates of the samples. Indeed, in our experiments, we observed that the\nproposed method requires fewer MCMC steps (not included in the paper). Moreover, one can use\nthe evaluated gradient of the local energy as the proposal vector field for updating the samples. This\nwould allow to decrease the number of MCMC steps at the low cost of additional hyperparameter\ntuning.", "md": "Since our method requires an extra gradient evaluation compared to QVMC, we include the runtime\nof all the algorithms in Figure 3, Appendix F. Namely, for a proper comparison, instead of reporting\nthe metrics per iteration, we report them per wall time in seconds. Note that all the claims of the\nexperimental section still hold in terms of wall time. All the models were benchmarked on four A40\nGPUs. Third-order derivatives can be efficiently computed using modern deep learning frameworks\nsuch as JAX (Bradbury et al., 2018), which we used to implement our method.\n\nPotentially, one can alleviate the extra cost of the iteration by coming up with more efficient Monte\nCarlo schemes or other updates of the samples. Indeed, in our experiments, we observed that the\nproposed method requires fewer MCMC steps (not included in the paper). Moreover, one can use\nthe evaluated gradient of the local energy as the proposal vector field for updating the samples. This\nwould allow to decrease the number of MCMC steps at the low cost of additional hyperparameter\ntuning."}, {"type": "heading", "lvl": 2, "value": "Discussion and Conclusion", "md": "## Discussion and Conclusion"}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "In the current paper, we propose a novel approach to solving the quantum many-body\nSchr\u00f6dinger equation, by incorporating the Wasserstein metric on the space of Born distributions.\nCompared to the Fisher\u2013Rao metric, which allows for probability mass \u201cteleportation\u201d, the Wasserstein metric constrains the evolution of the density to local changes under the locally-optimal\ntransportation plan, i.e., following fluid dynamics. This property is favorable when the evolution\nof the parametric model is accompanied by the evolution of samples (performed by an MCMC\nalgorithm). Indeed, by forbidding or regularizing non-local mass \u201cteleportation\u201d in the density\nchange, one prevents the appearance of distant modes in the density model, which would require\nlonger MCMC runs for proper mixing of the samples.\n\nIn practice, we demonstrate that following the gradient flow under the Wasserstein (or Wasserstein\nFisher\u2013Rao) metric results in better convergence to the ground state wave function. This is expected\nto be due to our proposed loss, which takes into account the gradient of the local energy and achieves\nits minimum when the norm of the gradient vanishes, therefore explicitly minimizing the norm of the\nlocal energy gradient.\n\nWe believe that our new theoretical framework for solving the time-independent Schr\u00f6dinger equation\nfor time-reversal symmetric Hamiltonians based on optimal transport will open new avenues to\ndevelop improved numerical methods for quantum chemistry and physics.", "md": "In the current paper, we propose a novel approach to solving the quantum many-body\nSchr\u00f6dinger equation, by incorporating the Wasserstein metric on the space of Born distributions.\nCompared to the Fisher\u2013Rao metric, which allows for probability mass \u201cteleportation\u201d, the Wasserstein metric constrains the evolution of the density to local changes under the locally-optimal\ntransportation plan, i.e., following fluid dynamics. This property is favorable when the evolution\nof the parametric model is accompanied by the evolution of samples (performed by an MCMC\nalgorithm). Indeed, by forbidding or regularizing non-local mass \u201cteleportation\u201d in the density\nchange, one prevents the appearance of distant modes in the density model, which would require\nlonger MCMC runs for proper mixing of the samples.\n\nIn practice, we demonstrate that following the gradient flow under the Wasserstein (or Wasserstein\nFisher\u2013Rao) metric results in better convergence to the ground state wave function. This is expected\nto be due to our proposed loss, which takes into account the gradient of the local energy and achieves\nits minimum when the norm of the gradient vanishes, therefore explicitly minimizing the norm of the\nlocal energy gradient.\n\nWe believe that our new theoretical framework for solving the time-independent Schr\u00f6dinger equation\nfor time-reversal symmetric Hamiltonians based on optimal transport will open new avenues to\ndevelop improved numerical methods for quantum chemistry and physics."}, {"type": "heading", "lvl": 3, "value": "Connection to Energy-Based and Score-Based Generative Models", "md": "### Connection to Energy-Based and Score-Based Generative Models"}, {"type": "text", "value": "The developed ideas of this paper, i.e., projecting gradient flows under different metrics onto a parametric family, can be extended\nto generative modeling by swapping the energy functional with the KL-divergence. More precisely,\nas we show in Appendix D, using the KL-divergence as our objective functional, the Fisher\u2013Rao\ngradient flow yields energy-based training scheme, while the 2-Wasserstein gradient flow corresponds\nto the score-matching, which is used for training diffusion generative models.", "md": "The developed ideas of this paper, i.e., projecting gradient flows under different metrics onto a parametric family, can be extended\nto generative modeling by swapping the energy functional with the KL-divergence. More precisely,\nas we show in Appendix D, using the KL-divergence as our objective functional, the Fisher\u2013Rao\ngradient flow yields energy-based training scheme, while the 2-Wasserstein gradient flow corresponds\nto the score-matching, which is used for training diffusion generative models."}, {"type": "heading", "lvl": 2, "value": "Acknowledgement", "md": "## Acknowledgement"}, {"type": "text", "value": "The authors thank Rob Brekelmans and anonymous reviewers for helpful discussions and feedback.\nJ.N. was supported by Microsoft Research. J.C. acknowledges support from the Natural Sciences and\nEngineering Research Council (NSERC), the Shared Hierarchical Academic Research Computing\nNetwork (SHARCNET), Compute Canada, and the Canadian Institute for Advanced Research\n(CIFAR) AI Chairs program. A.M. acknowledges support from the Canada CIFAR AI Chairs\nprogram. Resources used in preparing this research were provided, in part, by the Province of\nOntario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute\nwww.vectorinstitute.ai/#partners.", "md": "The authors thank Rob Brekelmans and anonymous reviewers for helpful discussions and feedback.\nJ.N. was supported by Microsoft Research. J.C. acknowledges support from the Natural Sciences and\nEngineering Research Council (NSERC), the Shared Hierarchical Academic Research Computing\nNetwork (SHARCNET), Compute Canada, and the Canadian Institute for Advanced Research\n(CIFAR) AI Chairs program. A.M. acknowledges support from the Canada CIFAR AI Chairs\nprogram. Resources used in preparing this research were provided, in part, by the Province of\nOntario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute\nwww.vectorinstitute.ai/#partners."}]}, {"page": 11, "text": "References\nAmari, S.-I. Natural gradient works effi  ciently in learning. Neural computation, 10(2):251\u2013276,\n  1998.\nAmbrosio, L., Gigli, N., and Savar\u00e9, G. Gradient flows: in metric spaces and in the space of\n  probability measures. Springer Science & Business Media, 2005.\nBenamou, J.-D. and Brenier, Y. A computational fluid mechanics solution to the Monge-Kantorovich\n  mass transfer problem. Numerische Mathematik, 84(3):375\u2013393, 2000.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke,\n  A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of\n  Python+NumPy programs, 2018. URL http://github.com/google/jax.\nCarleo, G. and Troyer, M. Solving the quantum many-body problem with artificial neural networks.\n  Science, 355(6325):602\u2013606, 2017.\nCeperley, D., Chester, G. V., and Kalos, M. H. Monte Carlo simulation of a many-fermion study.\n  Physical Review B, 16(7):3081, 1977.\nChizat, L., Peyr\u00e9, G., Schmitzer, B., and Vialard, F.-X. An interpolating distance between optimal\n  transport and Fisher\u2013Rao metrics. Foundations of Computational Mathematics, 18(1):1\u201344, 2018.\nChoo, K., Mezzacapo, A., and Carleo, G. Fermionic neural-network states for ab-initio electronic\n  structure. Nature communications, 11(1):2368, 2020.\nDu, Y. and Mordatch, I. Implicit generation and generalization in energy-based models. arXiv\n  preprint arXiv:1903.08689, 2019.\nGao, N. and G\u00fcnnemann, S. Generalizing neural wave functions. arXiv preprint arXiv:2302.04168,\n  2023.\nGnech, A., Adams, C., Brawand, N., Carleo, G., Lovato, A., and Rocco, N. Nuclei with up to a= 6\n  nucleons with artificial neural network wave functions. Few-Body Systems, 63(1):7, 2022.\nHermann, J., Sch\u00e4tzle, Z., and No\u00e9, F. Deep-neural-network solution of the electronic Schr\u00f6dinger\n  equation. Nature Chemistry, 12(10):891\u2013897, 2020.\nHermann, J., Spencer, J., Choo, K., Mezzacapo, A., Foulkes, W., Pfau, D., Carleo, G., and No\u00e9, F. Ab-\n  initio quantum chemistry with neural-network wavefunctions. arXiv preprint arXiv:2208.12590,\n  2022.\nHibat-Allah, M., Ganahl, M., Hayward, L. E., Melko, R. G., and Carrasquilla, J. Recurrent neural\n  network wave functions. Physical Review Research, 2(2):023358, 2020.\nHyv\u00e4rinen, A. and Dayan, P. Estimation of non-normalized statistical models by score matching.\n  Journal of Machine Learning Research, 6(4), 2005.\nKondratyev, S., Monsaingeon, L., and Vorotnikov, D. A new optimal transport distance on the space\n  of finite Radon measures. Advances in Differential Equations, 21(11/12):1117\u20131164, 2016.\nLiero, M., Mielke, A., and Savar\u00e9, G. Optimal transport in competition with reaction: The Hellinger\u2013\n  Kantorovich distance and geodesic curves. SIAM Journal on Mathematical Analysis, 48(4):\n  2869\u20132911, 2016.\nLuo, D. and Clark, B. K. Backflow transformations via neural networks for quantum many-body\n  wave functions. Physical review letters, 122(22):226401, 2019.\nMartens, J. and Grosse, R.     Optimizing neural networks with kronecker-factored approximate\n  curvature. In International conference on machine learning, pp. 2408\u20132417. PMLR, 2015.\nMcArdle, S., Jones, T., Endo, S., Li, Y., Benjamin, S. C., and Yuan, X. Variational ansatz-based\n  quantum simulation of imaginary time evolution. npj Quantum Information, 5(1):75, 2019.\n                                                 11", "md": "# References\n\n## References\n\n- Amari, S.-I. Natural gradient works efficiently in learning. Neural computation, 10(2):251\u2013276, 1998.\n- Ambrosio, L., Gigli, N., and Savar\u00e9, G. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.\n- Benamou, J.-D. and Brenier, Y. A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375\u2013393, 2000.\n- Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n- Carleo, G. and Troyer, M. Solving the quantum many-body problem with artificial neural networks. Science, 355(6325):602\u2013606, 2017.\n- Ceperley, D., Chester, G. V., and Kalos, M. H. Monte Carlo simulation of a many-fermion study. Physical Review B, 16(7):3081, 1977.\n- Chizat, L., Peyr\u00e9, G., Schmitzer, B., and Vialard, F.-X. An interpolating distance between optimal transport and Fisher\u2013Rao metrics. Foundations of Computational Mathematics, 18(1):1\u201344, 2018.\n- Choo, K., Mezzacapo, A., and Carleo, G. Fermionic neural-network states for ab-initio electronic structure. Nature communications, 11(1):2368, 2020.\n- Du, Y. and Mordatch, I. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689, 2019.\n- Gao, N. and G\u00fcnnemann, S. Generalizing neural wave functions. arXiv preprint arXiv:2302.04168, 2023.\n- Gnech, A., Adams, C., Brawand, N., Carleo, G., Lovato, A., and Rocco, N. Nuclei with up to a= 6 nucleons with artificial neural network wave functions. Few-Body Systems, 63(1):7, 2022.\n- Hermann, J., Sch\u00e4tzle, Z., and No\u00e9, F. Deep-neural-network solution of the electronic Schr\u00f6dinger equation. Nature Chemistry, 12(10):891\u2013897, 2020.\n- Hermann, J., Spencer, J., Choo, K., Mezzacapo, A., Foulkes, W., Pfau, D., Carleo, G., and No\u00e9, F. Ab-initio quantum chemistry with neural-network wavefunctions. arXiv preprint arXiv:2208.12590, 2022.\n- Hibat-Allah, M., Ganahl, M., Hayward, L. E., Melko, R. G., and Carrasquilla, J. Recurrent neural network wave functions. Physical Review Research, 2(2):023358, 2020.\n- Hyv\u00e4rinen, A. and Dayan, P. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.\n- Kondratyev, S., Monsaingeon, L., and Vorotnikov, D. A new optimal transport distance on the space of finite Radon measures. Advances in Differential Equations, 21(11/12):1117\u20131164, 2016.\n- Liero, M., Mielke, A., and Savar\u00e9, G. Optimal transport in competition with reaction: The Hellinger\u2013Kantorovich distance and geodesic curves. SIAM Journal on Mathematical Analysis, 48(4):2869\u20132911, 2016.\n- Luo, D. and Clark, B. K. Backflow transformations via neural networks for quantum many-body wave functions. Physical review letters, 122(22):226401, 2019.\n- Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408\u20132417. PMLR, 2015.\n- McArdle, S., Jones, T., Endo, S., Li, Y., Benjamin, S. C., and Yuan, X. Variational ansatz-based quantum simulation of imaginary time evolution. npj Quantum Information, 5(1):75, 2019.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- Amari, S.-I. Natural gradient works efficiently in learning. Neural computation, 10(2):251\u2013276, 1998.\n- Ambrosio, L., Gigli, N., and Savar\u00e9, G. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.\n- Benamou, J.-D. and Brenier, Y. A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375\u2013393, 2000.\n- Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n- Carleo, G. and Troyer, M. Solving the quantum many-body problem with artificial neural networks. Science, 355(6325):602\u2013606, 2017.\n- Ceperley, D., Chester, G. V., and Kalos, M. H. Monte Carlo simulation of a many-fermion study. Physical Review B, 16(7):3081, 1977.\n- Chizat, L., Peyr\u00e9, G., Schmitzer, B., and Vialard, F.-X. An interpolating distance between optimal transport and Fisher\u2013Rao metrics. Foundations of Computational Mathematics, 18(1):1\u201344, 2018.\n- Choo, K., Mezzacapo, A., and Carleo, G. Fermionic neural-network states for ab-initio electronic structure. Nature communications, 11(1):2368, 2020.\n- Du, Y. and Mordatch, I. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689, 2019.\n- Gao, N. and G\u00fcnnemann, S. Generalizing neural wave functions. arXiv preprint arXiv:2302.04168, 2023.\n- Gnech, A., Adams, C., Brawand, N., Carleo, G., Lovato, A., and Rocco, N. Nuclei with up to a= 6 nucleons with artificial neural network wave functions. Few-Body Systems, 63(1):7, 2022.\n- Hermann, J., Sch\u00e4tzle, Z., and No\u00e9, F. Deep-neural-network solution of the electronic Schr\u00f6dinger equation. Nature Chemistry, 12(10):891\u2013897, 2020.\n- Hermann, J., Spencer, J., Choo, K., Mezzacapo, A., Foulkes, W., Pfau, D., Carleo, G., and No\u00e9, F. Ab-initio quantum chemistry with neural-network wavefunctions. arXiv preprint arXiv:2208.12590, 2022.\n- Hibat-Allah, M., Ganahl, M., Hayward, L. E., Melko, R. G., and Carrasquilla, J. Recurrent neural network wave functions. Physical Review Research, 2(2):023358, 2020.\n- Hyv\u00e4rinen, A. and Dayan, P. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.\n- Kondratyev, S., Monsaingeon, L., and Vorotnikov, D. A new optimal transport distance on the space of finite Radon measures. Advances in Differential Equations, 21(11/12):1117\u20131164, 2016.\n- Liero, M., Mielke, A., and Savar\u00e9, G. Optimal transport in competition with reaction: The Hellinger\u2013Kantorovich distance and geodesic curves. SIAM Journal on Mathematical Analysis, 48(4):2869\u20132911, 2016.\n- Luo, D. and Clark, B. K. Backflow transformations via neural networks for quantum many-body wave functions. Physical review letters, 122(22):226401, 2019.\n- Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408\u20132417. PMLR, 2015.\n- McArdle, S., Jones, T., Endo, S., Li, Y., Benjamin, S. C., and Yuan, X. Variational ansatz-based quantum simulation of imaginary time evolution. npj Quantum Information, 5(1):75, 2019.", "md": "- Amari, S.-I. Natural gradient works efficiently in learning. Neural computation, 10(2):251\u2013276, 1998.\n- Ambrosio, L., Gigli, N., and Savar\u00e9, G. Gradient flows: in metric spaces and in the space of probability measures. Springer Science & Business Media, 2005.\n- Benamou, J.-D. and Brenier, Y. A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375\u2013393, 2000.\n- Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.\n- Carleo, G. and Troyer, M. Solving the quantum many-body problem with artificial neural networks. Science, 355(6325):602\u2013606, 2017.\n- Ceperley, D., Chester, G. V., and Kalos, M. H. Monte Carlo simulation of a many-fermion study. Physical Review B, 16(7):3081, 1977.\n- Chizat, L., Peyr\u00e9, G., Schmitzer, B., and Vialard, F.-X. An interpolating distance between optimal transport and Fisher\u2013Rao metrics. Foundations of Computational Mathematics, 18(1):1\u201344, 2018.\n- Choo, K., Mezzacapo, A., and Carleo, G. Fermionic neural-network states for ab-initio electronic structure. Nature communications, 11(1):2368, 2020.\n- Du, Y. and Mordatch, I. Implicit generation and generalization in energy-based models. arXiv preprint arXiv:1903.08689, 2019.\n- Gao, N. and G\u00fcnnemann, S. Generalizing neural wave functions. arXiv preprint arXiv:2302.04168, 2023.\n- Gnech, A., Adams, C., Brawand, N., Carleo, G., Lovato, A., and Rocco, N. Nuclei with up to a= 6 nucleons with artificial neural network wave functions. Few-Body Systems, 63(1):7, 2022.\n- Hermann, J., Sch\u00e4tzle, Z., and No\u00e9, F. Deep-neural-network solution of the electronic Schr\u00f6dinger equation. Nature Chemistry, 12(10):891\u2013897, 2020.\n- Hermann, J., Spencer, J., Choo, K., Mezzacapo, A., Foulkes, W., Pfau, D., Carleo, G., and No\u00e9, F. Ab-initio quantum chemistry with neural-network wavefunctions. arXiv preprint arXiv:2208.12590, 2022.\n- Hibat-Allah, M., Ganahl, M., Hayward, L. E., Melko, R. G., and Carrasquilla, J. Recurrent neural network wave functions. Physical Review Research, 2(2):023358, 2020.\n- Hyv\u00e4rinen, A. and Dayan, P. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.\n- Kondratyev, S., Monsaingeon, L., and Vorotnikov, D. A new optimal transport distance on the space of finite Radon measures. Advances in Differential Equations, 21(11/12):1117\u20131164, 2016.\n- Liero, M., Mielke, A., and Savar\u00e9, G. Optimal transport in competition with reaction: The Hellinger\u2013Kantorovich distance and geodesic curves. SIAM Journal on Mathematical Analysis, 48(4):2869\u20132911, 2016.\n- Luo, D. and Clark, B. K. Backflow transformations via neural networks for quantum many-body wave functions. Physical review letters, 122(22):226401, 2019.\n- Martens, J. and Grosse, R. Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning, pp. 2408\u20132417. PMLR, 2015.\n- McArdle, S., Jones, T., Endo, S., Li, Y., Benjamin, S. C., and Yuan, X. Variational ansatz-based quantum simulation of imaginary time evolution. npj Quantum Information, 5(1):75, 2019."}]}, {"page": 12, "text": "McMillan, W. L. Ground state of liquid he 4. Physical Review, 138(2A):A442, 1965.\nPescia, G., Han, J., Lovato, A., Lu, J., and Carleo, G. Neural-network quantum states for periodic\n  systems in continuous space. Physical Review Research, 4(2):023138, 2022.\nPescia, G., Nys, J., Kim, J., Lovato, A., and Carleo, G. Message-passing neural quantum states for\n  the homogeneous electron gas, 2023.\nPfau, D., Spencer, J. S., Matthews, A. G., and Foulkes, W. M. C. Ab initio solution of the many-\n  electron Schr\u00f6dinger equation with deep neural networks. Physical Review Research, 2(3):033429,\n  2020.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based\n  generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456,\n  2020.\nSorella, S. Green function Monte Carlo with stochastic reconfiguration. Phys. Rev. Lett., 80:4558\u2013\n  4561, May 1998. doi: 10.1103/PhysRevLett.80.4558. URL https://link.aps.org/doi/10.\n  1103/PhysRevLett.80.4558.\nStokes, J., Izaac, J., Killoran, N., and Carleo, G. Quantum natural gradient. Quantum, 4:269, 2020.\nVicentini, F., Hofmann, D., Szab\u00f3, A., Wu, D., Roth, C., Giuliani, C., Pescia, G., Nys, J., Vargas-\n  Calder\u00f3n, V., Astrakhantsev, N., et al. Netket 3: Machine learning toolbox for many-body quantum\n  systems. SciPost Physics Codebases, pp. 007, 2022.\nvon Glehn, I., Spencer, J. S., and Pfau, D. A self-attention ansatz for ab-initio quantum chemistry.\n  arXiv preprint arXiv:2211.13672, 2022.\nWhite, S. R. Density matrix formulation for quantum renormalization groups. Physical review letters,\n  69(19):2863, 1992.\nWu, D., Rossi, R., Vicentini, F., Astrakhantsev, N., Becca, F., Cao, X., Carrasquilla, J., Ferrari, F.,\n  Georges, A., Hibat-Allah, M., et al. Variational benchmarks for quantum many-body problems.\n  arXiv preprint arXiv:2302.04919, 2023.\nXie, J., Lu, Y., Zhu, S.-C., and Wu, Y. A theory of generative convnet. In International Conference\n  on Machine Learning, pp. 2635\u20132644. PMLR, 2016.\nYuan, X., Endo, S., Zhao, Q., Li, Y., and Benjamin, S. C. Theory of variational quantum simulation.\n  Quantum, 3:191, 2019.\n                                                  12", "md": "# References\n\n# References\n\n- McMillan, W. L. Ground state of liquid He 4. *Physical Review*, 138(2A):A442, 1965.\n- Pescia, G., Han, J., Lovato, A., Lu, J., and Carleo, G. Neural-network quantum states for periodic systems in continuous space. *Physical Review Research*, 4(2):023138, 2022.\n- Pescia, G., Nys, J., Kim, J., Lovato, A., and Carleo, G. Message-passing neural quantum states for the homogeneous electron gas, 2023.\n- Pfau, D., Spencer, J. S., Matthews, A. G., and Foulkes, W. M. C. Ab initio solution of the many-electron Schr\u00f6dinger equation with deep neural networks. *Physical Review Research*, 2(3):033429, 2020.\n- Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. *arXiv preprint* arXiv:2011.13456, 2020.\n- Sorella, S. Green function Monte Carlo with stochastic reconfiguration. *Phys. Rev. Lett.*, 80:4558\u20134561, May 1998. doi: 10.1103/PhysRevLett.80.4558. URL https://link.aps.org/doi/10.1103/PhysRevLett.80.4558.\n- Stokes, J., Izaac, J., Killoran, N., and Carleo, G. Quantum natural gradient. *Quantum*, 4:269, 2020.\n- Vicentini, F., Hofmann, D., Szab\u00f3, A., Wu, D., Roth, C., Giuliani, C., Pescia, G., Nys, J., Vargas-Calder\u00f3n, V., Astrakhantsev, N., et al. Netket 3: Machine learning toolbox for many-body quantum systems. *SciPost Physics Codebases*, pp. 007, 2022.\n- von Glehn, I., Spencer, J. S., and Pfau, D. A self-attention ansatz for ab-initio quantum chemistry. *arXiv preprint* arXiv:2211.13672, 2022.\n- White, S. R. Density matrix formulation for quantum renormalization groups. *Physical Review Letters*, 69(19):2863, 1992.\n- Wu, D., Rossi, R., Vicentini, F., Astrakhantsev, N., Becca, F., Cao, X., Carrasquilla, J., Ferrari, F., Georges, A., Hibat-Allah, M., et al. Variational benchmarks for quantum many-body problems. *arXiv preprint* arXiv:2302.04919, 2023.\n- Xie, J., Lu, Y., Zhu, S.-C., and Wu, Y. A theory of generative convnet. In *International Conference on Machine Learning*, pp. 2635\u20132644. PMLR, 2016.\n- Yuan, X., Endo, S., Zhao, Q., Li, Y., and Benjamin, S. C. Theory of variational quantum simulation. *Quantum*, 3:191, 2019.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "- McMillan, W. L. Ground state of liquid He 4. *Physical Review*, 138(2A):A442, 1965.\n- Pescia, G., Han, J., Lovato, A., Lu, J., and Carleo, G. Neural-network quantum states for periodic systems in continuous space. *Physical Review Research*, 4(2):023138, 2022.\n- Pescia, G., Nys, J., Kim, J., Lovato, A., and Carleo, G. Message-passing neural quantum states for the homogeneous electron gas, 2023.\n- Pfau, D., Spencer, J. S., Matthews, A. G., and Foulkes, W. M. C. Ab initio solution of the many-electron Schr\u00f6dinger equation with deep neural networks. *Physical Review Research*, 2(3):033429, 2020.\n- Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. *arXiv preprint* arXiv:2011.13456, 2020.\n- Sorella, S. Green function Monte Carlo with stochastic reconfiguration. *Phys. Rev. Lett.*, 80:4558\u20134561, May 1998. doi: 10.1103/PhysRevLett.80.4558. URL https://link.aps.org/doi/10.1103/PhysRevLett.80.4558.\n- Stokes, J., Izaac, J., Killoran, N., and Carleo, G. Quantum natural gradient. *Quantum*, 4:269, 2020.\n- Vicentini, F., Hofmann, D., Szab\u00f3, A., Wu, D., Roth, C., Giuliani, C., Pescia, G., Nys, J., Vargas-Calder\u00f3n, V., Astrakhantsev, N., et al. Netket 3: Machine learning toolbox for many-body quantum systems. *SciPost Physics Codebases*, pp. 007, 2022.\n- von Glehn, I., Spencer, J. S., and Pfau, D. A self-attention ansatz for ab-initio quantum chemistry. *arXiv preprint* arXiv:2211.13672, 2022.\n- White, S. R. Density matrix formulation for quantum renormalization groups. *Physical Review Letters*, 69(19):2863, 1992.\n- Wu, D., Rossi, R., Vicentini, F., Astrakhantsev, N., Becca, F., Cao, X., Carrasquilla, J., Ferrari, F., Georges, A., Hibat-Allah, M., et al. Variational benchmarks for quantum many-body problems. *arXiv preprint* arXiv:2302.04919, 2023.\n- Xie, J., Lu, Y., Zhu, S.-C., and Wu, Y. A theory of generative convnet. In *International Conference on Machine Learning*, pp. 2635\u20132644. PMLR, 2016.\n- Yuan, X., Endo, S., Zhao, Q., Li, Y., and Benjamin, S. C. Theory of variational quantum simulation. *Quantum*, 3:191, 2019.", "md": "- McMillan, W. L. Ground state of liquid He 4. *Physical Review*, 138(2A):A442, 1965.\n- Pescia, G., Han, J., Lovato, A., Lu, J., and Carleo, G. Neural-network quantum states for periodic systems in continuous space. *Physical Review Research*, 4(2):023138, 2022.\n- Pescia, G., Nys, J., Kim, J., Lovato, A., and Carleo, G. Message-passing neural quantum states for the homogeneous electron gas, 2023.\n- Pfau, D., Spencer, J. S., Matthews, A. G., and Foulkes, W. M. C. Ab initio solution of the many-electron Schr\u00f6dinger equation with deep neural networks. *Physical Review Research*, 2(3):033429, 2020.\n- Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. *arXiv preprint* arXiv:2011.13456, 2020.\n- Sorella, S. Green function Monte Carlo with stochastic reconfiguration. *Phys. Rev. Lett.*, 80:4558\u20134561, May 1998. doi: 10.1103/PhysRevLett.80.4558. URL https://link.aps.org/doi/10.1103/PhysRevLett.80.4558.\n- Stokes, J., Izaac, J., Killoran, N., and Carleo, G. Quantum natural gradient. *Quantum*, 4:269, 2020.\n- Vicentini, F., Hofmann, D., Szab\u00f3, A., Wu, D., Roth, C., Giuliani, C., Pescia, G., Nys, J., Vargas-Calder\u00f3n, V., Astrakhantsev, N., et al. Netket 3: Machine learning toolbox for many-body quantum systems. *SciPost Physics Codebases*, pp. 007, 2022.\n- von Glehn, I., Spencer, J. S., and Pfau, D. A self-attention ansatz for ab-initio quantum chemistry. *arXiv preprint* arXiv:2211.13672, 2022.\n- White, S. R. Density matrix formulation for quantum renormalization groups. *Physical Review Letters*, 69(19):2863, 1992.\n- Wu, D., Rossi, R., Vicentini, F., Astrakhantsev, N., Becca, F., Cao, X., Carrasquilla, J., Ferrari, F., Georges, A., Hibat-Allah, M., et al. Variational benchmarks for quantum many-body problems. *arXiv preprint* arXiv:2302.04919, 2023.\n- Xie, J., Lu, Y., Zhu, S.-C., and Wu, Y. A theory of generative convnet. In *International Conference on Machine Learning*, pp. 2635\u20132644. PMLR, 2016.\n- Yuan, X., Endo, S., Zhao, Q., Li, Y., and Benjamin, S. C. Theory of variational quantum simulation. *Quantum*, 3:191, 2019."}]}, {"page": 13, "text": "A       Gradient flows under Wasserstein Fisher\u2013Rao metric\nWe, first, remind the concept of the functional derivative. The change of the functional F                                    [q] : P2 \u2192       R\nalong the direction h can be expressed as\n                     F  [q + h] = F       [q] + dF     [h] + o(\u2225h\u2225), dF           [h] =         dx h(x) \u03b4F      \u03b4q[q](x)    .               (26)\nConsider a change of the density in time, the change of the functional can be defined through the              derivative\ndifferential as\n   F    qt + \u2206t\u2202qt           = F    [qt] + \u2206t \u00b7 dF        \u2202qt       + o(\u2225h\u2225), dF           \u2202qt       =        dx \u2202qt(x)       \u03b4F   [qt] (x).\n                     \u2202t                                     \u2202t                               \u2202t                       \u2202t         \u03b4qt\n                                                                                                                                            (27)\nIn particular, we have               d                     \u2202qt      =        dx \u2202qt(x)        \u03b4F  [qt]  (x).                                (28)\n                                    dtF    [qt] = dF         \u2202t                       \u2202t        \u03b4qt\nA.1      Minimizing movement scheme for 2-Wasserstein distance and Kullback-Leibler\n         divergence\nGradient flow under W2                    Consider the following minimizing movement scheme (MMS)\n                                               inf                            1                                                             (29)\n                                                 q\u2032 F  [q\u2032] \u2212    F  [q] +   2\u2206tW2(q, q\u2032),\nwhere the change of the density is restricted to the continuity equation, i.e.,\n              \u2202qt                                         and     q\u2032(x) = q(x) \u2212          \u2206t\u2207x \u00b7 (q(x)v(x)) + o(\u2206t).                        (30)\n              \u2202t = \u2212\u2207x \u00b7 (qt(x)vt(x)),\nUsing the static formulation of W2 distance, we have\n                      W2(q, q\u2032)2 =              dx q(x)\u2225x \u2212         T \u2217(x)\u22252 = \u2206t2               dx q(x)\u2225v\u2217(x)\u22252,                           (31)\nwhere T \u2217(x) is the optimal transportation plan, and v\u2217(x) is the corresponding optimal gradient field.\nThus, we can rewrite the MMS problem as\n            inf                       dx \u2207x \u00b7 (q(x)v(x))\u03b4F              [qt](x) \u2212      F  [q] + \u2206t           dx q(x)\u2225v(x)\u22252,                (32)\n              v F   [q] \u2212   \u2206t                                        \u03b4qt                           2\n            inf       dx q(x)        v(x), \u2207x     \u03b4F   [qt] (x)      + 1         dx q(x)\u2225v(x)\u22252,                                            (33)\n              v                                      \u03b4q t                2\n            inf       dx q(x)      v(x) + \u2207x         \u03b4F  [qt]  (x)    2.                                                                    (34)\n              v                                        \u03b4q t\nFrom the last optimization problem, we have\n                                                                           \u03b4F  [qt]\n                                                       v(x) = \u2212\u2207x            \u03b4qt     (x).                                                   (35)\nGradient flow under KL                    Consider the following minimizing movement scheme (MMS)\n                                              inf                            1                                                              (36)\n                                               q\u2032 F   [q\u2032] \u2212    F [q] +    2\u2206tDKL(q\u2032\u2225q),\nwhere the change of the density is restricted to the following weighting scheme\n                                \u2202qt\n                                 \u2202t = \u2212         gt(x)qt(x),         hence ,                                                                 (37)\n                              q\u2032(x) = qt(x) \u2212           \u2206t qt(x)gt(x) + o(\u2206t),                 and                                          (38)\n                         log q\u2032(x) = log qt(x) \u2212              \u2206t gt(x) + \u2206t2           \u22022 log qt(x)       + o(\u2206t2).                         (39)\n                                                                                  2          \u2202t2\n                                                                       13", "md": "# Gradient flows under Wasserstein Fisher\u2013Rao metric\n\nWe, first, remind the concept of the functional derivative. The change of the functional $$F[q]: P2 \\rightarrow \\mathbb{R}$$ along the direction h can be expressed as\n\n$$F[q + h] = F[q] + dF[h] + o(\\|h\\|), \\quad dF[h] = \\int dx \\, h(x) \\delta F \\frac{\\delta q[q](x)}{\\delta q}.$$ (26)\n\nConsider a change of the density in time, the change of the functional can be defined through the derivative differential as\n\n$$F_{qt + \\Delta t \\partial q_t} = F[qt] + \\Delta t \\cdot dF \\partial q_t + o(\\|h\\|), \\quad dF \\partial q_t = \\int dx \\, \\partial q_t(x) \\delta F [qt](x).$$ (27)\n\nIn particular, we have\n\n$$\\frac{d}{dt}F[qt] = dF \\partial t \\delta q_t = \\int dx \\, \\partial q_t(x) \\delta F [qt](x).$$ (28)\n\n## A.1 Minimizing movement scheme for 2-Wasserstein distance and Kullback-Leibler divergence\n\nGradient flow under W2 Consider the following minimizing movement scheme (MMS)\n\n$$\\inf_{q'} F[q'] - F[q] + 2\\Delta t W2(q, q'),$$\n\nwhere the change of the density is restricted to the continuity equation, i.e.,\n\n$$\\partial q_t = -\\nabla_x \\cdot (q_t(x)v_t(x)), \\quad q'(x) = q(x) - \\Delta t \\nabla_x \\cdot (q(x)v(x)) + o(\\Delta t).$$ (30)\n\nUsing the static formulation of W2 distance, we have\n\n$$W2(q, q')^2 = \\int dx \\, q(x)\\|x - T^*(x)\\|^2 = \\Delta t^2 \\int dx \\, q(x)\\|v^*(x)\\|^2,$$ (31)\n\nwhere $$T^*(x)$$ is the optimal transportation plan, and $$v^*(x)$$ is the corresponding optimal gradient field.\n\nThus, we can rewrite the MMS problem as\n\n$$\\inf_v \\int dx \\, \\nabla_x \\cdot (q(x)v(x))\\delta F [qt](x) - F[q] + \\Delta t \\int dx \\, q(x)\\|v(x)\\|^2,$$ (32)\n\n$$\\inf_v \\int dx \\, q(x)v(x), \\nabla_x \\delta F [qt](x) + \\frac{1}{2} \\int dx \\, q(x)\\|v(x)\\|^2,$$ (33)\n\n$$\\inf_v \\int dx \\, q(x)v(x) + \\nabla_x \\delta F [qt](x) 2.$$ (34)\n\nFrom the last optimization problem, we have\n\n$$v(x) = -\\nabla_x \\delta F [qt](x).$$ (35)\n\nGradient flow under KL Consider the following minimizing movement scheme (MMS)\n\n$$\\inf_{q'} F[q'] - F[q] + 2\\Delta t DKL(q' \\| q),$$\n\nwhere the change of the density is restricted to the following weighting scheme\n\n$$\\partial q_t = -g_t(x)q_t(x), \\quad \\text{hence},$$ (37)\n\n$$q'(x) = q_t(x) - \\Delta t q_t(x)g_t(x) + o(\\Delta t), \\quad \\text{and}$$ (38)\n\n$$\\log q'(x) = \\log q_t(x) - \\Delta t g_t(x) + \\Delta t^2 \\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} + o(\\Delta t^2).$$ (39)", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Gradient flows under Wasserstein Fisher\u2013Rao metric", "md": "# Gradient flows under Wasserstein Fisher\u2013Rao metric"}, {"type": "text", "value": "We, first, remind the concept of the functional derivative. The change of the functional $$F[q]: P2 \\rightarrow \\mathbb{R}$$ along the direction h can be expressed as\n\n$$F[q + h] = F[q] + dF[h] + o(\\|h\\|), \\quad dF[h] = \\int dx \\, h(x) \\delta F \\frac{\\delta q[q](x)}{\\delta q}.$$ (26)\n\nConsider a change of the density in time, the change of the functional can be defined through the derivative differential as\n\n$$F_{qt + \\Delta t \\partial q_t} = F[qt] + \\Delta t \\cdot dF \\partial q_t + o(\\|h\\|), \\quad dF \\partial q_t = \\int dx \\, \\partial q_t(x) \\delta F [qt](x).$$ (27)\n\nIn particular, we have\n\n$$\\frac{d}{dt}F[qt] = dF \\partial t \\delta q_t = \\int dx \\, \\partial q_t(x) \\delta F [qt](x).$$ (28)", "md": "We, first, remind the concept of the functional derivative. The change of the functional $$F[q]: P2 \\rightarrow \\mathbb{R}$$ along the direction h can be expressed as\n\n$$F[q + h] = F[q] + dF[h] + o(\\|h\\|), \\quad dF[h] = \\int dx \\, h(x) \\delta F \\frac{\\delta q[q](x)}{\\delta q}.$$ (26)\n\nConsider a change of the density in time, the change of the functional can be defined through the derivative differential as\n\n$$F_{qt + \\Delta t \\partial q_t} = F[qt] + \\Delta t \\cdot dF \\partial q_t + o(\\|h\\|), \\quad dF \\partial q_t = \\int dx \\, \\partial q_t(x) \\delta F [qt](x).$$ (27)\n\nIn particular, we have\n\n$$\\frac{d}{dt}F[qt] = dF \\partial t \\delta q_t = \\int dx \\, \\partial q_t(x) \\delta F [qt](x).$$ (28)"}, {"type": "heading", "lvl": 2, "value": "A.1 Minimizing movement scheme for 2-Wasserstein distance and Kullback-Leibler divergence", "md": "## A.1 Minimizing movement scheme for 2-Wasserstein distance and Kullback-Leibler divergence"}, {"type": "text", "value": "Gradient flow under W2 Consider the following minimizing movement scheme (MMS)\n\n$$\\inf_{q'} F[q'] - F[q] + 2\\Delta t W2(q, q'),$$\n\nwhere the change of the density is restricted to the continuity equation, i.e.,\n\n$$\\partial q_t = -\\nabla_x \\cdot (q_t(x)v_t(x)), \\quad q'(x) = q(x) - \\Delta t \\nabla_x \\cdot (q(x)v(x)) + o(\\Delta t).$$ (30)\n\nUsing the static formulation of W2 distance, we have\n\n$$W2(q, q')^2 = \\int dx \\, q(x)\\|x - T^*(x)\\|^2 = \\Delta t^2 \\int dx \\, q(x)\\|v^*(x)\\|^2,$$ (31)\n\nwhere $$T^*(x)$$ is the optimal transportation plan, and $$v^*(x)$$ is the corresponding optimal gradient field.\n\nThus, we can rewrite the MMS problem as\n\n$$\\inf_v \\int dx \\, \\nabla_x \\cdot (q(x)v(x))\\delta F [qt](x) - F[q] + \\Delta t \\int dx \\, q(x)\\|v(x)\\|^2,$$ (32)\n\n$$\\inf_v \\int dx \\, q(x)v(x), \\nabla_x \\delta F [qt](x) + \\frac{1}{2} \\int dx \\, q(x)\\|v(x)\\|^2,$$ (33)\n\n$$\\inf_v \\int dx \\, q(x)v(x) + \\nabla_x \\delta F [qt](x) 2.$$ (34)\n\nFrom the last optimization problem, we have\n\n$$v(x) = -\\nabla_x \\delta F [qt](x).$$ (35)\n\nGradient flow under KL Consider the following minimizing movement scheme (MMS)\n\n$$\\inf_{q'} F[q'] - F[q] + 2\\Delta t DKL(q' \\| q),$$\n\nwhere the change of the density is restricted to the following weighting scheme\n\n$$\\partial q_t = -g_t(x)q_t(x), \\quad \\text{hence},$$ (37)\n\n$$q'(x) = q_t(x) - \\Delta t q_t(x)g_t(x) + o(\\Delta t), \\quad \\text{and}$$ (38)\n\n$$\\log q'(x) = \\log q_t(x) - \\Delta t g_t(x) + \\Delta t^2 \\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} + o(\\Delta t^2).$$ (39)", "md": "Gradient flow under W2 Consider the following minimizing movement scheme (MMS)\n\n$$\\inf_{q'} F[q'] - F[q] + 2\\Delta t W2(q, q'),$$\n\nwhere the change of the density is restricted to the continuity equation, i.e.,\n\n$$\\partial q_t = -\\nabla_x \\cdot (q_t(x)v_t(x)), \\quad q'(x) = q(x) - \\Delta t \\nabla_x \\cdot (q(x)v(x)) + o(\\Delta t).$$ (30)\n\nUsing the static formulation of W2 distance, we have\n\n$$W2(q, q')^2 = \\int dx \\, q(x)\\|x - T^*(x)\\|^2 = \\Delta t^2 \\int dx \\, q(x)\\|v^*(x)\\|^2,$$ (31)\n\nwhere $$T^*(x)$$ is the optimal transportation plan, and $$v^*(x)$$ is the corresponding optimal gradient field.\n\nThus, we can rewrite the MMS problem as\n\n$$\\inf_v \\int dx \\, \\nabla_x \\cdot (q(x)v(x))\\delta F [qt](x) - F[q] + \\Delta t \\int dx \\, q(x)\\|v(x)\\|^2,$$ (32)\n\n$$\\inf_v \\int dx \\, q(x)v(x), \\nabla_x \\delta F [qt](x) + \\frac{1}{2} \\int dx \\, q(x)\\|v(x)\\|^2,$$ (33)\n\n$$\\inf_v \\int dx \\, q(x)v(x) + \\nabla_x \\delta F [qt](x) 2.$$ (34)\n\nFrom the last optimization problem, we have\n\n$$v(x) = -\\nabla_x \\delta F [qt](x).$$ (35)\n\nGradient flow under KL Consider the following minimizing movement scheme (MMS)\n\n$$\\inf_{q'} F[q'] - F[q] + 2\\Delta t DKL(q' \\| q),$$\n\nwhere the change of the density is restricted to the following weighting scheme\n\n$$\\partial q_t = -g_t(x)q_t(x), \\quad \\text{hence},$$ (37)\n\n$$q'(x) = q_t(x) - \\Delta t q_t(x)g_t(x) + o(\\Delta t), \\quad \\text{and}$$ (38)\n\n$$\\log q'(x) = \\log q_t(x) - \\Delta t g_t(x) + \\Delta t^2 \\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} + o(\\Delta t^2).$$ (39)"}]}, {"page": 14, "text": "The KL-divergence is then\n                   DKL(q\u2032\u2225q) =                dx q\u2032(x)       \u2212\u2206t gt(x) + \u2206t2             \u22022 log qt(x)         + o(\u2206t2)              (40)\n                                                                                    2          \u2202t2\n                                    = \u2212      \u2206t        dx qt(x)gt(x) + \u2206t2                  dx qt(x)\u22022 log qt(x)                    (41)\n                                         + \u2206t2          dx q  t(x)gt(x) + o(\u2206t2)  2                            \u2202t2                  (42)\n                                    = \u2206t2  2         dx qt(x)g2    t (x) + o(\u2206t2).                                                  (43)\nIn the last equation we are using the normalization condition, i.e.,\nThus, we can rewrite the MMS problem as        dx \u2202qt(x)\u2202t      =       dx gt(x)qt(x) = 0.                                          (44)\n                inf                      dx g(x)q(x)\u03b4F          [qt] (x) \u2212     F  [q] + \u2206t          dx q(x)g(x)2,                   (45)\n                 g F    [q] + \u2206t                              \u03b4qt                          4\n                inf      dx q(x)g(x)        \u03b4F     [qt] (x) \u2212     Eq(y)   \u03b4F    [qt]  (y)       + 1        dx q(x)g(x)2,            (46)\n                 g                               \u03b4q t                         \u03b4qt                   4\n                inf      dx q(x)      g(x) + 2      \u03b4F     [qt] (x) \u2212     Eq(y)   \u03b4F    [qt]  (y)    2    .                         (47)\n                 g                                       \u03b4q t                         \u03b4qt\nFrom the last optimization problem, we have\n                                     g(x) = \u22122        \u03b4F     [qt] (x) \u2212    Eq(y)    \u03b4F    [qt] (y)       .                          (48)\n                                                           \u03b4qt                          \u03b4qt\nNote, however, that DKL(\u00b7\u2225\u00b7) is not the same as the Fisher\u2013Rao metric. The derivations here\ndemonstrate that the Fisher\u2013Rao gradient flow can be derived as the MMS scheme with the KL-\ndivergence.\nA.2      Minimizing movement scheme for the Wasserstein Fisher\u2013Rao metric\nConsider the Wasserstein Fisher\u2013Rao distance\n    WFR\u03bb(p0, p1)2 :=                inf        1  Eqt(x)     \u2225vt(x)\u22252 + \u03bbgt(x)2             dt,     subj. to                        (49)\n                                 vt,gt,qt    0\n                 \u2202qt(x)\n                     \u2202t      = \u2212\u2207x \u00b7 (qt(x)vt(x)) + gt(x)qt(x) , q0(x) = p0(x), q1(x) = p1(x) .                                     (50)\nThe minimizing movement scheme (MMS) for this distance is\n                                           inf                           1                                                          (51)\n                                            q\u2032 F   [q\u2032] \u2212   F  [q] +   2\u2206tWFR\u03bb(q, q\u2032)2,\nwhere the change of the density is given by the continuity equation with the growth term\n                                          \u2202qt\n                                          \u2202t = \u2212\u2207x \u00b7 (qt(x)vt(x)) + gt(x)qt(x).                                                     (52)\nFor close enough q and q\u2032, WFR\u03bb(q, q\u2032)2 can be estimated via the metric derivative |\u00b5\u2032                                      t|2 that is\ndefined as\n                                            |\u00b5\u2032t|2 =        lim    WFR\u03bb(qt, qt+\u2206t)             2  ,                                 (53)\nhence,                                                    \u2206t\u21920                \u2206t\n                     WFR\u03bb(q, q\u2032)2 = \u2206t2|\u00b5\u2032              t|2 = \u2206t2     14   dx q(x)       \u2225v\u2217(x)\u22252 + \u03bbg\u2217(x)2              .          (54)", "md": "The KL-divergence is then\n$$\nDKL(q'\\|q) = \\int dx \\: q'(x) \\left(-\\Delta t \\: g_t(x) + \\Delta t^2 \\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} \\right) + o(\\Delta t^2) \\quad (40)\n$$\n$$\n= -\\Delta t \\int dx \\: q_t(x)g_t(x) + \\Delta t^2 \\int dx \\: q_t(x)\\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} + \\mathcal{O}(\\Delta t^2) \\quad (41)\n$$\n$$\n= \\Delta t^2 \\int dx \\: q_t(x)g^2_t(x) + \\mathcal{O}(\\Delta t^2) \\quad (43)\n$$\n\nIn the last equation we are using the normalization condition, i.e.,\n$$\n\\int dx \\: \\frac{\\partial q_t(x)}{\\partial t} = \\int dx \\: g_t(x)q_t(x) = 0 \\quad (44)\n$$\n\n$$\n\\begin{align*}\n&\\inf_{g} \\mathcal{F}[q] + \\Delta t \\left[ \\int dx \\: g(x)q(x)\\delta F[q_t](x) - F[q] + \\Delta t \\int dx \\: q(x)g(x)^2 \\right] \\quad (45) \\\\\n&\\inf_{g} \\int dx \\: q(x)g(x) \\delta F[q_t](x) - \\int dx \\: \\mathbb{E}_q(y) \\delta F[q_t](y) + \\frac{1}{4} \\int dx \\: q(x)g(x)^2 \\quad (46) \\\\\n&\\inf_{g} \\int dx \\: q(x)g(x) + 2 \\delta F[q_t](x) - \\mathbb{E}_q(y) \\delta F[q_t](y) \\frac{1}{4} \\quad (47)\n\\end{align*}\n$$\n\nFrom the last optimization problem, we have\n$$\ng(x) = -2 \\frac{\\delta F[q_t](x) - \\mathbb{E}_q(y) \\delta F[q_t](y)}{\\delta q_t} \\quad (48)\n$$\n\nNote, however, that DKL(\u00b7\u2225\u00b7) is not the same as the Fisher\u2013Rao metric. The derivations here demonstrate that the Fisher\u2013Rao gradient flow can be derived as the MMS scheme with the KL-divergence.\n\n### Minimizing movement scheme for the Wasserstein Fisher\u2013Rao metric\n\nConsider the Wasserstein Fisher\u2013Rao distance\n$$\nWFR_\\lambda(p_0, p_1)^2 = \\inf_{v_t, g_t, q_t} \\int \\frac{1}{\\sqrt{E[q_t](x)}} \\|v_t(x)\\|^2 + \\lambda g_t(x)^2 \\: dt, \\quad \\text{subj. to} \\quad (49)\n$$\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)) + g_t(x)q_t(x), \\quad q_0(x) = p_0(x), \\: q_1(x) = p_1(x) \\quad (50)\n$$\n\nThe minimizing movement scheme (MMS) for this distance is\n$$\n\\inf_{q'} \\mathcal{F}[q'] - \\mathcal{F}[q] + 2\\Delta t WFR_\\lambda(q, q')^2, \\quad (51)\n$$\nwhere the change of the density is given by the continuity equation with the growth term\n$$\n\\frac{\\partial q_t}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)) + g_t(x)q_t(x) \\quad (52)\n$$\n\nFor close enough q and q', WFR_\\lambda(q, q')^2 can be estimated via the metric derivative $|\\mu'_t|^2$ that is defined as\n$$\n|\\mu'_t|^2 = \\lim_{\\Delta t \\to 0} \\frac{WFR_\\lambda(q_t, q_{t+\\Delta t})^2}{\\Delta t} \\quad (53)\n$$\nhence,\n$$\nWFR_\\lambda(q, q')^2 = \\Delta t^2 |\\mu'_t|^2 = \\Delta t^2 \\frac{1}{4} \\int dx \\: q(x) \\|v^*(x)\\|^2 + \\lambda g^*(x)^2 \\quad (54)\n$$", "images": [], "items": [{"type": "text", "value": "The KL-divergence is then\n$$\nDKL(q'\\|q) = \\int dx \\: q'(x) \\left(-\\Delta t \\: g_t(x) + \\Delta t^2 \\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} \\right) + o(\\Delta t^2) \\quad (40)\n$$\n$$\n= -\\Delta t \\int dx \\: q_t(x)g_t(x) + \\Delta t^2 \\int dx \\: q_t(x)\\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} + \\mathcal{O}(\\Delta t^2) \\quad (41)\n$$\n$$\n= \\Delta t^2 \\int dx \\: q_t(x)g^2_t(x) + \\mathcal{O}(\\Delta t^2) \\quad (43)\n$$\n\nIn the last equation we are using the normalization condition, i.e.,\n$$\n\\int dx \\: \\frac{\\partial q_t(x)}{\\partial t} = \\int dx \\: g_t(x)q_t(x) = 0 \\quad (44)\n$$\n\n$$\n\\begin{align*}\n&\\inf_{g} \\mathcal{F}[q] + \\Delta t \\left[ \\int dx \\: g(x)q(x)\\delta F[q_t](x) - F[q] + \\Delta t \\int dx \\: q(x)g(x)^2 \\right] \\quad (45) \\\\\n&\\inf_{g} \\int dx \\: q(x)g(x) \\delta F[q_t](x) - \\int dx \\: \\mathbb{E}_q(y) \\delta F[q_t](y) + \\frac{1}{4} \\int dx \\: q(x)g(x)^2 \\quad (46) \\\\\n&\\inf_{g} \\int dx \\: q(x)g(x) + 2 \\delta F[q_t](x) - \\mathbb{E}_q(y) \\delta F[q_t](y) \\frac{1}{4} \\quad (47)\n\\end{align*}\n$$\n\nFrom the last optimization problem, we have\n$$\ng(x) = -2 \\frac{\\delta F[q_t](x) - \\mathbb{E}_q(y) \\delta F[q_t](y)}{\\delta q_t} \\quad (48)\n$$\n\nNote, however, that DKL(\u00b7\u2225\u00b7) is not the same as the Fisher\u2013Rao metric. The derivations here demonstrate that the Fisher\u2013Rao gradient flow can be derived as the MMS scheme with the KL-divergence.", "md": "The KL-divergence is then\n$$\nDKL(q'\\|q) = \\int dx \\: q'(x) \\left(-\\Delta t \\: g_t(x) + \\Delta t^2 \\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} \\right) + o(\\Delta t^2) \\quad (40)\n$$\n$$\n= -\\Delta t \\int dx \\: q_t(x)g_t(x) + \\Delta t^2 \\int dx \\: q_t(x)\\frac{\\partial^2 \\log q_t(x)}{\\partial t^2} + \\mathcal{O}(\\Delta t^2) \\quad (41)\n$$\n$$\n= \\Delta t^2 \\int dx \\: q_t(x)g^2_t(x) + \\mathcal{O}(\\Delta t^2) \\quad (43)\n$$\n\nIn the last equation we are using the normalization condition, i.e.,\n$$\n\\int dx \\: \\frac{\\partial q_t(x)}{\\partial t} = \\int dx \\: g_t(x)q_t(x) = 0 \\quad (44)\n$$\n\n$$\n\\begin{align*}\n&\\inf_{g} \\mathcal{F}[q] + \\Delta t \\left[ \\int dx \\: g(x)q(x)\\delta F[q_t](x) - F[q] + \\Delta t \\int dx \\: q(x)g(x)^2 \\right] \\quad (45) \\\\\n&\\inf_{g} \\int dx \\: q(x)g(x) \\delta F[q_t](x) - \\int dx \\: \\mathbb{E}_q(y) \\delta F[q_t](y) + \\frac{1}{4} \\int dx \\: q(x)g(x)^2 \\quad (46) \\\\\n&\\inf_{g} \\int dx \\: q(x)g(x) + 2 \\delta F[q_t](x) - \\mathbb{E}_q(y) \\delta F[q_t](y) \\frac{1}{4} \\quad (47)\n\\end{align*}\n$$\n\nFrom the last optimization problem, we have\n$$\ng(x) = -2 \\frac{\\delta F[q_t](x) - \\mathbb{E}_q(y) \\delta F[q_t](y)}{\\delta q_t} \\quad (48)\n$$\n\nNote, however, that DKL(\u00b7\u2225\u00b7) is not the same as the Fisher\u2013Rao metric. The derivations here demonstrate that the Fisher\u2013Rao gradient flow can be derived as the MMS scheme with the KL-divergence."}, {"type": "heading", "lvl": 3, "value": "Minimizing movement scheme for the Wasserstein Fisher\u2013Rao metric", "md": "### Minimizing movement scheme for the Wasserstein Fisher\u2013Rao metric"}, {"type": "text", "value": "Consider the Wasserstein Fisher\u2013Rao distance\n$$\nWFR_\\lambda(p_0, p_1)^2 = \\inf_{v_t, g_t, q_t} \\int \\frac{1}{\\sqrt{E[q_t](x)}} \\|v_t(x)\\|^2 + \\lambda g_t(x)^2 \\: dt, \\quad \\text{subj. to} \\quad (49)\n$$\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)) + g_t(x)q_t(x), \\quad q_0(x) = p_0(x), \\: q_1(x) = p_1(x) \\quad (50)\n$$\n\nThe minimizing movement scheme (MMS) for this distance is\n$$\n\\inf_{q'} \\mathcal{F}[q'] - \\mathcal{F}[q] + 2\\Delta t WFR_\\lambda(q, q')^2, \\quad (51)\n$$\nwhere the change of the density is given by the continuity equation with the growth term\n$$\n\\frac{\\partial q_t}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)) + g_t(x)q_t(x) \\quad (52)\n$$\n\nFor close enough q and q', WFR_\\lambda(q, q')^2 can be estimated via the metric derivative $|\\mu'_t|^2$ that is defined as\n$$", "md": "Consider the Wasserstein Fisher\u2013Rao distance\n$$\nWFR_\\lambda(p_0, p_1)^2 = \\inf_{v_t, g_t, q_t} \\int \\frac{1}{\\sqrt{E[q_t](x)}} \\|v_t(x)\\|^2 + \\lambda g_t(x)^2 \\: dt, \\quad \\text{subj. to} \\quad (49)\n$$\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)) + g_t(x)q_t(x), \\quad q_0(x) = p_0(x), \\: q_1(x) = p_1(x) \\quad (50)\n$$\n\nThe minimizing movement scheme (MMS) for this distance is\n$$\n\\inf_{q'} \\mathcal{F}[q'] - \\mathcal{F}[q] + 2\\Delta t WFR_\\lambda(q, q')^2, \\quad (51)\n$$\nwhere the change of the density is given by the continuity equation with the growth term\n$$\n\\frac{\\partial q_t}{\\partial t} = -\\nabla_x \\cdot (q_t(x)v_t(x)) + g_t(x)q_t(x) \\quad (52)\n$$\n\nFor close enough q and q', WFR_\\lambda(q, q')^2 can be estimated via the metric derivative $|\\mu'_t|^2$ that is defined as\n$$"}, {"type": "table", "rows": [["\\mu'_t"]], "md": "|\\mu'_t|^2 = \\lim_{\\Delta t \\to 0} \\frac{WFR_\\lambda(q_t, q_{t+\\Delta t})^2}{\\Delta t} \\quad (53)", "isPerfectTable": true, "csv": "\"\\mu'_t\""}, {"type": "text", "value": "$$\nhence,\n$$\nWFR_\\lambda(q, q')^2 = \\Delta t^2 |\\mu'_t|^2 = \\Delta t^2 \\frac{1}{4} \\int dx \\: q(x) \\|v^*(x)\\|^2 + \\lambda g^*(x)^2 \\quad (54)\n$$", "md": "$$\nhence,\n$$\nWFR_\\lambda(q, q')^2 = \\Delta t^2 |\\mu'_t|^2 = \\Delta t^2 \\frac{1}{4} \\int dx \\: q(x) \\|v^*(x)\\|^2 + \\lambda g^*(x)^2 \\quad (54)\n$$"}]}, {"page": 15, "text": "Thus, the MMS problem can be written as\n    inf                       dx \u2207x \u00b7 (q(x)v(x))\u03b4F              [qt](x) +         dx q(x)g(x)\u03b4F           [qt](x) \u2212     F  [q]\n    g,v F    [q] \u2212   \u2206t                                       \u03b4qt                                       \u03b4qt\n           + \u2206t 2       dx q(x)       \u2225v(x)\u22252 + \u03bbg(x)2             ,                                                                     (55)\n    inf        dx q(x)       v(x), \u2207x      \u03b4F  [qt]  (x)     +       dx q(x)g(x)        \u03b4F     [qt] (x) \u2212    Eq(y)    \u03b4F    [qt] (y)\n    g,v                                      \u03b4q t                                            \u03b4qt                          \u03b4qt\n           + 1 2      dx q(x)      \u2225v(x)\u22252 + \u03bbg(x)2             ,                                                                        (56)\n    inf        dx q(x)     v(x) + \u2207x         \u03b4F  [qt]  (x)    2\n    g,v                                        \u03b4q  t\n           + \u03bb       dx q(x)        g(x) + 1       \u03b4F    [qt]  (x) \u2212    Eq(y)    \u03b4F    [qt] (y)     2     .                              (57)\n                                               \u03bb       \u03b4qt                          \u03b4qt\nFrom the last optimization problem, we have\n                 v(x) = \u2212\u2207x          \u03b4F  [qt] (x),      g(x) = \u2212       1  \u03b4F     [qt] (x) \u2212     Eq(y)   \u03b4F    [qt]  (y)      .           (58)\n                                       \u03b4qt                             \u03bb       \u03b4qt                          \u03b4qt\nNote that different values of \u03bb result in different gradient flows. For instance, considering the limit\n\u03bb \u2192     \u221e    we have g(x) \u2192             0 and Eq. (52) just becomes W2 gradient flow, which is natural since\nwe have an infinite penalty for the mass teleportation in our metric. Setting \u03bb \u2192                                       0 requires some\nadditional consideration, since then the growth term explodes, and all the mass will be teleported\nwithout any cost. Indeed, for \u03bb \u2192                  0, our metric does not penalize for the mass teleportation at all,\nbut our change of density (Eq. (52)) is still able to teleport mass, hence, it will be doing so \u201cfor free\u201d.\nA.3      PDEs demonstrating the convergence\nConsider the change of the density qt under the continuity equation with the vector field vt(x) and\nthe growth term gt(x)\n                                       \u2202qt\n                                        \u2202t (x) = \u2212\u2207x \u00b7 (qt(x)vt(x)) + gt(x)qt(x).                                                        (59)\nThus, the change of the functional F                 [q] is\n             d                         dx \u2207x \u00b7 (qt(x)vt(x))\u03b4F              [qt](x) +           dx qt(x)gt(x)\u03b4F          [qt](x)          (60)\n             dtF   [qt] = \u2212                                              \u03b4qt                                          \u03b4qt\n                         =         dx qt(x)        vt(x), \u2207x      \u03b4F  [qt]  (x)     +       dx qt(x)gt(x)\u03b4F           [qt](x).           (61)\n                                                                    \u03b4qt                                             \u03b4qt\nFrom this equation, we can clearly see that vt(x) and gt(x) derived in the previous section minimize\nF [q]. Indeed, taking\n                vt(x) = \u2212\u2207x          \u03b4F  [qt] (x), gt(x) = \u2212           1  \u03b4F     [qt] (x) \u2212     Eqt(y)    \u03b4F   [qt]  (y)      ,          (62)\n                                       \u03b4qt                             \u03bb       \u03b4qt                           \u03b4qt\nwe get\n    d                         dx qt(x)     \u2207x     \u03b4F  [qt] (x)    2  \u2212   1       dx qt(x)      \u03b4F    [qt]  (x) \u2212    Eqt(y)   \u03b4F   [qt] (y)   2\n   dtF    [qt] = \u2212                                  \u03b4qt                  \u03bb                         \u03b4qt                          \u03b4qt      (63)\n                     \u2212   1       dx qt(x)      \u03b4F     [qt] (x) \u2212    Eqt(y)    \u03b4F    [qt] (y)       Eqt(z)    \u03b4F    [qt] (z)     \u2264   0.   (64)\n                         \u03bb                         \u03b4qt        =0      15          \u03b4qt                            \u03b4qt", "md": "Thus, the MMS problem can be written as\n\n$$\n\\begin{align*}\n&\\inf \\int dx \\nabla x \\cdot (q(x)v(x))\\delta F [qt](x) + \\int dx q(x)g(x)\\delta F [qt](x) - F[q] \\\\\n&+ \\Delta t \\int dx q(x) \\|v(x)\\|^2 + \\lambda g(x)^2, \\quad (55) \\\\\n&\\inf \\int dx q(x) v(x), \\nabla x \\delta F [qt](x) + \\int dx q(x)g(x) \\delta F [qt](x) - Eq(y) \\delta F [qt](y) \\\\\n&+ \\frac{1}{2} \\int dx q(x) \\|v(x)\\|^2 + \\lambda g(x)^2, \\quad (56) \\\\\n&\\inf \\int dx q(x) v(x) + \\nabla x \\delta F [qt](x) \\\\\n&+ \\lambda \\int dx q(x) g(x) + \\frac{1}{\\delta F [qt](x) - Eq(y) \\delta F [qt](y)}^2. \\quad (57)\n\\end{align*}\n$$\nFrom the last optimization problem, we have\n\n$$\n\\begin{align*}\nv(x) &= -\\nabla x \\delta F [qt](x), \\\\\ng(x) &= -\\frac{1}{\\delta F [qt](x) - Eq(y) \\delta F [qt](y)}. \\quad (58)\n\\end{align*}\n$$\nNote that different values of \u03bb result in different gradient flows. For instance, considering the limit\n\n$$\n\\lambda \\to \\infty, \\quad \\text{we have} \\quad g(x) \\to 0 \\quad \\text{and Eq. (52) just becomes W2 gradient flow, which is natural since we have an infinite penalty for the mass teleportation in our metric. Setting} \\quad \\lambda \\to 0 \\quad \\text{requires some additional consideration, since then the growth term explodes, and all the mass will be teleported without any cost. Indeed, for} \\quad \\lambda \\to 0, \\quad \\text{our metric does not penalize for the mass teleportation at all, but our change of density (Eq. (52)) is still able to teleport mass, hence, it will be doing so \u201cfor free\u201d.\n\nA.3 PDEs demonstrating the convergence\n\nConsider the change of the density qt under the continuity equation with the vector field vt(x) and the growth term gt(x)\n\n$$\n\\frac{\\partial qt}{\\partial t}(x) = -\\nabla x \\cdot (qt(x)vt(x)) + gt(x)qt(x). \\quad (59)\n$$\nThus, the change of the functional F [q] is\n\n$$\n\\begin{align*}\n\\frac{d}{dt}F [qt] &= -\\int dx \\nabla x \\cdot (qt(x)vt(x))\\delta F [qt](x) + \\int dx qt(x)gt(x)\\delta F [qt](x) \\\\\n&= \\int dx qt(x) vt(x), \\nabla x \\delta F [qt](x) + \\int dx qt(x)gt(x)\\delta F [qt](x). \\quad (61)\n\\end{align*}\n$$\nFrom this equation, we can clearly see that vt(x) and gt(x) derived in the previous section minimize F [q]. Indeed, taking\n\n$$\n\\begin{align*}\nvt(x) &= -\\nabla x \\delta F [qt](x), \\\\\ngt(x) &= -\\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y)}, \\quad (62)\n\\end{align*}\n$$\nwe get\n\n$$\n\\begin{align*}\n\\frac{d}{dt}F [qt] &= -\\int dx qt(x) \\nabla x \\delta F [qt](x)^2 - \\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y)^2} \\\\\n&- \\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y) - Eqt(z) \\delta F [qt](z)} \\leq 0. \\quad (64)\n\\end{align*}\n$$\n$$\n\\lambda \\delta qt = 0, \\quad 15 \\delta qt = 0\n$$", "images": [], "items": [{"type": "text", "value": "Thus, the MMS problem can be written as\n\n$$\n\\begin{align*}\n&\\inf \\int dx \\nabla x \\cdot (q(x)v(x))\\delta F [qt](x) + \\int dx q(x)g(x)\\delta F [qt](x) - F[q] \\\\\n&+ \\Delta t \\int dx q(x) \\|v(x)\\|^2 + \\lambda g(x)^2, \\quad (55) \\\\\n&\\inf \\int dx q(x) v(x), \\nabla x \\delta F [qt](x) + \\int dx q(x)g(x) \\delta F [qt](x) - Eq(y) \\delta F [qt](y) \\\\\n&+ \\frac{1}{2} \\int dx q(x) \\|v(x)\\|^2 + \\lambda g(x)^2, \\quad (56) \\\\\n&\\inf \\int dx q(x) v(x) + \\nabla x \\delta F [qt](x) \\\\\n&+ \\lambda \\int dx q(x) g(x) + \\frac{1}{\\delta F [qt](x) - Eq(y) \\delta F [qt](y)}^2. \\quad (57)\n\\end{align*}\n$$\nFrom the last optimization problem, we have\n\n$$\n\\begin{align*}\nv(x) &= -\\nabla x \\delta F [qt](x), \\\\\ng(x) &= -\\frac{1}{\\delta F [qt](x) - Eq(y) \\delta F [qt](y)}. \\quad (58)\n\\end{align*}\n$$\nNote that different values of \u03bb result in different gradient flows. For instance, considering the limit\n\n$$\n\\lambda \\to \\infty, \\quad \\text{we have} \\quad g(x) \\to 0 \\quad \\text{and Eq. (52) just becomes W2 gradient flow, which is natural since we have an infinite penalty for the mass teleportation in our metric. Setting} \\quad \\lambda \\to 0 \\quad \\text{requires some additional consideration, since then the growth term explodes, and all the mass will be teleported without any cost. Indeed, for} \\quad \\lambda \\to 0, \\quad \\text{our metric does not penalize for the mass teleportation at all, but our change of density (Eq. (52)) is still able to teleport mass, hence, it will be doing so \u201cfor free\u201d.\n\nA.3 PDEs demonstrating the convergence\n\nConsider the change of the density qt under the continuity equation with the vector field vt(x) and the growth term gt(x)\n\n$$\n\\frac{\\partial qt}{\\partial t}(x) = -\\nabla x \\cdot (qt(x)vt(x)) + gt(x)qt(x). \\quad (59)\n$$\nThus, the change of the functional F [q] is\n\n$$\n\\begin{align*}\n\\frac{d}{dt}F [qt] &= -\\int dx \\nabla x \\cdot (qt(x)vt(x))\\delta F [qt](x) + \\int dx qt(x)gt(x)\\delta F [qt](x) \\\\\n&= \\int dx qt(x) vt(x), \\nabla x \\delta F [qt](x) + \\int dx qt(x)gt(x)\\delta F [qt](x). \\quad (61)\n\\end{align*}\n$$\nFrom this equation, we can clearly see that vt(x) and gt(x) derived in the previous section minimize F [q]. Indeed, taking\n\n$$\n\\begin{align*}\nvt(x) &= -\\nabla x \\delta F [qt](x), \\\\\ngt(x) &= -\\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y)}, \\quad (62)\n\\end{align*}\n$$\nwe get\n\n$$\n\\begin{align*}\n\\frac{d}{dt}F [qt] &= -\\int dx qt(x) \\nabla x \\delta F [qt](x)^2 - \\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y)^2} \\\\\n&- \\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y) - Eqt(z) \\delta F [qt](z)} \\leq 0. \\quad (64)\n\\end{align*}\n$$\n$$\n\\lambda \\delta qt = 0, \\quad 15 \\delta qt = 0\n$$", "md": "Thus, the MMS problem can be written as\n\n$$\n\\begin{align*}\n&\\inf \\int dx \\nabla x \\cdot (q(x)v(x))\\delta F [qt](x) + \\int dx q(x)g(x)\\delta F [qt](x) - F[q] \\\\\n&+ \\Delta t \\int dx q(x) \\|v(x)\\|^2 + \\lambda g(x)^2, \\quad (55) \\\\\n&\\inf \\int dx q(x) v(x), \\nabla x \\delta F [qt](x) + \\int dx q(x)g(x) \\delta F [qt](x) - Eq(y) \\delta F [qt](y) \\\\\n&+ \\frac{1}{2} \\int dx q(x) \\|v(x)\\|^2 + \\lambda g(x)^2, \\quad (56) \\\\\n&\\inf \\int dx q(x) v(x) + \\nabla x \\delta F [qt](x) \\\\\n&+ \\lambda \\int dx q(x) g(x) + \\frac{1}{\\delta F [qt](x) - Eq(y) \\delta F [qt](y)}^2. \\quad (57)\n\\end{align*}\n$$\nFrom the last optimization problem, we have\n\n$$\n\\begin{align*}\nv(x) &= -\\nabla x \\delta F [qt](x), \\\\\ng(x) &= -\\frac{1}{\\delta F [qt](x) - Eq(y) \\delta F [qt](y)}. \\quad (58)\n\\end{align*}\n$$\nNote that different values of \u03bb result in different gradient flows. For instance, considering the limit\n\n$$\n\\lambda \\to \\infty, \\quad \\text{we have} \\quad g(x) \\to 0 \\quad \\text{and Eq. (52) just becomes W2 gradient flow, which is natural since we have an infinite penalty for the mass teleportation in our metric. Setting} \\quad \\lambda \\to 0 \\quad \\text{requires some additional consideration, since then the growth term explodes, and all the mass will be teleported without any cost. Indeed, for} \\quad \\lambda \\to 0, \\quad \\text{our metric does not penalize for the mass teleportation at all, but our change of density (Eq. (52)) is still able to teleport mass, hence, it will be doing so \u201cfor free\u201d.\n\nA.3 PDEs demonstrating the convergence\n\nConsider the change of the density qt under the continuity equation with the vector field vt(x) and the growth term gt(x)\n\n$$\n\\frac{\\partial qt}{\\partial t}(x) = -\\nabla x \\cdot (qt(x)vt(x)) + gt(x)qt(x). \\quad (59)\n$$\nThus, the change of the functional F [q] is\n\n$$\n\\begin{align*}\n\\frac{d}{dt}F [qt] &= -\\int dx \\nabla x \\cdot (qt(x)vt(x))\\delta F [qt](x) + \\int dx qt(x)gt(x)\\delta F [qt](x) \\\\\n&= \\int dx qt(x) vt(x), \\nabla x \\delta F [qt](x) + \\int dx qt(x)gt(x)\\delta F [qt](x). \\quad (61)\n\\end{align*}\n$$\nFrom this equation, we can clearly see that vt(x) and gt(x) derived in the previous section minimize F [q]. Indeed, taking\n\n$$\n\\begin{align*}\nvt(x) &= -\\nabla x \\delta F [qt](x), \\\\\ngt(x) &= -\\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y)}, \\quad (62)\n\\end{align*}\n$$\nwe get\n\n$$\n\\begin{align*}\n\\frac{d}{dt}F [qt] &= -\\int dx qt(x) \\nabla x \\delta F [qt](x)^2 - \\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y)^2} \\\\\n&- \\frac{1}{\\delta F [qt](x) - Eqt(y) \\delta F [qt](y) - Eqt(z) \\delta F [qt](z)} \\leq 0. \\quad (64)\n\\end{align*}\n$$\n$$\n\\lambda \\delta qt = 0, \\quad 15 \\delta qt = 0\n$$"}]}, {"page": 16, "text": "Note that the considered growth term preserves the normalization of the density, i.e.,\n        dx \u2202qt\u2202t (x) =           dx gt(x)qt(x) = \u2212                 dx qt(x)      \u03b4F    [qt]  (x) \u2212    Eqt(y)    \u03b4F    [qt] (y)       = 0.\n                                                                                     \u03b4qt                            \u03b4qt\n                                                                                                                                         (65)\nThus, our functional F           [q] decreases when the density qt evolves according to the PDE\n   \u2202qt                         qt(x)      \u2212   \u2207x   \u03b4F   [qt] (x)        \u2212  1   \u03b4F     [qt] (x) \u2212    Eqt(y)    \u03b4F    [qt]  (y)      qt(x),\n    \u2202t (x) = \u2212\u2207x \u00b7                                    \u03b4qt                  \u03bb        \u03b4qt                           \u03b4qt\n                                 the continuity equation                                          growth term                            (66)\nand reaches its stationary point when                        \u03b4F [qt]        2  = 0, i.e., \u03b4F [qt]\n                                                       \u2207x      \u03b4qt (x)                         \u03b4qt (x) \u2261       constant.\nNote, that in the same way we can consider the continuity equation and the growth term separately,\nwhich defines the gradient flows under 2-Wasserstein and Fisher\u2013Rao metrics respectively. The\ncorresponding PDEs are\n                               \u2202qt                           qt(x)      \u2212   \u2207x   \u03b4F   [qt] (x)       ,                                   (67)\n                               \u2202t (x) = \u2212          \u2207x \u00b7                             \u03b4qt\n                               \u2202qt                  \u03b4F     [qt] (x) \u2212    Eqt(y)    \u03b4F    [qt]  (y)      qt(x).                           (68)\n                               \u2202t (x) = \u2212                \u03b4qt                           \u03b4qt\nB      Imaginary-time Schr\u00f6dinger equation as the gradient flow under\n       Fisher\u2013Rao Metric\nTheorem. Eq. (12) defines the gradient flow of the energy functional E[q] under the Fisher\u2013Rao\nmetric.\nProof. First, we derive the functional derivative of the energy functional E[q]. We denote the\ndifferential of the functional F            (q) along the direction h as\nwhere \u03b4F      [q]/\u03b4q is the functional derivative.dF   (q)[h] =          dx h \u00b7 \u03b4F    \u03b4q[q],                                             (69)\nConsider the energy functional\n                                  E[q] =          dx q     V \u2212     1    x log q \u2212     1                     .                            (70)\nThe functional derivative of this functional is as follows         4\u22072                8\u2225\u2207x log q\u22252\ndE(q)[h] = \u2202E(q + \u03b5 \u00b7 h)   \u2202\u03b5           \u03b5=0                                                       1        h                              h\n              =        dx h     V \u2212     1    x log q \u2212      1                     \u2212        dx q          x                                     .\n                                        4\u22072                 8\u2225\u2207x log q\u22252                            4\u22072    q + 1   4\u27e8\u2207x log q, \u2207x         q \u27e9\nFor the last term, we do integration by parts and get\n    dx q    1      x h                             h      = \u22121           dx      \u2207xq, \u2207x      h     + 1         dx      \u2207xq, \u2207x      h     = 0.\nThus, we have 4\u22072    q + 1  4\u27e8\u2207x log q, \u2207x          q \u27e9          4                            q         4                            q   (71)\n                              dE(q)[h] =             dx h      V \u2212    1     x log q \u2212     1                     ,                        (72)\n                                                                      4\u22072        \u03b4E[q]/\u03b4q 8\u2225\u2207x log q\u22252\n                                                                      16", "md": "Note that the considered growth term preserves the normalization of the density, i.e.,\n\n$$\n\\int dx \\frac{\\partial q_t}{\\partial t}(x) = \\int dx \\, g_t(x)q_t(x) = -\\int dx \\, q_t(x) \\frac{\\delta F [q_t](x)}{\\delta q_t} - \\int E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} = 0. \\quad (65)\n$$\n\nThus, our functional $F[q]$ decreases when the density $q_t$ evolves according to the PDE\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\nabla_x \\cdot \\frac{\\delta F [q_t](x)}{\\delta q_t} - \\frac{1}{\\lambda} \\frac{\\delta F [q_t](x)}{\\delta q_t} - E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} q_t(x),\n$$\n\nthe continuity equation \\textit{(66)} growth term and reaches its stationary point when $\\nabla_x \\frac{\\delta F [q_t]}{\\delta q_t}(x) = 0$, i.e., $\\frac{\\delta F [q_t]}{\\delta q_t}(x) \\equiv$ constant.\n\nNote, that in the same way we can consider the continuity equation and the growth term separately, which defines the gradient flows under 2-Wasserstein and Fisher\u2013Rao metrics respectively. The corresponding PDEs are\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\nabla_x \\cdot \\frac{\\delta F [q_t](x)}{\\delta q_t}, \\quad (67)\n$$\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\frac{\\delta F [q_t](x)}{\\delta q_t} - E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} q_t(x). \\quad (68)\n$$\n\n### Imaginary-time Schr\u00f6dinger equation as the gradient flow under Fisher\u2013Rao Metric\n\n**Theorem.** Eq. (12) defines the gradient flow of the energy functional $E[q]$ under the Fisher\u2013Rao metric.\n\n**Proof.** First, we derive the functional derivative of the energy functional $E[q]$. We denote the differential of the functional $F(q)$ along the direction $h$ as\n\n$$\n\\frac{\\delta F [q]}{\\delta q} = \\text{functional derivative}. \\quad (69)\n$$\n\nConsider the energy functional\n\n$$\nE[q] = \\int dx \\, q V - \\frac{1}{x} \\log q - 1. \\quad (70)\n$$\n\nThe functional derivative of this functional is as follows\n\n$$\n\\begin{aligned}\n\\frac{dE(q)[h]}{\\partial \\epsilon} &= \\left. \\frac{\\partial E(q + \\epsilon \\cdot h)}{\\partial \\epsilon} \\right|_{\\epsilon=0} \\\\\n&= \\int dx \\, h V - \\frac{1}{x} \\log q - 1 - \\int dx \\, q \\frac{x}{4\\nabla^2} \\left(8\\| \\nabla_x \\log q \\| ^2\\right).\n\\end{aligned}\n$$\n\nFor the last term, we do integration by parts and get\n\n$$\n\\int dx \\, q \\frac{1}{4\\nabla^2} x h = -\\int dx \\, \\nabla_x q \\cdot \\nabla_x h + \\int dx \\, \\nabla_x q \\cdot \\nabla_x h = 0.\n$$\n\nThus, we have\n\n$$\n\\frac{4\\nabla^2 q + 1}{4} \\left\\langle \\nabla_x \\log q, \\nabla_x q \\right\\rangle = \\frac{4}{q} \\frac{4}{q}. \\quad (71)\n$$\n\n$$\n\\frac{dE(q)[h]}{dx} = \\int dx \\, h V - \\frac{1}{x} \\log q - 1, \\quad (72)\n$$\n\n$$\n\\frac{4\\nabla^2}{\\delta E[q]/\\delta q} 8\\| \\nabla_x \\log q \\| ^2\n$$", "images": [], "items": [{"type": "text", "value": "Note that the considered growth term preserves the normalization of the density, i.e.,\n\n$$\n\\int dx \\frac{\\partial q_t}{\\partial t}(x) = \\int dx \\, g_t(x)q_t(x) = -\\int dx \\, q_t(x) \\frac{\\delta F [q_t](x)}{\\delta q_t} - \\int E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} = 0. \\quad (65)\n$$\n\nThus, our functional $F[q]$ decreases when the density $q_t$ evolves according to the PDE\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\nabla_x \\cdot \\frac{\\delta F [q_t](x)}{\\delta q_t} - \\frac{1}{\\lambda} \\frac{\\delta F [q_t](x)}{\\delta q_t} - E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} q_t(x),\n$$\n\nthe continuity equation \\textit{(66)} growth term and reaches its stationary point when $\\nabla_x \\frac{\\delta F [q_t]}{\\delta q_t}(x) = 0$, i.e., $\\frac{\\delta F [q_t]}{\\delta q_t}(x) \\equiv$ constant.\n\nNote, that in the same way we can consider the continuity equation and the growth term separately, which defines the gradient flows under 2-Wasserstein and Fisher\u2013Rao metrics respectively. The corresponding PDEs are\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\nabla_x \\cdot \\frac{\\delta F [q_t](x)}{\\delta q_t}, \\quad (67)\n$$\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\frac{\\delta F [q_t](x)}{\\delta q_t} - E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} q_t(x). \\quad (68)\n$$", "md": "Note that the considered growth term preserves the normalization of the density, i.e.,\n\n$$\n\\int dx \\frac{\\partial q_t}{\\partial t}(x) = \\int dx \\, g_t(x)q_t(x) = -\\int dx \\, q_t(x) \\frac{\\delta F [q_t](x)}{\\delta q_t} - \\int E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} = 0. \\quad (65)\n$$\n\nThus, our functional $F[q]$ decreases when the density $q_t$ evolves according to the PDE\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\nabla_x \\cdot \\frac{\\delta F [q_t](x)}{\\delta q_t} - \\frac{1}{\\lambda} \\frac{\\delta F [q_t](x)}{\\delta q_t} - E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} q_t(x),\n$$\n\nthe continuity equation \\textit{(66)} growth term and reaches its stationary point when $\\nabla_x \\frac{\\delta F [q_t]}{\\delta q_t}(x) = 0$, i.e., $\\frac{\\delta F [q_t]}{\\delta q_t}(x) \\equiv$ constant.\n\nNote, that in the same way we can consider the continuity equation and the growth term separately, which defines the gradient flows under 2-Wasserstein and Fisher\u2013Rao metrics respectively. The corresponding PDEs are\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\nabla_x \\cdot \\frac{\\delta F [q_t](x)}{\\delta q_t}, \\quad (67)\n$$\n\n$$\n\\frac{\\partial q_t}{\\partial t}(x) = -\\frac{\\delta F [q_t](x)}{\\delta q_t} - E[q_t(y)] \\frac{\\delta F [q_t](y)}{\\delta q_t} q_t(x). \\quad (68)\n$$"}, {"type": "heading", "lvl": 3, "value": "Imaginary-time Schr\u00f6dinger equation as the gradient flow under Fisher\u2013Rao Metric", "md": "### Imaginary-time Schr\u00f6dinger equation as the gradient flow under Fisher\u2013Rao Metric"}, {"type": "text", "value": "**Theorem.** Eq. (12) defines the gradient flow of the energy functional $E[q]$ under the Fisher\u2013Rao metric.\n\n**Proof.** First, we derive the functional derivative of the energy functional $E[q]$. We denote the differential of the functional $F(q)$ along the direction $h$ as\n\n$$\n\\frac{\\delta F [q]}{\\delta q} = \\text{functional derivative}. \\quad (69)\n$$\n\nConsider the energy functional\n\n$$\nE[q] = \\int dx \\, q V - \\frac{1}{x} \\log q - 1. \\quad (70)\n$$\n\nThe functional derivative of this functional is as follows\n\n$$\n\\begin{aligned}\n\\frac{dE(q)[h]}{\\partial \\epsilon} &= \\left. \\frac{\\partial E(q + \\epsilon \\cdot h)}{\\partial \\epsilon} \\right|_{\\epsilon=0} \\\\\n&= \\int dx \\, h V - \\frac{1}{x} \\log q - 1 - \\int dx \\, q \\frac{x}{4\\nabla^2} \\left(8\\| \\nabla_x \\log q \\| ^2\\right).\n\\end{aligned}\n$$\n\nFor the last term, we do integration by parts and get\n\n$$\n\\int dx \\, q \\frac{1}{4\\nabla^2} x h = -\\int dx \\, \\nabla_x q \\cdot \\nabla_x h + \\int dx \\, \\nabla_x q \\cdot \\nabla_x h = 0.\n$$\n\nThus, we have\n\n$$\n\\frac{4\\nabla^2 q + 1}{4} \\left\\langle \\nabla_x \\log q, \\nabla_x q \\right\\rangle = \\frac{4}{q} \\frac{4}{q}. \\quad (71)\n$$\n\n$$\n\\frac{dE(q)[h]}{dx} = \\int dx \\, h V - \\frac{1}{x} \\log q - 1, \\quad (72)\n$$\n\n$$\n\\frac{4\\nabla^2}{\\delta E[q]/\\delta q} 8\\| \\nabla_x \\log q \\| ^2\n$$", "md": "**Theorem.** Eq. (12) defines the gradient flow of the energy functional $E[q]$ under the Fisher\u2013Rao metric.\n\n**Proof.** First, we derive the functional derivative of the energy functional $E[q]$. We denote the differential of the functional $F(q)$ along the direction $h$ as\n\n$$\n\\frac{\\delta F [q]}{\\delta q} = \\text{functional derivative}. \\quad (69)\n$$\n\nConsider the energy functional\n\n$$\nE[q] = \\int dx \\, q V - \\frac{1}{x} \\log q - 1. \\quad (70)\n$$\n\nThe functional derivative of this functional is as follows\n\n$$\n\\begin{aligned}\n\\frac{dE(q)[h]}{\\partial \\epsilon} &= \\left. \\frac{\\partial E(q + \\epsilon \\cdot h)}{\\partial \\epsilon} \\right|_{\\epsilon=0} \\\\\n&= \\int dx \\, h V - \\frac{1}{x} \\log q - 1 - \\int dx \\, q \\frac{x}{4\\nabla^2} \\left(8\\| \\nabla_x \\log q \\| ^2\\right).\n\\end{aligned}\n$$\n\nFor the last term, we do integration by parts and get\n\n$$\n\\int dx \\, q \\frac{1}{4\\nabla^2} x h = -\\int dx \\, \\nabla_x q \\cdot \\nabla_x h + \\int dx \\, \\nabla_x q \\cdot \\nabla_x h = 0.\n$$\n\nThus, we have\n\n$$\n\\frac{4\\nabla^2 q + 1}{4} \\left\\langle \\nabla_x \\log q, \\nabla_x q \\right\\rangle = \\frac{4}{q} \\frac{4}{q}. \\quad (71)\n$$\n\n$$\n\\frac{dE(q)[h]}{dx} = \\int dx \\, h V - \\frac{1}{x} \\log q - 1, \\quad (72)\n$$\n\n$$\n\\frac{4\\nabla^2}{\\delta E[q]/\\delta q} 8\\| \\nabla_x \\log q \\| ^2\n$$"}]}, {"page": 17, "text": "and we see that the derivative coincides with the local energy, i.e.,\n                      \u03b4E[q]   (x) = Eloc(x) = V (x) \u2212            1    x log q(x) \u2212     1                              (73)\n                        \u03b4q                                       4\u22072                   8\u2225\u2207x log q(x)\u22252.\nUsing the results from Section 2.2, the energy-minimizing gradient flow under Fisher\u2013Rao metric is\n                                         \u2202qt(x)    = \u2212       Eloc(x) \u2212     E[qt]    qt(x).                            (74)\n                                            \u2202t\nSecond, we derive the PDE for the time-evolution of the density qt under the imaginary-time\nSchr\u00f6dinger equation.\n                                             \u2202\u03c8t\n                                              \u2202t = 1    2\u22072  x\u03c8t \u2212    (V \u2212    E[qt])\u03c8t                                (75)\n                                             \u2202\u03c8t\n                                        2\u03c8t   \u2202t = \u03c8t\u22072       x\u03c8t \u2212    2(V \u2212     E[qt])\u03c82  t                          (76)\n                                              \u2202qt\n                                              \u2202t = \u03c8t\u22072       x\u03c8t \u2212    2(V \u2212     E[qt])qt                             (77)\n                                                                                                                      (78)\nUsing the identity\n               \u03c8\u22072  x\u03c8 = \u03c8\u2207x \u00b7 (\u03c8\u2207x log |\u03c8|) = \u27e8\u03c8\u2207x\u03c8, \u2207x log |\u03c8|\u27e9                        + \u03c82\u22072    x log |\u03c8|          (79)\nwe have                  = 1  4\u27e8\u2207xq, \u2207x log q\u27e9        + 1 2q\u22072  x log q = 1  4q\u2225\u2207x log q\u22252 + 1        2q\u22072  x log q,  (80)\n                              \u2202qt               V \u2212    1    x log q \u2212    1                               qt           (81)\n                               \u2202t = \u2212       2          4\u22072               8\u2225\u2207x log qt\u22252 \u2212        E[qt]\n                         \u2202qt(x)     = \u2212     2   Eloc(x) \u2212     E[qt]   qt(x),                                          (82)\n                            \u2202t\nwhich is equivalent to Eq. (74).\nC      Following the gradient flow by a parametric model\nProposition. For qt(x) = q(x, \u03b8), the KL-divergence can be expanded as\n              DKL(qt+\u2206t(x)\u2225q(x, \u03b8 + \u2206\u03b8)) = \u2212                    1        \u2206\u03b8,       \u2202\nProof. We have                                              + o 2\u2206t    \u2206t2 + \u2225\u2206\u03b8\u22252 \u2202tqt(x)\u2207\u03b8 log q(x, \u03b8) dx           (83)\n                         DKL(qt(x)\u2225q(x, \u03b8)) =                 qt(x) log qt(x)                                         (84)\n              =\u21d2      \u2202                                          qt(x) \u2202   q(x, \u03b8) dx                                 (85)\n                     \u2202\u03b8DKL(qt(x)\u2225q(x, \u03b8)) = \u2212        =    0              \u2202\u03b8 log q(x, \u03b8) dx         qt(x) = q(x, \u03b8)    (86)\n                                                            \u2212     qt(x) \u2202\u2202\u03b8 log q(x, \u03b8) dx         qt(x) \u0338= q(x, \u03b8)\n                                                                 17", "md": "and we see that the derivative coincides with the local energy, i.e.,\n\n$$\n\\delta E[q](x) = E_{\\text{loc}}(x) = V(x) - \\frac{1}{4}\\nabla^2x \\log q(x) - \\frac{1}{8}\\|\\nabla x \\log q(x)\\|^2. \\tag{73}\n$$\nUsing the results from Section 2.2, the energy-minimizing gradient flow under Fisher\u2013Rao metric is\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -E_{\\text{loc}}(x) - E[qt] \\, qt(x). \\tag{74}\n$$\nSecond, we derive the PDE for the time-evolution of the density qt under the imaginary-time Schr\u00f6dinger equation.\n\n$$\n\\begin{align*}\n\\frac{\\partial \\psi_t}{\\partial t} &= \\frac{1}{2}\\nabla^2x \\psi_t - (V - E[qt])\\psi_t \\tag{75} \\\\\n2\\psi_t \\frac{\\partial t}{\\partial t} &= \\psi_t \\nabla^2x \\psi_t - 2(V - E[qt])\\psi_t^2 \\tag{76} \\\\\n\\frac{\\partial qt}{\\partial t} &= \\psi_t \\nabla^2x \\psi_t - 2(V - E[qt])qt. \\tag{77}\n\\end{align*}\n$$\n$$\n\\begin{aligned}\n\\text{Using the identity} \\\\\n\\psi \\nabla^2x \\psi &= \\langle \\psi \\nabla x \\psi, \\nabla x \\log |\\psi|\\rangle + \\psi^2\\nabla^2x \\log |\\psi| \\tag{79} \\\\\n&= \\frac{1}{4}\\langle \\nabla x q, \\nabla x \\log q\\rangle + \\frac{1}{2}q\\nabla^2x \\log q = \\frac{1}{4}q\\|\\nabla x \\log q\\|^2 + \\frac{1}{2}q\\nabla^2x \\log q, \\tag{80}\n\\end{aligned}\n$$\n$$\n\\frac{\\partial qt}{\\partial t} = -\\frac{1}{2}E_{\\text{loc}}(x) - E[qt] \\, qt(x), \\tag{82}\n$$\nwhich is equivalent to Eq. (74).\n\nFollowing the gradient flow by a parametric model\n\nProposition. For qt(x) = q(x, \u03b8), the KL-divergence can be expanded as\n\n$$\nDKL(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)) = -\\frac{1}{2}\\Delta \\theta, \\frac{\\partial}{\\partial t} + o(2\\Delta t \\Delta t^2 + \\|\\Delta \\theta\\|^2 \\frac{\\partial t}{\\partial t}q_t(x)\\nabla \\theta \\log q(x, \\theta) dx. \\tag{83}\n$$\n$$\nDKL(q_t(x) \\| q(x, \\theta)) = \\int q_t(x) \\log q_t(x) dx \\tag{84}\n$$\n$$\n\\Rightarrow \\frac{\\partial}{\\partial \\theta}DKL(q_t(x) \\| q(x, \\theta)) = -\\int \\frac{\\partial}{\\partial \\theta}q_t(x) \\log q(x, \\theta) dx = 0 \\text{ if } q_t(x) = q(x, \\theta) \\tag{86}\n$$", "images": [], "items": [{"type": "text", "value": "and we see that the derivative coincides with the local energy, i.e.,\n\n$$\n\\delta E[q](x) = E_{\\text{loc}}(x) = V(x) - \\frac{1}{4}\\nabla^2x \\log q(x) - \\frac{1}{8}\\|\\nabla x \\log q(x)\\|^2. \\tag{73}\n$$\nUsing the results from Section 2.2, the energy-minimizing gradient flow under Fisher\u2013Rao metric is\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -E_{\\text{loc}}(x) - E[qt] \\, qt(x). \\tag{74}\n$$\nSecond, we derive the PDE for the time-evolution of the density qt under the imaginary-time Schr\u00f6dinger equation.\n\n$$\n\\begin{align*}\n\\frac{\\partial \\psi_t}{\\partial t} &= \\frac{1}{2}\\nabla^2x \\psi_t - (V - E[qt])\\psi_t \\tag{75} \\\\\n2\\psi_t \\frac{\\partial t}{\\partial t} &= \\psi_t \\nabla^2x \\psi_t - 2(V - E[qt])\\psi_t^2 \\tag{76} \\\\\n\\frac{\\partial qt}{\\partial t} &= \\psi_t \\nabla^2x \\psi_t - 2(V - E[qt])qt. \\tag{77}\n\\end{align*}\n$$\n$$\n\\begin{aligned}\n\\text{Using the identity} \\\\\n\\psi \\nabla^2x \\psi &= \\langle \\psi \\nabla x \\psi, \\nabla x \\log |\\psi|\\rangle + \\psi^2\\nabla^2x \\log |\\psi| \\tag{79} \\\\\n&= \\frac{1}{4}\\langle \\nabla x q, \\nabla x \\log q\\rangle + \\frac{1}{2}q\\nabla^2x \\log q = \\frac{1}{4}q\\|\\nabla x \\log q\\|^2 + \\frac{1}{2}q\\nabla^2x \\log q, \\tag{80}\n\\end{aligned}\n$$\n$$\n\\frac{\\partial qt}{\\partial t} = -\\frac{1}{2}E_{\\text{loc}}(x) - E[qt] \\, qt(x), \\tag{82}\n$$\nwhich is equivalent to Eq. (74).\n\nFollowing the gradient flow by a parametric model\n\nProposition. For qt(x) = q(x, \u03b8), the KL-divergence can be expanded as\n\n$$\nDKL(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)) = -\\frac{1}{2}\\Delta \\theta, \\frac{\\partial}{\\partial t} + o(2\\Delta t \\Delta t^2 + \\|\\Delta \\theta\\|^2 \\frac{\\partial t}{\\partial t}q_t(x)\\nabla \\theta \\log q(x, \\theta) dx. \\tag{83}\n$$\n$$\nDKL(q_t(x) \\| q(x, \\theta)) = \\int q_t(x) \\log q_t(x) dx \\tag{84}\n$$\n$$\n\\Rightarrow \\frac{\\partial}{\\partial \\theta}DKL(q_t(x) \\| q(x, \\theta)) = -\\int \\frac{\\partial}{\\partial \\theta}q_t(x) \\log q(x, \\theta) dx = 0 \\text{ if } q_t(x) = q(x, \\theta) \\tag{86}\n$$", "md": "and we see that the derivative coincides with the local energy, i.e.,\n\n$$\n\\delta E[q](x) = E_{\\text{loc}}(x) = V(x) - \\frac{1}{4}\\nabla^2x \\log q(x) - \\frac{1}{8}\\|\\nabla x \\log q(x)\\|^2. \\tag{73}\n$$\nUsing the results from Section 2.2, the energy-minimizing gradient flow under Fisher\u2013Rao metric is\n\n$$\n\\frac{\\partial q_t(x)}{\\partial t} = -E_{\\text{loc}}(x) - E[qt] \\, qt(x). \\tag{74}\n$$\nSecond, we derive the PDE for the time-evolution of the density qt under the imaginary-time Schr\u00f6dinger equation.\n\n$$\n\\begin{align*}\n\\frac{\\partial \\psi_t}{\\partial t} &= \\frac{1}{2}\\nabla^2x \\psi_t - (V - E[qt])\\psi_t \\tag{75} \\\\\n2\\psi_t \\frac{\\partial t}{\\partial t} &= \\psi_t \\nabla^2x \\psi_t - 2(V - E[qt])\\psi_t^2 \\tag{76} \\\\\n\\frac{\\partial qt}{\\partial t} &= \\psi_t \\nabla^2x \\psi_t - 2(V - E[qt])qt. \\tag{77}\n\\end{align*}\n$$\n$$\n\\begin{aligned}\n\\text{Using the identity} \\\\\n\\psi \\nabla^2x \\psi &= \\langle \\psi \\nabla x \\psi, \\nabla x \\log |\\psi|\\rangle + \\psi^2\\nabla^2x \\log |\\psi| \\tag{79} \\\\\n&= \\frac{1}{4}\\langle \\nabla x q, \\nabla x \\log q\\rangle + \\frac{1}{2}q\\nabla^2x \\log q = \\frac{1}{4}q\\|\\nabla x \\log q\\|^2 + \\frac{1}{2}q\\nabla^2x \\log q, \\tag{80}\n\\end{aligned}\n$$\n$$\n\\frac{\\partial qt}{\\partial t} = -\\frac{1}{2}E_{\\text{loc}}(x) - E[qt] \\, qt(x), \\tag{82}\n$$\nwhich is equivalent to Eq. (74).\n\nFollowing the gradient flow by a parametric model\n\nProposition. For qt(x) = q(x, \u03b8), the KL-divergence can be expanded as\n\n$$\nDKL(q_{t+\\Delta t}(x) \\| q(x, \\theta + \\Delta \\theta)) = -\\frac{1}{2}\\Delta \\theta, \\frac{\\partial}{\\partial t} + o(2\\Delta t \\Delta t^2 + \\|\\Delta \\theta\\|^2 \\frac{\\partial t}{\\partial t}q_t(x)\\nabla \\theta \\log q(x, \\theta) dx. \\tag{83}\n$$\n$$\nDKL(q_t(x) \\| q(x, \\theta)) = \\int q_t(x) \\log q_t(x) dx \\tag{84}\n$$\n$$\n\\Rightarrow \\frac{\\partial}{\\partial \\theta}DKL(q_t(x) \\| q(x, \\theta)) = -\\int \\frac{\\partial}{\\partial \\theta}q_t(x) \\log q(x, \\theta) dx = 0 \\text{ if } q_t(x) = q(x, \\theta) \\tag{86}\n$$"}]}, {"page": 18, "text": "         =\u21d2       \u2202                                       \u2202       qt(x) log qt(x) dx \u2212                \u2202                             (87)\n                  \u2202tDKL(qt(x)\u2225q(x, \u03b8)) =                 \u2202t                                          \u2202tqt(x) log q(x, \u03b8) dx\n                                                    =          \u2202                                     qt(x) \u2202                        (88)\n                                                              \u2202tqt(x) log qt(x) dx +                         \u2202t log qt(x) dx\n                                                          \u2212        \u2202                                                                (89)\n                                                    =          \u2202  \u2202tqt(x) log q(x, \u03b8) dx              \u2202                             (90)\n                                                              \u2202tqt(x) log qt(x) dx +                 \u2202tqt(x) dx\n                                                          \u2212        \u2202                                                                (91)\n                                                                  \u2202tqt(x) log q(x, \u03b8) dx\n                                                    =          \u2202            1 + log qt(x)           dx                              (92)\n                                                              \u2202tqt(x)                  q(x, \u03b8)\n                                                    =     0     \u2202           1 + log qt(x)                 qt(x) = q(x, \u03b8)           (93)\n                                                               \u2202tqt(x)                  q(x,\u03b8)    dx      qt(x) \u0338= q(x, \u03b8)\n     =\u21d2        \u22022                                                  \u2202                                                                (94)\n             \u2202\u03b8 \u2202tDKL(qt(x)\u2225q(x, \u03b8)) = \u2212                          \u2202tqt(x)\u2207       log q(x, \u03b8) dx\nThus at qt(x) = q(x, \u03b8) we have\n        =\u21d2      DKL(qt+\u2206t(x)\u2225q(x, \u03b8 + \u2206\u03b8)) \u2248                     \u22121  2\u2206t      \u2206\u03b8,        \u2202                                      .   (95)\n                                                                                         \u2202tqt(x)\u2207\u03b8 log q(x, \u03b8) dx\nD      Analogies to generative modeling literature\nTo draw a connection to generative models literature, let us consider the KL-divergence as an objective\nto minimize (instead of the energy), i.e.\n                                      F  [q] = DKL(p(x)\u2225q(x)),                  \u03b4F\u03b4q[q]  = \u2212p(x) q(x),                              (96)\nwhere p(x) is the data distribution given empirically. Thus, we have two PDEs that define gradient\nflows of this functional. Using equations Eqs. (9) and (10), we have\n  \u2202qt                p(x)                    p(y)        qt(x) = p(x) \u2212         qt(x),          Fisher\u2013Rao Gradient Flow, (97)\n   \u2202t (x) =         qt(x) \u2212      Eqt(y)     qt(y)\n  \u2202qt                         qt(x)\u2207x       p(x)      ,                                      2-Wasserstein Gradient Flow. (98)\n   \u2202t (x) = \u2212\u2207x \u00b7                          qt(x)\nHaving the functional minimizing PDEs, we can define the corresponding loss functions using\nProposition 3.2 and Eq. (17). For the Fisher\u2013Rao gradient flow, we have\n                                \u2206\u03b8\u2217  FR = \u2212Ep(x)\u2207\u03b8 log q(x, \u03b8) + Eqt(x)\u2207\u03b8 log q(x, \u03b8).                                              (99)\nRemember that in Proposition 3.2 we use qt(x) = q(x, \u03b8) as the density equal to the model density\nbut detached from the parameters \u03b8. Hence, we have\n                                                 \u2206\u03b8\u2217  FR = \u2212\u2207\u03b8Ep(x) log q(x, \u03b8),                                                   (100)\nwhich corresponds to the conventional energy-based models training (Xie et al., 2016; Du & Mordatch,\n2019).\nFor the 2-Wasserstein gradient flow, denoting the detached density as qt(x) = q(x, \u03b8), we have\n \u2206\u03b8\u2217                          dx qt(x)       \u2207x    p(x)                                                                            (101)\n      W2 = \u2212        \u2207\u03b8                            qt(x), \u2207x log q(x, \u03b8)\n           = \u2212      \u2207\u03b8        dx \u27e8\u2207xp(x), \u2207x log q(x, \u03b8)\u27e9                + \u2207\u03b8         dx p(x)\u27e8\u2207x log qt(x), \u2207x log q(x, \u03b8)\u27e9        (102)\n                     1\n           = \u2207\u03b8      2Ep(x)\u2225\u2207x log p(x) \u2212              \u2207x log q(x, \u03b8)\u22252,                                                           (103)\n                                                                      18", "md": "# Math Equations\n\n$$\\frac{\\partial}{\\partial t} D_{KL}(q_t(x) \\parallel q(x, \\theta)) = \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q_t(x) dx - \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q(x, \\theta) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q_t(x) dx + \\frac{\\partial}{\\partial t} \\log q_t(x) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int \\frac{\\partial}{\\partial t} q_t(x) \\log q(x, \\theta) dx - \\frac{\\partial}{\\partial t} q_t(x) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int \\frac{1 + \\log q_t(x)}{q(x, \\theta)} dx$$\n\n$$= 0 \\quad \\frac{\\partial}{\\partial t} \\int \\frac{1 + \\log q_t(x)}{q_t(x)} dx = q(x, \\theta) \\quad (q_t(x) \\neq q(x, \\theta))$$\n\n$$\\frac{\\partial^2}{\\partial \\theta \\partial t} D_{KL}(q_t(x) \\parallel q(x, \\theta)) = -\\frac{\\partial}{\\partial t} q_t(x) \\nabla \\cdot \\log q(x, \\theta) dx$$\n\nThus at \\(q_t(x) = q(x, \\theta)\\) we have\n\n$$D_{KL}(q_{t+\\Delta t}(x) \\parallel q(x, \\theta + \\Delta \\theta)) \\approx -\\frac{1}{2\\Delta t} \\Delta \\theta \\frac{\\partial}{\\partial t} q_t(x) \\nabla \\theta \\log q(x, \\theta) dx$$\n\nAnalogies to generative modeling literature\n\nTo draw a connection to generative models literature, let us consider the KL-divergence as an objective to minimize (instead of the energy), i.e.\n\n$$F[q] = D_{KL}(p(x) \\parallel q(x)), \\quad \\frac{\\delta F}{\\delta q}[q] = -p(x) q(x)$$\n\nwhere \\(p(x)\\) is the data distribution given empirically. Thus, we have two PDEs that define gradient flows of this functional. Using equations Eqs. (9) and (10), we have\n\n$$\\frac{\\partial q_t(x)}{\\partial t} = p(x) - p(y) \\quad q_t(x) = p(x) - q_t(x), \\quad \\text{Fisher\u2013Rao Gradient Flow}$$\n\n$$\\frac{\\partial q_t}{\\partial t} = -\\nabla \\cdot (q_t(x) \\nabla x p(x)), \\quad \\text{2-Wasserstein Gradient Flow}$$\n\nHaving the functional minimizing PDEs, we can define the corresponding loss functions using Proposition 3.2 and Eq. (17). For the Fisher\u2013Rao gradient flow, we have\n\n$$\\Delta \\theta^*_{FR} = -E p(x) \\nabla \\theta \\log q(x, \\theta) + E q_t(x) \\nabla \\theta \\log q(x, \\theta)$$\n\nRemember that in Proposition 3.2 we use \\(q_t(x) = q(x, \\theta)\\) as the density equal to the model density but detached from the parameters \\(\\theta\\). Hence, we have\n\n$$\\Delta \\theta^*_{FR} = -\\nabla \\theta E p(x) \\log q(x, \\theta)$$\n\nwhich corresponds to the conventional energy-based models training (Xie et al., 2016; Du & Mordatch, 2019).\n\nFor the 2-Wasserstein gradient flow, denoting the detached density as \\(q_t(x) = q(x, \\theta)\\), we have\n\n$$\\Delta \\theta^*_{W2} = -\\nabla \\theta \\int dx q_t(x) \\nabla x p(x) \\nabla x \\log q(x, \\theta)$$\n\n$$= -\\nabla \\theta \\int dx \\langle \\nabla x p(x), \\nabla x \\log q(x, \\theta) \\rangle + \\nabla \\theta \\int dx p(x) \\langle \\nabla x \\log q_t(x), \\nabla x \\log q(x, \\theta) \\rangle$$\n\n$$= \\nabla \\theta 2 E p(x) \\left\\| \\nabla x \\log p(x) - \\nabla x \\log q(x, \\theta) \\right\\|^2$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "$$\\frac{\\partial}{\\partial t} D_{KL}(q_t(x) \\parallel q(x, \\theta)) = \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q_t(x) dx - \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q(x, \\theta) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q_t(x) dx + \\frac{\\partial}{\\partial t} \\log q_t(x) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int \\frac{\\partial}{\\partial t} q_t(x) \\log q(x, \\theta) dx - \\frac{\\partial}{\\partial t} q_t(x) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int \\frac{1 + \\log q_t(x)}{q(x, \\theta)} dx$$\n\n$$= 0 \\quad \\frac{\\partial}{\\partial t} \\int \\frac{1 + \\log q_t(x)}{q_t(x)} dx = q(x, \\theta) \\quad (q_t(x) \\neq q(x, \\theta))$$\n\n$$\\frac{\\partial^2}{\\partial \\theta \\partial t} D_{KL}(q_t(x) \\parallel q(x, \\theta)) = -\\frac{\\partial}{\\partial t} q_t(x) \\nabla \\cdot \\log q(x, \\theta) dx$$\n\nThus at \\(q_t(x) = q(x, \\theta)\\) we have\n\n$$D_{KL}(q_{t+\\Delta t}(x) \\parallel q(x, \\theta + \\Delta \\theta)) \\approx -\\frac{1}{2\\Delta t} \\Delta \\theta \\frac{\\partial}{\\partial t} q_t(x) \\nabla \\theta \\log q(x, \\theta) dx$$\n\nAnalogies to generative modeling literature\n\nTo draw a connection to generative models literature, let us consider the KL-divergence as an objective to minimize (instead of the energy), i.e.\n\n$$F[q] = D_{KL}(p(x) \\parallel q(x)), \\quad \\frac{\\delta F}{\\delta q}[q] = -p(x) q(x)$$\n\nwhere \\(p(x)\\) is the data distribution given empirically. Thus, we have two PDEs that define gradient flows of this functional. Using equations Eqs. (9) and (10), we have\n\n$$\\frac{\\partial q_t(x)}{\\partial t} = p(x) - p(y) \\quad q_t(x) = p(x) - q_t(x), \\quad \\text{Fisher\u2013Rao Gradient Flow}$$\n\n$$\\frac{\\partial q_t}{\\partial t} = -\\nabla \\cdot (q_t(x) \\nabla x p(x)), \\quad \\text{2-Wasserstein Gradient Flow}$$\n\nHaving the functional minimizing PDEs, we can define the corresponding loss functions using Proposition 3.2 and Eq. (17). For the Fisher\u2013Rao gradient flow, we have\n\n$$\\Delta \\theta^*_{FR} = -E p(x) \\nabla \\theta \\log q(x, \\theta) + E q_t(x) \\nabla \\theta \\log q(x, \\theta)$$\n\nRemember that in Proposition 3.2 we use \\(q_t(x) = q(x, \\theta)\\) as the density equal to the model density but detached from the parameters \\(\\theta\\). Hence, we have\n\n$$\\Delta \\theta^*_{FR} = -\\nabla \\theta E p(x) \\log q(x, \\theta)$$\n\nwhich corresponds to the conventional energy-based models training (Xie et al., 2016; Du & Mordatch, 2019).\n\nFor the 2-Wasserstein gradient flow, denoting the detached density as \\(q_t(x) = q(x, \\theta)\\), we have\n\n$$\\Delta \\theta^*_{W2} = -\\nabla \\theta \\int dx q_t(x) \\nabla x p(x) \\nabla x \\log q(x, \\theta)$$\n\n$$= -\\nabla \\theta \\int dx \\langle \\nabla x p(x), \\nabla x \\log q(x, \\theta) \\rangle + \\nabla \\theta \\int dx p(x) \\langle \\nabla x \\log q_t(x), \\nabla x \\log q(x, \\theta) \\rangle$$\n\n$$= \\nabla \\theta 2 E p(x) \\left\\| \\nabla x \\log p(x) - \\nabla x \\log q(x, \\theta) \\right\\|^2$$", "md": "$$\\frac{\\partial}{\\partial t} D_{KL}(q_t(x) \\parallel q(x, \\theta)) = \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q_t(x) dx - \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q(x, \\theta) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int q_t(x) \\log q_t(x) dx + \\frac{\\partial}{\\partial t} \\log q_t(x) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int \\frac{\\partial}{\\partial t} q_t(x) \\log q(x, \\theta) dx - \\frac{\\partial}{\\partial t} q_t(x) dx$$\n\n$$= \\frac{\\partial}{\\partial t} \\int \\frac{1 + \\log q_t(x)}{q(x, \\theta)} dx$$\n\n$$= 0 \\quad \\frac{\\partial}{\\partial t} \\int \\frac{1 + \\log q_t(x)}{q_t(x)} dx = q(x, \\theta) \\quad (q_t(x) \\neq q(x, \\theta))$$\n\n$$\\frac{\\partial^2}{\\partial \\theta \\partial t} D_{KL}(q_t(x) \\parallel q(x, \\theta)) = -\\frac{\\partial}{\\partial t} q_t(x) \\nabla \\cdot \\log q(x, \\theta) dx$$\n\nThus at \\(q_t(x) = q(x, \\theta)\\) we have\n\n$$D_{KL}(q_{t+\\Delta t}(x) \\parallel q(x, \\theta + \\Delta \\theta)) \\approx -\\frac{1}{2\\Delta t} \\Delta \\theta \\frac{\\partial}{\\partial t} q_t(x) \\nabla \\theta \\log q(x, \\theta) dx$$\n\nAnalogies to generative modeling literature\n\nTo draw a connection to generative models literature, let us consider the KL-divergence as an objective to minimize (instead of the energy), i.e.\n\n$$F[q] = D_{KL}(p(x) \\parallel q(x)), \\quad \\frac{\\delta F}{\\delta q}[q] = -p(x) q(x)$$\n\nwhere \\(p(x)\\) is the data distribution given empirically. Thus, we have two PDEs that define gradient flows of this functional. Using equations Eqs. (9) and (10), we have\n\n$$\\frac{\\partial q_t(x)}{\\partial t} = p(x) - p(y) \\quad q_t(x) = p(x) - q_t(x), \\quad \\text{Fisher\u2013Rao Gradient Flow}$$\n\n$$\\frac{\\partial q_t}{\\partial t} = -\\nabla \\cdot (q_t(x) \\nabla x p(x)), \\quad \\text{2-Wasserstein Gradient Flow}$$\n\nHaving the functional minimizing PDEs, we can define the corresponding loss functions using Proposition 3.2 and Eq. (17). For the Fisher\u2013Rao gradient flow, we have\n\n$$\\Delta \\theta^*_{FR} = -E p(x) \\nabla \\theta \\log q(x, \\theta) + E q_t(x) \\nabla \\theta \\log q(x, \\theta)$$\n\nRemember that in Proposition 3.2 we use \\(q_t(x) = q(x, \\theta)\\) as the density equal to the model density but detached from the parameters \\(\\theta\\). Hence, we have\n\n$$\\Delta \\theta^*_{FR} = -\\nabla \\theta E p(x) \\log q(x, \\theta)$$\n\nwhich corresponds to the conventional energy-based models training (Xie et al., 2016; Du & Mordatch, 2019).\n\nFor the 2-Wasserstein gradient flow, denoting the detached density as \\(q_t(x) = q(x, \\theta)\\), we have\n\n$$\\Delta \\theta^*_{W2} = -\\nabla \\theta \\int dx q_t(x) \\nabla x p(x) \\nabla x \\log q(x, \\theta)$$\n\n$$= -\\nabla \\theta \\int dx \\langle \\nabla x p(x), \\nabla x \\log q(x, \\theta) \\rangle + \\nabla \\theta \\int dx p(x) \\langle \\nabla x \\log q_t(x), \\nabla x \\log q(x, \\theta) \\rangle$$\n\n$$= \\nabla \\theta 2 E p(x) \\left\\| \\nabla x \\log p(x) - \\nabla x \\log q(x, \\theta) \\right\\|^2$$"}]}, {"page": 19, "text": "which corresponds to the score-matching objective (Hyv\u00e4rinen & Dayan, 2005). This objective is\nactively used in the diffusion-based generative models (Song et al., 2020).\nE      c-Wasserstein gradient flow\nc-Wasserstein distance with the convex cost function c : Rd \u2192                              R is defined\n                                  W 2 c (p0, p1) =           inf           \u03c0(x, y)c(x \u2212        y) dx dy,                                (104)\n                                                        \u03c0\u2208\u0393(p0,p1)\nwhere \u0393(p0, p1) is the set of all possible couplings of p0 and p1. The dynamic formulation of this\ndistance is the following\n                        W 2c (p0, p1) := inf  vt,qt    0 1  Eq t(x)[c(vt(x))] dt,          subj. to                                     (105)\n                              \u2202qt(x)\n                                  \u2202t      = \u2212\u2207x \u00b7 (qt(x)vt(x)) ,                 and       q0 = p0, q1 = p1 .                           (106)\nProposition. The energy-minimizing c-Wasserstein gradient flow is defined by the following PDE\n                                      \u2202qt(x)      = \u2212\u2207x \u00b7 (qt(x)\u2207c\u2217(\u22122\u2207xEloc(x))) ,                                                     (107)\n                                         \u2202t\nwhere c\u2217(\u00b7) is the convex conjugate function of c(\u00b7).\nProof. The movement minimizing scheme for Wc(\u00b7, \u00b7) is the following optimization problem\n                                              inf                            1       c (q, q\u2032).                                         (108)\n                                               q\u2032 F   [q\u2032] \u2212   F  [q] +    2\u2206tW 2\nAssuming that the density changes according to the continuity equation q\u2032 = q \u2212                                     \u2206t\u27e8\u2207x, q(x)v(x)\u27e9,\nand \u2206t is small enough so that v(x) defines the optimal transportation plan, we have\n          inf                       \u2207x \u00b7 (q(x)v(x))\u03b4F           [q](x) dx \u2212       F  [q] +      1                                       (109)\n            v F   [q] \u2212   \u2206t                                  \u03b4q                              2\u2206t\u2206t2Eq(x)c(v(x))\n            = \u2206t inf  v       q(x)     v(x), \u2207x     \u03b4F \u03b4q[q] (x)     dx + 1   2Eq(x)c(v(x))                                             (110)\n            = 1 2\u2206t inf v       q(x)     c(v(x)) \u2212         v(x), \u22122\u2207x        \u03b4F \u03b4q[q] (x)       dx                                      (111)\n            = \u2212     1           q(x)c\u2217      \u22122\u2207x      \u03b4F   [q] (x)     dx                                                               (112)\n                    2\u2206t                                 \u03b4q\nand the infimum is achieved at                  v(x) = \u2207c\u2217          \u22122\u2207x      \u03b4F\u03b4q[q]  (x)    ,                                         (113)\nwhich gives the formula for the vector field. Using the energy gradient from Theorem 3.1 \u03b4E[q]                                   \u03b4q (x) =\nEloc, we get the result.\nProposition. Coordinate-wise application of tanh to the vector field, i.e.\n                                      \u2202qt(x)\n                                          \u2202t      = \u2212\u2207x \u00b7 (qt(x) tanh(\u2212\u2207xEloc(x))) ,                                                    (114)\ncorresponds to gradient descent with c-Wasserstein distance, where c : Rd \u2192                                    R is the following cost\nfunction\n                   c(x) =       d    1    (xi + 1) log(xi + 1) + (1 \u2212               xi) log(1 \u2212      xi)     \u2212   d log 2.               (115)\n                                 i   2\n                                                                      19", "md": "which corresponds to the score-matching objective (Hyv\u00e4rinen & Dayan, 2005). This objective is actively used in the diffusion-based generative models (Song et al., 2020).\n\n$$\n\\text{c-Wasserstein gradient flow}\n$$\n\nc-Wasserstein distance with the convex cost function \\(c : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) is defined as:\n\n$$\nW_{2}^{c}(p_{0}, p_{1}) = \\inf_{\\pi \\in \\Gamma(p_{0},p_{1})} \\int \\pi(x, y)c(x - y) \\, dx \\, dy \\quad (104)\n$$\n\nwhere \\(\\Gamma(p_{0}, p_{1})\\) is the set of all possible couplings of \\(p_{0}\\) and \\(p_{1}\\). The dynamic formulation of this distance is as follows:\n\n$$\nW_{2}^{c}(p_{0}, p_{1}) := \\inf_{v_{t}, q_{t}} \\int_{0}^{1} \\mathbb{E}_{q_{t}}[c(v_{t}(x))] \\, dt, \\quad \\text{subj. to} \\quad \\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x)v_{t}(x)), \\quad q_{0} = p_{0}, q_{1} = p_{1} \\quad (105), (106)\n$$\n\n**Proposition.** The energy-minimizing c-Wasserstein gradient flow is defined by the following PDE:\n\n$$\n\\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x)\\nabla c^{*}(-2\\nabla E_{\\text{loc}}(x))) \\quad (107)\n$$\n\nwhere \\(c^{*}(\\cdot)\\) is the convex conjugate function of \\(c(\\cdot)\\).\n\n**Proof.** The movement minimizing scheme for \\(W_{c}(\\cdot, \\cdot)\\) is the following optimization problem:\n\n$$\n\\inf_{q'} [F[q'] - F[q] + 2\\Delta t W_{2}^{c}(q, q')]\n$$\n\nAssuming that the density changes according to the continuity equation \\(q' = q - \\Delta t \\langle \\nabla x, q(x)v(x) \\rangle\\), and \\(\\Delta t\\) is small enough so that \\(v(x)\\) defines the optimal transportation plan, we have:\n\n\\[\n\\begin{aligned}\n&\\inf_{v} \\left( \\nabla_{x} \\cdot (q(x)v(x))\\delta F[q](x) \\, dx - F[q] + \\frac{1}{2\\Delta t} \\delta q \\left(2\\Delta t^{2} \\mathbb{E}_{q(x)}c(v(x))\\right) \\right) \\\\\n&= \\Delta t \\inf_{v} \\left( v \\cdot q(x) \\cdot v(x), \\nabla_{x} \\delta F/\\delta q[q](x) \\, dx + \\frac{1}{2} \\mathbb{E}_{q(x)}c(v(x)) \\right) \\\\\n&= \\frac{1}{2\\Delta t} \\inf_{v} \\left( v \\cdot q(x) \\cdot c(v(x)) - v(x), -2\\nabla_{x} \\delta F/\\delta q[q](x) \\, dx \\right) \\\\\n&= -\\frac{1}{2\\Delta t} \\int q(x)c^{*}(-2\\nabla_{x} \\delta F[q](x)) \\, dx\n\\end{aligned}\n\\]\n\nand the infimum is achieved at \\(v(x) = \\nabla c^{*}(-2\\nabla_{x} \\delta F/\\delta q[q](x))\\), which gives the formula for the vector field. Using the energy gradient from Theorem 3.1 \\(\\delta E[q]/\\delta q(x) = E_{\\text{loc}}\\), we get the result.\n\n**Proposition.** Coordinate-wise application of \\(\\tanh\\) to the vector field, i.e.\n\n$$\n\\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x) \\tanh(-\\nabla E_{\\text{loc}}(x))) \\quad (114)\n$$\n\ncorresponds to gradient descent with c-Wasserstein distance, where \\(c : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) is the following cost function:\n\n$$\nc(x) = \\sum_{i=1}^{d} \\left(\\frac{1}{2}(x_{i} + 1) \\log(x_{i} + 1) + (1 - x_{i}) \\log(1 - x_{i})\\right) - d \\log 2 \\quad (115)\n$$", "images": [], "items": [{"type": "text", "value": "which corresponds to the score-matching objective (Hyv\u00e4rinen & Dayan, 2005). This objective is actively used in the diffusion-based generative models (Song et al., 2020).\n\n$$\n\\text{c-Wasserstein gradient flow}\n$$\n\nc-Wasserstein distance with the convex cost function \\(c : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) is defined as:\n\n$$\nW_{2}^{c}(p_{0}, p_{1}) = \\inf_{\\pi \\in \\Gamma(p_{0},p_{1})} \\int \\pi(x, y)c(x - y) \\, dx \\, dy \\quad (104)\n$$\n\nwhere \\(\\Gamma(p_{0}, p_{1})\\) is the set of all possible couplings of \\(p_{0}\\) and \\(p_{1}\\). The dynamic formulation of this distance is as follows:\n\n$$\nW_{2}^{c}(p_{0}, p_{1}) := \\inf_{v_{t}, q_{t}} \\int_{0}^{1} \\mathbb{E}_{q_{t}}[c(v_{t}(x))] \\, dt, \\quad \\text{subj. to} \\quad \\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x)v_{t}(x)), \\quad q_{0} = p_{0}, q_{1} = p_{1} \\quad (105), (106)\n$$\n\n**Proposition.** The energy-minimizing c-Wasserstein gradient flow is defined by the following PDE:\n\n$$\n\\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x)\\nabla c^{*}(-2\\nabla E_{\\text{loc}}(x))) \\quad (107)\n$$\n\nwhere \\(c^{*}(\\cdot)\\) is the convex conjugate function of \\(c(\\cdot)\\).\n\n**Proof.** The movement minimizing scheme for \\(W_{c}(\\cdot, \\cdot)\\) is the following optimization problem:\n\n$$\n\\inf_{q'} [F[q'] - F[q] + 2\\Delta t W_{2}^{c}(q, q')]\n$$\n\nAssuming that the density changes according to the continuity equation \\(q' = q - \\Delta t \\langle \\nabla x, q(x)v(x) \\rangle\\), and \\(\\Delta t\\) is small enough so that \\(v(x)\\) defines the optimal transportation plan, we have:\n\n\\[\n\\begin{aligned}\n&\\inf_{v} \\left( \\nabla_{x} \\cdot (q(x)v(x))\\delta F[q](x) \\, dx - F[q] + \\frac{1}{2\\Delta t} \\delta q \\left(2\\Delta t^{2} \\mathbb{E}_{q(x)}c(v(x))\\right) \\right) \\\\\n&= \\Delta t \\inf_{v} \\left( v \\cdot q(x) \\cdot v(x), \\nabla_{x} \\delta F/\\delta q[q](x) \\, dx + \\frac{1}{2} \\mathbb{E}_{q(x)}c(v(x)) \\right) \\\\\n&= \\frac{1}{2\\Delta t} \\inf_{v} \\left( v \\cdot q(x) \\cdot c(v(x)) - v(x), -2\\nabla_{x} \\delta F/\\delta q[q](x) \\, dx \\right) \\\\\n&= -\\frac{1}{2\\Delta t} \\int q(x)c^{*}(-2\\nabla_{x} \\delta F[q](x)) \\, dx\n\\end{aligned}\n\\]\n\nand the infimum is achieved at \\(v(x) = \\nabla c^{*}(-2\\nabla_{x} \\delta F/\\delta q[q](x))\\), which gives the formula for the vector field. Using the energy gradient from Theorem 3.1 \\(\\delta E[q]/\\delta q(x) = E_{\\text{loc}}\\), we get the result.\n\n**Proposition.** Coordinate-wise application of \\(\\tanh\\) to the vector field, i.e.\n\n$$\n\\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x) \\tanh(-\\nabla E_{\\text{loc}}(x))) \\quad (114)\n$$\n\ncorresponds to gradient descent with c-Wasserstein distance, where \\(c : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) is the following cost function:\n\n$$\nc(x) = \\sum_{i=1}^{d} \\left(\\frac{1}{2}(x_{i} + 1) \\log(x_{i} + 1) + (1 - x_{i}) \\log(1 - x_{i})\\right) - d \\log 2 \\quad (115)\n$$", "md": "which corresponds to the score-matching objective (Hyv\u00e4rinen & Dayan, 2005). This objective is actively used in the diffusion-based generative models (Song et al., 2020).\n\n$$\n\\text{c-Wasserstein gradient flow}\n$$\n\nc-Wasserstein distance with the convex cost function \\(c : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) is defined as:\n\n$$\nW_{2}^{c}(p_{0}, p_{1}) = \\inf_{\\pi \\in \\Gamma(p_{0},p_{1})} \\int \\pi(x, y)c(x - y) \\, dx \\, dy \\quad (104)\n$$\n\nwhere \\(\\Gamma(p_{0}, p_{1})\\) is the set of all possible couplings of \\(p_{0}\\) and \\(p_{1}\\). The dynamic formulation of this distance is as follows:\n\n$$\nW_{2}^{c}(p_{0}, p_{1}) := \\inf_{v_{t}, q_{t}} \\int_{0}^{1} \\mathbb{E}_{q_{t}}[c(v_{t}(x))] \\, dt, \\quad \\text{subj. to} \\quad \\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x)v_{t}(x)), \\quad q_{0} = p_{0}, q_{1} = p_{1} \\quad (105), (106)\n$$\n\n**Proposition.** The energy-minimizing c-Wasserstein gradient flow is defined by the following PDE:\n\n$$\n\\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x)\\nabla c^{*}(-2\\nabla E_{\\text{loc}}(x))) \\quad (107)\n$$\n\nwhere \\(c^{*}(\\cdot)\\) is the convex conjugate function of \\(c(\\cdot)\\).\n\n**Proof.** The movement minimizing scheme for \\(W_{c}(\\cdot, \\cdot)\\) is the following optimization problem:\n\n$$\n\\inf_{q'} [F[q'] - F[q] + 2\\Delta t W_{2}^{c}(q, q')]\n$$\n\nAssuming that the density changes according to the continuity equation \\(q' = q - \\Delta t \\langle \\nabla x, q(x)v(x) \\rangle\\), and \\(\\Delta t\\) is small enough so that \\(v(x)\\) defines the optimal transportation plan, we have:\n\n\\[\n\\begin{aligned}\n&\\inf_{v} \\left( \\nabla_{x} \\cdot (q(x)v(x))\\delta F[q](x) \\, dx - F[q] + \\frac{1}{2\\Delta t} \\delta q \\left(2\\Delta t^{2} \\mathbb{E}_{q(x)}c(v(x))\\right) \\right) \\\\\n&= \\Delta t \\inf_{v} \\left( v \\cdot q(x) \\cdot v(x), \\nabla_{x} \\delta F/\\delta q[q](x) \\, dx + \\frac{1}{2} \\mathbb{E}_{q(x)}c(v(x)) \\right) \\\\\n&= \\frac{1}{2\\Delta t} \\inf_{v} \\left( v \\cdot q(x) \\cdot c(v(x)) - v(x), -2\\nabla_{x} \\delta F/\\delta q[q](x) \\, dx \\right) \\\\\n&= -\\frac{1}{2\\Delta t} \\int q(x)c^{*}(-2\\nabla_{x} \\delta F[q](x)) \\, dx\n\\end{aligned}\n\\]\n\nand the infimum is achieved at \\(v(x) = \\nabla c^{*}(-2\\nabla_{x} \\delta F/\\delta q[q](x))\\), which gives the formula for the vector field. Using the energy gradient from Theorem 3.1 \\(\\delta E[q]/\\delta q(x) = E_{\\text{loc}}\\), we get the result.\n\n**Proposition.** Coordinate-wise application of \\(\\tanh\\) to the vector field, i.e.\n\n$$\n\\frac{\\partial q_{t}(x)}{\\partial t} = -\\nabla_{x} \\cdot (q_{t}(x) \\tanh(-\\nabla E_{\\text{loc}}(x))) \\quad (114)\n$$\n\ncorresponds to gradient descent with c-Wasserstein distance, where \\(c : \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) is the following cost function:\n\n$$\nc(x) = \\sum_{i=1}^{d} \\left(\\frac{1}{2}(x_{i} + 1) \\log(x_{i} + 1) + (1 - x_{i}) \\log(1 - x_{i})\\right) - d \\log 2 \\quad (115)\n$$"}]}, {"page": 20, "text": "Proof. Consider c\u2217(x) =  i log(exp(xi) + exp(\u2212xi)). It corresponds to applying hyperbolic\ntangent non-linearity coordinate-wise to the vector field field, i.e.,\n                                      \u2202ic\u2217(x) = exp(xi) \u2212            exp(\u2212xi)                                                (116)\nThe corresponding cost function c(x) is the following exp(xi) + exp(\u2212xi) = tanh(xi).\n               c(x) = sup    y \u27e8x, y\u27e9     \u2212  c\u2217(y)                                                                           (117)\n                      = sup  y     d      xiyi \u2212    log(exp(yi) + exp(\u2212yi))                                                  (118)\n                                   i\n                      = sup  y     d      (xi + 1)yi \u2212       log(exp(2yi) + 1)                   //yi = 1   2 log 1 +1x\u2212  x  (119)\n                                   i\n                      =      d      (xi + 1)1   2 log    1 + xi         \u2212  log   1 + xi      + 1                             (120)\n                             i                             1 \u2212   xi                1 \u2212   xi\n                      =      d    1   (xi + 1) log(xi + 1) + (1 \u2212              xi) log(1 \u2212       xi)     \u2212  d log 2.         (121)\n                             i    2\n                                                                    20", "md": "Proof. Consider \\(c^*(x) = i \\log(\\exp(xi) + \\exp(-xi))\\). It corresponds to applying hyperbolic tangent non-linearity coordinate-wise to the vector field field, i.e.,\n\n$$\n\\partial_i c^*(x) = \\exp(xi) - \\exp(-xi) \\quad (116)\n$$\n\nThe corresponding cost function \\(c(x)\\) is the following \\(\\exp(xi) + \\exp(-xi) = \\tanh(xi)\\).\n\n\\[\nc(x) = \\sup_y \\langle x, y \\rangle - c^*(y) \\quad (117)\n\\]\n\n\\[\n= \\sup_y \\sum_i d(x_i y_i) - \\log(\\exp(y_i) + \\exp(-y_i)) \\quad (118)\n\\]\n\n\\[\n= \\sup_y \\sum_i d((xi + 1)y_i) - \\log(\\exp(2y_i) + 1) \\quad \\text{//} y_i = \\frac{1}{2} \\log(1 + 1x_i - x_i) \\quad (119)\n\\]\n\n\\[\n= \\sum_i d((xi + 1)\\frac{1}{2} \\log(1 + xi) - \\log(1 + xi) + 1 \\frac{1 - xi}{1 - xi} \\quad (120)\n\\]\n\n\\[\n= \\sum_i d \\left( \\frac{1}{2}(xi + 1) \\log(xi + 1) + (1 - xi) \\log(1 - xi) \\right) - d \\log 2 \\quad (121)\n\\]", "images": [], "items": [{"type": "text", "value": "Proof. Consider \\(c^*(x) = i \\log(\\exp(xi) + \\exp(-xi))\\). It corresponds to applying hyperbolic tangent non-linearity coordinate-wise to the vector field field, i.e.,\n\n$$\n\\partial_i c^*(x) = \\exp(xi) - \\exp(-xi) \\quad (116)\n$$\n\nThe corresponding cost function \\(c(x)\\) is the following \\(\\exp(xi) + \\exp(-xi) = \\tanh(xi)\\).\n\n\\[\nc(x) = \\sup_y \\langle x, y \\rangle - c^*(y) \\quad (117)\n\\]\n\n\\[\n= \\sup_y \\sum_i d(x_i y_i) - \\log(\\exp(y_i) + \\exp(-y_i)) \\quad (118)\n\\]\n\n\\[\n= \\sup_y \\sum_i d((xi + 1)y_i) - \\log(\\exp(2y_i) + 1) \\quad \\text{//} y_i = \\frac{1}{2} \\log(1 + 1x_i - x_i) \\quad (119)\n\\]\n\n\\[\n= \\sum_i d((xi + 1)\\frac{1}{2} \\log(1 + xi) - \\log(1 + xi) + 1 \\frac{1 - xi}{1 - xi} \\quad (120)\n\\]\n\n\\[\n= \\sum_i d \\left( \\frac{1}{2}(xi + 1) \\log(xi + 1) + (1 - xi) \\log(1 - xi) \\right) - d \\log 2 \\quad (121)\n\\]", "md": "Proof. Consider \\(c^*(x) = i \\log(\\exp(xi) + \\exp(-xi))\\). It corresponds to applying hyperbolic tangent non-linearity coordinate-wise to the vector field field, i.e.,\n\n$$\n\\partial_i c^*(x) = \\exp(xi) - \\exp(-xi) \\quad (116)\n$$\n\nThe corresponding cost function \\(c(x)\\) is the following \\(\\exp(xi) + \\exp(-xi) = \\tanh(xi)\\).\n\n\\[\nc(x) = \\sup_y \\langle x, y \\rangle - c^*(y) \\quad (117)\n\\]\n\n\\[\n= \\sup_y \\sum_i d(x_i y_i) - \\log(\\exp(y_i) + \\exp(-y_i)) \\quad (118)\n\\]\n\n\\[\n= \\sup_y \\sum_i d((xi + 1)y_i) - \\log(\\exp(2y_i) + 1) \\quad \\text{//} y_i = \\frac{1}{2} \\log(1 + 1x_i - x_i) \\quad (119)\n\\]\n\n\\[\n= \\sum_i d((xi + 1)\\frac{1}{2} \\log(1 + xi) - \\log(1 + xi) + 1 \\frac{1 - xi}{1 - xi} \\quad (120)\n\\]\n\n\\[\n= \\sum_i d \\left( \\frac{1}{2}(xi + 1) \\log(xi + 1) + (1 - xi) \\log(1 - xi) \\right) - d \\log 2 \\quad (121)\n\\]"}]}, {"page": 21, "text": "F        Additional experimental results\n     1e-3                 Be (4)                     1e-3                  B (5)                     1e-3          Li2 (6)                                       H10 (10)\n                                QVMC                                           QVMC                                     QVMC                                            QVMC\n                                W(FR)QMC                                       W(FR)QMC                                 W(FR)QMC             1e-3                       W(FR)QMC\n    Relative energy error       WQMC                                           WQMC                  5e-4               WQMC                                            WQMC\n                                chemical accuracy                              chemical accuracy                        chemical accuracy                               chemical accuracy\n     1e-4                                            1e-4                                                                                    6e-4\n                                                                                                     1e-4                                    3e-4\n     1e-5                                            1e-5\n     1e-6                                            1e-6                                            2e-5                                    1e-4\n                                       QVMC          3e0                               QVMC          1e0                       QVMC          4e-2                              QVMC\n     5e-1                              W(FR)QMC                                        W(FR)QMC                                W(FR)QMC                                        W(FR)QMC\n    Energy variance (Ha2)              WQMC          1e0                               WQMC          1e-1                      WQMC          1e-2                              WQMC\n     1e-1\n                                                     1e-1                                            1e-2\n     1e-2                                                                                                                                    1e-3\n                                                                                                     1e-3\n     1e-3                                            1e-2                                                                                    2e-4\n                                       QVMC                                            QVMC          2e0                       QVMC          5e-1                              QVMC\n      2e0                              W(FR)QMC      5e0                               W(FR)QMC                                W(FR)QMC                                        W(FR)QMC\n                                       WQMC                                            WQMC                                    WQMC                                            WQMC\n    Gradient norm                                                                                    8e-1\n      1e0                                            2e0                                                                                     2e-1\n     3e-1                                                                                            3e-1\n                                                     5e-1                                                                                    8e-2\n     1e-1       1000    2000    3000    4000         2e-1      1000  2000   3000  4000   5000        1e-1  2000      4000      6000          4e-2      5000 1000015000200002500030000\n                    wall time (seconds)                            wall time (seconds)                      wall time (seconds)                             wall time (seconds)\nFigure 3: Optimization results for different chemical systems (every column corresponds to a given\nmolecule). The number of electrons is given in the brackets next to systems\u2019 names. Throughout\nthe optimization, we monitor three values: the mean value of the local energy (lower is better), the\nvariance of the local energy, and the median value of the gradient norm of the local energy. In the first\nrow of plots, we average (removing 5% of outliers from both sides) the energy over 1000 iterations\nand report the relative error to the actual ground-state energy: (E \u2212                                                         E0)/E0. In the second row, we\nreport standard deviation averaged over 1000 iterations (removing 5% of outliers from both sides). In\nthe third row, we report the median gradient norm averaged over 1000 iterations (removing 5% of\noutliers from both sides). All the metrics are plotted versus the wall time of computations performed\non four Nvidia A40 GPUs.", "md": "# Additional Experimental Results\n\n**Additional experimental results**\n\n| |Be (4)|B (5)|Li2 (6)|H10 (10)|\n|---|---|---|---|---|\n|QVMC|W(FR)QMC|W(FR)QMC|W(FR)QMC|W(FR)QMC|\n|WQMC|WQMC|WQMC|WQMC|WQMC|\n|chemical accuracy|chemical accuracy|chemical accuracy|chemical accuracy|chemical accuracy|\n\n$$\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Be (4) & B (5) & Li2 (6) & H10 (10) \\\\\n\\hline\nQVMC & W(FR)QMC & W(FR)QMC & W(FR)QMC & W(FR)QMC \\\\\nWQMC & WQMC & WQMC & WQMC & WQMC \\\\\n\\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} \\\\\n\\hline\n\\end{array}$$\n\nFigure 3: Optimization results for different chemical systems (every column corresponds to a given molecule). The number of electrons is given in the brackets next to systems\u2019 names. Throughout the optimization, we monitor three values: the mean value of the local energy (lower is better), the variance of the local energy, and the median value of the gradient norm of the local energy. In the first row of plots, we average (removing 5% of outliers from both sides) the energy over 1000 iterations and report the relative error to the actual ground-state energy: $$\\left(\\frac{E - E_0}{E_0}\\right)$$. In the second row, we report standard deviation averaged over 1000 iterations (removing 5% of outliers from both sides). In the third row, we report the median gradient norm averaged over 1000 iterations (removing 5% of outliers from both sides). All the metrics are plotted versus the wall time of computations performed on four Nvidia A40 GPUs.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Experimental Results", "md": "# Additional Experimental Results"}, {"type": "text", "value": "**Additional experimental results**", "md": "**Additional experimental results**"}, {"type": "table", "rows": [["", "Be (4)", "B (5)", "Li2 (6)", "H10 (10)"], ["QVMC", "W(FR)QMC", "W(FR)QMC", "W(FR)QMC", "W(FR)QMC"], ["WQMC", "WQMC", "WQMC", "WQMC", "WQMC"], ["chemical accuracy", "chemical accuracy", "chemical accuracy", "chemical accuracy", "chemical accuracy"]], "md": "| |Be (4)|B (5)|Li2 (6)|H10 (10)|\n|---|---|---|---|---|\n|QVMC|W(FR)QMC|W(FR)QMC|W(FR)QMC|W(FR)QMC|\n|WQMC|WQMC|WQMC|WQMC|WQMC|\n|chemical accuracy|chemical accuracy|chemical accuracy|chemical accuracy|chemical accuracy|", "isPerfectTable": true, "csv": "\"\",\"Be (4)\",\"B (5)\",\"Li2 (6)\",\"H10 (10)\"\n\"QVMC\",\"W(FR)QMC\",\"W(FR)QMC\",\"W(FR)QMC\",\"W(FR)QMC\"\n\"WQMC\",\"WQMC\",\"WQMC\",\"WQMC\",\"WQMC\"\n\"chemical accuracy\",\"chemical accuracy\",\"chemical accuracy\",\"chemical accuracy\",\"chemical accuracy\""}, {"type": "text", "value": "$$\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Be (4) & B (5) & Li2 (6) & H10 (10) \\\\\n\\hline\nQVMC & W(FR)QMC & W(FR)QMC & W(FR)QMC & W(FR)QMC \\\\\nWQMC & WQMC & WQMC & WQMC & WQMC \\\\\n\\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} \\\\\n\\hline\n\\end{array}$$\n\nFigure 3: Optimization results for different chemical systems (every column corresponds to a given molecule). The number of electrons is given in the brackets next to systems\u2019 names. Throughout the optimization, we monitor three values: the mean value of the local energy (lower is better), the variance of the local energy, and the median value of the gradient norm of the local energy. In the first row of plots, we average (removing 5% of outliers from both sides) the energy over 1000 iterations and report the relative error to the actual ground-state energy: $$\\left(\\frac{E - E_0}{E_0}\\right)$$. In the second row, we report standard deviation averaged over 1000 iterations (removing 5% of outliers from both sides). In the third row, we report the median gradient norm averaged over 1000 iterations (removing 5% of outliers from both sides). All the metrics are plotted versus the wall time of computations performed on four Nvidia A40 GPUs.", "md": "$$\\begin{array}{|c|c|c|c|c|}\n\\hline\n& Be (4) & B (5) & Li2 (6) & H10 (10) \\\\\n\\hline\nQVMC & W(FR)QMC & W(FR)QMC & W(FR)QMC & W(FR)QMC \\\\\nWQMC & WQMC & WQMC & WQMC & WQMC \\\\\n\\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} & \\text{chemical accuracy} \\\\\n\\hline\n\\end{array}$$\n\nFigure 3: Optimization results for different chemical systems (every column corresponds to a given molecule). The number of electrons is given in the brackets next to systems\u2019 names. Throughout the optimization, we monitor three values: the mean value of the local energy (lower is better), the variance of the local energy, and the median value of the gradient norm of the local energy. In the first row of plots, we average (removing 5% of outliers from both sides) the energy over 1000 iterations and report the relative error to the actual ground-state energy: $$\\left(\\frac{E - E_0}{E_0}\\right)$$. In the second row, we report standard deviation averaged over 1000 iterations (removing 5% of outliers from both sides). In the third row, we report the median gradient norm averaged over 1000 iterations (removing 5% of outliers from both sides). All the metrics are plotted versus the wall time of computations performed on four Nvidia A40 GPUs."}]}], "job_id": "419ee8a8-2195-4ebd-8f3a-c39cca47b730", "file_path": "./corpus/2307.07050.pdf"}