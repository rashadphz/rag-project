{"pages": [{"page": 1, "text": "                             Provably Efficient Offline Goal-Conditioned\n                          Reinforcement Learning with General Function\n                        Approximation and Single-Policy Concentrability\narXiv:2302.03770v2  [cs.LG]  11 Oct 2023Hanlin Zhu                                     Amy Zhang\n                               EECS, UC Berkeley; Meta AI                       ECE, UT Austin; Meta AI\n                                hanlinzhu@berkeley.edu                     amy.zhang@austin.utexas.edu\n                                                                 Abstract\n                             Goal-conditioned reinforcement learning (GCRL) refers to learning general-\n                             purpose skills that aim to reach diverse goals. In particular, offl ine GCRL only\n                             requires purely pre-collected datasets to perform training tasks without additional\n                             interactions with the environment. Although offl    ine GCRL has become increas-\n                             ingly prevalent and many previous works have demonstrated its empirical success,\n                             the theoretical understanding of efficient offline GCRL algorithms is not well es-\n                             tablished, especially when the state space is huge and the offl     ine dataset only\n                             covers the policy we aim to learn. In this paper, we provide a rigorous theoret-\n                             ical analysis of an existing empirically successful offl ine GCRL algorithm. We\n                             prove that under slight modifi cation, this algorithm enjoys an \u02dcO(poly(1/\u01eb)) sam-\n                             ple complexity (where \u01eb is the desired suboptimality of the learned policy) with\n                             general function approximation thanks to the property of (semi-)strong convexity\n                             of the objective functions. We only require nearly minimal assumptions on the\n                             dataset (single-policy concentrability) and the function class (realizability). More-\n                             over, this algorithm consists of two uninterleaved optimization steps, which we\n                             refer to as V -learning and policy learning, and is computationally stable since it\n                             does not involve minimax optimization. We also empirically validate our theory\n                             by showing that the modifi   ed algorithm outperforms the previous algorithm in\n                             various real-world environments. To the best of our knowledge, this is the fi     rst\n                             algorithm that is both provably efficient with general function approximation and\n                             single-policy concentrability, and empirically successful without requiring solving\n                             minimax optimization problems.\n                    1   Introduction\n                    Goal-conditioned reinforcement learning (GCRL) aims to design agents that are able to learn\n                    general-purpose skills to reach diverse goals [Kaelbling, 1993, Schaul et al., 2015, Plappert et al.,\n                    2018]. In particular, offl\n                                             ine GCRL learns goal-reaching policies by purely pre-collected data with-\n                    out any further interactions with the environment [Chebotar et al., 2021, Yang et al., 2022]. Since\n                    such interaction can be expensive or even unsafe in practice, offline GCRL is increasingly popular as\n                    a way to learn generalist agents in real-world environments [Lange et al., 2012, Levine et al., 2020].\n                    Although offl ine GCRL is promising and achieves great success in various practical scenar-\n                    ios [Lynch et al., 2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022b,c], designing prac-\n                    tical algorithms that are provably efficient still remains an open question. On the practical side, an\n                    ideal algorithm should be scalable to huge (or infinite) state spaces and only require minimal dataset\n                    coverage assumptions. Moreover, the algorithm should be computationally effi     cient and stable (e.g.,\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Provably Efficient Offline Goal-Conditioned Reinforcement Learning\n\n## Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability\n\narXiv:2302.03770v2 [cs.LG] 11 Oct 2023\n\nHanlin Zhu - EECS, UC Berkeley; Meta AI - hanlinzhu@berkeley.edu\n\nAmy Zhang - ECE, UT Austin; Meta AI - amy.zhang@austin.utexas.edu\n\n### Abstract\n\nGoal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $$\\tilde{O}(\\text{poly}(1/\\epsilon))$$ sample complexity (where $$\\epsilon$$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the objective functions. We only require nearly minimal assumptions on the dataset (single-policy concentrability) and the function class (realizability). Moreover, this algorithm consists of two uninterleaved optimization steps, which we refer to as V-learning and policy learning, and is computationally stable since it does not involve minimax optimization. We also empirically validate our theory by showing that the modified algorithm outperforms the previous algorithm in various real-world environments. To the best of our knowledge, this is the first algorithm that is both provably efficient with general function approximation and single-policy concentrability, and empirically successful without requiring solving minimax optimization problems.\n\n### 1 Introduction\n\nGoal-conditioned reinforcement learning (GCRL) aims to design agents that are able to learn general-purpose skills to reach diverse goals [Kaelbling, 1993, Schaul et al., 2015, Plappert et al., 2018]. In particular, offline GCRL learns goal-reaching policies by purely pre-collected data without any further interactions with the environment [Chebotar et al., 2021, Yang et al., 2022]. Since such interaction can be expensive or even unsafe in practice, offline GCRL is increasingly popular as a way to learn generalist agents in real-world environments [Lange et al., 2012, Levine et al., 2020]. Although offline GCRL is promising and achieves great success in various practical scenarios [Lynch et al., 2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022b,c], designing practical algorithms that are provably efficient still remains an open question. On the practical side, an ideal algorithm should be scalable to huge (or infinite) state spaces and only require minimal dataset coverage assumptions. Moreover, the algorithm should be computationally efficient and stable (e.g., 37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Provably Efficient Offline Goal-Conditioned Reinforcement Learning", "md": "# Provably Efficient Offline Goal-Conditioned Reinforcement Learning"}, {"type": "heading", "lvl": 2, "value": "Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability", "md": "## Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability"}, {"type": "text", "value": "arXiv:2302.03770v2 [cs.LG] 11 Oct 2023\n\nHanlin Zhu - EECS, UC Berkeley; Meta AI - hanlinzhu@berkeley.edu\n\nAmy Zhang - ECE, UT Austin; Meta AI - amy.zhang@austin.utexas.edu", "md": "arXiv:2302.03770v2 [cs.LG] 11 Oct 2023\n\nHanlin Zhu - EECS, UC Berkeley; Meta AI - hanlinzhu@berkeley.edu\n\nAmy Zhang - ECE, UT Austin; Meta AI - amy.zhang@austin.utexas.edu"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $$\\tilde{O}(\\text{poly}(1/\\epsilon))$$ sample complexity (where $$\\epsilon$$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the objective functions. We only require nearly minimal assumptions on the dataset (single-policy concentrability) and the function class (realizability). Moreover, this algorithm consists of two uninterleaved optimization steps, which we refer to as V-learning and policy learning, and is computationally stable since it does not involve minimax optimization. We also empirically validate our theory by showing that the modified algorithm outperforms the previous algorithm in various real-world environments. To the best of our knowledge, this is the first algorithm that is both provably efficient with general function approximation and single-policy concentrability, and empirically successful without requiring solving minimax optimization problems.", "md": "Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills that aim to reach diverse goals. In particular, offline GCRL only requires purely pre-collected datasets to perform training tasks without additional interactions with the environment. Although offline GCRL has become increasingly prevalent and many previous works have demonstrated its empirical success, the theoretical understanding of efficient offline GCRL algorithms is not well established, especially when the state space is huge and the offline dataset only covers the policy we aim to learn. In this paper, we provide a rigorous theoretical analysis of an existing empirically successful offline GCRL algorithm. We prove that under slight modification, this algorithm enjoys an $$\\tilde{O}(\\text{poly}(1/\\epsilon))$$ sample complexity (where $$\\epsilon$$ is the desired suboptimality of the learned policy) with general function approximation thanks to the property of (semi-)strong convexity of the objective functions. We only require nearly minimal assumptions on the dataset (single-policy concentrability) and the function class (realizability). Moreover, this algorithm consists of two uninterleaved optimization steps, which we refer to as V-learning and policy learning, and is computationally stable since it does not involve minimax optimization. We also empirically validate our theory by showing that the modified algorithm outperforms the previous algorithm in various real-world environments. To the best of our knowledge, this is the first algorithm that is both provably efficient with general function approximation and single-policy concentrability, and empirically successful without requiring solving minimax optimization problems."}, {"type": "heading", "lvl": 3, "value": "1 Introduction", "md": "### 1 Introduction"}, {"type": "text", "value": "Goal-conditioned reinforcement learning (GCRL) aims to design agents that are able to learn general-purpose skills to reach diverse goals [Kaelbling, 1993, Schaul et al., 2015, Plappert et al., 2018]. In particular, offline GCRL learns goal-reaching policies by purely pre-collected data without any further interactions with the environment [Chebotar et al., 2021, Yang et al., 2022]. Since such interaction can be expensive or even unsafe in practice, offline GCRL is increasingly popular as a way to learn generalist agents in real-world environments [Lange et al., 2012, Levine et al., 2020]. Although offline GCRL is promising and achieves great success in various practical scenarios [Lynch et al., 2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022b,c], designing practical algorithms that are provably efficient still remains an open question. On the practical side, an ideal algorithm should be scalable to huge (or infinite) state spaces and only require minimal dataset coverage assumptions. Moreover, the algorithm should be computationally efficient and stable (e.g., 37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Goal-conditioned reinforcement learning (GCRL) aims to design agents that are able to learn general-purpose skills to reach diverse goals [Kaelbling, 1993, Schaul et al., 2015, Plappert et al., 2018]. In particular, offline GCRL learns goal-reaching policies by purely pre-collected data without any further interactions with the environment [Chebotar et al., 2021, Yang et al., 2022]. Since such interaction can be expensive or even unsafe in practice, offline GCRL is increasingly popular as a way to learn generalist agents in real-world environments [Lange et al., 2012, Levine et al., 2020]. Although offline GCRL is promising and achieves great success in various practical scenarios [Lynch et al., 2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022b,c], designing practical algorithms that are provably efficient still remains an open question. On the practical side, an ideal algorithm should be scalable to huge (or infinite) state spaces and only require minimal dataset coverage assumptions. Moreover, the algorithm should be computationally efficient and stable (e.g., 37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "only using regression-based methods to train policies to avoid unstable minimax optimization). On\nthe theoretical side, we aim to provide finite-sample guarantees of the learned policy.\nUnfortunately, most existing algorithms are not both theoretically and practically effi    cient. On the\none hand, many empirically effi  cient algorithms do not enjoy fi nite-sample guarantees [Lynch et al.,\n2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022c] or even suffer constant suboptimal-\nity in favorable settings given infinite data (e.g., Ma et al. [2022c]). On the other hand, although\nmany previous offl  ine RL algorithms with theoretical fi  nite-sample guarantees can be naturally ex-\ntended to offline GCRL settings, they either cannot handle general value function approximation in\nthe presence of huge (or infi nite) state spaces [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al.,\n2021, Shi et al., 2022, Li et al., 2022], or require impractically strong dataset coverage assumptions,\nsuch as all policy concentrability [Antos et al., 2008, Munos and Szepesv\u00e1ri, 2008, Xie and Jiang,\n2021b].\nRecently, several provably effi cient algorithms have been proposed under general function approxi-\nmation and single-policy concentrability [Zhan et al., 2022, Cheng et al., 2022, Rashidinejad et al.,\n2022]. In particular, the algorithm of [Zhan et al., 2022], based on the duality form of regularized\nlinear programming formulation of RL, only requires the realizability assumption of function class.\nHowever, they all require solving minimax optimization problems which can be diffi        cult or compu-\ntationally unstable [Daskalakis et al., 2021]. On the contrary, some practically effi   cient algorithms\n(e.g., Ma et al. [2022b,c]) do not involve minimax optimization and thus are computationally more\nstable. This naturally raises an important question:\n         Can we design an efficient offline GCRL algorithm that enjoys favorable theoret-\n          ical guarantees under mild assumptions and performs well empirically in real-\n          world scenarios without a minimax formulation?\nIn this paper, we answer the above question affi    rmatively by providing rigorous theoretical guar-\nantees for an empirically successful offl  ine GCRL algorithm named GoFAR proposed by Ma et al.\n[2022c]. We made some slight yet critical modifi     cations to GoFAR. For deterministic MDPs, we\nneed to carefully select the value of one hyperparameter that is set to 1 in the original GoFAR (which\ncan be tuned in practice). For stochastic MDPs, we need to fi    rst learn the true transition model via\nmaximum likelihood (MLE) and then plug in the learned model in the algorithm. To distinguish\nthe difference between the original algorithm and the modifi   ed ones, we name the modifi   ed versions\nVP-learning (Algorithm 1).\nWe show that the VP-learning algorithm has both good empirical performance in real-world sce-\nnarios (already shown by Ma et al. [2022c], and we compare VP-learning and GoFAR empirically\nand show that our modifi  cation further improves the performance of the previous algorithm GoFAR)\nand favorable theoretical guarantees under mild assumptions. Specifi    cally, it achieves \u02dc\n                                                                                          O(poly(1/\u01eb))\nsample complexity (where \u01eb is the desired suboptimality level of the learned policy) under general\nfunction approximation with realizability-only assumption and partial data coverage with single-\npolicy concentrability assumption. Moreover, the VP-learning algorithm can be decomposed into\ntwo uninterleaved learning (optimization) procedures (i.e., V -learning and policy learning), which\nonly require solving regression problems without minimax optimization.\nNote that the VP-learning algorithm can be naturally applied to single-task RL settings, and all the\nanalysis in this paper does not rely on whether the setting is goal-conditioned. Since the original al-\ngorithm is proposed and empirically validated in goal-conditioned settings, we analyze the algorithm\nin goal-conditioned settings as well.\n1.1   Related Work\nSince our algorithm can be naturally applied to single-task offl ine RL settings, we discuss the related\nwork in a broader scope, which also includes single-task offl   ine RL.\nOffline RL in tabular and linear function approximation settings.             In tabular and linear set-\ntings, a line of work proposed effi  cient (both statistically and computationally) algorithms under\nsingle-policy concentrability [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al., 2021, Shi et al.,\n2022, Li et al., 2022]. These algorithms construct uncertainty quantifi   ers to ensure pessimism such\nthat policies not well covered by the dataset (which, by single-policy concentrability assumption,\n                                                    2", "md": "only using regression-based methods to train policies to avoid unstable minimax optimization). On\nthe theoretical side, we aim to provide finite-sample guarantees of the learned policy.\nUnfortunately, most existing algorithms are not both theoretically and practically efficient. On the\none hand, many empirically efficient algorithms do not enjoy finite-sample guarantees [Lynch et al.,\n2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022c] or even suffer constant suboptimality\nin favorable settings given infinite data (e.g., Ma et al. [2022c]). On the other hand, although\nmany previous offline RL algorithms with theoretical finite-sample guarantees can be naturally extended to offline GCRL settings, they either cannot handle general value function approximation in\nthe presence of huge (or infinite) state spaces [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al.,\n2021, Shi et al., 2022, Li et al., 2022], or require impractically strong dataset coverage assumptions,\nsuch as all policy concentrability [Antos et al., 2008, Munos and Szepesv\u00e1ri, 2008, Xie and Jiang,\n2021b].\n\nRecently, several provably efficient algorithms have been proposed under general function approximation and single-policy concentrability [Zhan et al., 2022, Cheng et al., 2022, Rashidinejad et al.,\n2022]. In particular, the algorithm of [Zhan et al., 2022], based on the duality form of regularized\nlinear programming formulation of RL, only requires the realizability assumption of function class.\nHowever, they all require solving minimax optimization problems which can be difficult or computationally unstable [Daskalakis et al., 2021]. On the contrary, some practically efficient algorithms\n(e.g., Ma et al. [2022b,c]) do not involve minimax optimization and thus are computationally more\nstable. This naturally raises an important question:\n\nCan we design an efficient offline GCRL algorithm that enjoys favorable theoretical guarantees under mild assumptions and performs well empirically in real-world scenarios without a minimax formulation?\n\nIn this paper, we answer the above question affirmatively by providing rigorous theoretical guarantees for an empirically successful offline GCRL algorithm named GoFAR proposed by Ma et al.\n[2022c]. We made some slight yet critical modifications to GoFAR. For deterministic MDPs, we\nneed to carefully select the value of one hyperparameter that is set to 1 in the original GoFAR (which\ncan be tuned in practice). For stochastic MDPs, we need to first learn the true transition model via\nmaximum likelihood (MLE) and then plug in the learned model in the algorithm. To distinguish\nthe difference between the original algorithm and the modified ones, we name the modified versions\nVP-learning (Algorithm 1).\n\nWe show that the VP-learning algorithm has both good empirical performance in real-world scenarios (already shown by Ma et al. [2022c], and we compare VP-learning and GoFAR empirically\nand show that our modification further improves the performance of the previous algorithm GoFAR)\nand favorable theoretical guarantees under mild assumptions. Specifically, it achieves $$\\tilde{O}(\\text{poly}(1/\\epsilon))$$\nsample complexity (where $$\\epsilon$$ is the desired suboptimality level of the learned policy) under general\nfunction approximation with realizability-only assumption and partial data coverage with single-\npolicy concentrability assumption. Moreover, the VP-learning algorithm can be decomposed into\ntwo uninterleaved learning (optimization) procedures (i.e., V -learning and policy learning), which\nonly require solving regression problems without minimax optimization.\n\nNote that the VP-learning algorithm can be naturally applied to single-task RL settings, and all the\nanalysis in this paper does not rely on whether the setting is goal-conditioned. Since the original algorithm is proposed and empirically validated in goal-conditioned settings, we analyze the algorithm\nin goal-conditioned settings as well.\n\n## 1.1 Related Work\n\nSince our algorithm can be naturally applied to single-task offline RL settings, we discuss the related\nwork in a broader scope, which also includes single-task offline RL.\n\nOffline RL in tabular and linear function approximation settings. In tabular and linear settings, a line of work proposed efficient (both statistically and computationally) algorithms under\nsingle-policy concentrability [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al., 2021, Shi et al.,\n2022, Li et al., 2022]. These algorithms construct uncertainty quantifiers to ensure pessimism such\nthat policies not well covered by the dataset (which, by single-policy concentrability assumption,", "images": [], "items": [{"type": "text", "value": "only using regression-based methods to train policies to avoid unstable minimax optimization). On\nthe theoretical side, we aim to provide finite-sample guarantees of the learned policy.\nUnfortunately, most existing algorithms are not both theoretically and practically efficient. On the\none hand, many empirically efficient algorithms do not enjoy finite-sample guarantees [Lynch et al.,\n2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022c] or even suffer constant suboptimality\nin favorable settings given infinite data (e.g., Ma et al. [2022c]). On the other hand, although\nmany previous offline RL algorithms with theoretical finite-sample guarantees can be naturally extended to offline GCRL settings, they either cannot handle general value function approximation in\nthe presence of huge (or infinite) state spaces [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al.,\n2021, Shi et al., 2022, Li et al., 2022], or require impractically strong dataset coverage assumptions,\nsuch as all policy concentrability [Antos et al., 2008, Munos and Szepesv\u00e1ri, 2008, Xie and Jiang,\n2021b].\n\nRecently, several provably efficient algorithms have been proposed under general function approximation and single-policy concentrability [Zhan et al., 2022, Cheng et al., 2022, Rashidinejad et al.,\n2022]. In particular, the algorithm of [Zhan et al., 2022], based on the duality form of regularized\nlinear programming formulation of RL, only requires the realizability assumption of function class.\nHowever, they all require solving minimax optimization problems which can be difficult or computationally unstable [Daskalakis et al., 2021]. On the contrary, some practically efficient algorithms\n(e.g., Ma et al. [2022b,c]) do not involve minimax optimization and thus are computationally more\nstable. This naturally raises an important question:\n\nCan we design an efficient offline GCRL algorithm that enjoys favorable theoretical guarantees under mild assumptions and performs well empirically in real-world scenarios without a minimax formulation?\n\nIn this paper, we answer the above question affirmatively by providing rigorous theoretical guarantees for an empirically successful offline GCRL algorithm named GoFAR proposed by Ma et al.\n[2022c]. We made some slight yet critical modifications to GoFAR. For deterministic MDPs, we\nneed to carefully select the value of one hyperparameter that is set to 1 in the original GoFAR (which\ncan be tuned in practice). For stochastic MDPs, we need to first learn the true transition model via\nmaximum likelihood (MLE) and then plug in the learned model in the algorithm. To distinguish\nthe difference between the original algorithm and the modified ones, we name the modified versions\nVP-learning (Algorithm 1).\n\nWe show that the VP-learning algorithm has both good empirical performance in real-world scenarios (already shown by Ma et al. [2022c], and we compare VP-learning and GoFAR empirically\nand show that our modification further improves the performance of the previous algorithm GoFAR)\nand favorable theoretical guarantees under mild assumptions. Specifically, it achieves $$\\tilde{O}(\\text{poly}(1/\\epsilon))$$\nsample complexity (where $$\\epsilon$$ is the desired suboptimality level of the learned policy) under general\nfunction approximation with realizability-only assumption and partial data coverage with single-\npolicy concentrability assumption. Moreover, the VP-learning algorithm can be decomposed into\ntwo uninterleaved learning (optimization) procedures (i.e., V -learning and policy learning), which\nonly require solving regression problems without minimax optimization.\n\nNote that the VP-learning algorithm can be naturally applied to single-task RL settings, and all the\nanalysis in this paper does not rely on whether the setting is goal-conditioned. Since the original algorithm is proposed and empirically validated in goal-conditioned settings, we analyze the algorithm\nin goal-conditioned settings as well.", "md": "only using regression-based methods to train policies to avoid unstable minimax optimization). On\nthe theoretical side, we aim to provide finite-sample guarantees of the learned policy.\nUnfortunately, most existing algorithms are not both theoretically and practically efficient. On the\none hand, many empirically efficient algorithms do not enjoy finite-sample guarantees [Lynch et al.,\n2020, Chebotar et al., 2021, Yang et al., 2022, Ma et al., 2022c] or even suffer constant suboptimality\nin favorable settings given infinite data (e.g., Ma et al. [2022c]). On the other hand, although\nmany previous offline RL algorithms with theoretical finite-sample guarantees can be naturally extended to offline GCRL settings, they either cannot handle general value function approximation in\nthe presence of huge (or infinite) state spaces [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al.,\n2021, Shi et al., 2022, Li et al., 2022], or require impractically strong dataset coverage assumptions,\nsuch as all policy concentrability [Antos et al., 2008, Munos and Szepesv\u00e1ri, 2008, Xie and Jiang,\n2021b].\n\nRecently, several provably efficient algorithms have been proposed under general function approximation and single-policy concentrability [Zhan et al., 2022, Cheng et al., 2022, Rashidinejad et al.,\n2022]. In particular, the algorithm of [Zhan et al., 2022], based on the duality form of regularized\nlinear programming formulation of RL, only requires the realizability assumption of function class.\nHowever, they all require solving minimax optimization problems which can be difficult or computationally unstable [Daskalakis et al., 2021]. On the contrary, some practically efficient algorithms\n(e.g., Ma et al. [2022b,c]) do not involve minimax optimization and thus are computationally more\nstable. This naturally raises an important question:\n\nCan we design an efficient offline GCRL algorithm that enjoys favorable theoretical guarantees under mild assumptions and performs well empirically in real-world scenarios without a minimax formulation?\n\nIn this paper, we answer the above question affirmatively by providing rigorous theoretical guarantees for an empirically successful offline GCRL algorithm named GoFAR proposed by Ma et al.\n[2022c]. We made some slight yet critical modifications to GoFAR. For deterministic MDPs, we\nneed to carefully select the value of one hyperparameter that is set to 1 in the original GoFAR (which\ncan be tuned in practice). For stochastic MDPs, we need to first learn the true transition model via\nmaximum likelihood (MLE) and then plug in the learned model in the algorithm. To distinguish\nthe difference between the original algorithm and the modified ones, we name the modified versions\nVP-learning (Algorithm 1).\n\nWe show that the VP-learning algorithm has both good empirical performance in real-world scenarios (already shown by Ma et al. [2022c], and we compare VP-learning and GoFAR empirically\nand show that our modification further improves the performance of the previous algorithm GoFAR)\nand favorable theoretical guarantees under mild assumptions. Specifically, it achieves $$\\tilde{O}(\\text{poly}(1/\\epsilon))$$\nsample complexity (where $$\\epsilon$$ is the desired suboptimality level of the learned policy) under general\nfunction approximation with realizability-only assumption and partial data coverage with single-\npolicy concentrability assumption. Moreover, the VP-learning algorithm can be decomposed into\ntwo uninterleaved learning (optimization) procedures (i.e., V -learning and policy learning), which\nonly require solving regression problems without minimax optimization.\n\nNote that the VP-learning algorithm can be naturally applied to single-task RL settings, and all the\nanalysis in this paper does not rely on whether the setting is goal-conditioned. Since the original algorithm is proposed and empirically validated in goal-conditioned settings, we analyze the algorithm\nin goal-conditioned settings as well."}, {"type": "heading", "lvl": 2, "value": "1.1 Related Work", "md": "## 1.1 Related Work"}, {"type": "text", "value": "Since our algorithm can be naturally applied to single-task offline RL settings, we discuss the related\nwork in a broader scope, which also includes single-task offline RL.\n\nOffline RL in tabular and linear function approximation settings. In tabular and linear settings, a line of work proposed efficient (both statistically and computationally) algorithms under\nsingle-policy concentrability [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al., 2021, Shi et al.,\n2022, Li et al., 2022]. These algorithms construct uncertainty quantifiers to ensure pessimism such\nthat policies not well covered by the dataset (which, by single-policy concentrability assumption,", "md": "Since our algorithm can be naturally applied to single-task offline RL settings, we discuss the related\nwork in a broader scope, which also includes single-task offline RL.\n\nOffline RL in tabular and linear function approximation settings. In tabular and linear settings, a line of work proposed efficient (both statistically and computationally) algorithms under\nsingle-policy concentrability [Jin et al., 2021, Rashidinejad et al., 2021, Yin et al., 2021, Shi et al.,\n2022, Li et al., 2022]. These algorithms construct uncertainty quantifiers to ensure pessimism such\nthat policies not well covered by the dataset (which, by single-policy concentrability assumption,"}]}, {"page": 3, "text": "are thus suboptimal) suffer a large penalty. Yin and Wang [2021] also consider offl       ine RL with\nsingle-policy concentrability and achieve instance-dependent characterization. However, the above\nalgorithms cannot be directly applied to many practical scenarios when non-linear function approxi-\nmators are required, since it is hard to obtain uncertainty quantifi\n                                                                  ers without oracle access when func-\ntion approximators are non-linear [Jiang and Huang, 2020, Jin et al., 2021, Uehara and Sun, 2021,\nXie et al., 2021]. In our algorithm (V -learning step), we use a regularizer in the form of f-divergence\ninstead of uncertainty quantifi ers to ensure pessimism, which makes our algorithm effi     cient in the\npresence of non-linear function approximators without additional oracle access.\nOffline RL with all-policy concentrability.      Besides huge state spaces, another central challenge\nfor offl\n       ine RL is the lack of dataset coverability. Concentrability, defi     ned as the ratio of occu-\npancy frequency induced by a policy to the dataset distribution, is one of the most widely used\ndefinitions to characterize the dataset coverability [Munos, 2007, Scherrer, 2014]. Many previ-\nous works require all-policy concentrability to make the algorithm effi  cient [Szepesv\u00e1ri and Munos,\n2005, Munos, 2007, Antos et al., 2007, 2008, Farahmand et al., 2010, Scherrer, 2014, Liu et al.,\n2019, Chen and Jiang, 2019, Jiang, 2019, Wang et al., 2019, Feng et al., 2019, Liao et al., 2020,\nZhang et al., 2020, Uehara et al., 2020, Xie and Jiang, 2021a]. However, in practice, it is unrea-\nsonable to require that the offline dataset can cover all candidate policies, and our algorithm only\nrequires single-policy concentrability.\nOffline RL with general function approximation and single-policy concentrability.              A recent\nline of work, which is based on marginalized importance sampling (MIS) formulation of RL,\nhas shown success either empirically [Nachum et al., 2019a,b, Lee et al., 2021, Kim et al., 2021]\nor theoretically  [Zhan et al., 2022, Rashidinejad et al., 2022].    In particular, Zhan et al. [2022],\nRashidinejad et al. [2022] provide fi nite-sample guarantees for their algorithms under general func-\ntion approximation and only single-policy concentrability. Another line of work [Xie et al., 2021,\nCheng et al., 2022] also proposes provably effi  cient algorithms based on an actor-critic formulation\nunder similar assumptions. However, all the above algorithms require solving minimax optimization,\nwhich could be difficult or computationally unstable [Daskalakis et al., 2021]. Instead, our algorithm\nonly involves uninterleaved regression-based optimization without minimax optimization.\nOffline GCRL.      In the context of offline GCRL, the sparsity of the reward is another core chal-\nlenge [Kaelbling, 1993, Schaul et al., 2015]. Several previous works aim to solve the issue and\nshow empirical success without fi   nite-sample guarantee [Ghosh et al., 2019, Chebotar et al., 2021,\nYang et al., 2022, Ma et al., 2022c]. Although there exist theoretical studies of relevant problems\nsuch as the offline stochastic shortest path problem [Yin et al., 2022], theoretical understanding of\nthe offl\n       ine GCRL problem is still lacking. The most relevant work to this paper is Ma et al. [2022c],\nwhich shows great performance in several real-world settings without minimax optimization but\nlacks theoretical guarantee. In this paper, we proved that a slightly modifi  ed version of their algo-\nrithm (i.e., our VP-learning algorithm shown in Algorithm 1) is provably effi cient. Also, we note that\nMa et al. [2022a] can be viewed as the single-task version of Ma et al. [2022c], and our algorithm\nand analysis can be naturally extended to single-task offl ine RL settings.\n2   Preliminaries\nBasic Notations.    Throughout this paper, we use |X| and \u2206(X) to denote the cardinality and prob-\nability simplex of a given set X. We use x \u2272   y to denote that there exists a constant c > 0 such that\nx \u2264  cy, use x \u2273   y if y \u2272  x and use x \u224d    y if x \u2272  y and y \u2272   x. Also, we use the standard O(\u00b7)\nnotation where f(n) = O(g(n)) if there exists n0, C > 0 such that |f(n)|\u2264        Cg(n) for all n \u2265   n0,\nand denote f(n) = \u2126(g(n)) if g(n) = O(f(n)). For any x \u2208             R, defi ne x+ \u225c   max{x, 0}; for\nany general function f : X \u2192     R, define f+(x) = max{f(x), 0}, \u2200x \u2208       X. Also, for any function\nf : R \u2192   R, define \u00aff(x) = f(x) \u2212    minu\u2208R f(u) for any x \u2208     R if minu\u2208R f(u) exists and further\noverwrite the notation f+(x) =   1{f \u2032(x) \u2265   0} \u00b7 \u00af\n                                                  f(x) where   1{\u00b7} is the indicator function.\nMarkov decision process.       We consider an infi  nite-horizon discounted Markov decision process\n(MDP), which is described by a tuple M = (S, A, P, R, \u03c1, \u03b3), where S and A denote the state and\naction spaces respectively, P : S \u00d7 A \u2192      \u2206(S) is the transition kernel, R : S \u00d7 A \u2192       \u2206([0, 1])\nencodes a family of reward distributions given state-action pairs with r : S \u00d7 A \u2192         [0, 1] as the\n                                                   3", "md": "# Offline Reinforcement Learning\n\n## Offline Reinforcement Learning\n\nOffline reinforcement learning (RL) is a challenging problem that involves learning a policy from a fixed dataset without interacting with the environment. In this paper, various approaches and algorithms for offline RL are discussed.\n\n### Offline RL with Single-Policy Concentrability\n\nYin and Wang [2021] proposed an offline RL algorithm with single-policy concentrability, achieving instance-dependent characterization. However, existing algorithms face limitations when non-linear function approximators are needed, as obtaining uncertainty quantifiers without oracle access is challenging.\n\nIn our algorithm (V-learning step), we use a regularizer in the form of f-divergence to ensure pessimism, making it efficient with non-linear function approximators.\n\n### Offline RL with All-Policy Concentrability\n\nConcentrability, defined as the ratio of occupancy frequency induced by a policy to the dataset distribution, is crucial for dataset coverability in offline RL. While many algorithms require all-policy concentrability for efficiency, our algorithm only needs single-policy concentrability.\n\n### Offline RL with General Function Approximation\n\nRecent work based on marginalized importance sampling (MIS) formulation has shown success in offline RL with general function approximation and single-policy concentrability. These algorithms provide finite-sample guarantees without the need for minimax optimization.\n\n### Offline GCRL\n\nIn the context of offline Generative Counterfactual Reinforcement Learning (GCRL), addressing the sparsity of rewards is a core challenge. While empirical success has been demonstrated in previous works, theoretical understanding of offline GCRL remains limited.\n\n## Preliminaries\n\n### Basic Notations\n\nThroughout the paper, |X| denotes the cardinality of set X, and \u2206(X) represents the probability simplex of X. Various mathematical notations and definitions are used to establish the theoretical framework for offline RL algorithms.\n\n### Markov Decision Process\n\nWe consider an infinite-horizon discounted Markov decision process (MDP) described by a tuple M = (S, A, P, R, \u03c1, \u03b3). This framework forms the basis for modeling the environment and rewards in RL problems.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Offline Reinforcement Learning", "md": "# Offline Reinforcement Learning"}, {"type": "heading", "lvl": 2, "value": "Offline Reinforcement Learning", "md": "## Offline Reinforcement Learning"}, {"type": "text", "value": "Offline reinforcement learning (RL) is a challenging problem that involves learning a policy from a fixed dataset without interacting with the environment. In this paper, various approaches and algorithms for offline RL are discussed.", "md": "Offline reinforcement learning (RL) is a challenging problem that involves learning a policy from a fixed dataset without interacting with the environment. In this paper, various approaches and algorithms for offline RL are discussed."}, {"type": "heading", "lvl": 3, "value": "Offline RL with Single-Policy Concentrability", "md": "### Offline RL with Single-Policy Concentrability"}, {"type": "text", "value": "Yin and Wang [2021] proposed an offline RL algorithm with single-policy concentrability, achieving instance-dependent characterization. However, existing algorithms face limitations when non-linear function approximators are needed, as obtaining uncertainty quantifiers without oracle access is challenging.\n\nIn our algorithm (V-learning step), we use a regularizer in the form of f-divergence to ensure pessimism, making it efficient with non-linear function approximators.", "md": "Yin and Wang [2021] proposed an offline RL algorithm with single-policy concentrability, achieving instance-dependent characterization. However, existing algorithms face limitations when non-linear function approximators are needed, as obtaining uncertainty quantifiers without oracle access is challenging.\n\nIn our algorithm (V-learning step), we use a regularizer in the form of f-divergence to ensure pessimism, making it efficient with non-linear function approximators."}, {"type": "heading", "lvl": 3, "value": "Offline RL with All-Policy Concentrability", "md": "### Offline RL with All-Policy Concentrability"}, {"type": "text", "value": "Concentrability, defined as the ratio of occupancy frequency induced by a policy to the dataset distribution, is crucial for dataset coverability in offline RL. While many algorithms require all-policy concentrability for efficiency, our algorithm only needs single-policy concentrability.", "md": "Concentrability, defined as the ratio of occupancy frequency induced by a policy to the dataset distribution, is crucial for dataset coverability in offline RL. While many algorithms require all-policy concentrability for efficiency, our algorithm only needs single-policy concentrability."}, {"type": "heading", "lvl": 3, "value": "Offline RL with General Function Approximation", "md": "### Offline RL with General Function Approximation"}, {"type": "text", "value": "Recent work based on marginalized importance sampling (MIS) formulation has shown success in offline RL with general function approximation and single-policy concentrability. These algorithms provide finite-sample guarantees without the need for minimax optimization.", "md": "Recent work based on marginalized importance sampling (MIS) formulation has shown success in offline RL with general function approximation and single-policy concentrability. These algorithms provide finite-sample guarantees without the need for minimax optimization."}, {"type": "heading", "lvl": 3, "value": "Offline GCRL", "md": "### Offline GCRL"}, {"type": "text", "value": "In the context of offline Generative Counterfactual Reinforcement Learning (GCRL), addressing the sparsity of rewards is a core challenge. While empirical success has been demonstrated in previous works, theoretical understanding of offline GCRL remains limited.", "md": "In the context of offline Generative Counterfactual Reinforcement Learning (GCRL), addressing the sparsity of rewards is a core challenge. While empirical success has been demonstrated in previous works, theoretical understanding of offline GCRL remains limited."}, {"type": "heading", "lvl": 2, "value": "Preliminaries", "md": "## Preliminaries"}, {"type": "heading", "lvl": 3, "value": "Basic Notations", "md": "### Basic Notations"}, {"type": "text", "value": "Throughout the paper, |X| denotes the cardinality of set X, and \u2206(X) represents the probability simplex of X. Various mathematical notations and definitions are used to establish the theoretical framework for offline RL algorithms.", "md": "Throughout the paper, |X| denotes the cardinality of set X, and \u2206(X) represents the probability simplex of X. Various mathematical notations and definitions are used to establish the theoretical framework for offline RL algorithms."}, {"type": "heading", "lvl": 3, "value": "Markov Decision Process", "md": "### Markov Decision Process"}, {"type": "text", "value": "We consider an infinite-horizon discounted Markov decision process (MDP) described by a tuple M = (S, A, P, R, \u03c1, \u03b3). This framework forms the basis for modeling the environment and rewards in RL problems.", "md": "We consider an infinite-horizon discounted Markov decision process (MDP) described by a tuple M = (S, A, P, R, \u03c1, \u03b3). This framework forms the basis for modeling the environment and rewards in RL problems."}]}, {"page": 4, "text": " expected reward function, \u03c1 : S \u2192                  [0, 1] is the initial state distribution, and \u03b3 \u2208              [0, 1) is the\n discount factor. We assume A is fi           nite while S could be arbitrarily complex (even continuous) as in\n many real-world scenarios. A stationary (stochastic) policy \u03c0 : S \u2192                          \u2206(A) outputs a distribution\n over action space for each state.\n Goal-conditioned reinforcement learning.                      In goal-conditioned RL, we additionally assume a\n goal set G. Similar to Ma et al. [2022c], in goal-conditioned settings, the reward function R(s; g)\n (as well as the expected reward function r(s; g)) and policy \u03c0(a|s, g) also depend on the commanded\n goal g \u2208    G, and the reward no longer depends on the action a and is deterministic.\n Each (goal-conditioned) policy \u03c0 induces a (discounted) occupancy density over state-action pairs\n for any commanded goal d\u03c0 : S \u00d7A\u00d7G \u2192                     [0, 1] defi ned as d\u03c0(s, a; g) := (1\u2212\u03b3)  \u221e           t=0 \u03b3t Pr(st =\n s, at = a; \u03c0), where Pr(st = s, at = a; \u03c0) denotes the visitation probability of state-action pair\n(s, a) at step t, starting at s0 \u223c            \u03c1(\u00b7) and following \u03c0 given commanded goal g. We also write\n d\u03c0 (s; g) =  a\u2208A d\u03c0(s, a; g) to denote the marginalized state occupancy. Let p(g) be a distribution\n over desired goals, then we denote d\u03c0(s, a, g) = d\u03c0(s, a; g)p(g) and d\u03c0(s, g) = d\u03c0(s; g)p(g). An\n important property of occupancy density d\u03c0 is that it satisfi               es the following Bellman fl         ow constraint:\n                             a   d(s, a; g) = (1 \u2212      \u03b3)\u03c1(s)+\u03b3      s\u2032,a\u2032 P(s|s\u2032, a\u2032)d(s\u2032, a\u2032; g)                            (1)\n for all s \u2208   S and g \u2208      G when letting d = d\u03c0 for any policy \u03c0. Moreover, any d satisfying (1) is the\n occupancy density of a policy \u03c0d where\n          \u03c0d(a|s, g) =       d(s, a; g)/d(s; g),            d(s; g) > 0         and      d(s; g) =          d(s, a; g).        (2)\n                              1/|A|,                        d(s; g) = 0                               a\u2208A\n An important quantity associated with a policy \u03c0 is the value function, which is the expected dis-\n counted cumulative reward defi           ned as V \u03c0(s; g) := E [ \u221e         t=0 \u03b3trt | s0 = s, at \u223c        \u03c0(\u00b7|st, g) \u2200   t \u2265   0]\n starting at state s \u2208      S with a commanded goal g \u2208              G where rt = R(st; g) = r(st; g). We use the\n notation J(\u03c0) := (1 \u2212         \u03b3)Es\u223c\u03c1,g\u223cp[V \u03c0(s; g)] = E(s,a,g)\u223cd\u03c0[r(s; g)] to represent a scalar summary\n of the performance of a policy \u03c0. We denote by \u03c0\u2217                     the optimal policy that maximizes the above\n objective and use V \u2217       := V \u03c0\u2217    to denote the optimal value function.\n Offl ine GCRL.          In this paper, we focus on offl        ine GCRL, where the agent is only provided with a\n previously-collected offline dataset D = {(si, ai, ri, s\u2032              i, gi)}Ni=1. Here, ri \u223c        R(si; gi), s\u2032  i \u223c   P(\u00b7 |\n si, ai), and we assume that gi are i.i.d. sampled from p(\u00b7) and it is common that data are collected\n by a behavior policy \u00b5 of which the discounted occupancy density is d\u00b5. Therefore, we assume that\n(si, ai, gi) are sampled i.i.d. from a distribution \u00b5 where \u00b5(s, a, g) = p(g)d\u00b5(s, a; g) = d\u00b5(s, a, g).\n Note that we use \u00b5 to denote both the behavior policy and the dataset distribution. We also assume\n an additional dataset D0 = {(s0,i, g0,i)}N0            i=1 where s0,i are i.i.d. sampled from \u03c1(\u00b7) and g0,i are\n i.i.d. sampled from p(\u00b7). The goal of offl             ine RL is to learn a policy \u02c6       \u03c0 using the offl     ine dataset so\n as to minimize the sub-optimality compared to the optimal policy \u03c0\u2217, i.e., J(\u03c0\u2217) \u2212                           J(\u02c6\u03c0), with high\n probability.\n Function approximation.               To deal with huge state spaces, (general) function approximation is\n necessary for practical scenarios. In this paper, we assume access to two function classes: a value\n function class V \u2286        {V : S \u00d7 G \u2192           [0, Vmax]} that models the value function of the (regularized)\n optimal policies, and a policy class \u03a0 \u2286             {\u03c0 : S \u00d7 G \u2192       \u2206(A)} consisting of candidate policies. For\n stochastic MDP (Section 3.1.2), we also need a transition kernel class P \u2286                         {P : S \u00d7 A \u2192         \u2206(S)}\n which contains the ground-truth transition kernel. Additionally, for any function f : S \u00d7 G \u2192                                  R,\n we denote the operator T : RS\u00d7G \u2192                 RS\u00d7A\u00d7G as (T f)(s, a; g) = Es\u2032\u223cP (\u00b7|s,a)[f(s\u2032; g)]. Also, for\n any function V : S \u00d7 G \u2192            [0, Vmax], we defi     ne AV (s, a; g) = r(s; g) + \u03b3T V (s, a; g) \u2212             V (s; g).\n Offl ine data coverage assumption.                Our algorithm works within the single-policy concentrability\n framework [Rashidinejad et al., 2021], which is defi                ned as below.\n Defi nition 1 (Single-policy concentrability for GCRL). Given a policy \u03c0, define C\u03c0 to be the small-\n est constant that satisfies d\u03c0(s,a,g)\n                                   \u00b5(s,a,g) \u2264     C\u03c0 for all s \u2208 4   S, a \u2208    A and g \u2208      G.", "md": "Expected reward function, \\( \\rho : S \\rightarrow [0, 1] \\) is the initial state distribution, and \\( \\gamma \\in [0, 1) \\) is the discount factor. We assume \\( A \\) is finite while \\( S \\) could be arbitrarily complex (even continuous) as in many real-world scenarios. A stationary (stochastic) policy \\( \\pi : S \\rightarrow \\Delta(A) \\) outputs a distribution over action space for each state.\n\nGoal-conditioned reinforcement learning. In goal-conditioned RL, we additionally assume a goal set \\( G \\). Similar to Ma et al. [2022c], in goal-conditioned settings, the reward function \\( R(s; g) \\) (as well as the expected reward function \\( r(s; g) \\)) and policy \\( \\pi(a|s, g) \\) also depend on the commanded goal \\( g \\in G \\), and the reward no longer depends on the action \\( a \\) and is deterministic.\n\nEach (goal-conditioned) policy \\( \\pi \\) induces a (discounted) occupancy density over state-action pairs for any commanded goal \\( d\\pi : S \\times A \\times G \\rightarrow [0, 1] \\) defined as \\( d\\pi(s, a; g) := (1-\\gamma) \\sum_{t=0}^{\\infty} \\gamma^t \\text{Pr}(s_t = s, a_t = a; \\pi) \\), where \\( \\text{Pr}(s_t = s, a_t = a; \\pi) \\) denotes the visitation probability of state-action pair \\( (s, a) \\) at step \\( t \\), starting at \\( s_0 \\sim \\rho(\\cdot) \\) and following \\( \\pi \\) given commanded goal \\( g \\). We also write \\( d\\pi (s; g) = \\sum_{a \\in A} d\\pi(s, a; g) \\) to denote the marginalized state occupancy. Let \\( p(g) \\) be a distribution over desired goals, then we denote \\( d\\pi(s, a, g) = d\\pi(s, a; g)p(g) \\) and \\( d\\pi(s, g) = d\\pi(s; g)p(g) \\). An important property of occupancy density \\( d\\pi \\) is that it satisfies the following Bellman flow constraint:\n\n$$\n\\begin{equation}\na \\cdot d(s, a; g) = (1 - \\gamma)\\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g) \\tag{1}\n\\end{equation}\n$$\n\nfor all \\( s \\in S \\) and \\( g \\in G \\) when letting \\( d = d\\pi \\) for any policy \\( \\pi \\). Moreover, any \\( d \\) satisfying (1) is the occupancy density of a policy \\( \\pi_d \\) where\n\n$$\n\\begin{equation}\n\\pi_d(a|s, g) = \\frac{d(s, a; g)}{d(s; g)}, \\quad d(s; g) > 0 \\quad \\text{and} \\quad d(s; g) = \\frac{1}{|A|}, \\quad d(s; g) = 0 \\quad \\text{for} \\quad a \\in A \\tag{2}\n\\end{equation}\n$$\n\nAn important quantity associated with a policy \\( \\pi \\) is the value function, which is the expected discounted cumulative reward defined as \\( V^{\\pi}(s; g) := E [ \\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s, a_t \\sim \\pi(\\cdot|s_t, g) \\forall t \\geq 0] \\) starting at state \\( s \\in S \\) with a commanded goal \\( g \\in G \\) where \\( r_t = R(s_t; g) = r(s_t; g) \\). We use the notation \\( J(\\pi) := (1 - \\gamma)E_{s \\sim \\rho, g \\sim p}[V^{\\pi}(s; g)] = E_{(s,a,g) \\sim d\\pi}[r(s; g)] \\) to represent a scalar summary of the performance of a policy \\( \\pi \\). We denote by \\( \\pi^* \\) the optimal policy that maximizes the above objective and use \\( V^* := V^{\\pi^*} \\) to denote the optimal value function.\n\nOffline GCRL. In this paper, we focus on offline GCRL, where the agent is only provided with a previously-collected offline dataset \\( D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^N \\). Here, \\( r_i \\sim R(s_i; g_i) \\), \\( s'_i \\sim P(\\cdot | s_i, a_i) \\), and we assume that \\( g_i \\) are i.i.d. sampled from \\( p(\\cdot) \\) and it is common that data are collected by a behavior policy \\( \\mu \\) of which the discounted occupancy density is \\( d\\mu \\). Therefore, we assume that \\( (s_i, a_i, g_i) \\) are sampled i.i.d. from a distribution \\( \\mu \\) where \\( \\mu(s, a, g) = p(g)d\\mu(s, a; g) = d\\mu(s, a, g) \\). Note that we use \\( \\mu \\) to denote both the behavior policy and the dataset distribution. We also assume an additional dataset \\( D_0 = \\{(s_{0,i}, g_{0,i})\\}_{i=1}^{N_0} \\) where \\( s_{0,i} \\) are i.i.d. sampled from \\( \\rho(\\cdot) \\) and \\( g_{0,i} \\) are i.i.d. sampled from \\( p(\\cdot) \\). The goal of offline RL is to learn a policy \\( \\hat{\\pi} \\) using the offline dataset so as to minimize the sub-optimality compared to the optimal policy \\( \\pi^* \\), i.e., \\( J(\\pi^*) - J(\\hat{\\pi}) \\), with high probability.\n\nFunction approximation. To deal with huge state spaces, (general) function approximation is necessary for practical scenarios. In this paper, we assume access to two function classes: a value function class \\( V \\subseteq \\{V : S \\times G \\rightarrow [0, V_{\\text{max}}]\\} \\) that models the value function of the (regularized) optimal policies, and a policy class \\( \\Pi \\subseteq \\{\\pi : S \\times G \\rightarrow \\Delta(A)\\} \\) consisting of candidate policies. For stochastic MDP (Section 3.1.2), we also need a transition kernel class \\( P \\subseteq \\{P : S \\times A \\rightarrow \\Delta(S)\\} \\) which contains the ground-truth transition kernel. Additionally, for any function \\( f : S \\times G \\rightarrow \\mathbb{R} \\), we denote the operator \\( T : \\mathbb{R}^{S \\times G} \\rightarrow \\mathbb{R}^{S \\times A \\times G} \\) as \\( (T f)(s, a; g) = E_{s' \\sim P(\\cdot|s,a)}[f(s'; g)] \\). Also, for any function \\( V : S \\times G \\rightarrow [0, V_{\\text{max}}] \\), we define \\( AV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) \\).\n\nOffline data coverage assumption. Our algorithm works within the single-policy concentrability framework [Rashidinejad et al., 2021], which is defined as below.\n\nDefinition 1 (Single-policy concentrability for GCRL). Given a policy \\( \\pi \\), define \\( C_{\\pi} \\) to be the smallest constant that satisfies \\( d_{\\pi}(s,a,g) \\leq C_{\\pi} \\) for all \\( s \\in S \\), \\( a \\in A \\) and \\( g \\in G \\).", "images": [], "items": [{"type": "text", "value": "Expected reward function, \\( \\rho : S \\rightarrow [0, 1] \\) is the initial state distribution, and \\( \\gamma \\in [0, 1) \\) is the discount factor. We assume \\( A \\) is finite while \\( S \\) could be arbitrarily complex (even continuous) as in many real-world scenarios. A stationary (stochastic) policy \\( \\pi : S \\rightarrow \\Delta(A) \\) outputs a distribution over action space for each state.\n\nGoal-conditioned reinforcement learning. In goal-conditioned RL, we additionally assume a goal set \\( G \\). Similar to Ma et al. [2022c], in goal-conditioned settings, the reward function \\( R(s; g) \\) (as well as the expected reward function \\( r(s; g) \\)) and policy \\( \\pi(a|s, g) \\) also depend on the commanded goal \\( g \\in G \\), and the reward no longer depends on the action \\( a \\) and is deterministic.\n\nEach (goal-conditioned) policy \\( \\pi \\) induces a (discounted) occupancy density over state-action pairs for any commanded goal \\( d\\pi : S \\times A \\times G \\rightarrow [0, 1] \\) defined as \\( d\\pi(s, a; g) := (1-\\gamma) \\sum_{t=0}^{\\infty} \\gamma^t \\text{Pr}(s_t = s, a_t = a; \\pi) \\), where \\( \\text{Pr}(s_t = s, a_t = a; \\pi) \\) denotes the visitation probability of state-action pair \\( (s, a) \\) at step \\( t \\), starting at \\( s_0 \\sim \\rho(\\cdot) \\) and following \\( \\pi \\) given commanded goal \\( g \\). We also write \\( d\\pi (s; g) = \\sum_{a \\in A} d\\pi(s, a; g) \\) to denote the marginalized state occupancy. Let \\( p(g) \\) be a distribution over desired goals, then we denote \\( d\\pi(s, a, g) = d\\pi(s, a; g)p(g) \\) and \\( d\\pi(s, g) = d\\pi(s; g)p(g) \\). An important property of occupancy density \\( d\\pi \\) is that it satisfies the following Bellman flow constraint:\n\n$$\n\\begin{equation}\na \\cdot d(s, a; g) = (1 - \\gamma)\\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g) \\tag{1}\n\\end{equation}\n$$\n\nfor all \\( s \\in S \\) and \\( g \\in G \\) when letting \\( d = d\\pi \\) for any policy \\( \\pi \\). Moreover, any \\( d \\) satisfying (1) is the occupancy density of a policy \\( \\pi_d \\) where\n\n$$\n\\begin{equation}\n\\pi_d(a|s, g) = \\frac{d(s, a; g)}{d(s; g)}, \\quad d(s; g) > 0 \\quad \\text{and} \\quad d(s; g) = \\frac{1}{|A|}, \\quad d(s; g) = 0 \\quad \\text{for} \\quad a \\in A \\tag{2}\n\\end{equation}\n$$\n\nAn important quantity associated with a policy \\( \\pi \\) is the value function, which is the expected discounted cumulative reward defined as \\( V^{\\pi}(s; g) := E [ \\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s, a_t \\sim \\pi(\\cdot|s_t, g) \\forall t \\geq 0] \\) starting at state \\( s \\in S \\) with a commanded goal \\( g \\in G \\) where \\( r_t = R(s_t; g) = r(s_t; g) \\). We use the notation \\( J(\\pi) := (1 - \\gamma)E_{s \\sim \\rho, g \\sim p}[V^{\\pi}(s; g)] = E_{(s,a,g) \\sim d\\pi}[r(s; g)] \\) to represent a scalar summary of the performance of a policy \\( \\pi \\). We denote by \\( \\pi^* \\) the optimal policy that maximizes the above objective and use \\( V^* := V^{\\pi^*} \\) to denote the optimal value function.\n\nOffline GCRL. In this paper, we focus on offline GCRL, where the agent is only provided with a previously-collected offline dataset \\( D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^N \\). Here, \\( r_i \\sim R(s_i; g_i) \\), \\( s'_i \\sim P(\\cdot | s_i, a_i) \\), and we assume that \\( g_i \\) are i.i.d. sampled from \\( p(\\cdot) \\) and it is common that data are collected by a behavior policy \\( \\mu \\) of which the discounted occupancy density is \\( d\\mu \\). Therefore, we assume that \\( (s_i, a_i, g_i) \\) are sampled i.i.d. from a distribution \\( \\mu \\) where \\( \\mu(s, a, g) = p(g)d\\mu(s, a; g) = d\\mu(s, a, g) \\). Note that we use \\( \\mu \\) to denote both the behavior policy and the dataset distribution. We also assume an additional dataset \\( D_0 = \\{(s_{0,i}, g_{0,i})\\}_{i=1}^{N_0} \\) where \\( s_{0,i} \\) are i.i.d. sampled from \\( \\rho(\\cdot) \\) and \\( g_{0,i} \\) are i.i.d. sampled from \\( p(\\cdot) \\). The goal of offline RL is to learn a policy \\( \\hat{\\pi} \\) using the offline dataset so as to minimize the sub-optimality compared to the optimal policy \\( \\pi^* \\), i.e., \\( J(\\pi^*) - J(\\hat{\\pi}) \\), with high probability.\n\nFunction approximation. To deal with huge state spaces, (general) function approximation is necessary for practical scenarios. In this paper, we assume access to two function classes: a value function class \\( V \\subseteq \\{V : S \\times G \\rightarrow [0, V_{\\text{max}}]\\} \\) that models the value function of the (regularized) optimal policies, and a policy class \\( \\Pi \\subseteq \\{\\pi : S \\times G \\rightarrow \\Delta(A)\\} \\) consisting of candidate policies. For stochastic MDP (Section 3.1.2), we also need a transition kernel class \\( P \\subseteq \\{P : S \\times A \\rightarrow \\Delta(S)\\} \\) which contains the ground-truth transition kernel. Additionally, for any function \\( f : S \\times G \\rightarrow \\mathbb{R} \\), we denote the operator \\( T : \\mathbb{R}^{S \\times G} \\rightarrow \\mathbb{R}^{S \\times A \\times G} \\) as \\( (T f)(s, a; g) = E_{s' \\sim P(\\cdot|s,a)}[f(s'; g)] \\). Also, for any function \\( V : S \\times G \\rightarrow [0, V_{\\text{max}}] \\), we define \\( AV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) \\).\n\nOffline data coverage assumption. Our algorithm works within the single-policy concentrability framework [Rashidinejad et al., 2021], which is defined as below.\n\nDefinition 1 (Single-policy concentrability for GCRL). Given a policy \\( \\pi \\), define \\( C_{\\pi} \\) to be the smallest constant that satisfies \\( d_{\\pi}(s,a,g) \\leq C_{\\pi} \\) for all \\( s \\in S \\), \\( a \\in A \\) and \\( g \\in G \\).", "md": "Expected reward function, \\( \\rho : S \\rightarrow [0, 1] \\) is the initial state distribution, and \\( \\gamma \\in [0, 1) \\) is the discount factor. We assume \\( A \\) is finite while \\( S \\) could be arbitrarily complex (even continuous) as in many real-world scenarios. A stationary (stochastic) policy \\( \\pi : S \\rightarrow \\Delta(A) \\) outputs a distribution over action space for each state.\n\nGoal-conditioned reinforcement learning. In goal-conditioned RL, we additionally assume a goal set \\( G \\). Similar to Ma et al. [2022c], in goal-conditioned settings, the reward function \\( R(s; g) \\) (as well as the expected reward function \\( r(s; g) \\)) and policy \\( \\pi(a|s, g) \\) also depend on the commanded goal \\( g \\in G \\), and the reward no longer depends on the action \\( a \\) and is deterministic.\n\nEach (goal-conditioned) policy \\( \\pi \\) induces a (discounted) occupancy density over state-action pairs for any commanded goal \\( d\\pi : S \\times A \\times G \\rightarrow [0, 1] \\) defined as \\( d\\pi(s, a; g) := (1-\\gamma) \\sum_{t=0}^{\\infty} \\gamma^t \\text{Pr}(s_t = s, a_t = a; \\pi) \\), where \\( \\text{Pr}(s_t = s, a_t = a; \\pi) \\) denotes the visitation probability of state-action pair \\( (s, a) \\) at step \\( t \\), starting at \\( s_0 \\sim \\rho(\\cdot) \\) and following \\( \\pi \\) given commanded goal \\( g \\). We also write \\( d\\pi (s; g) = \\sum_{a \\in A} d\\pi(s, a; g) \\) to denote the marginalized state occupancy. Let \\( p(g) \\) be a distribution over desired goals, then we denote \\( d\\pi(s, a, g) = d\\pi(s, a; g)p(g) \\) and \\( d\\pi(s, g) = d\\pi(s; g)p(g) \\). An important property of occupancy density \\( d\\pi \\) is that it satisfies the following Bellman flow constraint:\n\n$$\n\\begin{equation}\na \\cdot d(s, a; g) = (1 - \\gamma)\\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g) \\tag{1}\n\\end{equation}\n$$\n\nfor all \\( s \\in S \\) and \\( g \\in G \\) when letting \\( d = d\\pi \\) for any policy \\( \\pi \\). Moreover, any \\( d \\) satisfying (1) is the occupancy density of a policy \\( \\pi_d \\) where\n\n$$\n\\begin{equation}\n\\pi_d(a|s, g) = \\frac{d(s, a; g)}{d(s; g)}, \\quad d(s; g) > 0 \\quad \\text{and} \\quad d(s; g) = \\frac{1}{|A|}, \\quad d(s; g) = 0 \\quad \\text{for} \\quad a \\in A \\tag{2}\n\\end{equation}\n$$\n\nAn important quantity associated with a policy \\( \\pi \\) is the value function, which is the expected discounted cumulative reward defined as \\( V^{\\pi}(s; g) := E [ \\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0 = s, a_t \\sim \\pi(\\cdot|s_t, g) \\forall t \\geq 0] \\) starting at state \\( s \\in S \\) with a commanded goal \\( g \\in G \\) where \\( r_t = R(s_t; g) = r(s_t; g) \\). We use the notation \\( J(\\pi) := (1 - \\gamma)E_{s \\sim \\rho, g \\sim p}[V^{\\pi}(s; g)] = E_{(s,a,g) \\sim d\\pi}[r(s; g)] \\) to represent a scalar summary of the performance of a policy \\( \\pi \\). We denote by \\( \\pi^* \\) the optimal policy that maximizes the above objective and use \\( V^* := V^{\\pi^*} \\) to denote the optimal value function.\n\nOffline GCRL. In this paper, we focus on offline GCRL, where the agent is only provided with a previously-collected offline dataset \\( D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^N \\). Here, \\( r_i \\sim R(s_i; g_i) \\), \\( s'_i \\sim P(\\cdot | s_i, a_i) \\), and we assume that \\( g_i \\) are i.i.d. sampled from \\( p(\\cdot) \\) and it is common that data are collected by a behavior policy \\( \\mu \\) of which the discounted occupancy density is \\( d\\mu \\). Therefore, we assume that \\( (s_i, a_i, g_i) \\) are sampled i.i.d. from a distribution \\( \\mu \\) where \\( \\mu(s, a, g) = p(g)d\\mu(s, a; g) = d\\mu(s, a, g) \\). Note that we use \\( \\mu \\) to denote both the behavior policy and the dataset distribution. We also assume an additional dataset \\( D_0 = \\{(s_{0,i}, g_{0,i})\\}_{i=1}^{N_0} \\) where \\( s_{0,i} \\) are i.i.d. sampled from \\( \\rho(\\cdot) \\) and \\( g_{0,i} \\) are i.i.d. sampled from \\( p(\\cdot) \\). The goal of offline RL is to learn a policy \\( \\hat{\\pi} \\) using the offline dataset so as to minimize the sub-optimality compared to the optimal policy \\( \\pi^* \\), i.e., \\( J(\\pi^*) - J(\\hat{\\pi}) \\), with high probability.\n\nFunction approximation. To deal with huge state spaces, (general) function approximation is necessary for practical scenarios. In this paper, we assume access to two function classes: a value function class \\( V \\subseteq \\{V : S \\times G \\rightarrow [0, V_{\\text{max}}]\\} \\) that models the value function of the (regularized) optimal policies, and a policy class \\( \\Pi \\subseteq \\{\\pi : S \\times G \\rightarrow \\Delta(A)\\} \\) consisting of candidate policies. For stochastic MDP (Section 3.1.2), we also need a transition kernel class \\( P \\subseteq \\{P : S \\times A \\rightarrow \\Delta(S)\\} \\) which contains the ground-truth transition kernel. Additionally, for any function \\( f : S \\times G \\rightarrow \\mathbb{R} \\), we denote the operator \\( T : \\mathbb{R}^{S \\times G} \\rightarrow \\mathbb{R}^{S \\times A \\times G} \\) as \\( (T f)(s, a; g) = E_{s' \\sim P(\\cdot|s,a)}[f(s'; g)] \\). Also, for any function \\( V : S \\times G \\rightarrow [0, V_{\\text{max}}] \\), we define \\( AV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) \\).\n\nOffline data coverage assumption. Our algorithm works within the single-policy concentrability framework [Rashidinejad et al., 2021], which is defined as below.\n\nDefinition 1 (Single-policy concentrability for GCRL). Given a policy \\( \\pi \\), define \\( C_{\\pi} \\) to be the smallest constant that satisfies \\( d_{\\pi}(s,a,g) \\leq C_{\\pi} \\) for all \\( s \\in S \\), \\( a \\in A \\) and \\( g \\in G \\)."}]}, {"page": 5, "text": "The single-policy concentrability parameter C\u03c0 captures the coverage of policy \u03c0 in the offl                                        ine data.\nOur algorithm only requires this parameter to be small for \u03c0\u2217                          \u03b1 which is a regularized optimal policy\n(see Section 3 for formal defi            nition) and is close to some optimal policy. This assumption is similar\nto Zhan et al. [2022] and and is much weaker than the widely used all-policy concentrability that\nassumes bounded C\u03c0 for all \u03c0 (e.g., Scherrer [2014]).\n3     Algorithms\nOffl ine GCRL can be formulated as the following program:\n                                                  max       E  (s,g)\u223cd\u03c0(s,g)[r(s; g)].                                                     (3)\n                                                    \u03c0\n(3) requires solving an optimization problem over the policy space. One can also optimize over\noccupancy density d(s, a; g) s.t. d = d\u03c0 for some policy \u03c0 which is equivalent to that d satisfi                                            es\nBellman fl     ow constraint (1). Therefore, the program (3) can be represented equivalently as follows:\n             max            E (s,g)\u223cd(s,g)[r(s; g)]\n          d(s,a;g)\u22650\n         s.t.       a   d(s, a; g) = (1 \u2212        \u03b3)\u03c1(s) + \u03b3       s\u2032,a\u2032  P (s|s\u2032, a\u2032)d(s\u2032, a\u2032; g),           \u2200(s, g) \u2208     S \u00d7 G.          (4)\nLet d\u2217    denote the optimal solution of (4), then (one of) the optimal policy can be induced by \u03c0\u2217                                         =\n\u03c0d\u2217   as in (2). Under partial data coverage assumptions, (4) might fail in empirical settings by\nchoosing a highly suboptimal policy that is not well covered by the dataset with constant probability.\nSimilar to Zhan et al. [2022], Ma et al. [2022c], a regularizer is needed to ensure that the learned\npolicy is well covered by the dataset. Therefore, one should instead solve a regularized version of\n(4), which is stated as follows:\n             max            E (s,g)\u223cd(s,g)[r(s; g)] \u2212         \u03b1Df    (d\u2225\u00b5)\n          d(s,a;g)\u22650\n         s.t.       a   d(s, a; g) = (1 \u2212        \u03b3)\u03c1(s) + \u03b3       s\u2032,a\u2032  P (s|s\u2032, a\u2032)d(s\u2032, a\u2032; g),           \u2200(s, g) \u2208     S \u00d7 G,          (5)\nwhere the f-divergence is defi              ned as Df(d\u2225\u00b5) \u225c             E(s,a,g)\u223c\u00b5[f(d(s, a, g)/\u00b5(s, a, g))] for a convex\nfunction f. Throughout this paper, we choose f(x) = 1                          2(x \u2212     1)2 as in Ma et al. [2022c], where the\nf-divergence is known as \u03c72-divergence under this specifi                           c choice of f and it is shown to be more\nstable than other divergences such as KL divergence [Ma et al., 2022c]. Let d\u2217                                   \u03b1 denote the optimal\nsolution of (5), then the regularized optimal policy can be induced by \u03c0\u2217                                  \u03b1 = \u03c0d\u2217     \u03b1 as in (2). The\nfollowing single-policy concentrability assumption assumes that \u03c0\u2217                               \u03b1 is well covered by the offl             ine\ndataset.\nAssumption 1 (Single-policy concentrability for \u03c0\u2217                       \u03b1). Let d\u2217   \u03b1 be the optimal solution of (5), and let\n\u03c0\u2217\u03b1 = \u03c0d\u2217   \u03b1 as defined in (2). We assume C\u03c0\u2217               \u03b1 \u2264    C\u2217\u03b1 where C\u03c0\u2217      \u03b1 is defined in Definition 1 and C\u2217             \u03b1 > 0\nis a constant.\nUnder Assumption 1, it can be observed that the performance difference between the regularized op-\ntimal policy \u03c0\u2217     \u03b1 and the optimal policy \u03c0\u2217              is bounded by O(\u03b1). The following proposition formally\npresents this observation.\nProposition 3.1. Let d\u2217          \u03b1 be the optimal solution of (5), and let \u03c0\u2217                  \u03b1 = \u03c0d\u2217    \u03b1 as defined in (2). Then\nunder Assumption 1, it holds that J(\u03c0\u2217) \u2212                     J(\u03c0\u2217  \u03b1) \u2264    O    \u03b1(C\u2217  \u03b1)2     .\nThe proof of Proposition 3.1 is deferred to Appendix A.1. Proposition 3.1 shows that by solving the\nregularized program (5), we can obtain a near-optimal policy as long as \u03b1 is small. The algorithm\nof Ma et al. [2022c] also aims to solve (5) and they simply choose \u03b1 = 1. We show empirically\nin Section 5 that \u03b1 < 1 achieves better performance than \u03b1 = 1. In theory, we must carefully\nchoose the value of \u03b1 s.t. the suboptimality of our learned policy vanishes to 0 with a reasonable\nrate. Finally, as in Ma et al. [2022c], we convert (5) to the dual form, which is an unconstrained\nproblem and amenable to solve:\nProposition 3.2 (Dual form of (5)). The duality form of (5) is\n     min                                                                             \u2217(AV (s, a; g)) \u2265        0}\u00af g\u2217(AV (s, a; g))]\n  V (s;g)\u22650(1 \u2212      \u03b3)E(s,g)\u223c(\u03c1,p(g))[V (s; g)] + E(s,a,g)\u223c\u00b5[1{g\u2032                                                                         (6)\n                                                                      5", "md": "# Math Equations\n\nThe single-policy concentrability parameter \\(C_{\\pi}\\) captures the coverage of policy \\(\\pi\\) in the offline data. Our algorithm only requires this parameter to be small for \\(\\pi^*\\) which is a regularized optimal policy (see Section 3 for formal definition) and is close to some optimal policy. This assumption is similar to Zhan et al. [2022] and is much weaker than the widely used all-policy concentrability that assumes bounded \\(C_{\\pi}\\) for all \\(\\pi\\) (e.g., Scherrer [2014]).\n\n### Algorithms\n\nOffline GCRL can be formulated as the following program:\n\n$$\n\\max_{\\pi} \\mathbb{E}_{(s,g) \\sim d_{\\pi}(s,g)}[r(s; g)]. \\tag{3}\n$$\n(3) requires solving an optimization problem over the policy space. One can also optimize over occupancy density \\(d(s, a; g)\\) s.t. \\(d = d_{\\pi}\\) for some policy \\(\\pi\\) which is equivalent to that \\(d\\) satisfies Bellman flow constraint (1). Therefore, the program (3) can be represented equivalently as follows:\n\n$$\n\\begin{aligned}\n& \\max_{d(s,a;g) \\geq 0} \\mathbb{E}(s,g) \\sim d(s,g)[r(s; g)] \\\\\n& \\text{s.t.} \\quad \\sum_{a} d(s, a; g) = (1 - \\gamma) \\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g), \\quad \\forall (s, g) \\in S \\times G. \\tag{4}\n\\end{aligned}\n$$\n\nLet \\(d^*\\) denote the optimal solution of (4), then (one of) the optimal policy can be induced by \\(\\pi^* = \\pi d^*\\) as in (2). Under partial data coverage assumptions, (4) might fail in empirical settings by choosing a highly suboptimal policy that is not well covered by the dataset with constant probability. Similar to Zhan et al. [2022], Ma et al. [2022c], a regularizer is needed to ensure that the learned policy is well covered by the dataset. Therefore, one should instead solve a regularized version of (4), which is stated as follows:\n\n$$\n\\begin{aligned}\n& \\max_{d(s,a;g) \\geq 0} \\mathbb{E}(s,g) \\sim d(s,g)[r(s; g)] - \\alpha D_f(d\\| \\mu) \\\\\n& \\text{s.t.} \\quad \\sum_{a} d(s, a; g) = (1 - \\gamma) \\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g), \\quad \\forall (s, g) \\in S \\times G, \\tag{5}\n\\end{aligned}\n$$\n\nwhere the f-divergence is defined as \\(D_f(d\\| \\mu) = \\mathbb{E}(s,a,g) \\sim \\mu[f(d(s, a, g)/\\mu(s, a, g))]\\) for a convex function \\(f\\). Throughout this paper, we choose \\(f(x) = \\frac{1}{2}(x - 1)^2\\) as in Ma et al. [2022c], where the f-divergence is known as \\(\\chi^2\\)-divergence under this specific choice of \\(f\\) and it is shown to be more stable than other divergences such as KL divergence [Ma et al., 2022c]. Let \\(d^*_\\alpha\\) denote the optimal solution of (5), then the regularized optimal policy can be induced by \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as in (2). The following single-policy concentrability assumption assumes that \\(\\pi^*_\\alpha\\) is well covered by the offline dataset.\n\nAssumption 1 (Single-policy concentrability for \\(\\pi^*_\\alpha\\)). Let \\(d^*_\\alpha\\) be the optimal solution of (5), and let \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as defined in (2). We assume \\(C_{\\pi^*_\\alpha} \\leq C^*_\\alpha\\) where \\(C_{\\pi^*_\\alpha}\\) is defined in Definition 1 and \\(C^*_\\alpha > 0\\) is a constant.\n\nUnder Assumption 1, it can be observed that the performance difference between the regularized optimal policy \\(\\pi^*_\\alpha\\) and the optimal policy \\(\\pi^*\\) is bounded by \\(O(\\alpha)\\). The following proposition formally presents this observation.\n\nProposition 3.1. Let \\(d^*_\\alpha\\) be the optimal solution of (5), and let \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as defined in (2). Then under Assumption 1, it holds that \\(J(\\pi^*) - J(\\pi^*_\\alpha) \\leq O(\\alpha(C^*_\\alpha)^2)\\).\n\nThe proof of Proposition 3.1 is deferred to Appendix A.1. Proposition 3.1 shows that by solving the regularized program (5), we can obtain a near-optimal policy as long as \\(\\alpha\\) is small. The algorithm of Ma et al. [2022c] also aims to solve (5) and they simply choose \\(\\alpha = 1\\). We show empirically in Section 5 that \\(\\alpha < 1\\) achieves better performance than \\(\\alpha = 1\\). In theory, we must carefully choose the value of \\(\\alpha\\) s.t. the suboptimality of our learned policy vanishes to 0 with a reasonable rate. Finally, as in Ma et al. [2022c], we convert (5) to the dual form, which is an unconstrained problem and amenable to solve:\n\nProposition 3.2 (Dual form of (5)). The duality form of (5) is\n\n$$\n\\begin{aligned}\n& \\min_{V(s;g) \\geq 0} \\left[(1 - \\gamma) \\mathbb{E}(s,g) \\sim (\\rho, p(g))[V(s; g)] + \\mathbb{E}(s,a,g) \\sim \\mu[1\\{g'\\} \\geq 0}\\right].\n\\end{aligned} \\tag{6}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "The single-policy concentrability parameter \\(C_{\\pi}\\) captures the coverage of policy \\(\\pi\\) in the offline data. Our algorithm only requires this parameter to be small for \\(\\pi^*\\) which is a regularized optimal policy (see Section 3 for formal definition) and is close to some optimal policy. This assumption is similar to Zhan et al. [2022] and is much weaker than the widely used all-policy concentrability that assumes bounded \\(C_{\\pi}\\) for all \\(\\pi\\) (e.g., Scherrer [2014]).", "md": "The single-policy concentrability parameter \\(C_{\\pi}\\) captures the coverage of policy \\(\\pi\\) in the offline data. Our algorithm only requires this parameter to be small for \\(\\pi^*\\) which is a regularized optimal policy (see Section 3 for formal definition) and is close to some optimal policy. This assumption is similar to Zhan et al. [2022] and is much weaker than the widely used all-policy concentrability that assumes bounded \\(C_{\\pi}\\) for all \\(\\pi\\) (e.g., Scherrer [2014])."}, {"type": "heading", "lvl": 3, "value": "Algorithms", "md": "### Algorithms"}, {"type": "text", "value": "Offline GCRL can be formulated as the following program:\n\n$$\n\\max_{\\pi} \\mathbb{E}_{(s,g) \\sim d_{\\pi}(s,g)}[r(s; g)]. \\tag{3}\n$$\n(3) requires solving an optimization problem over the policy space. One can also optimize over occupancy density \\(d(s, a; g)\\) s.t. \\(d = d_{\\pi}\\) for some policy \\(\\pi\\) which is equivalent to that \\(d\\) satisfies Bellman flow constraint (1). Therefore, the program (3) can be represented equivalently as follows:\n\n$$\n\\begin{aligned}\n& \\max_{d(s,a;g) \\geq 0} \\mathbb{E}(s,g) \\sim d(s,g)[r(s; g)] \\\\\n& \\text{s.t.} \\quad \\sum_{a} d(s, a; g) = (1 - \\gamma) \\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g), \\quad \\forall (s, g) \\in S \\times G. \\tag{4}\n\\end{aligned}\n$$\n\nLet \\(d^*\\) denote the optimal solution of (4), then (one of) the optimal policy can be induced by \\(\\pi^* = \\pi d^*\\) as in (2). Under partial data coverage assumptions, (4) might fail in empirical settings by choosing a highly suboptimal policy that is not well covered by the dataset with constant probability. Similar to Zhan et al. [2022], Ma et al. [2022c], a regularizer is needed to ensure that the learned policy is well covered by the dataset. Therefore, one should instead solve a regularized version of (4), which is stated as follows:\n\n$$\n\\begin{aligned}\n& \\max_{d(s,a;g) \\geq 0} \\mathbb{E}(s,g) \\sim d(s,g)[r(s; g)] - \\alpha D_f(d\\| \\mu) \\\\\n& \\text{s.t.} \\quad \\sum_{a} d(s, a; g) = (1 - \\gamma) \\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g), \\quad \\forall (s, g) \\in S \\times G, \\tag{5}\n\\end{aligned}\n$$\n\nwhere the f-divergence is defined as \\(D_f(d\\| \\mu) = \\mathbb{E}(s,a,g) \\sim \\mu[f(d(s, a, g)/\\mu(s, a, g))]\\) for a convex function \\(f\\). Throughout this paper, we choose \\(f(x) = \\frac{1}{2}(x - 1)^2\\) as in Ma et al. [2022c], where the f-divergence is known as \\(\\chi^2\\)-divergence under this specific choice of \\(f\\) and it is shown to be more stable than other divergences such as KL divergence [Ma et al., 2022c]. Let \\(d^*_\\alpha\\) denote the optimal solution of (5), then the regularized optimal policy can be induced by \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as in (2). The following single-policy concentrability assumption assumes that \\(\\pi^*_\\alpha\\) is well covered by the offline dataset.\n\nAssumption 1 (Single-policy concentrability for \\(\\pi^*_\\alpha\\)). Let \\(d^*_\\alpha\\) be the optimal solution of (5), and let \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as defined in (2). We assume \\(C_{\\pi^*_\\alpha} \\leq C^*_\\alpha\\) where \\(C_{\\pi^*_\\alpha}\\) is defined in Definition 1 and \\(C^*_\\alpha > 0\\) is a constant.\n\nUnder Assumption 1, it can be observed that the performance difference between the regularized optimal policy \\(\\pi^*_\\alpha\\) and the optimal policy \\(\\pi^*\\) is bounded by \\(O(\\alpha)\\). The following proposition formally presents this observation.\n\nProposition 3.1. Let \\(d^*_\\alpha\\) be the optimal solution of (5), and let \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as defined in (2). Then under Assumption 1, it holds that \\(J(\\pi^*) - J(\\pi^*_\\alpha) \\leq O(\\alpha(C^*_\\alpha)^2)\\).\n\nThe proof of Proposition 3.1 is deferred to Appendix A.1. Proposition 3.1 shows that by solving the regularized program (5), we can obtain a near-optimal policy as long as \\(\\alpha\\) is small. The algorithm of Ma et al. [2022c] also aims to solve (5) and they simply choose \\(\\alpha = 1\\). We show empirically in Section 5 that \\(\\alpha < 1\\) achieves better performance than \\(\\alpha = 1\\). In theory, we must carefully choose the value of \\(\\alpha\\) s.t. the suboptimality of our learned policy vanishes to 0 with a reasonable rate. Finally, as in Ma et al. [2022c], we convert (5) to the dual form, which is an unconstrained problem and amenable to solve:\n\nProposition 3.2 (Dual form of (5)). The duality form of (5) is\n\n$$\n\\begin{aligned}\n& \\min_{V(s;g) \\geq 0} \\left[(1 - \\gamma) \\mathbb{E}(s,g) \\sim (\\rho, p(g))[V(s; g)] + \\mathbb{E}(s,a,g) \\sim \\mu[1\\{g'\\} \\geq 0}\\right].\n\\end{aligned} \\tag{6}\n$$", "md": "Offline GCRL can be formulated as the following program:\n\n$$\n\\max_{\\pi} \\mathbb{E}_{(s,g) \\sim d_{\\pi}(s,g)}[r(s; g)]. \\tag{3}\n$$\n(3) requires solving an optimization problem over the policy space. One can also optimize over occupancy density \\(d(s, a; g)\\) s.t. \\(d = d_{\\pi}\\) for some policy \\(\\pi\\) which is equivalent to that \\(d\\) satisfies Bellman flow constraint (1). Therefore, the program (3) can be represented equivalently as follows:\n\n$$\n\\begin{aligned}\n& \\max_{d(s,a;g) \\geq 0} \\mathbb{E}(s,g) \\sim d(s,g)[r(s; g)] \\\\\n& \\text{s.t.} \\quad \\sum_{a} d(s, a; g) = (1 - \\gamma) \\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g), \\quad \\forall (s, g) \\in S \\times G. \\tag{4}\n\\end{aligned}\n$$\n\nLet \\(d^*\\) denote the optimal solution of (4), then (one of) the optimal policy can be induced by \\(\\pi^* = \\pi d^*\\) as in (2). Under partial data coverage assumptions, (4) might fail in empirical settings by choosing a highly suboptimal policy that is not well covered by the dataset with constant probability. Similar to Zhan et al. [2022], Ma et al. [2022c], a regularizer is needed to ensure that the learned policy is well covered by the dataset. Therefore, one should instead solve a regularized version of (4), which is stated as follows:\n\n$$\n\\begin{aligned}\n& \\max_{d(s,a;g) \\geq 0} \\mathbb{E}(s,g) \\sim d(s,g)[r(s; g)] - \\alpha D_f(d\\| \\mu) \\\\\n& \\text{s.t.} \\quad \\sum_{a} d(s, a; g) = (1 - \\gamma) \\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g), \\quad \\forall (s, g) \\in S \\times G, \\tag{5}\n\\end{aligned}\n$$\n\nwhere the f-divergence is defined as \\(D_f(d\\| \\mu) = \\mathbb{E}(s,a,g) \\sim \\mu[f(d(s, a, g)/\\mu(s, a, g))]\\) for a convex function \\(f\\). Throughout this paper, we choose \\(f(x) = \\frac{1}{2}(x - 1)^2\\) as in Ma et al. [2022c], where the f-divergence is known as \\(\\chi^2\\)-divergence under this specific choice of \\(f\\) and it is shown to be more stable than other divergences such as KL divergence [Ma et al., 2022c]. Let \\(d^*_\\alpha\\) denote the optimal solution of (5), then the regularized optimal policy can be induced by \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as in (2). The following single-policy concentrability assumption assumes that \\(\\pi^*_\\alpha\\) is well covered by the offline dataset.\n\nAssumption 1 (Single-policy concentrability for \\(\\pi^*_\\alpha\\)). Let \\(d^*_\\alpha\\) be the optimal solution of (5), and let \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as defined in (2). We assume \\(C_{\\pi^*_\\alpha} \\leq C^*_\\alpha\\) where \\(C_{\\pi^*_\\alpha}\\) is defined in Definition 1 and \\(C^*_\\alpha > 0\\) is a constant.\n\nUnder Assumption 1, it can be observed that the performance difference between the regularized optimal policy \\(\\pi^*_\\alpha\\) and the optimal policy \\(\\pi^*\\) is bounded by \\(O(\\alpha)\\). The following proposition formally presents this observation.\n\nProposition 3.1. Let \\(d^*_\\alpha\\) be the optimal solution of (5), and let \\(\\pi^*_\\alpha = \\pi d^*_\\alpha\\) as defined in (2). Then under Assumption 1, it holds that \\(J(\\pi^*) - J(\\pi^*_\\alpha) \\leq O(\\alpha(C^*_\\alpha)^2)\\).\n\nThe proof of Proposition 3.1 is deferred to Appendix A.1. Proposition 3.1 shows that by solving the regularized program (5), we can obtain a near-optimal policy as long as \\(\\alpha\\) is small. The algorithm of Ma et al. [2022c] also aims to solve (5) and they simply choose \\(\\alpha = 1\\). We show empirically in Section 5 that \\(\\alpha < 1\\) achieves better performance than \\(\\alpha = 1\\). In theory, we must carefully choose the value of \\(\\alpha\\) s.t. the suboptimality of our learned policy vanishes to 0 with a reasonable rate. Finally, as in Ma et al. [2022c], we convert (5) to the dual form, which is an unconstrained problem and amenable to solve:\n\nProposition 3.2 (Dual form of (5)). The duality form of (5) is\n\n$$\n\\begin{aligned}\n& \\min_{V(s;g) \\geq 0} \\left[(1 - \\gamma) \\mathbb{E}(s,g) \\sim (\\rho, p(g))[V(s; g)] + \\mathbb{E}(s,a,g) \\sim \\mu[1\\{g'\\} \\geq 0}\\right].\n\\end{aligned} \\tag{6}\n$$"}]}, {"page": 6, "text": "where g\u2217      is the convex conjugate of g = \u03b1 \u00b7 f. Moreover, let V \u2217                     \u03b1 denote the optimal solution of (6),\nthen it holds\n                         d\u2217                                                                             \u03b1 (s; g))+                         (7)\n                          \u03b1(s, a; g) = \u00b5(s, a; g)g\u2032         \u2217(r(s; g) + \u03b3T V \u2217      \u03b1 (s, a; g) \u2212     V \u2217\nfor all (s, a, g) \u2208       S \u00d7 A \u00d7 G.\nThe proof of Proposition 3.2 is shown in Appendix A.2. According to the above proposition, one can\nfirst learn the V function according to (6), and then use the learned V function to learn the desired\npolicy by (7). We call the fi            rst step V -learning and the second step policy learning, which will be\ndiscussed in detail in Sections 3.1 and 3.2 respectively. Finally, the main algorithm, which we call\nVP-learning, is presented in Algorithm 1.\nAlgorithm 1 VP-learning\n  1: Input: Dataset D = {(si, ai, ri, s\u2032                  i, gi)}N i=1, D0 = {(s0,i, g0,i)}N0         i=1, value function class V,\n      policy class \u03a0, model class P for stochastic settings.\n  2: Obtain \u02c6    U by V-Learning (Algorithm 2 or 3 ).\n  3: Obtain \u02c6    \u03c0 by policy learning (Algorithm 4) using learned function \u02c6                         U.\n  4: Output: \u02c6      \u03c0.\n3.1     V -Learning\nDefi  ne\nL\u03b1(V ) = \u03b1((1 \u2212           \u03b3)E(s,g)\u223c(\u03c1,p(g))[V (s; g)] + E(s,a,g)\u223c\u00b5[1{g\u2032                  \u2217(AV (s, a; g)) \u2265         0}\u00afg\u2217(AV (s, a; g))]).  (8)\nThen (6) is equivalent to min             V (s;g)\u22650 L\u03b1(V ). A natural estimator of L\u03b1(V ) is\n                   1 \u2212   \u03b3   N0   \u03b1 \u00b7 V (s0,i; g0,i) + 1          N    \u03b1 \u00b7 g\u2217+(ri + \u03b3V (s\u2032        i; gi) \u2212   V (si; gi)).                  (9)\n                    N0      i=1                              N   i=1\nHowever, when the transition kernel is not deterministic, this estimator is biased and will cause an\nover-estimation issue since g\u2217(x) = \u03b1f\u2217(x/\u03b1) = \u03b1(x/\u03b1+1)2                               \u2212   \u03b1\n                                                                               2           2 contains a square operator outside\nof the Bellman operator (consider estimating (E[X])2 using 1                           N       X2 i ).\nTherefore, we use the original version of Ma et al. [2022c] for V -learning in deterministic dynamics\n(Algorithm 2 in Section 3.1.1), and a slightly modifi                      ed version in stochastic dynamics (Algorithm 3\nin Section 3.1.2). For both settings, we assume realizability of V \u2217                        \u03b1 on value function class V:\nAssumption 2 (Realizability of V \u2217              \u03b1 ). Assume V \u2217      \u03b1 \u2208   V.\n3.1.1      V -Learning in Deterministic Dynamics\nWhen the transition kernel P is deterministic, it holds that T V (s, a; g) =                                            V (s\u2032; g) where\nP  (s\u2032|s, a) = 1. In this case, the natural estimator (9) is unbiased and can be directly applied to\nthe V -learning procedure. The V -learning algorithm for deterministic dynamic settings is presented\nin Algorithm 2.\nAlgorithm 2 V -learning in deterministic dynamics\n  1: Input: Dataset D = {(si, ai, ri, s\u2032               i, gi)}N i=1, D0 = {(s0,i, g0,i)}N0        i=1, value function class V.\n  2: V -learning by solving \u02c6          V = arg minV \u2208V \u02c6         L(d)   (V ) where\n             \u02c6                          N0   \u03b1 \u00b7 V (s0,i; g0,i) + \u03b1           N   g\u2217+(ri + \u03b3V (s\u2032                                        (10)\n            L(d)   (V ) \u225c     1 \u2212   \u03b3                                                                   i; gi) \u2212    V (si; gi)).\n                               N0      i=1                              N   i=1\n  3: \u02c6U(s, a; g) \u2190        r(s; g) + \u03b3 \u02c6   V (s\u2032; g) \u2212     V\u02c6(s; g) + \u03b1\n  4: Output: \u02c6      V , \u02c6\n                        U.\n                                                                       6", "md": "# Document\n\nwhere $$g^*$$ is the convex conjugate of $$g = \\alpha \\cdot f$$. Moreover, let $$V^*_{\\alpha}$$ denote the optimal solution of (6), then it holds\n\n$$\\alpha(s, a; g) = \\mu(s, a; g)g'^* (r(s; g) + \\gamma T V^*_{\\alpha}(s, a; g) - V^*_{\\alpha}$$ for all $$(s, a, g) \\in S \\times A \\times G$$.\n\nThe proof of Proposition 3.2 is shown in Appendix A.2. According to the above proposition, one can first learn the $$V$$ function according to (6), and then use the learned $$V$$ function to learn the desired policy by (7). We call the first step $$V$$-learning and the second step policy learning, which will be discussed in detail in Sections 3.1 and 3.2 respectively. Finally, the main algorithm, which we call VP-learning, is presented in Algorithm 1.\n\nAlgorithm 1 VP-learning\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{N i=1}$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{N_0 i=1}$, value function class $V$, policy class $\\Pi$, model class $P$ for stochastic settings.\n2. Obtain $\\hat{U}$ by $V$-Learning (Algorithm 2 or 3).\n3. Obtain $\\hat{\\pi}$ by policy learning (Algorithm 4) using learned function $\\hat{U}$.\n4. Output: $\\hat{\\pi}$.\n\n### 3.1 $$V$$-Learning\n\nDefine\n\n$$L_{\\alpha}(V) = \\alpha((1 - \\gamma)E(s,g) \\sim (\\rho, p(g))[V(s; g)] + E(s,a,g) \\sim \\mu[1\\{g'^*(AV(s, a; g)) \\geq 0\\}\\bar{g}^*(AV(s, a; g))]).$$ (8)\n\nThen (6) is equivalent to $$\\min_{V(s;g) \\geq 0} L_{\\alpha}(V)$$. A natural estimator of $$L_{\\alpha}(V)$$ is\n\n$$\\frac{1 - \\gamma}{N_0} \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\frac{1}{N} \\alpha \\cdot g^* + (r_i + \\gamma V(s'_i; g_i) - V(s_i; g_i)).$$ (9)\n\nHowever, when the transition kernel is not deterministic, this estimator is biased and will cause an over-estimation issue since $$g^*(x) = \\alpha f^*(x/\\alpha) = \\alpha(x/\\alpha+1)^2 - \\alpha^2$$ contains a square operator outside of the Bellman operator (consider estimating $$(E[X])^2$$ using $$\\frac{1}{N} X^2_i$$).\n\nTherefore, we use the original version of Ma et al. [2022c] for $$V$$-learning in deterministic dynamics (Algorithm 2 in Section 3.1.1), and a slightly modified version in stochastic dynamics (Algorithm 3 in Section 3.1.2). For both settings, we assume realizability of $$V^*_{\\alpha}$$ on value function class $$V$$:\n\nAssumption 2 (Realizability of $$V^*_{\\alpha}$$). Assume $$V^*_{\\alpha} \\in V$$.\n\n#### 3.1.1 $$V$$-Learning in Deterministic Dynamics\n\nWhen the transition kernel $$P$$ is deterministic, it holds that $$T V(s, a; g) = V(s'; g)$$ where $$P(s' | s, a) = 1$$. In this case, the natural estimator (9) is unbiased and can be directly applied to the $$V$$-learning procedure. The $$V$$-learning algorithm for deterministic dynamic settings is presented in Algorithm 2.\n\nAlgorithm 2 $$V$$-learning in deterministic dynamics\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{N i=1}$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{N_0 i=1}$, value function class $V$.\n2. $V$-learning by solving $\\hat{V} = \\arg \\min_{V \\in V} \\hat{L}(d) (V)$ where\n3. $\\hat{L}(d) (V) \\triangleq \\frac{1 - \\gamma}{N_0} \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\alpha \\frac{1}{N} g^+(r_i + \\gamma V(s'_i; g_i) - V(s_i; g_i)).$ (10)\n\n$\\hat{U}(s, a; g) \\leftarrow r(s; g) + \\gamma \\hat{V}(s'; g) - V\\hat{(s; g) + \\alpha$\n4. Output: $\\hat{V}$, $\\hat{U}$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "where $$g^*$$ is the convex conjugate of $$g = \\alpha \\cdot f$$. Moreover, let $$V^*_{\\alpha}$$ denote the optimal solution of (6), then it holds\n\n$$\\alpha(s, a; g) = \\mu(s, a; g)g'^* (r(s; g) + \\gamma T V^*_{\\alpha}(s, a; g) - V^*_{\\alpha}$$ for all $$(s, a, g) \\in S \\times A \\times G$$.\n\nThe proof of Proposition 3.2 is shown in Appendix A.2. According to the above proposition, one can first learn the $$V$$ function according to (6), and then use the learned $$V$$ function to learn the desired policy by (7). We call the first step $$V$$-learning and the second step policy learning, which will be discussed in detail in Sections 3.1 and 3.2 respectively. Finally, the main algorithm, which we call VP-learning, is presented in Algorithm 1.\n\nAlgorithm 1 VP-learning\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{N i=1}$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{N_0 i=1}$, value function class $V$, policy class $\\Pi$, model class $P$ for stochastic settings.\n2. Obtain $\\hat{U}$ by $V$-Learning (Algorithm 2 or 3).\n3. Obtain $\\hat{\\pi}$ by policy learning (Algorithm 4) using learned function $\\hat{U}$.\n4. Output: $\\hat{\\pi}$.", "md": "where $$g^*$$ is the convex conjugate of $$g = \\alpha \\cdot f$$. Moreover, let $$V^*_{\\alpha}$$ denote the optimal solution of (6), then it holds\n\n$$\\alpha(s, a; g) = \\mu(s, a; g)g'^* (r(s; g) + \\gamma T V^*_{\\alpha}(s, a; g) - V^*_{\\alpha}$$ for all $$(s, a, g) \\in S \\times A \\times G$$.\n\nThe proof of Proposition 3.2 is shown in Appendix A.2. According to the above proposition, one can first learn the $$V$$ function according to (6), and then use the learned $$V$$ function to learn the desired policy by (7). We call the first step $$V$$-learning and the second step policy learning, which will be discussed in detail in Sections 3.1 and 3.2 respectively. Finally, the main algorithm, which we call VP-learning, is presented in Algorithm 1.\n\nAlgorithm 1 VP-learning\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{N i=1}$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{N_0 i=1}$, value function class $V$, policy class $\\Pi$, model class $P$ for stochastic settings.\n2. Obtain $\\hat{U}$ by $V$-Learning (Algorithm 2 or 3).\n3. Obtain $\\hat{\\pi}$ by policy learning (Algorithm 4) using learned function $\\hat{U}$.\n4. Output: $\\hat{\\pi}$."}, {"type": "heading", "lvl": 3, "value": "3.1 $$V$$-Learning", "md": "### 3.1 $$V$$-Learning"}, {"type": "text", "value": "Define\n\n$$L_{\\alpha}(V) = \\alpha((1 - \\gamma)E(s,g) \\sim (\\rho, p(g))[V(s; g)] + E(s,a,g) \\sim \\mu[1\\{g'^*(AV(s, a; g)) \\geq 0\\}\\bar{g}^*(AV(s, a; g))]).$$ (8)\n\nThen (6) is equivalent to $$\\min_{V(s;g) \\geq 0} L_{\\alpha}(V)$$. A natural estimator of $$L_{\\alpha}(V)$$ is\n\n$$\\frac{1 - \\gamma}{N_0} \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\frac{1}{N} \\alpha \\cdot g^* + (r_i + \\gamma V(s'_i; g_i) - V(s_i; g_i)).$$ (9)\n\nHowever, when the transition kernel is not deterministic, this estimator is biased and will cause an over-estimation issue since $$g^*(x) = \\alpha f^*(x/\\alpha) = \\alpha(x/\\alpha+1)^2 - \\alpha^2$$ contains a square operator outside of the Bellman operator (consider estimating $$(E[X])^2$$ using $$\\frac{1}{N} X^2_i$$).\n\nTherefore, we use the original version of Ma et al. [2022c] for $$V$$-learning in deterministic dynamics (Algorithm 2 in Section 3.1.1), and a slightly modified version in stochastic dynamics (Algorithm 3 in Section 3.1.2). For both settings, we assume realizability of $$V^*_{\\alpha}$$ on value function class $$V$$:\n\nAssumption 2 (Realizability of $$V^*_{\\alpha}$$). Assume $$V^*_{\\alpha} \\in V$$.", "md": "Define\n\n$$L_{\\alpha}(V) = \\alpha((1 - \\gamma)E(s,g) \\sim (\\rho, p(g))[V(s; g)] + E(s,a,g) \\sim \\mu[1\\{g'^*(AV(s, a; g)) \\geq 0\\}\\bar{g}^*(AV(s, a; g))]).$$ (8)\n\nThen (6) is equivalent to $$\\min_{V(s;g) \\geq 0} L_{\\alpha}(V)$$. A natural estimator of $$L_{\\alpha}(V)$$ is\n\n$$\\frac{1 - \\gamma}{N_0} \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\frac{1}{N} \\alpha \\cdot g^* + (r_i + \\gamma V(s'_i; g_i) - V(s_i; g_i)).$$ (9)\n\nHowever, when the transition kernel is not deterministic, this estimator is biased and will cause an over-estimation issue since $$g^*(x) = \\alpha f^*(x/\\alpha) = \\alpha(x/\\alpha+1)^2 - \\alpha^2$$ contains a square operator outside of the Bellman operator (consider estimating $$(E[X])^2$$ using $$\\frac{1}{N} X^2_i$$).\n\nTherefore, we use the original version of Ma et al. [2022c] for $$V$$-learning in deterministic dynamics (Algorithm 2 in Section 3.1.1), and a slightly modified version in stochastic dynamics (Algorithm 3 in Section 3.1.2). For both settings, we assume realizability of $$V^*_{\\alpha}$$ on value function class $$V$$:\n\nAssumption 2 (Realizability of $$V^*_{\\alpha}$$). Assume $$V^*_{\\alpha} \\in V$$."}, {"type": "heading", "lvl": 4, "value": "3.1.1 $$V$$-Learning in Deterministic Dynamics", "md": "#### 3.1.1 $$V$$-Learning in Deterministic Dynamics"}, {"type": "text", "value": "When the transition kernel $$P$$ is deterministic, it holds that $$T V(s, a; g) = V(s'; g)$$ where $$P(s' | s, a) = 1$$. In this case, the natural estimator (9) is unbiased and can be directly applied to the $$V$$-learning procedure. The $$V$$-learning algorithm for deterministic dynamic settings is presented in Algorithm 2.\n\nAlgorithm 2 $$V$$-learning in deterministic dynamics\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{N i=1}$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{N_0 i=1}$, value function class $V$.\n2. $V$-learning by solving $\\hat{V} = \\arg \\min_{V \\in V} \\hat{L}(d) (V)$ where\n3. $\\hat{L}(d) (V) \\triangleq \\frac{1 - \\gamma}{N_0} \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\alpha \\frac{1}{N} g^+(r_i + \\gamma V(s'_i; g_i) - V(s_i; g_i)).$ (10)\n\n$\\hat{U}(s, a; g) \\leftarrow r(s; g) + \\gamma \\hat{V}(s'; g) - V\\hat{(s; g) + \\alpha$\n4. Output: $\\hat{V}$, $\\hat{U}$.", "md": "When the transition kernel $$P$$ is deterministic, it holds that $$T V(s, a; g) = V(s'; g)$$ where $$P(s' | s, a) = 1$$. In this case, the natural estimator (9) is unbiased and can be directly applied to the $$V$$-learning procedure. The $$V$$-learning algorithm for deterministic dynamic settings is presented in Algorithm 2.\n\nAlgorithm 2 $$V$$-learning in deterministic dynamics\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{N i=1}$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{N_0 i=1}$, value function class $V$.\n2. $V$-learning by solving $\\hat{V} = \\arg \\min_{V \\in V} \\hat{L}(d) (V)$ where\n3. $\\hat{L}(d) (V) \\triangleq \\frac{1 - \\gamma}{N_0} \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\alpha \\frac{1}{N} g^+(r_i + \\gamma V(s'_i; g_i) - V(s_i; g_i)).$ (10)\n\n$\\hat{U}(s, a; g) \\leftarrow r(s; g) + \\gamma \\hat{V}(s'; g) - V\\hat{(s; g) + \\alpha$\n4. Output: $\\hat{V}$, $\\hat{U}$."}]}, {"page": 7, "text": " Now for any V , we defi           ne UV (s, a; g) = r(s; g) + \u03b3T V (s, a; g) \u2212                   V (s; g) + \u03b1 = AV (s, a; g) + \u03b1\n which can be interpreted as the advantage function of V with an \u03b1-shift. We also denote U \u2217                                       \u03b1 = UV \u2217   \u03b1 .\n Note that besides the learned \u02c6            V function, Algorithm 2 also outputs a \u02c6                   U function. By (7), one can\n observe that in policy learning, what we indeed need is \u02c6                        U instead of \u02c6     V , and thus in the V -learning\n procedure we also compute this \u02c6              U function in preparation for policy learning.\n One may challenge that \u02c6           U cannot be computed for all (s, a; g) since we do not have knowledge of\n all r(s; g). However, we only need the value of \u02c6                     U(si, ai; gi) for (si, ai; gi) contained in the offl                 ine\n dataset, where ri is also contained. Therefore, we can evaluate the value of \u02c6                                U at all (s, a; g) tuples\n requested in the policy learning algorithm.\n Note that Algorithm 2 is equivalent to the fi                rst step of Ma et al. [2022c] except for the choice of \u03b1 and\n a clip for the value of g\u2217. However, the above V -learning, as well as the original GoFAR algorithm,\n might suffer the over-estimation issue under stochastic dynamics, and we present algorithms suitable\n for stochastic dynamics in Section 3.1.2.\n 3.1.2     V -Learning in Stochastic Dynamics\nWhen the transition kernel is stochastic, one cannot directly use V (s\u2032; g) to estimate T V (s, a; g).\n Since T V (s, a; g) = Es\u2032\u223cP (\u00b7|s,a)[V (s\u2032; g)], a natural idea is to learn the ground-truth transition\n kernel P \u22c61 fi    rst, and then use the learned transition kernel \u02c6                P to estimate T V (s, a; g): \u02c6         T V (s, a; g) =\n E s\u2032\u223c  \u02c6\n        P (\u00b7|s,a)[V (s\u2032; g)]. This is achievable under the following realizability assumption.\n Assumption 3 (Realizability of the ground-truth transition model). Assume the ground-truth transi-\n tion kernel P \u22c6       \u2208  P.\n The algorithm for stochastic dynamic settings is presented in Algorithm 3, where we fi                                        rst learn the\n transition kernel \u02c6      P , and then plug in the learned transition kernel to learn V function. Similar to\n Algorithm 2, we also compute \u02c6              U in V -learning procedure.\n Algorithm 3 V -learning in stochastic dynamics\n  1: Input: Dataset D = {(si, ai, ri, s\u2032                   i, gi)}N i=1, D0 = {(s0,i, g0,i)}N0         i=1, value function class V,\n       model class P.\n  2: Estimate the transition kernel via maximum likelihood estimation (MLE)\n                                                                         N\n                                                   P\u02c6= max P \u2208P    N1  i=1   log P   (s\u2032i|si, ai)                                         (11)\n  3: V -learning using the learned transition kernel: \u02c6                    V = arg minV \u2208V \u02c6         L(s)   (V ) with\n          \u02c6                         N0                                     N\n         L(s)  (V ) \u225c     1 \u2212    \u03b3       \u03b1 \u00b7 V (s0,i; g0,i) + \u03b1                g\u2217+(ri + \u03b3 \u02c6    T V (si, ai; gi) \u2212        V (si; gi)). (12)\n                            N0      i=1                              N    i=1\n       where \u02c6  T V (s, a, ; g) = Es\u2032\u223c         P\u02c6(\u00b7|s,a)[V (s\u2032; g)].\n  4: \u02c6 U(s, a; g) \u2190        r(s; g) + \u03b3 \u02c6   T \u02c6V (s, a; g) \u2212      \u02c6\n                                                                 V (s; g) + \u03b1\n  5: Output: \u02c6      V , \u02c6U.\n 3.2     Policy Learning\nWe now derive policy learning, the second step of the VP-learning algorithm. Note that \u03c0\u2217                                                \u03b1 =\n arg max\u03c0 E(s,a,g)\u223cd\u2217         \u03b1[log \u03c0(a|s, g)]. By (7), we also have\n      d\u2217\u03b1(s, a; g) = \u00b5(s, a; g)g\u2032        \u2217(r(s; g) + \u03b3T V \u2217       \u03b1 (s, a; g) \u2212     V \u2217                                   \u03b1(s, a; g)+     .\n                                                                                      \u03b1 (s; g))+ = \u00b5(s, a; g)U \u2217               \u03b1\n     1For notation convenience, in stochastic settings, we use P \u22c6                  to denote the ground-truth transition kernel.\n                                                                       7", "md": "Now for any V, we define $$UV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) + \\alpha = AV(s, a; g) + \\alpha$$ which can be interpreted as the advantage function of V with an \u03b1-shift. We also denote $$U^*_\\alpha = UV^*_\\alpha$$.\n\nNote that besides the learned $\\hat{V}$ function, Algorithm 2 also outputs a $\\hat{U}$ function. By (7), one can observe that in policy learning, what we indeed need is $\\hat{U}$ instead of $\\hat{V}$, and thus in the V-learning procedure we also compute this $\\hat{U}$ function in preparation for policy learning.\n\nOne may challenge that $\\hat{U}$ cannot be computed for all $(s, a; g)$ since we do not have knowledge of all $r(s; g)$. However, we only need the value of $\\hat{U}(s_i, a_i; g_i)$ for $(s_i, a_i; g_i)$ contained in the offline dataset, where $r_i$ is also contained. Therefore, we can evaluate the value of $\\hat{U}$ at all $(s, a; g)$ tuples requested in the policy learning algorithm.\n\nNote that Algorithm 2 is equivalent to the first step of Ma et al. [2022c] except for the choice of \u03b1 and a clip for the value of $g^*$. However, the above V-learning, as well as the original GoFAR algorithm, might suffer the over-estimation issue under stochastic dynamics, and we present algorithms suitable for stochastic dynamics in Section 3.1.2.\n\n### 3.1.2 V-Learning in Stochastic Dynamics\n\nWhen the transition kernel is stochastic, one cannot directly use $V(s'; g)$ to estimate $T V(s, a; g)$. Since $T V(s, a; g) = E_{s'\\sim P(\\cdot|s,a)}[V(s'; g)]$, a natural idea is to learn the ground-truth transition kernel $P^*$ first, and then use the learned transition kernel $\\hat{P}$ to estimate $T V(s, a; g)$: $\\hat{T} V(s, a; g) = E_{s'\\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]$. This is achievable under the following realizability assumption.\n\nAssumption 3 (Realizability of the ground-truth transition model). Assume the ground-truth transition kernel $P^* \\in \\mathcal{P}$.\n\nThe algorithm for stochastic dynamic settings is presented in Algorithm 3, where we first learn the transition kernel $\\hat{P}$, and then plug in the learned transition kernel to learn V function. Similar to Algorithm 2, we also compute $\\hat{U}$ in V-learning procedure.\n\nAlgorithm 3 V-Learning in Stochastic Dynamics\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^N$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{i=1}^{N_0}$, value function class $V$, model class $P$.\n2. Estimate the transition kernel via maximum likelihood estimation (MLE): $\\hat{P} = \\max_{P \\in \\mathcal{P}} \\sum_{i=1}^{N_1} \\log P(s'_i|s_i, a_i)$\n3. V-learning using the learned transition kernel: $\\hat{V} = \\arg \\min_{V \\in V} \\hat{L}(s)(V)$ with $\\hat{L}(s)(V) \\triangleq 1 - \\gamma \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\alpha g^* + (r_i + \\gamma \\hat{T} V(s_i, a_i; g_i) - V(s_i; g_i))$, where $\\hat{T} V(s, a, ; g) = E_{s'\\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]$.\n4. $\\hat{U}(s, a; g) \\leftarrow r(s; g) + \\gamma \\hat{T} \\hat{V}(s, a; g) - \\hat{V}(s; g) + \\alpha$\n5. Output: $\\hat{V}$, $\\hat{U}$.\n\n### 3.2 Policy Learning\n\nWe now derive policy learning, the second step of the VP-learning algorithm. Note that $\\pi^*_\\alpha = \\arg \\max_{\\pi} E(s,a,g) \\sim d^*_\\alpha [\\log \\pi(a|s, g)]$. By (7), we also have\n\n$d^*_\\alpha(s, a; g) = \\mu(s, a; g)g^* (r(s; g) + \\gamma T V^*_\\alpha(s, a; g) - V^*_\\alpha(s, a; g)+ \\alpha = \\mu(s, a; g)U^*_\\alpha$\n\n1For notation convenience, in stochastic settings, we use $P^*$ to denote the ground-truth transition kernel.", "images": [], "items": [{"type": "text", "value": "Now for any V, we define $$UV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) + \\alpha = AV(s, a; g) + \\alpha$$ which can be interpreted as the advantage function of V with an \u03b1-shift. We also denote $$U^*_\\alpha = UV^*_\\alpha$$.\n\nNote that besides the learned $\\hat{V}$ function, Algorithm 2 also outputs a $\\hat{U}$ function. By (7), one can observe that in policy learning, what we indeed need is $\\hat{U}$ instead of $\\hat{V}$, and thus in the V-learning procedure we also compute this $\\hat{U}$ function in preparation for policy learning.\n\nOne may challenge that $\\hat{U}$ cannot be computed for all $(s, a; g)$ since we do not have knowledge of all $r(s; g)$. However, we only need the value of $\\hat{U}(s_i, a_i; g_i)$ for $(s_i, a_i; g_i)$ contained in the offline dataset, where $r_i$ is also contained. Therefore, we can evaluate the value of $\\hat{U}$ at all $(s, a; g)$ tuples requested in the policy learning algorithm.\n\nNote that Algorithm 2 is equivalent to the first step of Ma et al. [2022c] except for the choice of \u03b1 and a clip for the value of $g^*$. However, the above V-learning, as well as the original GoFAR algorithm, might suffer the over-estimation issue under stochastic dynamics, and we present algorithms suitable for stochastic dynamics in Section 3.1.2.", "md": "Now for any V, we define $$UV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) + \\alpha = AV(s, a; g) + \\alpha$$ which can be interpreted as the advantage function of V with an \u03b1-shift. We also denote $$U^*_\\alpha = UV^*_\\alpha$$.\n\nNote that besides the learned $\\hat{V}$ function, Algorithm 2 also outputs a $\\hat{U}$ function. By (7), one can observe that in policy learning, what we indeed need is $\\hat{U}$ instead of $\\hat{V}$, and thus in the V-learning procedure we also compute this $\\hat{U}$ function in preparation for policy learning.\n\nOne may challenge that $\\hat{U}$ cannot be computed for all $(s, a; g)$ since we do not have knowledge of all $r(s; g)$. However, we only need the value of $\\hat{U}(s_i, a_i; g_i)$ for $(s_i, a_i; g_i)$ contained in the offline dataset, where $r_i$ is also contained. Therefore, we can evaluate the value of $\\hat{U}$ at all $(s, a; g)$ tuples requested in the policy learning algorithm.\n\nNote that Algorithm 2 is equivalent to the first step of Ma et al. [2022c] except for the choice of \u03b1 and a clip for the value of $g^*$. However, the above V-learning, as well as the original GoFAR algorithm, might suffer the over-estimation issue under stochastic dynamics, and we present algorithms suitable for stochastic dynamics in Section 3.1.2."}, {"type": "heading", "lvl": 3, "value": "3.1.2 V-Learning in Stochastic Dynamics", "md": "### 3.1.2 V-Learning in Stochastic Dynamics"}, {"type": "text", "value": "When the transition kernel is stochastic, one cannot directly use $V(s'; g)$ to estimate $T V(s, a; g)$. Since $T V(s, a; g) = E_{s'\\sim P(\\cdot|s,a)}[V(s'; g)]$, a natural idea is to learn the ground-truth transition kernel $P^*$ first, and then use the learned transition kernel $\\hat{P}$ to estimate $T V(s, a; g)$: $\\hat{T} V(s, a; g) = E_{s'\\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]$. This is achievable under the following realizability assumption.\n\nAssumption 3 (Realizability of the ground-truth transition model). Assume the ground-truth transition kernel $P^* \\in \\mathcal{P}$.\n\nThe algorithm for stochastic dynamic settings is presented in Algorithm 3, where we first learn the transition kernel $\\hat{P}$, and then plug in the learned transition kernel to learn V function. Similar to Algorithm 2, we also compute $\\hat{U}$ in V-learning procedure.\n\nAlgorithm 3 V-Learning in Stochastic Dynamics\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^N$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{i=1}^{N_0}$, value function class $V$, model class $P$.\n2. Estimate the transition kernel via maximum likelihood estimation (MLE): $\\hat{P} = \\max_{P \\in \\mathcal{P}} \\sum_{i=1}^{N_1} \\log P(s'_i|s_i, a_i)$\n3. V-learning using the learned transition kernel: $\\hat{V} = \\arg \\min_{V \\in V} \\hat{L}(s)(V)$ with $\\hat{L}(s)(V) \\triangleq 1 - \\gamma \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\alpha g^* + (r_i + \\gamma \\hat{T} V(s_i, a_i; g_i) - V(s_i; g_i))$, where $\\hat{T} V(s, a, ; g) = E_{s'\\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]$.\n4. $\\hat{U}(s, a; g) \\leftarrow r(s; g) + \\gamma \\hat{T} \\hat{V}(s, a; g) - \\hat{V}(s; g) + \\alpha$\n5. Output: $\\hat{V}$, $\\hat{U}$.", "md": "When the transition kernel is stochastic, one cannot directly use $V(s'; g)$ to estimate $T V(s, a; g)$. Since $T V(s, a; g) = E_{s'\\sim P(\\cdot|s,a)}[V(s'; g)]$, a natural idea is to learn the ground-truth transition kernel $P^*$ first, and then use the learned transition kernel $\\hat{P}$ to estimate $T V(s, a; g)$: $\\hat{T} V(s, a; g) = E_{s'\\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]$. This is achievable under the following realizability assumption.\n\nAssumption 3 (Realizability of the ground-truth transition model). Assume the ground-truth transition kernel $P^* \\in \\mathcal{P}$.\n\nThe algorithm for stochastic dynamic settings is presented in Algorithm 3, where we first learn the transition kernel $\\hat{P}$, and then plug in the learned transition kernel to learn V function. Similar to Algorithm 2, we also compute $\\hat{U}$ in V-learning procedure.\n\nAlgorithm 3 V-Learning in Stochastic Dynamics\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^N$, $D_0 = \\{(s_{0,i}, g_{0,i})\\}_{i=1}^{N_0}$, value function class $V$, model class $P$.\n2. Estimate the transition kernel via maximum likelihood estimation (MLE): $\\hat{P} = \\max_{P \\in \\mathcal{P}} \\sum_{i=1}^{N_1} \\log P(s'_i|s_i, a_i)$\n3. V-learning using the learned transition kernel: $\\hat{V} = \\arg \\min_{V \\in V} \\hat{L}(s)(V)$ with $\\hat{L}(s)(V) \\triangleq 1 - \\gamma \\alpha \\cdot V(s_{0,i}; g_{0,i}) + \\alpha g^* + (r_i + \\gamma \\hat{T} V(s_i, a_i; g_i) - V(s_i; g_i))$, where $\\hat{T} V(s, a, ; g) = E_{s'\\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]$.\n4. $\\hat{U}(s, a; g) \\leftarrow r(s; g) + \\gamma \\hat{T} \\hat{V}(s, a; g) - \\hat{V}(s; g) + \\alpha$\n5. Output: $\\hat{V}$, $\\hat{U}$."}, {"type": "heading", "lvl": 3, "value": "3.2 Policy Learning", "md": "### 3.2 Policy Learning"}, {"type": "text", "value": "We now derive policy learning, the second step of the VP-learning algorithm. Note that $\\pi^*_\\alpha = \\arg \\max_{\\pi} E(s,a,g) \\sim d^*_\\alpha [\\log \\pi(a|s, g)]$. By (7), we also have\n\n$d^*_\\alpha(s, a; g) = \\mu(s, a; g)g^* (r(s; g) + \\gamma T V^*_\\alpha(s, a; g) - V^*_\\alpha(s, a; g)+ \\alpha = \\mu(s, a; g)U^*_\\alpha$\n\n1For notation convenience, in stochastic settings, we use $P^*$ to denote the ground-truth transition kernel.", "md": "We now derive policy learning, the second step of the VP-learning algorithm. Note that $\\pi^*_\\alpha = \\arg \\max_{\\pi} E(s,a,g) \\sim d^*_\\alpha [\\log \\pi(a|s, g)]$. By (7), we also have\n\n$d^*_\\alpha(s, a; g) = \\mu(s, a; g)g^* (r(s; g) + \\gamma T V^*_\\alpha(s, a; g) - V^*_\\alpha(s, a; g)+ \\alpha = \\mu(s, a; g)U^*_\\alpha$\n\n1For notation convenience, in stochastic settings, we use $P^*$ to denote the ground-truth transition kernel."}]}, {"page": 8, "text": "Therefore, \u03c0\u2217        \u03b1 = arg max\u03c0 LMLE                   (\u03c0) where LMLE(\u03c0) \u225c                        E(s,a,g)\u223c\u00b5        U\u2217  \u03b1(s,a;g)+     log \u03c0(a|s, g)          .\n                                                   \u03b1                           \u03b1                                              \u03b1\nSince we already learned \u02c6               U, which is close to UN\u2217    \u02c6       \u03b1, we can use the following estimator for LMLE                       \u03b1     (\u03c0):\n                                      \u02c6                             U(si, ai; gi)+\n                                      LMLE    (\u03c0) = 1    N    i=1             \u03b1             log \u03c0(a     i|si, gi).                                      (13)\nAlgorithm 4 Policy learning\n  1: Input: Dataset D = {(si, ai, ri, s\u2032                     i, gi)}N  i=1, policy class \u03a0, \u02c6          U learned by Algorithm 2 or 3.\n  2: Policy learning by:\n                                                                              N     \u02c6\n                              \u02c6                     \u02c6                              U(si, ai; gi)+                                                       (14)\n  3: Output: \u02c6        \u03c0.      \u03c0 = arg max   \u03c0\u2208\u03a0    LMLE(\u03c0) \u225c           N1    i=1             \u03b1             log \u03c0(a     i|si, gi).\nThe policy learning algorithm is presented in Algorithm 4, which can be viewed as a weighted\nmaximum likelihood estimation (MLE) procedure. Finally, we make the following two assumptions\non the policy class \u03a0.\nAssumption 4 (Single-policy realizability). Assume \u03c0\u2217                                  \u03b1 \u2208    \u03a0.\nAssumption 5 (Lower bound of policy). For any policy \u03c0 \u2208                                           \u03a0, we assume that \u03c0(a|s, g) \u2265                     \u03c4 > 0\nfor any (s, a, g) \u2208           S \u00d7 A \u00d7 G.\nRemark 1. One may consider Assumption 5 strong if \u03c4 is a constant independent of \u03b1 or N. How-\never, we allow that \u03c4 depends on \u03b1. In that case, \u03c4 can be extremely small, and any policy mixed\nwith a uniform policy with a tiny probability satisfies this assumption. Therefore, Assumption 5 is\nmild.\n4      Theoretical Guarantees\nIn this section, we provide theoretical guarantees of our main algorithm (Algorithm 1). We fi                                                              rst\nshow the results for V -Learning and policy learning in Section 4.1 and Section 4.2 respectively and\nthen combine them to obtain our main theorem in Section 4.3.\n4.1      Analysis of V -Learning\nWe mainly focus on V -learning in deterministic dynamics in this section. The analysis for stochastic\ndynamics is similar and presented in Appendix B.2.\nAs discussed in Section 3.1.1, although the fi                          rst step of the algorithm is called V -learning, the main\ngoal of this step is to estimate U \u2217                \u03b1 = UV \u2217     \u03b1 accurately. The following lemma provides a theoretical\nguarantee that the output of V -learning algorithm \u02c6                            U is a good estimator of U \u2217               \u03b1 in positive parts:\nLemma 1 (Closeness of \u02c6            \u221a    U+ and U \u2217     \u03b1+). Under Assumptions 1 and 2, with probability at least 1 \u2212                         log(|V|/\u03b4).     \u03b4,\n\u2225  \u02c6\n  U+ \u2212      U \u2217                         \u01ebstat   , where \u02c6    U is the output of Algorithm 2 and \u01ebstat \u224d                         V 2\n               \u03b1+\u22252,\u00b5\u2264         O                                                                                                  max              N\nProof sketch. By standard concentration argument, it can be shown that the empirical estimator \u02c6                                                        L(d)\nin Algorithm 2 (which is unbiased in deterministic dynamics) concentrates well on L\u03b1 for all V \u2208                                                            V\n(Lemma 3). Therefore, by realizability of V \u2217                         \u03b1 , the value of L\u03b1 at V \u2217           \u03b1 and the learned V -function \u02c6                  V\nare close (Lemma 4). Finally, one can observe that L\u03b1 is \u201csemi-strongly\u201d convex w.r.t. UV + in\n\u2225\u00b7\u22252,\u00b5-norm, and thus we can show that \u02c6                        U+ and U \u2217      \u03b1+ are also close.\nThe complete proof of Lemma 1 is deferred to Appendix B.1. In Appendix B.2, we also show the\ncounterpart of Lemma 1 for stochastic dynamic settings.\n                                                                              8", "md": "# Math Equations and Text\n\nTherefore, $$\\pi^*_{\\alpha} = \\arg \\max_{\\pi} L_{MLE}(\\pi)$$ where $$L_{MLE}(\\pi) \\triangleq E(s,a,g) \\sim \\mu [U^*_{\\alpha}(s,a;g) + \\log \\pi(a|s,g)].$$\n\nSince we already learned $$\\hat{U}$$, which is close to $$U^*_{N\\hat{\\alpha}}$$, we can use the following estimator for $$L_{MLE}(\\pi):$$\n\n$$\nL_{MLE}(\\pi) = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{U}(s_i, a_i; g_i) + \\log \\pi(a_i|s_i, g_i). \\quad (13)\n$$\nAlgorithm 4 Policy learning\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^{N}$, policy class $\\Pi$, $\\hat{U}$ learned by Algorithm 2 or 3.\n2. Policy learning by:\n3. $\n\\hat{U}(s_i, a_i; g_i) + \\quad (14)\n$\nOutput: $\\hat{\\pi}$. $\\pi = \\arg \\max_{\\pi \\in \\Pi} L_{MLE}(\\pi) \\triangleq \\frac{1}{N} \\sum_{i=1}^{N} \\alpha \\log \\pi(a_i|s_i, g_i).$\n\nThe policy learning algorithm is presented in Algorithm 4, which can be viewed as a weighted maximum likelihood estimation (MLE) procedure. Finally, we make the following two assumptions on the policy class $$\\Pi$$.\n\nAssumption 4 (Single-policy realizability). Assume $$\\pi^*_{\\alpha} \\in \\Pi$$.\n\nAssumption 5 (Lower bound of policy). For any policy $$\\pi \\in \\Pi$$, we assume that $$\\pi(a|s,g) \\geq \\tau > 0$$ for any $$(s, a, g) \\in S \\times A \\times G$$.\n\nRemark 1. One may consider Assumption 5 strong if $$\\tau$$ is a constant independent of $$\\alpha$$ or $$N$$. However, we allow that $$\\tau$$ depends on $$\\alpha$$. In that case, $$\\tau$$ can be extremely small, and any policy mixed with a uniform policy with a tiny probability satisfies this assumption. Therefore, Assumption 5 is mild.\n\n## Theoretical Guarantees\n\nIn this section, we provide theoretical guarantees of our main algorithm (Algorithm 1). We first show the results for V-Learning and policy learning in Section 4.1 and Section 4.2 respectively and then combine them to obtain our main theorem in Section 4.3.\n\n### 4.1 Analysis of V-Learning\n\nWe mainly focus on V-learning in deterministic dynamics in this section. The analysis for stochastic dynamics is similar and presented in Appendix B.2.\n\nAs discussed in Section 3.1.1, although the first step of the algorithm is called V-learning, the main goal of this step is to estimate $$U^*_{\\alpha} = UV^*_{\\alpha}$$ accurately. The following lemma provides a theoretical guarantee that the output of V-learning algorithm $$\\hat{U}$$ is a good estimator of $$U^*_{\\alpha}$$ in positive parts:\n\nLemma 1 (Closeness of $$\\hat{U}^+_{\\alpha}$$ and $$U^*_{\\alpha}^+$$). Under Assumptions 1 and 2, with probability at least $$1 - \\log(|V|/\\delta)$$. $$\\| \\hat{U}^+_{\\alpha} - U^*_{\\alpha}^+ \\|_{2,\\mu} \\leq O(\\sqrt{\\max(N,V^2)})$$.\n\nProof sketch. By standard concentration argument, it can be shown that the empirical estimator $$\\hat{L}(d)$$ in Algorithm 2 (which is unbiased in deterministic dynamics) concentrates well on $$L_{\\alpha}$$ for all $$V \\in V$$ (Lemma 3). Therefore, by realizability of $$V^*_{\\alpha}$$, the value of $$L_{\\alpha}$$ at $$V^*_{\\alpha}$$ and the learned V-function $$\\hat{V}$$ are close (Lemma 4). Finally, one can observe that $$L_{\\alpha}$$ is \"semi-strongly\" convex w.r.t. $$UV^+$$ in $$\\| \\cdot \\|_{2,\\mu}$$-norm, and thus we can show that $$\\hat{U}^+_{\\alpha}$$ and $$U^*_{\\alpha}^+$$ are also close.\n\nThe complete proof of Lemma 1 is deferred to Appendix B.1. In Appendix B.2, we also show the counterpart of Lemma 1 for stochastic dynamic settings.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Therefore, $$\\pi^*_{\\alpha} = \\arg \\max_{\\pi} L_{MLE}(\\pi)$$ where $$L_{MLE}(\\pi) \\triangleq E(s,a,g) \\sim \\mu [U^*_{\\alpha}(s,a;g) + \\log \\pi(a|s,g)].$$\n\nSince we already learned $$\\hat{U}$$, which is close to $$U^*_{N\\hat{\\alpha}}$$, we can use the following estimator for $$L_{MLE}(\\pi):$$\n\n$$\nL_{MLE}(\\pi) = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{U}(s_i, a_i; g_i) + \\log \\pi(a_i|s_i, g_i). \\quad (13)\n$$\nAlgorithm 4 Policy learning\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^{N}$, policy class $\\Pi$, $\\hat{U}$ learned by Algorithm 2 or 3.\n2. Policy learning by:\n3. $\n\\hat{U}(s_i, a_i; g_i) + \\quad (14)\n$\nOutput: $\\hat{\\pi}$. $\\pi = \\arg \\max_{\\pi \\in \\Pi} L_{MLE}(\\pi) \\triangleq \\frac{1}{N} \\sum_{i=1}^{N} \\alpha \\log \\pi(a_i|s_i, g_i).$\n\nThe policy learning algorithm is presented in Algorithm 4, which can be viewed as a weighted maximum likelihood estimation (MLE) procedure. Finally, we make the following two assumptions on the policy class $$\\Pi$$.\n\nAssumption 4 (Single-policy realizability). Assume $$\\pi^*_{\\alpha} \\in \\Pi$$.\n\nAssumption 5 (Lower bound of policy). For any policy $$\\pi \\in \\Pi$$, we assume that $$\\pi(a|s,g) \\geq \\tau > 0$$ for any $$(s, a, g) \\in S \\times A \\times G$$.\n\nRemark 1. One may consider Assumption 5 strong if $$\\tau$$ is a constant independent of $$\\alpha$$ or $$N$$. However, we allow that $$\\tau$$ depends on $$\\alpha$$. In that case, $$\\tau$$ can be extremely small, and any policy mixed with a uniform policy with a tiny probability satisfies this assumption. Therefore, Assumption 5 is mild.", "md": "Therefore, $$\\pi^*_{\\alpha} = \\arg \\max_{\\pi} L_{MLE}(\\pi)$$ where $$L_{MLE}(\\pi) \\triangleq E(s,a,g) \\sim \\mu [U^*_{\\alpha}(s,a;g) + \\log \\pi(a|s,g)].$$\n\nSince we already learned $$\\hat{U}$$, which is close to $$U^*_{N\\hat{\\alpha}}$$, we can use the following estimator for $$L_{MLE}(\\pi):$$\n\n$$\nL_{MLE}(\\pi) = \\frac{1}{N} \\sum_{i=1}^{N} \\hat{U}(s_i, a_i; g_i) + \\log \\pi(a_i|s_i, g_i). \\quad (13)\n$$\nAlgorithm 4 Policy learning\n\n1. Input: Dataset $D = \\{(s_i, a_i, r_i, s'_i, g_i)\\}_{i=1}^{N}$, policy class $\\Pi$, $\\hat{U}$ learned by Algorithm 2 or 3.\n2. Policy learning by:\n3. $\n\\hat{U}(s_i, a_i; g_i) + \\quad (14)\n$\nOutput: $\\hat{\\pi}$. $\\pi = \\arg \\max_{\\pi \\in \\Pi} L_{MLE}(\\pi) \\triangleq \\frac{1}{N} \\sum_{i=1}^{N} \\alpha \\log \\pi(a_i|s_i, g_i).$\n\nThe policy learning algorithm is presented in Algorithm 4, which can be viewed as a weighted maximum likelihood estimation (MLE) procedure. Finally, we make the following two assumptions on the policy class $$\\Pi$$.\n\nAssumption 4 (Single-policy realizability). Assume $$\\pi^*_{\\alpha} \\in \\Pi$$.\n\nAssumption 5 (Lower bound of policy). For any policy $$\\pi \\in \\Pi$$, we assume that $$\\pi(a|s,g) \\geq \\tau > 0$$ for any $$(s, a, g) \\in S \\times A \\times G$$.\n\nRemark 1. One may consider Assumption 5 strong if $$\\tau$$ is a constant independent of $$\\alpha$$ or $$N$$. However, we allow that $$\\tau$$ depends on $$\\alpha$$. In that case, $$\\tau$$ can be extremely small, and any policy mixed with a uniform policy with a tiny probability satisfies this assumption. Therefore, Assumption 5 is mild."}, {"type": "heading", "lvl": 2, "value": "Theoretical Guarantees", "md": "## Theoretical Guarantees"}, {"type": "text", "value": "In this section, we provide theoretical guarantees of our main algorithm (Algorithm 1). We first show the results for V-Learning and policy learning in Section 4.1 and Section 4.2 respectively and then combine them to obtain our main theorem in Section 4.3.", "md": "In this section, we provide theoretical guarantees of our main algorithm (Algorithm 1). We first show the results for V-Learning and policy learning in Section 4.1 and Section 4.2 respectively and then combine them to obtain our main theorem in Section 4.3."}, {"type": "heading", "lvl": 3, "value": "4.1 Analysis of V-Learning", "md": "### 4.1 Analysis of V-Learning"}, {"type": "text", "value": "We mainly focus on V-learning in deterministic dynamics in this section. The analysis for stochastic dynamics is similar and presented in Appendix B.2.\n\nAs discussed in Section 3.1.1, although the first step of the algorithm is called V-learning, the main goal of this step is to estimate $$U^*_{\\alpha} = UV^*_{\\alpha}$$ accurately. The following lemma provides a theoretical guarantee that the output of V-learning algorithm $$\\hat{U}$$ is a good estimator of $$U^*_{\\alpha}$$ in positive parts:\n\nLemma 1 (Closeness of $$\\hat{U}^+_{\\alpha}$$ and $$U^*_{\\alpha}^+$$). Under Assumptions 1 and 2, with probability at least $$1 - \\log(|V|/\\delta)$$. $$\\| \\hat{U}^+_{\\alpha} - U^*_{\\alpha}^+ \\|_{2,\\mu} \\leq O(\\sqrt{\\max(N,V^2)})$$.\n\nProof sketch. By standard concentration argument, it can be shown that the empirical estimator $$\\hat{L}(d)$$ in Algorithm 2 (which is unbiased in deterministic dynamics) concentrates well on $$L_{\\alpha}$$ for all $$V \\in V$$ (Lemma 3). Therefore, by realizability of $$V^*_{\\alpha}$$, the value of $$L_{\\alpha}$$ at $$V^*_{\\alpha}$$ and the learned V-function $$\\hat{V}$$ are close (Lemma 4). Finally, one can observe that $$L_{\\alpha}$$ is \"semi-strongly\" convex w.r.t. $$UV^+$$ in $$\\| \\cdot \\|_{2,\\mu}$$-norm, and thus we can show that $$\\hat{U}^+_{\\alpha}$$ and $$U^*_{\\alpha}^+$$ are also close.\n\nThe complete proof of Lemma 1 is deferred to Appendix B.1. In Appendix B.2, we also show the counterpart of Lemma 1 for stochastic dynamic settings.", "md": "We mainly focus on V-learning in deterministic dynamics in this section. The analysis for stochastic dynamics is similar and presented in Appendix B.2.\n\nAs discussed in Section 3.1.1, although the first step of the algorithm is called V-learning, the main goal of this step is to estimate $$U^*_{\\alpha} = UV^*_{\\alpha}$$ accurately. The following lemma provides a theoretical guarantee that the output of V-learning algorithm $$\\hat{U}$$ is a good estimator of $$U^*_{\\alpha}$$ in positive parts:\n\nLemma 1 (Closeness of $$\\hat{U}^+_{\\alpha}$$ and $$U^*_{\\alpha}^+$$). Under Assumptions 1 and 2, with probability at least $$1 - \\log(|V|/\\delta)$$. $$\\| \\hat{U}^+_{\\alpha} - U^*_{\\alpha}^+ \\|_{2,\\mu} \\leq O(\\sqrt{\\max(N,V^2)})$$.\n\nProof sketch. By standard concentration argument, it can be shown that the empirical estimator $$\\hat{L}(d)$$ in Algorithm 2 (which is unbiased in deterministic dynamics) concentrates well on $$L_{\\alpha}$$ for all $$V \\in V$$ (Lemma 3). Therefore, by realizability of $$V^*_{\\alpha}$$, the value of $$L_{\\alpha}$$ at $$V^*_{\\alpha}$$ and the learned V-function $$\\hat{V}$$ are close (Lemma 4). Finally, one can observe that $$L_{\\alpha}$$ is \"semi-strongly\" convex w.r.t. $$UV^+$$ in $$\\| \\cdot \\|_{2,\\mu}$$-norm, and thus we can show that $$\\hat{U}^+_{\\alpha}$$ and $$U^*_{\\alpha}^+$$ are also close.\n\nThe complete proof of Lemma 1 is deferred to Appendix B.1. In Appendix B.2, we also show the counterpart of Lemma 1 for stochastic dynamic settings."}]}, {"page": 9, "text": " 4.2     Analysis of Policy Learning\n After obtaining an accurate estimator \u02c6                   U+ of U \u2217                                                                   U+ \u2212\n                                                                         \u03b1+ in the V -Learning procedure, i.e., \u2225                       \u02c6\n U \u2217                \u01ebstat, we can use \u02c6     U+ to perform policy learning and obtain the following guarantee:\n   \u03b1+\u22252,\u00b5\u2272       \u221a\n Lemma 2 (Closeness of \u03c0\u2217             \u03b1 and \u02c6  \u03c0). Under Assumptions 4 and 5, with probability at least 1 \u2212                             \u03b4, the\n output policy \u02c6     \u03c0 of Algorithm 4 satisfies\n                             E  s\u223cd\u2217                                 \u03c0(\u00b7|s, g)\u2225TV\u2264        O     \u01ebMLE stat /\u03c4 2     ,\n                                     \u03b1,g\u223cp(g)\u2225\u03c0\u2217    \u03b1(\u00b7|s, g) \u2212      \u02c6\n where \u01ebMLE stat is defined in Lemma 8.\n The proof of Lemma 2 is provided in Appendix C.2. This result shows that the TV distance between\n the regularized optimal policy \u03c0\u2217             \u03b1 and the output policy \u02c6         \u03c0 by Algorithm 4 is small, which translates\n to a bounded performance difference between these two policies as formalized in Theorem 1.\n Theorem 1 (Suboptimality of \u02c6               \u03c0). Under Assumptions 4 and 5, with probability at least 1 \u2212                               \u03b4, the\n output policy \u02c6     \u03c0 of Algorithm 4 satisfies J(\u03c0\u2217                       \u03c0) \u2264     O    Vmax        \u01ebMLE          .\n The proof of Theorem 1 is deferred to Appendix C.3.            \u03b1) \u2212    J(\u02c6                           stat /\u03c4 2\n 4.3     Main Theorem: Statistical Rate of Suboptimality\n Theorem 1 compares the performance difference between \u02c6                              \u03c0 and the regularized optimal policy \u03c0\u2217                 \u03b1.\n Since the ultimate goal is to compare with the optimal policy \u03c0\u2217, we also need to combine this\n result with Proposition 3.1. By carefully choosing the value of \u03b1 to balance J(\u02c6                                       \u03c0) \u2212    J(\u03c0\u2217  \u03b1) and\n J(\u03c0) \u2212     J(\u03c0\u2217  \u03b1), we can bound the suboptimality of the policy \u02c6                      \u03c0 output by Algorithm 1 compared to\n the optimal policy \u03c0\u2217, leading to the following main result:\n Theorem 2 (Statistical rate of suboptimality (in deterministic dynamics)). Under Assumptions 1,\n 2, 4 and 5, with probability at least 1 \u2212                   \u03b4, the output policy \u02c6       \u03c0 by Algorithm 1 (with the choice of\nAlgorithm 2 for V -learning in deterministic dynamics) satisfies\n                             J(\u03c0\u2217) \u2212      J(\u02c6 \u03c0) \u2272     V 3 max(C\u2217    \u03b1)3 log(1/\u03c4) log(|V||\u03a0|/\u03b4)               1/3\n                                                                             \u03c4 2N 1/4\n                             V 3                                 1/3\n                               max log(1/\u03c4) log(|V||\u03a0|/\u03b4)\n if we choose \u03b1 \u224d                     \u03c4 2(C\u2217\u03b1)3N 1/4                   and assume N = N0.\n The proof of Theorem 2 is deferred to Appendix D.1. Note that Theorem 2 provides a suboptimality\n rate of O(1/N 1/12) which implies an O(1/poly(\u01eb)) sample complexity and thus is statistically effi                                             -\n cient. A similar rate can also be obtained in stochastic dynamic settings, and we present the result in\n Appendix D.2. Note that our rate is slightly worse than the O(1/N 1/6) rate in Zhan et al. [2022], and\n worse than the optimal rate O(1/               \u221a  N) in Rashidinejad et al. [2022]. We briefl                   y discuss the intrinsic\n diffi culty to derive an optimal convergence rate. First, we only require a realizability assumption\n on our function class, while Rashidinejad et al. [2022] requires a much stronger completeness as-\n sumption. Second, our optimization procedure is uninterleaved and only requires solving regression\n problems, while Zhan et al. [2022] and Rashidinejad et al. [2022] require solving minimax problems.\n Finally, Rashidinejad et al. [2022] assumes that the behavior policy is known and directly computes\n the policy using the knowledge of behavior policy, while our algorithm uses a more practical method,\n i.e., MLE, to solve the policy in the policy learning step.\nWe also compare our theoretical results to Ma et al. [2022c]. Theorem 4.1 of Ma et al. [2022c]\n provides a fi     nite-sample guarantee for the suboptimality. However, they compare the performance\n of \u02c6\n    \u03c0 to \u03c0\u2217  \u03b1 (with \u03b1 = 1) instead of \u03c0\u2217. Since the performance gap between \u03c0\u2217                                 and \u03c0\u2217  \u03b1 can be as large\n as a constant when \u03b1 = 1, even zero suboptimality (compared to \u03c0\u2217                                \u03b1) cannot imply that the learned\n policy has good performance. Moreover, their theoretical analysis assumes that V \u2217                                        can be learned\n with zero error, which is unreasonable in practical scenarios. We also note that they only provide a\n proof for deterministic policy classes, which can be restrictive in practice.\n                                                                       9", "md": "## Analysis of Policy Learning\n\nAfter obtaining an accurate estimator $$\\hat{U}$$ of $$U^* - \\alpha^+$$ in the V-Learning procedure, i.e., $$\\| \\hat{U} - U^* \\|_{\\epsilon_{\\text{stat}}}$$, we can use $$\\hat{U}$$ to perform policy learning and obtain the following guarantee:\n\n$$\n\\alpha^2, \\mu \\lesssim \\sqrt{}\n$$\n\nLemma 2 (Closeness of $$\\pi^* - \\alpha$$ and $$\\hat{\\pi}$$). Under Assumptions 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ of Algorithm 4 satisfies\n\n$$\nE_{s \\sim d^*} \\| \\pi(\\cdot | s, g) - \\hat{\\pi} \\|_{\\text{TV}} \\leq O \\frac{\\epsilon_{\\text{MLE stat}}}{\\tau^2},\n$$\n\nwhere $$\\epsilon_{\\text{MLE stat}}$$ is defined in Lemma 8.\n\nThe proof of Lemma 2 is provided in Appendix C.2. This result shows that the TV distance between the regularized optimal policy $$\\pi^* - \\alpha$$ and the output policy $$\\hat{\\pi}$$ by Algorithm 4 is small, which translates to a bounded performance difference between these two policies as formalized in Theorem 1.\n\nTheorem 1 (Suboptimality of $$\\hat{\\pi}$$). Under Assumptions 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ of Algorithm 4 satisfies $$J(\\pi^* - \\pi) \\leq O V_{\\text{max}} \\epsilon_{\\text{MLE}}$$.\n\nThe proof of Theorem 1 is deferred to Appendix C.3.\n\n## Main Theorem: Statistical Rate of Suboptimality\n\nTheorem 1 compares the performance difference between $$\\hat{\\pi}$$ and the regularized optimal policy $$\\pi^* - \\alpha$$. Since the ultimate goal is to compare with the optimal policy $$\\pi^*$$, we also need to combine this result with Proposition 3.1. By carefully choosing the value of $$\\alpha$$ to balance $$J(\\hat{\\pi}) - J(\\pi^* - \\alpha)$$ and $$J(\\pi) - J(\\pi^* - \\alpha)$$, we can bound the suboptimality of the policy $$\\hat{\\pi}$$ output by Algorithm 1 compared to the optimal policy $$\\pi^*$$, leading to the following main result:\n\nTheorem 2 (Statistical rate of suboptimality (in deterministic dynamics)). Under Assumptions 1, 2, 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ by Algorithm 1 (with the choice of Algorithm 2 for V-learning in deterministic dynamics) satisfies\n\n$$\nJ(\\pi^*) - J(\\hat{\\pi}) \\lesssim V_{\\text{max}}^3(C^* \\alpha)^3 \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} \\frac{\\tau^2}{N^{1/4}}\n$$\n\nif we choose $$\\alpha \\approx \\frac{V_{\\text{max}}^3(C^* \\alpha)^3N^{1/4}}{\\tau^2}$$ and assume $$N = N_0$$.\n\nThe proof of Theorem 2 is deferred to Appendix D.1. Note that Theorem 2 provides a suboptimality rate of $$O(1/N^{1/12})$$ which implies an $$O(1/\\text{poly}(\\epsilon))$$ sample complexity and thus is statistically efficient. A similar rate can also be obtained in stochastic dynamic settings, and we present the result in Appendix D.2. Note that our rate is slightly worse than the $$O(1/N^{1/6})$$ rate in Zhan et al. [2022], and worse than the optimal rate $$O(1/\\sqrt{N})$$ in Rashidinejad et al. [2022]. We briefly discuss the intrinsic difficulty to derive an optimal convergence rate. First, we only require a realizability assumption on our function class, while Rashidinejad et al. [2022] requires a much stronger completeness assumption. Second, our optimization procedure is uninterleaved and only requires solving regression problems, while Zhan et al. [2022] and Rashidinejad et al. [2022] require solving minimax problems. Finally, Rashidinejad et al. [2022] assumes that the behavior policy is known and directly computes the policy using the knowledge of the behavior policy, while our algorithm uses a more practical method, i.e., MLE, to solve the policy in the policy learning step.\n\nWe also compare our theoretical results to Ma et al. [2022c]. Theorem 4.1 of Ma et al. [2022c] provides a finite-sample guarantee for the suboptimality. However, they compare the performance of $$\\hat{\\pi}$$ to $$\\pi^* - \\alpha$$ (with $$\\alpha = 1$$) instead of $$\\pi^*$$. Since the performance gap between $$\\pi^*$$ and $$\\pi^* - \\alpha$$ can be as large as a constant when $$\\alpha = 1$$, even zero suboptimality (compared to $$\\pi^* - \\alpha$$) cannot imply that the learned policy has good performance. Moreover, their theoretical analysis assumes that $$V^*$$ can be learned with zero error, which is unreasonable in practical scenarios. We also note that they only provide a proof for deterministic policy classes, which can be restrictive in practice.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Analysis of Policy Learning", "md": "## Analysis of Policy Learning"}, {"type": "text", "value": "After obtaining an accurate estimator $$\\hat{U}$$ of $$U^* - \\alpha^+$$ in the V-Learning procedure, i.e., $$\\| \\hat{U} - U^* \\|_{\\epsilon_{\\text{stat}}}$$, we can use $$\\hat{U}$$ to perform policy learning and obtain the following guarantee:\n\n$$\n\\alpha^2, \\mu \\lesssim \\sqrt{}\n$$\n\nLemma 2 (Closeness of $$\\pi^* - \\alpha$$ and $$\\hat{\\pi}$$). Under Assumptions 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ of Algorithm 4 satisfies\n\n$$\nE_{s \\sim d^*} \\| \\pi(\\cdot | s, g) - \\hat{\\pi} \\|_{\\text{TV}} \\leq O \\frac{\\epsilon_{\\text{MLE stat}}}{\\tau^2},\n$$\n\nwhere $$\\epsilon_{\\text{MLE stat}}$$ is defined in Lemma 8.\n\nThe proof of Lemma 2 is provided in Appendix C.2. This result shows that the TV distance between the regularized optimal policy $$\\pi^* - \\alpha$$ and the output policy $$\\hat{\\pi}$$ by Algorithm 4 is small, which translates to a bounded performance difference between these two policies as formalized in Theorem 1.\n\nTheorem 1 (Suboptimality of $$\\hat{\\pi}$$). Under Assumptions 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ of Algorithm 4 satisfies $$J(\\pi^* - \\pi) \\leq O V_{\\text{max}} \\epsilon_{\\text{MLE}}$$.\n\nThe proof of Theorem 1 is deferred to Appendix C.3.", "md": "After obtaining an accurate estimator $$\\hat{U}$$ of $$U^* - \\alpha^+$$ in the V-Learning procedure, i.e., $$\\| \\hat{U} - U^* \\|_{\\epsilon_{\\text{stat}}}$$, we can use $$\\hat{U}$$ to perform policy learning and obtain the following guarantee:\n\n$$\n\\alpha^2, \\mu \\lesssim \\sqrt{}\n$$\n\nLemma 2 (Closeness of $$\\pi^* - \\alpha$$ and $$\\hat{\\pi}$$). Under Assumptions 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ of Algorithm 4 satisfies\n\n$$\nE_{s \\sim d^*} \\| \\pi(\\cdot | s, g) - \\hat{\\pi} \\|_{\\text{TV}} \\leq O \\frac{\\epsilon_{\\text{MLE stat}}}{\\tau^2},\n$$\n\nwhere $$\\epsilon_{\\text{MLE stat}}$$ is defined in Lemma 8.\n\nThe proof of Lemma 2 is provided in Appendix C.2. This result shows that the TV distance between the regularized optimal policy $$\\pi^* - \\alpha$$ and the output policy $$\\hat{\\pi}$$ by Algorithm 4 is small, which translates to a bounded performance difference between these two policies as formalized in Theorem 1.\n\nTheorem 1 (Suboptimality of $$\\hat{\\pi}$$). Under Assumptions 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ of Algorithm 4 satisfies $$J(\\pi^* - \\pi) \\leq O V_{\\text{max}} \\epsilon_{\\text{MLE}}$$.\n\nThe proof of Theorem 1 is deferred to Appendix C.3."}, {"type": "heading", "lvl": 2, "value": "Main Theorem: Statistical Rate of Suboptimality", "md": "## Main Theorem: Statistical Rate of Suboptimality"}, {"type": "text", "value": "Theorem 1 compares the performance difference between $$\\hat{\\pi}$$ and the regularized optimal policy $$\\pi^* - \\alpha$$. Since the ultimate goal is to compare with the optimal policy $$\\pi^*$$, we also need to combine this result with Proposition 3.1. By carefully choosing the value of $$\\alpha$$ to balance $$J(\\hat{\\pi}) - J(\\pi^* - \\alpha)$$ and $$J(\\pi) - J(\\pi^* - \\alpha)$$, we can bound the suboptimality of the policy $$\\hat{\\pi}$$ output by Algorithm 1 compared to the optimal policy $$\\pi^*$$, leading to the following main result:\n\nTheorem 2 (Statistical rate of suboptimality (in deterministic dynamics)). Under Assumptions 1, 2, 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ by Algorithm 1 (with the choice of Algorithm 2 for V-learning in deterministic dynamics) satisfies\n\n$$\nJ(\\pi^*) - J(\\hat{\\pi}) \\lesssim V_{\\text{max}}^3(C^* \\alpha)^3 \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} \\frac{\\tau^2}{N^{1/4}}\n$$\n\nif we choose $$\\alpha \\approx \\frac{V_{\\text{max}}^3(C^* \\alpha)^3N^{1/4}}{\\tau^2}$$ and assume $$N = N_0$$.\n\nThe proof of Theorem 2 is deferred to Appendix D.1. Note that Theorem 2 provides a suboptimality rate of $$O(1/N^{1/12})$$ which implies an $$O(1/\\text{poly}(\\epsilon))$$ sample complexity and thus is statistically efficient. A similar rate can also be obtained in stochastic dynamic settings, and we present the result in Appendix D.2. Note that our rate is slightly worse than the $$O(1/N^{1/6})$$ rate in Zhan et al. [2022], and worse than the optimal rate $$O(1/\\sqrt{N})$$ in Rashidinejad et al. [2022]. We briefly discuss the intrinsic difficulty to derive an optimal convergence rate. First, we only require a realizability assumption on our function class, while Rashidinejad et al. [2022] requires a much stronger completeness assumption. Second, our optimization procedure is uninterleaved and only requires solving regression problems, while Zhan et al. [2022] and Rashidinejad et al. [2022] require solving minimax problems. Finally, Rashidinejad et al. [2022] assumes that the behavior policy is known and directly computes the policy using the knowledge of the behavior policy, while our algorithm uses a more practical method, i.e., MLE, to solve the policy in the policy learning step.\n\nWe also compare our theoretical results to Ma et al. [2022c]. Theorem 4.1 of Ma et al. [2022c] provides a finite-sample guarantee for the suboptimality. However, they compare the performance of $$\\hat{\\pi}$$ to $$\\pi^* - \\alpha$$ (with $$\\alpha = 1$$) instead of $$\\pi^*$$. Since the performance gap between $$\\pi^*$$ and $$\\pi^* - \\alpha$$ can be as large as a constant when $$\\alpha = 1$$, even zero suboptimality (compared to $$\\pi^* - \\alpha$$) cannot imply that the learned policy has good performance. Moreover, their theoretical analysis assumes that $$V^*$$ can be learned with zero error, which is unreasonable in practical scenarios. We also note that they only provide a proof for deterministic policy classes, which can be restrictive in practice.", "md": "Theorem 1 compares the performance difference between $$\\hat{\\pi}$$ and the regularized optimal policy $$\\pi^* - \\alpha$$. Since the ultimate goal is to compare with the optimal policy $$\\pi^*$$, we also need to combine this result with Proposition 3.1. By carefully choosing the value of $$\\alpha$$ to balance $$J(\\hat{\\pi}) - J(\\pi^* - \\alpha)$$ and $$J(\\pi) - J(\\pi^* - \\alpha)$$, we can bound the suboptimality of the policy $$\\hat{\\pi}$$ output by Algorithm 1 compared to the optimal policy $$\\pi^*$$, leading to the following main result:\n\nTheorem 2 (Statistical rate of suboptimality (in deterministic dynamics)). Under Assumptions 1, 2, 4 and 5, with probability at least $$1 - \\delta$$, the output policy $$\\hat{\\pi}$$ by Algorithm 1 (with the choice of Algorithm 2 for V-learning in deterministic dynamics) satisfies\n\n$$\nJ(\\pi^*) - J(\\hat{\\pi}) \\lesssim V_{\\text{max}}^3(C^* \\alpha)^3 \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} \\frac{\\tau^2}{N^{1/4}}\n$$\n\nif we choose $$\\alpha \\approx \\frac{V_{\\text{max}}^3(C^* \\alpha)^3N^{1/4}}{\\tau^2}$$ and assume $$N = N_0$$.\n\nThe proof of Theorem 2 is deferred to Appendix D.1. Note that Theorem 2 provides a suboptimality rate of $$O(1/N^{1/12})$$ which implies an $$O(1/\\text{poly}(\\epsilon))$$ sample complexity and thus is statistically efficient. A similar rate can also be obtained in stochastic dynamic settings, and we present the result in Appendix D.2. Note that our rate is slightly worse than the $$O(1/N^{1/6})$$ rate in Zhan et al. [2022], and worse than the optimal rate $$O(1/\\sqrt{N})$$ in Rashidinejad et al. [2022]. We briefly discuss the intrinsic difficulty to derive an optimal convergence rate. First, we only require a realizability assumption on our function class, while Rashidinejad et al. [2022] requires a much stronger completeness assumption. Second, our optimization procedure is uninterleaved and only requires solving regression problems, while Zhan et al. [2022] and Rashidinejad et al. [2022] require solving minimax problems. Finally, Rashidinejad et al. [2022] assumes that the behavior policy is known and directly computes the policy using the knowledge of the behavior policy, while our algorithm uses a more practical method, i.e., MLE, to solve the policy in the policy learning step.\n\nWe also compare our theoretical results to Ma et al. [2022c]. Theorem 4.1 of Ma et al. [2022c] provides a finite-sample guarantee for the suboptimality. However, they compare the performance of $$\\hat{\\pi}$$ to $$\\pi^* - \\alpha$$ (with $$\\alpha = 1$$) instead of $$\\pi^*$$. Since the performance gap between $$\\pi^*$$ and $$\\pi^* - \\alpha$$ can be as large as a constant when $$\\alpha = 1$$, even zero suboptimality (compared to $$\\pi^* - \\alpha$$) cannot imply that the learned policy has good performance. Moreover, their theoretical analysis assumes that $$V^*$$ can be learned with zero error, which is unreasonable in practical scenarios. We also note that they only provide a proof for deterministic policy classes, which can be restrictive in practice."}]}, {"page": 10, "text": "5      Experiments\nIn this section, we provide experimental results of our VP-learning algorithm with different choices\nof \u03b1 under fi      ve different environments: FetchReach, FetchPick, FetchPush, FetchSlide, and Han-\ndReach [Plappert et al., 2018]. Similar to Ma et al. [2022c], the datasets for the fi                                 ve tasks are from\nYang et al. [2022]. All the implementation details of our VP-learning are the same as GoFAR (see\ndataset details and implementation details in Ma et al. [2022c]) 2, except for the value of \u03b1. Note that\nour VP-learning algorithm with \u03b1 = 1 is equivalent to the GoFAR algorithm. Table 1 presents the\ndiscounted returns and Table 2 presents the fi                    nal distances of the policies trained after 100 epochs\nand evaluated over 10 runs. For each environment and each \u03b1 , the result was averaged over 3\nrandom seeds. The best results of each environment are in bold.\n            Table 1: Discounted return of different choices of \u03b1, averaged over 3 random seeds.\n                        \u03b1\\ Env       FetchReach        FetchPick        FetchPush        FetchSlide        HandReach\n                          0.01      27.4 \u00b1 0.29       18.5 \u00b1 0.1       18.0 \u00b1 1.8       2.36 \u00b1 1.13       8.72 \u00b1 1.69\n                          0.02      27.4 \u00b1 0.32       18.7 \u00b1 1.8       18.6 \u00b1 2.6       2.40 \u00b1 0.47       7.96 \u00b1 1.27\n                          0.05      27.4 \u00b1 0.32       17.3 \u00b1 1.1       19.3 \u00b1 2.0       3.18 \u00b1 0.90       8.98 \u00b1 3.11\n                          0.1       27.4 \u00b1 0.33       20.3 \u00b1 1.3       20.3 \u00b1 2.5       3.22 \u00b1 0.38       5.28 \u00b1 1.25\n                          0.2       27.4 \u00b1 0.32        20.7 \u00b1 0.9      17.7 \u00b1 2.9       2.25 \u00b1 0.23       2.92 \u00b1 0.98\n                          0.5       27.5 \u00b1 0.29       18.5 \u00b1 0.4       20.1 \u00b1 2.2       3.47 \u00b1 1.08       5.74 \u00b1 2.72\n                           1        27.3 \u00b1 0.34       18.2 \u00b1 1.2       19.6 \u00b1 1.6       2.75 \u00b1 1.84       7.13 \u00b1 3.60\n                           2        27.4 \u00b1 0.29       18.3 \u00b1 0.7       19.6 \u00b1 1.4       1.80 \u00b1 0.66       3.99 \u00b1 1.88\n              Table 2: Final distance of different choices of \u03b1, averaged over 3 random seeds.\n             \u03b1\\ Env          FetchReach            FetchPick           FetchPush           FetchSlide             HandReach\n               0.01      0.0171 \u00b1 0.0017        0.042 \u00b1 0.004       0.033 \u00b1 0.001      0.1177 \u00b1 0.012         0.0269 \u00b1 0.0049\n               0.02      0.0168 \u00b1 0.0016        0.045 \u00b1 0.012       0.031 \u00b1 0.002      0.1085 \u00b1 0.010         0.0274 \u00b1 0.0049\n               0.05      0.0181 \u00b1 0.0011        0.052 \u00b1 0.013       0.032 \u00b1 0.002      0.1061 \u00b1 0.009         0.0270 \u00b1 0.0049\n                0.1      0.0173 \u00b1 0.0014        0.032 \u00b1 0.010       0.027 \u00b1 0.002      0.1018 \u00b1 0.002         0.0275 \u00b1 0.0043\n                0.2      0.0172 \u00b1 0.0019        0.031 \u00b1 0.004       0.031 \u00b1 0.003      0.1029 \u00b1 0.010         0.0275 \u00b1 0.0046\n                0.5      0.0166 \u00b1 0.0011        0.044 \u00b1 0.009       0.031 \u00b1 0.005       0.1017 \u00b1 0.017       0.026826 \u00b1 0.0049\n                1        0.0175 \u00b1 0.0013        0.043 \u00b1 0.011       0.043 \u00b1 0.012      0.1202 \u00b1 0.019        0.026828 \u00b1 0.0044\n                2        0.0171 \u00b1 0.0011        0.034 \u00b1 0.005       0.032 \u00b1 0.001      0.1044 \u00b1 0.011         0.0275 \u00b1 0.0045\nThe empirical results demonstrate the correctness of our theoretical analysis: choosing \u03b1 = 1 will\nresult in a large suboptimality of \u03c0\u2217               \u03b1 and thus the learned policy \u02c6             \u03c0. Instead, we should carefully\nchoose the value of \u03b1 to ensure a vanishing suboptimality. In practice, we can tune the value of \u03b1\nand typically it is less than one. In our experiments, the best \u03b1 ranges over [0.05, 0.5].\n6      Conclusions\nIn this paper, we theoretically analyze the VP-learning algorithm (Algorithm 1, which is based on\nthe previous empirically successful algorithm in Ma et al. [2022c]) for both single-task and goal-\nconditioned offl       ine settings. This algorithm can deal with general value function approximation and\nonly requires near minimal assumptions on the dataset (single-policy concentrability) and function\nclass (realizability). We also provide an O(1/N 1/12) upper bound of the suboptimality of the policy\nlearned by the algorithm and empirically validate its effectiveness.\nAs for future directions, one important question is whether we can achieve the optimal suboptimality\nrate \u02dc O(1/    \u221a N) while keeping the algorithm practical without unreasonably strong assumptions.\n     2We use the code at https://github.com/JasonMa2016/GoFAR with different values of \u03b1 for our exper-\niments.\n                                                                      10", "md": "# Experimental Results\n\n## Experiments\n\nIn this section, we provide experimental results of our VP-learning algorithm with different choices of \u03b1 under five different environments: FetchReach, FetchPick, FetchPush, FetchSlide, and HandReach [Plappert et al., 2018]. Similar to Ma et al. [2022c], the datasets for the five tasks are from Yang et al. [2022]. All the implementation details of our VP-learning are the same as GoFAR (see dataset details and implementation details in Ma et al. [2022c]), except for the value of \u03b1. Note that our VP-learning algorithm with \u03b1 = 1 is equivalent to the GoFAR algorithm. Table 1 presents the discounted returns and Table 2 presents the final distances of the policies trained after 100 epochs and evaluated over 10 runs. For each environment and each \u03b1, the result was averaged over 3 random seeds. The best results of each environment are in bold.\n\n### Table 1: Discounted return of different choices of \u03b1, averaged over 3 random seeds.\n\n|\u03b1\\ Env|FetchReach|FetchPick|FetchPush|FetchSlide|HandReach|\n|---|---|---|---|---|---|\n|0.01|27.4 \u00b1 0.29|18.5 \u00b1 0.1|18.0 \u00b1 1.8|2.36 \u00b1 1.13|8.72 \u00b1 1.69|\n|0.02|27.4 \u00b1 0.32|18.7 \u00b1 1.8|18.6 \u00b1 2.6|2.40 \u00b1 0.47|7.96 \u00b1 1.27|\n|0.05|27.4 \u00b1 0.32|17.3 \u00b1 1.1|19.3 \u00b1 2.0|3.18 \u00b1 0.90|8.98 \u00b1 3.11|\n|0.1|27.4 \u00b1 0.33|20.3 \u00b1 1.3|20.3 \u00b1 2.5|3.22 \u00b1 0.38|5.28 \u00b1 1.25|\n|0.2|27.4 \u00b1 0.32|20.7 \u00b1 0.9|17.7 \u00b1 2.9|2.25 \u00b1 0.23|2.92 \u00b1 0.98|\n|0.5|27.5 \u00b1 0.29|18.5 \u00b1 0.4|20.1 \u00b1 2.2|3.47 \u00b1 1.08|5.74 \u00b1 2.72|\n|1|27.3 \u00b1 0.34|18.2 \u00b1 1.2|19.6 \u00b1 1.6|2.75 \u00b1 1.84|7.13 \u00b1 3.60|\n|2|27.4 \u00b1 0.29|18.3 \u00b1 0.7|19.6 \u00b1 1.4|1.80 \u00b1 0.66|3.99 \u00b1 1.88|\n\n### Table 2: Final distance of different choices of \u03b1, averaged over 3 random seeds.\n\n|\u03b1\\ Env|FetchReach|FetchPick|FetchPush|FetchSlide|HandReach|\n|---|---|---|---|---|---|\n|0.01|0.0171 \u00b1 0.0017|0.042 \u00b1 0.004|0.033 \u00b1 0.001|0.1177 \u00b1 0.012|0.0269 \u00b1 0.0049|\n|0.02|0.0168 \u00b1 0.0016|0.045 \u00b1 0.012|0.031 \u00b1 0.002|0.1085 \u00b1 0.010|0.0274 \u00b1 0.0049|\n|0.05|0.0181 \u00b1 0.0011|0.052 \u00b1 0.013|0.032 \u00b1 0.002|0.1061 \u00b1 0.009|0.0270 \u00b1 0.0049|\n|0.1|0.0173 \u00b1 0.0014|0.032 \u00b1 0.010|0.027 \u00b1 0.002|0.1018 \u00b1 0.002|0.0275 \u00b1 0.0043|\n|0.2|0.0172 \u00b1 0.0019|0.031 \u00b1 0.004|0.031 \u00b1 0.003|0.1029 \u00b1 0.010|0.0275 \u00b1 0.0046|\n|0.5|0.0166 \u00b1 0.0011|0.044 \u00b1 0.009|0.031 \u00b1 0.005|0.1017 \u00b1 0.017|0.026826 \u00b1 0.0049|\n|1|0.0175 \u00b1 0.0013|0.043 \u00b1 0.011|0.043 \u00b1 0.012|0.1202 \u00b1 0.019|0.026828 \u00b1 0.0044|\n|2|0.0171 \u00b1 0.0011|0.034 \u00b1 0.005|0.032 \u00b1 0.001|0.1044 \u00b1 0.011|0.0275 \u00b1 0.0045|\n\n## Conclusions\n\nIn this paper, we theoretically analyze the VP-learning algorithm (Algorithm 1, which is based on the previous empirically successful algorithm in Ma et al. [2022c]) for both single-task and goal-conditioned offline settings. This algorithm can deal with general value function approximation and only requires near minimal assumptions on the dataset (single-policy concentrability) and function class (realizability). We also provide an O(1/N^1/12) upper bound of the suboptimality of the policy learned by the algorithm and empirically validate its effectiveness.\n\nAs for future directions, one important question is whether we can achieve the optimal suboptimality rate O(1/\u221aN) while keeping the algorithm practical without unreasonably strong assumptions.\n\nWe use the code at https://github.com/JasonMa2016/GoFAR with different values of \u03b1 for our experiments.", "images": [{"name": "page-10-28.jpg", "height": 13, "width": 278, "x": 167, "y": 302}, {"name": "page-10-26.jpg", "height": 13, "width": 278, "x": 167, "y": 289}, {"name": "page-10-30.jpg", "height": 13, "width": 278, "x": 167, "y": 315}, {"name": "page-10-24.jpg", "height": 13, "width": 278, "x": 167, "y": 276}, {"name": "page-10-11.jpg", "height": 13, "width": 278, "x": 167, "y": 250}, {"name": "page-10-0.jpg", "height": 13, "width": 278, "x": 167, "y": 223}, {"name": "page-10-4.jpg", "height": 13, "width": 278, "x": 167, "y": 237}, {"name": "page-10-40.jpg", "height": 25, "width": 336, "x": 138, "y": 380}, {"name": "page-10-42.jpg", "height": 25, "width": 336, "x": 138, "y": 405}, {"name": "page-10-46.jpg", "height": 25, "width": 336, "x": 138, "y": 455}, {"name": "page-10-34.jpg", "height": 13, "width": 278, "x": 167, "y": 329}, {"name": "page-10-48.jpg", "height": 13, "width": 336, "x": 138, "y": 480}, {"name": "page-10-44.jpg", "height": 25, "width": 336, "x": 138, "y": 430}], "items": [{"type": "heading", "lvl": 1, "value": "Experimental Results", "md": "# Experimental Results"}, {"type": "heading", "lvl": 2, "value": "Experiments", "md": "## Experiments"}, {"type": "text", "value": "In this section, we provide experimental results of our VP-learning algorithm with different choices of \u03b1 under five different environments: FetchReach, FetchPick, FetchPush, FetchSlide, and HandReach [Plappert et al., 2018]. Similar to Ma et al. [2022c], the datasets for the five tasks are from Yang et al. [2022]. All the implementation details of our VP-learning are the same as GoFAR (see dataset details and implementation details in Ma et al. [2022c]), except for the value of \u03b1. Note that our VP-learning algorithm with \u03b1 = 1 is equivalent to the GoFAR algorithm. Table 1 presents the discounted returns and Table 2 presents the final distances of the policies trained after 100 epochs and evaluated over 10 runs. For each environment and each \u03b1, the result was averaged over 3 random seeds. The best results of each environment are in bold.", "md": "In this section, we provide experimental results of our VP-learning algorithm with different choices of \u03b1 under five different environments: FetchReach, FetchPick, FetchPush, FetchSlide, and HandReach [Plappert et al., 2018]. Similar to Ma et al. [2022c], the datasets for the five tasks are from Yang et al. [2022]. All the implementation details of our VP-learning are the same as GoFAR (see dataset details and implementation details in Ma et al. [2022c]), except for the value of \u03b1. Note that our VP-learning algorithm with \u03b1 = 1 is equivalent to the GoFAR algorithm. Table 1 presents the discounted returns and Table 2 presents the final distances of the policies trained after 100 epochs and evaluated over 10 runs. For each environment and each \u03b1, the result was averaged over 3 random seeds. The best results of each environment are in bold."}, {"type": "heading", "lvl": 3, "value": "Table 1: Discounted return of different choices of \u03b1, averaged over 3 random seeds.", "md": "### Table 1: Discounted return of different choices of \u03b1, averaged over 3 random seeds."}, {"type": "table", "rows": [["\u03b1\\ Env", "FetchReach", "FetchPick", "FetchPush", "FetchSlide", "HandReach"], ["0.01", "27.4 \u00b1 0.29", "18.5 \u00b1 0.1", "18.0 \u00b1 1.8", "2.36 \u00b1 1.13", "8.72 \u00b1 1.69"], ["0.02", "27.4 \u00b1 0.32", "18.7 \u00b1 1.8", "18.6 \u00b1 2.6", "2.40 \u00b1 0.47", "7.96 \u00b1 1.27"], ["0.05", "27.4 \u00b1 0.32", "17.3 \u00b1 1.1", "19.3 \u00b1 2.0", "3.18 \u00b1 0.90", "8.98 \u00b1 3.11"], ["0.1", "27.4 \u00b1 0.33", "20.3 \u00b1 1.3", "20.3 \u00b1 2.5", "3.22 \u00b1 0.38", "5.28 \u00b1 1.25"], ["0.2", "27.4 \u00b1 0.32", "20.7 \u00b1 0.9", "17.7 \u00b1 2.9", "2.25 \u00b1 0.23", "2.92 \u00b1 0.98"], ["0.5", "27.5 \u00b1 0.29", "18.5 \u00b1 0.4", "20.1 \u00b1 2.2", "3.47 \u00b1 1.08", "5.74 \u00b1 2.72"], ["1", "27.3 \u00b1 0.34", "18.2 \u00b1 1.2", "19.6 \u00b1 1.6", "2.75 \u00b1 1.84", "7.13 \u00b1 3.60"], ["2", "27.4 \u00b1 0.29", "18.3 \u00b1 0.7", "19.6 \u00b1 1.4", "1.80 \u00b1 0.66", "3.99 \u00b1 1.88"]], "md": "|\u03b1\\ Env|FetchReach|FetchPick|FetchPush|FetchSlide|HandReach|\n|---|---|---|---|---|---|\n|0.01|27.4 \u00b1 0.29|18.5 \u00b1 0.1|18.0 \u00b1 1.8|2.36 \u00b1 1.13|8.72 \u00b1 1.69|\n|0.02|27.4 \u00b1 0.32|18.7 \u00b1 1.8|18.6 \u00b1 2.6|2.40 \u00b1 0.47|7.96 \u00b1 1.27|\n|0.05|27.4 \u00b1 0.32|17.3 \u00b1 1.1|19.3 \u00b1 2.0|3.18 \u00b1 0.90|8.98 \u00b1 3.11|\n|0.1|27.4 \u00b1 0.33|20.3 \u00b1 1.3|20.3 \u00b1 2.5|3.22 \u00b1 0.38|5.28 \u00b1 1.25|\n|0.2|27.4 \u00b1 0.32|20.7 \u00b1 0.9|17.7 \u00b1 2.9|2.25 \u00b1 0.23|2.92 \u00b1 0.98|\n|0.5|27.5 \u00b1 0.29|18.5 \u00b1 0.4|20.1 \u00b1 2.2|3.47 \u00b1 1.08|5.74 \u00b1 2.72|\n|1|27.3 \u00b1 0.34|18.2 \u00b1 1.2|19.6 \u00b1 1.6|2.75 \u00b1 1.84|7.13 \u00b1 3.60|\n|2|27.4 \u00b1 0.29|18.3 \u00b1 0.7|19.6 \u00b1 1.4|1.80 \u00b1 0.66|3.99 \u00b1 1.88|", "isPerfectTable": true, "csv": "\"\u03b1\\ Env\",\"FetchReach\",\"FetchPick\",\"FetchPush\",\"FetchSlide\",\"HandReach\"\n\"0.01\",\"27.4 \u00b1 0.29\",\"18.5 \u00b1 0.1\",\"18.0 \u00b1 1.8\",\"2.36 \u00b1 1.13\",\"8.72 \u00b1 1.69\"\n\"0.02\",\"27.4 \u00b1 0.32\",\"18.7 \u00b1 1.8\",\"18.6 \u00b1 2.6\",\"2.40 \u00b1 0.47\",\"7.96 \u00b1 1.27\"\n\"0.05\",\"27.4 \u00b1 0.32\",\"17.3 \u00b1 1.1\",\"19.3 \u00b1 2.0\",\"3.18 \u00b1 0.90\",\"8.98 \u00b1 3.11\"\n\"0.1\",\"27.4 \u00b1 0.33\",\"20.3 \u00b1 1.3\",\"20.3 \u00b1 2.5\",\"3.22 \u00b1 0.38\",\"5.28 \u00b1 1.25\"\n\"0.2\",\"27.4 \u00b1 0.32\",\"20.7 \u00b1 0.9\",\"17.7 \u00b1 2.9\",\"2.25 \u00b1 0.23\",\"2.92 \u00b1 0.98\"\n\"0.5\",\"27.5 \u00b1 0.29\",\"18.5 \u00b1 0.4\",\"20.1 \u00b1 2.2\",\"3.47 \u00b1 1.08\",\"5.74 \u00b1 2.72\"\n\"1\",\"27.3 \u00b1 0.34\",\"18.2 \u00b1 1.2\",\"19.6 \u00b1 1.6\",\"2.75 \u00b1 1.84\",\"7.13 \u00b1 3.60\"\n\"2\",\"27.4 \u00b1 0.29\",\"18.3 \u00b1 0.7\",\"19.6 \u00b1 1.4\",\"1.80 \u00b1 0.66\",\"3.99 \u00b1 1.88\""}, {"type": "heading", "lvl": 3, "value": "Table 2: Final distance of different choices of \u03b1, averaged over 3 random seeds.", "md": "### Table 2: Final distance of different choices of \u03b1, averaged over 3 random seeds."}, {"type": "table", "rows": [["\u03b1\\ Env", "FetchReach", "FetchPick", "FetchPush", "FetchSlide", "HandReach"], ["0.01", "0.0171 \u00b1 0.0017", "0.042 \u00b1 0.004", "0.033 \u00b1 0.001", "0.1177 \u00b1 0.012", "0.0269 \u00b1 0.0049"], ["0.02", "0.0168 \u00b1 0.0016", "0.045 \u00b1 0.012", "0.031 \u00b1 0.002", "0.1085 \u00b1 0.010", "0.0274 \u00b1 0.0049"], ["0.05", "0.0181 \u00b1 0.0011", "0.052 \u00b1 0.013", "0.032 \u00b1 0.002", "0.1061 \u00b1 0.009", "0.0270 \u00b1 0.0049"], ["0.1", "0.0173 \u00b1 0.0014", "0.032 \u00b1 0.010", "0.027 \u00b1 0.002", "0.1018 \u00b1 0.002", "0.0275 \u00b1 0.0043"], ["0.2", "0.0172 \u00b1 0.0019", "0.031 \u00b1 0.004", "0.031 \u00b1 0.003", "0.1029 \u00b1 0.010", "0.0275 \u00b1 0.0046"], ["0.5", "0.0166 \u00b1 0.0011", "0.044 \u00b1 0.009", "0.031 \u00b1 0.005", "0.1017 \u00b1 0.017", "0.026826 \u00b1 0.0049"], ["1", "0.0175 \u00b1 0.0013", "0.043 \u00b1 0.011", "0.043 \u00b1 0.012", "0.1202 \u00b1 0.019", "0.026828 \u00b1 0.0044"], ["2", "0.0171 \u00b1 0.0011", "0.034 \u00b1 0.005", "0.032 \u00b1 0.001", "0.1044 \u00b1 0.011", "0.0275 \u00b1 0.0045"]], "md": "|\u03b1\\ Env|FetchReach|FetchPick|FetchPush|FetchSlide|HandReach|\n|---|---|---|---|---|---|\n|0.01|0.0171 \u00b1 0.0017|0.042 \u00b1 0.004|0.033 \u00b1 0.001|0.1177 \u00b1 0.012|0.0269 \u00b1 0.0049|\n|0.02|0.0168 \u00b1 0.0016|0.045 \u00b1 0.012|0.031 \u00b1 0.002|0.1085 \u00b1 0.010|0.0274 \u00b1 0.0049|\n|0.05|0.0181 \u00b1 0.0011|0.052 \u00b1 0.013|0.032 \u00b1 0.002|0.1061 \u00b1 0.009|0.0270 \u00b1 0.0049|\n|0.1|0.0173 \u00b1 0.0014|0.032 \u00b1 0.010|0.027 \u00b1 0.002|0.1018 \u00b1 0.002|0.0275 \u00b1 0.0043|\n|0.2|0.0172 \u00b1 0.0019|0.031 \u00b1 0.004|0.031 \u00b1 0.003|0.1029 \u00b1 0.010|0.0275 \u00b1 0.0046|\n|0.5|0.0166 \u00b1 0.0011|0.044 \u00b1 0.009|0.031 \u00b1 0.005|0.1017 \u00b1 0.017|0.026826 \u00b1 0.0049|\n|1|0.0175 \u00b1 0.0013|0.043 \u00b1 0.011|0.043 \u00b1 0.012|0.1202 \u00b1 0.019|0.026828 \u00b1 0.0044|\n|2|0.0171 \u00b1 0.0011|0.034 \u00b1 0.005|0.032 \u00b1 0.001|0.1044 \u00b1 0.011|0.0275 \u00b1 0.0045|", "isPerfectTable": true, "csv": "\"\u03b1\\ Env\",\"FetchReach\",\"FetchPick\",\"FetchPush\",\"FetchSlide\",\"HandReach\"\n\"0.01\",\"0.0171 \u00b1 0.0017\",\"0.042 \u00b1 0.004\",\"0.033 \u00b1 0.001\",\"0.1177 \u00b1 0.012\",\"0.0269 \u00b1 0.0049\"\n\"0.02\",\"0.0168 \u00b1 0.0016\",\"0.045 \u00b1 0.012\",\"0.031 \u00b1 0.002\",\"0.1085 \u00b1 0.010\",\"0.0274 \u00b1 0.0049\"\n\"0.05\",\"0.0181 \u00b1 0.0011\",\"0.052 \u00b1 0.013\",\"0.032 \u00b1 0.002\",\"0.1061 \u00b1 0.009\",\"0.0270 \u00b1 0.0049\"\n\"0.1\",\"0.0173 \u00b1 0.0014\",\"0.032 \u00b1 0.010\",\"0.027 \u00b1 0.002\",\"0.1018 \u00b1 0.002\",\"0.0275 \u00b1 0.0043\"\n\"0.2\",\"0.0172 \u00b1 0.0019\",\"0.031 \u00b1 0.004\",\"0.031 \u00b1 0.003\",\"0.1029 \u00b1 0.010\",\"0.0275 \u00b1 0.0046\"\n\"0.5\",\"0.0166 \u00b1 0.0011\",\"0.044 \u00b1 0.009\",\"0.031 \u00b1 0.005\",\"0.1017 \u00b1 0.017\",\"0.026826 \u00b1 0.0049\"\n\"1\",\"0.0175 \u00b1 0.0013\",\"0.043 \u00b1 0.011\",\"0.043 \u00b1 0.012\",\"0.1202 \u00b1 0.019\",\"0.026828 \u00b1 0.0044\"\n\"2\",\"0.0171 \u00b1 0.0011\",\"0.034 \u00b1 0.005\",\"0.032 \u00b1 0.001\",\"0.1044 \u00b1 0.011\",\"0.0275 \u00b1 0.0045\""}, {"type": "heading", "lvl": 2, "value": "Conclusions", "md": "## Conclusions"}, {"type": "text", "value": "In this paper, we theoretically analyze the VP-learning algorithm (Algorithm 1, which is based on the previous empirically successful algorithm in Ma et al. [2022c]) for both single-task and goal-conditioned offline settings. This algorithm can deal with general value function approximation and only requires near minimal assumptions on the dataset (single-policy concentrability) and function class (realizability). We also provide an O(1/N^1/12) upper bound of the suboptimality of the policy learned by the algorithm and empirically validate its effectiveness.\n\nAs for future directions, one important question is whether we can achieve the optimal suboptimality rate O(1/\u221aN) while keeping the algorithm practical without unreasonably strong assumptions.\n\nWe use the code at https://github.com/JasonMa2016/GoFAR with different values of \u03b1 for our experiments.", "md": "In this paper, we theoretically analyze the VP-learning algorithm (Algorithm 1, which is based on the previous empirically successful algorithm in Ma et al. [2022c]) for both single-task and goal-conditioned offline settings. This algorithm can deal with general value function approximation and only requires near minimal assumptions on the dataset (single-policy concentrability) and function class (realizability). We also provide an O(1/N^1/12) upper bound of the suboptimality of the policy learned by the algorithm and empirically validate its effectiveness.\n\nAs for future directions, one important question is whether we can achieve the optimal suboptimality rate O(1/\u221aN) while keeping the algorithm practical without unreasonably strong assumptions.\n\nWe use the code at https://github.com/JasonMa2016/GoFAR with different values of \u03b1 for our experiments."}]}, {"page": 11, "text": " Acknowledgements\nWe thank the anonymous reviewer for catching a technical issue in a previous version of our paper.\nThe work was done when HZ was a visiting researcher at Meta.\n References\n Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and\n   algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pages 10\u20134, 2019.\n Andras Antos, R\u00e9mi Munos, and Csaba Szepesvari. Fitted Q-iteration in continuous action-space\n   mdps. In Neural Information Processing Systems, 2007.\n Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with bellman-\n   residual minimization based fi   tted policy iteration and a single sample path. Machine Learning,\n   71(1):89\u2013129, 2008.\n Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan,\n   Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised offl           ine\n   reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.\n Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning.\n   arXiv preprint arXiv:1905.00360, 2019.\n Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal.                    Adversarially trained ac-\n   tor critic for offl ine reinforcement learning.         In Kamalika Chaudhuri, Stefanie Jegelka,\n   Le Song,      Csaba Szepesvari,      Gang Niu,      and Sivan Sabato,       editors,   Proceedings of\n   the 39th International Conference on Machine Learning, volume 162 of Proceedings\n   of Machine Learning Research,            pages 3852\u20133878. PMLR, 17\u201323 Jul 2022.                    URL\n   https://proceedings.mlr.press/v162/cheng22b.html.\n Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained\n   min-max optimization. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory\n   of Computing, pages 1466\u20131478, 2021.\n Amir Massoud Farahmand, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri. Error propagation for approximate\n   policy and value iteration. In Advances in Neural Information Processing Systems, 2010.\n Yihao Feng, Lihong Li, and Qiang Liu. A kernel loss for solving the Bellman equation. arXiv\n   preprint arXiv:1905.10506, 2019.\n Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach,\n   and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprint\n   arXiv:1912.06088, 2019.\n Nan Jiang.       On value functions and the agent-environment boundary.                   arXiv preprint\n   arXiv:1905.13341, 2019.\n Nan Jiang and Jiawei Huang. Minimax value interval for off-policy evaluation and policy optimiza-\n   tion. Advances in Neural Information Processing Systems, 33, 2020.\n Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably effi            cient for offl\n                                                                                              ine RL? In\n   International Conference on Machine Learning, pages 5084\u20135096. PMLR, 2021.\n Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, volume 2, pages 1094\u20138. Citeseer,\n   1993.\n Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok Yang,\n   and Kee-Eung Kim. Demodice: Offl        ine imitation learning with supplementary imperfect demon-\n   strations. In International Conference on Learning Representations, 2021.\n Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-\n   ment learning, pages 45\u201373. Springer, 2012.\n                                                    11", "md": "# Acknowledgements and References\n\n# Acknowledgements\n\nWe thank the anonymous reviewer for catching a technical issue in a previous version of our paper. The work was done when HZ was a visiting researcher at Meta.\n\n## References\n\n1. Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pages 10\u20134, 2019.\n2. Andras Antos, R\u00e9mi Munos, and Csaba Szepesvari. Fitted Q-iteration in continuous action-space mdps. In Neural Information Processing Systems, 2007.\n3. Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89\u2013129, 2008.\n4. Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.\n5. Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. arXiv preprint arXiv:1905.00360, 2019.\n\n|Author(s)|Title|Year|\n|---|---|---|\n|Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal|Adversarially trained actor critic for offline reinforcement learning|2022|\n|Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis|The complexity of constrained min-max optimization|2021|\n|Amir Massoud Farahmand, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri|Error propagation for approximate policy and value iteration|2010|\n|Yihao Feng, Lihong Li, and Qiang Liu|A kernel loss for solving the Bellman equation|2019|\n|Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine|Learning to reach goals via iterated supervised learning|2019|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgements and References", "md": "# Acknowledgements and References"}, {"type": "heading", "lvl": 1, "value": "Acknowledgements", "md": "# Acknowledgements"}, {"type": "text", "value": "We thank the anonymous reviewer for catching a technical issue in a previous version of our paper. The work was done when HZ was a visiting researcher at Meta.", "md": "We thank the anonymous reviewer for catching a technical issue in a previous version of our paper. The work was done when HZ was a visiting researcher at Meta."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pages 10\u20134, 2019.\n2. Andras Antos, R\u00e9mi Munos, and Csaba Szepesvari. Fitted Q-iteration in continuous action-space mdps. In Neural Information Processing Systems, 2007.\n3. Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89\u2013129, 2008.\n4. Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.\n5. Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. arXiv preprint arXiv:1905.00360, 2019.", "md": "1. Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, pages 10\u20134, 2019.\n2. Andras Antos, R\u00e9mi Munos, and Csaba Szepesvari. Fitted Q-iteration in continuous action-space mdps. In Neural Information Processing Systems, 2007.\n3. Andr\u00e1s Antos, Csaba Szepesv\u00e1ri, and R\u00e9mi Munos. Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89\u2013129, 2008.\n4. Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, et al. Actionable models: Unsupervised offline reinforcement learning of robotic skills. arXiv preprint arXiv:2104.07749, 2021.\n5. Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. arXiv preprint arXiv:1905.00360, 2019."}, {"type": "table", "rows": [["Author(s)", "Title", "Year"], ["Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal", "Adversarially trained actor critic for offline reinforcement learning", "2022"], ["Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis", "The complexity of constrained min-max optimization", "2021"], ["Amir Massoud Farahmand, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri", "Error propagation for approximate policy and value iteration", "2010"], ["Yihao Feng, Lihong Li, and Qiang Liu", "A kernel loss for solving the Bellman equation", "2019"], ["Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine", "Learning to reach goals via iterated supervised learning", "2019"]], "md": "|Author(s)|Title|Year|\n|---|---|---|\n|Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal|Adversarially trained actor critic for offline reinforcement learning|2022|\n|Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis|The complexity of constrained min-max optimization|2021|\n|Amir Massoud Farahmand, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri|Error propagation for approximate policy and value iteration|2010|\n|Yihao Feng, Lihong Li, and Qiang Liu|A kernel loss for solving the Bellman equation|2019|\n|Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine|Learning to reach goals via iterated supervised learning|2019|", "isPerfectTable": true, "csv": "\"Author(s)\",\"Title\",\"Year\"\n\"Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal\",\"Adversarially trained actor critic for offline reinforcement learning\",\"2022\"\n\"Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis\",\"The complexity of constrained min-max optimization\",\"2021\"\n\"Amir Massoud Farahmand, R\u00e9mi Munos, and Csaba Szepesv\u00e1ri\",\"Error propagation for approximate policy and value iteration\",\"2010\"\n\"Yihao Feng, Lihong Li, and Qiang Liu\",\"A kernel loss for solving the Bellman equation\",\"2019\"\n\"Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach, and Sergey Levine\",\"Learning to reach goals via iterated supervised learning\",\"2019\""}]}, {"page": 12, "text": "Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice: Offl         ine\n  policy optimization via stationary distribution correction estimation. In International Conference\n  on Machine Learning, pages 6120\u20136130. PMLR, 2021.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offl    ine reinforcement learning: Tuto-\n  rial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nGen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei. Settling the sample complexity of\n  model-based offl ine reinforcement learning. arXiv preprint arXiv:2204.05275, 2022.\nPeng Liao, Zhengling Qi, and Susan Murphy. Batch policy learning in average reward Markov\n  decision processes. arXiv preprint arXiv:2007.11771, 2020.\nBoyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimiza-\n  tion attains globally optimal policy. In Neural Information Processing Systems, 2019.\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and\n  Pierre Sermanet. Learning latent plans from play. In Conference on robot learning, pages 1113\u2013\n  1132. PMLR, 2020.\nYecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani. Smodice: Versatile offl     ine\n  imitation learning via state occupancy matching. arXiv preprint arXiv:2202.02433, 2022a.\nYecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy\n  Zhang. Vip: Towards universal visual reward and representation via value-implicit pre-training.\n  arXiv preprint arXiv:2210.00030, 2022b.\nYecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani. How far i\u2019ll go: Offl    ine goal-\n  conditioned reinforcement learning via f-advantage regression. arXiv preprint arXiv:2206.03023,\n  2022c.\nR\u00e9mi Munos. Performance bounds in \u2113p-norm for approximate value iteration. SIAM journal on\n  control and optimization, 46(2):541\u2013561, 2007.\nR\u00e9mi Munos and Csaba Szepesv\u00e1ri. Finite-time bounds for fi   tted value iteration. Journal of Machine\n  Learning Research, 9(5), 2008.\nOfir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of dis-\n  counted stationary distribution corrections. Advances in Neural Information Processing Systems,\n  32, 2019a.\nOfir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:\n  Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019b.\nMatthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Pow-\n  ell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al. Multi-goal reinforce-\n  ment learning: Challenging robotics environments and request for research.         arXiv preprint\n  arXiv:1802.09464, 2018.\nParia Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offl   ine rein-\n  forcement learning and imitation learning: A tale of pessimism. Advances in Neural Information\n  Processing Systems, 34:11702\u201311716, 2021.\nParia Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao. Optimal conser-\n  vative offline rl with general function approximation via augmented lagrangian. arXiv preprint\n  arXiv:2211.00716, 2022.\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approxima-\n  tors. In International conference on machine learning, pages 1312\u20131320. PMLR, 2015.\nBruno Scherrer. Approximate policy iteration schemes: A comparison. In International Conference\n  on Machine Learning, pages 1314\u20131322, 2014.\n                                                 12", "md": "# References\n\n# List of References\n\n|Authors|Title|Publication Details|\n|---|---|---|\n|Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim|Optidice: Offline policy optimization via stationary distribution correction estimation|International Conference on Machine Learning, pages 6120\u20136130, PMLR, 2021|\n|Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu|Offline reinforcement learning: Tutorial, review, and perspectives on open problems|arXiv preprint arXiv:2005.01643, 2020|\n|Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei|Settling the sample complexity of model-based offline reinforcement learning|arXiv preprint arXiv:2204.05275, 2022|\n|Peng Liao, Zhengling Qi, and Susan Murphy|Batch policy learning in average reward Markov decision processes|arXiv preprint arXiv:2007.11771, 2020|\n|Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang|Neural trust region/proximal policy optimization attains globally optimal policy|Neural Information Processing Systems, 2019|\n|Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet|Learning latent plans from play|Conference on robot learning, pages 1113\u20131132, PMLR, 2020|\n|Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani|Smodice: Versatile offline imitation learning via state occupancy matching|arXiv preprint arXiv:2202.02433, 2022a|\n|Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang|Vip: Towards universal visual reward and representation via value-implicit pre-training|arXiv preprint arXiv:2210.00030, 2022b|\n|Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani|How far I'll go: Offline goal-conditioned reinforcement learning via f-advantage regression|arXiv preprint arXiv:2206.03023, 2022c|\n|R\u00e9mi Munos|Performance bounds in $\\ell_p$-norm for approximate value iteration|SIAM journal on control and optimization, 46(2):541\u2013561, 2007|\n|R\u00e9mi Munos and Csaba Szepesv\u00e1ri|Finite-time bounds for fitted value iteration|Journal of Machine Learning Research, 9(5), 2008|\n|Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li|Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections|Advances in Neural Information Processing Systems, 32, 2019a|\n|Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans|Algaedice: Policy gradient from arbitrary experience|arXiv preprint arXiv:1912.02074, 2019b|\n|Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al.|Multi-goal reinforcement learning: Challenging robotics environments and request for research|arXiv preprint arXiv:1802.09464, 2018|\n|Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell|Bridging offline reinforcement learning and imitation learning: A tale of pessimism|Advances in Neural Information Processing Systems, 34:11702\u201311716, 2021|\n|Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao|Optimal conservative offline RL with general function approximation via augmented lagrangian|arXiv preprint arXiv:2211.00716, 2022|\n|Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver|Universal value function approximators|International conference on machine learning, pages 1312\u20131320, PMLR, 2015|\n|Bruno Scherrer|Approximate policy iteration schemes: A comparison|International Conference on Machine Learning, pages 1314\u20131322, 2014|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["Authors", "Title", "Publication Details"], ["Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim", "Optidice: Offline policy optimization via stationary distribution correction estimation", "International Conference on Machine Learning, pages 6120\u20136130, PMLR, 2021"], ["Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu", "Offline reinforcement learning: Tutorial, review, and perspectives on open problems", "arXiv preprint arXiv:2005.01643, 2020"], ["Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei", "Settling the sample complexity of model-based offline reinforcement learning", "arXiv preprint arXiv:2204.05275, 2022"], ["Peng Liao, Zhengling Qi, and Susan Murphy", "Batch policy learning in average reward Markov decision processes", "arXiv preprint arXiv:2007.11771, 2020"], ["Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang", "Neural trust region/proximal policy optimization attains globally optimal policy", "Neural Information Processing Systems, 2019"], ["Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet", "Learning latent plans from play", "Conference on robot learning, pages 1113\u20131132, PMLR, 2020"], ["Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani", "Smodice: Versatile offline imitation learning via state occupancy matching", "arXiv preprint arXiv:2202.02433, 2022a"], ["Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang", "Vip: Towards universal visual reward and representation via value-implicit pre-training", "arXiv preprint arXiv:2210.00030, 2022b"], ["Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani", "How far I'll go: Offline goal-conditioned reinforcement learning via f-advantage regression", "arXiv preprint arXiv:2206.03023, 2022c"], ["R\u00e9mi Munos", "Performance bounds in $\\ell_p$-norm for approximate value iteration", "SIAM journal on control and optimization, 46(2):541\u2013561, 2007"], ["R\u00e9mi Munos and Csaba Szepesv\u00e1ri", "Finite-time bounds for fitted value iteration", "Journal of Machine Learning Research, 9(5), 2008"], ["Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li", "Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections", "Advances in Neural Information Processing Systems, 32, 2019a"], ["Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans", "Algaedice: Policy gradient from arbitrary experience", "arXiv preprint arXiv:1912.02074, 2019b"], ["Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al.", "Multi-goal reinforcement learning: Challenging robotics environments and request for research", "arXiv preprint arXiv:1802.09464, 2018"], ["Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell", "Bridging offline reinforcement learning and imitation learning: A tale of pessimism", "Advances in Neural Information Processing Systems, 34:11702\u201311716, 2021"], ["Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao", "Optimal conservative offline RL with general function approximation via augmented lagrangian", "arXiv preprint arXiv:2211.00716, 2022"], ["Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver", "Universal value function approximators", "International conference on machine learning, pages 1312\u20131320, PMLR, 2015"], ["Bruno Scherrer", "Approximate policy iteration schemes: A comparison", "International Conference on Machine Learning, pages 1314\u20131322, 2014"]], "md": "|Authors|Title|Publication Details|\n|---|---|---|\n|Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim|Optidice: Offline policy optimization via stationary distribution correction estimation|International Conference on Machine Learning, pages 6120\u20136130, PMLR, 2021|\n|Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu|Offline reinforcement learning: Tutorial, review, and perspectives on open problems|arXiv preprint arXiv:2005.01643, 2020|\n|Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei|Settling the sample complexity of model-based offline reinforcement learning|arXiv preprint arXiv:2204.05275, 2022|\n|Peng Liao, Zhengling Qi, and Susan Murphy|Batch policy learning in average reward Markov decision processes|arXiv preprint arXiv:2007.11771, 2020|\n|Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang|Neural trust region/proximal policy optimization attains globally optimal policy|Neural Information Processing Systems, 2019|\n|Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet|Learning latent plans from play|Conference on robot learning, pages 1113\u20131132, PMLR, 2020|\n|Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani|Smodice: Versatile offline imitation learning via state occupancy matching|arXiv preprint arXiv:2202.02433, 2022a|\n|Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang|Vip: Towards universal visual reward and representation via value-implicit pre-training|arXiv preprint arXiv:2210.00030, 2022b|\n|Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani|How far I'll go: Offline goal-conditioned reinforcement learning via f-advantage regression|arXiv preprint arXiv:2206.03023, 2022c|\n|R\u00e9mi Munos|Performance bounds in $\\ell_p$-norm for approximate value iteration|SIAM journal on control and optimization, 46(2):541\u2013561, 2007|\n|R\u00e9mi Munos and Csaba Szepesv\u00e1ri|Finite-time bounds for fitted value iteration|Journal of Machine Learning Research, 9(5), 2008|\n|Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li|Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections|Advances in Neural Information Processing Systems, 32, 2019a|\n|Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans|Algaedice: Policy gradient from arbitrary experience|arXiv preprint arXiv:1912.02074, 2019b|\n|Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al.|Multi-goal reinforcement learning: Challenging robotics environments and request for research|arXiv preprint arXiv:1802.09464, 2018|\n|Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell|Bridging offline reinforcement learning and imitation learning: A tale of pessimism|Advances in Neural Information Processing Systems, 34:11702\u201311716, 2021|\n|Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao|Optimal conservative offline RL with general function approximation via augmented lagrangian|arXiv preprint arXiv:2211.00716, 2022|\n|Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver|Universal value function approximators|International conference on machine learning, pages 1312\u20131320, PMLR, 2015|\n|Bruno Scherrer|Approximate policy iteration schemes: A comparison|International Conference on Machine Learning, pages 1314\u20131322, 2014|", "isPerfectTable": true, "csv": "\"Authors\",\"Title\",\"Publication Details\"\n\"Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim\",\"Optidice: Offline policy optimization via stationary distribution correction estimation\",\"International Conference on Machine Learning, pages 6120\u20136130, PMLR, 2021\"\n\"Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu\",\"Offline reinforcement learning: Tutorial, review, and perspectives on open problems\",\"arXiv preprint arXiv:2005.01643, 2020\"\n\"Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, and Yuting Wei\",\"Settling the sample complexity of model-based offline reinforcement learning\",\"arXiv preprint arXiv:2204.05275, 2022\"\n\"Peng Liao, Zhengling Qi, and Susan Murphy\",\"Batch policy learning in average reward Markov decision processes\",\"arXiv preprint arXiv:2007.11771, 2020\"\n\"Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang\",\"Neural trust region/proximal policy optimization attains globally optimal policy\",\"Neural Information Processing Systems, 2019\"\n\"Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet\",\"Learning latent plans from play\",\"Conference on robot learning, pages 1113\u20131132, PMLR, 2020\"\n\"Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani\",\"Smodice: Versatile offline imitation learning via state occupancy matching\",\"arXiv preprint arXiv:2202.02433, 2022a\"\n\"Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, and Amy Zhang\",\"Vip: Towards universal visual reward and representation via value-implicit pre-training\",\"arXiv preprint arXiv:2210.00030, 2022b\"\n\"Yecheng Jason Ma, Jason Yan, Dinesh Jayaraman, and Osbert Bastani\",\"How far I'll go: Offline goal-conditioned reinforcement learning via f-advantage regression\",\"arXiv preprint arXiv:2206.03023, 2022c\"\n\"R\u00e9mi Munos\",\"Performance bounds in $\\ell_p$-norm for approximate value iteration\",\"SIAM journal on control and optimization, 46(2):541\u2013561, 2007\"\n\"R\u00e9mi Munos and Csaba Szepesv\u00e1ri\",\"Finite-time bounds for fitted value iteration\",\"Journal of Machine Learning Research, 9(5), 2008\"\n\"Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li\",\"Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections\",\"Advances in Neural Information Processing Systems, 32, 2019a\"\n\"Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans\",\"Algaedice: Policy gradient from arbitrary experience\",\"arXiv preprint arXiv:1912.02074, 2019b\"\n\"Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al.\",\"Multi-goal reinforcement learning: Challenging robotics environments and request for research\",\"arXiv preprint arXiv:1802.09464, 2018\"\n\"Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell\",\"Bridging offline reinforcement learning and imitation learning: A tale of pessimism\",\"Advances in Neural Information Processing Systems, 34:11702\u201311716, 2021\"\n\"Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao\",\"Optimal conservative offline RL with general function approximation via augmented lagrangian\",\"arXiv preprint arXiv:2211.00716, 2022\"\n\"Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver\",\"Universal value function approximators\",\"International conference on machine learning, pages 1312\u20131320, PMLR, 2015\"\n\"Bruno Scherrer\",\"Approximate policy iteration schemes: A comparison\",\"International Conference on Machine Learning, pages 1314\u20131322, 2014\""}]}, {"page": 13, "text": "Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic Q-learning for offl        ine\n   reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890,\n   2022.\nCsaba Szepesv\u00e1ri and R\u00e9mi Munos. Finite time bounds for sampling based fi      tted value iteration. In\n   Proceedings of the 22nd international conference on Machine learning, pages 880\u2013887, 2005.\nMasatoshi Uehara and Wen Sun.        Pessimistic model-based offl  ine reinforcement learning under\n   partial coverage. In International Conference on Learning Representations, 2021.\nMasatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and Q-function learning for off-\n   policy evaluation. In International Conference on Machine Learning, pages 9659\u20139668. PMLR,\n   2020.\nSara Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press,\n   2000.\nLingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global\n   optimality and rates of convergence. In International Conference on Learning Representations,\n   2019.\nTengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In Inter-\n   national Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021a.\nTengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In Inter-\n   national Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021b.\nTengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent\n   pessimism for offline reinforcement learning. Advances in neural information processing systems,\n   34:6683\u20136694, 2021.\nRui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie\n   Zhang. Rethinking goal-conditioned supervised learning and its connection to offl    ine rl. arXiv\n   preprint arXiv:2202.04478, 2022.\nMing Yin and Yu-Xiang Wang. Towards instance-optimal offl       ine reinforcement learning with pes-\n   simism. Advances in neural information processing systems, 34:4065\u20134078, 2021.\nMing Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offl        ine reinforcement learning via double\n   variance reduction. arXiv preprint arXiv:2102.01748, 2021.\nMing Yin, Wenjing Chen, Mengdi Wang, and Yu-Xiang Wang. Offl             ine stochastic shortest path:\n   Learning, evaluation and towards optimality. In Uncertainty in Artificial Intelligence, pages 2278\u2013\n   2288. PMLR, 2022.\nWenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offl             ine reinforcement\n   learning with realizability and single-policy concentrability. In Conference on Learning Theory,\n   pages 2730\u20132775. PMLR, 2022.\nJunyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang.                 Varia-\n   tional policy gradient method for reinforcement learning with general utilities. arXiv preprint\n   arXiv:2007.02151, 2020.\n                                                 13", "md": "# References\n\n## List of References:\n\n1. Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic Q-learning for offline reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890, 2022.\n2. Csaba Szepesv\u00e1ri and R\u00e9mi Munos. Finite time bounds for sampling based fitted value iteration. In Proceedings of the 22nd international conference on Machine learning, pages 880\u2013887, 2005.\n3. Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In International Conference on Learning Representations, 2021.\n4. Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and Q-function learning for off-policy evaluation. In International Conference on Machine Learning, pages 9659\u20139668. PMLR, 2020.\n5. Sara Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.\n6. Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. In International Conference on Learning Representations, 2019.\n7. Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021a.\n8. Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021b.\n9. Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683\u20136694, 2021.\n10. Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. arXiv preprint arXiv:2202.04478, 2022.\n11. Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism. Advances in neural information processing systems, 34:4065\u20134078, 2021.\n12. Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double variance reduction. arXiv preprint arXiv:2102.01748, 2021.\n13. Ming Yin, Wenjing Chen, Mengdi Wang, and Yu-Xiang Wang. Offline stochastic shortest path: Learning, evaluation and towards optimality. In Uncertainty in Artificial Intelligence, pages 2278\u20132288. PMLR, 2022.\n14. Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730\u20132775. PMLR, 2022.\n15. Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151, 2020.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "List of References:", "md": "## List of References:"}, {"type": "text", "value": "1. Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic Q-learning for offline reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890, 2022.\n2. Csaba Szepesv\u00e1ri and R\u00e9mi Munos. Finite time bounds for sampling based fitted value iteration. In Proceedings of the 22nd international conference on Machine learning, pages 880\u2013887, 2005.\n3. Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In International Conference on Learning Representations, 2021.\n4. Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and Q-function learning for off-policy evaluation. In International Conference on Machine Learning, pages 9659\u20139668. PMLR, 2020.\n5. Sara Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.\n6. Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. In International Conference on Learning Representations, 2019.\n7. Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021a.\n8. Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021b.\n9. Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683\u20136694, 2021.\n10. Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. arXiv preprint arXiv:2202.04478, 2022.\n11. Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism. Advances in neural information processing systems, 34:4065\u20134078, 2021.\n12. Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double variance reduction. arXiv preprint arXiv:2102.01748, 2021.\n13. Ming Yin, Wenjing Chen, Mengdi Wang, and Yu-Xiang Wang. Offline stochastic shortest path: Learning, evaluation and towards optimality. In Uncertainty in Artificial Intelligence, pages 2278\u20132288. PMLR, 2022.\n14. Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730\u20132775. PMLR, 2022.\n15. Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151, 2020.", "md": "1. Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic Q-learning for offline reinforcement learning: Towards optimal sample complexity. arXiv preprint arXiv:2202.13890, 2022.\n2. Csaba Szepesv\u00e1ri and R\u00e9mi Munos. Finite time bounds for sampling based fitted value iteration. In Proceedings of the 22nd international conference on Machine learning, pages 880\u2013887, 2005.\n3. Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In International Conference on Learning Representations, 2021.\n4. Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and Q-function learning for off-policy evaluation. In International Conference on Machine Learning, pages 9659\u20139668. PMLR, 2020.\n5. Sara Van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge university press, 2000.\n6. Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence. In International Conference on Learning Representations, 2019.\n7. Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021a.\n8. Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International Conference on Machine Learning, pages 11404\u201311413. PMLR, 2021b.\n9. Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34:6683\u20136694, 2021.\n10. Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han, and Chongjie Zhang. Rethinking goal-conditioned supervised learning and its connection to offline rl. arXiv preprint arXiv:2202.04478, 2022.\n11. Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism. Advances in neural information processing systems, 34:4065\u20134078, 2021.\n12. Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double variance reduction. arXiv preprint arXiv:2102.01748, 2021.\n13. Ming Yin, Wenjing Chen, Mengdi Wang, and Yu-Xiang Wang. Offline stochastic shortest path: Learning, evaluation and towards optimality. In Uncertainty in Artificial Intelligence, pages 2278\u20132288. PMLR, 2022.\n14. Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730\u20132775. PMLR, 2022.\n15. Junyu Zhang, Alec Koppel, Amrit Singh Bedi, Csaba Szepesvari, and Mengdi Wang. Variational policy gradient method for reinforcement learning with general utilities. arXiv preprint arXiv:2007.02151, 2020."}]}, {"page": 14, "text": "A      Missing Proofs for Propositions in Section 3\nA.1      Proof of Proposition 3.1\nProof. Since both d\u2217         \u03b1 and d\u2217     satisfy the Bellman fl         ow constraint (1), due to the optimality of d\u2217                  \u03b1 in\nthe regularized program (5), we have\n                  E  (s,g)\u223cd\u2217(s,g)[r(s; g)] + \u03b1Df(d\u2217\u2225\u00b5) \u2264                   E(s,g)\u223cd\u2217   \u03b1(s,g)[r(s; g)] + \u03b1Df(d\u2217           \u03b1\u2225\u00b5)\nTherefore,  =\u21d2E(s,g)\u223cd\u2217(s,g)[r(s; g)] \u2212               E(s,g)\u223cd\u2217   \u03b1(s,g)[r(s; g)] \u2264        \u03b1Df(d\u2217    \u03b1\u2225\u00b5) \u2264      \u03b1(C\u2217   \u03b1)2/2.\n           J(\u03c0\u2217) \u2212       J(\u03c0\u2217  \u03b1) = E(s,g)\u223cd\u2217(s,g)[r(s; g)] \u2212              E(s,g)\u223cd\u2217   \u03b1(s,g)[r(s; g)] \u2264        O    \u03b1(C\u2217  \u03b1)2    .\nA.2      Proof of Proposition 3.2\nProof. By Lagrangian duality, we can obtain that the dual problem is\n               min\n            V (s;g)\u22650 maxd(s,a;g)\u22650E(s,g)\u223cd(s,g)[r(s; g)] \u2212                    \u03b1Df(d(s, a; g)\u2225\u00b5(s, a; g))\n                                        \uf8eb                                                                                       \uf8f6        (15)\n            +    s,g  p(g)V (s; g)      \uf8ed(1 \u2212     \u03b3)\u03c1(s) + \u03b3       s\u2032,a\u2032  P (s|s\u2032, a\u2032)d(s\u2032, a\u2032; g) \u2212           a   d(s, a; g)   \uf8f8\nwhere p(g)V (s; g) is the Lagrangian vector. Similar to Ma et al. [2022c],\n                                          s,g  p(g)V (s; g)      s\u2032,a\u2032  P (s|s\u2032, a\u2032)d(s\u2032, a\u2032; g)\n                                      =  s\u2032,a\u2032,g  p(g)d(s\u2032, a\u2032; g)        s   P  (s|s\u2032, a\u2032)V (s; g)\nThen (15) is equivalent to            =  s\u2032,a\u2032,g  p(g)d(s\u2032, a\u2032; g)T V (s\u2032, a\u2032; g).\n   min\nV (s;g)\u22650 maxd(s,a;g)\u22650(1 \u2212             \u03b3)E(s,g)\u223c(\u03c1,p(g))[V (s; g)]+E(s,a,g)\u223cd[r(s; g) + \u03b3T V (s, a; g) \u2212                             V (s; g)]\nwhich can be further represented as                                          \u2212\u03b1Df      (d(s, a; g)\u2225\u00b5(s, a; g)),\n   min\nV (s;g)\u22650(1 \u2212      \u03b3)E(s,g)\u223c(\u03c1,p(g))[V (s; g)]+maxd(s,a;g)\u22650E(s,a,g)\u223cd[r(s; g) + \u03b3T V (s, a; g) \u2212                                    V (s; g)]\nwhere g = \u03b1 \u00b7 f. Combining the constraint that d(s, a; g) \u2265 \u2212  Dg(d(s, a; g)\u2225\u00b5(s, a; g)),   0 and the proof of Proposition 4.3\n(Proposition B.2) of Ma et al. [2022c], the above program is equivalent to\n      min                                                                             \u2217(AV (s, a; g)) \u2265        0}\u00af g\u2217(AV (s, a; g))],\n   V (s;g)\u22650(1 \u2212      \u03b3)E(s,g)\u223c(\u03c1,p(g))[V (s; g)] + E(s,a,g)\u223c\u00b5[1{g\u2032\nand it holds that\n                        d\u2217\u03b1(s, a; g) = \u00b5(s, a; g)g\u2032        \u2217(r(s; g) + \u03b3T V \u2217      \u03b1 (s, a; g) \u2212      V \u2217\n                                                                                                        \u03b1 (s; g))+.\nB      Proof for V -Learning\nIn this section, we provide theoretical analysis for the guarantees of V -learning in different settings\n(deterministic dynamics in Appendix B.1 and stochastic dynamics in Appendix B.2).\n                                                                      14", "md": "# Missing Proofs for Propositions in Section 3\n\n## A. Missing Proofs for Propositions in Section 3\n\n### A.1 Proof of Proposition 3.1\n\nProof. Since both \\(d^*\\) and \\(d^*_\\alpha\\) satisfy the Bellman flow constraint (1), due to the optimality of \\(d^*_\\alpha\\) in the regularized program (5), we have\n\n$$\n\\begin{align*}\n&E(s,g)\\sim d^*(s,g)[r(s; g)] + \\alpha D_f(d^* \\| \\mu) \\leq E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] + \\alpha D_f(d^*_\\alpha \\| \\mu) \\\\\n\\Rightarrow &E(s,g)\\sim d^*(s,g)[r(s; g)] - E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] \\leq \\alpha D_f(d^*_\\alpha \\| \\mu) \\leq \\alpha(C^*_\\alpha)^2/2.\n\\end{align*}\n$$\n\\(J(\\pi^*) - J(\\pi^*_\\alpha) = E(s,g)\\sim d^*(s,g)[r(s; g)] - E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] \\leq O(\\alpha(C^*_\\alpha)^2)\\).\n\n### A.2 Proof of Proposition 3.2\n\nProof. By Lagrangian duality, we can obtain that the dual problem is\n\n$$\n\\begin{align*}\n&\\min_{V(s;g)\\geq 0} \\max_{d(s,a;g)\\geq 0} E(s,g)\\sim d(s,g)[r(s; g)] - \\alpha D_f(d(s, a; g) \\| \\mu(s, a; g)) \\\\\n&+ \\sum_{s,g} p(g)V(s; g) \\left[ (1 - \\gamma)\\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g) - \\sum_a d(s, a; g) \\right]\n\\end{align*}\n$$\nwhere \\(p(g)V(s; g)\\) is the Lagrangian vector. Similar to Ma et al. [2022c],\n\n$$\n\\begin{align*}\n&\\sum_{s,g} p(g)V(s; g) \\sum_{s',a'} P(s|s', a')d(s', a'; g) \\\\\n&= \\sum_{s',a',g} p(g)d(s', a'; g) P(s|s', a')V(s; g)\n\\end{align*}\n$$\nThen the dual problem is equivalent to\n\n$$\n\\min_{V(s;g)\\geq 0} \\max_{d(s,a;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + E(s,a,g)\\sim d[r(s; g) + \\gamma^T V(s, a; g) - V(s; g)]\n$$\nwhich can be further represented as\n\n$$\n\\min_{V(s;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + \\max_{d(s,a;g)\\geq 0} E(s,a,g)\\sim d[r(s; g) + \\gamma^T V(s, a; g) - V(s; g)]\n$$\nwhere \\(g = \\alpha \\cdot f\\). Combining the constraint that \\(d(s, a; g) \\geq -D_g(d(s, a; g) \\| \\mu(s, a; g)), 0\\) and the proof of Proposition 4.3 (Proposition B.2) of Ma et al. [2022c], the above program is equivalent to\n\n$$\n\\min_{V(s;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + E(s,a,g)\\sim \\mu[1\\{g' \\geq \\ast(AV(s, a; g)) \\geq 0\\} - \\bar{g}^{\\ast}(AV(s, a; g))],\n$$\nand it holds that\n\n$$\nd^*_\\alpha(s, a; g) = \\mu(s, a; g)g' + \\ast(r(s; g) + \\gamma^T V^*_\\alpha(s, a; g) - V^*_\\alpha(s; g)).\n$$\n\n## B. Proof for V-Learning\n\nIn this section, we provide theoretical analysis for the guarantees of V-learning in different settings (deterministic dynamics in Appendix B.1 and stochastic dynamics in Appendix B.2).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Missing Proofs for Propositions in Section 3", "md": "# Missing Proofs for Propositions in Section 3"}, {"type": "heading", "lvl": 2, "value": "A. Missing Proofs for Propositions in Section 3", "md": "## A. Missing Proofs for Propositions in Section 3"}, {"type": "heading", "lvl": 3, "value": "A.1 Proof of Proposition 3.1", "md": "### A.1 Proof of Proposition 3.1"}, {"type": "text", "value": "Proof. Since both \\(d^*\\) and \\(d^*_\\alpha\\) satisfy the Bellman flow constraint (1), due to the optimality of \\(d^*_\\alpha\\) in the regularized program (5), we have\n\n$$\n\\begin{align*}\n&E(s,g)\\sim d^*(s,g)[r(s; g)] + \\alpha D_f(d^* \\| \\mu) \\leq E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] + \\alpha D_f(d^*_\\alpha \\| \\mu) \\\\\n\\Rightarrow &E(s,g)\\sim d^*(s,g)[r(s; g)] - E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] \\leq \\alpha D_f(d^*_\\alpha \\| \\mu) \\leq \\alpha(C^*_\\alpha)^2/2.\n\\end{align*}\n$$\n\\(J(\\pi^*) - J(\\pi^*_\\alpha) = E(s,g)\\sim d^*(s,g)[r(s; g)] - E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] \\leq O(\\alpha(C^*_\\alpha)^2)\\).", "md": "Proof. Since both \\(d^*\\) and \\(d^*_\\alpha\\) satisfy the Bellman flow constraint (1), due to the optimality of \\(d^*_\\alpha\\) in the regularized program (5), we have\n\n$$\n\\begin{align*}\n&E(s,g)\\sim d^*(s,g)[r(s; g)] + \\alpha D_f(d^* \\| \\mu) \\leq E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] + \\alpha D_f(d^*_\\alpha \\| \\mu) \\\\\n\\Rightarrow &E(s,g)\\sim d^*(s,g)[r(s; g)] - E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] \\leq \\alpha D_f(d^*_\\alpha \\| \\mu) \\leq \\alpha(C^*_\\alpha)^2/2.\n\\end{align*}\n$$\n\\(J(\\pi^*) - J(\\pi^*_\\alpha) = E(s,g)\\sim d^*(s,g)[r(s; g)] - E(s,g)\\sim d^*_\\alpha(s,g)[r(s; g)] \\leq O(\\alpha(C^*_\\alpha)^2)\\)."}, {"type": "heading", "lvl": 3, "value": "A.2 Proof of Proposition 3.2", "md": "### A.2 Proof of Proposition 3.2"}, {"type": "text", "value": "Proof. By Lagrangian duality, we can obtain that the dual problem is\n\n$$\n\\begin{align*}\n&\\min_{V(s;g)\\geq 0} \\max_{d(s,a;g)\\geq 0} E(s,g)\\sim d(s,g)[r(s; g)] - \\alpha D_f(d(s, a; g) \\| \\mu(s, a; g)) \\\\\n&+ \\sum_{s,g} p(g)V(s; g) \\left[ (1 - \\gamma)\\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g) - \\sum_a d(s, a; g) \\right]\n\\end{align*}\n$$\nwhere \\(p(g)V(s; g)\\) is the Lagrangian vector. Similar to Ma et al. [2022c],\n\n$$\n\\begin{align*}\n&\\sum_{s,g} p(g)V(s; g) \\sum_{s',a'} P(s|s', a')d(s', a'; g) \\\\\n&= \\sum_{s',a',g} p(g)d(s', a'; g) P(s|s', a')V(s; g)\n\\end{align*}\n$$\nThen the dual problem is equivalent to\n\n$$\n\\min_{V(s;g)\\geq 0} \\max_{d(s,a;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + E(s,a,g)\\sim d[r(s; g) + \\gamma^T V(s, a; g) - V(s; g)]\n$$\nwhich can be further represented as\n\n$$\n\\min_{V(s;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + \\max_{d(s,a;g)\\geq 0} E(s,a,g)\\sim d[r(s; g) + \\gamma^T V(s, a; g) - V(s; g)]\n$$\nwhere \\(g = \\alpha \\cdot f\\). Combining the constraint that \\(d(s, a; g) \\geq -D_g(d(s, a; g) \\| \\mu(s, a; g)), 0\\) and the proof of Proposition 4.3 (Proposition B.2) of Ma et al. [2022c], the above program is equivalent to\n\n$$\n\\min_{V(s;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + E(s,a,g)\\sim \\mu[1\\{g' \\geq \\ast(AV(s, a; g)) \\geq 0\\} - \\bar{g}^{\\ast}(AV(s, a; g))],\n$$\nand it holds that\n\n$$\nd^*_\\alpha(s, a; g) = \\mu(s, a; g)g' + \\ast(r(s; g) + \\gamma^T V^*_\\alpha(s, a; g) - V^*_\\alpha(s; g)).\n$$", "md": "Proof. By Lagrangian duality, we can obtain that the dual problem is\n\n$$\n\\begin{align*}\n&\\min_{V(s;g)\\geq 0} \\max_{d(s,a;g)\\geq 0} E(s,g)\\sim d(s,g)[r(s; g)] - \\alpha D_f(d(s, a; g) \\| \\mu(s, a; g)) \\\\\n&+ \\sum_{s,g} p(g)V(s; g) \\left[ (1 - \\gamma)\\rho(s) + \\gamma \\sum_{s',a'} P(s|s', a')d(s', a'; g) - \\sum_a d(s, a; g) \\right]\n\\end{align*}\n$$\nwhere \\(p(g)V(s; g)\\) is the Lagrangian vector. Similar to Ma et al. [2022c],\n\n$$\n\\begin{align*}\n&\\sum_{s,g} p(g)V(s; g) \\sum_{s',a'} P(s|s', a')d(s', a'; g) \\\\\n&= \\sum_{s',a',g} p(g)d(s', a'; g) P(s|s', a')V(s; g)\n\\end{align*}\n$$\nThen the dual problem is equivalent to\n\n$$\n\\min_{V(s;g)\\geq 0} \\max_{d(s,a;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + E(s,a,g)\\sim d[r(s; g) + \\gamma^T V(s, a; g) - V(s; g)]\n$$\nwhich can be further represented as\n\n$$\n\\min_{V(s;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + \\max_{d(s,a;g)\\geq 0} E(s,a,g)\\sim d[r(s; g) + \\gamma^T V(s, a; g) - V(s; g)]\n$$\nwhere \\(g = \\alpha \\cdot f\\). Combining the constraint that \\(d(s, a; g) \\geq -D_g(d(s, a; g) \\| \\mu(s, a; g)), 0\\) and the proof of Proposition 4.3 (Proposition B.2) of Ma et al. [2022c], the above program is equivalent to\n\n$$\n\\min_{V(s;g)\\geq 0} (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[V(s; g)] + E(s,a,g)\\sim \\mu[1\\{g' \\geq \\ast(AV(s, a; g)) \\geq 0\\} - \\bar{g}^{\\ast}(AV(s, a; g))],\n$$\nand it holds that\n\n$$\nd^*_\\alpha(s, a; g) = \\mu(s, a; g)g' + \\ast(r(s; g) + \\gamma^T V^*_\\alpha(s, a; g) - V^*_\\alpha(s; g)).\n$$"}, {"type": "heading", "lvl": 2, "value": "B. Proof for V-Learning", "md": "## B. Proof for V-Learning"}, {"type": "text", "value": "In this section, we provide theoretical analysis for the guarantees of V-learning in different settings (deterministic dynamics in Appendix B.1 and stochastic dynamics in Appendix B.2).", "md": "In this section, we provide theoretical analysis for the guarantees of V-learning in different settings (deterministic dynamics in Appendix B.1 and stochastic dynamics in Appendix B.2)."}]}, {"page": 15, "text": "Recall that we choose f(x) =                      (x\u22121)2    , and thus f\u2217(x) =               (x+1)2     \u2212   1     Therefore, we have\n                                  \u03b1(x\u22121)2             2                 \u03b1(x/\u03b1+1)2             \u03b1  2          2.\ng(x)      =    \u03b1f(x)        =          2        and g\u2217(x)         =            2         \u2212    2 .    Also, we defi         ne g\u2217,max        =\nmax    v\u2208[\u2212Vmax,Vmax+1]g\u2217(v), g\u2217,min = minv\u2208[\u2212Vmax,Vmax+1] g\u2217(v) and g\u2217,\u2206                                     = g\u2217,max \u2212        g\u2217,min. We\ncan easily obtain that g\u2217,\u2206            \u2264   \u03b1(1 + Vmax  \u03b1 )2 = O(V 2       max/\u03b1).\nFor the purpose of theoretical analysis, we further defi                      ne\n                         L1(V ) =(1 \u2212         \u03b3)E(s,g)\u223c(\u03c1,p(g))[\u03b1 \u00b7 V (s; g)],\nand                      L2(V ) =E(s,a,g)\u223c\u00b5[\u03b1 \u00b7 g\u2217+(r(s; g) + \u03b3T V (s, a; g) \u2212                          V (s; g))],\n                           \u02c6                         N0   \u03b1 \u00b7 V (s0,i; g0,i),\n                           L(d)\n                             1 (V ) =1 \u2212     N0  \u03b3   i=1\n                           \u02c6                     N\n                           L(d)                      \u03b1 \u00b7 g\u2217+(r(si; gi) + \u03b3V (s\u2032           i; gi) \u2212   V (si; gi)).\n                             2 (V ) = 1    N    i=1\nNote that L\u03b1(V ) = L1(V ) + L2(V ) and \u02c6                    L(d)   (V ) = \u02c6  L(d)             L(d)\n                                                                                1 (V ) + \u02c6      2 (V ).\nB.1      Proof for Deterministic Dynamics\nWe fi  rst show that with high probability, the estimator \u02c6                   L(d)   (V ) in Algorithm 2 concentrates well on\nL\u03b1(V ) for all V \u2208          V.\nLemma 3 (Concentration of \u02c6                 L(d)    L(d)\n                                              1 , \u02c6   2 ). Under Assumptions 1 and 2, when the dynamic of the\nenvironment is deterministic, with probability at\uf8ebleast 1 \u2212        log(|V|/\u03b4)      \uf8f6\u03b4, \u2200V \u2208      V, it simultaneously holds that\n              \u02c6L(d)                        \u2264O    \uf8ed\u03b1Vmax                            \uf8f8  ,\n                 1 (V ) \u2212      L1(V )                                   N  0\n               L(d)                                 \u03b1 \u00b7 g\u2217,\u2206        log(|V|/\u03b4)          = O       V 2         log(|V|/\u03b4)         ,\n              \u02c6  2 (V ) \u2212      L2(V )      \u2264O                             N                         max             N\nwhich immediately implies\n                                                   \uf8eb                log(|V|/\u03b4)                      log(|V|/\u03b4)      \uf8f6\n              L(d)   (V ) \u2212    L\u03b1(V )              \uf8ed\u03b1Vmax                            + V 2                          \uf8f8   \u225c   \u01ebstat.\n              \u02c6                            \u2264   O                         N0               max             N\nProof. First, fi     x any V \u2208       V. The expectation of \u02c6          L(d)\n                                                                        1 (V ) is\n                            E[\u02c6L(d)                        N0   E(s0,i,g0,i)\u223c(\u03c1,p(g))[\u03b1 \u00b7 V (s0,i; g0,i)]\n                                  1 (V )] =1 \u2212     N0  \u03b3  i=1\n                                             =(1 \u2212      \u03b3)E(s,g)\u223c(\u03c1,p(g))[\u03b1 \u00b7 V (s; g)]\n                                             =L1(V ).\nAlso,\n           E[\u02c6L(d)                   N    E (si,ai,gi)\u223c\u00b5[\u03b1 \u00b7 g\u2217+(r(si; gi) + \u03b3V (s\u2032               i; gi) \u2212   V (si; gi))]\n                 2 (V )] = 1    N   i=1\n                                     N\n                            = 1 N   i=1   E (si,ai,gi)\u223c\u00b5[\u03b1 \u00b7 g\u2217+(r(si; gi) + \u03b3T V (si, ai; gi) \u2212                     V (si; gi))]\n                            =L2(V ).\n                                                                      15", "md": "Recall that we choose \\( f(x) = (x-1)^2 \\), and thus \\( f^*(x) = (x+1)^2 - 1 \\). Therefore, we have\n\n$$\n\\begin{align*}\ng(x) &= \\alpha f(x) = 2 \\\\\ng^*(x) &= 2 - 2\n\\end{align*}\n$$\nAlso, we define \\( g^*_{\\text{max}} = \\max_{v\\in[-V_{\\text{max}},V_{\\text{max}}+1]} g^*(v) \\), \\( g^*_{\\text{min}} = \\min_{v\\in[-V_{\\text{max}},V_{\\text{max}}+1]} g^*(v) \\), and \\( g^*_{\\Delta} = g^*_{\\text{max}} - g^*_{\\text{min}} \\). We can easily obtain that \\( g^*_{\\Delta} \\leq \\alpha(1 + V_{\\text{max}}/\\alpha)^2 = O(V_{\\text{max}}^2/\\alpha) \\).\n\nFor the purpose of theoretical analysis, we further define\n\n$$\n\\begin{align*}\nL_1(V) &= (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[\\alpha \\cdot V(s;g)], \\\\\nL_2(V) &= E(s,a,g)\\sim\\mu[\\alpha \\cdot g^*+(r(s;g) + \\gamma T V(s,a;g) - V(s;g))], \\\\\n\\hat{N}_0 &= \\alpha \\cdot V(s_0,i;g_0,i), \\\\\nL(d)_1(V) &= 1 - \\hat{N}_0 \\gamma \\sum_{i=1}^{N} \\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i)), \\\\\nL(d)_2(V) &= \\frac{1}{N} \\sum_{i=1}^{N} \\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i)).\n\\end{align*}\n$$\nNote that \\( L_{\\alpha}(V) = L_1(V) + L_2(V) \\) and \\( \\hat{L}(d)(V) = \\hat{L}(d)_1(V) + \\hat{L}(d)_2(V) \\).\n\nB.1 Proof for Deterministic Dynamics\n\nWe first show that with high probability, the estimator \\( \\hat{L}(d)(V) \\) in Algorithm 2 concentrates well on \\( L_{\\alpha}(V) \\) for all \\( V \\in V \\).\n\nLemma 3 (Concentration of \\( \\hat{L}(d)_1, \\hat{L}(d)_2 \\) ): Under Assumptions 1 and 2, when the dynamic of the environment is deterministic, with probability at least \\( 1 - \\log(|V|/\\delta) \\delta \\), \\( \\forall V \\in V \\), it simultaneously holds that\n\n$$\n\\begin{align*}\n\\hat{L}(d)_1(V) - L_1(V) &\\leq O(\\alpha V_{\\text{max}}) \\log(|V|/\\delta) = O(V^2_{\\text{max}} \\log(|V|/\\delta)), \\\\\n\\hat{L}(d)_2(V) - L_2(V) &\\leq O(N) \\log(|V|/\\delta) = O(V_{\\text{max}} N \\log(|V|/\\delta)),\n\\end{align*}\n$$\nwhich immediately implies\n\n$$\nL(d)(V) - L_{\\alpha}(V) \\leq O\\left(\\alpha V_{\\text{max}} + \\frac{V^2_{\\text{max}}}{N}\\right) \\triangleq \\epsilon_{\\text{stat}}.\n$$\nProof: First, fix any \\( V \\in V \\). The expectation of \\( \\hat{L}(d)_1(V) \\) is\n\n$$\n\\begin{align*}\nE[\\hat{L}(d)_1(V)] &= E\\left[\\hat{N}_0 E(s_0,i,g_0,i)\\sim(\\rho,p(g))[\\alpha \\cdot V(s_0,i;g_0,i)]\\right] \\\\\n&= 1 - \\hat{N}_0 \\gamma \\sum_{i=1}^{N} E(s,g)\\sim(\\rho,p(g))[\\alpha \\cdot V(s;g)] \\\\\n&= L_1(V).\n\\end{align*}\n$$\nAlso,\n\n$$\n\\begin{align*}\nE[\\hat{L}(d)_2(V)] &= \\frac{1}{N} \\sum_{i=1}^{N} E(s,a,g)\\sim\\mu[\\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i))] \\\\\n&= L_2(V).\n\\end{align*}\n$$\n15", "images": [], "items": [{"type": "text", "value": "Recall that we choose \\( f(x) = (x-1)^2 \\), and thus \\( f^*(x) = (x+1)^2 - 1 \\). Therefore, we have\n\n$$\n\\begin{align*}\ng(x) &= \\alpha f(x) = 2 \\\\\ng^*(x) &= 2 - 2\n\\end{align*}\n$$\nAlso, we define \\( g^*_{\\text{max}} = \\max_{v\\in[-V_{\\text{max}},V_{\\text{max}}+1]} g^*(v) \\), \\( g^*_{\\text{min}} = \\min_{v\\in[-V_{\\text{max}},V_{\\text{max}}+1]} g^*(v) \\), and \\( g^*_{\\Delta} = g^*_{\\text{max}} - g^*_{\\text{min}} \\). We can easily obtain that \\( g^*_{\\Delta} \\leq \\alpha(1 + V_{\\text{max}}/\\alpha)^2 = O(V_{\\text{max}}^2/\\alpha) \\).\n\nFor the purpose of theoretical analysis, we further define\n\n$$\n\\begin{align*}\nL_1(V) &= (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[\\alpha \\cdot V(s;g)], \\\\\nL_2(V) &= E(s,a,g)\\sim\\mu[\\alpha \\cdot g^*+(r(s;g) + \\gamma T V(s,a;g) - V(s;g))], \\\\\n\\hat{N}_0 &= \\alpha \\cdot V(s_0,i;g_0,i), \\\\\nL(d)_1(V) &= 1 - \\hat{N}_0 \\gamma \\sum_{i=1}^{N} \\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i)), \\\\\nL(d)_2(V) &= \\frac{1}{N} \\sum_{i=1}^{N} \\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i)).\n\\end{align*}\n$$\nNote that \\( L_{\\alpha}(V) = L_1(V) + L_2(V) \\) and \\( \\hat{L}(d)(V) = \\hat{L}(d)_1(V) + \\hat{L}(d)_2(V) \\).\n\nB.1 Proof for Deterministic Dynamics\n\nWe first show that with high probability, the estimator \\( \\hat{L}(d)(V) \\) in Algorithm 2 concentrates well on \\( L_{\\alpha}(V) \\) for all \\( V \\in V \\).\n\nLemma 3 (Concentration of \\( \\hat{L}(d)_1, \\hat{L}(d)_2 \\) ): Under Assumptions 1 and 2, when the dynamic of the environment is deterministic, with probability at least \\( 1 - \\log(|V|/\\delta) \\delta \\), \\( \\forall V \\in V \\), it simultaneously holds that\n\n$$\n\\begin{align*}\n\\hat{L}(d)_1(V) - L_1(V) &\\leq O(\\alpha V_{\\text{max}}) \\log(|V|/\\delta) = O(V^2_{\\text{max}} \\log(|V|/\\delta)), \\\\\n\\hat{L}(d)_2(V) - L_2(V) &\\leq O(N) \\log(|V|/\\delta) = O(V_{\\text{max}} N \\log(|V|/\\delta)),\n\\end{align*}\n$$\nwhich immediately implies\n\n$$\nL(d)(V) - L_{\\alpha}(V) \\leq O\\left(\\alpha V_{\\text{max}} + \\frac{V^2_{\\text{max}}}{N}\\right) \\triangleq \\epsilon_{\\text{stat}}.\n$$\nProof: First, fix any \\( V \\in V \\). The expectation of \\( \\hat{L}(d)_1(V) \\) is\n\n$$\n\\begin{align*}\nE[\\hat{L}(d)_1(V)] &= E\\left[\\hat{N}_0 E(s_0,i,g_0,i)\\sim(\\rho,p(g))[\\alpha \\cdot V(s_0,i;g_0,i)]\\right] \\\\\n&= 1 - \\hat{N}_0 \\gamma \\sum_{i=1}^{N} E(s,g)\\sim(\\rho,p(g))[\\alpha \\cdot V(s;g)] \\\\\n&= L_1(V).\n\\end{align*}\n$$\nAlso,\n\n$$\n\\begin{align*}\nE[\\hat{L}(d)_2(V)] &= \\frac{1}{N} \\sum_{i=1}^{N} E(s,a,g)\\sim\\mu[\\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i))] \\\\\n&= L_2(V).\n\\end{align*}\n$$\n15", "md": "Recall that we choose \\( f(x) = (x-1)^2 \\), and thus \\( f^*(x) = (x+1)^2 - 1 \\). Therefore, we have\n\n$$\n\\begin{align*}\ng(x) &= \\alpha f(x) = 2 \\\\\ng^*(x) &= 2 - 2\n\\end{align*}\n$$\nAlso, we define \\( g^*_{\\text{max}} = \\max_{v\\in[-V_{\\text{max}},V_{\\text{max}}+1]} g^*(v) \\), \\( g^*_{\\text{min}} = \\min_{v\\in[-V_{\\text{max}},V_{\\text{max}}+1]} g^*(v) \\), and \\( g^*_{\\Delta} = g^*_{\\text{max}} - g^*_{\\text{min}} \\). We can easily obtain that \\( g^*_{\\Delta} \\leq \\alpha(1 + V_{\\text{max}}/\\alpha)^2 = O(V_{\\text{max}}^2/\\alpha) \\).\n\nFor the purpose of theoretical analysis, we further define\n\n$$\n\\begin{align*}\nL_1(V) &= (1 - \\gamma)E(s,g)\\sim(\\rho,p(g))[\\alpha \\cdot V(s;g)], \\\\\nL_2(V) &= E(s,a,g)\\sim\\mu[\\alpha \\cdot g^*+(r(s;g) + \\gamma T V(s,a;g) - V(s;g))], \\\\\n\\hat{N}_0 &= \\alpha \\cdot V(s_0,i;g_0,i), \\\\\nL(d)_1(V) &= 1 - \\hat{N}_0 \\gamma \\sum_{i=1}^{N} \\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i)), \\\\\nL(d)_2(V) &= \\frac{1}{N} \\sum_{i=1}^{N} \\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i)).\n\\end{align*}\n$$\nNote that \\( L_{\\alpha}(V) = L_1(V) + L_2(V) \\) and \\( \\hat{L}(d)(V) = \\hat{L}(d)_1(V) + \\hat{L}(d)_2(V) \\).\n\nB.1 Proof for Deterministic Dynamics\n\nWe first show that with high probability, the estimator \\( \\hat{L}(d)(V) \\) in Algorithm 2 concentrates well on \\( L_{\\alpha}(V) \\) for all \\( V \\in V \\).\n\nLemma 3 (Concentration of \\( \\hat{L}(d)_1, \\hat{L}(d)_2 \\) ): Under Assumptions 1 and 2, when the dynamic of the environment is deterministic, with probability at least \\( 1 - \\log(|V|/\\delta) \\delta \\), \\( \\forall V \\in V \\), it simultaneously holds that\n\n$$\n\\begin{align*}\n\\hat{L}(d)_1(V) - L_1(V) &\\leq O(\\alpha V_{\\text{max}}) \\log(|V|/\\delta) = O(V^2_{\\text{max}} \\log(|V|/\\delta)), \\\\\n\\hat{L}(d)_2(V) - L_2(V) &\\leq O(N) \\log(|V|/\\delta) = O(V_{\\text{max}} N \\log(|V|/\\delta)),\n\\end{align*}\n$$\nwhich immediately implies\n\n$$\nL(d)(V) - L_{\\alpha}(V) \\leq O\\left(\\alpha V_{\\text{max}} + \\frac{V^2_{\\text{max}}}{N}\\right) \\triangleq \\epsilon_{\\text{stat}}.\n$$\nProof: First, fix any \\( V \\in V \\). The expectation of \\( \\hat{L}(d)_1(V) \\) is\n\n$$\n\\begin{align*}\nE[\\hat{L}(d)_1(V)] &= E\\left[\\hat{N}_0 E(s_0,i,g_0,i)\\sim(\\rho,p(g))[\\alpha \\cdot V(s_0,i;g_0,i)]\\right] \\\\\n&= 1 - \\hat{N}_0 \\gamma \\sum_{i=1}^{N} E(s,g)\\sim(\\rho,p(g))[\\alpha \\cdot V(s;g)] \\\\\n&= L_1(V).\n\\end{align*}\n$$\nAlso,\n\n$$\n\\begin{align*}\nE[\\hat{L}(d)_2(V)] &= \\frac{1}{N} \\sum_{i=1}^{N} E(s,a,g)\\sim\\mu[\\alpha \\cdot g^*+(r(s_i;g_i) + \\gamma V(s'_i;g_i) - V(s_i;g_i))] \\\\\n&= L_2(V).\n\\end{align*}\n$$\n15"}]}, {"page": 16, "text": " Also note that V (s0,i; g0,i) \u2208            [0, Vmax] and g\u2217+(r(si; gi)            + \u03b3T V (si, ai; gi)        \u2212  V (si; gi)) \u2208      [0, g\u2217,\u2206].\n By Hoeffding\u2019s inequality and a union bound, we have with probability at\uf8ebleast 1 \u2212       log(1/\u03b4)     \uf8f6                  \u03b4,\n                                     L(d)                                \uf8ed\u03b1Vmax                        \uf8f8   ,\n                                     \u02c6  1 (V ) \u2212      L1(V )      \u2264   O                       N  0\n                                     L(d)                         \u2264   O     \u03b1 \u00b7 g\u2217,\u2206        log(1/\u03b4)        .\n                                     \u02c62 (V ) \u2212        L2(V )                                    N\n Applying a union bound over all V \u2208                    V concludes the result.\n Next, we show that the value of L\u03b1 at the regularized optimal V -function V \u2217                                \u03b1 and the learned func-\n tion \u02c6V is close.\n Lemma 4 (Closeness of the population objective of \u02c6                         V and V \u2217   \u03b1 ). Under Assumptions 1 and 2, with\n probability at least 1 \u2212          \u03b4, we have      L\u03b1( \u02c6 V ) \u2212   L\u03b1(V \u2217  \u03b1 ) \u2264    O(\u01ebstat).\n Proof. We condition on the high probability event in Lemma 3. Note that\n        L\u03b1( \u02c6 V ) \u2212    L\u03b1(V \u2217                 V ) \u2212    \u02c6\n                                                      L(d)   (V\u02c6)  + \u02c6L(d)   (V\u02c6) \u2212    \u02c6\n                               \u03b1 ) = L\u03b1( \u02c6                                             L(d)  (V \u2217\u03b1 ) + \u02c6 L(d)  (V \u2217\u03b1 ) \u2212   L\u03b1(V \u2217  \u03b1 ) .\n                                                   (1)                              (2)                               (3)\n(1), (3) \u2264      O(\u01ebstat) by Lemma 3 and (2) \u2264                 0 by the defi     nition of \u02c6  V , which completes the proof.\n Now we defi       ne \u201csemi-strong\u201d convexity, which is used to prove Lemma 1 and might be of indepen-\n dent interests.\n Defi  nition 2 (Positive semi-strong convexity). For an arbitrary set X                              , assume F \u2282          {f | f : X \u2192\n R} is a convex set. Let \u03c1 \u2208             \u2206(X     ) be a probability distribution over X                 . The \u2225\u00b7\u22252,\u03c1-norm of f \u2208             F\n is defined as \u2225f\u22252,\u03c1=               Ex\u223c\u03c1[f 2(x)]. Define function l : R \u2192                   R and define functional L : F \u2192                  R\n as\n                                                       L(f) = Ex\u223c\u03c1[l(f(x))].\nWe say L is \u03c3-positive-semi-strongly convex w.r.t. f in \u2225\u00b7\u22252,\u03c1-norm for some \u03c3 > 0 if\n is convex w.r.t. f in \u2225\u00b7\u22252,\u03c1-norm.                        L(f) \u2212      \u03c32 \u2225f+\u22252   2,\u03c1\n Note that the common \u03c3-strongly convex defi                      nition requires L(f)          \u2212  \u03c32 \u2225f\u22252 2,\u03c1 to be convex. Now we\n prove the following property for \u03c3-positive-semi-strong convexity.\n Proposition B.1. Under the same setting of Definition 2, if L is \u03c3-positive-semi-strongly convex\n w.r.t. f in \u2225\u00b7\u22252,\u03c1-norm, it holds that for any f, g \u2208                    F,\n                                  L(f) \u2212      L(g) \u2265      \u2207L(g)T(f \u2212          g) + \u03c3   2 \u2225f+ \u2212     g+\u22252  2,\u03c1.\n Proof. By Defi        nition 2, \u02dc  L(f) \u225c        L(f) \u2212      \u03c3         2,\u03c1 is convex, which, by the defi               nition of convex,\n implies that                                    \u02dc            2 \u2225f+\u22252\n                                                L(f) \u2212       \u02dc\n                                                            L(g) \u2265      \u2207   \u02dc\n Plugging in the defi        nition of \u02dc  L(\u00b7), we have                    L(g)T(f \u2212        g).\n       L(f) \u2212      \u03c32 \u2225f+\u22252   2,\u03c1\u2212L(g) + \u03c3       2 \u2225g+\u22252   2,\u03c1\u2265    \u2207L(g)T(f \u2212          g) \u2212   \u03c3Ex\u223c\u03c1[g(x)+(f(x) \u2212                g(x))].\n                                                                       16", "md": "Also note that $$V(s_0,i; g_0,i) \\in [0, V_{\\text{max}}]$$ and $$g^*+(r(s_i; g_i) + \\gamma T V(s_i, a_i; g_i) - V(s_i; g_i)) \\in [0, g^*,\\Delta]$$. By Hoeffding\u2019s inequality and a union bound, we have with probability at least $$1 - \\log(1/\\delta)$$ $$\\frac{L(d)}{\\alpha V_{\\text{max}}}$$, $$\\hat{1}(V) - L_1(V) \\le O N^0$$ $$\\frac{L(d)}{\\alpha \\cdot g^*,\\Delta \\log(1/\\delta)}$$. $$\\hat{2}(V) - L_2(V) \\le N$$. Applying a union bound over all $$V \\in V$$ concludes the result. Next, we show that the value of $$L_\\alpha$$ at the regularized optimal V-function $$V^*_\\alpha$$ and the learned function $$\\hat{V}$$ is close.\n\nLemma 4 (Closeness of the population objective of $$\\hat{V}$$ and $$V^*_\\alpha$$). Under Assumptions 1 and 2, with probability at least $$1 - \\delta$$, we have $$L_\\alpha(\\hat{V}) - L_\\alpha(V^*_\\alpha) \\le O(\\epsilon_{\\text{stat}})$$. Proof. We condition on the high probability event in Lemma 3. Note that $$L_\\alpha(\\hat{V}) - L_\\alpha(V^*_\\alpha) - \\hat{L}(V) + \\hat{L}(V) - \\hat{L}(V) - L_\\alpha(V^*_\\alpha)$$.\n\n(1), (3) $$\\le O(\\epsilon_{\\text{stat}})$$ by Lemma 3 and (2) $$\\le 0$$ by the definition of $$\\hat{V}$$, which completes the proof. Now we define \u201csemi-strong\u201d convexity, which is used to prove Lemma 1 and might be of independent interests.\n\nDefinition 2 (Positive semi-strong convexity). For an arbitrary set $$X$$, assume $$F \\subset \\{f | f : X \\to R\\}$$ is a convex set. Let $$\\rho \\in \\Delta(X)$$ be a probability distribution over $$X$$. The $$\\| \\cdot \\|_{2,\\rho}$$-norm of $$f \\in F$$ is defined as $$\\|f\\|_{2,\\rho} = E_{x \\sim \\rho}[f^2(x)]$$. Define function $$l : R \\to R$$ and define functional $$L : F \\to R$$ as $$L(f) = E_{x \\sim \\rho}[l(f(x))]$$. We say $$L$$ is $$\\sigma$$-positive-semi-strongly convex w.r.t. $$f$$ in $$\\| \\cdot \\|_{2,\\rho}$$-norm for some $$\\sigma > 0$$ if $$L(f) - \\sigma^2 \\|f^+\\|_{2,\\rho}$$.\n\nNote that the common $$\\sigma$$-strongly convex definition requires $$L(f) - \\sigma^2 \\|f\\|_{2,\\rho}$$ to be convex. Now we prove the following property for $$\\sigma$$-positive-semi-strong convexity.\n\nProposition B.1. Under the same setting of Definition 2, if $$L$$ is $$\\sigma$$-positive-semi-strongly convex w.r.t. $$f$$ in $$\\| \\cdot \\|_{2,\\rho}$$-norm, it holds that for any $$f, g \\in F$$, $$L(f) - L(g) \\ge \\nabla L(g)^T(f - g) + \\sigma^2 \\|f^+ - g^+\\|_{2,\\rho}$$. Proof. By Definition 2, $$\\tilde{L}(f) = L(f) - \\sigma^2$$ is convex, which, by the definition of convex, implies that $$\\tilde{L}(f) - \\tilde{L}(g) \\ge \\nabla \\tilde{L}(g)^T(f - g)$$. Plugging in the definition of $$\\tilde{L}(\\cdot)$$, we have $$L(f) - \\sigma^2 \\|f^+\\|_{2,\\rho} - L(g) + \\sigma^2 \\|g^+\\|_{2,\\rho} \\ge \\nabla L(g)^T(f - g) - \\sigma E_{x \\sim \\rho}[g(x) + (f(x) - g(x))]$$.", "images": [], "items": [{"type": "text", "value": "Also note that $$V(s_0,i; g_0,i) \\in [0, V_{\\text{max}}]$$ and $$g^*+(r(s_i; g_i) + \\gamma T V(s_i, a_i; g_i) - V(s_i; g_i)) \\in [0, g^*,\\Delta]$$. By Hoeffding\u2019s inequality and a union bound, we have with probability at least $$1 - \\log(1/\\delta)$$ $$\\frac{L(d)}{\\alpha V_{\\text{max}}}$$, $$\\hat{1}(V) - L_1(V) \\le O N^0$$ $$\\frac{L(d)}{\\alpha \\cdot g^*,\\Delta \\log(1/\\delta)}$$. $$\\hat{2}(V) - L_2(V) \\le N$$. Applying a union bound over all $$V \\in V$$ concludes the result. Next, we show that the value of $$L_\\alpha$$ at the regularized optimal V-function $$V^*_\\alpha$$ and the learned function $$\\hat{V}$$ is close.\n\nLemma 4 (Closeness of the population objective of $$\\hat{V}$$ and $$V^*_\\alpha$$). Under Assumptions 1 and 2, with probability at least $$1 - \\delta$$, we have $$L_\\alpha(\\hat{V}) - L_\\alpha(V^*_\\alpha) \\le O(\\epsilon_{\\text{stat}})$$. Proof. We condition on the high probability event in Lemma 3. Note that $$L_\\alpha(\\hat{V}) - L_\\alpha(V^*_\\alpha) - \\hat{L}(V) + \\hat{L}(V) - \\hat{L}(V) - L_\\alpha(V^*_\\alpha)$$.\n\n(1), (3) $$\\le O(\\epsilon_{\\text{stat}})$$ by Lemma 3 and (2) $$\\le 0$$ by the definition of $$\\hat{V}$$, which completes the proof. Now we define \u201csemi-strong\u201d convexity, which is used to prove Lemma 1 and might be of independent interests.\n\nDefinition 2 (Positive semi-strong convexity). For an arbitrary set $$X$$, assume $$F \\subset \\{f | f : X \\to R\\}$$ is a convex set. Let $$\\rho \\in \\Delta(X)$$ be a probability distribution over $$X$$. The $$\\| \\cdot \\|_{2,\\rho}$$-norm of $$f \\in F$$ is defined as $$\\|f\\|_{2,\\rho} = E_{x \\sim \\rho}[f^2(x)]$$. Define function $$l : R \\to R$$ and define functional $$L : F \\to R$$ as $$L(f) = E_{x \\sim \\rho}[l(f(x))]$$. We say $$L$$ is $$\\sigma$$-positive-semi-strongly convex w.r.t. $$f$$ in $$\\| \\cdot \\|_{2,\\rho}$$-norm for some $$\\sigma > 0$$ if $$L(f) - \\sigma^2 \\|f^+\\|_{2,\\rho}$$.\n\nNote that the common $$\\sigma$$-strongly convex definition requires $$L(f) - \\sigma^2 \\|f\\|_{2,\\rho}$$ to be convex. Now we prove the following property for $$\\sigma$$-positive-semi-strong convexity.\n\nProposition B.1. Under the same setting of Definition 2, if $$L$$ is $$\\sigma$$-positive-semi-strongly convex w.r.t. $$f$$ in $$\\| \\cdot \\|_{2,\\rho}$$-norm, it holds that for any $$f, g \\in F$$, $$L(f) - L(g) \\ge \\nabla L(g)^T(f - g) + \\sigma^2 \\|f^+ - g^+\\|_{2,\\rho}$$. Proof. By Definition 2, $$\\tilde{L}(f) = L(f) - \\sigma^2$$ is convex, which, by the definition of convex, implies that $$\\tilde{L}(f) - \\tilde{L}(g) \\ge \\nabla \\tilde{L}(g)^T(f - g)$$. Plugging in the definition of $$\\tilde{L}(\\cdot)$$, we have $$L(f) - \\sigma^2 \\|f^+\\|_{2,\\rho} - L(g) + \\sigma^2 \\|g^+\\|_{2,\\rho} \\ge \\nabla L(g)^T(f - g) - \\sigma E_{x \\sim \\rho}[g(x) + (f(x) - g(x))]$$.", "md": "Also note that $$V(s_0,i; g_0,i) \\in [0, V_{\\text{max}}]$$ and $$g^*+(r(s_i; g_i) + \\gamma T V(s_i, a_i; g_i) - V(s_i; g_i)) \\in [0, g^*,\\Delta]$$. By Hoeffding\u2019s inequality and a union bound, we have with probability at least $$1 - \\log(1/\\delta)$$ $$\\frac{L(d)}{\\alpha V_{\\text{max}}}$$, $$\\hat{1}(V) - L_1(V) \\le O N^0$$ $$\\frac{L(d)}{\\alpha \\cdot g^*,\\Delta \\log(1/\\delta)}$$. $$\\hat{2}(V) - L_2(V) \\le N$$. Applying a union bound over all $$V \\in V$$ concludes the result. Next, we show that the value of $$L_\\alpha$$ at the regularized optimal V-function $$V^*_\\alpha$$ and the learned function $$\\hat{V}$$ is close.\n\nLemma 4 (Closeness of the population objective of $$\\hat{V}$$ and $$V^*_\\alpha$$). Under Assumptions 1 and 2, with probability at least $$1 - \\delta$$, we have $$L_\\alpha(\\hat{V}) - L_\\alpha(V^*_\\alpha) \\le O(\\epsilon_{\\text{stat}})$$. Proof. We condition on the high probability event in Lemma 3. Note that $$L_\\alpha(\\hat{V}) - L_\\alpha(V^*_\\alpha) - \\hat{L}(V) + \\hat{L}(V) - \\hat{L}(V) - L_\\alpha(V^*_\\alpha)$$.\n\n(1), (3) $$\\le O(\\epsilon_{\\text{stat}})$$ by Lemma 3 and (2) $$\\le 0$$ by the definition of $$\\hat{V}$$, which completes the proof. Now we define \u201csemi-strong\u201d convexity, which is used to prove Lemma 1 and might be of independent interests.\n\nDefinition 2 (Positive semi-strong convexity). For an arbitrary set $$X$$, assume $$F \\subset \\{f | f : X \\to R\\}$$ is a convex set. Let $$\\rho \\in \\Delta(X)$$ be a probability distribution over $$X$$. The $$\\| \\cdot \\|_{2,\\rho}$$-norm of $$f \\in F$$ is defined as $$\\|f\\|_{2,\\rho} = E_{x \\sim \\rho}[f^2(x)]$$. Define function $$l : R \\to R$$ and define functional $$L : F \\to R$$ as $$L(f) = E_{x \\sim \\rho}[l(f(x))]$$. We say $$L$$ is $$\\sigma$$-positive-semi-strongly convex w.r.t. $$f$$ in $$\\| \\cdot \\|_{2,\\rho}$$-norm for some $$\\sigma > 0$$ if $$L(f) - \\sigma^2 \\|f^+\\|_{2,\\rho}$$.\n\nNote that the common $$\\sigma$$-strongly convex definition requires $$L(f) - \\sigma^2 \\|f\\|_{2,\\rho}$$ to be convex. Now we prove the following property for $$\\sigma$$-positive-semi-strong convexity.\n\nProposition B.1. Under the same setting of Definition 2, if $$L$$ is $$\\sigma$$-positive-semi-strongly convex w.r.t. $$f$$ in $$\\| \\cdot \\|_{2,\\rho}$$-norm, it holds that for any $$f, g \\in F$$, $$L(f) - L(g) \\ge \\nabla L(g)^T(f - g) + \\sigma^2 \\|f^+ - g^+\\|_{2,\\rho}$$. Proof. By Definition 2, $$\\tilde{L}(f) = L(f) - \\sigma^2$$ is convex, which, by the definition of convex, implies that $$\\tilde{L}(f) - \\tilde{L}(g) \\ge \\nabla \\tilde{L}(g)^T(f - g)$$. Plugging in the definition of $$\\tilde{L}(\\cdot)$$, we have $$L(f) - \\sigma^2 \\|f^+\\|_{2,\\rho} - L(g) + \\sigma^2 \\|g^+\\|_{2,\\rho} \\ge \\nabla L(g)^T(f - g) - \\sigma E_{x \\sim \\rho}[g(x) + (f(x) - g(x))]$$."}]}, {"page": 17, "text": "Rearranging, we can obtain that\n       L(f) \u2212     L(g) \u2265\u2207L(g)T(f \u2212                g) \u2212   \u03c3Ex\u223c\u03c1[g(x)+(f(x) \u2212               g(x))] + \u03c3    2 \u2225f+\u22252    2,\u03c1\u2212\u03c3  2 \u2225g+\u22252   2,\u03c1\n                           =\u2207L(g)T(f \u2212            g) + \u03c3               +(x) \u2212      g2\n                                                          2 Ex\u223c\u03c1[f 2                +(x) \u2212      2g(x)+(f(x) \u2212           g(x))]\n                           =\u2207L(g)T(f \u2212            g) + \u03c3               +(x) \u2212      g2\n                                                          2 Ex\u223c\u03c1[f 2                +(x) \u2212      2g(x)+(f(x) \u2212           g(x))]\n                           =\u2207L(g)T(f \u2212            g) + \u03c3  2 \u2225f+ \u2212      g+\u22252  2,\u03c1\nIt suffi ces to show that     + \u03c3Ex\u223c\u03c1[g+(x)(f+(x) \u2212                  f(x))] + \u03c3Ex\u223c\u03c1[g+(x)(g(x) \u2212                   g+(x))].\n                    E x\u223c\u03c1[g+(x)(f+(x) \u2212              f(x))] \u2265      0,    Ex\u223c\u03c1[g+(x)(g(x) \u2212              g+(x))] \u2265       0.\nNote that for any x \u2208           X  , g+(x) \u2265       0 and f+(x) \u2265          f(x), which implies g+(x)(f+(x) \u2212                     f(x)) \u2265      0.\nAlso, note that when g(x) \u2265              0, we have g(x)\u2212g+(x) = 0, and when g(x) < 0, we have g+(x) = 0,\nwhich means g+(x)(g(x) \u2212                 g+(x)) \u2261       0. Therefore, we have\nwhich concludes.    E x\u223c\u03c1[g+(x)(f+(x) \u2212              f(x))] \u2265      0,    Ex\u223c\u03c1[g+(x)(g(x) \u2212              g+(x))] = 0,\nFinally, by semi-strong convexity of L\u03b1 w.r.t. UV in \u2225\u00b7\u22252,\u00b5-norm, we can show that \u02c6                                       U and U \u2217    \u03b1 are\nalso close.\nProof of Lemma 1. We condition on the high probability event in Lemma 3.                                                      Recall that\nUV (s, a; g) = r(s; g) + \u03b3T V (s, a; g) \u2212                     V (s; g) + \u03b1 and in deterministic dynamics, \u02c6                      U = U \u02c6    V .\nLet U = {UV : V \u2208                R|S|\u00d7|G|} \u2286         R|S|\u00d7|A|\u00d7|G| which is a convex set by defi                     nition. Also, since\n                                    +\nUV is linear in V , we can also obtain that V can be linearly represented by U. Therefore, we can\ndefi ne that \u02dc  L\u03b1(UV ) = L\u03b1(V ) and \u02dc             L\u03b1(UV ) \u2212        1                    V (s, a, g)+] is linear and thus convex\n                                                                    2E(s,a,g)\u223c\u00b5[U 2\nin UV , which implies that \u02dc           L\u03b1(UV ) is 1-positive-semi-strongly convex w.r.t. UV and \u2225\u00b7\u22252,\u00b5. Then,\nby Proposition B.1, we have\n                        \u02dc                                                                       U+ \u2212     U \u2217\n                        L\u03b1( \u02c6 U) \u2212     \u02dc\n                                       L\u03b1(U \u2217             L\u03b1(U \u2217          U \u2212    U \u2217                       \u03b1+\u22252   2,\u00b5.\n                                               \u03b1) \u2265    \u2207\u02dc          \u03b1)T( \u02c6          \u03b1) + 1   2\u2225  \u02c6\nSince U \u2217    \u03b1    =      arg minU\u2208U \u02dc       L\u03b1(U), by the fi            rst order optimality condition, it holds that\n\u2207\u02dc L\u03b1(U \u2217  \u03b1)T( \u02c6 U \u2212    U \u2217\u03b1)\u2225\u2265\u02c6   0. Combining Lemma 4, we have\n                                U+ \u2212     U \u2217                 2(\u02dcL\u03b1( \u02c6 U) \u2212     \u02dc\n                                           \u03b1+\u22252,\u00b5\u2264                             L\u03b1(U \u2217  \u03b1)) \u2264     O (\u221a    \u01ebstat) .\nB.2     Proof for Stochastic Dynamics\nIn stochastic dynamic settings, we fi               rst learn the ground-truth transition model and then calculate \u02c6                        U.\nThe following lemma provides a theoretical guarantee for maximum likelihood estimation (MLE)\nthat \u02c6P and P \u22c6      are close.\nLemma 5 (Convergence rate of MLE, Van de Geer [2000]). For any fixed \u03b4 > 0, with probability\nat least 1 \u2212     \u03b4, we have    E (s,a,g)\u223c\u00b5      \u2225  \u02c6\n                                                  P(\u00b7|s, a) \u2212      P \u22c6(\u00b7|s, a)\u22252    TV    \u2272   log(|P|/\u03b4),\n                                                                                                     N\nwhere \u02c6  P is defined as in (11). This immediately implies that\n                             E  (s,a,g)\u223c\u00b5      \u2225P\u02c6 (\u00b7|s, a) \u2212     P \u22c6(\u00b7|s, a)\u2225TV        \u2272       log(|P|/\u03b4).\n                                                                                                       N\n                                                                      17", "md": "# Math Equations and Text\n\nRearranging, we can obtain that\n\n$$\nL(f) - L(g) \\geq \\nabla L(g)^T(f - g) - \\sigma E_{x \\sim \\rho}[g(x)+(f(x) - g(x))] + \\sigma^2 \\|f^+\\|^2_{2,\\rho} - \\sigma^2 \\|g^+\\|^2_{2,\\rho}\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma E_{x \\sim \\rho}[f^2_+(x) - 2g(x)+(f(x) - g(x))]\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma E_{x \\sim \\rho}[f^2_+(x) - 2g(x)+(f(x) - g(x))]\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma^2 \\|f^+ - g^+\\|^2_{2,\\rho}\n$$\nIt suffices to show that\n\n$$\nE_{x \\sim \\rho}[g^+(x)(f^+(x) - f(x))] \\geq 0, \\quad E_{x \\sim \\rho}[g^+(x)(g(x) - g^+(x))] \\geq 0.\n$$\nNote that for any \\(x \\in X\\), \\(g^+(x) \\geq 0\\) and \\(f^+(x) \\geq f(x)\\), which implies \\(g^+(x)(f^+(x) - f(x)) \\geq 0\\.\n\nAlso, note that when \\(g(x) \\geq 0\\), we have \\(g(x) - g^+(x) = 0\\), and when \\(g(x) < 0\\), we have \\(g^+(x) = 0\\,\n\nwhich means \\(g^+(x)(g(x) - g^+(x)) \\equiv 0\\). Therefore, we have\n\n$$\nE_{x \\sim \\rho}[g^+(x)(f^+(x) - f(x))] \\geq 0, \\quad E_{x \\sim \\rho}[g^+(x)(g(x) - g^+(x))] = 0,\n$$\nFinally, by semi-strong convexity of \\(L_\\alpha\\) w.r.t. UV in \\(\\| \\cdot \\|_2, \\mu\\)-norm, we can show that \\(\\hat{U}\\) and \\(U^*_\\alpha\\) are also close.\n\nProof of Lemma 1. We condition on the high probability event in Lemma 3. Recall that\n\n$$\nUV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) + \\alpha \\quad \\text{and in deterministic dynamics,} \\quad \\hat{U} = U \\hat{V}.\n$$\nLet \\(U = \\{UV : V \\in \\mathbb{R}^{|S| \\times |G|}\\} \\subseteq \\mathbb{R}^{|S| \\times |A| \\times |G|}\\) which is a convex set by definition. Also, since \\(UV\\) is linear in \\(V\\), we can also obtain that \\(V\\) can be linearly represented by \\(U\\). Therefore, we can define that \\(\\tilde{L}_\\alpha(UV) = L_\\alpha(V)\\) and \\(\\tilde{L}_\\alpha(UV) - \\frac{1}{2} E(s,a,g) \\sim \\mu[U^2]\\) is linear and thus convex in \\(UV\\), which implies that \\(\\tilde{L}_\\alpha(UV)\\) is 1-positive-semi-strongly convex w.r.t. UV and \\(\\| \\cdot \\|_2, \\mu\\). Then, by Proposition B.1, we have\n\n$$\n\\tilde{L}_\\alpha(\\hat{U}) - \\tilde{L}_\\alpha(U^*_\\alpha) \\geq \\nabla \\tilde{L}_\\alpha(U^*_\\alpha)^T(\\hat{U} - U^*_\\alpha) + \\frac{1}{2} \\| \\hat{U} - U^*_\\alpha \\|^2_{2,\\mu}.\n$$\nSince \\(U^*_\\alpha = \\text{arg min}_{U \\in U} \\tilde{L}_\\alpha(U)\\), by the first-order optimality condition, it holds that\n\n$$\n\\nabla \\tilde{L}_\\alpha(U^*_\\alpha)^T(\\hat{U} - U^*_\\alpha) \\geq 0.\n$$\nCombining Lemma 4, we have\n\n$$\n\\| \\hat{U} - U^*_\\alpha \\|^2_{2,\\mu} \\leq O(\\sqrt{\\epsilon_{\\text{stat}}}).\n$$\nProof for Stochastic Dynamics\n\nIn stochastic dynamic settings, we first learn the ground-truth transition model and then calculate \\(\\hat{U}\\). The following lemma provides a theoretical guarantee for maximum likelihood estimation (MLE) that \\(\\hat{P}\\) and \\(P^*\\) are close.\n\nLemma 5 (Convergence rate of MLE, Van de Geer [2000]). For any fixed \\(\\delta > 0\\), with probability at least \\(1 - \\delta\\), we have\n\n$$\nE(s,a,g) \\sim \\mu \\| \\hat{P}(\\cdot|s, a) - P^*(\\cdot|s, a) \\|^2_{\\text{TV}} \\lesssim \\log(|P|/\\delta),\n$$\nwhere \\(\\hat{P}\\) is defined as in (11). This immediately implies that\n\n$$\nE(s,a,g) \\sim \\mu \\| P\\hat{P}(\\cdot|s, a) - P^*(\\cdot|s, a) \\|^{\\text{TV}} \\lesssim \\log(|P|/\\delta).\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Rearranging, we can obtain that\n\n$$\nL(f) - L(g) \\geq \\nabla L(g)^T(f - g) - \\sigma E_{x \\sim \\rho}[g(x)+(f(x) - g(x))] + \\sigma^2 \\|f^+\\|^2_{2,\\rho} - \\sigma^2 \\|g^+\\|^2_{2,\\rho}\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma E_{x \\sim \\rho}[f^2_+(x) - 2g(x)+(f(x) - g(x))]\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma E_{x \\sim \\rho}[f^2_+(x) - 2g(x)+(f(x) - g(x))]\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma^2 \\|f^+ - g^+\\|^2_{2,\\rho}\n$$\nIt suffices to show that\n\n$$\nE_{x \\sim \\rho}[g^+(x)(f^+(x) - f(x))] \\geq 0, \\quad E_{x \\sim \\rho}[g^+(x)(g(x) - g^+(x))] \\geq 0.\n$$\nNote that for any \\(x \\in X\\), \\(g^+(x) \\geq 0\\) and \\(f^+(x) \\geq f(x)\\), which implies \\(g^+(x)(f^+(x) - f(x)) \\geq 0\\.\n\nAlso, note that when \\(g(x) \\geq 0\\), we have \\(g(x) - g^+(x) = 0\\), and when \\(g(x) < 0\\), we have \\(g^+(x) = 0\\,\n\nwhich means \\(g^+(x)(g(x) - g^+(x)) \\equiv 0\\). Therefore, we have\n\n$$\nE_{x \\sim \\rho}[g^+(x)(f^+(x) - f(x))] \\geq 0, \\quad E_{x \\sim \\rho}[g^+(x)(g(x) - g^+(x))] = 0,\n$$\nFinally, by semi-strong convexity of \\(L_\\alpha\\) w.r.t. UV in \\(\\| \\cdot \\|_2, \\mu\\)-norm, we can show that \\(\\hat{U}\\) and \\(U^*_\\alpha\\) are also close.\n\nProof of Lemma 1. We condition on the high probability event in Lemma 3. Recall that\n\n$$\nUV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) + \\alpha \\quad \\text{and in deterministic dynamics,} \\quad \\hat{U} = U \\hat{V}.\n$$\nLet \\(U = \\{UV : V \\in \\mathbb{R}^{|S| \\times |G|}\\} \\subseteq \\mathbb{R}^{|S| \\times |A| \\times |G|}\\) which is a convex set by definition. Also, since \\(UV\\) is linear in \\(V\\), we can also obtain that \\(V\\) can be linearly represented by \\(U\\). Therefore, we can define that \\(\\tilde{L}_\\alpha(UV) = L_\\alpha(V)\\) and \\(\\tilde{L}_\\alpha(UV) - \\frac{1}{2} E(s,a,g) \\sim \\mu[U^2]\\) is linear and thus convex in \\(UV\\), which implies that \\(\\tilde{L}_\\alpha(UV)\\) is 1-positive-semi-strongly convex w.r.t. UV and \\(\\| \\cdot \\|_2, \\mu\\). Then, by Proposition B.1, we have\n\n$$\n\\tilde{L}_\\alpha(\\hat{U}) - \\tilde{L}_\\alpha(U^*_\\alpha) \\geq \\nabla \\tilde{L}_\\alpha(U^*_\\alpha)^T(\\hat{U} - U^*_\\alpha) + \\frac{1}{2} \\| \\hat{U} - U^*_\\alpha \\|^2_{2,\\mu}.\n$$\nSince \\(U^*_\\alpha = \\text{arg min}_{U \\in U} \\tilde{L}_\\alpha(U)\\), by the first-order optimality condition, it holds that\n\n$$\n\\nabla \\tilde{L}_\\alpha(U^*_\\alpha)^T(\\hat{U} - U^*_\\alpha) \\geq 0.\n$$\nCombining Lemma 4, we have\n\n$$\n\\| \\hat{U} - U^*_\\alpha \\|^2_{2,\\mu} \\leq O(\\sqrt{\\epsilon_{\\text{stat}}}).\n$$\nProof for Stochastic Dynamics\n\nIn stochastic dynamic settings, we first learn the ground-truth transition model and then calculate \\(\\hat{U}\\). The following lemma provides a theoretical guarantee for maximum likelihood estimation (MLE) that \\(\\hat{P}\\) and \\(P^*\\) are close.\n\nLemma 5 (Convergence rate of MLE, Van de Geer [2000]). For any fixed \\(\\delta > 0\\), with probability at least \\(1 - \\delta\\), we have\n\n$$\nE(s,a,g) \\sim \\mu \\| \\hat{P}(\\cdot|s, a) - P^*(\\cdot|s, a) \\|^2_{\\text{TV}} \\lesssim \\log(|P|/\\delta),\n$$\nwhere \\(\\hat{P}\\) is defined as in (11). This immediately implies that\n\n$$\nE(s,a,g) \\sim \\mu \\| P\\hat{P}(\\cdot|s, a) - P^*(\\cdot|s, a) \\|^{\\text{TV}} \\lesssim \\log(|P|/\\delta).\n$$", "md": "Rearranging, we can obtain that\n\n$$\nL(f) - L(g) \\geq \\nabla L(g)^T(f - g) - \\sigma E_{x \\sim \\rho}[g(x)+(f(x) - g(x))] + \\sigma^2 \\|f^+\\|^2_{2,\\rho} - \\sigma^2 \\|g^+\\|^2_{2,\\rho}\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma E_{x \\sim \\rho}[f^2_+(x) - 2g(x)+(f(x) - g(x))]\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma E_{x \\sim \\rho}[f^2_+(x) - 2g(x)+(f(x) - g(x))]\n$$\n$$\n= \\nabla L(g)^T(f - g) + \\sigma^2 \\|f^+ - g^+\\|^2_{2,\\rho}\n$$\nIt suffices to show that\n\n$$\nE_{x \\sim \\rho}[g^+(x)(f^+(x) - f(x))] \\geq 0, \\quad E_{x \\sim \\rho}[g^+(x)(g(x) - g^+(x))] \\geq 0.\n$$\nNote that for any \\(x \\in X\\), \\(g^+(x) \\geq 0\\) and \\(f^+(x) \\geq f(x)\\), which implies \\(g^+(x)(f^+(x) - f(x)) \\geq 0\\.\n\nAlso, note that when \\(g(x) \\geq 0\\), we have \\(g(x) - g^+(x) = 0\\), and when \\(g(x) < 0\\), we have \\(g^+(x) = 0\\,\n\nwhich means \\(g^+(x)(g(x) - g^+(x)) \\equiv 0\\). Therefore, we have\n\n$$\nE_{x \\sim \\rho}[g^+(x)(f^+(x) - f(x))] \\geq 0, \\quad E_{x \\sim \\rho}[g^+(x)(g(x) - g^+(x))] = 0,\n$$\nFinally, by semi-strong convexity of \\(L_\\alpha\\) w.r.t. UV in \\(\\| \\cdot \\|_2, \\mu\\)-norm, we can show that \\(\\hat{U}\\) and \\(U^*_\\alpha\\) are also close.\n\nProof of Lemma 1. We condition on the high probability event in Lemma 3. Recall that\n\n$$\nUV(s, a; g) = r(s; g) + \\gamma T V(s, a; g) - V(s; g) + \\alpha \\quad \\text{and in deterministic dynamics,} \\quad \\hat{U} = U \\hat{V}.\n$$\nLet \\(U = \\{UV : V \\in \\mathbb{R}^{|S| \\times |G|}\\} \\subseteq \\mathbb{R}^{|S| \\times |A| \\times |G|}\\) which is a convex set by definition. Also, since \\(UV\\) is linear in \\(V\\), we can also obtain that \\(V\\) can be linearly represented by \\(U\\). Therefore, we can define that \\(\\tilde{L}_\\alpha(UV) = L_\\alpha(V)\\) and \\(\\tilde{L}_\\alpha(UV) - \\frac{1}{2} E(s,a,g) \\sim \\mu[U^2]\\) is linear and thus convex in \\(UV\\), which implies that \\(\\tilde{L}_\\alpha(UV)\\) is 1-positive-semi-strongly convex w.r.t. UV and \\(\\| \\cdot \\|_2, \\mu\\). Then, by Proposition B.1, we have\n\n$$\n\\tilde{L}_\\alpha(\\hat{U}) - \\tilde{L}_\\alpha(U^*_\\alpha) \\geq \\nabla \\tilde{L}_\\alpha(U^*_\\alpha)^T(\\hat{U} - U^*_\\alpha) + \\frac{1}{2} \\| \\hat{U} - U^*_\\alpha \\|^2_{2,\\mu}.\n$$\nSince \\(U^*_\\alpha = \\text{arg min}_{U \\in U} \\tilde{L}_\\alpha(U)\\), by the first-order optimality condition, it holds that\n\n$$\n\\nabla \\tilde{L}_\\alpha(U^*_\\alpha)^T(\\hat{U} - U^*_\\alpha) \\geq 0.\n$$\nCombining Lemma 4, we have\n\n$$\n\\| \\hat{U} - U^*_\\alpha \\|^2_{2,\\mu} \\leq O(\\sqrt{\\epsilon_{\\text{stat}}}).\n$$\nProof for Stochastic Dynamics\n\nIn stochastic dynamic settings, we first learn the ground-truth transition model and then calculate \\(\\hat{U}\\). The following lemma provides a theoretical guarantee for maximum likelihood estimation (MLE) that \\(\\hat{P}\\) and \\(P^*\\) are close.\n\nLemma 5 (Convergence rate of MLE, Van de Geer [2000]). For any fixed \\(\\delta > 0\\), with probability at least \\(1 - \\delta\\), we have\n\n$$\nE(s,a,g) \\sim \\mu \\| \\hat{P}(\\cdot|s, a) - P^*(\\cdot|s, a) \\|^2_{\\text{TV}} \\lesssim \\log(|P|/\\delta),\n$$\nwhere \\(\\hat{P}\\) is defined as in (11). This immediately implies that\n\n$$\nE(s,a,g) \\sim \\mu \\| P\\hat{P}(\\cdot|s, a) - P^*(\\cdot|s, a) \\|^{\\text{TV}} \\lesssim \\log(|P|/\\delta).\n$$"}]}, {"page": 18, "text": "Equipped with Lemma 5, we can guarantee that for any V , the population objective L\u03b1(V ) is close\nto the empirical objective \u02c6             L(s)   (V ), which is presented in Lemma 6.\nLemma 6 (Concentration of \u02c6                   L(s)   ). Under Assumptions 1 to 3, with probability at least 1 \u2212                                 \u03b4, \u2200V \u2208\nV, it holds that\n              \u02c6L(s)  (V ) \u2212      L\u03b1(V )                                                                                      \uf8f6\n                \uf8eb                  log(|V|/\u03b4)                         log(|V|/\u03b4)                           log(|P|/\u03b4)\uf8f8            \u225c   \u01ebstochastic\n          \u2264O    \uf8ed\u03b1Vmax                   N0          + V 2  max              N          + \u03b3V 2   max              N                     stat       .\nProof. For convenience, defi                  ne\n                          \u02c6                           N0    \u03b1 \u00b7 V (s0,i; g0,i),\n                         L(s)\n                          \u02c6 1 (V ) =1 \u2212      N0  N\u03b3   i=1\n                         L(s)                         \u03b1 \u00b7 g\u2217+(r(si; gi) + \u03b3 \u02c6          T V (si, ai; gi) \u2212          V (si; gi)).\n                            2 (V ) = 1     N    i=1\nNote that \u02c6     L(s)              L(d)\nall V \u2208      V,   1 (V ) = \u02c6         1 (V ) and thus by Lemma 3 we have that with probability at least 1 \u2212                                           \u03b4, for\n                                                                              \uf8eb                  log(|V|/\u03b4)       \uf8f6\n                                       L(s)                                   \uf8ed\u03b1Vmax                              \uf8f8   .                                (16)\nNow defi       ne          \u02dc           \u02c6  1 (V ) \u2212       L1(V )       \u2264   O                            N  0\n                           L(s)                                                             T V (s, a; g) \u2212         V (s; g))]\n                              2 (V ) =E(s,a,g)\u223c\u00b5[\u03b1 \u00b7 g\u2217+(r(s; g) + \u03b3 \u02c6\nSince E      (s,a,g)\u223c\u00b5[\u02c6    L(s)                L(s)\nV \u2208     V, it holds that      2 (V )] = \u02dc          2 (V ), by Lemma 3, we have that with probability 1 \u2212                                      \u03b4, for any\n                                       |\u02c6                                                    log(|V|/\u03b4)              .                                 (17)\n                                        L(s)              L(s)                     V 2\n                                           2 (V ) \u2212        \u02dc2 (V )|\u2264        O         max              N\nAlso, note that\n   |L2(V ) \u2212        \u02dc\n                    L(s)\n                       2 (V )|\n=    \u03b1E(s,a,g)\u223c\u00b5[g\u2217+(r(s; g) + \u03b3T V (s, a; g) \u2212                           V (s; g)) \u2212        g\u2217+(r(s; g) + \u03b3 \u02c6        T V (s, a; g) \u2212         V (s; g))]\n=1  2   E(s,a,g)\u223c\u00b5         (r(s; g) + \u03b3T V (s, a; g) \u2212                V (s; g) + \u03b1)2       + \u2212    (r(s; g) + \u03b3 \u02c6     T V (s, a; g) \u2212         V (s; g) + \u03b1)2  +\n\u2272VmaxE(s,a,g)\u223c\u00b5[\u03b3 \u00b7 |(T \u2212                    T\u02c6)V (s, a; g)|]\n=VmaxE(s,a,g)\u223c\u00b5[\u03b3 \u00b7 |Es\u2032\u223cP \u22c6(\u00b7|s,a)[V (s\u2032; g)] \u2212                            Es\u2032\u223c   P\u02c6(\u00b7|s,a)[V (s\u2032; g)]|]\n\u2272\u03b3V 2   maxE(s,a,g)\u223c\u00b5[\u2225P \u22c6(\u00b7|s, a) \u2212                  P\u02c6 (\u00b7|s, a)\u2225TV].\nBy Lemma 5, with probability at least 1 \u2212                           \u03b4, for all V \u2208        V,\n   |L2(V ) \u2212        \u02dc                                                                                                         log(|P|/\u03b4). (18)\n                   L(s)                   maxE(s,a,g)\u223c\u00b5[\u2225P \u22c6(\u00b7|s, a) \u2212                  P\u02c6 (\u00b7|s, a)\u2225TV] \u2272          \u03b3V 2max\n                      2 (V )|\u2272        \u03b3V 2                                                                                               N\nBy rescaling \u03b4 and applying a union bound, we can obtain that with probability at least 1 \u2212                                                       \u03b4, (16),\n(17), (18) hold simultaneously. The conclusion holds by applying the triangle inequality.\nFinally, we show that \u02c6            U+ and U \u2217      \u03b1+ are close.\n                                                                             18", "md": "Equipped with Lemma 5, we can guarantee that for any \\( V \\), the population objective \\( L_{\\alpha}(V) \\) is close to the empirical objective \\( \\hat{L}(s)(V) \\), which is presented in Lemma 6.\n\n$$\n\\text{Lemma 6 (Concentration of } \\hat{L}(s) \\text{). Under Assumptions 1 to 3, with probability at least } 1 - \\delta, \\forall V \\in V, \\text{ it holds that}\n$$\n\n$$\n\\hat{L}(s)(V) - L_{\\alpha}(V) \\leq O\\left(\\alpha V_{\\text{max}} N_0 + V_{\\text{max}}^2 N + \\gamma V_{\\text{max}}^2 N\\right)_{\\text{stat}}\n$$\n\n\\[\n\\text{Proof. For convenience, define}\n\\]\n\n\\[\n\\begin{align*}\n\\hat{N}_0 &= \\alpha \\cdot V(s_0,i; g_0,i), \\\\\n\\hat{L}_1(V) &= 1 - \\frac{N_0}{N\\gamma} \\sum_{i=1}^{N} \\left(\\alpha \\cdot g^* + (r(s_i; g_i) + \\gamma \\hat{T}V(s_i, a_i; g_i) - V(s_i; g_i))\\right), \\\\\n\\hat{L}_2(V) &= \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\alpha \\cdot g^* + (r(s_i; g_i) + \\gamma \\hat{T}V(s_i, a_i; g_i) - V(s_i; g_i))\\right).\n\\end{align*}\n\\]\n\nNote that for all \\( V \\in V \\), \\( \\hat{L}_1(V) = \\hat{L}_1(V) \\) and thus by Lemma 3 we have that with probability at least \\( 1 - \\delta \\), for\n\n$$\n\\hat{L}(s) \\leq \\left(\\alpha V_{\\text{max}}\\right)_{\\text{stat}}. \\quad (16)\n$$\n\nNow define\n\n$$\n\\tilde{L}_1(V) - \\hat{L}_1(V) \\leq O\\left(\\frac{N_0}{N} \\hat{T}V(s, a; g) - V(s; g)\\right)\n$$\n\nSince \\( E(s,a,g) \\sim \\mu[\\hat{L}(s)] = \\hat{L}(s) \\), for all \\( V \\in V \\), it holds that \\( \\hat{L}_2(V) = \\hat{L}_2(V) \\), by Lemma 3, we have that with probability \\( 1 - \\delta \\), for any\n\n$$\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| \\leq O\\left(\\frac{V_{\\text{max}}^2}{N}\\right). \\quad (17)\n$$\n\nAlso, note that\n\n\\[\n\\begin{align*}\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| &= \\alpha E(s,a,g) \\sim \\mu\\left[g^* + (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g)) - g^* + (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g))\\right] \\\\\n&= \\frac{1}{2} E(s,a,g) \\sim \\mu\\left[(r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g) + \\alpha)^2 - (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g) + \\alpha)^2\\right] \\\\\n&\\lesssim V_{\\text{max}} E(s,a,g) \\sim \\mu[\\gamma \\cdot \\left|(T - \\hat{T})V(s, a; g)\\right|] \\\\\n&= V_{\\text{max}} E(s,a,g) \\sim \\mu[\\gamma \\cdot \\left|E_{s' \\sim P^*(\\cdot|s,a)}[V(s'; g)] - E_{s' \\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]\\right|] \\\\\n&\\lesssim \\gamma V_{\\text{max}^2} E(s,a,g) \\sim \\mu[\\|P^*(\\cdot|s, a) - \\hat{P}(\\cdot|s, a)\\|_{TV}].\n\\end{align*}\n\\]\n\nBy Lemma 5, with probability at least \\( 1 - \\delta \\), for all \\( V \\in V \\),\n\n$$\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| \\lesssim \\gamma V_{\\text{max}}^2 \\log(|P|/\\delta). \\quad (18)\n$$\n\nBy rescaling \\( \\delta \\) and applying a union bound, we can obtain that with probability at least \\( 1 - \\delta \\), (16), (17), (18) hold simultaneously. The conclusion holds by applying the triangle inequality.\n\nFinally, we show that \\( \\hat{U}^+ \\) and \\( U^*_{\\alpha^+} \\) are close.", "images": [], "items": [{"type": "text", "value": "Equipped with Lemma 5, we can guarantee that for any \\( V \\), the population objective \\( L_{\\alpha}(V) \\) is close to the empirical objective \\( \\hat{L}(s)(V) \\), which is presented in Lemma 6.\n\n$$\n\\text{Lemma 6 (Concentration of } \\hat{L}(s) \\text{). Under Assumptions 1 to 3, with probability at least } 1 - \\delta, \\forall V \\in V, \\text{ it holds that}\n$$\n\n$$\n\\hat{L}(s)(V) - L_{\\alpha}(V) \\leq O\\left(\\alpha V_{\\text{max}} N_0 + V_{\\text{max}}^2 N + \\gamma V_{\\text{max}}^2 N\\right)_{\\text{stat}}\n$$\n\n\\[\n\\text{Proof. For convenience, define}\n\\]\n\n\\[\n\\begin{align*}\n\\hat{N}_0 &= \\alpha \\cdot V(s_0,i; g_0,i), \\\\\n\\hat{L}_1(V) &= 1 - \\frac{N_0}{N\\gamma} \\sum_{i=1}^{N} \\left(\\alpha \\cdot g^* + (r(s_i; g_i) + \\gamma \\hat{T}V(s_i, a_i; g_i) - V(s_i; g_i))\\right), \\\\\n\\hat{L}_2(V) &= \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\alpha \\cdot g^* + (r(s_i; g_i) + \\gamma \\hat{T}V(s_i, a_i; g_i) - V(s_i; g_i))\\right).\n\\end{align*}\n\\]\n\nNote that for all \\( V \\in V \\), \\( \\hat{L}_1(V) = \\hat{L}_1(V) \\) and thus by Lemma 3 we have that with probability at least \\( 1 - \\delta \\), for\n\n$$\n\\hat{L}(s) \\leq \\left(\\alpha V_{\\text{max}}\\right)_{\\text{stat}}. \\quad (16)\n$$\n\nNow define\n\n$$\n\\tilde{L}_1(V) - \\hat{L}_1(V) \\leq O\\left(\\frac{N_0}{N} \\hat{T}V(s, a; g) - V(s; g)\\right)\n$$\n\nSince \\( E(s,a,g) \\sim \\mu[\\hat{L}(s)] = \\hat{L}(s) \\), for all \\( V \\in V \\), it holds that \\( \\hat{L}_2(V) = \\hat{L}_2(V) \\), by Lemma 3, we have that with probability \\( 1 - \\delta \\), for any\n\n$$\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| \\leq O\\left(\\frac{V_{\\text{max}}^2}{N}\\right). \\quad (17)\n$$\n\nAlso, note that\n\n\\[\n\\begin{align*}\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| &= \\alpha E(s,a,g) \\sim \\mu\\left[g^* + (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g)) - g^* + (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g))\\right] \\\\\n&= \\frac{1}{2} E(s,a,g) \\sim \\mu\\left[(r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g) + \\alpha)^2 - (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g) + \\alpha)^2\\right] \\\\\n&\\lesssim V_{\\text{max}} E(s,a,g) \\sim \\mu[\\gamma \\cdot \\left|(T - \\hat{T})V(s, a; g)\\right|] \\\\\n&= V_{\\text{max}} E(s,a,g) \\sim \\mu[\\gamma \\cdot \\left|E_{s' \\sim P^*(\\cdot|s,a)}[V(s'; g)] - E_{s' \\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]\\right|] \\\\\n&\\lesssim \\gamma V_{\\text{max}^2} E(s,a,g) \\sim \\mu[\\|P^*(\\cdot|s, a) - \\hat{P}(\\cdot|s, a)\\|_{TV}].\n\\end{align*}\n\\]\n\nBy Lemma 5, with probability at least \\( 1 - \\delta \\), for all \\( V \\in V \\),\n\n$$\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| \\lesssim \\gamma V_{\\text{max}}^2 \\log(|P|/\\delta). \\quad (18)\n$$\n\nBy rescaling \\( \\delta \\) and applying a union bound, we can obtain that with probability at least \\( 1 - \\delta \\), (16), (17), (18) hold simultaneously. The conclusion holds by applying the triangle inequality.\n\nFinally, we show that \\( \\hat{U}^+ \\) and \\( U^*_{\\alpha^+} \\) are close.", "md": "Equipped with Lemma 5, we can guarantee that for any \\( V \\), the population objective \\( L_{\\alpha}(V) \\) is close to the empirical objective \\( \\hat{L}(s)(V) \\), which is presented in Lemma 6.\n\n$$\n\\text{Lemma 6 (Concentration of } \\hat{L}(s) \\text{). Under Assumptions 1 to 3, with probability at least } 1 - \\delta, \\forall V \\in V, \\text{ it holds that}\n$$\n\n$$\n\\hat{L}(s)(V) - L_{\\alpha}(V) \\leq O\\left(\\alpha V_{\\text{max}} N_0 + V_{\\text{max}}^2 N + \\gamma V_{\\text{max}}^2 N\\right)_{\\text{stat}}\n$$\n\n\\[\n\\text{Proof. For convenience, define}\n\\]\n\n\\[\n\\begin{align*}\n\\hat{N}_0 &= \\alpha \\cdot V(s_0,i; g_0,i), \\\\\n\\hat{L}_1(V) &= 1 - \\frac{N_0}{N\\gamma} \\sum_{i=1}^{N} \\left(\\alpha \\cdot g^* + (r(s_i; g_i) + \\gamma \\hat{T}V(s_i, a_i; g_i) - V(s_i; g_i))\\right), \\\\\n\\hat{L}_2(V) &= \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\alpha \\cdot g^* + (r(s_i; g_i) + \\gamma \\hat{T}V(s_i, a_i; g_i) - V(s_i; g_i))\\right).\n\\end{align*}\n\\]\n\nNote that for all \\( V \\in V \\), \\( \\hat{L}_1(V) = \\hat{L}_1(V) \\) and thus by Lemma 3 we have that with probability at least \\( 1 - \\delta \\), for\n\n$$\n\\hat{L}(s) \\leq \\left(\\alpha V_{\\text{max}}\\right)_{\\text{stat}}. \\quad (16)\n$$\n\nNow define\n\n$$\n\\tilde{L}_1(V) - \\hat{L}_1(V) \\leq O\\left(\\frac{N_0}{N} \\hat{T}V(s, a; g) - V(s; g)\\right)\n$$\n\nSince \\( E(s,a,g) \\sim \\mu[\\hat{L}(s)] = \\hat{L}(s) \\), for all \\( V \\in V \\), it holds that \\( \\hat{L}_2(V) = \\hat{L}_2(V) \\), by Lemma 3, we have that with probability \\( 1 - \\delta \\), for any\n\n$$\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| \\leq O\\left(\\frac{V_{\\text{max}}^2}{N}\\right). \\quad (17)\n$$\n\nAlso, note that\n\n\\[\n\\begin{align*}\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| &= \\alpha E(s,a,g) \\sim \\mu\\left[g^* + (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g)) - g^* + (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g))\\right] \\\\\n&= \\frac{1}{2} E(s,a,g) \\sim \\mu\\left[(r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g) + \\alpha)^2 - (r(s; g) + \\gamma \\hat{T}V(s, a; g) - V(s; g) + \\alpha)^2\\right] \\\\\n&\\lesssim V_{\\text{max}} E(s,a,g) \\sim \\mu[\\gamma \\cdot \\left|(T - \\hat{T})V(s, a; g)\\right|] \\\\\n&= V_{\\text{max}} E(s,a,g) \\sim \\mu[\\gamma \\cdot \\left|E_{s' \\sim P^*(\\cdot|s,a)}[V(s'; g)] - E_{s' \\sim \\hat{P}(\\cdot|s,a)}[V(s'; g)]\\right|] \\\\\n&\\lesssim \\gamma V_{\\text{max}^2} E(s,a,g) \\sim \\mu[\\|P^*(\\cdot|s, a) - \\hat{P}(\\cdot|s, a)\\|_{TV}].\n\\end{align*}\n\\]\n\nBy Lemma 5, with probability at least \\( 1 - \\delta \\), for all \\( V \\in V \\),\n\n$$\n\\left|\\hat{L}_2(V) - \\hat{L}_2(V)\\right| \\lesssim \\gamma V_{\\text{max}}^2 \\log(|P|/\\delta). \\quad (18)\n$$\n\nBy rescaling \\( \\delta \\) and applying a union bound, we can obtain that with probability at least \\( 1 - \\delta \\), (16), (17), (18) hold simultaneously. The conclusion holds by applying the triangle inequality.\n\nFinally, we show that \\( \\hat{U}^+ \\) and \\( U^*_{\\alpha^+} \\) are close."}]}, {"page": 19, "text": "Lemma 7 (Closeness of \u02c6                   U+ and U \u2217        \u03b1+ in stochastic dynamics). Under Assumptions 1 to 3, with\nprobability at least 1 \u2212               \u03b4,          \u2225  \u02c6                                                       .\n                                                     U+ \u2212      U \u2217                           \u01ebstochastic\n                                                                  \u03b1+\u22252,\u00b5\u2264         O            stat\n                                                                             log(|V||P|/\u03b4)\nwhere \u02c6    U is the output of Algorithm 3, \u03b1 = \u2126                                   \u221a  N             and\n                                                     \u01ebstochastic   \u224d    V 2      log(|V||P|/\u03b4)              .\n                                                       stat               max                 N\nProof. For convenience, let \u02dc                 U = U \u02c6     V . Following the same analysis as in the deterministic case, we\nhave \u2225U \u2217              U+\u22252,\u00b5\u2264          O         \u01ebstochastic      by Lemma 6. Also,\n             \u03b1+ \u2212       \u02dc                           stat\n             \u2225  \u02dc\n               U+ \u2212       \u02c6\n                          U+\u22252             U \u2212      \u02c6\n                                 2,\u00b5\u2264\u2225      \u02dc      U\u22252   2,\u00b5\n                                      =E    (s,a,g)\u223c\u00b5          \u03b3Es\u2032\u223cP \u22c6(\u00b7|s,a)[ \u02c6       V (s; g)] \u2212        \u03b3Es\u2032\u223c     P\u02c6(\u00b7|s,a)[ \u02c6V (s; g)]      2\n                                      \u2272\u03b32V 2    maxE(s,a,g)\u223c\u00b5            \u2225  P\u02c6 (\u00b7|s, a) \u2212      P \u22c6(\u00b7|s, a)\u22252      TV\n                                      \u2272\u03b32V 2    max   log(|P|/\u03b4)         ,\n                                                              N\nwhere the last inequality holds by Lemma 5. Therefore,\n                                      \u2225  \u02c6\n                                        U+ \u2212       U \u2217                U+ \u2212       U \u2217                 U+ \u2212       \u02c6\n                                                     \u03b1+\u22252,\u00b5\u2264\u2225          \u02dc            \u03b1+\u22252,\u00b5+\u2225         \u02dc         U+\u22252,\u00b5\n                                                                 \u2272      \u01ebstochastic   + \u03b3Vmax             log(|P|/\u03b4)\n                                                                          stat\n                                                                 \u2272   \u01ebstochastic.                                N\n                                                                          stat\nC       Proof for Policy Learning\nWe provide a theoretical analysis of policy learning in this section. We fi                                          rst show two key lemmas in\nAppendix C.1, and then provide missing proofs of the main text in Appendix C.2 and Appendix C.3.\nC.1       Key Lemmas\nLemma 8 (Statistical error of the weighted MLE objective). Under Assumption 5, with probability\nat least 1 \u2212        \u03b4, for any policy \u03c0 \u2208              \u03a0, it holds that\n                                      |LMLE(\u03c0) \u2212           \u02c6\n                                                          LMLE     (\u03c0)|\u2264      (\u01ebU + \u01eb\u03a0) log(1/\u03c4) \u225c                  \u01ebMLE\n                                          \u03b1                                                                           stat .\nwhere \u01ebU \u225c           O(       \u01ebstat/\u03b12) and \u01eb\u03a0 \u225c              O     C\u2217        log(|\u03a0|/\u03b4)      + Vmax                            .\n                                                                       \u03b1            N               \u03b1N log(|\u03a0|/\u03b4)\nProof. For convenience, we assume that the offl                                 ine dataset used in policy learning is independent\nof the dataset used in V -Learning. This can be easily achieved by splitting the original dataset D\nuniformly at random into two datasets of equal size. Also, for analysis, we defi                                                 ne\n                                                                      \u02c6\n                             \u02dc                                        U(s, a; g)+\n                             LMLE     (\u03c0) = E       (s,a,g)\u223c\u00b5                 \u03b1           log \u03c0(a|s, g)           ,    \u2200\u03c0 \u2208      \u03a0.\nNow we condition on the high probability event in Lemma 3, which we denote by E1. Then by\nLemma 1, we have                  U+\u02c6         U\u2217\u03b1+\n                                    \u03b1 \u2212         \u03b1      2,\u00b5 \u2264      \u01ebU. By Cauchy-Schwarz inequality, we can obtain that for\n                                                                               19", "md": "Lemma 7 (Closeness of $$\\hat{U}^+$$ and $$U^*_{\\alpha^+}$$ in stochastic dynamics). Under Assumptions 1 to 3, with probability at least 1 - $$\\delta$$, $$\\| \\hat{U}^+ - U^*_{\\alpha^+} \\|_{2,\\mu} \\leq O\\left( \\log(|V||P|/\\delta) \\right)$$ where $$\\hat{U}$$ is the output of Algorithm 3, $$\\alpha = \\Omega\\left( \\sqrt{N} \\right)$$ and $$\\epsilon_{\\text{stochastic}} \\approx V^2 \\log(|V||P|/\\delta)/\\max(N)$$.\n\nProof. For convenience, let $$\\tilde{U} = U \\hat{V}$$. Following the same analysis as in the deterministic case, we have $$\\|U^* - U^+\\|_{2,\\mu} \\leq O(\\epsilon_{\\text{stochastic}})$$ by Lemma 6. Also,\n\n$$\n\\begin{aligned}\n&\\alpha^+ - \\tilde{\\alpha}^+ \\\\\n&\\| \\tilde{U}^+ - \\hat{U}^+ \\|_{2,\\mu} \\leq \\| \\tilde{U} \\|_{2,\\mu}^2 \\\\\n&= E_{(s,a,g) \\sim \\mu} \\left[ \\gamma E_{s' \\sim P^*(\\cdot|s,a)}[ \\hat{V}(s;g) ] - \\gamma E_{s' \\sim P^{\\hat{V}}(\\cdot|s,a)}[ \\hat{V}(s;g) ] \\right]^2 \\\\\n&\\lesssim \\gamma^2 V^2 \\max_{E(s,a,g) \\sim \\mu} \\| P^{\\hat{V}}(\\cdot|s,a) - P^*(\\cdot|s,a) \\|_2^{TV} \\\\\n&\\lesssim \\gamma^2 V^2 \\max \\left( \\log(|P|/\\delta), N \\right),\n\\end{aligned}\n$$\n\nwhere the last inequality holds by Lemma 5. Therefore,\n\n$$\n\\begin{aligned}\n\\| \\hat{U}^+ - U^* \\|_{\\alpha^+, \\mu} &\\leq \\| \\tilde{U}^+ - \\alpha^+ \\|_{2,\\mu} + \\| \\tilde{U}^+ \\|_{2,\\mu} \\\\\n&\\lesssim \\epsilon_{\\text{stochastic}} + \\gamma V \\max \\left( \\log(|P|/\\delta), N \\right) \\\\\n&\\lesssim \\epsilon_{\\text{stochastic}}.\n\\end{aligned}\n$$\n\nProof for Policy Learning\n\nWe provide a theoretical analysis of policy learning in this section. We first show two key lemmas in Appendix C.1, and then provide missing proofs of the main text in Appendix C.2 and Appendix C.3.\n\nC.1 Key Lemmas\n\nLemma 8 (Statistical error of the weighted MLE objective). Under Assumption 5, with probability at least 1 - $$\\delta$$, for any policy $$\\pi \\in \\Pi$$, it holds that\n\n$$\n\\left| L_{\\text{MLE}}(\\pi) - \\hat{L}_{\\text{MLE}}(\\pi) \\right| \\leq (\\epsilon_U + \\epsilon_{\\Pi}) \\log(1/\\tau) \\triangleq \\epsilon_{\\text{MLE}}^{\\text{stat}},\n$$\n\nwhere $$\\epsilon_U \\triangleq O(\\epsilon_{\\text{stat}}/\\alpha^2)$$ and $$\\epsilon_{\\Pi} \\triangleq O\\left( C^* \\log(|\\Pi|/\\delta) + V_{\\text{max}}/\\alpha + \\alpha N \\log(|\\Pi|/\\delta) \\right)$$.\n\nProof. For convenience, we assume that the offline dataset used in policy learning is independent of the dataset used in V-Learning. This can be easily achieved by splitting the original dataset D uniformly at random into two datasets of equal size. Also, for analysis, we define\n\n$$\n\\hat{L}_{\\text{MLE}}(\\pi) = E_{(s,a,g) \\sim \\mu} \\left[ \\alpha \\log \\pi(a|s,g) \\right], \\quad \\forall \\pi \\in \\Pi.\n$$\n\nNow we condition on the high probability event in Lemma 3, which we denote by E1. Then by Lemma 1, we have $$U^+\\hat{U}^*_{\\alpha^+} - \\alpha^+ \\leq \\epsilon_U$$. By Cauchy-Schwarz inequality, we can obtain that for", "images": [], "items": [{"type": "text", "value": "Lemma 7 (Closeness of $$\\hat{U}^+$$ and $$U^*_{\\alpha^+}$$ in stochastic dynamics). Under Assumptions 1 to 3, with probability at least 1 - $$\\delta$$, $$\\| \\hat{U}^+ - U^*_{\\alpha^+} \\|_{2,\\mu} \\leq O\\left( \\log(|V||P|/\\delta) \\right)$$ where $$\\hat{U}$$ is the output of Algorithm 3, $$\\alpha = \\Omega\\left( \\sqrt{N} \\right)$$ and $$\\epsilon_{\\text{stochastic}} \\approx V^2 \\log(|V||P|/\\delta)/\\max(N)$$.\n\nProof. For convenience, let $$\\tilde{U} = U \\hat{V}$$. Following the same analysis as in the deterministic case, we have $$\\|U^* - U^+\\|_{2,\\mu} \\leq O(\\epsilon_{\\text{stochastic}})$$ by Lemma 6. Also,\n\n$$\n\\begin{aligned}\n&\\alpha^+ - \\tilde{\\alpha}^+ \\\\\n&\\| \\tilde{U}^+ - \\hat{U}^+ \\|_{2,\\mu} \\leq \\| \\tilde{U} \\|_{2,\\mu}^2 \\\\\n&= E_{(s,a,g) \\sim \\mu} \\left[ \\gamma E_{s' \\sim P^*(\\cdot|s,a)}[ \\hat{V}(s;g) ] - \\gamma E_{s' \\sim P^{\\hat{V}}(\\cdot|s,a)}[ \\hat{V}(s;g) ] \\right]^2 \\\\\n&\\lesssim \\gamma^2 V^2 \\max_{E(s,a,g) \\sim \\mu} \\| P^{\\hat{V}}(\\cdot|s,a) - P^*(\\cdot|s,a) \\|_2^{TV} \\\\\n&\\lesssim \\gamma^2 V^2 \\max \\left( \\log(|P|/\\delta), N \\right),\n\\end{aligned}\n$$\n\nwhere the last inequality holds by Lemma 5. Therefore,\n\n$$\n\\begin{aligned}\n\\| \\hat{U}^+ - U^* \\|_{\\alpha^+, \\mu} &\\leq \\| \\tilde{U}^+ - \\alpha^+ \\|_{2,\\mu} + \\| \\tilde{U}^+ \\|_{2,\\mu} \\\\\n&\\lesssim \\epsilon_{\\text{stochastic}} + \\gamma V \\max \\left( \\log(|P|/\\delta), N \\right) \\\\\n&\\lesssim \\epsilon_{\\text{stochastic}}.\n\\end{aligned}\n$$\n\nProof for Policy Learning\n\nWe provide a theoretical analysis of policy learning in this section. We first show two key lemmas in Appendix C.1, and then provide missing proofs of the main text in Appendix C.2 and Appendix C.3.\n\nC.1 Key Lemmas\n\nLemma 8 (Statistical error of the weighted MLE objective). Under Assumption 5, with probability at least 1 - $$\\delta$$, for any policy $$\\pi \\in \\Pi$$, it holds that\n\n$$\n\\left| L_{\\text{MLE}}(\\pi) - \\hat{L}_{\\text{MLE}}(\\pi) \\right| \\leq (\\epsilon_U + \\epsilon_{\\Pi}) \\log(1/\\tau) \\triangleq \\epsilon_{\\text{MLE}}^{\\text{stat}},\n$$\n\nwhere $$\\epsilon_U \\triangleq O(\\epsilon_{\\text{stat}}/\\alpha^2)$$ and $$\\epsilon_{\\Pi} \\triangleq O\\left( C^* \\log(|\\Pi|/\\delta) + V_{\\text{max}}/\\alpha + \\alpha N \\log(|\\Pi|/\\delta) \\right)$$.\n\nProof. For convenience, we assume that the offline dataset used in policy learning is independent of the dataset used in V-Learning. This can be easily achieved by splitting the original dataset D uniformly at random into two datasets of equal size. Also, for analysis, we define\n\n$$\n\\hat{L}_{\\text{MLE}}(\\pi) = E_{(s,a,g) \\sim \\mu} \\left[ \\alpha \\log \\pi(a|s,g) \\right], \\quad \\forall \\pi \\in \\Pi.\n$$\n\nNow we condition on the high probability event in Lemma 3, which we denote by E1. Then by Lemma 1, we have $$U^+\\hat{U}^*_{\\alpha^+} - \\alpha^+ \\leq \\epsilon_U$$. By Cauchy-Schwarz inequality, we can obtain that for", "md": "Lemma 7 (Closeness of $$\\hat{U}^+$$ and $$U^*_{\\alpha^+}$$ in stochastic dynamics). Under Assumptions 1 to 3, with probability at least 1 - $$\\delta$$, $$\\| \\hat{U}^+ - U^*_{\\alpha^+} \\|_{2,\\mu} \\leq O\\left( \\log(|V||P|/\\delta) \\right)$$ where $$\\hat{U}$$ is the output of Algorithm 3, $$\\alpha = \\Omega\\left( \\sqrt{N} \\right)$$ and $$\\epsilon_{\\text{stochastic}} \\approx V^2 \\log(|V||P|/\\delta)/\\max(N)$$.\n\nProof. For convenience, let $$\\tilde{U} = U \\hat{V}$$. Following the same analysis as in the deterministic case, we have $$\\|U^* - U^+\\|_{2,\\mu} \\leq O(\\epsilon_{\\text{stochastic}})$$ by Lemma 6. Also,\n\n$$\n\\begin{aligned}\n&\\alpha^+ - \\tilde{\\alpha}^+ \\\\\n&\\| \\tilde{U}^+ - \\hat{U}^+ \\|_{2,\\mu} \\leq \\| \\tilde{U} \\|_{2,\\mu}^2 \\\\\n&= E_{(s,a,g) \\sim \\mu} \\left[ \\gamma E_{s' \\sim P^*(\\cdot|s,a)}[ \\hat{V}(s;g) ] - \\gamma E_{s' \\sim P^{\\hat{V}}(\\cdot|s,a)}[ \\hat{V}(s;g) ] \\right]^2 \\\\\n&\\lesssim \\gamma^2 V^2 \\max_{E(s,a,g) \\sim \\mu} \\| P^{\\hat{V}}(\\cdot|s,a) - P^*(\\cdot|s,a) \\|_2^{TV} \\\\\n&\\lesssim \\gamma^2 V^2 \\max \\left( \\log(|P|/\\delta), N \\right),\n\\end{aligned}\n$$\n\nwhere the last inequality holds by Lemma 5. Therefore,\n\n$$\n\\begin{aligned}\n\\| \\hat{U}^+ - U^* \\|_{\\alpha^+, \\mu} &\\leq \\| \\tilde{U}^+ - \\alpha^+ \\|_{2,\\mu} + \\| \\tilde{U}^+ \\|_{2,\\mu} \\\\\n&\\lesssim \\epsilon_{\\text{stochastic}} + \\gamma V \\max \\left( \\log(|P|/\\delta), N \\right) \\\\\n&\\lesssim \\epsilon_{\\text{stochastic}}.\n\\end{aligned}\n$$\n\nProof for Policy Learning\n\nWe provide a theoretical analysis of policy learning in this section. We first show two key lemmas in Appendix C.1, and then provide missing proofs of the main text in Appendix C.2 and Appendix C.3.\n\nC.1 Key Lemmas\n\nLemma 8 (Statistical error of the weighted MLE objective). Under Assumption 5, with probability at least 1 - $$\\delta$$, for any policy $$\\pi \\in \\Pi$$, it holds that\n\n$$\n\\left| L_{\\text{MLE}}(\\pi) - \\hat{L}_{\\text{MLE}}(\\pi) \\right| \\leq (\\epsilon_U + \\epsilon_{\\Pi}) \\log(1/\\tau) \\triangleq \\epsilon_{\\text{MLE}}^{\\text{stat}},\n$$\n\nwhere $$\\epsilon_U \\triangleq O(\\epsilon_{\\text{stat}}/\\alpha^2)$$ and $$\\epsilon_{\\Pi} \\triangleq O\\left( C^* \\log(|\\Pi|/\\delta) + V_{\\text{max}}/\\alpha + \\alpha N \\log(|\\Pi|/\\delta) \\right)$$.\n\nProof. For convenience, we assume that the offline dataset used in policy learning is independent of the dataset used in V-Learning. This can be easily achieved by splitting the original dataset D uniformly at random into two datasets of equal size. Also, for analysis, we define\n\n$$\n\\hat{L}_{\\text{MLE}}(\\pi) = E_{(s,a,g) \\sim \\mu} \\left[ \\alpha \\log \\pi(a|s,g) \\right], \\quad \\forall \\pi \\in \\Pi.\n$$\n\nNow we condition on the high probability event in Lemma 3, which we denote by E1. Then by Lemma 1, we have $$U^+\\hat{U}^*_{\\alpha^+} - \\alpha^+ \\leq \\epsilon_U$$. By Cauchy-Schwarz inequality, we can obtain that for"}]}, {"page": 20, "text": " any policy \u03c0,\n                                                                    U \u2217                   U(s, a; g)+\n             |LMLE(\u03c0) \u2212         \u02dc                                     \u03b1(s, a; g)+ \u2212        \u02c6\n                 \u03b1             LMLE    (\u03c0)|\u2264E(s,a,g)\u223c\u00b5                                \u03b1                      |log \u03c0(a|s, g)|\n                                                    \u02c6\n                                             \u2264     U+\u03b1 \u2212     U \u2217\u03b1+         \u2225log \u03c0(a|s, g)\u22252,\u00b5\n                                                                \u03b1      2,\u00b5\n                                             \u2264\u01ebU log(1/\u03c4).\n Also, for any fi      xed \u03c0 \u2208       \u03a0, since E(s,a,g)\u223c\u00b5[\u02c6         LMLE   (\u03c0)] = \u02dc    LMLE    (\u03c0), we can obtain by Bernstein\u2019s\n inequality that with probability at least 1 \u2212                  \u03b4,\n             |\u02dc\n              LMLE    (\u03c0) \u2212     \u02c6\n                 \uf8eb             LMLE\u02c6   (\u03c0)|                                                                                       \uf8f6\n                        Var  \u00b5    U(s,a;g)+     log \u03c0(a|s, g)        log(1/\u03b4)                                                     \uf8f7\n                 \uf8ec                      \u03b1                                                U+/\u03b1\u2225\u221elog(1/\u03c4)                           \uf8f7\n          \u2264O     \uf8ec                                                                 + \u2225    \u02c6                          log(1/\u03b4)     \uf8f8  .\n                 \uf8ed                                 N                                                N\n Since U \u2217   \u03b1(s, a; g)+/\u03b1 = d\u2217           \u03b1(s, a; g)/\u00b5(s, a; g) \u2264              C\u2217 \u03b1, we have \u2225U \u2217        \u03b1+/\u03b1\u22252,\u00b5\u2264           C\u2217\u03b1 and thus\n \u2225 \u02c6\n  U+/\u03b1\u22252,\u00b5\u2264          \u2225U \u2217 \u03b1+/\u03b1\u22252,\u00b5+\u01ebU \u2264             C\u2217\u03b1 + \u01ebU \u2264       O(C\u2217   \u03b1). Also,\n                              \u02c6                                              \uf8ee     \u02c6                2                       \uf8f9\n                             U(s, a; g)+                                          U(s, a; g)+\n                  Var  \u00b5            \u03b1          log \u03c0(a|s, g)         \u2264E\u00b5     \uf8f0            \u03b1              log2 \u03c0(a|s, g)     \uf8fb\n                                                                             \uf8ee     \u02c6                2\uf8f9\n                                                                             \uf8f0    U(s, a; g)+           \uf8fb  log2(1/\u03c4)\n                                                                     \u2264E\u00b5                  \u03b1\n                                                                     \u2264O((C\u2217     \u03b1)2 log2(1/\u03c4)).\n Applying a union bound over all \u03c0 \u2208                   \u03a0, it holds that with probability at least 1 \u2212                  \u03b4, for all \u03c0 \u2208     \u03a0\n                             |\u02dc\n                              LMLE   (\u03c0) \u2212     \u02c6\n                                               LMLE    (\u03c0)|\n                          \u2264O       C\u2217\u03b1 log(1/\u03c4)       log(|\u03a0|/\u03b4)          + Vmax log(1/\u03c4)           log(|\u03a0|/\u03b4)\n                                                               N                      \u03b1N\n                          =\u01eb\u03a0 log(1/\u03c4).\nWe denote the above event by E2. When E1 and E2 hold simultaneously, we have by the triangle\n inequality that\n    |LMLE(\u03c0) \u2212         \u02c6\n                      LMLE    (\u03c0)|\u2264     |LMLE(\u03c0) \u2212         \u02dc\n                                                          LMLE    (\u03c0)|+|\u02dc   LMLE    (\u03c0) \u2212     \u02c6\n Also, \u03b1                                    \u03b1                                                 LMLE    (\u03c0)|\u2264    (\u01ebU + \u01eb\u03a0) log(1/\u03c4).\n                            P(\u00ac(E1 \u2229       E2)) =P(\u00acE1 \u222a           \u00acE2) \u2264      P(\u00acE1) + P(\u00acE2)\n                                                  \u2264\u03b4 + P(E1)P(\u00acE2|E1) + P(\u00acE1)P(\u00acE2|\u00acE1)\n The conclusion holds by rescaling \u03b4.             \u2264\u03b4 + 1 \u00d7 \u03b4 + \u03b4 \u00d7 1 \u2264               3\u03b4.\n Lemma 9 (Closeness of MLE objective of \u03c0\u2217                      \u03b1 and \u02c6  \u03c0). Under Assumption 4, with probability at least\n1 \u2212    \u03b4,                                      LMLE    (\u03c0\u2217                (\u02c6\u03c0) \u2264    O(\u01ebMLE\n                                                  \u03b1        \u03b1) \u2212   LMLE\n                                                                     \u03b1                    stat ).\n Proof. We condition on the high probability event in Lemma 8. Note that\n LMLE    (\u03c0\u2217                (\u02c6\n   \u03b1        \u03b1) \u2212    LMLE     \u03c0) = LMLE(\u03c0\u2217                LMLE(\u03c0\u2217         + \u02c6LMLE    (\u03c0\u2217        LMLE(\u02c6    \u03c0)  + \u02c6LMLE    (\u02c6\u03c0) \u2212    LMLE(\u02c6   \u03c0)  .\n                      \u03b1                 \u03b1        \u03b1) \u2212    \u02c6          \u03b1)                 \u03b1) \u2212     \u02c6                                   \u03b1\n                                                     (1)                                  (2)                                 (3)\n(1), (3) \u2264       O(\u01ebMLE                                                                             \u03c0 in empirical MLE objective,\n                      stat ) by Lemma 8 and (2) \u2264                  0 by the optimality of \u02c6\n which completes the proof.\n                                                                       20", "md": "$$\n\\begin{align*}\n&|LMLE(\\pi) - \\tilde{\\alpha} LMLE(\\pi)| \\leq E(s,a,g) \\sim \\mu \\left| \\alpha LMLE(\\pi) \\right| \\leq \\alpha |log \\pi(a|s, g)| \\\\\n&\\leq U+\\alpha - U^*\\alpha^+ + ||log \\pi(a|s, g)||_2,\\mu \\\\\n&\\leq \\epsilon U log(1/\\tau).\n\\end{align*}\n$$\n\nAlso, for any fixed $\\pi \\in \\Pi$, since $E(s,a,g) \\sim \\mu[\\hat{LMLE}(\\pi)] = \\tilde{LMLE}(\\pi)$, we can obtain by Bernstein\u2019s inequality that with probability at least $1 - \\delta$,\n$$\n\\begin{aligned}\n&|\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)| \\\\\n&\\leq O \\left( \\frac{Var_{\\mu} U(s,a;g)+ log \\pi(a|s, g)}{\\alpha U^+/\\alpha \\infty log(1/\\tau) + ||\\hat{LMLE}(\\pi) log(1/\\delta)||} \\right).\n\\end{aligned}\n$$\n\nSince $U^*\\alpha(s, a; g)+/\\alpha = d^* \\alpha(s, a; g)/\\mu(s, a; g) \\leq C^* \\alpha$, we have $||U^*\\alpha^+/\\alpha||_2,\\mu \\leq C^* \\alpha$ and thus $||\\hat{U}^+/\\alpha||_2,\\mu \\leq ||U^*\\alpha^+/\\alpha||_2,\\mu + \\epsilon U \\leq O(C^* \\alpha)$. Also,\n$$\n\\begin{aligned}\n&Var_{\\mu} U(s, a; g)+ \\alpha log \\pi(a|s, g) \\\\\n&\\leq E_{\\mu} \\left[ U(s, a; g)+ \\alpha log^2 \\pi(a|s, g) \\right] \\\\\n&\\leq O((C^* \\alpha)^2 log^2(1/\\tau)).\n\\end{aligned}\n$$\n\nApplying a union bound over all $\\pi \\in \\Pi$, it holds that with probability at least $1 - \\delta$, for all $\\pi \\in \\Pi$,\n$$\n\\begin{aligned}\n&|\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)| \\\\\n&\\leq O(C^* \\alpha log(1/\\tau) log(|\\Pi|/\\delta) + V_{max} log(1/\\tau) log(|\\Pi|/\\delta)) \\\\\n&= \\epsilon \\Pi log(1/\\tau).\n\\end{aligned}\n$$\n\nWe denote the above event by $E2$. When $E1$ and $E2$ hold simultaneously, we have by the triangle inequality that\n$$\n|LMLE(\\pi) - \\hat{LMLE}(\\pi)| \\leq |LMLE(\\pi) - \\tilde{LMLE}(\\pi)| + |\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)| \\leq (\\epsilon U + \\epsilon \\Pi) log(1/\\tau).\n$$\n\n$$\n\\begin{aligned}\n&P(\\neg(E1 \\cap E2)) = P(\\neg E1 \\cup \\neg E2) \\\\\n&\\leq P(\\neg E1) + P(\\neg E2) \\\\\n&\\leq \\delta + P(E1)P(\\neg E2|E1) + P(\\neg E1)P(\\neg E2|\\neg E1) \\\\\n&\\leq \\delta + 1 \\times \\delta + \\delta \\times 1 \\leq 3\\delta.\n\\end{aligned}\n$$\n\nLemma 9 (Closeness of MLE objective of $\\pi^*$ and $\\hat{\\pi}$). Under Assumption 4, with probability at least $1 - \\delta$,\n$$\nLMLE(\\pi^*) - LMLE(\\hat{\\pi}) \\leq O(\\epsilon_{MLE} - LMLE_{stat}).\n$$\n\nProof. We condition on the high probability event in Lemma 8. Note that\n$$\n\\begin{aligned}\n&LMLE(\\pi^*) - LMLE(\\hat{\\pi}) \\\\\n&= LMLE(\\pi^*) - LMLE(\\pi^*) + \\hat{LMLE}(\\pi^*) - LMLE(\\hat{\\pi}) + \\hat{LMLE}(\\hat{\\pi}) - LMLE(\\hat{\\pi}).\n\\end{aligned}\n$$\n\n$(1), (3) \\leq O(\\epsilon_{MLE} - LMLE_{stat})$ by Lemma 8 and $(2) \\leq 0$ by the optimality of $\\hat{\\pi}$ which completes the proof.", "images": [], "items": [{"type": "text", "value": "$$\n\\begin{align*}\n&|LMLE(\\pi) - \\tilde{\\alpha} LMLE(\\pi)| \\leq E(s,a,g) \\sim \\mu \\left| \\alpha LMLE(\\pi) \\right| \\leq \\alpha |log \\pi(a|s, g)| \\\\\n&\\leq U+\\alpha - U^*\\alpha^+ + ||log \\pi(a|s, g)||_2,\\mu \\\\\n&\\leq \\epsilon U log(1/\\tau).\n\\end{align*}\n$$\n\nAlso, for any fixed $\\pi \\in \\Pi$, since $E(s,a,g) \\sim \\mu[\\hat{LMLE}(\\pi)] = \\tilde{LMLE}(\\pi)$, we can obtain by Bernstein\u2019s inequality that with probability at least $1 - \\delta$,\n$$\n\\begin{aligned}\n&|\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)| \\\\\n&\\leq O \\left( \\frac{Var_{\\mu} U(s,a;g)+ log \\pi(a|s, g)}{\\alpha U^+/\\alpha \\infty log(1/\\tau) + ||\\hat{LMLE}(\\pi) log(1/\\delta)||} \\right).\n\\end{aligned}\n$$\n\nSince $U^*\\alpha(s, a; g)+/\\alpha = d^* \\alpha(s, a; g)/\\mu(s, a; g) \\leq C^* \\alpha$, we have $||U^*\\alpha^+/\\alpha||_2,\\mu \\leq C^* \\alpha$ and thus $||\\hat{U}^+/\\alpha||_2,\\mu \\leq ||U^*\\alpha^+/\\alpha||_2,\\mu + \\epsilon U \\leq O(C^* \\alpha)$. Also,\n$$\n\\begin{aligned}\n&Var_{\\mu} U(s, a; g)+ \\alpha log \\pi(a|s, g) \\\\\n&\\leq E_{\\mu} \\left[ U(s, a; g)+ \\alpha log^2 \\pi(a|s, g) \\right] \\\\\n&\\leq O((C^* \\alpha)^2 log^2(1/\\tau)).\n\\end{aligned}\n$$\n\nApplying a union bound over all $\\pi \\in \\Pi$, it holds that with probability at least $1 - \\delta$, for all $\\pi \\in \\Pi$,\n$$\n\\begin{aligned}\n&|\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)| \\\\\n&\\leq O(C^* \\alpha log(1/\\tau) log(|\\Pi|/\\delta) + V_{max} log(1/\\tau) log(|\\Pi|/\\delta)) \\\\\n&= \\epsilon \\Pi log(1/\\tau).\n\\end{aligned}\n$$\n\nWe denote the above event by $E2$. When $E1$ and $E2$ hold simultaneously, we have by the triangle inequality that\n$$", "md": "$$\n\\begin{align*}\n&|LMLE(\\pi) - \\tilde{\\alpha} LMLE(\\pi)| \\leq E(s,a,g) \\sim \\mu \\left| \\alpha LMLE(\\pi) \\right| \\leq \\alpha |log \\pi(a|s, g)| \\\\\n&\\leq U+\\alpha - U^*\\alpha^+ + ||log \\pi(a|s, g)||_2,\\mu \\\\\n&\\leq \\epsilon U log(1/\\tau).\n\\end{align*}\n$$\n\nAlso, for any fixed $\\pi \\in \\Pi$, since $E(s,a,g) \\sim \\mu[\\hat{LMLE}(\\pi)] = \\tilde{LMLE}(\\pi)$, we can obtain by Bernstein\u2019s inequality that with probability at least $1 - \\delta$,\n$$\n\\begin{aligned}\n&|\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)| \\\\\n&\\leq O \\left( \\frac{Var_{\\mu} U(s,a;g)+ log \\pi(a|s, g)}{\\alpha U^+/\\alpha \\infty log(1/\\tau) + ||\\hat{LMLE}(\\pi) log(1/\\delta)||} \\right).\n\\end{aligned}\n$$\n\nSince $U^*\\alpha(s, a; g)+/\\alpha = d^* \\alpha(s, a; g)/\\mu(s, a; g) \\leq C^* \\alpha$, we have $||U^*\\alpha^+/\\alpha||_2,\\mu \\leq C^* \\alpha$ and thus $||\\hat{U}^+/\\alpha||_2,\\mu \\leq ||U^*\\alpha^+/\\alpha||_2,\\mu + \\epsilon U \\leq O(C^* \\alpha)$. Also,\n$$\n\\begin{aligned}\n&Var_{\\mu} U(s, a; g)+ \\alpha log \\pi(a|s, g) \\\\\n&\\leq E_{\\mu} \\left[ U(s, a; g)+ \\alpha log^2 \\pi(a|s, g) \\right] \\\\\n&\\leq O((C^* \\alpha)^2 log^2(1/\\tau)).\n\\end{aligned}\n$$\n\nApplying a union bound over all $\\pi \\in \\Pi$, it holds that with probability at least $1 - \\delta$, for all $\\pi \\in \\Pi$,\n$$\n\\begin{aligned}\n&|\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)| \\\\\n&\\leq O(C^* \\alpha log(1/\\tau) log(|\\Pi|/\\delta) + V_{max} log(1/\\tau) log(|\\Pi|/\\delta)) \\\\\n&= \\epsilon \\Pi log(1/\\tau).\n\\end{aligned}\n$$\n\nWe denote the above event by $E2$. When $E1$ and $E2$ hold simultaneously, we have by the triangle inequality that\n$$"}, {"type": "table", "rows": [["LMLE(\\pi) - \\hat{LMLE}(\\pi)", "\\leq", "LMLE(\\pi) - \\tilde{LMLE}(\\pi)", "+", "\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)"]], "md": "|LMLE(\\pi) - \\hat{LMLE}(\\pi)| \\leq |LMLE(\\pi) - \\tilde{LMLE}(\\pi)| + |\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)| \\leq (\\epsilon U + \\epsilon \\Pi) log(1/\\tau).", "isPerfectTable": true, "csv": "\"LMLE(\\pi) - \\hat{LMLE}(\\pi)\",\"\\leq\",\"LMLE(\\pi) - \\tilde{LMLE}(\\pi)\",\"+\",\"\\tilde{LMLE}(\\pi) - \\hat{LMLE}(\\pi)\""}, {"type": "text", "value": "$$\n\n$$\n\\begin{aligned}\n&P(\\neg(E1 \\cap E2)) = P(\\neg E1 \\cup \\neg E2) \\\\\n&\\leq P(\\neg E1) + P(\\neg E2) \\\\\n&\\leq \\delta + P(E1)P(\\neg E2|E1) + P(\\neg E1)P(\\neg E2|\\neg E1) \\\\\n&\\leq \\delta + 1 \\times \\delta + \\delta \\times 1 \\leq 3\\delta.\n\\end{aligned}\n$$\n\nLemma 9 (Closeness of MLE objective of $\\pi^*$ and $\\hat{\\pi}$). Under Assumption 4, with probability at least $1 - \\delta$,\n$$\nLMLE(\\pi^*) - LMLE(\\hat{\\pi}) \\leq O(\\epsilon_{MLE} - LMLE_{stat}).\n$$\n\nProof. We condition on the high probability event in Lemma 8. Note that\n$$\n\\begin{aligned}\n&LMLE(\\pi^*) - LMLE(\\hat{\\pi}) \\\\\n&= LMLE(\\pi^*) - LMLE(\\pi^*) + \\hat{LMLE}(\\pi^*) - LMLE(\\hat{\\pi}) + \\hat{LMLE}(\\hat{\\pi}) - LMLE(\\hat{\\pi}).\n\\end{aligned}\n$$\n\n$(1), (3) \\leq O(\\epsilon_{MLE} - LMLE_{stat})$ by Lemma 8 and $(2) \\leq 0$ by the optimality of $\\hat{\\pi}$ which completes the proof.", "md": "$$\n\n$$\n\\begin{aligned}\n&P(\\neg(E1 \\cap E2)) = P(\\neg E1 \\cup \\neg E2) \\\\\n&\\leq P(\\neg E1) + P(\\neg E2) \\\\\n&\\leq \\delta + P(E1)P(\\neg E2|E1) + P(\\neg E1)P(\\neg E2|\\neg E1) \\\\\n&\\leq \\delta + 1 \\times \\delta + \\delta \\times 1 \\leq 3\\delta.\n\\end{aligned}\n$$\n\nLemma 9 (Closeness of MLE objective of $\\pi^*$ and $\\hat{\\pi}$). Under Assumption 4, with probability at least $1 - \\delta$,\n$$\nLMLE(\\pi^*) - LMLE(\\hat{\\pi}) \\leq O(\\epsilon_{MLE} - LMLE_{stat}).\n$$\n\nProof. We condition on the high probability event in Lemma 8. Note that\n$$\n\\begin{aligned}\n&LMLE(\\pi^*) - LMLE(\\hat{\\pi}) \\\\\n&= LMLE(\\pi^*) - LMLE(\\pi^*) + \\hat{LMLE}(\\pi^*) - LMLE(\\hat{\\pi}) + \\hat{LMLE}(\\hat{\\pi}) - LMLE(\\hat{\\pi}).\n\\end{aligned}\n$$\n\n$(1), (3) \\leq O(\\epsilon_{MLE} - LMLE_{stat})$ by Lemma 8 and $(2) \\leq 0$ by the optimality of $\\hat{\\pi}$ which completes the proof."}]}, {"page": 21, "text": "C.2       Proof of Lemma 2\nProof of Lemma 2. We condition on the high probability event in Lemma 8. Note that\n                 LMLE     (\u03c0) =E       (s,a,g)\u223c\u00b5[g\u2032     \u2217(r(s; g) + \u03b3T V \u2217          \u03b1 (s, a; g) \u2212       V \u2217\u03b1 (s; g))+ log \u03c0(a|s, g)]\n                    \u03b1\nWe also defi        ne           =E    (s,a,g)\u223cd\u2217   \u03b1 [log \u03c0(a|s, g)] .\n           LMLE                          (\u03c0) \u2212      E(s,a,g)\u223cd\u2217                                                             log \u03c0(a|s, g)             ,\n              \u03b1,Rel(\u03c0) = LMLE      \u03b1                                \u03b1 [log \u03c0\u2217    \u03b1(a|s, g)] = E(s,a,g)\u223cd\u2217             \u03b1           \u03c0\u2217\u03b1(a|s, g)\nand note that LMLE        \u03b1,Rel is a constant shift of LMLE              \u03b1      . For any \u03c0 \u2208             \u03a0, we further defi             ne r\u03c0 = \u03c0/\u03c0\u2217           \u03b1.\nBy Assumption 5, r\u03c0(a|s, g) \u2208                       [\u03c4, 1/\u03c4]. Let \u02dc      LMLE\n                                                                            \u03b1,Rel(r\u03c0) = E(s,a,g)\u223cd\u2217              \u03b1[log r\u03c0(a|s, g)] which is 2\u03c4 2-\nstrongly concave w.r.t. \u2225\u00b7\u22252,d\u2217                 \u03b1 when r\u03c0(a|s, g) \u2208                [\u03c4, 1/\u03c4]. Since \u03c0\u2217         \u03b1 is the maximizer of LMLE             \u03b1       and\nthus the maximizer of LMLE              \u03b1,Rel, we have that r\u03c0\u2217                                               LMLE\n                                                                          \u03b1 is the maximizer of \u02dc                \u03b1,Rel. By strong concavity and\nthe optimality of r\u03c0\u2217           \u03b1, we have\n                                \u03c4 2\u2225r\u03c0\u2217                           LMLE                    LMLE          \u03c0)\n                                          \u03b1 \u2212    r\u02c6\u03c0\u22252 2,d\u2217 \u03b1\u2264     \u02dc \u03b1,Rel(r\u03c0\u2217    \u03b1) \u2212     \u02dc \u03b1,Rel(r\u02c6\n                             =LMLE \u03b1,Rel(\u03c0\u2217   \u03b1) \u2212     LMLE        \u03c0) = LMLE(\u03c0\u2217                            (\u02c6\u03c0) \u2264     O(\u01ebMLE\n                                                          \u03b1,Rel(\u02c6              \u03b1         \u03b1) \u2212     LMLE\n                                                                                                     \u03b1                       stat ),\nwhere the last inequality holds by Lemma 9. Note that\n                                \u2225r\u03c0\u2217                                                 r\u02c6                                      2\n                                      \u03b1 \u2212    r\u02c6\u03c0\u22252  2,d\u2217\u03b1=E(s,a,g)\u223cd\u2217         \u03b1       \u03c0(a|s, g) \u2212         r\u03c0\u2217 \u03b1(a|s, g)\n                                                                                        \u02c6                       2\n                                                                                        \u03c0(a|s, g)\n                                                          =E    (s,a,g)\u223cd\u2217    \u03b1        \u03c0\u2217\u03b1(a|s, g) \u2212         1\n                                                          \u2273E    (s,g)\u223cd\u2217   \u03b1    \u2225\u02c6\u03c0(\u00b7|s, g) \u2212        \u03c0\u2217\u03b1(\u00b7|s, g)\u22252     TV      2\n                                                          \u2265     E(s,g)\u223cd\u2217     \u03b1 [\u2225\u02c6 \u03c0(\u00b7|s, g) \u2212        \u03c0\u2217\u03b1(\u00b7|s, g)\u2225TV]\nwhere the last inequality holds since TV distance is upper bounded by \u03c72 distance. Therefore, we\ncan fi   nally obtain that\n                                E                                                                              \u01ebMLE              .\n                                   (s,g)\u223cd\u2217   \u03b1 [\u2225\u02c6 \u03c0(\u00b7|s, g) \u2212        \u03c0\u2217\u03b1(\u00b7|s, g)\u2225TV] \u2264            O            stat /\u03c4 2\nC.3       Proof of Theorem 1\nProof of Theorem 1. We condition on the high probability event in Lemma 8. By performance dif-\nference lemma [Agarwal et al., 2019], we have\n                  J(\u03c0\u2217   \u03b1) \u2212     J(\u02c6 \u03c0) =E(s,a,g)\u223cd\u2217          \u03b1[A\u02c6  \u03c0  (s, a; g)]\n                                           =E    (s,g)\u223cd\u2217   \u03b1[Ea\u223c\u03c0\u2217     \u03b1(\u00b7|s,g)A\u02c6     \u03c0 (s, a; g) \u2212       Ea\u223c\u02c6   \u03c0(\u00b7|s,g)A\u02c6    \u03c0(s, a; g)]\n                                                                                               \u03c0(\u00b7|s, g)\u22251]\n                                           \u2272VmaxE(s,g)\u223cd\u2217            \u03b1[\u2225\u03c0\u2217   \u03b1(\u00b7|s, g) \u2212       \u02c6\n                                                                                               \u03c0(\u00b7|s, g)\u2225TV]\n                                           \u2272VmaxE(s,g)\u223cd\u2217            \u03b1[\u2225\u03c0\u2217   \u03b1(\u00b7|s, g) \u2212       \u02c6\n                                           \u2272Vmax        \u01ebMLE stat\n                                                             \u03c4 2 ,\nwhere the last inequality holds by Lemma 2.\nD       Statistical Rate of the Suboptimality in Different Settings\nIn this section, we analyze the statistical rate of the suboptimality of the output policy \u02c6                                                     \u03c0 by Algo-\nrithm 1 in different settings.\n                                                                               21", "md": "# Math Equations and Text\n\n## C.2 Proof of Lemma 2\n\nProof of Lemma 2. We condition on the high probability event in Lemma 8. Note that\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) &= \\mathbb{E}_{(s,a,g)\\sim\\mu}[g'^{*}(r(s;g) + \\gamma T V^{*}_{\\alpha}(s,a;g) - V^{*}_{\\alpha}(s;g)) + \\log\\pi(a|s,g)] \\\\\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi) &= \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) - \\mathbb{E}_{(s,a,g)\\sim d^*}[\\log\\pi(a|s,g)],\n\\end{align*}\n$$\nWe also define\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi) &= \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) - \\mathbb{E}_{(s,a,g)\\sim d^*}[\\log\\pi(a|s,g)], \\\\\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi^*) &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log\\pi^*_{\\alpha}(a|s,g)] = \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log\\pi^*_{\\alpha}(a|s,g)].\n\\end{align*}\n$$\nAnd note that $\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$ is a constant shift of $\\mathcal{L}_{\\text{MLE}}^{\\alpha}$. For any $\\pi \\in \\Pi$, we further define $r_{\\pi} = \\frac{\\pi}{\\pi^*_{\\alpha}}$. By Assumption 5, $r_{\\pi}(a|s,g) \\in [\\tau, 1/\\tau]$. Let $\\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(r_{\\pi}) = \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log r_{\\pi}(a|s,g)]$ which is $2\\tau$-strongly concave w.r.t. $\\| \\cdot \\|_2, d^*_{\\alpha}$ when $r_{\\pi}(a|s,g) \\in [\\tau, 1/\\tau]$. Since $\\pi^*_{\\alpha}$ is the maximizer of $\\mathcal{L}_{\\text{MLE}}^{\\alpha}$ and thus the maximizer of $\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$, we have that $r_{\\pi^*_{\\alpha}}$ is the maximizer of $\\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$. By strong concavity and the optimality of $r_{\\pi^*_{\\alpha}}$, we have\n\n$$\n\\tau^2 \\| r_{\\pi^*_{\\alpha}} - \\hat{r}_{\\pi} \\|_2^2, d^*_{\\alpha} \\leq \\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(r_{\\pi^*_{\\alpha}}) - \\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\hat{r}_{\\pi}) = \\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi^*_{\\alpha}) - \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\hat{\\pi}) \\leq O(\\epsilon_{\\text{MLE}_{\\text{stat}}}),\n$$\nwhere the last inequality holds by Lemma 9. Note that\n\n$$\n\\begin{align*}\n\\| r_{\\pi^*_{\\alpha}} - \\hat{r}_{\\pi} \\|_2^2, d^*_{\\alpha} &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}} \\left[ \\pi(a|s,g) - \\frac{r_{\\pi^*_{\\alpha}}(a|s,g)}{\\pi(a|s,g)} \\right]^2 \\\\\n&= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}} \\left[ \\pi^*_{\\alpha}(a|s,g) - 1 \\right]^2 \\\\\n&\\gtrapprox \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} \\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_2^{\\text{TV}} \\\\\n&\\geq \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} [\\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_{\\text{TV}}].\n\\end{align*}\n$$\nWhere the last inequality holds since TV distance is upper bounded by $\\chi^2$ distance. Therefore, we can finally obtain that\n\n$$\n\\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} [\\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_{\\text{TV}}] \\leq O(\\epsilon_{\\text{MLE}_{\\text{stat}}}/\\tau^2).\n$$\n\n## C.3 Proof of Theorem 1\n\nProof of Theorem 1. We condition on the high probability event in Lemma 8. By performance difference lemma [Agarwal et al., 2019], we have\n\n$$\n\\begin{align*}\nJ(\\pi^*_{\\alpha}) - J(\\hat{\\pi}) &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[A\\hat{\\pi}(s,a;g)] \\\\\n&= \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}}[\\mathbb{E}_{a\\sim\\pi^*_{\\alpha}(\\cdot|s,g)}[A\\hat{\\pi}(s,a;g) - \\mathbb{E}_{a\\sim\\hat{\\pi}(\\cdot|s,g)}[A\\hat{\\pi}(s,a;g)]] \\\\\n&\\lesssim V_{\\text{max}} \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}}[\\| \\pi^*_{\\alpha}(\\cdot|s,g) - \\hat{\\pi}(\\cdot|s,g) \\|_{\\ell_1}] \\\\\n&\\lesssim V_{\\text{max}} \\epsilon_{\\text{MLE}_{\\text{stat}}}/\\tau^2,\n\\end{align*}\n$$\nwhere the last inequality holds by Lemma 2.\n\n## D Statistical Rate of the Suboptimality in Different Settings\n\nIn this section, we analyze the statistical rate of the suboptimality of the output policy $\\hat{\\pi}$ by Algorithm 1 in different settings.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "C.2 Proof of Lemma 2", "md": "## C.2 Proof of Lemma 2"}, {"type": "text", "value": "Proof of Lemma 2. We condition on the high probability event in Lemma 8. Note that\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) &= \\mathbb{E}_{(s,a,g)\\sim\\mu}[g'^{*}(r(s;g) + \\gamma T V^{*}_{\\alpha}(s,a;g) - V^{*}_{\\alpha}(s;g)) + \\log\\pi(a|s,g)] \\\\\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi) &= \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) - \\mathbb{E}_{(s,a,g)\\sim d^*}[\\log\\pi(a|s,g)],\n\\end{align*}\n$$\nWe also define\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi) &= \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) - \\mathbb{E}_{(s,a,g)\\sim d^*}[\\log\\pi(a|s,g)], \\\\\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi^*) &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log\\pi^*_{\\alpha}(a|s,g)] = \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log\\pi^*_{\\alpha}(a|s,g)].\n\\end{align*}\n$$\nAnd note that $\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$ is a constant shift of $\\mathcal{L}_{\\text{MLE}}^{\\alpha}$. For any $\\pi \\in \\Pi$, we further define $r_{\\pi} = \\frac{\\pi}{\\pi^*_{\\alpha}}$. By Assumption 5, $r_{\\pi}(a|s,g) \\in [\\tau, 1/\\tau]$. Let $\\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(r_{\\pi}) = \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log r_{\\pi}(a|s,g)]$ which is $2\\tau$-strongly concave w.r.t. $\\| \\cdot \\|_2, d^*_{\\alpha}$ when $r_{\\pi}(a|s,g) \\in [\\tau, 1/\\tau]$. Since $\\pi^*_{\\alpha}$ is the maximizer of $\\mathcal{L}_{\\text{MLE}}^{\\alpha}$ and thus the maximizer of $\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$, we have that $r_{\\pi^*_{\\alpha}}$ is the maximizer of $\\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$. By strong concavity and the optimality of $r_{\\pi^*_{\\alpha}}$, we have\n\n$$\n\\tau^2 \\| r_{\\pi^*_{\\alpha}} - \\hat{r}_{\\pi} \\|_2^2, d^*_{\\alpha} \\leq \\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(r_{\\pi^*_{\\alpha}}) - \\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\hat{r}_{\\pi}) = \\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi^*_{\\alpha}) - \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\hat{\\pi}) \\leq O(\\epsilon_{\\text{MLE}_{\\text{stat}}}),\n$$\nwhere the last inequality holds by Lemma 9. Note that\n\n$$\n\\begin{align*}\n\\| r_{\\pi^*_{\\alpha}} - \\hat{r}_{\\pi} \\|_2^2, d^*_{\\alpha} &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}} \\left[ \\pi(a|s,g) - \\frac{r_{\\pi^*_{\\alpha}}(a|s,g)}{\\pi(a|s,g)} \\right]^2 \\\\\n&= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}} \\left[ \\pi^*_{\\alpha}(a|s,g) - 1 \\right]^2 \\\\\n&\\gtrapprox \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} \\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_2^{\\text{TV}} \\\\\n&\\geq \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} [\\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_{\\text{TV}}].\n\\end{align*}\n$$\nWhere the last inequality holds since TV distance is upper bounded by $\\chi^2$ distance. Therefore, we can finally obtain that\n\n$$\n\\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} [\\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_{\\text{TV}}] \\leq O(\\epsilon_{\\text{MLE}_{\\text{stat}}}/\\tau^2).\n$$", "md": "Proof of Lemma 2. We condition on the high probability event in Lemma 8. Note that\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) &= \\mathbb{E}_{(s,a,g)\\sim\\mu}[g'^{*}(r(s;g) + \\gamma T V^{*}_{\\alpha}(s,a;g) - V^{*}_{\\alpha}(s;g)) + \\log\\pi(a|s,g)] \\\\\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi) &= \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) - \\mathbb{E}_{(s,a,g)\\sim d^*}[\\log\\pi(a|s,g)],\n\\end{align*}\n$$\nWe also define\n\n$$\n\\begin{align*}\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi) &= \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\pi) - \\mathbb{E}_{(s,a,g)\\sim d^*}[\\log\\pi(a|s,g)], \\\\\n\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi^*) &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log\\pi^*_{\\alpha}(a|s,g)] = \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log\\pi^*_{\\alpha}(a|s,g)].\n\\end{align*}\n$$\nAnd note that $\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$ is a constant shift of $\\mathcal{L}_{\\text{MLE}}^{\\alpha}$. For any $\\pi \\in \\Pi$, we further define $r_{\\pi} = \\frac{\\pi}{\\pi^*_{\\alpha}}$. By Assumption 5, $r_{\\pi}(a|s,g) \\in [\\tau, 1/\\tau]$. Let $\\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(r_{\\pi}) = \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[\\log r_{\\pi}(a|s,g)]$ which is $2\\tau$-strongly concave w.r.t. $\\| \\cdot \\|_2, d^*_{\\alpha}$ when $r_{\\pi}(a|s,g) \\in [\\tau, 1/\\tau]$. Since $\\pi^*_{\\alpha}$ is the maximizer of $\\mathcal{L}_{\\text{MLE}}^{\\alpha}$ and thus the maximizer of $\\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$, we have that $r_{\\pi^*_{\\alpha}}$ is the maximizer of $\\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}$. By strong concavity and the optimality of $r_{\\pi^*_{\\alpha}}$, we have\n\n$$\n\\tau^2 \\| r_{\\pi^*_{\\alpha}} - \\hat{r}_{\\pi} \\|_2^2, d^*_{\\alpha} \\leq \\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(r_{\\pi^*_{\\alpha}}) - \\tilde{\\mathcal{L}}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\hat{r}_{\\pi}) = \\mathcal{L}_{\\text{MLE}}^{\\alpha,\\text{Rel}}(\\pi^*_{\\alpha}) - \\mathcal{L}_{\\text{MLE}}^{\\alpha}(\\hat{\\pi}) \\leq O(\\epsilon_{\\text{MLE}_{\\text{stat}}}),\n$$\nwhere the last inequality holds by Lemma 9. Note that\n\n$$\n\\begin{align*}\n\\| r_{\\pi^*_{\\alpha}} - \\hat{r}_{\\pi} \\|_2^2, d^*_{\\alpha} &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}} \\left[ \\pi(a|s,g) - \\frac{r_{\\pi^*_{\\alpha}}(a|s,g)}{\\pi(a|s,g)} \\right]^2 \\\\\n&= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}} \\left[ \\pi^*_{\\alpha}(a|s,g) - 1 \\right]^2 \\\\\n&\\gtrapprox \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} \\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_2^{\\text{TV}} \\\\\n&\\geq \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} [\\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_{\\text{TV}}].\n\\end{align*}\n$$\nWhere the last inequality holds since TV distance is upper bounded by $\\chi^2$ distance. Therefore, we can finally obtain that\n\n$$\n\\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}} [\\| \\hat{\\pi}(\\cdot|s,g) - \\pi^*_{\\alpha}(\\cdot|s,g) \\|_{\\text{TV}}] \\leq O(\\epsilon_{\\text{MLE}_{\\text{stat}}}/\\tau^2).\n$$"}, {"type": "heading", "lvl": 2, "value": "C.3 Proof of Theorem 1", "md": "## C.3 Proof of Theorem 1"}, {"type": "text", "value": "Proof of Theorem 1. We condition on the high probability event in Lemma 8. By performance difference lemma [Agarwal et al., 2019], we have\n\n$$\n\\begin{align*}\nJ(\\pi^*_{\\alpha}) - J(\\hat{\\pi}) &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[A\\hat{\\pi}(s,a;g)] \\\\\n&= \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}}[\\mathbb{E}_{a\\sim\\pi^*_{\\alpha}(\\cdot|s,g)}[A\\hat{\\pi}(s,a;g) - \\mathbb{E}_{a\\sim\\hat{\\pi}(\\cdot|s,g)}[A\\hat{\\pi}(s,a;g)]] \\\\\n&\\lesssim V_{\\text{max}} \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}}[\\| \\pi^*_{\\alpha}(\\cdot|s,g) - \\hat{\\pi}(\\cdot|s,g) \\|_{\\ell_1}] \\\\\n&\\lesssim V_{\\text{max}} \\epsilon_{\\text{MLE}_{\\text{stat}}}/\\tau^2,\n\\end{align*}\n$$\nwhere the last inequality holds by Lemma 2.", "md": "Proof of Theorem 1. We condition on the high probability event in Lemma 8. By performance difference lemma [Agarwal et al., 2019], we have\n\n$$\n\\begin{align*}\nJ(\\pi^*_{\\alpha}) - J(\\hat{\\pi}) &= \\mathbb{E}_{(s,a,g)\\sim d^*_{\\alpha}}[A\\hat{\\pi}(s,a;g)] \\\\\n&= \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}}[\\mathbb{E}_{a\\sim\\pi^*_{\\alpha}(\\cdot|s,g)}[A\\hat{\\pi}(s,a;g) - \\mathbb{E}_{a\\sim\\hat{\\pi}(\\cdot|s,g)}[A\\hat{\\pi}(s,a;g)]] \\\\\n&\\lesssim V_{\\text{max}} \\mathbb{E}_{(s,g)\\sim d^*_{\\alpha}}[\\| \\pi^*_{\\alpha}(\\cdot|s,g) - \\hat{\\pi}(\\cdot|s,g) \\|_{\\ell_1}] \\\\\n&\\lesssim V_{\\text{max}} \\epsilon_{\\text{MLE}_{\\text{stat}}}/\\tau^2,\n\\end{align*}\n$$\nwhere the last inequality holds by Lemma 2."}, {"type": "heading", "lvl": 2, "value": "D Statistical Rate of the Suboptimality in Different Settings", "md": "## D Statistical Rate of the Suboptimality in Different Settings"}, {"type": "text", "value": "In this section, we analyze the statistical rate of the suboptimality of the output policy $\\hat{\\pi}$ by Algorithm 1 in different settings.", "md": "In this section, we analyze the statistical rate of the suboptimality of the output policy $\\hat{\\pi}$ by Algorithm 1 in different settings."}]}, {"page": 22, "text": "D.1       Deterministic Settings\nProof of Theorem 2. By Theorem 1 and Proposition 3.1,\n                J(\u03c0\u2217) \u2212        J(\u02c6 \u03c0)\n            =J(\u03c0\u2217) \u2212           J(\u03c0\u2217  \u03b1) + J(\u03c0\u2217       \u03b1) \u2212     J(\u02c6 \u03c0)\n            \u2272\u03b1(C\u2217      \u03b1)2 + Vmax          \u01ebMLE stat\n                                                 \u03c4 2\n            \u2272\u03b1(C\u2217      \u03b1)2 + Vmax              log(1/\u03c4)               \u01ebstat/\u03b12 + C\u2217       \u03b1      log(|\u03a0|/\u03b4)          + Vmax\nSince \u01ebstat \u224d        V 2       log(|V|/\u03b4)      \u03c4 , we can further obtain that                           N                 \u03b1N log(|\u03a0|/\u03b4).\n                        max             N\n    J(\u03c0\u2217) \u2212         J(\u02c6 \u03c0)\n \u2272\u03b1(C\u2217     \u03b1)2 + Vmax              log(1/\u03c4)            Vmax       log(|V|/\u03b4) 1/4                + C\u2217   \u03b1      log(|\u03a0|/\u03b4)         + Vmax\n                                   \u03c4                      \u03b1                 N                                        N                 \u03b1N log(|\u03a0|/\u03b4)\n \u2272\u03b1(C\u2217     \u03b1)2 + Vmax           log(1/\u03c4)               VmaxC\u2217      \u03b1\n                                   \u03c4                     \u03b1N 1/4 log(|V||\u03a0|/\u03b4)\n \u2272    V 3  max(C\u2217     \u03b1)3 log(1/\u03c4) log(|V||\u03a0|/\u03b4)                    1/3\n                               \u03c4 2N 1/4\n                      V 3                                     1/3\n                        max log(1/\u03c4) log(|V||\u03a0|/\u03b4)\nwhere \u03b1 \u224d                       \u03c4 2(C\u2217 \u03b1)3N 1/4                      .\nD.2       Stochastic Settings\nTheorem 3 (Statistical rate of the suboptimality in stochastic settings). Under Assumptions 1 to 5,\nwith probability at least 1 \u2212                   \u03b4, the output policy \u02c6           \u03c0 by Algorithm 1 (with the choice of Algorithm 3\nfor V -learning in stochastic settings) satisfies\n                             J(\u03c0\u2217) \u2212        J(\u02c6 \u03c0) \u2272       V 3  max(C\u2217    \u03b1)3 log(1/\u03c4) log(|V||P||\u03a0|/\u03b4)                       1/3\n                                                                                       \u03c4 2N 1/4\n                                V 3                                          1/3\n                                  max log(1/\u03c4) log(|V||P||\u03a0|/\u03b4)\nif we choose \u03b1 \u224d                            \u03c4 2(C\u2217 \u03b1)3N 1/4                          and assume N = N0.\nProof. The proof is identical to the proof of Theorem 2 except that we replace \u01ebstat with \u01ebstochastic.                                         stat\n                                                                               22", "md": "D.1 Deterministic Settings\n\nProof of Theorem 2. By Theorem 1 and Proposition 3.1,\n\n$$\n\\begin{align*}\nJ(\\pi^*) - J(\\hat{\\pi}) & = J(\\pi^*) - J(\\pi^*_{\\alpha}) + J(\\pi^*_{\\alpha}) - J(\\hat{\\pi}) \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\frac{\\epsilon_{\\text{MLE stat}}}{\\tau^2} \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) \\frac{\\epsilon_{\\text{stat}}}{\\alpha^2} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}}\n\\end{align*}\n$$\nSince $\\epsilon_{\\text{stat}} \\approx V^2 \\log(|V|/\\delta) / \\tau$, we can further obtain that $J(\\pi^*) - J(\\hat{\\pi}) \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} \\log(|V|/\\delta)^{1/4} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}} \\tau \\alpha N \\log(|\\Pi|/\\delta)$.\n\n$$\n\\begin{align*}\nJ(\\pi^*) - J(\\hat{\\pi}) & \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} \\log(|V|/\\delta)^{1/4} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}} \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} C^*_{\\alpha} / \\tau \\alpha N^{1/4} \\log(|V||\\Pi|/\\delta) \\\\\n& \\lesssim V^3_{\\text{max}}(C^*_{\\alpha})^3 \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4} \\\\\n& \\lesssim V^3_{\\text{max}} \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n\\end{align*}\n$$\nwhere $\\alpha \\approx \\tau^2(C^*_{\\alpha})^3 N^{1/4}$.\n\nD.2 Stochastic Settings\n\nTheorem 3 (Statistical rate of the suboptimality in stochastic settings). Under Assumptions 1 to 5, with probability at least $1 - \\delta$, the output policy $\\hat{\\pi}$ by Algorithm 1 (with the choice of Algorithm 3 for $V$-learning in stochastic settings) satisfies\n\n$$\nJ(\\pi^*) - J(\\hat{\\pi}) \\lesssim V^3_{\\text{max}}(C^*_{\\alpha})^3 \\log(1/\\tau) \\log(|V||P||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n$$\n$$\nV^3_{\\text{max}} \\log(1/\\tau) \\log(|V||P||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n$$\nif we choose $\\alpha \\approx \\tau^2(C^*_{\\alpha})^3 N^{1/4}$ and assume $N = N_0$.\n\nProof. The proof is identical to the proof of Theorem 2 except that we replace $\\epsilon_{\\text{stat}}$ with $\\epsilon_{\\text{stochastic}}$.", "images": [], "items": [{"type": "text", "value": "D.1 Deterministic Settings\n\nProof of Theorem 2. By Theorem 1 and Proposition 3.1,\n\n$$\n\\begin{align*}\nJ(\\pi^*) - J(\\hat{\\pi}) & = J(\\pi^*) - J(\\pi^*_{\\alpha}) + J(\\pi^*_{\\alpha}) - J(\\hat{\\pi}) \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\frac{\\epsilon_{\\text{MLE stat}}}{\\tau^2} \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) \\frac{\\epsilon_{\\text{stat}}}{\\alpha^2} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}}\n\\end{align*}\n$$\nSince $\\epsilon_{\\text{stat}} \\approx V^2 \\log(|V|/\\delta) / \\tau$, we can further obtain that $J(\\pi^*) - J(\\hat{\\pi}) \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} \\log(|V|/\\delta)^{1/4} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}} \\tau \\alpha N \\log(|\\Pi|/\\delta)$.\n\n$$\n\\begin{align*}\nJ(\\pi^*) - J(\\hat{\\pi}) & \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} \\log(|V|/\\delta)^{1/4} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}} \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} C^*_{\\alpha} / \\tau \\alpha N^{1/4} \\log(|V||\\Pi|/\\delta) \\\\\n& \\lesssim V^3_{\\text{max}}(C^*_{\\alpha})^3 \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4} \\\\\n& \\lesssim V^3_{\\text{max}} \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n\\end{align*}\n$$\nwhere $\\alpha \\approx \\tau^2(C^*_{\\alpha})^3 N^{1/4}$.\n\nD.2 Stochastic Settings\n\nTheorem 3 (Statistical rate of the suboptimality in stochastic settings). Under Assumptions 1 to 5, with probability at least $1 - \\delta$, the output policy $\\hat{\\pi}$ by Algorithm 1 (with the choice of Algorithm 3 for $V$-learning in stochastic settings) satisfies\n\n$$\nJ(\\pi^*) - J(\\hat{\\pi}) \\lesssim V^3_{\\text{max}}(C^*_{\\alpha})^3 \\log(1/\\tau) \\log(|V||P||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n$$\n$$\nV^3_{\\text{max}} \\log(1/\\tau) \\log(|V||P||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n$$\nif we choose $\\alpha \\approx \\tau^2(C^*_{\\alpha})^3 N^{1/4}$ and assume $N = N_0$.\n\nProof. The proof is identical to the proof of Theorem 2 except that we replace $\\epsilon_{\\text{stat}}$ with $\\epsilon_{\\text{stochastic}}$.", "md": "D.1 Deterministic Settings\n\nProof of Theorem 2. By Theorem 1 and Proposition 3.1,\n\n$$\n\\begin{align*}\nJ(\\pi^*) - J(\\hat{\\pi}) & = J(\\pi^*) - J(\\pi^*_{\\alpha}) + J(\\pi^*_{\\alpha}) - J(\\hat{\\pi}) \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\frac{\\epsilon_{\\text{MLE stat}}}{\\tau^2} \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) \\frac{\\epsilon_{\\text{stat}}}{\\alpha^2} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}}\n\\end{align*}\n$$\nSince $\\epsilon_{\\text{stat}} \\approx V^2 \\log(|V|/\\delta) / \\tau$, we can further obtain that $J(\\pi^*) - J(\\hat{\\pi}) \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} \\log(|V|/\\delta)^{1/4} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}} \\tau \\alpha N \\log(|\\Pi|/\\delta)$.\n\n$$\n\\begin{align*}\nJ(\\pi^*) - J(\\hat{\\pi}) & \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} \\log(|V|/\\delta)^{1/4} + C^*_{\\alpha} \\log(|\\Pi|/\\delta) + V_{\\text{max}} \\\\\n& \\lesssim \\alpha(C^*_{\\alpha})^2 + V_{\\text{max}} \\log(1/\\tau) V_{\\text{max}} C^*_{\\alpha} / \\tau \\alpha N^{1/4} \\log(|V||\\Pi|/\\delta) \\\\\n& \\lesssim V^3_{\\text{max}}(C^*_{\\alpha})^3 \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4} \\\\\n& \\lesssim V^3_{\\text{max}} \\log(1/\\tau) \\log(|V||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n\\end{align*}\n$$\nwhere $\\alpha \\approx \\tau^2(C^*_{\\alpha})^3 N^{1/4}$.\n\nD.2 Stochastic Settings\n\nTheorem 3 (Statistical rate of the suboptimality in stochastic settings). Under Assumptions 1 to 5, with probability at least $1 - \\delta$, the output policy $\\hat{\\pi}$ by Algorithm 1 (with the choice of Algorithm 3 for $V$-learning in stochastic settings) satisfies\n\n$$\nJ(\\pi^*) - J(\\hat{\\pi}) \\lesssim V^3_{\\text{max}}(C^*_{\\alpha})^3 \\log(1/\\tau) \\log(|V||P||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n$$\n$$\nV^3_{\\text{max}} \\log(1/\\tau) \\log(|V||P||\\Pi|/\\delta)^{1/3} / \\tau^2 N^{1/4}\n$$\nif we choose $\\alpha \\approx \\tau^2(C^*_{\\alpha})^3 N^{1/4}$ and assume $N = N_0$.\n\nProof. The proof is identical to the proof of Theorem 2 except that we replace $\\epsilon_{\\text{stat}}$ with $\\epsilon_{\\text{stochastic}}$."}]}], "job_id": "523e7061-f456-4bd0-8d24-21d3100c3941", "file_path": "./corpus/2302.03770.pdf"}