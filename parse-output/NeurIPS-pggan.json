{"pages": [{"page": 1, "text": "   Learning a 1-layer conditional generative model in\n                                       total variation\n               Ajil Jalal            Justin Kang                            Ananya Uppal\n                          UC Berkeley                                         UT Austin\n      {ajiljalal, justin_kang}@berkeley.edu                        ananya.uppal09@gmail.com\n                  Kannan Ramchandran                                     Eric Price\n                        UC Berkeley                                      UT Austin\n             kannanr@eecs.berkeley.edu                          ecprice@cs.utexas.edu\n                                                Abstract\n          A conditional generative model is a method for sampling from a conditional\n          distribution p(y | x). For example, one may want to sample an image of a cat given\n          the label \u201ccat\u201d. A feed-forward conditional generative model is a function g(x, z)\n          that takes the input x and a random seed z, and outputs a sample y from p(y | x).\n          Ideally the distribution of outputs (x, g(x, z)) would be close in total variation to\n          the ideal distribution (x, y).\n          Generalization bounds for other learning models require assumptions on the distri-\n          bution of x, even in simple settings like linear regression with Gaussian noise. We\n          show these assumptions are unnecessary in our model, for both linear regression\n          and single-layer ReLU networks. Given samples (x, y), we show how to learn a\n         1-layer ReLU conditional generative model in total variation. As our result has no\n          assumption on the distribution of inputs x, if we are given access to the internal\n          activations of a deep generative model, we can compose our 1-layer guarantee to\n          progressively learn the deep model using a near-linear number of samples.\n1    Introduction\nGenerative models are in the midst of an explosion in accessibility, as models like DALL-E [31] or\nStable Diffusion [32] capture the attention of millions. In many cases, these generative models can be\nsuccinctly represented by a fundamental mathematical object\u2014the conditional distribution p(y | x).\nIn the example of text-to-image generative models, x can represent a text prompt or its Word2Vec\nembedding [26], and the model can be seen as sampling an image y from its conditional distribution\np(y | x). With large numbers of people accessing these models, a massive amount of sample pairs\n(yi, xi), are becoming available online. A natural question to ask is: How many samples (yi, xi) does\nit take to learn the conditional generative model p(y | x)?\nRecent empirical studies such as Stanford Alpaca [34], which attempt to learn GPT-3.5 from limited\nsamples, indicate that the number of samples needed may be within a practical range. In this paper,\nwe attempt to address this problem from a fundamental perspective grounded in a concept from\nclassical theoretical statistics: the Maximum Likelihood Estimator (MLE). Specifically, we focus on\nfeed-forward generative models from a relatively simple family and ask: with no assumptions on x,\nhow many samples are required to efficiently learn the conditional generative model p(y | x)?\nLinear Regression.      Consider ordinary linear regression with Gaussian noise: you observe indepen-\ndent samples (xi, yi) \u2208   Rk \u00d7 R of the form\n                                  y = x \u00b7 w\u2217  + \u03b7       for \u03b7 \u223c N(0, 1).\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Conditional Generative Model in Total Variation\n\n## Learning a 1-layer conditional generative model in total variation\n\n|Ajil Jalal|Justin Kang|Ananya Uppal|\n|---|---|---|\n|UC Berkeley| |UT Austin|\n|{ajiljalal, justin_kang}@berkeley.edu| |ananya.uppal09@gmail.com|\n|Kannan Ramchandran| |Eric Price|\n|UC Berkeley| |UT Austin|\n|kannanr@eecs.berkeley.edu| |ecprice@cs.utexas.edu|\n\n### Abstract\n\nA conditional generative model is a method for sampling from a conditional distribution $$p(y | x)$$. For example, one may want to sample an image of a cat given the label \u201ccat\u201d. A feed-forward conditional generative model is a function $$g(x, z)$$ that takes the input $$x$$ and a random seed $$z$$, and outputs a sample $$y$$ from $$p(y | x)$$. Ideally, the distribution of outputs $$(x, g(x, z))$$ would be close in total variation to the ideal distribution $$(x, y)$$. Generalization bounds for other learning models require assumptions on the distribution of $$x$$, even in simple settings like linear regression with Gaussian noise. We show these assumptions are unnecessary in our model, for both linear regression and single-layer ReLU networks. Given samples $$(x, y)$$, we show how to learn a 1-layer ReLU conditional generative model in total variation. As our result has no assumption on the distribution of inputs $$x$$, if we are given access to the internal activations of a deep generative model, we can compose our 1-layer guarantee to progressively learn the deep model using a near-linear number of samples.\n\n### Introduction\n\nGenerative models are in the midst of an explosion in accessibility, as models like DALL-E [31] or Stable Diffusion [32] capture the attention of millions. In many cases, these generative models can be succinctly represented by a fundamental mathematical object\u2014the conditional distribution $$p(y | x)$$. In the example of text-to-image generative models, $$x$$ can represent a text prompt or its Word2Vec embedding [26], and the model can be seen as sampling an image $$y$$ from its conditional distribution $$p(y | x)$$. With large numbers of people accessing these models, a massive amount of sample pairs $$(y_i, x_i)$$ are becoming available online. A natural question to ask is: How many samples $$(y_i, x_i)$$ does it take to learn the conditional generative model $$p(y | x)$$?\n\nRecent empirical studies such as Stanford Alpaca [34], which attempt to learn GPT-3.5 from limited samples, indicate that the number of samples needed may be within a practical range. In this paper, we attempt to address this problem from a fundamental perspective grounded in a concept from classical theoretical statistics: the Maximum Likelihood Estimator (MLE). Specifically, we focus on feed-forward generative models from a relatively simple family and ask: with no assumptions on $$x$$, how many samples are required to efficiently learn the conditional generative model $$p(y | x)$$?\n\n### Linear Regression\n\nConsider ordinary linear regression with Gaussian noise: you observe independent samples $$(x_i, y_i) \\in \\mathbb{R}^k \\times \\mathbb{R}$$ of the form\n\n$$\ny = x \\cdot w^* + \\eta \\text{ for } \\eta \\sim \\mathcal{N}(0, 1).\n$$\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Conditional Generative Model in Total Variation", "md": "# Conditional Generative Model in Total Variation"}, {"type": "heading", "lvl": 2, "value": "Learning a 1-layer conditional generative model in total variation", "md": "## Learning a 1-layer conditional generative model in total variation"}, {"type": "table", "rows": [["Ajil Jalal", "Justin Kang", "Ananya Uppal"], ["UC Berkeley", "", "UT Austin"], ["{ajiljalal, justin_kang}@berkeley.edu", "", "ananya.uppal09@gmail.com"], ["Kannan Ramchandran", "", "Eric Price"], ["UC Berkeley", "", "UT Austin"], ["kannanr@eecs.berkeley.edu", "", "ecprice@cs.utexas.edu"]], "md": "|Ajil Jalal|Justin Kang|Ananya Uppal|\n|---|---|---|\n|UC Berkeley| |UT Austin|\n|{ajiljalal, justin_kang}@berkeley.edu| |ananya.uppal09@gmail.com|\n|Kannan Ramchandran| |Eric Price|\n|UC Berkeley| |UT Austin|\n|kannanr@eecs.berkeley.edu| |ecprice@cs.utexas.edu|", "isPerfectTable": true, "csv": "\"Ajil Jalal\",\"Justin Kang\",\"Ananya Uppal\"\n\"UC Berkeley\",\"\",\"UT Austin\"\n\"{ajiljalal, justin_kang}@berkeley.edu\",\"\",\"ananya.uppal09@gmail.com\"\n\"Kannan Ramchandran\",\"\",\"Eric Price\"\n\"UC Berkeley\",\"\",\"UT Austin\"\n\"kannanr@eecs.berkeley.edu\",\"\",\"ecprice@cs.utexas.edu\""}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "A conditional generative model is a method for sampling from a conditional distribution $$p(y | x)$$. For example, one may want to sample an image of a cat given the label \u201ccat\u201d. A feed-forward conditional generative model is a function $$g(x, z)$$ that takes the input $$x$$ and a random seed $$z$$, and outputs a sample $$y$$ from $$p(y | x)$$. Ideally, the distribution of outputs $$(x, g(x, z))$$ would be close in total variation to the ideal distribution $$(x, y)$$. Generalization bounds for other learning models require assumptions on the distribution of $$x$$, even in simple settings like linear regression with Gaussian noise. We show these assumptions are unnecessary in our model, for both linear regression and single-layer ReLU networks. Given samples $$(x, y)$$, we show how to learn a 1-layer ReLU conditional generative model in total variation. As our result has no assumption on the distribution of inputs $$x$$, if we are given access to the internal activations of a deep generative model, we can compose our 1-layer guarantee to progressively learn the deep model using a near-linear number of samples.", "md": "A conditional generative model is a method for sampling from a conditional distribution $$p(y | x)$$. For example, one may want to sample an image of a cat given the label \u201ccat\u201d. A feed-forward conditional generative model is a function $$g(x, z)$$ that takes the input $$x$$ and a random seed $$z$$, and outputs a sample $$y$$ from $$p(y | x)$$. Ideally, the distribution of outputs $$(x, g(x, z))$$ would be close in total variation to the ideal distribution $$(x, y)$$. Generalization bounds for other learning models require assumptions on the distribution of $$x$$, even in simple settings like linear regression with Gaussian noise. We show these assumptions are unnecessary in our model, for both linear regression and single-layer ReLU networks. Given samples $$(x, y)$$, we show how to learn a 1-layer ReLU conditional generative model in total variation. As our result has no assumption on the distribution of inputs $$x$$, if we are given access to the internal activations of a deep generative model, we can compose our 1-layer guarantee to progressively learn the deep model using a near-linear number of samples."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "Generative models are in the midst of an explosion in accessibility, as models like DALL-E [31] or Stable Diffusion [32] capture the attention of millions. In many cases, these generative models can be succinctly represented by a fundamental mathematical object\u2014the conditional distribution $$p(y | x)$$. In the example of text-to-image generative models, $$x$$ can represent a text prompt or its Word2Vec embedding [26], and the model can be seen as sampling an image $$y$$ from its conditional distribution $$p(y | x)$$. With large numbers of people accessing these models, a massive amount of sample pairs $$(y_i, x_i)$$ are becoming available online. A natural question to ask is: How many samples $$(y_i, x_i)$$ does it take to learn the conditional generative model $$p(y | x)$$?\n\nRecent empirical studies such as Stanford Alpaca [34], which attempt to learn GPT-3.5 from limited samples, indicate that the number of samples needed may be within a practical range. In this paper, we attempt to address this problem from a fundamental perspective grounded in a concept from classical theoretical statistics: the Maximum Likelihood Estimator (MLE). Specifically, we focus on feed-forward generative models from a relatively simple family and ask: with no assumptions on $$x$$, how many samples are required to efficiently learn the conditional generative model $$p(y | x)$$?", "md": "Generative models are in the midst of an explosion in accessibility, as models like DALL-E [31] or Stable Diffusion [32] capture the attention of millions. In many cases, these generative models can be succinctly represented by a fundamental mathematical object\u2014the conditional distribution $$p(y | x)$$. In the example of text-to-image generative models, $$x$$ can represent a text prompt or its Word2Vec embedding [26], and the model can be seen as sampling an image $$y$$ from its conditional distribution $$p(y | x)$$. With large numbers of people accessing these models, a massive amount of sample pairs $$(y_i, x_i)$$ are becoming available online. A natural question to ask is: How many samples $$(y_i, x_i)$$ does it take to learn the conditional generative model $$p(y | x)$$?\n\nRecent empirical studies such as Stanford Alpaca [34], which attempt to learn GPT-3.5 from limited samples, indicate that the number of samples needed may be within a practical range. In this paper, we attempt to address this problem from a fundamental perspective grounded in a concept from classical theoretical statistics: the Maximum Likelihood Estimator (MLE). Specifically, we focus on feed-forward generative models from a relatively simple family and ask: with no assumptions on $$x$$, how many samples are required to efficiently learn the conditional generative model $$p(y | x)$$?"}, {"type": "heading", "lvl": 3, "value": "Linear Regression", "md": "### Linear Regression"}, {"type": "text", "value": "Consider ordinary linear regression with Gaussian noise: you observe independent samples $$(x_i, y_i) \\in \\mathbb{R}^k \\times \\mathbb{R}$$ of the form\n\n$$\ny = x \\cdot w^* + \\eta \\text{ for } \\eta \\sim \\mathcal{N}(0, 1).\n$$\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Consider ordinary linear regression with Gaussian noise: you observe independent samples $$(x_i, y_i) \\in \\mathbb{R}^k \\times \\mathbb{R}$$ of the form\n\n$$\ny = x \\cdot w^* + \\eta \\text{ for } \\eta \\sim \\mathcal{N}(0, 1).\n$$\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "  \"Beautiful\n   sunset in\nNew Orleans\"\nFigure 1: A conditional distribution defined by a conditional generative model. To sample from the\nconditional distribution p(y | x), we perform inference. Due to the stochastic nature of the model,\neach output is different.\nHow many samples does it take to get a \u201cgood\" solution? For standard metrics\u2014such as parameter\ndistance in w\u2217   or prediction error on y\u2014the sample complexity of linear regression depends on\nproperties of the distribution (such as the conditioning of x, or the variance of y) that could be\nunbounded and cannot be tested. For example, in the classic analysis (see [7], Chapter 3), sample\ncomplexity depends on the design matrix, which in turn results in a dependence on the expectation of\n\u2225x\u22252. The typical approach to deal with this would require bounded moments for x. For learning\nthe conditional distribution, the more natural metric is total variation (TV) distance: the parameters\nw induce a distribution pw(x, y) = p(x)pw(y | x) where p(x) is the true distribution of x, and we\nwould like to find  w such that       dT V (pw\u2217(x, y), p w(x, y)) \u2264    \u03b5.                                   (1)\nThis ensures that when the input x come from the user in the true unknown distribution p(x), the\nmodel generates a conditional sample that is close in TV to the true model. It turns out that this goal,\nunlike parameter distance, can be solved with no assumption on the distribution.\nTheorem 1.1 (Informal version of Theorem 4.1). The MLE (i.e., least squares regression) achieves (1)\nwith O( 1\u03b52 k log 1\n                  \u03b5) samples, regardless of the distribution of x.\nTo our knowledge, all previous guarantees for linear regression either require some assumptions\non, or give guarantees in terms of, the distribution of x or y. We avoid this dependence on the x\ndistribution by adopting a similar analysis to Theorem 11.2 in [20].\nMain Result: 1-Layer Networks.             Modern feed-forward conditional generative models (e.g.,\nStyleGAN2) are more complicated than linear regression. They are stochastic neural networks with\nlayers of the form:\n                                 y = \u03d5(W \u2217x + \u03b7)          for \u03b7 \u223c  N(0, \u03a3\u2217)                                 (2)\nfor x \u2208  Rk, y \u2208    Rd, \u03d5(x) = max(x, 0) is the ReLU activation, and some weights W \u2217                \u2208  Rd\u00d7k,\n\u03a3\u2217  \u2208 Rd\u00d7d.\nWe show that 1-layer generative models of the form (2) can also be estimated using the MLE (which\nis concave), with small TV error and without any assumption on x.\nTheorem 1.2 (Informal version of Theorem 4.5). Suppose y is drawn according to (2) where \u03a3\u2217                has\ncondition number at most \u03ba. Using O          kd+d2  log kd\u03ba    samples, the distribution generated by the\n                                               \u03b52         \u03b5\nMLE has TV error \u03b5, regardless of the distribution of x.\nLike Theorem 1.1, Theorem 1.2 shows that a conditional generative model can be learned in TV\nregardless of the distribution of the input x. It generalizes Theorem 1.1 in three ways: (a) the output\ny is d-dimensional rather than 1-dimensional; (b) the covariance of the noise \u03a3\u2217            is learned rather\nthan specified; and (c) the ReLU nonlinearity \u03d5 imparts additional complex structure.\nIndeed, the point (c) means that parameter distances are poor metrics for this problem. For example,\nwhen an element of W \u2217x has a large negative bias, the corresponding coordinate in y will almost\n                                                       2", "md": "# Document\n\nBeautiful sunset in New Orleans\n\nFigure 1: A conditional distribution defined by a conditional generative model. To sample from the conditional distribution $$p(y | x)$$, we perform inference. Due to the stochastic nature of the model, each output is different.\n\nHow many samples does it take to get a \u201cgood\" solution? For standard metrics\u2014such as parameter distance in $$w^*$$ or prediction error on y\u2014the sample complexity of linear regression depends on properties of the distribution (such as the conditioning of x, or the variance of y) that could be unbounded and cannot be tested. For example, in the classic analysis (see [7], Chapter 3), sample complexity depends on the design matrix, which in turn results in a dependence on the expectation of $$\\|x\\|^2$$. The typical approach to deal with this would require bounded moments for x. For learning the conditional distribution, the more natural metric is total variation (TV) distance: the parameters w induce a distribution $$p_w(x, y) = p(x)p_w(y | x)$$ where $$p(x)$$ is the true distribution of x, and we would like to find $$w$$ such that $$d_{TV}(p_{w^*}(x, y), p_w(x, y)) \\leq \\epsilon$$. This ensures that when the input x come from the user in the true unknown distribution $$p(x)$$, the model generates a conditional sample that is close in TV to the true model. It turns out that this goal, unlike parameter distance, can be solved with no assumption on the distribution.\n\nTheorem 1.1 (Informal version of Theorem 4.1): The MLE (i.e., least squares regression) achieves (1) with $$O\\left(\\frac{1}{\\epsilon^2} k \\log \\frac{1}{\\epsilon}\\right)$$ samples, regardless of the distribution of x.\n\nTo our knowledge, all previous guarantees for linear regression either require some assumptions on, or give guarantees in terms of, the distribution of x or y. We avoid this dependence on the x distribution by adopting a similar analysis to Theorem 11.2 in [20].\n\nMain Result: 1-Layer Networks. Modern feed-forward conditional generative models (e.g., StyleGAN2) are more complicated than linear regression. They are stochastic neural networks with layers of the form:\n\n$$y = \\phi(W^*x + \\eta)$$ for $$\\eta \\sim N(0, \\Sigma^*)$$ for $$x \\in \\mathbb{R}^k$$, $$y \\in \\mathbb{R}^d$$, $$\\phi(x) = \\max(x, 0)$$ is the ReLU activation, and some weights $$W^* \\in \\mathbb{R}^{d \\times k}$$, $$\\Sigma^* \\in \\mathbb{R}^{d \\times d}$$.\n\nWe show that 1-layer generative models of the form (2) can also be estimated using the MLE (which is concave), with small TV error and without any assumption on x.\n\nTheorem 1.2 (Informal version of Theorem 4.5): Suppose y is drawn according to (2) where $$\\Sigma^*$$ has condition number at most $$\\kappa$$. Using $$O\\left(\\frac{kd+d^2}{\\epsilon^2} \\log \\frac{kd\\kappa}{\\epsilon}\\right)$$ samples, the distribution generated by the MLE has TV error $$\\epsilon$$, regardless of the distribution of x.\n\nLike Theorem 1.1, Theorem 1.2 shows that a conditional generative model can be learned in TV regardless of the distribution of the input x. It generalizes Theorem 1.1 in three ways: (a) the output y is d-dimensional rather than 1-dimensional; (b) the covariance of the noise $$\\Sigma^*$$ is learned rather than specified; and (c) the ReLU nonlinearity $$\\phi$$ imparts additional complex structure.\n\nIndeed, the point (c) means that parameter distances are poor metrics for this problem. For example, when an element of $$W^*x$$ has a large negative bias, the corresponding coordinate in y will almost", "images": [{"name": "page-2-0.jpg", "height": 97, "width": 63, "x": 390, "y": 87}, {"name": "page-2-1.jpg", "height": 52, "width": 52, "x": 380, "y": 87}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Beautiful sunset in New Orleans\n\nFigure 1: A conditional distribution defined by a conditional generative model. To sample from the conditional distribution $$p(y | x)$$, we perform inference. Due to the stochastic nature of the model, each output is different.\n\nHow many samples does it take to get a \u201cgood\" solution? For standard metrics\u2014such as parameter distance in $$w^*$$ or prediction error on y\u2014the sample complexity of linear regression depends on properties of the distribution (such as the conditioning of x, or the variance of y) that could be unbounded and cannot be tested. For example, in the classic analysis (see [7], Chapter 3), sample complexity depends on the design matrix, which in turn results in a dependence on the expectation of $$\\|x\\|^2$$. The typical approach to deal with this would require bounded moments for x. For learning the conditional distribution, the more natural metric is total variation (TV) distance: the parameters w induce a distribution $$p_w(x, y) = p(x)p_w(y | x)$$ where $$p(x)$$ is the true distribution of x, and we would like to find $$w$$ such that $$d_{TV}(p_{w^*}(x, y), p_w(x, y)) \\leq \\epsilon$$. This ensures that when the input x come from the user in the true unknown distribution $$p(x)$$, the model generates a conditional sample that is close in TV to the true model. It turns out that this goal, unlike parameter distance, can be solved with no assumption on the distribution.\n\nTheorem 1.1 (Informal version of Theorem 4.1): The MLE (i.e., least squares regression) achieves (1) with $$O\\left(\\frac{1}{\\epsilon^2} k \\log \\frac{1}{\\epsilon}\\right)$$ samples, regardless of the distribution of x.\n\nTo our knowledge, all previous guarantees for linear regression either require some assumptions on, or give guarantees in terms of, the distribution of x or y. We avoid this dependence on the x distribution by adopting a similar analysis to Theorem 11.2 in [20].\n\nMain Result: 1-Layer Networks. Modern feed-forward conditional generative models (e.g., StyleGAN2) are more complicated than linear regression. They are stochastic neural networks with layers of the form:\n\n$$y = \\phi(W^*x + \\eta)$$ for $$\\eta \\sim N(0, \\Sigma^*)$$ for $$x \\in \\mathbb{R}^k$$, $$y \\in \\mathbb{R}^d$$, $$\\phi(x) = \\max(x, 0)$$ is the ReLU activation, and some weights $$W^* \\in \\mathbb{R}^{d \\times k}$$, $$\\Sigma^* \\in \\mathbb{R}^{d \\times d}$$.\n\nWe show that 1-layer generative models of the form (2) can also be estimated using the MLE (which is concave), with small TV error and without any assumption on x.\n\nTheorem 1.2 (Informal version of Theorem 4.5): Suppose y is drawn according to (2) where $$\\Sigma^*$$ has condition number at most $$\\kappa$$. Using $$O\\left(\\frac{kd+d^2}{\\epsilon^2} \\log \\frac{kd\\kappa}{\\epsilon}\\right)$$ samples, the distribution generated by the MLE has TV error $$\\epsilon$$, regardless of the distribution of x.\n\nLike Theorem 1.1, Theorem 1.2 shows that a conditional generative model can be learned in TV regardless of the distribution of the input x. It generalizes Theorem 1.1 in three ways: (a) the output y is d-dimensional rather than 1-dimensional; (b) the covariance of the noise $$\\Sigma^*$$ is learned rather than specified; and (c) the ReLU nonlinearity $$\\phi$$ imparts additional complex structure.\n\nIndeed, the point (c) means that parameter distances are poor metrics for this problem. For example, when an element of $$W^*x$$ has a large negative bias, the corresponding coordinate in y will almost", "md": "Beautiful sunset in New Orleans\n\nFigure 1: A conditional distribution defined by a conditional generative model. To sample from the conditional distribution $$p(y | x)$$, we perform inference. Due to the stochastic nature of the model, each output is different.\n\nHow many samples does it take to get a \u201cgood\" solution? For standard metrics\u2014such as parameter distance in $$w^*$$ or prediction error on y\u2014the sample complexity of linear regression depends on properties of the distribution (such as the conditioning of x, or the variance of y) that could be unbounded and cannot be tested. For example, in the classic analysis (see [7], Chapter 3), sample complexity depends on the design matrix, which in turn results in a dependence on the expectation of $$\\|x\\|^2$$. The typical approach to deal with this would require bounded moments for x. For learning the conditional distribution, the more natural metric is total variation (TV) distance: the parameters w induce a distribution $$p_w(x, y) = p(x)p_w(y | x)$$ where $$p(x)$$ is the true distribution of x, and we would like to find $$w$$ such that $$d_{TV}(p_{w^*}(x, y), p_w(x, y)) \\leq \\epsilon$$. This ensures that when the input x come from the user in the true unknown distribution $$p(x)$$, the model generates a conditional sample that is close in TV to the true model. It turns out that this goal, unlike parameter distance, can be solved with no assumption on the distribution.\n\nTheorem 1.1 (Informal version of Theorem 4.1): The MLE (i.e., least squares regression) achieves (1) with $$O\\left(\\frac{1}{\\epsilon^2} k \\log \\frac{1}{\\epsilon}\\right)$$ samples, regardless of the distribution of x.\n\nTo our knowledge, all previous guarantees for linear regression either require some assumptions on, or give guarantees in terms of, the distribution of x or y. We avoid this dependence on the x distribution by adopting a similar analysis to Theorem 11.2 in [20].\n\nMain Result: 1-Layer Networks. Modern feed-forward conditional generative models (e.g., StyleGAN2) are more complicated than linear regression. They are stochastic neural networks with layers of the form:\n\n$$y = \\phi(W^*x + \\eta)$$ for $$\\eta \\sim N(0, \\Sigma^*)$$ for $$x \\in \\mathbb{R}^k$$, $$y \\in \\mathbb{R}^d$$, $$\\phi(x) = \\max(x, 0)$$ is the ReLU activation, and some weights $$W^* \\in \\mathbb{R}^{d \\times k}$$, $$\\Sigma^* \\in \\mathbb{R}^{d \\times d}$$.\n\nWe show that 1-layer generative models of the form (2) can also be estimated using the MLE (which is concave), with small TV error and without any assumption on x.\n\nTheorem 1.2 (Informal version of Theorem 4.5): Suppose y is drawn according to (2) where $$\\Sigma^*$$ has condition number at most $$\\kappa$$. Using $$O\\left(\\frac{kd+d^2}{\\epsilon^2} \\log \\frac{kd\\kappa}{\\epsilon}\\right)$$ samples, the distribution generated by the MLE has TV error $$\\epsilon$$, regardless of the distribution of x.\n\nLike Theorem 1.1, Theorem 1.2 shows that a conditional generative model can be learned in TV regardless of the distribution of the input x. It generalizes Theorem 1.1 in three ways: (a) the output y is d-dimensional rather than 1-dimensional; (b) the covariance of the noise $$\\Sigma^*$$ is learned rather than specified; and (c) the ReLU nonlinearity $$\\phi$$ imparts additional complex structure.\n\nIndeed, the point (c) means that parameter distances are poor metrics for this problem. For example, when an element of $$W^*x$$ has a large negative bias, the corresponding coordinate in y will almost"}]}, {"page": 3, "text": " always be 0. This means sample pairs (xi, yi) provide little information about W \u2217, but still provide\n useful information about the conditional distribution. As we will later see, exploiting this valuable\n information in the truncated samples allows us to significantly outperform prior works such as [35],\nwhich do not exploit these 0-valued samples.\n Multilayer Networks Given Activations.                    Given our distribution-free results on 1-layer networks, it\n is possible to extend our results to deep multi-layer networks. Given access to the internal activations\n of a neural network (but not the weights), our results can be applied layer-wise. Intermediate layers\n may have poorly conditioned input distributions, but since our result does not depend on the input\n distribution, we achieve strong guarantees for layer-wise learning in Theorem 4.6.\n 1.1    Proof Approach\n In this outline we focus on the case of 1-dimensional y, and and standard Gaussian noise \u03b7 \u223c                             N(0, 1).\n Our proof approach is inspired by learning bounds that exploit finite VC dimension. We would like\n to show (1), or equivalently,\n                                  d(w\u2217,   w) := E   x[dT V (pw\u2217(y | x), p      w(y | x))] \u2264      \u03b5,                              (3)\nwhen we see n samples xi of x, and one sample yi for each xi. We do this in two stages. First, we\n show that the empirical distance between w and                  w is small i.e.,                                                (4)\nwhere    Ex[f(x)] = 1         n d(w\u2217,   w) :=    Ex[dT V (pw\u2217(y | x), p      w(y | x))] \u2264      0.5\u03b5,\n                          n      i=1 f(xi) denotes the empirical expectation over x.\n Second, we show that the empirical distance is a good proxy for the true distance, i.e.,\nwhich gives (3).                            d(w\u2217,   w) \u2264     d(w\u2217,   w) + 0.5\u03b5 \u2264       \u03b5,                                        (5)\n Linear Case.         In the linear case, both stages are straightforward. The linear regression solution has\n an explicit form, and it is well known and easy to show that\n                                                  E(xT (w\u2217     \u2212  w))2 \u221d     k/n.\n Since dT V (pw\u2217(y | x), p       w(y | x)) = \u0398(min(1, |x \u00b7 (w\u2217             \u2212   w)|)), Jensen\u2019s inequality implies (4) for\n n > k/\u03b52.\n Secondly, fw(x) := dT V (pw\u2217(y | x), pw(y | x)) is bounded and unimodal in w. Thus, it suffices to\n bound the deviation of the empirical average from the true fw(x) with Chernoff\u2019s inequality.\n ReLU Case.         In the ReLU case, we have y = \u03d5(w\u2217                \u00b7x+\u03b7), and both stages of the previous analysis\n are more difficult.\nThe most interesting part of our proof is showing the first stage for the ReLU case, which states that\n the w maximizing                             L(w) := 1    n    n  log p  w(yi | xi)\n                                                              i=1\n satisfies (4). Now, for any w not satisfying (4),\n                      Ey[L(w) \u2212      L(w\u2217)] = \u2212\u2264   \u22122 ExEdKL(pw\u2217(y | x)\u2225pw(y | x))\n                                                        x[dT V (pw\u2217(y | x), pw(y | x))2] \u2264             \u22122\u03b52,\nwhere the first inequality follows from Pinsker\u2019s inequality. Unfortunately, L(w) \u2212                             L(w\u2217) does not\n concentrate well, by virtue of the KL-divergence being unbounded. However, we can upper bound                           1\n it via the Bernstein inequality, such that for a fixed w not satisfying (4), and given n =                              \u03b52 log( 1\u03b4 )\n samples, we have                                  L(w) \u2212     L(w\u2217) \u2264       \u2212\u03b52,\n                                                                  3", "md": "always be 0. This means sample pairs (xi, yi) provide little information about W \u2217, but still provide\nuseful information about the conditional distribution. As we will later see, exploiting this valuable\ninformation in the truncated samples allows us to significantly outperform prior works such as [35],\nwhich do not exploit these 0-valued samples.\n\nMultilayer Networks Given Activations. Given our distribution-free results on 1-layer networks, it\nis possible to extend our results to deep multi-layer networks. Given access to the internal activations\nof a neural network (but not the weights), our results can be applied layer-wise. Intermediate layers\nmay have poorly conditioned input distributions, but since our result does not depend on the input\ndistribution, we achieve strong guarantees for layer-wise learning in Theorem 4.6.\n\n### 1.1 Proof Approach\n\nIn this outline we focus on the case of 1-dimensional y, and and standard Gaussian noise \u03b7 \u223c $$N(0, 1)$$. Our proof approach is inspired by learning bounds that exploit finite VC dimension. We would like\nto show (1), or equivalently,\n\n$$\nd(w^*, w) := E_x[d_T V(pw^*(y | x), pw(y | x))] \\leq \\epsilon \\quad (3)\n$$\n\nwhen we see n samples xi of x, and one sample yi for each xi. We do this in two stages. First, we\nshow that the empirical distance between w and w is small i.e.,\n\n$$\n\\text{where } E_x[f(x)] = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\text{ denotes the empirical expectation over x.}\n$$\n\nSecond, we show that the empirical distance is a good proxy for the true distance, i.e.,\n\n$$\nd(w^*, w) \\leq d(w^*, w) + 0.5\\epsilon \\leq \\epsilon \\quad (5)\n$$\n\nLinear Case. In the linear case, both stages are straightforward. The linear regression solution has\nan explicit form, and it is well known and easy to show that\n\n$$\nE(x^T(w^* - w))^2 \\propto \\frac{k}{n}.\n$$\n\nSince $$d_T V(pw^*(y | x), pw(y | x)) = \\Theta(\\min(1, |x \\cdot (w^* - w)|))$$, Jensen\u2019s inequality implies (4) for\nn > k/\u03b5^2.\n\nSecondly, $$f_w(x) := d_T V(pw^*(y | x), pw(y | x))$$ is bounded and unimodal in w. Thus, it suffices to\nbound the deviation of the empirical average from the true $$f_w(x)$$ with Chernoff\u2019s inequality.\n\nReLU Case. In the ReLU case, we have y = \u03d5(w^* \\cdot x + \u03b7), and both stages of the previous analysis\nare more difficult.\n\nThe most interesting part of our proof is showing the first stage for the ReLU case, which states that\nthe w maximizing\n\n$$\nL(w) := \\frac{1}{n} \\sum_{i=1}^{n} \\log p_w(y_i | x_i)\n$$\n\nsatisfies (4). Now, for any w not satisfying (4),\n\n$$\nE_y[L(w) - L(w^*)] \\leq -2 E_x \\text{EdKL}(pw^*(y | x) \\parallel pw(y | x)) \\leq -2\\epsilon^2,\n$$\n\nwhere the first inequality follows from Pinsker\u2019s inequality. Unfortunately, $$L(w) - L(w^*)$$ does not\nconcentrate well, by virtue of the KL-divergence being unbounded. However, we can upper bound it via the Bernstein inequality, such that for a fixed w not satisfying (4), and given $$n = \\epsilon^2 \\log(\\frac{1}{\\delta})$$\nsamples, we have\n\n$$\nL(w) - L(w^*) \\leq -\\epsilon^2.\n$$", "images": [], "items": [{"type": "text", "value": "always be 0. This means sample pairs (xi, yi) provide little information about W \u2217, but still provide\nuseful information about the conditional distribution. As we will later see, exploiting this valuable\ninformation in the truncated samples allows us to significantly outperform prior works such as [35],\nwhich do not exploit these 0-valued samples.\n\nMultilayer Networks Given Activations. Given our distribution-free results on 1-layer networks, it\nis possible to extend our results to deep multi-layer networks. Given access to the internal activations\nof a neural network (but not the weights), our results can be applied layer-wise. Intermediate layers\nmay have poorly conditioned input distributions, but since our result does not depend on the input\ndistribution, we achieve strong guarantees for layer-wise learning in Theorem 4.6.", "md": "always be 0. This means sample pairs (xi, yi) provide little information about W \u2217, but still provide\nuseful information about the conditional distribution. As we will later see, exploiting this valuable\ninformation in the truncated samples allows us to significantly outperform prior works such as [35],\nwhich do not exploit these 0-valued samples.\n\nMultilayer Networks Given Activations. Given our distribution-free results on 1-layer networks, it\nis possible to extend our results to deep multi-layer networks. Given access to the internal activations\nof a neural network (but not the weights), our results can be applied layer-wise. Intermediate layers\nmay have poorly conditioned input distributions, but since our result does not depend on the input\ndistribution, we achieve strong guarantees for layer-wise learning in Theorem 4.6."}, {"type": "heading", "lvl": 3, "value": "1.1 Proof Approach", "md": "### 1.1 Proof Approach"}, {"type": "text", "value": "In this outline we focus on the case of 1-dimensional y, and and standard Gaussian noise \u03b7 \u223c $$N(0, 1)$$. Our proof approach is inspired by learning bounds that exploit finite VC dimension. We would like\nto show (1), or equivalently,\n\n$$\nd(w^*, w) := E_x[d_T V(pw^*(y | x), pw(y | x))] \\leq \\epsilon \\quad (3)\n$$\n\nwhen we see n samples xi of x, and one sample yi for each xi. We do this in two stages. First, we\nshow that the empirical distance between w and w is small i.e.,\n\n$$\n\\text{where } E_x[f(x)] = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\text{ denotes the empirical expectation over x.}\n$$\n\nSecond, we show that the empirical distance is a good proxy for the true distance, i.e.,\n\n$$\nd(w^*, w) \\leq d(w^*, w) + 0.5\\epsilon \\leq \\epsilon \\quad (5)\n$$\n\nLinear Case. In the linear case, both stages are straightforward. The linear regression solution has\nan explicit form, and it is well known and easy to show that\n\n$$\nE(x^T(w^* - w))^2 \\propto \\frac{k}{n}.\n$$\n\nSince $$d_T V(pw^*(y | x), pw(y | x)) = \\Theta(\\min(1, |x \\cdot (w^* - w)|))$$, Jensen\u2019s inequality implies (4) for\nn > k/\u03b5^2.\n\nSecondly, $$f_w(x) := d_T V(pw^*(y | x), pw(y | x))$$ is bounded and unimodal in w. Thus, it suffices to\nbound the deviation of the empirical average from the true $$f_w(x)$$ with Chernoff\u2019s inequality.\n\nReLU Case. In the ReLU case, we have y = \u03d5(w^* \\cdot x + \u03b7), and both stages of the previous analysis\nare more difficult.\n\nThe most interesting part of our proof is showing the first stage for the ReLU case, which states that\nthe w maximizing\n\n$$\nL(w) := \\frac{1}{n} \\sum_{i=1}^{n} \\log p_w(y_i | x_i)\n$$\n\nsatisfies (4). Now, for any w not satisfying (4),\n\n$$\nE_y[L(w) - L(w^*)] \\leq -2 E_x \\text{EdKL}(pw^*(y | x) \\parallel pw(y | x)) \\leq -2\\epsilon^2,\n$$\n\nwhere the first inequality follows from Pinsker\u2019s inequality. Unfortunately, $$L(w) - L(w^*)$$ does not\nconcentrate well, by virtue of the KL-divergence being unbounded. However, we can upper bound it via the Bernstein inequality, such that for a fixed w not satisfying (4), and given $$n = \\epsilon^2 \\log(\\frac{1}{\\delta})$$\nsamples, we have\n\n$$\nL(w) - L(w^*) \\leq -\\epsilon^2.\n$$", "md": "In this outline we focus on the case of 1-dimensional y, and and standard Gaussian noise \u03b7 \u223c $$N(0, 1)$$. Our proof approach is inspired by learning bounds that exploit finite VC dimension. We would like\nto show (1), or equivalently,\n\n$$\nd(w^*, w) := E_x[d_T V(pw^*(y | x), pw(y | x))] \\leq \\epsilon \\quad (3)\n$$\n\nwhen we see n samples xi of x, and one sample yi for each xi. We do this in two stages. First, we\nshow that the empirical distance between w and w is small i.e.,\n\n$$\n\\text{where } E_x[f(x)] = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\text{ denotes the empirical expectation over x.}\n$$\n\nSecond, we show that the empirical distance is a good proxy for the true distance, i.e.,\n\n$$\nd(w^*, w) \\leq d(w^*, w) + 0.5\\epsilon \\leq \\epsilon \\quad (5)\n$$\n\nLinear Case. In the linear case, both stages are straightforward. The linear regression solution has\nan explicit form, and it is well known and easy to show that\n\n$$\nE(x^T(w^* - w))^2 \\propto \\frac{k}{n}.\n$$\n\nSince $$d_T V(pw^*(y | x), pw(y | x)) = \\Theta(\\min(1, |x \\cdot (w^* - w)|))$$, Jensen\u2019s inequality implies (4) for\nn > k/\u03b5^2.\n\nSecondly, $$f_w(x) := d_T V(pw^*(y | x), pw(y | x))$$ is bounded and unimodal in w. Thus, it suffices to\nbound the deviation of the empirical average from the true $$f_w(x)$$ with Chernoff\u2019s inequality.\n\nReLU Case. In the ReLU case, we have y = \u03d5(w^* \\cdot x + \u03b7), and both stages of the previous analysis\nare more difficult.\n\nThe most interesting part of our proof is showing the first stage for the ReLU case, which states that\nthe w maximizing\n\n$$\nL(w) := \\frac{1}{n} \\sum_{i=1}^{n} \\log p_w(y_i | x_i)\n$$\n\nsatisfies (4). Now, for any w not satisfying (4),\n\n$$\nE_y[L(w) - L(w^*)] \\leq -2 E_x \\text{EdKL}(pw^*(y | x) \\parallel pw(y | x)) \\leq -2\\epsilon^2,\n$$\n\nwhere the first inequality follows from Pinsker\u2019s inequality. Unfortunately, $$L(w) - L(w^*)$$ does not\nconcentrate well, by virtue of the KL-divergence being unbounded. However, we can upper bound it via the Bernstein inequality, such that for a fixed w not satisfying (4), and given $$n = \\epsilon^2 \\log(\\frac{1}{\\delta})$$\nsamples, we have\n\n$$\nL(w) - L(w^*) \\leq -\\epsilon^2.\n$$"}]}, {"page": 4, "text": "with probability 1 \u2212  \u03b4. Using a careful covering argument and n = (k/\u03b52) log(1/\u03b4) samples, we can\nuniformly extend this to all w not satisfying Eq (4). By definition, the MLE has L(     w) \u2265   L(w\u2217), and\nby our uniform bound, it must satisfy (4).\nThe second stage changes because fw(x) depends on x \u00b7 w and x \u00b7 w\u2217           in a more complicated way\nthan through x \u00b7 (w \u2212   w\u2217). This makes showing a bounded VC dimension more difficult; however,\nunpacking the proof that VC implies generalization, we can still show that the net (normally given by\nSauer\u2019s lemma) is bounded. This generalization holds as long as fw(x) is unimodal in x \u00b7 w.\n2    Contributions\n1. We show that MLE can perform distribution learning in the setting of linear regression and\n    multi-layer ReLU networks. Our bounds do not make assumptions on the distribution of x or the\n    condition number of W \u2217, and achieve a sample complexity polynomial in the system parameters.\n2. We improve the sample complexity bound in [35], which estimates the parameters of a one-layer\n    ReLU network but suffers an exponential dependence on the W \u2217         term. In contrast, as we seek to\n    estimate the distribution of (x, y), rather than the parameter W \u2217, we are able to avoid this. See\n    Section 4.2 for more details.\n3. Our algorithm for learning multi-layer ReLU networks is considerably simpler than [1], who\n    learn discriminators that are engineered to perform moment-matching on the output of each layer\n    of the network. Furthermore, [1] impose a strong requirement on the sparsity and independence\n    of the activations at each layer, which essentially allows standard techniques in sparse coding to\n    recover these activations.\n3    Related Work\nGenerative adversarial networks (GANs) [19, 2, 30] are a popular family of generative models\nthat train a generator and discriminator in an adversarial training framework. The seminal result\nby [22] proposed progressive growing of GANs (PGGANs) as a way to stabilize and accelerate the\ntraining phase of these models. Future results, such as StyleGAN [23] introduce more complicated\narchitectures and \u201cstyle\u201d variables. Additionally, these models add noise at each layer of the generator\nin order to introduce greater stochasticity in the generated images, which is important for textures\nsuch as hair and skin.\nDistributional Learning        Most theoretical results have focused on the min-max optimality of\nGANs [15, 28, 29], characterizing their stationary points [21, 17], or characterizing their generaliza-\ntion once they have reached a global minimum [3, 5]. The closest result to ours is [35]. Setting x\nto a deterministic scalar in our problem statement reduces it to [35], who consider y = \u03d5(b + \u03b7) s.t.\nb, \u03b7 \u2208 Rd, and they seek to learn the covariance of \u03b7 along with the bias vector b. However, their\nsample complexity bound suffers an exponential dependence on \u2225b\u22252        \u221e.\nSingle layer networks have attracted recent attention, as they provide a tractable formulation for\nstudying the dynamics and generalization of adversarial learning [24, 18, 25, 11]. The recent results\nof [1] show that multi-layered models that satisfy a property known as forward super-resolution (such\nas PGGANs) can be learned in polynomial time and sample complexity using stochastic gradient\ndescent-ascent. In this case, the discriminator is designed to detect differences between higher\norder moments of the generated and training distribution. Deep models have also been considered\nin [12, 10].\n4    Main Results\nIn this section we first show that the MLE of the parameters learns the input-output joint distribution\nfor linear regression. Then, we extend this guarantee to the case where the ReLU activation function\n\u03d5 is applied to the multi-dimensional output y. Finally, we show that we can compose the 1-layer\nReLU guarantee to learn the distribution generated by a multi-layer model.\n                                                     4", "md": "with probability 1 - \u03b4. Using a careful covering argument and n = (k/\u03b52) log(1/\u03b4) samples, we can\nuniformly extend this to all w not satisfying Eq (4). By definition, the MLE has L(w) \u2265 L(w*), and\nby our uniform bound, it must satisfy (4).\n\nThe second stage changes because fw(x) depends on x \u00b7 w and x \u00b7 w* in a more complicated way\nthan through x \u00b7 (w - w*). This makes showing a bounded VC dimension more difficult; however,\nunpacking the proof that VC implies generalization, we can still show that the net (normally given by\nSauer\u2019s lemma) is bounded. This generalization holds as long as fw(x) is unimodal in x \u00b7 w.\n\n## Contributions\n\n1. We show that MLE can perform distribution learning in the setting of linear regression and\nmulti-layer ReLU networks. Our bounds do not make assumptions on the distribution of x or the\ncondition number of W*, and achieve a sample complexity polynomial in the system parameters.\n2. We improve the sample complexity bound in [35], which estimates the parameters of a one-layer\nReLU network but suffers an exponential dependence on the W* term. In contrast, as we seek to\nestimate the distribution of (x, y), rather than the parameter W*, we are able to avoid this. See\nSection 4.2 for more details.\n3. Our algorithm for learning multi-layer ReLU networks is considerably simpler than [1], who\nlearn discriminators that are engineered to perform moment-matching on the output of each layer\nof the network. Furthermore, [1] impose a strong requirement on the sparsity and independence\nof the activations at each layer, which essentially allows standard techniques in sparse coding to\nrecover these activations.\n\n## Related Work\n\nGenerative adversarial networks (GANs) [19, 2, 30] are a popular family of generative models\nthat train a generator and discriminator in an adversarial training framework. The seminal result\nby [22] proposed progressive growing of GANs (PGGANs) as a way to stabilize and accelerate the\ntraining phase of these models. Future results, such as StyleGAN [23] introduce more complicated\narchitectures and \u201cstyle\u201d variables. Additionally, these models add noise at each layer of the generator\nin order to introduce greater stochasticity in the generated images, which is important for textures\nsuch as hair and skin.\n\nDistributional Learning Most theoretical results have focused on the min-max optimality of\nGANs [15, 28, 29], characterizing their stationary points [21, 17], or characterizing their generalization\nonce they have reached a global minimum [3, 5]. The closest result to ours is [35]. Setting x\nto a deterministic scalar in our problem statement reduces it to [35], who consider y = \u03d5(b + \u03b7) s.t.\nb, \u03b7 \u2208 \u211dd, and they seek to learn the covariance of \u03b7 along with the bias vector b. However, their\nsample complexity bound suffers an exponential dependence on \u2016b\u20162 \u221e.\n\nSingle layer networks have attracted recent attention, as they provide a tractable formulation for\nstudying the dynamics and generalization of adversarial learning [24, 18, 25, 11]. The recent results\nof [1] show that multi-layered models that satisfy a property known as forward super-resolution (such\nas PGGANs) can be learned in polynomial time and sample complexity using stochastic gradient\ndescent-ascent. In this case, the discriminator is designed to detect differences between higher\norder moments of the generated and training distribution. Deep models have also been considered\nin [12, 10].\n\n## Main Results\n\nIn this section we first show that the MLE of the parameters learns the input-output joint distribution\nfor linear regression. Then, we extend this guarantee to the case where the ReLU activation function\n\u03d5 is applied to the multi-dimensional output y. Finally, we show that we can compose the 1-layer\nReLU guarantee to learn the distribution generated by a multi-layer model.", "images": [], "items": [{"type": "text", "value": "with probability 1 - \u03b4. Using a careful covering argument and n = (k/\u03b52) log(1/\u03b4) samples, we can\nuniformly extend this to all w not satisfying Eq (4). By definition, the MLE has L(w) \u2265 L(w*), and\nby our uniform bound, it must satisfy (4).\n\nThe second stage changes because fw(x) depends on x \u00b7 w and x \u00b7 w* in a more complicated way\nthan through x \u00b7 (w - w*). This makes showing a bounded VC dimension more difficult; however,\nunpacking the proof that VC implies generalization, we can still show that the net (normally given by\nSauer\u2019s lemma) is bounded. This generalization holds as long as fw(x) is unimodal in x \u00b7 w.", "md": "with probability 1 - \u03b4. Using a careful covering argument and n = (k/\u03b52) log(1/\u03b4) samples, we can\nuniformly extend this to all w not satisfying Eq (4). By definition, the MLE has L(w) \u2265 L(w*), and\nby our uniform bound, it must satisfy (4).\n\nThe second stage changes because fw(x) depends on x \u00b7 w and x \u00b7 w* in a more complicated way\nthan through x \u00b7 (w - w*). This makes showing a bounded VC dimension more difficult; however,\nunpacking the proof that VC implies generalization, we can still show that the net (normally given by\nSauer\u2019s lemma) is bounded. This generalization holds as long as fw(x) is unimodal in x \u00b7 w."}, {"type": "heading", "lvl": 2, "value": "Contributions", "md": "## Contributions"}, {"type": "text", "value": "1. We show that MLE can perform distribution learning in the setting of linear regression and\nmulti-layer ReLU networks. Our bounds do not make assumptions on the distribution of x or the\ncondition number of W*, and achieve a sample complexity polynomial in the system parameters.\n2. We improve the sample complexity bound in [35], which estimates the parameters of a one-layer\nReLU network but suffers an exponential dependence on the W* term. In contrast, as we seek to\nestimate the distribution of (x, y), rather than the parameter W*, we are able to avoid this. See\nSection 4.2 for more details.\n3. Our algorithm for learning multi-layer ReLU networks is considerably simpler than [1], who\nlearn discriminators that are engineered to perform moment-matching on the output of each layer\nof the network. Furthermore, [1] impose a strong requirement on the sparsity and independence\nof the activations at each layer, which essentially allows standard techniques in sparse coding to\nrecover these activations.", "md": "1. We show that MLE can perform distribution learning in the setting of linear regression and\nmulti-layer ReLU networks. Our bounds do not make assumptions on the distribution of x or the\ncondition number of W*, and achieve a sample complexity polynomial in the system parameters.\n2. We improve the sample complexity bound in [35], which estimates the parameters of a one-layer\nReLU network but suffers an exponential dependence on the W* term. In contrast, as we seek to\nestimate the distribution of (x, y), rather than the parameter W*, we are able to avoid this. See\nSection 4.2 for more details.\n3. Our algorithm for learning multi-layer ReLU networks is considerably simpler than [1], who\nlearn discriminators that are engineered to perform moment-matching on the output of each layer\nof the network. Furthermore, [1] impose a strong requirement on the sparsity and independence\nof the activations at each layer, which essentially allows standard techniques in sparse coding to\nrecover these activations."}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "Generative adversarial networks (GANs) [19, 2, 30] are a popular family of generative models\nthat train a generator and discriminator in an adversarial training framework. The seminal result\nby [22] proposed progressive growing of GANs (PGGANs) as a way to stabilize and accelerate the\ntraining phase of these models. Future results, such as StyleGAN [23] introduce more complicated\narchitectures and \u201cstyle\u201d variables. Additionally, these models add noise at each layer of the generator\nin order to introduce greater stochasticity in the generated images, which is important for textures\nsuch as hair and skin.\n\nDistributional Learning Most theoretical results have focused on the min-max optimality of\nGANs [15, 28, 29], characterizing their stationary points [21, 17], or characterizing their generalization\nonce they have reached a global minimum [3, 5]. The closest result to ours is [35]. Setting x\nto a deterministic scalar in our problem statement reduces it to [35], who consider y = \u03d5(b + \u03b7) s.t.\nb, \u03b7 \u2208 \u211dd, and they seek to learn the covariance of \u03b7 along with the bias vector b. However, their\nsample complexity bound suffers an exponential dependence on \u2016b\u20162 \u221e.\n\nSingle layer networks have attracted recent attention, as they provide a tractable formulation for\nstudying the dynamics and generalization of adversarial learning [24, 18, 25, 11]. The recent results\nof [1] show that multi-layered models that satisfy a property known as forward super-resolution (such\nas PGGANs) can be learned in polynomial time and sample complexity using stochastic gradient\ndescent-ascent. In this case, the discriminator is designed to detect differences between higher\norder moments of the generated and training distribution. Deep models have also been considered\nin [12, 10].", "md": "Generative adversarial networks (GANs) [19, 2, 30] are a popular family of generative models\nthat train a generator and discriminator in an adversarial training framework. The seminal result\nby [22] proposed progressive growing of GANs (PGGANs) as a way to stabilize and accelerate the\ntraining phase of these models. Future results, such as StyleGAN [23] introduce more complicated\narchitectures and \u201cstyle\u201d variables. Additionally, these models add noise at each layer of the generator\nin order to introduce greater stochasticity in the generated images, which is important for textures\nsuch as hair and skin.\n\nDistributional Learning Most theoretical results have focused on the min-max optimality of\nGANs [15, 28, 29], characterizing their stationary points [21, 17], or characterizing their generalization\nonce they have reached a global minimum [3, 5]. The closest result to ours is [35]. Setting x\nto a deterministic scalar in our problem statement reduces it to [35], who consider y = \u03d5(b + \u03b7) s.t.\nb, \u03b7 \u2208 \u211dd, and they seek to learn the covariance of \u03b7 along with the bias vector b. However, their\nsample complexity bound suffers an exponential dependence on \u2016b\u20162 \u221e.\n\nSingle layer networks have attracted recent attention, as they provide a tractable formulation for\nstudying the dynamics and generalization of adversarial learning [24, 18, 25, 11]. The recent results\nof [1] show that multi-layered models that satisfy a property known as forward super-resolution (such\nas PGGANs) can be learned in polynomial time and sample complexity using stochastic gradient\ndescent-ascent. In this case, the discriminator is designed to detect differences between higher\norder moments of the generated and training distribution. Deep models have also been considered\nin [12, 10]."}, {"type": "heading", "lvl": 2, "value": "Main Results", "md": "## Main Results"}, {"type": "text", "value": "In this section we first show that the MLE of the parameters learns the input-output joint distribution\nfor linear regression. Then, we extend this guarantee to the case where the ReLU activation function\n\u03d5 is applied to the multi-dimensional output y. Finally, we show that we can compose the 1-layer\nReLU guarantee to learn the distribution generated by a multi-layer model.", "md": "In this section we first show that the MLE of the parameters learns the input-output joint distribution\nfor linear regression. Then, we extend this guarantee to the case where the ReLU activation function\n\u03d5 is applied to the multi-dimensional output y. Finally, we show that we can compose the 1-layer\nReLU guarantee to learn the distribution generated by a multi-layer model."}]}, {"page": 5, "text": " 4.1     Linear Regression\nWe begin with the classic linear regression problem of learning the parameter w\u2217                                    from a linear model\n                                                             y = x \u00b7 w\u2217      + \u03b7,                                                           (6)\n where \u03b7 \u223c        N  (0, \u03c32), w \u2208       Rk, \u03c3 is known and x \u2208               Rk with some distribution. This problem has\n been studied for centuries. Our novelty is that we view (6) as a conditional generative process, and\n instead of studying error in Euclidean distance in parameter space, i.e., minimizing \u2225                                 w \u00b7 x \u2212     w\u2217   \u00b7 x\u22252,\n we focus on error in d(w\u2217,               w) as defined in (3), which only captures the error in                           w insofar as it\n impacts our distribution estimate. Given data {(xi, yi)}n                       i=1 generated by (6), the MLE is:\n                         w := arg max                 log pw(yi|xi) \u2261         arg min             (yi \u2212    w \u00b7 xi)2    .\n                                    w\u2208Rk       i\u2208[n]                            w\u2208Rk      i\u2208[n]           \u03c32\n The following theorem establishes that the MLE is close in TV distance. The proofs for all results in\n this section are in Appendix A.\n Theorem 4.1. Let {(xi, yi)}n             i=1 be i.i.d. random variables generated from the linear model (6), and\n assume that \u03c3 is known. Then, for a sufficiently large constant C > 0,\n                                                             n = C k   \u03b52 log 1  \u03b5\n samples suffice to ensure that with probability 1 \u2212                      e\u2212\u2126(\u03b52n) over the data,\n                                                              d( w, w\u2217) \u2264       \u03b5.\n Note that one cannot hope to get such a guarantee in the classical setting where error is measured\n in \u2225 w \u00b7 x \u2212     w\u2217   \u00b7 x\u22252 without additional assumptions on the distribution of x because if x is badly\n conditioned, the error may be dominated by very rare directions of x that we never sample. For\n example, the bounds in [7], Chapter 3, require the second moment of x to be bounded.\n Since we only wish to learn the distribution of y in total variation, no single x can contribute much\n to our loss and we get a distribution-free result. This is possible as the total variation distance is\n bounded, and we can invoke Theorem 11.2 in Gy\u00f6rfi et al [20].\nWe now state two lemmas needed to prove Thereom 4.1, assuming without loss of generality that\n \u03c32 = 1. We split the proof into two stages. In the first stage, we bound                              d( w, w\u2217), which denotes the\n empirical TV distance (4) over the training set {xi}n                     i=1.\n Lemma 4.2. Let {(xi, yi)}n              i=1 be i.i.d. random variables such that yi = xi \u00b7 w\u2217                         + N    (0, 1). Then,\n for n \u2265     k                                                           w satisfies\n             2 , with probability 1 \u2212          e\u2212\u2126(n), the MLE                    k\n                                                           d( w, w\u2217) \u2264           2n.\n The proof relies on the fact that pw\u2217(y|xi) and p                    w(y|xi) are Gaussian distributions, so\n                                 d(p  w(y|xi), pw\u2217(y|xi)) = \u0398(min{1, |xT                  i ( w \u2212    w\u2217)|}).\n Using the explicit form of the MLE, we can show that, with high probability,\n                                                   1         xTi ( w \u2212    w\u2217)    2 \u2264     k\n                                                   n     i                              2n,\n and Lemma 4.2 follows from Jensen\u2019s inequality.\n The second stage shows that the empirical average of the TV distance                                       d(  w, w\u2217) is close to the\n population average d(          w, w\u2217).\n Lemma 4.3. Let {xi}n             i=1 be i.i.d. random variables such that xi \u223c                        Dx. For a sufficiently large\n constant C > 0, and for n = C k               \u03b52 log 1 \u03b5 with n \u2265        k2, we have:\n                                   Pr        sup    d(w, w\u2217) \u2212        d(w, w\u2217)        > \u03b5     \u2264   e\u2212\u2126(n\u03b52).\n                                 xi\u223cDx      w\u2208Rk\n                                                                       5", "md": "## 4.1 Linear Regression\n\nWe begin with the classic linear regression problem of learning the parameter \\(w^*\\) from a linear model\n\n\\[\ny = x \\cdot w^* + \\eta, \\quad (6)\n\\]\nwhere \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\), \\(w \\in \\mathbb{R}^k\\), \\(\\sigma\\) is known and \\(x \\in \\mathbb{R}^k\\) with some distribution. This problem has been studied for centuries. Our novelty is that we view (6) as a conditional generative process, and instead of studying error in Euclidean distance in parameter space, i.e., minimizing \\(\\|w \\cdot x - w^* \\cdot x\\|^2\\), we focus on error in \\(d(w^*, w)\\) as defined in (3), which only captures the error in \\(w\\) insofar as it impacts our distribution estimate. Given data \\(\\{(x_i, y_i)\\}_{i=1}^n\\) generated by (6), the MLE is:\n\n\\[\nw := \\arg \\max_{w \\in \\mathbb{R}^k} \\sum_{i=1}^{n} \\log p_w(y_i|x_i) \\equiv \\arg \\min_{w \\in \\mathbb{R}^k} \\sum_{i=1}^{n} (y_i - w \\cdot x_i)^2 / \\sigma^2\n\\]\nThe following theorem establishes that the MLE is close in TV distance. The proofs for all results in this section are in Appendix A.\n\nTheorem 4.1. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables generated from the linear model (6), and assume that \\(\\sigma\\) is known. Then, for a sufficiently large constant \\(C > 0\\),\n\n\\[\nn = Ck \\epsilon^2 \\log \\frac{1}{\\epsilon}\n\\]\nsamples suffice to ensure that with probability \\(1 - e^{-\\Omega(\\epsilon^2n)}\\) over the data,\n\n\\[\nd(w, w^*) \\leq \\epsilon.\n\\]\nNote that one cannot hope to get such a guarantee in the classical setting where error is measured in \\(\\|w \\cdot x - w^* \\cdot x\\|^2\\) without additional assumptions on the distribution of \\(x\\) because if \\(x\\) is badly conditioned, the error may be dominated by very rare directions of \\(x\\) that we never sample. For example, the bounds in [7], Chapter 3, require the second moment of \\(x\\) to be bounded.\n\nSince we only wish to learn the distribution of \\(y\\) in total variation, no single \\(x\\) can contribute much to our loss and we get a distribution-free result. This is possible as the total variation distance is bounded, and we can invoke Theorem 11.2 in Gy\u00f6rfi et al [20].\n\nWe now state two lemmas needed to prove Theorem 4.1, assuming without loss of generality that \\(\\sigma^2 = 1\\). We split the proof into two stages. In the first stage, we bound \\(d(w, w^*)\\), which denotes the empirical TV distance (4) over the training set \\(\\{x_i\\}_{i=1}^n\\).\n\nLemma 4.2. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables such that \\(y_i = x_i \\cdot w^* + \\mathcal{N}(0, 1)\\). Then, for \\(n \\geq \\frac{k}{2}\\), with probability \\(1 - e^{-\\Omega(n)}\\), the MLE \\(w\\) satisfies\n\n\\[\nd(w, w^*) \\leq 2n.\n\\]\nThe proof relies on the fact that \\(p_{w^*}(y|x_i)\\) and \\(p_w(y|x_i)\\) are Gaussian distributions, so\n\n\\[\nd(p_w(y|x_i), p_{w^*}(y|x_i)) = \\Theta(\\min\\{1, |x_i^T (w - w^*)|\\}).\n\\]\nUsing the explicit form of the MLE, we can show that, with high probability,\n\n\\[\n\\frac{1}{n} \\sum_{i} x_i^T (w - w^*)^2 \\leq \\frac{k}{2} 2n,\n\\]\nand Lemma 4.2 follows from Jensen\u2019s inequality.\n\nThe second stage shows that the empirical average of the TV distance \\(d(w, w^*)\\) is close to the population average \\(d(w, w^*)\\).\n\nLemma 4.3. Let \\(\\{x_i\\}_{i=1}^n\\) be i.i.d. random variables such that \\(x_i \\sim D_x\\). For a sufficiently large constant \\(C > 0\\), and for \\(n = Ck \\epsilon^2 \\log \\frac{1}{\\epsilon}\\) with \\(n \\geq k^2\\), we have:\n\n\\[\n\\Pr\\left(\\sup_{x_i \\sim D_x} d(w, w^*) - d(w, w^*) > \\epsilon\\right) \\leq e^{-\\Omega(n\\epsilon^2)}.\n\\]", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "4.1 Linear Regression", "md": "## 4.1 Linear Regression"}, {"type": "text", "value": "We begin with the classic linear regression problem of learning the parameter \\(w^*\\) from a linear model\n\n\\[\ny = x \\cdot w^* + \\eta, \\quad (6)\n\\]\nwhere \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\), \\(w \\in \\mathbb{R}^k\\), \\(\\sigma\\) is known and \\(x \\in \\mathbb{R}^k\\) with some distribution. This problem has been studied for centuries. Our novelty is that we view (6) as a conditional generative process, and instead of studying error in Euclidean distance in parameter space, i.e., minimizing \\(\\|w \\cdot x - w^* \\cdot x\\|^2\\), we focus on error in \\(d(w^*, w)\\) as defined in (3), which only captures the error in \\(w\\) insofar as it impacts our distribution estimate. Given data \\(\\{(x_i, y_i)\\}_{i=1}^n\\) generated by (6), the MLE is:\n\n\\[\nw := \\arg \\max_{w \\in \\mathbb{R}^k} \\sum_{i=1}^{n} \\log p_w(y_i|x_i) \\equiv \\arg \\min_{w \\in \\mathbb{R}^k} \\sum_{i=1}^{n} (y_i - w \\cdot x_i)^2 / \\sigma^2\n\\]\nThe following theorem establishes that the MLE is close in TV distance. The proofs for all results in this section are in Appendix A.\n\nTheorem 4.1. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables generated from the linear model (6), and assume that \\(\\sigma\\) is known. Then, for a sufficiently large constant \\(C > 0\\),\n\n\\[\nn = Ck \\epsilon^2 \\log \\frac{1}{\\epsilon}\n\\]\nsamples suffice to ensure that with probability \\(1 - e^{-\\Omega(\\epsilon^2n)}\\) over the data,\n\n\\[\nd(w, w^*) \\leq \\epsilon.\n\\]\nNote that one cannot hope to get such a guarantee in the classical setting where error is measured in \\(\\|w \\cdot x - w^* \\cdot x\\|^2\\) without additional assumptions on the distribution of \\(x\\) because if \\(x\\) is badly conditioned, the error may be dominated by very rare directions of \\(x\\) that we never sample. For example, the bounds in [7], Chapter 3, require the second moment of \\(x\\) to be bounded.\n\nSince we only wish to learn the distribution of \\(y\\) in total variation, no single \\(x\\) can contribute much to our loss and we get a distribution-free result. This is possible as the total variation distance is bounded, and we can invoke Theorem 11.2 in Gy\u00f6rfi et al [20].\n\nWe now state two lemmas needed to prove Theorem 4.1, assuming without loss of generality that \\(\\sigma^2 = 1\\). We split the proof into two stages. In the first stage, we bound \\(d(w, w^*)\\), which denotes the empirical TV distance (4) over the training set \\(\\{x_i\\}_{i=1}^n\\).\n\nLemma 4.2. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables such that \\(y_i = x_i \\cdot w^* + \\mathcal{N}(0, 1)\\). Then, for \\(n \\geq \\frac{k}{2}\\), with probability \\(1 - e^{-\\Omega(n)}\\), the MLE \\(w\\) satisfies\n\n\\[\nd(w, w^*) \\leq 2n.\n\\]\nThe proof relies on the fact that \\(p_{w^*}(y|x_i)\\) and \\(p_w(y|x_i)\\) are Gaussian distributions, so\n\n\\[\nd(p_w(y|x_i), p_{w^*}(y|x_i)) = \\Theta(\\min\\{1, |x_i^T (w - w^*)|\\}).\n\\]\nUsing the explicit form of the MLE, we can show that, with high probability,\n\n\\[\n\\frac{1}{n} \\sum_{i} x_i^T (w - w^*)^2 \\leq \\frac{k}{2} 2n,\n\\]\nand Lemma 4.2 follows from Jensen\u2019s inequality.\n\nThe second stage shows that the empirical average of the TV distance \\(d(w, w^*)\\) is close to the population average \\(d(w, w^*)\\).\n\nLemma 4.3. Let \\(\\{x_i\\}_{i=1}^n\\) be i.i.d. random variables such that \\(x_i \\sim D_x\\). For a sufficiently large constant \\(C > 0\\), and for \\(n = Ck \\epsilon^2 \\log \\frac{1}{\\epsilon}\\) with \\(n \\geq k^2\\), we have:\n\n\\[\n\\Pr\\left(\\sup_{x_i \\sim D_x} d(w, w^*) - d(w, w^*) > \\epsilon\\right) \\leq e^{-\\Omega(n\\epsilon^2)}.\n\\]", "md": "We begin with the classic linear regression problem of learning the parameter \\(w^*\\) from a linear model\n\n\\[\ny = x \\cdot w^* + \\eta, \\quad (6)\n\\]\nwhere \\(\\eta \\sim \\mathcal{N}(0, \\sigma^2)\\), \\(w \\in \\mathbb{R}^k\\), \\(\\sigma\\) is known and \\(x \\in \\mathbb{R}^k\\) with some distribution. This problem has been studied for centuries. Our novelty is that we view (6) as a conditional generative process, and instead of studying error in Euclidean distance in parameter space, i.e., minimizing \\(\\|w \\cdot x - w^* \\cdot x\\|^2\\), we focus on error in \\(d(w^*, w)\\) as defined in (3), which only captures the error in \\(w\\) insofar as it impacts our distribution estimate. Given data \\(\\{(x_i, y_i)\\}_{i=1}^n\\) generated by (6), the MLE is:\n\n\\[\nw := \\arg \\max_{w \\in \\mathbb{R}^k} \\sum_{i=1}^{n} \\log p_w(y_i|x_i) \\equiv \\arg \\min_{w \\in \\mathbb{R}^k} \\sum_{i=1}^{n} (y_i - w \\cdot x_i)^2 / \\sigma^2\n\\]\nThe following theorem establishes that the MLE is close in TV distance. The proofs for all results in this section are in Appendix A.\n\nTheorem 4.1. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables generated from the linear model (6), and assume that \\(\\sigma\\) is known. Then, for a sufficiently large constant \\(C > 0\\),\n\n\\[\nn = Ck \\epsilon^2 \\log \\frac{1}{\\epsilon}\n\\]\nsamples suffice to ensure that with probability \\(1 - e^{-\\Omega(\\epsilon^2n)}\\) over the data,\n\n\\[\nd(w, w^*) \\leq \\epsilon.\n\\]\nNote that one cannot hope to get such a guarantee in the classical setting where error is measured in \\(\\|w \\cdot x - w^* \\cdot x\\|^2\\) without additional assumptions on the distribution of \\(x\\) because if \\(x\\) is badly conditioned, the error may be dominated by very rare directions of \\(x\\) that we never sample. For example, the bounds in [7], Chapter 3, require the second moment of \\(x\\) to be bounded.\n\nSince we only wish to learn the distribution of \\(y\\) in total variation, no single \\(x\\) can contribute much to our loss and we get a distribution-free result. This is possible as the total variation distance is bounded, and we can invoke Theorem 11.2 in Gy\u00f6rfi et al [20].\n\nWe now state two lemmas needed to prove Theorem 4.1, assuming without loss of generality that \\(\\sigma^2 = 1\\). We split the proof into two stages. In the first stage, we bound \\(d(w, w^*)\\), which denotes the empirical TV distance (4) over the training set \\(\\{x_i\\}_{i=1}^n\\).\n\nLemma 4.2. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables such that \\(y_i = x_i \\cdot w^* + \\mathcal{N}(0, 1)\\). Then, for \\(n \\geq \\frac{k}{2}\\), with probability \\(1 - e^{-\\Omega(n)}\\), the MLE \\(w\\) satisfies\n\n\\[\nd(w, w^*) \\leq 2n.\n\\]\nThe proof relies on the fact that \\(p_{w^*}(y|x_i)\\) and \\(p_w(y|x_i)\\) are Gaussian distributions, so\n\n\\[\nd(p_w(y|x_i), p_{w^*}(y|x_i)) = \\Theta(\\min\\{1, |x_i^T (w - w^*)|\\}).\n\\]\nUsing the explicit form of the MLE, we can show that, with high probability,\n\n\\[\n\\frac{1}{n} \\sum_{i} x_i^T (w - w^*)^2 \\leq \\frac{k}{2} 2n,\n\\]\nand Lemma 4.2 follows from Jensen\u2019s inequality.\n\nThe second stage shows that the empirical average of the TV distance \\(d(w, w^*)\\) is close to the population average \\(d(w, w^*)\\).\n\nLemma 4.3. Let \\(\\{x_i\\}_{i=1}^n\\) be i.i.d. random variables such that \\(x_i \\sim D_x\\). For a sufficiently large constant \\(C > 0\\), and for \\(n = Ck \\epsilon^2 \\log \\frac{1}{\\epsilon}\\) with \\(n \\geq k^2\\), we have:\n\n\\[\n\\Pr\\left(\\sup_{x_i \\sim D_x} d(w, w^*) - d(w, w^*) > \\epsilon\\right) \\leq e^{-\\Omega(n\\epsilon^2)}.\n\\]"}]}, {"page": 6, "text": "Note the probability in the above statement is with respect to the distribution of x, and does not\ndepend on y. The proof follows Theorem 11.2 in Gy\u00f6rfi et al [20]: it relies on the fact that the TV\ndistance is bounded and a unimodal function of w \u00b7 x. This implies that for each xi, we are able to\npartition the space of w with O(1/\u03b5) hyperplanes such that within each cell the TV distance varies\nby at most \u03b5. As we have n samples and w \u2208        Rk, the number of cells induced in Rk is \u221d       (n/\u03b5)k,\nand it is sufficient to provide concentration bounds for one representative in each cell. This approach\nis similar to bounding the VC dimension of a set of binary functions. Setting n = \u0398( k       \u03b52 log 1\ncombining Lemma 4.2 with Lemma 4.3 gives d(         w, w\u2217) \u2264   \u03b5.                                   \u03b5) and\n4.2   ReLU Case\nNow consider the single-layer ReLU. We observe (x, y) \u2208        Rk \u00d7 Rd such that:\n                                  y = \u03d5(W \u2217x + \u03b7),       \u03b7 \u223c  N(0, \u03a3\u2217),                                 (7)\nwhere \u03b7 \u2208   Rd, W \u2217   and \u03d5(\u00b7) = max(\u00b7, 0) is applied coordinate-wise. The matrices W \u2217           \u2208  Rd\u00d7k\nand \u03a3\u2217  \u2208  Rd\u00d7d are unknown, and we do not observe \u03b7. The variable x is drawn from an arbitrary\nprobability distribution Dx, and we make no additional assumptions on Dx: this is important, as we\nwill progressively cascade layers, and one should think of Dx being the distribution of activations at\neach layer.\nGiven a sample (x, y) \u2208   Rk \u00d7 Rd, let S denote the co-ordinates of y that are zero-valued, and let Sc\ndenote the compliment of S. Then, the log-likelihood of W, \u03a3, on this sample is given by\n        log pW,\u03a3(y|x) = c \u2212    log|\u03a3|  + log           exp   \u2212(t \u2212   Wx)T \u03a3\u22121(t \u2212     Wx)     dtS.      (8)\n                                  2                                          2\n                                               tS\u22640\n                                             tSc=ySc\nwhere c is a normalization constant which does not depend on W or \u03a3. This function is a mixed\ndensity: in the coordinates of y that are 0, i.e., in the set S, we integrate the Gaussian density over the\nnegative orthant, as W \u2217x + \u03b7 could have been any negative value in those coordinates.In Lemma F.1\nin the Appendix, we show that Eqn (8) is concave after an invertible reparameterization of W, \u03a3.\nIn this setting, proving an analogue of Theorem 4.1 poses multiple challenges:\n\u2022 The output y is d-dimensional rather than a scalar, and \u03b7 in Eqn (7) introduces correlations\n  between the coordinates of W \u2217x, such that we cannot decompose the log-likelihood in Eqn (8) per\n  coordinate.\n\u2022 We do not know the covariance matrix \u03a3\u2217, and it must be estimated.\n\u2022 Lemma 4.2 requires the explicit form of the MLE in linear regression. In the absence of such a\n  closed-form solution, we need to directly analyze the log-likelihood, which is a mixed density and\n  involves integrating the Gaussian likelihood over the zero-valued coordinates of y. In order to\n  handle this, we use the Gy\u00f6rfi approach again on the log-likelihood. This is challenging because\n  the variables we concentrate are KL-divergences, which are unbounded and require Bernstein type\n  inequalities.\n\u2022 Recovering the true parameters W \u2217, \u03a3\u2217       is difficult: if we see a zero in y, we do not know its\n  magnitude in W \u2217x + \u03b7 before the ReLU. This manifests in the results in [35], where it is assumed\n  that each entry in W \u2217  is positive \u2013 otherwise, their sample complexity scales as e\u2225W \u22252   \u221e.\nNonetheless, we can handle most of these difficulties, and the only assumption we make is that the\ncondition number of \u03a3\u2217    is bounded and known to our estimator.\nAssumption 4.4 (Condition number bound). Let \u03bb\u2217      max, \u03bb\u2217min denote the largest and smallest singular\nvalues of \u03a3\u2217. We assume there exists \u03ba < \u221e       such that \u03bb\u2217max\n                                                            \u03bb\u2217\nof \u03ba is known to our estimator.                              min \u2264  \u03ba. We further assume that the value\nNote that the condition number only allows us to control the correlation between the coordinates of\nW \u2217x + \u03b7. The other challenges introduce by the ReLU, such as the lack of a closed form MLE, the\nneed to estimate \u03a3\u2217   and a mixed density log-likelihood remain.\nUnder Assumption 4.4, the following theorem shows that the MLE              W,  \u03a3 achieves a small total\nvariation distance. The proof of this theorem is in Appendix C.\n                                                     6", "md": "Note the probability in the above statement is with respect to the distribution of x, and does not\ndepend on y. The proof follows Theorem 11.2 in Gy\u00f6rfi et al [20]: it relies on the fact that the TV\ndistance is bounded and a unimodal function of w \u00b7 x. This implies that for each xi, we are able to\npartition the space of w with O(1/\u03b5) hyperplanes such that within each cell the TV distance varies\nby at most \u03b5. As we have n samples and \\(w \\in \\mathbb{R}^k\\), the number of cells induced in \\(\\mathbb{R}^k\\) is proportional to \\((n/\\epsilon)^k\\),\nand it is sufficient to provide concentration bounds for one representative in each cell. This approach\nis similar to bounding the VC dimension of a set of binary functions. Setting \\(n = \\Theta(k/\\epsilon^2 \\log(1/\\epsilon)\\) combining Lemma 4.2 with Lemma 4.3 gives \\(d(w, w^*) \\leq \\epsilon\\).\n\n## 4.2 ReLU Case\n\nNow consider the single-layer ReLU. We observe \\((x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}^d\\) such that:\n\n\\[\ny = \\phi(W^*x + \\eta), \\quad \\eta \\sim \\mathcal{N}(0, \\Sigma^*), \\quad (7)\n\\]\nwhere \\(\\eta \\in \\mathbb{R}^d\\), \\(W^*\\) and \\(\\phi(\\cdot) = \\max(\\cdot, 0)\\) is applied coordinate-wise. The matrices \\(W^* \\in \\mathbb{R}^{d \\times k}\\)\nand \\(\\Sigma^* \\in \\mathbb{R}^{d \\times d}\\) are unknown, and we do not observe \\(\\eta\\). The variable x is drawn from an arbitrary\nprobability distribution \\(D_x\\), and we make no additional assumptions on \\(D_x\\): this is important, as we\nwill progressively cascade layers, and one should think of \\(D_x\\) being the distribution of activations at\neach layer.\n\nGiven a sample \\((x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}^d\\), let \\(S\\) denote the coordinates of \\(y\\) that are zero-valued, and let \\(S^c\\)\ndenote the complement of \\(S\\). Then, the log-likelihood of \\(W, \\Sigma\\), on this sample is given by\n\n\\[\n\\log p_{W,\\Sigma}(y|x) = c - \\log|\\Sigma| + \\log \\int_{t^S \\leq 0, t^{S^c}=y^{S^c}} \\exp\\left(-(t - Wx)^T \\Sigma^{-1} (t - Wx)\\right) dt^S. \\quad (8)\n\\]\nwhere \\(c\\) is a normalization constant which does not depend on \\(W\\) or \\(\\Sigma\\). This function is a mixed\ndensity: in the coordinates of \\(y\\) that are 0, i.e., in the set \\(S\\), we integrate the Gaussian density over the\nnegative orthant, as \\(W^*x + \\eta\\) could have been any negative value in those coordinates. In Lemma F.1\nin the Appendix, we show that Eqn (8) is concave after an invertible reparameterization of \\(W, \\Sigma\\).\n\nIn this setting, proving an analogue of Theorem 4.1 poses multiple challenges:\n\n- The output \\(y\\) is \\(d\\)-dimensional rather than a scalar, and \\(\\eta\\) in Eqn (7) introduces correlations\nbetween the coordinates of \\(W^*x\\), such that we cannot decompose the log-likelihood in Eqn (8) per\ncoordinate.\n- We do not know the covariance matrix \\(\\Sigma^*\\), and it must be estimated.\n- Lemma 4.2 requires the explicit form of the MLE in linear regression. In the absence of such a\nclosed-form solution, we need to directly analyze the log-likelihood, which is a mixed density and\ninvolves integrating the Gaussian likelihood over the zero-valued coordinates of \\(y\\). In order to\nhandle this, we use the Gy\u00f6rfi approach again on the log-likelihood. This is challenging because\nthe variables we concentrate are KL-divergences, which are unbounded and require Bernstein type\ninequalities.\n- Recovering the true parameters \\(W^*, \\Sigma^*\\) is difficult: if we see a zero in \\(y\\), we do not know its\nmagnitude in \\(W^*x + \\eta\\) before the ReLU. This manifests in the results in [35], where it is assumed\nthat each entry in \\(W^*\\) is positive \u2013 otherwise, their sample complexity scales as \\(e^{\\|W\\|_2^\\infty}\\).\n\nNonetheless, we can handle most of these difficulties, and the only assumption we make is that the\ncondition number of \\(\\Sigma^*\\) is bounded and known to our estimator.\n\nAssumption 4.4 (Condition number bound). Let \\(\\lambda^*_{\\text{max}}, \\lambda^*_{\\text{min}}\\) denote the largest and smallest singular\nvalues of \\(\\Sigma^*\\). We assume there exists \\(\\kappa < \\infty\\) such that \\(\\lambda^*_{\\text{max}}/\\lambda^*_{\\text{min}} \\leq \\kappa\\). We further assume that the value\nof \\(\\kappa\\) is known to our estimator.\n\nNote that the condition number only allows us to control the correlation between the coordinates of\n\\(W^*x + \\eta\\). The other challenges introduced by the ReLU, such as the lack of a closed form MLE, the\nneed to estimate \\(\\Sigma^*\\) and a mixed density log-likelihood remain.\n\nUnder Assumption 4.4, the following theorem shows that the MLE \\(W, \\Sigma\\) achieves a small total\nvariation distance. The proof of this theorem is in Appendix C.", "images": [], "items": [{"type": "text", "value": "Note the probability in the above statement is with respect to the distribution of x, and does not\ndepend on y. The proof follows Theorem 11.2 in Gy\u00f6rfi et al [20]: it relies on the fact that the TV\ndistance is bounded and a unimodal function of w \u00b7 x. This implies that for each xi, we are able to\npartition the space of w with O(1/\u03b5) hyperplanes such that within each cell the TV distance varies\nby at most \u03b5. As we have n samples and \\(w \\in \\mathbb{R}^k\\), the number of cells induced in \\(\\mathbb{R}^k\\) is proportional to \\((n/\\epsilon)^k\\),\nand it is sufficient to provide concentration bounds for one representative in each cell. This approach\nis similar to bounding the VC dimension of a set of binary functions. Setting \\(n = \\Theta(k/\\epsilon^2 \\log(1/\\epsilon)\\) combining Lemma 4.2 with Lemma 4.3 gives \\(d(w, w^*) \\leq \\epsilon\\).", "md": "Note the probability in the above statement is with respect to the distribution of x, and does not\ndepend on y. The proof follows Theorem 11.2 in Gy\u00f6rfi et al [20]: it relies on the fact that the TV\ndistance is bounded and a unimodal function of w \u00b7 x. This implies that for each xi, we are able to\npartition the space of w with O(1/\u03b5) hyperplanes such that within each cell the TV distance varies\nby at most \u03b5. As we have n samples and \\(w \\in \\mathbb{R}^k\\), the number of cells induced in \\(\\mathbb{R}^k\\) is proportional to \\((n/\\epsilon)^k\\),\nand it is sufficient to provide concentration bounds for one representative in each cell. This approach\nis similar to bounding the VC dimension of a set of binary functions. Setting \\(n = \\Theta(k/\\epsilon^2 \\log(1/\\epsilon)\\) combining Lemma 4.2 with Lemma 4.3 gives \\(d(w, w^*) \\leq \\epsilon\\)."}, {"type": "heading", "lvl": 2, "value": "4.2 ReLU Case", "md": "## 4.2 ReLU Case"}, {"type": "text", "value": "Now consider the single-layer ReLU. We observe \\((x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}^d\\) such that:\n\n\\[\ny = \\phi(W^*x + \\eta), \\quad \\eta \\sim \\mathcal{N}(0, \\Sigma^*), \\quad (7)\n\\]\nwhere \\(\\eta \\in \\mathbb{R}^d\\), \\(W^*\\) and \\(\\phi(\\cdot) = \\max(\\cdot, 0)\\) is applied coordinate-wise. The matrices \\(W^* \\in \\mathbb{R}^{d \\times k}\\)\nand \\(\\Sigma^* \\in \\mathbb{R}^{d \\times d}\\) are unknown, and we do not observe \\(\\eta\\). The variable x is drawn from an arbitrary\nprobability distribution \\(D_x\\), and we make no additional assumptions on \\(D_x\\): this is important, as we\nwill progressively cascade layers, and one should think of \\(D_x\\) being the distribution of activations at\neach layer.\n\nGiven a sample \\((x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}^d\\), let \\(S\\) denote the coordinates of \\(y\\) that are zero-valued, and let \\(S^c\\)\ndenote the complement of \\(S\\). Then, the log-likelihood of \\(W, \\Sigma\\), on this sample is given by\n\n\\[\n\\log p_{W,\\Sigma}(y|x) = c - \\log|\\Sigma| + \\log \\int_{t^S \\leq 0, t^{S^c}=y^{S^c}} \\exp\\left(-(t - Wx)^T \\Sigma^{-1} (t - Wx)\\right) dt^S. \\quad (8)\n\\]\nwhere \\(c\\) is a normalization constant which does not depend on \\(W\\) or \\(\\Sigma\\). This function is a mixed\ndensity: in the coordinates of \\(y\\) that are 0, i.e., in the set \\(S\\), we integrate the Gaussian density over the\nnegative orthant, as \\(W^*x + \\eta\\) could have been any negative value in those coordinates. In Lemma F.1\nin the Appendix, we show that Eqn (8) is concave after an invertible reparameterization of \\(W, \\Sigma\\).\n\nIn this setting, proving an analogue of Theorem 4.1 poses multiple challenges:\n\n- The output \\(y\\) is \\(d\\)-dimensional rather than a scalar, and \\(\\eta\\) in Eqn (7) introduces correlations\nbetween the coordinates of \\(W^*x\\), such that we cannot decompose the log-likelihood in Eqn (8) per\ncoordinate.\n- We do not know the covariance matrix \\(\\Sigma^*\\), and it must be estimated.\n- Lemma 4.2 requires the explicit form of the MLE in linear regression. In the absence of such a\nclosed-form solution, we need to directly analyze the log-likelihood, which is a mixed density and\ninvolves integrating the Gaussian likelihood over the zero-valued coordinates of \\(y\\). In order to\nhandle this, we use the Gy\u00f6rfi approach again on the log-likelihood. This is challenging because\nthe variables we concentrate are KL-divergences, which are unbounded and require Bernstein type\ninequalities.\n- Recovering the true parameters \\(W^*, \\Sigma^*\\) is difficult: if we see a zero in \\(y\\), we do not know its\nmagnitude in \\(W^*x + \\eta\\) before the ReLU. This manifests in the results in [35], where it is assumed\nthat each entry in \\(W^*\\) is positive \u2013 otherwise, their sample complexity scales as \\(e^{\\|W\\|_2^\\infty}\\).\n\nNonetheless, we can handle most of these difficulties, and the only assumption we make is that the\ncondition number of \\(\\Sigma^*\\) is bounded and known to our estimator.\n\nAssumption 4.4 (Condition number bound). Let \\(\\lambda^*_{\\text{max}}, \\lambda^*_{\\text{min}}\\) denote the largest and smallest singular\nvalues of \\(\\Sigma^*\\). We assume there exists \\(\\kappa < \\infty\\) such that \\(\\lambda^*_{\\text{max}}/\\lambda^*_{\\text{min}} \\leq \\kappa\\). We further assume that the value\nof \\(\\kappa\\) is known to our estimator.\n\nNote that the condition number only allows us to control the correlation between the coordinates of\n\\(W^*x + \\eta\\). The other challenges introduced by the ReLU, such as the lack of a closed form MLE, the\nneed to estimate \\(\\Sigma^*\\) and a mixed density log-likelihood remain.\n\nUnder Assumption 4.4, the following theorem shows that the MLE \\(W, \\Sigma\\) achieves a small total\nvariation distance. The proof of this theorem is in Appendix C.", "md": "Now consider the single-layer ReLU. We observe \\((x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}^d\\) such that:\n\n\\[\ny = \\phi(W^*x + \\eta), \\quad \\eta \\sim \\mathcal{N}(0, \\Sigma^*), \\quad (7)\n\\]\nwhere \\(\\eta \\in \\mathbb{R}^d\\), \\(W^*\\) and \\(\\phi(\\cdot) = \\max(\\cdot, 0)\\) is applied coordinate-wise. The matrices \\(W^* \\in \\mathbb{R}^{d \\times k}\\)\nand \\(\\Sigma^* \\in \\mathbb{R}^{d \\times d}\\) are unknown, and we do not observe \\(\\eta\\). The variable x is drawn from an arbitrary\nprobability distribution \\(D_x\\), and we make no additional assumptions on \\(D_x\\): this is important, as we\nwill progressively cascade layers, and one should think of \\(D_x\\) being the distribution of activations at\neach layer.\n\nGiven a sample \\((x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}^d\\), let \\(S\\) denote the coordinates of \\(y\\) that are zero-valued, and let \\(S^c\\)\ndenote the complement of \\(S\\). Then, the log-likelihood of \\(W, \\Sigma\\), on this sample is given by\n\n\\[\n\\log p_{W,\\Sigma}(y|x) = c - \\log|\\Sigma| + \\log \\int_{t^S \\leq 0, t^{S^c}=y^{S^c}} \\exp\\left(-(t - Wx)^T \\Sigma^{-1} (t - Wx)\\right) dt^S. \\quad (8)\n\\]\nwhere \\(c\\) is a normalization constant which does not depend on \\(W\\) or \\(\\Sigma\\). This function is a mixed\ndensity: in the coordinates of \\(y\\) that are 0, i.e., in the set \\(S\\), we integrate the Gaussian density over the\nnegative orthant, as \\(W^*x + \\eta\\) could have been any negative value in those coordinates. In Lemma F.1\nin the Appendix, we show that Eqn (8) is concave after an invertible reparameterization of \\(W, \\Sigma\\).\n\nIn this setting, proving an analogue of Theorem 4.1 poses multiple challenges:\n\n- The output \\(y\\) is \\(d\\)-dimensional rather than a scalar, and \\(\\eta\\) in Eqn (7) introduces correlations\nbetween the coordinates of \\(W^*x\\), such that we cannot decompose the log-likelihood in Eqn (8) per\ncoordinate.\n- We do not know the covariance matrix \\(\\Sigma^*\\), and it must be estimated.\n- Lemma 4.2 requires the explicit form of the MLE in linear regression. In the absence of such a\nclosed-form solution, we need to directly analyze the log-likelihood, which is a mixed density and\ninvolves integrating the Gaussian likelihood over the zero-valued coordinates of \\(y\\). In order to\nhandle this, we use the Gy\u00f6rfi approach again on the log-likelihood. This is challenging because\nthe variables we concentrate are KL-divergences, which are unbounded and require Bernstein type\ninequalities.\n- Recovering the true parameters \\(W^*, \\Sigma^*\\) is difficult: if we see a zero in \\(y\\), we do not know its\nmagnitude in \\(W^*x + \\eta\\) before the ReLU. This manifests in the results in [35], where it is assumed\nthat each entry in \\(W^*\\) is positive \u2013 otherwise, their sample complexity scales as \\(e^{\\|W\\|_2^\\infty}\\).\n\nNonetheless, we can handle most of these difficulties, and the only assumption we make is that the\ncondition number of \\(\\Sigma^*\\) is bounded and known to our estimator.\n\nAssumption 4.4 (Condition number bound). Let \\(\\lambda^*_{\\text{max}}, \\lambda^*_{\\text{min}}\\) denote the largest and smallest singular\nvalues of \\(\\Sigma^*\\). We assume there exists \\(\\kappa < \\infty\\) such that \\(\\lambda^*_{\\text{max}}/\\lambda^*_{\\text{min}} \\leq \\kappa\\). We further assume that the value\nof \\(\\kappa\\) is known to our estimator.\n\nNote that the condition number only allows us to control the correlation between the coordinates of\n\\(W^*x + \\eta\\). The other challenges introduced by the ReLU, such as the lack of a closed form MLE, the\nneed to estimate \\(\\Sigma^*\\) and a mixed density log-likelihood remain.\n\nUnder Assumption 4.4, the following theorem shows that the MLE \\(W, \\Sigma\\) achieves a small total\nvariation distance. The proof of this theorem is in Appendix C."}]}, {"page": 7, "text": " Theorem 4.5. Let Rd\u00d7d        \u03ba     denote the set of positive definite matrices with condition number \u03ba. Given n\n samples {(xi, yi)}n       i=1 satisfying Assumption 4.4, where xi \u223c                    Dx i.i.d., and yi is generated according\n                                                                   1\n to (7), let   W   , \u03a3 := arg max         W \u2208Rd\u00d7k,\u03a3\u2208Rd\u00d7d    \u03ba      n     i log pW,\u03a3(yi | xi). Then, for a suffi               ciently large\n constant C > 0,\n                                                n = C \u00b7       kd + d2          log   \u03bakd\n                                                                    \u03b52                  \u03b5\u03b4\n samples suffice to ensure that with probability 1 \u2212                      \u03b4, we have\n                                                  dT V    ( W   ,\u03a3), (W \u2217, \u03a3\u2217)          \u2264   \u03b5.\n Comparison to [35].               Our result is closely related to [35]. Our ReLU model reduces to their model\n by setting W \u2217       \u2208   Rd as a vector, and x = 1. In order to learn the distribution of y, they first estimate\n the parameters W \u2217, \u03a3\u2217, in \u21132 norm and then convert the \u21132 error to a TV error. The parameter\n recovery is done per coordinate of W \u2217, \u03a3\u2217, by performing MAP estimation on the positive samples\n in y. This crucially assumes that each coordinate has enough positive samples, and to that end, they\n assume that each entry in W \u2217                is positive\u2014otherwise, their sample complexity scales as e\u2225W \u2217\u22252                               \u221e.\n Our results do not make any assumptions on W \u2217                           and handles a wider class of matrix valued W \u2217.\n Additionally, the objective function (8) does not discard the zero valued samples in y, making it more\n sample efficient.\nWe assumed that the covariance matrix \u03a3\u2217                     has condition number \u03ba, and our sample complexity scales\n as log \u03ba. Hence, even if \u03ba = epoly(d,k), we only pay a poly(d, k) penalty. This improves on the\n result in [35], where the sample complexity scales as \u03ba2. While our statistical guarantees are strictly\n better, [35] gives poly-time and poly-space algorithmic guarantees for their estimator. We discuss the\n empirical limitations of our algorithm in Section 6.\n Lower Bounds             Ignoring log factors, the complexity factor of kd is obviously required. Furthermore,\n learning a Gaussian with unknown covariance matrix in total variation takes \u02dc                                     \u2126(d2) samples; see\n [6]. Our Theorem 4.5 would solve their lower bound instance, the same lower bound applies to our\n problem.\n Extension to Multi-Layer Generative Models.                             Consider the following (L + 1)-layered generative\n model.\n          xL+1 = W \u2217      LxL + \u03b7L,            where        x\u2113   = \u03d5(W \u2217    \u2113\u22121x\u2113\u22121 + \u03b7\u2113\u22121) \u2208            Rd\u2113      \u2200     \u2113 \u2208   [1, L],       (9)\n              x0 \u223c     D0       and       \u03b7\u2113  \u223c  N   (0, \u03a3\u2217 \u2113)     \u2200     \u2113 \u2208   [0, L].                                                    (10)\nWe can compose the guarantees provided by Theorem 4.1 and 4.5 to show that we can learn this\n model.\n Theorem 4.6. Given n i.i.d. samples of (x0, . . . , xL+1), such that each matrix \u03a3\u2217                                  \u2113 satisfies Assump-\n tion 4.4, let     W\u2113,   \u03a3\u2113, be the MLE estimates of W \u2217               \u2113  , \u03a3\u2217\u2113 learned from samples of (x\u2113, x\u2113+1). Define\n m := max\u2113        d2\u2113. Then,                        n = O      l2m\u03b52 log      lm\u03ba\u03b5\u03b4         ,\n samples suffice to ensure that with probability 1 \u2212                      \u03b4,\n                                         dT V     { W\u2113,   \u03a3\u2113}L  \u2113=0, {(W \u2217   \u2113  , \u03a3\u2217\u2113)}L \u2113=0     \u2264   \u03b5.\n Comparison to [1].               The modelling assumptions in [1] are similar to ours \u2013 the authors learn a\n generative model per layer, using images produced per layer. The key differences of their model\n are: (i) each layer is deterministic (there is no \u03b7\u2113), (ii) their learning algorithm does not require\n access to the activations of each layer, (iii) their algorithm performs moment-matching by crafting\n the discriminator strategically.\n In order to avoid requiring activations at each layer, [1] imposes a sparsity assumption on the\n activations: this allows them to leverage tools from existing results in sparse coding [4], such that\n                                                                       7", "md": "# Math Equations and Text\n\n## Theorem 4.5\n\nLet $$\\mathbb{R}^{d\\times d}_\\kappa$$ denote the set of positive definite matrices with condition number \u03ba. Given n samples {($$x_i$$, $$y_i$$)}i=1 satisfying Assumption 4.4, where $$x_i \\sim D_x$$ i.i.d., and $$y_i$$ is generated according to (7), let $$W, \\Sigma := \\text{arg max}_{W \\in \\mathbb{R}^{d\\times k}, \\Sigma \\in \\mathbb{R}^{d\\times d}_\\kappa} \\frac{1}{n} \\sum_{i} \\log p_{W,\\Sigma}(y_i | x_i)$$. Then, for a sufficiently large constant C > 0,\n\n$$n = C \\cdot kd + d^2 \\cdot \\log \\frac{\\kappa kd}{\\varepsilon^2 \\delta}$$ samples suffice to ensure that with probability 1 - $$\\delta$$, we have $$d_T V(W, \\Sigma), (W^*, \\Sigma^*) \\leq \\varepsilon$$.\n\n### Comparison to [35]\n\nOur result is closely related to [35]. Our ReLU model reduces to their model by setting $$W^* \\in \\mathbb{R}^d$$ as a vector, and x = 1. In order to learn the distribution of y, they first estimate the parameters $$W^*, \\Sigma^*$$, in \u21132 norm and then convert the \u21132 error to a TV error. The parameter recovery is done per coordinate of $$W^*, \\Sigma^*$$, by performing MAP estimation on the positive samples in y. This crucially assumes that each coordinate has enough positive samples, and to that end, they assume that each entry in $$W^*$$ is positive\u2014otherwise, their sample complexity scales as $$e^{\\|W^*\\|_2}_\\infty$$. Our results do not make any assumptions on $$W^*$$ and handle a wider class of matrix valued $$W^*$$. Additionally, the objective function (8) does not discard the zero valued samples in y, making it more sample efficient.\n\n## Lower Bounds\n\nIgnoring log factors, the complexity factor of kd is obviously required. Furthermore, learning a Gaussian with unknown covariance matrix in total variation takes approximately $$\\Omega(d^2)$$ samples; see [6]. Our Theorem 4.5 would solve their lower bound instance, the same lower bound applies to our problem.\n\n### Extension to Multi-Layer Generative Models\n\nConsider the following (L + 1)-layered generative model.\n\n$$x_{L+1} = W^*_L x_L + \\eta_L$$, where $$x_{\\ell} = \\phi(W^*_{\\ell-1} x_{\\ell-1} + \\eta_{\\ell-1}) \\in \\mathbb{R}^{d_{\\ell}} \\forall \\ell \\in [1, L]$$\n\n$$x_0 \\sim D_0$$ and $$\\eta_{\\ell} \\sim \\mathcal{N}(0, \\Sigma^*_{\\ell}) \\forall \\ell \\in [0, L]$$.\n\n## Theorem 4.6\n\nGiven n i.i.d. samples of ($$x_0, ..., x_{L+1}$$), such that each matrix $$\\Sigma^*_{\\ell}$$ satisfies Assumption 4.4, let $$W_{\\ell}, \\Sigma_{\\ell}$$ be the MLE estimates of $$W^*_{\\ell}, \\Sigma^*_{\\ell}$$ learned from samples of ($$x_{\\ell}, x_{\\ell+1}$$). Define $$m := \\max_{\\ell} d^2_{\\ell}$$. Then, $$n = O(l^2m\\varepsilon^2 \\log lm\\kappa\\varepsilon\\delta)$$ samples suffice to ensure that with probability 1 - $$\\delta$$, $$d_T V\\{W_{\\ell}, \\Sigma_{\\ell}\\}_{\\ell=0}^{L}, \\{(W^*_{\\ell}, \\Sigma^*_{\\ell})\\}_{\\ell=0}^{L} \\leq \\varepsilon$$.\n\n### Comparison to [1]\n\nThe modelling assumptions in [1] are similar to ours \u2013 the authors learn a generative model per layer, using images produced per layer. The key differences of their model are: (i) each layer is deterministic (there is no $$\\eta_{\\ell}$$), (ii) their learning algorithm does not require access to the activations of each layer, (iii) their algorithm performs moment-matching by crafting the discriminator strategically. In order to avoid requiring activations at each layer, [1] imposes a sparsity assumption on the activations: this allows them to leverage tools from existing results in sparse coding [4], such that", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Theorem 4.5", "md": "## Theorem 4.5"}, {"type": "text", "value": "Let $$\\mathbb{R}^{d\\times d}_\\kappa$$ denote the set of positive definite matrices with condition number \u03ba. Given n samples {($$x_i$$, $$y_i$$)}i=1 satisfying Assumption 4.4, where $$x_i \\sim D_x$$ i.i.d., and $$y_i$$ is generated according to (7), let $$W, \\Sigma := \\text{arg max}_{W \\in \\mathbb{R}^{d\\times k}, \\Sigma \\in \\mathbb{R}^{d\\times d}_\\kappa} \\frac{1}{n} \\sum_{i} \\log p_{W,\\Sigma}(y_i | x_i)$$. Then, for a sufficiently large constant C > 0,\n\n$$n = C \\cdot kd + d^2 \\cdot \\log \\frac{\\kappa kd}{\\varepsilon^2 \\delta}$$ samples suffice to ensure that with probability 1 - $$\\delta$$, we have $$d_T V(W, \\Sigma), (W^*, \\Sigma^*) \\leq \\varepsilon$$.", "md": "Let $$\\mathbb{R}^{d\\times d}_\\kappa$$ denote the set of positive definite matrices with condition number \u03ba. Given n samples {($$x_i$$, $$y_i$$)}i=1 satisfying Assumption 4.4, where $$x_i \\sim D_x$$ i.i.d., and $$y_i$$ is generated according to (7), let $$W, \\Sigma := \\text{arg max}_{W \\in \\mathbb{R}^{d\\times k}, \\Sigma \\in \\mathbb{R}^{d\\times d}_\\kappa} \\frac{1}{n} \\sum_{i} \\log p_{W,\\Sigma}(y_i | x_i)$$. Then, for a sufficiently large constant C > 0,\n\n$$n = C \\cdot kd + d^2 \\cdot \\log \\frac{\\kappa kd}{\\varepsilon^2 \\delta}$$ samples suffice to ensure that with probability 1 - $$\\delta$$, we have $$d_T V(W, \\Sigma), (W^*, \\Sigma^*) \\leq \\varepsilon$$."}, {"type": "heading", "lvl": 3, "value": "Comparison to [35]", "md": "### Comparison to [35]"}, {"type": "text", "value": "Our result is closely related to [35]. Our ReLU model reduces to their model by setting $$W^* \\in \\mathbb{R}^d$$ as a vector, and x = 1. In order to learn the distribution of y, they first estimate the parameters $$W^*, \\Sigma^*$$, in \u21132 norm and then convert the \u21132 error to a TV error. The parameter recovery is done per coordinate of $$W^*, \\Sigma^*$$, by performing MAP estimation on the positive samples in y. This crucially assumes that each coordinate has enough positive samples, and to that end, they assume that each entry in $$W^*$$ is positive\u2014otherwise, their sample complexity scales as $$e^{\\|W^*\\|_2}_\\infty$$. Our results do not make any assumptions on $$W^*$$ and handle a wider class of matrix valued $$W^*$$. Additionally, the objective function (8) does not discard the zero valued samples in y, making it more sample efficient.", "md": "Our result is closely related to [35]. Our ReLU model reduces to their model by setting $$W^* \\in \\mathbb{R}^d$$ as a vector, and x = 1. In order to learn the distribution of y, they first estimate the parameters $$W^*, \\Sigma^*$$, in \u21132 norm and then convert the \u21132 error to a TV error. The parameter recovery is done per coordinate of $$W^*, \\Sigma^*$$, by performing MAP estimation on the positive samples in y. This crucially assumes that each coordinate has enough positive samples, and to that end, they assume that each entry in $$W^*$$ is positive\u2014otherwise, their sample complexity scales as $$e^{\\|W^*\\|_2}_\\infty$$. Our results do not make any assumptions on $$W^*$$ and handle a wider class of matrix valued $$W^*$$. Additionally, the objective function (8) does not discard the zero valued samples in y, making it more sample efficient."}, {"type": "heading", "lvl": 2, "value": "Lower Bounds", "md": "## Lower Bounds"}, {"type": "text", "value": "Ignoring log factors, the complexity factor of kd is obviously required. Furthermore, learning a Gaussian with unknown covariance matrix in total variation takes approximately $$\\Omega(d^2)$$ samples; see [6]. Our Theorem 4.5 would solve their lower bound instance, the same lower bound applies to our problem.", "md": "Ignoring log factors, the complexity factor of kd is obviously required. Furthermore, learning a Gaussian with unknown covariance matrix in total variation takes approximately $$\\Omega(d^2)$$ samples; see [6]. Our Theorem 4.5 would solve their lower bound instance, the same lower bound applies to our problem."}, {"type": "heading", "lvl": 3, "value": "Extension to Multi-Layer Generative Models", "md": "### Extension to Multi-Layer Generative Models"}, {"type": "text", "value": "Consider the following (L + 1)-layered generative model.\n\n$$x_{L+1} = W^*_L x_L + \\eta_L$$, where $$x_{\\ell} = \\phi(W^*_{\\ell-1} x_{\\ell-1} + \\eta_{\\ell-1}) \\in \\mathbb{R}^{d_{\\ell}} \\forall \\ell \\in [1, L]$$\n\n$$x_0 \\sim D_0$$ and $$\\eta_{\\ell} \\sim \\mathcal{N}(0, \\Sigma^*_{\\ell}) \\forall \\ell \\in [0, L]$$.", "md": "Consider the following (L + 1)-layered generative model.\n\n$$x_{L+1} = W^*_L x_L + \\eta_L$$, where $$x_{\\ell} = \\phi(W^*_{\\ell-1} x_{\\ell-1} + \\eta_{\\ell-1}) \\in \\mathbb{R}^{d_{\\ell}} \\forall \\ell \\in [1, L]$$\n\n$$x_0 \\sim D_0$$ and $$\\eta_{\\ell} \\sim \\mathcal{N}(0, \\Sigma^*_{\\ell}) \\forall \\ell \\in [0, L]$$."}, {"type": "heading", "lvl": 2, "value": "Theorem 4.6", "md": "## Theorem 4.6"}, {"type": "text", "value": "Given n i.i.d. samples of ($$x_0, ..., x_{L+1}$$), such that each matrix $$\\Sigma^*_{\\ell}$$ satisfies Assumption 4.4, let $$W_{\\ell}, \\Sigma_{\\ell}$$ be the MLE estimates of $$W^*_{\\ell}, \\Sigma^*_{\\ell}$$ learned from samples of ($$x_{\\ell}, x_{\\ell+1}$$). Define $$m := \\max_{\\ell} d^2_{\\ell}$$. Then, $$n = O(l^2m\\varepsilon^2 \\log lm\\kappa\\varepsilon\\delta)$$ samples suffice to ensure that with probability 1 - $$\\delta$$, $$d_T V\\{W_{\\ell}, \\Sigma_{\\ell}\\}_{\\ell=0}^{L}, \\{(W^*_{\\ell}, \\Sigma^*_{\\ell})\\}_{\\ell=0}^{L} \\leq \\varepsilon$$.", "md": "Given n i.i.d. samples of ($$x_0, ..., x_{L+1}$$), such that each matrix $$\\Sigma^*_{\\ell}$$ satisfies Assumption 4.4, let $$W_{\\ell}, \\Sigma_{\\ell}$$ be the MLE estimates of $$W^*_{\\ell}, \\Sigma^*_{\\ell}$$ learned from samples of ($$x_{\\ell}, x_{\\ell+1}$$). Define $$m := \\max_{\\ell} d^2_{\\ell}$$. Then, $$n = O(l^2m\\varepsilon^2 \\log lm\\kappa\\varepsilon\\delta)$$ samples suffice to ensure that with probability 1 - $$\\delta$$, $$d_T V\\{W_{\\ell}, \\Sigma_{\\ell}\\}_{\\ell=0}^{L}, \\{(W^*_{\\ell}, \\Sigma^*_{\\ell})\\}_{\\ell=0}^{L} \\leq \\varepsilon$$."}, {"type": "heading", "lvl": 3, "value": "Comparison to [1]", "md": "### Comparison to [1]"}, {"type": "text", "value": "The modelling assumptions in [1] are similar to ours \u2013 the authors learn a generative model per layer, using images produced per layer. The key differences of their model are: (i) each layer is deterministic (there is no $$\\eta_{\\ell}$$), (ii) their learning algorithm does not require access to the activations of each layer, (iii) their algorithm performs moment-matching by crafting the discriminator strategically. In order to avoid requiring activations at each layer, [1] imposes a sparsity assumption on the activations: this allows them to leverage tools from existing results in sparse coding [4], such that", "md": "The modelling assumptions in [1] are similar to ours \u2013 the authors learn a generative model per layer, using images produced per layer. The key differences of their model are: (i) each layer is deterministic (there is no $$\\eta_{\\ell}$$), (ii) their learning algorithm does not require access to the activations of each layer, (iii) their algorithm performs moment-matching by crafting the discriminator strategically. In order to avoid requiring activations at each layer, [1] imposes a sparsity assumption on the activations: this allows them to leverage tools from existing results in sparse coding [4], such that"}]}, {"page": 8, "text": "                                                                                                                                   10-1\n     10-1\n     10-2\n                                                                                                                                   10-2\n     10-3\n        102                           103                            104                           105                                                  101\n                          (a) Total Variation vs. n                                                                                                     (b) Total Variation vs. k\nFigure 2: (a) Plot of TV distance vs. n. \u03c32 = 1, w\u2217                                                                      = 1k\u00d71, for two different values of k = 5 and\nk = 20. Plot includes data for two different distributions of x. Note that distribution has little impact\non TV distance, and in both cases, we see the error decreasing with a slope of \u22121/2 in alignment\nwith our theory. (b) Plot of TV distance vs. input dimension k. For both n = 103 and n = 104, the\nerror grows with a slope of roughly 1/2, in alignment with our theory. In both plots 2000 runs are\nused to compute the mean. Error bars represent 95% confidence intervals.\nsparse activations at each layer can be recovered using images produced by the layer. This assumption\ncan be somewhat strong, as it implies that the activations are roughly independent of one another, and\nthe sparsity remains constant over layers, despite the layers themselves expanding by a factor of 4,\ni.e., d\u2113         \u2265     4d\u2113\u22121 \u2200             \u2113   \u2264     L \u2212        1.\n5         Simulations\nWe now numerically verify our theoretical claims and compare against other approaches. A detailed\ndescription of simulation methods are included in the appendix. Our code is available at https:\n//github.com/basics-lab/learningGenerativeModels.git.\n5.1          Scaling in n and k\nFigure 2 numerically investigates how TV distance of the MLE scales with the number of samples\nn and the input dimension k. We consider a model with 1-dimensional output and a k-dimensional\ninput: y = \u03d5(x \u00b7 w\u2217                                + \u03b7), for w\u2217                     \u2208     Rk and \u03b7 \u223c                       N    (0, \u03c32). We set \u03c32 = 1 and w\u2217                       = 1k\u00d71,\nboth unknown to the optimizer, which has samples (yi, xi)n                                                                                i=1. Figure 2a, which plots the error in\nTV distance against n on a log-log plot, has a slope of roughly \u22121/2 as predicted by our theory.\nSimilarly, Figure 2b has a slope of 1/2, which is in line with our theory on scaling with respect to\ninput dimension k. We defer simulations involving scaling in d to the appendix because computing\nTV distance becomes increasingly difficult as d becomes large, and we must resort to using upper\nbounds.\n5.2          Distribution Independence\nThe fact that our guarantee does not depend on the distribution of x suggests that the expected TV error\nof the distribution learned from the MLE may be similar for all distributions over x. To test this we\nconsider both x \u223c                           N    (0, Ik) and x \u223c                        k Lap(0, 1), i.e., each element of x is drawn independently\nstandard Laplace. Figure 2a verifies our hypothesis, showing only very slight differences in our\nobserved empirical average TV error between the two distributions over a wide range of n, and for\nk = 5 and k = 20.\n                                                                                                                 8", "md": "# Document\n\n## 10-1\n\n10-1\n\n10-2\n\n## 10-2\n\n10-3\n\n102\n103\n104\n105\n101\n(a) Total Variation vs. n\n\n(b) Total Variation vs. k\n\nFigure 2: (a) Plot of TV distance vs. n. $$\\sigma^2 = 1, w^* = 1k \\times 1$$, for two different values of $$k = 5$$ and $$k = 20$$. Plot includes data for two different distributions of $$x$$. Note that distribution has little impact on TV distance, and in both cases, we see the error decreasing with a slope of $$-\\frac{1}{2}$$ in alignment with our theory. (b) Plot of TV distance vs. input dimension $$k$$. For both $$n = 10^3$$ and $$n = 10^4$$, the error grows with a slope of roughly $$\\frac{1}{2}$$, in alignment with our theory. In both plots 2000 runs are used to compute the mean. Error bars represent 95% confidence intervals.\n\nsparse activations at each layer can be recovered using images produced by the layer. This assumption can be somewhat strong, as it implies that the activations are roughly independent of one another, and the sparsity remains constant over layers, despite the layers themselves expanding by a factor of 4, i.e., $$d_{\\ell} \\geq 4d_{\\ell-1} \\forall \\ell \\leq L - 1$$.\n\n### 5 Simulations\n\nWe now numerically verify our theoretical claims and compare against other approaches. A detailed description of simulation methods are included in the appendix. Our code is available at https://github.com/basics-lab/learningGenerativeModels.git.\n\n#### 5.1 Scaling in n and k\n\nFigure 2 numerically investigates how TV distance of the MLE scales with the number of samples $$n$$ and the input dimension $$k$$. We consider a model with 1-dimensional output and a $$k$$-dimensional input: $$y = \\phi(x \\cdot w^* + \\eta)$$, for $$w^* \\in \\mathbb{R}^k$$ and $$\\eta \\sim N(0, \\sigma^2)$$. We set $$\\sigma^2 = 1$$ and $$w^* = 1k \\times 1$$, both unknown to the optimizer, which has samples $$(y_i, x_i)_{i=1}^n$$. Figure 2a, which plots the error in TV distance against $$n$$ on a log-log plot, has a slope of roughly $$-\\frac{1}{2}$$ as predicted by our theory. Similarly, Figure 2b has a slope of $$\\frac{1}{2}$$, which is in line with our theory on scaling with respect to input dimension $$k$$. We defer simulations involving scaling in $$d$$ to the appendix because computing TV distance becomes increasingly difficult as $$d$$ becomes large, and we must resort to using upper bounds.\n\n#### 5.2 Distribution Independence\n\nThe fact that our guarantee does not depend on the distribution of $$x$$ suggests that the expected TV error of the distribution learned from the MLE may be similar for all distributions over $$x$$. To test this we consider both $$x \\sim N(0, I_k)$$ and $$x \\sim k \\text{ Lap}(0, 1)$$, i.e., each element of $$x$$ is drawn independently standard Laplace. Figure 2a verifies our hypothesis, showing only very slight differences in our observed empirical average TV error between the two distributions over a wide range of $$n$$, and for $$k = 5$$ and $$k = 20$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "10-1", "md": "## 10-1"}, {"type": "text", "value": "10-1\n\n10-2", "md": "10-1\n\n10-2"}, {"type": "heading", "lvl": 2, "value": "10-2", "md": "## 10-2"}, {"type": "text", "value": "10-3\n\n102\n103\n104\n105\n101\n(a) Total Variation vs. n\n\n(b) Total Variation vs. k\n\nFigure 2: (a) Plot of TV distance vs. n. $$\\sigma^2 = 1, w^* = 1k \\times 1$$, for two different values of $$k = 5$$ and $$k = 20$$. Plot includes data for two different distributions of $$x$$. Note that distribution has little impact on TV distance, and in both cases, we see the error decreasing with a slope of $$-\\frac{1}{2}$$ in alignment with our theory. (b) Plot of TV distance vs. input dimension $$k$$. For both $$n = 10^3$$ and $$n = 10^4$$, the error grows with a slope of roughly $$\\frac{1}{2}$$, in alignment with our theory. In both plots 2000 runs are used to compute the mean. Error bars represent 95% confidence intervals.\n\nsparse activations at each layer can be recovered using images produced by the layer. This assumption can be somewhat strong, as it implies that the activations are roughly independent of one another, and the sparsity remains constant over layers, despite the layers themselves expanding by a factor of 4, i.e., $$d_{\\ell} \\geq 4d_{\\ell-1} \\forall \\ell \\leq L - 1$$.", "md": "10-3\n\n102\n103\n104\n105\n101\n(a) Total Variation vs. n\n\n(b) Total Variation vs. k\n\nFigure 2: (a) Plot of TV distance vs. n. $$\\sigma^2 = 1, w^* = 1k \\times 1$$, for two different values of $$k = 5$$ and $$k = 20$$. Plot includes data for two different distributions of $$x$$. Note that distribution has little impact on TV distance, and in both cases, we see the error decreasing with a slope of $$-\\frac{1}{2}$$ in alignment with our theory. (b) Plot of TV distance vs. input dimension $$k$$. For both $$n = 10^3$$ and $$n = 10^4$$, the error grows with a slope of roughly $$\\frac{1}{2}$$, in alignment with our theory. In both plots 2000 runs are used to compute the mean. Error bars represent 95% confidence intervals.\n\nsparse activations at each layer can be recovered using images produced by the layer. This assumption can be somewhat strong, as it implies that the activations are roughly independent of one another, and the sparsity remains constant over layers, despite the layers themselves expanding by a factor of 4, i.e., $$d_{\\ell} \\geq 4d_{\\ell-1} \\forall \\ell \\leq L - 1$$."}, {"type": "heading", "lvl": 3, "value": "5 Simulations", "md": "### 5 Simulations"}, {"type": "text", "value": "We now numerically verify our theoretical claims and compare against other approaches. A detailed description of simulation methods are included in the appendix. Our code is available at https://github.com/basics-lab/learningGenerativeModels.git.", "md": "We now numerically verify our theoretical claims and compare against other approaches. A detailed description of simulation methods are included in the appendix. Our code is available at https://github.com/basics-lab/learningGenerativeModels.git."}, {"type": "heading", "lvl": 4, "value": "5.1 Scaling in n and k", "md": "#### 5.1 Scaling in n and k"}, {"type": "text", "value": "Figure 2 numerically investigates how TV distance of the MLE scales with the number of samples $$n$$ and the input dimension $$k$$. We consider a model with 1-dimensional output and a $$k$$-dimensional input: $$y = \\phi(x \\cdot w^* + \\eta)$$, for $$w^* \\in \\mathbb{R}^k$$ and $$\\eta \\sim N(0, \\sigma^2)$$. We set $$\\sigma^2 = 1$$ and $$w^* = 1k \\times 1$$, both unknown to the optimizer, which has samples $$(y_i, x_i)_{i=1}^n$$. Figure 2a, which plots the error in TV distance against $$n$$ on a log-log plot, has a slope of roughly $$-\\frac{1}{2}$$ as predicted by our theory. Similarly, Figure 2b has a slope of $$\\frac{1}{2}$$, which is in line with our theory on scaling with respect to input dimension $$k$$. We defer simulations involving scaling in $$d$$ to the appendix because computing TV distance becomes increasingly difficult as $$d$$ becomes large, and we must resort to using upper bounds.", "md": "Figure 2 numerically investigates how TV distance of the MLE scales with the number of samples $$n$$ and the input dimension $$k$$. We consider a model with 1-dimensional output and a $$k$$-dimensional input: $$y = \\phi(x \\cdot w^* + \\eta)$$, for $$w^* \\in \\mathbb{R}^k$$ and $$\\eta \\sim N(0, \\sigma^2)$$. We set $$\\sigma^2 = 1$$ and $$w^* = 1k \\times 1$$, both unknown to the optimizer, which has samples $$(y_i, x_i)_{i=1}^n$$. Figure 2a, which plots the error in TV distance against $$n$$ on a log-log plot, has a slope of roughly $$-\\frac{1}{2}$$ as predicted by our theory. Similarly, Figure 2b has a slope of $$\\frac{1}{2}$$, which is in line with our theory on scaling with respect to input dimension $$k$$. We defer simulations involving scaling in $$d$$ to the appendix because computing TV distance becomes increasingly difficult as $$d$$ becomes large, and we must resort to using upper bounds."}, {"type": "heading", "lvl": 4, "value": "5.2 Distribution Independence", "md": "#### 5.2 Distribution Independence"}, {"type": "text", "value": "The fact that our guarantee does not depend on the distribution of $$x$$ suggests that the expected TV error of the distribution learned from the MLE may be similar for all distributions over $$x$$. To test this we consider both $$x \\sim N(0, I_k)$$ and $$x \\sim k \\text{ Lap}(0, 1)$$, i.e., each element of $$x$$ is drawn independently standard Laplace. Figure 2a verifies our hypothesis, showing only very slight differences in our observed empirical average TV error between the two distributions over a wide range of $$n$$, and for $$k = 5$$ and $$k = 20$$.", "md": "The fact that our guarantee does not depend on the distribution of $$x$$ suggests that the expected TV error of the distribution learned from the MLE may be similar for all distributions over $$x$$. To test this we consider both $$x \\sim N(0, I_k)$$ and $$x \\sim k \\text{ Lap}(0, 1)$$, i.e., each element of $$x$$ is drawn independently standard Laplace. Figure 2a verifies our hypothesis, showing only very slight differences in our observed empirical average TV error between the two distributions over a wide range of $$n$$, and for $$k = 5$$ and $$k = 20$$."}]}, {"page": 9, "text": "                                                                                                                                        0.032\n                                                                                                                                         0.03\n        10-1                                                                                                   10-1\n                                                                                                                                        0.028\n                                                                                                                                        0.026\n        10-2                                                                                                                            0.024\n                                                                                                               10-2\n                                                                                                                                        0.022\n                                                                                                                                         0.02\n        10-3                                                                                                                            0.018\n                                                                                                               10-3                     0.016\n                                                                                                                                        0.014\n        10-4-4         -3         -2        -1          0         1          2          3          4         5                          0.012         50      100     150      200      250     300     350     400      450     500      550\n                              (a) Total Variation vs. Bias                                                                                     (b) Total Variation vs. Condition Number\nFigure 3: (a) Left hand axis shows TV distance vs. bias vector b with y = \u03d5(\u03b7 + b1d\u00d71), d = 3,\n\u03b7 = N             (0, \u03a3), and \u03a3 = Id. Note that MLE (blue) has error going to zero as bias becomes negative,\nwhile the opposite is true for the baseline (red). Right hand axis shows the mean-squared error of the\nparameters \u03a3 and the mean \u00b5, each point was run a total of 2000 times. (b) TV distance vs. condition\nnumber, d = 3. MLE does not exhibit trend with condition number, but baseline does. Error bars are\n95% confidence intervals, over 20000 runs.\n5.3            Scaling with Bias and Condition Number of Covariance Matrix\nA key feature of the MLE is that it makes use of truncated samples. This is in contrast to [35], which\nleverages results on learning truncated normal distributions [14] where truncated samples are not\nobserved. This leads to a stark difference in performance as the number of truncated samples becomes\nlarge. To show this, we consider a model with a d-dimensional output and 1-dimensional input. We\nlet x = 1 almost surely, and then take w\u2217                                                                       = b1d\u00d71 for some bias b \u2208                                                  R and \u03b7 = N                         (0, \u03a3) with\n\u03a3 = Id, thus y = \u03d5(\u03b7 + b1d\u00d71). As b becomes more negative, the number of truncated samples\nincreases. Figure 3a shows the differing behavior of MLE and that of [35] as b becomes negative. For\nease of computing the TV distance we set d = 3, and restrict optimization over diagonal \u03a3. The solid\nblue line depicts the performance of the MLE. We observe that the TV error is constant for b > 0\nand begins to decrease rapidly for b < 0. This happens because as b becomes more negative, the\ntruncation places more probability mass at y = 0. Indeed, the dashed blue lines indicate that even\nas the TV error is decreasing rapidly, the mean square estimation error of the covariance and mean\nincrease, however, since most of the probability mass is at zero, this does not significantly impact\nthe TV. In contrast, the method of [35] rapidly deteriorates as b < 1 as the number of untruncated\nsamples decreases. We also point out that even when the bias is large, [35] is still significantly worse.\nWe attribute this to the fact that even when there is no truncated samples, [35] is still minimizing a\ndifferent MAP objective. More discussion of this is provided in the appendix.\nRobustness to Condition Number of \u03a3.                                                                           Another concern is how TV error scales as a function of\nthe condition number of \u03a3\u2217. Poorly conditioned \u03a3\u2217                                                                                    can put significant probability masses on small\nsets, and potentially cause large error. We consider a similar environment to the one described above,\nbut fix b = 1 and alter diagonal entries of \u03a3\u2217                                                                    such that one entry is \u221a\u03ba, another is                                                      \u221a     \u03ba\u22121 and the rest\nare 1, making the condition number \u03ba. Figure 3b shows that the MLE is not measurably impacted by\nthe changing condition number over the range plotted. This is not true of [35], where we observe that\nTV does grow with condition number.\n6           Limitations\nThis work is only a first step to understanding fundamental limits of learning generative models.\nWe showed that our theorems for single-layer networks can be composed to get sample complexity\nbounds on deep networks, with a critical caveat: we require access to not just the input and output\npairs, but also the intermediate activations. This is not practical in many scenarios, and removing this\nrestriction will be an important direction for future research. Beyond this, we assume that the learner\nhas an understanding of the model architecture. In many cases, however, a learner may not be aware\n                                                                                                                             9", "md": "# Document\n\n## Scaling with Bias and Condition Number of Covariance Matrix\n\nA key feature of the MLE is that it makes use of truncated samples. This is in contrast to [35], which leverages results on learning truncated normal distributions [14] where truncated samples are not observed. This leads to a stark difference in performance as the number of truncated samples becomes large. To show this, we consider a model with a d-dimensional output and 1-dimensional input. We let x = 1 almost surely, and then take $$w^* = b1d\\times1$$ for some bias $$b \\in R$$ and $$\\eta = N(0, \\Sigma)$$ with $$\\Sigma = Id$$, thus $$y = \\phi(\\eta + b1d\\times1)$$. As $$b$$ becomes more negative, the number of truncated samples increases. Figure 3a shows the differing behavior of MLE and that of [35] as $$b$$ becomes negative. For ease of computing the TV distance we set $$d = 3$$, and restrict optimization over diagonal $$\\Sigma$$. The solid blue line depicts the performance of the MLE. We observe that the TV error is constant for $$b > 0$$ and begins to decrease rapidly for $$b < 0$$. This happens because as $$b$$ becomes more negative, the truncation places more probability mass at $$y = 0$$. Indeed, the dashed blue lines indicate that even as the TV error is decreasing rapidly, the mean square estimation error of the covariance and mean increase, however, since most of the probability mass is at zero, this does not significantly impact the TV. In contrast, the method of [35] rapidly deteriorates as $$b < 1$$ as the number of untruncated samples decreases. We also point out that even when the bias is large, [35] is still significantly worse. We attribute this to the fact that even when there are no truncated samples, [35] is still minimizing a different MAP objective. More discussion of this is provided in the appendix.\n\nRobustness to Condition Number of $$\\Sigma$$. Another concern is how TV error scales as a function of the condition number of $$\\Sigma^*$$ . Poorly conditioned $$\\Sigma^*$$ can put significant probability masses on small sets, and potentially cause large error. We consider a similar environment to the one described above, but fix $$b = 1$$ and alter diagonal entries of $$\\Sigma^*$$ such that one entry is $$\\sqrt{\\kappa}$$, another is $$\\sqrt{\\kappa-1}$$ and the rest are 1, making the condition number $$\\kappa$$. Figure 3b shows that the MLE is not measurably impacted by the changing condition number over the range plotted. This is not true of [35], where we observe that TV does grow with condition number.\n\n## Limitations\n\nThis work is only a first step to understanding fundamental limits of learning generative models. We showed that our theorems for single-layer networks can be composed to get sample complexity bounds on deep networks, with a critical caveat: we require access to not just the input and output pairs, but also the intermediate activations. This is not practical in many scenarios, and removing this restriction will be an important direction for future research. Beyond this, we assume that the learner has an understanding of the model architecture. In many cases, however, a learner may not be aware", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Scaling with Bias and Condition Number of Covariance Matrix", "md": "## Scaling with Bias and Condition Number of Covariance Matrix"}, {"type": "text", "value": "A key feature of the MLE is that it makes use of truncated samples. This is in contrast to [35], which leverages results on learning truncated normal distributions [14] where truncated samples are not observed. This leads to a stark difference in performance as the number of truncated samples becomes large. To show this, we consider a model with a d-dimensional output and 1-dimensional input. We let x = 1 almost surely, and then take $$w^* = b1d\\times1$$ for some bias $$b \\in R$$ and $$\\eta = N(0, \\Sigma)$$ with $$\\Sigma = Id$$, thus $$y = \\phi(\\eta + b1d\\times1)$$. As $$b$$ becomes more negative, the number of truncated samples increases. Figure 3a shows the differing behavior of MLE and that of [35] as $$b$$ becomes negative. For ease of computing the TV distance we set $$d = 3$$, and restrict optimization over diagonal $$\\Sigma$$. The solid blue line depicts the performance of the MLE. We observe that the TV error is constant for $$b > 0$$ and begins to decrease rapidly for $$b < 0$$. This happens because as $$b$$ becomes more negative, the truncation places more probability mass at $$y = 0$$. Indeed, the dashed blue lines indicate that even as the TV error is decreasing rapidly, the mean square estimation error of the covariance and mean increase, however, since most of the probability mass is at zero, this does not significantly impact the TV. In contrast, the method of [35] rapidly deteriorates as $$b < 1$$ as the number of untruncated samples decreases. We also point out that even when the bias is large, [35] is still significantly worse. We attribute this to the fact that even when there are no truncated samples, [35] is still minimizing a different MAP objective. More discussion of this is provided in the appendix.\n\nRobustness to Condition Number of $$\\Sigma$$. Another concern is how TV error scales as a function of the condition number of $$\\Sigma^*$$ . Poorly conditioned $$\\Sigma^*$$ can put significant probability masses on small sets, and potentially cause large error. We consider a similar environment to the one described above, but fix $$b = 1$$ and alter diagonal entries of $$\\Sigma^*$$ such that one entry is $$\\sqrt{\\kappa}$$, another is $$\\sqrt{\\kappa-1}$$ and the rest are 1, making the condition number $$\\kappa$$. Figure 3b shows that the MLE is not measurably impacted by the changing condition number over the range plotted. This is not true of [35], where we observe that TV does grow with condition number.", "md": "A key feature of the MLE is that it makes use of truncated samples. This is in contrast to [35], which leverages results on learning truncated normal distributions [14] where truncated samples are not observed. This leads to a stark difference in performance as the number of truncated samples becomes large. To show this, we consider a model with a d-dimensional output and 1-dimensional input. We let x = 1 almost surely, and then take $$w^* = b1d\\times1$$ for some bias $$b \\in R$$ and $$\\eta = N(0, \\Sigma)$$ with $$\\Sigma = Id$$, thus $$y = \\phi(\\eta + b1d\\times1)$$. As $$b$$ becomes more negative, the number of truncated samples increases. Figure 3a shows the differing behavior of MLE and that of [35] as $$b$$ becomes negative. For ease of computing the TV distance we set $$d = 3$$, and restrict optimization over diagonal $$\\Sigma$$. The solid blue line depicts the performance of the MLE. We observe that the TV error is constant for $$b > 0$$ and begins to decrease rapidly for $$b < 0$$. This happens because as $$b$$ becomes more negative, the truncation places more probability mass at $$y = 0$$. Indeed, the dashed blue lines indicate that even as the TV error is decreasing rapidly, the mean square estimation error of the covariance and mean increase, however, since most of the probability mass is at zero, this does not significantly impact the TV. In contrast, the method of [35] rapidly deteriorates as $$b < 1$$ as the number of untruncated samples decreases. We also point out that even when the bias is large, [35] is still significantly worse. We attribute this to the fact that even when there are no truncated samples, [35] is still minimizing a different MAP objective. More discussion of this is provided in the appendix.\n\nRobustness to Condition Number of $$\\Sigma$$. Another concern is how TV error scales as a function of the condition number of $$\\Sigma^*$$ . Poorly conditioned $$\\Sigma^*$$ can put significant probability masses on small sets, and potentially cause large error. We consider a similar environment to the one described above, but fix $$b = 1$$ and alter diagonal entries of $$\\Sigma^*$$ such that one entry is $$\\sqrt{\\kappa}$$, another is $$\\sqrt{\\kappa-1}$$ and the rest are 1, making the condition number $$\\kappa$$. Figure 3b shows that the MLE is not measurably impacted by the changing condition number over the range plotted. This is not true of [35], where we observe that TV does grow with condition number."}, {"type": "heading", "lvl": 2, "value": "Limitations", "md": "## Limitations"}, {"type": "text", "value": "This work is only a first step to understanding fundamental limits of learning generative models. We showed that our theorems for single-layer networks can be composed to get sample complexity bounds on deep networks, with a critical caveat: we require access to not just the input and output pairs, but also the intermediate activations. This is not practical in many scenarios, and removing this restriction will be an important direction for future research. Beyond this, we assume that the learner has an understanding of the model architecture. In many cases, however, a learner may not be aware", "md": "This work is only a first step to understanding fundamental limits of learning generative models. We showed that our theorems for single-layer networks can be composed to get sample complexity bounds on deep networks, with a critical caveat: we require access to not just the input and output pairs, but also the intermediate activations. This is not practical in many scenarios, and removing this restriction will be an important direction for future research. Beyond this, we assume that the learner has an understanding of the model architecture. In many cases, however, a learner may not be aware"}]}, {"page": 10, "text": "of the number of layers a network has or a vast number of other architectural details. Additionally,\nwe inherit known problems with the MLE, such as exacerbation of biases that exist in the training\ndata (Chapter 24.1.3 in [33]).\nOur results place emphasis on sample complexity over computational complexity. Though we show\nthat the MLE problem is concave, this work does not provide a thorough analysis of the optimization\nproblem. It is possible that similar results to [14, 35] can be derived for the MLE problem. Indeed,\nempirically, we find that a similar projected stochastic gradient ascent performs well in our problem.\nA careful analysis must consider factors like the distribution over x, the condition number of \u03a3\u2217  and\nthe truncation probability, all of which are likely to impact the optimization.\n7   Conclusions\nWe have studied the problem of learning conditional generative models from a limited number of\nsamples. We have shown that it is possible to learn a 1-layer ReLU conditional generative model\nin total variation, with no assumption on the distribution of the conditioning variable x using the\nMLE. We have also shown that this result can be extended to multi-layer ReLU networks, given\naccess to the internal activations. Our results suggest that MLE is a promising approach for learning\nfeed-forward generative models from limited samples.\n8   Acknowledgements\nAjil Jalal and Kannan Ramchandran are supported by ARO 051242-002. Justin Kang and Kannan\nRamchandran are supported by NSF EAGER Award 2232146. Eric Price is supported by NSF awards\nCCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine\nLearning (IFML).\nReferences\n [1] Zeyuan Allen-Zhu and Yuanzhi Li. Forward super-resolution: How can gans learn hierarchical\n     generative models for real-world distributions. arXiv preprint arXiv:2106.02619, 2021.\n [2] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou.           Wasserstein gan.    arXiv preprint\n     arXiv:1701.07875, 2017.\n [3] Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and\n     equilibrium in generative adversarial nets (gans). In International Conference on Machine\n     Learning, pages 224\u2013232. PMLR, 2017.\n [4] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms\n     for sparse coding. In Conference on learning theory, pages 113\u2013149. PMLR, 2015.\n [5] Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and\n     empirics. In International Conference on Learning Representations, 2018.\n [6] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian,\n     and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian\n     mixtures via compression schemes. Journal of the ACM (JACM), 67(6):1\u201342, 2020.\n [7] Francis Bach. Learning theory from first principles. Draft of a book, version of Sept, 6:2021,\n     2021.\n [8] St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart.            Concentration inequalities: A\n     nonasymptotic theory of independence. Oxford university press, 2013.\n [9] Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,\n     2004.\n[10] Sitan Chen, Jerry Li, and Yuanzhi Li. Learning (very) simple generative models is hard. arXiv\n     preprint arXiv:2205.16003, 2022.\n                                                  10", "md": "of the number of layers a network has or a vast number of other architectural details. Additionally, we inherit known problems with the MLE, such as exacerbation of biases that exist in the training data (Chapter 24.1.3 in [33]).\n\nOur results place emphasis on sample complexity over computational complexity. Though we show that the MLE problem is concave, this work does not provide a thorough analysis of the optimization problem. It is possible that similar results to [14, 35] can be derived for the MLE problem. Indeed, empirically, we find that a similar projected stochastic gradient ascent performs well in our problem. A careful analysis must consider factors like the distribution over x, the condition number of $$\\Sigma^*$$ and the truncation probability, all of which are likely to impact the optimization.\n\n## Conclusions\n\nWe have studied the problem of learning conditional generative models from a limited number of samples. We have shown that it is possible to learn a 1-layer ReLU conditional generative model in total variation, with no assumption on the distribution of the conditioning variable x using the MLE. We have also shown that this result can be extended to multi-layer ReLU networks, given access to the internal activations. Our results suggest that MLE is a promising approach for learning feed-forward generative models from limited samples.\n\n## Acknowledgements\n\nAjil Jalal and Kannan Ramchandran are supported by ARO 051242-002. Justin Kang and Kannan Ramchandran are supported by NSF EAGER Award 2232146. Eric Price is supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML).\n\n## References\n\n1. Zeyuan Allen-Zhu and Yuanzhi Li. Forward super-resolution: How can gans learn hierarchical generative models for real-world distributions. arXiv preprint arXiv:2106.02619, 2021.\n2. Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n3. Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In International Conference on Machine Learning, pages 224\u2013232. PMLR, 2017.\n4. Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In Conference on learning theory, pages 113\u2013149. PMLR, 2015.\n5. Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and empirics. In International Conference on Learning Representations, 2018.\n6. Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. Journal of the ACM (JACM), 67(6):1\u201342, 2020.\n7. Francis Bach. Learning theory from first principles. Draft of a book, version of Sept, 6:2021, 2021.\n8. St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.\n9. Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n10. Sitan Chen, Jerry Li, and Yuanzhi Li. Learning (very) simple generative models is hard. arXiv preprint arXiv:2205.16003, 2022.", "images": [], "items": [{"type": "text", "value": "of the number of layers a network has or a vast number of other architectural details. Additionally, we inherit known problems with the MLE, such as exacerbation of biases that exist in the training data (Chapter 24.1.3 in [33]).\n\nOur results place emphasis on sample complexity over computational complexity. Though we show that the MLE problem is concave, this work does not provide a thorough analysis of the optimization problem. It is possible that similar results to [14, 35] can be derived for the MLE problem. Indeed, empirically, we find that a similar projected stochastic gradient ascent performs well in our problem. A careful analysis must consider factors like the distribution over x, the condition number of $$\\Sigma^*$$ and the truncation probability, all of which are likely to impact the optimization.", "md": "of the number of layers a network has or a vast number of other architectural details. Additionally, we inherit known problems with the MLE, such as exacerbation of biases that exist in the training data (Chapter 24.1.3 in [33]).\n\nOur results place emphasis on sample complexity over computational complexity. Though we show that the MLE problem is concave, this work does not provide a thorough analysis of the optimization problem. It is possible that similar results to [14, 35] can be derived for the MLE problem. Indeed, empirically, we find that a similar projected stochastic gradient ascent performs well in our problem. A careful analysis must consider factors like the distribution over x, the condition number of $$\\Sigma^*$$ and the truncation probability, all of which are likely to impact the optimization."}, {"type": "heading", "lvl": 2, "value": "Conclusions", "md": "## Conclusions"}, {"type": "text", "value": "We have studied the problem of learning conditional generative models from a limited number of samples. We have shown that it is possible to learn a 1-layer ReLU conditional generative model in total variation, with no assumption on the distribution of the conditioning variable x using the MLE. We have also shown that this result can be extended to multi-layer ReLU networks, given access to the internal activations. Our results suggest that MLE is a promising approach for learning feed-forward generative models from limited samples.", "md": "We have studied the problem of learning conditional generative models from a limited number of samples. We have shown that it is possible to learn a 1-layer ReLU conditional generative model in total variation, with no assumption on the distribution of the conditioning variable x using the MLE. We have also shown that this result can be extended to multi-layer ReLU networks, given access to the internal activations. Our results suggest that MLE is a promising approach for learning feed-forward generative models from limited samples."}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "Ajil Jalal and Kannan Ramchandran are supported by ARO 051242-002. Justin Kang and Kannan Ramchandran are supported by NSF EAGER Award 2232146. Eric Price is supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML).", "md": "Ajil Jalal and Kannan Ramchandran are supported by ARO 051242-002. Justin Kang and Kannan Ramchandran are supported by NSF EAGER Award 2232146. Eric Price is supported by NSF awards CCF-2008868, CCF-1751040 (CAREER), and the NSF AI Institute for Foundations of Machine Learning (IFML)."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. Zeyuan Allen-Zhu and Yuanzhi Li. Forward super-resolution: How can gans learn hierarchical generative models for real-world distributions. arXiv preprint arXiv:2106.02619, 2021.\n2. Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n3. Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In International Conference on Machine Learning, pages 224\u2013232. PMLR, 2017.\n4. Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In Conference on learning theory, pages 113\u2013149. PMLR, 2015.\n5. Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and empirics. In International Conference on Learning Representations, 2018.\n6. Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. Journal of the ACM (JACM), 67(6):1\u201342, 2020.\n7. Francis Bach. Learning theory from first principles. Draft of a book, version of Sept, 6:2021, 2021.\n8. St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.\n9. Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n10. Sitan Chen, Jerry Li, and Yuanzhi Li. Learning (very) simple generative models is hard. arXiv preprint arXiv:2205.16003, 2022.", "md": "1. Zeyuan Allen-Zhu and Yuanzhi Li. Forward super-resolution: How can gans learn hierarchical generative models for real-world distributions. arXiv preprint arXiv:2106.02619, 2021.\n2. Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n3. Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang. Generalization and equilibrium in generative adversarial nets (gans). In International Conference on Machine Learning, pages 224\u2013232. PMLR, 2017.\n4. Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In Conference on learning theory, pages 113\u2013149. PMLR, 2015.\n5. Sanjeev Arora, Andrej Risteski, and Yi Zhang. Do gans learn the distribution? some theory and empirics. In International Conference on Learning Representations, 2018.\n6. Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. Journal of the ACM (JACM), 67(6):1\u201342, 2020.\n7. Francis Bach. Learning theory from first principles. Draft of a book, version of Sept, 6:2021, 2021.\n8. St\u00e9phane Boucheron, G\u00e1bor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymptotic theory of independence. Oxford university press, 2013.\n9. Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\n10. Sitan Chen, Jerry Li, and Yuanzhi Li. Learning (very) simple generative models is hard. arXiv preprint arXiv:2205.16003, 2022."}]}, {"page": 11, "text": "[11] Sitan Chen, Jerry Li, Yuanzhi Li, and Raghu Meka. Minimax optimality (probably) doesn\u2019t\n     imply distribution learning for gans. arXiv preprint arXiv:2201.07206, 2022.\n[12] Sitan Chen, Jerry Li, Yuanzhi Li, and Anru R Zhang. Learning polynomial transformations.\n     arXiv preprint arXiv:2204.04209, 2022.\n[13] Annie AM Cuyt, Vigdis Petersen, Brigitte Verdonk, Haakon Waadeland, and William B Jones.\n     Handbook of continued fractions for special functions. Springer Science & Business Media,\n     2008.\n[14] Constantinos Daskalakis, Themis Gouleakis, Chistos Tzamos, and Manolis Zampetakis. Ef-\n     ficient statistics, in high dimensions, from truncated samples. In 2018 IEEE 59th Annual\n     Symposium on Foundations of Computer Science (FOCS), pages 639\u2013649. IEEE, 2018.\n[15] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans\n     with optimism. arXiv preprint arXiv:1711.00141, 2017.\n[16] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between\n     high-dimensional gaussians. arXiv preprint arXiv:1810.08693, 2018.\n[17] Farzan Farnia and Asuman Ozdaglar. Do gans always have nash equilibria? In International\n     Conference on Machine Learning, pages 3029\u20133039. PMLR, 2020.\n[18] Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding gans in the lqg setting:\n     Formulation, generalization and stability. IEEE Journal on Selected Areas in Information\n     Theory, 1(1):304\u2013311, 2020.\n[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\n     Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural\n     information processing systems, pages 2672\u20132680, 2014.\n[20] L\u00e1szl\u00f3 Gy\u00f6rfi, Michael K\u00f6hler, Adam Krzy\u02d9     zak, and Harro Walk. A distribution-free theory of\n     nonparametric regression, volume 1. Springer, 2002.\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\n     Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\n     neural information processing systems, 30, 2017.\n[22] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-\n     proved quality, stability, and variation. In International Conference on Learning Representations,\n     2018.\n[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\n     adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n     Pattern Recognition, pages 4401\u20134410, 2019.\n[24] Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. Sgd learns one-layer networks\n     in wgans. In International Conference on Machine Learning, pages 5799\u20135808. PMLR, 2020.\n[25] Yuanzhi Li and Zehao Dou. Making method of moments great again?\u2013how can gans learn\n     distributions. arXiv preprint arXiv:2003.04033, 2020.\n[26] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Effi           cient estimation of word\n     representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[27] John P Mills. Table of the ratio: area to bounding ordinate, for any portion of normal curve.\n     Biometrika, pages 395\u2013400, 1926.\n[28] Aryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of o(1/k) for\n     optimistic gradient and extragradient methods in smooth convex-concave saddle point problems.\n     SIAM Journal on Optimization, 30(4):3230\u20133251, 2020.\n[29] Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable.\n     Advances in neural information processing systems, 30, 2017.\n                                                  11", "md": "- Sitan Chen, Jerry Li, Yuanzhi Li, and Raghu Meka. Minimax optimality (probably) doesn\u2019t imply distribution learning for GANs. arXiv preprint arXiv:2201.07206, 2022.\n- Sitan Chen, Jerry Li, Yuanzhi Li, and Anru R Zhang. Learning polynomial transformations. arXiv preprint arXiv:2204.04209, 2022.\n- Annie AM Cuyt, Vigdis Petersen, Brigitte Verdonk, Haakon Waadeland, and William B Jones. Handbook of continued fractions for special functions. Springer Science & Business Media, 2008.\n- Constantinos Daskalakis, Themis Gouleakis, Chistos Tzamos, and Manolis Zampetakis. Efficient statistics, in high dimensions, from truncated samples. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pages 639\u2013649. IEEE, 2018.\n- Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. arXiv preprint arXiv:1711.00141, 2017.\n- Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-dimensional Gaussians. arXiv preprint arXiv:1810.08693, 2018.\n- Farzan Farnia and Asuman Ozdaglar. Do GANs always have Nash equilibria? In International Conference on Machine Learning, pages 3029\u20133039. PMLR, 2020.\n- Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding GANs in the LQG setting: Formulation, generalization and stability. IEEE Journal on Selected Areas in Information Theory, 1(1):304\u2013311, 2020.\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.\n- L\u00e1szl\u00f3 Gy\u00f6rfi, Michael K\u00f6hler, Adam Krzy\u02d9zak, and Harro Walk. A distribution-free theory of nonparametric regression, volume 1. Springer, 2002.\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in neural information processing systems, 30, 2017.\n- Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.\n- Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4401\u20134410, 2019.\n- Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. SGD learns one-layer networks in WGANs. In International Conference on Machine Learning, pages 5799\u20135808. PMLR, 2020.\n- Yuanzhi Li and Zehao Dou. Making method of moments great again?\u2013how can GANs learn distributions. arXiv preprint arXiv:2003.04033, 2020.\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n- John P Mills. Table of the ratio: area to bounding ordinate, for any portion of normal curve. Biometrika, pages 395\u2013400, 1926.\n- Aryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of O(1/k) for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. SIAM Journal on Optimization, 30(4):3230\u20133251, 2020.\n- Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. Advances in neural information processing systems, 30, 2017.", "images": [], "items": [{"type": "text", "value": "- Sitan Chen, Jerry Li, Yuanzhi Li, and Raghu Meka. Minimax optimality (probably) doesn\u2019t imply distribution learning for GANs. arXiv preprint arXiv:2201.07206, 2022.\n- Sitan Chen, Jerry Li, Yuanzhi Li, and Anru R Zhang. Learning polynomial transformations. arXiv preprint arXiv:2204.04209, 2022.\n- Annie AM Cuyt, Vigdis Petersen, Brigitte Verdonk, Haakon Waadeland, and William B Jones. Handbook of continued fractions for special functions. Springer Science & Business Media, 2008.\n- Constantinos Daskalakis, Themis Gouleakis, Chistos Tzamos, and Manolis Zampetakis. Efficient statistics, in high dimensions, from truncated samples. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pages 639\u2013649. IEEE, 2018.\n- Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. arXiv preprint arXiv:1711.00141, 2017.\n- Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-dimensional Gaussians. arXiv preprint arXiv:1810.08693, 2018.\n- Farzan Farnia and Asuman Ozdaglar. Do GANs always have Nash equilibria? In International Conference on Machine Learning, pages 3029\u20133039. PMLR, 2020.\n- Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding GANs in the LQG setting: Formulation, generalization and stability. IEEE Journal on Selected Areas in Information Theory, 1(1):304\u2013311, 2020.\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.\n- L\u00e1szl\u00f3 Gy\u00f6rfi, Michael K\u00f6hler, Adam Krzy\u02d9zak, and Harro Walk. A distribution-free theory of nonparametric regression, volume 1. Springer, 2002.\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in neural information processing systems, 30, 2017.\n- Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.\n- Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4401\u20134410, 2019.\n- Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. SGD learns one-layer networks in WGANs. In International Conference on Machine Learning, pages 5799\u20135808. PMLR, 2020.\n- Yuanzhi Li and Zehao Dou. Making method of moments great again?\u2013how can GANs learn distributions. arXiv preprint arXiv:2003.04033, 2020.\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n- John P Mills. Table of the ratio: area to bounding ordinate, for any portion of normal curve. Biometrika, pages 395\u2013400, 1926.\n- Aryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of O(1/k) for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. SIAM Journal on Optimization, 30(4):3230\u20133251, 2020.\n- Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. Advances in neural information processing systems, 30, 2017.", "md": "- Sitan Chen, Jerry Li, Yuanzhi Li, and Raghu Meka. Minimax optimality (probably) doesn\u2019t imply distribution learning for GANs. arXiv preprint arXiv:2201.07206, 2022.\n- Sitan Chen, Jerry Li, Yuanzhi Li, and Anru R Zhang. Learning polynomial transformations. arXiv preprint arXiv:2204.04209, 2022.\n- Annie AM Cuyt, Vigdis Petersen, Brigitte Verdonk, Haakon Waadeland, and William B Jones. Handbook of continued fractions for special functions. Springer Science & Business Media, 2008.\n- Constantinos Daskalakis, Themis Gouleakis, Chistos Tzamos, and Manolis Zampetakis. Efficient statistics, in high dimensions, from truncated samples. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pages 639\u2013649. IEEE, 2018.\n- Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. arXiv preprint arXiv:1711.00141, 2017.\n- Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation distance between high-dimensional Gaussians. arXiv preprint arXiv:1810.08693, 2018.\n- Farzan Farnia and Asuman Ozdaglar. Do GANs always have Nash equilibria? In International Conference on Machine Learning, pages 3029\u20133039. PMLR, 2020.\n- Soheil Feizi, Farzan Farnia, Tony Ginart, and David Tse. Understanding GANs in the LQG setting: Formulation, generalization and stability. IEEE Journal on Selected Areas in Information Theory, 1(1):304\u2013311, 2020.\n- Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672\u20132680, 2014.\n- L\u00e1szl\u00f3 Gy\u00f6rfi, Michael K\u00f6hler, Adam Krzy\u02d9zak, and Harro Walk. A distribution-free theory of nonparametric regression, volume 1. Springer, 2002.\n- Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. Advances in neural information processing systems, 30, 2017.\n- Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In International Conference on Learning Representations, 2018.\n- Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4401\u20134410, 2019.\n- Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. SGD learns one-layer networks in WGANs. In International Conference on Machine Learning, pages 5799\u20135808. PMLR, 2020.\n- Yuanzhi Li and Zehao Dou. Making method of moments great again?\u2013how can GANs learn distributions. arXiv preprint arXiv:2003.04033, 2020.\n- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n- John P Mills. Table of the ratio: area to bounding ordinate, for any portion of normal curve. Biometrika, pages 395\u2013400, 1926.\n- Aryan Mokhtari, Asuman E Ozdaglar, and Sarath Pattathil. Convergence rate of O(1/k) for optimistic gradient and extragradient methods in smooth convex-concave saddle point problems. SIAM Journal on Optimization, 30(4):3230\u20133251, 2020.\n- Vaishnavh Nagarajan and J Zico Kolter. Gradient descent GAN optimization is locally stable. Advances in neural information processing systems, 30, 2017."}]}, {"page": 12, "text": "[30] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with\n     deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n[31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\n     Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\n     Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\n     resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\n     Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[33] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to\n     algorithms. Cambridge university press, 2014.\n[34] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\n     Liang, and Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model.\n     2023.\n[35] Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by\n     one-layer relu networks. Advances in neural information processing systems, 32, 2019.\n                                               12", "md": "- Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n- Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model. 2023.\n- Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by one-layer ReLU networks. Advances in neural information processing systems, 32, 2019.", "images": [], "items": [{"type": "text", "value": "- Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n- Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model. 2023.\n- Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by one-layer ReLU networks. Advances in neural information processing systems, 32, 2019.", "md": "- Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\n- Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n- Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n- Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.\n- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca: A strong, replicable instruction-following model. 2023.\n- Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by one-layer ReLU networks. Advances in neural information processing systems, 32, 2019."}]}, {"page": 13, "text": " A      Proofs of Linear Case\n Throughout the appendix, for ease of notation, we overload the definition of the function dT V (\u00b7, \u00b7).\nWhen inputs are random variables, it represent the TV distance between the distributions of those\n random variables.\n Lemma 4.2. Let {(xi, yi)}n              i=1 be i.i.d. random variables such that yi = xi \u00b7 w\u2217                         + N    (0, 1). Then,\n for n \u2265     k                                                           w satisfies\n             2 , with probability 1 \u2212          e\u2212\u2126(n), the MLE                    k\n                                                           d( w, w\u2217) \u2264           2n.\n The proof of this lemma requires Lemma A.1, which characterizes the distribution of the residual\n error of the MLE.\n Lemma A.1. Given y \u2208                Rn, X \u2208       Rn\u00d7k satisfying y = Xw\u2217                + \u03b7, where \u03b7 \u223c         N   (0, \u03c32In), the least\n square solution        w satisfies\n                    Xw\u2217     \u2212   X  w \u223c    N   (0, \u03c32X(XT X)\u22121XT ) \u21d2                   E[\u2225X \u02c6  w \u2212    Xw\u2217\u22252] = \u03c32k.\n Proof. The least squares solution is given by\n                                                 w\u02c6= (XT X)\u22121XT y,\n                                                     = (XT X)\u22121XT (Xw\u2217                  + \u03b7),\n                                                     = w\u2217     + (XT X)\u22121XT \u03b7.\n Multiplying on the left by X, we have\n                                               X \u02c6 w = Xw\u2217         + X(XT X)\u22121XT \u03b7.\n Since \u03b7 is i.i.d. Gaussian with variance \u03c32, we have,\n                          X(XT X)\u22121XT \u03b7 \u223c                N  (0, \u03c32X(XT X)\u22121XT X(XT X)\u22121XT )\n This implies                                        \u223c   N  (0, \u03c32X(XT X)\u22121XT )\n                                       E[\u2225X \u02c6  w \u2212     Xw\u2217\u22252] = \u03c32Tr[X(XT X)\u22121XT ],\n                                                                    = \u03c32Tr[(XT X)\u22121XT X],\n                                                                    = \u03c32k.\n Proof of Lemma 4.2. The KL divergence between two Gaussians P = N                                       (\u00b51, \u03a3) and Q = N            (\u00b52, \u03a3)\n is:\n By Pinsker\u2019s inequality, this implies    dKL(P     \u2225Q) = 1     2(\u00b51 \u2212      \u00b52)\u03a3\u22121(\u00b51 \u2212          \u00b52).\n                                dT V (P    \u2225Q) \u2264      min      1, 12    (\u00b51 \u2212     \u00b52)\u03a3\u22121(\u00b51 \u2212          \u00b52)     .\n                                                                       13", "md": "A Proofs of Linear Case\n\nThroughout the appendix, for ease of notation, we overload the definition of the function \\(d_{TV}(\\cdot, \\cdot)\\). When inputs are random variables, it represents the TV distance between the distributions of those random variables.\n\nLemma 4.2. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables such that \\(y_i = x_i \\cdot w^* + N(0, 1)\\). Then, for \\(n \\geq k^2\\), with probability \\(1 - e^{-\\Omega(n)}\\), the MLE \\(w\\) satisfies \\(d(w, w^*) \\leq 2n\\).\n\nThe proof of this lemma requires Lemma A.1, which characterizes the distribution of the residual error of the MLE.\n\nLemma A.1. Given \\(y \\in \\mathbb{R}^n\\), \\(X \\in \\mathbb{R}^{n \\times k}\\) satisfying \\(y = Xw^* + \\eta\\), where \\(\\eta \\sim N(0, \\sigma^2I_n)\\), the least square solution \\(w\\) satisfies\n\n\\[\nXw^* - Xw \\sim N(0, \\sigma^2X(X^T X)^{-1}X^T) \\Rightarrow E[\\|X \\hat{w} - Xw^*\\|^2] = \\sigma^2k.\n\\]\n\nProof. The least squares solution is given by\n\n\\[\n\\hat{w} = (X^T X)^{-1}X^T y,\n\\]\n\n\\[\n= (X^T X)^{-1}X^T (Xw^* + \\eta),\n\\]\n\n\\[\n= w^* + (X^T X)^{-1}X^T \\eta.\n\\]\n\nMultiplying on the left by \\(X\\), we have\n\n\\[\nX \\hat{w} = Xw^* + X(X^T X)^{-1}X^T \\eta.\n\\]\n\nSince \\(\\eta\\) is i.i.d. Gaussian with variance \\(\\sigma^2\\), we have,\n\n\\[\nX(X^T X)^{-1}X^T \\eta \\sim N(0, \\sigma^2X(X^T X)^{-1}X^T X(X^T X)^{-1}X^T)\n\\]\n\nThis implies\n\n\\[\n\\sim N(0, \\sigma^2X(X^T X)^{-1}X^T)\n\\]\n\n\\[\nE[\\|X \\hat{w} - Xw^*\\|^2] = \\sigma^2\\text{Tr}[X(X^T X)^{-1}X^T],\n\\]\n\n\\[\n= \\sigma^2\\text{Tr}[(X^T X)^{-1}X^T X],\n\\]\n\n\\[\n= \\sigma^2k.\n\\]\n\nProof of Lemma 4.2. The KL divergence between two Gaussians \\(P = N(\\mu_1, \\Sigma)\\) and \\(Q = N(\\mu_2, \\Sigma)\\) is:\n\nBy Pinsker\u2019s inequality, this implies\n\n\\[\nd_{KL}(P \\| Q) = \\frac{1}{2}(\\mu_1 - \\mu_2)\\Sigma^{-1}(\\mu_1 - \\mu_2).\n\\]\n\n\\(d_{TV}(P \\| Q) \\leq \\min\\left(1, \\frac{1}{2}(\\mu_1 - \\mu_2)\\Sigma^{-1}(\\mu_1 - \\mu_2)\\right).\\)", "images": [], "items": [{"type": "text", "value": "A Proofs of Linear Case\n\nThroughout the appendix, for ease of notation, we overload the definition of the function \\(d_{TV}(\\cdot, \\cdot)\\). When inputs are random variables, it represents the TV distance between the distributions of those random variables.\n\nLemma 4.2. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables such that \\(y_i = x_i \\cdot w^* + N(0, 1)\\). Then, for \\(n \\geq k^2\\), with probability \\(1 - e^{-\\Omega(n)}\\), the MLE \\(w\\) satisfies \\(d(w, w^*) \\leq 2n\\).\n\nThe proof of this lemma requires Lemma A.1, which characterizes the distribution of the residual error of the MLE.\n\nLemma A.1. Given \\(y \\in \\mathbb{R}^n\\), \\(X \\in \\mathbb{R}^{n \\times k}\\) satisfying \\(y = Xw^* + \\eta\\), where \\(\\eta \\sim N(0, \\sigma^2I_n)\\), the least square solution \\(w\\) satisfies\n\n\\[\nXw^* - Xw \\sim N(0, \\sigma^2X(X^T X)^{-1}X^T) \\Rightarrow E[\\|X \\hat{w} - Xw^*\\|^2] = \\sigma^2k.\n\\]\n\nProof. The least squares solution is given by\n\n\\[\n\\hat{w} = (X^T X)^{-1}X^T y,\n\\]\n\n\\[\n= (X^T X)^{-1}X^T (Xw^* + \\eta),\n\\]\n\n\\[\n= w^* + (X^T X)^{-1}X^T \\eta.\n\\]\n\nMultiplying on the left by \\(X\\), we have\n\n\\[\nX \\hat{w} = Xw^* + X(X^T X)^{-1}X^T \\eta.\n\\]\n\nSince \\(\\eta\\) is i.i.d. Gaussian with variance \\(\\sigma^2\\), we have,\n\n\\[\nX(X^T X)^{-1}X^T \\eta \\sim N(0, \\sigma^2X(X^T X)^{-1}X^T X(X^T X)^{-1}X^T)\n\\]\n\nThis implies\n\n\\[\n\\sim N(0, \\sigma^2X(X^T X)^{-1}X^T)\n\\]\n\n\\[\nE[\\|X \\hat{w} - Xw^*\\|^2] = \\sigma^2\\text{Tr}[X(X^T X)^{-1}X^T],\n\\]\n\n\\[\n= \\sigma^2\\text{Tr}[(X^T X)^{-1}X^T X],\n\\]\n\n\\[\n= \\sigma^2k.\n\\]\n\nProof of Lemma 4.2. The KL divergence between two Gaussians \\(P = N(\\mu_1, \\Sigma)\\) and \\(Q = N(\\mu_2, \\Sigma)\\) is:\n\nBy Pinsker\u2019s inequality, this implies\n\n\\[\nd_{KL}(P \\| Q) = \\frac{1}{2}(\\mu_1 - \\mu_2)\\Sigma^{-1}(\\mu_1 - \\mu_2).\n\\]\n\n\\(d_{TV}(P \\| Q) \\leq \\min\\left(1, \\frac{1}{2}(\\mu_1 - \\mu_2)\\Sigma^{-1}(\\mu_1 - \\mu_2)\\right).\\)", "md": "A Proofs of Linear Case\n\nThroughout the appendix, for ease of notation, we overload the definition of the function \\(d_{TV}(\\cdot, \\cdot)\\). When inputs are random variables, it represents the TV distance between the distributions of those random variables.\n\nLemma 4.2. Let \\(\\{(x_i, y_i)\\}_{i=1}^n\\) be i.i.d. random variables such that \\(y_i = x_i \\cdot w^* + N(0, 1)\\). Then, for \\(n \\geq k^2\\), with probability \\(1 - e^{-\\Omega(n)}\\), the MLE \\(w\\) satisfies \\(d(w, w^*) \\leq 2n\\).\n\nThe proof of this lemma requires Lemma A.1, which characterizes the distribution of the residual error of the MLE.\n\nLemma A.1. Given \\(y \\in \\mathbb{R}^n\\), \\(X \\in \\mathbb{R}^{n \\times k}\\) satisfying \\(y = Xw^* + \\eta\\), where \\(\\eta \\sim N(0, \\sigma^2I_n)\\), the least square solution \\(w\\) satisfies\n\n\\[\nXw^* - Xw \\sim N(0, \\sigma^2X(X^T X)^{-1}X^T) \\Rightarrow E[\\|X \\hat{w} - Xw^*\\|^2] = \\sigma^2k.\n\\]\n\nProof. The least squares solution is given by\n\n\\[\n\\hat{w} = (X^T X)^{-1}X^T y,\n\\]\n\n\\[\n= (X^T X)^{-1}X^T (Xw^* + \\eta),\n\\]\n\n\\[\n= w^* + (X^T X)^{-1}X^T \\eta.\n\\]\n\nMultiplying on the left by \\(X\\), we have\n\n\\[\nX \\hat{w} = Xw^* + X(X^T X)^{-1}X^T \\eta.\n\\]\n\nSince \\(\\eta\\) is i.i.d. Gaussian with variance \\(\\sigma^2\\), we have,\n\n\\[\nX(X^T X)^{-1}X^T \\eta \\sim N(0, \\sigma^2X(X^T X)^{-1}X^T X(X^T X)^{-1}X^T)\n\\]\n\nThis implies\n\n\\[\n\\sim N(0, \\sigma^2X(X^T X)^{-1}X^T)\n\\]\n\n\\[\nE[\\|X \\hat{w} - Xw^*\\|^2] = \\sigma^2\\text{Tr}[X(X^T X)^{-1}X^T],\n\\]\n\n\\[\n= \\sigma^2\\text{Tr}[(X^T X)^{-1}X^T X],\n\\]\n\n\\[\n= \\sigma^2k.\n\\]\n\nProof of Lemma 4.2. The KL divergence between two Gaussians \\(P = N(\\mu_1, \\Sigma)\\) and \\(Q = N(\\mu_2, \\Sigma)\\) is:\n\nBy Pinsker\u2019s inequality, this implies\n\n\\[\nd_{KL}(P \\| Q) = \\frac{1}{2}(\\mu_1 - \\mu_2)\\Sigma^{-1}(\\mu_1 - \\mu_2).\n\\]\n\n\\(d_{TV}(P \\| Q) \\leq \\min\\left(1, \\frac{1}{2}(\\mu_1 - \\mu_2)\\Sigma^{-1}(\\mu_1 - \\mu_2)\\right).\\)"}]}, {"page": 14, "text": "Hence, the empirical TV on the dataset can be bounded by\n                  1        dT V (p  w(y|xi), pw\u2217(y|xi)) \u2264              1       min      1, 1    xTi ( w \u2212    w\u2217)       ,\n                  n    i                                              n     i               2           \u03c3\n                                                                  \u2264       1        min      1, 1    xTi ( w \u2212    w\u2217)     2   ,\n                                                                          n     i               2           \u03c3\n                                                                  \u2264    min         1, 14n     i    xT i (w \u2212\u03c32  w\u2217)    2     ,\n                                                                  =       min     1, 1      1         w \u2212    w\u2217)\u22252       .\nwhere the second line follows from Jensen\u2019s inequality.                               4n   \u03c32 \u2225X(\nBy Lemma A.1, we have                               E[\u2225X(     w \u2212    w\u2217)\u22252] = \u03c32k.\nwhich implies that with probability 1 \u2212                  e\u2212\u2126(n), we have\nSubstituting in the earlier inequality, we get       \u2225X(    w \u2212    w\u2217)\u22252 \u2264       2\u03c32k.\n                   1        dT V (p  w(y|xi), pw\u2217(y|xi)) \u2264                 min     1, k        =        k\n                   n    i                                                              2n              2n for n \u2265        k2 .\nLemma 4.3. Let {xi}n             i=1 be i.i.d. random variables such that xi \u223c                        Dx. For a sufficiently large\nconstant C > 0, and for n = C k               \u03b52 log 1 \u03b5 with n \u2265        k2, we have:\n                                  Pr        sup    d(w, w\u2217) \u2212        d(w, w\u2217)        > \u03b5     \u2264   e\u2212\u2126(n\u03b52).\n                                xi\u223cDx      w\u2208Rk\nProof. The proof is inspired by Theorem 11.2 in [20], with modifications to our setting.\nLet Since fw(x) is bounded, for any fixed w, the Chernoff bound gives\n                                       Pr     d(w, w\u2217) \u2212        d(w, w\u2217)       > \u03b1      \u2264   e\u22122n\u03b12.                                      (11)\nfor any \u03b1 > 0. The challenge lies in constructing a \u201cnet\u201d to be able to union bound over Rk without\nassuming any bound on w or the covariate x. A net is a partitioning of an space, where within each\npart, points are close together in some way. In this case, we construct a net using what we will refer\nto as \u201cghost\u201d samples.\nGhost samples.             First, we construct a \u201cghost\u201d dataset D\u2032                   x consisting of n new samples, drawn\ni.i.d. {x\u2032 i}i\u2208[n] of Dx. This gives another metric                   d\u2032(\u00b7, \u00b7). Instead of directly considering the distance\nbetween      d(w, w\u2217) and d(w, w\u2217), it is sufficient to consider the difference between                                    d(w, w\u2217) and\nd\u2032(w, w\u2217) i.e.,\n            Pr    sup   d(w, w\u2217) \u2212        d(w, w\u2217)        > \u03b5     \u2264   2 Pr    sup    d(w, w\u2217) \u2212       d\u2032(w, w\u2217)        > \u03b5/2      .      (12)\n                   w                                                            w\nTo see this, let \u00af    w maximize         d(w, w\u2217) \u2212        d\u2032(w, w\u2217). Since \u00af       w and {x\u2032     i}i\u2208[n] are independent, by the\nChernoff bound,\n                                   d\u2032( \u00afw, w\u2217) \u2212      d( \u00af\n                             Pr                          w, w\u2217)       > \u03b5/2|Dx          \u2264  e\u2212n\u03b52/2 \u2264        1/2.\n                                                                      14", "md": "Hence, the empirical TV on the dataset can be bounded by\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i} d_{TV}\\left(p_{w}(y|\\mathbf{x}_i), p_{w^*}(y|\\mathbf{x}_i)\\right) \\leq \\frac{1}{n} \\min\\left\\{1, \\frac{1}{2} \\mathbf{x}_i^T ( \\mathbf{w} - \\mathbf{w}^*)\\right\\}, \\\\\n&\\leq \\frac{1}{n} \\min\\left\\{1, \\frac{1}{2} \\mathbf{x}_i^T ( \\mathbf{w} - \\mathbf{w}^*)\\right\\}^2, \\\\\n&\\leq \\min\\left\\{1, \\frac{1}{4n} \\sum_{i} \\mathbf{x}_i^T (\\mathbf{w} - \\sigma^2 \\mathbf{w}^*)^2\\right\\}, \\\\\n&= \\min\\left\\{1, \\frac{1}{2} \\|\\mathbf{w} - \\mathbf{w}^*\\|^2\\right\\}.\n\\end{align*}\n$$\nwhere the second line follows from Jensen\u2019s inequality.\n\nBy Lemma A.1, we have\n\n$$\nE[\\|\\mathbf{X}(\\mathbf{w} - \\mathbf{w}^*)\\|^2] = \\sigma^2k.\n$$\nwhich implies that with probability $1 - e^{-\\Omega(n)}$, we have\n\nSubstituting in the earlier inequality, we get\n\n$$\n\\|\\mathbf{X}(\\mathbf{w} - \\mathbf{w}^*)\\|^2 \\leq 2\\sigma^2k.\n$$\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i} d_{TV}\\left(p_{w}(y|\\mathbf{x}_i), p_{w^*}(y|\\mathbf{x}_i)\\right) \\leq \\min\\left\\{1, k\\right\\} = k \\\\\n&\\text{for } n \\geq k^2.\n\\end{align*}\n$$\n\nLemma 4.3. Let $\\{x_i\\}_{i=1}^n$ be i.i.d. random variables such that $x_i \\sim D_x$. For a sufficiently large constant $C > 0$, and for $n = C k \\epsilon^2 \\log \\frac{1}{\\epsilon}$ with $n \\geq k^2$, we have:\n\n$$\n\\begin{align*}\n&\\Pr\\left(\\sup_{x_i \\sim D_x} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{x_i \\sim D_x} d(\\mathbf{w}, \\mathbf{w}^*) > \\epsilon\\right) \\leq e^{-\\Omega(n\\epsilon^2)}.\n\\end{align*}\n$$\nProof. The proof is inspired by Theorem 11.2 in [20], with modifications to our setting.\n\nLet Since $f_w(x)$ is bounded, for any fixed $w$, the Chernoff bound gives\n\n$$\n\\Pr\\left(d(\\mathbf{w}, \\mathbf{w}^*) - d(\\mathbf{w}, \\mathbf{w}^*) > \\alpha\\right) \\leq e^{-2n\\alpha^2}. \\quad (11)\n$$\nfor any $\\alpha > 0$. The challenge lies in constructing a \u201cnet\u201d to be able to union bound over $\\mathbb{R}^k$ without assuming any bound on $w$ or the covariate $x$. A net is a partitioning of an space, where within each part, points are close together in some way. In this case, we construct a net using what we will refer to as \u201cghost\u201d samples.\n\nGhost samples. First, we construct a \u201cghost\u201d dataset $D'_x$ consisting of $n$ new samples, drawn i.i.d. $\\{x'_i\\}_{i=1}^n$ of $D_x$. This gives another metric $d'(\\cdot, \\cdot)$. Instead of directly considering the distance between $d(\\mathbf{w}, \\mathbf{w}^*)$ and $d(\\mathbf{w}, \\mathbf{w}^*)$, it is sufficient to consider the difference between $d(\\mathbf{w}, \\mathbf{w}^*)$ and $d'(\\mathbf{w}, \\mathbf{w}^*)$ i.e.,\n\n$$\n\\Pr\\left(\\sup_{\\mathbf{w}} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{\\mathbf{w}} d'(\\mathbf{w}, \\mathbf{w}^*) > \\epsilon\\right) \\leq 2 \\Pr\\left(\\sup_{\\mathbf{w}} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{\\mathbf{w}} d'(\\mathbf{w}, \\mathbf{w}^*) > \\frac{\\epsilon}{2}\\right). \\quad (12)\n$$\nTo see this, let $\\bar{w}$ maximize $d(\\mathbf{w}, \\mathbf{w}^*) - d'(\\mathbf{w}, \\mathbf{w}^*)$. Since $\\bar{w}$ and $\\{x'_i\\}_{i=1}^n$ are independent, by the Chernoff bound,\n\n$$\n\\Pr\\left(d'(\\bar{w}, \\mathbf{w}^*) - d(\\bar{w}, \\mathbf{w}^*) > \\frac{\\epsilon}{2} | D_x\\right) \\leq e^{-n\\epsilon^2/2} \\leq \\frac{1}{2}.\n$$", "images": [], "items": [{"type": "text", "value": "Hence, the empirical TV on the dataset can be bounded by\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i} d_{TV}\\left(p_{w}(y|\\mathbf{x}_i), p_{w^*}(y|\\mathbf{x}_i)\\right) \\leq \\frac{1}{n} \\min\\left\\{1, \\frac{1}{2} \\mathbf{x}_i^T ( \\mathbf{w} - \\mathbf{w}^*)\\right\\}, \\\\\n&\\leq \\frac{1}{n} \\min\\left\\{1, \\frac{1}{2} \\mathbf{x}_i^T ( \\mathbf{w} - \\mathbf{w}^*)\\right\\}^2, \\\\\n&\\leq \\min\\left\\{1, \\frac{1}{4n} \\sum_{i} \\mathbf{x}_i^T (\\mathbf{w} - \\sigma^2 \\mathbf{w}^*)^2\\right\\}, \\\\\n&= \\min\\left\\{1, \\frac{1}{2} \\|\\mathbf{w} - \\mathbf{w}^*\\|^2\\right\\}.\n\\end{align*}\n$$\nwhere the second line follows from Jensen\u2019s inequality.\n\nBy Lemma A.1, we have\n\n$$\nE[\\|\\mathbf{X}(\\mathbf{w} - \\mathbf{w}^*)\\|^2] = \\sigma^2k.\n$$\nwhich implies that with probability $1 - e^{-\\Omega(n)}$, we have\n\nSubstituting in the earlier inequality, we get\n\n$$\n\\|\\mathbf{X}(\\mathbf{w} - \\mathbf{w}^*)\\|^2 \\leq 2\\sigma^2k.\n$$\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i} d_{TV}\\left(p_{w}(y|\\mathbf{x}_i), p_{w^*}(y|\\mathbf{x}_i)\\right) \\leq \\min\\left\\{1, k\\right\\} = k \\\\\n&\\text{for } n \\geq k^2.\n\\end{align*}\n$$\n\nLemma 4.3. Let $\\{x_i\\}_{i=1}^n$ be i.i.d. random variables such that $x_i \\sim D_x$. For a sufficiently large constant $C > 0$, and for $n = C k \\epsilon^2 \\log \\frac{1}{\\epsilon}$ with $n \\geq k^2$, we have:\n\n$$\n\\begin{align*}\n&\\Pr\\left(\\sup_{x_i \\sim D_x} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{x_i \\sim D_x} d(\\mathbf{w}, \\mathbf{w}^*) > \\epsilon\\right) \\leq e^{-\\Omega(n\\epsilon^2)}.\n\\end{align*}\n$$\nProof. The proof is inspired by Theorem 11.2 in [20], with modifications to our setting.\n\nLet Since $f_w(x)$ is bounded, for any fixed $w$, the Chernoff bound gives\n\n$$\n\\Pr\\left(d(\\mathbf{w}, \\mathbf{w}^*) - d(\\mathbf{w}, \\mathbf{w}^*) > \\alpha\\right) \\leq e^{-2n\\alpha^2}. \\quad (11)\n$$\nfor any $\\alpha > 0$. The challenge lies in constructing a \u201cnet\u201d to be able to union bound over $\\mathbb{R}^k$ without assuming any bound on $w$ or the covariate $x$. A net is a partitioning of an space, where within each part, points are close together in some way. In this case, we construct a net using what we will refer to as \u201cghost\u201d samples.\n\nGhost samples. First, we construct a \u201cghost\u201d dataset $D'_x$ consisting of $n$ new samples, drawn i.i.d. $\\{x'_i\\}_{i=1}^n$ of $D_x$. This gives another metric $d'(\\cdot, \\cdot)$. Instead of directly considering the distance between $d(\\mathbf{w}, \\mathbf{w}^*)$ and $d(\\mathbf{w}, \\mathbf{w}^*)$, it is sufficient to consider the difference between $d(\\mathbf{w}, \\mathbf{w}^*)$ and $d'(\\mathbf{w}, \\mathbf{w}^*)$ i.e.,\n\n$$\n\\Pr\\left(\\sup_{\\mathbf{w}} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{\\mathbf{w}} d'(\\mathbf{w}, \\mathbf{w}^*) > \\epsilon\\right) \\leq 2 \\Pr\\left(\\sup_{\\mathbf{w}} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{\\mathbf{w}} d'(\\mathbf{w}, \\mathbf{w}^*) > \\frac{\\epsilon}{2}\\right). \\quad (12)\n$$\nTo see this, let $\\bar{w}$ maximize $d(\\mathbf{w}, \\mathbf{w}^*) - d'(\\mathbf{w}, \\mathbf{w}^*)$. Since $\\bar{w}$ and $\\{x'_i\\}_{i=1}^n$ are independent, by the Chernoff bound,\n\n$$\n\\Pr\\left(d'(\\bar{w}, \\mathbf{w}^*) - d(\\bar{w}, \\mathbf{w}^*) > \\frac{\\epsilon}{2} | D_x\\right) \\leq e^{-n\\epsilon^2/2} \\leq \\frac{1}{2}.\n$$", "md": "Hence, the empirical TV on the dataset can be bounded by\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i} d_{TV}\\left(p_{w}(y|\\mathbf{x}_i), p_{w^*}(y|\\mathbf{x}_i)\\right) \\leq \\frac{1}{n} \\min\\left\\{1, \\frac{1}{2} \\mathbf{x}_i^T ( \\mathbf{w} - \\mathbf{w}^*)\\right\\}, \\\\\n&\\leq \\frac{1}{n} \\min\\left\\{1, \\frac{1}{2} \\mathbf{x}_i^T ( \\mathbf{w} - \\mathbf{w}^*)\\right\\}^2, \\\\\n&\\leq \\min\\left\\{1, \\frac{1}{4n} \\sum_{i} \\mathbf{x}_i^T (\\mathbf{w} - \\sigma^2 \\mathbf{w}^*)^2\\right\\}, \\\\\n&= \\min\\left\\{1, \\frac{1}{2} \\|\\mathbf{w} - \\mathbf{w}^*\\|^2\\right\\}.\n\\end{align*}\n$$\nwhere the second line follows from Jensen\u2019s inequality.\n\nBy Lemma A.1, we have\n\n$$\nE[\\|\\mathbf{X}(\\mathbf{w} - \\mathbf{w}^*)\\|^2] = \\sigma^2k.\n$$\nwhich implies that with probability $1 - e^{-\\Omega(n)}$, we have\n\nSubstituting in the earlier inequality, we get\n\n$$\n\\|\\mathbf{X}(\\mathbf{w} - \\mathbf{w}^*)\\|^2 \\leq 2\\sigma^2k.\n$$\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i} d_{TV}\\left(p_{w}(y|\\mathbf{x}_i), p_{w^*}(y|\\mathbf{x}_i)\\right) \\leq \\min\\left\\{1, k\\right\\} = k \\\\\n&\\text{for } n \\geq k^2.\n\\end{align*}\n$$\n\nLemma 4.3. Let $\\{x_i\\}_{i=1}^n$ be i.i.d. random variables such that $x_i \\sim D_x$. For a sufficiently large constant $C > 0$, and for $n = C k \\epsilon^2 \\log \\frac{1}{\\epsilon}$ with $n \\geq k^2$, we have:\n\n$$\n\\begin{align*}\n&\\Pr\\left(\\sup_{x_i \\sim D_x} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{x_i \\sim D_x} d(\\mathbf{w}, \\mathbf{w}^*) > \\epsilon\\right) \\leq e^{-\\Omega(n\\epsilon^2)}.\n\\end{align*}\n$$\nProof. The proof is inspired by Theorem 11.2 in [20], with modifications to our setting.\n\nLet Since $f_w(x)$ is bounded, for any fixed $w$, the Chernoff bound gives\n\n$$\n\\Pr\\left(d(\\mathbf{w}, \\mathbf{w}^*) - d(\\mathbf{w}, \\mathbf{w}^*) > \\alpha\\right) \\leq e^{-2n\\alpha^2}. \\quad (11)\n$$\nfor any $\\alpha > 0$. The challenge lies in constructing a \u201cnet\u201d to be able to union bound over $\\mathbb{R}^k$ without assuming any bound on $w$ or the covariate $x$. A net is a partitioning of an space, where within each part, points are close together in some way. In this case, we construct a net using what we will refer to as \u201cghost\u201d samples.\n\nGhost samples. First, we construct a \u201cghost\u201d dataset $D'_x$ consisting of $n$ new samples, drawn i.i.d. $\\{x'_i\\}_{i=1}^n$ of $D_x$. This gives another metric $d'(\\cdot, \\cdot)$. Instead of directly considering the distance between $d(\\mathbf{w}, \\mathbf{w}^*)$ and $d(\\mathbf{w}, \\mathbf{w}^*)$, it is sufficient to consider the difference between $d(\\mathbf{w}, \\mathbf{w}^*)$ and $d'(\\mathbf{w}, \\mathbf{w}^*)$ i.e.,\n\n$$\n\\Pr\\left(\\sup_{\\mathbf{w}} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{\\mathbf{w}} d'(\\mathbf{w}, \\mathbf{w}^*) > \\epsilon\\right) \\leq 2 \\Pr\\left(\\sup_{\\mathbf{w}} d(\\mathbf{w}, \\mathbf{w}^*) - \\sup_{\\mathbf{w}} d'(\\mathbf{w}, \\mathbf{w}^*) > \\frac{\\epsilon}{2}\\right). \\quad (12)\n$$\nTo see this, let $\\bar{w}$ maximize $d(\\mathbf{w}, \\mathbf{w}^*) - d'(\\mathbf{w}, \\mathbf{w}^*)$. Since $\\bar{w}$ and $\\{x'_i\\}_{i=1}^n$ are independent, by the Chernoff bound,\n\n$$\n\\Pr\\left(d'(\\bar{w}, \\mathbf{w}^*) - d(\\bar{w}, \\mathbf{w}^*) > \\frac{\\epsilon}{2} | D_x\\right) \\leq e^{-n\\epsilon^2/2} \\leq \\frac{1}{2}.\n$$"}]}, {"page": 15, "text": " for any (Dx, \u00af     w) and large enough n. Thus,\n Pr    d\u2032( \u00afw, w\u2217) \u2212      d( \u00af\n                             w, w\u2217)                  \u2265   Pr        w, w\u2217) \u2212       d( \u00af\n                                                                                     w, w\u2217)                    w, w\u2217) \u2212       d\u2032( \u00af\n                                          > \u03b5/2      = E       1d( \u00af                             > \u03b5 \u2229      d( \u00af                  w, w\u2217)      < \u03b5/2\n                                                                 {|d( \u00af                                    w, w\u2217) \u2212       d\u2032( \u00af\n                                                                      w,w\u2217)\u2212    d( \u00af                    d( \u00af                  w, w\u2217)      < \u03b5/2|Dx\n                                                         Dx                        w,w\u2217)|>\u03b5} Pr\n which implies (12).                                 \u2265   (1 \u2212    1/2) Pr       d(w, w\u2217) \u2212        d(w, w\u2217)       > \u03b5     ,\n Symmetrization.              Since Dx and D\u2032         x each have n independent samples, we could instead draw the\n datasets by first sampling 2n elements x1, . . . , x2n from Dx, then randomly partition this sample into\n two equal datasets. Let si \u2208             {\u00b11} so si = 1 if zi lies in D\u2032            x and \u22121 if it lies in Dx. Then\n                           d\u2032( \u00af                                    2n\n                               w, w\u2217) \u2212      d( \u00afw, w\u2217) = 1     n  i=1   si \u00b7 dT V (pw(y|xi), pw\u2217(y|xi)).\n For a fixed w and x1, . . . , x2n, the random variables (s1, . . . , s2n) are a permutation distribution, so\n negatively associated. Then the variables si \u00b7 dT V (pw(y|xi), pw\u2217(y|xi)) are monotone functions of\n si, so also negatively associated. They are also bounded in [\u22121, 1]. Hence we can apply a Chernoff\n bound:                                 Pr     d\u2032( \u00af\n                                                   w, w\u2217) \u2212       d( \u00af\n for any fixed w.                                                    w, w\u2217)      > \u03b5     < e\u2212n\u03b52/2                                        (13)\n Constructing a net.              We partition Rk the space of w s.t. if w, w\u2032 are in the same partition then,\n                             dT V (pw(y|x), pw\u2217(y|x)) \u2212               dT V (pw\u2032(y|x), pw\u2217(y|x))              < \u03b1.\n for each x in the dataset x1, . . . , x2n. Then take the intersection of all 2n partitions to construct a net\n over Rk.\n As the total variation distance is a unimodal function of xi \u00b7 w \u2212                            xi \u00b7 w\u2217, we partition w the sets\n                                     {w : dT V (pw(y|xi), pw\u2217(y|xi)) \u2208                 [j\u03b1, (j + 1)\u03b1]\n where j goes from 0 to 1/\u03b1 \u2212                  1. So the space of w, Rk is partitioned by 2n sets of 1/\u03b1 parallel\n hyper-planes. Then the total number of cells is at most\n                                                    k    2n i    (2/\u03b1)i \u2264       2 4en\u03b1k     k\n                                                  i=0\n                                                                                                                                             n\nWe define a net N by choosing one representative of each cell in the partition, so |N| \u2264                                          e2k log   \u03b1k .\n By (13),                                                                                                       n\n                   Pr    max     d\u2032( \u00af                                                                         \u03b1k \u2212\u03b52n/2.\n                                     w, w\u2217) \u2212       d( \u00af\n                         w\u2208N                           w, w\u2217)      > \u03b5      < |N|e\u2212n\u03b52/2 \u2264           e2k log\n Finally, for any w \u2208          Rd let \u00af w \u2208     N be the representative of its cell. By definition of the cells,\n for all i \u2208    [2n]. Thus |dT V (pw(y|xi), pw\u2217(y|xi)) \u2212               dT V (p \u00af w(y|xi), pw\u2217(y|xi))| < \u03b1\n    d\u2032(w, w\u2217) \u2212        d(w, w\u2217)        \u2212     d\u2032( \u00af\n                                                 w, w\u2217) \u2212       d( \u00af\n                                                                   w, w\u2217             d(w, w\u2217) \u2212        d( \u00af\n and so                                                                         \u2264                         w, w\u2217)      +    d\u2032(w, w\u2217) \u2212       d\u2032( \u00afw, w\u2217)    \u2264  2\u03b1\n                                                                                                                                             n\n Pr    sup     d\u2032(w, w\u2217) \u2212        d(w, w\u2217)       > \u03b5      \u2264   Pr   max     d\u2032(w, w\u2217) \u2212        d(w, w\u2217)       > \u03b5 \u2212     2\u03b1     \u2264   e2k log   \u03b1k \u2212(\u03b5\u22122\u03b1)2n/2\n      w\u2208Rd                                                         w\u2208N\n Setting \u03b1 = \u03b5/4, we have that\n                                                              n \u2272     1\n                                                                     \u03b52 k log 1  \u03b5\n suffices for                       Pr    max     d\u2032(w, w\u2217) \u2212       d(w, w\u2217) > \u03b5           < e\u2212\u2126(\u03b52n).\n                                          w\u2208Rk                         15", "md": "For any $$(Dx, \\bar{w})$$ and large enough n. Thus,\n\n$$\\begin{aligned} \\text{Pr} & d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) \\geq \\text{Pr} w, w^*) - d(\\bar{w}, w^*) \\\\ & > \\epsilon/2 = E 1_{d(\\bar{w}, w^*) > \\epsilon} \\cap \\{d(\\bar{w}, w^*) - d'(\\bar{w}, w^*) < \\epsilon/2|Dx \\in Dx, w, w^*)| > \\epsilon\\} \\end{aligned}$$\n\nwhich implies (12).\n\nSymmetrization. Since Dx and D'x each have n independent samples, we could instead draw the datasets by first sampling 2n elements $$x_1, ..., x_{2n}$$ from Dx, then randomly partition this sample into two equal datasets. Let $$s_i \\in \\{\u00b11\\}$$ so $$s_i = 1$$ if $$z_i$$ lies in D'x and $$-1$$ if it lies in Dx. Then\n\n$$d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) = \\frac{1}{n} \\sum_{i=1}^{n} s_i \\cdot d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)).$$\n\nFor a fixed w and $$x_1, ..., x_{2n}$$, the random variables $$(s_1, ..., s_{2n})$$ are a permutation distribution, so negatively associated. Then the variables $$s_i \\cdot d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i))$$ are monotone functions of $$s_i$$, so also negatively associated. They are also bounded in [-1, 1]. Hence we can apply a Chernoff bound:\n\n$$\\text{Pr} d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) > \\epsilon < e^{-n\\epsilon^2/2}$$ (13)\n\nConstructing a net. We partition $$\\mathbb{R}^k$$ the space of w s.t. if w, w' are in the same partition then,\n\n$$d_{TV}(p_w(y|x), p_{w^*}(y|x)) - d_{TV}(p_{w'}(y|x), p_{w^*}(y|x)) < \\alpha$$\n\nfor each x in the dataset $$x_1, ..., x_{2n}$$. Then take the intersection of all 2n partitions to construct a net over $$\\mathbb{R}^k$$.\n\nAs the total variation distance is a unimodal function of $$x_i \\cdot w - x_i \\cdot w^*$$, we partition w the sets\n\n$$\\{w : d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)) \\in [j\\alpha, (j + 1)\\alpha]\\}$$\n\nwhere j goes from 0 to $$1/\\alpha - 1$$. So the space of w, $$\\mathbb{R}^k$$ is partitioned by 2n sets of $$1/\\alpha$$ parallel hyper-planes. Then the total number of cells is at most\n\n$$\\sum_{i=0}^{k} 2^n i (2/\\alpha)^i \\leq 2^{4en\\alpha k}$$\n\nWe define a net N by choosing one representative of each cell in the partition, so $$|N| \\leq e^{2k \\log \\alpha k}$$. By (13),\n\n$$\\begin{aligned} \\text{Pr} & \\max_{w \\in N} d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) > \\epsilon \\\\ & < |N|e^{-n\\epsilon^2/2} \\leq e^{2k \\log \\alpha k - \\epsilon^2n/2} \\end{aligned}$$\n\nFinally, for any $$w \\in \\mathbb{R}^d$$ let $$\\bar{w} \\in N$$ be the representative of its cell. By definition of the cells, for all $$i \\in [2n]$$, thus $$|d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)) - d_{TV}(p_{\\bar{w}}(y|x_i), p_{w^*}(y|x_i))| < \\alpha$$\n\n$$\\begin{aligned} & d'(w, w^*) - d(w, w^*) - d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) \\\\ & \\leq d(w, w^*) + d'(w, w^*) - d'( \\bar{w}, w^*) \\leq 2\\alpha/n \\end{aligned}$$\n\n$$\\begin{aligned} \\text{Pr} & \\sup_{w \\in \\mathbb{R}^d} d'(w, w^*) - d(w, w^*) > \\epsilon \\\\ & \\leq \\text{Pr} \\max_{w \\in N} d'(w, w^*) - d(w, w^*) > \\epsilon - 2\\alpha \\\\ & \\leq e^{2k \\log \\alpha k - (\\epsilon - 2\\alpha)^2n/2} \\end{aligned}$$\n\nSetting $$\\alpha = \\epsilon/4$$, we have that\n\n$$n \\lesssim \\frac{1}{\\epsilon^2 k \\log \\frac{1}{\\epsilon}}$$\n\nsuffices for $$\\text{Pr} \\max_{w \\in \\mathbb{R}^k} d'(w, w^*) - d(w, w^*) > \\epsilon < e^{-\\Omega(\\epsilon^2n)}.$$", "images": [], "items": [{"type": "text", "value": "For any $$(Dx, \\bar{w})$$ and large enough n. Thus,\n\n$$\\begin{aligned} \\text{Pr} & d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) \\geq \\text{Pr} w, w^*) - d(\\bar{w}, w^*) \\\\ & > \\epsilon/2 = E 1_{d(\\bar{w}, w^*) > \\epsilon} \\cap \\{d(\\bar{w}, w^*) - d'(\\bar{w}, w^*) < \\epsilon/2|Dx \\in Dx, w, w^*)| > \\epsilon\\} \\end{aligned}$$\n\nwhich implies (12).\n\nSymmetrization. Since Dx and D'x each have n independent samples, we could instead draw the datasets by first sampling 2n elements $$x_1, ..., x_{2n}$$ from Dx, then randomly partition this sample into two equal datasets. Let $$s_i \\in \\{\u00b11\\}$$ so $$s_i = 1$$ if $$z_i$$ lies in D'x and $$-1$$ if it lies in Dx. Then\n\n$$d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) = \\frac{1}{n} \\sum_{i=1}^{n} s_i \\cdot d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)).$$\n\nFor a fixed w and $$x_1, ..., x_{2n}$$, the random variables $$(s_1, ..., s_{2n})$$ are a permutation distribution, so negatively associated. Then the variables $$s_i \\cdot d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i))$$ are monotone functions of $$s_i$$, so also negatively associated. They are also bounded in [-1, 1]. Hence we can apply a Chernoff bound:\n\n$$\\text{Pr} d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) > \\epsilon < e^{-n\\epsilon^2/2}$$ (13)\n\nConstructing a net. We partition $$\\mathbb{R}^k$$ the space of w s.t. if w, w' are in the same partition then,\n\n$$d_{TV}(p_w(y|x), p_{w^*}(y|x)) - d_{TV}(p_{w'}(y|x), p_{w^*}(y|x)) < \\alpha$$\n\nfor each x in the dataset $$x_1, ..., x_{2n}$$. Then take the intersection of all 2n partitions to construct a net over $$\\mathbb{R}^k$$.\n\nAs the total variation distance is a unimodal function of $$x_i \\cdot w - x_i \\cdot w^*$$, we partition w the sets\n\n$$\\{w : d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)) \\in [j\\alpha, (j + 1)\\alpha]\\}$$\n\nwhere j goes from 0 to $$1/\\alpha - 1$$. So the space of w, $$\\mathbb{R}^k$$ is partitioned by 2n sets of $$1/\\alpha$$ parallel hyper-planes. Then the total number of cells is at most\n\n$$\\sum_{i=0}^{k} 2^n i (2/\\alpha)^i \\leq 2^{4en\\alpha k}$$\n\nWe define a net N by choosing one representative of each cell in the partition, so $$|N| \\leq e^{2k \\log \\alpha k}$$. By (13),\n\n$$\\begin{aligned} \\text{Pr} & \\max_{w \\in N} d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) > \\epsilon \\\\ & < |N|e^{-n\\epsilon^2/2} \\leq e^{2k \\log \\alpha k - \\epsilon^2n/2} \\end{aligned}$$\n\nFinally, for any $$w \\in \\mathbb{R}^d$$ let $$\\bar{w} \\in N$$ be the representative of its cell. By definition of the cells, for all $$i \\in [2n]$$, thus $$|d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)) - d_{TV}(p_{\\bar{w}}(y|x_i), p_{w^*}(y|x_i))| < \\alpha$$\n\n$$\\begin{aligned} & d'(w, w^*) - d(w, w^*) - d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) \\\\ & \\leq d(w, w^*) + d'(w, w^*) - d'( \\bar{w}, w^*) \\leq 2\\alpha/n \\end{aligned}$$\n\n$$\\begin{aligned} \\text{Pr} & \\sup_{w \\in \\mathbb{R}^d} d'(w, w^*) - d(w, w^*) > \\epsilon \\\\ & \\leq \\text{Pr} \\max_{w \\in N} d'(w, w^*) - d(w, w^*) > \\epsilon - 2\\alpha \\\\ & \\leq e^{2k \\log \\alpha k - (\\epsilon - 2\\alpha)^2n/2} \\end{aligned}$$\n\nSetting $$\\alpha = \\epsilon/4$$, we have that\n\n$$n \\lesssim \\frac{1}{\\epsilon^2 k \\log \\frac{1}{\\epsilon}}$$\n\nsuffices for $$\\text{Pr} \\max_{w \\in \\mathbb{R}^k} d'(w, w^*) - d(w, w^*) > \\epsilon < e^{-\\Omega(\\epsilon^2n)}.$$", "md": "For any $$(Dx, \\bar{w})$$ and large enough n. Thus,\n\n$$\\begin{aligned} \\text{Pr} & d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) \\geq \\text{Pr} w, w^*) - d(\\bar{w}, w^*) \\\\ & > \\epsilon/2 = E 1_{d(\\bar{w}, w^*) > \\epsilon} \\cap \\{d(\\bar{w}, w^*) - d'(\\bar{w}, w^*) < \\epsilon/2|Dx \\in Dx, w, w^*)| > \\epsilon\\} \\end{aligned}$$\n\nwhich implies (12).\n\nSymmetrization. Since Dx and D'x each have n independent samples, we could instead draw the datasets by first sampling 2n elements $$x_1, ..., x_{2n}$$ from Dx, then randomly partition this sample into two equal datasets. Let $$s_i \\in \\{\u00b11\\}$$ so $$s_i = 1$$ if $$z_i$$ lies in D'x and $$-1$$ if it lies in Dx. Then\n\n$$d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) = \\frac{1}{n} \\sum_{i=1}^{n} s_i \\cdot d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)).$$\n\nFor a fixed w and $$x_1, ..., x_{2n}$$, the random variables $$(s_1, ..., s_{2n})$$ are a permutation distribution, so negatively associated. Then the variables $$s_i \\cdot d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i))$$ are monotone functions of $$s_i$$, so also negatively associated. They are also bounded in [-1, 1]. Hence we can apply a Chernoff bound:\n\n$$\\text{Pr} d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) > \\epsilon < e^{-n\\epsilon^2/2}$$ (13)\n\nConstructing a net. We partition $$\\mathbb{R}^k$$ the space of w s.t. if w, w' are in the same partition then,\n\n$$d_{TV}(p_w(y|x), p_{w^*}(y|x)) - d_{TV}(p_{w'}(y|x), p_{w^*}(y|x)) < \\alpha$$\n\nfor each x in the dataset $$x_1, ..., x_{2n}$$. Then take the intersection of all 2n partitions to construct a net over $$\\mathbb{R}^k$$.\n\nAs the total variation distance is a unimodal function of $$x_i \\cdot w - x_i \\cdot w^*$$, we partition w the sets\n\n$$\\{w : d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)) \\in [j\\alpha, (j + 1)\\alpha]\\}$$\n\nwhere j goes from 0 to $$1/\\alpha - 1$$. So the space of w, $$\\mathbb{R}^k$$ is partitioned by 2n sets of $$1/\\alpha$$ parallel hyper-planes. Then the total number of cells is at most\n\n$$\\sum_{i=0}^{k} 2^n i (2/\\alpha)^i \\leq 2^{4en\\alpha k}$$\n\nWe define a net N by choosing one representative of each cell in the partition, so $$|N| \\leq e^{2k \\log \\alpha k}$$. By (13),\n\n$$\\begin{aligned} \\text{Pr} & \\max_{w \\in N} d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) > \\epsilon \\\\ & < |N|e^{-n\\epsilon^2/2} \\leq e^{2k \\log \\alpha k - \\epsilon^2n/2} \\end{aligned}$$\n\nFinally, for any $$w \\in \\mathbb{R}^d$$ let $$\\bar{w} \\in N$$ be the representative of its cell. By definition of the cells, for all $$i \\in [2n]$$, thus $$|d_{TV}(p_w(y|x_i), p_{w^*}(y|x_i)) - d_{TV}(p_{\\bar{w}}(y|x_i), p_{w^*}(y|x_i))| < \\alpha$$\n\n$$\\begin{aligned} & d'(w, w^*) - d(w, w^*) - d'(\\bar{w}, w^*) - d(\\bar{w}, w^*) \\\\ & \\leq d(w, w^*) + d'(w, w^*) - d'( \\bar{w}, w^*) \\leq 2\\alpha/n \\end{aligned}$$\n\n$$\\begin{aligned} \\text{Pr} & \\sup_{w \\in \\mathbb{R}^d} d'(w, w^*) - d(w, w^*) > \\epsilon \\\\ & \\leq \\text{Pr} \\max_{w \\in N} d'(w, w^*) - d(w, w^*) > \\epsilon - 2\\alpha \\\\ & \\leq e^{2k \\log \\alpha k - (\\epsilon - 2\\alpha)^2n/2} \\end{aligned}$$\n\nSetting $$\\alpha = \\epsilon/4$$, we have that\n\n$$n \\lesssim \\frac{1}{\\epsilon^2 k \\log \\frac{1}{\\epsilon}}$$\n\nsuffices for $$\\text{Pr} \\max_{w \\in \\mathbb{R}^k} d'(w, w^*) - d(w, w^*) > \\epsilon < e^{-\\Omega(\\epsilon^2n)}.$$"}]}, {"page": 16, "text": "B       ReLU Activation with Scalar y\nIn this section, we consider the model of\n                                                   y = \u03d5(w\u2217         \u00b7 x + \u03b7),          \u03b7 \u223c    N   (0, 1),\nwhere w\u2217, x \u2208           Rk, y, \u03b7 \u2208        R. We are given samples (x, y) \u2208                       Rk \u00d7 R, and want to estimate a                      w that\nestimates the distribution of y in TV.\nThe most challenging aspect of the ReLU setting is that we do not have an expression for the TV\nsuffered by the MLE, such as Lemma 4.2 in the linear case. This forces us to directly analyze the\nlog-likelihood.\nFor a fixed x, w,, the expectation of the log-likelihood ratio over y is\n                                  E    log pw(y | x)              = \u2212dKL(w\u2217\u2225w) \u2264                   \u22122d2   T V (w\u2217, w),\n                                  y          p w\u2217(y | x)\nwhere the last inequality is via Pinsker\u2019s inequality. This equation implies that if w is \u03b5-far from w\u2217,\nthen the expected log-likelihood ratio(LLR) is < \u22122\u03b52. By definition, the MLE has a non-negative\nLLR. Hence, if the empirical LLR is close to the expectation, this would imply that the MLE has\nsmall TV.\nHowever, we only receive a single sample of y per x. For a fixed w, we can prove a Bernstein\ninequality, showing that given 1/\u03b52 log(1/\u03b4) samples, the empirical LLR is < \u2212\u03b52 for w that are\n\u03b5-far.\nLemma B.1. Let p1, . . . , pn and q1, . . . , qn be distributions with Ei[dT V (pi, qi)] \u2265                                            \u03b5, where we use\nthe uniform measure on i \u2208                    [n]. Let xi \u223c         pi for i \u2208      [n]. Then w.p. 1 \u2212             \u03b4, Ei[log qi(xi)\nn \u2265     O     1             .                                                                                                     pi(xi)] \u2264       \u2212\u03b52 4 for\n             \u03b52 log 1   \u03b4\nThe proof of this Lemma, as well as other Lemmas in this section, can be found in Appendix B.1.\nIn order to extend this to all w \u2208                    Rk that are \u03b5-far, we will construct a cover over Rk depending on\nthe values the log-likelihood ratio can take, and then apply the Bernstein inequality to each element\nin the cover.\nIn order to construct the cover, we first show that the log-likelihood ratio is bounded above by the\nmagnitude of noise in y. For ease of notation, for a fixed x \u2208                                    Rk, and each w \u2208              Rk, define\nand let \u03b8\u2217       = \u27e8x, w\u2217\u27e9.Similar to the notation for w, for each\u03b8\u03b8=\u2208\u27e8x, w\u27e9         \u2208   R,             R, define p\u03b8 as the distribution of\n\u03d5(\u03b8 + \u03b7) for \u03b7 \u223c            N(0, 1). Define the log likelihood ratio\n                                                             \u03b3\u03b8(y) := log p\u03b8(y|x) p\u03b8\u2217(y|x).\nThe following Lemma states that for a fixed datapoint (x, y), the log-likelihood ratio is bounded by\nthe noise in y:\nLemma B.2. For any y = \u03d5(\u03b8 + \u03b7),          \u03b3\u03b8(y) =        log \u03a6(\u2212\u03b8) \u2212             log \u03a6(\u2212\u03b8\u2217)             if y = 0\n                                                           \u03b7(\u03b8 \u2212      \u03b8\u2217) \u2212      (\u03b8\u2212\u03b8\u2217)2                if y > 0\n                                                                                      2\nand therefore, for all y,                                          \u03b3\u03b8(y) \u2264       |\u03b7|2/2.\nNow, as \u03b3 is bounded above by |\u03b7|2                  2 , and it is concave wrt \u03b8, the following Lemma shows that we can\npartition \u03b8 into O          A \u03b5     intervals, such that in each interval, \u03b3 changes by atmost \u03b5, or is very negative,\ni.e., \u03b3 < \u2212A.\nLemma B.3 (One-dimensional net). Let A > B2 > 1. There exists a partition of R into O(A/\u03b5)\nintervals such that, for each interval I in the partition and every y = \u03d5(\u03b8\u2217                                          + \u03b7) with |\u03b7| \u2264           B, one of\nthe following holds:\n                                                                             16", "md": "# Math Equations and Lemmas\n\nReLU Activation with Scalar y\n\nIn this section, we consider the model of\n\n$$ y = \\phi(w^* \\cdot x + \\eta), \\quad \\eta \\sim \\mathcal{N}(0, 1), $$\nwhere $w^*, x \\in \\mathbb{R}^k$, $y, \\eta \\in \\mathbb{R}$. We are given samples $(x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}$, and want to estimate a $w$ that estimates the distribution of $y$ in TV.\n\nThe most challenging aspect of the ReLU setting is that we do not have an expression for the TV suffered by the MLE, such as Lemma 4.2 in the linear case. This forces us to directly analyze the log-likelihood.\n\nFor a fixed $x, w$, the expectation of the log-likelihood ratio over $y$ is\n\n$$ E[\\log p_w(y | x)] = -d_{KL}(w^* \\| w) \\leq -2d^2 \\text{TV}(w^*, w), $$\nwhere the last inequality is via Pinsker\u2019s inequality. This equation implies that if $w$ is $\\epsilon$-far from $w^*$, then the expected log-likelihood ratio (LLR) is $< -2\\epsilon^2$. By definition, the MLE has a non-negative LLR. Hence, if the empirical LLR is close to the expectation, this would imply that the MLE has small TV.\n\nHowever, we only receive a single sample of $y$ per $x$. For a fixed $w$, we can prove a Bernstein inequality, showing that given $\\frac{1}{\\epsilon^2} \\log\\left(\\frac{1}{\\delta}\\right)$ samples, the empirical LLR is $< -\\epsilon^2$ for $w$ that are $\\epsilon$-far.\n\nLemma B.1. Let $p_1, ..., p_n$ and $q_1, ..., q_n$ be distributions with $E_i[d_{TV}(p_i, q_i)] \\geq \\epsilon$, where we use the uniform measure on $i \\in [n]$. Let $x_i \\sim p_i$ for $i \\in [n]$. Then w.p. $1 - \\delta$, $E_i[\\log q_i(x_i) - \\log p_i(x_i)] \\leq -\\frac{\\epsilon^2}{4}$ for $\\frac{\\epsilon^2 \\log(1/\\delta)}{4}$.\n\nThe proof of this Lemma, as well as other Lemmas in this section, can be found in Appendix B.1.\n\nIn order to extend this to all $w \\in \\mathbb{R}^k$ that are $\\epsilon$-far, we will construct a cover over $\\mathbb{R}^k$ depending on the values the log-likelihood ratio can take, and then apply the Bernstein inequality to each element in the cover.\n\nIn order to construct the cover, we first show that the log-likelihood ratio is bounded above by the magnitude of noise in $y$. For ease of notation, for a fixed $x \\in \\mathbb{R}^k$, and each $w \\in \\mathbb{R}^k$, define and let $\\theta^* = \\langle x, w^* \\rangle$. Similar to the notation for $w$, for each $\\theta \\in \\langle x, w \\rangle \\in \\mathbb{R}$, define $p_{\\theta}$ as the distribution of $\\phi(\\theta + \\eta)$ for $\\eta \\sim \\mathcal{N}(0, 1)$. Define the log likelihood ratio\n\n$$ \\gamma_{\\theta}(y) := \\log p_{\\theta}(y|x) - \\log p_{\\theta^*}(y|x). $$\nThe following Lemma states that for a fixed datapoint $(x, y)$, the log-likelihood ratio is bounded by the noise in $y$:\n\nLemma B.2. For any $y = \\phi(\\theta + \\eta)$,\n\n$$ \\gamma_{\\theta}(y) = \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases} $$\nand therefore, for all $y$, $\\gamma_{\\theta}(y) \\leq |\\eta|^2/2$.\n\nNow, as $\\gamma$ is bounded above by $|\\eta|^2/2$, and it is concave with respect to $\\theta$, the following Lemma shows that we can partition $\\theta$ into $O(\\frac{A}{\\epsilon})$ intervals, such that in each interval, $\\gamma$ changes by at most $\\epsilon$, or is very negative, i.e., $\\gamma < -A$.\n\nLemma B.3 (One-dimensional net). Let $A > B^2 > 1$. There exists a partition of $\\mathbb{R}$ into $O(A/\\epsilon)$ intervals such that, for each interval $I$ in the partition and every $y = \\phi(\\theta^* + \\eta)$ with $|\\eta| \\leq B$, one of the following holds:", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Lemmas", "md": "# Math Equations and Lemmas"}, {"type": "text", "value": "ReLU Activation with Scalar y\n\nIn this section, we consider the model of\n\n$$ y = \\phi(w^* \\cdot x + \\eta), \\quad \\eta \\sim \\mathcal{N}(0, 1), $$\nwhere $w^*, x \\in \\mathbb{R}^k$, $y, \\eta \\in \\mathbb{R}$. We are given samples $(x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}$, and want to estimate a $w$ that estimates the distribution of $y$ in TV.\n\nThe most challenging aspect of the ReLU setting is that we do not have an expression for the TV suffered by the MLE, such as Lemma 4.2 in the linear case. This forces us to directly analyze the log-likelihood.\n\nFor a fixed $x, w$, the expectation of the log-likelihood ratio over $y$ is\n\n$$ E[\\log p_w(y | x)] = -d_{KL}(w^* \\| w) \\leq -2d^2 \\text{TV}(w^*, w), $$\nwhere the last inequality is via Pinsker\u2019s inequality. This equation implies that if $w$ is $\\epsilon$-far from $w^*$, then the expected log-likelihood ratio (LLR) is $< -2\\epsilon^2$. By definition, the MLE has a non-negative LLR. Hence, if the empirical LLR is close to the expectation, this would imply that the MLE has small TV.\n\nHowever, we only receive a single sample of $y$ per $x$. For a fixed $w$, we can prove a Bernstein inequality, showing that given $\\frac{1}{\\epsilon^2} \\log\\left(\\frac{1}{\\delta}\\right)$ samples, the empirical LLR is $< -\\epsilon^2$ for $w$ that are $\\epsilon$-far.\n\nLemma B.1. Let $p_1, ..., p_n$ and $q_1, ..., q_n$ be distributions with $E_i[d_{TV}(p_i, q_i)] \\geq \\epsilon$, where we use the uniform measure on $i \\in [n]$. Let $x_i \\sim p_i$ for $i \\in [n]$. Then w.p. $1 - \\delta$, $E_i[\\log q_i(x_i) - \\log p_i(x_i)] \\leq -\\frac{\\epsilon^2}{4}$ for $\\frac{\\epsilon^2 \\log(1/\\delta)}{4}$.\n\nThe proof of this Lemma, as well as other Lemmas in this section, can be found in Appendix B.1.\n\nIn order to extend this to all $w \\in \\mathbb{R}^k$ that are $\\epsilon$-far, we will construct a cover over $\\mathbb{R}^k$ depending on the values the log-likelihood ratio can take, and then apply the Bernstein inequality to each element in the cover.\n\nIn order to construct the cover, we first show that the log-likelihood ratio is bounded above by the magnitude of noise in $y$. For ease of notation, for a fixed $x \\in \\mathbb{R}^k$, and each $w \\in \\mathbb{R}^k$, define and let $\\theta^* = \\langle x, w^* \\rangle$. Similar to the notation for $w$, for each $\\theta \\in \\langle x, w \\rangle \\in \\mathbb{R}$, define $p_{\\theta}$ as the distribution of $\\phi(\\theta + \\eta)$ for $\\eta \\sim \\mathcal{N}(0, 1)$. Define the log likelihood ratio\n\n$$ \\gamma_{\\theta}(y) := \\log p_{\\theta}(y|x) - \\log p_{\\theta^*}(y|x). $$\nThe following Lemma states that for a fixed datapoint $(x, y)$, the log-likelihood ratio is bounded by the noise in $y$:\n\nLemma B.2. For any $y = \\phi(\\theta + \\eta)$,\n\n$$ \\gamma_{\\theta}(y) = \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases} $$\nand therefore, for all $y$, $\\gamma_{\\theta}(y) \\leq |\\eta|^2/2$.\n\nNow, as $\\gamma$ is bounded above by $|\\eta|^2/2$, and it is concave with respect to $\\theta$, the following Lemma shows that we can partition $\\theta$ into $O(\\frac{A}{\\epsilon})$ intervals, such that in each interval, $\\gamma$ changes by at most $\\epsilon$, or is very negative, i.e., $\\gamma < -A$.\n\nLemma B.3 (One-dimensional net). Let $A > B^2 > 1$. There exists a partition of $\\mathbb{R}$ into $O(A/\\epsilon)$ intervals such that, for each interval $I$ in the partition and every $y = \\phi(\\theta^* + \\eta)$ with $|\\eta| \\leq B$, one of the following holds:", "md": "ReLU Activation with Scalar y\n\nIn this section, we consider the model of\n\n$$ y = \\phi(w^* \\cdot x + \\eta), \\quad \\eta \\sim \\mathcal{N}(0, 1), $$\nwhere $w^*, x \\in \\mathbb{R}^k$, $y, \\eta \\in \\mathbb{R}$. We are given samples $(x, y) \\in \\mathbb{R}^k \\times \\mathbb{R}$, and want to estimate a $w$ that estimates the distribution of $y$ in TV.\n\nThe most challenging aspect of the ReLU setting is that we do not have an expression for the TV suffered by the MLE, such as Lemma 4.2 in the linear case. This forces us to directly analyze the log-likelihood.\n\nFor a fixed $x, w$, the expectation of the log-likelihood ratio over $y$ is\n\n$$ E[\\log p_w(y | x)] = -d_{KL}(w^* \\| w) \\leq -2d^2 \\text{TV}(w^*, w), $$\nwhere the last inequality is via Pinsker\u2019s inequality. This equation implies that if $w$ is $\\epsilon$-far from $w^*$, then the expected log-likelihood ratio (LLR) is $< -2\\epsilon^2$. By definition, the MLE has a non-negative LLR. Hence, if the empirical LLR is close to the expectation, this would imply that the MLE has small TV.\n\nHowever, we only receive a single sample of $y$ per $x$. For a fixed $w$, we can prove a Bernstein inequality, showing that given $\\frac{1}{\\epsilon^2} \\log\\left(\\frac{1}{\\delta}\\right)$ samples, the empirical LLR is $< -\\epsilon^2$ for $w$ that are $\\epsilon$-far.\n\nLemma B.1. Let $p_1, ..., p_n$ and $q_1, ..., q_n$ be distributions with $E_i[d_{TV}(p_i, q_i)] \\geq \\epsilon$, where we use the uniform measure on $i \\in [n]$. Let $x_i \\sim p_i$ for $i \\in [n]$. Then w.p. $1 - \\delta$, $E_i[\\log q_i(x_i) - \\log p_i(x_i)] \\leq -\\frac{\\epsilon^2}{4}$ for $\\frac{\\epsilon^2 \\log(1/\\delta)}{4}$.\n\nThe proof of this Lemma, as well as other Lemmas in this section, can be found in Appendix B.1.\n\nIn order to extend this to all $w \\in \\mathbb{R}^k$ that are $\\epsilon$-far, we will construct a cover over $\\mathbb{R}^k$ depending on the values the log-likelihood ratio can take, and then apply the Bernstein inequality to each element in the cover.\n\nIn order to construct the cover, we first show that the log-likelihood ratio is bounded above by the magnitude of noise in $y$. For ease of notation, for a fixed $x \\in \\mathbb{R}^k$, and each $w \\in \\mathbb{R}^k$, define and let $\\theta^* = \\langle x, w^* \\rangle$. Similar to the notation for $w$, for each $\\theta \\in \\langle x, w \\rangle \\in \\mathbb{R}$, define $p_{\\theta}$ as the distribution of $\\phi(\\theta + \\eta)$ for $\\eta \\sim \\mathcal{N}(0, 1)$. Define the log likelihood ratio\n\n$$ \\gamma_{\\theta}(y) := \\log p_{\\theta}(y|x) - \\log p_{\\theta^*}(y|x). $$\nThe following Lemma states that for a fixed datapoint $(x, y)$, the log-likelihood ratio is bounded by the noise in $y$:\n\nLemma B.2. For any $y = \\phi(\\theta + \\eta)$,\n\n$$ \\gamma_{\\theta}(y) = \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases} $$\nand therefore, for all $y$, $\\gamma_{\\theta}(y) \\leq |\\eta|^2/2$.\n\nNow, as $\\gamma$ is bounded above by $|\\eta|^2/2$, and it is concave with respect to $\\theta$, the following Lemma shows that we can partition $\\theta$ into $O(\\frac{A}{\\epsilon})$ intervals, such that in each interval, $\\gamma$ changes by at most $\\epsilon$, or is very negative, i.e., $\\gamma < -A$.\n\nLemma B.3 (One-dimensional net). Let $A > B^2 > 1$. There exists a partition of $\\mathbb{R}$ into $O(A/\\epsilon)$ intervals such that, for each interval $I$ in the partition and every $y = \\phi(\\theta^* + \\eta)$ with $|\\eta| \\leq B$, one of the following holds:"}]}, {"page": 17, "text": "           \u2022 For all \u03b8 \u2208         I, \u03b3\u03b8(y) \u2264        \u2212A\n           \u2022 For all \u03b8, \u03b8\u2032 \u2208          I, |\u03b3\u03b8(y) \u2212        \u03b3\u03b8\u2032(y)| \u2264       \u03b5\nUsing Lemma B.2 and Lemma B.3, we can form a uniform bound, such that all w that are \u03b5-far from\nw\u2217    in distribution will have log-likelihood ratio smaller than \u2212\u03b52                                    4 on the training set. With some\nadditional arguments, we can now show that as the MLE has positive log-likelihood ratio, it has small\nempirical TV.\nLemma B.4. Let x1, . . . , xn be fixed, and yi \u223c                            \u03d5(xT                                                                 1\nthe MLE        w satisfies                                          d( w, w\u2217) \u2264   i w\u2217  \u03b5.+ \u03b7i) for \u03b7i \u223c          N   (0, 1). For n \u2265           \u03b52 k log 1   \u03b5 ,\nThis sample complexity guarantees that the MLE is good for the set of empirical xi \u223c                                                        Dx, and we\nneed to extend this to the expectation over x \u223c                            Dx, for which we use a Lemma similar to Lemma 4.3\nin the linear case, and this completes the proof.\nA straight forward combination of Lemma 4.3 and Lemma B.4 gives the following Theorem.\nTheorem B.5. Let y = \u03d5                        xT w\u2217     + \u03b7     , for w\u2217        \u2208   Rk, x \u223c          Dx, and \u03b7 \u223c             N   (0, 1). Then for a\nsufficiently large constant C > 0,\n                                                                 n = C \u00b7 k      \u03b52 log 1   \u03b5\nsamples of {(yi, xi)}n            i=1 suffices to guarantee that the MLE                        w satisfies\nB.1       Proofs                                                    d( w, w\u2217) \u2264         \u03b5.\nLemma B.1. Let p1, . . . , pn and q1, . . . , qn be distributions with Ei[dT V (pi, qi)] \u2265                                            \u03b5, where we use\nthe uniform measure on i \u2208                    [n]. Let xi \u223c         pi for i \u2208      [n]. Then w.p. 1 \u2212             \u03b4, Ei[log qi(xi)\nn \u2265     O     1             .                                                                                                     pi(xi)] \u2264       \u2212\u03b52 4 for\n             \u03b52 log 1   \u03b4\nProof. Define \u03b3i(x) = log qi(x)E            pi(x) and ai(x) := max(\u03b3i(x), \u22122). We have that\n                              i,x[\u03b3i(x)] = \u2212          Ei [dKL(pi, qi)] \u2264           \u2212   Ei [2dT V (pi, qi)2] \u2264           \u22122\u03b52\nand want to show that Ei[\u03b3i(x)] \u2264                          \u2212\u03b52/4 with high probability. Note that ai(x) \u2265                                    \u03b3i(x), so it\nsuffices to show Ei[ai(x)] \u2264                   \u2212\u03b52/4. We will do this with Bernstein\u2019s inequality, for which we need\nbounds on the moments of ai(x).\nTo simplify notation, fix a particular i and consider p = pi, q = qi, a = ai, and x \u223c                                                 p.\nFor a random variable v, define v+, v\u2212                              to be the positive/negative parts of v, respectively, so\nv = v\u2212       + v+. Define \u2206(x) = q(x)         E   p(x) \u2212     1. We have that Ex\u223cp[\u2206(x)] = 0, and                                                        (14)\n                                            x\u223cp[\u2206+(x)] = E           x\u223cp[\u2212\u2206\u2212(x)] = dT V (p, q).\nNow, consider the function b(z) := max(log(1 + z), \u22122) \u2212                                            z. This function is nonpositive over\nz \u2265    \u22121, and b(z) \u2264            \u2212z2/2 for z \u2264            0. Since\n                                                           a(x) = b(\u2206(x)) + \u2206(x)\nand Ex\u223cp[\u2206(x)] = 0, Ex\u223cp[\u2212a(x)] = Ex\u223cp[\u2212b(\u2206(x))]. This means\n                                   E\n                                 x\u223cp[\u2212a(x)] = E          x\u223cp[\u2212b(\u2206(x))] \u2265                 E\n                                                     \u2265     E        \u2212(x)/2]            x\u223cp[\u2212b(\u2206(x))1\u2206(x)<0]\nor by (14),                                              x\u223cp[\u22062\n                                             E                           \u2212(x)/2] \u2265          1                                                           (15)\n                                              x[\u2212a(x)] \u2265          Ex[\u22062                     2dT V (p, q)2.\n                                                                             17", "md": "- For all \\( \\theta \\in I \\), \\( \\gamma_{\\theta}(y) \\leq -A \\)\n- For all \\( \\theta, \\theta' \\in I \\), \\( |\\gamma_{\\theta}(y) - \\gamma_{\\theta'}(y)| \\leq \\epsilon \\)\n\nUsing Lemma B.2 and Lemma B.3, we can form a uniform bound, such that all \\( w \\) that are \\( \\epsilon \\)-far from \\( w^* \\) in distribution will have log-likelihood ratio smaller than \\( -\\epsilon^2 \\) on the training set. With some additional arguments, we can now show that as the MLE has a positive log-likelihood ratio, it has a small empirical TV.\n\nLemma B.4. Let \\( x_1, ..., x_n \\) be fixed, and \\( y_i \\sim \\phi(x^T_1 w) \\), the MLE \\( w \\) satisfies \\( d(w, w^*) \\leq \\frac{i w^* \\epsilon}{\\sqrt{n}} + \\eta_i \\) for \\( \\eta_i \\sim N(0, 1) \\). For \\( n \\geq \\frac{\\epsilon^2 k \\log \\frac{1}{\\epsilon}}{\\sqrt{n}} \\), this sample complexity guarantees that the MLE is good for the set of empirical \\( x_i \\sim D_x \\), and we need to extend this to the expectation over \\( x \\sim D_x \\), for which we use a Lemma similar to Lemma 4.3 in the linear case, and this completes the proof.\n\nA straightforward combination of Lemma 4.3 and Lemma B.4 gives the following Theorem.\n\nTheorem B.5. Let \\( y = \\phi(x^T w^* + \\eta) \\), for \\( w^* \\in \\mathbb{R}^k \\), \\( x \\sim D_x \\), and \\( \\eta \\sim N(0, 1) \\). Then for a sufficiently large constant \\( C > 0 \\),\n\n\\[\nn = C \\cdot k \\frac{\\epsilon^2 \\log \\frac{1}{\\epsilon}}{\\sqrt{n}}\n\\]\n\nsamples of \\( \\{(y_i, x_i)\\}_{n i=1} \\) suffices to guarantee that the MLE \\( w \\) satisfies\n\nB.1 Proofs\n\nLemma B.1. Let \\( p_1, ..., p_n \\) and \\( q_1, ..., q_n \\) be distributions with \\( E_i[dTV(p_i, q_i)] \\geq \\epsilon \\), where we use the uniform measure on \\( i \\in [n] \\). Let \\( x_i \\sim p_i \\) for \\( i \\in [n] \\). Then with probability \\( 1 - \\delta \\),\n\n\\[\nE_i[\\log \\frac{q_i(x_i)}{p_i(x_i)}] \\leq -\\frac{\\epsilon^2}{4}\n\\]\n\nProof. Define \\( \\gamma_i(x) = \\log \\frac{q_i(x)}{p_i(x)} \\) and \\( a_i(x) := \\max(\\gamma_i(x), -2) \\). We have that\n\n\\[\nE_{i,x}[\\gamma_i(x)] = -E_i[dKL(p_i, q_i)] \\leq -E_i[2dTV(p_i, q_i)^2] \\leq -2\\epsilon^2\n\\]\n\nand want to show that \\( E_i[\\gamma_i(x)] \\leq -\\frac{\\epsilon^2}{4} \\) with high probability. Note that \\( a_i(x) \\geq \\gamma_i(x) \\), so it suffices to show \\( E_i[a_i(x)] \\leq -\\frac{\\epsilon^2}{4} \\). We will do this with Bernstein\u2019s inequality, for which we need bounds on the moments of \\( a_i(x) \\).\n\nTo simplify notation, fix a particular \\( i \\) and consider \\( p = p_i \\), \\( q = q_i \\), \\( a = a_i \\), and \\( x \\sim p \\).\n\nFor a random variable \\( v \\), define \\( v^+, v^- \\) to be the positive/negative parts of \\( v \\), respectively, so \\( v = v^- + v^+ \\). Define \\( \\Delta(x) = q(x) - E[p(x)] - 1 \\). We have that \\( E_{x \\sim p}[\\Delta(x)] = 0 \\), and\n\n\\[\nE_{x \\sim p}[\\Delta^+(x)] = E_{x \\sim p}[-\\Delta^-(x)] = dTV(p, q)\n\\]\n\nNow, consider the function \\( b(z) := \\max(\\log(1 + z), -2) - z \\). This function is nonpositive over \\( z \\geq -1 \\), and \\( b(z) \\leq -\\frac{z^2}{2} \\) for \\( z \\leq 0 \\). Since\n\n\\[\na(x) = b(\\Delta(x)) + \\Delta(x)\n\\]\n\nand \\( E_{x \\sim p}[\\Delta(x)] = 0 \\),\n\n\\[\nE_{x \\sim p}[-a(x)] = E_{x \\sim p}[-b(\\Delta(x))]\n\\]\n\nThis means\n\n\\[\nE_{x \\sim p}[-a(x)] \\geq E_{x \\sim p}[-b(\\Delta(x))] \\geq E_{x \\sim p}[-b(\\Delta(x))1_{\\Delta(x)<0}]\n\\]\n\nor by (14),\n\n\\[\nE_{x}[-a(x)] \\geq E_{x}[\\Delta^2] \\geq 2dTV(p, q)^2\n\\]", "images": [], "items": [{"type": "text", "value": "- For all \\( \\theta \\in I \\), \\( \\gamma_{\\theta}(y) \\leq -A \\)\n- For all \\( \\theta, \\theta' \\in I \\), \\( |\\gamma_{\\theta}(y) - \\gamma_{\\theta'}(y)| \\leq \\epsilon \\)\n\nUsing Lemma B.2 and Lemma B.3, we can form a uniform bound, such that all \\( w \\) that are \\( \\epsilon \\)-far from \\( w^* \\) in distribution will have log-likelihood ratio smaller than \\( -\\epsilon^2 \\) on the training set. With some additional arguments, we can now show that as the MLE has a positive log-likelihood ratio, it has a small empirical TV.\n\nLemma B.4. Let \\( x_1, ..., x_n \\) be fixed, and \\( y_i \\sim \\phi(x^T_1 w) \\), the MLE \\( w \\) satisfies \\( d(w, w^*) \\leq \\frac{i w^* \\epsilon}{\\sqrt{n}} + \\eta_i \\) for \\( \\eta_i \\sim N(0, 1) \\). For \\( n \\geq \\frac{\\epsilon^2 k \\log \\frac{1}{\\epsilon}}{\\sqrt{n}} \\), this sample complexity guarantees that the MLE is good for the set of empirical \\( x_i \\sim D_x \\), and we need to extend this to the expectation over \\( x \\sim D_x \\), for which we use a Lemma similar to Lemma 4.3 in the linear case, and this completes the proof.\n\nA straightforward combination of Lemma 4.3 and Lemma B.4 gives the following Theorem.\n\nTheorem B.5. Let \\( y = \\phi(x^T w^* + \\eta) \\), for \\( w^* \\in \\mathbb{R}^k \\), \\( x \\sim D_x \\), and \\( \\eta \\sim N(0, 1) \\). Then for a sufficiently large constant \\( C > 0 \\),\n\n\\[\nn = C \\cdot k \\frac{\\epsilon^2 \\log \\frac{1}{\\epsilon}}{\\sqrt{n}}\n\\]\n\nsamples of \\( \\{(y_i, x_i)\\}_{n i=1} \\) suffices to guarantee that the MLE \\( w \\) satisfies\n\nB.1 Proofs\n\nLemma B.1. Let \\( p_1, ..., p_n \\) and \\( q_1, ..., q_n \\) be distributions with \\( E_i[dTV(p_i, q_i)] \\geq \\epsilon \\), where we use the uniform measure on \\( i \\in [n] \\). Let \\( x_i \\sim p_i \\) for \\( i \\in [n] \\). Then with probability \\( 1 - \\delta \\),\n\n\\[\nE_i[\\log \\frac{q_i(x_i)}{p_i(x_i)}] \\leq -\\frac{\\epsilon^2}{4}\n\\]\n\nProof. Define \\( \\gamma_i(x) = \\log \\frac{q_i(x)}{p_i(x)} \\) and \\( a_i(x) := \\max(\\gamma_i(x), -2) \\). We have that\n\n\\[\nE_{i,x}[\\gamma_i(x)] = -E_i[dKL(p_i, q_i)] \\leq -E_i[2dTV(p_i, q_i)^2] \\leq -2\\epsilon^2\n\\]\n\nand want to show that \\( E_i[\\gamma_i(x)] \\leq -\\frac{\\epsilon^2}{4} \\) with high probability. Note that \\( a_i(x) \\geq \\gamma_i(x) \\), so it suffices to show \\( E_i[a_i(x)] \\leq -\\frac{\\epsilon^2}{4} \\). We will do this with Bernstein\u2019s inequality, for which we need bounds on the moments of \\( a_i(x) \\).\n\nTo simplify notation, fix a particular \\( i \\) and consider \\( p = p_i \\), \\( q = q_i \\), \\( a = a_i \\), and \\( x \\sim p \\).\n\nFor a random variable \\( v \\), define \\( v^+, v^- \\) to be the positive/negative parts of \\( v \\), respectively, so \\( v = v^- + v^+ \\). Define \\( \\Delta(x) = q(x) - E[p(x)] - 1 \\). We have that \\( E_{x \\sim p}[\\Delta(x)] = 0 \\), and\n\n\\[\nE_{x \\sim p}[\\Delta^+(x)] = E_{x \\sim p}[-\\Delta^-(x)] = dTV(p, q)\n\\]\n\nNow, consider the function \\( b(z) := \\max(\\log(1 + z), -2) - z \\). This function is nonpositive over \\( z \\geq -1 \\), and \\( b(z) \\leq -\\frac{z^2}{2} \\) for \\( z \\leq 0 \\). Since\n\n\\[\na(x) = b(\\Delta(x)) + \\Delta(x)\n\\]\n\nand \\( E_{x \\sim p}[\\Delta(x)] = 0 \\),\n\n\\[\nE_{x \\sim p}[-a(x)] = E_{x \\sim p}[-b(\\Delta(x))]\n\\]\n\nThis means\n\n\\[\nE_{x \\sim p}[-a(x)] \\geq E_{x \\sim p}[-b(\\Delta(x))] \\geq E_{x \\sim p}[-b(\\Delta(x))1_{\\Delta(x)<0}]\n\\]\n\nor by (14),\n\n\\[\nE_{x}[-a(x)] \\geq E_{x}[\\Delta^2] \\geq 2dTV(p, q)^2\n\\]", "md": "- For all \\( \\theta \\in I \\), \\( \\gamma_{\\theta}(y) \\leq -A \\)\n- For all \\( \\theta, \\theta' \\in I \\), \\( |\\gamma_{\\theta}(y) - \\gamma_{\\theta'}(y)| \\leq \\epsilon \\)\n\nUsing Lemma B.2 and Lemma B.3, we can form a uniform bound, such that all \\( w \\) that are \\( \\epsilon \\)-far from \\( w^* \\) in distribution will have log-likelihood ratio smaller than \\( -\\epsilon^2 \\) on the training set. With some additional arguments, we can now show that as the MLE has a positive log-likelihood ratio, it has a small empirical TV.\n\nLemma B.4. Let \\( x_1, ..., x_n \\) be fixed, and \\( y_i \\sim \\phi(x^T_1 w) \\), the MLE \\( w \\) satisfies \\( d(w, w^*) \\leq \\frac{i w^* \\epsilon}{\\sqrt{n}} + \\eta_i \\) for \\( \\eta_i \\sim N(0, 1) \\). For \\( n \\geq \\frac{\\epsilon^2 k \\log \\frac{1}{\\epsilon}}{\\sqrt{n}} \\), this sample complexity guarantees that the MLE is good for the set of empirical \\( x_i \\sim D_x \\), and we need to extend this to the expectation over \\( x \\sim D_x \\), for which we use a Lemma similar to Lemma 4.3 in the linear case, and this completes the proof.\n\nA straightforward combination of Lemma 4.3 and Lemma B.4 gives the following Theorem.\n\nTheorem B.5. Let \\( y = \\phi(x^T w^* + \\eta) \\), for \\( w^* \\in \\mathbb{R}^k \\), \\( x \\sim D_x \\), and \\( \\eta \\sim N(0, 1) \\). Then for a sufficiently large constant \\( C > 0 \\),\n\n\\[\nn = C \\cdot k \\frac{\\epsilon^2 \\log \\frac{1}{\\epsilon}}{\\sqrt{n}}\n\\]\n\nsamples of \\( \\{(y_i, x_i)\\}_{n i=1} \\) suffices to guarantee that the MLE \\( w \\) satisfies\n\nB.1 Proofs\n\nLemma B.1. Let \\( p_1, ..., p_n \\) and \\( q_1, ..., q_n \\) be distributions with \\( E_i[dTV(p_i, q_i)] \\geq \\epsilon \\), where we use the uniform measure on \\( i \\in [n] \\). Let \\( x_i \\sim p_i \\) for \\( i \\in [n] \\). Then with probability \\( 1 - \\delta \\),\n\n\\[\nE_i[\\log \\frac{q_i(x_i)}{p_i(x_i)}] \\leq -\\frac{\\epsilon^2}{4}\n\\]\n\nProof. Define \\( \\gamma_i(x) = \\log \\frac{q_i(x)}{p_i(x)} \\) and \\( a_i(x) := \\max(\\gamma_i(x), -2) \\). We have that\n\n\\[\nE_{i,x}[\\gamma_i(x)] = -E_i[dKL(p_i, q_i)] \\leq -E_i[2dTV(p_i, q_i)^2] \\leq -2\\epsilon^2\n\\]\n\nand want to show that \\( E_i[\\gamma_i(x)] \\leq -\\frac{\\epsilon^2}{4} \\) with high probability. Note that \\( a_i(x) \\geq \\gamma_i(x) \\), so it suffices to show \\( E_i[a_i(x)] \\leq -\\frac{\\epsilon^2}{4} \\). We will do this with Bernstein\u2019s inequality, for which we need bounds on the moments of \\( a_i(x) \\).\n\nTo simplify notation, fix a particular \\( i \\) and consider \\( p = p_i \\), \\( q = q_i \\), \\( a = a_i \\), and \\( x \\sim p \\).\n\nFor a random variable \\( v \\), define \\( v^+, v^- \\) to be the positive/negative parts of \\( v \\), respectively, so \\( v = v^- + v^+ \\). Define \\( \\Delta(x) = q(x) - E[p(x)] - 1 \\). We have that \\( E_{x \\sim p}[\\Delta(x)] = 0 \\), and\n\n\\[\nE_{x \\sim p}[\\Delta^+(x)] = E_{x \\sim p}[-\\Delta^-(x)] = dTV(p, q)\n\\]\n\nNow, consider the function \\( b(z) := \\max(\\log(1 + z), -2) - z \\). This function is nonpositive over \\( z \\geq -1 \\), and \\( b(z) \\leq -\\frac{z^2}{2} \\) for \\( z \\leq 0 \\). Since\n\n\\[\na(x) = b(\\Delta(x)) + \\Delta(x)\n\\]\n\nand \\( E_{x \\sim p}[\\Delta(x)] = 0 \\),\n\n\\[\nE_{x \\sim p}[-a(x)] = E_{x \\sim p}[-b(\\Delta(x))]\n\\]\n\nThis means\n\n\\[\nE_{x \\sim p}[-a(x)] \\geq E_{x \\sim p}[-b(\\Delta(x))] \\geq E_{x \\sim p}[-b(\\Delta(x))1_{\\Delta(x)<0}]\n\\]\n\nor by (14),\n\n\\[\nE_{x}[-a(x)] \\geq E_{x}[\\Delta^2] \\geq 2dTV(p, q)^2\n\\]"}]}, {"page": 18, "text": "Bounding the positive higher moments.         We have that p(x)ea(x) = max(q(x), e\u22122p(x)) so\n                                E[ea(x)] =     max(q(x), e\u22122p(x))dx\n                                         \u2264  1 + e\u22122 Pr[a(x) = \u22122].\nIn the following, we use that et \u2265  1 + t for all t, as well as et = 1 + t +  \u221e     1\n                                                                               k=2  k!tk. Therefore\n                             1 + e\u22122 Pr[a(x) = \u22122]\n                       \u2265  E[ea(x)] = E[ea\u2212(x)1a(x)\u22640 + ea+(x)1a(x)>0]\n                                   \u2265  E[1 + (a\u2212(x) + a+(x)) +      \u221e   1   +(x)]\n                                                                  k=2  k!ak\n                                   = 1 + E[a(x)] +    \u221e   1      +(x)]\n                                                     k=2  k! E[ak\nso                       \u221e   1     +(x)] \u2264   E[\u2212a(x)] + e\u22122 Pr[a(x) = \u22122].\n                        k=2  k! E[ak\nWe now show that the Pr[a(x) = \u22122] is smaller than the E[\u2212a(x)] term, by relating to \u2212b. When\na(x) = \u22122, \u2206(x) \u2264     \u22121+1/e2, and b(\u2206(x)) = \u22122\u2212\u2206(x) \u2264           \u22121. Since \u2212b(\u2206(x)) is non-negative,\nand at least 1 whenever a(x) = \u22122,\nand hence                  E[\u2212a(x)] = E[\u2212b(\u2206(x))] \u2265       Pr[a(x) = \u22122] \u00b7 1\n                                \u221e   1     +(x)] \u2264   (1 + 1                                         (16)\nIn particular, E[ak            k=2 k! E[ak                e2 ) E[\u2212a(x)].\n                  +(x)] \u2264  2k! E[\u2212a(x)] for all k \u2265   2.\nBounding the second moment of a.         We have that\n                                    E[a(x)2] = E[a2 +(x) + a2 \u2212(x)]\nand E[a2+(x)] \u2264  4 E[\u2212a(x)] by (16). We now bound E[a2                                     2\nby the construction of a. Therefore                       \u2212(x)]. Note that |a\u2212(x)| \u2264    1\u22121/e2 |\u2206\u2212(x)|\nand so by (15),                           a2\u2212(x) \u2264  6\u22062 \u2212(x)\nThus                          E[a2\u2212(x)] \u2264   6 E[\u22062\u2212(x)] \u2264  12 E[\u2212a(x)].\n                                      E[a2(x)] \u2264   16 E[\u2212a(x)].                                    (17)\nBernstein Concentration.       Now we can apply Bernstein\u2019s inequality (Theorem 2.10 of [8]).\nWe apply the theorem to Xi := ai(xi), which are independent. The theorem uses that\n                           n\n                          E[X2    i ] = n E\nby (17), and since        i=1             i,x[ai(x)2] \u2264 16n Ei,x[\u2212ai(x)] =: v\n                      n\n                      E[(Xi)k    +] = n Ei,x[ai,+(x)k] \u2264  2k! E\n                                                              i,x[\u2212ai(x)] \u2264  1\n                     i=1                                                     2vk!\nso we can set c = 1. Applying the theorem, we have that S =  ai(xi) \u2212         E[ai(xi)] satisfies\n                                       S \u2264     2v log 1\n                                                      \u03b4 + log 1\u03b4\n                                                  18", "md": "# Math Equations\n\nBounding the positive higher moments. We have that $$p(x)e^{a(x)} = \\max(q(x), e^{-2p(x)})$$ so\n\n$$E[e^{a(x)}] = \\max(q(x), e^{-2p(x))}dx$$\n\n$$\\leq 1 + e^{-2} \\Pr[a(x) = -2]$$.\n\nIn the following, we use that $$e^t \\geq 1 + t$$ for all t, as well as $$e^t = 1 + t + \\sum_{k=2}^{\\infty} \\frac{1}{k!}t^k$$. Therefore\n\n$$1 + e^{-2} \\Pr[a(x) = -2] \\geq E[e^{a(x)}] = E[e^{a^-(x)1_{a(x) \\leq 0} + e^{a^+(x)1_{a(x) > 0}}]$$\n\n$$\\geq E[1 + (a^-(x) + a^+(x)) + \\sum_{k=2}^{\\infty} \\frac{1}{k!}a^k]$$\n\n$$= 1 + E[a(x)] + \\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k]$$\n\nso $$\\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k] \\leq E[-a(x)] + e^{-2} \\Pr[a(x) = -2]$$.\n\nWe now show that the $$\\Pr[a(x) = -2]$$ is smaller than the $$E[-a(x)]$$ term, by relating to -b. When\n\n$$a(x) = -2, \\Delta(x) \\leq -1+1/e^2$$, and $$b(\\Delta(x)) = -2-\\Delta(x) \\leq -1$$. Since $$-b(\\Delta(x))$$ is non-negative,\n\nand at least 1 whenever $$a(x) = -2$$,\n\nand hence $$E[-a(x)] = E[-b(\\Delta(x))] \\geq \\Pr[a(x) = -2] \\cdot 1$$\n\n$$\\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k] \\leq (1 + 1)e^2 E[-a(x)]$$.\n\nIn particular, $$E[a^k] \\leq 2k! E[-a(x)]$$ for all $$k \\geq 2$$.\n\nBounding the second moment of a. We have that\n\n$$E[a(x)^2] = E[a^+(x) + a^-(x)]$$\n\nand $$E[a^+(x)] \\leq 4 E[-a(x)]$$ by (16). We now bound $$E[a^2(x)]$$ by the construction of a. Therefore\n\n$$a^-(x)]$$. Note that $$|a^-(x)| \\leq 1-1/e^2 |\\Delta^-(x)|$$\n\nand so by (15), $$a^2-(x) \\leq 6\\Delta^2-(x)$$\n\nThus $$E[a^2-(x)] \\leq 6 E[\\Delta^2-(x)] \\leq 12 E[-a(x)]$$.\n\n$$E[a^2(x)] \\leq 16 E[-a(x)]$$ (17)\n\nBernstein Concentration. Now we can apply Bernstein\u2019s inequality (Theorem 2.10 of [8]).\n\nWe apply the theorem to $$X_i := a_i(x_i)$$, which are independent. The theorem uses that\n\n$$n E[X^2_i] = n \\sum_{i=1}^{n} E_{i,x}[a_i(x)^2] \\leq 16n \\sum_{i=1}^{n} E_{i,x}[-a_i(x)] =: v$$\n\n$$n E[(X_i)^k_+] = n \\sum_{i=1}^{n} E_{i,x}[a_i^+(x)^k] \\leq 2k! \\sum_{i=1}^{n} E_{i,x}[-a_i(x)] \\leq \\frac{1}{2vk!}$$\n\nso we can set c = 1. Applying the theorem, we have that $$S = a_i(x_i) - E[a_i(x)]$$ satisfies\n\n$$S \\leq 2v \\log \\frac{1}{\\delta} + \\log \\frac{1}{\\delta}$$\n\n$$18$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Bounding the positive higher moments. We have that $$p(x)e^{a(x)} = \\max(q(x), e^{-2p(x)})$$ so\n\n$$E[e^{a(x)}] = \\max(q(x), e^{-2p(x))}dx$$\n\n$$\\leq 1 + e^{-2} \\Pr[a(x) = -2]$$.\n\nIn the following, we use that $$e^t \\geq 1 + t$$ for all t, as well as $$e^t = 1 + t + \\sum_{k=2}^{\\infty} \\frac{1}{k!}t^k$$. Therefore\n\n$$1 + e^{-2} \\Pr[a(x) = -2] \\geq E[e^{a(x)}] = E[e^{a^-(x)1_{a(x) \\leq 0} + e^{a^+(x)1_{a(x) > 0}}]$$\n\n$$\\geq E[1 + (a^-(x) + a^+(x)) + \\sum_{k=2}^{\\infty} \\frac{1}{k!}a^k]$$\n\n$$= 1 + E[a(x)] + \\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k]$$\n\nso $$\\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k] \\leq E[-a(x)] + e^{-2} \\Pr[a(x) = -2]$$.\n\nWe now show that the $$\\Pr[a(x) = -2]$$ is smaller than the $$E[-a(x)]$$ term, by relating to -b. When\n\n$$a(x) = -2, \\Delta(x) \\leq -1+1/e^2$$, and $$b(\\Delta(x)) = -2-\\Delta(x) \\leq -1$$. Since $$-b(\\Delta(x))$$ is non-negative,\n\nand at least 1 whenever $$a(x) = -2$$,\n\nand hence $$E[-a(x)] = E[-b(\\Delta(x))] \\geq \\Pr[a(x) = -2] \\cdot 1$$\n\n$$\\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k] \\leq (1 + 1)e^2 E[-a(x)]$$.\n\nIn particular, $$E[a^k] \\leq 2k! E[-a(x)]$$ for all $$k \\geq 2$$.\n\nBounding the second moment of a. We have that\n\n$$E[a(x)^2] = E[a^+(x) + a^-(x)]$$\n\nand $$E[a^+(x)] \\leq 4 E[-a(x)]$$ by (16). We now bound $$E[a^2(x)]$$ by the construction of a. Therefore\n\n$$a^-(x)]$$. Note that $$|a^-(x)| \\leq 1-1/e^2 |\\Delta^-(x)|$$\n\nand so by (15), $$a^2-(x) \\leq 6\\Delta^2-(x)$$\n\nThus $$E[a^2-(x)] \\leq 6 E[\\Delta^2-(x)] \\leq 12 E[-a(x)]$$.\n\n$$E[a^2(x)] \\leq 16 E[-a(x)]$$ (17)\n\nBernstein Concentration. Now we can apply Bernstein\u2019s inequality (Theorem 2.10 of [8]).\n\nWe apply the theorem to $$X_i := a_i(x_i)$$, which are independent. The theorem uses that\n\n$$n E[X^2_i] = n \\sum_{i=1}^{n} E_{i,x}[a_i(x)^2] \\leq 16n \\sum_{i=1}^{n} E_{i,x}[-a_i(x)] =: v$$\n\n$$n E[(X_i)^k_+] = n \\sum_{i=1}^{n} E_{i,x}[a_i^+(x)^k] \\leq 2k! \\sum_{i=1}^{n} E_{i,x}[-a_i(x)] \\leq \\frac{1}{2vk!}$$\n\nso we can set c = 1. Applying the theorem, we have that $$S = a_i(x_i) - E[a_i(x)]$$ satisfies\n\n$$S \\leq 2v \\log \\frac{1}{\\delta} + \\log \\frac{1}{\\delta}$$\n\n$$18$$", "md": "Bounding the positive higher moments. We have that $$p(x)e^{a(x)} = \\max(q(x), e^{-2p(x)})$$ so\n\n$$E[e^{a(x)}] = \\max(q(x), e^{-2p(x))}dx$$\n\n$$\\leq 1 + e^{-2} \\Pr[a(x) = -2]$$.\n\nIn the following, we use that $$e^t \\geq 1 + t$$ for all t, as well as $$e^t = 1 + t + \\sum_{k=2}^{\\infty} \\frac{1}{k!}t^k$$. Therefore\n\n$$1 + e^{-2} \\Pr[a(x) = -2] \\geq E[e^{a(x)}] = E[e^{a^-(x)1_{a(x) \\leq 0} + e^{a^+(x)1_{a(x) > 0}}]$$\n\n$$\\geq E[1 + (a^-(x) + a^+(x)) + \\sum_{k=2}^{\\infty} \\frac{1}{k!}a^k]$$\n\n$$= 1 + E[a(x)] + \\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k]$$\n\nso $$\\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k] \\leq E[-a(x)] + e^{-2} \\Pr[a(x) = -2]$$.\n\nWe now show that the $$\\Pr[a(x) = -2]$$ is smaller than the $$E[-a(x)]$$ term, by relating to -b. When\n\n$$a(x) = -2, \\Delta(x) \\leq -1+1/e^2$$, and $$b(\\Delta(x)) = -2-\\Delta(x) \\leq -1$$. Since $$-b(\\Delta(x))$$ is non-negative,\n\nand at least 1 whenever $$a(x) = -2$$,\n\nand hence $$E[-a(x)] = E[-b(\\Delta(x))] \\geq \\Pr[a(x) = -2] \\cdot 1$$\n\n$$\\sum_{k=2}^{\\infty} \\frac{1}{k!}E[a^k] \\leq (1 + 1)e^2 E[-a(x)]$$.\n\nIn particular, $$E[a^k] \\leq 2k! E[-a(x)]$$ for all $$k \\geq 2$$.\n\nBounding the second moment of a. We have that\n\n$$E[a(x)^2] = E[a^+(x) + a^-(x)]$$\n\nand $$E[a^+(x)] \\leq 4 E[-a(x)]$$ by (16). We now bound $$E[a^2(x)]$$ by the construction of a. Therefore\n\n$$a^-(x)]$$. Note that $$|a^-(x)| \\leq 1-1/e^2 |\\Delta^-(x)|$$\n\nand so by (15), $$a^2-(x) \\leq 6\\Delta^2-(x)$$\n\nThus $$E[a^2-(x)] \\leq 6 E[\\Delta^2-(x)] \\leq 12 E[-a(x)]$$.\n\n$$E[a^2(x)] \\leq 16 E[-a(x)]$$ (17)\n\nBernstein Concentration. Now we can apply Bernstein\u2019s inequality (Theorem 2.10 of [8]).\n\nWe apply the theorem to $$X_i := a_i(x_i)$$, which are independent. The theorem uses that\n\n$$n E[X^2_i] = n \\sum_{i=1}^{n} E_{i,x}[a_i(x)^2] \\leq 16n \\sum_{i=1}^{n} E_{i,x}[-a_i(x)] =: v$$\n\n$$n E[(X_i)^k_+] = n \\sum_{i=1}^{n} E_{i,x}[a_i^+(x)^k] \\leq 2k! \\sum_{i=1}^{n} E_{i,x}[-a_i(x)] \\leq \\frac{1}{2vk!}$$\n\nso we can set c = 1. Applying the theorem, we have that $$S = a_i(x_i) - E[a_i(x)]$$ satisfies\n\n$$S \\leq 2v \\log \\frac{1}{\\delta} + \\log \\frac{1}{\\delta}$$\n\n$$18$$"}]}, {"page": 19, "text": "with probability 1 \u2212   \u03b4. Plugging in v and rescaling by n, with probability 1 \u2212     \u03b4 we have:\n                    E                                    E[\u2212a(x)] 1\n                    i [ai(xi)] \u2264  E\n                                 i,x[ai(x)] + O(1) \u00b7                n log 1\n                                                                          \u03b4 + 1 n log 1\n                                                                                      \u03b4\n By our assumption on n for a sufficiently large constant in the big O, this implies\n                           Ei [ai(xi)] \u2264 E                E\n                                         i,x[ai(x)] + 1     i,x[\u2212ai(x)] + \u03b52/8\n                                                      6\u03b5\n Since by (15), \u03b5 \u2264     Ei[dT V (pi, qi)2] \u2264    Ei,x[\u22122ai(x)], this means\n                       Ei [\u03b3i(xi)] \u2264 Ei [ai(xi)] \u2264(\u22121 +    \u221a 2  i,x[\u2212ai(x)] + \u03b52/8\n                                                            6 ) E\n                                                \u2264 (\u22121 +    \u221a62)12\u03b52 + 18\u03b52\n as desired.                                    \u2264 \u22121 4\u03b52\n Lemma B.2. For any y = \u03d5(\u03b8 + \u03b7),\n                             \u03b3\u03b8(y) =   log \u03a6(\u2212\u03b8) \u2212     log \u03a6(\u2212\u03b8\u2217)     if y = 0\n                                        \u03b7(\u03b8 \u2212  \u03b8\u2217) \u2212   (\u03b8\u2212\u03b8\u2217)2        if y > 0\n                                                          2\n and therefore, for all y,                   \u03b3\u03b8(y) \u2264   |\u03b7|2/2.\n Proof. Let \u03a6(x) be the cdf of a standard Gaussian. For y > 0,\n                                   \u03b3\u03b8(y) = 1 2((y \u2212   \u03b8\u2217)2 \u2212  (y \u2212  \u03b8)2)\n                                          = 12(\u03b72 \u2212   (\u03b7 + \u03b8\u2217  \u2212  \u03b8)2)\n                                          = \u03b7(\u03b8 \u2212   \u03b8\u2217) \u2212  (\u03b8 \u2212 2\u03b8\u2217)2\nThus:                       \u03b3\u03b8(y) =    log \u03a6(\u2212\u03b8) \u2212     log \u03a6(\u2212\u03b8\u2217)     if y = 0\n                                        \u03b7(\u03b8 \u2212  \u03b8\u2217) \u2212   (\u03b8\u2212\u03b8\u2217)2        if y > 0\n                                                          2\n Now suppose |\u03b7| \u2264    B. We can upper bound \u03b3\u03b8(y) for all \u03b8:\n        \u2022 If y = 0, then \u2212\u03b8\u2217   \u2265  \u2212B, so\n        \u2022 If y > 0, then        \u03b3\u03b8(0) \u2264   \u2212log \u03a6(\u2212\u03b8\u2217) \u2264     \u2212log e\u2212B2/2 = B2/2.\n as desired.                    \u03b3\u03b8(y) = (\u03b8 \u2212   \u03b8\u2217)\u03b7 \u2212   (\u03b8 \u22122\u03b8\u2217)2  \u2264  \u03b72/2 \u2264   B2/2.\n Lemma B.3 (One-dimensional net). Let A > B2 > 1. There exists a partition of R into O(A/\u03b5)\n intervals such that, for each interval I in the partition and every y = \u03d5(\u03b8\u2217   + \u03b7) with |\u03b7| \u2264  B, one of\n the following holds:\n        \u2022 For all \u03b8 \u2208  I, \u03b3\u03b8(y) \u2264  \u2212A\n        \u2022 For all \u03b8, \u03b8\u2032 \u2208 I, |\u03b3\u03b8(y) \u2212  \u03b3\u03b8\u2032(y)| \u2264  \u03b5 19", "md": "with probability $$1 - \\delta$$. Plugging in v and rescaling by n, with probability $$1 - \\delta$$ we have:\n\n$$\n\\begin{align*}\nE[i \\{a_i(x_i)\\}] &\\leq E[i,x\\{a_i(x)\\}] + O(1) \\cdot \\frac{n \\log \\frac{1}{\\delta} + 1}{n \\log \\frac{1}{\\delta}} \\\\\n\\end{align*}\n$$\nBy our assumption on n for a sufficiently large constant in the big O, this implies\n\n$$\n\\begin{align*}\nE_i \\{a_i(x_i)\\} &\\leq E \\{E_i,x[a_i(x)] + 1 \\{i,x[-a_i(x)] + \\frac{\\varepsilon^2}{8}\\} \\\\\n&\\leq \\left(-1 + \\sqrt{2} \\{i,x[-a_i(x)] + \\frac{\\varepsilon^2}{8}\\right) E \\\\\n&\\leq \\left(-1 + \\sqrt{62}\\right) \\frac{12\\varepsilon^2}{18\\varepsilon^2} \\\\\n&\\text{as desired.}\n\\end{align*}\n$$\nLemma B.2. For any $$y = \\phi(\\theta + \\eta)$$,\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases}\n\\end{align*}\n$$\nand therefore, for all y,\n\n$$\n\\gamma_\\theta(y) \\leq |\\eta|^2/2.\n$$\nProof. Let $$\\Phi(x)$$ be the cdf of a standard Gaussian. For $$y > 0$$,\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\frac{1}{2}((y - \\theta^*)^2 - (y - \\theta)^2) \\\\\n&= \\frac{1}{2}(\\eta^2 - (\\eta + \\theta^* - \\theta)^2) \\\\\n&= \\eta(\\theta - \\theta^*) - (\\theta - 2\\theta^*)^2\n\\end{align*}\n$$\nThus:\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases}\n\\end{align*}\n$$\nNow suppose $$|\\eta| \\leq B$$. We can upper bound $$\\gamma_\\theta(y)$$ for all $$\\theta$$:\n\n- If $y = 0$, then $-\\theta^* \\geq -B$, so\n- If $y > 0$, then $\\gamma_\\theta(0) \\leq -\\log \\Phi(-\\theta^*) \\leq -\\log e^{-B^2/2} = B^2/2$.\n\nas desired. $$\\gamma_\\theta(y) = (\\theta - \\theta^*)\\eta - (\\theta - 2\\theta^*)^2 \\leq \\eta^2/2 \\leq B^2/2$$.\n\nLemma B.3 (One-dimensional net). Let $$A > B^2 > 1$$. There exists a partition of $$\\mathbb{R}$$ into $$O(A/\\varepsilon)$$ intervals such that, for each interval I in the partition and every $$y = \\phi(\\theta^* + \\eta)$$ with $$|\\eta| \\leq B$$, one of the following holds:\n\n- For all $\\theta \\in I$, $\\gamma_\\theta(y) \\leq -A$\n- For all $\\theta, \\theta' \\in I$, $|\\gamma_\\theta(y) - \\gamma_{\\theta'}(y)| \\leq \\varepsilon/19$", "images": [], "items": [{"type": "text", "value": "with probability $$1 - \\delta$$. Plugging in v and rescaling by n, with probability $$1 - \\delta$$ we have:\n\n$$\n\\begin{align*}\nE[i \\{a_i(x_i)\\}] &\\leq E[i,x\\{a_i(x)\\}] + O(1) \\cdot \\frac{n \\log \\frac{1}{\\delta} + 1}{n \\log \\frac{1}{\\delta}} \\\\\n\\end{align*}\n$$\nBy our assumption on n for a sufficiently large constant in the big O, this implies\n\n$$\n\\begin{align*}\nE_i \\{a_i(x_i)\\} &\\leq E \\{E_i,x[a_i(x)] + 1 \\{i,x[-a_i(x)] + \\frac{\\varepsilon^2}{8}\\} \\\\\n&\\leq \\left(-1 + \\sqrt{2} \\{i,x[-a_i(x)] + \\frac{\\varepsilon^2}{8}\\right) E \\\\\n&\\leq \\left(-1 + \\sqrt{62}\\right) \\frac{12\\varepsilon^2}{18\\varepsilon^2} \\\\\n&\\text{as desired.}\n\\end{align*}\n$$\nLemma B.2. For any $$y = \\phi(\\theta + \\eta)$$,\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases}\n\\end{align*}\n$$\nand therefore, for all y,\n\n$$\n\\gamma_\\theta(y) \\leq |\\eta|^2/2.\n$$\nProof. Let $$\\Phi(x)$$ be the cdf of a standard Gaussian. For $$y > 0$$,\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\frac{1}{2}((y - \\theta^*)^2 - (y - \\theta)^2) \\\\\n&= \\frac{1}{2}(\\eta^2 - (\\eta + \\theta^* - \\theta)^2) \\\\\n&= \\eta(\\theta - \\theta^*) - (\\theta - 2\\theta^*)^2\n\\end{align*}\n$$\nThus:\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases}\n\\end{align*}\n$$\nNow suppose $$|\\eta| \\leq B$$. We can upper bound $$\\gamma_\\theta(y)$$ for all $$\\theta$$:\n\n- If $y = 0$, then $-\\theta^* \\geq -B$, so\n- If $y > 0$, then $\\gamma_\\theta(0) \\leq -\\log \\Phi(-\\theta^*) \\leq -\\log e^{-B^2/2} = B^2/2$.\n\nas desired. $$\\gamma_\\theta(y) = (\\theta - \\theta^*)\\eta - (\\theta - 2\\theta^*)^2 \\leq \\eta^2/2 \\leq B^2/2$$.\n\nLemma B.3 (One-dimensional net). Let $$A > B^2 > 1$$. There exists a partition of $$\\mathbb{R}$$ into $$O(A/\\varepsilon)$$ intervals such that, for each interval I in the partition and every $$y = \\phi(\\theta^* + \\eta)$$ with $$|\\eta| \\leq B$$, one of the following holds:\n\n- For all $\\theta \\in I$, $\\gamma_\\theta(y) \\leq -A$\n- For all $\\theta, \\theta' \\in I$, $|\\gamma_\\theta(y) - \\gamma_{\\theta'}(y)| \\leq \\varepsilon/19$", "md": "with probability $$1 - \\delta$$. Plugging in v and rescaling by n, with probability $$1 - \\delta$$ we have:\n\n$$\n\\begin{align*}\nE[i \\{a_i(x_i)\\}] &\\leq E[i,x\\{a_i(x)\\}] + O(1) \\cdot \\frac{n \\log \\frac{1}{\\delta} + 1}{n \\log \\frac{1}{\\delta}} \\\\\n\\end{align*}\n$$\nBy our assumption on n for a sufficiently large constant in the big O, this implies\n\n$$\n\\begin{align*}\nE_i \\{a_i(x_i)\\} &\\leq E \\{E_i,x[a_i(x)] + 1 \\{i,x[-a_i(x)] + \\frac{\\varepsilon^2}{8}\\} \\\\\n&\\leq \\left(-1 + \\sqrt{2} \\{i,x[-a_i(x)] + \\frac{\\varepsilon^2}{8}\\right) E \\\\\n&\\leq \\left(-1 + \\sqrt{62}\\right) \\frac{12\\varepsilon^2}{18\\varepsilon^2} \\\\\n&\\text{as desired.}\n\\end{align*}\n$$\nLemma B.2. For any $$y = \\phi(\\theta + \\eta)$$,\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases}\n\\end{align*}\n$$\nand therefore, for all y,\n\n$$\n\\gamma_\\theta(y) \\leq |\\eta|^2/2.\n$$\nProof. Let $$\\Phi(x)$$ be the cdf of a standard Gaussian. For $$y > 0$$,\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\frac{1}{2}((y - \\theta^*)^2 - (y - \\theta)^2) \\\\\n&= \\frac{1}{2}(\\eta^2 - (\\eta + \\theta^* - \\theta)^2) \\\\\n&= \\eta(\\theta - \\theta^*) - (\\theta - 2\\theta^*)^2\n\\end{align*}\n$$\nThus:\n\n$$\n\\begin{align*}\n\\gamma_\\theta(y) &= \\begin{cases} \\log \\Phi(-\\theta) - \\log \\Phi(-\\theta^*) & \\text{if } y = 0 \\\\ \\eta(\\theta - \\theta^*) - (\\theta - \\theta^*)^2/2 & \\text{if } y > 0 \\end{cases}\n\\end{align*}\n$$\nNow suppose $$|\\eta| \\leq B$$. We can upper bound $$\\gamma_\\theta(y)$$ for all $$\\theta$$:\n\n- If $y = 0$, then $-\\theta^* \\geq -B$, so\n- If $y > 0$, then $\\gamma_\\theta(0) \\leq -\\log \\Phi(-\\theta^*) \\leq -\\log e^{-B^2/2} = B^2/2$.\n\nas desired. $$\\gamma_\\theta(y) = (\\theta - \\theta^*)\\eta - (\\theta - 2\\theta^*)^2 \\leq \\eta^2/2 \\leq B^2/2$$.\n\nLemma B.3 (One-dimensional net). Let $$A > B^2 > 1$$. There exists a partition of $$\\mathbb{R}$$ into $$O(A/\\varepsilon)$$ intervals such that, for each interval I in the partition and every $$y = \\phi(\\theta^* + \\eta)$$ with $$|\\eta| \\leq B$$, one of the following holds:\n\n- For all $\\theta \\in I$, $\\gamma_\\theta(y) \\leq -A$\n- For all $\\theta, \\theta' \\in I$, $|\\gamma_\\theta(y) - \\gamma_{\\theta'}(y)| \\leq \\varepsilon/19$"}]}, {"page": 20, "text": "Proof. To define our partition, we actually define two partitions, depending on whether y = 0, then\nintersect them for our final partition.\nFirst, consider y = 0. By Lemma B.2, \u03b3\u03b8(0) is monotonically decreasing in \u03b8, from its maximum\nof at most B2/2. We can thus define a partition P1 consisting of intervals of the form Ii := {\u03b8 |\n\u03b3\u03b8(0) \u2208     (B2/2 \u2212       (i + 1)\u03b5, B2/2 \u2212         i\u03b5)}, for i \u2208     {0, 1, . . . , (A + B2/2)/\u03b5}, plus a special interval\nI\u2032 of {\u03b8 | \u03b3\u03b8(0) < \u2212A}. When y = 0, this partition satisfies the desired conclusion to the lemma:\n|\u03b3\u03b8(0) \u2212     \u03b3\u03b8\u2032(0)| \u2264      \u03b5 for all \u03b8, \u03b8\u2032 \u2208     Ii, while \u03b3\u03b8(0) < \u2212A for \u03b8 \u2208               I\u2032. Call this partition P0, which\nhas size O(A/\u03b5).\nSecond, consider y > 0. Define R =                  \u221a  2A + B. Note that R2 \u2272             A and (R \u2212        B)2 \u2265     2A. Therefore\nfor |\u03b8 \u2212    \u03b8\u2217| \u2265    R,                \u03b3\u03b8(y) \u2264     \u22121 2 max(0, |\u03b8 \u2212        \u03b8\u2217| \u2212    \u03b7)2 \u2264    \u2212A.\nConsider any \u03b8, \u03b8\u2032 \u2208          [\u03b8\u2217  \u2212  R, \u03b8\u2217   + R] with \u03b1 := |\u03b8 \u2212           \u03b8\u2032|. We have\n                           |\u03b3\u03b8(y) \u2212     \u03b3\u03b8\u2032(y)| \u2264      |\u03b7(\u03b8 \u2212    \u03b8\u2032)| + 1 2   (\u03b8\u2032 \u2212   \u03b8\u2217)2 \u2212    (\u03b8 \u2212    \u03b8\u2217)2\n                                                   \u2264   B\u03b1 + 1   2|(\u03b8\u2032 \u2212     \u03b8)(\u22122\u03b8\u2217      + (\u03b8\u2032 + \u03b8))|\n                      \u03b5                            \u2264   B\u03b1 + 1   2\u03b1(2R) = \u03b1(B + R).\nThus, for \u03b1 =        2R, this is at most \u03b5. If we partition [\u03b8\u2217            \u2212  R, \u03b8\u2217    + R] into length-\u03b1 intervals, we get a\nsize O(R2/\u03b5) = O(A/\u03b5) partition P1 of R that has the desired property for all y > 0.\nOur final partition is defi       ned by all endpoints in either P0 and P1. This has size O(A/\u03b5), and within\neach interval the conclusion holds for both y = 0 and y > 0, as needed.\nLemma B.4. Let x1, . . . , xn be fixed, and yi \u223c                  \u03d5(xT                                                        1\nthe MLE      w satisfies                                   d( w, w\u2217) \u2264  i w\u2217 \u03b5.+ \u03b7i) for \u03b7i \u223c      N   (0, 1). For n \u2265       \u03b52 k log 1 \u03b5,\nProof. For any w \u2208             Rk, and a sample (xi, yi), let pw(y|xi) be the conditional distribution of\ny = \u03d5(\u27e8xi, w\u27e9        + \u03b7), and let \u03b3i,w be the log-likelihood ratio between w and w\u2217                          on this sample:\n                                                   \u03b3i,w(y) := log pw(y|xi)\nThen                                 E                                  pw\u2217(y|xi).\nDefine                               y[\u03b3i,w(y)] = \u2212dKL(pi,w\u2217(y|xi)||pi,w(y|xi)).\n                                                            n\n                                dKL(w\u2217, w) := 1        n   i=1  dKL(pi,w\u2217(y|xi)||pi,w(y|xi)).\nConcentration.            From Lemma B.1, we see\u03b3that if       n        d(w\u2217, w) \u2265       \u03b5, then for n \u2265       O( 1\u03b52 log 1  \u03b4 ),\nwith probability 1 \u2212         \u03b4.                   w := 1  n  i=1   \u03b3i,w(yi) < \u2212\u03b52      4 ,                                           (18)\nOf course, whenever \u03b3          w < 0, the likelihood under w\u2217             is larger than the likelihood under w. Thus, for\neach fixed w with        d(w\u2217, w) \u2265       \u03b5, maximizing likelihood would prefer w\u2217                   to w with probability 1 \u2212          \u03b4\nif n \u2265    O( 1\u03b52 log 1 \u03b4 ).\nNothing above is specific to our ReLU-based distribution. But to extend to the MLE over all w, we\nneed to build a net using properties of our distribution.\n                                                                   20", "md": "# Math Equations and Text\n\n## Proof:\n\nTo define our partition, we actually define two partitions, depending on whether \\( y = 0 \\), then intersect them for our final partition.\n\nFirst, consider \\( y = 0 \\). By Lemma B.2, \\( \\gamma_{\\theta}(0) \\) is monotonically decreasing in \\( \\theta \\), from its maximum of at most \\( \\frac{B^2}{2} \\). We can thus define a partition \\( P_1 \\) consisting of intervals of the form \\( I_i := \\{ \\theta | \\gamma_{\\theta}(0) \\in ( \\frac{B^2}{2} - (i + 1)\\epsilon, \\frac{B^2}{2} - i\\epsilon) \\} \\), for \\( i \\in \\{0, 1, ..., (\\frac{A + B^2}{2})/\\epsilon \\} \\), plus a special interval \\( I' \\) of \\( \\{ \\theta | \\gamma_{\\theta}(0) < -A \\} \\). When \\( y = 0 \\), this partition satisfies the desired conclusion to the lemma: \\( |\\gamma_{\\theta}(0) - \\gamma_{\\theta'}(0)| \\leq \\epsilon \\) for all \\( \\theta, \\theta' \\in I_i \\), while \\( \\gamma_{\\theta}(0) < -A \\) for \\( \\theta \\in I' \\). Call this partition \\( P_0 \\), which has size \\( O(A/\\epsilon) \\).\n\nSecond, consider \\( y > 0 \\). Define \\( R = \\sqrt{2A + B} \\). Note that \\( R^2 \\lesssim A \\) and \\( (R - B)^2 \\geq 2A \\). Therefore for \\( |\\theta - \\theta^*| \\geq R \\), \\( \\gamma_{\\theta}(y) \\leq -\\frac{1}{2} \\max(0, |\\theta - \\theta^*| - \\eta)^2 \\leq -A \\).\n\nConsider any \\( \\theta, \\theta' \\in [\\theta^* - R, \\theta^* + R] \\) with \\( \\alpha := |\\theta - \\theta'| \\). We have\n\n$$\n\\begin{align*}\n|\\gamma_{\\theta}(y) - \\gamma_{\\theta'}(y)| &\\leq |\\eta(\\theta - \\theta')| + \\frac{1}{2} (\\theta' - \\theta^*)^2 - (\\theta - \\theta^*)^2 \\\\\n&\\leq B\\alpha + \\frac{1}{2} |(\\theta' - \\theta)(-2\\theta^* + (\\theta' + \\theta))| \\\\\n&\\leq B\\alpha + \\frac{1}{2}\\alpha(2R) = \\alpha(B + R).\n\\end{align*}\n$$\nThus, for \\( \\alpha = 2R \\), this is at most \\( \\epsilon \\). If we partition \\( [\\theta^* - R, \\theta^* + R] \\) into length-\\( \\alpha \\) intervals, we get a size \\( O(R^2/\\epsilon) = O(A/\\epsilon) \\) partition \\( P_1 \\) of \\( R \\) that has the desired property for all \\( y > 0 \\).\n\nOur final partition is defined by all endpoints in either \\( P_0 \\) and \\( P_1 \\). This has size \\( O(A/\\epsilon) \\), and within each interval the conclusion holds for both \\( y = 0 \\) and \\( y > 0 \\), as needed.\n\n## Lemma B.4:\n\nLet \\( x_1, ..., x_n \\) be fixed, and \\( y_i \\sim \\phi(x_1^T) \\) the MLE \\( w \\) satisfies \\( d(w, w^*) \\leq i w^* \\epsilon + \\eta_i \\) for \\( \\eta_i \\sim N(0, 1) \\). For \\( n \\geq \\epsilon^2 k \\log \\frac{1}{\\epsilon} \\).\n\nProof: For any \\( w \\in \\mathbb{R}^k \\), and a sample \\( (x_i, y_i) \\), let \\( p_w(y|xi) \\) be the conditional distribution of \\( y = \\phi(\\langle x_i, w \\rangle + \\eta) \\), and let \\( \\gamma_{i,w} \\) be the log-likelihood ratio between \\( w \\) and \\( w^* \\) on this sample:\n\n$$\n\\gamma_{i,w}(y) := \\log p_w(y|xi)\n$$\nThen \\( E[p_w^*(y|xi)] \\). Define\n\n$$\ny[\\gamma_{i,w}(y)] = -d_{KL}(p_{i,w^*}(y|xi)||p_{i,w}(y|xi)).\n$$\n$$\nd_{KL}(w^*, w) := \\frac{1}{n} \\sum_{i=1}^{n} d_{KL}(p_{i,w^*}(y|xi)||p_{i,w}(y|xi)).\n$$\n## Concentration:\n\nFrom Lemma B.1, we see that if \\( n \\cdot d(w^*, w) \\geq \\epsilon \\), then for \\( n \\geq O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\delta}) \\), with probability \\( 1 - \\delta \\),\n\n$$\nw := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{i,w}(y_i) < -\\frac{\\epsilon^2}{4} \\quad (18)\n$$\nOf course, whenever \\( \\gamma_w < 0 \\), the likelihood under \\( w^* \\) is larger than the likelihood under \\( w \\). Thus, for each fixed \\( w \\) with \\( d(w^*, w) \\geq \\epsilon \\), maximizing likelihood would prefer \\( w^* \\) to \\( w \\) with probability \\( 1 - \\delta \\) if \\( n \\geq O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\delta}) \\).\n\nNothing above is specific to our ReLU-based distribution. But to extend to the MLE over all \\( w \\), we need to build a net using properties of our distribution.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Proof:", "md": "## Proof:"}, {"type": "text", "value": "To define our partition, we actually define two partitions, depending on whether \\( y = 0 \\), then intersect them for our final partition.\n\nFirst, consider \\( y = 0 \\). By Lemma B.2, \\( \\gamma_{\\theta}(0) \\) is monotonically decreasing in \\( \\theta \\), from its maximum of at most \\( \\frac{B^2}{2} \\). We can thus define a partition \\( P_1 \\) consisting of intervals of the form \\( I_i := \\{ \\theta | \\gamma_{\\theta}(0) \\in ( \\frac{B^2}{2} - (i + 1)\\epsilon, \\frac{B^2}{2} - i\\epsilon) \\} \\), for \\( i \\in \\{0, 1, ..., (\\frac{A + B^2}{2})/\\epsilon \\} \\), plus a special interval \\( I' \\) of \\( \\{ \\theta | \\gamma_{\\theta}(0) < -A \\} \\). When \\( y = 0 \\), this partition satisfies the desired conclusion to the lemma: \\( |\\gamma_{\\theta}(0) - \\gamma_{\\theta'}(0)| \\leq \\epsilon \\) for all \\( \\theta, \\theta' \\in I_i \\), while \\( \\gamma_{\\theta}(0) < -A \\) for \\( \\theta \\in I' \\). Call this partition \\( P_0 \\), which has size \\( O(A/\\epsilon) \\).\n\nSecond, consider \\( y > 0 \\). Define \\( R = \\sqrt{2A + B} \\). Note that \\( R^2 \\lesssim A \\) and \\( (R - B)^2 \\geq 2A \\). Therefore for \\( |\\theta - \\theta^*| \\geq R \\), \\( \\gamma_{\\theta}(y) \\leq -\\frac{1}{2} \\max(0, |\\theta - \\theta^*| - \\eta)^2 \\leq -A \\).\n\nConsider any \\( \\theta, \\theta' \\in [\\theta^* - R, \\theta^* + R] \\) with \\( \\alpha := |\\theta - \\theta'| \\). We have\n\n$$\n\\begin{align*}", "md": "To define our partition, we actually define two partitions, depending on whether \\( y = 0 \\), then intersect them for our final partition.\n\nFirst, consider \\( y = 0 \\). By Lemma B.2, \\( \\gamma_{\\theta}(0) \\) is monotonically decreasing in \\( \\theta \\), from its maximum of at most \\( \\frac{B^2}{2} \\). We can thus define a partition \\( P_1 \\) consisting of intervals of the form \\( I_i := \\{ \\theta | \\gamma_{\\theta}(0) \\in ( \\frac{B^2}{2} - (i + 1)\\epsilon, \\frac{B^2}{2} - i\\epsilon) \\} \\), for \\( i \\in \\{0, 1, ..., (\\frac{A + B^2}{2})/\\epsilon \\} \\), plus a special interval \\( I' \\) of \\( \\{ \\theta | \\gamma_{\\theta}(0) < -A \\} \\). When \\( y = 0 \\), this partition satisfies the desired conclusion to the lemma: \\( |\\gamma_{\\theta}(0) - \\gamma_{\\theta'}(0)| \\leq \\epsilon \\) for all \\( \\theta, \\theta' \\in I_i \\), while \\( \\gamma_{\\theta}(0) < -A \\) for \\( \\theta \\in I' \\). Call this partition \\( P_0 \\), which has size \\( O(A/\\epsilon) \\).\n\nSecond, consider \\( y > 0 \\). Define \\( R = \\sqrt{2A + B} \\). Note that \\( R^2 \\lesssim A \\) and \\( (R - B)^2 \\geq 2A \\). Therefore for \\( |\\theta - \\theta^*| \\geq R \\), \\( \\gamma_{\\theta}(y) \\leq -\\frac{1}{2} \\max(0, |\\theta - \\theta^*| - \\eta)^2 \\leq -A \\).\n\nConsider any \\( \\theta, \\theta' \\in [\\theta^* - R, \\theta^* + R] \\) with \\( \\alpha := |\\theta - \\theta'| \\). We have\n\n$$\n\\begin{align*}"}, {"type": "table", "rows": [["\\gamma_{\\theta}(y) - \\gamma_{\\theta'}(y)", "&\\leq", "\\eta(\\theta - \\theta')"]], "md": "|\\gamma_{\\theta}(y) - \\gamma_{\\theta'}(y)| &\\leq |\\eta(\\theta - \\theta')| + \\frac{1}{2} (\\theta' - \\theta^*)^2 - (\\theta - \\theta^*)^2 \\\\", "isPerfectTable": true, "csv": "\"\\gamma_{\\theta}(y) - \\gamma_{\\theta'}(y)\",\"&\\leq\",\"\\eta(\\theta - \\theta')\""}, {"type": "text", "value": "&\\leq B\\alpha + \\frac{1}{2} |(\\theta' - \\theta)(-2\\theta^* + (\\theta' + \\theta))| \\\\\n&\\leq B\\alpha + \\frac{1}{2}\\alpha(2R) = \\alpha(B + R).\n\\end{align*}\n$$\nThus, for \\( \\alpha = 2R \\), this is at most \\( \\epsilon \\). If we partition \\( [\\theta^* - R, \\theta^* + R] \\) into length-\\( \\alpha \\) intervals, we get a size \\( O(R^2/\\epsilon) = O(A/\\epsilon) \\) partition \\( P_1 \\) of \\( R \\) that has the desired property for all \\( y > 0 \\).\n\nOur final partition is defined by all endpoints in either \\( P_0 \\) and \\( P_1 \\). This has size \\( O(A/\\epsilon) \\), and within each interval the conclusion holds for both \\( y = 0 \\) and \\( y > 0 \\), as needed.", "md": "&\\leq B\\alpha + \\frac{1}{2} |(\\theta' - \\theta)(-2\\theta^* + (\\theta' + \\theta))| \\\\\n&\\leq B\\alpha + \\frac{1}{2}\\alpha(2R) = \\alpha(B + R).\n\\end{align*}\n$$\nThus, for \\( \\alpha = 2R \\), this is at most \\( \\epsilon \\). If we partition \\( [\\theta^* - R, \\theta^* + R] \\) into length-\\( \\alpha \\) intervals, we get a size \\( O(R^2/\\epsilon) = O(A/\\epsilon) \\) partition \\( P_1 \\) of \\( R \\) that has the desired property for all \\( y > 0 \\).\n\nOur final partition is defined by all endpoints in either \\( P_0 \\) and \\( P_1 \\). This has size \\( O(A/\\epsilon) \\), and within each interval the conclusion holds for both \\( y = 0 \\) and \\( y > 0 \\), as needed."}, {"type": "heading", "lvl": 2, "value": "Lemma B.4:", "md": "## Lemma B.4:"}, {"type": "text", "value": "Let \\( x_1, ..., x_n \\) be fixed, and \\( y_i \\sim \\phi(x_1^T) \\) the MLE \\( w \\) satisfies \\( d(w, w^*) \\leq i w^* \\epsilon + \\eta_i \\) for \\( \\eta_i \\sim N(0, 1) \\). For \\( n \\geq \\epsilon^2 k \\log \\frac{1}{\\epsilon} \\).\n\nProof: For any \\( w \\in \\mathbb{R}^k \\), and a sample \\( (x_i, y_i) \\), let \\( p_w(y|xi) \\) be the conditional distribution of \\( y = \\phi(\\langle x_i, w \\rangle + \\eta) \\), and let \\( \\gamma_{i,w} \\) be the log-likelihood ratio between \\( w \\) and \\( w^* \\) on this sample:\n\n$$\n\\gamma_{i,w}(y) := \\log p_w(y|xi)\n$$\nThen \\( E[p_w^*(y|xi)] \\). Define\n\n$$\ny[\\gamma_{i,w}(y)] = -d_{KL}(p_{i,w^*}(y|xi)||p_{i,w}(y|xi)).\n$$\n$$\nd_{KL}(w^*, w) := \\frac{1}{n} \\sum_{i=1}^{n} d_{KL}(p_{i,w^*}(y|xi)||p_{i,w}(y|xi)).\n$$", "md": "Let \\( x_1, ..., x_n \\) be fixed, and \\( y_i \\sim \\phi(x_1^T) \\) the MLE \\( w \\) satisfies \\( d(w, w^*) \\leq i w^* \\epsilon + \\eta_i \\) for \\( \\eta_i \\sim N(0, 1) \\). For \\( n \\geq \\epsilon^2 k \\log \\frac{1}{\\epsilon} \\).\n\nProof: For any \\( w \\in \\mathbb{R}^k \\), and a sample \\( (x_i, y_i) \\), let \\( p_w(y|xi) \\) be the conditional distribution of \\( y = \\phi(\\langle x_i, w \\rangle + \\eta) \\), and let \\( \\gamma_{i,w} \\) be the log-likelihood ratio between \\( w \\) and \\( w^* \\) on this sample:\n\n$$\n\\gamma_{i,w}(y) := \\log p_w(y|xi)\n$$\nThen \\( E[p_w^*(y|xi)] \\). Define\n\n$$\ny[\\gamma_{i,w}(y)] = -d_{KL}(p_{i,w^*}(y|xi)||p_{i,w}(y|xi)).\n$$\n$$\nd_{KL}(w^*, w) := \\frac{1}{n} \\sum_{i=1}^{n} d_{KL}(p_{i,w^*}(y|xi)||p_{i,w}(y|xi)).\n$$"}, {"type": "heading", "lvl": 2, "value": "Concentration:", "md": "## Concentration:"}, {"type": "text", "value": "From Lemma B.1, we see that if \\( n \\cdot d(w^*, w) \\geq \\epsilon \\), then for \\( n \\geq O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\delta}) \\), with probability \\( 1 - \\delta \\),\n\n$$\nw := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{i,w}(y_i) < -\\frac{\\epsilon^2}{4} \\quad (18)\n$$\nOf course, whenever \\( \\gamma_w < 0 \\), the likelihood under \\( w^* \\) is larger than the likelihood under \\( w \\). Thus, for each fixed \\( w \\) with \\( d(w^*, w) \\geq \\epsilon \\), maximizing likelihood would prefer \\( w^* \\) to \\( w \\) with probability \\( 1 - \\delta \\) if \\( n \\geq O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\delta}) \\).\n\nNothing above is specific to our ReLU-based distribution. But to extend to the MLE over all \\( w \\), we need to build a net using properties of our distribution.", "md": "From Lemma B.1, we see that if \\( n \\cdot d(w^*, w) \\geq \\epsilon \\), then for \\( n \\geq O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\delta}) \\), with probability \\( 1 - \\delta \\),\n\n$$\nw := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{i,w}(y_i) < -\\frac{\\epsilon^2}{4} \\quad (18)\n$$\nOf course, whenever \\( \\gamma_w < 0 \\), the likelihood under \\( w^* \\) is larger than the likelihood under \\( w \\). Thus, for each fixed \\( w \\) with \\( d(w^*, w) \\geq \\epsilon \\), maximizing likelihood would prefer \\( w^* \\) to \\( w \\) with probability \\( 1 - \\delta \\) if \\( n \\geq O(\\frac{1}{\\epsilon^2} \\log \\frac{1}{\\delta}) \\).\n\nNothing above is specific to our ReLU-based distribution. But to extend to the MLE over all \\( w \\), we need to build a net using properties of our distribution."}]}, {"page": 21, "text": " Building a net.          First, with high probability, |\u03b7i| \u2264              B = O(\u221alog n) for all i. Suppose this happens.\n For each i, by an abuse of notation, let \u03b3i,w(y) = \u03b3\u27e8xi,w\u27e9(y) where the value of \u03b8\u2217                                   when considering\n i is \u27e8xi, w\u2217\u27e9. By Lemma B.2,                               \u03b3i,w(yi) \u2264       B2/2\n for all i. Let A = O(n log n) > nB2. By Lemma B.3, for each i \u2208                                  [n], there exists a partition Pi of\n R into O(A/\u03b52) intervals, such that for interval I \u2208                       Pi, and any w, w\u2032 with xT           i w, xT  i w\u2032 \u2208    I, either\n or \u03b3i,w(yi) < \u2212A.                                 |\u03b3i,w(yi) \u2212      \u03b3i,w\u2032(yi)| \u2264       \u03b52/2                                               (19)\n These individual partitions Pi on \u27e8xi, w\u27e9                 induce a partition P on Rk, where w, w\u2032 lie in the same cell\n of P if \u27e8xi, w\u27e9       and \u27e8xi, w\u2217\u27e9       are in the same cell of Pi for all i \u2208              [n]. Since P is defined by n sets of\n O    A    parallel hyperplanes in Rk, the number of cells in P is:\n      \u03b52\n                                                               2  2Aen k         .\n                                                                     \u03b52k\nWe choose a net N to contain, for each cell in P                        , the w in the cell maximizing              d(w\u2217, w). This has\n size                                                      log|N    | \u2272   k log n \u03b5 .\n By (18), for our n \u2265           O     1             , we have with high probability that \u03b3w \u2264                      \u2212\u03b52\n                                     \u03b52 k log k  \u03b5                                                                     4 , for all w \u2208       N\n with   d(w\u2217, w) \u2265        \u03b5. Suppose that both this happens, and |\u03b7i| \u2264                    B for all i. We claim that the MLE                w\n must have      d(w\u2217,    w) < \u03b5.\n Consider any w \u2208          Rd with      dT V (w\u2217, w) \u2265        \u03b5. Let w\u2032 \u2208       N lie in the same cell of P           . By our choice of\n N  , we know       dT V (w\u2217, w\u2032) \u2265         dT V (w\u2217, w) \u2265         \u03b5, so \u03b3w\u2032 \u2264       \u2212\u03b52. Now we consider two cases. In the\n first case, there exists i with \u03b3i,w(yi) < \u2212A. Then\n Otherwise, by (19),                     \u03b3 w = 1   n    i   \u03b3 i,w(yi) \u2264      \u2212A n + B2/2 < 0.\n                    \u03b3 w \u2264     \u03b3w\u2032 + |\u03b3w \u2212        \u03b3w\u2032| \u2264     \u2212\u03b52 + max     i |\u03b3i,w(yi) \u2212        \u03b3i,w\u2032(yi)| \u2264       \u2212\u03b52/2.\n In either case, \u03b3      w < 0 and the likelihood under w\u2217                   exceeds that under w. Hence the MLE                      w must\n have   d(w\u2217,    w) \u2264     \u03b5.\n Theorem B.5. Let y = \u03d5                   xT w\u2217     + \u03b7    , for w\u2217      \u2208   Rk, x \u223c        Dx, and \u03b7 \u223c          N   (0, 1). Then for a\n sufficiently large constant C > 0,\n                                                            n = C \u00b7 k    \u03b52 log 1  \u03b5\n samples of {(yi, xi)}n        i=1 suffices to guarantee that the MLE                   w satisfies\n                                                              d( w, w\u2217) \u2264       \u03b5.\n Proof. Let Dx denote the dataset {xi}i\u2208[n] that is used to find the MLE. Notice that the MLE is\n found using this finite subset, but we would like to make a claim about Dx without making any\n parametric or simplifying assumptions on the distribution Dx.\n An application of Lemma 4.3 tells us that with probability 1 \u2212                               e\u2212\u2126(n\u03b52), the expectation over the\n distribution Dx and the dataset Dx are within \u03b5/2 of one another:\n                                                   d(  w, w\u2217) \u2264      d(  w, w\u2217) + \u03b5/2.\n Now, all we need to show is that the MLE has a small TV distance on the finite dataset, and Lemma B.4\n tells us that with probability 1 \u2212             e\u2212\u2126(n\u03b52),\n Substituting in the above inequality, we get d(            d( w, w\u2217) \u2264       \u03b5/2.\n                                                                   w, w\u2217) \u2264       \u03b5.\n                                                                       21", "md": "# Math Equations\n\nBuilding a net. First, with high probability, $$|\\eta_i| \\leq B = O(\\sqrt{\\log n})$$ for all i. Suppose this happens.\n\nFor each i, by an abuse of notation, let $$\\gamma_{i,w}(y) = \\gamma\\langle x_i, w\\rangle(y)$$ where the value of $$\\theta^*$$ when considering i is $$\\langle x_i, w^*\\rangle$$. By Lemma B.2, $$\\gamma_{i,w}(y_i) \\leq \\frac{B^2}{2}$$ for all i. Let A = O(n log n) > nB^2. By Lemma B.3, for each i \u2208 [n], there exists a partition Pi of R into O(A/\\epsilon^2) intervals, such that for interval I \u2208 Pi, and any w, w' with $$x_i^Tw, x_i^Tw' \\in I$$, either or $$\\gamma_{i,w}(y_i) < -A$$. $$|\\gamma_{i,w}(y_i) - \\gamma_{i,w'}(y_i)| \\leq \\frac{\\epsilon^2}{2}$$ (19)\n\nThese individual partitions Pi on $$\\langle x_i, w\\rangle$$ induce a partition P on Rk, where w, w' lie in the same cell of P if $$\\langle x_i, w\\rangle$$ and $$\\langle x_i, w^*\\rangle$$ are in the same cell of Pi for all i \u2208 [n]. Since P is defined by n sets of O(A) parallel hyperplanes in Rk, the number of cells in P is: $$\\frac{\\epsilon^2}{2A\\epsilon k} = \\frac{2Aen}{\\epsilon^2k}$$.\n\nWe choose a net N to contain, for each cell in P, the w in the cell maximizing $$d(w^*, w)$$. This has size $$\\log|N| \\lesssim k \\log n \\epsilon$$.\n\nBy (18), for our n \u2265 O(1), we have with high probability that $$\\gamma_w \\leq -\\frac{\\epsilon^2}{k \\log k \\epsilon 4}$$, for all w \u2208 N with $$d(w^*, w) \\geq \\epsilon$$. Suppose that both this happens, and $$|\\eta_i| \\leq B$$ for all i. We claim that the MLE w must have $$d(w^*, w) < \\epsilon$$.\n\nConsider any w \u2208 Rd with $$d^TV(w^*, w) \\geq \\epsilon$$. Let w' \u2208 N lie in the same cell of P. By our choice of N, we know $$d^TV(w^*, w') \\geq d^TV(w^*, w) \\geq \\epsilon$$, so $$\\gamma_{w'} \\leq -\\epsilon^2$$. Now we consider two cases. In the first case, there exists i with $$\\gamma_{i,w}(y_i) < -A$$. Then\n\nOtherwise, by (19), $$\\gamma_w = \\frac{1}{n} \\sum_{i} \\gamma_{i,w}(y_i) \\leq -\\frac{A}{n} + \\frac{B^2}{2} < 0$$.\n\n$$\\gamma_w \\leq \\gamma_{w'} + |\\gamma_w - \\gamma_{w'}| \\leq -\\epsilon^2 + \\max_i |\\gamma_{i,w}(y_i) - \\gamma_{i,w'}(y_i)| \\leq -\\frac{\\epsilon^2}{2}$$. In either case, $$\\gamma_w < 0$$ and the likelihood under w* exceeds that under w. Hence the MLE w must have $$d(w^*, w) \\leq \\epsilon$$.\n\nTheorem B.5. Let y = $$\\phi x^Tw^* + \\eta$$, for $$w^* \\in Rk$$, x ~ Dx, and $$\\eta \\sim N(0,1)$$. Then for a sufficiently large constant C > 0,\n\n$$n = C \\cdot k \\epsilon^2 \\log \\frac{1}{\\epsilon}$$ samples of {($$y_i, x_i$$)}i=1n suffices to guarantee that the MLE w satisfies $$d(w, w^*) \\leq \\epsilon$$.\n\nProof. Let Dx denote the dataset {xi}i\u2208[n] that is used to find the MLE. Notice that the MLE is found using this finite subset, but we would like to make a claim about Dx without making any parametric or simplifying assumptions on the distribution Dx.\n\nAn application of Lemma 4.3 tells us that with probability 1 - $$e^{-\\Omega(n\\epsilon^2)}$$, the expectation over the distribution Dx and the dataset Dx are within $$\\epsilon/2$$ of one another: $$d(w, w^*) \\leq d(w, w^*) + \\epsilon/2$$.\n\nNow, all we need to show is that the MLE has a small TV distance on the finite dataset, and Lemma B.4 tells us that with probability 1 - $$e^{-\\Omega(n\\epsilon^2)}$$,\n\nSubstituting in the above inequality, we get $$d(w, w^*) \\leq \\epsilon/2$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Building a net. First, with high probability, $$|\\eta_i| \\leq B = O(\\sqrt{\\log n})$$ for all i. Suppose this happens.\n\nFor each i, by an abuse of notation, let $$\\gamma_{i,w}(y) = \\gamma\\langle x_i, w\\rangle(y)$$ where the value of $$\\theta^*$$ when considering i is $$\\langle x_i, w^*\\rangle$$. By Lemma B.2, $$\\gamma_{i,w}(y_i) \\leq \\frac{B^2}{2}$$ for all i. Let A = O(n log n) > nB^2. By Lemma B.3, for each i \u2208 [n], there exists a partition Pi of R into O(A/\\epsilon^2) intervals, such that for interval I \u2208 Pi, and any w, w' with $$x_i^Tw, x_i^Tw' \\in I$$, either or $$\\gamma_{i,w}(y_i) < -A$$. $$|\\gamma_{i,w}(y_i) - \\gamma_{i,w'}(y_i)| \\leq \\frac{\\epsilon^2}{2}$$ (19)\n\nThese individual partitions Pi on $$\\langle x_i, w\\rangle$$ induce a partition P on Rk, where w, w' lie in the same cell of P if $$\\langle x_i, w\\rangle$$ and $$\\langle x_i, w^*\\rangle$$ are in the same cell of Pi for all i \u2208 [n]. Since P is defined by n sets of O(A) parallel hyperplanes in Rk, the number of cells in P is: $$\\frac{\\epsilon^2}{2A\\epsilon k} = \\frac{2Aen}{\\epsilon^2k}$$.\n\nWe choose a net N to contain, for each cell in P, the w in the cell maximizing $$d(w^*, w)$$. This has size $$\\log|N| \\lesssim k \\log n \\epsilon$$.\n\nBy (18), for our n \u2265 O(1), we have with high probability that $$\\gamma_w \\leq -\\frac{\\epsilon^2}{k \\log k \\epsilon 4}$$, for all w \u2208 N with $$d(w^*, w) \\geq \\epsilon$$. Suppose that both this happens, and $$|\\eta_i| \\leq B$$ for all i. We claim that the MLE w must have $$d(w^*, w) < \\epsilon$$.\n\nConsider any w \u2208 Rd with $$d^TV(w^*, w) \\geq \\epsilon$$. Let w' \u2208 N lie in the same cell of P. By our choice of N, we know $$d^TV(w^*, w') \\geq d^TV(w^*, w) \\geq \\epsilon$$, so $$\\gamma_{w'} \\leq -\\epsilon^2$$. Now we consider two cases. In the first case, there exists i with $$\\gamma_{i,w}(y_i) < -A$$. Then\n\nOtherwise, by (19), $$\\gamma_w = \\frac{1}{n} \\sum_{i} \\gamma_{i,w}(y_i) \\leq -\\frac{A}{n} + \\frac{B^2}{2} < 0$$.\n\n$$\\gamma_w \\leq \\gamma_{w'} + |\\gamma_w - \\gamma_{w'}| \\leq -\\epsilon^2 + \\max_i |\\gamma_{i,w}(y_i) - \\gamma_{i,w'}(y_i)| \\leq -\\frac{\\epsilon^2}{2}$$. In either case, $$\\gamma_w < 0$$ and the likelihood under w* exceeds that under w. Hence the MLE w must have $$d(w^*, w) \\leq \\epsilon$$.\n\nTheorem B.5. Let y = $$\\phi x^Tw^* + \\eta$$, for $$w^* \\in Rk$$, x ~ Dx, and $$\\eta \\sim N(0,1)$$. Then for a sufficiently large constant C > 0,\n\n$$n = C \\cdot k \\epsilon^2 \\log \\frac{1}{\\epsilon}$$ samples of {($$y_i, x_i$$)}i=1n suffices to guarantee that the MLE w satisfies $$d(w, w^*) \\leq \\epsilon$$.\n\nProof. Let Dx denote the dataset {xi}i\u2208[n] that is used to find the MLE. Notice that the MLE is found using this finite subset, but we would like to make a claim about Dx without making any parametric or simplifying assumptions on the distribution Dx.\n\nAn application of Lemma 4.3 tells us that with probability 1 - $$e^{-\\Omega(n\\epsilon^2)}$$, the expectation over the distribution Dx and the dataset Dx are within $$\\epsilon/2$$ of one another: $$d(w, w^*) \\leq d(w, w^*) + \\epsilon/2$$.\n\nNow, all we need to show is that the MLE has a small TV distance on the finite dataset, and Lemma B.4 tells us that with probability 1 - $$e^{-\\Omega(n\\epsilon^2)}$$,\n\nSubstituting in the above inequality, we get $$d(w, w^*) \\leq \\epsilon/2$$.", "md": "Building a net. First, with high probability, $$|\\eta_i| \\leq B = O(\\sqrt{\\log n})$$ for all i. Suppose this happens.\n\nFor each i, by an abuse of notation, let $$\\gamma_{i,w}(y) = \\gamma\\langle x_i, w\\rangle(y)$$ where the value of $$\\theta^*$$ when considering i is $$\\langle x_i, w^*\\rangle$$. By Lemma B.2, $$\\gamma_{i,w}(y_i) \\leq \\frac{B^2}{2}$$ for all i. Let A = O(n log n) > nB^2. By Lemma B.3, for each i \u2208 [n], there exists a partition Pi of R into O(A/\\epsilon^2) intervals, such that for interval I \u2208 Pi, and any w, w' with $$x_i^Tw, x_i^Tw' \\in I$$, either or $$\\gamma_{i,w}(y_i) < -A$$. $$|\\gamma_{i,w}(y_i) - \\gamma_{i,w'}(y_i)| \\leq \\frac{\\epsilon^2}{2}$$ (19)\n\nThese individual partitions Pi on $$\\langle x_i, w\\rangle$$ induce a partition P on Rk, where w, w' lie in the same cell of P if $$\\langle x_i, w\\rangle$$ and $$\\langle x_i, w^*\\rangle$$ are in the same cell of Pi for all i \u2208 [n]. Since P is defined by n sets of O(A) parallel hyperplanes in Rk, the number of cells in P is: $$\\frac{\\epsilon^2}{2A\\epsilon k} = \\frac{2Aen}{\\epsilon^2k}$$.\n\nWe choose a net N to contain, for each cell in P, the w in the cell maximizing $$d(w^*, w)$$. This has size $$\\log|N| \\lesssim k \\log n \\epsilon$$.\n\nBy (18), for our n \u2265 O(1), we have with high probability that $$\\gamma_w \\leq -\\frac{\\epsilon^2}{k \\log k \\epsilon 4}$$, for all w \u2208 N with $$d(w^*, w) \\geq \\epsilon$$. Suppose that both this happens, and $$|\\eta_i| \\leq B$$ for all i. We claim that the MLE w must have $$d(w^*, w) < \\epsilon$$.\n\nConsider any w \u2208 Rd with $$d^TV(w^*, w) \\geq \\epsilon$$. Let w' \u2208 N lie in the same cell of P. By our choice of N, we know $$d^TV(w^*, w') \\geq d^TV(w^*, w) \\geq \\epsilon$$, so $$\\gamma_{w'} \\leq -\\epsilon^2$$. Now we consider two cases. In the first case, there exists i with $$\\gamma_{i,w}(y_i) < -A$$. Then\n\nOtherwise, by (19), $$\\gamma_w = \\frac{1}{n} \\sum_{i} \\gamma_{i,w}(y_i) \\leq -\\frac{A}{n} + \\frac{B^2}{2} < 0$$.\n\n$$\\gamma_w \\leq \\gamma_{w'} + |\\gamma_w - \\gamma_{w'}| \\leq -\\epsilon^2 + \\max_i |\\gamma_{i,w}(y_i) - \\gamma_{i,w'}(y_i)| \\leq -\\frac{\\epsilon^2}{2}$$. In either case, $$\\gamma_w < 0$$ and the likelihood under w* exceeds that under w. Hence the MLE w must have $$d(w^*, w) \\leq \\epsilon$$.\n\nTheorem B.5. Let y = $$\\phi x^Tw^* + \\eta$$, for $$w^* \\in Rk$$, x ~ Dx, and $$\\eta \\sim N(0,1)$$. Then for a sufficiently large constant C > 0,\n\n$$n = C \\cdot k \\epsilon^2 \\log \\frac{1}{\\epsilon}$$ samples of {($$y_i, x_i$$)}i=1n suffices to guarantee that the MLE w satisfies $$d(w, w^*) \\leq \\epsilon$$.\n\nProof. Let Dx denote the dataset {xi}i\u2208[n] that is used to find the MLE. Notice that the MLE is found using this finite subset, but we would like to make a claim about Dx without making any parametric or simplifying assumptions on the distribution Dx.\n\nAn application of Lemma 4.3 tells us that with probability 1 - $$e^{-\\Omega(n\\epsilon^2)}$$, the expectation over the distribution Dx and the dataset Dx are within $$\\epsilon/2$$ of one another: $$d(w, w^*) \\leq d(w, w^*) + \\epsilon/2$$.\n\nNow, all we need to show is that the MLE has a small TV distance on the finite dataset, and Lemma B.4 tells us that with probability 1 - $$e^{-\\Omega(n\\epsilon^2)}$$,\n\nSubstituting in the above inequality, we get $$d(w, w^*) \\leq \\epsilon/2$$."}]}, {"page": 22, "text": " C      ReLU Activations with d > 1, Unknown Covariance\nWe recommend the reader review Appendix B, which contains the proof recipe for the case of scalar\n y. The proofs in this section generalize those of Appendix B.\n Consider a sample (x, y) \u2208              Rk\u00d7d, with        y = \u03d5(W \u2217x + \u03b7),                                                               (20)\n where W \u2217        \u2208   Rd \u00d7 k, and noise \u03b7 \u223c                 N  (0, \u03a3\u2217). The matrices W \u2217                and \u03a3\u2217       are unknown. For\n each matrix W \u2208            Rd\u00d7k, let \u03b8 = W           x \u2208    Rd, denote a reparametrization of W                     , and let \u03b8\u2217     denote\n \u03b8\u2217  = W \u2217x. Let S denote the co-ordinates of y that are zero-valued. Then the log-likelihood for each\n \u03b8, \u03a3 is given by\n f\u03b8,\u03a3(y) := log pW,\u03a3(y | x) = c \u2212                   1                                            exp     \u2212(t \u2212     \u03b8)T \u03a3\u22121(t \u2212        \u03b8)/2     .\n                                                    2 log|\u03a3| + log         t:tS\u22640,tSc=ySc\n where c is a normalization constant which does not depend on \u03b8 or \u03a3. Let\n                                                                 P := \u03a3\u22121\n and let P \u2217     be the precision matrix of the noise \u03b7, and PS, PSSc, PScS, PSc be the block matrices of\n P corresponding to the index sets S and its complement Sc.\n By some arithmetic involving completion of squares, we can decompose the integral in f into the\n sum of two functions g, h, such that\n                                       f\u03b8,\u03a3(y) = c \u2212         1\n                                                             2 log|\u03a3| + g\u03b8,\u03a3(y) + h\u03b8,\u03a3(y).\n The first term g corresponds to the quadratic term involving the observed positive-valued coordinates\n ySc:\n                      g\u03b8,\u03a3(y) = \u2212(ySc \u2212             \u03b8Sc)T (PSc \u2212        PScS(PS)\u22121PSSc)(ySc \u2212                  \u03b8Sc)/2.\n As the matrix PSc \u2212            PScS(PS)\u22121PSSc = ((P \u22121)Sc)\u22121 = \u03a3\u22121                       Sc is the precision matrix of \u03b7S, if \u03a3\n were the covariance of \u03b7, we can simplify the above equation as\n                                    g\u03b8,\u03a3(y) = \u2212(ySc \u2212            \u03b8Sc)T (\u03a3Sc)\u22121(ySc \u2212              \u03b8Sc)/2.                                 (21)\n The second term corresponds to the probability under \u03b8, P of observing zero-valued coordinates\n corresponding to the index set S, given the positive coordinates ySc:\n                                                             1\n                                                             2\n                h\u03b8,\u03a3(y) = log           t\u22640  exp     \u2212\u2225PS (t \u2212        \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212                  \u03b8Sc)\u22252/2        .         (22)\n The log-likelihood ratio is the difference between f\u03b8,\u03a3 and f\u03b8\u2217,\u03a3\u2217, which we denote by\n                                                 \u03b3\u03b8,\u03a3(y) := f\u03b8,\u03a3(y) \u2212             f\u03b8\u2217,\u03a3\u2217(y)\n Over a dataset {(xi, yi)}i\u2208[n], the average log-likelihood ratio is given by\n                                                    \u00af\n                                                    \u03b3W,\u03a3 := 1     n    i   \u03b3W x   i,\u03a3(yi).\n Remark C.1. For ease of analysis, we will interchange between the precision matrix P in \u03b3\u03b8,P and\n the covariance matrix \u03a3 in \u03b3\u03b8,\u03a3, and it should be understood that P = \u03a3\u22121. The same applies to the\n functions g\u03b8,\u03a3 and h\u03b8,\u03a3. Finally, the matrix P \u2217                    refers to the ground truth precision matrix (= \u03a3\u2217\u22121).\n Analogous to Appendix B, we start by showing that the log-likelihood ratio is bounded by the noise\n in the sample. The proofs of results in this Section are in Subsection C.1.\n                                                                       22", "md": "ReLU Activations with d &gt; 1, Unknown Covariance\n\nWe recommend the reader review Appendix B, which contains the proof recipe for the case of scalar\ny. The proofs in this section generalize those of Appendix B.\nConsider a sample (x, y) \u2208 $$\\mathbb{R}^{k \\times d}$$, with $$y = \\phi(W^*x + \\eta)$$, $$(20)$$\nwhere $$W^* \\in \\mathbb{R}^{d \\times k}$$, and noise $$\\eta \\sim \\mathcal{N}(0, \\Sigma^*)$$. The matrices $$W^*$$ and $$\\Sigma^*$$ are unknown. For\neach matrix $$W \\in \\mathbb{R}^{d \\times k}$$, let $$\\theta = Wx \\in \\mathbb{R}^d$$, denote a reparametrization of $$W$$, and let $$\\theta^*$$ denote\n$$\\theta^* = W^*x$$. Let $$S$$ denote the co-ordinates of $$y$$ that are zero-valued. Then the log-likelihood for each\n$$\\theta, \\Sigma$$ is given by\n$$f_{\\theta, \\Sigma}(y) := \\log p_{W, \\Sigma}(y | x) = c - \\frac{1}{2} \\log|\\Sigma| + \\log \\left\\{ t : t_S \\leq 0, t_{S^c} = y_{S^c} \\right\\} \\exp \\left\\{ - (t - \\theta)^T \\Sigma^{-1} (t - \\theta)/2 \\right\\}$$.\n\nwhere $$c$$ is a normalization constant which does not depend on $$\\theta$$ or $$\\Sigma$$. Let\n$$P := \\Sigma^{-1}$$\nand let $$P^*$$ be the precision matrix of the noise $$\\eta$$, and $$P_S$$, $$P_{S^c}$$, $$P_{S^cS}$$, $$P_{Sc}$$ be the block matrices of\n$$P$$ corresponding to the index sets $$S$$ and its complement $$S^c$$.\nBy some arithmetic involving completion of squares, we can decompose the integral in $$f$$ into the\nsum of two functions $$g$$, $$h$$, such that\n$$f_{\\theta, \\Sigma}(y) = c - \\frac{1}{2} \\log|\\Sigma| + g_{\\theta, \\Sigma}(y) + h_{\\theta, \\Sigma}(y)$$.\nThe first term $$g$$ corresponds to the quadratic term involving the observed positive-valued coordinates\n$$y_{S^c}$$:\n$$g_{\\theta, \\Sigma}(y) = - (y_{S^c} - \\theta_{S^c})^T \\left( P_{Sc} - P_{ScS}(PS)^{-1}P_{SSc} \\right) (y_{S^c} - \\theta_{S^c})/2$$.\nAs the matrix $$P_{Sc} - P_{ScS}(PS)^{-1}P_{SSc} = ((P^{-1})_{Sc})^{-1} = \\Sigma^{-1}_{Sc}$$ is the precision matrix of $$\\eta_S$$, if $$\\Sigma$$\nwere the covariance of $$\\eta$$, we can simplify the above equation as\n$$g_{\\theta, \\Sigma}(y) = - (y_{S^c} - \\theta_{S^c})^T (\\Sigma_{Sc})^{-1} (y_{S^c} - \\theta_{S^c})/2$$. $$(21)$$\nThe second term corresponds to the probability under $$\\theta, P$$ of observing zero-valued coordinates\ncorresponding to the index set $$S$$, given the positive coordinates $$y_{S^c}$$:\n$$h_{\\theta, \\Sigma}(y) = \\log \\left\\{ t \\leq 0 \\right\\} \\exp \\left\\{ - \\|PS (t - \\theta_S) + (PS)^{-1/2}P_{SSc}(y_{S^c} - \\theta_{S^c})\\|^2/2 \\right\\}$$. $$(22)$$\nThe log-likelihood ratio is the difference between $$f_{\\theta, \\Sigma}$$ and $$f_{\\theta^*, \\Sigma^*}$$, which we denote by\n$$\\gamma_{\\theta, \\Sigma}(y) := f_{\\theta, \\Sigma}(y) - f_{\\theta^*, \\Sigma^*}(y)$$\nOver a dataset $${(x_i, y_i)}_{i \\in [n]}$$, the average log-likelihood ratio is given by\n$$\\overline{\\gamma}_{W, \\Sigma} := \\frac{1}{n} \\sum_{i} \\gamma_{W x_i, \\Sigma}(y_i)$$.\nRemark C.1. For ease of analysis, we will interchange between the precision matrix $$P$$ in $$\\gamma_{\\theta, P}$$ and\nthe covariance matrix $$\\Sigma$$ in $$\\gamma_{\\theta, \\Sigma}$$, and it should be understood that $$P = \\Sigma^{-1}$$. The same applies to the\nfunctions $$g_{\\theta, \\Sigma}$$ and $$h_{\\theta, \\Sigma}$$. Finally, the matrix $$P^*$$ refers to the ground truth precision matrix (= $$\\Sigma^*{-1}$$).\nAnalogous to Appendix B, we start by showing that the log-likelihood ratio is bounded by the noise\nin the sample. The proofs of results in this Section are in Subsection C.1.", "images": [], "items": [{"type": "text", "value": "ReLU Activations with d &gt; 1, Unknown Covariance\n\nWe recommend the reader review Appendix B, which contains the proof recipe for the case of scalar\ny. The proofs in this section generalize those of Appendix B.\nConsider a sample (x, y) \u2208 $$\\mathbb{R}^{k \\times d}$$, with $$y = \\phi(W^*x + \\eta)$$, $$(20)$$\nwhere $$W^* \\in \\mathbb{R}^{d \\times k}$$, and noise $$\\eta \\sim \\mathcal{N}(0, \\Sigma^*)$$. The matrices $$W^*$$ and $$\\Sigma^*$$ are unknown. For\neach matrix $$W \\in \\mathbb{R}^{d \\times k}$$, let $$\\theta = Wx \\in \\mathbb{R}^d$$, denote a reparametrization of $$W$$, and let $$\\theta^*$$ denote\n$$\\theta^* = W^*x$$. Let $$S$$ denote the co-ordinates of $$y$$ that are zero-valued. Then the log-likelihood for each\n$$\\theta, \\Sigma$$ is given by\n$$f_{\\theta, \\Sigma}(y) := \\log p_{W, \\Sigma}(y | x) = c - \\frac{1}{2} \\log|\\Sigma| + \\log \\left\\{ t : t_S \\leq 0, t_{S^c} = y_{S^c} \\right\\} \\exp \\left\\{ - (t - \\theta)^T \\Sigma^{-1} (t - \\theta)/2 \\right\\}$$.\n\nwhere $$c$$ is a normalization constant which does not depend on $$\\theta$$ or $$\\Sigma$$. Let\n$$P := \\Sigma^{-1}$$\nand let $$P^*$$ be the precision matrix of the noise $$\\eta$$, and $$P_S$$, $$P_{S^c}$$, $$P_{S^cS}$$, $$P_{Sc}$$ be the block matrices of\n$$P$$ corresponding to the index sets $$S$$ and its complement $$S^c$$.\nBy some arithmetic involving completion of squares, we can decompose the integral in $$f$$ into the\nsum of two functions $$g$$, $$h$$, such that\n$$f_{\\theta, \\Sigma}(y) = c - \\frac{1}{2} \\log|\\Sigma| + g_{\\theta, \\Sigma}(y) + h_{\\theta, \\Sigma}(y)$$.\nThe first term $$g$$ corresponds to the quadratic term involving the observed positive-valued coordinates\n$$y_{S^c}$$:\n$$g_{\\theta, \\Sigma}(y) = - (y_{S^c} - \\theta_{S^c})^T \\left( P_{Sc} - P_{ScS}(PS)^{-1}P_{SSc} \\right) (y_{S^c} - \\theta_{S^c})/2$$.\nAs the matrix $$P_{Sc} - P_{ScS}(PS)^{-1}P_{SSc} = ((P^{-1})_{Sc})^{-1} = \\Sigma^{-1}_{Sc}$$ is the precision matrix of $$\\eta_S$$, if $$\\Sigma$$\nwere the covariance of $$\\eta$$, we can simplify the above equation as\n$$g_{\\theta, \\Sigma}(y) = - (y_{S^c} - \\theta_{S^c})^T (\\Sigma_{Sc})^{-1} (y_{S^c} - \\theta_{S^c})/2$$. $$(21)$$\nThe second term corresponds to the probability under $$\\theta, P$$ of observing zero-valued coordinates\ncorresponding to the index set $$S$$, given the positive coordinates $$y_{S^c}$$:\n$$h_{\\theta, \\Sigma}(y) = \\log \\left\\{ t \\leq 0 \\right\\} \\exp \\left\\{ - \\|PS (t - \\theta_S) + (PS)^{-1/2}P_{SSc}(y_{S^c} - \\theta_{S^c})\\|^2/2 \\right\\}$$. $$(22)$$\nThe log-likelihood ratio is the difference between $$f_{\\theta, \\Sigma}$$ and $$f_{\\theta^*, \\Sigma^*}$$, which we denote by\n$$\\gamma_{\\theta, \\Sigma}(y) := f_{\\theta, \\Sigma}(y) - f_{\\theta^*, \\Sigma^*}(y)$$\nOver a dataset $${(x_i, y_i)}_{i \\in [n]}$$, the average log-likelihood ratio is given by\n$$\\overline{\\gamma}_{W, \\Sigma} := \\frac{1}{n} \\sum_{i} \\gamma_{W x_i, \\Sigma}(y_i)$$.\nRemark C.1. For ease of analysis, we will interchange between the precision matrix $$P$$ in $$\\gamma_{\\theta, P}$$ and\nthe covariance matrix $$\\Sigma$$ in $$\\gamma_{\\theta, \\Sigma}$$, and it should be understood that $$P = \\Sigma^{-1}$$. The same applies to the\nfunctions $$g_{\\theta, \\Sigma}$$ and $$h_{\\theta, \\Sigma}$$. Finally, the matrix $$P^*$$ refers to the ground truth precision matrix (= $$\\Sigma^*{-1}$$).\nAnalogous to Appendix B, we start by showing that the log-likelihood ratio is bounded by the noise\nin the sample. The proofs of results in this Section are in Subsection C.1.", "md": "ReLU Activations with d &gt; 1, Unknown Covariance\n\nWe recommend the reader review Appendix B, which contains the proof recipe for the case of scalar\ny. The proofs in this section generalize those of Appendix B.\nConsider a sample (x, y) \u2208 $$\\mathbb{R}^{k \\times d}$$, with $$y = \\phi(W^*x + \\eta)$$, $$(20)$$\nwhere $$W^* \\in \\mathbb{R}^{d \\times k}$$, and noise $$\\eta \\sim \\mathcal{N}(0, \\Sigma^*)$$. The matrices $$W^*$$ and $$\\Sigma^*$$ are unknown. For\neach matrix $$W \\in \\mathbb{R}^{d \\times k}$$, let $$\\theta = Wx \\in \\mathbb{R}^d$$, denote a reparametrization of $$W$$, and let $$\\theta^*$$ denote\n$$\\theta^* = W^*x$$. Let $$S$$ denote the co-ordinates of $$y$$ that are zero-valued. Then the log-likelihood for each\n$$\\theta, \\Sigma$$ is given by\n$$f_{\\theta, \\Sigma}(y) := \\log p_{W, \\Sigma}(y | x) = c - \\frac{1}{2} \\log|\\Sigma| + \\log \\left\\{ t : t_S \\leq 0, t_{S^c} = y_{S^c} \\right\\} \\exp \\left\\{ - (t - \\theta)^T \\Sigma^{-1} (t - \\theta)/2 \\right\\}$$.\n\nwhere $$c$$ is a normalization constant which does not depend on $$\\theta$$ or $$\\Sigma$$. Let\n$$P := \\Sigma^{-1}$$\nand let $$P^*$$ be the precision matrix of the noise $$\\eta$$, and $$P_S$$, $$P_{S^c}$$, $$P_{S^cS}$$, $$P_{Sc}$$ be the block matrices of\n$$P$$ corresponding to the index sets $$S$$ and its complement $$S^c$$.\nBy some arithmetic involving completion of squares, we can decompose the integral in $$f$$ into the\nsum of two functions $$g$$, $$h$$, such that\n$$f_{\\theta, \\Sigma}(y) = c - \\frac{1}{2} \\log|\\Sigma| + g_{\\theta, \\Sigma}(y) + h_{\\theta, \\Sigma}(y)$$.\nThe first term $$g$$ corresponds to the quadratic term involving the observed positive-valued coordinates\n$$y_{S^c}$$:\n$$g_{\\theta, \\Sigma}(y) = - (y_{S^c} - \\theta_{S^c})^T \\left( P_{Sc} - P_{ScS}(PS)^{-1}P_{SSc} \\right) (y_{S^c} - \\theta_{S^c})/2$$.\nAs the matrix $$P_{Sc} - P_{ScS}(PS)^{-1}P_{SSc} = ((P^{-1})_{Sc})^{-1} = \\Sigma^{-1}_{Sc}$$ is the precision matrix of $$\\eta_S$$, if $$\\Sigma$$\nwere the covariance of $$\\eta$$, we can simplify the above equation as\n$$g_{\\theta, \\Sigma}(y) = - (y_{S^c} - \\theta_{S^c})^T (\\Sigma_{Sc})^{-1} (y_{S^c} - \\theta_{S^c})/2$$. $$(21)$$\nThe second term corresponds to the probability under $$\\theta, P$$ of observing zero-valued coordinates\ncorresponding to the index set $$S$$, given the positive coordinates $$y_{S^c}$$:\n$$h_{\\theta, \\Sigma}(y) = \\log \\left\\{ t \\leq 0 \\right\\} \\exp \\left\\{ - \\|PS (t - \\theta_S) + (PS)^{-1/2}P_{SSc}(y_{S^c} - \\theta_{S^c})\\|^2/2 \\right\\}$$. $$(22)$$\nThe log-likelihood ratio is the difference between $$f_{\\theta, \\Sigma}$$ and $$f_{\\theta^*, \\Sigma^*}$$, which we denote by\n$$\\gamma_{\\theta, \\Sigma}(y) := f_{\\theta, \\Sigma}(y) - f_{\\theta^*, \\Sigma^*}(y)$$\nOver a dataset $${(x_i, y_i)}_{i \\in [n]}$$, the average log-likelihood ratio is given by\n$$\\overline{\\gamma}_{W, \\Sigma} := \\frac{1}{n} \\sum_{i} \\gamma_{W x_i, \\Sigma}(y_i)$$.\nRemark C.1. For ease of analysis, we will interchange between the precision matrix $$P$$ in $$\\gamma_{\\theta, P}$$ and\nthe covariance matrix $$\\Sigma$$ in $$\\gamma_{\\theta, \\Sigma}$$, and it should be understood that $$P = \\Sigma^{-1}$$. The same applies to the\nfunctions $$g_{\\theta, \\Sigma}$$ and $$h_{\\theta, \\Sigma}$$. Finally, the matrix $$P^*$$ refers to the ground truth precision matrix (= $$\\Sigma^*{-1}$$).\nAnalogous to Appendix B, we start by showing that the log-likelihood ratio is bounded by the noise\nin the sample. The proofs of results in this Section are in Subsection C.1."}]}, {"page": 23, "text": " Lemma C.2. Assume P \u2217                      := \u03a3\u2217\u22121 satisfies Assumption 4.4.\nFor all y           =      \u03d5(\u03b8\u2217      + \u03b7) such that S denotes the zero-coordinates of y, and \u03b7 such that\n      \u221712               \u221712\n \u2225PS \u03b7S\u2225, \u2225PSc \u03b7Sc\u2225                  \u2264   B, if the max eigenvalue \u03bbmax(P                        ) satisfies\n                                                                      \u03bbmax(P       )\n then for all \u03b8 \u2208          Rd, we have                               \u03bbmin(P \u2217) \u2264           C,\n                                                             \u03b3\u03b8,P \u2264       d\n                                                                          2 log(C) + 3B2.\n For the ease of stating the next Lemma, we assume that across the samples of y in the training data,\n at least one coordinate has suffi                  ciently many positive samples. The proof of our theorem separately\n handles cases violating this assumption.\n Assumption C.3. Let \u03b4 \u2208                       (0, 1) be a parameter corresponding to the failure probability of our\n algorithm. Then, there exists a coordinate j \u2208                                 [d], such that for at least n\u2032 = O                        log 1 \u03b4     samples\n yi1, . . . , yi  n\u2032 in the dataset, the j-th coordinate is positive.\n This is a very weak assumption: if it is violated, then W = 0d\u00d7k, \u03a3 = 0 will achieve a TV distance\n smaller than 2\u03b52       d .\n Appendix B assumed that the variance in y was 1. Since Section 4.2 considers an unknown \u03a3\u2217, we\n need the following Lemma to show that the MLE will select a precision matrix P                                                    , whose eigenvalues\n are reasonably bounded wrt \u03a3\u2217\u22121.\n Lemma C.4. Under Assumption 4.4, C.3, consider P \u2208                                           Rd\u00d7d      such that \u03bbmax(P )\n                                                                                                 +                       \u03bb min(P ) \u2264       \u03ba and\n                                                    \u03bbmax(P       )            \u03ba3d2n2          + B2n\u03ba            .\n                                                   \u03bbmax(P \u2217) \u2265            O          k2                k\n Then, for all W \u2208             Rd\u00d7k, and for all yi = \u03d5(W \u2217xi + \u03b7i) with \u2225PSc \u03b7Sc\u2225, \u2225PS \u03b7S\u2225                 \u221712                \u221712         \u2264   B, we have\n                                                       \u00af\n                                                      \u03b3W,P := 1       n   i\u2208[n]   \u03b3W x   i,P (yi) < 0.\n Lemma C.2 and Lemma C.4 show that the MLE will only select precision matrices P that have max\n eigenvalues in a certain range of the true precision matric P \u2217.\n Now, for matrices in the above eigenvalue range, we first construct a geometric net over the max\n eigenvalue \u03c1 of the precision matrix, and then cover the matrices whose max eigenvalue is smaller\n than \u03c1.\n Lemma C.5 (\u03a3 cover). For B > 1, and 0 < L < U, let A > max                                                        log 1  \u03b5 , B2U\u03ba, d      2 log    \u03baUL     , 1  .\n Let P \u2217      := \u03a3\u2217\u22121 be the precision matrix of \u03b7. Let \u2126                                   \u2282    Rd\u00d7d      denote the set of positive definite\n                                                                                                    +\n matrices P \u2208           Rd\u00d7d      with condition number \u03ba and whose maximum eigenvalue lies in [L\u03bbmin(P \u2217), U \u00b7\n \u03bbmax(P \u2217)].               +\n Then, there exists a partition of \u2126                     of size\n                                                                     poly       A, 1 \u03b5    d2         \u221712               \u221712\n such that for all \u03b8 \u2208            Rd and all y = \u03d5(\u03b8\u2217                + \u03b7) \u2208       Rd with \u2225PS \u03b7S\u2225, \u2225PSc \u03b7Sc\u2225                        \u2264   B, and each cell\n I in the partition, one of the following holds:\n            \u2022 for all P \u2208          I, \u03b3\u03b8,P (y) < \u2212A, or\n            \u2022 for all P, P \u2032 \u2208          I, we have |\u03b3\u03b8,P (y) \u2212                \u03b3\u03b8,P \u2032(y)| \u2264        \u03f5.\n                                                                                23", "md": "Lemma C.2. Assume \\( P^* := \\Sigma^*{-1} \\) satisfies Assumption 4.4.\nFor all \\( y = \\phi(\\theta^* + \\eta) \\) such that \\( S \\) denotes the zero-coordinates of \\( y \\), and \\( \\eta \\) such that\n\\[\n\\|P_S \\eta_S\\|, \\|P_{S^c} \\eta_{S^c}\\| \\leq B,\n\\]\nif the max eigenvalue \\( \\lambda_{\\text{max}}(P) \\) satisfies\n\\[\n\\frac{\\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} \\leq C,\n\\]\nthen for all \\( \\theta \\in \\mathbb{R}^d \\), we have\n\\[\n\\gamma_{\\theta,P} \\leq \\frac{d}{2} \\log(C) + 3B^2.\n\\]\n\nFor the ease of stating the next Lemma, we assume that across the samples of \\( y \\) in the training data,\nat least one coordinate has sufficiently many positive samples. The proof of our theorem separately\nhandles cases violating this assumption.\n\nAssumption C.3. Let \\( \\delta \\in (0, 1) \\) be a parameter corresponding to the failure probability of our\nalgorithm. Then, there exists a coordinate \\( j \\in [d] \\), such that for at least \\( n' = O(\\log \\frac{1}{\\delta}) \\) samples\n\\( y_{i1}, \\ldots, y_{i n'} \\) in the dataset, the \\( j \\)-th coordinate is positive.\n\nThis is a very weak assumption: if it is violated, then \\( W = 0_{d \\times k} \\), \\( \\Sigma = 0 \\) will achieve a TV distance\nsmaller than \\( 2\\epsilon^2 d \\).\n\nAppendix B assumed that the variance in \\( y \\) was 1. Since Section 4.2 considers an unknown \\( \\Sigma^* \\), we\nneed the following Lemma to show that the MLE will select a precision matrix \\( P \\), whose eigenvalues\nare reasonably bounded with respect to \\( \\Sigma^*{-1} \\).\n\nLemma C.4. Under Assumption 4.4, C.3, consider \\( P \\in \\mathbb{R}^{d \\times d} \\) such that \\( \\lambda_{\\text{max}}(P) + \\lambda_{\\text{min}}(P) \\leq \\kappa \\) and\n\\[\n\\frac{\\lambda_{\\text{max}}(P)}{\\kappa^3 d^2 n^2} + B^2 n \\kappa \\leq \\lambda_{\\text{max}}(P^*) \\geq O\\left(\\frac{k^2}{k}\\right).\n\\]\nThen, for all \\( W \\in \\mathbb{R}^{d \\times k} \\), and for all \\( y_i = \\phi(W^* x_i + \\eta_i) \\) with \\( \\|P_{S^c} \\eta_{S^c}\\|, \\|P_S \\eta_S\\| \\leq B \\), we have\n\\[\n\\overline{\\gamma}_{W,P} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{W x_i,P}(y_i) < 0.\n\\]\n\nLemma C.2 and Lemma C.4 show that the MLE will only select precision matrices \\( P \\) that have max\neigenvalues in a certain range of the true precision matrix \\( P^* \\).\n\nNow, for matrices in the above eigenvalue range, we first construct a geometric net over the max\neigenvalue \\( \\rho \\) of the precision matrix, and then cover the matrices whose max eigenvalue is smaller\nthan \\( \\rho \\).\n\nLemma C.5 (\u03a3 cover). For \\( B > 1 \\), and \\( 0 < L < U \\), let \\( A > \\max(\\log \\frac{1}{\\epsilon}, B^2 U \\kappa, \\frac{d}{2} \\log \\kappa U L, 1) \\).\nLet \\( P^* := \\Sigma^*{-1} \\) be the precision matrix of \\( \\eta \\). Let \\( \\Omega \\subset \\mathbb{R}^{d \\times d} \\) denote the set of positive definite\nmatrices \\( P \\in \\mathbb{R}^{d \\times d} \\) with condition number \\( \\kappa \\) and whose maximum eigenvalue lies in \\([L \\lambda_{\\text{min}}(P^*), U \\lambda_{\\text{max}}(P^*)]\\).\nThen, there exists a partition of \\( \\Omega \\) of size\n\\[\n\\text{poly}(A, \\frac{1}{\\epsilon} d^2) \\leq \\frac{1}{2} \\left(\\|P_{S^c} \\eta_{S^c}\\|, \\|P_S \\eta_S\\|\\right) \\leq B,\n\\]\nand each cell \\( I \\) in the partition, one of the following holds:\n\\begin{itemize}\n\\item for all \\( P \\in I \\), \\( \\gamma_{\\theta,P}(y) < -A \\), or\n\\item for all \\( P, P' \\in I \\), we have \\( |\\gamma_{\\theta,P}(y) - \\gamma_{\\theta,P'}(y)| \\leq \\epsilon \\).\n\\end{itemize}", "images": [], "items": [{"type": "text", "value": "Lemma C.2. Assume \\( P^* := \\Sigma^*{-1} \\) satisfies Assumption 4.4.\nFor all \\( y = \\phi(\\theta^* + \\eta) \\) such that \\( S \\) denotes the zero-coordinates of \\( y \\), and \\( \\eta \\) such that\n\\[\n\\|P_S \\eta_S\\|, \\|P_{S^c} \\eta_{S^c}\\| \\leq B,\n\\]\nif the max eigenvalue \\( \\lambda_{\\text{max}}(P) \\) satisfies\n\\[\n\\frac{\\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} \\leq C,\n\\]\nthen for all \\( \\theta \\in \\mathbb{R}^d \\), we have\n\\[\n\\gamma_{\\theta,P} \\leq \\frac{d}{2} \\log(C) + 3B^2.\n\\]\n\nFor the ease of stating the next Lemma, we assume that across the samples of \\( y \\) in the training data,\nat least one coordinate has sufficiently many positive samples. The proof of our theorem separately\nhandles cases violating this assumption.\n\nAssumption C.3. Let \\( \\delta \\in (0, 1) \\) be a parameter corresponding to the failure probability of our\nalgorithm. Then, there exists a coordinate \\( j \\in [d] \\), such that for at least \\( n' = O(\\log \\frac{1}{\\delta}) \\) samples\n\\( y_{i1}, \\ldots, y_{i n'} \\) in the dataset, the \\( j \\)-th coordinate is positive.\n\nThis is a very weak assumption: if it is violated, then \\( W = 0_{d \\times k} \\), \\( \\Sigma = 0 \\) will achieve a TV distance\nsmaller than \\( 2\\epsilon^2 d \\).\n\nAppendix B assumed that the variance in \\( y \\) was 1. Since Section 4.2 considers an unknown \\( \\Sigma^* \\), we\nneed the following Lemma to show that the MLE will select a precision matrix \\( P \\), whose eigenvalues\nare reasonably bounded with respect to \\( \\Sigma^*{-1} \\).\n\nLemma C.4. Under Assumption 4.4, C.3, consider \\( P \\in \\mathbb{R}^{d \\times d} \\) such that \\( \\lambda_{\\text{max}}(P) + \\lambda_{\\text{min}}(P) \\leq \\kappa \\) and\n\\[\n\\frac{\\lambda_{\\text{max}}(P)}{\\kappa^3 d^2 n^2} + B^2 n \\kappa \\leq \\lambda_{\\text{max}}(P^*) \\geq O\\left(\\frac{k^2}{k}\\right).\n\\]\nThen, for all \\( W \\in \\mathbb{R}^{d \\times k} \\), and for all \\( y_i = \\phi(W^* x_i + \\eta_i) \\) with \\( \\|P_{S^c} \\eta_{S^c}\\|, \\|P_S \\eta_S\\| \\leq B \\), we have\n\\[\n\\overline{\\gamma}_{W,P} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{W x_i,P}(y_i) < 0.\n\\]\n\nLemma C.2 and Lemma C.4 show that the MLE will only select precision matrices \\( P \\) that have max\neigenvalues in a certain range of the true precision matrix \\( P^* \\).\n\nNow, for matrices in the above eigenvalue range, we first construct a geometric net over the max\neigenvalue \\( \\rho \\) of the precision matrix, and then cover the matrices whose max eigenvalue is smaller\nthan \\( \\rho \\).\n\nLemma C.5 (\u03a3 cover). For \\( B > 1 \\), and \\( 0 < L < U \\), let \\( A > \\max(\\log \\frac{1}{\\epsilon}, B^2 U \\kappa, \\frac{d}{2} \\log \\kappa U L, 1) \\).\nLet \\( P^* := \\Sigma^*{-1} \\) be the precision matrix of \\( \\eta \\). Let \\( \\Omega \\subset \\mathbb{R}^{d \\times d} \\) denote the set of positive definite\nmatrices \\( P \\in \\mathbb{R}^{d \\times d} \\) with condition number \\( \\kappa \\) and whose maximum eigenvalue lies in \\([L \\lambda_{\\text{min}}(P^*), U \\lambda_{\\text{max}}(P^*)]\\).\nThen, there exists a partition of \\( \\Omega \\) of size\n\\[\n\\text{poly}(A, \\frac{1}{\\epsilon} d^2) \\leq \\frac{1}{2} \\left(\\|P_{S^c} \\eta_{S^c}\\|, \\|P_S \\eta_S\\|\\right) \\leq B,\n\\]\nand each cell \\( I \\) in the partition, one of the following holds:\n\\begin{itemize}\n\\item for all \\( P \\in I \\), \\( \\gamma_{\\theta,P}(y) < -A \\), or\n\\item for all \\( P, P' \\in I \\), we have \\( |\\gamma_{\\theta,P}(y) - \\gamma_{\\theta,P'}(y)| \\leq \\epsilon \\).\n\\end{itemize}", "md": "Lemma C.2. Assume \\( P^* := \\Sigma^*{-1} \\) satisfies Assumption 4.4.\nFor all \\( y = \\phi(\\theta^* + \\eta) \\) such that \\( S \\) denotes the zero-coordinates of \\( y \\), and \\( \\eta \\) such that\n\\[\n\\|P_S \\eta_S\\|, \\|P_{S^c} \\eta_{S^c}\\| \\leq B,\n\\]\nif the max eigenvalue \\( \\lambda_{\\text{max}}(P) \\) satisfies\n\\[\n\\frac{\\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} \\leq C,\n\\]\nthen for all \\( \\theta \\in \\mathbb{R}^d \\), we have\n\\[\n\\gamma_{\\theta,P} \\leq \\frac{d}{2} \\log(C) + 3B^2.\n\\]\n\nFor the ease of stating the next Lemma, we assume that across the samples of \\( y \\) in the training data,\nat least one coordinate has sufficiently many positive samples. The proof of our theorem separately\nhandles cases violating this assumption.\n\nAssumption C.3. Let \\( \\delta \\in (0, 1) \\) be a parameter corresponding to the failure probability of our\nalgorithm. Then, there exists a coordinate \\( j \\in [d] \\), such that for at least \\( n' = O(\\log \\frac{1}{\\delta}) \\) samples\n\\( y_{i1}, \\ldots, y_{i n'} \\) in the dataset, the \\( j \\)-th coordinate is positive.\n\nThis is a very weak assumption: if it is violated, then \\( W = 0_{d \\times k} \\), \\( \\Sigma = 0 \\) will achieve a TV distance\nsmaller than \\( 2\\epsilon^2 d \\).\n\nAppendix B assumed that the variance in \\( y \\) was 1. Since Section 4.2 considers an unknown \\( \\Sigma^* \\), we\nneed the following Lemma to show that the MLE will select a precision matrix \\( P \\), whose eigenvalues\nare reasonably bounded with respect to \\( \\Sigma^*{-1} \\).\n\nLemma C.4. Under Assumption 4.4, C.3, consider \\( P \\in \\mathbb{R}^{d \\times d} \\) such that \\( \\lambda_{\\text{max}}(P) + \\lambda_{\\text{min}}(P) \\leq \\kappa \\) and\n\\[\n\\frac{\\lambda_{\\text{max}}(P)}{\\kappa^3 d^2 n^2} + B^2 n \\kappa \\leq \\lambda_{\\text{max}}(P^*) \\geq O\\left(\\frac{k^2}{k}\\right).\n\\]\nThen, for all \\( W \\in \\mathbb{R}^{d \\times k} \\), and for all \\( y_i = \\phi(W^* x_i + \\eta_i) \\) with \\( \\|P_{S^c} \\eta_{S^c}\\|, \\|P_S \\eta_S\\| \\leq B \\), we have\n\\[\n\\overline{\\gamma}_{W,P} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{W x_i,P}(y_i) < 0.\n\\]\n\nLemma C.2 and Lemma C.4 show that the MLE will only select precision matrices \\( P \\) that have max\neigenvalues in a certain range of the true precision matrix \\( P^* \\).\n\nNow, for matrices in the above eigenvalue range, we first construct a geometric net over the max\neigenvalue \\( \\rho \\) of the precision matrix, and then cover the matrices whose max eigenvalue is smaller\nthan \\( \\rho \\).\n\nLemma C.5 (\u03a3 cover). For \\( B > 1 \\), and \\( 0 < L < U \\), let \\( A > \\max(\\log \\frac{1}{\\epsilon}, B^2 U \\kappa, \\frac{d}{2} \\log \\kappa U L, 1) \\).\nLet \\( P^* := \\Sigma^*{-1} \\) be the precision matrix of \\( \\eta \\). Let \\( \\Omega \\subset \\mathbb{R}^{d \\times d} \\) denote the set of positive definite\nmatrices \\( P \\in \\mathbb{R}^{d \\times d} \\) with condition number \\( \\kappa \\) and whose maximum eigenvalue lies in \\([L \\lambda_{\\text{min}}(P^*), U \\lambda_{\\text{max}}(P^*)]\\).\nThen, there exists a partition of \\( \\Omega \\) of size\n\\[\n\\text{poly}(A, \\frac{1}{\\epsilon} d^2) \\leq \\frac{1}{2} \\left(\\|P_{S^c} \\eta_{S^c}\\|, \\|P_S \\eta_S\\|\\right) \\leq B,\n\\]\nand each cell \\( I \\) in the partition, one of the following holds:\n\\begin{itemize}\n\\item for all \\( P \\in I \\), \\( \\gamma_{\\theta,P}(y) < -A \\), or\n\\item for all \\( P, P' \\in I \\), we have \\( |\\gamma_{\\theta,P}(y) - \\gamma_{\\theta,P'}(y)| \\leq \\epsilon \\).\n\\end{itemize}"}]}, {"page": 24, "text": " Analogous to Appendix B, we now construct a partition over W for a fixed precision matrix P                                            , such\n that each cell in the partition has very small log-likelihood (in which case the MLE will not choose it)\n or the log-likelihood changes slowly.\n Lemma C.6 (W            -net). Let \u03b7Sc, \u03b7S be such that\n                                                    \u221712                       \u221712\n for B1, B2 \u2265        0.                        \u2225PSc \u03b7Sc\u2225        \u2264  B1, \u2225PS \u03b7S\u2225          \u2264   B2,\n Let A > max{B2           1, B2  2, poly(C, \u03ba)}. Let P \u2217             = \u03a3\u2217\u22121 be the precision matrix of \u03b7. For a fixed\n matrix P \u2208       Rd\u00d7d whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy\n \u03bbmax(P     ) \u2208   [e\u22122A  d \u03bbmin(P \u2217), C\u03bbmax(P \u2217)], there exists a partition I of Rd with size\n                                                             poly      A, 1 \u03b5   3d\n such that for each interval I \u2208              I, we have one of the following:\n           \u2022 for all \u03b8 \u2208      I, \u03b3\u03b8,P (y) < \u2212A, or\n           \u2022 for all \u03b8, \u03b8\u2032 \u2208      I, |\u03b3\u03b8,P (y) \u2212      \u03b3\u03b8\u2032,P (y)| \u2264       \u03f5.\n Using the above lemmas, we can show that the MLE will only pick out                                       W   ,P such that they have\n small TV on the dataset of {xi}.\n Lemma C.7. Let x1, . . . , xn be fixed, and yi = \u03d5(W \u2217xi + \u03b7i) for \u03b7i \u223c                                N   (0, \u03a3\u2217), and W \u2217         \u2208  Rd\u00d7k\n with \u03a3\u2217     \u2208   Rd\u00d7d satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant\n C > 0,                                             n = C \u00b7 (d2 + kd)  \u03b52        log kd\u03ba\u03b5\n samples suffice to guarantee that with high probability, the MLE                             W   , \u03a3 satisfies\n Lemma C.8. Let {xi}n                               d   ( W  , \u03a3), (W \u2217, \u03a3\u2217)          \u2264   \u03b5.\n                                  i=1 be i.i.d. random variables such that xi \u223c                     Dx.\n Let P \u2217   := \u03a3\u2217\u22121. Let \u03bb\u2217        min, \u03bb\u2217 max be the minimum and maximum eigenvalues of P \u2217. For 0 < L < U,\n let \u2126   denote the following set of precision matrices\n                  \u2126   :=      P \u2208    Rd\u00d7d     : \u03bbmax(P      )                                         min, U \u00b7 \u03bb\u2217   max]     .\n                                       +         \u03bbmin(P     ) \u2264    \u03ba and \u03bbmax(P         ) \u2208   [L \u00b7 \u03bb\u2217\nThen, for a sufficiently large constant C > 0, and for\n we have:                                n = C \u00b7      kd + d2\u03b52         log  kd\u03ba  \u03b5    log  U  L       ,\n            Pr             sup          d((W, P      ), (W \u2217, P \u2217)) \u2212       d((W, P     ), (W \u2217, P \u2217))       > \u03b5      \u2264   e\u2212\u2126(n\u03b52).\n         xi\u223cDx      W \u2208Rd\u00d7k,P \u2208\u2126\n Theorem 4.5. Let Rd\u00d7d        \u03ba     denote the set of positive definite matrices with condition number \u03ba. Given n\n samples {(xi, yi)}n       i=1 satisfying Assumption 4.4, where xi \u223c                    Dx i.i.d., and yi is generated according\n                                                                   1\n to (7), let   W   , \u03a3 := arg max         W \u2208Rd\u00d7k,\u03a3\u2208Rd\u00d7d    \u03ba      n     i log pW,\u03a3(yi | xi). Then, for a suffi               ciently large\n constant C > 0,\n                                                n = C \u00b7       kd + d2          log   \u03bakd\n                                                                    \u03b52                  \u03b5\u03b4\n samples suffice to ensure that with probability 1 \u2212                      \u03b4, we have\n                                                  dT V    ( W   ,\u03a3), (W \u2217, \u03a3\u2217)          \u2264   \u03b5.\n                                                                       24", "md": "Analogous to Appendix B, we now construct a partition over W for a fixed precision matrix P, such\nthat each cell in the partition has very small log-likelihood (in which case the MLE will not choose it)\nor the log-likelihood changes slowly.\n\n$$\\text{Lemma C.6 (W-net). Let }\\eta_{Sc}, \\eta_{S}\\text{ be such that}$$\n\n$$\n\\begin{align*}\n&\\text{for } B_{1}, B_{2} \\geq 0, \\|\\text{PSc }\\eta_{Sc}\\| \\leq B_{1}, \\|\\text{PS }\\eta_{S}\\| \\leq B_{2}, \\\\\n&\\text{Let } A > \\max\\{B_{2}, B_{1}, \\text{poly}(C, \\kappa)\\}. \\\\\n&\\text{Let } P^{*} = \\Sigma^{*-1} \\text{ be the precision matrix of }\\eta. \\\\\n&\\text{For a fixed matrix } P \\in \\mathbb{R}^{d \\times d} \\text{ whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy} \\\\\n&\\lambda_{\\text{max}}(P) \\in [e^{-2A} \\lambda_{\\text{min}}(P^{*}), C\\lambda_{\\text{max}}(P^{*})], \\\\\n&\\text{there exists a partition } I \\text{ of }\\mathbb{R}^{d} \\text{ with size }\\text{poly}(A, \\frac{1}{\\epsilon}, 3d) \\\\\n&\\text{such that for each interval } I \\in I, \\text{we have one of the following:} \\\\\n&\\bullet \\text{ for all } \\theta \\in I, \\gamma_{\\theta, P}(y) < -A, \\text{ or} \\\\\n&\\bullet \\text{ for all } \\theta, \\theta' \\in I, |\\gamma_{\\theta, P}(y) - \\gamma_{\\theta', P}(y)| \\leq \\epsilon.\n\\end{align*}\n$$\n\nUsing the above lemmas, we can show that the MLE will only pick out W, P such that they have small TV on the dataset of {xi}.\n\n$$\\text{Lemma C.7. Let } x_{1}, \\ldots, x_{n} \\text{ be fixed, and } y_{i} = \\varphi(W^{*}x_{i} + \\eta_{i}) \\text{ for } \\eta_{i} \\sim \\mathcal{N}(0, \\Sigma^{*}), \\text{ and } W^{*} \\in \\mathbb{R}^{d \\times k} \\text{ with } \\Sigma^{*} \\in \\mathbb{R}^{d \\times d} \\text{ satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant } C > 0,$$\n\n$$\n\\begin{align*}\n&n = C \\cdot (d^{2} + kd) \\frac{1}{\\epsilon^{2}} \\log kd\\kappa\\epsilon \\text{ samples suffice to guarantee that with high probability, the MLE } W, \\Sigma \\text{ satisfies} \\\\\n&\\text{Lemma C.8. Let } \\{x_{i}\\}_{n}^{d} \\sim (W, \\Sigma), (W^{*}, \\Sigma^{*}) \\leq \\epsilon. \\\\\n&\\text{be i.i.d. random variables such that } x_{i} \\sim \\mathcal{D}_{x}. \\\\\n&\\text{Let } P^{*} := \\Sigma^{*-1}. \\text{ Let } \\lambda_{\\text{min}}^{*}, \\lambda_{\\text{max}}^{*} \\text{ be the minimum and maximum eigenvalues of } P^{*}. \\text{ For } 0 < L < U, \\\\\n&\\text{let } \\Omega \\text{ denote the following set of precision matrices} \\\\\n&\\Omega := \\{P \\in \\mathbb{R}^{d \\times d} : \\lambda_{\\text{max}}(P) \\geq \\lambda_{\\text{min}}^{*}, U \\cdot \\lambda_{\\text{max}}^{*}] \\\\\n&\\quad \\quad \\quad \\quad + \\lambda_{\\text{min}}(P) \\leq \\kappa \\text{ and } \\lambda_{\\text{max}}(P) \\in [L \\cdot \\lambda_{\\text{max}}^{*}, U \\cdot \\lambda_{\\text{max}}^{*}] \\}. \\\\\n&\\text{Then, for a sufficiently large constant } C > 0, \\text{ and for} \\\\\n&n = C \\cdot (kd + d^{2}\\epsilon^{2}) \\log kd\\kappa\\epsilon \\log \\frac{U}{L}, \\\\\n&\\Pr\\left[\\sup_{xi \\sim \\mathcal{D}_{x}} d((W, P), (W^{*}, P^{*})) - d((W, P), (W^{*}, P^{*})) > \\epsilon\\right] \\leq e^{-\\Omega(n\\epsilon^{2})}.\n\\end{align*}\n$$\n\n$$\\text{Theorem 4.5. Let } \\mathbb{R}^{d \\times d}_{\\kappa} \\text{ denote the set of positive definite matrices with condition number } \\kappa.$$\n\n$$\\text{Given } n \\text{ samples } \\{(x_{i}, y_{i})\\}_{n}^{i=1} \\text{ satisfying Assumption 4.4, where } x_{i} \\sim \\mathcal{D}_{x} \\text{ i.i.d., and } y_{i} \\text{ is generated according to (7), let } W, \\Sigma := \\arg\\max_{W \\in \\mathbb{R}^{d \\times k}, \\Sigma \\in \\mathbb{R}^{d \\times d}_{\\kappa}} \\frac{1}{n} \\sum_{i} \\log p_{W,\\Sigma}(y_{i} | x_{i}).$$\n\n$$\\text{Then, for a sufficiently large constant } C > 0,$$\n\n$$\n\\begin{align*}\n&n = C \\cdot (kd + d^{2}) \\log \\kappa kd \\frac{\\epsilon^{2}}{\\delta} \\text{ samples suffice to ensure that with probability } 1 - \\delta, \\text{ we have} \\\\\n&d_{TV}((W, \\Sigma), (W^{*}, \\Sigma^{*})) \\leq \\epsilon.\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Analogous to Appendix B, we now construct a partition over W for a fixed precision matrix P, such\nthat each cell in the partition has very small log-likelihood (in which case the MLE will not choose it)\nor the log-likelihood changes slowly.\n\n$$\\text{Lemma C.6 (W-net). Let }\\eta_{Sc}, \\eta_{S}\\text{ be such that}$$\n\n$$\n\\begin{align*}\n&\\text{for } B_{1}, B_{2} \\geq 0, \\|\\text{PSc }\\eta_{Sc}\\| \\leq B_{1}, \\|\\text{PS }\\eta_{S}\\| \\leq B_{2}, \\\\\n&\\text{Let } A > \\max\\{B_{2}, B_{1}, \\text{poly}(C, \\kappa)\\}. \\\\\n&\\text{Let } P^{*} = \\Sigma^{*-1} \\text{ be the precision matrix of }\\eta. \\\\\n&\\text{For a fixed matrix } P \\in \\mathbb{R}^{d \\times d} \\text{ whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy} \\\\\n&\\lambda_{\\text{max}}(P) \\in [e^{-2A} \\lambda_{\\text{min}}(P^{*}), C\\lambda_{\\text{max}}(P^{*})], \\\\\n&\\text{there exists a partition } I \\text{ of }\\mathbb{R}^{d} \\text{ with size }\\text{poly}(A, \\frac{1}{\\epsilon}, 3d) \\\\\n&\\text{such that for each interval } I \\in I, \\text{we have one of the following:} \\\\\n&\\bullet \\text{ for all } \\theta \\in I, \\gamma_{\\theta, P}(y) < -A, \\text{ or} \\\\\n&\\bullet \\text{ for all } \\theta, \\theta' \\in I, |\\gamma_{\\theta, P}(y) - \\gamma_{\\theta', P}(y)| \\leq \\epsilon.\n\\end{align*}\n$$\n\nUsing the above lemmas, we can show that the MLE will only pick out W, P such that they have small TV on the dataset of {xi}.\n\n$$\\text{Lemma C.7. Let } x_{1}, \\ldots, x_{n} \\text{ be fixed, and } y_{i} = \\varphi(W^{*}x_{i} + \\eta_{i}) \\text{ for } \\eta_{i} \\sim \\mathcal{N}(0, \\Sigma^{*}), \\text{ and } W^{*} \\in \\mathbb{R}^{d \\times k} \\text{ with } \\Sigma^{*} \\in \\mathbb{R}^{d \\times d} \\text{ satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant } C > 0,$$\n\n$$\n\\begin{align*}\n&n = C \\cdot (d^{2} + kd) \\frac{1}{\\epsilon^{2}} \\log kd\\kappa\\epsilon \\text{ samples suffice to guarantee that with high probability, the MLE } W, \\Sigma \\text{ satisfies} \\\\\n&\\text{Lemma C.8. Let } \\{x_{i}\\}_{n}^{d} \\sim (W, \\Sigma), (W^{*}, \\Sigma^{*}) \\leq \\epsilon. \\\\\n&\\text{be i.i.d. random variables such that } x_{i} \\sim \\mathcal{D}_{x}. \\\\\n&\\text{Let } P^{*} := \\Sigma^{*-1}. \\text{ Let } \\lambda_{\\text{min}}^{*}, \\lambda_{\\text{max}}^{*} \\text{ be the minimum and maximum eigenvalues of } P^{*}. \\text{ For } 0 < L < U, \\\\\n&\\text{let } \\Omega \\text{ denote the following set of precision matrices} \\\\\n&\\Omega := \\{P \\in \\mathbb{R}^{d \\times d} : \\lambda_{\\text{max}}(P) \\geq \\lambda_{\\text{min}}^{*}, U \\cdot \\lambda_{\\text{max}}^{*}] \\\\\n&\\quad \\quad \\quad \\quad + \\lambda_{\\text{min}}(P) \\leq \\kappa \\text{ and } \\lambda_{\\text{max}}(P) \\in [L \\cdot \\lambda_{\\text{max}}^{*}, U \\cdot \\lambda_{\\text{max}}^{*}] \\}. \\\\\n&\\text{Then, for a sufficiently large constant } C > 0, \\text{ and for} \\\\\n&n = C \\cdot (kd + d^{2}\\epsilon^{2}) \\log kd\\kappa\\epsilon \\log \\frac{U}{L}, \\\\\n&\\Pr\\left[\\sup_{xi \\sim \\mathcal{D}_{x}} d((W, P), (W^{*}, P^{*})) - d((W, P), (W^{*}, P^{*})) > \\epsilon\\right] \\leq e^{-\\Omega(n\\epsilon^{2})}.\n\\end{align*}\n$$\n\n$$\\text{Theorem 4.5. Let } \\mathbb{R}^{d \\times d}_{\\kappa} \\text{ denote the set of positive definite matrices with condition number } \\kappa.$$\n\n$$\\text{Given } n \\text{ samples } \\{(x_{i}, y_{i})\\}_{n}^{i=1} \\text{ satisfying Assumption 4.4, where } x_{i} \\sim \\mathcal{D}_{x} \\text{ i.i.d., and } y_{i} \\text{ is generated according to (7), let } W, \\Sigma := \\arg\\max_{W \\in \\mathbb{R}^{d \\times k}, \\Sigma \\in \\mathbb{R}^{d \\times d}_{\\kappa}} \\frac{1}{n} \\sum_{i} \\log p_{W,\\Sigma}(y_{i} | x_{i}).$$\n\n$$\\text{Then, for a sufficiently large constant } C > 0,$$\n\n$$\n\\begin{align*}\n&n = C \\cdot (kd + d^{2}) \\log \\kappa kd \\frac{\\epsilon^{2}}{\\delta} \\text{ samples suffice to ensure that with probability } 1 - \\delta, \\text{ we have} \\\\\n&d_{TV}((W, \\Sigma), (W^{*}, \\Sigma^{*})) \\leq \\epsilon.\n\\end{align*}\n$$", "md": "Analogous to Appendix B, we now construct a partition over W for a fixed precision matrix P, such\nthat each cell in the partition has very small log-likelihood (in which case the MLE will not choose it)\nor the log-likelihood changes slowly.\n\n$$\\text{Lemma C.6 (W-net). Let }\\eta_{Sc}, \\eta_{S}\\text{ be such that}$$\n\n$$\n\\begin{align*}\n&\\text{for } B_{1}, B_{2} \\geq 0, \\|\\text{PSc }\\eta_{Sc}\\| \\leq B_{1}, \\|\\text{PS }\\eta_{S}\\| \\leq B_{2}, \\\\\n&\\text{Let } A > \\max\\{B_{2}, B_{1}, \\text{poly}(C, \\kappa)\\}. \\\\\n&\\text{Let } P^{*} = \\Sigma^{*-1} \\text{ be the precision matrix of }\\eta. \\\\\n&\\text{For a fixed matrix } P \\in \\mathbb{R}^{d \\times d} \\text{ whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy} \\\\\n&\\lambda_{\\text{max}}(P) \\in [e^{-2A} \\lambda_{\\text{min}}(P^{*}), C\\lambda_{\\text{max}}(P^{*})], \\\\\n&\\text{there exists a partition } I \\text{ of }\\mathbb{R}^{d} \\text{ with size }\\text{poly}(A, \\frac{1}{\\epsilon}, 3d) \\\\\n&\\text{such that for each interval } I \\in I, \\text{we have one of the following:} \\\\\n&\\bullet \\text{ for all } \\theta \\in I, \\gamma_{\\theta, P}(y) < -A, \\text{ or} \\\\\n&\\bullet \\text{ for all } \\theta, \\theta' \\in I, |\\gamma_{\\theta, P}(y) - \\gamma_{\\theta', P}(y)| \\leq \\epsilon.\n\\end{align*}\n$$\n\nUsing the above lemmas, we can show that the MLE will only pick out W, P such that they have small TV on the dataset of {xi}.\n\n$$\\text{Lemma C.7. Let } x_{1}, \\ldots, x_{n} \\text{ be fixed, and } y_{i} = \\varphi(W^{*}x_{i} + \\eta_{i}) \\text{ for } \\eta_{i} \\sim \\mathcal{N}(0, \\Sigma^{*}), \\text{ and } W^{*} \\in \\mathbb{R}^{d \\times k} \\text{ with } \\Sigma^{*} \\in \\mathbb{R}^{d \\times d} \\text{ satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant } C > 0,$$\n\n$$\n\\begin{align*}\n&n = C \\cdot (d^{2} + kd) \\frac{1}{\\epsilon^{2}} \\log kd\\kappa\\epsilon \\text{ samples suffice to guarantee that with high probability, the MLE } W, \\Sigma \\text{ satisfies} \\\\\n&\\text{Lemma C.8. Let } \\{x_{i}\\}_{n}^{d} \\sim (W, \\Sigma), (W^{*}, \\Sigma^{*}) \\leq \\epsilon. \\\\\n&\\text{be i.i.d. random variables such that } x_{i} \\sim \\mathcal{D}_{x}. \\\\\n&\\text{Let } P^{*} := \\Sigma^{*-1}. \\text{ Let } \\lambda_{\\text{min}}^{*}, \\lambda_{\\text{max}}^{*} \\text{ be the minimum and maximum eigenvalues of } P^{*}. \\text{ For } 0 < L < U, \\\\\n&\\text{let } \\Omega \\text{ denote the following set of precision matrices} \\\\\n&\\Omega := \\{P \\in \\mathbb{R}^{d \\times d} : \\lambda_{\\text{max}}(P) \\geq \\lambda_{\\text{min}}^{*}, U \\cdot \\lambda_{\\text{max}}^{*}] \\\\\n&\\quad \\quad \\quad \\quad + \\lambda_{\\text{min}}(P) \\leq \\kappa \\text{ and } \\lambda_{\\text{max}}(P) \\in [L \\cdot \\lambda_{\\text{max}}^{*}, U \\cdot \\lambda_{\\text{max}}^{*}] \\}. \\\\\n&\\text{Then, for a sufficiently large constant } C > 0, \\text{ and for} \\\\\n&n = C \\cdot (kd + d^{2}\\epsilon^{2}) \\log kd\\kappa\\epsilon \\log \\frac{U}{L}, \\\\\n&\\Pr\\left[\\sup_{xi \\sim \\mathcal{D}_{x}} d((W, P), (W^{*}, P^{*})) - d((W, P), (W^{*}, P^{*})) > \\epsilon\\right] \\leq e^{-\\Omega(n\\epsilon^{2})}.\n\\end{align*}\n$$\n\n$$\\text{Theorem 4.5. Let } \\mathbb{R}^{d \\times d}_{\\kappa} \\text{ denote the set of positive definite matrices with condition number } \\kappa.$$\n\n$$\\text{Given } n \\text{ samples } \\{(x_{i}, y_{i})\\}_{n}^{i=1} \\text{ satisfying Assumption 4.4, where } x_{i} \\sim \\mathcal{D}_{x} \\text{ i.i.d., and } y_{i} \\text{ is generated according to (7), let } W, \\Sigma := \\arg\\max_{W \\in \\mathbb{R}^{d \\times k}, \\Sigma \\in \\mathbb{R}^{d \\times d}_{\\kappa}} \\frac{1}{n} \\sum_{i} \\log p_{W,\\Sigma}(y_{i} | x_{i}).$$\n\n$$\\text{Then, for a sufficiently large constant } C > 0,$$\n\n$$\n\\begin{align*}\n&n = C \\cdot (kd + d^{2}) \\log \\kappa kd \\frac{\\epsilon^{2}}{\\delta} \\text{ samples suffice to ensure that with probability } 1 - \\delta, \\text{ we have} \\\\\n&d_{TV}((W, \\Sigma), (W^{*}, \\Sigma^{*})) \\leq \\epsilon.\n\\end{align*}\n$$"}]}, {"page": 25, "text": "Proof of Theorem 4.5. First, we consider the cases violating Assumption C.3.\nAs n \u221d     d2\n           \u03b52 log 1\n                  \u03b4, if assumption C.3 is violated, then it implies that each coordinate is non-zero in\natmost a \u03b52/d2 fraction of the samples, and a union bound implies that the probability of seeing a\nnon-zero vector is atmost \u03b52/d. Hence, with high probability over the draws of the data, returning the\nall-zeros vector always will achieve a TV distance smaller than 2\u03b52          d .\nLet  P, P \u2217  =  \u03a3\u22121, \u03a3\u2217\u22121. Now, if Assumption C.3 holds, Lemma C.7 guarantees that the MLE has\nsmall TV on the xi observed in the dataset:\n                                           d(( W,  P), (W \u2217, P \u2217)) \u2264     \u03b5.\nThe above result is over the finite xi observed in our dataset. To generalize it over x \u223c               Dx, we use\nLemma C.8, which gives\n                            d(( W,  P), (W \u2217, P \u2217)) \u2212     d((W,   P), (W \u2217, P \u2217)) \u2264    \u03b5.\nRescaling \u03b5 gives the conclusion of the Theorem.\nC.1     Proofs of Appendix C.\nLemma C.2. Assume P \u2217          := \u03a3\u2217\u22121 satisfies Assumption 4.4.\nFor all y      =   \u03d5(\u03b8\u2217   + \u03b7) such that S denotes the zero-coordinates of y, and \u03b7 such that\n    \u22171           \u22171\n      2           2\n\u2225PS \u03b7S\u2225, \u2225PSc \u03b7Sc\u2225         \u2264  B, if the max eigenvalue \u03bbmax(P) satisfies\n                                                  \u03bbmax(P)\nthen for all \u03b8 \u2208   Rd, we have                    \u03bbmin(P \u2217) \u2264     C,\nProof. We have                              \u03b3\u03b8,P \u2264   d2 log(C) + 3B2.\n                           \u03b3\u03b8,\u03a3 \u2264    1                                                                           (23)\n                                     2 log |\u03a3\u2217|\nFrom Lemma C.9, C.10, we have               |\u03a3| + g\u03b8,\u03a3 \u2212      g\u03b8\u2217,\u03a3\u2217  + h\u03b8,\u03a3 \u2212    h\u03b8\u2217,\u03a3\u2217.\n                       g\u03b8,\u03a3 \u2212   g\u03b8\u2217,\u03a3\u2217  + h\u03b8,\u03a3 \u2212    h\u03b8\u2217,\u03a3\u2217  \u2264   g\u03b8,\u03a3 + 1          S|\n                                                                         2 log |P \u2217\nSubstituting in Eqn (23), we get                                                |PS| + 3B2.\n                              \u03b3\u03b8,\u03a3 \u2264    g\u03b8,\u03a3 + 1 2 log |\u03a3\u2217|                S|\n                                                        |\u03a3| + 1  2 log |P \u2217\n                                                                        |PS| + 3B2.\nAs (P \u2217S)\u22121 = \u03a3\u2217   S \u2212   \u03a3\u2217SSc\u03a3\u2217\u22121      ScS, by the matrix determinant rule, we have\nThis gives                       Sc \u03a3\u2217  log|\u03a3\u2217| + log|P \u2217   S| = log|\u03a3\u2217  Sc|.\n                                      \u03b3\u03b8,\u03a3 \u2264   g\u03b8,\u03a3 + 1 2 log |\u03a3\u2217 Sc|\nThis gives                                                     |\u03a3Sc| + 3B2.\n                                   \u03b3\u03b8,\u03a3 \u2264   g\u03b8,\u03a3 + d  2 log \u03bbmax(\u03a3\u2217)\n                                                             \u03bbmin(\u03a3) + 3B2,\n                                         = g\u03b8,\u03a3 + d   2 log \u03bbmax(P)                                              (24)\n                                                             \u03bbmin(P \u2217) + 3B2.\n                                                         25", "md": "Proof of Theorem 4.5. First, we consider the cases violating Assumption C.3.\nAs n $$\\propto$$ d^2 / (\\epsilon^2 \\log(1/\\delta))$$, if assumption C.3 is violated, then it implies that each coordinate is non-zero in at most a $$\\epsilon^2/d^2$$ fraction of the samples, and a union bound implies that the probability of seeing a non-zero vector is at most $$\\epsilon^2/\\delta$$. Hence, with high probability over the draws of the data, returning the all-zeros vector always will achieve a TV distance smaller than $$2\\epsilon^2 / d$$.\nLet $$P, P^* = \\Sigma^{-1}, \\Sigma^{*^{-1}}$$. Now, if Assumption C.3 holds, Lemma C.7 guarantees that the MLE has small TV on the $$x_i$$ observed in the dataset:\n$$d((W, P), (W^{*}, P^{*})) \\leq \\epsilon$$.\nThe above result is over the finite $$x_i$$ observed in our dataset. To generalize it over $$x \\sim D_x$$, we use Lemma C.8, which gives\n$$d((W, P), (W^{*}, P^{*})) - d((W, P), (W^{*}, P^{*})) \\leq \\epsilon$$.\nRescaling $$\\epsilon$$ gives the conclusion of the Theorem.\n\n## C.1 Proofs of Appendix C.\n\nLemma C.2. Assume $$P^{*} := \\Sigma^{*^{-1}}$$ satisfies Assumption 4.4.\nFor all $$y = \\phi(\\theta^{*} + \\eta)$$ such that S denotes the zero-coordinates of y, and $$\\eta$$ such that\n$$\\|P_S \\eta_S\\|, \\|P_{S^c} \\eta_{S^c}\\| \\leq B$$, if the max eigenvalue $$\\lambda_{\\text{max}}(P)$$ satisfies\n$$\\lambda_{\\text{max}}(P)$$\nthen for all $$\\theta \\in \\mathbb{R}^d$$, we have\n$$\\lambda_{\\text{min}}(P^{*}) \\leq C$$.\n\nProof. We have\n$$\\gamma_{\\theta,P} \\leq d^2 \\log(C) + 3B^2$$.\n$$\\gamma_{\\theta,\\Sigma} \\leq 1/2 \\log |\\Sigma^{*}|$$.\nFrom Lemma C.9, C.10, we have\n$$|\\Sigma| + g_{\\theta,\\Sigma} - g_{\\theta^{*},\\Sigma^{*}} + h_{\\theta,\\Sigma} - h_{\\theta^{*},\\Sigma^{*}} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |P^{*}| + 3B^2$$.\nSubstituting in Eqn (23), we get\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |\\Sigma^{*}| / |\\Sigma| + 1/2 \\log |P^{*}| / |P_S| + 3B^2$$.\nAs $$(P^{*}_S)^{-1} = \\Sigma^{*}_S - \\Sigma^{*}_{S^c} \\Sigma^{*^{-1}}_{ScS}$$, by the matrix determinant rule, we have\nThis gives\n$$\\Sigma^{*}_{Sc} \\log|\\Sigma^{*}| + \\log|P^{*}_S| = \\log|\\Sigma^{*}_{Sc}|$$.\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |\\Sigma^{*}_{Sc}| / |\\Sigma_{Sc}| + 3B^2$$.\nThis gives\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + d/2 \\log \\lambda_{\\text{max}}(\\Sigma^{*}) / \\lambda_{\\text{min}}(\\Sigma) + 3B^2$$,\n$$= g_{\\theta,\\Sigma} + d/2 \\log \\lambda_{\\text{max}}(P) / \\lambda_{\\text{min}}(P^{*}) + 3B^2$$.", "images": [], "items": [{"type": "text", "value": "Proof of Theorem 4.5. First, we consider the cases violating Assumption C.3.\nAs n $$\\propto$$ d^2 / (\\epsilon^2 \\log(1/\\delta))$$, if assumption C.3 is violated, then it implies that each coordinate is non-zero in at most a $$\\epsilon^2/d^2$$ fraction of the samples, and a union bound implies that the probability of seeing a non-zero vector is at most $$\\epsilon^2/\\delta$$. Hence, with high probability over the draws of the data, returning the all-zeros vector always will achieve a TV distance smaller than $$2\\epsilon^2 / d$$.\nLet $$P, P^* = \\Sigma^{-1}, \\Sigma^{*^{-1}}$$. Now, if Assumption C.3 holds, Lemma C.7 guarantees that the MLE has small TV on the $$x_i$$ observed in the dataset:\n$$d((W, P), (W^{*}, P^{*})) \\leq \\epsilon$$.\nThe above result is over the finite $$x_i$$ observed in our dataset. To generalize it over $$x \\sim D_x$$, we use Lemma C.8, which gives\n$$d((W, P), (W^{*}, P^{*})) - d((W, P), (W^{*}, P^{*})) \\leq \\epsilon$$.\nRescaling $$\\epsilon$$ gives the conclusion of the Theorem.", "md": "Proof of Theorem 4.5. First, we consider the cases violating Assumption C.3.\nAs n $$\\propto$$ d^2 / (\\epsilon^2 \\log(1/\\delta))$$, if assumption C.3 is violated, then it implies that each coordinate is non-zero in at most a $$\\epsilon^2/d^2$$ fraction of the samples, and a union bound implies that the probability of seeing a non-zero vector is at most $$\\epsilon^2/\\delta$$. Hence, with high probability over the draws of the data, returning the all-zeros vector always will achieve a TV distance smaller than $$2\\epsilon^2 / d$$.\nLet $$P, P^* = \\Sigma^{-1}, \\Sigma^{*^{-1}}$$. Now, if Assumption C.3 holds, Lemma C.7 guarantees that the MLE has small TV on the $$x_i$$ observed in the dataset:\n$$d((W, P), (W^{*}, P^{*})) \\leq \\epsilon$$.\nThe above result is over the finite $$x_i$$ observed in our dataset. To generalize it over $$x \\sim D_x$$, we use Lemma C.8, which gives\n$$d((W, P), (W^{*}, P^{*})) - d((W, P), (W^{*}, P^{*})) \\leq \\epsilon$$.\nRescaling $$\\epsilon$$ gives the conclusion of the Theorem."}, {"type": "heading", "lvl": 2, "value": "C.1 Proofs of Appendix C.", "md": "## C.1 Proofs of Appendix C."}, {"type": "text", "value": "Lemma C.2. Assume $$P^{*} := \\Sigma^{*^{-1}}$$ satisfies Assumption 4.4.\nFor all $$y = \\phi(\\theta^{*} + \\eta)$$ such that S denotes the zero-coordinates of y, and $$\\eta$$ such that\n$$\\|P_S \\eta_S\\|, \\|P_{S^c} \\eta_{S^c}\\| \\leq B$$, if the max eigenvalue $$\\lambda_{\\text{max}}(P)$$ satisfies\n$$\\lambda_{\\text{max}}(P)$$\nthen for all $$\\theta \\in \\mathbb{R}^d$$, we have\n$$\\lambda_{\\text{min}}(P^{*}) \\leq C$$.\n\nProof. We have\n$$\\gamma_{\\theta,P} \\leq d^2 \\log(C) + 3B^2$$.\n$$\\gamma_{\\theta,\\Sigma} \\leq 1/2 \\log |\\Sigma^{*}|$$.\nFrom Lemma C.9, C.10, we have\n$$|\\Sigma| + g_{\\theta,\\Sigma} - g_{\\theta^{*},\\Sigma^{*}} + h_{\\theta,\\Sigma} - h_{\\theta^{*},\\Sigma^{*}} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |P^{*}| + 3B^2$$.\nSubstituting in Eqn (23), we get\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |\\Sigma^{*}| / |\\Sigma| + 1/2 \\log |P^{*}| / |P_S| + 3B^2$$.\nAs $$(P^{*}_S)^{-1} = \\Sigma^{*}_S - \\Sigma^{*}_{S^c} \\Sigma^{*^{-1}}_{ScS}$$, by the matrix determinant rule, we have\nThis gives\n$$\\Sigma^{*}_{Sc} \\log|\\Sigma^{*}| + \\log|P^{*}_S| = \\log|\\Sigma^{*}_{Sc}|$$.\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |\\Sigma^{*}_{Sc}| / |\\Sigma_{Sc}| + 3B^2$$.\nThis gives\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + d/2 \\log \\lambda_{\\text{max}}(\\Sigma^{*}) / \\lambda_{\\text{min}}(\\Sigma) + 3B^2$$,\n$$= g_{\\theta,\\Sigma} + d/2 \\log \\lambda_{\\text{max}}(P) / \\lambda_{\\text{min}}(P^{*}) + 3B^2$$.", "md": "Lemma C.2. Assume $$P^{*} := \\Sigma^{*^{-1}}$$ satisfies Assumption 4.4.\nFor all $$y = \\phi(\\theta^{*} + \\eta)$$ such that S denotes the zero-coordinates of y, and $$\\eta$$ such that\n$$\\|P_S \\eta_S\\|, \\|P_{S^c} \\eta_{S^c}\\| \\leq B$$, if the max eigenvalue $$\\lambda_{\\text{max}}(P)$$ satisfies\n$$\\lambda_{\\text{max}}(P)$$\nthen for all $$\\theta \\in \\mathbb{R}^d$$, we have\n$$\\lambda_{\\text{min}}(P^{*}) \\leq C$$.\n\nProof. We have\n$$\\gamma_{\\theta,P} \\leq d^2 \\log(C) + 3B^2$$.\n$$\\gamma_{\\theta,\\Sigma} \\leq 1/2 \\log |\\Sigma^{*}|$$.\nFrom Lemma C.9, C.10, we have\n$$|\\Sigma| + g_{\\theta,\\Sigma} - g_{\\theta^{*},\\Sigma^{*}} + h_{\\theta,\\Sigma} - h_{\\theta^{*},\\Sigma^{*}} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |P^{*}| + 3B^2$$.\nSubstituting in Eqn (23), we get\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |\\Sigma^{*}| / |\\Sigma| + 1/2 \\log |P^{*}| / |P_S| + 3B^2$$.\nAs $$(P^{*}_S)^{-1} = \\Sigma^{*}_S - \\Sigma^{*}_{S^c} \\Sigma^{*^{-1}}_{ScS}$$, by the matrix determinant rule, we have\nThis gives\n$$\\Sigma^{*}_{Sc} \\log|\\Sigma^{*}| + \\log|P^{*}_S| = \\log|\\Sigma^{*}_{Sc}|$$.\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + 1/2 \\log |\\Sigma^{*}_{Sc}| / |\\Sigma_{Sc}| + 3B^2$$.\nThis gives\n$$\\gamma_{\\theta,\\Sigma} \\leq g_{\\theta,\\Sigma} + d/2 \\log \\lambda_{\\text{max}}(\\Sigma^{*}) / \\lambda_{\\text{min}}(\\Sigma) + 3B^2$$,\n$$= g_{\\theta,\\Sigma} + d/2 \\log \\lambda_{\\text{max}}(P) / \\lambda_{\\text{min}}(P^{*}) + 3B^2$$."}]}, {"page": 26, "text": " As the matrix \u03a3\u22121      Sc is positive definite, we trivially get\n                            g\u03b8 i,\u03a3(y) = \u2212(yi,Sc \u2212            \u03b8i,Sc)T (\u03a3Sc)\u22121(yi,Sc \u2212             \u03b8i,Sc)/2 \u2264       0.\n Substituting in Eqn (24), we get\n                                                 \u03b3\u03b8,\u03a3 \u2264      d\n                                                             2 log \u03bbmax(P         )\n As the Lemma assumes                                                \u03bbmin(P \u2217) + 3B2.\n we get                                               \u03bbmax(P      ) \u2264   C\u03bbmin(P \u2217),\n                                                      \u03b3\u03b8,\u03a3 \u2264      d\n                                                                  2 log(C) + 3B2.\n Lemma C.9. Consider the function g defined in Eq (21). For the ground truth parameters \u03b8\u2217, \u03a3\u2217,\n the function g\u03b8\u2217,\u03a3\u2217         satisfies                 \u2212g\u03b8\u2217,\u03a3\u2217      \u2264   1          \u03a3Sc ,\n which is, with probability 1 \u2212             e\u2212\u2126(d),                     2\u2225\u03b7Sc\u22252\n                                                            \u2212g\u03b8\u2217,\u03a3\u2217      \u2264   O(d).\n Proof. As ySc are the positive valued coordinates in y, we have\n which gives                                               ySc \u2212     \u03b8\u2217Sc = \u03b7Sc,\n                                  g\u03b8\u2217,\u03a3\u2217(y) = \u2212(ySc \u2212              \u03b8\u2217Sc)T (\u03a3\u2217   Sc)\u22121(ySc \u2212        \u03b8\u2217Sc)/2,\n                                                 = \u2212\u2225\u03b7Sc\u22252       \u03a3Sc /2.\n As \u03b7Sc is Gaussian with covariance \u03a3\u2217               Sc, the expected norm is |Sc|        2 , which implies that with probability\n1 \u2212    e\u2212\u2126(|Sc|), we have                              \u2212g\u03b8\u2217,\u03a3\u2217(y) \u2264          O(|Sc|).\n Lemma C.10. Consider y generated according to Eqn (20) by\n                                                y = \u03d5(\u03b8\u2217       + \u03b7),       \u03b7 \u223c    N  (0, \u03a3\u2217).\nFor all \u03b8 \u2208        Rd, \u03a3 \u2208       Rd\u00d7d, and the function h\u03b8,\u03a3 defined in Eqn (22), the difference h\u03b8,\u03a3(y) \u2212\n                                   +\n h\u03b8\u2217,\u03a3\u2217(y) satisfies\n     h\u03b8,\u03a3(y) \u2212       h\u03b8\u2217,\u03a3\u2217(y) \u2264         1           S|           \u221712                    \u221712                     \u03a3Sc + O(|S|),      (25)\n                                         2 log |P \u2217              Sc \u03b7Sc\u22252 + 2\u2225PS \u03b7S\u22252 \u2212                 \u2225\u03b7Sc\u22252\n                                                  |PS| + \u2225P\n where P \u2217      = \u03a3\u2217\u22121 is the precision matrix of \u03b7.\n Proof. For \u03b8 \u2208        Rd, \u03a3 \u2208      Rd\u00d7d, and P = \u03a3\u22121, we have\n                                       +\n                                                           1\n              h\u03b8,\u03a3(y) = log           t\u22640  exp     \u2212\u2225P    S2(t1\u2212    \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212                  \u03b8Sc)\u22252/2        ,\n                           \u2264  log     t\u2208R|S| exp       \u2212\u2225P   S2(t \u2212    \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212                   \u03b8Sc)\u22252/2       ,\n                           \u2264   |S|                                                                                                  (26)\n                                2 log(2\u03c0) \u2212          1\n                                                     2 log|PS|,\n                                                                       26", "md": "As the matrix $$\\Sigma^{-1} Sc$$ is positive definite, we trivially get\n\n$$\ng_{\\theta i,\\Sigma}(y) = -(y_i,Sc - \\theta_i,Sc)^T (\\Sigma Sc)^{-1}(y_i,Sc - \\theta_i,Sc)/2 \\leq 0.\n$$\n\nSubstituting in Eqn (24), we get\n\n$$\n\\gamma_{\\theta,\\Sigma} \\leq \\frac{d}{2} \\log \\lambda_{\\text{max}}(P)\n$$\n\nAs the Lemma assumes $$\\lambda_{\\text{min}}(P^*) + 3B^2$$, we get\n\n$$\n\\lambda_{\\text{max}}(P) \\leq C\\lambda_{\\text{min}}(P^*),\n$$\n\n$$\n\\gamma_{\\theta,\\Sigma} \\leq \\frac{d}{2} \\log(C) + 3B^2.\n$$\n\nLemma C.9. Consider the function g defined in Eq (21). For the ground truth parameters $$\\theta^*, \\Sigma^*$$, the function $$g_{\\theta^*,\\Sigma^*}$$ satisfies\n\n$$\n-g_{\\theta^*,\\Sigma^*} \\leq \\frac{1}{\\Sigma Sc},\n$$\n\nwhich is, with probability $$1 - e^{-\\Omega(d)}, \\frac{2\\| \\eta Sc \\|_2}{-g_{\\theta^*,\\Sigma^*}} \\leq O(d)$$.\n\nProof. As $$ySc$$ are the positive valued coordinates in y, we have\n\n$$\nySc - \\theta^*Sc = \\eta Sc,\n$$\n\n$$\ng_{\\theta^*,\\Sigma^*}(y) = -(ySc - \\theta^*Sc)^T (\\Sigma^* Sc)^{-1}(ySc - \\theta^*Sc)/2,\n$$\n\n$$\n= -\\| \\eta Sc \\|_2 \\Sigma Sc /2.\n$$\n\nAs $$\\eta Sc$$ is Gaussian with covariance $$\\Sigma^* Sc$$, the expected norm is $$|Sc|^2$$, which implies that with probability $$1 - e^{-\\Omega(|Sc|)}, -g_{\\theta^*,\\Sigma^*}(y) \\leq O(|Sc|)$$.\n\nLemma C.10. Consider y generated according to Eqn (20) by\n\n$$\ny = \\phi(\\theta^* + \\eta), \\eta \\sim N(0, \\Sigma^*).\n$$\n\nFor all $$\\theta \\in \\mathbb{R}^d, \\Sigma \\in \\mathbb{R}^{d \\times d}$$, and the function $$h_{\\theta,\\Sigma}$$ defined in Eqn (22), the difference $$h_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y)$$ satisfies\n\n$$\nh_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y) \\leq \\frac{1}{S|\\Sigma^*_{12}|} \\Sigma^*_{12} \\Sigma Sc + O(|S|), (25)\n$$\n\n$$\n\\frac{2 \\log |P^*|}{|PS| + \\|P\\|} + 2\\|PS \\eta S\\|_2 - \\| \\eta Sc \\|_2\n$$\n\nwhere $$P^* = \\Sigma^*{-1}$$ is the precision matrix of $$\\eta$$.\n\nProof. For $$\\theta \\in \\mathbb{R}^d, \\Sigma \\in \\mathbb{R}^{d \\times d}$$, and $$P = \\Sigma^{-1}$$, we have\n\n$$\nh_{\\theta,\\Sigma}(y) = \\log \\left( \\sum_{t \\leq 0} \\exp \\left( -\\|P_S^2(t1 - \\theta_S) + (PS)^{-1/2}PSSc(ySc - \\theta_Sc)\\|_2/2 \\right) \\right),\n$$\n\n$$\n\\leq \\log \\left( \\sum_{t \\in \\mathbb{R}|S|} \\exp \\left( -\\|P_S^2(t - \\theta_S) + (PS)^{-1/2}PSSc(ySc - \\theta_Sc)\\|_2/2 \\right) \\right),\n$$\n\n$$\n\\leq |S| \\left( \\frac{2 \\log(2\\pi)}{2 \\log|PS|} \\right).\n$$", "images": [], "items": [{"type": "text", "value": "As the matrix $$\\Sigma^{-1} Sc$$ is positive definite, we trivially get\n\n$$\ng_{\\theta i,\\Sigma}(y) = -(y_i,Sc - \\theta_i,Sc)^T (\\Sigma Sc)^{-1}(y_i,Sc - \\theta_i,Sc)/2 \\leq 0.\n$$\n\nSubstituting in Eqn (24), we get\n\n$$\n\\gamma_{\\theta,\\Sigma} \\leq \\frac{d}{2} \\log \\lambda_{\\text{max}}(P)\n$$\n\nAs the Lemma assumes $$\\lambda_{\\text{min}}(P^*) + 3B^2$$, we get\n\n$$\n\\lambda_{\\text{max}}(P) \\leq C\\lambda_{\\text{min}}(P^*),\n$$\n\n$$\n\\gamma_{\\theta,\\Sigma} \\leq \\frac{d}{2} \\log(C) + 3B^2.\n$$\n\nLemma C.9. Consider the function g defined in Eq (21). For the ground truth parameters $$\\theta^*, \\Sigma^*$$, the function $$g_{\\theta^*,\\Sigma^*}$$ satisfies\n\n$$\n-g_{\\theta^*,\\Sigma^*} \\leq \\frac{1}{\\Sigma Sc},\n$$\n\nwhich is, with probability $$1 - e^{-\\Omega(d)}, \\frac{2\\| \\eta Sc \\|_2}{-g_{\\theta^*,\\Sigma^*}} \\leq O(d)$$.\n\nProof. As $$ySc$$ are the positive valued coordinates in y, we have\n\n$$\nySc - \\theta^*Sc = \\eta Sc,\n$$\n\n$$\ng_{\\theta^*,\\Sigma^*}(y) = -(ySc - \\theta^*Sc)^T (\\Sigma^* Sc)^{-1}(ySc - \\theta^*Sc)/2,\n$$\n\n$$\n= -\\| \\eta Sc \\|_2 \\Sigma Sc /2.\n$$\n\nAs $$\\eta Sc$$ is Gaussian with covariance $$\\Sigma^* Sc$$, the expected norm is $$|Sc|^2$$, which implies that with probability $$1 - e^{-\\Omega(|Sc|)}, -g_{\\theta^*,\\Sigma^*}(y) \\leq O(|Sc|)$$.\n\nLemma C.10. Consider y generated according to Eqn (20) by\n\n$$\ny = \\phi(\\theta^* + \\eta), \\eta \\sim N(0, \\Sigma^*).\n$$\n\nFor all $$\\theta \\in \\mathbb{R}^d, \\Sigma \\in \\mathbb{R}^{d \\times d}$$, and the function $$h_{\\theta,\\Sigma}$$ defined in Eqn (22), the difference $$h_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y)$$ satisfies\n\n$$\nh_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y) \\leq \\frac{1}{S|\\Sigma^*_{12}|} \\Sigma^*_{12} \\Sigma Sc + O(|S|), (25)\n$$\n\n$$\n\\frac{2 \\log |P^*|}{|PS| + \\|P\\|} + 2\\|PS \\eta S\\|_2 - \\| \\eta Sc \\|_2\n$$\n\nwhere $$P^* = \\Sigma^*{-1}$$ is the precision matrix of $$\\eta$$.\n\nProof. For $$\\theta \\in \\mathbb{R}^d, \\Sigma \\in \\mathbb{R}^{d \\times d}$$, and $$P = \\Sigma^{-1}$$, we have\n\n$$\nh_{\\theta,\\Sigma}(y) = \\log \\left( \\sum_{t \\leq 0} \\exp \\left( -\\|P_S^2(t1 - \\theta_S) + (PS)^{-1/2}PSSc(ySc - \\theta_Sc)\\|_2/2 \\right) \\right),\n$$\n\n$$\n\\leq \\log \\left( \\sum_{t \\in \\mathbb{R}|S|} \\exp \\left( -\\|P_S^2(t - \\theta_S) + (PS)^{-1/2}PSSc(ySc - \\theta_Sc)\\|_2/2 \\right) \\right),\n$$\n\n$$\n\\leq |S| \\left( \\frac{2 \\log(2\\pi)}{2 \\log|PS|} \\right).\n$$", "md": "As the matrix $$\\Sigma^{-1} Sc$$ is positive definite, we trivially get\n\n$$\ng_{\\theta i,\\Sigma}(y) = -(y_i,Sc - \\theta_i,Sc)^T (\\Sigma Sc)^{-1}(y_i,Sc - \\theta_i,Sc)/2 \\leq 0.\n$$\n\nSubstituting in Eqn (24), we get\n\n$$\n\\gamma_{\\theta,\\Sigma} \\leq \\frac{d}{2} \\log \\lambda_{\\text{max}}(P)\n$$\n\nAs the Lemma assumes $$\\lambda_{\\text{min}}(P^*) + 3B^2$$, we get\n\n$$\n\\lambda_{\\text{max}}(P) \\leq C\\lambda_{\\text{min}}(P^*),\n$$\n\n$$\n\\gamma_{\\theta,\\Sigma} \\leq \\frac{d}{2} \\log(C) + 3B^2.\n$$\n\nLemma C.9. Consider the function g defined in Eq (21). For the ground truth parameters $$\\theta^*, \\Sigma^*$$, the function $$g_{\\theta^*,\\Sigma^*}$$ satisfies\n\n$$\n-g_{\\theta^*,\\Sigma^*} \\leq \\frac{1}{\\Sigma Sc},\n$$\n\nwhich is, with probability $$1 - e^{-\\Omega(d)}, \\frac{2\\| \\eta Sc \\|_2}{-g_{\\theta^*,\\Sigma^*}} \\leq O(d)$$.\n\nProof. As $$ySc$$ are the positive valued coordinates in y, we have\n\n$$\nySc - \\theta^*Sc = \\eta Sc,\n$$\n\n$$\ng_{\\theta^*,\\Sigma^*}(y) = -(ySc - \\theta^*Sc)^T (\\Sigma^* Sc)^{-1}(ySc - \\theta^*Sc)/2,\n$$\n\n$$\n= -\\| \\eta Sc \\|_2 \\Sigma Sc /2.\n$$\n\nAs $$\\eta Sc$$ is Gaussian with covariance $$\\Sigma^* Sc$$, the expected norm is $$|Sc|^2$$, which implies that with probability $$1 - e^{-\\Omega(|Sc|)}, -g_{\\theta^*,\\Sigma^*}(y) \\leq O(|Sc|)$$.\n\nLemma C.10. Consider y generated according to Eqn (20) by\n\n$$\ny = \\phi(\\theta^* + \\eta), \\eta \\sim N(0, \\Sigma^*).\n$$\n\nFor all $$\\theta \\in \\mathbb{R}^d, \\Sigma \\in \\mathbb{R}^{d \\times d}$$, and the function $$h_{\\theta,\\Sigma}$$ defined in Eqn (22), the difference $$h_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y)$$ satisfies\n\n$$\nh_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y) \\leq \\frac{1}{S|\\Sigma^*_{12}|} \\Sigma^*_{12} \\Sigma Sc + O(|S|), (25)\n$$\n\n$$\n\\frac{2 \\log |P^*|}{|PS| + \\|P\\|} + 2\\|PS \\eta S\\|_2 - \\| \\eta Sc \\|_2\n$$\n\nwhere $$P^* = \\Sigma^*{-1}$$ is the precision matrix of $$\\eta$$.\n\nProof. For $$\\theta \\in \\mathbb{R}^d, \\Sigma \\in \\mathbb{R}^{d \\times d}$$, and $$P = \\Sigma^{-1}$$, we have\n\n$$\nh_{\\theta,\\Sigma}(y) = \\log \\left( \\sum_{t \\leq 0} \\exp \\left( -\\|P_S^2(t1 - \\theta_S) + (PS)^{-1/2}PSSc(ySc - \\theta_Sc)\\|_2/2 \\right) \\right),\n$$\n\n$$\n\\leq \\log \\left( \\sum_{t \\in \\mathbb{R}|S|} \\exp \\left( -\\|P_S^2(t - \\theta_S) + (PS)^{-1/2}PSSc(ySc - \\theta_Sc)\\|_2/2 \\right) \\right),\n$$\n\n$$\n\\leq |S| \\left( \\frac{2 \\log(2\\pi)}{2 \\log|PS|} \\right).\n$$"}]}, {"page": 27, "text": " where the last step follows from the integral of a Gaussian pdf. This gives a sufficient upper bound\n on h\u03b8,\u03a3(y), and now we will focus on lower bounding h\u03b8\u2217,\u03a3\u2217(y).\n For the coordinates of y in Sc, we have ySc \u2212                      \u03b8\u2217Sc = \u03b7Sc. Substituting in Eqn (22), we get\n                    h\u03b8\u2217,\u03a3\u2217(y) = log                 exp    \u2212\u2225P     \u221712         S) + (P \u2217   S)\u22121/2P \u2217    SSc\u03b7Sc\u22252/2          ,\n                                              t\u22640                 S (t \u2212      \u03b8\u2217\n                                    = log           exp    \u2212\u2225P     \u221712         S) + (P \u2217   S)\u22121/2P \u2217    SSc\u03b7Sc\u22252/2          .\n Using \u2225a + b\u22252 \u2264           2a2 + 2b2, we get t\u22640                 S (t \u2212      \u03b8\u2217                         \u221712                  .\n                 h\u03b8\u2217,\u03a3\u2217(y) \u2265        \u2212   \u2225(P \u2217 S)\u22121/2P \u2217                                   exp    \u2212\u2225P                 S)\u22252\n                 \u221712                                      SSc\u03b7Sc\u22252 + log            t\u22640                 S (t \u2212      \u03b8\u2217\n Set u := P                  S), and by the change of variables formula, we get:\n                S (t \u2212      \u03b8\u2217\n        h\u03b8\u2217,\u03a3\u2217(y) \u2265        \u2212   \u2225(P \u2217S)\u22121/2P \u2217    SSc\u03b7Sc\u22252 + log            PS\u2217\u221212 u+\u03b8\u2217  S\u22640   (P \u2217S)\u22121/2       \u00b7 exp    \u2212\u2225u\u22252 ,\n                       = \u2212     \u2225(P \u2217S)\u22121/2P \u2217                               P \u2217\u22121      + log                         exp    \u2212\u2225u\u22252 .\n                                                 SSc\u03b7Sc\u22252 + 1       2 log      S                 P S\u2217\u221212 u+\u03b8\u2217  S\u22640\n For i \u2208    S, we have \u03b8\u2217      i + \u03b7i \u2264      0. This gives\n P \u2217\u22121  2u \u2264    \u03b7S \u21d2      P  \u2217\u22121 2 u+\u03b8\u2217  S \u2264    0 \u21d2     log                       exp    \u2212\u2225u\u22252        \u2265   log                    exp    \u2212\u2225u\u22252 ,\n   S                        S                                  P \u2217\u22121 2 u+\u03b8\u2217                                      P \u2217\u22121 2 u\u2264\u03b7S\n using which we get                                              S          S\u22640                                    S\n           h\u03b8\u2217,\u03a3\u2217(y) \u2265        \u2212   \u2225(PS)\u2217\u22121/2P \u2217      SSc\u03b7Sc\u22252 \u2212         1          S| + log         \u2217\u22121 2         exp    \u2212\u2225u\u22252 .\n                                                              \u2217\u22121 2     2 log|P \u2217                 P S     u\u2264\u03b7S\n By another change of variables via v := P                    S     u \u2212    \u03b7S, we get\n h\u03b8\u2217,\u03a3\u2217(y) \u2265        \u2212  \u2225(P \u2217             SSc\u03b7Sc\u22252 \u2212         1                               P  \u221712   exp     \u2212\u2225P    \u221712                   ,\n                             S)\u22121/2P \u2217                      2 log|P \u2217  S| + log       v\u22640      S                    S (v + \u03b7S)\u22252\n                \u2265   \u2212  \u2225(P \u2217 S)\u22121/2P \u2217   SSc\u03b7Sc\u22252 \u2212         1          S| \u2212    2\u2225P   \u221712                          P  \u221712   exp     \u22122\u2225P     \u221712   ,\n                                                            2 log|P \u2217                S \u03b7S\u22252 + log           v\u22640      S                     S v\u22252\n                = \u2212    \u2225(P \u2217             SSc\u03b7Sc\u22252 \u2212         1                        \u221712\n                             S)\u22121/2P \u2217                      2 log|P \u2217  S| \u2212    2\u2225PS \u03b7S\u22252 + O(|S|).\n As (\u03a3\u2217                            ScS(P \u2217   S)\u22121P \u2217  SSc, we have\n         Sc)\u22121 = P \u2217     Sc \u2212    P \u2217\n                                \u2212\u2225(PS)\u2217\u22121/2P \u2217         SSc\u03b7Sc\u22252 = \u2225\u03b7Sc\u22252          \u03a3\u2217            Sc     \u03b7Sc\u22252,\n which gives                                                                        Sc \u2212    \u2225P \u22171/2\n                 h\u03b8\u2217,\u03a3\u2217(y) \u2265\u2225\u03b7Sc\u22252          \u03a3\u2217             \u221712                         S| \u2212   2\u2225P    \u221712                                  (27)\n                                                          Sc \u03b7Sc\u22252 \u2212        1                       S \u03b7S\u22252 + O(|S|).\n From Eqn (26) \u2212           Eqn (27), we get   Sc \u2212    \u2225P                    2 log|P \u2217\n     h\u03b8,\u03a3(y) \u2212       h\u03b8\u2217,\u03a3\u2217(y) \u2264         \u2225P  \u221712                     \u221712                     \u03a3\u2217                    S|                     (28)\n                                             Sc \u03b7Sc\u22252 + 2\u2225PS \u03b7S\u22252 \u2212                 \u2225\u03b7Sc\u22252     Sc + 1  2 log |P \u2217\n                                                                                                                |PS| + O(|S|).\n Lemma C.4. Under Assumption 4.4, C.3, consider P \u2208                                Rd\u00d7d     such that \u03bbmax(P )\n                                                                                      +                    \u03bbmin(P ) \u2264      \u03ba and\n                                              \u03bbmax(P      )          \u03ba3d2n2        + B2n\u03ba          .\n                                             \u03bbmax(P \u2217) \u2265         O         k2              k\nThen, for all W \u2208           Rd\u00d7k, and for all yi\u00af= \u03d5(W \u2217xi + \u03b7i) with \u2225PSc \u03b7Sc\u2225, \u2225PS \u03b7S\u2225       \u221712              \u221712        \u2264   B, we have\n                                                \u03b3W,P := 1     n  i\u2208[n]  \u03b3W x   i,P (yi) < 0.\n                                                                       27", "md": "where the last step follows from the integral of a Gaussian pdf. This gives a sufficient upper bound\non $$h_{\\theta,\\Sigma}(y)$$, and now we will focus on lower bounding $$h_{\\theta^*,\\Sigma^*}(y)$$.\n\nFor the coordinates of y in $$S_c$$, we have $$y_{S_c} - \\theta^*_{S_c} = \\eta_{S_c}$$. Substituting in Eqn (22), we get\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &= \\log \\exp\\left(-\\left\\|P^*_{12}S\\right) + (P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2/2\\right), \\\\\n&= \\log \\exp\\left(-\\left\\|P^*_{12}S\\right) + (P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2/2\\right).\n\\end{align*}\n$$\nUsing $$\\left\\|a + b\\right\\|^2 \\leq 2a^2 + 2b^2$$, we get\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq -\\left\\|(P^*S)^{-1/2}P^*\\right\\| \\exp\\left(-\\left\\|P^*S\\right)\\right\\|^2 + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right),\n$$\nSet $$u := P^*S$$, and by the change of variables formula, we get:\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &\\geq -\\left\\|(P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right), \\\\\n&= -\\left\\|(P^*S)^{-1/2}P^*\\right\\| + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right).\n\\end{align*}\n$$\nFor $$i \\in S$$, we have $$\\theta^*_i + \\eta_i \\leq 0$$. This gives\n\n$$\nP^* \\leq \\eta_S \\Rightarrow P^* + \\theta^*S \\leq 0 \\Rightarrow \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right) \\geq \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right),\n$$\nusing which we get\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq -\\left\\|(PS)^*{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right).\n$$\nBy another change of variables via $$v := PSu - \\eta_S$$, we get\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &\\geq -\\left\\|(P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| + \\log \\left(v \\leq 0 S(v + \\eta_S)\\right\\|^2), \\\\\n&\\geq -\\left\\|(P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| - 2\\left\\|P^*_{12}\\right\\| \\exp\\left(-2\\left\\|P^*_{12}\\right\\|^2\\right) + O(|S|).\n\\end{align*}\n$$\nAs $$\\left(\\Sigma^*_{Sc}S(P^*S)^{-1}P^*SS_{c}\\right$$, we have\n\n$$\n\\Sigma^*_{Sc}^{-1} = P^*_{Sc} - P^* \\Rightarrow -\\left\\|(PS)^*{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 = \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma^*_{Sc} \\eta_{S_{c}}\\right\\|^2,\n$$\nwhich gives\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma^*_{Sc} - 2\\left\\|P^*_{12}\\right\\| \\left(27\\right) \\Sigma^*_{Sc} \\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\left|S\\right| \\eta_S\\right\\|^2 + O(|S|).\n$$\nFrom Eqn (26) - Eqn (27), we get\n\n$$\nh_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y) \\leq \\left\\|P^*_{12}\\right\\| \\left\\|P^*_{12}\\right\\| \\Sigma^*_{S} \\eta_{Sc}\\right\\|^2 + 2\\left\\|PS\\right\\| \\eta_S\\right\\|^2 - \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma_{Sc} + \\frac{1}{2} \\log \\left|P^*S\\right| + O(|S|).\n$$\n\nLemma C.4. Under Assumption 4.4, C.3, consider $$P \\in \\mathbb{R}^{d \\times d}$$ such that $$\\lambda_{\\text{max}}(P) + \\lambda_{\\text{min}}(P) \\leq \\kappa$$ and\n\n$$\n\\begin{align*}\n\\lambda_{\\text{max}}(P) &\\leq \\kappa^{3d^2n^2} + B^2n\\kappa, \\\\\n\\lambda_{\\text{max}}(P^*) &\\geq O(k^2)k.\n\\end{align*}\n$$\nThen, for all $$W \\in \\mathbb{R}^{d \\times k}$$, and for all $$\\bar{y}_i = \\phi(W^*x_i + \\eta_i)$$ with $$\\left\\|PSc \\eta_{Sc}\\right\\|, \\left\\|PS \\eta_S\\right\\| \\leq B$$, we have\n\n$$\n\\gamma_{W,P} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{W x_i,P} (y_i) < 0.\n$$", "images": [], "items": [{"type": "text", "value": "where the last step follows from the integral of a Gaussian pdf. This gives a sufficient upper bound\non $$h_{\\theta,\\Sigma}(y)$$, and now we will focus on lower bounding $$h_{\\theta^*,\\Sigma^*}(y)$$.\n\nFor the coordinates of y in $$S_c$$, we have $$y_{S_c} - \\theta^*_{S_c} = \\eta_{S_c}$$. Substituting in Eqn (22), we get\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &= \\log \\exp\\left(-\\left\\|P^*_{12}S\\right) + (P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2/2\\right), \\\\\n&= \\log \\exp\\left(-\\left\\|P^*_{12}S\\right) + (P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2/2\\right).\n\\end{align*}\n$$\nUsing $$\\left\\|a + b\\right\\|^2 \\leq 2a^2 + 2b^2$$, we get\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq -\\left\\|(P^*S)^{-1/2}P^*\\right\\| \\exp\\left(-\\left\\|P^*S\\right)\\right\\|^2 + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right),\n$$\nSet $$u := P^*S$$, and by the change of variables formula, we get:\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &\\geq -\\left\\|(P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right), \\\\\n&= -\\left\\|(P^*S)^{-1/2}P^*\\right\\| + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right).\n\\end{align*}\n$$\nFor $$i \\in S$$, we have $$\\theta^*_i + \\eta_i \\leq 0$$. This gives\n\n$$\nP^* \\leq \\eta_S \\Rightarrow P^* + \\theta^*S \\leq 0 \\Rightarrow \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right) \\geq \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right),\n$$\nusing which we get\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq -\\left\\|(PS)^*{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right).\n$$\nBy another change of variables via $$v := PSu - \\eta_S$$, we get\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &\\geq -\\left\\|(P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| + \\log \\left(v \\leq 0 S(v + \\eta_S)\\right\\|^2), \\\\\n&\\geq -\\left\\|(P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| - 2\\left\\|P^*_{12}\\right\\| \\exp\\left(-2\\left\\|P^*_{12}\\right\\|^2\\right) + O(|S|).\n\\end{align*}\n$$\nAs $$\\left(\\Sigma^*_{Sc}S(P^*S)^{-1}P^*SS_{c}\\right$$, we have\n\n$$\n\\Sigma^*_{Sc}^{-1} = P^*_{Sc} - P^* \\Rightarrow -\\left\\|(PS)^*{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 = \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma^*_{Sc} \\eta_{S_{c}}\\right\\|^2,\n$$\nwhich gives\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma^*_{Sc} - 2\\left\\|P^*_{12}\\right\\| \\left(27\\right) \\Sigma^*_{Sc} \\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\left|S\\right| \\eta_S\\right\\|^2 + O(|S|).\n$$\nFrom Eqn (26) - Eqn (27), we get\n\n$$\nh_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y) \\leq \\left\\|P^*_{12}\\right\\| \\left\\|P^*_{12}\\right\\| \\Sigma^*_{S} \\eta_{Sc}\\right\\|^2 + 2\\left\\|PS\\right\\| \\eta_S\\right\\|^2 - \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma_{Sc} + \\frac{1}{2} \\log \\left|P^*S\\right| + O(|S|).\n$$\n\nLemma C.4. Under Assumption 4.4, C.3, consider $$P \\in \\mathbb{R}^{d \\times d}$$ such that $$\\lambda_{\\text{max}}(P) + \\lambda_{\\text{min}}(P) \\leq \\kappa$$ and\n\n$$\n\\begin{align*}\n\\lambda_{\\text{max}}(P) &\\leq \\kappa^{3d^2n^2} + B^2n\\kappa, \\\\\n\\lambda_{\\text{max}}(P^*) &\\geq O(k^2)k.\n\\end{align*}\n$$\nThen, for all $$W \\in \\mathbb{R}^{d \\times k}$$, and for all $$\\bar{y}_i = \\phi(W^*x_i + \\eta_i)$$ with $$\\left\\|PSc \\eta_{Sc}\\right\\|, \\left\\|PS \\eta_S\\right\\| \\leq B$$, we have\n\n$$\n\\gamma_{W,P} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{W x_i,P} (y_i) < 0.\n$$", "md": "where the last step follows from the integral of a Gaussian pdf. This gives a sufficient upper bound\non $$h_{\\theta,\\Sigma}(y)$$, and now we will focus on lower bounding $$h_{\\theta^*,\\Sigma^*}(y)$$.\n\nFor the coordinates of y in $$S_c$$, we have $$y_{S_c} - \\theta^*_{S_c} = \\eta_{S_c}$$. Substituting in Eqn (22), we get\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &= \\log \\exp\\left(-\\left\\|P^*_{12}S\\right) + (P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2/2\\right), \\\\\n&= \\log \\exp\\left(-\\left\\|P^*_{12}S\\right) + (P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2/2\\right).\n\\end{align*}\n$$\nUsing $$\\left\\|a + b\\right\\|^2 \\leq 2a^2 + 2b^2$$, we get\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq -\\left\\|(P^*S)^{-1/2}P^*\\right\\| \\exp\\left(-\\left\\|P^*S\\right)\\right\\|^2 + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right),\n$$\nSet $$u := P^*S$$, and by the change of variables formula, we get:\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &\\geq -\\left\\|(P^*S)^{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right), \\\\\n&= -\\left\\|(P^*S)^{-1/2}P^*\\right\\| + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right).\n\\end{align*}\n$$\nFor $$i \\in S$$, we have $$\\theta^*_i + \\eta_i \\leq 0$$. This gives\n\n$$\nP^* \\leq \\eta_S \\Rightarrow P^* + \\theta^*S \\leq 0 \\Rightarrow \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right) \\geq \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right),\n$$\nusing which we get\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq -\\left\\|(PS)^*{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| + \\log \\left(\\exp\\left(-\\left\\|u\\right\\|^2\\right)\\right).\n$$\nBy another change of variables via $$v := PSu - \\eta_S$$, we get\n\n$$\n\\begin{align*}\nh_{\\theta^*,\\Sigma^*}(y) &\\geq -\\left\\|(P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| + \\log \\left(v \\leq 0 S(v + \\eta_S)\\right\\|^2), \\\\\n&\\geq -\\left\\|(P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\log \\left|P^*S\\right| - 2\\left\\|P^*_{12}\\right\\| \\exp\\left(-2\\left\\|P^*_{12}\\right\\|^2\\right) + O(|S|).\n\\end{align*}\n$$\nAs $$\\left(\\Sigma^*_{Sc}S(P^*S)^{-1}P^*SS_{c}\\right$$, we have\n\n$$\n\\Sigma^*_{Sc}^{-1} = P^*_{Sc} - P^* \\Rightarrow -\\left\\|(PS)^*{-1/2}P^*SS_{c}\\eta_{S_{c}}\\right\\|^2 = \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma^*_{Sc} \\eta_{S_{c}}\\right\\|^2,\n$$\nwhich gives\n\n$$\nh_{\\theta^*,\\Sigma^*}(y) \\geq \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma^*_{Sc} - 2\\left\\|P^*_{12}\\right\\| \\left(27\\right) \\Sigma^*_{Sc} \\eta_{S_{c}}\\right\\|^2 - \\frac{1}{2} \\left|S\\right| \\eta_S\\right\\|^2 + O(|S|).\n$$\nFrom Eqn (26) - Eqn (27), we get\n\n$$\nh_{\\theta,\\Sigma}(y) - h_{\\theta^*,\\Sigma^*}(y) \\leq \\left\\|P^*_{12}\\right\\| \\left\\|P^*_{12}\\right\\| \\Sigma^*_{S} \\eta_{Sc}\\right\\|^2 + 2\\left\\|PS\\right\\| \\eta_S\\right\\|^2 - \\left\\|\\eta_{S_{c}}\\right\\|^2 \\Sigma_{Sc} + \\frac{1}{2} \\log \\left|P^*S\\right| + O(|S|).\n$$\n\nLemma C.4. Under Assumption 4.4, C.3, consider $$P \\in \\mathbb{R}^{d \\times d}$$ such that $$\\lambda_{\\text{max}}(P) + \\lambda_{\\text{min}}(P) \\leq \\kappa$$ and\n\n$$\n\\begin{align*}\n\\lambda_{\\text{max}}(P) &\\leq \\kappa^{3d^2n^2} + B^2n\\kappa, \\\\\n\\lambda_{\\text{max}}(P^*) &\\geq O(k^2)k.\n\\end{align*}\n$$\nThen, for all $$W \\in \\mathbb{R}^{d \\times k}$$, and for all $$\\bar{y}_i = \\phi(W^*x_i + \\eta_i)$$ with $$\\left\\|PSc \\eta_{Sc}\\right\\|, \\left\\|PS \\eta_S\\right\\| \\leq B$$, we have\n\n$$\n\\gamma_{W,P} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_{W x_i,P} (y_i) < 0.\n$$"}]}, {"page": 28, "text": "Proof of Lemma C.4. For each W \u2208                     Rd\u00d7k, let \u03b8i := W      xi.\nFrom Eqn (24) in Lemma C.2, for each i \u2208                       [n], we have,\n                                         \u03b3\u03b8 i,\u03a3 \u2264    g\u03b8i,\u03a3 + d    2 log \u03bbmax(P         )\n                                                                          \u03bbmin(P \u2217) + 3B2,\n                                                  \u2264  g\u03b8i,\u03a3 + d    2 log \u03ba\u03bbmax(P         )\n                                                                          \u03bbmax(P \u2217) + 3B2.\nNow consider\n                          g\u03b8i,\u03a3(y) = \u22121        2(yi,Sc \u2212      \u03b8i,Sc)T (\u03a3Sc)\u22121(yi,Sc \u2212             \u03b8i,Sc),\n                                        \u2264  \u22121                          \u03a3\u22121     ,\n                                               2\u2225y \u2212     \u03b8\u22252\u03bbmin         Sc\n                                        \u2264  \u22121  2\u2225y \u2212     \u03b8\u22252\u03bbmin       \u03a3\u22121      = \u22121   2\u2225y \u2212     \u03b8\u22252\u03bbmin(P       ),\n                                        \u2264  \u22121  2\u2225y \u2212     \u03b8\u22252 \u03bbmax(P \u03ba     ) ,\nwhere the second inequality comes from the eigenvalue interlacing Theorem, and the last line follows\nfrom the condition number assumption on \u03a3, P.\nBy Assumption C.3, there exist at least \u03b52n samples for a coordinate j such that (yi)j > 0. Averaging\ng\u03b8 i,\u03a3, by Lemma A.1, we get that with high probability,\n                                                                                  j k\nwhich gives                                             i   \u2225yi \u2212    \u03b8i\u22252 \u2265     \u03c3\u221722    ,\n                                             1        g\u03b8i,\u03a3(yi) \u2264       \u2212\u03c3\u22172  j k\u03bbmax(P        ),\n                                             n     i                               4n\u03ba\n                                                                              k\u03bbmax(P       )\nThis gives                                                          \u2264   \u2212   4n\u03ba\u03bbmax(P \u2217).\n                            \u00af                 \u03bbmax(P      )k\n                            \u03b3W,\u03a3 \u2264      \u2212   4n\u03ba\u03bbmax(P \u2217) + d          2 log     \u03ba \u00b7 \u03bbmax(P        )     + 3B2,\n                                                                                     \u03bbmax(P \u2217)\n                                    \u2264   \u2212     \u03bbmax(P      )k               \u03ba \u00b7 \u03bbmax(P        )\n                                            4n\u03ba\u03bbmax(P \u2217) + d                    \u03bbmax(P \u2217) + 3B2.\nCompleting the squares, we get\n                           \u00af                        \u03bbmax(P      )k              n     2\nFor                        \u03b3W,\u03a3 \u2264      \u2212         4n\u03ba\u03bbmax(P \u2217) \u2212            \u03bad      k      + \u03ba2d2n k      + 3B2.\n                                             \u03bbmax(P      )          \u03ba3d2n2        + B2n\u03ba          ,\nthe above inequality satisfies              \u03bbmax(P \u2217) \u2265         O         k2              k\n                                                                \u00af\n                                                                \u03b3W,\u03a3 \u2264      0.\n                                                                      28", "md": "Proof of Lemma C.4. For each \\(W \\in \\mathbb{R}^{d \\times k}\\), let \\(\\theta_i := W x_i\\).\n\nFrom Eqn (24) in Lemma C.2, for each \\(i \\in [n]\\), we have,\n\n$$\n\\begin{align*}\n&\\gamma \\theta_i, \\Sigma \\leq g \\theta_i, \\Sigma + \\frac{d}{2} \\log \\lambda_{\\text{max}}(P) \\lambda_{\\text{min}}(P^*) + 3B^2, \\\\\n&\\leq g \\theta_i, \\Sigma + \\frac{d}{2} \\log \\kappa \\lambda_{\\text{max}}(P) \\lambda_{\\text{max}}(P^*) + 3B^2.\n\\end{align*}\n$$\nNow consider\n\n$$\n\\begin{align*}\n&g \\theta_i, \\Sigma(y) = -\\frac{1}{2}(y_i, S_c - \\theta_i, S_c)^T (\\Sigma_{S_c})^{-1}(y_i, S_c - \\theta_i, S_c), \\\\\n&\\leq -\\frac{1}{2} \\lambda_{\\text{min}}(\\Sigma_c) \\|y - \\theta\\|^2, \\\\\n&\\leq -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{min}}(\\Sigma^{-1}) = -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{min}}(P), \\\\\n&\\leq -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{max}}(P \\kappa),\n\\end{align*}\n$$\nwhere the second inequality comes from the eigenvalue interlacing Theorem, and the last line follows from the condition number assumption on \\(\\Sigma, P\\).\n\nBy Assumption C.3, there exist at least \\(\\epsilon^2 n\\) samples for a coordinate \\(j\\) such that \\((y_i)_j > 0\\). Averaging \\(g \\theta_i, \\Sigma\\), by Lemma A.1, we get that with high probability,\n\n$$\n\\frac{1}{j} \\sum_{i} \\|y_i - \\theta_i\\|^2 \\geq \\sigma^2_{\\ast 2},\n$$\n$$\n\\frac{1}{n} \\sum_{i} g \\theta_i, \\Sigma(y_i) \\leq -\\sigma^2_{\\ast 2} \\frac{j}{k} \\lambda_{\\text{max}}(P),\n$$\n$$\n\\frac{k \\lambda_{\\text{max}}(P)}{4n\\kappa} \\leq -\\frac{4n\\kappa \\lambda_{\\text{max}}(P^*)}{\\lambda_{\\text{max}}(P)}.$$\nThis gives\n\n$$\n\\begin{align*}\n&\\gamma W, \\Sigma \\leq -\\frac{4n\\kappa \\lambda_{\\text{max}}(P^*) + d}{\\lambda_{\\text{max}}(P)} \\frac{d}{2} \\log \\kappa \\cdot \\lambda_{\\text{max}}(P) + 3B^2, \\\\\n&\\leq -\\frac{\\lambda_{\\text{max}}(P)k}{4n\\kappa \\lambda_{\\text{max}}(P^*) + d} \\frac{\\kappa \\cdot \\lambda_{\\text{max}}(P)}{\\lambda_{\\text{max}}(P^*) + 3B^2}.\n\\end{align*}\n$$\nCompleting the squares, we get\n\n$$\n\\gamma W, \\Sigma \\leq -\\frac{\\lambda_{\\text{max}}(P)k}{4n\\kappa \\lambda_{\\text{max}}(P^*) - \\kappa d k + \\kappa^2 d^2 n k + 3B^2}.\n$$\nFor the above inequality satisfies \\(\\lambda_{\\text{max}}(P^*) \\geq O(k^2)\\),\n\n$$\n\\gamma W, \\Sigma \\leq 0.\n$$\n28", "images": [], "items": [{"type": "text", "value": "Proof of Lemma C.4. For each \\(W \\in \\mathbb{R}^{d \\times k}\\), let \\(\\theta_i := W x_i\\).\n\nFrom Eqn (24) in Lemma C.2, for each \\(i \\in [n]\\), we have,\n\n$$\n\\begin{align*}\n&\\gamma \\theta_i, \\Sigma \\leq g \\theta_i, \\Sigma + \\frac{d}{2} \\log \\lambda_{\\text{max}}(P) \\lambda_{\\text{min}}(P^*) + 3B^2, \\\\\n&\\leq g \\theta_i, \\Sigma + \\frac{d}{2} \\log \\kappa \\lambda_{\\text{max}}(P) \\lambda_{\\text{max}}(P^*) + 3B^2.\n\\end{align*}\n$$\nNow consider\n\n$$\n\\begin{align*}\n&g \\theta_i, \\Sigma(y) = -\\frac{1}{2}(y_i, S_c - \\theta_i, S_c)^T (\\Sigma_{S_c})^{-1}(y_i, S_c - \\theta_i, S_c), \\\\\n&\\leq -\\frac{1}{2} \\lambda_{\\text{min}}(\\Sigma_c) \\|y - \\theta\\|^2, \\\\\n&\\leq -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{min}}(\\Sigma^{-1}) = -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{min}}(P), \\\\\n&\\leq -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{max}}(P \\kappa),\n\\end{align*}\n$$\nwhere the second inequality comes from the eigenvalue interlacing Theorem, and the last line follows from the condition number assumption on \\(\\Sigma, P\\).\n\nBy Assumption C.3, there exist at least \\(\\epsilon^2 n\\) samples for a coordinate \\(j\\) such that \\((y_i)_j > 0\\). Averaging \\(g \\theta_i, \\Sigma\\), by Lemma A.1, we get that with high probability,\n\n$$\n\\frac{1}{j} \\sum_{i} \\|y_i - \\theta_i\\|^2 \\geq \\sigma^2_{\\ast 2},\n$$\n$$\n\\frac{1}{n} \\sum_{i} g \\theta_i, \\Sigma(y_i) \\leq -\\sigma^2_{\\ast 2} \\frac{j}{k} \\lambda_{\\text{max}}(P),\n$$\n$$\n\\frac{k \\lambda_{\\text{max}}(P)}{4n\\kappa} \\leq -\\frac{4n\\kappa \\lambda_{\\text{max}}(P^*)}{\\lambda_{\\text{max}}(P)}.$$\nThis gives\n\n$$\n\\begin{align*}\n&\\gamma W, \\Sigma \\leq -\\frac{4n\\kappa \\lambda_{\\text{max}}(P^*) + d}{\\lambda_{\\text{max}}(P)} \\frac{d}{2} \\log \\kappa \\cdot \\lambda_{\\text{max}}(P) + 3B^2, \\\\\n&\\leq -\\frac{\\lambda_{\\text{max}}(P)k}{4n\\kappa \\lambda_{\\text{max}}(P^*) + d} \\frac{\\kappa \\cdot \\lambda_{\\text{max}}(P)}{\\lambda_{\\text{max}}(P^*) + 3B^2}.\n\\end{align*}\n$$\nCompleting the squares, we get\n\n$$\n\\gamma W, \\Sigma \\leq -\\frac{\\lambda_{\\text{max}}(P)k}{4n\\kappa \\lambda_{\\text{max}}(P^*) - \\kappa d k + \\kappa^2 d^2 n k + 3B^2}.\n$$\nFor the above inequality satisfies \\(\\lambda_{\\text{max}}(P^*) \\geq O(k^2)\\),\n\n$$\n\\gamma W, \\Sigma \\leq 0.\n$$\n28", "md": "Proof of Lemma C.4. For each \\(W \\in \\mathbb{R}^{d \\times k}\\), let \\(\\theta_i := W x_i\\).\n\nFrom Eqn (24) in Lemma C.2, for each \\(i \\in [n]\\), we have,\n\n$$\n\\begin{align*}\n&\\gamma \\theta_i, \\Sigma \\leq g \\theta_i, \\Sigma + \\frac{d}{2} \\log \\lambda_{\\text{max}}(P) \\lambda_{\\text{min}}(P^*) + 3B^2, \\\\\n&\\leq g \\theta_i, \\Sigma + \\frac{d}{2} \\log \\kappa \\lambda_{\\text{max}}(P) \\lambda_{\\text{max}}(P^*) + 3B^2.\n\\end{align*}\n$$\nNow consider\n\n$$\n\\begin{align*}\n&g \\theta_i, \\Sigma(y) = -\\frac{1}{2}(y_i, S_c - \\theta_i, S_c)^T (\\Sigma_{S_c})^{-1}(y_i, S_c - \\theta_i, S_c), \\\\\n&\\leq -\\frac{1}{2} \\lambda_{\\text{min}}(\\Sigma_c) \\|y - \\theta\\|^2, \\\\\n&\\leq -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{min}}(\\Sigma^{-1}) = -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{min}}(P), \\\\\n&\\leq -\\frac{1}{2} \\|y - \\theta\\|^2 \\lambda_{\\text{max}}(P \\kappa),\n\\end{align*}\n$$\nwhere the second inequality comes from the eigenvalue interlacing Theorem, and the last line follows from the condition number assumption on \\(\\Sigma, P\\).\n\nBy Assumption C.3, there exist at least \\(\\epsilon^2 n\\) samples for a coordinate \\(j\\) such that \\((y_i)_j > 0\\). Averaging \\(g \\theta_i, \\Sigma\\), by Lemma A.1, we get that with high probability,\n\n$$\n\\frac{1}{j} \\sum_{i} \\|y_i - \\theta_i\\|^2 \\geq \\sigma^2_{\\ast 2},\n$$\n$$\n\\frac{1}{n} \\sum_{i} g \\theta_i, \\Sigma(y_i) \\leq -\\sigma^2_{\\ast 2} \\frac{j}{k} \\lambda_{\\text{max}}(P),\n$$\n$$\n\\frac{k \\lambda_{\\text{max}}(P)}{4n\\kappa} \\leq -\\frac{4n\\kappa \\lambda_{\\text{max}}(P^*)}{\\lambda_{\\text{max}}(P)}.$$\nThis gives\n\n$$\n\\begin{align*}\n&\\gamma W, \\Sigma \\leq -\\frac{4n\\kappa \\lambda_{\\text{max}}(P^*) + d}{\\lambda_{\\text{max}}(P)} \\frac{d}{2} \\log \\kappa \\cdot \\lambda_{\\text{max}}(P) + 3B^2, \\\\\n&\\leq -\\frac{\\lambda_{\\text{max}}(P)k}{4n\\kappa \\lambda_{\\text{max}}(P^*) + d} \\frac{\\kappa \\cdot \\lambda_{\\text{max}}(P)}{\\lambda_{\\text{max}}(P^*) + 3B^2}.\n\\end{align*}\n$$\nCompleting the squares, we get\n\n$$\n\\gamma W, \\Sigma \\leq -\\frac{\\lambda_{\\text{max}}(P)k}{4n\\kappa \\lambda_{\\text{max}}(P^*) - \\kappa d k + \\kappa^2 d^2 n k + 3B^2}.\n$$\nFor the above inequality satisfies \\(\\lambda_{\\text{max}}(P^*) \\geq O(k^2)\\),\n\n$$\n\\gamma W, \\Sigma \\leq 0.\n$$\n28"}]}, {"page": 29, "text": "Lemma C.11. Assume P \u2217            := \u03a3\u2217\u22121 satisfies Assumption 4.4 with condition number \u03ba.\nFor all y      =    \u03d5(\u03b8\u2217   + \u03b7) such that S denotes the zero-coordinates of y, and \u03b7 such that\n    \u221712          \u221712\n\u2225PS \u03b7S\u2225, \u2225PSc \u03b7Sc\u2225          \u2264   B, consider precision matrices P whose max eigenvalue \u03bbmax(P) sat-\nisfies\n                                                    \u03bbmax(P)\n                                                    \u03bbmin(P \u2217) \u2264     C.\nLet A \u2265    4 max{ d  2 log C, 3B2}. Then for V := (P \u22121)Sc and RP defined as\n                                              RP := 2B     \u221a  C +       3                                             (29)\n                                                                        2A,\nwe have\n                                    \u2225\u03b8Sc \u2212    \u03b8\u2217                  \u21d2    \u03b3\u03b8,P \u2264    \u2212A.\n                                               Sc\u2225V \u2265     RP =\nProof of Lemma C.11. Consider Eqn (24) in Lemma C.2. We have\n                                     \u03b3\u03b8,P \u2264   g\u03b8,P + d  2 log \u03bbmax(P)\n                                                               \u03bbmin(P \u2217) + 3B2,\n                                           \u2264  g\u03b8,P + d  2 log C + 3B2,\nwhere the last inequality follows from \u03bbmax(P ) \u03bbmin(P \u2217) \u2264    C in the statement of the Lemma.\nBy the definition of g\u03b8,P , we have\n                      g\u03b8,P := \u22121    2(ySc \u2212    \u03b8Sc)T (PSc \u2212     PScSP \u22121 S PSSc)(ySc \u2212        \u03b8Sc).\nWe can rewrite the matrix (PSc \u2212          PScSP \u22121 S PSSc) as\nBy setting                         (PSc \u2212    PScSP \u22121 S PSSc) =       (P \u22121)Sc \u22121.\nwe can rewrite g\u03b8,P as                              V := (P \u22121)Sc,\n                     g\u03b8,P := \u22121   2\u2225ySc \u2212     \u03b8Sc\u22252 V = \u22121   2(ySc \u2212    \u03b8Sc)T V \u22121(ySc \u2212       \u03b8Sc).\nNow, as ySc = \u03b7Sc + \u03b8\u2217       Sc, we have\n                       g\u03b8,P = \u22121   2\u2225\u03b7Sc + \u03b8\u2217    Sc \u2212  \u03b8Sc\u22252 V ,\n                             = \u22121       Sc \u2212   \u03b8Sc\u22252 V + \u2225\u03b7Sc\u2225V \u2225\u03b8\u2217     Sc \u2212  \u03b8S\u2225   \u2212  1         V .\n                                   2\u2225\u03b8\u2217                                                2\u2225\u03b7Sc\u22252\nIgnoring the \u2225\u03b7Sc\u22252     V term, we get\n                              g\u03b8,P \u2264   \u22121 2\u2225\u03b8\u2217 Sc \u2212   \u03b8Sc\u22252 V + \u2225\u03b7Sc\u2225V \u2225\u03b8\u2217    Sc \u2212   \u03b8S\u2225V .\nBy the Cauchy-Schwartz inequality and the eigenvalue interlacing theorem, we have\n                 1                                                                                   1\n \u2225\u03b7Sc\u2225V \u2264      \u03bb 2                                        =     \u2225\u03b7Sc\u22252       \u2264     \u2225\u03b7Sc\u22252       = \u03bb  2\n                 max(V \u22121) \u00b7 \u2225\u03b7Sc\u22252 = \u2225\u03b7Sc\u22252    1              1                  1                  max(P) \u00b7 \u2225\u03b7Sc\u22252\n                                                2              2                  2\n                                             \u03bb min(V )       \u03bb min(P \u22121         \u03bb min(P \u22121)\n                                                                      Sc )\n                                                            29", "md": "Lemma C.11. Assume \\( P^* := \\Sigma^*{-1} \\) satisfies Assumption 4.4 with condition number \u03ba. For all \\( y = \\phi(\\theta^* + \\eta) \\) such that S denotes the zero-coordinates of y, and \u03b7 such that\n\n$$\n\\|PS \\eta_S\\|, \\|PSc \\eta_{Sc}\\| \\leq B\n$$\nconsider precision matrices P whose max eigenvalue \\( \\lambda_{\\text{max}}(P) \\) satisfies\n\n$$\n\\frac{\\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} \\leq C.\n$$\nLet \\( A \\geq 4 \\max\\{d^2 \\log C, 3B^2\\} \\). Then for \\( V := (P^{-1})_{Sc} \\) and \\( RP \\) defined as\n\n$$\nRP := 2B \\sqrt{C} + \\frac{3}{2A},\n$$\nwe have\n\n$$\n\\|\\theta_{Sc} - \\theta^* \\|_V \\geq RP = -A.\n$$\nProof of Lemma C.11. Consider Eqn (24) in Lemma C.2. We have\n\n$$\n\\gamma_{\\theta,P} \\leq g_{\\theta,P} + \\frac{d^2 \\log \\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} + 3B^2,\n$$\n$$\n\\leq g_{\\theta,P} + \\frac{d^2 \\log C + 3B^2}{\\lambda_{\\text{max}}(P) / \\lambda_{\\text{min}}(P^*) \\leq C} \\text{ in the statement of the Lemma.}\n$$\nBy the definition of \\( g_{\\theta,P} \\), we have\n\n$$\ng_{\\theta,P} := -\\frac{1}{2}(y_{Sc} - \\theta_{Sc})^T(P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc})(y_{Sc} - \\theta_{Sc}).\n$$\nWe can rewrite the matrix \\( (P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc}) \\) as\n\nBy setting \\( (P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc}) = (P^{-1})_{Sc}^{-1} \\), we can rewrite \\( g_{\\theta,P} \\) as\n\n$$\nV := (P^{-1})_{Sc},\n$$\n$$\ng_{\\theta,P} := -\\frac{1}{2}\\|y_{Sc} - \\theta_{Sc}\\|^2 V = -\\frac{1}{2}(y_{Sc} - \\theta_{Sc})^T V^{-1}(y_{Sc} - \\theta_{Sc}).\n$$\nNow, as \\( y_{Sc} = \\eta_{Sc} + \\theta^*_{Sc} \\), we have\n\n$$\ng_{\\theta,P} = -\\frac{1}{2}\\|\\eta_{Sc} + \\theta^*_{Sc} - \\theta_{Sc}\\|^2 V,\n$$\n$$\n= -\\frac{1}{2}\\|\\theta^*_{Sc} - \\theta_{Sc}\\|^2 V + \\|\\eta_{Sc}\\|_V \\|\\theta^*_{Sc} - \\theta_{Sc}\\| - 1_V.\n$$\nIgnoring the \\( \\|\\eta_{Sc}\\|^2_V \\) term, we get\n\n$$\ng_{\\theta,P} \\leq -\\frac{1}{2}\\|\\theta^*_{Sc} - \\theta_{Sc}\\|^2 V + \\|\\eta_{Sc}\\|_V \\|\\theta^*_{Sc} - \\theta_{Sc}\\|_V.\n$$\nBy the Cauchy-Schwartz inequality and the eigenvalue interlacing theorem, we have\n\n$$\n\\frac{1}{\\| \\eta_{Sc} \\|_V} \\leq \\frac{\\lambda_2}{\\max(V^{-1}) \\cdot \\| \\eta_{Sc} \\|_2} = \\frac{\\| \\eta_{Sc} \\|_2}{\\lambda_1} \\leq \\frac{\\| \\eta_{Sc} \\|_2}{\\lambda_2 \\cdot \\max(P) \\cdot \\| \\eta_{Sc} \\|_2}\n$$\n$$\n= \\frac{\\lambda_{\\text{min}}(V)}{\\lambda_{\\text{min}}(P^{-1}_{Sc})} \\leq \\lambda_{\\text{min}}(P^{-1}_{Sc}).\n$$`", "images": [], "items": [{"type": "text", "value": "Lemma C.11. Assume \\( P^* := \\Sigma^*{-1} \\) satisfies Assumption 4.4 with condition number \u03ba. For all \\( y = \\phi(\\theta^* + \\eta) \\) such that S denotes the zero-coordinates of y, and \u03b7 such that\n\n$$\n\\|PS \\eta_S\\|, \\|PSc \\eta_{Sc}\\| \\leq B\n$$\nconsider precision matrices P whose max eigenvalue \\( \\lambda_{\\text{max}}(P) \\) satisfies\n\n$$\n\\frac{\\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} \\leq C.\n$$\nLet \\( A \\geq 4 \\max\\{d^2 \\log C, 3B^2\\} \\). Then for \\( V := (P^{-1})_{Sc} \\) and \\( RP \\) defined as\n\n$$\nRP := 2B \\sqrt{C} + \\frac{3}{2A},\n$$\nwe have\n\n$$\n\\|\\theta_{Sc} - \\theta^* \\|_V \\geq RP = -A.\n$$\nProof of Lemma C.11. Consider Eqn (24) in Lemma C.2. We have\n\n$$\n\\gamma_{\\theta,P} \\leq g_{\\theta,P} + \\frac{d^2 \\log \\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} + 3B^2,\n$$\n$$\n\\leq g_{\\theta,P} + \\frac{d^2 \\log C + 3B^2}{\\lambda_{\\text{max}}(P) / \\lambda_{\\text{min}}(P^*) \\leq C} \\text{ in the statement of the Lemma.}\n$$\nBy the definition of \\( g_{\\theta,P} \\), we have\n\n$$\ng_{\\theta,P} := -\\frac{1}{2}(y_{Sc} - \\theta_{Sc})^T(P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc})(y_{Sc} - \\theta_{Sc}).\n$$\nWe can rewrite the matrix \\( (P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc}) \\) as\n\nBy setting \\( (P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc}) = (P^{-1})_{Sc}^{-1} \\), we can rewrite \\( g_{\\theta,P} \\) as\n\n$$\nV := (P^{-1})_{Sc},\n$$\n$$\ng_{\\theta,P} := -\\frac{1}{2}\\|y_{Sc} - \\theta_{Sc}\\|^2 V = -\\frac{1}{2}(y_{Sc} - \\theta_{Sc})^T V^{-1}(y_{Sc} - \\theta_{Sc}).\n$$\nNow, as \\( y_{Sc} = \\eta_{Sc} + \\theta^*_{Sc} \\), we have\n\n$$\ng_{\\theta,P} = -\\frac{1}{2}\\|\\eta_{Sc} + \\theta^*_{Sc} - \\theta_{Sc}\\|^2 V,\n$$\n$$\n= -\\frac{1}{2}\\|\\theta^*_{Sc} - \\theta_{Sc}\\|^2 V + \\|\\eta_{Sc}\\|_V \\|\\theta^*_{Sc} - \\theta_{Sc}\\| - 1_V.\n$$\nIgnoring the \\( \\|\\eta_{Sc}\\|^2_V \\) term, we get\n\n$$\ng_{\\theta,P} \\leq -\\frac{1}{2}\\|\\theta^*_{Sc} - \\theta_{Sc}\\|^2 V + \\|\\eta_{Sc}\\|_V \\|\\theta^*_{Sc} - \\theta_{Sc}\\|_V.\n$$\nBy the Cauchy-Schwartz inequality and the eigenvalue interlacing theorem, we have\n\n$$\n\\frac{1}{\\| \\eta_{Sc} \\|_V} \\leq \\frac{\\lambda_2}{\\max(V^{-1}) \\cdot \\| \\eta_{Sc} \\|_2} = \\frac{\\| \\eta_{Sc} \\|_2}{\\lambda_1} \\leq \\frac{\\| \\eta_{Sc} \\|_2}{\\lambda_2 \\cdot \\max(P) \\cdot \\| \\eta_{Sc} \\|_2}\n$$\n$$\n= \\frac{\\lambda_{\\text{min}}(V)}{\\lambda_{\\text{min}}(P^{-1}_{Sc})} \\leq \\lambda_{\\text{min}}(P^{-1}_{Sc}).\n$$`", "md": "Lemma C.11. Assume \\( P^* := \\Sigma^*{-1} \\) satisfies Assumption 4.4 with condition number \u03ba. For all \\( y = \\phi(\\theta^* + \\eta) \\) such that S denotes the zero-coordinates of y, and \u03b7 such that\n\n$$\n\\|PS \\eta_S\\|, \\|PSc \\eta_{Sc}\\| \\leq B\n$$\nconsider precision matrices P whose max eigenvalue \\( \\lambda_{\\text{max}}(P) \\) satisfies\n\n$$\n\\frac{\\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} \\leq C.\n$$\nLet \\( A \\geq 4 \\max\\{d^2 \\log C, 3B^2\\} \\). Then for \\( V := (P^{-1})_{Sc} \\) and \\( RP \\) defined as\n\n$$\nRP := 2B \\sqrt{C} + \\frac{3}{2A},\n$$\nwe have\n\n$$\n\\|\\theta_{Sc} - \\theta^* \\|_V \\geq RP = -A.\n$$\nProof of Lemma C.11. Consider Eqn (24) in Lemma C.2. We have\n\n$$\n\\gamma_{\\theta,P} \\leq g_{\\theta,P} + \\frac{d^2 \\log \\lambda_{\\text{max}}(P)}{\\lambda_{\\text{min}}(P^*)} + 3B^2,\n$$\n$$\n\\leq g_{\\theta,P} + \\frac{d^2 \\log C + 3B^2}{\\lambda_{\\text{max}}(P) / \\lambda_{\\text{min}}(P^*) \\leq C} \\text{ in the statement of the Lemma.}\n$$\nBy the definition of \\( g_{\\theta,P} \\), we have\n\n$$\ng_{\\theta,P} := -\\frac{1}{2}(y_{Sc} - \\theta_{Sc})^T(P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc})(y_{Sc} - \\theta_{Sc}).\n$$\nWe can rewrite the matrix \\( (P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc}) \\) as\n\nBy setting \\( (P_{Sc} - P_{Sc}SP^{-1}_S P_{SSc}) = (P^{-1})_{Sc}^{-1} \\), we can rewrite \\( g_{\\theta,P} \\) as\n\n$$\nV := (P^{-1})_{Sc},\n$$\n$$\ng_{\\theta,P} := -\\frac{1}{2}\\|y_{Sc} - \\theta_{Sc}\\|^2 V = -\\frac{1}{2}(y_{Sc} - \\theta_{Sc})^T V^{-1}(y_{Sc} - \\theta_{Sc}).\n$$\nNow, as \\( y_{Sc} = \\eta_{Sc} + \\theta^*_{Sc} \\), we have\n\n$$\ng_{\\theta,P} = -\\frac{1}{2}\\|\\eta_{Sc} + \\theta^*_{Sc} - \\theta_{Sc}\\|^2 V,\n$$\n$$\n= -\\frac{1}{2}\\|\\theta^*_{Sc} - \\theta_{Sc}\\|^2 V + \\|\\eta_{Sc}\\|_V \\|\\theta^*_{Sc} - \\theta_{Sc}\\| - 1_V.\n$$\nIgnoring the \\( \\|\\eta_{Sc}\\|^2_V \\) term, we get\n\n$$\ng_{\\theta,P} \\leq -\\frac{1}{2}\\|\\theta^*_{Sc} - \\theta_{Sc}\\|^2 V + \\|\\eta_{Sc}\\|_V \\|\\theta^*_{Sc} - \\theta_{Sc}\\|_V.\n$$\nBy the Cauchy-Schwartz inequality and the eigenvalue interlacing theorem, we have\n\n$$\n\\frac{1}{\\| \\eta_{Sc} \\|_V} \\leq \\frac{\\lambda_2}{\\max(V^{-1}) \\cdot \\| \\eta_{Sc} \\|_2} = \\frac{\\| \\eta_{Sc} \\|_2}{\\lambda_1} \\leq \\frac{\\| \\eta_{Sc} \\|_2}{\\lambda_2 \\cdot \\max(P) \\cdot \\| \\eta_{Sc} \\|_2}\n$$\n$$\n= \\frac{\\lambda_{\\text{min}}(V)}{\\lambda_{\\text{min}}(P^{-1}_{Sc})} \\leq \\lambda_{\\text{min}}(P^{-1}_{Sc}).\n$$`"}]}, {"page": 30, "text": " By the statement of the Lemma, we have \u2225P                                 \u221712                       \u21d2     \u2225\u03b7Sc\u22252 \u2264            1   B         . Substituting\n                                                                          Sc \u03b7Sc\u2225       \u2264    B =                               2\n in the above inequality, we get                                       1                                                     \u03bb min(P \u2217 Sc)\n                                                                       2\n                                                 \u2225\u03b7Sc\u2225V \u2264           \u03bb max(P  1   ) \u00b7 \u2225\u03b7Sc\u22252        \u2264    \u221a  CB.\n                                                                             2\n                                                                          \u03bb min(P \u2217   Sc)\n Substituting in the function g\u03b8,P , we get\n Hence, for \u03b8 satisfying                  g\u03b8,P \u2264      \u22121  2\u2225\u03b8\u2217   Sc \u2212    \u03b8Sc\u22252   V + B      \u221a   C\u2225\u03b8\u2217   Sc \u2212     \u03b8S\u2225V .\n we get                                         \u2225\u03b8Sc \u2212       \u03b8\u2217Sc\u2225V \u2265        RP := 2B          \u221a  C + 2     \u221a   A,\n                                                                        \u03b3\u03b8,P \u2264       \u2212A.\n In order to cover our precision matrices, we will consider a subset of matrices whose entries are\n quantized by an interval size \u03b2:\n Definition C.12 (Quantized Precision Matrices). For \u03ba > 0, define \u2126                                               \u2282    Rd\u00d7d as the set of positive\n definite matrices with condition number \u03ba.\nFor \u03c1 > 0, define the set \u2126\u03c1 \u2282                     \u2126   as\n                                                   \u2126\u03c1 :=         P \u2208     \u2126   : \u03bbmax(P        ) \u2208    \u03c1 2 , \u03c1\nFor a quantization size \u03b2 > 0, define                         \u2126\u03c1,\u03b2 \u2282       \u2126\u03c1 as:\n         \u2126\u03c1,\u03b2 := {P \u2208             \u2126\u03c1 : Pij \u2208         {\u2212\u03c1, \u2212\u03c1(1 \u2212            \u03b2), \u2212\u03c1(1 \u2212          2\u03b2), \u00b7 \u00b7 \u00b7 , \u03c1(1 \u2212        2\u03b2), \u03c1(1 \u2212         \u03b2), \u03c1}.}\n Lemma C.5 (\u03a3 cover). For B > 1, and 0 < L < U, let A > max                                                        log 1  \u03b5 , B2U\u03ba, d      2 log    \u03baUL     , 1  .\n Let P \u2217      := \u03a3\u2217\u22121 be the precision matrix of \u03b7. Let \u2126                                   \u2282    Rd\u00d7d      denote the set of positive definite\n                                                                                                    +\n matrices P \u2208           Rd\u00d7d      with condition number \u03ba and whose maximum eigenvalue lies in [L\u03bbmin(P \u2217), U \u00b7\n \u03bbmax(P \u2217)].               +\n Then, there exists a partition of \u2126                     of size\n                                                                     poly       A, 1 \u03b5    d2         \u221712               \u221712\n such that for all \u03b8 \u2208            Rd and all y = \u03d5(\u03b8\u2217                + \u03b7) \u2208       Rd with \u2225PS \u03b7S\u2225, \u2225PSc \u03b7Sc\u2225                        \u2264   B, and each cell\n I in the partition, one of the following holds:\n            \u2022 for all P \u2208          I, \u03b3\u03b8,P (y) < \u2212A, or\n            \u2022 for all P, P \u2032 \u2208          I, we have |\u03b3\u03b8,P (y) \u2212                \u03b3\u03b8,P \u2032(y)| \u2264        \u03f5.\n Proof. In order to construct the net over the precision matrices, we will consider geometrically spaced\n values of \u03c1 \u2208          [L \u00b7 \u03bbmin(P \u2217), U \u00b7 \u03bbmax(P \u2217)], and for each \u03c1, we will construct a net over matrices\n that have max eigenvalue \u2264                    \u03c1.\n Now consider \u03c1 > 0 that lies in the following discrete set:\n                                                          \u03bbmin(P \u2217)2j, j \u2208            \u2308log2(\u03ba UL )\u2309\n This set is a geometric partition over the possible max eigenvalues that the MLE can return.\n For the current \u03c1, let \u2126\u03c1 follow Definition C.12. Now consider P \u2208                                             \u2126\u03c1.\n                                                                                30", "md": "# Math Equations and Lemma\n\nBy the statement of the Lemma, we have $$\\|P_{12}^* \\Rightarrow \\|\\eta S c\\|^2 \\leq \\frac{1}{B}$$. Substituting\n\n$Sc \\eta Sc\\|^2 \\leq B = \\frac{1}{2} \\lambda_{\\min}(P^* Sc)$\n\nin the above inequality, we get\n\n$$\\|\\eta Sc\\|_V \\leq \\lambda_{\\max}(P_1) \\cdot \\|\\eta Sc\\|^2 \\leq \\sqrt{CB}.$$\n\nSubstituting in the function $$g_{\\theta,P}$$, we get\n\nHence, for $$\\theta$$ satisfying $$g_{\\theta,P} \\leq -\\frac{1}{2}\\|\\theta^* Sc - \\theta Sc\\|_V + B \\sqrt{C}\\|\\theta^* Sc - \\theta S\\|_V$$,\n\nwe get $$\\|\\theta Sc - \\theta^* Sc\\|_V \\geq R_P := 2B \\sqrt{C} + 2\\sqrt{A},$$\n\n$$\\gamma_{\\theta,P} \\leq -A$$.\n\nIn order to cover our precision matrices, we will consider a subset of matrices whose entries are quantized by an interval size $$\\beta$$:\n\nDefinition C.12 (Quantized Precision Matrices). For $$\\kappa > 0$$, define $$\\Omega \\subset \\mathbb{R}^{d \\times d}$$ as the set of positive definite matrices with condition number $$\\kappa$$.\n\nFor $$\\rho > 0$$, define the set $$\\Omega_{\\rho} \\subset \\Omega$$ as\n\n$$\\Omega_{\\rho} := \\{P \\in \\Omega : \\lambda_{\\max}(P) \\in [\\rho/2, \\rho]\\}$$\n\nFor a quantization size $$\\beta > 0$$, define $$\\Omega_{\\rho,\\beta} \\subset \\Omega_{\\rho}$$ as:\n\n$$\\Omega_{\\rho,\\beta} := \\{P \\in \\Omega_{\\rho} : P_{ij} \\in \\{-\\rho, -\\rho(1-\\beta), -\\rho(1-2\\beta), \\ldots, \\rho(1-2\\beta), \\rho(1-\\beta), \\rho\\}\\}.$$\n\nLemma C.5 (\u03a3 cover). For $$B > 1$$, and $$0 < L < U$$, let $$A > \\max\\{\\log(1/\\epsilon), B^2U\\kappa, d^2\\log(\\kappa UL), 1\\}$$. Let $$P^* := \\Sigma^*$$ be the precision matrix of $$\\eta$$. Let $$\\Omega \\subset \\mathbb{R}^{d \\times d}$$ denote the set of positive definite matrices $$P \\in \\mathbb{R}^{d \\times d}$$ with condition number $$\\kappa$$ and whose maximum eigenvalue lies in $$[L\\lambda_{\\min}(P^*), U\\lambda_{\\max}(P^*)]$$. Then, there exists a partition of $$\\Omega$$ of size\n\n$$\\text{poly}(A, 1/\\epsilon) d^2$$\n\nsuch that for all $$\\theta \\in \\mathbb{R}^d$$ and all $$y = \\phi(\\theta^* + \\eta) \\in \\mathbb{R}^d$$ with $$\\|P_S \\eta S\\|, \\|P_S c \\eta S c\\| \\leq B$$, and each cell $$I$$ in the partition, one of the following holds:\n\n- for all $P \\in I$, $\\gamma_{\\theta,P}(y) < -A$, or\n- for all $P, P' \\in I$, we have $|\\gamma_{\\theta,P}(y) - \\gamma_{\\theta,P'}(y)| \\leq \\epsilon$.\n\nProof. In order to construct the net over the precision matrices, we will consider geometrically spaced values of $$\\rho \\in [L\\lambda_{\\min}(P^*), U\\lambda_{\\max}(P^*)]$$, and for each $$\\rho$$, we will construct a net over matrices that have max eigenvalue $$\\leq \\rho$$.\n\nNow consider $$\\rho > 0$$ that lies in the following discrete set:\n\n$$\\lambda_{\\min}(P^*)2^j, j \\in \\lceil\\log_2(\\kappa UL)\\rceil$$\n\nThis set is a geometric partition over the possible max eigenvalues that the MLE can return.\n\nFor the current $$\\rho$$, let $$\\Omega_{\\rho}$$ follow Definition C.12. Now consider $$P \\in \\Omega_{\\rho}$$.\n\n30", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Lemma", "md": "# Math Equations and Lemma"}, {"type": "text", "value": "By the statement of the Lemma, we have $$\\|P_{12}^* \\Rightarrow \\|\\eta S c\\|^2 \\leq \\frac{1}{B}$$. Substituting\n\n$Sc \\eta Sc\\|^2 \\leq B = \\frac{1}{2} \\lambda_{\\min}(P^* Sc)$\n\nin the above inequality, we get\n\n$$\\|\\eta Sc\\|_V \\leq \\lambda_{\\max}(P_1) \\cdot \\|\\eta Sc\\|^2 \\leq \\sqrt{CB}.$$\n\nSubstituting in the function $$g_{\\theta,P}$$, we get\n\nHence, for $$\\theta$$ satisfying $$g_{\\theta,P} \\leq -\\frac{1}{2}\\|\\theta^* Sc - \\theta Sc\\|_V + B \\sqrt{C}\\|\\theta^* Sc - \\theta S\\|_V$$,\n\nwe get $$\\|\\theta Sc - \\theta^* Sc\\|_V \\geq R_P := 2B \\sqrt{C} + 2\\sqrt{A},$$\n\n$$\\gamma_{\\theta,P} \\leq -A$$.\n\nIn order to cover our precision matrices, we will consider a subset of matrices whose entries are quantized by an interval size $$\\beta$$:\n\nDefinition C.12 (Quantized Precision Matrices). For $$\\kappa > 0$$, define $$\\Omega \\subset \\mathbb{R}^{d \\times d}$$ as the set of positive definite matrices with condition number $$\\kappa$$.\n\nFor $$\\rho > 0$$, define the set $$\\Omega_{\\rho} \\subset \\Omega$$ as\n\n$$\\Omega_{\\rho} := \\{P \\in \\Omega : \\lambda_{\\max}(P) \\in [\\rho/2, \\rho]\\}$$\n\nFor a quantization size $$\\beta > 0$$, define $$\\Omega_{\\rho,\\beta} \\subset \\Omega_{\\rho}$$ as:\n\n$$\\Omega_{\\rho,\\beta} := \\{P \\in \\Omega_{\\rho} : P_{ij} \\in \\{-\\rho, -\\rho(1-\\beta), -\\rho(1-2\\beta), \\ldots, \\rho(1-2\\beta), \\rho(1-\\beta), \\rho\\}\\}.$$\n\nLemma C.5 (\u03a3 cover). For $$B > 1$$, and $$0 < L < U$$, let $$A > \\max\\{\\log(1/\\epsilon), B^2U\\kappa, d^2\\log(\\kappa UL), 1\\}$$. Let $$P^* := \\Sigma^*$$ be the precision matrix of $$\\eta$$. Let $$\\Omega \\subset \\mathbb{R}^{d \\times d}$$ denote the set of positive definite matrices $$P \\in \\mathbb{R}^{d \\times d}$$ with condition number $$\\kappa$$ and whose maximum eigenvalue lies in $$[L\\lambda_{\\min}(P^*), U\\lambda_{\\max}(P^*)]$$. Then, there exists a partition of $$\\Omega$$ of size\n\n$$\\text{poly}(A, 1/\\epsilon) d^2$$\n\nsuch that for all $$\\theta \\in \\mathbb{R}^d$$ and all $$y = \\phi(\\theta^* + \\eta) \\in \\mathbb{R}^d$$ with $$\\|P_S \\eta S\\|, \\|P_S c \\eta S c\\| \\leq B$$, and each cell $$I$$ in the partition, one of the following holds:\n\n- for all $P \\in I$, $\\gamma_{\\theta,P}(y) < -A$, or\n- for all $P, P' \\in I$, we have $|\\gamma_{\\theta,P}(y) - \\gamma_{\\theta,P'}(y)| \\leq \\epsilon$.\n\nProof. In order to construct the net over the precision matrices, we will consider geometrically spaced values of $$\\rho \\in [L\\lambda_{\\min}(P^*), U\\lambda_{\\max}(P^*)]$$, and for each $$\\rho$$, we will construct a net over matrices that have max eigenvalue $$\\leq \\rho$$.\n\nNow consider $$\\rho > 0$$ that lies in the following discrete set:\n\n$$\\lambda_{\\min}(P^*)2^j, j \\in \\lceil\\log_2(\\kappa UL)\\rceil$$\n\nThis set is a geometric partition over the possible max eigenvalues that the MLE can return.\n\nFor the current $$\\rho$$, let $$\\Omega_{\\rho}$$ follow Definition C.12. Now consider $$P \\in \\Omega_{\\rho}$$.\n\n30", "md": "By the statement of the Lemma, we have $$\\|P_{12}^* \\Rightarrow \\|\\eta S c\\|^2 \\leq \\frac{1}{B}$$. Substituting\n\n$Sc \\eta Sc\\|^2 \\leq B = \\frac{1}{2} \\lambda_{\\min}(P^* Sc)$\n\nin the above inequality, we get\n\n$$\\|\\eta Sc\\|_V \\leq \\lambda_{\\max}(P_1) \\cdot \\|\\eta Sc\\|^2 \\leq \\sqrt{CB}.$$\n\nSubstituting in the function $$g_{\\theta,P}$$, we get\n\nHence, for $$\\theta$$ satisfying $$g_{\\theta,P} \\leq -\\frac{1}{2}\\|\\theta^* Sc - \\theta Sc\\|_V + B \\sqrt{C}\\|\\theta^* Sc - \\theta S\\|_V$$,\n\nwe get $$\\|\\theta Sc - \\theta^* Sc\\|_V \\geq R_P := 2B \\sqrt{C} + 2\\sqrt{A},$$\n\n$$\\gamma_{\\theta,P} \\leq -A$$.\n\nIn order to cover our precision matrices, we will consider a subset of matrices whose entries are quantized by an interval size $$\\beta$$:\n\nDefinition C.12 (Quantized Precision Matrices). For $$\\kappa > 0$$, define $$\\Omega \\subset \\mathbb{R}^{d \\times d}$$ as the set of positive definite matrices with condition number $$\\kappa$$.\n\nFor $$\\rho > 0$$, define the set $$\\Omega_{\\rho} \\subset \\Omega$$ as\n\n$$\\Omega_{\\rho} := \\{P \\in \\Omega : \\lambda_{\\max}(P) \\in [\\rho/2, \\rho]\\}$$\n\nFor a quantization size $$\\beta > 0$$, define $$\\Omega_{\\rho,\\beta} \\subset \\Omega_{\\rho}$$ as:\n\n$$\\Omega_{\\rho,\\beta} := \\{P \\in \\Omega_{\\rho} : P_{ij} \\in \\{-\\rho, -\\rho(1-\\beta), -\\rho(1-2\\beta), \\ldots, \\rho(1-2\\beta), \\rho(1-\\beta), \\rho\\}\\}.$$\n\nLemma C.5 (\u03a3 cover). For $$B > 1$$, and $$0 < L < U$$, let $$A > \\max\\{\\log(1/\\epsilon), B^2U\\kappa, d^2\\log(\\kappa UL), 1\\}$$. Let $$P^* := \\Sigma^*$$ be the precision matrix of $$\\eta$$. Let $$\\Omega \\subset \\mathbb{R}^{d \\times d}$$ denote the set of positive definite matrices $$P \\in \\mathbb{R}^{d \\times d}$$ with condition number $$\\kappa$$ and whose maximum eigenvalue lies in $$[L\\lambda_{\\min}(P^*), U\\lambda_{\\max}(P^*)]$$. Then, there exists a partition of $$\\Omega$$ of size\n\n$$\\text{poly}(A, 1/\\epsilon) d^2$$\n\nsuch that for all $$\\theta \\in \\mathbb{R}^d$$ and all $$y = \\phi(\\theta^* + \\eta) \\in \\mathbb{R}^d$$ with $$\\|P_S \\eta S\\|, \\|P_S c \\eta S c\\| \\leq B$$, and each cell $$I$$ in the partition, one of the following holds:\n\n- for all $P \\in I$, $\\gamma_{\\theta,P}(y) < -A$, or\n- for all $P, P' \\in I$, we have $|\\gamma_{\\theta,P}(y) - \\gamma_{\\theta,P'}(y)| \\leq \\epsilon$.\n\nProof. In order to construct the net over the precision matrices, we will consider geometrically spaced values of $$\\rho \\in [L\\lambda_{\\min}(P^*), U\\lambda_{\\max}(P^*)]$$, and for each $$\\rho$$, we will construct a net over matrices that have max eigenvalue $$\\leq \\rho$$.\n\nNow consider $$\\rho > 0$$ that lies in the following discrete set:\n\n$$\\lambda_{\\min}(P^*)2^j, j \\in \\lceil\\log_2(\\kappa UL)\\rceil$$\n\nThis set is a geometric partition over the possible max eigenvalues that the MLE can return.\n\nFor the current $$\\rho$$, let $$\\Omega_{\\rho}$$ follow Definition C.12. Now consider $$P \\in \\Omega_{\\rho}$$.\n\n30"}]}, {"page": 31, "text": " Constructing the interval for which \u03b3\u03b8,P < \u2212A.         \u221a                         By Lemma C.11, for V = (P \u22121)Sc, and\n                            \u03c1          \u221a\n RP = O         B      \u03bbmin(P \u2217) +        A     = O(       A), we have\n                                           \u2225\u03b8Sc \u2212     \u03b8\u2217Sc\u2225V \u2265       RP =     \u21d2     \u03b3\u03b8,P < \u2212A.\n For any \u03b8, notice that the set of matrices P satisfying \u2225\u03b8Sc \u2212                               \u03b8\u2217Sc\u2225V \u2265       RP is connected (as its\n complement is compact). This forms the set I for which \u03b3\u03b8,P < \u2212A.\n Constructing intervals for which |\u03b3\u03b8,P \u2212                     \u03b3\u03b8,P \u2032| \u2264     \u03b5.    We will now construct a partition over those\n P which satisfy \u2225\u03b8Sc \u2212            \u03b8\u2217Sc\u2225V < RP , and show that the log-likelihood changes by atmost \u03b5 for each\n cell in this partition.\n If P \u2208    \u2126\u03c1, then each of its elements Pij \u2208                 [\u2212\u03c1, \u03c1]. For a parameter \u03b2 > 0 that we will specify later,\n consider the partition         \u2126\u03c1,\u03b2 of \u2126\u03c1, following Definition C.12. Clearly, the size of                           \u2126\u03c1,\u03b2 can be upper\n bounded by\n                                                            \u2126\u03c1,\u03b2            2   d2   .\nWe will now analyze the effect of rounding down P \u2208                  \u2264      \u03b2   \u2126\u03c1 to its nearest element in            \u2126\u03c1,\u03b2.\n By Claim C.13, for \u03b3 = 2\u03ba\u03b2d2, we have\n                                   (1 \u2212    \u03b3)\u2225t \u2212     \u03b8\u22252 \u03a3 \u2264    \u2225t \u2212   \u03b8\u22252 \u03a3\u2032 \u2264    (1 + \u03b3)\u2225t \u2212       \u03b8\u22252 \u03a3,                             (30)\n Consider the log-likelihood at \u03b8, P \u2032:\n                           f\u03b8,P \u2032(y) = 1    2 log|P \u2032| + log         t:tS\u22640,tSc=ySc       exp     \u2212\u2225t \u2212      \u03b8\u22252\u03a3\u2032   .\nWe will use the LHS of Eqn (30) to show that\n                                     f\u03b8,P \u2032(y) \u2212      1\n                                                      2 log|P \u2032| \u2264      f\u03b8,P (y) \u2212      1\n                                                                                        2 log|P     | + \u03b5,\n and deal with the log|P \u2032| term later. The lower bound for the log-likelihood at P \u2032 can be obtained via\n analogous proof using the RHS of Eqn (30).\n By the LHS of Eqn (30), we get\n                      f\u03b8,P \u2032(y) \u2212      1                                             exp     \u2212(1 \u2212      \u03b3)\u2225t \u2212     \u03b8\u22252\u03a3   .\n Rearranging the terms, we get         2 log|P \u2032| \u2264      log    t:tS\u22640,tSc=ySc\n    f\u03b8,P \u2032(y) \u2212      1                                \u2225ySc \u2212      \u03b8Sc\u22252  \u03a3Sc\n                     2 log|P \u2032| \u2264      \u2212   (1 \u22122  \u03b3)                                1                    \u22121\n                                       + log      t\u22640  exp      \u2212(1 \u2212   2  \u03b3)\u2225P    S2(t \u2212    \u03b8)S + PS       2 PSSc(ySc \u2212        \u03b8Sc)\u22252\n The non-integral term corresponds to g\u03b8,P in Eqn (21), while the integral term corresponds to h\u03b8,P in\n Eqn (22).\n Handling the non-integral term.                     As we are only considering \u03b8 such that \u2225ySc \u2212                       \u03b8Sc\u2225\u03a3Sc \u2264        RP ,\n we have that for                            \u03b2 = O             \u03b5         = O            \u03b5         ,\n                                                          R2 P d2\u03ba                 poly(A)\n the non-integral term corresponds to g\u03b8,P + \u03b5, which gives\n                                                                                             1                     \u22121\n f\u03b8,P \u2032(y) \u2212      1                                              exp     \u2212(1 \u2212      \u03b3)\u2225P    S2(t \u2212    \u03b8)S + PS       2 PSSc(ySc \u2212        \u03b8Sc)\u22252\n                  2 log|P \u2032| \u2264g\u03b8,P + \u03b5 + log               t\u22640                   2                                                       (31)\n                                                                       31", "md": "Constructing the interval for which $$\\gamma_{\\theta,P} < -A$$. By Lemma C.11, for $$V = (P^{-1})Sc$$, and $$\\rho \\sqrt{RP} = O(\\lambda_{\\min}(P^*) + A) = O(A)$$, we have $$\\|\\theta Sc - \\theta^*Sc\\|_V \\geq \\sqrt{RP} \\Rightarrow \\gamma_{\\theta,P} < -A$$.\n\nFor any $$\\theta$$, notice that the set of matrices $$P$$ satisfying $$\\|\\theta Sc - \\theta^*Sc\\|_V \\geq RP$$ is connected (as its complement is compact). This forms the set $$I$$ for which $$\\gamma_{\\theta,P} < -A$$.\n\nConstructing intervals for which $$|\\gamma_{\\theta,P} - \\gamma_{\\theta,P'}| \\leq \\varepsilon$$. We will now construct a partition over those $$P$$ which satisfy $$\\|\\theta Sc - \\theta^*Sc\\|_V < RP$$, and show that the log-likelihood changes by at most $$\\varepsilon$$ for each cell in this partition.\n\nIf $$P \\in \\Omega_\\rho$$, then each of its elements $$P_{ij} \\in [-\\rho, \\rho]$$. For a parameter $$\\beta > 0$$ that we will specify later, consider the partition $$\\Omega_{\\rho,\\beta}$$ of $$\\Omega_\\rho$$, following Definition C.12. Clearly, the size of $$\\Omega_{\\rho,\\beta}$$ can be upper bounded by $$|\\Omega_{\\rho,\\beta}| \\leq 2d^2$$.\n\nWe will now analyze the effect of rounding down $$P \\in \\Omega$$ to its nearest element in $$\\Omega_{\\rho,\\beta}$$. By Claim C.13, for $$\\gamma = 2\\kappa\\beta d^2$$, we have $$(1 - \\gamma)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + \\gamma)\\|t - \\theta\\|_2 \\Sigma$$.\n\nConsider the log-likelihood at $$\\theta, P'$$:\n\n$$\nf_{\\theta,P'}(y) = \\frac{1}{2} \\log|P'| + \\log \\{t: tS \\leq 0, tSc = ySc\\} \\exp(-\\|t - \\theta\\|_2 \\Sigma')\n$$\n\nWe will use the LHS of Eqn (30) to show that\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\log|P'| \\leq f_{\\theta,P}(y) - \\frac{1}{2} \\log|P| + \\varepsilon\n$$\nand deal with the $$\\log|P'|$$ term later. The lower bound for the log-likelihood at $$P'$$ can be obtained via analogous proof using the RHS of Eqn (30).\n\nBy the LHS of Eqn (30), we get\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\leq \\exp(-(1 - \\gamma)\\|t - \\theta\\|_2 \\Sigma)\n$$\n\nRearranging the terms, we get\n\n$$\n2 \\log|P'| \\leq \\log \\{t: tS \\leq 0, tSc = ySc\\} f_{\\theta,P'}(y) - \\frac{1}{2} \\leq - (1 - 2\\gamma) + \\log \\{t \\leq 0\\} \\exp(-(1 - 2\\gamma)\\|P S^2(t - \\theta)S + PS^2 PSSc(ySc - \\theta Sc)\\|_2\n$$\n\nThe non-integral term corresponds to $$g_{\\theta,P}$$ in Eqn (21), while the integral term corresponds to $$h_{\\theta,P}$$ in Eqn (22).\n\nHandling the non-integral term. As we are only considering $$\\theta$$ such that $$\\|ySc - \\theta Sc\\|_{\\Sigma Sc} \\leq RP$$, we have that for $$\\beta = O(\\varepsilon) = O(\\varepsilon)$$,\n\n$$\nR^2 P d^2\\kappa \\text{poly}(A)\n$$\nthe non-integral term corresponds to $$g_{\\theta,P} + \\varepsilon$$, which gives\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\leq g_{\\theta,P} + \\varepsilon + \\log \\{t \\leq 0\\}\n$$", "images": [], "items": [{"type": "text", "value": "Constructing the interval for which $$\\gamma_{\\theta,P} < -A$$. By Lemma C.11, for $$V = (P^{-1})Sc$$, and $$\\rho \\sqrt{RP} = O(\\lambda_{\\min}(P^*) + A) = O(A)$$, we have $$\\|\\theta Sc - \\theta^*Sc\\|_V \\geq \\sqrt{RP} \\Rightarrow \\gamma_{\\theta,P} < -A$$.\n\nFor any $$\\theta$$, notice that the set of matrices $$P$$ satisfying $$\\|\\theta Sc - \\theta^*Sc\\|_V \\geq RP$$ is connected (as its complement is compact). This forms the set $$I$$ for which $$\\gamma_{\\theta,P} < -A$$.\n\nConstructing intervals for which $$|\\gamma_{\\theta,P} - \\gamma_{\\theta,P'}| \\leq \\varepsilon$$. We will now construct a partition over those $$P$$ which satisfy $$\\|\\theta Sc - \\theta^*Sc\\|_V < RP$$, and show that the log-likelihood changes by at most $$\\varepsilon$$ for each cell in this partition.\n\nIf $$P \\in \\Omega_\\rho$$, then each of its elements $$P_{ij} \\in [-\\rho, \\rho]$$. For a parameter $$\\beta > 0$$ that we will specify later, consider the partition $$\\Omega_{\\rho,\\beta}$$ of $$\\Omega_\\rho$$, following Definition C.12. Clearly, the size of $$\\Omega_{\\rho,\\beta}$$ can be upper bounded by $$|\\Omega_{\\rho,\\beta}| \\leq 2d^2$$.\n\nWe will now analyze the effect of rounding down $$P \\in \\Omega$$ to its nearest element in $$\\Omega_{\\rho,\\beta}$$. By Claim C.13, for $$\\gamma = 2\\kappa\\beta d^2$$, we have $$(1 - \\gamma)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + \\gamma)\\|t - \\theta\\|_2 \\Sigma$$.\n\nConsider the log-likelihood at $$\\theta, P'$$:\n\n$$\nf_{\\theta,P'}(y) = \\frac{1}{2} \\log|P'| + \\log \\{t: tS \\leq 0, tSc = ySc\\} \\exp(-\\|t - \\theta\\|_2 \\Sigma')\n$$\n\nWe will use the LHS of Eqn (30) to show that\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\log|P'| \\leq f_{\\theta,P}(y) - \\frac{1}{2} \\log|P| + \\varepsilon\n$$\nand deal with the $$\\log|P'|$$ term later. The lower bound for the log-likelihood at $$P'$$ can be obtained via analogous proof using the RHS of Eqn (30).\n\nBy the LHS of Eqn (30), we get\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\leq \\exp(-(1 - \\gamma)\\|t - \\theta\\|_2 \\Sigma)\n$$\n\nRearranging the terms, we get\n\n$$\n2 \\log|P'| \\leq \\log \\{t: tS \\leq 0, tSc = ySc\\} f_{\\theta,P'}(y) - \\frac{1}{2} \\leq - (1 - 2\\gamma) + \\log \\{t \\leq 0\\} \\exp(-(1 - 2\\gamma)\\|P S^2(t - \\theta)S + PS^2 PSSc(ySc - \\theta Sc)\\|_2\n$$\n\nThe non-integral term corresponds to $$g_{\\theta,P}$$ in Eqn (21), while the integral term corresponds to $$h_{\\theta,P}$$ in Eqn (22).\n\nHandling the non-integral term. As we are only considering $$\\theta$$ such that $$\\|ySc - \\theta Sc\\|_{\\Sigma Sc} \\leq RP$$, we have that for $$\\beta = O(\\varepsilon) = O(\\varepsilon)$$,\n\n$$\nR^2 P d^2\\kappa \\text{poly}(A)\n$$\nthe non-integral term corresponds to $$g_{\\theta,P} + \\varepsilon$$, which gives\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\leq g_{\\theta,P} + \\varepsilon + \\log \\{t \\leq 0\\}\n$$", "md": "Constructing the interval for which $$\\gamma_{\\theta,P} < -A$$. By Lemma C.11, for $$V = (P^{-1})Sc$$, and $$\\rho \\sqrt{RP} = O(\\lambda_{\\min}(P^*) + A) = O(A)$$, we have $$\\|\\theta Sc - \\theta^*Sc\\|_V \\geq \\sqrt{RP} \\Rightarrow \\gamma_{\\theta,P} < -A$$.\n\nFor any $$\\theta$$, notice that the set of matrices $$P$$ satisfying $$\\|\\theta Sc - \\theta^*Sc\\|_V \\geq RP$$ is connected (as its complement is compact). This forms the set $$I$$ for which $$\\gamma_{\\theta,P} < -A$$.\n\nConstructing intervals for which $$|\\gamma_{\\theta,P} - \\gamma_{\\theta,P'}| \\leq \\varepsilon$$. We will now construct a partition over those $$P$$ which satisfy $$\\|\\theta Sc - \\theta^*Sc\\|_V < RP$$, and show that the log-likelihood changes by at most $$\\varepsilon$$ for each cell in this partition.\n\nIf $$P \\in \\Omega_\\rho$$, then each of its elements $$P_{ij} \\in [-\\rho, \\rho]$$. For a parameter $$\\beta > 0$$ that we will specify later, consider the partition $$\\Omega_{\\rho,\\beta}$$ of $$\\Omega_\\rho$$, following Definition C.12. Clearly, the size of $$\\Omega_{\\rho,\\beta}$$ can be upper bounded by $$|\\Omega_{\\rho,\\beta}| \\leq 2d^2$$.\n\nWe will now analyze the effect of rounding down $$P \\in \\Omega$$ to its nearest element in $$\\Omega_{\\rho,\\beta}$$. By Claim C.13, for $$\\gamma = 2\\kappa\\beta d^2$$, we have $$(1 - \\gamma)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + \\gamma)\\|t - \\theta\\|_2 \\Sigma$$.\n\nConsider the log-likelihood at $$\\theta, P'$$:\n\n$$\nf_{\\theta,P'}(y) = \\frac{1}{2} \\log|P'| + \\log \\{t: tS \\leq 0, tSc = ySc\\} \\exp(-\\|t - \\theta\\|_2 \\Sigma')\n$$\n\nWe will use the LHS of Eqn (30) to show that\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\log|P'| \\leq f_{\\theta,P}(y) - \\frac{1}{2} \\log|P| + \\varepsilon\n$$\nand deal with the $$\\log|P'|$$ term later. The lower bound for the log-likelihood at $$P'$$ can be obtained via analogous proof using the RHS of Eqn (30).\n\nBy the LHS of Eqn (30), we get\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\leq \\exp(-(1 - \\gamma)\\|t - \\theta\\|_2 \\Sigma)\n$$\n\nRearranging the terms, we get\n\n$$\n2 \\log|P'| \\leq \\log \\{t: tS \\leq 0, tSc = ySc\\} f_{\\theta,P'}(y) - \\frac{1}{2} \\leq - (1 - 2\\gamma) + \\log \\{t \\leq 0\\} \\exp(-(1 - 2\\gamma)\\|P S^2(t - \\theta)S + PS^2 PSSc(ySc - \\theta Sc)\\|_2\n$$\n\nThe non-integral term corresponds to $$g_{\\theta,P}$$ in Eqn (21), while the integral term corresponds to $$h_{\\theta,P}$$ in Eqn (22).\n\nHandling the non-integral term. As we are only considering $$\\theta$$ such that $$\\|ySc - \\theta Sc\\|_{\\Sigma Sc} \\leq RP$$, we have that for $$\\beta = O(\\varepsilon) = O(\\varepsilon)$$,\n\n$$\nR^2 P d^2\\kappa \\text{poly}(A)\n$$\nthe non-integral term corresponds to $$g_{\\theta,P} + \\varepsilon$$, which gives\n\n$$\nf_{\\theta,P'}(y) - \\frac{1}{2} \\leq g_{\\theta,P} + \\varepsilon + \\log \\{t \\leq 0\\}\n$$"}]}, {"page": 32, "text": " Handling the integral.               Now we consider the integral term. Define the integral\n                                                                                        1\n                                                                                        2\n                                      I1 = log        t\u22640  exp      \u2212(1 \u2212   2  \u03b3)\u2225PS (t \u2212        \u00b5)\u22252\n for \u03b3 = 2d2\u03ba\u03b2 and \u00b5 = \u03b8S \u2212                 P \u22121\n                                               S PSSc(ySc \u2212           \u03b8Sc).\n Define the analogous integral that does not have the (1 \u2212                         1\u03b3) term in the exponential:\n                                           I2 = log       t\u22640   exp      \u22121 2\u2225P   S2(t \u2212    \u00b5)\u22252\n Clearly, I1 \u2265       I2.                                                  \u221a\nWe only need to consider \u00b5 such that \u2225\u00b5\u2225\u221e                         \u2264   O(     A\u03c1): otherwise the likelihood will be smaller\n than \u2212A.\n By Lemma C.14, for \u03b3 = O                       \u03b5       , we have\n                                            poly(A)\n                                                               I1 \u2264    I2 + \u03b5.\n Handling the log-determinant term.                         Now consider the log|P             | term. As we are decreasing each\n element by atmost \u03b2\u03c1, none of the eigenvalues can increase. Moreover, as\n                                                     Tr(P \u2032) \u2212      Tr(P   ) \u2265    \u2212d\u03b2\u03c1,\n we can conclude that each eigenvalue decreases by at most \u2212d\u03b2\u03c1. Also, as \u03c1 \u2264                                         \u03ba\u03bbj(P     ) \u2200  j \u2208   [d],\n we can conclude that each eigenvalue satisfies\n Hence, the log-determinant satisfies               \u03bbj(P \u2032) \u2265      \u03bbj(P    )(1 \u2212    d\u03b2\u03ba).\n                                                                          d2 \u03b2\u03ba                                               \u03b5           \u03b5\n log|P \u2032| \u2265     log|P   | + d log(1 \u2212        \u03b2d\u03ba) \u2265      log|P    | \u2212  1 \u2212    d\u03b2\u03ba \u2265      log|P   | \u2212   O(\u03b5) for \u03b2 \u2264         \u03bad2 \u2264     \u03bar2d2 .\n This finally gives\n                                                        |\u03b3\u03b8,P \u2212     \u03b3\u03b8,P \u2032| \u2264    O(\u03b5).\n Bounding the size of the net                   As \u03b2 = O              \u03b5       , and the max radius is also O(poly(A)), we\n                                                                  poly(A)\n have a cover of size          poly(A)        per entry of the precision matrix (for a fixed \u2126\u03c1).\n                                     \u03b5\n Intersecting the d2 nets means that for each \u2126\u03c1, we have a net of size\n                                                             poly     A, 1 \u03b5   d2     .\n As we are considering poly(A) many \u2126\u03c1s, the size of the net remains the same as the above.\n Claim C.13. In the setting of Lemma C.5, if P \u2208                        \u2126\u03c1 and P \u2032 \u2208        \u2126\u03c1,\u03b2 is its nearest neighbor, then for\n \u03b3 = 2\u03ba\u03b2d2, we have\n where \u03a3 := P \u22121, \u03a3\u2032 := P          (1 \u2212\u2032\u22121.\u03b3)\u2225t \u2212     \u03b8\u22252 \u03a3 \u2264    \u2225t \u2212   \u03b8\u22252 \u03a3\u2032 \u2264    (1 + \u03b3)\u2225t \u2212       \u03b8\u22252 \u03a3,                             (32)\n Proof. Consider P \u2208             \u2126\u03c1 and P \u2032 \u2208       \u2126\u03c1,\u03b2 such that P = P \u2032 + \u2206. Since P \u2032 is the rounding down of\n P , we have \u2206ij \u2208          [0, \u03b2\u03c1].\n As Tr(\u2206) \u2208        [0, \u03c1\u03b2d], and \u2225\u2206\u2225F \u2264             \u03c1\u03b2d, we have\n                                          \u03bbmax(\u2206) \u2264         \u03c1\u03b2d and \u03bbmin(\u2206) \u2265              \u2212\u03c1\u03b2d2.\n                                                                       32", "md": "# Math Equations\n\nHandling the integral.\n\nNow we consider the integral term. Define the integral\n\n$$\nI_1 = \\log \\int_{t\\leq 0} \\exp \\left( -(1 - \\frac{1}{2}\\gamma)\\|PS(t - \\mu)\\|^2 \\right)\n$$\nfor \\( \\gamma = 2d^2\\kappa\\beta \\) and \\( \\mu = \\theta_S - P_S^{-1}P_{SSc}(y_{Sc} - \\theta_{Sc}) \\).\n\nDefine the analogous integral that does not have the \\( (1 - \\frac{1}{\\gamma}) \\) term in the exponential:\n\n$$\nI_2 = \\log \\int_{t\\leq 0} \\exp \\left( -\\frac{1}{2}\\|P_S^2(t - \\mu)\\|^2 \\right)\n$$\nClearly, \\( I_1 \\geq I_2 \\).\n\nWe only need to consider \\( \\mu \\) such that \\( \\|\\mu\\|_{\\infty} \\leq O(\\sqrt{A\\rho}) \\): otherwise the likelihood will be smaller than \\( -A \\).\n\nBy Lemma C.14, for \\( \\gamma = O(\\varepsilon) \\), we have\n\n$$\n\\text{poly}(A) \\cdot I_1 \\leq I_2 + \\varepsilon.\n$$\n\nHandling the log-determinant term.\n\nNow consider the \\( \\log|P| \\) term. As we are decreasing each element by at most \\( \\beta\\rho \\), none of the eigenvalues can increase. Moreover, as\n\n$$\n\\text{Tr}(P') - \\text{Tr}(P) \\geq -d\\beta\\rho,\n$$\nwe can conclude that each eigenvalue decreases by at most \\( -d\\beta\\rho \\). Also, as \\( \\rho \\leq \\kappa\\lambda_j(P) \\) for all \\( j \\in [d] \\), we can conclude that each eigenvalue satisfies\n\n$$\n\\lambda_j(P') \\geq \\lambda_j(P)(1 - d^2\\beta\\kappa).\n$$\nHence, the log-determinant satisfies\n\n$$\n\\log|P'| \\geq \\log|P| + d\\log(1 - \\beta d\\kappa) \\geq \\log|P| - 1 - d\\beta\\kappa \\geq \\log|P| - O(\\varepsilon) \\text{ for } \\beta \\leq \\kappa d^2 \\leq \\kappa r^2d^2.\n$$\nThis finally gives\n\n$$\n|\\gamma\\theta,P - \\gamma\\theta,P'| \\leq O(\\varepsilon).\n$$\n\nBounding the size of the net\n\nAs \\( \\beta = O(\\varepsilon) \\), and the max radius is also \\( O(\\text{poly}(A)) \\), we have a cover of size \\( \\text{poly}(A) \\) per entry of the precision matrix (for a fixed \\( \\Omega\\rho \\)).\n\nIntersecting the \\( d^2 \\) nets means that for each \\( \\Omega\\rho \\), we have a net of size\n\n$$\n\\text{poly}A, 1/\\varepsilon, d^2.\n$$\nAs we are considering \\( \\text{poly}(A) \\) many \\( \\Omega\\rho \\)s, the size of the net remains the same as the above.\n\nClaim C.13. In the setting of Lemma C.5, if \\( P \\in \\Omega\\rho \\) and \\( P' \\in \\Omega\\rho,\\beta \\) is its nearest neighbor, then for \\( \\gamma = 2\\kappa\\beta d^2 \\), we have\n\n$$\n\\begin{align*}\n\\text{where } \\Sigma &:= P^{-1}, \\\\\n\\Sigma' &:= P(1 - \\gamma)^{-1}\\|t - \\theta\\|^2 \\Sigma \\leq \\|t - \\theta\\|^2 \\Sigma' \\leq (1 + \\gamma)\\|t - \\theta\\|^2 \\Sigma, \\quad (32)\n\\end{align*}\n$$\nProof. Consider \\( P \\in \\Omega\\rho \\) and \\( P' \\in \\Omega\\rho,\\beta \\) such that \\( P = P' + \\Delta \\). Since \\( P' \\) is the rounding down of \\( P \\), we have \\( \\Delta_{ij} \\in [0, \\beta\\rho] \\).\n\nAs \\( \\text{Tr}(\\Delta) \\in [0, \\rho\\beta d] \\), and \\( \\|\\Delta\\|_F \\leq \\rho\\beta d \\), we have\n\n$$\n\\lambda_{\\text{max}}(\\Delta) \\leq \\rho\\beta d \\text{ and } \\lambda_{\\text{min}}(\\Delta) \\geq -\\rho\\beta d^2.\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Handling the integral.\n\nNow we consider the integral term. Define the integral\n\n$$\nI_1 = \\log \\int_{t\\leq 0} \\exp \\left( -(1 - \\frac{1}{2}\\gamma)\\|PS(t - \\mu)\\|^2 \\right)\n$$\nfor \\( \\gamma = 2d^2\\kappa\\beta \\) and \\( \\mu = \\theta_S - P_S^{-1}P_{SSc}(y_{Sc} - \\theta_{Sc}) \\).\n\nDefine the analogous integral that does not have the \\( (1 - \\frac{1}{\\gamma}) \\) term in the exponential:\n\n$$\nI_2 = \\log \\int_{t\\leq 0} \\exp \\left( -\\frac{1}{2}\\|P_S^2(t - \\mu)\\|^2 \\right)\n$$\nClearly, \\( I_1 \\geq I_2 \\).\n\nWe only need to consider \\( \\mu \\) such that \\( \\|\\mu\\|_{\\infty} \\leq O(\\sqrt{A\\rho}) \\): otherwise the likelihood will be smaller than \\( -A \\).\n\nBy Lemma C.14, for \\( \\gamma = O(\\varepsilon) \\), we have\n\n$$\n\\text{poly}(A) \\cdot I_1 \\leq I_2 + \\varepsilon.\n$$\n\nHandling the log-determinant term.\n\nNow consider the \\( \\log|P| \\) term. As we are decreasing each element by at most \\( \\beta\\rho \\), none of the eigenvalues can increase. Moreover, as\n\n$$\n\\text{Tr}(P') - \\text{Tr}(P) \\geq -d\\beta\\rho,\n$$\nwe can conclude that each eigenvalue decreases by at most \\( -d\\beta\\rho \\). Also, as \\( \\rho \\leq \\kappa\\lambda_j(P) \\) for all \\( j \\in [d] \\), we can conclude that each eigenvalue satisfies\n\n$$\n\\lambda_j(P') \\geq \\lambda_j(P)(1 - d^2\\beta\\kappa).\n$$\nHence, the log-determinant satisfies\n\n$$\n\\log|P'| \\geq \\log|P| + d\\log(1 - \\beta d\\kappa) \\geq \\log|P| - 1 - d\\beta\\kappa \\geq \\log|P| - O(\\varepsilon) \\text{ for } \\beta \\leq \\kappa d^2 \\leq \\kappa r^2d^2.\n$$\nThis finally gives\n\n$$", "md": "Handling the integral.\n\nNow we consider the integral term. Define the integral\n\n$$\nI_1 = \\log \\int_{t\\leq 0} \\exp \\left( -(1 - \\frac{1}{2}\\gamma)\\|PS(t - \\mu)\\|^2 \\right)\n$$\nfor \\( \\gamma = 2d^2\\kappa\\beta \\) and \\( \\mu = \\theta_S - P_S^{-1}P_{SSc}(y_{Sc} - \\theta_{Sc}) \\).\n\nDefine the analogous integral that does not have the \\( (1 - \\frac{1}{\\gamma}) \\) term in the exponential:\n\n$$\nI_2 = \\log \\int_{t\\leq 0} \\exp \\left( -\\frac{1}{2}\\|P_S^2(t - \\mu)\\|^2 \\right)\n$$\nClearly, \\( I_1 \\geq I_2 \\).\n\nWe only need to consider \\( \\mu \\) such that \\( \\|\\mu\\|_{\\infty} \\leq O(\\sqrt{A\\rho}) \\): otherwise the likelihood will be smaller than \\( -A \\).\n\nBy Lemma C.14, for \\( \\gamma = O(\\varepsilon) \\), we have\n\n$$\n\\text{poly}(A) \\cdot I_1 \\leq I_2 + \\varepsilon.\n$$\n\nHandling the log-determinant term.\n\nNow consider the \\( \\log|P| \\) term. As we are decreasing each element by at most \\( \\beta\\rho \\), none of the eigenvalues can increase. Moreover, as\n\n$$\n\\text{Tr}(P') - \\text{Tr}(P) \\geq -d\\beta\\rho,\n$$\nwe can conclude that each eigenvalue decreases by at most \\( -d\\beta\\rho \\). Also, as \\( \\rho \\leq \\kappa\\lambda_j(P) \\) for all \\( j \\in [d] \\), we can conclude that each eigenvalue satisfies\n\n$$\n\\lambda_j(P') \\geq \\lambda_j(P)(1 - d^2\\beta\\kappa).\n$$\nHence, the log-determinant satisfies\n\n$$\n\\log|P'| \\geq \\log|P| + d\\log(1 - \\beta d\\kappa) \\geq \\log|P| - 1 - d\\beta\\kappa \\geq \\log|P| - O(\\varepsilon) \\text{ for } \\beta \\leq \\kappa d^2 \\leq \\kappa r^2d^2.\n$$\nThis finally gives\n\n$$"}, {"type": "table", "rows": [["\\gamma\\theta,P - \\gamma\\theta,P'"]], "md": "|\\gamma\\theta,P - \\gamma\\theta,P'| \\leq O(\\varepsilon).", "isPerfectTable": true, "csv": "\"\\gamma\\theta,P - \\gamma\\theta,P'\""}, {"type": "text", "value": "$$\n\nBounding the size of the net\n\nAs \\( \\beta = O(\\varepsilon) \\), and the max radius is also \\( O(\\text{poly}(A)) \\), we have a cover of size \\( \\text{poly}(A) \\) per entry of the precision matrix (for a fixed \\( \\Omega\\rho \\)).\n\nIntersecting the \\( d^2 \\) nets means that for each \\( \\Omega\\rho \\), we have a net of size\n\n$$\n\\text{poly}A, 1/\\varepsilon, d^2.\n$$\nAs we are considering \\( \\text{poly}(A) \\) many \\( \\Omega\\rho \\)s, the size of the net remains the same as the above.\n\nClaim C.13. In the setting of Lemma C.5, if \\( P \\in \\Omega\\rho \\) and \\( P' \\in \\Omega\\rho,\\beta \\) is its nearest neighbor, then for \\( \\gamma = 2\\kappa\\beta d^2 \\), we have\n\n$$\n\\begin{align*}\n\\text{where } \\Sigma &:= P^{-1}, \\\\\n\\Sigma' &:= P(1 - \\gamma)^{-1}\\|t - \\theta\\|^2 \\Sigma \\leq \\|t - \\theta\\|^2 \\Sigma' \\leq (1 + \\gamma)\\|t - \\theta\\|^2 \\Sigma, \\quad (32)\n\\end{align*}\n$$\nProof. Consider \\( P \\in \\Omega\\rho \\) and \\( P' \\in \\Omega\\rho,\\beta \\) such that \\( P = P' + \\Delta \\). Since \\( P' \\) is the rounding down of \\( P \\), we have \\( \\Delta_{ij} \\in [0, \\beta\\rho] \\).\n\nAs \\( \\text{Tr}(\\Delta) \\in [0, \\rho\\beta d] \\), and \\( \\|\\Delta\\|_F \\leq \\rho\\beta d \\), we have\n\n$$\n\\lambda_{\\text{max}}(\\Delta) \\leq \\rho\\beta d \\text{ and } \\lambda_{\\text{min}}(\\Delta) \\geq -\\rho\\beta d^2.\n$$", "md": "$$\n\nBounding the size of the net\n\nAs \\( \\beta = O(\\varepsilon) \\), and the max radius is also \\( O(\\text{poly}(A)) \\), we have a cover of size \\( \\text{poly}(A) \\) per entry of the precision matrix (for a fixed \\( \\Omega\\rho \\)).\n\nIntersecting the \\( d^2 \\) nets means that for each \\( \\Omega\\rho \\), we have a net of size\n\n$$\n\\text{poly}A, 1/\\varepsilon, d^2.\n$$\nAs we are considering \\( \\text{poly}(A) \\) many \\( \\Omega\\rho \\)s, the size of the net remains the same as the above.\n\nClaim C.13. In the setting of Lemma C.5, if \\( P \\in \\Omega\\rho \\) and \\( P' \\in \\Omega\\rho,\\beta \\) is its nearest neighbor, then for \\( \\gamma = 2\\kappa\\beta d^2 \\), we have\n\n$$\n\\begin{align*}\n\\text{where } \\Sigma &:= P^{-1}, \\\\\n\\Sigma' &:= P(1 - \\gamma)^{-1}\\|t - \\theta\\|^2 \\Sigma \\leq \\|t - \\theta\\|^2 \\Sigma' \\leq (1 + \\gamma)\\|t - \\theta\\|^2 \\Sigma, \\quad (32)\n\\end{align*}\n$$\nProof. Consider \\( P \\in \\Omega\\rho \\) and \\( P' \\in \\Omega\\rho,\\beta \\) such that \\( P = P' + \\Delta \\). Since \\( P' \\) is the rounding down of \\( P \\), we have \\( \\Delta_{ij} \\in [0, \\beta\\rho] \\).\n\nAs \\( \\text{Tr}(\\Delta) \\in [0, \\rho\\beta d] \\), and \\( \\|\\Delta\\|_F \\leq \\rho\\beta d \\), we have\n\n$$\n\\lambda_{\\text{max}}(\\Delta) \\leq \\rho\\beta d \\text{ and } \\lambda_{\\text{min}}(\\Delta) \\geq -\\rho\\beta d^2.\n$$"}]}, {"page": 33, "text": " This implies that when considering untruncated Gaussians with precision matrices P, P \u2032, we have\n that for all t, \u03b8 \u2208      Rd,\n                       \u2225t \u2212 \u03c1 \u03b8\u22252 \u03a3 \u2212   \u03c1\u03b2d2\u2225t \u2212       \u03b8\u22252 \u2264      \u2225t \u2212   \u03b8\u22252 \u03a3\u2032 \u2264    \u2225t \u2212   \u03b8\u22252 \u03a3 + \u03c1\u03b2d\u2225t \u2212         \u03b8\u22252.\n Since \u03bbmin(P        ) \u2265    2\u03ba, we have               \u03c1\u2225t \u2212    \u03b8\u22252 \u2264     2\u03ba\u2225t \u2212     \u03b8\u22252 \u03a3.\n Substituting in the previous inequality, we get\n                        \u2225t \u2212   \u03b8\u22252 \u03a3 \u2212    \u03c1\u03b2d2\u2225t \u2212       \u03b8\u22252 \u2264     \u2225t \u2212   \u03b8\u22252 \u03a3\u2032 \u2264    \u2225t \u2212    \u03b8\u22252\u03a3 + \u03c1\u03b2d\u2225t \u2212         \u03b8\u22252,\n                      =\u21d2      (1 \u2212    2\u03ba\u03b2d2)\u2225t \u2212        \u03b8\u22252 \u03a3 \u2264    \u2225t \u2212   \u03b8\u22252 \u03a3\u2032 \u2264    (1 + 2\u03ba\u03b2d)\u2225t \u2212           \u03b8\u22252\u03a3,\n For the sake of symmetry, we will use the weaker bound of\n                            (1 \u2212    2\u03ba\u03b2d2)\u2225t \u2212        \u03b8\u22252 \u03a3 \u2264    \u2225t \u2212   \u03b8\u22252 \u03a3\u2032 \u2264    (1 + 2\u03ba\u03b2d2)\u2225t \u2212           \u03b8\u22252 \u03a3,\n Setting \u03b3 = 2\u03ba\u03b2d2 completes the proof.\n Lemma C.14. Consider a bounded mean vector \u00b5 with \u2225\u00b5\u2225\u221e                                   \u2264   \u03b1 and precision matrix P with max\n eigenvalue \u03c1 and condition number \u03ba.\nFor \u03b3 = O         min            \u03b5            \u03b5        , we have\n                           \u03b1\u03c11/2d3/2 ,    \u03b12d3\u03c1\n                                                    1                                                             1\n           log    t\u22640   exp      \u2212(1 \u2212   2 \u03b3)  \u2225P   2 (t \u2212   \u00b5)\u22252       \u2264   \u03b5 + log      t\u22640   exp      \u22121 2\u2225P    2 (t \u2212   \u00b5)\u22252  .\n Proof of Lemma C.14. Wlog, consider \u00b5 \u2265                           0. The case where the entries are possibly negative\n follow a similar proof.\n Define the integral on the LHS and RHS of the Lemma statement by I1 and I2 respectively.\n By a change of variables, we set t\u2032 = \u221a1 \u2212                     \u03b3(t \u2212     \u00b5) + \u00b5 in I1, to get\n                                          1                                                          1\n                        I1 = log     \u221a  1 \u2212    \u03b3 + log       t\u2032\u2264(1\u2212\u221a1\u2212\u03b3)\u00b5        exp      \u22121 2\u2225P     2 (t\u2032 \u2212   \u00b5)\u22252     .\n Since \u03b3 < 1, we have (1 \u2212              \u221a1 \u2212      \u03b3)\u00b5 < \u03b3\u00b5. Substituting in I1, and for \u03b3 = O(\u03b5), we get\n                                                1                                              1\n                              I1 \u2264    log  \u221a  1 \u2212    \u03b3 + log       t\u2032\u2264\u03b3\u00b5   exp      \u22121 2\u2225P     2 (t\u2032 \u2212   \u00b5)\u22252     ,\n                                                                                    1\n                                  \u2264   O(\u03b5) + log         t\u2032\u2264\u03b3\u00b5   exp      \u22121 2\u2225P    2 (t\u2032 \u2212   \u00b5)\u22252      .\n The integrating set in the above inequality can be split into two parts: one over the negative orthant\n (which is exactly to eI2) and another over the shell\n This gives                                          C = {t\u2032 \u2264       \u03b3\u00b5} \\ {t\u2032 \u2264        0}.  1\n                             I1 \u2264    O(\u03b5) + log         eI2 +      t\u2032\u2208C   exp     \u22121  2\u2225P    2 (t\u2032 \u2212   \u00b5)\u22252        .\n In the above inequality, let eI3 denote the integral over the shell C. We will now show that I3 satisfies\n                                                                eI3 \u2264    \u03b5eI2.\n Let f(x) denote the Gaussian density with mean \u00b5 and precision matrix P                                    .\n For a subset of co-ordinates S \u2286                [d], S \u0338= \u2205, and t \u2208        Rd, let x+, x\u2212        \u2208   Rd be such that\n                         x+,S(i) =        \u03b3\u00b5i         if i \u2208   S,     ,    x\u2212,S(i) =        \u2212\u03b3   \u03b5 \u00b5i      if i \u2208   S,\n                                            ti        if i /\n                                                           \u2208   S,                             ti           if i /\u2208  S.\n                                                                       33", "md": "This implies that when considering untruncated Gaussians with precision matrices P, P', we have that for all $$t, \\theta \\in \\mathbb{R}^d$$,\n\n$$\n\\|t - \\rho \\theta\\|_2 \\Sigma - \\rho\\beta d^2\\|t - \\theta\\|_2 \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq \\|t - \\theta\\|_2 \\Sigma + \\rho\\beta d\\|t - \\theta\\|_2.\n$$\nSince $$\\lambda_{\\text{min}}(P) \\geq 2\\kappa$$, we have $$\\rho\\|t - \\theta\\|_2 \\leq 2\\kappa\\|t - \\theta\\|_2 \\Sigma$$.\n\nSubstituting in the previous inequality, we get\n\n$$\n\\|t - \\theta\\|_2 \\Sigma - \\rho\\beta d^2\\|t - \\theta\\|_2 \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq \\|t - \\theta\\|_2 \\Sigma + \\rho\\beta d\\|t - \\theta\\|_2,\n$$\n$$\n\\Rightarrow (1 - 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + 2\\kappa\\beta d)\\|t - \\theta\\|_2 \\Sigma.\n$$\nFor the sake of symmetry, we will use the weaker bound of\n\n$$\n(1 - 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma,\n$$\nSetting $$\\gamma = 2\\kappa\\beta d^2$$ completes the proof.\n\nLemma C.14. Consider a bounded mean vector $$\\mu$$ with $$\\|\\mu\\|_{\\infty} \\leq \\alpha$$ and precision matrix P with max eigenvalue $$\\rho$$ and condition number $$\\kappa$$.\n\nFor $$\\gamma = O(\\min(\\frac{\\alpha\\rho^{1/2}d^{3/2}}{\\alpha^2d^3\\rho^{1/2}}, 1))$$, we have\n\n$$\n\\log \\int_{t\\leq 0} \\exp\\left(-(1 - 2\\gamma)\\|P^{1/2}(t - \\mu)\\|_2^2\\right) \\leq \\epsilon + \\log \\int_{t\\leq 0} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t - \\mu)\\|_2^2\\right).\n$$\nProof of Lemma C.14. Wlog, consider $$\\mu \\geq 0$$. The case where the entries are possibly negative follow a similar proof.\n\nDefine the integral on the LHS and RHS of the Lemma statement by I1 and I2 respectively.\n\nBy a change of variables, we set $$t' = \\sqrt{1 - \\gamma}(t - \\mu) + \\mu$$ in I1, to get\n\n$$\nI1 = \\log \\sqrt{1 - \\gamma} + \\log \\int_{t'\\leq(1-\\sqrt{1-\\gamma})\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nSince $$\\gamma < 1$$, we have $$(1 - \\sqrt{1 - \\gamma})\\mu < \\gamma\\mu$$. Substituting in I1, and for $$\\gamma = O(\\epsilon)$$, we get\n\n$$\nI1 \\leq \\log \\sqrt{1 - \\gamma} + \\log \\int_{t'\\leq\\gamma\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right),\n$$\n$$\n\\leq O(\\epsilon) + \\log \\int_{t'\\leq\\gamma\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nThe integrating set in the above inequality can be split into two parts: one over the negative orthant (which is exactly to $$eI2$$) and another over the shell\n\nThis gives $$C = \\{t' \\leq \\gamma\\mu\\} \\backslash \\{t' \\leq 0\\}$$.\n\n$$\nI1 \\leq O(\\epsilon) + \\log eI2 + \\int_{t'\\in C} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nIn the above inequality, let $$eI3$$ denote the integral over the shell C. We will now show that I3 satisfies\n\n$$\neI3 \\leq \\epsilon eI2.\n$$\nLet $$f(x)$$ denote the Gaussian density with mean $$\\mu$$ and precision matrix P.\n\nFor a subset of co-ordinates $$S \\subseteq [d], S \\neq \\emptyset$$, and $$t \\in \\mathbb{R}^d$$, let $$x^+, x^- \\in \\mathbb{R}^d$$ be such that\n\n$$\nx^+,S(i) = \\gamma\\mu_i \\text{ if } i \\in S, \\quad x^-,S(i) = -\\gamma \\epsilon \\mu_i \\text{ if } i \\in S,\n$$\n$$\nx^+,S(i) = t_i \\text{ if } i \\notin S, \\quad x^-,S(i) = t_i \\text{ if } i \\notin S.\n$$", "images": [], "items": [{"type": "text", "value": "This implies that when considering untruncated Gaussians with precision matrices P, P', we have that for all $$t, \\theta \\in \\mathbb{R}^d$$,\n\n$$\n\\|t - \\rho \\theta\\|_2 \\Sigma - \\rho\\beta d^2\\|t - \\theta\\|_2 \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq \\|t - \\theta\\|_2 \\Sigma + \\rho\\beta d\\|t - \\theta\\|_2.\n$$\nSince $$\\lambda_{\\text{min}}(P) \\geq 2\\kappa$$, we have $$\\rho\\|t - \\theta\\|_2 \\leq 2\\kappa\\|t - \\theta\\|_2 \\Sigma$$.\n\nSubstituting in the previous inequality, we get\n\n$$\n\\|t - \\theta\\|_2 \\Sigma - \\rho\\beta d^2\\|t - \\theta\\|_2 \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq \\|t - \\theta\\|_2 \\Sigma + \\rho\\beta d\\|t - \\theta\\|_2,\n$$\n$$\n\\Rightarrow (1 - 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + 2\\kappa\\beta d)\\|t - \\theta\\|_2 \\Sigma.\n$$\nFor the sake of symmetry, we will use the weaker bound of\n\n$$\n(1 - 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma,\n$$\nSetting $$\\gamma = 2\\kappa\\beta d^2$$ completes the proof.\n\nLemma C.14. Consider a bounded mean vector $$\\mu$$ with $$\\|\\mu\\|_{\\infty} \\leq \\alpha$$ and precision matrix P with max eigenvalue $$\\rho$$ and condition number $$\\kappa$$.\n\nFor $$\\gamma = O(\\min(\\frac{\\alpha\\rho^{1/2}d^{3/2}}{\\alpha^2d^3\\rho^{1/2}}, 1))$$, we have\n\n$$\n\\log \\int_{t\\leq 0} \\exp\\left(-(1 - 2\\gamma)\\|P^{1/2}(t - \\mu)\\|_2^2\\right) \\leq \\epsilon + \\log \\int_{t\\leq 0} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t - \\mu)\\|_2^2\\right).\n$$\nProof of Lemma C.14. Wlog, consider $$\\mu \\geq 0$$. The case where the entries are possibly negative follow a similar proof.\n\nDefine the integral on the LHS and RHS of the Lemma statement by I1 and I2 respectively.\n\nBy a change of variables, we set $$t' = \\sqrt{1 - \\gamma}(t - \\mu) + \\mu$$ in I1, to get\n\n$$\nI1 = \\log \\sqrt{1 - \\gamma} + \\log \\int_{t'\\leq(1-\\sqrt{1-\\gamma})\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nSince $$\\gamma < 1$$, we have $$(1 - \\sqrt{1 - \\gamma})\\mu < \\gamma\\mu$$. Substituting in I1, and for $$\\gamma = O(\\epsilon)$$, we get\n\n$$\nI1 \\leq \\log \\sqrt{1 - \\gamma} + \\log \\int_{t'\\leq\\gamma\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right),\n$$\n$$\n\\leq O(\\epsilon) + \\log \\int_{t'\\leq\\gamma\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nThe integrating set in the above inequality can be split into two parts: one over the negative orthant (which is exactly to $$eI2$$) and another over the shell\n\nThis gives $$C = \\{t' \\leq \\gamma\\mu\\} \\backslash \\{t' \\leq 0\\}$$.\n\n$$\nI1 \\leq O(\\epsilon) + \\log eI2 + \\int_{t'\\in C} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nIn the above inequality, let $$eI3$$ denote the integral over the shell C. We will now show that I3 satisfies\n\n$$\neI3 \\leq \\epsilon eI2.\n$$\nLet $$f(x)$$ denote the Gaussian density with mean $$\\mu$$ and precision matrix P.\n\nFor a subset of co-ordinates $$S \\subseteq [d], S \\neq \\emptyset$$, and $$t \\in \\mathbb{R}^d$$, let $$x^+, x^- \\in \\mathbb{R}^d$$ be such that\n\n$$\nx^+,S(i) = \\gamma\\mu_i \\text{ if } i \\in S, \\quad x^-,S(i) = -\\gamma \\epsilon \\mu_i \\text{ if } i \\in S,\n$$\n$$\nx^+,S(i) = t_i \\text{ if } i \\notin S, \\quad x^-,S(i) = t_i \\text{ if } i \\notin S.\n$$", "md": "This implies that when considering untruncated Gaussians with precision matrices P, P', we have that for all $$t, \\theta \\in \\mathbb{R}^d$$,\n\n$$\n\\|t - \\rho \\theta\\|_2 \\Sigma - \\rho\\beta d^2\\|t - \\theta\\|_2 \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq \\|t - \\theta\\|_2 \\Sigma + \\rho\\beta d\\|t - \\theta\\|_2.\n$$\nSince $$\\lambda_{\\text{min}}(P) \\geq 2\\kappa$$, we have $$\\rho\\|t - \\theta\\|_2 \\leq 2\\kappa\\|t - \\theta\\|_2 \\Sigma$$.\n\nSubstituting in the previous inequality, we get\n\n$$\n\\|t - \\theta\\|_2 \\Sigma - \\rho\\beta d^2\\|t - \\theta\\|_2 \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq \\|t - \\theta\\|_2 \\Sigma + \\rho\\beta d\\|t - \\theta\\|_2,\n$$\n$$\n\\Rightarrow (1 - 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + 2\\kappa\\beta d)\\|t - \\theta\\|_2 \\Sigma.\n$$\nFor the sake of symmetry, we will use the weaker bound of\n\n$$\n(1 - 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma \\leq \\|t - \\theta\\|_2 \\Sigma' \\leq (1 + 2\\kappa\\beta d^2)\\|t - \\theta\\|_2 \\Sigma,\n$$\nSetting $$\\gamma = 2\\kappa\\beta d^2$$ completes the proof.\n\nLemma C.14. Consider a bounded mean vector $$\\mu$$ with $$\\|\\mu\\|_{\\infty} \\leq \\alpha$$ and precision matrix P with max eigenvalue $$\\rho$$ and condition number $$\\kappa$$.\n\nFor $$\\gamma = O(\\min(\\frac{\\alpha\\rho^{1/2}d^{3/2}}{\\alpha^2d^3\\rho^{1/2}}, 1))$$, we have\n\n$$\n\\log \\int_{t\\leq 0} \\exp\\left(-(1 - 2\\gamma)\\|P^{1/2}(t - \\mu)\\|_2^2\\right) \\leq \\epsilon + \\log \\int_{t\\leq 0} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t - \\mu)\\|_2^2\\right).\n$$\nProof of Lemma C.14. Wlog, consider $$\\mu \\geq 0$$. The case where the entries are possibly negative follow a similar proof.\n\nDefine the integral on the LHS and RHS of the Lemma statement by I1 and I2 respectively.\n\nBy a change of variables, we set $$t' = \\sqrt{1 - \\gamma}(t - \\mu) + \\mu$$ in I1, to get\n\n$$\nI1 = \\log \\sqrt{1 - \\gamma} + \\log \\int_{t'\\leq(1-\\sqrt{1-\\gamma})\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nSince $$\\gamma < 1$$, we have $$(1 - \\sqrt{1 - \\gamma})\\mu < \\gamma\\mu$$. Substituting in I1, and for $$\\gamma = O(\\epsilon)$$, we get\n\n$$\nI1 \\leq \\log \\sqrt{1 - \\gamma} + \\log \\int_{t'\\leq\\gamma\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right),\n$$\n$$\n\\leq O(\\epsilon) + \\log \\int_{t'\\leq\\gamma\\mu} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nThe integrating set in the above inequality can be split into two parts: one over the negative orthant (which is exactly to $$eI2$$) and another over the shell\n\nThis gives $$C = \\{t' \\leq \\gamma\\mu\\} \\backslash \\{t' \\leq 0\\}$$.\n\n$$\nI1 \\leq O(\\epsilon) + \\log eI2 + \\int_{t'\\in C} \\exp\\left(-\\frac{1}{2}\\|P^{1/2}(t' - \\mu)\\|_2^2\\right).\n$$\nIn the above inequality, let $$eI3$$ denote the integral over the shell C. We will now show that I3 satisfies\n\n$$\neI3 \\leq \\epsilon eI2.\n$$\nLet $$f(x)$$ denote the Gaussian density with mean $$\\mu$$ and precision matrix P.\n\nFor a subset of co-ordinates $$S \\subseteq [d], S \\neq \\emptyset$$, and $$t \\in \\mathbb{R}^d$$, let $$x^+, x^- \\in \\mathbb{R}^d$$ be such that\n\n$$\nx^+,S(i) = \\gamma\\mu_i \\text{ if } i \\in S, \\quad x^-,S(i) = -\\gamma \\epsilon \\mu_i \\text{ if } i \\in S,\n$$\n$$\nx^+,S(i) = t_i \\text{ if } i \\notin S, \\quad x^-,S(i) = t_i \\text{ if } i \\notin S.\n$$"}]}, {"page": 34, "text": "By the monotonicity of the Gaussian density, the integral over the shell C can be upper bounded by\nbreaking up into a sum of integrals over lower-dimensional strips, where for a fixed subset S \u2208                                            [d],\nthe variables tSc are integrated over (\u2212\u221e, \u03b3\u00b5Sc], while the variables in S are fixed to \u03b3\u00b5S.\nThis gives\n                                 eI3 \u2264    S\u2286[d]     tSc\u2264\u03b3\u00b5Sc      f(x+,S)      i\u2208S  \u03b3\u00b5i,\n                                      \u2264   S\u2286[d]     tSc\u2264\u03b3\u00b5Sc      f(x+,S)(\u03b3\u03b1)|S|,\n                                      \u2264     d    d     (\u03b3\u03b1)k         max                         f(x+,S).\n                                          k=1      k             S\u2286[d]:|S|=k       tSc\u2264\u03b3\u00b5Sc\nBy Claim C.15, for any S, and \u03b3 = O                      min        \u03b5         \u03b5          each summand satisfies\n                                                                 \u03b1\u221a\u03c1d,     \u03b12d2\u03c1\nFurthermore, for any S \u2286              [d], we have      f(x+,S) \u2264        2f(x\u2212,S)\nThis gives                                     tSc\u2264\u03b3\u00b5Sc      f(x\u2212,S)       \u03b3\u03b5 \u03b1   |S|   \u2264   eI2.\n                                        eI3 \u2264      d   dk2\u03b5keI2 \u2264        3\u03b5deI2         if \u03b5d \u2264     1\n                                                 k=1                                                3.\n                       \u03b5\nRescaling \u03b5 \u2190          3d completes the proof.\nClaim C.15. Let f be the Gaussian density with mean \u00b5 \u2208                                [0, \u03b1]d and precision matrix P \u2208                Rd\u00d7d\nwith max eigenvalue \u03c1 and condition number \u03ba.\nLet \u03b3 = O        min         \u03b5         \u03b5         . For any subset of co-ordinates S \u2286                   [d], S \u0338= \u2205, and t \u2208         Rd, let\n                          \u03b1\u221a\u03c1d,     \u03b12d2\u03c1\nx+, x\u2212     \u2208   Rd be such that\n                           x+(i) =       \u03b3\u00b5i         if i \u2208   S,     ,    x\u2212(i) =        \u2212\u03b3   \u03b5 \u00b5i     if i \u2208   S,\n                                           ti        if i /\nwe have                                                   \u2208   S,                          ti           if i /\u2208  S.\n                                                           f(x+) \u2264       2f(x\u2212)\nProof. WLOG, let S be a contiguous set such that we can separate the coordinates of x+ and x\u2212\ninto disjoint sets. For the coordinates belonging to S, let \u00b5S denote the coordinates of \u00b5 belonging to\n\u00b5, and \u00b5Sc the coordinates not belonging to S (similarly for tS and tSc).\nTaking the logarithm on both sides of the claimed inequality, we want to show that\n                                     1   \u03b3\u00b5S \u2212      \u00b5S      2                1   \u2212\u03b3 \u03b5 \u00b5S \u2212     \u00b5S      2\n                           \u22121 2  P   2   tSc \u2212    \u00b5Sc          \u2264  \u22121  2  P   2     tSc \u2212    \u00b5Sc           + log 2\nLet a and b denote the vectors whose norms correspond to the log-densities in the claimed inequality,\nand let \u03b4 = a \u2212        b.\nThis gives\n            1   \u2212\u00b5S(1 \u2212       \u03b3)                         1   \u2212\u00b5S(1 + \u03b3      \u03b5 )                                   1   \u2212\u00b5S\u03b3      1\u03b5 + 1\n  b = P     2    tSc \u2212     \u00b5Sc      ,        a    = P    2     tSc \u2212    \u00b5Sc      ,        \u03b4 := a \u2212      b = P     2          0Sc\n                                                                      34", "md": "By the monotonicity of the Gaussian density, the integral over the shell C can be upper bounded by breaking up into a sum of integrals over lower-dimensional strips, where for a fixed subset \\( S \\in [d] \\), the variables \\( t_{Sc} \\) are integrated over \\((-\u221e, \u03b3\\mu_{Sc}]\\), while the variables in \\( S \\) are fixed to \\( \u03b3\\mu_{S} \\). This gives\n\n$$\neI3 \\leq \\sum_{S \\subseteq [d]} \\prod_{i \\in S} \\gamma\\mu_i \\leq \\sum_{S \\subseteq [d]} \\prod_{i \\in S} \\gamma\\mu_i f(x^+,S) (\\gamma\\alpha)^{|S|} \\leq \\sum_{k=1}^{d} (\\gamma\\alpha)^k \\max_{S \\subseteq [d]: |S|=k} \\prod_{i \\in S} t_{Sc} \\leq \\frac{1}{\\alpha\\sqrt{\\rho}d} \\min\\left(\\frac{\\varepsilon}{\\alpha^2d^2\\rho}\\right)\n$$\n\nFurthermore, for any \\( S \\subseteq [d] \\), we have \\( f(x^+,S) \\leq 2f(x^-,S) \\). This gives\n\n$$\neI3 \\leq \\sum_{k=1}^{d} dk^2\\varepsilon^k eI2 \\leq 3\\varepsilon deI2 \\quad \\text{if} \\quad \\varepsilon d \\leq 1.\n$$\n\nRescaling \\( \\varepsilon \\) as \\( \\varepsilon \\leftarrow \\frac{\\varepsilon}{3d} \\) completes the proof.\n\n**Claim C.15:** Let \\( f \\) be the Gaussian density with mean \\( \\mu \\in [0, \\alpha]^d \\) and precision matrix \\( P \\in \\mathbb{R}^{d \\times d} \\) with max eigenvalue \\( \\rho \\) and condition number \\( \\kappa \\). Let \\( \\gamma = O\\left(\\min\\left(\\frac{\\varepsilon}{\\alpha\\sqrt{\\rho}d}, \\frac{\\varepsilon}{\\alpha^2d^2\\rho}\\right)\\right) \\). For any subset of co-ordinates \\( S \\subseteq [d] \\), \\( S \\neq \\emptyset \\), and \\( t \\in \\mathbb{R}^d \\), let \\( x^+, x^- \\in \\mathbb{R}^d \\) be such that\n\n\\[\nx^+(i) = \\begin{cases} \\gamma\\mu_i & \\text{if } i \\in S, \\\\ -\\gamma\\varepsilon\\mu_i & \\text{if } i \\in S, \\end{cases}\n\\]\n\nwe have \\( f(x^+) \\leq 2f(x^-) \\).\n\n**Proof:** WLOG, let \\( S \\) be a contiguous set such that we can separate the coordinates of \\( x^+ \\) and \\( x^- \\) into disjoint sets. For the coordinates belonging to \\( S \\), let \\( \\mu_S \\) denote the coordinates of \\( \\mu \\) belonging to \\( S \\), and \\( \\mu_{Sc} \\) the coordinates not belonging to \\( S \\) (similarly for \\( t_S \\) and \\( t_{Sc} \\)). Taking the logarithm on both sides of the claimed inequality, we want to show that\n\n\\[\n-\\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}) \\leq -\\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}) + \\log 2\n\\]\n\nLet \\( a \\) and \\( b \\) denote the vectors whose norms correspond to the log-densities in the claimed inequality, and let \\( \\delta = a - b \\). This gives\n\n\\[\nb = \\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}), \\quad a = \\frac{1}{2} (t_{Sc} - \\mu_{Sc}), \\quad \\delta := a - b = \\frac{1}{2} (t_{Sc} - \\mu_{Sc})\n\\]", "images": [], "items": [{"type": "text", "value": "By the monotonicity of the Gaussian density, the integral over the shell C can be upper bounded by breaking up into a sum of integrals over lower-dimensional strips, where for a fixed subset \\( S \\in [d] \\), the variables \\( t_{Sc} \\) are integrated over \\((-\u221e, \u03b3\\mu_{Sc}]\\), while the variables in \\( S \\) are fixed to \\( \u03b3\\mu_{S} \\). This gives\n\n$$\neI3 \\leq \\sum_{S \\subseteq [d]} \\prod_{i \\in S} \\gamma\\mu_i \\leq \\sum_{S \\subseteq [d]} \\prod_{i \\in S} \\gamma\\mu_i f(x^+,S) (\\gamma\\alpha)^{|S|} \\leq \\sum_{k=1}^{d} (\\gamma\\alpha)^k \\max_{S \\subseteq [d]: |S|=k} \\prod_{i \\in S} t_{Sc} \\leq \\frac{1}{\\alpha\\sqrt{\\rho}d} \\min\\left(\\frac{\\varepsilon}{\\alpha^2d^2\\rho}\\right)\n$$\n\nFurthermore, for any \\( S \\subseteq [d] \\), we have \\( f(x^+,S) \\leq 2f(x^-,S) \\). This gives\n\n$$\neI3 \\leq \\sum_{k=1}^{d} dk^2\\varepsilon^k eI2 \\leq 3\\varepsilon deI2 \\quad \\text{if} \\quad \\varepsilon d \\leq 1.\n$$\n\nRescaling \\( \\varepsilon \\) as \\( \\varepsilon \\leftarrow \\frac{\\varepsilon}{3d} \\) completes the proof.\n\n**Claim C.15:** Let \\( f \\) be the Gaussian density with mean \\( \\mu \\in [0, \\alpha]^d \\) and precision matrix \\( P \\in \\mathbb{R}^{d \\times d} \\) with max eigenvalue \\( \\rho \\) and condition number \\( \\kappa \\). Let \\( \\gamma = O\\left(\\min\\left(\\frac{\\varepsilon}{\\alpha\\sqrt{\\rho}d}, \\frac{\\varepsilon}{\\alpha^2d^2\\rho}\\right)\\right) \\). For any subset of co-ordinates \\( S \\subseteq [d] \\), \\( S \\neq \\emptyset \\), and \\( t \\in \\mathbb{R}^d \\), let \\( x^+, x^- \\in \\mathbb{R}^d \\) be such that\n\n\\[\nx^+(i) = \\begin{cases} \\gamma\\mu_i & \\text{if } i \\in S, \\\\ -\\gamma\\varepsilon\\mu_i & \\text{if } i \\in S, \\end{cases}\n\\]\n\nwe have \\( f(x^+) \\leq 2f(x^-) \\).\n\n**Proof:** WLOG, let \\( S \\) be a contiguous set such that we can separate the coordinates of \\( x^+ \\) and \\( x^- \\) into disjoint sets. For the coordinates belonging to \\( S \\), let \\( \\mu_S \\) denote the coordinates of \\( \\mu \\) belonging to \\( S \\), and \\( \\mu_{Sc} \\) the coordinates not belonging to \\( S \\) (similarly for \\( t_S \\) and \\( t_{Sc} \\)). Taking the logarithm on both sides of the claimed inequality, we want to show that\n\n\\[\n-\\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}) \\leq -\\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}) + \\log 2\n\\]\n\nLet \\( a \\) and \\( b \\) denote the vectors whose norms correspond to the log-densities in the claimed inequality, and let \\( \\delta = a - b \\). This gives\n\n\\[\nb = \\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}), \\quad a = \\frac{1}{2} (t_{Sc} - \\mu_{Sc}), \\quad \\delta := a - b = \\frac{1}{2} (t_{Sc} - \\mu_{Sc})\n\\]", "md": "By the monotonicity of the Gaussian density, the integral over the shell C can be upper bounded by breaking up into a sum of integrals over lower-dimensional strips, where for a fixed subset \\( S \\in [d] \\), the variables \\( t_{Sc} \\) are integrated over \\((-\u221e, \u03b3\\mu_{Sc}]\\), while the variables in \\( S \\) are fixed to \\( \u03b3\\mu_{S} \\). This gives\n\n$$\neI3 \\leq \\sum_{S \\subseteq [d]} \\prod_{i \\in S} \\gamma\\mu_i \\leq \\sum_{S \\subseteq [d]} \\prod_{i \\in S} \\gamma\\mu_i f(x^+,S) (\\gamma\\alpha)^{|S|} \\leq \\sum_{k=1}^{d} (\\gamma\\alpha)^k \\max_{S \\subseteq [d]: |S|=k} \\prod_{i \\in S} t_{Sc} \\leq \\frac{1}{\\alpha\\sqrt{\\rho}d} \\min\\left(\\frac{\\varepsilon}{\\alpha^2d^2\\rho}\\right)\n$$\n\nFurthermore, for any \\( S \\subseteq [d] \\), we have \\( f(x^+,S) \\leq 2f(x^-,S) \\). This gives\n\n$$\neI3 \\leq \\sum_{k=1}^{d} dk^2\\varepsilon^k eI2 \\leq 3\\varepsilon deI2 \\quad \\text{if} \\quad \\varepsilon d \\leq 1.\n$$\n\nRescaling \\( \\varepsilon \\) as \\( \\varepsilon \\leftarrow \\frac{\\varepsilon}{3d} \\) completes the proof.\n\n**Claim C.15:** Let \\( f \\) be the Gaussian density with mean \\( \\mu \\in [0, \\alpha]^d \\) and precision matrix \\( P \\in \\mathbb{R}^{d \\times d} \\) with max eigenvalue \\( \\rho \\) and condition number \\( \\kappa \\). Let \\( \\gamma = O\\left(\\min\\left(\\frac{\\varepsilon}{\\alpha\\sqrt{\\rho}d}, \\frac{\\varepsilon}{\\alpha^2d^2\\rho}\\right)\\right) \\). For any subset of co-ordinates \\( S \\subseteq [d] \\), \\( S \\neq \\emptyset \\), and \\( t \\in \\mathbb{R}^d \\), let \\( x^+, x^- \\in \\mathbb{R}^d \\) be such that\n\n\\[\nx^+(i) = \\begin{cases} \\gamma\\mu_i & \\text{if } i \\in S, \\\\ -\\gamma\\varepsilon\\mu_i & \\text{if } i \\in S, \\end{cases}\n\\]\n\nwe have \\( f(x^+) \\leq 2f(x^-) \\).\n\n**Proof:** WLOG, let \\( S \\) be a contiguous set such that we can separate the coordinates of \\( x^+ \\) and \\( x^- \\) into disjoint sets. For the coordinates belonging to \\( S \\), let \\( \\mu_S \\) denote the coordinates of \\( \\mu \\) belonging to \\( S \\), and \\( \\mu_{Sc} \\) the coordinates not belonging to \\( S \\) (similarly for \\( t_S \\) and \\( t_{Sc} \\)). Taking the logarithm on both sides of the claimed inequality, we want to show that\n\n\\[\n-\\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}) \\leq -\\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}) + \\log 2\n\\]\n\nLet \\( a \\) and \\( b \\) denote the vectors whose norms correspond to the log-densities in the claimed inequality, and let \\( \\delta = a - b \\). This gives\n\n\\[\nb = \\frac{1}{2} (t_{Sc} - \\mu_{Sc})^T P (t_{Sc} - \\mu_{Sc}), \\quad a = \\frac{1}{2} (t_{Sc} - \\mu_{Sc}), \\quad \\delta := a - b = \\frac{1}{2} (t_{Sc} - \\mu_{Sc})\n\\]"}]}, {"page": 35, "text": "We want to show that\n                                                          \u22121 2\u2225b\u22252 \u2264       \u22121  2\u2225a\u22252 + log 2,\n                                            \u21d4    \u27e8\u03b4, b\u27e9  + 1 2\u2225\u03b4\u22252 \u2264       log 2.                                         (33)\n As \u2225P    \u2225  \u2264   \u03c1 and \u2225\u00b5\u2225\u221e        \u2264   \u03b1, we have \u2225\u03b4\u22252 2 \u2264   \u03c1(\u03b12|S|)\u03b32          1 + 1  \u03b5  2  .\n For \u03b3 = O            \u03b5      , we get\n                   \u03b1\u221a\u03c1d\n                                                             \u2225\u03b4\u22252 2 \u2264    1                                                (34)\n                                                                         2 log 2.\n Similarly, consider the inner product \u27e8\u03b4, b\u27e9                  in Eqn (33). By the trace trick, we get\n where                                                      \u27e8\u03b4, b\u27e9  = Tr(\u2206P        ),\n                                            \u2206   =     \u2212\u00b5S(1 \u2212       \u03b3)      \u2212\u00b5S\u03b3      1\u03b5 + 1      T\n                                                       tSc \u2212     \u00b5Sc                0Sc\n Notice that the diagonal elements of \u2206                  are all non-negative. This implies that all singular values are\n non-negative. The trace of \u2206               is\n                                              Tr(\u2206) = \u2225\u00b5S\u22252         2(1 \u2212     \u03b3)\u03b3   1 \u03b5 + 1      .\n Hence, by Von Neumann\u2019s trace inequality, we get\n                                  \u27e8\u03b4, b\u27e9   \u2264  Tr(\u2206)Tr(P        ) \u2264   \u2225\u00b5S\u22252   2(1 \u2212    \u03b3)\u03b3    1 \u03b5 + 1      \u03c1d.\n For \u03b3 = O             \u03b5       , this gives\n                   \u03b12|S|\u03c1d                                   \u27e8\u03b4, b\u27e9  \u2264   1                                                (35)\n                                                                         2 log 2.\n Substituting Eqn (34) and Eqn (35) in Eqn (33) completes the proof.\n Lemma C.6 (W            -net). Let \u03b7Sc, \u03b7S be such that\n                                                    \u221712                       \u221712\n for B1, B2 \u2265        0.                        \u2225PSc \u03b7Sc\u2225        \u2264  B1, \u2225PS \u03b7S\u2225          \u2264   B2,\n Let A > max{B2           1, B2  2, poly(C, \u03ba)}. Let P \u2217             = \u03a3\u2217\u22121 be the precision matrix of \u03b7. For a fixed\n matrix P \u2208       Rd\u00d7d whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy\n \u03bbmax(P     ) \u2208   [e\u22122A  d \u03bbmin(P \u2217), C\u03bbmax(P \u2217)], there exists a partition I of Rd with size\n                                                             poly      A, 1 \u03b5   3d\n such that for each interval I \u2208              I, we have one of the following:\n           \u2022 for all \u03b8 \u2208      I, \u03b3\u03b8,P (y) < \u2212A, or\n           \u2022 for all \u03b8, \u03b8\u2032 \u2208      I, |\u03b3\u03b8,P (y) \u2212      \u03b3\u03b8\u2032,P (y)| \u2264     35\u03f5.", "md": "We want to show that\n\n$$\n- \\frac{1}{2} \\|b\\|_2^2 \\leq - \\frac{1}{2} \\|a\\|_2^2 + \\log 2, \\quad \\Rightarrow \\quad \\langle \\delta, b \\rangle + \\frac{1}{2} \\|\\delta\\|_2^2 \\leq \\log 2. \\quad (33)\n$$\n\nAs $\\|P\\| \\leq \\rho$ and $\\|\\mu\\|_{\\infty} \\leq \\alpha$, we have $\\|\\delta\\|_2^2 \\leq \\rho(\\alpha^2|S|)\\gamma^2 + \\frac{1}{\\varepsilon^2}$.\nFor $\\gamma = O\\left(\\frac{1}{\\varepsilon}\\right)$, we get\n\n$$\n\\frac{\\alpha\\sqrt{\\rho}d}{2} \\|\\delta\\|_2^2 \\leq 1 \\leq 2 \\log 2. \\quad (34)\n$$\n\nSimilarly, consider the inner product $\\langle \\delta, b \\rangle$ in Eqn (33). By the trace trick, we get\n\n$$\n\\langle \\delta, b \\rangle = \\text{Tr}(\\Delta P), \\quad \\Delta = -\\mu S(1 - \\gamma) - \\mu S\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon} I.\n$$\n\nNotice that the diagonal elements of $\\Delta$ are all non-negative. This implies that all singular values are non-negative. The trace of $\\Delta$ is\n\n$$\n\\text{Tr}(\\Delta) = \\|\\mu S\\|_2^2 \\left(2(1 - \\gamma)\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon}\\right).\n$$\n\nHence, by Von Neumann\u2019s trace inequality, we get\n\n$$\n\\langle \\delta, b \\rangle \\leq \\text{Tr}(\\Delta)\\text{Tr}(P) \\leq \\|\\mu S\\|_2^2 \\left(2(1 - \\gamma)\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon}\\right) \\rho d.\n$$\n\nFor $\\gamma = O\\left(\\frac{1}{\\varepsilon}\\right)$, this gives\n\n$$\n\\frac{\\alpha^2|S|\\rho d}{2} \\langle \\delta, b \\rangle \\leq 1 \\leq 2 \\log 2. \\quad (35)\n$$\n\nSubstituting Eqn (34) and Eqn (35) in Eqn (33) completes the proof.\n\nLemma C.6 (W-net). Let $\\eta_{Sc}, \\eta_{S}$ be such that\n\n$$\n\\begin{align*}\n&\\text{for } B_1, B_2 \\geq 0, \\quad \\|P_{Sc} \\eta_{Sc}\\| \\leq B_1, \\quad \\|PS \\eta_{S}\\| \\leq B_2, \\\\\n&\\text{Let } A > \\max\\{B_2, B_1, \\text{poly}(C, \\kappa)\\}. \\text{Let } P^* = \\Sigma^*{-1} \\text{ be the precision matrix of } \\eta. \\text{For a fixed} \\\\\n&\\text{matrix } P \\in \\mathbb{R}^{d \\times d} \\text{ whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy} \\\\\n&\\lambda_{\\text{max}}(P) \\in [e^{-2A} \\lambda_{\\text{min}}(P^*), C\\lambda_{\\text{max}}(P^*)], \\text{there exists a partition } I \\text{ of } \\mathbb{R}^d \\text{ with size} \\\\\n&\\text{poly}(A, \\frac{1}{\\varepsilon}, 3d) \\text{ such that for each interval } I \\in I, \\text{we have one of the following:} \\\\\n&\\bullet \\text{for all } \\theta \\in I, \\gamma_{\\theta, P}(y) < -A, \\text{or} \\\\\n&\\bullet \\text{for all } \\theta, \\theta' \\in I, |\\gamma_{\\theta, P}(y) - \\gamma_{\\theta', P}(y)| \\leq \\frac{3}{5}\\epsilon.\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "We want to show that\n\n$$\n- \\frac{1}{2} \\|b\\|_2^2 \\leq - \\frac{1}{2} \\|a\\|_2^2 + \\log 2, \\quad \\Rightarrow \\quad \\langle \\delta, b \\rangle + \\frac{1}{2} \\|\\delta\\|_2^2 \\leq \\log 2. \\quad (33)\n$$\n\nAs $\\|P\\| \\leq \\rho$ and $\\|\\mu\\|_{\\infty} \\leq \\alpha$, we have $\\|\\delta\\|_2^2 \\leq \\rho(\\alpha^2|S|)\\gamma^2 + \\frac{1}{\\varepsilon^2}$.\nFor $\\gamma = O\\left(\\frac{1}{\\varepsilon}\\right)$, we get\n\n$$\n\\frac{\\alpha\\sqrt{\\rho}d}{2} \\|\\delta\\|_2^2 \\leq 1 \\leq 2 \\log 2. \\quad (34)\n$$\n\nSimilarly, consider the inner product $\\langle \\delta, b \\rangle$ in Eqn (33). By the trace trick, we get\n\n$$\n\\langle \\delta, b \\rangle = \\text{Tr}(\\Delta P), \\quad \\Delta = -\\mu S(1 - \\gamma) - \\mu S\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon} I.\n$$\n\nNotice that the diagonal elements of $\\Delta$ are all non-negative. This implies that all singular values are non-negative. The trace of $\\Delta$ is\n\n$$\n\\text{Tr}(\\Delta) = \\|\\mu S\\|_2^2 \\left(2(1 - \\gamma)\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon}\\right).\n$$\n\nHence, by Von Neumann\u2019s trace inequality, we get\n\n$$\n\\langle \\delta, b \\rangle \\leq \\text{Tr}(\\Delta)\\text{Tr}(P) \\leq \\|\\mu S\\|_2^2 \\left(2(1 - \\gamma)\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon}\\right) \\rho d.\n$$\n\nFor $\\gamma = O\\left(\\frac{1}{\\varepsilon}\\right)$, this gives\n\n$$\n\\frac{\\alpha^2|S|\\rho d}{2} \\langle \\delta, b \\rangle \\leq 1 \\leq 2 \\log 2. \\quad (35)\n$$\n\nSubstituting Eqn (34) and Eqn (35) in Eqn (33) completes the proof.\n\nLemma C.6 (W-net). Let $\\eta_{Sc}, \\eta_{S}$ be such that\n\n$$\n\\begin{align*}\n&\\text{for } B_1, B_2 \\geq 0, \\quad \\|P_{Sc} \\eta_{Sc}\\| \\leq B_1, \\quad \\|PS \\eta_{S}\\| \\leq B_2, \\\\\n&\\text{Let } A > \\max\\{B_2, B_1, \\text{poly}(C, \\kappa)\\}. \\text{Let } P^* = \\Sigma^*{-1} \\text{ be the precision matrix of } \\eta. \\text{For a fixed} \\\\\n&\\text{matrix } P \\in \\mathbb{R}^{d \\times d} \\text{ whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy} \\\\\n&\\lambda_{\\text{max}}(P) \\in [e^{-2A} \\lambda_{\\text{min}}(P^*), C\\lambda_{\\text{max}}(P^*)], \\text{there exists a partition } I \\text{ of } \\mathbb{R}^d \\text{ with size} \\\\\n&\\text{poly}(A, \\frac{1}{\\varepsilon}, 3d) \\text{ such that for each interval } I \\in I, \\text{we have one of the following:} \\\\\n&\\bullet \\text{for all } \\theta \\in I, \\gamma_{\\theta, P}(y) < -A, \\text{or} \\\\\n&\\bullet \\text{for all } \\theta, \\theta' \\in I, |\\gamma_{\\theta, P}(y) - \\gamma_{\\theta', P}(y)| \\leq \\frac{3}{5}\\epsilon.\n\\end{align*}\n$$", "md": "We want to show that\n\n$$\n- \\frac{1}{2} \\|b\\|_2^2 \\leq - \\frac{1}{2} \\|a\\|_2^2 + \\log 2, \\quad \\Rightarrow \\quad \\langle \\delta, b \\rangle + \\frac{1}{2} \\|\\delta\\|_2^2 \\leq \\log 2. \\quad (33)\n$$\n\nAs $\\|P\\| \\leq \\rho$ and $\\|\\mu\\|_{\\infty} \\leq \\alpha$, we have $\\|\\delta\\|_2^2 \\leq \\rho(\\alpha^2|S|)\\gamma^2 + \\frac{1}{\\varepsilon^2}$.\nFor $\\gamma = O\\left(\\frac{1}{\\varepsilon}\\right)$, we get\n\n$$\n\\frac{\\alpha\\sqrt{\\rho}d}{2} \\|\\delta\\|_2^2 \\leq 1 \\leq 2 \\log 2. \\quad (34)\n$$\n\nSimilarly, consider the inner product $\\langle \\delta, b \\rangle$ in Eqn (33). By the trace trick, we get\n\n$$\n\\langle \\delta, b \\rangle = \\text{Tr}(\\Delta P), \\quad \\Delta = -\\mu S(1 - \\gamma) - \\mu S\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon} I.\n$$\n\nNotice that the diagonal elements of $\\Delta$ are all non-negative. This implies that all singular values are non-negative. The trace of $\\Delta$ is\n\n$$\n\\text{Tr}(\\Delta) = \\|\\mu S\\|_2^2 \\left(2(1 - \\gamma)\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon}\\right).\n$$\n\nHence, by Von Neumann\u2019s trace inequality, we get\n\n$$\n\\langle \\delta, b \\rangle \\leq \\text{Tr}(\\Delta)\\text{Tr}(P) \\leq \\|\\mu S\\|_2^2 \\left(2(1 - \\gamma)\\gamma \\frac{1}{\\varepsilon} + \\frac{1}{\\varepsilon}\\right) \\rho d.\n$$\n\nFor $\\gamma = O\\left(\\frac{1}{\\varepsilon}\\right)$, this gives\n\n$$\n\\frac{\\alpha^2|S|\\rho d}{2} \\langle \\delta, b \\rangle \\leq 1 \\leq 2 \\log 2. \\quad (35)\n$$\n\nSubstituting Eqn (34) and Eqn (35) in Eqn (33) completes the proof.\n\nLemma C.6 (W-net). Let $\\eta_{Sc}, \\eta_{S}$ be such that\n\n$$\n\\begin{align*}\n&\\text{for } B_1, B_2 \\geq 0, \\quad \\|P_{Sc} \\eta_{Sc}\\| \\leq B_1, \\quad \\|PS \\eta_{S}\\| \\leq B_2, \\\\\n&\\text{Let } A > \\max\\{B_2, B_1, \\text{poly}(C, \\kappa)\\}. \\text{Let } P^* = \\Sigma^*{-1} \\text{ be the precision matrix of } \\eta. \\text{For a fixed} \\\\\n&\\text{matrix } P \\in \\mathbb{R}^{d \\times d} \\text{ whose condition number satisfies Assumption 4.4 and whose eigenvalues satisfy} \\\\\n&\\lambda_{\\text{max}}(P) \\in [e^{-2A} \\lambda_{\\text{min}}(P^*), C\\lambda_{\\text{max}}(P^*)], \\text{there exists a partition } I \\text{ of } \\mathbb{R}^d \\text{ with size} \\\\\n&\\text{poly}(A, \\frac{1}{\\varepsilon}, 3d) \\text{ such that for each interval } I \\in I, \\text{we have one of the following:} \\\\\n&\\bullet \\text{for all } \\theta \\in I, \\gamma_{\\theta, P}(y) < -A, \\text{or} \\\\\n&\\bullet \\text{for all } \\theta, \\theta' \\in I, |\\gamma_{\\theta, P}(y) - \\gamma_{\\theta', P}(y)| \\leq \\frac{3}{5}\\epsilon.\n\\end{align*}\n$$"}]}, {"page": 36, "text": " Proof of Lemma C.6. Recall that the log-likelihood ratio \u03b3\u03b8 can be decomposed into the difference\n of two terms that depend on \u03b8:\n                      \u03b3\u03b8,P (y) \u2212      1\n                                      2 log |P     |\n                                               |P \u2032| =g\u03b8,P (y) \u2212         g\u03b8\u2217,P \u2217(y) + h\u03b8,P (y) \u2212           h\u03b8\u2217,P \u2217(y).\nWithout loss of generality, consider the net for the first coordinate \u03b81. The final net will be the\n intersection of the per-coordinate nets.\nWe will construct three partitions: the first is Ih,0 for h when y1 = 0, the second is is Ih,1 for h\n when y1 > 0, and the last is Ig for g when y1 > 0. The final partition will be the intersection of these\n partitions.\n Case 1: Net over h, y1 = 0.                 As y1 = 0, we have 1 \u2208              S. For \u03b8 \u2208       Rd, we have\n                                                             1\n                h\u03b8,P (y) = log          t\u22640  exp     \u2212\u2225P    S2(t \u2212    \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212                  \u03b8Sc)\u22252/2       ,\n                                        \u221a C\u03baA\n By Claim C.16, if \u03b81 \u2265            \u0398(      p1    ), then the log-likelihood is smaller than \u2212A.\n Now, consider \u03b81 < O(             \u221a  C\u03baA   ). Let \u03b8\u2032 = \u03b8 + \u03b1e1 for \u03b1 > 0. As h is monotonically decreasing per\n                                      p1\n coordinate, \u03b81 < \u03b8\u2032       1 =   \u21d2     h\u03b8,P \u2265      h\u03b8\u2032,P . We would like to now upper bound h\u03b8,P in terms of h\u03b8\u2032,P .\n Let\n                                               \u00b5 :=\u03b8S + (PS)\u22121PSSc(ySc \u2212                    \u03b8Sc),\n                                              \u00b5\u2032 :=\u03b8\u2032  S + (PS)\u22121PSSc(ySc \u2212                 \u03b8Sc)\n In the function h\u03b8, break the integrating set into two domains: one where t1\u2212                                   \u00b5 is small,\n and another where it is large:             \u21261 =        t \u2208   R|S| : \u2225P    S2(t \u2212    \u00b5)\u2225   \u2264   r   ,\n                                                                            1\n                                                                            2\n for some r > 0 that we will specify later. \u21262 =        t \u2208   R|S| : \u2225PS (t \u2212        \u00b5)\u2225   > r     ,\n Let I1 and I2 denote the integrals over \u21261 and \u21262 respectively.\n I2 corresponds to the tail of an unnormalized Gaussian distribution, and hence we have\n                                                                                            1\n                             h\u03b8,P (y) = log           I2 +      t\u22640,t\u2208\u21261    exp     \u2212\u2225P    S2(t \u2212    \u00b5)\u22252/2          ,\n                                                     |S|     \u22121 2\n                            where I2 \u2264        (2\u03c0)    2   P S     e\u2212r2.\nWe can simplify I2 be comparing |P                    | to |P \u2217|:\n                                |S|   P \u22121/2                                       |S|                  |S|\n                I2 \u2264     (2\u03c0)    2      S          P \u2217\u22121/2 e\u2212r2 \u2264          (2\u03c0)     2       \u03bb\u2217max             P  \u2217\u22121 2  e\u2212r2,\n                                     P \u2217\u22121/2          S                                   \u03bbmin(P     )           S\n                                        S\n                                |S|       A  |S|      \u2217\u22121  2\n                     \u2264   (2\u03c0)    2    \u03bae  d        P  S      e\u2212r2.\n By Lemma C.10, we have\n                                                 |S|     \u2217\u22121 2                                 2+B2 3)\n                                         (2\u03c0)     2   P S        \u2264   eh\u03b8\u2217,P \u2217(y)+O(d+B2\n                                                                       36", "md": "Proof of Lemma C.6. Recall that the log-likelihood ratio $$\\gamma_{\\theta}$$ can be decomposed into the difference\nof two terms that depend on $$\\theta$$:\n$$\n\\gamma_{\\theta,P}(y) - \\frac{1}{2} \\log |P| |P'| = g_{\\theta,P}(y) - g_{\\theta^*,P^*}(y) + h_{\\theta,P}(y) - h_{\\theta^*,P^*}(y).\n$$\nWithout loss of generality, consider the net for the first coordinate $$\\theta_1$$. The final net will be the\nintersection of the per-coordinate nets.\nWe will construct three partitions: the first is $$I_{h,0}$$ for $$h$$ when $$y_1 = 0$$, the second is $$I_{h,1}$$ for $$h$$\nwhen $$y_1 > 0$$, and the last is $$I_g$$ for $$g$$ when $$y_1 > 0$$. The final partition will be the intersection of these\npartitions.\n\nCase 1: Net over $$h, y_1 = 0$$. As $$y_1 = 0$$, we have $$1 \\in S$$. For $$\\theta \\in \\mathbb{R}^d$$, we have\n$$\nh_{\\theta,P}(y) = \\log \\int_{t\\leq 0} \\exp\\left(-\\frac{\\|P S_2(t - \\theta_S) + (PS)^{-1/2}PSS^c(y^c - \\theta^c)\\|^2}{2}\\right),\n$$\nBy Claim C.16, if $$\\theta_1 \\geq \\Theta(p_1)$$, then the log-likelihood is smaller than $$-A$$.\nNow, consider $$\\theta_1 < O(\\sqrt{C\\kappa A})$$. Let $$\\theta' = \\theta + \\alpha e_1$$ for $$\\alpha > 0$$. As $$h$$ is monotonically decreasing per\ncoordinate, $$\\theta_1 < \\theta'_1 = \\Rightarrow h_{\\theta,P} \\geq h_{\\theta',P}$$. We would like to now upper bound $$h_{\\theta,P}$$ in terms of $$h_{\\theta',P}$$.\nLet\n$$\n\\mu := \\theta_S + (PS)^{-1}PSS^c(y^c - \\theta^c), \\quad \\mu' := \\theta'_S + (PS)^{-1}PSS^c(y^c - \\theta^c)\n$$\nIn the function $$h_{\\theta}$$, break the integrating set into two domains: one where $$t_1 - \\mu$$ is small,\nand another where it is large: $$\\Omega_1 = \\{t \\in \\mathbb{R}^{|S|} : \\|P S_2(t - \\mu)\\| \\leq r\\}$$,\nfor some $$r > 0$$ that we will specify later. $$\\Omega_2 = \\{t \\in \\mathbb{R}^{|S|} : \\|PS(t - \\mu) \\| > r\\}$$,\nLet $$I_1$$ and $$I_2$$ denote the integrals over $$\\Omega_1$$ and $$\\Omega_2$$ respectively.\n$$I_2$$ corresponds to the tail of an unnormalized Gaussian distribution, and hence we have\n$$\nh_{\\theta,P}(y) = \\log I_2 + \\int_{t\\leq 0, t\\in\\Omega_1} \\exp\\left(-\\frac{\\|P S_2(t - \\mu)\\|^2}{2}\\right),\n$$\nwhere $$I_2 \\leq (2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} e^{-r^2}$$.\nWe can simplify $$I_2$$ by comparing $$|P|$$ to $$|P^*|$$:\n$$\nI_2 \\leq (2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} e^{-r^2} \\leq (2\\pi)^{1/2} |S|^{-1/2} \\lambda^*_{\\text{max}} P^{*-1/2} e^{-r^2},\n$$\n$$\n\\leq (2\\pi)^{1/2} \\kappa e^{d} A |S|^{-1/2} P^{*-1/2} e^{-r^2}.\n$$\nBy Lemma C.10, we have\n$$\n(2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} \\leq e^{h_{\\theta^*,P^*}(y) + O(d+B^2/3)}.\n$$", "images": [], "items": [{"type": "text", "value": "Proof of Lemma C.6. Recall that the log-likelihood ratio $$\\gamma_{\\theta}$$ can be decomposed into the difference\nof two terms that depend on $$\\theta$$:\n$$\n\\gamma_{\\theta,P}(y) - \\frac{1}{2} \\log |P| |P'| = g_{\\theta,P}(y) - g_{\\theta^*,P^*}(y) + h_{\\theta,P}(y) - h_{\\theta^*,P^*}(y).\n$$\nWithout loss of generality, consider the net for the first coordinate $$\\theta_1$$. The final net will be the\nintersection of the per-coordinate nets.\nWe will construct three partitions: the first is $$I_{h,0}$$ for $$h$$ when $$y_1 = 0$$, the second is $$I_{h,1}$$ for $$h$$\nwhen $$y_1 > 0$$, and the last is $$I_g$$ for $$g$$ when $$y_1 > 0$$. The final partition will be the intersection of these\npartitions.\n\nCase 1: Net over $$h, y_1 = 0$$. As $$y_1 = 0$$, we have $$1 \\in S$$. For $$\\theta \\in \\mathbb{R}^d$$, we have\n$$\nh_{\\theta,P}(y) = \\log \\int_{t\\leq 0} \\exp\\left(-\\frac{\\|P S_2(t - \\theta_S) + (PS)^{-1/2}PSS^c(y^c - \\theta^c)\\|^2}{2}\\right),\n$$\nBy Claim C.16, if $$\\theta_1 \\geq \\Theta(p_1)$$, then the log-likelihood is smaller than $$-A$$.\nNow, consider $$\\theta_1 < O(\\sqrt{C\\kappa A})$$. Let $$\\theta' = \\theta + \\alpha e_1$$ for $$\\alpha > 0$$. As $$h$$ is monotonically decreasing per\ncoordinate, $$\\theta_1 < \\theta'_1 = \\Rightarrow h_{\\theta,P} \\geq h_{\\theta',P}$$. We would like to now upper bound $$h_{\\theta,P}$$ in terms of $$h_{\\theta',P}$$.\nLet\n$$\n\\mu := \\theta_S + (PS)^{-1}PSS^c(y^c - \\theta^c), \\quad \\mu' := \\theta'_S + (PS)^{-1}PSS^c(y^c - \\theta^c)\n$$\nIn the function $$h_{\\theta}$$, break the integrating set into two domains: one where $$t_1 - \\mu$$ is small,\nand another where it is large: $$\\Omega_1 = \\{t \\in \\mathbb{R}^{|S|} : \\|P S_2(t - \\mu)\\| \\leq r\\}$$,\nfor some $$r > 0$$ that we will specify later. $$\\Omega_2 = \\{t \\in \\mathbb{R}^{|S|} : \\|PS(t - \\mu) \\| > r\\}$$,\nLet $$I_1$$ and $$I_2$$ denote the integrals over $$\\Omega_1$$ and $$\\Omega_2$$ respectively.\n$$I_2$$ corresponds to the tail of an unnormalized Gaussian distribution, and hence we have\n$$\nh_{\\theta,P}(y) = \\log I_2 + \\int_{t\\leq 0, t\\in\\Omega_1} \\exp\\left(-\\frac{\\|P S_2(t - \\mu)\\|^2}{2}\\right),\n$$\nwhere $$I_2 \\leq (2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} e^{-r^2}$$.\nWe can simplify $$I_2$$ by comparing $$|P|$$ to $$|P^*|$$:\n$$\nI_2 \\leq (2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} e^{-r^2} \\leq (2\\pi)^{1/2} |S|^{-1/2} \\lambda^*_{\\text{max}} P^{*-1/2} e^{-r^2},\n$$\n$$\n\\leq (2\\pi)^{1/2} \\kappa e^{d} A |S|^{-1/2} P^{*-1/2} e^{-r^2}.\n$$\nBy Lemma C.10, we have\n$$\n(2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} \\leq e^{h_{\\theta^*,P^*}(y) + O(d+B^2/3)}.\n$$", "md": "Proof of Lemma C.6. Recall that the log-likelihood ratio $$\\gamma_{\\theta}$$ can be decomposed into the difference\nof two terms that depend on $$\\theta$$:\n$$\n\\gamma_{\\theta,P}(y) - \\frac{1}{2} \\log |P| |P'| = g_{\\theta,P}(y) - g_{\\theta^*,P^*}(y) + h_{\\theta,P}(y) - h_{\\theta^*,P^*}(y).\n$$\nWithout loss of generality, consider the net for the first coordinate $$\\theta_1$$. The final net will be the\nintersection of the per-coordinate nets.\nWe will construct three partitions: the first is $$I_{h,0}$$ for $$h$$ when $$y_1 = 0$$, the second is $$I_{h,1}$$ for $$h$$\nwhen $$y_1 > 0$$, and the last is $$I_g$$ for $$g$$ when $$y_1 > 0$$. The final partition will be the intersection of these\npartitions.\n\nCase 1: Net over $$h, y_1 = 0$$. As $$y_1 = 0$$, we have $$1 \\in S$$. For $$\\theta \\in \\mathbb{R}^d$$, we have\n$$\nh_{\\theta,P}(y) = \\log \\int_{t\\leq 0} \\exp\\left(-\\frac{\\|P S_2(t - \\theta_S) + (PS)^{-1/2}PSS^c(y^c - \\theta^c)\\|^2}{2}\\right),\n$$\nBy Claim C.16, if $$\\theta_1 \\geq \\Theta(p_1)$$, then the log-likelihood is smaller than $$-A$$.\nNow, consider $$\\theta_1 < O(\\sqrt{C\\kappa A})$$. Let $$\\theta' = \\theta + \\alpha e_1$$ for $$\\alpha > 0$$. As $$h$$ is monotonically decreasing per\ncoordinate, $$\\theta_1 < \\theta'_1 = \\Rightarrow h_{\\theta,P} \\geq h_{\\theta',P}$$. We would like to now upper bound $$h_{\\theta,P}$$ in terms of $$h_{\\theta',P}$$.\nLet\n$$\n\\mu := \\theta_S + (PS)^{-1}PSS^c(y^c - \\theta^c), \\quad \\mu' := \\theta'_S + (PS)^{-1}PSS^c(y^c - \\theta^c)\n$$\nIn the function $$h_{\\theta}$$, break the integrating set into two domains: one where $$t_1 - \\mu$$ is small,\nand another where it is large: $$\\Omega_1 = \\{t \\in \\mathbb{R}^{|S|} : \\|P S_2(t - \\mu)\\| \\leq r\\}$$,\nfor some $$r > 0$$ that we will specify later. $$\\Omega_2 = \\{t \\in \\mathbb{R}^{|S|} : \\|PS(t - \\mu) \\| > r\\}$$,\nLet $$I_1$$ and $$I_2$$ denote the integrals over $$\\Omega_1$$ and $$\\Omega_2$$ respectively.\n$$I_2$$ corresponds to the tail of an unnormalized Gaussian distribution, and hence we have\n$$\nh_{\\theta,P}(y) = \\log I_2 + \\int_{t\\leq 0, t\\in\\Omega_1} \\exp\\left(-\\frac{\\|P S_2(t - \\mu)\\|^2}{2}\\right),\n$$\nwhere $$I_2 \\leq (2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} e^{-r^2}$$.\nWe can simplify $$I_2$$ by comparing $$|P|$$ to $$|P^*|$$:\n$$\nI_2 \\leq (2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} e^{-r^2} \\leq (2\\pi)^{1/2} |S|^{-1/2} \\lambda^*_{\\text{max}} P^{*-1/2} e^{-r^2},\n$$\n$$\n\\leq (2\\pi)^{1/2} \\kappa e^{d} A |S|^{-1/2} P^{*-1/2} e^{-r^2}.\n$$\nBy Lemma C.10, we have\n$$\n(2\\pi)^{1/2} |S|^{-1/2} P^{-1/2} \\leq e^{h_{\\theta^*,P^*}(y) + O(d+B^2/3)}.\n$$"}]}, {"page": 37, "text": "As we are only consider \u03b8\u2032 such that h\u03b8\u2217,P \u2217(y) \u2212                               A < h\u03b8\u2032,P (y), for\n                                                r2 = O(d log \u03ba + A + log 1                 \u03b5  ) = O(A),\nwe have\nSubsituting in h\u03b8,P (y), we get                                    I2 \u2264     \u03b5eh\u03b8\u2032,P (y)                   1\n                          h\u03b8,P (y) \u2264        log     \u03b5eh\u03b8\u2032,P (y) +          t\u22640,t\u2208\u21261  1   exp (\u2212\u2225PS (t \u2212   2         \u00b5)\u22252/2)\nNow consider the integral I1 =                       t\u22640,t\u2208\u21261 exp           \u2212\u2225P     S2(t \u2212     \u00b5)\u22252/2        .\nBy Claim C.17, as \u21261 is defined for t bounded by r, and \u00b5 \u2212                                       \u00b5\u2032 = \u03b1e1, we have\n                                         I1 \u2264     exp     2\u03b1p1r + \u03b12p2          1   \u00b7 I\u20321,\n                                                                                    1\n                              where I\u2032     1 =      t\u22640,t\u2208\u21261      exp      \u2212\u2225P     S2(t \u2212     \u00b5\u2032)\u22252/2         \u2264    eh\u03b8\u2032,P (y).\nSubstituting in the expression for h\u03b8,P , we get\n                                   h\u03b8,P (y) \u2264        log    eh\u03b8\u2032,P (y) \u00b7       \u03b5 + exp        2\u03b1p1r + \u03b12p2          1\nAs log(\u03b5 + ex) \u2264             \u03b5 + x for x \u2265           0, we have\nSetting \u03b1 = O( \u03b5         p1r   ), we get     h\u03b8,P (y) \u2264        h\u03b8\u2032,P (y) + \u03b5 + 2\u03b1p1r + \u03b12p2                   1.\n                                                          h\u03b8,P (y) \u2264         h\u03b8\u2032,P (y) + 2\u03b5.\nThis shows that h\u03b8,P changes by at most \u03b5 for the considered net. We need to defined the other end\npoint for the net. By a similar argument to the positive end point, if \u03b81 = \u2212O                                                  C\u03ba log( 1  \u03b5)  , the\n                                                                                                                                    p1\nlog-likelihood ratio changes by at most \u03b5 until \u03b81 = \u2212\u221e.                            \u221a  C\u03baA\nAs we are only trying to cover \u03b8 such that |\u03b81| \u2264                              O(      p1     ), this net has size\n                                              O   \u221a     C\u03baA          = O      \u221a     C\u03baA      p1r        = A\n                                                       p1\u03b1                          p1         \u03b5             \u03b5 .\nCase 2: Net over h, y > 0.                      A similar argument to Case 1 works here as well.\nCase 3: Net over g, y1 > 0.                      By Lemma C.11, if |\u03b8 \u2212           \u221a      \u03b8\u2217|PSc > R1 for\n                                                                   R1 = O(            A),\nthen\nNow consider \u03b8 such that                                       g\u03b8,P \u2212      g\u03b8\u2217,P < \u2212A.\nand \u03b8, \u03b8\u2032 such that \u03b8 \u2212              \u03b8\u2032 = \u03b1e1.                     |\u03b81 \u2212     \u03b8\u22171| \u2264    R,\n                                                                             37", "md": "# Math Equations\n\nAs we are only consider $$\\theta'$$ such that $$h_{\\theta^*,P^*}(y) - A < h_{\\theta',P}(y)$$, for $$r^2 = O(d \\log \\kappa + A + \\log \\frac{1}{\\varepsilon}) = O(A)$$, we have\n\nSubstituting in $$h_{\\theta,P}(y)$$, we get $$I_2 \\leq \\varepsilon e^{h_{\\theta',P}(y)} \\cdot \\frac{1}{h_{\\theta,P}(y)} \\leq \\log \\varepsilon e^{h_{\\theta',P}(y)} + \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS(t - \\frac{1}{2}\\mu)\\|^2}{2}\\right)$$\n\nNow consider the integral $$I_1 = \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS^2(t - \\mu)\\|^2}{2}\\right)$$. By Claim C.17, as $$\\Omega_1$$ is defined for $$t$$ bounded by $$r$$, and $$\\mu - \\mu' = \\alpha e_1$$, we have\n\n$$I_1 \\leq \\exp(2\\alpha p_1 r + \\alpha^2 p_2) \\cdot 1 \\cdot I'_1$$, where $$I'_1 = \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS^2(t - \\mu')\\|^2}{2}\\right) \\leq e^{h_{\\theta',P}(y)}$$.\n\nSubstituting in the expression for $$h_{\\theta,P}$$, we get\n\n$$h_{\\theta,P}(y) \\leq \\log e^{h_{\\theta',P}(y)} \\cdot \\varepsilon + \\exp(2\\alpha p_1 r + \\alpha^2 p_2) \\cdot 1$$\n\nAs $$\\log(\\varepsilon + ex) \\leq \\varepsilon + x$$ for $$x \\geq 0$$, we have\n\nSetting $$\\alpha = O(\\frac{\\varepsilon}{p_1 r})$$, we get $$h_{\\theta,P}(y) \\leq h_{\\theta',P}(y) + \\varepsilon + 2\\alpha p_1 r + \\alpha^2 p_2$$.\n\n$$h_{\\theta,P}(y) \\leq h_{\\theta',P}(y) + 2\\varepsilon$$.\n\nThis shows that $$h_{\\theta,P}$$ changes by at most $$\\varepsilon$$ for the considered net. We need to defined the other end point for the net. By a similar argument to the positive end point, if $$\\theta_1 = -O\\left(\\frac{C\\kappa \\log(\\frac{1}{\\varepsilon})}{p_1}\\right)$$, the log-likelihood ratio changes by at most $$\\varepsilon$$ until $$\\theta_1 = -\\infty$$.\n\nAs we are only trying to cover $$\\theta$$ such that $$|\\theta_1| \\leq O(\\frac{\\sqrt{C\\kappa A}}{p_1})$$, this net has size $$O\\left(\\sqrt{C\\kappa A}\\right) = O\\left(\\frac{\\sqrt{C\\kappa A}}{p_1 r}\\right) = A \\cdot \\frac{p_1 \\alpha}{\\varepsilon}$$.\n\nCase 2: Net over $$h, y > 0$$. A similar argument to Case 1 works here as well.\n\nCase 3: Net over $$g, y_1 > 0$$. By Lemma C.11, if $$|\\theta - \\sqrt{\\theta^*}|PSc > R_1$$ for $$R_1 = O(A)$$, then\n\nNow consider $$\\theta$$ such that $$g_{\\theta,P} - g_{\\theta^*,P} < -A$$, and $$\\theta, \\theta'$$ such that $$\\theta - \\theta' = \\alpha e_1$$.\n\n$$|\\theta_1 - \\theta^*_1| \\leq R$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "As we are only consider $$\\theta'$$ such that $$h_{\\theta^*,P^*}(y) - A < h_{\\theta',P}(y)$$, for $$r^2 = O(d \\log \\kappa + A + \\log \\frac{1}{\\varepsilon}) = O(A)$$, we have\n\nSubstituting in $$h_{\\theta,P}(y)$$, we get $$I_2 \\leq \\varepsilon e^{h_{\\theta',P}(y)} \\cdot \\frac{1}{h_{\\theta,P}(y)} \\leq \\log \\varepsilon e^{h_{\\theta',P}(y)} + \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS(t - \\frac{1}{2}\\mu)\\|^2}{2}\\right)$$\n\nNow consider the integral $$I_1 = \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS^2(t - \\mu)\\|^2}{2}\\right)$$. By Claim C.17, as $$\\Omega_1$$ is defined for $$t$$ bounded by $$r$$, and $$\\mu - \\mu' = \\alpha e_1$$, we have\n\n$$I_1 \\leq \\exp(2\\alpha p_1 r + \\alpha^2 p_2) \\cdot 1 \\cdot I'_1$$, where $$I'_1 = \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS^2(t - \\mu')\\|^2}{2}\\right) \\leq e^{h_{\\theta',P}(y)}$$.\n\nSubstituting in the expression for $$h_{\\theta,P}$$, we get\n\n$$h_{\\theta,P}(y) \\leq \\log e^{h_{\\theta',P}(y)} \\cdot \\varepsilon + \\exp(2\\alpha p_1 r + \\alpha^2 p_2) \\cdot 1$$\n\nAs $$\\log(\\varepsilon + ex) \\leq \\varepsilon + x$$ for $$x \\geq 0$$, we have\n\nSetting $$\\alpha = O(\\frac{\\varepsilon}{p_1 r})$$, we get $$h_{\\theta,P}(y) \\leq h_{\\theta',P}(y) + \\varepsilon + 2\\alpha p_1 r + \\alpha^2 p_2$$.\n\n$$h_{\\theta,P}(y) \\leq h_{\\theta',P}(y) + 2\\varepsilon$$.\n\nThis shows that $$h_{\\theta,P}$$ changes by at most $$\\varepsilon$$ for the considered net. We need to defined the other end point for the net. By a similar argument to the positive end point, if $$\\theta_1 = -O\\left(\\frac{C\\kappa \\log(\\frac{1}{\\varepsilon})}{p_1}\\right)$$, the log-likelihood ratio changes by at most $$\\varepsilon$$ until $$\\theta_1 = -\\infty$$.\n\nAs we are only trying to cover $$\\theta$$ such that $$|\\theta_1| \\leq O(\\frac{\\sqrt{C\\kappa A}}{p_1})$$, this net has size $$O\\left(\\sqrt{C\\kappa A}\\right) = O\\left(\\frac{\\sqrt{C\\kappa A}}{p_1 r}\\right) = A \\cdot \\frac{p_1 \\alpha}{\\varepsilon}$$.\n\nCase 2: Net over $$h, y > 0$$. A similar argument to Case 1 works here as well.\n\nCase 3: Net over $$g, y_1 > 0$$. By Lemma C.11, if $$|\\theta - \\sqrt{\\theta^*}|PSc > R_1$$ for $$R_1 = O(A)$$, then\n\nNow consider $$\\theta$$ such that $$g_{\\theta,P} - g_{\\theta^*,P} < -A$$, and $$\\theta, \\theta'$$ such that $$\\theta - \\theta' = \\alpha e_1$$.\n\n$$|\\theta_1 - \\theta^*_1| \\leq R$$.", "md": "As we are only consider $$\\theta'$$ such that $$h_{\\theta^*,P^*}(y) - A < h_{\\theta',P}(y)$$, for $$r^2 = O(d \\log \\kappa + A + \\log \\frac{1}{\\varepsilon}) = O(A)$$, we have\n\nSubstituting in $$h_{\\theta,P}(y)$$, we get $$I_2 \\leq \\varepsilon e^{h_{\\theta',P}(y)} \\cdot \\frac{1}{h_{\\theta,P}(y)} \\leq \\log \\varepsilon e^{h_{\\theta',P}(y)} + \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS(t - \\frac{1}{2}\\mu)\\|^2}{2}\\right)$$\n\nNow consider the integral $$I_1 = \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS^2(t - \\mu)\\|^2}{2}\\right)$$. By Claim C.17, as $$\\Omega_1$$ is defined for $$t$$ bounded by $$r$$, and $$\\mu - \\mu' = \\alpha e_1$$, we have\n\n$$I_1 \\leq \\exp(2\\alpha p_1 r + \\alpha^2 p_2) \\cdot 1 \\cdot I'_1$$, where $$I'_1 = \\sum_{t \\leq 0, t \\in \\Omega_1} \\exp \\left(-\\frac{\\|PS^2(t - \\mu')\\|^2}{2}\\right) \\leq e^{h_{\\theta',P}(y)}$$.\n\nSubstituting in the expression for $$h_{\\theta,P}$$, we get\n\n$$h_{\\theta,P}(y) \\leq \\log e^{h_{\\theta',P}(y)} \\cdot \\varepsilon + \\exp(2\\alpha p_1 r + \\alpha^2 p_2) \\cdot 1$$\n\nAs $$\\log(\\varepsilon + ex) \\leq \\varepsilon + x$$ for $$x \\geq 0$$, we have\n\nSetting $$\\alpha = O(\\frac{\\varepsilon}{p_1 r})$$, we get $$h_{\\theta,P}(y) \\leq h_{\\theta',P}(y) + \\varepsilon + 2\\alpha p_1 r + \\alpha^2 p_2$$.\n\n$$h_{\\theta,P}(y) \\leq h_{\\theta',P}(y) + 2\\varepsilon$$.\n\nThis shows that $$h_{\\theta,P}$$ changes by at most $$\\varepsilon$$ for the considered net. We need to defined the other end point for the net. By a similar argument to the positive end point, if $$\\theta_1 = -O\\left(\\frac{C\\kappa \\log(\\frac{1}{\\varepsilon})}{p_1}\\right)$$, the log-likelihood ratio changes by at most $$\\varepsilon$$ until $$\\theta_1 = -\\infty$$.\n\nAs we are only trying to cover $$\\theta$$ such that $$|\\theta_1| \\leq O(\\frac{\\sqrt{C\\kappa A}}{p_1})$$, this net has size $$O\\left(\\sqrt{C\\kappa A}\\right) = O\\left(\\frac{\\sqrt{C\\kappa A}}{p_1 r}\\right) = A \\cdot \\frac{p_1 \\alpha}{\\varepsilon}$$.\n\nCase 2: Net over $$h, y > 0$$. A similar argument to Case 1 works here as well.\n\nCase 3: Net over $$g, y_1 > 0$$. By Lemma C.11, if $$|\\theta - \\sqrt{\\theta^*}|PSc > R_1$$ for $$R_1 = O(A)$$, then\n\nNow consider $$\\theta$$ such that $$g_{\\theta,P} - g_{\\theta^*,P} < -A$$, and $$\\theta, \\theta'$$ such that $$\\theta - \\theta' = \\alpha e_1$$.\n\n$$|\\theta_1 - \\theta^*_1| \\leq R$$."}]}, {"page": 38, "text": "The difference in g\u03b8 \u2212           g\u03b8\u2032 is\n         g\u03b8(y) \u2212      g\u03b8\u2032(y) = \u03b7T    Sc(\u03a3Sc)\u22121(\u03b8Sc \u2212            \u03b8\u2032Sc) \u2212     1     Sc \u2212   \u03b8Sc\u22252  \u03a3Sc + 1         Sc \u2212    \u03b8\u2032Sc\u22252 \u03a3Sc ,\n                                                                            2\u2225\u03b8\u2217                           2\u2225\u03b8\u2217\nThe second and third terms in the RHS can bounded by observing that\nand hence we get                     |2\u03b8\u2217   \u2212   \u03b8\u2032 \u2212   \u03b8| \u2264    2R1 = O(       \u221a  A), |\u03b8\u2032 \u2212     \u03b8| \u2264   \u03b1,\n                                \u22121 2\u2225\u03b8\u2217  Sc \u2212   \u03b8Sc\u22252  \u03a3Sc + 1    2\u2225\u03b8\u2217 Sc \u2212    \u03b8\u2032Sc\u22252 \u03a3Sc \u2264     O(   \u221a  A)\u03b1.\nNow, for the first term in the RHS, we have\n                              \u2225\u03b7Sc\u2225P \u2217                   \u21d2     \u2225\u03b7Sc\u2225     \u2264     1  B1         \u2264     1B1  \u221a  \u03ba    .\n                                        Sc \u2264    B1 =                           2                   2\n                                                                             \u03bb min(P \u2217)          \u03bbmax(P \u2217)\nThis further implies that\n                             \u03b7T                                         B1   \u221a  \u03ba    \u221ap1\u03b1 = poly(A)\u03b1.\n                                                           Sc) \u2264        1\n                               Sc(\u03a3Sc)\u22121(\u03b8Sc \u2212            \u03b8\u2032            2\n                                                                     \u03bbmax(P \u2217)\nSetting                                                  \u03b1 = O             \u03f5         ,\n                                                                      poly(A)\nwe get\n                                                       |g\u03b8(y) \u2212      g\u03b8\u2032(y)| \u2264      \u03f5\n                                                                                    d.\nAs we are covering a set of size R1 using a grid size of \u03b1, the size of this partition is\n                                                        O(R1  \u03b1 ) = poly(A)   \u03b5      .\nClaim C.16. In the setting of Lemma C.6, we have \u03bbmax(P                               ) \u2264   C\u03bbmax(P \u2217). Let p1 denote the first\ndiagonal element of P           .\n               \u221a  C\u03baA\nIf \u03b81 \u2265    \u0398(     p1    ), then the function h\u03b8,P is such that\n                                                        h\u03b8,P \u2212     h\u03b8\u2217,P < \u2212A.\nProof of Claim C.16. Recall that the function h\u03b8,P is defined as:\n                                                            1\n               h\u03b8,P (y) = log  1       t\u22640  exp     \u2212\u2225P    S2(t \u2212    \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212                  \u03b8Sc)\u22252/2        .\nConsider the term \u2225PS (t \u2212     2        \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212                   \u03b8Sc)\u2225. By the triangle inequality, we have\n                    1\n                    2\n               \u2225PS (t \u2212    1 \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212                  \u03b8Sc)\u2225                   \u22121\n                   \u2265\u2225P     2                                          Sc \u2212    \u03b8\u2217                  2 PSSc(ySc \u2212         \u03b8\u2217\n                          S (t \u2212    \u03b8S) + (PS)\u22121/2PSSc(\u03b8\u2032                       Sc)\u2225   \u2212   \u2225P  S                        Sc)\u2225,\n                           1                                                               \u221a\n                   \u2265\u2225P    S2(t \u2212    \u03b8S) + (PS)\u22121/2PSSc(\u03b8\u2032             Sc \u2212    \u03b8\u2217Sc)\u2225   \u2212      C\u03baA,\n                                                                      38", "md": "The difference in $$g_{\\theta} - g_{\\theta'}$$ is\n\n$$\ng_{\\theta}(y) - g_{\\theta'}(y) = \\eta^T Sc(\\Sigma_{Sc})^{-1}(\\theta_{Sc} - \\theta'_{Sc}) - \\frac{1}{2} \\left\\| \\theta_{Sc} \\right\\|_2 \\Sigma_{Sc} + \\frac{1}{2} \\left\\| \\theta'_{Sc} \\right\\|_2 \\Sigma_{Sc},\n$$\nThe second and third terms in the RHS can be bounded by observing that\n\nand hence we get $$|2\\theta^* - \\theta' - \\theta| \\leq 2R_1 = O(\\sqrt{A}), \\quad |\\theta' - \\theta| \\leq \\alpha,$$\n\n$$\n\\begin{align*}\n&\\frac{-1}{2} \\left\\| \\theta^*_{Sc} - \\theta_{Sc} \\right\\|_2 \\Sigma_{Sc} + \\frac{1}{2} \\left\\| \\theta^*_{Sc} - \\theta'_{Sc} \\right\\|_2 \\Sigma_{Sc} \\leq O(\\sqrt{A})\\alpha.\n\\end{align*}\n$$\nNow, for the first term in the RHS, we have\n\n$$\n\\left\\| \\eta Sc \\right\\|_{P^*} \\Rightarrow \\left\\| \\eta Sc \\right\\| \\leq \\frac{1}{B_1} \\leq \\frac{1}{B_1} \\sqrt{\\kappa}.\n$$\nThis further implies that\n\n$$\n\\eta^T Sc(\\Sigma_{Sc})^{-1}(\\theta_{Sc} - \\theta') \\leq \\frac{1}{\\lambda_{\\text{min}}(P^*)} B_1 \\sqrt{\\kappa} \\sqrt{p_1}\\alpha = \\text{poly}(A)\\alpha.\n$$\nSetting $$\\alpha = O\\left(\\frac{\\epsilon}{\\text{poly}(A)}\\right),$$\n\nwe get\n\n$$\n\\left| g_{\\theta}(y) - g_{\\theta'}(y) \\right| \\leq \\epsilon d.\n$$\nAs we are covering a set of size $$R_1$$ using a grid size of $$\\alpha$$, the size of this partition is\n\n$$\nO(R_1 \\alpha) = \\text{poly}(A) \\epsilon.\n$$\nClaim C.16. In the setting of Lemma C.6, we have $$\\lambda_{\\text{max}}(P) \\leq C\\lambda_{\\text{max}}(P^*)$$. Let $$p_1$$ denote the first diagonal element of $$P$$.\n\n$$\n\\sqrt{C\\kappa A} \\quad \\text{If} \\quad \\theta_1 \\geq \\Theta(p_1), \\quad \\text{then the function} \\quad h_{\\theta,P} \\quad \\text{is such that}\n$$\n$$\nh_{\\theta,P} - h_{\\theta^*,P} < -A.\n$$\nProof of Claim C.16. Recall that the function $$h_{\\theta,P}$$ is defined as:\n\n$$\nh_{\\theta,P}(y) = \\log \\left(1 + \\sum_{t \\leq 0} \\exp \\left( -\\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\|_2^2/2 \\right) \\right).\n$$\nConsider the term $$\\left\\| P_S(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\|$$. By the triangle inequality, we have\n\n$$\n\\begin{align*}\n&\\left\\| P_S(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\| \\\\\n&\\geq \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(\\theta'_{Sc}) \\right\\| - \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c \\right\\|, \\\\\n&\\geq \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(\\theta'_{Sc} - \\theta^*_{Sc}) \\right\\| - C\\kappa A.\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "The difference in $$g_{\\theta} - g_{\\theta'}$$ is\n\n$$\ng_{\\theta}(y) - g_{\\theta'}(y) = \\eta^T Sc(\\Sigma_{Sc})^{-1}(\\theta_{Sc} - \\theta'_{Sc}) - \\frac{1}{2} \\left\\| \\theta_{Sc} \\right\\|_2 \\Sigma_{Sc} + \\frac{1}{2} \\left\\| \\theta'_{Sc} \\right\\|_2 \\Sigma_{Sc},\n$$\nThe second and third terms in the RHS can be bounded by observing that\n\nand hence we get $$|2\\theta^* - \\theta' - \\theta| \\leq 2R_1 = O(\\sqrt{A}), \\quad |\\theta' - \\theta| \\leq \\alpha,$$\n\n$$\n\\begin{align*}\n&\\frac{-1}{2} \\left\\| \\theta^*_{Sc} - \\theta_{Sc} \\right\\|_2 \\Sigma_{Sc} + \\frac{1}{2} \\left\\| \\theta^*_{Sc} - \\theta'_{Sc} \\right\\|_2 \\Sigma_{Sc} \\leq O(\\sqrt{A})\\alpha.\n\\end{align*}\n$$\nNow, for the first term in the RHS, we have\n\n$$\n\\left\\| \\eta Sc \\right\\|_{P^*} \\Rightarrow \\left\\| \\eta Sc \\right\\| \\leq \\frac{1}{B_1} \\leq \\frac{1}{B_1} \\sqrt{\\kappa}.\n$$\nThis further implies that\n\n$$\n\\eta^T Sc(\\Sigma_{Sc})^{-1}(\\theta_{Sc} - \\theta') \\leq \\frac{1}{\\lambda_{\\text{min}}(P^*)} B_1 \\sqrt{\\kappa} \\sqrt{p_1}\\alpha = \\text{poly}(A)\\alpha.\n$$\nSetting $$\\alpha = O\\left(\\frac{\\epsilon}{\\text{poly}(A)}\\right),$$\n\nwe get\n\n$$\n\\left| g_{\\theta}(y) - g_{\\theta'}(y) \\right| \\leq \\epsilon d.\n$$\nAs we are covering a set of size $$R_1$$ using a grid size of $$\\alpha$$, the size of this partition is\n\n$$\nO(R_1 \\alpha) = \\text{poly}(A) \\epsilon.\n$$\nClaim C.16. In the setting of Lemma C.6, we have $$\\lambda_{\\text{max}}(P) \\leq C\\lambda_{\\text{max}}(P^*)$$. Let $$p_1$$ denote the first diagonal element of $$P$$.\n\n$$\n\\sqrt{C\\kappa A} \\quad \\text{If} \\quad \\theta_1 \\geq \\Theta(p_1), \\quad \\text{then the function} \\quad h_{\\theta,P} \\quad \\text{is such that}\n$$\n$$\nh_{\\theta,P} - h_{\\theta^*,P} < -A.\n$$\nProof of Claim C.16. Recall that the function $$h_{\\theta,P}$$ is defined as:\n\n$$\nh_{\\theta,P}(y) = \\log \\left(1 + \\sum_{t \\leq 0} \\exp \\left( -\\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\|_2^2/2 \\right) \\right).\n$$\nConsider the term $$\\left\\| P_S(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\|$$. By the triangle inequality, we have\n\n$$\n\\begin{align*}\n&\\left\\| P_S(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\| \\\\\n&\\geq \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(\\theta'_{Sc}) \\right\\| - \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c \\right\\|, \\\\\n&\\geq \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(\\theta'_{Sc} - \\theta^*_{Sc}) \\right\\| - C\\kappa A.\n\\end{align*}\n$$", "md": "The difference in $$g_{\\theta} - g_{\\theta'}$$ is\n\n$$\ng_{\\theta}(y) - g_{\\theta'}(y) = \\eta^T Sc(\\Sigma_{Sc})^{-1}(\\theta_{Sc} - \\theta'_{Sc}) - \\frac{1}{2} \\left\\| \\theta_{Sc} \\right\\|_2 \\Sigma_{Sc} + \\frac{1}{2} \\left\\| \\theta'_{Sc} \\right\\|_2 \\Sigma_{Sc},\n$$\nThe second and third terms in the RHS can be bounded by observing that\n\nand hence we get $$|2\\theta^* - \\theta' - \\theta| \\leq 2R_1 = O(\\sqrt{A}), \\quad |\\theta' - \\theta| \\leq \\alpha,$$\n\n$$\n\\begin{align*}\n&\\frac{-1}{2} \\left\\| \\theta^*_{Sc} - \\theta_{Sc} \\right\\|_2 \\Sigma_{Sc} + \\frac{1}{2} \\left\\| \\theta^*_{Sc} - \\theta'_{Sc} \\right\\|_2 \\Sigma_{Sc} \\leq O(\\sqrt{A})\\alpha.\n\\end{align*}\n$$\nNow, for the first term in the RHS, we have\n\n$$\n\\left\\| \\eta Sc \\right\\|_{P^*} \\Rightarrow \\left\\| \\eta Sc \\right\\| \\leq \\frac{1}{B_1} \\leq \\frac{1}{B_1} \\sqrt{\\kappa}.\n$$\nThis further implies that\n\n$$\n\\eta^T Sc(\\Sigma_{Sc})^{-1}(\\theta_{Sc} - \\theta') \\leq \\frac{1}{\\lambda_{\\text{min}}(P^*)} B_1 \\sqrt{\\kappa} \\sqrt{p_1}\\alpha = \\text{poly}(A)\\alpha.\n$$\nSetting $$\\alpha = O\\left(\\frac{\\epsilon}{\\text{poly}(A)}\\right),$$\n\nwe get\n\n$$\n\\left| g_{\\theta}(y) - g_{\\theta'}(y) \\right| \\leq \\epsilon d.\n$$\nAs we are covering a set of size $$R_1$$ using a grid size of $$\\alpha$$, the size of this partition is\n\n$$\nO(R_1 \\alpha) = \\text{poly}(A) \\epsilon.\n$$\nClaim C.16. In the setting of Lemma C.6, we have $$\\lambda_{\\text{max}}(P) \\leq C\\lambda_{\\text{max}}(P^*)$$. Let $$p_1$$ denote the first diagonal element of $$P$$.\n\n$$\n\\sqrt{C\\kappa A} \\quad \\text{If} \\quad \\theta_1 \\geq \\Theta(p_1), \\quad \\text{then the function} \\quad h_{\\theta,P} \\quad \\text{is such that}\n$$\n$$\nh_{\\theta,P} - h_{\\theta^*,P} < -A.\n$$\nProof of Claim C.16. Recall that the function $$h_{\\theta,P}$$ is defined as:\n\n$$\nh_{\\theta,P}(y) = \\log \\left(1 + \\sum_{t \\leq 0} \\exp \\left( -\\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\|_2^2/2 \\right) \\right).\n$$\nConsider the term $$\\left\\| P_S(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\|$$. By the triangle inequality, we have\n\n$$\n\\begin{align*}\n&\\left\\| P_S(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(y_{Sc} - \\theta_{Sc}) \\right\\| \\\\\n&\\geq \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(\\theta'_{Sc}) \\right\\| - \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c \\right\\|, \\\\\n&\\geq \\left\\| P_S^2(t - \\theta_S) + (P_S)^{-1/2}P_SS_c(\\theta'_{Sc} - \\theta^*_{Sc}) \\right\\| - C\\kappa A.\n\\end{align*}\n$$"}]}, {"page": 39, "text": "where the last inequality follows as\n          \u22121                                \u22121                        1                  C\u03bb\u2217 max        \u221a\n      \u2225P    2PSSc(ySc \u2212      \u03b8\u2217               2PSSc(\u03b7Sc)\u2225      \u2264  \u2225P  2                           B \u2264      C\u03baA.\n         S                    Sc)\u2225   = \u2225P  S                         Sc\u2225\u2225\u03b7Sc\u2225     \u2264       \u03bb\u2217min\n                                                                     1                     \u221a\n                                                                     2\nSimilarly, the function g\u03b8,P only considers \u03b8 such that \u2225P           Sc(\u03b8Sc \u2212   \u03b8\u2217Sc)\u2225  \u2264     C\u03baA ( otherwise, the\nlog-likelihood ratio is smaller than \u2212A by virtue of g\u03b8,P , irrespective of h\u03b8,P ). For these \u03b8, we have\n                                     1\n                                 \u2225P S2(t \u2212   \u03b8S) + (PS)\u22121/2PSSc(\u03b8\u2217       Sc \u2212   \u03b8Sc)\u2225\n                                           1                \u221a\nwhich gives        1                \u2265\u2225PS (t2\u2212     \u03b8S)\u2225   \u2212    C\u03baA,          1                  \u221a\n                   2                                                        2\n               \u2225P  S (t \u2212  \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212           \u03b8Sc)\u2225   \u2265\u2225PS (t \u2212     \u03b8S)\u2225  \u2212  2   C\u03baA.\n                      \u221a C\u03baA\nHence, if \u03b81 \u2265     O(  1 p1   ), then we have                                   \u221a\n                       2\n                   \u2225P  S (t \u2212  \u03b8S) + (PS)\u22121/2PSSc(ySc \u2212           \u03b8Sc)\u2225   \u2265\u2126(     C\u03baA) \u2200     t \u2264  0,\nand hence the Gaussian integral is at most (2\u03c0)|S|/2           PS \u221212  e\u2212\u2126(C\u03baA).\nBy Lemma C.10, we have h\u03b8\u2217,P \u2217           \u2265  \u22121 2 log|P \u2217S| \u2212  O(A), which gives\n    h\u03b8,P (y) \u2212   h\u03b8\u2217,P \u2217(y) < 1  2 log |P \u2217S|\n                                        |PS| \u2212    \u2126(C\u03baA)\n                              < d2 log    \u03bb\u2217max\n                                        \u03bbmin(P) \u2212      \u2126(C\u03baA) < O(A log \u03ba) \u2212           \u2126(C\u03baA) = \u2212\u2126(C\u03baA).\nThis gives a contiguous interval over \u03b81 for which \u03b3\u03b8,P < \u2212A.\nClaim C.17. In the setting of Lemma C.6, let \u00b5, \u00b5\u2032 be such that \u00b5 \u2212                \u00b5\u2032 = \u03b1e1\nThen, for all t such that                            1\n                                                     2\nand p1 := P11, we have          1                \u2225PS (t \u2212   \u00b5)\u2225   \u2264  r,      1\n                           \u2225P  S2(t \u2212  \u00b5)\u22252 \u2265    \u2212  2\u03b1p1r \u2212    \u03b12p2 1 + \u2225P  S2(t \u2212   \u00b5\u2032)\u22252.\n                                                     1\nProof of Claim C.17. Consider the1term \u2225PS (t \u2212      2       \u00b5)\u22252.\nAdding and subtracting \u2225PS (t \u2212  2       \u00b5\u2032)\u22252, we get\n              1                    1                     1                     1\n              2                    2                     2                     2\n          \u2225PS (t \u2212   \u00b5)\u22252 = \u2225PS (t \u2212       \u00b5)\u22252 \u2212   \u2225PS (t \u2212    \u00b5\u2032)\u22252 + \u2225PS (t \u2212      \u00b5\u2032)\u22252,\n                                   1                     1                    1\n                                   2                     2                    2\n                            = \u27e8PS (2t \u2212     \u00b5\u2032 \u2212  \u00b5), PS (\u00b5\u2032 \u2212    \u00b5)\u27e9  + \u2225PS (t \u2212     \u00b5\u2032)\u22252,\n                                   1                               1                    1\n                                   2                               2                    2\n                            = \u27e8PS (2t \u2212     2\u00b5 \u2212   (\u00b5\u2032 \u2212  \u00b5)), PS (\u00b5\u2032 \u2212     \u00b5)\u27e9  + \u2225PS (t \u2212    \u00b5\u2032)\u22252,\n                                     1             1                    1                      1\n                                     2             2                    2                      2\nAs \u00b5 \u2212    \u00b5\u2032 = \u03b1e1, we have = 2\u27e8PS (t \u2212     \u00b5), PS (\u00b5\u2032 \u2212    \u00b5)\u27e9  \u2212  \u2225PS (\u00b5 \u2212    \u00b5\u2032)\u22252 + \u2225PS (t \u2212      \u00b5\u2032)\u22252.\n                                                 1\n                                             \u2225P S2(\u00b5 \u2212   \u00b5\u2032)\u22252 = \u03b12p2    1.\nBy the Cauchy-Schwartz inequality, and since \u00b5 \u2212            \u00b5\u2032 = \u03b1e1, the inner product can be lower bounded\nas                  1              1                        1               1\n                    2              2                        2               2\n               2\u27e8PS (t \u2212    \u00b5), PS (\u00b5\u2032 \u2212    \u00b5)\u27e9  \u2265  \u22122\u2225PS (t \u2212      \u00b5)\u2225\u2225PS (\u00b5\u2032 \u2212     \u00b5)\u2225   \u2265  \u22122\u03b1p1r.\nSubstituting, we get\n                                1                                            1\nThis completes the proof.   \u2225P S2(t \u2212  \u00b5)\u22252 \u2265    \u22122\u03b1p1r \u2212      \u03b12p2 1 + \u2225P  S2(t \u2212  \u00b5\u2032)\u22252.\n                                                          39", "md": "where the last inequality follows as\n\n$$\n\\begin{align*}\n&\\|P^{-1}2PSSc(ySc - \\theta^*)2PSSc(\\eta Sc)\\| \\le \\|P^{-1}2S_{Sc}\\| = \\|PS_{Sc}\\|\\|\\eta Sc\\| \\le \\lambda^*_{min} \\\\\n&\\text{Similarly, the function } g_{\\theta,P} \\text{ only considers } \\theta \\text{ such that } \\|PS_{Sc}(\\theta Sc - \\theta^*Sc)\\| \\le C\\kappa A \\text{ (otherwise, the log-likelihood ratio is smaller than } -A \\text{ by virtue of } g_{\\theta,P} \\text{, irrespective of } h_{\\theta,P} \\text{). For these } \\theta \\text{, we have} \\\\\n&\\|PS^2(t - \\theta S) + (PS)^{-1/2}PSSc(\\theta^*Sc - \\theta Sc)\\| \\ge 1 \\ge \\|PS(t^2 - \\theta S)\\| - C\\kappa A, \\\\\n&\\|PS(t - \\theta S) + (PS)^{-1/2}PSSc(ySc - \\theta Sc)\\| \\ge \\|PS(t - \\theta S)\\| - 2C\\kappa A.\n\\end{align*}\n$$\nHence, if $\\theta_1 \\ge O\\left(\\frac{1}{\\sqrt{p_1}}\\right)$, then we have\n\n$$\n\\|PS(t - \\theta S) + (PS)^{-1/2}PSSc(ySc - \\theta Sc)\\| \\ge \\Omega(C\\kappa A) \\quad \\forall t \\le 0,\n$$\nand hence the Gaussian integral is at most $(2\\pi)|S|/2 \\cdot (PS)^{-1/2}e^{-\\Omega(C\\kappa A)}$.\n\nBy Lemma C.10, we have $h_{\\theta^*,P^*} \\ge -\\frac{1}{2}\\log|P^*S| - O(A)$, which gives\n\n$$\nh_{\\theta,P}(y) - h_{\\theta^*,P^*}(y) < \\frac{1}{2}\\log|P^*S| - |PS| - \\Omega(C\\kappa A) < d^2\\log\\frac{\\lambda^*_{max}}{\\lambda_{min}(P)} - \\Omega(C\\kappa A) < O(A\\log\\kappa) - \\Omega(C\\kappa A) = -\\Omega(C\\kappa A).\n$$\nThis gives a contiguous interval over $\\theta_1$ for which $\\gamma_{\\theta,P} < -A$.\n\nClaim C.17. In the setting of Lemma C.6, let $\\mu, \\mu'$ be such that $\\mu - \\mu' = \\alpha e_1$. Then, for all $t$ such that $\\frac{1}{2}$ and $p_1 := P_{11}$, we have\n\n$$\n\\|PS(t - \\mu)\\| \\le r, \\quad \\|PS^2(t - \\mu)\\|^2 \\ge -2\\alpha p_1 r - \\alpha^2 p_2 1 + \\|PS^2(t - \\mu')\\|^2.\n$$\nProof of Claim C.17. Consider the term $\\|PS(t - \\mu)\\|^2$. Adding and subtracting $\\|PS(t - \\mu')\\|^2$, we get\n\n$$\n\\begin{align*}\n&\\|PS(t - \\mu)\\|^2 = \\|PS(t - \\mu)\\|^2 - \\|PS(t - \\mu')\\|^2 + \\|PS(t - \\mu')\\|^2, \\\\\n&= \\langle PS(2t - \\mu' - \\mu), PS(\\mu' - \\mu)\\rangle + \\|PS(t - \\mu')\\|^2, \\\\\n&= \\langle PS(2t - 2\\mu - (\\mu' - \\mu)), PS(\\mu' - \\mu)\\rangle + \\|PS(t - \\mu')\\|^2, \\\\\n&= 2\\langle PS(t - \\mu), PS(\\mu' - \\mu)\\rangle - \\|PS(\\mu - \\mu')\\|^2 + \\|PS(t - \\mu')\\|^2.\n\\end{align*}\n$$\nAs $\\mu - \\mu' = \\alpha e_1$, we have $\\|PS^2(\\mu - \\mu')\\|^2 = \\alpha^2 p_2 1$. By the Cauchy-Schwartz inequality, and since $\\mu - \\mu' = \\alpha e_1$, the inner product can be lower bounded as\n\n$$\n2\\langle PS(t - \\mu), PS(\\mu' - \\mu)\\rangle \\ge -2\\|PS(t - \\mu)\\|\\|PS(\\mu' - \\mu)\\| \\ge -2\\alpha p_1 r.\n$$\nSubstituting, we get\n\n$$\n\\|PS^2(t - \\mu)\\|^2 \\ge -2\\alpha p_1 r - \\alpha^2 p_2 1 + \\|PS^2(t - \\mu')\\|^2.\n$$\nThis completes the proof.", "images": [], "items": [{"type": "text", "value": "where the last inequality follows as\n\n$$\n\\begin{align*}\n&\\|P^{-1}2PSSc(ySc - \\theta^*)2PSSc(\\eta Sc)\\| \\le \\|P^{-1}2S_{Sc}\\| = \\|PS_{Sc}\\|\\|\\eta Sc\\| \\le \\lambda^*_{min} \\\\\n&\\text{Similarly, the function } g_{\\theta,P} \\text{ only considers } \\theta \\text{ such that } \\|PS_{Sc}(\\theta Sc - \\theta^*Sc)\\| \\le C\\kappa A \\text{ (otherwise, the log-likelihood ratio is smaller than } -A \\text{ by virtue of } g_{\\theta,P} \\text{, irrespective of } h_{\\theta,P} \\text{). For these } \\theta \\text{, we have} \\\\\n&\\|PS^2(t - \\theta S) + (PS)^{-1/2}PSSc(\\theta^*Sc - \\theta Sc)\\| \\ge 1 \\ge \\|PS(t^2 - \\theta S)\\| - C\\kappa A, \\\\\n&\\|PS(t - \\theta S) + (PS)^{-1/2}PSSc(ySc - \\theta Sc)\\| \\ge \\|PS(t - \\theta S)\\| - 2C\\kappa A.\n\\end{align*}\n$$\nHence, if $\\theta_1 \\ge O\\left(\\frac{1}{\\sqrt{p_1}}\\right)$, then we have\n\n$$\n\\|PS(t - \\theta S) + (PS)^{-1/2}PSSc(ySc - \\theta Sc)\\| \\ge \\Omega(C\\kappa A) \\quad \\forall t \\le 0,\n$$\nand hence the Gaussian integral is at most $(2\\pi)|S|/2 \\cdot (PS)^{-1/2}e^{-\\Omega(C\\kappa A)}$.\n\nBy Lemma C.10, we have $h_{\\theta^*,P^*} \\ge -\\frac{1}{2}\\log|P^*S| - O(A)$, which gives\n\n$$\nh_{\\theta,P}(y) - h_{\\theta^*,P^*}(y) < \\frac{1}{2}\\log|P^*S| - |PS| - \\Omega(C\\kappa A) < d^2\\log\\frac{\\lambda^*_{max}}{\\lambda_{min}(P)} - \\Omega(C\\kappa A) < O(A\\log\\kappa) - \\Omega(C\\kappa A) = -\\Omega(C\\kappa A).\n$$\nThis gives a contiguous interval over $\\theta_1$ for which $\\gamma_{\\theta,P} < -A$.\n\nClaim C.17. In the setting of Lemma C.6, let $\\mu, \\mu'$ be such that $\\mu - \\mu' = \\alpha e_1$. Then, for all $t$ such that $\\frac{1}{2}$ and $p_1 := P_{11}$, we have\n\n$$\n\\|PS(t - \\mu)\\| \\le r, \\quad \\|PS^2(t - \\mu)\\|^2 \\ge -2\\alpha p_1 r - \\alpha^2 p_2 1 + \\|PS^2(t - \\mu')\\|^2.\n$$\nProof of Claim C.17. Consider the term $\\|PS(t - \\mu)\\|^2$. Adding and subtracting $\\|PS(t - \\mu')\\|^2$, we get\n\n$$\n\\begin{align*}\n&\\|PS(t - \\mu)\\|^2 = \\|PS(t - \\mu)\\|^2 - \\|PS(t - \\mu')\\|^2 + \\|PS(t - \\mu')\\|^2, \\\\\n&= \\langle PS(2t - \\mu' - \\mu), PS(\\mu' - \\mu)\\rangle + \\|PS(t - \\mu')\\|^2, \\\\\n&= \\langle PS(2t - 2\\mu - (\\mu' - \\mu)), PS(\\mu' - \\mu)\\rangle + \\|PS(t - \\mu')\\|^2, \\\\\n&= 2\\langle PS(t - \\mu), PS(\\mu' - \\mu)\\rangle - \\|PS(\\mu - \\mu')\\|^2 + \\|PS(t - \\mu')\\|^2.\n\\end{align*}\n$$\nAs $\\mu - \\mu' = \\alpha e_1$, we have $\\|PS^2(\\mu - \\mu')\\|^2 = \\alpha^2 p_2 1$. By the Cauchy-Schwartz inequality, and since $\\mu - \\mu' = \\alpha e_1$, the inner product can be lower bounded as\n\n$$\n2\\langle PS(t - \\mu), PS(\\mu' - \\mu)\\rangle \\ge -2\\|PS(t - \\mu)\\|\\|PS(\\mu' - \\mu)\\| \\ge -2\\alpha p_1 r.\n$$\nSubstituting, we get\n\n$$\n\\|PS^2(t - \\mu)\\|^2 \\ge -2\\alpha p_1 r - \\alpha^2 p_2 1 + \\|PS^2(t - \\mu')\\|^2.\n$$\nThis completes the proof.", "md": "where the last inequality follows as\n\n$$\n\\begin{align*}\n&\\|P^{-1}2PSSc(ySc - \\theta^*)2PSSc(\\eta Sc)\\| \\le \\|P^{-1}2S_{Sc}\\| = \\|PS_{Sc}\\|\\|\\eta Sc\\| \\le \\lambda^*_{min} \\\\\n&\\text{Similarly, the function } g_{\\theta,P} \\text{ only considers } \\theta \\text{ such that } \\|PS_{Sc}(\\theta Sc - \\theta^*Sc)\\| \\le C\\kappa A \\text{ (otherwise, the log-likelihood ratio is smaller than } -A \\text{ by virtue of } g_{\\theta,P} \\text{, irrespective of } h_{\\theta,P} \\text{). For these } \\theta \\text{, we have} \\\\\n&\\|PS^2(t - \\theta S) + (PS)^{-1/2}PSSc(\\theta^*Sc - \\theta Sc)\\| \\ge 1 \\ge \\|PS(t^2 - \\theta S)\\| - C\\kappa A, \\\\\n&\\|PS(t - \\theta S) + (PS)^{-1/2}PSSc(ySc - \\theta Sc)\\| \\ge \\|PS(t - \\theta S)\\| - 2C\\kappa A.\n\\end{align*}\n$$\nHence, if $\\theta_1 \\ge O\\left(\\frac{1}{\\sqrt{p_1}}\\right)$, then we have\n\n$$\n\\|PS(t - \\theta S) + (PS)^{-1/2}PSSc(ySc - \\theta Sc)\\| \\ge \\Omega(C\\kappa A) \\quad \\forall t \\le 0,\n$$\nand hence the Gaussian integral is at most $(2\\pi)|S|/2 \\cdot (PS)^{-1/2}e^{-\\Omega(C\\kappa A)}$.\n\nBy Lemma C.10, we have $h_{\\theta^*,P^*} \\ge -\\frac{1}{2}\\log|P^*S| - O(A)$, which gives\n\n$$\nh_{\\theta,P}(y) - h_{\\theta^*,P^*}(y) < \\frac{1}{2}\\log|P^*S| - |PS| - \\Omega(C\\kappa A) < d^2\\log\\frac{\\lambda^*_{max}}{\\lambda_{min}(P)} - \\Omega(C\\kappa A) < O(A\\log\\kappa) - \\Omega(C\\kappa A) = -\\Omega(C\\kappa A).\n$$\nThis gives a contiguous interval over $\\theta_1$ for which $\\gamma_{\\theta,P} < -A$.\n\nClaim C.17. In the setting of Lemma C.6, let $\\mu, \\mu'$ be such that $\\mu - \\mu' = \\alpha e_1$. Then, for all $t$ such that $\\frac{1}{2}$ and $p_1 := P_{11}$, we have\n\n$$\n\\|PS(t - \\mu)\\| \\le r, \\quad \\|PS^2(t - \\mu)\\|^2 \\ge -2\\alpha p_1 r - \\alpha^2 p_2 1 + \\|PS^2(t - \\mu')\\|^2.\n$$\nProof of Claim C.17. Consider the term $\\|PS(t - \\mu)\\|^2$. Adding and subtracting $\\|PS(t - \\mu')\\|^2$, we get\n\n$$\n\\begin{align*}\n&\\|PS(t - \\mu)\\|^2 = \\|PS(t - \\mu)\\|^2 - \\|PS(t - \\mu')\\|^2 + \\|PS(t - \\mu')\\|^2, \\\\\n&= \\langle PS(2t - \\mu' - \\mu), PS(\\mu' - \\mu)\\rangle + \\|PS(t - \\mu')\\|^2, \\\\\n&= \\langle PS(2t - 2\\mu - (\\mu' - \\mu)), PS(\\mu' - \\mu)\\rangle + \\|PS(t - \\mu')\\|^2, \\\\\n&= 2\\langle PS(t - \\mu), PS(\\mu' - \\mu)\\rangle - \\|PS(\\mu - \\mu')\\|^2 + \\|PS(t - \\mu')\\|^2.\n\\end{align*}\n$$\nAs $\\mu - \\mu' = \\alpha e_1$, we have $\\|PS^2(\\mu - \\mu')\\|^2 = \\alpha^2 p_2 1$. By the Cauchy-Schwartz inequality, and since $\\mu - \\mu' = \\alpha e_1$, the inner product can be lower bounded as\n\n$$\n2\\langle PS(t - \\mu), PS(\\mu' - \\mu)\\rangle \\ge -2\\|PS(t - \\mu)\\|\\|PS(\\mu' - \\mu)\\| \\ge -2\\alpha p_1 r.\n$$\nSubstituting, we get\n\n$$\n\\|PS^2(t - \\mu)\\|^2 \\ge -2\\alpha p_1 r - \\alpha^2 p_2 1 + \\|PS^2(t - \\mu')\\|^2.\n$$\nThis completes the proof."}]}, {"page": 40, "text": " Lemma C.18. Following Definition C.12 let \u2126\u03c1 be the set of precision matrices with condition\n number \u03ba satisfying \u03bbmax(P               ) \u2208   [ \u03c1                 \u2126\u03c1,\u03b2 be the quantized net with quantization level \u03b2.\nFor any P \u2208         \u2126\u03c1, let   P \u2208                2, \u03c1], and let\n                                      \u2126\u03c1,\u03b2 be its element-wise rounding down. Then, for any \u00b5 \u2208                             Rd, we have\n                                            dT V (N     (\u00b5; P   ), N  (\u00b5;  P  )) \u2264    O   d2  \u03b2\u03ba    .                                     (36)\n Proof of Lemma C.18. Let \u03a3 = P \u22121 and                       \u03a3 =     P \u22121.\n By Theorem 1.1 in [16], the TV between two Gaussians with the same mean is\n                                                                            \uf8eb         \uf8f1                  \uf8fc  \uf8f6\n                                dT V (N     (\u00b5; \u03a3), N     (\u00b5;  \u03a3)) = \u0398      \uf8edmin      \uf8f2               \u03be2 \uf8fd  \uf8f8,\n where \u03bei are the eigenvalues of              \u03a3\u22121\u03a3 \u2212        Id.                       \uf8f31,         i    i \uf8fe\nWe can convert the bound on the eigenvalues to the Frobenius norm of                                   \u03a3\u22121\u03a3 \u2212        Id:\n                                                         i   \u03be2i \u2264   \u2225 \u03a3\u22121\u03a3 \u2212        Id\u2225F .\n Recall that     P is the rounding down per entry of P                   . Hence,\n                              \u03a3\u22121\u03a3 \u2212       Id = (\u03a3\u22121 \u2212          [\u03bdij])\u03a3 \u2212      Id, where 0 \u2264         \u03bdij < \u03b2\u03c1,\n Taking the Frobenius norm, we get              = \u2212\u03bd\u03a3.\n                             \u2225 \u03a3\u22121\u03a3 \u2212       Id\u2225F = \u2225\u03bd\u03a3\u2225F ,                                               d\n                                                     \u2264  (d\u03b2\u03c1)(d\u03c1max(\u03a3)) = (d\u03b2\u03c1)                    \u03c1min(P     )    ,\n                                                     \u2264  (d\u03b2\u03c1)            d\u03ba          \u2264   2d2\u03b2\u03ba.\n                                                                    \u03c1max(P      )\n where the first inequality follows as each element of \u03bd is at most \u03b2\u03c1, the second inequality follows as\n P has condition number \u03ba, and the third follows as P \u2208                           \u2126\u03c1 =\u21d2         \u03c1max(P      ) \u2265    \u03c1\n                                                                                                                   2.\n This completes the proof.\n Lemma C.7. Let x1, . . . , xn be fixed, and yi = \u03d5(W \u2217xi + \u03b7i) for \u03b7i \u223c                                N   (0, \u03a3\u2217), and W \u2217         \u2208  Rd\u00d7k\n with \u03a3\u2217     \u2208   Rd\u00d7d satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant\n C > 0,                                             n = C \u00b7 (d2 + kd)  \u03b52        log kd\u03ba\u03b5\n samples suffice to guarantee that with high probability, the MLE                             W   , \u03a3 satisfies\n                                                    d   ( W  , \u03a3), (W \u2217, \u03a3\u2217)          \u2264   \u03b5.\n Proof of Lemma C.7. For any W \u2208                     Rd\u00d7k, \u03a3 \u2208        Rd\u00d7d and a sample (xi, yi), let pi,W,\u03a3(y|xi) be the\n conditional distribution of y = \u03d5(W                x + \u03b7), and let \u03b3i,W,\u03a3 be the log-likelihood ratio between (W, \u03a3)\n and (W \u2217, \u03a3\u2217) on this sample:                 \u03b3i,W,\u03a3(y) := log pi,W,\u03a3(y | xi)\n Then                                                                    pi,W \u2217,\u03a3\u2217(y | xi).\n                               Ey [\u03b3i,W,\u03a3(y)] = \u2212KL(pi,W \u2217,\u03a3\u2217(y | xi)\u2225pi,W,\u03a3(y | xi)).\n                                                                       40", "md": "Lemma C.18. Following Definition C.12 let $$\\Omega_{\\rho}$$ be the set of precision matrices with condition number \u03ba satisfying $$\\lambda_{\\text{max}}(P) \\in [\\rho, \\rho\\Omega_{\\rho},\\beta$$ be the quantized net with quantization level \u03b2.\nFor any $$P \\in \\Omega_{\\rho}$$, let $$P \\in [\\rho/2, \\rho]$$, and let $$\\Omega_{\\rho,\\beta}$$ be its element-wise rounding down. Then, for any $$\\mu \\in \\mathbb{R}^d$$, we have\n$$\nd_{TV}\\left(N(\\mu; P), N(\\mu; P)\\right) \\leq O(d^2 \\beta \\kappa). \\quad (36)\n$$\n\nProof of Lemma C.18. Let $$\\Sigma = P^{-1}$$ and $$\\Sigma = P^{-1}$$.\nBy Theorem 1.1 in [16], the TV between two Gaussians with the same mean is\n$$\nd_{TV}\\left(N(\\mu; \\Sigma), N(\\mu; \\Sigma)\\right) = \\Theta\\left(\\min_i \\left\\{\\xi_i^2\\right\\}\\right),\n$$\nwhere $$\\xi_i$$ are the eigenvalues of $$\\Sigma^{-1}\\Sigma - \\text{Id}$$.\n\nWe can convert the bound on the eigenvalues to the Frobenius norm of $$\\Sigma^{-1}\\Sigma - \\text{Id}$$:\n$$\n\\begin{aligned}\n\\xi_i^2 &\\leq \\left\\| \\Sigma^{-1}\\Sigma - \\text{Id} \\right\\|_F \\\\\n&= \\left\\| (\\Sigma^{-1} - [\\nu_{ij}])\\Sigma - \\text{Id} \\right\\|_F, \\text{ where } 0 \\leq \\nu_{ij} < \\beta\\rho.\n\\end{aligned}\n$$\nTaking the Frobenius norm, we get\n$$\n\\left\\| \\Sigma^{-1}\\Sigma - \\text{Id} \\right\\|_F = \\left\\| \\nu\\Sigma \\right\\|_F \\leq (d\\beta\\rho)(d\\rho\\text{max}(\\Sigma)) = (d\\beta\\rho) \\rho\\text{min}(P) \\leq (d\\beta) d\\kappa \\leq 2d^2\\beta\\kappa,\n$$\nwhere the first inequality follows as each element of $$\\nu$$ is at most $$\\beta\\rho$$, the second inequality follows as $$P$$ has condition number $$\\kappa$$, and the third follows as $$P \\in \\Omega_{\\rho} \\Rightarrow \\rho\\text{max}(P) \\geq \\rho/2$$.\nThis completes the proof.\n\nLemma C.7. Let $$x_1, \\ldots, x_n$$ be fixed, and $$y_i = \\phi(W^*x_i + \\eta_i)$$ for $$\\eta_i \\sim N(0, \\Sigma^*)$$, and $$W^* \\in \\mathbb{R}^{d \\times k}$$ with $$\\Sigma^* \\in \\mathbb{R}^{d \\times d}$$ satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant $$C > 0$$,\n$$\nn = C \\cdot (d^2 + kd) \\frac{\\varepsilon^2}{\\log kd\\kappa}\n$$\nsamples suffice to guarantee that with high probability, the MLE $$W, \\Sigma$$ satisfies\n$$\nd\\left(W, \\Sigma), (W^*, \\Sigma^*)\\right) \\leq \\varepsilon.\n$$\n\nProof of Lemma C.7. For any $$W \\in \\mathbb{R}^{d \\times k}$$, $$\\Sigma \\in \\mathbb{R}^{d \\times d}$$ and a sample $$(x_i, y_i)$$, let $$p_{i,W,\\Sigma}(y|x_i)$$ be the conditional distribution of $$y = \\phi(Wx + \\eta)$$, and let $$\\gamma_{i,W,\\Sigma}$$ be the log-likelihood ratio between $$(W, \\Sigma)$$ and $$(W^*, \\Sigma^*)$$ on this sample:\n$$\n\\gamma_{i,W,\\Sigma}(y) := \\log p_{i,W,\\Sigma}(y | x_i).\n$$\nThen\n$$\nE_y[\\gamma_{i,W,\\Sigma}(y)] = -\\text{KL}(p_{i,W^*,\\Sigma^*}(y | x_i) \\| p_{i,W,\\Sigma}(y | x_i)).\n$$", "images": [], "items": [{"type": "text", "value": "Lemma C.18. Following Definition C.12 let $$\\Omega_{\\rho}$$ be the set of precision matrices with condition number \u03ba satisfying $$\\lambda_{\\text{max}}(P) \\in [\\rho, \\rho\\Omega_{\\rho},\\beta$$ be the quantized net with quantization level \u03b2.\nFor any $$P \\in \\Omega_{\\rho}$$, let $$P \\in [\\rho/2, \\rho]$$, and let $$\\Omega_{\\rho,\\beta}$$ be its element-wise rounding down. Then, for any $$\\mu \\in \\mathbb{R}^d$$, we have\n$$\nd_{TV}\\left(N(\\mu; P), N(\\mu; P)\\right) \\leq O(d^2 \\beta \\kappa). \\quad (36)\n$$\n\nProof of Lemma C.18. Let $$\\Sigma = P^{-1}$$ and $$\\Sigma = P^{-1}$$.\nBy Theorem 1.1 in [16], the TV between two Gaussians with the same mean is\n$$\nd_{TV}\\left(N(\\mu; \\Sigma), N(\\mu; \\Sigma)\\right) = \\Theta\\left(\\min_i \\left\\{\\xi_i^2\\right\\}\\right),\n$$\nwhere $$\\xi_i$$ are the eigenvalues of $$\\Sigma^{-1}\\Sigma - \\text{Id}$$.\n\nWe can convert the bound on the eigenvalues to the Frobenius norm of $$\\Sigma^{-1}\\Sigma - \\text{Id}$$:\n$$\n\\begin{aligned}\n\\xi_i^2 &\\leq \\left\\| \\Sigma^{-1}\\Sigma - \\text{Id} \\right\\|_F \\\\\n&= \\left\\| (\\Sigma^{-1} - [\\nu_{ij}])\\Sigma - \\text{Id} \\right\\|_F, \\text{ where } 0 \\leq \\nu_{ij} < \\beta\\rho.\n\\end{aligned}\n$$\nTaking the Frobenius norm, we get\n$$\n\\left\\| \\Sigma^{-1}\\Sigma - \\text{Id} \\right\\|_F = \\left\\| \\nu\\Sigma \\right\\|_F \\leq (d\\beta\\rho)(d\\rho\\text{max}(\\Sigma)) = (d\\beta\\rho) \\rho\\text{min}(P) \\leq (d\\beta) d\\kappa \\leq 2d^2\\beta\\kappa,\n$$\nwhere the first inequality follows as each element of $$\\nu$$ is at most $$\\beta\\rho$$, the second inequality follows as $$P$$ has condition number $$\\kappa$$, and the third follows as $$P \\in \\Omega_{\\rho} \\Rightarrow \\rho\\text{max}(P) \\geq \\rho/2$$.\nThis completes the proof.\n\nLemma C.7. Let $$x_1, \\ldots, x_n$$ be fixed, and $$y_i = \\phi(W^*x_i + \\eta_i)$$ for $$\\eta_i \\sim N(0, \\Sigma^*)$$, and $$W^* \\in \\mathbb{R}^{d \\times k}$$ with $$\\Sigma^* \\in \\mathbb{R}^{d \\times d}$$ satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant $$C > 0$$,\n$$\nn = C \\cdot (d^2 + kd) \\frac{\\varepsilon^2}{\\log kd\\kappa}\n$$\nsamples suffice to guarantee that with high probability, the MLE $$W, \\Sigma$$ satisfies\n$$\nd\\left(W, \\Sigma), (W^*, \\Sigma^*)\\right) \\leq \\varepsilon.\n$$\n\nProof of Lemma C.7. For any $$W \\in \\mathbb{R}^{d \\times k}$$, $$\\Sigma \\in \\mathbb{R}^{d \\times d}$$ and a sample $$(x_i, y_i)$$, let $$p_{i,W,\\Sigma}(y|x_i)$$ be the conditional distribution of $$y = \\phi(Wx + \\eta)$$, and let $$\\gamma_{i,W,\\Sigma}$$ be the log-likelihood ratio between $$(W, \\Sigma)$$ and $$(W^*, \\Sigma^*)$$ on this sample:\n$$\n\\gamma_{i,W,\\Sigma}(y) := \\log p_{i,W,\\Sigma}(y | x_i).\n$$\nThen\n$$\nE_y[\\gamma_{i,W,\\Sigma}(y)] = -\\text{KL}(p_{i,W^*,\\Sigma^*}(y | x_i) \\| p_{i,W,\\Sigma}(y | x_i)).\n$$", "md": "Lemma C.18. Following Definition C.12 let $$\\Omega_{\\rho}$$ be the set of precision matrices with condition number \u03ba satisfying $$\\lambda_{\\text{max}}(P) \\in [\\rho, \\rho\\Omega_{\\rho},\\beta$$ be the quantized net with quantization level \u03b2.\nFor any $$P \\in \\Omega_{\\rho}$$, let $$P \\in [\\rho/2, \\rho]$$, and let $$\\Omega_{\\rho,\\beta}$$ be its element-wise rounding down. Then, for any $$\\mu \\in \\mathbb{R}^d$$, we have\n$$\nd_{TV}\\left(N(\\mu; P), N(\\mu; P)\\right) \\leq O(d^2 \\beta \\kappa). \\quad (36)\n$$\n\nProof of Lemma C.18. Let $$\\Sigma = P^{-1}$$ and $$\\Sigma = P^{-1}$$.\nBy Theorem 1.1 in [16], the TV between two Gaussians with the same mean is\n$$\nd_{TV}\\left(N(\\mu; \\Sigma), N(\\mu; \\Sigma)\\right) = \\Theta\\left(\\min_i \\left\\{\\xi_i^2\\right\\}\\right),\n$$\nwhere $$\\xi_i$$ are the eigenvalues of $$\\Sigma^{-1}\\Sigma - \\text{Id}$$.\n\nWe can convert the bound on the eigenvalues to the Frobenius norm of $$\\Sigma^{-1}\\Sigma - \\text{Id}$$:\n$$\n\\begin{aligned}\n\\xi_i^2 &\\leq \\left\\| \\Sigma^{-1}\\Sigma - \\text{Id} \\right\\|_F \\\\\n&= \\left\\| (\\Sigma^{-1} - [\\nu_{ij}])\\Sigma - \\text{Id} \\right\\|_F, \\text{ where } 0 \\leq \\nu_{ij} < \\beta\\rho.\n\\end{aligned}\n$$\nTaking the Frobenius norm, we get\n$$\n\\left\\| \\Sigma^{-1}\\Sigma - \\text{Id} \\right\\|_F = \\left\\| \\nu\\Sigma \\right\\|_F \\leq (d\\beta\\rho)(d\\rho\\text{max}(\\Sigma)) = (d\\beta\\rho) \\rho\\text{min}(P) \\leq (d\\beta) d\\kappa \\leq 2d^2\\beta\\kappa,\n$$\nwhere the first inequality follows as each element of $$\\nu$$ is at most $$\\beta\\rho$$, the second inequality follows as $$P$$ has condition number $$\\kappa$$, and the third follows as $$P \\in \\Omega_{\\rho} \\Rightarrow \\rho\\text{max}(P) \\geq \\rho/2$$.\nThis completes the proof.\n\nLemma C.7. Let $$x_1, \\ldots, x_n$$ be fixed, and $$y_i = \\phi(W^*x_i + \\eta_i)$$ for $$\\eta_i \\sim N(0, \\Sigma^*)$$, and $$W^* \\in \\mathbb{R}^{d \\times k}$$ with $$\\Sigma^* \\in \\mathbb{R}^{d \\times d}$$ satisfying Assumption 4.4 and Assumption C.3. For a sufficiently large constant $$C > 0$$,\n$$\nn = C \\cdot (d^2 + kd) \\frac{\\varepsilon^2}{\\log kd\\kappa}\n$$\nsamples suffice to guarantee that with high probability, the MLE $$W, \\Sigma$$ satisfies\n$$\nd\\left(W, \\Sigma), (W^*, \\Sigma^*)\\right) \\leq \\varepsilon.\n$$\n\nProof of Lemma C.7. For any $$W \\in \\mathbb{R}^{d \\times k}$$, $$\\Sigma \\in \\mathbb{R}^{d \\times d}$$ and a sample $$(x_i, y_i)$$, let $$p_{i,W,\\Sigma}(y|x_i)$$ be the conditional distribution of $$y = \\phi(Wx + \\eta)$$, and let $$\\gamma_{i,W,\\Sigma}$$ be the log-likelihood ratio between $$(W, \\Sigma)$$ and $$(W^*, \\Sigma^*)$$ on this sample:\n$$\n\\gamma_{i,W,\\Sigma}(y) := \\log p_{i,W,\\Sigma}(y | x_i).\n$$\nThen\n$$\nE_y[\\gamma_{i,W,\\Sigma}(y)] = -\\text{KL}(p_{i,W^*,\\Sigma^*}(y | x_i) \\| p_{i,W,\\Sigma}(y | x_i)).\n$$"}]}, {"page": 41, "text": "Concentration.            From Lemma B.1, we see that if dT V ((W \u2217, \u03a3\u2217), (W, \u03a3)) \u2265                                \u03b5, then for n \u2265\nO( 1 \u03b52 log 1 \u03b4 ),\n                                                               n\nwith probability 1 \u2212         \u03b4.              \u03b3 W,\u03a3 := 1   n  i=1   \u03b3i,W,\u03a3(yi) < \u2212\u03b52       2 ,                                       (37)\nOf course, whenever \u03b3          W,\u03a3 < 0, the likelihood under W \u2217, \u03a3\u2217                is larger than the likelihood under W, \u03a3.\nThus, for each fixed W, \u03a3 with dT V ((W \u2217, \u03a3\u2217), (W, \u03a3)) \u2265                          \u03b5, maximizing likelihood would prefer\nW \u2217, \u03a3\u2217     to W, \u03a3 with probability 1 \u2212             \u03b4 if n \u2265    O( 1 \u03b52 log 1 \u03b4 ).\nNothing above is specific to our ReLU-based distribution. But to extend to the MLE over all W, \u03a3,\nwe need to build a net using properties of our distribution.\nBuilding a net.          First, for a given sample yi, let Si be the set of coordinates of yi that are zero, and\nSci be its complement. Then, with high probability,\n                               \u221712                 \u221712                              \u03bad log n) \u2200      i \u2208   [n],\n                           \u2225PSi (\u03b7i)Si\u2225, \u2225PSc       i (\u03b7i)Si\u2225     \u2264  B = O(\nwhere P \u2217      = \u03a3\u2217\u22121, and P \u2217      S, P \u2217Sc are the block matrices in P \u2217.\nSupposing the above event happens, we will construct a net over the precision matrices P = \u03a3\u22121.\nNote that as we are only considering matrices with bounded condition number, this is a bijective\nmapping.\nNet over \u03a3\u22121.            By Lemma C.4, any precision matrix P = \u03a3\u22121 satisfying Assumption 4.4 and\nwhose max eigenvalue satisfies\n                                                  \u03bbmax(P) \u2265         U \u00b7 \u03bbmax(P \u2217),\nfor U = O         \u03ba3 d2n2   + \u03ba2dn log n      , will have \u00af   \u03b3 W,P \u22121 < 0, irrespective of W.\n                     k2              k\nSimilarly, by Lemma C.2, any precision matrix satisfying Assumption 4.4 and whose max eigenvalue\nsatisfies\n                                                   \u03bbmax(P) \u2264        L \u00b7 \u03bbmin(P \u2217)\nfor L = e\u2212O(\u03ba log n) has \u03b3i,W,P \u22121 \u2264                0 for all i \u2208    [n], and hence its average \u00af        \u03b3 W,P \u22121 is also < 0.\nThis shows that for all precision matrices P whose max eigenvalue is extremely small / large when\ncompared to the min / max eigenvalues of P \u2217, has           \u00af\n                                                            \u03b3 W,P \u22121 < 0,\nirrespective of W, and the MLE, which has non-negative \u00af                        \u03b3, will never pick these P.\nLet A = poly(n, d, \u03ba, 1         \u03b5) be large enough such that A > n(d log(U\u03ba) + B2), and such that it meets\nthe requirements of Lemma C.5 and Lemma C.6.\nThen, by Lemma C.5 with U = poly(d, \u03ba, n) and log( 1                               L) = poly(\u03ba, n), there exists a parti-\ntion P of precision matrices whose max-eigenvalue lies in [L \u00b7 \u03bbmax(P \u2217), U \u00b7 \u03bbmax(P \u2217)] into\n  poly    d, \u03ba, n, 1 \u03b5   d2   cells, such that for each cell I \u2208            P, and P, P \u2032 \u2208       I, the following holds for all\ni \u2208   [n] and W \u2208       Rd\u00d7k:                 |\u03b3i,W,P (yi) \u2212      \u03b3i,W,P \u2032(yi)| \u2264       \u03b52                                          (38)\n                                                                                        16\nor \u03b3i,W,P (yi) < \u2212A.\nUsing Lemma C.18, we also have that for all W,\n                                                dT V ((W, P), (W, P \u2032)) \u2264            \u03b52                                             (39)\n                                                                                     16.\n                                                                   41", "md": "Concentration. From Lemma B.1, we see that if $$dT_V((W^*, \\Sigma^*), (W, \\Sigma)) \\geq \\varepsilon$$, then for $$n \\geq O\\left(\\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta}\\right)$$, with probability $$1 - \\delta$$,\n\n$$\\gamma^{W,\\Sigma} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_i^{W,\\Sigma}(y_i) < -\\frac{\\varepsilon^2}{2}$$   $$(37)$$\n\nOf course, whenever $$\\gamma^{W,\\Sigma} < 0$$, the likelihood under $$W^*, \\Sigma^*$$ is larger than the likelihood under $$W, \\Sigma$$. Thus, for each fixed $$W, \\Sigma$$ with $$dT_V((W^*, \\Sigma^*), (W, \\Sigma)) \\geq \\varepsilon$$, maximizing likelihood would prefer $$W^*, \\Sigma^*$$ to $$W, \\Sigma$$ with probability $$1 - \\delta$$ if $$n \\geq O\\left(\\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta}\\right)$$.\n\nNothing above is specific to our ReLU-based distribution. But to extend to the MLE over all $$W, \\Sigma$$, we need to build a net using properties of our distribution.\n\nBuilding a net. First, for a given sample $$y_i$$, let $$S_i$$ be the set of coordinates of $$y_i$$ that are zero, and $$S^c_i$$ be its complement. Then, with high probability,\n\n$$\\|P_{S_i}(\\eta_i)S_i\\|, \\|P_{S^c_i}(\\eta_i)S_i\\| \\leq B = O(\\varepsilon^{12} \\kappa \\log n) \\quad \\forall i \\in [n],$$\n\nwhere $$P^* = \\Sigma^*{-1}$$, and $$P^*_{S}, P^*_{S^c}$$ are the block matrices in $$P^*$$.\n\nSupposing the above event happens, we will construct a net over the precision matrices $$P = \\Sigma^{-1}$$. Note that as we are only considering matrices with bounded condition number, this is a bijective mapping.\n\nNet over $$\\Sigma^{-1}$$. By Lemma C.4, any precision matrix $$P = \\Sigma^{-1}$$ satisfying Assumption 4.4 and whose max eigenvalue satisfies\n\n$$\\lambda_{\\text{max}}(P) \\geq U \\cdot \\lambda_{\\text{max}}(P^*),$$\n\nfor $$U = O(\\kappa^3 d^2n^2 + \\kappa^2 dn \\log n)$$, will have $$\\overline{\\gamma}^{W,P^{-1}} < 0$$, irrespective of $$W$$. Similarly, by Lemma C.2, any precision matrix satisfying Assumption 4.4 and whose max eigenvalue satisfies\n\n$$\\lambda_{\\text{max}}(P) \\leq L \\cdot \\lambda_{\\text{min}}(P^*)$$\n\nfor $$L = e^{-O(\\kappa \\log n)$$, has $$\\gamma_i^{W,P^{-1}} \\leq 0$$ for all $$i \\in [n]$$, and hence its average $$\\overline{\\gamma}^{W,P^{-1}}$$ is also $$< 0$$. This shows that for all precision matrices $$P$$ whose max eigenvalue is extremely small/large when compared to the min/max eigenvalues of $$P^*$$, has $$\\overline{\\gamma}^{W,P^{-1}} < 0$$, irrespective of $$W$$, and the MLE, which has non-negative $$\\overline{\\gamma}$$, will never pick these $$P$$.\n\nLet $$A = \\text{poly}(n, d, \\kappa, \\frac{1}{\\varepsilon})$$ be large enough such that $$A > n(d \\log(U\\kappa) + B^2)$$, and such that it meets the requirements of Lemma C.5 and Lemma C.6. Then, by Lemma C.5 with $$U = \\text{poly}(d, \\kappa, n)$$ and $$\\log\\left(\\frac{1}{L}\\right) = \\text{poly}(\\kappa, n)$$, there exists a partition $$\\mathcal{P}$$ of precision matrices whose max-eigenvalue lies in $$[L \\cdot \\lambda_{\\text{max}}(P^*), U \\cdot \\lambda_{\\text{max}}(P^*)]$$ into\n\n$$\\text{poly}\\left(d, \\kappa, n, \\frac{1}{\\varepsilon}\\right) d^2$$ cells, such that for each cell $$I \\in \\mathcal{P}$$, and $$P, P' \\in I$$, the following holds for all $$i \\in [n]$$ and $$W \\in \\mathbb{R}^{d \\times k}$$:\n\n$$|\\gamma_i^{W,P}(y_i) - \\gamma_i^{W,P'}(y_i)| \\leq \\frac{\\varepsilon^2}{16}$$ or $$\\gamma_i^{W,P}(y_i) < -A$$. Using Lemma C.18, we also have that for all $$W$$,\n\n$$dT_V((W, P), (W, P')) \\leq \\frac{\\varepsilon^2}{16}$$   $$(39)$$", "images": [], "items": [{"type": "text", "value": "Concentration. From Lemma B.1, we see that if $$dT_V((W^*, \\Sigma^*), (W, \\Sigma)) \\geq \\varepsilon$$, then for $$n \\geq O\\left(\\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta}\\right)$$, with probability $$1 - \\delta$$,\n\n$$\\gamma^{W,\\Sigma} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_i^{W,\\Sigma}(y_i) < -\\frac{\\varepsilon^2}{2}$$   $$(37)$$\n\nOf course, whenever $$\\gamma^{W,\\Sigma} < 0$$, the likelihood under $$W^*, \\Sigma^*$$ is larger than the likelihood under $$W, \\Sigma$$. Thus, for each fixed $$W, \\Sigma$$ with $$dT_V((W^*, \\Sigma^*), (W, \\Sigma)) \\geq \\varepsilon$$, maximizing likelihood would prefer $$W^*, \\Sigma^*$$ to $$W, \\Sigma$$ with probability $$1 - \\delta$$ if $$n \\geq O\\left(\\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta}\\right)$$.\n\nNothing above is specific to our ReLU-based distribution. But to extend to the MLE over all $$W, \\Sigma$$, we need to build a net using properties of our distribution.\n\nBuilding a net. First, for a given sample $$y_i$$, let $$S_i$$ be the set of coordinates of $$y_i$$ that are zero, and $$S^c_i$$ be its complement. Then, with high probability,\n\n$$\\|P_{S_i}(\\eta_i)S_i\\|, \\|P_{S^c_i}(\\eta_i)S_i\\| \\leq B = O(\\varepsilon^{12} \\kappa \\log n) \\quad \\forall i \\in [n],$$\n\nwhere $$P^* = \\Sigma^*{-1}$$, and $$P^*_{S}, P^*_{S^c}$$ are the block matrices in $$P^*$$.\n\nSupposing the above event happens, we will construct a net over the precision matrices $$P = \\Sigma^{-1}$$. Note that as we are only considering matrices with bounded condition number, this is a bijective mapping.\n\nNet over $$\\Sigma^{-1}$$. By Lemma C.4, any precision matrix $$P = \\Sigma^{-1}$$ satisfying Assumption 4.4 and whose max eigenvalue satisfies\n\n$$\\lambda_{\\text{max}}(P) \\geq U \\cdot \\lambda_{\\text{max}}(P^*),$$\n\nfor $$U = O(\\kappa^3 d^2n^2 + \\kappa^2 dn \\log n)$$, will have $$\\overline{\\gamma}^{W,P^{-1}} < 0$$, irrespective of $$W$$. Similarly, by Lemma C.2, any precision matrix satisfying Assumption 4.4 and whose max eigenvalue satisfies\n\n$$\\lambda_{\\text{max}}(P) \\leq L \\cdot \\lambda_{\\text{min}}(P^*)$$\n\nfor $$L = e^{-O(\\kappa \\log n)$$, has $$\\gamma_i^{W,P^{-1}} \\leq 0$$ for all $$i \\in [n]$$, and hence its average $$\\overline{\\gamma}^{W,P^{-1}}$$ is also $$< 0$$. This shows that for all precision matrices $$P$$ whose max eigenvalue is extremely small/large when compared to the min/max eigenvalues of $$P^*$$, has $$\\overline{\\gamma}^{W,P^{-1}} < 0$$, irrespective of $$W$$, and the MLE, which has non-negative $$\\overline{\\gamma}$$, will never pick these $$P$$.\n\nLet $$A = \\text{poly}(n, d, \\kappa, \\frac{1}{\\varepsilon})$$ be large enough such that $$A > n(d \\log(U\\kappa) + B^2)$$, and such that it meets the requirements of Lemma C.5 and Lemma C.6. Then, by Lemma C.5 with $$U = \\text{poly}(d, \\kappa, n)$$ and $$\\log\\left(\\frac{1}{L}\\right) = \\text{poly}(\\kappa, n)$$, there exists a partition $$\\mathcal{P}$$ of precision matrices whose max-eigenvalue lies in $$[L \\cdot \\lambda_{\\text{max}}(P^*), U \\cdot \\lambda_{\\text{max}}(P^*)]$$ into\n\n$$\\text{poly}\\left(d, \\kappa, n, \\frac{1}{\\varepsilon}\\right) d^2$$ cells, such that for each cell $$I \\in \\mathcal{P}$$, and $$P, P' \\in I$$, the following holds for all $$i \\in [n]$$ and $$W \\in \\mathbb{R}^{d \\times k}$$:\n\n$$|\\gamma_i^{W,P}(y_i) - \\gamma_i^{W,P'}(y_i)| \\leq \\frac{\\varepsilon^2}{16}$$ or $$\\gamma_i^{W,P}(y_i) < -A$$. Using Lemma C.18, we also have that for all $$W$$,\n\n$$dT_V((W, P), (W, P')) \\leq \\frac{\\varepsilon^2}{16}$$   $$(39)$$", "md": "Concentration. From Lemma B.1, we see that if $$dT_V((W^*, \\Sigma^*), (W, \\Sigma)) \\geq \\varepsilon$$, then for $$n \\geq O\\left(\\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta}\\right)$$, with probability $$1 - \\delta$$,\n\n$$\\gamma^{W,\\Sigma} := \\frac{1}{n} \\sum_{i=1}^{n} \\gamma_i^{W,\\Sigma}(y_i) < -\\frac{\\varepsilon^2}{2}$$   $$(37)$$\n\nOf course, whenever $$\\gamma^{W,\\Sigma} < 0$$, the likelihood under $$W^*, \\Sigma^*$$ is larger than the likelihood under $$W, \\Sigma$$. Thus, for each fixed $$W, \\Sigma$$ with $$dT_V((W^*, \\Sigma^*), (W, \\Sigma)) \\geq \\varepsilon$$, maximizing likelihood would prefer $$W^*, \\Sigma^*$$ to $$W, \\Sigma$$ with probability $$1 - \\delta$$ if $$n \\geq O\\left(\\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta}\\right)$$.\n\nNothing above is specific to our ReLU-based distribution. But to extend to the MLE over all $$W, \\Sigma$$, we need to build a net using properties of our distribution.\n\nBuilding a net. First, for a given sample $$y_i$$, let $$S_i$$ be the set of coordinates of $$y_i$$ that are zero, and $$S^c_i$$ be its complement. Then, with high probability,\n\n$$\\|P_{S_i}(\\eta_i)S_i\\|, \\|P_{S^c_i}(\\eta_i)S_i\\| \\leq B = O(\\varepsilon^{12} \\kappa \\log n) \\quad \\forall i \\in [n],$$\n\nwhere $$P^* = \\Sigma^*{-1}$$, and $$P^*_{S}, P^*_{S^c}$$ are the block matrices in $$P^*$$.\n\nSupposing the above event happens, we will construct a net over the precision matrices $$P = \\Sigma^{-1}$$. Note that as we are only considering matrices with bounded condition number, this is a bijective mapping.\n\nNet over $$\\Sigma^{-1}$$. By Lemma C.4, any precision matrix $$P = \\Sigma^{-1}$$ satisfying Assumption 4.4 and whose max eigenvalue satisfies\n\n$$\\lambda_{\\text{max}}(P) \\geq U \\cdot \\lambda_{\\text{max}}(P^*),$$\n\nfor $$U = O(\\kappa^3 d^2n^2 + \\kappa^2 dn \\log n)$$, will have $$\\overline{\\gamma}^{W,P^{-1}} < 0$$, irrespective of $$W$$. Similarly, by Lemma C.2, any precision matrix satisfying Assumption 4.4 and whose max eigenvalue satisfies\n\n$$\\lambda_{\\text{max}}(P) \\leq L \\cdot \\lambda_{\\text{min}}(P^*)$$\n\nfor $$L = e^{-O(\\kappa \\log n)$$, has $$\\gamma_i^{W,P^{-1}} \\leq 0$$ for all $$i \\in [n]$$, and hence its average $$\\overline{\\gamma}^{W,P^{-1}}$$ is also $$< 0$$. This shows that for all precision matrices $$P$$ whose max eigenvalue is extremely small/large when compared to the min/max eigenvalues of $$P^*$$, has $$\\overline{\\gamma}^{W,P^{-1}} < 0$$, irrespective of $$W$$, and the MLE, which has non-negative $$\\overline{\\gamma}$$, will never pick these $$P$$.\n\nLet $$A = \\text{poly}(n, d, \\kappa, \\frac{1}{\\varepsilon})$$ be large enough such that $$A > n(d \\log(U\\kappa) + B^2)$$, and such that it meets the requirements of Lemma C.5 and Lemma C.6. Then, by Lemma C.5 with $$U = \\text{poly}(d, \\kappa, n)$$ and $$\\log\\left(\\frac{1}{L}\\right) = \\text{poly}(\\kappa, n)$$, there exists a partition $$\\mathcal{P}$$ of precision matrices whose max-eigenvalue lies in $$[L \\cdot \\lambda_{\\text{max}}(P^*), U \\cdot \\lambda_{\\text{max}}(P^*)]$$ into\n\n$$\\text{poly}\\left(d, \\kappa, n, \\frac{1}{\\varepsilon}\\right) d^2$$ cells, such that for each cell $$I \\in \\mathcal{P}$$, and $$P, P' \\in I$$, the following holds for all $$i \\in [n]$$ and $$W \\in \\mathbb{R}^{d \\times k}$$:\n\n$$|\\gamma_i^{W,P}(y_i) - \\gamma_i^{W,P'}(y_i)| \\leq \\frac{\\varepsilon^2}{16}$$ or $$\\gamma_i^{W,P}(y_i) < -A$$. Using Lemma C.18, we also have that for all $$W$$,\n\n$$dT_V((W, P), (W, P')) \\leq \\frac{\\varepsilon^2}{16}$$   $$(39)$$"}]}, {"page": 42, "text": "We can choose a net N consisting of precision matrices from each cell in P. This net has size\n                                                      log|N| \u2272       d2 log   d\u03ban  \u03b5      .\n This gives a sufficient net over the precision matrices.\n Now we will construct a net over W for each precision matrix in the net.\n W   -net.     Now, for each         P \u2208    NP , by Lemma C.6, for each i \u2208                   [n], there exists a partition P           P ,i of\n Rd into      poly     d, k, \u03ba, n, 1  \u03b5   d cells such that for each cell I \u2208                  P P ,i, and W, W \u2032 \u2208          I, one of the\n following holds:\n                                                 \u03b3i,W,   P (yi) \u2212    \u03b3i,W \u2032,  P (yi)    \u2264    \u03b52                                           (40)\n                                                                                             16\n or \u03b3 i,W,  P (yi) < \u2212A.\n Let Wj be the j-th row of W               . The individual partitions P             P ,i on \u27e8xi, Wj\u27e9       induce a partition P          P ,i,j\n on Rk, where Wj, W \u2032          j lie in the same cell of P          P ,i,j if \u27e8xi, Wj\u27e9      and \u27e8xi, W \u2032    j\u27e9  are in the same cell of\n P P ,i for all i \u2208    [n]. Since P      P ,i,j is defined by n sets of           poly     d, k, \u03ba, n, 1  \u03b5     parallel hyperplanes in\n Rk, the number of cells in P             P ,i,j is     poly      d, k, \u03ba, n, 1  \u03b5   k    .\n As there are d rows in W            , we can intersect P         P ,i,j over j \u2208      [d], which induces poly(d, k, \u03ba, n, 1              \u03b5)kd\n cells in Rk. We choose a net N                   P to contain, for each cell in                   j\u2208[d] P   P ,i,j, the W in the cell\n maximizing dT V ((W \u2217, P \u2217), (W,                P  )). This has size          \u03badkn\n                                                    log  N  P    \u2272   kd log         \u03b5       .\n Proving MLE works.                  By (37), for our n \u2265           O     (kd+d2)    log kd\u03ba      , we have with high probability\n                                                                              \u03b52             \u03b5\n that                                                         \u03b3 W,P \u2264      \u2212\u03b52 2 ,\n for all P \u2208       N and for all W \u2208              N  P with dT V ((W \u2217, P \u2217), (W, P              )) \u2265     \u03b5. Suppose that both this\n happens, and               \u2225P   \u221712                 \u221712                                \u03bad log n) \u2200       i \u2208  [n].\n                                Si (\u03b7i)Si\u2225, \u2225PSc       i (\u03b7i)Si\u2225     \u2264   B = O(\nWe claim that the MLE              W   , \u03a3 must have dT V ((W \u2217, \u03a3\u2217), (              W   ,\u03a3)) < 17   16\u03b5.\n Consider any W \u2208             Rd\u00d7k and P \u2208           Rd\u00d7d with dT V ((W \u2217, P \u2217), (W, P                 )) \u2265     17\n                                                                                                                16\u03b5. Using our net on\n precision matrices, we can find              P \u2208     N such that\n               dT V ((W \u2217, P \u2217), (W,          P )) \u2265    dT V ((W \u2217, P \u2217), (W, P          )) \u2212   dT V ((W, P      ), (W,    P  )).\n Recall that we are only currently considering W, P such that dT V ((W \u2217, P \u2217), (W, P                                       )) \u2265     17\n Eqn (39), we have dT V ((W, P               ), (W,   P  )) \u2264    \u03b52                                                                  16\u03b5. By\n                                                                 16, which gives\n                                         dT V ((W \u2217, P \u2217), (W,         P  )) \u2265   \u03b51716 \u2212     \u03b52\n                                                                                            16 \u2265     \u03b5.\n Now, for this      P  , we can find a       W \u2208     N  P , and by our choice of N            P , we know that\n                              dT V ((W \u2217, P \u2217), (      W   , P  )) \u2265   dT V ((W \u2217, P \u2217), (W,          P )) \u2265    \u03b5,\n                                                                       42", "md": "We can choose a net $N$ consisting of precision matrices from each cell in $P$. This net has size\n$$\n\\log|N| \\lesssim d^2 \\log d\\kappa n \\epsilon.\n$$\nThis gives a sufficient net over the precision matrices.\nNow we will construct a net over $W$ for each precision matrix in the net.\n$W$-net. Now, for each $P \\in N_P$, by Lemma C.6, for each $i \\in [n]$, there exists a partition $P_{P,i}$ of $\\mathbb{R}^d$ into $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})$ cells such that for each cell $I \\in P_{P,i}$, and $W, W' \\in I$, one of the following holds:\n$$\n\\gamma_i,W,P(y_i) - \\gamma_i,W',P(y_i) \\leq \\frac{\\epsilon^2}{16} \\quad \\text{(40)}\n$$\nor $\\gamma_i,W,P(y_i) < -A$.\nLet $W_j$ be the $j$-th row of $W$. The individual partitions $P_{P,i}$ on $\\langle x_i, W_j \\rangle$ induce a partition $P_{P,i,j}$ on $\\mathbb{R}^k$, where $W_j, W'_j$ lie in the same cell of $P_{P,i,j}$ if $\\langle x_i, W_j \\rangle$ and $\\langle x_i, W'_j \\rangle$ are in the same cell of $P_{P,i}$ for all $i \\in [n]$. Since $P_{P,i,j}$ is defined by $n$ sets of $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})$ parallel hyperplanes in $\\mathbb{R}^k$, the number of cells in $P_{P,i,j}$ is $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})k$.\nAs there are $d$ rows in $W$, we can intersect $P_{P,i,j}$ over $j \\in [d]$, which induces $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})kd$ cells in $\\mathbb{R}^k$. We choose a net $N_{P}$ to contain, for each cell in $\\bigcup_{j\\in[d]} P_{P,i,j}$, the $W$ in the cell maximizing $dT V((W^*, P^*), (W, P))$. This has size $\\kappa dkn$\n$$\n\\log N_{P} \\lesssim kd \\log \\frac{1}{\\epsilon}.\n$$\nProving MLE works. By (37), for our $n \\geq O(kd+d^2) \\log kd\\kappa$, we have with high probability that $\\gamma_{W,P} \\leq -\\frac{\\epsilon^2}{2}$, for all $P \\in N$ and for all $W \\in N_{P}$ with $dT V((W^*, P^*), (W, P)) \\geq \\epsilon$. Suppose that both this happens, and $\\|P^*_{12} - \\Sigma^*_{12}\\|, \\|P^*_{c} - \\Sigma^*_{c}\\| \\leq B = O(\\kappa d \\log n)$ for all $i \\in [n]$.\nWe claim that the MLE $W, \\Sigma$ must have $dT V((W^*, \\Sigma^*), (W, \\Sigma)) < \\frac{17}{16}\\epsilon$.\nConsider any $W \\in \\mathbb{R}^{d \\times k}$ and $P \\in \\mathbb{R}^{d \\times d}$ with $dT V((W^*, P^*), (W, P)) \\geq \\frac{17}{16}\\epsilon$. Using our net on precision matrices, we can find $P' \\in N$ such that\n$$\ndT V((W^*, P^*), (W, P)) \\geq dT V((W^*, P^*), (W, P)) - dT V((W, P), (W, P')).\n$$\nRecall that we are only currently considering $W, P$ such that $dT V((W^*, P^*), (W, P)) \\geq \\frac{17}{16}\\epsilon$. By Eqn (39), we have $dT V((W, P), (W, P')) \\leq \\frac{\\epsilon^2}{16\\epsilon}$. By 16, which gives\n$$\ndT V((W^*, P^*), (W, P)) \\geq \\frac{\\epsilon}{17/16} - \\frac{\\epsilon^2}{16} \\geq \\epsilon.\n$$\nNow, for this $P'$, we can find a $W' \\in N_{P}$, and by our choice of $N_{P}$, we know that\n$$\ndT V((W^*, P^*), (W', P')) \\geq dT V((W^*, P^*), (W, P)) \\geq \\epsilon.\n$$", "images": [], "items": [{"type": "text", "value": "We can choose a net $N$ consisting of precision matrices from each cell in $P$. This net has size\n$$\n\\log|N| \\lesssim d^2 \\log d\\kappa n \\epsilon.\n$$\nThis gives a sufficient net over the precision matrices.\nNow we will construct a net over $W$ for each precision matrix in the net.\n$W$-net. Now, for each $P \\in N_P$, by Lemma C.6, for each $i \\in [n]$, there exists a partition $P_{P,i}$ of $\\mathbb{R}^d$ into $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})$ cells such that for each cell $I \\in P_{P,i}$, and $W, W' \\in I$, one of the following holds:\n$$\n\\gamma_i,W,P(y_i) - \\gamma_i,W',P(y_i) \\leq \\frac{\\epsilon^2}{16} \\quad \\text{(40)}\n$$\nor $\\gamma_i,W,P(y_i) < -A$.\nLet $W_j$ be the $j$-th row of $W$. The individual partitions $P_{P,i}$ on $\\langle x_i, W_j \\rangle$ induce a partition $P_{P,i,j}$ on $\\mathbb{R}^k$, where $W_j, W'_j$ lie in the same cell of $P_{P,i,j}$ if $\\langle x_i, W_j \\rangle$ and $\\langle x_i, W'_j \\rangle$ are in the same cell of $P_{P,i}$ for all $i \\in [n]$. Since $P_{P,i,j}$ is defined by $n$ sets of $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})$ parallel hyperplanes in $\\mathbb{R}^k$, the number of cells in $P_{P,i,j}$ is $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})k$.\nAs there are $d$ rows in $W$, we can intersect $P_{P,i,j}$ over $j \\in [d]$, which induces $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})kd$ cells in $\\mathbb{R}^k$. We choose a net $N_{P}$ to contain, for each cell in $\\bigcup_{j\\in[d]} P_{P,i,j}$, the $W$ in the cell maximizing $dT V((W^*, P^*), (W, P))$. This has size $\\kappa dkn$\n$$\n\\log N_{P} \\lesssim kd \\log \\frac{1}{\\epsilon}.\n$$\nProving MLE works. By (37), for our $n \\geq O(kd+d^2) \\log kd\\kappa$, we have with high probability that $\\gamma_{W,P} \\leq -\\frac{\\epsilon^2}{2}$, for all $P \\in N$ and for all $W \\in N_{P}$ with $dT V((W^*, P^*), (W, P)) \\geq \\epsilon$. Suppose that both this happens, and $\\|P^*_{12} - \\Sigma^*_{12}\\|, \\|P^*_{c} - \\Sigma^*_{c}\\| \\leq B = O(\\kappa d \\log n)$ for all $i \\in [n]$.\nWe claim that the MLE $W, \\Sigma$ must have $dT V((W^*, \\Sigma^*), (W, \\Sigma)) < \\frac{17}{16}\\epsilon$.\nConsider any $W \\in \\mathbb{R}^{d \\times k}$ and $P \\in \\mathbb{R}^{d \\times d}$ with $dT V((W^*, P^*), (W, P)) \\geq \\frac{17}{16}\\epsilon$. Using our net on precision matrices, we can find $P' \\in N$ such that\n$$\ndT V((W^*, P^*), (W, P)) \\geq dT V((W^*, P^*), (W, P)) - dT V((W, P), (W, P')).\n$$\nRecall that we are only currently considering $W, P$ such that $dT V((W^*, P^*), (W, P)) \\geq \\frac{17}{16}\\epsilon$. By Eqn (39), we have $dT V((W, P), (W, P')) \\leq \\frac{\\epsilon^2}{16\\epsilon}$. By 16, which gives\n$$\ndT V((W^*, P^*), (W, P)) \\geq \\frac{\\epsilon}{17/16} - \\frac{\\epsilon^2}{16} \\geq \\epsilon.\n$$\nNow, for this $P'$, we can find a $W' \\in N_{P}$, and by our choice of $N_{P}$, we know that\n$$\ndT V((W^*, P^*), (W', P')) \\geq dT V((W^*, P^*), (W, P)) \\geq \\epsilon.\n$$", "md": "We can choose a net $N$ consisting of precision matrices from each cell in $P$. This net has size\n$$\n\\log|N| \\lesssim d^2 \\log d\\kappa n \\epsilon.\n$$\nThis gives a sufficient net over the precision matrices.\nNow we will construct a net over $W$ for each precision matrix in the net.\n$W$-net. Now, for each $P \\in N_P$, by Lemma C.6, for each $i \\in [n]$, there exists a partition $P_{P,i}$ of $\\mathbb{R}^d$ into $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})$ cells such that for each cell $I \\in P_{P,i}$, and $W, W' \\in I$, one of the following holds:\n$$\n\\gamma_i,W,P(y_i) - \\gamma_i,W',P(y_i) \\leq \\frac{\\epsilon^2}{16} \\quad \\text{(40)}\n$$\nor $\\gamma_i,W,P(y_i) < -A$.\nLet $W_j$ be the $j$-th row of $W$. The individual partitions $P_{P,i}$ on $\\langle x_i, W_j \\rangle$ induce a partition $P_{P,i,j}$ on $\\mathbb{R}^k$, where $W_j, W'_j$ lie in the same cell of $P_{P,i,j}$ if $\\langle x_i, W_j \\rangle$ and $\\langle x_i, W'_j \\rangle$ are in the same cell of $P_{P,i}$ for all $i \\in [n]$. Since $P_{P,i,j}$ is defined by $n$ sets of $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})$ parallel hyperplanes in $\\mathbb{R}^k$, the number of cells in $P_{P,i,j}$ is $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})k$.\nAs there are $d$ rows in $W$, we can intersect $P_{P,i,j}$ over $j \\in [d]$, which induces $\\text{poly}(d, k, \\kappa, n, \\frac{1}{\\epsilon})kd$ cells in $\\mathbb{R}^k$. We choose a net $N_{P}$ to contain, for each cell in $\\bigcup_{j\\in[d]} P_{P,i,j}$, the $W$ in the cell maximizing $dT V((W^*, P^*), (W, P))$. This has size $\\kappa dkn$\n$$\n\\log N_{P} \\lesssim kd \\log \\frac{1}{\\epsilon}.\n$$\nProving MLE works. By (37), for our $n \\geq O(kd+d^2) \\log kd\\kappa$, we have with high probability that $\\gamma_{W,P} \\leq -\\frac{\\epsilon^2}{2}$, for all $P \\in N$ and for all $W \\in N_{P}$ with $dT V((W^*, P^*), (W, P)) \\geq \\epsilon$. Suppose that both this happens, and $\\|P^*_{12} - \\Sigma^*_{12}\\|, \\|P^*_{c} - \\Sigma^*_{c}\\| \\leq B = O(\\kappa d \\log n)$ for all $i \\in [n]$.\nWe claim that the MLE $W, \\Sigma$ must have $dT V((W^*, \\Sigma^*), (W, \\Sigma)) < \\frac{17}{16}\\epsilon$.\nConsider any $W \\in \\mathbb{R}^{d \\times k}$ and $P \\in \\mathbb{R}^{d \\times d}$ with $dT V((W^*, P^*), (W, P)) \\geq \\frac{17}{16}\\epsilon$. Using our net on precision matrices, we can find $P' \\in N$ such that\n$$\ndT V((W^*, P^*), (W, P)) \\geq dT V((W^*, P^*), (W, P)) - dT V((W, P), (W, P')).\n$$\nRecall that we are only currently considering $W, P$ such that $dT V((W^*, P^*), (W, P)) \\geq \\frac{17}{16}\\epsilon$. By Eqn (39), we have $dT V((W, P), (W, P')) \\leq \\frac{\\epsilon^2}{16\\epsilon}$. By 16, which gives\n$$\ndT V((W^*, P^*), (W, P)) \\geq \\frac{\\epsilon}{17/16} - \\frac{\\epsilon^2}{16} \\geq \\epsilon.\n$$\nNow, for this $P'$, we can find a $W' \\in N_{P}$, and by our choice of $N_{P}$, we know that\n$$\ndT V((W^*, P^*), (W', P')) \\geq dT V((W^*, P^*), (W, P)) \\geq \\epsilon.\n$$"}]}, {"page": 43, "text": "and by (37), we have \u00af         \u03b3 W , P \u2264    \u2212\u03b52 2 .\nNow we consider two cases. In the first case, there exists i with \u03b3i,W,P (yi) < \u2212A. Then\n                                     \u03b3 W,P = 1    n     i   \u03b3i,W,P (yi) \u2264       \u2212A n + B2/2 < 0.\nOtherwise, by Eqn (38) and Eqn (40), we have\n                       \u03b3 W,P \u2264      \u03b3 W , P +     \u03b3 W , P \u2212    \u03b3W,   P   +    \u03b3W,   P \u2212    \u03b3W,P     ,\n                                \u2264   \u2212\u03b52 2 + max    i   \u03b3i,  W , P \u2212   \u03b3i,W,   P   + max  i    \u03b3i,W,  P \u2212    \u03b3i,W,P     ,\n                                \u2264   \u2212\u03b52 2 + \u03b52  16 + \u03b52  16 < 0.\nIn either case, \u03b3      W,P < 0 and the likelihood under w\u2217                   exceeds that under w. Hence the MLE                     w must\nhave dT V (w\u2217, w) \u2264            17\n                               16\u03b5. Rescaling \u03b5 gives the conclusion of the Lemma.\nLemma C.8. Let {xi}n             i=1 be i.i.d. random variables such that xi \u223c                      Dx.\nLet P \u2217    := \u03a3\u2217\u22121. Let \u03bb\u2217       min, \u03bb\u2217  max be the minimum and maximum eigenvalues of P \u2217. For 0 < L < U,\nlet \u2126   denote the following set of precision matrices\n                  \u2126  :=      P \u2208    Rd\u00d7d     : \u03bbmax(P       )                                         min, U \u00b7 \u03bb\u2217   max]     .\n                                       +        \u03bbmin(P     ) \u2264    \u03ba and \u03bbmax(P         ) \u2208   [L \u00b7 \u03bb\u2217\nThen, for a sufficiently large constant C > 0, and for\n                                        n = C \u00b7       kd + d2          log   kd\u03ba      log   U          ,\n                                                            \u03b52                   \u03b5            L\nwe have:   Pr             sup          d((W, P      ), (W \u2217, P \u2217)) \u2212       d((W, P      ), (W \u2217, P \u2217))       > \u03b5      \u2264  e\u2212\u2126(n\u03b52).\n        xi\u223cDx      W \u2208Rd\u00d7k,P \u2208\u2126\nProof of Lemma C.8. For P = \u03a3\u22121 and P \u2217                          = \u03a3\u2217\u22121, let\nand                                            f(W, P     ) := d((W, \u03a3), (W \u2217, \u03a3\u2217))\n              fn(W, P      ) :=   d((W, \u03a3), (W \u2217, \u03a3\u2217)) = 1            n    [dT V (pW,P (y|xi), pW \u2217,P \u2217(y|xi))].\n                                                                            i\nSince the function is bounded, for any fixed W, P                       , the Chernoff bound gives\n                                        Pr[|fn(W, P        ) \u2212   f(W, P     )| > \u03b1] \u2264       e\u22122n\u03b12.                                       (41)\nfor any \u03b1 > 0. The challenge lies in constructing a net to be able to union bound over Rk without\nassuming any bound on W or the covariate x. As before, we do so by constructing a \u201cghost\u201d sample,\nsymmetrizing, and constructing a net based on these samples.\nGhost sample.            First, we construct a \u201cghost\u201d dataset D\u2032                x consisting of n fresh samples IID samples\n{x\u2032 i}i\u2208[n] of Dx. This gives another metric\n             f \u2032                  d\u2032((W, \u03a3), (W \u2217, \u03a3\u2217)) = 1                    [dT V (pW,P (y|x\u2032\n               n(W, P     ) :=                                         n    i                         i), pW \u2217,P \u2217(y|x\u2032     i))].\nSimilar to the proof in Lemma 4.3, it is sufficient to consider the difference between fn(W, P                                          ) and\nf \u2032\n  n(W, P     ) i.e.,\n            Pr    sup|f(W, P       ) \u2212   fn(W, P      )| > \u03b5      \u2264   2 Pr    sup|fn(W, P         ) \u2212   f \u2032                        .      (42)\n                 W,P                                                          W,P                        n(W, P      )| > \u03b5/2\n                                                                      43", "md": "and by (37), we have $$\\bar{\\gamma}_{W,P} \\leq -\\frac{\\varepsilon^2}{2}$$.\nNow we consider two cases. In the first case, there exists i with $$\\gamma_{i,W,P}(y_i) < -A$$. Then\n$$\n\\begin{align*}\n\\gamma_{W,P} &= \\frac{1}{n} \\sum_{i} \\gamma_{i,W,P}(y_i) \\\\\n&\\leq -\\frac{A}{n} + \\frac{B^2}{2} < 0.\n\\end{align*}\n$$\nOtherwise, by Eqn (38) and Eqn (40), we have\n$$\n\\begin{align*}\n\\gamma_{W,P} &\\leq \\gamma_{W,P} + \\gamma_{W,P} - \\gamma_{W,P} + \\gamma_{W,P} - \\gamma_{W,P} \\\\\n&\\leq -\\frac{\\varepsilon^2}{2} + \\max_i \\gamma_{i,W,P} - \\gamma_{i,W,P} + \\max_i \\gamma_{i,W,P} - \\gamma_{i,W,P} \\\\\n&\\leq -\\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^2}{16} + \\frac{\\varepsilon^2}{16} < 0.\n\\end{align*}\n$$\nIn either case, $$\\gamma_{W,P} < 0$$ and the likelihood under $$w^*$$ exceeds that under $$w$$. Hence the MLE $$w$$ must have $$dT V(w^*, w) \\leq \\frac{17}{16}\\varepsilon$$. Rescaling $$\\varepsilon$$ gives the conclusion of the Lemma.\n\nLemma C.8. Let {$x_i$}$_{i=1}^n$ be i.i.d. random variables such that $$x_i \\sim D_x$$.\nLet $$P^* := \\Sigma^*{-1}$$. Let $$\\lambda^*_{\\text{min}}, \\lambda^*_{\\text{max}}$$ be the minimum and maximum eigenvalues of $$P^*$$. For $$0 < L < U$$, let $$\\Omega$$ denote the following set of precision matrices\n$$\n\\Omega := \\{P \\in \\mathbb{R}^{d \\times d} : \\lambda_{\\text{max}}(P) \\leq U \\cdot \\lambda^*_{\\text{max}}, \\lambda_{\\text{min}}(P) \\leq \\kappa \\text{ and } \\lambda_{\\text{max}}(P) \\in [L \\cdot \\lambda^*_{\\text{min}}, U \\cdot \\lambda^*_{\\text{max}}]\\}.\n$$\nThen, for a sufficiently large constant $$C > 0$$, and for\n$$\nn = C \\cdot \\left(kd + d^2 \\log(kd\\kappa) \\log(U)\\right),\n$$\n$$\n\\epsilon^2, \\epsilon, L\n$$\nwe have:\n$$\n\\Pr\\left\\{\\sup_{xi \\sim D_x} d((W, P), (W^*, P^*)) - d((W, P), (W^*, P^*)) > \\epsilon\\right\\} \\leq e^{-\\Omega(n\\epsilon^2)}.\n$$\n$$xi \\sim D_x, W \\in \\mathbb{R}^{d \\times k}, P \\in \\Omega$$\n\nProof of Lemma C.8. For $$P = \\Sigma^{-1}$$ and $$P^* = \\Sigma^*{-1}$$, let\n$$\nf(W, P) := d((W, \\Sigma), (W^*, \\Sigma^*)).\n$$\n$$\nf_n(W, P) := d((W, \\Sigma), (W^*, \\Sigma^*)) = \\frac{1}{n} \\left[dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i))\\right]_{i}.\n$$\nSince the function is bounded, for any fixed $$W, P$$, the Chernoff bound gives\n$$\n\\Pr\\{|f_n(W, P) - f(W, P)| > \\alpha\\} \\leq e^{-2n\\alpha^2}. \\quad (41)\n$$\nfor any $$\\alpha > 0$$. The challenge lies in constructing a net to be able to union bound over $$\\mathbb{R}^k$$ without assuming any bound on $$W$$ or the covariate $$x$$. As before, we do so by constructing a \u201cghost\u201d sample, symmetrizing, and constructing a net based on these samples.\n\nGhost sample. First, we construct a \u201cghost\u201d dataset $$D'_x$$ consisting of $$n$$ fresh samples IID samples {$x'_i$}$_{i \\in [n]}$ of $$D_x$$. This gives another metric\n$$\nf'_n(W, P) := \\frac{1}{n} \\left[dT V(p_{W,P}(y|x'_i), p_{W^*,P^*}(y|x'_i))\\right].\n$$\nSimilar to the proof in Lemma 4.3, it is sufficient to consider the difference between $$f_n(W, P)$$ and $$f'_n(W, P)$$ i.e.,\n$$\n\\Pr\\left\\{\\sup_{W,P} |f(W, P) - f_n(W, P)| > \\epsilon\\right\\} \\leq 2 \\Pr\\left\\{\\sup_{W,P} |f_n(W, P) - f'_n(W, P)| > \\frac{\\epsilon}{2}\\right\\}. \\quad (42)\n$$", "images": [], "items": [{"type": "text", "value": "and by (37), we have $$\\bar{\\gamma}_{W,P} \\leq -\\frac{\\varepsilon^2}{2}$$.\nNow we consider two cases. In the first case, there exists i with $$\\gamma_{i,W,P}(y_i) < -A$$. Then\n$$\n\\begin{align*}\n\\gamma_{W,P} &= \\frac{1}{n} \\sum_{i} \\gamma_{i,W,P}(y_i) \\\\\n&\\leq -\\frac{A}{n} + \\frac{B^2}{2} < 0.\n\\end{align*}\n$$\nOtherwise, by Eqn (38) and Eqn (40), we have\n$$\n\\begin{align*}\n\\gamma_{W,P} &\\leq \\gamma_{W,P} + \\gamma_{W,P} - \\gamma_{W,P} + \\gamma_{W,P} - \\gamma_{W,P} \\\\\n&\\leq -\\frac{\\varepsilon^2}{2} + \\max_i \\gamma_{i,W,P} - \\gamma_{i,W,P} + \\max_i \\gamma_{i,W,P} - \\gamma_{i,W,P} \\\\\n&\\leq -\\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^2}{16} + \\frac{\\varepsilon^2}{16} < 0.\n\\end{align*}\n$$\nIn either case, $$\\gamma_{W,P} < 0$$ and the likelihood under $$w^*$$ exceeds that under $$w$$. Hence the MLE $$w$$ must have $$dT V(w^*, w) \\leq \\frac{17}{16}\\varepsilon$$. Rescaling $$\\varepsilon$$ gives the conclusion of the Lemma.\n\nLemma C.8. Let {$x_i$}$_{i=1}^n$ be i.i.d. random variables such that $$x_i \\sim D_x$$.\nLet $$P^* := \\Sigma^*{-1}$$. Let $$\\lambda^*_{\\text{min}}, \\lambda^*_{\\text{max}}$$ be the minimum and maximum eigenvalues of $$P^*$$. For $$0 < L < U$$, let $$\\Omega$$ denote the following set of precision matrices\n$$\n\\Omega := \\{P \\in \\mathbb{R}^{d \\times d} : \\lambda_{\\text{max}}(P) \\leq U \\cdot \\lambda^*_{\\text{max}}, \\lambda_{\\text{min}}(P) \\leq \\kappa \\text{ and } \\lambda_{\\text{max}}(P) \\in [L \\cdot \\lambda^*_{\\text{min}}, U \\cdot \\lambda^*_{\\text{max}}]\\}.\n$$\nThen, for a sufficiently large constant $$C > 0$$, and for\n$$\nn = C \\cdot \\left(kd + d^2 \\log(kd\\kappa) \\log(U)\\right),\n$$\n$$\n\\epsilon^2, \\epsilon, L\n$$\nwe have:\n$$\n\\Pr\\left\\{\\sup_{xi \\sim D_x} d((W, P), (W^*, P^*)) - d((W, P), (W^*, P^*)) > \\epsilon\\right\\} \\leq e^{-\\Omega(n\\epsilon^2)}.\n$$\n$$xi \\sim D_x, W \\in \\mathbb{R}^{d \\times k}, P \\in \\Omega$$\n\nProof of Lemma C.8. For $$P = \\Sigma^{-1}$$ and $$P^* = \\Sigma^*{-1}$$, let\n$$\nf(W, P) := d((W, \\Sigma), (W^*, \\Sigma^*)).\n$$\n$$\nf_n(W, P) := d((W, \\Sigma), (W^*, \\Sigma^*)) = \\frac{1}{n} \\left[dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i))\\right]_{i}.\n$$\nSince the function is bounded, for any fixed $$W, P$$, the Chernoff bound gives\n$$\n\\Pr\\{|f_n(W, P) - f(W, P)| > \\alpha\\} \\leq e^{-2n\\alpha^2}. \\quad (41)\n$$\nfor any $$\\alpha > 0$$. The challenge lies in constructing a net to be able to union bound over $$\\mathbb{R}^k$$ without assuming any bound on $$W$$ or the covariate $$x$$. As before, we do so by constructing a \u201cghost\u201d sample, symmetrizing, and constructing a net based on these samples.\n\nGhost sample. First, we construct a \u201cghost\u201d dataset $$D'_x$$ consisting of $$n$$ fresh samples IID samples {$x'_i$}$_{i \\in [n]}$ of $$D_x$$. This gives another metric\n$$\nf'_n(W, P) := \\frac{1}{n} \\left[dT V(p_{W,P}(y|x'_i), p_{W^*,P^*}(y|x'_i))\\right].\n$$\nSimilar to the proof in Lemma 4.3, it is sufficient to consider the difference between $$f_n(W, P)$$ and $$f'_n(W, P)$$ i.e.,\n$$\n\\Pr\\left\\{\\sup_{W,P} |f(W, P) - f_n(W, P)| > \\epsilon\\right\\} \\leq 2 \\Pr\\left\\{\\sup_{W,P} |f_n(W, P) - f'_n(W, P)| > \\frac{\\epsilon}{2}\\right\\}. \\quad (42)\n$$", "md": "and by (37), we have $$\\bar{\\gamma}_{W,P} \\leq -\\frac{\\varepsilon^2}{2}$$.\nNow we consider two cases. In the first case, there exists i with $$\\gamma_{i,W,P}(y_i) < -A$$. Then\n$$\n\\begin{align*}\n\\gamma_{W,P} &= \\frac{1}{n} \\sum_{i} \\gamma_{i,W,P}(y_i) \\\\\n&\\leq -\\frac{A}{n} + \\frac{B^2}{2} < 0.\n\\end{align*}\n$$\nOtherwise, by Eqn (38) and Eqn (40), we have\n$$\n\\begin{align*}\n\\gamma_{W,P} &\\leq \\gamma_{W,P} + \\gamma_{W,P} - \\gamma_{W,P} + \\gamma_{W,P} - \\gamma_{W,P} \\\\\n&\\leq -\\frac{\\varepsilon^2}{2} + \\max_i \\gamma_{i,W,P} - \\gamma_{i,W,P} + \\max_i \\gamma_{i,W,P} - \\gamma_{i,W,P} \\\\\n&\\leq -\\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^2}{16} + \\frac{\\varepsilon^2}{16} < 0.\n\\end{align*}\n$$\nIn either case, $$\\gamma_{W,P} < 0$$ and the likelihood under $$w^*$$ exceeds that under $$w$$. Hence the MLE $$w$$ must have $$dT V(w^*, w) \\leq \\frac{17}{16}\\varepsilon$$. Rescaling $$\\varepsilon$$ gives the conclusion of the Lemma.\n\nLemma C.8. Let {$x_i$}$_{i=1}^n$ be i.i.d. random variables such that $$x_i \\sim D_x$$.\nLet $$P^* := \\Sigma^*{-1}$$. Let $$\\lambda^*_{\\text{min}}, \\lambda^*_{\\text{max}}$$ be the minimum and maximum eigenvalues of $$P^*$$. For $$0 < L < U$$, let $$\\Omega$$ denote the following set of precision matrices\n$$\n\\Omega := \\{P \\in \\mathbb{R}^{d \\times d} : \\lambda_{\\text{max}}(P) \\leq U \\cdot \\lambda^*_{\\text{max}}, \\lambda_{\\text{min}}(P) \\leq \\kappa \\text{ and } \\lambda_{\\text{max}}(P) \\in [L \\cdot \\lambda^*_{\\text{min}}, U \\cdot \\lambda^*_{\\text{max}}]\\}.\n$$\nThen, for a sufficiently large constant $$C > 0$$, and for\n$$\nn = C \\cdot \\left(kd + d^2 \\log(kd\\kappa) \\log(U)\\right),\n$$\n$$\n\\epsilon^2, \\epsilon, L\n$$\nwe have:\n$$\n\\Pr\\left\\{\\sup_{xi \\sim D_x} d((W, P), (W^*, P^*)) - d((W, P), (W^*, P^*)) > \\epsilon\\right\\} \\leq e^{-\\Omega(n\\epsilon^2)}.\n$$\n$$xi \\sim D_x, W \\in \\mathbb{R}^{d \\times k}, P \\in \\Omega$$\n\nProof of Lemma C.8. For $$P = \\Sigma^{-1}$$ and $$P^* = \\Sigma^*{-1}$$, let\n$$\nf(W, P) := d((W, \\Sigma), (W^*, \\Sigma^*)).\n$$\n$$\nf_n(W, P) := d((W, \\Sigma), (W^*, \\Sigma^*)) = \\frac{1}{n} \\left[dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i))\\right]_{i}.\n$$\nSince the function is bounded, for any fixed $$W, P$$, the Chernoff bound gives\n$$\n\\Pr\\{|f_n(W, P) - f(W, P)| > \\alpha\\} \\leq e^{-2n\\alpha^2}. \\quad (41)\n$$\nfor any $$\\alpha > 0$$. The challenge lies in constructing a net to be able to union bound over $$\\mathbb{R}^k$$ without assuming any bound on $$W$$ or the covariate $$x$$. As before, we do so by constructing a \u201cghost\u201d sample, symmetrizing, and constructing a net based on these samples.\n\nGhost sample. First, we construct a \u201cghost\u201d dataset $$D'_x$$ consisting of $$n$$ fresh samples IID samples {$x'_i$}$_{i \\in [n]}$ of $$D_x$$. This gives another metric\n$$\nf'_n(W, P) := \\frac{1}{n} \\left[dT V(p_{W,P}(y|x'_i), p_{W^*,P^*}(y|x'_i))\\right].\n$$\nSimilar to the proof in Lemma 4.3, it is sufficient to consider the difference between $$f_n(W, P)$$ and $$f'_n(W, P)$$ i.e.,\n$$\n\\Pr\\left\\{\\sup_{W,P} |f(W, P) - f_n(W, P)| > \\epsilon\\right\\} \\leq 2 \\Pr\\left\\{\\sup_{W,P} |f_n(W, P) - f'_n(W, P)| > \\frac{\\epsilon}{2}\\right\\}. \\quad (42)\n$$"}]}, {"page": 44, "text": " Symmetrization.              Since Dx and D\u2032         x each have n independent samples, we could instead draw the\n datasets by first sampling 2n elements x1, . . . , x2n from Dx, then randomly partition this sample into\n two equal datasets. Let si \u2208             {\u00b11} so si = 1 if zi lies in D\u2032            x and \u22121 if it lies in Dx. Then\n                      fn(W, P      ) \u2212  f \u2032                     2n   si \u00b7 dT V (pW,P (y|xi), pW \u2217,P \u2217(y|xi)).\n                                          n(W, P     ) = 1  n  i=1\n For a fixed W, P and x1, . . . , x2n, the random variables (s1, . . . , s2n) are a permutation distribution,\n so negatively associated. Then the variables si \u00b7 dT V (pW,P (y|xi), pW \u2217,P \u2217(y|xi)) are monotone\n functions of si, so also negatively associated. They are also bounded in [\u22121, 1]. Hence we can apply\n a Chernoff bound:\n                                        Pr[|fn(W, P        ) \u2212   f \u2032                                                                    (43)\n for any fixed W, P         .                                     n(W, P     )| > \u03b5] < e\u2212n\u03b52/2,\n Constructing a net.               We will first construct a net over the precision matrices P (independent of\n W   ), and then for each element in the P                -net, we will construct a net over W                .\n Net over \u03a3\u22121.           In the following, \u03bbmax(P             ) denotes max eigenvalue of a matrix P                  , \u03bbmin(P   ) denotes\n the min eigenvalue, and \u03bbi(P              ) denotes the i-th eigenvalue, in decreasing order.\n In order to construct the net over the precision matrices, we will consider geometrically spaced values\n of \u03bb \u2208    [L \u00b7 \u03bbmin(P \u2217), U \u00b7 \u03bbmax(P \u2217)], and for each \u03bb, we will construct a net over matrices that have\n max eigenvalue \u2264           \u03bb.\n Now consider \u03bb > 0 that lies in the following discrete set:\n                                                   \u03bbmin(P \u2217)2j, j \u2208         \u2308log2(\u03ba UL )\u2309\n This set is a geometric partition over the possible max eigenvalues that the MLE can return.\n Following definition C.12, let \u2126\u03bb denote the subset of positive definite matrices in Rd\u00d7d that have\n condition number \u03ba and max-eigenvalue in                         \u03bb2 , \u03bb   . Similarly, following Definition C.12, let                 \u2126\u03bb,\u03b2\n denote the gridded version of \u2126\u03bb, where entries in the matrix are multiples of \u03bb\u03b2.\n For any P \u2208        \u2126\u03bb, let    P \u2208    \u2126\u03bb,\u03b2 be the matrix obtained by rounding down every element in P                               .\n By the Data Processing Inequality, for any W \u2208                        Rd\u00d7k, we have\n                          dT V    pW,P (y|x), pW,        P (y|x)      \u2264   dT V (N    (W    x; P  ), N   (W   x; P  )).\n By Lemma C.18, we can upper bound the RHS of the above inequality by\n Setting                                 dT V (N    (W   x; P   ), N  (W    x; P  )) \u2264    O(d2\u03b2\u03ba).\n                                                              \u03b2 = O         \u03b5     ,\n                                                                          d2\u03ba\n we have a partition of size O              (d2\u03ba/\u03b5)d2         per \u03bb such that:\n                                            dT V     pW,P (y|x), pW,       P (y|x)      \u2264   O(\u03b5).\nWe will now construct a net over W                   , so as to show Eqn (43) for all W, P                .\n W   -net.     By repeated triangle inequalities, we have\n |fn(W, P      ) \u2212  f \u2032                  fn(W, P      ) \u2212   fn(W,     P  )   +   fn(W,     P  ) \u2212   f \u2032       P  )  +    f \u2032     P  ) \u2212   f \u2032         .\n                      n(W, P     )| \u2264                                                                 n(W,                 n(W,             n(W, P  )\n Using the cover over P           , the first and last term on the RHS are O(\u03b5). This gives\n                            |fn(W, P      ) \u2212   f \u2032                              fn(W,     P  ) \u2212   f \u2032       P  ) .                    (44)\n                                                  n(W, P     )| \u2264   O(\u03b5) +                            n(W,\n                                                                       44", "md": "Symmetrization. Since $$D_x$$ and $$D'_x$$ each have n independent samples, we could instead draw the datasets by first sampling 2n elements $$x_1, ..., x_{2n}$$ from $$D_x$$, then randomly partition this sample into two equal datasets. Let $$s_i \\in \\{\u00b11\\}$$ so $$s_i = 1$$ if $$z_i$$ lies in $$D'_x$$ and $$-1$$ if it lies in $$D_x$$. Then\n\n$$\nf_n(W, P) - f'_{2n} = \\sum_{i=1}^{n} si \\cdot dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i)).\n$$\nFor a fixed W, P and $$x_1, ..., x_{2n}$$, the random variables $$(s_1, ..., s_{2n})$$ are a permutation distribution, so negatively associated. Then the variables $$si \\cdot dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i))$$ are monotone functions of $$s_i$$, so also negatively associated. They are also bounded in [-1, 1]. Hence we can apply a Chernoff bound:\n\n$$\nPr[|f_n(W, P) - f'_{n}| > \\epsilon] < e^{-n\\epsilon^2/2}.\n$$\nConstructing a net. We will first construct a net over the precision matrices P (independent of W), and then for each element in the P-net, we will construct a net over W.\n\nNet over $$\\Sigma^{-1}$$. In the following, $$\\lambda_{\\text{max}}(P)$$ denotes max eigenvalue of a matrix P, $$\\lambda_{\\text{min}}(P)$$ denotes the min eigenvalue, and $$\\lambda_i(P)$$ denotes the i-th eigenvalue, in decreasing order.\n\nIn order to construct the net over the precision matrices, we will consider geometrically spaced values of $$\\lambda \\in [L \\cdot \\lambda_{\\text{min}}(P^*), U \\cdot \\lambda_{\\text{max}}(P^*)]$$, and for each $$\\lambda$$, we will construct a net over matrices that have max eigenvalue $$\\leq \\lambda$$.\n\nNow consider $$\\lambda > 0$$ that lies in the following discrete set:\n\n$$\n\\lambda_{\\text{min}}(P^*)2^j, j \\in \\lceil \\log_2(\\kappa UL) \\rceil\n$$\nThis set is a geometric partition over the possible max eigenvalues that the MLE can return. Following definition C.12, let $$\\Omega_\\lambda$$ denote the subset of positive definite matrices in $$\\mathbb{R}^{d \\times d}$$ that have condition number $$\\kappa$$ and max-eigenvalue in $$[\\lambda/2, \\lambda]$$. Similarly, following Definition C.12, let $$\\Omega_{\\lambda,\\beta}$$ denote the gridded version of $$\\Omega_\\lambda$$, where entries in the matrix are multiples of $$\\lambda^\\beta$$.\n\nFor any $$P \\in \\Omega_\\lambda$$, let $$P' \\in \\Omega_{\\lambda,\\beta}$$ be the matrix obtained by rounding down every element in P.\n\nBy the Data Processing Inequality, for any $$W \\in \\mathbb{R}^{d \\times k}$$, we have\n\n$$\ndT V(p_{W,P}(y|x), p_{W',P'}(y|x)) \\leq dT V(N(Wx;P), N(Wx;P')).\n$$\nBy Lemma C.18, we can upper bound the RHS of the above inequality by setting\n\n$$\ndT V(N(Wx;P), N(Wx;P')) \\leq O(d^2\\beta\\kappa).\n$$\nwe have a partition of size $$O((d^2\\kappa/\\epsilon)d^2)$$ per $$\\lambda$$ such that:\n\n$$\ndT V(p_{W,P}(y|x), p_{W',P'}(y|x)) \\leq O(\\epsilon).\n$$\nWe will now construct a net over W, so as to show Eqn (43) for all W, P.\n\nW-net. By repeated triangle inequalities, we have\n\n$$\n|f_n(W, P) - f'_{n}| \\leq |f_n(W, P) - f_n(W, P') + f_n(W, P') - f'_{P'} + f'_{P'} - f'_{n}| \\leq O(\\epsilon).\n$$", "images": [], "items": [{"type": "text", "value": "Symmetrization. Since $$D_x$$ and $$D'_x$$ each have n independent samples, we could instead draw the datasets by first sampling 2n elements $$x_1, ..., x_{2n}$$ from $$D_x$$, then randomly partition this sample into two equal datasets. Let $$s_i \\in \\{\u00b11\\}$$ so $$s_i = 1$$ if $$z_i$$ lies in $$D'_x$$ and $$-1$$ if it lies in $$D_x$$. Then\n\n$$\nf_n(W, P) - f'_{2n} = \\sum_{i=1}^{n} si \\cdot dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i)).\n$$\nFor a fixed W, P and $$x_1, ..., x_{2n}$$, the random variables $$(s_1, ..., s_{2n})$$ are a permutation distribution, so negatively associated. Then the variables $$si \\cdot dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i))$$ are monotone functions of $$s_i$$, so also negatively associated. They are also bounded in [-1, 1]. Hence we can apply a Chernoff bound:\n\n$$\nPr[|f_n(W, P) - f'_{n}| > \\epsilon] < e^{-n\\epsilon^2/2}.\n$$\nConstructing a net. We will first construct a net over the precision matrices P (independent of W), and then for each element in the P-net, we will construct a net over W.\n\nNet over $$\\Sigma^{-1}$$. In the following, $$\\lambda_{\\text{max}}(P)$$ denotes max eigenvalue of a matrix P, $$\\lambda_{\\text{min}}(P)$$ denotes the min eigenvalue, and $$\\lambda_i(P)$$ denotes the i-th eigenvalue, in decreasing order.\n\nIn order to construct the net over the precision matrices, we will consider geometrically spaced values of $$\\lambda \\in [L \\cdot \\lambda_{\\text{min}}(P^*), U \\cdot \\lambda_{\\text{max}}(P^*)]$$, and for each $$\\lambda$$, we will construct a net over matrices that have max eigenvalue $$\\leq \\lambda$$.\n\nNow consider $$\\lambda > 0$$ that lies in the following discrete set:\n\n$$\n\\lambda_{\\text{min}}(P^*)2^j, j \\in \\lceil \\log_2(\\kappa UL) \\rceil\n$$\nThis set is a geometric partition over the possible max eigenvalues that the MLE can return. Following definition C.12, let $$\\Omega_\\lambda$$ denote the subset of positive definite matrices in $$\\mathbb{R}^{d \\times d}$$ that have condition number $$\\kappa$$ and max-eigenvalue in $$[\\lambda/2, \\lambda]$$. Similarly, following Definition C.12, let $$\\Omega_{\\lambda,\\beta}$$ denote the gridded version of $$\\Omega_\\lambda$$, where entries in the matrix are multiples of $$\\lambda^\\beta$$.\n\nFor any $$P \\in \\Omega_\\lambda$$, let $$P' \\in \\Omega_{\\lambda,\\beta}$$ be the matrix obtained by rounding down every element in P.\n\nBy the Data Processing Inequality, for any $$W \\in \\mathbb{R}^{d \\times k}$$, we have\n\n$$\ndT V(p_{W,P}(y|x), p_{W',P'}(y|x)) \\leq dT V(N(Wx;P), N(Wx;P')).\n$$\nBy Lemma C.18, we can upper bound the RHS of the above inequality by setting\n\n$$\ndT V(N(Wx;P), N(Wx;P')) \\leq O(d^2\\beta\\kappa).\n$$\nwe have a partition of size $$O((d^2\\kappa/\\epsilon)d^2)$$ per $$\\lambda$$ such that:\n\n$$\ndT V(p_{W,P}(y|x), p_{W',P'}(y|x)) \\leq O(\\epsilon).\n$$\nWe will now construct a net over W, so as to show Eqn (43) for all W, P.\n\nW-net. By repeated triangle inequalities, we have\n\n$$", "md": "Symmetrization. Since $$D_x$$ and $$D'_x$$ each have n independent samples, we could instead draw the datasets by first sampling 2n elements $$x_1, ..., x_{2n}$$ from $$D_x$$, then randomly partition this sample into two equal datasets. Let $$s_i \\in \\{\u00b11\\}$$ so $$s_i = 1$$ if $$z_i$$ lies in $$D'_x$$ and $$-1$$ if it lies in $$D_x$$. Then\n\n$$\nf_n(W, P) - f'_{2n} = \\sum_{i=1}^{n} si \\cdot dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i)).\n$$\nFor a fixed W, P and $$x_1, ..., x_{2n}$$, the random variables $$(s_1, ..., s_{2n})$$ are a permutation distribution, so negatively associated. Then the variables $$si \\cdot dT V(p_{W,P}(y|x_i), p_{W^*,P^*}(y|x_i))$$ are monotone functions of $$s_i$$, so also negatively associated. They are also bounded in [-1, 1]. Hence we can apply a Chernoff bound:\n\n$$\nPr[|f_n(W, P) - f'_{n}| > \\epsilon] < e^{-n\\epsilon^2/2}.\n$$\nConstructing a net. We will first construct a net over the precision matrices P (independent of W), and then for each element in the P-net, we will construct a net over W.\n\nNet over $$\\Sigma^{-1}$$. In the following, $$\\lambda_{\\text{max}}(P)$$ denotes max eigenvalue of a matrix P, $$\\lambda_{\\text{min}}(P)$$ denotes the min eigenvalue, and $$\\lambda_i(P)$$ denotes the i-th eigenvalue, in decreasing order.\n\nIn order to construct the net over the precision matrices, we will consider geometrically spaced values of $$\\lambda \\in [L \\cdot \\lambda_{\\text{min}}(P^*), U \\cdot \\lambda_{\\text{max}}(P^*)]$$, and for each $$\\lambda$$, we will construct a net over matrices that have max eigenvalue $$\\leq \\lambda$$.\n\nNow consider $$\\lambda > 0$$ that lies in the following discrete set:\n\n$$\n\\lambda_{\\text{min}}(P^*)2^j, j \\in \\lceil \\log_2(\\kappa UL) \\rceil\n$$\nThis set is a geometric partition over the possible max eigenvalues that the MLE can return. Following definition C.12, let $$\\Omega_\\lambda$$ denote the subset of positive definite matrices in $$\\mathbb{R}^{d \\times d}$$ that have condition number $$\\kappa$$ and max-eigenvalue in $$[\\lambda/2, \\lambda]$$. Similarly, following Definition C.12, let $$\\Omega_{\\lambda,\\beta}$$ denote the gridded version of $$\\Omega_\\lambda$$, where entries in the matrix are multiples of $$\\lambda^\\beta$$.\n\nFor any $$P \\in \\Omega_\\lambda$$, let $$P' \\in \\Omega_{\\lambda,\\beta}$$ be the matrix obtained by rounding down every element in P.\n\nBy the Data Processing Inequality, for any $$W \\in \\mathbb{R}^{d \\times k}$$, we have\n\n$$\ndT V(p_{W,P}(y|x), p_{W',P'}(y|x)) \\leq dT V(N(Wx;P), N(Wx;P')).\n$$\nBy Lemma C.18, we can upper bound the RHS of the above inequality by setting\n\n$$\ndT V(N(Wx;P), N(Wx;P')) \\leq O(d^2\\beta\\kappa).\n$$\nwe have a partition of size $$O((d^2\\kappa/\\epsilon)d^2)$$ per $$\\lambda$$ such that:\n\n$$\ndT V(p_{W,P}(y|x), p_{W',P'}(y|x)) \\leq O(\\epsilon).\n$$\nWe will now construct a net over W, so as to show Eqn (43) for all W, P.\n\nW-net. By repeated triangle inequalities, we have\n\n$$"}, {"type": "table", "rows": [["f_n(W, P) - f'_{n}", "\\leq", "f_n(W, P) - f_n(W, P') + f_n(W, P') - f'_{P'} + f'_{P'} - f'_{n}"]], "md": "|f_n(W, P) - f'_{n}| \\leq |f_n(W, P) - f_n(W, P') + f_n(W, P') - f'_{P'} + f'_{P'} - f'_{n}| \\leq O(\\epsilon).", "isPerfectTable": true, "csv": "\"f_n(W, P) - f'_{n}\",\"\\leq\",\"f_n(W, P) - f_n(W, P') + f_n(W, P') - f'_{P'} + f'_{P'} - f'_{n}\""}, {"type": "text", "value": "$$", "md": "$$"}]}, {"page": 45, "text": "For a fixed       W   , P  , we will have (43) using a Chernoff bound. Since                                P is already finite, we will now\nconstruct a net over W for each                     P  .\nIt is sufficient to bound dT V (p                W, P (y|xi), pW \u2032,       P (y|xi)) as the triangle inequality implies that this is\nlarger than the RHS above.\nWe want, for any W, W \u2032 in a cell,\n                |dT V (pW,     P (y|xi), pW \u2217,P \u2217(y|xi)) \u2212                dT V (pW \u2032,      P (y|xi), pW \u2217,P \u2217(y|xi)| \u2264                O(\u03b5).\nfor all i \u2208     [2n]. It is also sufficient to bound dT V (pW,                      P (y|xi), pW \u2032,       P (y|xi)) as the triangle inequality\nimplies that this is larger than the left hand side above.\nLemma C.19 implies that we can find O                                d       log( 2d \u03f5 )      per row of W such that for any W, W \u2032 in\n                                                                   \u03f53/2\na cell, either\nwhich implies   dT V (p    W,  P (y|xi), pW \u2217,P \u2217(y|xi)) \u2265                dT V (pW,      P (yj|xi), pW \u2217,P \u2217(yj|xi) \u2265                 1 \u2212    \u03b5\nor            dT V (p                              dT V (p    W,  P (y|xi), pW \u2032,       P (y|xi) \u2264        \u03f5\n                        W,  P (y|xi), pW \u2032,       P (y|xi) \u2264          j    dT V (pW,     P (y|xi)j|zi, pW \u2032,         P (yj|xi)) \u2264         \u03f5/d.\nTherefore, for each i regardless of the value of W \u2217                          j zi there are at most O                  d       log( 2d  \u03f5 )     partition-\n                                                                                                                      \u03f53/2\ning hyperplanes.\nWe then take the intersection of all 2n partitions (for each data point zi). The cells of this partition\nare defined by 2n sets of O                       d        log( 2d \u03f5 )     parallel hyperplanes. Since z \u2208                       Rk, the number of\n                                                \u03f53/2\ncells is at most O                 nd        log   2d\u03f5 k          .\n                                  \u03f53/2\nHence the total number of cells for d rows is at most O                                         nd       log   2d\u03f5 dk           .\n                                                                                               \u03f53/2\nPutting everything together.                         Finally, for any W \u2208               Rd let     W \u2208       N  P be the representative of its\ncell. Recall that each representative                     P of P induces a different cover N                      P over W        . Let N be the net\nover the precision matrices P                  .\nBy definition of the cells,\n                dT V (pW,      P (y|xi), pW \u2217,P \u2217(y|xi)) \u2212                dT V (p    W , P (y|xi), pW \u2217,\u03a3\u2217(y|xi))                 < O(\u03b5).\nfor all i \u2208      [2n]. Thus   f \u2032        P  ) \u2212    fn(W, P       )    \u2212     f \u2032   W   , P  ) \u2212    fn(  W    , P  )     \u2264   O(\u03b5).\nand so                          n(W, Pr[             sup                n(W, Pn(     ) \u2212   fn(W, P       )| > \u03b5]\n                                           W \u2208Rd\u00d7k,P \u2208Rd\u00d7d|f \u2032\n                                            \u2264   Pr          max            n(W, P       ) \u2212   fn(W, P       )| > \u03b5\n                                                      w\u2208NP ,P \u2208N|f \u2032                                                 4\n                                            \u2264   elog |N|+log |NP |\u2212( \u03b5       4 )2n/2\nAs there are log \u03ba U         L partitions over P (corresponding to the maximum possible eigenvalue of P                                                      ),\neach with (O( d2\u03ba        \u03b5 ))d2 elements, we have\n                                             log |N| \u2272        d2 log     d2\u03ba \u03b5       + log log         \u03baU L      .\n                                                                             45", "md": "For a fixed \\(W\\), \\(P\\), we will have (43) using a Chernoff bound. Since \\(P\\) is already finite, we will now construct a net over \\(W\\) for each \\(P\\). It is sufficient to bound \\(dT V(pW, P(y|xi), pW', P(y|xi))\\) as the triangle inequality implies that this is larger than the RHS above.\n\nWe want, for any \\(W\\), \\(W'\\) in a cell,\n\n\\[\n|dT V(pW, P(y|xi), pW^*, P^*(y|xi)) - dT V(pW', P(y|xi), pW^*, P^*(y|xi))| \\leq O(\\epsilon)\n\\]\n\nfor all \\(i \\in [2n]\\). It is also sufficient to bound \\(dT V(pW, P(y|xi), pW', P(y|xi))\\) as the triangle inequality implies that this is larger than the left-hand side above. Lemma C.19 implies that we can find \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) per row of \\(W\\) such that for any \\(W\\), \\(W'\\) in a cell, either\n\n\\[\n\\begin{align*}\n&\\text{dT V}(pW, P(y|xi), pW^*, P^*(y|xi)) \\geq \\text{dT V}(pW, P(y_j|xi), pW^*, P^*(y_j|xi)) \\geq 1 - \\epsilon \\\\\n\\text{or} &\\quad \\text{dT V}(pW, P(y|xi), pW', P(y|xi)) \\leq \\epsilon \\\\\n&\\quad \\text{dT V}(pW, P(y|xi), pW', P(y|xi)) \\leq \\epsilon/d.\n\\end{align*}\n\\]\n\nTherefore, for each \\(i\\) regardless of the value of \\(W^*\\) there are at most \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) partitioning hyperplanes. We then take the intersection of all \\(2n\\) partitions (for each data point \\(z_i\\)). The cells of this partition are defined by \\(2n\\) sets of \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) parallel hyperplanes. Since \\(z \\in \\mathbb{R}^k\\), the number of cells is at most \\(O(nd \\log(2d\\epsilon k)/\\epsilon^{3/2})\\). Hence the total number of cells for \\(d\\) rows is at most \\(O(nd \\log(2d\\epsilon dk)/\\epsilon^{3/2})\\).\n\nPutting everything together, finally, for any \\(W \\in \\mathbb{R}^d\\) let \\(W \\in N_P\\) be the representative of its cell. Recall that each representative \\(P\\) of \\(P\\) induces a different cover \\(N_P\\) over \\(W\\). Let \\(N\\) be the net over the precision matrices \\(P\\).\n\nBy definition of the cells,\n\n\\[\n\\text{dT V}(pW, P(y|xi), pW^*, P^*(y|xi)) - \\text{dT V}(pW, P(y|xi), pW^*, \\Sigma^*(y|xi)) < O(\\epsilon)\n\\]\n\nfor all \\(i \\in [2n]\\). Thus\n\n\\[\nf'(P) - f_n(W, P) - f'(W, P) - f_n(W, P) \\leq O(\\epsilon)\n\\]\n\nand so\n\n\\[\nPr\\left[\\sup_{W \\in \\mathbb{R}^{d \\times k}, P \\in \\mathbb{R}^{d \\times d}}|f'(W, P) - f_n(W, P)| > \\epsilon\\right] \\leq Pr\\left[\\max_{w \\in N_P, P \\in N}|f'(W, P) - f_n(W, P)| > \\epsilon\\right] \\leq e^{\\log|N| + \\log|N_P| - (\\epsilon/4)^{2n/2}}\n\\]\n\nAs there are \\(\\log \\kappa U L\\) partitions over \\(P\\) (corresponding to the maximum possible eigenvalue of \\(P\\)), each with \\((O(d^2\\kappa/\\epsilon))^d\\) elements, we have\n\n\\[\n\\log|N| \\lesssim d^2 \\log(d^2\\kappa/\\epsilon) + \\log\\log\\kappa U L.\n\\]", "images": [], "items": [{"type": "text", "value": "For a fixed \\(W\\), \\(P\\), we will have (43) using a Chernoff bound. Since \\(P\\) is already finite, we will now construct a net over \\(W\\) for each \\(P\\). It is sufficient to bound \\(dT V(pW, P(y|xi), pW', P(y|xi))\\) as the triangle inequality implies that this is larger than the RHS above.\n\nWe want, for any \\(W\\), \\(W'\\) in a cell,\n\n\\[", "md": "For a fixed \\(W\\), \\(P\\), we will have (43) using a Chernoff bound. Since \\(P\\) is already finite, we will now construct a net over \\(W\\) for each \\(P\\). It is sufficient to bound \\(dT V(pW, P(y|xi), pW', P(y|xi))\\) as the triangle inequality implies that this is larger than the RHS above.\n\nWe want, for any \\(W\\), \\(W'\\) in a cell,\n\n\\["}, {"type": "table", "rows": [["dT V(pW, P(y", "xi), pW^*, P^*(y", "xi)) - dT V(pW', P(y", "xi), pW^*, P^*(y", "xi))"]], "md": "|dT V(pW, P(y|xi), pW^*, P^*(y|xi)) - dT V(pW', P(y|xi), pW^*, P^*(y|xi))| \\leq O(\\epsilon)", "isPerfectTable": true, "csv": "\"dT V(pW, P(y\",\"xi), pW^*, P^*(y\",\"xi)) - dT V(pW', P(y\",\"xi), pW^*, P^*(y\",\"xi))\""}, {"type": "text", "value": "\\]\n\nfor all \\(i \\in [2n]\\). It is also sufficient to bound \\(dT V(pW, P(y|xi), pW', P(y|xi))\\) as the triangle inequality implies that this is larger than the left-hand side above. Lemma C.19 implies that we can find \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) per row of \\(W\\) such that for any \\(W\\), \\(W'\\) in a cell, either\n\n\\[\n\\begin{align*}\n&\\text{dT V}(pW, P(y|xi), pW^*, P^*(y|xi)) \\geq \\text{dT V}(pW, P(y_j|xi), pW^*, P^*(y_j|xi)) \\geq 1 - \\epsilon \\\\\n\\text{or} &\\quad \\text{dT V}(pW, P(y|xi), pW', P(y|xi)) \\leq \\epsilon \\\\\n&\\quad \\text{dT V}(pW, P(y|xi), pW', P(y|xi)) \\leq \\epsilon/d.\n\\end{align*}\n\\]\n\nTherefore, for each \\(i\\) regardless of the value of \\(W^*\\) there are at most \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) partitioning hyperplanes. We then take the intersection of all \\(2n\\) partitions (for each data point \\(z_i\\)). The cells of this partition are defined by \\(2n\\) sets of \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) parallel hyperplanes. Since \\(z \\in \\mathbb{R}^k\\), the number of cells is at most \\(O(nd \\log(2d\\epsilon k)/\\epsilon^{3/2})\\). Hence the total number of cells for \\(d\\) rows is at most \\(O(nd \\log(2d\\epsilon dk)/\\epsilon^{3/2})\\).\n\nPutting everything together, finally, for any \\(W \\in \\mathbb{R}^d\\) let \\(W \\in N_P\\) be the representative of its cell. Recall that each representative \\(P\\) of \\(P\\) induces a different cover \\(N_P\\) over \\(W\\). Let \\(N\\) be the net over the precision matrices \\(P\\).\n\nBy definition of the cells,\n\n\\[\n\\text{dT V}(pW, P(y|xi), pW^*, P^*(y|xi)) - \\text{dT V}(pW, P(y|xi), pW^*, \\Sigma^*(y|xi)) < O(\\epsilon)\n\\]\n\nfor all \\(i \\in [2n]\\). Thus\n\n\\[\nf'(P) - f_n(W, P) - f'(W, P) - f_n(W, P) \\leq O(\\epsilon)\n\\]\n\nand so\n\n\\[\nPr\\left[\\sup_{W \\in \\mathbb{R}^{d \\times k}, P \\in \\mathbb{R}^{d \\times d}}|f'(W, P) - f_n(W, P)| > \\epsilon\\right] \\leq Pr\\left[\\max_{w \\in N_P, P \\in N}|f'(W, P) - f_n(W, P)| > \\epsilon\\right] \\leq e^{\\log|N| + \\log|N_P| - (\\epsilon/4)^{2n/2}}\n\\]\n\nAs there are \\(\\log \\kappa U L\\) partitions over \\(P\\) (corresponding to the maximum possible eigenvalue of \\(P\\)), each with \\((O(d^2\\kappa/\\epsilon))^d\\) elements, we have\n\n\\[\n\\log|N| \\lesssim d^2 \\log(d^2\\kappa/\\epsilon) + \\log\\log\\kappa U L.\n\\]", "md": "\\]\n\nfor all \\(i \\in [2n]\\). It is also sufficient to bound \\(dT V(pW, P(y|xi), pW', P(y|xi))\\) as the triangle inequality implies that this is larger than the left-hand side above. Lemma C.19 implies that we can find \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) per row of \\(W\\) such that for any \\(W\\), \\(W'\\) in a cell, either\n\n\\[\n\\begin{align*}\n&\\text{dT V}(pW, P(y|xi), pW^*, P^*(y|xi)) \\geq \\text{dT V}(pW, P(y_j|xi), pW^*, P^*(y_j|xi)) \\geq 1 - \\epsilon \\\\\n\\text{or} &\\quad \\text{dT V}(pW, P(y|xi), pW', P(y|xi)) \\leq \\epsilon \\\\\n&\\quad \\text{dT V}(pW, P(y|xi), pW', P(y|xi)) \\leq \\epsilon/d.\n\\end{align*}\n\\]\n\nTherefore, for each \\(i\\) regardless of the value of \\(W^*\\) there are at most \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) partitioning hyperplanes. We then take the intersection of all \\(2n\\) partitions (for each data point \\(z_i\\)). The cells of this partition are defined by \\(2n\\) sets of \\(O(d \\log(2d/\\epsilon)/\\epsilon^{3/2})\\) parallel hyperplanes. Since \\(z \\in \\mathbb{R}^k\\), the number of cells is at most \\(O(nd \\log(2d\\epsilon k)/\\epsilon^{3/2})\\). Hence the total number of cells for \\(d\\) rows is at most \\(O(nd \\log(2d\\epsilon dk)/\\epsilon^{3/2})\\).\n\nPutting everything together, finally, for any \\(W \\in \\mathbb{R}^d\\) let \\(W \\in N_P\\) be the representative of its cell. Recall that each representative \\(P\\) of \\(P\\) induces a different cover \\(N_P\\) over \\(W\\). Let \\(N\\) be the net over the precision matrices \\(P\\).\n\nBy definition of the cells,\n\n\\[\n\\text{dT V}(pW, P(y|xi), pW^*, P^*(y|xi)) - \\text{dT V}(pW, P(y|xi), pW^*, \\Sigma^*(y|xi)) < O(\\epsilon)\n\\]\n\nfor all \\(i \\in [2n]\\). Thus\n\n\\[\nf'(P) - f_n(W, P) - f'(W, P) - f_n(W, P) \\leq O(\\epsilon)\n\\]\n\nand so\n\n\\[\nPr\\left[\\sup_{W \\in \\mathbb{R}^{d \\times k}, P \\in \\mathbb{R}^{d \\times d}}|f'(W, P) - f_n(W, P)| > \\epsilon\\right] \\leq Pr\\left[\\max_{w \\in N_P, P \\in N}|f'(W, P) - f_n(W, P)| > \\epsilon\\right] \\leq e^{\\log|N| + \\log|N_P| - (\\epsilon/4)^{2n/2}}\n\\]\n\nAs there are \\(\\log \\kappa U L\\) partitions over \\(P\\) (corresponding to the maximum possible eigenvalue of \\(P\\)), each with \\((O(d^2\\kappa/\\epsilon))^d\\) elements, we have\n\n\\[\n\\log|N| \\lesssim d^2 \\log(d^2\\kappa/\\epsilon) + \\log\\log\\kappa U L.\n\\]"}]}, {"page": 46, "text": "and each cover NP over W has size\n                                          log |NP | = 2kd log               d       log  2d \u03f5        .\n                                                                         \u03f53/2\nThis implies that\n                                        n = C \u00b7       kd + d2          log   kd\u03ba      log   U          ,\n                                                            \u03b52                   \u03b5            L\nsuffices for                        Pr[sup      f \u2032\n                                         W,P     n(W, P      ) \u2212  fn(W, P      ) > \u03b5] < e\u2212\u2126(\u03b52n).\nLemma C.19. Let y = \u03d5(\u00b5\u2217                    + \u03b7\u03c3\u2217) where \u00b5\u2217, \u03c3\u2217            are fixed, and y\u00b5,\u03c3 = \u03d5(\u00b5 + \u03b7\u03c3). We partition\nthe space R of \u00b5 s.t. for \u00b5, \u00b5\u2032 in a cell, either\nor                                              dT V (p\u00b5,\u03c3(y), p\u00b5\u2032,\u03c3(y)) \u2264             \u03f5/2d.\n               dT V (p\u00b5,\u03c3(y), p\u00b5\u2217,\u03c3\u2217(y)) \u2265              1 \u2212   \u03f5     and       dT V (p\u00b5\u2032,\u03c3(y), p\u00b5\u2217,\u03c3\u2217(y)) \u2265              1 \u2212   \u03f5.\nThen the number of cells is at most O(                    d      log( 2d \u03f5 )).\n                                                        \u03f53/2\nProof. In one dimension\n                               dT V (p\u00b5,\u03c3(y), p\u00b5\u2217,\u03c3\u2217(y)) = dT V (pc\u00b5,c\u03c3(y), pc\u00b5\u2217,c\u03c3\u2217(y))\nwhere c is a constant. So, we can assume WLOG that \u03c3\u2217                               = 1. The number of cells in the grid only\ndepends on \u03c3/\u03c3\u2217.\nNow, we show that, regardless of the value of \u00b5\u2217                      we only need to make a grid on a segment of length\nat most 3 max(\u03c3, 1)              log(2d/\u03f5). This is because for any \u00b5 outside the ranges specified below the\ndT V (p\u00b5,\u03c3(y), p\u00b5\u2217,\u03c3\u2217(y)) \u2265              1 \u2212   \u03f5.\n          \u2022 If \u00b5\u2217         \u2264      \u2212     log(2d/\u03f5) and for any \u00b5 such that \u00b5                                  \u2265      \u03c3     log(2d/\u03f5),        the\n             dT V (p\u00b5\u2217,\u03c3\u2217(y), p\u00b5,\u03c3(y)) \u2265               the difference in the probabilities at 0 which is bigger than\n             1 \u2212   \u03f5.\n          \u2022 If 0 \u2265         \u00b5\u2217    \u2265     \u2212     log(2d/\u03f5) and for any \u00b5 s.t.                    \u00b5 \u2265       max(\u03c3, 1)          log(2d/\u03f5), the\n             dT V (p\u00b5\u2217,\u03c3\u2217(y), p\u00b5,\u03c3(y)) is the same as in the linear case and since, \u00b5 \u2212                                               \u00b5\u2217     \u2265\n             max(\u03c3, 1)         log(2d/\u03f5), the dT V (p\u00b5\u2217,\u03c3\u2217(y), p\u00b5,\u03c3(y)) \u2265                     1 \u2212   \u03f5.\n          \u2022 If 0 \u2264         \u00b5\u2217    \u2264        log(2d/\u03f5), for any \u00b5 s.t.                 \u00b5 \u2212     \u00b5\u2217    \u2265    max(\u03c3, 1)          log(2d/\u03f5), the\n             dT V (p\u00b5\u2217,\u03c3\u2217(y), p\u00b5,\u03c3(y)) \u2265              1 \u2212   \u03f5.\n          \u2022 If \u00b5\u2217    \u2265       log(2d/\u03f5) then for any \u00b5 s.t. \u00b5 \u2212                 \u00b5\u2217   \u2265   max(\u03c3, 1)         log(2d/\u03f5) using the same\n             argument as above, we have, the dT V (p\u00b5\u2217,\u03c3\u2217(y), p\u00b5,\u03c3(y)) \u2265                              1 \u2212    \u03f5. Moreover, this is also\n             true for \u00b5 s.t. \u2212\u03c3           log(2d/\u03f5) \u2264        \u00b5 \u2264    \u00b5\u2217   \u2212  max(\u03c3, 1)          log(2d/\u03f5). Therefore, in this case,\n             we have an additional cell.\nIn addition to the above, for any \u00b5, \u00b5\u2032 \u2264                  \u2212\u03c3       log(2d/\u03f5), the dT V (p\u00b5,\u03c3(y), p\u00b5\u2032,\u03c3(y)) \u2264                    \u03f5/2d since\nboth y\u00b5,\u03c3, y\u00b5\u2032,\u03c3 are only non-zero with probability at most \u03f5/2d. Therefore, for all the above cases,\nwe only need to partition a segment of length at most 3 max(\u03c3, 1)                                  log(2d/\u03f5)\nMoreover, for \u03c3 sufficiently small we can do better. We only need to partition a space of \u03c3                                      log(2d/\u03f5).\nThis is primarily because when \u03c3 sufficiently small, for any \u00b5 in the linear case we have that\ndT V (p\u00b5,\u03c3(y), p\u00b5\u2217,\u03c3\u2217(y)) \u2265              1 \u2212   \u03f5.\n                                                                      46", "md": "and each cover $$NP$$ over $$W$$ has size\n\n$$\n\\log |NP| = 2kd \\log d \\frac{\\log 2d \\epsilon}{\\epsilon^{3/2}}\n$$\nThis implies that\n\n$$\nn = C \\cdot \\frac{kd + d^2 \\log kd\\kappa}{\\epsilon^2 \\log U} L\n$$\nsuffices for\n\n$$\nPr\\left[\\sup_{W,P} \\left| n(W, P) - f_n(W, P) \\right| > \\epsilon\\right] < e^{-\\Omega(\\epsilon^2n)}\n$$\nLemma C.19. Let $$y = \\phi(\\mu^* + \\eta\\sigma^*)$$ where $$\\mu^*, \\sigma^*$$ are fixed, and $$y_{\\mu,\\sigma} = \\phi(\\mu + \\eta\\sigma)$$. We partition the space $$R$$ of $$\\mu$$ s.t. for $$\\mu, \\mu'$$ in a cell, either\n\nor\n\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\frac{\\epsilon}{2d}\n$$\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon \\quad \\text{and} \\quad dT V(p_{\\mu',\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon\n$$\nThen the number of cells is at most $$O\\left(\\frac{d \\log(2d\\epsilon)}{\\epsilon^{3/2}}\\right)$$.\n\nProof. In one dimension\n\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) = dT V(p_{c\\mu,c\\sigma}(y), p_{c\\mu^*,c\\sigma^*}(y))\n$$\nwhere $$c$$ is a constant. So, we can assume WLOG that $$\\sigma^* = 1$$. The number of cells in the grid only depends on $$\\sigma/\\sigma^*$$. Now, we show that, regardless of the value of $$\\mu^*$$ we only need to make a grid on a segment of length at most $$3 \\max(\\sigma, 1) \\log(2d/\\epsilon)$$. This is because for any $$\\mu$$ outside the ranges specified below the\n\n- If $\\mu^* \\leq -\\log(2d/\\epsilon)$ and for any $\\mu$ such that $\\mu \\geq \\sigma \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $0 \\geq \\mu^* \\geq -\\log(2d/\\epsilon)$ and for any $\\mu$ s.t. $\\mu \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y))$ is the same as in the linear case and since, $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $0 \\leq \\mu^* \\leq \\log(2d/\\epsilon)$, for any $\\mu$ s.t. $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $\\mu^* \\geq \\log(2d/\\epsilon)$ then for any $\\mu$ s.t. $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$ using the same argument as above, we have, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$. Moreover, this is also true for $\\mu$ s.t. $-\\sigma \\log(2d/\\epsilon) \\leq \\mu \\leq \\mu^* - \\max(\\sigma, 1) \\log(2d/\\epsilon)$. Therefore, in this case, we have an additional cell.\n\nIn addition to the above, for any $$\\mu, \\mu' \\leq -\\sigma \\log(2d/\\epsilon)$$, the $$dT V(p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\epsilon/2d$$ since both $$y_{\\mu,\\sigma}, y_{\\mu',\\sigma}$$ are only non-zero with probability at most $$\\epsilon/2d$$. Therefore, for all the above cases, we only need to partition a segment of length at most $$3 \\max(\\sigma, 1) \\log(2d/\\epsilon)$$. Moreover, for $$\\sigma$$ sufficiently small we can do better. We only need to partition a space of $$\\sigma \\log(2d/\\epsilon)$$. This is primarily because when $$\\sigma$$ sufficiently small, for any $$\\mu$$ in the linear case we have that $$dT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$.\n\n46", "images": [], "items": [{"type": "text", "value": "and each cover $$NP$$ over $$W$$ has size\n\n$$\n\\log |NP| = 2kd \\log d \\frac{\\log 2d \\epsilon}{\\epsilon^{3/2}}\n$$\nThis implies that\n\n$$\nn = C \\cdot \\frac{kd + d^2 \\log kd\\kappa}{\\epsilon^2 \\log U} L\n$$\nsuffices for\n\n$$\nPr\\left[\\sup_{W,P} \\left| n(W, P) - f_n(W, P) \\right| > \\epsilon\\right] < e^{-\\Omega(\\epsilon^2n)}\n$$\nLemma C.19. Let $$y = \\phi(\\mu^* + \\eta\\sigma^*)$$ where $$\\mu^*, \\sigma^*$$ are fixed, and $$y_{\\mu,\\sigma} = \\phi(\\mu + \\eta\\sigma)$$. We partition the space $$R$$ of $$\\mu$$ s.t. for $$\\mu, \\mu'$$ in a cell, either\n\nor\n\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\frac{\\epsilon}{2d}\n$$\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon \\quad \\text{and} \\quad dT V(p_{\\mu',\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon\n$$\nThen the number of cells is at most $$O\\left(\\frac{d \\log(2d\\epsilon)}{\\epsilon^{3/2}}\\right)$$.\n\nProof. In one dimension\n\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) = dT V(p_{c\\mu,c\\sigma}(y), p_{c\\mu^*,c\\sigma^*}(y))\n$$\nwhere $$c$$ is a constant. So, we can assume WLOG that $$\\sigma^* = 1$$. The number of cells in the grid only depends on $$\\sigma/\\sigma^*$$. Now, we show that, regardless of the value of $$\\mu^*$$ we only need to make a grid on a segment of length at most $$3 \\max(\\sigma, 1) \\log(2d/\\epsilon)$$. This is because for any $$\\mu$$ outside the ranges specified below the\n\n- If $\\mu^* \\leq -\\log(2d/\\epsilon)$ and for any $\\mu$ such that $\\mu \\geq \\sigma \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $0 \\geq \\mu^* \\geq -\\log(2d/\\epsilon)$ and for any $\\mu$ s.t. $\\mu \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y))$ is the same as in the linear case and since, $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $0 \\leq \\mu^* \\leq \\log(2d/\\epsilon)$, for any $\\mu$ s.t. $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $\\mu^* \\geq \\log(2d/\\epsilon)$ then for any $\\mu$ s.t. $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$ using the same argument as above, we have, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$. Moreover, this is also true for $\\mu$ s.t. $-\\sigma \\log(2d/\\epsilon) \\leq \\mu \\leq \\mu^* - \\max(\\sigma, 1) \\log(2d/\\epsilon)$. Therefore, in this case, we have an additional cell.\n\nIn addition to the above, for any $$\\mu, \\mu' \\leq -\\sigma \\log(2d/\\epsilon)$$, the $$dT V(p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\epsilon/2d$$ since both $$y_{\\mu,\\sigma}, y_{\\mu',\\sigma}$$ are only non-zero with probability at most $$\\epsilon/2d$$. Therefore, for all the above cases, we only need to partition a segment of length at most $$3 \\max(\\sigma, 1) \\log(2d/\\epsilon)$$. Moreover, for $$\\sigma$$ sufficiently small we can do better. We only need to partition a space of $$\\sigma \\log(2d/\\epsilon)$$. This is primarily because when $$\\sigma$$ sufficiently small, for any $$\\mu$$ in the linear case we have that $$dT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$.\n\n46", "md": "and each cover $$NP$$ over $$W$$ has size\n\n$$\n\\log |NP| = 2kd \\log d \\frac{\\log 2d \\epsilon}{\\epsilon^{3/2}}\n$$\nThis implies that\n\n$$\nn = C \\cdot \\frac{kd + d^2 \\log kd\\kappa}{\\epsilon^2 \\log U} L\n$$\nsuffices for\n\n$$\nPr\\left[\\sup_{W,P} \\left| n(W, P) - f_n(W, P) \\right| > \\epsilon\\right] < e^{-\\Omega(\\epsilon^2n)}\n$$\nLemma C.19. Let $$y = \\phi(\\mu^* + \\eta\\sigma^*)$$ where $$\\mu^*, \\sigma^*$$ are fixed, and $$y_{\\mu,\\sigma} = \\phi(\\mu + \\eta\\sigma)$$. We partition the space $$R$$ of $$\\mu$$ s.t. for $$\\mu, \\mu'$$ in a cell, either\n\nor\n\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\frac{\\epsilon}{2d}\n$$\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon \\quad \\text{and} \\quad dT V(p_{\\mu',\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon\n$$\nThen the number of cells is at most $$O\\left(\\frac{d \\log(2d\\epsilon)}{\\epsilon^{3/2}}\\right)$$.\n\nProof. In one dimension\n\n$$\ndT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) = dT V(p_{c\\mu,c\\sigma}(y), p_{c\\mu^*,c\\sigma^*}(y))\n$$\nwhere $$c$$ is a constant. So, we can assume WLOG that $$\\sigma^* = 1$$. The number of cells in the grid only depends on $$\\sigma/\\sigma^*$$. Now, we show that, regardless of the value of $$\\mu^*$$ we only need to make a grid on a segment of length at most $$3 \\max(\\sigma, 1) \\log(2d/\\epsilon)$$. This is because for any $$\\mu$$ outside the ranges specified below the\n\n- If $\\mu^* \\leq -\\log(2d/\\epsilon)$ and for any $\\mu$ such that $\\mu \\geq \\sigma \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $0 \\geq \\mu^* \\geq -\\log(2d/\\epsilon)$ and for any $\\mu$ s.t. $\\mu \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y))$ is the same as in the linear case and since, $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $0 \\leq \\mu^* \\leq \\log(2d/\\epsilon)$, for any $\\mu$ s.t. $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$.\n- If $\\mu^* \\geq \\log(2d/\\epsilon)$ then for any $\\mu$ s.t. $\\mu - \\mu^* \\geq \\max(\\sigma, 1) \\log(2d/\\epsilon)$ using the same argument as above, we have, the $dT V(p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq 1 - \\epsilon$. Moreover, this is also true for $\\mu$ s.t. $-\\sigma \\log(2d/\\epsilon) \\leq \\mu \\leq \\mu^* - \\max(\\sigma, 1) \\log(2d/\\epsilon)$. Therefore, in this case, we have an additional cell.\n\nIn addition to the above, for any $$\\mu, \\mu' \\leq -\\sigma \\log(2d/\\epsilon)$$, the $$dT V(p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\epsilon/2d$$ since both $$y_{\\mu,\\sigma}, y_{\\mu',\\sigma}$$ are only non-zero with probability at most $$\\epsilon/2d$$. Therefore, for all the above cases, we only need to partition a segment of length at most $$3 \\max(\\sigma, 1) \\log(2d/\\epsilon)$$. Moreover, for $$\\sigma$$ sufficiently small we can do better. We only need to partition a space of $$\\sigma \\log(2d/\\epsilon)$$. This is primarily because when $$\\sigma$$ sufficiently small, for any $$\\mu$$ in the linear case we have that $$dT V(p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$.\n\n46"}]}, {"page": 47, "text": "It is easy to see that dT V (p\u00b5,\u03c3(y), p0,1(y)) \u2265         dT V (p0,\u03c3(y), p0,1(y)). The PDFs of N(0, \u03c3) and\nN(0, 1) intersect at x = \u00b1\u03c3         log(1/\u03c32)    . To show that dT V (p0,\u03c3(y), p0,1(y)) \u2265         1 \u2212   \u03f5, it is now\nsufficient to show that                  1\u2212\u03c32\n                           1 \u2212  2\u03a6(\u2212|x|/\u03c3) \u2265       1 \u2212  \u03f5    and     1 \u2212  2\u03d5(\u2212|x|) \u2264     \u03f5,\nwhere \u03a6(x) is the CDF of the standard normal distribution. By using classical bounds on \u03a6(x), we\nhave that                                                                \u2212log(1/\u03c32)\n                                 \u03a6(\u2212|x|/\u03c3) \u2264      exp\u2212x2/2\u03c32     = exp      2(1\u2212\u03c32)\nwhich is \u2264    \u03f5/2 if \u03c32 \u2264   \u03f5/2. And,                 |x|/\u03c3              |x|/\u03c3\n                                            \u03a6(\u2212|x|) \u2265     |x| exp\u2212x2/2\nwhich is \u2265    (1 \u2212  \u03f5)/2 if                                   x2 + 1\nWhen \u00b5\u2217     \u2264   \u2212    log(2d/\u03f5) the same argument as above shows that if \u00b5 > \u03c3                  log(2d/\u03f5) then the\ndT V (p\u00b5\u2217,\u03c3\u2217(y), p\u00b5,\u03c3(y)) \u2265       the difference in the probabilities at 0 which is bigger than 1 \u2212            \u03f5.We\nconsider the case where \u00b5\u2217       \u2265  \u2212    log(2d/\u03f5.\nSince, when \u03c3 is small, for any \u00b5 in the linear case the TV distance is large it is sufficient to have \u00b5\nlarge enough so that the intersection of the PDFs are positive and we are in the linear case.\nThe point of intersection assuming \u03c3\u2217         = 1 is given by\n                                                   (\u00b51 \u2212   \u00b52)2/\u03c32 +       1        log(\u03c32)\n                        x =   \u00b51 \u2212   \u00b52/\u03c32 \u00b1              1               \u03c32 \u2212   1\nwhich is positive whenever                                \u03c32 \u2212  1\n                                  \u00b52 \u2265   \u03c3    log(1/\u03c32 + \u00b52   1 \u2265  \u03c3    log(2d/\u03f5.\nFor the rest of the space, we partition \u00b5 \u2208       Rk s.t. for any \u00b5, \u00b5\u2032 in a cell,\n                                                |\u00b5 \u2212  \u00b5\u2032| \u2264  \u03c3\u03f5/2d.\nThis implies that for any \u00b5, \u00b5\u2032 in a cell, either,\nor                                      dT V (p\u00b5,\u03c3(y), p\u00b5\u2032,\u03c3(y)) \u2264      \u03f5/2d\n                dT V (p\u00b5,\u03c3(y), p\u00b5\u2217,\u03c3\u2217(y)) \u2265      1 \u2212  \u03f5 and dT V (p\u00b5\u2032,\u03c3(y), p\u00b5\u2217,\u03c3\u2217(y)) \u2265       1 \u2212  \u03f5.\nThen     the   number      of   cells   is   the   max     of   O(d     log(2d/\u03f5)/\u03f5)      (when     \u03c3    small)    or\nO(d     log(2d/\u03f5)/\u03c3\u03f5) \u2264       d   log(2d/\u03f5)/\u03f53/2 (when \u03c3 large).\nD     Proof of composition of layers\nProof. We can use the triangle inequality to compose our single layer guarantees. Suppose, for layer\nj and j + 1 we have\n                          dT V (\u03d5( \u02c6                        dT V (Xj, \u02c6Xj  ) \u2264  \u03f5/2    and\nthen,                              W jMLEXj + \u03b7j), \u03d5(W jXj + \u03b7j)) \u2264             \u03f5/2\n                    dT V (\u03d5( \u02c6W jMLE \u02c6 X j + \u03b7j), \u03d5(W jXj + \u03b7j))\n                                       \u2264   dT V (\u03d5( \u02c6\n                                                    W jMLE \u02c6 X j + \u03b7j), \u03d5( \u02c6W jMLEXj + \u03b7j))\n                                       + dT V (\u03d5( \u02c6 W j\n                                                      MLEXj + \u03b7j), \u03d5(W jXj + \u03b7j))\n                                       \u2264   \u03f5\nwhere we use the fact that dT V (f(X), f(Y )) \u2264          dT V (X, Y ).\n                                                         47", "md": "# Math Equations\n\nIt is easy to see that $$dT V (p_{\\mu,\\sigma}(y), p_{0,1}(y)) \\geq dT V (p_{0,\\sigma}(y), p_{0,1}(y))$$. The PDFs of N(0, \u03c3) and N(0, 1) intersect at x = \u00b1\u03c3 log(1/\u03c3^2). To show that $$dT V (p_{0,\\sigma}(y), p_{0,1}(y)) \\geq 1 - \\epsilon$$, it is now sufficient to show that $$1-\\sigma^2 \\geq 1 - 2\\Phi(-|x|/\\sigma) \\geq 1 - \\epsilon$$ and $$1 - 2\\phi(-|x|) \\leq \\epsilon$$, where $$\\Phi(x)$$ is the CDF of the standard normal distribution. By using classical bounds on $$\\Phi(x)$$, we have that $$\\Phi(-|x|/\\sigma) \\leq \\exp(-x^2/2\\sigma^2) = \\exp(2(1-\\sigma^2))$$ which is $$\\leq \\epsilon/2$$ if $$\\sigma^2 \\leq \\epsilon/2$$. And, $$\\Phi(-|x|) \\geq |x| \\exp(-x^2/2) / (x^2 + 1)$$ which is $$\\geq (1 - \\epsilon)/2$$ if ...\n\nWhen $$\\mu^* \\leq -\\log(2d/\\epsilon)$$ the same argument as above shows that if $$\\mu > \\sigma \\log(2d/\\epsilon)$$ then the $$dT V (p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq$$ the difference in the probabilities at 0 which is bigger than $$1 - \\epsilon$$. We consider the case where $$\\mu^* \\geq -\\log(2d/\\epsilon)$$. Since, when $$\\sigma$$ is small, for any $$\\mu$$ in the linear case the TV distance is large it is sufficient to have $$\\mu$$ large enough so that the intersection of the PDFs are positive and we are in the linear case. The point of intersection assuming $$\\sigma^* = 1$$ is given by $$(\\mu_1 - \\mu_2)^2/\\sigma^2 + 1 \\log(\\sigma^2)$$ $$x = \\mu_1 - \\mu_2/\\sigma^2 \\pm 1 \\sigma^2 - 1$$ which is positive whenever $$\\sigma^2 - 1 \\mu_2 \\geq \\sigma \\log(1/\\sigma^2 + \\mu_2 1 \\geq \\sigma \\log(2d/\\epsilon)$$. For the rest of the space, we partition $$\\mu \\in R^k$$ s.t. for any $$\\mu, \\mu'$$ in a cell, $$|\\mu - \\mu'| \\leq \\sigma\\epsilon/2d$$. This implies that for any $$\\mu, \\mu'$$ in a cell, either, or $$dT V (p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\epsilon/2d$$ $$dT V (p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$ and $$dT V (p_{\\mu',\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$. Then the number of cells is the max of $$O(d \\log(2d/\\epsilon)/\\epsilon)$$ (when $$\\sigma$$ small) or $$O(d \\log(2d/\\epsilon)/\\sigma\\epsilon) \\leq d \\log(2d/\\epsilon)/\\epsilon^{3/2}$$ (when $$\\sigma$$ large).\n\nProof of composition of layers\n\nProof. We can use the triangle inequality to compose our single layer guarantees. Suppose, for layer j and j + 1 we have $$dT V (\\phi(\\hat{W}_jMLEX_j + \\eta_j), \\phi(W_jX_j + \\eta_j)) \\leq \\epsilon/2$$ and then, $$dT V (\\phi(\\hat{W}_jMLE \\hat{X}_j + \\eta_j), \\phi(W_jX_j + \\eta_j)) \\leq \\epsilon$$ where we use the fact that $$dT V (f(X), f(Y)) \\leq dT V (X, Y)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "It is easy to see that $$dT V (p_{\\mu,\\sigma}(y), p_{0,1}(y)) \\geq dT V (p_{0,\\sigma}(y), p_{0,1}(y))$$. The PDFs of N(0, \u03c3) and N(0, 1) intersect at x = \u00b1\u03c3 log(1/\u03c3^2). To show that $$dT V (p_{0,\\sigma}(y), p_{0,1}(y)) \\geq 1 - \\epsilon$$, it is now sufficient to show that $$1-\\sigma^2 \\geq 1 - 2\\Phi(-|x|/\\sigma) \\geq 1 - \\epsilon$$ and $$1 - 2\\phi(-|x|) \\leq \\epsilon$$, where $$\\Phi(x)$$ is the CDF of the standard normal distribution. By using classical bounds on $$\\Phi(x)$$, we have that $$\\Phi(-|x|/\\sigma) \\leq \\exp(-x^2/2\\sigma^2) = \\exp(2(1-\\sigma^2))$$ which is $$\\leq \\epsilon/2$$ if $$\\sigma^2 \\leq \\epsilon/2$$. And, $$\\Phi(-|x|) \\geq |x| \\exp(-x^2/2) / (x^2 + 1)$$ which is $$\\geq (1 - \\epsilon)/2$$ if ...\n\nWhen $$\\mu^* \\leq -\\log(2d/\\epsilon)$$ the same argument as above shows that if $$\\mu > \\sigma \\log(2d/\\epsilon)$$ then the $$dT V (p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq$$ the difference in the probabilities at 0 which is bigger than $$1 - \\epsilon$$. We consider the case where $$\\mu^* \\geq -\\log(2d/\\epsilon)$$. Since, when $$\\sigma$$ is small, for any $$\\mu$$ in the linear case the TV distance is large it is sufficient to have $$\\mu$$ large enough so that the intersection of the PDFs are positive and we are in the linear case. The point of intersection assuming $$\\sigma^* = 1$$ is given by $$(\\mu_1 - \\mu_2)^2/\\sigma^2 + 1 \\log(\\sigma^2)$$ $$x = \\mu_1 - \\mu_2/\\sigma^2 \\pm 1 \\sigma^2 - 1$$ which is positive whenever $$\\sigma^2 - 1 \\mu_2 \\geq \\sigma \\log(1/\\sigma^2 + \\mu_2 1 \\geq \\sigma \\log(2d/\\epsilon)$$. For the rest of the space, we partition $$\\mu \\in R^k$$ s.t. for any $$\\mu, \\mu'$$ in a cell, $$|\\mu - \\mu'| \\leq \\sigma\\epsilon/2d$$. This implies that for any $$\\mu, \\mu'$$ in a cell, either, or $$dT V (p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\epsilon/2d$$ $$dT V (p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$ and $$dT V (p_{\\mu',\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$. Then the number of cells is the max of $$O(d \\log(2d/\\epsilon)/\\epsilon)$$ (when $$\\sigma$$ small) or $$O(d \\log(2d/\\epsilon)/\\sigma\\epsilon) \\leq d \\log(2d/\\epsilon)/\\epsilon^{3/2}$$ (when $$\\sigma$$ large).\n\nProof of composition of layers\n\nProof. We can use the triangle inequality to compose our single layer guarantees. Suppose, for layer j and j + 1 we have $$dT V (\\phi(\\hat{W}_jMLEX_j + \\eta_j), \\phi(W_jX_j + \\eta_j)) \\leq \\epsilon/2$$ and then, $$dT V (\\phi(\\hat{W}_jMLE \\hat{X}_j + \\eta_j), \\phi(W_jX_j + \\eta_j)) \\leq \\epsilon$$ where we use the fact that $$dT V (f(X), f(Y)) \\leq dT V (X, Y)$$.", "md": "It is easy to see that $$dT V (p_{\\mu,\\sigma}(y), p_{0,1}(y)) \\geq dT V (p_{0,\\sigma}(y), p_{0,1}(y))$$. The PDFs of N(0, \u03c3) and N(0, 1) intersect at x = \u00b1\u03c3 log(1/\u03c3^2). To show that $$dT V (p_{0,\\sigma}(y), p_{0,1}(y)) \\geq 1 - \\epsilon$$, it is now sufficient to show that $$1-\\sigma^2 \\geq 1 - 2\\Phi(-|x|/\\sigma) \\geq 1 - \\epsilon$$ and $$1 - 2\\phi(-|x|) \\leq \\epsilon$$, where $$\\Phi(x)$$ is the CDF of the standard normal distribution. By using classical bounds on $$\\Phi(x)$$, we have that $$\\Phi(-|x|/\\sigma) \\leq \\exp(-x^2/2\\sigma^2) = \\exp(2(1-\\sigma^2))$$ which is $$\\leq \\epsilon/2$$ if $$\\sigma^2 \\leq \\epsilon/2$$. And, $$\\Phi(-|x|) \\geq |x| \\exp(-x^2/2) / (x^2 + 1)$$ which is $$\\geq (1 - \\epsilon)/2$$ if ...\n\nWhen $$\\mu^* \\leq -\\log(2d/\\epsilon)$$ the same argument as above shows that if $$\\mu > \\sigma \\log(2d/\\epsilon)$$ then the $$dT V (p_{\\mu^*,\\sigma^*}(y), p_{\\mu,\\sigma}(y)) \\geq$$ the difference in the probabilities at 0 which is bigger than $$1 - \\epsilon$$. We consider the case where $$\\mu^* \\geq -\\log(2d/\\epsilon)$$. Since, when $$\\sigma$$ is small, for any $$\\mu$$ in the linear case the TV distance is large it is sufficient to have $$\\mu$$ large enough so that the intersection of the PDFs are positive and we are in the linear case. The point of intersection assuming $$\\sigma^* = 1$$ is given by $$(\\mu_1 - \\mu_2)^2/\\sigma^2 + 1 \\log(\\sigma^2)$$ $$x = \\mu_1 - \\mu_2/\\sigma^2 \\pm 1 \\sigma^2 - 1$$ which is positive whenever $$\\sigma^2 - 1 \\mu_2 \\geq \\sigma \\log(1/\\sigma^2 + \\mu_2 1 \\geq \\sigma \\log(2d/\\epsilon)$$. For the rest of the space, we partition $$\\mu \\in R^k$$ s.t. for any $$\\mu, \\mu'$$ in a cell, $$|\\mu - \\mu'| \\leq \\sigma\\epsilon/2d$$. This implies that for any $$\\mu, \\mu'$$ in a cell, either, or $$dT V (p_{\\mu,\\sigma}(y), p_{\\mu',\\sigma}(y)) \\leq \\epsilon/2d$$ $$dT V (p_{\\mu,\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$ and $$dT V (p_{\\mu',\\sigma}(y), p_{\\mu^*,\\sigma^*}(y)) \\geq 1 - \\epsilon$$. Then the number of cells is the max of $$O(d \\log(2d/\\epsilon)/\\epsilon)$$ (when $$\\sigma$$ small) or $$O(d \\log(2d/\\epsilon)/\\sigma\\epsilon) \\leq d \\log(2d/\\epsilon)/\\epsilon^{3/2}$$ (when $$\\sigma$$ large).\n\nProof of composition of layers\n\nProof. We can use the triangle inequality to compose our single layer guarantees. Suppose, for layer j and j + 1 we have $$dT V (\\phi(\\hat{W}_jMLEX_j + \\eta_j), \\phi(W_jX_j + \\eta_j)) \\leq \\epsilon/2$$ and then, $$dT V (\\phi(\\hat{W}_jMLE \\hat{X}_j + \\eta_j), \\phi(W_jX_j + \\eta_j)) \\leq \\epsilon$$ where we use the fact that $$dT V (f(X), f(Y)) \\leq dT V (X, Y)$$."}]}, {"page": 48, "text": "                 10-3                                                                                                  10-1\n             4\n            3.5\n             3\n            2.5\n             2\n                                                                                                                       10-2\n            1.5\n               6                    8               10            12          14        16      18      20                102                                           103            104\n                  (a) Total Variation Upper Bound vs d                                                                      (b) Total Variation vs. n for Different Dx\nFigure 4: (a) Plots Pinsker\u2019s upper bound on the TV distance as d gets large. We set \u03a3\u2217                                                                                      = Id and\nW \u2217       = 1d\u00d71, thus setting input dimension k = 1. n = 5000 samples are taken. As we might expect,\nthe upper bound is increasing in d. each point is determined by 2000 samples. (b) Plot of TV vs. n\nfor additional distributions of x. All three distributions follow roughly the same trend, each point is\ndetermined by 2500 samples.\nE          Simulations\nE.1          Additional Simulations\nIn this section, we provide additional simulations to supplement some of the discussion in Section 5.\nE.2          Simulation Details\nE.2.1             Figure 2\nIn these experiment, we set d = 1, and plot the results for various values of the number of samples n\nin Figure 2a and various values of the input dimension k in Figure 2b. For each plot, we fix the true\n\u03c3\u2217     = 1 and the w\u2217                      = 1k\u00d71. In each case the MLE is solved via gradient descent with backtracking\nline search, and we check a first order condition \u2225\u2207w,\u03c3 log pw,\u03c3((y | x))\u22252 < \u03b4 = 10\u22123 as the exit\ncondition. We verify that increasing or decreasing \u03b4 by one order of magnitude makes no difference\nto the Figure.\nThe expected total variation distance for the two distributions is calculated as follows. We sample\nx according to the true distribution (in this case either Laplace or Normal). Then we compute\ndT V (p \u02c6      w,\u02c6  \u03c3(y | x), pw,\u03c3(y | x)) via the MATLAB integral function which uses vectorized adaptive\nquadrature. We repeat this a total of 100 times and take the average to compute our expected total\nvariation. We then repeat the entire process 2000 times, each time optimizing to find an MLE, and\nthen compute its average total variation distance. Lines indicate the average of these experiments,\nand the error bars, (not easily visible due to their size) indicates one standard error.\nE.2.2             Figure 3\nIn these experiments we fix d = 3 to retain reasonable complexity for computing the TV distance,\nand take input dimension k = 1 with deterministic x in order to compare with [35]. In Figure 3a we\nfix \u03a3\u2217         = Id and take let W \u2217                            = b1d\u00d71, where b will vary across our experiments. We set n = 10000.\nIn Figure 3b we set n = 5000 and adjust \u03a3 such that one diagonal entry is \u03ba1/2, and the other is\n\u03ba\u22121/2, making the total condition number \u03ba.\nIn both of these experiments, we restrict the MLE computation to be over diagonal \u03a3 only. This is\nnot because computation of the MLE is too difficult, but rather because computing the TV distance is\ngreatly simplified in this case. The algorithm of Wu et al. is hence modified to use the knowledge that\nthe output must be diagonal. This is simply done, because the procedure of Wu et al. essentially first\n                                                                                                            48", "md": "## Figure 4: (a) Plots Pinsker\u2019s upper bound on the TV distance as d gets large. We set $$\\Sigma^* = I_d$$ and $$W^* = 1_{d \\times 1}$$, thus setting input dimension $$k = 1$$. $$n = 5000$$ samples are taken. As we might expect, the upper bound is increasing in $$d$$. each point is determined by 2000 samples. (b) Plot of TV vs. $$n$$ for additional distributions of $$x$$. All three distributions follow roughly the same trend, each point is determined by 2500 samples.\n\n### E Simulations\n\n#### E.1 Additional Simulations\n\nIn this section, we provide additional simulations to supplement some of the discussion in Section 5.\n\n#### E.2 Simulation Details\n\n##### E.2.1 Figure 2\n\nIn these experiment, we set $$d = 1$$, and plot the results for various values of the number of samples $$n$$ in Figure 2a and various values of the input dimension $$k$$ in Figure 2b. For each plot, we fix the true $$\\sigma^* = 1$$ and the $$w^* = 1_{k \\times 1}$$. In each case the MLE is solved via gradient descent with backtracking line search, and we check a first order condition $$\\| \\nabla w, \\sigma \\log p_{w, \\sigma}((y | x)) \\|_2 < \\delta = 10^{-3}$$ as the exit condition. We verify that increasing or decreasing $$\\delta$$ by one order of magnitude makes no difference to the Figure.\n\nThe expected total variation distance for the two distributions is calculated as follows. We sample $$x$$ according to the true distribution (in this case either Laplace or Normal). Then we compute $$d_{TV}(p_{\\hat{w}, \\hat{\\sigma}}(y | x), p_{w, \\sigma}(y | x))$$ via the MATLAB integral function which uses vectorized adaptive quadrature. We repeat this a total of 100 times and take the average to compute our expected total variation. We then repeat the entire process 2000 times, each time optimizing to find an MLE, and then compute its average total variation distance. Lines indicate the average of these experiments, and the error bars, (not easily visible due to their size) indicates one standard error.\n\n##### E.2.2 Figure 3\n\nIn these experiments we fix $$d = 3$$ to retain reasonable complexity for computing the TV distance, and take input dimension $$k = 1$$ with deterministic $$x$$ in order to compare with [35]. In Figure 3a we fix $$\\Sigma^* = I_d$$ and take let $$W^* = b1_{d \\times 1}$$, where $$b$$ will vary across our experiments. We set $$n = 10000$$. In Figure 3b we set $$n = 5000$$ and adjust $$\\Sigma$$ such that one diagonal entry is $$\\kappa^{1/2}$$, and the other is $$\\kappa^{-1/2}$$, making the total condition number $$\\kappa$$.\n\nIn both of these experiments, we restrict the MLE computation to be over diagonal $$\\Sigma$$ only. This is not because computation of the MLE is too difficult, but rather because computing the TV distance is greatly simplified in this case. The algorithm of Wu et al. is hence modified to use the knowledge that the output must be diagonal. This is simply done, because the procedure of Wu et al. essentially first", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Figure 4: (a) Plots Pinsker\u2019s upper bound on the TV distance as d gets large. We set $$\\Sigma^* = I_d$$ and $$W^* = 1_{d \\times 1}$$, thus setting input dimension $$k = 1$$. $$n = 5000$$ samples are taken. As we might expect, the upper bound is increasing in $$d$$. each point is determined by 2000 samples. (b) Plot of TV vs. $$n$$ for additional distributions of $$x$$. All three distributions follow roughly the same trend, each point is determined by 2500 samples.", "md": "## Figure 4: (a) Plots Pinsker\u2019s upper bound on the TV distance as d gets large. We set $$\\Sigma^* = I_d$$ and $$W^* = 1_{d \\times 1}$$, thus setting input dimension $$k = 1$$. $$n = 5000$$ samples are taken. As we might expect, the upper bound is increasing in $$d$$. each point is determined by 2000 samples. (b) Plot of TV vs. $$n$$ for additional distributions of $$x$$. All three distributions follow roughly the same trend, each point is determined by 2500 samples."}, {"type": "heading", "lvl": 3, "value": "E Simulations", "md": "### E Simulations"}, {"type": "heading", "lvl": 4, "value": "E.1 Additional Simulations", "md": "#### E.1 Additional Simulations"}, {"type": "text", "value": "In this section, we provide additional simulations to supplement some of the discussion in Section 5.", "md": "In this section, we provide additional simulations to supplement some of the discussion in Section 5."}, {"type": "heading", "lvl": 4, "value": "E.2 Simulation Details", "md": "#### E.2 Simulation Details"}, {"type": "heading", "lvl": 5, "value": "E.2.1 Figure 2", "md": "##### E.2.1 Figure 2"}, {"type": "text", "value": "In these experiment, we set $$d = 1$$, and plot the results for various values of the number of samples $$n$$ in Figure 2a and various values of the input dimension $$k$$ in Figure 2b. For each plot, we fix the true $$\\sigma^* = 1$$ and the $$w^* = 1_{k \\times 1}$$. In each case the MLE is solved via gradient descent with backtracking line search, and we check a first order condition $$\\| \\nabla w, \\sigma \\log p_{w, \\sigma}((y | x)) \\|_2 < \\delta = 10^{-3}$$ as the exit condition. We verify that increasing or decreasing $$\\delta$$ by one order of magnitude makes no difference to the Figure.\n\nThe expected total variation distance for the two distributions is calculated as follows. We sample $$x$$ according to the true distribution (in this case either Laplace or Normal). Then we compute $$d_{TV}(p_{\\hat{w}, \\hat{\\sigma}}(y | x), p_{w, \\sigma}(y | x))$$ via the MATLAB integral function which uses vectorized adaptive quadrature. We repeat this a total of 100 times and take the average to compute our expected total variation. We then repeat the entire process 2000 times, each time optimizing to find an MLE, and then compute its average total variation distance. Lines indicate the average of these experiments, and the error bars, (not easily visible due to their size) indicates one standard error.", "md": "In these experiment, we set $$d = 1$$, and plot the results for various values of the number of samples $$n$$ in Figure 2a and various values of the input dimension $$k$$ in Figure 2b. For each plot, we fix the true $$\\sigma^* = 1$$ and the $$w^* = 1_{k \\times 1}$$. In each case the MLE is solved via gradient descent with backtracking line search, and we check a first order condition $$\\| \\nabla w, \\sigma \\log p_{w, \\sigma}((y | x)) \\|_2 < \\delta = 10^{-3}$$ as the exit condition. We verify that increasing or decreasing $$\\delta$$ by one order of magnitude makes no difference to the Figure.\n\nThe expected total variation distance for the two distributions is calculated as follows. We sample $$x$$ according to the true distribution (in this case either Laplace or Normal). Then we compute $$d_{TV}(p_{\\hat{w}, \\hat{\\sigma}}(y | x), p_{w, \\sigma}(y | x))$$ via the MATLAB integral function which uses vectorized adaptive quadrature. We repeat this a total of 100 times and take the average to compute our expected total variation. We then repeat the entire process 2000 times, each time optimizing to find an MLE, and then compute its average total variation distance. Lines indicate the average of these experiments, and the error bars, (not easily visible due to their size) indicates one standard error."}, {"type": "heading", "lvl": 5, "value": "E.2.2 Figure 3", "md": "##### E.2.2 Figure 3"}, {"type": "text", "value": "In these experiments we fix $$d = 3$$ to retain reasonable complexity for computing the TV distance, and take input dimension $$k = 1$$ with deterministic $$x$$ in order to compare with [35]. In Figure 3a we fix $$\\Sigma^* = I_d$$ and take let $$W^* = b1_{d \\times 1}$$, where $$b$$ will vary across our experiments. We set $$n = 10000$$. In Figure 3b we set $$n = 5000$$ and adjust $$\\Sigma$$ such that one diagonal entry is $$\\kappa^{1/2}$$, and the other is $$\\kappa^{-1/2}$$, making the total condition number $$\\kappa$$.\n\nIn both of these experiments, we restrict the MLE computation to be over diagonal $$\\Sigma$$ only. This is not because computation of the MLE is too difficult, but rather because computing the TV distance is greatly simplified in this case. The algorithm of Wu et al. is hence modified to use the knowledge that the output must be diagonal. This is simply done, because the procedure of Wu et al. essentially first", "md": "In these experiments we fix $$d = 3$$ to retain reasonable complexity for computing the TV distance, and take input dimension $$k = 1$$ with deterministic $$x$$ in order to compare with [35]. In Figure 3a we fix $$\\Sigma^* = I_d$$ and take let $$W^* = b1_{d \\times 1}$$, where $$b$$ will vary across our experiments. We set $$n = 10000$$. In Figure 3b we set $$n = 5000$$ and adjust $$\\Sigma$$ such that one diagonal entry is $$\\kappa^{1/2}$$, and the other is $$\\kappa^{-1/2}$$, making the total condition number $$\\kappa$$.\n\nIn both of these experiments, we restrict the MLE computation to be over diagonal $$\\Sigma$$ only. This is not because computation of the MLE is too difficult, but rather because computing the TV distance is greatly simplified in this case. The algorithm of Wu et al. is hence modified to use the knowledge that the output must be diagonal. This is simply done, because the procedure of Wu et al. essentially first"}]}, {"page": 49, "text": "estimates the diagonal entries of the matrix as if it were diagonal and then computes the correlations.\nRemoving this second phase allows us to achieve our goal.\nSince x is deterministic, we do not need to consider randomness in computing the expected TV\ndistance, though other challenges remain. Since our distribution is degenerate, we must be very\ncareful in computing the TV distance in higher dimensions. Specifically, in the diagonal case, the TV\nmay be written as:\n             dT V     (W,    \u03a3), (W \u2217, \u03a3\u2217)         = 1 2    Rd\u22650  p W , \u03a3(y | x) \u2212      pW \u2217,\u03a3\u2217(y | x)        dy\n                                                   = 1 2    R3      d   pW , \u03a3(yi | x) \u2212        d   pW \u2217,\u03a3\u2217(yi | x)       dy,\n                                                              \u22650  i=1                         i=1\nwhere yi is the ith element of y. Though at first glace it seems that this is a single high-dimensional\nintegral, the reality is that due to the truncation, the probability mass on the boundary of the non-\nnegative orthant cone R3          \u22650 has a complex structure that cannot be ignored. Instead we perform a\nseries of integrals of continuous bounded functions, which are much more amenable to Monte-Carlo\nintegration techniques:\n1  dT V     (W,    \u03a3), (W \u2217, \u03a3\u2217)         =                                     \u2212         pW \u2217,\u03a3\u2217(yi | x)                \u03a6    \u03a3\u22121/2Wi     dyS\u2032.\n                             p                           \u03a6    \u03a3\u22121/2Wi                                                           ii\n2              R|S\u2032|          W , \u03a3(yi | x)                      ii\n   S\u2032\u22082[d]      \u22650    i\u2208S\u2032                      i\u2208(S\u2032)c                           i\u2208S\u2032                         i\u2208(S\u2032)c                         (45)\nEssentially, for each possible support of y, S\u2032, we integrate over the absolute deviation in those\ncoordinates.\nE.2.3      Figure 4\nIn Figure 4a, we plot an upper bounds for the TV distance of the MLE as the output dimension\nd grows. We set the input dimension k = 1 with deterministic x and fix the number of samples\nn = 5000. To estimate the KL divergence, we repeatedly sample y according to the true distribution,\nand compute the empirical average log-likelihood ratio.\nIn Figure 4b we fix the output dimension d = 1 and input dimension k = 5, and compute the TV over\na range of values of n. In addition to x sampled i.i.d. from the Normal and Laplace distributions, we\nalso plot a performance with a Normal mixture, where with probability 0.01, the normal distribution\nhas mean shifted by 100. We observe, as our theory suggests, that in all cases, there is only very\nminor differences in the expected TV distance.\nNote that in the case where x is distributed according to a Normal mixture, we observe that the\noptimization may become very challenging, and in the plot above, we have omitted some of the\ninstances where optimization failed due to lack of smoothness in the objective and numerical\nimprecision. Omitting these point may lead to a small systematic error in the figure, which may\nexplain why it is lower than the other plots. In practice, for a fixed optimization budget, we may\nobserve meaningful differences in TV for different distributions of x, since computing the MLE\nbecomes more challenging for more complex heavy-tailed distributions.\nF      The Likelihood Function\nIn this section, we discuss the likelihood function, proving log-concavity, as well as discussing\ncomputational challenges.\nF.1     One Dimensional Case\nIn this section, we consider the case where the output dimension d = 1, with some \u03c3\u2217                                           and some\nW \u2217    \u2208   R1,k and describe how to compute the likelihood function. We defer the proof of the\n                                                                    49", "md": "Estimates the diagonal entries of the matrix as if it were diagonal and then computes the correlations. Removing this second phase allows us to achieve our goal. Since \\( x \\) is deterministic, we do not need to consider randomness in computing the expected TV distance, though other challenges remain. Since our distribution is degenerate, we must be very careful in computing the TV distance in higher dimensions. Specifically, in the diagonal case, the TV may be written as:\n\n$$\n\\begin{aligned}\nd_{TV} (W, \\Sigma), (W^*, \\Sigma^*) & = \\frac{1}{2} \\int_{\\mathbb{R}^d \\geq 0} \\left| p(W, \\Sigma | y \\, | \\, x) - p(W^*, \\Sigma^* | y \\, | \\, x) \\right| \\, dy \\\\\n& = \\frac{1}{2} \\int_{\\mathbb{R}^3} \\sum_{i=1}^{d} \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\, dy,\n\\end{aligned}\n$$\n\nwhere \\( y_i \\) is the \\( i \\)th element of \\( y \\). Though at first glance it seems that this is a single high-dimensional integral, the reality is that due to the truncation, the probability mass on the boundary of the non-negative orthant cone \\( \\mathbb{R}^3 \\geq 0 \\) has a complex structure that cannot be ignored. Instead we perform a series of integrals of continuous bounded functions, which are much more amenable to Monte-Carlo integration techniques:\n\n1. $ d_{TV} (W, \\Sigma), (W^*, \\Sigma^*) = \\int_{\\mathcal{S}'} \\left( \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\Phi(\\Sigma^{-1/2} W_i) \\right) \\, dy_{\\mathcal{S}'} $\n2. $ \\begin{aligned} & \\int_{\\mathbb{R}^{|\\mathcal{S}'|}} \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\, dy_i \\, \\text{ for } \\mathcal{S}' \\in 2^{[d]}, i \\in \\mathcal{S}', i \\in (\\mathcal{S}')^c, i \\in \\mathcal{S}', i \\in (\\mathcal{S}')^c \\end{aligned} $\n\nEssentially, for each possible support of \\( y \\), \\( \\mathcal{S}' \\), we integrate over the absolute deviation in those coordinates.\n\nFigure 4\n\nIn Figure 4a, we plot an upper bound for the TV distance of the MLE as the output dimension \\( d \\) grows. We set the input dimension \\( k = 1 \\) with deterministic \\( x \\) and fix the number of samples \\( n = 5000 \\). To estimate the KL divergence, we repeatedly sample \\( y \\) according to the true distribution, and compute the empirical average log-likelihood ratio.\n\nIn Figure 4b, we fix the output dimension \\( d = 1 \\) and input dimension \\( k = 5 \\), and compute the TV over a range of values of \\( n \\). In addition to \\( x \\) sampled i.i.d. from the Normal and Laplace distributions, we also plot a performance with a Normal mixture, where with probability 0.01, the normal distribution has mean shifted by 100. We observe, as our theory suggests, that in all cases, there are only very minor differences in the expected TV distance.\n\nNote that in the case where \\( x \\) is distributed according to a Normal mixture, we observe that the optimization may become very challenging, and in the plot above, we have omitted some of the instances where optimization failed due to lack of smoothness in the objective and numerical imprecision. Omitting these points may lead to a small systematic error in the figure, which may explain why it is lower than the other plots. In practice, for a fixed optimization budget, we may observe meaningful differences in TV for different distributions of \\( x \\), since computing the MLE becomes more challenging for more complex heavy-tailed distributions.\n\nThe Likelihood Function\n\nIn this section, we discuss the likelihood function, proving log-concavity, as well as discussing computational challenges.\n\nOne Dimensional Case\n\nIn this section, we consider the case where the output dimension \\( d = 1 \\), with some \\( \\sigma^* \\) and some \\( W^* \\in \\mathbb{R}^{1,k} \\) and describe how to compute the likelihood function. We defer the proof of the", "images": [], "items": [{"type": "text", "value": "Estimates the diagonal entries of the matrix as if it were diagonal and then computes the correlations. Removing this second phase allows us to achieve our goal. Since \\( x \\) is deterministic, we do not need to consider randomness in computing the expected TV distance, though other challenges remain. Since our distribution is degenerate, we must be very careful in computing the TV distance in higher dimensions. Specifically, in the diagonal case, the TV may be written as:\n\n$$\n\\begin{aligned}\nd_{TV} (W, \\Sigma), (W^*, \\Sigma^*) & = \\frac{1}{2} \\int_{\\mathbb{R}^d \\geq 0} \\left| p(W, \\Sigma | y \\, | \\, x) - p(W^*, \\Sigma^* | y \\, | \\, x) \\right| \\, dy \\\\\n& = \\frac{1}{2} \\int_{\\mathbb{R}^3} \\sum_{i=1}^{d} \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\, dy,\n\\end{aligned}\n$$\n\nwhere \\( y_i \\) is the \\( i \\)th element of \\( y \\). Though at first glance it seems that this is a single high-dimensional integral, the reality is that due to the truncation, the probability mass on the boundary of the non-negative orthant cone \\( \\mathbb{R}^3 \\geq 0 \\) has a complex structure that cannot be ignored. Instead we perform a series of integrals of continuous bounded functions, which are much more amenable to Monte-Carlo integration techniques:\n\n1. $ d_{TV} (W, \\Sigma), (W^*, \\Sigma^*) = \\int_{\\mathcal{S}'} \\left( \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\Phi(\\Sigma^{-1/2} W_i) \\right) \\, dy_{\\mathcal{S}'} $\n2. $ \\begin{aligned} & \\int_{\\mathbb{R}^{|\\mathcal{S}'|}} \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\, dy_i \\, \\text{ for } \\mathcal{S}' \\in 2^{[d]}, i \\in \\mathcal{S}', i \\in (\\mathcal{S}')^c, i \\in \\mathcal{S}', i \\in (\\mathcal{S}')^c \\end{aligned} $\n\nEssentially, for each possible support of \\( y \\), \\( \\mathcal{S}' \\), we integrate over the absolute deviation in those coordinates.\n\nFigure 4\n\nIn Figure 4a, we plot an upper bound for the TV distance of the MLE as the output dimension \\( d \\) grows. We set the input dimension \\( k = 1 \\) with deterministic \\( x \\) and fix the number of samples \\( n = 5000 \\). To estimate the KL divergence, we repeatedly sample \\( y \\) according to the true distribution, and compute the empirical average log-likelihood ratio.\n\nIn Figure 4b, we fix the output dimension \\( d = 1 \\) and input dimension \\( k = 5 \\), and compute the TV over a range of values of \\( n \\). In addition to \\( x \\) sampled i.i.d. from the Normal and Laplace distributions, we also plot a performance with a Normal mixture, where with probability 0.01, the normal distribution has mean shifted by 100. We observe, as our theory suggests, that in all cases, there are only very minor differences in the expected TV distance.\n\nNote that in the case where \\( x \\) is distributed according to a Normal mixture, we observe that the optimization may become very challenging, and in the plot above, we have omitted some of the instances where optimization failed due to lack of smoothness in the objective and numerical imprecision. Omitting these points may lead to a small systematic error in the figure, which may explain why it is lower than the other plots. In practice, for a fixed optimization budget, we may observe meaningful differences in TV for different distributions of \\( x \\), since computing the MLE becomes more challenging for more complex heavy-tailed distributions.\n\nThe Likelihood Function\n\nIn this section, we discuss the likelihood function, proving log-concavity, as well as discussing computational challenges.\n\nOne Dimensional Case\n\nIn this section, we consider the case where the output dimension \\( d = 1 \\), with some \\( \\sigma^* \\) and some \\( W^* \\in \\mathbb{R}^{1,k} \\) and describe how to compute the likelihood function. We defer the proof of the", "md": "Estimates the diagonal entries of the matrix as if it were diagonal and then computes the correlations. Removing this second phase allows us to achieve our goal. Since \\( x \\) is deterministic, we do not need to consider randomness in computing the expected TV distance, though other challenges remain. Since our distribution is degenerate, we must be very careful in computing the TV distance in higher dimensions. Specifically, in the diagonal case, the TV may be written as:\n\n$$\n\\begin{aligned}\nd_{TV} (W, \\Sigma), (W^*, \\Sigma^*) & = \\frac{1}{2} \\int_{\\mathbb{R}^d \\geq 0} \\left| p(W, \\Sigma | y \\, | \\, x) - p(W^*, \\Sigma^* | y \\, | \\, x) \\right| \\, dy \\\\\n& = \\frac{1}{2} \\int_{\\mathbb{R}^3} \\sum_{i=1}^{d} \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\, dy,\n\\end{aligned}\n$$\n\nwhere \\( y_i \\) is the \\( i \\)th element of \\( y \\). Though at first glance it seems that this is a single high-dimensional integral, the reality is that due to the truncation, the probability mass on the boundary of the non-negative orthant cone \\( \\mathbb{R}^3 \\geq 0 \\) has a complex structure that cannot be ignored. Instead we perform a series of integrals of continuous bounded functions, which are much more amenable to Monte-Carlo integration techniques:\n\n1. $ d_{TV} (W, \\Sigma), (W^*, \\Sigma^*) = \\int_{\\mathcal{S}'} \\left( \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\Phi(\\Sigma^{-1/2} W_i) \\right) \\, dy_{\\mathcal{S}'} $\n2. $ \\begin{aligned} & \\int_{\\mathbb{R}^{|\\mathcal{S}'|}} \\left| p(W, \\Sigma | y_i \\, | \\, x) - p(W^*, \\Sigma^* | y_i \\, | \\, x) \\right| \\, dy_i \\, \\text{ for } \\mathcal{S}' \\in 2^{[d]}, i \\in \\mathcal{S}', i \\in (\\mathcal{S}')^c, i \\in \\mathcal{S}', i \\in (\\mathcal{S}')^c \\end{aligned} $\n\nEssentially, for each possible support of \\( y \\), \\( \\mathcal{S}' \\), we integrate over the absolute deviation in those coordinates.\n\nFigure 4\n\nIn Figure 4a, we plot an upper bound for the TV distance of the MLE as the output dimension \\( d \\) grows. We set the input dimension \\( k = 1 \\) with deterministic \\( x \\) and fix the number of samples \\( n = 5000 \\). To estimate the KL divergence, we repeatedly sample \\( y \\) according to the true distribution, and compute the empirical average log-likelihood ratio.\n\nIn Figure 4b, we fix the output dimension \\( d = 1 \\) and input dimension \\( k = 5 \\), and compute the TV over a range of values of \\( n \\). In addition to \\( x \\) sampled i.i.d. from the Normal and Laplace distributions, we also plot a performance with a Normal mixture, where with probability 0.01, the normal distribution has mean shifted by 100. We observe, as our theory suggests, that in all cases, there are only very minor differences in the expected TV distance.\n\nNote that in the case where \\( x \\) is distributed according to a Normal mixture, we observe that the optimization may become very challenging, and in the plot above, we have omitted some of the instances where optimization failed due to lack of smoothness in the objective and numerical imprecision. Omitting these points may lead to a small systematic error in the figure, which may explain why it is lower than the other plots. In practice, for a fixed optimization budget, we may observe meaningful differences in TV for different distributions of \\( x \\), since computing the MLE becomes more challenging for more complex heavy-tailed distributions.\n\nThe Likelihood Function\n\nIn this section, we discuss the likelihood function, proving log-concavity, as well as discussing computational challenges.\n\nOne Dimensional Case\n\nIn this section, we consider the case where the output dimension \\( d = 1 \\), with some \\( \\sigma^* \\) and some \\( W^* \\in \\mathbb{R}^{1,k} \\) and describe how to compute the likelihood function. We defer the proof of the"}]}, {"page": 50, "text": "log-concavity to the follwing section, which covers the more general case with d \u2265                                            1 When we\nre-parameterize as u = \u2212W/\u03c3, v = 1/\u03c3, the likelihood function is written as:\n                     fu,v(y) = \u22121       2  i\u2208S\u2032  (vyi \u2212    u \u00b7 xi)2 + |S\u2032| log(v) +            i\u2208S  log \u03a6(\u2212u \u00b7 xi)                       (46)\nwhere in this case we let S = {i | yi = 0}, where yi is the ith sample in the set {yi, xi}n                                      i=1. Note\nthis is distinct from how we define S and S\u2032 in the multidimensional case, where it corresponds to the\nzero and non-zero coordinates of a single sample yi. The case of d > 0 with uncorrelated \u03b7 follows a\nsimilar approach.\nNumerical concerns.                 In (46), the term log \u03a6(\u2212u \u00b7 xi) presents some numerical concerns when\nu \u00b7 xi \u226b     0 if we naively compute \u03a6(\u2212u \u00b7 xi) and then take the logarithm. Instead we compute it from\nthe mills ratio m(x) [27], defined to be the ratio of the standard normal pdf and the complementary\ncdf. The mills ratio is easily computed, with many well-known expansions, see for example, [13].\nThen we can write:\n                               log \u03a6(\u2212x) = \u2212           log m(x) \u2212        1\n                                                                         2 log(2\u03c0) \u2212        1\n                                                                                            2x2, x > 0.\nSince m(x) changes relatively slowly in x compared to \u03a6(\u2212x), this greatly improves numerical\nstability.\nF.2     Multidimensional Case\nIn the multi-dimensional case, we will generally use the more standard natural parameters:\n                                                           U := \u03a3\u22121   2 ,\n                                                            v := \u2212\u03a3\u22121W           x.\nNote that in the one-dimensional case, we could have also use the natural parameters, but due to the\ntruncation structure, the parameters we used make the computation simpler, in a way that does not\napply to the multidimensional case. Also note that here we are considering a fixed x and writing v as\na vector. In full generality, we should take V = \u2212\u03a3\u22121W                            , however, this is a simple extension which\nwe omit here for readability. It turns out that density is log-concave in these natural parameters:\nLemma F.1. The log-likelihood function in Eqn (8) is concave in the natural parameter space.\nProof. First, let\u2019s write the un-truncated density in terms of these parameters:\n        fW,\u03a3(y|x)          =      exp     \u22121 2(y \u2212     W   x)T \u03a3\u22121(y \u2212         W   x) \u2212    1                  ,                          (47)\n                                                                                           2 log|2\u03c0\u03a3|\n                           =      exp     \u22121 2yT \u03a3\u22121y + xT W T \u03a3\u22121y \u2212                   xT W T \u03a3\u22121W          x \u2212    1                    (48)\n                           =      exp     \u22121 2yT Uy \u2212        vT y \u2212    vT U \u22121v \u2212        1                          2 log|2\u03c0\u03a3|           (49)\n                                                                                         2 log((2\u03c0)n/|U|)                                (50)\nThus, the untrucated conditional density can be written as:\n                                     fU,v(y) = exp           \u22121 2yT Uy + yT v \u2212           A(U, v)        ,\nwhere A(U, v) is the cumulant function (note this is distinct from the related cumulant generating\nfunction). A well known result is that A is jointly convex in its arguments, U and v. Taking logs and\nusing this fact, shows us that fU,v(y) is log-concave in U, v.\nOur truncated density is simply:               fU,v(y|x) =          yS\u22640   pU,v(y|x)dyS,\nFor any log-concave density f(x), integration over a convex subset of the coordinates preserves\nlog-concavity ([9], Example 3.42-3.44). Thus the objective is log-concave.\n                                                                      50", "md": "log-concavity to the following section, which covers the more general case with \\(d \\geq 1\\) When we re-parameterize as \\(u = -\\frac{W}{\\sigma}\\), \\(v = \\frac{1}{\\sigma}\\), the likelihood function is written as:\n\n\\[\nf_{u,v}(y) = -\\frac{1}{2} \\sum_{i \\in S'} (vy_i - u \\cdot x_i)^2 + |S'| \\log(v) + \\sum_{i \\in S} \\log \\Phi(-u \\cdot x_i) \\quad (46)\n\\]\n\nwhere in this case we let \\(S = \\{i | y_i = 0\\}\\), where \\(y_i\\) is the \\(i\\)th sample in the set \\(\\{y_i, x_i\\}_{i=1}^n\\). Note this is distinct from how we define \\(S\\) and \\(S'\\) in the multidimensional case, where it corresponds to the zero and non-zero coordinates of a single sample \\(y_i\\). The case of \\(d > 0\\) with uncorrelated \\(\\eta\\) follows a similar approach.\n\nNumerical concerns. In (46), the term \\(\\log \\Phi(-u \\cdot x_i)\\) presents some numerical concerns when \\(u \\cdot x_i \\gg 0\\) if we naively compute \\(\\Phi(-u \\cdot x_i)\\) and then take the logarithm. Instead we compute it from the mills ratio \\(m(x)\\) [27], defined to be the ratio of the standard normal pdf and the complementary cdf. The mills ratio is easily computed, with many well-known expansions, see for example, [13]. Then we can write:\n\n\\[\n\\log \\Phi(-x) = -\\log m(x) - \\frac{1}{2} \\log(2\\pi) - \\frac{1}{2}x^2, x > 0.\n\\]\n\nSince \\(m(x)\\) changes relatively slowly in \\(x\\) compared to \\(\\Phi(-x)\\), this greatly improves numerical stability.\n\nMultidimensional Case\n\nIn the multi-dimensional case, we will generally use the more standard natural parameters:\n\n\\[\nU := \\Sigma^{-1}, \\quad v := -\\Sigma^{-1}W \\cdot x.\n\\]\n\nNote that in the one-dimensional case, we could have also used the natural parameters, but due to the truncation structure, the parameters we used make the computation simpler, in a way that does not apply to the multidimensional case. Also note that here we are considering a fixed \\(x\\) and writing \\(v\\) as a vector. In full generality, we should take \\(V = -\\Sigma^{-1}W\\), however, this is a simple extension which we omit here for readability. It turns out that density is log-concave in these natural parameters:\n\nLemma F.1. The log-likelihood function in Eqn (8) is concave in the natural parameter space.\n\nProof. First, let\u2019s write the un-truncated density in terms of these parameters:\n\n\\[\nf_{W,\\Sigma}(y|x) = \\exp\\left(-\\frac{1}{2}(y - W \\cdot x)^T \\Sigma^{-1}(y - W \\cdot x) - \\frac{1}{2} \\log|2\\pi\\Sigma|\\right) \\quad (47)\n\\]\n\n\\[\n= \\exp\\left(-\\frac{1}{2}y^T \\Sigma^{-1}y + x^T W^T \\Sigma^{-1}y - x^T W^T \\Sigma^{-1}W \\cdot x - \\frac{1}{2}\\right) \\quad (48)\n\\]\n\n\\[\n= \\exp\\left(-\\frac{1}{2}y^T Uy - v^T y - v^T U^{-1}v - \\frac{1}{2} \\log|2\\pi\\Sigma| - \\frac{1}{2} \\log\\left(\\frac{(2\\pi)^n}{|U|}\\right)\\right) \\quad (49), (50)\n\\]\n\nThus, the untruncated conditional density can be written as:\n\n\\[\nf_{U,v}(y) = \\exp\\left(-\\frac{1}{2}y^T Uy + y^T v - A(U, v)\\right),\n\\]\n\nwhere \\(A(U, v)\\) is the cumulant function (note this is distinct from the related cumulant generating function). A well-known result is that \\(A\\) is jointly convex in its arguments, \\(U\\) and \\(v\\). Taking logs and using this fact, shows us that \\(f_{U,v}(y)\\) is log-concave in \\(U, v\\).\n\nOur truncated density is simply:\n\n\\[\nf_{U,v}(y|x) = \\int_{y_S \\leq 0} p_{U,v}(y|x)dy_S,\n\\]\n\nFor any log-concave density \\(f(x)\\), integration over a convex subset of the coordinates preserves log-concavity ([9], Example 3.42-3.44). Thus the objective is log-concave.", "images": [], "items": [{"type": "text", "value": "log-concavity to the following section, which covers the more general case with \\(d \\geq 1\\) When we re-parameterize as \\(u = -\\frac{W}{\\sigma}\\), \\(v = \\frac{1}{\\sigma}\\), the likelihood function is written as:\n\n\\[\nf_{u,v}(y) = -\\frac{1}{2} \\sum_{i \\in S'} (vy_i - u \\cdot x_i)^2 + |S'| \\log(v) + \\sum_{i \\in S} \\log \\Phi(-u \\cdot x_i) \\quad (46)\n\\]\n\nwhere in this case we let \\(S = \\{i | y_i = 0\\}\\), where \\(y_i\\) is the \\(i\\)th sample in the set \\(\\{y_i, x_i\\}_{i=1}^n\\). Note this is distinct from how we define \\(S\\) and \\(S'\\) in the multidimensional case, where it corresponds to the zero and non-zero coordinates of a single sample \\(y_i\\). The case of \\(d > 0\\) with uncorrelated \\(\\eta\\) follows a similar approach.\n\nNumerical concerns. In (46), the term \\(\\log \\Phi(-u \\cdot x_i)\\) presents some numerical concerns when \\(u \\cdot x_i \\gg 0\\) if we naively compute \\(\\Phi(-u \\cdot x_i)\\) and then take the logarithm. Instead we compute it from the mills ratio \\(m(x)\\) [27], defined to be the ratio of the standard normal pdf and the complementary cdf. The mills ratio is easily computed, with many well-known expansions, see for example, [13]. Then we can write:\n\n\\[\n\\log \\Phi(-x) = -\\log m(x) - \\frac{1}{2} \\log(2\\pi) - \\frac{1}{2}x^2, x > 0.\n\\]\n\nSince \\(m(x)\\) changes relatively slowly in \\(x\\) compared to \\(\\Phi(-x)\\), this greatly improves numerical stability.\n\nMultidimensional Case\n\nIn the multi-dimensional case, we will generally use the more standard natural parameters:\n\n\\[\nU := \\Sigma^{-1}, \\quad v := -\\Sigma^{-1}W \\cdot x.\n\\]\n\nNote that in the one-dimensional case, we could have also used the natural parameters, but due to the truncation structure, the parameters we used make the computation simpler, in a way that does not apply to the multidimensional case. Also note that here we are considering a fixed \\(x\\) and writing \\(v\\) as a vector. In full generality, we should take \\(V = -\\Sigma^{-1}W\\), however, this is a simple extension which we omit here for readability. It turns out that density is log-concave in these natural parameters:\n\nLemma F.1. The log-likelihood function in Eqn (8) is concave in the natural parameter space.\n\nProof. First, let\u2019s write the un-truncated density in terms of these parameters:\n\n\\[\nf_{W,\\Sigma}(y|x) = \\exp\\left(-\\frac{1}{2}(y - W \\cdot x)^T \\Sigma^{-1}(y - W \\cdot x) - \\frac{1}{2} \\log|2\\pi\\Sigma|\\right) \\quad (47)\n\\]\n\n\\[\n= \\exp\\left(-\\frac{1}{2}y^T \\Sigma^{-1}y + x^T W^T \\Sigma^{-1}y - x^T W^T \\Sigma^{-1}W \\cdot x - \\frac{1}{2}\\right) \\quad (48)\n\\]\n\n\\[\n= \\exp\\left(-\\frac{1}{2}y^T Uy - v^T y - v^T U^{-1}v - \\frac{1}{2} \\log|2\\pi\\Sigma| - \\frac{1}{2} \\log\\left(\\frac{(2\\pi)^n}{|U|}\\right)\\right) \\quad (49), (50)\n\\]\n\nThus, the untruncated conditional density can be written as:\n\n\\[\nf_{U,v}(y) = \\exp\\left(-\\frac{1}{2}y^T Uy + y^T v - A(U, v)\\right),\n\\]\n\nwhere \\(A(U, v)\\) is the cumulant function (note this is distinct from the related cumulant generating function). A well-known result is that \\(A\\) is jointly convex in its arguments, \\(U\\) and \\(v\\). Taking logs and using this fact, shows us that \\(f_{U,v}(y)\\) is log-concave in \\(U, v\\).\n\nOur truncated density is simply:\n\n\\[\nf_{U,v}(y|x) = \\int_{y_S \\leq 0} p_{U,v}(y|x)dy_S,\n\\]\n\nFor any log-concave density \\(f(x)\\), integration over a convex subset of the coordinates preserves log-concavity ([9], Example 3.42-3.44). Thus the objective is log-concave.", "md": "log-concavity to the following section, which covers the more general case with \\(d \\geq 1\\) When we re-parameterize as \\(u = -\\frac{W}{\\sigma}\\), \\(v = \\frac{1}{\\sigma}\\), the likelihood function is written as:\n\n\\[\nf_{u,v}(y) = -\\frac{1}{2} \\sum_{i \\in S'} (vy_i - u \\cdot x_i)^2 + |S'| \\log(v) + \\sum_{i \\in S} \\log \\Phi(-u \\cdot x_i) \\quad (46)\n\\]\n\nwhere in this case we let \\(S = \\{i | y_i = 0\\}\\), where \\(y_i\\) is the \\(i\\)th sample in the set \\(\\{y_i, x_i\\}_{i=1}^n\\). Note this is distinct from how we define \\(S\\) and \\(S'\\) in the multidimensional case, where it corresponds to the zero and non-zero coordinates of a single sample \\(y_i\\). The case of \\(d > 0\\) with uncorrelated \\(\\eta\\) follows a similar approach.\n\nNumerical concerns. In (46), the term \\(\\log \\Phi(-u \\cdot x_i)\\) presents some numerical concerns when \\(u \\cdot x_i \\gg 0\\) if we naively compute \\(\\Phi(-u \\cdot x_i)\\) and then take the logarithm. Instead we compute it from the mills ratio \\(m(x)\\) [27], defined to be the ratio of the standard normal pdf and the complementary cdf. The mills ratio is easily computed, with many well-known expansions, see for example, [13]. Then we can write:\n\n\\[\n\\log \\Phi(-x) = -\\log m(x) - \\frac{1}{2} \\log(2\\pi) - \\frac{1}{2}x^2, x > 0.\n\\]\n\nSince \\(m(x)\\) changes relatively slowly in \\(x\\) compared to \\(\\Phi(-x)\\), this greatly improves numerical stability.\n\nMultidimensional Case\n\nIn the multi-dimensional case, we will generally use the more standard natural parameters:\n\n\\[\nU := \\Sigma^{-1}, \\quad v := -\\Sigma^{-1}W \\cdot x.\n\\]\n\nNote that in the one-dimensional case, we could have also used the natural parameters, but due to the truncation structure, the parameters we used make the computation simpler, in a way that does not apply to the multidimensional case. Also note that here we are considering a fixed \\(x\\) and writing \\(v\\) as a vector. In full generality, we should take \\(V = -\\Sigma^{-1}W\\), however, this is a simple extension which we omit here for readability. It turns out that density is log-concave in these natural parameters:\n\nLemma F.1. The log-likelihood function in Eqn (8) is concave in the natural parameter space.\n\nProof. First, let\u2019s write the un-truncated density in terms of these parameters:\n\n\\[\nf_{W,\\Sigma}(y|x) = \\exp\\left(-\\frac{1}{2}(y - W \\cdot x)^T \\Sigma^{-1}(y - W \\cdot x) - \\frac{1}{2} \\log|2\\pi\\Sigma|\\right) \\quad (47)\n\\]\n\n\\[\n= \\exp\\left(-\\frac{1}{2}y^T \\Sigma^{-1}y + x^T W^T \\Sigma^{-1}y - x^T W^T \\Sigma^{-1}W \\cdot x - \\frac{1}{2}\\right) \\quad (48)\n\\]\n\n\\[\n= \\exp\\left(-\\frac{1}{2}y^T Uy - v^T y - v^T U^{-1}v - \\frac{1}{2} \\log|2\\pi\\Sigma| - \\frac{1}{2} \\log\\left(\\frac{(2\\pi)^n}{|U|}\\right)\\right) \\quad (49), (50)\n\\]\n\nThus, the untruncated conditional density can be written as:\n\n\\[\nf_{U,v}(y) = \\exp\\left(-\\frac{1}{2}y^T Uy + y^T v - A(U, v)\\right),\n\\]\n\nwhere \\(A(U, v)\\) is the cumulant function (note this is distinct from the related cumulant generating function). A well-known result is that \\(A\\) is jointly convex in its arguments, \\(U\\) and \\(v\\). Taking logs and using this fact, shows us that \\(f_{U,v}(y)\\) is log-concave in \\(U, v\\).\n\nOur truncated density is simply:\n\n\\[\nf_{U,v}(y|x) = \\int_{y_S \\leq 0} p_{U,v}(y|x)dy_S,\n\\]\n\nFor any log-concave density \\(f(x)\\), integration over a convex subset of the coordinates preserves log-concavity ([9], Example 3.42-3.44). Thus the objective is log-concave."}]}, {"page": 51, "text": "Then the likelihood function at U, v can be rewritten as\n               fU,v(y) = log                             exp      \u2212tT Ut \u2212       tT v \u2212    vT U \u22121v      + 1                  ,\n                                      tS\u22640,tS\u2032=yS\u2032                                              4            2 log|2U|\n                           = \u2212vT U \u22121v  4       + 1 2 log|2U| + log           tS\u22640,tS\u2032=yS\u2032       exp    \u2212tT Ut \u2212       tT v    ,\nSeparating the terms corresponding to S and S\u2032, we get\nfU,v(y) = \u2212vT U \u22121v     4        + 1 2 log|2U| \u2212       yTS\u2032US\u2032yS\u2032 \u2212        yTS\u2032vS\u2032 + log        tS\u22640    exp    \u2212tT  S UStS \u2212      2yT S\u2032US\u2032StS \u2212  vTS tS  .\nThe last term resembles the log \u03a6 term that appears in the univariate case. This resemblance can be\nmade more clear as follows. Let rS = USS\u2032y\u2032                      S + 1  2vS and M T M = US.\n  = log       tS\u22640   exp     \u2212   tTS M T MtS + 2tT        S rS\n  = log       tS\u22640   exp     \u2212   (MtS)T MtS + 2tT           S  rS + rT  S M \u22121M \u2212T rS \u2212            rTS M \u22121M \u2212T rS\n  = log       tS\u22640   exp     \u2212   MtS + M \u2212T rS             2 + rT   S M \u22121M \u2212T rS 2\n  = rT  S M \u22121M \u2212T rS + log               tS\u22640   exp      \u2212  tS + U \u22121   S rS      U S\n                                                     U \u22121       1/2\n  = rT       s   rS + log               (2\u03c0)d/2        S /2                   \u2212   tS + U \u22121           2\n        S U \u22121                                       U \u22121       1/2 exp                      S rS      US\n  = rT       s   rS \u2212     1     tS\u22640    (2\u03c0)d/2        S /2               1                        \u22121     tS + U \u22121           2         + c\n        S U \u22121            2 log|2US| + log                               U \u22121        1/2 exp           2             S rS      2US\n                 rS \u2212     1                          tS\u22640    (2\u03c0)d/2        S /2\n  = rT       s                                          0; \u00b5 = \u2212U \u22121                                  + c\n        S U \u22121                                                          S rS, \u03a3 = 1           S\n                          2 log|2US| + log \u03a6                                              2U \u22121\nPutting this together, fU,v can be written as:\n   fU,v(y) = \u2212vT U \u22121v              + 1                    S\u2032US\u2032yS\u2032 \u2212       yTS\u2032vS\u2032 + rT    S U \u22121\n                                                                                                 S rS \u2212      1\n                            4           2 log|U| \u2212       yT                                                  2 log|US| + |S\u2032|log(2)      2\n                                                                     + log \u03a6       0; \u00b5 = \u2212U \u22121    S rS, \u03a3 = 1      2U \u22121S       + c     (51)\nThus, it appears that evaluating the likelihood for even a single sample involves the high-dimensional\nintegral that is the rectangular cdf in equation (51).\nF.3     Computing Gradients\nF.3.1     One Dimensional Case\nIn the one-dimensional case, the gradient with respect to u is easily computed as:\n                                                                                                  1\n                                \u2207ufu,v(y) =         i\u2208S\u2032  (vyi \u2212     u \u00b7 xi)xi \u2212      i\u2208S   m(u \u00b7 xi)xi,\nwhere we have previously defined m(x) as the mills ratio. Furthermore, we have:\n                                        \u2207vfu,v(y) = |S\u2032|1         v \u2212 51 i\u2208S\u2032   yi(vyi \u2212     u \u00b7 xi)", "md": "Then the likelihood function at U, v can be rewritten as\n\n$$\n\\begin{align*}\nf_{U,v}(y) &= \\log \\exp\\left(-t^T Ut - t^T v - v^T U^{-1}v\\right) + 1, \\\\\n&= -v^T U^{-1}v + \\frac{1}{4}\\log|2U| + \\log \\left(t_S \\leq 0, t_{S'} = y_{S'}\\right) \\exp\\left(-t^T Ut - t^T v\\right),\n\\end{align*}\n$$\n\nSeparating the terms corresponding to S and S', we get\n\n$$\nf_{U,v}(y) = -v^T U^{-1}v + \\frac{1}{4}\\log|2U| - y_{S'}^T U_{S'}y_{S'} - y_{S'}^T v_{S'} + \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t^T S U_{S}t_S - 2y^T S'U_{S'}t_S - v^T S t_S\\right).\n$$\n\nThe last term resembles the log \u03a6 term that appears in the univariate case. This resemblance can be made more clear as follows. Let \\(r_{S} = U_{S}S'y_{S'} + \\frac{1}{2}v_{S}\\) and \\(M^T M = U_{S}\\).\n\n$$\n\\begin{align*}\n&= \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t^T S M^T Mt_S + 2t^T S r_{S}\\right) \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t_S + U^{-1}_{S}r_{S}U_{S}\\right) \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 + c \\\\\n&= r^T S r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 + c \\\\\n&= r^T S r_{S} - \\frac{1}{2}\\log|2US| + \\log \\Phi_{2U^{-1}} \\\\\n&= r^T S r_{S} - \\frac{1}{2}\\log|2US| + \\log \\Phi_{2U^{-1}} + c\n\\end{align*}\n$$\n\nPutting this together, \\(f_{U,v}(y)\\) can be written as:\n\n$$\nf_{U,v}(y) = -v^T U^{-1}v + 1 - y_{S'}^T U_{S'}y_{S'} - y_{S'}^T v_{S'} + r^T S U^{-1}_{S}r_{S} - \\frac{1}{4}\\log|U| - y^T \\frac{1}{2}\\log|US| + |S'|\\log(2) + \\log \\Phi_{0; \\mu = -U^{-1}S r_{S}, \\Sigma = 1}2U^{-1}S + c \\quad (51)$$\n\nThus, it appears that evaluating the likelihood for even a single sample involves the high-dimensional integral that is the rectangular cdf in equation (51).\n\nF.3 Computing Gradients\n\nF.3.1 One Dimensional Case\n\nIn the one-dimensional case, the gradient with respect to u is easily computed as:\n\n$$\n\\nabla_u f_{u,v}(y) = \\sum_{i \\in S'} \\left(vy_i - u \\cdot x_i\\right)x_i - \\sum_{i \\in S} m(u \\cdot x_i)x_i,\n$$\n\nwhere we have previously defined \\(m(x)\\) as the mills ratio. Furthermore, we have:\n\n$$\n\\nabla_v f_{u,v}(y) = |S'| \\mathbf{1} v - \\frac{1}{2} \\sum_{i \\in S'} yi(vy_i - u \\cdot x_i)\n$$", "images": [], "items": [{"type": "text", "value": "Then the likelihood function at U, v can be rewritten as\n\n$$\n\\begin{align*}\nf_{U,v}(y) &= \\log \\exp\\left(-t^T Ut - t^T v - v^T U^{-1}v\\right) + 1, \\\\\n&= -v^T U^{-1}v + \\frac{1}{4}\\log|2U| + \\log \\left(t_S \\leq 0, t_{S'} = y_{S'}\\right) \\exp\\left(-t^T Ut - t^T v\\right),\n\\end{align*}\n$$\n\nSeparating the terms corresponding to S and S', we get\n\n$$\nf_{U,v}(y) = -v^T U^{-1}v + \\frac{1}{4}\\log|2U| - y_{S'}^T U_{S'}y_{S'} - y_{S'}^T v_{S'} + \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t^T S U_{S}t_S - 2y^T S'U_{S'}t_S - v^T S t_S\\right).\n$$\n\nThe last term resembles the log \u03a6 term that appears in the univariate case. This resemblance can be made more clear as follows. Let \\(r_{S} = U_{S}S'y_{S'} + \\frac{1}{2}v_{S}\\) and \\(M^T M = U_{S}\\).\n\n$$\n\\begin{align*}\n&= \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t^T S M^T Mt_S + 2t^T S r_{S}\\right) \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t_S + U^{-1}_{S}r_{S}U_{S}\\right) \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 + c \\\\\n&= r^T S r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 + c \\\\\n&= r^T S r_{S} - \\frac{1}{2}\\log|2US| + \\log \\Phi_{2U^{-1}} \\\\\n&= r^T S r_{S} - \\frac{1}{2}\\log|2US| + \\log \\Phi_{2U^{-1}} + c\n\\end{align*}\n$$\n\nPutting this together, \\(f_{U,v}(y)\\) can be written as:\n\n$$\nf_{U,v}(y) = -v^T U^{-1}v + 1 - y_{S'}^T U_{S'}y_{S'} - y_{S'}^T v_{S'} + r^T S U^{-1}_{S}r_{S} - \\frac{1}{4}\\log|U| - y^T \\frac{1}{2}\\log|US| + |S'|\\log(2) + \\log \\Phi_{0; \\mu = -U^{-1}S r_{S}, \\Sigma = 1}2U^{-1}S + c \\quad (51)$$\n\nThus, it appears that evaluating the likelihood for even a single sample involves the high-dimensional integral that is the rectangular cdf in equation (51).\n\nF.3 Computing Gradients\n\nF.3.1 One Dimensional Case\n\nIn the one-dimensional case, the gradient with respect to u is easily computed as:\n\n$$\n\\nabla_u f_{u,v}(y) = \\sum_{i \\in S'} \\left(vy_i - u \\cdot x_i\\right)x_i - \\sum_{i \\in S} m(u \\cdot x_i)x_i,\n$$\n\nwhere we have previously defined \\(m(x)\\) as the mills ratio. Furthermore, we have:\n\n$$\n\\nabla_v f_{u,v}(y) = |S'| \\mathbf{1} v - \\frac{1}{2} \\sum_{i \\in S'} yi(vy_i - u \\cdot x_i)\n$$", "md": "Then the likelihood function at U, v can be rewritten as\n\n$$\n\\begin{align*}\nf_{U,v}(y) &= \\log \\exp\\left(-t^T Ut - t^T v - v^T U^{-1}v\\right) + 1, \\\\\n&= -v^T U^{-1}v + \\frac{1}{4}\\log|2U| + \\log \\left(t_S \\leq 0, t_{S'} = y_{S'}\\right) \\exp\\left(-t^T Ut - t^T v\\right),\n\\end{align*}\n$$\n\nSeparating the terms corresponding to S and S', we get\n\n$$\nf_{U,v}(y) = -v^T U^{-1}v + \\frac{1}{4}\\log|2U| - y_{S'}^T U_{S'}y_{S'} - y_{S'}^T v_{S'} + \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t^T S U_{S}t_S - 2y^T S'U_{S'}t_S - v^T S t_S\\right).\n$$\n\nThe last term resembles the log \u03a6 term that appears in the univariate case. This resemblance can be made more clear as follows. Let \\(r_{S} = U_{S}S'y_{S'} + \\frac{1}{2}v_{S}\\) and \\(M^T M = U_{S}\\).\n\n$$\n\\begin{align*}\n&= \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t^T S M^T Mt_S + 2t^T S r_{S}\\right) \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(t_S \\leq 0\\right) \\exp\\left(-t_S + U^{-1}_{S}r_{S}U_{S}\\right) \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 \\\\\n&= r^T S M^{-1}M^{-T} r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 + c \\\\\n&= r^T S r_{S} + \\log \\left(2\\pi\\right)^{d/2} |S|^{1/2} - t_S + U^{-1}_{S}2 + c \\\\\n&= r^T S r_{S} - \\frac{1}{2}\\log|2US| + \\log \\Phi_{2U^{-1}} \\\\\n&= r^T S r_{S} - \\frac{1}{2}\\log|2US| + \\log \\Phi_{2U^{-1}} + c\n\\end{align*}\n$$\n\nPutting this together, \\(f_{U,v}(y)\\) can be written as:\n\n$$\nf_{U,v}(y) = -v^T U^{-1}v + 1 - y_{S'}^T U_{S'}y_{S'} - y_{S'}^T v_{S'} + r^T S U^{-1}_{S}r_{S} - \\frac{1}{4}\\log|U| - y^T \\frac{1}{2}\\log|US| + |S'|\\log(2) + \\log \\Phi_{0; \\mu = -U^{-1}S r_{S}, \\Sigma = 1}2U^{-1}S + c \\quad (51)$$\n\nThus, it appears that evaluating the likelihood for even a single sample involves the high-dimensional integral that is the rectangular cdf in equation (51).\n\nF.3 Computing Gradients\n\nF.3.1 One Dimensional Case\n\nIn the one-dimensional case, the gradient with respect to u is easily computed as:\n\n$$\n\\nabla_u f_{u,v}(y) = \\sum_{i \\in S'} \\left(vy_i - u \\cdot x_i\\right)x_i - \\sum_{i \\in S} m(u \\cdot x_i)x_i,\n$$\n\nwhere we have previously defined \\(m(x)\\) as the mills ratio. Furthermore, we have:\n\n$$\n\\nabla_v f_{u,v}(y) = |S'| \\mathbf{1} v - \\frac{1}{2} \\sum_{i \\in S'} yi(vy_i - u \\cdot x_i)\n$$"}]}, {"page": 52, "text": "F.3.2     Multidimensional Case\nFirst we consider the non-integral terms in the likelihood. Differentiating each term wrt U, we get\n                                              \u2212\u2207U      vT U \u22121v      = 1\n                                                      1     4            4(U \u22121vvT U \u22121),\n                                                \u2207U    2 log|2U| = 1      2U \u22121,\n                                            \u2212\u2207UyT     S\u2032US\u2032yS\u2032 =          0           0\nDifferentiating each term wrt v, we get                                    0     \u2212yS\u2032yT   S\u2032\n                                                           vT U \u22121v      = \u22121\n                                                   \u2212\u2207v          4               2U \u22121v,\n                                                       \u2212\u2207vyT    S\u2032vS\u2032 =           0\nNow consider the integral term. Differentiating wrt U, we get                  \u2212yS\u2032\n   \u2207U log        tS\u22640   exp     \u2212tT S UStS \u2212      2yT S\u2032US\u2032StS \u2212        vTS tS\n                                                 \uf8eb   \u2212tStT  S      \u2212tSyT   S\u2032 \uf8f6\n                                          tS\u22640   \uf8ed  \u2212yS\u2032tT   S         0      \uf8f8  exp     \u2212tT S  UStS \u2212      2yT S\u2032US\u2032StS \u2212        vTS tS\nLet M be a matrix such that         =                     tS\u22640 exp       \u2212tT S  UStS \u2212      2yTS\u2032US\u2032StS \u2212         vTS tS\nThen via completion of squares in the exponential term, we getM T M = US\n                   \u2207U log              exp     \u2212tT                                       S tS                                            (52)\n                                tS\u22640               S UStS \u2212       2yTS\u2032US\u2032StS \u2212         vT\n                            \uf8eb  \u2212tStT   S      \u2212tSyT   S\u2032 \uf8f6\n                     tS\u22640   \uf8ed  \u2212yS\u2032tT   S         0      \uf8f8  exp     \u2212\u2225MtS + (M \u22121)T  USS\u2032yS\u2032 + vS                    2   \u22252\n                =                    tS\u22640 exp       \u2212\u2225MtS + (M \u22121)T  USS\u2032yS\u2032 + vS                    2   \u22252                              (53)\nNotice that this density is Gaussian, with mean and covariance:\n                                                                                                 S\n                                     N     \u2212M \u22121 M \u22121 T  USS\u2032yS\u2032 + vS                        ; U \u22121      .\n                                                                                       2         2\nAnd hence, the gradient can be estimated as\n                                                                                                \uf8ee\uf8eb    \u2212tStT   S     \u2212tSyT   S\u2032 \uf8f6\uf8f9\n          \u2207U log       tS\u22640    exp    \u2212tT S  UStS \u2212      2yT S\u2032US\u2032StS \u2212        vTS tS    = E tS \uf8f0\uf8ed   \u2212yS\u2032tT   S          0     \uf8f8\uf8fb\nwhere tS is the truncation of\n                                                                                                     S\n                                  z \u223c   N      \u2212M \u22121 M \u22121 T  USS\u2032yS\u2032 + vS                       ; U \u22121      .\n                                                                                           2         2\nto the negative quadrant. A similar calculation gives the gradient for v as\n                     \u2207v log              exp     \u2212tT S UStS \u2212       2yTS\u2032US\u2032StS \u2212        vTS tS     = E        \u2212tS\n                                  tS\u22640                                                                 tS        0\n                                                                      52", "md": "# Math Equations\n\n## Multidimensional Case\n\nFirst we consider the non-integral terms in the likelihood. Differentiating each term with respect to U, we get:\n\n$$\n\\begin{align*}\n-\\nabla U v^T U^{-1}v &= 1 \\\\\n\\nabla U 2\\log|2U| &= 2U^{-1} \\\\\n-\\nabla U y^T S'US'yS' &= \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\end{align*}\n$$\n\nDifferentiating each term with respect to v, we get:\n\n$$\n\\begin{align*}\nv^T U^{-1}v &= -1 \\\\\n-\\nabla v 4 &= 2U^{-1}v \\\\\n-\\nabla v y^T S'vS' &= 0\n\\end{align*}\n$$\n\nNow consider the integral term. Differentiating with respect to U, we get:\n\n$$\n\\begin{align*}\n\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right)\n\\end{align*}\n$$\n\nLet M be a matrix such that:\n\n$$\n\\begin{align*}\nM^T M &= US\n\\end{align*}\n$$\n\nThen via completion of squares in the exponential term, we get:\n\n$$\n\\begin{align*}\n&\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) \\\\\n&= \\int_{t\\leq0} \\exp\\left(-\\|MtS + (M^{-1})^T USS'yS' + vS\\|^2\\)\n\\end{align*}\n$$\n\nNotice that this density is Gaussian, with mean and covariance:\n\n$$\n\\begin{align*}\nN\\left(-M^{-1}M^{-1T}USS'yS' + vS; U^{-1}\\right)\n\\end{align*}\n$$\n\nAnd hence, the gradient can be estimated as:\n\n$$\n\\begin{align*}\n\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= E_{tS}\\left(\\begin{bmatrix} -tSt^T S & -tSy^T S' \\\\ -yS't^T S & 0 \\end{bmatrix}\\right)\n\\end{align*}\n$$\n\nwhere tS is the truncation of:\n\n$$\n\\begin{align*}\nz \\sim N\\left(-M^{-1}M^{-1T}USS'yS' + vS; U^{-1}\\right)\n\\end{align*}\n$$\n\nA similar calculation gives the gradient for v as:\n\n$$\n\\begin{align*}\n\\nabla v \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= E_{tS}\\left(\\begin{bmatrix} -tS \\\\ tS \\end{bmatrix}\\right)\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "heading", "lvl": 2, "value": "Multidimensional Case", "md": "## Multidimensional Case"}, {"type": "text", "value": "First we consider the non-integral terms in the likelihood. Differentiating each term with respect to U, we get:\n\n$$\n\\begin{align*}\n-\\nabla U v^T U^{-1}v &= 1 \\\\\n\\nabla U 2\\log|2U| &= 2U^{-1} \\\\\n-\\nabla U y^T S'US'yS' &= \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\end{align*}\n$$\n\nDifferentiating each term with respect to v, we get:\n\n$$\n\\begin{align*}\nv^T U^{-1}v &= -1 \\\\\n-\\nabla v 4 &= 2U^{-1}v \\\\\n-\\nabla v y^T S'vS' &= 0\n\\end{align*}\n$$\n\nNow consider the integral term. Differentiating with respect to U, we get:\n\n$$\n\\begin{align*}\n\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right)\n\\end{align*}\n$$\n\nLet M be a matrix such that:\n\n$$\n\\begin{align*}\nM^T M &= US\n\\end{align*}\n$$\n\nThen via completion of squares in the exponential term, we get:\n\n$$\n\\begin{align*}\n&\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) \\\\\n&= \\int_{t\\leq0} \\exp\\left(-\\|MtS + (M^{-1})^T USS'yS' + vS\\|^2\\)\n\\end{align*}\n$$\n\nNotice that this density is Gaussian, with mean and covariance:\n\n$$\n\\begin{align*}\nN\\left(-M^{-1}M^{-1T}USS'yS' + vS; U^{-1}\\right)\n\\end{align*}\n$$\n\nAnd hence, the gradient can be estimated as:\n\n$$\n\\begin{align*}\n\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= E_{tS}\\left(\\begin{bmatrix} -tSt^T S & -tSy^T S' \\\\ -yS't^T S & 0 \\end{bmatrix}\\right)\n\\end{align*}\n$$\n\nwhere tS is the truncation of:\n\n$$\n\\begin{align*}\nz \\sim N\\left(-M^{-1}M^{-1T}USS'yS' + vS; U^{-1}\\right)\n\\end{align*}\n$$\n\nA similar calculation gives the gradient for v as:\n\n$$\n\\begin{align*}\n\\nabla v \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= E_{tS}\\left(\\begin{bmatrix} -tS \\\\ tS \\end{bmatrix}\\right)\n\\end{align*}\n$$", "md": "First we consider the non-integral terms in the likelihood. Differentiating each term with respect to U, we get:\n\n$$\n\\begin{align*}\n-\\nabla U v^T U^{-1}v &= 1 \\\\\n\\nabla U 2\\log|2U| &= 2U^{-1} \\\\\n-\\nabla U y^T S'US'yS' &= \\begin{bmatrix} 0 & 0 \\end{bmatrix}\n\\end{align*}\n$$\n\nDifferentiating each term with respect to v, we get:\n\n$$\n\\begin{align*}\nv^T U^{-1}v &= -1 \\\\\n-\\nabla v 4 &= 2U^{-1}v \\\\\n-\\nabla v y^T S'vS' &= 0\n\\end{align*}\n$$\n\nNow consider the integral term. Differentiating with respect to U, we get:\n\n$$\n\\begin{align*}\n\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right)\n\\end{align*}\n$$\n\nLet M be a matrix such that:\n\n$$\n\\begin{align*}\nM^T M &= US\n\\end{align*}\n$$\n\nThen via completion of squares in the exponential term, we get:\n\n$$\n\\begin{align*}\n&\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) \\\\\n&= \\int_{t\\leq0} \\exp\\left(-\\|MtS + (M^{-1})^T USS'yS' + vS\\|^2\\)\n\\end{align*}\n$$\n\nNotice that this density is Gaussian, with mean and covariance:\n\n$$\n\\begin{align*}\nN\\left(-M^{-1}M^{-1T}USS'yS' + vS; U^{-1}\\right)\n\\end{align*}\n$$\n\nAnd hence, the gradient can be estimated as:\n\n$$\n\\begin{align*}\n\\nabla U \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= E_{tS}\\left(\\begin{bmatrix} -tSt^T S & -tSy^T S' \\\\ -yS't^T S & 0 \\end{bmatrix}\\right)\n\\end{align*}\n$$\n\nwhere tS is the truncation of:\n\n$$\n\\begin{align*}\nz \\sim N\\left(-M^{-1}M^{-1T}USS'yS' + vS; U^{-1}\\right)\n\\end{align*}\n$$\n\nA similar calculation gives the gradient for v as:\n\n$$\n\\begin{align*}\n\\nabla v \\log \\int_{t\\leq0} \\exp\\left(-t^T S UStS - 2y^T S'US'StS - vTS tS\\right) &= E_{tS}\\left(\\begin{bmatrix} -tS \\\\ tS \\end{bmatrix}\\right)\n\\end{align*}\n$$"}]}], "job_id": "86f62c27-26a0-46c0-a978-48bf7c95f4ff", "file_path": "./corpus/NeurIPS-pggan.pdf"}