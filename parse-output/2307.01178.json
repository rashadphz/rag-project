{"pages": [{"page": 1, "text": "               Learning Mixtures of Gaussians Using the DDPM Objective\n                                 Kulin Shah\u2217              Sitan Chen\u2020             Adam Klivans\u2021\n                                  UT Austin              UC Berkeley                UT Austin\n                                                           July 4, 2023\narXiv:2307.01178v1  [cs.DS]  3 Jul 2023\n                                                             Abstract\n                    Recent works have shown that diff  usion models can learn essentially any distribution provided\n                one can perform score estimation. Yet it remains poorly understood under what settings score\n                estimation is possible, let alone when practical gradient-based algorithms for this task can\n                provably succeed.\n                    In this work, we give the first provably efficient results along these lines for one of the most\n                fundamental distribution families, Gaussian mixture models. We prove that gradient descent on\n                the denoising diffusion probabilistic model (DDPM) objective can effi    ciently recover the ground\n                truth parameters of the mixture model in the following two settings:\n                   1. We show gradient descent with random initialization learns mixtures of two spherical\n                      Gaussians in d dimensions with 1/poly(d)-separated centers.\n                   2. We show gradient descent with a warm start learns mixtures of K spherical Gaussians\n                      with \u2126(  log(min(K, d)))-separated centers.\n                A key ingredient in our proofs is a new connection between score-based methods and two other\n                approaches to distribution learning, the EM algorithm and spectral methods.\n          Contents\n          1   Introduction                                                                                               2\n              1.1   Related work     . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     3\n              1.2   Technical overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         5\n              1.3   Preliminaries    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     7\n          2   Warmup: mixtures of two Gaussians with constant separation                                                 9\n              2.1   Result and algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .         9\n              2.2   Proof outline of Theorem 7       . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     9\n          3   Extensions: small separation and more Components                                                         11\n              3.1   Mixtures of two Gaussians with small separation           . . . . . . . . . . . . . . . . . . . .   11\n              3.2   Mixtures of K Gaussians, from a warm start . . . . . . . . . . . . . . . . . . . . . . .            12\n             \u2217Email: kulinshah@utexas.edu. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n             \u2020\n              Email: sitan@seas.harvard.edu. Supported by NSF Award 2103300.\n             \u2021\n              Email: klivans@cs.utexas.edu.     Supported by the NSF AI Institute for Foundations of Machine Learning\n          (IFML).\n                                                                  1", "md": "# Learning Mixtures of Gaussians Using the DDPM Objective\n\n# Learning Mixtures of Gaussians Using the DDPM Objective\n\nKulin Shah* UT Austin\n\nSitan Chen\u2020 UC Berkeley\n\nAdam Klivans\u2021 UT Austin\n\nDate: July 4, 2023\n\narXiv:2307.01178v1 [cs.DS] 3 Jul 2023\n\n## Abstract\n\nRecent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.\n\nIn this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings:\n\n1. We show gradient descent with random initialization learns mixtures of two spherical Gaussians in d dimensions with 1/poly(d)-separated centers.\n2. We show gradient descent with a warm start learns mixtures of K spherical Gaussians with \u2126(log(min(K, d)))-separated centers.\n\nA key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, the EM algorithm and spectral methods.\n\n## Contents\n\n1. Introduction\n2. - Related work\n- Technical overview\n- Preliminaries\n\nWarmup: mixtures of two Gaussians with constant separation\n3. - Result and algorithm\n- Proof outline of Theorem 7\n\nExtensions: small separation and more Components\n\n*Email: kulinshah@utexas.edu. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n\n\u2020Email: sitan@seas.harvard.edu. Supported by NSF Award 2103300.\n\n\u2021Email: klivans@cs.utexas.edu. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Learning Mixtures of Gaussians Using the DDPM Objective", "md": "# Learning Mixtures of Gaussians Using the DDPM Objective"}, {"type": "heading", "lvl": 1, "value": "Learning Mixtures of Gaussians Using the DDPM Objective", "md": "# Learning Mixtures of Gaussians Using the DDPM Objective"}, {"type": "text", "value": "Kulin Shah* UT Austin\n\nSitan Chen\u2020 UC Berkeley\n\nAdam Klivans\u2021 UT Austin\n\nDate: July 4, 2023\n\narXiv:2307.01178v1 [cs.DS] 3 Jul 2023", "md": "Kulin Shah* UT Austin\n\nSitan Chen\u2020 UC Berkeley\n\nAdam Klivans\u2021 UT Austin\n\nDate: July 4, 2023\n\narXiv:2307.01178v1 [cs.DS] 3 Jul 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.\n\nIn this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings:\n\n1. We show gradient descent with random initialization learns mixtures of two spherical Gaussians in d dimensions with 1/poly(d)-separated centers.\n2. We show gradient descent with a warm start learns mixtures of K spherical Gaussians with \u2126(log(min(K, d)))-separated centers.\n\nA key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, the EM algorithm and spectral methods.", "md": "Recent works have shown that diffusion models can learn essentially any distribution provided one can perform score estimation. Yet it remains poorly understood under what settings score estimation is possible, let alone when practical gradient-based algorithms for this task can provably succeed.\n\nIn this work, we give the first provably efficient results along these lines for one of the most fundamental distribution families, Gaussian mixture models. We prove that gradient descent on the denoising diffusion probabilistic model (DDPM) objective can efficiently recover the ground truth parameters of the mixture model in the following two settings:\n\n1. We show gradient descent with random initialization learns mixtures of two spherical Gaussians in d dimensions with 1/poly(d)-separated centers.\n2. We show gradient descent with a warm start learns mixtures of K spherical Gaussians with \u2126(log(min(K, d)))-separated centers.\n\nA key ingredient in our proofs is a new connection between score-based methods and two other approaches to distribution learning, the EM algorithm and spectral methods."}, {"type": "heading", "lvl": 2, "value": "Contents", "md": "## Contents"}, {"type": "text", "value": "1. Introduction\n2. - Related work\n- Technical overview\n- Preliminaries\n\nWarmup: mixtures of two Gaussians with constant separation\n3. - Result and algorithm\n- Proof outline of Theorem 7\n\nExtensions: small separation and more Components\n\n*Email: kulinshah@utexas.edu. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n\n\u2020Email: sitan@seas.harvard.edu. Supported by NSF Award 2103300.\n\n\u2021Email: klivans@cs.utexas.edu. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML).", "md": "1. Introduction\n2. - Related work\n- Technical overview\n- Preliminaries\n\nWarmup: mixtures of two Gaussians with constant separation\n3. - Result and algorithm\n- Proof outline of Theorem 7\n\nExtensions: small separation and more Components\n\n*Email: kulinshah@utexas.edu. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n\n\u2020Email: sitan@seas.harvard.edu. Supported by NSF Award 2103300.\n\n\u2021Email: klivans@cs.utexas.edu. Supported by the NSF AI Institute for Foundations of Machine Learning (IFML)."}]}, {"page": 2, "text": "A Proofs from Section 1.3                                                                                                                 18\n     A.1 Xt is a mixture of Gaussians . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                     18\n     A.2 Derivation of score function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                     18\nB Additional notations and preliminaries                                                                                                  19\nC Learning mixtures of two Gaussians with constant separation                                                                             21\n     C.1 High noise regime\u2013connection to power iteration . . . . . . . . . . . . . . . . . . . . .                                        22\n     C.2 Low noise regime - connection to EM algorithm . . . . . . . . . . . . . . . . . . . . .                                          26\nD Learning mixtures of two Gaussians with small separation                                                                                29\nE Learning mixtures of K Gaussians from a warm start                                                                                      31\n     E.1      EM and population gradient descent on DDPM objective                                         . . . . . . . . . . . . . . .  32\n     E.2      Closeness between population gradient descent and empirical gradient descent . . . .                                        37\n     E.3      Proof of Theorem E.1                 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                  39\nF Additional proofs                                                                                                                       40\n     F.1      Proof of Lemma C.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                  40\n     F.2      Proof of Lemma C.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                  41\n     F.3      Proof of Lemma C.10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                 44\n     F.4     Additional proofs for mixtures of two Gaussians . . . . . . . . . . . . . . . . . . . . .                                    48\n1       Introduction\nIn recent years diffusion models [SSDK+20, SDWMG15, SE19] have emerged as a powerful frame-\nwork for generative modeling and now form the backbone of notable image generation systems like\nDALL\u00b7E 2 [RDN+22], Imagen [SCS+22], and Stable Diffusion [RBL+22]. At the heart of this frame-\nwork is a reduction from distribution learning to denoising or score estimation. That is, in order\nto generate new samples from a data distribution q given a collection of independent samples, it\nsuffi ces to learn the score function, i.e., the gradient of the log-density of the data distribution when\nconvolved with varying levels of noise (see Section 1.3). A popular and well-studied objective for\nscore matching is the denoising diff                  usion probabilistic model (DDPM) objective due to [HJA20]. Op-\ntimizing this objective amounts to solving the following type of problem: given a noisy observation\nx of a sample x from q, estimate the mean of the posterior distribution over x.\n      While a number of theoretical works [DBTHD21, BMR22, CLL22, DB22, LLT22, LWYL22,\nPid22, WY22, CCL+23b, CDD23, LLT23, CCL+23a, LWCC23, BDD23] have established rigorous\nconvergence guarantees for diffusion models under mild assumptions on the data distribution, these\nworks assume the existence of an oracle for score estimation and leave open whether one can actually\nprovably implement such an oracle for interesting families of data distributions. In practice, the\nalgorithm of choice for score estimation is simply to train a student network via gradient descent\n(GD) to fi     t a set of examples (x,             x). We thus ask:\n Are there natural data distributions under which GD provably achieves accurate score estimation?\n      In this work, we consider the setting where q is given by a mixture of Gaussians. Concretely, we\nassume that there exist centers \u00b5\u2217                  1, . . . , \u00b5\u2217\n                                                               K \u2208    Rd such that\n                                                                         K\n                                                            q = 1  K    i=1 2N   (\u00b5\u2217i , Id) .", "md": "# Proofs from Section 1.3\n\n## A. Proofs from Section 1.3\n\nA.1 $$X_t$$ is a mixture of Gaussians...\n\nA.2 Derivation of score function...\n\n## B. Additional notations and preliminaries\n\n## C. Learning mixtures of two Gaussians with constant separation\n\nC.1 High noise regime\u2013connection to power iteration...\n\nC.2 Low noise regime - connection to EM algorithm...\n\n## D. Learning mixtures of two Gaussians with small separation\n\n## E. Learning mixtures of K Gaussians from a warm start\n\nE.1 $$EM$$ and population gradient descent on $$DDPM$$ objective...\n\nE.2 Closeness between population gradient descent and empirical gradient descent...\n\nE.3 Proof of Theorem E.1...\n\n## F. Additional proofs\n\nF.1 Proof of Lemma C.2...\n\nF.2 Proof of Lemma C.8...\n\nF.3 Proof of Lemma C.10...\n\nF.4 Additional proofs for mixtures of two Gaussians...\n\n# Introduction\n\nIn recent years diffusion models [SSDK+20, SDWMG15, SE19] have emerged as a powerful framework for generative modeling and now form the backbone of notable image generation systems like DALL\u00b7E 2 [RDN+22], Imagen [SCS+22], and Stable Diffusion [RBL+22]. At the heart of this framework is a reduction from distribution learning to denoising or score estimation. That is, in order to generate new samples from a data distribution $$q$$ given a collection of independent samples, it suffices to learn the score function, i.e., the gradient of the log-density of the data distribution when convolved with varying levels of noise (see Section 1.3). A popular and well-studied objective for score matching is the denoising diffusion probabilistic model ($$DDPM$$) objective due to [HJA20]. Optimizing this objective amounts to solving the following type of problem: given a noisy observation $$x$$ of a sample $$x$$ from $$q$$, estimate the mean of the posterior distribution over $$x$$.\n\nWhile a number of theoretical works [DBTHD21, BMR22, CLL22, DB22, LLT22, LWYL22, Pid22, WY22, CCL+23b, CDD23, LLT23, CCL+23a, LWCC23, BDD23] have established rigorous convergence guarantees for diffusion models under mild assumptions on the data distribution, these works assume the existence of an oracle for score estimation and leave open whether one can actually provably implement such an oracle for interesting families of data distributions. In practice, the algorithm of choice for score estimation is simply to train a student network via gradient descent (GD) to fit a set of examples $$(x, \\hat{x})$$. We thus ask:\n\nAre there natural data distributions under which GD provably achieves accurate score estimation?\n\nIn this work, we consider the setting where $$q$$ is given by a mixture of Gaussians. Concretely, we assume that there exist centers $$\\mu^*_1, ..., \\mu^*_K \\in \\mathbb{R}^d$$ such that\n\n$$q = \\frac{1}{K} \\sum_{i=1}^{K} 2N(\\mu^*_i, I_d)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Proofs from Section 1.3", "md": "# Proofs from Section 1.3"}, {"type": "heading", "lvl": 2, "value": "A. Proofs from Section 1.3", "md": "## A. Proofs from Section 1.3"}, {"type": "text", "value": "A.1 $$X_t$$ is a mixture of Gaussians...\n\nA.2 Derivation of score function...", "md": "A.1 $$X_t$$ is a mixture of Gaussians...\n\nA.2 Derivation of score function..."}, {"type": "heading", "lvl": 2, "value": "B. Additional notations and preliminaries", "md": "## B. Additional notations and preliminaries"}, {"type": "heading", "lvl": 2, "value": "C. Learning mixtures of two Gaussians with constant separation", "md": "## C. Learning mixtures of two Gaussians with constant separation"}, {"type": "text", "value": "C.1 High noise regime\u2013connection to power iteration...\n\nC.2 Low noise regime - connection to EM algorithm...", "md": "C.1 High noise regime\u2013connection to power iteration...\n\nC.2 Low noise regime - connection to EM algorithm..."}, {"type": "heading", "lvl": 2, "value": "D. Learning mixtures of two Gaussians with small separation", "md": "## D. Learning mixtures of two Gaussians with small separation"}, {"type": "heading", "lvl": 2, "value": "E. Learning mixtures of K Gaussians from a warm start", "md": "## E. Learning mixtures of K Gaussians from a warm start"}, {"type": "text", "value": "E.1 $$EM$$ and population gradient descent on $$DDPM$$ objective...\n\nE.2 Closeness between population gradient descent and empirical gradient descent...\n\nE.3 Proof of Theorem E.1...", "md": "E.1 $$EM$$ and population gradient descent on $$DDPM$$ objective...\n\nE.2 Closeness between population gradient descent and empirical gradient descent...\n\nE.3 Proof of Theorem E.1..."}, {"type": "heading", "lvl": 2, "value": "F. Additional proofs", "md": "## F. Additional proofs"}, {"type": "text", "value": "F.1 Proof of Lemma C.2...\n\nF.2 Proof of Lemma C.8...\n\nF.3 Proof of Lemma C.10...\n\nF.4 Additional proofs for mixtures of two Gaussians...", "md": "F.1 Proof of Lemma C.2...\n\nF.2 Proof of Lemma C.8...\n\nF.3 Proof of Lemma C.10...\n\nF.4 Additional proofs for mixtures of two Gaussians..."}, {"type": "heading", "lvl": 1, "value": "Introduction", "md": "# Introduction"}, {"type": "text", "value": "In recent years diffusion models [SSDK+20, SDWMG15, SE19] have emerged as a powerful framework for generative modeling and now form the backbone of notable image generation systems like DALL\u00b7E 2 [RDN+22], Imagen [SCS+22], and Stable Diffusion [RBL+22]. At the heart of this framework is a reduction from distribution learning to denoising or score estimation. That is, in order to generate new samples from a data distribution $$q$$ given a collection of independent samples, it suffices to learn the score function, i.e., the gradient of the log-density of the data distribution when convolved with varying levels of noise (see Section 1.3). A popular and well-studied objective for score matching is the denoising diffusion probabilistic model ($$DDPM$$) objective due to [HJA20]. Optimizing this objective amounts to solving the following type of problem: given a noisy observation $$x$$ of a sample $$x$$ from $$q$$, estimate the mean of the posterior distribution over $$x$$.\n\nWhile a number of theoretical works [DBTHD21, BMR22, CLL22, DB22, LLT22, LWYL22, Pid22, WY22, CCL+23b, CDD23, LLT23, CCL+23a, LWCC23, BDD23] have established rigorous convergence guarantees for diffusion models under mild assumptions on the data distribution, these works assume the existence of an oracle for score estimation and leave open whether one can actually provably implement such an oracle for interesting families of data distributions. In practice, the algorithm of choice for score estimation is simply to train a student network via gradient descent (GD) to fit a set of examples $$(x, \\hat{x})$$. We thus ask:\n\nAre there natural data distributions under which GD provably achieves accurate score estimation?\n\nIn this work, we consider the setting where $$q$$ is given by a mixture of Gaussians. Concretely, we assume that there exist centers $$\\mu^*_1, ..., \\mu^*_K \\in \\mathbb{R}^d$$ such that\n\n$$q = \\frac{1}{K} \\sum_{i=1}^{K} 2N(\\mu^*_i, I_d)$$.", "md": "In recent years diffusion models [SSDK+20, SDWMG15, SE19] have emerged as a powerful framework for generative modeling and now form the backbone of notable image generation systems like DALL\u00b7E 2 [RDN+22], Imagen [SCS+22], and Stable Diffusion [RBL+22]. At the heart of this framework is a reduction from distribution learning to denoising or score estimation. That is, in order to generate new samples from a data distribution $$q$$ given a collection of independent samples, it suffices to learn the score function, i.e., the gradient of the log-density of the data distribution when convolved with varying levels of noise (see Section 1.3). A popular and well-studied objective for score matching is the denoising diffusion probabilistic model ($$DDPM$$) objective due to [HJA20]. Optimizing this objective amounts to solving the following type of problem: given a noisy observation $$x$$ of a sample $$x$$ from $$q$$, estimate the mean of the posterior distribution over $$x$$.\n\nWhile a number of theoretical works [DBTHD21, BMR22, CLL22, DB22, LLT22, LWYL22, Pid22, WY22, CCL+23b, CDD23, LLT23, CCL+23a, LWCC23, BDD23] have established rigorous convergence guarantees for diffusion models under mild assumptions on the data distribution, these works assume the existence of an oracle for score estimation and leave open whether one can actually provably implement such an oracle for interesting families of data distributions. In practice, the algorithm of choice for score estimation is simply to train a student network via gradient descent (GD) to fit a set of examples $$(x, \\hat{x})$$. We thus ask:\n\nAre there natural data distributions under which GD provably achieves accurate score estimation?\n\nIn this work, we consider the setting where $$q$$ is given by a mixture of Gaussians. Concretely, we assume that there exist centers $$\\mu^*_1, ..., \\mu^*_K \\in \\mathbb{R}^d$$ such that\n\n$$q = \\frac{1}{K} \\sum_{i=1}^{K} 2N(\\mu^*_i, I_d)$$."}]}, {"page": 3, "text": "We answer the above question in the affi                           rmative for this class of distributions:\nTheorem 1 (Informal, see Theorems 7 and 13). Gradient descent on the DDPM objective with ran-\ndom initialization efficiently learns the parameters of an unknown mixture of two spherical Gaussians\nwith 1/poly(d)-separated centers.\nTheorem 2 (Informal, see Theorem 16). When there is a warm start of the centers, gradient\ndescent on the DDPM objective efficiently learns the parameters an unknown mixture of K spherical\nGaussians with \u2126(              log(min(K, d)))-separated centers.\nThe DDPM objective is described in Algorithm 1. The term \u201ceffi                                                ciently\u201d above means that both\n the running time and sample complexity of our algorithm is polynomial in the dimension d, the\n inverse accuracy 1/\u03b5, and the number of components K. In the informal discussion, we often work\nwith population gradients for simplicity, but in our proofs we show that empirical estimates of the\n gradient suffi       ce (full details can be found in the Appendix).\n   Algorithm 1: GMMDenoiser(t, {\u00b5(0)}K                           i     i=1, H)\n      Input: Noise scale t, initialization {\u00b5(0)                     i }K  i=1, number of gradient descent steps H\n   1 Initialize the parameters for the score estimate at \u03b8(0)                              t    = {\u00b5(0) i,t }Ki=1 (see Eq. (9) for how the\n        estimate s\u03b8 depends on the parameters \u03b8, and Eq. (8) for the defi                                           nition of \u00b5(0)    i,t )\n   2 Run gradient descent on the DDPM objective Lt(s\u03b8                                       t) for H steps where\n                                               Lt(s\u03b8   t) = E       s\u03b8t(Xt) +                    Zt               2   ,\n                                                                                          1 \u2212    exp(\u22122t)\n   3 return \u03b8(H)      t     = {\u00b5(H) i,t }K i=1 where \u03b8(H)     t     denotes the parameters after H steps of GD.\nWe refer to Section 1.3 for a formal description of the quantities used in Algorithm 1. Note that\n there are by now a host of different algorithms for provably learning mixtures of Gaussians (see\n Section 1.1). For instance, it is already known that expectation-maximization (EM) achieves the\n quantitative guarantees of Theorems 1 and 2 [DTZ17, XHM16, KC20, SN21], and in fact even\n stronger guarantees are known via the method of moments. Unlike works based on the method of\n moments however, our algorithm is practical. And unlike works based on EM, it is based on an\n approach which is empirically successful for a wide range of realistic data distributions. Furthermore,\n as we discuss in Section 1.2, the analysis of Algorithm 1 leverages an intriguing and, to our knowledge,\n novel connection from score estimation to EM, as well as to another notable approach for learning\n mixture models, namely spectral methods. Roughly speaking, at large noise levels, the gradient\n updates in Algorithm 1 are essentially performing a type of power iteration, while at small noise\n levels, the gradient updates are performing the \u201cM\u201d step in the EM algorithm.\n 1.1       Related work\n Theory for diffusion models.                            A number of works have given convergence guarantees for DDPMs\n and variants [DBTHD21, BMR22, CLL22, DB22, LLT22, LWYL22, Pid22, WY22, CCL+23b, CDD23,\n LLT23, LWCC23, BDD23, CCL+23a]. These results show that, given an oracle for accurate score\n estimation, diffusion models can learn essentially any distribution over Rd (e.g. [CCL+23b, LLT23,\n CLL22] show this for arbitrary compactly supported distributions). Additionally, two recent works\n                                                                                 3", "md": "# Math Equations and Text\n\nWe answer the above question in the affirmative for this class of distributions:\n\nTheorem 1 (Informal, see Theorems 7 and 13). Gradient descent on the DDPM objective with random initialization efficiently learns the parameters of an unknown mixture of two spherical Gaussians with $$\\frac{1}{\\text{poly}(d)}$$-separated centers.\n\nTheorem 2 (Informal, see Theorem 16). When there is a warm start of the centers, gradient descent on the DDPM objective efficiently learns the parameters an unknown mixture of K spherical Gaussians with $$\\Omega(\\log(\\min(K, d)))$$-separated centers.\n\nThe DDPM objective is described in Algorithm 1. The term \u201cefficiently\u201d above means that both the running time and sample complexity of our algorithm is polynomial in the dimension d, the inverse accuracy $$\\frac{1}{\\varepsilon}$$, and the number of components K. In the informal discussion, we often work with population gradients for simplicity, but in our proofs we show that empirical estimates of the gradient suffice (full details can be found in the Appendix).\n\nAlgorithm 1: GMMDenoiser(t, {\u00b5(0)}K i=1, H)\n\nInput: Noise scale t, initialization {\u00b5(0)i}K i=1, number of gradient descent steps H\n\n1. Initialize the parameters for the score estimate at \u03b8(0)t = {\u00b5(0)i,t}K i=1 (see Eq. (9) for how the estimate s\u03b8 depends on the parameters \u03b8, and Eq. (8) for the definition of \u00b5(0)i,t)\n2. Run gradient descent on the DDPM objective $L_t(s\u03b8_t)$ for H steps where $L_t(s\u03b8_t) = E[s\u03b8_t(X_t) + \\frac{Z_t}{1 - \\exp(-2t)}]$\n3. Return \u03b8(H)t = {\u00b5(H)i,t}K i=1 where \u03b8(H)t denotes the parameters after H steps of GD.\n\nWe refer to Section 1.3 for a formal description of the quantities used in Algorithm 1. Note that there are by now a host of different algorithms for provably learning mixtures of Gaussians (see Section 1.1). For instance, it is already known that expectation-maximization (EM) achieves the quantitative guarantees of Theorems 1 and 2 [DTZ17, XHM16, KC20, SN21], and in fact even stronger guarantees are known via the method of moments. Unlike works based on the method of moments however, our algorithm is practical. And unlike works based on EM, it is based on an approach which is empirically successful for a wide range of realistic data distributions. Furthermore, as we discuss in Section 1.2, the analysis of Algorithm 1 leverages an intriguing and, to our knowledge, novel connection from score estimation to EM, as well as to another notable approach for learning mixture models, namely spectral methods. Roughly speaking, at large noise levels, the gradient updates in Algorithm 1 are essentially performing a type of power iteration, while at small noise levels, the gradient updates are performing the \u201cM\u201d step in the EM algorithm.\n\n## 1.1 Related work\n\nTheory for diffusion models. A number of works have given convergence guarantees for DDPMs and variants [DBTHD21, BMR22, CLL22, DB22, LLT22, LWYL22, Pid22, WY22, CCL+23b, CDD23, LLT23, LWCC23, BDD23, CCL+23a]. These results show that, given an oracle for accurate score estimation, diffusion models can learn essentially any distribution over $$\\mathbb{R}^d$$ (e.g. [CCL+23b, LLT23, CLL22] show this for arbitrary compactly supported distributions). Additionally, two recent works", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "We answer the above question in the affirmative for this class of distributions:\n\nTheorem 1 (Informal, see Theorems 7 and 13). Gradient descent on the DDPM objective with random initialization efficiently learns the parameters of an unknown mixture of two spherical Gaussians with $$\\frac{1}{\\text{poly}(d)}$$-separated centers.\n\nTheorem 2 (Informal, see Theorem 16). When there is a warm start of the centers, gradient descent on the DDPM objective efficiently learns the parameters an unknown mixture of K spherical Gaussians with $$\\Omega(\\log(\\min(K, d)))$$-separated centers.\n\nThe DDPM objective is described in Algorithm 1. The term \u201cefficiently\u201d above means that both the running time and sample complexity of our algorithm is polynomial in the dimension d, the inverse accuracy $$\\frac{1}{\\varepsilon}$$, and the number of components K. In the informal discussion, we often work with population gradients for simplicity, but in our proofs we show that empirical estimates of the gradient suffice (full details can be found in the Appendix).\n\nAlgorithm 1: GMMDenoiser(t, {\u00b5(0)}K i=1, H)\n\nInput: Noise scale t, initialization {\u00b5(0)i}K i=1, number of gradient descent steps H\n\n1. Initialize the parameters for the score estimate at \u03b8(0)t = {\u00b5(0)i,t}K i=1 (see Eq. (9) for how the estimate s\u03b8 depends on the parameters \u03b8, and Eq. (8) for the definition of \u00b5(0)i,t)\n2. Run gradient descent on the DDPM objective $L_t(s\u03b8_t)$ for H steps where $L_t(s\u03b8_t) = E[s\u03b8_t(X_t) + \\frac{Z_t}{1 - \\exp(-2t)}]$\n3. Return \u03b8(H)t = {\u00b5(H)i,t}K i=1 where \u03b8(H)t denotes the parameters after H steps of GD.\n\nWe refer to Section 1.3 for a formal description of the quantities used in Algorithm 1. Note that there are by now a host of different algorithms for provably learning mixtures of Gaussians (see Section 1.1). For instance, it is already known that expectation-maximization (EM) achieves the quantitative guarantees of Theorems 1 and 2 [DTZ17, XHM16, KC20, SN21], and in fact even stronger guarantees are known via the method of moments. Unlike works based on the method of moments however, our algorithm is practical. And unlike works based on EM, it is based on an approach which is empirically successful for a wide range of realistic data distributions. Furthermore, as we discuss in Section 1.2, the analysis of Algorithm 1 leverages an intriguing and, to our knowledge, novel connection from score estimation to EM, as well as to another notable approach for learning mixture models, namely spectral methods. Roughly speaking, at large noise levels, the gradient updates in Algorithm 1 are essentially performing a type of power iteration, while at small noise levels, the gradient updates are performing the \u201cM\u201d step in the EM algorithm.", "md": "We answer the above question in the affirmative for this class of distributions:\n\nTheorem 1 (Informal, see Theorems 7 and 13). Gradient descent on the DDPM objective with random initialization efficiently learns the parameters of an unknown mixture of two spherical Gaussians with $$\\frac{1}{\\text{poly}(d)}$$-separated centers.\n\nTheorem 2 (Informal, see Theorem 16). When there is a warm start of the centers, gradient descent on the DDPM objective efficiently learns the parameters an unknown mixture of K spherical Gaussians with $$\\Omega(\\log(\\min(K, d)))$$-separated centers.\n\nThe DDPM objective is described in Algorithm 1. The term \u201cefficiently\u201d above means that both the running time and sample complexity of our algorithm is polynomial in the dimension d, the inverse accuracy $$\\frac{1}{\\varepsilon}$$, and the number of components K. In the informal discussion, we often work with population gradients for simplicity, but in our proofs we show that empirical estimates of the gradient suffice (full details can be found in the Appendix).\n\nAlgorithm 1: GMMDenoiser(t, {\u00b5(0)}K i=1, H)\n\nInput: Noise scale t, initialization {\u00b5(0)i}K i=1, number of gradient descent steps H\n\n1. Initialize the parameters for the score estimate at \u03b8(0)t = {\u00b5(0)i,t}K i=1 (see Eq. (9) for how the estimate s\u03b8 depends on the parameters \u03b8, and Eq. (8) for the definition of \u00b5(0)i,t)\n2. Run gradient descent on the DDPM objective $L_t(s\u03b8_t)$ for H steps where $L_t(s\u03b8_t) = E[s\u03b8_t(X_t) + \\frac{Z_t}{1 - \\exp(-2t)}]$\n3. Return \u03b8(H)t = {\u00b5(H)i,t}K i=1 where \u03b8(H)t denotes the parameters after H steps of GD.\n\nWe refer to Section 1.3 for a formal description of the quantities used in Algorithm 1. Note that there are by now a host of different algorithms for provably learning mixtures of Gaussians (see Section 1.1). For instance, it is already known that expectation-maximization (EM) achieves the quantitative guarantees of Theorems 1 and 2 [DTZ17, XHM16, KC20, SN21], and in fact even stronger guarantees are known via the method of moments. Unlike works based on the method of moments however, our algorithm is practical. And unlike works based on EM, it is based on an approach which is empirically successful for a wide range of realistic data distributions. Furthermore, as we discuss in Section 1.2, the analysis of Algorithm 1 leverages an intriguing and, to our knowledge, novel connection from score estimation to EM, as well as to another notable approach for learning mixture models, namely spectral methods. Roughly speaking, at large noise levels, the gradient updates in Algorithm 1 are essentially performing a type of power iteration, while at small noise levels, the gradient updates are performing the \u201cM\u201d step in the EM algorithm."}, {"type": "heading", "lvl": 2, "value": "1.1 Related work", "md": "## 1.1 Related work"}, {"type": "text", "value": "Theory for diffusion models. A number of works have given convergence guarantees for DDPMs and variants [DBTHD21, BMR22, CLL22, DB22, LLT22, LWYL22, Pid22, WY22, CCL+23b, CDD23, LLT23, LWCC23, BDD23, CCL+23a]. These results show that, given an oracle for accurate score estimation, diffusion models can learn essentially any distribution over $$\\mathbb{R}^d$$ (e.g. [CCL+23b, LLT23, CLL22] show this for arbitrary compactly supported distributions). Additionally, two recent works", "md": "Theory for diffusion models. A number of works have given convergence guarantees for DDPMs and variants [DBTHD21, BMR22, CLL22, DB22, LLT22, LWYL22, Pid22, WY22, CCL+23b, CDD23, LLT23, LWCC23, BDD23, CCL+23a]. These results show that, given an oracle for accurate score estimation, diffusion models can learn essentially any distribution over $$\\mathbb{R}^d$$ (e.g. [CCL+23b, LLT23, CLL22] show this for arbitrary compactly supported distributions). Additionally, two recent works"}]}, {"page": 4, "text": "[EAMS22, MW23] have used Eldan\u2019s stochastic localization [Eld13, Eld20], which is a reparametriza-\ntion in time and space of the reverse SDE for DDPMs, to give sampling algorithms for certain\ndistributions arising in statistical physics. As we discuss next, these works are end-to-end in that\nthey also give provable algorithms for score estimation via approximate message passing, though\nthe statistical task they address is not distribution learning.\nProvable score estimation.         There is a rich literature giving Bayes-optimal algorithms for var-\nious natural denoising problems via methods inspired by statistical physics, like approximate mes-\nsage passing (AMP) (e.g. [MV21, CFM21, BM11, Kab03, DMM09, DMM10]) and natural gradient\ndescent (NGD) on the TAP free energy [CFM21, EAMS22, Cel22]. The abovementioned works\n[EAMS22, MW23] (see also [Cel22]) build on these techniques to give algorithms for the denoising\nproblems that arise in their implementation of stochastic localization. These works on denoising\nvia AMP or NGD are themselves part of a broader literature on variational inference, a suitable\nliterature review would be beyond the scope of this work, see e.g. [BKM17, WJ+08, MM09].\n    We are not aware of any provable algorithms for score estimation explicitly in the context\nof distribution learning.   That said, it may be possible to extract a distribution learning result\nfrom [EAMS22]. While their algorithm was for sampling from the Sherrington-Kirkpatrick (SK)\nmodel given the Hamiltonian rather than training examples as input, if one is instead given training\nexamples drawn from the SK measure, then at suffi     ciently high temperature one can approximately\nrecover the Hamiltonian [AG22]. In this case, a suitable modifi    cation [EAMS22] should be able to\nyield an algorithm for approximately generating fresh samples from the SK model given training\nexamples.\nLearning mixtures of Gaussians.           The literature on provable algorithms for learning Gaussian\nmixture models is vast, dating back to the pioneering work of Pearson [Pea94], and we cannot do\njustice to it here. We mention only works whose quantitative guarantees are closest in spirit to\nours and refer to the introduction of [LL22] for a comprehensive overview of recent works in this\ndirection. For mixtures of identity-covariance Gaussians in high dimensions, the strongest existing\nguarantee is a polynomial-time algorithm [LL22] for learning the centers as long as their pairwise\nseparation slightly exceeds \u2126(\u221a  log K) based on a sophisticated instantiation of method of moments\ninspired by the quasipolynomial-time algorithms of [DKS18, HL18, KSS18]. By the lower bound in\n[RV17], this is essentially optimal. In contrast, our Theorem 2 only applies given one initializes in\na neighborhood of the true parameters of the mixture. We also note the exponential-time spectral\nalgorithm of [SOAJ14] and quasipolynomial-time tensor-based algorithm of [DK20], which achieve\ndensity estimation even in the regime where the centers are arbitrarily closely spaced and learning\nthe centers is information-theoretically impossible.\n    A separate line of work has investigated the \u201ctextbook\u201d algorithm for learning Gaussian mixtures,\nnamely the EM algorithm [BWY17, DS07, DTZ17, XHM16, YYS17, ZLS20, KC20, SN21]. Notably,\nfor balanced mixtures of two Gaussians with the same covariance, [DTZ17] showed that fi     nite-sample\nEM with random initialization converges exponentially quickly to the true centers. For mixtures of\nK Gaussians with identity covariance, [KC20, SN21] showed that from an initialization suffi      ciently\nclose to the true centers, finite-sample EM converges exponentially quickly to the true centers as\nlong as their pairwise separation is \u2126(\u221alog K). In particular, [SN21] establish this local convergence\nas long as every center estimate is initialized at distance at most \u2206/2 away from the corresponding\ntrue center, where \u2206    is the minimum separation between any pair of true centers; this radius of\nconvergence is provably best possible for EM.\n    Lastly, we note that there are many works giving parameter recovery algorithms mixtures of\n                                                    4", "md": "# Math Equations and Text\n\n[EAMS22, MW23] have used Eldan\u2019s stochastic localization [Eld13, Eld20], which is a reparametrization in time and space of the reverse SDE for DDPMs, to give sampling algorithms for certain distributions arising in statistical physics. As we discuss next, these works are end-to-end in that they also give provable algorithms for score estimation via approximate message passing, though the statistical task they address is not distribution learning.\n\nProvable score estimation. There is a rich literature giving Bayes-optimal algorithms for various natural denoising problems via methods inspired by statistical physics, like approximate message passing (AMP) (e.g. [MV21, CFM21, BM11, Kab03, DMM09, DMM10]) and natural gradient descent (NGD) on the TAP free energy [CFM21, EAMS22, Cel22]. The abovementioned works [EAMS22, MW23] (see also [Cel22]) build on these techniques to give algorithms for the denoising problems that arise in their implementation of stochastic localization. These works on denoising via AMP or NGD are themselves part of a broader literature on variational inference, a suitable literature review would be beyond the scope of this work, see e.g. [BKM17, WJ+08, MM09].\n\nWe are not aware of any provable algorithms for score estimation explicitly in the context of distribution learning. That said, it may be possible to extract a distribution learning result from [EAMS22]. While their algorithm was for sampling from the Sherrington-Kirkpatrick (SK) model given the Hamiltonian rather than training examples as input, if one is instead given training examples drawn from the SK measure, then at sufficiently high temperature one can approximately recover the Hamiltonian [AG22]. In this case, a suitable modification [EAMS22] should be able to yield an algorithm for approximately generating fresh samples from the SK model given training examples.\n\nLearning mixtures of Gaussians. The literature on provable algorithms for learning Gaussian mixture models is vast, dating back to the pioneering work of Pearson [Pea94], and we cannot do justice to it here. We mention only works whose quantitative guarantees are closest in spirit to ours and refer to the introduction of [LL22] for a comprehensive overview of recent works in this direction. For mixtures of identity-covariance Gaussians in high dimensions, the strongest existing guarantee is a polynomial-time algorithm [LL22] for learning the centers as long as their pairwise separation slightly exceeds $$\\Omega(\\sqrt{\\log K})$$ based on a sophisticated instantiation of method of moments inspired by the quasipolynomial-time algorithms of [DKS18, HL18, KSS18]. By the lower bound in [RV17], this is essentially optimal. In contrast, our Theorem 2 only applies given one initializes in a neighborhood of the true parameters of the mixture. We also note the exponential-time spectral algorithm of [SOAJ14] and quasipolynomial-time tensor-based algorithm of [DK20], which achieve density estimation even in the regime where the centers are arbitrarily closely spaced and learning the centers is information-theoretically impossible.\n\nA separate line of work has investigated the \u201ctextbook\u201d algorithm for learning Gaussian mixtures, namely the EM algorithm [BWY17, DS07, DTZ17, XHM16, YYS17, ZLS20, KC20, SN21]. Notably, for balanced mixtures of two Gaussians with the same covariance, [DTZ17] showed that finite-sample EM with random initialization converges exponentially quickly to the true centers. For mixtures of K Gaussians with identity covariance, [KC20, SN21] showed that from an initialization sufficiently close to the true centers, finite-sample EM converges exponentially quickly to the true centers as long as their pairwise separation is $$\\Omega(\\sqrt{\\log K})$$. In particular, [SN21] establish this local convergence as long as every center estimate is initialized at distance at most $$\\Delta/2$$ away from the corresponding true center, where $$\\Delta$$ is the minimum separation between any pair of true centers; this radius of convergence is provably best possible for EM.\n\nLastly, we note that there are many works giving parameter recovery algorithms mixtures of", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "[EAMS22, MW23] have used Eldan\u2019s stochastic localization [Eld13, Eld20], which is a reparametrization in time and space of the reverse SDE for DDPMs, to give sampling algorithms for certain distributions arising in statistical physics. As we discuss next, these works are end-to-end in that they also give provable algorithms for score estimation via approximate message passing, though the statistical task they address is not distribution learning.\n\nProvable score estimation. There is a rich literature giving Bayes-optimal algorithms for various natural denoising problems via methods inspired by statistical physics, like approximate message passing (AMP) (e.g. [MV21, CFM21, BM11, Kab03, DMM09, DMM10]) and natural gradient descent (NGD) on the TAP free energy [CFM21, EAMS22, Cel22]. The abovementioned works [EAMS22, MW23] (see also [Cel22]) build on these techniques to give algorithms for the denoising problems that arise in their implementation of stochastic localization. These works on denoising via AMP or NGD are themselves part of a broader literature on variational inference, a suitable literature review would be beyond the scope of this work, see e.g. [BKM17, WJ+08, MM09].\n\nWe are not aware of any provable algorithms for score estimation explicitly in the context of distribution learning. That said, it may be possible to extract a distribution learning result from [EAMS22]. While their algorithm was for sampling from the Sherrington-Kirkpatrick (SK) model given the Hamiltonian rather than training examples as input, if one is instead given training examples drawn from the SK measure, then at sufficiently high temperature one can approximately recover the Hamiltonian [AG22]. In this case, a suitable modification [EAMS22] should be able to yield an algorithm for approximately generating fresh samples from the SK model given training examples.\n\nLearning mixtures of Gaussians. The literature on provable algorithms for learning Gaussian mixture models is vast, dating back to the pioneering work of Pearson [Pea94], and we cannot do justice to it here. We mention only works whose quantitative guarantees are closest in spirit to ours and refer to the introduction of [LL22] for a comprehensive overview of recent works in this direction. For mixtures of identity-covariance Gaussians in high dimensions, the strongest existing guarantee is a polynomial-time algorithm [LL22] for learning the centers as long as their pairwise separation slightly exceeds $$\\Omega(\\sqrt{\\log K})$$ based on a sophisticated instantiation of method of moments inspired by the quasipolynomial-time algorithms of [DKS18, HL18, KSS18]. By the lower bound in [RV17], this is essentially optimal. In contrast, our Theorem 2 only applies given one initializes in a neighborhood of the true parameters of the mixture. We also note the exponential-time spectral algorithm of [SOAJ14] and quasipolynomial-time tensor-based algorithm of [DK20], which achieve density estimation even in the regime where the centers are arbitrarily closely spaced and learning the centers is information-theoretically impossible.\n\nA separate line of work has investigated the \u201ctextbook\u201d algorithm for learning Gaussian mixtures, namely the EM algorithm [BWY17, DS07, DTZ17, XHM16, YYS17, ZLS20, KC20, SN21]. Notably, for balanced mixtures of two Gaussians with the same covariance, [DTZ17] showed that finite-sample EM with random initialization converges exponentially quickly to the true centers. For mixtures of K Gaussians with identity covariance, [KC20, SN21] showed that from an initialization sufficiently close to the true centers, finite-sample EM converges exponentially quickly to the true centers as long as their pairwise separation is $$\\Omega(\\sqrt{\\log K})$$. In particular, [SN21] establish this local convergence as long as every center estimate is initialized at distance at most $$\\Delta/2$$ away from the corresponding true center, where $$\\Delta$$ is the minimum separation between any pair of true centers; this radius of convergence is provably best possible for EM.\n\nLastly, we note that there are many works giving parameter recovery algorithms mixtures of", "md": "[EAMS22, MW23] have used Eldan\u2019s stochastic localization [Eld13, Eld20], which is a reparametrization in time and space of the reverse SDE for DDPMs, to give sampling algorithms for certain distributions arising in statistical physics. As we discuss next, these works are end-to-end in that they also give provable algorithms for score estimation via approximate message passing, though the statistical task they address is not distribution learning.\n\nProvable score estimation. There is a rich literature giving Bayes-optimal algorithms for various natural denoising problems via methods inspired by statistical physics, like approximate message passing (AMP) (e.g. [MV21, CFM21, BM11, Kab03, DMM09, DMM10]) and natural gradient descent (NGD) on the TAP free energy [CFM21, EAMS22, Cel22]. The abovementioned works [EAMS22, MW23] (see also [Cel22]) build on these techniques to give algorithms for the denoising problems that arise in their implementation of stochastic localization. These works on denoising via AMP or NGD are themselves part of a broader literature on variational inference, a suitable literature review would be beyond the scope of this work, see e.g. [BKM17, WJ+08, MM09].\n\nWe are not aware of any provable algorithms for score estimation explicitly in the context of distribution learning. That said, it may be possible to extract a distribution learning result from [EAMS22]. While their algorithm was for sampling from the Sherrington-Kirkpatrick (SK) model given the Hamiltonian rather than training examples as input, if one is instead given training examples drawn from the SK measure, then at sufficiently high temperature one can approximately recover the Hamiltonian [AG22]. In this case, a suitable modification [EAMS22] should be able to yield an algorithm for approximately generating fresh samples from the SK model given training examples.\n\nLearning mixtures of Gaussians. The literature on provable algorithms for learning Gaussian mixture models is vast, dating back to the pioneering work of Pearson [Pea94], and we cannot do justice to it here. We mention only works whose quantitative guarantees are closest in spirit to ours and refer to the introduction of [LL22] for a comprehensive overview of recent works in this direction. For mixtures of identity-covariance Gaussians in high dimensions, the strongest existing guarantee is a polynomial-time algorithm [LL22] for learning the centers as long as their pairwise separation slightly exceeds $$\\Omega(\\sqrt{\\log K})$$ based on a sophisticated instantiation of method of moments inspired by the quasipolynomial-time algorithms of [DKS18, HL18, KSS18]. By the lower bound in [RV17], this is essentially optimal. In contrast, our Theorem 2 only applies given one initializes in a neighborhood of the true parameters of the mixture. We also note the exponential-time spectral algorithm of [SOAJ14] and quasipolynomial-time tensor-based algorithm of [DK20], which achieve density estimation even in the regime where the centers are arbitrarily closely spaced and learning the centers is information-theoretically impossible.\n\nA separate line of work has investigated the \u201ctextbook\u201d algorithm for learning Gaussian mixtures, namely the EM algorithm [BWY17, DS07, DTZ17, XHM16, YYS17, ZLS20, KC20, SN21]. Notably, for balanced mixtures of two Gaussians with the same covariance, [DTZ17] showed that finite-sample EM with random initialization converges exponentially quickly to the true centers. For mixtures of K Gaussians with identity covariance, [KC20, SN21] showed that from an initialization sufficiently close to the true centers, finite-sample EM converges exponentially quickly to the true centers as long as their pairwise separation is $$\\Omega(\\sqrt{\\log K})$$. In particular, [SN21] establish this local convergence as long as every center estimate is initialized at distance at most $$\\Delta/2$$ away from the corresponding true center, where $$\\Delta$$ is the minimum separation between any pair of true centers; this radius of convergence is provably best possible for EM.\n\nLastly, we note that there are many works giving parameter recovery algorithms mixtures of"}]}, {"page": 5, "text": "Gaussians with general mixing weights and covariances, all of which are based on method of mo-\nments [KMV10, HP15, Kan21, BS15, MV10, LM23, BDJ+22, DHKK20]. Unfortunately, for general\nmixtures of K Gaussians, these algorithms run in time at least dO(K), and there is strong evi-\ndence [DKS17, BRST21] that this is unavoidable for computationally effi                  cient algorithms.\n1.2     Technical overview\nWe begin by describing in greater detail the algorithm we analyze in this work. For the sake of\nintuition, in this overview we will focus on the case of mixtures of two Gaussians (K = 2) where the\ncenters are well-separated and symmetric about the origin, that is, the data distribution is given by\n                                         q = 12N(\u00b5\u2217, Id) + 1   2N(\u2212\u00b5\u2217, Id) .                                         (1)\nAt the end of the overview, we briefl       y discuss the key challenges for handling smaller separation and\ngeneral K.\nLoss function, architecture of the score function and student network.                               The algorithmic\ntask at the heart of score estimation is that of denoising. Formally, for some noise level t > 0, we\nare given a noisy sample             Xt = exp(\u2212t)X0 +         1 \u2212   exp(\u22122t)Zt ,\nwhere X0 is a clean sample drawn from the data distribution q, and Zt \u223c                     N(0, Id). Conditioning\non Xt induces some posterior distribution over the noise Zt, and our goal is to form an estimate s\nfor the mean of this posterior which achieves small error on average over the randomness of X0 and\nZt. That is, we would like to minimize the DDPM objective, which up to rescaling is given by1\n                                          Lt(s) = EX    0,Zt\u2225s(Xt) \u2212    Zt\u22252 .\nAs discussed in the introduction, the algorithm of choice for minimizing this objective in practice is\ngradient descent on some student network. To motivate our choice of architecture, note that when\nthe data distribution is given by (1), the true minimizer of Lt is, up to scaling,\n                                tanh(\u27e8\u00b5\u2217 t , x\u27e9)\u00b5\u2217          where \u00b5\u2217                                                 (2)\n                                                 t \u2212  x ,            t \u225c  \u00b5\u2217  exp(\u2212t) .\nSee Appendix A for the derivation. Notably, Eq. (2) is exactly a two-layer neural network with tanh\nactivation. As a result, we use the same architecture for our student network when running gradient\ndescent. That is, given weights \u00b5 \u2208          Rd, our student network is given by s\u00b5(x) \u225c             tanh(\u00b5\u22a4x)\u00b5 \u2212      x.\nThe exact gradient updates on \u00b5 are given in Lemma C.2.\n    As we discuss next, depending on whether the noise level t is large or small, this update closely\napproximates the update in one of two well-studied algorithms for learning mixtures of Gaussians:\npower method and EM respectively.\nLearning mixtures of two Gaussians.                  We fi rst provide a brief overview of the analysis and then\ngo into the details of the analysis. We start with mixtures of two Gaussians of the form (1) where\n\u2225\u00b5\u2217\u2225   is \u2126(1). In this case, we analyze the following two-stage algorithm. We fi                    rst use gradient\ndescent on the DDPM objective with large t starting from random initialization. We show that\ngradient descent in this \u201chigh noise\u201d regime resembles a type of power iteration and gives \u00b5 that\nhas a nontrivial correlation with \u00b5\u2217      t. Starting from this \u00b5, we then run gradient descent with small\nt. We show that the gradient descent in this \u201csmall noise\u201d regime corresponds to the EM algorithm\nand converges exponentially quickly to the ground truth.\n   1\n    The real DDPM objective is slightly different, see (5). The latter is what we actually consider in this paper, but\nthis distinction is unimportant for the intuition in this overview.\n                                                            5", "md": "# Document\n\n## Gaussians with general mixing weights and covariances\n\nAll of which are based on the method of moments [KMV10], [HP15], [Kan21], [BS15], [MV10], [LM23], [BDJ+22], [DHKK20]. Unfortunately, for general mixtures of K Gaussians, these algorithms run in time at least dO(K), and there is strong evidence [DKS17], [BRST21] that this is unavoidable for computationally efficient algorithms.\n\n## Technical overview\n\nWe begin by describing in greater detail the algorithm we analyze in this work. For the sake of intuition, in this overview, we will focus on the case of mixtures of two Gaussians (K = 2) where the centers are well-separated and symmetric about the origin, that is, the data distribution is given by\n\n$$q = \\frac{1}{2}N(\\mu^*, I_d) + \\frac{1}{2}N(-\\mu^*, I_d)$$\nAt the end of the overview, we briefly discuss the key challenges for handling smaller separation and general K.\n\n### Loss function, architecture of the score function, and student network\n\nThe algorithmic task at the heart of score estimation is that of denoising. Formally, for some noise level t > 0, we are given a noisy sample\n\n$$X_t = \\exp(-t)X_0 + (1 - \\exp(-2t))Z_t$$\nwhere \\(X_0\\) is a clean sample drawn from the data distribution q, and \\(Z_t \\sim N(0, I_d)\\). Conditioning on \\(X_t\\) induces some posterior distribution over the noise \\(Z_t\\), and our goal is to form an estimate s for the mean of this posterior which achieves small error on average over the randomness of \\(X_0\\) and \\(Z_t\\). That is, we would like to minimize the DDPM objective, which up to rescaling is given by\n\n$$L_t(s) = \\mathbb{E}_{X_0,Z_t} \\|s(X_t) - Z_t\\|^2$$\nAs discussed in the introduction, the algorithm of choice for minimizing this objective in practice is gradient descent on some student network. To motivate our choice of architecture, note that when the data distribution is given by (1), the true minimizer of \\(L_t\\) is, up to scaling,\n\n$$\\tanh(\\langle\\mu^*_t, x\\rangle)\\mu^*_t \\quad \\text{where} \\quad \\mu^*_t = x, \\quad t \\triangleq \\mu^* \\exp(-t)$$\nSee Appendix A for the derivation. Notably, Eq. (2) is exactly a two-layer neural network with tanh activation. As a result, we use the same architecture for our student network when running gradient descent. That is, given weights \\(\\mu \\in \\mathbb{R}^d\\), our student network is given by \\(s_\\mu(x) = \\tanh(\\mu^\\top x)\\mu - x\\). The exact gradient updates on \\(\\mu\\) are given in Lemma C.2.\n\nAs we discuss next, depending on whether the noise level t is large or small, this update closely approximates the update in one of two well-studied algorithms for learning mixtures of Gaussians: power method and EM respectively.\n\n### Learning mixtures of two Gaussians\n\nWe first provide a brief overview of the analysis and then go into the details of the analysis. We start with mixtures of two Gaussians of the form (1) where \\(\\|\\mu^*\\|\\) is \\(\\Omega(1)\\). In this case, we analyze the following two-stage algorithm. We first use gradient descent on the DDPM objective with large t starting from random initialization. We show that gradient descent in this \u201chigh noise\u201d regime resembles a type of power iteration and gives \\(\\mu\\) that has a nontrivial correlation with \\(\\mu^*_t\\). Starting from this \\(\\mu\\), we then run gradient descent with small t. We show that the gradient descent in this \u201csmall noise\u201d regime corresponds to the EM algorithm and converges exponentially quickly to the ground truth.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Gaussians with general mixing weights and covariances", "md": "## Gaussians with general mixing weights and covariances"}, {"type": "text", "value": "All of which are based on the method of moments [KMV10], [HP15], [Kan21], [BS15], [MV10], [LM23], [BDJ+22], [DHKK20]. Unfortunately, for general mixtures of K Gaussians, these algorithms run in time at least dO(K), and there is strong evidence [DKS17], [BRST21] that this is unavoidable for computationally efficient algorithms.", "md": "All of which are based on the method of moments [KMV10], [HP15], [Kan21], [BS15], [MV10], [LM23], [BDJ+22], [DHKK20]. Unfortunately, for general mixtures of K Gaussians, these algorithms run in time at least dO(K), and there is strong evidence [DKS17], [BRST21] that this is unavoidable for computationally efficient algorithms."}, {"type": "heading", "lvl": 2, "value": "Technical overview", "md": "## Technical overview"}, {"type": "text", "value": "We begin by describing in greater detail the algorithm we analyze in this work. For the sake of intuition, in this overview, we will focus on the case of mixtures of two Gaussians (K = 2) where the centers are well-separated and symmetric about the origin, that is, the data distribution is given by\n\n$$q = \\frac{1}{2}N(\\mu^*, I_d) + \\frac{1}{2}N(-\\mu^*, I_d)$$\nAt the end of the overview, we briefly discuss the key challenges for handling smaller separation and general K.", "md": "We begin by describing in greater detail the algorithm we analyze in this work. For the sake of intuition, in this overview, we will focus on the case of mixtures of two Gaussians (K = 2) where the centers are well-separated and symmetric about the origin, that is, the data distribution is given by\n\n$$q = \\frac{1}{2}N(\\mu^*, I_d) + \\frac{1}{2}N(-\\mu^*, I_d)$$\nAt the end of the overview, we briefly discuss the key challenges for handling smaller separation and general K."}, {"type": "heading", "lvl": 3, "value": "Loss function, architecture of the score function, and student network", "md": "### Loss function, architecture of the score function, and student network"}, {"type": "text", "value": "The algorithmic task at the heart of score estimation is that of denoising. Formally, for some noise level t > 0, we are given a noisy sample\n\n$$X_t = \\exp(-t)X_0 + (1 - \\exp(-2t))Z_t$$\nwhere \\(X_0\\) is a clean sample drawn from the data distribution q, and \\(Z_t \\sim N(0, I_d)\\). Conditioning on \\(X_t\\) induces some posterior distribution over the noise \\(Z_t\\), and our goal is to form an estimate s for the mean of this posterior which achieves small error on average over the randomness of \\(X_0\\) and \\(Z_t\\). That is, we would like to minimize the DDPM objective, which up to rescaling is given by\n\n$$L_t(s) = \\mathbb{E}_{X_0,Z_t} \\|s(X_t) - Z_t\\|^2$$\nAs discussed in the introduction, the algorithm of choice for minimizing this objective in practice is gradient descent on some student network. To motivate our choice of architecture, note that when the data distribution is given by (1), the true minimizer of \\(L_t\\) is, up to scaling,\n\n$$\\tanh(\\langle\\mu^*_t, x\\rangle)\\mu^*_t \\quad \\text{where} \\quad \\mu^*_t = x, \\quad t \\triangleq \\mu^* \\exp(-t)$$\nSee Appendix A for the derivation. Notably, Eq. (2) is exactly a two-layer neural network with tanh activation. As a result, we use the same architecture for our student network when running gradient descent. That is, given weights \\(\\mu \\in \\mathbb{R}^d\\), our student network is given by \\(s_\\mu(x) = \\tanh(\\mu^\\top x)\\mu - x\\). The exact gradient updates on \\(\\mu\\) are given in Lemma C.2.\n\nAs we discuss next, depending on whether the noise level t is large or small, this update closely approximates the update in one of two well-studied algorithms for learning mixtures of Gaussians: power method and EM respectively.", "md": "The algorithmic task at the heart of score estimation is that of denoising. Formally, for some noise level t > 0, we are given a noisy sample\n\n$$X_t = \\exp(-t)X_0 + (1 - \\exp(-2t))Z_t$$\nwhere \\(X_0\\) is a clean sample drawn from the data distribution q, and \\(Z_t \\sim N(0, I_d)\\). Conditioning on \\(X_t\\) induces some posterior distribution over the noise \\(Z_t\\), and our goal is to form an estimate s for the mean of this posterior which achieves small error on average over the randomness of \\(X_0\\) and \\(Z_t\\). That is, we would like to minimize the DDPM objective, which up to rescaling is given by\n\n$$L_t(s) = \\mathbb{E}_{X_0,Z_t} \\|s(X_t) - Z_t\\|^2$$\nAs discussed in the introduction, the algorithm of choice for minimizing this objective in practice is gradient descent on some student network. To motivate our choice of architecture, note that when the data distribution is given by (1), the true minimizer of \\(L_t\\) is, up to scaling,\n\n$$\\tanh(\\langle\\mu^*_t, x\\rangle)\\mu^*_t \\quad \\text{where} \\quad \\mu^*_t = x, \\quad t \\triangleq \\mu^* \\exp(-t)$$\nSee Appendix A for the derivation. Notably, Eq. (2) is exactly a two-layer neural network with tanh activation. As a result, we use the same architecture for our student network when running gradient descent. That is, given weights \\(\\mu \\in \\mathbb{R}^d\\), our student network is given by \\(s_\\mu(x) = \\tanh(\\mu^\\top x)\\mu - x\\). The exact gradient updates on \\(\\mu\\) are given in Lemma C.2.\n\nAs we discuss next, depending on whether the noise level t is large or small, this update closely approximates the update in one of two well-studied algorithms for learning mixtures of Gaussians: power method and EM respectively."}, {"type": "heading", "lvl": 3, "value": "Learning mixtures of two Gaussians", "md": "### Learning mixtures of two Gaussians"}, {"type": "text", "value": "We first provide a brief overview of the analysis and then go into the details of the analysis. We start with mixtures of two Gaussians of the form (1) where \\(\\|\\mu^*\\|\\) is \\(\\Omega(1)\\). In this case, we analyze the following two-stage algorithm. We first use gradient descent on the DDPM objective with large t starting from random initialization. We show that gradient descent in this \u201chigh noise\u201d regime resembles a type of power iteration and gives \\(\\mu\\) that has a nontrivial correlation with \\(\\mu^*_t\\). Starting from this \\(\\mu\\), we then run gradient descent with small t. We show that the gradient descent in this \u201csmall noise\u201d regime corresponds to the EM algorithm and converges exponentially quickly to the ground truth.", "md": "We first provide a brief overview of the analysis and then go into the details of the analysis. We start with mixtures of two Gaussians of the form (1) where \\(\\|\\mu^*\\|\\) is \\(\\Omega(1)\\). In this case, we analyze the following two-stage algorithm. We first use gradient descent on the DDPM objective with large t starting from random initialization. We show that gradient descent in this \u201chigh noise\u201d regime resembles a type of power iteration and gives \\(\\mu\\) that has a nontrivial correlation with \\(\\mu^*_t\\). Starting from this \\(\\mu\\), we then run gradient descent with small t. We show that the gradient descent in this \u201csmall noise\u201d regime corresponds to the EM algorithm and converges exponentially quickly to the ground truth."}]}, {"page": 6, "text": "Large noise level: connection to power iteration.                                            When t is large, we show that gradient\ndescent on the DDPM objective is closely approximated by power iteration. More precisely, in this\nregime, the negative gradient of Lt(s\u00b5) is well-approximated by\n                                                    \u2212\u2207\u00b5Lt(s\u00b5) \u2248            (2\u00b5\u2217 t \u00b5\u2217\u22a4    \u2212  rId) \u00b5 ,\n                                                                                    t\nwhere r is a scalar that depends on \u00b5 (See Lemma 8). So the result of a single gradient update with\nstep size \u03b7 starting from \u00b5 is given by\n                                       \u00b5\u2032 \u225c    \u00b5 \u2212    \u03b7\u2207\u00b5Lt(s\u00b5) \u2248          ((1 \u2212    \u03b7r) Id + 2\u03b7\u00b5\u2217       t \u00b5\u2217\u22a4                                         (3)\n                                                                                                           t )\u00b5 .\nThis shows us that each gradient step can be approximated by one step of power iteration (without\nnormalization) on the matrix (1 \u2212                        \u03b7r) Id + 2\u03b7\u00b5\u2217       t \u00b5\u2217\u22a4\n                                                                                t . It is know that running enough iterations\nof the latter from a random initialization will converge in angular distance to the top eigenvector,\nwhich in this case is given by \u00b5\u2217                 t. This suggests that if we can keep the approximation error in (3)\nunder control, then gradient descent on \u00b5 will also allow us to converge to a neighborhood of the\nground truth. We implement this strategy in Lemma 10. Next, we argue that once we are in a\nneighborhood of the ground truth, we can run GD on the DDPM objective at low noise level to\nrefi ne our estimate.\nLow noise level: connection to the EM algorithm.                                              When t is small, we show that gradient\ndescent on the DDPM objective is closely approximated by EM. Here, our analysis uses the fact\nthat \u00b5\u2217      is suffi ciently large and requires that we initialize \u00b5 to have suffi                            ciently large correlation with\nthe true direction \u00b5\u2217          t . We can achieve the latter using the large-t analysis in the previous section.\n      Provided we have this, when t is small it turns out that the negative gradient is well-approximated\nby\n                                         \u2212\u2207\u00b5Lt(s\u00b5) \u2248           EX\u223cN (\u00b5\u2217     t ,Id)[tanh(\u27e8\u00b5, X\u27e9)X] \u2212            \u00b5 .\nNote that the expectation is precisely the \u201cM\u201d-step in the EM algorithm for learning mixtures of\ntwo Gaussians (see e.g. Eq. (2.2) of [DTZ17]). We conclude that a single gradient update with step\nsize \u03b7 starting from \u00b5 is given by mixing the old weights \u00b5 with the result of the \u201cM\u201d-step in EM:\n                           \u00b5\u2032 \u225c    \u00b5 \u2212   \u03b7\u2207\u00b5Lt(s\u00b5) \u2248           (1 \u2212   \u03b7)\u00b5 + \u03b7 EX\u223cN (\u00b5\u2217         t ,Id)[tanh(\u27e8\u00b5, X\u27e9)X]            .\n                                                                                      \u201cM\u201d step in the EM algorithm\n[XHM16] and [DTZ17] showed that EM converges exponentially quickly to the ground truth \u00b5\u2217                                                                t\nfrom a warm start, and we leverage ingredients from their analysis to prove the same guarantee for\ngradient descent on the DDPM objective at small noise level t (see Lemma 12).\nExtending to small separation.                            Next, suppose we instead only assume that \u2225\u00b5\u2217\u2225                            is \u2126(1/poly(d)),\ni.e. the two components in the mixture may have small separation. The above analysis breaks down\nfor the following reason: while it is always possible to show that gradient descent at large noise level\nconverges in angular distance to the ground truth, if \u2225\u00b5\u2217\u2225                                  is small, then we cannot translate this to\nconvergence in Euclidean distance.\n      We circumvent this as follows. Extending the connection between gradient descent at large t and\npower iteration, we show that a similar analysis where we instead run projected gradient descent\nover the ball of radius \u2225\u00b5\u2217\u2225                yields a solution arbitrarily close to the ground truth, even without the\n                                                                             6", "md": "# Math Equations and Text\n\n## Large noise level: connection to power iteration\n\nWhen \\( t \\) is large, we show that gradient descent on the DDPM objective is closely approximated by power iteration. More precisely, in this regime, the negative gradient of \\( L_t(s\\mu) \\) is well-approximated by\n\n$$\n-\\nabla_{\\mu}L_t(s\\mu) \\approx \\left(2\\mu^* t \\mu^{*T} - rI_d\\right) \\mu,\n$$\nwhere \\( r \\) is a scalar that depends on \\( \\mu \\) (See Lemma 8). So the result of a single gradient update with step size \\( \\eta \\) starting from \\( \\mu \\) is given by\n\n$$\n\\mu' \\triangleq \\mu - \\eta\\nabla_{\\mu}L_t(s\\mu) \\approx \\left((1 - \\eta r)I_d + 2\\eta\\mu^* t \\mu^{*T}\\right)\\mu.\n$$\nThis shows us that each gradient step can be approximated by one step of power iteration (without normalization) on the matrix \\( (1 - \\eta r)I_d + 2\\eta\\mu^* t \\mu^{*T} \\). It is known that running enough iterations of the latter from a random initialization will converge in angular distance to the top eigenvector, which in this case is given by \\( \\mu^* t \\). This suggests that if we can keep the approximation error in the above equation under control, then gradient descent on \\( \\mu \\) will also allow us to converge to a neighborhood of the ground truth. We implement this strategy in Lemma 10.\n\n## Low noise level: connection to the EM algorithm\n\nWhen \\( t \\) is small, we show that gradient descent on the DDPM objective is closely approximated by EM. Here, our analysis uses the fact that \\( \\mu^* \\) is sufficiently large and requires that we initialize \\( \\mu \\) to have sufficiently large correlation with the true direction \\( \\mu^* t \\). We can achieve the latter using the large-\\( t \\) analysis in the previous section. Provided we have this, when \\( t \\) is small it turns out that the negative gradient is well-approximated by\n\n$$\n-\\nabla_{\\mu}L_t(s\\mu) \\approx \\mathbb{E}_{X \\sim N(\\mu^* t, I_d)}[\\tanh(\\langle \\mu, X \\rangle)X] - \\mu.\n$$\nNote that the expectation is precisely the \"M\"-step in the EM algorithm for learning mixtures of two Gaussians. We conclude that a single gradient update with step size \\( \\eta \\) starting from \\( \\mu \\) is given by mixing the old weights \\( \\mu \\) with the result of the \"M\"-step in EM:\n\n$$\n\\mu' \\triangleq \\mu - \\eta\\nabla_{\\mu}L_t(s\\mu) \\approx (1 - \\eta)\\mu + \\eta\\mathbb{E}_{X \\sim N(\\mu^* t, I_d)}[\\tanh(\\langle \\mu, X \\rangle)X].\n$$\n[XHM16] and [DTZ17] showed that EM converges exponentially quickly to the ground truth \\( \\mu^* t \\) from a warm start, and we leverage ingredients from their analysis to prove the same guarantee for gradient descent on the DDPM objective at small noise level \\( t \\) (see Lemma 12).\n\n## Extending to small separation\n\nNext, suppose we instead only assume that \\( \\|\\mu^*\\| \\) is \\( \\Omega(1/\\text{poly}(d)) \\), i.e., the two components in the mixture may have small separation. The above analysis breaks down for the following reason: while it is always possible to show that gradient descent at large noise level converges in angular distance to the ground truth, if \\( \\|\\mu^*\\| \\) is small, then we cannot translate this to convergence in Euclidean distance.\n\nWe circumvent this as follows. Extending the connection between gradient descent at large \\( t \\) and power iteration, we show that a similar analysis where we instead run projected gradient descent over the ball of radius \\( \\|\\mu^*\\| \\) yields a solution arbitrarily close to the ground truth, even without the", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Large noise level: connection to power iteration", "md": "## Large noise level: connection to power iteration"}, {"type": "text", "value": "When \\( t \\) is large, we show that gradient descent on the DDPM objective is closely approximated by power iteration. More precisely, in this regime, the negative gradient of \\( L_t(s\\mu) \\) is well-approximated by\n\n$$\n-\\nabla_{\\mu}L_t(s\\mu) \\approx \\left(2\\mu^* t \\mu^{*T} - rI_d\\right) \\mu,\n$$\nwhere \\( r \\) is a scalar that depends on \\( \\mu \\) (See Lemma 8). So the result of a single gradient update with step size \\( \\eta \\) starting from \\( \\mu \\) is given by\n\n$$\n\\mu' \\triangleq \\mu - \\eta\\nabla_{\\mu}L_t(s\\mu) \\approx \\left((1 - \\eta r)I_d + 2\\eta\\mu^* t \\mu^{*T}\\right)\\mu.\n$$\nThis shows us that each gradient step can be approximated by one step of power iteration (without normalization) on the matrix \\( (1 - \\eta r)I_d + 2\\eta\\mu^* t \\mu^{*T} \\). It is known that running enough iterations of the latter from a random initialization will converge in angular distance to the top eigenvector, which in this case is given by \\( \\mu^* t \\). This suggests that if we can keep the approximation error in the above equation under control, then gradient descent on \\( \\mu \\) will also allow us to converge to a neighborhood of the ground truth. We implement this strategy in Lemma 10.", "md": "When \\( t \\) is large, we show that gradient descent on the DDPM objective is closely approximated by power iteration. More precisely, in this regime, the negative gradient of \\( L_t(s\\mu) \\) is well-approximated by\n\n$$\n-\\nabla_{\\mu}L_t(s\\mu) \\approx \\left(2\\mu^* t \\mu^{*T} - rI_d\\right) \\mu,\n$$\nwhere \\( r \\) is a scalar that depends on \\( \\mu \\) (See Lemma 8). So the result of a single gradient update with step size \\( \\eta \\) starting from \\( \\mu \\) is given by\n\n$$\n\\mu' \\triangleq \\mu - \\eta\\nabla_{\\mu}L_t(s\\mu) \\approx \\left((1 - \\eta r)I_d + 2\\eta\\mu^* t \\mu^{*T}\\right)\\mu.\n$$\nThis shows us that each gradient step can be approximated by one step of power iteration (without normalization) on the matrix \\( (1 - \\eta r)I_d + 2\\eta\\mu^* t \\mu^{*T} \\). It is known that running enough iterations of the latter from a random initialization will converge in angular distance to the top eigenvector, which in this case is given by \\( \\mu^* t \\). This suggests that if we can keep the approximation error in the above equation under control, then gradient descent on \\( \\mu \\) will also allow us to converge to a neighborhood of the ground truth. We implement this strategy in Lemma 10."}, {"type": "heading", "lvl": 2, "value": "Low noise level: connection to the EM algorithm", "md": "## Low noise level: connection to the EM algorithm"}, {"type": "text", "value": "When \\( t \\) is small, we show that gradient descent on the DDPM objective is closely approximated by EM. Here, our analysis uses the fact that \\( \\mu^* \\) is sufficiently large and requires that we initialize \\( \\mu \\) to have sufficiently large correlation with the true direction \\( \\mu^* t \\). We can achieve the latter using the large-\\( t \\) analysis in the previous section. Provided we have this, when \\( t \\) is small it turns out that the negative gradient is well-approximated by\n\n$$\n-\\nabla_{\\mu}L_t(s\\mu) \\approx \\mathbb{E}_{X \\sim N(\\mu^* t, I_d)}[\\tanh(\\langle \\mu, X \\rangle)X] - \\mu.\n$$\nNote that the expectation is precisely the \"M\"-step in the EM algorithm for learning mixtures of two Gaussians. We conclude that a single gradient update with step size \\( \\eta \\) starting from \\( \\mu \\) is given by mixing the old weights \\( \\mu \\) with the result of the \"M\"-step in EM:\n\n$$\n\\mu' \\triangleq \\mu - \\eta\\nabla_{\\mu}L_t(s\\mu) \\approx (1 - \\eta)\\mu + \\eta\\mathbb{E}_{X \\sim N(\\mu^* t, I_d)}[\\tanh(\\langle \\mu, X \\rangle)X].\n$$\n[XHM16] and [DTZ17] showed that EM converges exponentially quickly to the ground truth \\( \\mu^* t \\) from a warm start, and we leverage ingredients from their analysis to prove the same guarantee for gradient descent on the DDPM objective at small noise level \\( t \\) (see Lemma 12).", "md": "When \\( t \\) is small, we show that gradient descent on the DDPM objective is closely approximated by EM. Here, our analysis uses the fact that \\( \\mu^* \\) is sufficiently large and requires that we initialize \\( \\mu \\) to have sufficiently large correlation with the true direction \\( \\mu^* t \\). We can achieve the latter using the large-\\( t \\) analysis in the previous section. Provided we have this, when \\( t \\) is small it turns out that the negative gradient is well-approximated by\n\n$$\n-\\nabla_{\\mu}L_t(s\\mu) \\approx \\mathbb{E}_{X \\sim N(\\mu^* t, I_d)}[\\tanh(\\langle \\mu, X \\rangle)X] - \\mu.\n$$\nNote that the expectation is precisely the \"M\"-step in the EM algorithm for learning mixtures of two Gaussians. We conclude that a single gradient update with step size \\( \\eta \\) starting from \\( \\mu \\) is given by mixing the old weights \\( \\mu \\) with the result of the \"M\"-step in EM:\n\n$$\n\\mu' \\triangleq \\mu - \\eta\\nabla_{\\mu}L_t(s\\mu) \\approx (1 - \\eta)\\mu + \\eta\\mathbb{E}_{X \\sim N(\\mu^* t, I_d)}[\\tanh(\\langle \\mu, X \\rangle)X].\n$$\n[XHM16] and [DTZ17] showed that EM converges exponentially quickly to the ground truth \\( \\mu^* t \\) from a warm start, and we leverage ingredients from their analysis to prove the same guarantee for gradient descent on the DDPM objective at small noise level \\( t \\) (see Lemma 12)."}, {"type": "heading", "lvl": 2, "value": "Extending to small separation", "md": "## Extending to small separation"}, {"type": "text", "value": "Next, suppose we instead only assume that \\( \\|\\mu^*\\| \\) is \\( \\Omega(1/\\text{poly}(d)) \\), i.e., the two components in the mixture may have small separation. The above analysis breaks down for the following reason: while it is always possible to show that gradient descent at large noise level converges in angular distance to the ground truth, if \\( \\|\\mu^*\\| \\) is small, then we cannot translate this to convergence in Euclidean distance.\n\nWe circumvent this as follows. Extending the connection between gradient descent at large \\( t \\) and power iteration, we show that a similar analysis where we instead run projected gradient descent over the ball of radius \\( \\|\\mu^*\\| \\) yields a solution arbitrarily close to the ground truth, even without the", "md": "Next, suppose we instead only assume that \\( \\|\\mu^*\\| \\) is \\( \\Omega(1/\\text{poly}(d)) \\), i.e., the two components in the mixture may have small separation. The above analysis breaks down for the following reason: while it is always possible to show that gradient descent at large noise level converges in angular distance to the ground truth, if \\( \\|\\mu^*\\| \\) is small, then we cannot translate this to convergence in Euclidean distance.\n\nWe circumvent this as follows. Extending the connection between gradient descent at large \\( t \\) and power iteration, we show that a similar analysis where we instead run projected gradient descent over the ball of radius \\( \\|\\mu^*\\| \\) yields a solution arbitrarily close to the ground truth, even without the"}]}, {"page": 7, "text": "EM step.2 The projection step can be thought of as mimicking the normalization step in power\niteration.\n      It might appear to the reader that this projected gradient-based approach is strictly superior to\nthe two-stage algorithm described at the outset. However, in addition to obviating the need for a\nprojection step when separation is large, our analysis for the two-stage algorithm has the advantage\nof giving much more favorable statistical rates. Indeed, we can show that the sample complexity\nof the two-stage algorithm has optimal dependence on the target error (1/\u03b52), whereas we can only\nshow a suboptimal dependence (1/\u03b58) for the single-stage algorithm.\nExtending to general K.                          The connection between gradient descent on the DDPM objective\nat small t and the EM algorithm is suffi                        ciently robust that for general K, our analysis for K = 2\ncan generalize once we replace the ingredients from [XHM16] and [DTZ17] with the analogous\ningredients in existing analyses for EM with K Gaussians. For the latter, it is known that if the\ncenters of the Gaussians have separation \u2126(                              log min(K, d)), then EM will converge from a warm\nstart [KC20, SN21]. By carefully tracking the error in approximating the negative gradient with\nthe \u201cM\u201d-step in EM, we are able to show that gradient descent on the DDPM objective at small t\nachieves the same guarantee.\n1.3       Preliminaries\nDiffusion models.                  Throughout the paper, we use either q or q0 to denote the data distribution\nand X or X0 to denote the corresponding random variable on Rd. The two main components in\ndiffusion models are the forward process and the reverse process. The forward process transforms\nsamples from the data distribution into noise, for instance via the Ornstein-Uhlenbeck (OU) process:\n                                             dXt = \u2212Xt dt +            \u221a  2 dWt       with      X0 \u223c      q0 ,\nwhere (Wt)t\u22650 is a standard Brownian motion in Rd. We use qt to denote the law of the OU process\nat time t. Note that for Xt \u223c                   qt,\n                       Xt = exp(\u2212t)X0 +                  1 \u2212   exp(\u22122t)Zt           with X0 \u223c          q0, Zt \u223c       N  (0, Id) .\n      The reverse process then transforms noise into samples, thus performing generative modeling.\nIdeally, this could be achieved by running the following stochastic differential equation for some\nchoice of terminal time T               :\n                          dX\u2190  t   = {X\u2190    t   + 2\u2207x ln qT\u2212t(X\u2190         t )} dt +      \u221a  2 dWt       with      X\u2190 0 \u223c     qT ,\nwhere now Wt is the reversed Brownian motion. In this reverse process, the iterate X\u2190                                              t   is distributed\nacccording to qT\u2212t for every t \u2208                    [0, T  ], so that the fi       nal iterate X\u2190      T    is distributed according to the\ndata distribution q0. The function \u2207x ln qt is called the score function, and because it depends on\nq which is unknown, in practice one estimates it by minimizing the score matching loss\n                                               min      EX   t\u223cqt[\u2225st(Xt) \u2212         \u2207x ln qt(Xt)\u22252] .                                                  (4)\n                                                 st\nA standard calculation (see e.g. Appendix A of [CCL+23b]) shows that this is equivalent to mini-\nmizing the DDPM objective in which one wants to predict the noise Zt from the noisy observation\nXt, i.e.\n                                     min     Lt(st) = EX        0,Zt    st(Xt) +                  Zt              2   .                                (5)\n                                      st                                                   1 \u2212    exp(\u22122t)\n    2 Note that although \u00b5\u2217          is unknown, we can estimate its norm from samples.\n                                                                             7", "md": "EM step.2 The projection step can be thought of as mimicking the normalization step in power iteration.\n\nIt might appear to the reader that this projected gradient-based approach is strictly superior to the two-stage algorithm described at the outset. However, in addition to obviating the need for a projection step when separation is large, our analysis for the two-stage algorithm has the advantage of giving much more favorable statistical rates. Indeed, we can show that the sample complexity of the two-stage algorithm has optimal dependence on the target error ($$ \\frac{1}{\\epsilon^2} $$), whereas we can only show a suboptimal dependence ($$ \\frac{1}{\\epsilon^8} $$) for the single-stage algorithm.\n\nExtending to general K. The connection between gradient descent on the DDPM objective at small t and the EM algorithm is sufficiently robust that for general K, our analysis for K = 2 can generalize once we replace the ingredients from [XHM16] and [DTZ17] with the analogous ingredients in existing analyses for EM with K Gaussians. For the latter, it is known that if the centers of the Gaussians have separation \u2126(log min(K, d)), then EM will converge from a warm start [KC20, SN21]. By carefully tracking the error in approximating the negative gradient with the \u201cM\u201d-step in EM, we are able to show that gradient descent on the DDPM objective at small t achieves the same guarantee.\n\n## 1.3 Preliminaries\n\nDiffusion models. Throughout the paper, we use either q or q0 to denote the data distribution and X or X0 to denote the corresponding random variable on Rd. The two main components in diffusion models are the forward process and the reverse process. The forward process transforms samples from the data distribution into noise, for instance via the Ornstein-Uhlenbeck (OU) process:\n\n$$\ndX_t = -X_t dt + \\sqrt{2} dW_t \\quad \\text{with} \\quad X_0 \\sim q_0,\n$$\n\nwhere (W_t)_{t\\geq0} is a standard Brownian motion in Rd. We use q_t to denote the law of the OU process at time t. Note that for X_t \\sim q_t,\n\n$$\nX_t = \\exp(-t)X_0 + (1 - \\exp(-2t))Z_t \\quad \\text{with} \\quad X_0 \\sim q_0, Z_t \\sim N(0, Id).\n$$\n\nThe reverse process then transforms noise into samples, thus performing generative modeling. Ideally, this could be achieved by running the following stochastic differential equation for some choice of terminal time T:\n\n$$\ndX_{\\leftarrow t} = \\left\\{X_{\\leftarrow t} + 2\\nabla_x \\ln q_{T-t}(X_{\\leftarrow t})\\right\\} dt + \\sqrt{2} dW_t \\quad \\text{with} \\quad X_{\\leftarrow 0} \\sim q_T,\n$$\n\nwhere now W_t is the reversed Brownian motion. In this reverse process, the iterate X_{\\leftarrow t} is distributed according to q_{T-t} for every t \\in [0, T], so that the final iterate X_{\\leftarrow T} is distributed according to the data distribution q_0. The function \\nabla_x \\ln q_t is called the score function, and because it depends on q which is unknown, in practice one estimates it by minimizing the score matching loss\n\n$$\n\\min \\mathbb{E}_{X_t \\sim q_t}[\\|s_t(X_t) - \\nabla_x \\ln q_t(X_t)\\|^2].\n$$\n\nA standard calculation (see e.g. Appendix A of [CCL+23b]) shows that this is equivalent to minimizing the DDPM objective in which one wants to predict the noise Z_t from the noisy observation X_t, i.e.\n\n$$\n\\min L_t(s) = \\mathbb{E}_{X_0,Z_t} [s_t(X_t) + Z_t]^2 \\quad \\text{with} \\quad 1 - \\exp(-2t)\n$$\n\nNote that although \u00b5* is unknown, we can estimate its norm from samples.", "images": [], "items": [{"type": "text", "value": "EM step.2 The projection step can be thought of as mimicking the normalization step in power iteration.\n\nIt might appear to the reader that this projected gradient-based approach is strictly superior to the two-stage algorithm described at the outset. However, in addition to obviating the need for a projection step when separation is large, our analysis for the two-stage algorithm has the advantage of giving much more favorable statistical rates. Indeed, we can show that the sample complexity of the two-stage algorithm has optimal dependence on the target error ($$ \\frac{1}{\\epsilon^2} $$), whereas we can only show a suboptimal dependence ($$ \\frac{1}{\\epsilon^8} $$) for the single-stage algorithm.\n\nExtending to general K. The connection between gradient descent on the DDPM objective at small t and the EM algorithm is sufficiently robust that for general K, our analysis for K = 2 can generalize once we replace the ingredients from [XHM16] and [DTZ17] with the analogous ingredients in existing analyses for EM with K Gaussians. For the latter, it is known that if the centers of the Gaussians have separation \u2126(log min(K, d)), then EM will converge from a warm start [KC20, SN21]. By carefully tracking the error in approximating the negative gradient with the \u201cM\u201d-step in EM, we are able to show that gradient descent on the DDPM objective at small t achieves the same guarantee.", "md": "EM step.2 The projection step can be thought of as mimicking the normalization step in power iteration.\n\nIt might appear to the reader that this projected gradient-based approach is strictly superior to the two-stage algorithm described at the outset. However, in addition to obviating the need for a projection step when separation is large, our analysis for the two-stage algorithm has the advantage of giving much more favorable statistical rates. Indeed, we can show that the sample complexity of the two-stage algorithm has optimal dependence on the target error ($$ \\frac{1}{\\epsilon^2} $$), whereas we can only show a suboptimal dependence ($$ \\frac{1}{\\epsilon^8} $$) for the single-stage algorithm.\n\nExtending to general K. The connection between gradient descent on the DDPM objective at small t and the EM algorithm is sufficiently robust that for general K, our analysis for K = 2 can generalize once we replace the ingredients from [XHM16] and [DTZ17] with the analogous ingredients in existing analyses for EM with K Gaussians. For the latter, it is known that if the centers of the Gaussians have separation \u2126(log min(K, d)), then EM will converge from a warm start [KC20, SN21]. By carefully tracking the error in approximating the negative gradient with the \u201cM\u201d-step in EM, we are able to show that gradient descent on the DDPM objective at small t achieves the same guarantee."}, {"type": "heading", "lvl": 2, "value": "1.3 Preliminaries", "md": "## 1.3 Preliminaries"}, {"type": "text", "value": "Diffusion models. Throughout the paper, we use either q or q0 to denote the data distribution and X or X0 to denote the corresponding random variable on Rd. The two main components in diffusion models are the forward process and the reverse process. The forward process transforms samples from the data distribution into noise, for instance via the Ornstein-Uhlenbeck (OU) process:\n\n$$\ndX_t = -X_t dt + \\sqrt{2} dW_t \\quad \\text{with} \\quad X_0 \\sim q_0,\n$$\n\nwhere (W_t)_{t\\geq0} is a standard Brownian motion in Rd. We use q_t to denote the law of the OU process at time t. Note that for X_t \\sim q_t,\n\n$$\nX_t = \\exp(-t)X_0 + (1 - \\exp(-2t))Z_t \\quad \\text{with} \\quad X_0 \\sim q_0, Z_t \\sim N(0, Id).\n$$\n\nThe reverse process then transforms noise into samples, thus performing generative modeling. Ideally, this could be achieved by running the following stochastic differential equation for some choice of terminal time T:\n\n$$\ndX_{\\leftarrow t} = \\left\\{X_{\\leftarrow t} + 2\\nabla_x \\ln q_{T-t}(X_{\\leftarrow t})\\right\\} dt + \\sqrt{2} dW_t \\quad \\text{with} \\quad X_{\\leftarrow 0} \\sim q_T,\n$$\n\nwhere now W_t is the reversed Brownian motion. In this reverse process, the iterate X_{\\leftarrow t} is distributed according to q_{T-t} for every t \\in [0, T], so that the final iterate X_{\\leftarrow T} is distributed according to the data distribution q_0. The function \\nabla_x \\ln q_t is called the score function, and because it depends on q which is unknown, in practice one estimates it by minimizing the score matching loss\n\n$$\n\\min \\mathbb{E}_{X_t \\sim q_t}[\\|s_t(X_t) - \\nabla_x \\ln q_t(X_t)\\|^2].\n$$\n\nA standard calculation (see e.g. Appendix A of [CCL+23b]) shows that this is equivalent to minimizing the DDPM objective in which one wants to predict the noise Z_t from the noisy observation X_t, i.e.\n\n$$\n\\min L_t(s) = \\mathbb{E}_{X_0,Z_t} [s_t(X_t) + Z_t]^2 \\quad \\text{with} \\quad 1 - \\exp(-2t)\n$$\n\nNote that although \u00b5* is unknown, we can estimate its norm from samples.", "md": "Diffusion models. Throughout the paper, we use either q or q0 to denote the data distribution and X or X0 to denote the corresponding random variable on Rd. The two main components in diffusion models are the forward process and the reverse process. The forward process transforms samples from the data distribution into noise, for instance via the Ornstein-Uhlenbeck (OU) process:\n\n$$\ndX_t = -X_t dt + \\sqrt{2} dW_t \\quad \\text{with} \\quad X_0 \\sim q_0,\n$$\n\nwhere (W_t)_{t\\geq0} is a standard Brownian motion in Rd. We use q_t to denote the law of the OU process at time t. Note that for X_t \\sim q_t,\n\n$$\nX_t = \\exp(-t)X_0 + (1 - \\exp(-2t))Z_t \\quad \\text{with} \\quad X_0 \\sim q_0, Z_t \\sim N(0, Id).\n$$\n\nThe reverse process then transforms noise into samples, thus performing generative modeling. Ideally, this could be achieved by running the following stochastic differential equation for some choice of terminal time T:\n\n$$\ndX_{\\leftarrow t} = \\left\\{X_{\\leftarrow t} + 2\\nabla_x \\ln q_{T-t}(X_{\\leftarrow t})\\right\\} dt + \\sqrt{2} dW_t \\quad \\text{with} \\quad X_{\\leftarrow 0} \\sim q_T,\n$$\n\nwhere now W_t is the reversed Brownian motion. In this reverse process, the iterate X_{\\leftarrow t} is distributed according to q_{T-t} for every t \\in [0, T], so that the final iterate X_{\\leftarrow T} is distributed according to the data distribution q_0. The function \\nabla_x \\ln q_t is called the score function, and because it depends on q which is unknown, in practice one estimates it by minimizing the score matching loss\n\n$$\n\\min \\mathbb{E}_{X_t \\sim q_t}[\\|s_t(X_t) - \\nabla_x \\ln q_t(X_t)\\|^2].\n$$\n\nA standard calculation (see e.g. Appendix A of [CCL+23b]) shows that this is equivalent to minimizing the DDPM objective in which one wants to predict the noise Z_t from the noisy observation X_t, i.e.\n\n$$\n\\min L_t(s) = \\mathbb{E}_{X_0,Z_t} [s_t(X_t) + Z_t]^2 \\quad \\text{with} \\quad 1 - \\exp(-2t)\n$$\n\nNote that although \u00b5* is unknown, we can estimate its norm from samples."}]}, {"page": 8, "text": "While we have provided background on diffusion models for context, in this work we focus specifi                                                     cally\non the optimization problem (5).\nMixtures of Gaussians.                       We consider the case of learning mixtures of K equally weighted Gaus-\nsians:\n                                                                              K\n                                                          q = q0 = 1     K   i=1   N  (\u00b5\u2217 i , Id),                                                     (6)\nwhere \u00b5\u2217     i denotes the mean of the ith Gaussian component. We defi                                                         1, \u00b5\u22172 . . . , \u00b5\u2217\nthe mixtures of two Gaussians, we can simplify the data distribution as                                      ne \u03b8\u2217     = {\u00b5\u2217                   K}. For\n                                                 q = q0 = 1     2N    (\u00b5\u2217, Id) + 1    2N    (\u2212\u00b5\u2217, Id).                                                 (7)\nNote that distribution in Eq. (7) is equivalent to the distribution Eq. (6) with K = 2 because\nshifting the latter by its mean will give the former distribution, and furthermore the necessary shift\ncan be estimated from samples. The following is immediate:\nLemma 3. If q0 is a mixture of K Gaussians as in Eq. (6), then for any t > 0, qt is the mixture\nof K Gaussians given by\n                                                     K\n                                        qt = 1  K   i=1   N  (\u00b5\u2217 i,t, Id)    where \u00b5\u2217     i,t \u225c   \u00b5\u2217i exp(\u2212t) .                                        (8)\nSee Appendix A for a proof of this fact. We can see that the means of qt get rescaled according to\nthe noise level t. We also defi                ne \u03b8\u2217 t = {\u00b5\u2217    1,t, \u00b5\u22172,t, . . . , \u00b5\u2217\nLemma 4. The score function for distribution qt, for any t > 0, is given by          K,t}.\n             \u2207x ln qt(x) =          K   w\u2217 i,t(x)\u00b5\u2217  i,t \u2212  x ,       where          w\u2217 i,t(x) =      K   exp(\u2212\u2225x \u2212         \u00b5\u2217i,t\u22252/2)        .\n                                   i=1                                                                   j=1 exp(\u2212\u2225x \u2212          \u00b5\u2217 j,t\u22252/2)\nFor a mixture of two Gaussians, the score function simplifies to\n                             \u2207x log qt(x) = tanh(\u00b5\u2217\u22a4           t x)\u00b5\u2217    t \u2212   x ,      where        \u00b5\u2217 t \u225c   \u00b5\u2217  exp(\u2212t)\nSee Appendix A for the calculation.\n      Recall that \u2207x log qt(x) is the minimizer for the score-matching objective given in Eq. (4). There-\nfore, we parametrize our student network architecture similarly to the optimal score function. Our\nstudent architecture for mixtures of K Gaussians is\n                 s\u03b8t(x) =       K    wi,t(x)\u00b5i,t \u2212       x ,       where          wi,t(x) \u225c        K   exp(\u2212\u2225x \u2212         \u00b5i,t\u22252/2)                     (9)\n                               i=1                                                                    j=1 exp(\u2212\u2225x \u2212          \u00b5j,t\u22252/2)\n                                                                                       \u00b5i,t \u225c    \u00b5i exp(\u2212t).\nwhere \u03b8t = {\u00b51,t, \u00b52,t, . . . , \u00b5K,t} denotes the set of parameters at the noise scale t. For mixtures of\ntwo Gaussians, we simplify the student architecture as follows:\n                                     s\u03b8t(x) = tanh(\u00b5\u22a4         t x)\u00b5t \u2212     x ,     where       \u00b5t \u225c    \u00b5 exp(\u2212t).\nAs \u03b8t only depends on \u00b5t in the case of mixtures of two Gaussians, we simplify the notation of the\nscore function from s\u03b8             t(x) to s\u00b5     t(x) in that case. We use \u02c6              \u00b5t and \u02c6   \u00b5\u2217t to denote the unit vector along\nthe direction of \u00b5t and \u00b5\u2217               t respectively. Note that we often use \u00b5t (or \u03b8t) to denote the current\niterate of gradient descent on the DDPM objective and \u00b5\u2032                                       t to denote the iterate after taking a\ngradient descent step from \u00b5t.\n                                                                             8", "md": "While we have provided background on diffusion models for context, in this work we focus specifically on the optimization problem (5).\n\nMixtures of Gaussians. We consider the case of learning mixtures of K equally weighted Gaussians:\n\n$$\nq = q_0 = 1 - \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^*_i, I_d) \\quad (6)\n$$\n\nwhere $\\mu^*_i$ denotes the mean of the ith Gaussian component. We define the mixtures of two Gaussians as:\n\n$$\nq = q_0 = \\frac{1}{2N}(\\mu^*, I_d) + \\frac{1}{2N}(-\\mu^*, I_d) \\quad (7)\n$$\n\nNote that the distribution in Eq. (7) is equivalent to the distribution in Eq. (6) with K = 2 because shifting the latter by its mean will give the former distribution, and furthermore the necessary shift can be estimated from samples. The following is immediate:\n\nLemma 3. If $q_0$ is a mixture of K Gaussians as in Eq. (6), then for any $t > 0$, $q_t$ is the mixture of K Gaussians given by:\n\n$$\nq_t = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^*_{i,t}, I_d) \\quad \\text{where } \\mu^*_{i,t} \\triangleq \\mu^*_i \\exp(-t) \\quad (8)\n$$\n\nSee Appendix A for a proof of this fact. We can see that the means of $q_t$ get rescaled according to the noise level t. We also define $\\theta^*_t = \\{\\mu^*_{1,t}, \\mu^*_{2,t}, ..., \\mu^*_{K,t}\\}$.\n\nLemma 4. The score function for distribution $q_t$, for any $t > 0$, is given by:\n\n$$\n\\nabla_x \\ln q_t(x) = \\sum_{i=1}^{K} w^*_{i,t}(x)\\mu^*_{i,t} - x, \\quad \\text{where } w^*_{i,t}(x) = \\frac{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu^*_{j,t}\\|^2/2)}{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu^*_{j,t}\\|^2/2)}\n$$\n\nFor a mixture of two Gaussians, the score function simplifies to:\n\n$$\n\\nabla_x \\log q_t(x) = \\tanh(\\mu^{*\\top}_t x)\\mu^*_t - x, \\quad \\text{where } \\mu^*_t \\triangleq \\mu^* \\exp(-t)\n$$\n\nSee Appendix A for the calculation.\n\nRecall that $\\nabla_x \\log q_t(x)$ is the minimizer for the score-matching objective given in Eq. (4). Therefore, we parametrize our student network architecture similarly to the optimal score function. Our student architecture for mixtures of K Gaussians is:\n\n$$\ns_{\\theta_t}(x) = \\sum_{i=1}^{K} w_{i,t}(x)\\mu_{i,t} - x, \\quad \\text{where } w_{i,t}(x) \\triangleq \\frac{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu_{j,t}\\|^2/2)}{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu_{j,t}\\|^2/2)}\n$$\n\nwhere $\\theta_t = \\{\\mu_{1,t}, \\mu_{2,t}, ..., \\mu_{K,t}\\}$ denotes the set of parameters at the noise scale t. For mixtures of two Gaussians, we simplify the student architecture as follows:\n\n$$\ns_{\\theta_t}(x) = \\tanh(\\mu^{*\\top}_t x)\\mu_t - x, \\quad \\text{where } \\mu_t \\triangleq \\mu \\exp(-t)\n$$\n\nAs $\\theta_t$ only depends on $\\mu_t$ in the case of mixtures of two Gaussians, we simplify the notation of the score function from $s_{\\theta_t}(x)$ to $s_{\\mu_t}(x)$ in that case. We use $\\hat{\\mu}_t$ and $\\hat{\\mu}^*_t$ to denote the unit vector along the direction of $\\mu_t$ and $\\mu^*_t$ respectively. Note that we often use $\\mu_t$ (or $\\theta_t$) to denote the current iterate of gradient descent on the DDPM objective and $\\mu'_t$ to denote the iterate after taking a gradient descent step from $\\mu_t$.\n\n8", "images": [], "items": [{"type": "text", "value": "While we have provided background on diffusion models for context, in this work we focus specifically on the optimization problem (5).\n\nMixtures of Gaussians. We consider the case of learning mixtures of K equally weighted Gaussians:\n\n$$\nq = q_0 = 1 - \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^*_i, I_d) \\quad (6)\n$$\n\nwhere $\\mu^*_i$ denotes the mean of the ith Gaussian component. We define the mixtures of two Gaussians as:\n\n$$\nq = q_0 = \\frac{1}{2N}(\\mu^*, I_d) + \\frac{1}{2N}(-\\mu^*, I_d) \\quad (7)\n$$\n\nNote that the distribution in Eq. (7) is equivalent to the distribution in Eq. (6) with K = 2 because shifting the latter by its mean will give the former distribution, and furthermore the necessary shift can be estimated from samples. The following is immediate:\n\nLemma 3. If $q_0$ is a mixture of K Gaussians as in Eq. (6), then for any $t > 0$, $q_t$ is the mixture of K Gaussians given by:\n\n$$\nq_t = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^*_{i,t}, I_d) \\quad \\text{where } \\mu^*_{i,t} \\triangleq \\mu^*_i \\exp(-t) \\quad (8)\n$$\n\nSee Appendix A for a proof of this fact. We can see that the means of $q_t$ get rescaled according to the noise level t. We also define $\\theta^*_t = \\{\\mu^*_{1,t}, \\mu^*_{2,t}, ..., \\mu^*_{K,t}\\}$.\n\nLemma 4. The score function for distribution $q_t$, for any $t > 0$, is given by:\n\n$$\n\\nabla_x \\ln q_t(x) = \\sum_{i=1}^{K} w^*_{i,t}(x)\\mu^*_{i,t} - x, \\quad \\text{where } w^*_{i,t}(x) = \\frac{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu^*_{j,t}\\|^2/2)}{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu^*_{j,t}\\|^2/2)}\n$$\n\nFor a mixture of two Gaussians, the score function simplifies to:\n\n$$\n\\nabla_x \\log q_t(x) = \\tanh(\\mu^{*\\top}_t x)\\mu^*_t - x, \\quad \\text{where } \\mu^*_t \\triangleq \\mu^* \\exp(-t)\n$$\n\nSee Appendix A for the calculation.\n\nRecall that $\\nabla_x \\log q_t(x)$ is the minimizer for the score-matching objective given in Eq. (4). Therefore, we parametrize our student network architecture similarly to the optimal score function. Our student architecture for mixtures of K Gaussians is:\n\n$$\ns_{\\theta_t}(x) = \\sum_{i=1}^{K} w_{i,t}(x)\\mu_{i,t} - x, \\quad \\text{where } w_{i,t}(x) \\triangleq \\frac{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu_{j,t}\\|^2/2)}{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu_{j,t}\\|^2/2)}\n$$\n\nwhere $\\theta_t = \\{\\mu_{1,t}, \\mu_{2,t}, ..., \\mu_{K,t}\\}$ denotes the set of parameters at the noise scale t. For mixtures of two Gaussians, we simplify the student architecture as follows:\n\n$$\ns_{\\theta_t}(x) = \\tanh(\\mu^{*\\top}_t x)\\mu_t - x, \\quad \\text{where } \\mu_t \\triangleq \\mu \\exp(-t)\n$$\n\nAs $\\theta_t$ only depends on $\\mu_t$ in the case of mixtures of two Gaussians, we simplify the notation of the score function from $s_{\\theta_t}(x)$ to $s_{\\mu_t}(x)$ in that case. We use $\\hat{\\mu}_t$ and $\\hat{\\mu}^*_t$ to denote the unit vector along the direction of $\\mu_t$ and $\\mu^*_t$ respectively. Note that we often use $\\mu_t$ (or $\\theta_t$) to denote the current iterate of gradient descent on the DDPM objective and $\\mu'_t$ to denote the iterate after taking a gradient descent step from $\\mu_t$.\n\n8", "md": "While we have provided background on diffusion models for context, in this work we focus specifically on the optimization problem (5).\n\nMixtures of Gaussians. We consider the case of learning mixtures of K equally weighted Gaussians:\n\n$$\nq = q_0 = 1 - \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^*_i, I_d) \\quad (6)\n$$\n\nwhere $\\mu^*_i$ denotes the mean of the ith Gaussian component. We define the mixtures of two Gaussians as:\n\n$$\nq = q_0 = \\frac{1}{2N}(\\mu^*, I_d) + \\frac{1}{2N}(-\\mu^*, I_d) \\quad (7)\n$$\n\nNote that the distribution in Eq. (7) is equivalent to the distribution in Eq. (6) with K = 2 because shifting the latter by its mean will give the former distribution, and furthermore the necessary shift can be estimated from samples. The following is immediate:\n\nLemma 3. If $q_0$ is a mixture of K Gaussians as in Eq. (6), then for any $t > 0$, $q_t$ is the mixture of K Gaussians given by:\n\n$$\nq_t = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^*_{i,t}, I_d) \\quad \\text{where } \\mu^*_{i,t} \\triangleq \\mu^*_i \\exp(-t) \\quad (8)\n$$\n\nSee Appendix A for a proof of this fact. We can see that the means of $q_t$ get rescaled according to the noise level t. We also define $\\theta^*_t = \\{\\mu^*_{1,t}, \\mu^*_{2,t}, ..., \\mu^*_{K,t}\\}$.\n\nLemma 4. The score function for distribution $q_t$, for any $t > 0$, is given by:\n\n$$\n\\nabla_x \\ln q_t(x) = \\sum_{i=1}^{K} w^*_{i,t}(x)\\mu^*_{i,t} - x, \\quad \\text{where } w^*_{i,t}(x) = \\frac{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu^*_{j,t}\\|^2/2)}{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu^*_{j,t}\\|^2/2)}\n$$\n\nFor a mixture of two Gaussians, the score function simplifies to:\n\n$$\n\\nabla_x \\log q_t(x) = \\tanh(\\mu^{*\\top}_t x)\\mu^*_t - x, \\quad \\text{where } \\mu^*_t \\triangleq \\mu^* \\exp(-t)\n$$\n\nSee Appendix A for the calculation.\n\nRecall that $\\nabla_x \\log q_t(x)$ is the minimizer for the score-matching objective given in Eq. (4). Therefore, we parametrize our student network architecture similarly to the optimal score function. Our student architecture for mixtures of K Gaussians is:\n\n$$\ns_{\\theta_t}(x) = \\sum_{i=1}^{K} w_{i,t}(x)\\mu_{i,t} - x, \\quad \\text{where } w_{i,t}(x) \\triangleq \\frac{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu_{j,t}\\|^2/2)}{\\sum_{j=1}^{K} \\exp(-\\|x - \\mu_{j,t}\\|^2/2)}\n$$\n\nwhere $\\theta_t = \\{\\mu_{1,t}, \\mu_{2,t}, ..., \\mu_{K,t}\\}$ denotes the set of parameters at the noise scale t. For mixtures of two Gaussians, we simplify the student architecture as follows:\n\n$$\ns_{\\theta_t}(x) = \\tanh(\\mu^{*\\top}_t x)\\mu_t - x, \\quad \\text{where } \\mu_t \\triangleq \\mu \\exp(-t)\n$$\n\nAs $\\theta_t$ only depends on $\\mu_t$ in the case of mixtures of two Gaussians, we simplify the notation of the score function from $s_{\\theta_t}(x)$ to $s_{\\mu_t}(x)$ in that case. We use $\\hat{\\mu}_t$ and $\\hat{\\mu}^*_t$ to denote the unit vector along the direction of $\\mu_t$ and $\\mu^*_t$ respectively. Note that we often use $\\mu_t$ (or $\\theta_t$) to denote the current iterate of gradient descent on the DDPM objective and $\\mu'_t$ to denote the iterate after taking a gradient descent step from $\\mu_t$.\n\n8"}]}, {"page": 9, "text": "Expectation-Maximization (EM) algorithm.                                            The EM algorithm is composed of two steps:\nthe E-step and the M-step.                      For mixtures of Gaussians, the E-step computes the expected log-\nlikelihood based on the current mean parameters and the M-step maximizes this expectation to fi                                                     nd\na new estimate of the parameters.\nFact 5 (See e.g., [DTZ17, YYS17, KC20] for more details). When X is the mixture of K Gaussian\nand {\u00b51, \u00b52, . . . , \u00b5K} are current estimates of the means, the population EM update for all i \u2208\n{1, 2, . . . , K} is given by\n                           \u00b5\u2032i = EX[wi(X)X]                  where wi(X) =             K   exp(\u2212\u2225X \u2212           \u00b5i\u22252/2)          .\n                                     E  X[wi(X)] ,                                        j=1 exp(\u2212\u2225X \u2212            \u00b5j\u22252/2)\nThe EM update for mixtures of two Gaussians given in Eq. (7) simplifies to\n                                                    \u00b5\u2032 = EX\u223cN (\u00b5\u2217,Id)[tanh(\u00b5\u22a4X)X].\n      An analogous version of the EM algorithm, called the gradient EM algorithm, takes a gradient\nstep in the direction of the M-step instead of optimizing the objective in the M-step fully.\nFact 6 (See e.g., [YYS17, SN21] for more details). For all i \u2208                                  {1, 2, . . . , K}, the gradient EM-update\nfor mixtures of K Gaussian is given by\nwhere \u03b7 is the learning rate.                        \u00b5\u2032i = \u00b5i + \u03b7 EX[wi(X)(X \u2212                  \u00b5i)],\n2       Warmup: mixtures of two Gaussians with constant separation\nIn this section, we formally state our result for learning mixtures of two Gaussians with constant\nseparation. This case highlights the main proof techniques, namely viewing gradient descent on the\nDDPM objective as power iteration and as the EM algorithm.\n2.1       Result and algorithm\nTheorem 7. There is an absolute constant c > 0 such that the following holds. Suppose a mixture\nof two Gaussians with the mean parameter \u00b5\u2217                                satisfies \u2225\u00b5\u2217\u2225       > c. Then, for any \u03b5 > 0, there is a\nprocedure that calls Algorithm 1 at two diff                       erent noise scales t and outputs \u02dc                \u00b5 such that \u2225\u02dc      \u00b5 \u2212  \u00b5\u2217\u2225  \u2264   \u03b5\nwith high probability.               Moreover, the algorithm has time and sample complexity poly(d)/\u03b52 (see\nTheorem C.1 for more precise quantitative bounds).\nAlgorithm.              The algorithm has two stages. In the fi                          rst stage we run gradient descent on the\nDDPM objective described in Algorithm 1 from a random Gaussian initialization and noise scale t1\nfor a fi   xed number of iterations H where t1 = O(log d) (\u201chigh noise\u201d) and H = poly(d, 1/\u03b5). In the\nsecond stage, the procedure uses the output of the fi                              rst step as initialization and runs Algorithm 1\nat a \u201clow noise\u201d scale of t2 = O(1).\n2.2       Proof outline of Theorem 7\nWe provide a proof sketch of correctness of the above algorithm and summarize the main technical\nlemmas here. All proofs of the following lemmas can be found in Appendix C.\n                                                                             9", "md": "Expectation-Maximization (EM) algorithm. The EM algorithm is composed of two steps: the E-step and the M-step. For mixtures of Gaussians, the E-step computes the expected log-likelihood based on the current mean parameters and the M-step maximizes this expectation to find a new estimate of the parameters.\n\nFact 5 (See e.g., [DTZ17, YYS17, KC20] for more details). When \\(X\\) is the mixture of \\(K\\) Gaussian and \\(\\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) are current estimates of the means, the population EM update for all \\(i \\in \\{1, 2, ..., K\\}\\) is given by\n\\[\n\\mu'_i = \\mathbb{E}_{X}[w_i(X)X] \\quad \\text{where} \\quad w_i(X) = \\frac{\\exp\\left(-\\frac{\\|X - \\mu_i\\|^2}{2}\\right)}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X - \\mu_j\\|^2}{2}\\right)}\n\\]\n\nThe EM update for mixtures of two Gaussians given in Eq. (7) simplifies to\n\\[\n\\mu' = \\mathbb{E}_{X \\sim N(\\mu^*, I_d)}[\\tanh(\\mu^{\\top}X)X].\n\\]\n\nAn analogous version of the EM algorithm, called the gradient EM algorithm, takes a gradient step in the direction of the M-step instead of optimizing the objective in the M-step fully.\n\nFact 6 (See e.g., [YYS17, SN21] for more details). For all \\(i \\in \\{1, 2, ..., K\\}\\), the gradient EM-update for mixtures of \\(K\\) Gaussian is given by\n\\[\n\\mu'_i = \\mu_i + \\eta \\mathbb{E}_{X}[w_i(X)(X - \\mu_i)],\n\\]\nwhere \\(\\eta\\) is the learning rate.\n\n### Warmup: mixtures of two Gaussians with constant separation\n\nIn this section, we formally state our result for learning mixtures of two Gaussians with constant separation. This case highlights the main proof techniques, namely viewing gradient descent on the DDPM objective as power iteration and as the EM algorithm.\n\n#### Result and algorithm\n\n**Theorem 7.** There is an absolute constant \\(c > 0\\) such that the following holds. Suppose a mixture of two Gaussians with the mean parameter \\(\\mu^*\\) satisfies \\(\\|\\mu^*\\| > c\\). Then, for any \\(\\varepsilon > 0\\), there is a procedure that calls Algorithm 1 at two different noise scales \\(t\\) and outputs \\(\\tilde{\\mu}\\) such that \\(\\|\\tilde{\\mu} - \\mu^*\\| \\leq \\varepsilon\\) with high probability. Moreover, the algorithm has time and sample complexity \\(\\text{poly}(d)/\\varepsilon^2\\) (see Theorem C.1 for more precise quantitative bounds).\n\n**Algorithm.** The algorithm has two stages. In the first stage, we run gradient descent on the DDPM objective described in Algorithm 1 from a random Gaussian initialization and noise scale \\(t_1\\) for a fixed number of iterations \\(H\\) where \\(t_1 = O(\\log d)\\) (\u201chigh noise\u201d) and \\(H = \\text{poly}(d, 1/\\varepsilon\\)). In the second stage, the procedure uses the output of the first step as initialization and runs Algorithm 1 at a \u201clow noise\u201d scale of \\(t_2 = O(1)\\).\n\n#### Proof outline of Theorem 7\n\nWe provide a proof sketch of correctness of the above algorithm and summarize the main technical lemmas here. All proofs of the following lemmas can be found in Appendix C.", "images": [], "items": [{"type": "text", "value": "Expectation-Maximization (EM) algorithm. The EM algorithm is composed of two steps: the E-step and the M-step. For mixtures of Gaussians, the E-step computes the expected log-likelihood based on the current mean parameters and the M-step maximizes this expectation to find a new estimate of the parameters.\n\nFact 5 (See e.g., [DTZ17, YYS17, KC20] for more details). When \\(X\\) is the mixture of \\(K\\) Gaussian and \\(\\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) are current estimates of the means, the population EM update for all \\(i \\in \\{1, 2, ..., K\\}\\) is given by\n\\[\n\\mu'_i = \\mathbb{E}_{X}[w_i(X)X] \\quad \\text{where} \\quad w_i(X) = \\frac{\\exp\\left(-\\frac{\\|X - \\mu_i\\|^2}{2}\\right)}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X - \\mu_j\\|^2}{2}\\right)}\n\\]\n\nThe EM update for mixtures of two Gaussians given in Eq. (7) simplifies to\n\\[\n\\mu' = \\mathbb{E}_{X \\sim N(\\mu^*, I_d)}[\\tanh(\\mu^{\\top}X)X].\n\\]\n\nAn analogous version of the EM algorithm, called the gradient EM algorithm, takes a gradient step in the direction of the M-step instead of optimizing the objective in the M-step fully.\n\nFact 6 (See e.g., [YYS17, SN21] for more details). For all \\(i \\in \\{1, 2, ..., K\\}\\), the gradient EM-update for mixtures of \\(K\\) Gaussian is given by\n\\[\n\\mu'_i = \\mu_i + \\eta \\mathbb{E}_{X}[w_i(X)(X - \\mu_i)],\n\\]\nwhere \\(\\eta\\) is the learning rate.", "md": "Expectation-Maximization (EM) algorithm. The EM algorithm is composed of two steps: the E-step and the M-step. For mixtures of Gaussians, the E-step computes the expected log-likelihood based on the current mean parameters and the M-step maximizes this expectation to find a new estimate of the parameters.\n\nFact 5 (See e.g., [DTZ17, YYS17, KC20] for more details). When \\(X\\) is the mixture of \\(K\\) Gaussian and \\(\\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) are current estimates of the means, the population EM update for all \\(i \\in \\{1, 2, ..., K\\}\\) is given by\n\\[\n\\mu'_i = \\mathbb{E}_{X}[w_i(X)X] \\quad \\text{where} \\quad w_i(X) = \\frac{\\exp\\left(-\\frac{\\|X - \\mu_i\\|^2}{2}\\right)}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X - \\mu_j\\|^2}{2}\\right)}\n\\]\n\nThe EM update for mixtures of two Gaussians given in Eq. (7) simplifies to\n\\[\n\\mu' = \\mathbb{E}_{X \\sim N(\\mu^*, I_d)}[\\tanh(\\mu^{\\top}X)X].\n\\]\n\nAn analogous version of the EM algorithm, called the gradient EM algorithm, takes a gradient step in the direction of the M-step instead of optimizing the objective in the M-step fully.\n\nFact 6 (See e.g., [YYS17, SN21] for more details). For all \\(i \\in \\{1, 2, ..., K\\}\\), the gradient EM-update for mixtures of \\(K\\) Gaussian is given by\n\\[\n\\mu'_i = \\mu_i + \\eta \\mathbb{E}_{X}[w_i(X)(X - \\mu_i)],\n\\]\nwhere \\(\\eta\\) is the learning rate."}, {"type": "heading", "lvl": 3, "value": "Warmup: mixtures of two Gaussians with constant separation", "md": "### Warmup: mixtures of two Gaussians with constant separation"}, {"type": "text", "value": "In this section, we formally state our result for learning mixtures of two Gaussians with constant separation. This case highlights the main proof techniques, namely viewing gradient descent on the DDPM objective as power iteration and as the EM algorithm.", "md": "In this section, we formally state our result for learning mixtures of two Gaussians with constant separation. This case highlights the main proof techniques, namely viewing gradient descent on the DDPM objective as power iteration and as the EM algorithm."}, {"type": "heading", "lvl": 4, "value": "Result and algorithm", "md": "#### Result and algorithm"}, {"type": "text", "value": "**Theorem 7.** There is an absolute constant \\(c > 0\\) such that the following holds. Suppose a mixture of two Gaussians with the mean parameter \\(\\mu^*\\) satisfies \\(\\|\\mu^*\\| > c\\). Then, for any \\(\\varepsilon > 0\\), there is a procedure that calls Algorithm 1 at two different noise scales \\(t\\) and outputs \\(\\tilde{\\mu}\\) such that \\(\\|\\tilde{\\mu} - \\mu^*\\| \\leq \\varepsilon\\) with high probability. Moreover, the algorithm has time and sample complexity \\(\\text{poly}(d)/\\varepsilon^2\\) (see Theorem C.1 for more precise quantitative bounds).\n\n**Algorithm.** The algorithm has two stages. In the first stage, we run gradient descent on the DDPM objective described in Algorithm 1 from a random Gaussian initialization and noise scale \\(t_1\\) for a fixed number of iterations \\(H\\) where \\(t_1 = O(\\log d)\\) (\u201chigh noise\u201d) and \\(H = \\text{poly}(d, 1/\\varepsilon\\)). In the second stage, the procedure uses the output of the first step as initialization and runs Algorithm 1 at a \u201clow noise\u201d scale of \\(t_2 = O(1)\\).", "md": "**Theorem 7.** There is an absolute constant \\(c > 0\\) such that the following holds. Suppose a mixture of two Gaussians with the mean parameter \\(\\mu^*\\) satisfies \\(\\|\\mu^*\\| > c\\). Then, for any \\(\\varepsilon > 0\\), there is a procedure that calls Algorithm 1 at two different noise scales \\(t\\) and outputs \\(\\tilde{\\mu}\\) such that \\(\\|\\tilde{\\mu} - \\mu^*\\| \\leq \\varepsilon\\) with high probability. Moreover, the algorithm has time and sample complexity \\(\\text{poly}(d)/\\varepsilon^2\\) (see Theorem C.1 for more precise quantitative bounds).\n\n**Algorithm.** The algorithm has two stages. In the first stage, we run gradient descent on the DDPM objective described in Algorithm 1 from a random Gaussian initialization and noise scale \\(t_1\\) for a fixed number of iterations \\(H\\) where \\(t_1 = O(\\log d)\\) (\u201chigh noise\u201d) and \\(H = \\text{poly}(d, 1/\\varepsilon\\)). In the second stage, the procedure uses the output of the first step as initialization and runs Algorithm 1 at a \u201clow noise\u201d scale of \\(t_2 = O(1)\\)."}, {"type": "heading", "lvl": 4, "value": "Proof outline of Theorem 7", "md": "#### Proof outline of Theorem 7"}, {"type": "text", "value": "We provide a proof sketch of correctness of the above algorithm and summarize the main technical lemmas here. All proofs of the following lemmas can be found in Appendix C.", "md": "We provide a proof sketch of correctness of the above algorithm and summarize the main technical lemmas here. All proofs of the following lemmas can be found in Appendix C."}]}, {"page": 10, "text": "Part I: Analysis of high noise regime and connection to power iteration.                                                                               We show that in\nthe large noise regime, the negative gradient \u2212\u2207Lt(st) is well-approximated by 2\u00b5\u2217                                                                 t \u00b5\u2217\u22a4\nRecall that this result is the key to showing the resemblance between gradient descent and power                                                       t \u00b5t\u22123\u2225\u00b5t\u22252 \u00b5t.\niteration. Concretely, we show the following lemma:\nLemma 8 (See Lemma C.3 for more details). For t = O(log d), the gradient descent update on the\nDDPM objective Lt(st) can be approximated with 2\u00b5\u2217                                             t \u00b5\u2217\u22a4t \u00b5t \u2212       3\u2225\u00b5t\u22252 \u00b5t:\n                                             \u2212\u2207Lt(st)            \u2212      2\u00b5\u2217 t \u00b5\u2217\u22a4t \u00b5t \u2212       3\u2225\u00b5t\u22252 \u00b5t              \u2264    poly(1/d).\nFrom Lemma 8, it immediately follows that \u00b5\u2032t, the result of taking a single gradient step starting\nfrom \u00b5t, is well-approximated by the result of taking a single step of power iteration for a matrix\nwhose leading eigenvector is \u00b5\u2217                        t :\n                                         \u00b5\u2032 t = \u00b5t \u2212        \u03b7\u2207Lt(s\u00b5) \u2248             (Id(1 \u2212       3\u03b7\u2225\u00b5t\u22252) + 2\u00b5\u2217            t \u00b5\u2217\u22a4\n                                                                                                                               t )\u00b5t .\n       The second key element is to show that as a consequence of the above power iteration update,\nthe gradient descent converges in angular distance to the leading eigenvector. Concretely, we show\nthe following lemma:\nLemma 9 (Informal, see Lemma C.5 for more details). Suppose \u00b5\u2032                                                            t is the iterate after one step of\ngradient descent on the DDPM objective from \u00b5t. Denote the angle between \u00b5t and \u00b5\u2217                                                                          t to be \u03b8 and\nbetween \u00b5\u2032        t and \u00b5\u2217     t to be \u03b8\u2032. In this case, we show that\n                                                                  tan \u03b8\u2032 = max (\u03ba1 tan \u03b8, \u03ba2) ,\nwhere \u03ba1 < 1 and \u03ba2 \u2264                      1/poly(d).\nNote tan \u03b8\u2032 < tan \u03b8 implies that \u03b8\u2032 < \u03b8 or equivalently \u27e8\u02c6                                             \u00b5\u2032   \u00b5\u2217            \u00b5t, \u02c6 \u00b5\u2217\nshows that by taking a gradient step in the DDPM objective, the angle between \u00b5t and \u00b5\u2217                  t, \u02c6  t \u27e9  > \u27e8\u02c6          t \u27e9. Thus, the above lemma    t decreases.\nBy iterating this, we obtain the following lemma:\nLemma 10 (Informal, see Lemma C.6 for more details). Running gradient descent from a random\ninitialization on the DDPM objective Lt(s\u00b5) for t = O(log d) gives \u00b5t for which \u27e8\u02c6                                                                 \u00b5t, \u02c6 \u00b5\u2217t \u27e9  is \u2126(1).\n       Note that we cannot keep running gradient descent at this high noise scale and hope to achieve\n\u00b5 such that \u2225\u00b5 \u2212               \u00b5\u2217\u2225     is O(\u03b5). This is because Lemma 9 can only guarantee that the angle between\n\u00b5t and \u00b5\u2217       t is O(\u03b5), but this does not imply \u2225\u00b5 \u2212                                   \u00b5\u2217\u2225     is O(\u03b5). Instead, as described in Part II, we\nwill proceed with a smaller noise scale.\nPart II: Analysis of low noise regime and connection to EM.                                                                       In the low noise regime, we\nrun Algorithm 1 using the output from Part I as our initialization. Our analysis here shows that\nwhenever the initialization \u00b5t satisfi                           es the condition of \u27e8\u02c6              \u00b5t, \u02c6 \u00b5\u2217t \u27e9  being \u2126(1),          \u2225\u00b5t \u2212      \u00b5\u2217 t\u2225   contracts after\nevery gradient step. To start with, we show that the result of a population gradient step on the\nDDPM objective Lt(s\u00b5) results in the following:\n                                      \u00b5\u2032 t = (1 \u2212       \u03b7)\u00b5t + \u03b7 Ex\u223cN (\u00b5\u2217             t ,Id)[tanh(\u00b5\u22a4      t x)x] + \u03b7G(\u00b5t, \u00b5\u2217             t ),\nwhere \u00b5\u2032       t is the parameter after a gradient step, \u03b7 is the learning rate, and function G is given by\n                G(\u00b5, \u00b5\u2217) = Ex\u223cN (\u00b5\u2217,Id)[\u22121                    2 tanh\u2032\u2032(\u00b5\u22a4x)\u2225\u00b5\u22252 x + tanh\u2032(\u00b5\u22a4x)\u00b5\u22a4xx \u2212                                       tanh\u2032(\u00b5\u22a4x)\u00b5].\n                                                                                        10", "md": "# Analysis of high noise regime and connection to power iteration\n\n## Part I: Analysis of high noise regime and connection to power iteration\n\nWe show that in the large noise regime, the negative gradient $$- \\nabla L_t(s_t)$$ is well-approximated by $$2\\mu^*_t \\mu^{*T}$$.\n\nRecall that this result is the key to showing the resemblance between gradient descent and power iteration. Concretely, we show the following lemma:\n\nLemma 8 (See Lemma C.3 for more details). For $$t = O(\\log d)$$, the gradient descent update on the DDPM objective $$L_t(s_t)$$ can be approximated with $$2\\mu^*_t \\mu^{*T}t \\mu_t - 3\\| \\mu_t \\| ^2 \\mu_t$$:\n\n$$- \\nabla L_t(s_t) - 2\\mu^*_t \\mu^{*T}t \\mu_t - 3\\| \\mu_t \\| ^2 \\mu_t \\leq \\text{poly}(1/d)$$.\n\nFrom Lemma 8, it immediately follows that $$\\mu'_t$$, the result of taking a single gradient step starting from $$\\mu_t$$, is well-approximated by the result of taking a single step of power iteration for a matrix whose leading eigenvector is $$\\mu^*_t$$:\n\n$$\\mu'_t = \\mu_t - \\eta \\nabla L_t(s_\\mu) \\approx (Id(1 - 3\\eta \\| \\mu_t \\| ^2) + 2\\mu^*_t \\mu^{*T}t) \\mu_t$$.\n\nThe second key element is to show that as a consequence of the above power iteration update, the gradient descent converges in angular distance to the leading eigenvector. Concretely, we show the following lemma:\n\nLemma 9 (Informal, see Lemma C.5 for more details). Suppose $$\\mu'_t$$ is the iterate after one step of gradient descent on the DDPM objective from $$\\mu_t$$. Denote the angle between $$\\mu_t$$ and $$\\mu^*_t$$ to be $$\\theta$$ and between $$\\mu'_t$$ and $$\\mu^*_t$$ to be $$\\theta'$$.\n\nIn this case, we show that $$\\tan \\theta' = \\max (\\kappa_1 \\tan \\theta, \\kappa_2)$$, where $$\\kappa_1 < 1$$ and $$\\kappa_2 \\leq 1/\\text{poly}(d)$$.\n\nNote $$\\tan \\theta' < \\tan \\theta$$ implies that $$\\theta' < \\theta$$ or equivalently $$\\langle \\hat{\\mu}'_t, \\hat{\\mu}^*_t \\rangle > \\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$.\n\nThus, the above lemma shows that by taking a gradient step in the DDPM objective, the angle between $$\\mu_t$$ and $$\\mu^*_t$$ decreases.\n\nBy iterating this, we obtain the following lemma:\n\nLemma 10 (Informal, see Lemma C.6 for more details). Running gradient descent from a random initialization on the DDPM objective $$L_t(s_\\mu)$$ for $$t = O(\\log d)$$ gives $$\\mu_t$$ for which $$\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$ is $$\\Omega(1)$$.\n\nNote that we cannot keep running gradient descent at this high noise scale and hope to achieve $$\\mu$$ such that $$\\| \\mu - \\mu^* \\|$$ is $$O(\\epsilon)$$.\n\nThis is because Lemma 9 can only guarantee that the angle between $$\\mu_t$$ and $$\\mu^*_t$$ is $$O(\\epsilon)$$, but this does not imply $$\\| \\mu - \\mu^* \\|$$ is $$O(\\epsilon)$$.\n\nInstead, as described in Part II, we will proceed with a smaller noise scale.\n\n## Part II: Analysis of low noise regime and connection to EM\n\nIn the low noise regime, we run Algorithm 1 using the output from Part I as our initialization.\n\nOur analysis here shows that whenever the initialization $$\\mu_t$$ satisfies the condition of $$\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$ being $$\\Omega(1)$$, $$\\| \\mu_t - \\mu^*_t \\|$$ contracts after every gradient step.\n\nTo start with, we show that the result of a population gradient step on the DDPM objective $$L_t(s_\\mu)$$ results in the following:\n\n$$\\mu'_t = (1 - \\eta)\\mu_t + \\eta \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] + \\eta G(\\mu_t, \\mu^*_t)$$,\n\nwhere $$\\mu'_t$$ is the parameter after a gradient step, $$\\eta$$ is the learning rate, and function $$G$$ is given by\n\n$$G(\\mu, \\mu^*) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*,Id)}[-\\frac{1}{2} \\tanh''(\\mu^Tx)\\| \\mu \\| ^2 x + \\tanh'(\\mu^Tx)\\mu^Tx - \\tanh'(\\mu^Tx)\\mu]$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Analysis of high noise regime and connection to power iteration", "md": "# Analysis of high noise regime and connection to power iteration"}, {"type": "heading", "lvl": 2, "value": "Part I: Analysis of high noise regime and connection to power iteration", "md": "## Part I: Analysis of high noise regime and connection to power iteration"}, {"type": "text", "value": "We show that in the large noise regime, the negative gradient $$- \\nabla L_t(s_t)$$ is well-approximated by $$2\\mu^*_t \\mu^{*T}$$.\n\nRecall that this result is the key to showing the resemblance between gradient descent and power iteration. Concretely, we show the following lemma:\n\nLemma 8 (See Lemma C.3 for more details). For $$t = O(\\log d)$$, the gradient descent update on the DDPM objective $$L_t(s_t)$$ can be approximated with $$2\\mu^*_t \\mu^{*T}t \\mu_t - 3\\| \\mu_t \\| ^2 \\mu_t$$:\n\n$$- \\nabla L_t(s_t) - 2\\mu^*_t \\mu^{*T}t \\mu_t - 3\\| \\mu_t \\| ^2 \\mu_t \\leq \\text{poly}(1/d)$$.\n\nFrom Lemma 8, it immediately follows that $$\\mu'_t$$, the result of taking a single gradient step starting from $$\\mu_t$$, is well-approximated by the result of taking a single step of power iteration for a matrix whose leading eigenvector is $$\\mu^*_t$$:\n\n$$\\mu'_t = \\mu_t - \\eta \\nabla L_t(s_\\mu) \\approx (Id(1 - 3\\eta \\| \\mu_t \\| ^2) + 2\\mu^*_t \\mu^{*T}t) \\mu_t$$.\n\nThe second key element is to show that as a consequence of the above power iteration update, the gradient descent converges in angular distance to the leading eigenvector. Concretely, we show the following lemma:\n\nLemma 9 (Informal, see Lemma C.5 for more details). Suppose $$\\mu'_t$$ is the iterate after one step of gradient descent on the DDPM objective from $$\\mu_t$$. Denote the angle between $$\\mu_t$$ and $$\\mu^*_t$$ to be $$\\theta$$ and between $$\\mu'_t$$ and $$\\mu^*_t$$ to be $$\\theta'$$.\n\nIn this case, we show that $$\\tan \\theta' = \\max (\\kappa_1 \\tan \\theta, \\kappa_2)$$, where $$\\kappa_1 < 1$$ and $$\\kappa_2 \\leq 1/\\text{poly}(d)$$.\n\nNote $$\\tan \\theta' < \\tan \\theta$$ implies that $$\\theta' < \\theta$$ or equivalently $$\\langle \\hat{\\mu}'_t, \\hat{\\mu}^*_t \\rangle > \\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$.\n\nThus, the above lemma shows that by taking a gradient step in the DDPM objective, the angle between $$\\mu_t$$ and $$\\mu^*_t$$ decreases.\n\nBy iterating this, we obtain the following lemma:\n\nLemma 10 (Informal, see Lemma C.6 for more details). Running gradient descent from a random initialization on the DDPM objective $$L_t(s_\\mu)$$ for $$t = O(\\log d)$$ gives $$\\mu_t$$ for which $$\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$ is $$\\Omega(1)$$.\n\nNote that we cannot keep running gradient descent at this high noise scale and hope to achieve $$\\mu$$ such that $$\\| \\mu - \\mu^* \\|$$ is $$O(\\epsilon)$$.\n\nThis is because Lemma 9 can only guarantee that the angle between $$\\mu_t$$ and $$\\mu^*_t$$ is $$O(\\epsilon)$$, but this does not imply $$\\| \\mu - \\mu^* \\|$$ is $$O(\\epsilon)$$.\n\nInstead, as described in Part II, we will proceed with a smaller noise scale.", "md": "We show that in the large noise regime, the negative gradient $$- \\nabla L_t(s_t)$$ is well-approximated by $$2\\mu^*_t \\mu^{*T}$$.\n\nRecall that this result is the key to showing the resemblance between gradient descent and power iteration. Concretely, we show the following lemma:\n\nLemma 8 (See Lemma C.3 for more details). For $$t = O(\\log d)$$, the gradient descent update on the DDPM objective $$L_t(s_t)$$ can be approximated with $$2\\mu^*_t \\mu^{*T}t \\mu_t - 3\\| \\mu_t \\| ^2 \\mu_t$$:\n\n$$- \\nabla L_t(s_t) - 2\\mu^*_t \\mu^{*T}t \\mu_t - 3\\| \\mu_t \\| ^2 \\mu_t \\leq \\text{poly}(1/d)$$.\n\nFrom Lemma 8, it immediately follows that $$\\mu'_t$$, the result of taking a single gradient step starting from $$\\mu_t$$, is well-approximated by the result of taking a single step of power iteration for a matrix whose leading eigenvector is $$\\mu^*_t$$:\n\n$$\\mu'_t = \\mu_t - \\eta \\nabla L_t(s_\\mu) \\approx (Id(1 - 3\\eta \\| \\mu_t \\| ^2) + 2\\mu^*_t \\mu^{*T}t) \\mu_t$$.\n\nThe second key element is to show that as a consequence of the above power iteration update, the gradient descent converges in angular distance to the leading eigenvector. Concretely, we show the following lemma:\n\nLemma 9 (Informal, see Lemma C.5 for more details). Suppose $$\\mu'_t$$ is the iterate after one step of gradient descent on the DDPM objective from $$\\mu_t$$. Denote the angle between $$\\mu_t$$ and $$\\mu^*_t$$ to be $$\\theta$$ and between $$\\mu'_t$$ and $$\\mu^*_t$$ to be $$\\theta'$$.\n\nIn this case, we show that $$\\tan \\theta' = \\max (\\kappa_1 \\tan \\theta, \\kappa_2)$$, where $$\\kappa_1 < 1$$ and $$\\kappa_2 \\leq 1/\\text{poly}(d)$$.\n\nNote $$\\tan \\theta' < \\tan \\theta$$ implies that $$\\theta' < \\theta$$ or equivalently $$\\langle \\hat{\\mu}'_t, \\hat{\\mu}^*_t \\rangle > \\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$.\n\nThus, the above lemma shows that by taking a gradient step in the DDPM objective, the angle between $$\\mu_t$$ and $$\\mu^*_t$$ decreases.\n\nBy iterating this, we obtain the following lemma:\n\nLemma 10 (Informal, see Lemma C.6 for more details). Running gradient descent from a random initialization on the DDPM objective $$L_t(s_\\mu)$$ for $$t = O(\\log d)$$ gives $$\\mu_t$$ for which $$\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$ is $$\\Omega(1)$$.\n\nNote that we cannot keep running gradient descent at this high noise scale and hope to achieve $$\\mu$$ such that $$\\| \\mu - \\mu^* \\|$$ is $$O(\\epsilon)$$.\n\nThis is because Lemma 9 can only guarantee that the angle between $$\\mu_t$$ and $$\\mu^*_t$$ is $$O(\\epsilon)$$, but this does not imply $$\\| \\mu - \\mu^* \\|$$ is $$O(\\epsilon)$$.\n\nInstead, as described in Part II, we will proceed with a smaller noise scale."}, {"type": "heading", "lvl": 2, "value": "Part II: Analysis of low noise regime and connection to EM", "md": "## Part II: Analysis of low noise regime and connection to EM"}, {"type": "text", "value": "In the low noise regime, we run Algorithm 1 using the output from Part I as our initialization.\n\nOur analysis here shows that whenever the initialization $$\\mu_t$$ satisfies the condition of $$\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$ being $$\\Omega(1)$$, $$\\| \\mu_t - \\mu^*_t \\|$$ contracts after every gradient step.\n\nTo start with, we show that the result of a population gradient step on the DDPM objective $$L_t(s_\\mu)$$ results in the following:\n\n$$\\mu'_t = (1 - \\eta)\\mu_t + \\eta \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] + \\eta G(\\mu_t, \\mu^*_t)$$,\n\nwhere $$\\mu'_t$$ is the parameter after a gradient step, $$\\eta$$ is the learning rate, and function $$G$$ is given by\n\n$$G(\\mu, \\mu^*) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*,Id)}[-\\frac{1}{2} \\tanh''(\\mu^Tx)\\| \\mu \\| ^2 x + \\tanh'(\\mu^Tx)\\mu^Tx - \\tanh'(\\mu^Tx)\\mu]$$.", "md": "In the low noise regime, we run Algorithm 1 using the output from Part I as our initialization.\n\nOur analysis here shows that whenever the initialization $$\\mu_t$$ satisfies the condition of $$\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle$$ being $$\\Omega(1)$$, $$\\| \\mu_t - \\mu^*_t \\|$$ contracts after every gradient step.\n\nTo start with, we show that the result of a population gradient step on the DDPM objective $$L_t(s_\\mu)$$ results in the following:\n\n$$\\mu'_t = (1 - \\eta)\\mu_t + \\eta \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] + \\eta G(\\mu_t, \\mu^*_t)$$,\n\nwhere $$\\mu'_t$$ is the parameter after a gradient step, $$\\eta$$ is the learning rate, and function $$G$$ is given by\n\n$$G(\\mu, \\mu^*) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*,Id)}[-\\frac{1}{2} \\tanh''(\\mu^Tx)\\| \\mu \\| ^2 x + \\tanh'(\\mu^Tx)\\mu^Tx - \\tanh'(\\mu^Tx)\\mu]$$."}]}, {"page": 11, "text": "Note we use the population gradient here only for simplicity; in the Appendix we show that empirical\nestimates of the gradient suffi               ce. After some calculation, we can show that\n             \u00b5\u2032 t \u2212   \u00b5\u2217t   \u2264   (1 \u2212    \u03b7) \u00b5t \u2212      \u00b5\u2217t   + \u03b7\u2225Ex\u223cN (\u00b5\u2217       t ,Id)[tanh(\u00b5\u22a4    t x)x] \u2212     \u00b5\u2217t\u2225  + \u03b7   G(\u00b5t, \u00b5\u2217     t)    .      (10)\nUsing Fact 5, we know that E                 x\u223cN (\u00b5\u2217  t ,Id)[tanh(\u00b5\u22a4   t x)x] is precisely the result of one step of EM starting\nfrom \u00b5t, and it is known [DTZ17] that the EM update contracts the distance between \u00b5t and \u00b5\u2217                                                        t as\nfollows:\n                          \u2225Ex\u223cN (\u00b5\u2217    t ,Id)[tanh(\u00b5\u22a4   t x)x] \u2212      \u00b5\u2217t\u2225  \u2264   \u03bb1   \u00b5t \u2212    \u00b5\u2217t      for some \u03bb1 < 1                              (11)\nIt remains to control the second term in Eq. (10), for which we prove the following:\nLemma 11 (Informal, see Lemma C.9 for more details). When \u2225\u00b5\u2217\u2225                                                 = \u2126(1) and the noise scale\nt = O(1), then for every \u00b5 with \u27e8\u02c6                 \u00b5, \u02c6\u00b5\u2217\u27e9   being \u2126(1), the following inequality holds:\n                                        \u2225G(\u00b5t, \u00b5t\u2217)\u2225        \u2264   \u03bb2  \u00b5t \u2212     \u00b5\u2217t      for some \u03bb2 < 1 .\n      Combining Eq. (11) and Lemma 11 with Eq. (10), we have\n                                           \u00b5\u2032 t \u2212   \u00b5\u2217t    \u2264  (1 \u2212    \u03b7(1 \u2212    \u03bb1 \u2212    \u03bb2)) \u00b5t \u2212       \u00b5\u2217 t   .                                    (12)\nWe can set parameters to ensure that \u03bb1 + \u03bb2 < 1 and therefore that \u2225\u00b5t \u2212                                            \u00b5\u2217t\u2225  contracts with each\ngradient step. Applying Lemma 11 and Eq. (12), we obtain the following lemma summarizing the\nbehavior of gradient descent on the DDPM objective in the low noise regime.\nLemma 12 (Informal). For any \u03b5 > 0 and for the noise scale t = O(1), starting from an initializa-\ntion \u00b5t for which \u27e8\u02c6         \u00b5t, \u02c6\u00b5\u2217t\u27e9  = \u2126(1), running gradient descent on the DDPM objective Lt(s\u00b5) will give\nus mean parameter \u02dc            \u00b5 such that \u2225\u02dc      \u00b5 \u2212    \u00b5\u2217\u2225   \u2264   O(\u03b5).\n      Combining Lemma 10 and Lemma 12, we obtain our fi                                    rst main result, Theorem 7, for learning\nmixtures of two Gaussians with constant separation. For the full technical details, see Appendix C.\n3       Extensions: small separation and more Components\n3.1       Mixtures of two Gaussians with small separation\nIn this section, we briefl            y sketch how the ideas from Section 2 can be extended to give our second\nmain result, namely on learning mixtures of two Gaussians even with small separation. We defer\nthe full technical details to Appendix D.\nTheorem 13. Suppose a mixture of two Gaussians has mean parameter \u00b5\u2217                                                     that satisfies \u2225\u00b5\u2217\u2225           =\n        1\n\u2126(  poly(d)). Then, for any \u03b5 > 0, there exists a modification of Algorithm 1 that provides an estimate\n\u00b5 such that \u2225\u00b5 \u2212            \u00b5\u2217\u2225    \u2264   O(\u03b5) with high probability. Moreover, the algorithm has time and sample\ncomplexity poly(d)/\u03b58 (see Theorem D.1 for more precise quantitative bounds).\nAlgorithm modification.                       The algorithm that we analyze runs projected gradient descent on the\nDDPM objective but only in the high noise scale regime where t = O(log d). At each step, we\nproject the iterate \u00b5 to the ball of radius R, where R is an empirical estimate for \u2225\u00b5\u2217\u2225                              n                obtained by\ndrawing samples x1, . . . , xn from the data distribution and forming R \u225c                                        ( 1     i=1\u2225xi\u22252 \u2212        d)1/2.\n                                                                                                                   n\n                                                                           11", "md": "Note we use the population gradient here only for simplicity; in the Appendix we show that empirical\nestimates of the gradient suffice. After some calculation, we can show that\n\n$$\n\\mu'_t - \\mu^*_t \\leq (1 - \\eta) \\mu_t - \\mu^*_t + \\eta\\left\\| \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] - \\mu^*_t \\right\\| + \\eta G(\\mu_t, \\mu^*_t) . \\quad (10)\n$$\n\nUsing Fact 5, we know that $\\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x]$ is precisely the result of one step of EM starting from $\\mu_t$, and it is known [DTZ17] that the EM update contracts the distance between $\\mu_t$ and $\\mu^*_t$ as follows:\n\n$$\n\\left\\| \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] - \\mu^*_t \\right\\| \\leq \\lambda_1 \\mu_t - \\mu^*_t \\text{ for some } \\lambda_1 < 1 \\quad (11)\n$$\n\nIt remains to control the second term in Eq. (10), for which we prove the following:\n\n**Lemma 11** (Informal, see Lemma C.9 for more details). When $\\left\\| \\mu^* \\right\\| = \\Omega(1)$ and the noise scale $t = O(1)$, then for every $\\mu$ with $\\langle \\hat{\\mu}, \\hat{\\mu}^* \\rangle$ being $\\Omega(1)$, the following inequality holds:\n\n$$\n\\left\\| G(\\mu_t, \\mu_t^*) \\right\\| \\leq \\lambda_2 \\mu_t - \\mu^*_t \\text{ for some } \\lambda_2 < 1 .\n$$\n\nCombining Eq. (11) and Lemma 11 with Eq. (10), we have\n\n$$\n\\mu'_t - \\mu^*_t \\leq (1 - \\eta(1 - \\lambda_1 - \\lambda_2)) \\mu_t - \\mu^*_t . \\quad (12)\n$$\n\nWe can set parameters to ensure that $\\lambda_1 + \\lambda_2 < 1$ and therefore that $\\left\\| \\mu_t - \\mu^*_t \\right\\|$ contracts with each gradient step. Applying Lemma 11 and Eq. (12), we obtain the following lemma summarizing the behavior of gradient descent on the DDPM objective in the low noise regime.\n\n**Lemma 12** (Informal). For any $\\varepsilon > 0$ and for the noise scale $t = O(1)$, starting from an initialization $\\mu_t$ for which $\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle = \\Omega(1)$, running gradient descent on the DDPM objective $L_t(s\\mu)$ will give us mean parameter $\\tilde{\\mu}$ such that $\\left\\| \\tilde{\\mu} - \\mu^* \\right\\| \\leq O(\\varepsilon)$.\n\nCombining Lemma 10 and Lemma 12, we obtain our first main result, **Theorem 7**, for learning mixtures of two Gaussians with constant separation. For the full technical details, see Appendix C.\n\n### Extensions: small separation and more Components\n\n#### Mixtures of two Gaussians with small separation\n\nIn this section, we briefly sketch how the ideas from Section 2 can be extended to give our second main result, namely on learning mixtures of two Gaussians even with small separation. We defer the full technical details to Appendix D.\n\n**Theorem 13**. Suppose a mixture of two Gaussians has mean parameter $\\mu^*$ that satisfies $\\left\\| \\mu^* \\right\\| = \\Omega\\left(\\frac{1}{\\text{poly}(d)}\\right)$. Then, for any $\\varepsilon > 0$, there exists a modification of Algorithm 1 that provides an estimate $\\mu$ such that $\\left\\| \\mu - \\mu^* \\right\\| \\leq O(\\varepsilon)$ with high probability. Moreover, the algorithm has time and sample complexity $\\text{poly}(d)/\\varepsilon^8$ (see Theorem D.1 for more precise quantitative bounds).\n\n**Algorithm modification**. The algorithm that we analyze runs projected gradient descent on the DDPM objective but only in the high noise scale regime where $t = O(\\log d)$. At each step, we project the iterate $\\mu$ to the ball of radius $R$, where $R$ is an empirical estimate for $\\left\\| \\mu^* \\right\\|_2$ obtained by drawing samples $x_1, \\ldots, x_n$ from the data distribution and forming $R \\coloneqq \\left( \\frac{1}{n} \\sum_{i=1}^n \\|x_i\\|_2^2 - d \\right)^{1/2}$.", "images": [], "items": [{"type": "text", "value": "Note we use the population gradient here only for simplicity; in the Appendix we show that empirical\nestimates of the gradient suffice. After some calculation, we can show that\n\n$$\n\\mu'_t - \\mu^*_t \\leq (1 - \\eta) \\mu_t - \\mu^*_t + \\eta\\left\\| \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] - \\mu^*_t \\right\\| + \\eta G(\\mu_t, \\mu^*_t) . \\quad (10)\n$$\n\nUsing Fact 5, we know that $\\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x]$ is precisely the result of one step of EM starting from $\\mu_t$, and it is known [DTZ17] that the EM update contracts the distance between $\\mu_t$ and $\\mu^*_t$ as follows:\n\n$$\n\\left\\| \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] - \\mu^*_t \\right\\| \\leq \\lambda_1 \\mu_t - \\mu^*_t \\text{ for some } \\lambda_1 < 1 \\quad (11)\n$$\n\nIt remains to control the second term in Eq. (10), for which we prove the following:\n\n**Lemma 11** (Informal, see Lemma C.9 for more details). When $\\left\\| \\mu^* \\right\\| = \\Omega(1)$ and the noise scale $t = O(1)$, then for every $\\mu$ with $\\langle \\hat{\\mu}, \\hat{\\mu}^* \\rangle$ being $\\Omega(1)$, the following inequality holds:\n\n$$\n\\left\\| G(\\mu_t, \\mu_t^*) \\right\\| \\leq \\lambda_2 \\mu_t - \\mu^*_t \\text{ for some } \\lambda_2 < 1 .\n$$\n\nCombining Eq. (11) and Lemma 11 with Eq. (10), we have\n\n$$\n\\mu'_t - \\mu^*_t \\leq (1 - \\eta(1 - \\lambda_1 - \\lambda_2)) \\mu_t - \\mu^*_t . \\quad (12)\n$$\n\nWe can set parameters to ensure that $\\lambda_1 + \\lambda_2 < 1$ and therefore that $\\left\\| \\mu_t - \\mu^*_t \\right\\|$ contracts with each gradient step. Applying Lemma 11 and Eq. (12), we obtain the following lemma summarizing the behavior of gradient descent on the DDPM objective in the low noise regime.\n\n**Lemma 12** (Informal). For any $\\varepsilon > 0$ and for the noise scale $t = O(1)$, starting from an initialization $\\mu_t$ for which $\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle = \\Omega(1)$, running gradient descent on the DDPM objective $L_t(s\\mu)$ will give us mean parameter $\\tilde{\\mu}$ such that $\\left\\| \\tilde{\\mu} - \\mu^* \\right\\| \\leq O(\\varepsilon)$.\n\nCombining Lemma 10 and Lemma 12, we obtain our first main result, **Theorem 7**, for learning mixtures of two Gaussians with constant separation. For the full technical details, see Appendix C.", "md": "Note we use the population gradient here only for simplicity; in the Appendix we show that empirical\nestimates of the gradient suffice. After some calculation, we can show that\n\n$$\n\\mu'_t - \\mu^*_t \\leq (1 - \\eta) \\mu_t - \\mu^*_t + \\eta\\left\\| \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] - \\mu^*_t \\right\\| + \\eta G(\\mu_t, \\mu^*_t) . \\quad (10)\n$$\n\nUsing Fact 5, we know that $\\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x]$ is precisely the result of one step of EM starting from $\\mu_t$, and it is known [DTZ17] that the EM update contracts the distance between $\\mu_t$ and $\\mu^*_t$ as follows:\n\n$$\n\\left\\| \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu^T_t x)x] - \\mu^*_t \\right\\| \\leq \\lambda_1 \\mu_t - \\mu^*_t \\text{ for some } \\lambda_1 < 1 \\quad (11)\n$$\n\nIt remains to control the second term in Eq. (10), for which we prove the following:\n\n**Lemma 11** (Informal, see Lemma C.9 for more details). When $\\left\\| \\mu^* \\right\\| = \\Omega(1)$ and the noise scale $t = O(1)$, then for every $\\mu$ with $\\langle \\hat{\\mu}, \\hat{\\mu}^* \\rangle$ being $\\Omega(1)$, the following inequality holds:\n\n$$\n\\left\\| G(\\mu_t, \\mu_t^*) \\right\\| \\leq \\lambda_2 \\mu_t - \\mu^*_t \\text{ for some } \\lambda_2 < 1 .\n$$\n\nCombining Eq. (11) and Lemma 11 with Eq. (10), we have\n\n$$\n\\mu'_t - \\mu^*_t \\leq (1 - \\eta(1 - \\lambda_1 - \\lambda_2)) \\mu_t - \\mu^*_t . \\quad (12)\n$$\n\nWe can set parameters to ensure that $\\lambda_1 + \\lambda_2 < 1$ and therefore that $\\left\\| \\mu_t - \\mu^*_t \\right\\|$ contracts with each gradient step. Applying Lemma 11 and Eq. (12), we obtain the following lemma summarizing the behavior of gradient descent on the DDPM objective in the low noise regime.\n\n**Lemma 12** (Informal). For any $\\varepsilon > 0$ and for the noise scale $t = O(1)$, starting from an initialization $\\mu_t$ for which $\\langle \\hat{\\mu}_t, \\hat{\\mu}^*_t \\rangle = \\Omega(1)$, running gradient descent on the DDPM objective $L_t(s\\mu)$ will give us mean parameter $\\tilde{\\mu}$ such that $\\left\\| \\tilde{\\mu} - \\mu^* \\right\\| \\leq O(\\varepsilon)$.\n\nCombining Lemma 10 and Lemma 12, we obtain our first main result, **Theorem 7**, for learning mixtures of two Gaussians with constant separation. For the full technical details, see Appendix C."}, {"type": "heading", "lvl": 3, "value": "Extensions: small separation and more Components", "md": "### Extensions: small separation and more Components"}, {"type": "heading", "lvl": 4, "value": "Mixtures of two Gaussians with small separation", "md": "#### Mixtures of two Gaussians with small separation"}, {"type": "text", "value": "In this section, we briefly sketch how the ideas from Section 2 can be extended to give our second main result, namely on learning mixtures of two Gaussians even with small separation. We defer the full technical details to Appendix D.\n\n**Theorem 13**. Suppose a mixture of two Gaussians has mean parameter $\\mu^*$ that satisfies $\\left\\| \\mu^* \\right\\| = \\Omega\\left(\\frac{1}{\\text{poly}(d)}\\right)$. Then, for any $\\varepsilon > 0$, there exists a modification of Algorithm 1 that provides an estimate $\\mu$ such that $\\left\\| \\mu - \\mu^* \\right\\| \\leq O(\\varepsilon)$ with high probability. Moreover, the algorithm has time and sample complexity $\\text{poly}(d)/\\varepsilon^8$ (see Theorem D.1 for more precise quantitative bounds).\n\n**Algorithm modification**. The algorithm that we analyze runs projected gradient descent on the DDPM objective but only in the high noise scale regime where $t = O(\\log d)$. At each step, we project the iterate $\\mu$ to the ball of radius $R$, where $R$ is an empirical estimate for $\\left\\| \\mu^* \\right\\|_2$ obtained by drawing samples $x_1, \\ldots, x_n$ from the data distribution and forming $R \\coloneqq \\left( \\frac{1}{n} \\sum_{i=1}^n \\|x_i\\|_2^2 - d \\right)^{1/2}$.", "md": "In this section, we briefly sketch how the ideas from Section 2 can be extended to give our second main result, namely on learning mixtures of two Gaussians even with small separation. We defer the full technical details to Appendix D.\n\n**Theorem 13**. Suppose a mixture of two Gaussians has mean parameter $\\mu^*$ that satisfies $\\left\\| \\mu^* \\right\\| = \\Omega\\left(\\frac{1}{\\text{poly}(d)}\\right)$. Then, for any $\\varepsilon > 0$, there exists a modification of Algorithm 1 that provides an estimate $\\mu$ such that $\\left\\| \\mu - \\mu^* \\right\\| \\leq O(\\varepsilon)$ with high probability. Moreover, the algorithm has time and sample complexity $\\text{poly}(d)/\\varepsilon^8$ (see Theorem D.1 for more precise quantitative bounds).\n\n**Algorithm modification**. The algorithm that we analyze runs projected gradient descent on the DDPM objective but only in the high noise scale regime where $t = O(\\log d)$. At each step, we project the iterate $\\mu$ to the ball of radius $R$, where $R$ is an empirical estimate for $\\left\\| \\mu^* \\right\\|_2$ obtained by drawing samples $x_1, \\ldots, x_n$ from the data distribution and forming $R \\coloneqq \\left( \\frac{1}{n} \\sum_{i=1}^n \\|x_i\\|_2^2 - d \\right)^{1/2}$."}]}, {"page": 12, "text": " Proof sketch.               Lemma 9 and Lemma 10 apply even when the components of the mixture have\n small separation, and they show that running gradient descent on the DDPM objective results in\n \u00b5t and \u00b5\u2217     t being O(1) close in angular distance. Although our analysis can be extended to show\n that gradient descent can achieve O(\u03b5) angular distance, this does not guarantee that \u2225\u00b5t \u2212                                                              \u00b5\u2217t\u2225   is\n O(\u03b5). If in addition to being O(\u03b5) close in angular distance, we also have that \u2225\u00b5t\u2225                                                      \u2248\u2225\u00b5\u2217   t \u2225, then it\n is easy to see that \u2225\u00b5t \u2212               \u00b5\u2217t \u2225  is indeed O(\u03b5).\n      Observe that if R is approximately equal to \u2225\u00b5\u2217                                  t\u2225, then the projection step in our algorithm\n ensures that our fi           nal estimate \u00b5t satisfi             es this additional condition of \u2225\u00b5t\u2225                       \u2248   \u2225\u00b5\u2217 t \u2225. It is not hard\n to show that R2 is an unbiased estimate of \u2225\u00b5\u2217                                 t\u22252, so standard concentration shows that taking\n n = poly(d, 1       \u03b5) suffi   ces to ensure that R is suffi                ciently close to \u2225\u00b5\u2217          t\u2225.\n 3.2       Mixtures of K Gaussians, from a warm start\n In this section, we state our third main result, namely for learning mixtures of K Gaussians given\n by Eq. (6) from a warm start, and provide an overview of how the ideas from Section 2 can be\n extended to obtain this result.\nAssumption 14. (Separation) For a mixture of K Gaussians given by Eq. (6), for every pair of\ncomponents i, j \u2208               {1, 2, . . . , K} with i \u0338= j, we assume that the separation between their means\n \u2225\u00b5\u2217 i \u2212   \u00b5\u2217j\u2225   \u2265   C   log(min(K, d)) for sufficiently large absolute constant C > 0.\nAssumption 15. (Initialization) For each component i \u2208                                             {1, 2, . . . , K}, we have an initialization\n \u00b5(0)   with the property that \u2225\u00b5(0)                 \u2212   \u00b5\u2217i \u2225  \u2264   C\u2032     log(min(K, d)) for sufficiently small absolute constant\n   i                                             i\n C\u2032 > 0.\nTheorem 16. Suppose a mixture of K Gaussians satisfies Assumption 14. Then, for any \u03b5 =\n\u0398(1/poly(d)), running gradient descent on the DDPM objective (Algorithm 1) at low noise scale\n t = O(1) and with initialization satisfying Assumption 15 results in mean parameters {\u00b5i}K                                                           i=1 such\nthat with high probability, the mean parameters satisfy \u2225\u00b5i \u2212                                        \u00b5\u2217i \u2225  \u2264   O(\u03b5) for each i \u2208            {1, 2, . . . , K}.\nAdditionally, the runtime and sample complexity of the algorithm is poly(d, 1/\u03b5) (see Theorem E.1\n for more precise quantitative bounds).\nWe provide a brief overview of the proof here. The full proof can be found in Appendix E.\n Proof sketch.               For learning mixtures of two Gaussians, we have already established the connection\n between gradient descent on the DDPM objective and the EM algorithm.                                                              For mixtures of K\n Gaussians, however, in a local neighborhood around the ground truth parameters \u03b8\u2217, we show an\n equivalence between gradient EM (recall gradient EM performs one-step of gradient descent on\n the \u201cM\u201d step objective) and gradient descent on the DDPM objective.                                                        In particular, our main\n technical lemma (Lemma E.4) shows that for noise scale t = O(1) and for any \u00b5i that satisfi                                                                    es\n \u2225\u00b5i \u2212     \u00b5\u2217i \u2225 \u2264   O(       log(min(K, d))), we have\n                                                \u2212\u2207\u00b5i,tLt(s\u03b8t) \u2248             EXt[wi,t(Xt)(Xt \u2212             \u00b5i,t)].\nTherefore, the iterate \u00b5\u2032              i,t resulting from a single gradient step on the DDPM objective Lt(s\u03b8                                            t) with\n learning rate \u03b7 is given by\n                               \u00b5\u20321,t = \u00b51,t \u2212       \u03b7\u2207\u00b51,tLt(s\u03b8t) \u2248            \u00b51,t + \u03b7 EXt[w1,t(Xt)(Xt \u2212                  \u00b51,t)].                          (13)\n Comparing Fact 6 with Eq. (13), we see the correspondence in this regime between gradient descent\n on the DDPM objective to gradient EM. Using this connection and an existing local convergence\n guarantee from the gradient EM literature [SN21, KC20], we obtain our main theorem for mixtures\n of K Gaussians. Full details can be found in Appendix E.\n                                                                                12", "md": "# Math Equations and Text\n\n## Proof sketch.\n\nLemma 9 and Lemma 10 apply even when the components of the mixture have small separation, and they show that running gradient descent on the DDPM objective results in $$\\mu_t$$ and $$\\mu^*_t$$ being O(1) close in angular distance. Although our analysis can be extended to show that gradient descent can achieve O(\u03b5) angular distance, this does not guarantee that $$\\|\\mu_t - \\mu^*_t\\|$$ is O(\u03b5). If in addition to being O(\u03b5) close in angular distance, we also have that $$\\|\\mu_t\\| \\approx \\|\\mu^*_t\\|$$, then it is easy to see that $$\\|\\mu_t - \\mu^*_t\\|$$ is indeed O(\u03b5).\n\nObserve that if R is approximately equal to $$\\|\\mu^*_t\\|$$, then the projection step in our algorithm ensures that our final estimate $$\\mu_t$$ satisfies this additional condition of $$\\|\\mu_t\\| \\approx \\|\\mu^*_t\\|$$. It is not hard to show that $$R^2$$ is an unbiased estimate of $$\\|\\mu^*_t\\|^2$$, so standard concentration shows that taking $$n = \\text{poly}(d, \\frac{1}{\\epsilon})$$ suffices to ensure that R is sufficiently close to $$\\|\\mu^*_t\\|$$.\n\n### Mixtures of K Gaussians, from a warm start\n\nIn this section, we state our third main result, namely for learning mixtures of K Gaussians given by Eq. (6) from a warm start, and provide an overview of how the ideas from Section 2 can be extended to obtain this result.\n\nAssumption 14. (Separation) For a mixture of K Gaussians given by Eq. (6), for every pair of components i, j in {1, 2, ..., K} with i \u2260 j, we assume that the separation between their means $$\\|\\mu^*_i - \\mu^*_j\\| \\geq C \\log(\\min(K, d))$$ for sufficiently large absolute constant C > 0.\n\nAssumption 15. (Initialization) For each component i in {1, 2, ..., K}, we have an initialization $$\\mu^{(0)}$$ with the property that $$\\|\\mu^{(0)} - \\mu^*_i\\| \\leq C' \\log(\\min(K, d))$$ for sufficiently small absolute constant C' > 0.\n\nTheorem 16. Suppose a mixture of K Gaussians satisfies Assumption 14. Then, for any $$\\epsilon = \\Theta(1/\\text{poly}(d))$$, running gradient descent on the DDPM objective (Algorithm 1) at low noise scale t = O(1) and with initialization satisfying Assumption 15 results in mean parameters {$$\\mu_i$$} such that with high probability, the mean parameters satisfy $$\\|\\mu_i - \\mu^*_i\\| \\leq O(\\epsilon)$$ for each i in {1, 2, ..., K}. Additionally, the runtime and sample complexity of the algorithm is poly(d, 1/\u03b5) (see Theorem E.1 for more precise quantitative bounds).\n\nWe provide a brief overview of the proof here. The full proof can be found in Appendix E.\n\n#### Proof sketch.\n\nFor learning mixtures of two Gaussians, we have already established the connection between gradient descent on the DDPM objective and the EM algorithm. For mixtures of K Gaussians, however, in a local neighborhood around the ground truth parameters $$\\theta^*$$, we show an equivalence between gradient EM (recall gradient EM performs one-step of gradient descent on the \"M\" step objective) and gradient descent on the DDPM objective. In particular, our main technical lemma (Lemma E.4) shows that for noise scale t = O(1) and for any $$\\mu_i$$ that satisfies $$\\|\\mu_i - \\mu^*_i\\| \\leq O(\\log(\\min(K, d)))$$, we have\n\n$$-\\nabla_{\\mu_i,t}L_t(s\\theta_t) \\approx \\mathbb{E}_X[t][w_{i,t}(X_t)(X_t - \\mu_{i,t})].$$\n\nTherefore, the iterate $$\\mu'_{i,t}$$ resulting from a single gradient step on the DDPM objective $$L_t(s\\theta_t)$$ with learning rate \u03b7 is given by\n\n$$\\mu'_{1,t} = \\mu_{1,t} - \\eta\\nabla_{\\mu_{1,t}}L_t(s\\theta_t) \\approx \\mu_{1,t} + \\eta \\mathbb{E}_X[t][w_{1,t}(X_t)(X_t - \\mu_{1,t})].$$\n\nComparing Fact 6 with the above equation, we see the correspondence in this regime between gradient descent on the DDPM objective to gradient EM. Using this connection and an existing local convergence guarantee from the gradient EM literature [SN21, KC20], we obtain our main theorem for mixtures of K Gaussians. Full details can be found in Appendix E.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Proof sketch.", "md": "## Proof sketch."}, {"type": "text", "value": "Lemma 9 and Lemma 10 apply even when the components of the mixture have small separation, and they show that running gradient descent on the DDPM objective results in $$\\mu_t$$ and $$\\mu^*_t$$ being O(1) close in angular distance. Although our analysis can be extended to show that gradient descent can achieve O(\u03b5) angular distance, this does not guarantee that $$\\|\\mu_t - \\mu^*_t\\|$$ is O(\u03b5). If in addition to being O(\u03b5) close in angular distance, we also have that $$\\|\\mu_t\\| \\approx \\|\\mu^*_t\\|$$, then it is easy to see that $$\\|\\mu_t - \\mu^*_t\\|$$ is indeed O(\u03b5).\n\nObserve that if R is approximately equal to $$\\|\\mu^*_t\\|$$, then the projection step in our algorithm ensures that our final estimate $$\\mu_t$$ satisfies this additional condition of $$\\|\\mu_t\\| \\approx \\|\\mu^*_t\\|$$. It is not hard to show that $$R^2$$ is an unbiased estimate of $$\\|\\mu^*_t\\|^2$$, so standard concentration shows that taking $$n = \\text{poly}(d, \\frac{1}{\\epsilon})$$ suffices to ensure that R is sufficiently close to $$\\|\\mu^*_t\\|$$.", "md": "Lemma 9 and Lemma 10 apply even when the components of the mixture have small separation, and they show that running gradient descent on the DDPM objective results in $$\\mu_t$$ and $$\\mu^*_t$$ being O(1) close in angular distance. Although our analysis can be extended to show that gradient descent can achieve O(\u03b5) angular distance, this does not guarantee that $$\\|\\mu_t - \\mu^*_t\\|$$ is O(\u03b5). If in addition to being O(\u03b5) close in angular distance, we also have that $$\\|\\mu_t\\| \\approx \\|\\mu^*_t\\|$$, then it is easy to see that $$\\|\\mu_t - \\mu^*_t\\|$$ is indeed O(\u03b5).\n\nObserve that if R is approximately equal to $$\\|\\mu^*_t\\|$$, then the projection step in our algorithm ensures that our final estimate $$\\mu_t$$ satisfies this additional condition of $$\\|\\mu_t\\| \\approx \\|\\mu^*_t\\|$$. It is not hard to show that $$R^2$$ is an unbiased estimate of $$\\|\\mu^*_t\\|^2$$, so standard concentration shows that taking $$n = \\text{poly}(d, \\frac{1}{\\epsilon})$$ suffices to ensure that R is sufficiently close to $$\\|\\mu^*_t\\|$$."}, {"type": "heading", "lvl": 3, "value": "Mixtures of K Gaussians, from a warm start", "md": "### Mixtures of K Gaussians, from a warm start"}, {"type": "text", "value": "In this section, we state our third main result, namely for learning mixtures of K Gaussians given by Eq. (6) from a warm start, and provide an overview of how the ideas from Section 2 can be extended to obtain this result.\n\nAssumption 14. (Separation) For a mixture of K Gaussians given by Eq. (6), for every pair of components i, j in {1, 2, ..., K} with i \u2260 j, we assume that the separation between their means $$\\|\\mu^*_i - \\mu^*_j\\| \\geq C \\log(\\min(K, d))$$ for sufficiently large absolute constant C > 0.\n\nAssumption 15. (Initialization) For each component i in {1, 2, ..., K}, we have an initialization $$\\mu^{(0)}$$ with the property that $$\\|\\mu^{(0)} - \\mu^*_i\\| \\leq C' \\log(\\min(K, d))$$ for sufficiently small absolute constant C' > 0.\n\nTheorem 16. Suppose a mixture of K Gaussians satisfies Assumption 14. Then, for any $$\\epsilon = \\Theta(1/\\text{poly}(d))$$, running gradient descent on the DDPM objective (Algorithm 1) at low noise scale t = O(1) and with initialization satisfying Assumption 15 results in mean parameters {$$\\mu_i$$} such that with high probability, the mean parameters satisfy $$\\|\\mu_i - \\mu^*_i\\| \\leq O(\\epsilon)$$ for each i in {1, 2, ..., K}. Additionally, the runtime and sample complexity of the algorithm is poly(d, 1/\u03b5) (see Theorem E.1 for more precise quantitative bounds).\n\nWe provide a brief overview of the proof here. The full proof can be found in Appendix E.", "md": "In this section, we state our third main result, namely for learning mixtures of K Gaussians given by Eq. (6) from a warm start, and provide an overview of how the ideas from Section 2 can be extended to obtain this result.\n\nAssumption 14. (Separation) For a mixture of K Gaussians given by Eq. (6), for every pair of components i, j in {1, 2, ..., K} with i \u2260 j, we assume that the separation between their means $$\\|\\mu^*_i - \\mu^*_j\\| \\geq C \\log(\\min(K, d))$$ for sufficiently large absolute constant C > 0.\n\nAssumption 15. (Initialization) For each component i in {1, 2, ..., K}, we have an initialization $$\\mu^{(0)}$$ with the property that $$\\|\\mu^{(0)} - \\mu^*_i\\| \\leq C' \\log(\\min(K, d))$$ for sufficiently small absolute constant C' > 0.\n\nTheorem 16. Suppose a mixture of K Gaussians satisfies Assumption 14. Then, for any $$\\epsilon = \\Theta(1/\\text{poly}(d))$$, running gradient descent on the DDPM objective (Algorithm 1) at low noise scale t = O(1) and with initialization satisfying Assumption 15 results in mean parameters {$$\\mu_i$$} such that with high probability, the mean parameters satisfy $$\\|\\mu_i - \\mu^*_i\\| \\leq O(\\epsilon)$$ for each i in {1, 2, ..., K}. Additionally, the runtime and sample complexity of the algorithm is poly(d, 1/\u03b5) (see Theorem E.1 for more precise quantitative bounds).\n\nWe provide a brief overview of the proof here. The full proof can be found in Appendix E."}, {"type": "heading", "lvl": 4, "value": "Proof sketch.", "md": "#### Proof sketch."}, {"type": "text", "value": "For learning mixtures of two Gaussians, we have already established the connection between gradient descent on the DDPM objective and the EM algorithm. For mixtures of K Gaussians, however, in a local neighborhood around the ground truth parameters $$\\theta^*$$, we show an equivalence between gradient EM (recall gradient EM performs one-step of gradient descent on the \"M\" step objective) and gradient descent on the DDPM objective. In particular, our main technical lemma (Lemma E.4) shows that for noise scale t = O(1) and for any $$\\mu_i$$ that satisfies $$\\|\\mu_i - \\mu^*_i\\| \\leq O(\\log(\\min(K, d)))$$, we have\n\n$$-\\nabla_{\\mu_i,t}L_t(s\\theta_t) \\approx \\mathbb{E}_X[t][w_{i,t}(X_t)(X_t - \\mu_{i,t})].$$\n\nTherefore, the iterate $$\\mu'_{i,t}$$ resulting from a single gradient step on the DDPM objective $$L_t(s\\theta_t)$$ with learning rate \u03b7 is given by\n\n$$\\mu'_{1,t} = \\mu_{1,t} - \\eta\\nabla_{\\mu_{1,t}}L_t(s\\theta_t) \\approx \\mu_{1,t} + \\eta \\mathbb{E}_X[t][w_{1,t}(X_t)(X_t - \\mu_{1,t})].$$\n\nComparing Fact 6 with the above equation, we see the correspondence in this regime between gradient descent on the DDPM objective to gradient EM. Using this connection and an existing local convergence guarantee from the gradient EM literature [SN21, KC20], we obtain our main theorem for mixtures of K Gaussians. Full details can be found in Appendix E.", "md": "For learning mixtures of two Gaussians, we have already established the connection between gradient descent on the DDPM objective and the EM algorithm. For mixtures of K Gaussians, however, in a local neighborhood around the ground truth parameters $$\\theta^*$$, we show an equivalence between gradient EM (recall gradient EM performs one-step of gradient descent on the \"M\" step objective) and gradient descent on the DDPM objective. In particular, our main technical lemma (Lemma E.4) shows that for noise scale t = O(1) and for any $$\\mu_i$$ that satisfies $$\\|\\mu_i - \\mu^*_i\\| \\leq O(\\log(\\min(K, d)))$$, we have\n\n$$-\\nabla_{\\mu_i,t}L_t(s\\theta_t) \\approx \\mathbb{E}_X[t][w_{i,t}(X_t)(X_t - \\mu_{i,t})].$$\n\nTherefore, the iterate $$\\mu'_{i,t}$$ resulting from a single gradient step on the DDPM objective $$L_t(s\\theta_t)$$ with learning rate \u03b7 is given by\n\n$$\\mu'_{1,t} = \\mu_{1,t} - \\eta\\nabla_{\\mu_{1,t}}L_t(s\\theta_t) \\approx \\mu_{1,t} + \\eta \\mathbb{E}_X[t][w_{1,t}(X_t)(X_t - \\mu_{1,t})].$$\n\nComparing Fact 6 with the above equation, we see the correspondence in this regime between gradient descent on the DDPM objective to gradient EM. Using this connection and an existing local convergence guarantee from the gradient EM literature [SN21, KC20], we obtain our main theorem for mixtures of K Gaussians. Full details can be found in Appendix E."}]}, {"page": 13, "text": "Acknowledgments\nSC would like to thank Sinho Chewi, Khashayar Gatmiry, Frederic Koehler, and Holden Lee for\nenlightening discussions on sampling and score estimation.\nReferences\n[AG22]        Ahmed El Alaoui and Jason Gaitonde.        Bounds on the covariance matrix of the\n              sherrington-kirkpatrick model. arXiv preprint arXiv:2212.02445, 2022.\n[BDD23]       Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for fl   ow match-\n              ing methods. arXiv preprint arXiv:2305.16860, 2023.\n[BDJ+22]      Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and\n              Santosh S Vempala. Robustly learning mixtures of k arbitrary gaussians. In Proceed-\n              ings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages\n              1234\u20131247, 2022.\n[BKM17]       David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review\n              for statisticians. Journal of the American statistical Association, 112(518):859\u2013877,\n              2017.\n[BM11]        Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense\n              graphs, with applications to compressed sensing. IEEE Transactions on Information\n             Theory, 57(2):764\u2013785, 2011.\n[BMR22]       Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with\n              denoising auto-encoders and Langevin sampling. arXiv preprint 2002.00107, 2022.\n[BRST21]      Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous lwe. In Proceedings\n             of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 694\u2013\n              707, 2021.\n[BS15]        Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. SIAM\n              Journal on Computing, 44(4):889\u2013911, 2015.\n[BWY17]       Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees\n              for the em algorithm: From population to sample-based analysis. 2017.\n[CCL+23a]     Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The\n              probability fl\n                           ow ode is provably fast. arXiv preprint arXiv:2305.11798, 2023.\n[CCL+23b]     Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sam-\n              pling is as easy as learning the score: theory for diffusion models with minimal data\n              assumptions. In The Eleventh International Conference on Learning Representations,\n              2023.\n[CDD23]       Sitan Chen, Giannis Daras, and Alexandros G Dimakis.        Restoration-degradation\n              beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. arXiv\n              preprint arXiv:2303.03384, 2023.\n                                                13", "md": "# Acknowledgments and References\n\n## Acknowledgments\n\nSC would like to thank Sinho Chewi, Khashayar Gatmiry, Frederic Koehler, and Holden Lee for enlightening discussions on sampling and score estimation.\n\n## References\n\n|[AG22]|Ahmed El Alaoui and Jason Gaitonde. Bounds on the covariance matrix of the Sherrington-Kirkpatrick model. arXiv preprint arXiv:2212.02445, 2022.|\n|---|---|\n|[BDD23]|Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. arXiv preprint arXiv:2305.16860, 2023.|\n|[BDJ+22]|Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234\u20131247, 2022.|\n|[BKM17]|David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.|\n|[BM11]|Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011.|\n|[BMR22]|Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. arXiv preprint 2002.00107, 2022.|\n|[BRST21]|Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous LWE. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 694\u2013707, 2021.|\n|[BS15]|Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. SIAM Journal on Computing, 44(4):889\u2013911, 2015.|\n|[BWY17]|Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. 2017.|\n|[CCL+23a]|Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. arXiv preprint arXiv:2305.11798, 2023.|\n|[CCL+23b]|Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.|\n|[CDD23]|Sitan Chen, Giannis Daras, and Alexandros G Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. arXiv preprint arXiv:2303.03384, 2023.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgments and References", "md": "# Acknowledgments and References"}, {"type": "heading", "lvl": 2, "value": "Acknowledgments", "md": "## Acknowledgments"}, {"type": "text", "value": "SC would like to thank Sinho Chewi, Khashayar Gatmiry, Frederic Koehler, and Holden Lee for enlightening discussions on sampling and score estimation.", "md": "SC would like to thank Sinho Chewi, Khashayar Gatmiry, Frederic Koehler, and Holden Lee for enlightening discussions on sampling and score estimation."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "table", "rows": [["[AG22]", "Ahmed El Alaoui and Jason Gaitonde. Bounds on the covariance matrix of the Sherrington-Kirkpatrick model. arXiv preprint arXiv:2212.02445, 2022."], ["[BDD23]", "Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. arXiv preprint arXiv:2305.16860, 2023."], ["[BDJ+22]", "Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234\u20131247, 2022."], ["[BKM17]", "David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017."], ["[BM11]", "Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011."], ["[BMR22]", "Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. arXiv preprint 2002.00107, 2022."], ["[BRST21]", "Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous LWE. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 694\u2013707, 2021."], ["[BS15]", "Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. SIAM Journal on Computing, 44(4):889\u2013911, 2015."], ["[BWY17]", "Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. 2017."], ["[CCL+23a]", "Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. arXiv preprint arXiv:2305.11798, 2023."], ["[CCL+23b]", "Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023."], ["[CDD23]", "Sitan Chen, Giannis Daras, and Alexandros G Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. arXiv preprint arXiv:2303.03384, 2023."]], "md": "|[AG22]|Ahmed El Alaoui and Jason Gaitonde. Bounds on the covariance matrix of the Sherrington-Kirkpatrick model. arXiv preprint arXiv:2212.02445, 2022.|\n|---|---|\n|[BDD23]|Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. arXiv preprint arXiv:2305.16860, 2023.|\n|[BDJ+22]|Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234\u20131247, 2022.|\n|[BKM17]|David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.|\n|[BM11]|Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011.|\n|[BMR22]|Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. arXiv preprint 2002.00107, 2022.|\n|[BRST21]|Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous LWE. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 694\u2013707, 2021.|\n|[BS15]|Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. SIAM Journal on Computing, 44(4):889\u2013911, 2015.|\n|[BWY17]|Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. 2017.|\n|[CCL+23a]|Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. arXiv preprint arXiv:2305.11798, 2023.|\n|[CCL+23b]|Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.|\n|[CDD23]|Sitan Chen, Giannis Daras, and Alexandros G Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. arXiv preprint arXiv:2303.03384, 2023.|", "isPerfectTable": true, "csv": "\"[AG22]\",\"Ahmed El Alaoui and Jason Gaitonde. Bounds on the covariance matrix of the Sherrington-Kirkpatrick model. arXiv preprint arXiv:2212.02445, 2022.\"\n\"[BDD23]\",\"Joe Benton, George Deligiannidis, and Arnaud Doucet. Error bounds for flow matching methods. arXiv preprint arXiv:2305.16860, 2023.\"\n\"[BDJ+22]\",\"Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1234\u20131247, 2022.\"\n\"[BKM17]\",\"David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians. Journal of the American Statistical Association, 112(518):859\u2013877, 2017.\"\n\"[BM11]\",\"Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. IEEE Transactions on Information Theory, 57(2):764\u2013785, 2011.\"\n\"[BMR22]\",\"Adam Block, Youssef Mroueh, and Alexander Rakhlin. Generative modeling with denoising auto-encoders and Langevin sampling. arXiv preprint 2002.00107, 2022.\"\n\"[BRST21]\",\"Joan Bruna, Oded Regev, Min Jae Song, and Yi Tang. Continuous LWE. In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 694\u2013707, 2021.\"\n\"[BS15]\",\"Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. SIAM Journal on Computing, 44(4):889\u2013911, 2015.\"\n\"[BWY17]\",\"Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm: From population to sample-based analysis. 2017.\"\n\"[CCL+23a]\",\"Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ODE is provably fast. arXiv preprint arXiv:2305.11798, 2023.\"\n\"[CCL+23b]\",\"Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference on Learning Representations, 2023.\"\n\"[CDD23]\",\"Sitan Chen, Giannis Daras, and Alexandros G Dimakis. Restoration-degradation beyond linear diffusions: A non-asymptotic analysis for ddim-type samplers. arXiv preprint arXiv:2303.03384, 2023.\""}]}, {"page": 14, "text": "[Cel22]    Michael Celentano. Sudakov-fernique post-amp, and a new proof of the local convexity\n           of the tap free energy. arXiv preprint arXiv:2208.09550, 2022.\n[CFM21]    Michael Celentano, Zhou Fan, and Song Mei. Local convexity of the tap free energy\n           and amp convergence for z2-synchronization. arXiv preprint arXiv:2106.11428, 2021.\n[CLL22]    Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based gener-\n           ative modeling: user-friendly bounds under minimal smoothness assumptions. arXiv\n           preprint arXiv:2211.01916, 2022.\n[DB22]     Valentin De Bortoli. Convergence of denoising diffusion models under the manifold\n           hypothesis. Transactions on Machine Learning Research, 2022.\n[DBTHD21]  Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion\n           Schr\u00f6dinger bridge with applications to score-based generative modeling. In M. Ran-\n           zato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\n           Advances in Neural Information Processing Systems, volume 34, pages 17695\u201317709.\n           Curran Associates, Inc., 2021.\n[DHKK20]   Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly\n           learning any clusterable mixture of gaussians. arXiv preprint arXiv:2005.06417, 2020.\n[DK20]     Ilias Diakonikolas and Daniel M Kane. Small covers for near-zero sets of polynomi-\n           als and learning latent variable models. In 2020 IEEE 61st Annual Symposium on\n           Foundations of Computer Science (FOCS), pages 184\u2013195. IEEE, 2020.\n[DKS17]    Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart.       Statistical query lower\n           bounds for robust estimation of high-dimensional gaussians and gaussian mixtures.\n           In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),\n           pages 73\u201384. IEEE, 2017.\n[DKS18]    Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean\n           estimation and learning mixtures of spherical gaussians. In Proceedings of the 50th\n           Annual ACM SIGACT Symposium on Theory of Computing, pages 1047\u20131060, 2018.\n[DMM09]    David L Donoho, Arian Maleki, and Andrea Montanari.              Message-passing algo-\n           rithms for compressed sensing.     Proceedings of the National Academy of Sciences,\n           106(45):18914\u201318919, 2009.\n[DMM10]    David L Donoho, Arian Maleki, and Andrea Montanari. Message passing algorithms\n           for compressed sensing: I. motivation and construction. In 2010 IEEE information\n           theory workshop on information theory (ITW 2010, Cairo), pages 1\u20135. IEEE, 2010.\n[DS07]     Sanjoy Dasgupta and Leonard J Schulman. A probabilistic analysis of em for mixtures\n           of separated, spherical gaussians. Journal of Machine Learning Research, 8:203\u2013226,\n           2007.\n[DTZ17]    Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of\n           em suffice for mixtures of two gaussians. In Conference on Learning Theory, pages\n           704\u2013710. PMLR, 2017.\n                                              14", "md": "- [Cel22] Michael Celentano. Sudakov-fernique post-amp, and a new proof of the local convexity\nof the tap free energy. arXiv preprint arXiv:2208.09550, 2022.\n- [CFM21] Michael Celentano, Zhou Fan, and Song Mei. Local convexity of the tap free energy\nand amp convergence for z2-synchronization. arXiv preprint arXiv:2106.11428, 2021.\n- [CLL22] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based gener-\native modeling: user-friendly bounds under minimal smoothness assumptions. arXiv\npreprint arXiv:2211.01916, 2022.\n- [DB22] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold\nhypothesis. Transactions on Machine Learning Research, 2022.\n- [DBTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion\nSchr\u00f6dinger bridge with applications to score-based generative modeling. In M. Ran-\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, volume 34, pages 17695\u201317709.\nCurran Associates, Inc., 2021.\n- [DHKK20] Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly\nlearning any clusterable mixture of gaussians. arXiv preprint arXiv:2005.06417, 2020.\n- [DK20] Ilias Diakonikolas and Daniel M Kane. Small covers for near-zero sets of polynomi-\nals and learning latent variable models. In 2020 IEEE 61st Annual Symposium on\nFoundations of Computer Science (FOCS), pages 184\u2013195. IEEE, 2020.\n- [DKS17] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower\nbounds for robust estimation of high-dimensional gaussians and gaussian mixtures.\nIn 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),\npages 73\u201384. IEEE, 2017.\n- [DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean\nestimation and learning mixtures of spherical gaussians. In Proceedings of the 50th\nAnnual ACM SIGACT Symposium on Theory of Computing, pages 1047\u20131060, 2018.\n- [DMM09] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algo-\nrithms for compressed sensing. Proceedings of the National Academy of Sciences,\n106(45):18914\u201318919, 2009.\n- [DMM10] David L Donoho, Arian Maleki, and Andrea Montanari. Message passing algorithms\nfor compressed sensing: I. motivation and construction. In 2010 IEEE information\ntheory workshop on information theory (ITW 2010, Cairo), pages 1\u20135. IEEE, 2010.\n- [DS07] Sanjoy Dasgupta and Leonard J Schulman. A probabilistic analysis of em for mixtures\nof separated, spherical gaussians. Journal of Machine Learning Research, 8:203\u2013226,\n2007.\n- [DTZ17] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of\nem suffice for mixtures of two gaussians. In Conference on Learning Theory, pages\n704\u2013710. PMLR, 2017.", "images": [], "items": [{"type": "text", "value": "- [Cel22] Michael Celentano. Sudakov-fernique post-amp, and a new proof of the local convexity\nof the tap free energy. arXiv preprint arXiv:2208.09550, 2022.\n- [CFM21] Michael Celentano, Zhou Fan, and Song Mei. Local convexity of the tap free energy\nand amp convergence for z2-synchronization. arXiv preprint arXiv:2106.11428, 2021.\n- [CLL22] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based gener-\native modeling: user-friendly bounds under minimal smoothness assumptions. arXiv\npreprint arXiv:2211.01916, 2022.\n- [DB22] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold\nhypothesis. Transactions on Machine Learning Research, 2022.\n- [DBTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion\nSchr\u00f6dinger bridge with applications to score-based generative modeling. In M. Ran-\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, volume 34, pages 17695\u201317709.\nCurran Associates, Inc., 2021.\n- [DHKK20] Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly\nlearning any clusterable mixture of gaussians. arXiv preprint arXiv:2005.06417, 2020.\n- [DK20] Ilias Diakonikolas and Daniel M Kane. Small covers for near-zero sets of polynomi-\nals and learning latent variable models. In 2020 IEEE 61st Annual Symposium on\nFoundations of Computer Science (FOCS), pages 184\u2013195. IEEE, 2020.\n- [DKS17] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower\nbounds for robust estimation of high-dimensional gaussians and gaussian mixtures.\nIn 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),\npages 73\u201384. IEEE, 2017.\n- [DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean\nestimation and learning mixtures of spherical gaussians. In Proceedings of the 50th\nAnnual ACM SIGACT Symposium on Theory of Computing, pages 1047\u20131060, 2018.\n- [DMM09] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algo-\nrithms for compressed sensing. Proceedings of the National Academy of Sciences,\n106(45):18914\u201318919, 2009.\n- [DMM10] David L Donoho, Arian Maleki, and Andrea Montanari. Message passing algorithms\nfor compressed sensing: I. motivation and construction. In 2010 IEEE information\ntheory workshop on information theory (ITW 2010, Cairo), pages 1\u20135. IEEE, 2010.\n- [DS07] Sanjoy Dasgupta and Leonard J Schulman. A probabilistic analysis of em for mixtures\nof separated, spherical gaussians. Journal of Machine Learning Research, 8:203\u2013226,\n2007.\n- [DTZ17] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of\nem suffice for mixtures of two gaussians. In Conference on Learning Theory, pages\n704\u2013710. PMLR, 2017.", "md": "- [Cel22] Michael Celentano. Sudakov-fernique post-amp, and a new proof of the local convexity\nof the tap free energy. arXiv preprint arXiv:2208.09550, 2022.\n- [CFM21] Michael Celentano, Zhou Fan, and Song Mei. Local convexity of the tap free energy\nand amp convergence for z2-synchronization. arXiv preprint arXiv:2106.11428, 2021.\n- [CLL22] Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based gener-\native modeling: user-friendly bounds under minimal smoothness assumptions. arXiv\npreprint arXiv:2211.01916, 2022.\n- [DB22] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold\nhypothesis. Transactions on Machine Learning Research, 2022.\n- [DBTHD21] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion\nSchr\u00f6dinger bridge with applications to score-based generative modeling. In M. Ran-\nzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors,\nAdvances in Neural Information Processing Systems, volume 34, pages 17695\u201317709.\nCurran Associates, Inc., 2021.\n- [DHKK20] Ilias Diakonikolas, Samuel B Hopkins, Daniel Kane, and Sushrut Karmalkar. Robustly\nlearning any clusterable mixture of gaussians. arXiv preprint arXiv:2005.06417, 2020.\n- [DK20] Ilias Diakonikolas and Daniel M Kane. Small covers for near-zero sets of polynomi-\nals and learning latent variable models. In 2020 IEEE 61st Annual Symposium on\nFoundations of Computer Science (FOCS), pages 184\u2013195. IEEE, 2020.\n- [DKS17] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower\nbounds for robust estimation of high-dimensional gaussians and gaussian mixtures.\nIn 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),\npages 73\u201384. IEEE, 2017.\n- [DKS18] Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. List-decodable robust mean\nestimation and learning mixtures of spherical gaussians. In Proceedings of the 50th\nAnnual ACM SIGACT Symposium on Theory of Computing, pages 1047\u20131060, 2018.\n- [DMM09] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algo-\nrithms for compressed sensing. Proceedings of the National Academy of Sciences,\n106(45):18914\u201318919, 2009.\n- [DMM10] David L Donoho, Arian Maleki, and Andrea Montanari. Message passing algorithms\nfor compressed sensing: I. motivation and construction. In 2010 IEEE information\ntheory workshop on information theory (ITW 2010, Cairo), pages 1\u20135. IEEE, 2010.\n- [DS07] Sanjoy Dasgupta and Leonard J Schulman. A probabilistic analysis of em for mixtures\nof separated, spherical gaussians. Journal of Machine Learning Research, 8:203\u2013226,\n2007.\n- [DTZ17] Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis. Ten steps of\nem suffice for mixtures of two gaussians. In Conference on Learning Theory, pages\n704\u2013710. PMLR, 2017."}]}, {"page": 15, "text": "[EAMS22]   Ahmed El Alaoui, Andrea Montanari, and Mark Sellke.                Sampling from the\n           sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In 2022\n           IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages\n           323\u2013334. IEEE, 2022.\n[Eld13]    Ronen Eldan. Thin shell implies spectral gap up to polylog via a stochastic localization\n           scheme. Geometric and Functional Analysis, 23(2):532\u2013569, 2013.\n[Eld20]    Ronen Eldan. Taming correlations through entropy-effi    cient measure decompositions\n           with applications to mean-field approximation. Probability Theory and Related Fields,\n           176(3-4):737\u2013755, 2020.\n[HJA20]    Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.\n           Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.\n[HL18]     Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares\n           proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of\n          Computing, pages 1021\u20131034, 2018.\n[HP15]     Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians.\n           In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,\n           pages 753\u2013760, 2015.\n[Kab03]    Yoshiyuki Kabashima. A cdma multiuser detection algorithm on the basis of belief\n           propagation. Journal of Physics A: Mathematical and General, 36(43):11111, 2003.\n[Kan21]    Daniel M Kane. Robust learning of mixtures of gaussians. In Proceedings of the 2021\n           ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1246\u20131258. SIAM,\n           2021.\n[KC20]     Jeongyeol Kwon and Constantine Caramanis.           The em algorithm gives sample-\n           optimality for learning mixtures of well-separated gaussians. In Conference on Learn-\n           ing Theory, pages 2425\u20132487. PMLR, 2020.\n[KMV10]    Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Effi        ciently learning mix-\n           tures of two gaussians. In Proceedings of the forty-second ACM symposium on Theory\n           of computing, pages 553\u2013562, 2010.\n[KSS18]    Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation\n           and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM\n           SIGACT Symposium on Theory of Computing, pages 1035\u20131046, 2018.\n[LL22]     Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in poly-\n           nomial time. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory\n           of Computing, pages 1248\u20131261, 2022.\n[LLT22]    Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative mod-\n           eling with polynomial complexity. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,\n           and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems,\n           2022.\n                                              15", "md": "# References\n\n# References\n\n|[EAMS22]|Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 323\u2013334. IEEE, 2022.|\n|---|---|\n|[Eld13]|Ronen Eldan. Thin shell implies spectral gap up to polylog via a stochastic localization scheme. Geometric and Functional Analysis, 23(2):532\u2013569, 2013.|\n|[Eld20]|Ronen Eldan. Taming correlations through entropy-efficient measure decompositions with applications to mean-field approximation. Probability Theory and Related Fields, 176(3-4):737\u2013755, 2020.|\n|[HJA20]|Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.|\n|[HL18]|Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1021\u20131034, 2018.|\n|[HP15]|Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 753\u2013760, 2015.|\n|[Kab03]|Yoshiyuki Kabashima. A cdma multiuser detection algorithm on the basis of belief propagation. Journal of Physics A: Mathematical and General, 36(43):11111, 2003.|\n|[Kan21]|Daniel M Kane. Robust learning of mixtures of gaussians. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1246\u20131258. SIAM, 2021.|\n|[KC20]|Jeongyeol Kwon and Constantine Caramanis. The em algorithm gives sample-optimality for learning mixtures of well-separated gaussians. In Conference on Learning Theory, pages 2425\u20132487. PMLR, 2020.|\n|[KMV10]|Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two gaussians. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 553\u2013562, 2010.|\n|[KSS18]|Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1035\u20131046, 2018.|\n|[LL22]|Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in polynomial time. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1248\u20131261, 2022.|\n|[LLT22]|Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "table", "rows": [["[EAMS22]", "Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 323\u2013334. IEEE, 2022."], ["[Eld13]", "Ronen Eldan. Thin shell implies spectral gap up to polylog via a stochastic localization scheme. Geometric and Functional Analysis, 23(2):532\u2013569, 2013."], ["[Eld20]", "Ronen Eldan. Taming correlations through entropy-efficient measure decompositions with applications to mean-field approximation. Probability Theory and Related Fields, 176(3-4):737\u2013755, 2020."], ["[HJA20]", "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020."], ["[HL18]", "Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1021\u20131034, 2018."], ["[HP15]", "Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 753\u2013760, 2015."], ["[Kab03]", "Yoshiyuki Kabashima. A cdma multiuser detection algorithm on the basis of belief propagation. Journal of Physics A: Mathematical and General, 36(43):11111, 2003."], ["[Kan21]", "Daniel M Kane. Robust learning of mixtures of gaussians. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1246\u20131258. SIAM, 2021."], ["[KC20]", "Jeongyeol Kwon and Constantine Caramanis. The em algorithm gives sample-optimality for learning mixtures of well-separated gaussians. In Conference on Learning Theory, pages 2425\u20132487. PMLR, 2020."], ["[KMV10]", "Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two gaussians. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 553\u2013562, 2010."], ["[KSS18]", "Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1035\u20131046, 2018."], ["[LL22]", "Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in polynomial time. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1248\u20131261, 2022."], ["[LLT22]", "Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022."]], "md": "|[EAMS22]|Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 323\u2013334. IEEE, 2022.|\n|---|---|\n|[Eld13]|Ronen Eldan. Thin shell implies spectral gap up to polylog via a stochastic localization scheme. Geometric and Functional Analysis, 23(2):532\u2013569, 2013.|\n|[Eld20]|Ronen Eldan. Taming correlations through entropy-efficient measure decompositions with applications to mean-field approximation. Probability Theory and Related Fields, 176(3-4):737\u2013755, 2020.|\n|[HJA20]|Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.|\n|[HL18]|Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1021\u20131034, 2018.|\n|[HP15]|Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 753\u2013760, 2015.|\n|[Kab03]|Yoshiyuki Kabashima. A cdma multiuser detection algorithm on the basis of belief propagation. Journal of Physics A: Mathematical and General, 36(43):11111, 2003.|\n|[Kan21]|Daniel M Kane. Robust learning of mixtures of gaussians. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1246\u20131258. SIAM, 2021.|\n|[KC20]|Jeongyeol Kwon and Constantine Caramanis. The em algorithm gives sample-optimality for learning mixtures of well-separated gaussians. In Conference on Learning Theory, pages 2425\u20132487. PMLR, 2020.|\n|[KMV10]|Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two gaussians. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 553\u2013562, 2010.|\n|[KSS18]|Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1035\u20131046, 2018.|\n|[LL22]|Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in polynomial time. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1248\u20131261, 2022.|\n|[LLT22]|Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.|", "isPerfectTable": true, "csv": "\"[EAMS22]\",\"Ahmed El Alaoui, Andrea Montanari, and Mark Sellke. Sampling from the sherrington-kirkpatrick gibbs measure via algorithmic stochastic localization. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pages 323\u2013334. IEEE, 2022.\"\n\"[Eld13]\",\"Ronen Eldan. Thin shell implies spectral gap up to polylog via a stochastic localization scheme. Geometric and Functional Analysis, 23(2):532\u2013569, 2013.\"\n\"[Eld20]\",\"Ronen Eldan. Taming correlations through entropy-efficient measure decompositions with applications to mean-field approximation. Probability Theory and Related Fields, 176(3-4):737\u2013755, 2020.\"\n\"[HJA20]\",\"Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.\"\n\"[HL18]\",\"Samuel B Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1021\u20131034, 2018.\"\n\"[HP15]\",\"Moritz Hardt and Eric Price. Tight bounds for learning a mixture of two gaussians. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pages 753\u2013760, 2015.\"\n\"[Kab03]\",\"Yoshiyuki Kabashima. A cdma multiuser detection algorithm on the basis of belief propagation. Journal of Physics A: Mathematical and General, 36(43):11111, 2003.\"\n\"[Kan21]\",\"Daniel M Kane. Robust learning of mixtures of gaussians. In Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1246\u20131258. SIAM, 2021.\"\n\"[KC20]\",\"Jeongyeol Kwon and Constantine Caramanis. The em algorithm gives sample-optimality for learning mixtures of well-separated gaussians. In Conference on Learning Theory, pages 2425\u20132487. PMLR, 2020.\"\n\"[KMV10]\",\"Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Efficiently learning mixtures of two gaussians. In Proceedings of the forty-second ACM symposium on Theory of computing, pages 553\u2013562, 2010.\"\n\"[KSS18]\",\"Pravesh K Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 1035\u20131046, 2018.\"\n\"[LL22]\",\"Allen Liu and Jerry Li. Clustering mixtures with almost optimal separation in polynomial time. In Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing, pages 1248\u20131261, 2022.\"\n\"[LLT22]\",\"Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\""}]}, {"page": 16, "text": "[LLT23]   Holden Lee, Jianfeng Lu, and Yixin Tan.          Convergence of score-based generative\n          modeling for general data distributions. In International Conference on Algorithmic\n          Learning Theory, pages 946\u2013985. PMLR, 2023.\n[LM23]    Allen Liu and Ankur Moitra. Robustly learning general mixtures of gaussians. Journal\n          of the ACM, 2023.\n[LWCC23]  Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic\n          convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251,\n          2023.\n[LWYL22]  Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: understand-\n          ing and extending diffusion generative models. arXiv preprint arXiv:2208.14699, 2022.\n[MM09]    Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford\n          University Press, 2009.\n[MV10]    Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures\n          of gaussians. In 2010 IEEE 51st Annual Symposium on Foundations of Computer\n          Science, pages 93\u2013102. IEEE, 2010.\n[MV21]    Andrea Montanari and Ramji Venkataramanan. Estimation of low-rank matrices via\n          approximate message passing. The Annals of Statistics, 49(1), 2021.\n[MW23]    Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via\n          diffusion processes. arXiv preprint arXiv:2304.11449, 2023.\n[Pea94]   Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical\n          Transactions of the Royal Society of London. A, 185:71\u2013110, 1894.\n[Pid22]   Jakiw Pidstrigach.    Score-based generative models detect manifolds. In S. Koyejo,\n          S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural\n          Information Processing Systems, volume 35, pages 35852\u201335865. Curran Associates,\n          Inc., 2022.\n[RBL+22]  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Om-\n          mer. High-resolution image synthesis with latent diffusion models. In Proceedings\n          of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n          10684\u201310695, 2022.\n[RDN+22]  Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n          Hierarchical text-conditional image generation with clip latents.          arXiv preprint\n          arXiv:2204.06125, 2022.\n[RV17]    Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated\n          gaussians. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Sci-\n          ence (FOCS), pages 85\u201396. IEEE, 2017.\n[SCS+22]  Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Den-\n          ton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\n          Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep\n          language understanding. arXiv preprint arXiv:2205.11487, 2022.\n                                               16", "md": "# References\n\n# List of References\n\n|[LLT23]|Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023.|\n|---|---|\n|[LM23]|Allen Liu and Ankur Moitra. Robustly learning general mixtures of Gaussians. Journal of the ACM, 2023.|\n|[LWCC23]|Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023.|\n|[LWYL22]|Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: understanding and extending diffusion generative models. arXiv preprint arXiv:2208.14699, 2022.|\n|[MM09]|Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009.|\n|[MV10]|Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 93\u2013102. IEEE, 2010.|\n|[MV21]|Andrea Montanari and Ramji Venkataramanan. Estimation of low-rank matrices via approximate message passing. The Annals of Statistics, 49(1), 2021.|\n|[MW23]|Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. arXiv preprint arXiv:2304.11449, 2023.|\n|[Pea94]|Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71\u2013110, 1894.|\n|[Pid22]|Jakiw Pidstrigach. Score-based generative models detect manifolds. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 35852\u201335865. Curran Associates, Inc., 2022.|\n|[RBL+22]|Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.|\n|[RDN+22]|Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.|\n|[RV17]|Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated Gaussians. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 85\u201396. IEEE, 2017.|\n|[SCS+22]|Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["[LLT23]", "Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023."], ["[LM23]", "Allen Liu and Ankur Moitra. Robustly learning general mixtures of Gaussians. Journal of the ACM, 2023."], ["[LWCC23]", "Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023."], ["[LWYL22]", "Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: understanding and extending diffusion generative models. arXiv preprint arXiv:2208.14699, 2022."], ["[MM09]", "Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009."], ["[MV10]", "Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 93\u2013102. IEEE, 2010."], ["[MV21]", "Andrea Montanari and Ramji Venkataramanan. Estimation of low-rank matrices via approximate message passing. The Annals of Statistics, 49(1), 2021."], ["[MW23]", "Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. arXiv preprint arXiv:2304.11449, 2023."], ["[Pea94]", "Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71\u2013110, 1894."], ["[Pid22]", "Jakiw Pidstrigach. Score-based generative models detect manifolds. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 35852\u201335865. Curran Associates, Inc., 2022."], ["[RBL+22]", "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022."], ["[RDN+22]", "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022."], ["[RV17]", "Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated Gaussians. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 85\u201396. IEEE, 2017."], ["[SCS+22]", "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022."]], "md": "|[LLT23]|Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023.|\n|---|---|\n|[LM23]|Allen Liu and Ankur Moitra. Robustly learning general mixtures of Gaussians. Journal of the ACM, 2023.|\n|[LWCC23]|Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023.|\n|[LWYL22]|Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: understanding and extending diffusion generative models. arXiv preprint arXiv:2208.14699, 2022.|\n|[MM09]|Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009.|\n|[MV10]|Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 93\u2013102. IEEE, 2010.|\n|[MV21]|Andrea Montanari and Ramji Venkataramanan. Estimation of low-rank matrices via approximate message passing. The Annals of Statistics, 49(1), 2021.|\n|[MW23]|Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. arXiv preprint arXiv:2304.11449, 2023.|\n|[Pea94]|Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71\u2013110, 1894.|\n|[Pid22]|Jakiw Pidstrigach. Score-based generative models detect manifolds. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 35852\u201335865. Curran Associates, Inc., 2022.|\n|[RBL+22]|Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.|\n|[RDN+22]|Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.|\n|[RV17]|Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated Gaussians. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 85\u201396. IEEE, 2017.|\n|[SCS+22]|Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.|", "isPerfectTable": true, "csv": "\"[LLT23]\",\"Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory, pages 946\u2013985. PMLR, 2023.\"\n\"[LM23]\",\"Allen Liu and Ankur Moitra. Robustly learning general mixtures of Gaussians. Journal of the ACM, 2023.\"\n\"[LWCC23]\",\"Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based generative models. arXiv preprint arXiv:2306.09251, 2023.\"\n\"[LWYL22]\",\"Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us build bridges: understanding and extending diffusion generative models. arXiv preprint arXiv:2208.14699, 2022.\"\n\"[MM09]\",\"Marc Mezard and Andrea Montanari. Information, physics, and computation. Oxford University Press, 2009.\"\n\"[MV10]\",\"Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of Gaussians. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pages 93\u2013102. IEEE, 2010.\"\n\"[MV21]\",\"Andrea Montanari and Ramji Venkataramanan. Estimation of low-rank matrices via approximate message passing. The Annals of Statistics, 49(1), 2021.\"\n\"[MW23]\",\"Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. arXiv preprint arXiv:2304.11449, 2023.\"\n\"[Pea94]\",\"Karl Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of the Royal Society of London. A, 185:71\u2013110, 1894.\"\n\"[Pid22]\",\"Jakiw Pidstrigach. Score-based generative models detect manifolds. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 35852\u201335865. Curran Associates, Inc., 2022.\"\n\"[RBL+22]\",\"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\"\n\"[RDN+22]\",\"Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\"\n\"[RV17]\",\"Oded Regev and Aravindan Vijayaraghavan. On learning mixtures of well-separated Gaussians. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 85\u201396. IEEE, 2017.\"\n\"[SCS+22]\",\"Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\""}]}, {"page": 17, "text": "[SDWMG15] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep\n               unsupervised learning using nonequilibrium thermodynamics. In International Con-\n               ference on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n[SE19]         Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the\n               data distribution. Advances in Neural Information Processing Systems, 32, 2019.\n[SN21]         Nimrod Segol and Boaz Nadler.          Improved convergence guarantees for learning\n               gaussian mixture models by em and gradient em.           Electronic journal of statistics,\n              15(2):4510\u20134544, 2021.\n[SOAJ14]       Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour.\n               Near-optimal-sample estimators for spherical gaussian mixtures. Advances in Neural\n              Information Processing Systems, 27, 2014.\n[SSDK+20]      Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Er-\n               mon, and Ben Poole. Score-based generative modeling through stochastic differential\n               equations. arXiv preprint arXiv:2011.13456, 2020.\n[Ver]          Roman Vershynin. High-Dimensional Probability: An Introduction with Applications\n              in Data Science.     Number 47 in Cambridge Series in Statistical and Probabilistic\n               Mathematics. Cambridge University Press.\n[VW04]         Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models.\n              Journal of Computer and System Sciences, 68(4):841\u2013860, 2004.            Special Issue on\n               FOCS 2002.\n[WJ+08]        Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families,\n               and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013\n              305, 2008.\n[WY22]         Andre Wibisono and Kaylee Y. Yang. Convergence in KL divergence of the inexact\n               Langevin algorithm with application to score-based generative models. arXiv preprint\n              2211.01512, 2022.\n[XHM16]        Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization\n               for mixtures of two gaussians. Advances in Neural Information Processing Systems,\n              29, 2016.\n[YYS17]        Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence analysis of gradient\n               em for multi-component gaussian mixture. arXiv preprint arXiv:1705.08530, 2017.\n[ZLS20]        Ruofei Zhao, Yuanzhi Li, and Yuekai Sun. Statistical convergence of the em algorithm\n               on gaussian mixture models. 2020.\n                                                   17", "md": "# References\n\n## List of References\n\n|[SDWMG15]|Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.|\n|---|---|\n|[SE19]|Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.|\n|[SN21]|Nimrod Segol and Boaz Nadler. Improved convergence guarantees for learning gaussian mixture models by em and gradient em. Electronic journal of statistics, 15(2):4510\u20134544, 2021.|\n|[SOAJ14]|Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical gaussian mixtures. Advances in Neural Information Processing Systems, 27, 2014.|\n|[SSDK+20]|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.|\n|[Ver]|Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.|\n|[VW04]|Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841\u2013860, 2004. Special Issue on FOCS 2002.|\n|[WJ+08]|Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013305, 2008.|\n|[WY22]|Andre Wibisono and Kaylee Y. Yang. Convergence in KL divergence of the inexact Langevin algorithm with application to score-based generative models. arXiv preprint 2211.01512, 2022.|\n|[XHM16]|Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. Advances in Neural Information Processing Systems, 29, 2016.|\n|[YYS17]|Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence analysis of gradient em for multi-component gaussian mixture. arXiv preprint arXiv:1705.08530, 2017.|\n|[ZLS20]|Ruofei Zhao, Yuanzhi Li, and Yuekai Sun. Statistical convergence of the em algorithm on gaussian mixture models. 2020.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "List of References", "md": "## List of References"}, {"type": "table", "rows": [["[SDWMG15]", "Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015."], ["[SE19]", "Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019."], ["[SN21]", "Nimrod Segol and Boaz Nadler. Improved convergence guarantees for learning gaussian mixture models by em and gradient em. Electronic journal of statistics, 15(2):4510\u20134544, 2021."], ["[SOAJ14]", "Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical gaussian mixtures. Advances in Neural Information Processing Systems, 27, 2014."], ["[SSDK+20]", "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020."], ["[Ver]", "Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press."], ["[VW04]", "Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841\u2013860, 2004. Special Issue on FOCS 2002."], ["[WJ+08]", "Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013305, 2008."], ["[WY22]", "Andre Wibisono and Kaylee Y. Yang. Convergence in KL divergence of the inexact Langevin algorithm with application to score-based generative models. arXiv preprint 2211.01512, 2022."], ["[XHM16]", "Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. Advances in Neural Information Processing Systems, 29, 2016."], ["[YYS17]", "Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence analysis of gradient em for multi-component gaussian mixture. arXiv preprint arXiv:1705.08530, 2017."], ["[ZLS20]", "Ruofei Zhao, Yuanzhi Li, and Yuekai Sun. Statistical convergence of the em algorithm on gaussian mixture models. 2020."]], "md": "|[SDWMG15]|Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.|\n|---|---|\n|[SE19]|Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.|\n|[SN21]|Nimrod Segol and Boaz Nadler. Improved convergence guarantees for learning gaussian mixture models by em and gradient em. Electronic journal of statistics, 15(2):4510\u20134544, 2021.|\n|[SOAJ14]|Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical gaussian mixtures. Advances in Neural Information Processing Systems, 27, 2014.|\n|[SSDK+20]|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.|\n|[Ver]|Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.|\n|[VW04]|Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841\u2013860, 2004. Special Issue on FOCS 2002.|\n|[WJ+08]|Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013305, 2008.|\n|[WY22]|Andre Wibisono and Kaylee Y. Yang. Convergence in KL divergence of the inexact Langevin algorithm with application to score-based generative models. arXiv preprint 2211.01512, 2022.|\n|[XHM16]|Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. Advances in Neural Information Processing Systems, 29, 2016.|\n|[YYS17]|Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence analysis of gradient em for multi-component gaussian mixture. arXiv preprint arXiv:1705.08530, 2017.|\n|[ZLS20]|Ruofei Zhao, Yuanzhi Li, and Yuekai Sun. Statistical convergence of the em algorithm on gaussian mixture models. 2020.|", "isPerfectTable": true, "csv": "\"[SDWMG15]\",\"Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.\"\n\"[SE19]\",\"Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.\"\n\"[SN21]\",\"Nimrod Segol and Boaz Nadler. Improved convergence guarantees for learning gaussian mixture models by em and gradient em. Electronic journal of statistics, 15(2):4510\u20134544, 2021.\"\n\"[SOAJ14]\",\"Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical gaussian mixtures. Advances in Neural Information Processing Systems, 27, 2014.\"\n\"[SSDK+20]\",\"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\"\n\"[Ver]\",\"Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press.\"\n\"[VW04]\",\"Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841\u2013860, 2004. Special Issue on FOCS 2002.\"\n\"[WJ+08]\",\"Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. Foundations and Trends\u00ae in Machine Learning, 1(1\u20132):1\u2013305, 2008.\"\n\"[WY22]\",\"Andre Wibisono and Kaylee Y. Yang. Convergence in KL divergence of the inexact Langevin algorithm with application to score-based generative models. arXiv preprint 2211.01512, 2022.\"\n\"[XHM16]\",\"Ji Xu, Daniel J Hsu, and Arian Maleki. Global analysis of expectation maximization for mixtures of two gaussians. Advances in Neural Information Processing Systems, 29, 2016.\"\n\"[YYS17]\",\"Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar. Convergence analysis of gradient em for multi-component gaussian mixture. arXiv preprint arXiv:1705.08530, 2017.\"\n\"[ZLS20]\",\"Ruofei Zhao, Yuanzhi Li, and Yuekai Sun. Statistical convergence of the em algorithm on gaussian mixture models. 2020.\""}]}, {"page": 18, "text": "Roadmap.              In Appendix A, we provide proofs of some simple lemmas from Section 1.3 and some\nbasic inequalities. In Appendix B we give additional notation and preliminaries. In Appendix C,\nwe provide the proof details for Theorem 7, our result on learning mixtures of two Gaussians with\nconstant separation. In Appendix D, we extend this analysis to give a proof of Theorem 13, our\nresult on learning mixtures of two Gaussians with small separation. In Appendix E, we provide the\nproof details for Theorem 16, our result on learning mixtures of K Gaussians. Finally, in Appendix F\nwe give further deferred proofs.\nA        Proofs from Section 1.3\nA.1         X  t is a mixture of Gaussians\nProof of Lemma 3. Suppose X0 is mixture of K Gaussians with density function given by\n                                                                          K\n                                                            q0 = 1   K   i=1   N  (\u00b5\u2217 i,0, Id)\nWe know that Xt = exp(\u2212t)X0 +                                1 \u2212   exp(\u22122t)Zt where Zt \u223c                    N  (0, Id). Then, by change of\nvariable of probability density, we have\n                                                                             K\n                                      pdf of exp(\u2212t)X0 = 1             K    i=1  N   (\u00b5\u2217i,0 exp(\u2212t), exp(\u22122t) \u00b7 Id)\nCombining these, we have    pdf of        1 \u2212   exp(\u22122t)Zt = N            (0, (1 \u2212     exp(\u22122t)) \u00b7 Id) .\n                                                 K\nas claimed.                  qt(Xt) = 1     K   i=1   N  (\u00b5\u2217 i,t, I)          where              \u00b5\u2217i,t = \u00b5\u2217  i,0 exp(\u2212t) ,\nA.2         Derivation of score function\nProof of Lemma 4. For mixtures of K Gaussians in the form of Eq. (6), the score function at time\nt is given by\n                                                  K i=1 e\u2212    \u2225x\u2212\u00b5\u22172i,t\u22252 (x \u2212    \u00b5\u2217i,t)\n                        \u2207   log qt(x) = \u2212      K         K j=1 e\u2212    \u2225x\u2212\u00b5\u22172j,t\u22252                                 \u2225x\u2212\u00b5\u2217 i,t\u22252\n                                          =        w\u2217 i,t(x)\u00b5\u2217              where        w\u2217                  e\u2212       2           .\n                                              i=1               i,t \u2212  x                   i,t(x) =      K  j=1 e\u2212    \u2225x\u2212\u00b5\u22172j,t\u22252\nFor mixtures of two Gaussians in the form of Eq. (7), the score function is given by\n                                           \u2207  log qt(x) = w\u2217       1,t(x)\u00b5\u2217  1,t + w\u2217  2,t(x)\u00b5\u2217   2,t \u2212  x\n                                                            = w\u2217   1,t(x)\u00b5\u2217    \u2212   (1 \u2212    w\u22171,t(x))\u00b5\u2217     \u2212   x\n                                                            = (2w\u2217    1,t(x) \u2212    1)\u00b5\u2217    \u2212   x                                             (A.1)\n                                                                            18", "md": "Roadmap. In Appendix A, we provide proofs of some simple lemmas from Section 1.3 and some basic inequalities. In Appendix B we give additional notation and preliminaries. In Appendix C, we provide the proof details for Theorem 7, our result on learning mixtures of two Gaussians with constant separation. In Appendix D, we extend this analysis to give a proof of Theorem 13, our result on learning mixtures of two Gaussians with small separation. In Appendix E, we provide the proof details for Theorem 16, our result on learning mixtures of K Gaussians. Finally, in Appendix F we give further deferred proofs.\n\nA Proofs from Section 1.3\n\nA.1 X t is a mixture of Gaussians\n\nProof of Lemma 3. Suppose X\u2080 is mixture of K Gaussians with density function given by\n\n$$\nq\u2080 = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,0}, I_d)\n$$\nWe know that X\u209c = exp(\u2212t)X\u2080 + (1 \u2212 exp(\u22122t))Z\u209c where Z\u209c \u223c $\\mathcal{N}(0, I_d)$. Then, by change of variable of probability density, we have\n\n$$\n\\text{pdf of exp(\u2212t)X\u2080} = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,0} \\exp(-t), \\exp(-2t) \\cdot I_d)\n$$\nCombining these, we have\n\n$$\n\\text{pdf of } 1 - \\exp(-2t)Z\u209c = \\mathcal{N}(0, (1 - \\exp(-2t)) \\cdot I_d)\n$$\nas claimed. $q_t(X_t) = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,t}, I)$ where $\\mu^{*}_{i,t} = \\mu^{*}_{i,0} \\exp(-t)$.\n\nA.2 Derivation of score function\n\nProof of Lemma 4. For mixtures of K Gaussians in the form of Eq. (6), the score function at time t is given by\n\n$$\n\\nabla \\log q_t(x) = - \\sum_{i=1}^{K} e^{-\\|x-\\mu^{*2}_{i,t}\\|^2} (x - \\mu^{*}_{i,t}) \\bigg/ \\sum_{j=1}^{K} e^{-\\|x-\\mu^{*2}_{j,t}\\|^2} \\|x-\\mu^{*}_{i,t}\\|^2 = \\sum_{i=1}^{K} w^{*}_{i,t}(x)\\mu^{*}_{i,t} \\text{ where } w^{*}_{i,t} = \\frac{e^{-\\|x-\\mu^{*2}_{i,t}\\|^2}}{\\sum_{j=1}^{K} e^{-\\|x-\\mu^{*2}_{j,t}\\|^2}}\n$$\nFor mixtures of two Gaussians in the form of Eq. (7), the score function is given by\n\n$$\n\\nabla \\log q_t(x) = w^{*}_{1,t}(x)\\mu^{*}_{1,t} + w^{*}_{2,t}(x)\\mu^{*}_{2,t} - x = w^{*}_{1,t}(x)\\mu^{*}_{1} - (1 - w^{*}_{1,t}(x))\\mu^{*}_{2} - x = (2w^{*}_{1,t}(x) - 1)\\mu^{*}_{1} - x \\quad \\text{(A.1)}\n$$\n18", "images": [], "items": [{"type": "text", "value": "Roadmap. In Appendix A, we provide proofs of some simple lemmas from Section 1.3 and some basic inequalities. In Appendix B we give additional notation and preliminaries. In Appendix C, we provide the proof details for Theorem 7, our result on learning mixtures of two Gaussians with constant separation. In Appendix D, we extend this analysis to give a proof of Theorem 13, our result on learning mixtures of two Gaussians with small separation. In Appendix E, we provide the proof details for Theorem 16, our result on learning mixtures of K Gaussians. Finally, in Appendix F we give further deferred proofs.\n\nA Proofs from Section 1.3\n\nA.1 X t is a mixture of Gaussians\n\nProof of Lemma 3. Suppose X\u2080 is mixture of K Gaussians with density function given by\n\n$$\nq\u2080 = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,0}, I_d)\n$$\nWe know that X\u209c = exp(\u2212t)X\u2080 + (1 \u2212 exp(\u22122t))Z\u209c where Z\u209c \u223c $\\mathcal{N}(0, I_d)$. Then, by change of variable of probability density, we have\n\n$$\n\\text{pdf of exp(\u2212t)X\u2080} = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,0} \\exp(-t), \\exp(-2t) \\cdot I_d)\n$$\nCombining these, we have\n\n$$\n\\text{pdf of } 1 - \\exp(-2t)Z\u209c = \\mathcal{N}(0, (1 - \\exp(-2t)) \\cdot I_d)\n$$\nas claimed. $q_t(X_t) = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,t}, I)$ where $\\mu^{*}_{i,t} = \\mu^{*}_{i,0} \\exp(-t)$.\n\nA.2 Derivation of score function\n\nProof of Lemma 4. For mixtures of K Gaussians in the form of Eq. (6), the score function at time t is given by\n\n$$\n\\nabla \\log q_t(x) = - \\sum_{i=1}^{K} e^{-\\|x-\\mu^{*2}_{i,t}\\|^2} (x - \\mu^{*}_{i,t}) \\bigg/ \\sum_{j=1}^{K} e^{-\\|x-\\mu^{*2}_{j,t}\\|^2} \\|x-\\mu^{*}_{i,t}\\|^2 = \\sum_{i=1}^{K} w^{*}_{i,t}(x)\\mu^{*}_{i,t} \\text{ where } w^{*}_{i,t} = \\frac{e^{-\\|x-\\mu^{*2}_{i,t}\\|^2}}{\\sum_{j=1}^{K} e^{-\\|x-\\mu^{*2}_{j,t}\\|^2}}\n$$\nFor mixtures of two Gaussians in the form of Eq. (7), the score function is given by\n\n$$\n\\nabla \\log q_t(x) = w^{*}_{1,t}(x)\\mu^{*}_{1,t} + w^{*}_{2,t}(x)\\mu^{*}_{2,t} - x = w^{*}_{1,t}(x)\\mu^{*}_{1} - (1 - w^{*}_{1,t}(x))\\mu^{*}_{2} - x = (2w^{*}_{1,t}(x) - 1)\\mu^{*}_{1} - x \\quad \\text{(A.1)}\n$$\n18", "md": "Roadmap. In Appendix A, we provide proofs of some simple lemmas from Section 1.3 and some basic inequalities. In Appendix B we give additional notation and preliminaries. In Appendix C, we provide the proof details for Theorem 7, our result on learning mixtures of two Gaussians with constant separation. In Appendix D, we extend this analysis to give a proof of Theorem 13, our result on learning mixtures of two Gaussians with small separation. In Appendix E, we provide the proof details for Theorem 16, our result on learning mixtures of K Gaussians. Finally, in Appendix F we give further deferred proofs.\n\nA Proofs from Section 1.3\n\nA.1 X t is a mixture of Gaussians\n\nProof of Lemma 3. Suppose X\u2080 is mixture of K Gaussians with density function given by\n\n$$\nq\u2080 = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,0}, I_d)\n$$\nWe know that X\u209c = exp(\u2212t)X\u2080 + (1 \u2212 exp(\u22122t))Z\u209c where Z\u209c \u223c $\\mathcal{N}(0, I_d)$. Then, by change of variable of probability density, we have\n\n$$\n\\text{pdf of exp(\u2212t)X\u2080} = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,0} \\exp(-t), \\exp(-2t) \\cdot I_d)\n$$\nCombining these, we have\n\n$$\n\\text{pdf of } 1 - \\exp(-2t)Z\u209c = \\mathcal{N}(0, (1 - \\exp(-2t)) \\cdot I_d)\n$$\nas claimed. $q_t(X_t) = \\frac{1}{K} \\sum_{i=1}^{K} \\mathcal{N}(\\mu^{*}_{i,t}, I)$ where $\\mu^{*}_{i,t} = \\mu^{*}_{i,0} \\exp(-t)$.\n\nA.2 Derivation of score function\n\nProof of Lemma 4. For mixtures of K Gaussians in the form of Eq. (6), the score function at time t is given by\n\n$$\n\\nabla \\log q_t(x) = - \\sum_{i=1}^{K} e^{-\\|x-\\mu^{*2}_{i,t}\\|^2} (x - \\mu^{*}_{i,t}) \\bigg/ \\sum_{j=1}^{K} e^{-\\|x-\\mu^{*2}_{j,t}\\|^2} \\|x-\\mu^{*}_{i,t}\\|^2 = \\sum_{i=1}^{K} w^{*}_{i,t}(x)\\mu^{*}_{i,t} \\text{ where } w^{*}_{i,t} = \\frac{e^{-\\|x-\\mu^{*2}_{i,t}\\|^2}}{\\sum_{j=1}^{K} e^{-\\|x-\\mu^{*2}_{j,t}\\|^2}}\n$$\nFor mixtures of two Gaussians in the form of Eq. (7), the score function is given by\n\n$$\n\\nabla \\log q_t(x) = w^{*}_{1,t}(x)\\mu^{*}_{1,t} + w^{*}_{2,t}(x)\\mu^{*}_{2,t} - x = w^{*}_{1,t}(x)\\mu^{*}_{1} - (1 - w^{*}_{1,t}(x))\\mu^{*}_{2} - x = (2w^{*}_{1,t}(x) - 1)\\mu^{*}_{1} - x \\quad \\text{(A.1)}\n$$\n18"}]}, {"page": 19, "text": "By simplifying w\u2217          1,t(x), we obtain\n                                                 w\u2217 1,t(x) =    1 + exp(\u2225x\u2212\u00b5\u2217\u22252     1     \u2212  \u2225x+\u00b5\u2217\u22252     )\n                                                                                  2               2\n                                                             =                1\n                                                                 1 + exp(\u22122\u00b5\u2217\u22a4x)\n                                                             = \u03c3(2\u00b5\u2217\u22a4x)                                                                         (A.2)\nwhere \u03c3(\u00b7) denotes the sigmoid function. Using Eq. (A.2) in Eq. (A.1), we obtain\n                                                    \u2207   log qt(x) = tanh(\u00b5\u2217\u22a4x)\u00b5\u2217                \u2212   x.\nB        Additional notations and preliminaries\nIn this section, we provide additional notations and preliminaries for the proofs to follow. Recall\nthat we use Lt(s\u03b8           t) to denote the population denoising loss at noise scale t.\n                                            Lt(s\u03b8   t) = E      s\u03b8t(Xt) +                  Zt               2  .\n                                                                                     1 \u2212   exp(\u22122t)\nWe use Lt(s\u03b8         t(x0, zt)) to denote the denoising loss at noise scale t on a sample x0 from the data\ndistribution and zt from the standard Gaussian distribution:\n                                         Lt(s\u03b8   t(x0, zt)) =                                   zt             2 ,\n                                                                     s\u03b8t(xt) +           1 \u2212   exp(\u22122t)\nwhere xt = exp(\u2212t)x0 +                  1 \u2212      exp(\u22122t)zt. We use \u03b1t as shorthand notation for exp(\u2212t) and \u03b2t as\nshorthand notation for                   1 \u2212   exp(\u22122t).\n      For mixtures of two Gaussians, we use B to denote the upper bound on \u2225\u00b5\u2217\u22252, that is,\nThroughout, we assume that B = poly(d).                              \u2225\u00b5\u2217\u22252 \u2264      B .\n      For any vector v, we use \u02c6               v to denote the unit vector along the direction of v. For a vector v,\nwe use [v]i to denote the ith coordinate of v. Similarly, for a matrix X, we use [X]i to denote the\nith row of the matrix. For any positive integer n, we use [n] to denote the set {1, 2, . . . , n}. We\nuse N     (\u00b5, \u03c32 \u00b7 Id) to denote the standard Gaussian with mean \u00b5 and covariance \u03c32 \u00b7 Id. Sometimes,\nwe use a shorter notation N\u00b5 to denote N                            (\u00b5, Id). For any two quantities X and Y that are both\nimplicitly functions of some parameter a over R\u22650, we use the shorthand X \u2272                                                 Y and X = O(Y )\ninterchangeably to denote that there exists absolute constant C > 0 such that for all a suffi                                                 ciently\nlarge, X(a) \u2264           CY (a). We also use the shorthand X \u2273                            Y and X = \u2126(Y ), defi             ned in the obvious\nway.\n      Finally, we will use the following standard bounds.\nLemma B.1 (Sub-Gaussian norm, see e.g. [Ver]). The sub-Gaussian norm of a random variable\nX \u2208     R, denoted by \u2225X\u2225\u03c82 is defined as    \u2225X\u2225\u03c82 = inf{t > 0 : E[exp(X2/t2)] \u2264                          2}.\nThe sub-Gaussian norm has the following properties:\n                                                                            19", "md": "# Additional Notations and Preliminaries\n\n## By simplifying \\(w^*_{1,t}(x)\\), we obtain\n\n$$\nw^*_{1,t}(x) = \\frac{1 + \\exp(\\|x-\\mu^*\\|^2_1 - \\|x+\\mu^*\\|^2_2)}{2} = \\frac{1}{1 + \\exp(-2\\mu^{\\top}x)} = \\sigma(2\\mu^{\\top}x) \\quad \\text{(A.2)}\n$$\n\nwhere \\(\\sigma(\\cdot)\\) denotes the sigmoid function. Using Eq. (A.2) in Eq. (A.1), we obtain\n\n$$\n\\nabla \\log q_t(x) = \\tanh(\\mu^{\\top}x)\\mu^* - x.\n$$\n\n## Additional Notations and Preliminaries\n\nIn this section, we provide additional notations and preliminaries for the proofs to follow. Recall that we use \\(L_t(s\\theta_t)\\) to denote the population denoising loss at noise scale \\(t\\).\n\n$$\nL_t(s\\theta_t) = E[s\\theta_t(X_t) + Z_t]^2 \\cdot \\frac{1}{1 - \\exp(-2t)}\n$$\n\nWe use \\(L_t(s\\theta_t(x_0, z_t))\\) to denote the denoising loss at noise scale \\(t\\) on a sample \\(x_0\\) from the data distribution and \\(z_t\\) from the standard Gaussian distribution:\n\n$$\nL_t(s\\theta_t(x_0, z_t)) = s\\theta_t(x_t) + \\frac{z_t^2}{1 - \\exp(-2t)}\n$$\n\nwhere \\(x_t = \\exp(-t)x_0 + \\frac{1 - \\exp(-2t)}{z_t}\\). We use \\(\\alpha_t\\) as shorthand notation for \\(\\exp(-t)\\) and \\(\\beta_t\\) as shorthand notation for \\(\\frac{1 - \\exp(-2t)}{t}\\).\n\nFor mixtures of two Gaussians, we use \\(B\\) to denote the upper bound on \\(\\|\\mu^*\\|^2\\), that is, throughout, we assume that \\(B = \\text{poly}(d)\\), \\(\\|\\mu^*\\|^2 \\leq B\\).\n\nFor any vector \\(v\\), we use \\(\\hat{v}\\) to denote the unit vector along the direction of \\(v\\). For a vector \\(v\\), we use \\([v]_i\\) to denote the \\(i\\)th coordinate of \\(v\\). Similarly, for a matrix \\(X\\), we use \\([X]_i\\) to denote the \\(i\\)th row of the matrix. For any positive integer \\(n\\), we use \\([n]\\) to denote the set \\(\\{1, 2, ..., n\\}\\). We use \\(N(\\mu, \\sigma^2 \\cdot \\text{Id})\\) to denote the standard Gaussian with mean \\(\\mu\\) and covariance \\(\\sigma^2 \\cdot \\text{Id}\\). Sometimes, we use a shorter notation \\(N_{\\mu}\\) to denote \\(N(\\mu, \\text{Id}\\). For any two quantities \\(X\\) and \\(Y\\) that are both implicitly functions of some parameter \\(a\\) over \\(\\mathbb{R}_{\\geq 0}\\), we use the shorthand \\(X \\lesssim Y\\) and \\(X = O(Y)\\) interchangeably to denote that there exists an absolute constant \\(C > 0\\) such that for all \\(a\\) sufficiently large, \\(X(a) \\leq CY(a)\\). We also use the shorthand \\(X \\gtrsim Y\\) and \\(X = \\Omega(Y)\\), defined in the obvious way.\n\nFinally, we will use the following standard bounds.\n\nLemma B.1 (Sub-Gaussian norm, see e.g. [Ver]): The sub-Gaussian norm of a random variable \\(X \\in \\mathbb{R}\\), denoted by \\(\\|X\\|_{\\psi_2}\\) is defined as \\(\\|X\\|_{\\psi_2} = \\inf\\{t > 0 : E[\\exp(X^2/t^2)] \\leq 2\\}\\). The sub-Gaussian norm has the following properties:\n\n$$\n19\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Notations and Preliminaries", "md": "# Additional Notations and Preliminaries"}, {"type": "heading", "lvl": 2, "value": "By simplifying \\(w^*_{1,t}(x)\\), we obtain", "md": "## By simplifying \\(w^*_{1,t}(x)\\), we obtain"}, {"type": "text", "value": "$$\nw^*_{1,t}(x) = \\frac{1 + \\exp(\\|x-\\mu^*\\|^2_1 - \\|x+\\mu^*\\|^2_2)}{2} = \\frac{1}{1 + \\exp(-2\\mu^{\\top}x)} = \\sigma(2\\mu^{\\top}x) \\quad \\text{(A.2)}\n$$\n\nwhere \\(\\sigma(\\cdot)\\) denotes the sigmoid function. Using Eq. (A.2) in Eq. (A.1), we obtain\n\n$$\n\\nabla \\log q_t(x) = \\tanh(\\mu^{\\top}x)\\mu^* - x.\n$$", "md": "$$\nw^*_{1,t}(x) = \\frac{1 + \\exp(\\|x-\\mu^*\\|^2_1 - \\|x+\\mu^*\\|^2_2)}{2} = \\frac{1}{1 + \\exp(-2\\mu^{\\top}x)} = \\sigma(2\\mu^{\\top}x) \\quad \\text{(A.2)}\n$$\n\nwhere \\(\\sigma(\\cdot)\\) denotes the sigmoid function. Using Eq. (A.2) in Eq. (A.1), we obtain\n\n$$\n\\nabla \\log q_t(x) = \\tanh(\\mu^{\\top}x)\\mu^* - x.\n$$"}, {"type": "heading", "lvl": 2, "value": "Additional Notations and Preliminaries", "md": "## Additional Notations and Preliminaries"}, {"type": "text", "value": "In this section, we provide additional notations and preliminaries for the proofs to follow. Recall that we use \\(L_t(s\\theta_t)\\) to denote the population denoising loss at noise scale \\(t\\).\n\n$$\nL_t(s\\theta_t) = E[s\\theta_t(X_t) + Z_t]^2 \\cdot \\frac{1}{1 - \\exp(-2t)}\n$$\n\nWe use \\(L_t(s\\theta_t(x_0, z_t))\\) to denote the denoising loss at noise scale \\(t\\) on a sample \\(x_0\\) from the data distribution and \\(z_t\\) from the standard Gaussian distribution:\n\n$$\nL_t(s\\theta_t(x_0, z_t)) = s\\theta_t(x_t) + \\frac{z_t^2}{1 - \\exp(-2t)}\n$$\n\nwhere \\(x_t = \\exp(-t)x_0 + \\frac{1 - \\exp(-2t)}{z_t}\\). We use \\(\\alpha_t\\) as shorthand notation for \\(\\exp(-t)\\) and \\(\\beta_t\\) as shorthand notation for \\(\\frac{1 - \\exp(-2t)}{t}\\).\n\nFor mixtures of two Gaussians, we use \\(B\\) to denote the upper bound on \\(\\|\\mu^*\\|^2\\), that is, throughout, we assume that \\(B = \\text{poly}(d)\\), \\(\\|\\mu^*\\|^2 \\leq B\\).\n\nFor any vector \\(v\\), we use \\(\\hat{v}\\) to denote the unit vector along the direction of \\(v\\). For a vector \\(v\\), we use \\([v]_i\\) to denote the \\(i\\)th coordinate of \\(v\\). Similarly, for a matrix \\(X\\), we use \\([X]_i\\) to denote the \\(i\\)th row of the matrix. For any positive integer \\(n\\), we use \\([n]\\) to denote the set \\(\\{1, 2, ..., n\\}\\). We use \\(N(\\mu, \\sigma^2 \\cdot \\text{Id})\\) to denote the standard Gaussian with mean \\(\\mu\\) and covariance \\(\\sigma^2 \\cdot \\text{Id}\\). Sometimes, we use a shorter notation \\(N_{\\mu}\\) to denote \\(N(\\mu, \\text{Id}\\). For any two quantities \\(X\\) and \\(Y\\) that are both implicitly functions of some parameter \\(a\\) over \\(\\mathbb{R}_{\\geq 0}\\), we use the shorthand \\(X \\lesssim Y\\) and \\(X = O(Y)\\) interchangeably to denote that there exists an absolute constant \\(C > 0\\) such that for all \\(a\\) sufficiently large, \\(X(a) \\leq CY(a)\\). We also use the shorthand \\(X \\gtrsim Y\\) and \\(X = \\Omega(Y)\\), defined in the obvious way.\n\nFinally, we will use the following standard bounds.\n\nLemma B.1 (Sub-Gaussian norm, see e.g. [Ver]): The sub-Gaussian norm of a random variable \\(X \\in \\mathbb{R}\\), denoted by \\(\\|X\\|_{\\psi_2}\\) is defined as \\(\\|X\\|_{\\psi_2} = \\inf\\{t > 0 : E[\\exp(X^2/t^2)] \\leq 2\\}\\). The sub-Gaussian norm has the following properties:\n\n$$\n19\n$$", "md": "In this section, we provide additional notations and preliminaries for the proofs to follow. Recall that we use \\(L_t(s\\theta_t)\\) to denote the population denoising loss at noise scale \\(t\\).\n\n$$\nL_t(s\\theta_t) = E[s\\theta_t(X_t) + Z_t]^2 \\cdot \\frac{1}{1 - \\exp(-2t)}\n$$\n\nWe use \\(L_t(s\\theta_t(x_0, z_t))\\) to denote the denoising loss at noise scale \\(t\\) on a sample \\(x_0\\) from the data distribution and \\(z_t\\) from the standard Gaussian distribution:\n\n$$\nL_t(s\\theta_t(x_0, z_t)) = s\\theta_t(x_t) + \\frac{z_t^2}{1 - \\exp(-2t)}\n$$\n\nwhere \\(x_t = \\exp(-t)x_0 + \\frac{1 - \\exp(-2t)}{z_t}\\). We use \\(\\alpha_t\\) as shorthand notation for \\(\\exp(-t)\\) and \\(\\beta_t\\) as shorthand notation for \\(\\frac{1 - \\exp(-2t)}{t}\\).\n\nFor mixtures of two Gaussians, we use \\(B\\) to denote the upper bound on \\(\\|\\mu^*\\|^2\\), that is, throughout, we assume that \\(B = \\text{poly}(d)\\), \\(\\|\\mu^*\\|^2 \\leq B\\).\n\nFor any vector \\(v\\), we use \\(\\hat{v}\\) to denote the unit vector along the direction of \\(v\\). For a vector \\(v\\), we use \\([v]_i\\) to denote the \\(i\\)th coordinate of \\(v\\). Similarly, for a matrix \\(X\\), we use \\([X]_i\\) to denote the \\(i\\)th row of the matrix. For any positive integer \\(n\\), we use \\([n]\\) to denote the set \\(\\{1, 2, ..., n\\}\\). We use \\(N(\\mu, \\sigma^2 \\cdot \\text{Id})\\) to denote the standard Gaussian with mean \\(\\mu\\) and covariance \\(\\sigma^2 \\cdot \\text{Id}\\). Sometimes, we use a shorter notation \\(N_{\\mu}\\) to denote \\(N(\\mu, \\text{Id}\\). For any two quantities \\(X\\) and \\(Y\\) that are both implicitly functions of some parameter \\(a\\) over \\(\\mathbb{R}_{\\geq 0}\\), we use the shorthand \\(X \\lesssim Y\\) and \\(X = O(Y)\\) interchangeably to denote that there exists an absolute constant \\(C > 0\\) such that for all \\(a\\) sufficiently large, \\(X(a) \\leq CY(a)\\). We also use the shorthand \\(X \\gtrsim Y\\) and \\(X = \\Omega(Y)\\), defined in the obvious way.\n\nFinally, we will use the following standard bounds.\n\nLemma B.1 (Sub-Gaussian norm, see e.g. [Ver]): The sub-Gaussian norm of a random variable \\(X \\in \\mathbb{R}\\), denoted by \\(\\|X\\|_{\\psi_2}\\) is defined as \\(\\|X\\|_{\\psi_2} = \\inf\\{t > 0 : E[\\exp(X^2/t^2)] \\leq 2\\}\\). The sub-Gaussian norm has the following properties:\n\n$$\n19\n$$"}]}, {"page": 20, "text": "     1. (Bounded): Any bounded random variable X (i.e., there is a finite A for which |X| \u2264                                                  A with\n         probability 1) is sub-Gaussian:\n     2. (Centering): If X is a sub-Gaussian random variable, then X \u2212 \u2225X\u2225\u03c82 \u2264         \u221a Aln 2                  E[X] is also a sub-Gaussian\n         random variable. Specifically, the following holds for some absolute constant C.\n                                                              \u2225X \u2212      E[X]\u2225\u03c82 \u2264        C\u2225X\u2225\u03c82\n     3. (Moment generating function bound): If X is a sub-Gaussian random variable with E[X] = 0,\n         then\n         where C is some absolute constant.  E[exp(\u03bbX)] \u2264           exp(C\u03bb2\u2225X\u22252          \u03c82)      for all \u03bb \u2208      R,\n     4. (Sum of sub-Gaussian random variables): If X1 and X2 are mean zero sub-Gaussian random\n         variables, then\n                                                       \u2225X1 + X2\u2225\u03c82 \u2264             \u2225X1\u2225\u03c82 + \u2225X2\u2225\u03c82 .\n     5. (Product with a bounded random variable): If X is a sub-Gaussian random variable and Y is\n         a bounded random variable Y \u2208                      [0, 1], then\n                                                                   \u2225XY \u2225\u03c82 \u2264         \u2225X\u2225\u03c82 .\nLemma B.2 (Sub-exponential norm, see e.g. [Ver]). The sub-exponential norm of a random variable\nX \u2208     R, denoted by \u2225X\u2225\u03c81 is defined as    \u2225X\u2225\u03c81 = inf{t > 0 : E[exp(|X|/t)] \u2264                          2}.\nThe sub-exponential norm has the following properties:\n     1. (Sum of sub-exponential distributions): If X1 and X2 are mean-zero sub-exponential random\n         variables, then X1 + X2 is also a mean-zero sub-exponential variable. Specifically,\n                                                   \u2225X1 + X2\u2225\u03c81 \u2264             \u221a  2(\u2225X1\u2225\u03c81 + \u2225X2\u2225\u03c81) .\n     2. (Centering) If X is a sub-exponential random variable, then X \u2212                                        E[X] is sub-exponential with\n         where C is some absolute constant.                   \u2225X \u2212     E[X]\u2225\u03c81 \u2264         C\u2225X\u2225\u03c81,\nProof. The proof follows from following the equivalent defi                                 nition of a sub-exponential random vari-\nable: If any random variable X satisfi                      es\n                          E[exp(\u03bbX)] \u2264           exp(C\u2225X\u22252        \u03c81\u03bb2) for all \u03bb such that |\u03bb| \u2264                  C\u2225X\u222521   \u03c81  ,\n                                                                            20", "md": "1. (Bounded): Any bounded random variable \\(X\\) (i.e., there is a finite \\(A\\) for which \\(|X| \\leq A\\) with probability 1) is sub-Gaussian:\n2. (Centering): If \\(X\\) is a sub-Gaussian random variable, then \\(X - \\|X\\|_{\\psi_2} \\leq \\sqrt{A\\ln 2}\\, E[X]\\) is also a sub-Gaussian random variable. Specifically, the following holds for some absolute constant \\(C\\):\n\\[\n\\|X - E[X]\\|_{\\psi_2} \\leq C\\|X\\|_{\\psi_2}\n\\]\n3. (Moment generating function bound): If \\(X\\) is a sub-Gaussian random variable with \\(E[X] = 0\\), then\n\\[\nE[\\exp(\\lambda X)] \\leq \\exp(C\\lambda^2\\|X\\|_{\\psi_2}^2) \\text{ for all } \\lambda \\in \\mathbb{R},\n\\]\nwhere \\(C\\) is some absolute constant.\n4. (Sum of sub-Gaussian random variables): If \\(X_1\\) and \\(X_2\\) are mean zero sub-Gaussian random variables, then\n\\[\n\\|X_1 + X_2\\|_{\\psi_2} \\leq \\|X_1\\|_{\\psi_2} + \\|X_2\\|_{\\psi_2}.\n\\]\n5. (Product with a bounded random variable): If \\(X\\) is a sub-Gaussian random variable and \\(Y\\) is a bounded random variable \\(Y \\in [0, 1]\\), then\n\\[\n\\|XY\\|_{\\psi_2} \\leq \\|X\\|_{\\psi_2}.\n\\]\n\nLemma B.2 (Sub-exponential norm, see e.g. [Ver]). The sub-exponential norm of a random variable \\(X \\in \\mathbb{R}\\), denoted by \\(\\|X\\|_{\\psi_1}\\) is defined as \\(\\|X\\|_{\\psi_1} = \\inf\\{t > 0 : E[\\exp(|X|/t)] \\leq 2\\}\\).\n\nThe sub-exponential norm has the following properties:\n\n1. (Sum of sub-exponential distributions): If \\(X_1\\) and \\(X_2\\) are mean-zero sub-exponential random variables, then \\(X_1 + X_2\\) is also a mean-zero sub-exponential variable. Specifically,\n\\[\n\\|X_1 + X_2\\|_{\\psi_1} \\leq \\sqrt{2(\\|X_1\\|_{\\psi_1} + \\|X_2\\|_{\\psi_1})}.\n\\]\n2. (Centering) If \\(X\\) is a sub-exponential random variable, then \\(X - E[X]\\) is sub-exponential with\n\\[\n\\|X - E[X]\\|_{\\psi_1} \\leq C\\|X\\|_{\\psi_1},\n\\]\nwhere \\(C\\) is some absolute constant.\n\nProof. The proof follows from following the equivalent definition of a sub-exponential random variable: If any random variable \\(X\\) satisfies\n\\[\nE[\\exp(\\lambda X)] \\leq \\exp(C\\|X\\|_{\\psi_1}^2\\lambda^2) \\text{ for all } \\lambda \\text{ such that } |\\lambda| \\leq C\\|X\\|_{\\psi_1}^2,\n\\]", "images": [], "items": [{"type": "text", "value": "1. (Bounded): Any bounded random variable \\(X\\) (i.e., there is a finite \\(A\\) for which \\(|X| \\leq A\\) with probability 1) is sub-Gaussian:\n2. (Centering): If \\(X\\) is a sub-Gaussian random variable, then \\(X - \\|X\\|_{\\psi_2} \\leq \\sqrt{A\\ln 2}\\, E[X]\\) is also a sub-Gaussian random variable. Specifically, the following holds for some absolute constant \\(C\\):\n\\[\n\\|X - E[X]\\|_{\\psi_2} \\leq C\\|X\\|_{\\psi_2}\n\\]\n3. (Moment generating function bound): If \\(X\\) is a sub-Gaussian random variable with \\(E[X] = 0\\), then\n\\[\nE[\\exp(\\lambda X)] \\leq \\exp(C\\lambda^2\\|X\\|_{\\psi_2}^2) \\text{ for all } \\lambda \\in \\mathbb{R},\n\\]\nwhere \\(C\\) is some absolute constant.\n4. (Sum of sub-Gaussian random variables): If \\(X_1\\) and \\(X_2\\) are mean zero sub-Gaussian random variables, then\n\\[\n\\|X_1 + X_2\\|_{\\psi_2} \\leq \\|X_1\\|_{\\psi_2} + \\|X_2\\|_{\\psi_2}.\n\\]\n5. (Product with a bounded random variable): If \\(X\\) is a sub-Gaussian random variable and \\(Y\\) is a bounded random variable \\(Y \\in [0, 1]\\), then\n\\[\n\\|XY\\|_{\\psi_2} \\leq \\|X\\|_{\\psi_2}.\n\\]\n\nLemma B.2 (Sub-exponential norm, see e.g. [Ver]). The sub-exponential norm of a random variable \\(X \\in \\mathbb{R}\\), denoted by \\(\\|X\\|_{\\psi_1}\\) is defined as \\(\\|X\\|_{\\psi_1} = \\inf\\{t > 0 : E[\\exp(|X|/t)] \\leq 2\\}\\).\n\nThe sub-exponential norm has the following properties:\n\n1. (Sum of sub-exponential distributions): If \\(X_1\\) and \\(X_2\\) are mean-zero sub-exponential random variables, then \\(X_1 + X_2\\) is also a mean-zero sub-exponential variable. Specifically,\n\\[\n\\|X_1 + X_2\\|_{\\psi_1} \\leq \\sqrt{2(\\|X_1\\|_{\\psi_1} + \\|X_2\\|_{\\psi_1})}.\n\\]\n2. (Centering) If \\(X\\) is a sub-exponential random variable, then \\(X - E[X]\\) is sub-exponential with\n\\[\n\\|X - E[X]\\|_{\\psi_1} \\leq C\\|X\\|_{\\psi_1},\n\\]\nwhere \\(C\\) is some absolute constant.\n\nProof. The proof follows from following the equivalent definition of a sub-exponential random variable: If any random variable \\(X\\) satisfies\n\\[\nE[\\exp(\\lambda X)] \\leq \\exp(C\\|X\\|_{\\psi_1}^2\\lambda^2) \\text{ for all } \\lambda \\text{ such that } |\\lambda| \\leq C\\|X\\|_{\\psi_1}^2,\n\\]", "md": "1. (Bounded): Any bounded random variable \\(X\\) (i.e., there is a finite \\(A\\) for which \\(|X| \\leq A\\) with probability 1) is sub-Gaussian:\n2. (Centering): If \\(X\\) is a sub-Gaussian random variable, then \\(X - \\|X\\|_{\\psi_2} \\leq \\sqrt{A\\ln 2}\\, E[X]\\) is also a sub-Gaussian random variable. Specifically, the following holds for some absolute constant \\(C\\):\n\\[\n\\|X - E[X]\\|_{\\psi_2} \\leq C\\|X\\|_{\\psi_2}\n\\]\n3. (Moment generating function bound): If \\(X\\) is a sub-Gaussian random variable with \\(E[X] = 0\\), then\n\\[\nE[\\exp(\\lambda X)] \\leq \\exp(C\\lambda^2\\|X\\|_{\\psi_2}^2) \\text{ for all } \\lambda \\in \\mathbb{R},\n\\]\nwhere \\(C\\) is some absolute constant.\n4. (Sum of sub-Gaussian random variables): If \\(X_1\\) and \\(X_2\\) are mean zero sub-Gaussian random variables, then\n\\[\n\\|X_1 + X_2\\|_{\\psi_2} \\leq \\|X_1\\|_{\\psi_2} + \\|X_2\\|_{\\psi_2}.\n\\]\n5. (Product with a bounded random variable): If \\(X\\) is a sub-Gaussian random variable and \\(Y\\) is a bounded random variable \\(Y \\in [0, 1]\\), then\n\\[\n\\|XY\\|_{\\psi_2} \\leq \\|X\\|_{\\psi_2}.\n\\]\n\nLemma B.2 (Sub-exponential norm, see e.g. [Ver]). The sub-exponential norm of a random variable \\(X \\in \\mathbb{R}\\), denoted by \\(\\|X\\|_{\\psi_1}\\) is defined as \\(\\|X\\|_{\\psi_1} = \\inf\\{t > 0 : E[\\exp(|X|/t)] \\leq 2\\}\\).\n\nThe sub-exponential norm has the following properties:\n\n1. (Sum of sub-exponential distributions): If \\(X_1\\) and \\(X_2\\) are mean-zero sub-exponential random variables, then \\(X_1 + X_2\\) is also a mean-zero sub-exponential variable. Specifically,\n\\[\n\\|X_1 + X_2\\|_{\\psi_1} \\leq \\sqrt{2(\\|X_1\\|_{\\psi_1} + \\|X_2\\|_{\\psi_1})}.\n\\]\n2. (Centering) If \\(X\\) is a sub-exponential random variable, then \\(X - E[X]\\) is sub-exponential with\n\\[\n\\|X - E[X]\\|_{\\psi_1} \\leq C\\|X\\|_{\\psi_1},\n\\]\nwhere \\(C\\) is some absolute constant.\n\nProof. The proof follows from following the equivalent definition of a sub-exponential random variable: If any random variable \\(X\\) satisfies\n\\[\nE[\\exp(\\lambda X)] \\leq \\exp(C\\|X\\|_{\\psi_1}^2\\lambda^2) \\text{ for all } \\lambda \\text{ such that } |\\lambda| \\leq C\\|X\\|_{\\psi_1}^2,\n\\]"}]}, {"page": 21, "text": "for some constant C, then X is sub-exponential random variable with sub-exponential norm \u2225X\u2225\u03c81.\nThen, for any |\u03bb| \u2264             2C max(\u2225X1\u22252   1 \u03c8 1,\u2225X2\u22252  \u03c81), the MGF of X1 + X2 is given by\n                                E[exp(\u03bb(X1 + X2))] \u2264                 E[exp(2\u03bbX1)]1/2E[exp(2\u03bbX2)]1/2\n                                                                 \u2264   exp(C\u2225X1\u22252        \u03c812\u03bb2) exp(C\u2225X2\u22252            \u03c812\u03bb2)\n                                                                 \u2264   exp(C\u03bb2(2\u2225X1\u22252           \u03c81 + 2\u2225X2\u22252       \u03c81)) .\nUsing \u2225X1\u2225\u03c81 + \u2225X2\u2225\u03c81 \u2265                     max(\u2225X1\u2225\u03c81, \u2225X2\u2225\u03c81), we know that above inequality is true for any \u03bb\n                                1                                   1\nwith |\u03bb| \u2264        2C(\u2225X1\u2225\u03c81+\u2225X2\u2225\u03c81)2 \u2264               2C max(\u2225X1\u22252     \u03c8 1,\u2225X2\u22252  \u03c81). This completes the proof.\nLemma B.3 (Corollary 2.8.4 in [Ver]). (Bernstein\u2019s inequality for sub-exponential random variable)\nLet X1, X2, . . . , XN be independent, mean zero, sub-exponential random variables. Then, for every\n\u03b5 \u2265    0, we have  Pr        1    N   Xi     \u2265   \u03b5   \u2264   2 exp       \u2212   cN min                  \u03b5          ,            \u03b52\n                            N    i=1                                                    maxi \u2225Xi\u2225\u03c81           (maxi \u2225Xi\u2225\u03c81)2\nwhere c > 0 is some absolute constant.\nC        Learning mixtures of two Gaussians with constant separation\nIn this section, we provide the details and proofs for learning mixtures of two Gaussians with constant\nseparation. Our results in this section can be summarized in the following theorem statement.\nTheorem C.1 (Formal version of Theorem 7). Let q be a mixture of two Gaussians (in the form\nof Eq. (7)) with mean parameter \u00b5\u2217                        satisfying \u2225\u00b5\u2217\u2225         > c for some absolute constant c > 0. Recalling                        1\nthat B denotes an a priori upper bound on \u2225\u00b5\u2217\u2225, we have that for any \u03b5 \u2264                                                       \u03b5\u2032 where \u03b5\u2032 \u2272          d2B9 ,\nthere exists a procedure satisfying the following. If the procedure is run for at least \u2126(B6 log(d/\u03b5))\niterations with at least poly(d, B)/\u03b52 samples from q, then it outputs \u02dc                                       \u00b5 such that \u2225\u02dc       \u00b5 \u2212   \u00b5\u2217\u2225    \u2264  \u03b5 with\nhigh probability.\nAs described earlier, the procedure fi                       rst runs gradient descent on the DDPM objective described\nin Algorithm 1 from a random Gaussian initialization in a high noise scale regime with noise scale\nt1 = O(log d). It then uses the output of the fi                          rst step as initialization and runs the Algorithm 1 in\na low noise scale regime with noise scale t2 = O(1).\n      We begin by calculating the form of the gradient updates:\nLemma C.2. For any noise scale t > 0, the gradient update for the mixture of two Gaussians on\nthe DDPM objective is given by\n            \u2212\u2207\u00b5tLt(s\u00b5t) = Ex\u223cN (\u00b5\u2217               t ,Id)    tanh(\u00b5\u22a4    t x) \u2212     1               t x)\u2225\u00b5t\u22252 + tanh\u2032(\u00b5\u22a4           t x)\u00b5\u22a4  t x    x\n                                                                                 2 tanh\u2032\u2032(\u00b5\u22a4\n                                     \u2212   \u00b5t \u2212    Ex\u223cN (\u00b5\u2217    t ,Id)  tanh\u2032(\u00b5\u22a4    t x)\u00b5t       .\nThe proof of Lemma C.2 is given in Appendix F.1.\n                                                                             21", "md": "For some constant \\(C\\), then \\(X\\) is sub-exponential random variable with sub-exponential norm \\(\\|X\\|_{\\psi_1}\\).\n\nThen, for any \\(|\\lambda| \\leq 2C \\max(\\|X_1\\|_{2\\psi_1}, \\|X_2\\|_{2\\psi_1})\\), the MGF of \\(X_1 + X_2\\) is given by\n\n\\[\n\\begin{align*}\nE[\\exp(\\lambda(X_1 + X_2))] &\\leq E[\\exp(2\\lambda X_1)]^{1/2} E[\\exp(2\\lambda X_2)]^{1/2} \\\\\n&\\leq \\exp(C\\|X_1\\|_{2\\psi_1}\\lambda^2) \\exp(C\\|X_2\\|_{2\\psi_1}\\lambda^2) \\\\\n&\\leq \\exp(C\\lambda^2(2\\|X_1\\|_{\\psi_1} + 2\\|X_2\\|_{\\psi_1})) .\n\\end{align*}\n\\]\nUsing \\(\\|X_1\\|_{\\psi_1} + \\|X_2\\|_{\\psi_1} \\geq \\max(\\|X_1\\|_{\\psi_1}, \\|X_2\\|_{\\psi_1})\\), we know that above inequality is true for any \\(\\lambda\\) with \\(|\\lambda| \\leq 2C(\\|X_1\\|_{\\psi_1}+\\|X_2\\|_{\\psi_1})^2 \\leq 2C \\max(\\|X_1\\|_{2\\psi_1}, \\|X_2\\|_{2\\psi_1})\\). This completes the proof.\n\nLemma B.3 (Corollary 2.8.4 in [Ver]). (Bernstein\u2019s inequality for sub-exponential random variable)\n\nLet \\(X_1, X_2, ..., X_N\\) be independent, mean zero, sub-exponential random variables. Then, for every \\(\\epsilon \\geq 0\\), we have\n\n\\[\n\\text{Pr}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_i \\geq \\epsilon\\right) \\leq 2 \\exp\\left(-cN \\min\\left(\\frac{\\epsilon}{\\max_i \\|X_i\\|_{\\psi_1}}, \\frac{\\epsilon^2}{(\\max_i \\|X_i\\|_{\\psi_1})^2}\\right)\\right)\n\\]\nwhere \\(c > 0\\) is some absolute constant.\n\nLearning mixtures of two Gaussians with constant separation\n\nIn this section, we provide the details and proofs for learning mixtures of two Gaussians with constant separation. Our results in this section can be summarized in the following theorem statement.\n\nTheorem C.1 (Formal version of Theorem 7). Let \\(q\\) be a mixture of two Gaussians (in the form of Eq. (7)) with mean parameter \\(\\mu^*\\) satisfying \\(\\|\\mu^*\\| > c\\) for some absolute constant \\(c > 0\\). Recalling that \\(B\\) denotes an a priori upper bound on \\(\\|\\mu^*\\|\\), we have that for any \\(\\epsilon \\leq \\epsilon'\\) where \\(\\epsilon' \\lesssim d^2B^9\\), there exists a procedure satisfying the following. If the procedure is run for at least \\(\\Omega(B^6 \\log(d/\\epsilon))\\) iterations with at least \\(\\text{poly}(d, B)/\\epsilon^2\\) samples from \\(q\\), then it outputs \\(\\tilde{\\mu}\\) such that \\(\\|\\tilde{\\mu} - \\mu^*\\| \\leq \\epsilon\\) with high probability.\n\nAs described earlier, the procedure first runs gradient descent on the DDPM objective described in Algorithm 1 from a random Gaussian initialization in a high noise scale regime with noise scale \\(t_1 = O(\\log d)\\). It then uses the output of the first step as initialization and runs the Algorithm 1 in a low noise scale regime with noise scale \\(t_2 = O(1)\\).\n\nWe begin by calculating the form of the gradient updates:\n\nLemma C.2. For any noise scale \\(t > 0\\), the gradient update for the mixture of two Gaussians on the DDPM objective is given by\n\n\\[\n-\\nabla_{\\mu} t L_t(s\\mu_t) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t, I_d)} \\left[\\tanh(\\mu_t^{\\top} x) - \\frac{1}{t} x\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^{\\top} x)\\mu_t^{\\top} x\\right]\n\\]\n\\[\n- \\frac{1}{2} \\tanh''(\\mu_t^{\\top} x)(\\mu_t - \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t, I_d)}[\\tanh'(\\mu_t^{\\top} x)\\mu_t]).\n\\]\nThe proof of Lemma C.2 is given in Appendix F.1.\n\n21", "images": [], "items": [{"type": "text", "value": "For some constant \\(C\\), then \\(X\\) is sub-exponential random variable with sub-exponential norm \\(\\|X\\|_{\\psi_1}\\).\n\nThen, for any \\(|\\lambda| \\leq 2C \\max(\\|X_1\\|_{2\\psi_1}, \\|X_2\\|_{2\\psi_1})\\), the MGF of \\(X_1 + X_2\\) is given by\n\n\\[\n\\begin{align*}\nE[\\exp(\\lambda(X_1 + X_2))] &\\leq E[\\exp(2\\lambda X_1)]^{1/2} E[\\exp(2\\lambda X_2)]^{1/2} \\\\\n&\\leq \\exp(C\\|X_1\\|_{2\\psi_1}\\lambda^2) \\exp(C\\|X_2\\|_{2\\psi_1}\\lambda^2) \\\\\n&\\leq \\exp(C\\lambda^2(2\\|X_1\\|_{\\psi_1} + 2\\|X_2\\|_{\\psi_1})) .\n\\end{align*}\n\\]\nUsing \\(\\|X_1\\|_{\\psi_1} + \\|X_2\\|_{\\psi_1} \\geq \\max(\\|X_1\\|_{\\psi_1}, \\|X_2\\|_{\\psi_1})\\), we know that above inequality is true for any \\(\\lambda\\) with \\(|\\lambda| \\leq 2C(\\|X_1\\|_{\\psi_1}+\\|X_2\\|_{\\psi_1})^2 \\leq 2C \\max(\\|X_1\\|_{2\\psi_1}, \\|X_2\\|_{2\\psi_1})\\). This completes the proof.\n\nLemma B.3 (Corollary 2.8.4 in [Ver]). (Bernstein\u2019s inequality for sub-exponential random variable)\n\nLet \\(X_1, X_2, ..., X_N\\) be independent, mean zero, sub-exponential random variables. Then, for every \\(\\epsilon \\geq 0\\), we have\n\n\\[\n\\text{Pr}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_i \\geq \\epsilon\\right) \\leq 2 \\exp\\left(-cN \\min\\left(\\frac{\\epsilon}{\\max_i \\|X_i\\|_{\\psi_1}}, \\frac{\\epsilon^2}{(\\max_i \\|X_i\\|_{\\psi_1})^2}\\right)\\right)\n\\]\nwhere \\(c > 0\\) is some absolute constant.\n\nLearning mixtures of two Gaussians with constant separation\n\nIn this section, we provide the details and proofs for learning mixtures of two Gaussians with constant separation. Our results in this section can be summarized in the following theorem statement.\n\nTheorem C.1 (Formal version of Theorem 7). Let \\(q\\) be a mixture of two Gaussians (in the form of Eq. (7)) with mean parameter \\(\\mu^*\\) satisfying \\(\\|\\mu^*\\| > c\\) for some absolute constant \\(c > 0\\). Recalling that \\(B\\) denotes an a priori upper bound on \\(\\|\\mu^*\\|\\), we have that for any \\(\\epsilon \\leq \\epsilon'\\) where \\(\\epsilon' \\lesssim d^2B^9\\), there exists a procedure satisfying the following. If the procedure is run for at least \\(\\Omega(B^6 \\log(d/\\epsilon))\\) iterations with at least \\(\\text{poly}(d, B)/\\epsilon^2\\) samples from \\(q\\), then it outputs \\(\\tilde{\\mu}\\) such that \\(\\|\\tilde{\\mu} - \\mu^*\\| \\leq \\epsilon\\) with high probability.\n\nAs described earlier, the procedure first runs gradient descent on the DDPM objective described in Algorithm 1 from a random Gaussian initialization in a high noise scale regime with noise scale \\(t_1 = O(\\log d)\\). It then uses the output of the first step as initialization and runs the Algorithm 1 in a low noise scale regime with noise scale \\(t_2 = O(1)\\).\n\nWe begin by calculating the form of the gradient updates:\n\nLemma C.2. For any noise scale \\(t > 0\\), the gradient update for the mixture of two Gaussians on the DDPM objective is given by\n\n\\[\n-\\nabla_{\\mu} t L_t(s\\mu_t) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t, I_d)} \\left[\\tanh(\\mu_t^{\\top} x) - \\frac{1}{t} x\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^{\\top} x)\\mu_t^{\\top} x\\right]\n\\]\n\\[\n- \\frac{1}{2} \\tanh''(\\mu_t^{\\top} x)(\\mu_t - \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t, I_d)}[\\tanh'(\\mu_t^{\\top} x)\\mu_t]).\n\\]\nThe proof of Lemma C.2 is given in Appendix F.1.\n\n21", "md": "For some constant \\(C\\), then \\(X\\) is sub-exponential random variable with sub-exponential norm \\(\\|X\\|_{\\psi_1}\\).\n\nThen, for any \\(|\\lambda| \\leq 2C \\max(\\|X_1\\|_{2\\psi_1}, \\|X_2\\|_{2\\psi_1})\\), the MGF of \\(X_1 + X_2\\) is given by\n\n\\[\n\\begin{align*}\nE[\\exp(\\lambda(X_1 + X_2))] &\\leq E[\\exp(2\\lambda X_1)]^{1/2} E[\\exp(2\\lambda X_2)]^{1/2} \\\\\n&\\leq \\exp(C\\|X_1\\|_{2\\psi_1}\\lambda^2) \\exp(C\\|X_2\\|_{2\\psi_1}\\lambda^2) \\\\\n&\\leq \\exp(C\\lambda^2(2\\|X_1\\|_{\\psi_1} + 2\\|X_2\\|_{\\psi_1})) .\n\\end{align*}\n\\]\nUsing \\(\\|X_1\\|_{\\psi_1} + \\|X_2\\|_{\\psi_1} \\geq \\max(\\|X_1\\|_{\\psi_1}, \\|X_2\\|_{\\psi_1})\\), we know that above inequality is true for any \\(\\lambda\\) with \\(|\\lambda| \\leq 2C(\\|X_1\\|_{\\psi_1}+\\|X_2\\|_{\\psi_1})^2 \\leq 2C \\max(\\|X_1\\|_{2\\psi_1}, \\|X_2\\|_{2\\psi_1})\\). This completes the proof.\n\nLemma B.3 (Corollary 2.8.4 in [Ver]). (Bernstein\u2019s inequality for sub-exponential random variable)\n\nLet \\(X_1, X_2, ..., X_N\\) be independent, mean zero, sub-exponential random variables. Then, for every \\(\\epsilon \\geq 0\\), we have\n\n\\[\n\\text{Pr}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_i \\geq \\epsilon\\right) \\leq 2 \\exp\\left(-cN \\min\\left(\\frac{\\epsilon}{\\max_i \\|X_i\\|_{\\psi_1}}, \\frac{\\epsilon^2}{(\\max_i \\|X_i\\|_{\\psi_1})^2}\\right)\\right)\n\\]\nwhere \\(c > 0\\) is some absolute constant.\n\nLearning mixtures of two Gaussians with constant separation\n\nIn this section, we provide the details and proofs for learning mixtures of two Gaussians with constant separation. Our results in this section can be summarized in the following theorem statement.\n\nTheorem C.1 (Formal version of Theorem 7). Let \\(q\\) be a mixture of two Gaussians (in the form of Eq. (7)) with mean parameter \\(\\mu^*\\) satisfying \\(\\|\\mu^*\\| > c\\) for some absolute constant \\(c > 0\\). Recalling that \\(B\\) denotes an a priori upper bound on \\(\\|\\mu^*\\|\\), we have that for any \\(\\epsilon \\leq \\epsilon'\\) where \\(\\epsilon' \\lesssim d^2B^9\\), there exists a procedure satisfying the following. If the procedure is run for at least \\(\\Omega(B^6 \\log(d/\\epsilon))\\) iterations with at least \\(\\text{poly}(d, B)/\\epsilon^2\\) samples from \\(q\\), then it outputs \\(\\tilde{\\mu}\\) such that \\(\\|\\tilde{\\mu} - \\mu^*\\| \\leq \\epsilon\\) with high probability.\n\nAs described earlier, the procedure first runs gradient descent on the DDPM objective described in Algorithm 1 from a random Gaussian initialization in a high noise scale regime with noise scale \\(t_1 = O(\\log d)\\). It then uses the output of the first step as initialization and runs the Algorithm 1 in a low noise scale regime with noise scale \\(t_2 = O(1)\\).\n\nWe begin by calculating the form of the gradient updates:\n\nLemma C.2. For any noise scale \\(t > 0\\), the gradient update for the mixture of two Gaussians on the DDPM objective is given by\n\n\\[\n-\\nabla_{\\mu} t L_t(s\\mu_t) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t, I_d)} \\left[\\tanh(\\mu_t^{\\top} x) - \\frac{1}{t} x\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^{\\top} x)\\mu_t^{\\top} x\\right]\n\\]\n\\[\n- \\frac{1}{2} \\tanh''(\\mu_t^{\\top} x)(\\mu_t - \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t, I_d)}[\\tanh'(\\mu_t^{\\top} x)\\mu_t]).\n\\]\nThe proof of Lemma C.2 is given in Appendix F.1.\n\n21"}]}, {"page": 22, "text": "C.1        High noise regime\u2013connection to power iteration\nHere we show that running population gradient descent on the DDPM objective at high noise scale\nbehaves like power iteration on the covariance matrix of the data and thus reaches an iterate \u00b5 with\nconstant correlation with \u00b5\u2217.\nLemma C.3. For any noise scale t > t\u2032 and number of samples n > n\u2032 where t\u2032 \u2272                                                                   log d and\nn\u2032 = \u0398      d4B3      , with high probability, the negative gradient of the diff                            usion model objective Lt(st) can\n               \u03b52\nbe approximated by 2\u00b5\u2217              t \u00b5\u2217\u22a4\n                                        t \u00b5t \u2212      3\u2225\u00b5t\u22252 \u00b5t. More precisely, given independent samples {xi,t}i=1,...,n\nfrom qt generated using noise vectors {zi,t}i=1,...,n sampled from N                                       (0, Id), we have\n               1    n   Lt(s\u00b5   t(xi,t, zi,t))      \u2212     2\u00b5\u2217 t \u00b5\u2217\u22a4                                         \u221a  d\u2225\u00b5t\u22255 + 10\u2225\u00b5t\u22253             \u00b5\u2217 2 + \u03b5 .\n     \u2212   \u2207     n  i=1                                             t \u00b5t \u2212      3\u2225\u00b5t\u22252 \u00b5t            \u2264   250                                     t\nProof. Recall that the population gradient update on the DDPM objective is given by\n             \u2212\u2207Lt(s\u00b5t) = Ex\u223cN (\u00b5\u2217              t ,Id)  tanh(\u00b5\u22a4    t x)x \u2212      1               t x)\u2225\u00b5t\u22252 x + tanh\u2032(\u00b5\u22a4            t x)\u00b5\u22a4  t xx\n                                                                               2 tanh\u2032\u2032(\u00b5\u22a4\n                                       \u2212  \u00b5t \u2212    Ex\u223cN (\u00b5\u2217    t ,Id)[tanh\u2032(\u00b5\u22a4    t x)\u00b5t]\n                               = E    x\u223cN (\u00b5\u2217  t ,Id)  tanh(\u00b5\u22a4    t x)x \u2212      1               t x)\u2225\u00b5t\u22252 x + tanh\u2032(\u00b5\u22a4            t x)\u00b5\u22a4  t x\u00b5\u2217  t\n                                       + tanh\u2032\u2032(\u00b5\u22a4     t x)\u00b5\u22a4  t x\u00b5t      \u2212  \u00b5t2,tanh\u2032\u2032(\u00b5\u22a4\nwhere the last equality follows from the Stein\u2019s lemma on E                                     x\u223cN (\u00b5\u2217   t ,Id)[tanh\u2032(\u00b5\u22a4    t x)\u00b5\u22a4  t xx], as\nE x\u223cN (\u00b5\u2217   t ,Id)[tanh\u2032(\u00b5\u22a4    t x)\u00b5\u22a4  t xx] = Ex\u223cN (\u00b5\u2217         t ,Id)[tanh\u2032(\u00b5\u22a4    t x)\u00b5\u22a4  t x\u00b5\u2217  t +tanh\u2032(\u00b5\u22a4      t x)\u00b5t+tanh\u2032\u2032(\u00b5\u22a4         t x)\u00b5\u22a4  t x\u00b5t] .\nUsing Taylor\u2019s theorem, we know that\n              tanh(\u00b5\u22a4    t x) = \u00b5\u22a4    t x \u2212    2     t x)3 + O(\u03be(x)5)                     where \u03be(x) \u2208          [0, \u00b5\u22a4 t x]\n                                               3(\u00b5\u22a4\n      =\u21d2      tanh(\u00b5\u22a4x)x = \u00b5\u22a4xx \u2212                  2     t x)3x + O(\u03be(x)5x)\n                                                   3(\u00b5\u22a4\n      =\u21d2      Ex\u223cN (\u00b5\u2217     t ,Id)[tanh(\u00b5\u22a4    t x)x] \u2212      Ex\u223cN (\u00b5\u2217    t ,Id) \u00b5\u22a4 t xx \u2212     2     t x)3x        \u2264   \u2225E[\u03be(x)5x]\u2225         \u2272   \u221a  d\u2225\u00b5t\u22255\n                                                                                            3(\u00b5\u22a4\nwhere the last inequality follows\u221afrom                     E[\u03be(x)5x]           \u2264   E[|\u00b5\u22a4 t x|5\u2225x\u2225] \u2264          E[|\u00b5\u22a4  t x|10]    1/2     E[ \u2225x\u22252]      1/2 \u2272\n            d +    \u00b5\u2217 2 \u2272           d\u2225\u00b5t\u22255. Similarly, using Taylor\u2019s theorem, we get\n\u2225\u00b5t\u22255                 t\n                 tanh\u2032\u2032(\u00b5\u22a4    t x) = \u22122\u00b5\u22a4       t x + O(\u03be(x)3)                    where \u03be(x) \u2208          [0, \u00b5\u22a4t x]\n         =\u21d2      tanh\u2032\u2032(\u00b5\u22a4    t x)     \u22121 2\u2225\u00b5t\u22252x + \u00b5\u22a4        t x\u00b5t       =     \u22122\u00b5\u22a4   t x + O(\u03be(x)3)               \u22121 2\u2225\u00b5t\u22252 x + \u00b5\u22a4        t x\u00b5t\n         =\u21d2      E[tanh\u2032\u2032(\u00b5\u22a4       t x)    \u2212   1                    t x\u00b5t    ] \u2212   E    \u2212   2\u00b5\u22a4 t x     \u22121                     t x\u00b5t\n                                               2\u2225\u00b5t\u22252 x + \u00b5\u22a4                                               2\u2225\u00b5t\u22252 x + \u00b5\u22a4\n                 \u2264      \u2212   1                       t ,I)[O(\u03be(x)3)x] + Ex\u223cN (\u00b5\u2217             t ,I)[O(\u03be(x)3)\u00b5\u22a4      t x\u00b5t]\n                            2\u2225\u00b5t\u22252 Ex\u223cN (\u00b5\u2217\n                 \u2264    1                 t x|3\u2225x\u2225] +\u2225\u00b5t\u2225           E[|\u00b5\u22a4 t x|4]\n                      2\u2225\u00b5t\u22252 E[|\u00b5\u22a4\n                 \u2264    1              E[|\u00b5\u22a4  t x|6]E[   \u2225x\u22252] +\u2225\u00b5t\u2225         E[|\u00b5\u22a4  t x|4]\n                      2\u2225\u00b5t\u22252\n                 \u2264   10\u2225\u00b5t\u22255 \u221ad + 6\u2225\u00b5t\u22255\n                                                                             22", "md": "C.1 High noise regime\u2013connection to power iteration\n\nHere we show that running population gradient descent on the DDPM objective at high noise scale behaves like power iteration on the covariance matrix of the data and thus reaches an iterate \u00b5 with constant correlation with \u00b5\u2217.\n\nLemma C.3. For any noise scale t > t\u2032 and number of samples n > n\u2032 where t\u2032 \u2272 log d and n\u2032 = \u0398 d4B3, with high probability, the negative gradient of the diffusion model objective Lt(st) can be approximated by $$2\\mu^*_t \\mu^{*T}_t \\mu_t - \\frac{3}{\\|\\mu_t\\|^2} \\mu_t$$. More precisely, given independent samples {xi,t}n from qt generated using noise vectors {zi,t}n sampled from N(0, Id), we have\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i=1}^{n} L_t(s_{\\mu_t}(x_{i,t}, z_{i,t})) - 2\\mu^*_t \\mu^{*T}_t \\sqrt{d\\|\\mu_t\\|^5 + 10\\|\\mu_t\\|^3 \\mu^{*2} + \\varepsilon} \\\\\n&- \\nabla_t \\mu_t - \\frac{3}{\\|\\mu_t\\|^2} \\mu_t \\leq 250t\n\\end{align*}\n$$\nProof. Recall that the population gradient update on the DDPM objective is given by\n\n$$\n\\begin{align*}\n&-\\nabla L_t(s_{\\mu_t}) = E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x - \\frac{1}{t} \\tanh(\\mu^{T}_t x)\\|\\mu_t\\|^2 x + \\tanh'(\\mu^{T}_t x)\\mu^{T}_t xx \\\\\n&- \\mu_t - E_{x \\sim N(\\mu^*_t, Id)}[\\tanh'(\\mu^{T}_t x)\\mu_t] \\\\\n&= E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x - \\frac{1}{t} \\tanh(\\mu^{T}_t x)\\|\\mu_t\\|^2 x + \\tanh'(\\mu^{T}_t x)\\mu^{T}_t x\\mu^*_t \\\\\n&+ \\tanh''(\\mu^{T}_t x)\\mu^{T}_t x\\mu_t - \\mu_t^2\\tanh''(\\mu^{T}_t x)\n\\end{align*}\n$$\nwhere the last equality follows from the Stein\u2019s lemma on Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt xx], as Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt xx] = Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt x\\mu*t + tanh'(\\muTt x)\u00b5t+tanh''(\\muTt x)\u00b5Tt x\\mut]. Using Taylor\u2019s theorem, we know that\n\n$$\n\\begin{align*}\n&\\tanh(\\mu^{T}_t x) = \\mu^{T}_t x - \\frac{2}{3}(\\mu^{T}_t x)^3 + O(\\xi(x)^5) \\text{ where } \\xi(x) \\in [0, \\mu^{T}_t x] \\\\\n&\\Rightarrow \\tanh(\\mu^{T}_t x)x = \\mu^{T}_t xx - \\frac{2}{3}(\\mu^{T}_t x)^3x + O(\\xi(x)^5x) \\\\\n&\\Rightarrow E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x] - E_{x \\sim N(\\mu^*_t, Id)}[\\mu^{T}_t xx - \\frac{2}{3}(\\mu^{T}_t x)^3x] \\leq \\|E[\\xi(x)^5x]\\| \\leq \\sqrt{d\\|\\mu_t\\|^5}\n\\end{align*}\n$$\nwhere the last inequality follows from E[\\xi(x)^5x] \u2264 E[|\\mu^{T}_t x|^5\\|x\\|] \u2264 E[|\\mu^{T}_t x|^{10}]^{1/2} E[\\|x\\|^2]^{1/2} \u2272 d + \\mu^{*2} \u2272 d\\|\\mu_t\\|^5. Similarly, using Taylor\u2019s theorem, we get\n\n$$\n\\begin{align*}\n&\\tanh''(\\mu^{T}_t x) = -2\\mu^{T}_t x + O(\\xi(x)^3) \\text{ where } \\xi(x) \\in [0, \\mu^{T}_t x] \\\\\n&\\Rightarrow \\tanh''(\\mu^{T}_t x) - \\frac{1}{2}\\|\\mu_t\\|^2x + \\mu^{T}_t x\\mu_t = -2\\mu^{T}_t x + O(\\xi(x)^3) - \\frac{1}{2}\\|\\mu_t\\|^2 x + \\mu^{T}_t x\\mu_t \\\\\n&\\Rightarrow E[\\tanh''(\\mu^{T}_t x) - \\frac{1}{2}\\|\\mu_t\\|^2x + \\mu^{T}_t x\\mu_t] - E[-2\\mu^{T}_t x - \\frac{1}{2}\\|\\mu_t\\|^2 x + \\mu^{T}_t x\\mu_t] \\\\\n&\\leq -\\frac{1}{2}E[O(\\xi(x)^3)x] + E_{x \\sim N(\\mu^*_t, I)}[O(\\xi(x)^3)\\mu^{T}_t x\\mu_t] \\\\\n&\\leq \\frac{1}{2\\|\\mu_t\\|^2}E[|\\mu^{T}_t x|^3\\|x\\|] + \\|\\mu_t\\|E[|\\mu^{T}_t x|^4] \\\\\n&\\leq \\frac{1}{2\\|\\mu_t\\|^2}E[|\\mu^{T}_t x|^6]E[\\|x\\|^2] + \\|\\mu_t\\|E[|\\mu^{T}_t x|^4] \\\\\n&\\leq 10\\|\\mu_t\\|^5 \\sqrt{d} + 6\\|\\mu_t\\|^5\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "C.1 High noise regime\u2013connection to power iteration\n\nHere we show that running population gradient descent on the DDPM objective at high noise scale behaves like power iteration on the covariance matrix of the data and thus reaches an iterate \u00b5 with constant correlation with \u00b5\u2217.\n\nLemma C.3. For any noise scale t > t\u2032 and number of samples n > n\u2032 where t\u2032 \u2272 log d and n\u2032 = \u0398 d4B3, with high probability, the negative gradient of the diffusion model objective Lt(st) can be approximated by $$2\\mu^*_t \\mu^{*T}_t \\mu_t - \\frac{3}{\\|\\mu_t\\|^2} \\mu_t$$. More precisely, given independent samples {xi,t}n from qt generated using noise vectors {zi,t}n sampled from N(0, Id), we have\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i=1}^{n} L_t(s_{\\mu_t}(x_{i,t}, z_{i,t})) - 2\\mu^*_t \\mu^{*T}_t \\sqrt{d\\|\\mu_t\\|^5 + 10\\|\\mu_t\\|^3 \\mu^{*2} + \\varepsilon} \\\\\n&- \\nabla_t \\mu_t - \\frac{3}{\\|\\mu_t\\|^2} \\mu_t \\leq 250t\n\\end{align*}\n$$\nProof. Recall that the population gradient update on the DDPM objective is given by\n\n$$\n\\begin{align*}\n&-\\nabla L_t(s_{\\mu_t}) = E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x - \\frac{1}{t} \\tanh(\\mu^{T}_t x)\\|\\mu_t\\|^2 x + \\tanh'(\\mu^{T}_t x)\\mu^{T}_t xx \\\\\n&- \\mu_t - E_{x \\sim N(\\mu^*_t, Id)}[\\tanh'(\\mu^{T}_t x)\\mu_t] \\\\\n&= E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x - \\frac{1}{t} \\tanh(\\mu^{T}_t x)\\|\\mu_t\\|^2 x + \\tanh'(\\mu^{T}_t x)\\mu^{T}_t x\\mu^*_t \\\\\n&+ \\tanh''(\\mu^{T}_t x)\\mu^{T}_t x\\mu_t - \\mu_t^2\\tanh''(\\mu^{T}_t x)\n\\end{align*}\n$$\nwhere the last equality follows from the Stein\u2019s lemma on Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt xx], as Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt xx] = Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt x\\mu*t + tanh'(\\muTt x)\u00b5t+tanh''(\\muTt x)\u00b5Tt x\\mut]. Using Taylor\u2019s theorem, we know that\n\n$$\n\\begin{align*}\n&\\tanh(\\mu^{T}_t x) = \\mu^{T}_t x - \\frac{2}{3}(\\mu^{T}_t x)^3 + O(\\xi(x)^5) \\text{ where } \\xi(x) \\in [0, \\mu^{T}_t x] \\\\\n&\\Rightarrow \\tanh(\\mu^{T}_t x)x = \\mu^{T}_t xx - \\frac{2}{3}(\\mu^{T}_t x)^3x + O(\\xi(x)^5x) \\\\\n&\\Rightarrow E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x] - E_{x \\sim N(\\mu^*_t, Id)}[\\mu^{T}_t xx - \\frac{2}{3}(\\mu^{T}_t x)^3x] \\leq \\|E[\\xi(x)^5x]\\| \\leq \\sqrt{d\\|\\mu_t\\|^5}\n\\end{align*}\n$$\nwhere the last inequality follows from E[\\xi(x)^5x] \u2264 E[|\\mu^{T}_t x|^5\\|x\\|] \u2264 E[|\\mu^{T}_t x|^{10}]^{1/2} E[\\|x\\|^2]^{1/2} \u2272 d + \\mu^{*2} \u2272 d\\|\\mu_t\\|^5. Similarly, using Taylor\u2019s theorem, we get\n\n$$\n\\begin{align*}\n&\\tanh''(\\mu^{T}_t x) = -2\\mu^{T}_t x + O(\\xi(x)^3) \\text{ where } \\xi(x) \\in [0, \\mu^{T}_t x] \\\\\n&\\Rightarrow \\tanh''(\\mu^{T}_t x) - \\frac{1}{2}\\|\\mu_t\\|^2x + \\mu^{T}_t x\\mu_t = -2\\mu^{T}_t x + O(\\xi(x)^3) - \\frac{1}{2}\\|\\mu_t\\|^2 x + \\mu^{T}_t x\\mu_t \\\\\n&\\Rightarrow E[\\tanh''(\\mu^{T}_t x) - \\frac{1}{2}\\|\\mu_t\\|^2x + \\mu^{T}_t x\\mu_t] - E[-2\\mu^{T}_t x - \\frac{1}{2}\\|\\mu_t\\|^2 x + \\mu^{T}_t x\\mu_t] \\\\\n&\\leq -\\frac{1}{2}E[O(\\xi(x)^3)x] + E_{x \\sim N(\\mu^*_t, I)}[O(\\xi(x)^3)\\mu^{T}_t x\\mu_t] \\\\\n&\\leq \\frac{1}{2\\|\\mu_t\\|^2}E[|\\mu^{T}_t x|^3\\|x\\|] + \\|\\mu_t\\|E[|\\mu^{T}_t x|^4] \\\\\n&\\leq \\frac{1}{2\\|\\mu_t\\|^2}E[|\\mu^{T}_t x|^6]E[\\|x\\|^2] + \\|\\mu_t\\|E[|\\mu^{T}_t x|^4] \\\\\n&\\leq 10\\|\\mu_t\\|^5 \\sqrt{d} + 6\\|\\mu_t\\|^5\n\\end{align*}\n$$", "md": "C.1 High noise regime\u2013connection to power iteration\n\nHere we show that running population gradient descent on the DDPM objective at high noise scale behaves like power iteration on the covariance matrix of the data and thus reaches an iterate \u00b5 with constant correlation with \u00b5\u2217.\n\nLemma C.3. For any noise scale t > t\u2032 and number of samples n > n\u2032 where t\u2032 \u2272 log d and n\u2032 = \u0398 d4B3, with high probability, the negative gradient of the diffusion model objective Lt(st) can be approximated by $$2\\mu^*_t \\mu^{*T}_t \\mu_t - \\frac{3}{\\|\\mu_t\\|^2} \\mu_t$$. More precisely, given independent samples {xi,t}n from qt generated using noise vectors {zi,t}n sampled from N(0, Id), we have\n\n$$\n\\begin{align*}\n&\\frac{1}{n} \\sum_{i=1}^{n} L_t(s_{\\mu_t}(x_{i,t}, z_{i,t})) - 2\\mu^*_t \\mu^{*T}_t \\sqrt{d\\|\\mu_t\\|^5 + 10\\|\\mu_t\\|^3 \\mu^{*2} + \\varepsilon} \\\\\n&- \\nabla_t \\mu_t - \\frac{3}{\\|\\mu_t\\|^2} \\mu_t \\leq 250t\n\\end{align*}\n$$\nProof. Recall that the population gradient update on the DDPM objective is given by\n\n$$\n\\begin{align*}\n&-\\nabla L_t(s_{\\mu_t}) = E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x - \\frac{1}{t} \\tanh(\\mu^{T}_t x)\\|\\mu_t\\|^2 x + \\tanh'(\\mu^{T}_t x)\\mu^{T}_t xx \\\\\n&- \\mu_t - E_{x \\sim N(\\mu^*_t, Id)}[\\tanh'(\\mu^{T}_t x)\\mu_t] \\\\\n&= E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x - \\frac{1}{t} \\tanh(\\mu^{T}_t x)\\|\\mu_t\\|^2 x + \\tanh'(\\mu^{T}_t x)\\mu^{T}_t x\\mu^*_t \\\\\n&+ \\tanh''(\\mu^{T}_t x)\\mu^{T}_t x\\mu_t - \\mu_t^2\\tanh''(\\mu^{T}_t x)\n\\end{align*}\n$$\nwhere the last equality follows from the Stein\u2019s lemma on Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt xx], as Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt xx] = Ex \u223c N(\u00b5*t, Id)[tanh'(\\muTt x)\u00b5Tt x\\mu*t + tanh'(\\muTt x)\u00b5t+tanh''(\\muTt x)\u00b5Tt x\\mut]. Using Taylor\u2019s theorem, we know that\n\n$$\n\\begin{align*}\n&\\tanh(\\mu^{T}_t x) = \\mu^{T}_t x - \\frac{2}{3}(\\mu^{T}_t x)^3 + O(\\xi(x)^5) \\text{ where } \\xi(x) \\in [0, \\mu^{T}_t x] \\\\\n&\\Rightarrow \\tanh(\\mu^{T}_t x)x = \\mu^{T}_t xx - \\frac{2}{3}(\\mu^{T}_t x)^3x + O(\\xi(x)^5x) \\\\\n&\\Rightarrow E_{x \\sim N(\\mu^*_t, Id)}[\\tanh(\\mu^{T}_t x)x] - E_{x \\sim N(\\mu^*_t, Id)}[\\mu^{T}_t xx - \\frac{2}{3}(\\mu^{T}_t x)^3x] \\leq \\|E[\\xi(x)^5x]\\| \\leq \\sqrt{d\\|\\mu_t\\|^5}\n\\end{align*}\n$$\nwhere the last inequality follows from E[\\xi(x)^5x] \u2264 E[|\\mu^{T}_t x|^5\\|x\\|] \u2264 E[|\\mu^{T}_t x|^{10}]^{1/2} E[\\|x\\|^2]^{1/2} \u2272 d + \\mu^{*2} \u2272 d\\|\\mu_t\\|^5. Similarly, using Taylor\u2019s theorem, we get\n\n$$\n\\begin{align*}\n&\\tanh''(\\mu^{T}_t x) = -2\\mu^{T}_t x + O(\\xi(x)^3) \\text{ where } \\xi(x) \\in [0, \\mu^{T}_t x] \\\\\n&\\Rightarrow \\tanh''(\\mu^{T}_t x) - \\frac{1}{2}\\|\\mu_t\\|^2x + \\mu^{T}_t x\\mu_t = -2\\mu^{T}_t x + O(\\xi(x)^3) - \\frac{1}{2}\\|\\mu_t\\|^2 x + \\mu^{T}_t x\\mu_t \\\\\n&\\Rightarrow E[\\tanh''(\\mu^{T}_t x) - \\frac{1}{2}\\|\\mu_t\\|^2x + \\mu^{T}_t x\\mu_t] - E[-2\\mu^{T}_t x - \\frac{1}{2}\\|\\mu_t\\|^2 x + \\mu^{T}_t x\\mu_t] \\\\\n&\\leq -\\frac{1}{2}E[O(\\xi(x)^3)x] + E_{x \\sim N(\\mu^*_t, I)}[O(\\xi(x)^3)\\mu^{T}_t x\\mu_t] \\\\\n&\\leq \\frac{1}{2\\|\\mu_t\\|^2}E[|\\mu^{T}_t x|^3\\|x\\|] + \\|\\mu_t\\|E[|\\mu^{T}_t x|^4] \\\\\n&\\leq \\frac{1}{2\\|\\mu_t\\|^2}E[|\\mu^{T}_t x|^6]E[\\|x\\|^2] + \\|\\mu_t\\|E[|\\mu^{T}_t x|^4] \\\\\n&\\leq 10\\|\\mu_t\\|^5 \\sqrt{d} + 6\\|\\mu_t\\|^5\n\\end{align*}\n$$"}]}, {"page": 23, "text": "Using Taylor\u2019s theorem for tanh\u2032, we get\n                 tanh\u2032(\u00b5\u22a4      t x) = 1 \u2212          (\u00b5\u22a4 t x)2 + O(\u03be(x)4)                        where \u03be(x) \u2208             [0, \u00b5\u22a4  t x]\n         =\u21d2      tanh\u2032(\u00b5\u22a4      t x)\u00b5\u22a4   t x\u00b5\u2217   t = \u00b5\u22a4    t x\u00b5\u2217  t \u2212    (\u00b5\u22a4 t x)3\u00b5\u2217     t + O(\u03be(x)4\u00b5\u22a4          t x\u00b5\u2217   t )           where \u03be(x) \u2208             [0, \u00b5\u22a4  t x]\n         =\u21d2       E[tanh\u2032(\u00b5\u22a4         t x)\u00b5\u22a4   t x\u00b5\u2217  t ] \u2212    E[\u00b5\u22a4  t x\u00b5\u2217   t \u2212    (\u00b5\u22a4 t x)3\u00b5\u2217    t ]   \u2264 E[\u03be(x)4(\u00b5\u22a4            t x)\u00b5\u2217   t ]\nAdditionally, we have                                                                        \u2264   E[|\u00b5\u22a4   t x|5]    \u00b5\u2217  t    \u2272    \u00b5\u2217  t  \u2225\u00b5t\u22255\n                     E  x\u223cN (\u00b5\u2217    t ,Id)[xx\u22a4\u00b5t(1 +\u2225\u00b5t\u22252) \u2212                     2     t x)3x \u2212        2\u00b5t(\u00b5\u22a4     t x)2 + \u00b5\u22a4      t x\u00b5\u2217  t \u2212    (\u00b5\u22a4 t x)3\u00b5\u2217     t ]\n                                                                                3(\u00b5\u22a4\n                  = (I + \u00b5\u2217       t \u00b5\u2217\u22a4\n                                      t )\u00b5t(1 +\u2225\u00b5t\u22252) \u2212                  5          t x)3\u00b5\u2217    t ] + \u00b5\u2217   t \u00b5\u2217\u22a4\n                                                                         3E[(\u00b5\u22a4                               t \u00b5t \u2212       4E[\u00b5t(\u00b5\u22a4       t x)2]\n                  = (I + \u00b5\u2217       t \u00b5\u2217\u22a4                                       t       t \u00b5\u2217 t )3 + 3(\u00b5\u22a4      t \u00b5\u2217 t )\u2225\u00b5t\u22252)\n                                      t )\u00b5t(1 +\u2225\u00b5t\u22252) \u2212                  5\u00b5\u22173 ((\u00b5\u22a4\n                  = \u00b5\u2217   t \u00b5\u2217\u22a4 + \u00b5\u2217  t \u00b5\u2217\u22a4t \u00b5t \u2212       4\u00b5t(\u2225\u00b5t\u22252 + (\u00b5\u22a4            t \u00b5\u2217  t)2)          t (\u00b5\u22a4 t \u00b5\u2217 t )3   \u2212   4\u00b5t(\u00b5\u22a4    t \u00b5\u2217 t )2\n                             t \u00b5t(2 \u2212          4\u2225\u00b5t\u22252) + \u00b5t(1 \u2212               3\u2225\u00b5t\u22252) \u2212          5\u00b5\u2217       3\nwhere the second equality uses Stein\u2019s lemma on E[(\u00b5\u22a4                                                                                                 t     and the third\n                                                                                                t x)3x] and E[xx\u22a4] = Id+\u00b5\u2217                        t \u00b5\u2217\u22a4\nequality uses Gaussian moments for E[(\u00b5\u22a4                                     t x)2] and E[(\u00b5\u22a4            t x)3]. Putting it all together and using\ntriangle inequality, we obtain the desired bound on \u2225                                            \u2212    \u2207Lt(s\u00b5t) \u2212           (2\u00b5\u2217  t \u00b5\u2217\u22a4t \u00b5t \u2212       3\u2225\u00b5t\u22252 \u00b5t)\u2225.\n          \u2225  \u2212   \u2207Lt(s\u00b5t) \u2212           (2\u00b5\u2217  t \u00b5\u2217\u22a4t \u00b5t \u2212       3\u2225\u00b5t\u22252 \u00b5t)\u2225\n      \u2264       \u2212   \u2207Lt(s\u00b5t) \u2212            E[xx\u22a4\u00b5t(1 +\u2225\u00b5t\u22252) \u2212                     2      t x)3x \u2212        2\u00b5t(\u00b5\u22a4    t x)2 + \u00b5\u22a4      t x\u00b5\u2217   t \u2212    (\u00b5\u22a4 t x)3\u00b5\u2217    t \u2212    \u00b5t]\n                                                                                3(\u00b5\u22a4\n           +    E[xx\u22a4\u00b5t(1 +\u2225\u00b5t\u22252) \u2212                       2     t x)3x \u2212        2\u00b5t(\u00b5\u22a4    t x)2 + \u00b5\u22a4       t x\u00b5\u2217  t \u2212    (\u00b5\u22a4 t x)3\u00b5\u2217    t \u2212    \u00b5t]\n                   \u2212      2\u00b5\u2217 t \u00b5\u2217\u22a4                       3(\u00b5\u22a4\n                                  t \u00b5t \u2212        3\u2225\u00b5t\u22252 \u00b5t\n      \u2264       200   \u221a   d\u2225\u00b5t\u22255 + 10\u2225\u00b5t\u22255 \u221ad + 6\u2225\u00b5t\u22255 + 20                                \u00b5\u2217 \u2225\u00b5t\u22255             + 10\u2225\u00b5t\u22253          \u00b5\u2217 2\n                                                                                             t                                       t\n      \u2264    250   \u221a   d\u2225\u00b5t\u22255 + 10\u2225\u00b5t\u22253                \u00b5\u2217 2t\nUsing Lemma E.7 and triangle inequality, we obtain the result.\nWe will use the following simple bound on the correlation between the ground truth and a random\ninitialization:\nLemma C.4. A randomly initialized \u00b50 \u223c                                         N   (0, Id) satisfies that               \u27e8\u02c6\u00b50, \u02c6          \u2265     1\nleast 1 \u2212        O(d\u22121/2).                                                                                                      \u00b5\u2217\u27e9           2d with probability at\nProof. For \u00b50 \u223c               N   (0, I), we know that \u27e8\u00b50, \u02c6                  \u00b5\u2217\u27e9    \u223c    N   (0, I). Using Gaussian anti-concentration, with\nprobability at least 1\u22121/                     \u221a  d , we have          \u27e8\u00b50, \u02c6   \u00b5\u2217\u27e9     \u2265    1/  \u221a  d. Because the L2 norm of a Gaussian vector \u221a\nis sub-exponential, with probability at least 1 \u2212                        \u221a              exp(\u2212\u2126(d)), we have \u2225\u00b50\u2225                        \u2264    2    d. Using the norm\nbound, with probability at least 1 \u2212                                 1/     d \u2212     exp(\u2212O(d)) = 1 \u2212                    O(d\u22121/2), we obtain the claimed\n                   \u27e8  \u02c6           .\nbound on             \u00b50, \u02c6 \u00b5\u2217\u27e9\nWe can now track the correlation between the iterates of gradient descent and the ground truth:\n                                                                                        23", "md": "# Math Equations\n\nUsing Taylor\u2019s theorem for $$\\tanh'$$, we get\n\n$$\n\\begin{align*}\n\\tanh'(\\mu^{\\top} t x) &= 1 - (\\mu^{\\top} t x)^2 + O(\\xi(x)^4) \\quad \\text{where } \\xi(x) \\in [0, \\mu^{\\top} t x] \\\\\n&\\Rightarrow \\tanh'(\\mu^{\\top} t x)\\mu^{\\top} t x\\mu^{*} t = \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t + O(\\xi(x)^4\\mu^{\\top} t x\\mu^{*} t) \\quad \\text{where } \\xi(x) \\in [0, \\mu^{\\top} t x] \\\\\n&\\Rightarrow E[\\tanh'(\\mu^{\\top} t x)\\mu^{\\top} t x\\mu^{*} t] - E[\\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t] \\leq E[\\xi(x)^4(\\mu^{\\top} t x)\\mu^{*} t]\n\\end{align*}\n$$\n\nAdditionally, we have\n\n$$\n\\begin{align*}\n&\\leq E[|\\mu^{\\top} t x|^5]\\mu^{*} t \\lesssim \\mu^{*} t \\lVert \\mu_t \\rVert^5 \\\\\n&\\begin{aligned}\nE[x \\sim N(\\mu^{*} t, Id)][xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t] \\\\\n&= (I + \\mu^{*} t \\mu^{*\\top} t)\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 5 t x)^3\\mu^{*} t] + \\mu^{*} t \\mu^{*\\top} t \\mu_t(2 - 4\\lVert \\mu_t \\rVert^2) + \\mu_t(1 - 3\\lVert \\mu_t \\rVert^2) - 5\\mu^{*} t^3\n\\end{aligned}\n\\end{align*}\n$$\n\nWhere the second equality uses Stein\u2019s lemma on E[(\\mu^{\\top} t x)^3x] and the third equality uses Gaussian moments for E[(\\mu^{\\top} t x)^2] and E[(\\mu^{\\top} t x)^3]. Putting it all together and using triangle inequality, we obtain the desired bound on $$\\lVert - \\nabla L_t(s\\mu_t) - (2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\rVert$$.\n\n$$\n\\begin{align*}\n\\lVert - \\nabla L_t(s\\mu_t) - (2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\rVert &\\leq - \\nabla L_t(s\\mu_t) - E[xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t - \\mu_t] \\\\\n&+ E[xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t - \\mu_t] \\\\\n&- 2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3( \\mu^{\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\\\\n&\\leq 200 \\sqrt{d}\\lVert \\mu_t \\rVert^5 + 10\\lVert \\mu_t \\rVert^5 \\sqrt{d} + 6\\lVert \\mu_t \\rVert^5 + 20\\mu^{*} \\lVert \\mu_t \\rVert^5 + 10\\lVert \\mu_t \\rVert^3 \\mu^{*2} t\n\\end{align*}\n$$\n\nUsing Lemma E.7 and triangle inequality, we obtain the result.\n\nWe will use the following simple bound on the correlation between the ground truth and a random initialization:\n\nLemma C.4. A randomly initialized $$\\mu_0 \\sim N(0, Id)$$ satisfies that $$\\langle \\hat{\\mu}_0, \\hat{\\mu}^{*} \\rangle \\geq 1 - O(d^{-1/2})$$ with probability at least $$1 - O(d^{-1/2})$$.\n\nProof. For $$\\mu_0 \\sim N(0, I)$$, we know that $$\\langle \\mu_0, \\hat{\\mu}^{*} \\rangle \\sim N(0, I)$$. Using Gaussian anti-concentration, with probability at least $$1 - 1/\\sqrt{d}$$, we have $$\\langle \\mu_0, \\hat{\\mu}^{*} \\rangle \\geq 1/\\sqrt{d}$$. Because the L2 norm of a Gaussian vector is sub-exponential, with probability at least $$1 - \\exp(-\\Omega(d))$$, we have $$\\lVert \\mu_0 \\rVert \\leq 2\\sqrt{d}$$. Using the norm bound, with probability at least $$1 - 1/\\sqrt{d} - \\exp(-O(d)) = 1 - O(d^{-1/2})$$, we obtain the claimed bound on $$\\langle \\hat{\\mu}_0, \\hat{\\mu}^{*} \\rangle$$.\n\nWe can now track the correlation between the iterates of gradient descent and the ground truth:\n\n$$23$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Using Taylor\u2019s theorem for $$\\tanh'$$, we get\n\n$$\n\\begin{align*}\n\\tanh'(\\mu^{\\top} t x) &= 1 - (\\mu^{\\top} t x)^2 + O(\\xi(x)^4) \\quad \\text{where } \\xi(x) \\in [0, \\mu^{\\top} t x] \\\\\n&\\Rightarrow \\tanh'(\\mu^{\\top} t x)\\mu^{\\top} t x\\mu^{*} t = \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t + O(\\xi(x)^4\\mu^{\\top} t x\\mu^{*} t) \\quad \\text{where } \\xi(x) \\in [0, \\mu^{\\top} t x] \\\\\n&\\Rightarrow E[\\tanh'(\\mu^{\\top} t x)\\mu^{\\top} t x\\mu^{*} t] - E[\\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t] \\leq E[\\xi(x)^4(\\mu^{\\top} t x)\\mu^{*} t]\n\\end{align*}\n$$\n\nAdditionally, we have\n\n$$\n\\begin{align*}\n&\\leq E[|\\mu^{\\top} t x|^5]\\mu^{*} t \\lesssim \\mu^{*} t \\lVert \\mu_t \\rVert^5 \\\\\n&\\begin{aligned}\nE[x \\sim N(\\mu^{*} t, Id)][xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t] \\\\\n&= (I + \\mu^{*} t \\mu^{*\\top} t)\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 5 t x)^3\\mu^{*} t] + \\mu^{*} t \\mu^{*\\top} t \\mu_t(2 - 4\\lVert \\mu_t \\rVert^2) + \\mu_t(1 - 3\\lVert \\mu_t \\rVert^2) - 5\\mu^{*} t^3\n\\end{aligned}\n\\end{align*}\n$$\n\nWhere the second equality uses Stein\u2019s lemma on E[(\\mu^{\\top} t x)^3x] and the third equality uses Gaussian moments for E[(\\mu^{\\top} t x)^2] and E[(\\mu^{\\top} t x)^3]. Putting it all together and using triangle inequality, we obtain the desired bound on $$\\lVert - \\nabla L_t(s\\mu_t) - (2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\rVert$$.\n\n$$\n\\begin{align*}\n\\lVert - \\nabla L_t(s\\mu_t) - (2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\rVert &\\leq - \\nabla L_t(s\\mu_t) - E[xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t - \\mu_t] \\\\\n&+ E[xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t - \\mu_t] \\\\\n&- 2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3( \\mu^{\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\\\\n&\\leq 200 \\sqrt{d}\\lVert \\mu_t \\rVert^5 + 10\\lVert \\mu_t \\rVert^5 \\sqrt{d} + 6\\lVert \\mu_t \\rVert^5 + 20\\mu^{*} \\lVert \\mu_t \\rVert^5 + 10\\lVert \\mu_t \\rVert^3 \\mu^{*2} t\n\\end{align*}\n$$\n\nUsing Lemma E.7 and triangle inequality, we obtain the result.\n\nWe will use the following simple bound on the correlation between the ground truth and a random initialization:\n\nLemma C.4. A randomly initialized $$\\mu_0 \\sim N(0, Id)$$ satisfies that $$\\langle \\hat{\\mu}_0, \\hat{\\mu}^{*} \\rangle \\geq 1 - O(d^{-1/2})$$ with probability at least $$1 - O(d^{-1/2})$$.\n\nProof. For $$\\mu_0 \\sim N(0, I)$$, we know that $$\\langle \\mu_0, \\hat{\\mu}^{*} \\rangle \\sim N(0, I)$$. Using Gaussian anti-concentration, with probability at least $$1 - 1/\\sqrt{d}$$, we have $$\\langle \\mu_0, \\hat{\\mu}^{*} \\rangle \\geq 1/\\sqrt{d}$$. Because the L2 norm of a Gaussian vector is sub-exponential, with probability at least $$1 - \\exp(-\\Omega(d))$$, we have $$\\lVert \\mu_0 \\rVert \\leq 2\\sqrt{d}$$. Using the norm bound, with probability at least $$1 - 1/\\sqrt{d} - \\exp(-O(d)) = 1 - O(d^{-1/2})$$, we obtain the claimed bound on $$\\langle \\hat{\\mu}_0, \\hat{\\mu}^{*} \\rangle$$.\n\nWe can now track the correlation between the iterates of gradient descent and the ground truth:\n\n$$23$$", "md": "Using Taylor\u2019s theorem for $$\\tanh'$$, we get\n\n$$\n\\begin{align*}\n\\tanh'(\\mu^{\\top} t x) &= 1 - (\\mu^{\\top} t x)^2 + O(\\xi(x)^4) \\quad \\text{where } \\xi(x) \\in [0, \\mu^{\\top} t x] \\\\\n&\\Rightarrow \\tanh'(\\mu^{\\top} t x)\\mu^{\\top} t x\\mu^{*} t = \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t + O(\\xi(x)^4\\mu^{\\top} t x\\mu^{*} t) \\quad \\text{where } \\xi(x) \\in [0, \\mu^{\\top} t x] \\\\\n&\\Rightarrow E[\\tanh'(\\mu^{\\top} t x)\\mu^{\\top} t x\\mu^{*} t] - E[\\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t] \\leq E[\\xi(x)^4(\\mu^{\\top} t x)\\mu^{*} t]\n\\end{align*}\n$$\n\nAdditionally, we have\n\n$$\n\\begin{align*}\n&\\leq E[|\\mu^{\\top} t x|^5]\\mu^{*} t \\lesssim \\mu^{*} t \\lVert \\mu_t \\rVert^5 \\\\\n&\\begin{aligned}\nE[x \\sim N(\\mu^{*} t, Id)][xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t] \\\\\n&= (I + \\mu^{*} t \\mu^{*\\top} t)\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 5 t x)^3\\mu^{*} t] + \\mu^{*} t \\mu^{*\\top} t \\mu_t(2 - 4\\lVert \\mu_t \\rVert^2) + \\mu_t(1 - 3\\lVert \\mu_t \\rVert^2) - 5\\mu^{*} t^3\n\\end{aligned}\n\\end{align*}\n$$\n\nWhere the second equality uses Stein\u2019s lemma on E[(\\mu^{\\top} t x)^3x] and the third equality uses Gaussian moments for E[(\\mu^{\\top} t x)^2] and E[(\\mu^{\\top} t x)^3]. Putting it all together and using triangle inequality, we obtain the desired bound on $$\\lVert - \\nabla L_t(s\\mu_t) - (2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\rVert$$.\n\n$$\n\\begin{align*}\n\\lVert - \\nabla L_t(s\\mu_t) - (2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\rVert &\\leq - \\nabla L_t(s\\mu_t) - E[xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t - \\mu_t] \\\\\n&+ E[xx^{\\top}\\mu_t(1 + \\lVert \\mu_t \\rVert^2) - 2 t x)^3x - 2\\mu_t(\\mu^{\\top} t x)^2 + \\mu^{\\top} t x\\mu^{*} t - (\\mu^{\\top} t x)^3\\mu^{*} t - \\mu_t] \\\\\n&- 2\\mu^{*} t \\mu^{*\\top} t \\mu_t - 3( \\mu^{\\top} t \\mu_t - 3\\lVert \\mu_t \\rVert^2 \\mu_t) \\\\\n&\\leq 200 \\sqrt{d}\\lVert \\mu_t \\rVert^5 + 10\\lVert \\mu_t \\rVert^5 \\sqrt{d} + 6\\lVert \\mu_t \\rVert^5 + 20\\mu^{*} \\lVert \\mu_t \\rVert^5 + 10\\lVert \\mu_t \\rVert^3 \\mu^{*2} t\n\\end{align*}\n$$\n\nUsing Lemma E.7 and triangle inequality, we obtain the result.\n\nWe will use the following simple bound on the correlation between the ground truth and a random initialization:\n\nLemma C.4. A randomly initialized $$\\mu_0 \\sim N(0, Id)$$ satisfies that $$\\langle \\hat{\\mu}_0, \\hat{\\mu}^{*} \\rangle \\geq 1 - O(d^{-1/2})$$ with probability at least $$1 - O(d^{-1/2})$$.\n\nProof. For $$\\mu_0 \\sim N(0, I)$$, we know that $$\\langle \\mu_0, \\hat{\\mu}^{*} \\rangle \\sim N(0, I)$$. Using Gaussian anti-concentration, with probability at least $$1 - 1/\\sqrt{d}$$, we have $$\\langle \\mu_0, \\hat{\\mu}^{*} \\rangle \\geq 1/\\sqrt{d}$$. Because the L2 norm of a Gaussian vector is sub-exponential, with probability at least $$1 - \\exp(-\\Omega(d))$$, we have $$\\lVert \\mu_0 \\rVert \\leq 2\\sqrt{d}$$. Using the norm bound, with probability at least $$1 - 1/\\sqrt{d} - \\exp(-O(d)) = 1 - O(d^{-1/2})$$, we obtain the claimed bound on $$\\langle \\hat{\\mu}_0, \\hat{\\mu}^{*} \\rangle$$.\n\nWe can now track the correlation between the iterates of gradient descent and the ground truth:\n\n$$23$$"}]}, {"page": 24, "text": "                                                                                                        1\nLemma C.5. Suppose that the vector \u00b5t satisfies |\u27e8\u02c6                                   \u00b5t, \u02c6\u00b5\u2217t \u27e9| \u2265    2d, and let \u00b5\u2032        t denote the iterate\nresulting from a single empirical gradient step with learning rate \u03b7 starting from \u00b5t. Suppose that\nthe empirical gradient and the population gradient diff                              er by at most \u03b5. Denote the angle between \u00b5t\n(resp. \u00b5\u2032    t) and \u00b5\u2217    t by \u03b8 (resp. \u03b8\u2032). Then\n                                                          tan \u03b8\u2032 = max (\u03ba1 tan \u03b8, \u03ba2)\nfor\n                       \u03ba1 =                                             1 \u2212  \u221a3\u03b7\u2225\u00b5t\u22252                                               ,\n                                1 \u2212    3\u03b7\u2225\u00b5t\u22252 + \u03b7(\u2225\u00b5\u2217         t \u22252 \u2212   500     d3\u2225\u00b5t\u22254 \u2212        20d\u2225\u00b5t\u22252\u2225\u00b5\u2217       t \u22252 \u2212   \u03b7\u02dc\u03b5)\n                       \u03ba2 = 500\u03b7        \u221a  d3\u2225\u00b5t\u22254 + 20\u03b7d\u2225\u00b5t\u22252\u2225\u00b5\u2217              t \u22252 + \u03b7\u02dc  \u03b5    and \u02dc  \u03b5 \u2272      d\u03b5\n                                                          \u2225\u00b5\u2217 t \u22252                                           \u2225\u00b5t\u2225    .\nProof. Defi       ne \u02c6 \u00b5\u2217\u22a5    as the orthogonal vector to \u00b5\u2217                 t in the plane of \u00b5t and \u00b5\u2217              t . Note that \u00b5\u2032       t still lies\n                         t\nin this plane, so the orthogonal vector to \u00b5\u2217                       t in the plane of \u00b5\u2032         t and \u00b5\u2217    t is also given by \u02c6        \u00b5\u2217\u22a5\n                                                                                                                                           t .\n      We have\n                         \u00b5\u2217\u22a5, \u02c6  \u00b5\u2032t\u27e9        \u00b5\u2217\u22a5      t\u27e9\n          tan \u03b8\u2032 = \u27e8\u02c6    \u27e8\u02c6                    t , \u00b5\u2032\n                          \u00b5\u2217    \u00b5\u2032          \u27e8\u02c6\u00b5\u2217\n                             t, \u02c6 t\u27e9   = \u27e8\u02c6     t, \u00b5\u2032t\u27e9\n                =\u27e8\u02c6 \u00b5\u2217\u22a5t , \u00b5t + \u03b7F(\u00b5t, \u00b5\u2217        t )\u27e9 + \u27e8\u02c6 \u00b5\u2217\u22a5t , \u2212\u03b7\u2207Lt(st) \u2212           \u03b7F(\u00b5t, \u00b5\u2217     t)\u27e9  + \u03b7\u03b5\n                    \u27e8\u02c6\u00b5\u2217t, \u00b5t + \u03b7F(\u00b5t, \u00b5\u2217                 \u00b5\u2217\u22a5\n                                                t )\u27e9 + \u27e8\u02c6t , \u2212\u03b7\u2207Lt(st) \u2212               \u03b7F(\u00b5t, \u00b5\u2217     t )\u27e9 \u2212  \u03b7\u03b5\n                                                                            where         F(\u00b5, \u00b5\u2217) =          2\u00b5\u2217 t \u00b5\u2217\u22a4\n                        \u00b5\u2217\u22a5                 \u2207Lt(st) + F(\u00b5t, \u00b5\u2217                 + \u03b7\u03b5                                   t \u00b5t \u2212      3\u2225\u00b5t\u22252 \u00b5t\n                \u2264\u03c32\u27e8\u02c6     t , \u00b5t\u27e9    + \u03b7   \u2207Lt(st) + F(\u00b5t, \u00b5\u2217            t )  \u2212  \u03b7\u03b5                                                                 (C.1)\n                    \u03c31\u27e8\u02c6 \u00b5\u2217t , \u00b5t\u27e9  \u2212   \u03b7                               t )\nwhere \u03c31 and \u03c32 are the fi               rst and second eigenvalues of Id + F(\u00b5t, \u00b5\u2217                      t ) = (1 \u2212     3\u03b7\u2225\u00b5t\u22252)Id + 2\u03b7\u00b5\u2217          t\u00b5\u2217\u22a4t ,\ngiven by\n                                                        \u03c31 = 1 + \u03b7(2\u2225\u00b5\u2217        t \u22252 \u2212    3\u2225\u00b5t\u22252)\nThe last inequality (C.1) follows from the fact that    \u03c32 = 1 \u2212      3\u03b7\u2225\u00b5t\u22252 .\n                        \u27e8\u02c6\u00b5\u2217t , \u00b5t + \u03b7F(\u00b5t, \u00b5\u2217               \u00b5\u2217\u22a4\n                                                    t )\u27e9  = \u02c6t ((1 \u2212        3\u03b7\u2225\u00b5t\u22252)Id + 2\u03b7\u00b5\u2217           t \u00b5\u2217\u22a4\n                                                                                                            t )\u00b5t\n                                                          = \u00b5\u22a4                                        t \u00b5\u2217\u22a4    \u00b5\u2217               \u00b5\u2217\n                                                                t ((1 \u2212    3\u03b7\u2225\u00b5t\u22252)Id + 2\u03b7\u00b5\u2217              t )\u02c6t = \u03c31\u00b5\u22a4        t \u02c6t\nbecause \u02c6     \u00b5\u2217   is the fi   rst eigenvector of (1 \u2212             3\u03b7\u2225\u00b5t\u22252)Id + 2\u03b7\u00b5\u2217          t \u00b5\u2217\u22a4\ndeviation between the negative population gradient and the power iteration update F(\u00b5t, \u00b5\u2217        t . Recall from Lemma C.3 that the                 t ) is\nbounded by\n     \u2207Lt(st) + F(\u00b5t, \u00b5\u2217           t)    \u2264   250\u03b7    \u221a  d\u2225\u00b5t\u22254 + 10\u03b7\u2225\u00b5t\u22252\u2225\u00b5\u2217             t\u22252   \u2264   500\u03b7   \u221a  d3\u2225\u00b5t\u22254 + 20d\u03b7\u2225\u00b5t\u22252               \u00b5\u2217 t  2 .\n                \u27e8\u00b5t, \u02c6\u00b5\u2217                                       \u27e8\u02c6\n                        t \u27e9                                     \u00b5t, \u02c6\u00b5\u2217t \u27e9\n                                                                            24", "md": "Lemma C.5. Suppose that the vector \u00b5t satisfies |\u27e8\u02c6 \u00b5t, \u02c6\u00b5\u2217t \u27e9| \u2265 2d, and let \u00b5\u2032 t denote the iterate resulting from a single empirical gradient step with learning rate \u03b7 starting from \u00b5t. Suppose that the empirical gradient and the population gradient differ by at most \u03b5. Denote the angle between \u00b5t (resp. \u00b5\u2032 t) and \u00b5\u2217 t by \u03b8 (resp. \u03b8\u2032). Then\n\n$$\n\\tan \\theta' = \\max (\\kappa_1 \\tan \\theta, \\kappa_2)\n$$\nfor\n\n$$\n\\begin{align*}\n\\kappa_1 &= \\frac{1 - \\sqrt{3}\\eta\\| \\mu_t \\|^2}{1 - 3\\eta\\| \\mu_t \\| ^2 + \\eta(\\| \\mu^*_t \\| ^2 - 500d^3\\| \\mu_t \\| ^4 - 20d\\| \\mu_t \\| ^2\\| \\mu^*_t \\| ^2 - \\eta\\tilde{\\varepsilon})} \\\\\n\\kappa_2 &= 500\\eta \\sqrt{d^3\\| \\mu_t \\| ^4 + 20\\eta d\\| \\mu_t \\| ^2\\| \\mu^*_t \\| ^2 + \\eta\\tilde{\\varepsilon}} \\quad \\text{and} \\quad \\tilde{\\varepsilon} \\lesssim d\\varepsilon\n\\end{align*}\n$$\nProof. Define \u02c6 \u00b5\u2217\u22a5 as the orthogonal vector to \u00b5\u2217 t in the plane of \u00b5t and \u00b5\u2217 t. Note that \u00b5\u2032 t still lies in this plane, so the orthogonal vector to \u00b5\u2217 t in the plane of \u00b5\u2032 t and \u00b5\u2217 t is also given by \u02c6 \u00b5\u2217\u22a5 t. We have\n\n$$\n\\begin{align*}\n\\tan \\theta' &= \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5\u2032 t \\rangle = \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5\u2032 t \\rangle \\\\\n&= \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\langle \u02c6 \u00b5\u2217\u22a5 t, -\\eta \\nabla L_t(s_t) - \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\eta \\varepsilon \\\\\n&= \\langle \u02c6 \u00b5\u2217 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\langle \u02c6 t, -\\eta \\nabla L_t(s_t) - \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle - \\eta \\varepsilon\n\\end{align*}\n$$\nwhere $F(\u00b5, \u00b5\u2217) = 2\u00b5\u2217 t \u00b5\u2217\u22a4$\n\n$$\n\\begin{align*}\n\\mu\u2217\u22a5, \\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) + \\eta \\varepsilon &\\leq \\sigma^2 \\langle \u02c6 t, \u00b5t \\rangle + \\eta \\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) - \\eta \\varepsilon \\quad (C.1) \\\\\n&\\leq \\sigma_1 \\langle \u02c6 \u00b5\u2217 t, \u00b5t \\rangle - \\eta t)\n\\end{align*}\n$$\nwhere \u03c31 and \u03c32 are the first and second eigenvalues of Id + F(\u00b5t, \u00b5\u2217 t) = (1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4 t, given by\n\n$$\n\\begin{align*}\n\\sigma_1 &= 1 + \\eta(2\\| \\mu\u2217 t \\| ^2 - 3\\| \\mu_t \\| ^2)\n\\end{align*}\n$$\nThe last inequality (C.1) follows from the fact that $\\sigma_2 = 1 - 3\u03b7\\| \\mu_t \\| ^2$.\n\n$$\n\\begin{align*}\n\\langle \u02c6 \u00b5\u2217 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle &= \u02c6 t ((1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4 t)\u00b5t \\\\\n&= \u00b5\u22a4 t \u00b5\u2217\u22a4 t \u00b5\u2217 t ((1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t)\u02c6 t = \u03c31\u00b5\u22a4 t \u02c6 t\n\\end{align*}\n$$\nbecause \u02c6 \u00b5\u2217 is the first eigenvector of $(1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4$ deviation between the negative population gradient and the power iteration update $F(\u00b5t, \u00b5\u2217 t)$. Recall from Lemma C.3 that the t) is bounded by\n\n$$\n\\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) \\leq 250\u03b7 \\sqrt{d\\| \\mu_t \\| ^4 + 10\u03b7\\| \\mu_t \\| ^2\\| \\mu\u2217 t\\| ^2 \\leq 500\u03b7 \\sqrt{d^3\\| \\mu_t \\| ^4 + 20d\u03b7\\| \\mu_t \\| ^2 \u00b5\u2217 t 2} .\n$$\n$$\n\\langle \u00b5t, \u02c6\u00b5\u2217 t \u27e9\n$$", "images": [], "items": [{"type": "text", "value": "Lemma C.5. Suppose that the vector \u00b5t satisfies |\u27e8\u02c6 \u00b5t, \u02c6\u00b5\u2217t \u27e9| \u2265 2d, and let \u00b5\u2032 t denote the iterate resulting from a single empirical gradient step with learning rate \u03b7 starting from \u00b5t. Suppose that the empirical gradient and the population gradient differ by at most \u03b5. Denote the angle between \u00b5t (resp. \u00b5\u2032 t) and \u00b5\u2217 t by \u03b8 (resp. \u03b8\u2032). Then\n\n$$\n\\tan \\theta' = \\max (\\kappa_1 \\tan \\theta, \\kappa_2)\n$$\nfor\n\n$$\n\\begin{align*}\n\\kappa_1 &= \\frac{1 - \\sqrt{3}\\eta\\| \\mu_t \\|^2}{1 - 3\\eta\\| \\mu_t \\| ^2 + \\eta(\\| \\mu^*_t \\| ^2 - 500d^3\\| \\mu_t \\| ^4 - 20d\\| \\mu_t \\| ^2\\| \\mu^*_t \\| ^2 - \\eta\\tilde{\\varepsilon})} \\\\\n\\kappa_2 &= 500\\eta \\sqrt{d^3\\| \\mu_t \\| ^4 + 20\\eta d\\| \\mu_t \\| ^2\\| \\mu^*_t \\| ^2 + \\eta\\tilde{\\varepsilon}} \\quad \\text{and} \\quad \\tilde{\\varepsilon} \\lesssim d\\varepsilon\n\\end{align*}\n$$\nProof. Define \u02c6 \u00b5\u2217\u22a5 as the orthogonal vector to \u00b5\u2217 t in the plane of \u00b5t and \u00b5\u2217 t. Note that \u00b5\u2032 t still lies in this plane, so the orthogonal vector to \u00b5\u2217 t in the plane of \u00b5\u2032 t and \u00b5\u2217 t is also given by \u02c6 \u00b5\u2217\u22a5 t. We have\n\n$$\n\\begin{align*}\n\\tan \\theta' &= \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5\u2032 t \\rangle = \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5\u2032 t \\rangle \\\\\n&= \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\langle \u02c6 \u00b5\u2217\u22a5 t, -\\eta \\nabla L_t(s_t) - \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\eta \\varepsilon \\\\\n&= \\langle \u02c6 \u00b5\u2217 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\langle \u02c6 t, -\\eta \\nabla L_t(s_t) - \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle - \\eta \\varepsilon\n\\end{align*}\n$$\nwhere $F(\u00b5, \u00b5\u2217) = 2\u00b5\u2217 t \u00b5\u2217\u22a4$\n\n$$\n\\begin{align*}\n\\mu\u2217\u22a5, \\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) + \\eta \\varepsilon &\\leq \\sigma^2 \\langle \u02c6 t, \u00b5t \\rangle + \\eta \\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) - \\eta \\varepsilon \\quad (C.1) \\\\\n&\\leq \\sigma_1 \\langle \u02c6 \u00b5\u2217 t, \u00b5t \\rangle - \\eta t)\n\\end{align*}\n$$\nwhere \u03c31 and \u03c32 are the first and second eigenvalues of Id + F(\u00b5t, \u00b5\u2217 t) = (1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4 t, given by\n\n$$\n\\begin{align*}\n\\sigma_1 &= 1 + \\eta(2\\| \\mu\u2217 t \\| ^2 - 3\\| \\mu_t \\| ^2)\n\\end{align*}\n$$\nThe last inequality (C.1) follows from the fact that $\\sigma_2 = 1 - 3\u03b7\\| \\mu_t \\| ^2$.\n\n$$\n\\begin{align*}\n\\langle \u02c6 \u00b5\u2217 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle &= \u02c6 t ((1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4 t)\u00b5t \\\\\n&= \u00b5\u22a4 t \u00b5\u2217\u22a4 t \u00b5\u2217 t ((1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t)\u02c6 t = \u03c31\u00b5\u22a4 t \u02c6 t\n\\end{align*}\n$$\nbecause \u02c6 \u00b5\u2217 is the first eigenvector of $(1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4$ deviation between the negative population gradient and the power iteration update $F(\u00b5t, \u00b5\u2217 t)$. Recall from Lemma C.3 that the t) is bounded by\n\n$$\n\\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) \\leq 250\u03b7 \\sqrt{d\\| \\mu_t \\| ^4 + 10\u03b7\\| \\mu_t \\| ^2\\| \\mu\u2217 t\\| ^2 \\leq 500\u03b7 \\sqrt{d^3\\| \\mu_t \\| ^4 + 20d\u03b7\\| \\mu_t \\| ^2 \u00b5\u2217 t 2} .\n$$\n$$\n\\langle \u00b5t, \u02c6\u00b5\u2217 t \u27e9\n$$", "md": "Lemma C.5. Suppose that the vector \u00b5t satisfies |\u27e8\u02c6 \u00b5t, \u02c6\u00b5\u2217t \u27e9| \u2265 2d, and let \u00b5\u2032 t denote the iterate resulting from a single empirical gradient step with learning rate \u03b7 starting from \u00b5t. Suppose that the empirical gradient and the population gradient differ by at most \u03b5. Denote the angle between \u00b5t (resp. \u00b5\u2032 t) and \u00b5\u2217 t by \u03b8 (resp. \u03b8\u2032). Then\n\n$$\n\\tan \\theta' = \\max (\\kappa_1 \\tan \\theta, \\kappa_2)\n$$\nfor\n\n$$\n\\begin{align*}\n\\kappa_1 &= \\frac{1 - \\sqrt{3}\\eta\\| \\mu_t \\|^2}{1 - 3\\eta\\| \\mu_t \\| ^2 + \\eta(\\| \\mu^*_t \\| ^2 - 500d^3\\| \\mu_t \\| ^4 - 20d\\| \\mu_t \\| ^2\\| \\mu^*_t \\| ^2 - \\eta\\tilde{\\varepsilon})} \\\\\n\\kappa_2 &= 500\\eta \\sqrt{d^3\\| \\mu_t \\| ^4 + 20\\eta d\\| \\mu_t \\| ^2\\| \\mu^*_t \\| ^2 + \\eta\\tilde{\\varepsilon}} \\quad \\text{and} \\quad \\tilde{\\varepsilon} \\lesssim d\\varepsilon\n\\end{align*}\n$$\nProof. Define \u02c6 \u00b5\u2217\u22a5 as the orthogonal vector to \u00b5\u2217 t in the plane of \u00b5t and \u00b5\u2217 t. Note that \u00b5\u2032 t still lies in this plane, so the orthogonal vector to \u00b5\u2217 t in the plane of \u00b5\u2032 t and \u00b5\u2217 t is also given by \u02c6 \u00b5\u2217\u22a5 t. We have\n\n$$\n\\begin{align*}\n\\tan \\theta' &= \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5\u2032 t \\rangle = \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5\u2032 t \\rangle \\\\\n&= \\langle \u02c6 \u00b5\u2217\u22a5 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\langle \u02c6 \u00b5\u2217\u22a5 t, -\\eta \\nabla L_t(s_t) - \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\eta \\varepsilon \\\\\n&= \\langle \u02c6 \u00b5\u2217 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle + \\langle \u02c6 t, -\\eta \\nabla L_t(s_t) - \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle - \\eta \\varepsilon\n\\end{align*}\n$$\nwhere $F(\u00b5, \u00b5\u2217) = 2\u00b5\u2217 t \u00b5\u2217\u22a4$\n\n$$\n\\begin{align*}\n\\mu\u2217\u22a5, \\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) + \\eta \\varepsilon &\\leq \\sigma^2 \\langle \u02c6 t, \u00b5t \\rangle + \\eta \\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) - \\eta \\varepsilon \\quad (C.1) \\\\\n&\\leq \\sigma_1 \\langle \u02c6 \u00b5\u2217 t, \u00b5t \\rangle - \\eta t)\n\\end{align*}\n$$\nwhere \u03c31 and \u03c32 are the first and second eigenvalues of Id + F(\u00b5t, \u00b5\u2217 t) = (1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4 t, given by\n\n$$\n\\begin{align*}\n\\sigma_1 &= 1 + \\eta(2\\| \\mu\u2217 t \\| ^2 - 3\\| \\mu_t \\| ^2)\n\\end{align*}\n$$\nThe last inequality (C.1) follows from the fact that $\\sigma_2 = 1 - 3\u03b7\\| \\mu_t \\| ^2$.\n\n$$\n\\begin{align*}\n\\langle \u02c6 \u00b5\u2217 t, \u00b5t + \\eta F(\u00b5t, \u00b5\u2217 t) \\rangle &= \u02c6 t ((1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4 t)\u00b5t \\\\\n&= \u00b5\u22a4 t \u00b5\u2217\u22a4 t \u00b5\u2217 t ((1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t)\u02c6 t = \u03c31\u00b5\u22a4 t \u02c6 t\n\\end{align*}\n$$\nbecause \u02c6 \u00b5\u2217 is the first eigenvector of $(1 - 3\u03b7\\| \\mu_t \\| ^2)Id + 2\u03b7\u00b5\u2217 t\u00b5\u2217\u22a4$ deviation between the negative population gradient and the power iteration update $F(\u00b5t, \u00b5\u2217 t)$. Recall from Lemma C.3 that the t) is bounded by\n\n$$\n\\nabla L_t(s_t) + F(\u00b5t, \u00b5\u2217 t) \\leq 250\u03b7 \\sqrt{d\\| \\mu_t \\| ^4 + 10\u03b7\\| \\mu_t \\| ^2\\| \\mu\u2217 t\\| ^2 \\leq 500\u03b7 \\sqrt{d^3\\| \\mu_t \\| ^4 + 20d\u03b7\\| \\mu_t \\| ^2 \u00b5\u2217 t 2} .\n$$\n$$\n\\langle \u00b5t, \u02c6\u00b5\u2217 t \u27e9\n$$"}]}, {"page": 25, "text": "Substituting this into Eq. (C.1), we get\n                           \u03c32\u27e8\u02c6 \u00b5\u2217\u22a5\n                                  t , \u00b5t\u27e9    + \u03b7\u2225\u2207Lt(st) + F(\u00b5t, \u00b5\u2217              t)\u2225  + \u03b7\u03b5               where        \u02dc\n     tan \u03b8\u2032 \u2264                                  \u221a                                                                      \u03b5 \u2272     d\u03b5\n                   \u27e8\u02c6\u00b5\u2217                           d3\u2225\u00b5t\u22254 \u2212        20d\u03b7\u2225\u00b5t\u22252        \u00b5\u2217 2 \u2212      \u03b7\u02dc\u03b5)                         \u2225\u00b5\u2225\n                       t, \u00b5t\u27e9(\u03c31 \u2212      500\u03b7                                           t\n               \u2264   \u03c32   tan \u03b8 + 1          500\u03b7   \u221a  d3\u2225\u00b5\u22254 + 20d\u03b7\u2225\u00b5\u22252 \u2225\u00b5\u2217              t \u22252 + \u03b7\u02dc  \u03b5\n                   \u02dc\n                   \u03c31               \u02dc\n                                    \u03c31     where        \u02dc                      \u221a\n                                                       \u03c31 \u225c     \u03c31 \u2212    500\u03b7      d3\u2225\u00b5\u22254 \u2212       20d\u03b7\u2225\u00b5\u22252 \u2225\u00b5\u2217        t\u22252 \u2212    \u03b7\u02dc\u03b5\n               \u2264     1 \u2212   \u03b7\u2225\u00b5\u2217\u02dc t \u22252            \u03c32                       \u03b7\u2225\u00b5\u2217    t \u22252  500\u03b7     \u221a  d3\u2225\u00b5t\u22254 + 20d\u03b7\u2225\u00b5t\u22252 \u2225\u00b5\u2217              t \u22252 + \u03b7\u02dc  \u03b5\n                               \u03c31         \u02dc\n                                         \u03c31 \u2212    \u03b7\u2225\u00b5\u2217                           \u02dc\n                                                       t \u22252 tan \u03b8\u221a+            \u03c31                                 \u03b7\u2225\u00b5\u2217  t \u22252\n               \u2264   max       \u02dc      \u03c32                              d3\u2225\u00b5t\u22254 + 20\u03b7d \u2225\u00b5t\u22252 \u2225\u00b5\u2217              t \u22252 + \u03b7\u02dc  \u03b5\n                             \u03c31 \u2212    \u03b7\u2225\u00b5\u2217  t \u22252 tan \u03b8, 500\u03b7                         \u2225\u00b5\u2217 t \u22252\nwhere the last inequality uses the fact that convex combinations of two values is less than the\nmaximum of two values.\nFinally, we obtain the following bound on the correlation between the ground truth and the fi                                                         nal\niterate of gradient descent:\nLemma C.6. For any h \u2208                     N, let \u00b5(h)t    denote the iterate after h empirical gradient steps with learning\nrate \u03b7 = 1/20 starting from random initialization, where the empirical gradients are estimated from\nat least \u0398(d4B3                                                                                   t    and \u00b5\u2217    t. For any \u03b5 \u2272              1\n                   \u03b52 ) samples. Let \u03b8(h) denote the angle between \u00b5(h)                                                                   d2B9 , there\nexists H\u2032 \u2272        B6 log d such that for any H \u2265                    H\u2032, if     1          t \u2225  \u2264    1\n                                                                               B3 \u2264\u2225\u00b5\u2217              B2 , we have\n                                                                   tan \u03b8(H) \u2272        1 .\nProof. Denote the h-th iterate of gradient descent by \u00b5(h). In Lemma C.7 we show that                                                    \u00b5(h)      \u2264    1\n                                                                                      t                                                     t          B2\nfor all h. We would like to apply the bound in Lemma C.5 to argue that the angle with \u00b5\u2217                                                  t decreases\nwhen going from \u00b5(h)                to \u00b5(h+1). Using that                1            t \u2225  \u2264    1                        1\n                               t           t                            B3 \u2264      \u2225\u00b5\u2217          B2 and \u2225\u00b5t\u2225          \u2264   B2 , we can bound the\nquantity \u03ba1 that appears in Lemma C.5 by\n                                                                       1 \u2212    3\u03b7\u2225\u00b5t\u22252 \u221a\n                                      \u03ba1 \u2264                             \u03b7                d3  \u2212   20d\n                                               1 \u2212   3\u03b7\u2225\u00b5t\u22252 +        B6 (1 \u2212     500B2          B2 \u2212    \u03b5dB9)\n                                          \u2264           \u03b7              \u221a 1d3                          \u2264   1 +  1  \u03b7    .\n                                               1 +                          \u2212   20d                           2B6\n                                                     B6 (1 \u2212     500B2          B2 \u2212     \u03b5dB9)                                     1                    1\nOn the other hand, for B a suffi                 ciently large polynomial in d, we can again use that                             B3 \u2264     \u2225\u00b5\u2217 t\u2225  \u2264   B2\n                    1\nand \u2225\u00b5t\u2225       \u2264   B2 to bound the quantity \u03ba2 that appears in Lemma C.5 by\n                                                \u03ba2 \u2264     500\u03b7   \u221a  d3   + 20\u03b7d\n                                                              B2             B4 + B9\u03b7d\u03b5 \u2272              \u03b7\n                                                                                                       d .\n      \u27e8\u02c6           \u2265    1\nAs     \u00b5, \u02c6\u00b5\u2217\u27e9         2d, this implies | tan \u03b8(h)| \u2264             2d. Without loss of generality assume that tan \u03b8(h) \u2264                                2d.\n      By Lemma C.5, for any h we either have tan \u03b8(h) \u2272                                 \u03b7/d \u226a    \u03b7 1, in which case we are done as this\nbound will also hold for subsequent iterates, or tan \u03b8(h) \u2272                              (1+2B6 )\u22121 tan \u03b8(h\u22121). If the latter happens\n                                           log d                                                  \u03b7\nconsecutively for H \u2265                 log(1+    \u03b7                                               2B6 )\u2212H = 1       d, the angle \u03b8 will satisfy\n                                               2B6 ) steps, then because (1 +\ntan \u03b8 \u2264      2d \u00b7 (1/d) \u2272         1. The proof is complete because, by hypothesis, H \u2265                                     4B6 log d    \u2265        log d\u03b7\n                                                                                                                                \u03b7           log(1+   2B6 )\n(the last inequality follows from log(1 + x) \u2265                          x2 for any 0 < x < 1).\n                                                                            25", "md": "Substituting this into Eq. (C.1), we get\n\n$$\n\\sigma^2 \\langle \\hat{\\mu}^*_{\\perp t}, \\mu_t \\rangle + \\eta \\| \\nabla L_t(s_t) + F(\\mu_t, \\mu^*_t) \\| + \\eta \\epsilon \\leq \\tilde{\\epsilon}\n$$\n\nwhere\n\n$$\n\\tilde{\\epsilon} \\leq \\frac{\\langle \\hat{\\mu}^*_t, \\mu_t \\rangle}{\\sigma_1 - 500\\eta} \\sqrt{d^3 \\| \\mu \\|^4 + 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\n$$\n\n$$\n\\leq \\frac{\\sigma_2 \\tan \\theta + 1}{500\\eta} \\sqrt{d^3 \\| \\mu \\| ^4 + 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\n$$\n\nwhere\n\n$$\n\\tilde{\\sigma}_1 = \\sigma_1 - 500\\eta \\sqrt{d^3 \\| \\mu \\| ^4 - 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 - \\eta \\tilde{\\epsilon}}\n$$\n\n$$\n\\leq \\max \\left(1 - \\eta \\| \\mu^*_t \\|^2 \\sigma_2, \\frac{\\eta \\| \\mu^*_t \\| ^2}{500\\eta} \\sqrt{d^3 \\| \\mu_t \\| ^4 + 20d\\eta \\| \\mu_t \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\\right)\n$$\n\nwhere the last inequality uses the fact that convex combinations of two values is less than the maximum of two values.\n\nFinally, we obtain the following bound on the correlation between the ground truth and the final iterate of gradient descent:\n\n**Lemma C.6.** For any $h \\in \\mathbb{N}$, let $\\mu(h)_t$ denote the iterate after $h$ empirical gradient steps with learning rate $\\eta = 1/20$ starting from random initialization, where the empirical gradients are estimated from at least $\\Theta(d^4B^3_t)$ and $\\mu^*_t$. For any $\\epsilon \\lesssim \\frac{1}{\\epsilon^2}$ samples. Let $\\theta(h)$ denote the angle between $\\mu(h)_t$ and $\\mu^*_t$. For any $\\epsilon \\lesssim \\frac{1}{\\epsilon^2}$, there exists $H' \\lesssim B^6 \\log d$ such that for any $H \\geq H'$, if $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$, we have $\\tan \\theta(H) \\lesssim 1$.\n\n**Proof.** Denote the $h$-th iterate of gradient descent by $\\mu(h)_t$. In Lemma C.7 we show that $\\| \\mu(h)_t \\| \\leq B^2$ for all $h$. We would like to apply the bound in Lemma C.5 to argue that the angle with $\\mu^*_t$ decreases when going from $\\mu(h)_t$ to $\\mu(h+1)_t$. Using that $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$ and $\\| \\mu_t \\| \\leq B^2$, we can bound the quantity $\\kappa_1$ that appears in Lemma C.5 by\n\n$$\n\\kappa_1 \\leq \\frac{1 - 3\\eta \\| \\mu_t \\| ^2}{\\sqrt{\\eta} \\sqrt{d^3} - 20d} \\left(1 - 500B^2 \\right) \\left(1 - \\epsilon dB^9 \\right) \\leq \\frac{\\eta}{1 + \\sqrt{1 + \\frac{1}{\\eta}}}\n$$\n\nOn the other hand, for $B$ a sufficiently large polynomial in $d$, we can again use that $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$ and $\\| \\mu_t \\| \\leq B^2$ to bound the quantity $\\kappa_2$ that appears in Lemma C.5 by\n\n$$\n\\kappa_2 \\leq 500\\eta \\sqrt{d^3} + 20\\eta d B^2 + B^9 \\eta d \\epsilon \\lesssim \\eta d\n$$\n\nAs $\\langle \\hat{\\mu}, \\hat{\\mu}^* \\rangle \\geq 2d$, this implies $|\\tan \\theta(h)| \\leq \\frac{2d}{1}$. Without loss of generality assume that $\\tan \\theta(h) \\leq \\frac{2d}{1}$. By Lemma C.5, for any $h$ we either have $\\tan \\theta(h) \\lesssim \\frac{\\eta}{d} \\ll \\eta 1$, in which case we are done as this bound will also hold for subsequent iterates, or $\\tan \\theta(h) \\lesssim (1+2B^6)^{-1} \\tan \\theta(h-1)$. If the latter happens consecutively for $H \\geq \\log(1+\\frac{\\eta}{2B^6})^{-H} = \\frac{1}{d}$ steps, then because $(1 + \\tan \\theta \\leq \\frac{2d \\cdot (1/d)}{\\lesssim 1}$. The proof is complete because, by hypothesis, $H \\geq 4B^6 \\log d \\geq \\log d \\eta$ (the last inequality follows from $\\log(1 + x) \\geq x^2$ for any $0 < x < 1$).", "images": [], "items": [{"type": "text", "value": "Substituting this into Eq. (C.1), we get\n\n$$\n\\sigma^2 \\langle \\hat{\\mu}^*_{\\perp t}, \\mu_t \\rangle + \\eta \\| \\nabla L_t(s_t) + F(\\mu_t, \\mu^*_t) \\| + \\eta \\epsilon \\leq \\tilde{\\epsilon}\n$$\n\nwhere\n\n$$\n\\tilde{\\epsilon} \\leq \\frac{\\langle \\hat{\\mu}^*_t, \\mu_t \\rangle}{\\sigma_1 - 500\\eta} \\sqrt{d^3 \\| \\mu \\|^4 + 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\n$$\n\n$$\n\\leq \\frac{\\sigma_2 \\tan \\theta + 1}{500\\eta} \\sqrt{d^3 \\| \\mu \\| ^4 + 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\n$$\n\nwhere\n\n$$\n\\tilde{\\sigma}_1 = \\sigma_1 - 500\\eta \\sqrt{d^3 \\| \\mu \\| ^4 - 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 - \\eta \\tilde{\\epsilon}}\n$$\n\n$$\n\\leq \\max \\left(1 - \\eta \\| \\mu^*_t \\|^2 \\sigma_2, \\frac{\\eta \\| \\mu^*_t \\| ^2}{500\\eta} \\sqrt{d^3 \\| \\mu_t \\| ^4 + 20d\\eta \\| \\mu_t \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\\right)\n$$\n\nwhere the last inequality uses the fact that convex combinations of two values is less than the maximum of two values.\n\nFinally, we obtain the following bound on the correlation between the ground truth and the final iterate of gradient descent:\n\n**Lemma C.6.** For any $h \\in \\mathbb{N}$, let $\\mu(h)_t$ denote the iterate after $h$ empirical gradient steps with learning rate $\\eta = 1/20$ starting from random initialization, where the empirical gradients are estimated from at least $\\Theta(d^4B^3_t)$ and $\\mu^*_t$. For any $\\epsilon \\lesssim \\frac{1}{\\epsilon^2}$ samples. Let $\\theta(h)$ denote the angle between $\\mu(h)_t$ and $\\mu^*_t$. For any $\\epsilon \\lesssim \\frac{1}{\\epsilon^2}$, there exists $H' \\lesssim B^6 \\log d$ such that for any $H \\geq H'$, if $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$, we have $\\tan \\theta(H) \\lesssim 1$.\n\n**Proof.** Denote the $h$-th iterate of gradient descent by $\\mu(h)_t$. In Lemma C.7 we show that $\\| \\mu(h)_t \\| \\leq B^2$ for all $h$. We would like to apply the bound in Lemma C.5 to argue that the angle with $\\mu^*_t$ decreases when going from $\\mu(h)_t$ to $\\mu(h+1)_t$. Using that $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$ and $\\| \\mu_t \\| \\leq B^2$, we can bound the quantity $\\kappa_1$ that appears in Lemma C.5 by\n\n$$\n\\kappa_1 \\leq \\frac{1 - 3\\eta \\| \\mu_t \\| ^2}{\\sqrt{\\eta} \\sqrt{d^3} - 20d} \\left(1 - 500B^2 \\right) \\left(1 - \\epsilon dB^9 \\right) \\leq \\frac{\\eta}{1 + \\sqrt{1 + \\frac{1}{\\eta}}}\n$$\n\nOn the other hand, for $B$ a sufficiently large polynomial in $d$, we can again use that $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$ and $\\| \\mu_t \\| \\leq B^2$ to bound the quantity $\\kappa_2$ that appears in Lemma C.5 by\n\n$$\n\\kappa_2 \\leq 500\\eta \\sqrt{d^3} + 20\\eta d B^2 + B^9 \\eta d \\epsilon \\lesssim \\eta d\n$$\n\nAs $\\langle \\hat{\\mu}, \\hat{\\mu}^* \\rangle \\geq 2d$, this implies $|\\tan \\theta(h)| \\leq \\frac{2d}{1}$. Without loss of generality assume that $\\tan \\theta(h) \\leq \\frac{2d}{1}$. By Lemma C.5, for any $h$ we either have $\\tan \\theta(h) \\lesssim \\frac{\\eta}{d} \\ll \\eta 1$, in which case we are done as this bound will also hold for subsequent iterates, or $\\tan \\theta(h) \\lesssim (1+2B^6)^{-1} \\tan \\theta(h-1)$. If the latter happens consecutively for $H \\geq \\log(1+\\frac{\\eta}{2B^6})^{-H} = \\frac{1}{d}$ steps, then because $(1 + \\tan \\theta \\leq \\frac{2d \\cdot (1/d)}{\\lesssim 1}$. The proof is complete because, by hypothesis, $H \\geq 4B^6 \\log d \\geq \\log d \\eta$ (the last inequality follows from $\\log(1 + x) \\geq x^2$ for any $0 < x < 1$).", "md": "Substituting this into Eq. (C.1), we get\n\n$$\n\\sigma^2 \\langle \\hat{\\mu}^*_{\\perp t}, \\mu_t \\rangle + \\eta \\| \\nabla L_t(s_t) + F(\\mu_t, \\mu^*_t) \\| + \\eta \\epsilon \\leq \\tilde{\\epsilon}\n$$\n\nwhere\n\n$$\n\\tilde{\\epsilon} \\leq \\frac{\\langle \\hat{\\mu}^*_t, \\mu_t \\rangle}{\\sigma_1 - 500\\eta} \\sqrt{d^3 \\| \\mu \\|^4 + 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\n$$\n\n$$\n\\leq \\frac{\\sigma_2 \\tan \\theta + 1}{500\\eta} \\sqrt{d^3 \\| \\mu \\| ^4 + 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\n$$\n\nwhere\n\n$$\n\\tilde{\\sigma}_1 = \\sigma_1 - 500\\eta \\sqrt{d^3 \\| \\mu \\| ^4 - 20d\\eta \\| \\mu \\| ^2 \\| \\mu^*_t \\| ^2 - \\eta \\tilde{\\epsilon}}\n$$\n\n$$\n\\leq \\max \\left(1 - \\eta \\| \\mu^*_t \\|^2 \\sigma_2, \\frac{\\eta \\| \\mu^*_t \\| ^2}{500\\eta} \\sqrt{d^3 \\| \\mu_t \\| ^4 + 20d\\eta \\| \\mu_t \\| ^2 \\| \\mu^*_t \\| ^2 + \\eta \\tilde{\\epsilon}}\\right)\n$$\n\nwhere the last inequality uses the fact that convex combinations of two values is less than the maximum of two values.\n\nFinally, we obtain the following bound on the correlation between the ground truth and the final iterate of gradient descent:\n\n**Lemma C.6.** For any $h \\in \\mathbb{N}$, let $\\mu(h)_t$ denote the iterate after $h$ empirical gradient steps with learning rate $\\eta = 1/20$ starting from random initialization, where the empirical gradients are estimated from at least $\\Theta(d^4B^3_t)$ and $\\mu^*_t$. For any $\\epsilon \\lesssim \\frac{1}{\\epsilon^2}$ samples. Let $\\theta(h)$ denote the angle between $\\mu(h)_t$ and $\\mu^*_t$. For any $\\epsilon \\lesssim \\frac{1}{\\epsilon^2}$, there exists $H' \\lesssim B^6 \\log d$ such that for any $H \\geq H'$, if $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$, we have $\\tan \\theta(H) \\lesssim 1$.\n\n**Proof.** Denote the $h$-th iterate of gradient descent by $\\mu(h)_t$. In Lemma C.7 we show that $\\| \\mu(h)_t \\| \\leq B^2$ for all $h$. We would like to apply the bound in Lemma C.5 to argue that the angle with $\\mu^*_t$ decreases when going from $\\mu(h)_t$ to $\\mu(h+1)_t$. Using that $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$ and $\\| \\mu_t \\| \\leq B^2$, we can bound the quantity $\\kappa_1$ that appears in Lemma C.5 by\n\n$$\n\\kappa_1 \\leq \\frac{1 - 3\\eta \\| \\mu_t \\| ^2}{\\sqrt{\\eta} \\sqrt{d^3} - 20d} \\left(1 - 500B^2 \\right) \\left(1 - \\epsilon dB^9 \\right) \\leq \\frac{\\eta}{1 + \\sqrt{1 + \\frac{1}{\\eta}}}\n$$\n\nOn the other hand, for $B$ a sufficiently large polynomial in $d$, we can again use that $\\frac{1}{B^3} \\leq \\| \\mu^*_t \\| \\leq B^2$ and $\\| \\mu_t \\| \\leq B^2$ to bound the quantity $\\kappa_2$ that appears in Lemma C.5 by\n\n$$\n\\kappa_2 \\leq 500\\eta \\sqrt{d^3} + 20\\eta d B^2 + B^9 \\eta d \\epsilon \\lesssim \\eta d\n$$\n\nAs $\\langle \\hat{\\mu}, \\hat{\\mu}^* \\rangle \\geq 2d$, this implies $|\\tan \\theta(h)| \\leq \\frac{2d}{1}$. Without loss of generality assume that $\\tan \\theta(h) \\leq \\frac{2d}{1}$. By Lemma C.5, for any $h$ we either have $\\tan \\theta(h) \\lesssim \\frac{\\eta}{d} \\ll \\eta 1$, in which case we are done as this bound will also hold for subsequent iterates, or $\\tan \\theta(h) \\lesssim (1+2B^6)^{-1} \\tan \\theta(h-1)$. If the latter happens consecutively for $H \\geq \\log(1+\\frac{\\eta}{2B^6})^{-H} = \\frac{1}{d}$ steps, then because $(1 + \\tan \\theta \\leq \\frac{2d \\cdot (1/d)}{\\lesssim 1}$. The proof is complete because, by hypothesis, $H \\geq 4B^6 \\log d \\geq \\log d \\eta$ (the last inequality follows from $\\log(1 + x) \\geq x^2$ for any $0 < x < 1$)."}]}, {"page": 26, "text": "                                                                                          1\n Lemma C.7. When parameter \u00b5t satisfies \u2225\u00b5t\u2225                                        \u2264    B2 for the noise scale t = O(log d) and \u00b5\u2032                             t is\n the new parameter after performing a gradient descent update on the DDPM objective at noise scale\n t = O(log d), then parameter \u00b5\u2032                    t satisfies \u2225\u00b5\u2032                1\n Proof. When \u2225\u00b5t\u2225               \u2264   0.9\u2225\u00b5\u2217   t \u2225  \u2264   0.9                t\u2225   \u2264   B2 .\n                                                      B2 , we have\n         \u2225\u00b5\u2032 t\u2225  \u2264   \u2225\u00b5t + \u03b7F        (\u00b5t, \u00b5\u2217  t1)\u2225  + \u03b7\u2225(\u2212\u2207Lt(s\u00b5t) \u2212                F  (\u00b5, \u00b5\u2217))\u2225      + \u03b7\u03b5 \u2264       (1 + 2\u03b7\u2225\u00b5\u2217      t \u22252)\u2225\u00b5t\u2225      +   dB91\n                     \u2264   1.05\u2225\u00b5t\u2225       +    dB9 \u2264       B21.\n When \u2225\u00b5t\u2225 1         \u2265   0.9\u2225\u00b5\u2217   t \u2225, then maximum eigenvalue of F                          (\u00b5t, \u00b5\u2217  t ) is negative. Therefore, \u2225\u00b5\u2032                  t\u2225   is less\n than     B2 . Specifi      cally, we have\n             \u2225\u00b5\u2032 t\u2225  \u2264   \u2225\u00b5t + \u03b7F        (\u00b5t, \u00b5\u2217  t)\u2225   + \u03b7\u2225(\u2212\u2207Lt(s\u00b5t) \u2212              1 F  (\u00b5, \u00b5\u2217))\u2225      + \u03b7\u03b5  \u00b5\u2217 2)\u2225\u00b5t\u2225         +      1\n                         \u2264   (1 + \u03b7(2\u2225\u00b5\u2217       t \u22252 \u2212    3\u2225\u00b5t\u22252))\u2225\u00b5t\u2225          +   dB9 \u2264        (1 \u2212    0.01       t                  dB9 \u2264        B21.\n C.2         Low noise regime - connection to EM algorithm\n In the previous section we showed how to obtain a warm start by running gradient descent on the\n DDPM objective at high noise. We now focus on proving the contraction of \u2225\u00b5t \u2212                                                                  \u00b5\u2217t \u2225   starting\n from this warm start, by running gradient descent at low noise. We fi                                                rst prove the contraction for\n population gradient descent and then, we argue that the empirical gradient descent concentrates\n well around the population gradient descent.\n       As before, we denote \u00b5t as the current iterate and \u00b5\u2032                                 t as the next iterate obtained by performing\n(population) gradient descent on the DDPM objective with step size \u03b7. We upper bound \u2225\u00b5\u2032                                                                  t \u2212   \u00b5\u2217t \u2225\n as follows:\n                 \u2225\u00b5\u2032 t \u2212   \u00b5\u2217 t \u2225 = \u2225\u00b5t \u2212        \u03b7\u2207\u00b5tLt(s\u00b5t) \u2212           \u00b5\u2217 t \u2225\n                              =   (1 \u2212      \u03b7)(\u00b5t \u2212      \u00b5\u2217t ) + \u03b7 Ex\u223cN (\u00b5\u2217       t ,1)    tanh(\u00b5\u22a4     t x) \u2212     1               t x)\u2225\u00b5t\u22252\n                                                                                                                  2 tanh\u2032\u2032(\u00b5\u22a4\n                                  + tanh\u2032(\u00b5\u22a4      t x)\u00b5\u22a4   t x   x    \u2212   \u03b7 Ex\u223cN (\u00b5\u2217     t ,1)[tanh\u2032(\u00b5\u22a4     t x)\u00b5t] \u2212      \u03b7\u00b5\u2217  t\n where                        \u2264(1 \u2212      \u03b7) \u2225\u00b5t \u2212       \u00b5\u2217t \u2225  + \u03b7   Ex\u223cN (\u00b5\u2217      t ,1)[tanh(\u00b5\u22a4     t x)x] \u2212      \u00b5\u2217t    + \u03b7 \u2225G(\u00b5t, \u00b5\u2217       t )\u2225  ,\n          G(\u00b5t, \u00b5\u2217    t ) \u225c   Ex\u223cN (\u00b5\u2217     t ,Id)   \u2212   1               t x)\u2225\u00b5t\u22252 x + (tanh\u2032(\u00b5\u22a4              t x)\u00b5\u22a4   t x)x \u2212      tanh\u2032(\u00b5\u22a4    t x)\u00b5t      .\n                                                        2 tanh\u2032\u2032(\u00b5\u22a4\n Recall that E         x\u223cN (\u00b5\u2217   t ,1)[tanh(\u00b5\u22a4     t x)x] is the EM update for mixtures of two Gaussians (See Fact 5).\n If we can show that the G(\u00b5t, \u00b5\u2217                      t) term above is \u201ccontractive\u201d in the sense that it is decreasing in\n \u2225\u00b5t \u2212      \u00b5\u2217t\u2225, then we can invoke existing results on convergence of EM to show that the distance\n between the current iterate and \u00b5\u2217                      t contracts in a single gradient step [DTZ17, XHM16]. Our goal\n is thus to control G(\u00b5t, \u00b5\u2217               t ).\n       For this, we start with the 1D case in Lemma C.8. We then extend to the multi-dimensional\n case in Lemma C.9.\n Lemma C.8 (One-dimensional version). Let \u00b5, \u00b5\u2217                                       > 0, and consider \u00b5 \u2208                 [c, 4\u00b5\u2217\n                                                                                                                                  3 ] for some constant\n c. In this one-dimensional case, the function G specializes to\n                      G(\u00b5, \u00b5\u2217) = Ex\u223cN (\u00b5\u2217,1)                 \u22121 2 tanh\u2032\u2032(\u00b5x)\u00b52x + tanh\u2032(\u00b5x)\u00b5x2 \u2212                          tanh\u2032(\u00b5x)\u00b5           ,             (C.2)\n                                                                                 26", "md": "## Lemma C.7\n\nWhen parameter $$\\mu_t$$ satisfies $$\\|\\mu_t\\| \\leq B^2$$ for the noise scale $$t = O(\\log d)$$ and $$\\mu'_t$$ is the new parameter after performing a gradient descent update on the DDPM objective at noise scale $$t = O(\\log d)$$, then parameter $$\\mu'_t$$ satisfies $$\\|\\mu'_t\\| \\leq B^2$$.\n\n### Proof:\n\nWhen $$\\|\\mu_t\\| \\leq 0.9\\|\\mu^*_t\\| \\leq 0.9B^2$$, we have\n\n$$\n\\begin{align*}\n\\|\\mu'_t\\| &\\leq \\|\\mu_t + \\eta F(\\mu_t, \\mu^*_{t1})\\| + \\eta\\|(-\\nabla L_t(s\\mu_t) - F(\\mu, \\mu^*))\\| + \\eta\\epsilon \\\\\n&\\leq (1 + 2\\eta\\|\\mu^*_t\\|^2)\\|\\mu_t\\| + dB91 \\\\\n&\\leq 1.05\\|\\mu_t\\| + dB9 \\leq B^2.\n\\end{align*}\n$$\nWhen $$\\|\\mu_t\\| \\geq 0.9\\|\\mu^*_t\\|$$, then the maximum eigenvalue of $$F(\\mu_t, \\mu^*_t)$$ is negative. Therefore, $$\\|\\mu'_t\\|$$ is less than $$B^2$$. Specifically, we have\n\n$$\n\\begin{align*}\n\\|\\mu'_t\\| &\\leq \\|\\mu_t + \\eta F(\\mu_t, \\mu^*_t)\\| + \\eta\\|(-\\nabla L_t(s\\mu_t) - F(\\mu, \\mu^*))\\| + \\eta\\epsilon\\|\\mu^*_t\\|^2\\|\\mu_t\\| + 1 \\\\\n&\\leq (1 + \\eta(2\\|\\mu^*_t\\|^2 - 3\\|\\mu_t\\|^2))\\|\\mu_t\\| + dB9 \\leq (1 - 0.01)B^2 \\leq B^2.\n\\end{align*}\n$$\n\n## C.2 Low noise regime - connection to EM algorithm\n\nIn the previous section, we showed how to obtain a warm start by running gradient descent on the DDPM objective at high noise. We now focus on proving the contraction of $$\\|\\mu_t - \\mu^*_t\\|$$ starting from this warm start, by running gradient descent at low noise. We first prove the contraction for population gradient descent and then, we argue that the empirical gradient descent concentrates well around the population gradient descent.\n\nAs before, we denote $$\\mu_t$$ as the current iterate and $$\\mu'_t$$ as the next iterate obtained by performing (population) gradient descent on the DDPM objective with step size $$\\eta$$. We upper bound $$\\|\\mu'_t - \\mu^*_t\\|$$ as follows:\n\n$$\n\\begin{align*}\n\\|\\mu'_t - \\mu^*_t\\| &= \\|\\mu_t - \\eta\\nabla_{\\mu_t}L_t(s\\mu_t) - \\mu^*_t\\| \\\\\n&= (1 - \\eta)(\\mu_t - \\mu^*_t) + \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x) - \\tanh(\\mu_t^{\\top}x)]\\|\\mu_t\\|^2 \\\\\n&\\quad + \\tanh'(\\mu_t^{\\top}x)\\mu_t^{\\top}xx - \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh'(\\mu_t^{\\top}x)\\mu_t] - \\eta\\mu^*_t \\\\\n&\\leq (1 - \\eta) \\|\\mu_t - \\mu^*_t\\| + \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x)x] - \\mu^*_t + \\eta\\|G(\\mu_t, \\mu^*_t)\\|,\n\\end{align*}\n$$\nwhere $$G(\\mu_t, \\mu^*_t) \\triangleq \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu_t^{\\top}x)x] - \\tanh'(\\mu_t^{\\top}x)\\mu_t^{\\top}xx - \\tanh'(\\mu_t^{\\top}x)\\mu_t$$.\n\nRecall that $$\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x)x]$$ is the EM update for mixtures of two Gaussians (See Fact 5). If we can show that the $$G(\\mu_t, \\mu^*_t)$$ term above is \"contractive\" in the sense that it is decreasing in $$\\|\\mu_t - \\mu^*_t\\|$$, then we can invoke existing results on convergence of EM to show that the distance between the current iterate and $$\\mu^*_t$$ contracts in a single gradient step [DTZ17, XHM16]. Our goal is thus to control $$G(\\mu_t, \\mu^*_t)$$.\n\n## Lemma C.8 (One-dimensional version)\n\nLet $$\\mu, \\mu^* > 0$$, and consider $$\\mu \\in [c, 4\\mu^3]$$ for some constant $$c$$. In this one-dimensional case, the function $$G$$ specializes to\n\n$$\nG(\\mu, \\mu^*) = \\mathbb{E}_{x\\sim N(\\mu^*,1)}[-\\frac{1}{2}\\tanh''(\\mu x)\\mu^2x + \\tanh'(\\mu x)\\mu x^2 - \\tanh'(\\mu x)\\mu] \\quad \\text{(C.2)}\n$$", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Lemma C.7", "md": "## Lemma C.7"}, {"type": "text", "value": "When parameter $$\\mu_t$$ satisfies $$\\|\\mu_t\\| \\leq B^2$$ for the noise scale $$t = O(\\log d)$$ and $$\\mu'_t$$ is the new parameter after performing a gradient descent update on the DDPM objective at noise scale $$t = O(\\log d)$$, then parameter $$\\mu'_t$$ satisfies $$\\|\\mu'_t\\| \\leq B^2$$.", "md": "When parameter $$\\mu_t$$ satisfies $$\\|\\mu_t\\| \\leq B^2$$ for the noise scale $$t = O(\\log d)$$ and $$\\mu'_t$$ is the new parameter after performing a gradient descent update on the DDPM objective at noise scale $$t = O(\\log d)$$, then parameter $$\\mu'_t$$ satisfies $$\\|\\mu'_t\\| \\leq B^2$$."}, {"type": "heading", "lvl": 3, "value": "Proof:", "md": "### Proof:"}, {"type": "text", "value": "When $$\\|\\mu_t\\| \\leq 0.9\\|\\mu^*_t\\| \\leq 0.9B^2$$, we have\n\n$$\n\\begin{align*}\n\\|\\mu'_t\\| &\\leq \\|\\mu_t + \\eta F(\\mu_t, \\mu^*_{t1})\\| + \\eta\\|(-\\nabla L_t(s\\mu_t) - F(\\mu, \\mu^*))\\| + \\eta\\epsilon \\\\\n&\\leq (1 + 2\\eta\\|\\mu^*_t\\|^2)\\|\\mu_t\\| + dB91 \\\\\n&\\leq 1.05\\|\\mu_t\\| + dB9 \\leq B^2.\n\\end{align*}\n$$\nWhen $$\\|\\mu_t\\| \\geq 0.9\\|\\mu^*_t\\|$$, then the maximum eigenvalue of $$F(\\mu_t, \\mu^*_t)$$ is negative. Therefore, $$\\|\\mu'_t\\|$$ is less than $$B^2$$. Specifically, we have\n\n$$\n\\begin{align*}\n\\|\\mu'_t\\| &\\leq \\|\\mu_t + \\eta F(\\mu_t, \\mu^*_t)\\| + \\eta\\|(-\\nabla L_t(s\\mu_t) - F(\\mu, \\mu^*))\\| + \\eta\\epsilon\\|\\mu^*_t\\|^2\\|\\mu_t\\| + 1 \\\\\n&\\leq (1 + \\eta(2\\|\\mu^*_t\\|^2 - 3\\|\\mu_t\\|^2))\\|\\mu_t\\| + dB9 \\leq (1 - 0.01)B^2 \\leq B^2.\n\\end{align*}\n$$", "md": "When $$\\|\\mu_t\\| \\leq 0.9\\|\\mu^*_t\\| \\leq 0.9B^2$$, we have\n\n$$\n\\begin{align*}\n\\|\\mu'_t\\| &\\leq \\|\\mu_t + \\eta F(\\mu_t, \\mu^*_{t1})\\| + \\eta\\|(-\\nabla L_t(s\\mu_t) - F(\\mu, \\mu^*))\\| + \\eta\\epsilon \\\\\n&\\leq (1 + 2\\eta\\|\\mu^*_t\\|^2)\\|\\mu_t\\| + dB91 \\\\\n&\\leq 1.05\\|\\mu_t\\| + dB9 \\leq B^2.\n\\end{align*}\n$$\nWhen $$\\|\\mu_t\\| \\geq 0.9\\|\\mu^*_t\\|$$, then the maximum eigenvalue of $$F(\\mu_t, \\mu^*_t)$$ is negative. Therefore, $$\\|\\mu'_t\\|$$ is less than $$B^2$$. Specifically, we have\n\n$$\n\\begin{align*}\n\\|\\mu'_t\\| &\\leq \\|\\mu_t + \\eta F(\\mu_t, \\mu^*_t)\\| + \\eta\\|(-\\nabla L_t(s\\mu_t) - F(\\mu, \\mu^*))\\| + \\eta\\epsilon\\|\\mu^*_t\\|^2\\|\\mu_t\\| + 1 \\\\\n&\\leq (1 + \\eta(2\\|\\mu^*_t\\|^2 - 3\\|\\mu_t\\|^2))\\|\\mu_t\\| + dB9 \\leq (1 - 0.01)B^2 \\leq B^2.\n\\end{align*}\n$$"}, {"type": "heading", "lvl": 2, "value": "C.2 Low noise regime - connection to EM algorithm", "md": "## C.2 Low noise regime - connection to EM algorithm"}, {"type": "text", "value": "In the previous section, we showed how to obtain a warm start by running gradient descent on the DDPM objective at high noise. We now focus on proving the contraction of $$\\|\\mu_t - \\mu^*_t\\|$$ starting from this warm start, by running gradient descent at low noise. We first prove the contraction for population gradient descent and then, we argue that the empirical gradient descent concentrates well around the population gradient descent.\n\nAs before, we denote $$\\mu_t$$ as the current iterate and $$\\mu'_t$$ as the next iterate obtained by performing (population) gradient descent on the DDPM objective with step size $$\\eta$$. We upper bound $$\\|\\mu'_t - \\mu^*_t\\|$$ as follows:\n\n$$\n\\begin{align*}\n\\|\\mu'_t - \\mu^*_t\\| &= \\|\\mu_t - \\eta\\nabla_{\\mu_t}L_t(s\\mu_t) - \\mu^*_t\\| \\\\\n&= (1 - \\eta)(\\mu_t - \\mu^*_t) + \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x) - \\tanh(\\mu_t^{\\top}x)]\\|\\mu_t\\|^2 \\\\\n&\\quad + \\tanh'(\\mu_t^{\\top}x)\\mu_t^{\\top}xx - \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh'(\\mu_t^{\\top}x)\\mu_t] - \\eta\\mu^*_t \\\\\n&\\leq (1 - \\eta) \\|\\mu_t - \\mu^*_t\\| + \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x)x] - \\mu^*_t + \\eta\\|G(\\mu_t, \\mu^*_t)\\|,\n\\end{align*}\n$$\nwhere $$G(\\mu_t, \\mu^*_t) \\triangleq \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu_t^{\\top}x)x] - \\tanh'(\\mu_t^{\\top}x)\\mu_t^{\\top}xx - \\tanh'(\\mu_t^{\\top}x)\\mu_t$$.\n\nRecall that $$\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x)x]$$ is the EM update for mixtures of two Gaussians (See Fact 5). If we can show that the $$G(\\mu_t, \\mu^*_t)$$ term above is \"contractive\" in the sense that it is decreasing in $$\\|\\mu_t - \\mu^*_t\\|$$, then we can invoke existing results on convergence of EM to show that the distance between the current iterate and $$\\mu^*_t$$ contracts in a single gradient step [DTZ17, XHM16]. Our goal is thus to control $$G(\\mu_t, \\mu^*_t)$$.", "md": "In the previous section, we showed how to obtain a warm start by running gradient descent on the DDPM objective at high noise. We now focus on proving the contraction of $$\\|\\mu_t - \\mu^*_t\\|$$ starting from this warm start, by running gradient descent at low noise. We first prove the contraction for population gradient descent and then, we argue that the empirical gradient descent concentrates well around the population gradient descent.\n\nAs before, we denote $$\\mu_t$$ as the current iterate and $$\\mu'_t$$ as the next iterate obtained by performing (population) gradient descent on the DDPM objective with step size $$\\eta$$. We upper bound $$\\|\\mu'_t - \\mu^*_t\\|$$ as follows:\n\n$$\n\\begin{align*}\n\\|\\mu'_t - \\mu^*_t\\| &= \\|\\mu_t - \\eta\\nabla_{\\mu_t}L_t(s\\mu_t) - \\mu^*_t\\| \\\\\n&= (1 - \\eta)(\\mu_t - \\mu^*_t) + \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x) - \\tanh(\\mu_t^{\\top}x)]\\|\\mu_t\\|^2 \\\\\n&\\quad + \\tanh'(\\mu_t^{\\top}x)\\mu_t^{\\top}xx - \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh'(\\mu_t^{\\top}x)\\mu_t] - \\eta\\mu^*_t \\\\\n&\\leq (1 - \\eta) \\|\\mu_t - \\mu^*_t\\| + \\eta\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x)x] - \\mu^*_t + \\eta\\|G(\\mu_t, \\mu^*_t)\\|,\n\\end{align*}\n$$\nwhere $$G(\\mu_t, \\mu^*_t) \\triangleq \\mathbb{E}_{x\\sim N(\\mu^*_t,Id)}[\\tanh(\\mu_t^{\\top}x)x] - \\tanh'(\\mu_t^{\\top}x)\\mu_t^{\\top}xx - \\tanh'(\\mu_t^{\\top}x)\\mu_t$$.\n\nRecall that $$\\mathbb{E}_{x\\sim N(\\mu^*_t,1)}[\\tanh(\\mu_t^{\\top}x)x]$$ is the EM update for mixtures of two Gaussians (See Fact 5). If we can show that the $$G(\\mu_t, \\mu^*_t)$$ term above is \"contractive\" in the sense that it is decreasing in $$\\|\\mu_t - \\mu^*_t\\|$$, then we can invoke existing results on convergence of EM to show that the distance between the current iterate and $$\\mu^*_t$$ contracts in a single gradient step [DTZ17, XHM16]. Our goal is thus to control $$G(\\mu_t, \\mu^*_t)$$."}, {"type": "heading", "lvl": 2, "value": "Lemma C.8 (One-dimensional version)", "md": "## Lemma C.8 (One-dimensional version)"}, {"type": "text", "value": "Let $$\\mu, \\mu^* > 0$$, and consider $$\\mu \\in [c, 4\\mu^3]$$ for some constant $$c$$. In this one-dimensional case, the function $$G$$ specializes to\n\n$$\nG(\\mu, \\mu^*) = \\mathbb{E}_{x\\sim N(\\mu^*,1)}[-\\frac{1}{2}\\tanh''(\\mu x)\\mu^2x + \\tanh'(\\mu x)\\mu x^2 - \\tanh'(\\mu x)\\mu] \\quad \\text{(C.2)}\n$$", "md": "Let $$\\mu, \\mu^* > 0$$, and consider $$\\mu \\in [c, 4\\mu^3]$$ for some constant $$c$$. In this one-dimensional case, the function $$G$$ specializes to\n\n$$\nG(\\mu, \\mu^*) = \\mathbb{E}_{x\\sim N(\\mu^*,1)}[-\\frac{1}{2}\\tanh''(\\mu x)\\mu^2x + \\tanh'(\\mu x)\\mu x^2 - \\tanh'(\\mu x)\\mu] \\quad \\text{(C.2)}\n$$"}]}, {"page": 27, "text": "and we have\n       The proof uses the fact that the function G only contains fi   G(\u00b5, \u00b5\u2217) \u2264           0.01     \u00b5 \u2212     \u00b5\u2217       rst or higher-order derivatives of the\ntanh function and all the derivatives of tanh decay exponential quickly as \u00b5 increases. Therefore,\nwhen \u00b5 is at least a constant, we obtain the result. The complete proof of lemma C.8 is given in\nAppendix F.2.\nLemma C.9 (Multi-dimensional version). For any noise scale t, when the current parameter at\nnoise scale t, \u00b5t, satisfies \u2225\u00b5t\u2225                        \u2208   [c, 4\u27e8\u02c6 \u00b5t,\u00b5\u2217  t \u27e9] for some sufficiently large constant c, then the following\n                                                                        3\ninequality holds:\n                                                                  G(\u00b5t, \u00b5\u2217       t )    \u2264   0.01     \u00b5t \u2212       \u00b5\u2217t\nProof. Suppose {v1, v2, . . . , vd} are d orthonormal directions such that v1 = \u02c6                                                               \u00b5t and v2 is either of\nthe two unit vectors \u02c6                 \u00b5\u22a5t which are orthogonal to \u02c6                        \u00b5t in the plane of \u00b5t and \u00b5\u2217                      t . Recall that\n           G(\u00b5t, \u00b5\u2217      t ) = Ex\u223cN (\u00b5\u2217         t ,Id)    \u2212    1                 t x)\u2225\u00b5t\u22252 x + (tanh\u2032(\u00b5\u22a4                  t x)\u00b5\u22a4    t x)x \u2212       tanh\u2032(\u00b5\u22a4      t x)\u00b5t\n                             = E     x\u223cN (0,I)         \u2212   1   2 tanh\u2032\u2032(\u00b5\u22a4   t (x + \u00b5\u2217      t ))\u2225\u00b5t\u22252 (x + \u00b5\u2217           t )\n                                                           2 tanh\u2032\u2032(\u00b5\u22a4\n                                      + tanh\u2032(\u00b5\u22a4        t (x + \u00b5\u2217      t ))(\u00b5\u22a4  t (x + \u00b5\u2217      t ))(x + \u00b5\u2217      t ) \u2212    tanh\u2032(\u00b5\u22a4      t (x + \u00b5\u2217      t ))\u00b5t\n                             = E     \u03b11,\u03b12,...,\u03b1d\u223cN (0,1)             \u2212   1                                    \u00b5\u22a4 t \u00b5\u2217 t ))\u2225\u00b5t\u22252                i \u03b1ivi + \u00b5\u2217      t\n                                                                          2 tanh\u2032\u2032(\u2225\u00b5t\u2225            (\u03b11 + \u02c6\n                                      + tanh\u2032(\u2225\u00b5t\u2225             (\u03b11 + \u02c6    \u00b5\u22a4 t \u00b5\u2217 t ))\u2225\u00b5t\u2225       (\u03b11 + \u02c6t \u00b5\u2217\u00b5\u22a4      t )          i \u03b1ivi + \u00b5\u2217      t\n                                      \u2212   tanh\u2032(\u2225\u00b5t\u2225           (\u03b11 + \u02c6    \u00b5\u22a4 t \u00b5\u2217 t ))\u00b5t      ,\nwhere in the last equality we rewrote x \u223c                                         N   (0, I) as  d          i=1 \u03b1ivi for \u03b1i \u223c                N   (0, 1). Therefore, we\nhave\n  \u27e8  \u02c6\n    \u00b5t, G(\u00b5t, \u00b5\u2217       t )\u27e9\n        = E    \u03b11,\u03b12,...,\u03b1d\u223cN (0,I)             \u2212    1                                   \u00b5\u22a4 t \u00b5\u2217 t ))\u2225\u00b5t\u22252 (\u03b11 + \u02c6           \u00b5t\u22a4\u00b5\u2217    t )\n                                                     2 tanh\u2032\u2032(\u2225\u00b5t\u2225            (\u03b11 + \u02c6\n                + tanh\u2032(\u2225\u00b5t\u2225             (\u03b11 + \u02c6     \u00b5t\u22a4\u00b5\u2217    t ))\u2225\u00b5t\u2225      (\u03b11 + \u02c6t \u00b5\u2217 \u00b5\u22a4      t )2 \u2212     tanh\u2032(\u2225\u00b5t\u2225          (\u03b11 + \u02c6     \u00b5t\u22a4\u00b5\u2217    t ))\u2225\u00b5t\u2225\n        = E    \u03b11\u223cN (\u02c6    \u00b5\u22a4t \u00b5\u2217t ,1)    \u2212    1                                                                                         1 \u2212    tanh\u2032(\u2225\u00b5t\u2225           \u03b11)\u2225\u00b5t\u2225  .\n                                              2 tanh\u2032\u2032(\u2225\u00b5t\u2225\u03b11)\u2225\u00b5t\u22252 \u03b11 + tanh\u2032(\u2225\u00b5t\u2225                                   \u03b11)\u2225\u00b5t\u2225        \u03b12\nBy taking \u2225\u00b5t\u2225                to be \u00b5 and \u27e8\u02c6            \u00b5t, \u00b5\u2217  t \u27e9  to be \u00b5\u2217, we observe the similarity between the right side of\nthe above equation and the one-dimensional defi                                           nition of G defi            ned in Eq. (C.2). Using Lemma C.8\nand if \u2225\u00b5t\u2225          \u2208   [c, 4\u27e8\u02c6 \u00b5t,\u00b5\u2217  t \u27e9], we have\n                                    3\n                                                       \u27e8 \u02c6                                     \u27e8  \u02c6\n                                                        \u00b5t, G(\u00b5t, \u00b5\u2217                             \u00b5t, \u00b5t\u27e9      \u2212   \u27e8  \u02c6\n                                                                            t )\u27e9  \u2264    0.01                         \u00b5t, \u00b5\u2217   t \u27e9\n                                                                                          27", "md": "and we have\n\nThe proof uses the fact that the function G only contains fi $$G(\\mu, \\mu^*) \\leq 0.01 \\frac{\\mu - \\mu^*}{\\text{rst or higher-order derivatives of the tanh function}}$$ and all the derivatives of tanh decay exponential quickly as $$\\mu$$ increases. Therefore, when $$\\mu$$ is at least a constant, we obtain the result. The complete proof of lemma C.8 is given in Appendix F.2.\n\nLemma C.9 (Multi-dimensional version). For any noise scale t, when the current parameter at noise scale t, $$\\mu_t$$, satisfies $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle]$$ for some sufficiently large constant c, then the following inequality holds:\n\n$$\nG(\\mu_t, \\mu^*_t) \\leq 0.01 \\frac{\\mu_t - \\mu^*_t}{3}\n$$\nProof. Suppose {v1, v2, . . . , vd} are d orthonormal directions such that $$v_1 = \\hat{\\mu}_t$$ and $$v_2$$ is either of the two unit vectors $$\\hat{\\mu}_\\perp_t$$ which are orthogonal to $$\\hat{\\mu}_t$$ in the plane of $$\\mu_t$$ and $$\\mu^*_t$$. Recall that\n\n$$\n\\begin{align*}\nG(\\mu_t, \\mu^*_t) & = E_{x\\sim N(\\mu^*_t,Id)}\\left[1 - \\tanh(\\mu_t^Tx)\\right]\\|\\mu_t\\|^2 x + \\left(\\tanh'(\\mu_t^Tx)\\mu_t^Tx\\right) - \\left(\\tanh'(\\mu_t^Tx)\\mu_t\\right) \\\\\n& = E_{x\\sim N(0,I)}\\left[1 - \\frac{1}{2}\\tanh''(\\mu_t^T(x + \\mu^*_t))\\|\\mu_t\\|^2(x + \\mu^*_t) + \\frac{1}{2}\\tanh'(\\mu_t^T(x + \\mu^*_t))(\\mu_t^T(x + \\mu^*_t))(x + \\mu^*_t) - \\tanh'(\\mu_t^T(x + \\mu^*_t))\\mu_t\\right] \\\\\n& = E_{\\alpha_1,\\alpha_2,...,\\alpha_d\\sim N(0,1)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|^2\\sum_{i}\\alpha_iv_i + \\mu^*_t + \\frac{1}{2}\\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t)\\sum_{i}\\alpha_iv_i + \\mu^*_t - \\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\mu_t\\right]\n\\end{align*}\n$$\nwhere in the last equality we rewrote $$x \\sim N(0, I)$$ as $$\\sum_{i=1}^{d}\\alpha_iv_i$$ for $$\\alpha_i \\sim N(0, 1)$$. Therefore, we have\n\n$$\n\\langle\\hat{\\mu}_t, G(\\mu_t, \\mu^*_t)\\rangle = E_{\\alpha_1,\\alpha_2,...,\\alpha_d\\sim N(0,I)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|^2(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t) + \\frac{1}{2}\\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t)^2 - \\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|\\right]\n$$\n$$\n= E_{\\alpha_1\\sim N(\\hat{\\mu}_t^T\\mu^*_t,1)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2\\alpha_1 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_2\\right]\n$$\nBy taking $$\\|\\mu_t\\|$$ to be $$\\mu$$ and $$\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle$$ to be $$\\mu^*$$, we observe the similarity between the right side of the above equation and the one-dimensional definition of G defined in Eq. (C.2). Using Lemma C.8 and if $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle]$$, we have\n\n$$\n\\langle\\hat{\\mu}_t, G(\\mu_t, \\mu^*_t)\\rangle \\leq 0.01\\frac{\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle}{27}\n$$", "images": [], "items": [{"type": "text", "value": "and we have\n\nThe proof uses the fact that the function G only contains fi $$G(\\mu, \\mu^*) \\leq 0.01 \\frac{\\mu - \\mu^*}{\\text{rst or higher-order derivatives of the tanh function}}$$ and all the derivatives of tanh decay exponential quickly as $$\\mu$$ increases. Therefore, when $$\\mu$$ is at least a constant, we obtain the result. The complete proof of lemma C.8 is given in Appendix F.2.\n\nLemma C.9 (Multi-dimensional version). For any noise scale t, when the current parameter at noise scale t, $$\\mu_t$$, satisfies $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle]$$ for some sufficiently large constant c, then the following inequality holds:\n\n$$\nG(\\mu_t, \\mu^*_t) \\leq 0.01 \\frac{\\mu_t - \\mu^*_t}{3}\n$$\nProof. Suppose {v1, v2, . . . , vd} are d orthonormal directions such that $$v_1 = \\hat{\\mu}_t$$ and $$v_2$$ is either of the two unit vectors $$\\hat{\\mu}_\\perp_t$$ which are orthogonal to $$\\hat{\\mu}_t$$ in the plane of $$\\mu_t$$ and $$\\mu^*_t$$. Recall that\n\n$$\n\\begin{align*}\nG(\\mu_t, \\mu^*_t) & = E_{x\\sim N(\\mu^*_t,Id)}\\left[1 - \\tanh(\\mu_t^Tx)\\right]\\|\\mu_t\\|^2 x + \\left(\\tanh'(\\mu_t^Tx)\\mu_t^Tx\\right) - \\left(\\tanh'(\\mu_t^Tx)\\mu_t\\right) \\\\\n& = E_{x\\sim N(0,I)}\\left[1 - \\frac{1}{2}\\tanh''(\\mu_t^T(x + \\mu^*_t))\\|\\mu_t\\|^2(x + \\mu^*_t) + \\frac{1}{2}\\tanh'(\\mu_t^T(x + \\mu^*_t))(\\mu_t^T(x + \\mu^*_t))(x + \\mu^*_t) - \\tanh'(\\mu_t^T(x + \\mu^*_t))\\mu_t\\right] \\\\\n& = E_{\\alpha_1,\\alpha_2,...,\\alpha_d\\sim N(0,1)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|^2\\sum_{i}\\alpha_iv_i + \\mu^*_t + \\frac{1}{2}\\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t)\\sum_{i}\\alpha_iv_i + \\mu^*_t - \\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\mu_t\\right]\n\\end{align*}\n$$\nwhere in the last equality we rewrote $$x \\sim N(0, I)$$ as $$\\sum_{i=1}^{d}\\alpha_iv_i$$ for $$\\alpha_i \\sim N(0, 1)$$. Therefore, we have\n\n$$\n\\langle\\hat{\\mu}_t, G(\\mu_t, \\mu^*_t)\\rangle = E_{\\alpha_1,\\alpha_2,...,\\alpha_d\\sim N(0,I)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|^2(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t) + \\frac{1}{2}\\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t)^2 - \\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|\\right]\n$$\n$$\n= E_{\\alpha_1\\sim N(\\hat{\\mu}_t^T\\mu^*_t,1)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2\\alpha_1 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_2\\right]\n$$\nBy taking $$\\|\\mu_t\\|$$ to be $$\\mu$$ and $$\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle$$ to be $$\\mu^*$$, we observe the similarity between the right side of the above equation and the one-dimensional definition of G defined in Eq. (C.2). Using Lemma C.8 and if $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle]$$, we have\n\n$$\n\\langle\\hat{\\mu}_t, G(\\mu_t, \\mu^*_t)\\rangle \\leq 0.01\\frac{\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle}{27}\n$$", "md": "and we have\n\nThe proof uses the fact that the function G only contains fi $$G(\\mu, \\mu^*) \\leq 0.01 \\frac{\\mu - \\mu^*}{\\text{rst or higher-order derivatives of the tanh function}}$$ and all the derivatives of tanh decay exponential quickly as $$\\mu$$ increases. Therefore, when $$\\mu$$ is at least a constant, we obtain the result. The complete proof of lemma C.8 is given in Appendix F.2.\n\nLemma C.9 (Multi-dimensional version). For any noise scale t, when the current parameter at noise scale t, $$\\mu_t$$, satisfies $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle]$$ for some sufficiently large constant c, then the following inequality holds:\n\n$$\nG(\\mu_t, \\mu^*_t) \\leq 0.01 \\frac{\\mu_t - \\mu^*_t}{3}\n$$\nProof. Suppose {v1, v2, . . . , vd} are d orthonormal directions such that $$v_1 = \\hat{\\mu}_t$$ and $$v_2$$ is either of the two unit vectors $$\\hat{\\mu}_\\perp_t$$ which are orthogonal to $$\\hat{\\mu}_t$$ in the plane of $$\\mu_t$$ and $$\\mu^*_t$$. Recall that\n\n$$\n\\begin{align*}\nG(\\mu_t, \\mu^*_t) & = E_{x\\sim N(\\mu^*_t,Id)}\\left[1 - \\tanh(\\mu_t^Tx)\\right]\\|\\mu_t\\|^2 x + \\left(\\tanh'(\\mu_t^Tx)\\mu_t^Tx\\right) - \\left(\\tanh'(\\mu_t^Tx)\\mu_t\\right) \\\\\n& = E_{x\\sim N(0,I)}\\left[1 - \\frac{1}{2}\\tanh''(\\mu_t^T(x + \\mu^*_t))\\|\\mu_t\\|^2(x + \\mu^*_t) + \\frac{1}{2}\\tanh'(\\mu_t^T(x + \\mu^*_t))(\\mu_t^T(x + \\mu^*_t))(x + \\mu^*_t) - \\tanh'(\\mu_t^T(x + \\mu^*_t))\\mu_t\\right] \\\\\n& = E_{\\alpha_1,\\alpha_2,...,\\alpha_d\\sim N(0,1)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|^2\\sum_{i}\\alpha_iv_i + \\mu^*_t + \\frac{1}{2}\\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t)\\sum_{i}\\alpha_iv_i + \\mu^*_t - \\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\mu_t\\right]\n\\end{align*}\n$$\nwhere in the last equality we rewrote $$x \\sim N(0, I)$$ as $$\\sum_{i=1}^{d}\\alpha_iv_i$$ for $$\\alpha_i \\sim N(0, 1)$$. Therefore, we have\n\n$$\n\\langle\\hat{\\mu}_t, G(\\mu_t, \\mu^*_t)\\rangle = E_{\\alpha_1,\\alpha_2,...,\\alpha_d\\sim N(0,I)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|^2(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t) + \\frac{1}{2}\\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t)^2 - \\tanh'(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^T\\mu^*_t))\\|\\mu_t\\|\\right]\n$$\n$$\n= E_{\\alpha_1\\sim N(\\hat{\\mu}_t^T\\mu^*_t,1)}\\left[1 - \\frac{1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2\\alpha_1 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_2\\right]\n$$\nBy taking $$\\|\\mu_t\\|$$ to be $$\\mu$$ and $$\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle$$ to be $$\\mu^*$$, we observe the similarity between the right side of the above equation and the one-dimensional definition of G defined in Eq. (C.2). Using Lemma C.8 and if $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle]$$, we have\n\n$$\n\\langle\\hat{\\mu}_t, G(\\mu_t, \\mu^*_t)\\rangle \\leq 0.01\\frac{\\langle\\hat{\\mu}_t, \\mu^*_t\\rangle}{27}\n$$"}]}, {"page": 28, "text": "Taking the dot product of G(\u00b5t, \u00b5\u2217                             t ) with v2 = \u02c6          \u00b5\u22a5t , we have\n     \u27e8\u02c6                                                                                                            \u00b5\u22a4\n      \u00b5\u22a5 t , G(\u00b5t, \u00b5\u2217      t )\u27e9   = E\u03b11,\u03b12,...,\u03b1d\u223cN (0,1)                 \u2212    1                                      t \u00b5\u2217 t ))\u2225\u00b5t\u22252 (\u03b12 + \u27e8\u02c6            \u00b5\u22a5t , \u00b5\u2217  t \u27e9)\n                                                                               2 tanh\u2032\u2032(\u2225\u00b5t\u2225            (\u03b11 + \u02c6\n                                            + tanh\u2032(\u2225\u00b5t\u2225             (\u03b11 + \u02c6    \u00b5\u22a4 t \u00b5\u2217 t ))\u2225\u00b5t\u2225      (\u03b11 + \u02c6t \u00b5\u2217 \u00b5\u22a4      t )(\u03b12 + \u27e8\u02c6t , \u00b5\u2217\u00b5\u22a5        t\u27e9)\n                                  = E    \u03b11\u223cN (\u02c6    \u00b5\u22a4t \u00b5\u2217t ,1)    \u2212    1                                           \u00b5\u22a5 t , \u00b5\u2217 t \u27e9\n                                                                        2 tanh\u2032\u2032(\u2225\u00b5t\u2225            \u03b11)\u2225\u00b5t\u22252 \u27e8\u02c6\n                                            + tanh\u2032(\u2225\u00b5t\u2225             \u03b11)\u2225\u00b5t\u2225        \u03b11\u27e8\u02c6  \u00b5\u22a5 t , \u00b5\u2217 t \u27e9\n                                  = \u27e8\u02c6  \u00b5\u22a5 t , \u00b5\u2217 t \u27e9 E\u03b11\u223cN (\u02c6      \u00b5\u22a4t \u00b5\u2217t ,1)    \u2212    1                                                                                      .\n                                                                                        2 tanh\u2032\u2032(\u2225\u00b5t\u2225            \u03b11)\u2225\u00b5t\u22252 + tanh\u2032(\u2225\u00b5t\u2225                      \u03b11)\u2225\u00b5t\u2225        \u03b11\nIn Lemma F.5 below, we show that when \u2225\u00b5t\u2225                                              \u2208   [c, 4\u27e8\u02c6  \u00b5t,\u00b5\u2217 t \u27e9], the expectation in the last expression\n                                                                                                       3\nis upper bounded by 0.01. Therefore, we have\n                     \u27e8\u02c6\u00b5\u22a5 t , G(\u00b5t, \u00b5\u2217      t )\u27e9    \u2264   0.01|\u27e8\u02c6    \u00b5\u22a5t , \u00b5\u2217  t \u27e9| =   \u21d2       \u27e8\u02c6\u00b5\u22a5 t , G(\u00b5t, \u00b5\u2217      t )\u27e9    \u2264   0.01 \u27e8\u02c6    \u00b5\u22a5 t , \u00b5t \u2212      \u00b5\u2217t \u27e9\nObserve that for i = 3, . . . , d, \u27e8G(\u00b5t, \u00b5\u2217                           t ), vi\u27e9    = 0. Therefore, we have\n                                                                         d\n                                          G(\u00b5t, \u00b5\u2217       t ) 2 =       i=1   \u27e8vi, G(\u00b5t, \u00b5\u2217       t )\u27e92 \u2264      0.012      \u00b5t \u2212      \u00b5\u2217 t  2 .\nThe next Lemma ensures that the parameter \u00b5t after a few steps of gradient descent on the DDPM\nobjective stays in the region where the function G satisfi                                              es   G(\u00b5t, \u00b5\u2217       t )    \u2264   0.01\u2225\u00b5t \u2212          \u00b5\u2217 t \u2225. Recall that\nthe condition of the Lemma is satisfi                               ed because we initialize at the warm start obtained by gradient\ndescent in the high noise regime.\nLemma C.10. Suppose the angle between initialization \u02c6                                                  \u00b5(0)    and optimal parameter \u00b5\u2217                     t is \u0398(1), then\n                                                              \u00b5(h)  ,\u00b5\u2217                                   t\nfor any h, we have \u2225\u00b5(h)\u2225                t       \u2208   [c, 4\u27e8\u02c6    t3     t \u27e9].\nThe proof of Lemma C.10 is given in Appendix F.3. Finally, we are ready to prove the main result\nof this section:\nProof of Theorem C.1. To obtain the contraction of \u2225\u00b5(h)                                              t    \u2212   \u00b5\u2217 t \u2225  after a gradient descent step on the\nDDPM objective, we write \u2225\u00b5(h+1)                        t         \u2212   \u00b5\u2217 t \u2225  in terms of \u2225\u00b5(h)          t    \u2212    \u00b5\u2217t \u2225   as follows:\n          \u00b5(h+1)        \u2212   \u00b5\u2217 t    =     \u00b5(h)     \u2212    \u03b7\u2207Lt(s\u00b5(h)                   t     + \u03b7         1     n   \u2207Lt(s\u00b5(h)                           \u2212   \u2207Lt(s\u00b5(h)\n              t                               t                         t ) \u2212      \u00b5\u2217                  n   i=1                 t (xi, zi))                             t )\n          \u2264    (1 \u2212     \u03b7)\u2225\u00b5(h) t     \u2212   \u00b5\u2217 t \u2225  + \u03b7     Ex\u223cN (\u00b5\u2217        t ,1)[(tanh(\u00b5(h)\u22a4    t      x))x] \u2212        \u00b5\u2217 t    + \u03b7\u2225G(\u00b5(h)      t    , \u00b5\u2217t )\u2225   + \u03b7\u03b5 ,\nwhere in the last step we used Lemma E.7 below to bound the distance between the population and\nempirical gradient.\n       Recall that gradient descent in the low noise regime was initialized using the output of the\ngradient descent in the high noise regime. Therefore, \u27e8\u02c6                                               \u00b5(0)     \u00b5\u2217t \u27e9  \u2273    1. Using Lemma C.10, we know\n                                                                                                          t , \u02c6\nthat the condition on Lemma C.8 is always satisfi                                              ed. Using the contractivity of G established in\nLemma C.8 combined with [DTZ17, Theorem 2], and choosing \u03b7 = 0.05, we conclude that the\ndistance to the ground truth contracts:\n                  \u00b5(h+1)        \u2212   \u00b5\u2217t     \u2264   (1 \u2212      0.05) \u00b5(h)         \u2212    \u00b5\u2217t     + 0.01 \u00b5(h)           \u2212   \u00b5\u2217 t    + 0.01 \u00b5(h)            \u2212   \u00b5\u2217 t    + \u03b7\u03b5\n                      t                                                 t                                  t                                 t\n                                            \u2264   0.97     \u00b5(h)t     \u2212   \u00b5\u2217 t    + \u03b7\u03b5.      28", "md": "# Math Equations\n\nTaking the dot product of $$G(\\mu_t, \\mu_t^*)$$ with $$v_2 = \\hat{\\mu}_t^{\\perp}$$, we have\n\n$$\n\\begin{align*}\n\\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle & = E_{\\alpha_1,\\alpha_2,...,\\alpha_d \\sim N(0,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^{\\top}\\mu_t^*))\\|\\mu_t\\|^2(\\alpha_2 + \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle)\\right] \\\\\n& = \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle E_{\\alpha_1 \\sim N(\\hat{\\mu}_t^{\\top}\\mu_t^*,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_1\\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle\\right] \\\\\n& = \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle E_{\\alpha_1 \\sim N(\\hat{\\mu}_t^{\\top}\\mu_t^*,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_1\\right].\n\\end{align*}\n$$\nIn Lemma F.5 below, we show that when $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu_t^*\\rangle]$$, the expectation in the last expression is upper bounded by 0.01. Therefore, we have\n\n$$\\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle \\leq 0.01|\\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle| \\Rightarrow \\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle \\leq 0.01 \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t - \\mu_t^*\\rangle$$\n\nObserve that for $$i = 3, ..., d$$, $$\\langle G(\\mu_t, \\mu_t^*), v_i\\rangle = 0$$. Therefore, we have\n\n$$\nG(\\mu_t, \\mu_t^*)^2 = \\sum_{i=1}^{d} \\langle v_i, G(\\mu_t, \\mu_t^*)\\rangle^2 \\leq 0.01^2\\|\\mu_t - \\mu_t^*\\|^2.\n$$\nThe next Lemma ensures that the parameter $$\\mu_t$$ after a few steps of gradient descent on the DDPM objective stays in the region where the function G satisfies $$G(\\mu_t, \\mu_t^*) \\leq 0.01\\|\\mu_t - \\mu_t^*\\|$$. Recall that the condition of the Lemma is satisfied because we initialize at the warm start obtained by gradient descent in the high noise regime.\n\nLemma C.10. Suppose the angle between initialization $$\\hat{\\mu}(0)$$ and optimal parameter $$\\mu_t^*$$ is $$\\Theta(1)$$, then for any $$h$$, we have $$\\|\\mu(h)\\|_t \\in [c, 4\\langle\\hat{t}_3, t\\rangle]$$.\n\nThe proof of Lemma C.10 is given in Appendix F.3. Finally, we are ready to prove the main result of this section:\n\nProof of Theorem C.1. To obtain the contraction of $$\\|\\mu(h)_t - \\mu_t^*\\|$$ after a gradient descent step on the DDPM objective, we write $$\\|\\mu(h+1)_t - \\mu_t^*\\|$$ in terms of $$\\|\\mu(h)_t - \\mu_t^*\\|$$ as follows:\n\n$$\n\\begin{align*}\n\\|\\mu(h+1)_t - \\mu_t^*\\| & = \\|\\mu(h)_t - \\eta\\nabla L_t(s\\mu(h)_t + \\eta\\frac{1}{n}\\sum_{i=1}^{n}\\nabla L_t(s\\mu(h)_t - \\nabla L_t(s\\mu(h)_t)(x_i, z_i))\\| \\\\\n& \\leq (1 - \\eta)\\|\\mu(h)_t - \\mu_t^*\\| + \\eta E_{x \\sim N(\\mu_t^*,1)}\\left[\\left(\\tanh(\\mu(h)^{\\top}_t x)x\\right)\\right] - \\mu_t^* + \\eta\\|G(\\mu(h)_t, \\mu_t^*)\\| + \\eta\\epsilon,\n\\end{align*}\n$$\nRecall that gradient descent in the low noise regime was initialized using the output of the gradient descent in the high noise regime. Therefore, $$\\langle\\hat{\\mu}(0), \\mu_t^*\\rangle \\gtrapprox 1$$. Using Lemma C.10, we know that the condition on Lemma C.8 is always satisfied. Using the contractivity of G established in Lemma C.8 combined with [DTZ17, Theorem 2], and choosing $$\\eta = 0.05$$, we conclude that the distance to the ground truth contracts:\n\n$$\n\\begin{align*}\n\\|\\mu(h+1)_t - \\mu_t^*\\| & \\leq (1 - 0.05)\\|\\mu(h)_t - \\mu_t^*\\| + 0.01\\|\\mu(h)_t - \\mu_t^*\\| + 0.01\\|\\mu(h)_t - \\mu_t^*\\| + \\eta\\epsilon \\\\\n& \\leq 0.97\\|\\mu(h)_t - \\mu_t^*\\| + \\eta\\epsilon.\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Taking the dot product of $$G(\\mu_t, \\mu_t^*)$$ with $$v_2 = \\hat{\\mu}_t^{\\perp}$$, we have\n\n$$\n\\begin{align*}\n\\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle & = E_{\\alpha_1,\\alpha_2,...,\\alpha_d \\sim N(0,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^{\\top}\\mu_t^*))\\|\\mu_t\\|^2(\\alpha_2 + \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle)\\right] \\\\\n& = \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle E_{\\alpha_1 \\sim N(\\hat{\\mu}_t^{\\top}\\mu_t^*,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_1\\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle\\right] \\\\\n& = \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle E_{\\alpha_1 \\sim N(\\hat{\\mu}_t^{\\top}\\mu_t^*,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_1\\right].\n\\end{align*}\n$$\nIn Lemma F.5 below, we show that when $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu_t^*\\rangle]$$, the expectation in the last expression is upper bounded by 0.01. Therefore, we have\n\n$$\\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle \\leq 0.01|\\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle| \\Rightarrow \\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle \\leq 0.01 \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t - \\mu_t^*\\rangle$$\n\nObserve that for $$i = 3, ..., d$$, $$\\langle G(\\mu_t, \\mu_t^*), v_i\\rangle = 0$$. Therefore, we have\n\n$$\nG(\\mu_t, \\mu_t^*)^2 = \\sum_{i=1}^{d} \\langle v_i, G(\\mu_t, \\mu_t^*)\\rangle^2 \\leq 0.01^2\\|\\mu_t - \\mu_t^*\\|^2.\n$$\nThe next Lemma ensures that the parameter $$\\mu_t$$ after a few steps of gradient descent on the DDPM objective stays in the region where the function G satisfies $$G(\\mu_t, \\mu_t^*) \\leq 0.01\\|\\mu_t - \\mu_t^*\\|$$. Recall that the condition of the Lemma is satisfied because we initialize at the warm start obtained by gradient descent in the high noise regime.\n\nLemma C.10. Suppose the angle between initialization $$\\hat{\\mu}(0)$$ and optimal parameter $$\\mu_t^*$$ is $$\\Theta(1)$$, then for any $$h$$, we have $$\\|\\mu(h)\\|_t \\in [c, 4\\langle\\hat{t}_3, t\\rangle]$$.\n\nThe proof of Lemma C.10 is given in Appendix F.3. Finally, we are ready to prove the main result of this section:\n\nProof of Theorem C.1. To obtain the contraction of $$\\|\\mu(h)_t - \\mu_t^*\\|$$ after a gradient descent step on the DDPM objective, we write $$\\|\\mu(h+1)_t - \\mu_t^*\\|$$ in terms of $$\\|\\mu(h)_t - \\mu_t^*\\|$$ as follows:\n\n$$\n\\begin{align*}\n\\|\\mu(h+1)_t - \\mu_t^*\\| & = \\|\\mu(h)_t - \\eta\\nabla L_t(s\\mu(h)_t + \\eta\\frac{1}{n}\\sum_{i=1}^{n}\\nabla L_t(s\\mu(h)_t - \\nabla L_t(s\\mu(h)_t)(x_i, z_i))\\| \\\\\n& \\leq (1 - \\eta)\\|\\mu(h)_t - \\mu_t^*\\| + \\eta E_{x \\sim N(\\mu_t^*,1)}\\left[\\left(\\tanh(\\mu(h)^{\\top}_t x)x\\right)\\right] - \\mu_t^* + \\eta\\|G(\\mu(h)_t, \\mu_t^*)\\| + \\eta\\epsilon,\n\\end{align*}\n$$\nRecall that gradient descent in the low noise regime was initialized using the output of the gradient descent in the high noise regime. Therefore, $$\\langle\\hat{\\mu}(0), \\mu_t^*\\rangle \\gtrapprox 1$$. Using Lemma C.10, we know that the condition on Lemma C.8 is always satisfied. Using the contractivity of G established in Lemma C.8 combined with [DTZ17, Theorem 2], and choosing $$\\eta = 0.05$$, we conclude that the distance to the ground truth contracts:\n\n$$\n\\begin{align*}\n\\|\\mu(h+1)_t - \\mu_t^*\\| & \\leq (1 - 0.05)\\|\\mu(h)_t - \\mu_t^*\\| + 0.01\\|\\mu(h)_t - \\mu_t^*\\| + 0.01\\|\\mu(h)_t - \\mu_t^*\\| + \\eta\\epsilon \\\\\n& \\leq 0.97\\|\\mu(h)_t - \\mu_t^*\\| + \\eta\\epsilon.\n\\end{align*}\n$$", "md": "Taking the dot product of $$G(\\mu_t, \\mu_t^*)$$ with $$v_2 = \\hat{\\mu}_t^{\\perp}$$, we have\n\n$$\n\\begin{align*}\n\\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle & = E_{\\alpha_1,\\alpha_2,...,\\alpha_d \\sim N(0,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|(\\alpha_1 + \\hat{\\mu}_t^{\\top}\\mu_t^*))\\|\\mu_t\\|^2(\\alpha_2 + \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle)\\right] \\\\\n& = \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle E_{\\alpha_1 \\sim N(\\hat{\\mu}_t^{\\top}\\mu_t^*,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_1\\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle\\right] \\\\\n& = \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle E_{\\alpha_1 \\sim N(\\hat{\\mu}_t^{\\top}\\mu_t^*,1)}\\left[\\frac{-1}{2}\\tanh''(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|^2 + \\tanh'(\\|\\mu_t\\|\\alpha_1)\\|\\mu_t\\|\\alpha_1\\right].\n\\end{align*}\n$$\nIn Lemma F.5 below, we show that when $$\\|\\mu_t\\| \\in [c, 4\\langle\\hat{\\mu}_t, \\mu_t^*\\rangle]$$, the expectation in the last expression is upper bounded by 0.01. Therefore, we have\n\n$$\\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle \\leq 0.01|\\langle\\hat{\\mu}_t^{\\perp}, \\mu_t^*\\rangle| \\Rightarrow \\langle\\hat{\\mu}_t^{\\perp}, G(\\mu_t, \\mu_t^*)\\rangle \\leq 0.01 \\langle\\hat{\\mu}_t^{\\perp}, \\mu_t - \\mu_t^*\\rangle$$\n\nObserve that for $$i = 3, ..., d$$, $$\\langle G(\\mu_t, \\mu_t^*), v_i\\rangle = 0$$. Therefore, we have\n\n$$\nG(\\mu_t, \\mu_t^*)^2 = \\sum_{i=1}^{d} \\langle v_i, G(\\mu_t, \\mu_t^*)\\rangle^2 \\leq 0.01^2\\|\\mu_t - \\mu_t^*\\|^2.\n$$\nThe next Lemma ensures that the parameter $$\\mu_t$$ after a few steps of gradient descent on the DDPM objective stays in the region where the function G satisfies $$G(\\mu_t, \\mu_t^*) \\leq 0.01\\|\\mu_t - \\mu_t^*\\|$$. Recall that the condition of the Lemma is satisfied because we initialize at the warm start obtained by gradient descent in the high noise regime.\n\nLemma C.10. Suppose the angle between initialization $$\\hat{\\mu}(0)$$ and optimal parameter $$\\mu_t^*$$ is $$\\Theta(1)$$, then for any $$h$$, we have $$\\|\\mu(h)\\|_t \\in [c, 4\\langle\\hat{t}_3, t\\rangle]$$.\n\nThe proof of Lemma C.10 is given in Appendix F.3. Finally, we are ready to prove the main result of this section:\n\nProof of Theorem C.1. To obtain the contraction of $$\\|\\mu(h)_t - \\mu_t^*\\|$$ after a gradient descent step on the DDPM objective, we write $$\\|\\mu(h+1)_t - \\mu_t^*\\|$$ in terms of $$\\|\\mu(h)_t - \\mu_t^*\\|$$ as follows:\n\n$$\n\\begin{align*}\n\\|\\mu(h+1)_t - \\mu_t^*\\| & = \\|\\mu(h)_t - \\eta\\nabla L_t(s\\mu(h)_t + \\eta\\frac{1}{n}\\sum_{i=1}^{n}\\nabla L_t(s\\mu(h)_t - \\nabla L_t(s\\mu(h)_t)(x_i, z_i))\\| \\\\\n& \\leq (1 - \\eta)\\|\\mu(h)_t - \\mu_t^*\\| + \\eta E_{x \\sim N(\\mu_t^*,1)}\\left[\\left(\\tanh(\\mu(h)^{\\top}_t x)x\\right)\\right] - \\mu_t^* + \\eta\\|G(\\mu(h)_t, \\mu_t^*)\\| + \\eta\\epsilon,\n\\end{align*}\n$$\nRecall that gradient descent in the low noise regime was initialized using the output of the gradient descent in the high noise regime. Therefore, $$\\langle\\hat{\\mu}(0), \\mu_t^*\\rangle \\gtrapprox 1$$. Using Lemma C.10, we know that the condition on Lemma C.8 is always satisfied. Using the contractivity of G established in Lemma C.8 combined with [DTZ17, Theorem 2], and choosing $$\\eta = 0.05$$, we conclude that the distance to the ground truth contracts:\n\n$$\n\\begin{align*}\n\\|\\mu(h+1)_t - \\mu_t^*\\| & \\leq (1 - 0.05)\\|\\mu(h)_t - \\mu_t^*\\| + 0.01\\|\\mu(h)_t - \\mu_t^*\\| + 0.01\\|\\mu(h)_t - \\mu_t^*\\| + \\eta\\epsilon \\\\\n& \\leq 0.97\\|\\mu(h)_t - \\mu_t^*\\| + \\eta\\epsilon.\n\\end{align*}\n$$"}]}, {"page": 29, "text": "Applying the above for all h \u2208                  [H], we obtain\n                                               \u2225\u00b5(H)    \u2212   \u00b5\u2217t \u2225 \u2264   0.97H\u2225\u00b5(0)       \u2212   \u00b5\u2217t \u2225 + 50\u03b5.\n                                                  t                                t\nThe choice of H given in the Theorem statement proves the result.\nD        Learning mixtures of two Gaussians with small separation\nIn this section, we extend the analysis for learning mixtures of two Gaussians with constant separa-\ntion, provided in Section C, to the low-separation regime and prove the following:\nTheorem D.1 (Formal version of Theorem 13). For any L > 0, let q be a mixture of two Gaussians\n(in the form of Eq. (7)) with mean parameter \u00b5\u2217                              satisfying \u2225\u00b5\u2217\u2225         > L. Recalling that B denotes an\n                                                                                                               1\na priori upper bound on \u2225\u00b5\u2217\u2225, we have that for any \u03b5 \u2264                                \u03b5\u2032, where \u03b5\u2032 \u2272         d2B9 , there exists a procedure\nsatisfying the following. If the procedure is run for at least poly(d, B, 1                                   L) 1\u03b53 iterations with at least\npoly(d, B, 1     L) \u2217    1                                                    \u00b5 such that \u2225\u02dc      \u00b5 \u2212    \u00b5\u2217\u2225   \u2264   \u03b5 with high probability.\n                        \u03b58 samples from q, then it outputs \u02dc\nAs described in Section 1.2, the algorithm is a simple modifi                               cation of Algorithm 1 in which gradient\ndescent is replaced by projected gradient descent. We start in Lemma D.2 by showing that the\nprojection step in the algorithm ensures that the norm of the current iterate \u00b5t is approximately\nthat of \u00b5\u2217    t. Then in Lemma D.3, we extend the analysis of Lemma C.5 to show that every projected\ngradient step contracts the distance to the ground truth. Combined with Lemma D.2, this allows\nus to conclude the proof of Theorem 13.\nLemma D.2. Let x1, . . . , xn be independent samples from q, and define radius parameter R by\nR2 \u225c      1  n  i=1 \u2225xi\u22252 \u2212       d. For any \u03b5 > 0, provided that n \u2273                      B4+d2\n          n                                                                                 \u03b52L2 , we have |R \u2212            \u2225\u00b5\u2217\u2225| \u2264      \u03b5 with high\nprobability.\nProof. Observe that we can write the random variable corresponding to the mixture of two Gaussians\nX0 = X = Z + p\u00b5\u2217                where Z \u223c          N  (0, I) and p is a Rademacher random variable. Using Theorem\n3.1.1 (concentration of norms) from [Ver], we know that \u2225\u2225Z\u2225                                \u221a          \u2212   \u221a  d\u2225\u03c82 \u2272        1.    Therefore, sub-\nGaussian norm            \u2225X0\u2225  \u2225X0\u2225 2\u03c82 \u2272     \u2225Z\u2225      \u03c82 +    \u2225p\u00b5\u2217\u2225       \u03c82 \u2272     B +        d. Using Lemma 2.7.4 from [Ver], we\nhave     \u2225X0\u22252        \u03c81 \u2272                 \u03c82 \u2272     B2 + d. Therefore, using number of samples n specifi                                   ed in the\nLemma statement, with high probability, we have\n                n1   n   \u2225xi\u22252 \u2212     E[\u2225X0\u22252]        \u2264   \u03b5L =     \u21d2     \u2225\u00b5\u22252 \u2212      \u2225\u00b5\u2217\u22252      \u2264   \u03b5L =    \u21d2      \u2225\u00b5\u2225   \u2212   \u2225\u00b5\u2217\u2225     \u2264   \u03b5\n                    i=1\nwhere the penultimate implication uses the fact that EX                               0[\u2225X0\u22252] = E[\u2225Z\u22252+\u2225\u00b5\u2217\u22252] = d+\u2225\u00b5\u2217\u22252.\nLemma D.3. Assume that L \u2264                          \u2225\u00b5\u2217\u2225  1  \u2264   B. Then, for any small \u03b5 > 0, running projected GD on\ndiff usion models with step size \u03b7 =                     20 at noise scale t = log d             \u03b5 for number of steps H > H\u2032 and\nnumber of samples n > n\u2032 steps will achieve\n                     d2                                      \u00b5(H) \u2212      \u00b5\u2217     \u2272  d2B4\u03b5,\nwhere H\u2032 =         L2\u03b53 and n\u2032 = d10B3     \u03b58L6 .\n                                                                           29", "md": "Applying the above for all \\( h \\in [H] \\), we obtain\n\n\\[ \\| \\mu(H) - \\mu^*_t \\| \\leq 0.97H \\| \\mu(0) - \\mu^*_t \\| + 50\\epsilon_t. \\]\nThe choice of \\( H \\) given in the Theorem statement proves the result.\n\n## Learning mixtures of two Gaussians with small separation\n\nIn this section, we extend the analysis for learning mixtures of two Gaussians with constant separation, provided in Section C, to the low-separation regime and prove the following:\n\nTheorem D.1 (Formal version of Theorem 13). For any \\( L > 0 \\), let \\( q \\) be a mixture of two Gaussians (in the form of Eq. (7)) with mean parameter \\( \\mu^* \\) satisfying \\( \\| \\mu^* \\| > L \\). Recalling that \\( B \\) denotes an a priori upper bound on \\( \\| \\mu^* \\| \\), we have that for any \\( \\epsilon \\leq \\epsilon' \\), where \\( \\epsilon' \\lesssim \\frac{d^2B}{9} \\), there exists a procedure satisfying the following. If the procedure is run for at least \\( \\text{poly}(d, B, \\frac{1}{L}) \\epsilon^3 \\) iterations with at least \\( \\text{poly}(d, B, \\frac{1}{L}) \\) samples from \\( q \\), then it outputs \\( \\tilde{\\mu} \\) such that \\( \\| \\tilde{\\mu} - \\mu^* \\| \\leq \\epsilon \\) with high probability.\n\nAs described in Section 1.2, the algorithm is a simple modification of Algorithm 1 in which gradient descent is replaced by projected gradient descent. We start in Lemma D.2 by showing that the projection step in the algorithm ensures that the norm of the current iterate \\( \\mu_t \\) is approximately that of \\( \\mu^*_t \\). Then in Lemma D.3, we extend the analysis of Lemma C.5 to show that every projected gradient step contracts the distance to the ground truth. Combined with Lemma D.2, this allows us to conclude the proof of Theorem 13.\n\nLemma D.2. Let \\( x_1, \\ldots, x_n \\) be independent samples from \\( q \\), and define radius parameter \\( R \\) by \\( R^2 \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i\\|^2 - d \\). For any \\( \\epsilon > 0 \\), provided that \\( n \\gtrsim \\frac{B^4+d^2}{\\epsilon^2L^2} \\), we have \\( |R - \\| \\mu^* \\| | \\leq \\epsilon \\) with high probability.\n\nProof. Observe that we can write the random variable corresponding to the mixture of two Gaussians \\( X_0 = X = Z + p\\mu^* \\) where \\( Z \\sim \\mathcal{N}(0, I) \\) and \\( p \\) is a Rademacher random variable. Using Theorem 3.1.1 (concentration of norms) from [Ver], we know that \\( \\|Z\\| \\sqrt{2} - \\sqrt{d}\\|_{\\psi_2} \\lesssim 1 \\). Therefore, sub-Gaussian norm \\( \\|X_0\\| \\|X_0\\|_{2\\psi_2} \\lesssim \\|Z\\|_{\\psi_2} + \\|p\\mu^*\\|_{\\psi_2} \\lesssim B + d \\). Using Lemma 2.7.4 from [Ver], we have \\( \\|X_0\\|^2_{\\psi_1} \\lesssim \\|X_0\\|^2_{\\psi_2} \\lesssim B^2 + d \\). Therefore, using number of samples \\( n \\) specified in the Lemma statement, with high probability, we have\n\\[ \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i\\|^2 - E[\\|X_0\\|^2] \\leq \\epsilon L \\Rightarrow \\| \\mu \\|^2 - \\| \\mu^* \\|^2 \\leq \\epsilon L \\Rightarrow \\| \\mu \\| - \\| \\mu^* \\| \\leq \\epsilon \\]\nwhere the penultimate implication uses the fact that \\( E[X_0][\\|X_0\\|^2] = E[\\|Z\\|^2+\\| \\mu^* \\|^2] = d+\\| \\mu^* \\|^2 \\).\n\nLemma D.3. Assume that \\( L \\leq \\| \\mu^* \\| \\leq B \\). Then, for any small \\( \\epsilon > 0 \\), running projected GD on diffusion models with step size \\( \\eta = 20 \\) at noise scale \\( t = \\log d \\epsilon \\) for number of steps \\( H > H' \\) and number of samples \\( n > n' \\) steps will achieve\n\\[ \\| \\mu(H) - \\mu^* \\| \\lesssim d^2B^4\\epsilon, \\]\nwhere \\( H' = L^2\\epsilon^3 \\) and \\( n' = d^{10}B^3 \\epsilon^8L^6 \\).", "images": [], "items": [{"type": "text", "value": "Applying the above for all \\( h \\in [H] \\), we obtain\n\n\\[ \\| \\mu(H) - \\mu^*_t \\| \\leq 0.97H \\| \\mu(0) - \\mu^*_t \\| + 50\\epsilon_t. \\]\nThe choice of \\( H \\) given in the Theorem statement proves the result.", "md": "Applying the above for all \\( h \\in [H] \\), we obtain\n\n\\[ \\| \\mu(H) - \\mu^*_t \\| \\leq 0.97H \\| \\mu(0) - \\mu^*_t \\| + 50\\epsilon_t. \\]\nThe choice of \\( H \\) given in the Theorem statement proves the result."}, {"type": "heading", "lvl": 2, "value": "Learning mixtures of two Gaussians with small separation", "md": "## Learning mixtures of two Gaussians with small separation"}, {"type": "text", "value": "In this section, we extend the analysis for learning mixtures of two Gaussians with constant separation, provided in Section C, to the low-separation regime and prove the following:\n\nTheorem D.1 (Formal version of Theorem 13). For any \\( L > 0 \\), let \\( q \\) be a mixture of two Gaussians (in the form of Eq. (7)) with mean parameter \\( \\mu^* \\) satisfying \\( \\| \\mu^* \\| > L \\). Recalling that \\( B \\) denotes an a priori upper bound on \\( \\| \\mu^* \\| \\), we have that for any \\( \\epsilon \\leq \\epsilon' \\), where \\( \\epsilon' \\lesssim \\frac{d^2B}{9} \\), there exists a procedure satisfying the following. If the procedure is run for at least \\( \\text{poly}(d, B, \\frac{1}{L}) \\epsilon^3 \\) iterations with at least \\( \\text{poly}(d, B, \\frac{1}{L}) \\) samples from \\( q \\), then it outputs \\( \\tilde{\\mu} \\) such that \\( \\| \\tilde{\\mu} - \\mu^* \\| \\leq \\epsilon \\) with high probability.\n\nAs described in Section 1.2, the algorithm is a simple modification of Algorithm 1 in which gradient descent is replaced by projected gradient descent. We start in Lemma D.2 by showing that the projection step in the algorithm ensures that the norm of the current iterate \\( \\mu_t \\) is approximately that of \\( \\mu^*_t \\). Then in Lemma D.3, we extend the analysis of Lemma C.5 to show that every projected gradient step contracts the distance to the ground truth. Combined with Lemma D.2, this allows us to conclude the proof of Theorem 13.\n\nLemma D.2. Let \\( x_1, \\ldots, x_n \\) be independent samples from \\( q \\), and define radius parameter \\( R \\) by \\( R^2 \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i\\|^2 - d \\). For any \\( \\epsilon > 0 \\), provided that \\( n \\gtrsim \\frac{B^4+d^2}{\\epsilon^2L^2} \\), we have \\( |R - \\| \\mu^* \\| | \\leq \\epsilon \\) with high probability.\n\nProof. Observe that we can write the random variable corresponding to the mixture of two Gaussians \\( X_0 = X = Z + p\\mu^* \\) where \\( Z \\sim \\mathcal{N}(0, I) \\) and \\( p \\) is a Rademacher random variable. Using Theorem 3.1.1 (concentration of norms) from [Ver], we know that \\( \\|Z\\| \\sqrt{2} - \\sqrt{d}\\|_{\\psi_2} \\lesssim 1 \\). Therefore, sub-Gaussian norm \\( \\|X_0\\| \\|X_0\\|_{2\\psi_2} \\lesssim \\|Z\\|_{\\psi_2} + \\|p\\mu^*\\|_{\\psi_2} \\lesssim B + d \\). Using Lemma 2.7.4 from [Ver], we have \\( \\|X_0\\|^2_{\\psi_1} \\lesssim \\|X_0\\|^2_{\\psi_2} \\lesssim B^2 + d \\). Therefore, using number of samples \\( n \\) specified in the Lemma statement, with high probability, we have\n\\[ \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i\\|^2 - E[\\|X_0\\|^2] \\leq \\epsilon L \\Rightarrow \\| \\mu \\|^2 - \\| \\mu^* \\|^2 \\leq \\epsilon L \\Rightarrow \\| \\mu \\| - \\| \\mu^* \\| \\leq \\epsilon \\]\nwhere the penultimate implication uses the fact that \\( E[X_0][\\|X_0\\|^2] = E[\\|Z\\|^2+\\| \\mu^* \\|^2] = d+\\| \\mu^* \\|^2 \\).\n\nLemma D.3. Assume that \\( L \\leq \\| \\mu^* \\| \\leq B \\). Then, for any small \\( \\epsilon > 0 \\), running projected GD on diffusion models with step size \\( \\eta = 20 \\) at noise scale \\( t = \\log d \\epsilon \\) for number of steps \\( H > H' \\) and number of samples \\( n > n' \\) steps will achieve\n\\[ \\| \\mu(H) - \\mu^* \\| \\lesssim d^2B^4\\epsilon, \\]\nwhere \\( H' = L^2\\epsilon^3 \\) and \\( n' = d^{10}B^3 \\epsilon^8L^6 \\).", "md": "In this section, we extend the analysis for learning mixtures of two Gaussians with constant separation, provided in Section C, to the low-separation regime and prove the following:\n\nTheorem D.1 (Formal version of Theorem 13). For any \\( L > 0 \\), let \\( q \\) be a mixture of two Gaussians (in the form of Eq. (7)) with mean parameter \\( \\mu^* \\) satisfying \\( \\| \\mu^* \\| > L \\). Recalling that \\( B \\) denotes an a priori upper bound on \\( \\| \\mu^* \\| \\), we have that for any \\( \\epsilon \\leq \\epsilon' \\), where \\( \\epsilon' \\lesssim \\frac{d^2B}{9} \\), there exists a procedure satisfying the following. If the procedure is run for at least \\( \\text{poly}(d, B, \\frac{1}{L}) \\epsilon^3 \\) iterations with at least \\( \\text{poly}(d, B, \\frac{1}{L}) \\) samples from \\( q \\), then it outputs \\( \\tilde{\\mu} \\) such that \\( \\| \\tilde{\\mu} - \\mu^* \\| \\leq \\epsilon \\) with high probability.\n\nAs described in Section 1.2, the algorithm is a simple modification of Algorithm 1 in which gradient descent is replaced by projected gradient descent. We start in Lemma D.2 by showing that the projection step in the algorithm ensures that the norm of the current iterate \\( \\mu_t \\) is approximately that of \\( \\mu^*_t \\). Then in Lemma D.3, we extend the analysis of Lemma C.5 to show that every projected gradient step contracts the distance to the ground truth. Combined with Lemma D.2, this allows us to conclude the proof of Theorem 13.\n\nLemma D.2. Let \\( x_1, \\ldots, x_n \\) be independent samples from \\( q \\), and define radius parameter \\( R \\) by \\( R^2 \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i\\|^2 - d \\). For any \\( \\epsilon > 0 \\), provided that \\( n \\gtrsim \\frac{B^4+d^2}{\\epsilon^2L^2} \\), we have \\( |R - \\| \\mu^* \\| | \\leq \\epsilon \\) with high probability.\n\nProof. Observe that we can write the random variable corresponding to the mixture of two Gaussians \\( X_0 = X = Z + p\\mu^* \\) where \\( Z \\sim \\mathcal{N}(0, I) \\) and \\( p \\) is a Rademacher random variable. Using Theorem 3.1.1 (concentration of norms) from [Ver], we know that \\( \\|Z\\| \\sqrt{2} - \\sqrt{d}\\|_{\\psi_2} \\lesssim 1 \\). Therefore, sub-Gaussian norm \\( \\|X_0\\| \\|X_0\\|_{2\\psi_2} \\lesssim \\|Z\\|_{\\psi_2} + \\|p\\mu^*\\|_{\\psi_2} \\lesssim B + d \\). Using Lemma 2.7.4 from [Ver], we have \\( \\|X_0\\|^2_{\\psi_1} \\lesssim \\|X_0\\|^2_{\\psi_2} \\lesssim B^2 + d \\). Therefore, using number of samples \\( n \\) specified in the Lemma statement, with high probability, we have\n\\[ \\frac{1}{n} \\sum_{i=1}^{n} \\|x_i\\|^2 - E[\\|X_0\\|^2] \\leq \\epsilon L \\Rightarrow \\| \\mu \\|^2 - \\| \\mu^* \\|^2 \\leq \\epsilon L \\Rightarrow \\| \\mu \\| - \\| \\mu^* \\| \\leq \\epsilon \\]\nwhere the penultimate implication uses the fact that \\( E[X_0][\\|X_0\\|^2] = E[\\|Z\\|^2+\\| \\mu^* \\|^2] = d+\\| \\mu^* \\|^2 \\).\n\nLemma D.3. Assume that \\( L \\leq \\| \\mu^* \\| \\leq B \\). Then, for any small \\( \\epsilon > 0 \\), running projected GD on diffusion models with step size \\( \\eta = 20 \\) at noise scale \\( t = \\log d \\epsilon \\) for number of steps \\( H > H' \\) and number of samples \\( n > n' \\) steps will achieve\n\\[ \\| \\mu(H) - \\mu^* \\| \\lesssim d^2B^4\\epsilon, \\]\nwhere \\( H' = L^2\\epsilon^3 \\) and \\( n' = d^{10}B^3 \\epsilon^8L^6 \\)."}]}, {"page": 30, "text": "Proof. Recalling that \u00b5\u2217             t = \u00b5\u2217   0 exp(\u2212t), note that for t = log d\u03b5, \u03b5L                            t \u2225 \u2264    \u03b5B\napply Lemma C.5. Note that we may apply this even though it is only stated for gradient descent       d \u2264    \u2225\u00b5\u2217           d . We would like to\n(without projection). The reason is that it bounds the change in angle between the iterate and the\nground truth after a single gradient step, and this angle is unaffected by projection.\n      Suppose we take one projected gradient step with learning rate \u03b7 starting from an iterate \u00b5t.       \u00b5(h)      \u2272    \u03b5B\nAs \u00b5t was the result of a projection, by Lemma D.2 we have \u03b5L                                     d \u2272        t            d .\n      We now bound \u03ba2 in Lemma C.5:\n                                           \u03ba2 = 500\u03b7       \u221a   d3\u2225\u00b5t\u22254 + 20\u03b7d\u2225\u00b5t\u22252\u2225\u00b5\u2217              t\u22252 + \u03b7\u02dc   \u03b5\n                                                                              \u00b5\u2217 2\n                                                                                 t\n                                                \u2272   500\u03b7   \u221a  d7\u2225\u00b5t\u22252 + 20\u03b7d\u2225\u00b5t\u22252 +                  d2\u03b5\n                                                                                                    \u00b5\u2217 3\n                                                                                                       t\n                                                \u2264   550d7/2B2 exp(\u22122t) + d5\u03b5            \u03b53L3\n                                                \u2272   d2B2\u03b5,\nwhere the last inequality follows by choosing population gradient estimation error parameter \u03b5 =\n \u03b54L3   with the number of samples n\u2032 = d11B6\n  d3                                                          \u03b58L6 . Additionally, \u03ba1 in Lemma C.5 is given by\n                                                                         1 \u2212   3\u03b7\u2225\u00b5t\u22252\n                        \u03ba1 =                                                     \u221a                                 \u00b5\u2217 2 \u2212       \u02dc\n                                 (1 \u2212    3\u03b7\u2225\u00b5t\u22252) + \u03b7(\u2225\u00b5\u2217         t \u22252 \u2212   500      d3\u2225\u00b5t\u22254 \u2212       20d\u2225\u00b5t\u22252          t         \u03b5)\n                             =                  1 \u2212    3\u03b7\u2225\u00b5t\u22252\n                                 (1 \u2212    3\u03b7\u2225\u00b5t\u22252) + \u03b7\u2225\u00b5\u2217         t \u22252(1 \u2212    \u03ba2)\n                                                   1 \u2212    3\u03b7\u2225\u00b5(h)\u22252\n                             \u2272   (1 \u2212    3\u03b7\u2225\u00b5(h)\u22252) + \u03b7\u2225\u00b5\u2217       t\n                                                t                  t \u22252(1 \u2212     d2B2\u03b5)\n                             \u2264   1 + L2\u03b52       1                 .\n                                        20d2 (1 \u2212     d2B2\u03b5)\nUsing bounds on \u03ba1 and \u03ba2 and Lemma C.5, we conclude that if \u03b8 (resp. \u03b8\u2032) is the angle between\n\u00b5t (resp. the next iterate of projected gradient descent after \u00b5t) and \u00b5\u2217                                      t\n                                        tan \u03b8\u2032 \u2264     max        1 + L2\u03b52     1               tan \u03b8, d2B2\u03b5          .\n                                                                       20d2 (1 \u2212     B2\u03b5)\nDoing projected gradient descent for H = 20d2                         L2\u03b53 steps, if \u03b8(h) denotes the angle between the h-th\niterate and \u00b5\u2217       t, we obtain\n                      tan \u03b8(H) \u2264       tan \u03b8(h+1) \u2264        max         1 + L2\u03b52       1                 H    tan \u03b8(0), d2B2\u03b5\n                                                                              20d2 (1 \u2212     d2B2\u03b5)\n                                   \u2264   max       1 + HL2\u03b52 tan \u03b8(0)             , d2B2\u03b5        \u2264   d2B2\u03b5 ,\nwhere the last inequality uses 1+ HL2\u03b52                  20d2 (1 \u2212      B2\u03b5)   \u03b5 for \u03b5 \u2272      1\n                                                    20d2 (1\u2212B2\u03b5) \u2265             1             B3 . Additionally, for a random initializa-\ntion, Lemma C.4 shows that cos \u03b8(0) \u2265                        1                                            sec2 \u03b8(0) \u2212      1 \u2272    d. Using Lemma\n                                                             2d which implies tan \u03b8(0) \u2264\n                                                                            30", "md": "Proof. Recalling that $$\\mu^*_t = \\mu^*_0 \\exp(-t)$$, note that for $$t = \\log d\\epsilon$$, $$\\epsilon \\|\\mu^*_t\\| \\leq \\epsilon B$$ apply Lemma C.5. Note that we may apply this even though it is only stated for gradient descent $$d \\leq \\|\\mu^*_t\\| d$$. We would like to (without projection). The reason is that it bounds the change in angle between the iterate and the ground truth after a single gradient step, and this angle is unaffected by projection.\n\nSuppose we take one projected gradient step with learning rate $$\\eta$$ starting from an iterate $$\\mu_t$$. $$\\mu(h) \\lesssim \\epsilon B$$ As $$\\mu_t$$ was the result of a projection, by Lemma D.2 we have $$\\epsilon \\|\\mu^*_t\\| \\lesssim t d$$.\n\nWe now bound $$\\kappa_2$$ in Lemma C.5:\n\\[\n\\begin{aligned}\n\\kappa_2 & = 500\\eta \\sqrt{d^3\\|\\mu_t\\|^4 + 20\\eta d\\|\\mu_t\\|^2\\|\\mu^*_t\\|^2 + \\eta \\tilde{\\epsilon}\\mu^{*2}_t} \\\\\n& \\lesssim 500\\eta \\sqrt{d^7\\|\\mu_t\\|^2 + 20\\eta d\\|\\mu_t\\|^2 + d^2\\epsilon\\mu^{*3}_t} \\\\\n& \\leq 550d^{7/2}B^2 \\exp(-2t) + d^5\\epsilon^3L^3 \\\\\n& \\lesssim d^2B^2\\epsilon,\n\\end{aligned}\n\\]\nwhere the last inequality follows by choosing population gradient estimation error parameter $$\\epsilon = \\epsilon^4L^3$$ with the number of samples $$n' = d^{11}B^6\\epsilon^8L^6$$. Additionally, $$\\kappa_1$$ in Lemma C.5 is given by\n\\[\n\\begin{aligned}\n\\kappa_1 & = \\frac{1 - 3\\eta\\|\\mu_t\\|^2}{(1 - 3\\eta\\|\\mu_t\\|^2) + \\eta(\\|\\mu^*_t\\|^2 - 500d^3\\|\\mu_t\\|^4 - 20d\\|\\mu_t\\|^2\\epsilon)} \\\\\n& = \\frac{1 - 3\\eta\\|\\mu_t\\|^2}{(1 - 3\\eta\\|\\mu_t\\|^2) + \\eta\\|\\mu^*_t\\|^2(1 - \\kappa_2)} \\\\\n& \\lesssim (1 - 3\\eta\\|\\mu(h)\\|^2) + \\eta\\|\\mu^*_t\\|^2(1 - d^2B^2\\epsilon) \\\\\n& \\leq 1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - d^2B^2\\epsilon)}.\n\\end{aligned}\n\\]\n\nUsing bounds on $$\\kappa_1$$ and $$\\kappa_2$$ and Lemma C.5, we conclude that if $$\\theta$$ (resp. $$\\theta'$$) is the angle between $$\\mu_t$$ (resp. the next iterate of projected gradient descent after $$\\mu_t$$) and $$\\mu^*_t$$, $$\\tan \\theta' \\leq \\max \\left(1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - B^2\\epsilon)}, \\tan \\theta, d^2B^2\\epsilon\\right)$$.\n\nDoing projected gradient descent for $$H = 20d^2L^2\\epsilon^3$$ steps, if $$\\theta(h)$$ denotes the angle between the $$h$$-th iterate and $$\\mu^*_t$$, we obtain\n\\[\n\\begin{aligned}\n\\tan \\theta(H) & \\leq \\tan \\theta(h+1) \\leq \\max \\left(1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - d^2B^2\\epsilon)} H \\tan \\theta(0), d^2B^2\\epsilon\\right) \\\\\n& \\leq \\max \\left(1 + HL^2\\epsilon^2 \\tan \\theta(0), d^2B^2\\epsilon\\right) \\leq d^2B^2\\epsilon,\n\\end{aligned}\n\\]\nwhere the last inequality uses $$1+ HL^2\\epsilon^2 \\frac{1}{20d^2(1 - B^2\\epsilon)} \\epsilon \\leq \\frac{1}{20d^2(1-B^2\\epsilon)} \\geq \\frac{1}{B^3}$$. Additionally, for a random initialization, Lemma C.4 shows that $$\\cos \\theta(0) \\geq \\frac{1}{2d}$$ which implies $$\\tan \\theta(0) \\leq \\frac{1}{30}$$.", "images": [], "items": [{"type": "text", "value": "Proof. Recalling that $$\\mu^*_t = \\mu^*_0 \\exp(-t)$$, note that for $$t = \\log d\\epsilon$$, $$\\epsilon \\|\\mu^*_t\\| \\leq \\epsilon B$$ apply Lemma C.5. Note that we may apply this even though it is only stated for gradient descent $$d \\leq \\|\\mu^*_t\\| d$$. We would like to (without projection). The reason is that it bounds the change in angle between the iterate and the ground truth after a single gradient step, and this angle is unaffected by projection.\n\nSuppose we take one projected gradient step with learning rate $$\\eta$$ starting from an iterate $$\\mu_t$$. $$\\mu(h) \\lesssim \\epsilon B$$ As $$\\mu_t$$ was the result of a projection, by Lemma D.2 we have $$\\epsilon \\|\\mu^*_t\\| \\lesssim t d$$.\n\nWe now bound $$\\kappa_2$$ in Lemma C.5:\n\\[\n\\begin{aligned}\n\\kappa_2 & = 500\\eta \\sqrt{d^3\\|\\mu_t\\|^4 + 20\\eta d\\|\\mu_t\\|^2\\|\\mu^*_t\\|^2 + \\eta \\tilde{\\epsilon}\\mu^{*2}_t} \\\\\n& \\lesssim 500\\eta \\sqrt{d^7\\|\\mu_t\\|^2 + 20\\eta d\\|\\mu_t\\|^2 + d^2\\epsilon\\mu^{*3}_t} \\\\\n& \\leq 550d^{7/2}B^2 \\exp(-2t) + d^5\\epsilon^3L^3 \\\\\n& \\lesssim d^2B^2\\epsilon,\n\\end{aligned}\n\\]\nwhere the last inequality follows by choosing population gradient estimation error parameter $$\\epsilon = \\epsilon^4L^3$$ with the number of samples $$n' = d^{11}B^6\\epsilon^8L^6$$. Additionally, $$\\kappa_1$$ in Lemma C.5 is given by\n\\[\n\\begin{aligned}\n\\kappa_1 & = \\frac{1 - 3\\eta\\|\\mu_t\\|^2}{(1 - 3\\eta\\|\\mu_t\\|^2) + \\eta(\\|\\mu^*_t\\|^2 - 500d^3\\|\\mu_t\\|^4 - 20d\\|\\mu_t\\|^2\\epsilon)} \\\\\n& = \\frac{1 - 3\\eta\\|\\mu_t\\|^2}{(1 - 3\\eta\\|\\mu_t\\|^2) + \\eta\\|\\mu^*_t\\|^2(1 - \\kappa_2)} \\\\\n& \\lesssim (1 - 3\\eta\\|\\mu(h)\\|^2) + \\eta\\|\\mu^*_t\\|^2(1 - d^2B^2\\epsilon) \\\\\n& \\leq 1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - d^2B^2\\epsilon)}.\n\\end{aligned}\n\\]\n\nUsing bounds on $$\\kappa_1$$ and $$\\kappa_2$$ and Lemma C.5, we conclude that if $$\\theta$$ (resp. $$\\theta'$$) is the angle between $$\\mu_t$$ (resp. the next iterate of projected gradient descent after $$\\mu_t$$) and $$\\mu^*_t$$, $$\\tan \\theta' \\leq \\max \\left(1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - B^2\\epsilon)}, \\tan \\theta, d^2B^2\\epsilon\\right)$$.\n\nDoing projected gradient descent for $$H = 20d^2L^2\\epsilon^3$$ steps, if $$\\theta(h)$$ denotes the angle between the $$h$$-th iterate and $$\\mu^*_t$$, we obtain\n\\[\n\\begin{aligned}\n\\tan \\theta(H) & \\leq \\tan \\theta(h+1) \\leq \\max \\left(1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - d^2B^2\\epsilon)} H \\tan \\theta(0), d^2B^2\\epsilon\\right) \\\\\n& \\leq \\max \\left(1 + HL^2\\epsilon^2 \\tan \\theta(0), d^2B^2\\epsilon\\right) \\leq d^2B^2\\epsilon,\n\\end{aligned}\n\\]\nwhere the last inequality uses $$1+ HL^2\\epsilon^2 \\frac{1}{20d^2(1 - B^2\\epsilon)} \\epsilon \\leq \\frac{1}{20d^2(1-B^2\\epsilon)} \\geq \\frac{1}{B^3}$$. Additionally, for a random initialization, Lemma C.4 shows that $$\\cos \\theta(0) \\geq \\frac{1}{2d}$$ which implies $$\\tan \\theta(0) \\leq \\frac{1}{30}$$.", "md": "Proof. Recalling that $$\\mu^*_t = \\mu^*_0 \\exp(-t)$$, note that for $$t = \\log d\\epsilon$$, $$\\epsilon \\|\\mu^*_t\\| \\leq \\epsilon B$$ apply Lemma C.5. Note that we may apply this even though it is only stated for gradient descent $$d \\leq \\|\\mu^*_t\\| d$$. We would like to (without projection). The reason is that it bounds the change in angle between the iterate and the ground truth after a single gradient step, and this angle is unaffected by projection.\n\nSuppose we take one projected gradient step with learning rate $$\\eta$$ starting from an iterate $$\\mu_t$$. $$\\mu(h) \\lesssim \\epsilon B$$ As $$\\mu_t$$ was the result of a projection, by Lemma D.2 we have $$\\epsilon \\|\\mu^*_t\\| \\lesssim t d$$.\n\nWe now bound $$\\kappa_2$$ in Lemma C.5:\n\\[\n\\begin{aligned}\n\\kappa_2 & = 500\\eta \\sqrt{d^3\\|\\mu_t\\|^4 + 20\\eta d\\|\\mu_t\\|^2\\|\\mu^*_t\\|^2 + \\eta \\tilde{\\epsilon}\\mu^{*2}_t} \\\\\n& \\lesssim 500\\eta \\sqrt{d^7\\|\\mu_t\\|^2 + 20\\eta d\\|\\mu_t\\|^2 + d^2\\epsilon\\mu^{*3}_t} \\\\\n& \\leq 550d^{7/2}B^2 \\exp(-2t) + d^5\\epsilon^3L^3 \\\\\n& \\lesssim d^2B^2\\epsilon,\n\\end{aligned}\n\\]\nwhere the last inequality follows by choosing population gradient estimation error parameter $$\\epsilon = \\epsilon^4L^3$$ with the number of samples $$n' = d^{11}B^6\\epsilon^8L^6$$. Additionally, $$\\kappa_1$$ in Lemma C.5 is given by\n\\[\n\\begin{aligned}\n\\kappa_1 & = \\frac{1 - 3\\eta\\|\\mu_t\\|^2}{(1 - 3\\eta\\|\\mu_t\\|^2) + \\eta(\\|\\mu^*_t\\|^2 - 500d^3\\|\\mu_t\\|^4 - 20d\\|\\mu_t\\|^2\\epsilon)} \\\\\n& = \\frac{1 - 3\\eta\\|\\mu_t\\|^2}{(1 - 3\\eta\\|\\mu_t\\|^2) + \\eta\\|\\mu^*_t\\|^2(1 - \\kappa_2)} \\\\\n& \\lesssim (1 - 3\\eta\\|\\mu(h)\\|^2) + \\eta\\|\\mu^*_t\\|^2(1 - d^2B^2\\epsilon) \\\\\n& \\leq 1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - d^2B^2\\epsilon)}.\n\\end{aligned}\n\\]\n\nUsing bounds on $$\\kappa_1$$ and $$\\kappa_2$$ and Lemma C.5, we conclude that if $$\\theta$$ (resp. $$\\theta'$$) is the angle between $$\\mu_t$$ (resp. the next iterate of projected gradient descent after $$\\mu_t$$) and $$\\mu^*_t$$, $$\\tan \\theta' \\leq \\max \\left(1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - B^2\\epsilon)}, \\tan \\theta, d^2B^2\\epsilon\\right)$$.\n\nDoing projected gradient descent for $$H = 20d^2L^2\\epsilon^3$$ steps, if $$\\theta(h)$$ denotes the angle between the $$h$$-th iterate and $$\\mu^*_t$$, we obtain\n\\[\n\\begin{aligned}\n\\tan \\theta(H) & \\leq \\tan \\theta(h+1) \\leq \\max \\left(1 + L^2\\epsilon^2 \\frac{1}{20d^2(1 - d^2B^2\\epsilon)} H \\tan \\theta(0), d^2B^2\\epsilon\\right) \\\\\n& \\leq \\max \\left(1 + HL^2\\epsilon^2 \\tan \\theta(0), d^2B^2\\epsilon\\right) \\leq d^2B^2\\epsilon,\n\\end{aligned}\n\\]\nwhere the last inequality uses $$1+ HL^2\\epsilon^2 \\frac{1}{20d^2(1 - B^2\\epsilon)} \\epsilon \\leq \\frac{1}{20d^2(1-B^2\\epsilon)} \\geq \\frac{1}{B^3}$$. Additionally, for a random initialization, Lemma C.4 shows that $$\\cos \\theta(0) \\geq \\frac{1}{2d}$$ which implies $$\\tan \\theta(0) \\leq \\frac{1}{30}$$."}]}, {"page": 31, "text": "D.2, we have \u2225\u00b5(H)\u2225               \u2265   \u2225\u00b5\u2217\u2225    \u2212   \u03b5 which implies \u22122\u2225\u00b5(H)\u2225\u2225\u00b5\u2217\u2225                    cos \u03b8(H) \u2264        \u22122\u2225\u00b5\u2217\u22252 cos \u03b8(H) + 2B\u03b5\nand \u2225\u00b5(H)\u22252 \u2264           \u2225\u00b5\u2217\u22252 + 3B\u03b5. Using this result, we obtain\n      \u2225\u00b5(H) \u2212      \u00b5\u2217\u22252 = \u2225\u00b5(H)\u22252 + \u2225\u00b5\u2217\u22252 \u2212                 2\u2225\u00b5(H)\u2225\u2225\u00b5\u2217\u2225         cos \u03b8(H)\n                           \u2272  2\u2225\u00b5\u2217\u22252 \u2212       2\u2225\u00b5\u2217\u22252 cos \u03b8(H) + 5B\u03b5 \u2272                2B2 1 \u2212        \u221a        1             + 5B\u03b5 \u2272        d2B4\u03b5,\n                                                                                                      1 + d4B4\u03b52\nwhere the last inequality follows from the fact that \u221a                             1 + x \u2264      1 + \u221ax for any x > 0.\nE        Learning mixtures of K Gaussians from a warm start\nIn this section, we provide details about our main result on learning mixtures of K Gaussians. We\nstart by describing our main theorem in this case.\nTheorem E.1 (Formal version of Theorem 16). Let q be a mixture of Gaussians (in the form of\nEq. (6)) with center parameters \u03b8\u2217                   = {\u00b5\u2217   1, \u00b5\u22172, . . . , \u00b5\u2217\n                                                                             K} \u2208     Rd satisfying the separation Assumption 14,\nand suppose we have estimates \u03b8 for the centers such that the warm initialization Assumption 15 is\nsatisfied. For any \u03b5 > \u03b50 and noise scale t where\n                                                     \u03b50 = 1/poly(d) and t = \u0398(\u03b5) ,\n                                                                                                                         \u03b8 = {\u02dc  \u00b51, \u02dc\u00b52, . . . , \u02dc\ngradient descent on the DDPM objective at noise scale t\u2032 (Algorithm 1) outputs \u02dc                                                                  \u00b5K}\nsuch that mini \u2225\u02dc        \u00b5i \u2212   \u00b5\u2217i \u2225  \u2264   \u03b5 with high probability. The algorithm runs for H \u2265                                H\u2032 iterations and\nuses n \u2265      n\u2032 number of samples where\n                                     H\u2032 = \u0398(log(\u03b5\u22121 log d))                and     n\u2032 = \u0398(K4d5B6/\u03b52) .\nWe fi   rst give an overview of the proof for population gradient descent, and then show that the\nempirical gradients concentrate well around the population gradients. We start by simplifying the\npopulation gradient update for mixtures of K Gaussians using Stein\u2019s lemma in Lemma E.2, which\nyields\n                                  \u2212\u2207\u00b51,tLt(s\u03b8t) = E[w1,t(Xt)(Xt \u2212                    \u00b51,t)] + [extra terms] ,\nrecalling the notation of Eq. (9). As discussed in the body of the paper, E[w1,t(Xt)(Xt \u2212                                                        \u00b51,t)]\nis precisely the update for the gradient EM algorithm (see Fact 6) and known results for the\nlatter [KC20, SN21] can be used to show that the distance \u2225\u00b51,t \u2212                                     \u00b5\u22171,t\u2225  contracts in each step when\nthe separation Assumption 14 and the warm initialization Assumption 15 are satisfi                                                 ed. Therefore,\nshowing that the \u201cextra terms\u201d do not disturb the progress coming from the gradient EM update\nis suffi  cient. We prove that the \u201cextra terms\u201d are 1/poly(d) in Lemma E.4 when the separation\nAssumption 14 and warm initialization Assumption 15 hold.\n     The intuition behind Lemma E.4 is as follows: We start with a key observation that each of\nthe \u201cextra terms\u201d either contains w1,t(Xt)(1 \u2212                            w1,t(Xt)) or w1,t(Xt)wj,t(Xt) where j \u0338= 1. Note\nthat the w1,t(Xt) can be interpreted as the conditional probability of the underlying component\nbeing N      (\u00b51,t, I) given Xt. When Assumption 14 and Assumption 15 are satisfi                                          ed, Proposition 4.1\nof [SN21] shows that\n                                     E Xt\u223cN (\u00b5\u2217   1,t,I)[wj,t(Xt)] \u2272        1/poly(d)         for any j \u0338= 1 .\nThis result can be extended to show both EX                                t[w1,t(Xt)(1 \u2212         w1,t(Xt))] \u2272          1/poly(d) as well as\nE Xt[w1,t(Xt)wj,t(Xt)] \u2272               1/poly(d) for any j \u0338= 1 (see Lemma E.5 for the proof).                                        Using these\nbounds, we conclude that [\u201cextra terms\u2032\u2032] \u2272                          1/poly(d) in Lemma E.4.\n                                                                           31", "md": "$$\n\\|\\mu(H)\\| \\geq \\|\\mu^*\\| - \\varepsilon \\quad \\text{which implies} \\quad -2\\|\\mu(H)\\|\\|\\mu^*\\|\\cos \\theta(H) \\leq -2\\|\\mu^*\\|^2 \\cos \\theta(H) + 2B\\varepsilon\n$$\n\n$$\n\\text{and} \\quad \\|\\mu(H)\\|^2 \\leq \\|\\mu^*\\|^2 + 3B\\varepsilon. \\quad \\text{Using this result, we obtain} \\quad \\|\\mu(H) - \\mu^*\\|^2 = \\|\\mu(H)\\|^2 + \\|\\mu^*\\|^2 - 2\\|\\mu(H)\\|\\|\\mu^*\\|\\cos \\theta(H) \\lesssim 2\\|\\mu^*\\|^2 - 2\\|\\mu^*\\|^2 \\cos \\theta(H) + 5B\\varepsilon \\lesssim 2B^2 \\frac{1 - \\sqrt{1 + 5B\\varepsilon}}{1 + \\sqrt{1 + 5B\\varepsilon}} \\lesssim \\frac{d^2B^4\\varepsilon}{1 + d^4B^4\\varepsilon^2},\n$$\n\nwhere the last inequality follows from the fact that $$\\sqrt{1 + x} \\leq 1 + \\sqrt{x}$$ for any $$x > 0$$.\n\nE Learning mixtures of K Gaussians from a warm start\n\nIn this section, we provide details about our main result on learning mixtures of K Gaussians. We start by describing our main theorem in this case.\n\nTheorem E.1 (Formal version of Theorem 16). Let q be a mixture of Gaussians (in the form of Eq. (6)) with center parameters $$\\theta^* = \\{\\mu^*_1, \\mu^*_2, ..., \\mu^*_K\\} \\in \\mathbb{R}^d$$ satisfying the separation Assumption 14, and suppose we have estimates $$\\theta$$ for the centers such that the warm initialization Assumption 15 is satisfied. For any $$\\varepsilon > \\varepsilon_0$$ and noise scale $$t$$ where $$\\varepsilon_0 = 1/\\text{poly}(d)$$ and $$t = \\Theta(\\varepsilon)$$, $$\\theta = \\{\\tilde{\\mu}_1, \\tilde{\\mu}_2, ..., \\tilde{\\mu}_K\\}$$ gradient descent on the DDPM objective at noise scale $$t'$$ (Algorithm 1) outputs $$\\tilde{\\mu}_K$$ such that $$\\min_i \\|\\tilde{\\mu}_i - \\mu^*_i\\| \\leq \\varepsilon$$ with high probability. The algorithm runs for $$H \\geq H'$$ iterations and uses $$n \\geq n'$$ number of samples where $$H' = \\Theta(\\log(\\varepsilon^{-1} \\log d))$$ and $$n' = \\Theta(K^4d^5B^6/\\varepsilon^2)$$. We first give an overview of the proof for population gradient descent, and then show that the empirical gradients concentrate well around the population gradients. We start by simplifying the population gradient update for mixtures of K Gaussians using Stein\u2019s lemma in Lemma E.2, which yields\n\n$$\n-\\nabla\\mu_{1,t}L_t(s\\theta_t) = E[w_{1,t}(X_t)(X_t - \\mu_{1,t})] + [\\text{extra terms}],\n$$\n\nrecalling the notation of Eq. (9). As discussed in the body of the paper, $$E[w_{1,t}(X_t)(X_t - \\mu_{1,t})]$$ is precisely the update for the gradient EM algorithm (see Fact 6) and known results for the latter [KC20, SN21] can be used to show that the distance $$\\|\\mu_{1,t} - \\mu^*_{1,t}\\|$$ contracts in each step when the separation Assumption 14 and the warm initialization Assumption 15 are satisfied. Therefore, showing that the \u201cextra terms\u201d do not disturb the progress coming from the gradient EM update is sufficient. We prove that the \u201cextra terms\u201d are $$1/\\text{poly}(d)$$ in Lemma E.4 when the separation Assumption 14 and warm initialization Assumption 15 hold.\n\nThe intuition behind Lemma E.4 is as follows: We start with a key observation that each of the \u201cextra terms\u201d either contains $$w_{1,t}(X_t)(1 - w_{1,t}(X_t))$$ or $$w_{1,t}(X_t)w_{j,t}(X_t)$$ where $$j \\neq 1$$. Note that the $$w_{1,t}(X_t)$$ can be interpreted as the conditional probability of the underlying component being $$N(\\mu_{1,t}, I)$$ given $$X_t$$. When Assumption 14 and Assumption 15 are satisfied, Proposition 4.1 of [SN21] shows that\n\n$$\nE_{X_t \\sim N(\\mu^*_{1,t},I)}[w_{j,t}(X_t)] \\lesssim 1/\\text{poly}(d) \\quad \\text{for any} \\quad j \\neq 1.\n$$\n\nThis result can be extended to show both $$E_{X_t}[w_{1,t}(X_t)(1 - w_{1,t}(X_t))] \\lesssim 1/\\text{poly}(d)$$ as well as $$E_{X_t}[w_{1,t}(X_t)w_{j,t}(X_t)] \\lesssim 1/\\text{poly}(d)$$ for any $$j \\neq 1$$ (see Lemma E.5 for the proof). Using these bounds, we conclude that $$[\\text{\u201cextra terms\u201d}] \\lesssim 1/\\text{poly}(d)$$ in Lemma E.4.\n\n31", "images": [], "items": [{"type": "text", "value": "$$\n\\|\\mu(H)\\| \\geq \\|\\mu^*\\| - \\varepsilon \\quad \\text{which implies} \\quad -2\\|\\mu(H)\\|\\|\\mu^*\\|\\cos \\theta(H) \\leq -2\\|\\mu^*\\|^2 \\cos \\theta(H) + 2B\\varepsilon\n$$\n\n$$\n\\text{and} \\quad \\|\\mu(H)\\|^2 \\leq \\|\\mu^*\\|^2 + 3B\\varepsilon. \\quad \\text{Using this result, we obtain} \\quad \\|\\mu(H) - \\mu^*\\|^2 = \\|\\mu(H)\\|^2 + \\|\\mu^*\\|^2 - 2\\|\\mu(H)\\|\\|\\mu^*\\|\\cos \\theta(H) \\lesssim 2\\|\\mu^*\\|^2 - 2\\|\\mu^*\\|^2 \\cos \\theta(H) + 5B\\varepsilon \\lesssim 2B^2 \\frac{1 - \\sqrt{1 + 5B\\varepsilon}}{1 + \\sqrt{1 + 5B\\varepsilon}} \\lesssim \\frac{d^2B^4\\varepsilon}{1 + d^4B^4\\varepsilon^2},\n$$\n\nwhere the last inequality follows from the fact that $$\\sqrt{1 + x} \\leq 1 + \\sqrt{x}$$ for any $$x > 0$$.\n\nE Learning mixtures of K Gaussians from a warm start\n\nIn this section, we provide details about our main result on learning mixtures of K Gaussians. We start by describing our main theorem in this case.\n\nTheorem E.1 (Formal version of Theorem 16). Let q be a mixture of Gaussians (in the form of Eq. (6)) with center parameters $$\\theta^* = \\{\\mu^*_1, \\mu^*_2, ..., \\mu^*_K\\} \\in \\mathbb{R}^d$$ satisfying the separation Assumption 14, and suppose we have estimates $$\\theta$$ for the centers such that the warm initialization Assumption 15 is satisfied. For any $$\\varepsilon > \\varepsilon_0$$ and noise scale $$t$$ where $$\\varepsilon_0 = 1/\\text{poly}(d)$$ and $$t = \\Theta(\\varepsilon)$$, $$\\theta = \\{\\tilde{\\mu}_1, \\tilde{\\mu}_2, ..., \\tilde{\\mu}_K\\}$$ gradient descent on the DDPM objective at noise scale $$t'$$ (Algorithm 1) outputs $$\\tilde{\\mu}_K$$ such that $$\\min_i \\|\\tilde{\\mu}_i - \\mu^*_i\\| \\leq \\varepsilon$$ with high probability. The algorithm runs for $$H \\geq H'$$ iterations and uses $$n \\geq n'$$ number of samples where $$H' = \\Theta(\\log(\\varepsilon^{-1} \\log d))$$ and $$n' = \\Theta(K^4d^5B^6/\\varepsilon^2)$$. We first give an overview of the proof for population gradient descent, and then show that the empirical gradients concentrate well around the population gradients. We start by simplifying the population gradient update for mixtures of K Gaussians using Stein\u2019s lemma in Lemma E.2, which yields\n\n$$\n-\\nabla\\mu_{1,t}L_t(s\\theta_t) = E[w_{1,t}(X_t)(X_t - \\mu_{1,t})] + [\\text{extra terms}],\n$$\n\nrecalling the notation of Eq. (9). As discussed in the body of the paper, $$E[w_{1,t}(X_t)(X_t - \\mu_{1,t})]$$ is precisely the update for the gradient EM algorithm (see Fact 6) and known results for the latter [KC20, SN21] can be used to show that the distance $$\\|\\mu_{1,t} - \\mu^*_{1,t}\\|$$ contracts in each step when the separation Assumption 14 and the warm initialization Assumption 15 are satisfied. Therefore, showing that the \u201cextra terms\u201d do not disturb the progress coming from the gradient EM update is sufficient. We prove that the \u201cextra terms\u201d are $$1/\\text{poly}(d)$$ in Lemma E.4 when the separation Assumption 14 and warm initialization Assumption 15 hold.\n\nThe intuition behind Lemma E.4 is as follows: We start with a key observation that each of the \u201cextra terms\u201d either contains $$w_{1,t}(X_t)(1 - w_{1,t}(X_t))$$ or $$w_{1,t}(X_t)w_{j,t}(X_t)$$ where $$j \\neq 1$$. Note that the $$w_{1,t}(X_t)$$ can be interpreted as the conditional probability of the underlying component being $$N(\\mu_{1,t}, I)$$ given $$X_t$$. When Assumption 14 and Assumption 15 are satisfied, Proposition 4.1 of [SN21] shows that\n\n$$\nE_{X_t \\sim N(\\mu^*_{1,t},I)}[w_{j,t}(X_t)] \\lesssim 1/\\text{poly}(d) \\quad \\text{for any} \\quad j \\neq 1.\n$$\n\nThis result can be extended to show both $$E_{X_t}[w_{1,t}(X_t)(1 - w_{1,t}(X_t))] \\lesssim 1/\\text{poly}(d)$$ as well as $$E_{X_t}[w_{1,t}(X_t)w_{j,t}(X_t)] \\lesssim 1/\\text{poly}(d)$$ for any $$j \\neq 1$$ (see Lemma E.5 for the proof). Using these bounds, we conclude that $$[\\text{\u201cextra terms\u201d}] \\lesssim 1/\\text{poly}(d)$$ in Lemma E.4.\n\n31", "md": "$$\n\\|\\mu(H)\\| \\geq \\|\\mu^*\\| - \\varepsilon \\quad \\text{which implies} \\quad -2\\|\\mu(H)\\|\\|\\mu^*\\|\\cos \\theta(H) \\leq -2\\|\\mu^*\\|^2 \\cos \\theta(H) + 2B\\varepsilon\n$$\n\n$$\n\\text{and} \\quad \\|\\mu(H)\\|^2 \\leq \\|\\mu^*\\|^2 + 3B\\varepsilon. \\quad \\text{Using this result, we obtain} \\quad \\|\\mu(H) - \\mu^*\\|^2 = \\|\\mu(H)\\|^2 + \\|\\mu^*\\|^2 - 2\\|\\mu(H)\\|\\|\\mu^*\\|\\cos \\theta(H) \\lesssim 2\\|\\mu^*\\|^2 - 2\\|\\mu^*\\|^2 \\cos \\theta(H) + 5B\\varepsilon \\lesssim 2B^2 \\frac{1 - \\sqrt{1 + 5B\\varepsilon}}{1 + \\sqrt{1 + 5B\\varepsilon}} \\lesssim \\frac{d^2B^4\\varepsilon}{1 + d^4B^4\\varepsilon^2},\n$$\n\nwhere the last inequality follows from the fact that $$\\sqrt{1 + x} \\leq 1 + \\sqrt{x}$$ for any $$x > 0$$.\n\nE Learning mixtures of K Gaussians from a warm start\n\nIn this section, we provide details about our main result on learning mixtures of K Gaussians. We start by describing our main theorem in this case.\n\nTheorem E.1 (Formal version of Theorem 16). Let q be a mixture of Gaussians (in the form of Eq. (6)) with center parameters $$\\theta^* = \\{\\mu^*_1, \\mu^*_2, ..., \\mu^*_K\\} \\in \\mathbb{R}^d$$ satisfying the separation Assumption 14, and suppose we have estimates $$\\theta$$ for the centers such that the warm initialization Assumption 15 is satisfied. For any $$\\varepsilon > \\varepsilon_0$$ and noise scale $$t$$ where $$\\varepsilon_0 = 1/\\text{poly}(d)$$ and $$t = \\Theta(\\varepsilon)$$, $$\\theta = \\{\\tilde{\\mu}_1, \\tilde{\\mu}_2, ..., \\tilde{\\mu}_K\\}$$ gradient descent on the DDPM objective at noise scale $$t'$$ (Algorithm 1) outputs $$\\tilde{\\mu}_K$$ such that $$\\min_i \\|\\tilde{\\mu}_i - \\mu^*_i\\| \\leq \\varepsilon$$ with high probability. The algorithm runs for $$H \\geq H'$$ iterations and uses $$n \\geq n'$$ number of samples where $$H' = \\Theta(\\log(\\varepsilon^{-1} \\log d))$$ and $$n' = \\Theta(K^4d^5B^6/\\varepsilon^2)$$. We first give an overview of the proof for population gradient descent, and then show that the empirical gradients concentrate well around the population gradients. We start by simplifying the population gradient update for mixtures of K Gaussians using Stein\u2019s lemma in Lemma E.2, which yields\n\n$$\n-\\nabla\\mu_{1,t}L_t(s\\theta_t) = E[w_{1,t}(X_t)(X_t - \\mu_{1,t})] + [\\text{extra terms}],\n$$\n\nrecalling the notation of Eq. (9). As discussed in the body of the paper, $$E[w_{1,t}(X_t)(X_t - \\mu_{1,t})]$$ is precisely the update for the gradient EM algorithm (see Fact 6) and known results for the latter [KC20, SN21] can be used to show that the distance $$\\|\\mu_{1,t} - \\mu^*_{1,t}\\|$$ contracts in each step when the separation Assumption 14 and the warm initialization Assumption 15 are satisfied. Therefore, showing that the \u201cextra terms\u201d do not disturb the progress coming from the gradient EM update is sufficient. We prove that the \u201cextra terms\u201d are $$1/\\text{poly}(d)$$ in Lemma E.4 when the separation Assumption 14 and warm initialization Assumption 15 hold.\n\nThe intuition behind Lemma E.4 is as follows: We start with a key observation that each of the \u201cextra terms\u201d either contains $$w_{1,t}(X_t)(1 - w_{1,t}(X_t))$$ or $$w_{1,t}(X_t)w_{j,t}(X_t)$$ where $$j \\neq 1$$. Note that the $$w_{1,t}(X_t)$$ can be interpreted as the conditional probability of the underlying component being $$N(\\mu_{1,t}, I)$$ given $$X_t$$. When Assumption 14 and Assumption 15 are satisfied, Proposition 4.1 of [SN21] shows that\n\n$$\nE_{X_t \\sim N(\\mu^*_{1,t},I)}[w_{j,t}(X_t)] \\lesssim 1/\\text{poly}(d) \\quad \\text{for any} \\quad j \\neq 1.\n$$\n\nThis result can be extended to show both $$E_{X_t}[w_{1,t}(X_t)(1 - w_{1,t}(X_t))] \\lesssim 1/\\text{poly}(d)$$ as well as $$E_{X_t}[w_{1,t}(X_t)w_{j,t}(X_t)] \\lesssim 1/\\text{poly}(d)$$ for any $$j \\neq 1$$ (see Lemma E.5 for the proof). Using these bounds, we conclude that $$[\\text{\u201cextra terms\u201d}] \\lesssim 1/\\text{poly}(d)$$ in Lemma E.4.\n\n31"}]}, {"page": 32, "text": " E.1        EM and population gradient descent on DDPM objective\nWe begin by writing out the gradient update explicitly:\n Lemma E.2. For any noise scale t > 0, the gradient of the population DDPM objective E[Lt(s\u03b8                                                              t(Xt))]\nwith respect to parameter \u00b51,t is given by\n                                                                                                        K\n \u2207\u00b51,tLt(s\u03b8t) = E              \u2212   w1,t(Xt)(Xt \u2212           \u00b51,t) + w1,t(Xt)(Xt \u2212               \u00b51,t)   i=1   wi,t(Xt)\u00b5\u22a4     i,t(Xt \u2212      \u00b51,t)\n                                                                                                                                            K\n                                  + w1,t(Xt)\u00b51,t \u2212    K       w1,t(Xt)(Xt \u2212           \u00b51,t)\u22a4\u00b51,t(Xt \u2212           \u00b51,t) \u2212     w1,t(Xt)       i=1  wi,t(Xt)\u00b5i,t\n                                  \u2212   w1,t(Xt)       i=1  \u2207xwi,t(Xt)\u22a4\u00b5i,t(Xt \u2212                 \u00b51,t)\nwhere w1,t(x) and \u00b51,t are defined in Eq. (9).\n Proof. Recall that the score function of mixture of Gaussians is given by\n Finding the gradient \u2207\u00b51,twi,t(Xt), we have          s\u03b8t(Xt) =          i   wi,t(Xt)\u00b5i,t \u2212         Xt\n                              \u2207\u00b51,twi,t(Xt) =            w1,t(Xt)(1 \u2212            w1,t(Xt))(Xt \u2212           \u00b51,t)      if i = 1\nThe gradient of the score function is given by             \u2212w1,t(Xt)wi,t(Xt)(Xt \u2212                  \u00b51,t)             otherwise.\n                                                                     K\n      \u2207\u00b51,ts\u03b8t(Xt) = \u2207\u00b51,t                w1,t(Xt)\u00b51,t          +   i=2   \u2207\u00b51,t      wi,t(Xt)\u00b5i,t                 K\n       = w1,t(Xt)(1 \u2212            w1,t(Xt))\u00b51,t(Xt \u2212             \u00b51,t)\u22a4     + w1,t(Xt)I \u2212   K       w1,t(Xt)      i=2   wi,t(Xt)\u00b5i,t(Xt \u2212            \u00b51,t)\u22a4\n       = w1,t(Xt)\u00b51,t(Xt \u2212               \u00b51,t)\u22a4     + w1,t(Xt)I \u2212           w1,t(Xt)      i=1   wi,t(Xt)\u00b5i,t(Xt \u2212            \u00b51,t)\u22a4    .\nThe gradient of 1           2\u2225s\u03b8t\u22252 is given by         d\n                          1     s\u03b8t(Xt) 2 =                 [s\u03b8t(Xt)]j[\u2207\u00b51,ts\u03b8t(Xt)]j = \u2207\u00b51,ts\u03b8t(Xt)\u22a4s\u03b8t(Xt)\n                          2\u2207                          j=1\nThe gradient of this is given by                        where [\u2207\u00b51,ts\u03b8t(Xt)]j is jth row of \u2207\u00b51,ts\u03b8t(Xt) .\n                        \u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt             = 1       w  1,t(Xt)(Xt \u2212         \u00b51,t)\u00b5\u22a4   1,tZt + w1,t(Xt)Zt\n                                   \u03b2t                   \u03b2t\n                                                                                             K\n                                                                          \u2212   w1,t(Xt)      i=1   wi,t(Xt)(Xt \u2212           \u00b51,t)\u00b5\u22a4  i,tZt                   (E.1)\n                                                                                32", "md": "E.1 EM and population gradient descent on DDPM objective\n\nWe begin by writing out the gradient update explicitly:\n\nLemma E.2. For any noise scale \\( t > 0 \\), the gradient of the population DDPM objective \\( E[L_t(s_{\\theta_t}(X_t))] \\) with respect to parameter \\( \\mu_{1,t} \\) is given by\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} L_t(s_{\\theta_t}) & = E \\left[ -w_{1,t}(X_t)(X_t - \\mu_{1,t}) + w_{1,t}(X_t)(X_t - \\mu_{1,t}) \\right]_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t}^T(X_t - \\mu_{1,t}) \\\\\n& + w_{1,t}(X_t)\\mu_{1,t} - K w_{1,t}(X_t)(X_t - \\mu_{1,t})^T\\mu_{1,t}(X_t - \\mu_{1,t}) - \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t} \\\\\n& - w_{1,t}(X_t) \\sum_{i=1} \\nabla x w_{i,t}(X_t)^T\\mu_{i,t}(X_t - \\mu_{1,t})\n\\end{align*}\n$$\nwhere \\( w_{1,t}(x) \\) and \\( \\mu_{1,t} \\) are defined in Eq. (9).\n\nProof. Recall that the score function of mixture of Gaussians is given by\n\nFinding the gradient \\( \\nabla \\mu_{1,t} w_{i,t}(X_t) \\), we have \\( s_{\\theta_t}(X_t) = \\sum_{i} w_{i,t}(X_t)\\mu_{i,t} - X_t \\)\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} w_{i,t}(X_t) & = w_{1,t}(X_t)(1 - w_{1,t}(X_t))(X_t - \\mu_{1,t}) \\quad \\text{if } i = 1 \\\\\n& = -w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t}) \\quad \\text{otherwise}\n\\end{align*}\n$$\nThe gradient of the score function is given by\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} s_{\\theta_t}(X_t) & = \\nabla \\mu_{1,t} \\left[ w_{1,t}(X_t)\\mu_{1,t} + \\sum_{i=2} \\nabla \\mu_{1,t} w_{i,t}(X_t)\\mu_{i,t} \\right]_{K} \\\\\n& = w_{1,t}(X_t)(1 - w_{1,t}(X_t))\\mu_{1,t}(X_t - \\mu_{1,t})^T + w_{1,t}(X_t)I - K w_{1,t}(X_t) \\sum_{i=2} w_{i,t}(X_t)\\mu_{i,t}(X_t - \\mu_{1,t})^T \\\\\n& = w_{1,t}(X_t)\\mu_{1,t}(X_t - \\mu_{1,t})^T + w_{1,t}(X_t)I - w_{1,t}(X_t) \\sum_{i=1} w_{i,t}(X_t)\\mu_{i,t}(X_t - \\mu_{1,t})^T.\n\\end{align*}\n$$\nThe gradient of \\( \\frac{1}{2}\\|s_{\\theta_t}\\|^2 \\) is given by\n\n$$\n\\begin{align*}\n\\frac{1}{2}\\|s_{\\theta_t}\\|^2 & = \\frac{1}{2} \\sum_{j} [s_{\\theta_t}(X_t)]_j[\\nabla \\mu_{1,t} s_{\\theta_t}(X_t)]_j = \\nabla \\mu_{1,t} s_{\\theta_t}(X_t)^T s_{\\theta_t}(X_t) \\\\\n& = \\nabla \\mu_{1,t} s_{\\theta_t}(X_t)^T Z_t = \\frac{1}{\\beta_t} w_{1,t}(X_t)(X_t - \\mu_{1,t})\\mu_{1,t}^T Z_t + w_{1,t}(X_t)Z_t \\\\\n& - w_{1,t}(X_t) \\sum_{i=1} w_{i,t}(X_t)(X_t - \\mu_{1,t})\\mu_{i,t}^T Z_t \\quad \\text{(E.1)}\n\\end{align*}\n$$\n32", "images": [], "items": [{"type": "text", "value": "E.1 EM and population gradient descent on DDPM objective\n\nWe begin by writing out the gradient update explicitly:\n\nLemma E.2. For any noise scale \\( t > 0 \\), the gradient of the population DDPM objective \\( E[L_t(s_{\\theta_t}(X_t))] \\) with respect to parameter \\( \\mu_{1,t} \\) is given by\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} L_t(s_{\\theta_t}) & = E \\left[ -w_{1,t}(X_t)(X_t - \\mu_{1,t}) + w_{1,t}(X_t)(X_t - \\mu_{1,t}) \\right]_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t}^T(X_t - \\mu_{1,t}) \\\\\n& + w_{1,t}(X_t)\\mu_{1,t} - K w_{1,t}(X_t)(X_t - \\mu_{1,t})^T\\mu_{1,t}(X_t - \\mu_{1,t}) - \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t} \\\\\n& - w_{1,t}(X_t) \\sum_{i=1} \\nabla x w_{i,t}(X_t)^T\\mu_{i,t}(X_t - \\mu_{1,t})\n\\end{align*}\n$$\nwhere \\( w_{1,t}(x) \\) and \\( \\mu_{1,t} \\) are defined in Eq. (9).\n\nProof. Recall that the score function of mixture of Gaussians is given by\n\nFinding the gradient \\( \\nabla \\mu_{1,t} w_{i,t}(X_t) \\), we have \\( s_{\\theta_t}(X_t) = \\sum_{i} w_{i,t}(X_t)\\mu_{i,t} - X_t \\)\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} w_{i,t}(X_t) & = w_{1,t}(X_t)(1 - w_{1,t}(X_t))(X_t - \\mu_{1,t}) \\quad \\text{if } i = 1 \\\\\n& = -w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t}) \\quad \\text{otherwise}\n\\end{align*}\n$$\nThe gradient of the score function is given by\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} s_{\\theta_t}(X_t) & = \\nabla \\mu_{1,t} \\left[ w_{1,t}(X_t)\\mu_{1,t} + \\sum_{i=2} \\nabla \\mu_{1,t} w_{i,t}(X_t)\\mu_{i,t} \\right]_{K} \\\\\n& = w_{1,t}(X_t)(1 - w_{1,t}(X_t))\\mu_{1,t}(X_t - \\mu_{1,t})^T + w_{1,t}(X_t)I - K w_{1,t}(X_t) \\sum_{i=2} w_{i,t}(X_t)\\mu_{i,t}(X_t - \\mu_{1,t})^T \\\\\n& = w_{1,t}(X_t)\\mu_{1,t}(X_t - \\mu_{1,t})^T + w_{1,t}(X_t)I - w_{1,t}(X_t) \\sum_{i=1} w_{i,t}(X_t)\\mu_{i,t}(X_t - \\mu_{1,t})^T.\n\\end{align*}\n$$\nThe gradient of \\( \\frac{1}{2}\\|s_{\\theta_t}\\|^2 \\) is given by\n\n$$\n\\begin{align*}\n\\frac{1}{2}\\|s_{\\theta_t}\\|^2 & = \\frac{1}{2} \\sum_{j} [s_{\\theta_t}(X_t)]_j[\\nabla \\mu_{1,t} s_{\\theta_t}(X_t)]_j = \\nabla \\mu_{1,t} s_{\\theta_t}(X_t)^T s_{\\theta_t}(X_t) \\\\\n& = \\nabla \\mu_{1,t} s_{\\theta_t}(X_t)^T Z_t = \\frac{1}{\\beta_t} w_{1,t}(X_t)(X_t - \\mu_{1,t})\\mu_{1,t}^T Z_t + w_{1,t}(X_t)Z_t \\\\\n& - w_{1,t}(X_t) \\sum_{i=1} w_{i,t}(X_t)(X_t - \\mu_{1,t})\\mu_{i,t}^T Z_t \\quad \\text{(E.1)}\n\\end{align*}\n$$\n32", "md": "E.1 EM and population gradient descent on DDPM objective\n\nWe begin by writing out the gradient update explicitly:\n\nLemma E.2. For any noise scale \\( t > 0 \\), the gradient of the population DDPM objective \\( E[L_t(s_{\\theta_t}(X_t))] \\) with respect to parameter \\( \\mu_{1,t} \\) is given by\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} L_t(s_{\\theta_t}) & = E \\left[ -w_{1,t}(X_t)(X_t - \\mu_{1,t}) + w_{1,t}(X_t)(X_t - \\mu_{1,t}) \\right]_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t}^T(X_t - \\mu_{1,t}) \\\\\n& + w_{1,t}(X_t)\\mu_{1,t} - K w_{1,t}(X_t)(X_t - \\mu_{1,t})^T\\mu_{1,t}(X_t - \\mu_{1,t}) - \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t} \\\\\n& - w_{1,t}(X_t) \\sum_{i=1} \\nabla x w_{i,t}(X_t)^T\\mu_{i,t}(X_t - \\mu_{1,t})\n\\end{align*}\n$$\nwhere \\( w_{1,t}(x) \\) and \\( \\mu_{1,t} \\) are defined in Eq. (9).\n\nProof. Recall that the score function of mixture of Gaussians is given by\n\nFinding the gradient \\( \\nabla \\mu_{1,t} w_{i,t}(X_t) \\), we have \\( s_{\\theta_t}(X_t) = \\sum_{i} w_{i,t}(X_t)\\mu_{i,t} - X_t \\)\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} w_{i,t}(X_t) & = w_{1,t}(X_t)(1 - w_{1,t}(X_t))(X_t - \\mu_{1,t}) \\quad \\text{if } i = 1 \\\\\n& = -w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t}) \\quad \\text{otherwise}\n\\end{align*}\n$$\nThe gradient of the score function is given by\n\n$$\n\\begin{align*}\n\\nabla \\mu_{1,t} s_{\\theta_t}(X_t) & = \\nabla \\mu_{1,t} \\left[ w_{1,t}(X_t)\\mu_{1,t} + \\sum_{i=2} \\nabla \\mu_{1,t} w_{i,t}(X_t)\\mu_{i,t} \\right]_{K} \\\\\n& = w_{1,t}(X_t)(1 - w_{1,t}(X_t))\\mu_{1,t}(X_t - \\mu_{1,t})^T + w_{1,t}(X_t)I - K w_{1,t}(X_t) \\sum_{i=2} w_{i,t}(X_t)\\mu_{i,t}(X_t - \\mu_{1,t})^T \\\\\n& = w_{1,t}(X_t)\\mu_{1,t}(X_t - \\mu_{1,t})^T + w_{1,t}(X_t)I - w_{1,t}(X_t) \\sum_{i=1} w_{i,t}(X_t)\\mu_{i,t}(X_t - \\mu_{1,t})^T.\n\\end{align*}\n$$\nThe gradient of \\( \\frac{1}{2}\\|s_{\\theta_t}\\|^2 \\) is given by\n\n$$\n\\begin{align*}\n\\frac{1}{2}\\|s_{\\theta_t}\\|^2 & = \\frac{1}{2} \\sum_{j} [s_{\\theta_t}(X_t)]_j[\\nabla \\mu_{1,t} s_{\\theta_t}(X_t)]_j = \\nabla \\mu_{1,t} s_{\\theta_t}(X_t)^T s_{\\theta_t}(X_t) \\\\\n& = \\nabla \\mu_{1,t} s_{\\theta_t}(X_t)^T Z_t = \\frac{1}{\\beta_t} w_{1,t}(X_t)(X_t - \\mu_{1,t})\\mu_{1,t}^T Z_t + w_{1,t}(X_t)Z_t \\\\\n& - w_{1,t}(X_t) \\sum_{i=1} w_{i,t}(X_t)(X_t - \\mu_{1,t})\\mu_{i,t}^T Z_t \\quad \\text{(E.1)}\n\\end{align*}\n$$\n32"}]}, {"page": 33, "text": "Applying Stein\u2019s lemma to the expectation of the fi                                  rst term in Eq. (E.1), we have\n                                                             d\n E X0,Zt[w1,t(Xt)(Xt \u2212              \u00b51,t)\u00b5\u22a4   1,tZt] =     j=1   EX0,Zt[w1,t(Xt)(Xt \u2212                \u00b51,t)\u00b51,t,jZt,j]\n                                                             d\n                                                       =   j=1   EX0,Zt[w1,t(Xt)\u03b2tej\u00b51,t,j + \u03b2t\u2207xw1,t(Xt)\u22a4ej(Xt \u2212                                   \u00b51,t)\u00b51,t,j]\n                                                       = EX     0,Zt[w1,t(Xt)\u03b2t\u00b51,t + \u03b2t\u2207xw1,t(Xt)\u22a4\u00b51,t(Xt \u2212                               \u00b51,t)]\nThe expectation of the second term in Eq. (E.1) simplifi                                     es to \u03b2tEX       t[\u2207xw1,t(Xt)] by Stein\u2019s Lemma.\n Each summand in the third term in Eq. (E.1) simplifi                                    es as following:\n              E  X0,Zt     w1,t(Xt)wi,t(Xt)(Xt \u2212                \u00b51,t)\u00b5\u22a4   i,tZt\n                             d\n                       =   j=1   E  X0,Zt     w1,t(Xt)wi,t(Xt)(Xt \u2212                \u00b51,t)\u00b5i,t,jZt,j\n                       =     j   \u00b5i,t,jEX0,Zt        w1,t(Xt)wi,t(Xt)\u03b2tej + \u03b2tw1,t(Xt)\u2207xwi,t(Xt)\u22a4ej(Xt \u2212                                      \u00b51,t)\n                                                            + \u03b2t\u2207xw1,t(Xt)\u22a4ejwi,t(Xt)(Xt \u2212                         \u00b51,t)\n                       = \u03b2t EX      0,Zt  w1,t(Xt)wi,t(Xt)\u00b5i,t + w1,t(Xt)\u2207xwi,t(Xt)\u22a4\u00b5i,t(Xt \u2212                                     \u00b51,t)\n                                                            + \u2207xw1,t(Xt)\u22a4\u00b5i,twi,t(Xt)(Xt \u2212                        \u00b51,t)                                    (E.2)\n Combining the gradients of all the terms of Eq. (E.2), we have\n \u2207\u00b51,tLt(s\u03b8t)\n         = E     w1,t(Xt)(Xt \u2212           \u00b51,t)\u00b5\u22a4   1,ts\u03b8t(Xt) + w1,t(Xt)s\u03b8t(Xt) \u2212                    w1,t(Xt)(Xt \u2212           \u00b51,t)     i   wi,t(Xt)\u00b5\u22a4      i,ts\u03b8t(Xt)\n                    + \u2207xw1,t(Xt) + w1,t(Xt)\u00b51,t + \u2207xw1,t(Xt)\u22a4\u00b51,t(Xt \u2212                                       \u00b51,t) \u2212    w1,t(Xt)         i   wi,t(Xt)\u00b5i,t\n                    \u2212   w1,t(Xt)        i   \u2207xwi,t(Xt)\u22a4\u00b5i,t(Xt \u2212                 \u00b51,t) \u2212       i   \u2207xw1,t(Xt)\u22a4\u00b5i,twi,t(Xt)(Xt \u2212                      \u00b51,t)\n         = E      \u2212   w1,t(Xt)(Xt \u2212           \u00b51,t) + w1,t(Xt)(Xt \u2212               \u00b51,t)     i   wi,t(Xt)\u00b5\u22a4      i,t(Xt \u2212     \u00b51,t)\n                    + w1,t(Xt)\u00b51,t \u2212            w1,t(Xt)(Xt \u2212           \u00b51,t)\u22a4\u00b51,t(Xt \u2212           \u00b51,t) \u2212     w1,t(Xt)        i   wi,t(Xt)\u00b5i,t\n                    \u2212   w1,t(Xt)        i   \u2207xwi,t(Xt)\u22a4\u00b5i,t(Xt \u2212                 \u00b51,t)    ,\nwhere the last equality uses Lemma E.3. Specifi                                 cally, it uses\n                                       \u2207xw1,t(Xt) + w1,t(Xt)s\u03b8t(Xt) = \u2212w1,t(Xt)(Xt \u2212                                   \u00b51,t)\n          (\u2207xw1,t(Xt) + w1,t(Xt)s\u03b8t(Xt))\u22a4\u00b51,t(Xt \u2212                              \u00b51,t) = \u2212w1,t(Xt)(Xt \u2212                 \u00b51,t)\u22a4\u00b51,t(Xt \u2212           \u00b51,t) .\nWe will also need the following intermediate calculation:\n                                                                                33", "md": "Applying Stein\u2019s lemma to the expectation of the first term in Eq. (E.1), we have\n\n$$\n\\begin{aligned}\n&\\mathbb{E}_{X0,Zt}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t}^{\\top}Zt] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t,j}Zt,j] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)\\beta_{t}e_{j}\\mu_{1,t,j} + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}e_{j}(Xt - \\mu_{1,t})\\mu_{1,t,j}] \\\\\n&= \\mathbb{E}_{X0,Zt}[w1,t(Xt)\\beta_{t}\\mu_{1,t} + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t)}]\n\\end{aligned}\n$$\n\nThe expectation of the second term in Eq. (E.1) simplifies to $\\beta_{t}\\mathbb{E}_{t}[\\nabla_{x}w1,t(Xt)]$ by Stein\u2019s Lemma.\n\nEach summand in the third term in Eq. (E.1) simplifies as following:\n\n$$\n\\begin{aligned}\n&\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)(Xt - \\mu_{1,t})\\mu_{i,t}^{\\top}Zt] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)(Xt - \\mu_{1,t})\\mu_{i,t,j}Zt,j] \\\\\n&= \\sum_{j} \\mu_{i,t,j}\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)\\beta_{t}e_{j} + \\beta_{t}w1,t(Xt)\\nabla_{x}w_{i,t}(Xt)^{\\top}e_{j}(Xt - \\mu_{1,t}) \\\\\n&\\quad + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}e_{j}w_{i,t}(Xt)(Xt - \\mu_{1,t})] \\\\\n&= \\beta_{t}\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)\\mu_{i,t} + w1,t(Xt)\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad + \\nabla_{x}w1,t(Xt)^{\\top}\\mu_{i,t}w_{i,t}(Xt)(Xt - \\mu_{1,t}]\n\\end{aligned}\n$$\n\nCombining the gradients of all the terms of Eq. (E.2), we have\n\n$$\n\\begin{aligned}\n\\nabla\\mu_{1,t}L_{t}(s\\theta_{t}) &= \\mathbb{E}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t}^{\\top}s\\theta_{t}(Xt) + w1,t(Xt)s\\theta_{t}(Xt) \\\\\n&\\quad - w1,t(Xt)(Xt - \\mu_{1,t})\\sum_{i}w_{i,t}(Xt)\\mu_{i,t}^{\\top}s\\theta_{t}(Xt) \\\\\n&\\quad + \\nabla_{x}w1,t(Xt) + w1,t(Xt)\\mu_{1,t} + \\nabla_{x}w1,t(Xt)^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad - w1,t(Xt)\\sum_{i}w_{i,t}(Xt)\\mu_{i,t} - w1,t(Xt)\\sum_{i}\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad - \\sum_{i}\\nabla_{x}w1,t(Xt)^{\\top}\\mu_{i,t}w_{i,t}(Xt)(Xt - \\mu_{1,t})] \\\\\n&= \\mathbb{E}[-w1,t(Xt)(Xt - \\mu_{1,t}) + w1,t(Xt)(Xt - \\mu_{1,t})\\sum_{i}w_{i,t}(Xt)\\mu_{i,t}^{\\top}(Xt - \\mu_{1,t}) \\\\\n&\\quad + w1,t(Xt)\\mu_{1,t} - w1,t(Xt)(Xt - \\mu_{1,t})^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) - w1,t(Xt)\\sum_{i}w_{i,t}(Xt)\\mu_{i,t} \\\\\n&\\quad - w1,t(Xt)\\sum_{i}\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t})]\n\\end{aligned}\n$$\n\nwhere the last equality uses Lemma E.3. Specifically, it uses\n\n$$\n\\begin{aligned}\n&\\nabla_{x}w1,t(Xt) + w1,t(Xt)s\\theta_{t}(Xt) = -w1,t(Xt)(Xt - \\mu_{1,t}) \\\\\n&(\\nabla_{x}w1,t(Xt) + w1,t(Xt)s\\theta_{t}(Xt))^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) = -w1,t(Xt)(Xt - \\mu_{1,t})^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t})\n\\end{aligned}\n$$\n\nWe will also need the following intermediate calculation:\n\n33", "images": [], "items": [{"type": "text", "value": "Applying Stein\u2019s lemma to the expectation of the first term in Eq. (E.1), we have\n\n$$\n\\begin{aligned}\n&\\mathbb{E}_{X0,Zt}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t}^{\\top}Zt] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t,j}Zt,j] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)\\beta_{t}e_{j}\\mu_{1,t,j} + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}e_{j}(Xt - \\mu_{1,t})\\mu_{1,t,j}] \\\\\n&= \\mathbb{E}_{X0,Zt}[w1,t(Xt)\\beta_{t}\\mu_{1,t} + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t)}]\n\\end{aligned}\n$$\n\nThe expectation of the second term in Eq. (E.1) simplifies to $\\beta_{t}\\mathbb{E}_{t}[\\nabla_{x}w1,t(Xt)]$ by Stein\u2019s Lemma.\n\nEach summand in the third term in Eq. (E.1) simplifies as following:\n\n$$\n\\begin{aligned}\n&\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)(Xt - \\mu_{1,t})\\mu_{i,t}^{\\top}Zt] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)(Xt - \\mu_{1,t})\\mu_{i,t,j}Zt,j] \\\\\n&= \\sum_{j} \\mu_{i,t,j}\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)\\beta_{t}e_{j} + \\beta_{t}w1,t(Xt)\\nabla_{x}w_{i,t}(Xt)^{\\top}e_{j}(Xt - \\mu_{1,t}) \\\\\n&\\quad + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}e_{j}w_{i,t}(Xt)(Xt - \\mu_{1,t})] \\\\\n&= \\beta_{t}\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)\\mu_{i,t} + w1,t(Xt)\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad + \\nabla_{x}w1,t(Xt)^{\\top}\\mu_{i,t}w_{i,t}(Xt)(Xt - \\mu_{1,t}]\n\\end{aligned}\n$$\n\nCombining the gradients of all the terms of Eq. (E.2), we have\n\n$$\n\\begin{aligned}\n\\nabla\\mu_{1,t}L_{t}(s\\theta_{t}) &= \\mathbb{E}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t}^{\\top}s\\theta_{t}(Xt) + w1,t(Xt)s\\theta_{t}(Xt) \\\\\n&\\quad - w1,t(Xt)(Xt - \\mu_{1,t})\\sum_{i}w_{i,t}(Xt)\\mu_{i,t}^{\\top}s\\theta_{t}(Xt) \\\\\n&\\quad + \\nabla_{x}w1,t(Xt) + w1,t(Xt)\\mu_{1,t} + \\nabla_{x}w1,t(Xt)^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad - w1,t(Xt)\\sum_{i}w_{i,t}(Xt)\\mu_{i,t} - w1,t(Xt)\\sum_{i}\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad - \\sum_{i}\\nabla_{x}w1,t(Xt)^{\\top}\\mu_{i,t}w_{i,t}(Xt)(Xt - \\mu_{1,t})] \\\\\n&= \\mathbb{E}[-w1,t(Xt)(Xt - \\mu_{1,t}) + w1,t(Xt)(Xt - \\mu_{1,t})\\sum_{i}w_{i,t}(Xt)\\mu_{i,t}^{\\top}(Xt - \\mu_{1,t}) \\\\\n&\\quad + w1,t(Xt)\\mu_{1,t} - w1,t(Xt)(Xt - \\mu_{1,t})^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) - w1,t(Xt)\\sum_{i}w_{i,t}(Xt)\\mu_{i,t} \\\\\n&\\quad - w1,t(Xt)\\sum_{i}\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t})]\n\\end{aligned}\n$$\n\nwhere the last equality uses Lemma E.3. Specifically, it uses\n\n$$\n\\begin{aligned}\n&\\nabla_{x}w1,t(Xt) + w1,t(Xt)s\\theta_{t}(Xt) = -w1,t(Xt)(Xt - \\mu_{1,t}) \\\\\n&(\\nabla_{x}w1,t(Xt) + w1,t(Xt)s\\theta_{t}(Xt))^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) = -w1,t(Xt)(Xt - \\mu_{1,t})^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t})\n\\end{aligned}\n$$\n\nWe will also need the following intermediate calculation:\n\n33", "md": "Applying Stein\u2019s lemma to the expectation of the first term in Eq. (E.1), we have\n\n$$\n\\begin{aligned}\n&\\mathbb{E}_{X0,Zt}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t}^{\\top}Zt] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t,j}Zt,j] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)\\beta_{t}e_{j}\\mu_{1,t,j} + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}e_{j}(Xt - \\mu_{1,t})\\mu_{1,t,j}] \\\\\n&= \\mathbb{E}_{X0,Zt}[w1,t(Xt)\\beta_{t}\\mu_{1,t} + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t)}]\n\\end{aligned}\n$$\n\nThe expectation of the second term in Eq. (E.1) simplifies to $\\beta_{t}\\mathbb{E}_{t}[\\nabla_{x}w1,t(Xt)]$ by Stein\u2019s Lemma.\n\nEach summand in the third term in Eq. (E.1) simplifies as following:\n\n$$\n\\begin{aligned}\n&\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)(Xt - \\mu_{1,t})\\mu_{i,t}^{\\top}Zt] \\\\\n&= \\sum_{j=1}^{d} \\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)(Xt - \\mu_{1,t})\\mu_{i,t,j}Zt,j] \\\\\n&= \\sum_{j} \\mu_{i,t,j}\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)\\beta_{t}e_{j} + \\beta_{t}w1,t(Xt)\\nabla_{x}w_{i,t}(Xt)^{\\top}e_{j}(Xt - \\mu_{1,t}) \\\\\n&\\quad + \\beta_{t}\\nabla_{x}w1,t(Xt)^{\\top}e_{j}w_{i,t}(Xt)(Xt - \\mu_{1,t})] \\\\\n&= \\beta_{t}\\mathbb{E}_{X0,Zt}[w1,t(Xt)w_{i,t}(Xt)\\mu_{i,t} + w1,t(Xt)\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad + \\nabla_{x}w1,t(Xt)^{\\top}\\mu_{i,t}w_{i,t}(Xt)(Xt - \\mu_{1,t}]\n\\end{aligned}\n$$\n\nCombining the gradients of all the terms of Eq. (E.2), we have\n\n$$\n\\begin{aligned}\n\\nabla\\mu_{1,t}L_{t}(s\\theta_{t}) &= \\mathbb{E}[w1,t(Xt)(Xt - \\mu_{1,t})\\mu_{1,t}^{\\top}s\\theta_{t}(Xt) + w1,t(Xt)s\\theta_{t}(Xt) \\\\\n&\\quad - w1,t(Xt)(Xt - \\mu_{1,t})\\sum_{i}w_{i,t}(Xt)\\mu_{i,t}^{\\top}s\\theta_{t}(Xt) \\\\\n&\\quad + \\nabla_{x}w1,t(Xt) + w1,t(Xt)\\mu_{1,t} + \\nabla_{x}w1,t(Xt)^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad - w1,t(Xt)\\sum_{i}w_{i,t}(Xt)\\mu_{i,t} - w1,t(Xt)\\sum_{i}\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t}) \\\\\n&\\quad - \\sum_{i}\\nabla_{x}w1,t(Xt)^{\\top}\\mu_{i,t}w_{i,t}(Xt)(Xt - \\mu_{1,t})] \\\\\n&= \\mathbb{E}[-w1,t(Xt)(Xt - \\mu_{1,t}) + w1,t(Xt)(Xt - \\mu_{1,t})\\sum_{i}w_{i,t}(Xt)\\mu_{i,t}^{\\top}(Xt - \\mu_{1,t}) \\\\\n&\\quad + w1,t(Xt)\\mu_{1,t} - w1,t(Xt)(Xt - \\mu_{1,t})^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) - w1,t(Xt)\\sum_{i}w_{i,t}(Xt)\\mu_{i,t} \\\\\n&\\quad - w1,t(Xt)\\sum_{i}\\nabla_{x}w_{i,t}(Xt)^{\\top}\\mu_{i,t}(Xt - \\mu_{1,t})]\n\\end{aligned}\n$$\n\nwhere the last equality uses Lemma E.3. Specifically, it uses\n\n$$\n\\begin{aligned}\n&\\nabla_{x}w1,t(Xt) + w1,t(Xt)s\\theta_{t}(Xt) = -w1,t(Xt)(Xt - \\mu_{1,t}) \\\\\n&(\\nabla_{x}w1,t(Xt) + w1,t(Xt)s\\theta_{t}(Xt))^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t}) = -w1,t(Xt)(Xt - \\mu_{1,t})^{\\top}\\mu_{1,t}(Xt - \\mu_{1,t})\n\\end{aligned}\n$$\n\nWe will also need the following intermediate calculation:\n\n33"}]}, {"page": 34, "text": "Lemma E.3. For any i \u2208                    [K], the gradient of wi,t(Xt) with respect to Xt is given by\n        \u2207xwi,t(Xt) = \u2212wi,t(Xt)(Xt \u2212                    \u00b5i,t) \u2212    wi,t(Xt)s\u03b8t(Xt)\n                          = \u2212wi,t(Xt)(1 \u2212            wi,t(Xt))(Xt \u2212         \u00b5i,t) + wi,t(Xt) \u00b7        j\u2208[K]:j\u0338=i   wj,t(Xt)(Xt \u2212         \u00b5j,t) .\nProof. By taking the gradient of wi,t(Xt) and simplifying it, we get the result:\n                                exp      \u2212  \u2225Xt\u2212\u00b5i,t\u2225     2   (Xt \u2212     \u00b5i,t)\n                                                   2\n       \u2207xwi,t(Xt) = \u2212               K  j=1 exp        \u2212  \u2225Xt\u2212\u00b5j,t\u2225     2\n                                                               2\u03c32\n                                                  exp     \u2212   \u2225Xt\u2212\u00b5i,t\u2225    2    \u00b7  K j=1 exp        \u2212  \u2225Xt\u2212\u00b5j,t\u2225     2   (Xt \u2212     \u00b5j,t)\n                                                                     2                                        2\n                                              +                          K   j=1 exp       \u2212   \u2225Xt\u2212\u00b5j,t\u2225     2  2\n                                                                                                      2\n                                                                             \uf8eb    K                                \uf8f6\n                         = \u2212wi,t(Xt)(Xt \u2212             \u00b5i,t) + wi,t(Xt)       \uf8ed   j=1  wj,t(Xt)(Xt \u2212\uf8eb      K  \u00b5j,t) \uf8f8                         \uf8f6\n                         = \u2212wi,t(Xt)(1 \u2212           wi,t(Xt))(Xt \u2212          \u00b5i,t) + wi,t(Xt)        \uf8ed  j=1,j\u0338=i  wj,t(Xt)(Xt \u2212          \u00b5j,t) \uf8f8    .\nWe are now ready to establish the connection between gradient descent on the DDPM objective\nand the gradient EM update, for mixtures of K Gaussians:\nLemma E.4. Suppose the centers of the mixture of K Gaussians are well-separated according to\nAssumption 14, and the parameters \u03b8 = {\u00b51, \u00b52, . . . , \u00b5K} that the student network is initialized to\nsatisfy the warm start Assumption 15. Then, for noise scale t = O(1), gradient descent on the\nDDPM objective is close to the gradient EM update:\n                                                                                          \u2272     K2B2                1\nwhere cr is a large constant. \u2207\u00b51,tLt(s\u03b8t) + E[w1,t(Xt)(Xt \u2212                   \u00b51,t)]         dc2r/4000 =      poly(d) ,\nProof. Observe that the fi                rst term in the expression for the population gradient of the DDPM\nobjective in Lemma E.2 is exactly the gradient EM update for the mixture of K Gaussian in Fact\n6. To prove the closeness between the GD update and the gradient EM update, we will show that\nthe additional terms in Lemma E.2 are small.\n      Note that when the ground truth parameters \u03b8\u2217                               = {\u00b5\u2217   1, \u00b5\u2217 2, . . . , \u00b5\u2217                                        t\nalso satisfi    es Assumption 14 for t = O(1). Similarly, it is straightforward to show that when the      K} satisfy Assumption 14, \u03b8\u2217\nparameters \u03b8 satisfy Assumption 15, \u03b8t = {\u00b51,t, \u00b52,t, . . . , \u00b5K,t} also satisfi                                 es the assumption.\n     We focus on the d \u2264                K case for this proof. A similar calculation with projection onto O(K)\ndimensional subspace of \u00b5\u2217               i,t will give the result for d \u2265              K case [VW04, YYS17].\n      Using Lemma E.6 below, we have\n                          E                                                                                    \u2264    d2c2 rB\n                               w1,t(Xt)(1 \u2212        w1,t(Xt))(Xt \u2212          \u00b51,t)(Xt \u2212       \u00b51,t)\u22a4 \u00b51,t            dc2r/1000 ,\n                                                                           34", "md": "# Math Equations\n\nLemma E.3. For any \\(i \\in [K]\\), the gradient of \\(w_{i,t}(X_t)\\) with respect to \\(X_t\\) is given by\n\n$$\n\\begin{align*}\n\\nabla_x w_{i,t}(X_t) & = -w_{i,t}(X_t)(X_t - \\mu_{i,t}) - w_{i,t}(X_t)s\\theta_t(X_t) \\\\\n& = -w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\cdot \\sum_{j \\in [K]: j \\neq i} w_{j,t}(X_t)(X_t - \\mu_{j,t}) .\n\\end{align*}\n$$\nProof. By taking the gradient of \\(w_{i,t}(X_t)\\) and simplifying it, we get the result:\n\n$$\n\\begin{align*}\n\\nabla_x w_{i,t}(X_t) & = -\\frac{\\exp\\left(-\\frac{\\|X_t-\\mu_{i,t}\\|^2}{2}\\right)(X_t - \\mu_{i,t})}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2\\sigma^2}\\right)} \\\\\n& \\quad + \\frac{\\exp\\left(-\\frac{\\|X_t-\\mu_{i,t}\\|^2}{2}\\right) \\cdot \\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2}\\right)(X_t - \\mu_{j,t})}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2\\sigma^2}\\right)} \\\\\n& = -w_{i,t}(X_t)(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\sum_{j=1} \\left( w_{j,t}(X_t)(X_t - \\mu_{j,t}) \\right) \\\\\n& = -w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\sum_{j=1,j\\neq i} \\left( w_{j,t}(X_t)(X_t - \\mu_{j,t}) \\right) .\n\\end{align*}\n$$\nWe are now ready to establish the connection between gradient descent on the DDPM objective and the gradient EM update, for mixtures of \\(K\\) Gaussians:\n\nLemma E.4. Suppose the centers of the mixture of \\(K\\) Gaussians are well-separated according to Assumption 14, and the parameters \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) that the student network is initialized to satisfy the warm start Assumption 15. Then, for noise scale \\(t = O(1)\\), gradient descent on the DDPM objective is close to the gradient EM update:\n\n$$ \\lesssim \\frac{K^2B^2}{1} $$\nwhere \\(c\\) is a large constant. \\(\\nabla_{\\mu_{1,t}} L_t(s\\theta_t) + E[w_{1,t}(X_t)(X_t - \\mu_{1,t})] \\frac{dc^2r}{4000} = \\text{poly}(d)\\),\n\nProof. Observe that the first term in the expression for the population gradient of the DDPM objective in Lemma E.2 is exactly the gradient EM update for the mixture of \\(K\\) Gaussian in Fact 6. To prove the closeness between the GD update and the gradient EM update, we will show that the additional terms in Lemma E.2 are small.\n\nNote that when the ground truth parameters \\(\\theta^* = \\{\\mu^*_1, \\mu^*_2, ..., \\mu^*_t\\}\\) also satisfy Assumption 14 for \\(t = O(1\\). Similarly, it is straightforward to show that when the parameters \\(\\theta\\) satisfy Assumption 14, \\(\\theta^* = \\{\\mu_1,t, \\mu_2,t, ..., \\mu_K,t\\}\\) also satisfy the assumption.\n\nWe focus on the \\(d \\leq K\\) case for this proof. A similar calculation with projection onto \\(O(K)\\) dimensional subspace of \\(\\mu^*_i,t\\) will give the result for \\(d \\geq K\\) case [VW04, YYS17].\n\nUsing Lemma E.6 below, we have\n\n$$ E \\lesssim \\frac{d^2c^2 rB}{1000} w_{1,t}(X_t)(1 - w_{1,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top \\mu_{1,t} \\frac{dc^2r}{1000} , $$\n34", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Lemma E.3. For any \\(i \\in [K]\\), the gradient of \\(w_{i,t}(X_t)\\) with respect to \\(X_t\\) is given by\n\n$$\n\\begin{align*}\n\\nabla_x w_{i,t}(X_t) & = -w_{i,t}(X_t)(X_t - \\mu_{i,t}) - w_{i,t}(X_t)s\\theta_t(X_t) \\\\\n& = -w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\cdot \\sum_{j \\in [K]: j \\neq i} w_{j,t}(X_t)(X_t - \\mu_{j,t}) .\n\\end{align*}\n$$\nProof. By taking the gradient of \\(w_{i,t}(X_t)\\) and simplifying it, we get the result:\n\n$$\n\\begin{align*}\n\\nabla_x w_{i,t}(X_t) & = -\\frac{\\exp\\left(-\\frac{\\|X_t-\\mu_{i,t}\\|^2}{2}\\right)(X_t - \\mu_{i,t})}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2\\sigma^2}\\right)} \\\\\n& \\quad + \\frac{\\exp\\left(-\\frac{\\|X_t-\\mu_{i,t}\\|^2}{2}\\right) \\cdot \\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2}\\right)(X_t - \\mu_{j,t})}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2\\sigma^2}\\right)} \\\\\n& = -w_{i,t}(X_t)(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\sum_{j=1} \\left( w_{j,t}(X_t)(X_t - \\mu_{j,t}) \\right) \\\\\n& = -w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\sum_{j=1,j\\neq i} \\left( w_{j,t}(X_t)(X_t - \\mu_{j,t}) \\right) .\n\\end{align*}\n$$\nWe are now ready to establish the connection between gradient descent on the DDPM objective and the gradient EM update, for mixtures of \\(K\\) Gaussians:\n\nLemma E.4. Suppose the centers of the mixture of \\(K\\) Gaussians are well-separated according to Assumption 14, and the parameters \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) that the student network is initialized to satisfy the warm start Assumption 15. Then, for noise scale \\(t = O(1)\\), gradient descent on the DDPM objective is close to the gradient EM update:\n\n$$ \\lesssim \\frac{K^2B^2}{1} $$\nwhere \\(c\\) is a large constant. \\(\\nabla_{\\mu_{1,t}} L_t(s\\theta_t) + E[w_{1,t}(X_t)(X_t - \\mu_{1,t})] \\frac{dc^2r}{4000} = \\text{poly}(d)\\),\n\nProof. Observe that the first term in the expression for the population gradient of the DDPM objective in Lemma E.2 is exactly the gradient EM update for the mixture of \\(K\\) Gaussian in Fact 6. To prove the closeness between the GD update and the gradient EM update, we will show that the additional terms in Lemma E.2 are small.\n\nNote that when the ground truth parameters \\(\\theta^* = \\{\\mu^*_1, \\mu^*_2, ..., \\mu^*_t\\}\\) also satisfy Assumption 14 for \\(t = O(1\\). Similarly, it is straightforward to show that when the parameters \\(\\theta\\) satisfy Assumption 14, \\(\\theta^* = \\{\\mu_1,t, \\mu_2,t, ..., \\mu_K,t\\}\\) also satisfy the assumption.\n\nWe focus on the \\(d \\leq K\\) case for this proof. A similar calculation with projection onto \\(O(K)\\) dimensional subspace of \\(\\mu^*_i,t\\) will give the result for \\(d \\geq K\\) case [VW04, YYS17].\n\nUsing Lemma E.6 below, we have\n\n$$ E \\lesssim \\frac{d^2c^2 rB}{1000} w_{1,t}(X_t)(1 - w_{1,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top \\mu_{1,t} \\frac{dc^2r}{1000} , $$\n34", "md": "Lemma E.3. For any \\(i \\in [K]\\), the gradient of \\(w_{i,t}(X_t)\\) with respect to \\(X_t\\) is given by\n\n$$\n\\begin{align*}\n\\nabla_x w_{i,t}(X_t) & = -w_{i,t}(X_t)(X_t - \\mu_{i,t}) - w_{i,t}(X_t)s\\theta_t(X_t) \\\\\n& = -w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\cdot \\sum_{j \\in [K]: j \\neq i} w_{j,t}(X_t)(X_t - \\mu_{j,t}) .\n\\end{align*}\n$$\nProof. By taking the gradient of \\(w_{i,t}(X_t)\\) and simplifying it, we get the result:\n\n$$\n\\begin{align*}\n\\nabla_x w_{i,t}(X_t) & = -\\frac{\\exp\\left(-\\frac{\\|X_t-\\mu_{i,t}\\|^2}{2}\\right)(X_t - \\mu_{i,t})}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2\\sigma^2}\\right)} \\\\\n& \\quad + \\frac{\\exp\\left(-\\frac{\\|X_t-\\mu_{i,t}\\|^2}{2}\\right) \\cdot \\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2}\\right)(X_t - \\mu_{j,t})}{\\sum_{j=1}^{K} \\exp\\left(-\\frac{\\|X_t-\\mu_{j,t}\\|^2}{2\\sigma^2}\\right)} \\\\\n& = -w_{i,t}(X_t)(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\sum_{j=1} \\left( w_{j,t}(X_t)(X_t - \\mu_{j,t}) \\right) \\\\\n& = -w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{i,t}) + w_{i,t}(X_t) \\sum_{j=1,j\\neq i} \\left( w_{j,t}(X_t)(X_t - \\mu_{j,t}) \\right) .\n\\end{align*}\n$$\nWe are now ready to establish the connection between gradient descent on the DDPM objective and the gradient EM update, for mixtures of \\(K\\) Gaussians:\n\nLemma E.4. Suppose the centers of the mixture of \\(K\\) Gaussians are well-separated according to Assumption 14, and the parameters \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) that the student network is initialized to satisfy the warm start Assumption 15. Then, for noise scale \\(t = O(1)\\), gradient descent on the DDPM objective is close to the gradient EM update:\n\n$$ \\lesssim \\frac{K^2B^2}{1} $$\nwhere \\(c\\) is a large constant. \\(\\nabla_{\\mu_{1,t}} L_t(s\\theta_t) + E[w_{1,t}(X_t)(X_t - \\mu_{1,t})] \\frac{dc^2r}{4000} = \\text{poly}(d)\\),\n\nProof. Observe that the first term in the expression for the population gradient of the DDPM objective in Lemma E.2 is exactly the gradient EM update for the mixture of \\(K\\) Gaussian in Fact 6. To prove the closeness between the GD update and the gradient EM update, we will show that the additional terms in Lemma E.2 are small.\n\nNote that when the ground truth parameters \\(\\theta^* = \\{\\mu^*_1, \\mu^*_2, ..., \\mu^*_t\\}\\) also satisfy Assumption 14 for \\(t = O(1\\). Similarly, it is straightforward to show that when the parameters \\(\\theta\\) satisfy Assumption 14, \\(\\theta^* = \\{\\mu_1,t, \\mu_2,t, ..., \\mu_K,t\\}\\) also satisfy the assumption.\n\nWe focus on the \\(d \\leq K\\) case for this proof. A similar calculation with projection onto \\(O(K)\\) dimensional subspace of \\(\\mu^*_i,t\\) will give the result for \\(d \\geq K\\) case [VW04, YYS17].\n\nUsing Lemma E.6 below, we have\n\n$$ E \\lesssim \\frac{d^2c^2 rB}{1000} w_{1,t}(X_t)(1 - w_{1,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top \\mu_{1,t} \\frac{dc^2r}{1000} , $$\n34"}]}, {"page": 35, "text": "for any i \u2208   [K]. We can simplify additional terms as\n                     K  E[w1,t(Xt)wi,t(Xt)(Xt \u2212        \u00b51,t)(Xt \u2212   \u00b51,t)\u22a4\u00b5i,t]\n                   i=2      K\n                        \u2264   i=2 E[\u2225w1,t(Xt)wi,t(Xt)(Xt \u2212        \u00b51,t)(Xt \u2212   \u00b51,t)\u22a4\u00b5i,t\u2225]\n                        \u2264   K      E  |w1,t(Xt)wi,t(Xt)|2      \u00b7 E \u2225(Xt \u2212    \u00b51,t)(Xt \u2212   \u00b51,t)\u22a4\u00b5i,t\u22252\n                            i=2\n                             KB2\n                        \u2264     r/2000 ,\n                            dc2\nwhere in the last step we used the second part of Lemma E.5. This will allow us to prove that\n\u2225E[w1,t(Xt)(Xt \u2212       \u00b51,t)  K i=1 wi,t(Xt)\u00b5\u22a4 i,t(Xt \u2212  \u00b51,t) \u2212  w1,t(Xt)(Xt \u2212      \u00b51,t)\u22a4\u00b51,t(Xt \u2212    \u00b51,t)]\u2225  is small.\n     Using the expression for \u2207xwi,t(Xt) from Lemma E.3, we have\n                K\n                w1,t(Xt)\u2207xwi,t(Xt)\u22a4\u00b5i,t(Xt \u2212             \u00b51,t)\n               i=1\n                            K\n                     = \u2212   w1,t(Xt)wi,t(Xt)(1 \u2212          wi,t(Xt))(Xt \u2212     \u00b51,t)(Xt \u2212   \u00b5i,t)\u22a4\u00b5i,t\n                           i=1\n                                 K      K\n                             +  i=1  j=1,j\u0338=iw1,t(Xt)wi,t(Xt)wj,t(Xt)(Xt \u2212         \u00b51,t)(Xt \u2212   \u00b5j,t)\u22a4\u00b5i,t .\nThe fi  rst term can be simplifi     ed as follows:\n        K   E  w1,t(Xt)wi,t(Xt)(1 \u2212      wi,t(Xt))(Xt \u2212     \u00b51,t)(Xt \u2212   \u00b5i,t)\u22a4\u00b5i,t\n       i=1\n                K\n            \u2264  i=1  E   w1,t(Xt)wi,t(Xt)(1 \u2212      wi,t(Xt))(Xt \u2212     \u00b51,t)(Xt \u2212    \u00b5i,t)\u22a4\u00b5i,t\n            \u2264   K     E[w1,t(Xt)2wi,t(Xt)2] \u00b7 E       (1 \u2212  wi,t(Xt))2 \u00b7 \u2225Xt \u2212    \u00b51,t\u22252 \u00b7 \u2225Xt \u2212   \u00b5i,t\u22252 \u00b7 \u2225\u00b5i,t\u22252\n               i=2\n                 KB2\n            \u2272  dc2r/4000 ,\nwhere the last inequality follows from\n                 E  \u2225Xt \u2212    \u00b51,t\u22252\u2225Xt \u2212    \u00b5i,t\u22252   \u2264    E  \u2225Xt \u2212    \u00b51,t\u22254 E   \u2225Xt \u2212   \u00b5i,t\u22254   \u2272  B2 .\nSimilarly, by simplifying the second term, we get\n      K      K\n      i=1 j=1,j\u0338=iE   w1,t(Xt)wi,t(Xt)wj,t(Xt)(Xt \u2212          \u00b51,t)(Xt \u2212   \u00b5j,t)\u22a4\u00b5i,t\n           \u2264   K      K       E  w2i,t(Xt)w2 j,t(Xt)  E  w21,t(Xt)\u2225(Xt \u2212     \u00b51,t)(Xt \u2212   \u00b5j,t)\u00b5i,t\u22252   \u2272   K2B2\n               i=1 j=1,j\u0338=i                                                                                dc2r/4000 ,\n                                                            35", "md": "for any i \u2208 [K]. We can simplify additional terms as\n\n$$\n\\begin{aligned}\n&\\sum_{i=2}^{K} E[w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}] \\\\\n&\\leq \\sum_{i=2}^{K} E[\\|w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}\\|] \\\\\n&\\leq \\sum_{i=2}^{K} E |w_{1,t}(X_t)w_{i,t}(X_t)|^2 \\cdot E \\|(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}\\|^2 \\\\\n&\\leq \\frac{KB^2}{dc^2} \\leq \\frac{r}{2000},\n\\end{aligned}\n$$\nwhere in the last step we used the second part of Lemma E.5. This will allow us to prove that\n\n$$\n\\|E[w_{1,t}(X_t)(X_t - \\mu_{1,t}) - \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_i^\\top(X_t - \\mu_{1,t}) - w_{1,t}(X_t)(X_t - \\mu_{1,t})^\\top\\mu_{1,t}(X_t - \\mu_{1,t})]\\| \\text{ is small.}\n$$\nUsing the expression for \u2207xwi,t(Xt) from Lemma E.3, we have\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} w_{1,t}(X_t)\\nabla_xw_{i,t}(X_t)^\\top\\mu_{i,t}(X_t - \\mu_{1,t}) \\\\\n&= -\\sum_{i=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&+ \\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_j,t)^\\top\\mu_{i,t}.\n\\end{aligned}\n$$\nThe first term can be simplified as follows:\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} E w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&\\leq \\sum_{i=1}^{K} E w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&\\leq K E[w_{1,t}(X_t)^2w_{i,t}(X_t)^2] \\cdot E (1 - w_{i,t}(X_t))^2 \\cdot \\|X_t - \\mu_{1,t}\\|^2 \\cdot \\|X_t - \\mu_i,t\\|^2 \\cdot \\|\\mu_{i,t}\\|^2 \\\\\n&\\leq \\frac{dc^2r}{4000},\n\\end{aligned}\n$$\nwhere the last inequality follows from\n\n$$\nE \\|X_t - \\mu_{1,t}\\|^2\\|X_t - \\mu_i,t\\|^2 \\leq E \\|X_t - \\mu_{1,t}\\|^4 E \\|X_t - \\mu_i,t\\|^4 \\leq B^2.\n$$\nSimilarly, by simplifying the second term, we get\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} E w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_j,t)^\\top\\mu_{i,t} \\\\\n&\\leq \\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} E w_{2i,t}(X_t)w_{2j,t}(X_t) E w_{1,t}(X_t)\\|(X_t - \\mu_{1,t})(X_t - \\mu_j,t)\\mu_{i,t}\\|^2 \\leq K^2B^2 \\frac{dc^2r}{4000},\n\\end{aligned}\n$$\nwhere the last inequality follows from\n\n$$\n35\n$$", "images": [], "items": [{"type": "text", "value": "for any i \u2208 [K]. We can simplify additional terms as\n\n$$\n\\begin{aligned}\n&\\sum_{i=2}^{K} E[w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}] \\\\\n&\\leq \\sum_{i=2}^{K} E[\\|w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}\\|] \\\\\n&\\leq \\sum_{i=2}^{K} E |w_{1,t}(X_t)w_{i,t}(X_t)|^2 \\cdot E \\|(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}\\|^2 \\\\\n&\\leq \\frac{KB^2}{dc^2} \\leq \\frac{r}{2000},\n\\end{aligned}\n$$\nwhere in the last step we used the second part of Lemma E.5. This will allow us to prove that\n\n$$\n\\|E[w_{1,t}(X_t)(X_t - \\mu_{1,t}) - \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_i^\\top(X_t - \\mu_{1,t}) - w_{1,t}(X_t)(X_t - \\mu_{1,t})^\\top\\mu_{1,t}(X_t - \\mu_{1,t})]\\| \\text{ is small.}\n$$\nUsing the expression for \u2207xwi,t(Xt) from Lemma E.3, we have\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} w_{1,t}(X_t)\\nabla_xw_{i,t}(X_t)^\\top\\mu_{i,t}(X_t - \\mu_{1,t}) \\\\\n&= -\\sum_{i=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&+ \\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_j,t)^\\top\\mu_{i,t}.\n\\end{aligned}\n$$\nThe first term can be simplified as follows:\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} E w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&\\leq \\sum_{i=1}^{K} E w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&\\leq K E[w_{1,t}(X_t)^2w_{i,t}(X_t)^2] \\cdot E (1 - w_{i,t}(X_t))^2 \\cdot \\|X_t - \\mu_{1,t}\\|^2 \\cdot \\|X_t - \\mu_i,t\\|^2 \\cdot \\|\\mu_{i,t}\\|^2 \\\\\n&\\leq \\frac{dc^2r}{4000},\n\\end{aligned}\n$$\nwhere the last inequality follows from\n\n$$\nE \\|X_t - \\mu_{1,t}\\|^2\\|X_t - \\mu_i,t\\|^2 \\leq E \\|X_t - \\mu_{1,t}\\|^4 E \\|X_t - \\mu_i,t\\|^4 \\leq B^2.\n$$\nSimilarly, by simplifying the second term, we get\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} E w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_j,t)^\\top\\mu_{i,t} \\\\\n&\\leq \\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} E w_{2i,t}(X_t)w_{2j,t}(X_t) E w_{1,t}(X_t)\\|(X_t - \\mu_{1,t})(X_t - \\mu_j,t)\\mu_{i,t}\\|^2 \\leq K^2B^2 \\frac{dc^2r}{4000},\n\\end{aligned}\n$$\nwhere the last inequality follows from\n\n$$\n35\n$$", "md": "for any i \u2208 [K]. We can simplify additional terms as\n\n$$\n\\begin{aligned}\n&\\sum_{i=2}^{K} E[w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}] \\\\\n&\\leq \\sum_{i=2}^{K} E[\\|w_{1,t}(X_t)w_{i,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}\\|] \\\\\n&\\leq \\sum_{i=2}^{K} E |w_{1,t}(X_t)w_{i,t}(X_t)|^2 \\cdot E \\|(X_t - \\mu_{1,t})(X_t - \\mu_{1,t})^\\top\\mu_{i,t}\\|^2 \\\\\n&\\leq \\frac{KB^2}{dc^2} \\leq \\frac{r}{2000},\n\\end{aligned}\n$$\nwhere in the last step we used the second part of Lemma E.5. This will allow us to prove that\n\n$$\n\\|E[w_{1,t}(X_t)(X_t - \\mu_{1,t}) - \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_i^\\top(X_t - \\mu_{1,t}) - w_{1,t}(X_t)(X_t - \\mu_{1,t})^\\top\\mu_{1,t}(X_t - \\mu_{1,t})]\\| \\text{ is small.}\n$$\nUsing the expression for \u2207xwi,t(Xt) from Lemma E.3, we have\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} w_{1,t}(X_t)\\nabla_xw_{i,t}(X_t)^\\top\\mu_{i,t}(X_t - \\mu_{1,t}) \\\\\n&= -\\sum_{i=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&+ \\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_j,t)^\\top\\mu_{i,t}.\n\\end{aligned}\n$$\nThe first term can be simplified as follows:\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} E w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&\\leq \\sum_{i=1}^{K} E w_{1,t}(X_t)w_{i,t}(X_t)(1 - w_{i,t}(X_t))(X_t - \\mu_{1,t})(X_t - \\mu_i,t)^\\top\\mu_{i,t} \\\\\n&\\leq K E[w_{1,t}(X_t)^2w_{i,t}(X_t)^2] \\cdot E (1 - w_{i,t}(X_t))^2 \\cdot \\|X_t - \\mu_{1,t}\\|^2 \\cdot \\|X_t - \\mu_i,t\\|^2 \\cdot \\|\\mu_{i,t}\\|^2 \\\\\n&\\leq \\frac{dc^2r}{4000},\n\\end{aligned}\n$$\nwhere the last inequality follows from\n\n$$\nE \\|X_t - \\mu_{1,t}\\|^2\\|X_t - \\mu_i,t\\|^2 \\leq E \\|X_t - \\mu_{1,t}\\|^4 E \\|X_t - \\mu_i,t\\|^4 \\leq B^2.\n$$\nSimilarly, by simplifying the second term, we get\n\n$$\n\\begin{aligned}\n&\\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} E w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)(X_t - \\mu_{1,t})(X_t - \\mu_j,t)^\\top\\mu_{i,t} \\\\\n&\\leq \\sum_{i=1}^{K} \\sum_{j=1,j\\neq i} E w_{2i,t}(X_t)w_{2j,t}(X_t) E w_{1,t}(X_t)\\|(X_t - \\mu_{1,t})(X_t - \\mu_j,t)\\mu_{i,t}\\|^2 \\leq K^2B^2 \\frac{dc^2r}{4000},\n\\end{aligned}\n$$\nwhere the last inequality follows from\n\n$$\n35\n$$"}]}, {"page": 36, "text": "where the last inequality uses Lemma E.5. Simplifying the following term using Lemma E.5, we\nhave\n               E[w1,t(Xt)\u00b51,t \u2212           w1,t(Xt)       K    wi,t(Xt)\u00b5i,t]\n                                                        i=1\n                            K        w1,t(Xt)wi,t(Xt)\u00b5i,t               +    K    E   w1,t(Xt)wi,t(Xt)\u00b51,t                \u2264     2KB\n                       \u2264   i=2  E                                           i=2                                                dc2r/200 .\nCombining all the results, we obtain the theorem statement.\nThe above proof made use of the following two helper lemmas which follow from prior work analyzing\nEM for learning mixtures of Gaussians:\nLemma E.5. There is some absolute constant cr > 0 for which the following holds.                                                            For any\n\u03b8 = {\u00b51, \u00b52, . . . , \u00b5K} such that \u2225\u00b5i \u2212                  \u00b5\u2217i \u2225  \u2264   cr \u221a  log d for all i \u2208        [K] and any j such that j \u0338= i, we\n                                                                      4\nhave\n                                                                                              1\n                                                   E Xt\u223cN (\u00b5\u2217   i,t,I)[wj,t(Xt)] \u2264        dc2r/100 .\nAdditionally, for any j \u0338= k such that j \u2208                      [K] and k \u2208         [K], we have\n                                                                                              1\n                                                   E  Xt[wj,t(Xt)wk,t(Xt)] \u2264              dc2r/200 .\nProof. Using Proposition 4.1 from [SN21], for any \u03b8 = {\u00b51, \u00b52, . . . , \u00b5K} such that \u2225\u00b5i \u2212                                                     \u00b5\u2217i \u2225  \u2264\ncr \u221a\n 4    log d for all i \u2208        [K] and j \u0338= i, we have                                         1\n                                                   E  Xt\u223cN (\u00b5\u2217   i,t,I)[wj,t(Xt)] \u2264       dc2r/100 .\nComputing the expectation of the product of the weights wj,t and wk,t for any distinct j, k, we have\n                                                           K\n                    E Xt[wj,t(Xt)wk,t(Xt)] =              i=1   K1Ex\u223cN (\u00b5\u2217     i ,I)[wj,t(x)wk,t(x)]\n                                                      \u2264    1    K       Ex\u223cN (\u00b5\u2217    i ,I)[wj,t(x)2]Ex\u223cN (\u00b5\u2217       i ,I)[wk,t(x)2]\n                                                           K   i=1\n                                                               1\n                                                      \u2264    dc2r/200\nwhere the last inequality uses the fact that either i \u0338= j or i \u0338= k and wj,t(x)2 \u2264                                         wj,t(x) \u2264      1.\nLemma E.6 (Lemma 4.3 of [SN21]). Suppose X is distributed according to a mixture of K Gaussians\nwith centers \u03b8\u2217         = {\u00b5\u2217   1, . . . , \u00b5\u2217                                                                                                     i \u2225 \u2264\ncr \u221a                                       K} as in Eq. (6). For any \u03b8 = {\u00b51, \u00b52, . . . , \u00b5K} such that \u2225\u00b5i \u2212                                   \u00b5\u2217\n 4    log d for all i \u2208        [K], then for any distinct i, j \u2208                [K], we have                       d2c2 r\n                             EX[wi(X, \u00b5)(1 \u2212             wi(X, \u00b5))(X \u2212           \u00b5i)(X \u2212      \u00b5i)\u22a4]     op \u2264    dc2r/1000\n                                                                                                                   d2c2 r\n                                       EX[wi(X, \u03b8)wj(x, \u03b8)(X \u2212             36   \u00b5i)(X \u2212       \u00b5j)\u22a4]     op \u2264    dc2r/1000", "md": "where the last inequality uses Lemma E.5. Simplifying the following term using Lemma E.5, we have\n\n$$\n\\begin{align*}\n& E[w_{1,t}(X_t)\\mu_{1,t} - w_{1,t}(X_t) \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t}] \\\\\n& \\leq K \\sum_{i=2}^{K} E[w_{1,t}(X_t)w_{i,t}(X_t)\\mu_{i,t}] + K E[w_{1,t}(X_t)w_{i,t}(X_t)\\mu_{1,t}] \\\\\n& \\leq 2KB \\leq \\frac{dc^2r}{200}.\n\\end{align*}\n$$\nCombining all the results, we obtain the theorem statement.\n\nThe above proof made use of the following two helper lemmas which follow from prior work analyzing EM for learning mixtures of Gaussians:\n\nLemma E.5. There is some absolute constant \\(c_r > 0\\) for which the following holds. For any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{\\log d}\\) for all \\(i \\in [K]\\) and any \\(j\\) such that \\(j \\neq i\\), we have\n\n$$\nE[X_t \\sim N(\\mu^*_i,t,I)][w_{j,t}(X_t)] \\leq \\frac{dc^2r}{100}.\n$$\nAdditionally, for any \\(j \\neq k\\) such that \\(j \\in [K]\\) and \\(k \\in [K]\\), we have\n\n$$\nE[X_t[w_{j,t}(X_t)w_{k,t}(X_t)]] \\leq \\frac{dc^2r}{200}.\n$$\nProof. Using Proposition 4.1 from [SN21], for any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{4 \\log d}\\) for all \\(i \\in [K]\\) and \\(j \\neq i\\), we have\n\n$$\nE[X_t \\sim N(\\mu^*_i,t,I)][w_{j,t}(X_t)] \\leq \\frac{dc^2r}{100}.\n$$\nComputing the expectation of the product of the weights \\(w_{j,t}\\) and \\(w_{k,t}\\) for any distinct \\(j, k\\), we have\n\n$$\n\\begin{align*}\n& E[X_t[w_{j,t}(X_t)w_{k,t}(X_t)]] \\\\\n& = \\sum_{i=1}^{K} K1E_x \\sim N(\\mu^*_i,I)[w_{j,t}(x)w_{k,t}(x)] \\\\\n& \\leq \\sum_{i=1}^{K} E_x \\sim N(\\mu^*_i,I)[w_{j,t}(x)^2]E_x \\sim N(\\mu^*_i,I)[w_{k,t}(x)^2] \\\\\n& \\leq \\frac{dc^2r}{200}\n\\end{align*}\n$$\nwhere the last inequality uses the fact that either \\(i \\neq j\\) or \\(i \\neq k\\) and \\(w_{j,t}(x)^2 \\leq w_{j,t}(x) \\leq 1.\n\nLemma E.6 (Lemma 4.3 of [SN21]). Suppose \\(X\\) is distributed according to a mixture of \\(K\\) Gaussians with centers \\(\\theta^* = \\{\\mu^*_1, ..., \\mu^*_i\\}\\) as in Eq. (6). For any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{4 \\log d}\\) for all \\(i \\in [K]\\), then for any distinct \\(i, j \\in [K]\\), we have\n\n$$\n\\begin{align*}\n& E[X[w_i(X, \\mu)(1 - w_i(X, \\mu))(X - \\mu_i)(X - \\mu_i)^T]] \\text{op} \\leq \\frac{dc^2r}{1000} \\\\\n& E[X[w_i(X, \\theta)w_j(x, \\theta)(X - \\mu_i)(X - \\mu_j)^T]] \\text{op} \\leq \\frac{dc^2r}{1000}\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "where the last inequality uses Lemma E.5. Simplifying the following term using Lemma E.5, we have\n\n$$\n\\begin{align*}\n& E[w_{1,t}(X_t)\\mu_{1,t} - w_{1,t}(X_t) \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t}] \\\\\n& \\leq K \\sum_{i=2}^{K} E[w_{1,t}(X_t)w_{i,t}(X_t)\\mu_{i,t}] + K E[w_{1,t}(X_t)w_{i,t}(X_t)\\mu_{1,t}] \\\\\n& \\leq 2KB \\leq \\frac{dc^2r}{200}.\n\\end{align*}\n$$\nCombining all the results, we obtain the theorem statement.\n\nThe above proof made use of the following two helper lemmas which follow from prior work analyzing EM for learning mixtures of Gaussians:\n\nLemma E.5. There is some absolute constant \\(c_r > 0\\) for which the following holds. For any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{\\log d}\\) for all \\(i \\in [K]\\) and any \\(j\\) such that \\(j \\neq i\\), we have\n\n$$\nE[X_t \\sim N(\\mu^*_i,t,I)][w_{j,t}(X_t)] \\leq \\frac{dc^2r}{100}.\n$$\nAdditionally, for any \\(j \\neq k\\) such that \\(j \\in [K]\\) and \\(k \\in [K]\\), we have\n\n$$\nE[X_t[w_{j,t}(X_t)w_{k,t}(X_t)]] \\leq \\frac{dc^2r}{200}.\n$$\nProof. Using Proposition 4.1 from [SN21], for any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{4 \\log d}\\) for all \\(i \\in [K]\\) and \\(j \\neq i\\), we have\n\n$$\nE[X_t \\sim N(\\mu^*_i,t,I)][w_{j,t}(X_t)] \\leq \\frac{dc^2r}{100}.\n$$\nComputing the expectation of the product of the weights \\(w_{j,t}\\) and \\(w_{k,t}\\) for any distinct \\(j, k\\), we have\n\n$$\n\\begin{align*}\n& E[X_t[w_{j,t}(X_t)w_{k,t}(X_t)]] \\\\\n& = \\sum_{i=1}^{K} K1E_x \\sim N(\\mu^*_i,I)[w_{j,t}(x)w_{k,t}(x)] \\\\\n& \\leq \\sum_{i=1}^{K} E_x \\sim N(\\mu^*_i,I)[w_{j,t}(x)^2]E_x \\sim N(\\mu^*_i,I)[w_{k,t}(x)^2] \\\\\n& \\leq \\frac{dc^2r}{200}\n\\end{align*}\n$$\nwhere the last inequality uses the fact that either \\(i \\neq j\\) or \\(i \\neq k\\) and \\(w_{j,t}(x)^2 \\leq w_{j,t}(x) \\leq 1.\n\nLemma E.6 (Lemma 4.3 of [SN21]). Suppose \\(X\\) is distributed according to a mixture of \\(K\\) Gaussians with centers \\(\\theta^* = \\{\\mu^*_1, ..., \\mu^*_i\\}\\) as in Eq. (6). For any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{4 \\log d}\\) for all \\(i \\in [K]\\), then for any distinct \\(i, j \\in [K]\\), we have\n\n$$\n\\begin{align*}\n& E[X[w_i(X, \\mu)(1 - w_i(X, \\mu))(X - \\mu_i)(X - \\mu_i)^T]] \\text{op} \\leq \\frac{dc^2r}{1000} \\\\\n& E[X[w_i(X, \\theta)w_j(x, \\theta)(X - \\mu_i)(X - \\mu_j)^T]] \\text{op} \\leq \\frac{dc^2r}{1000}\n\\end{align*}\n$$", "md": "where the last inequality uses Lemma E.5. Simplifying the following term using Lemma E.5, we have\n\n$$\n\\begin{align*}\n& E[w_{1,t}(X_t)\\mu_{1,t} - w_{1,t}(X_t) \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{i,t}] \\\\\n& \\leq K \\sum_{i=2}^{K} E[w_{1,t}(X_t)w_{i,t}(X_t)\\mu_{i,t}] + K E[w_{1,t}(X_t)w_{i,t}(X_t)\\mu_{1,t}] \\\\\n& \\leq 2KB \\leq \\frac{dc^2r}{200}.\n\\end{align*}\n$$\nCombining all the results, we obtain the theorem statement.\n\nThe above proof made use of the following two helper lemmas which follow from prior work analyzing EM for learning mixtures of Gaussians:\n\nLemma E.5. There is some absolute constant \\(c_r > 0\\) for which the following holds. For any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{\\log d}\\) for all \\(i \\in [K]\\) and any \\(j\\) such that \\(j \\neq i\\), we have\n\n$$\nE[X_t \\sim N(\\mu^*_i,t,I)][w_{j,t}(X_t)] \\leq \\frac{dc^2r}{100}.\n$$\nAdditionally, for any \\(j \\neq k\\) such that \\(j \\in [K]\\) and \\(k \\in [K]\\), we have\n\n$$\nE[X_t[w_{j,t}(X_t)w_{k,t}(X_t)]] \\leq \\frac{dc^2r}{200}.\n$$\nProof. Using Proposition 4.1 from [SN21], for any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{4 \\log d}\\) for all \\(i \\in [K]\\) and \\(j \\neq i\\), we have\n\n$$\nE[X_t \\sim N(\\mu^*_i,t,I)][w_{j,t}(X_t)] \\leq \\frac{dc^2r}{100}.\n$$\nComputing the expectation of the product of the weights \\(w_{j,t}\\) and \\(w_{k,t}\\) for any distinct \\(j, k\\), we have\n\n$$\n\\begin{align*}\n& E[X_t[w_{j,t}(X_t)w_{k,t}(X_t)]] \\\\\n& = \\sum_{i=1}^{K} K1E_x \\sim N(\\mu^*_i,I)[w_{j,t}(x)w_{k,t}(x)] \\\\\n& \\leq \\sum_{i=1}^{K} E_x \\sim N(\\mu^*_i,I)[w_{j,t}(x)^2]E_x \\sim N(\\mu^*_i,I)[w_{k,t}(x)^2] \\\\\n& \\leq \\frac{dc^2r}{200}\n\\end{align*}\n$$\nwhere the last inequality uses the fact that either \\(i \\neq j\\) or \\(i \\neq k\\) and \\(w_{j,t}(x)^2 \\leq w_{j,t}(x) \\leq 1.\n\nLemma E.6 (Lemma 4.3 of [SN21]). Suppose \\(X\\) is distributed according to a mixture of \\(K\\) Gaussians with centers \\(\\theta^* = \\{\\mu^*_1, ..., \\mu^*_i\\}\\) as in Eq. (6). For any \\(\\theta = \\{\\mu_1, \\mu_2, ..., \\mu_K\\}\\) such that \\(\\|\\mu_i - \\mu^*_i\\| \\leq c_r \\sqrt{4 \\log d}\\) for all \\(i \\in [K]\\), then for any distinct \\(i, j \\in [K]\\), we have\n\n$$\n\\begin{align*}\n& E[X[w_i(X, \\mu)(1 - w_i(X, \\mu))(X - \\mu_i)(X - \\mu_i)^T]] \\text{op} \\leq \\frac{dc^2r}{1000} \\\\\n& E[X[w_i(X, \\theta)w_j(x, \\theta)(X - \\mu_i)(X - \\mu_j)^T]] \\text{op} \\leq \\frac{dc^2r}{1000}\n\\end{align*}\n$$"}]}, {"page": 37, "text": "E.2         Closeness between population gradient descent and empirical gradient de-\n            scent\nIn this section, we show that the population gradient descent on the DDPM objective is close to\nthe empirical gradient descent for mixtures of K Gaussians.\n                                                               1\nLemma E.7. For any \u03b5 that is \u0398(                            poly(d)) and noise scale t > t\u2032 where t\u2032 \u2272                          1, the empirical estimate\nof gradient descent update on the DDPM objective with the number of samples n > n\u2032 concentrates\nwell to the population gradient descent update where n\u2032 = O(K4d5B6                                        \u03b52     ). More specifically, the following\ninequality holds with probability at least 1 \u2212                             exp(\u2212d0.99):\n                                          \u2207\u00b51,t       1    n    Lt(s\u03b8t(xi,0, zi,t))           \u2212   \u2207\u00b51,tLt(s\u03b8t)           \u2264    \u03b5.\nProof. Recall that the population gradient is given byn   i=1\nwhere                               \u2207\u00b51,tLt(s\u03b8t) = E              12\u2207\u00b51,t      s\u03b8t(Xt)       2 + \u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt \u03b2  t               ,\n      E   12\u2207\u00b51,t      s\u03b8t(Xt)       2      = E        w1,t(Xt)(Xt \u2212      K     \u00b51,t)\u00b5\u22a4   1,t + w1,t(Xt) \u00b7 Id              K\nand                                                   \u2212   w1,t(Xt)       i=1   wi,t(Xt)(Xt \u2212           \u00b51,t)\u00b5\u22a4   i,t   \u00b7  i=1    wi,t(Xt)\u00b5i,t \u2212          Xt       ,\n            E   \u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt                = E        w1,t(Xt)(Xt \u2212            \u00b51,t)\u00b5\u22a4   1,tZt       K\n                                                              + w1,t(Xt)Zt \u2212             w1,t(Xt)       i=1   wi,t(Xt)(Xt \u2212           \u00b51,t)\u00b5\u22a4   i,tZt       . (E.3)\nWe will prove that the sample estimate of each coordinate in Eq. (E.3) concentrates well around the\nexpectation. We will prove the concentration of the fi                                     rst coordinate and a similar analysis holds for\nother coordinates. For the rest of the proof, we use \u02dc                                   xt to denote the fi            rst coordinate of Xt and \u02dc                 \u00b5i,t\nto indicate the fi           rst coordinate \u00b5i,t. For any random variable Y \u2208                                       R, we use \u2225Y \u2225\u03c81 to denote the\nsub-exponential norm of Y and \u2225Y \u2225\u03c82 to denote the sub-gaussian norm of Y (See lemma B.1 for\ndetails). Using properties of a sub-Gaussian random variable from Lemma B.1, we get\n                           K    w1,t(Xt)wj,t(Xt)(\u02dc            xt \u2212    \u02dc\n                          j=1                                         \u00b51,t)\u00b5\u22a4   1,t\u00b5j,t    \u03c82\n                          K                                    xt \u2212    \u02dc\n                    \u2272    j=1    w1,t(Xt)wj,t(Xt)(\u02dc                     \u00b51,t)\u00b5\u22a4   1,t\u00b5j,t     \u03c82\n                                               (Using sum of sub-Gaussian random variables property in Lemma B.1)\n                    \u2272     K     w1,t(Xt)wj,t(Xt)\u00b5\u22a4                                     w1,t(Xt)wj,t(Xt)\u00b5\u22a4                               \u00b51,t)\n                         j=1                                    1,t\u00b5j,tz     \u03c82 +                                     1,t\u00b5j,t(\u03c4 \u2212       \u02dc         \u03c8 2\n                    \u2272   KB2 + KB3 \u2272                KB3,                                                                                                         (E.4)\n                                                                                   37", "md": "E.2 Closeness between population gradient descent and empirical gradient descent\n\nIn this section, we show that the population gradient descent on the DDPM objective is close to the empirical gradient descent for mixtures of K Gaussians.\n\nLemma E.7. For any \u03b5 that is \u0398($$poly(d)$$) and noise scale t > t\u2032 where t\u2032 \u2272 1, the empirical estimate of gradient descent update on the DDPM objective with the number of samples n > n\u2032 concentrates well to the population gradient descent update where n\u2032 = O(K\u2074d\u2075B\u2076\u03b5\u00b2). More specifically, the following inequality holds with probability at least 1 \u2212 exp(\u2212d\u2070\u00b7\u2079):\n\n$$\n\\begin{align*}\n&\\nabla_{\\mu_{1,t}}\\frac{1}{n}\\sum_{i=1}^{n}L_t(s_{\\theta_t}(x_i,0, z_{i,t})) - \\nabla_{\\mu_{1,t}}L_t(s_{\\theta_t}) \\leq \\epsilon.\n\\end{align*}\n$$\nProof. Recall that the population gradient is given by\n\n$$\n\\begin{align*}\n&\\nabla_{\\mu_{1,t}}L_t(s_{\\theta_t}) = E\\left[\\frac{1}{2}\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^2 + \\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^\\top Z_t \\beta_t\\right],\n\\end{align*}\n$$\nwhere\n\n$$\n\\begin{align*}\n&E\\left[\\frac{1}{2}\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^2\\right] = E\\left[w_{1,t}(X_t)(X_t - \\frac{1}{K}\\mu_{1,t})\\mu_{1,t}^\\top + w_{1,t}(X_t) \\cdot Id\\right],\n\\end{align*}\n$$\nand\n\n$$\n\\begin{align*}\n&E\\left[\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^\\top Z_t\\right] = E\\left[w_{1,t}(X_t)(X_t - \\mu_{1,t})\\mu_{1,t}^\\top Z_t + w_{1,t}(X_t)Z_t - w_{1,t}(X_t)\\sum_{i=1}^{K}w_{i,t}(X_t)(X_t - \\mu_{1,t})\\mu_{i,t}^\\top Z_t\\right].\n\\end{align*}\n$$\nWe will prove that the sample estimate of each coordinate in Eq. (E.3) concentrates well around the expectation. We will prove the concentration of the first coordinate and a similar analysis holds for other coordinates. For the rest of the proof, we use $\\tilde{x}_t$ to denote the first coordinate of $X_t$ and $\\tilde{\\mu}_{i,t}$ to indicate the first coordinate $\\mu_{i,t}$. For any random variable $Y \\in \\mathbb{R}$, we use $\\|Y\\|_{\\psi_1}$ to denote the sub-exponential norm of $Y$ and $\\|Y\\|_{\\psi_2}$ to denote the sub-gaussian norm of $Y$ (See lemma B.1 for details). Using properties of a sub-Gaussian random variable from Lemma B.1, we get\n\n$$\n\\begin{align*}\n&\\sum_{j=1}^{K}w_{1,t}(X_t)w_{j,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top\\mu_{j,t} \\lesssim \\sum_{j=1}^{K}w_{1,t}(X_t)w_{j,t}(X_t)(\\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top\\mu_{j,t} \\lesssim KB^2 + KB^3 \\lesssim KB^3.\n\\end{align*}\n$$\n(E.4)", "images": [], "items": [{"type": "text", "value": "E.2 Closeness between population gradient descent and empirical gradient descent\n\nIn this section, we show that the population gradient descent on the DDPM objective is close to the empirical gradient descent for mixtures of K Gaussians.\n\nLemma E.7. For any \u03b5 that is \u0398($$poly(d)$$) and noise scale t > t\u2032 where t\u2032 \u2272 1, the empirical estimate of gradient descent update on the DDPM objective with the number of samples n > n\u2032 concentrates well to the population gradient descent update where n\u2032 = O(K\u2074d\u2075B\u2076\u03b5\u00b2). More specifically, the following inequality holds with probability at least 1 \u2212 exp(\u2212d\u2070\u00b7\u2079):\n\n$$\n\\begin{align*}\n&\\nabla_{\\mu_{1,t}}\\frac{1}{n}\\sum_{i=1}^{n}L_t(s_{\\theta_t}(x_i,0, z_{i,t})) - \\nabla_{\\mu_{1,t}}L_t(s_{\\theta_t}) \\leq \\epsilon.\n\\end{align*}\n$$\nProof. Recall that the population gradient is given by\n\n$$\n\\begin{align*}\n&\\nabla_{\\mu_{1,t}}L_t(s_{\\theta_t}) = E\\left[\\frac{1}{2}\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^2 + \\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^\\top Z_t \\beta_t\\right],\n\\end{align*}\n$$\nwhere\n\n$$\n\\begin{align*}\n&E\\left[\\frac{1}{2}\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^2\\right] = E\\left[w_{1,t}(X_t)(X_t - \\frac{1}{K}\\mu_{1,t})\\mu_{1,t}^\\top + w_{1,t}(X_t) \\cdot Id\\right],\n\\end{align*}\n$$\nand\n\n$$\n\\begin{align*}\n&E\\left[\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^\\top Z_t\\right] = E\\left[w_{1,t}(X_t)(X_t - \\mu_{1,t})\\mu_{1,t}^\\top Z_t + w_{1,t}(X_t)Z_t - w_{1,t}(X_t)\\sum_{i=1}^{K}w_{i,t}(X_t)(X_t - \\mu_{1,t})\\mu_{i,t}^\\top Z_t\\right].\n\\end{align*}\n$$\nWe will prove that the sample estimate of each coordinate in Eq. (E.3) concentrates well around the expectation. We will prove the concentration of the first coordinate and a similar analysis holds for other coordinates. For the rest of the proof, we use $\\tilde{x}_t$ to denote the first coordinate of $X_t$ and $\\tilde{\\mu}_{i,t}$ to indicate the first coordinate $\\mu_{i,t}$. For any random variable $Y \\in \\mathbb{R}$, we use $\\|Y\\|_{\\psi_1}$ to denote the sub-exponential norm of $Y$ and $\\|Y\\|_{\\psi_2}$ to denote the sub-gaussian norm of $Y$ (See lemma B.1 for details). Using properties of a sub-Gaussian random variable from Lemma B.1, we get\n\n$$\n\\begin{align*}\n&\\sum_{j=1}^{K}w_{1,t}(X_t)w_{j,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top\\mu_{j,t} \\lesssim \\sum_{j=1}^{K}w_{1,t}(X_t)w_{j,t}(X_t)(\\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top\\mu_{j,t} \\lesssim KB^2 + KB^3 \\lesssim KB^3.\n\\end{align*}\n$$\n(E.4)", "md": "E.2 Closeness between population gradient descent and empirical gradient descent\n\nIn this section, we show that the population gradient descent on the DDPM objective is close to the empirical gradient descent for mixtures of K Gaussians.\n\nLemma E.7. For any \u03b5 that is \u0398($$poly(d)$$) and noise scale t > t\u2032 where t\u2032 \u2272 1, the empirical estimate of gradient descent update on the DDPM objective with the number of samples n > n\u2032 concentrates well to the population gradient descent update where n\u2032 = O(K\u2074d\u2075B\u2076\u03b5\u00b2). More specifically, the following inequality holds with probability at least 1 \u2212 exp(\u2212d\u2070\u00b7\u2079):\n\n$$\n\\begin{align*}\n&\\nabla_{\\mu_{1,t}}\\frac{1}{n}\\sum_{i=1}^{n}L_t(s_{\\theta_t}(x_i,0, z_{i,t})) - \\nabla_{\\mu_{1,t}}L_t(s_{\\theta_t}) \\leq \\epsilon.\n\\end{align*}\n$$\nProof. Recall that the population gradient is given by\n\n$$\n\\begin{align*}\n&\\nabla_{\\mu_{1,t}}L_t(s_{\\theta_t}) = E\\left[\\frac{1}{2}\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^2 + \\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^\\top Z_t \\beta_t\\right],\n\\end{align*}\n$$\nwhere\n\n$$\n\\begin{align*}\n&E\\left[\\frac{1}{2}\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^2\\right] = E\\left[w_{1,t}(X_t)(X_t - \\frac{1}{K}\\mu_{1,t})\\mu_{1,t}^\\top + w_{1,t}(X_t) \\cdot Id\\right],\n\\end{align*}\n$$\nand\n\n$$\n\\begin{align*}\n&E\\left[\\nabla_{\\mu_{1,t}}s_{\\theta_t}(X_t)^\\top Z_t\\right] = E\\left[w_{1,t}(X_t)(X_t - \\mu_{1,t})\\mu_{1,t}^\\top Z_t + w_{1,t}(X_t)Z_t - w_{1,t}(X_t)\\sum_{i=1}^{K}w_{i,t}(X_t)(X_t - \\mu_{1,t})\\mu_{i,t}^\\top Z_t\\right].\n\\end{align*}\n$$\nWe will prove that the sample estimate of each coordinate in Eq. (E.3) concentrates well around the expectation. We will prove the concentration of the first coordinate and a similar analysis holds for other coordinates. For the rest of the proof, we use $\\tilde{x}_t$ to denote the first coordinate of $X_t$ and $\\tilde{\\mu}_{i,t}$ to indicate the first coordinate $\\mu_{i,t}$. For any random variable $Y \\in \\mathbb{R}$, we use $\\|Y\\|_{\\psi_1}$ to denote the sub-exponential norm of $Y$ and $\\|Y\\|_{\\psi_2}$ to denote the sub-gaussian norm of $Y$ (See lemma B.1 for details). Using properties of a sub-Gaussian random variable from Lemma B.1, we get\n\n$$\n\\begin{align*}\n&\\sum_{j=1}^{K}w_{1,t}(X_t)w_{j,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top\\mu_{j,t} \\lesssim \\sum_{j=1}^{K}w_{1,t}(X_t)w_{j,t}(X_t)(\\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top\\mu_{j,t} \\lesssim KB^2 + KB^3 \\lesssim KB^3.\n\\end{align*}\n$$\n(E.4)"}]}, {"page": 38, "text": " where the third inequality follows by writing \u02dc             xt = z + \u03c4 where z \u223c          N(0, 1) and \u03c4 is a random\n                                                                          1\n variable that takes \u02dc   \u00b5\u2217i,t for every i \u2208   [K] with probability       K . The fourth inequality follows from the\n sub-Gaussian property of a bounded random variable and the product of a sub-Gaussian random\n variable with bounded random variable property in Lemma B.1. Using the sum of sub-Gaussian\n random variable property in Lemma B.1, we have\n                         K   w1,t(Xt)wi,t(X)\u02dc    \u00b5i,t \u03c82  \u2272   K   \u2225w1,t(Xt)wi,t(X)\u02dc    \u00b5i,t\u2225\u03c82 \u2272   KB.                  (E.5)\n                        i=1                                  i=1\n Using properties of the sub-Gaussian random variable from Lemma B.1 in a similar way of Eq. (E.4),\n we have\n         K    K  w1,t(Xt)wi,t(Xt)wj,t(Xt)\u00b5\u22a4               xt \u2212  \u02dc\n        i=1  j=1                                  i,t\u00b5j,t(\u02dc     \u00b51,t)  \u03c82\n        K    K                                             xt \u2212  \u02dc\n    \u2264  i=1  j=1   w1,t(Xt)wi,t(Xt)wj,t(Xt)\u00b5\u22a4       i,t\u00b5j,t(\u02dc     \u00b51,t)  \u03c82\n    \u2264   K    K    w1,t(Xt)wi,t(Xt)wj,t(Xt)\u00b5\u22a4                        w1,t(Xt)wi,t(Xt)wj,t(Xt)\u00b5\u22a4                    \u00b5i,t)\n       i=1  j=1                                    i,t\u00b5j,tz  \u03c82 +                                    i,t\u00b5j,t(\u03c4 \u2212  \u02dc      \u03c8 2\n    \u2264  K2B2 + K2B3 \u2272          K2B3                                                                                      (E.6)\n We know that \u2225w1,t(Xt)\u00b5\u22a4                                                                   xt \u2212  \u02dc\n                                 1,tXt\u2225\u03c82 \u2264    \u2225 d  i=1 \u00b51,t(i)Xt(i)\u2225\u03c82 \u2272      dB2 and \u2225\u02dc        \u00b51,t\u2225\u03c82 \u2272    B. Using the\n fact that the product of two sub-Gaussian random variables is a sub-exponential random variable,\n we have\n                  \u2225w1,t(Xt)\u00b5\u22a4          xt \u2212   \u02dc\n                                              \u00b51,t)\u2225\u03c81 \u2264   \u2225\u02dcxt \u2212  \u02dc\n                                1,tXt(\u02dc                            \u00b51,t\u2225\u03c82\u2225w1,t(Xt)\u00b5\u22a4    1,tXt\u2225\u03c82 \u2272     dB3             (E.7)\n The sub-gaussian norm of w1,t(Xt)\u02dc          xt term in the gradient is given by\n                                 \u2225w1,t(Xt)\u02dc  xt\u2225\u03c82 \u2264    \u2225Xt\u2225\u03c82 \u2272     \u2225Z\u2225\u03c82 + \u2225\u03c4\u2225\u03c82 \u2272        B                           (E.8)\n Using the property that the product of two sub-Gaussian random variables is a sub-exponential\n random variable, we obtain\n                                            xt \u2212   \u02dc       K\n                                w1,t(Xt)(\u02dc        \u00b51,t)    i=1 wi,t(Xt)\u00b5\u22a4  i,tXt    \u03c81\n                                \u2272  \u2225w1,t(Xt)(\u02dc  xt \u2212  \u02dc              K   wi,t(Xt)\u00b5\u22a4\n                                \u2272  KdB3               \u00b51,t)\u2225\u03c82      i=1             i,tXt     \u03c82                        (E.9)\n For any random variable Y , we know that \u2225X\u2225\u03c81 \u2264                  \u2225X\u2225\u03c82. Therefore, combining Eq. (E.4), (E.5),\n(E.6), (E.7), (E.8) and (E.9), we have\n         \u2225[\u2207\u00b51,ts\u03b8t(Xt)\u22a4s\u03b8t(Xt)]1 \u2212         E[\u2207\u00b51,ts\u03b8t(Xt)\u22a4s\u03b8t(Xt)]1\u2225\u03c81 \u2272          \u2225[\u2207\u00b51,ts\u03b8t(Xt)\u22a4s\u03b8t(Xt)]1\u2225\u03c81\n                                                                                \u2272  K2dB3                               (E.10)\n                                                              38", "md": "where the third inequality follows by writing $$\\tilde{x}_t = z + \\tau$$ where $$z \\sim N(0, 1)$$ and $$\\tau$$ is a random variable that takes $$\\tilde{\\mu}_{i,t}$$ for every $$i \\in [K]$$ with probability $$\\frac{1}{K}$$. The fourth inequality follows from the sub-Gaussian property of a bounded random variable and the product of a sub-Gaussian random variable with bounded random variable property in Lemma B.1. Using the sum of sub-Gaussian random variable property in Lemma B.1, we have\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{K} w_{1,t}(X_t)w_{i,t}(X)\\tilde{\\mu}_{i,t} \\psi^2 \\lesssim \\sum_{i=1}^{K} \\|w_{1,t}(X_t)w_{i,t}(X)\\tilde{\\mu}_{i,t}\\| \\psi^2 \\lesssim KB. \\quad \\text{(E.5)}\n\\end{align*}\n$$\n\nUsing properties of the sub-Gaussian random variable from Lemma B.1 in a similar way of Eq. (E.4), we have\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{K} \\sum_{j=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} (x_t - \\tilde{\\mu}_{1,t}) \\psi^2 \\\\\n&\\leq \\sum_{i=1}^{K} \\sum_{j=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} \\psi^2 \\\\\n&\\leq K^2B^2 + K^2B^3 \\lesssim K^2B^3. \\quad \\text{(E.6)}\n\\end{align*}\n$$\n\nWe know that $$\\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} (x_t - \\tilde{\\mu}_{1,t})\\| \\psi^2 \\leq \\| \\sum_{i=1}^{d} \\mu_{1,t}(i)X_t(i)\\| \\psi^2 \\lesssim dB^2$$ and $$\\|\\tilde{\\mu}_{1,t}\\| \\psi^2 \\lesssim B$$. Using the fact that the product of two sub-Gaussian random variables is a sub-exponential random variable, we have\n\n$$\n\\begin{align*}\n&\\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} (x_t - \\tilde{\\mu}_{1,t})\\| \\psi^1 \\leq \\|x_t - \\tilde{\\mu}_{1,t}\\| \\psi^1 \\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} X_t\\| \\psi^2 \\lesssim dB^3. \\quad \\text{(E.7)}\n\\end{align*}\n$$\n\nThe sub-gaussian norm of $$w_{1,t}(X_t)\\tilde{x}_t$$ term in the gradient is given by\n\n$$\n\\begin{align*}\n&\\|w_{1,t}(X_t)\\tilde{x}_t\\| \\psi^2 \\leq \\|X_t\\| \\psi^2 \\lesssim \\|Z\\| \\psi^2 + \\|\\tau\\| \\psi^2 \\lesssim B. \\quad \\text{(E.8)}\n\\end{align*}\n$$\n\nUsing the property that the product of two sub-Gaussian random variables is a sub-exponential random variable, we obtain\n\n$$\n\\begin{align*}\n&w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t}) \\leq \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu^{\\top}_{i,t} X_t \\psi^1 \\\\\n&\\lesssim \\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\| \\lesssim KdB^3. \\quad \\text{(E.9)}\n\\end{align*}\n$$\n\nFor any random variable Y, we know that $$\\|X\\| \\psi^1 \\leq \\|X\\| \\psi^2$$. Therefore, combining Eq. (E.4), (E.5), (E.6), (E.7), (E.8) and (E.9), we have\n\n$$\n\\begin{align*}\n&\\|[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1 - E[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1\\| \\psi^1 \\lesssim \\|[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1\\| \\psi^1 \\\\\n&\\lesssim K^2dB^3. \\quad \\text{(E.10)}\n\\end{align*}\n$$\n\n38", "images": [], "items": [{"type": "text", "value": "where the third inequality follows by writing $$\\tilde{x}_t = z + \\tau$$ where $$z \\sim N(0, 1)$$ and $$\\tau$$ is a random variable that takes $$\\tilde{\\mu}_{i,t}$$ for every $$i \\in [K]$$ with probability $$\\frac{1}{K}$$. The fourth inequality follows from the sub-Gaussian property of a bounded random variable and the product of a sub-Gaussian random variable with bounded random variable property in Lemma B.1. Using the sum of sub-Gaussian random variable property in Lemma B.1, we have\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{K} w_{1,t}(X_t)w_{i,t}(X)\\tilde{\\mu}_{i,t} \\psi^2 \\lesssim \\sum_{i=1}^{K} \\|w_{1,t}(X_t)w_{i,t}(X)\\tilde{\\mu}_{i,t}\\| \\psi^2 \\lesssim KB. \\quad \\text{(E.5)}\n\\end{align*}\n$$\n\nUsing properties of the sub-Gaussian random variable from Lemma B.1 in a similar way of Eq. (E.4), we have\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{K} \\sum_{j=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} (x_t - \\tilde{\\mu}_{1,t}) \\psi^2 \\\\\n&\\leq \\sum_{i=1}^{K} \\sum_{j=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} \\psi^2 \\\\\n&\\leq K^2B^2 + K^2B^3 \\lesssim K^2B^3. \\quad \\text{(E.6)}\n\\end{align*}\n$$\n\nWe know that $$\\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} (x_t - \\tilde{\\mu}_{1,t})\\| \\psi^2 \\leq \\| \\sum_{i=1}^{d} \\mu_{1,t}(i)X_t(i)\\| \\psi^2 \\lesssim dB^2$$ and $$\\|\\tilde{\\mu}_{1,t}\\| \\psi^2 \\lesssim B$$. Using the fact that the product of two sub-Gaussian random variables is a sub-exponential random variable, we have\n\n$$\n\\begin{align*}\n&\\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} (x_t - \\tilde{\\mu}_{1,t})\\| \\psi^1 \\leq \\|x_t - \\tilde{\\mu}_{1,t}\\| \\psi^1 \\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} X_t\\| \\psi^2 \\lesssim dB^3. \\quad \\text{(E.7)}\n\\end{align*}\n$$\n\nThe sub-gaussian norm of $$w_{1,t}(X_t)\\tilde{x}_t$$ term in the gradient is given by\n\n$$\n\\begin{align*}\n&\\|w_{1,t}(X_t)\\tilde{x}_t\\| \\psi^2 \\leq \\|X_t\\| \\psi^2 \\lesssim \\|Z\\| \\psi^2 + \\|\\tau\\| \\psi^2 \\lesssim B. \\quad \\text{(E.8)}\n\\end{align*}\n$$\n\nUsing the property that the product of two sub-Gaussian random variables is a sub-exponential random variable, we obtain\n\n$$\n\\begin{align*}\n&w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t}) \\leq \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu^{\\top}_{i,t} X_t \\psi^1 \\\\\n&\\lesssim \\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\| \\lesssim KdB^3. \\quad \\text{(E.9)}\n\\end{align*}\n$$\n\nFor any random variable Y, we know that $$\\|X\\| \\psi^1 \\leq \\|X\\| \\psi^2$$. Therefore, combining Eq. (E.4), (E.5), (E.6), (E.7), (E.8) and (E.9), we have\n\n$$\n\\begin{align*}\n&\\|[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1 - E[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1\\| \\psi^1 \\lesssim \\|[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1\\| \\psi^1 \\\\\n&\\lesssim K^2dB^3. \\quad \\text{(E.10)}\n\\end{align*}\n$$\n\n38", "md": "where the third inequality follows by writing $$\\tilde{x}_t = z + \\tau$$ where $$z \\sim N(0, 1)$$ and $$\\tau$$ is a random variable that takes $$\\tilde{\\mu}_{i,t}$$ for every $$i \\in [K]$$ with probability $$\\frac{1}{K}$$. The fourth inequality follows from the sub-Gaussian property of a bounded random variable and the product of a sub-Gaussian random variable with bounded random variable property in Lemma B.1. Using the sum of sub-Gaussian random variable property in Lemma B.1, we have\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{K} w_{1,t}(X_t)w_{i,t}(X)\\tilde{\\mu}_{i,t} \\psi^2 \\lesssim \\sum_{i=1}^{K} \\|w_{1,t}(X_t)w_{i,t}(X)\\tilde{\\mu}_{i,t}\\| \\psi^2 \\lesssim KB. \\quad \\text{(E.5)}\n\\end{align*}\n$$\n\nUsing properties of the sub-Gaussian random variable from Lemma B.1 in a similar way of Eq. (E.4), we have\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{K} \\sum_{j=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} (x_t - \\tilde{\\mu}_{1,t}) \\psi^2 \\\\\n&\\leq \\sum_{i=1}^{K} \\sum_{j=1}^{K} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} w_{1,t}(X_t)w_{i,t}(X_t)w_{j,t}(X_t)\\mu^{\\top}_{i,t} \\psi^2 \\\\\n&\\leq K^2B^2 + K^2B^3 \\lesssim K^2B^3. \\quad \\text{(E.6)}\n\\end{align*}\n$$\n\nWe know that $$\\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} (x_t - \\tilde{\\mu}_{1,t})\\| \\psi^2 \\leq \\| \\sum_{i=1}^{d} \\mu_{1,t}(i)X_t(i)\\| \\psi^2 \\lesssim dB^2$$ and $$\\|\\tilde{\\mu}_{1,t}\\| \\psi^2 \\lesssim B$$. Using the fact that the product of two sub-Gaussian random variables is a sub-exponential random variable, we have\n\n$$\n\\begin{align*}\n&\\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} (x_t - \\tilde{\\mu}_{1,t})\\| \\psi^1 \\leq \\|x_t - \\tilde{\\mu}_{1,t}\\| \\psi^1 \\|w_{1,t}(X_t)\\mu^{\\top}_{1,t} X_t\\| \\psi^2 \\lesssim dB^3. \\quad \\text{(E.7)}\n\\end{align*}\n$$\n\nThe sub-gaussian norm of $$w_{1,t}(X_t)\\tilde{x}_t$$ term in the gradient is given by\n\n$$\n\\begin{align*}\n&\\|w_{1,t}(X_t)\\tilde{x}_t\\| \\psi^2 \\leq \\|X_t\\| \\psi^2 \\lesssim \\|Z\\| \\psi^2 + \\|\\tau\\| \\psi^2 \\lesssim B. \\quad \\text{(E.8)}\n\\end{align*}\n$$\n\nUsing the property that the product of two sub-Gaussian random variables is a sub-exponential random variable, we obtain\n\n$$\n\\begin{align*}\n&w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t}) \\leq \\sum_{i=1}^{K} w_{i,t}(X_t)\\mu^{\\top}_{i,t} X_t \\psi^1 \\\\\n&\\lesssim \\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\| \\lesssim KdB^3. \\quad \\text{(E.9)}\n\\end{align*}\n$$\n\nFor any random variable Y, we know that $$\\|X\\| \\psi^1 \\leq \\|X\\| \\psi^2$$. Therefore, combining Eq. (E.4), (E.5), (E.6), (E.7), (E.8) and (E.9), we have\n\n$$\n\\begin{align*}\n&\\|[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1 - E[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1\\| \\psi^1 \\lesssim \\|[\\nabla \\mu_{1,t}s\\theta_t(X_t)^{\\top}s\\theta_t(X_t)]_1\\| \\psi^1 \\\\\n&\\lesssim K^2dB^3. \\quad \\text{(E.10)}\n\\end{align*}\n$$\n\n38"}]}, {"page": 39, "text": "Now, we shift our focus on obtaining the sub-exponential norm of \u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt. Using \u2225w1,t(Xt)(\u02dc                                                                   xt\u2212\n \u02dc\n\u00b51,t)\u2225\u03c82 \u2272         B and \u2225\u00b5\u22a4       1,tZt\u2225\u03c82 \u2272         dB, we obtain\n                       \u2225w1,t(Xt)(\u02dc       xt \u2212    \u02dc\n                                                 \u00b51,t)\u00b5\u22a4                                       xt \u2212     \u02dc\n                                                           1,tZt\u2225\u03c81 \u2264        \u2225w1,t(Xt)(\u02dc               \u00b51,t)\u2225\u03c82\u2225\u00b5\u22a4       1,tZt\u2225\u03c82 \u2272        dB2                (E.11)\nUsing Lemma B.1, we have \u2225w1,t(Xt)zt\u2225\u03c82 \u2264                                     \u2225zt\u2225\u03c82 \u2272        1. For the last term, we have\n                                            K    wi,t(Xt)\u00b5\u22a4                                          xt \u2212     \u02dc                K    wi,t(Xt)\u00b5\u22a4\n                          xt \u2212     \u02dc                                                                         \u00b51,t)\u2225\u03c82\n          w1,t(Xt)(\u02dc               \u00b51,t)   i=1                   i,tZt     \u03c81 \u2264     \u2225w1,t(Xt)(\u02dc                               i=1                   i,tZt    \u03c82\n                                                                               \u2272    KdB2                                                                      (E.12)\nCombining Eq. (E.11), (E.12), we have\n                [\u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt]1              \u2212    E[\u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt]1                             [\u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt]1                                    , (E.13)\n                              \u03b2t                                    \u03b2t                   \u03c81 \u2272                      \u03b2t                  \u03c81 \u2272     KdB2\u03b2t\nwhere [\u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt]1 denotes the fi                              rst coordinate of \u2207\u00b51,ts\u03b8t(Xt)\u22a4Zt. Combining Eq. (E.10)\nand Eq. (E.13), we have\n                                                                                                               \u2272    K2dB3\n                                          [\u2207\u00b51,tLt(s\u03b8t(Xt))]1 \u2212                 [\u2207\u00b51,tLt(s\u03b8t)]1           \u03c81            \u03b2t\nFor each i.i.d. sample xi,t, the term [\u2207\u00b51,tLt(s\u03b8t(xi,t))]1 \u2212                                         [\u2207\u00b51,tLt(s\u03b8t)]1 is also independent and\n                                                                                                                                  1\nidentically distributed. Therefore, using Lemma B.3, for any \u03b5 that is \u0398(                                                     poly(d)   ), we have\n                    Pr       1    n   [\u2207\u00b51,tLt(s\u03b8t(xi,t))]1 \u2212              [\u2207\u00b51,tLt(s\u03b8t)]1            \u2265   \u03b5    \u2264   2 exp       \u2212     n\u03b52\u03b22    t     .\n                            n   i=1                                                                                                 K4d2B6\nA similar analysis will give the concentration for each coordinate.                                                     Using the union bound and\nrescaling \u03b5 as \u03b5                                                                             \u2212     n\u03b52\u03b22  t     , we have\n                         d, with probability at least 1 \u2212                      2d exp             K4d4B6\n                                              \u2207\u00b51,t       1    n    Lt(s\u03b8   t(xi,t))       \u2212   \u2207\u00b51,tLt(s\u03b8t)           \u2264    \u03b5\n                                                          n   i=1\nNote that for any t = \u2126(1), \u03b2t \u2265                             c for some constant c. Therefore, choosing n provided in the\nLemma E.7 statement, we obtain the result.\nE.3         Proof of Theorem E.1\nProof of Theorem E.1. For any training iteration h, assume that parameters \u03b8(h)                                                                  are such that\n \u00b5(h)           i,t   \u2264    cr \u221a  log d we can write the update on the DDPM objective as follows:                                           t\n    i,t \u2212    \u00b5\u2217   \u2225\u00b5(h+1)  4   \u2212   \u00b5\u22171,t\u2225   =    \u00b5(h)                 1    n    Lt(s\u03b8(h)                         \u2212   \u00b5\u22171,t\n                      1,t                            1,t \u2212    \u03b7\u2207      n   i=1           t (xi,0, zi,t))\n                                            \u2264    \u00b5(h)1,t + \u03b7 E[w1,t(Xt)(Xt \u2212                 \u00b5(h)             1,t\n                                                                                                1,t )] \u2212   \u00b5\u2217\n                                                        + \u03b7        \u2212\u2207\u00b51,tLt(s\u03b8t)             \u2212   E[w1,t(Xt)(Xt \u2212              \u00b5(h)\n                                                                                                                                1,t )]\n                                                        + \u03b7        \u2207\u00b51,tLt(s\u03b8t)           \u2212   \u2207\u00b51,t       1    n    Lt(s\u03b8(h)t (xi,0, zi,t))            .\n                                                                                                          n   i=1\n                                                                                   39", "md": "# Math Equations\n\nNow, we shift our focus on obtaining the sub-exponential norm of $$\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t$$. Using $$\\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\|_{\\psi_2} \\lesssim B$$ and $$\\|\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim dB$$, we obtain\n\n$$\\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top x_t - \\tilde{\\mu}_{1,t}Z_t\\|_{\\psi_1} \\leq \\|w_{1,t}(X_t)(\\tilde{\\mu}_{1,t})\\|_{\\psi_2}\\|\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim dB^2$$ (E.11)\n\nUsing Lemma B.1, we have $$\\|w_{1,t}(X_t)z_t\\|_{\\psi_2} \\leq \\|z_t\\|_{\\psi_2} \\lesssim 1$$. For the last term, we have\n\n$$\\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{1,t}^\\top x_t - \\tilde{\\mu}_{1,t}w_{i,t}(X_t)\\mu_{1,t}^\\top Z_t\\|_{\\psi_1} \\leq \\|w_{1,t}(X_t)(\\tilde{\\mu}_{1,t})\\|_{\\psi_2}\\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim KdB^2$$ (E.12)\n\nCombining Eq. (E.11), (E.12), we have\n\n$$[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1 - E[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1 \\leq [\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1$$, (E.13)\n\n$$\\beta_t \\lesssim \\beta_t \\lesssim KdB^2\\beta_t$$\n\nwhere $$[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1$$ denotes the first coordinate of $$\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t$$. Combining Eq. (E.10) and Eq. (E.13), we have\n\n$$\\leq K^2dB^3[\\nabla\\mu_{1,t}Lt(s\\theta_t(X_t))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1 \\leq \\beta_t$$\n\nFor each i.i.d. sample $$x_{i,t}$$, the term $$[\\nabla\\mu_{1,t}Lt(s\\theta_t(x_{i,t}))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1$$ is also independent and identically distributed. Therefore, using Lemma B.3, for any $$\\epsilon$$ that is $$\\Theta(poly(d))$$, we have\n\n$$\\text{Pr}\\left(\\frac{1}{n}\\sum_{i=1}^{n}[\\nabla\\mu_{1,t}Lt(s\\theta_t(x_{i,t}))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1 \\geq \\epsilon\\right) \\leq 2\\exp\\left(-\\frac{n\\epsilon^2\\beta^2_t}{K^4d^2B^6}\\right)$$\n\nA similar analysis will give the concentration for each coordinate. Using the union bound and rescaling $$\\epsilon$$ as $$\\epsilon \\rightarrow \\frac{\\epsilon}{d}$$, with probability at least $$1 - 2d\\exp\\left(-\\frac{K^4d^4B^6}{\\beta_t}\\right)$$\n\n$$\\|\\nabla\\mu_{1,t}\\|_{\\frac{1}{n}Lt(s\\theta_t(x_{i,t})) - \\nabla\\mu_{1,t}Lt(s\\theta_t)} \\leq \\epsilon$$\n\nNote that for any $$t = \\Omega(1)$$, $$\\beta_t \\geq c$$ for some constant $$c$$. Therefore, choosing $$n$$ provided in the Lemma E.7 statement, we obtain the result.\n\n### Proof of Theorem E.1\n\nProof of Theorem E.1. For any training iteration $$h$$, assume that parameters $$\\theta(h)$$ are such that $$\\mu(h)_{i,t} \\leq cr\\sqrt{\\log d}$$ we can write the update on the DDPM objective as follows:\n\n$$\\|\\mu(h+1)_{i,t} - \\mu^*\\| \\leq \\mu(h)_{1,t} + \\eta E[w_{1,t}(X_t)(X_t - \\mu(h)_{1,t})] - \\mu^*$$\n\n$$+ \\eta -\\nabla\\mu_{1,t}Lt(s\\theta_t) - E[w_{1,t}(X_t)(X_t - \\mu(h)_{1,t})]$$\n\n$$+ \\eta \\nabla\\mu_{1,t}Lt(s\\theta_t) - \\nabla\\mu_{1,t}\\frac{1}{n}\\sum_{i=1}^{n}Lt(s\\theta(h)_t(x_{i,0}, z_{i,t}))$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Now, we shift our focus on obtaining the sub-exponential norm of $$\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t$$. Using $$\\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\|_{\\psi_2} \\lesssim B$$ and $$\\|\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim dB$$, we obtain\n\n$$\\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top x_t - \\tilde{\\mu}_{1,t}Z_t\\|_{\\psi_1} \\leq \\|w_{1,t}(X_t)(\\tilde{\\mu}_{1,t})\\|_{\\psi_2}\\|\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim dB^2$$ (E.11)\n\nUsing Lemma B.1, we have $$\\|w_{1,t}(X_t)z_t\\|_{\\psi_2} \\leq \\|z_t\\|_{\\psi_2} \\lesssim 1$$. For the last term, we have\n\n$$\\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{1,t}^\\top x_t - \\tilde{\\mu}_{1,t}w_{i,t}(X_t)\\mu_{1,t}^\\top Z_t\\|_{\\psi_1} \\leq \\|w_{1,t}(X_t)(\\tilde{\\mu}_{1,t})\\|_{\\psi_2}\\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim KdB^2$$ (E.12)\n\nCombining Eq. (E.11), (E.12), we have\n\n$$[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1 - E[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1 \\leq [\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1$$, (E.13)\n\n$$\\beta_t \\lesssim \\beta_t \\lesssim KdB^2\\beta_t$$\n\nwhere $$[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1$$ denotes the first coordinate of $$\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t$$. Combining Eq. (E.10) and Eq. (E.13), we have\n\n$$\\leq K^2dB^3[\\nabla\\mu_{1,t}Lt(s\\theta_t(X_t))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1 \\leq \\beta_t$$\n\nFor each i.i.d. sample $$x_{i,t}$$, the term $$[\\nabla\\mu_{1,t}Lt(s\\theta_t(x_{i,t}))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1$$ is also independent and identically distributed. Therefore, using Lemma B.3, for any $$\\epsilon$$ that is $$\\Theta(poly(d))$$, we have\n\n$$\\text{Pr}\\left(\\frac{1}{n}\\sum_{i=1}^{n}[\\nabla\\mu_{1,t}Lt(s\\theta_t(x_{i,t}))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1 \\geq \\epsilon\\right) \\leq 2\\exp\\left(-\\frac{n\\epsilon^2\\beta^2_t}{K^4d^2B^6}\\right)$$\n\nA similar analysis will give the concentration for each coordinate. Using the union bound and rescaling $$\\epsilon$$ as $$\\epsilon \\rightarrow \\frac{\\epsilon}{d}$$, with probability at least $$1 - 2d\\exp\\left(-\\frac{K^4d^4B^6}{\\beta_t}\\right)$$\n\n$$\\|\\nabla\\mu_{1,t}\\|_{\\frac{1}{n}Lt(s\\theta_t(x_{i,t})) - \\nabla\\mu_{1,t}Lt(s\\theta_t)} \\leq \\epsilon$$\n\nNote that for any $$t = \\Omega(1)$$, $$\\beta_t \\geq c$$ for some constant $$c$$. Therefore, choosing $$n$$ provided in the Lemma E.7 statement, we obtain the result.", "md": "Now, we shift our focus on obtaining the sub-exponential norm of $$\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t$$. Using $$\\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\|_{\\psi_2} \\lesssim B$$ and $$\\|\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim dB$$, we obtain\n\n$$\\|w_{1,t}(X_t)(\\tilde{x}_t - \\tilde{\\mu}_{1,t})\\mu_{1,t}^\\top x_t - \\tilde{\\mu}_{1,t}Z_t\\|_{\\psi_1} \\leq \\|w_{1,t}(X_t)(\\tilde{\\mu}_{1,t})\\|_{\\psi_2}\\|\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim dB^2$$ (E.11)\n\nUsing Lemma B.1, we have $$\\|w_{1,t}(X_t)z_t\\|_{\\psi_2} \\leq \\|z_t\\|_{\\psi_2} \\lesssim 1$$. For the last term, we have\n\n$$\\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{1,t}^\\top x_t - \\tilde{\\mu}_{1,t}w_{i,t}(X_t)\\mu_{1,t}^\\top Z_t\\|_{\\psi_1} \\leq \\|w_{1,t}(X_t)(\\tilde{\\mu}_{1,t})\\|_{\\psi_2}\\sum_{i=1}^{K} w_{i,t}(X_t)\\mu_{1,t}^\\top Z_t\\|_{\\psi_2} \\lesssim KdB^2$$ (E.12)\n\nCombining Eq. (E.11), (E.12), we have\n\n$$[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1 - E[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1 \\leq [\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1$$, (E.13)\n\n$$\\beta_t \\lesssim \\beta_t \\lesssim KdB^2\\beta_t$$\n\nwhere $$[\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t]_1$$ denotes the first coordinate of $$\\nabla\\mu_{1,t}s\\theta_t(X_t)^\\top Z_t$$. Combining Eq. (E.10) and Eq. (E.13), we have\n\n$$\\leq K^2dB^3[\\nabla\\mu_{1,t}Lt(s\\theta_t(X_t))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1 \\leq \\beta_t$$\n\nFor each i.i.d. sample $$x_{i,t}$$, the term $$[\\nabla\\mu_{1,t}Lt(s\\theta_t(x_{i,t}))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1$$ is also independent and identically distributed. Therefore, using Lemma B.3, for any $$\\epsilon$$ that is $$\\Theta(poly(d))$$, we have\n\n$$\\text{Pr}\\left(\\frac{1}{n}\\sum_{i=1}^{n}[\\nabla\\mu_{1,t}Lt(s\\theta_t(x_{i,t}))]_1 - [\\nabla\\mu_{1,t}Lt(s\\theta_t)]_1 \\geq \\epsilon\\right) \\leq 2\\exp\\left(-\\frac{n\\epsilon^2\\beta^2_t}{K^4d^2B^6}\\right)$$\n\nA similar analysis will give the concentration for each coordinate. Using the union bound and rescaling $$\\epsilon$$ as $$\\epsilon \\rightarrow \\frac{\\epsilon}{d}$$, with probability at least $$1 - 2d\\exp\\left(-\\frac{K^4d^4B^6}{\\beta_t}\\right)$$\n\n$$\\|\\nabla\\mu_{1,t}\\|_{\\frac{1}{n}Lt(s\\theta_t(x_{i,t})) - \\nabla\\mu_{1,t}Lt(s\\theta_t)} \\leq \\epsilon$$\n\nNote that for any $$t = \\Omega(1)$$, $$\\beta_t \\geq c$$ for some constant $$c$$. Therefore, choosing $$n$$ provided in the Lemma E.7 statement, we obtain the result."}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem E.1", "md": "### Proof of Theorem E.1"}, {"type": "text", "value": "Proof of Theorem E.1. For any training iteration $$h$$, assume that parameters $$\\theta(h)$$ are such that $$\\mu(h)_{i,t} \\leq cr\\sqrt{\\log d}$$ we can write the update on the DDPM objective as follows:\n\n$$\\|\\mu(h+1)_{i,t} - \\mu^*\\| \\leq \\mu(h)_{1,t} + \\eta E[w_{1,t}(X_t)(X_t - \\mu(h)_{1,t})] - \\mu^*$$\n\n$$+ \\eta -\\nabla\\mu_{1,t}Lt(s\\theta_t) - E[w_{1,t}(X_t)(X_t - \\mu(h)_{1,t})]$$\n\n$$+ \\eta \\nabla\\mu_{1,t}Lt(s\\theta_t) - \\nabla\\mu_{1,t}\\frac{1}{n}\\sum_{i=1}^{n}Lt(s\\theta(h)_t(x_{i,0}, z_{i,t}))$$", "md": "Proof of Theorem E.1. For any training iteration $$h$$, assume that parameters $$\\theta(h)$$ are such that $$\\mu(h)_{i,t} \\leq cr\\sqrt{\\log d}$$ we can write the update on the DDPM objective as follows:\n\n$$\\|\\mu(h+1)_{i,t} - \\mu^*\\| \\leq \\mu(h)_{1,t} + \\eta E[w_{1,t}(X_t)(X_t - \\mu(h)_{1,t})] - \\mu^*$$\n\n$$+ \\eta -\\nabla\\mu_{1,t}Lt(s\\theta_t) - E[w_{1,t}(X_t)(X_t - \\mu(h)_{1,t})]$$\n\n$$+ \\eta \\nabla\\mu_{1,t}Lt(s\\theta_t) - \\nabla\\mu_{1,t}\\frac{1}{n}\\sum_{i=1}^{n}Lt(s\\theta(h)_t(x_{i,0}, z_{i,t}))$$"}]}, {"page": 40, "text": "Using Lemma E.4, Lemma E.7 and Theorem 3.2 from [SN21], for any \u03b7 \u2208                                                  (0, K), we have\n                                \u2225\u00b5(h+1)     \u2212   \u00b5\u2217             1 \u2212    3\u03b7       \u2225\u00b5(h)         1,t\u2225  + \u03b7K2B2          + \u03b7\u03b5.\n                                    1,t           1,t\u2225  \u2264                         1,t \u2212    \u00b5\u2217               c2\n                                                                      8K                                     r\n                                                                                                         d 4000\nChoosing \u03b7 = 2K         3 , cr to be suffi        ciently large constant and \u03b5 to be \u0398(                         1\n                                                                                                            poly(d)), we have\n                                                  \u2225\u00b5(h+1)     \u2212  \u00b5\u2217 1,t\u2225  \u2264   3     1,t \u2212    \u00b5\u22171,t\u2225  + \u03b5\n                                                     1,t                      4\u2225\u00b5(h)\nBy assumption 15, \u2225\u00b5(0)             1,t \u2212    \u00b5\u22171,t\u2225   \u2264   O(\u221a    log d) and therefore, choosing H to be \u2126(log(log d                           \u03b5 )), we\nobtain the result.\nF        Additional proofs\nF.1        Proof of Lemma C.2\nProof of Lemma C.2. By calculating the negative gradient of the DDPM objective in Eq. (5), we\nobtain\n         \u2212\u2207\u00b5tLt(s\u00b5t) = \u2212EX0,Zt[(tanh(\u00b5\u22a4                    t Xt)I + tanh\u2032(\u00b5\u22a4         t Xt)Xt\u00b5\u22a4     t )(s\u00b5t(Xt) + Zt      \u03b2t  )]\n                             = \u2212E[(tanh(\u00b5\u22a4          t Xt)I + tanh\u2032(\u00b5\u22a4         t Xt)Xt\u00b5\u22a4     t )(tanh(\u00b5\u22a4     t Xt)\u00b5t \u2212      Xt + Zt   \u03b2t )]\n                             = E[\u2212      tanh2(\u00b5\u22a4    t Xt)\u00b5t \u2212       tanh(\u00b5\u22a4    t Xt) tanh\u2032(\u00b5\u22a4      t Xt)Xt\u2225\u00b5t\u22252 + tanh(\u00b5\u22a4              t Xt)Xt\n                                 + tanh\u2032(\u00b5\u22a4     t Xt)\u00b5\u22a4   t XtXt \u2212       tanh(\u00b5\u22a4    t Xt)Zt      \u2212   tanh\u2032(\u00b5\u22a4    t Xt)Xt\u00b5\u22a4     t  Zt  ]\n                                                                                             \u03b2t                                   \u03b2t\nBy simplifying the gradient terms involving Zt by the Stein\u2019s identity as in Lemma F.1 and plugging\nit back in the gradient, we obtain\n      \u2212\u2207\u00b5tLt(s\u00b5t) = E                 tanh(\u00b5\u22a4   t Xt) \u2212      tanh(\u00b5\u22a4   t Xt) tanh\u2032(\u00b5\u22a4       t Xt)\u2225\u00b5t\u22252 + tanh\u2032(\u00b5\u22a4           t Xt)\u00b5\u22a4   t Xt      Xt\n                              \u2212   \u00b5t \u2212    E   tanh\u2032\u2032(\u00b5\u22a4    t Xt)\u2225\u00b5t\u22252 Xt           \u2212  E    tanh\u2032(\u00b5\u22a4    t Xt)\u00b5t\n                           = E        tanh(\u00b5\u22a4   t Xt) \u2212      0.5 tanh\u2032\u2032(\u00b5\u22a4    t Xt)\u2225\u00b5t\u22252 + tanh\u2032(\u00b5\u22a4            t Xt)\u00b5\u22a4   t Xt     Xt\n                              \u2212   \u00b5t \u2212    E   tanh\u2032(\u00b5\u22a4    t Xt)\u00b5t\nObserve that            tanh(\u00b5\u22a4x) \u2212          1                                                           x and tanh\u2032(\u00b5\u22a4x) are even func-\n                                             2 tanh\u2032\u2032(\u00b5\u22a4x)\u2225\u00b5\u22252 + tanh\u2032(\u00b5\u22a4x)\u00b5\u22a4x\ntions and Xt is a symmetric distribution, therefore, for any even function f, we can write EX                                                t[f(Xt)] =\n 1\n 2EXt\u223cN (\u00b5\u2217     t ,Id)[f(Xt)] + 1     2EXt\u223cN (\u2212\u00b5\u2217      t ,I)[f(Xt)] = EX         t\u223cN (\u00b5\u2217 t ,Id)[f(Xt)]. Applying this property of the\neven function on the gradient update, we obtain the result.\nLemma F.1. When random variable Xt = \u03b1tX0 + \u03b2tZt where Zt \u223c                                                      N  (0, I), \u03b1t = exp(\u2212t) and\n\u03b2t =     1 \u2212      exp(\u22122t), then for any t > 0, the following two equations hold.\n              E  X0,Zt     tanh(\u00b5\u22a4   t Xt)Zt  \u03b2t   + tanh2(\u00b5\u22a4      t Xt)\u00b5t       = \u00b5t\n              E  X0,Zt     tanh\u2032(\u00b5\u22a4   t Xt)\u00b5\u22a4    t Zt  Xt     = EX0,Zt        tanh\u2032\u2032(\u00b5\u22a4    t Xt)\u2225\u00b5t\u22252 Xt + tanh\u2032(\u00b5\u22a4             t Xt)\u00b5t\n                                                 \u03b2t\n                                                                            40", "md": "# Math Equations\n\nUsing Lemma E.4, Lemma E.7 and Theorem 3.2 from [SN21], for any \u03b7 \u2208 (0, K), we have\n\n$$\\left\\| \\mu(h+1) - \\mu^* \\right\\|_{1,t} \\leq \\frac{1}{8K} \\left( \\left\\| \\mu(h) - \\mu^* \\right\\|_{1,t} + \\eta K^2B^2 \\right) + \\eta \\epsilon.$$\nChoosing $$\\eta = \\frac{2K}{3}$$, $$c$$ to be sufficiently large constant and $$\\epsilon$$ to be $$\\Theta(\\text{poly}(d))$$, we have\n\n$$\\left\\| \\mu(h+1) - \\mu^* \\right\\|_{1,t} \\leq \\frac{3}{4} \\left\\| \\mu(h) - \\mu^* \\right\\|_{1,t} + \\epsilon.$$\nBy assumption 15, $$\\left\\| \\mu(0) - \\mu^* \\right\\|_{1,t} \\leq O(\\sqrt{\\log d})$$ and therefore, choosing $$H$$ to be $$\\Omega(\\log(\\log d) + \\frac{\\epsilon}{4000})$$, we obtain the result.\n\nAdditional proofs\n\nProof of Lemma C.2\n\nProof of Lemma C.2. By calculating the negative gradient of the DDPM objective in Eq. (5), we obtain\n\n$$\\begin{align*}\n-\\nabla_{\\mu_t} L_t(s, \\mu_t) &= -\\mathbb{E}_{X_0,Z_t} \\left[ \\left( \\tanh(\\mu_t^T X_t)I + \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T \\right) \\left( s\\mu_t(X_t) + Z_t\\beta_t \\right) \\right] \\\\\n&= -\\mathbb{E} \\left[ \\left( \\tanh(\\mu_t^T X_t)I + \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T \\right) \\left( \\tanh(\\mu_t^T X_t)\\mu_t - X_t + Z_t\\beta_t \\right) \\right] \\\\\n&= \\mathbb{E} \\left[ -\\tanh^2(\\mu_t^T X_t)\\mu_t - \\tanh(\\mu_t^T X_t)\\tanh'(\\mu_t^T X_t)X_t\\|\\mu_t\\|^2 + \\tanh(\\mu_t^T X_t)X_t + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_tX_t - \\tanh(\\mu_t^T X_t)Z_t - \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T Z_t \\right]\n\\end{align*}$$\nBy simplifying the gradient terms involving $$Z_t$$ by the Stein\u2019s identity as in Lemma F.1 and plugging it back in the gradient, we obtain\n\n$$\\begin{align*}\n-\\nabla_{\\mu_t} L_t(s, \\mu_t) &= \\mathbb{E} \\left[ \\tanh(\\mu_t^T X_t) - \\tanh(\\mu_t^T X_t)\\tanh'(\\mu_t^T X_t)\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_t X_t - \\mu_t - \\mathbb{E} \\tanh'(\\mu_t^T X_t)\\mu_t \\right] \\\\\n&= \\mathbb{E} \\left[ \\tanh(\\mu_t^T X_t) - 0.5\\tanh''(\\mu_t^T X_t)\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_t X_t - \\mu_t - \\mathbb{E} \\tanh'(\\mu_t^T X_t)\\mu_t \\right]\n\\end{align*}$$\nObserve that $$\\tanh(\\mu^Tx) - \\frac{1}{2}\\tanh''(\\mu^Tx)\\|\\mu\\|^2 + \\tanh'(\\mu^Tx)\\mu^Tx$$ and $$\\tanh'(\\mu^Tx)$$ are even functions and $$X_t$$ is a symmetric distribution, therefore, for any even function $$f$$, we can write $$\\mathbb{E}_{X_t}[f(X_t)] = \\frac{1}{2}\\mathbb{E}_{X_t \\sim N(\\mu_t^*, I_d)}[f(X_t)] + \\frac{1}{2}\\mathbb{E}_{X_t \\sim N(-\\mu_t^*, I)}[f(X_t)] = \\mathbb{E}_{X_t \\sim N(\\mu_t^*, I_d)}[f(X_t)]$$. Applying this property of the even function on the gradient update, we obtain the result.\n\nLemma F.1. When random variable $$X_t = \\alpha_t X_0 + \\beta_t Z_t$$ where $$Z_t \\sim N(0, I)$$, $$\\alpha_t = \\exp(-t)$$ and $$\\beta_t = 1 - \\exp(-2t)$$, then for any $$t > 0$$, the following two equations hold.\n\n$$\\begin{align*}\n\\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu_t^T X_t)Z_t\\beta_t + \\tanh^2(\\mu_t^T X_t)\\mu_t] &= \\mu_t \\\\\n\\mathbb{E}_{X_0,Z_t}[\\tanh'(\\mu_t^T X_t)\\mu_t^T Z_t X_t] &= \\mathbb{E}_{X_0,Z_t}[\\tanh''(\\mu_t^T X_t)\\|\\mu_t\\|^2 X_t + \\tanh'(\\mu_t^T X_t)\\mu_t\\beta_t]\n\\end{align*}$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Using Lemma E.4, Lemma E.7 and Theorem 3.2 from [SN21], for any \u03b7 \u2208 (0, K), we have\n\n$$\\left\\| \\mu(h+1) - \\mu^* \\right\\|_{1,t} \\leq \\frac{1}{8K} \\left( \\left\\| \\mu(h) - \\mu^* \\right\\|_{1,t} + \\eta K^2B^2 \\right) + \\eta \\epsilon.$$\nChoosing $$\\eta = \\frac{2K}{3}$$, $$c$$ to be sufficiently large constant and $$\\epsilon$$ to be $$\\Theta(\\text{poly}(d))$$, we have\n\n$$\\left\\| \\mu(h+1) - \\mu^* \\right\\|_{1,t} \\leq \\frac{3}{4} \\left\\| \\mu(h) - \\mu^* \\right\\|_{1,t} + \\epsilon.$$\nBy assumption 15, $$\\left\\| \\mu(0) - \\mu^* \\right\\|_{1,t} \\leq O(\\sqrt{\\log d})$$ and therefore, choosing $$H$$ to be $$\\Omega(\\log(\\log d) + \\frac{\\epsilon}{4000})$$, we obtain the result.\n\nAdditional proofs\n\nProof of Lemma C.2\n\nProof of Lemma C.2. By calculating the negative gradient of the DDPM objective in Eq. (5), we obtain\n\n$$\\begin{align*}\n-\\nabla_{\\mu_t} L_t(s, \\mu_t) &= -\\mathbb{E}_{X_0,Z_t} \\left[ \\left( \\tanh(\\mu_t^T X_t)I + \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T \\right) \\left( s\\mu_t(X_t) + Z_t\\beta_t \\right) \\right] \\\\\n&= -\\mathbb{E} \\left[ \\left( \\tanh(\\mu_t^T X_t)I + \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T \\right) \\left( \\tanh(\\mu_t^T X_t)\\mu_t - X_t + Z_t\\beta_t \\right) \\right] \\\\\n&= \\mathbb{E} \\left[ -\\tanh^2(\\mu_t^T X_t)\\mu_t - \\tanh(\\mu_t^T X_t)\\tanh'(\\mu_t^T X_t)X_t\\|\\mu_t\\|^2 + \\tanh(\\mu_t^T X_t)X_t + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_tX_t - \\tanh(\\mu_t^T X_t)Z_t - \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T Z_t \\right]\n\\end{align*}$$\nBy simplifying the gradient terms involving $$Z_t$$ by the Stein\u2019s identity as in Lemma F.1 and plugging it back in the gradient, we obtain\n\n$$\\begin{align*}\n-\\nabla_{\\mu_t} L_t(s, \\mu_t) &= \\mathbb{E} \\left[ \\tanh(\\mu_t^T X_t) - \\tanh(\\mu_t^T X_t)\\tanh'(\\mu_t^T X_t)\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_t X_t - \\mu_t - \\mathbb{E} \\tanh'(\\mu_t^T X_t)\\mu_t \\right] \\\\\n&= \\mathbb{E} \\left[ \\tanh(\\mu_t^T X_t) - 0.5\\tanh''(\\mu_t^T X_t)\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_t X_t - \\mu_t - \\mathbb{E} \\tanh'(\\mu_t^T X_t)\\mu_t \\right]\n\\end{align*}$$\nObserve that $$\\tanh(\\mu^Tx) - \\frac{1}{2}\\tanh''(\\mu^Tx)\\|\\mu\\|^2 + \\tanh'(\\mu^Tx)\\mu^Tx$$ and $$\\tanh'(\\mu^Tx)$$ are even functions and $$X_t$$ is a symmetric distribution, therefore, for any even function $$f$$, we can write $$\\mathbb{E}_{X_t}[f(X_t)] = \\frac{1}{2}\\mathbb{E}_{X_t \\sim N(\\mu_t^*, I_d)}[f(X_t)] + \\frac{1}{2}\\mathbb{E}_{X_t \\sim N(-\\mu_t^*, I)}[f(X_t)] = \\mathbb{E}_{X_t \\sim N(\\mu_t^*, I_d)}[f(X_t)]$$. Applying this property of the even function on the gradient update, we obtain the result.\n\nLemma F.1. When random variable $$X_t = \\alpha_t X_0 + \\beta_t Z_t$$ where $$Z_t \\sim N(0, I)$$, $$\\alpha_t = \\exp(-t)$$ and $$\\beta_t = 1 - \\exp(-2t)$$, then for any $$t > 0$$, the following two equations hold.\n\n$$\\begin{align*}\n\\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu_t^T X_t)Z_t\\beta_t + \\tanh^2(\\mu_t^T X_t)\\mu_t] &= \\mu_t \\\\\n\\mathbb{E}_{X_0,Z_t}[\\tanh'(\\mu_t^T X_t)\\mu_t^T Z_t X_t] &= \\mathbb{E}_{X_0,Z_t}[\\tanh''(\\mu_t^T X_t)\\|\\mu_t\\|^2 X_t + \\tanh'(\\mu_t^T X_t)\\mu_t\\beta_t]\n\\end{align*}$$", "md": "Using Lemma E.4, Lemma E.7 and Theorem 3.2 from [SN21], for any \u03b7 \u2208 (0, K), we have\n\n$$\\left\\| \\mu(h+1) - \\mu^* \\right\\|_{1,t} \\leq \\frac{1}{8K} \\left( \\left\\| \\mu(h) - \\mu^* \\right\\|_{1,t} + \\eta K^2B^2 \\right) + \\eta \\epsilon.$$\nChoosing $$\\eta = \\frac{2K}{3}$$, $$c$$ to be sufficiently large constant and $$\\epsilon$$ to be $$\\Theta(\\text{poly}(d))$$, we have\n\n$$\\left\\| \\mu(h+1) - \\mu^* \\right\\|_{1,t} \\leq \\frac{3}{4} \\left\\| \\mu(h) - \\mu^* \\right\\|_{1,t} + \\epsilon.$$\nBy assumption 15, $$\\left\\| \\mu(0) - \\mu^* \\right\\|_{1,t} \\leq O(\\sqrt{\\log d})$$ and therefore, choosing $$H$$ to be $$\\Omega(\\log(\\log d) + \\frac{\\epsilon}{4000})$$, we obtain the result.\n\nAdditional proofs\n\nProof of Lemma C.2\n\nProof of Lemma C.2. By calculating the negative gradient of the DDPM objective in Eq. (5), we obtain\n\n$$\\begin{align*}\n-\\nabla_{\\mu_t} L_t(s, \\mu_t) &= -\\mathbb{E}_{X_0,Z_t} \\left[ \\left( \\tanh(\\mu_t^T X_t)I + \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T \\right) \\left( s\\mu_t(X_t) + Z_t\\beta_t \\right) \\right] \\\\\n&= -\\mathbb{E} \\left[ \\left( \\tanh(\\mu_t^T X_t)I + \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T \\right) \\left( \\tanh(\\mu_t^T X_t)\\mu_t - X_t + Z_t\\beta_t \\right) \\right] \\\\\n&= \\mathbb{E} \\left[ -\\tanh^2(\\mu_t^T X_t)\\mu_t - \\tanh(\\mu_t^T X_t)\\tanh'(\\mu_t^T X_t)X_t\\|\\mu_t\\|^2 + \\tanh(\\mu_t^T X_t)X_t + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_tX_t - \\tanh(\\mu_t^T X_t)Z_t - \\tanh'(\\mu_t^T X_t)X_t\\mu_t^T Z_t \\right]\n\\end{align*}$$\nBy simplifying the gradient terms involving $$Z_t$$ by the Stein\u2019s identity as in Lemma F.1 and plugging it back in the gradient, we obtain\n\n$$\\begin{align*}\n-\\nabla_{\\mu_t} L_t(s, \\mu_t) &= \\mathbb{E} \\left[ \\tanh(\\mu_t^T X_t) - \\tanh(\\mu_t^T X_t)\\tanh'(\\mu_t^T X_t)\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_t X_t - \\mu_t - \\mathbb{E} \\tanh'(\\mu_t^T X_t)\\mu_t \\right] \\\\\n&= \\mathbb{E} \\left[ \\tanh(\\mu_t^T X_t) - 0.5\\tanh''(\\mu_t^T X_t)\\|\\mu_t\\|^2 + \\tanh'(\\mu_t^T X_t)\\mu_t^T X_t X_t - \\mu_t - \\mathbb{E} \\tanh'(\\mu_t^T X_t)\\mu_t \\right]\n\\end{align*}$$\nObserve that $$\\tanh(\\mu^Tx) - \\frac{1}{2}\\tanh''(\\mu^Tx)\\|\\mu\\|^2 + \\tanh'(\\mu^Tx)\\mu^Tx$$ and $$\\tanh'(\\mu^Tx)$$ are even functions and $$X_t$$ is a symmetric distribution, therefore, for any even function $$f$$, we can write $$\\mathbb{E}_{X_t}[f(X_t)] = \\frac{1}{2}\\mathbb{E}_{X_t \\sim N(\\mu_t^*, I_d)}[f(X_t)] + \\frac{1}{2}\\mathbb{E}_{X_t \\sim N(-\\mu_t^*, I)}[f(X_t)] = \\mathbb{E}_{X_t \\sim N(\\mu_t^*, I_d)}[f(X_t)]$$. Applying this property of the even function on the gradient update, we obtain the result.\n\nLemma F.1. When random variable $$X_t = \\alpha_t X_0 + \\beta_t Z_t$$ where $$Z_t \\sim N(0, I)$$, $$\\alpha_t = \\exp(-t)$$ and $$\\beta_t = 1 - \\exp(-2t)$$, then for any $$t > 0$$, the following two equations hold.\n\n$$\\begin{align*}\n\\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu_t^T X_t)Z_t\\beta_t + \\tanh^2(\\mu_t^T X_t)\\mu_t] &= \\mu_t \\\\\n\\mathbb{E}_{X_0,Z_t}[\\tanh'(\\mu_t^T X_t)\\mu_t^T Z_t X_t] &= \\mathbb{E}_{X_0,Z_t}[\\tanh''(\\mu_t^T X_t)\\|\\mu_t\\|^2 X_t + \\tanh'(\\mu_t^T X_t)\\mu_t\\beta_t]\n\\end{align*}$$"}]}, {"page": 41, "text": "Proof. Applying Stein\u2019s lemma on the fi                         rst term, we get the fi            rst equation of the statement in the\nLemma.\n        E X0,Zt     tanh(\u00b5\u22a4    t Xt)Zt  \u03b2t    = EX     0,Zt    tanh(\u00b5\u22a4    t (\u03b1tX0 + \u03b2tZt))Zt        \u03b2t     = EX0,Zt        tanh\u2032(\u00b5\u22a4    t Xt)\u00b5t\nFor the second term, we have                  = EX     0,Zt      1 \u2212   tanh2(\u00b5\u22a4     t Xt)     \u00b5t\n      E    tanh\u2032(\u00b5\u22a4    t Xt)\u00b5\u22a4   t Zt   Xt     = E     tanh\u2032(\u00b5\u22a4    t Xt)\u00b5\u22a4   t Zt   \u03b1tX0      + E      tanh\u2032(\u00b5\u22a4   t Xt)\u00b5\u22a4   t ZtZt\n                                  \u03b2t                                          \u03b2t\n          =    d   E    \u03b1tX0 tanh\u2032(\u00b5\u22a4       t Xt)\u00b5t(i)Zt(i)           + E      tanh\u2032(\u00b5\u22a4   t Xt)\u00b5t       + E     tanh\u2032\u2032(\u00b5\u22a4    t Xt)\u00b5\u22a4   t Zt\u03b2t\u00b5t\n              i=1                                         \u03b2t\n               d\n          =   i=1  E    \u03b1tX0 tanh\u2032\u2032(\u00b5\u22a4       t Xt)\u00b5t(i)\u00b5t(i)          + E     tanh\u2032(\u00b5\u22a4    t Xt)\u00b5t       + E     tanh\u2032\u2032(\u00b5\u22a4    t Xt)\u00b5\u22a4   t Zt\u03b2t\u00b5t\nwhere the second equality follows from the Stein\u2019s lemma on the E[tanh\u2032(\u00b5\u22a4                                               t Xt)\u00b5\u22a4   t ZtZt] and the\nlast equality follows from the Stein\u2019s lemma on E[\u03b1tX0 tanh\u2032\u2032(\u00b5\u22a4                                      t Xt)\u00b5t(i)Zt(i)]. Applying Stein\u2019s\ninequality on the E              tanh\u2032\u2032(\u00b5\u22a4   t Xt)\u00b5\u22a4   t Zt\u03b2t\u00b5t       , we obtain             d\n      = E      \u03b1tX0 tanh\u2032\u2032(\u00b5\u22a4       t Xt)\u2225\u00b5t\u22252         + E     tanh\u2032(\u00b5\u22a4    t Xt)\u00b5t       +   i=1  \u03b2t\u00b5tE      tanh\u2032\u2032\u2032(\u00b5\u22a4    t Xt)\u00b5t(i)\u03b2t\u00b5t(i)\n      = E      Xt tanh\u2032\u2032(\u00b5\u22a4     t Xt)\u2225\u00b5t\u22252         \u2212   E   \u03b2tZt tanh\u2032\u2032(\u00b5\u22a4      t Xt)\u2225\u00b5t\u22252         + E      tanh\u2032(\u00b5\u22a4   t Xt)\u00b5t\n          + \u03b22 t \u2225\u00b5t\u22252 \u00b5tE        tanh\u2032\u2032\u2032(\u00b5\u22a4   t Xt)\n      = E      Xt tanh\u2032\u2032(\u00b5\u22a4     t Xt)\u2225\u00b5t\u22252         + E     tanh\u2032(\u00b5\u22a4    t Xt)\u00b5t       .\nF.2        Proof of Lemma C.8\nProof of Lemma C.8. Recall that the gradient update for any \u00b5\u2217                                       t is given by\n                              \u2212\u2207\u00b5\u2217   t Lt(s\u00b5\u2217  t ) = G(\u00b5\u2217    t , \u00b5\u2217t ) + \u03b7Ex\u223cN (\u00b5\u2217      t ,Id)[tanh(\u00b5\u2217\u22a4   t x)x] \u2212      \u03b7\u00b5\u2217 t                      (F.1)\nWe know that E           x\u223cN (\u00b5\u2217  t ,Id)[tanh(\u00b5\u2217\u22a4   t x)x] = \u00b5\u2217       t (Eq.(2.1) of [DTZ17]) and \u2207\u00b5\u2217                 t Lt(s\u00b5\u2217  t ) = 0 because \u00b5\u2217        t\nis a stationary point of the regression objective of diffusion model. This implies that G(\u00b5\u2217                                                t , \u00b5\u2217t) = 0\nfor any \u00b5\u2217     t.\n      Note that this proof only talks about 1D case therefore, for the purpose of this proof, we use a\nto denote \u00b5 and b to denote \u00b5\u2217. In 1D, using Mean value theorem, we have\n                                 G(a, b) \u2212      G(a, a)      = dG(a, \u03be)        for some \u03be \u2208         [a, b] (if a < b)                              (F.2)\n                                          b \u2212   a                     d\u03be\nUsing the fact that G(a, a) = 0 in Eq. (F.2), we have\n                                                         G(a, b)      =   dG(a, \u03be)       |b \u2212   a|\n                                                                            41 d\u03be", "md": "# Math Equations\n\n## Proof:\n\nApplying Stein\u2019s lemma on the first term, we get the first equation of the statement in the Lemma.\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu^{\\top}_t X_t)Z_t \\beta_t] = \\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu^{\\top}_t (\\alpha_t X_0 + \\beta_t Z_t))Z_t \\beta_t] = \\mathbb{E}_{X_0,Z_t}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] \\\\\n&\\text{For the second term, we have} \\\\\n&= \\mathbb{E}_{X_0,Z_t}[1 - \\tanh^2(\\mu^{\\top}_t X_t) \\mu_t] \\\\\n&= \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t X_t] = \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\alpha_t X_0] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t Z_t \\beta_t] \\\\\n&= \\sum_{i=1}^{d} \\mathbb{E}[\\alpha_t X_0 \\tanh'(\\mu^{\\top}_t X_t)\\mu_t(i)Z_t(i)] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\beta_t \\mu_t] \\\\\n&\\text{where the second equality follows from the Stein\u2019s lemma on the } \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t Z_t] \\text{ and the last equality follows from the Stein\u2019s lemma on } \\mathbb{E}[\\alpha_t X_0 \\tanh''(\\mu^{\\top}_t X_t)\\mu_t(i)Z_t(i)]. \\\\\n&\\text{Applying Stein\u2019s inequality on the } \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\beta_t \\mu_t], \\text{ we obtain} \\\\\n&= \\mathbb{E}[\\alpha_t X_0 \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\sum_{i=1}^{d} \\beta_t \\mu_t \\mathbb{E}[\\tanh'''(\\mu^{\\top}_t X_t)\\mu_t(i)\\beta_t \\mu_t(i)] \\\\\n&= \\mathbb{E}[X_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] - \\mathbb{E}[\\beta_t Z_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\beta^2_t \\|\\mu_t\\|^2 \\mu_t \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)] \\\\\n&= \\mathbb{E}[X_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t].\n\\end{align*}\n$$\n\n## Proof of Lemma C.8:\n\nProof of Lemma C.8. Recall that the gradient update for any $\\mu^*_t$ is given by\n\n$$\n-\\nabla_{\\mu^*_t} L_t(s,\\mu^*_t) = G(\\mu^*_t, \\mu^*_t) + \\eta\\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^{*\\top}_t x)x] - \\eta\\mu^*_t \\quad \\text{(F.1)}\n$$\n\nWe know that $\\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^{*\\top}_t x)x] = \\mu^*_t$ (Eq.(2.1) of [DTZ17]) and $\\nabla_{\\mu^*_t} L_t(s,\\mu^*_t) = 0$ because $\\mu^*_t$ is a stationary point of the regression objective of the diffusion model. This implies that $G(\\mu^*_t, \\mu^*_t) = 0$ for any $\\mu^*_t$.\n\nNote that this proof only talks about 1D case therefore, for the purpose of this proof, we use $a$ to denote $\\mu$ and $b$ to denote $\\mu^*$. In 1D, using Mean value theorem, we have\n\n$$\nG(a, b) - G(a, a) = \\frac{dG(a, \\xi)}{d\\xi} \\quad \\text{for some } \\xi \\in [a, b] \\text{ (if } a < b) \\quad \\text{(F.2)}\n$$\n\nUsing the fact that $G(a, a) = 0$ in Eq. (F.2), we have\n\n$$\nG(a, b) = \\frac{dG(a, \\xi)}{d\\xi} |b - a|\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "heading", "lvl": 2, "value": "Proof:", "md": "## Proof:"}, {"type": "text", "value": "Applying Stein\u2019s lemma on the first term, we get the first equation of the statement in the Lemma.\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu^{\\top}_t X_t)Z_t \\beta_t] = \\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu^{\\top}_t (\\alpha_t X_0 + \\beta_t Z_t))Z_t \\beta_t] = \\mathbb{E}_{X_0,Z_t}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] \\\\\n&\\text{For the second term, we have} \\\\\n&= \\mathbb{E}_{X_0,Z_t}[1 - \\tanh^2(\\mu^{\\top}_t X_t) \\mu_t] \\\\\n&= \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t X_t] = \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\alpha_t X_0] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t Z_t \\beta_t] \\\\\n&= \\sum_{i=1}^{d} \\mathbb{E}[\\alpha_t X_0 \\tanh'(\\mu^{\\top}_t X_t)\\mu_t(i)Z_t(i)] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\beta_t \\mu_t] \\\\\n&\\text{where the second equality follows from the Stein\u2019s lemma on the } \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t Z_t] \\text{ and the last equality follows from the Stein\u2019s lemma on } \\mathbb{E}[\\alpha_t X_0 \\tanh''(\\mu^{\\top}_t X_t)\\mu_t(i)Z_t(i)]. \\\\\n&\\text{Applying Stein\u2019s inequality on the } \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\beta_t \\mu_t], \\text{ we obtain} \\\\\n&= \\mathbb{E}[\\alpha_t X_0 \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\sum_{i=1}^{d} \\beta_t \\mu_t \\mathbb{E}[\\tanh'''(\\mu^{\\top}_t X_t)\\mu_t(i)\\beta_t \\mu_t(i)] \\\\\n&= \\mathbb{E}[X_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] - \\mathbb{E}[\\beta_t Z_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\beta^2_t \\|\\mu_t\\|^2 \\mu_t \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)] \\\\\n&= \\mathbb{E}[X_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t].\n\\end{align*}\n$$", "md": "Applying Stein\u2019s lemma on the first term, we get the first equation of the statement in the Lemma.\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu^{\\top}_t X_t)Z_t \\beta_t] = \\mathbb{E}_{X_0,Z_t}[\\tanh(\\mu^{\\top}_t (\\alpha_t X_0 + \\beta_t Z_t))Z_t \\beta_t] = \\mathbb{E}_{X_0,Z_t}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] \\\\\n&\\text{For the second term, we have} \\\\\n&= \\mathbb{E}_{X_0,Z_t}[1 - \\tanh^2(\\mu^{\\top}_t X_t) \\mu_t] \\\\\n&= \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t X_t] = \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\alpha_t X_0] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t Z_t \\beta_t] \\\\\n&= \\sum_{i=1}^{d} \\mathbb{E}[\\alpha_t X_0 \\tanh'(\\mu^{\\top}_t X_t)\\mu_t(i)Z_t(i)] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\beta_t \\mu_t] \\\\\n&\\text{where the second equality follows from the Stein\u2019s lemma on the } \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t Z_t] \\text{ and the last equality follows from the Stein\u2019s lemma on } \\mathbb{E}[\\alpha_t X_0 \\tanh''(\\mu^{\\top}_t X_t)\\mu_t(i)Z_t(i)]. \\\\\n&\\text{Applying Stein\u2019s inequality on the } \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)\\mu^{\\top}_t Z_t \\beta_t \\mu_t], \\text{ we obtain} \\\\\n&= \\mathbb{E}[\\alpha_t X_0 \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\sum_{i=1}^{d} \\beta_t \\mu_t \\mathbb{E}[\\tanh'''(\\mu^{\\top}_t X_t)\\mu_t(i)\\beta_t \\mu_t(i)] \\\\\n&= \\mathbb{E}[X_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] - \\mathbb{E}[\\beta_t Z_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t] + \\beta^2_t \\|\\mu_t\\|^2 \\mu_t \\mathbb{E}[\\tanh''(\\mu^{\\top}_t X_t)] \\\\\n&= \\mathbb{E}[X_t \\tanh''(\\mu^{\\top}_t X_t)\\|\\mu_t\\|^2] + \\mathbb{E}[\\tanh'(\\mu^{\\top}_t X_t)\\mu_t].\n\\end{align*}\n$$"}, {"type": "heading", "lvl": 2, "value": "Proof of Lemma C.8:", "md": "## Proof of Lemma C.8:"}, {"type": "text", "value": "Proof of Lemma C.8. Recall that the gradient update for any $\\mu^*_t$ is given by\n\n$$\n-\\nabla_{\\mu^*_t} L_t(s,\\mu^*_t) = G(\\mu^*_t, \\mu^*_t) + \\eta\\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^{*\\top}_t x)x] - \\eta\\mu^*_t \\quad \\text{(F.1)}\n$$\n\nWe know that $\\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^{*\\top}_t x)x] = \\mu^*_t$ (Eq.(2.1) of [DTZ17]) and $\\nabla_{\\mu^*_t} L_t(s,\\mu^*_t) = 0$ because $\\mu^*_t$ is a stationary point of the regression objective of the diffusion model. This implies that $G(\\mu^*_t, \\mu^*_t) = 0$ for any $\\mu^*_t$.\n\nNote that this proof only talks about 1D case therefore, for the purpose of this proof, we use $a$ to denote $\\mu$ and $b$ to denote $\\mu^*$. In 1D, using Mean value theorem, we have\n\n$$\nG(a, b) - G(a, a) = \\frac{dG(a, \\xi)}{d\\xi} \\quad \\text{for some } \\xi \\in [a, b] \\text{ (if } a < b) \\quad \\text{(F.2)}\n$$\n\nUsing the fact that $G(a, a) = 0$ in Eq. (F.2), we have\n\n$$\nG(a, b) = \\frac{dG(a, \\xi)}{d\\xi} |b - a|\n$$", "md": "Proof of Lemma C.8. Recall that the gradient update for any $\\mu^*_t$ is given by\n\n$$\n-\\nabla_{\\mu^*_t} L_t(s,\\mu^*_t) = G(\\mu^*_t, \\mu^*_t) + \\eta\\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^{*\\top}_t x)x] - \\eta\\mu^*_t \\quad \\text{(F.1)}\n$$\n\nWe know that $\\mathbb{E}_{x \\sim \\mathcal{N}(\\mu^*_t,Id)}[\\tanh(\\mu^{*\\top}_t x)x] = \\mu^*_t$ (Eq.(2.1) of [DTZ17]) and $\\nabla_{\\mu^*_t} L_t(s,\\mu^*_t) = 0$ because $\\mu^*_t$ is a stationary point of the regression objective of the diffusion model. This implies that $G(\\mu^*_t, \\mu^*_t) = 0$ for any $\\mu^*_t$.\n\nNote that this proof only talks about 1D case therefore, for the purpose of this proof, we use $a$ to denote $\\mu$ and $b$ to denote $\\mu^*$. In 1D, using Mean value theorem, we have\n\n$$\nG(a, b) - G(a, a) = \\frac{dG(a, \\xi)}{d\\xi} \\quad \\text{for some } \\xi \\in [a, b] \\text{ (if } a < b) \\quad \\text{(F.2)}\n$$\n\nUsing the fact that $G(a, a) = 0$ in Eq. (F.2), we have\n\n$$\nG(a, b) = \\frac{dG(a, \\xi)}{d\\xi} |b - a|\n$$"}]}, {"page": 42, "text": "Observe that it suffi      ces to prove     dG(a,\u03be)    \u2264  0.01 to obtain the lemma. By computing the gradient\n                                               d\u03be\nof G, we obtain\n        dG(a, \u03be)    = \u03b7E   x\u223cN (\u03be,1)   2 tanh\u2032(ax)ax + tanh\u2032\u2032(ax)           \u22123a2    + a2x2      \u2212  1\n            d\u03be                                                                 2                   2a3x tanh\u2032\u2032\u2032(ax)\nFor the fi  rst term, we have\n                 E x\u223cN (\u03be,I)[tanh\u2032(ax)ax] =        \u221a 1       \u221e  tanh\u2032(ax)axe\u2212(x\u2212\u03be)2   2   dx\n                                                     2\u03c0    \u2212\u221e\u221e\n                                                     1                                  2             2\n                                               =   \u221a 2\u03c0    0    tanh\u2032(ax)ax       e\u2212(x\u2212\u03be)2   \u2212  e\u2212(x+\u03be)2     dx\n                                               \u2264   \u221a 1       \u221e  e\u2212axaxe\u2212(x\u2212\u03be)2 2   dx\n                                                     2\u03c0    0\n                                                      a2 \u22122a\u03be    \u221e\n                                                         2\n                                                     \u221a                        2\n                                               \u2264   ae   2\u03c0      0   xe\u2212(x\u2212\u03be+a)2    dx\n                                                      a2\u22122a\u03be      2                             \u03be \u2212    a\n                                                         2                2\n                                               \u2264   ae        (    \u03c0e\u2212(\u03be\u2212a)2    + (\u03be \u2212   a)erf      \u221a 2     )\n                                                                          \u22122a(\u03be\u2212a)\u2212a2\n                                               \u2264   ae\u2212\u03be22 + a|\u03be \u2212     a| e      2\nUsing Lemma 1 of [DTZ17], we know that E                   x\u223cN (\u03be,I)[tanh\u2032(ax)ax] > 0. Therefore, we have\n                                                                                       \u22122a(\u03be\u2212a)\u2212a2\n                             Ex\u223cN (\u03be,I)[tanh\u2032(ax)ax]         \u2264  ae\u2212\u03be2 2 + a|\u03be \u2212    a| e      2\n     For the second term, we have\n               E x\u223cN (\u03be,1)[tanh\u2032\u2032(ax)(\u22123a2    2   + a2x2)]\n               =   \u221a 1       \u221e  a2 tanh\u2032\u2032(ax)(\u22123    2 + x2)     exp(\u2212(x \u2212      \u03be)2) \u2212   exp(\u2212(x + \u03be)2)          dx\n                     2\u03c0    0                                                 2                       2\n               \u2264   \u221a 1        23 a2 e\u22122ax(3  2 \u2212  x2) exp(\u2212(x \u2212     2 \u03be)2 )dx\n                     2\u03c0    0\n               \u2264   \u221a 3   a2 exp(\u2212a2  16)\n                     2\u03c0\nAssuming a \u2265        \u221a  6, then when \u03be \u2265         a \u2265   \u221a 6, we have exp(\u2212(x\u2212\u03be)2) \u2264  2          exp(\u2212a2  4 ) and when \u03be \u2264  a,\n                                                               42", "md": "Observe that it suffices to prove $$dG(a,\\xi) \\leq 0.01$$ to obtain the lemma. By computing the gradient\n\n$$\\frac{dG(a, \\xi)}{d\\xi} = \\eta \\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,1)} 2 \\tanh'(ax)ax + \\tanh''(ax) -3a^2 + a^2x^2 - 1$$\n\nFor the first term, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\tanh'(ax)axe^{-(x-\\xi)^2/2} dx \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\tanh'(ax)ax e^{-(x-\\xi)^2} - e^{-(x+\\xi)^2} dx \\\\\n&\\leq \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} e^{-ax}axe^{-(x-\\xi)^2} dx \\\\\n&\\leq ae^{\\frac{-(\\xi-a)^2}{2}} + a|\\xi - a| e^2\n\\end{align*}\n$$\nUsing Lemma 1 of [DTZ17], we know that $$\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] > 0$$. Therefore, we have\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] &\\leq ae^{\\frac{-(\\xi)^2}{2}} + a|\\xi - a| e^2\n\\end{align*}\n$$\nFor the second term, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,1)}[\\tanh''(ax)(-3a^2 + a^2x^2)] \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} a^2 \\tanh''(ax)(-3 + x^2) e^{-(x - \\xi)^2} - e^{-(x + \\xi)^2} dx \\\\\n&\\leq \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} 3a^2 e^{-2ax}(3 - x^2) e^{-(x - 2\\xi)^2} dx \\\\\n&\\leq \\frac{\\sqrt{3} a^2}{2\\pi} e^{-a^2/16}\n\\end{align*}\n$$\nAssuming $$a \\geq \\sqrt{6}$$, then when $$\\xi \\geq a \\geq \\sqrt{6}$$, we have $$e^{-(x-\\xi)^2} \\leq 2 e^{-a^2/4}$$ and when $$\\xi \\leq a$$,\n\n$$\n\\frac{4}{2}\n$$", "images": [], "items": [{"type": "text", "value": "Observe that it suffices to prove $$dG(a,\\xi) \\leq 0.01$$ to obtain the lemma. By computing the gradient\n\n$$\\frac{dG(a, \\xi)}{d\\xi} = \\eta \\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,1)} 2 \\tanh'(ax)ax + \\tanh''(ax) -3a^2 + a^2x^2 - 1$$\n\nFor the first term, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\tanh'(ax)axe^{-(x-\\xi)^2/2} dx \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\tanh'(ax)ax e^{-(x-\\xi)^2} - e^{-(x+\\xi)^2} dx \\\\\n&\\leq \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} e^{-ax}axe^{-(x-\\xi)^2} dx \\\\\n&\\leq ae^{\\frac{-(\\xi-a)^2}{2}} + a|\\xi - a| e^2\n\\end{align*}\n$$\nUsing Lemma 1 of [DTZ17], we know that $$\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] > 0$$. Therefore, we have\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] &\\leq ae^{\\frac{-(\\xi)^2}{2}} + a|\\xi - a| e^2\n\\end{align*}\n$$\nFor the second term, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,1)}[\\tanh''(ax)(-3a^2 + a^2x^2)] \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} a^2 \\tanh''(ax)(-3 + x^2) e^{-(x - \\xi)^2} - e^{-(x + \\xi)^2} dx \\\\\n&\\leq \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} 3a^2 e^{-2ax}(3 - x^2) e^{-(x - 2\\xi)^2} dx \\\\\n&\\leq \\frac{\\sqrt{3} a^2}{2\\pi} e^{-a^2/16}\n\\end{align*}\n$$\nAssuming $$a \\geq \\sqrt{6}$$, then when $$\\xi \\geq a \\geq \\sqrt{6}$$, we have $$e^{-(x-\\xi)^2} \\leq 2 e^{-a^2/4}$$ and when $$\\xi \\leq a$$,\n\n$$\n\\frac{4}{2}\n$$", "md": "Observe that it suffices to prove $$dG(a,\\xi) \\leq 0.01$$ to obtain the lemma. By computing the gradient\n\n$$\\frac{dG(a, \\xi)}{d\\xi} = \\eta \\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,1)} 2 \\tanh'(ax)ax + \\tanh''(ax) -3a^2 + a^2x^2 - 1$$\n\nFor the first term, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{\\infty} \\tanh'(ax)axe^{-(x-\\xi)^2/2} dx \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\tanh'(ax)ax e^{-(x-\\xi)^2} - e^{-(x+\\xi)^2} dx \\\\\n&\\leq \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} e^{-ax}axe^{-(x-\\xi)^2} dx \\\\\n&\\leq ae^{\\frac{-(\\xi-a)^2}{2}} + a|\\xi - a| e^2\n\\end{align*}\n$$\nUsing Lemma 1 of [DTZ17], we know that $$\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] > 0$$. Therefore, we have\n\n$$\n\\begin{align*}\n\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,I)}[\\tanh'(ax)ax] &\\leq ae^{\\frac{-(\\xi)^2}{2}} + a|\\xi - a| e^2\n\\end{align*}\n$$\nFor the second term, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E}_{x \\sim \\mathcal{N}(\\xi,1)}[\\tanh''(ax)(-3a^2 + a^2x^2)] \\\\\n&= \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} a^2 \\tanh''(ax)(-3 + x^2) e^{-(x - \\xi)^2} - e^{-(x + \\xi)^2} dx \\\\\n&\\leq \\frac{1}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} 3a^2 e^{-2ax}(3 - x^2) e^{-(x - 2\\xi)^2} dx \\\\\n&\\leq \\frac{\\sqrt{3} a^2}{2\\pi} e^{-a^2/16}\n\\end{align*}\n$$\nAssuming $$a \\geq \\sqrt{6}$$, then when $$\\xi \\geq a \\geq \\sqrt{6}$$, we have $$e^{-(x-\\xi)^2} \\leq 2 e^{-a^2/4}$$ and when $$\\xi \\leq a$$,\n\n$$\n\\frac{4}{2}\n$$"}]}, {"page": 43, "text": "using \u03be \u2265        3a                                    ) \u2264   exp(\u2212a2\n                 4 , we have exp(\u2212(x\u2212\u03be)2         2                     16). For the lower bound, we have\n                E x\u223cN (\u03be,1)[tanh\u2032\u2032(ax)(\u22123a2          2    + a2x2)]\n                 =   \u221a  1        \u221e  tanh\u2032\u2032(ax)(\u22123a2       2    + a2x2)         exp(\u2212(x \u2212      2  \u03be)2  ) \u2212  exp(\u2212(x + \u03be)2   2       )   dx\n                        2\u03c0     0\n                 \u2265   \u221a  1       3\u221e   tanh\u2032\u2032(ax)(\u22123a2       2   + a2x2)         exp(\u2212(x \u2212      2  \u03be)2  ) \u2212   exp(\u2212(x + \u03be)2  2       )    dx\n                        2\u03c0        2\n                 \u2265   \u221a  1       3\u221e   tanh\u2032\u2032(ax)a2x2            exp(\u2212(x \u2212      2  \u03be)2  ) \u2212   exp(\u2212(x + \u03be)2  2       )    dx\n                        2\u03c0        2\n                 \u2265   \u2212  \u221a8a2       \u221e   e\u22122axx2         exp(\u2212(x \u2212         \u03be)2  ) \u2212   exp(\u2212(x + \u03be)2          )    dx\n                           2\u03c0      3 2                                2                            2\n                 \u2265   \u22128a2e\u2212 \u221a  2\u03c0\u221a 6a      \u221e 3 x2 exp(\u2212(x \u2212        2 \u03be)2  )dx \u2265     \u22128a2e\u2212      \u221a  6a\n                                             2\nUsing upper bound and lower bound, we have\nFor the third term, we have               Ex\u223cN (\u03be,1)[tanh\u2032\u2032(ax)a2(\u22123             2 + x2)]       \u2264   8a2e\u2212    \u221a 6a\n          Ex\u223cN (\u03be,1)[a3x    2 tanh\u2032\u2032\u2032(ax)]\n          =     32 \u221a1 2\u03c0     0 \u221e   a3x\u03c3(2ax)(1 \u2212          \u03c3(2ax))        1 \u2212   6\u03c3(2ax)(1 \u2212         \u03c3(2ax))           exp       \u2212   (x \u2212 2 \u03be)2\n                 \u2212   exp      \u2212   (x + \u03be)2           dx\n                  3a3          \u221e        2\n          \u2264        \u221a               x\u03c32(2ax)(1 \u2212          \u03c3(2ax))2         exp      \u2212   (x \u2212    \u03be)2      \u2212  exp       \u2212   (x + \u03be)2          dx\n                16    2\u03c0     0\u221e                                                             2                                 2\n                 3a3\n          \u2264    16 \u221a  2\u03c0     0    xe\u2212ax exp           \u2212   (x \u2212 2 \u03be)2     dx\n                                               \u22122a(\u03be\u2212a)\u2212a2\n          \u2264    a3      2 + a3                          2        .\n               10e\u2212\u03be2          10|\u03be \u2212     a| e\nWe can lower bound the third term as follows:\n                    E x\u223cN (\u03be,1)[a3x   2 tanh\u2032\u2032\u2032(ax)]\n                     \u2265     \u221a 1        c a3x tanh\u2032\u2032\u2032(ax)           exp       \u2212   (x + \u03be)2        \u2212   exp      \u2212   (x \u2212    \u03be)2        dx\n                         2    2\u03c0     0                                               2                                 2\n                     \u2265   2 \u221aa32\u03c0     0c x exp        \u2212   (x \u2212 2  \u03be)2       exp (\u22122\u03bex) \u2212         1   dx\n                            \u221a          c                                                    \u221a       4 )\n                     \u2265   \u2212   a3\u03be         x2 exp        \u2212   (x \u2212    \u03be)2     dx \u2265     \u2212\u03be exp(\u2212\u03be2\n                               2\u03c0     0                          2                             2\u03c0\n                                                                            43", "md": "Using $$\\xi \\geq 3a$$, we have $$\\exp(-a^2/4)$$, we have $$\\exp(-(x-\\xi)^2/2) \\leq \\exp(-a^2/16)$$. For the lower bound, we have\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[tanh''(ax)(-3a^2/2 + a^2x^2)] \\\\\n&= \\int_{-\\infty}^{\\infty} tanh''(ax)(-3a^2/2 + a^2x^2) \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq \\int_{-\\infty}^{\\infty} tanh''(ax)(-3a^2/2 + a^2x^2) \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq \\int_{-\\infty}^{\\infty} tanh''(ax)a^2x^2 \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq -\\sqrt{8a^2} \\int_{-\\infty}^{\\infty} e^{-2ax}x^2 \\exp(-(x - \\xi)^2) - \\exp(-(x + \\xi)^2) dx \\\\\n&\\geq -8a^2e^{-\\sqrt{2\\pi}\\sqrt{6a}} \\int_{\\frac{3}{2}}^{\\infty} 3x^2 \\exp(-(x - \\frac{1}{2}\\xi)^2)dx \\geq -8a^2e^{-\\sqrt{6a}}\n\\end{align*}\n$$\nUsing upper bound and lower bound, we have\n\nFor the third term, we have\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[tanh''(ax)a^2(-3 + x^2)] \\leq 8a^2e^{-\\sqrt{6a}} \\\\\n&E_{x \\sim N(\\xi,1)}[a^3x^2 tanh'''(ax)] \\\\\n&= \\frac{32}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} a^3x\\sigma(2ax)(1 - \\sigma(2ax))(1 - 6\\sigma(2ax)(1 - \\sigma(2ax))) \\exp(-(x - 2\\xi)^2 - \\exp(-(x + \\xi)^2) dx \\\\\n&\\leq \\int_{0}^{\\infty} xe^{-ax} \\exp(-(x - 2\\xi)^2) dx \\\\\n&\\leq a^3(2 + a^3/2) e^{-\\xi^2} (10|\\xi - a|e)\n\\end{align*}\n$$\nWe can lower bound the third term as follows:\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[a^3x^2 tanh'''(ax)] \\\\\n&\\geq \\sqrt{1/2} \\int_{0}^{c} a^3x tanh'''(ax) \\exp(-(x + \\xi)^2 - \\exp(-(x - \\xi)^2) dx \\\\\n&\\geq 2\\sqrt{a^3/2\\pi} \\int_{0}^{c} x \\exp(-(x - 2\\xi)^2) \\exp(-2\\xi x) - 1 dx \\\\\n&\\geq -a^3\\xi/2\\pi \\int_{0}^{\\infty} x^2 \\exp(-(x - \\xi)^2) dx \\geq -\\xi \\exp(-\\xi^2/4/3)\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Using $$\\xi \\geq 3a$$, we have $$\\exp(-a^2/4)$$, we have $$\\exp(-(x-\\xi)^2/2) \\leq \\exp(-a^2/16)$$. For the lower bound, we have\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[tanh''(ax)(-3a^2/2 + a^2x^2)] \\\\\n&= \\int_{-\\infty}^{\\infty} tanh''(ax)(-3a^2/2 + a^2x^2) \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq \\int_{-\\infty}^{\\infty} tanh''(ax)(-3a^2/2 + a^2x^2) \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq \\int_{-\\infty}^{\\infty} tanh''(ax)a^2x^2 \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq -\\sqrt{8a^2} \\int_{-\\infty}^{\\infty} e^{-2ax}x^2 \\exp(-(x - \\xi)^2) - \\exp(-(x + \\xi)^2) dx \\\\\n&\\geq -8a^2e^{-\\sqrt{2\\pi}\\sqrt{6a}} \\int_{\\frac{3}{2}}^{\\infty} 3x^2 \\exp(-(x - \\frac{1}{2}\\xi)^2)dx \\geq -8a^2e^{-\\sqrt{6a}}\n\\end{align*}\n$$\nUsing upper bound and lower bound, we have\n\nFor the third term, we have\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[tanh''(ax)a^2(-3 + x^2)] \\leq 8a^2e^{-\\sqrt{6a}} \\\\\n&E_{x \\sim N(\\xi,1)}[a^3x^2 tanh'''(ax)] \\\\\n&= \\frac{32}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} a^3x\\sigma(2ax)(1 - \\sigma(2ax))(1 - 6\\sigma(2ax)(1 - \\sigma(2ax))) \\exp(-(x - 2\\xi)^2 - \\exp(-(x + \\xi)^2) dx \\\\\n&\\leq \\int_{0}^{\\infty} xe^{-ax} \\exp(-(x - 2\\xi)^2) dx \\\\\n&\\leq a^3(2 + a^3/2) e^{-\\xi^2} (10|\\xi - a|e)\n\\end{align*}\n$$\nWe can lower bound the third term as follows:\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[a^3x^2 tanh'''(ax)] \\\\\n&\\geq \\sqrt{1/2} \\int_{0}^{c} a^3x tanh'''(ax) \\exp(-(x + \\xi)^2 - \\exp(-(x - \\xi)^2) dx \\\\\n&\\geq 2\\sqrt{a^3/2\\pi} \\int_{0}^{c} x \\exp(-(x - 2\\xi)^2) \\exp(-2\\xi x) - 1 dx \\\\\n&\\geq -a^3\\xi/2\\pi \\int_{0}^{\\infty} x^2 \\exp(-(x - \\xi)^2) dx \\geq -\\xi \\exp(-\\xi^2/4/3)\n\\end{align*}\n$$", "md": "Using $$\\xi \\geq 3a$$, we have $$\\exp(-a^2/4)$$, we have $$\\exp(-(x-\\xi)^2/2) \\leq \\exp(-a^2/16)$$. For the lower bound, we have\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[tanh''(ax)(-3a^2/2 + a^2x^2)] \\\\\n&= \\int_{-\\infty}^{\\infty} tanh''(ax)(-3a^2/2 + a^2x^2) \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq \\int_{-\\infty}^{\\infty} tanh''(ax)(-3a^2/2 + a^2x^2) \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq \\int_{-\\infty}^{\\infty} tanh''(ax)a^2x^2 \\exp(-(x - \\frac{1}{2}\\xi)^2) - \\exp(-(x + \\xi)^2/2) dx \\\\\n&\\geq -\\sqrt{8a^2} \\int_{-\\infty}^{\\infty} e^{-2ax}x^2 \\exp(-(x - \\xi)^2) - \\exp(-(x + \\xi)^2) dx \\\\\n&\\geq -8a^2e^{-\\sqrt{2\\pi}\\sqrt{6a}} \\int_{\\frac{3}{2}}^{\\infty} 3x^2 \\exp(-(x - \\frac{1}{2}\\xi)^2)dx \\geq -8a^2e^{-\\sqrt{6a}}\n\\end{align*}\n$$\nUsing upper bound and lower bound, we have\n\nFor the third term, we have\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[tanh''(ax)a^2(-3 + x^2)] \\leq 8a^2e^{-\\sqrt{6a}} \\\\\n&E_{x \\sim N(\\xi,1)}[a^3x^2 tanh'''(ax)] \\\\\n&= \\frac{32}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} a^3x\\sigma(2ax)(1 - \\sigma(2ax))(1 - 6\\sigma(2ax)(1 - \\sigma(2ax))) \\exp(-(x - 2\\xi)^2 - \\exp(-(x + \\xi)^2) dx \\\\\n&\\leq \\int_{0}^{\\infty} xe^{-ax} \\exp(-(x - 2\\xi)^2) dx \\\\\n&\\leq a^3(2 + a^3/2) e^{-\\xi^2} (10|\\xi - a|e)\n\\end{align*}\n$$\nWe can lower bound the third term as follows:\n\n$$\n\\begin{align*}\n&E_{x \\sim N(\\xi,1)}[a^3x^2 tanh'''(ax)] \\\\\n&\\geq \\sqrt{1/2} \\int_{0}^{c} a^3x tanh'''(ax) \\exp(-(x + \\xi)^2 - \\exp(-(x - \\xi)^2) dx \\\\\n&\\geq 2\\sqrt{a^3/2\\pi} \\int_{0}^{c} x \\exp(-(x - 2\\xi)^2) \\exp(-2\\xi x) - 1 dx \\\\\n&\\geq -a^3\\xi/2\\pi \\int_{0}^{\\infty} x^2 \\exp(-(x - \\xi)^2) dx \\geq -\\xi \\exp(-\\xi^2/4/3)\n\\end{align*}\n$$"}]}, {"page": 44, "text": "Using all the bounds, we have\n            dG(a, \u03be)                                                      \u22122a(\u03be\u2212a)\u2212a2                        \u221a  6a + ae\u2212\u03be2                                 \u22122a(\u03be\u2212a)\u2212a2\n                              \u2264    a3         2 + a3                               2          + 8a2e\u2212                            2 + a|\u03be \u2212          a| e           2\n                 d\u03be                10e\u2212\u03be2             10|\u03be \u2212        a| e\nWhen \u03be \u2265            a and a \u2265           c for some suffi            ciently large constant c (for example, c = 25), then, we have\n                  dG(a, \u03be)                                                      \u2212a2                    \u221a 6a + ae\u2212a2                                 \u2212a2\n                                    \u2264    a3         2 + a3                        2    + 8a2e\u2212                             2 + a|\u03be \u2212          a| e    2     \u2264   0.01\n                       d\u03be                10e\u2212a2             10|\u03be \u2212        a| e\nWhen 3a      4 \u2264      \u03be \u2264     a and a > c for suffi                 ciently large constant c (for example, c = 25), we have\n                            dG(a, \u03be)                                            \u2212a2                   \u221a  6a + ae\u2212a2                       \u2212a2\n                                               \u2264    a3        32 + a4             4    + 8a2e\u2212                            2 + a2            4    \u2264    0.01\n                                  d\u03be                10e\u22129a2             40e                                                         4 e\nPluggint the bound on |dG(a,\u03be)                   d\u03be      | in Eq. (F.1), we obtain the fi                         nal result.\nF.3          Proof of Lemma C.10\nProof of Lemma C.10. We will prove this by induction. For h = 0, this is true because the algorithm\ninitializes the gradient descent on the low noise regime with the output of gradient descent on the\nhigh noise regime, and the output is guaranteed to have \u27e8\u02c6                                                    \u00b5(0)     \u00b5\u2217 t\u27e9   to be \u2126(1) and by assumption\n                                                               \u00b5(0)  ,\u00b5\u2217t \u27e9                                      t , \u02c6\n\u2225\u00b5\u2217  t \u2225  > c\u2032, therefore \u2225\u00b5(0)            t \u2225    \u2208\u00b5(h)[c, 4\u27e8\u02c6   t3        ].\n       Suppose \u2225\u00b5(h)\u2225        t        \u2208    [c, 4\u27e8\u02c6   t 3 ,\u00b5\u2217t \u27e9 ], then we know that \u2225\u00b5(h+1)                   t          \u2212   \u00b5\u2217 t \u2225   < \u2225\u00b5(h)   t     \u2212    \u00b5\u2217 t\u2225.     To prove\n                               \u00b5(h+1)    ,\u00b5\u2217                                                                                               \u00b5(h)  ,\u00b5\u2217\n\u2225\u00b5(h+1)\u2225          \u2208    [c, 4\u27e8\u02c6   t          t \u27e9 ], fi  rst we will prove that \u27e8\u02c6                     \u00b5(h)   , \u00b5(r+1)     \u27e9   \u2208    [c, 6\u27e8\u02c6    t      t \u27e9].     Note that the\n     t                               3                                                                  t        t                             5\nupdate in the direction of \u27e8\u02c6                      \u00b5t, \u00b5t\u27e9      works like 1D. Therefore, we have a contraction for it as follows.\n                                             \u27e8\u02c6\u00b5(h)    , \u00b5(h+1)     \u27e9  \u2212   \u27e8\u02c6\u00b5(h)   , \u00b5\u2217t \u27e9    < \u27e8\u02c6  \u00b5(h), \u00b5(h)\u27e9         \u2212   \u27e8\u02c6\u00b5t, \u00b5\u2217  t \u27e9\n                                                  t        t                   t                        t        t\n                                                                                                                                      \u00b5(h)  ,\u00b5\u2217t \u27e9\nIf \u2225\u00b5(h)\u2225        \u2264   \u27e8\u02c6\u00b5(h)   , \u00b5\u2217                                                                    \u00b5(h)    , \u00b5(h+1)     \u27e9  \u2264    6\u27e8\u02c6  t           and \u27e8\u02c6   \u00b5(h)    , \u00b5(h+1)     \u27e9  \u2265\n         t               t         t \u27e9, then using Lemma F.4, we know \u27e8\u02c6t                                         t                       5                     t        t\n\u2225\u00b5(h)\u2225       \u2265    c because of the contraction. If \u2225\u00b5(h)\u2225                                    \u2265    \u27e8\u02c6\u00b5(h)   , \u00b5\u2217 t\u27e9   and \u27e8\u02c6    \u00b5(h), \u00b5(h+1)\u27e9           \u2265    \u27e8\u02c6\u00b5(h)   , \u00b5\u2217 t \u27e9, then\n     t                                                                               t                t                           t        t                    t\n\u27e8\u02c6\u00b5(h)   , \u00b5(h+1)     \u27e9   \u2264     \u2225\u00b5(h)\u2225        because of the contraction.                             If \u2225\u00b5(h)\u2225          \u2265    \u27e8\u02c6\u00b5(h)   , \u00b5\u2217 t \u27e9   and \u27e8\u02c6    \u00b5(h+1), \u00b5(h)\u27e9            \u2264\n    t        t                       t                                                                          t                  t           \u00b5(h)  ,\u00b5\u2217       t    \u00b5(0),\u00b5\u2217 tt \u27e9\n\u27e8\u02c6\u00b5(h)   , \u00b5\u2217t \u27e9, then using \u27e8\u02c6           \u00b5(h+1), \u00b5(h)\u27e9          \u2265    \u2225\u00b5(h)\u2225      \u2212   U(\u27e8\u02c6   \u00b5(h), \u00b5(h)\u27e9, \u27e8\u02c6       \u00b5(h)   , \u00b5\u2217 t \u27e9)    \u2265    4\u27e8\u02c6  t      t \u27e9 \u2265    4\u27e8\u02c6  t           \u2265   c\n    t                                       t             t               t                     t        t            t                           5                     5\n                                                                                                                   \u00b5(h)   ,\u00b5\u2217t \u27e9\nfrom Lemma F.2, we get the result that \u27e8\u02c6                                      \u00b5(h)   , \u00b5(h+1)      \u27e9  \u2208   [c, 6\u27e8\u02c6   t          ]. Now, using Lemma F.2, we\n                                                                                 t         t                           5\nget\n                                                                  \u00b5(h)   , \u00b5\u2217 t\u27e9                                           c               t \u2225  cos \u03b2h\n                           \u27e8\u02c6\u00b5(h)   , \u00b5(h+1)      \u27e9  \u2208   [c, 6\u27e8\u02c6    t             ] =\u21d2         \u2225\u00b5(h+1)\u2225         \u2208                 , 6\u2225\u00b5\u2217\n                               t         t                            5                            t                  cos \u03b1h            5 cos \u03b1h\n                                                                                     =\u21d2        \u2225\u00b5(h+1)\u2225         \u2208    c, 4\u2225\u00b5\u2217     t \u2225 cos \u03b2h+1\n                                                                                                   t                                  3\n                                                                                                                             \u00b5(h+1)      , \u00b5\u2217 t \u27e9\n                                                                                     =\u21d2        \u2225\u00b5(h+1)      \u2225   \u2208    c, 4\u27e8\u02c6     t\n                                                                                                   t                                3\nLemma F.2. Suppose the angle between \u00b5(r) and \u00b5\u2217                                                       is \u03b2r and \u03b1r is the angle between \u00b5(r) and\n\u00b5(r+1) and assume the contraction is true at time r. Assume that \u03b20 \u2208                                                                (0, \u03c02 ). Then:\nwhich implies that                              \u03b1r \u2208      (0, \u03c0/2) \u2200r                   and             cos \u03b2r \u2264         cos \u03b2r+1\n                                                cos \u03b2r \u2264        cos \u03b2r+1 \u2200r =\u21d2            44     \u27e8\u02c6\u00b5(r)   , \u00b5\u2217\u27e9    \u2265    \u27e8\u02c6\u00b5(0)   , \u00b5\u2217\u27e9", "md": "Using all the bounds, we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-2a(\\xi-a)-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-\\xi^2}}}{10e^{-\\xi^2}} + \\frac{8a^2e^{-2}+a|\\xi-a|e^2}{10|\\xi-a|e}\n$$\n\nWhen $\\xi \\geq a$ and $a \\geq c$ for some sufficiently large constant $c$ (for example, $c = 25$), then, we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-a^2}}}{10e^{-a^2}} + \\frac{8a^2e^{-2}+a|\\xi-a|e^2}{10|\\xi-a|e} \\leq 0.01\n$$\n\nWhen $\\frac{3a}{4} \\leq \\xi \\leq a$ and $a > c$ for sufficiently large constant $c$ (for example, $c = 25$), we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-a^2}}}{10e^{-9a^2}} + \\frac{8a^2e^{-2}+a^2}{40e^4} \\leq 0.01\n$$\n\nPlugging the bound on $|dG(a,\\xi)/d\\xi|$ in Eq. (F.1), we obtain the final result.\n\n### Proof of Lemma C.10\n\nProof of Lemma C.10. We will prove this by induction. For $h = 0$, this is true because the algorithm initializes the gradient descent on the low noise regime with the output of gradient descent on the high noise regime, and the output is guaranteed to have $\\langle \\hat{\\mu}(0), \\mu^*_t \\rangle$ to be $\\Omega(1)$ and by assumption $\\|\\mu^*_t\\| > c'$, therefore $\\|\\mu(0)_t\\| \\in \\mu(h)[c, 4\\langle \\hat{t}^3 \\rangle]$.\n\nSuppose $\\|\\mu(h)_t\\| \\in [c, 4\\langle \\hat{t}^3, \\mu^*_t \\rangle]$, then we know that $\\|\\mu(h+1)_t - \\mu^*_t\\| < \\|\\mu(h)_t - \\mu^*_t\\|$. To prove $\\|\\mu(h+1)\\|_t \\in [c, 4\\langle \\hat{t}^3, \\mu^*_t \\rangle]$, first we will prove that $\\langle \\hat{\\mu}(h), \\mu(r+1) \\rangle \\in [c, 6\\langle \\hat{t}^5, t \\rangle]$. Note that the update in the direction of $\\langle \\hat{\\mu}_t, \\mu_t \\rangle$ works like 1D. Therefore, we have a contraction for it as follows:\n\n$$\n\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle - \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle < \\langle \\hat{\\mu}(h), \\mu(h) \\rangle - \\langle \\hat{\\mu}_t, \\mu^*_t \\rangle\n$$\n\nIf $\\|\\mu(h)\\| \\leq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then using Lemma F.4, we know $\\|\\mu(h)\\| \\geq c$ because of the contraction. If $\\|\\mu(h)\\| \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$ and $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\leq \\|\\mu(h)\\|$ because of the contraction. If $\\|\\mu(h)\\| \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$ and $\\langle \\hat{\\mu}(h+1), \\mu(h) \\rangle \\leq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then using $\\langle \\hat{\\mu}(h+1), \\mu(h) \\rangle \\geq \\|\\mu(h)\\| - U(\\langle \\hat{\\mu}(h), \\mu(h) \\rangle, \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle) \\geq 4\\langle \\hat{t}^5 \\rangle \\geq c$ from Lemma F.2, we get the result that $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\in [c, 6\\langle \\hat{t}^5 \\rangle]$. Now, using Lemma F.2, we get\n\n$$\n\\langle \\mu(h), \\mu^*_t \\rangle \\leq c \\Rightarrow \\|\\mu(h+1)\\| \\in \\left[ c, 6\\|\\mu^*_t\\| \\cos \\beta_h \\right] \\Rightarrow \\|\\mu(h+1)\\| \\in \\left[ c, 4\\|\\mu^*_t\\| \\cos \\beta_{h+1}^3 \\right]\n$$\n\n### Lemma F.2\n\nSuppose the angle between $\\mu(r)$ and $\\mu^*_t$ is $\\beta_r$ and $\\alpha_r$ is the angle between $\\mu(r)$ and $\\mu(r+1)$ and assume the contraction is true at time $r$. Assume that $\\beta_0 \\in (0, \\frac{\\pi}{2})$ which implies that $\\alpha_r \\in (0, \\frac{\\pi}{2}) \\forall r$ and $\\cos \\beta_r \\leq \\cos \\beta_{r+1}$.\n\n$$\n\\cos \\beta_r \\leq \\cos \\beta_{r+1} \\Rightarrow 4^4 \\langle \\hat{\\mu}(r), \\mu^* \\rangle \\geq \\langle \\hat{\\mu}(0), \\mu^* \\rangle\n$$", "images": [], "items": [{"type": "text", "value": "Using all the bounds, we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-2a(\\xi-a)-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-\\xi^2}}}{10e^{-\\xi^2}} + \\frac{8a^2e^{-2}+a|\\xi-a|e^2}{10|\\xi-a|e}\n$$\n\nWhen $\\xi \\geq a$ and $a \\geq c$ for some sufficiently large constant $c$ (for example, $c = 25$), then, we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-a^2}}}{10e^{-a^2}} + \\frac{8a^2e^{-2}+a|\\xi-a|e^2}{10|\\xi-a|e} \\leq 0.01\n$$\n\nWhen $\\frac{3a}{4} \\leq \\xi \\leq a$ and $a > c$ for sufficiently large constant $c$ (for example, $c = 25$), we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-a^2}}}{10e^{-9a^2}} + \\frac{8a^2e^{-2}+a^2}{40e^4} \\leq 0.01\n$$\n\nPlugging the bound on $|dG(a,\\xi)/d\\xi|$ in Eq. (F.1), we obtain the final result.", "md": "Using all the bounds, we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-2a(\\xi-a)-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-\\xi^2}}}{10e^{-\\xi^2}} + \\frac{8a^2e^{-2}+a|\\xi-a|e^2}{10|\\xi-a|e}\n$$\n\nWhen $\\xi \\geq a$ and $a \\geq c$ for some sufficiently large constant $c$ (for example, $c = 25$), then, we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-a^2}}}{10e^{-a^2}} + \\frac{8a^2e^{-2}+a|\\xi-a|e^2}{10|\\xi-a|e} \\leq 0.01\n$$\n\nWhen $\\frac{3a}{4} \\leq \\xi \\leq a$ and $a > c$ for sufficiently large constant $c$ (for example, $c = 25$), we have\n\n$$\n\\frac{dG(a, \\xi)}{d\\xi} \\leq \\frac{-a^2}{a^3} + \\frac{2\\sqrt{6a+ae^{-a^2}}}{10e^{-9a^2}} + \\frac{8a^2e^{-2}+a^2}{40e^4} \\leq 0.01\n$$\n\nPlugging the bound on $|dG(a,\\xi)/d\\xi|$ in Eq. (F.1), we obtain the final result."}, {"type": "heading", "lvl": 3, "value": "Proof of Lemma C.10", "md": "### Proof of Lemma C.10"}, {"type": "text", "value": "Proof of Lemma C.10. We will prove this by induction. For $h = 0$, this is true because the algorithm initializes the gradient descent on the low noise regime with the output of gradient descent on the high noise regime, and the output is guaranteed to have $\\langle \\hat{\\mu}(0), \\mu^*_t \\rangle$ to be $\\Omega(1)$ and by assumption $\\|\\mu^*_t\\| > c'$, therefore $\\|\\mu(0)_t\\| \\in \\mu(h)[c, 4\\langle \\hat{t}^3 \\rangle]$.\n\nSuppose $\\|\\mu(h)_t\\| \\in [c, 4\\langle \\hat{t}^3, \\mu^*_t \\rangle]$, then we know that $\\|\\mu(h+1)_t - \\mu^*_t\\| < \\|\\mu(h)_t - \\mu^*_t\\|$. To prove $\\|\\mu(h+1)\\|_t \\in [c, 4\\langle \\hat{t}^3, \\mu^*_t \\rangle]$, first we will prove that $\\langle \\hat{\\mu}(h), \\mu(r+1) \\rangle \\in [c, 6\\langle \\hat{t}^5, t \\rangle]$. Note that the update in the direction of $\\langle \\hat{\\mu}_t, \\mu_t \\rangle$ works like 1D. Therefore, we have a contraction for it as follows:\n\n$$\n\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle - \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle < \\langle \\hat{\\mu}(h), \\mu(h) \\rangle - \\langle \\hat{\\mu}_t, \\mu^*_t \\rangle\n$$\n\nIf $\\|\\mu(h)\\| \\leq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then using Lemma F.4, we know $\\|\\mu(h)\\| \\geq c$ because of the contraction. If $\\|\\mu(h)\\| \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$ and $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\leq \\|\\mu(h)\\|$ because of the contraction. If $\\|\\mu(h)\\| \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$ and $\\langle \\hat{\\mu}(h+1), \\mu(h) \\rangle \\leq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then using $\\langle \\hat{\\mu}(h+1), \\mu(h) \\rangle \\geq \\|\\mu(h)\\| - U(\\langle \\hat{\\mu}(h), \\mu(h) \\rangle, \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle) \\geq 4\\langle \\hat{t}^5 \\rangle \\geq c$ from Lemma F.2, we get the result that $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\in [c, 6\\langle \\hat{t}^5 \\rangle]$. Now, using Lemma F.2, we get\n\n$$\n\\langle \\mu(h), \\mu^*_t \\rangle \\leq c \\Rightarrow \\|\\mu(h+1)\\| \\in \\left[ c, 6\\|\\mu^*_t\\| \\cos \\beta_h \\right] \\Rightarrow \\|\\mu(h+1)\\| \\in \\left[ c, 4\\|\\mu^*_t\\| \\cos \\beta_{h+1}^3 \\right]\n$$", "md": "Proof of Lemma C.10. We will prove this by induction. For $h = 0$, this is true because the algorithm initializes the gradient descent on the low noise regime with the output of gradient descent on the high noise regime, and the output is guaranteed to have $\\langle \\hat{\\mu}(0), \\mu^*_t \\rangle$ to be $\\Omega(1)$ and by assumption $\\|\\mu^*_t\\| > c'$, therefore $\\|\\mu(0)_t\\| \\in \\mu(h)[c, 4\\langle \\hat{t}^3 \\rangle]$.\n\nSuppose $\\|\\mu(h)_t\\| \\in [c, 4\\langle \\hat{t}^3, \\mu^*_t \\rangle]$, then we know that $\\|\\mu(h+1)_t - \\mu^*_t\\| < \\|\\mu(h)_t - \\mu^*_t\\|$. To prove $\\|\\mu(h+1)\\|_t \\in [c, 4\\langle \\hat{t}^3, \\mu^*_t \\rangle]$, first we will prove that $\\langle \\hat{\\mu}(h), \\mu(r+1) \\rangle \\in [c, 6\\langle \\hat{t}^5, t \\rangle]$. Note that the update in the direction of $\\langle \\hat{\\mu}_t, \\mu_t \\rangle$ works like 1D. Therefore, we have a contraction for it as follows:\n\n$$\n\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle - \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle < \\langle \\hat{\\mu}(h), \\mu(h) \\rangle - \\langle \\hat{\\mu}_t, \\mu^*_t \\rangle\n$$\n\nIf $\\|\\mu(h)\\| \\leq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then using Lemma F.4, we know $\\|\\mu(h)\\| \\geq c$ because of the contraction. If $\\|\\mu(h)\\| \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$ and $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\leq \\|\\mu(h)\\|$ because of the contraction. If $\\|\\mu(h)\\| \\geq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$ and $\\langle \\hat{\\mu}(h+1), \\mu(h) \\rangle \\leq \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle$, then using $\\langle \\hat{\\mu}(h+1), \\mu(h) \\rangle \\geq \\|\\mu(h)\\| - U(\\langle \\hat{\\mu}(h), \\mu(h) \\rangle, \\langle \\hat{\\mu}(h), \\mu^*_t \\rangle) \\geq 4\\langle \\hat{t}^5 \\rangle \\geq c$ from Lemma F.2, we get the result that $\\langle \\hat{\\mu}(h), \\mu(h+1) \\rangle \\in [c, 6\\langle \\hat{t}^5 \\rangle]$. Now, using Lemma F.2, we get\n\n$$\n\\langle \\mu(h), \\mu^*_t \\rangle \\leq c \\Rightarrow \\|\\mu(h+1)\\| \\in \\left[ c, 6\\|\\mu^*_t\\| \\cos \\beta_h \\right] \\Rightarrow \\|\\mu(h+1)\\| \\in \\left[ c, 4\\|\\mu^*_t\\| \\cos \\beta_{h+1}^3 \\right]\n$$"}, {"type": "heading", "lvl": 3, "value": "Lemma F.2", "md": "### Lemma F.2"}, {"type": "text", "value": "Suppose the angle between $\\mu(r)$ and $\\mu^*_t$ is $\\beta_r$ and $\\alpha_r$ is the angle between $\\mu(r)$ and $\\mu(r+1)$ and assume the contraction is true at time $r$. Assume that $\\beta_0 \\in (0, \\frac{\\pi}{2})$ which implies that $\\alpha_r \\in (0, \\frac{\\pi}{2}) \\forall r$ and $\\cos \\beta_r \\leq \\cos \\beta_{r+1}$.\n\n$$\n\\cos \\beta_r \\leq \\cos \\beta_{r+1} \\Rightarrow 4^4 \\langle \\hat{\\mu}(r), \\mu^* \\rangle \\geq \\langle \\hat{\\mu}(0), \\mu^* \\rangle\n$$", "md": "Suppose the angle between $\\mu(r)$ and $\\mu^*_t$ is $\\beta_r$ and $\\alpha_r$ is the angle between $\\mu(r)$ and $\\mu(r+1)$ and assume the contraction is true at time $r$. Assume that $\\beta_0 \\in (0, \\frac{\\pi}{2})$ which implies that $\\alpha_r \\in (0, \\frac{\\pi}{2}) \\forall r$ and $\\cos \\beta_r \\leq \\cos \\beta_{r+1}$.\n\n$$\n\\cos \\beta_r \\leq \\cos \\beta_{r+1} \\Rightarrow 4^4 \\langle \\hat{\\mu}(r), \\mu^* \\rangle \\geq \\langle \\hat{\\mu}(0), \\mu^* \\rangle\n$$"}]}, {"page": 45, "text": "                                                                                                                         \u00b5(r)  ,\u00b5\u2217\nProof. First, we will prove that if \u03b2r \u2208                                   (0, \u03c0  2 ) and \u2225\u00b5(r)\u2225            \u2208    [c, 4\u27e8\u02c6   t 3    t \u27e9 ], then \u03b1r \u2208            (0, \u03b2r) for any\nr. We denote \u03b1r > 0 if \u00b5(r) moves towards \u00b5(r)\u22a5                                                   and hence towards \u00b5\u2217. The following simple\nobservation of \u27e8\u02c6            \u00b5(r)\u22a5, \u00b5(r+1)\u27e9            \u2265   0 proves that \u03b1r > 0.\n  \u27e8\u02c6\u00b5(r)\u22a5, \u00b5(r+1)\u27e9                                                                                                                                               \u00b7 \u27e8\u02c6\n            = E    x\u223cN (\u00b5\u2217,1)         \u03b7    tanh(\u00b5(r)\u22a4x) \u2212                1                                                                                          \u00b5(r)\u22a5, x\u27e9\n                                                                         2 tanh\u2032\u2032(\u00b5(r)\u22a4x)\u2225\u00b5(r)\u22252 + tanh\u2032(\u00b5(r)\u22a4x)\u00b5(r)\u22a4x\n            = E                     \u03b7     tanh(\u00b5(r)\u22a4(x + \u00b5\u2217)) \u2212                     1\n                   x\u223cN (0,1)                                                        2 tanh\u2032\u2032(\u00b5(r)\u22a4(x + \u00b5\u2217))\u2225\u00b5(r)\u22252\n                         + tanh\u2032(\u00b5(r)\u22a4(x + \u00b5\u2217))\u00b5(r)\u22a4(x + \u00b5\u2217)                                    \u00b7 \u27e8\u02c6\n            = E    \u03b11,\u03b12\u223cN (\u27e8\u02c6      \u00b5(r)              \u03b7     tanh(\u2225\u00b5(r)\u2225\u03b11) \u2212                 1     \u00b5(r)\u22a5, (x + \u00b5\u2217)\u27e9\n                                          ,\u00b5\u2217\u27e9,1)                                            2 tanh\u2032\u2032(\u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u22252\n                         + tanh\u2032(\u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u2225\u03b11                          (\u03b12 + \u27e8\u02c6     \u00b5(r)\u22a5, \u00b5\u2217\u27e9)\n            = E    \u03b11,\u03b12\u223cN (\u27e8\u02c6      \u00b5(r)              \u03b7     tanh(\u2225\u00b5(r)\u2225\u03b11) \u2212                 1\n                                          ,\u00b5\u2217\u27e9,1)                                            2 tanh\u2032\u2032(\u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u22252\n                                                               + tanh\u2032(\u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u2225\u03b11                            \u00b7 \u27e8\u02c6\n                                                                                                                       \u00b5(r)\u22a5, \u00b5\u2217\u27e9           > 0 ,\nwhere in the last step we used the fact that \u27e8\u02c6                                     \u00b5(r)   , \u00b5\u2217\u27e9     > 0 and \u27e8\u02c6        \u00b5(r)\u22a5, \u00b5\u2217\u27e9         > 0.\n       Now, we will prove that cot \u03b1r > cot \u03b2r which will prove that \u03b1r \u2208                                                           (0, \u03b2r). Note that\n                         cot \u03b1r = \u27e8\u02c6         \u00b5(r)   , \u00b5(r+1)     \u27e9              where\n        \u27e8\u02c6                                \u27e8\u02c6\u00b5(r)\u22a5, \u00b5(r+1)\u27e9\n         \u00b5(r)   , \u00b5(r+1)      \u27e9 = (1 \u2212        \u03b7)\u2225\u00b5(r)\u2225        + \u03b7E\u03b11\u223cN (\u02c6          \u00b5(r)\u22a4   \u00b5\u2217,1)[tanh(\u2225\u00b5(r)\u2225\u03b11)\u03b11]\n                                     + \u03b7E     \u03b11\u223cN (\u02c6    \u00b5(r)\u22a4   \u00b5\u2217,1)[\u22121     2 tanh\u2032\u2032(\u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u22252\u03b11 + tanh\u2032(\u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u2225\u03b12                                            1\n                                     \u2212   tanh\u2032(\u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u2225]\n     \u27e8\u02c6\u00b5(r)\u22a5, \u00b5(r+1)\u27e9           = \u03b7\u27e8\u02c6    \u00b5(r)\u22a5, \u00b5\u2217\u27e9E\u03b11\u223cN (\u02c6             \u00b5(r)\u22a4   \u00b5\u2217,1)[tanh(\u2225\u00b5(r)\u2225\u03b11) \u2212                     1\n                                     + tanh\u2032( \u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u2225\u03b11]                                                           2 tanh\u2032\u2032( \u2225\u00b5(r)\u2225\u03b11)\u2225\u00b5(r)\u22252\n                         and        cot \u03b2r = \u27e8\u02c6         \u00b5(r)   , \u00b5\u2217\u27e9\n                                                     \u27e8\u02c6\u00b5(r)\u22a5, \u00b5\u2217\u27e9\nObserve the fact that to prove a+c\u2032                        b+c \u2212        ab > 0, it is suffi          cient to prove c\u2032 > ac               b for b, c > 0. Using this\nobservation, to prove cot \u03b1r > cot \u03b2r, it is suffi                                     cient to prove\n                1 \u2212    \u03b7 \u2212     \u03b7E[tanh\u2032(\u2225\u00b5(r)\u2225x)]                   \u2225\u00b5(r)\u2225      + \u03b7Ex         \u2212    1                                                      \u00b5(r), \u00b5\u2217\u27e9)\n                                                                                                   2 tanh\u2032\u2032(\u2225\u00b5(r)\u2225x)\u2225\u00b5(r)\u22252(x \u2212                          \u27e8\u02c6\n              + tanh\u2032(\u2225\u00b5(r)\u2225x)(x2 \u2212                     \u27e8\u02c6\u00b5(r)   , \u00b5\u2217\u27e9x) + tanh(\u2225\u00b5(r)\u2225x)(x \u2212                          \u27e8\u02c6\nwhere the expectation is wrt N                       1     (\u27e8\u00b5(r), \u00b5\u2217\u27e9, 1). Lemma F.3 shows that this is indeed true.  \u00b5(r)   , \u00b5\u2217\u27e9)       > 0,\nLemma F.3. For any \u03b7 =                              20  , assuming a \u2208               [30, 4b  3 ], we have\n          (1 \u2212     \u03b7 \u2212     \u03b7Ex\u223cN (b,1)[tanh\u2032(ax)])a\n                    + \u03b7 E     x\u223cN (b,1)         \u2212   1                                                                                                                > 0 .\n                                                    2 tanh\u2032\u2032(ax)a2(x \u2212                  b) tanh\u2032(ax)(x2 \u2212                bx) + tanh(ax)(x \u2212                    b)\n                                                                                          45", "md": "Proof. First, we will prove that if $$\\beta r \\in (0, \\frac{\\pi}{2})$$ and $$\\|\\mu(r)\\| \\in [c, 4\\langle\\hat{t}^3 t\\rangle]$$, then $$\\alpha r \\in (0, \\beta r)$$ for any r. We denote $$\\alpha r > 0$$ if $$\\mu(r)$$ moves towards $$\\mu(r)^\\perp$$ and hence towards $$\\mu^*$$. The following simple observation of $$\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle \\geq 0$$ proves that $$\\alpha r > 0$$.\n\n$$\n\\begin{align*}\n\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle &= E_{x\\sim N(\\mu^*,1)}[\\eta \\tanh(\\mu(r)^\\top x) - 1 \\mu(r)^\\perp, x] \\\\\n&= E[\\eta \\tanh(\\mu(r)^\\top(x + \\mu^*)) - 1 2 \\tanh''(\\mu(r)^\\top(x + \\mu^*))\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\mu(r)^\\top(x + \\mu^*))\\mu(r)^\\top(x + \\mu^*) \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle \\\\\n&= E[\\alpha_1, \\alpha_2\\sim N(\\langle\\hat{\\mu(r)}\\eta \\tanh(\\|\\mu(r)\\|\\alpha_1) - 1, \\mu^*\\rangle, 1) 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1 (\\alpha_2 + \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle) \\\\\n&= E[\\alpha_1, \\alpha_2\\sim N(\\langle\\hat{\\mu(r)}\\eta \\tanh(\\|\\mu(r)\\|\\alpha_1) - 1, \\mu^*\\rangle, 1) 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1 \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle] > 0,\n\\end{align*}\n$$\nwhere in the last step we used the fact that $$\\langle\\hat{\\mu(r)}, \\mu^*\\rangle > 0$$ and $$\\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle > 0$$.\n\nNow, we will prove that $$\\cot \\alpha r > \\cot \\beta r$$ which will prove that $$\\alpha r \\in (0, \\beta r)$$. Note that\n\n$$\n\\begin{align*}\n\\cot \\alpha r &= \\langle\\hat{\\mu(r)}, \\mu(r+1)\\rangle \\text{ where} \\\\\n\\langle\\hat{\\mu(r)}, \\mu(r+1)\\rangle &= (1 - \\eta)\\|\\mu(r)\\| + \\eta E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[\\tanh(\\|\\mu(r)\\|\\alpha_1)\\alpha_1] \\\\\n&\\quad + \\eta E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[-1 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2\\alpha_1 + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_2 1 \\\\\n&\\quad - \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|] \\\\\n\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle &= \\eta\\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[\\tanh(\\|\\mu(r)\\|\\alpha_1) - 1 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1] 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2\n\\end{align*}\n$$\nObserve the fact that to prove $$a+c'b+c - ab > 0$$, it is sufficient to prove $$c' > ac b$$ for $$b, c > 0$$. Using this observation, to prove $$\\cot \\alpha r > \\cot \\beta r$$, it is sufficient to prove\n\n$$\n\\begin{align*}\n1 - \\eta - \\eta E[\\tanh'(\\|\\mu(r)\\|x)] \\|\\mu(r)\\| + \\eta Ex - 1 \\mu(r), \\mu^*\\rangle \\\\\n2 \\tanh''(\\|\\mu(r)\\|x)\\|\\mu(r)\\|^2(x - \\langle\\hat{\\mu(r)}, \\mu^*\\rangle x) + \\tanh(\\|\\mu(r)\\|x)(x - \\langle\\hat{\\mu(r)}, \\mu^*\\rangle)\n\\end{align*}\n$$\nwhere the expectation is wrt $$N_1(\\langle\\mu(r), \\mu^*\\rangle, 1)$$. Lemma F.3 shows that this is indeed true. $$\\langle\\mu(r), \\mu^*\\rangle > 0$$,\n\nLemma F.3. For any $$\\eta = \\frac{1}{20}$$, assuming $$a \\in [30, 4b^3]$$, we have\n\n$$\n\\begin{align*}\n(1 - \\eta - \\eta E[x\\sim N(b,1)[\\tanh'(ax)])a + \\eta E[x\\sim N(b,1) - 1 \\\\\n2 \\tanh''(ax)a^2(x - b) \\tanh'(ax)(x^2 - bx) + \\tanh(ax)(x - b)] > 0.\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Proof. First, we will prove that if $$\\beta r \\in (0, \\frac{\\pi}{2})$$ and $$\\|\\mu(r)\\| \\in [c, 4\\langle\\hat{t}^3 t\\rangle]$$, then $$\\alpha r \\in (0, \\beta r)$$ for any r. We denote $$\\alpha r > 0$$ if $$\\mu(r)$$ moves towards $$\\mu(r)^\\perp$$ and hence towards $$\\mu^*$$. The following simple observation of $$\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle \\geq 0$$ proves that $$\\alpha r > 0$$.\n\n$$\n\\begin{align*}\n\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle &= E_{x\\sim N(\\mu^*,1)}[\\eta \\tanh(\\mu(r)^\\top x) - 1 \\mu(r)^\\perp, x] \\\\\n&= E[\\eta \\tanh(\\mu(r)^\\top(x + \\mu^*)) - 1 2 \\tanh''(\\mu(r)^\\top(x + \\mu^*))\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\mu(r)^\\top(x + \\mu^*))\\mu(r)^\\top(x + \\mu^*) \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle \\\\\n&= E[\\alpha_1, \\alpha_2\\sim N(\\langle\\hat{\\mu(r)}\\eta \\tanh(\\|\\mu(r)\\|\\alpha_1) - 1, \\mu^*\\rangle, 1) 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1 (\\alpha_2 + \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle) \\\\\n&= E[\\alpha_1, \\alpha_2\\sim N(\\langle\\hat{\\mu(r)}\\eta \\tanh(\\|\\mu(r)\\|\\alpha_1) - 1, \\mu^*\\rangle, 1) 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1 \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle] > 0,\n\\end{align*}\n$$\nwhere in the last step we used the fact that $$\\langle\\hat{\\mu(r)}, \\mu^*\\rangle > 0$$ and $$\\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle > 0$$.\n\nNow, we will prove that $$\\cot \\alpha r > \\cot \\beta r$$ which will prove that $$\\alpha r \\in (0, \\beta r)$$. Note that\n\n$$\n\\begin{align*}\n\\cot \\alpha r &= \\langle\\hat{\\mu(r)}, \\mu(r+1)\\rangle \\text{ where} \\\\\n\\langle\\hat{\\mu(r)}, \\mu(r+1)\\rangle &= (1 - \\eta)\\|\\mu(r)\\| + \\eta E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[\\tanh(\\|\\mu(r)\\|\\alpha_1)\\alpha_1] \\\\\n&\\quad + \\eta E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[-1 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2\\alpha_1 + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_2 1 \\\\\n&\\quad - \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|] \\\\\n\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle &= \\eta\\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[\\tanh(\\|\\mu(r)\\|\\alpha_1) - 1 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1] 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2\n\\end{align*}\n$$\nObserve the fact that to prove $$a+c'b+c - ab > 0$$, it is sufficient to prove $$c' > ac b$$ for $$b, c > 0$$. Using this observation, to prove $$\\cot \\alpha r > \\cot \\beta r$$, it is sufficient to prove\n\n$$\n\\begin{align*}\n1 - \\eta - \\eta E[\\tanh'(\\|\\mu(r)\\|x)] \\|\\mu(r)\\| + \\eta Ex - 1 \\mu(r), \\mu^*\\rangle \\\\\n2 \\tanh''(\\|\\mu(r)\\|x)\\|\\mu(r)\\|^2(x - \\langle\\hat{\\mu(r)}, \\mu^*\\rangle x) + \\tanh(\\|\\mu(r)\\|x)(x - \\langle\\hat{\\mu(r)}, \\mu^*\\rangle)\n\\end{align*}\n$$\nwhere the expectation is wrt $$N_1(\\langle\\mu(r), \\mu^*\\rangle, 1)$$. Lemma F.3 shows that this is indeed true. $$\\langle\\mu(r), \\mu^*\\rangle > 0$$,\n\nLemma F.3. For any $$\\eta = \\frac{1}{20}$$, assuming $$a \\in [30, 4b^3]$$, we have\n\n$$\n\\begin{align*}\n(1 - \\eta - \\eta E[x\\sim N(b,1)[\\tanh'(ax)])a + \\eta E[x\\sim N(b,1) - 1 \\\\\n2 \\tanh''(ax)a^2(x - b) \\tanh'(ax)(x^2 - bx) + \\tanh(ax)(x - b)] > 0.\n\\end{align*}\n$$", "md": "Proof. First, we will prove that if $$\\beta r \\in (0, \\frac{\\pi}{2})$$ and $$\\|\\mu(r)\\| \\in [c, 4\\langle\\hat{t}^3 t\\rangle]$$, then $$\\alpha r \\in (0, \\beta r)$$ for any r. We denote $$\\alpha r > 0$$ if $$\\mu(r)$$ moves towards $$\\mu(r)^\\perp$$ and hence towards $$\\mu^*$$. The following simple observation of $$\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle \\geq 0$$ proves that $$\\alpha r > 0$$.\n\n$$\n\\begin{align*}\n\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle &= E_{x\\sim N(\\mu^*,1)}[\\eta \\tanh(\\mu(r)^\\top x) - 1 \\mu(r)^\\perp, x] \\\\\n&= E[\\eta \\tanh(\\mu(r)^\\top(x + \\mu^*)) - 1 2 \\tanh''(\\mu(r)^\\top(x + \\mu^*))\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\mu(r)^\\top(x + \\mu^*))\\mu(r)^\\top(x + \\mu^*) \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle \\\\\n&= E[\\alpha_1, \\alpha_2\\sim N(\\langle\\hat{\\mu(r)}\\eta \\tanh(\\|\\mu(r)\\|\\alpha_1) - 1, \\mu^*\\rangle, 1) 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1 (\\alpha_2 + \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle) \\\\\n&= E[\\alpha_1, \\alpha_2\\sim N(\\langle\\hat{\\mu(r)}\\eta \\tanh(\\|\\mu(r)\\|\\alpha_1) - 1, \\mu^*\\rangle, 1) 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1 \\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle] > 0,\n\\end{align*}\n$$\nwhere in the last step we used the fact that $$\\langle\\hat{\\mu(r)}, \\mu^*\\rangle > 0$$ and $$\\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle > 0$$.\n\nNow, we will prove that $$\\cot \\alpha r > \\cot \\beta r$$ which will prove that $$\\alpha r \\in (0, \\beta r)$$. Note that\n\n$$\n\\begin{align*}\n\\cot \\alpha r &= \\langle\\hat{\\mu(r)}, \\mu(r+1)\\rangle \\text{ where} \\\\\n\\langle\\hat{\\mu(r)}, \\mu(r+1)\\rangle &= (1 - \\eta)\\|\\mu(r)\\| + \\eta E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[\\tanh(\\|\\mu(r)\\|\\alpha_1)\\alpha_1] \\\\\n&\\quad + \\eta E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[-1 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2\\alpha_1 + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_2 1 \\\\\n&\\quad - \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|] \\\\\n\\langle\\hat{\\mu(r)}^\\perp, \\mu(r+1)\\rangle &= \\eta\\langle\\hat{\\mu(r)}^\\perp, \\mu^*\\rangle E[\\alpha_1\\sim N(\\hat{\\mu(r)}^\\top \\mu^*,1)[\\tanh(\\|\\mu(r)\\|\\alpha_1) - 1 \\\\\n&\\quad + \\tanh'(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|\\alpha_1] 2 \\tanh''(\\|\\mu(r)\\|\\alpha_1)\\|\\mu(r)\\|^2\n\\end{align*}\n$$\nObserve the fact that to prove $$a+c'b+c - ab > 0$$, it is sufficient to prove $$c' > ac b$$ for $$b, c > 0$$. Using this observation, to prove $$\\cot \\alpha r > \\cot \\beta r$$, it is sufficient to prove\n\n$$\n\\begin{align*}\n1 - \\eta - \\eta E[\\tanh'(\\|\\mu(r)\\|x)] \\|\\mu(r)\\| + \\eta Ex - 1 \\mu(r), \\mu^*\\rangle \\\\\n2 \\tanh''(\\|\\mu(r)\\|x)\\|\\mu(r)\\|^2(x - \\langle\\hat{\\mu(r)}, \\mu^*\\rangle x) + \\tanh(\\|\\mu(r)\\|x)(x - \\langle\\hat{\\mu(r)}, \\mu^*\\rangle)\n\\end{align*}\n$$\nwhere the expectation is wrt $$N_1(\\langle\\mu(r), \\mu^*\\rangle, 1)$$. Lemma F.3 shows that this is indeed true. $$\\langle\\mu(r), \\mu^*\\rangle > 0$$,\n\nLemma F.3. For any $$\\eta = \\frac{1}{20}$$, assuming $$a \\in [30, 4b^3]$$, we have\n\n$$\n\\begin{align*}\n(1 - \\eta - \\eta E[x\\sim N(b,1)[\\tanh'(ax)])a + \\eta E[x\\sim N(b,1) - 1 \\\\\n2 \\tanh''(ax)a^2(x - b) \\tanh'(ax)(x^2 - bx) + \\tanh(ax)(x - b)] > 0.\n\\end{align*}\n$$"}]}, {"page": 46, "text": "Proof. First, we will fi           nd the upper bound on E[tanh\u2032\u2032(ax)(x \u2212                           b)].\n   E[tanh\u2032\u2032(ax)(x \u2212           b)] =      \u2212\u221e\u221e   tanh\u2032\u2032(ax)(x \u2212         b) exp       \u2212   (x \u2212 2  b)2    dx\n                                   \u2264     0 b tanh\u2032\u2032(ax)(x \u2212         b) exp       \u2212   (x \u2212 2  b)2    dx\n                                   \u2264     0 b tanh\u2032\u2032(ax)x exp            \u2212   (x \u2212 2  b)2    dx\n                                   \u2264     0 b exp(\u2212ax)x exp             \u2212   (x \u2212  2 b)2    dx\n                                   \u2264   exp    a2 \u2212      2ab         b x exp      \u2212   (x \u2212    b)2 + 2a(x \u2212         b) + a2      dx\n                                                     2            0                                     2\n                                   \u2264   exp(a2 \u2212     2 2ab   )   0 \u221e  x   exp       \u2212   (x \u2212   b2+ a)2        + exp       \u2212   (x + b \u2212 2    a)2     dx\nNow, for the second term, we have  \u2264   exp(\u2212b2/2) +|a \u2212             b| \u00b7 exp    a2 \u2212    2 2ab     .\n            E x\u223cN (b,1)[tanh\u2032(ax)(x2 \u2212              bx)]\n                    =      \u221e   tanh\u2032(ax)x(x \u2212           b) exp       \u2212   (x \u2212 2  b)2    dx\n                    \u2265   \u2212b\u2212\u221e   0b xe\u2212ax exp          \u2212   (x \u2212 2  b)2    dx\n                    \u2265   \u2212b exp      a2 \u2212   2 2ab        0\u221e   x   exp      \u2212   (x \u2212    b2+ a)2       + exp        \u2212   (x + b \u22122     a)2      dx\n                    \u2265   \u2212b exp(\u2212b2/2) \u2212            b|a \u2212    b| \u00b7 exp    a2 \u2212   2 2ab\nWe can rewrite the last term as E                     x\u223cN (0,1)[tanh(a(x + b))x]. Using the fact that tanh(a(x + b)) >\ntanh(a(\u2212x + b)), we get that Ex\u223cN (0,1)[tanh(a(x + b))x] > 0. Finally, using the upper bound on\nE[tanh\u2032(ax)], we get the following lower bound.\n    (1 \u2212    \u03b7 \u2212   \u03b7 Ex\u223cN (b,1)[tanh\u2032(ax)]) a + \u03b7Ex\u223cN (b,1)                     \u2212   1\n                                                                                   2 tanh\u2032\u2032(ax)a2(x \u2212            b) + tanh\u2032(ax)(x2 \u2212            bx)\n                                  a2\u22122ab                                                                 a2 \u2212      2ab\n            \u2265    a                   2    ) + 1         \u2212   a2    exp(\u2212b2/2) +|a \u2212             b| exp\n                20(19 \u2212       4e                 20         2                                                   2\n                                                           \u2212  b exp(\u2212b2/2) \u2212           b|a \u2212    b| exp(a2 \u2212     2 2ab   )   \u2265   1 .\nLemma F.4. For any a, b > 0 and a \u2208                            [30, 4b 3 ], the following holds. Define\nU(a, b) \u225c       \u03b7E  x\u223cN (b,1)        tanh(ax)\u2212         1                                              x   \u2212\u03b7Ex\u223cN (b,1)          tanh\u2032(ax)a        \u2212\u03b7a .\n                                                       2 tanh\u2032\u2032(ax)a2 +tanh\u2032(ax)ax\nWhen the learning rate \u03b7 = 1                20, is given by, we have\n                                                                 U(a, b)      \u2264   a + b\n                                                                                     10\n                                                                            46", "md": "Proof. First, we will find the upper bound on $$E[\\tanh''(ax)(x - b)]$$.\n\n$$\n\\begin{align*}\nE[\\tanh''(ax)(x - b)] & = \\int_{-\\infty}^{\\infty} \\tanh''(ax)(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\tanh''(ax)(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\tanh''(ax)x \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\exp(-ax)x \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\exp\\left(a^2 - 2ab\\right) \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + 2a(x - b) + a^2\\right) dx \\\\\n& \\leq \\exp\\left(a^2 - 2ab\\right) \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + a\\right) + \\exp\\left(-\\frac{(x + b - 2a)^2}{2}\\right) dx\n\\end{align*}\n$$\n\nNow, for the second term, we have\n\n$$\n\\begin{align*}\n& \\leq \\exp\\left(-\\frac{b^2}{2}\\right) + |a - b| \\cdot \\exp\\left(a^2 - 2ab\\right) \\\\\nE_{x \\sim N(b,1)}[\\tanh'(ax)(x^2 - bx)] & = \\int_{-\\infty}^{\\infty} \\tanh'(ax)x(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\geq \\int_{-b}^{\\infty} 0 \\leq \\int_{0}^{b} xe^{-ax} \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\geq -b \\exp\\left(a^2 - 2ab\\right) - \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + a\\right) + \\exp\\left(-\\frac{(x + b - 2a)^2}{2}\\right) dx \\\\\n& \\geq -b \\exp\\left(-\\frac{b^2}{2}\\right) - |a - b| \\cdot \\exp\\left(a^2 - 2ab\\right)\n\\end{align*}\n$$\n\nWe can rewrite the last term as $$E_{x \\sim N(0,1)}[\\tanh(a(x + b))x]$$. Using the fact that $$\\tanh(a(x + b)) > \\tanh(a(-x + b))$$, we get that $$E_{x \\sim N(0,1)}[\\tanh(a(x + b))x] > 0$$. Finally, using the upper bound on $$E[\\tanh'(ax)]$$, we get the following lower bound.\n\n$$\n\\begin{align*}\n& (1 - \\eta - \\eta E_{x \\sim N(b,1)}[\\tanh'(ax)])a + \\eta E_{x \\sim N(b,1)}\\left[\\frac{-1}{2}\\tanh''(ax)a^2(x - b) + \\tanh'(ax)(x^2 - bx)\\right] \\\\\n& \\geq a^2 - 2ab \\geq a\\left(\\frac{1}{20}(19 - 4e^{20})\\right) + 1 - a^2 \\exp\\left(-\\frac{b^2}{2}\\right) + |a - b| \\exp\\left(a^2 - 2ab\\right) \\geq 1 .\n\\end{align*}\n$$\n\nLemma F.4. For any $$a, b > 0$$ and $$a \\in [30, 4b/3]$$, the following holds. Define $$U(a, b) \\triangleq \\eta E_{x \\sim N(b,1)}\\left[\\tanh(ax) - \\frac{1}{2}\\tanh''(ax)a^2 + \\tanh'(ax)ax\\right] - \\eta a$$.\n\nWhen the learning rate $$\\eta = \\frac{1}{20}$$, is given by, we have\n\n$$U(a, b) \\leq a + \\frac{b}{10}$$", "images": [], "items": [{"type": "text", "value": "Proof. First, we will find the upper bound on $$E[\\tanh''(ax)(x - b)]$$.\n\n$$\n\\begin{align*}\nE[\\tanh''(ax)(x - b)] & = \\int_{-\\infty}^{\\infty} \\tanh''(ax)(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\tanh''(ax)(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\tanh''(ax)x \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\exp(-ax)x \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\exp\\left(a^2 - 2ab\\right) \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + 2a(x - b) + a^2\\right) dx \\\\\n& \\leq \\exp\\left(a^2 - 2ab\\right) \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + a\\right) + \\exp\\left(-\\frac{(x + b - 2a)^2}{2}\\right) dx\n\\end{align*}\n$$\n\nNow, for the second term, we have\n\n$$\n\\begin{align*}\n& \\leq \\exp\\left(-\\frac{b^2}{2}\\right) + |a - b| \\cdot \\exp\\left(a^2 - 2ab\\right) \\\\\nE_{x \\sim N(b,1)}[\\tanh'(ax)(x^2 - bx)] & = \\int_{-\\infty}^{\\infty} \\tanh'(ax)x(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\geq \\int_{-b}^{\\infty} 0 \\leq \\int_{0}^{b} xe^{-ax} \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\geq -b \\exp\\left(a^2 - 2ab\\right) - \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + a\\right) + \\exp\\left(-\\frac{(x + b - 2a)^2}{2}\\right) dx \\\\\n& \\geq -b \\exp\\left(-\\frac{b^2}{2}\\right) - |a - b| \\cdot \\exp\\left(a^2 - 2ab\\right)\n\\end{align*}\n$$\n\nWe can rewrite the last term as $$E_{x \\sim N(0,1)}[\\tanh(a(x + b))x]$$. Using the fact that $$\\tanh(a(x + b)) > \\tanh(a(-x + b))$$, we get that $$E_{x \\sim N(0,1)}[\\tanh(a(x + b))x] > 0$$. Finally, using the upper bound on $$E[\\tanh'(ax)]$$, we get the following lower bound.\n\n$$\n\\begin{align*}\n& (1 - \\eta - \\eta E_{x \\sim N(b,1)}[\\tanh'(ax)])a + \\eta E_{x \\sim N(b,1)}\\left[\\frac{-1}{2}\\tanh''(ax)a^2(x - b) + \\tanh'(ax)(x^2 - bx)\\right] \\\\\n& \\geq a^2 - 2ab \\geq a\\left(\\frac{1}{20}(19 - 4e^{20})\\right) + 1 - a^2 \\exp\\left(-\\frac{b^2}{2}\\right) + |a - b| \\exp\\left(a^2 - 2ab\\right) \\geq 1 .\n\\end{align*}\n$$\n\nLemma F.4. For any $$a, b > 0$$ and $$a \\in [30, 4b/3]$$, the following holds. Define $$U(a, b) \\triangleq \\eta E_{x \\sim N(b,1)}\\left[\\tanh(ax) - \\frac{1}{2}\\tanh''(ax)a^2 + \\tanh'(ax)ax\\right] - \\eta a$$.\n\nWhen the learning rate $$\\eta = \\frac{1}{20}$$, is given by, we have\n\n$$U(a, b) \\leq a + \\frac{b}{10}$$", "md": "Proof. First, we will find the upper bound on $$E[\\tanh''(ax)(x - b)]$$.\n\n$$\n\\begin{align*}\nE[\\tanh''(ax)(x - b)] & = \\int_{-\\infty}^{\\infty} \\tanh''(ax)(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\tanh''(ax)(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\tanh''(ax)x \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\int_{0}^{b} \\exp(-ax)x \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\leq \\exp\\left(a^2 - 2ab\\right) \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + 2a(x - b) + a^2\\right) dx \\\\\n& \\leq \\exp\\left(a^2 - 2ab\\right) \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + a\\right) + \\exp\\left(-\\frac{(x + b - 2a)^2}{2}\\right) dx\n\\end{align*}\n$$\n\nNow, for the second term, we have\n\n$$\n\\begin{align*}\n& \\leq \\exp\\left(-\\frac{b^2}{2}\\right) + |a - b| \\cdot \\exp\\left(a^2 - 2ab\\right) \\\\\nE_{x \\sim N(b,1)}[\\tanh'(ax)(x^2 - bx)] & = \\int_{-\\infty}^{\\infty} \\tanh'(ax)x(x - b) \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\geq \\int_{-b}^{\\infty} 0 \\leq \\int_{0}^{b} xe^{-ax} \\exp\\left(-\\frac{(x - 2b)^2}{2}\\right) dx \\\\\n& \\geq -b \\exp\\left(a^2 - 2ab\\right) - \\int_{0}^{\\infty} x \\exp\\left(-\\frac{(x - b)^2}{2} + a\\right) + \\exp\\left(-\\frac{(x + b - 2a)^2}{2}\\right) dx \\\\\n& \\geq -b \\exp\\left(-\\frac{b^2}{2}\\right) - |a - b| \\cdot \\exp\\left(a^2 - 2ab\\right)\n\\end{align*}\n$$\n\nWe can rewrite the last term as $$E_{x \\sim N(0,1)}[\\tanh(a(x + b))x]$$. Using the fact that $$\\tanh(a(x + b)) > \\tanh(a(-x + b))$$, we get that $$E_{x \\sim N(0,1)}[\\tanh(a(x + b))x] > 0$$. Finally, using the upper bound on $$E[\\tanh'(ax)]$$, we get the following lower bound.\n\n$$\n\\begin{align*}\n& (1 - \\eta - \\eta E_{x \\sim N(b,1)}[\\tanh'(ax)])a + \\eta E_{x \\sim N(b,1)}\\left[\\frac{-1}{2}\\tanh''(ax)a^2(x - b) + \\tanh'(ax)(x^2 - bx)\\right] \\\\\n& \\geq a^2 - 2ab \\geq a\\left(\\frac{1}{20}(19 - 4e^{20})\\right) + 1 - a^2 \\exp\\left(-\\frac{b^2}{2}\\right) + |a - b| \\exp\\left(a^2 - 2ab\\right) \\geq 1 .\n\\end{align*}\n$$\n\nLemma F.4. For any $$a, b > 0$$ and $$a \\in [30, 4b/3]$$, the following holds. Define $$U(a, b) \\triangleq \\eta E_{x \\sim N(b,1)}\\left[\\tanh(ax) - \\frac{1}{2}\\tanh''(ax)a^2 + \\tanh'(ax)ax\\right] - \\eta a$$.\n\nWhen the learning rate $$\\eta = \\frac{1}{20}$$, is given by, we have\n\n$$U(a, b) \\leq a + \\frac{b}{10}$$"}]}, {"page": 47, "text": "Proof. We upper bound each term in U(a, b) and they apply triangle inequality to get the result.\nWe start with |Ex\u223cN (b,1)                tanh\u2032\u2032(ax)a2x        \u221e  |:\n\u2212Ex\u223cN (b,1)         tanh\u2032\u2032(ax)a2x           =    8\u221aa22\u03c0     0     x\u03c3(2ax)(1 \u2212         \u03c3(2ax))(2\u03c3(2ax) \u2212              1)    e\u2212(x\u2212b)22     + e\u2212(x+b)2 2  dx\n                                            \u2264    4\u221aa22\u03c0     0 \u221e   xe\u22122axe\u2212(x\u2212b)2    2    dx\n                                            \u2264    4\u221aa22\u03c0     0 \u221e   e\u2212axxe\u2212(x\u2212b)2   2    dx\n                                            \u2264    a2      2 + a2                           2\n                                                 2 e\u2212b2          2 |b \u2212    a| e\u2212\u22122a(b\u2212a)\u2212a2\n                   E  x\u223cN (b,1)[tanh\u2032(ax)ax2] =               \u221a 1        \u221e   tanh\u2032(ax)ax2                   2     + e\u2212(x+b)2 2\n                                                                 2\u03c0     0                            e\u2212(x\u2212b)2                        dx\n                                                         \u2264   a    0\u221e   e\u2212axx2e\u2212(x\u2212b)2    2    dx\n                                                                 a2 \u22122ab      \u221e\n                                                                     2            x2           2\n                                                         \u2264   ae              0 a2 \u22122abe\u2212(x\u2212b+a)2     dx\n                                                         \u2264   2a(a \u2212      b)2e      2\n                     \u2212Ex\u223cN (b,1)[a tanh\u2032(ax)] = \u2212                 \u221a a2\u03c0     0\u221e   tanh\u2032(ax)         e\u2212(x\u2212b)22     + e\u2212(x+b)2 2       dx\n                                                           \u2265   \u2212a     0\u221e   e\u2212axe\u2212(x\u2212b)2   2    dx\n                                                                     a2 \u22122ab      \u221e\n                                                                         2                     2\n                                                           \u2265   \u2212ae     a2 \u22122ab   0    e\u2212(x\u2212b+a)2      dx\n                                                           \u2265   \u22124ae        2     .\nNow, using the fact that tanh\u2032(x) and \u2212                         tanh\u2032\u2032(x)x are always positive, we have the following upper\nbound.\n          U(a, b)      \u2264   \u03b7   Ex\u223cN (b,1)        tanh(ax) \u2212         1                                               \u00b7 x\n                                                                    2 tanh\u2032\u2032(ax)a2 + tanh\u2032(ax)ax\n                                      + \u03b7|a| + \u03b7        \u2212   Ex\u223cN (b,I)      tanh\u2032(ax)a\n                                                                              \u22122a(b\u2212a)\u2212a2                            a2\u22122ab             a2\u22122ab\n                       \u2264   \u03b7   2b + a + a2     2 e\u2212b2  2 + a2  2 |b \u2212    a| e        2         + 2a(b \u2212       a)2e      2      + 2ae       2\nIf b \u2265    a and a \u2265        30, then we have\nIf b \u2264    a \u2264     4b                                      U(a, b)      \u2264   \u03b7 (2b + a + 0.1)\n                  3 and a \u2265         30, then\nUsing \u03b7 = 1/20 and for any a > 30, we have                U(a, b)      \u2264   \u03b7 (2b + a + 0.1)\n                                                                 U(a, b)      \u2264    a + b\n                                                                                     10 .\n                                                                             47", "md": "# Math Equations\n\nProof. We upper bound each term in U(a, b) and then apply triangle inequality to get the result.\n\nWe start with $$|E_{x\\sim N(b,1)} \\tanh''(ax)a^2x|:$$\n\n$$-E_{x\\sim N(b,1)} \\tanh''(ax)a^2x = 8\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} x e^{-2ax}e^{-(x-b)^2/2} dx$$\n\n$$\\leq 4\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} xe^{-2ax}e^{-(x-b)^2/2} dx$$\n\n$$\\leq 4\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} e^{-ax}xe^{-(x-b)^2/2} dx$$\n\n$$\\leq \\frac{a^2}{2} + \\frac{a^2}{2} e^{-b^2/2} |b-a| e^{-2a(b-a)-a^2}$$\n\n$$E_{x\\sim N(b,1)}[ \\tanh'(ax)ax^2] = \\frac{\\sqrt{1}}{2\\pi} \\int_{0}^{\\infty} \\tanh'(ax)ax^2 e^{-(x-b)^2/2} dx$$\n\n$$\\leq a \\int_{0}^{\\infty} e^{-ax}x^2e^{-(x-b)^2/2} dx$$\n\n$$\\leq \\frac{a^2 - 2ab}{2} \\int_{0}^{\\infty} x^2 e^{-(x-b+a)^2} dx$$\n\n$$\\leq 2a(a - b)^2e^2$$\n\n$$-E_{x\\sim N(b,1)}[a \\tanh'(ax)] = -\\frac{a}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\tanh'(ax) e^{-(x-b)^2/2} dx$$\n\n$$\\geq -a \\int_{0}^{\\infty} e^{-ax}e^{-(x-b)^2/2} dx$$\n\n$$\\geq -ae^{a^2 - 2ab} \\int_{0} e^{-(x-b+a)^2} dx$$\n\n$$\\geq -4ae^2$$\n\nNow, using the fact that $$\\tanh'(x)$$ and $$-\\tanh''(x)x$$ are always positive, we have the following upper bound.\n\n$$U(a, b) \\leq \\eta E_{x\\sim N(b,1)} [\\tanh(ax) - 1] \\cdot x / 2 \\tanh''(ax)a^2 + \\tanh'(ax)ax$$\n\n$$+ \\eta|a| + \\eta - E_{x\\sim N(b,I)} [\\tanh'(ax)a - 2a(b-a)-a^2] / a^2-2ab \\leq \\eta (2b + a + a^2/2 e^{-b^2/2} + a^2/2 |b-a| e^2 + 2a(b - a)^2e^2 + 2ae^2$$\n\nIf $$b \\geq a$$ and $$a \\geq 30$$, then we have\n\nIf $$b \\leq a \\leq 4b/3$$ and $$a \\geq 30$$, then\n\nUsing $$\\eta = 1/20$$ and for any $$a > 30$$, we have\n\n$$U(a, b) \\leq \\eta (2b + a + 0.1)$$\n\n$$U(a, b) \\leq \\eta (2b + a + 0.1)$$\n\n$$U(a, b) \\leq a + b / 10$$\n\n$$47$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Proof. We upper bound each term in U(a, b) and then apply triangle inequality to get the result.\n\nWe start with $$|E_{x\\sim N(b,1)} \\tanh''(ax)a^2x|:$$\n\n$$-E_{x\\sim N(b,1)} \\tanh''(ax)a^2x = 8\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} x e^{-2ax}e^{-(x-b)^2/2} dx$$\n\n$$\\leq 4\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} xe^{-2ax}e^{-(x-b)^2/2} dx$$\n\n$$\\leq 4\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} e^{-ax}xe^{-(x-b)^2/2} dx$$\n\n$$\\leq \\frac{a^2}{2} + \\frac{a^2}{2} e^{-b^2/2} |b-a| e^{-2a(b-a)-a^2}$$\n\n$$E_{x\\sim N(b,1)}[ \\tanh'(ax)ax^2] = \\frac{\\sqrt{1}}{2\\pi} \\int_{0}^{\\infty} \\tanh'(ax)ax^2 e^{-(x-b)^2/2} dx$$\n\n$$\\leq a \\int_{0}^{\\infty} e^{-ax}x^2e^{-(x-b)^2/2} dx$$\n\n$$\\leq \\frac{a^2 - 2ab}{2} \\int_{0}^{\\infty} x^2 e^{-(x-b+a)^2} dx$$\n\n$$\\leq 2a(a - b)^2e^2$$\n\n$$-E_{x\\sim N(b,1)}[a \\tanh'(ax)] = -\\frac{a}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\tanh'(ax) e^{-(x-b)^2/2} dx$$\n\n$$\\geq -a \\int_{0}^{\\infty} e^{-ax}e^{-(x-b)^2/2} dx$$\n\n$$\\geq -ae^{a^2 - 2ab} \\int_{0} e^{-(x-b+a)^2} dx$$\n\n$$\\geq -4ae^2$$\n\nNow, using the fact that $$\\tanh'(x)$$ and $$-\\tanh''(x)x$$ are always positive, we have the following upper bound.\n\n$$U(a, b) \\leq \\eta E_{x\\sim N(b,1)} [\\tanh(ax) - 1] \\cdot x / 2 \\tanh''(ax)a^2 + \\tanh'(ax)ax$$\n\n$$+ \\eta|a| + \\eta - E_{x\\sim N(b,I)} [\\tanh'(ax)a - 2a(b-a)-a^2] / a^2-2ab \\leq \\eta (2b + a + a^2/2 e^{-b^2/2} + a^2/2 |b-a| e^2 + 2a(b - a)^2e^2 + 2ae^2$$\n\nIf $$b \\geq a$$ and $$a \\geq 30$$, then we have\n\nIf $$b \\leq a \\leq 4b/3$$ and $$a \\geq 30$$, then\n\nUsing $$\\eta = 1/20$$ and for any $$a > 30$$, we have\n\n$$U(a, b) \\leq \\eta (2b + a + 0.1)$$\n\n$$U(a, b) \\leq \\eta (2b + a + 0.1)$$\n\n$$U(a, b) \\leq a + b / 10$$\n\n$$47$$", "md": "Proof. We upper bound each term in U(a, b) and then apply triangle inequality to get the result.\n\nWe start with $$|E_{x\\sim N(b,1)} \\tanh''(ax)a^2x|:$$\n\n$$-E_{x\\sim N(b,1)} \\tanh''(ax)a^2x = 8\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} x e^{-2ax}e^{-(x-b)^2/2} dx$$\n\n$$\\leq 4\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} xe^{-2ax}e^{-(x-b)^2/2} dx$$\n\n$$\\leq 4\\sqrt{a^2} \\frac{2}{\\pi} \\int_{0}^{\\infty} e^{-ax}xe^{-(x-b)^2/2} dx$$\n\n$$\\leq \\frac{a^2}{2} + \\frac{a^2}{2} e^{-b^2/2} |b-a| e^{-2a(b-a)-a^2}$$\n\n$$E_{x\\sim N(b,1)}[ \\tanh'(ax)ax^2] = \\frac{\\sqrt{1}}{2\\pi} \\int_{0}^{\\infty} \\tanh'(ax)ax^2 e^{-(x-b)^2/2} dx$$\n\n$$\\leq a \\int_{0}^{\\infty} e^{-ax}x^2e^{-(x-b)^2/2} dx$$\n\n$$\\leq \\frac{a^2 - 2ab}{2} \\int_{0}^{\\infty} x^2 e^{-(x-b+a)^2} dx$$\n\n$$\\leq 2a(a - b)^2e^2$$\n\n$$-E_{x\\sim N(b,1)}[a \\tanh'(ax)] = -\\frac{a}{\\sqrt{2\\pi}} \\int_{0}^{\\infty} \\tanh'(ax) e^{-(x-b)^2/2} dx$$\n\n$$\\geq -a \\int_{0}^{\\infty} e^{-ax}e^{-(x-b)^2/2} dx$$\n\n$$\\geq -ae^{a^2 - 2ab} \\int_{0} e^{-(x-b+a)^2} dx$$\n\n$$\\geq -4ae^2$$\n\nNow, using the fact that $$\\tanh'(x)$$ and $$-\\tanh''(x)x$$ are always positive, we have the following upper bound.\n\n$$U(a, b) \\leq \\eta E_{x\\sim N(b,1)} [\\tanh(ax) - 1] \\cdot x / 2 \\tanh''(ax)a^2 + \\tanh'(ax)ax$$\n\n$$+ \\eta|a| + \\eta - E_{x\\sim N(b,I)} [\\tanh'(ax)a - 2a(b-a)-a^2] / a^2-2ab \\leq \\eta (2b + a + a^2/2 e^{-b^2/2} + a^2/2 |b-a| e^2 + 2a(b - a)^2e^2 + 2ae^2$$\n\nIf $$b \\geq a$$ and $$a \\geq 30$$, then we have\n\nIf $$b \\leq a \\leq 4b/3$$ and $$a \\geq 30$$, then\n\nUsing $$\\eta = 1/20$$ and for any $$a > 30$$, we have\n\n$$U(a, b) \\leq \\eta (2b + a + 0.1)$$\n\n$$U(a, b) \\leq \\eta (2b + a + 0.1)$$\n\n$$U(a, b) \\leq a + b / 10$$\n\n$$47$$"}]}, {"page": 48, "text": " F.4    Additional proofs for mixtures of two Gaussians\n Lemma F.5. Suppose a, b > 0 satisfy a \u2208         [30, 4b\n                                                      3 ], then the following inequality holds:\n                           |Ex\u223cN (b,1)[\u22120.5 tanh\u2032\u2032(ax)a2 + tanh\u2032(ax)ax]| \u2264      0.01\n Proof. We fi rst show that E   x\u223cN (b,1)[\u22120.5 tanh\u2032\u2032(ax)a2] > 0 for any a, b > 0.\n    E x\u223cN (b,1)[\u22120.5 tanh\u2032\u2032(ax)a2] = \u22120.5a2       \u221e  tanh\u2032\u2032(ax) exp(\u22120.5(x \u2212    b)2)dx\n                                         \u221e      \u2212\u221e\n                           = \u22120.5a2     0  tanh\u2032\u2032(ax)(exp(\u22120.5(x \u2212     b)2) \u2212  exp(\u22120.5(x + b)2))dx > 0\nwhere the last inequality follows from exp(\u22120.5(x \u2212         b)2) > exp(\u22120.5(x + b)2) and tanh\u2032\u2032(ax) < 0\n for x > 0. We can upper bound E       x\u223cN (b,1)[\u22120.5 tanh\u2032\u2032(ax)a2] as follows:\n            E x\u223cN (b,1)[\u221212 tanh\u2032\u2032(ax)a2] \u2264   \u221212a2   0\u221e  tanh\u2032\u2032(ax) exp(\u22121 2(x \u2212   b)2)dx\n                                           \u2264  a2  0\u221e  exp(\u2212ax) exp(\u22121   2(x \u2212  b)2)dx\n                                           \u2264  a2 exp(12(a2 \u2212  2ab))   0\u221e exp(\u22121  2(x \u2212  b + a)2)dx\n                                           \u2264  a2 exp(12(a2 \u2212  2ab))\nWhen a \u2264     b, by writing a2 \u2212    2ab = \u22122a(b \u2212    a) \u2212  a2 \u2264  \u2212a2, we have E[\u22121    2 tanh\u2032\u2032(ax)a2] \u2264   0.005\n for a \u2265  30. When a \u2208    [b, 4b\n                              3 ], a2 \u2212 2ab =\u2264   \u22122b2\n the E x\u223cN (b,1)[\u22121                                 9 , we have |E[\u22121   2 tanh\u2032\u2032(ax)a2]| \u2264  0.005. Similar to\n                  2 tanh\u2032\u2032(ax)a2], we prove Ex\u223cN (b,1)[tanh\u2032(ax)ax] > 0 and Ex\u223cN (b,1)[tanh\u2032(ax)ax] <\n 0.005. Combining bounds for |E[tanh\u2032(ax)ax]| and |E[\u22121        2 tanh\u2032\u2032(ax)a2]| using triangle inequality, we\n obtain the result.\n                                                       48", "md": "# Additional Proofs for Mixtures of Two Gaussians\n\n## Additional proofs for mixtures of two Gaussians\n\nLemma F.5. Suppose \\(a, b > 0\\) satisfy \\(a \\in [30, \\frac{4b}{3}]\\), then the following inequality holds:\n\n$$\n\\left| E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2 + \\tanh'(ax)ax\\right] \\right| \\leq 0.01\n$$\n\nProof. We first show that \\(E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right] > 0\\) for any \\(a, b > 0\\).\n\n$$\n\\begin{align*}\nE_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right] & = -0.5a^2 \\int_{-\\infty}^{\\infty} \\tanh''(ax) \\exp\\left(-0.5(x - b)^2\\right)dx \\\\\n& = -0.5a^2 \\int_{0}^{\\infty} \\tanh''(ax)\\left(\\exp\\left(-0.5(x - b)^2\\right) - \\exp\\left(-0.5(x + b)^2\\right)\\right)dx > 0\n\\end{align*}\n$$\n\nWhere the last inequality follows from \\(\\exp\\left(-0.5(x - b)^2\\right) > \\exp\\left(-0.5(x + b)^2\\right)\\) and \\(\\tanh''(ax) < 0\\) for \\(x > 0\\). We can upper bound \\(E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right]\\) as follows:\n\n$$\n\\begin{align*}\nE_{x \\sim N(b,1)}\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right] & \\leq -\\frac{1}{2}a^2 \\int_{0}^{\\infty} \\tanh''(ax) \\exp\\left(-\\frac{1}{2}(x - b)^2\\right)dx \\\\\n& \\leq a^2 \\int_{0}^{\\infty} \\exp(-ax) \\exp\\left(-\\frac{1}{2}(x - b)^2\\right)dx \\\\\n& \\leq a^2 \\exp\\left(\\frac{1}{2}(a^2 - 2ab)\\right) \\int_{0}^{\\infty} \\exp\\left(-\\frac{1}{2}(x - b + a)^2\\right)dx \\\\\n& \\leq a^2 \\exp\\left(\\frac{1}{2}(a^2 - 2ab)\\right)\n\\end{align*}\n$$\n\nWhen \\(a \\leq b\\), by writing \\(a^2 - 2ab = -2a(b - a) - a^2 \\leq -a^2\\), we have \\(E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right] \\leq 0.005\\) for \\(a \\geq 30\\). When \\(a \\in [b, \\frac{4b}{3}]\\), \\(a^2 - 2ab \\leq -\\frac{2b^2}{9}\\), we have \\(\\left|E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right| \\leq 0.005\\). Similar to \\(\\left|E\\left[\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right|\\), we prove \\(E_{x \\sim N(b,1)}[\\tanh'(ax)ax] > 0\\) and \\(E_{x \\sim N(b,1)}[\\tanh'(ax)ax] < 0.005\\). Combining bounds for \\(\\left|E[\\tanh'(ax)ax]\\right|\\) and \\(\\left|E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right|\\) using triangle inequality, we obtain the result.\n\n48", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Proofs for Mixtures of Two Gaussians", "md": "# Additional Proofs for Mixtures of Two Gaussians"}, {"type": "heading", "lvl": 2, "value": "Additional proofs for mixtures of two Gaussians", "md": "## Additional proofs for mixtures of two Gaussians"}, {"type": "text", "value": "Lemma F.5. Suppose \\(a, b > 0\\) satisfy \\(a \\in [30, \\frac{4b}{3}]\\), then the following inequality holds:\n\n$$\n\\left| E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2 + \\tanh'(ax)ax\\right] \\right| \\leq 0.01\n$$\n\nProof. We first show that \\(E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right] > 0\\) for any \\(a, b > 0\\).\n\n$$\n\\begin{align*}\nE_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right] & = -0.5a^2 \\int_{-\\infty}^{\\infty} \\tanh''(ax) \\exp\\left(-0.5(x - b)^2\\right)dx \\\\\n& = -0.5a^2 \\int_{0}^{\\infty} \\tanh''(ax)\\left(\\exp\\left(-0.5(x - b)^2\\right) - \\exp\\left(-0.5(x + b)^2\\right)\\right)dx > 0\n\\end{align*}\n$$\n\nWhere the last inequality follows from \\(\\exp\\left(-0.5(x - b)^2\\right) > \\exp\\left(-0.5(x + b)^2\\right)\\) and \\(\\tanh''(ax) < 0\\) for \\(x > 0\\). We can upper bound \\(E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right]\\) as follows:\n\n$$\n\\begin{align*}\nE_{x \\sim N(b,1)}\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right] & \\leq -\\frac{1}{2}a^2 \\int_{0}^{\\infty} \\tanh''(ax) \\exp\\left(-\\frac{1}{2}(x - b)^2\\right)dx \\\\\n& \\leq a^2 \\int_{0}^{\\infty} \\exp(-ax) \\exp\\left(-\\frac{1}{2}(x - b)^2\\right)dx \\\\\n& \\leq a^2 \\exp\\left(\\frac{1}{2}(a^2 - 2ab)\\right) \\int_{0}^{\\infty} \\exp\\left(-\\frac{1}{2}(x - b + a)^2\\right)dx \\\\\n& \\leq a^2 \\exp\\left(\\frac{1}{2}(a^2 - 2ab)\\right)\n\\end{align*}\n$$\n\nWhen \\(a \\leq b\\), by writing \\(a^2 - 2ab = -2a(b - a) - a^2 \\leq -a^2\\), we have \\(E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right] \\leq 0.005\\) for \\(a \\geq 30\\). When \\(a \\in [b, \\frac{4b}{3}]\\), \\(a^2 - 2ab \\leq -\\frac{2b^2}{9}\\), we have \\(\\left|E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right| \\leq 0.005\\). Similar to \\(\\left|E\\left[\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right|\\), we prove \\(E_{x \\sim N(b,1)}[\\tanh'(ax)ax] > 0\\) and \\(E_{x \\sim N(b,1)}[\\tanh'(ax)ax] < 0.005\\). Combining bounds for \\(\\left|E[\\tanh'(ax)ax]\\right|\\) and \\(\\left|E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right|\\) using triangle inequality, we obtain the result.\n\n48", "md": "Lemma F.5. Suppose \\(a, b > 0\\) satisfy \\(a \\in [30, \\frac{4b}{3}]\\), then the following inequality holds:\n\n$$\n\\left| E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2 + \\tanh'(ax)ax\\right] \\right| \\leq 0.01\n$$\n\nProof. We first show that \\(E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right] > 0\\) for any \\(a, b > 0\\).\n\n$$\n\\begin{align*}\nE_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right] & = -0.5a^2 \\int_{-\\infty}^{\\infty} \\tanh''(ax) \\exp\\left(-0.5(x - b)^2\\right)dx \\\\\n& = -0.5a^2 \\int_{0}^{\\infty} \\tanh''(ax)\\left(\\exp\\left(-0.5(x - b)^2\\right) - \\exp\\left(-0.5(x + b)^2\\right)\\right)dx > 0\n\\end{align*}\n$$\n\nWhere the last inequality follows from \\(\\exp\\left(-0.5(x - b)^2\\right) > \\exp\\left(-0.5(x + b)^2\\right)\\) and \\(\\tanh''(ax) < 0\\) for \\(x > 0\\). We can upper bound \\(E_{x \\sim N(b,1)}\\left[-0.5 \\tanh''(ax)a^2\\right]\\) as follows:\n\n$$\n\\begin{align*}\nE_{x \\sim N(b,1)}\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right] & \\leq -\\frac{1}{2}a^2 \\int_{0}^{\\infty} \\tanh''(ax) \\exp\\left(-\\frac{1}{2}(x - b)^2\\right)dx \\\\\n& \\leq a^2 \\int_{0}^{\\infty} \\exp(-ax) \\exp\\left(-\\frac{1}{2}(x - b)^2\\right)dx \\\\\n& \\leq a^2 \\exp\\left(\\frac{1}{2}(a^2 - 2ab)\\right) \\int_{0}^{\\infty} \\exp\\left(-\\frac{1}{2}(x - b + a)^2\\right)dx \\\\\n& \\leq a^2 \\exp\\left(\\frac{1}{2}(a^2 - 2ab)\\right)\n\\end{align*}\n$$\n\nWhen \\(a \\leq b\\), by writing \\(a^2 - 2ab = -2a(b - a) - a^2 \\leq -a^2\\), we have \\(E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right] \\leq 0.005\\) for \\(a \\geq 30\\). When \\(a \\in [b, \\frac{4b}{3}]\\), \\(a^2 - 2ab \\leq -\\frac{2b^2}{9}\\), we have \\(\\left|E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right| \\leq 0.005\\). Similar to \\(\\left|E\\left[\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right|\\), we prove \\(E_{x \\sim N(b,1)}[\\tanh'(ax)ax] > 0\\) and \\(E_{x \\sim N(b,1)}[\\tanh'(ax)ax] < 0.005\\). Combining bounds for \\(\\left|E[\\tanh'(ax)ax]\\right|\\) and \\(\\left|E\\left[-\\frac{1}{2} \\tanh''(ax)a^2\\right]\\right|\\) using triangle inequality, we obtain the result.\n\n48"}]}], "job_id": "4b5ca682-7f65-4a9e-adf8-3fbd1990485c", "file_path": "./corpus/2307.01178.pdf"}