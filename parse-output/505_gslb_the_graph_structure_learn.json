{"pages": [{"page": 1, "text": "   GSLB: The Graph Structure Learning Benchmark\n     Zhixun Li1      Liang Wang2,3       Xin Sun4     Yifan Luo5      Yanqiao Zhu6       Dingshuo Chen2,3\n  Yingtao Luo7      Xiangxin Zhou2,3       Qiang Liu2,3\u2217    Shu Wu2,3      Liang Wang2,3,4      Jeffrey Xu Yu1\u2217\n                        1 Department of Systems Engineering and Engineering Management\n                           2           The Chinese University of Hong Kong\n                            Center for Research on Intelligent Perception and Computing\n                        State Key Laboratory of Multimodal Artifi cial Intelligence Systems\n                              Institute of Automation, Chinese Academy of Sciences\n                   34School of Artificial Intelligence, University of Chinese Academy of Sciences\n               5     Department of Automation, University of Science and Technology of China\n                School of Cyberspace Security, Beijing University of Posts and Telecommunications\n             7       6Department of Computer Science, University of California, Los Angeles\n              Heinz College of Information Systems and Public Policy, Machine Learning Department\n                             School of Computer Science, Carnegie Mellon University\n                                    Primary contact: zxli@se.cuhk.edu.hk\n                                                   Abstract\n          Graph Structure Learning (GSL) has recently garnered considerable attention due\n          to its ability to optimize both the parameters of Graph Neural Networks (GNNs)\n          and the computation graph structure simultaneously. Despite the proliferation of\n          GSL methods developed in recent years, there is no standard experimental setting\n          or fair comparison for performance evaluation, which creates a great obstacle to\n          understanding the progress in this field. To fill this gap, we systematically analyze\n          the performance of GSL in different scenarios and develop a comprehensive Graph\n          Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets\n          and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the\n          characteristics of GSL in terms of three dimensions: effectiveness, robustness, and\n          complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node-\n          and graph-level tasks, and analyze their performance in robust learning and model\n          complexity. Further, to facilitate reproducible research, we have developed an\n          easy-to-use library for training, evaluating, and visualizing different GSL methods.\n          Empirical results of our extensive experiments demonstrate the ability of GSL\n          and reveal its potential benefits on various downstream tasks, offering insights\n          and opportunities for future research. The code of GSLB is available at: https:\n          //github.com/GSL-Benchmark/GSLB.\n1    Introduction\nGraphs, structures made of vertices and edges, are ubiquitous in real-world applications. A wide\nvariety of applications spanning social network [51, 9], molecular property prediction [40, 14], fake\nnews detection [45, 1], and fraud detection [23, 27] have found graphs instrumental in modeling\ncomplex systems. In recent years, Graph Neural Networks (GNNs) have attracted increasing attention\ndue to their powerful ability to learn node or graph representations. However, most of the GNNs\nheavily rely on the assumption that the initial structure of the graph is trustworthy enough to serve as\nground-truth for training. Due to uncertainty and complexity in data collection, graph structures are\ninevitably redundant, biased, noisy, incomplete, or the original graph structures are even unavailable,\nwhich will bring great challenges for the deployment of GNNs in real-world applications.\n   \u2217Corresponding authors: Qiang Liu (qiang.liu@nlpr.ia.ac.cn), Jeffrey Xu Yu (yu@se.cuhk.edu.hk)\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "md": "# GSLB: The Graph Structure Learning Benchmark\n\n# GSLB: The Graph Structure Learning Benchmark\n\nZhixun Li1, Liang Wang2,3, Xin Sun4, Yifan Luo5, Yanqiao Zhu6, Dingshuo Chen2,3, Yingtao Luo7, Xiangxin Zhou2,3, Qiang Liu2,3\u2217, Shu Wu2,3, Liang Wang2,3,4, Jeffrey Xu Yu1\u2217\n\nAffiliations:\n\n1 Department of Systems Engineering and Engineering Management\n\n2 The Chinese University of Hong Kong, Center for Research on Intelligent Perception and Computing\n\n3 State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences\n\n4 School of Artificial Intelligence, University of Chinese Academy of Sciences\n\n5 Department of Automation, University of Science and Technology of China\n\n6 School of Cyberspace Security, Beijing University of Posts and Telecommunications\n\n7 Department of Computer Science, University of California, Los Angeles\n\nHeinz College of Information Systems and Public Policy, Machine Learning Department, School of Computer Science, Carnegie Mellon University\n\nPrimary contact: zxli@se.cuhk.edu.hk\n\n## Abstract\n\nGraph Structure Learning (GSL) has recently garnered considerable attention due to its ability to optimize both the parameters of Graph Neural Networks (GNNs) and the computation graph structure simultaneously. Despite the proliferation of GSL methods developed in recent years, there is no standard experimental setting or fair comparison for performance evaluation, which creates a great obstacle to understanding the progress in this field. To fill this gap, we systematically analyze the performance of GSL in different scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node- and graph-level tasks, and analyze their performance in robust learning and model complexity. Further, to facilitate reproducible research, we have developed an easy-to-use library for training, evaluating, and visualizing different GSL methods. Empirical results of our extensive experiments demonstrate the ability of GSL and reveal its potential benefits on various downstream tasks, offering insights and opportunities for future research. The code of GSLB is available at: https://github.com/GSL-Benchmark/GSLB.\n\n## Introduction\n\nGraphs, structures made of vertices and edges, are ubiquitous in real-world applications. A wide variety of applications spanning social network [51, 9], molecular property prediction [40, 14], fake news detection [45, 1], and fraud detection [23, 27] have found graphs instrumental in modeling complex systems. In recent years, Graph Neural Networks (GNNs) have attracted increasing attention due to their powerful ability to learn node or graph representations. However, most of the GNNs heavily rely on the assumption that the initial structure of the graph is trustworthy enough to serve as ground-truth for training. Due to uncertainty and complexity in data collection, graph structures are inevitably redundant, biased, noisy, incomplete, or the original graph structures are even unavailable, which will bring great challenges for the deployment of GNNs in real-world applications.\n\nCorresponding authors: Qiang Liu (qiang.liu@nlpr.ia.ac.cn), Jeffrey Xu Yu (yu@se.cuhk.edu.hk)\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "GSLB: The Graph Structure Learning Benchmark", "md": "# GSLB: The Graph Structure Learning Benchmark"}, {"type": "heading", "lvl": 1, "value": "GSLB: The Graph Structure Learning Benchmark", "md": "# GSLB: The Graph Structure Learning Benchmark"}, {"type": "text", "value": "Zhixun Li1, Liang Wang2,3, Xin Sun4, Yifan Luo5, Yanqiao Zhu6, Dingshuo Chen2,3, Yingtao Luo7, Xiangxin Zhou2,3, Qiang Liu2,3\u2217, Shu Wu2,3, Liang Wang2,3,4, Jeffrey Xu Yu1\u2217\n\nAffiliations:\n\n1 Department of Systems Engineering and Engineering Management\n\n2 The Chinese University of Hong Kong, Center for Research on Intelligent Perception and Computing\n\n3 State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences\n\n4 School of Artificial Intelligence, University of Chinese Academy of Sciences\n\n5 Department of Automation, University of Science and Technology of China\n\n6 School of Cyberspace Security, Beijing University of Posts and Telecommunications\n\n7 Department of Computer Science, University of California, Los Angeles\n\nHeinz College of Information Systems and Public Policy, Machine Learning Department, School of Computer Science, Carnegie Mellon University\n\nPrimary contact: zxli@se.cuhk.edu.hk", "md": "Zhixun Li1, Liang Wang2,3, Xin Sun4, Yifan Luo5, Yanqiao Zhu6, Dingshuo Chen2,3, Yingtao Luo7, Xiangxin Zhou2,3, Qiang Liu2,3\u2217, Shu Wu2,3, Liang Wang2,3,4, Jeffrey Xu Yu1\u2217\n\nAffiliations:\n\n1 Department of Systems Engineering and Engineering Management\n\n2 The Chinese University of Hong Kong, Center for Research on Intelligent Perception and Computing\n\n3 State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences\n\n4 School of Artificial Intelligence, University of Chinese Academy of Sciences\n\n5 Department of Automation, University of Science and Technology of China\n\n6 School of Cyberspace Security, Beijing University of Posts and Telecommunications\n\n7 Department of Computer Science, University of California, Los Angeles\n\nHeinz College of Information Systems and Public Policy, Machine Learning Department, School of Computer Science, Carnegie Mellon University\n\nPrimary contact: zxli@se.cuhk.edu.hk"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Graph Structure Learning (GSL) has recently garnered considerable attention due to its ability to optimize both the parameters of Graph Neural Networks (GNNs) and the computation graph structure simultaneously. Despite the proliferation of GSL methods developed in recent years, there is no standard experimental setting or fair comparison for performance evaluation, which creates a great obstacle to understanding the progress in this field. To fill this gap, we systematically analyze the performance of GSL in different scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node- and graph-level tasks, and analyze their performance in robust learning and model complexity. Further, to facilitate reproducible research, we have developed an easy-to-use library for training, evaluating, and visualizing different GSL methods. Empirical results of our extensive experiments demonstrate the ability of GSL and reveal its potential benefits on various downstream tasks, offering insights and opportunities for future research. The code of GSLB is available at: https://github.com/GSL-Benchmark/GSLB.", "md": "Graph Structure Learning (GSL) has recently garnered considerable attention due to its ability to optimize both the parameters of Graph Neural Networks (GNNs) and the computation graph structure simultaneously. Despite the proliferation of GSL methods developed in recent years, there is no standard experimental setting or fair comparison for performance evaluation, which creates a great obstacle to understanding the progress in this field. To fill this gap, we systematically analyze the performance of GSL in different scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node- and graph-level tasks, and analyze their performance in robust learning and model complexity. Further, to facilitate reproducible research, we have developed an easy-to-use library for training, evaluating, and visualizing different GSL methods. Empirical results of our extensive experiments demonstrate the ability of GSL and reveal its potential benefits on various downstream tasks, offering insights and opportunities for future research. The code of GSLB is available at: https://github.com/GSL-Benchmark/GSLB."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Graphs, structures made of vertices and edges, are ubiquitous in real-world applications. A wide variety of applications spanning social network [51, 9], molecular property prediction [40, 14], fake news detection [45, 1], and fraud detection [23, 27] have found graphs instrumental in modeling complex systems. In recent years, Graph Neural Networks (GNNs) have attracted increasing attention due to their powerful ability to learn node or graph representations. However, most of the GNNs heavily rely on the assumption that the initial structure of the graph is trustworthy enough to serve as ground-truth for training. Due to uncertainty and complexity in data collection, graph structures are inevitably redundant, biased, noisy, incomplete, or the original graph structures are even unavailable, which will bring great challenges for the deployment of GNNs in real-world applications.\n\nCorresponding authors: Qiang Liu (qiang.liu@nlpr.ia.ac.cn), Jeffrey Xu Yu (yu@se.cuhk.edu.hk)\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.", "md": "Graphs, structures made of vertices and edges, are ubiquitous in real-world applications. A wide variety of applications spanning social network [51, 9], molecular property prediction [40, 14], fake news detection [45, 1], and fraud detection [23, 27] have found graphs instrumental in modeling complex systems. In recent years, Graph Neural Networks (GNNs) have attracted increasing attention due to their powerful ability to learn node or graph representations. However, most of the GNNs heavily rely on the assumption that the initial structure of the graph is trustworthy enough to serve as ground-truth for training. Due to uncertainty and complexity in data collection, graph structures are inevitably redundant, biased, noisy, incomplete, or the original graph structures are even unavailable, which will bring great challenges for the deployment of GNNs in real-world applications.\n\nCorresponding authors: Qiang Liu (qiang.liu@nlpr.ia.ac.cn), Jeffrey Xu Yu (yu@se.cuhk.edu.hk)\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks."}]}, {"page": 2, "text": "Table 1: An overview of GSLB. Both algorithms and datasets are divided into three categories:\nhomogeneous node-level, heterogeneous node-level, and graph-level. The evaluation is divided into\nthree dimensions: effectiveness, robustness, and complexity.\n                                                   Algorithms\n Homogeneous GSL            LDS [11], GRCN [48], ProGNN [18], IDGL [4], CoGSL [26], SUBLIME [28],\n                            GEN [39], STABLE [21], NodeFormer [43], SLAPS [10], GSR [54], HES-GSL [42]\n Heterogeneous GSL          GTN [49], HGSL [53]\n Graph-level GSL            HGP-SL [52], VIB-GSL [36]\n                                                     Datasets\n Homogeneous datasets       Cora [47], Citeseer [47], Pubmed [47], ogbn-arxiv [15], Polblogs, Cornell [34],\n                            Texas [34], Wisconsin [34], Actor [37]\n Heterogeneous datasets     ACM [49], DBLP [49], Yelp [29]\n Graph-level datasets       IMDB-B [3], IMDB-M [3], COLLAB [46], REDDIT-B [46], MUTAG [5],\n                            PROTEINS [2], Peptides-Func [8], Peptides-Struct [8]\n                                                   Evaluations\n Effectiveness              Homogeneous node classification (Topology Refinement/Topology Inference),\n                            Heterogeneous node classification, Graph-level tasks\n Robustness                 Supervision signal robustness, Structure robustness, Feature robustness\n Complexity                 Time complexity, Space complexity\nTo mitigate the aforementioned problems, Graph Structure Learning (GSL) [4, 55, 30, 10, 57, 50] has\nbecome an important theme in graph learning. GSL aims to make the computation structure of GNNs\nmore suitable for downstream tasks and improve the quality of the learned representations. While it\nis widespread in different communities and the research enthusiasm for GSL is increasing, there is no\nstandardized benchmark that could offer a fair and consistent comparison of different GSL algorithms.\nMoreover, due to the complexity and diversity of graph datasets, the experimental setups in existing\nwork are not consistent, such as varying ratios of the training set and different train/validation/test\nsplits. This poses a great obstacle to a holistic understanding of the current research status. Therefore,\nthe development of a standardized and comprehensive benchmark for GSL is an urgent need within\nthe community.\nIn this work, we propose Graph Structure Learning Benchmark (GSLB), which serves as the first\ncomprehensive benchmark for GSL. Our benchmark encompasses 16 state-of-the-art GSL algorithms\nand 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-\nlevel tasks. We systematically investigate the characteristics of GSL in terms of three dimensions:\neffectiveness, robustness, and complexity. Based on these three dimensions, we conduct an\nextensive comparative study of existing GSL algorithms in different scenarios. For effectiveness,\nGSLB provides a fair and comprehensive comparison of existing algorithms on homogeneous node-\nlevel, heterogeneous node-level, and graph-level tasks, where we consider both homophilic and\nheterophilic graph datasets for homogeneous node-level tasks, and cover both Topology Refinement\n(TR, i.e., refining graphs from data with the original topology) and Topology Inference (TI, i.e.,\ninferring graphs from data without initial topology) settings. For robustness, GSLB evaluates GSL\nmodels under three types of noise: supervision signal noise, structure noise, and feature noise. We also\ncompare GSL algorithms with the models specifically designed to improve these types of robustness.\nFor complexity, GSLB conducts a detailed evaluation of representative GSL algorithms in terms of\ntime complexity and space complexity.\nThrough extensive experiments, we observe that: (1) GSL generally brings performance improvement\nfor node-level tasks, especially on heterophilic graphs; (2) on graph-level tasks, current GSL models\nbring limited improvement and their performance varies greatly across different datasets; (3) most\nGSL algorithms (especially unsupervised GSL algorithms) show impressive robustness; (4) GSL\nmodels require significant time and memory overhead, making them challenging to deploy on\nlarge-scale graphs. In summary, we make the following three contributions:\n                                                        2", "md": "# GSLB Overview\n\n## Table 1: An overview of GSLB\n\nBoth algorithms and datasets are divided into three categories: homogeneous node-level, heterogeneous node-level, and graph-level. The evaluation is divided into three dimensions: effectiveness, robustness, and complexity.\n\n### Algorithms\n\n|Homogeneous GSL|Heterogeneous GSL|Graph-level GSL|\n|---|---|---|\n|LDS [11], GRCN [48], ProGNN [18], IDGL [4], CoGSL [26], SUBLIME [28], GEN [39], STABLE [21], NodeFormer [43], SLAPS [10], GSR [54], HES-GSL [42]|GTN [49], HGSL [53]|HGP-SL [52], VIB-GSL [36]|\n\n### Datasets\n\n|Homogeneous datasets|Heterogeneous datasets|Graph-level datasets|\n|---|---|---|\n|Cora [47], Citeseer [47], Pubmed [47], ogbn-arxiv [15], Polblogs, Cornell [34], Texas [34], Wisconsin [34], Actor [37]|ACM [49], DBLP [49], Yelp [29]|IMDB-B [3], IMDB-M [3], COLLAB [46], REDDIT-B [46], MUTAG [5], PROTEINS [2], Peptides-Func [8], Peptides-Struct [8]|\n\n### Evaluations\n\n- Effectiveness: Homogeneous node classification (Topology Refinement/Topology Inference), Heterogeneous node classification, Graph-level tasks\n- Robustness: Supervision signal robustness, Structure robustness, Feature robustness\n- Complexity: Time complexity, Space complexity\n\nTo mitigate the aforementioned problems, Graph Structure Learning (GSL) [4, 55, 30, 10, 57, 50] has become an important theme in graph learning. GSL aims to make the computation structure of GNNs more suitable for downstream tasks and improve the quality of the learned representations. While it is widespread in different communities and the research enthusiasm for GSL is increasing, there is no standardized benchmark that could offer a fair and consistent comparison of different GSL algorithms. Moreover, due to the complexity and diversity of graph datasets, the experimental setups in existing work are not consistent, such as varying ratios of the training set and different train/validation/test splits. This poses a great obstacle to a holistic understanding of the current research status. Therefore, the development of a standardized and comprehensive benchmark for GSL is an urgent need within the community.\n\nIn this work, we propose Graph Structure Learning Benchmark (GSLB), which serves as the first comprehensive benchmark for GSL. Our benchmark encompasses 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. We systematically investigate the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. Based on these three dimensions, we conduct an extensive comparative study of existing GSL algorithms in different scenarios. For effectiveness, GSLB provides a fair and comprehensive comparison of existing algorithms on homogeneous node-level, heterogeneous node-level, and graph-level tasks, where we consider both homophilic and heterophilic graph datasets for homogeneous node-level tasks, and cover both Topology Refinement (TR, i.e., refining graphs from data with the original topology) and Topology Inference (TI, i.e., inferring graphs from data without initial topology) settings. For robustness, GSLB evaluates GSL models under three types of noise: supervision signal noise, structure noise, and feature noise. We also compare GSL algorithms with the models specifically designed to improve these types of robustness. For complexity, GSLB conducts a detailed evaluation of representative GSL algorithms in terms of time complexity and space complexity.\n\nThrough extensive experiments, we observe that: (1) GSL generally brings performance improvement for node-level tasks, especially on heterophilic graphs; (2) on graph-level tasks, current GSL models bring limited improvement and their performance varies greatly across different datasets; (3) most GSL algorithms (especially unsupervised GSL algorithms) show impressive robustness; (4) GSL models require significant time and memory overhead, making them challenging to deploy on large-scale graphs. In summary, we make the following three contributions:\n\n1. GSL generally brings performance improvement for node-level tasks, especially on heterophilic graphs.\n2. On graph-level tasks, current GSL models bring limited improvement and their performance varies greatly across different datasets.\n3. Most GSL algorithms (especially unsupervised GSL algorithms) show impressive robustness.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "GSLB Overview", "md": "# GSLB Overview"}, {"type": "heading", "lvl": 2, "value": "Table 1: An overview of GSLB", "md": "## Table 1: An overview of GSLB"}, {"type": "text", "value": "Both algorithms and datasets are divided into three categories: homogeneous node-level, heterogeneous node-level, and graph-level. The evaluation is divided into three dimensions: effectiveness, robustness, and complexity.", "md": "Both algorithms and datasets are divided into three categories: homogeneous node-level, heterogeneous node-level, and graph-level. The evaluation is divided into three dimensions: effectiveness, robustness, and complexity."}, {"type": "heading", "lvl": 3, "value": "Algorithms", "md": "### Algorithms"}, {"type": "table", "rows": [["Homogeneous GSL", "Heterogeneous GSL", "Graph-level GSL"], ["LDS [11], GRCN [48], ProGNN [18], IDGL [4], CoGSL [26], SUBLIME [28], GEN [39], STABLE [21], NodeFormer [43], SLAPS [10], GSR [54], HES-GSL [42]", "GTN [49], HGSL [53]", "HGP-SL [52], VIB-GSL [36]"]], "md": "|Homogeneous GSL|Heterogeneous GSL|Graph-level GSL|\n|---|---|---|\n|LDS [11], GRCN [48], ProGNN [18], IDGL [4], CoGSL [26], SUBLIME [28], GEN [39], STABLE [21], NodeFormer [43], SLAPS [10], GSR [54], HES-GSL [42]|GTN [49], HGSL [53]|HGP-SL [52], VIB-GSL [36]|", "isPerfectTable": true, "csv": "\"Homogeneous GSL\",\"Heterogeneous GSL\",\"Graph-level GSL\"\n\"LDS [11], GRCN [48], ProGNN [18], IDGL [4], CoGSL [26], SUBLIME [28], GEN [39], STABLE [21], NodeFormer [43], SLAPS [10], GSR [54], HES-GSL [42]\",\"GTN [49], HGSL [53]\",\"HGP-SL [52], VIB-GSL [36]\""}, {"type": "heading", "lvl": 3, "value": "Datasets", "md": "### Datasets"}, {"type": "table", "rows": [["Homogeneous datasets", "Heterogeneous datasets", "Graph-level datasets"], ["Cora [47], Citeseer [47], Pubmed [47], ogbn-arxiv [15], Polblogs, Cornell [34], Texas [34], Wisconsin [34], Actor [37]", "ACM [49], DBLP [49], Yelp [29]", "IMDB-B [3], IMDB-M [3], COLLAB [46], REDDIT-B [46], MUTAG [5], PROTEINS [2], Peptides-Func [8], Peptides-Struct [8]"]], "md": "|Homogeneous datasets|Heterogeneous datasets|Graph-level datasets|\n|---|---|---|\n|Cora [47], Citeseer [47], Pubmed [47], ogbn-arxiv [15], Polblogs, Cornell [34], Texas [34], Wisconsin [34], Actor [37]|ACM [49], DBLP [49], Yelp [29]|IMDB-B [3], IMDB-M [3], COLLAB [46], REDDIT-B [46], MUTAG [5], PROTEINS [2], Peptides-Func [8], Peptides-Struct [8]|", "isPerfectTable": true, "csv": "\"Homogeneous datasets\",\"Heterogeneous datasets\",\"Graph-level datasets\"\n\"Cora [47], Citeseer [47], Pubmed [47], ogbn-arxiv [15], Polblogs, Cornell [34], Texas [34], Wisconsin [34], Actor [37]\",\"ACM [49], DBLP [49], Yelp [29]\",\"IMDB-B [3], IMDB-M [3], COLLAB [46], REDDIT-B [46], MUTAG [5], PROTEINS [2], Peptides-Func [8], Peptides-Struct [8]\""}, {"type": "heading", "lvl": 3, "value": "Evaluations", "md": "### Evaluations"}, {"type": "text", "value": "- Effectiveness: Homogeneous node classification (Topology Refinement/Topology Inference), Heterogeneous node classification, Graph-level tasks\n- Robustness: Supervision signal robustness, Structure robustness, Feature robustness\n- Complexity: Time complexity, Space complexity\n\nTo mitigate the aforementioned problems, Graph Structure Learning (GSL) [4, 55, 30, 10, 57, 50] has become an important theme in graph learning. GSL aims to make the computation structure of GNNs more suitable for downstream tasks and improve the quality of the learned representations. While it is widespread in different communities and the research enthusiasm for GSL is increasing, there is no standardized benchmark that could offer a fair and consistent comparison of different GSL algorithms. Moreover, due to the complexity and diversity of graph datasets, the experimental setups in existing work are not consistent, such as varying ratios of the training set and different train/validation/test splits. This poses a great obstacle to a holistic understanding of the current research status. Therefore, the development of a standardized and comprehensive benchmark for GSL is an urgent need within the community.\n\nIn this work, we propose Graph Structure Learning Benchmark (GSLB), which serves as the first comprehensive benchmark for GSL. Our benchmark encompasses 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. We systematically investigate the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. Based on these three dimensions, we conduct an extensive comparative study of existing GSL algorithms in different scenarios. For effectiveness, GSLB provides a fair and comprehensive comparison of existing algorithms on homogeneous node-level, heterogeneous node-level, and graph-level tasks, where we consider both homophilic and heterophilic graph datasets for homogeneous node-level tasks, and cover both Topology Refinement (TR, i.e., refining graphs from data with the original topology) and Topology Inference (TI, i.e., inferring graphs from data without initial topology) settings. For robustness, GSLB evaluates GSL models under three types of noise: supervision signal noise, structure noise, and feature noise. We also compare GSL algorithms with the models specifically designed to improve these types of robustness. For complexity, GSLB conducts a detailed evaluation of representative GSL algorithms in terms of time complexity and space complexity.\n\nThrough extensive experiments, we observe that: (1) GSL generally brings performance improvement for node-level tasks, especially on heterophilic graphs; (2) on graph-level tasks, current GSL models bring limited improvement and their performance varies greatly across different datasets; (3) most GSL algorithms (especially unsupervised GSL algorithms) show impressive robustness; (4) GSL models require significant time and memory overhead, making them challenging to deploy on large-scale graphs. In summary, we make the following three contributions:\n\n1. GSL generally brings performance improvement for node-level tasks, especially on heterophilic graphs.\n2. On graph-level tasks, current GSL models bring limited improvement and their performance varies greatly across different datasets.\n3. Most GSL algorithms (especially unsupervised GSL algorithms) show impressive robustness.", "md": "- Effectiveness: Homogeneous node classification (Topology Refinement/Topology Inference), Heterogeneous node classification, Graph-level tasks\n- Robustness: Supervision signal robustness, Structure robustness, Feature robustness\n- Complexity: Time complexity, Space complexity\n\nTo mitigate the aforementioned problems, Graph Structure Learning (GSL) [4, 55, 30, 10, 57, 50] has become an important theme in graph learning. GSL aims to make the computation structure of GNNs more suitable for downstream tasks and improve the quality of the learned representations. While it is widespread in different communities and the research enthusiasm for GSL is increasing, there is no standardized benchmark that could offer a fair and consistent comparison of different GSL algorithms. Moreover, due to the complexity and diversity of graph datasets, the experimental setups in existing work are not consistent, such as varying ratios of the training set and different train/validation/test splits. This poses a great obstacle to a holistic understanding of the current research status. Therefore, the development of a standardized and comprehensive benchmark for GSL is an urgent need within the community.\n\nIn this work, we propose Graph Structure Learning Benchmark (GSLB), which serves as the first comprehensive benchmark for GSL. Our benchmark encompasses 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. We systematically investigate the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. Based on these three dimensions, we conduct an extensive comparative study of existing GSL algorithms in different scenarios. For effectiveness, GSLB provides a fair and comprehensive comparison of existing algorithms on homogeneous node-level, heterogeneous node-level, and graph-level tasks, where we consider both homophilic and heterophilic graph datasets for homogeneous node-level tasks, and cover both Topology Refinement (TR, i.e., refining graphs from data with the original topology) and Topology Inference (TI, i.e., inferring graphs from data without initial topology) settings. For robustness, GSLB evaluates GSL models under three types of noise: supervision signal noise, structure noise, and feature noise. We also compare GSL algorithms with the models specifically designed to improve these types of robustness. For complexity, GSLB conducts a detailed evaluation of representative GSL algorithms in terms of time complexity and space complexity.\n\nThrough extensive experiments, we observe that: (1) GSL generally brings performance improvement for node-level tasks, especially on heterophilic graphs; (2) on graph-level tasks, current GSL models bring limited improvement and their performance varies greatly across different datasets; (3) most GSL algorithms (especially unsupervised GSL algorithms) show impressive robustness; (4) GSL models require significant time and memory overhead, making them challenging to deploy on large-scale graphs. In summary, we make the following three contributions:\n\n1. GSL generally brings performance improvement for node-level tasks, especially on heterophilic graphs.\n2. On graph-level tasks, current GSL models bring limited improvement and their performance varies greatly across different datasets.\n3. Most GSL algorithms (especially unsupervised GSL algorithms) show impressive robustness."}]}, {"page": 3, "text": "                                Input data                                                     Graph Learner                                                 Refined computation graph                                     Post Process                                             Graph Encoder\n                                 ! = (A, X)                                                                                                                                  !    \u22c6    = (A\u22c6, X)\nStructure Modeling\n    Message Passing                                                                                                                                                                                                             KNNSparsify\n                                                                                                                                                                                                                                ThresholdSparsify                                                  \u2112    \"#$%\n                                                                                                                                                                                                                                    Discretize                                                           +\n                                          \u00b7\u00b7\u00b7                                                                                                                                                                                     Symmetrize                                                         \u2112&'(\n  Figure 1: A general framework of Graph Structure Learning (GSL). GSL methods start with input\n  features and an optional initial graph structure. Its corresponding computation graph is refined/inferred\n  through a structure learning module. With the learned computation graph, Graph Neural Networks\n (GNNs) are used to generate graph representations.\n  \u2022 We propose GSLB, the first comprehensive benchmark for graph structure learning. We integrate\n         16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level,\n          heterogeneous node-level, and graph-level tasks. An overview of our benchmark is shown in\n          Table 1.\n  \u2022 To explore the ability and limitations of GSL, we systematically evaluate existing algorithms from\n          three dimensions: effectiveness, robustness, and complexity. Based on the results, we reveal the\n          potential benefits and drawbacks of GSL to assist future research efforts.\n  \u2022 To facilitate future work and help researchers quickly use the latest models, we develop an easy-to-\n          use open-source library. Besides, users can evaluate their own models or datasets with less effort.\n          Our code is available at https://github.com/GSL-Benchmark/GSLB.\n  2                Problem Definition\n  In this section, we will briefly review the advances and basic concepts of GSL. Given an undirected\n  graph G = (A, X), where A \u2208                                                                                                        RN            \u00d7N is the adjacency matrix, auv = 1 if edge (u, v) exists and\n  auv = 0 otherwise, and X \u2208                                                                                                RN            \u00d7F is the node features matrix, N is the number of nodes, F is\n  the dimension of node features. Given an optional graph G                                                                                                                                                       , the goal of GSL is to jointly optimize\n  computation graph G                                                                  \u22c6       = (A\u22c6                       , X) and the parameters of graph encoder \u0398f to obtain high-quality\n  node representations Z\u22c6                                                                              \u2208        RN             \u00d7F \u2032 for downstream tasks, where A\u22c6                                                                                        is the refined graph by graph\n  learner.\n  In general, the objective of GSL can be summarized as the following formula:\n where the first term LTask refers to a task-specific objective with respect to the learned representation        LGSL = LTask(Z\u22c6                                                      , Y) + \u03bbLReg(A\u22c6                          , Z\u22c6            , G        )                                                  (1)\n  Z\u22c6            and ground-truth Y, the second term LReg imposes constraints on the learned graph structure and\n  representations, and \u03bb is a hyper-parameter that controls the trade-off between the two terms. The\n  general framework of GSL is shown in Figure 1.\n  3                GSLB: Graph Structure Learning Benchmark\n  In this section, we introduce the overview of Graph Structure Learning Benchmark, with considera-\n  tions of algorithms (Section 3.1), datasets (Section 3.2) and evaluations (Section 3.3).\n  3.1                   Benchmark Algorithms\n Table 1 shows the overall 16 algorithms integrated in GSLB. They are divided into three categories:\n  homogeneous GSL, heterogeneous GSL, and graph-level GSL. We briefly introduce each category in\n  the following, and more details are provided in Appendix A.2.\n  Homogeneous GSL. Most of the existing GSL algorithms are designed for homogeneous graphs.\n They assume there is only one type of nodes and edges in the graph. We select 7 TR-oriented\n                                                                                                                                                                                                     3", "md": "# Graph Structure Learning\n\n## Input data\n\n! = (A, X)\n\n## Graph Learner\n\nRefined computation graph: ! \u22c6 = (A\u22c6, X)\n\n## Structure Modeling\n\n- Message Passing\n- KNNSparsify\n- ThresholdSparsify\n- Discretize\n- Symmetrize\n\nFigure 1: A general framework of Graph Structure Learning (GSL). GSL methods start with input features and an optional initial graph structure. Its corresponding computation graph is refined/inferred through a structure learning module. With the learned computation graph, Graph Neural Networks (GNNs) are used to generate graph representations.\n\n### We propose GSLB, the first comprehensive benchmark for graph structure learning. We integrate 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. An overview of our benchmark is shown in Table 1.\n\n### To explore the ability and limitations of GSL, we systematically evaluate existing algorithms from three dimensions: effectiveness, robustness, and complexity. Based on the results, we reveal the potential benefits and drawbacks of GSL to assist future research efforts.\n\n### To facilitate future work and help researchers quickly use the latest models, we develop an easy-to-use open-source library. Besides, users can evaluate their own models or datasets with less effort. Our code is available at https://github.com/GSL-Benchmark/GSLB.\n\n## Problem Definition\n\nIn this section, we will briefly review the advances and basic concepts of GSL. Given an undirected graph G = (A, X), where A \u2208 $$\\mathbb{R}^{N \\times N}$$ is the adjacency matrix, auv = 1 if edge (u, v) exists and auv = 0 otherwise, and X \u2208 $$\\mathbb{R}^{N \\times F}$$ is the node features matrix, N is the number of nodes, F is the dimension of node features. Given an optional graph G, the goal of GSL is to jointly optimize computation graph G \u22c6 = (A\u22c6, X) and the parameters of graph encoder \u0398f to obtain high-quality node representations Z\u22c6 \u2208 $$\\mathbb{R}^{N \\times F'}$$ for downstream tasks, where A\u22c6 is the refined graph by graph learner.\n\nIn general, the objective of GSL can be summarized as the following formula:\n\n$$\nLGSL = L_{Task}(Z\u22c6, Y) + \\lambda L_{Reg}(A\u22c6, Z\u22c6, G)\n$$\n\nwhere the first term LTask refers to a task-specific objective with respect to the learned representation Z\u22c6 and ground-truth Y, the second term LReg imposes constraints on the learned graph structure and representations, and \u03bb is a hyper-parameter that controls the trade-off between the two terms. The general framework of GSL is shown in Figure 1.\n\n## GSLB: Graph Structure Learning Benchmark\n\n### 3.1 Benchmark Algorithms\n\nTable 1 shows the overall 16 algorithms integrated in GSLB. They are divided into three categories: homogeneous GSL, heterogeneous GSL, and graph-level GSL. We briefly introduce each category in the following, and more details are provided in Appendix A.2.\n\n#### Homogeneous GSL\n\nMost of the existing GSL algorithms are designed for homogeneous graphs. They assume there is only one type of nodes and edges in the graph. We select 7 TR-oriented algorithms.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Graph Structure Learning", "md": "# Graph Structure Learning"}, {"type": "heading", "lvl": 2, "value": "Input data", "md": "## Input data"}, {"type": "text", "value": "! = (A, X)", "md": "! = (A, X)"}, {"type": "heading", "lvl": 2, "value": "Graph Learner", "md": "## Graph Learner"}, {"type": "text", "value": "Refined computation graph: ! \u22c6 = (A\u22c6, X)", "md": "Refined computation graph: ! \u22c6 = (A\u22c6, X)"}, {"type": "heading", "lvl": 2, "value": "Structure Modeling", "md": "## Structure Modeling"}, {"type": "text", "value": "- Message Passing\n- KNNSparsify\n- ThresholdSparsify\n- Discretize\n- Symmetrize\n\nFigure 1: A general framework of Graph Structure Learning (GSL). GSL methods start with input features and an optional initial graph structure. Its corresponding computation graph is refined/inferred through a structure learning module. With the learned computation graph, Graph Neural Networks (GNNs) are used to generate graph representations.", "md": "- Message Passing\n- KNNSparsify\n- ThresholdSparsify\n- Discretize\n- Symmetrize\n\nFigure 1: A general framework of Graph Structure Learning (GSL). GSL methods start with input features and an optional initial graph structure. Its corresponding computation graph is refined/inferred through a structure learning module. With the learned computation graph, Graph Neural Networks (GNNs) are used to generate graph representations."}, {"type": "heading", "lvl": 3, "value": "We propose GSLB, the first comprehensive benchmark for graph structure learning. We integrate 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. An overview of our benchmark is shown in Table 1.", "md": "### We propose GSLB, the first comprehensive benchmark for graph structure learning. We integrate 16 state-of-the-art GSL algorithms and 20 diverse graph datasets covering homogeneous node-level, heterogeneous node-level, and graph-level tasks. An overview of our benchmark is shown in Table 1."}, {"type": "heading", "lvl": 3, "value": "To explore the ability and limitations of GSL, we systematically evaluate existing algorithms from three dimensions: effectiveness, robustness, and complexity. Based on the results, we reveal the potential benefits and drawbacks of GSL to assist future research efforts.", "md": "### To explore the ability and limitations of GSL, we systematically evaluate existing algorithms from three dimensions: effectiveness, robustness, and complexity. Based on the results, we reveal the potential benefits and drawbacks of GSL to assist future research efforts."}, {"type": "heading", "lvl": 3, "value": "To facilitate future work and help researchers quickly use the latest models, we develop an easy-to-use open-source library. Besides, users can evaluate their own models or datasets with less effort. Our code is available at https://github.com/GSL-Benchmark/GSLB.", "md": "### To facilitate future work and help researchers quickly use the latest models, we develop an easy-to-use open-source library. Besides, users can evaluate their own models or datasets with less effort. Our code is available at https://github.com/GSL-Benchmark/GSLB."}, {"type": "heading", "lvl": 2, "value": "Problem Definition", "md": "## Problem Definition"}, {"type": "text", "value": "In this section, we will briefly review the advances and basic concepts of GSL. Given an undirected graph G = (A, X), where A \u2208 $$\\mathbb{R}^{N \\times N}$$ is the adjacency matrix, auv = 1 if edge (u, v) exists and auv = 0 otherwise, and X \u2208 $$\\mathbb{R}^{N \\times F}$$ is the node features matrix, N is the number of nodes, F is the dimension of node features. Given an optional graph G, the goal of GSL is to jointly optimize computation graph G \u22c6 = (A\u22c6, X) and the parameters of graph encoder \u0398f to obtain high-quality node representations Z\u22c6 \u2208 $$\\mathbb{R}^{N \\times F'}$$ for downstream tasks, where A\u22c6 is the refined graph by graph learner.\n\nIn general, the objective of GSL can be summarized as the following formula:\n\n$$\nLGSL = L_{Task}(Z\u22c6, Y) + \\lambda L_{Reg}(A\u22c6, Z\u22c6, G)\n$$\n\nwhere the first term LTask refers to a task-specific objective with respect to the learned representation Z\u22c6 and ground-truth Y, the second term LReg imposes constraints on the learned graph structure and representations, and \u03bb is a hyper-parameter that controls the trade-off between the two terms. The general framework of GSL is shown in Figure 1.", "md": "In this section, we will briefly review the advances and basic concepts of GSL. Given an undirected graph G = (A, X), where A \u2208 $$\\mathbb{R}^{N \\times N}$$ is the adjacency matrix, auv = 1 if edge (u, v) exists and auv = 0 otherwise, and X \u2208 $$\\mathbb{R}^{N \\times F}$$ is the node features matrix, N is the number of nodes, F is the dimension of node features. Given an optional graph G, the goal of GSL is to jointly optimize computation graph G \u22c6 = (A\u22c6, X) and the parameters of graph encoder \u0398f to obtain high-quality node representations Z\u22c6 \u2208 $$\\mathbb{R}^{N \\times F'}$$ for downstream tasks, where A\u22c6 is the refined graph by graph learner.\n\nIn general, the objective of GSL can be summarized as the following formula:\n\n$$\nLGSL = L_{Task}(Z\u22c6, Y) + \\lambda L_{Reg}(A\u22c6, Z\u22c6, G)\n$$\n\nwhere the first term LTask refers to a task-specific objective with respect to the learned representation Z\u22c6 and ground-truth Y, the second term LReg imposes constraints on the learned graph structure and representations, and \u03bb is a hyper-parameter that controls the trade-off between the two terms. The general framework of GSL is shown in Figure 1."}, {"type": "heading", "lvl": 2, "value": "GSLB: Graph Structure Learning Benchmark", "md": "## GSLB: Graph Structure Learning Benchmark"}, {"type": "heading", "lvl": 3, "value": "3.1 Benchmark Algorithms", "md": "### 3.1 Benchmark Algorithms"}, {"type": "text", "value": "Table 1 shows the overall 16 algorithms integrated in GSLB. They are divided into three categories: homogeneous GSL, heterogeneous GSL, and graph-level GSL. We briefly introduce each category in the following, and more details are provided in Appendix A.2.", "md": "Table 1 shows the overall 16 algorithms integrated in GSLB. They are divided into three categories: homogeneous GSL, heterogeneous GSL, and graph-level GSL. We briefly introduce each category in the following, and more details are provided in Appendix A.2."}, {"type": "heading", "lvl": 4, "value": "Homogeneous GSL", "md": "#### Homogeneous GSL"}, {"type": "text", "value": "Most of the existing GSL algorithms are designed for homogeneous graphs. They assume there is only one type of nodes and edges in the graph. We select 7 TR-oriented algorithms.", "md": "Most of the existing GSL algorithms are designed for homogeneous graphs. They assume there is only one type of nodes and edges in the graph. We select 7 TR-oriented algorithms."}]}, {"page": 4, "text": " algorithms including GRCN [48], ProGNN [18], IDGL [4], GEN [39], CoGSL [26], STABLE [21],\n and GSR [54]. For TI-oriented algorithms, we select SUBLIME [28], NodeFormer [43], SLAPS [10],\n and HES-GSL [42]. It is worth noting that TR-oriented algorithms can only be applied if the original\n graph structure is available, but we can construct a preliminary graph based on node features (e.g.,\n kNN graphs or \u03f5-graphs).\n Heterogeneous GSL. We integrate two representative heterogeneous GSL algorithms: Graph Trans-\n former Networks (GTN) [49] and Heterogeneous Graph Structure Learning (HGSL) [53], which can\n handle the heterogeneity and capture complex interactions in heterogeneous graphs.\n Graph-level GSL. Graph-level GSL algorithms aim to refine each graph structure in datasets. We\n select two graph-level algorithms: Hierarchical Graph Pooling with Structure Learning (HGP-SL) [52]\n and Variational Information Bottleneck guided Graph Structure Learning (VIB-GSL) [36].\n 3.2  Benchmark Datasets\n To comprehensively and effectively evaluate the characteristics of GSL in the field of graph learning,\nwe have integrated a large number of datasets from various domains for different types of tasks.\n For node-level tasks, to evaluate the most mainstream task of GSL, node classification, we use four\n citation networks (i.e., Cora, Citeseer, Pubmed [47]), and ogbn-arxiv [15], three website networks\n from WebKB (i.e., Cornell, Texas, and Wisconsin [34]), and a cooccurrence network Actor with\n homophily ratio ranging from strong homophily to strong heterophily. Subsequently, to validate the\n effectiveness of GSL in heterogeneous node classification, we utilized three heterogeneous graph\n datasets (i.e., DBLP [49], ACM [49], and Yelp [29]). To investigate the robustness of GSL, we\n further incorporate the Polblogs dataset for evaluation. For graph-level tasks, we select six public\n graph classification benchmark dataset from TUDataset [31] for evaluation, including IMDB-B [3],\n IMDB-M [3], RDT-B [46], COLLAB [46], MUTAG [5] and PROTEINS [2]. Each dataset is a\n collection of graphs where each graph is associated with a level. Besides, exploring whether GSL can\n capture long-range information is an exciting topic. Therefore, we have utilized recently proposed\n long-range graph datasets: Peptides-func and Peptides-struct [8]. See more details and statistics about\n datasets in Appendix A.1.\n 3.3  Benchmark Evaluations\n To comprehensively investigate the pros and cons of GSL, our benchmark evaluations encompass three\n dimensions: effectiveness, robustness, and complexity. For effectiveness, GSLB provides a fair\n and comprehensive comparison of existing algorithms from three perspectives: homogeneous node\n classification, heterogeneous node classification, and graph-level tasks. In the case of homogeneous\n node classification, we evaluated them on both homophilic and heterophilic graph datasets, conducting\n experiments in both TR and TI scenarios. For graph-level, we evaluate graph-level GSL algorithms\n on TUDataset and long-range graph datasets for exploring the capabilities on graph-level tasks. For\n most datasets, we use accuracy as our evaluation metric. For robustness, GSLB evaluates three\n types of robustness: supervision signal robustness, structure robustness, and feature robustness. We\n control the count of labels to explore the supervision signal robustness of GSL and find that GSL\n exhibits excellent performance in the scenarios with few labels. We inject random structure noise and\n graph topology attacks to investigate the structure robustness. We also study the feature robustness\n by randomly masking a certain proportion of node features. For complexity, we conduct a detailed\n evaluation of representative GSL algorithms in terms of time complexity and space complexity. It\nwill help to facilitate the deployment of GSL in real-world applications.\n 4   Experiments and Analysis\n In this section, we systematically investigate the effectiveness, robustness, and complexity of GSL\n algorithms by answering the following specific questions:\n \u2022 For effectiveness, RQ1: How effective are the algorithms on node-level representation learning\n   (Section 4.2)? RQ2: Can GSL mitigate homophily inductive bias of traditional message-passing\n   based GNNs (Section 4.2)? RQ3: How does GSL perform on heterogeneous graph datasets\n   (Section 4.3)? RQ4: How effective are the algorithms on graph-level representation learning\n   (Section 4.4)? RQ5: Can GSL methods capture long-range information on the graph (Appendix B)?\n                                                   4", "md": "## Algorithms\n\nAlgorithms including GRCN [48], ProGNN [18], IDGL [4], GEN [39], CoGSL [26], STABLE [21], and GSR [54]. For TI-oriented algorithms, we select SUBLIME [28], NodeFormer [43], SLAPS [10], and HES-GSL [42]. It is worth noting that TR-oriented algorithms can only be applied if the original graph structure is available, but we can construct a preliminary graph based on node features (e.g., kNN graphs or \u03f5-graphs).\n\n### Heterogeneous GSL\n\nWe integrate two representative heterogeneous GSL algorithms: Graph Transformer Networks (GTN) [49] and Heterogeneous Graph Structure Learning (HGSL) [53], which can handle the heterogeneity and capture complex interactions in heterogeneous graphs.\n\n### Graph-level GSL\n\nGraph-level GSL algorithms aim to refine each graph structure in datasets. We select two graph-level algorithms: Hierarchical Graph Pooling with Structure Learning (HGP-SL) [52] and Variational Information Bottleneck guided Graph Structure Learning (VIB-GSL) [36].\n\n## Benchmark Datasets\n\nTo comprehensively and effectively evaluate the characteristics of GSL in the field of graph learning, we have integrated a large number of datasets from various domains for different types of tasks.\n\nFor node-level tasks, to evaluate the most mainstream task of GSL, node classification, we use four citation networks (i.e., Cora, Citeseer, Pubmed [47]), and ogbn-arxiv [15], three website networks from WebKB (i.e., Cornell, Texas, and Wisconsin [34]), and a cooccurrence network Actor with homophily ratio ranging from strong homophily to strong heterophily. Subsequently, to validate the effectiveness of GSL in heterogeneous node classification, we utilized three heterogeneous graph datasets (i.e., DBLP [49], ACM [49], and Yelp [29]). To investigate the robustness of GSL, we further incorporate the Polblogs dataset for evaluation. For graph-level tasks, we select six public graph classification benchmark datasets from TUDataset [31] for evaluation, including IMDB-B [3], IMDB-M [3], RDT-B [46], COLLAB [46], MUTAG [5], and PROTEINS [2]. Each dataset is a collection of graphs where each graph is associated with a level. Besides, exploring whether GSL can capture long-range information is an exciting topic. Therefore, we have utilized recently proposed long-range graph datasets: Peptides-func and Peptides-struct [8]. See more details and statistics about datasets in Appendix A.1.\n\n## Benchmark Evaluations\n\nTo comprehensively investigate the pros and cons of GSL, our benchmark evaluations encompass three dimensions: effectiveness, robustness, and complexity. For effectiveness, GSLB provides a fair and comprehensive comparison of existing algorithms from three perspectives: homogeneous node classification, heterogeneous node classification, and graph-level tasks. In the case of homogeneous node classification, we evaluated them on both homophilic and heterophilic graph datasets, conducting experiments in both TR and TI scenarios. For graph-level, we evaluate graph-level GSL algorithms on TUDataset and long-range graph datasets for exploring the capabilities on graph-level tasks. For most datasets, we use accuracy as our evaluation metric. For robustness, GSLB evaluates three types of robustness: supervision signal robustness, structure robustness, and feature robustness. We control the count of labels to explore the supervision signal robustness of GSL and find that GSL exhibits excellent performance in the scenarios with few labels. We inject random structure noise and graph topology attacks to investigate the structure robustness. We also study the feature robustness by randomly masking a certain proportion of node features. For complexity, we conduct a detailed evaluation of representative GSL algorithms in terms of time complexity and space complexity. It will help to facilitate the deployment of GSL in real-world applications.\n\n## Experiments and Analysis\n\nIn this section, we systematically investigate the effectiveness, robustness, and complexity of GSL algorithms by answering the following specific questions:\n\n- RQ1: How effective are the algorithms on node-level representation learning (Section 4.2)?\n- RQ2: Can GSL mitigate homophily inductive bias of traditional message-passing based GNNs (Section 4.2)?\n- RQ3: How does GSL perform on heterogeneous graph datasets (Section 4.3)?\n- RQ4: How effective are the algorithms on graph-level representation learning (Section 4.4)?\n- RQ5: Can GSL methods capture long-range information on the graph (Appendix B)?", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Algorithms", "md": "## Algorithms"}, {"type": "text", "value": "Algorithms including GRCN [48], ProGNN [18], IDGL [4], GEN [39], CoGSL [26], STABLE [21], and GSR [54]. For TI-oriented algorithms, we select SUBLIME [28], NodeFormer [43], SLAPS [10], and HES-GSL [42]. It is worth noting that TR-oriented algorithms can only be applied if the original graph structure is available, but we can construct a preliminary graph based on node features (e.g., kNN graphs or \u03f5-graphs).", "md": "Algorithms including GRCN [48], ProGNN [18], IDGL [4], GEN [39], CoGSL [26], STABLE [21], and GSR [54]. For TI-oriented algorithms, we select SUBLIME [28], NodeFormer [43], SLAPS [10], and HES-GSL [42]. It is worth noting that TR-oriented algorithms can only be applied if the original graph structure is available, but we can construct a preliminary graph based on node features (e.g., kNN graphs or \u03f5-graphs)."}, {"type": "heading", "lvl": 3, "value": "Heterogeneous GSL", "md": "### Heterogeneous GSL"}, {"type": "text", "value": "We integrate two representative heterogeneous GSL algorithms: Graph Transformer Networks (GTN) [49] and Heterogeneous Graph Structure Learning (HGSL) [53], which can handle the heterogeneity and capture complex interactions in heterogeneous graphs.", "md": "We integrate two representative heterogeneous GSL algorithms: Graph Transformer Networks (GTN) [49] and Heterogeneous Graph Structure Learning (HGSL) [53], which can handle the heterogeneity and capture complex interactions in heterogeneous graphs."}, {"type": "heading", "lvl": 3, "value": "Graph-level GSL", "md": "### Graph-level GSL"}, {"type": "text", "value": "Graph-level GSL algorithms aim to refine each graph structure in datasets. We select two graph-level algorithms: Hierarchical Graph Pooling with Structure Learning (HGP-SL) [52] and Variational Information Bottleneck guided Graph Structure Learning (VIB-GSL) [36].", "md": "Graph-level GSL algorithms aim to refine each graph structure in datasets. We select two graph-level algorithms: Hierarchical Graph Pooling with Structure Learning (HGP-SL) [52] and Variational Information Bottleneck guided Graph Structure Learning (VIB-GSL) [36]."}, {"type": "heading", "lvl": 2, "value": "Benchmark Datasets", "md": "## Benchmark Datasets"}, {"type": "text", "value": "To comprehensively and effectively evaluate the characteristics of GSL in the field of graph learning, we have integrated a large number of datasets from various domains for different types of tasks.\n\nFor node-level tasks, to evaluate the most mainstream task of GSL, node classification, we use four citation networks (i.e., Cora, Citeseer, Pubmed [47]), and ogbn-arxiv [15], three website networks from WebKB (i.e., Cornell, Texas, and Wisconsin [34]), and a cooccurrence network Actor with homophily ratio ranging from strong homophily to strong heterophily. Subsequently, to validate the effectiveness of GSL in heterogeneous node classification, we utilized three heterogeneous graph datasets (i.e., DBLP [49], ACM [49], and Yelp [29]). To investigate the robustness of GSL, we further incorporate the Polblogs dataset for evaluation. For graph-level tasks, we select six public graph classification benchmark datasets from TUDataset [31] for evaluation, including IMDB-B [3], IMDB-M [3], RDT-B [46], COLLAB [46], MUTAG [5], and PROTEINS [2]. Each dataset is a collection of graphs where each graph is associated with a level. Besides, exploring whether GSL can capture long-range information is an exciting topic. Therefore, we have utilized recently proposed long-range graph datasets: Peptides-func and Peptides-struct [8]. See more details and statistics about datasets in Appendix A.1.", "md": "To comprehensively and effectively evaluate the characteristics of GSL in the field of graph learning, we have integrated a large number of datasets from various domains for different types of tasks.\n\nFor node-level tasks, to evaluate the most mainstream task of GSL, node classification, we use four citation networks (i.e., Cora, Citeseer, Pubmed [47]), and ogbn-arxiv [15], three website networks from WebKB (i.e., Cornell, Texas, and Wisconsin [34]), and a cooccurrence network Actor with homophily ratio ranging from strong homophily to strong heterophily. Subsequently, to validate the effectiveness of GSL in heterogeneous node classification, we utilized three heterogeneous graph datasets (i.e., DBLP [49], ACM [49], and Yelp [29]). To investigate the robustness of GSL, we further incorporate the Polblogs dataset for evaluation. For graph-level tasks, we select six public graph classification benchmark datasets from TUDataset [31] for evaluation, including IMDB-B [3], IMDB-M [3], RDT-B [46], COLLAB [46], MUTAG [5], and PROTEINS [2]. Each dataset is a collection of graphs where each graph is associated with a level. Besides, exploring whether GSL can capture long-range information is an exciting topic. Therefore, we have utilized recently proposed long-range graph datasets: Peptides-func and Peptides-struct [8]. See more details and statistics about datasets in Appendix A.1."}, {"type": "heading", "lvl": 2, "value": "Benchmark Evaluations", "md": "## Benchmark Evaluations"}, {"type": "text", "value": "To comprehensively investigate the pros and cons of GSL, our benchmark evaluations encompass three dimensions: effectiveness, robustness, and complexity. For effectiveness, GSLB provides a fair and comprehensive comparison of existing algorithms from three perspectives: homogeneous node classification, heterogeneous node classification, and graph-level tasks. In the case of homogeneous node classification, we evaluated them on both homophilic and heterophilic graph datasets, conducting experiments in both TR and TI scenarios. For graph-level, we evaluate graph-level GSL algorithms on TUDataset and long-range graph datasets for exploring the capabilities on graph-level tasks. For most datasets, we use accuracy as our evaluation metric. For robustness, GSLB evaluates three types of robustness: supervision signal robustness, structure robustness, and feature robustness. We control the count of labels to explore the supervision signal robustness of GSL and find that GSL exhibits excellent performance in the scenarios with few labels. We inject random structure noise and graph topology attacks to investigate the structure robustness. We also study the feature robustness by randomly masking a certain proportion of node features. For complexity, we conduct a detailed evaluation of representative GSL algorithms in terms of time complexity and space complexity. It will help to facilitate the deployment of GSL in real-world applications.", "md": "To comprehensively investigate the pros and cons of GSL, our benchmark evaluations encompass three dimensions: effectiveness, robustness, and complexity. For effectiveness, GSLB provides a fair and comprehensive comparison of existing algorithms from three perspectives: homogeneous node classification, heterogeneous node classification, and graph-level tasks. In the case of homogeneous node classification, we evaluated them on both homophilic and heterophilic graph datasets, conducting experiments in both TR and TI scenarios. For graph-level, we evaluate graph-level GSL algorithms on TUDataset and long-range graph datasets for exploring the capabilities on graph-level tasks. For most datasets, we use accuracy as our evaluation metric. For robustness, GSLB evaluates three types of robustness: supervision signal robustness, structure robustness, and feature robustness. We control the count of labels to explore the supervision signal robustness of GSL and find that GSL exhibits excellent performance in the scenarios with few labels. We inject random structure noise and graph topology attacks to investigate the structure robustness. We also study the feature robustness by randomly masking a certain proportion of node features. For complexity, we conduct a detailed evaluation of representative GSL algorithms in terms of time complexity and space complexity. It will help to facilitate the deployment of GSL in real-world applications."}, {"type": "heading", "lvl": 2, "value": "Experiments and Analysis", "md": "## Experiments and Analysis"}, {"type": "text", "value": "In this section, we systematically investigate the effectiveness, robustness, and complexity of GSL algorithms by answering the following specific questions:\n\n- RQ1: How effective are the algorithms on node-level representation learning (Section 4.2)?\n- RQ2: Can GSL mitigate homophily inductive bias of traditional message-passing based GNNs (Section 4.2)?\n- RQ3: How does GSL perform on heterogeneous graph datasets (Section 4.3)?\n- RQ4: How effective are the algorithms on graph-level representation learning (Section 4.4)?\n- RQ5: Can GSL methods capture long-range information on the graph (Appendix B)?", "md": "In this section, we systematically investigate the effectiveness, robustness, and complexity of GSL algorithms by answering the following specific questions:\n\n- RQ1: How effective are the algorithms on node-level representation learning (Section 4.2)?\n- RQ2: Can GSL mitigate homophily inductive bias of traditional message-passing based GNNs (Section 4.2)?\n- RQ3: How does GSL perform on heterogeneous graph datasets (Section 4.3)?\n- RQ4: How effective are the algorithms on graph-level representation learning (Section 4.4)?\n- RQ5: Can GSL methods capture long-range information on the graph (Appendix B)?"}]}, {"page": 5, "text": "\u2022 For robustness, RQ6: How robust are GSL algorithms when faced with a scarcity of labeled\n  samples? RQ7: How robust are GSL algorithms in the face of structure attack or noise? RQ8:\n  How is the feature robustness of GSL? (Section 4.5)\n\u2022 For complexity, RQ9: How efficient are these algorithms in terms of time and space (Section 4.6)?\n\u2022 Otherwise, RQ10: What does the learned graph structure look like (Appendix B.2)?\n4.1   Experimental Settings\nAll algorithms in GSLB are implemented by PyTorch [33], and unless specifically indicated, the\nencoders for all algorithms are Graph Convolutional Networks. All experiments are conducted on a\nLinux server with GPU (NVIDIA GeForce 3090 and NVIDIA A100) and CPU (AMD EPYC 7763),\nusing PyTorch 1.13.0, DGL 1.1.0 [38] and Python 3.9.16.\n4.2   Performance on node-level representation learning\nFor node-level representation learning, we conduct experiments on homogeneous graph datasets under\nboth TR and TI scenarios, and use classification accuracy as our evaluation metric. Table 2 shows\nthe experimental results of various GSL algorithms under the standard setting of transductive node\nclassification task in the TR scenario. We can observe that: 1) Most GSL algorithms generally show\nimprovements in node classification task, particularly on datasets with high heterophily ratio. Due\nto the presence of heterophilic connections in heterophily graphs, where nodes are often connected\nto nodes with different labels, it violates the homophily assumption of message-passing neural\nnetworks. As a result, traditional GNNs like GCN and GAT exhibit poor performance. However,\nGSL can improve significantly on heterophily graph datasets by learning new graph structures based\non downstream tasks and specific learning objectives, thus enhancing the homophily of the graph and\npromoting the performance on node-level representation learning. 2) SUBLIME achieves optimal\nor near-optimal results on most datasets. It learns graph structure through contrastive learning in an\nunsupervised manner. As mentioned in the recent literature [10], optimizing graph structures solely\nbased on label information is insufficient. Leveraging a large and abundant amount of unlabeled\ninformation can enhance the performance of GSL. 3) The scalability of GSL still needs improvement,\nas only a few models can be trained on large-scale datasets (e.g., ogbn-arxiv). We will discuss the\nscalability of GSL algorithms in detail in a subsequent section (Section 4.6).\nTable 3 shows the experimental results of the transductive node classification task in the TI scenario.\nSome GSL algorithms are designed for TR scenario (i.e., GRCN, IDGL, etc.), so we use kNN\ngraphs as their original graph structure. As we can observe, on the homophily graph datasets, GSL\noutperforms baselines, such as MLP, GCNknn and GATknn. However, on the heterophily graph\ndatasets, most GSL algorithms often have difficulty achieving better results than baseline models. As\nmentioned in earlier literature, a network with randomness tends to get better performance utilizing\nkNN for direct information propagation [17]. Therefore, traditional message-passing neural networks\nwith kNN graphs demonstrate powerful performance. In addition, as observed in the TR scenario,\nmodels that leverage self-supervision to extract abundant unlabeled information often achieve better\nperformance.\n4.3   Performance on heterogeneous graph node-level representation learning\nIn this section, we evaluate the performance of GSL algorithms on heterogeneous node classification\ntask and use Macro-F1 and Micro-F1 as our evaluation metrics. Table 4 shows the experimental\nresults on heterogeneous graph datasets. By observing the results, we can find that: 1) Because\nGTN and HGSL consider both heterogeneity and structure learning, they generally outperform other\nmodels on heterogeneous graph datasets. 2) GSL algorithms generally outperform the vanilla GNN\nmodels (e.g. GCN and GAT) since they have learned better structures to facilitate message passing. 3)\nDue to the majority of GSL algorithms not explicitly accounting for heterogeneity, they may exhibit\npoorer performance on heterogeneous graph datasets. 4) Some datasets (e.g. Yelp) exhibit stronger\nheterogeneity, and on such datasets, models that consider heterogeneity (e.g. HAN, GTN, and HGSL)\nperform significantly better.\n                                                   5", "md": "## Research Questions\n\n- RQ6: How robust are GSL algorithms when faced with a scarcity of labeled samples?\n- RQ7: How robust are GSL algorithms in the face of structure attack or noise?\n- RQ8: How is the feature robustness of GSL? (Section 4.5)\n- RQ9: How efficient are these algorithms in terms of time and space (Section 4.6)?\n- RQ10: What does the learned graph structure look like (Appendix B.2)?\n\n### Experimental Settings\n\nAll algorithms in GSLB are implemented by PyTorch [33], and unless specifically indicated, the encoders for all algorithms are Graph Convolutional Networks. All experiments are conducted on a Linux server with GPU (NVIDIA GeForce 3090 and NVIDIA A100) and CPU (AMD EPYC 7763), using PyTorch 1.13.0, DGL 1.1.0 [38] and Python 3.9.16.\n\n### Performance on Node-level Representation Learning\n\nFor node-level representation learning, we conduct experiments on homogeneous graph datasets under both TR and TI scenarios, and use classification accuracy as our evaluation metric.\n\nTable 2: Experimental results of various GSL algorithms under the standard setting of transductive node classification task in the TR scenario.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Performance} \\\\\n\\hline\n\\text{Algorithm 1} & \\text{Performance 1} \\\\\n\\text{Algorithm 2} & \\text{Performance 2} \\\\\n\\text{Algorithm 3} & \\text{Performance 3} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. Most GSL algorithms generally show improvements in node classification task, particularly on datasets with high heterophily ratio.\n2. SUBLIME achieves optimal or near-optimal results on most datasets.\n3. The scalability of GSL still needs improvement, especially on large-scale datasets.\n\nTable 3: Experimental results of the transductive node classification task in the TI scenario.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Performance} \\\\\n\\hline\n\\text{Algorithm A} & \\text{Performance A} \\\\\n\\text{Algorithm B} & \\text{Performance B} \\\\\n\\text{Algorithm C} & \\text{Performance C} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. GSL outperforms baselines on homophily graph datasets.\n2. Models that leverage self-supervision achieve better performance.\n\n### Performance on Heterogeneous Graph Node-level Representation Learning\n\nTable 4: Experimental results on heterogeneous graph datasets.\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Macro-F1} & \\text{Micro-F1} \\\\\n\\hline\n\\text{GTN} & \\text{F1 score} & \\text{F1 score} \\\\\n\\text{HGSL} & \\text{F1 score} & \\text{F1 score} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. GTN and HGSL generally outperform other models on heterogeneous graph datasets.\n2. GSL algorithms generally outperform vanilla GNN models.\n3. Models that consider heterogeneity perform significantly better on datasets with stronger heterogeneity.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Research Questions", "md": "## Research Questions"}, {"type": "text", "value": "- RQ6: How robust are GSL algorithms when faced with a scarcity of labeled samples?\n- RQ7: How robust are GSL algorithms in the face of structure attack or noise?\n- RQ8: How is the feature robustness of GSL? (Section 4.5)\n- RQ9: How efficient are these algorithms in terms of time and space (Section 4.6)?\n- RQ10: What does the learned graph structure look like (Appendix B.2)?", "md": "- RQ6: How robust are GSL algorithms when faced with a scarcity of labeled samples?\n- RQ7: How robust are GSL algorithms in the face of structure attack or noise?\n- RQ8: How is the feature robustness of GSL? (Section 4.5)\n- RQ9: How efficient are these algorithms in terms of time and space (Section 4.6)?\n- RQ10: What does the learned graph structure look like (Appendix B.2)?"}, {"type": "heading", "lvl": 3, "value": "Experimental Settings", "md": "### Experimental Settings"}, {"type": "text", "value": "All algorithms in GSLB are implemented by PyTorch [33], and unless specifically indicated, the encoders for all algorithms are Graph Convolutional Networks. All experiments are conducted on a Linux server with GPU (NVIDIA GeForce 3090 and NVIDIA A100) and CPU (AMD EPYC 7763), using PyTorch 1.13.0, DGL 1.1.0 [38] and Python 3.9.16.", "md": "All algorithms in GSLB are implemented by PyTorch [33], and unless specifically indicated, the encoders for all algorithms are Graph Convolutional Networks. All experiments are conducted on a Linux server with GPU (NVIDIA GeForce 3090 and NVIDIA A100) and CPU (AMD EPYC 7763), using PyTorch 1.13.0, DGL 1.1.0 [38] and Python 3.9.16."}, {"type": "heading", "lvl": 3, "value": "Performance on Node-level Representation Learning", "md": "### Performance on Node-level Representation Learning"}, {"type": "text", "value": "For node-level representation learning, we conduct experiments on homogeneous graph datasets under both TR and TI scenarios, and use classification accuracy as our evaluation metric.\n\nTable 2: Experimental results of various GSL algorithms under the standard setting of transductive node classification task in the TR scenario.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Performance} \\\\\n\\hline\n\\text{Algorithm 1} & \\text{Performance 1} \\\\\n\\text{Algorithm 2} & \\text{Performance 2} \\\\\n\\text{Algorithm 3} & \\text{Performance 3} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. Most GSL algorithms generally show improvements in node classification task, particularly on datasets with high heterophily ratio.\n2. SUBLIME achieves optimal or near-optimal results on most datasets.\n3. The scalability of GSL still needs improvement, especially on large-scale datasets.\n\nTable 3: Experimental results of the transductive node classification task in the TI scenario.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Performance} \\\\\n\\hline\n\\text{Algorithm A} & \\text{Performance A} \\\\\n\\text{Algorithm B} & \\text{Performance B} \\\\\n\\text{Algorithm C} & \\text{Performance C} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. GSL outperforms baselines on homophily graph datasets.\n2. Models that leverage self-supervision achieve better performance.", "md": "For node-level representation learning, we conduct experiments on homogeneous graph datasets under both TR and TI scenarios, and use classification accuracy as our evaluation metric.\n\nTable 2: Experimental results of various GSL algorithms under the standard setting of transductive node classification task in the TR scenario.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Performance} \\\\\n\\hline\n\\text{Algorithm 1} & \\text{Performance 1} \\\\\n\\text{Algorithm 2} & \\text{Performance 2} \\\\\n\\text{Algorithm 3} & \\text{Performance 3} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. Most GSL algorithms generally show improvements in node classification task, particularly on datasets with high heterophily ratio.\n2. SUBLIME achieves optimal or near-optimal results on most datasets.\n3. The scalability of GSL still needs improvement, especially on large-scale datasets.\n\nTable 3: Experimental results of the transductive node classification task in the TI scenario.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Performance} \\\\\n\\hline\n\\text{Algorithm A} & \\text{Performance A} \\\\\n\\text{Algorithm B} & \\text{Performance B} \\\\\n\\text{Algorithm C} & \\text{Performance C} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. GSL outperforms baselines on homophily graph datasets.\n2. Models that leverage self-supervision achieve better performance."}, {"type": "heading", "lvl": 3, "value": "Performance on Heterogeneous Graph Node-level Representation Learning", "md": "### Performance on Heterogeneous Graph Node-level Representation Learning"}, {"type": "text", "value": "Table 4: Experimental results on heterogeneous graph datasets.\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Macro-F1} & \\text{Micro-F1} \\\\\n\\hline\n\\text{GTN} & \\text{F1 score} & \\text{F1 score} \\\\\n\\text{HGSL} & \\text{F1 score} & \\text{F1 score} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. GTN and HGSL generally outperform other models on heterogeneous graph datasets.\n2. GSL algorithms generally outperform vanilla GNN models.\n3. Models that consider heterogeneity perform significantly better on datasets with stronger heterogeneity.", "md": "Table 4: Experimental results on heterogeneous graph datasets.\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Algorithm} & \\text{Macro-F1} & \\text{Micro-F1} \\\\\n\\hline\n\\text{GTN} & \\text{F1 score} & \\text{F1 score} \\\\\n\\text{HGSL} & \\text{F1 score} & \\text{F1 score} \\\\\n\\hline\n\\end{array}\n$$\n\nObservations:\n\n1. GTN and HGSL generally outperform other models on heterogeneous graph datasets.\n2. GSL algorithms generally outperform vanilla GNN models.\n3. Models that consider heterogeneity perform significantly better on datasets with stronger heterogeneity."}]}, {"page": 6, "text": "Table 2: Accuracy \u00b1 STD comparison (%) under the standard setting of transductive node classifi-\ncation task in the Topology Refinement (TR) scenario, which means the original graph structure is\navailable for each method. Performance is averaged from 10 independent repetitions. The highest\nresults are highlighted with bold , while the second highest results are marked with underline .\n\"OOM\" denotes out of memory.\n                      Cora        Citeseer      Pubmed       ogbn-arxiv      Cornell        Texas       Wisconsin     Actor\n   Edge Hom.          0.81          0.74          0.80          0.65          0.12           0.06          0.18        0.22\n   GCN             81.46\u00b10.58    71.36\u00b10.31    79.18\u00b10.29     70.77\u00b10.19    47.84\u00b15.55    57.83\u00b12.76    57.45\u00b14.30  30.01\u00b10.77\n   GAT             81.41\u00b10.77    70.69\u00b10.58    77.85\u00b10.42    69.90\u00b10.25     46.22\u00b16.33    54.05\u00b17.35    57.65\u00b17.75  28.91\u00b10.83\n   GPRGNN          83.66\u00b10.77    71.64\u00b10.49    75.99\u00b11.63    50.80\u00b10.29     76.76\u00b15.30    85.14\u00b13.68    83.33\u00b13.42  34.09\u00b11.09\n   LDS             83.01\u00b10.41    73.55\u00b10.54      OOM            OOM         47.87\u00b17.14    58.92\u00b14.32    61.70\u00b13.58  31.05\u00b11.31\n   GRCN            83.87\u00b10.49    72.43\u00b10.61    78.92\u00b10.39       OOM         54.32\u00b18.24    62.16\u00b17.05    56.08\u00b17.19  29.97\u00b10.71\n   ProGNN          80.30\u00b10.57    68.51\u00b10.52      OOM            OOM         54.05\u00b16.16   48.37\u00b112.17    62.54\u00b17.56  22.35\u00b10.88\n   IDGL            83.88\u00b10.42    72.20\u00b11.18     80.00\u00b10.38      OOM         50.00\u00b18.98    62.43\u00b16.09    59.41\u00b14.11  28.16\u00b11.41\n   GEN             80.21\u00b11.72    71.15\u00b11.81    78.91\u00b10.69       OOM         57.02\u00b17.19    65.94\u00b11.38    66.07\u00b13.72  27.21\u00b12.05\n   CoGSL           81.76\u00b10.24    73.09\u00b10.42      OOM            OOM         52.16\u00b13.21    59.46\u00b14.36    58.82\u00b11.52  32.95\u00b11.20\n   SUBLIME         83.40\u00b10.42    72.30\u00b11.09     80.90\u00b10.94    71.75\u00b10.36    70.54\u00b15.98    77.03\u00b14.23    78.82\u00b16.55   33.57\u00b10.68\n   STABLE          80.20\u00b10.68    68.91\u00b11.01      OOM            OOM         44.03\u00b14.05    55.24\u00b16.04    53.00\u00b15.27  30.18\u00b11.00\n   NodeFormer      80.28\u00b10.82    71.31\u00b10.98    78.21\u00b11.43    55.40\u00b10.23     42.70\u00b15.51    58.92\u00b14.32    48.43\u00b17.02  25.51\u00b11.77\n   GSR             82.48\u00b10.43    71.10\u00b10.25    78.09\u00b10.53       OOM         44.32\u00b12.16    60.81\u00b14.87    56.86\u00b11.24  30.23\u00b10.38\nTable 3: Accuracy \u00b1 STD comparison (%) under the standard setting of transductive node classifica-\ntion task in the Topology Inference (TI) scenario, which means the original graph structure is not\navailable for each method.\n                       Cora        Citeseer      Pubmed       ogbn-arxiv      Cornell       Texas       Wisconsin     Actor\n   Edge Hom.           0.81          0.74          0.80          0.65          0.12          0.06          0.18        0.22\n   MLP              58.55\u00b10.80    59.52\u00b10.64    73.00\u00b10.30    55.21\u00b10.11    71.35\u00b16.19    80.27\u00b15.93    84.71\u00b13.14  35.49\u00b11.04\n   GCNknn           66.10\u00b10.44    68.33\u00b10.89    69.23\u00b10.49    55.21\u00b10.22    75.14\u00b12.65    75.95\u00b14.43    84.12\u00b13.97  32.98\u00b10.49\n   GATknn           64.62\u00b11.04    68.05\u00b11.12    68.76\u00b10.80     55.92\u00b10.30   74.05\u00b15.16    76.49\u00b14.99    82.16\u00b14.06  31.67\u00b11.19\n   GPRGNNknn        69.27\u00b10.62    70.29\u00b10.54    68.19\u00b11.19    51.39\u00b10.13    75.68\u00b12.70    81.08\u00b14.18    84.12\u00b13.22  34.71\u00b11.51\n   LDS              69.87\u00b10.41    72.43\u00b10.61      OOM            OOM        72.65\u00b13.86    70.20\u00b15.07    78.14\u00b14.50  32.39\u00b10.79\n   GRCNknn          69.48\u00b10.66    68.41\u00b10.50    68.96\u00b10.85       OOM        71.08\u00b16.84    74.32\u00b15.02    78.63\u00b14.92  30.83\u00b10.76\n   ProGNNknn        67.11\u00b10.56    64.55\u00b10.95      OOM            OOM        71.35\u00b14.04    71.89\u00b15.69    72.94\u00b17.93  31.56\u00b11.14\n   IDGLknn          69.74\u00b10.57    66.33\u00b10.84    74.01\u00b10.64       OOM        72.70\u00b14.75    75.40\u00b14.75    79.21\u00b13.94  33.07\u00b11.37\n   GENknn           66.95\u00b11.40    67.29\u00b11.17    69.76\u00b11.53       OOM        71.08\u00b15.54    74.59\u00b13.46    81.76\u00b12.91  31.28\u00b11.06\n   CoGSLknn         66.65\u00b10.37    68.72\u00b10.84      OOM            OOM        70.27\u00b13.42    72.70\u00b14.26    76.96\u00b15.25  34.52\u00b11.56\n   GSRknn           66.28\u00b10.59    66.77\u00b10.62    68.49\u00b11.49       OOM        70.27\u00b13.62    74.86\u00b13.63    78.62\u00b15.91  33.73\u00b11.12\n   SLAPS            72.28\u00b10.97    70.71\u00b11.13    74.50\u00b11.47    55.19\u00b10.21    74.59\u00b13.67    79.19\u00b14.99    81.96\u00b13.26  37.16\u00b10.91\n   SUBLIME          72.74\u00b11.91    72.63\u00b10.60    75.08\u00b10.55    55.57\u00b10.18    72.35\u00b13.57    75.51\u00b15.08    82.14\u00b12.62  32.20\u00b11.02\n   NodeFormer       54.35\u00b15.33    45.90\u00b15.42    59.83\u00b16.50    55.37\u00b10.23    42.70\u00b15.51    58.92\u00b14.32    48.24\u00b16.63  29.24\u00b11.68\n   HES-GSL           73.68\u00b11.04   70.12\u00b11.11     77.08\u00b10.78    56.46\u00b10.27   66.22\u00b16.19    74.05\u00b16.42    79.61\u00b15.28  36.73\u00b10.76\n4.4    Performance of GSL algorithms on graph-level tasks\nIn this section, we conduct graph classification experiments on four social datasets (i.e., IMDB-B,\nRDT-B, COLLAB, and IMDB-M) and two biological datasets (i.e., MUTAG and PROTEINS). Table 5 shows\nthe experimental results of average accuracy and the standard deviation of 10-fold cross-validation.\nWe can observe that HGP-SL (with GCN as the encoder) consistently outperforms GCN on all\ndatasets. However, we find that VIB-GSL exhibits strong instability across different random seeds.\nAnd due to the absence of training scripts in the official code2, we performed hyperparameter tuning\nbased on the parameter search space (\u03b2 \u2208                 {10\u22121, 10\u22122, 10\u22123, 10\u22124, 10\u22125, 10\u22126}) provided in the\npaper, but we are unable to surpass the performance of the baseline models consistently. Lastly, we\nconducted an analysis of graph-level GSL algorithms on long-range graph dataset [8]. For detailed\ninformation, please refer to Appendix B.\n    2https://github.com/RingBDStack/VIB-GSL\n                                                                6", "md": "# OCR Text\n\n## Table 2: Accuracy \u00b1 STD comparison (%) under the standard setting of transductive node classification task in the Topology Refinement (TR) scenario\n\nPerformance is averaged from 10 independent repetitions. The highest results are highlighted with bold, while the second highest results are marked with underline. \"OOM\" denotes out of memory.\n\n| |Cora|Citeseer|Pubmed|ogbn-arxiv|Cornell|Texas|Wisconsin|Actor|\n|---|---|---|---|---|---|---|---|---|\n|Edge Hom.|0.81|0.74|0.80|0.65|0.12|0.06|0.18|0.22|\n|GCN|$81.46\\pm0.58$|$71.36\\pm0.31$|$79.18\\pm0.29$|$70.77\\pm0.19$|$47.84\\pm5.55$|$57.83\\pm2.76$|$57.45\\pm4.30$|$30.01\\pm0.77$|\n|GAT|$81.41\\pm0.77$|$70.69\\pm0.58$|$77.85\\pm0.42$|$69.90\\pm0.25$|$46.22\\pm6.33$|$54.05\\pm7.35$|$57.65\\pm7.75$|$28.91\\pm0.83$|\n|GPRGNN|$83.66\\pm0.77$|$71.64\\pm0.49$|$75.99\\pm1.63$|$50.80\\pm0.29$|$76.76\\pm5.30$|$85.14\\pm3.68$|$83.33\\pm3.42$|$34.09\\pm1.09$|\n|LDS|$83.01\\pm0.41$|$73.55\\pm0.54$|OOM|OOM|$47.87\\pm7.14$|$58.92\\pm4.32$|$61.70\\pm3.58$|$31.05\\pm1.31$|\n|GRCN|$83.87\\pm0.49$|$72.43\\pm0.61$|$78.92\\pm0.39$|OOM|$54.32\\pm8.24$|$62.16\\pm7.05$|$56.08\\pm7.19$|$29.97\\pm0.71$|\n|ProGNN|$80.30\\pm0.57$|$68.51\\pm0.52$|OOM|OOM|$54.05\\pm6.16$|$48.37\\pm12.17$|$62.54\\pm7.56$|$22.35\\pm0.88$|\n|IDGL|$83.88\\pm0.42$|$72.20\\pm1.18$|$80.00\\pm0.38$|OOM|$50.00\\pm8.98$|$62.43\\pm6.09$|$59.41\\pm4.11$|$28.16\\pm1.41$|\n|GEN|$80.21\\pm1.72$|$71.15\\pm1.81$|$78.91\\pm0.69$|OOM|$57.02\\pm7.19$|$65.94\\pm1.38$|$66.07\\pm3.72$|$27.21\\pm2.05$|\n|CoGSL|$81.76\\pm0.24$|$73.09\\pm0.42$|OOM|OOM|$52.16\\pm3.21$|$59.46\\pm4.36$|$58.82\\pm1.52$|$32.95\\pm1.20$|\n|SUBLIME|$83.40\\pm0.42$|$72.30\\pm1.09$|$80.90\\pm0.94$|$71.75\\pm0.36$|$70.54\\pm5.98$|$77.03\\pm4.23$|$78.82\\pm6.55$|$33.57\\pm0.68$|\n|STABLE|$80.20\\pm0.68$|$68.91\\pm1.01$|OOM|OOM|$44.03\\pm4.05$|$55.24\\pm6.04$|$53.00\\pm5.27$|$30.18\\pm1.00$|\n|NodeFormer|$80.28\\pm0.82$|$71.31\\pm0.98$|$78.21\\pm1.43$|$55.40\\pm0.23$|$42.70\\pm5.51$|$58.92\\pm4.32$|$48.43\\pm7.02$|$25.51\\pm1.77$|\n|GSR|$82.48\\pm0.43$|$71.10\\pm0.25$|$78.09\\pm0.53$|OOM|$44.32\\pm2.16$|$60.81\\pm4.87$|$56.86\\pm1.24$|$30.23\\pm0.38$|\n\n## Table 3: Accuracy \u00b1 STD comparison (%) under the standard setting of transductive node classification task in the Topology Inference (TI) scenario\n\nPerformance is averaged from 10 independent repetitions.\n\n| |Cora|Citeseer|Pubmed|ogbn-arxiv|Cornell|Texas|Wisconsin|Actor|\n|---|---|---|---|---|---|---|---|---|\n|Edge Hom.|0.81|0.74|0.80|0.65|0.12|0.06|0.18|0.22|\n|MLP|$58.55\\pm0.80$|$59.52\\pm0.64$|$73.00\\pm0.30$|$55.21\\pm0.11$|$71.35\\pm6.19$|$80.27\\pm5.93$|$84.71\\pm3.14$|$35.49\\pm1.04$|\n|GCNknn|$66.10\\pm0.44$|$68.33\\pm0.89$|$69.23\\pm0.49$|$55.21\\pm0.22$|$75.14\\pm2.65$|$75.95\\pm4.43$|$84.12\\pm3.97$|$32.98\\pm0.49$|\n|GATknn|$64.62\\pm1.04$|$68.05\\pm1.12$|$68.76\\pm0.80$|$55.92\\pm0.30$|$74.05\\pm5.16$|$76.49\\pm4.99$|$82.16\\pm4.06$|$31.67\\pm1.19$|\n\n## Performance of GSL algorithms on graph-level tasks\n\nIn this section, we conduct graph classification experiments on four social datasets (i.e., IMDB-B, RDT-B, COLLAB, and IMDB-M) and two biological datasets (i.e., MUTAG and PROTEINS).\n\nTable 5 shows the experimental results of average accuracy and the standard deviation of 10-fold cross-validation.\n\nWe can observe that HGP-SL (with GCN as the encoder) consistently outperforms GCN on all datasets. However, we find that VIB-GSL exhibits strong instability across different random seeds.\n\nAnd due to the absence of training scripts in the official code, we performed hyperparameter tuning based on the parameter search space ($$\\beta \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\\}$$) provided in the paper, but we are unable to surpass the performance of the baseline models consistently.\n\nLastly, we conducted an analysis of graph-level GSL algorithms on long-range graph dataset. For detailed information, please refer to Appendix B.\n\nLink to VIB-GSL GitHub Repository", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "heading", "lvl": 2, "value": "Table 2: Accuracy \u00b1 STD comparison (%) under the standard setting of transductive node classification task in the Topology Refinement (TR) scenario", "md": "## Table 2: Accuracy \u00b1 STD comparison (%) under the standard setting of transductive node classification task in the Topology Refinement (TR) scenario"}, {"type": "text", "value": "Performance is averaged from 10 independent repetitions. The highest results are highlighted with bold, while the second highest results are marked with underline. \"OOM\" denotes out of memory.", "md": "Performance is averaged from 10 independent repetitions. The highest results are highlighted with bold, while the second highest results are marked with underline. \"OOM\" denotes out of memory."}, {"type": "table", "rows": [["", "Cora", "Citeseer", "Pubmed", "ogbn-arxiv", "Cornell", "Texas", "Wisconsin", "Actor"], ["Edge Hom.", "0.81", "0.74", "0.80", "0.65", "0.12", "0.06", "0.18", "0.22"], ["GCN", "$81.46\\pm0.58$", "$71.36\\pm0.31$", "$79.18\\pm0.29$", "$70.77\\pm0.19$", "$47.84\\pm5.55$", "$57.83\\pm2.76$", "$57.45\\pm4.30$", "$30.01\\pm0.77$"], ["GAT", "$81.41\\pm0.77$", "$70.69\\pm0.58$", "$77.85\\pm0.42$", "$69.90\\pm0.25$", "$46.22\\pm6.33$", "$54.05\\pm7.35$", "$57.65\\pm7.75$", "$28.91\\pm0.83$"], ["GPRGNN", "$83.66\\pm0.77$", "$71.64\\pm0.49$", "$75.99\\pm1.63$", "$50.80\\pm0.29$", "$76.76\\pm5.30$", "$85.14\\pm3.68$", "$83.33\\pm3.42$", "$34.09\\pm1.09$"], ["LDS", "$83.01\\pm0.41$", "$73.55\\pm0.54$", "OOM", "OOM", "$47.87\\pm7.14$", "$58.92\\pm4.32$", "$61.70\\pm3.58$", "$31.05\\pm1.31$"], ["GRCN", "$83.87\\pm0.49$", "$72.43\\pm0.61$", "$78.92\\pm0.39$", "OOM", "$54.32\\pm8.24$", "$62.16\\pm7.05$", "$56.08\\pm7.19$", "$29.97\\pm0.71$"], ["ProGNN", "$80.30\\pm0.57$", "$68.51\\pm0.52$", "OOM", "OOM", "$54.05\\pm6.16$", "$48.37\\pm12.17$", "$62.54\\pm7.56$", "$22.35\\pm0.88$"], ["IDGL", "$83.88\\pm0.42$", "$72.20\\pm1.18$", "$80.00\\pm0.38$", "OOM", "$50.00\\pm8.98$", "$62.43\\pm6.09$", "$59.41\\pm4.11$", "$28.16\\pm1.41$"], ["GEN", "$80.21\\pm1.72$", "$71.15\\pm1.81$", "$78.91\\pm0.69$", "OOM", "$57.02\\pm7.19$", "$65.94\\pm1.38$", "$66.07\\pm3.72$", "$27.21\\pm2.05$"], ["CoGSL", "$81.76\\pm0.24$", "$73.09\\pm0.42$", "OOM", "OOM", "$52.16\\pm3.21$", "$59.46\\pm4.36$", "$58.82\\pm1.52$", "$32.95\\pm1.20$"], ["SUBLIME", "$83.40\\pm0.42$", "$72.30\\pm1.09$", "$80.90\\pm0.94$", "$71.75\\pm0.36$", "$70.54\\pm5.98$", "$77.03\\pm4.23$", "$78.82\\pm6.55$", "$33.57\\pm0.68$"], ["STABLE", "$80.20\\pm0.68$", "$68.91\\pm1.01$", "OOM", "OOM", "$44.03\\pm4.05$", "$55.24\\pm6.04$", "$53.00\\pm5.27$", "$30.18\\pm1.00$"], ["NodeFormer", "$80.28\\pm0.82$", "$71.31\\pm0.98$", "$78.21\\pm1.43$", "$55.40\\pm0.23$", "$42.70\\pm5.51$", "$58.92\\pm4.32$", "$48.43\\pm7.02$", "$25.51\\pm1.77$"], ["GSR", "$82.48\\pm0.43$", "$71.10\\pm0.25$", "$78.09\\pm0.53$", "OOM", "$44.32\\pm2.16$", "$60.81\\pm4.87$", "$56.86\\pm1.24$", "$30.23\\pm0.38$"]], "md": "| |Cora|Citeseer|Pubmed|ogbn-arxiv|Cornell|Texas|Wisconsin|Actor|\n|---|---|---|---|---|---|---|---|---|\n|Edge Hom.|0.81|0.74|0.80|0.65|0.12|0.06|0.18|0.22|\n|GCN|$81.46\\pm0.58$|$71.36\\pm0.31$|$79.18\\pm0.29$|$70.77\\pm0.19$|$47.84\\pm5.55$|$57.83\\pm2.76$|$57.45\\pm4.30$|$30.01\\pm0.77$|\n|GAT|$81.41\\pm0.77$|$70.69\\pm0.58$|$77.85\\pm0.42$|$69.90\\pm0.25$|$46.22\\pm6.33$|$54.05\\pm7.35$|$57.65\\pm7.75$|$28.91\\pm0.83$|\n|GPRGNN|$83.66\\pm0.77$|$71.64\\pm0.49$|$75.99\\pm1.63$|$50.80\\pm0.29$|$76.76\\pm5.30$|$85.14\\pm3.68$|$83.33\\pm3.42$|$34.09\\pm1.09$|\n|LDS|$83.01\\pm0.41$|$73.55\\pm0.54$|OOM|OOM|$47.87\\pm7.14$|$58.92\\pm4.32$|$61.70\\pm3.58$|$31.05\\pm1.31$|\n|GRCN|$83.87\\pm0.49$|$72.43\\pm0.61$|$78.92\\pm0.39$|OOM|$54.32\\pm8.24$|$62.16\\pm7.05$|$56.08\\pm7.19$|$29.97\\pm0.71$|\n|ProGNN|$80.30\\pm0.57$|$68.51\\pm0.52$|OOM|OOM|$54.05\\pm6.16$|$48.37\\pm12.17$|$62.54\\pm7.56$|$22.35\\pm0.88$|\n|IDGL|$83.88\\pm0.42$|$72.20\\pm1.18$|$80.00\\pm0.38$|OOM|$50.00\\pm8.98$|$62.43\\pm6.09$|$59.41\\pm4.11$|$28.16\\pm1.41$|\n|GEN|$80.21\\pm1.72$|$71.15\\pm1.81$|$78.91\\pm0.69$|OOM|$57.02\\pm7.19$|$65.94\\pm1.38$|$66.07\\pm3.72$|$27.21\\pm2.05$|\n|CoGSL|$81.76\\pm0.24$|$73.09\\pm0.42$|OOM|OOM|$52.16\\pm3.21$|$59.46\\pm4.36$|$58.82\\pm1.52$|$32.95\\pm1.20$|\n|SUBLIME|$83.40\\pm0.42$|$72.30\\pm1.09$|$80.90\\pm0.94$|$71.75\\pm0.36$|$70.54\\pm5.98$|$77.03\\pm4.23$|$78.82\\pm6.55$|$33.57\\pm0.68$|\n|STABLE|$80.20\\pm0.68$|$68.91\\pm1.01$|OOM|OOM|$44.03\\pm4.05$|$55.24\\pm6.04$|$53.00\\pm5.27$|$30.18\\pm1.00$|\n|NodeFormer|$80.28\\pm0.82$|$71.31\\pm0.98$|$78.21\\pm1.43$|$55.40\\pm0.23$|$42.70\\pm5.51$|$58.92\\pm4.32$|$48.43\\pm7.02$|$25.51\\pm1.77$|\n|GSR|$82.48\\pm0.43$|$71.10\\pm0.25$|$78.09\\pm0.53$|OOM|$44.32\\pm2.16$|$60.81\\pm4.87$|$56.86\\pm1.24$|$30.23\\pm0.38$|", "isPerfectTable": true, "csv": "\"\",\"Cora\",\"Citeseer\",\"Pubmed\",\"ogbn-arxiv\",\"Cornell\",\"Texas\",\"Wisconsin\",\"Actor\"\n\"Edge Hom.\",\"0.81\",\"0.74\",\"0.80\",\"0.65\",\"0.12\",\"0.06\",\"0.18\",\"0.22\"\n\"GCN\",\"$81.46\\pm0.58$\",\"$71.36\\pm0.31$\",\"$79.18\\pm0.29$\",\"$70.77\\pm0.19$\",\"$47.84\\pm5.55$\",\"$57.83\\pm2.76$\",\"$57.45\\pm4.30$\",\"$30.01\\pm0.77$\"\n\"GAT\",\"$81.41\\pm0.77$\",\"$70.69\\pm0.58$\",\"$77.85\\pm0.42$\",\"$69.90\\pm0.25$\",\"$46.22\\pm6.33$\",\"$54.05\\pm7.35$\",\"$57.65\\pm7.75$\",\"$28.91\\pm0.83$\"\n\"GPRGNN\",\"$83.66\\pm0.77$\",\"$71.64\\pm0.49$\",\"$75.99\\pm1.63$\",\"$50.80\\pm0.29$\",\"$76.76\\pm5.30$\",\"$85.14\\pm3.68$\",\"$83.33\\pm3.42$\",\"$34.09\\pm1.09$\"\n\"LDS\",\"$83.01\\pm0.41$\",\"$73.55\\pm0.54$\",\"OOM\",\"OOM\",\"$47.87\\pm7.14$\",\"$58.92\\pm4.32$\",\"$61.70\\pm3.58$\",\"$31.05\\pm1.31$\"\n\"GRCN\",\"$83.87\\pm0.49$\",\"$72.43\\pm0.61$\",\"$78.92\\pm0.39$\",\"OOM\",\"$54.32\\pm8.24$\",\"$62.16\\pm7.05$\",\"$56.08\\pm7.19$\",\"$29.97\\pm0.71$\"\n\"ProGNN\",\"$80.30\\pm0.57$\",\"$68.51\\pm0.52$\",\"OOM\",\"OOM\",\"$54.05\\pm6.16$\",\"$48.37\\pm12.17$\",\"$62.54\\pm7.56$\",\"$22.35\\pm0.88$\"\n\"IDGL\",\"$83.88\\pm0.42$\",\"$72.20\\pm1.18$\",\"$80.00\\pm0.38$\",\"OOM\",\"$50.00\\pm8.98$\",\"$62.43\\pm6.09$\",\"$59.41\\pm4.11$\",\"$28.16\\pm1.41$\"\n\"GEN\",\"$80.21\\pm1.72$\",\"$71.15\\pm1.81$\",\"$78.91\\pm0.69$\",\"OOM\",\"$57.02\\pm7.19$\",\"$65.94\\pm1.38$\",\"$66.07\\pm3.72$\",\"$27.21\\pm2.05$\"\n\"CoGSL\",\"$81.76\\pm0.24$\",\"$73.09\\pm0.42$\",\"OOM\",\"OOM\",\"$52.16\\pm3.21$\",\"$59.46\\pm4.36$\",\"$58.82\\pm1.52$\",\"$32.95\\pm1.20$\"\n\"SUBLIME\",\"$83.40\\pm0.42$\",\"$72.30\\pm1.09$\",\"$80.90\\pm0.94$\",\"$71.75\\pm0.36$\",\"$70.54\\pm5.98$\",\"$77.03\\pm4.23$\",\"$78.82\\pm6.55$\",\"$33.57\\pm0.68$\"\n\"STABLE\",\"$80.20\\pm0.68$\",\"$68.91\\pm1.01$\",\"OOM\",\"OOM\",\"$44.03\\pm4.05$\",\"$55.24\\pm6.04$\",\"$53.00\\pm5.27$\",\"$30.18\\pm1.00$\"\n\"NodeFormer\",\"$80.28\\pm0.82$\",\"$71.31\\pm0.98$\",\"$78.21\\pm1.43$\",\"$55.40\\pm0.23$\",\"$42.70\\pm5.51$\",\"$58.92\\pm4.32$\",\"$48.43\\pm7.02$\",\"$25.51\\pm1.77$\"\n\"GSR\",\"$82.48\\pm0.43$\",\"$71.10\\pm0.25$\",\"$78.09\\pm0.53$\",\"OOM\",\"$44.32\\pm2.16$\",\"$60.81\\pm4.87$\",\"$56.86\\pm1.24$\",\"$30.23\\pm0.38$\""}, {"type": "heading", "lvl": 2, "value": "Table 3: Accuracy \u00b1 STD comparison (%) under the standard setting of transductive node classification task in the Topology Inference (TI) scenario", "md": "## Table 3: Accuracy \u00b1 STD comparison (%) under the standard setting of transductive node classification task in the Topology Inference (TI) scenario"}, {"type": "text", "value": "Performance is averaged from 10 independent repetitions.", "md": "Performance is averaged from 10 independent repetitions."}, {"type": "table", "rows": [["", "Cora", "Citeseer", "Pubmed", "ogbn-arxiv", "Cornell", "Texas", "Wisconsin", "Actor"], ["Edge Hom.", "0.81", "0.74", "0.80", "0.65", "0.12", "0.06", "0.18", "0.22"], ["MLP", "$58.55\\pm0.80$", "$59.52\\pm0.64$", "$73.00\\pm0.30$", "$55.21\\pm0.11$", "$71.35\\pm6.19$", "$80.27\\pm5.93$", "$84.71\\pm3.14$", "$35.49\\pm1.04$"], ["GCNknn", "$66.10\\pm0.44$", "$68.33\\pm0.89$", "$69.23\\pm0.49$", "$55.21\\pm0.22$", "$75.14\\pm2.65$", "$75.95\\pm4.43$", "$84.12\\pm3.97$", "$32.98\\pm0.49$"], ["GATknn", "$64.62\\pm1.04$", "$68.05\\pm1.12$", "$68.76\\pm0.80$", "$55.92\\pm0.30$", "$74.05\\pm5.16$", "$76.49\\pm4.99$", "$82.16\\pm4.06$", "$31.67\\pm1.19$"]], "md": "| |Cora|Citeseer|Pubmed|ogbn-arxiv|Cornell|Texas|Wisconsin|Actor|\n|---|---|---|---|---|---|---|---|---|\n|Edge Hom.|0.81|0.74|0.80|0.65|0.12|0.06|0.18|0.22|\n|MLP|$58.55\\pm0.80$|$59.52\\pm0.64$|$73.00\\pm0.30$|$55.21\\pm0.11$|$71.35\\pm6.19$|$80.27\\pm5.93$|$84.71\\pm3.14$|$35.49\\pm1.04$|\n|GCNknn|$66.10\\pm0.44$|$68.33\\pm0.89$|$69.23\\pm0.49$|$55.21\\pm0.22$|$75.14\\pm2.65$|$75.95\\pm4.43$|$84.12\\pm3.97$|$32.98\\pm0.49$|\n|GATknn|$64.62\\pm1.04$|$68.05\\pm1.12$|$68.76\\pm0.80$|$55.92\\pm0.30$|$74.05\\pm5.16$|$76.49\\pm4.99$|$82.16\\pm4.06$|$31.67\\pm1.19$|", "isPerfectTable": true, "csv": "\"\",\"Cora\",\"Citeseer\",\"Pubmed\",\"ogbn-arxiv\",\"Cornell\",\"Texas\",\"Wisconsin\",\"Actor\"\n\"Edge Hom.\",\"0.81\",\"0.74\",\"0.80\",\"0.65\",\"0.12\",\"0.06\",\"0.18\",\"0.22\"\n\"MLP\",\"$58.55\\pm0.80$\",\"$59.52\\pm0.64$\",\"$73.00\\pm0.30$\",\"$55.21\\pm0.11$\",\"$71.35\\pm6.19$\",\"$80.27\\pm5.93$\",\"$84.71\\pm3.14$\",\"$35.49\\pm1.04$\"\n\"GCNknn\",\"$66.10\\pm0.44$\",\"$68.33\\pm0.89$\",\"$69.23\\pm0.49$\",\"$55.21\\pm0.22$\",\"$75.14\\pm2.65$\",\"$75.95\\pm4.43$\",\"$84.12\\pm3.97$\",\"$32.98\\pm0.49$\"\n\"GATknn\",\"$64.62\\pm1.04$\",\"$68.05\\pm1.12$\",\"$68.76\\pm0.80$\",\"$55.92\\pm0.30$\",\"$74.05\\pm5.16$\",\"$76.49\\pm4.99$\",\"$82.16\\pm4.06$\",\"$31.67\\pm1.19$\""}, {"type": "heading", "lvl": 2, "value": "Performance of GSL algorithms on graph-level tasks", "md": "## Performance of GSL algorithms on graph-level tasks"}, {"type": "text", "value": "In this section, we conduct graph classification experiments on four social datasets (i.e., IMDB-B, RDT-B, COLLAB, and IMDB-M) and two biological datasets (i.e., MUTAG and PROTEINS).\n\nTable 5 shows the experimental results of average accuracy and the standard deviation of 10-fold cross-validation.\n\nWe can observe that HGP-SL (with GCN as the encoder) consistently outperforms GCN on all datasets. However, we find that VIB-GSL exhibits strong instability across different random seeds.\n\nAnd due to the absence of training scripts in the official code, we performed hyperparameter tuning based on the parameter search space ($$\\beta \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\\}$$) provided in the paper, but we are unable to surpass the performance of the baseline models consistently.\n\nLastly, we conducted an analysis of graph-level GSL algorithms on long-range graph dataset. For detailed information, please refer to Appendix B.\n\nLink to VIB-GSL GitHub Repository", "md": "In this section, we conduct graph classification experiments on four social datasets (i.e., IMDB-B, RDT-B, COLLAB, and IMDB-M) and two biological datasets (i.e., MUTAG and PROTEINS).\n\nTable 5 shows the experimental results of average accuracy and the standard deviation of 10-fold cross-validation.\n\nWe can observe that HGP-SL (with GCN as the encoder) consistently outperforms GCN on all datasets. However, we find that VIB-GSL exhibits strong instability across different random seeds.\n\nAnd due to the absence of training scripts in the official code, we performed hyperparameter tuning based on the parameter search space ($$\\beta \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\\}$$) provided in the paper, but we are unable to surpass the performance of the baseline models consistently.\n\nLastly, we conducted an analysis of graph-level GSL algorithms on long-range graph dataset. For detailed information, please refer to Appendix B.\n\nLink to VIB-GSL GitHub Repository"}]}, {"page": 7, "text": "Table 4: Macro-F1 and Micro-F1 \u00b1 STD comparison (%) under the standard setting of heterogeneous\n node classification task.\n               Method                      Macro-F1         ACM     Micro-F1                 Macro-F1       DBLP      Micro-F1                Macro-F1          Yelp    Micro-F1\n               GCN                         90.27\u00b10.59              90.18\u00b10.61               90.01\u00b10.32               90.99\u00b10.28               78.01\u00b11.89               81.03\u00b11.81\n               GAT                         91.52\u00b10.62              91.46\u00b10.62               90.22\u00b10.37               91.13\u00b10.40               82.12\u00b11.47               84.43\u00b11.56\n               HAN                         91.67\u00b10.39              91.47\u00b10.22               90.53\u00b10.24               91.47\u00b10.22               88.49\u00b11.73               88.78\u00b11.40\n               LDS                         92.35\u00b10.43              92.05\u00b10.26               88.11\u00b10.86               88.74\u00b10.85               75.98\u00b12.35               78.14\u00b11.98\n               GRCN                        93.04\u00b10.17              92.94\u00b10.18               88.33\u00b10.47               89.43\u00b10.44               76.05\u00b11.05               80.68\u00b10.96\n               IDGL                        91.69\u00b11.24              91.63\u00b11.24               89.65\u00b10.60               90.61\u00b10.56               76.98\u00b15.78               79.15\u00b15.06\n               ProGNN                      90.57\u00b11.03              90.50\u00b11.29               83.13\u00b11.56               84.83\u00b11.36               51.76\u00b11.46               58.39\u00b11.25\n               GEN                         87.91\u00b12.78              87.88\u00b12.61               89.74\u00b10.69               90.65\u00b10.71               80.43\u00b13.78               82.68\u00b12.84\n               STABLE                      83.54\u00b14.20              83.38\u00b14.51               75.18\u00b11.95               76.42\u00b11.95               71.48\u00b14.71               76.62\u00b12.75\n               GEN                         87.91\u00b12.78              87.88\u00b12.61               89.74\u00b10.69               90.65\u00b10.71               80.43\u00b13.78               82.68\u00b12.84\n               SUBLIME                     92.42\u00b10.16              92.13\u00b10.37               90.98\u00b10.37               91.82\u00b10.27               79.68\u00b10.79               82.99\u00b10.82\n               NodeFormer                  91.33\u00b10.77              90.60\u00b10.95               79.54\u00b10.78               80.56\u00b10.62               91.69\u00b10.65               90.59\u00b11.21\n               GSR                         92.14\u00b11.08              92.11\u00b10.99               76.59\u00b10.45               77.69\u00b10.42               83.85\u00b10.76               85.73\u00b10.54\n               GTN                         92.04\u00b10.38              91.94\u00b10.39               90.52\u00b10.45               91.48\u00b10.39               92.98\u00b10.52               92.44\u00b10.46\n               HGSL                        93.23\u00b10.50               93.13\u00b10.51               91.58\u00b10.40               92.49\u00b10.35               92.79\u00b10.44               92.24\u00b10.48\n      Table 5: Accuracy \u00b1 STD comparison (%) under the setting of graph-level classification task.\n        Method                                  IMDB-B                    RDT-B                 COLLAB                   IMDB-M                   MUTAG                   PROTEINS\n        GCN                                    73.20\u00b14.29               70.10\u00b15.80              76.96\u00b12.28              49.85\u00b13.84               73.92\u00b18.84                67.52\u00b16.71\n        VIB-GSL (GCN)                          71.90\u00b14.48               68.95\u00b12.66              77.14\u00b11.59              49.05\u00b15.52               68.63\u00b15.15                65.68\u00b18.53\n        HGP-SL (GCN)                           74.10\u00b14.55                   OOM                 78.06\u00b12.17              51.07\u00b12.00              78.07\u00b110.85                70.80\u00b14.25\n        GAT                                    72.30\u00b12.26               73.55\u00b14.76              79.08\u00b11.36              48.90\u00b12.98               78.71\u00b17.51                68.63\u00b16.24\n        VIB-GSL (GAT)                          72.10\u00b15.69                   OOM                 77.54\u00b11.85              49.06\u00b14.55               77.13\u00b19.95                67.09\u00b18.43\n        SAGE                                   72.60\u00b13.69               70.20\u00b14.11              75.58\u00b12.04              48.55\u00b12.03               68.65\u00b14.31                64.47\u00b17.15\n        VIB-GSL (SAGE)                         73.00\u00b14.78               65.75\u00b13.17              77.74\u00b11.52              48.79\u00b15.06              72.81\u00b111.41                66.61\u00b14.48\n        HGP-SL (SAGE)                          71.50\u00b15.24                   OOM                 78.64\u00b11.47              49.67\u00b13.09               77.13\u00b13.29                73.32\u00b12.06\n        GIN                                    73.00\u00b12.67               71.70\u00b15.01              79.86\u00b11.64              50.30\u00b13.52               87.19\u00b18.05                69.07\u00b15.62\n        VIB-GSL (GIN)                          69.90\u00b13.90               75.85\u00b13.63              77.25\u00b12.34              49.97\u00b13.65              85.18\u00b110.11                75.15\u00b15.72\n        HGP-SL (GIN)                           73.50\u00b16.25                   OOM                 80.14\u00b11.51              48.67\u00b12.58               73.92\u00b16.24                69.37\u00b13.95\n 4.5        Robustness analysis of GSL algorithms\nTo investigate the robustness of GSL algorithms, we primarily focus on three aspects: structure robust-\n ness, feature robustness, and supervision signal robustness. Due to limited space, we predominantly\n investigate the transductive node classification task in our paper. Nevertheless, researchers can utilize\n our GSLB library to efficiently and conveniently conduct experiments on other tasks as well.\n Robustness analysis with re-                                                                         Cora                                                         Citeseer\n spect to different supervision                                                                                                            70\n signals.             We have discovered                                  Accuracy (%)                                                    Accuracy (%)\n                                                                           80                                                              65\n that GSL maintains surprising                                                                                                             60\n performance in scenarios with                                             75          GCN                                                             GCN\n                                                                                       DAGNN                                               55          DAGNN\n a scarcity of labeled samples.                                                        Self-Training                                                   Self-Training\n                                                                                       GRCN                                                50          GRCN\nWe varied the number of labels                                             70          IDGL                                                            IDGL\n per class in the Cora and Cite-                                                       SUBLIME                                             45          SUBLIME\n seer datasets and selected two                                                 20               10               5               3             20               10               5    3\n                                                                                             Labels per class                                                Labels per class\n baseline models, DAGNN [25]\n and Self-Training [22], that per-                                    Figure 2: Performance of baselines and GSL algorithms with respect to\n formed well in scenarios with                                        different numbers of labels per class on Cora and Citeseer.\n limited labels. As shown in Fig-\n                                                                                                  7", "md": "Table 4: Macro-F1 and Micro-F1 \u00b1 STD comparison (%) under the standard setting of heterogeneous node classification task.\n\nMethod\nMacro-F1\nACM\nMicro-F1\nMacro-F1\nDBLP\nMicro-F1\nMacro-F1\nYelp\nMicro-F1\n\nGCN\n90.27\u00b10.59\n\n90.18\u00b10.61\n\n90.01\u00b10.32\n\n90.99\u00b10.28\n78.01\u00b11.89\n81.03\u00b11.81\n\nGAT\n91.52\u00b10.62\n\n91.46\u00b10.62\n\n90.22\u00b10.37\n\n91.13\u00b10.40\n82.12\u00b11.47\n84.43\u00b11.56\n\nHAN\n91.67\u00b10.39\n\n91.47\u00b10.22\n\n90.53\u00b10.24\n\n91.47\u00b10.22\n88.49\u00b11.73\n88.78\u00b11.40\n\nLDS\n92.35\u00b10.43\n\n92.05\u00b10.26\n\n88.11\u00b10.86\n\n88.74\u00b10.85\n75.98\u00b12.35\n78.14\u00b11.98\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\text{Macro-F1} & \\text{ACM} & \\text{Micro-F1} & \\text{Macro-F1} & \\text{DBLP} & \\text{Micro-F1} & \\text{Macro-F1} & \\text{Yelp} & \\text{Micro-F1} \\\\\n\\hline\n\\text{GCN} & 90.27\\pm0.59 & & 90.18\\pm0.61 & & 90.01\\pm0.32 & & 90.99\\pm0.28 & 78.01\\pm1.89 & 81.03\\pm1.81 \\\\\n\\text{GAT} & 91.52\\pm0.62 & & 91.46\\pm0.62 & & 90.22\\pm0.37 & & 91.13\\pm0.40 & 82.12\\pm1.47 & 84.43\\pm1.56 \\\\\n\\text{HAN} & 91.67\\pm0.39 & & 91.47\\pm0.22 & & 90.53\\pm0.24 & & 91.47\\pm0.22 & 88.49\\pm1.73 & 88.78\\pm1.40 \\\\\n\\text{LDS} & 92.35\\pm0.43 & & 92.05\\pm0.26 & & 88.11\\pm0.86 & & 88.74\\pm0.85 & 75.98\\pm2.35 & 78.14\\pm1.98 \\\\\n\\hline\n\\end{array}\n$$", "images": [], "items": [{"type": "text", "value": "Table 4: Macro-F1 and Micro-F1 \u00b1 STD comparison (%) under the standard setting of heterogeneous node classification task.\n\nMethod\nMacro-F1\nACM\nMicro-F1\nMacro-F1\nDBLP\nMicro-F1\nMacro-F1\nYelp\nMicro-F1\n\nGCN\n90.27\u00b10.59\n\n90.18\u00b10.61\n\n90.01\u00b10.32\n\n90.99\u00b10.28\n78.01\u00b11.89\n81.03\u00b11.81\n\nGAT\n91.52\u00b10.62\n\n91.46\u00b10.62\n\n90.22\u00b10.37\n\n91.13\u00b10.40\n82.12\u00b11.47\n84.43\u00b11.56\n\nHAN\n91.67\u00b10.39\n\n91.47\u00b10.22\n\n90.53\u00b10.24\n\n91.47\u00b10.22\n88.49\u00b11.73\n88.78\u00b11.40\n\nLDS\n92.35\u00b10.43\n\n92.05\u00b10.26\n\n88.11\u00b10.86\n\n88.74\u00b10.85\n75.98\u00b12.35\n78.14\u00b11.98\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\text{Macro-F1} & \\text{ACM} & \\text{Micro-F1} & \\text{Macro-F1} & \\text{DBLP} & \\text{Micro-F1} & \\text{Macro-F1} & \\text{Yelp} & \\text{Micro-F1} \\\\\n\\hline\n\\text{GCN} & 90.27\\pm0.59 & & 90.18\\pm0.61 & & 90.01\\pm0.32 & & 90.99\\pm0.28 & 78.01\\pm1.89 & 81.03\\pm1.81 \\\\\n\\text{GAT} & 91.52\\pm0.62 & & 91.46\\pm0.62 & & 90.22\\pm0.37 & & 91.13\\pm0.40 & 82.12\\pm1.47 & 84.43\\pm1.56 \\\\\n\\text{HAN} & 91.67\\pm0.39 & & 91.47\\pm0.22 & & 90.53\\pm0.24 & & 91.47\\pm0.22 & 88.49\\pm1.73 & 88.78\\pm1.40 \\\\\n\\text{LDS} & 92.35\\pm0.43 & & 92.05\\pm0.26 & & 88.11\\pm0.86 & & 88.74\\pm0.85 & 75.98\\pm2.35 & 78.14\\pm1.98 \\\\\n\\hline\n\\end{array}\n$$", "md": "Table 4: Macro-F1 and Micro-F1 \u00b1 STD comparison (%) under the standard setting of heterogeneous node classification task.\n\nMethod\nMacro-F1\nACM\nMicro-F1\nMacro-F1\nDBLP\nMicro-F1\nMacro-F1\nYelp\nMicro-F1\n\nGCN\n90.27\u00b10.59\n\n90.18\u00b10.61\n\n90.01\u00b10.32\n\n90.99\u00b10.28\n78.01\u00b11.89\n81.03\u00b11.81\n\nGAT\n91.52\u00b10.62\n\n91.46\u00b10.62\n\n90.22\u00b10.37\n\n91.13\u00b10.40\n82.12\u00b11.47\n84.43\u00b11.56\n\nHAN\n91.67\u00b10.39\n\n91.47\u00b10.22\n\n90.53\u00b10.24\n\n91.47\u00b10.22\n88.49\u00b11.73\n88.78\u00b11.40\n\nLDS\n92.35\u00b10.43\n\n92.05\u00b10.26\n\n88.11\u00b10.86\n\n88.74\u00b10.85\n75.98\u00b12.35\n78.14\u00b11.98\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\text{Macro-F1} & \\text{ACM} & \\text{Micro-F1} & \\text{Macro-F1} & \\text{DBLP} & \\text{Micro-F1} & \\text{Macro-F1} & \\text{Yelp} & \\text{Micro-F1} \\\\\n\\hline\n\\text{GCN} & 90.27\\pm0.59 & & 90.18\\pm0.61 & & 90.01\\pm0.32 & & 90.99\\pm0.28 & 78.01\\pm1.89 & 81.03\\pm1.81 \\\\\n\\text{GAT} & 91.52\\pm0.62 & & 91.46\\pm0.62 & & 90.22\\pm0.37 & & 91.13\\pm0.40 & 82.12\\pm1.47 & 84.43\\pm1.56 \\\\\n\\text{HAN} & 91.67\\pm0.39 & & 91.47\\pm0.22 & & 90.53\\pm0.24 & & 91.47\\pm0.22 & 88.49\\pm1.73 & 88.78\\pm1.40 \\\\\n\\text{LDS} & 92.35\\pm0.43 & & 92.05\\pm0.26 & & 88.11\\pm0.86 & & 88.74\\pm0.85 & 75.98\\pm2.35 & 78.14\\pm1.98 \\\\\n\\hline\n\\end{array}\n$$"}]}, {"page": 8, "text": "Table 6: Accuracy \u00b1 STD comparison (%) with respect to different perturbation rates. Jaccard and\nSimPGCN are representative state-of-the-art defense GNNs.\n   Dataset             Ptb Rate               GCN                 Jaccard             SimPGCN                   IDGL                 GRCN           ProGNN              STABLE               SUBLIME\n                           0%             83.68\u00b10.37            83.78\u00b10.50            82.66\u00b10.48            84.69\u00b11.13            84.43\u00b10.26       84.53\u00b10.89          83.70\u00b10.30             83.84\u00b10.28\n                           5%             80.61\u00b10.39            81.44\u00b10.48            80.35\u00b10.82            82.56\u00b10.24            81.34\u00b10.50       81.47\u00b10.44          81.52\u00b10.85             79.93\u00b10.58\n   Cora                   10%             74.38\u00b10.59            75.90\u00b10.64            76.50\u00b11.12            78.06\u00b10.62            77.12\u00b10.38       72.61\u00b10.73          78.64\u00b11.82             78.71\u00b10.46\n                          15%             65.17\u00b10.99            77.14\u00b10.70            73.77\u00b11.88            76.88\u00b10.44            73.74\u00b10.61       65.68\u00b11.97          79.70\u00b11.71             79.34\u00b10.61\n                          20%             61.98\u00b11.23            70.71\u00b10.91            69.08\u00b12.78            67.19\u00b10.69            69.54\u00b10.58       61.07\u00b10.61          76.44\u00b12.47             75.25\u00b11.08\n                           0%             76.56\u00b10.36            74.34\u00b10.26            74.35\u00b10.74            73.87\u00b10.70            76.34\u00b10.11       73.36\u00b11.52          72.65\u00b11.36             73.34\u00b11.17\n                           5%             72.51\u00b10.30            70.01\u00b10.79            72.99\u00b11.05            72.46\u00b10.47            74.66\u00b10.27       71.46\u00b10.47          69.66\u00b10.95             72.63\u00b10.50\n   Citeseer               10%             71.92\u00b10.68            70.28\u00b11.30            72.68\u00b10.54            69.72\u00b10.59            74.06\u00b10.43       69.03\u00b10.60          72.79\u00b10.71             73.02\u00b10.29\n                          15%             64.44\u00b10.53            67.13\u00b11.28            71.74\u00b11.46            62.83\u00b11.28            66.46\u00b11.12       65.42\u00b11.20          70.98\u00b10.61             73.90\u00b10.52\n                          20%             57.51\u00b11.03            67.82\u00b10.74            70.06\u00b11.86            61.16\u00b10.99            69.42\u00b11.14       57.51\u00b10.36          71.90\u00b11.12             72.55\u00b10.62\n                           0%             95.62\u00b10.69            94.93\u00b10.28            94.50\u00b10.43            94.83\u00b10.20            95.65\u00b10.28       94.84\u00b10.19          95.63\u00b10.32             95.27\u00b10.51\n                           5%             80.57\u00b10.66            78.17\u00b10.55            76.02\u00b11.14            79.62\u00b10.65            93.70\u00b10.18       92.36\u00b10.42          89.41\u00b11.63             93.24\u00b11.50\n   Polblogs               10%             71.83\u00b12.37            71.86\u00b11.34            70.12\u00b11.10            74.54\u00b10.69            87.99\u00b11.56       84.66\u00b10.52          89.87\u00b10.82             93.62\u00b10.50\n                          15%             66.38\u00b12.17            69.93\u00b10.66            64.19\u00b11.55            75.53\u00b10.83            71.85\u00b11.58       77.38\u00b10.51          89.94\u00b10.89             94.29\u00b10.27\n                          20%             68.19\u00b12.24            69.22\u00b10.34            63.64\u00b11.41            71.63\u00b10.62            71.73\u00b11.58       73.57\u00b10.29          87.42\u00b10.69             92.60\u00b10.72\n                                          GCN                 GRCN                  LDS                Pro-GNN                    STABLE               IDGL              GEN\n      85                   Cora                             75                Citeseer                           85                    Cora                     75                 Citeseer\n                                                                                                                                                                70\n    Accuracy (%)                                          Accuracy (%)                                          Accuracy (%)                                   Accuracy (%)\n      80\n                                                            70                                                   80\n      75                                                                                                                                                        65\n      70                                                    65                                                   75                                             60\n      65                                                    60                                                   70                                             55\n      60                                                                                                         65                                             50\n      55                                                    55                                                                                                  45\n          0.0      0.2      0.4      0.6      0.8               0.0      0.2      0.4      0.6     0.8                0.0      0.2     0.4    0.6   0.8              0.0     0.2      0.4      0.6      0.8\n               Edge Deletion Rate                                    Edge Deletion Rate                                   Edge Addition Rate                             Edge Addition Rate\n                 Figure 3: Analysis of robustness when injecting random noise on Cora and Citeseer.\nure 2, we can observe that GSL algorithms (for the sake of brevity, we opted to select only three\nmodels: GRCN, IDGL, and SUBLIME) achieve the best results in scenarios with fewer labels\navailable. We speculate that this may be because the learned graph structure is denser and exhibits\ncleaner community boundaries. As a result, the supervision signals can propagate more effectively\nwithin such a structure.\nRobustness analysis with respect to random noise. We randomly remove edges from or add\nedges to the original graph structures of Cora and Citeseer, then evaluated the performance of\nGSL algorithms on the corrupted graphs. We change the ratios of modified edges from 0 to 0.9\nto simulate different attack intensities. As shown in Figure 3, as the noise intensity increases, the\nmodels\u2019 performance generally exhibits a downward trend. And we can observe that GSL algorithms\ncommonly demonstrate a certain degree of robustness, as they tend to exhibit more stable performance\nthan GCN when random noise is injected. Besides, we also found that, due to variations in the graph\nmodeling process, different algorithms display varying levels of robustness when facing edge deletion\nand edge addition scenarios. For example, GRCN demonstrates strong robustness in edge deletion\nscenarios. However, in the edge addition scenarios, it only exhibits slight performance improvements\ncompared to GCN. On the contrary, STABLE exhibits strong robustness in the edge deletion scenario,\nwhile showing the opposite trend in edge addition.\nRobust analysis with respect to graph topology attack. Following [21, 55], we conduct robust\nanalysis on three graph datasets, i.e., Cora, Citeseer, and Polblogs. First, we select the largest\nconnected component in the graph, and utilize Mettack [58], a non-targeted adversarial topology\nattack method, to generate perturbed graphs. We select the perturbation rate from 0% to 20%. Table 6\nshows the performance of GSL algorithms on three datasets with respect to various perturbation\nrates. Surprisingly, we can observe that most GSL algorithms exhibit strong robustness against\ngraph topology attacks, even better than state-of-the-art defense GNNs (e.g., Jaccard [41] and\nSimPGCN [19]). GSL can effectively remove the newly added adversarial edges, and recover\nimportant edges to promote message passing. As mentioned in Li et al. [21], optimizing graph\nstructures based on either features or supervised signals might not be reliable. We found that\nself-supervised graph structure modeling methods (e.g., STABLE and SUBLIME) show excellent\n                                                                                                           8", "md": "# Table 6: Accuracy \u00b1 STD comparison (%) with respect to different perturbation rates\n\n## Table 6: Accuracy \u00b1 STD comparison (%) with respect to different perturbation rates\n\n|Dataset|Ptb Rate|GCN|Jaccard|SimPGCN|IDGL|GRCN|ProGNN|STABLE|SUBLIME|\n|---|---|---|---|---|---|---|---|---|---|\n|Cora|0%|83.68\u00b10.37|83.78\u00b10.50|82.66\u00b10.48|84.69\u00b11.13|84.43\u00b10.26|84.53\u00b10.89|83.70\u00b10.30|83.84\u00b10.28|\n| |5%|80.61\u00b10.39|81.44\u00b10.48|80.35\u00b10.82|82.56\u00b10.24|81.34\u00b10.50|81.47\u00b10.44|81.52\u00b10.85|79.93\u00b10.58|\n| |20%|61.98\u00b11.23|70.71\u00b10.91|69.08\u00b12.78|67.19\u00b10.69|69.54\u00b10.58|61.07\u00b10.61|76.44\u00b12.47|75.25\u00b11.08|\n|Citeseer|0%|76.56\u00b10.36|74.34\u00b10.26|74.35\u00b10.74|73.87\u00b10.70|76.34\u00b10.11|73.36\u00b11.52|72.65\u00b11.36|73.34\u00b11.17|\n| |5%|72.51\u00b10.30|70.01\u00b10.79|72.99\u00b11.05|72.46\u00b10.47|74.66\u00b10.27|71.46\u00b10.47|69.66\u00b10.95|72.63\u00b10.50|\n| |10%|71.92\u00b10.68|70.28\u00b11.30|72.68\u00b10.54|69.72\u00b10.59|74.06\u00b10.43|69.03\u00b10.60|72.79\u00b10.71|73.02\u00b10.29|\n| |15%|64.44\u00b10.53|67.13\u00b11.28|71.74\u00b11.46|62.83\u00b11.28|66.46\u00b11.12|65.42\u00b11.20|70.98\u00b10.61|73.90\u00b10.52|\n| |20%|57.51\u00b11.03|67.82\u00b10.74|70.06\u00b11.86|61.16\u00b10.99|69.42\u00b11.14|57.51\u00b10.36|71.90\u00b11.12|72.55\u00b10.62|\n|Polblogs|0%|95.62\u00b10.69|94.93\u00b10.28|94.50\u00b10.43|94.83\u00b10.20|95.65\u00b10.28|94.84\u00b10.19|95.63\u00b10.32|95.27\u00b10.51|\n| |5%|80.57\u00b10.66|78.17\u00b10.55|76.02\u00b11.14|79.62\u00b10.65|93.70\u00b10.18|92.36\u00b10.42|89.41\u00b11.63|93.24\u00b11.50|\n| |10%|71.83\u00b12.37|71.86\u00b11.34|70.12\u00b11.10|74.54\u00b10.69|87.99\u00b11.56|84.66\u00b10.52|89.87\u00b10.82|93.62\u00b10.50|\n| |15%|66.38\u00b12.17|69.93\u00b10.66|64.19\u00b11.55|75.53\u00b10.83|71.85\u00b11.58|77.38\u00b10.51|89.94\u00b10.89|94.29\u00b10.27|\n| |20%|68.19\u00b12.24|69.22\u00b10.34|63.64\u00b11.41|71.63\u00b10.62|71.73\u00b11.58|73.57\u00b10.29|87.42\u00b10.69|92.60\u00b10.72|\n\n## Figure 3: Analysis of robustness when injecting random noise on Cora and Citeseer\n\nIn Figure 3, we can observe that GSL algorithms (for the sake of brevity, we opted to select only three models: GRCN, IDGL, and SUBLIME) achieve the best results in scenarios with fewer labels available. We speculate that this may be because the learned graph structure is denser and exhibits cleaner community boundaries. As a result, the supervision signals can propagate more effectively within such a structure.\n\nRobustness analysis with respect to random noise. We randomly remove edges from or add edges to the original graph structures of Cora and Citeseer, then evaluated the performance of GSL algorithms on the corrupted graphs. We change the ratios of modified edges from 0 to 0.9 to simulate different attack intensities. As shown in Figure 3, as the noise intensity increases, the models\u2019 performance generally exhibits a downward trend. And we can observe that GSL algorithms commonly demonstrate a certain degree of robustness, as they tend to exhibit more stable performance than GCN when random noise is injected. Besides, we also found that, due to variations in the graph modeling process, different algorithms display varying levels of robustness when facing edge deletion and edge addition scenarios. For example, GRCN demonstrates strong robustness in edge deletion scenarios. However, in the edge addition scenarios, it only exhibits slight performance improvements compared to GCN. On the contrary, STABLE exhibits strong robustness in the edge deletion scenario, while showing the opposite trend in edge addition.\n\nRobust analysis with respect to graph topology attack. Following [21, 55], we conduct robust analysis on three graph datasets, i.e., Cora, Citeseer, and Polblogs. First, we select the largest connected component in the graph, and utilize Mettack [58], a non-targeted adversarial topology attack method, to generate perturbed graphs. We select the perturbation rate from 0% to 20%. Table 6 shows the performance of GSL algorithms on three datasets with respect to various perturbation rates. Surprisingly, we can observe that most GSL algorithms exhibit strong robustness against graph topology attacks, even better than state-of-the-art defense GNNs (e.g., Jaccard [41] and SimPGCN [19]). GSL can effectively remove the newly added adversarial edges, and recover important edges to promote message passing. As mentioned in Li et al. [21], optimizing graph structures based on either features or supervised signals might not be reliable. We found that self-supervised graph structure modeling methods (e.g., STABLE and SUBLIME) show excellent", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Table 6: Accuracy \u00b1 STD comparison (%) with respect to different perturbation rates", "md": "# Table 6: Accuracy \u00b1 STD comparison (%) with respect to different perturbation rates"}, {"type": "heading", "lvl": 2, "value": "Table 6: Accuracy \u00b1 STD comparison (%) with respect to different perturbation rates", "md": "## Table 6: Accuracy \u00b1 STD comparison (%) with respect to different perturbation rates"}, {"type": "table", "rows": [["Dataset", "Ptb Rate", "GCN", "Jaccard", "SimPGCN", "IDGL", "GRCN", "ProGNN", "STABLE", "SUBLIME"], ["Cora", "0%", "83.68\u00b10.37", "83.78\u00b10.50", "82.66\u00b10.48", "84.69\u00b11.13", "84.43\u00b10.26", "84.53\u00b10.89", "83.70\u00b10.30", "83.84\u00b10.28"], ["", "5%", "80.61\u00b10.39", "81.44\u00b10.48", "80.35\u00b10.82", "82.56\u00b10.24", "81.34\u00b10.50", "81.47\u00b10.44", "81.52\u00b10.85", "79.93\u00b10.58"], ["", "20%", "61.98\u00b11.23", "70.71\u00b10.91", "69.08\u00b12.78", "67.19\u00b10.69", "69.54\u00b10.58", "61.07\u00b10.61", "76.44\u00b12.47", "75.25\u00b11.08"], ["Citeseer", "0%", "76.56\u00b10.36", "74.34\u00b10.26", "74.35\u00b10.74", "73.87\u00b10.70", "76.34\u00b10.11", "73.36\u00b11.52", "72.65\u00b11.36", "73.34\u00b11.17"], ["", "5%", "72.51\u00b10.30", "70.01\u00b10.79", "72.99\u00b11.05", "72.46\u00b10.47", "74.66\u00b10.27", "71.46\u00b10.47", "69.66\u00b10.95", "72.63\u00b10.50"], ["", "10%", "71.92\u00b10.68", "70.28\u00b11.30", "72.68\u00b10.54", "69.72\u00b10.59", "74.06\u00b10.43", "69.03\u00b10.60", "72.79\u00b10.71", "73.02\u00b10.29"], ["", "15%", "64.44\u00b10.53", "67.13\u00b11.28", "71.74\u00b11.46", "62.83\u00b11.28", "66.46\u00b11.12", "65.42\u00b11.20", "70.98\u00b10.61", "73.90\u00b10.52"], ["", "20%", "57.51\u00b11.03", "67.82\u00b10.74", "70.06\u00b11.86", "61.16\u00b10.99", "69.42\u00b11.14", "57.51\u00b10.36", "71.90\u00b11.12", "72.55\u00b10.62"], ["Polblogs", "0%", "95.62\u00b10.69", "94.93\u00b10.28", "94.50\u00b10.43", "94.83\u00b10.20", "95.65\u00b10.28", "94.84\u00b10.19", "95.63\u00b10.32", "95.27\u00b10.51"], ["", "5%", "80.57\u00b10.66", "78.17\u00b10.55", "76.02\u00b11.14", "79.62\u00b10.65", "93.70\u00b10.18", "92.36\u00b10.42", "89.41\u00b11.63", "93.24\u00b11.50"], ["", "10%", "71.83\u00b12.37", "71.86\u00b11.34", "70.12\u00b11.10", "74.54\u00b10.69", "87.99\u00b11.56", "84.66\u00b10.52", "89.87\u00b10.82", "93.62\u00b10.50"], ["", "15%", "66.38\u00b12.17", "69.93\u00b10.66", "64.19\u00b11.55", "75.53\u00b10.83", "71.85\u00b11.58", "77.38\u00b10.51", "89.94\u00b10.89", "94.29\u00b10.27"], ["", "20%", "68.19\u00b12.24", "69.22\u00b10.34", "63.64\u00b11.41", "71.63\u00b10.62", "71.73\u00b11.58", "73.57\u00b10.29", "87.42\u00b10.69", "92.60\u00b10.72"]], "md": "|Dataset|Ptb Rate|GCN|Jaccard|SimPGCN|IDGL|GRCN|ProGNN|STABLE|SUBLIME|\n|---|---|---|---|---|---|---|---|---|---|\n|Cora|0%|83.68\u00b10.37|83.78\u00b10.50|82.66\u00b10.48|84.69\u00b11.13|84.43\u00b10.26|84.53\u00b10.89|83.70\u00b10.30|83.84\u00b10.28|\n| |5%|80.61\u00b10.39|81.44\u00b10.48|80.35\u00b10.82|82.56\u00b10.24|81.34\u00b10.50|81.47\u00b10.44|81.52\u00b10.85|79.93\u00b10.58|\n| |20%|61.98\u00b11.23|70.71\u00b10.91|69.08\u00b12.78|67.19\u00b10.69|69.54\u00b10.58|61.07\u00b10.61|76.44\u00b12.47|75.25\u00b11.08|\n|Citeseer|0%|76.56\u00b10.36|74.34\u00b10.26|74.35\u00b10.74|73.87\u00b10.70|76.34\u00b10.11|73.36\u00b11.52|72.65\u00b11.36|73.34\u00b11.17|\n| |5%|72.51\u00b10.30|70.01\u00b10.79|72.99\u00b11.05|72.46\u00b10.47|74.66\u00b10.27|71.46\u00b10.47|69.66\u00b10.95|72.63\u00b10.50|\n| |10%|71.92\u00b10.68|70.28\u00b11.30|72.68\u00b10.54|69.72\u00b10.59|74.06\u00b10.43|69.03\u00b10.60|72.79\u00b10.71|73.02\u00b10.29|\n| |15%|64.44\u00b10.53|67.13\u00b11.28|71.74\u00b11.46|62.83\u00b11.28|66.46\u00b11.12|65.42\u00b11.20|70.98\u00b10.61|73.90\u00b10.52|\n| |20%|57.51\u00b11.03|67.82\u00b10.74|70.06\u00b11.86|61.16\u00b10.99|69.42\u00b11.14|57.51\u00b10.36|71.90\u00b11.12|72.55\u00b10.62|\n|Polblogs|0%|95.62\u00b10.69|94.93\u00b10.28|94.50\u00b10.43|94.83\u00b10.20|95.65\u00b10.28|94.84\u00b10.19|95.63\u00b10.32|95.27\u00b10.51|\n| |5%|80.57\u00b10.66|78.17\u00b10.55|76.02\u00b11.14|79.62\u00b10.65|93.70\u00b10.18|92.36\u00b10.42|89.41\u00b11.63|93.24\u00b11.50|\n| |10%|71.83\u00b12.37|71.86\u00b11.34|70.12\u00b11.10|74.54\u00b10.69|87.99\u00b11.56|84.66\u00b10.52|89.87\u00b10.82|93.62\u00b10.50|\n| |15%|66.38\u00b12.17|69.93\u00b10.66|64.19\u00b11.55|75.53\u00b10.83|71.85\u00b11.58|77.38\u00b10.51|89.94\u00b10.89|94.29\u00b10.27|\n| |20%|68.19\u00b12.24|69.22\u00b10.34|63.64\u00b11.41|71.63\u00b10.62|71.73\u00b11.58|73.57\u00b10.29|87.42\u00b10.69|92.60\u00b10.72|", "isPerfectTable": true, "csv": "\"Dataset\",\"Ptb Rate\",\"GCN\",\"Jaccard\",\"SimPGCN\",\"IDGL\",\"GRCN\",\"ProGNN\",\"STABLE\",\"SUBLIME\"\n\"Cora\",\"0%\",\"83.68\u00b10.37\",\"83.78\u00b10.50\",\"82.66\u00b10.48\",\"84.69\u00b11.13\",\"84.43\u00b10.26\",\"84.53\u00b10.89\",\"83.70\u00b10.30\",\"83.84\u00b10.28\"\n\"\",\"5%\",\"80.61\u00b10.39\",\"81.44\u00b10.48\",\"80.35\u00b10.82\",\"82.56\u00b10.24\",\"81.34\u00b10.50\",\"81.47\u00b10.44\",\"81.52\u00b10.85\",\"79.93\u00b10.58\"\n\"\",\"20%\",\"61.98\u00b11.23\",\"70.71\u00b10.91\",\"69.08\u00b12.78\",\"67.19\u00b10.69\",\"69.54\u00b10.58\",\"61.07\u00b10.61\",\"76.44\u00b12.47\",\"75.25\u00b11.08\"\n\"Citeseer\",\"0%\",\"76.56\u00b10.36\",\"74.34\u00b10.26\",\"74.35\u00b10.74\",\"73.87\u00b10.70\",\"76.34\u00b10.11\",\"73.36\u00b11.52\",\"72.65\u00b11.36\",\"73.34\u00b11.17\"\n\"\",\"5%\",\"72.51\u00b10.30\",\"70.01\u00b10.79\",\"72.99\u00b11.05\",\"72.46\u00b10.47\",\"74.66\u00b10.27\",\"71.46\u00b10.47\",\"69.66\u00b10.95\",\"72.63\u00b10.50\"\n\"\",\"10%\",\"71.92\u00b10.68\",\"70.28\u00b11.30\",\"72.68\u00b10.54\",\"69.72\u00b10.59\",\"74.06\u00b10.43\",\"69.03\u00b10.60\",\"72.79\u00b10.71\",\"73.02\u00b10.29\"\n\"\",\"15%\",\"64.44\u00b10.53\",\"67.13\u00b11.28\",\"71.74\u00b11.46\",\"62.83\u00b11.28\",\"66.46\u00b11.12\",\"65.42\u00b11.20\",\"70.98\u00b10.61\",\"73.90\u00b10.52\"\n\"\",\"20%\",\"57.51\u00b11.03\",\"67.82\u00b10.74\",\"70.06\u00b11.86\",\"61.16\u00b10.99\",\"69.42\u00b11.14\",\"57.51\u00b10.36\",\"71.90\u00b11.12\",\"72.55\u00b10.62\"\n\"Polblogs\",\"0%\",\"95.62\u00b10.69\",\"94.93\u00b10.28\",\"94.50\u00b10.43\",\"94.83\u00b10.20\",\"95.65\u00b10.28\",\"94.84\u00b10.19\",\"95.63\u00b10.32\",\"95.27\u00b10.51\"\n\"\",\"5%\",\"80.57\u00b10.66\",\"78.17\u00b10.55\",\"76.02\u00b11.14\",\"79.62\u00b10.65\",\"93.70\u00b10.18\",\"92.36\u00b10.42\",\"89.41\u00b11.63\",\"93.24\u00b11.50\"\n\"\",\"10%\",\"71.83\u00b12.37\",\"71.86\u00b11.34\",\"70.12\u00b11.10\",\"74.54\u00b10.69\",\"87.99\u00b11.56\",\"84.66\u00b10.52\",\"89.87\u00b10.82\",\"93.62\u00b10.50\"\n\"\",\"15%\",\"66.38\u00b12.17\",\"69.93\u00b10.66\",\"64.19\u00b11.55\",\"75.53\u00b10.83\",\"71.85\u00b11.58\",\"77.38\u00b10.51\",\"89.94\u00b10.89\",\"94.29\u00b10.27\"\n\"\",\"20%\",\"68.19\u00b12.24\",\"69.22\u00b10.34\",\"63.64\u00b11.41\",\"71.63\u00b10.62\",\"71.73\u00b11.58\",\"73.57\u00b10.29\",\"87.42\u00b10.69\",\"92.60\u00b10.72\""}, {"type": "heading", "lvl": 2, "value": "Figure 3: Analysis of robustness when injecting random noise on Cora and Citeseer", "md": "## Figure 3: Analysis of robustness when injecting random noise on Cora and Citeseer"}, {"type": "text", "value": "In Figure 3, we can observe that GSL algorithms (for the sake of brevity, we opted to select only three models: GRCN, IDGL, and SUBLIME) achieve the best results in scenarios with fewer labels available. We speculate that this may be because the learned graph structure is denser and exhibits cleaner community boundaries. As a result, the supervision signals can propagate more effectively within such a structure.\n\nRobustness analysis with respect to random noise. We randomly remove edges from or add edges to the original graph structures of Cora and Citeseer, then evaluated the performance of GSL algorithms on the corrupted graphs. We change the ratios of modified edges from 0 to 0.9 to simulate different attack intensities. As shown in Figure 3, as the noise intensity increases, the models\u2019 performance generally exhibits a downward trend. And we can observe that GSL algorithms commonly demonstrate a certain degree of robustness, as they tend to exhibit more stable performance than GCN when random noise is injected. Besides, we also found that, due to variations in the graph modeling process, different algorithms display varying levels of robustness when facing edge deletion and edge addition scenarios. For example, GRCN demonstrates strong robustness in edge deletion scenarios. However, in the edge addition scenarios, it only exhibits slight performance improvements compared to GCN. On the contrary, STABLE exhibits strong robustness in the edge deletion scenario, while showing the opposite trend in edge addition.\n\nRobust analysis with respect to graph topology attack. Following [21, 55], we conduct robust analysis on three graph datasets, i.e., Cora, Citeseer, and Polblogs. First, we select the largest connected component in the graph, and utilize Mettack [58], a non-targeted adversarial topology attack method, to generate perturbed graphs. We select the perturbation rate from 0% to 20%. Table 6 shows the performance of GSL algorithms on three datasets with respect to various perturbation rates. Surprisingly, we can observe that most GSL algorithms exhibit strong robustness against graph topology attacks, even better than state-of-the-art defense GNNs (e.g., Jaccard [41] and SimPGCN [19]). GSL can effectively remove the newly added adversarial edges, and recover important edges to promote message passing. As mentioned in Li et al. [21], optimizing graph structures based on either features or supervised signals might not be reliable. We found that self-supervised graph structure modeling methods (e.g., STABLE and SUBLIME) show excellent", "md": "In Figure 3, we can observe that GSL algorithms (for the sake of brevity, we opted to select only three models: GRCN, IDGL, and SUBLIME) achieve the best results in scenarios with fewer labels available. We speculate that this may be because the learned graph structure is denser and exhibits cleaner community boundaries. As a result, the supervision signals can propagate more effectively within such a structure.\n\nRobustness analysis with respect to random noise. We randomly remove edges from or add edges to the original graph structures of Cora and Citeseer, then evaluated the performance of GSL algorithms on the corrupted graphs. We change the ratios of modified edges from 0 to 0.9 to simulate different attack intensities. As shown in Figure 3, as the noise intensity increases, the models\u2019 performance generally exhibits a downward trend. And we can observe that GSL algorithms commonly demonstrate a certain degree of robustness, as they tend to exhibit more stable performance than GCN when random noise is injected. Besides, we also found that, due to variations in the graph modeling process, different algorithms display varying levels of robustness when facing edge deletion and edge addition scenarios. For example, GRCN demonstrates strong robustness in edge deletion scenarios. However, in the edge addition scenarios, it only exhibits slight performance improvements compared to GCN. On the contrary, STABLE exhibits strong robustness in the edge deletion scenario, while showing the opposite trend in edge addition.\n\nRobust analysis with respect to graph topology attack. Following [21, 55], we conduct robust analysis on three graph datasets, i.e., Cora, Citeseer, and Polblogs. First, we select the largest connected component in the graph, and utilize Mettack [58], a non-targeted adversarial topology attack method, to generate perturbed graphs. We select the perturbation rate from 0% to 20%. Table 6 shows the performance of GSL algorithms on three datasets with respect to various perturbation rates. Surprisingly, we can observe that most GSL algorithms exhibit strong robustness against graph topology attacks, even better than state-of-the-art defense GNNs (e.g., Jaccard [41] and SimPGCN [19]). GSL can effectively remove the newly added adversarial edges, and recover important edges to promote message passing. As mentioned in Li et al. [21], optimizing graph structures based on either features or supervised signals might not be reliable. We found that self-supervised graph structure modeling methods (e.g., STABLE and SUBLIME) show excellent"}]}, {"page": 9, "text": "                                                GCN              GRCN                IDGL              ProGNN                 SUBLIME                  MLP               SLAPS               HES-GSL\n     85                  TR on Cora                               75              TR on Citeseer                              75                  TI on Cora                               75               TI on Citeseer\n                                                                                                                                                                                           70\n                                                                  70\n    Accuracy (%)                                                Accuracy (%)                                                 Accuracy (%)                                                 Accuracy (%)\n                                                                                                                              70\n     80                                                                                                                                                                                    65\n                                                                  65                                                          65                                                           60\n                                                                                                                              60                                                           55\n     75                                                           60\n                                                                                                                              55                                                           50\n     70                                                           55                                                          50                                                           45\n                                                                                                                                                                                           40\n                                                                  50                                                          45\n                                                                                                                                                                                           35\n     65                                                           45                                                          40                                                           30\n          0.0        0.2         0.4        0.6         0.8           0.0         0.2        0.4         0.6         0.8           0.0         0.1        0.2         0.3        0.4            0.0        0.1         0.2  0.3   0.4\n                  Mask Feature Rate                                            Mask Feature Rate                                           Mask Feature Rate                                            Mask Feature Rate\n         Figure 4: Analysis of robustness when injecting random feature noise on Cora and Citeseer.\n                                         GCN              LDS              GRCN                 IDGL              ProGNN                  CoGSL                 SUBLIME                    NodeFormer\n                                         Cora                                                                         Citeseer                                                                         Pubmed\n      84                                                                              74                                                                               81\n    Accuracy (%)                                                                    Accuracy (%)                                                                     Accuracy (%)\n      83\n                                                                                      72                                                                               80\n      82\n                                                                                      70                                                                               79\n      81\n      80                                                                              68                                                                               78\n                         101                                102                                        101                           102                                               101                        102            103\n                              Training Time (s)                                                                Training Time (s)                                                               Training Time (s)\n                                         Cora                                                                         Citeseer                                                                         Pubmed\n      84                                                                              74                                                                               81\n    Accuracy (%)                                                                    Accuracy (%)                                                                     Accuracy (%)\n      83\n                                                                                      72                                                                               80\n      82\n                                                                                      70                                                                               79\n      81\n      80                                                                              68                                                                               78\n                       102                               103                                 102                                 103                                                                        104\n                           Training Space (MB)                                                             Training Space (MB)                                                              Training Space (MB)\n                            Figure 5: Training time and space analysis on Cora, Citeseer and Pubmed.\nperformance on corrupted graph structure datasets, which means unsupervised representation learning\nmight produce more reliable and high-quality representations to conduct structure modeling.\nRobust analysis with respect to feature noise. On the basis of exploring structural robustness, we\nalso study the feature robustness of GSL. We randomly mask a certain proportion of node features by\nfilling them with zeros, to investigate the performance of GSL algorithms when node features are\nsubjected to varying degrees of damage. As shown in Figure 4, we can observe that: 1) the node\nfeatures play a more critical role than the structure on certain datasets. Under the same noise degree,\nfeature noise brings more performance degradation compared with structure noise; 2) Interestingly,\nwhile most existing GSL methods rely on feature similarity between pairs of nodes to learn graph\nstructure, they still exhibit good robustness when facing noisy node features; 3) edge-oriented\nalgorithms (e.g., ProGNN) show stronger feature robustness, because they optimize adjacency matrix\ndirectly, and have less dependence on pairs of node features.\n4.6           Efficiency and scalability analysis\nIn this section, we analyze the efficiency and scalability of GSL algorithms on Cora, Citeseer, and\nPubmed datasets. For time efficiency, we evaluate the effi                                                                                        ciency of the algorithms by measuring\nthe time it takes for them to converge, i.e., achieve the best performance on the validation set.\nFor scalability, we set all models to their dense version to ensure a fair comparison. As shown\nin Figure 5, GSL algorithms generally have higher time and space complexity compared to GCN.\nThis limitation restricts the application of GSL on large-scale graphs. We can observe that some\nalgorithms (e.g., GRCN) can achieve relatively good performance improvement with less complexity\nincrease. Besides, although some algorithms (e.g., IDGL, LDS, and SUBLIME) achieve remarkable\neffectiveness improvement, they largely increase the complexity of time and space.\n5          Conclusion and Future Directions\nIn this paper, we give a brief introduction and overview of graph structure learning. Then we present\nthe first Graph Structure Learning Benchmark (GSLB) consisting of 16 algorithms and 20 datasets\n                                                                                                                        9", "md": "# Graph Structure Learning\n\n## Performance Analysis\n\n| |GCN|GRCN|IDGL|ProGNN|SUBLIME|MLP|SLAPS|HES-GSL|\n|---|---|---|---|---|---|---|---|---|\n|TR on Cora|85| | | | | | | |\n|TR on Citeseer| |75| | | | | | |\n|TI on Cora|75| | | | | | | |\n|TI on Citeseer| | | | | | | |70|\n\nAccuracy (%)\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n\\hline\n80 & & & & & & & & \\\\\n\\hline\n75 & & & & & & & & \\\\\n\\hline\n70 & & & & & & & & \\\\\n\\hline\n65 & & & & & & & & \\\\\n\\hline\n\\end{array}\n$$\nFigure 4: Analysis of robustness when injecting random feature noise on Cora and Citeseer.\n\n## Robust Analysis\n\n| |GCN|LDS|GRCN|IDGL|ProGNN|CoGSL|SUBLIME|NodeFormer|\n|---|---|---|---|---|---|---|---|---|\n|Cora|84| |74| |81| | | |\n|Citeseer| |72| | |80| | | |\n|Pubmed| | | | | | | | |\n\nAccuracy (%)\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n83 & & & & & & & & & \\\\\n\\hline\n82 & & & & & & & & & \\\\\n\\hline\n81 & & & & & & & & & \\\\\n\\hline\n80 & & & & & & & & & \\\\\n\\hline\n\\end{array}\n$$\nFigure 5: Training time and space analysis on Cora, Citeseer and Pubmed.\n\nPerformance on corrupted graph structure datasets, which means unsupervised representation learning might produce more reliable and high-quality representations to conduct structure modeling.\n\nRobust analysis with respect to feature noise. On the basis of exploring structural robustness, we also study the feature robustness of GSL. We randomly mask a certain proportion of node features by filling them with zeros, to investigate the performance of GSL algorithms when node features are subjected to varying degrees of damage. As shown in Figure 4, we can observe that: 1) the node features play a more critical role than the structure on certain datasets. Under the same noise degree, feature noise brings more performance degradation compared with structure noise; 2) Interestingly, while most existing GSL methods rely on feature similarity between pairs of nodes to learn graph structure, they still exhibit good robustness when facing noisy node features; 3) edge-oriented algorithms (e.g., ProGNN) show stronger feature robustness, because they optimize adjacency matrix directly, and have less dependence on pairs of node features.\n\n## Efficiency and Scalability Analysis\n\nIn this section, we analyze the efficiency and scalability of GSL algorithms on Cora, Citeseer, and Pubmed datasets. For time efficiency, we evaluate the efficiency of the algorithms by measuring the time it takes for them to converge, i.e., achieve the best performance on the validation set. For scalability, we set all models to their dense version to ensure a fair comparison. As shown in Figure 5, GSL algorithms generally have higher time and space complexity compared to GCN. This limitation restricts the application of GSL on large-scale graphs. We can observe that some algorithms (e.g., GRCN) can achieve relatively good performance improvement with less complexity increase. Besides, although some algorithms (e.g., IDGL, LDS, and SUBLIME) achieve remarkable effectiveness improvement, they largely increase the complexity of time and space.\n\n## Conclusion and Future Directions\n\nIn this paper, we give a brief introduction and overview of graph structure learning. Then we present the first Graph Structure Learning Benchmark (GSLB) consisting of 16 algorithms and 20 datasets.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Graph Structure Learning", "md": "# Graph Structure Learning"}, {"type": "heading", "lvl": 2, "value": "Performance Analysis", "md": "## Performance Analysis"}, {"type": "table", "rows": [["", "GCN", "GRCN", "IDGL", "ProGNN", "SUBLIME", "MLP", "SLAPS", "HES-GSL"], ["TR on Cora", "85", "", "", "", "", "", "", ""], ["TR on Citeseer", "", "75", "", "", "", "", "", ""], ["TI on Cora", "75", "", "", "", "", "", "", ""], ["TI on Citeseer", "", "", "", "", "", "", "", "70"]], "md": "| |GCN|GRCN|IDGL|ProGNN|SUBLIME|MLP|SLAPS|HES-GSL|\n|---|---|---|---|---|---|---|---|---|\n|TR on Cora|85| | | | | | | |\n|TR on Citeseer| |75| | | | | | |\n|TI on Cora|75| | | | | | | |\n|TI on Citeseer| | | | | | | |70|", "isPerfectTable": true, "csv": "\"\",\"GCN\",\"GRCN\",\"IDGL\",\"ProGNN\",\"SUBLIME\",\"MLP\",\"SLAPS\",\"HES-GSL\"\n\"TR on Cora\",\"85\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"TR on Citeseer\",\"\",\"75\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"TI on Cora\",\"75\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"TI on Citeseer\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"70\""}, {"type": "text", "value": "Accuracy (%)\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n\\hline\n80 & & & & & & & & \\\\\n\\hline\n75 & & & & & & & & \\\\\n\\hline\n70 & & & & & & & & \\\\\n\\hline\n65 & & & & & & & & \\\\\n\\hline\n\\end{array}\n$$\nFigure 4: Analysis of robustness when injecting random feature noise on Cora and Citeseer.", "md": "Accuracy (%)\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n\\hline\n80 & & & & & & & & \\\\\n\\hline\n75 & & & & & & & & \\\\\n\\hline\n70 & & & & & & & & \\\\\n\\hline\n65 & & & & & & & & \\\\\n\\hline\n\\end{array}\n$$\nFigure 4: Analysis of robustness when injecting random feature noise on Cora and Citeseer."}, {"type": "heading", "lvl": 2, "value": "Robust Analysis", "md": "## Robust Analysis"}, {"type": "table", "rows": [["", "GCN", "LDS", "GRCN", "IDGL", "ProGNN", "CoGSL", "SUBLIME", "NodeFormer"], ["Cora", "84", "", "74", "", "81", "", "", ""], ["Citeseer", "", "72", "", "", "80", "", "", ""], ["Pubmed", "", "", "", "", "", "", "", ""]], "md": "| |GCN|LDS|GRCN|IDGL|ProGNN|CoGSL|SUBLIME|NodeFormer|\n|---|---|---|---|---|---|---|---|---|\n|Cora|84| |74| |81| | | |\n|Citeseer| |72| | |80| | | |\n|Pubmed| | | | | | | | |", "isPerfectTable": true, "csv": "\"\",\"GCN\",\"LDS\",\"GRCN\",\"IDGL\",\"ProGNN\",\"CoGSL\",\"SUBLIME\",\"NodeFormer\"\n\"Cora\",\"84\",\"\",\"74\",\"\",\"81\",\"\",\"\",\"\"\n\"Citeseer\",\"\",\"72\",\"\",\"\",\"80\",\"\",\"\",\"\"\n\"Pubmed\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "Accuracy (%)\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n83 & & & & & & & & & \\\\\n\\hline\n82 & & & & & & & & & \\\\\n\\hline\n81 & & & & & & & & & \\\\\n\\hline\n80 & & & & & & & & & \\\\\n\\hline\n\\end{array}\n$$\nFigure 5: Training time and space analysis on Cora, Citeseer and Pubmed.\n\nPerformance on corrupted graph structure datasets, which means unsupervised representation learning might produce more reliable and high-quality representations to conduct structure modeling.\n\nRobust analysis with respect to feature noise. On the basis of exploring structural robustness, we also study the feature robustness of GSL. We randomly mask a certain proportion of node features by filling them with zeros, to investigate the performance of GSL algorithms when node features are subjected to varying degrees of damage. As shown in Figure 4, we can observe that: 1) the node features play a more critical role than the structure on certain datasets. Under the same noise degree, feature noise brings more performance degradation compared with structure noise; 2) Interestingly, while most existing GSL methods rely on feature similarity between pairs of nodes to learn graph structure, they still exhibit good robustness when facing noisy node features; 3) edge-oriented algorithms (e.g., ProGNN) show stronger feature robustness, because they optimize adjacency matrix directly, and have less dependence on pairs of node features.", "md": "Accuracy (%)\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n83 & & & & & & & & & \\\\\n\\hline\n82 & & & & & & & & & \\\\\n\\hline\n81 & & & & & & & & & \\\\\n\\hline\n80 & & & & & & & & & \\\\\n\\hline\n\\end{array}\n$$\nFigure 5: Training time and space analysis on Cora, Citeseer and Pubmed.\n\nPerformance on corrupted graph structure datasets, which means unsupervised representation learning might produce more reliable and high-quality representations to conduct structure modeling.\n\nRobust analysis with respect to feature noise. On the basis of exploring structural robustness, we also study the feature robustness of GSL. We randomly mask a certain proportion of node features by filling them with zeros, to investigate the performance of GSL algorithms when node features are subjected to varying degrees of damage. As shown in Figure 4, we can observe that: 1) the node features play a more critical role than the structure on certain datasets. Under the same noise degree, feature noise brings more performance degradation compared with structure noise; 2) Interestingly, while most existing GSL methods rely on feature similarity between pairs of nodes to learn graph structure, they still exhibit good robustness when facing noisy node features; 3) edge-oriented algorithms (e.g., ProGNN) show stronger feature robustness, because they optimize adjacency matrix directly, and have less dependence on pairs of node features."}, {"type": "heading", "lvl": 2, "value": "Efficiency and Scalability Analysis", "md": "## Efficiency and Scalability Analysis"}, {"type": "text", "value": "In this section, we analyze the efficiency and scalability of GSL algorithms on Cora, Citeseer, and Pubmed datasets. For time efficiency, we evaluate the efficiency of the algorithms by measuring the time it takes for them to converge, i.e., achieve the best performance on the validation set. For scalability, we set all models to their dense version to ensure a fair comparison. As shown in Figure 5, GSL algorithms generally have higher time and space complexity compared to GCN. This limitation restricts the application of GSL on large-scale graphs. We can observe that some algorithms (e.g., GRCN) can achieve relatively good performance improvement with less complexity increase. Besides, although some algorithms (e.g., IDGL, LDS, and SUBLIME) achieve remarkable effectiveness improvement, they largely increase the complexity of time and space.", "md": "In this section, we analyze the efficiency and scalability of GSL algorithms on Cora, Citeseer, and Pubmed datasets. For time efficiency, we evaluate the efficiency of the algorithms by measuring the time it takes for them to converge, i.e., achieve the best performance on the validation set. For scalability, we set all models to their dense version to ensure a fair comparison. As shown in Figure 5, GSL algorithms generally have higher time and space complexity compared to GCN. This limitation restricts the application of GSL on large-scale graphs. We can observe that some algorithms (e.g., GRCN) can achieve relatively good performance improvement with less complexity increase. Besides, although some algorithms (e.g., IDGL, LDS, and SUBLIME) achieve remarkable effectiveness improvement, they largely increase the complexity of time and space."}, {"type": "heading", "lvl": 2, "value": "Conclusion and Future Directions", "md": "## Conclusion and Future Directions"}, {"type": "text", "value": "In this paper, we give a brief introduction and overview of graph structure learning. Then we present the first Graph Structure Learning Benchmark (GSLB) consisting of 16 algorithms and 20 datasets.", "md": "In this paper, we give a brief introduction and overview of graph structure learning. Then we present the first Graph Structure Learning Benchmark (GSLB) consisting of 16 algorithms and 20 datasets."}]}, {"page": 10, "text": " for various tasks. Based on GSLB, we conducted extensive experiments to reveal and analyze the\n performance of GSL algorithms in different scenarios and tasks. Through our comparative study, we\n find that GSL achieves promising results in heterophily, robustness, etc. The goal of this work is to\n understand the current state of development of GSL and provide insights for future research.\n Notwithstanding the promising results that have been made, there are still some critical challenges\n and research directions worthy of future investigation.\n \u2022 Insufficient scalability. Most existing works model the existence probability of edges based on\n   node pairs, with a complexity of O(N 2). This makes it challenging to employ GSL in large-scale\n   graphs in real-world applications. Future work should focus on overcoming the limitations of GSL\n   in terms of complexity.\n \u2022 Surprising performance with few labels. We have observed that GSL learns denser and more\n   distinct graph structures, which facilitates the propagation of supervision signals. Most existing\n   GNNs that address few label problem are based on deep GNNs [25, 13] or semi-supervised\n   approaches [6, 35, 20], without refining the graph structure. In the future, it would be worth\n   exploring the combination of increasing the supervision signals and making the graph structure\n   more suitable for propagating those signals.\n \u2022 Excellent performance of unsupervised GSL in robustness. Some algorithms using self-\n   supervised methods for learning graph structures exhibit excellent performance in robustness,\n   which may be attributed to the avoidance of unreliable supervision signals. In the future, further\n   exploration can be done to utilize unsupervised structure learning for designing defense models.\n \u2022 Hard to apply on incomplete graphs. Most existing algorithms rely on pairwise node embeddings\n   to generate the probability of edge existence. The underlying assumption is that all attributes of\n   nodes on the graph are complete. However, it is common in practice that some nodes or all nodes\n   have no features. Future research should address the challenges of structure learning on incomplete\n   graphs.\n Acknowledge\nThis work was partially supported by the Research Grants Council of Hong Kong, No. 14202919 and\n No. 14205520.\n References\n  [1] Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang. Rumor\n      detection on social media with bi-directional graph convolutional networks. In Proceedings of the AAAI\n      conference on artificial intelligence, volume 34, pages 549\u2013556, 2020. 1\n  [2] Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, SVN Vishwanathan, Alex J Smola, and\n      Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013i56,\n      2005. 2, 4\n  [3] Chen Cai and Yusu Wang. A simple yet effective baseline for non-attributed graph classifi  cation. arXiv\n      preprint arXiv:1811.03508, 2018. 2, 4\n  [4] Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks:\n      Better and robust node embeddings. Advances in neural information processing systems, 33:19314\u201319326,\n      2020. 2, 4, 16\n  [5] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin\n      Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds.\n      correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2):\n      786\u2013797, 1991. 2, 4\n  [6] Kaize Ding, Jianling Wang, James Caverlee, and Huan Liu. Meta propagation networks for graph few-shot\n      semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36,\n      pages 6524\u20136531, 2022. 10\n  [7] Paul D Dobson and Andrew J Doig.         Distinguishing enzyme structures from non-enzymes without\n      alignments. Journal of molecular biology, 330(4):771\u2013783, 2003. 15\n                                                      10", "md": "for various tasks. Based on GSLB, we conducted extensive experiments to reveal and analyze the\nperformance of GSL algorithms in different scenarios and tasks. Through our comparative study, we\nfind that GSL achieves promising results in heterophily, robustness, etc. The goal of this work is to\nunderstand the current state of development of GSL and provide insights for future research.\nNotwithstanding the promising results that have been made, there are still some critical challenges\nand research directions worthy of future investigation.\n\n- Insufficient scalability. Most existing works model the existence probability of edges based on\nnode pairs, with a complexity of $O(N^2)$. This makes it challenging to employ GSL in large-scale\ngraphs in real-world applications. Future work should focus on overcoming the limitations of GSL\nin terms of complexity.\n- Surprising performance with few labels. We have observed that GSL learns denser and more\ndistinct graph structures, which facilitates the propagation of supervision signals. Most existing\nGNNs that address few label problem are based on deep GNNs [25, 13] or semi-supervised\napproaches [6, 35, 20], without refining the graph structure. In the future, it would be worth\nexploring the combination of increasing the supervision signals and making the graph structure\nmore suitable for propagating those signals.\n- Excellent performance of unsupervised GSL in robustness. Some algorithms using self-\nsupervised methods for learning graph structures exhibit excellent performance in robustness,\nwhich may be attributed to the avoidance of unreliable supervision signals. In the future, further\nexploration can be done to utilize unsupervised structure learning for designing defense models.\n- Hard to apply on incomplete graphs. Most existing algorithms rely on pairwise node embeddings\nto generate the probability of edge existence. The underlying assumption is that all attributes of\nnodes on the graph are complete. However, it is common in practice that some nodes or all nodes\nhave no features. Future research should address the challenges of structure learning on incomplete\ngraphs.\n\nAcknowledgment: This work was partially supported by the Research Grants Council of Hong Kong, No. 14202919 and\nNo. 14205520.\n\n### References\n\n|[1]|Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang. Rumor detection on social media with bi-directional graph convolutional networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 549\u2013556, 2020.|\n|---|---|\n|[2]|Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013i56, 2005.|\n|[3]|Chen Cai and Yusu Wang. A simple yet effective baseline for non-attributed graph classification. arXiv preprint arXiv:1811.03508, 2018.|\n|[4]|Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Advances in neural information processing systems, 33:19314\u201319326, 2020.|\n|[5]|Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2): 786\u2013797, 1991.|\n|[6]|Kaize Ding, Jianling Wang, James Caverlee, and Huan Liu. Meta propagation networks for graph few-shot semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6524\u20136531, 2022.|\n|[7]|Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 330(4):771\u2013783, 2003.|", "images": [], "items": [{"type": "text", "value": "for various tasks. Based on GSLB, we conducted extensive experiments to reveal and analyze the\nperformance of GSL algorithms in different scenarios and tasks. Through our comparative study, we\nfind that GSL achieves promising results in heterophily, robustness, etc. The goal of this work is to\nunderstand the current state of development of GSL and provide insights for future research.\nNotwithstanding the promising results that have been made, there are still some critical challenges\nand research directions worthy of future investigation.\n\n- Insufficient scalability. Most existing works model the existence probability of edges based on\nnode pairs, with a complexity of $O(N^2)$. This makes it challenging to employ GSL in large-scale\ngraphs in real-world applications. Future work should focus on overcoming the limitations of GSL\nin terms of complexity.\n- Surprising performance with few labels. We have observed that GSL learns denser and more\ndistinct graph structures, which facilitates the propagation of supervision signals. Most existing\nGNNs that address few label problem are based on deep GNNs [25, 13] or semi-supervised\napproaches [6, 35, 20], without refining the graph structure. In the future, it would be worth\nexploring the combination of increasing the supervision signals and making the graph structure\nmore suitable for propagating those signals.\n- Excellent performance of unsupervised GSL in robustness. Some algorithms using self-\nsupervised methods for learning graph structures exhibit excellent performance in robustness,\nwhich may be attributed to the avoidance of unreliable supervision signals. In the future, further\nexploration can be done to utilize unsupervised structure learning for designing defense models.\n- Hard to apply on incomplete graphs. Most existing algorithms rely on pairwise node embeddings\nto generate the probability of edge existence. The underlying assumption is that all attributes of\nnodes on the graph are complete. However, it is common in practice that some nodes or all nodes\nhave no features. Future research should address the challenges of structure learning on incomplete\ngraphs.\n\nAcknowledgment: This work was partially supported by the Research Grants Council of Hong Kong, No. 14202919 and\nNo. 14205520.", "md": "for various tasks. Based on GSLB, we conducted extensive experiments to reveal and analyze the\nperformance of GSL algorithms in different scenarios and tasks. Through our comparative study, we\nfind that GSL achieves promising results in heterophily, robustness, etc. The goal of this work is to\nunderstand the current state of development of GSL and provide insights for future research.\nNotwithstanding the promising results that have been made, there are still some critical challenges\nand research directions worthy of future investigation.\n\n- Insufficient scalability. Most existing works model the existence probability of edges based on\nnode pairs, with a complexity of $O(N^2)$. This makes it challenging to employ GSL in large-scale\ngraphs in real-world applications. Future work should focus on overcoming the limitations of GSL\nin terms of complexity.\n- Surprising performance with few labels. We have observed that GSL learns denser and more\ndistinct graph structures, which facilitates the propagation of supervision signals. Most existing\nGNNs that address few label problem are based on deep GNNs [25, 13] or semi-supervised\napproaches [6, 35, 20], without refining the graph structure. In the future, it would be worth\nexploring the combination of increasing the supervision signals and making the graph structure\nmore suitable for propagating those signals.\n- Excellent performance of unsupervised GSL in robustness. Some algorithms using self-\nsupervised methods for learning graph structures exhibit excellent performance in robustness,\nwhich may be attributed to the avoidance of unreliable supervision signals. In the future, further\nexploration can be done to utilize unsupervised structure learning for designing defense models.\n- Hard to apply on incomplete graphs. Most existing algorithms rely on pairwise node embeddings\nto generate the probability of edge existence. The underlying assumption is that all attributes of\nnodes on the graph are complete. However, it is common in practice that some nodes or all nodes\nhave no features. Future research should address the challenges of structure learning on incomplete\ngraphs.\n\nAcknowledgment: This work was partially supported by the Research Grants Council of Hong Kong, No. 14202919 and\nNo. 14205520."}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "table", "rows": [["[1]", "Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang. Rumor detection on social media with bi-directional graph convolutional networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 549\u2013556, 2020."], ["[2]", "Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013i56, 2005."], ["[3]", "Chen Cai and Yusu Wang. A simple yet effective baseline for non-attributed graph classification. arXiv preprint arXiv:1811.03508, 2018."], ["[4]", "Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Advances in neural information processing systems, 33:19314\u201319326, 2020."], ["[5]", "Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2): 786\u2013797, 1991."], ["[6]", "Kaize Ding, Jianling Wang, James Caverlee, and Huan Liu. Meta propagation networks for graph few-shot semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6524\u20136531, 2022."], ["[7]", "Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 330(4):771\u2013783, 2003."]], "md": "|[1]|Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang. Rumor detection on social media with bi-directional graph convolutional networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 549\u2013556, 2020.|\n|---|---|\n|[2]|Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013i56, 2005.|\n|[3]|Chen Cai and Yusu Wang. A simple yet effective baseline for non-attributed graph classification. arXiv preprint arXiv:1811.03508, 2018.|\n|[4]|Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Advances in neural information processing systems, 33:19314\u201319326, 2020.|\n|[5]|Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2): 786\u2013797, 1991.|\n|[6]|Kaize Ding, Jianling Wang, James Caverlee, and Huan Liu. Meta propagation networks for graph few-shot semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6524\u20136531, 2022.|\n|[7]|Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 330(4):771\u2013783, 2003.|", "isPerfectTable": true, "csv": "\"[1]\",\"Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang. Rumor detection on social media with bi-directional graph convolutional networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 549\u2013556, 2020.\"\n\"[2]\",\"Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(suppl_1):i47\u2013i56, 2005.\"\n\"[3]\",\"Chen Cai and Yusu Wang. A simple yet effective baseline for non-attributed graph classification. arXiv preprint arXiv:1811.03508, 2018.\"\n\"[4]\",\"Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. Advances in neural information processing systems, 33:19314\u201319326, 2020.\"\n\"[5]\",\"Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2): 786\u2013797, 1991.\"\n\"[6]\",\"Kaize Ding, Jianling Wang, James Caverlee, and Huan Liu. Meta propagation networks for graph few-shot semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 6524\u20136531, 2022.\"\n\"[7]\",\"Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. Journal of molecular biology, 330(4):771\u2013783, 2003.\""}]}, {"page": 11, "text": "  [8] Vijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and\n      Dominique Beaini. Long range graph benchmark. Advances in Neural Information Processing Systems,\n      35:22326\u201322340, 2022. 2, 4, 6, 15, 17\n  [9] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks\n      for social recommendation. In The world wide web conference, pages 417\u2013426, 2019. 1\n[10] Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi. Slaps: Self-supervision improves structure\n      learning for graph neural networks. Advances in Neural Information Processing Systems, 34:22667\u201322681,\n      2021. 2, 4, 5, 16\n[11] Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures for\n      graph neural networks. In International conference on machine learning, pages 1972\u20131982. PMLR, 2019.\n      2, 15, 16\n[12] Xiang Gao, Wei Hu, and Zongming Guo. Exploring structure-adaptive graph learning for robust semi-\n      supervised classification. In 2020 ieee international conference on multimedia and expo (icme), pages 1\u20136.\n      IEEE, 2020. 16\n[13] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G\u00fcnnemann. Predict then propagate: Graph\n      neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018. 10\n[14] Zhongkai Hao, Chengqiang Lu, Zhenya Huang, Hao Wang, Zheyuan Hu, Qi Liu, Enhong Chen, and\n      Cheekong Lee. Asgn: An active semi-supervised graph neural network for molecular property prediction.\n      In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data\n      Mining, pages 731\u2013752, 2020. 1\n[15] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and\n      Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neural\n      information processing systems, 33:22118\u201322133, 2020. 2, 4, 14\n[16] Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo. Semi-supervised learning with graph learning-\n      convolutional networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\n      recognition, pages 11313\u201311320, 2019. 16\n[17] Di Jin, Zhizhi Yu, Cuiying Huo, Rui Wang, Xiao Wang, Dongxiao He, and Jiawei Han. Universal graph\n      convolutional networks. Advances in Neural Information Processing Systems, 34:10654\u201310664, 2021. 5\n[18] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning\n      for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on\n      knowledge discovery & data mining, pages 66\u201374, 2020. 2, 4, 15, 16\n[19] Wei Jin, Tyler Derr, Yiqi Wang, Yao Ma, Zitao Liu, and Jiliang Tang. Node similarity preserving graph\n      convolutional networks. In Proceedings of the 14th ACM international conference on web search and data\n      mining, pages 148\u2013156, 2021. 8\n[20] Junseok Lee, Yunhak Oh, Yeonjun In, Namkyeong Lee, Dongmin Hyun, and Chanyoung Park. Grafn:\n      Semi-supervised node classification on graph with few labels via non-parametric distribution assignment. In\n      Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information\n      Retrieval, pages 2243\u20132248, 2022. 10\n[21] Kuan Li, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Reliable representa-\n      tions make a stronger defender: Unsupervised structure refinement for robust gnn. In Proceedings of the\n      28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 925\u2013935, 2022. 2, 4, 8,\n      16\n[22] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for\n      semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32,\n      2018. 7\n[23] Zhixun Li, Dingshuo Chen, Qiang Liu, and Shu Wu. The devil is in the conflict: Disentangled information\n      graph neural networks for fraud detection. arXiv preprint arXiv:2210.12384, 2022. 1\n[24] Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao, and Ser Nam\n      Lim. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods.\n      Advances in Neural Information Processing Systems, 34:20887\u201320902, 2021. 17\n                                                        11", "md": "# Research Papers\n\n# List of Research Papers\n\n|Authors|Title|Conference/Journal|Pages|\n|---|---|---|---|\n|Vijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini|Long range graph benchmark|Advances in Neural Information Processing Systems|35:22326\u201322340|\n|Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin|Graph neural networks for social recommendation|The world wide web conference|417\u2013426|\n|Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi|Slaps: Self-supervision improves structure learning for graph neural networks|Advances in Neural Information Processing Systems|34:22667\u201322681|\n|Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He|Learning discrete structures for graph neural networks|International conference on machine learning|1972\u20131982|\n|Xiang Gao, Wei Hu, and Zongming Guo|Exploring structure-adaptive graph learning for robust semi-supervised classification|2020 ieee international conference on multimedia and expo (icme)|1\u20136|\n\nFor more research papers, please refer to the complete list.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Papers", "md": "# Research Papers"}, {"type": "heading", "lvl": 1, "value": "List of Research Papers", "md": "# List of Research Papers"}, {"type": "table", "rows": [["Authors", "Title", "Conference/Journal", "Pages"], ["Vijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini", "Long range graph benchmark", "Advances in Neural Information Processing Systems", "35:22326\u201322340"], ["Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin", "Graph neural networks for social recommendation", "The world wide web conference", "417\u2013426"], ["Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi", "Slaps: Self-supervision improves structure learning for graph neural networks", "Advances in Neural Information Processing Systems", "34:22667\u201322681"], ["Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He", "Learning discrete structures for graph neural networks", "International conference on machine learning", "1972\u20131982"], ["Xiang Gao, Wei Hu, and Zongming Guo", "Exploring structure-adaptive graph learning for robust semi-supervised classification", "2020 ieee international conference on multimedia and expo (icme)", "1\u20136"]], "md": "|Authors|Title|Conference/Journal|Pages|\n|---|---|---|---|\n|Vijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini|Long range graph benchmark|Advances in Neural Information Processing Systems|35:22326\u201322340|\n|Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin|Graph neural networks for social recommendation|The world wide web conference|417\u2013426|\n|Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi|Slaps: Self-supervision improves structure learning for graph neural networks|Advances in Neural Information Processing Systems|34:22667\u201322681|\n|Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He|Learning discrete structures for graph neural networks|International conference on machine learning|1972\u20131982|\n|Xiang Gao, Wei Hu, and Zongming Guo|Exploring structure-adaptive graph learning for robust semi-supervised classification|2020 ieee international conference on multimedia and expo (icme)|1\u20136|", "isPerfectTable": true, "csv": "\"Authors\",\"Title\",\"Conference/Journal\",\"Pages\"\n\"Vijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini\",\"Long range graph benchmark\",\"Advances in Neural Information Processing Systems\",\"35:22326\u201322340\"\n\"Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin\",\"Graph neural networks for social recommendation\",\"The world wide web conference\",\"417\u2013426\"\n\"Bahare Fatemi, Layla El Asri, and Seyed Mehran Kazemi\",\"Slaps: Self-supervision improves structure learning for graph neural networks\",\"Advances in Neural Information Processing Systems\",\"34:22667\u201322681\"\n\"Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He\",\"Learning discrete structures for graph neural networks\",\"International conference on machine learning\",\"1972\u20131982\"\n\"Xiang Gao, Wei Hu, and Zongming Guo\",\"Exploring structure-adaptive graph learning for robust semi-supervised classification\",\"2020 ieee international conference on multimedia and expo (icme)\",\"1\u20136\""}, {"type": "text", "value": "For more research papers, please refer to the complete list.", "md": "For more research papers, please refer to the complete list."}]}, {"page": 12, "text": "[25] Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Proceedings of\n      the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 338\u2013348,\n      2020. 7, 10\n[26] Nian Liu, Xiao Wang, Lingfei Wu, Yu Chen, Xiaojie Guo, and Chuan Shi. Compact graph structure\n      learning via mutual information compression. In Proceedings of the ACM Web Conference 2022, pages\n     1601\u20131610, 2022. 2, 4, 16\n[27] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and choose: a\n      gnn-based imbalanced learning approach for fraud detection. In Proceedings of the Web Conference 2021,\n      pages 3168\u20133177, 2021. 1\n[28] Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan. Towards unsupervised\n      deep graph structure learning. In Proceedings of the ACM Web Conference 2022, pages 1392\u20131403, 2022.\n      2, 4, 16\n[29] Yuanfu Lu, Chuan Shi, Linmei Hu, and Zhiyuan Liu. Relation structure-aware heterogeneous information\n      network embedding. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages\n     4456\u20134463, 2019. 2, 4, 14\n[30] Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang.\n      Learning to drop: Robust graph neural network via topological denoising. In Proceedings of the 14th ACM\n      international conference on web search and data mining, pages 779\u2013787, 2021. 2, 16\n[31] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann.\n     Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint arXiv:2007.08663,\n      2020. 4\n[32] Mark EJ Newman. Mixing patterns in networks. Physical review E, 67(2):026126, 2003. 18\n[33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\n      Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\n      learning library. Advances in neural information processing systems, 32, 2019. 5\n[34] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric graph\n      convolutional networks. arXiv preprint arXiv:2002.05287, 2020. 2, 4, 14, 17\n[35] Ke Sun, Zhouchen Lin, and Zhanxing Zhu. Multi-stage self-supervised learning for graph convolutional\n      networks on graphs with few labeled nodes. In Proceedings of the AAAI conference on artificial intelligence,\n     volume 34, pages 5892\u20135899, 2020. 10\n[36] Qingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Xingcheng Fu, Cheng Ji, and S Yu Philip. Graph structure\n      learning with variational information bottleneck. In Proceedings of the AAAI Conference on Artificial\n     Intelligence, volume 36, pages 4165\u20134174, 2022. 2, 4, 16, 17\n[37] Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social influence analysis in large-scale networks. In\n     Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,\n      pages 807\u2013816, 2009. 2, 14\n[38] Minjie Yu Wang. Deep graph library: Towards efficient and scalable deep learning on graphs. In ICLR\n      workshop on representation learning on graphs and manifolds, 2019. 5\n[39] Ruijia Wang, Shuai Mou, Xiao Wang, Wanpeng Xiao, Qi Ju, Chuan Shi, and Xing Xie. Graph structure\n      estimation neural networks. In Proceedings of the Web Conference 2021, pages 342\u2013353, 2021. 2, 4, 16\n[40] Oliver Wieder, Stefan Kohlbacher, M\u00e9laine Kuenemann, Arthur Garon, Pierre Ducrot, Thomas Seidel, and\n     Thierry Langer. A compact review of molecular property prediction with graph neural networks. Drug\n     Discovery Today: Technologies, 37:1\u201312, 2020. 1\n[41] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial\n      examples on graph data: Deep insights into attack and defense. arXiv preprint arXiv:1903.01610, 2019. 8\n[42] Lirong Wu, Haitao Lin, Zihan Liu, Zicheng Liu, Yufei Huang, and Stan Z Li. Homophily-enhanced\n      self-supervision for graph structure learning: Insights and directions. IEEE Transactions on Neural\n     Networks and Learning Systems, 2023. 2, 4, 16\n[43] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. Nodeformer: A scalable graph\n      structure learning transformer for node classification. Advances in Neural Information Processing Systems,\n      35:27387\u201327401, 2022. 2, 4, 16\n                                                         12", "md": "# References\n\n# List of References\n\n|#|Authors|Title|Conference/Journal|Pages|Year|\n|---|---|---|---|---|---|\n|25|Meng Liu, Hongyang Gao, and Shuiwang Ji|Towards Deeper Graph Neural Networks|Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining|338-348|2020|\n|26|Nian Liu, Xiao Wang, Lingfei Wu, Yu Chen, Xiaojie Guo, and Chuan Shi|Compact Graph Structure Learning via Mutual Information Compression|Proceedings of the ACM Web Conference 2022|1601-1610|2022|\n|27|Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He|Pick and Choose: A GNN-Based Imbalanced Learning Approach for Fraud Detection|Proceedings of the Web Conference 2021|3168-3177|2021|\n|28|Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan|Towards Unsupervised Deep Graph Structure Learning|Proceedings of the ACM Web Conference 2022|1392-1403|2022|\n|29|Yuanfu Lu, Chuan Shi, Linmei Hu, and Zhiyuan Liu|Relation Structure-Aware Heterogeneous Information Network Embedding|Proceedings of the AAAI Conference on Artificial Intelligence|4456-4463|2019|\n\n$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{#} & \\text{Authors} & \\text{Title} & \\text{Conference/Journal} & \\text{Pages} & \\text{Year} \\\\\n\\hline\n30 & Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang & Learning to Drop: Robust Graph Neural Network via Topological Denoising & Proceedings of the 14th ACM International Conference on Web Search and Data Mining & 779-787 & 2021 \\\\\n\\hline\n31 & Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann & Tudataset: A Collection of Benchmark Datasets for Learning with Graphs & arXiv preprint arXiv:2007.08663 & 2020 \\\\\n\\hline\n32 & Mark EJ Newman & Mixing Patterns in Networks & Physical Review E & 67(2):026126 & 2003 \\\\\n\\hline\n33 & Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. & Pytorch: An Imperative Style, High-Performance Deep Learning Library & Advances in Neural Information Processing Systems & 32 & 2019 \\\\\n\\hline\n34 & Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang & Geom-GCN: Geometric Graph Convolutional Networks & arXiv preprint arXiv:2002.05287 & 2020 \\\\\n\\hline\n\\end{array}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["#", "Authors", "Title", "Conference/Journal", "Pages", "Year"], ["25", "Meng Liu, Hongyang Gao, and Shuiwang Ji", "Towards Deeper Graph Neural Networks", "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining", "338-348", "2020"], ["26", "Nian Liu, Xiao Wang, Lingfei Wu, Yu Chen, Xiaojie Guo, and Chuan Shi", "Compact Graph Structure Learning via Mutual Information Compression", "Proceedings of the ACM Web Conference 2022", "1601-1610", "2022"], ["27", "Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He", "Pick and Choose: A GNN-Based Imbalanced Learning Approach for Fraud Detection", "Proceedings of the Web Conference 2021", "3168-3177", "2021"], ["28", "Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan", "Towards Unsupervised Deep Graph Structure Learning", "Proceedings of the ACM Web Conference 2022", "1392-1403", "2022"], ["29", "Yuanfu Lu, Chuan Shi, Linmei Hu, and Zhiyuan Liu", "Relation Structure-Aware Heterogeneous Information Network Embedding", "Proceedings of the AAAI Conference on Artificial Intelligence", "4456-4463", "2019"]], "md": "|#|Authors|Title|Conference/Journal|Pages|Year|\n|---|---|---|---|---|---|\n|25|Meng Liu, Hongyang Gao, and Shuiwang Ji|Towards Deeper Graph Neural Networks|Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining|338-348|2020|\n|26|Nian Liu, Xiao Wang, Lingfei Wu, Yu Chen, Xiaojie Guo, and Chuan Shi|Compact Graph Structure Learning via Mutual Information Compression|Proceedings of the ACM Web Conference 2022|1601-1610|2022|\n|27|Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He|Pick and Choose: A GNN-Based Imbalanced Learning Approach for Fraud Detection|Proceedings of the Web Conference 2021|3168-3177|2021|\n|28|Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan|Towards Unsupervised Deep Graph Structure Learning|Proceedings of the ACM Web Conference 2022|1392-1403|2022|\n|29|Yuanfu Lu, Chuan Shi, Linmei Hu, and Zhiyuan Liu|Relation Structure-Aware Heterogeneous Information Network Embedding|Proceedings of the AAAI Conference on Artificial Intelligence|4456-4463|2019|", "isPerfectTable": true, "csv": "\"#\",\"Authors\",\"Title\",\"Conference/Journal\",\"Pages\",\"Year\"\n\"25\",\"Meng Liu, Hongyang Gao, and Shuiwang Ji\",\"Towards Deeper Graph Neural Networks\",\"Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining\",\"338-348\",\"2020\"\n\"26\",\"Nian Liu, Xiao Wang, Lingfei Wu, Yu Chen, Xiaojie Guo, and Chuan Shi\",\"Compact Graph Structure Learning via Mutual Information Compression\",\"Proceedings of the ACM Web Conference 2022\",\"1601-1610\",\"2022\"\n\"27\",\"Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He\",\"Pick and Choose: A GNN-Based Imbalanced Learning Approach for Fraud Detection\",\"Proceedings of the Web Conference 2021\",\"3168-3177\",\"2021\"\n\"28\",\"Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and Shirui Pan\",\"Towards Unsupervised Deep Graph Structure Learning\",\"Proceedings of the ACM Web Conference 2022\",\"1392-1403\",\"2022\"\n\"29\",\"Yuanfu Lu, Chuan Shi, Linmei Hu, and Zhiyuan Liu\",\"Relation Structure-Aware Heterogeneous Information Network Embedding\",\"Proceedings of the AAAI Conference on Artificial Intelligence\",\"4456-4463\",\"2019\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{#} & \\text{Authors} & \\text{Title} & \\text{Conference/Journal} & \\text{Pages} & \\text{Year} \\\\\n\\hline\n30 & Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang & Learning to Drop: Robust Graph Neural Network via Topological Denoising & Proceedings of the 14th ACM International Conference on Web Search and Data Mining & 779-787 & 2021 \\\\\n\\hline\n31 & Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann & Tudataset: A Collection of Benchmark Datasets for Learning with Graphs & arXiv preprint arXiv:2007.08663 & 2020 \\\\\n\\hline\n32 & Mark EJ Newman & Mixing Patterns in Networks & Physical Review E & 67(2):026126 & 2003 \\\\\n\\hline\n33 & Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. & Pytorch: An Imperative Style, High-Performance Deep Learning Library & Advances in Neural Information Processing Systems & 32 & 2019 \\\\\n\\hline\n34 & Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang & Geom-GCN: Geometric Graph Convolutional Networks & arXiv preprint arXiv:2002.05287 & 2020 \\\\\n\\hline\n\\end{array}\n$$", "md": "$$\n\\begin{array}{|c|c|c|c|c|c|}\n\\hline\n\\text{#} & \\text{Authors} & \\text{Title} & \\text{Conference/Journal} & \\text{Pages} & \\text{Year} \\\\\n\\hline\n30 & Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang & Learning to Drop: Robust Graph Neural Network via Topological Denoising & Proceedings of the 14th ACM International Conference on Web Search and Data Mining & 779-787 & 2021 \\\\\n\\hline\n31 & Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann & Tudataset: A Collection of Benchmark Datasets for Learning with Graphs & arXiv preprint arXiv:2007.08663 & 2020 \\\\\n\\hline\n32 & Mark EJ Newman & Mixing Patterns in Networks & Physical Review E & 67(2):026126 & 2003 \\\\\n\\hline\n33 & Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. & Pytorch: An Imperative Style, High-Performance Deep Learning Library & Advances in Neural Information Processing Systems & 32 & 2019 \\\\\n\\hline\n34 & Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang & Geom-GCN: Geometric Graph Convolutional Networks & arXiv preprint arXiv:2002.05287 & 2020 \\\\\n\\hline\n\\end{array}\n$$"}]}, {"page": 13, "text": "[44] Hui Xu, Liyao Xiang, Jiahao Yu, Anqi Cao, and Xinbing Wang. Speedup robust graph structure learning\n      with low-rank information. In Proceedings of the 30th ACM International Conference on Information &\n      Knowledge Management, pages 2241\u20132250, 2021. 16\n[45] Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, and Liang Wang. Evidence-aware fake news detection with\n      graph neural networks. In Proceedings of the ACM Web Conference 2022, pages 2501\u20132510, 2022. 1\n[46] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD\n      international conference on knowledge discovery and data mining, pages 1365\u20131374, 2015. 2, 4\n[47] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph\n      embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016. 2, 4, 14\n[48] Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised convolutional\n      network. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML\n      PKDD 2020, Ghent, Belgium, September 14\u201318, 2020, Proceedings, Part III, pages 378\u2013393. Springer,\n      2021. 2, 4, 15, 16\n[49] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer\n      networks. Advances in neural information processing systems, 32, 2019. 2, 4, 14, 16, 17\n[50] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang. Mining latent structures\n      for multimedia recommendation. In Proceedings of the 29th ACM International Conference on Multimedia,\n      pages 3872\u20133880, 2021. 2\n[51] Yanfu Zhang, Hongchang Gao, Jian Pei, and Heng Huang. Robust self-supervised structural graph neural\n      network for social network prediction. In Proceedings of the ACM Web Conference 2022, pages 1352\u20131361,\n      2022. 1\n[52] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. Hierarchical\n      graph pooling with structure learning. arXiv preprint arXiv:1911.05954, 2019. 2, 4, 16, 17\n[53] Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song, and Yanfang Ye. Heterogeneous graph struc-\n      ture learning for graph neural networks. In Proceedings of the AAAI conference on artificial intelligence,\n      volume 35, pages 4697\u20134705, 2021. 2, 4, 16, 17\n[54] Jianan Zhao, Qianlong Wen, Mingxuan Ju, Chuxu Zhang, and Yanfang Ye. Self-supervised graph structure\n      refinement for graph neural networks. In Proceedings of the Sixteenth ACM International Conference on\n     Web Search and Data Mining, pages 159\u2013167, 2023. 2, 4, 16\n[55] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei\n     Wang. Robust graph representation learning via neural sparsification. In International Conference on\n      Machine Learning, pages 11458\u201311468. PMLR, 2020. 2, 8, 16\n[56] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond\n      homophily in graph neural networks: Current limitations and effective designs. Advances in Neural\n      Information Processing Systems, 33:7793\u20137804, 2020. 17\n[57] Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Yuanqi Du, Jieyu Zhang, Qiang Liu, Carl Yang, and Shu Wu. A\n      survey on graph structure learning: Progress and opportunities. arXiv e-prints, pages arXiv\u20132103, 2021. 2\n[58] Daniel Z\u00fcgner and Stephan G\u00fcnnemann. Adversarial attacks on graph neural networks via meta learning.\n      2019. 8\n                                                      13", "md": "- [44] Hui Xu, Liyao Xiang, Jiahao Yu, Anqi Cao, and Xinbing Wang. Speedup robust graph structure learning with low-rank information. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2241\u20132250, 2021. 16\n- [45] Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, and Liang Wang. Evidence-aware fake news detection with graph neural networks. In Proceedings of the ACM Web Conference 2022, pages 2501\u20132510, 2022. 1\n- [46] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365\u20131374, 2015. 2, 4\n- [47] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016. 2, 4, 14\n- [48] Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised convolutional network. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14\u201318, 2020, Proceedings, Part III, pages 378\u2013393. Springer, 2021. 2, 4, 15, 16\n- [49] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. Advances in neural information processing systems, 32, 2019. 2, 4, 14, 16, 17\n- [50] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang. Mining latent structures for multimedia recommendation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 3872\u20133880, 2021. 2\n- [51] Yanfu Zhang, Hongchang Gao, Jian Pei, and Heng Huang. Robust self-supervised structural graph neural network for social network prediction. In Proceedings of the ACM Web Conference 2022, pages 1352\u20131361, 2022. 1\n- [52] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. Hierarchical graph pooling with structure learning. arXiv preprint arXiv:1911.05954, 2019. 2, 4, 16, 17\n- [53] Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song, and Yanfang Ye. Heterogeneous graph structure learning for graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 4697\u20134705, 2021. 2, 4, 16, 17\n- [54] Jianan Zhao, Qianlong Wen, Mingxuan Ju, Chuxu Zhang, and Yanfang Ye. Self-supervised graph structure refinement for graph neural networks. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 159\u2013167, 2023. 2, 4, 16\n- [55] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In International Conference on Machine Learning, pages 11458\u201311468. PMLR, 2020. 2, 8, 16\n- [56] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793\u20137804, 2020. 17\n- [57] Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Yuanqi Du, Jieyu Zhang, Qiang Liu, Carl Yang, and Shu Wu. A survey on graph structure learning: Progress and opportunities. arXiv e-prints, pages arXiv\u20132103, 2021. 2\n- [58] Daniel Z\u00fcgner and Stephan G\u00fcnnemann. Adversarial attacks on graph neural networks via meta learning. 2019. 8", "images": [], "items": [{"type": "text", "value": "- [44] Hui Xu, Liyao Xiang, Jiahao Yu, Anqi Cao, and Xinbing Wang. Speedup robust graph structure learning with low-rank information. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2241\u20132250, 2021. 16\n- [45] Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, and Liang Wang. Evidence-aware fake news detection with graph neural networks. In Proceedings of the ACM Web Conference 2022, pages 2501\u20132510, 2022. 1\n- [46] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365\u20131374, 2015. 2, 4\n- [47] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016. 2, 4, 14\n- [48] Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised convolutional network. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14\u201318, 2020, Proceedings, Part III, pages 378\u2013393. Springer, 2021. 2, 4, 15, 16\n- [49] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. Advances in neural information processing systems, 32, 2019. 2, 4, 14, 16, 17\n- [50] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang. Mining latent structures for multimedia recommendation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 3872\u20133880, 2021. 2\n- [51] Yanfu Zhang, Hongchang Gao, Jian Pei, and Heng Huang. Robust self-supervised structural graph neural network for social network prediction. In Proceedings of the ACM Web Conference 2022, pages 1352\u20131361, 2022. 1\n- [52] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. Hierarchical graph pooling with structure learning. arXiv preprint arXiv:1911.05954, 2019. 2, 4, 16, 17\n- [53] Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song, and Yanfang Ye. Heterogeneous graph structure learning for graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 4697\u20134705, 2021. 2, 4, 16, 17\n- [54] Jianan Zhao, Qianlong Wen, Mingxuan Ju, Chuxu Zhang, and Yanfang Ye. Self-supervised graph structure refinement for graph neural networks. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 159\u2013167, 2023. 2, 4, 16\n- [55] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In International Conference on Machine Learning, pages 11458\u201311468. PMLR, 2020. 2, 8, 16\n- [56] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793\u20137804, 2020. 17\n- [57] Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Yuanqi Du, Jieyu Zhang, Qiang Liu, Carl Yang, and Shu Wu. A survey on graph structure learning: Progress and opportunities. arXiv e-prints, pages arXiv\u20132103, 2021. 2\n- [58] Daniel Z\u00fcgner and Stephan G\u00fcnnemann. Adversarial attacks on graph neural networks via meta learning. 2019. 8", "md": "- [44] Hui Xu, Liyao Xiang, Jiahao Yu, Anqi Cao, and Xinbing Wang. Speedup robust graph structure learning with low-rank information. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 2241\u20132250, 2021. 16\n- [45] Weizhi Xu, Junfei Wu, Qiang Liu, Shu Wu, and Liang Wang. Evidence-aware fake news detection with graph neural networks. In Proceedings of the ACM Web Conference 2022, pages 2501\u20132510, 2022. 1\n- [46] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining, pages 1365\u20131374, 2015. 2, 4\n- [47] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning, pages 40\u201348. PMLR, 2016. 2, 4, 14\n- [48] Donghan Yu, Ruohong Zhang, Zhengbao Jiang, Yuexin Wu, and Yiming Yang. Graph-revised convolutional network. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14\u201318, 2020, Proceedings, Part III, pages 378\u2013393. Springer, 2021. 2, 4, 15, 16\n- [49] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer networks. Advances in neural information processing systems, 32, 2019. 2, 4, 14, 16, 17\n- [50] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, and Liang Wang. Mining latent structures for multimedia recommendation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 3872\u20133880, 2021. 2\n- [51] Yanfu Zhang, Hongchang Gao, Jian Pei, and Heng Huang. Robust self-supervised structural graph neural network for social network prediction. In Proceedings of the ACM Web Conference 2022, pages 1352\u20131361, 2022. 1\n- [52] Zhen Zhang, Jiajun Bu, Martin Ester, Jianfeng Zhang, Chengwei Yao, Zhi Yu, and Can Wang. Hierarchical graph pooling with structure learning. arXiv preprint arXiv:1911.05954, 2019. 2, 4, 16, 17\n- [53] Jianan Zhao, Xiao Wang, Chuan Shi, Binbin Hu, Guojie Song, and Yanfang Ye. Heterogeneous graph structure learning for graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 4697\u20134705, 2021. 2, 4, 16, 17\n- [54] Jianan Zhao, Qianlong Wen, Mingxuan Ju, Chuxu Zhang, and Yanfang Ye. Self-supervised graph structure refinement for graph neural networks. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages 159\u2013167, 2023. 2, 4, 16\n- [55] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In International Conference on Machine Learning, pages 11458\u201311468. PMLR, 2020. 2, 8, 16\n- [56] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793\u20137804, 2020. 17\n- [57] Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Yuanqi Du, Jieyu Zhang, Qiang Liu, Carl Yang, and Shu Wu. A survey on graph structure learning: Progress and opportunities. arXiv e-prints, pages arXiv\u20132103, 2021. 2\n- [58] Daniel Z\u00fcgner and Stephan G\u00fcnnemann. Adversarial attacks on graph neural networks via meta learning. 2019. 8"}]}], "job_id": "4512ff40-6fcf-49f4-85b5-922aedaf27ed", "file_path": "./corpus/505_gslb_the_graph_structure_learn.pdf"}