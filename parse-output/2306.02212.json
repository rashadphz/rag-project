{"pages": [{"page": 1, "text": "             Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for\n                                                        Smooth Convex Optimization\n                                                         Ruichen Jiang\u2217                        Aryan Mokhtari\u2020\narXiv:2306.02212v1  [math.OC]  3 Jun 2023\n                                                                                  Abstract\n                           In this paper, we present an accelerated quasi-Newton proximal extragradient method\n                      for solving unconstrained smooth convex optimization problems.                                     With access only to the\n                      gradients of the objective function, we prove that our method can achieve a convergence rate of\n                      O   min{ 1  k2 ,\u221a  d log k}  , where d is the problem dimension and k is the number of iterations. In\n                                         k2.5\n                      particular, in the regime where k = O(d), our method matches the optimal rate of O( 1                                       k2 ) by\n                      Nesterov\u2019s accelerated gradient (NAG). Moreover, in the the regime where k = \u2126(d log d), it\n                      outperforms NAG and converges at a faster rate of O                           \u221ad log k    . To the best of our knowledge,\n                                                                                                       k2.5\n                      this result is the first to demonstrate a provable gain for a quasi-Newton-type method over NAG\n                      in the convex setting. To achieve such results, we build our method on a recent variant of the\n                      Monteiro-Svaiter acceleration framework and adopt an online learning perspective to update the\n                      Hessian approximation matrices, in which we relate the convergence rate of our method to the\n                      dynamic regret of a specific online convex optimization problem in the space of matrices.\n                 \u2217Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n             {rjiang@utexas.edu}\n                 \u2020Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n             {mokhtari@austin.utexas.edu}\n                                                                                         1", "md": "# Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization\n\n# Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization\n\nRuichen Jiang* Aryan Mokhtari\u2020\n\narXiv:2306.02212v1 [math.OC] 3 Jun 2023\n\n## Abstract\n\nIn this paper, we present an accelerated quasi-Newton proximal extragradient method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective function, we prove that our method can achieve a convergence rate of $$O\\left(\\min\\left\\{ \\frac{1}{k^2}, \\sqrt{d \\log k} \\right\\}\\right)$$, where d is the problem dimension and k is the number of iterations. In particular, in the regime where k = O(d), our method matches the optimal rate of $$O\\left(\\frac{1}{k^2}\\right)$$ by Nesterov\u2019s accelerated gradient (NAG). Moreover, in the regime where k = \u2126(d log d), it outperforms NAG and converges at a faster rate of $$O\\left(\\sqrt{d \\log k}\\right)$$. To the best of our knowledge, this result is the first to demonstrate a provable gain for a quasi-Newton-type method over NAG in the convex setting. To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an online learning perspective to update the Hessian approximation matrices, in which we relate the convergence rate of our method to the dynamic regret of a specific online convex optimization problem in the space of matrices.\n\n*Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n\n{rjiang@utexas.edu}\n\n\u2020Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n\n{mokhtari@austin.utexas.edu}", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization", "md": "# Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization"}, {"type": "heading", "lvl": 1, "value": "Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization", "md": "# Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization"}, {"type": "text", "value": "Ruichen Jiang* Aryan Mokhtari\u2020\n\narXiv:2306.02212v1 [math.OC] 3 Jun 2023", "md": "Ruichen Jiang* Aryan Mokhtari\u2020\n\narXiv:2306.02212v1 [math.OC] 3 Jun 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "In this paper, we present an accelerated quasi-Newton proximal extragradient method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective function, we prove that our method can achieve a convergence rate of $$O\\left(\\min\\left\\{ \\frac{1}{k^2}, \\sqrt{d \\log k} \\right\\}\\right)$$, where d is the problem dimension and k is the number of iterations. In particular, in the regime where k = O(d), our method matches the optimal rate of $$O\\left(\\frac{1}{k^2}\\right)$$ by Nesterov\u2019s accelerated gradient (NAG). Moreover, in the regime where k = \u2126(d log d), it outperforms NAG and converges at a faster rate of $$O\\left(\\sqrt{d \\log k}\\right)$$. To the best of our knowledge, this result is the first to demonstrate a provable gain for a quasi-Newton-type method over NAG in the convex setting. To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an online learning perspective to update the Hessian approximation matrices, in which we relate the convergence rate of our method to the dynamic regret of a specific online convex optimization problem in the space of matrices.\n\n*Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n\n{rjiang@utexas.edu}\n\n\u2020Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n\n{mokhtari@austin.utexas.edu}", "md": "In this paper, we present an accelerated quasi-Newton proximal extragradient method for solving unconstrained smooth convex optimization problems. With access only to the gradients of the objective function, we prove that our method can achieve a convergence rate of $$O\\left(\\min\\left\\{ \\frac{1}{k^2}, \\sqrt{d \\log k} \\right\\}\\right)$$, where d is the problem dimension and k is the number of iterations. In particular, in the regime where k = O(d), our method matches the optimal rate of $$O\\left(\\frac{1}{k^2}\\right)$$ by Nesterov\u2019s accelerated gradient (NAG). Moreover, in the regime where k = \u2126(d log d), it outperforms NAG and converges at a faster rate of $$O\\left(\\sqrt{d \\log k}\\right)$$. To the best of our knowledge, this result is the first to demonstrate a provable gain for a quasi-Newton-type method over NAG in the convex setting. To achieve such results, we build our method on a recent variant of the Monteiro-Svaiter acceleration framework and adopt an online learning perspective to update the Hessian approximation matrices, in which we relate the convergence rate of our method to the dynamic regret of a specific online convex optimization problem in the space of matrices.\n\n*Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n\n{rjiang@utexas.edu}\n\n\u2020Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA\n\n{mokhtari@austin.utexas.edu}"}]}, {"page": 2, "text": "1     Introduction\nIn this paper, we consider the following unconstrained convex minimization problem\n                                              min    f(x),                                           (1)\n                                              x\u2208Rd\nwhere the objective function f : Rd \u2192   R is convex and differentiable. We are particularly interested\nin quasi-Newton methods, which are among the most popular iterative methods for solving the\nproblem in (1) [Dav59; FP63; Bro70; Fle70; Gol70; Sha70; CGT91; KBS93]. Like gradient descent\nand other first-order methods, quasi-Newton methods require only the objective\u2019s gradients to update\nthe iterates. On the other hand, they can better exploit the local curvature of f by constructing a\nHessian approximation matrix and using it as a preconditioner, leading to superior convergence\nperformance. In particular, when the objective function in (1) is strictly convex or strongly convex,\nit has long been proved that quasi-Newton methods achieve an asymptotic superlinear convergence\nrate [CGT91; KBS93; BDM73; DM74; Pow71; Dix72; BNY87; BKS96; NW06], which significantly\nimproves the linear convergence rate obtained by first-order methods. More recently, there has\nbeen progress on establishing a local non-asymptotic superlinear rate of the form O((1/      \u221a k)k) for\nclassical quasi-Newton methods and their variants [RN21a; RN21c; RN21b; JM22; JKRM22; LYZ21;\nYLCZ22].\nHowever, all of the results above only apply under the restrictive assumption that the objective\nfunction f is strictly or strongly convex. In the more general setting where f is merely convex, to the\nbest of our knowledge, there is no result that demonstrates any form of convergence improvement\nby quasi-Newton methods over first-order methods. More precisely, it is well known that Nesterov\u2019s\naccelerated gradient (NAG) [Nes83] can achieve a convergence rate of O(1/k2) if f is convex and has\nLipschitz gradients. On the other hand, under the same setting, asymptotic convergence of classical\nquasi-Newton methods has been shown in [BNY87; Pow72] but no explicit rate is given. With\ncertain conditions on the Hessian approximation matrices, the works in [ST16; GS18; KZAT23]\npresented quasi-Newton-type methods with convergence rates of O(1/k) and O(1/k2), respectively,\nwhich are no better than the rate of NAG and, in fact, can be even worse in terms of constants.\nThis gap raises the following fundamental question:\n      Can we design a quasi-Newton-type method that achieves a convergence rate faster than\n      O(1/k2) for the smooth convex minimization problem in (1)?\nAt first glance, this may seem impossible, as for any first-order method that has access only to a\ngradient oracle, one can construct a \u201cworst-case\u201d instance and establish a lower bound of \u2126(1/k2)\non the optimality gap [NY83; Nes18]. It is worth noting that while such a lower bound is typically\nshown under a \u201clinear span\u201d assumption, i.e., the methods only query points in the span of the\ngradients they observe, this assumption is in fact not necessary and can be removed by the technique\nof resisting oracle (see, [CDHS20, Section 3.3]). In particular, this \u2126(1/k2) lower bound applies for\nany iterative method that only queries gradients of the objective, including quasi-Newton methods.\nOn the other hand, this lower bound is subject to a crucial assumption: it only works in the\nhigh-dimensional regime where the problem dimension d exceeds the number of iterations k. As\nsuch, it does not rule out the possibility of a faster rate than O(1/k2) when the number of iterations\nk is larger than d. Hence, ideally, we are looking for a method that attains the optimal rate of\nO(1/k2) in the regime that k = O(d) and surpasses this rate in the regime that k = \u2126(d).\nContributions. In this paper, we achieve the above goal by presenting an accelerated quasi-Newton\n                                                   2", "md": "# Introduction\n\n# Introduction\n\nIn this paper, we consider the following unconstrained convex minimization problem\n\n$$\n\\min_{x \\in \\mathbb{R}^d} f(x), \\quad (1)\n$$\nwhere the objective function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) is convex and differentiable. We are particularly interested in quasi-Newton methods, which are among the most popular iterative methods for solving the problem in (1) [Dav59; FP63; Bro70; Fle70; Gol70; Sha70; CGT91; KBS93]. Like gradient descent and other first-order methods, quasi-Newton methods require only the objective\u2019s gradients to update the iterates. On the other hand, they can better exploit the local curvature of \\( f \\) by constructing a Hessian approximation matrix and using it as a preconditioner, leading to superior convergence performance. In particular, when the objective function in (1) is strictly convex or strongly convex, it has long been proved that quasi-Newton methods achieve an asymptotic superlinear convergence rate [CGT91; KBS93; BDM73; DM74; Pow71; Dix72; BNY87; BKS96; NW06], which significantly improves the linear convergence rate obtained by first-order methods. More recently, there has been progress on establishing a local non-asymptotic superlinear rate of the form \\( O\\left(\\frac{1}{\\sqrt{k}}\\right)^k \\) for classical quasi-Newton methods and their variants [RN21a; RN21c; RN21b; JM22; JKRM22; LYZ21; YLCZ22].\n\nHowever, all of the results above only apply under the restrictive assumption that the objective function \\( f \\) is strictly or strongly convex. In the more general setting where \\( f \\) is merely convex, to the best of our knowledge, there is no result that demonstrates any form of convergence improvement by quasi-Newton methods over first-order methods. More precisely, it is well known that Nesterov\u2019s accelerated gradient (NAG) [Nes83] can achieve a convergence rate of \\( O\\left(\\frac{1}{k^2}\\right) \\) if \\( f \\) is convex and has Lipschitz gradients. On the other hand, under the same setting, asymptotic convergence of classical quasi-Newton methods has been shown in [BNY87; Pow72] but no explicit rate is given. With certain conditions on the Hessian approximation matrices, the works in [ST16; GS18; KZAT23] presented quasi-Newton-type methods with convergence rates of \\( O\\left(\\frac{1}{k}\\right) \\) and \\( O\\left(\\frac{1}{k^2}\\right) \\), respectively, which are no better than the rate of NAG and, in fact, can be even worse in terms of constants. This gap raises the following fundamental question:\n\nCan we design a quasi-Newton-type method that achieves a convergence rate faster than \\( O\\left(\\frac{1}{k^2}\\right) \\) for the smooth convex minimization problem in (1)?\n\nAt first glance, this may seem impossible, as for any first-order method that has access only to a gradient oracle, one can construct a \u201cworst-case\u201d instance and establish a lower bound of \\( \\Omega\\left(\\frac{1}{k^2}\\right) \\) on the optimality gap [NY83; Nes18]. It is worth noting that while such a lower bound is typically shown under a \u201clinear span\u201d assumption, i.e., the methods only query points in the span of the gradients they observe, this assumption is in fact not necessary and can be removed by the technique of resisting oracle (see, [CDHS20, Section 3.3]). In particular, this \\( \\Omega\\left(\\frac{1}{k^2}\\right) \\) lower bound applies for any iterative method that only queries gradients of the objective, including quasi-Newton methods.\n\nOn the other hand, this lower bound is subject to a crucial assumption: it only works in the high-dimensional regime where the problem dimension \\( d \\) exceeds the number of iterations \\( k \\). As such, it does not rule out the possibility of a faster rate than \\( O\\left(\\frac{1}{k^2}\\right) \\) when the number of iterations \\( k \\) is larger than \\( d \\). Hence, ideally, we are looking for a method that attains the optimal rate of \\( O\\left(\\frac{1}{k^2}\\right) \\) in the regime that \\( k = O(d) \\) and surpasses this rate in the regime that \\( k = \\Omega(d) \\).\n\n## Contributions\n\nIn this paper, we achieve the above goal by presenting an accelerated quasi-Newton", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Introduction", "md": "# Introduction"}, {"type": "heading", "lvl": 1, "value": "Introduction", "md": "# Introduction"}, {"type": "text", "value": "In this paper, we consider the following unconstrained convex minimization problem\n\n$$\n\\min_{x \\in \\mathbb{R}^d} f(x), \\quad (1)\n$$\nwhere the objective function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) is convex and differentiable. We are particularly interested in quasi-Newton methods, which are among the most popular iterative methods for solving the problem in (1) [Dav59; FP63; Bro70; Fle70; Gol70; Sha70; CGT91; KBS93]. Like gradient descent and other first-order methods, quasi-Newton methods require only the objective\u2019s gradients to update the iterates. On the other hand, they can better exploit the local curvature of \\( f \\) by constructing a Hessian approximation matrix and using it as a preconditioner, leading to superior convergence performance. In particular, when the objective function in (1) is strictly convex or strongly convex, it has long been proved that quasi-Newton methods achieve an asymptotic superlinear convergence rate [CGT91; KBS93; BDM73; DM74; Pow71; Dix72; BNY87; BKS96; NW06], which significantly improves the linear convergence rate obtained by first-order methods. More recently, there has been progress on establishing a local non-asymptotic superlinear rate of the form \\( O\\left(\\frac{1}{\\sqrt{k}}\\right)^k \\) for classical quasi-Newton methods and their variants [RN21a; RN21c; RN21b; JM22; JKRM22; LYZ21; YLCZ22].\n\nHowever, all of the results above only apply under the restrictive assumption that the objective function \\( f \\) is strictly or strongly convex. In the more general setting where \\( f \\) is merely convex, to the best of our knowledge, there is no result that demonstrates any form of convergence improvement by quasi-Newton methods over first-order methods. More precisely, it is well known that Nesterov\u2019s accelerated gradient (NAG) [Nes83] can achieve a convergence rate of \\( O\\left(\\frac{1}{k^2}\\right) \\) if \\( f \\) is convex and has Lipschitz gradients. On the other hand, under the same setting, asymptotic convergence of classical quasi-Newton methods has been shown in [BNY87; Pow72] but no explicit rate is given. With certain conditions on the Hessian approximation matrices, the works in [ST16; GS18; KZAT23] presented quasi-Newton-type methods with convergence rates of \\( O\\left(\\frac{1}{k}\\right) \\) and \\( O\\left(\\frac{1}{k^2}\\right) \\), respectively, which are no better than the rate of NAG and, in fact, can be even worse in terms of constants. This gap raises the following fundamental question:\n\nCan we design a quasi-Newton-type method that achieves a convergence rate faster than \\( O\\left(\\frac{1}{k^2}\\right) \\) for the smooth convex minimization problem in (1)?\n\nAt first glance, this may seem impossible, as for any first-order method that has access only to a gradient oracle, one can construct a \u201cworst-case\u201d instance and establish a lower bound of \\( \\Omega\\left(\\frac{1}{k^2}\\right) \\) on the optimality gap [NY83; Nes18]. It is worth noting that while such a lower bound is typically shown under a \u201clinear span\u201d assumption, i.e., the methods only query points in the span of the gradients they observe, this assumption is in fact not necessary and can be removed by the technique of resisting oracle (see, [CDHS20, Section 3.3]). In particular, this \\( \\Omega\\left(\\frac{1}{k^2}\\right) \\) lower bound applies for any iterative method that only queries gradients of the objective, including quasi-Newton methods.\n\nOn the other hand, this lower bound is subject to a crucial assumption: it only works in the high-dimensional regime where the problem dimension \\( d \\) exceeds the number of iterations \\( k \\). As such, it does not rule out the possibility of a faster rate than \\( O\\left(\\frac{1}{k^2}\\right) \\) when the number of iterations \\( k \\) is larger than \\( d \\). Hence, ideally, we are looking for a method that attains the optimal rate of \\( O\\left(\\frac{1}{k^2}\\right) \\) in the regime that \\( k = O(d) \\) and surpasses this rate in the regime that \\( k = \\Omega(d) \\).", "md": "In this paper, we consider the following unconstrained convex minimization problem\n\n$$\n\\min_{x \\in \\mathbb{R}^d} f(x), \\quad (1)\n$$\nwhere the objective function \\( f : \\mathbb{R}^d \\rightarrow \\mathbb{R} \\) is convex and differentiable. We are particularly interested in quasi-Newton methods, which are among the most popular iterative methods for solving the problem in (1) [Dav59; FP63; Bro70; Fle70; Gol70; Sha70; CGT91; KBS93]. Like gradient descent and other first-order methods, quasi-Newton methods require only the objective\u2019s gradients to update the iterates. On the other hand, they can better exploit the local curvature of \\( f \\) by constructing a Hessian approximation matrix and using it as a preconditioner, leading to superior convergence performance. In particular, when the objective function in (1) is strictly convex or strongly convex, it has long been proved that quasi-Newton methods achieve an asymptotic superlinear convergence rate [CGT91; KBS93; BDM73; DM74; Pow71; Dix72; BNY87; BKS96; NW06], which significantly improves the linear convergence rate obtained by first-order methods. More recently, there has been progress on establishing a local non-asymptotic superlinear rate of the form \\( O\\left(\\frac{1}{\\sqrt{k}}\\right)^k \\) for classical quasi-Newton methods and their variants [RN21a; RN21c; RN21b; JM22; JKRM22; LYZ21; YLCZ22].\n\nHowever, all of the results above only apply under the restrictive assumption that the objective function \\( f \\) is strictly or strongly convex. In the more general setting where \\( f \\) is merely convex, to the best of our knowledge, there is no result that demonstrates any form of convergence improvement by quasi-Newton methods over first-order methods. More precisely, it is well known that Nesterov\u2019s accelerated gradient (NAG) [Nes83] can achieve a convergence rate of \\( O\\left(\\frac{1}{k^2}\\right) \\) if \\( f \\) is convex and has Lipschitz gradients. On the other hand, under the same setting, asymptotic convergence of classical quasi-Newton methods has been shown in [BNY87; Pow72] but no explicit rate is given. With certain conditions on the Hessian approximation matrices, the works in [ST16; GS18; KZAT23] presented quasi-Newton-type methods with convergence rates of \\( O\\left(\\frac{1}{k}\\right) \\) and \\( O\\left(\\frac{1}{k^2}\\right) \\), respectively, which are no better than the rate of NAG and, in fact, can be even worse in terms of constants. This gap raises the following fundamental question:\n\nCan we design a quasi-Newton-type method that achieves a convergence rate faster than \\( O\\left(\\frac{1}{k^2}\\right) \\) for the smooth convex minimization problem in (1)?\n\nAt first glance, this may seem impossible, as for any first-order method that has access only to a gradient oracle, one can construct a \u201cworst-case\u201d instance and establish a lower bound of \\( \\Omega\\left(\\frac{1}{k^2}\\right) \\) on the optimality gap [NY83; Nes18]. It is worth noting that while such a lower bound is typically shown under a \u201clinear span\u201d assumption, i.e., the methods only query points in the span of the gradients they observe, this assumption is in fact not necessary and can be removed by the technique of resisting oracle (see, [CDHS20, Section 3.3]). In particular, this \\( \\Omega\\left(\\frac{1}{k^2}\\right) \\) lower bound applies for any iterative method that only queries gradients of the objective, including quasi-Newton methods.\n\nOn the other hand, this lower bound is subject to a crucial assumption: it only works in the high-dimensional regime where the problem dimension \\( d \\) exceeds the number of iterations \\( k \\). As such, it does not rule out the possibility of a faster rate than \\( O\\left(\\frac{1}{k^2}\\right) \\) when the number of iterations \\( k \\) is larger than \\( d \\). Hence, ideally, we are looking for a method that attains the optimal rate of \\( O\\left(\\frac{1}{k^2}\\right) \\) in the regime that \\( k = O(d) \\) and surpasses this rate in the regime that \\( k = \\Omega(d) \\)."}, {"type": "heading", "lvl": 2, "value": "Contributions", "md": "## Contributions"}, {"type": "text", "value": "In this paper, we achieve the above goal by presenting an accelerated quasi-Newton", "md": "In this paper, we achieve the above goal by presenting an accelerated quasi-Newton"}]}, {"page": 3, "text": "proximal extragradient (A-QNPE) method. Specifically, under the assumptions that f in (1) is\nconvex and its gradient and Hessian are Lipschitz, we prove the following guarantees:\n     \u2022 From any initialization, A-QNPE can attain a global convergence rate of O                                                 min{ 1  k2 ,  \u221a d log k }  .\n         In particular, this implies that our method matches the optimal rate of O( 1                                         k2 ) when k = O(d),k2.5\n                                                                              \u221a d log k\n         while it converges at a faster rate of O(                              k2.5    ) when k = \u2126(d log d).                   Alternatively, we\n         can bound the number of iterations required to achieve an \u03f5-accurate solution by N\u03f5 =\n         O    min{ 1  \u03f50.5 , d0.2                    .\n                             \u03f50.4 (log d  \u03f52 )0.2}\n     \u2022 In terms of computational cost, we show that the total number of gradient queries after N\n         iterations can be bounded by 3N, i.e., on average no more than 3 per iteration. Moreover,\n         the number of matrix-vector products to achieve an \u03f5-accurate solution can be bounded by\n         O\u02dc   min{d0.25\u03f50.5 ,    1        .\n                               \u03f50.625 }\nCombining the two results above, we conclude that A-QNPE requires O                                                   min{ 1  \u03f50.5 , d0.2\ngradient queries to reach an \u03f5-accurate solution, which is at least as good as NAG and is further                                    \u03f50.4 (log d \u03f52 )0.2}\nsuperior when \u03f5 = O                    1        . To the best of our knowledge, this is the first result that demonstrates\n                                  d2 log2(d)\na provable advantage of a quasi-Newton-type method over NAG in terms of gradient oracle complexity\nin the smooth convex setting.\nTo obtain these results, we significantly deviate from the classical quasi-Newton methods such as\nBFGS and DFP. Specifically, instead of mimicking Newton\u2019s method as in the classical updates, our\nA-QNPE method is built upon the celebrated Monteiro-Svaiter (MS) acceleration framework [MS13;\nCHJJS22], which can be regarded as an inexact version of the accelerated proximal point method\n[G\u00a8 ul92; SV12]. Another major difference lies in the update rule of the Hessian approximation\nmatrix.        Classical quasi-Newton methods typically perform a low-rank update of the Hessian\napproximation matrix while enforcing the secant condition. On the contrary, our update rule is\npurely driven by our convergence analysis of the MS acceleration framework. In particular, inspired\nby [JJM23], we assign certain loss functions to the Hessian approximation matrices and formulate\nthe Hessian approximation matrix update as an online convex optimization problem in the space\nof matrices. Therefore, we propose to update the Hessian approximation matrices via an online\nlearning algorithm.\nRelated work. The authors in [CHJJS22] proposed a refined MS acceleration framework, which\nsimplifies the line search subroutine in the original MS method [MS13]. By instantiating it with an\nadaptive second-order oracle, they presented an accelerated second-order method that achieves the\n                               1\noptimal rate of O(k3.5 ). The framework in [CHJJS22] serves as a basis for our method, but we focus\non the setting where we have access only to a gradient oracle and we consider a quasi-Newton-type\nupdate. Another closely related work is [JJM23], where the authors proposed a quasi-Newton\nproximal extragradient method with a global non-asymptotic superlinear rate. In particular, our\nHessian approximation update is inspired by the online learning framework in [JJM23]. On the\nother hand, the major difference is that the authors in [JJM23] focused on the case where f is\nstrongly convex and presented a global superlinear rate, while we consider the more general convex\nsetting where f is only convex (may not be strongly convex). Moreover, we further incorporate the\nacceleration mechanism into our method, which greatly complicates the convergence analysis; see\nRemark 3.1 for more discussions.\n                                                                              3", "md": "proximal extragradient (A-QNPE) method. Specifically, under the assumptions that \\( f \\) in (1) is\nconvex and its gradient and Hessian are Lipschitz, we prove the following guarantees:\n\n- From any initialization, A-QNPE can attain a global convergence rate of \\( O \\min\\left\\{ \\frac{1}{k^2}, \\sqrt{d \\log k} \\right\\} \\). In particular, this implies that our method matches the optimal rate of \\( O\\left( \\frac{1}{k^2} \\right) \\) when \\( k = O(d), k^{2.5} \\sqrt{d \\log k} \\) while it converges at a faster rate of \\( O\\left( k^{2.5} \\right) \\) when \\( k = \\Omega(d \\log d) \\). Alternatively, we can bound the number of iterations required to achieve an \\( \\epsilon \\)-accurate solution by \\( N_{\\epsilon} = O \\min\\left\\{ \\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} \\left( \\log d \\frac{1}{\\epsilon^2} \\right)^{0.2} \\right\\} \\).\n- In terms of computational cost, we show that the total number of gradient queries after \\( N \\) iterations can be bounded by \\( 3N \\), i.e., on average no more than 3 per iteration. Moreover, the number of matrix-vector products to achieve an \\( \\epsilon \\)-accurate solution can be bounded by \\( \\tilde{O} \\min\\left\\{ d^{0.25} \\epsilon^{0.5}, \\frac{1}{\\epsilon^{0.625}} \\right\\} \\).\n\nCombining the two results above, we conclude that A-QNPE requires \\( O \\min\\left\\{ \\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} \\left( \\log d \\frac{1}{\\epsilon^2} \\right)^{0.2} \\right\\} \\) gradient queries to reach an \\( \\epsilon \\)-accurate solution, which is at least as good as NAG and is further superior when \\( \\epsilon = O \\frac{1}{d^2 \\log^2(d)} \\). To the best of our knowledge, this is the first result that demonstrates a provable advantage of a quasi-Newton-type method over NAG in terms of gradient oracle complexity in the smooth convex setting.\n\nTo obtain these results, we significantly deviate from the classical quasi-Newton methods such as BFGS and DFP. Specifically, instead of mimicking Newton\u2019s method as in the classical updates, our A-QNPE method is built upon the celebrated Monteiro-Svaiter (MS) acceleration framework [MS13; CHJJS22], which can be regarded as an inexact version of the accelerated proximal point method [G\u00fcl92; SV12]. Another major difference lies in the update rule of the Hessian approximation matrix. Classical quasi-Newton methods typically perform a low-rank update of the Hessian approximation matrix while enforcing the secant condition. On the contrary, our update rule is purely driven by our convergence analysis of the MS acceleration framework. In particular, inspired by [JJM23], we assign certain loss functions to the Hessian approximation matrices and formulate the Hessian approximation matrix update as an online convex optimization problem in the space of matrices. Therefore, we propose to update the Hessian approximation matrices via an online learning algorithm.\n\nRelated work. The authors in [CHJJS22] proposed a refined MS acceleration framework, which simplifies the line search subroutine in the original MS method [MS13]. By instantiating it with an adaptive second-order oracle, they presented an accelerated second-order method that achieves the optimal rate of \\( O(k^{3.5}) \\). The framework in [CHJJS22] serves as a basis for our method, but we focus on the setting where we have access only to a gradient oracle and we consider a quasi-Newton-type update. Another closely related work is [JJM23], where the authors proposed a quasi-Newton proximal extragradient method with a global non-asymptotic superlinear rate. In particular, our Hessian approximation update is inspired by the online learning framework in [JJM23]. On the other hand, the major difference is that the authors in [JJM23] focused on the case where \\( f \\) is strongly convex and presented a global superlinear rate, while we consider the more general convex setting where \\( f \\) is only convex (may not be strongly convex). Moreover, we further incorporate the acceleration mechanism into our method, which greatly complicates the convergence analysis; see Remark 3.1 for more discussions.", "images": [], "items": [{"type": "text", "value": "proximal extragradient (A-QNPE) method. Specifically, under the assumptions that \\( f \\) in (1) is\nconvex and its gradient and Hessian are Lipschitz, we prove the following guarantees:\n\n- From any initialization, A-QNPE can attain a global convergence rate of \\( O \\min\\left\\{ \\frac{1}{k^2}, \\sqrt{d \\log k} \\right\\} \\). In particular, this implies that our method matches the optimal rate of \\( O\\left( \\frac{1}{k^2} \\right) \\) when \\( k = O(d), k^{2.5} \\sqrt{d \\log k} \\) while it converges at a faster rate of \\( O\\left( k^{2.5} \\right) \\) when \\( k = \\Omega(d \\log d) \\). Alternatively, we can bound the number of iterations required to achieve an \\( \\epsilon \\)-accurate solution by \\( N_{\\epsilon} = O \\min\\left\\{ \\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} \\left( \\log d \\frac{1}{\\epsilon^2} \\right)^{0.2} \\right\\} \\).\n- In terms of computational cost, we show that the total number of gradient queries after \\( N \\) iterations can be bounded by \\( 3N \\), i.e., on average no more than 3 per iteration. Moreover, the number of matrix-vector products to achieve an \\( \\epsilon \\)-accurate solution can be bounded by \\( \\tilde{O} \\min\\left\\{ d^{0.25} \\epsilon^{0.5}, \\frac{1}{\\epsilon^{0.625}} \\right\\} \\).\n\nCombining the two results above, we conclude that A-QNPE requires \\( O \\min\\left\\{ \\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} \\left( \\log d \\frac{1}{\\epsilon^2} \\right)^{0.2} \\right\\} \\) gradient queries to reach an \\( \\epsilon \\)-accurate solution, which is at least as good as NAG and is further superior when \\( \\epsilon = O \\frac{1}{d^2 \\log^2(d)} \\). To the best of our knowledge, this is the first result that demonstrates a provable advantage of a quasi-Newton-type method over NAG in terms of gradient oracle complexity in the smooth convex setting.\n\nTo obtain these results, we significantly deviate from the classical quasi-Newton methods such as BFGS and DFP. Specifically, instead of mimicking Newton\u2019s method as in the classical updates, our A-QNPE method is built upon the celebrated Monteiro-Svaiter (MS) acceleration framework [MS13; CHJJS22], which can be regarded as an inexact version of the accelerated proximal point method [G\u00fcl92; SV12]. Another major difference lies in the update rule of the Hessian approximation matrix. Classical quasi-Newton methods typically perform a low-rank update of the Hessian approximation matrix while enforcing the secant condition. On the contrary, our update rule is purely driven by our convergence analysis of the MS acceleration framework. In particular, inspired by [JJM23], we assign certain loss functions to the Hessian approximation matrices and formulate the Hessian approximation matrix update as an online convex optimization problem in the space of matrices. Therefore, we propose to update the Hessian approximation matrices via an online learning algorithm.\n\nRelated work. The authors in [CHJJS22] proposed a refined MS acceleration framework, which simplifies the line search subroutine in the original MS method [MS13]. By instantiating it with an adaptive second-order oracle, they presented an accelerated second-order method that achieves the optimal rate of \\( O(k^{3.5}) \\). The framework in [CHJJS22] serves as a basis for our method, but we focus on the setting where we have access only to a gradient oracle and we consider a quasi-Newton-type update. Another closely related work is [JJM23], where the authors proposed a quasi-Newton proximal extragradient method with a global non-asymptotic superlinear rate. In particular, our Hessian approximation update is inspired by the online learning framework in [JJM23]. On the other hand, the major difference is that the authors in [JJM23] focused on the case where \\( f \\) is strongly convex and presented a global superlinear rate, while we consider the more general convex setting where \\( f \\) is only convex (may not be strongly convex). Moreover, we further incorporate the acceleration mechanism into our method, which greatly complicates the convergence analysis; see Remark 3.1 for more discussions.", "md": "proximal extragradient (A-QNPE) method. Specifically, under the assumptions that \\( f \\) in (1) is\nconvex and its gradient and Hessian are Lipschitz, we prove the following guarantees:\n\n- From any initialization, A-QNPE can attain a global convergence rate of \\( O \\min\\left\\{ \\frac{1}{k^2}, \\sqrt{d \\log k} \\right\\} \\). In particular, this implies that our method matches the optimal rate of \\( O\\left( \\frac{1}{k^2} \\right) \\) when \\( k = O(d), k^{2.5} \\sqrt{d \\log k} \\) while it converges at a faster rate of \\( O\\left( k^{2.5} \\right) \\) when \\( k = \\Omega(d \\log d) \\). Alternatively, we can bound the number of iterations required to achieve an \\( \\epsilon \\)-accurate solution by \\( N_{\\epsilon} = O \\min\\left\\{ \\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} \\left( \\log d \\frac{1}{\\epsilon^2} \\right)^{0.2} \\right\\} \\).\n- In terms of computational cost, we show that the total number of gradient queries after \\( N \\) iterations can be bounded by \\( 3N \\), i.e., on average no more than 3 per iteration. Moreover, the number of matrix-vector products to achieve an \\( \\epsilon \\)-accurate solution can be bounded by \\( \\tilde{O} \\min\\left\\{ d^{0.25} \\epsilon^{0.5}, \\frac{1}{\\epsilon^{0.625}} \\right\\} \\).\n\nCombining the two results above, we conclude that A-QNPE requires \\( O \\min\\left\\{ \\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} \\left( \\log d \\frac{1}{\\epsilon^2} \\right)^{0.2} \\right\\} \\) gradient queries to reach an \\( \\epsilon \\)-accurate solution, which is at least as good as NAG and is further superior when \\( \\epsilon = O \\frac{1}{d^2 \\log^2(d)} \\). To the best of our knowledge, this is the first result that demonstrates a provable advantage of a quasi-Newton-type method over NAG in terms of gradient oracle complexity in the smooth convex setting.\n\nTo obtain these results, we significantly deviate from the classical quasi-Newton methods such as BFGS and DFP. Specifically, instead of mimicking Newton\u2019s method as in the classical updates, our A-QNPE method is built upon the celebrated Monteiro-Svaiter (MS) acceleration framework [MS13; CHJJS22], which can be regarded as an inexact version of the accelerated proximal point method [G\u00fcl92; SV12]. Another major difference lies in the update rule of the Hessian approximation matrix. Classical quasi-Newton methods typically perform a low-rank update of the Hessian approximation matrix while enforcing the secant condition. On the contrary, our update rule is purely driven by our convergence analysis of the MS acceleration framework. In particular, inspired by [JJM23], we assign certain loss functions to the Hessian approximation matrices and formulate the Hessian approximation matrix update as an online convex optimization problem in the space of matrices. Therefore, we propose to update the Hessian approximation matrices via an online learning algorithm.\n\nRelated work. The authors in [CHJJS22] proposed a refined MS acceleration framework, which simplifies the line search subroutine in the original MS method [MS13]. By instantiating it with an adaptive second-order oracle, they presented an accelerated second-order method that achieves the optimal rate of \\( O(k^{3.5}) \\). The framework in [CHJJS22] serves as a basis for our method, but we focus on the setting where we have access only to a gradient oracle and we consider a quasi-Newton-type update. Another closely related work is [JJM23], where the authors proposed a quasi-Newton proximal extragradient method with a global non-asymptotic superlinear rate. In particular, our Hessian approximation update is inspired by the online learning framework in [JJM23]. On the other hand, the major difference is that the authors in [JJM23] focused on the case where \\( f \\) is strongly convex and presented a global superlinear rate, while we consider the more general convex setting where \\( f \\) is only convex (may not be strongly convex). Moreover, we further incorporate the acceleration mechanism into our method, which greatly complicates the convergence analysis; see Remark 3.1 for more discussions."}]}, {"page": 4, "text": "2       Preliminaries\nNext, we formally state the required assumptions for our main results.\nAssumption 2.1. The function f is twice differentiable, convex, and L1-smooth. As a result, we\nhave 0 \u2aaf       \u22072f(x) \u2aaf         L1I for any x \u2208           Rd, where I \u2208          Rd\u00d7d is the identity matrix.\nAssumption 2.2. The Hessian of f is L2-Lipschitz, i.e., we have \u2225\u22072f(x)\u2212\u22072f(y)\u2225op \u2264                                                        L2\u2225x\u2212y\u22252\nfor any x, y \u2208         Rd, where \u2225A\u2225op \u225c               supx:\u2225x\u22252=1 \u2225Ax\u22252.\nWe note that both assumptions are standard in the optimization literature and are satisfied by\nvarious loss functions such as the logistic loss and the log-sum-exp function (see, e.g., [RN21a]).\nMonteiro-Svaiter acceleration. As our proposed method uses ideas from the celebrated Monteiro-\nSvaiter (MS) acceleration algorithm [MS13], we first briefly recap this method. MS acceleration, also\nknown as accelerated hybrid proximal extragradient (A-HPE), consists of intertwining sequences of\niterates {xk}, {yk}, {zk}, scalar variables {ak} and {Ak} as well as step sizes {\u03b7k}. The algorithm\nhas three main steps. In the first step, we compute the auxiliary iterate yk according to\n                        yk =         Ak       xk +         ak      zk,          where       ak =     \u03b7k +     \u03b72   k + 4\u03b7kAk       .                 (2)\n                                 Ak + ak              Ak + ak                                                      2\nIn the second step, an inexact proximal point step xk+1 \u2248                                      yk \u2212    \u03b7k\u2207f(xk+1) is performed. To be\nprecise, given a parameter \u03c3 \u2208                    [0, 1), we find xk+1 that satisfies\n                                           \u2225xk+1 \u2212      yk + \u03b7k\u2207f(xk+1)\u2225              \u2264   \u03c3\u2225xk+1 \u2212        yk\u2225.                                       (3)\nThen in the third step, the iterate z is updated by following the update\n                                                          zk+1 = zk \u2212         ak\u2207f(xk+1).\nFinally, we update the scalar Ak+1 by Ak+1 = Ak + ak. The above method has two implementation\nissues. First, to perform the update in (3) directly, one needs to solve the nonlinear system of\nequations x \u2212           yk + \u03b7k\u2207f(x) = 0 to a certain accuracy, which could be costly in general. To\naddress this issue, a principled approach is to replace the gradient operator \u2207f(x) with a simpler\napproximation function P                 (x; yk) and select xk+1 as the (approximate) solution of the equation:\n                                                     xk+1 \u2212      yk + \u03b7kP      (xk+1; yk) = 0.                                                       (4)\nFor instance, we can use P                   (x; yk) = \u2207f(yk) and accordingly (4) is equivalent to xk+1 = yk \u2212\n\u03b7k\u2207f(yk), leading to the accelerated first-order method in [MS13]. If we further have access to\nthe Hessian oracle, we can use P                      (x; yk) = \u2207f(yk) + \u22072f(yk)(x \u2212                         yk) and (4) becomes xk+1 =\nyk \u2212     \u03b7k(I + \u03b7k\u22072f(yk))\u22121\u2207f(yk), leading to the second-order method in [MS13].\nHowever, approximating \u2207f(x) by P                         (x; yk) leads to a second issue related to finding a proper step\nsize \u03b7k. More precisely, one needs to first select \u03b7k, compute yk from (2), and then solve the system\nin (4) exactly or approximately to obtain xk+1. However, these three variables, i.e., xk+1, yk and \u03b7k\nmay not satisfy the condition in (3) due to the gap between \u2207f(x) and P                                              (x; yk). If that happens,\nwe need to re-select \u03b7k and recalculate both yk and xk+1 until the condition in (3) is satisfied. To\naddress this issue, several bisection subroutines have been proposed in the literature [MS13; JWZ21;\nBJLLS19; GDGVSU19] and they all incur a computational cost of log(1/\u03f5) per iteration.\n                                                                             4", "md": "# Preliminaries\n\n## Preliminaries\n\nNext, we formally state the required assumptions for our main results.\n\nAssumption 2.1. The function \\( f \\) is twice differentiable, convex, and L1-smooth. As a result, we have \\( 0 \\preceq \\nabla^2 f(x) \\preceq L_1 I \\) for any \\( x \\in \\mathbb{R}^d \\), where \\( I \\in \\mathbb{R}^{d \\times d} \\) is the identity matrix.\n\nAssumption 2.2. The Hessian of \\( f \\) is L2-Lipschitz, i.e., we have \\( \\| \\nabla^2 f(x) - \\nabla^2 f(y) \\|_{op} \\leq L_2 \\| x - y \\|_2 \\) for any \\( x, y \\in \\mathbb{R}^d \\), where \\( \\| A \\|_{op} \\triangleq \\sup_{x: \\| x \\|_2 = 1} \\| Ax \\|_2 \\).\n\nWe note that both assumptions are standard in the optimization literature and are satisfied by various loss functions such as the logistic loss and the log-sum-exp function (see, e.g., [RN21a]).\n\nMonteiro-Svaiter acceleration. As our proposed method uses ideas from the celebrated Monteiro-Svaiter (MS) acceleration algorithm [MS13], we first briefly recap this method. MS acceleration, also known as accelerated hybrid proximal extragradient (A-HPE), consists of intertwining sequences of iterates \\( \\{x_k\\} \\), \\( \\{y_k\\} \\), \\( \\{z_k\\} \\), scalar variables \\( \\{a_k\\} \\) and \\( \\{A_k\\} \\) as well as step sizes \\( \\{\\eta_k\\} \\). The algorithm has three main steps. In the first step, we compute the auxiliary iterate \\( y_k \\) according to\n\n$$ y_k = A_k x_k + a_k z_k, \\quad \\text{where} \\quad a_k = \\eta_k + \\eta_k^2 + 4\\eta_k A_k/2. \\quad (2) $$\nIn the second step, an inexact proximal point step \\( x_{k+1} \\approx y_k - \\eta_k \\nabla f(x_{k+1}) \\) is performed. To be precise, given a parameter \\( \\sigma \\in [0, 1) \\), we find \\( x_{k+1} \\) that satisfies\n\n$$ \\| x_{k+1} - y_k + \\eta_k \\nabla f(x_{k+1}) \\| \\leq \\sigma \\| x_{k+1} - y_k \\| \\quad (3) $$\nThen in the third step, the iterate \\( z \\) is updated by following the update\n\n$$ z_{k+1} = z_k - a_k \\nabla f(x_{k+1}). $$\nFinally, we update the scalar \\( A_{k+1} \\) by \\( A_{k+1} = A_k + a_k \\). The above method has two implementation issues. First, to perform the update in (3) directly, one needs to solve the nonlinear system of equations \\( x - y_k + \\eta_k \\nabla f(x) = 0 \\) to a certain accuracy, which could be costly in general. To address this issue, a principled approach is to replace the gradient operator \\( \\nabla f(x) \\) with a simpler approximation function \\( P(x; y_k) \\) and select \\( x_{k+1} \\) as the (approximate) solution of the equation:\n\n$$ x_{k+1} - y_k + \\eta_k P(x_{k+1}; y_k) = 0. \\quad (4) $$\nFor instance, we can use \\( P(x; y_k) = \\nabla f(y_k) \\) and accordingly (4) is equivalent to \\( x_{k+1} = y_k - \\eta_k \\nabla f(y_k) \\), leading to the accelerated first-order method in [MS13]. If we further have access to the Hessian oracle, we can use \\( P(x; y_k) = \\nabla f(y_k) + \\nabla^2 f(y_k)(x - y_k) \\) and (4) becomes \\( x_{k+1} = y_k - \\eta_k(I + \\eta_k \\nabla^2 f(y_k))^{-1} \\nabla f(y_k) \\), leading to the second-order method in [MS13].\n\nHowever, approximating \\( \\nabla f(x) \\) by \\( P(x; y_k) \\) leads to a second issue related to finding a proper step size \\( \\eta_k \\). More precisely, one needs to first select \\( \\eta_k \\), compute \\( y_k \\) from (2), and then solve the system in (4) exactly or approximately to obtain \\( x_{k+1} \\). However, these three variables, i.e., \\( x_{k+1} \\), \\( y_k \\) and \\( \\eta_k \\) may not satisfy the condition in (3) due to the gap between \\( \\nabla f(x) \\) and \\( P(x; y_k) \\). If that happens, we need to re-select \\( \\eta_k \\) and recalculate both \\( y_k \\) and \\( x_{k+1} \\) until the condition in (3) is satisfied. To address this issue, several bisection subroutines have been proposed in the literature [MS13; JWZ21; BJLLS19; GDGVSU19] and they all incur a computational cost of \\( \\log(1/\\epsilon) \\) per iteration.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Preliminaries", "md": "# Preliminaries"}, {"type": "heading", "lvl": 2, "value": "Preliminaries", "md": "## Preliminaries"}, {"type": "text", "value": "Next, we formally state the required assumptions for our main results.\n\nAssumption 2.1. The function \\( f \\) is twice differentiable, convex, and L1-smooth. As a result, we have \\( 0 \\preceq \\nabla^2 f(x) \\preceq L_1 I \\) for any \\( x \\in \\mathbb{R}^d \\), where \\( I \\in \\mathbb{R}^{d \\times d} \\) is the identity matrix.\n\nAssumption 2.2. The Hessian of \\( f \\) is L2-Lipschitz, i.e., we have \\( \\| \\nabla^2 f(x) - \\nabla^2 f(y) \\|_{op} \\leq L_2 \\| x - y \\|_2 \\) for any \\( x, y \\in \\mathbb{R}^d \\), where \\( \\| A \\|_{op} \\triangleq \\sup_{x: \\| x \\|_2 = 1} \\| Ax \\|_2 \\).\n\nWe note that both assumptions are standard in the optimization literature and are satisfied by various loss functions such as the logistic loss and the log-sum-exp function (see, e.g., [RN21a]).\n\nMonteiro-Svaiter acceleration. As our proposed method uses ideas from the celebrated Monteiro-Svaiter (MS) acceleration algorithm [MS13], we first briefly recap this method. MS acceleration, also known as accelerated hybrid proximal extragradient (A-HPE), consists of intertwining sequences of iterates \\( \\{x_k\\} \\), \\( \\{y_k\\} \\), \\( \\{z_k\\} \\), scalar variables \\( \\{a_k\\} \\) and \\( \\{A_k\\} \\) as well as step sizes \\( \\{\\eta_k\\} \\). The algorithm has three main steps. In the first step, we compute the auxiliary iterate \\( y_k \\) according to\n\n$$ y_k = A_k x_k + a_k z_k, \\quad \\text{where} \\quad a_k = \\eta_k + \\eta_k^2 + 4\\eta_k A_k/2. \\quad (2) $$\nIn the second step, an inexact proximal point step \\( x_{k+1} \\approx y_k - \\eta_k \\nabla f(x_{k+1}) \\) is performed. To be precise, given a parameter \\( \\sigma \\in [0, 1) \\), we find \\( x_{k+1} \\) that satisfies\n\n$$ \\| x_{k+1} - y_k + \\eta_k \\nabla f(x_{k+1}) \\| \\leq \\sigma \\| x_{k+1} - y_k \\| \\quad (3) $$\nThen in the third step, the iterate \\( z \\) is updated by following the update\n\n$$ z_{k+1} = z_k - a_k \\nabla f(x_{k+1}). $$\nFinally, we update the scalar \\( A_{k+1} \\) by \\( A_{k+1} = A_k + a_k \\). The above method has two implementation issues. First, to perform the update in (3) directly, one needs to solve the nonlinear system of equations \\( x - y_k + \\eta_k \\nabla f(x) = 0 \\) to a certain accuracy, which could be costly in general. To address this issue, a principled approach is to replace the gradient operator \\( \\nabla f(x) \\) with a simpler approximation function \\( P(x; y_k) \\) and select \\( x_{k+1} \\) as the (approximate) solution of the equation:\n\n$$ x_{k+1} - y_k + \\eta_k P(x_{k+1}; y_k) = 0. \\quad (4) $$\nFor instance, we can use \\( P(x; y_k) = \\nabla f(y_k) \\) and accordingly (4) is equivalent to \\( x_{k+1} = y_k - \\eta_k \\nabla f(y_k) \\), leading to the accelerated first-order method in [MS13]. If we further have access to the Hessian oracle, we can use \\( P(x; y_k) = \\nabla f(y_k) + \\nabla^2 f(y_k)(x - y_k) \\) and (4) becomes \\( x_{k+1} = y_k - \\eta_k(I + \\eta_k \\nabla^2 f(y_k))^{-1} \\nabla f(y_k) \\), leading to the second-order method in [MS13].\n\nHowever, approximating \\( \\nabla f(x) \\) by \\( P(x; y_k) \\) leads to a second issue related to finding a proper step size \\( \\eta_k \\). More precisely, one needs to first select \\( \\eta_k \\), compute \\( y_k \\) from (2), and then solve the system in (4) exactly or approximately to obtain \\( x_{k+1} \\). However, these three variables, i.e., \\( x_{k+1} \\), \\( y_k \\) and \\( \\eta_k \\) may not satisfy the condition in (3) due to the gap between \\( \\nabla f(x) \\) and \\( P(x; y_k) \\). If that happens, we need to re-select \\( \\eta_k \\) and recalculate both \\( y_k \\) and \\( x_{k+1} \\) until the condition in (3) is satisfied. To address this issue, several bisection subroutines have been proposed in the literature [MS13; JWZ21; BJLLS19; GDGVSU19] and they all incur a computational cost of \\( \\log(1/\\epsilon) \\) per iteration.", "md": "Next, we formally state the required assumptions for our main results.\n\nAssumption 2.1. The function \\( f \\) is twice differentiable, convex, and L1-smooth. As a result, we have \\( 0 \\preceq \\nabla^2 f(x) \\preceq L_1 I \\) for any \\( x \\in \\mathbb{R}^d \\), where \\( I \\in \\mathbb{R}^{d \\times d} \\) is the identity matrix.\n\nAssumption 2.2. The Hessian of \\( f \\) is L2-Lipschitz, i.e., we have \\( \\| \\nabla^2 f(x) - \\nabla^2 f(y) \\|_{op} \\leq L_2 \\| x - y \\|_2 \\) for any \\( x, y \\in \\mathbb{R}^d \\), where \\( \\| A \\|_{op} \\triangleq \\sup_{x: \\| x \\|_2 = 1} \\| Ax \\|_2 \\).\n\nWe note that both assumptions are standard in the optimization literature and are satisfied by various loss functions such as the logistic loss and the log-sum-exp function (see, e.g., [RN21a]).\n\nMonteiro-Svaiter acceleration. As our proposed method uses ideas from the celebrated Monteiro-Svaiter (MS) acceleration algorithm [MS13], we first briefly recap this method. MS acceleration, also known as accelerated hybrid proximal extragradient (A-HPE), consists of intertwining sequences of iterates \\( \\{x_k\\} \\), \\( \\{y_k\\} \\), \\( \\{z_k\\} \\), scalar variables \\( \\{a_k\\} \\) and \\( \\{A_k\\} \\) as well as step sizes \\( \\{\\eta_k\\} \\). The algorithm has three main steps. In the first step, we compute the auxiliary iterate \\( y_k \\) according to\n\n$$ y_k = A_k x_k + a_k z_k, \\quad \\text{where} \\quad a_k = \\eta_k + \\eta_k^2 + 4\\eta_k A_k/2. \\quad (2) $$\nIn the second step, an inexact proximal point step \\( x_{k+1} \\approx y_k - \\eta_k \\nabla f(x_{k+1}) \\) is performed. To be precise, given a parameter \\( \\sigma \\in [0, 1) \\), we find \\( x_{k+1} \\) that satisfies\n\n$$ \\| x_{k+1} - y_k + \\eta_k \\nabla f(x_{k+1}) \\| \\leq \\sigma \\| x_{k+1} - y_k \\| \\quad (3) $$\nThen in the third step, the iterate \\( z \\) is updated by following the update\n\n$$ z_{k+1} = z_k - a_k \\nabla f(x_{k+1}). $$\nFinally, we update the scalar \\( A_{k+1} \\) by \\( A_{k+1} = A_k + a_k \\). The above method has two implementation issues. First, to perform the update in (3) directly, one needs to solve the nonlinear system of equations \\( x - y_k + \\eta_k \\nabla f(x) = 0 \\) to a certain accuracy, which could be costly in general. To address this issue, a principled approach is to replace the gradient operator \\( \\nabla f(x) \\) with a simpler approximation function \\( P(x; y_k) \\) and select \\( x_{k+1} \\) as the (approximate) solution of the equation:\n\n$$ x_{k+1} - y_k + \\eta_k P(x_{k+1}; y_k) = 0. \\quad (4) $$\nFor instance, we can use \\( P(x; y_k) = \\nabla f(y_k) \\) and accordingly (4) is equivalent to \\( x_{k+1} = y_k - \\eta_k \\nabla f(y_k) \\), leading to the accelerated first-order method in [MS13]. If we further have access to the Hessian oracle, we can use \\( P(x; y_k) = \\nabla f(y_k) + \\nabla^2 f(y_k)(x - y_k) \\) and (4) becomes \\( x_{k+1} = y_k - \\eta_k(I + \\eta_k \\nabla^2 f(y_k))^{-1} \\nabla f(y_k) \\), leading to the second-order method in [MS13].\n\nHowever, approximating \\( \\nabla f(x) \\) by \\( P(x; y_k) \\) leads to a second issue related to finding a proper step size \\( \\eta_k \\). More precisely, one needs to first select \\( \\eta_k \\), compute \\( y_k \\) from (2), and then solve the system in (4) exactly or approximately to obtain \\( x_{k+1} \\). However, these three variables, i.e., \\( x_{k+1} \\), \\( y_k \\) and \\( \\eta_k \\) may not satisfy the condition in (3) due to the gap between \\( \\nabla f(x) \\) and \\( P(x; y_k) \\). If that happens, we need to re-select \\( \\eta_k \\) and recalculate both \\( y_k \\) and \\( x_{k+1} \\) until the condition in (3) is satisfied. To address this issue, several bisection subroutines have been proposed in the literature [MS13; JWZ21; BJLLS19; GDGVSU19] and they all incur a computational cost of \\( \\log(1/\\epsilon) \\) per iteration."}]}, {"page": 5, "text": "Optimal Monteiro-Svaiter acceleration. A recent paper [CHJJS22] refines the MS acceleration\nalgorithm by separating the update of yk from the line search subroutine. In particular, in the first\nstage, we use \u03b7k to compute ak and then yk from (2), which will stay fixed throughout the line\nsearch scheme. In the second stage, we aim to find a pair \u02c6   xk+1 and \u02c6\u03b7k such that they satisfy\n                             \u2225\u02c6\n                              xk+1 \u2212  yk + \u02c6\n                                           \u03b7k\u2207f(\u02c6 xk+1)\u2225  \u2264 \u03c3\u2225\u02c6xk+1 \u2212  yk\u2225.                          (5)\nTo find that pair, we follow a similar line search scheme as above, with the key difference that yk is\nfixed and \u02c6\u03b7k can be different from \u03b7k that is used to compute yk. More precisely, for a given \u02c6 \u03b7k, we\nfind the solution of (4) denoted by \u02c6  xk+1 and check whether it satisfies (5) or not. If it does not,\nthen we adapt the step size and redo the process until (5) is satisfied. Then given the values of\nthese two parameters \u03b7k and \u02c6  \u03b7k, the updates for x and z would change as we describe next:\n\u2022 If \u02c6\n     \u03b7k \u2265  \u03b7k, we update xk+1 = \u02c6 xk+1, Ak+1 = Ak + ak and zk+1 = zk \u2212      ak\u2207f(\u02c6xk+1). Moreover, we\n   increase the next tentative step size by choosing \u03b7k+1 = \u03b7k/\u03b2 for some \u03b2 \u2208     (0, 1).\n\u2022 Otherwise, if \u02c6\u03b7k < \u03b7k, the authors in [CHJJS22] introduced a momentum damping mechanism.\n                                                             (1\u2212\u03b3k)Ak\n   Define \u03b3k = \u02c6  \u03b7k/\u03b7k < 1.    We then choose xk+1 =        Ak+\u03b3kak xk + \u03b3k(Ak+ak)  xk+1, which is a\n                                                                            A k+\u03b3kak \u02c6\n   convex combination of xk and \u02c6    xk+1. Moreover, we update Ak+1 = Ak + \u03b3kak and zk+1 =\n   zk \u2212 \u03b3kak\u2207f(\u02c6 xk+1). Finally, we decrease the next tentative step size by choosing \u03b7k+1 = \u03b2\u03b7k.\nThis approach not only simplifies the procedure by separating the update of {yk} from the line\nsearch scheme, but it also shaves a factor of log(1/\u03f5) from the computational cost of the algorithm,\nleading to optimal first and second-order variants of the MS acceleration method. Therefore, as we\ndiscuss in the next section, we build our method upon this more refined MS acceleration framework.\n3     Accelerated Quasi-Newton Proximal Extragradient\nIn this section, we present our accelerated quasi-Newton proximal extragradient (A-QNPE) method.\nAn informal description of our method is provided in Algorithm 1. On a high level, our method\ncan be viewed as the quasi-Newton counterpart of the adaptive Monteiro-Svaiter-Newton method\nproposed in [CHJJS22]. In particular, we only query a gradient oracle and choose the approximation\nfunction in (4) as P(x; yk) = \u2207f(yk) + Bk(x \u2212      yk), where Bk is a Hessian approximation matrix\nobtained only using gradient information. Moreover, another central piece of our method is the\nupdate scheme of Bk. Instead of following the classical quasi-Newton updates such as BFGS or\nDFP, we use an online learning framework, where we choose a sequence of matrices Bk to achieve a\nsmall dynamic regret for an online learning problem defined by our analysis; more details will be\nprovided later in Section 3.2. We initialize our method by choosing x0, z0 \u2208    Rd and setting A0 = 0\nand \u03b70 = \u03c30, where \u03c30 is a user-specified parameter. Our method can be divided into the following\nfour stages:\n\u2022 In the first stage, we compute the scalar ak and the auxiliary iterate yk according to (2) using\n   the step size \u03b7k. Note that yk is then fixed throughout the k-th iteration.\n\u2022 In the second stage, given the Hessian approximation matrix Bk and the iterate yk, we use a\n   line search scheme to find the step size \u02c6\n                                            \u03b7k and the iterate \u02c6xk+1 such that\n                    \u2225\u02c6xk+1 \u2212 yk + \u02c6\u03b7k(\u2207f(yk) + Bk(\u02c6  xk+1 \u2212  yk))\u2225 \u2264  \u03b11\u2225\u02c6xk+1 \u2212  yk\u2225,               (6)\n                          \u2225\u02c6\n                           xk+1 \u2212  yk + \u02c6\n                                        \u03b7k\u2207f(\u02c6 xk+1)\u2225  \u2264 (\u03b11 + \u03b12)\u2225\u02c6xk+1 \u2212  yk\u2225,                     (7)\n                                                   5", "md": "# Optimal Monteiro-Svaiter Acceleration\n\n## Optimal Monteiro-Svaiter Acceleration\n\nA recent paper [CHJJS22] refines the MS acceleration algorithm by separating the update of yk from the line search subroutine. In particular, in the first stage, we use \u03b7k to compute ak and then yk from (2), which will stay fixed throughout the line search scheme. In the second stage, we aim to find a pair \u02c6 xk+1 and \u02c6\u03b7k such that they satisfy\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k \\nabla f(\\hat{x}_{k+1})\\| \\leq \\sigma\\|\\hat{x}_{k+1} - y_k\\|.$$\n\nTo find that pair, we follow a similar line search scheme as above, with the key difference that yk is fixed and \u02c6\u03b7k can be different from \u03b7k that is used to compute yk. More precisely, for a given \u02c6\u03b7k, we find the solution of (4) denoted by \u02c6 xk+1 and check whether it satisfies (5) or not. If it does not, then we adapt the step size and redo the process until (5) is satisfied. Then given the values of these two parameters \u03b7k and \u02c6\u03b7k, the updates for x and z would change as we describe next:\n\n- If \u02c6\u03b7k \u2265 \u03b7k, we update xk+1 = \u02c6 xk+1, Ak+1 = Ak + ak and zk+1 = zk - ak\\nabla f(\\hat{x}_{k+1})\n- Moreover, we increase the next tentative step size by choosing \u03b7_{k+1} = \\frac{\u03b7_k}{\\beta}$ for some $\\beta \\in (0, 1)$.\n- Otherwise, if \u02c6\u03b7k < \u03b7k, the authors in [CHJJS22] introduced a momentum damping mechanism.\n\nDefine $$\\gamma_k = \\frac{\\hat{\\eta}_k}{\\eta_k} < 1.$$ We then choose $$x_{k+1} = \\frac{A_k + \\gamma_k a_k}{A_k + \\gamma_k a_k} x_k + \\gamma_k(A_k + a_k) \\hat{x}_{k+1},$$ which is a convex combination of xk and \u02c6 xk+1. Moreover, we update Ak+1 = Ak + \u03b3kak and zk+1 = zk - \u03b3kak\\nabla f(\\hat{x}_{k+1}). Finally, we decrease the next tentative step size by choosing \u03b7_{k+1} = \\beta\\eta_k.$$\n\nThis approach not only simplifies the procedure by separating the update of {yk} from the line search scheme, but it also shaves a factor of log(1/\u03f5) from the computational cost of the algorithm, leading to optimal first and second-order variants of the MS acceleration method. Therefore, as we discuss in the next section, we build our method upon this more refined MS acceleration framework.\n\n### 3 Accelerated Quasi-Newton Proximal Extragradient\n\nIn this section, we present our accelerated quasi-Newton proximal extragradient (A-QNPE) method. An informal description of our method is provided in Algorithm 1. On a high level, our method can be viewed as the quasi-Newton counterpart of the adaptive Monteiro-Svaiter-Newton method proposed in [CHJJS22]. In particular, we only query a gradient oracle and choose the approximation function in (4) as $$P(x; y_k) = \\nabla f(y_k) + B_k(x - y_k),$$ where Bk is a Hessian approximation matrix obtained only using gradient information.\n\nMoreover, another central piece of our method is the update scheme of Bk. Instead of following the classical quasi-Newton updates such as BFGS or DFP, we use an online learning framework, where we choose a sequence of matrices Bk to achieve a small dynamic regret for an online learning problem defined by our analysis; more details will be provided later in Section 3.2. We initialize our method by choosing x0, z0 \u2208 Rd and setting A0 = 0 and \u03b70 = \u03c30, where \u03c30 is a user-specified parameter. Our method can be divided into the following four stages:\n\n1. In the first stage, we compute the scalar ak and the auxiliary iterate yk according to (2) using the step size \u03b7k. Note that yk is then fixed throughout the k-th iteration.\n2. In the second stage, given the Hessian approximation matrix Bk and the iterate yk, we use a line search scheme to find the step size \u02c6\u03b7k and the iterate \u02c6xk+1 such that\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k(\\nabla f(y_k) + B_k(\\hat{x}_{k+1} - y_k))\\| \\leq \\alpha_1\\|\\hat{x}_{k+1} - y_k\\|,$$\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}_{k+1} - y_k\\|.$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Optimal Monteiro-Svaiter Acceleration", "md": "# Optimal Monteiro-Svaiter Acceleration"}, {"type": "heading", "lvl": 2, "value": "Optimal Monteiro-Svaiter Acceleration", "md": "## Optimal Monteiro-Svaiter Acceleration"}, {"type": "text", "value": "A recent paper [CHJJS22] refines the MS acceleration algorithm by separating the update of yk from the line search subroutine. In particular, in the first stage, we use \u03b7k to compute ak and then yk from (2), which will stay fixed throughout the line search scheme. In the second stage, we aim to find a pair \u02c6 xk+1 and \u02c6\u03b7k such that they satisfy\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k \\nabla f(\\hat{x}_{k+1})\\| \\leq \\sigma\\|\\hat{x}_{k+1} - y_k\\|.$$\n\nTo find that pair, we follow a similar line search scheme as above, with the key difference that yk is fixed and \u02c6\u03b7k can be different from \u03b7k that is used to compute yk. More precisely, for a given \u02c6\u03b7k, we find the solution of (4) denoted by \u02c6 xk+1 and check whether it satisfies (5) or not. If it does not, then we adapt the step size and redo the process until (5) is satisfied. Then given the values of these two parameters \u03b7k and \u02c6\u03b7k, the updates for x and z would change as we describe next:\n\n- If \u02c6\u03b7k \u2265 \u03b7k, we update xk+1 = \u02c6 xk+1, Ak+1 = Ak + ak and zk+1 = zk - ak\\nabla f(\\hat{x}_{k+1})\n- Moreover, we increase the next tentative step size by choosing \u03b7_{k+1} = \\frac{\u03b7_k}{\\beta}$ for some $\\beta \\in (0, 1)$.\n- Otherwise, if \u02c6\u03b7k < \u03b7k, the authors in [CHJJS22] introduced a momentum damping mechanism.\n\nDefine $$\\gamma_k = \\frac{\\hat{\\eta}_k}{\\eta_k} < 1.$$ We then choose $$x_{k+1} = \\frac{A_k + \\gamma_k a_k}{A_k + \\gamma_k a_k} x_k + \\gamma_k(A_k + a_k) \\hat{x}_{k+1},$$ which is a convex combination of xk and \u02c6 xk+1. Moreover, we update Ak+1 = Ak + \u03b3kak and zk+1 = zk - \u03b3kak\\nabla f(\\hat{x}_{k+1}). Finally, we decrease the next tentative step size by choosing \u03b7_{k+1} = \\beta\\eta_k.$$\n\nThis approach not only simplifies the procedure by separating the update of {yk} from the line search scheme, but it also shaves a factor of log(1/\u03f5) from the computational cost of the algorithm, leading to optimal first and second-order variants of the MS acceleration method. Therefore, as we discuss in the next section, we build our method upon this more refined MS acceleration framework.", "md": "A recent paper [CHJJS22] refines the MS acceleration algorithm by separating the update of yk from the line search subroutine. In particular, in the first stage, we use \u03b7k to compute ak and then yk from (2), which will stay fixed throughout the line search scheme. In the second stage, we aim to find a pair \u02c6 xk+1 and \u02c6\u03b7k such that they satisfy\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k \\nabla f(\\hat{x}_{k+1})\\| \\leq \\sigma\\|\\hat{x}_{k+1} - y_k\\|.$$\n\nTo find that pair, we follow a similar line search scheme as above, with the key difference that yk is fixed and \u02c6\u03b7k can be different from \u03b7k that is used to compute yk. More precisely, for a given \u02c6\u03b7k, we find the solution of (4) denoted by \u02c6 xk+1 and check whether it satisfies (5) or not. If it does not, then we adapt the step size and redo the process until (5) is satisfied. Then given the values of these two parameters \u03b7k and \u02c6\u03b7k, the updates for x and z would change as we describe next:\n\n- If \u02c6\u03b7k \u2265 \u03b7k, we update xk+1 = \u02c6 xk+1, Ak+1 = Ak + ak and zk+1 = zk - ak\\nabla f(\\hat{x}_{k+1})\n- Moreover, we increase the next tentative step size by choosing \u03b7_{k+1} = \\frac{\u03b7_k}{\\beta}$ for some $\\beta \\in (0, 1)$.\n- Otherwise, if \u02c6\u03b7k < \u03b7k, the authors in [CHJJS22] introduced a momentum damping mechanism.\n\nDefine $$\\gamma_k = \\frac{\\hat{\\eta}_k}{\\eta_k} < 1.$$ We then choose $$x_{k+1} = \\frac{A_k + \\gamma_k a_k}{A_k + \\gamma_k a_k} x_k + \\gamma_k(A_k + a_k) \\hat{x}_{k+1},$$ which is a convex combination of xk and \u02c6 xk+1. Moreover, we update Ak+1 = Ak + \u03b3kak and zk+1 = zk - \u03b3kak\\nabla f(\\hat{x}_{k+1}). Finally, we decrease the next tentative step size by choosing \u03b7_{k+1} = \\beta\\eta_k.$$\n\nThis approach not only simplifies the procedure by separating the update of {yk} from the line search scheme, but it also shaves a factor of log(1/\u03f5) from the computational cost of the algorithm, leading to optimal first and second-order variants of the MS acceleration method. Therefore, as we discuss in the next section, we build our method upon this more refined MS acceleration framework."}, {"type": "heading", "lvl": 3, "value": "3 Accelerated Quasi-Newton Proximal Extragradient", "md": "### 3 Accelerated Quasi-Newton Proximal Extragradient"}, {"type": "text", "value": "In this section, we present our accelerated quasi-Newton proximal extragradient (A-QNPE) method. An informal description of our method is provided in Algorithm 1. On a high level, our method can be viewed as the quasi-Newton counterpart of the adaptive Monteiro-Svaiter-Newton method proposed in [CHJJS22]. In particular, we only query a gradient oracle and choose the approximation function in (4) as $$P(x; y_k) = \\nabla f(y_k) + B_k(x - y_k),$$ where Bk is a Hessian approximation matrix obtained only using gradient information.\n\nMoreover, another central piece of our method is the update scheme of Bk. Instead of following the classical quasi-Newton updates such as BFGS or DFP, we use an online learning framework, where we choose a sequence of matrices Bk to achieve a small dynamic regret for an online learning problem defined by our analysis; more details will be provided later in Section 3.2. We initialize our method by choosing x0, z0 \u2208 Rd and setting A0 = 0 and \u03b70 = \u03c30, where \u03c30 is a user-specified parameter. Our method can be divided into the following four stages:\n\n1. In the first stage, we compute the scalar ak and the auxiliary iterate yk according to (2) using the step size \u03b7k. Note that yk is then fixed throughout the k-th iteration.\n2. In the second stage, given the Hessian approximation matrix Bk and the iterate yk, we use a line search scheme to find the step size \u02c6\u03b7k and the iterate \u02c6xk+1 such that\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k(\\nabla f(y_k) + B_k(\\hat{x}_{k+1} - y_k))\\| \\leq \\alpha_1\\|\\hat{x}_{k+1} - y_k\\|,$$\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}_{k+1} - y_k\\|.$$", "md": "In this section, we present our accelerated quasi-Newton proximal extragradient (A-QNPE) method. An informal description of our method is provided in Algorithm 1. On a high level, our method can be viewed as the quasi-Newton counterpart of the adaptive Monteiro-Svaiter-Newton method proposed in [CHJJS22]. In particular, we only query a gradient oracle and choose the approximation function in (4) as $$P(x; y_k) = \\nabla f(y_k) + B_k(x - y_k),$$ where Bk is a Hessian approximation matrix obtained only using gradient information.\n\nMoreover, another central piece of our method is the update scheme of Bk. Instead of following the classical quasi-Newton updates such as BFGS or DFP, we use an online learning framework, where we choose a sequence of matrices Bk to achieve a small dynamic regret for an online learning problem defined by our analysis; more details will be provided later in Section 3.2. We initialize our method by choosing x0, z0 \u2208 Rd and setting A0 = 0 and \u03b70 = \u03c30, where \u03c30 is a user-specified parameter. Our method can be divided into the following four stages:\n\n1. In the first stage, we compute the scalar ak and the auxiliary iterate yk according to (2) using the step size \u03b7k. Note that yk is then fixed throughout the k-th iteration.\n2. In the second stage, given the Hessian approximation matrix Bk and the iterate yk, we use a line search scheme to find the step size \u02c6\u03b7k and the iterate \u02c6xk+1 such that\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k(\\nabla f(y_k) + B_k(\\hat{x}_{k+1} - y_k))\\| \\leq \\alpha_1\\|\\hat{x}_{k+1} - y_k\\|,$$\n\n$$\\|\\hat{x}_{k+1} - y_k + \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}_{k+1} - y_k\\|.$$"}]}, {"page": 6, "text": "Algorithm 1 Accelerated Quasi-Newton Proximal Extragradient Method\n  1: Input: initial points x0, z0 \u2208       Rd, initial step size \u03c30 > 0, \u03b11, \u03b12 \u2208      (0, 1) with \u03b11 + \u03b12 < 1, \u03b2 \u2208       (0, 1)\n  2: Initialize: set A0 \u2190       0 and \u03b70 \u2190    \u03c30\n  3: for iteration k = 0, . . . , N \u2212   1 do\n                            \u03b7k+\u221a\u03b72 k+4\u03b7kAk                   Ak             ak\n  4:    Compute ak \u2190                2         and yk \u2190     Ak+ak xk +    Ak+ak zk\n  5:    Let \u02c6\u03b7k be the largest possible step size in {\u03b7k\u03b2i : i \u2265         0} such that\n           \u02c6                 \u03b7k(I + \u02c6 \u03b7kBk)\u22121\u2207f(yk) and \u2225\u02c6         xk+1 \u2212   yk \u2212  \u02c6\n           xk+1 \u2248\u03b11 yk \u2212      \u02c6                                                   \u03b7k\u2207f(\u02c6  xk+1)\u2225   \u2264  (\u03b11 + \u03b12)\u2225\u02c6  xk+1 \u2212   yk\u2225\n  6:    if \u02c6\n           \u03b7k = \u03b7k then\n  7:       Set xk+1 \u2190      \u02c6\n                          xk+1, zk+1 \u2190      zk \u2212  ak\u2207f(\u02c6  xk+1), Ak+1 \u2190      Ak + ak\n  8:       Set \u03b7k+1 \u2190     \u02c6\n                          \u03b7k/\u03b2\n  9:       Set Bk+1 \u2190      Bk\n 10:    else\n 11:       Let \u03b3k \u2190     \u02c6\n                        \u03b7k/\u03b7k < 1\n 12:       Set xk+1 \u2190     (1\u2212\u03b3k)Ak                    xk+1, zk+1 \u2190zk \u2212       \u03b3kak\u2207f(\u02c6   xk+1), Ak+1 \u2190Ak + \u03b3kak\n                          Ak+\u03b3kak xk + \u03b3k(Ak+ak)\n 13:       Set \u03b7k+1 \u2190     \u02c6                 Ak+\u03b3kak \u02c6\n                          \u03b7k\n 14:       Set wk \u2190     \u2207f(\u02dc xk) \u2212   \u2207f(yk) and sk \u2190       \u02dc\n                                                           xk \u2212   yk, where \u02dc  xk is the last rejected iterate in LS\n 15:       Feed \u2113k(B) \u225c      \u2225wk\u2212Bsk\u22252    to an online learning algorithm and obtain Bk+1\n 16:    end if                  \u2225sk\u22252\n 17: end for\n    where \u03b11 \u2208       [0, 1) and \u03b12 \u2208       (0, 1) are user-specified parameters with \u03b11 + \u03b12 < 1. The first\n    condition in (6) requires that \u02c6       xk+1 inexactly solves the linear system of equations (I + \u02c6               \u03b7kBk)(x \u2212\n    yk) + \u02c6 \u03b7k\u2207f(yk) = 0, where \u03b11 \u2208                 [0, 1) controls the accuracy.            As a special case, we have\n    \u02c6                      \u03b7kBk)\u22121\u2207f(yk) when \u03b11 = 0. The second condition in (7) directly comes from\n    xk+1 = yk \u2212      (I + \u02c6\n   (5) in the optimal MS acceleration framework, which ensures that we approximately follow the\n    proximal point step \u02c6                         \u03b7k\u2207f(\u02c6\n                                xk+1 = yk \u2212       \u02c6        xk+1). To find the pair (\u02c6        \u03b7k, \u02c6xk+1) satisfying both (6)\n    and (7), we implement a backtracking line search scheme. Specifically, for some \u03b2 \u2208                                (0, 1), we\n    iteratively try \u02c6  \u03b7k = \u03b7k\u03b2i for i \u2265       0 and solve \u02c6   xk+1 from (6) until the condition in (7) is satisfied.\n    The line search scheme will be discussed in more detail in Section 3.1.\n \u2022 In the third stage, we update the variables xk+1, zk+1, Ak+1 and set the step size \u03b7k+1 in the\n    next iteration. Specifically, the update rule we follow depends on the outcome of the line search\n    scheme. In the first case where \u02c6         \u03b7k = \u03b7k, i.e., the line search scheme accepts the initial trial step\n    size, we let\n                           xk+1 = \u02c6   xk+1,     zk+1 = zk \u2212      ak\u2207f(\u02c6   xk+1),      Ak+1 = Ak + ak,                           (8)\n    as in the original MS acceleration framework. Moreover, this also suggests our choice of the step\n    size \u03b7k may be too conservative. Therefore, we increase the step size in the next iteration by\n    \u03b7k+1 = \u02c6  \u03b7k/\u03b2. In the second case where \u02c6          \u03b7k < \u03b7k, i.e., the line search scheme backtracks, we adopt\n    the momentum damping mechanism in [CHJJS22]:\n                                                       \u02c6\n      xk+1 = (1 \u2212      \u03b3k)Ak   xk + \u03b3k(Ak + ak)       xk+1, zk+1 = zk \u2212         \u03b3kak\u2207f(\u02c6   xk+1), Ak+1 = Ak + \u03b3kak, (9)\n                 Ak + \u03b3kak             Ak + \u03b3kak\n    where \u03b3k = \u02c6    \u03b7k/\u03b7k < 1. Accordingly, we decrease the step size in the next iteration by letting\n    \u03b7k+1 = \u02c6  \u03b7k (note that \u02c6   \u03b7k < \u03b7k).\n \u2022 In the fourth stage, we update the Hessian approximation matrix Bk+1. Inspired by [JJM23],\n    we depart from the classical quasi-Newton methods and instead let the convergence analysis\n                                                                 6", "md": "Algorithm 1 Accelerated Quasi-Newton Proximal Extragradient Method\n\n1: Input: initial points \\(x_0, z_0 \\in \\mapbb{R}^d\\), initial step size \\(\\sigma_0 > 0\\), \\(\\alpha_1, \\alpha_2 \\in (0, 1)\\) wip \\(\\alpha_1 + \\alpha_2 < 1\\), \\(\\beta \\in (0, 1\\)\n2: Initialize: set \\(A_0 \\leftarrow 0\\) and \\(\\eta_0 \\leftarrow \\sigma_0\\)\n3: for iteration \\(k = 0, ..., N - 1\\) do\n4: Compute \\(a_k \\leftarrow \\frac{\\eta_k + \\sqrt{\\eta_k^2 + 4\\eta_k A_k}}{2}\\) and \\(y_k \\leftarrow A_k + a_k x_k + A_k + a_k z_k\\)\n5: Let \\(\\hat{\\eta}_k\\) be pe largest possible step size in \\(\\{\\eta_k \\beta^i : i \\geq 0\\}\\) such pat\n\\(\\hat{\\eta}_k(I + \\hat{\\eta}_k B_k)^{-1}\\nabla f(y_k)\\) and \\(\\|\\hat{x}_{k+1} - y_k - \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}_{k+1} - y_k\\|\\)\n6: if \\(\\hat{\\eta}_k = \\eta_k\\) pen\n7: Set \\(x_{k+1} \\leftarrow \\hat{x}_{k+1}\\), \\(z_{k+1} \\leftarrow z_k - a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} \\leftarrow A_k + a_k\\)\n8: Set \\(\\eta_{k+1} \\leftarrow \\hat{\\eta}_k/\\beta\\)\n9: Set \\(B_{k+1} \\leftarrow B_k\\)\n10: else\n11: Let \\(\\gamma_k \\leftarrow \\hat{\\eta}_k/\\eta_k < 1\\)\n12: Set \\(x_{k+1} \\leftarrow (1-\\gamma_k)A_k + \\gamma_k(A_k + a_k) x_k + \\gamma_k(A_k + a_k) z_k\\), \\(A_{k+1} \\leftarrow A_k + \\gamma_k a_k\\)\n13: Set \\(\\eta_{k+1} \\leftarrow \\hat{\\eta}_k A_k + \\gamma_k a_k \\hat{\\eta}_k\\)\n14: Set \\(w_k \\leftarrow \\nabla f(\\tilde{x}_k) - \\nabla f(y_k)\\) and \\(s_k \\leftarrow \\tilde{x}_k - y_k\\), where \\(\\tilde{x}_k\\) is pe last rejected iterate in LS\n15: Feed \\(\\ell_k(B) \\triangleq \\|\\mapbf{w}_k - B\\mapbf{s}_k\\|^2\\) to an online learning algoripm and obtain \\(B_{k+1}\\)\n16: end if\n17: end for\n\nwhere \\(\\alpha_1 \\in [0, 1)\\) and \\(\\alpha_2 \\in (0, 1)\\) are user-specified parameters with \\(\\alpha_1 + \\alpha_2 < 1\\). The first condition in (6) requires that \\(\\hat{x}_{k+1}\\) inexactly solves the linear system of equations \\((I + \\hat{\\eta}_k B_k)(x - y_k) + \\hat{\\eta}_k\\nabla f(y_k) = 0\\), where \\(\\alpha_1 \\in [0, 1)\\) controls the accuracy. As a special case, we have \\((I + \\hat{\\eta}_k B_k)^{-1}\\nabla f(y_k)\\) when \\(\\alpha_1 = 0\\). The second condition in (7) directly comes from \\(x_{k+1} = y_k - (I + \\hat{\\eta}_k\\)\n\n(5) in the optimal MS acceleration framework, which ensures that we approximately follow the proximal point step \\(\\hat{x}_{k+1} = y_k - \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\). To find the pair \\((\\hat{\\eta}_k, \\hat{x}_{k+1})\\) satisfying both (6) and (7), we implement a backtracking line search scheme. Specifically, for some \\(\\beta \\in (0, 1)\\), we iteratively try \\(\\hat{\\eta}_k = \\eta_k\\beta^i\\) for \\(i \\geq 0\\) and solve \\(\\hat{x}_{k+1}\\) from (6) until the condition in (7) is satisfied. The line search scheme will be discussed in more detail in Section 3.1.\n\nIn the third stage, we update the variables \\(x_{k+1}\\), \\(z_{k+1}\\), \\(A_{k+1}\\) and set the step size \\(\\eta_{k+1}\\) in the next iteration. Specifically, the update rule we follow depends on the outcome of the line search scheme. In the first case where \\(\\hat{\\eta}_k = \\eta_k\\), i.e., the line search scheme accepts the initial trial step size, we let\n\n\\(x_{k+1} = \\hat{x}_{k+1}\\), \\(z_{k+1} = z_k - a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} = A_k + a_k\\), as in the original MS acceleration framework. Moreover, this also suggests our choice of the step size \\(\\eta_k\\) may be too conservative. Therefore, we increase the step size in the next iteration by \\(\\eta_{k+1} = \\hat{\\eta}_k/\\beta\\). In the second case where \\(\\hat{\\eta}_k < \\eta_k\\), i.e., the line search scheme backtracks, we adopt the momentum damping mechanism in [CHJJS22]:\n\n\\(x_{k+1} = (1 - \\gamma_k)A_k + \\gamma_k(A_k + a_k) x_k + \\gamma_k(A_k + a_k) z_k\\), \\(z_{k+1} = z_k - \\gamma_k a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} = A_k + \\gamma_k a_k\\), where \\(\\gamma_k = \\hat{\\eta}_k/\\eta_k < 1\\). Accordingly, we decrease the step size in the next iteration by letting \\(\\eta_{k+1} = \\hat{\\eta}_k\\) (note that \\(\\hat{\\eta}_k < \\eta_k\\)).\n\nIn the fourth stage, we update the Hessian approximation matrix \\(B_{k+1}\\). Inspired by [JJM23], we depart from the classical quasi-Newton methods and instead let the convergence analysis\n\n6", "images": [], "items": [{"type": "text", "value": "Algorithm 1 Accelerated Quasi-Newton Proximal Extragradient Method\n\n1: Input: initial points \\(x_0, z_0 \\in \\mapbb{R}^d\\), initial step size \\(\\sigma_0 > 0\\), \\(\\alpha_1, \\alpha_2 \\in (0, 1)\\) wip \\(\\alpha_1 + \\alpha_2 < 1\\), \\(\\beta \\in (0, 1\\)\n2: Initialize: set \\(A_0 \\leftarrow 0\\) and \\(\\eta_0 \\leftarrow \\sigma_0\\)\n3: for iteration \\(k = 0, ..., N - 1\\) do\n4: Compute \\(a_k \\leftarrow \\frac{\\eta_k + \\sqrt{\\eta_k^2 + 4\\eta_k A_k}}{2}\\) and \\(y_k \\leftarrow A_k + a_k x_k + A_k + a_k z_k\\)\n5: Let \\(\\hat{\\eta}_k\\) be pe largest possible step size in \\(\\{\\eta_k \\beta^i : i \\geq 0\\}\\) such pat\n\\(\\hat{\\eta}_k(I + \\hat{\\eta}_k B_k)^{-1}\\nabla f(y_k)\\) and \\(\\|\\hat{x}_{k+1} - y_k - \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}_{k+1} - y_k\\|\\)\n6: if \\(\\hat{\\eta}_k = \\eta_k\\) pen\n7: Set \\(x_{k+1} \\leftarrow \\hat{x}_{k+1}\\), \\(z_{k+1} \\leftarrow z_k - a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} \\leftarrow A_k + a_k\\)\n8: Set \\(\\eta_{k+1} \\leftarrow \\hat{\\eta}_k/\\beta\\)\n9: Set \\(B_{k+1} \\leftarrow B_k\\)\n10: else\n11: Let \\(\\gamma_k \\leftarrow \\hat{\\eta}_k/\\eta_k < 1\\)\n12: Set \\(x_{k+1} \\leftarrow (1-\\gamma_k)A_k + \\gamma_k(A_k + a_k) x_k + \\gamma_k(A_k + a_k) z_k\\), \\(A_{k+1} \\leftarrow A_k + \\gamma_k a_k\\)\n13: Set \\(\\eta_{k+1} \\leftarrow \\hat{\\eta}_k A_k + \\gamma_k a_k \\hat{\\eta}_k\\)\n14: Set \\(w_k \\leftarrow \\nabla f(\\tilde{x}_k) - \\nabla f(y_k)\\) and \\(s_k \\leftarrow \\tilde{x}_k - y_k\\), where \\(\\tilde{x}_k\\) is pe last rejected iterate in LS\n15: Feed \\(\\ell_k(B) \\triangleq \\|\\mapbf{w}_k - B\\mapbf{s}_k\\|^2\\) to an online learning algoripm and obtain \\(B_{k+1}\\)\n16: end if\n17: end for\n\nwhere \\(\\alpha_1 \\in [0, 1)\\) and \\(\\alpha_2 \\in (0, 1)\\) are user-specified parameters with \\(\\alpha_1 + \\alpha_2 < 1\\). The first condition in (6) requires that \\(\\hat{x}_{k+1}\\) inexactly solves the linear system of equations \\((I + \\hat{\\eta}_k B_k)(x - y_k) + \\hat{\\eta}_k\\nabla f(y_k) = 0\\), where \\(\\alpha_1 \\in [0, 1)\\) controls the accuracy. As a special case, we have \\((I + \\hat{\\eta}_k B_k)^{-1}\\nabla f(y_k)\\) when \\(\\alpha_1 = 0\\). The second condition in (7) directly comes from \\(x_{k+1} = y_k - (I + \\hat{\\eta}_k\\)\n\n(5) in the optimal MS acceleration framework, which ensures that we approximately follow the proximal point step \\(\\hat{x}_{k+1} = y_k - \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\). To find the pair \\((\\hat{\\eta}_k, \\hat{x}_{k+1})\\) satisfying both (6) and (7), we implement a backtracking line search scheme. Specifically, for some \\(\\beta \\in (0, 1)\\), we iteratively try \\(\\hat{\\eta}_k = \\eta_k\\beta^i\\) for \\(i \\geq 0\\) and solve \\(\\hat{x}_{k+1}\\) from (6) until the condition in (7) is satisfied. The line search scheme will be discussed in more detail in Section 3.1.\n\nIn the third stage, we update the variables \\(x_{k+1}\\), \\(z_{k+1}\\), \\(A_{k+1}\\) and set the step size \\(\\eta_{k+1}\\) in the next iteration. Specifically, the update rule we follow depends on the outcome of the line search scheme. In the first case where \\(\\hat{\\eta}_k = \\eta_k\\), i.e., the line search scheme accepts the initial trial step size, we let\n\n\\(x_{k+1} = \\hat{x}_{k+1}\\), \\(z_{k+1} = z_k - a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} = A_k + a_k\\), as in the original MS acceleration framework. Moreover, this also suggests our choice of the step size \\(\\eta_k\\) may be too conservative. Therefore, we increase the step size in the next iteration by \\(\\eta_{k+1} = \\hat{\\eta}_k/\\beta\\). In the second case where \\(\\hat{\\eta}_k < \\eta_k\\), i.e., the line search scheme backtracks, we adopt the momentum damping mechanism in [CHJJS22]:\n\n\\(x_{k+1} = (1 - \\gamma_k)A_k + \\gamma_k(A_k + a_k) x_k + \\gamma_k(A_k + a_k) z_k\\), \\(z_{k+1} = z_k - \\gamma_k a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} = A_k + \\gamma_k a_k\\), where \\(\\gamma_k = \\hat{\\eta}_k/\\eta_k < 1\\). Accordingly, we decrease the step size in the next iteration by letting \\(\\eta_{k+1} = \\hat{\\eta}_k\\) (note that \\(\\hat{\\eta}_k < \\eta_k\\)).\n\nIn the fourth stage, we update the Hessian approximation matrix \\(B_{k+1}\\). Inspired by [JJM23], we depart from the classical quasi-Newton methods and instead let the convergence analysis\n\n6", "md": "Algorithm 1 Accelerated Quasi-Newton Proximal Extragradient Method\n\n1: Input: initial points \\(x_0, z_0 \\in \\mapbb{R}^d\\), initial step size \\(\\sigma_0 > 0\\), \\(\\alpha_1, \\alpha_2 \\in (0, 1)\\) wip \\(\\alpha_1 + \\alpha_2 < 1\\), \\(\\beta \\in (0, 1\\)\n2: Initialize: set \\(A_0 \\leftarrow 0\\) and \\(\\eta_0 \\leftarrow \\sigma_0\\)\n3: for iteration \\(k = 0, ..., N - 1\\) do\n4: Compute \\(a_k \\leftarrow \\frac{\\eta_k + \\sqrt{\\eta_k^2 + 4\\eta_k A_k}}{2}\\) and \\(y_k \\leftarrow A_k + a_k x_k + A_k + a_k z_k\\)\n5: Let \\(\\hat{\\eta}_k\\) be pe largest possible step size in \\(\\{\\eta_k \\beta^i : i \\geq 0\\}\\) such pat\n\\(\\hat{\\eta}_k(I + \\hat{\\eta}_k B_k)^{-1}\\nabla f(y_k)\\) and \\(\\|\\hat{x}_{k+1} - y_k - \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}_{k+1} - y_k\\|\\)\n6: if \\(\\hat{\\eta}_k = \\eta_k\\) pen\n7: Set \\(x_{k+1} \\leftarrow \\hat{x}_{k+1}\\), \\(z_{k+1} \\leftarrow z_k - a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} \\leftarrow A_k + a_k\\)\n8: Set \\(\\eta_{k+1} \\leftarrow \\hat{\\eta}_k/\\beta\\)\n9: Set \\(B_{k+1} \\leftarrow B_k\\)\n10: else\n11: Let \\(\\gamma_k \\leftarrow \\hat{\\eta}_k/\\eta_k < 1\\)\n12: Set \\(x_{k+1} \\leftarrow (1-\\gamma_k)A_k + \\gamma_k(A_k + a_k) x_k + \\gamma_k(A_k + a_k) z_k\\), \\(A_{k+1} \\leftarrow A_k + \\gamma_k a_k\\)\n13: Set \\(\\eta_{k+1} \\leftarrow \\hat{\\eta}_k A_k + \\gamma_k a_k \\hat{\\eta}_k\\)\n14: Set \\(w_k \\leftarrow \\nabla f(\\tilde{x}_k) - \\nabla f(y_k)\\) and \\(s_k \\leftarrow \\tilde{x}_k - y_k\\), where \\(\\tilde{x}_k\\) is pe last rejected iterate in LS\n15: Feed \\(\\ell_k(B) \\triangleq \\|\\mapbf{w}_k - B\\mapbf{s}_k\\|^2\\) to an online learning algoripm and obtain \\(B_{k+1}\\)\n16: end if\n17: end for\n\nwhere \\(\\alpha_1 \\in [0, 1)\\) and \\(\\alpha_2 \\in (0, 1)\\) are user-specified parameters with \\(\\alpha_1 + \\alpha_2 < 1\\). The first condition in (6) requires that \\(\\hat{x}_{k+1}\\) inexactly solves the linear system of equations \\((I + \\hat{\\eta}_k B_k)(x - y_k) + \\hat{\\eta}_k\\nabla f(y_k) = 0\\), where \\(\\alpha_1 \\in [0, 1)\\) controls the accuracy. As a special case, we have \\((I + \\hat{\\eta}_k B_k)^{-1}\\nabla f(y_k)\\) when \\(\\alpha_1 = 0\\). The second condition in (7) directly comes from \\(x_{k+1} = y_k - (I + \\hat{\\eta}_k\\)\n\n(5) in the optimal MS acceleration framework, which ensures that we approximately follow the proximal point step \\(\\hat{x}_{k+1} = y_k - \\hat{\\eta}_k\\nabla f(\\hat{x}_{k+1})\\). To find the pair \\((\\hat{\\eta}_k, \\hat{x}_{k+1})\\) satisfying both (6) and (7), we implement a backtracking line search scheme. Specifically, for some \\(\\beta \\in (0, 1)\\), we iteratively try \\(\\hat{\\eta}_k = \\eta_k\\beta^i\\) for \\(i \\geq 0\\) and solve \\(\\hat{x}_{k+1}\\) from (6) until the condition in (7) is satisfied. The line search scheme will be discussed in more detail in Section 3.1.\n\nIn the third stage, we update the variables \\(x_{k+1}\\), \\(z_{k+1}\\), \\(A_{k+1}\\) and set the step size \\(\\eta_{k+1}\\) in the next iteration. Specifically, the update rule we follow depends on the outcome of the line search scheme. In the first case where \\(\\hat{\\eta}_k = \\eta_k\\), i.e., the line search scheme accepts the initial trial step size, we let\n\n\\(x_{k+1} = \\hat{x}_{k+1}\\), \\(z_{k+1} = z_k - a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} = A_k + a_k\\), as in the original MS acceleration framework. Moreover, this also suggests our choice of the step size \\(\\eta_k\\) may be too conservative. Therefore, we increase the step size in the next iteration by \\(\\eta_{k+1} = \\hat{\\eta}_k/\\beta\\). In the second case where \\(\\hat{\\eta}_k < \\eta_k\\), i.e., the line search scheme backtracks, we adopt the momentum damping mechanism in [CHJJS22]:\n\n\\(x_{k+1} = (1 - \\gamma_k)A_k + \\gamma_k(A_k + a_k) x_k + \\gamma_k(A_k + a_k) z_k\\), \\(z_{k+1} = z_k - \\gamma_k a_k\\nabla f(\\hat{x}_{k+1})\\), \\(A_{k+1} = A_k + \\gamma_k a_k\\), where \\(\\gamma_k = \\hat{\\eta}_k/\\eta_k < 1\\). Accordingly, we decrease the step size in the next iteration by letting \\(\\eta_{k+1} = \\hat{\\eta}_k\\) (note that \\(\\hat{\\eta}_k < \\eta_k\\)).\n\nIn the fourth stage, we update the Hessian approximation matrix \\(B_{k+1}\\). Inspired by [JJM23], we depart from the classical quasi-Newton methods and instead let the convergence analysis\n\n6"}]}, {"page": 7, "text": "    guide our update scheme. As we will show in Section 3.2, the convergence rate of our method\n    is closely related to the cumulative loss  k\u2208B \u2113k(Bk) incurred by our choices of {Bk}, where\n    B = {k : \u02c6    \u03b7k < \u03b7k} denotes the indices where the line search scheme backtracks. Moreover, the\n    loss function has the form \u2113k(Bk) \u225c                         \u2225wk\u2212Bksk\u22252       , where wk \u225c           \u2207f(\u02dc  xk) \u2212     \u2207f(xk), sk \u225c           \u02dc\n                                                                    \u2225sk\u22252                                                                      xk \u2212    xk\n    and \u02dc  xk is an auxiliary iterate returned by our line search scheme. Thus, this motivates us to\n    employ an online learning algorithm to minimize the cumulative loss. Specifically, in the first case\n    where \u02c6   \u03b7k = \u03b7k (i.e., k /      \u2208   B), the current Hessian approximation matrix Bk does not contribute to\n    the cumulative loss and thus we keep it unchanged (cf. Line 9). Otherwise, we follow an online\n    learning algorithm in the space of matrices. The details will be discussed in Section 3.2.\nFinally, we provide a convergence result in the following Proposition for Algorithm 1, which serves as\nthe basis for our convergence analysis. We note that the following results do not require additional\nconditions on Bk other than the ones in (6) and (7). The proof is available in Appendix A.1.\nProposition 3.1. Let {xk}N                   k=0 be the iterates generated by Algorithm 1. If f is convex, we have\n                       f(xN) \u2212       f(x\u2217) \u2264       \u2225z0 \u2212    x\u2217\u22252        and      AN \u2265        (1 \u2212   \u221a\u03b2)2       N\u22121          \u02c6   2\n                                                        2AN                                 4(2 \u2212    \u221a\u03b2)2         k=0      \u03b7k       .\nProposition 3.1 characterizes the convergence rate of Algorithm 1 by the quantity AN, which can\nbe further lower bounded in terms of the step sizes {\u02c6                            \u03b7k}. Moreover, we can observe that larger step\nsizes will lead to a faster convergence rate. On the other hand, the step size \u02c6                                           \u03b7k is constrained by\nthe condition in (7), which, in turn, depends on our choice of the Hessian approximation matrix Bk.\nThus, the central goal of our line search scheme and the Hessian approximation update is to make\nthe step size \u02c6      \u03b7k as large as possible, which we will describe next.\n3.1       Line Search Subroutine\nIn this section, we specify our line search subroutine to select the step size \u02c6                                       \u03b7k and the iterate \u02c6         xk+1\nin the second stage of A-QNPE. For simplicity, denote \u2207f(yk) by g and drop the subscript k in yk\nand Bk. In light of (6) and (7), our goal in the second stage is to find a pair (\u02c6                                          \u03b7, \u02c6\n                                                                                                                               x+) such that\n                                    \u2225\u02c6x+ \u2212     y + \u02c6  \u03b7(g + B(\u02c6     x+ \u2212     y))\u2225    \u2264   \u03b11\u2225\u02c6 x+ \u2212     y\u2225,                                           (10)\n                                                  \u2225\u02c6x+ \u2212     y + \u02c6 \u03b7\u2207f(\u02c6    x+)\u2225     \u2264   (\u03b11 + \u03b12)\u2225\u02c6     x+ \u2212     y\u2225.                                (11)\nAs mentioned in the previous section, the condition in (10) can be satisfied by solving the linear\nsystem (I + \u02c6       \u03b7B)(\u02c6  x+ \u2212     y) = \u2212\u02c6     \u03b7g to a desired accuracy. Specifically, we let\n                                 s+ = LinearSolver(I + \u02c6            \u03b7B, \u2212\u02c6                           \u02c6\n                                                                             \u03b7g; \u03b11)        and      x+ = y + s+,                                    (12)\nwhere the oracle LinearSolver is defined as follows.\nDefinition 3.1. The oracle LinearSolver(A, b; \u03b1) takes a matrix A \u2208                                                Sd+, a vector b \u2208            Rd and\n\u03b1 \u2208    (0, 1) as input, and returns an approximate solution s+ satisfying \u2225As+ \u2212                                             b\u2225   \u2264   \u03b1\u2225s+\u2225.\nThe most direct way to implement LinearSolver(A, b; \u03b1) is to compute s+ = A\u22121b, which however\ncosts O(d3) arithmetic operations. Alternatively, we can implement the oracle more efficiently by\nusing the conjugate residual method [Saa03], which only requires computing matrix-vector products\n                                                                             7", "md": "# Math Equations in HTML\n\n## Guide our update scheme\n\nAs we will show in Section 3.2, the convergence rate of our method is closely related to the cumulative loss $$\\sum_{k\\in B} \\ell_k(B_k)$$ incurred by our choices of {B_k}, where B = {k : \u02c6 \u03b7_k &lt; \u03b7_k} denotes the indices where the line search scheme backtracks. Moreover, the loss function has the form $$\\ell_k(B_k) \\triangleq \\|\\mathbf{w}_k - B_k\\mathbf{s}_k\\|^2$$, where $$\\mathbf{w}_k \\triangleq \\nabla f(\\tilde{x}_k) - \\nabla f(x_k)$$, $$\\mathbf{s}_k \\triangleq \\frac{\\tilde{x}_k - x_k}{\\|\\mathbf{s}_k\\|^2}$$ and $$\\tilde{x}_k$$ is an auxiliary iterate returned by our line search scheme. Thus, this motivates us to employ an online learning algorithm to minimize the cumulative loss. Specifically, in the first case where \u02c6 \u03b7_k = \u03b7_k (i.e., k \u2209 B), the current Hessian approximation matrix B_k does not contribute to the cumulative loss and thus we keep it unchanged (cf. Line 9). Otherwise, we follow an online learning algorithm in the space of matrices. The details will be discussed in Section 3.2.\n\n## Convergence Result - Proposition 3.1\n\nLet {x_k}^N_{k=0} be the iterates generated by Algorithm 1. If f is convex, we have $$f(x_N) - f(x^*) \\leq \\|\\mathbf{z}_0 - x^*\\|^2$$ and $$A_N \\geq (1 - \\sqrt{\\beta})^2 4(2 - \\sqrt{\\beta})^2 \\sum_{k=0}^{N-1} \\eta_k^2$$.\n\n## Line Search Subroutine - Section 3.1\n\nIn this section, we specify our line search subroutine to select the step size \u02c6 \u03b7_k and the iterate \u02c6 x_{k+1} in the second stage of A-QNPE. For simplicity, denote \u2207f(y_k) by g and drop the subscript k in y_k and B_k. In light of (6) and (7), our goal in the second stage is to find a pair (\u02c6 \u03b7, \u02c6 x^+) such that\n\n$$\\|\\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y))\\| \\leq \\alpha_1\\|\\hat{x}^+ - y\\| \\quad (10)$$\n$$\\|\\hat{x}^+ - y + \\hat{\\eta}\\nabla f(\\hat{x}^+)\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}^+ - y\\| \\quad (11)$$\nAs mentioned in the previous section, the condition in (10) can be satisfied by solving the linear system $$(I + \\hat{\\eta}B)(\\hat{x}^+ - y) = -\\hat{\\eta}g$$ to a desired accuracy. Specifically, we let\n\n$$s^+ = \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\quad \\text{and} \\quad x^+ = y + s+ \\quad (12)$$\n\n### Definition 3.1 - Oracle LinearSolver\n\nThe oracle LinearSolver(A, b; \u03b1) takes a matrix A \u2208 S_d^+, a vector b \u2208 R^d and \u03b1 \u2208 (0, 1) as input, and returns an approximate solution s+ satisfying $$\\|As+ - b\\| \\leq \\alpha\\|s+\\|$$.\n\nThe most direct way to implement LinearSolver(A, b; \u03b1) is to compute $$s+ = A^{-1}b$$, which however costs O(d^3) arithmetic operations. Alternatively, we can implement the oracle more efficiently by using the conjugate residual method [Saa03], which only requires computing matrix-vector products.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations in HTML", "md": "# Math Equations in HTML"}, {"type": "heading", "lvl": 2, "value": "Guide our update scheme", "md": "## Guide our update scheme"}, {"type": "text", "value": "As we will show in Section 3.2, the convergence rate of our method is closely related to the cumulative loss $$\\sum_{k\\in B} \\ell_k(B_k)$$ incurred by our choices of {B_k}, where B = {k : \u02c6 \u03b7_k &lt; \u03b7_k} denotes the indices where the line search scheme backtracks. Moreover, the loss function has the form $$\\ell_k(B_k) \\triangleq \\|\\mathbf{w}_k - B_k\\mathbf{s}_k\\|^2$$, where $$\\mathbf{w}_k \\triangleq \\nabla f(\\tilde{x}_k) - \\nabla f(x_k)$$, $$\\mathbf{s}_k \\triangleq \\frac{\\tilde{x}_k - x_k}{\\|\\mathbf{s}_k\\|^2}$$ and $$\\tilde{x}_k$$ is an auxiliary iterate returned by our line search scheme. Thus, this motivates us to employ an online learning algorithm to minimize the cumulative loss. Specifically, in the first case where \u02c6 \u03b7_k = \u03b7_k (i.e., k \u2209 B), the current Hessian approximation matrix B_k does not contribute to the cumulative loss and thus we keep it unchanged (cf. Line 9). Otherwise, we follow an online learning algorithm in the space of matrices. The details will be discussed in Section 3.2.", "md": "As we will show in Section 3.2, the convergence rate of our method is closely related to the cumulative loss $$\\sum_{k\\in B} \\ell_k(B_k)$$ incurred by our choices of {B_k}, where B = {k : \u02c6 \u03b7_k &lt; \u03b7_k} denotes the indices where the line search scheme backtracks. Moreover, the loss function has the form $$\\ell_k(B_k) \\triangleq \\|\\mathbf{w}_k - B_k\\mathbf{s}_k\\|^2$$, where $$\\mathbf{w}_k \\triangleq \\nabla f(\\tilde{x}_k) - \\nabla f(x_k)$$, $$\\mathbf{s}_k \\triangleq \\frac{\\tilde{x}_k - x_k}{\\|\\mathbf{s}_k\\|^2}$$ and $$\\tilde{x}_k$$ is an auxiliary iterate returned by our line search scheme. Thus, this motivates us to employ an online learning algorithm to minimize the cumulative loss. Specifically, in the first case where \u02c6 \u03b7_k = \u03b7_k (i.e., k \u2209 B), the current Hessian approximation matrix B_k does not contribute to the cumulative loss and thus we keep it unchanged (cf. Line 9). Otherwise, we follow an online learning algorithm in the space of matrices. The details will be discussed in Section 3.2."}, {"type": "heading", "lvl": 2, "value": "Convergence Result - Proposition 3.1", "md": "## Convergence Result - Proposition 3.1"}, {"type": "text", "value": "Let {x_k}^N_{k=0} be the iterates generated by Algorithm 1. If f is convex, we have $$f(x_N) - f(x^*) \\leq \\|\\mathbf{z}_0 - x^*\\|^2$$ and $$A_N \\geq (1 - \\sqrt{\\beta})^2 4(2 - \\sqrt{\\beta})^2 \\sum_{k=0}^{N-1} \\eta_k^2$$.", "md": "Let {x_k}^N_{k=0} be the iterates generated by Algorithm 1. If f is convex, we have $$f(x_N) - f(x^*) \\leq \\|\\mathbf{z}_0 - x^*\\|^2$$ and $$A_N \\geq (1 - \\sqrt{\\beta})^2 4(2 - \\sqrt{\\beta})^2 \\sum_{k=0}^{N-1} \\eta_k^2$$."}, {"type": "heading", "lvl": 2, "value": "Line Search Subroutine - Section 3.1", "md": "## Line Search Subroutine - Section 3.1"}, {"type": "text", "value": "In this section, we specify our line search subroutine to select the step size \u02c6 \u03b7_k and the iterate \u02c6 x_{k+1} in the second stage of A-QNPE. For simplicity, denote \u2207f(y_k) by g and drop the subscript k in y_k and B_k. In light of (6) and (7), our goal in the second stage is to find a pair (\u02c6 \u03b7, \u02c6 x^+) such that\n\n$$\\|\\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y))\\| \\leq \\alpha_1\\|\\hat{x}^+ - y\\| \\quad (10)$$\n$$\\|\\hat{x}^+ - y + \\hat{\\eta}\\nabla f(\\hat{x}^+)\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}^+ - y\\| \\quad (11)$$\nAs mentioned in the previous section, the condition in (10) can be satisfied by solving the linear system $$(I + \\hat{\\eta}B)(\\hat{x}^+ - y) = -\\hat{\\eta}g$$ to a desired accuracy. Specifically, we let\n\n$$s^+ = \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\quad \\text{and} \\quad x^+ = y + s+ \\quad (12)$$", "md": "In this section, we specify our line search subroutine to select the step size \u02c6 \u03b7_k and the iterate \u02c6 x_{k+1} in the second stage of A-QNPE. For simplicity, denote \u2207f(y_k) by g and drop the subscript k in y_k and B_k. In light of (6) and (7), our goal in the second stage is to find a pair (\u02c6 \u03b7, \u02c6 x^+) such that\n\n$$\\|\\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y))\\| \\leq \\alpha_1\\|\\hat{x}^+ - y\\| \\quad (10)$$\n$$\\|\\hat{x}^+ - y + \\hat{\\eta}\\nabla f(\\hat{x}^+)\\| \\leq (\\alpha_1 + \\alpha_2)\\|\\hat{x}^+ - y\\| \\quad (11)$$\nAs mentioned in the previous section, the condition in (10) can be satisfied by solving the linear system $$(I + \\hat{\\eta}B)(\\hat{x}^+ - y) = -\\hat{\\eta}g$$ to a desired accuracy. Specifically, we let\n\n$$s^+ = \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\quad \\text{and} \\quad x^+ = y + s+ \\quad (12)$$"}, {"type": "heading", "lvl": 3, "value": "Definition 3.1 - Oracle LinearSolver", "md": "### Definition 3.1 - Oracle LinearSolver"}, {"type": "text", "value": "The oracle LinearSolver(A, b; \u03b1) takes a matrix A \u2208 S_d^+, a vector b \u2208 R^d and \u03b1 \u2208 (0, 1) as input, and returns an approximate solution s+ satisfying $$\\|As+ - b\\| \\leq \\alpha\\|s+\\|$$.\n\nThe most direct way to implement LinearSolver(A, b; \u03b1) is to compute $$s+ = A^{-1}b$$, which however costs O(d^3) arithmetic operations. Alternatively, we can implement the oracle more efficiently by using the conjugate residual method [Saa03], which only requires computing matrix-vector products.", "md": "The oracle LinearSolver(A, b; \u03b1) takes a matrix A \u2208 S_d^+, a vector b \u2208 R^d and \u03b1 \u2208 (0, 1) as input, and returns an approximate solution s+ satisfying $$\\|As+ - b\\| \\leq \\alpha\\|s+\\|$$.\n\nThe most direct way to implement LinearSolver(A, b; \u03b1) is to compute $$s+ = A^{-1}b$$, which however costs O(d^3) arithmetic operations. Alternatively, we can implement the oracle more efficiently by using the conjugate residual method [Saa03], which only requires computing matrix-vector products."}]}, {"page": 8, "text": "and thus incurs a cost of O(d2). The details are discussed in Appendix E.1. We characterize the\ntotal number of required matrix-vector products for this oracle in Theorem 4.2.\nNow we are ready to describe our line search scheme with the LinearSolver oracle (see also Subroutine 1\nin Appendix B). Specifically, we start with the step size \u03b7 and then reduce it by a factor \u03b2 until we\nfind a pair (\u02c6     \u03b7, \u02c6x+) that satisfies (11). It can be shown that the line search scheme will terminate in\na finite number of steps and return a pair (\u02c6                        \u03b7, \u02c6x+) satisfying both conditions in (10) and (11) (see\nAppendix B.1). Regarding the output, we distinguish two cases: (i) If we pass the test in (11) on\nour first attempt, we accept the initial step size \u03b7 and the corresponding iterate \u02c6                                               x+ (cf. Line 10\nin Subroutine 1). (ii) Otherwise, along with the pair (\u02c6                                \u03b7, \u02c6x+), we also return an auxiliary iterate\n \u02dc\nx+ that we compute from (12) using the rejected step size \u02c6                                    \u03b7/\u03b2 (cf. Line 12 in Subroutine 1). As\nwe shall see in Lemma 3.2, the iterate \u02dc                      x+ is used to derive a lower bound on \u02c6                       \u03b7, which will be the\nkey to our convergence analysis and guide our update of the Hessian approximation matrix. For\nease of notation, let B be the set of iteration indices where the line search scheme backtracks, i.e.,\nB \u225c     {k : \u02c6\u03b7k < \u03b7k}.\nLemma 3.2. For k /               \u2208  B we have \u02c6       \u03b7k = \u03b7k, while for k \u2208             B we have\n  \u02c6                           \u03b12\u03b2\u2225\u02dc   xk+1 \u2212     yk\u2225\n  \u03b7k >     \u2225\u2207f(\u02dc   xk+1) \u2212       \u2207f(yk) \u2212        Bk(\u02dc                         and      \u2225\u02dcxk+1 \u2212      yk\u2225    \u2264    (1 + \u03b11)        xk+1 \u2212      yk\u2225. (13)\n                                                       xk+1 \u2212      yk)\u2225                                         \u03b2(1 \u2212     \u03b11)\u2225\u02c6\nLemma 3.2 provides a lower bound on the step size \u02c6                                       \u03b7k in terms of the approximation error\n\u2225\u2207f(\u02dc    xk+1) \u2212      \u2207f(yk) \u2212        Bk(\u02dc  xk+1 \u2212     yk)\u2225. Hence, a better Hessian approximation matrix Bk leads to\na larger step size, which in turn implies faster convergence. Also note that the lower bound uses the\nauxiliary iterate \u02dc       xk+1 that is not accepted as the actual iterate. Thus, the second inequality in (13)\nwill be used to relate \u2225\u02dc           xk+1 \u2212      yk\u2225    with \u2225\u02c6   xk+1 \u2212      yk\u2225. Finally, we remark that to fully characterize\nthe computational cost, we need to upper bound the total number of line search steps, each of\nwhich requires a call to LinearSolver and a call to the gradient oracle. This will be discussed in\nTheorem 4.2.\n3.2       Hessian Approximation Update via Online Learning with Dynamic Regret\nIn this section, we discuss how to update the Hessian approximation matrix Bk in the fourth stage of\nA-QNPE. As mentioned earlier, instead of following the classical quasi-Newton updates, we directly\nmotivate our update policy for Bk from the convergence analysis. The first step is to connect the\nconvergence rate of A-QNPE with the Hessian approximation matrices {Bk}. By Proposition 3.1, if\nwe define the absolute constant C1 \u225c                        2(2\u2212\u221a\u03b2)2\n                                                             (1\u2212\u221a\u03b2)2 , then we can write                                 N\u22121       1\n                    f(xN) \u2212       f(x\u2217) \u2264       \u2225z0 \u2212     x\u2217\u22252    \u2264    C1\u2225z0 \u2212       x\u2217\u22252                                             ,              (14)\n                                                     2AN                 N\u22121       \u221a  \u02c6   2 \u2264     C1\u2225z0 \u2212N2.5   x\u2217\u22252               \u02c6\n                                                                                                                                   \u03b72\n                                                                            k=0       \u03b7k                                    k=0     k\nwhere the last inequality follows from H\u00a8                      older\u2019s inequality. Furthermore, we can establish an upper\nbound on  N\u22121               1\n                    k=0     \u02c6\n                            \u03b72\n                             k in terms of the Hessian approximation matrices {Bk}, as we show next.\nLemma 3.3. Let {\u02c6              \u03b7k}N\u22121\n                                    k=0 be the step sizes in Algorithm 1 using Subroutine 1. Then we have\n                          N\u22121     1   \u2264      2 \u2212    \u03b22      +        2 \u2212   \u03b22                             \u2225wk \u2212      Bksk\u22252      ,                   (15)\n                                 \u02c6                                                                               \u2225sk\u22252\n                          k=0    \u03b72k      (1 \u2212    \u03b22)\u03c32  0      (1 \u2212    \u03b22)\u03b12  2\u03b22   0\u2264k\u2264N\u22121,k\u2208B\nwhere wk \u225c          \u2207f(\u02dc  xk+1) \u2212       \u2207f(yk) and sk \u225c             \u02dc\n                                                                    xk+1 \u2212   8  yk for k \u2208       B.", "md": "and thus incurs a cost of O(d^2). The details are discussed in Appendix E.1. We characterize the\ntotal number of required matrix-vector products for this oracle in Theorem 4.2.\n\nNow we are ready to describe our line search scheme with the LinearSolver oracle (see also Subroutine 1\nin Appendix B). Specifically, we start with the step size \u03b7 and then reduce it by a factor \u03b2 until we\nfind a pair ($$\\hat{\\eta}, \\hat{x}^+$$) that satisfies (11). It can be shown that the line search scheme will terminate in\na finite number of steps and return a pair ($$\\hat{\\eta}, \\hat{x}^+)$$ satisfying both conditions in (10) and (11) (see\nAppendix B.1). Regarding the output, we distinguish two cases: (i) If we pass the test in (11) on\nour first attempt, we accept the initial step size \u03b7 and the corresponding iterate $$\\hat{x}^+$$ (cf. Line 10\nin Subroutine 1). (ii) Otherwise, along with the pair ($$\\hat{\\eta}, \\hat{x}^+$$), we also return an auxiliary iterate\n$$\\tilde{x}^+$$ that we compute from (12) using the rejected step size $$\\hat{\\eta}/\\beta$$ (cf. Line 12 in Subroutine 1). As\nwe shall see in Lemma 3.2, the iterate $$\\tilde{x}^+$$ is used to derive a lower bound on $$\\hat{\\eta}$$, which will be the\nkey to our convergence analysis and guide our update of the Hessian approximation matrix. For\nease of notation, let B be the set of iteration indices where the line search scheme backtracks, i.e.,\n$$B \\triangleq \\{k : \\hat{\\eta}_k < \\eta_k\\}$$.\n\nLemma 3.2. For k not in B we have $$\\hat{\\eta}_k = \\eta_k$$, while for k in B we have\n\n$$\n\\begin{aligned}\n&\\hat{\\eta}_k > \\frac{\\alpha^2\\beta\\|\\tilde{x}_k+1 - y_k\\|}{\\|\\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k) - B_k(\\tilde{x}_k+1 - y_k)\\|}, \\\\\n&\\|\\tilde{x}_k+1 - y_k\\| \\leq (1 + \\alpha_1)\\|\\hat{x}_k+1 - y_k\\|.\n\\end{aligned}\n$$\nLemma 3.2 provides a lower bound on the step size $$\\hat{\\eta}_k$$ in terms of the approximation error\n$$\\|\\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k) - B_k(\\tilde{x}_k+1 - y_k)\\|$$. Hence, a better Hessian approximation matrix B_k leads to\na larger step size, which in turn implies faster convergence. Also note that the lower bound uses the\nauxiliary iterate $$\\tilde{x}_k+1$$ that is not accepted as the actual iterate. Thus, the second inequality in (13)\nwill be used to relate $$\\|\\tilde{x}_k+1 - y_k\\|$$ with $$\\|\\hat{x}_k+1 - y_k\\$$. Finally, we remark that to fully characterize\nthe computational cost, we need to upper bound the total number of line search steps, each of\nwhich requires a call to LinearSolver and a call to the gradient oracle. This will be discussed in\nTheorem 4.2.\n\n3.2 Hessian Approximation Update via Online Learning with Dynamic Regret\n\nIn this section, we discuss how to update the Hessian approximation matrix B_k in the fourth stage of\nA-QNPE. As mentioned earlier, instead of following the classical quasi-Newton updates, we directly\nmotivate our update policy for B_k from the convergence analysis. The first step is to connect the\nconvergence rate of A-QNPE with the Hessian approximation matrices {B_k}. By Proposition 3.1, if\nwe define the absolute constant C1 as $$C1 \\triangleq \\frac{2(2-\\sqrt{\\beta})^2}{(1-\\sqrt{\\beta})^2}$$, then we can write\n\n$$\nf(x_N) - f(x^*) \\leq \\|z_0 - x^*\\|^2 \\leq C1\\|z_0 - x^*\\|^2,\n$$\nwhere the last inequality follows from H\u00f6lder's inequality. Furthermore, we can establish an upper\nbound on $$\\sum_{k=0}^{N-1} \\hat{\\eta}_k^2$$ in terms of the Hessian approximation matrices {B_k}, as we show next.\n\nLemma 3.3. Let {$$\\hat{\\eta}_k$$} be the step sizes in Algorithm 1 using Subroutine 1. Then we have\n\n$$\n\\sum_{k=0}^{N-1} \\frac{1}{\\hat{\\eta}_k^2} \\leq \\frac{2 - \\beta^2}{(1 - \\beta^2)\\sigma_0^2} + \\frac{2 - \\beta^2}{(1 - \\beta^2)\\alpha_2^2\\beta^2} \\sum_{0 \\leq k \\leq N-1, k \\in B} \\|w_k - B_k s_k\\|^2,\n$$\nwhere $$w_k \\triangleq \\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k)$$ and $$s_k \\triangleq \\tilde{x}_k+1 - y_k$$ for $$k \\in B$$.", "images": [], "items": [{"type": "text", "value": "and thus incurs a cost of O(d^2). The details are discussed in Appendix E.1. We characterize the\ntotal number of required matrix-vector products for this oracle in Theorem 4.2.\n\nNow we are ready to describe our line search scheme with the LinearSolver oracle (see also Subroutine 1\nin Appendix B). Specifically, we start with the step size \u03b7 and then reduce it by a factor \u03b2 until we\nfind a pair ($$\\hat{\\eta}, \\hat{x}^+$$) that satisfies (11). It can be shown that the line search scheme will terminate in\na finite number of steps and return a pair ($$\\hat{\\eta}, \\hat{x}^+)$$ satisfying both conditions in (10) and (11) (see\nAppendix B.1). Regarding the output, we distinguish two cases: (i) If we pass the test in (11) on\nour first attempt, we accept the initial step size \u03b7 and the corresponding iterate $$\\hat{x}^+$$ (cf. Line 10\nin Subroutine 1). (ii) Otherwise, along with the pair ($$\\hat{\\eta}, \\hat{x}^+$$), we also return an auxiliary iterate\n$$\\tilde{x}^+$$ that we compute from (12) using the rejected step size $$\\hat{\\eta}/\\beta$$ (cf. Line 12 in Subroutine 1). As\nwe shall see in Lemma 3.2, the iterate $$\\tilde{x}^+$$ is used to derive a lower bound on $$\\hat{\\eta}$$, which will be the\nkey to our convergence analysis and guide our update of the Hessian approximation matrix. For\nease of notation, let B be the set of iteration indices where the line search scheme backtracks, i.e.,\n$$B \\triangleq \\{k : \\hat{\\eta}_k < \\eta_k\\}$$.\n\nLemma 3.2. For k not in B we have $$\\hat{\\eta}_k = \\eta_k$$, while for k in B we have\n\n$$\n\\begin{aligned}\n&\\hat{\\eta}_k > \\frac{\\alpha^2\\beta\\|\\tilde{x}_k+1 - y_k\\|}{\\|\\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k) - B_k(\\tilde{x}_k+1 - y_k)\\|}, \\\\\n&\\|\\tilde{x}_k+1 - y_k\\| \\leq (1 + \\alpha_1)\\|\\hat{x}_k+1 - y_k\\|.\n\\end{aligned}\n$$\nLemma 3.2 provides a lower bound on the step size $$\\hat{\\eta}_k$$ in terms of the approximation error\n$$\\|\\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k) - B_k(\\tilde{x}_k+1 - y_k)\\|$$. Hence, a better Hessian approximation matrix B_k leads to\na larger step size, which in turn implies faster convergence. Also note that the lower bound uses the\nauxiliary iterate $$\\tilde{x}_k+1$$ that is not accepted as the actual iterate. Thus, the second inequality in (13)\nwill be used to relate $$\\|\\tilde{x}_k+1 - y_k\\|$$ with $$\\|\\hat{x}_k+1 - y_k\\$$. Finally, we remark that to fully characterize\nthe computational cost, we need to upper bound the total number of line search steps, each of\nwhich requires a call to LinearSolver and a call to the gradient oracle. This will be discussed in\nTheorem 4.2.\n\n3.2 Hessian Approximation Update via Online Learning with Dynamic Regret\n\nIn this section, we discuss how to update the Hessian approximation matrix B_k in the fourth stage of\nA-QNPE. As mentioned earlier, instead of following the classical quasi-Newton updates, we directly\nmotivate our update policy for B_k from the convergence analysis. The first step is to connect the\nconvergence rate of A-QNPE with the Hessian approximation matrices {B_k}. By Proposition 3.1, if\nwe define the absolute constant C1 as $$C1 \\triangleq \\frac{2(2-\\sqrt{\\beta})^2}{(1-\\sqrt{\\beta})^2}$$, then we can write\n\n$$\nf(x_N) - f(x^*) \\leq \\|z_0 - x^*\\|^2 \\leq C1\\|z_0 - x^*\\|^2,\n$$\nwhere the last inequality follows from H\u00f6lder's inequality. Furthermore, we can establish an upper\nbound on $$\\sum_{k=0}^{N-1} \\hat{\\eta}_k^2$$ in terms of the Hessian approximation matrices {B_k}, as we show next.\n\nLemma 3.3. Let {$$\\hat{\\eta}_k$$} be the step sizes in Algorithm 1 using Subroutine 1. Then we have\n\n$$\n\\sum_{k=0}^{N-1} \\frac{1}{\\hat{\\eta}_k^2} \\leq \\frac{2 - \\beta^2}{(1 - \\beta^2)\\sigma_0^2} + \\frac{2 - \\beta^2}{(1 - \\beta^2)\\alpha_2^2\\beta^2} \\sum_{0 \\leq k \\leq N-1, k \\in B} \\|w_k - B_k s_k\\|^2,\n$$\nwhere $$w_k \\triangleq \\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k)$$ and $$s_k \\triangleq \\tilde{x}_k+1 - y_k$$ for $$k \\in B$$.", "md": "and thus incurs a cost of O(d^2). The details are discussed in Appendix E.1. We characterize the\ntotal number of required matrix-vector products for this oracle in Theorem 4.2.\n\nNow we are ready to describe our line search scheme with the LinearSolver oracle (see also Subroutine 1\nin Appendix B). Specifically, we start with the step size \u03b7 and then reduce it by a factor \u03b2 until we\nfind a pair ($$\\hat{\\eta}, \\hat{x}^+$$) that satisfies (11). It can be shown that the line search scheme will terminate in\na finite number of steps and return a pair ($$\\hat{\\eta}, \\hat{x}^+)$$ satisfying both conditions in (10) and (11) (see\nAppendix B.1). Regarding the output, we distinguish two cases: (i) If we pass the test in (11) on\nour first attempt, we accept the initial step size \u03b7 and the corresponding iterate $$\\hat{x}^+$$ (cf. Line 10\nin Subroutine 1). (ii) Otherwise, along with the pair ($$\\hat{\\eta}, \\hat{x}^+$$), we also return an auxiliary iterate\n$$\\tilde{x}^+$$ that we compute from (12) using the rejected step size $$\\hat{\\eta}/\\beta$$ (cf. Line 12 in Subroutine 1). As\nwe shall see in Lemma 3.2, the iterate $$\\tilde{x}^+$$ is used to derive a lower bound on $$\\hat{\\eta}$$, which will be the\nkey to our convergence analysis and guide our update of the Hessian approximation matrix. For\nease of notation, let B be the set of iteration indices where the line search scheme backtracks, i.e.,\n$$B \\triangleq \\{k : \\hat{\\eta}_k < \\eta_k\\}$$.\n\nLemma 3.2. For k not in B we have $$\\hat{\\eta}_k = \\eta_k$$, while for k in B we have\n\n$$\n\\begin{aligned}\n&\\hat{\\eta}_k > \\frac{\\alpha^2\\beta\\|\\tilde{x}_k+1 - y_k\\|}{\\|\\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k) - B_k(\\tilde{x}_k+1 - y_k)\\|}, \\\\\n&\\|\\tilde{x}_k+1 - y_k\\| \\leq (1 + \\alpha_1)\\|\\hat{x}_k+1 - y_k\\|.\n\\end{aligned}\n$$\nLemma 3.2 provides a lower bound on the step size $$\\hat{\\eta}_k$$ in terms of the approximation error\n$$\\|\\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k) - B_k(\\tilde{x}_k+1 - y_k)\\|$$. Hence, a better Hessian approximation matrix B_k leads to\na larger step size, which in turn implies faster convergence. Also note that the lower bound uses the\nauxiliary iterate $$\\tilde{x}_k+1$$ that is not accepted as the actual iterate. Thus, the second inequality in (13)\nwill be used to relate $$\\|\\tilde{x}_k+1 - y_k\\|$$ with $$\\|\\hat{x}_k+1 - y_k\\$$. Finally, we remark that to fully characterize\nthe computational cost, we need to upper bound the total number of line search steps, each of\nwhich requires a call to LinearSolver and a call to the gradient oracle. This will be discussed in\nTheorem 4.2.\n\n3.2 Hessian Approximation Update via Online Learning with Dynamic Regret\n\nIn this section, we discuss how to update the Hessian approximation matrix B_k in the fourth stage of\nA-QNPE. As mentioned earlier, instead of following the classical quasi-Newton updates, we directly\nmotivate our update policy for B_k from the convergence analysis. The first step is to connect the\nconvergence rate of A-QNPE with the Hessian approximation matrices {B_k}. By Proposition 3.1, if\nwe define the absolute constant C1 as $$C1 \\triangleq \\frac{2(2-\\sqrt{\\beta})^2}{(1-\\sqrt{\\beta})^2}$$, then we can write\n\n$$\nf(x_N) - f(x^*) \\leq \\|z_0 - x^*\\|^2 \\leq C1\\|z_0 - x^*\\|^2,\n$$\nwhere the last inequality follows from H\u00f6lder's inequality. Furthermore, we can establish an upper\nbound on $$\\sum_{k=0}^{N-1} \\hat{\\eta}_k^2$$ in terms of the Hessian approximation matrices {B_k}, as we show next.\n\nLemma 3.3. Let {$$\\hat{\\eta}_k$$} be the step sizes in Algorithm 1 using Subroutine 1. Then we have\n\n$$\n\\sum_{k=0}^{N-1} \\frac{1}{\\hat{\\eta}_k^2} \\leq \\frac{2 - \\beta^2}{(1 - \\beta^2)\\sigma_0^2} + \\frac{2 - \\beta^2}{(1 - \\beta^2)\\alpha_2^2\\beta^2} \\sum_{0 \\leq k \\leq N-1, k \\in B} \\|w_k - B_k s_k\\|^2,\n$$\nwhere $$w_k \\triangleq \\nabla f(\\tilde{x}_k+1) - \\nabla f(y_k)$$ and $$s_k \\triangleq \\tilde{x}_k+1 - y_k$$ for $$k \\in B$$."}]}, {"page": 9, "text": "The proof of Lemma 3.3 is given in Appendix C.1. On a high level, for those step sizes \u02c6                                                       \u03b7k with\nk \u2208    B, we can apply Lemma 3.2 and directly obtain a lower bound in terms of Bk. On the other\nhand, for k /      \u2208  B, we have \u02c6      \u03b7k = \u03b7k and our update rule in Lines 8 and 13 of Algorithm 1 allows us to\nconnect the sequence {\u03b7k}N\u22121              k=0 with the backtracked step sizes {\u02c6                       \u03b7k : k \u2208     B}. As a result, we note\nthat the sum in (15) only involves the Hessian approximation matrices {Bk : k \u2208                                                   B}.\nIn light of (14) and (15), our update for Bk aims to make the right-hand side of (15) as small as\npossible. To achieve this, we adopt the online learning approach in [JJM23] and view the sum in\n(15) as the cumulative loss incurred by our choice of {Bk}. To formalize, define the loss at iteration\nk by                                               \u2113k(B) \u225c        0,\u2225wk\u2212Bsk\u22252           if k / \u2208  B,                                                 (16)\n                                                                        \u2225sk\u22252       ,   otherwise,\nand consider the following online learning problem: (i) At the k-th iteration, we choose Bk \u2208                                                           Z\nwhere Z \u225c          {B \u2208     Sd+ : 0 \u2aaf      B \u2aaf     L1I}; (ii) We receive the loss function \u2113k(B) defined in (16); (iii)\nWe update our Hessian approximation matrix to Bk+1. Therefore, we propose to employ an online\nlearning algorithm to update the Hessian approximation matrices {Bk}, and the task of proving a\nconvergence rate for our A-QNPE algorithm boils down to analyzing the performance of our online\nlearning algorithm. In particular, an upper bound on the cumulative loss  N\u22121                                         k=0 \u2113k(Bk) will directly\ntranslate into a convergence rate for A-QNPE by using (14) and (15).\nNaturally, the first idea is to update Bk by following projected online gradient descent [Zin03].\nWhile this approach would indeed serve our purpose, its implementation could be computationally\nexpensive. Specifically, like other projection-based methods, it requires computing the Euclidean\nprojection onto the set Z in each iteration, which in our case amounts to performing a full d \u00d7 d\nmatrix eigendecomposition and would incur a cost of O(d3) (see Appendix C.2). Inspired by\nthe recent work in [Mha22], we circumvent this issue by using a projection-free online learning\nalgorithm, which relies on an approximate separation oracle for Z instead of a projection oracle.                         2\nFor simplicity, we first translate and rescale the set Z via the transform \u02c6                                     B =     L1 (B \u2212     L1\n                                                                                                                                      2 I) to obtain\nZ\u02c6\u225c      {B\u02c6\u2208     Sd : \u2225    \u02c6\nfollows.                   B\u2225op \u2264       1}. The approximate separation oracle SEP(W; \u03b4, q) is then defined as\nDefinition 3.2. The oracle SEP(W; \u03b4, q) takes a symmetric matrix W \u2208                                                  Sd, \u03b4 > 0, and q \u2208            (0, 1)\nas input and returns a scalar \u03b3 > 0 and a matrix S \u2208                                Sd with one of the following possible outcomes:\n\u2022 Case I: \u03b3 \u2264          1, which implies that, with probability at least 1 \u2212                           q, W \u2208        \u02c6\n                                                                                                                   Z;\n\u2022 Case II: \u03b3 > 1, which implies that, with probability at least 1 \u2212                                          q, W/\u03b3 \u2208         Z,\u02c6\u2225S\u2225F \u2264           3 and\n    \u27e8S, W \u2212        \u02c6\n                  B\u27e9   \u2265   \u03b3 \u2212    1 \u2212   \u03b4 for any \u02c6     B such that \u02c6       B \u2208     \u02c6\nTo sum up, SEP(W; \u03b4, q) has two possible outcomes: with probability 1 \u2212            Z.                                 q, either it certifies that\nW \u2208       \u02c6\n         Z, or it produces a scaled version of W that belongs to \u02c6                                   Z and an approximate separation\nhyperplane between W and the set \u02c6                         Z. As we show in Appendix E.2, implementing this oracle\nrequires computing the two extreme eigenvectors and eigenvalues of the matrix W inexactly, which\ncan be implemented efficiently by the randomized Lanczos method [KW92].\nBuilding on the SEP(W; \u03b4, q) oracle, we design a projection-free online learning algorithm adapted\nfrom [JJM23, Subroutine 2]. Since the algorithm is similar to the one proposed in [JJM23], we\nrelegate the details to Appendix C but sketch the main steps in the analysis. To upper bound the\ncumulative loss  N\u22121         k=0 \u2113k(Bk), we compare the performance of our online learning algorithm against\na sequence of reference matrices {Hk}N\u22121                   k=0 . Specifically, we aim to control the dynamic regret [Zin03;\n                                                                             9", "md": "The proof of Lemma 3.3 is given in Appendix C.1. On a high level, for those step sizes $$\\hat{\\eta}_k$$ with $$k \\in \\mathcal{B}$$, we can apply Lemma 3.2 and directly obtain a lower bound in terms of $$B_k$$. On the other hand, for $$k \\notin \\mathcal{B}$$, we have $$\\hat{\\eta}_k = \\eta_k$$ and our update rule in Lines 8 and 13 of Algorithm 1 allows us to connect the sequence {$$\\eta_k$$}$$^{N-1}_{k=0}$$ with the backtracked step sizes {$$\\hat{\\eta}_k : k \\in \\mathcal{B}$$}. As a result, we note that the sum in (15) only involves the Hessian approximation matrices {$$B_k : k \\in \\mathcal{B}$$}.\n\nIn light of (14) and (15), our update for $$B_k$$ aims to make the right-hand side of (15) as small as possible. To achieve this, we adopt the online learning approach in [JJM23] and view the sum in (15) as the cumulative loss incurred by our choice of {$$B_k$$}. To formalize, define the loss at iteration $$k$$ by\n\n$$\n\\ell_k(B) \\triangleq\n\\begin{cases}\n0, & \\text{if } k \\notin \\mathcal{B}, \\|w_k - B s_k\\|_2^2 \\\\\n\\|s_k\\|_2^2, & \\text{otherwise},\n\\end{cases}\n$$\n\nand consider the following online learning problem:\n(i) At the $$k$$-th iteration, we choose $$B_k \\in \\mathcal{Z}$$ where $$\\mathcal{Z} \\triangleq \\{B \\in \\mathcal{S}^d_+ : 0 \\preceq B \\preceq L_1 I\\}$$;\n(ii) We receive the loss function $$\\ell_k(B)$$ defined in (16);\n(iii) We update our Hessian approximation matrix to $$B_{k+1}$$. Therefore, we propose to employ an online learning algorithm to update the Hessian approximation matrices {$$B_k$$}, and the task of proving a convergence rate for our A-QNPE algorithm boils down to analyzing the performance of our online learning algorithm. In particular, an upper bound on the cumulative loss $$\\sum^{N-1}_{k=0} \\ell_k(B_k)$$ will directly translate into a convergence rate for A-QNPE by using (14) and (15).\n\nNaturally, the first idea is to update $$B_k$$ by following projected online gradient descent [Zin03]. While this approach would indeed serve our purpose, its implementation could be computationally expensive. Specifically, like other projection-based methods, it requires computing the Euclidean projection onto the set $$\\mathcal{Z}$$ in each iteration, which in our case amounts to performing a full $$d \\times d$$ matrix eigendecomposition and would incur a cost of $$O(d^3)$$ (see Appendix C.2). Inspired by the recent work in [Mha22], we circumvent this issue by using a projection-free online learning algorithm, which relies on an approximate separation oracle for $$\\mathcal{Z}$$ instead of a projection oracle.\n\nFor simplicity, we first translate and rescale the set $$\\mathcal{Z}$$ via the transform $$\\hat{B} = L_1 (B - \\frac{L_1}{2} I)$$ to obtain $$\\mathcal{Z}^\\wedge \\triangleq \\{B^\\wedge \\in \\mathcal{S}^d : \\|B^\\wedge\\|_{op} \\leq 1\\}$$. The approximate separation oracle $$\\text{SEP}(W; \\delta, q)$$ is then defined as follows.\n\n**Definition 3.2.** The oracle $$\\text{SEP}(W; \\delta, q)$$ takes a symmetric matrix $$W \\in \\mathcal{S}^d$$, $$\\delta > 0$$, and $$q \\in (0, 1)$$ as input and returns a scalar $$\\gamma > 0$$ and a matrix $$S \\in \\mathcal{S}^d$$ with one of the following possible outcomes:\n- Case I: $$\\gamma \\leq 1$$, which implies that, with probability at least $$1 - q$$, $$W \\in \\mathcal{Z}^\\wedge$$;\n- Case II: $$\\gamma > 1$$, which implies that, with probability at least $$1 - q$$, $$\\frac{W}{\\gamma} \\in \\mathcal{Z}^\\wedge$$, $$\\|S\\|_F \\leq 3$$ and $$\\langle S, W - B \\rangle \\geq \\gamma - 1 - \\delta$$ for any $$B^\\wedge$$ such that $$B^\\wedge \\in \\mathcal{Z}^\\wedge$$.\n\nTo sum up, $$\\text{SEP}(W; \\delta, q)$$ has two possible outcomes: with probability $$1 - q$$, either it certifies that $$W \\in \\mathcal{Z}^\\wedge$$, or it produces a scaled version of $$W$$ that belongs to $$\\mathcal{Z}^\\wedge$$ and an approximate separation hyperplane between $$W$$ and the set $$\\mathcal{Z}^\\wedge$$. As we show in Appendix E.2, implementing this oracle requires computing the two extreme eigenvectors and eigenvalues of the matrix $$W$$ inexactly, which can be implemented efficiently by the randomized Lanczos method [KW92].\n\nBuilding on the $$\\text{SEP}(W; \\delta, q)$$ oracle, we design a projection-free online learning algorithm adapted from [JJM23, Subroutine 2]. Since the algorithm is similar to the one proposed in [JJM23], we relegate the details to Appendix C but sketch the main steps in the analysis. To upper bound the cumulative loss $$\\sum^{N-1}_{k=0} \\ell_k(B_k)$$, we compare the performance of our online learning algorithm against a sequence of reference matrices {$$H_k$$}$$^{N-1}_{k=0}$$. Specifically, we aim to control the dynamic regret [Zin03].", "images": [], "items": [{"type": "text", "value": "The proof of Lemma 3.3 is given in Appendix C.1. On a high level, for those step sizes $$\\hat{\\eta}_k$$ with $$k \\in \\mathcal{B}$$, we can apply Lemma 3.2 and directly obtain a lower bound in terms of $$B_k$$. On the other hand, for $$k \\notin \\mathcal{B}$$, we have $$\\hat{\\eta}_k = \\eta_k$$ and our update rule in Lines 8 and 13 of Algorithm 1 allows us to connect the sequence {$$\\eta_k$$}$$^{N-1}_{k=0}$$ with the backtracked step sizes {$$\\hat{\\eta}_k : k \\in \\mathcal{B}$$}. As a result, we note that the sum in (15) only involves the Hessian approximation matrices {$$B_k : k \\in \\mathcal{B}$$}.\n\nIn light of (14) and (15), our update for $$B_k$$ aims to make the right-hand side of (15) as small as possible. To achieve this, we adopt the online learning approach in [JJM23] and view the sum in (15) as the cumulative loss incurred by our choice of {$$B_k$$}. To formalize, define the loss at iteration $$k$$ by\n\n$$\n\\ell_k(B) \\triangleq\n\\begin{cases}\n0, & \\text{if } k \\notin \\mathcal{B}, \\|w_k - B s_k\\|_2^2 \\\\\n\\|s_k\\|_2^2, & \\text{otherwise},\n\\end{cases}\n$$\n\nand consider the following online learning problem:\n(i) At the $$k$$-th iteration, we choose $$B_k \\in \\mathcal{Z}$$ where $$\\mathcal{Z} \\triangleq \\{B \\in \\mathcal{S}^d_+ : 0 \\preceq B \\preceq L_1 I\\}$$;\n(ii) We receive the loss function $$\\ell_k(B)$$ defined in (16);\n(iii) We update our Hessian approximation matrix to $$B_{k+1}$$. Therefore, we propose to employ an online learning algorithm to update the Hessian approximation matrices {$$B_k$$}, and the task of proving a convergence rate for our A-QNPE algorithm boils down to analyzing the performance of our online learning algorithm. In particular, an upper bound on the cumulative loss $$\\sum^{N-1}_{k=0} \\ell_k(B_k)$$ will directly translate into a convergence rate for A-QNPE by using (14) and (15).\n\nNaturally, the first idea is to update $$B_k$$ by following projected online gradient descent [Zin03]. While this approach would indeed serve our purpose, its implementation could be computationally expensive. Specifically, like other projection-based methods, it requires computing the Euclidean projection onto the set $$\\mathcal{Z}$$ in each iteration, which in our case amounts to performing a full $$d \\times d$$ matrix eigendecomposition and would incur a cost of $$O(d^3)$$ (see Appendix C.2). Inspired by the recent work in [Mha22], we circumvent this issue by using a projection-free online learning algorithm, which relies on an approximate separation oracle for $$\\mathcal{Z}$$ instead of a projection oracle.\n\nFor simplicity, we first translate and rescale the set $$\\mathcal{Z}$$ via the transform $$\\hat{B} = L_1 (B - \\frac{L_1}{2} I)$$ to obtain $$\\mathcal{Z}^\\wedge \\triangleq \\{B^\\wedge \\in \\mathcal{S}^d : \\|B^\\wedge\\|_{op} \\leq 1\\}$$. The approximate separation oracle $$\\text{SEP}(W; \\delta, q)$$ is then defined as follows.\n\n**Definition 3.2.** The oracle $$\\text{SEP}(W; \\delta, q)$$ takes a symmetric matrix $$W \\in \\mathcal{S}^d$$, $$\\delta > 0$$, and $$q \\in (0, 1)$$ as input and returns a scalar $$\\gamma > 0$$ and a matrix $$S \\in \\mathcal{S}^d$$ with one of the following possible outcomes:\n- Case I: $$\\gamma \\leq 1$$, which implies that, with probability at least $$1 - q$$, $$W \\in \\mathcal{Z}^\\wedge$$;\n- Case II: $$\\gamma > 1$$, which implies that, with probability at least $$1 - q$$, $$\\frac{W}{\\gamma} \\in \\mathcal{Z}^\\wedge$$, $$\\|S\\|_F \\leq 3$$ and $$\\langle S, W - B \\rangle \\geq \\gamma - 1 - \\delta$$ for any $$B^\\wedge$$ such that $$B^\\wedge \\in \\mathcal{Z}^\\wedge$$.\n\nTo sum up, $$\\text{SEP}(W; \\delta, q)$$ has two possible outcomes: with probability $$1 - q$$, either it certifies that $$W \\in \\mathcal{Z}^\\wedge$$, or it produces a scaled version of $$W$$ that belongs to $$\\mathcal{Z}^\\wedge$$ and an approximate separation hyperplane between $$W$$ and the set $$\\mathcal{Z}^\\wedge$$. As we show in Appendix E.2, implementing this oracle requires computing the two extreme eigenvectors and eigenvalues of the matrix $$W$$ inexactly, which can be implemented efficiently by the randomized Lanczos method [KW92].\n\nBuilding on the $$\\text{SEP}(W; \\delta, q)$$ oracle, we design a projection-free online learning algorithm adapted from [JJM23, Subroutine 2]. Since the algorithm is similar to the one proposed in [JJM23], we relegate the details to Appendix C but sketch the main steps in the analysis. To upper bound the cumulative loss $$\\sum^{N-1}_{k=0} \\ell_k(B_k)$$, we compare the performance of our online learning algorithm against a sequence of reference matrices {$$H_k$$}$$^{N-1}_{k=0}$$. Specifically, we aim to control the dynamic regret [Zin03].", "md": "The proof of Lemma 3.3 is given in Appendix C.1. On a high level, for those step sizes $$\\hat{\\eta}_k$$ with $$k \\in \\mathcal{B}$$, we can apply Lemma 3.2 and directly obtain a lower bound in terms of $$B_k$$. On the other hand, for $$k \\notin \\mathcal{B}$$, we have $$\\hat{\\eta}_k = \\eta_k$$ and our update rule in Lines 8 and 13 of Algorithm 1 allows us to connect the sequence {$$\\eta_k$$}$$^{N-1}_{k=0}$$ with the backtracked step sizes {$$\\hat{\\eta}_k : k \\in \\mathcal{B}$$}. As a result, we note that the sum in (15) only involves the Hessian approximation matrices {$$B_k : k \\in \\mathcal{B}$$}.\n\nIn light of (14) and (15), our update for $$B_k$$ aims to make the right-hand side of (15) as small as possible. To achieve this, we adopt the online learning approach in [JJM23] and view the sum in (15) as the cumulative loss incurred by our choice of {$$B_k$$}. To formalize, define the loss at iteration $$k$$ by\n\n$$\n\\ell_k(B) \\triangleq\n\\begin{cases}\n0, & \\text{if } k \\notin \\mathcal{B}, \\|w_k - B s_k\\|_2^2 \\\\\n\\|s_k\\|_2^2, & \\text{otherwise},\n\\end{cases}\n$$\n\nand consider the following online learning problem:\n(i) At the $$k$$-th iteration, we choose $$B_k \\in \\mathcal{Z}$$ where $$\\mathcal{Z} \\triangleq \\{B \\in \\mathcal{S}^d_+ : 0 \\preceq B \\preceq L_1 I\\}$$;\n(ii) We receive the loss function $$\\ell_k(B)$$ defined in (16);\n(iii) We update our Hessian approximation matrix to $$B_{k+1}$$. Therefore, we propose to employ an online learning algorithm to update the Hessian approximation matrices {$$B_k$$}, and the task of proving a convergence rate for our A-QNPE algorithm boils down to analyzing the performance of our online learning algorithm. In particular, an upper bound on the cumulative loss $$\\sum^{N-1}_{k=0} \\ell_k(B_k)$$ will directly translate into a convergence rate for A-QNPE by using (14) and (15).\n\nNaturally, the first idea is to update $$B_k$$ by following projected online gradient descent [Zin03]. While this approach would indeed serve our purpose, its implementation could be computationally expensive. Specifically, like other projection-based methods, it requires computing the Euclidean projection onto the set $$\\mathcal{Z}$$ in each iteration, which in our case amounts to performing a full $$d \\times d$$ matrix eigendecomposition and would incur a cost of $$O(d^3)$$ (see Appendix C.2). Inspired by the recent work in [Mha22], we circumvent this issue by using a projection-free online learning algorithm, which relies on an approximate separation oracle for $$\\mathcal{Z}$$ instead of a projection oracle.\n\nFor simplicity, we first translate and rescale the set $$\\mathcal{Z}$$ via the transform $$\\hat{B} = L_1 (B - \\frac{L_1}{2} I)$$ to obtain $$\\mathcal{Z}^\\wedge \\triangleq \\{B^\\wedge \\in \\mathcal{S}^d : \\|B^\\wedge\\|_{op} \\leq 1\\}$$. The approximate separation oracle $$\\text{SEP}(W; \\delta, q)$$ is then defined as follows.\n\n**Definition 3.2.** The oracle $$\\text{SEP}(W; \\delta, q)$$ takes a symmetric matrix $$W \\in \\mathcal{S}^d$$, $$\\delta > 0$$, and $$q \\in (0, 1)$$ as input and returns a scalar $$\\gamma > 0$$ and a matrix $$S \\in \\mathcal{S}^d$$ with one of the following possible outcomes:\n- Case I: $$\\gamma \\leq 1$$, which implies that, with probability at least $$1 - q$$, $$W \\in \\mathcal{Z}^\\wedge$$;\n- Case II: $$\\gamma > 1$$, which implies that, with probability at least $$1 - q$$, $$\\frac{W}{\\gamma} \\in \\mathcal{Z}^\\wedge$$, $$\\|S\\|_F \\leq 3$$ and $$\\langle S, W - B \\rangle \\geq \\gamma - 1 - \\delta$$ for any $$B^\\wedge$$ such that $$B^\\wedge \\in \\mathcal{Z}^\\wedge$$.\n\nTo sum up, $$\\text{SEP}(W; \\delta, q)$$ has two possible outcomes: with probability $$1 - q$$, either it certifies that $$W \\in \\mathcal{Z}^\\wedge$$, or it produces a scaled version of $$W$$ that belongs to $$\\mathcal{Z}^\\wedge$$ and an approximate separation hyperplane between $$W$$ and the set $$\\mathcal{Z}^\\wedge$$. As we show in Appendix E.2, implementing this oracle requires computing the two extreme eigenvectors and eigenvalues of the matrix $$W$$ inexactly, which can be implemented efficiently by the randomized Lanczos method [KW92].\n\nBuilding on the $$\\text{SEP}(W; \\delta, q)$$ oracle, we design a projection-free online learning algorithm adapted from [JJM23, Subroutine 2]. Since the algorithm is similar to the one proposed in [JJM23], we relegate the details to Appendix C but sketch the main steps in the analysis. To upper bound the cumulative loss $$\\sum^{N-1}_{k=0} \\ell_k(B_k)$$, we compare the performance of our online learning algorithm against a sequence of reference matrices {$$H_k$$}$$^{N-1}_{k=0}$$. Specifically, we aim to control the dynamic regret [Zin03]."}]}, {"page": 10, "text": "MSJR16] defined by D-Reg                   N({Hk}N\u22121    k=0 ) \u225c      N\u22121k=0 (\u2113k(Bk) \u2212          \u2113k(Hk)), as well as the the cumulative\nloss  N\u22121  k=0 \u2113k(Hk) by the reference sequence. In particular, in our analysis we show that the choice\nof Hk \u225c       \u22072f(yk) for k = 0, . . . , N \u2212               1 allows us to upper bound both quantities.\nRemark 3.1. While our online learning algorithm is similar to the one in [JJM23], our analysis is\nmore challenging due to the lack of strong convexity. Specifically, since f is assumed to be strongly\nconvex in [JJM23], the iterates converge to x\u2217                              at least linearly, resulting in less variation in the\nloss functions {\u2113k}. Hence, the authors in [JJM23] let Hk = H\u2217                                           \u225c   \u22072f(x\u2217) for all k and proved\nthat  N\u22121   k=0 \u2113k(H\u2217) remains bounded. In contrast, without linear convergence, we need to use a\ntime-varying sequence {Hk} to control the cumulative loss. This in turn requires us to bound the\nvariation  N\u22122      k=0 \u2225Hk+1 \u2212           Hk\u2225F , which involves a careful analysis of the stability property of the\nsequence {yk} in Algorithm 1.\n4       Complexity Analysis of A-QNPE\nIn this section, we present our main theoretical results: we establish the convergence rate of A-QNPE\n(Theorem 4.1) and characterize its computational cost in terms of gradient queries and matrix-vector\nproduct evaluations (Theorem 4.2). The proofs are provided in Appendices D and E.3.\nTheorem 4.1. Let {xk} be the iterates generated by Algorithm 1 using the line search scheme\nin Section 3.1, where \u03b11, \u03b12 \u2208                      (0, 1) with \u03b11 + \u03b12 < 1 and \u03b2 \u2208                         (0, 1), and using the Hessian\napproximation update in Section 3.2 (the hyperparameters are given in Appendix D). Then with\nprobability at least 1 \u2212               p, the following statements hold, where Ci (i = 4, . . . , 10) are absolute\nconstants only depending on \u03b11, \u03b12 and \u03b2.\n(a) For any k \u2265           0, we have     f(xk) \u2212      f(x\u2217) \u2264       C4L1\u2225z0 \u2212  k2     x\u2217\u22252    + C5\u2225z0 \u2212 \u03c3 0k2.5 x\u2217\u22252   .                             (17)\n(b) Furthermore, for any k \u2265                   0,                                                           max{ L1  \u03b12\u03b2   , 1            1   2\n          f(xk) \u2212      f(x\u2217) \u2264       \u2225z0 \u2212 k2.5x\u2217\u22252       M + C10L1L2d\u2225z0 \u2212                x\u2217\u2225    log+                \u221a  M   \u03c30 }k2.5           ,    (18)\n       where we define log+(x) \u225c                  max{log(x), 0} and the quantity M is given by\n              M \u225c      C6   + C7L2    1 + C8\u2225B0 \u2212          \u22072f(z0)\u22252      F + C9L2     2\u2225z0 \u2212      x\u2217\u22252 + C10L1L2d\u2225z0 \u2212                x\u2217\u2225.          (19)\n                       \u03c320\nBoth results in Theorem 4.1 are global, as they are valid for any initial points x0, z0 and any initial\nmatrix B0. Specifically, Part (a) of Theorem 4.1 shows that A-QNPE converges at a rate of O(1/k2),\nmatching the rate of NAG [Nes83] that is known to be optimal in the regime where k = O(d) [NY83;\nNes18]. Furthermore, Part (b) of Theorem 4.1 presents a convergence rate of O(                                                        d log(k)/k2.5).\nTo see this, note that since we have 0 \u2aaf                            B0 \u2aaf       L1I and 0 \u2aaf          \u22072f(z0) \u2aaf          L1I, in the worst case\n\u2225B0 \u2212      \u22072f(z0)\u22252      F in the expression of (19) can be upper bounded by L2                                    1d. Thus, assuming that\nL1, L2 and \u2225z0 \u2212             x\u2217\u2225    are on the order of O(1), we have M = O(d) and the convergence rate in\n(18) can be simplified to O(                     d log(k)/k2.5). Notably, this rate surpasses the O(1/k2) rate when\nk = \u2126(d log d). To the best of our knowledge, this is the first work to show a convergence rate faster\nthan O(1/k2) for a quasi-Newton-type method in the convex setting, thus establishing a provable\nadvantage over NAG.\n                                                                            10", "md": "MSJR16] defined by D-Reg N({Hk}N\u22121 k=0 ) \u225c N\u22121k=0 (\u2113k(Bk) \u2212 \u2113k(Hk)), as well as the the cumulative loss N\u22121 k=0 \u2113k(Hk) by the reference sequence. In particular, in our analysis we show that the choice of Hk \u225c \u22072f(yk) for k = 0, . . . , N \u2212 1 allows us to upper bound both quantities.\n\nRemark 3.1. While our online learning algorithm is similar to the one in [JJM23], our analysis is more challenging due to the lack of strong convexity. Specifically, since f is assumed to be strongly convex in [JJM23], the iterates converge to x* at least linearly, resulting in less variation in the loss functions {\u2113k}. Hence, the authors in [JJM23] let Hk = H* \u225c \u22072f(x*) for all k and proved that N\u22121 k=0 \u2113k(H*) remains bounded. In contrast, without linear convergence, we need to use a time-varying sequence {Hk} to control the cumulative loss. This in turn requires us to bound the variation N\u22122 k=0 \u2225Hk+1 \u2212 Hk\u2225F, which involves a careful analysis of the stability property of the sequence {yk} in Algorithm 1.\n\n4 Complexity Analysis of A-QNPE\n\nIn this section, we present our main theoretical results: we establish the convergence rate of A-QNPE (Theorem 4.1) and characterize its computational cost in terms of gradient queries and matrix-vector product evaluations (Theorem 4.2). The proofs are provided in Appendices D and E.3.\n\nTheorem 4.1. Let {xk} be the iterates generated by Algorithm 1 using the line search scheme in Section 3.1, where \u03b11, \u03b12 \u2208 (0, 1) with \u03b11 + \u03b12 < 1 and \u03b2 \u2208 (0, 1), and using the Hessian approximation update in Section 3.2 (the hyperparameters are given in Appendix D). Then with probability at least 1 \u2212 p, the following statements hold, where Ci (i = 4, . . . , 10) are absolute constants only depending on \u03b11, \u03b12 and \u03b2.\n\n(a) For any k \u2265 0, we have f(xk) \u2212 f(x*) \u2264 C4L1\u2225z0 \u2212 k2 x*\u22252 + C5\u2225z0 \u2212 \u03c3 0k2.5 x*\u22252 . (17)\n\n(b) Furthermore, for any k \u2265 0, max{ L1 \u03b12\u03b2 , 1 1 2 f(xk) \u2212 f(x*) \u2264 \u2225z0 \u2212 k2.5x*\u22252 M + C10L1L2d\u2225z0 \u2212 x*\u2225 log+ \u221a M \u03c30 }k2.5 , (18) where we define log+(x) \u225c max{log(x), 0} and the quantity M is given by M \u225c C6 + C7L2 1 + C8\u2225B0 \u2212 \u22072f(z0)\u22252 F + C9L2 2\u2225z0 \u2212 x*\u22252 + C10L1L2d\u2225z0 \u2212 x*\u2225. (19) \u03c320\n\nBoth results in Theorem 4.1 are global, as they are valid for any initial points x0, z0 and any initial matrix B0. Specifically, Part (a) of Theorem 4.1 shows that A-QNPE converges at a rate of O(1/k2), matching the rate of NAG [Nes83] that is known to be optimal in the regime where k = O(d) [NY83; Nes18]. Furthermore, Part (b) of Theorem 4.1 presents a convergence rate of O( d log(k)/k2.5). To see this, note that since we have 0 \u2aaf B0 \u2aaf L1I and 0 \u2aaf \u22072f(z0) \u2aaf L1I, in the worst case \u2225B0 \u2212 \u22072f(z0)\u22252 F in the expression of (19) can be upper bounded by L2 1d. Thus, assuming that L1, L2 and \u2225z0 \u2212 x*\u2225 are on the order of O(1), we have M = O(d) and the convergence rate in (18) can be simplified to O( d log(k)/k2.5). Notably, this rate surpasses the O(1/k2) rate when k = \u2126(d log d). To the best of our knowledge, this is the first work to show a convergence rate faster than O(1/k2) for a quasi-Newton-type method in the convex setting, thus establishing a provable advantage over NAG.\n\n10", "images": [], "items": [{"type": "text", "value": "MSJR16] defined by D-Reg N({Hk}N\u22121 k=0 ) \u225c N\u22121k=0 (\u2113k(Bk) \u2212 \u2113k(Hk)), as well as the the cumulative loss N\u22121 k=0 \u2113k(Hk) by the reference sequence. In particular, in our analysis we show that the choice of Hk \u225c \u22072f(yk) for k = 0, . . . , N \u2212 1 allows us to upper bound both quantities.\n\nRemark 3.1. While our online learning algorithm is similar to the one in [JJM23], our analysis is more challenging due to the lack of strong convexity. Specifically, since f is assumed to be strongly convex in [JJM23], the iterates converge to x* at least linearly, resulting in less variation in the loss functions {\u2113k}. Hence, the authors in [JJM23] let Hk = H* \u225c \u22072f(x*) for all k and proved that N\u22121 k=0 \u2113k(H*) remains bounded. In contrast, without linear convergence, we need to use a time-varying sequence {Hk} to control the cumulative loss. This in turn requires us to bound the variation N\u22122 k=0 \u2225Hk+1 \u2212 Hk\u2225F, which involves a careful analysis of the stability property of the sequence {yk} in Algorithm 1.\n\n4 Complexity Analysis of A-QNPE\n\nIn this section, we present our main theoretical results: we establish the convergence rate of A-QNPE (Theorem 4.1) and characterize its computational cost in terms of gradient queries and matrix-vector product evaluations (Theorem 4.2). The proofs are provided in Appendices D and E.3.\n\nTheorem 4.1. Let {xk} be the iterates generated by Algorithm 1 using the line search scheme in Section 3.1, where \u03b11, \u03b12 \u2208 (0, 1) with \u03b11 + \u03b12 < 1 and \u03b2 \u2208 (0, 1), and using the Hessian approximation update in Section 3.2 (the hyperparameters are given in Appendix D). Then with probability at least 1 \u2212 p, the following statements hold, where Ci (i = 4, . . . , 10) are absolute constants only depending on \u03b11, \u03b12 and \u03b2.\n\n(a) For any k \u2265 0, we have f(xk) \u2212 f(x*) \u2264 C4L1\u2225z0 \u2212 k2 x*\u22252 + C5\u2225z0 \u2212 \u03c3 0k2.5 x*\u22252 . (17)\n\n(b) Furthermore, for any k \u2265 0, max{ L1 \u03b12\u03b2 , 1 1 2 f(xk) \u2212 f(x*) \u2264 \u2225z0 \u2212 k2.5x*\u22252 M + C10L1L2d\u2225z0 \u2212 x*\u2225 log+ \u221a M \u03c30 }k2.5 , (18) where we define log+(x) \u225c max{log(x), 0} and the quantity M is given by M \u225c C6 + C7L2 1 + C8\u2225B0 \u2212 \u22072f(z0)\u22252 F + C9L2 2\u2225z0 \u2212 x*\u22252 + C10L1L2d\u2225z0 \u2212 x*\u2225. (19) \u03c320\n\nBoth results in Theorem 4.1 are global, as they are valid for any initial points x0, z0 and any initial matrix B0. Specifically, Part (a) of Theorem 4.1 shows that A-QNPE converges at a rate of O(1/k2), matching the rate of NAG [Nes83] that is known to be optimal in the regime where k = O(d) [NY83; Nes18]. Furthermore, Part (b) of Theorem 4.1 presents a convergence rate of O( d log(k)/k2.5). To see this, note that since we have 0 \u2aaf B0 \u2aaf L1I and 0 \u2aaf \u22072f(z0) \u2aaf L1I, in the worst case \u2225B0 \u2212 \u22072f(z0)\u22252 F in the expression of (19) can be upper bounded by L2 1d. Thus, assuming that L1, L2 and \u2225z0 \u2212 x*\u2225 are on the order of O(1), we have M = O(d) and the convergence rate in (18) can be simplified to O( d log(k)/k2.5). Notably, this rate surpasses the O(1/k2) rate when k = \u2126(d log d). To the best of our knowledge, this is the first work to show a convergence rate faster than O(1/k2) for a quasi-Newton-type method in the convex setting, thus establishing a provable advantage over NAG.\n\n10", "md": "MSJR16] defined by D-Reg N({Hk}N\u22121 k=0 ) \u225c N\u22121k=0 (\u2113k(Bk) \u2212 \u2113k(Hk)), as well as the the cumulative loss N\u22121 k=0 \u2113k(Hk) by the reference sequence. In particular, in our analysis we show that the choice of Hk \u225c \u22072f(yk) for k = 0, . . . , N \u2212 1 allows us to upper bound both quantities.\n\nRemark 3.1. While our online learning algorithm is similar to the one in [JJM23], our analysis is more challenging due to the lack of strong convexity. Specifically, since f is assumed to be strongly convex in [JJM23], the iterates converge to x* at least linearly, resulting in less variation in the loss functions {\u2113k}. Hence, the authors in [JJM23] let Hk = H* \u225c \u22072f(x*) for all k and proved that N\u22121 k=0 \u2113k(H*) remains bounded. In contrast, without linear convergence, we need to use a time-varying sequence {Hk} to control the cumulative loss. This in turn requires us to bound the variation N\u22122 k=0 \u2225Hk+1 \u2212 Hk\u2225F, which involves a careful analysis of the stability property of the sequence {yk} in Algorithm 1.\n\n4 Complexity Analysis of A-QNPE\n\nIn this section, we present our main theoretical results: we establish the convergence rate of A-QNPE (Theorem 4.1) and characterize its computational cost in terms of gradient queries and matrix-vector product evaluations (Theorem 4.2). The proofs are provided in Appendices D and E.3.\n\nTheorem 4.1. Let {xk} be the iterates generated by Algorithm 1 using the line search scheme in Section 3.1, where \u03b11, \u03b12 \u2208 (0, 1) with \u03b11 + \u03b12 < 1 and \u03b2 \u2208 (0, 1), and using the Hessian approximation update in Section 3.2 (the hyperparameters are given in Appendix D). Then with probability at least 1 \u2212 p, the following statements hold, where Ci (i = 4, . . . , 10) are absolute constants only depending on \u03b11, \u03b12 and \u03b2.\n\n(a) For any k \u2265 0, we have f(xk) \u2212 f(x*) \u2264 C4L1\u2225z0 \u2212 k2 x*\u22252 + C5\u2225z0 \u2212 \u03c3 0k2.5 x*\u22252 . (17)\n\n(b) Furthermore, for any k \u2265 0, max{ L1 \u03b12\u03b2 , 1 1 2 f(xk) \u2212 f(x*) \u2264 \u2225z0 \u2212 k2.5x*\u22252 M + C10L1L2d\u2225z0 \u2212 x*\u2225 log+ \u221a M \u03c30 }k2.5 , (18) where we define log+(x) \u225c max{log(x), 0} and the quantity M is given by M \u225c C6 + C7L2 1 + C8\u2225B0 \u2212 \u22072f(z0)\u22252 F + C9L2 2\u2225z0 \u2212 x*\u22252 + C10L1L2d\u2225z0 \u2212 x*\u2225. (19) \u03c320\n\nBoth results in Theorem 4.1 are global, as they are valid for any initial points x0, z0 and any initial matrix B0. Specifically, Part (a) of Theorem 4.1 shows that A-QNPE converges at a rate of O(1/k2), matching the rate of NAG [Nes83] that is known to be optimal in the regime where k = O(d) [NY83; Nes18]. Furthermore, Part (b) of Theorem 4.1 presents a convergence rate of O( d log(k)/k2.5). To see this, note that since we have 0 \u2aaf B0 \u2aaf L1I and 0 \u2aaf \u22072f(z0) \u2aaf L1I, in the worst case \u2225B0 \u2212 \u22072f(z0)\u22252 F in the expression of (19) can be upper bounded by L2 1d. Thus, assuming that L1, L2 and \u2225z0 \u2212 x*\u2225 are on the order of O(1), we have M = O(d) and the convergence rate in (18) can be simplified to O( d log(k)/k2.5). Notably, this rate surpasses the O(1/k2) rate when k = \u2126(d log d). To the best of our knowledge, this is the first work to show a convergence rate faster than O(1/k2) for a quasi-Newton-type method in the convex setting, thus establishing a provable advantage over NAG.\n\n10"}]}, {"page": 11, "text": "Remark 4.1 (Iteration complexity). Based on Theorem 4.1, we can find A-QNPE\u2019s iteration\ncomplexity. Define N\u03f5 as the number of iterations required by1A-QNPE to find an \u03f5-accurate solution,\ni.e., f(x) \u2212       f(x\u2217) \u2264       \u03f5. When1\u221a\u03f5 >             d log d, the rate in (17) is better and we have N\u03f5 = O( 1                           \u03f50.5 ).\nConversely, when \u221a\u03f5 <                d log d, the rate in (18) is the better one, resulting in N\u03f5 = O(( d                         \u03f52 log d \u03f52 )0.2).\nHence, to achieve an \u03f5-accurate solution A-QNPE requires O(min{ 1                                     \u03f50.5 , d0.2\n                                                                                                             \u03f50.4 (log d\u03f52 )0.2}) iterations.\nRemark 4.2 (Special case). If the initial point z0 is close to an optimal solution x\u2217                                           and the initial\nHessian approximation matrix B0 is chosen properly, the dependence on d in the convergence rate\nof (18) can be eliminated. Specifically, if \u2225z0 \u2212                      x\u2217\u2225   = O( 1  d) and we set B0 = \u22072f(z0), then we have\nM = O(1) and this leads to a local dimension-independent rate of O(\u221alog k/k2.5).\nRecall that in each iteration of Algorithm 1, we need to execute a line search subroutine (Section 3.1)\nand a Hessian approximation update subroutine (Section 3.2). Thus, to fully characterize the\ncomputational cost of Algorithm 1, we need to upper bound the total number of gradient queries as\nwell as the total number of matrix-vector product evaluations, which is the goal of Theorem 4.2.\nTheorem 4.2. Recall that N\u03f5 denotes the minimum number of iterations required by Algorithm 1\nto find an \u03f5-accurate solution according to Theorem 4.1. Then, with probability at least 1 \u2212                                               p:\n   (a) The total number of gradient queries is bounded by 3N\u03f5 + log                                  1/\u03b2( \u03c30L1\n                                                                                                             \u03b12 ).\n   (b) The total number of matrix-vector product evaluations in the LinearSolver oracle is bounded by\n         N\u03f5 + C11      \u221a \u03c30L1 + C12           L1\u2225z0\u2212x\u2217\u22252      , where C11 and C12 are absolute constants.\n                                                    2\u03f5\n   (c) The total number of matrix-vector product evaluations in the SEP oracle is bounded by\n         O   N1.25(log N\u03f5)0.5 log          \u221a  dN\u03f5     .\n                \u03f5                              p\nIf the initial step size is chosen as \u03c30 = \u03b12               L1 , Theorem 4.2(a) implies that A-QNPE requires no more\nthan 3 gradient queries per iteration on average. Thus, the gradient oracle complexity of A-QNPE\nis the same as the iteration complexity, i.e., O(min{ 1                          \u03f50.5 , d0.2\n                                                                                        \u03f50.4 (log d\u03f52 )0.2}). On the other hand, the\ncomplexity in terms of matrix-vector products is worse. More precisely, by using the expression\nof N\u03f5 in Remark 4.1, Parts (b) and (c) imply that the total number of matrix-vector product\nevaluations in the LinearSolver and SEP oracles can be bounded by O( 1                                   \u03f50.5 ) and \u02dc O(min{d0.25  \u03f50.5 ,    1\nrespectively. While this is a limitation of our method, in some cases, gradient evaluations are the                                       \u03f50.625 }),\nmain bottleneck and can be more expensive than matrix-vector products. For instance, in large-scale\nempirical risk minimization problems where we minimize the sum of n functions (n is the number\nof samples and is very large), each gradient computation requires computing n gradients. In such\ncases, A-QNPE has a lower overall computational cost compared to NAG.\n5       Experiments\nIn this section, we compare the numerical performance of our proposed A-QNPE method with NAG\nand the classical BFGS quasi-Newton method. For fair comparison, we also use a line search scheme\nin NAG and BFGS to obtain their best performance [Bec17; NW06]. We would like to highlight that\nour paper mainly focuses on establishing a provable gain for quasi-Newton methods with respect\nto NAG, and our experimental results are presented to numerically verify our theoretical findings.\nWe focus on a logistic regression problem with the loss function f(x) = 111                                   n   n j=1 log(1 + e\u2212yj\u27e8ai,x\u27e9),", "md": "## Remark 4.1 (Iteration complexity)\n\nBased on Theorem 4.1, we can find A-QNPE\u2019s iteration complexity. Define N\u03f5 as the number of iterations required by A-QNPE to find an \u03f5-accurate solution, i.e., $$f(x) - f(x^*) \\leq \\epsilon$$. When $$\\sqrt{\\epsilon} > d \\log d$$, the rate in (17) is better and we have $$N_\\epsilon = O\\left(\\frac{1}{\\epsilon^{0.5}}\\right)$$. Conversely, when $$\\sqrt{\\epsilon} < d \\log d$$, the rate in (18) is the better one, resulting in $$N_\\epsilon = O\\left(\\left(\\frac{d}{\\epsilon^2 \\log d \\epsilon^2}\\right)^{0.2}\\right)$$. Hence, to achieve an $$\\epsilon$$-accurate solution, A-QNPE requires $$O\\left(\\min\\left\\{\\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} (\\log d \\epsilon^2)^{0.2}\\right\\}\\right)$$ iterations.\n\n## Remark 4.2 (Special case)\n\nIf the initial point $$z_0$$ is close to an optimal solution $$x^*$$ and the initial Hessian approximation matrix $$B_0$$ is chosen properly, the dependence on d in the convergence rate of (18) can be eliminated. Specifically, if $$\\|z_0 - x^*\\| = O\\left(\\frac{1}{d}\\right)$$ and we set $$B_0 = \\nabla^2 f(z_0)$$, then we have $$M = O(1)$$ and this leads to a local dimension-independent rate of $$O\\left(\\sqrt{\\frac{\\log k}{k^{2.5}}}\\right)$$.\n\nRecall that in each iteration of Algorithm 1, we need to execute a line search subroutine (Section 3.1) and a Hessian approximation update subroutine (Section 3.2). Thus, to fully characterize the computational cost of Algorithm 1, we need to upper bound the total number of gradient queries as well as the total number of matrix-vector product evaluations, which is the goal of Theorem 4.2.\n\n## Theorem 4.2\n\nRecall that $$N_\\epsilon$$ denotes the minimum number of iterations required by Algorithm 1 to find an $$\\epsilon$$-accurate solution according to Theorem 4.1. Then, with probability at least $$1 - p$$:\n\n1. The total number of gradient queries is bounded by $3N_\\epsilon + \\log \\frac{1}{\\beta}(\\sigma_0 L_1 \\alpha^2)$.\n2. The total number of matrix-vector product evaluations in the LinearSolver oracle is bounded by $N_\\epsilon + C_{11} \\sqrt{\\sigma_0 L_1} + C_{12} L_1 \\|z_0 - x^*\\|^2 / (2\\epsilon)$, where $C_{11}$ and $C_{12}$ are absolute constants.\n3. The total number of matrix-vector product evaluations in the SEP oracle is bounded by $O\\left(N^{1.25}(\\log N_\\epsilon)^{0.5} \\log \\sqrt{dN_\\epsilon} / \\epsilon\\right)$.\n\nIf the initial step size is chosen as $$\\sigma_0 = \\alpha^2 L_1$$, Theorem 4.2(a) implies that A-QNPE requires no more than 3 gradient queries per iteration on average. Thus, the gradient oracle complexity of A-QNPE is the same as the iteration complexity, i.e., $$O\\left(\\min\\left\\{\\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} (\\log d \\epsilon^2)^{0.2}\\right\\}\\right)$$. On the other hand, the complexity in terms of matrix-vector products is worse. More precisely, by using the expression of $$N_\\epsilon$$ in Remark 4.1, Parts (b) and (c) imply that the total number of matrix-vector product evaluations in the LinearSolver and SEP oracles can be bounded by $$O\\left(\\frac{1}{\\epsilon^{0.5}}\\right)$$ and $$\\tilde{O}\\left(\\min\\{d^{0.25} \\epsilon^{0.5}, \\frac{1}{\\epsilon^{0.625}}\\}\\right)$$ respectively.\n\nWhile this is a limitation of our method, in some cases, gradient evaluations are the main bottleneck and can be more expensive than matrix-vector products. For instance, in large-scale empirical risk minimization problems where we minimize the sum of n functions (n is the number of samples and is very large), each gradient computation requires computing n gradients. In such cases, A-QNPE has a lower overall computational cost compared to NAG.\n\n## 5 Experiments\n\nIn this section, we compare the numerical performance of our proposed A-QNPE method with NAG and the classical BFGS quasi-Newton method. For fair comparison, we also use a line search scheme in NAG and BFGS to obtain their best performance [Bec17; NW06]. We would like to highlight that our paper mainly focuses on establishing a provable gain for quasi-Newton methods with respect to NAG, and our experimental results are presented to numerically verify our theoretical findings.\n\nWe focus on a logistic regression problem with the loss function $$f(x) = \\frac{1}{n} \\sum_{j=1}^{n} \\log(1 + e^{-y_j \\langle a_i, x \\rangle})$$.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Remark 4.1 (Iteration complexity)", "md": "## Remark 4.1 (Iteration complexity)"}, {"type": "text", "value": "Based on Theorem 4.1, we can find A-QNPE\u2019s iteration complexity. Define N\u03f5 as the number of iterations required by A-QNPE to find an \u03f5-accurate solution, i.e., $$f(x) - f(x^*) \\leq \\epsilon$$. When $$\\sqrt{\\epsilon} > d \\log d$$, the rate in (17) is better and we have $$N_\\epsilon = O\\left(\\frac{1}{\\epsilon^{0.5}}\\right)$$. Conversely, when $$\\sqrt{\\epsilon} < d \\log d$$, the rate in (18) is the better one, resulting in $$N_\\epsilon = O\\left(\\left(\\frac{d}{\\epsilon^2 \\log d \\epsilon^2}\\right)^{0.2}\\right)$$. Hence, to achieve an $$\\epsilon$$-accurate solution, A-QNPE requires $$O\\left(\\min\\left\\{\\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} (\\log d \\epsilon^2)^{0.2}\\right\\}\\right)$$ iterations.", "md": "Based on Theorem 4.1, we can find A-QNPE\u2019s iteration complexity. Define N\u03f5 as the number of iterations required by A-QNPE to find an \u03f5-accurate solution, i.e., $$f(x) - f(x^*) \\leq \\epsilon$$. When $$\\sqrt{\\epsilon} > d \\log d$$, the rate in (17) is better and we have $$N_\\epsilon = O\\left(\\frac{1}{\\epsilon^{0.5}}\\right)$$. Conversely, when $$\\sqrt{\\epsilon} < d \\log d$$, the rate in (18) is the better one, resulting in $$N_\\epsilon = O\\left(\\left(\\frac{d}{\\epsilon^2 \\log d \\epsilon^2}\\right)^{0.2}\\right)$$. Hence, to achieve an $$\\epsilon$$-accurate solution, A-QNPE requires $$O\\left(\\min\\left\\{\\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} (\\log d \\epsilon^2)^{0.2}\\right\\}\\right)$$ iterations."}, {"type": "heading", "lvl": 2, "value": "Remark 4.2 (Special case)", "md": "## Remark 4.2 (Special case)"}, {"type": "text", "value": "If the initial point $$z_0$$ is close to an optimal solution $$x^*$$ and the initial Hessian approximation matrix $$B_0$$ is chosen properly, the dependence on d in the convergence rate of (18) can be eliminated. Specifically, if $$\\|z_0 - x^*\\| = O\\left(\\frac{1}{d}\\right)$$ and we set $$B_0 = \\nabla^2 f(z_0)$$, then we have $$M = O(1)$$ and this leads to a local dimension-independent rate of $$O\\left(\\sqrt{\\frac{\\log k}{k^{2.5}}}\\right)$$.\n\nRecall that in each iteration of Algorithm 1, we need to execute a line search subroutine (Section 3.1) and a Hessian approximation update subroutine (Section 3.2). Thus, to fully characterize the computational cost of Algorithm 1, we need to upper bound the total number of gradient queries as well as the total number of matrix-vector product evaluations, which is the goal of Theorem 4.2.", "md": "If the initial point $$z_0$$ is close to an optimal solution $$x^*$$ and the initial Hessian approximation matrix $$B_0$$ is chosen properly, the dependence on d in the convergence rate of (18) can be eliminated. Specifically, if $$\\|z_0 - x^*\\| = O\\left(\\frac{1}{d}\\right)$$ and we set $$B_0 = \\nabla^2 f(z_0)$$, then we have $$M = O(1)$$ and this leads to a local dimension-independent rate of $$O\\left(\\sqrt{\\frac{\\log k}{k^{2.5}}}\\right)$$.\n\nRecall that in each iteration of Algorithm 1, we need to execute a line search subroutine (Section 3.1) and a Hessian approximation update subroutine (Section 3.2). Thus, to fully characterize the computational cost of Algorithm 1, we need to upper bound the total number of gradient queries as well as the total number of matrix-vector product evaluations, which is the goal of Theorem 4.2."}, {"type": "heading", "lvl": 2, "value": "Theorem 4.2", "md": "## Theorem 4.2"}, {"type": "text", "value": "Recall that $$N_\\epsilon$$ denotes the minimum number of iterations required by Algorithm 1 to find an $$\\epsilon$$-accurate solution according to Theorem 4.1. Then, with probability at least $$1 - p$$:\n\n1. The total number of gradient queries is bounded by $3N_\\epsilon + \\log \\frac{1}{\\beta}(\\sigma_0 L_1 \\alpha^2)$.\n2. The total number of matrix-vector product evaluations in the LinearSolver oracle is bounded by $N_\\epsilon + C_{11} \\sqrt{\\sigma_0 L_1} + C_{12} L_1 \\|z_0 - x^*\\|^2 / (2\\epsilon)$, where $C_{11}$ and $C_{12}$ are absolute constants.\n3. The total number of matrix-vector product evaluations in the SEP oracle is bounded by $O\\left(N^{1.25}(\\log N_\\epsilon)^{0.5} \\log \\sqrt{dN_\\epsilon} / \\epsilon\\right)$.\n\nIf the initial step size is chosen as $$\\sigma_0 = \\alpha^2 L_1$$, Theorem 4.2(a) implies that A-QNPE requires no more than 3 gradient queries per iteration on average. Thus, the gradient oracle complexity of A-QNPE is the same as the iteration complexity, i.e., $$O\\left(\\min\\left\\{\\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} (\\log d \\epsilon^2)^{0.2}\\right\\}\\right)$$. On the other hand, the complexity in terms of matrix-vector products is worse. More precisely, by using the expression of $$N_\\epsilon$$ in Remark 4.1, Parts (b) and (c) imply that the total number of matrix-vector product evaluations in the LinearSolver and SEP oracles can be bounded by $$O\\left(\\frac{1}{\\epsilon^{0.5}}\\right)$$ and $$\\tilde{O}\\left(\\min\\{d^{0.25} \\epsilon^{0.5}, \\frac{1}{\\epsilon^{0.625}}\\}\\right)$$ respectively.\n\nWhile this is a limitation of our method, in some cases, gradient evaluations are the main bottleneck and can be more expensive than matrix-vector products. For instance, in large-scale empirical risk minimization problems where we minimize the sum of n functions (n is the number of samples and is very large), each gradient computation requires computing n gradients. In such cases, A-QNPE has a lower overall computational cost compared to NAG.", "md": "Recall that $$N_\\epsilon$$ denotes the minimum number of iterations required by Algorithm 1 to find an $$\\epsilon$$-accurate solution according to Theorem 4.1. Then, with probability at least $$1 - p$$:\n\n1. The total number of gradient queries is bounded by $3N_\\epsilon + \\log \\frac{1}{\\beta}(\\sigma_0 L_1 \\alpha^2)$.\n2. The total number of matrix-vector product evaluations in the LinearSolver oracle is bounded by $N_\\epsilon + C_{11} \\sqrt{\\sigma_0 L_1} + C_{12} L_1 \\|z_0 - x^*\\|^2 / (2\\epsilon)$, where $C_{11}$ and $C_{12}$ are absolute constants.\n3. The total number of matrix-vector product evaluations in the SEP oracle is bounded by $O\\left(N^{1.25}(\\log N_\\epsilon)^{0.5} \\log \\sqrt{dN_\\epsilon} / \\epsilon\\right)$.\n\nIf the initial step size is chosen as $$\\sigma_0 = \\alpha^2 L_1$$, Theorem 4.2(a) implies that A-QNPE requires no more than 3 gradient queries per iteration on average. Thus, the gradient oracle complexity of A-QNPE is the same as the iteration complexity, i.e., $$O\\left(\\min\\left\\{\\frac{1}{\\epsilon^{0.5}}, d^{0.2} \\epsilon^{0.4} (\\log d \\epsilon^2)^{0.2}\\right\\}\\right)$$. On the other hand, the complexity in terms of matrix-vector products is worse. More precisely, by using the expression of $$N_\\epsilon$$ in Remark 4.1, Parts (b) and (c) imply that the total number of matrix-vector product evaluations in the LinearSolver and SEP oracles can be bounded by $$O\\left(\\frac{1}{\\epsilon^{0.5}}\\right)$$ and $$\\tilde{O}\\left(\\min\\{d^{0.25} \\epsilon^{0.5}, \\frac{1}{\\epsilon^{0.625}}\\}\\right)$$ respectively.\n\nWhile this is a limitation of our method, in some cases, gradient evaluations are the main bottleneck and can be more expensive than matrix-vector products. For instance, in large-scale empirical risk minimization problems where we minimize the sum of n functions (n is the number of samples and is very large), each gradient computation requires computing n gradients. In such cases, A-QNPE has a lower overall computational cost compared to NAG."}, {"type": "heading", "lvl": 2, "value": "5 Experiments", "md": "## 5 Experiments"}, {"type": "text", "value": "In this section, we compare the numerical performance of our proposed A-QNPE method with NAG and the classical BFGS quasi-Newton method. For fair comparison, we also use a line search scheme in NAG and BFGS to obtain their best performance [Bec17; NW06]. We would like to highlight that our paper mainly focuses on establishing a provable gain for quasi-Newton methods with respect to NAG, and our experimental results are presented to numerically verify our theoretical findings.\n\nWe focus on a logistic regression problem with the loss function $$f(x) = \\frac{1}{n} \\sum_{j=1}^{n} \\log(1 + e^{-y_j \\langle a_i, x \\rangle})$$.", "md": "In this section, we compare the numerical performance of our proposed A-QNPE method with NAG and the classical BFGS quasi-Newton method. For fair comparison, we also use a line search scheme in NAG and BFGS to obtain their best performance [Bec17; NW06]. We would like to highlight that our paper mainly focuses on establishing a provable gain for quasi-Newton methods with respect to NAG, and our experimental results are presented to numerically verify our theoretical findings.\n\nWe focus on a logistic regression problem with the loss function $$f(x) = \\frac{1}{n} \\sum_{j=1}^{n} \\log(1 + e^{-y_j \\langle a_i, x \\rangle})$$."}]}, {"page": 12, "text": "     100                                                 100\n    10-10                                              10-10\n                                                            0          500         1000        1500\n         0          500         1000        1500\n      (a) Convergence by iteration                  (b) Convergence by gradient queries                  (c) Histogram of gradient queries\n                    Figure 1: Numerical results for logistic regression on a synthetic dataset.\n where a1, . . . , an \u2208       Rd are feature vectors and y1, . . . , yn \u2208                   {\u22121, 1} are binary labels. We perform\n our numerical experiments on a synthetic dataset and the data generation process is described in\n Appendix F. As we observe in Fig. 1(a), our proposed A-QNPE method converges in much fewer\n iterations than NAG, while the best performance is achieved by BFGS. Due to the use of line search,\n we also compare these algorithms in terms of the total number of gradient queries. As illustrated in\n Fig. 1(b), A-QNPE still outperforms NAG but the relative gain becomes less substantial. This is\n because the line search scheme in NAG only queries the function value at the new point, and thus\n it only requires one gradient per iteration. On the other hand, we should add that the number of\n gradient queries per iteration for A-QNPE is still small as guaranteed by our theory. In particular,\n the histogram of gradient queries in Fig. 1(c) shows that most of the iterations of A-QNPE require\n 2-3 gradient queries with an average of less than 3. Finally, although there is no theoretical guarantee\n showing a convergence gain for BFGS with respect to NAG, we observe that BFGS outperforms\n all the other considered methods in our experiments. Hence, studying the convergence behavior of\n BFGS (with line search) in the convex setting is an interesting research direction to explore.\n 6      Conclusions\nWe proposed a quasi-Newton variant of the accelerated proximal extragradient method for solving\n smooth convex optimization problems. We established two global convergence rates for our A-QNPE\n method, showing that it requires \u02dc                 O(min{ 1    \u03f50.5 , d0.2\n                                                                       \u03f50.4 }) gradient queries to find an \u03f5-accurate solution.\n In particular, in the regime where \u03f5 = \u2126( 1                      d2 ), A-QNPE achieves a gradient oracle complexity of\n O( 1\u03f50.5 ), matching the complexity of NAG. Moreover, in the regime where \u03f5 = \u02dc                                      O( 1 d2 ), it outperforms\n NAG and improves the complexity to \u02dc                      O( d0.2\n                                                                \u03f50.4 ). To the best of our knowledge, this is the first result\n showing a provable gain for a quasi-Newton-type method over NAG in the convex setting.\n Acknowledgements\n The research of R. Jiang and A. Mokhtari is supported in part by NSF Grants 2007668, 2019844,\n and 2112471, ARO Grant W911NF2110226, the Machine Learning Lab (MLL) at UT Austin, the\nWireless Networking and Communications Group (WNCG) Industrial Affiliates Program, and the\n NSF AI Institute for Foundations of Machine Learning (IFML).\n                                                                          12", "md": "# Numerical Results for Logistic Regression\n\n## Numerical results for logistic regression on a synthetic dataset\n\nWhere \\(a_1, ..., a_n \\in \\mathbb{R}^d\\) are feature vectors and \\(y_1, ..., y_n \\in \\{-1, 1\\}\\) are binary labels. We perform our numerical experiments on a synthetic dataset and the data generation process is described in Appendix F.\n\n### Figure 1: Numerical results for logistic regression on a synthetic dataset\n\n| |0|500|1000|1500|\n|---|---|---|---|---|\n|0|(a) Convergence by iteration|(b) Convergence by gradient queries| |(c) Histogram of gradient queries|\n\nAs we observe in Fig. 1(a), our proposed A-QNPE method converges in much fewer iterations than NAG, while the best performance is achieved by BFGS. Due to the use of line search, we also compare these algorithms in terms of the total number of gradient queries. As illustrated in Fig. 1(b), A-QNPE still outperforms NAG but the relative gain becomes less substantial. This is because the line search scheme in NAG only queries the function value at the new point, and thus it only requires one gradient per iteration. On the other hand, we should add that the number of gradient queries per iteration for A-QNPE is still small as guaranteed by our theory. In particular, the histogram of gradient queries in Fig. 1(c) shows that most of the iterations of A-QNPE require 2-3 gradient queries with an average of less than 3.\n\nFinally, although there is no theoretical guarantee showing a convergence gain for BFGS with respect to NAG, we observe that BFGS outperforms all the other considered methods in our experiments. Hence, studying the convergence behavior of BFGS (with line search) in the convex setting is an interesting research direction to explore.\n\n### Conclusions\n\nWe proposed a quasi-Newton variant of the accelerated proximal extragradient method for solving smooth convex optimization problems. We established two global convergence rates for our A-QNPE method, showing that it requires approximately \\(O(\\min\\{1, \\epsilon^{0.5}, d^{0.2}\\epsilon^{0.4}\\})\\) gradient queries to find an \\(\\epsilon\\)-accurate solution. In particular, in the regime where \\(\\epsilon = \\Omega(\\frac{1}{d^2})\\), A-QNPE achieves a gradient oracle complexity of \\(O(\\frac{1}{\\epsilon^{0.5}})\\), matching the complexity of NAG. Moreover, in the regime where \\(\\epsilon = \\tilde{O}(\\frac{1}{d^2})\\), it outperforms NAG and improves the complexity to \\(\\tilde{O}(d^{0.2}\\epsilon^{0.4})\\). To the best of our knowledge, this is the first result showing a provable gain for a quasi-Newton-type method over NAG in the convex setting.\n\n### Acknowledgements\n\nThe research of R. Jiang and A. Mokhtari is supported in part by NSF Grants 2007668, 2019844, and 2112471, ARO Grant W911NF2110226, the Machine Learning Lab (MLL) at UT Austin, the Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, and the NSF AI Institute for Foundations of Machine Learning (IFML).\n\n12", "images": [{"name": "page-12-1.jpg", "height": 28, "width": 36, "x": 426, "y": 75}, {"name": "page-12-0.jpg", "height": 28, "width": 36, "x": 390, "y": 75}, {"name": "page-12-2.jpg", "height": 28, "width": 71, "x": 462, "y": 75}], "items": [{"type": "heading", "lvl": 1, "value": "Numerical Results for Logistic Regression", "md": "# Numerical Results for Logistic Regression"}, {"type": "heading", "lvl": 2, "value": "Numerical results for logistic regression on a synthetic dataset", "md": "## Numerical results for logistic regression on a synthetic dataset"}, {"type": "text", "value": "Where \\(a_1, ..., a_n \\in \\mathbb{R}^d\\) are feature vectors and \\(y_1, ..., y_n \\in \\{-1, 1\\}\\) are binary labels. We perform our numerical experiments on a synthetic dataset and the data generation process is described in Appendix F.", "md": "Where \\(a_1, ..., a_n \\in \\mathbb{R}^d\\) are feature vectors and \\(y_1, ..., y_n \\in \\{-1, 1\\}\\) are binary labels. We perform our numerical experiments on a synthetic dataset and the data generation process is described in Appendix F."}, {"type": "heading", "lvl": 3, "value": "Figure 1: Numerical results for logistic regression on a synthetic dataset", "md": "### Figure 1: Numerical results for logistic regression on a synthetic dataset"}, {"type": "table", "rows": [["", "0", "500", "1000", "1500"], ["0", "(a) Convergence by iteration", "(b) Convergence by gradient queries", "", "(c) Histogram of gradient queries"]], "md": "| |0|500|1000|1500|\n|---|---|---|---|---|\n|0|(a) Convergence by iteration|(b) Convergence by gradient queries| |(c) Histogram of gradient queries|", "isPerfectTable": true, "csv": "\"\",\"0\",\"500\",\"1000\",\"1500\"\n\"0\",\"(a) Convergence by iteration\",\"(b) Convergence by gradient queries\",\"\",\"(c) Histogram of gradient queries\""}, {"type": "text", "value": "As we observe in Fig. 1(a), our proposed A-QNPE method converges in much fewer iterations than NAG, while the best performance is achieved by BFGS. Due to the use of line search, we also compare these algorithms in terms of the total number of gradient queries. As illustrated in Fig. 1(b), A-QNPE still outperforms NAG but the relative gain becomes less substantial. This is because the line search scheme in NAG only queries the function value at the new point, and thus it only requires one gradient per iteration. On the other hand, we should add that the number of gradient queries per iteration for A-QNPE is still small as guaranteed by our theory. In particular, the histogram of gradient queries in Fig. 1(c) shows that most of the iterations of A-QNPE require 2-3 gradient queries with an average of less than 3.\n\nFinally, although there is no theoretical guarantee showing a convergence gain for BFGS with respect to NAG, we observe that BFGS outperforms all the other considered methods in our experiments. Hence, studying the convergence behavior of BFGS (with line search) in the convex setting is an interesting research direction to explore.", "md": "As we observe in Fig. 1(a), our proposed A-QNPE method converges in much fewer iterations than NAG, while the best performance is achieved by BFGS. Due to the use of line search, we also compare these algorithms in terms of the total number of gradient queries. As illustrated in Fig. 1(b), A-QNPE still outperforms NAG but the relative gain becomes less substantial. This is because the line search scheme in NAG only queries the function value at the new point, and thus it only requires one gradient per iteration. On the other hand, we should add that the number of gradient queries per iteration for A-QNPE is still small as guaranteed by our theory. In particular, the histogram of gradient queries in Fig. 1(c) shows that most of the iterations of A-QNPE require 2-3 gradient queries with an average of less than 3.\n\nFinally, although there is no theoretical guarantee showing a convergence gain for BFGS with respect to NAG, we observe that BFGS outperforms all the other considered methods in our experiments. Hence, studying the convergence behavior of BFGS (with line search) in the convex setting is an interesting research direction to explore."}, {"type": "heading", "lvl": 3, "value": "Conclusions", "md": "### Conclusions"}, {"type": "text", "value": "We proposed a quasi-Newton variant of the accelerated proximal extragradient method for solving smooth convex optimization problems. We established two global convergence rates for our A-QNPE method, showing that it requires approximately \\(O(\\min\\{1, \\epsilon^{0.5}, d^{0.2}\\epsilon^{0.4}\\})\\) gradient queries to find an \\(\\epsilon\\)-accurate solution. In particular, in the regime where \\(\\epsilon = \\Omega(\\frac{1}{d^2})\\), A-QNPE achieves a gradient oracle complexity of \\(O(\\frac{1}{\\epsilon^{0.5}})\\), matching the complexity of NAG. Moreover, in the regime where \\(\\epsilon = \\tilde{O}(\\frac{1}{d^2})\\), it outperforms NAG and improves the complexity to \\(\\tilde{O}(d^{0.2}\\epsilon^{0.4})\\). To the best of our knowledge, this is the first result showing a provable gain for a quasi-Newton-type method over NAG in the convex setting.", "md": "We proposed a quasi-Newton variant of the accelerated proximal extragradient method for solving smooth convex optimization problems. We established two global convergence rates for our A-QNPE method, showing that it requires approximately \\(O(\\min\\{1, \\epsilon^{0.5}, d^{0.2}\\epsilon^{0.4}\\})\\) gradient queries to find an \\(\\epsilon\\)-accurate solution. In particular, in the regime where \\(\\epsilon = \\Omega(\\frac{1}{d^2})\\), A-QNPE achieves a gradient oracle complexity of \\(O(\\frac{1}{\\epsilon^{0.5}})\\), matching the complexity of NAG. Moreover, in the regime where \\(\\epsilon = \\tilde{O}(\\frac{1}{d^2})\\), it outperforms NAG and improves the complexity to \\(\\tilde{O}(d^{0.2}\\epsilon^{0.4})\\). To the best of our knowledge, this is the first result showing a provable gain for a quasi-Newton-type method over NAG in the convex setting."}, {"type": "heading", "lvl": 3, "value": "Acknowledgements", "md": "### Acknowledgements"}, {"type": "text", "value": "The research of R. Jiang and A. Mokhtari is supported in part by NSF Grants 2007668, 2019844, and 2112471, ARO Grant W911NF2110226, the Machine Learning Lab (MLL) at UT Austin, the Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, and the NSF AI Institute for Foundations of Machine Learning (IFML).\n\n12", "md": "The research of R. Jiang and A. Mokhtari is supported in part by NSF Grants 2007668, 2019844, and 2112471, ARO Grant W911NF2110226, the Machine Learning Lab (MLL) at UT Austin, the Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, and the NSF AI Institute for Foundations of Machine Learning (IFML).\n\n12"}]}, {"page": 13, "text": "References\n[Bec17]      Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied\n             Mathematics, 2017 (pages 11, 44).\n[Bro70]      Charles G Broyden. \u201cThe convergence of single-rank quasi-Newton methods\u201d. In:\n             Mathematics of Computation 24.110 (1970), pp. 365\u2013382 (page 2).\n[BDM73]      Charles George Broyden, John E Dennis Jr, and Jorge J Mor\u00b4e. \u201cOn the local and\n             superlinear convergence of quasi-Newton methods\u201d. In: IMA Journal of Applied\n             Mathematics 12.3 (1973), pp. 223\u2013245 (page 2).\n[BJLLS19]    S\u00b4\n              ebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. \u201cNear-\n             optimal method for highly smooth convex optimization\u201d. In: Conference on Learning\n            Theory. PMLR. 2019, pp. 492\u2013507 (page 4).\n[BKS96]      Richard H Byrd, Humaid Fayez Khalfan, and Robert B Schnabel. \u201cAnalysis of a\n             symmetric rank-one trust region method\u201d. In: SIAM Journal on Optimization 6.4\n             (1996), pp. 1025\u20131039 (page 2).\n[BNY87]      Richard H Byrd, Jorge Nocedal, and Ya-Xiang Yuan. \u201cGlobal convergence of a class\n             of quasi-Newton methods on convex problems\u201d. In: SIAM Journal on Numerical\n             Analysis 24.5 (1987), pp. 1171\u20131190 (page 2).\n[CDHS20]     Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. \u201cLower bounds\n             for finding stationary points I\u201d. In: Mathematical Programming 184.1-2 (2020),\n             pp. 71\u2013120 (page 2).\n[CHJJS22]    Yair Carmon, Danielle Hausler, Arun Jambulapati, Yujia Jin, and Aaron Sidford.\n            \u201cOptimal and Adaptive Monteiro-Svaiter Acceleration\u201d. In: Advances in Neural\n             Information Processing Systems. 2022 (pages 3, 5, 6, 17, 42).\n[CGT91]      Andrew R Conn, Nicholas IM Gould, and Ph L Toint. \u201cConvergence of quasi-\n             Newton matrices generated by the symmetric rank one update\u201d. In: Mathematical\n             programming 50.1 (1991), pp. 177\u2013195 (page 2).\n[dST21]      Alexandre d\u2019Aspremont, Damien Scieur, and Adrien Taylor. \u201cAcceleration methods\u201d.\n             In: Foundations and Trends\u00ae in Optimization 5.1-2 (2021), pp. 1\u2013245 (page 17).\n[Dav59]      W. C. Davidon. Variable metric method for minimization. Techinical Report ANL-\n             5990. Argonne, IL: Argonne National Laboratory, 1959 (page 2).\n[DM74]       John E Dennis and Jorge J Mor\u00b4e. \u201cA characterization of superlinear convergence\n             and its application to quasi-Newton methods\u201d. In: Mathematics of computation\n             28.126 (1974), pp. 549\u2013560 (page 2).\n[Dix72]      Laurence Charles Ward Dixon. \u201cVariable metric algorithms: necessary and suffi-\n             cient conditions for identical behavior of nonquadratic functions\u201d. In: Journal of\n             Optimization Theory and Applications 10.1 (1972), pp. 34\u201340 (page 2).\n[Fle70]      Roger Fletcher. \u201cA new approach to variable metric algorithms\u201d. In: The computer\n             journal 13.3 (1970), pp. 317\u2013322 (page 2).\n[FP63]       Roger Fletcher and Michael JD Powell. \u201cA rapidly convergent descent method for\n             minimization\u201d. In: The Computer Journal 6.2 (1963), pp. 163\u2013168 (page 2).\n                                              13", "md": "# References\n\n## References\n\n|[Bec17]|Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics, 2017 (pages 11, 44).|\n|---|---|\n|[Bro70]|Charles G Broyden. \"The convergence of single-rank quasi-Newton methods\". In: Mathematics of Computation 24.110 (1970), pp. 365\u2013382 (page 2).|\n|[BDM73]|Charles George Broyden, John E Dennis Jr, and Jorge J Mor\u00b4e. \"On the local and superlinear convergence of quasi-Newton methods\". In: IMA Journal of Applied Mathematics 12.3 (1973), pp. 223\u2013245 (page 2).|\n|[BJLLS19]|S\u00b4ebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. \"Near-optimal method for highly smooth convex optimization\". In: Conference on Learning Theory. PMLR. 2019, pp. 492\u2013507 (page 4).|\n|[BKS96]|Richard H Byrd, Humaid Fayez Khalfan, and Robert B Schnabel. \"Analysis of a symmetric rank-one trust region method\". In: SIAM Journal on Optimization 6.4 (1996), pp. 1025\u20131039 (page 2).|\n|[BNY87]|Richard H Byrd, Jorge Nocedal, and Ya-Xiang Yuan. \"Global convergence of a class of quasi-Newton methods on convex problems\". In: SIAM Journal on Numerical Analysis 24.5 (1987), pp. 1171\u20131190 (page 2).|\n|[CDHS20]|Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. \"Lower bounds for finding stationary points I\". In: Mathematical Programming 184.1-2 (2020), pp. 71\u2013120 (page 2).|\n|[CHJJS22]|Yair Carmon, Danielle Hausler, Arun Jambulapati, Yujia Jin, and Aaron Sidford. \"Optimal and Adaptive Monteiro-Svaiter Acceleration\". In: Advances in Neural Information Processing Systems. 2022 (pages 3, 5, 6, 17, 42).|\n|[CGT91]|Andrew R Conn, Nicholas IM Gould, and Ph L Toint. \"Convergence of quasi-Newton matrices generated by the symmetric rank one update\". In: Mathematical programming 50.1 (1991), pp. 177\u2013195 (page 2).|\n|[dST21]|Alexandre d\u2019Aspremont, Damien Scieur, and Adrien Taylor. \"Acceleration methods\". In: Foundations and Trends\u00ae in Optimization 5.1-2 (2021), pp. 1\u2013245 (page 17).|\n|[Dav59]|W. C. Davidon. Variable metric method for minimization. Techinical Report ANL-5990. Argonne, IL: Argonne National Laboratory, 1959 (page 2).|\n|[DM74]|John E Dennis and Jorge J Mor\u00b4e. \"A characterization of superlinear convergence and its application to quasi-Newton methods\". In: Mathematics of computation 28.126 (1974), pp. 549\u2013560 (page 2).|\n|[Dix72]|Laurence Charles Ward Dixon. \"Variable metric algorithms: necessary and sufficient conditions for identical behavior of nonquadratic functions\". In: Journal of Optimization Theory and Applications 10.1 (1972), pp. 34\u201340 (page 2).|\n|[Fle70]|Roger Fletcher. \"A new approach to variable metric algorithms\". In: The computer journal 13.3 (1970), pp. 317\u2013322 (page 2).|\n|[FP63]|Roger Fletcher and Michael JD Powell. \"A rapidly convergent descent method for minimization\". In: The Computer Journal 6.2 (1963), pp. 163\u2013168 (page 2).|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "table", "rows": [["[Bec17]", "Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics, 2017 (pages 11, 44)."], ["[Bro70]", "Charles G Broyden. \"The convergence of single-rank quasi-Newton methods\". In: Mathematics of Computation 24.110 (1970), pp. 365\u2013382 (page 2)."], ["[BDM73]", "Charles George Broyden, John E Dennis Jr, and Jorge J Mor\u00b4e. \"On the local and superlinear convergence of quasi-Newton methods\". In: IMA Journal of Applied Mathematics 12.3 (1973), pp. 223\u2013245 (page 2)."], ["[BJLLS19]", "S\u00b4ebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. \"Near-optimal method for highly smooth convex optimization\". In: Conference on Learning Theory. PMLR. 2019, pp. 492\u2013507 (page 4)."], ["[BKS96]", "Richard H Byrd, Humaid Fayez Khalfan, and Robert B Schnabel. \"Analysis of a symmetric rank-one trust region method\". In: SIAM Journal on Optimization 6.4 (1996), pp. 1025\u20131039 (page 2)."], ["[BNY87]", "Richard H Byrd, Jorge Nocedal, and Ya-Xiang Yuan. \"Global convergence of a class of quasi-Newton methods on convex problems\". In: SIAM Journal on Numerical Analysis 24.5 (1987), pp. 1171\u20131190 (page 2)."], ["[CDHS20]", "Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. \"Lower bounds for finding stationary points I\". In: Mathematical Programming 184.1-2 (2020), pp. 71\u2013120 (page 2)."], ["[CHJJS22]", "Yair Carmon, Danielle Hausler, Arun Jambulapati, Yujia Jin, and Aaron Sidford. \"Optimal and Adaptive Monteiro-Svaiter Acceleration\". In: Advances in Neural Information Processing Systems. 2022 (pages 3, 5, 6, 17, 42)."], ["[CGT91]", "Andrew R Conn, Nicholas IM Gould, and Ph L Toint. \"Convergence of quasi-Newton matrices generated by the symmetric rank one update\". In: Mathematical programming 50.1 (1991), pp. 177\u2013195 (page 2)."], ["[dST21]", "Alexandre d\u2019Aspremont, Damien Scieur, and Adrien Taylor. \"Acceleration methods\". In: Foundations and Trends\u00ae in Optimization 5.1-2 (2021), pp. 1\u2013245 (page 17)."], ["[Dav59]", "W. C. Davidon. Variable metric method for minimization. Techinical Report ANL-5990. Argonne, IL: Argonne National Laboratory, 1959 (page 2)."], ["[DM74]", "John E Dennis and Jorge J Mor\u00b4e. \"A characterization of superlinear convergence and its application to quasi-Newton methods\". In: Mathematics of computation 28.126 (1974), pp. 549\u2013560 (page 2)."], ["[Dix72]", "Laurence Charles Ward Dixon. \"Variable metric algorithms: necessary and sufficient conditions for identical behavior of nonquadratic functions\". In: Journal of Optimization Theory and Applications 10.1 (1972), pp. 34\u201340 (page 2)."], ["[Fle70]", "Roger Fletcher. \"A new approach to variable metric algorithms\". In: The computer journal 13.3 (1970), pp. 317\u2013322 (page 2)."], ["[FP63]", "Roger Fletcher and Michael JD Powell. \"A rapidly convergent descent method for minimization\". In: The Computer Journal 6.2 (1963), pp. 163\u2013168 (page 2)."]], "md": "|[Bec17]|Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics, 2017 (pages 11, 44).|\n|---|---|\n|[Bro70]|Charles G Broyden. \"The convergence of single-rank quasi-Newton methods\". In: Mathematics of Computation 24.110 (1970), pp. 365\u2013382 (page 2).|\n|[BDM73]|Charles George Broyden, John E Dennis Jr, and Jorge J Mor\u00b4e. \"On the local and superlinear convergence of quasi-Newton methods\". In: IMA Journal of Applied Mathematics 12.3 (1973), pp. 223\u2013245 (page 2).|\n|[BJLLS19]|S\u00b4ebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. \"Near-optimal method for highly smooth convex optimization\". In: Conference on Learning Theory. PMLR. 2019, pp. 492\u2013507 (page 4).|\n|[BKS96]|Richard H Byrd, Humaid Fayez Khalfan, and Robert B Schnabel. \"Analysis of a symmetric rank-one trust region method\". In: SIAM Journal on Optimization 6.4 (1996), pp. 1025\u20131039 (page 2).|\n|[BNY87]|Richard H Byrd, Jorge Nocedal, and Ya-Xiang Yuan. \"Global convergence of a class of quasi-Newton methods on convex problems\". In: SIAM Journal on Numerical Analysis 24.5 (1987), pp. 1171\u20131190 (page 2).|\n|[CDHS20]|Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. \"Lower bounds for finding stationary points I\". In: Mathematical Programming 184.1-2 (2020), pp. 71\u2013120 (page 2).|\n|[CHJJS22]|Yair Carmon, Danielle Hausler, Arun Jambulapati, Yujia Jin, and Aaron Sidford. \"Optimal and Adaptive Monteiro-Svaiter Acceleration\". In: Advances in Neural Information Processing Systems. 2022 (pages 3, 5, 6, 17, 42).|\n|[CGT91]|Andrew R Conn, Nicholas IM Gould, and Ph L Toint. \"Convergence of quasi-Newton matrices generated by the symmetric rank one update\". In: Mathematical programming 50.1 (1991), pp. 177\u2013195 (page 2).|\n|[dST21]|Alexandre d\u2019Aspremont, Damien Scieur, and Adrien Taylor. \"Acceleration methods\". In: Foundations and Trends\u00ae in Optimization 5.1-2 (2021), pp. 1\u2013245 (page 17).|\n|[Dav59]|W. C. Davidon. Variable metric method for minimization. Techinical Report ANL-5990. Argonne, IL: Argonne National Laboratory, 1959 (page 2).|\n|[DM74]|John E Dennis and Jorge J Mor\u00b4e. \"A characterization of superlinear convergence and its application to quasi-Newton methods\". In: Mathematics of computation 28.126 (1974), pp. 549\u2013560 (page 2).|\n|[Dix72]|Laurence Charles Ward Dixon. \"Variable metric algorithms: necessary and sufficient conditions for identical behavior of nonquadratic functions\". In: Journal of Optimization Theory and Applications 10.1 (1972), pp. 34\u201340 (page 2).|\n|[Fle70]|Roger Fletcher. \"A new approach to variable metric algorithms\". In: The computer journal 13.3 (1970), pp. 317\u2013322 (page 2).|\n|[FP63]|Roger Fletcher and Michael JD Powell. \"A rapidly convergent descent method for minimization\". In: The Computer Journal 6.2 (1963), pp. 163\u2013168 (page 2).|", "isPerfectTable": true, "csv": "\"[Bec17]\",\"Amir Beck. First-Order Methods in Optimization. Society for Industrial and Applied Mathematics, 2017 (pages 11, 44).\"\n\"[Bro70]\",\"Charles G Broyden. \"\"The convergence of single-rank quasi-Newton methods\"\". In: Mathematics of Computation 24.110 (1970), pp. 365\u2013382 (page 2).\"\n\"[BDM73]\",\"Charles George Broyden, John E Dennis Jr, and Jorge J Mor\u00b4e. \"\"On the local and superlinear convergence of quasi-Newton methods\"\". In: IMA Journal of Applied Mathematics 12.3 (1973), pp. 223\u2013245 (page 2).\"\n\"[BJLLS19]\",\"S\u00b4ebastien Bubeck, Qijia Jiang, Yin Tat Lee, Yuanzhi Li, and Aaron Sidford. \"\"Near-optimal method for highly smooth convex optimization\"\". In: Conference on Learning Theory. PMLR. 2019, pp. 492\u2013507 (page 4).\"\n\"[BKS96]\",\"Richard H Byrd, Humaid Fayez Khalfan, and Robert B Schnabel. \"\"Analysis of a symmetric rank-one trust region method\"\". In: SIAM Journal on Optimization 6.4 (1996), pp. 1025\u20131039 (page 2).\"\n\"[BNY87]\",\"Richard H Byrd, Jorge Nocedal, and Ya-Xiang Yuan. \"\"Global convergence of a class of quasi-Newton methods on convex problems\"\". In: SIAM Journal on Numerical Analysis 24.5 (1987), pp. 1171\u20131190 (page 2).\"\n\"[CDHS20]\",\"Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. \"\"Lower bounds for finding stationary points I\"\". In: Mathematical Programming 184.1-2 (2020), pp. 71\u2013120 (page 2).\"\n\"[CHJJS22]\",\"Yair Carmon, Danielle Hausler, Arun Jambulapati, Yujia Jin, and Aaron Sidford. \"\"Optimal and Adaptive Monteiro-Svaiter Acceleration\"\". In: Advances in Neural Information Processing Systems. 2022 (pages 3, 5, 6, 17, 42).\"\n\"[CGT91]\",\"Andrew R Conn, Nicholas IM Gould, and Ph L Toint. \"\"Convergence of quasi-Newton matrices generated by the symmetric rank one update\"\". In: Mathematical programming 50.1 (1991), pp. 177\u2013195 (page 2).\"\n\"[dST21]\",\"Alexandre d\u2019Aspremont, Damien Scieur, and Adrien Taylor. \"\"Acceleration methods\"\". In: Foundations and Trends\u00ae in Optimization 5.1-2 (2021), pp. 1\u2013245 (page 17).\"\n\"[Dav59]\",\"W. C. Davidon. Variable metric method for minimization. Techinical Report ANL-5990. Argonne, IL: Argonne National Laboratory, 1959 (page 2).\"\n\"[DM74]\",\"John E Dennis and Jorge J Mor\u00b4e. \"\"A characterization of superlinear convergence and its application to quasi-Newton methods\"\". In: Mathematics of computation 28.126 (1974), pp. 549\u2013560 (page 2).\"\n\"[Dix72]\",\"Laurence Charles Ward Dixon. \"\"Variable metric algorithms: necessary and sufficient conditions for identical behavior of nonquadratic functions\"\". In: Journal of Optimization Theory and Applications 10.1 (1972), pp. 34\u201340 (page 2).\"\n\"[Fle70]\",\"Roger Fletcher. \"\"A new approach to variable metric algorithms\"\". In: The computer journal 13.3 (1970), pp. 317\u2013322 (page 2).\"\n\"[FP63]\",\"Roger Fletcher and Michael JD Powell. \"\"A rapidly convergent descent method for minimization\"\". In: The Computer Journal 6.2 (1963), pp. 163\u2013168 (page 2).\""}]}, {"page": 14, "text": "[GDGVSU19]  Alexander Gasnikov, Pavel Dvurechensky, Eduard Gorbunov, Evgeniya Vorontsova,\n            Daniil Selikhanovych, and C\u00b4esar A Uribe. \u201cOptimal tensor methods in smooth\n            convex and uniformly convexoptimization\u201d. In: Conference on Learning Theory.\n            PMLR. 2019, pp. 1374\u20131391 (page 4).\n[GS18]      Hiva Ghanbari and Katya Scheinberg. \u201cProximal quasi-Newton methods for regular-\n            ized convex optimization with linear and accelerated sublinear convergence rates\u201d.\n            In: Computational Optimization and Applications 69 (2018), pp. 597\u2013627 (page 2).\n[Gol70]     Donald Goldfarb. \u201cA family of variable-metric methods derived by variational\n            means\u201d. In: Mathematics of computation 24.109 (1970), pp. 23\u201326 (page 2).\n[G\u00a8ul92]    Osman G\u00a8   uler. \u201cNew proximal point algorithms for convex minimization\u201d. In: SIAM\n            Journal on Optimization 2.4 (1992), pp. 649\u2013664 (page 3).\n[JWZ21]     Bo Jiang, Haoyue Wang, and Shuzhong Zhang. \u201cAn optimal high-order tensor\n            method for convex optimization\u201d. In: Mathematics of Operations Research 46.4\n            (2021), pp. 1390\u20131412 (page 4).\n[JJM23]     Ruichen Jiang, Qiujiang Jin, and Aryan Mokhtari. \u201cOnline Learning Guided Cur-\n            vature Approximation: A Quasi-Newton Method with Global Non-Asymptotic\n            Superlinear Convergence\u201d. In: arXiv preprint arXiv:2302.08580 (2023) (pages 3, 6,\n            9, 10, 27, 30, 34).\n[JKRM22]    Qiujiang Jin, Alec Koppel, Ketan Rajawat, and Aryan Mokhtari. \u201cSharpened Quasi-\n            Newton Methods: Faster Superlinear Rate and Larger Local Convergence Neighbor-\n            hood\u201d. In: Proceedings of the 39th International Conference on Machine Learning.\n            Vol. 162. Proceedings of Machine Learning Research. PMLR, 2022, pp. 10228\u201310250\n            (page 2).\n[JM22]      Qiujiang Jin and Aryan Mokhtari. \u201cNon-asymptotic superlinear convergence of\n            standard quasi-Newton methods\u201d. In: Mathematical Programming (2022) (page 2).\n[KZAT23]    Dmitry Kamzolov, Klea Ziu, Artem Agafonov, and Martin Tak\u00b4        a\u02c7\n                                                                               c. \u201cAccelerated Adap-\n            tive Cubic Regularized Quasi-Newton Methods\u201d. In: arXiv preprint arXiv:2302.04987\n            (2023) (page 2).\n[KBS93]     H Fayez Khalfan, Richard H Byrd, and Robert B Schnabel. \u201cA theoretical and\n            experimental study of the symmetric rank-one update\u201d. In: SIAM Journal on\n            Optimization 3.1 (1993), pp. 1\u201324 (page 2).\n[KW92]      J. Kuczy\u00b4 nski and H. Wo\u00b4zniakowski. \u201cEstimating the Largest Eigenvalue by the\n            Power and Lanczos Algorithms with a Random Start\u201d. In: SIAM Journal on Matrix\n            Analysis and Applications 13.4 (1992), pp. 1094\u20131122 (pages 9, 39).\n[LYZ21]     Dachao Lin, Haishan Ye, and Zhihua Zhang. \u201cGreedy and random quasi-newton\n            methods with faster explicit superlinear convergence\u201d. In: Advances in Neural\n            Information Processing Systems 34 (2021), pp. 6646\u20136657 (page 2).\n[Mha22]     Zakaria Mhammedi. \u201cEfficient Projection-Free Online Convex Optimization with\n            Membership Oracle\u201d. In: Proceedings of Thirty Fifth Conference on Learning Theory.\n            Vol. 178. Proceedings of Machine Learning Research. PMLR, 2022, pp. 5314\u20135390\n            (pages 9, 30).\n                                               14", "md": "# References\n\n## List of References\n\n|Reference|Authors|Title|Journal/Conference|Year|Pages|\n|---|---|---|---|---|---|\n|[GDGVSU19]|Alexander Gasnikov, Pavel Dvurechensky, Eduard Gorbunov, Evgeniya Vorontsova, Daniil Selikhanovych, and C\u00e9sar A Uribe|Optimal tensor methods in smooth convex and uniformly convex optimization|Conference on Learning Theory|2019|1374-1391 (page 4)|\n|[GS18]|Hiva Ghanbari and Katya Scheinberg|Proximal quasi-Newton methods for regularized convex optimization with linear and accelerated sublinear convergence rates|Computational Optimization and Applications|2018|597-627 (page 2)|\n|[Gol70]|Donald Goldfarb|A family of variable-metric methods derived by variational means|Mathematics of Computation|1970|23-26 (page 2)|\n|[G\u00fcler92]|Osman G\u00fcler|New proximal point algorithms for convex minimization|SIAM Journal on Optimization|1992|649-664 (page 3)|\n|[JWZ21]|Bo Jiang, Haoyue Wang, and Shuzhong Zhang|An optimal high-order tensor method for convex optimization|Mathematics of Operations Research|2021|1390-1412 (page 4)|\n|[JJM23]|Ruichen Jiang, Qiujiang Jin, and Aryan Mokhtari|Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence|arXiv preprint arXiv:2302.08580|2023|3, 6, 9, 10, 27, 30, 34|\n|[JKRM22]|Qiujiang Jin, Alec Koppel, Ketan Rajawat, and Aryan Mokhtari|Sharpened Quasi-Newton Methods: Faster Superlinear Rate and Larger Local Convergence Neighborhood|Proceedings of the 39th International Conference on Machine Learning|2022|10228-10250 (page 2)|\n|[JM22]|Qiujiang Jin and Aryan Mokhtari|Non-asymptotic superlinear convergence of standard quasi-Newton methods|Mathematical Programming|2022|page 2|\n|[KZAT23]|Dmitry Kamzolov, Klea Ziu, Artem Agafonov, and Martin Tak\u00e1\u010d|Accelerated Adaptive Cubic Regularized Quasi-Newton Methods|arXiv preprint arXiv:2302.04987|2023|page 2|\n|[KBS93]|H Fayez Khalfan, Richard H Byrd, and Robert B Schnabel|A theoretical and experimental study of the symmetric rank-one update|SIAM Journal on Optimization|1993|1-24 (page 2)|\n|[KW92]|J. Kuczy\u0144ski and H. Wo\u017aniakowski|Estimating the Largest Eigenvalue by the Power and Lanczos Algorithms with a Random Start|SIAM Journal on Matrix Analysis and Applications|1992|1094-1122 (pages 9, 39)|\n|[LYZ21]|Dachao Lin, Haishan Ye, and Zhihua Zhang|Greedy and random quasi-Newton methods with faster explicit superlinear convergence|Advances in Neural Information Processing Systems 34|2021|6646-6657 (page 2)|\n|[Mha22]|Zakaria Mhammedi|Efficient Projection-Free Online Convex Optimization with Membership Oracle|Proceedings of Thirty Fifth Conference on Learning Theory|2022|5314-5390 (pages 9, 30)|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "List of References", "md": "## List of References"}, {"type": "table", "rows": [["Reference", "Authors", "Title", "Journal/Conference", "Year", "Pages"], ["[GDGVSU19]", "Alexander Gasnikov, Pavel Dvurechensky, Eduard Gorbunov, Evgeniya Vorontsova, Daniil Selikhanovych, and C\u00e9sar A Uribe", "Optimal tensor methods in smooth convex and uniformly convex optimization", "Conference on Learning Theory", "2019", "1374-1391 (page 4)"], ["[GS18]", "Hiva Ghanbari and Katya Scheinberg", "Proximal quasi-Newton methods for regularized convex optimization with linear and accelerated sublinear convergence rates", "Computational Optimization and Applications", "2018", "597-627 (page 2)"], ["[Gol70]", "Donald Goldfarb", "A family of variable-metric methods derived by variational means", "Mathematics of Computation", "1970", "23-26 (page 2)"], ["[G\u00fcler92]", "Osman G\u00fcler", "New proximal point algorithms for convex minimization", "SIAM Journal on Optimization", "1992", "649-664 (page 3)"], ["[JWZ21]", "Bo Jiang, Haoyue Wang, and Shuzhong Zhang", "An optimal high-order tensor method for convex optimization", "Mathematics of Operations Research", "2021", "1390-1412 (page 4)"], ["[JJM23]", "Ruichen Jiang, Qiujiang Jin, and Aryan Mokhtari", "Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence", "arXiv preprint arXiv:2302.08580", "2023", "3, 6, 9, 10, 27, 30, 34"], ["[JKRM22]", "Qiujiang Jin, Alec Koppel, Ketan Rajawat, and Aryan Mokhtari", "Sharpened Quasi-Newton Methods: Faster Superlinear Rate and Larger Local Convergence Neighborhood", "Proceedings of the 39th International Conference on Machine Learning", "2022", "10228-10250 (page 2)"], ["[JM22]", "Qiujiang Jin and Aryan Mokhtari", "Non-asymptotic superlinear convergence of standard quasi-Newton methods", "Mathematical Programming", "2022", "page 2"], ["[KZAT23]", "Dmitry Kamzolov, Klea Ziu, Artem Agafonov, and Martin Tak\u00e1\u010d", "Accelerated Adaptive Cubic Regularized Quasi-Newton Methods", "arXiv preprint arXiv:2302.04987", "2023", "page 2"], ["[KBS93]", "H Fayez Khalfan, Richard H Byrd, and Robert B Schnabel", "A theoretical and experimental study of the symmetric rank-one update", "SIAM Journal on Optimization", "1993", "1-24 (page 2)"], ["[KW92]", "J. Kuczy\u0144ski and H. Wo\u017aniakowski", "Estimating the Largest Eigenvalue by the Power and Lanczos Algorithms with a Random Start", "SIAM Journal on Matrix Analysis and Applications", "1992", "1094-1122 (pages 9, 39)"], ["[LYZ21]", "Dachao Lin, Haishan Ye, and Zhihua Zhang", "Greedy and random quasi-Newton methods with faster explicit superlinear convergence", "Advances in Neural Information Processing Systems 34", "2021", "6646-6657 (page 2)"], ["[Mha22]", "Zakaria Mhammedi", "Efficient Projection-Free Online Convex Optimization with Membership Oracle", "Proceedings of Thirty Fifth Conference on Learning Theory", "2022", "5314-5390 (pages 9, 30)"]], "md": "|Reference|Authors|Title|Journal/Conference|Year|Pages|\n|---|---|---|---|---|---|\n|[GDGVSU19]|Alexander Gasnikov, Pavel Dvurechensky, Eduard Gorbunov, Evgeniya Vorontsova, Daniil Selikhanovych, and C\u00e9sar A Uribe|Optimal tensor methods in smooth convex and uniformly convex optimization|Conference on Learning Theory|2019|1374-1391 (page 4)|\n|[GS18]|Hiva Ghanbari and Katya Scheinberg|Proximal quasi-Newton methods for regularized convex optimization with linear and accelerated sublinear convergence rates|Computational Optimization and Applications|2018|597-627 (page 2)|\n|[Gol70]|Donald Goldfarb|A family of variable-metric methods derived by variational means|Mathematics of Computation|1970|23-26 (page 2)|\n|[G\u00fcler92]|Osman G\u00fcler|New proximal point algorithms for convex minimization|SIAM Journal on Optimization|1992|649-664 (page 3)|\n|[JWZ21]|Bo Jiang, Haoyue Wang, and Shuzhong Zhang|An optimal high-order tensor method for convex optimization|Mathematics of Operations Research|2021|1390-1412 (page 4)|\n|[JJM23]|Ruichen Jiang, Qiujiang Jin, and Aryan Mokhtari|Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence|arXiv preprint arXiv:2302.08580|2023|3, 6, 9, 10, 27, 30, 34|\n|[JKRM22]|Qiujiang Jin, Alec Koppel, Ketan Rajawat, and Aryan Mokhtari|Sharpened Quasi-Newton Methods: Faster Superlinear Rate and Larger Local Convergence Neighborhood|Proceedings of the 39th International Conference on Machine Learning|2022|10228-10250 (page 2)|\n|[JM22]|Qiujiang Jin and Aryan Mokhtari|Non-asymptotic superlinear convergence of standard quasi-Newton methods|Mathematical Programming|2022|page 2|\n|[KZAT23]|Dmitry Kamzolov, Klea Ziu, Artem Agafonov, and Martin Tak\u00e1\u010d|Accelerated Adaptive Cubic Regularized Quasi-Newton Methods|arXiv preprint arXiv:2302.04987|2023|page 2|\n|[KBS93]|H Fayez Khalfan, Richard H Byrd, and Robert B Schnabel|A theoretical and experimental study of the symmetric rank-one update|SIAM Journal on Optimization|1993|1-24 (page 2)|\n|[KW92]|J. Kuczy\u0144ski and H. Wo\u017aniakowski|Estimating the Largest Eigenvalue by the Power and Lanczos Algorithms with a Random Start|SIAM Journal on Matrix Analysis and Applications|1992|1094-1122 (pages 9, 39)|\n|[LYZ21]|Dachao Lin, Haishan Ye, and Zhihua Zhang|Greedy and random quasi-Newton methods with faster explicit superlinear convergence|Advances in Neural Information Processing Systems 34|2021|6646-6657 (page 2)|\n|[Mha22]|Zakaria Mhammedi|Efficient Projection-Free Online Convex Optimization with Membership Oracle|Proceedings of Thirty Fifth Conference on Learning Theory|2022|5314-5390 (pages 9, 30)|", "isPerfectTable": true, "csv": "\"Reference\",\"Authors\",\"Title\",\"Journal/Conference\",\"Year\",\"Pages\"\n\"[GDGVSU19]\",\"Alexander Gasnikov, Pavel Dvurechensky, Eduard Gorbunov, Evgeniya Vorontsova, Daniil Selikhanovych, and C\u00e9sar A Uribe\",\"Optimal tensor methods in smooth convex and uniformly convex optimization\",\"Conference on Learning Theory\",\"2019\",\"1374-1391 (page 4)\"\n\"[GS18]\",\"Hiva Ghanbari and Katya Scheinberg\",\"Proximal quasi-Newton methods for regularized convex optimization with linear and accelerated sublinear convergence rates\",\"Computational Optimization and Applications\",\"2018\",\"597-627 (page 2)\"\n\"[Gol70]\",\"Donald Goldfarb\",\"A family of variable-metric methods derived by variational means\",\"Mathematics of Computation\",\"1970\",\"23-26 (page 2)\"\n\"[G\u00fcler92]\",\"Osman G\u00fcler\",\"New proximal point algorithms for convex minimization\",\"SIAM Journal on Optimization\",\"1992\",\"649-664 (page 3)\"\n\"[JWZ21]\",\"Bo Jiang, Haoyue Wang, and Shuzhong Zhang\",\"An optimal high-order tensor method for convex optimization\",\"Mathematics of Operations Research\",\"2021\",\"1390-1412 (page 4)\"\n\"[JJM23]\",\"Ruichen Jiang, Qiujiang Jin, and Aryan Mokhtari\",\"Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence\",\"arXiv preprint arXiv:2302.08580\",\"2023\",\"3, 6, 9, 10, 27, 30, 34\"\n\"[JKRM22]\",\"Qiujiang Jin, Alec Koppel, Ketan Rajawat, and Aryan Mokhtari\",\"Sharpened Quasi-Newton Methods: Faster Superlinear Rate and Larger Local Convergence Neighborhood\",\"Proceedings of the 39th International Conference on Machine Learning\",\"2022\",\"10228-10250 (page 2)\"\n\"[JM22]\",\"Qiujiang Jin and Aryan Mokhtari\",\"Non-asymptotic superlinear convergence of standard quasi-Newton methods\",\"Mathematical Programming\",\"2022\",\"page 2\"\n\"[KZAT23]\",\"Dmitry Kamzolov, Klea Ziu, Artem Agafonov, and Martin Tak\u00e1\u010d\",\"Accelerated Adaptive Cubic Regularized Quasi-Newton Methods\",\"arXiv preprint arXiv:2302.04987\",\"2023\",\"page 2\"\n\"[KBS93]\",\"H Fayez Khalfan, Richard H Byrd, and Robert B Schnabel\",\"A theoretical and experimental study of the symmetric rank-one update\",\"SIAM Journal on Optimization\",\"1993\",\"1-24 (page 2)\"\n\"[KW92]\",\"J. Kuczy\u0144ski and H. Wo\u017aniakowski\",\"Estimating the Largest Eigenvalue by the Power and Lanczos Algorithms with a Random Start\",\"SIAM Journal on Matrix Analysis and Applications\",\"1992\",\"1094-1122 (pages 9, 39)\"\n\"[LYZ21]\",\"Dachao Lin, Haishan Ye, and Zhihua Zhang\",\"Greedy and random quasi-Newton methods with faster explicit superlinear convergence\",\"Advances in Neural Information Processing Systems 34\",\"2021\",\"6646-6657 (page 2)\"\n\"[Mha22]\",\"Zakaria Mhammedi\",\"Efficient Projection-Free Online Convex Optimization with Membership Oracle\",\"Proceedings of Thirty Fifth Conference on Learning Theory\",\"2022\",\"5314-5390 (pages 9, 30)\""}]}, {"page": 15, "text": "[MSJR16]  Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro.\n          \u201cOnline optimization in dynamic environments: Improved regret rates for strongly\n          convex problems\u201d. In: 2016 IEEE 55th Conference on Decision and Control (CDC).\n          IEEE. 2016, pp. 7195\u20137201 (pages 10, 30).\n[MS13]    Renato DC Monteiro and Benar Fux Svaiter. \u201cAn accelerated hybrid proximal\n          extragradient method for convex optimization and its implications to second-order\n          methods\u201d. In: SIAM Journal on Optimization 23.2 (2013), pp. 1092\u20131125 (pages 3,\n          4).\n[NY83]    A. Nemirovski and D Yudin. Problem Complexity and Method Efficiency in Opti-\n          mization. Hoboken: Wiley, 1983 (pages 2, 10).\n[Nem95]   Arkadi Nemirovski. \u201cInformation-based complexity of convex programming\u201d. In:\n          Lecture notes 834 (1995) (page 39).\n[Nes83]   Yurii Nesterov. \u201cA method of solving a convex programming problem with conver-\n          gence rate O(1/k2)\u201d. In: Soviet Mathematics Doklady (1983), pp. 372\u2013376 (pages 2,\n          10).\n[Nes18]   Yurii Nesterov. Lectures on Convex Optimization. Springer International Publishing,\n          2018 (pages 2, 10).\n[NW06]    Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Sci-\n          ence+Business Media, LLC, 2006 (pages 2, 11).\n[Pow72]   M.J.D. Powell. \u201cSome properties of the variable metric algorithm\u201d. In: Numerical\n          methods for nonlinear optimization (1972), pp. 1\u201317 (page 2).\n[Pow71]   MJD Powell. \u201cOn the convergence of the variable metric algorithm\u201d. In: IMA\n          Journal of Applied Mathematics 7.1 (1971), pp. 21\u201336 (page 2).\n[RN21a]   Anton Rodomanov and Yurii Nesterov. \u201cGreedy quasi-Newton methods with explicit\n          superlinear convergence\u201d. In: SIAM Journal on Optimization 31.1 (2021), pp. 785\u2013\n          811 (pages 2, 4).\n[RN21b]   Anton Rodomanov and Yurii Nesterov. \u201cNew Results on Superlinear Convergence\n          of Classical Quasi-Newton Methods\u201d. In: Journal of Optimization Theory and\n          Applications 188.3 (2021), pp. 744\u2013769 (page 2).\n[RN21c]   Anton Rodomanov and Yurii Nesterov. \u201cRates of superlinear convergence for classical\n          quasi-Newton methods\u201d. In: Mathematical Programming (2021) (page 2).\n[Saa03]   Yousef Saad. Iterative Methods for Sparse Linear Systems. Second. Society for\n          Industrial and Applied Mathematics, 2003 (pages 7, 39).\n[Saa11]   Yousef Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM,\n          2011 (page 39).\n[SV12]    Saverio Salzo and Silvia Villa. \u201cInexact and accelerated proximal point algorithms\u201d.\n          In: Journal of Convex analysis 19.4 (2012), pp. 1167\u20131192 (page 3).\n[ST16]    Katya Scheinberg and Xiaocheng Tang. \u201cPractical inexact proximal quasi-Newton\n          method with global complexity analysis\u201d. In: Mathematical Programming 160 (2016),\n          pp. 495\u2013529 (page 2).\n[Sha70]   David F Shanno. \u201cConditioning of quasi-Newton methods for function minimization\u201d.\n          In: Mathematics of computation 24.111 (1970), pp. 647\u2013656 (page 2).\n                                              15", "md": "# References\n\n# List of References\n\n|Reference|Details|\n|---|---|\n|[MSJR16]|Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. \"Online optimization in dynamic environments: Improved regret rates for strongly convex problems\". In: 2016 IEEE 55th Conference on Decision and Control (CDC). IEEE. 2016, pp. 7195\u20137201 (pages 10, 30).|\n|[MS13]|Renato DC Monteiro and Benar Fux Svaiter. \"An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods\". In: SIAM Journal on Optimization 23.2 (2013), pp. 1092\u20131125 (pages 3, 4).|\n|[NY83]|A. Nemirovski and D Yudin. Problem Complexity and Method Efficiency in Optimization. Hoboken: Wiley, 1983 (pages 2, 10).|\n|[Nem95]|Arkadi Nemirovski. \"Information-based complexity of convex programming\". In: Lecture notes 834 (1995) (page 39).|\n|[Nes83]|Yurii Nesterov. \"A method of solving a convex programming problem with convergence rate $O\\left(\\frac{1}{k^2}\\right)$. In: Soviet Mathematics Doklady (1983), pp. 372\u2013376 (pages 2, 10).|\n|[Nes18]|Yurii Nesterov. Lectures on Convex Optimization. Springer International Publishing, 2018 (pages 2, 10).|\n|[NW06]|Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Science+Business Media, LLC, 2006 (pages 2, 11).|\n|[Pow72]|M.J.D. Powell. \"Some properties of the variable metric algorithm\". In: Numerical methods for nonlinear optimization (1972), pp. 1\u201317 (page 2).|\n|[Pow71]|MJD Powell. \"On the convergence of the variable metric algorithm\". In: IMA Journal of Applied Mathematics 7.1 (1971), pp. 21\u201336 (page 2).|\n|[RN21a]|Anton Rodomanov and Yurii Nesterov. \"Greedy quasi-Newton methods with explicit superlinear convergence\". In: SIAM Journal on Optimization 31.1 (2021), pp. 785\u2013811 (pages 2, 4).|\n|[RN21b]|Anton Rodomanov and Yurii Nesterov. \"New Results on Superlinear Convergence of Classical Quasi-Newton Methods\". In: Journal of Optimization Theory and Applications 188.3 (2021), pp. 744\u2013769 (page 2).|\n|[RN21c]|Anton Rodomanov and Yurii Nesterov. \"Rates of superlinear convergence for classical quasi-Newton methods\". In: Mathematical Programming (2021) (page 2).|\n|[Saa03]|Yousef Saad. Iterative Methods for Sparse Linear Systems. Second. Society for Industrial and Applied Mathematics, 2003 (pages 7, 39).|\n|[Saa11]|Yousef Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011 (page 39).|\n|[SV12]|Saverio Salzo and Silvia Villa. \"Inexact and accelerated proximal point algorithms\". In: Journal of Convex analysis 19.4 (2012), pp. 1167\u20131192 (page 3).|\n|[ST16]|Katya Scheinberg and Xiaocheng Tang. \"Practical inexact proximal quasi-Newton method with global complexity analysis\". In: Mathematical Programming 160 (2016), pp. 495\u2013529 (page 2).|\n|[Sha70]|David F Shanno. \"Conditioning of quasi-Newton methods for function minimization\". In: Mathematics of computation 24.111 (1970), pp. 647\u2013656 (page 2).|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["Reference", "Details"], ["[MSJR16]", "Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. \"Online optimization in dynamic environments: Improved regret rates for strongly convex problems\". In: 2016 IEEE 55th Conference on Decision and Control (CDC). IEEE. 2016, pp. 7195\u20137201 (pages 10, 30)."], ["[MS13]", "Renato DC Monteiro and Benar Fux Svaiter. \"An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods\". In: SIAM Journal on Optimization 23.2 (2013), pp. 1092\u20131125 (pages 3, 4)."], ["[NY83]", "A. Nemirovski and D Yudin. Problem Complexity and Method Efficiency in Optimization. Hoboken: Wiley, 1983 (pages 2, 10)."], ["[Nem95]", "Arkadi Nemirovski. \"Information-based complexity of convex programming\". In: Lecture notes 834 (1995) (page 39)."], ["[Nes83]", "Yurii Nesterov. \"A method of solving a convex programming problem with convergence rate $O\\left(\\frac{1}{k^2}\\right)$. In: Soviet Mathematics Doklady (1983), pp. 372\u2013376 (pages 2, 10)."], ["[Nes18]", "Yurii Nesterov. Lectures on Convex Optimization. Springer International Publishing, 2018 (pages 2, 10)."], ["[NW06]", "Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Science+Business Media, LLC, 2006 (pages 2, 11)."], ["[Pow72]", "M.J.D. Powell. \"Some properties of the variable metric algorithm\". In: Numerical methods for nonlinear optimization (1972), pp. 1\u201317 (page 2)."], ["[Pow71]", "MJD Powell. \"On the convergence of the variable metric algorithm\". In: IMA Journal of Applied Mathematics 7.1 (1971), pp. 21\u201336 (page 2)."], ["[RN21a]", "Anton Rodomanov and Yurii Nesterov. \"Greedy quasi-Newton methods with explicit superlinear convergence\". In: SIAM Journal on Optimization 31.1 (2021), pp. 785\u2013811 (pages 2, 4)."], ["[RN21b]", "Anton Rodomanov and Yurii Nesterov. \"New Results on Superlinear Convergence of Classical Quasi-Newton Methods\". In: Journal of Optimization Theory and Applications 188.3 (2021), pp. 744\u2013769 (page 2)."], ["[RN21c]", "Anton Rodomanov and Yurii Nesterov. \"Rates of superlinear convergence for classical quasi-Newton methods\". In: Mathematical Programming (2021) (page 2)."], ["[Saa03]", "Yousef Saad. Iterative Methods for Sparse Linear Systems. Second. Society for Industrial and Applied Mathematics, 2003 (pages 7, 39)."], ["[Saa11]", "Yousef Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011 (page 39)."], ["[SV12]", "Saverio Salzo and Silvia Villa. \"Inexact and accelerated proximal point algorithms\". In: Journal of Convex analysis 19.4 (2012), pp. 1167\u20131192 (page 3)."], ["[ST16]", "Katya Scheinberg and Xiaocheng Tang. \"Practical inexact proximal quasi-Newton method with global complexity analysis\". In: Mathematical Programming 160 (2016), pp. 495\u2013529 (page 2)."], ["[Sha70]", "David F Shanno. \"Conditioning of quasi-Newton methods for function minimization\". In: Mathematics of computation 24.111 (1970), pp. 647\u2013656 (page 2)."]], "md": "|Reference|Details|\n|---|---|\n|[MSJR16]|Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. \"Online optimization in dynamic environments: Improved regret rates for strongly convex problems\". In: 2016 IEEE 55th Conference on Decision and Control (CDC). IEEE. 2016, pp. 7195\u20137201 (pages 10, 30).|\n|[MS13]|Renato DC Monteiro and Benar Fux Svaiter. \"An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods\". In: SIAM Journal on Optimization 23.2 (2013), pp. 1092\u20131125 (pages 3, 4).|\n|[NY83]|A. Nemirovski and D Yudin. Problem Complexity and Method Efficiency in Optimization. Hoboken: Wiley, 1983 (pages 2, 10).|\n|[Nem95]|Arkadi Nemirovski. \"Information-based complexity of convex programming\". In: Lecture notes 834 (1995) (page 39).|\n|[Nes83]|Yurii Nesterov. \"A method of solving a convex programming problem with convergence rate $O\\left(\\frac{1}{k^2}\\right)$. In: Soviet Mathematics Doklady (1983), pp. 372\u2013376 (pages 2, 10).|\n|[Nes18]|Yurii Nesterov. Lectures on Convex Optimization. Springer International Publishing, 2018 (pages 2, 10).|\n|[NW06]|Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Science+Business Media, LLC, 2006 (pages 2, 11).|\n|[Pow72]|M.J.D. Powell. \"Some properties of the variable metric algorithm\". In: Numerical methods for nonlinear optimization (1972), pp. 1\u201317 (page 2).|\n|[Pow71]|MJD Powell. \"On the convergence of the variable metric algorithm\". In: IMA Journal of Applied Mathematics 7.1 (1971), pp. 21\u201336 (page 2).|\n|[RN21a]|Anton Rodomanov and Yurii Nesterov. \"Greedy quasi-Newton methods with explicit superlinear convergence\". In: SIAM Journal on Optimization 31.1 (2021), pp. 785\u2013811 (pages 2, 4).|\n|[RN21b]|Anton Rodomanov and Yurii Nesterov. \"New Results on Superlinear Convergence of Classical Quasi-Newton Methods\". In: Journal of Optimization Theory and Applications 188.3 (2021), pp. 744\u2013769 (page 2).|\n|[RN21c]|Anton Rodomanov and Yurii Nesterov. \"Rates of superlinear convergence for classical quasi-Newton methods\". In: Mathematical Programming (2021) (page 2).|\n|[Saa03]|Yousef Saad. Iterative Methods for Sparse Linear Systems. Second. Society for Industrial and Applied Mathematics, 2003 (pages 7, 39).|\n|[Saa11]|Yousef Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011 (page 39).|\n|[SV12]|Saverio Salzo and Silvia Villa. \"Inexact and accelerated proximal point algorithms\". In: Journal of Convex analysis 19.4 (2012), pp. 1167\u20131192 (page 3).|\n|[ST16]|Katya Scheinberg and Xiaocheng Tang. \"Practical inexact proximal quasi-Newton method with global complexity analysis\". In: Mathematical Programming 160 (2016), pp. 495\u2013529 (page 2).|\n|[Sha70]|David F Shanno. \"Conditioning of quasi-Newton methods for function minimization\". In: Mathematics of computation 24.111 (1970), pp. 647\u2013656 (page 2).|", "isPerfectTable": true, "csv": "\"Reference\",\"Details\"\n\"[MSJR16]\",\"Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro. \"\"Online optimization in dynamic environments: Improved regret rates for strongly convex problems\"\". In: 2016 IEEE 55th Conference on Decision and Control (CDC). IEEE. 2016, pp. 7195\u20137201 (pages 10, 30).\"\n\"[MS13]\",\"Renato DC Monteiro and Benar Fux Svaiter. \"\"An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods\"\". In: SIAM Journal on Optimization 23.2 (2013), pp. 1092\u20131125 (pages 3, 4).\"\n\"[NY83]\",\"A. Nemirovski and D Yudin. Problem Complexity and Method Efficiency in Optimization. Hoboken: Wiley, 1983 (pages 2, 10).\"\n\"[Nem95]\",\"Arkadi Nemirovski. \"\"Information-based complexity of convex programming\"\". In: Lecture notes 834 (1995) (page 39).\"\n\"[Nes83]\",\"Yurii Nesterov. \"\"A method of solving a convex programming problem with convergence rate $O\\left(\\frac{1}{k^2}\\right)$. In: Soviet Mathematics Doklady (1983), pp. 372\u2013376 (pages 2, 10).\"\n\"[Nes18]\",\"Yurii Nesterov. Lectures on Convex Optimization. Springer International Publishing, 2018 (pages 2, 10).\"\n\"[NW06]\",\"Jorge Nocedal and Stephen J. Wright. Numerical Optimization. Springer Science+Business Media, LLC, 2006 (pages 2, 11).\"\n\"[Pow72]\",\"M.J.D. Powell. \"\"Some properties of the variable metric algorithm\"\". In: Numerical methods for nonlinear optimization (1972), pp. 1\u201317 (page 2).\"\n\"[Pow71]\",\"MJD Powell. \"\"On the convergence of the variable metric algorithm\"\". In: IMA Journal of Applied Mathematics 7.1 (1971), pp. 21\u201336 (page 2).\"\n\"[RN21a]\",\"Anton Rodomanov and Yurii Nesterov. \"\"Greedy quasi-Newton methods with explicit superlinear convergence\"\". In: SIAM Journal on Optimization 31.1 (2021), pp. 785\u2013811 (pages 2, 4).\"\n\"[RN21b]\",\"Anton Rodomanov and Yurii Nesterov. \"\"New Results on Superlinear Convergence of Classical Quasi-Newton Methods\"\". In: Journal of Optimization Theory and Applications 188.3 (2021), pp. 744\u2013769 (page 2).\"\n\"[RN21c]\",\"Anton Rodomanov and Yurii Nesterov. \"\"Rates of superlinear convergence for classical quasi-Newton methods\"\". In: Mathematical Programming (2021) (page 2).\"\n\"[Saa03]\",\"Yousef Saad. Iterative Methods for Sparse Linear Systems. Second. Society for Industrial and Applied Mathematics, 2003 (pages 7, 39).\"\n\"[Saa11]\",\"Yousef Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM, 2011 (page 39).\"\n\"[SV12]\",\"Saverio Salzo and Silvia Villa. \"\"Inexact and accelerated proximal point algorithms\"\". In: Journal of Convex analysis 19.4 (2012), pp. 1167\u20131192 (page 3).\"\n\"[ST16]\",\"Katya Scheinberg and Xiaocheng Tang. \"\"Practical inexact proximal quasi-Newton method with global complexity analysis\"\". In: Mathematical Programming 160 (2016), pp. 495\u2013529 (page 2).\"\n\"[Sha70]\",\"David F Shanno. \"\"Conditioning of quasi-Newton methods for function minimization\"\". In: Mathematics of computation 24.111 (1970), pp. 647\u2013656 (page 2).\""}]}, {"page": 16, "text": "[YLCZ22]    Haishan Ye, Dachao Lin, Xiangyu Chang, and Zhihua Zhang. \u201cTowards explicit\n            superlinear convergence rate for SR1\u201d. In: Mathematical Programming (2022), pp. 1\u2013\n           31 (page 2).\n[YTFUC21]   Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher.\n           \u201cScalable semidefinite programming\u201d. In: SIAM Journal on Mathematics of Data\n           Science 3.1 (2021), pp. 171\u2013200 (page 39).\n[Zin03]     Martin Zinkevich. \u201cOnline convex programming and generalized infinitesimal gradi-\n            ent ascent\u201d. In: Proceedings of the 20th international conference on machine learning\n           (icml-03). 2003, pp. 928\u2013936 (pages 9, 30).\n                                             16", "md": "# References\n\n## References\n\n- [YLCZ22] Haishan Ye, Dachao Lin, Xiangyu Chang, and Zhihua Zhang. \u201cTowards explicit superlinear convergence rate for SR1\u201d. In: Mathematical Programming (2022), pp. 1\u201331 (page 2).\n- [YTFUC21] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. \u201cScalable semidefinite programming\u201d. In: SIAM Journal on Mathematics of Data Science 3.1 (2021), pp. 171\u2013200 (page 39).\n- [Zin03] Martin Zinkevich. \u201cOnline convex programming and generalized infinitesimal gradient ascent\u201d. In: Proceedings of the 20th international conference on machine learning (icml-03). 2003, pp. 928\u2013936 (pages 9, 30).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- [YLCZ22] Haishan Ye, Dachao Lin, Xiangyu Chang, and Zhihua Zhang. \u201cTowards explicit superlinear convergence rate for SR1\u201d. In: Mathematical Programming (2022), pp. 1\u201331 (page 2).\n- [YTFUC21] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. \u201cScalable semidefinite programming\u201d. In: SIAM Journal on Mathematics of Data Science 3.1 (2021), pp. 171\u2013200 (page 39).\n- [Zin03] Martin Zinkevich. \u201cOnline convex programming and generalized infinitesimal gradient ascent\u201d. In: Proceedings of the 20th international conference on machine learning (icml-03). 2003, pp. 928\u2013936 (pages 9, 30).", "md": "- [YLCZ22] Haishan Ye, Dachao Lin, Xiangyu Chang, and Zhihua Zhang. \u201cTowards explicit superlinear convergence rate for SR1\u201d. In: Mathematical Programming (2022), pp. 1\u201331 (page 2).\n- [YTFUC21] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. \u201cScalable semidefinite programming\u201d. In: SIAM Journal on Mathematics of Data Science 3.1 (2021), pp. 171\u2013200 (page 39).\n- [Zin03] Martin Zinkevich. \u201cOnline convex programming and generalized infinitesimal gradient ascent\u201d. In: Proceedings of the 20th international conference on machine learning (icml-03). 2003, pp. 928\u2013936 (pages 9, 30)."}]}, {"page": 17, "text": "Appendix\nA      Optimal Monteiro-Svaiter Acceleration Framework\nIn this section, we present some general results that hold for the optimal MS Acceleration framework.\nIn particular, in the first part of this section (Section A.1), we present the proof of Proposition 3.1.\nIn the second part (Section A.2), we further provide some useful additional lemmas.\nA.1     Proof of Proposition 3.1\nTo begin with, we establish a potential function for Algorithm 1, as shown in Proposition A.1. The\nresult is similar to Proposition 1 in [CHJJS22], but for completeness we present its proof loosely\nfollowing the strategy in [dST21, Theorem 5.3]. To simplify the notations, we use f\u2217           to denote the\noptimal f(x\u2217).\nProposition A.1. Consider the iterates generated by Algorithm 1. If f is convex, then\n               Ak+1(f(xk+1) \u2212     f\u2217) + 12\u2225zk+1 \u2212  x\u2217\u22252 \u2264   Ak(f(xk) \u2212   f\u2217) + 12\u2225zk \u2212   x\u2217\u22252.           (20)\nMoreover, let \u03c3 = \u03b11 + \u03b12 and we have\n                                N\u22121  a2                      1\n                                      k \u2225\u02c6\n                                     \u03b72  xk+1 \u2212  yk\u22252 \u2264   1 \u2212 \u03c32 \u2225z0 \u2212  x\u2217\u22252.                            (21)\n                                k=0   k\nProof. Since f is convex, it holds that\n                             f(xk) \u2212  f(\u02c6xk+1) \u2212  \u27e8\u2207f(\u02c6 xk+1), xk \u2212  \u02c6\n                                                                     xk+1\u27e9  \u2265  0,\n                             f(x\u2217) \u2212  f(\u02c6xk+1) \u2212  \u27e8\u2207f(\u02c6 xk+1), x\u2217 \u2212  \u02c6\n                                                                     xk+1\u27e9  \u2265  0.\nBy summing up the two inequalities with weights ak and Ak respectively, we get\n  Ak(f(xk) \u2212   f\u2217) \u2212  (Ak + ak)(f(\u02c6 xk+1) \u2212  f\u2217) \u2212  ak\u27e8\u2207f(\u02c6 xk+1), x\u2217 \u2212  \u02c6\n                                                                         xk+1 \u2212  Ak(\u02c6 xk+1 \u2212  xk)\u27e9 \u2265  0. (22)\n                                                                                  ak\nLet \u02dczk+1 = \u02c6xk+1 + Aka   xk+1 \u2212  xk). By rearranging the terms, (22) can be rewritten as\n                       k (\u02c6\n               (Ak + ak)(f(\u02c6  xk+1) \u2212  f\u2217) \u2212  Ak(f(xk) \u2212   f\u2217) \u2264  ak\u27e8\u2207f(\u02c6 xk+1), \u02dczk+1 \u2212  x\u2217\u27e9.           (23)\nMoreover, note that the update rule for zk+1 in both (8) and (9) can be written as\n                                                      \u03b7k\n                                      zk+1 \u2212  zk = \u2212\u02c6 \u03b7k ak\u2207f(\u02c6 xk+1).                                   (24)\nAlso, since we also have zk = yk + Ak   ak (yk \u2212 xk) from (2), we can write\n                     \u02dc              \u02c6           (\u02c6\n                     zk+1 \u2212  zk =   xk+1 + Akak  xk+1 \u2212   xk)  \u2212   yk + Akak (yk \u2212 xk)\n                                = Ak + ak   (\u02c6\n                                             xk+1 \u2212   yk) = ak(\u02c6 xk+1 \u2212  yk),                            (25)\n                                      ak                     \u03b7k\n                                                     17", "md": "## Appendix\n\n### A Optimal Monteiro-Svaiter Acceleration Framework\n\nIn this section, we present some general results that hold for the optimal MS Acceleration framework. In particular, in the first part of this section (Section A.1), we present the proof of Proposition 3.1. In the second part (Section A.2), we further provide some useful additional lemmas.\n\n#### A.1 Proof of Proposition 3.1\n\nTo begin with, we establish a potential function for Algorithm 1, as shown in Proposition A.1. The result is similar to Proposition 1 in [CHJJS22], but for completeness we present its proof loosely following the strategy in [dST21, Theorem 5.3]. To simplify the notations, we use $$f^*$$ to denote the optimal $$f(x^*)$$.\n\nProposition A.1. Consider the iterates generated by Algorithm 1. If $$f$$ is convex, then\n\n$$\nAk+1(f(x_{k+1}) - f^*) + \\frac{1}{2}\\|z_{k+1} - x^*\\|^2 \\leq Ak(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2. \\quad (20)\n$$\n\nMoreover, let $$\\sigma = \\alpha_1 + \\alpha_2$$ and we have\n\n$$\n\\sum_{k=0}^{N-1} a_k^2 \\|\\hat{\\eta}_2 x_{k+1} - y_k\\|^2 \\leq (1 - \\sigma^2) \\|z_0 - x^*\\|^2. \\quad (21)\n$$\n\nProof. Since $$f$$ is convex, it holds that\n\n$$\nf(x_k) - f(\\hat{x}_{k+1}) - \\langle \\nabla f(\\hat{x}_{k+1}), x_k - \\hat{x}_{k+1} \\rangle \\geq 0,\n$$\n$$\nf(x^*) - f(\\hat{x}_{k+1}) - \\langle \\nabla f(\\hat{x}_{k+1}), x^* - \\hat{x}_{k+1} \\rangle \\geq 0.\n$$\n\nBy summing up the two inequalities with weights $$a_k$$ and $$A_k$$ respectively, we get\n\n$$\nAk(f(x_k) - f^*) - (Ak + a_k)(f(\\hat{x}_{k+1}) - f^*) - a_k \\langle \\nabla f(\\hat{x}_{k+1}), x^* - \\hat{x}_{k+1} - Ak(\\hat{x}_{k+1} - x_k) \\rangle \\geq 0. \\quad (22)\n$$\n\nLet $$\\tilde{z}_{k+1} = \\hat{x}_{k+1} + Ak(a_k x_{k+1} - x_k)$$. By rearranging the terms, (22) can be rewritten as\n\n$$\n(Ak + a_k)(f(\\hat{x}_{k+1}) - f^*) - Ak(f(x_k) - f^*) \\leq a_k \\langle \\nabla f(\\hat{x}_{k+1}), \\tilde{z}_{k+1} - x^* \\rangle. \\quad (23)\n$$\n\nMoreover, note that the update rule for $$z_{k+1}$$ in both (8) and (9) can be written as\n\n$$\nz_{k+1} - z_k = -\\hat{\\eta}_k a_k \\nabla f(\\hat{x}_{k+1}). \\quad (24)\n$$\n\nAlso, since we also have $$z_k = y_k + Ak a_k (y_k - x_k)$$ from (2), we can write\n\n$$\n\\begin{aligned}\n\\tilde{z}_{k+1} - z_k & = \\hat{x}_{k+1} + Ak a_k (x_{k+1} - x_k) - y_k + Ak a_k (y_k - x_k) \\\\\n& = Ak + a_k (\\hat{x}_{k+1} - y_k) = a_k (\\hat{x}_{k+1} - y_k). \\quad (25)\n\\end{aligned}\n$$", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Appendix", "md": "## Appendix"}, {"type": "heading", "lvl": 3, "value": "A Optimal Monteiro-Svaiter Acceleration Framework", "md": "### A Optimal Monteiro-Svaiter Acceleration Framework"}, {"type": "text", "value": "In this section, we present some general results that hold for the optimal MS Acceleration framework. In particular, in the first part of this section (Section A.1), we present the proof of Proposition 3.1. In the second part (Section A.2), we further provide some useful additional lemmas.", "md": "In this section, we present some general results that hold for the optimal MS Acceleration framework. In particular, in the first part of this section (Section A.1), we present the proof of Proposition 3.1. In the second part (Section A.2), we further provide some useful additional lemmas."}, {"type": "heading", "lvl": 4, "value": "A.1 Proof of Proposition 3.1", "md": "#### A.1 Proof of Proposition 3.1"}, {"type": "text", "value": "To begin with, we establish a potential function for Algorithm 1, as shown in Proposition A.1. The result is similar to Proposition 1 in [CHJJS22], but for completeness we present its proof loosely following the strategy in [dST21, Theorem 5.3]. To simplify the notations, we use $$f^*$$ to denote the optimal $$f(x^*)$$.\n\nProposition A.1. Consider the iterates generated by Algorithm 1. If $$f$$ is convex, then\n\n$$\nAk+1(f(x_{k+1}) - f^*) + \\frac{1}{2}\\|z_{k+1} - x^*\\|^2 \\leq Ak(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2. \\quad (20)\n$$\n\nMoreover, let $$\\sigma = \\alpha_1 + \\alpha_2$$ and we have\n\n$$\n\\sum_{k=0}^{N-1} a_k^2 \\|\\hat{\\eta}_2 x_{k+1} - y_k\\|^2 \\leq (1 - \\sigma^2) \\|z_0 - x^*\\|^2. \\quad (21)\n$$\n\nProof. Since $$f$$ is convex, it holds that\n\n$$\nf(x_k) - f(\\hat{x}_{k+1}) - \\langle \\nabla f(\\hat{x}_{k+1}), x_k - \\hat{x}_{k+1} \\rangle \\geq 0,\n$$\n$$\nf(x^*) - f(\\hat{x}_{k+1}) - \\langle \\nabla f(\\hat{x}_{k+1}), x^* - \\hat{x}_{k+1} \\rangle \\geq 0.\n$$\n\nBy summing up the two inequalities with weights $$a_k$$ and $$A_k$$ respectively, we get\n\n$$\nAk(f(x_k) - f^*) - (Ak + a_k)(f(\\hat{x}_{k+1}) - f^*) - a_k \\langle \\nabla f(\\hat{x}_{k+1}), x^* - \\hat{x}_{k+1} - Ak(\\hat{x}_{k+1} - x_k) \\rangle \\geq 0. \\quad (22)\n$$\n\nLet $$\\tilde{z}_{k+1} = \\hat{x}_{k+1} + Ak(a_k x_{k+1} - x_k)$$. By rearranging the terms, (22) can be rewritten as\n\n$$\n(Ak + a_k)(f(\\hat{x}_{k+1}) - f^*) - Ak(f(x_k) - f^*) \\leq a_k \\langle \\nabla f(\\hat{x}_{k+1}), \\tilde{z}_{k+1} - x^* \\rangle. \\quad (23)\n$$\n\nMoreover, note that the update rule for $$z_{k+1}$$ in both (8) and (9) can be written as\n\n$$\nz_{k+1} - z_k = -\\hat{\\eta}_k a_k \\nabla f(\\hat{x}_{k+1}). \\quad (24)\n$$\n\nAlso, since we also have $$z_k = y_k + Ak a_k (y_k - x_k)$$ from (2), we can write\n\n$$\n\\begin{aligned}\n\\tilde{z}_{k+1} - z_k & = \\hat{x}_{k+1} + Ak a_k (x_{k+1} - x_k) - y_k + Ak a_k (y_k - x_k) \\\\\n& = Ak + a_k (\\hat{x}_{k+1} - y_k) = a_k (\\hat{x}_{k+1} - y_k). \\quad (25)\n\\end{aligned}\n$$", "md": "To begin with, we establish a potential function for Algorithm 1, as shown in Proposition A.1. The result is similar to Proposition 1 in [CHJJS22], but for completeness we present its proof loosely following the strategy in [dST21, Theorem 5.3]. To simplify the notations, we use $$f^*$$ to denote the optimal $$f(x^*)$$.\n\nProposition A.1. Consider the iterates generated by Algorithm 1. If $$f$$ is convex, then\n\n$$\nAk+1(f(x_{k+1}) - f^*) + \\frac{1}{2}\\|z_{k+1} - x^*\\|^2 \\leq Ak(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2. \\quad (20)\n$$\n\nMoreover, let $$\\sigma = \\alpha_1 + \\alpha_2$$ and we have\n\n$$\n\\sum_{k=0}^{N-1} a_k^2 \\|\\hat{\\eta}_2 x_{k+1} - y_k\\|^2 \\leq (1 - \\sigma^2) \\|z_0 - x^*\\|^2. \\quad (21)\n$$\n\nProof. Since $$f$$ is convex, it holds that\n\n$$\nf(x_k) - f(\\hat{x}_{k+1}) - \\langle \\nabla f(\\hat{x}_{k+1}), x_k - \\hat{x}_{k+1} \\rangle \\geq 0,\n$$\n$$\nf(x^*) - f(\\hat{x}_{k+1}) - \\langle \\nabla f(\\hat{x}_{k+1}), x^* - \\hat{x}_{k+1} \\rangle \\geq 0.\n$$\n\nBy summing up the two inequalities with weights $$a_k$$ and $$A_k$$ respectively, we get\n\n$$\nAk(f(x_k) - f^*) - (Ak + a_k)(f(\\hat{x}_{k+1}) - f^*) - a_k \\langle \\nabla f(\\hat{x}_{k+1}), x^* - \\hat{x}_{k+1} - Ak(\\hat{x}_{k+1} - x_k) \\rangle \\geq 0. \\quad (22)\n$$\n\nLet $$\\tilde{z}_{k+1} = \\hat{x}_{k+1} + Ak(a_k x_{k+1} - x_k)$$. By rearranging the terms, (22) can be rewritten as\n\n$$\n(Ak + a_k)(f(\\hat{x}_{k+1}) - f^*) - Ak(f(x_k) - f^*) \\leq a_k \\langle \\nabla f(\\hat{x}_{k+1}), \\tilde{z}_{k+1} - x^* \\rangle. \\quad (23)\n$$\n\nMoreover, note that the update rule for $$z_{k+1}$$ in both (8) and (9) can be written as\n\n$$\nz_{k+1} - z_k = -\\hat{\\eta}_k a_k \\nabla f(\\hat{x}_{k+1}). \\quad (24)\n$$\n\nAlso, since we also have $$z_k = y_k + Ak a_k (y_k - x_k)$$ from (2), we can write\n\n$$\n\\begin{aligned}\n\\tilde{z}_{k+1} - z_k & = \\hat{x}_{k+1} + Ak a_k (x_{k+1} - x_k) - y_k + Ak a_k (y_k - x_k) \\\\\n& = Ak + a_k (\\hat{x}_{k+1} - y_k) = a_k (\\hat{x}_{k+1} - y_k). \\quad (25)\n\\end{aligned}\n$$"}]}, {"page": 18, "text": "where we used the fact that (Ak + ak)\u03b7k = a2    k in the last equality (cf. (2)). Hence, combining (24)\nand (25) leads to\n                                                    \u2225\u02c6\n \u2225\u02dc                                                  xk+1\u2212yk+ \u02c6   \u03b7k\u2207f(\u02c6 xk+1)\u2225  \u2264 \u03c3ak\u2225\u02c6 xk+1\u2212yk\u2225. (26)\n  zk+1\u2212zk+1\u2225    = \u2225\u02dczk+1 \u2212  zk\u2212(zk+1\u2212zk)\u2225     = ak\n                                                 \u03b7k                                  \u03b7k\nwhere we used (7) in the last inequality. In the following, we distinguish two cases depending on\n\u02c6\n\u03b7k = \u03b7k or \u02c6\u03b7k < \u03b7k. In both cases, we shall prove that\n Ak+1(f(xk+1)\u2212f\u2217)+ 1                                                                 k \u2225\u02c6\n                        2\u2225zk+1\u2212x\u2217\u22252 \u2264     Ak(f(xk)\u2212f\u2217)+ 1   2\u2225zk\u2212x\u2217\u22252\u2212(1 \u2212      \u03c32)a2   xk+1\u2212yk\u22252. (27)\n                                                                               2\u03b72k\nIf this is true, then Proposition A.1 immediately follows. Indeed, since \u03c3 < 1, the last term in the\nright-hand side of (27) is negative, which implies (20). Moreover, (21) follows from summing the\ninequality in (27) from k = 0 to N \u2212    1.\nCase I: \u02c6\u03b7k = \u03b7k. Since by (8) we have xk+1 = \u02c6    xk+1 and Ak+1 = Ak + ak, (23) becomes\n                 Ak+1(f(xk+1) \u2212    f\u2217) \u2212  Ak(f(xk) \u2212   f\u2217) \u2264  ak\u27e8\u2207f(xk+1), \u02dc zk+1 \u2212  x\u2217\u27e9.\nUsing zk+1 = zk \u2212   ak\u2207f(xk+1) in (8), we have\n                        Ak+1(f(xk+1) \u2212    f\u2217) \u2212 Ak(f(xk) \u2212   f\u2217)\n                     \u2264  \u27e8zk \u2212 zk+1, \u02dc\n                                    zk+1 \u2212  x\u2217\u27e9\n                     = \u27e8zk \u2212  zk+1, \u02dc\n                       X            zk+1 \u2212  zk+1\u27e9 + \u27e8zk \u2212  zk+1, zk+1 \u2212  x\u2217\u27e9\n                     =  1 X X X X  X X       zk+1 \u2212  zk+1\u22252 \u2212   1 zk+1 \u2212  zk\u22252                        (28)\n                                     X\n                        2\u2225zk \u2212  zk+1\u22252 + 1 2\u2225\u02dc                X 2\u2225\u02dc\n                        + 1                                   1 X X X  X X X\n                           2\u2225zk \u2212  x\u2217\u22252 \u2212  1                               X\n                                           2\u2225zk+1 \u2212   x\u2217\u22252 \u2212  2\u2225zk \u2212  zk+1\u22252\n                     \u2264  1                                            k \u2225xk+1 \u2212  yk\u22252,\n                        2\u2225zk \u2212  x\u2217\u22252 \u2212  1\n                                        2\u2225zk+1 \u2212   x\u2217\u22252 \u2212  (1 \u2212 \u03c32)a2\n                                                               2\u03b72\n                                                                 k\nwhere we used (25) and (26) in the last inequality. This immediately leads to (27) after rearranging\nthe terms.\nCase II: \u02c6 \u03b7k < \u03b7k. Since 0 < \u03b3k < 1 and xk+1 = (1\u2212\u03b3k)Ak                      xk+1 according to (9), by\n                                                      Ak+\u03b3kak xk + \u03b3k(Ak+ak)\n                                                                     A k+\u03b3kak \u02c6\nJensen\u2019s inequality we have (Ak + \u03b3kak)f(xk+1) \u2264        \u03b3k(Ak + ak)f(\u02c6 xk+1) + (1 \u2212   \u03b3k)Akf(xk), which\nfurther implies that\n (Ak + \u03b3kak)(f(xk+1) \u2212     f\u2217) \u2212 Ak(f(xk) \u2212   f\u2217) \u2264  \u03b3k(Ak + ak)(f(\u02c6 xk+1) \u2212  f\u2217) \u2212  \u03b3kAk(f(xk) \u2212   f\u2217).\nMoreover, since Ak+1 = Ak + \u03b3kak by (9), together with (23) we obtain\n                Ak+1(f(xk+1) \u2212    f\u2217) \u2212 Ak(f(xk) \u2212   f\u2217) \u2264  \u03b3kak\u27e8\u2207f(\u02c6  xk+1), \u02dc\n                                                                              zk+1 \u2212  x\u2217\u27e9.\n                                                    18", "md": "# Math Equations\n\nwhere we used the fact that $$(Ak + ak)\\eta_k = a^2_k$$ in the last equality (cf. (2)). Hence, combining (24) and (25) leads to\n\n$$\\| \\hat{x}_k+1 - y_k + \\hat{\\eta}_k \\nabla f(\\hat{x}_k+1) \\| \\leq \\sigma a_k \\| \\hat{x}_k+1 - y_k \\|.$$ (26)\n\n$$\\| z_k+1 - z_k + 1 \\| = \\| \\tilde{z}_k+1 - z_k - (z_k+1 - z_k) \\| = a_k \\frac{\\eta_k}{\\eta_k}$$ where we used (7) in the last inequality. In the following, we distinguish two cases depending on $$\\hat{\\eta}_k = \\eta_k$$ or $$\\hat{\\eta}_k < \\eta_k$$. In both cases, we shall prove that\n\n$$A_{k+1}(f(x_k+1) - f^*) + \\frac{1}{2} \\| \\hat{z}_k+1 - x^* \\| ^2 \\leq A_k(f(x_k) - f^*) + \\frac{1}{2} \\| z_k - x^* \\| ^2 - (1 - \\sigma^2) a^2_k \\| x_k+1 - y_k \\| ^2.$$ (27)\n\nIf this is true, then Proposition A.1 immediately follows. Indeed, since $$\\sigma < 1$$, the last term in the right-hand side of (27) is negative, which implies (20). Moreover, (21) follows from summing the inequality in (27) from $$k = 0$$ to $$N - 1$$.\n\nCase I: $$\\hat{\\eta}_k = \\eta_k$$. Since by (8) we have $$x_k+1 = \\hat{x}_k+1$$ and $$A_{k+1} = A_k + a_k$$, (23) becomes\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq a_k \\langle \\nabla f(x_k+1), \\tilde{z}_k+1 - x^* \\rangle.$$\n\nUsing $$z_k+1 = z_k - a_k \\nabla f(x_k+1)$$ in (8), we have\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\langle z_k - z_k+1, \\tilde{z}_k+1 - x^* \\rangle$$\n\n$$= \\langle z_k - z_k+1, \\tilde{z}_k+1 - z_k+1 \\rangle + \\langle z_k - z_k+1, z_k+1 - x^* \\rangle$$\n\n$$= \\frac{1}{2} \\| z_k+1 - z_k+1 \\| ^2 - \\frac{1}{2} \\| z_k+1 - z_k \\| ^2$$\n\n$$+ \\frac{1}{2} \\| z_k - x^* \\| ^2 - \\frac{1}{2} \\| z_k+1 - x^* \\| ^2 - a_k \\| x_k+1 - y_k \\| ^2$$\n\n$$\\leq \\frac{1}{2} \\| z_k - x^* \\| ^2 - \\frac{1}{2} \\| z_k+1 - x^* \\| ^2 - (1 - \\sigma^2) a^2_k \\frac{x_k+1 - y_k \\| ^2}{2\\eta^2_k}$$ where we used (25) and (26) in the last inequality. This immediately leads to (27) after rearranging the terms.\n\nCase II: $$\\hat{\\eta}_k < \\eta_k$$. Since $$0 < \\gamma_k < 1$$ and $$x_k+1 = (1 - \\gamma_k)A_k x_k+1$$ according to (9), by Jensen\u2019s inequality we have $$(A_k + \\gamma_k a_k)f(x_k+1) \\leq \\gamma_k(A_k + a_k)f(\\hat{x}_k+1) + (1 - \\gamma_k)A_k f(x_k)$$, which further implies that\n\n$$(A_k + \\gamma_k a_k)(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\gamma_k(A_k + a_k)(f(\\hat{x}_k+1) - f^*) - \\gamma_k A_k(f(x_k) - f^*).$$\n\nMoreover, since $$A_{k+1} = A_k + \\gamma_k a_k$$ by (9), together with (23) we obtain\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\gamma_k a_k \\langle \\nabla f(\\hat{x}_k+1), \\tilde{z}_k+1 - x^* \\rangle.$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "where we used the fact that $$(Ak + ak)\\eta_k = a^2_k$$ in the last equality (cf. (2)). Hence, combining (24) and (25) leads to\n\n$$\\| \\hat{x}_k+1 - y_k + \\hat{\\eta}_k \\nabla f(\\hat{x}_k+1) \\| \\leq \\sigma a_k \\| \\hat{x}_k+1 - y_k \\|.$$ (26)\n\n$$\\| z_k+1 - z_k + 1 \\| = \\| \\tilde{z}_k+1 - z_k - (z_k+1 - z_k) \\| = a_k \\frac{\\eta_k}{\\eta_k}$$ where we used (7) in the last inequality. In the following, we distinguish two cases depending on $$\\hat{\\eta}_k = \\eta_k$$ or $$\\hat{\\eta}_k < \\eta_k$$. In both cases, we shall prove that\n\n$$A_{k+1}(f(x_k+1) - f^*) + \\frac{1}{2} \\| \\hat{z}_k+1 - x^* \\| ^2 \\leq A_k(f(x_k) - f^*) + \\frac{1}{2} \\| z_k - x^* \\| ^2 - (1 - \\sigma^2) a^2_k \\| x_k+1 - y_k \\| ^2.$$ (27)\n\nIf this is true, then Proposition A.1 immediately follows. Indeed, since $$\\sigma < 1$$, the last term in the right-hand side of (27) is negative, which implies (20). Moreover, (21) follows from summing the inequality in (27) from $$k = 0$$ to $$N - 1$$.\n\nCase I: $$\\hat{\\eta}_k = \\eta_k$$. Since by (8) we have $$x_k+1 = \\hat{x}_k+1$$ and $$A_{k+1} = A_k + a_k$$, (23) becomes\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq a_k \\langle \\nabla f(x_k+1), \\tilde{z}_k+1 - x^* \\rangle.$$\n\nUsing $$z_k+1 = z_k - a_k \\nabla f(x_k+1)$$ in (8), we have\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\langle z_k - z_k+1, \\tilde{z}_k+1 - x^* \\rangle$$\n\n$$= \\langle z_k - z_k+1, \\tilde{z}_k+1 - z_k+1 \\rangle + \\langle z_k - z_k+1, z_k+1 - x^* \\rangle$$\n\n$$= \\frac{1}{2} \\| z_k+1 - z_k+1 \\| ^2 - \\frac{1}{2} \\| z_k+1 - z_k \\| ^2$$\n\n$$+ \\frac{1}{2} \\| z_k - x^* \\| ^2 - \\frac{1}{2} \\| z_k+1 - x^* \\| ^2 - a_k \\| x_k+1 - y_k \\| ^2$$\n\n$$\\leq \\frac{1}{2} \\| z_k - x^* \\| ^2 - \\frac{1}{2} \\| z_k+1 - x^* \\| ^2 - (1 - \\sigma^2) a^2_k \\frac{x_k+1 - y_k \\| ^2}{2\\eta^2_k}$$ where we used (25) and (26) in the last inequality. This immediately leads to (27) after rearranging the terms.\n\nCase II: $$\\hat{\\eta}_k < \\eta_k$$. Since $$0 < \\gamma_k < 1$$ and $$x_k+1 = (1 - \\gamma_k)A_k x_k+1$$ according to (9), by Jensen\u2019s inequality we have $$(A_k + \\gamma_k a_k)f(x_k+1) \\leq \\gamma_k(A_k + a_k)f(\\hat{x}_k+1) + (1 - \\gamma_k)A_k f(x_k)$$, which further implies that\n\n$$(A_k + \\gamma_k a_k)(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\gamma_k(A_k + a_k)(f(\\hat{x}_k+1) - f^*) - \\gamma_k A_k(f(x_k) - f^*).$$\n\nMoreover, since $$A_{k+1} = A_k + \\gamma_k a_k$$ by (9), together with (23) we obtain\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\gamma_k a_k \\langle \\nabla f(\\hat{x}_k+1), \\tilde{z}_k+1 - x^* \\rangle.$$", "md": "where we used the fact that $$(Ak + ak)\\eta_k = a^2_k$$ in the last equality (cf. (2)). Hence, combining (24) and (25) leads to\n\n$$\\| \\hat{x}_k+1 - y_k + \\hat{\\eta}_k \\nabla f(\\hat{x}_k+1) \\| \\leq \\sigma a_k \\| \\hat{x}_k+1 - y_k \\|.$$ (26)\n\n$$\\| z_k+1 - z_k + 1 \\| = \\| \\tilde{z}_k+1 - z_k - (z_k+1 - z_k) \\| = a_k \\frac{\\eta_k}{\\eta_k}$$ where we used (7) in the last inequality. In the following, we distinguish two cases depending on $$\\hat{\\eta}_k = \\eta_k$$ or $$\\hat{\\eta}_k < \\eta_k$$. In both cases, we shall prove that\n\n$$A_{k+1}(f(x_k+1) - f^*) + \\frac{1}{2} \\| \\hat{z}_k+1 - x^* \\| ^2 \\leq A_k(f(x_k) - f^*) + \\frac{1}{2} \\| z_k - x^* \\| ^2 - (1 - \\sigma^2) a^2_k \\| x_k+1 - y_k \\| ^2.$$ (27)\n\nIf this is true, then Proposition A.1 immediately follows. Indeed, since $$\\sigma < 1$$, the last term in the right-hand side of (27) is negative, which implies (20). Moreover, (21) follows from summing the inequality in (27) from $$k = 0$$ to $$N - 1$$.\n\nCase I: $$\\hat{\\eta}_k = \\eta_k$$. Since by (8) we have $$x_k+1 = \\hat{x}_k+1$$ and $$A_{k+1} = A_k + a_k$$, (23) becomes\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq a_k \\langle \\nabla f(x_k+1), \\tilde{z}_k+1 - x^* \\rangle.$$\n\nUsing $$z_k+1 = z_k - a_k \\nabla f(x_k+1)$$ in (8), we have\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\langle z_k - z_k+1, \\tilde{z}_k+1 - x^* \\rangle$$\n\n$$= \\langle z_k - z_k+1, \\tilde{z}_k+1 - z_k+1 \\rangle + \\langle z_k - z_k+1, z_k+1 - x^* \\rangle$$\n\n$$= \\frac{1}{2} \\| z_k+1 - z_k+1 \\| ^2 - \\frac{1}{2} \\| z_k+1 - z_k \\| ^2$$\n\n$$+ \\frac{1}{2} \\| z_k - x^* \\| ^2 - \\frac{1}{2} \\| z_k+1 - x^* \\| ^2 - a_k \\| x_k+1 - y_k \\| ^2$$\n\n$$\\leq \\frac{1}{2} \\| z_k - x^* \\| ^2 - \\frac{1}{2} \\| z_k+1 - x^* \\| ^2 - (1 - \\sigma^2) a^2_k \\frac{x_k+1 - y_k \\| ^2}{2\\eta^2_k}$$ where we used (25) and (26) in the last inequality. This immediately leads to (27) after rearranging the terms.\n\nCase II: $$\\hat{\\eta}_k < \\eta_k$$. Since $$0 < \\gamma_k < 1$$ and $$x_k+1 = (1 - \\gamma_k)A_k x_k+1$$ according to (9), by Jensen\u2019s inequality we have $$(A_k + \\gamma_k a_k)f(x_k+1) \\leq \\gamma_k(A_k + a_k)f(\\hat{x}_k+1) + (1 - \\gamma_k)A_k f(x_k)$$, which further implies that\n\n$$(A_k + \\gamma_k a_k)(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\gamma_k(A_k + a_k)(f(\\hat{x}_k+1) - f^*) - \\gamma_k A_k(f(x_k) - f^*).$$\n\nMoreover, since $$A_{k+1} = A_k + \\gamma_k a_k$$ by (9), together with (23) we obtain\n\n$$A_{k+1}(f(x_k+1) - f^*) - A_k(f(x_k) - f^*) \\leq \\gamma_k a_k \\langle \\nabla f(\\hat{x}_k+1), \\tilde{z}_k+1 - x^* \\rangle.$$"}]}, {"page": 19, "text": "Using zk+1 = zk \u2212             \u03b3kak\u2207f(\u02c6      xk+1) in (9), we follow the same reasoning as in (28) to get:\n                                   Ak+1(f(xk+1) \u2212            f\u2217) \u2212     Ak(f(xk) \u2212         f\u2217)\n                               \u2264   \u27e8zk \u2212    zk+1, \u02dc  zk+1 \u2212      x\u2217\u27e9\n                               = \u27e8zk \u2212      zk+1, \u02dc  zk+1 \u2212      zk+1\u27e9    + \u27e8zk \u2212      zk+1, zk+1 \u2212        x\u2217\u27e9\n                                   X\n                               =   1  X  X  X   X  X  XX           zk+1 \u2212     zk+1\u22252 \u2212       1   zk+1 \u2212      zk\u22252\n                                   2\u2225zk \u2212      zk+1\u22252 + 1      2\u2225\u02dc                        X  2\u2225\u02dc\n                                    + 1 2\u2225zk \u2212      x\u2217\u22252 \u2212      1                          1  X  X  X   X  X  XX\n                                                                2\u2225zk+1 \u2212       x\u2217\u22252 \u2212      2\u2225zk \u2212      zk+1\u22252\n                               \u2264   1                                                                 k \u2225\u02c6xk+1 \u2212      yk\u22252,\n                                   2\u2225zk \u2212      x\u2217\u22252 \u2212      1\n                                                           2\u2225zk+1 \u2212        x\u2217\u22252 \u2212      (1 \u2212   \u03c32)a2\n                                                                                            2\u03b72 k\nwhich also leads to (27).\nNext, we prove a lower bound on AN. Recall that B denotes the set of iteration indices where the\nline search scheme backtracks, i.e., B \u225c                       {k : \u02c6 \u03b7k < \u03b7k}.\nLemma A.2. For any N \u2265                        0, it holds that\n                                                AN \u2265       1       \u02c6                                   2\n                                                                   \u03b70 +                            \u02c6\n                                                           4               1\u2264k\u2264N\u22121,k/      \u2208B      \u03b7k     .                                        (29)\nProof. To begin with, according to the update rule of Ak+1 in (8) and (9) and the expression of ak\nin (2), the sequence {Ak} follows the dynamic:\n                        Ak + ak,            if \u02c6                                                                     \u03b7k +        \u03b72\n        Ak+1 =                                 \u03b7k = \u03b7k (k /      \u2208  B);       where \u03b3k = \u02c6       \u03b7k  and ak =                     k + 4\u03b7kAk      .\n                        Ak + \u03b3kak,          if \u02c6                                                 \u03b7k                                2\n                                               \u03b7k < \u03b7k (k \u2208         B),\nSince we initialize A0 = 0, we have a0 = \u03b70. We further have A1 = \u02c6                                    \u03b70, since we get A1 = A0+a0 = \u02c6                \u03b70\n                                                                 \u03b70\nif 0 /\u2208   B, while we get A1 = A0 + \u03b30a0 = \u02c6                     \u03b70 \u03b70 = \u02c6   \u03b70 if 0 \u2208     B. Moreover:\n\u2022 In Case I where k /              \u2208  B, we have   \u03b7k +        \u03b72                                                                      \u221a\u03b7k   2\n            Ak+1 = Ak + ak = Ak +                               k2+ 4\u03b7kAk         \u2265  Ak + \u03b7k   2 +        \u03b7kAk \u2265             Ak +        2      ,\n    which further implies that                    Ak+1 \u2265       \u221aAk +       \u221a\u03b7k    = \u221aAk +         \u221a\u02c6\u03b7k\n                                                                             2                      2 .\n\u2022 In Case II where k \u2208                  B, we have Ak+1 = Ak + \u03b3kak \u2265                       Ak, which implies that                  Ak+1 \u2265     \u221aAk.\n                                                                                                        \u221a \u02c6\nConsidering the above, we obtain \u221aAN \u2265                             \u221aA1 +  1\u2264k\u2264N\u22121,k/                      \u03b7k\n                                                                                                   \u2208B    2 , which leads to (29).\nLemma A.2 provides a lower bound on AN in terms of the step sizes \u02c6                                            \u03b7k in those iterations where\nthe line search scheme does not backtrack, i.e., k /                             \u2208   B. The following lemma shows how we can\nfurther prove a lower bound in terms of all the step sizes {\u02c6                                \u03b7k}N\u22121\n                                                                                                  k=0 .\n                                                                            19", "md": "Using $$zk+1 = zk - \\gamma_k a_k \\nabla f(\\hat{x}_{k+1})$$ in (9), we follow the same reasoning as in (28) to get:\n\n$$\n\\begin{aligned}\n& A_{k+1}(f(x_{k+1}) - f^*) - A_k(f(x_k) - f^*) \\\\\n& \\le \\langle zk - zk+1, \\tilde{zk+1} - x^* \\rangle \\\\\n& = \\langle zk - zk+1, \\tilde{zk+1} - zk+1 \\rangle + \\langle zk - zk+1, zk+1 - x^* \\rangle \\\\\n& = \\frac{1}{2} \\frac{\\|zk+1 - zk+1\\|^2 - 1}{\\|zk - zk+1\\|^2 + 1} \\frac{\\|zk - x^*\\|^2 - 1}{\\|zk+1 - x^*\\|^2 - 2\\|zk - zk+1\\|^2} \\\\\n& \\le \\frac{1}{2} \\frac{\\|x_{k+1} - y_k\\|^2}{\\|zk - x^*\\|^2 - 1} \\frac{\\|zk+1 - x^*\\|^2 - (1 - \\sigma^2)a^2}{2\\eta^2_k}\n\\end{aligned}\n$$\n\nwhich also leads to (27).\n\nNext, we prove a lower bound on AN. Recall that B denotes the set of iteration indices where the line search scheme backtracks, i.e., $$B \\triangleq \\{k : \\hat{\\eta}_k < \\eta_k\\}$$.\n\n**Lemma A.2.** For any N $\\ge$ 0, it holds that\n\n$$A_N \\ge \\frac{1}{\\hat{\\eta}_0 + \\frac{1}{4} \\sum_{1 \\le k \\le N-1, k \\notin B} \\eta_k} \\quad (29)$$\n\n**Proof.** To begin with, according to the update rule of $A_{k+1}$ in (8) and (9) and the expression of $a_k$ in (2), the sequence $\\{A_k\\}$ follows the dynamic:\n\n$$\nA_{k+1} =\n\\begin{cases}\nA_k + a_k, & \\text{if } \\hat{\\eta}_k + \\eta^2_k \\le \\eta_k \\quad (k \\notin B); \\\\\nA_k + \\gamma_k a_k, & \\text{if } \\hat{\\eta}_k < \\eta_k \\quad (k \\in B),\n\\end{cases}\n$$\n\nSince we initialize $A_0 = 0$, we have $a_0 = \\eta_0$. We further have $A_1 = \\hat{\\eta}_0$, since we get $A_1 = A_0+a_0 = \\hat{\\eta}_0$ if 0 $\\notin$ B, while we get $A_1 = A_0 + \\gamma_0a_0 = \\hat{\\eta}_0$ if 0 $\\in$ B. Moreover:\n- In Case I where $k \\notin B$, we have $A_{k+1} = A_k + a_k = A_k + \\sqrt{k^2 + 4\\eta_k A_k} \\ge A_k + \\frac{\\eta_k}{2} + \\eta_k A_k \\ge A_k + 2$, which further implies that $A_{k+1} \\ge \\sqrt{A_k + \\sqrt{\\eta_k}} = \\sqrt{A_k + \\sqrt{\\hat{\\eta}_k}}$.\n- In Case II where $k \\in B$, we have $A_{k+1} = A_k + \\gamma_k a_k \\ge A_k$, which implies that $A_{k+1} \\ge \\sqrt{A_k}$.\n\nConsidering the above, we obtain $\\sqrt{A_N} \\ge \\sqrt{A_1} + \\sum_{1 \\le k \\le N-1, k \\notin B} \\sqrt{\\eta_k}$, which leads to (29).\n\nLemma A.2 provides a lower bound on AN in terms of the step sizes $\\hat{\\eta}_k$ in those iterations where the line search scheme does not backtrack, i.e., $k \\notin B$. The following lemma shows how we can further prove a lower bound in terms of all the step sizes $\\{\\hat{\\eta}_k\\}_{k=0}^{N-1}$.", "images": [], "items": [{"type": "text", "value": "Using $$zk+1 = zk - \\gamma_k a_k \\nabla f(\\hat{x}_{k+1})$$ in (9), we follow the same reasoning as in (28) to get:\n\n$$\n\\begin{aligned}\n& A_{k+1}(f(x_{k+1}) - f^*) - A_k(f(x_k) - f^*) \\\\\n& \\le \\langle zk - zk+1, \\tilde{zk+1} - x^* \\rangle \\\\\n& = \\langle zk - zk+1, \\tilde{zk+1} - zk+1 \\rangle + \\langle zk - zk+1, zk+1 - x^* \\rangle \\\\\n& = \\frac{1}{2} \\frac{\\|zk+1 - zk+1\\|^2 - 1}{\\|zk - zk+1\\|^2 + 1} \\frac{\\|zk - x^*\\|^2 - 1}{\\|zk+1 - x^*\\|^2 - 2\\|zk - zk+1\\|^2} \\\\\n& \\le \\frac{1}{2} \\frac{\\|x_{k+1} - y_k\\|^2}{\\|zk - x^*\\|^2 - 1} \\frac{\\|zk+1 - x^*\\|^2 - (1 - \\sigma^2)a^2}{2\\eta^2_k}\n\\end{aligned}\n$$\n\nwhich also leads to (27).\n\nNext, we prove a lower bound on AN. Recall that B denotes the set of iteration indices where the line search scheme backtracks, i.e., $$B \\triangleq \\{k : \\hat{\\eta}_k < \\eta_k\\}$$.\n\n**Lemma A.2.** For any N $\\ge$ 0, it holds that\n\n$$A_N \\ge \\frac{1}{\\hat{\\eta}_0 + \\frac{1}{4} \\sum_{1 \\le k \\le N-1, k \\notin B} \\eta_k} \\quad (29)$$\n\n**Proof.** To begin with, according to the update rule of $A_{k+1}$ in (8) and (9) and the expression of $a_k$ in (2), the sequence $\\{A_k\\}$ follows the dynamic:\n\n$$\nA_{k+1} =\n\\begin{cases}\nA_k + a_k, & \\text{if } \\hat{\\eta}_k + \\eta^2_k \\le \\eta_k \\quad (k \\notin B); \\\\\nA_k + \\gamma_k a_k, & \\text{if } \\hat{\\eta}_k < \\eta_k \\quad (k \\in B),\n\\end{cases}\n$$\n\nSince we initialize $A_0 = 0$, we have $a_0 = \\eta_0$. We further have $A_1 = \\hat{\\eta}_0$, since we get $A_1 = A_0+a_0 = \\hat{\\eta}_0$ if 0 $\\notin$ B, while we get $A_1 = A_0 + \\gamma_0a_0 = \\hat{\\eta}_0$ if 0 $\\in$ B. Moreover:\n- In Case I where $k \\notin B$, we have $A_{k+1} = A_k + a_k = A_k + \\sqrt{k^2 + 4\\eta_k A_k} \\ge A_k + \\frac{\\eta_k}{2} + \\eta_k A_k \\ge A_k + 2$, which further implies that $A_{k+1} \\ge \\sqrt{A_k + \\sqrt{\\eta_k}} = \\sqrt{A_k + \\sqrt{\\hat{\\eta}_k}}$.\n- In Case II where $k \\in B$, we have $A_{k+1} = A_k + \\gamma_k a_k \\ge A_k$, which implies that $A_{k+1} \\ge \\sqrt{A_k}$.\n\nConsidering the above, we obtain $\\sqrt{A_N} \\ge \\sqrt{A_1} + \\sum_{1 \\le k \\le N-1, k \\notin B} \\sqrt{\\eta_k}$, which leads to (29).\n\nLemma A.2 provides a lower bound on AN in terms of the step sizes $\\hat{\\eta}_k$ in those iterations where the line search scheme does not backtrack, i.e., $k \\notin B$. The following lemma shows how we can further prove a lower bound in terms of all the step sizes $\\{\\hat{\\eta}_k\\}_{k=0}^{N-1}$.", "md": "Using $$zk+1 = zk - \\gamma_k a_k \\nabla f(\\hat{x}_{k+1})$$ in (9), we follow the same reasoning as in (28) to get:\n\n$$\n\\begin{aligned}\n& A_{k+1}(f(x_{k+1}) - f^*) - A_k(f(x_k) - f^*) \\\\\n& \\le \\langle zk - zk+1, \\tilde{zk+1} - x^* \\rangle \\\\\n& = \\langle zk - zk+1, \\tilde{zk+1} - zk+1 \\rangle + \\langle zk - zk+1, zk+1 - x^* \\rangle \\\\\n& = \\frac{1}{2} \\frac{\\|zk+1 - zk+1\\|^2 - 1}{\\|zk - zk+1\\|^2 + 1} \\frac{\\|zk - x^*\\|^2 - 1}{\\|zk+1 - x^*\\|^2 - 2\\|zk - zk+1\\|^2} \\\\\n& \\le \\frac{1}{2} \\frac{\\|x_{k+1} - y_k\\|^2}{\\|zk - x^*\\|^2 - 1} \\frac{\\|zk+1 - x^*\\|^2 - (1 - \\sigma^2)a^2}{2\\eta^2_k}\n\\end{aligned}\n$$\n\nwhich also leads to (27).\n\nNext, we prove a lower bound on AN. Recall that B denotes the set of iteration indices where the line search scheme backtracks, i.e., $$B \\triangleq \\{k : \\hat{\\eta}_k < \\eta_k\\}$$.\n\n**Lemma A.2.** For any N $\\ge$ 0, it holds that\n\n$$A_N \\ge \\frac{1}{\\hat{\\eta}_0 + \\frac{1}{4} \\sum_{1 \\le k \\le N-1, k \\notin B} \\eta_k} \\quad (29)$$\n\n**Proof.** To begin with, according to the update rule of $A_{k+1}$ in (8) and (9) and the expression of $a_k$ in (2), the sequence $\\{A_k\\}$ follows the dynamic:\n\n$$\nA_{k+1} =\n\\begin{cases}\nA_k + a_k, & \\text{if } \\hat{\\eta}_k + \\eta^2_k \\le \\eta_k \\quad (k \\notin B); \\\\\nA_k + \\gamma_k a_k, & \\text{if } \\hat{\\eta}_k < \\eta_k \\quad (k \\in B),\n\\end{cases}\n$$\n\nSince we initialize $A_0 = 0$, we have $a_0 = \\eta_0$. We further have $A_1 = \\hat{\\eta}_0$, since we get $A_1 = A_0+a_0 = \\hat{\\eta}_0$ if 0 $\\notin$ B, while we get $A_1 = A_0 + \\gamma_0a_0 = \\hat{\\eta}_0$ if 0 $\\in$ B. Moreover:\n- In Case I where $k \\notin B$, we have $A_{k+1} = A_k + a_k = A_k + \\sqrt{k^2 + 4\\eta_k A_k} \\ge A_k + \\frac{\\eta_k}{2} + \\eta_k A_k \\ge A_k + 2$, which further implies that $A_{k+1} \\ge \\sqrt{A_k + \\sqrt{\\eta_k}} = \\sqrt{A_k + \\sqrt{\\hat{\\eta}_k}}$.\n- In Case II where $k \\in B$, we have $A_{k+1} = A_k + \\gamma_k a_k \\ge A_k$, which implies that $A_{k+1} \\ge \\sqrt{A_k}$.\n\nConsidering the above, we obtain $\\sqrt{A_N} \\ge \\sqrt{A_1} + \\sum_{1 \\le k \\le N-1, k \\notin B} \\sqrt{\\eta_k}$, which leads to (29).\n\nLemma A.2 provides a lower bound on AN in terms of the step sizes $\\hat{\\eta}_k$ in those iterations where the line search scheme does not backtrack, i.e., $k \\notin B$. The following lemma shows how we can further prove a lower bound in terms of all the step sizes $\\{\\hat{\\eta}_k\\}_{k=0}^{N-1}$."}]}, {"page": 20, "text": "Lemma A.3. We have\n                                               1    \uf8eb                           \uf8f6\n                                      \u02c6             \uf8ed                           \uf8f8.\n                                      \u03b7k \u2264              \u02c6\n                                                        \u03b70 +                 \u02c6\n                      1\u2264k\u2264N\u22121,k\u2208B          1 \u2212 \u221a\u03b2            1\u2264k\u2264N\u22121,k/ \u2208B   \u03b7k                     (30)\nAs a corollary, we have\n                               \u02c6                                  N\u22121\n                               \u03b70 +                 \u02c6\n                                                    \u03b7k \u2265  1 \u2212 \u221a\u03b2         \u02c6\n                                    1\u2264k\u2264N\u22121,k/ \u2208B         2 \u2212 \u221a\u03b2  k=0    \u03b7k.                        (31)\nProof. When the line search scheme backtracks, i.e., k \u2208     B, we have \u02c6\u03b7k \u2264 \u03b2\u03b7k. Therefore,\n                            \u02c6                           N\u22121                    N\u22122\n            1\u2264k\u2264N\u22121,k\u2208B     \u03b7k \u2264 1\u2264k\u2264N\u22121,k\u2208B     \u03b2\u03b7k \u2264  k=1    \u03b2\u03b7k =    \u03b2\u03b71 +  k=1    \u03b2\u03b7k+1.        (32)\nMoreover, in the update of Algorithm 1, we have \u03b7k+1 = \u02c6     \u03b7k/\u03b2 if k /\notherwise (cf. Line 13). This implies that \u03b71 \u2264    \u02c6                   \u2208  B (cf. Line 8) and \u03b7k+1 = \u02c6 \u03b7k\n                                                   \u03b70/\u03b2 and we further have\n              \u03b2\u03b71 +  N\u22122   \u03b2\u03b7k+1 =     \u03b2\u03b71 +                  \u03b2\u03b7k+1 +                  \u03b2\u03b7k+1\n                     k=1          \u2264    \u02c6      1\u2264k\u2264N\u22122,k/\u2208B             1\u2264k\u2264N\u22122,k\u2208B\n                                       \u03b70 +                 \u02c6\n                                                            \u03b7k +                 \u03b2\u02c6\u03b7k\n                                  \u2264    \u02c6    1\u2264k\u2264N\u22122,k/ \u2208B         1\u2264k\u2264N\u22122,k\u2208B\n                                       \u03b70 +                 \u02c6\n                                                            \u03b7k +                 \u03b2\u02c6\u03b7k.              (33)\nWe combine (32) and (33) to get             1\u2264k\u2264N\u22121,k/ \u2208B         1\u2264k\u2264N\u22121,k\u2208B\n                                 \u02c6\n                                 \u03b7k \u2264    \u02c6\n                                         \u03b70 +                 \u02c6\n                                                              \u03b7k +                 \u03b2\u02c6\n                                                                                    \u03b7k.\n                 1\u2264k\u2264N\u22121,k\u2208B                  1\u2264k\u2264N\u22121,k/\u2208B         1\u2264k\u2264N\u22121,k\u2208B\nBy rearranging the terms and simple algebraic manipulation, we obtain (30) as desired. Finally,\n(31) follows by adding \u221a\u02c6                       \u221a \u02c6\n                          \u03b70 +    1\u2264k\u2264N\u22121,k/ \u2208B   \u03b7k to both sides of (30).\nNow we are ready to prove Proposition 3.1.\nProof of Proposition 3.1. By Proposition A.1, the potential function \u03d5k \u225c     Ak(f(xk) \u2212f\u2217) + 1  2\u2225zk \u2212\nx\u2217\u22252 is non-increasing in each iteration. Hence, via a recursive augment we have AN(f(xN) \u2212f\u2217) \u2264\n\u03d5N \u2264   \u00b7 \u00b7 \u00b7 \u2264\u03d50 = 1                                                . Moreover, combining Lemma A.2\n                   2\u2225z0\u2212x\u2217\u22252, which yields f(xN)\u2212f\u2217      \u2264  \u2225z0\u2212x\u2217\u22252\n                                                              2AN\nand (31) in Lemma A.3 leads to the second inequality in Proposition 3.1.\nA.2     Additional Supporting Lemmas\nA crucial part of our analysis is to bound the path length of the sequence {yk}N   k=0. This is done in\nLemma A.7. To achieve this goal we first present the results in Lemmas A.4-A.6, which provide\nthe required ingredients for proving the claim in Lemma A.7. In our first intermediate result, we\nestablish uniform upper bounds for the error terms \u2225zk \u2212      x\u2217\u2225 and \u2225xk \u2212   x\u2217\u2225.\n                                                   20", "md": "# Math Equations\n\n## Lemma A.3\n\nWe have\n\n$$\n\\eta_k \\leq \\hat{\\eta}_0 + \\hat{\\eta}_{1-\\sqrt{\\beta}} \\quad \\text{(30)}\n$$\nAs a corollary, we have\n\n$$\n\\hat{\\eta}_0 + \\hat{\\eta}_k \\geq 1-\\sqrt{\\beta} \\hat{\\eta}_{2-\\sqrt{\\beta}} \\quad \\text{(31)}\n$$\nProof: When the line search scheme backtracks, i.e., k \u2208 B, we have $\\hat{\\eta}_k \\leq \\beta\\eta_k$. Therefore,\n\n$$\n\\sum_{k=1}^{N-1, k\\in B} \\eta_k \\leq \\sum_{k=1}^{N-1, k\\in B} \\beta\\eta_k = \\beta\\eta_1 + \\sum_{k=1}^{N-2} \\beta\\eta_{k+1} \\quad \\text{(32)}\n$$\nMoreover, in the update of Algorithm 1, we have $\\eta_{k+1} = \\hat{\\eta}_k/\\beta$ if $k \\notin B$ otherwise (cf. Line 13). This implies that $\\eta_1 \\leq \\hat{\\eta}_0/\\beta$ and we further have\n\n$$\n\\beta\\eta_1 + \\sum_{k=1}^{N-2} \\beta\\eta_{k+1} \\leq \\sum_{k=1}^{N-2, k\\notin B} \\hat{\\eta}_k + \\sum_{k=1}^{N-2, k\\in B} \\beta\\hat{\\eta}_k \\quad \\text{(33)}\n$$\nWe combine (32) and (33) to get\n\n$$\n\\sum_{k=1}^{N-1, k\\notin B} \\hat{\\eta}_k \\leq \\hat{\\eta}_0 + \\sum_{k=1}^{N-1, k\\in B} \\hat{\\eta}_k + \\beta\\sum_{k=1}^{N-1} \\hat{\\eta}_k.\n$$\nBy rearranging the terms and simple algebraic manipulation, we obtain (30) as desired. Finally, (31) follows by adding $\\sqrt{\\hat{\\eta}_0} + \\sqrt{\\sum_{k=1}^{N-1, k\\notin B} \\eta_k}$ to both sides of (30).\n\n## Proof of Proposition 3.1\n\nBy Proposition A.1, the potential function $\\phi_k \\triangleq A_k(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2$ is non-increasing in each iteration. Hence, via a recursive augment we have $A_N(f(x_N) - f^*) \\leq \\phi_N \\leq \\ldots \\leq \\phi_0 = \\frac{1}{2}\\|z_0 - x^*\\|^2$. Moreover, combining Lemma A.2 which yields $f(x_N) - f^* \\leq \\frac{\\|z_0 - x^*\\|^2}{2A_N}$ and (31) in Lemma A.3 leads to the second inequality in Proposition 3.1.\n\n## Additional Supporting Lemmas\n\nA crucial part of our analysis is to bound the path length of the sequence $\\{y_k\\}_{k=0}^N$. This is done in Lemma A.7. To achieve this goal we first present the results in Lemmas A.4-A.6, which provide the required ingredients for proving the claim in Lemma A.7. In our first intermediate result, we establish uniform upper bounds for the error terms $\\|z_k - x^*\\|$ and $\\|x_k - x^*\\|$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "heading", "lvl": 2, "value": "Lemma A.3", "md": "## Lemma A.3"}, {"type": "text", "value": "We have\n\n$$\n\\eta_k \\leq \\hat{\\eta}_0 + \\hat{\\eta}_{1-\\sqrt{\\beta}} \\quad \\text{(30)}\n$$\nAs a corollary, we have\n\n$$\n\\hat{\\eta}_0 + \\hat{\\eta}_k \\geq 1-\\sqrt{\\beta} \\hat{\\eta}_{2-\\sqrt{\\beta}} \\quad \\text{(31)}\n$$\nProof: When the line search scheme backtracks, i.e., k \u2208 B, we have $\\hat{\\eta}_k \\leq \\beta\\eta_k$. Therefore,\n\n$$\n\\sum_{k=1}^{N-1, k\\in B} \\eta_k \\leq \\sum_{k=1}^{N-1, k\\in B} \\beta\\eta_k = \\beta\\eta_1 + \\sum_{k=1}^{N-2} \\beta\\eta_{k+1} \\quad \\text{(32)}\n$$\nMoreover, in the update of Algorithm 1, we have $\\eta_{k+1} = \\hat{\\eta}_k/\\beta$ if $k \\notin B$ otherwise (cf. Line 13). This implies that $\\eta_1 \\leq \\hat{\\eta}_0/\\beta$ and we further have\n\n$$\n\\beta\\eta_1 + \\sum_{k=1}^{N-2} \\beta\\eta_{k+1} \\leq \\sum_{k=1}^{N-2, k\\notin B} \\hat{\\eta}_k + \\sum_{k=1}^{N-2, k\\in B} \\beta\\hat{\\eta}_k \\quad \\text{(33)}\n$$\nWe combine (32) and (33) to get\n\n$$\n\\sum_{k=1}^{N-1, k\\notin B} \\hat{\\eta}_k \\leq \\hat{\\eta}_0 + \\sum_{k=1}^{N-1, k\\in B} \\hat{\\eta}_k + \\beta\\sum_{k=1}^{N-1} \\hat{\\eta}_k.\n$$\nBy rearranging the terms and simple algebraic manipulation, we obtain (30) as desired. Finally, (31) follows by adding $\\sqrt{\\hat{\\eta}_0} + \\sqrt{\\sum_{k=1}^{N-1, k\\notin B} \\eta_k}$ to both sides of (30).", "md": "We have\n\n$$\n\\eta_k \\leq \\hat{\\eta}_0 + \\hat{\\eta}_{1-\\sqrt{\\beta}} \\quad \\text{(30)}\n$$\nAs a corollary, we have\n\n$$\n\\hat{\\eta}_0 + \\hat{\\eta}_k \\geq 1-\\sqrt{\\beta} \\hat{\\eta}_{2-\\sqrt{\\beta}} \\quad \\text{(31)}\n$$\nProof: When the line search scheme backtracks, i.e., k \u2208 B, we have $\\hat{\\eta}_k \\leq \\beta\\eta_k$. Therefore,\n\n$$\n\\sum_{k=1}^{N-1, k\\in B} \\eta_k \\leq \\sum_{k=1}^{N-1, k\\in B} \\beta\\eta_k = \\beta\\eta_1 + \\sum_{k=1}^{N-2} \\beta\\eta_{k+1} \\quad \\text{(32)}\n$$\nMoreover, in the update of Algorithm 1, we have $\\eta_{k+1} = \\hat{\\eta}_k/\\beta$ if $k \\notin B$ otherwise (cf. Line 13). This implies that $\\eta_1 \\leq \\hat{\\eta}_0/\\beta$ and we further have\n\n$$\n\\beta\\eta_1 + \\sum_{k=1}^{N-2} \\beta\\eta_{k+1} \\leq \\sum_{k=1}^{N-2, k\\notin B} \\hat{\\eta}_k + \\sum_{k=1}^{N-2, k\\in B} \\beta\\hat{\\eta}_k \\quad \\text{(33)}\n$$\nWe combine (32) and (33) to get\n\n$$\n\\sum_{k=1}^{N-1, k\\notin B} \\hat{\\eta}_k \\leq \\hat{\\eta}_0 + \\sum_{k=1}^{N-1, k\\in B} \\hat{\\eta}_k + \\beta\\sum_{k=1}^{N-1} \\hat{\\eta}_k.\n$$\nBy rearranging the terms and simple algebraic manipulation, we obtain (30) as desired. Finally, (31) follows by adding $\\sqrt{\\hat{\\eta}_0} + \\sqrt{\\sum_{k=1}^{N-1, k\\notin B} \\eta_k}$ to both sides of (30)."}, {"type": "heading", "lvl": 2, "value": "Proof of Proposition 3.1", "md": "## Proof of Proposition 3.1"}, {"type": "text", "value": "By Proposition A.1, the potential function $\\phi_k \\triangleq A_k(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2$ is non-increasing in each iteration. Hence, via a recursive augment we have $A_N(f(x_N) - f^*) \\leq \\phi_N \\leq \\ldots \\leq \\phi_0 = \\frac{1}{2}\\|z_0 - x^*\\|^2$. Moreover, combining Lemma A.2 which yields $f(x_N) - f^* \\leq \\frac{\\|z_0 - x^*\\|^2}{2A_N}$ and (31) in Lemma A.3 leads to the second inequality in Proposition 3.1.", "md": "By Proposition A.1, the potential function $\\phi_k \\triangleq A_k(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2$ is non-increasing in each iteration. Hence, via a recursive augment we have $A_N(f(x_N) - f^*) \\leq \\phi_N \\leq \\ldots \\leq \\phi_0 = \\frac{1}{2}\\|z_0 - x^*\\|^2$. Moreover, combining Lemma A.2 which yields $f(x_N) - f^* \\leq \\frac{\\|z_0 - x^*\\|^2}{2A_N}$ and (31) in Lemma A.3 leads to the second inequality in Proposition 3.1."}, {"type": "heading", "lvl": 2, "value": "Additional Supporting Lemmas", "md": "## Additional Supporting Lemmas"}, {"type": "text", "value": "A crucial part of our analysis is to bound the path length of the sequence $\\{y_k\\}_{k=0}^N$. This is done in Lemma A.7. To achieve this goal we first present the results in Lemmas A.4-A.6, which provide the required ingredients for proving the claim in Lemma A.7. In our first intermediate result, we establish uniform upper bounds for the error terms $\\|z_k - x^*\\|$ and $\\|x_k - x^*\\|$.", "md": "A crucial part of our analysis is to bound the path length of the sequence $\\{y_k\\}_{k=0}^N$. This is done in Lemma A.7. To achieve this goal we first present the results in Lemmas A.4-A.6, which provide the required ingredients for proving the claim in Lemma A.7. In our first intermediate result, we establish uniform upper bounds for the error terms $\\|z_k - x^*\\|$ and $\\|x_k - x^*\\|$."}]}, {"page": 21, "text": " Lemma A.4. Recall that \u03c3 = \u03b11 + \u03b12. For all k \u2265                 0, we have \u2225zk \u2212     x\u2217\u2225  \u2264   \u2225z0 \u2212  x\u2217\u2225  and\n                    2\n \u2225xk \u2212  x\u2217\u2225 \u2264     1\u2212\u03c32 \u2225z0 \u2212  x\u2217\u2225.\n Proof. To begin with, it follows from (20) in Proposition A.1 that\n   1\n   2\u2225zk \u2212  x\u2217\u22252 \u2264   Ak(f(xk) \u2212   f\u2217) + 12\u2225zk \u2212   x\u2217\u22252 \u2264   A0(f(x0) \u2212   f\u2217) + 12\u2225z0 \u2212   x\u2217\u22252 = 12\u2225z0 \u2212   x\u2217\u22252.\n Hence, we get \u2225zk \u2212    x\u2217\u2225  \u2264 \u2225z0 \u2212  x\u2217\u2225  for any k \u2265   0. To show the second inequality, we distinguish\n two cases and in both cases we will prove that\n  Ak+1\u2225xk+1\u2212x\u2217\u22252 \u2264       Ak\u2225xk\u2212x\u2217\u22252+(Ak+1\u2212Ak)2\u03c32a2\u2225\u02c6     \u03b72 k  xk+1\u2212yk\u22252+2(Ak+1\u2212Ak)\u2225zk+1\u2212x\u2217\u22252. (34)\n                                                          k\n Case I: \u02c6\u03b7k = \u03b7k. Recall that in the proof of Proposition A.1 we defined \u02dc    zk+1 = \u02c6 xk+1+ Aka   xk+1\u2212xk).\n                                                                                                 k (\u02c6\n Since xk+1 = \u02c6 xk+1, we have xk+1 =        Ak           ak  zk+1 and by Jensen\u2019s inequality\n                                          Ak+ak xk +   Ak+ak \u02dc\n                      \u2225xk+1 \u2212   x\u2217\u22252 \u2264     Ak     \u2225xk \u2212  x\u2217\u22252 +      ak    \u2225\u02dc\n Furthermore, we have                    Ak + ak                  Ak + ak   zk+1 \u2212  x\u2217\u22252.\n                                                                    k \u2225\u02c6\n    \u2225\u02dc                                                                 xk+1 \u2212  yk\u22252 + 2\u2225zk+1 \u2212     x\u2217\u22252,   (35)\n     zk+1 \u2212  x\u2217\u22252 \u2264   2\u2225\u02dc\n                        zk+1 \u2212  zk+1\u22252 + 2\u2225zk+1 \u2212     x\u2217\u22252 \u2264   2\u03c32a2\n                                                                 \u03b72\n                                                                  k\nwhere we used (26) in the last inequality. By combining the above two inequalities, we obtain\n        (Ak + ak)\u2225xk+1 \u2212     x\u2217\u22252 \u2264   Ak\u2225xk \u2212   x\u2217\u22252 + ak  2\u03c32a2 k\u2225\u02c6\nwhich leads to (34) (note that Ak+1 = Ak + ak in Case I).    \u03b72k   xk+1 \u2212   yk\u22252 + 2ak\u2225zk+1 \u2212     x\u2217\u22252,\n Case II: Since xk+1 = (1\u2212\u03b3k)Ak                      xk+1 and \u02c6 xk+1 =     A k          ak   zk+1, we have\n                            Ak+\u03b3kak xk + \u03b3k(Ak+ak)\n                                            Ak+\u03b3kak \u02c6                    Ak+ak xk +   Ak+ak \u02dc\n                                               Ak               \u03b3kak    \u02dc\n                                  xk+1 =   Ak + \u03b3kak   xk +  Ak + \u03b3kak  zk+1.\n Similarly, by Jensen\u2019s inequality we have\n                     (Ak + \u03b3kak)\u2225xk+1 \u2212     x\u2217\u22252 \u2264  Ak\u2225xk \u2212    x\u2217\u22252 + \u03b3kak\u2225\u02dc zk+1 \u2212   x\u2217\u22252.\n Combining this inequality with (35), we obtain\n                                                           2\u03c32a2\n   (Ak + \u03b3kak)\u2225xk+1 \u2212     x\u2217\u22252 \u2264  Ak\u2225xk \u2212    x\u2217\u22252 + \u03b3kak     \u03b72 k\u2225\u02c6xk+1 \u2212  yk\u22252 + 2\u03b3kak\u2225zk+1 \u2212     x\u2217\u22252. (36)\n                                                              k\nwhich leads to (34) (note that Ak+1 = Ak + \u03b3kak in Case II).\n                                                       21", "md": "Lemma A.4. Recall that $$\\sigma = \\alpha_1 + \\alpha_2$$. For all $$k \\geq 0$$, we have $$\\|z_k - x^*\\| \\leq \\|z_0 - x^*\\|$$ and\n$$\\|x_k - x^*\\| \\leq (1-\\sigma^2) \\|z_0 - x^*\\|$$.\nProof. To begin with, it follows from (20) in Proposition A.1 that\n$$\n\\frac{1}{2}\\|z_k - x^*\\|^2 \\leq A_k(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2 \\leq A_0(f(x_0) - f^*) + \\frac{1}{2}\\|z_0 - x^*\\|^2 = \\frac{1}{2}\\|z_0 - x^*\\|^2.\n$$\nHence, we get $$\\|z_k - x^*\\| \\leq \\|z_0 - x^*\\|$$ for any $$k \\geq 0$$. To show the second inequality, we distinguish\ntwo cases and in both cases we will prove that\n$$\nA_{k+1}\\|x_{k+1}-x^*\\|^2 \\leq A_k\\|x_k-x^*\\|^2+(A_{k+1}-A_k)^2\\sigma^2a^2\\|\\hat{\\eta}_k x_{k+1}-y_k\\|^2+2(A_{k+1}-A_k)\\|z_{k+1}-x^*\\|^2. \\tag{34}\n$$\nCase I: $$\\hat{\\eta}_k = \\eta_k$$. Recall that in the proof of Proposition A.1 we defined $$\\tilde{z}_{k+1} = \\hat{x}_{k+1}+A_ka\\hat{x}_{k+1}-x_k)$$.\nSince $$x_{k+1} = \\hat{x}_{k+1}$$, we have $$x_{k+1} = A_k a z_{k+1}$$ and by Jensen\u2019s inequality\n$$\n\\|x_{k+1} - x^*\\|^2 \\leq A_k \\|x_k - x^*\\|^2 + a^2\\|\\hat{x}_{k+1} - x^*\\|^2.\n$$\nFurthermore, we have\n$$\n\\| \\hat{x}_{k+1} - x^*\\|^2 \\leq 2\\|\\hat{x}_{k+1} - z_{k+1}\\|^2 + 2\\|z_{k+1} - x^*\\|^2 \\leq 2\\sigma^2a^2\\eta_k^2\n$$\nwhere we used (26) in the last inequality. By combining the above two inequalities, we obtain\n$$\n(A_k + a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + a^2\\sigma^2\\eta_k^2\\|\\hat{\\eta}_k x_{k+1} - y_k\\|^2 + 2a\\|z_{k+1} - x^*\\|^2,\n$$\nwhich leads to (34) (note that $$A_{k+1} = A_k + a$$ in Case I).\nCase II: Since $$x_{k+1} = (1-\\gamma_k)A_k x_{k+1}$$ and $$\\hat{x}_{k+1} = A_k a z_{k+1}$$, we have\n$$\n(A_k+\\gamma_k a) x_{k+1} + \\gamma_k(A_k+a)\\hat{x}_{k+1} = A_k + \\gamma_k a x_k + A_k + \\gamma_k a z_{k+1}.\n$$\nSimilarly, by Jensen\u2019s inequality we have\n$$\n(A_k + \\gamma_k a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + \\gamma_k a\\|\\hat{z}_{k+1} - x^*\\|^2.\n$$\nCombining this inequality with (35), we obtain\n$$\n(A_k + \\gamma_k a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + \\gamma_k a \\eta_k^2\\|\\hat{x}_{k+1} - y_k\\|^2 + 2\\gamma_k a\\|z_{k+1} - x^*\\|^2. \\tag{36}\n$$\nwhich leads to (34) (note that $$A_{k+1} = A_k + \\gamma_k a$$ in Case II).", "images": [], "items": [{"type": "text", "value": "Lemma A.4. Recall that $$\\sigma = \\alpha_1 + \\alpha_2$$. For all $$k \\geq 0$$, we have $$\\|z_k - x^*\\| \\leq \\|z_0 - x^*\\|$$ and\n$$\\|x_k - x^*\\| \\leq (1-\\sigma^2) \\|z_0 - x^*\\|$$.\nProof. To begin with, it follows from (20) in Proposition A.1 that\n$$\n\\frac{1}{2}\\|z_k - x^*\\|^2 \\leq A_k(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2 \\leq A_0(f(x_0) - f^*) + \\frac{1}{2}\\|z_0 - x^*\\|^2 = \\frac{1}{2}\\|z_0 - x^*\\|^2.\n$$\nHence, we get $$\\|z_k - x^*\\| \\leq \\|z_0 - x^*\\|$$ for any $$k \\geq 0$$. To show the second inequality, we distinguish\ntwo cases and in both cases we will prove that\n$$\nA_{k+1}\\|x_{k+1}-x^*\\|^2 \\leq A_k\\|x_k-x^*\\|^2+(A_{k+1}-A_k)^2\\sigma^2a^2\\|\\hat{\\eta}_k x_{k+1}-y_k\\|^2+2(A_{k+1}-A_k)\\|z_{k+1}-x^*\\|^2. \\tag{34}\n$$\nCase I: $$\\hat{\\eta}_k = \\eta_k$$. Recall that in the proof of Proposition A.1 we defined $$\\tilde{z}_{k+1} = \\hat{x}_{k+1}+A_ka\\hat{x}_{k+1}-x_k)$$.\nSince $$x_{k+1} = \\hat{x}_{k+1}$$, we have $$x_{k+1} = A_k a z_{k+1}$$ and by Jensen\u2019s inequality\n$$\n\\|x_{k+1} - x^*\\|^2 \\leq A_k \\|x_k - x^*\\|^2 + a^2\\|\\hat{x}_{k+1} - x^*\\|^2.\n$$\nFurthermore, we have\n$$\n\\| \\hat{x}_{k+1} - x^*\\|^2 \\leq 2\\|\\hat{x}_{k+1} - z_{k+1}\\|^2 + 2\\|z_{k+1} - x^*\\|^2 \\leq 2\\sigma^2a^2\\eta_k^2\n$$\nwhere we used (26) in the last inequality. By combining the above two inequalities, we obtain\n$$\n(A_k + a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + a^2\\sigma^2\\eta_k^2\\|\\hat{\\eta}_k x_{k+1} - y_k\\|^2 + 2a\\|z_{k+1} - x^*\\|^2,\n$$\nwhich leads to (34) (note that $$A_{k+1} = A_k + a$$ in Case I).\nCase II: Since $$x_{k+1} = (1-\\gamma_k)A_k x_{k+1}$$ and $$\\hat{x}_{k+1} = A_k a z_{k+1}$$, we have\n$$\n(A_k+\\gamma_k a) x_{k+1} + \\gamma_k(A_k+a)\\hat{x}_{k+1} = A_k + \\gamma_k a x_k + A_k + \\gamma_k a z_{k+1}.\n$$\nSimilarly, by Jensen\u2019s inequality we have\n$$\n(A_k + \\gamma_k a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + \\gamma_k a\\|\\hat{z}_{k+1} - x^*\\|^2.\n$$\nCombining this inequality with (35), we obtain\n$$\n(A_k + \\gamma_k a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + \\gamma_k a \\eta_k^2\\|\\hat{x}_{k+1} - y_k\\|^2 + 2\\gamma_k a\\|z_{k+1} - x^*\\|^2. \\tag{36}\n$$\nwhich leads to (34) (note that $$A_{k+1} = A_k + \\gamma_k a$$ in Case II).", "md": "Lemma A.4. Recall that $$\\sigma = \\alpha_1 + \\alpha_2$$. For all $$k \\geq 0$$, we have $$\\|z_k - x^*\\| \\leq \\|z_0 - x^*\\|$$ and\n$$\\|x_k - x^*\\| \\leq (1-\\sigma^2) \\|z_0 - x^*\\|$$.\nProof. To begin with, it follows from (20) in Proposition A.1 that\n$$\n\\frac{1}{2}\\|z_k - x^*\\|^2 \\leq A_k(f(x_k) - f^*) + \\frac{1}{2}\\|z_k - x^*\\|^2 \\leq A_0(f(x_0) - f^*) + \\frac{1}{2}\\|z_0 - x^*\\|^2 = \\frac{1}{2}\\|z_0 - x^*\\|^2.\n$$\nHence, we get $$\\|z_k - x^*\\| \\leq \\|z_0 - x^*\\|$$ for any $$k \\geq 0$$. To show the second inequality, we distinguish\ntwo cases and in both cases we will prove that\n$$\nA_{k+1}\\|x_{k+1}-x^*\\|^2 \\leq A_k\\|x_k-x^*\\|^2+(A_{k+1}-A_k)^2\\sigma^2a^2\\|\\hat{\\eta}_k x_{k+1}-y_k\\|^2+2(A_{k+1}-A_k)\\|z_{k+1}-x^*\\|^2. \\tag{34}\n$$\nCase I: $$\\hat{\\eta}_k = \\eta_k$$. Recall that in the proof of Proposition A.1 we defined $$\\tilde{z}_{k+1} = \\hat{x}_{k+1}+A_ka\\hat{x}_{k+1}-x_k)$$.\nSince $$x_{k+1} = \\hat{x}_{k+1}$$, we have $$x_{k+1} = A_k a z_{k+1}$$ and by Jensen\u2019s inequality\n$$\n\\|x_{k+1} - x^*\\|^2 \\leq A_k \\|x_k - x^*\\|^2 + a^2\\|\\hat{x}_{k+1} - x^*\\|^2.\n$$\nFurthermore, we have\n$$\n\\| \\hat{x}_{k+1} - x^*\\|^2 \\leq 2\\|\\hat{x}_{k+1} - z_{k+1}\\|^2 + 2\\|z_{k+1} - x^*\\|^2 \\leq 2\\sigma^2a^2\\eta_k^2\n$$\nwhere we used (26) in the last inequality. By combining the above two inequalities, we obtain\n$$\n(A_k + a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + a^2\\sigma^2\\eta_k^2\\|\\hat{\\eta}_k x_{k+1} - y_k\\|^2 + 2a\\|z_{k+1} - x^*\\|^2,\n$$\nwhich leads to (34) (note that $$A_{k+1} = A_k + a$$ in Case I).\nCase II: Since $$x_{k+1} = (1-\\gamma_k)A_k x_{k+1}$$ and $$\\hat{x}_{k+1} = A_k a z_{k+1}$$, we have\n$$\n(A_k+\\gamma_k a) x_{k+1} + \\gamma_k(A_k+a)\\hat{x}_{k+1} = A_k + \\gamma_k a x_k + A_k + \\gamma_k a z_{k+1}.\n$$\nSimilarly, by Jensen\u2019s inequality we have\n$$\n(A_k + \\gamma_k a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + \\gamma_k a\\|\\hat{z}_{k+1} - x^*\\|^2.\n$$\nCombining this inequality with (35), we obtain\n$$\n(A_k + \\gamma_k a)\\|x_{k+1} - x^*\\|^2 \\leq A_k\\|x_k - x^*\\|^2 + \\gamma_k a \\eta_k^2\\|\\hat{x}_{k+1} - y_k\\|^2 + 2\\gamma_k a\\|z_{k+1} - x^*\\|^2. \\tag{36}\n$$\nwhich leads to (34) (note that $$A_{k+1} = A_k + \\gamma_k a$$ in Case II)."}]}, {"page": 22, "text": "Now by summing (34) over k = 0, . . . , N \u2212                          1, we get\n    AN\u2225xN \u2212         x\u2217\u22252 \u2264      N\u22121   (Ak+1 \u2212       Ak)2\u03c32a2 \u03b72k  k \u2225\u02c6xk+1 \u2212      yk\u22252 +     N\u22121    2(Ak+1 \u2212        Ak)\u2225zk+1 \u2212         x\u2217\u22252          (37)\n                                 k=0                                                          k=0\n                            \u2264   2\u03c32   N\u22121    (Ak+1 \u2212      Ak)    N\u22121    a2k \u2225\u02c6xk+1 \u2212      yk\u22252 + 2\u2225z0 \u2212          x\u2217\u22252   N\u22121   (Ak+1 \u2212       Ak)      (38)\n                                       k=0                       k=0    \u03b72k                                              k=0\n                                   2\u03c32\n                            \u2264    1 \u2212   \u03c32 AN\u2225z0 \u2212         x\u2217\u22252 + 2AN\u2225z0 \u2212             x\u2217\u22252                                                           (39)\n                            = 2AN1 \u2212   \u03c32 \u2225z0 \u2212      x\u2217\u22252.      2                                                                                    (40)\nHence, this implies that \u2225xk \u2212                   x\u2217\u22252 \u2264       1\u2212\u03c32 \u2225z0 \u2212      x\u2217\u22252 for any k \u2265            0.\n                                                                                     a k+1\nA key term appearing in several of our bounds is                                 A k+1+ak+1 . In the next lemma, we establish an\nupper bound for this ratio based on a factor of its previous value, for both cases of our algorithm.\n                                                                                                                             ak+1             1     ak\nLemma A.5. Without loss of generality assume \u03b2 > 1/5. In Case I we have                                                 A k+1+ak+1 \u2264         \u221a \u03b2 Ak+ak .\n                                                        ak+1             2\u221a\u03b2       ak\nOtherwise, in Case II we have                      A  k+1+ak+1 \u2264        \u221a \u03b2+1   Ak+ak .\nProof. By the choice of ak in (2) we have \u03b7k(Ak + ak) = a2                                  k for all k \u2265        0. As a result, we have\n                                        ak                                2\u03b7k                              2\nand similarly                      Ak + ak       = \u03b7kak   =   \u03b7k +        \u03b72k + 4\u03b7kAk        =   1 +       1 + 4 Ak \u03b7k  ,\n                                                          ak+1                           2\n                                                    Ak+1 + ak+1          =   1 +       1 + 4 Ak+1      .\n                                                                                                \u03b7k+1\nIn Case I, we have \u03b7k+1 = \u03b7k/\u03b2 and Ak+1 \u2265                                   Ak. Hence, it implies that Ak+1/\u03b7k+1 \u2265                             \u03b2Ak/\u03b7k,\nwhich leads to\n              ak+1                         2                               2                    1              2                  1        ak\n       Ak+1 + ak+1          \u2264    1 +      1 + 4\u03b2Ak \u03b7k     \u2264   \u221a  \u03b2 +       \u03b2 + 4\u03b2Ak \u03b7k     =   \u221a\u03b2    1 +       1 + 4 Ak\u03b7 k  =    \u221a\u03b2    Ak + ak      .\nwhere the second inequality follows from the fact that \u03b2 \u2264                                    1.\nIn Case II, we have \u03b7k+1 = \u02c6                   \u03b7k = \u03b3k\u03b7k and Ak+1 = Ak + \u03b3kak. Since we also have ak \u2265                                          \u03b7k and\n\u03b3k \u2264     \u03b2, we obtain Ak+1/\u03b7k+1 \u2265                  Ak/(\u03b3k\u03b7k) + 1 \u2265            Ak/(\u03b2\u03b7k) + 1. Hence,\n         ak+1                         2                             2                     2\u221a\u03b2                 2                  2\u221a\u03b2            ak\n   Ak+1 + ak+1          \u2264   1 +       5 + 4Ak      \u2264   1 +    \u221a1\u03b2     1 + 4Ak      \u2264    \u221a\u03b2 + 1      1 +       1 + 4Ak      =   \u221a\u03b2 + 1      Ak + ak      ,\n                                            \u03b2\u03b7  k                             \u03b7k                                     \u03b7k\n                                                                                                               1         \u221a  \u03b2+1\nwhere we used \u03b2 > 1/5 in the second inequality and the fact that 1 +                                          \u221a \u03b2 x \u2265     2\u221a\u03b2 (1 + x) for x \u2265             1\nin the last inequality.\n                                                                                               a k+1             1     ak\nRemark A.1. If \u03b2 \u2264a                 1/5, then in Case I we still have                      Ak+1+ak+1 \u2264         \u221a  \u03b2 Ak+ak , while in Case II\n                    k+1               2       ak\nwe have       A k+1+ak+1 \u2264         \u221a 5+1   Ak+ak . Thus, in the case where \u03b2 \u2264                      1/5, the derivation below still holds\nexcept that the absolute constant C2 will be different.\n                                                                            22", "md": "Now by summing (34) over \\(k = 0, \\ldots, N - 1\\), we get\n\n\\[\nA_N \\|x_N - x^*\\|^2 \\leq \\sum_{k=0}^{N-1} (A_{k+1} - A_k)^2 \\sigma^2 a^2 \\eta^{2k} \\| \\hat{x}_{k+1} - y_k \\|^2 + \\sum_{k=0}^{N-1} 2(A_{k+1} - A_k) \\|z_{k+1} - x^*\\|^2\n\\]\n\n\\[\n\\leq 2\\sigma^2 \\sum_{k=0}^{N-1} (A_{k+1} - A_k) \\sum_{k=0}^{N-1} a^{2k} \\| \\hat{x}_{k+1} - y_k \\|^2 + 2\\|z_0 - x^*\\|^2 \\sum_{k=0}^{N-1} (A_{k+1} - A_k)\n\\]\n\n\\[\n\\leq 2\\sigma^2 \\left(1 - \\sigma^2 A_N \\|z_0 - x^*\\|^2 + 2A_N \\|z_0 - x^*\\|^2 \\right) = 2A_N \\left(1 - \\sigma^2 \\|z_0 - x^*\\|^2 \\right).\n\\]\n\nHence, this implies that \\|x_k - x^*\\|^2 \\leq 1 - \\sigma^2 \\|z_0 - x^*\\|^2\\) for any \\(k \\geq 0\\).\n\nA key term appearing in several of our bounds is \\(A_{k+1} + a_{k+1}\\). In the next lemma, we establish an upper bound for this ratio based on a factor of its previous value, for both cases of our algorithm.\n\nLemma A.5. Without loss of generality assume \\(\\beta > 1/5\\). In Case I we have \\(A_{k+1} + a_{k+1} \\leq \\frac{\\sqrt{\\beta}}{2\\sqrt{\\beta}} A_k + a_k\\).\n\nOtherwise, in Case II we have \\(A_{k+1} + a_{k+1} \\leq \\frac{\\sqrt{\\beta+1}}{2} A_k + a_k\\).\n\nProof. By the choice of \\(a_k\\) in (2) we have \\(\\eta_k(A_k + a_k) = a_k^2\\) for all \\(k \\geq 0\\). As a result, we have\n\n\\[\n\\frac{a_{k+1}}{2\\eta_k} \\leq \\frac{1}{2} + \\frac{1}{2} + 4A_k \\eta_k = 1 + 1 + 4A_k \\eta_k,\n\\]\n\nand similarly\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} = 1 + \\frac{1}{2} + 4A_{k+1} \\eta_{k+1}.\n\\]\n\nIn Case I, we have \\(\\eta_{k+1} = \\frac{\\eta_k}{\\beta}\\) and \\(A_{k+1} \\geq A_k\\). Hence, it implies that \\(\\frac{A_{k+1}}{\\eta_{k+1}} \\geq \\beta \\frac{A_k}{\\eta_k}\\), which leads to\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} \\leq \\sqrt{\\beta} \\frac{A_k + a_k}{\\eta_k}.\n\\]\n\nwhere the second inequality follows from the fact that \\(\\beta \\leq 1\\).\n\nIn Case II, we have \\(\\eta_{k+1} = \\hat{\\eta}_k = \\gamma_k \\eta_k\\) and \\(A_{k+1} = A_k + \\gamma_k a_k\\). Since we also have \\(a_k \\geq \\eta_k\\) and \\(\\gamma_k \\leq \\beta\\), we obtain \\(\\frac{A_{k+1}}{\\eta_{k+1}} \\geq \\frac{A_k}{\\gamma_k \\eta_k} + 1 \\geq \\frac{A_k}{\\beta \\eta_k} + 1\\). Hence,\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} \\leq 1 + \\frac{5}{\\beta} + 4A_k \\leq 1 + \\sqrt{\\beta} + 4A_k = \\sqrt{\\beta} + 1 \\frac{A_k + a_k}{\\eta_k},\n\\]\n\nwhere we used \\(\\beta > 1/5\\) in the second inequality and the fact that \\(1 + \\sqrt{\\beta} x \\geq 2\\sqrt{\\beta} (1 + x)\\) for \\(x \\geq 1\\) in the last inequality.\n\nRemark A.1. If \\(\\beta \\leq 1/5\\), then in Case I we still have \\(A_{k+1} + a_{k+1} \\leq \\sqrt{\\beta} A_k + a_k\\), while in Case II we have \\(A_{k+1} + a_{k+1} \\leq \\sqrt{5+1} A_k + a_k\\). Thus, in the case where \\(\\beta \\leq 1/5\\), the derivation below still holds except that the absolute constant \\(C_2\\) will be different.", "images": [], "items": [{"type": "text", "value": "Now by summing (34) over \\(k = 0, \\ldots, N - 1\\), we get\n\n\\[\nA_N \\|x_N - x^*\\|^2 \\leq \\sum_{k=0}^{N-1} (A_{k+1} - A_k)^2 \\sigma^2 a^2 \\eta^{2k} \\| \\hat{x}_{k+1} - y_k \\|^2 + \\sum_{k=0}^{N-1} 2(A_{k+1} - A_k) \\|z_{k+1} - x^*\\|^2\n\\]\n\n\\[\n\\leq 2\\sigma^2 \\sum_{k=0}^{N-1} (A_{k+1} - A_k) \\sum_{k=0}^{N-1} a^{2k} \\| \\hat{x}_{k+1} - y_k \\|^2 + 2\\|z_0 - x^*\\|^2 \\sum_{k=0}^{N-1} (A_{k+1} - A_k)\n\\]\n\n\\[\n\\leq 2\\sigma^2 \\left(1 - \\sigma^2 A_N \\|z_0 - x^*\\|^2 + 2A_N \\|z_0 - x^*\\|^2 \\right) = 2A_N \\left(1 - \\sigma^2 \\|z_0 - x^*\\|^2 \\right).\n\\]\n\nHence, this implies that \\|x_k - x^*\\|^2 \\leq 1 - \\sigma^2 \\|z_0 - x^*\\|^2\\) for any \\(k \\geq 0\\).\n\nA key term appearing in several of our bounds is \\(A_{k+1} + a_{k+1}\\). In the next lemma, we establish an upper bound for this ratio based on a factor of its previous value, for both cases of our algorithm.\n\nLemma A.5. Without loss of generality assume \\(\\beta > 1/5\\). In Case I we have \\(A_{k+1} + a_{k+1} \\leq \\frac{\\sqrt{\\beta}}{2\\sqrt{\\beta}} A_k + a_k\\).\n\nOtherwise, in Case II we have \\(A_{k+1} + a_{k+1} \\leq \\frac{\\sqrt{\\beta+1}}{2} A_k + a_k\\).\n\nProof. By the choice of \\(a_k\\) in (2) we have \\(\\eta_k(A_k + a_k) = a_k^2\\) for all \\(k \\geq 0\\). As a result, we have\n\n\\[\n\\frac{a_{k+1}}{2\\eta_k} \\leq \\frac{1}{2} + \\frac{1}{2} + 4A_k \\eta_k = 1 + 1 + 4A_k \\eta_k,\n\\]\n\nand similarly\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} = 1 + \\frac{1}{2} + 4A_{k+1} \\eta_{k+1}.\n\\]\n\nIn Case I, we have \\(\\eta_{k+1} = \\frac{\\eta_k}{\\beta}\\) and \\(A_{k+1} \\geq A_k\\). Hence, it implies that \\(\\frac{A_{k+1}}{\\eta_{k+1}} \\geq \\beta \\frac{A_k}{\\eta_k}\\), which leads to\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} \\leq \\sqrt{\\beta} \\frac{A_k + a_k}{\\eta_k}.\n\\]\n\nwhere the second inequality follows from the fact that \\(\\beta \\leq 1\\).\n\nIn Case II, we have \\(\\eta_{k+1} = \\hat{\\eta}_k = \\gamma_k \\eta_k\\) and \\(A_{k+1} = A_k + \\gamma_k a_k\\). Since we also have \\(a_k \\geq \\eta_k\\) and \\(\\gamma_k \\leq \\beta\\), we obtain \\(\\frac{A_{k+1}}{\\eta_{k+1}} \\geq \\frac{A_k}{\\gamma_k \\eta_k} + 1 \\geq \\frac{A_k}{\\beta \\eta_k} + 1\\). Hence,\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} \\leq 1 + \\frac{5}{\\beta} + 4A_k \\leq 1 + \\sqrt{\\beta} + 4A_k = \\sqrt{\\beta} + 1 \\frac{A_k + a_k}{\\eta_k},\n\\]\n\nwhere we used \\(\\beta > 1/5\\) in the second inequality and the fact that \\(1 + \\sqrt{\\beta} x \\geq 2\\sqrt{\\beta} (1 + x)\\) for \\(x \\geq 1\\) in the last inequality.\n\nRemark A.1. If \\(\\beta \\leq 1/5\\), then in Case I we still have \\(A_{k+1} + a_{k+1} \\leq \\sqrt{\\beta} A_k + a_k\\), while in Case II we have \\(A_{k+1} + a_{k+1} \\leq \\sqrt{5+1} A_k + a_k\\). Thus, in the case where \\(\\beta \\leq 1/5\\), the derivation below still holds except that the absolute constant \\(C_2\\) will be different.", "md": "Now by summing (34) over \\(k = 0, \\ldots, N - 1\\), we get\n\n\\[\nA_N \\|x_N - x^*\\|^2 \\leq \\sum_{k=0}^{N-1} (A_{k+1} - A_k)^2 \\sigma^2 a^2 \\eta^{2k} \\| \\hat{x}_{k+1} - y_k \\|^2 + \\sum_{k=0}^{N-1} 2(A_{k+1} - A_k) \\|z_{k+1} - x^*\\|^2\n\\]\n\n\\[\n\\leq 2\\sigma^2 \\sum_{k=0}^{N-1} (A_{k+1} - A_k) \\sum_{k=0}^{N-1} a^{2k} \\| \\hat{x}_{k+1} - y_k \\|^2 + 2\\|z_0 - x^*\\|^2 \\sum_{k=0}^{N-1} (A_{k+1} - A_k)\n\\]\n\n\\[\n\\leq 2\\sigma^2 \\left(1 - \\sigma^2 A_N \\|z_0 - x^*\\|^2 + 2A_N \\|z_0 - x^*\\|^2 \\right) = 2A_N \\left(1 - \\sigma^2 \\|z_0 - x^*\\|^2 \\right).\n\\]\n\nHence, this implies that \\|x_k - x^*\\|^2 \\leq 1 - \\sigma^2 \\|z_0 - x^*\\|^2\\) for any \\(k \\geq 0\\).\n\nA key term appearing in several of our bounds is \\(A_{k+1} + a_{k+1}\\). In the next lemma, we establish an upper bound for this ratio based on a factor of its previous value, for both cases of our algorithm.\n\nLemma A.5. Without loss of generality assume \\(\\beta > 1/5\\). In Case I we have \\(A_{k+1} + a_{k+1} \\leq \\frac{\\sqrt{\\beta}}{2\\sqrt{\\beta}} A_k + a_k\\).\n\nOtherwise, in Case II we have \\(A_{k+1} + a_{k+1} \\leq \\frac{\\sqrt{\\beta+1}}{2} A_k + a_k\\).\n\nProof. By the choice of \\(a_k\\) in (2) we have \\(\\eta_k(A_k + a_k) = a_k^2\\) for all \\(k \\geq 0\\). As a result, we have\n\n\\[\n\\frac{a_{k+1}}{2\\eta_k} \\leq \\frac{1}{2} + \\frac{1}{2} + 4A_k \\eta_k = 1 + 1 + 4A_k \\eta_k,\n\\]\n\nand similarly\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} = 1 + \\frac{1}{2} + 4A_{k+1} \\eta_{k+1}.\n\\]\n\nIn Case I, we have \\(\\eta_{k+1} = \\frac{\\eta_k}{\\beta}\\) and \\(A_{k+1} \\geq A_k\\). Hence, it implies that \\(\\frac{A_{k+1}}{\\eta_{k+1}} \\geq \\beta \\frac{A_k}{\\eta_k}\\), which leads to\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} \\leq \\sqrt{\\beta} \\frac{A_k + a_k}{\\eta_k}.\n\\]\n\nwhere the second inequality follows from the fact that \\(\\beta \\leq 1\\).\n\nIn Case II, we have \\(\\eta_{k+1} = \\hat{\\eta}_k = \\gamma_k \\eta_k\\) and \\(A_{k+1} = A_k + \\gamma_k a_k\\). Since we also have \\(a_k \\geq \\eta_k\\) and \\(\\gamma_k \\leq \\beta\\), we obtain \\(\\frac{A_{k+1}}{\\eta_{k+1}} \\geq \\frac{A_k}{\\gamma_k \\eta_k} + 1 \\geq \\frac{A_k}{\\beta \\eta_k} + 1\\). Hence,\n\n\\[\n\\frac{A_{k+1} + a_{k+1}}{2} \\leq 1 + \\frac{5}{\\beta} + 4A_k \\leq 1 + \\sqrt{\\beta} + 4A_k = \\sqrt{\\beta} + 1 \\frac{A_k + a_k}{\\eta_k},\n\\]\n\nwhere we used \\(\\beta > 1/5\\) in the second inequality and the fact that \\(1 + \\sqrt{\\beta} x \\geq 2\\sqrt{\\beta} (1 + x)\\) for \\(x \\geq 1\\) in the last inequality.\n\nRemark A.1. If \\(\\beta \\leq 1/5\\), then in Case I we still have \\(A_{k+1} + a_{k+1} \\leq \\sqrt{\\beta} A_k + a_k\\), while in Case II we have \\(A_{k+1} + a_{k+1} \\leq \\sqrt{5+1} A_k + a_k\\). Thus, in the case where \\(\\beta \\leq 1/5\\), the derivation below still holds except that the absolute constant \\(C_2\\) will be different."}]}, {"page": 23, "text": "Next, as a corollary of Lemma A.5, we establish an upper bound on the series  N\u22121                                                       k=0  A  ak\nMoreover, we use this result to establish an upper bound for  N\u22121                                    k=0 \u2225\u02c6   xk+1 \u2212     yk\u2225.                  k+ak .\nLemma A.6. We have\n                                           N\u22121         ak               \u221a\u03b2 \u2212      \u03b2\n                                            k=0   Ak + ak       \u2264   1 + 2\u221a\u03b2 \u2212         \u03b2    1 + log AN   A1     .                                (41)\nMoreover,                N\u22121                                    1      1 + 2\u221a\u03b2 \u2212         \u03b2\n                          k=0   \u2225\u02c6xk+1 \u2212      yk\u2225   \u2264       1 \u2212   \u03c32       \u221a  \u03b2 \u2212    \u03b2        1 + log AN  A1      \u2225z0 \u2212     x\u2217\u2225.                (42)\nProof. Given the initial values of Ak and ak we have\n                    N\u22121        ak        = 1 +     N\u22121        ak       = 1 +                      ak       +                   ak               (43)\n                    k=0    Ak + ak                 k=1    Ak + ak                k\u2208B,k\u22651      Ak + ak          k/          Ak + ak\nNote that using the result in Lemma A.5                                                                         \u2208B,k\u22651\n                                     ak        \u2264   N\u22122          ak+1                                                                            (44)\n                     k\u2208B,k\u22651     Ak + ak       =   k=0    Ak+1 + ak+1ak+1           +                     ak+1                                  (45)\n                                                   k/          Ak+1 + ak+1             k\u2208B,k\u22650     Ak+1 + ak+1\n                                                    \u2208B,k\u22650     \u221a 1       ak                         2\u221a\u03b2            ak\n                                               \u2264   k/             \u03b2  Ak + ak      +   k\u2208B,k\u22650     \u221a\u03b2 + 1      Ak + ak                           (46)\n                                                    \u2208B,k\u22650\n                                                   \u221a 1                     1        ak                         2\u221a\u03b2           ak\n                                               \u2264      \u03b2 +    k/\u2208B,k\u22651    \u221a\u03b2    Ak + ak       +  k\u2208B,k\u22651      \u221a\u03b2 + 1      Ak + ak    .           (47)\nHence, if we move the last term in the above upper bound to the left hand side and rescale both\nsides of the resulted inequality we obtain\n                                                      ak            \u221a  \u03b2 \u2212    \u03b2                           ak\n                                      k\u2208B,k\u22651     Ak + ak       \u2264   1 + \u221a\u03b2         1 +   k/\u2208B,k\u22651    Ak + ak         .\nNow, if we replace the above upper bound into (43) we obtain\n                                    N\u22121         ak               \u221a  \u03b2 \u2212    \u03b2                               ak\n                                     k=0   Ak + ak       \u2264   1 + 2\u221a\u03b2 \u2212         \u03b2    1 +    k/          Ak + ak        .                         (48)\nMoreover, note that for k /               \u2208  B, we have Ak+1 = Ak + ak. Hence,              \u2208B,k\u22651\n                                ak       =                 1 \u2212      Ak        \u2264              (log(Ak+1) \u2212          log(Ak))\n               k/\u2208B,k\u22651     Ak + ak          k/\u2208B,k\u22651             Ak+1            k/\n                                                                                   \u2208B,k\u22651\n                                                                              \u2264   N\u22121   (log(Ak+1) \u2212          log(Ak)) = log AN     A1   .\n                                                                                  k=1\n                                                                            23", "md": "Next, as a corollary of Lemma A.5, we establish an upper bound on the series $$\\sum_{k=0}^{N-1} \\frac{A_k}{a_k}$$\nMoreover, we use this result to establish an upper bound for $$\\sum_{k=0}^{N-1} \\left\\| \\hat{x}_k+1 - y_k \\right\\|_{k+a_k}$$.\n\nLemma A.6. We have\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} \\leq 1 + 2\\sqrt{\\beta} - \\beta \\frac{1 + \\log AN}{A_1} \\quad (41)\n$$\nMoreover,\n$$\n\\sum_{k=0}^{N-1} \\left\\| \\hat{x}_k+1 - y_k \\right\\| \\leq 1 - \\sigma^2 \\sqrt{\\beta} - \\beta \\frac{1 + \\log AN}{A_1} \\left\\|z_0 - x^*\\right\\| \\quad (42)\n$$\n\nProof. Given the initial values of A_k and a_k we have\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} = 1 + \\sum_{k=1}^{N-1} \\frac{a_k}{A_k + a_k} = 1 + \\frac{a_k}{A_k + a_k} + \\frac{a_k}{A_k + a_k} \\quad (43)\n$$\nNote that using the result in Lemma A.5\n$$\na_k \\leq \\sqrt{N-2} a_{k+1} \\quad (44)\n$$\n$$\n\\sum_{k \\in B, k \\geq 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} = \\sum_{k=0}^{N-1} \\frac{A_{k+1} + a_{k+1}a_{k+1}}{A_{k+1} + a_{k+1}} + \\frac{a_{k+1}}{A_{k+1} + a_{k+1}} \\quad (45)\n$$\n$$\n\\leq \\sum_{k \\in B, k \\geq 0} \\sqrt{1} \\frac{a_k}{\\beta A_k + a_k} + \\sum_{k \\in B, k \\geq 0} \\sqrt{\\beta + 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} \\quad (46)\n$$\n$$\n\\leq \\sqrt{1} \\frac{1}{\\beta} a_k + \\sum_{k \\in B, k \\geq 1} \\sqrt{\\beta} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} + \\sum_{k \\in B, k \\geq 1} \\sqrt{\\beta + 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} \\quad (47)\n$$\n\nHence, if we move the last term in the above upper bound to the left hand side and rescale both sides of the resulted inequality we obtain\n$$\n\\sum_{k \\in B, k \\geq 1} \\frac{a_k}{A_k + a_k} \\leq 1 + \\sqrt{\\beta} \\frac{1 + \\sum_{k \\notin B, k \\geq 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}}}{A_k + a_k} \\quad .\n$$\n\nNow, if we replace the above upper bound into (43) we obtain\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} \\leq 1 + 2\\sqrt{\\beta} - \\beta \\frac{1 + \\sum_{k \\notin B} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}}}{A_k + a_k} \\quad (48)\n$$\n\nMoreover, note that for $k \\notin B$, we have $A_{k+1} = A_k + a_k$. Hence, $k \\in B, k \\geq 1$\n$$\na_k = 1 - \\frac{A_k}{A_{k+1}} \\leq \\left( \\log(A_{k+1}) - \\log(A_k) \\right) \\quad (log AN A1) \\quad .\n$$", "images": [], "items": [{"type": "text", "value": "Next, as a corollary of Lemma A.5, we establish an upper bound on the series $$\\sum_{k=0}^{N-1} \\frac{A_k}{a_k}$$\nMoreover, we use this result to establish an upper bound for $$\\sum_{k=0}^{N-1} \\left\\| \\hat{x}_k+1 - y_k \\right\\|_{k+a_k}$$.\n\nLemma A.6. We have\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} \\leq 1 + 2\\sqrt{\\beta} - \\beta \\frac{1 + \\log AN}{A_1} \\quad (41)\n$$\nMoreover,\n$$\n\\sum_{k=0}^{N-1} \\left\\| \\hat{x}_k+1 - y_k \\right\\| \\leq 1 - \\sigma^2 \\sqrt{\\beta} - \\beta \\frac{1 + \\log AN}{A_1} \\left\\|z_0 - x^*\\right\\| \\quad (42)\n$$\n\nProof. Given the initial values of A_k and a_k we have\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} = 1 + \\sum_{k=1}^{N-1} \\frac{a_k}{A_k + a_k} = 1 + \\frac{a_k}{A_k + a_k} + \\frac{a_k}{A_k + a_k} \\quad (43)\n$$\nNote that using the result in Lemma A.5\n$$\na_k \\leq \\sqrt{N-2} a_{k+1} \\quad (44)\n$$\n$$\n\\sum_{k \\in B, k \\geq 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} = \\sum_{k=0}^{N-1} \\frac{A_{k+1} + a_{k+1}a_{k+1}}{A_{k+1} + a_{k+1}} + \\frac{a_{k+1}}{A_{k+1} + a_{k+1}} \\quad (45)\n$$\n$$\n\\leq \\sum_{k \\in B, k \\geq 0} \\sqrt{1} \\frac{a_k}{\\beta A_k + a_k} + \\sum_{k \\in B, k \\geq 0} \\sqrt{\\beta + 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} \\quad (46)\n$$\n$$\n\\leq \\sqrt{1} \\frac{1}{\\beta} a_k + \\sum_{k \\in B, k \\geq 1} \\sqrt{\\beta} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} + \\sum_{k \\in B, k \\geq 1} \\sqrt{\\beta + 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} \\quad (47)\n$$\n\nHence, if we move the last term in the above upper bound to the left hand side and rescale both sides of the resulted inequality we obtain\n$$\n\\sum_{k \\in B, k \\geq 1} \\frac{a_k}{A_k + a_k} \\leq 1 + \\sqrt{\\beta} \\frac{1 + \\sum_{k \\notin B, k \\geq 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}}}{A_k + a_k} \\quad .\n$$\n\nNow, if we replace the above upper bound into (43) we obtain\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} \\leq 1 + 2\\sqrt{\\beta} - \\beta \\frac{1 + \\sum_{k \\notin B} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}}}{A_k + a_k} \\quad (48)\n$$\n\nMoreover, note that for $k \\notin B$, we have $A_{k+1} = A_k + a_k$. Hence, $k \\in B, k \\geq 1$\n$$\na_k = 1 - \\frac{A_k}{A_{k+1}} \\leq \\left( \\log(A_{k+1}) - \\log(A_k) \\right) \\quad (log AN A1) \\quad .\n$$", "md": "Next, as a corollary of Lemma A.5, we establish an upper bound on the series $$\\sum_{k=0}^{N-1} \\frac{A_k}{a_k}$$\nMoreover, we use this result to establish an upper bound for $$\\sum_{k=0}^{N-1} \\left\\| \\hat{x}_k+1 - y_k \\right\\|_{k+a_k}$$.\n\nLemma A.6. We have\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} \\leq 1 + 2\\sqrt{\\beta} - \\beta \\frac{1 + \\log AN}{A_1} \\quad (41)\n$$\nMoreover,\n$$\n\\sum_{k=0}^{N-1} \\left\\| \\hat{x}_k+1 - y_k \\right\\| \\leq 1 - \\sigma^2 \\sqrt{\\beta} - \\beta \\frac{1 + \\log AN}{A_1} \\left\\|z_0 - x^*\\right\\| \\quad (42)\n$$\n\nProof. Given the initial values of A_k and a_k we have\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} = 1 + \\sum_{k=1}^{N-1} \\frac{a_k}{A_k + a_k} = 1 + \\frac{a_k}{A_k + a_k} + \\frac{a_k}{A_k + a_k} \\quad (43)\n$$\nNote that using the result in Lemma A.5\n$$\na_k \\leq \\sqrt{N-2} a_{k+1} \\quad (44)\n$$\n$$\n\\sum_{k \\in B, k \\geq 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} = \\sum_{k=0}^{N-1} \\frac{A_{k+1} + a_{k+1}a_{k+1}}{A_{k+1} + a_{k+1}} + \\frac{a_{k+1}}{A_{k+1} + a_{k+1}} \\quad (45)\n$$\n$$\n\\leq \\sum_{k \\in B, k \\geq 0} \\sqrt{1} \\frac{a_k}{\\beta A_k + a_k} + \\sum_{k \\in B, k \\geq 0} \\sqrt{\\beta + 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} \\quad (46)\n$$\n$$\n\\leq \\sqrt{1} \\frac{1}{\\beta} a_k + \\sum_{k \\in B, k \\geq 1} \\sqrt{\\beta} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} + \\sum_{k \\in B, k \\geq 1} \\sqrt{\\beta + 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}} \\quad (47)\n$$\n\nHence, if we move the last term in the above upper bound to the left hand side and rescale both sides of the resulted inequality we obtain\n$$\n\\sum_{k \\in B, k \\geq 1} \\frac{a_k}{A_k + a_k} \\leq 1 + \\sqrt{\\beta} \\frac{1 + \\sum_{k \\notin B, k \\geq 1} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}}}{A_k + a_k} \\quad .\n$$\n\nNow, if we replace the above upper bound into (43) we obtain\n$$\n\\sum_{k=0}^{N-1} \\frac{a_k}{A_k + a_k} \\leq 1 + 2\\sqrt{\\beta} - \\beta \\frac{1 + \\sum_{k \\notin B} \\frac{A_k + a_k}{A_{k+1} + a_{k+1}}}{A_k + a_k} \\quad (48)\n$$\n\nMoreover, note that for $k \\notin B$, we have $A_{k+1} = A_k + a_k$. Hence, $k \\in B, k \\geq 1$\n$$\na_k = 1 - \\frac{A_k}{A_{k+1}} \\leq \\left( \\log(A_{k+1}) - \\log(A_k) \\right) \\quad (log AN A1) \\quad .\n$$"}]}, {"page": 24, "text": "Now if we replace the above upper bound, i.e., log AN                                              k/             ak\nthe right-hand side of (48) we obtain the result in (41).                        A1 with            \u2208B,k\u22651     Ak+ak into the expression in\nNext, note that by Cauchy-Schwarz inequality, we have\n               N\u22121    \u2225\u02c6                          N\u22121    \u03b72k N\u22121    a2k                                   1      N\u22121    \u03b72k\n               k=0     xk+1 \u2212      yk\u2225    \u2264       k=0    a2k  k=0   \u03b72k \u2225\u02c6xk+1 \u2212      yk\u22252 \u2264          1 \u2212    \u03c32   k=0   a2k \u2225z0 \u2212     x\u2217\u2225,\nwhere the last inequality follows from (21). Moreover, based on the expression for ak in (2) and the\nresult in (41) that we just proved, we have\n                     N\u22121    \u03b72k     N\u22121           a2k             N\u22121        ak                \u221a  \u03b2 \u2212   \u03b2         1 + log AN\n                     k=0    a2k  =   k=0    (Ak + ak)2 \u2264          k=0    Ak + ak      \u2264    1 + 2\u221a\u03b2 \u2212        \u03b2                 A1      .\nCombining the two inequalities above leads to (42).\nNow we are ready to present and prove Lemma A.7 which characterizes a bound on the path length\nof the sequence {yk}N            k=0\nLemma A.7. Consider the iterates generated by Algorithm 1. Then for any N,\n                                         N\u22121   \u2225yk+1 \u2212       yk\u2225    \u2264   C2    1 + log AN   A 1     \u2225z0 \u2212    x\u2217\u2225.\n                                         k=0\nwhere                                        1      1 + 2\u221a\u03b2 \u2212         \u03b2        1                    2      1 + 2\u221a\u03b2 \u2212           \u03b2\n                          C2 = 2         1 \u2212   \u03c32       \u221a  \u03b2 \u2212    \u03b2      +   \u221a\u03b2      1 +       1 \u2212    \u03c32         \u221a\u03b2 \u2212      \u03b2                         (49)\nProof. By the triangle inequality, we have\nWe again distinguish two cases.           \u2225yk \u2212     yk+1\u2225     \u2264   \u2225\u02c6xk+1 \u2212      yk\u2225   + \u2225\u02c6  xk+1 \u2212     yk+1\u2225.                                        (50)\n                                                                                             A k+1                      ak+1\nCase I: \u02c6     \u03b7k = \u03b7k. In this case \u02c6           xk+1 = xk+1 and yk+1 =                  A  k+1+ak+1 xk+1 +          Ak+1+ak+1 zk+1, hence\n      \u2225\u02c6xk+1 \u2212      yk+1\u2225     = \u2225xk+1 \u2212        yk+1\u2225      = ak+1\u2225zk+1 \u2212           xk+1\u2225      \u2264   \u221a 1\u03b2     1 +           2      ak\u2225z0 \u2212         x\u2217\u2225  ,\n                                                                   Ak+1 + ak+1                                      1 \u2212   \u03c32        Ak + ak\nwhere we used Lemma A.5 and the fact that \u2225zk+1 \u2212                                          xk+1\u2225      \u2264   \u2225zk+1 \u2212       x\u2217\u2225    + \u2225xk+1 \u2212         x\u2217\u2225    \u2264\n(1 +          2\n            1\u2212\u03c32 )\u2225z0 \u2212       x\u2217\u2225   in the last inequality. Therefore, using (50) and the above bound we have\n                                                                       \u221a 1                    2             ak\n                       \u2225yk \u2212     yk+1\u2225     \u2264   \u2225\u02c6xk+1 \u2212      yk\u2225   +      \u03b2     1 +       1 \u2212   \u03c32     Ak + ak      \u2225z0 \u2212     x\u2217\u2225.                   (51)\n                                                         A k                 \u03b3kak                                    Ak                ak\nCase II: \u02c6      \u03b7k < \u03b7k. Since xk+1 =                A k+\u03b3kak xk +        Ak+\u03b3kak \u02dc   zk+1 and \u02c6      xk+1 =      Ak+ak xk +        Ak+ak \u02dc   zk+1, we\nget\n \u02c6               Ak                                                              ak      \u02dc                                                         \u02dc\nxk+1 =                      xk+1 + \u03b3kak         (x k+1 \u2212     \u02dc\n                                                             zk+1)       +               zk+1 = Ak + \u03b3kak             xk+1 + (1 \u2212         \u03b3k)ak    zk+1.\n             Ak + ak                      Ak                                 Ak + ak                    Ak + ak                     Ak + ak\n                                                                            24", "md": "Now if we replace the above upper bound, i.e., $$\\log AN_{k}/a_{k}$$, on the right-hand side of (48) we obtain the result in (41). Next, note that by Cauchy-Schwarz inequality, we have\n\n$$\n\\begin{align*}\n&\\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_{k} \\| \\leq \\sqrt{\\sum_{k=0}^{N-1} a_{2k} \\sum_{k=0}^{N-1} \\eta_{2k}} \\\\\n&\\leq \\sqrt{1 - \\sigma^{2} \\sum_{k=0}^{N-1} a_{2k}} \\|z_{0} - x^{*}\\|,\n\\end{align*}\n$$\n\nwhere the last inequality follows from (21). Moreover, based on the expression for $$a_{k}$$ in (2) and the result in (41) that we just proved, we have\n\n$$\n\\begin{align*}\n&\\sum_{k=0}^{N-1} \\frac{\\eta_{2k}}{a_{2k}} = \\sum_{k=0}^{N-1} \\frac{(A_{k} + a_{k})^{2}}{a_{2k}} \\\\\n&\\leq \\sum_{k=0}^{N-1} (A_{k} + a_{k}) \\leq 1 + 2\\sqrt{\\beta} - \\beta + \\frac{1 + \\log AN_{1}}{A_{1}}.\n\\end{align*}\n$$\n\nCombining the two inequalities above leads to (42).\n\nNow we are ready to present and prove Lemma A.7 which characterizes a bound on the path length of the sequence $$\\{y_{k}\\}_{k=0}^{N}$$.\n\n**Lemma A.7.** Consider the iterates generated by Algorithm 1. Then for any N,\n\n$$\n\\sum_{k=0}^{N-1} \\|y_{k+1} - y_{k}\\| \\leq C_{2} (1 + \\log AN_{1}) \\|z_{0} - x^{*}\\|,\n$$\n\nwhere\n\n$$\nC_{2} = 2 \\left(1 - \\sigma^{2} \\sqrt{\\beta} - \\beta + \\sqrt{\\beta} \\left(1 + \\frac{1 - \\sigma^{2} \\sqrt{\\beta} - \\beta}{1 + 2\\sqrt{\\beta} - \\beta}\\right)\\right) \\tag{49}\n$$\n\n**Proof.** By the triangle inequality, we have\n\nWe again distinguish two cases.\n\n**Case I:** $$\\hat{\\eta}_{k} = \\eta_{k}$$. In this case $$\\hat{x}_{k+1} = x_{k+1}$$ and $$y_{k+1} = A_{k+1}+a_{k+1}x_{k+1}+Ak+1+a_{k+1}z_{k+1}$$, hence\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| = \\|x_{k+1} - y_{k+1}\\| = a_{k+1} \\|z_{k+1} - x_{k+1}\\| \\leq \\sqrt{1+\\beta} \\left(1 + 2\\gamma_{k} a_{k} \\|z_{0} - x^{*}\\|\\right),\n$$\n\nwhere we used Lemma A.5 and the fact that $$\\|z_{k+1} - x_{k+1}\\| \\leq \\|z_{k+1} - x^{*}\\| + \\|x_{k+1} - x^{*}\\| \\leq (1 + \\sqrt{1-\\sigma^{2}}) \\|z_{0} - x^{*}\\|$$ in the last inequality. Therefore, using (50) and the above bound we have\n\n$$\n\\|y_{k} - y_{k+1}\\| \\leq \\| \\hat{x}_{k+1} - y_{k} \\| + \\beta \\left(1 + \\frac{1 - \\sigma^{2}}{\\gamma_{k} a_{k}} A_{k} + a_{k} \\|z_{0} - x^{*}\\|\\right). \\tag{51}\n$$\n\n**Case II:** $$\\hat{\\eta}_{k} < \\eta_{k}$$. Since $$x_{k+1} = A_{k} + \\gamma_{k} a_{k} x_{k} + Ak+\\gamma_{k} a_{k}z_{k+1}$$ and $$\\hat{x}_{k+1} = Ak+a_{k}x_{k}+Ak+a_{k}z_{k+1}$$, we get\n\n$$\n\\hat{x}_{k+1} = Ak x_{k+1} + (1 - \\gamma_{k}) a_{k} z_{k+1}.\n$$", "images": [], "items": [{"type": "text", "value": "Now if we replace the above upper bound, i.e., $$\\log AN_{k}/a_{k}$$, on the right-hand side of (48) we obtain the result in (41). Next, note that by Cauchy-Schwarz inequality, we have\n\n$$\n\\begin{align*}\n&\\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_{k} \\| \\leq \\sqrt{\\sum_{k=0}^{N-1} a_{2k} \\sum_{k=0}^{N-1} \\eta_{2k}} \\\\\n&\\leq \\sqrt{1 - \\sigma^{2} \\sum_{k=0}^{N-1} a_{2k}} \\|z_{0} - x^{*}\\|,\n\\end{align*}\n$$\n\nwhere the last inequality follows from (21). Moreover, based on the expression for $$a_{k}$$ in (2) and the result in (41) that we just proved, we have\n\n$$\n\\begin{align*}\n&\\sum_{k=0}^{N-1} \\frac{\\eta_{2k}}{a_{2k}} = \\sum_{k=0}^{N-1} \\frac{(A_{k} + a_{k})^{2}}{a_{2k}} \\\\\n&\\leq \\sum_{k=0}^{N-1} (A_{k} + a_{k}) \\leq 1 + 2\\sqrt{\\beta} - \\beta + \\frac{1 + \\log AN_{1}}{A_{1}}.\n\\end{align*}\n$$\n\nCombining the two inequalities above leads to (42).\n\nNow we are ready to present and prove Lemma A.7 which characterizes a bound on the path length of the sequence $$\\{y_{k}\\}_{k=0}^{N}$$.\n\n**Lemma A.7.** Consider the iterates generated by Algorithm 1. Then for any N,\n\n$$\n\\sum_{k=0}^{N-1} \\|y_{k+1} - y_{k}\\| \\leq C_{2} (1 + \\log AN_{1}) \\|z_{0} - x^{*}\\|,\n$$\n\nwhere\n\n$$\nC_{2} = 2 \\left(1 - \\sigma^{2} \\sqrt{\\beta} - \\beta + \\sqrt{\\beta} \\left(1 + \\frac{1 - \\sigma^{2} \\sqrt{\\beta} - \\beta}{1 + 2\\sqrt{\\beta} - \\beta}\\right)\\right) \\tag{49}\n$$\n\n**Proof.** By the triangle inequality, we have\n\nWe again distinguish two cases.\n\n**Case I:** $$\\hat{\\eta}_{k} = \\eta_{k}$$. In this case $$\\hat{x}_{k+1} = x_{k+1}$$ and $$y_{k+1} = A_{k+1}+a_{k+1}x_{k+1}+Ak+1+a_{k+1}z_{k+1}$$, hence\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| = \\|x_{k+1} - y_{k+1}\\| = a_{k+1} \\|z_{k+1} - x_{k+1}\\| \\leq \\sqrt{1+\\beta} \\left(1 + 2\\gamma_{k} a_{k} \\|z_{0} - x^{*}\\|\\right),\n$$\n\nwhere we used Lemma A.5 and the fact that $$\\|z_{k+1} - x_{k+1}\\| \\leq \\|z_{k+1} - x^{*}\\| + \\|x_{k+1} - x^{*}\\| \\leq (1 + \\sqrt{1-\\sigma^{2}}) \\|z_{0} - x^{*}\\|$$ in the last inequality. Therefore, using (50) and the above bound we have\n\n$$\n\\|y_{k} - y_{k+1}\\| \\leq \\| \\hat{x}_{k+1} - y_{k} \\| + \\beta \\left(1 + \\frac{1 - \\sigma^{2}}{\\gamma_{k} a_{k}} A_{k} + a_{k} \\|z_{0} - x^{*}\\|\\right). \\tag{51}\n$$\n\n**Case II:** $$\\hat{\\eta}_{k} < \\eta_{k}$$. Since $$x_{k+1} = A_{k} + \\gamma_{k} a_{k} x_{k} + Ak+\\gamma_{k} a_{k}z_{k+1}$$ and $$\\hat{x}_{k+1} = Ak+a_{k}x_{k}+Ak+a_{k}z_{k+1}$$, we get\n\n$$\n\\hat{x}_{k+1} = Ak x_{k+1} + (1 - \\gamma_{k}) a_{k} z_{k+1}.\n$$", "md": "Now if we replace the above upper bound, i.e., $$\\log AN_{k}/a_{k}$$, on the right-hand side of (48) we obtain the result in (41). Next, note that by Cauchy-Schwarz inequality, we have\n\n$$\n\\begin{align*}\n&\\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_{k} \\| \\leq \\sqrt{\\sum_{k=0}^{N-1} a_{2k} \\sum_{k=0}^{N-1} \\eta_{2k}} \\\\\n&\\leq \\sqrt{1 - \\sigma^{2} \\sum_{k=0}^{N-1} a_{2k}} \\|z_{0} - x^{*}\\|,\n\\end{align*}\n$$\n\nwhere the last inequality follows from (21). Moreover, based on the expression for $$a_{k}$$ in (2) and the result in (41) that we just proved, we have\n\n$$\n\\begin{align*}\n&\\sum_{k=0}^{N-1} \\frac{\\eta_{2k}}{a_{2k}} = \\sum_{k=0}^{N-1} \\frac{(A_{k} + a_{k})^{2}}{a_{2k}} \\\\\n&\\leq \\sum_{k=0}^{N-1} (A_{k} + a_{k}) \\leq 1 + 2\\sqrt{\\beta} - \\beta + \\frac{1 + \\log AN_{1}}{A_{1}}.\n\\end{align*}\n$$\n\nCombining the two inequalities above leads to (42).\n\nNow we are ready to present and prove Lemma A.7 which characterizes a bound on the path length of the sequence $$\\{y_{k}\\}_{k=0}^{N}$$.\n\n**Lemma A.7.** Consider the iterates generated by Algorithm 1. Then for any N,\n\n$$\n\\sum_{k=0}^{N-1} \\|y_{k+1} - y_{k}\\| \\leq C_{2} (1 + \\log AN_{1}) \\|z_{0} - x^{*}\\|,\n$$\n\nwhere\n\n$$\nC_{2} = 2 \\left(1 - \\sigma^{2} \\sqrt{\\beta} - \\beta + \\sqrt{\\beta} \\left(1 + \\frac{1 - \\sigma^{2} \\sqrt{\\beta} - \\beta}{1 + 2\\sqrt{\\beta} - \\beta}\\right)\\right) \\tag{49}\n$$\n\n**Proof.** By the triangle inequality, we have\n\nWe again distinguish two cases.\n\n**Case I:** $$\\hat{\\eta}_{k} = \\eta_{k}$$. In this case $$\\hat{x}_{k+1} = x_{k+1}$$ and $$y_{k+1} = A_{k+1}+a_{k+1}x_{k+1}+Ak+1+a_{k+1}z_{k+1}$$, hence\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| = \\|x_{k+1} - y_{k+1}\\| = a_{k+1} \\|z_{k+1} - x_{k+1}\\| \\leq \\sqrt{1+\\beta} \\left(1 + 2\\gamma_{k} a_{k} \\|z_{0} - x^{*}\\|\\right),\n$$\n\nwhere we used Lemma A.5 and the fact that $$\\|z_{k+1} - x_{k+1}\\| \\leq \\|z_{k+1} - x^{*}\\| + \\|x_{k+1} - x^{*}\\| \\leq (1 + \\sqrt{1-\\sigma^{2}}) \\|z_{0} - x^{*}\\|$$ in the last inequality. Therefore, using (50) and the above bound we have\n\n$$\n\\|y_{k} - y_{k+1}\\| \\leq \\| \\hat{x}_{k+1} - y_{k} \\| + \\beta \\left(1 + \\frac{1 - \\sigma^{2}}{\\gamma_{k} a_{k}} A_{k} + a_{k} \\|z_{0} - x^{*}\\|\\right). \\tag{51}\n$$\n\n**Case II:** $$\\hat{\\eta}_{k} < \\eta_{k}$$. Since $$x_{k+1} = A_{k} + \\gamma_{k} a_{k} x_{k} + Ak+\\gamma_{k} a_{k}z_{k+1}$$ and $$\\hat{x}_{k+1} = Ak+a_{k}x_{k}+Ak+a_{k}z_{k+1}$$, we get\n\n$$\n\\hat{x}_{k+1} = Ak x_{k+1} + (1 - \\gamma_{k}) a_{k} z_{k+1}.\n$$"}]}, {"page": 25, "text": "                                                                                               A  k+1                      ak+1\nThus, given the above equality and the expression yk+1 =                                   A k+1+ak+1 xk+1 +           Ak+1+ak+1 zk+1, we have\n     \u2225\u02c6xk+1 \u2212      yk+1\u2225     \u2264   (1 \u2212    \u03b3k)ak    \u2225\u02dczk+1 \u2212     zk+1\u2225     +    (1 \u2212    \u03b3k)ak    \u2212          ak+1           \u2225zk+1 \u2212       xk+1\u2225.        (52)\n                                   Ak + ak                                      Ak + ak            Ak+1 + ak+1\nMoreover, based on the result in (26), we can upper bound \u2225\u02dc                                  zk+1 \u2212     zk+1\u2225     by \u03c3 ak \u03b7k \u2225\u02c6xk+1 \u2212     yk\u2225    which\nimplies that\n    (1 \u2212    \u03b3k)ak    \u2225\u02dc                                           k\n                      zk+1 \u2212      zk+1\u2225     \u2264   \u03c3 (1 \u2212     \u03b3k)a2       xk+1 \u2212      yk\u2225    = \u03c3(1 \u2212      \u03b3k)\u2225\u02c6  xk+1 \u2212      yk\u2225    \u2264   \u2225\u02c6\n      Ak + ak                                     \u03b7k(Ak + ak)\u2225\u02c6                                                                       xk+1 \u2212      yk\u2225\nwhere the equality holds due to the definition of ak, and the last inequality holds as both \u03b3k and \u03c3\nare in (0, 1). On the other hand, note that\n                     (1 \u2212    \u03b3k)ak    \u2212          ak+1          \u2264    (1 \u2212   \u03b3k)ak     \u2264        ak      ,                                              (53)\n                       Ak + ak            Ak+1 + ak+1                Ak + ak             Ak + ak\n                           ak+1                                             2\u221a\u03b2ak                                             ak\n                                          \u2212   (1 \u2212   \u03b3k)ak     \u2264    \u221a  \u03b2 + 1(Ak + ak) \u2212            (1 \u2212    \u03b3k)ak     \u2264                .              (54)\n                     Ak+1 + ak+1                Ak + ak                                              Ak + ak             Ak + ak\nwhere in the second bound we used the result in Lemma A.5 and the fact that 2sqrt\u03b2                                              \u221a \u03b2+1 < 1. Hence,\nwe get\n \u2225\u02c6xk+1 \u2212      yk+1\u2225     \u2264   \u2225\u02c6xk+1 \u2212     yk\u2225    + ak\u2225zk+1 \u2212          xk+1\u2225     \u2264   \u2225\u02c6xk+1 \u2212      yk\u2225   +      1 +           2      ak\u2225z0 \u2212         x\u2217\u2225   ,\n                                                            Ak + ak                                                      1 \u2212   \u03c32         Ak + ak\nwhere the last inequality follows from the fact \u2225zk+1 \u2212                               xk+1\u2225     \u2264   \u2225zk+1 \u2212      x\u2217\u2225    + \u2225xk+1 \u2212        x\u2217\u2225   and the\nbounds in Lemma A.4. Now by applying the above upper bound into (50) we obtain that\n                         \u2225yk \u2212     yk+1\u2225      \u2264  2\u2225\u02c6 xk+1 \u2212      yk\u2225   +      1 +           2             ak      \u2225z0 \u2212     x\u2217\u2225.                     (55)\n                                                                                        1 \u2212   \u03c32     Ak + ak\nConsidering the upper bounds established for \u2225yk \u2212                                    yk+1\u2225     in case I (equation (51)) and case II\n(equation (55)), we can conclude that\n                                                                        \u221a 1                    2             ak\n                      \u2225yk \u2212     yk+1\u2225      \u2264  2\u2225\u02c6 xk+1 \u2212      yk\u2225   +      \u03b2     1 +       1 \u2212   \u03c32     Ak + ak      \u2225z0 \u2212     x\u2217\u2225.                  (56)\nFinally, Lemma A.7 follows from summing (56) over k = 0 to N \u22121 and the result of Lemma A.6.\n                                                                            25", "md": "# Math Equations\n\nThus, given the above equality and the expression \\( y_{k+1} = A_{k+1} + a_{k+1} x_{k+1} + A_{k+1} + a_{k+1} z_{k+1} \\), we have\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| \\leq (1 - \\gamma_k) a_k \\| \\tilde{z}_{k+1} - z_{k+1} \\| + (1 - \\gamma_k) a_k - a_{k+1} \\| z_{k+1} - x_{k+1} \\| \\quad (52)\n$$\nMoreover, based on the result in (26), we can upper bound \\( \\| \\tilde{z}_{k+1} - z_{k+1} \\| \\) by \\( \\sigma a_k \\eta_k \\| \\hat{x}_{k+1} - y_k \\| \\) which implies that\n\n$$\n(1 - \\gamma_k) a_k \\| \\tilde{z}_{k+1} - z_{k+1} \\| \\leq \\sigma (1 - \\gamma_k) a^2 \\| x_{k+1} - y_k \\| = \\sigma(1 - \\gamma_k) \\| \\hat{x}_{k+1} - y_k \\| \\leq \\| \\hat{x}_{k+1} - y_k \\|\n$$\nwhere the equality holds due to the definition of \\( a_k \\), and the last inequality holds as both \\( \\gamma_k \\) and \\( \\sigma \\) are in (0, 1). On the other hand, note that\n\n$$\n(1 - \\gamma_k) a_k - a_{k+1} \\leq (1 - \\gamma_k) a_k \\leq a_k, \\quad (53)\n$$\n$$\na_{k+1} - (1 - \\gamma_k) a_k \\leq \\sqrt{\\beta} a_k - (1 - \\gamma_k) a_k \\leq \\sqrt{\\beta + 1} a_k \\quad (54)\n$$\nwhere in the second bound we used the result in Lemma A.5 and the fact that \\( 2\\sqrt{\\beta} < \\sqrt{\\beta + 1} < 1 \\). Hence, we get\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| \\leq \\| \\hat{x}_{k+1} - y_k \\| + a_k \\| z_{k+1} - x_{k+1} \\| \\leq \\| \\hat{x}_{k+1} - y_k \\| + \\frac{1 + 2a_k}{1 - \\sigma^2} \\| z_0 - x^* \\| ,\n$$\nwhere the last inequality follows from the fact \\( \\| z_{k+1} - x_{k+1} \\| \\leq \\| z_{k+1} - x^* \\| + \\| x_{k+1} - x^* \\| \\) and the bounds in Lemma A.4. Now by applying the above upper bound into (50) we obtain that\n\n$$\n\\| y_k - y_{k+1} \\| \\leq 2 \\| \\hat{x}_{k+1} - y_k \\| + \\frac{1 + 2a_k}{1 - \\sigma^2} \\| z_0 - x^* \\| . \\quad (55)\n$$\nConsidering the upper bounds established for \\( \\| y_k - y_{k+1} \\| \\) in case I (equation (51)) and case II (equation (55)), we can conclude that\n\n$$\n\\| y_k - y_{k+1} \\| \\leq 2 \\| \\hat{x}_{k+1} - y_k \\| + \\beta \\sqrt{1 + \\frac{2a_k}{1 - \\sigma^2}} \\| z_0 - x^* \\| . \\quad (56)\n$$\nFinally, Lemma A.7 follows from summing (56) over \\( k = 0 \\) to \\( N - 1 \\) and the result of Lemma A.6.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Thus, given the above equality and the expression \\( y_{k+1} = A_{k+1} + a_{k+1} x_{k+1} + A_{k+1} + a_{k+1} z_{k+1} \\), we have\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| \\leq (1 - \\gamma_k) a_k \\| \\tilde{z}_{k+1} - z_{k+1} \\| + (1 - \\gamma_k) a_k - a_{k+1} \\| z_{k+1} - x_{k+1} \\| \\quad (52)\n$$\nMoreover, based on the result in (26), we can upper bound \\( \\| \\tilde{z}_{k+1} - z_{k+1} \\| \\) by \\( \\sigma a_k \\eta_k \\| \\hat{x}_{k+1} - y_k \\| \\) which implies that\n\n$$\n(1 - \\gamma_k) a_k \\| \\tilde{z}_{k+1} - z_{k+1} \\| \\leq \\sigma (1 - \\gamma_k) a^2 \\| x_{k+1} - y_k \\| = \\sigma(1 - \\gamma_k) \\| \\hat{x}_{k+1} - y_k \\| \\leq \\| \\hat{x}_{k+1} - y_k \\|\n$$\nwhere the equality holds due to the definition of \\( a_k \\), and the last inequality holds as both \\( \\gamma_k \\) and \\( \\sigma \\) are in (0, 1). On the other hand, note that\n\n$$\n(1 - \\gamma_k) a_k - a_{k+1} \\leq (1 - \\gamma_k) a_k \\leq a_k, \\quad (53)\n$$\n$$\na_{k+1} - (1 - \\gamma_k) a_k \\leq \\sqrt{\\beta} a_k - (1 - \\gamma_k) a_k \\leq \\sqrt{\\beta + 1} a_k \\quad (54)\n$$\nwhere in the second bound we used the result in Lemma A.5 and the fact that \\( 2\\sqrt{\\beta} < \\sqrt{\\beta + 1} < 1 \\). Hence, we get\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| \\leq \\| \\hat{x}_{k+1} - y_k \\| + a_k \\| z_{k+1} - x_{k+1} \\| \\leq \\| \\hat{x}_{k+1} - y_k \\| + \\frac{1 + 2a_k}{1 - \\sigma^2} \\| z_0 - x^* \\| ,\n$$\nwhere the last inequality follows from the fact \\( \\| z_{k+1} - x_{k+1} \\| \\leq \\| z_{k+1} - x^* \\| + \\| x_{k+1} - x^* \\| \\) and the bounds in Lemma A.4. Now by applying the above upper bound into (50) we obtain that\n\n$$\n\\| y_k - y_{k+1} \\| \\leq 2 \\| \\hat{x}_{k+1} - y_k \\| + \\frac{1 + 2a_k}{1 - \\sigma^2} \\| z_0 - x^* \\| . \\quad (55)\n$$\nConsidering the upper bounds established for \\( \\| y_k - y_{k+1} \\| \\) in case I (equation (51)) and case II (equation (55)), we can conclude that\n\n$$\n\\| y_k - y_{k+1} \\| \\leq 2 \\| \\hat{x}_{k+1} - y_k \\| + \\beta \\sqrt{1 + \\frac{2a_k}{1 - \\sigma^2}} \\| z_0 - x^* \\| . \\quad (56)\n$$\nFinally, Lemma A.7 follows from summing (56) over \\( k = 0 \\) to \\( N - 1 \\) and the result of Lemma A.6.", "md": "Thus, given the above equality and the expression \\( y_{k+1} = A_{k+1} + a_{k+1} x_{k+1} + A_{k+1} + a_{k+1} z_{k+1} \\), we have\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| \\leq (1 - \\gamma_k) a_k \\| \\tilde{z}_{k+1} - z_{k+1} \\| + (1 - \\gamma_k) a_k - a_{k+1} \\| z_{k+1} - x_{k+1} \\| \\quad (52)\n$$\nMoreover, based on the result in (26), we can upper bound \\( \\| \\tilde{z}_{k+1} - z_{k+1} \\| \\) by \\( \\sigma a_k \\eta_k \\| \\hat{x}_{k+1} - y_k \\| \\) which implies that\n\n$$\n(1 - \\gamma_k) a_k \\| \\tilde{z}_{k+1} - z_{k+1} \\| \\leq \\sigma (1 - \\gamma_k) a^2 \\| x_{k+1} - y_k \\| = \\sigma(1 - \\gamma_k) \\| \\hat{x}_{k+1} - y_k \\| \\leq \\| \\hat{x}_{k+1} - y_k \\|\n$$\nwhere the equality holds due to the definition of \\( a_k \\), and the last inequality holds as both \\( \\gamma_k \\) and \\( \\sigma \\) are in (0, 1). On the other hand, note that\n\n$$\n(1 - \\gamma_k) a_k - a_{k+1} \\leq (1 - \\gamma_k) a_k \\leq a_k, \\quad (53)\n$$\n$$\na_{k+1} - (1 - \\gamma_k) a_k \\leq \\sqrt{\\beta} a_k - (1 - \\gamma_k) a_k \\leq \\sqrt{\\beta + 1} a_k \\quad (54)\n$$\nwhere in the second bound we used the result in Lemma A.5 and the fact that \\( 2\\sqrt{\\beta} < \\sqrt{\\beta + 1} < 1 \\). Hence, we get\n\n$$\n\\| \\hat{x}_{k+1} - y_{k+1} \\| \\leq \\| \\hat{x}_{k+1} - y_k \\| + a_k \\| z_{k+1} - x_{k+1} \\| \\leq \\| \\hat{x}_{k+1} - y_k \\| + \\frac{1 + 2a_k}{1 - \\sigma^2} \\| z_0 - x^* \\| ,\n$$\nwhere the last inequality follows from the fact \\( \\| z_{k+1} - x_{k+1} \\| \\leq \\| z_{k+1} - x^* \\| + \\| x_{k+1} - x^* \\| \\) and the bounds in Lemma A.4. Now by applying the above upper bound into (50) we obtain that\n\n$$\n\\| y_k - y_{k+1} \\| \\leq 2 \\| \\hat{x}_{k+1} - y_k \\| + \\frac{1 + 2a_k}{1 - \\sigma^2} \\| z_0 - x^* \\| . \\quad (55)\n$$\nConsidering the upper bounds established for \\( \\| y_k - y_{k+1} \\| \\) in case I (equation (51)) and case II (equation (55)), we can conclude that\n\n$$\n\\| y_k - y_{k+1} \\| \\leq 2 \\| \\hat{x}_{k+1} - y_k \\| + \\beta \\sqrt{1 + \\frac{2a_k}{1 - \\sigma^2}} \\| z_0 - x^* \\| . \\quad (56)\n$$\nFinally, Lemma A.7 follows from summing (56) over \\( k = 0 \\) to \\( N - 1 \\) and the result of Lemma A.6."}]}, {"page": 26, "text": " Subroutine 1 Backtracking line search\n  1: Input: iterate y \u2208      Rd, gradient g \u2208     Rd, Hessian approximation B \u2208           Sd\n                                                                                           +, initial trial step size \u03b7 > 0\n  2: Parameters: line search parameters \u03b2 \u2208              (0, 1), \u03b11 \u2265  0 and \u03b12 > 0 such that \u03b11 + \u03b12 < 1\n  3: Set \u02c6\u03b7 \u2190   \u03b7\n  4: Compute s+ \u2190        LinearSolver(I + \u02c6  \u03b7B, \u2212\u02c6 \u03b7g; \u03b11) and \u02c6  x+ \u2190   y + s+\n  5: while \u2225\u02c6  x+ \u2212   y + \u02c6\u03b7\u2207f(\u02c6  x+)\u22252 > (\u03b11 + \u03b12)\u2225\u02c6     x+ \u2212  y\u22252 do\n  6:    Set \u02dcx+ \u2190    \u02c6\n                     x+ and \u02c6  \u03b7 \u2190   \u03b2\u02c6\n                                      \u03b7\n  7:    Compute s+ \u2190        LinearSolver(I + \u02c6  \u03b7B, \u2212\u02c6 \u03b7g; \u03b11) and \u02c6  x+ \u2190    y + s+\n  8: end while\n  9: if \u02c6\n        \u03b7 = \u03b7 then\n 10:    Return \u02c6   \u03b7 and \u02c6 x+\n 11: else\n 12:    Return \u02c6   \u03b7, \u02c6\n                      x+ and \u02dc  x+\n 13: end if\n B      Line Search Subroutine\n In this section, we provide further details on our line search subroutine in Section 3.1.                                     For\n completeness, the pseudocode of our line search scheme is shown in Subroutine 1. In Section B.1,\nwe prove that Subrountine 1 will always terminate in a finite number of steps. In Section B.2, we\n provide the proof of Lemma 3.2.\n B.1      The Line Search Subroutine Terminates Properly\n Recall that in our line search scheme, we keep decreasing the step size \u02c6                         \u03b7 by a factor of \u03b2 until\nwe find a pair (\u02c6     \u03b7, \u02c6\n                         x+) satisfying (11) (also see Lines 5 and 6 in Subroutine 1). In the following\n lemma, we show that when the step size \u02c6              \u03b7 is smaller than a certain threshold, then the pair (\u02c6              \u03b7, \u02c6\n                                                                                                                               x+)\n satisfies both conditions in (10) and (11), which further implies that Subroutine 1 will stop in a\n finite number of steps.\n                                                                              \u03b12\n Lemma B.1. Suppose Assumption 2.1 holds. If \u02c6                      \u03b7 <   L1+\u2225B\u2225op and \u02c6     x+ is computed according to\n(12), then the pair (\u02c6     \u03b7, \u02c6\n                              x+) satisfies the conditions in (10) and (11).\n Proof. By Definition 3.1, the pair (\u02c6        \u03b7, \u02c6\n                                                 x+) always satisfies the condition in (10) when \u02c6             x+ is computed\n from (12). Hence, in the following we only need to prove that the condition in (11) also holds.\n Recall that g = \u2207f(y). By Assumption 2.1, the function f is L1-smooth and thus we have\n                               \u2225\u2207f(\u02c6  x+) \u2212    g\u2225  = \u2225\u2207f(\u02c6   x+) \u2212    \u2207f(y)\u2225     \u2264  L1\u2225\u02c6 x+ \u2212    y\u2225.\n Moreover, by using the triangle inequality, we get\n        \u2225\u2207f(\u02c6  x+) \u2212   g \u2212   B(\u02c6 x+ \u2212   y)\u2225   \u2264  \u2225\u2207f(\u02c6  x+) \u2212    g\u2225  + \u2225B(\u02c6  x+ \u2212   y)\u2225   \u2264  (L1 + \u2225B\u2225op)\u2225\u02c6     x+ \u2212    y\u2225.\n                        \u03b12\n Hence, if \u02c6 \u03b7 \u2264   L1+\u2225B\u2225op , we have\n                                    \u02c6\n                                    \u03b7\u2225\u2207f(\u02c6   x+) \u2212    g \u2212  B(\u02c6 x+ \u2212   y)\u2225   \u2264  \u03b12\u2225\u02c6 x+ \u2212   y\u2225.                                (57)\n                                                                26", "md": "Subroutine 1 Backtracking line search\n\n1: Input: iterate \\( y \\in \\mathbb{R}^d \\), gradient \\( g \\in \\mathbb{R}^d \\), Hessian approximation \\( B \\in \\mathbb{S}^d_+ \\), initial trial step size \\( \\eta > 0 \\)\n\n2: Parameters: line search parameters \\( \\beta \\in (0, 1) \\), \\( \\alpha_1 \\geq 0 \\) and \\( \\alpha_2 > 0 \\) such that \\( \\alpha_1 + \\alpha_2 < 1 \\)\n\n3: Set \\( \\hat{\\eta} \\leftarrow \\eta \\)\n\n4: Compute \\( s^+ \\leftarrow \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\) and \\( \\hat{x}^+ \\leftarrow y + s^+ \\)\n\n5: while \\( \\| \\hat{x}^+ - y + \\hat{\\eta}\\nabla f(\\hat{x}^+) \\|_2 > (\\alpha_1 + \\alpha_2)\\| \\hat{x}^+ - y \\|_2 \\) do\n\n6: Set \\( \\tilde{x}^+ \\leftarrow \\hat{x}^+ \\) and \\( \\hat{\\eta} \\leftarrow \\beta\\hat{\\eta} \\)\n\n7: Compute \\( s^+ \\leftarrow \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\) and \\( \\hat{x}^+ \\leftarrow y + s^+ \\)\n\n8: end while\n\n9: if \\( \\hat{\\eta} = \\eta \\) then\n\n10: Return \\( \\hat{\\eta} \\) and \\( \\hat{x}^+ \\)\n\n11: else\n\n12: Return \\( \\hat{\\eta} \\), \\( \\hat{x}^+ \\) and \\( \\tilde{x}^+ \\)\n\n13: end if\n\nB Line Search Subroutine\n\nIn this section, we provide further details on our line search subroutine in Section 3.1. For completeness, the pseudocode of our line search scheme is shown in Subroutine 1. In Section B.1, we prove that Subrountine 1 will always terminate in a finite number of steps. In Section B.2, we provide the proof of Lemma 3.2.\n\nB.1 The Line Search Subroutine Terminates Properly\n\nRecall that in our line search scheme, we keep decreasing the step size \\( \\hat{\\eta} \\) by a factor of \\( \\beta \\) until we find a pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfying (11) (also see Lines 5 and 6 in Subroutine 1). In the following lemma, we show that when the step size \\( \\hat{\\eta} \\) is smaller than a certain threshold, then the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfies both conditions in (10) and (11), which further implies that Subroutine 1 will stop in a finite number of steps.\n\nLemma B.1. Suppose Assumption 2.1 holds. If \\( \\hat{\\eta} < L1+\\|B\\|_{\\text{op}} \\) and \\( \\hat{x}^+ \\) is computed according to (12), then the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfies the conditions in (10) and (11).\n\nProof. By Definition 3.1, the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) always satisfies the condition in (10) when \\( \\hat{x}^+ \\) is computed from (12). Hence, in the following we only need to prove that the condition in (11) also holds.\n\nRecall that \\( g = \\nabla f(y) \\). By Assumption 2.1, the function \\( f \\) is \\( L1 \\)-smooth and thus we have\n\n$$ \\| \\nabla f(\\hat{x}^+) - g \\| = \\| \\nabla f(\\hat{x}^+) - \\nabla f(y) \\| \\leq L1\\|\\hat{x}^+ - y\\| $$\nMoreover, by using the triangle inequality, we get\n\n$$ \\| \\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y) \\| \\leq \\| \\nabla f(\\hat{x}^+) - g \\| + \\| B(\\hat{x}^+ - y) \\| \\leq (L1 + \\|B\\|_{\\text{op}})\\|\\hat{x}^+ - y\\| $$\nHence, if \\( \\hat{\\eta} \\leq L1+\\|B\\|_{\\text{op}} \\), we have\n\n$$ \\hat{\\eta}\\| \\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y) \\| \\leq \\alpha2\\|\\hat{x}^+ - y\\| $$\n26", "images": [], "items": [{"type": "text", "value": "Subroutine 1 Backtracking line search\n\n1: Input: iterate \\( y \\in \\mathbb{R}^d \\), gradient \\( g \\in \\mathbb{R}^d \\), Hessian approximation \\( B \\in \\mathbb{S}^d_+ \\), initial trial step size \\( \\eta > 0 \\)\n\n2: Parameters: line search parameters \\( \\beta \\in (0, 1) \\), \\( \\alpha_1 \\geq 0 \\) and \\( \\alpha_2 > 0 \\) such that \\( \\alpha_1 + \\alpha_2 < 1 \\)\n\n3: Set \\( \\hat{\\eta} \\leftarrow \\eta \\)\n\n4: Compute \\( s^+ \\leftarrow \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\) and \\( \\hat{x}^+ \\leftarrow y + s^+ \\)\n\n5: while \\( \\| \\hat{x}^+ - y + \\hat{\\eta}\\nabla f(\\hat{x}^+) \\|_2 > (\\alpha_1 + \\alpha_2)\\| \\hat{x}^+ - y \\|_2 \\) do\n\n6: Set \\( \\tilde{x}^+ \\leftarrow \\hat{x}^+ \\) and \\( \\hat{\\eta} \\leftarrow \\beta\\hat{\\eta} \\)\n\n7: Compute \\( s^+ \\leftarrow \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\) and \\( \\hat{x}^+ \\leftarrow y + s^+ \\)\n\n8: end while\n\n9: if \\( \\hat{\\eta} = \\eta \\) then\n\n10: Return \\( \\hat{\\eta} \\) and \\( \\hat{x}^+ \\)\n\n11: else\n\n12: Return \\( \\hat{\\eta} \\), \\( \\hat{x}^+ \\) and \\( \\tilde{x}^+ \\)\n\n13: end if\n\nB Line Search Subroutine\n\nIn this section, we provide further details on our line search subroutine in Section 3.1. For completeness, the pseudocode of our line search scheme is shown in Subroutine 1. In Section B.1, we prove that Subrountine 1 will always terminate in a finite number of steps. In Section B.2, we provide the proof of Lemma 3.2.\n\nB.1 The Line Search Subroutine Terminates Properly\n\nRecall that in our line search scheme, we keep decreasing the step size \\( \\hat{\\eta} \\) by a factor of \\( \\beta \\) until we find a pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfying (11) (also see Lines 5 and 6 in Subroutine 1). In the following lemma, we show that when the step size \\( \\hat{\\eta} \\) is smaller than a certain threshold, then the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfies both conditions in (10) and (11), which further implies that Subroutine 1 will stop in a finite number of steps.\n\nLemma B.1. Suppose Assumption 2.1 holds. If \\( \\hat{\\eta} < L1+\\|B\\|_{\\text{op}} \\) and \\( \\hat{x}^+ \\) is computed according to (12), then the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfies the conditions in (10) and (11).\n\nProof. By Definition 3.1, the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) always satisfies the condition in (10) when \\( \\hat{x}^+ \\) is computed from (12). Hence, in the following we only need to prove that the condition in (11) also holds.\n\nRecall that \\( g = \\nabla f(y) \\). By Assumption 2.1, the function \\( f \\) is \\( L1 \\)-smooth and thus we have\n\n$$ \\| \\nabla f(\\hat{x}^+) - g \\| = \\| \\nabla f(\\hat{x}^+) - \\nabla f(y) \\| \\leq L1\\|\\hat{x}^+ - y\\| $$\nMoreover, by using the triangle inequality, we get\n\n$$ \\| \\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y) \\| \\leq \\| \\nabla f(\\hat{x}^+) - g \\| + \\| B(\\hat{x}^+ - y) \\| \\leq (L1 + \\|B\\|_{\\text{op}})\\|\\hat{x}^+ - y\\| $$\nHence, if \\( \\hat{\\eta} \\leq L1+\\|B\\|_{\\text{op}} \\), we have\n\n$$ \\hat{\\eta}\\| \\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y) \\| \\leq \\alpha2\\|\\hat{x}^+ - y\\| $$\n26", "md": "Subroutine 1 Backtracking line search\n\n1: Input: iterate \\( y \\in \\mathbb{R}^d \\), gradient \\( g \\in \\mathbb{R}^d \\), Hessian approximation \\( B \\in \\mathbb{S}^d_+ \\), initial trial step size \\( \\eta > 0 \\)\n\n2: Parameters: line search parameters \\( \\beta \\in (0, 1) \\), \\( \\alpha_1 \\geq 0 \\) and \\( \\alpha_2 > 0 \\) such that \\( \\alpha_1 + \\alpha_2 < 1 \\)\n\n3: Set \\( \\hat{\\eta} \\leftarrow \\eta \\)\n\n4: Compute \\( s^+ \\leftarrow \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\) and \\( \\hat{x}^+ \\leftarrow y + s^+ \\)\n\n5: while \\( \\| \\hat{x}^+ - y + \\hat{\\eta}\\nabla f(\\hat{x}^+) \\|_2 > (\\alpha_1 + \\alpha_2)\\| \\hat{x}^+ - y \\|_2 \\) do\n\n6: Set \\( \\tilde{x}^+ \\leftarrow \\hat{x}^+ \\) and \\( \\hat{\\eta} \\leftarrow \\beta\\hat{\\eta} \\)\n\n7: Compute \\( s^+ \\leftarrow \\text{LinearSolver}(I + \\hat{\\eta}B, -\\hat{\\eta}g; \\alpha_1) \\) and \\( \\hat{x}^+ \\leftarrow y + s^+ \\)\n\n8: end while\n\n9: if \\( \\hat{\\eta} = \\eta \\) then\n\n10: Return \\( \\hat{\\eta} \\) and \\( \\hat{x}^+ \\)\n\n11: else\n\n12: Return \\( \\hat{\\eta} \\), \\( \\hat{x}^+ \\) and \\( \\tilde{x}^+ \\)\n\n13: end if\n\nB Line Search Subroutine\n\nIn this section, we provide further details on our line search subroutine in Section 3.1. For completeness, the pseudocode of our line search scheme is shown in Subroutine 1. In Section B.1, we prove that Subrountine 1 will always terminate in a finite number of steps. In Section B.2, we provide the proof of Lemma 3.2.\n\nB.1 The Line Search Subroutine Terminates Properly\n\nRecall that in our line search scheme, we keep decreasing the step size \\( \\hat{\\eta} \\) by a factor of \\( \\beta \\) until we find a pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfying (11) (also see Lines 5 and 6 in Subroutine 1). In the following lemma, we show that when the step size \\( \\hat{\\eta} \\) is smaller than a certain threshold, then the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfies both conditions in (10) and (11), which further implies that Subroutine 1 will stop in a finite number of steps.\n\nLemma B.1. Suppose Assumption 2.1 holds. If \\( \\hat{\\eta} < L1+\\|B\\|_{\\text{op}} \\) and \\( \\hat{x}^+ \\) is computed according to (12), then the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) satisfies the conditions in (10) and (11).\n\nProof. By Definition 3.1, the pair \\( (\\hat{\\eta}, \\hat{x}^+) \\) always satisfies the condition in (10) when \\( \\hat{x}^+ \\) is computed from (12). Hence, in the following we only need to prove that the condition in (11) also holds.\n\nRecall that \\( g = \\nabla f(y) \\). By Assumption 2.1, the function \\( f \\) is \\( L1 \\)-smooth and thus we have\n\n$$ \\| \\nabla f(\\hat{x}^+) - g \\| = \\| \\nabla f(\\hat{x}^+) - \\nabla f(y) \\| \\leq L1\\|\\hat{x}^+ - y\\| $$\nMoreover, by using the triangle inequality, we get\n\n$$ \\| \\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y) \\| \\leq \\| \\nabla f(\\hat{x}^+) - g \\| + \\| B(\\hat{x}^+ - y) \\| \\leq (L1 + \\|B\\|_{\\text{op}})\\|\\hat{x}^+ - y\\| $$\nHence, if \\( \\hat{\\eta} \\leq L1+\\|B\\|_{\\text{op}} \\), we have\n\n$$ \\hat{\\eta}\\| \\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y) \\| \\leq \\alpha2\\|\\hat{x}^+ - y\\| $$\n26"}]}, {"page": 27, "text": "Finally, by using the triangle inequality, we can combine (10) and (57) to show that\n      \u2225\u02c6\n       x+ \u2212  y + \u02c6\u03b7\u2207f(\u02c6 x+)\u2225  = \u2225\u02c6x+ \u2212   y + \u02c6\n                                             \u03b7(g + B(\u02c6 x+ \u2212   y)) + \u02c6\n                              \u2264  \u2225\u02c6                                 \u03b7(\u2207f(\u02c6 x+) \u2212   g \u2212 B(\u02c6x+ \u2212   y))\u2225\n                                  x+ \u2212   y + \u02c6\n                                             \u03b7(g + B(\u02c6 x+ \u2212   y))\u2225 + \u2225\u02c6\u03b7(\u2207f(\u02c6 x+) \u2212  g \u2212  B(\u02c6x+ \u2212  y))\u2225\n                              \u2264  \u03b11\u2225\u02c6x+ \u2212  y\u2225  + \u03b12\u2225\u02c6x+ \u2212   y\u2225\n                              \u2264  (\u03b11 + \u03b12)\u2225\u02c6 x+ \u2212  y\u2225,\nwhich means the condition in (11) is satisfied. The proof is now complete.\nB.2     Proof of Lemma 3.2\nWe follow a similar proof strategy as Lemma 3 in [JJM23]. In the first case where k /                \u2208 B, by\ndefinition, the line search subroutine accepts the initial step size \u03b7k, i.e., \u02c6\u03b7k = \u03b7k. In the second case\nwhere k \u2208   B, the line search subroutine backtracks and returns the auxiliary iterate \u02dc        xk+1, which\nis computed from (12) using the step size \u02dc     \u03b7k \u225c  \u02c6\n                                                      \u03b7k/\u03b2. Since the step size \u02dc \u03b7k is rejected in our line\nsearch subroutine, it implies that the pair (\u02dc   xk+1, \u02dc\n                                                       \u03b7k) does not satisfy (11), i.e.,\n                          \u2225\u02dc\n                           xk+1 \u2212  yk + \u02dc\u03b7k\u2207f(\u02dc xk+1)\u2225   > (\u03b11 + \u03b12)\u2225\u02dc xk+1 \u2212  yk\u2225.                      (58)\nMoreover, since we compute \u02dc    xk+1 from (12) using step size \u02dc  \u03b7k, the pair (\u02dc\u03b7k, \u02dc\n                                                                                     xk+1) also satisfies the\ncondition in (10), which means\n                    \u2225\u02dc\n                     xk+1 \u2212   yk + \u02dc\u03b7k(\u2207f(yk) + Bk(\u02dc   xk+1 \u2212  yk))\u2225  \u2264  \u03b11\u2225\u02dcxk+1 \u2212  yk\u2225.                (59)\nHence, by using the triangle inequality, we can combine (58) and (59) to get\n                \u02dc\n               \u03b7k\u2225\u2207f(\u02dc  xk+1) \u2212  \u2207f(yk) \u2212    Bk(\u02dcxk+1 \u2212  yk)\u2225\n             \u2265 \u2225\u02dcxk+1 \u2212  yk + \u02dc\u03b7k\u2207f(\u02dc xk+1)\u2225  \u2212  \u2225\u02dc\n                                                  xk+1 \u2212  yk + \u02dc\u03b7k(\u2207f(yk) + Bk(\u02dc   xk+1 \u2212   yk))\u2225\n             > (\u03b11 + \u03b12)\u2225\u02dc xk+1 \u2212   yk\u2225 \u2212  \u03b11\u2225\u02dcxk+1 \u2212  yk\u2225\n             = \u03b12\u2225\u02dcxk+1 \u2212   yk\u2225,\nwhich implies that\n                          \u02c6                         \u03b12\u03b2\u2225\u02dc xk+1 \u2212  yk\u2225\n                         \u03b7k = \u03b2\u02dc \u03b7k >  \u2225\u2207f(\u02dc xk+1) \u2212  \u2207f(yk) \u2212    Bk(\u02dcxk+1 \u2212  yk)\u2225.\nThis proves the first inequality in (13).\nTo show the second inequality in (13), first note that \u02dc   xk+1 and \u02c6 xk+1 are the inexact solutions of the\nlinear system of equations\n                  (I + \u02dc\n                       \u03b7kBk)(x \u2212    yk) = \u2212\u02dc \u03b7kgk    and   (I + \u02c6\nrespectively.   Let \u02dcx\u2217          x\u2217                             \u03b7kBk)(x \u2212    yk) = \u2212\u02c6 \u03b7kgk,\n                       k+1 and \u02c6k+1 be the exact solutions of the above linear systems, that is,\n\u02dc\nx\u2217             \u03b7k(I + \u02dc\n                      \u03b7kBk)\u22121gk and \u02c6   x\u2217            \u03b7k(I + \u02c6\u03b7kBk)\u22121gk. We first establish the following\n  k+1 = yk \u2212   \u02dc                          k+1 = yk \u2212   \u02c6\ninequality between \u2225\u02dc  x\u2217                  x\u2217\n                         k+1 \u2212  yk\u2225 and \u2225\u02c6k+1 \u2212    yk\u2225:\n                                      \u2225\u02dc\n                                       x\u2217                  x\u2217                                            (60)\n                                         k+1 \u2212  yk\u2225  \u2264 \u03b21\u2225\u02c6 k+1 \u2212  yk\u2225.\n                                                      27", "md": "Finally, by using the triangle inequality, we can combine (10) and (57) to show that\n\n$$\n\\left\\| \\hat{x}^+ - y + \\hat{\\eta} \\nabla f(\\hat{x}^+) \\right\\| = \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y)) \\right\\| \\leq \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(\\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y)) \\right\\| \\leq \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y)) \\right\\| + \\left\\| \\hat{\\eta}(\\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y)) \\right\\| \\leq \\alpha_1 \\left\\| \\hat{x}^+ - y \\right\\| + \\alpha_2 \\left\\| \\hat{x}^+ - y \\right\\| \\leq (\\alpha_1 + \\alpha_2) \\left\\| \\hat{x}^+ - y \\right\\|,$$\nwhich means the condition in (11) is satisfied. The proof is now complete.\n\nB.2 Proof of Lemma 3.2\n\nWe follow a similar proof strategy as Lemma 3 in [JJM23]. In the first case where k \u2209 B, by definition, the line search subroutine accepts the initial step size \u03b7k, i.e., $\\hat{\\eta}_k = \\eta_k$. In the second case where k \u2208 B, the line search subroutine backtracks and returns the auxiliary iterate $\\tilde{x}_{k+1}$, which is computed from (12) using the step size $\\tilde{\\eta}_k \\triangleq \\hat{\\eta}_k/\\beta$. Since the step size $\\tilde{\\eta}_k$ is rejected in our line search subroutine, it implies that the pair ($\\tilde{x}_{k+1}, \\tilde{\\eta}_k$) does not satisfy (11), i.e.,\n\n$$\n\\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k \\nabla f(\\tilde{x}_{k+1}) \\right\\| > (\\alpha_1 + \\alpha_2) \\left\\| \\tilde{x}_{k+1} - y_k \\right|. \\quad (58)\n$$\nMoreover, since we compute $\\tilde{x}_{k+1}$ from (12) using step size $\\tilde{\\eta}_k$, the pair ($\\tilde{\\eta}_k, \\tilde{x}_{k+1}$) also satisfies the condition in (10), which means\n\n$$\n\\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k(\\nabla f(y_k) + B_k(\\tilde{x}_{k+1} - y_k)) \\right\\| \\leq \\alpha_1 \\left\\| \\tilde{x}_{k+1} - y_k \\right|. \\quad (59)\n$$\nHence, by using the triangle inequality, we can combine (58) and (59) to get\n\n$$\n\\tilde{\\eta}_k \\left\\| \\nabla f(\\tilde{x}_{k+1}) - \\nabla f(y_k) - B_k(\\tilde{x}_{k+1} - y_k) \\right\\| \\geq \\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k \\nabla f(\\tilde{x}_{k+1}) \\right\\| - \\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k(\\nabla f(y_k) + B_k(\\tilde{x}_{k+1} - y_k)) \\right\\| > (\\alpha_1 + \\alpha_2) \\left\\| \\tilde{x}_{k+1} - y_k \\right| - \\alpha_1 \\left\\| \\tilde{x}_{k+1} - y_k \\right| = \\alpha_2 \\left\\| \\tilde{x}_{k+1} - y_k \\right|,$$\nwhich implies that\n\n$$\n\\hat{\\eta}_k = \\beta \\tilde{\\eta}_k > \\alpha_2 \\beta \\left\\| \\tilde{x}_{k+1} - y_k \\right\\|.\n$$\nThis proves the first inequality in (13).\n\nTo show the second inequality in (13), first note that $\\tilde{x}_{k+1}$ and $\\hat{x}_{k+1}$ are the inexact solutions of the linear system of equations\n\n$$\n(I + \\tilde{\\eta}_k B_k)(x - y_k) = -\\tilde{\\eta}_k g_k \\quad \\text{and} \\quad (I + \\hat{\\eta}_k B_k)(x - y_k) = -\\hat{\\eta}_k g_k,\n$$\nrespectively. Let $\\tilde{x}^*_k+1$ and $\\hat{x}^*_k+1$ be the exact solutions of the above linear systems, that is,\n\n$$\n\\tilde{x}^*_k+1 = y_k - \\tilde{\\eta}_k(I + \\tilde{\\eta}_k B_k)^{-1}g_k \\quad \\text{and} \\quad \\hat{x}^*_k+1 = y_k - \\hat{\\eta}_k(I + \\hat{\\eta}_k B_k)^{-1}g_k.\n$$\nWe first establish the following inequality between $\\left\\| \\tilde{x}^*_k+1 - y_k \\right\\|$ and $\\left\\| \\hat{x}^*_k+1 - y_k \\right\\|$:\n\n$$\n\\left\\| \\tilde{x}^*_k+1 - y_k \\right\\| \\leq \\beta_1 \\left\\| \\hat{x}^*_k+1 - y_k \\right|. \\quad (60)\n$$", "images": [], "items": [{"type": "text", "value": "Finally, by using the triangle inequality, we can combine (10) and (57) to show that\n\n$$\n\\left\\| \\hat{x}^+ - y + \\hat{\\eta} \\nabla f(\\hat{x}^+) \\right\\| = \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y)) \\right\\| \\leq \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(\\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y)) \\right\\| \\leq \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y)) \\right\\| + \\left\\| \\hat{\\eta}(\\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y)) \\right\\| \\leq \\alpha_1 \\left\\| \\hat{x}^+ - y \\right\\| + \\alpha_2 \\left\\| \\hat{x}^+ - y \\right\\| \\leq (\\alpha_1 + \\alpha_2) \\left\\| \\hat{x}^+ - y \\right\\|,$$\nwhich means the condition in (11) is satisfied. The proof is now complete.\n\nB.2 Proof of Lemma 3.2\n\nWe follow a similar proof strategy as Lemma 3 in [JJM23]. In the first case where k \u2209 B, by definition, the line search subroutine accepts the initial step size \u03b7k, i.e., $\\hat{\\eta}_k = \\eta_k$. In the second case where k \u2208 B, the line search subroutine backtracks and returns the auxiliary iterate $\\tilde{x}_{k+1}$, which is computed from (12) using the step size $\\tilde{\\eta}_k \\triangleq \\hat{\\eta}_k/\\beta$. Since the step size $\\tilde{\\eta}_k$ is rejected in our line search subroutine, it implies that the pair ($\\tilde{x}_{k+1}, \\tilde{\\eta}_k$) does not satisfy (11), i.e.,\n\n$$\n\\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k \\nabla f(\\tilde{x}_{k+1}) \\right\\| > (\\alpha_1 + \\alpha_2) \\left\\| \\tilde{x}_{k+1} - y_k \\right|. \\quad (58)\n$$\nMoreover, since we compute $\\tilde{x}_{k+1}$ from (12) using step size $\\tilde{\\eta}_k$, the pair ($\\tilde{\\eta}_k, \\tilde{x}_{k+1}$) also satisfies the condition in (10), which means\n\n$$\n\\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k(\\nabla f(y_k) + B_k(\\tilde{x}_{k+1} - y_k)) \\right\\| \\leq \\alpha_1 \\left\\| \\tilde{x}_{k+1} - y_k \\right|. \\quad (59)\n$$\nHence, by using the triangle inequality, we can combine (58) and (59) to get\n\n$$\n\\tilde{\\eta}_k \\left\\| \\nabla f(\\tilde{x}_{k+1}) - \\nabla f(y_k) - B_k(\\tilde{x}_{k+1} - y_k) \\right\\| \\geq \\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k \\nabla f(\\tilde{x}_{k+1}) \\right\\| - \\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k(\\nabla f(y_k) + B_k(\\tilde{x}_{k+1} - y_k)) \\right\\| > (\\alpha_1 + \\alpha_2) \\left\\| \\tilde{x}_{k+1} - y_k \\right| - \\alpha_1 \\left\\| \\tilde{x}_{k+1} - y_k \\right| = \\alpha_2 \\left\\| \\tilde{x}_{k+1} - y_k \\right|,$$\nwhich implies that\n\n$$\n\\hat{\\eta}_k = \\beta \\tilde{\\eta}_k > \\alpha_2 \\beta \\left\\| \\tilde{x}_{k+1} - y_k \\right\\|.\n$$\nThis proves the first inequality in (13).\n\nTo show the second inequality in (13), first note that $\\tilde{x}_{k+1}$ and $\\hat{x}_{k+1}$ are the inexact solutions of the linear system of equations\n\n$$\n(I + \\tilde{\\eta}_k B_k)(x - y_k) = -\\tilde{\\eta}_k g_k \\quad \\text{and} \\quad (I + \\hat{\\eta}_k B_k)(x - y_k) = -\\hat{\\eta}_k g_k,\n$$\nrespectively. Let $\\tilde{x}^*_k+1$ and $\\hat{x}^*_k+1$ be the exact solutions of the above linear systems, that is,\n\n$$\n\\tilde{x}^*_k+1 = y_k - \\tilde{\\eta}_k(I + \\tilde{\\eta}_k B_k)^{-1}g_k \\quad \\text{and} \\quad \\hat{x}^*_k+1 = y_k - \\hat{\\eta}_k(I + \\hat{\\eta}_k B_k)^{-1}g_k.\n$$\nWe first establish the following inequality between $\\left\\| \\tilde{x}^*_k+1 - y_k \\right\\|$ and $\\left\\| \\hat{x}^*_k+1 - y_k \\right\\|$:\n\n$$\n\\left\\| \\tilde{x}^*_k+1 - y_k \\right\\| \\leq \\beta_1 \\left\\| \\hat{x}^*_k+1 - y_k \\right|. \\quad (60)\n$$", "md": "Finally, by using the triangle inequality, we can combine (10) and (57) to show that\n\n$$\n\\left\\| \\hat{x}^+ - y + \\hat{\\eta} \\nabla f(\\hat{x}^+) \\right\\| = \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y)) \\right\\| \\leq \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(\\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y)) \\right\\| \\leq \\left\\| \\hat{x}^+ - y + \\hat{\\eta}(g + B(\\hat{x}^+ - y)) \\right\\| + \\left\\| \\hat{\\eta}(\\nabla f(\\hat{x}^+) - g - B(\\hat{x}^+ - y)) \\right\\| \\leq \\alpha_1 \\left\\| \\hat{x}^+ - y \\right\\| + \\alpha_2 \\left\\| \\hat{x}^+ - y \\right\\| \\leq (\\alpha_1 + \\alpha_2) \\left\\| \\hat{x}^+ - y \\right\\|,$$\nwhich means the condition in (11) is satisfied. The proof is now complete.\n\nB.2 Proof of Lemma 3.2\n\nWe follow a similar proof strategy as Lemma 3 in [JJM23]. In the first case where k \u2209 B, by definition, the line search subroutine accepts the initial step size \u03b7k, i.e., $\\hat{\\eta}_k = \\eta_k$. In the second case where k \u2208 B, the line search subroutine backtracks and returns the auxiliary iterate $\\tilde{x}_{k+1}$, which is computed from (12) using the step size $\\tilde{\\eta}_k \\triangleq \\hat{\\eta}_k/\\beta$. Since the step size $\\tilde{\\eta}_k$ is rejected in our line search subroutine, it implies that the pair ($\\tilde{x}_{k+1}, \\tilde{\\eta}_k$) does not satisfy (11), i.e.,\n\n$$\n\\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k \\nabla f(\\tilde{x}_{k+1}) \\right\\| > (\\alpha_1 + \\alpha_2) \\left\\| \\tilde{x}_{k+1} - y_k \\right|. \\quad (58)\n$$\nMoreover, since we compute $\\tilde{x}_{k+1}$ from (12) using step size $\\tilde{\\eta}_k$, the pair ($\\tilde{\\eta}_k, \\tilde{x}_{k+1}$) also satisfies the condition in (10), which means\n\n$$\n\\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k(\\nabla f(y_k) + B_k(\\tilde{x}_{k+1} - y_k)) \\right\\| \\leq \\alpha_1 \\left\\| \\tilde{x}_{k+1} - y_k \\right|. \\quad (59)\n$$\nHence, by using the triangle inequality, we can combine (58) and (59) to get\n\n$$\n\\tilde{\\eta}_k \\left\\| \\nabla f(\\tilde{x}_{k+1}) - \\nabla f(y_k) - B_k(\\tilde{x}_{k+1} - y_k) \\right\\| \\geq \\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k \\nabla f(\\tilde{x}_{k+1}) \\right\\| - \\left\\| \\tilde{x}_{k+1} - y_k + \\tilde{\\eta}_k(\\nabla f(y_k) + B_k(\\tilde{x}_{k+1} - y_k)) \\right\\| > (\\alpha_1 + \\alpha_2) \\left\\| \\tilde{x}_{k+1} - y_k \\right| - \\alpha_1 \\left\\| \\tilde{x}_{k+1} - y_k \\right| = \\alpha_2 \\left\\| \\tilde{x}_{k+1} - y_k \\right|,$$\nwhich implies that\n\n$$\n\\hat{\\eta}_k = \\beta \\tilde{\\eta}_k > \\alpha_2 \\beta \\left\\| \\tilde{x}_{k+1} - y_k \\right\\|.\n$$\nThis proves the first inequality in (13).\n\nTo show the second inequality in (13), first note that $\\tilde{x}_{k+1}$ and $\\hat{x}_{k+1}$ are the inexact solutions of the linear system of equations\n\n$$\n(I + \\tilde{\\eta}_k B_k)(x - y_k) = -\\tilde{\\eta}_k g_k \\quad \\text{and} \\quad (I + \\hat{\\eta}_k B_k)(x - y_k) = -\\hat{\\eta}_k g_k,\n$$\nrespectively. Let $\\tilde{x}^*_k+1$ and $\\hat{x}^*_k+1$ be the exact solutions of the above linear systems, that is,\n\n$$\n\\tilde{x}^*_k+1 = y_k - \\tilde{\\eta}_k(I + \\tilde{\\eta}_k B_k)^{-1}g_k \\quad \\text{and} \\quad \\hat{x}^*_k+1 = y_k - \\hat{\\eta}_k(I + \\hat{\\eta}_k B_k)^{-1}g_k.\n$$\nWe first establish the following inequality between $\\left\\| \\tilde{x}^*_k+1 - y_k \\right\\|$ and $\\left\\| \\hat{x}^*_k+1 - y_k \\right\\|$:\n\n$$\n\\left\\| \\tilde{x}^*_k+1 - y_k \\right\\| \\leq \\beta_1 \\left\\| \\hat{x}^*_k+1 - y_k \\right|. \\quad (60)\n$$"}]}, {"page": 28, "text": "This follows from\n  \u2225\u02dcx\u2217               \u03b7k(I + \u02dc                                          \u03b7k\n                            \u03b7kBk)\u22121gk\u2225    \u2264  \u02dc\n                                             \u03b7k\u2225(I + \u02c6\u03b7kBk)\u22121gk\u2225    = \u02dc   \u2225\u02c6\n     k+1 \u2212  yk\u2225  = \u2225\u02dc                                                  \u02c6   x\u2217k+1 \u2212 yk\u2225  = 1   x\u2217k+1 \u2212  yk\u2225,\nwhere we used the fact that (I + \u02dc   \u03b7kBk)\u22121 \u2aaf    (I + \u02c6               \u03b7k                  \u03b2 \u2225\u02c6\ncan show that                                          \u03b7kBk)\u22121 in the first inequality. Furthermore, we\n                      (1 \u2212 \u03b11)\u2225\u02c6xk+1 \u2212  yk\u2225  \u2264  \u2225\u02c6\n                                                 x\u2217k+1 \u2212 yk\u2225  \u2264  (1 + \u03b11)\u2225\u02c6xk+1 \u2212   yk\u2225,                 (61)\n                      (1 \u2212 \u03b11)\u2225\u02dcxk+1 \u2212  yk\u2225  \u2264  \u2225\u02dc\n                                                 x\u2217k+1 \u2212 yk\u2225  \u2264  (1 + \u03b11)\u2225\u02dcxk+1 \u2212   yk\u2225.                 (62)\nWe will only prove (61) in the following, as (62) can be proved similarly. Note that since (\u02c6       \u03b7k, \u02c6\n                                                                                                        xk+1)\nsatisfies the condition in (10), we can write\n      \u2225\u02c6xk+1 \u2212  yk + \u02c6\u03b7k(gk + Bk(\u02c6 xk+1 \u2212   yk))\u2225 = \u2225(I + \u02c6\u03b7kBk)(\u02c6 xk+1 \u2212   \u02c6\n                                                                            x\u2217             xk+1 \u2212  yk\u2225.\nMoreover, since Bk \u2ab0      0, we have \u2225\u02c6 xk+1 \u2212   \u02c6                           k+1)\u2225  \u2264 \u03b11\u2225\u02c6\n                                                 x\u2217              \u03b7kBk)(\u02c6 xk+1 \u2212   \u02c6\n                                                                                  x\u2217             xk+1 \u2212  yk\u2225.\nThus, by the triangle inequality, we obtain       k+1\u2225  \u2264  \u2225(I + \u02c6                 k+1)\u2225  \u2264  \u03b11\u2225\u02c6\n                \u2225\u02c6x\u2217               xk+1 \u2212  yk\u2225  + \u2225\u02c6x\u2217      xk+1\u2225  \u2264  (1 + \u03b11)\u2225\u02c6xk+1 \u2212   yk\u2225.\n                   k+1 \u2212  yk\u2225  \u2264  \u2225\u02c6                 k+1 \u2212  \u02c6\n                \u2225\u02c6x\u2217               xk+1 \u2212  yk\u2225  \u2212 \u2225\u02c6x\u2217      xk+1\u2225  \u2265  (1 \u2212 \u03b11)\u2225\u02c6xk+1 \u2212   yk\u2225.\n                   k+1 \u2212  yk\u2225  \u2265  \u2225\u02c6                 k+1 \u2212  \u02c6\nwhich proves (61). Finally, by combining (60), (61) and (62), we conclude that\n                           1    \u2225\u02dc                    1                        1 + \u03b11\n       \u2225\u02dc                        x\u2217                          x\u2217                           xk+1 \u2212  yk\u2225.\n        xk+1 \u2212   yk\u2225 \u2264  1 \u2212  \u03b11    k+1 \u2212 yk\u2225  \u2264  (1 \u2212  \u03b11)\u03b2 \u2225\u02c6 k+1 \u2212  yk\u2225 \u2264  (1 \u2212  \u03b11)\u03b2 \u2225\u02c6\nThis completes the proof.\n                                                      28", "md": "This follows from\n\n$$\n\\parallel \\tilde{x}^* - \\eta_k(I + \\tilde{\\eta}_k B_k)^{-1} g_k \\parallel \\leq \\tilde{\\eta}_k \\parallel (I + \\hat{\\eta}_k B_k)^{-1} g_k \\parallel = \\tilde{\\eta}_k \\parallel \\hat{x}^{k+1} - y_k \\parallel = \\parallel \\tilde{x}^*_{k+1} - y_k \\parallel = 1 \\parallel x^*_{k+1} - y_k \\parallel,\n$$\n\nwhere we used the fact that $(I + \\tilde{\\eta}_k B_k)^{-1} \\preceq (I + \\hat{\\eta}_k B_k)^{-1}$ in the first inequality. Furthermore, we can show that\n\n$$\n(1 - \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel \\leq \\parallel x^*_{k+1} - y_k \\parallel \\leq (1 + \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel, \\quad (61)\n$$\n\n$$\n(1 - \\alpha_1) \\parallel \\tilde{x}^{k+1} - y_k \\parallel \\leq \\parallel x^*_{k+1} - y_k \\parallel \\leq (1 + \\alpha_1) \\parallel \\tilde{x}^{k+1} - y_k \\parallel. \\quad (62)\n$$\n\nWe will only prove (61) in the following, as (62) can be proved similarly. Note that since $(\\hat{\\eta}_k, \\hat{x}^{k+1})$ satisfies the condition in (10), we can write\n\n$$\n\\parallel \\hat{x}^{k+1} - y_k + \\hat{\\eta}_k(g_k + B_k(\\hat{x}^{k+1} - y_k)) \\parallel = \\parallel (I + \\hat{\\eta}_k B_k)(\\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel.\n$$\n\nMoreover, since $B_k \\succeq 0$, we have\n\n$$\n\\parallel \\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel \\leq \\alpha_1 \\parallel \\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel.\n$$\n\nThus, by the triangle inequality, we obtain\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel \\leq \\parallel (I + \\hat{\\eta}_k B_k)(\\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel \\leq \\alpha_1 \\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel.\n$$\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel - \\parallel \\hat{x}^* - x^*_{k+1} \\parallel \\leq (1 + \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel.\n$$\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel + \\parallel \\hat{x}^* - x^*_{k+1} \\parallel \\geq (1 - \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel.\n$$\n\nwhich proves (61). Finally, by combining (60), (61) and (62), we conclude that\n\n$$\n1 \\parallel \\tilde{x}^* \\leq 1 - \\alpha_1 \\parallel x^* \\leq (1 - \\alpha_1) \\beta \\parallel \\hat{x} \\text{.}\n$$\n\nThis completes the proof.", "images": [], "items": [{"type": "text", "value": "This follows from\n\n$$\n\\parallel \\tilde{x}^* - \\eta_k(I + \\tilde{\\eta}_k B_k)^{-1} g_k \\parallel \\leq \\tilde{\\eta}_k \\parallel (I + \\hat{\\eta}_k B_k)^{-1} g_k \\parallel = \\tilde{\\eta}_k \\parallel \\hat{x}^{k+1} - y_k \\parallel = \\parallel \\tilde{x}^*_{k+1} - y_k \\parallel = 1 \\parallel x^*_{k+1} - y_k \\parallel,\n$$\n\nwhere we used the fact that $(I + \\tilde{\\eta}_k B_k)^{-1} \\preceq (I + \\hat{\\eta}_k B_k)^{-1}$ in the first inequality. Furthermore, we can show that\n\n$$\n(1 - \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel \\leq \\parallel x^*_{k+1} - y_k \\parallel \\leq (1 + \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel, \\quad (61)\n$$\n\n$$\n(1 - \\alpha_1) \\parallel \\tilde{x}^{k+1} - y_k \\parallel \\leq \\parallel x^*_{k+1} - y_k \\parallel \\leq (1 + \\alpha_1) \\parallel \\tilde{x}^{k+1} - y_k \\parallel. \\quad (62)\n$$\n\nWe will only prove (61) in the following, as (62) can be proved similarly. Note that since $(\\hat{\\eta}_k, \\hat{x}^{k+1})$ satisfies the condition in (10), we can write\n\n$$\n\\parallel \\hat{x}^{k+1} - y_k + \\hat{\\eta}_k(g_k + B_k(\\hat{x}^{k+1} - y_k)) \\parallel = \\parallel (I + \\hat{\\eta}_k B_k)(\\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel.\n$$\n\nMoreover, since $B_k \\succeq 0$, we have\n\n$$\n\\parallel \\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel \\leq \\alpha_1 \\parallel \\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel.\n$$\n\nThus, by the triangle inequality, we obtain\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel \\leq \\parallel (I + \\hat{\\eta}_k B_k)(\\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel \\leq \\alpha_1 \\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel.\n$$\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel - \\parallel \\hat{x}^* - x^*_{k+1} \\parallel \\leq (1 + \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel.\n$$\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel + \\parallel \\hat{x}^* - x^*_{k+1} \\parallel \\geq (1 - \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel.\n$$\n\nwhich proves (61). Finally, by combining (60), (61) and (62), we conclude that\n\n$$\n1 \\parallel \\tilde{x}^* \\leq 1 - \\alpha_1 \\parallel x^* \\leq (1 - \\alpha_1) \\beta \\parallel \\hat{x} \\text{.}\n$$\n\nThis completes the proof.", "md": "This follows from\n\n$$\n\\parallel \\tilde{x}^* - \\eta_k(I + \\tilde{\\eta}_k B_k)^{-1} g_k \\parallel \\leq \\tilde{\\eta}_k \\parallel (I + \\hat{\\eta}_k B_k)^{-1} g_k \\parallel = \\tilde{\\eta}_k \\parallel \\hat{x}^{k+1} - y_k \\parallel = \\parallel \\tilde{x}^*_{k+1} - y_k \\parallel = 1 \\parallel x^*_{k+1} - y_k \\parallel,\n$$\n\nwhere we used the fact that $(I + \\tilde{\\eta}_k B_k)^{-1} \\preceq (I + \\hat{\\eta}_k B_k)^{-1}$ in the first inequality. Furthermore, we can show that\n\n$$\n(1 - \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel \\leq \\parallel x^*_{k+1} - y_k \\parallel \\leq (1 + \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel, \\quad (61)\n$$\n\n$$\n(1 - \\alpha_1) \\parallel \\tilde{x}^{k+1} - y_k \\parallel \\leq \\parallel x^*_{k+1} - y_k \\parallel \\leq (1 + \\alpha_1) \\parallel \\tilde{x}^{k+1} - y_k \\parallel. \\quad (62)\n$$\n\nWe will only prove (61) in the following, as (62) can be proved similarly. Note that since $(\\hat{\\eta}_k, \\hat{x}^{k+1})$ satisfies the condition in (10), we can write\n\n$$\n\\parallel \\hat{x}^{k+1} - y_k + \\hat{\\eta}_k(g_k + B_k(\\hat{x}^{k+1} - y_k)) \\parallel = \\parallel (I + \\hat{\\eta}_k B_k)(\\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel.\n$$\n\nMoreover, since $B_k \\succeq 0$, we have\n\n$$\n\\parallel \\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel \\leq \\alpha_1 \\parallel \\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel.\n$$\n\nThus, by the triangle inequality, we obtain\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel \\leq \\parallel (I + \\hat{\\eta}_k B_k)(\\hat{x}^{k+1} - \\hat{x}^*_{k+1} - y_k \\parallel \\leq \\alpha_1 \\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel.\n$$\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel - \\parallel \\hat{x}^* - x^*_{k+1} \\parallel \\leq (1 + \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel.\n$$\n\n$$\n\\parallel \\hat{x}^* - x^*_{k+1} - y_k \\parallel + \\parallel \\hat{x}^* - x^*_{k+1} \\parallel \\geq (1 - \\alpha_1) \\parallel \\hat{x}^{k+1} - y_k \\parallel.\n$$\n\nwhich proves (61). Finally, by combining (60), (61) and (62), we conclude that\n\n$$\n1 \\parallel \\tilde{x}^* \\leq 1 - \\alpha_1 \\parallel x^* \\leq (1 - \\alpha_1) \\beta \\parallel \\hat{x} \\text{.}\n$$\n\nThis completes the proof."}]}, {"page": 29, "text": "C         Hessian Approximation Update\nIn this section, we first prove Lemma 3.3 in Section C.1 and remark on the computational cost of\nEuclidean projection in Section C.2. Then we present a general online learning algorithm using an\napproximate separation oracle in Section C.3 and fully describe our Hessian approximation update\nin Section C.4.\nC.1        Proof of Lemma 3.3\nWe decompose the sum  N\u22121                         1\n                                          k=0     \u02c6\n                                                 \u03b72k as\n                                       N\u22121    \u02c61   = 1      +                        1   +                       1                              (63)\n                                              \u03b72       \u02c6\n                                                       \u03b72                           \u02c6\n                                                                                    \u03b72                           \u02c6\n                                                                                                                 \u03b72\n                                       k=0      k        0     1\u2264k\u2264N\u22121,k\u2208B            k     1\u2264k\u2264N\u22121,k/      \u2208B    k\nRecall that we have \u02c6            \u03b7k = \u03b7k for k /      \u2208   B. Hence, we can further bound the last term by\n                                  \u02c61   =                        1   \u2264   N\u22121     1\n              1\u2264k\u2264N\u22121,k/     \u2208B   \u03b72k      1\u2264k\u2264N\u22121,k/     \u2208B   \u03b72k      k=1    \u03b72k\n                                                                    = 1 \u03b72   +                       \u03b72 1    +                       \u03b72 1    .\n                                                                          1      1\u2264k\u2264N\u22122,k\u2208B           k+1       1\u2264k\u2264N\u22122,k/      \u2208B    k+1\nRecall that we have \u03b7k+1 = \u02c6                 \u03b7k if k \u2208     B and \u03b7k+1 = \u02c6         \u03b7k/\u03b2 otherwise. Hence, we further have\n                                                 \u02c61   \u2264    1   +                          1    +                          1\n                             1\u2264k\u2264N\u22121,k/     \u2208B   \u03b72k      \u03b721      1\u2264k\u2264N\u22122,k\u2208B         \u03b72k+1       1\u2264k\u2264N\u22122,k/      \u2208B  \u03b72k+1\n                                                      = 1 \u03b72   +                       \u02c61   +                       \u03b22\n                                                                                       \u03b72                           \u02c6\n                                                            1      1\u2264k\u2264N\u22122,k\u2208B           k     1\u2264k\u2264N\u22122,k/      \u2208B   \u03b72k\n                                                      \u2264    1   +                        1   +                       \u03b22  .\n                                                          \u03b72                           \u02c6\n                                                                                       \u03b72                           \u02c6\n                                                            1      1\u2264k\u2264N\u22121,k\u2208B           k     1\u2264k\u2264N\u22121,k/      \u2208B   \u03b72k\nBy moving the last term to the left-hand side and dividing both sides by 1 \u2212                                               \u03b22, we obtain\n                                                          1            1      \uf8eb                                1  \uf8f6\n                                                          \u02c6   \u2264    1 \u2212   \u03b22   \uf8ed   1   +                           \uf8f8   .                         (64)\n                                                         \u03b72                      \u03b72                           \u02c6\n                                     1\u2264k\u2264N\u22121,k/      \u2208B    k1       1              1     1\u2264k\u2264N\u22121,k\u2208B          \u03b72k\nFurthermore, since \u03b71 \u2265                 \u02c6\n                                       \u03b70, we have         \u03b72       \u02c6\n                                                            1 \u2264    \u03b720 . Hence, by combining (63) and (64), we get\n              N\u22121     1   \u2264   2 \u2212   \u03b22      1   +                        1      \u2264      2 \u2212   \u03b22       + 2 \u2212     \u03b22                        1  ,  (65)\n                     \u02c6        1 \u2212   \u03b22                                                                    1 \u2212   \u03b22\n                     \u03b72                     \u02c6\n                                           \u03b72                           \u02c6\n                                                                        \u03b72          (1 \u2212   \u03b22)\u03c32                                         \u02c6\n              k=0      k                     0      1\u2264k\u2264N\u22121,k\u2208B           k                        0                 0\u2264k\u2264N\u22121,k\u2208B         \u03b72k\nwhere in the last inequality we used the fact that \u02c6                               \u03b7k = \u03c30 if 0 /       \u2208   B. Finally, (15) follows from\nLemma 3.2 and (65).\n                                                                            29", "md": "# Hessian Approximation Update\n\n## Hessian Approximation Update\n\nIn this section, we first prove Lemma 3.3 in Section C.1 and remark on the computational cost of Euclidean projection in Section C.2. Then we present a general online learning algorithm using an approximate separation oracle in Section C.3 and fully describe our Hessian approximation update in Section C.4.\n\n### Proof of Lemma 3.3\n\nWe decompose the sum $$\\sum_{k=0}^{N-1} \\hat{\\eta}_{2k}$$ as\n\n$$\\begin{align*}\n\\sum_{k=0}^{N-1} \\hat{\\eta}_{2k} &= 1 + \\sum_{1\\leq k \\leq N-1, k\\in B} \\hat{\\eta}_{2k} + \\sum_{1\\leq k \\leq N-1, k\\notin B} \\hat{\\eta}_{2k} \\quad (63)\n\\end{align*}$$\n\nRecall that we have $$\\hat{\\eta}_k = \\eta_k$$ for $$k\\notin B$$. Hence, we can further bound the last term by\n\n$$\\begin{align*}\n\\sum_{1\\leq k \\leq N-1, k\\notin B} \\hat{\\eta}_{2k} &\\leq \\sum_{1\\leq k \\leq N-1, k\\notin B} \\eta_{2k} \\\\\n&= \\eta_1^2 + \\sum_{1\\leq k \\leq N-2, k\\in B} \\eta_{2k+1} + \\sum_{1\\leq k \\leq N-2, k\\notin B} \\eta_{2k+1} \\\\\n&\\leq \\eta_1^2 + \\hat{\\eta}_1 + \\beta^2\n\\end{align*}$$\n\nBy moving the last term to the left-hand side and dividing both sides by $$1 - \\beta^2$$, we obtain\n\n$$\\hat{\\eta}_1 \\leq (1 - \\beta^2) \\left(1 + \\frac{1}{\\eta_2}\\right) \\quad (64)$$\n\nFurthermore, since $$\\eta_1 \\geq \\hat{\\eta}_0$$, we have $$\\eta_1^2 \\leq \\hat{\\eta}_0$$. Hence, by combining (63) and (64), we get\n\n$$\\begin{align*}\n\\sum_{k=0}^{N-1} \\hat{\\eta}_1 &\\leq \\eta_2 - \\beta^2 \\hat{\\eta}_1 \\leq \\eta_2 - \\beta^2 + \\frac{2 - \\beta^2}{1 - \\beta^2} \\hat{\\eta}_0 \\quad (65)\n\\end{align*}$$\n\nwhere in the last inequality we used the fact that $$\\hat{\\eta}_k = \\sigma_0$$ if $$0\\notin B$$. Finally, (15) follows from Lemma 3.2 and (65).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Hessian Approximation Update", "md": "# Hessian Approximation Update"}, {"type": "heading", "lvl": 2, "value": "Hessian Approximation Update", "md": "## Hessian Approximation Update"}, {"type": "text", "value": "In this section, we first prove Lemma 3.3 in Section C.1 and remark on the computational cost of Euclidean projection in Section C.2. Then we present a general online learning algorithm using an approximate separation oracle in Section C.3 and fully describe our Hessian approximation update in Section C.4.", "md": "In this section, we first prove Lemma 3.3 in Section C.1 and remark on the computational cost of Euclidean projection in Section C.2. Then we present a general online learning algorithm using an approximate separation oracle in Section C.3 and fully describe our Hessian approximation update in Section C.4."}, {"type": "heading", "lvl": 3, "value": "Proof of Lemma 3.3", "md": "### Proof of Lemma 3.3"}, {"type": "text", "value": "We decompose the sum $$\\sum_{k=0}^{N-1} \\hat{\\eta}_{2k}$$ as\n\n$$\\begin{align*}\n\\sum_{k=0}^{N-1} \\hat{\\eta}_{2k} &= 1 + \\sum_{1\\leq k \\leq N-1, k\\in B} \\hat{\\eta}_{2k} + \\sum_{1\\leq k \\leq N-1, k\\notin B} \\hat{\\eta}_{2k} \\quad (63)\n\\end{align*}$$\n\nRecall that we have $$\\hat{\\eta}_k = \\eta_k$$ for $$k\\notin B$$. Hence, we can further bound the last term by\n\n$$\\begin{align*}\n\\sum_{1\\leq k \\leq N-1, k\\notin B} \\hat{\\eta}_{2k} &\\leq \\sum_{1\\leq k \\leq N-1, k\\notin B} \\eta_{2k} \\\\\n&= \\eta_1^2 + \\sum_{1\\leq k \\leq N-2, k\\in B} \\eta_{2k+1} + \\sum_{1\\leq k \\leq N-2, k\\notin B} \\eta_{2k+1} \\\\\n&\\leq \\eta_1^2 + \\hat{\\eta}_1 + \\beta^2\n\\end{align*}$$\n\nBy moving the last term to the left-hand side and dividing both sides by $$1 - \\beta^2$$, we obtain\n\n$$\\hat{\\eta}_1 \\leq (1 - \\beta^2) \\left(1 + \\frac{1}{\\eta_2}\\right) \\quad (64)$$\n\nFurthermore, since $$\\eta_1 \\geq \\hat{\\eta}_0$$, we have $$\\eta_1^2 \\leq \\hat{\\eta}_0$$. Hence, by combining (63) and (64), we get\n\n$$\\begin{align*}\n\\sum_{k=0}^{N-1} \\hat{\\eta}_1 &\\leq \\eta_2 - \\beta^2 \\hat{\\eta}_1 \\leq \\eta_2 - \\beta^2 + \\frac{2 - \\beta^2}{1 - \\beta^2} \\hat{\\eta}_0 \\quad (65)\n\\end{align*}$$\n\nwhere in the last inequality we used the fact that $$\\hat{\\eta}_k = \\sigma_0$$ if $$0\\notin B$$. Finally, (15) follows from Lemma 3.2 and (65).", "md": "We decompose the sum $$\\sum_{k=0}^{N-1} \\hat{\\eta}_{2k}$$ as\n\n$$\\begin{align*}\n\\sum_{k=0}^{N-1} \\hat{\\eta}_{2k} &= 1 + \\sum_{1\\leq k \\leq N-1, k\\in B} \\hat{\\eta}_{2k} + \\sum_{1\\leq k \\leq N-1, k\\notin B} \\hat{\\eta}_{2k} \\quad (63)\n\\end{align*}$$\n\nRecall that we have $$\\hat{\\eta}_k = \\eta_k$$ for $$k\\notin B$$. Hence, we can further bound the last term by\n\n$$\\begin{align*}\n\\sum_{1\\leq k \\leq N-1, k\\notin B} \\hat{\\eta}_{2k} &\\leq \\sum_{1\\leq k \\leq N-1, k\\notin B} \\eta_{2k} \\\\\n&= \\eta_1^2 + \\sum_{1\\leq k \\leq N-2, k\\in B} \\eta_{2k+1} + \\sum_{1\\leq k \\leq N-2, k\\notin B} \\eta_{2k+1} \\\\\n&\\leq \\eta_1^2 + \\hat{\\eta}_1 + \\beta^2\n\\end{align*}$$\n\nBy moving the last term to the left-hand side and dividing both sides by $$1 - \\beta^2$$, we obtain\n\n$$\\hat{\\eta}_1 \\leq (1 - \\beta^2) \\left(1 + \\frac{1}{\\eta_2}\\right) \\quad (64)$$\n\nFurthermore, since $$\\eta_1 \\geq \\hat{\\eta}_0$$, we have $$\\eta_1^2 \\leq \\hat{\\eta}_0$$. Hence, by combining (63) and (64), we get\n\n$$\\begin{align*}\n\\sum_{k=0}^{N-1} \\hat{\\eta}_1 &\\leq \\eta_2 - \\beta^2 \\hat{\\eta}_1 \\leq \\eta_2 - \\beta^2 + \\frac{2 - \\beta^2}{1 - \\beta^2} \\hat{\\eta}_0 \\quad (65)\n\\end{align*}$$\n\nwhere in the last inequality we used the fact that $$\\hat{\\eta}_k = \\sigma_0$$ if $$0\\notin B$$. Finally, (15) follows from Lemma 3.2 and (65)."}]}, {"page": 30, "text": "C.2     The Computational Cost of Euclidean Projection\nRecall that Z \u225c    {B \u2208   Sd+ : 0 \u2aaf B \u2aaf   L1I}. As described in [JJM23, Section D.1], the Euclidean\nprojection on Z has a closed form solution.           Specifically, Given the input A \u2208         Sd, we first\nneed to perform the eigendecomposition A = V\u039bV\u22a4, where V is an orthogonal matrix and\n\u039b = diag(\u03bb1, . . . , \u03bbd) is a diagonal matrix. Then the Euclidean projection of A onto Z is given\nby V \u02c6\u039bV\u22a4, where \u02c6   \u039b is a diagonal matrix with the diagonals being \u02c6      \u03bbk = min{L1, max{0, \u03bbk}} for\n1 \u2264  k \u2264  d. Since the eigendecomposition requires O(d3) arithmetic operations in general, the cost\nof computing the Euclidean projection can be prohibitive.\nC.3     Online Learning with an Approximate Separation Oracle\nTo set the stage for our Hessian approximation matrix update, we first describe a projection-free\nonline learning algorithm in a general setup. Specifically, the online learning protocol is as follows:\nFor rounds t = 0, 1, . . . , T \u2212 1, a learner chooses an action xt \u2208     C from a convex set C and then\nobserves a loss function \u2113t : Rn \u2192    R. We measure the performance of an online learning algorithm\nby the dynamic regret [Zin03; MSJR16] defined by\n                            D-Reg                       T\u22121           T\u22121\n                                   T (u1, . . . , uT\u22121) \u225c   \u2113t(xt) \u2212      \u2113t(ut),\n                                                        t=0           t=0\nwhere {ut}T t=1 is a sequence of comparators. Moreover, we assume that the convex set C is contained\nin the Euclidean ball BR(0) for some R > 0, and we assume 0 \u2208           C without loss of generality.\nMost existing online learning algorithms are projection-based, that is, they require computing the\nEuclidean projection on the action set C. However, as we have seen in Section C.2, computing\nthe projection is computationally costly in our setting. Inspired by the work in [Mha22], we will\ndescribe an online learning algorithm that relies on an approximate separation oracle defined in\nDefinition C.1.\nDefinition C.1. The oracle SEP(w; \u03b4) takes w \u2208           BR(0) and \u03b4 > 0 as input and returns a scalar\n\u03b3 > 0 and a vector s \u2208    Rn with one of the following possible outcomes:\n    \u2022 Case I: \u03b3 \u2264   1 which implies that w \u2208    C;\n    \u2022 Case II: \u03b3 > 1 which implies that w/\u03b3 \u2208       C and \u27e8s, w \u2212     x\u27e9 \u2265  \u03b3 \u2212 1 \u2212  \u03b4   \u2200x \u2208  C.\nTo sum up, the oracle SEP(w; \u03b4) has two possible outcomes: it either certifies that w is feasible,\ni.e., w \u2208 C, or it produces a scaled version of w that is in C and gives an approximate separating\nhyperplane between w and the set C.\nThe full algorithm is shown in Algorithm 2. The key idea here is to introduce surrogate loss functions\n\u02dc\n\u2113t(w) = \u27e8\u02dc gt, w\u27e9 on the larger set BR(0) for 0 \u2264    t \u2264 T \u2212  1, where \u02dcgt is the surrogate gradient to be\ndefined later. On a high level, we will run online projected gradient descent with \u02dc    \u2113t(w) to update the\nauxiliary iterates {wt}t\u22650 (note that the projection on BR(0) is easy to compute), and then produce\nthe actions {xt}t\u22650 for the original problem by calling the SEP(wt; \u03b4) oracle in Definition C.1. The\nfollow lemma shows that the immediate regret \u02dc         \u2113t(wt) \u2212  \u02dc\n                                                                 \u2113t(x) can serve as an upper bound on\n\u2113t(xt) \u2212  \u2113t(x) for any x \u2208  C.\n                                                     30", "md": "## C.2 The Computational Cost of Euclidean Projection\n\nRecall that $$Z \\triangleq \\{B \\in S^{d+} : 0 \\preceq B \\preceq L1I\\}$$. As described in [JJM23, Section D.1], the Euclidean projection on Z has a closed form solution. Specifically, Given the input $$A \\in S^d$$, we first need to perform the eigendecomposition $$A = V\\Lambda V^T$$, where V is an orthogonal matrix and $$\\Lambda = \\text{diag}(\\lambda_1, . . . , \\lambda_d)$$ is a diagonal matrix. Then the Euclidean projection of A onto Z is given by $$V \\hat{\\Lambda} V^T$$, where $$\\hat{\\Lambda}$$ is a diagonal matrix with the diagonals being $$\\hat{\\lambda}_k = \\min\\{L1, \\max\\{0, \\lambda_k\\}\\}$$ for $$1 \\leq k \\leq d$$. Since the eigendecomposition requires $$O(d^3)$$ arithmetic operations in general, the cost of computing the Euclidean projection can be prohibitive.\n\n## C.3 Online Learning with an Approximate Separation Oracle\n\nTo set the stage for our Hessian approximation matrix update, we first describe a projection-free online learning algorithm in a general setup. Specifically, the online learning protocol is as follows: For rounds $$t = 0, 1, . . . , T - 1$$, a learner chooses an action $$x_t \\in C$$ from a convex set C and then observes a loss function $$\\ell_t : \\mathbb{R}^n \\rightarrow \\mathbb{R}$$. We measure the performance of an online learning algorithm by the dynamic regret [Zin03; MSJR16] defined by\n\n$$\n\\text{D-Reg}_T(u_1, . . . , u_{T-1}) \\triangleq \\sum_{t=0}^{T-1} \\ell_t(x_t) - \\ell_t(u_t),\n$$\nwhere $$\\{u_t\\}_{t=1}^T$$ is a sequence of comparators. Moreover, we assume that the convex set C is contained in the Euclidean ball $$BR(0)$$ for some $$R > 0$$, and we assume $$0 \\in C$$ without loss of generality. Most existing online learning algorithms are projection-based, that is, they require computing the Euclidean projection on the action set C. However, as we have seen in Section C.2, computing the projection is computationally costly in our setting. Inspired by the work in [Mha22], we will describe an online learning algorithm that relies on an approximate separation oracle defined in Definition C.1.\n\nDefinition C.1. The oracle $$\\text{SEP}(w; \\delta)$$ takes $$w \\in BR(0)$$ and $$\\delta > 0$$ as input and returns a scalar $$\\gamma > 0$$ and a vector $$s \\in \\mathbb{R}^n$$ with one of the following possible outcomes:\n\n- Case I: $\\gamma \\leq 1$ which implies that $w \\in C$;\n- Case II: $\\gamma > 1$ which implies that $w/\\gamma \\in C$ and $\\langle s, w - x \\rangle \\geq \\gamma - 1 - \\delta$ for all $x \\in C$.\n\nTo sum up, the oracle $$\\text{SEP}(w; \\delta)$$ has two possible outcomes: it either certifies that w is feasible, i.e., $$w \\in C$$, or it produces a scaled version of w that is in C and gives an approximate separating hyperplane between w and the set C.\n\nThe full algorithm is shown in Algorithm 2. The key idea here is to introduce surrogate loss functions $$\\tilde{\\ell}_t(w) = \\langle \\tilde{g}_t, w \\rangle$$ on the larger set $$BR(0)$$ for $$0 \\leq t \\leq T - 1$$, where $$\\tilde{g}_t$$ is the surrogate gradient to be defined later. On a high level, we will run online projected gradient descent with $$\\tilde{\\ell}_t(w)$$ to update the auxiliary iterates $$\\{w_t\\}_{t \\geq 0}$$ (note that the projection on $$BR(0)$$ is easy to compute), and then produce the actions $$\\{x_t\\}_{t \\geq 0}$$ for the original problem by calling the $$\\text{SEP}(w_t; \\delta)$$ oracle in Definition C.1. The following lemma shows that the immediate regret $$\\tilde{\\ell}_t(w_t) - \\tilde{\\ell}_t(x)$$ can serve as an upper bound on $$\\ell_t(x_t) - \\ell_t(x)$$ for any $$x \\in C$$.\n\n30", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "C.2 The Computational Cost of Euclidean Projection", "md": "## C.2 The Computational Cost of Euclidean Projection"}, {"type": "text", "value": "Recall that $$Z \\triangleq \\{B \\in S^{d+} : 0 \\preceq B \\preceq L1I\\}$$. As described in [JJM23, Section D.1], the Euclidean projection on Z has a closed form solution. Specifically, Given the input $$A \\in S^d$$, we first need to perform the eigendecomposition $$A = V\\Lambda V^T$$, where V is an orthogonal matrix and $$\\Lambda = \\text{diag}(\\lambda_1, . . . , \\lambda_d)$$ is a diagonal matrix. Then the Euclidean projection of A onto Z is given by $$V \\hat{\\Lambda} V^T$$, where $$\\hat{\\Lambda}$$ is a diagonal matrix with the diagonals being $$\\hat{\\lambda}_k = \\min\\{L1, \\max\\{0, \\lambda_k\\}\\}$$ for $$1 \\leq k \\leq d$$. Since the eigendecomposition requires $$O(d^3)$$ arithmetic operations in general, the cost of computing the Euclidean projection can be prohibitive.", "md": "Recall that $$Z \\triangleq \\{B \\in S^{d+} : 0 \\preceq B \\preceq L1I\\}$$. As described in [JJM23, Section D.1], the Euclidean projection on Z has a closed form solution. Specifically, Given the input $$A \\in S^d$$, we first need to perform the eigendecomposition $$A = V\\Lambda V^T$$, where V is an orthogonal matrix and $$\\Lambda = \\text{diag}(\\lambda_1, . . . , \\lambda_d)$$ is a diagonal matrix. Then the Euclidean projection of A onto Z is given by $$V \\hat{\\Lambda} V^T$$, where $$\\hat{\\Lambda}$$ is a diagonal matrix with the diagonals being $$\\hat{\\lambda}_k = \\min\\{L1, \\max\\{0, \\lambda_k\\}\\}$$ for $$1 \\leq k \\leq d$$. Since the eigendecomposition requires $$O(d^3)$$ arithmetic operations in general, the cost of computing the Euclidean projection can be prohibitive."}, {"type": "heading", "lvl": 2, "value": "C.3 Online Learning with an Approximate Separation Oracle", "md": "## C.3 Online Learning with an Approximate Separation Oracle"}, {"type": "text", "value": "To set the stage for our Hessian approximation matrix update, we first describe a projection-free online learning algorithm in a general setup. Specifically, the online learning protocol is as follows: For rounds $$t = 0, 1, . . . , T - 1$$, a learner chooses an action $$x_t \\in C$$ from a convex set C and then observes a loss function $$\\ell_t : \\mathbb{R}^n \\rightarrow \\mathbb{R}$$. We measure the performance of an online learning algorithm by the dynamic regret [Zin03; MSJR16] defined by\n\n$$\n\\text{D-Reg}_T(u_1, . . . , u_{T-1}) \\triangleq \\sum_{t=0}^{T-1} \\ell_t(x_t) - \\ell_t(u_t),\n$$\nwhere $$\\{u_t\\}_{t=1}^T$$ is a sequence of comparators. Moreover, we assume that the convex set C is contained in the Euclidean ball $$BR(0)$$ for some $$R > 0$$, and we assume $$0 \\in C$$ without loss of generality. Most existing online learning algorithms are projection-based, that is, they require computing the Euclidean projection on the action set C. However, as we have seen in Section C.2, computing the projection is computationally costly in our setting. Inspired by the work in [Mha22], we will describe an online learning algorithm that relies on an approximate separation oracle defined in Definition C.1.\n\nDefinition C.1. The oracle $$\\text{SEP}(w; \\delta)$$ takes $$w \\in BR(0)$$ and $$\\delta > 0$$ as input and returns a scalar $$\\gamma > 0$$ and a vector $$s \\in \\mathbb{R}^n$$ with one of the following possible outcomes:\n\n- Case I: $\\gamma \\leq 1$ which implies that $w \\in C$;\n- Case II: $\\gamma > 1$ which implies that $w/\\gamma \\in C$ and $\\langle s, w - x \\rangle \\geq \\gamma - 1 - \\delta$ for all $x \\in C$.\n\nTo sum up, the oracle $$\\text{SEP}(w; \\delta)$$ has two possible outcomes: it either certifies that w is feasible, i.e., $$w \\in C$$, or it produces a scaled version of w that is in C and gives an approximate separating hyperplane between w and the set C.\n\nThe full algorithm is shown in Algorithm 2. The key idea here is to introduce surrogate loss functions $$\\tilde{\\ell}_t(w) = \\langle \\tilde{g}_t, w \\rangle$$ on the larger set $$BR(0)$$ for $$0 \\leq t \\leq T - 1$$, where $$\\tilde{g}_t$$ is the surrogate gradient to be defined later. On a high level, we will run online projected gradient descent with $$\\tilde{\\ell}_t(w)$$ to update the auxiliary iterates $$\\{w_t\\}_{t \\geq 0}$$ (note that the projection on $$BR(0)$$ is easy to compute), and then produce the actions $$\\{x_t\\}_{t \\geq 0}$$ for the original problem by calling the $$\\text{SEP}(w_t; \\delta)$$ oracle in Definition C.1. The following lemma shows that the immediate regret $$\\tilde{\\ell}_t(w_t) - \\tilde{\\ell}_t(x)$$ can serve as an upper bound on $$\\ell_t(x_t) - \\ell_t(x)$$ for any $$x \\in C$$.\n\n30", "md": "To set the stage for our Hessian approximation matrix update, we first describe a projection-free online learning algorithm in a general setup. Specifically, the online learning protocol is as follows: For rounds $$t = 0, 1, . . . , T - 1$$, a learner chooses an action $$x_t \\in C$$ from a convex set C and then observes a loss function $$\\ell_t : \\mathbb{R}^n \\rightarrow \\mathbb{R}$$. We measure the performance of an online learning algorithm by the dynamic regret [Zin03; MSJR16] defined by\n\n$$\n\\text{D-Reg}_T(u_1, . . . , u_{T-1}) \\triangleq \\sum_{t=0}^{T-1} \\ell_t(x_t) - \\ell_t(u_t),\n$$\nwhere $$\\{u_t\\}_{t=1}^T$$ is a sequence of comparators. Moreover, we assume that the convex set C is contained in the Euclidean ball $$BR(0)$$ for some $$R > 0$$, and we assume $$0 \\in C$$ without loss of generality. Most existing online learning algorithms are projection-based, that is, they require computing the Euclidean projection on the action set C. However, as we have seen in Section C.2, computing the projection is computationally costly in our setting. Inspired by the work in [Mha22], we will describe an online learning algorithm that relies on an approximate separation oracle defined in Definition C.1.\n\nDefinition C.1. The oracle $$\\text{SEP}(w; \\delta)$$ takes $$w \\in BR(0)$$ and $$\\delta > 0$$ as input and returns a scalar $$\\gamma > 0$$ and a vector $$s \\in \\mathbb{R}^n$$ with one of the following possible outcomes:\n\n- Case I: $\\gamma \\leq 1$ which implies that $w \\in C$;\n- Case II: $\\gamma > 1$ which implies that $w/\\gamma \\in C$ and $\\langle s, w - x \\rangle \\geq \\gamma - 1 - \\delta$ for all $x \\in C$.\n\nTo sum up, the oracle $$\\text{SEP}(w; \\delta)$$ has two possible outcomes: it either certifies that w is feasible, i.e., $$w \\in C$$, or it produces a scaled version of w that is in C and gives an approximate separating hyperplane between w and the set C.\n\nThe full algorithm is shown in Algorithm 2. The key idea here is to introduce surrogate loss functions $$\\tilde{\\ell}_t(w) = \\langle \\tilde{g}_t, w \\rangle$$ on the larger set $$BR(0)$$ for $$0 \\leq t \\leq T - 1$$, where $$\\tilde{g}_t$$ is the surrogate gradient to be defined later. On a high level, we will run online projected gradient descent with $$\\tilde{\\ell}_t(w)$$ to update the auxiliary iterates $$\\{w_t\\}_{t \\geq 0}$$ (note that the projection on $$BR(0)$$ is easy to compute), and then produce the actions $$\\{x_t\\}_{t \\geq 0}$$ for the original problem by calling the $$\\text{SEP}(w_t; \\delta)$$ oracle in Definition C.1. The following lemma shows that the immediate regret $$\\tilde{\\ell}_t(w_t) - \\tilde{\\ell}_t(x)$$ can serve as an upper bound on $$\\ell_t(x_t) - \\ell_t(x)$$ for any $$x \\in C$$.\n\n30"}]}, {"page": 31, "text": "Algorithm 2 Projection-Free Online Learning\n  1: Input: Initial point w0 \u2208        BR(0), step size \u03c1 > 0, \u03b4 > 0\n  2: for t = 0, 1, . . . T \u2212 1 do\n  3:    Query the oracle (\u03b3t, st) \u2190       SEP(wt; \u03b4t)\n  4:    if \u03b3t \u2264   1 then      # Case I: we have wt \u2208        C\n  5:       Set xt \u2190    wt and play the action xt\n  6:       Receive the loss \u2113t(xt) and the gradient gt = \u2207\u2113t(xt)\n  7:       Set \u02dcgt \u2190   gt\n  8:    else     # Case II: we have wt/\u03b3t \u2208         C\n  9:       Set xt \u2190    wt/\u03b3t and play the action xt\n 10:       Receive the loss \u2113t(xt) and the gradient gt = \u2207\u2113t(xt)\n 11:       Set \u02dcgt \u2190   gt + max{0, \u2212\u27e8gt, xt\u27e9}st\n 12:    end if\n 13:    Update wt+1 \u2190       max{\u2225wt\u2212\u03c1\u02dcR  gt\u22252,R}(wt\u2212\u03c1\u02dc   gt)    # Euclidean projection onto BR(0)\n 14: end for\n Lemma C.1. Let {xt}T\u22121          t=0 be the iterates generated by Algorithm 2. Then we have xt \u2208                            C for\n t = 0, 1, . . . , T \u2212 1. Also, for any x \u2208        C, we have\n              \u27e8gt, xt \u2212   x\u27e9  \u2264  \u27e8\u02dc\n                                  gt, wt \u2212    x\u27e9 + max{0, \u2212\u27e8gt, xt\u27e9}\u03b4t                                                        (66)\n                              \u2264   1              2 \u2212   1                 2 + \u03c1    gt\u222522 + max{0, \u2212\u27e8gt, xt\u27e9}\u03b4t,                (67)\nand                               2\u03c1\u2225wt \u2212     x\u22252     2\u03c1\u2225wt+1 \u2212       x\u22252     2\u2225\u02dc\n                                                \u2225\u02dc\n                                                 gt\u2225  \u2264  \u2225gt\u2225   + |\u27e8gt, xt\u27e9|\u2225st\u2225.                                             (68)\n Proof. By the definition of SEP in Definition C.1, we can see that xt \u2208                         C for all t = 1, . . . , T. We\n now show that both (66) and (68) hold. We distinguish two cases depending on the outcomes of\n SEP(wt; \u03b4t).\n     \u2022 If \u03b3t \u2264    1, then we have xt = wt and \u02dc          gt = gt. In this case, (66) and (68) trivially hold.\n     \u2022 If \u03b3t > 1, then xt = wt/\u03b3t and \u02dc           gt = gt + max{0, \u2212\u27e8gt, xt\u27e9}st. We can then write\n                   \u27e8\u02dc\n                    gt, wt \u2212    x\u27e9  = \u27e8gt + max{0, \u2212\u27e8gt, xt\u27e9}st, wt \u2212            x\u27e9\n                                    = \u27e8gt, \u03b3txt \u2212    x\u27e9  + max{0, \u2212\u27e8gt, xt\u27e9}\u27e8st, wt \u2212          x\u27e9\n                                    \u2265  \u27e8gt, xt \u2212   x\u27e9 + (\u03b3t \u2212    1)\u27e8gt, xt\u27e9   + max{0, \u2212\u27e8gt, xt\u27e9}(\u03b3t \u2212         1 \u2212   \u03b4t)\n                                    = \u27e8gt, xt \u2212    x\u27e9 \u2212  max{0, \u2212\u27e8gt, xt\u27e9}\u03b4t + (\u03b3t \u2212          1) max{0, \u27e8gt, xt\u27e9}\n                                    \u2265  \u27e8gt, xt \u2212   x\u27e9 \u2212  max{0, \u2212\u27e8gt, xt\u27e9}\u03b4t,\n        which leads to (66) after rearranging. Also, by the triangle inequality we obtain\n        which proves (68).    \u2225\u02dcgt\u2225  \u2264  \u2225gt\u2225   + max{0, \u2212\u27e8gt, xt\u27e9}\u2225st\u2225         \u2264  \u2225gt\u2225   + |\u27e8gt, xt\u27e9|\u2225st\u2225,\n Finally, from the update rule of wt+1, for any x \u2208              C \u2282   BR(0) we have \u27e8wt\u2212\u03c1\u02dc         gt\u2212wt+1, wt+1\u2212x\u27e9          \u2265  0,\n                                                                31", "md": "Algorithm 2 Projection-Free Online Learning\n\n1. Input: Initial point \\(w_0 \\in BR(0)\\), step size \\(\\rho > 0\\), \\(\\delta > 0\\)\n2. for \\(t = 0, 1, ..., T - 1\\) do\n3. Query the oracle \\((\\gamma_t, s_t) \\leftarrow SEP(w_t; \\delta_t)\\)\n4. if \\(\\gamma_t \\leq 1\\) then # Case I: we have \\(w_t \\in C\\)\n5. Set \\(x_t \\leftarrow w_t\\) and play the action \\(x_t\\)\n6. Receive the loss \\(\\ell_t(x_t)\\) and the gradient \\(g_t = \\nabla \\ell_t(x_t)\\)\n7. Set \\(\\tilde{g}_t \\leftarrow g_t\\)\n8. else # Case II: we have \\(w_t/\\gamma_t \\in C\\)\n9. Set \\(x_t \\leftarrow w_t/\\gamma_t\\) and play the action \\(x_t\\)\n10. Receive the loss \\(\\ell_t(x_t)\\) and the gradient \\(g_t = \\nabla \\ell_t(x_t)\\)\n11. Set \\(\\tilde{g}_t \\leftarrow g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t\\)\n12. end if\n13. Update \\(w_{t+1} \\leftarrow \\max\\{\\|w_t - \\rho \\tilde{R} g_t\\|_2, R\\}(w_t - \\rho \\tilde{g}_t)\\) # Euclidean projection onto \\(BR(0)\\)\n\nLemma C.1. Let \\(\\{x_t\\}_{t=0}^{T-1}\\) be the iterates generated by Algorithm 2. Then we have \\(x_t \\in C\\) for \\(t = 0, 1, ..., T - 1\\). Also, for any \\(x \\in C\\), we have\n\n$$\\langle g_t, x_t - x \\rangle \\leq \\langle \\tilde{g}_t, w_t - x \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t \\quad (66)$$\n$$\\leq \\frac{1}{2} \\|g_t\\|_2^2 - \\frac{1}{2} + \\rho \\|g_t\\|_2^2 + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t \\quad (67)$$\nand\n\n$$2\\rho \\|w_t - x\\|_2^2 \\leq 2\\rho \\|w_{t+1} - x\\|_2^2 \\leq 2\\|\\tilde{g}_t\\| \\leq \\|g_t\\| + |\\langle g_t, x_t\\rangle| \\|s_t\\| \\quad (68)$$\nProof. By the definition of SEP in Definition C.1, we can see that \\(x_t \\in C\\) for all \\(t = 1, ..., T\\). We now show that both (66) and (68) hold. We distinguish two cases depending on the outcomes of SEP\\((w_t; \\delta_t)\\).\n\n- If \\(\\gamma_t \\leq 1\\), then we have \\(x_t = w_t\\) and \\(\\tilde{g}_t = g_t\\). In this case, (66) and (68) trivially hold.\n- If \\(\\gamma_t > 1\\), then \\(x_t = w_t/\\gamma_t\\) and \\(\\tilde{g}_t = g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t\\). We can then write\n\n$$\\begin{aligned}\n\\langle \\tilde{g}_t, w_t - x \\rangle & = \\langle g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t, w_t - x \\rangle \\\\\n& = \\langle g_t, \\gamma_t x_t - x \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\langle s_t, w_t - x \\rangle \\\\\n& \\geq \\langle g_t, x_t - x \\rangle + (\\gamma_t - 1) \\langle g_t, x_t \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\}(\\gamma_t - 1 - \\delta_t) \\\\\n& = \\langle g_t, x_t - x \\rangle - \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t,\n\\end{aligned}$$\nwhich leads to (66) after rearranging. Also, by the triangle inequality we obtain\n\n$$\\|\\tilde{g}_t\\| \\leq \\|g_t\\| + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\|s_t\\| \\leq \\|g_t\\| + |\\langle g_t, x_t\\rangle| \\|s_t\\|,$$\nwhich proves (68). Finally, from the update rule of \\(w_{t+1}\\), for any \\(x \\in C \\subset BR(0)\\) we have \\(\\langle w_t - \\rho \\tilde{g}_t - w_{t+1}, w_{t+1} - x \\rangle \\geq 0\\).", "images": [], "items": [{"type": "text", "value": "Algorithm 2 Projection-Free Online Learning\n\n1. Input: Initial point \\(w_0 \\in BR(0)\\), step size \\(\\rho > 0\\), \\(\\delta > 0\\)\n2. for \\(t = 0, 1, ..., T - 1\\) do\n3. Query the oracle \\((\\gamma_t, s_t) \\leftarrow SEP(w_t; \\delta_t)\\)\n4. if \\(\\gamma_t \\leq 1\\) then # Case I: we have \\(w_t \\in C\\)\n5. Set \\(x_t \\leftarrow w_t\\) and play the action \\(x_t\\)\n6. Receive the loss \\(\\ell_t(x_t)\\) and the gradient \\(g_t = \\nabla \\ell_t(x_t)\\)\n7. Set \\(\\tilde{g}_t \\leftarrow g_t\\)\n8. else # Case II: we have \\(w_t/\\gamma_t \\in C\\)\n9. Set \\(x_t \\leftarrow w_t/\\gamma_t\\) and play the action \\(x_t\\)\n10. Receive the loss \\(\\ell_t(x_t)\\) and the gradient \\(g_t = \\nabla \\ell_t(x_t)\\)\n11. Set \\(\\tilde{g}_t \\leftarrow g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t\\)\n12. end if\n13. Update \\(w_{t+1} \\leftarrow \\max\\{\\|w_t - \\rho \\tilde{R} g_t\\|_2, R\\}(w_t - \\rho \\tilde{g}_t)\\) # Euclidean projection onto \\(BR(0)\\)\n\nLemma C.1. Let \\(\\{x_t\\}_{t=0}^{T-1}\\) be the iterates generated by Algorithm 2. Then we have \\(x_t \\in C\\) for \\(t = 0, 1, ..., T - 1\\). Also, for any \\(x \\in C\\), we have\n\n$$\\langle g_t, x_t - x \\rangle \\leq \\langle \\tilde{g}_t, w_t - x \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t \\quad (66)$$\n$$\\leq \\frac{1}{2} \\|g_t\\|_2^2 - \\frac{1}{2} + \\rho \\|g_t\\|_2^2 + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t \\quad (67)$$\nand\n\n$$2\\rho \\|w_t - x\\|_2^2 \\leq 2\\rho \\|w_{t+1} - x\\|_2^2 \\leq 2\\|\\tilde{g}_t\\| \\leq \\|g_t\\| + |\\langle g_t, x_t\\rangle| \\|s_t\\| \\quad (68)$$\nProof. By the definition of SEP in Definition C.1, we can see that \\(x_t \\in C\\) for all \\(t = 1, ..., T\\). We now show that both (66) and (68) hold. We distinguish two cases depending on the outcomes of SEP\\((w_t; \\delta_t)\\).\n\n- If \\(\\gamma_t \\leq 1\\), then we have \\(x_t = w_t\\) and \\(\\tilde{g}_t = g_t\\). In this case, (66) and (68) trivially hold.\n- If \\(\\gamma_t > 1\\), then \\(x_t = w_t/\\gamma_t\\) and \\(\\tilde{g}_t = g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t\\). We can then write\n\n$$\\begin{aligned}\n\\langle \\tilde{g}_t, w_t - x \\rangle & = \\langle g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t, w_t - x \\rangle \\\\\n& = \\langle g_t, \\gamma_t x_t - x \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\langle s_t, w_t - x \\rangle \\\\\n& \\geq \\langle g_t, x_t - x \\rangle + (\\gamma_t - 1) \\langle g_t, x_t \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\}(\\gamma_t - 1 - \\delta_t) \\\\\n& = \\langle g_t, x_t - x \\rangle - \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t,\n\\end{aligned}$$\nwhich leads to (66) after rearranging. Also, by the triangle inequality we obtain\n\n$$\\|\\tilde{g}_t\\| \\leq \\|g_t\\| + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\|s_t\\| \\leq \\|g_t\\| + |\\langle g_t, x_t\\rangle| \\|s_t\\|,$$\nwhich proves (68). Finally, from the update rule of \\(w_{t+1}\\), for any \\(x \\in C \\subset BR(0)\\) we have \\(\\langle w_t - \\rho \\tilde{g}_t - w_{t+1}, w_{t+1} - x \\rangle \\geq 0\\).", "md": "Algorithm 2 Projection-Free Online Learning\n\n1. Input: Initial point \\(w_0 \\in BR(0)\\), step size \\(\\rho > 0\\), \\(\\delta > 0\\)\n2. for \\(t = 0, 1, ..., T - 1\\) do\n3. Query the oracle \\((\\gamma_t, s_t) \\leftarrow SEP(w_t; \\delta_t)\\)\n4. if \\(\\gamma_t \\leq 1\\) then # Case I: we have \\(w_t \\in C\\)\n5. Set \\(x_t \\leftarrow w_t\\) and play the action \\(x_t\\)\n6. Receive the loss \\(\\ell_t(x_t)\\) and the gradient \\(g_t = \\nabla \\ell_t(x_t)\\)\n7. Set \\(\\tilde{g}_t \\leftarrow g_t\\)\n8. else # Case II: we have \\(w_t/\\gamma_t \\in C\\)\n9. Set \\(x_t \\leftarrow w_t/\\gamma_t\\) and play the action \\(x_t\\)\n10. Receive the loss \\(\\ell_t(x_t)\\) and the gradient \\(g_t = \\nabla \\ell_t(x_t)\\)\n11. Set \\(\\tilde{g}_t \\leftarrow g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t\\)\n12. end if\n13. Update \\(w_{t+1} \\leftarrow \\max\\{\\|w_t - \\rho \\tilde{R} g_t\\|_2, R\\}(w_t - \\rho \\tilde{g}_t)\\) # Euclidean projection onto \\(BR(0)\\)\n\nLemma C.1. Let \\(\\{x_t\\}_{t=0}^{T-1}\\) be the iterates generated by Algorithm 2. Then we have \\(x_t \\in C\\) for \\(t = 0, 1, ..., T - 1\\). Also, for any \\(x \\in C\\), we have\n\n$$\\langle g_t, x_t - x \\rangle \\leq \\langle \\tilde{g}_t, w_t - x \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t \\quad (66)$$\n$$\\leq \\frac{1}{2} \\|g_t\\|_2^2 - \\frac{1}{2} + \\rho \\|g_t\\|_2^2 + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t \\quad (67)$$\nand\n\n$$2\\rho \\|w_t - x\\|_2^2 \\leq 2\\rho \\|w_{t+1} - x\\|_2^2 \\leq 2\\|\\tilde{g}_t\\| \\leq \\|g_t\\| + |\\langle g_t, x_t\\rangle| \\|s_t\\| \\quad (68)$$\nProof. By the definition of SEP in Definition C.1, we can see that \\(x_t \\in C\\) for all \\(t = 1, ..., T\\). We now show that both (66) and (68) hold. We distinguish two cases depending on the outcomes of SEP\\((w_t; \\delta_t)\\).\n\n- If \\(\\gamma_t \\leq 1\\), then we have \\(x_t = w_t\\) and \\(\\tilde{g}_t = g_t\\). In this case, (66) and (68) trivially hold.\n- If \\(\\gamma_t > 1\\), then \\(x_t = w_t/\\gamma_t\\) and \\(\\tilde{g}_t = g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t\\). We can then write\n\n$$\\begin{aligned}\n\\langle \\tilde{g}_t, w_t - x \\rangle & = \\langle g_t + \\max\\{0, -\\langle g_t, x_t \\rangle\\} s_t, w_t - x \\rangle \\\\\n& = \\langle g_t, \\gamma_t x_t - x \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\langle s_t, w_t - x \\rangle \\\\\n& \\geq \\langle g_t, x_t - x \\rangle + (\\gamma_t - 1) \\langle g_t, x_t \\rangle + \\max\\{0, -\\langle g_t, x_t \\rangle\\}(\\gamma_t - 1 - \\delta_t) \\\\\n& = \\langle g_t, x_t - x \\rangle - \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\delta_t,\n\\end{aligned}$$\nwhich leads to (66) after rearranging. Also, by the triangle inequality we obtain\n\n$$\\|\\tilde{g}_t\\| \\leq \\|g_t\\| + \\max\\{0, -\\langle g_t, x_t \\rangle\\} \\|s_t\\| \\leq \\|g_t\\| + |\\langle g_t, x_t\\rangle| \\|s_t\\|,$$\nwhich proves (68). Finally, from the update rule of \\(w_{t+1}\\), for any \\(x \\in C \\subset BR(0)\\) we have \\(\\langle w_t - \\rho \\tilde{g}_t - w_{t+1}, w_{t+1} - x \\rangle \\geq 0\\)."}]}, {"page": 32, "text": " Subroutine 2 Online Learning Guided Hessian Approximation Update\n  1: Input: Initial matrix B0 \u2208        Sd s.t. 0 \u2aaf   B0 \u2aaf    L1I, step size \u03c1 > 0, \u03b4 > 0, {qt}T \u22121 t=1\n                                  2                          2\n  2: Initialize: set W0 \u2190        L1 (B0 \u2212    L1                                 G0 \u2190    G0\n  3: for t = 1, . . . , T \u2212 1 do             2 I), G0 \u2190     L1 \u2207\u21130(B0) and \u02dc\n  4:    Query the oracle (\u03b3t, St) \u2190        SEP(Wt; \u03b4t, qt)\n  5:    if \u03b3t \u2264   1 then      # Case I       Bt + L1\n  6:       Set \u02c6Bt \u2190    Wt and Bt \u2190       L1\n                         2                 2 \u02c6       2 I\n  7:       Set Gt \u2190     L1 \u2207\u2113t(Bt) and \u02dc    Gt \u2190   Gt\n  8:    else     # Case II\n  9:       Set \u02c6Bt \u2190    Wt/\u03b3t and Bt \u2190        L1Bt + L1\n                         2                     2 \u02c6       2 I\n 10:       Set Gt \u2190     L1 \u2207\u2113t(Bt) and \u02dc    Gt \u2190   Gt + max{0, \u2212\u27e8Gt, Bt\u27e9}St\n 11:    end if                          \u221a\n 12:    Update Wt+1 \u2190              \u221a     d                    Gt)     # Euclidean projection onto B\u221a         d(0)\n 13: end for                 max{    d,\u2225Wt\u2212\u03c1 \u02dcGt\u2225F }(Wt\u2212\u03c1 \u02dc\nwhich further implies that\n           \u27e8\u02dc\n            gt, wt \u2212   x\u27e9  \u2264  \u27e8\u02dcgt, wt \u2212   wt+1\u27e9   + 1 \u03c1\u27e8wt \u2212    wt+1, wt+1 \u2212      x\u27e9                                        (69)\n                           = \u27e8\u02dc gt, wt \u2212   wt+1\u27e9   + 1                2 \u2212   1                 2 \u2212   1                   2    (70)\n                                                       2\u03c1\u2225wt \u2212     x\u22252      2\u03c1\u2225wt+1 \u2212      x\u22252      2\u03c1\u2225wt \u2212     wt+1\u22252\n                           \u2264   1              2 \u2212   1                 2 + \u03c1    gt\u222522.                                        (71)\n                               2\u03c1\u2225wt \u2212     x\u22252      2\u03c1\u2225wt+1 \u2212      x\u22252      2\u2225\u02dc\n Combining (66) and (71) leads to (67).\n C.4      Projection-free Hessian Approximation Update\n Now we are ready to describe our Hessian approximation matrix update, which is an specific\n instantiation of the general projection-free online learning algorithm shown in Algorithm 2. In\n particular, we only need to specify the convex set C as well as the SEP oracle.\n Note that we have Z = {B \u2208               Sd+ : 0 \u2aaf    B \u2aaf    L1I} in our online learning problem in Section 3.2.\n Since the projection-free scheme in Subroutine 2 requires the set C to contain the origin, we consider\n the transform \u02c6    B \u225c     2  B \u2212    L1     and let C = \u02c6   Z \u225c    {B\u02c6\u2208   Sd : \u2212I \u2aaf     \u02c6\nWe note that 0 \u2208          \u02c6L1          2 I                                        \u221a      B \u2aaf   I} = { \u02c6 B \u2208   Sd : \u2225\u02c6B\u2225op \u2264    1}.\n                         Z and \u02c6  Z \u2282    B\u221a  d(0) = {W \u2208        Sd : \u2225W\u2225F \u2264          d}. Moreover, we can see that the\n approximate separation oracle SEP(W; \u03b4, q) defined in Definition 3.2 corresponds to a stochastic\n version of the oracle in Definition C.1. The full algorithm is described in Subroutine 2 and we defer\n the specific implementation details of SEP(W; \u03b4, q) to Section E.2.\n                                                                32", "md": "Subroutine 2 Online Learning Guided Hessian Approximation Update\n\n1. Input: Initial matrix $B_0 \\in S^d$ s.t. $0 \\preceq B_0 \\preceq L_1I$, step size $\\rho > 0$, $\\delta > 0$, {qt}T\u22121t=1\n2. Initialize: set $W_0 \\leftarrow L_1(B_0 - L_1)$, $G_0 \\leftarrow G_0$\n3. for $t = 1, ..., T - 1$ do\n4. Query the oracle $(\\gamma_t, S_t) \\leftarrow SEP(W_t; \\delta_t, q_t)$\n5. if $\\gamma_t \\leq 1$ then # Case I\n6. Set $\\hat{B}_t \\leftarrow W_t$ and $B_t \\leftarrow L_1$\n7. Set $G_t \\leftarrow L_1 \\nabla \\ell_t(B_t)$ and $\\tilde{G}_t \\leftarrow \\tilde{G}_t$\n8. else # Case II\n9. Set $\\hat{B}_t \\leftarrow \\frac{W_t}{\\gamma_t}$ and $B_t \\leftarrow L_1B_t + L_1$\n10. Set $G_t \\leftarrow L_1 \\nabla \\ell_t(B_t)$ and $\\tilde{G}_t \\leftarrow \\tilde{G}_t + \\max\\{0, -\\langle G_t, B_t \\rangle\\}S_t$\n11. end if\n12. Update $W_{t+1} \\leftarrow \\max\\{\\sqrt{d}, \\|W_t - \\rho \\tilde{G}_t\\|_F\\}(W_t - \\rho \\tilde{G}_t)$ # Euclidean projection onto $B\\sqrt{d}(0)$\n13. end for\n\nwhich further implies that\n\n$$\n\\langle \\tilde{g}_t, w_t - x \\rangle \\leq \\langle \\tilde{g}_t, w_t - w_{t+1} \\rangle + \\frac{1}{\\rho} \\langle w_t - w_{t+1}, w_{t+1} - x \\rangle \\quad (69)\n$$\n$$\n= \\langle \\tilde{g}_t, w_t - w_{t+1} \\rangle + \\frac{1}{2\\rho} \\|w_t - x\\|_2^2 - \\frac{1}{2\\rho} \\|w_{t+1} - x\\|_2^2 - \\frac{1}{2\\rho} \\|w_t - w_{t+1}\\|_2^2 \\quad (70)\n$$\n$$\n\\leq \\frac{1}{2} \\|w_t - x\\|_2^2 - \\frac{1}{2} \\|w_{t+1} - x\\|_2^2 + \\rho \\|\\tilde{g}_t\\|_2^2 \\quad (71)\n$$\nCombining (66) and (71) leads to (67).\n\nC.4 Projection-free Hessian Approximation Update\n\nNow we are ready to describe our Hessian approximation matrix update, which is a specific instantiation of the general projection-free online learning algorithm shown in Algorithm 2. In particular, we only need to specify the convex set C as well as the SEP oracle.\n\nNote that we have $$Z = \\{B \\in S^{d+} : 0 \\preceq B \\preceq L_1I\\}$$ in our online learning problem in Section 3.2. Since the projection-free scheme in Subroutine 2 requires the set C to contain the origin, we consider the transform $$\\hat{B} \\triangleq 2B - L_1$$ and let $$C = \\hat{Z} \\triangleq \\{B^\\hat{\\in} S^d : -I \\preceq \\hat{B} \\preceq I\\} = \\{ \\hat{B} \\in S^d : \\|\\hat{B}\\|_{op} \\leq 1\\}$$. We note that $$0 \\in \\hat{Z}$$ and $$\\hat{Z} \\subset B\\sqrt{d}(0) = \\{W \\in S^d : \\|W\\|_F \\leq \\sqrt{d}\\}$$. Moreover, we can see that the approximate separation oracle SEP(W; \\delta, q) defined in Definition 3.2 corresponds to a stochastic version of the oracle in Definition C.1. The full algorithm is described in Subroutine 2 and we defer the specific implementation details of SEP(W; \\delta, q) to Section E.2.\n\n32", "images": [], "items": [{"type": "text", "value": "Subroutine 2 Online Learning Guided Hessian Approximation Update\n\n1. Input: Initial matrix $B_0 \\in S^d$ s.t. $0 \\preceq B_0 \\preceq L_1I$, step size $\\rho > 0$, $\\delta > 0$, {qt}T\u22121t=1\n2. Initialize: set $W_0 \\leftarrow L_1(B_0 - L_1)$, $G_0 \\leftarrow G_0$\n3. for $t = 1, ..., T - 1$ do\n4. Query the oracle $(\\gamma_t, S_t) \\leftarrow SEP(W_t; \\delta_t, q_t)$\n5. if $\\gamma_t \\leq 1$ then # Case I\n6. Set $\\hat{B}_t \\leftarrow W_t$ and $B_t \\leftarrow L_1$\n7. Set $G_t \\leftarrow L_1 \\nabla \\ell_t(B_t)$ and $\\tilde{G}_t \\leftarrow \\tilde{G}_t$\n8. else # Case II\n9. Set $\\hat{B}_t \\leftarrow \\frac{W_t}{\\gamma_t}$ and $B_t \\leftarrow L_1B_t + L_1$\n10. Set $G_t \\leftarrow L_1 \\nabla \\ell_t(B_t)$ and $\\tilde{G}_t \\leftarrow \\tilde{G}_t + \\max\\{0, -\\langle G_t, B_t \\rangle\\}S_t$\n11. end if\n12. Update $W_{t+1} \\leftarrow \\max\\{\\sqrt{d}, \\|W_t - \\rho \\tilde{G}_t\\|_F\\}(W_t - \\rho \\tilde{G}_t)$ # Euclidean projection onto $B\\sqrt{d}(0)$\n13. end for\n\nwhich further implies that\n\n$$\n\\langle \\tilde{g}_t, w_t - x \\rangle \\leq \\langle \\tilde{g}_t, w_t - w_{t+1} \\rangle + \\frac{1}{\\rho} \\langle w_t - w_{t+1}, w_{t+1} - x \\rangle \\quad (69)\n$$\n$$\n= \\langle \\tilde{g}_t, w_t - w_{t+1} \\rangle + \\frac{1}{2\\rho} \\|w_t - x\\|_2^2 - \\frac{1}{2\\rho} \\|w_{t+1} - x\\|_2^2 - \\frac{1}{2\\rho} \\|w_t - w_{t+1}\\|_2^2 \\quad (70)\n$$\n$$\n\\leq \\frac{1}{2} \\|w_t - x\\|_2^2 - \\frac{1}{2} \\|w_{t+1} - x\\|_2^2 + \\rho \\|\\tilde{g}_t\\|_2^2 \\quad (71)\n$$\nCombining (66) and (71) leads to (67).\n\nC.4 Projection-free Hessian Approximation Update\n\nNow we are ready to describe our Hessian approximation matrix update, which is a specific instantiation of the general projection-free online learning algorithm shown in Algorithm 2. In particular, we only need to specify the convex set C as well as the SEP oracle.\n\nNote that we have $$Z = \\{B \\in S^{d+} : 0 \\preceq B \\preceq L_1I\\}$$ in our online learning problem in Section 3.2. Since the projection-free scheme in Subroutine 2 requires the set C to contain the origin, we consider the transform $$\\hat{B} \\triangleq 2B - L_1$$ and let $$C = \\hat{Z} \\triangleq \\{B^\\hat{\\in} S^d : -I \\preceq \\hat{B} \\preceq I\\} = \\{ \\hat{B} \\in S^d : \\|\\hat{B}\\|_{op} \\leq 1\\}$$. We note that $$0 \\in \\hat{Z}$$ and $$\\hat{Z} \\subset B\\sqrt{d}(0) = \\{W \\in S^d : \\|W\\|_F \\leq \\sqrt{d}\\}$$. Moreover, we can see that the approximate separation oracle SEP(W; \\delta, q) defined in Definition 3.2 corresponds to a stochastic version of the oracle in Definition C.1. The full algorithm is described in Subroutine 2 and we defer the specific implementation details of SEP(W; \\delta, q) to Section E.2.\n\n32", "md": "Subroutine 2 Online Learning Guided Hessian Approximation Update\n\n1. Input: Initial matrix $B_0 \\in S^d$ s.t. $0 \\preceq B_0 \\preceq L_1I$, step size $\\rho > 0$, $\\delta > 0$, {qt}T\u22121t=1\n2. Initialize: set $W_0 \\leftarrow L_1(B_0 - L_1)$, $G_0 \\leftarrow G_0$\n3. for $t = 1, ..., T - 1$ do\n4. Query the oracle $(\\gamma_t, S_t) \\leftarrow SEP(W_t; \\delta_t, q_t)$\n5. if $\\gamma_t \\leq 1$ then # Case I\n6. Set $\\hat{B}_t \\leftarrow W_t$ and $B_t \\leftarrow L_1$\n7. Set $G_t \\leftarrow L_1 \\nabla \\ell_t(B_t)$ and $\\tilde{G}_t \\leftarrow \\tilde{G}_t$\n8. else # Case II\n9. Set $\\hat{B}_t \\leftarrow \\frac{W_t}{\\gamma_t}$ and $B_t \\leftarrow L_1B_t + L_1$\n10. Set $G_t \\leftarrow L_1 \\nabla \\ell_t(B_t)$ and $\\tilde{G}_t \\leftarrow \\tilde{G}_t + \\max\\{0, -\\langle G_t, B_t \\rangle\\}S_t$\n11. end if\n12. Update $W_{t+1} \\leftarrow \\max\\{\\sqrt{d}, \\|W_t - \\rho \\tilde{G}_t\\|_F\\}(W_t - \\rho \\tilde{G}_t)$ # Euclidean projection onto $B\\sqrt{d}(0)$\n13. end for\n\nwhich further implies that\n\n$$\n\\langle \\tilde{g}_t, w_t - x \\rangle \\leq \\langle \\tilde{g}_t, w_t - w_{t+1} \\rangle + \\frac{1}{\\rho} \\langle w_t - w_{t+1}, w_{t+1} - x \\rangle \\quad (69)\n$$\n$$\n= \\langle \\tilde{g}_t, w_t - w_{t+1} \\rangle + \\frac{1}{2\\rho} \\|w_t - x\\|_2^2 - \\frac{1}{2\\rho} \\|w_{t+1} - x\\|_2^2 - \\frac{1}{2\\rho} \\|w_t - w_{t+1}\\|_2^2 \\quad (70)\n$$\n$$\n\\leq \\frac{1}{2} \\|w_t - x\\|_2^2 - \\frac{1}{2} \\|w_{t+1} - x\\|_2^2 + \\rho \\|\\tilde{g}_t\\|_2^2 \\quad (71)\n$$\nCombining (66) and (71) leads to (67).\n\nC.4 Projection-free Hessian Approximation Update\n\nNow we are ready to describe our Hessian approximation matrix update, which is a specific instantiation of the general projection-free online learning algorithm shown in Algorithm 2. In particular, we only need to specify the convex set C as well as the SEP oracle.\n\nNote that we have $$Z = \\{B \\in S^{d+} : 0 \\preceq B \\preceq L_1I\\}$$ in our online learning problem in Section 3.2. Since the projection-free scheme in Subroutine 2 requires the set C to contain the origin, we consider the transform $$\\hat{B} \\triangleq 2B - L_1$$ and let $$C = \\hat{Z} \\triangleq \\{B^\\hat{\\in} S^d : -I \\preceq \\hat{B} \\preceq I\\} = \\{ \\hat{B} \\in S^d : \\|\\hat{B}\\|_{op} \\leq 1\\}$$. We note that $$0 \\in \\hat{Z}$$ and $$\\hat{Z} \\subset B\\sqrt{d}(0) = \\{W \\in S^d : \\|W\\|_F \\leq \\sqrt{d}\\}$$. Moreover, we can see that the approximate separation oracle SEP(W; \\delta, q) defined in Definition 3.2 corresponds to a stochastic version of the oracle in Definition C.1. The full algorithm is described in Subroutine 2 and we defer the specific implementation details of SEP(W; \\delta, q) to Section E.2.\n\n32"}]}, {"page": 33, "text": "D         Proof of Theorem 4.1\nRegarding the choices of the hyper-parameters, we consider Algorithm 1 with the line search\nscheme in Subroutine 1, where \u03b11, \u03b12 \u2208                                 (0, 1) with \u03b11 + \u03b12 < 1 and \u03b2 \u2208                           (0, 1), and with the\n                                                                                                   1\nHessian approximation update in Subroutine 2, where \u03c1 =                                           128, qt = p/2.5(t+1) log2(t+1) for t \u2265                1, and\n\u03b4t = 1/(\u221at + 2 ln(t + 2)) for t \u2265                     0. In the following, we first provide a proof sketch of Theorem 4.1.\nThe complete proofs of the lemmas shown below will be provided in the subsequent sections.\nProof Sketch. To begin with, throughout the proof, we assume that every call of the SEP oracle in\nDefinition 3.2 is successful during the execution of Algorithm 1. Indeed, by using the union bound,\nwe can bound the failure probability by  T\u22121                           t=1 qt \u2264        p    \u221e  t=2      1\n                                                                                      2.5            t log2 t \u2264   p. In particular, we note that\nSubroutine 2 ensures that 0 \u2aaf                      Bk \u2aaf      L1I for any k \u2265            0.\nWe first prove Part (a) of Theorem 4.1, which relies on the following lemma.\nLemma D.1. For k \u2208                     B, we have \u2113k(Bk) \u225c                 \u2225wk\u2212Bksk\u22252         \u2264   L2 1.\nWe combine Lemma 3.3 and Lemma D.1 to derive                                    \u2225sk\u22252\n        N\u22121     1    \u2264      2 \u2212    \u03b22       +        2 \u2212    \u03b22               \u2225wk \u2212      Bksk\u22252        \u2264       2 \u2212   \u03b22       +     (2 \u2212    \u03b22)L2   1\n                \u02c6                                                                   \u2225sk\u22252\n         k=0   \u03b72k       (1 \u2212    \u03b22)\u03c32   0      (1 \u2212    \u03b22)\u03b12   2\u03b22   k\u2208B                                 (1 \u2212    \u03b22)\u03c32   0      (1 \u2212    \u03b22)\u03b12   2\u03b22 N.\nBy further using (14) and the elementary inequality that                                        \u221a  a + b \u2264      \u221aa +      \u221a  b, we obtain\n                                       f(xN) \u2212        f(x\u2217) \u2264        C4L1\u2225z0 \u2212 N2       x\u2217\u22252    + C5\u2225z0 \u2212 \u03c3 0N2.5  x\u2217\u22252    ,\nwhere C4 = C1                   2\u2212\u03b22              (2\u2212\u03b22)                                     2\u2212\u03b22\n                             (1\u2212\u03b22)\u03c32   0 +    (1\u2212\u03b22)\u03b12   2\u03b22 and C5 = C1                 (1\u2212\u03b22)\u03c32   0 .\nNext, we divide the proof of Part (b) of Theorem 4.1 into the following steps.\nStep 1: We first use regret analysis to control the cumulative loss  T\u22121                                              t=0 \u2113t(Bt) incurred by our\nonline learning algorithm in Subroutine 2. In particular, we prove a dynamic regret bound, where\nwe compare the cumulative loss of our algorithm against the one achieved by the sequence {Ht}T\u22121                                                            t=0 .\nLemma D.2. We have\n         T\u22121    \u2113t(Bt) \u2264       256\u2225B0 \u2212         H0\u22252   F + 4    T\u22121    \u2113t(Ht) + 2L2       1 T\u22121    \u03b42t + 512L1       \u221a  d  T\u22121   \u2225Ht+1 \u2212        Ht\u2225F ,\n          t=0                                                    t=0                         t=0                           t=0\nwhere Ht \u225c           \u22072f(yt).\nStep 2: In light of Lemma D.2, it suffices to upper bound the cumulative loss  T\u22121                                                        t=0 \u2113t(Ht) and\nthe path-length  T\u22121           t=0 \u2225Ht+1 \u2212           Ht\u2225F in the following lemma. To achieve this, we use the stability\nproperties of our algorithm in (21) and Lemma A.7, which is most technical part of the proof.\nLemma D.3. We have\n  T\u22121    \u2113t(Ht) \u2264        C3     2\u2225z0 \u2212      x\u2217\u22252       and       T\u22121    \u2225Ht+1 \u2212        Ht\u2225F \u2264        C2  \u221a  dL2      1 + log AN           \u2225z0 \u2212     x\u2217\u2225, (72)\n   t=0                    4 L2                                    t=0                                                             A1\n                                                                      (1+\u03b11)2\nwhere C2 is defined in (49) and C3 =                            \u03b22(1\u2212\u03b11)2(1\u2212\u03c32).\n                                                                                33", "md": "Proof of Theorem 4.1\n\nRegarding the choices of the hyper-parameters, we consider Algorithm 1 with the line search scheme in Subroutine 1, where $$\\alpha_1, \\alpha_2 \\in (0, 1)$$ with $$\\alpha_1 + \\alpha_2 < 1$$ and $$\\beta \\in (0, 1)$$, and with the Hessian approximation update in Subroutine 2, where $$\\rho = \\frac{1}{128}$$, $$q_t = \\frac{p}{2.5(t+1) \\log_2(t+1)}$$ for $$t \\geq 1$$, and $$\\delta_t = \\frac{1}{\\sqrt{t} + 2 \\ln(t + 2)}$$ for $$t \\geq 0$$. In the following, we first provide a proof sketch of Theorem 4.1. The complete proofs of the lemmas shown below will be provided in the subsequent sections.\n\nProof Sketch. To begin with, throughout the proof, we assume that every call of the SEP oracle in Definition 3.2 is successful during the execution of Algorithm 1. Indeed, by using the union bound, we can bound the failure probability by $$\\sum_{t=1}^{T-1} q_t \\leq p \\cdot \\sum_{t=2}^{\\infty} \\frac{1}{2.5t \\log_2 t} \\leq p$$. In particular, we note that Subroutine 2 ensures that $$0 \\preceq B_k \\preceq L_1 I$$ for any $$k \\geq 0$$. We first prove Part (a) of Theorem 4.1, which relies on the following lemma.\n\nLemma D.1. For $$k \\in B$$, we have $$\\ell_k(B_k) \\triangleq \\left\\|w_k - B_k s_k\\right\\|_2 \\leq L_2^2$$.\n\nWe combine Lemma 3.3 and Lemma D.1 to derive\n\n$$\n\\begin{align*}\n\\left\\|\\mathbf{s}_k\\right\\|_2^2 & \\leq 2^{-\\beta^2} + 2^{-\\beta^2} \\left\\|w_k - B_k s_k\\right\\|_2^2 \\\\\n& \\leq 2^{-\\beta^2} + (2^{-\\beta^2} L_2^2) \\left\\|\\mathbf{s}_k\\right\\|_2 \\\\\n& \\leq 2^{-\\beta^2} + (2^{-\\beta^2} L_2^2) \\left\\|\\mathbf{s}_k\\right\\|_2\n\\end{align*}\n$$\nBy further using (14) and the elementary inequality that $$\\sqrt{a + b} \\leq \\sqrt{a} + \\sqrt{b}$$, we obtain\n\n$$\nf(x_N) - f(x^*) \\leq C_4 L_1 \\left\\|z_0 - \\frac{N}{2} x^*\\right\\|_2 + C_5 \\left\\|z_0 - \\sigma_0^{2.5} x^*\\right\\|_2\n$$\nwhere $$C_4 = C_1 2^{-\\beta^2} (2^{-\\beta^2}) (1-\\beta^2) \\sigma_0^2 + (1-\\beta^2) \\alpha_2 2\\beta^2$$ and $$C_5 = C_1 (1-\\beta^2) \\sigma_0^2$$. Next, we divide the proof of Part (b) of Theorem 4.1 into the following steps.\n\nStep 1: We first use regret analysis to control the cumulative loss $$\\sum_{t=0}^{T-1} \\ell_t(B_t)$$ incurred by our online learning algorithm in Subroutine 2. In particular, we prove a dynamic regret bound, where we compare the cumulative loss of our algorithm against the one achieved by the sequence $$\\{H_t\\}_{t=0}^{T-1}$$.\n\nLemma D.2. We have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(B_t) & \\leq 256 \\left\\|B_0 - H_0\\right\\|_F^2 + 4 \\sum_{t=0}^{T-1} \\ell_t(H_t) + 2L_2^2 \\sum_{t=0}^{T-1} \\delta^2_t + 512L_1 \\sqrt{d} \\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F,\n\\end{align*}\n$$\nwhere $$H_t \\triangleq \\nabla^2 f(y_t)$$.\n\nStep 2: In light of Lemma D.2, it suffices to upper bound the cumulative loss $$\\sum_{t=0}^{T-1} \\ell_t(H_t)$$ and the path-length $$\\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F$$ in the following lemma. To achieve this, we use the stability properties of our algorithm in (21) and Lemma A.7, which is the most technical part of the proof.\n\nLemma D.3. We have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(H_t) & \\leq C_3 2 \\left\\|z_0 - x^*\\right\\|_2 \\quad \\text{and} \\quad \\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F \\leq C_2 \\sqrt{d} L_2^2 + \\log A_N \\left\\|z_0 - x^*\\right\\|, (72)\n\\end{align*}\n$$\nwhere $$C_2$$ is defined in (49) and $$C_3 = \\beta^2 (1-\\alpha_1)^2 (1-\\sigma^2)$$. (33)", "images": [], "items": [{"type": "text", "value": "Proof of Theorem 4.1\n\nRegarding the choices of the hyper-parameters, we consider Algorithm 1 with the line search scheme in Subroutine 1, where $$\\alpha_1, \\alpha_2 \\in (0, 1)$$ with $$\\alpha_1 + \\alpha_2 < 1$$ and $$\\beta \\in (0, 1)$$, and with the Hessian approximation update in Subroutine 2, where $$\\rho = \\frac{1}{128}$$, $$q_t = \\frac{p}{2.5(t+1) \\log_2(t+1)}$$ for $$t \\geq 1$$, and $$\\delta_t = \\frac{1}{\\sqrt{t} + 2 \\ln(t + 2)}$$ for $$t \\geq 0$$. In the following, we first provide a proof sketch of Theorem 4.1. The complete proofs of the lemmas shown below will be provided in the subsequent sections.\n\nProof Sketch. To begin with, throughout the proof, we assume that every call of the SEP oracle in Definition 3.2 is successful during the execution of Algorithm 1. Indeed, by using the union bound, we can bound the failure probability by $$\\sum_{t=1}^{T-1} q_t \\leq p \\cdot \\sum_{t=2}^{\\infty} \\frac{1}{2.5t \\log_2 t} \\leq p$$. In particular, we note that Subroutine 2 ensures that $$0 \\preceq B_k \\preceq L_1 I$$ for any $$k \\geq 0$$. We first prove Part (a) of Theorem 4.1, which relies on the following lemma.\n\nLemma D.1. For $$k \\in B$$, we have $$\\ell_k(B_k) \\triangleq \\left\\|w_k - B_k s_k\\right\\|_2 \\leq L_2^2$$.\n\nWe combine Lemma 3.3 and Lemma D.1 to derive\n\n$$\n\\begin{align*}\n\\left\\|\\mathbf{s}_k\\right\\|_2^2 & \\leq 2^{-\\beta^2} + 2^{-\\beta^2} \\left\\|w_k - B_k s_k\\right\\|_2^2 \\\\\n& \\leq 2^{-\\beta^2} + (2^{-\\beta^2} L_2^2) \\left\\|\\mathbf{s}_k\\right\\|_2 \\\\\n& \\leq 2^{-\\beta^2} + (2^{-\\beta^2} L_2^2) \\left\\|\\mathbf{s}_k\\right\\|_2\n\\end{align*}\n$$\nBy further using (14) and the elementary inequality that $$\\sqrt{a + b} \\leq \\sqrt{a} + \\sqrt{b}$$, we obtain\n\n$$\nf(x_N) - f(x^*) \\leq C_4 L_1 \\left\\|z_0 - \\frac{N}{2} x^*\\right\\|_2 + C_5 \\left\\|z_0 - \\sigma_0^{2.5} x^*\\right\\|_2\n$$\nwhere $$C_4 = C_1 2^{-\\beta^2} (2^{-\\beta^2}) (1-\\beta^2) \\sigma_0^2 + (1-\\beta^2) \\alpha_2 2\\beta^2$$ and $$C_5 = C_1 (1-\\beta^2) \\sigma_0^2$$. Next, we divide the proof of Part (b) of Theorem 4.1 into the following steps.\n\nStep 1: We first use regret analysis to control the cumulative loss $$\\sum_{t=0}^{T-1} \\ell_t(B_t)$$ incurred by our online learning algorithm in Subroutine 2. In particular, we prove a dynamic regret bound, where we compare the cumulative loss of our algorithm against the one achieved by the sequence $$\\{H_t\\}_{t=0}^{T-1}$$.\n\nLemma D.2. We have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(B_t) & \\leq 256 \\left\\|B_0 - H_0\\right\\|_F^2 + 4 \\sum_{t=0}^{T-1} \\ell_t(H_t) + 2L_2^2 \\sum_{t=0}^{T-1} \\delta^2_t + 512L_1 \\sqrt{d} \\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F,\n\\end{align*}\n$$\nwhere $$H_t \\triangleq \\nabla^2 f(y_t)$$.\n\nStep 2: In light of Lemma D.2, it suffices to upper bound the cumulative loss $$\\sum_{t=0}^{T-1} \\ell_t(H_t)$$ and the path-length $$\\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F$$ in the following lemma. To achieve this, we use the stability properties of our algorithm in (21) and Lemma A.7, which is the most technical part of the proof.\n\nLemma D.3. We have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(H_t) & \\leq C_3 2 \\left\\|z_0 - x^*\\right\\|_2 \\quad \\text{and} \\quad \\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F \\leq C_2 \\sqrt{d} L_2^2 + \\log A_N \\left\\|z_0 - x^*\\right\\|, (72)\n\\end{align*}\n$$\nwhere $$C_2$$ is defined in (49) and $$C_3 = \\beta^2 (1-\\alpha_1)^2 (1-\\sigma^2)$$. (33)", "md": "Proof of Theorem 4.1\n\nRegarding the choices of the hyper-parameters, we consider Algorithm 1 with the line search scheme in Subroutine 1, where $$\\alpha_1, \\alpha_2 \\in (0, 1)$$ with $$\\alpha_1 + \\alpha_2 < 1$$ and $$\\beta \\in (0, 1)$$, and with the Hessian approximation update in Subroutine 2, where $$\\rho = \\frac{1}{128}$$, $$q_t = \\frac{p}{2.5(t+1) \\log_2(t+1)}$$ for $$t \\geq 1$$, and $$\\delta_t = \\frac{1}{\\sqrt{t} + 2 \\ln(t + 2)}$$ for $$t \\geq 0$$. In the following, we first provide a proof sketch of Theorem 4.1. The complete proofs of the lemmas shown below will be provided in the subsequent sections.\n\nProof Sketch. To begin with, throughout the proof, we assume that every call of the SEP oracle in Definition 3.2 is successful during the execution of Algorithm 1. Indeed, by using the union bound, we can bound the failure probability by $$\\sum_{t=1}^{T-1} q_t \\leq p \\cdot \\sum_{t=2}^{\\infty} \\frac{1}{2.5t \\log_2 t} \\leq p$$. In particular, we note that Subroutine 2 ensures that $$0 \\preceq B_k \\preceq L_1 I$$ for any $$k \\geq 0$$. We first prove Part (a) of Theorem 4.1, which relies on the following lemma.\n\nLemma D.1. For $$k \\in B$$, we have $$\\ell_k(B_k) \\triangleq \\left\\|w_k - B_k s_k\\right\\|_2 \\leq L_2^2$$.\n\nWe combine Lemma 3.3 and Lemma D.1 to derive\n\n$$\n\\begin{align*}\n\\left\\|\\mathbf{s}_k\\right\\|_2^2 & \\leq 2^{-\\beta^2} + 2^{-\\beta^2} \\left\\|w_k - B_k s_k\\right\\|_2^2 \\\\\n& \\leq 2^{-\\beta^2} + (2^{-\\beta^2} L_2^2) \\left\\|\\mathbf{s}_k\\right\\|_2 \\\\\n& \\leq 2^{-\\beta^2} + (2^{-\\beta^2} L_2^2) \\left\\|\\mathbf{s}_k\\right\\|_2\n\\end{align*}\n$$\nBy further using (14) and the elementary inequality that $$\\sqrt{a + b} \\leq \\sqrt{a} + \\sqrt{b}$$, we obtain\n\n$$\nf(x_N) - f(x^*) \\leq C_4 L_1 \\left\\|z_0 - \\frac{N}{2} x^*\\right\\|_2 + C_5 \\left\\|z_0 - \\sigma_0^{2.5} x^*\\right\\|_2\n$$\nwhere $$C_4 = C_1 2^{-\\beta^2} (2^{-\\beta^2}) (1-\\beta^2) \\sigma_0^2 + (1-\\beta^2) \\alpha_2 2\\beta^2$$ and $$C_5 = C_1 (1-\\beta^2) \\sigma_0^2$$. Next, we divide the proof of Part (b) of Theorem 4.1 into the following steps.\n\nStep 1: We first use regret analysis to control the cumulative loss $$\\sum_{t=0}^{T-1} \\ell_t(B_t)$$ incurred by our online learning algorithm in Subroutine 2. In particular, we prove a dynamic regret bound, where we compare the cumulative loss of our algorithm against the one achieved by the sequence $$\\{H_t\\}_{t=0}^{T-1}$$.\n\nLemma D.2. We have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(B_t) & \\leq 256 \\left\\|B_0 - H_0\\right\\|_F^2 + 4 \\sum_{t=0}^{T-1} \\ell_t(H_t) + 2L_2^2 \\sum_{t=0}^{T-1} \\delta^2_t + 512L_1 \\sqrt{d} \\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F,\n\\end{align*}\n$$\nwhere $$H_t \\triangleq \\nabla^2 f(y_t)$$.\n\nStep 2: In light of Lemma D.2, it suffices to upper bound the cumulative loss $$\\sum_{t=0}^{T-1} \\ell_t(H_t)$$ and the path-length $$\\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F$$ in the following lemma. To achieve this, we use the stability properties of our algorithm in (21) and Lemma A.7, which is the most technical part of the proof.\n\nLemma D.3. We have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(H_t) & \\leq C_3 2 \\left\\|z_0 - x^*\\right\\|_2 \\quad \\text{and} \\quad \\sum_{t=0}^{T-1} \\left\\|H_{t+1} - H_t\\right\\|_F \\leq C_2 \\sqrt{d} L_2^2 + \\log A_N \\left\\|z_0 - x^*\\right\\|, (72)\n\\end{align*}\n$$\nwhere $$C_2$$ is defined in (49) and $$C_3 = \\beta^2 (1-\\alpha_1)^2 (1-\\sigma^2)$$. (33)"}]}, {"page": 34, "text": "Step 3: Thus, we obtain an upper bound on  T\u22121                           t=0 \u2113t(Bt) by combining Lemma D.2 and Lemma D.3.\nFinally, in the following lemma, we prove an upper bound on                                       AN1by further using Lemma 3.3 and\nProposition 3.1.\nLemma D.4. We have\n                        1           1                                                          max{ L1  \u03b1 2\u03b2 , 1              1   2\n                       AN    \u2264    N2.5       M + C10L1L2d\u2225z0 \u2212                x\u2217\u2225   log+                  \u221a  M \u03c30 }N2.5             ,\nwhere we define log+(x) \u225c                  max{log(x), 0},\n                  M = C6   \u03c320  + C7L2    1 + C8\u2225B0 \u2212          H0\u22252   F + C9L2     2\u2225z0 \u2212      x\u2217\u22252 + C10L1L2d\u2225z0 \u2212                x\u2217\u2225,\nand Ci (i = 6, . . . , 10) are absolute constants given by\n                  C6 = 4C2      1(2 \u2212    \u03b22)   , C7 = 5C6                                                                                .\n                                1 \u2212   \u03b22                   \u03b122\u03b22 , C8 = 256C6   \u03b122\u03b22 , C9 = C3C6    \u03b122\u03b22 , C10 = 512C2C6    \u03b122\u03b22\nTherefore, Part (b) of Theorem 4.1 immediately follows from Proposition 3.1.\nIn the remaining of this section, we present the proofs for the above lemmas that we used to prove\nthe results in Theorem 4.1.\nD.1         Proof of Lemma D.1\nRecall that wk \u225c             \u2207f(\u02dc                                             xk+1 \u2212      yk for k \u2208        B. We can write \u2207f(\u02dc\n\u2207f(yk) = \u00af                          xk+1) \u2212      \u2207f(yk) and sk \u225c               \u02dc                                                      1        xk+1) \u2212\n                 Hk(\u02dc  xk+1\u2212yk) by using the fundamental theorem of calculus, where \u00af                                     Hk =       0 \u22072f(t\u02dc    xk+1+\n(1 \u2212    t)yk) dt. Since we have 0 \u2aaf                  \u22072f(x) \u2aaf         L1I for all x \u2208         Rd by Assumption 2.1, it implies that\n0 \u2aaf     \u00af\n       Hk \u2aaf       L1I. Moreover, since 0 \u2aaf                  Bk \u2aaf      L1I, we further have \u2212L1I \u2aaf                      \u00af\nyields \u2225     \u00af                                                                                                        Hk \u2212      Bk \u2aaf      L1I, which\n            Hk \u2212      Bk\u2225op \u2264       L1. Thus, we have\n                               \u2225wk \u2212      Bksk\u2225      = \u2225( \u00af Hk \u2212     Bk)(\u02dc  xk+1 \u2212      yk)\u2225    \u2264   L1\u2225\u02dc  xk+1 \u2212      yk\u2225,\nwhich proves that \u2113k(Bk) \u2264                   L21.\nD.2         Proof of Lemma D.2\nTo prove Lemma D.2, we first present the following lemma showing a smooth property of the loss\nfunction \u2113k. The proof is similar to [JJM23, Lemma 15].\nLemma D.5. For k \u2208                   B, we have\n                                   \u2207\u2113k(B) =             1       \u2212sk(wk \u2212         Bsk)T \u2212       (wk \u2212     Bsk)sT    k   .                             (73)\n                                                    \u2225sk\u22252\nMoreover, for any B \u2208                Sd, it holds that\n                                                \u2225\u2207\u2113k(B)\u2225F \u2264           \u2225\u2207\u2113k(B)\u2225\u2217          \u2264  2     \u2113k(B),                                             (74)\nwhere \u2225      \u00b7 \u2225F and \u2225       \u00b7 \u2225\u2217  denote the Frobenius norm and the nuclear norm, respectively.\n                                                                            34", "md": "Step 3: Thus, we obtain an upper bound on  \\(T-1\\) \\[t=0 \\ell_t(B_t)\\] by combining Lemma D.2 and Lemma D.3.\nFinally, in the following lemma, we prove an upper bound on  \\(AN_1\\) by further using Lemma 3.3 and Proposition 3.1.\n\nLemma D.4. We have\n\\[ \\frac{1}{AN} \\leq N^{2.5} M + C_{10}L_1L_2d\\|z_0 - x^*\\| \\log^+ \\left( \\sqrt{M\\sigma_0} \\right) N^{2.5}, \\]\nwhere we define \\(\\log^+(x) \\triangleq \\max\\{\\log(x), 0\\}\\),\n\\[ M = C_6\\sigma_0^2 + C_7L_2 + C_8\\|B_0 - H_0\\|_F^2 + C_9L_2\\|z_0 - x^*\\|^2 + C_{10}L_1L_2d\\|z_0 - x^*\\|, \\]\nand \\(C_i\\) (i = 6, ..., 10) are absolute constants given by\n\\[ C_6 = 4C_2\\frac{1}{2-\\beta^2}, \\quad C_7 = 5C_6\\frac{1-\\beta^2}{\\alpha^2\\beta^2}, \\]\n\\[ C_8 = 256C_6\\frac{\\alpha^2}{2\\beta^2}, \\quad C_9 = C_3C_6\\frac{\\alpha^2}{2\\beta^2}, \\quad C_{10} = 512C_2C_6\\frac{\\alpha^2}{2\\beta^2} \\]\n\nTherefore, Part (b) of Theorem 4.1 immediately follows from Proposition 3.1.\nIn the remaining of this section, we present the proofs for the above lemmas that we used to prove the results in Theorem 4.1.\n\nD.1 Proof of Lemma D.1\nRecall that \\(w_k \\triangleq \\nabla f(\\tilde{x}_{k+1}) - y_k\\) for \\(k \\in B\\). We can write \\(\\nabla f(\\tilde{x}_{k+1}) - \\nabla f(y_k) = \\bar{H}_k(\\tilde{x}_{k+1} - y_k)\\) and \\(s_k \\triangleq \\tilde{x}_{k+1} - x_k\\) by using the fundamental theorem of calculus, where \\(\\bar{H}_k = \\int_0^1 \\nabla^2 f(t\\tilde{x}_{k+1} + (1-t)y_k) dt\\). Since we have \\(0 \\preceq \\nabla^2 f(x) \\preceq L_1I\\) for all \\(x \\in \\mathbb{R}^d\\) by Assumption 2.1, it implies that \\(0 \\preceq \\bar{H}_k \\preceq L_1I\\). Moreover, since \\(0 \\preceq B_k \\preceq L_1I\\), we further have \\(-L_1I \\preceq \\bar{H}_k - B_k \\preceq L_1I\\), which yields \\(\\|\\bar{H}_k - B_k\\|_{op} \\leq L_1\\). Thus, we have\n\\[ \\|w_k - B_k s_k\\| = \\|( \\bar{H}_k - B_k)(\\tilde{x}_{k+1} - y_k)\\| \\leq L_1\\|\\tilde{x}_{k+1} - y_k\\|, \\]\nwhich proves that \\(\\ell_k(B_k) \\leq L_1^2\\).\n\nD.2 Proof of Lemma D.2\nTo prove Lemma D.2, we first present the following lemma showing a smooth property of the loss function \\(\\ell_k\\). The proof is similar to [JJM23, Lemma 15].\n\nLemma D.5. For \\(k \\in B\\), we have\n\\[ \\nabla \\ell_k(B) = \\frac{1}{\\|s_k\\|^2} - s_k(w_k - B_k s_k)^T - (w_k - B_k s_k)s_k^T. \\]\nMoreover, for any \\(B \\in S_d\\), it holds that\n\\[ \\|\\nabla \\ell_k(B)\\|_F \\leq \\|\\nabla \\ell_k(B)\\|_* \\leq 2\\ell_k(B), \\]\nwhere \\(\\| \\cdot \\|_F\\) and \\(\\| \\cdot \\|_*\\) denote the Frobenius norm and the nuclear norm, respectively.", "images": [], "items": [{"type": "text", "value": "Step 3: Thus, we obtain an upper bound on  \\(T-1\\) \\[t=0 \\ell_t(B_t)\\] by combining Lemma D.2 and Lemma D.3.\nFinally, in the following lemma, we prove an upper bound on  \\(AN_1\\) by further using Lemma 3.3 and Proposition 3.1.\n\nLemma D.4. We have\n\\[ \\frac{1}{AN} \\leq N^{2.5} M + C_{10}L_1L_2d\\|z_0 - x^*\\| \\log^+ \\left( \\sqrt{M\\sigma_0} \\right) N^{2.5}, \\]\nwhere we define \\(\\log^+(x) \\triangleq \\max\\{\\log(x), 0\\}\\),\n\\[ M = C_6\\sigma_0^2 + C_7L_2 + C_8\\|B_0 - H_0\\|_F^2 + C_9L_2\\|z_0 - x^*\\|^2 + C_{10}L_1L_2d\\|z_0 - x^*\\|, \\]\nand \\(C_i\\) (i = 6, ..., 10) are absolute constants given by\n\\[ C_6 = 4C_2\\frac{1}{2-\\beta^2}, \\quad C_7 = 5C_6\\frac{1-\\beta^2}{\\alpha^2\\beta^2}, \\]\n\\[ C_8 = 256C_6\\frac{\\alpha^2}{2\\beta^2}, \\quad C_9 = C_3C_6\\frac{\\alpha^2}{2\\beta^2}, \\quad C_{10} = 512C_2C_6\\frac{\\alpha^2}{2\\beta^2} \\]\n\nTherefore, Part (b) of Theorem 4.1 immediately follows from Proposition 3.1.\nIn the remaining of this section, we present the proofs for the above lemmas that we used to prove the results in Theorem 4.1.\n\nD.1 Proof of Lemma D.1\nRecall that \\(w_k \\triangleq \\nabla f(\\tilde{x}_{k+1}) - y_k\\) for \\(k \\in B\\). We can write \\(\\nabla f(\\tilde{x}_{k+1}) - \\nabla f(y_k) = \\bar{H}_k(\\tilde{x}_{k+1} - y_k)\\) and \\(s_k \\triangleq \\tilde{x}_{k+1} - x_k\\) by using the fundamental theorem of calculus, where \\(\\bar{H}_k = \\int_0^1 \\nabla^2 f(t\\tilde{x}_{k+1} + (1-t)y_k) dt\\). Since we have \\(0 \\preceq \\nabla^2 f(x) \\preceq L_1I\\) for all \\(x \\in \\mathbb{R}^d\\) by Assumption 2.1, it implies that \\(0 \\preceq \\bar{H}_k \\preceq L_1I\\). Moreover, since \\(0 \\preceq B_k \\preceq L_1I\\), we further have \\(-L_1I \\preceq \\bar{H}_k - B_k \\preceq L_1I\\), which yields \\(\\|\\bar{H}_k - B_k\\|_{op} \\leq L_1\\). Thus, we have\n\\[ \\|w_k - B_k s_k\\| = \\|( \\bar{H}_k - B_k)(\\tilde{x}_{k+1} - y_k)\\| \\leq L_1\\|\\tilde{x}_{k+1} - y_k\\|, \\]\nwhich proves that \\(\\ell_k(B_k) \\leq L_1^2\\).\n\nD.2 Proof of Lemma D.2\nTo prove Lemma D.2, we first present the following lemma showing a smooth property of the loss function \\(\\ell_k\\). The proof is similar to [JJM23, Lemma 15].\n\nLemma D.5. For \\(k \\in B\\), we have\n\\[ \\nabla \\ell_k(B) = \\frac{1}{\\|s_k\\|^2} - s_k(w_k - B_k s_k)^T - (w_k - B_k s_k)s_k^T. \\]\nMoreover, for any \\(B \\in S_d\\), it holds that\n\\[ \\|\\nabla \\ell_k(B)\\|_F \\leq \\|\\nabla \\ell_k(B)\\|_* \\leq 2\\ell_k(B), \\]\nwhere \\(\\| \\cdot \\|_F\\) and \\(\\| \\cdot \\|_*\\) denote the Frobenius norm and the nuclear norm, respectively.", "md": "Step 3: Thus, we obtain an upper bound on  \\(T-1\\) \\[t=0 \\ell_t(B_t)\\] by combining Lemma D.2 and Lemma D.3.\nFinally, in the following lemma, we prove an upper bound on  \\(AN_1\\) by further using Lemma 3.3 and Proposition 3.1.\n\nLemma D.4. We have\n\\[ \\frac{1}{AN} \\leq N^{2.5} M + C_{10}L_1L_2d\\|z_0 - x^*\\| \\log^+ \\left( \\sqrt{M\\sigma_0} \\right) N^{2.5}, \\]\nwhere we define \\(\\log^+(x) \\triangleq \\max\\{\\log(x), 0\\}\\),\n\\[ M = C_6\\sigma_0^2 + C_7L_2 + C_8\\|B_0 - H_0\\|_F^2 + C_9L_2\\|z_0 - x^*\\|^2 + C_{10}L_1L_2d\\|z_0 - x^*\\|, \\]\nand \\(C_i\\) (i = 6, ..., 10) are absolute constants given by\n\\[ C_6 = 4C_2\\frac{1}{2-\\beta^2}, \\quad C_7 = 5C_6\\frac{1-\\beta^2}{\\alpha^2\\beta^2}, \\]\n\\[ C_8 = 256C_6\\frac{\\alpha^2}{2\\beta^2}, \\quad C_9 = C_3C_6\\frac{\\alpha^2}{2\\beta^2}, \\quad C_{10} = 512C_2C_6\\frac{\\alpha^2}{2\\beta^2} \\]\n\nTherefore, Part (b) of Theorem 4.1 immediately follows from Proposition 3.1.\nIn the remaining of this section, we present the proofs for the above lemmas that we used to prove the results in Theorem 4.1.\n\nD.1 Proof of Lemma D.1\nRecall that \\(w_k \\triangleq \\nabla f(\\tilde{x}_{k+1}) - y_k\\) for \\(k \\in B\\). We can write \\(\\nabla f(\\tilde{x}_{k+1}) - \\nabla f(y_k) = \\bar{H}_k(\\tilde{x}_{k+1} - y_k)\\) and \\(s_k \\triangleq \\tilde{x}_{k+1} - x_k\\) by using the fundamental theorem of calculus, where \\(\\bar{H}_k = \\int_0^1 \\nabla^2 f(t\\tilde{x}_{k+1} + (1-t)y_k) dt\\). Since we have \\(0 \\preceq \\nabla^2 f(x) \\preceq L_1I\\) for all \\(x \\in \\mathbb{R}^d\\) by Assumption 2.1, it implies that \\(0 \\preceq \\bar{H}_k \\preceq L_1I\\). Moreover, since \\(0 \\preceq B_k \\preceq L_1I\\), we further have \\(-L_1I \\preceq \\bar{H}_k - B_k \\preceq L_1I\\), which yields \\(\\|\\bar{H}_k - B_k\\|_{op} \\leq L_1\\). Thus, we have\n\\[ \\|w_k - B_k s_k\\| = \\|( \\bar{H}_k - B_k)(\\tilde{x}_{k+1} - y_k)\\| \\leq L_1\\|\\tilde{x}_{k+1} - y_k\\|, \\]\nwhich proves that \\(\\ell_k(B_k) \\leq L_1^2\\).\n\nD.2 Proof of Lemma D.2\nTo prove Lemma D.2, we first present the following lemma showing a smooth property of the loss function \\(\\ell_k\\). The proof is similar to [JJM23, Lemma 15].\n\nLemma D.5. For \\(k \\in B\\), we have\n\\[ \\nabla \\ell_k(B) = \\frac{1}{\\|s_k\\|^2} - s_k(w_k - B_k s_k)^T - (w_k - B_k s_k)s_k^T. \\]\nMoreover, for any \\(B \\in S_d\\), it holds that\n\\[ \\|\\nabla \\ell_k(B)\\|_F \\leq \\|\\nabla \\ell_k(B)\\|_* \\leq 2\\ell_k(B), \\]\nwhere \\(\\| \\cdot \\|_F\\) and \\(\\| \\cdot \\|_*\\) denote the Frobenius norm and the nuclear norm, respectively."}]}, {"page": 35, "text": " Proof. It is straightforward to verify the expression in (73). The first inequality in (74) follows from\n the fact that \u2225A\u2225F \u2264                   \u2225A\u2225\u2217      for any matrix A \u2208                  Sd. For the second inequality, note that\n                             \u2225\u2207\u2113k(B)\u2225\u2217           \u2264        1        \u2225sk(wk \u2212         Bsk)T\u2225\u2217         + \u2225(wk \u2212          Bsk)sT    k \u2225\u2217\n                                                      \u2225sk\u22252\n                                                 \u2264        2                                                                = 2       \u2113k(B),\n                                                      \u2225sk\u22252 \u2225wk \u2212           Bsk\u2225\u2225sk\u2225          = 2\u2225wk \u2212    \u2225sk\u2225   Bsk\u2225\n where in the first inequality we used the triangle inequality, and in the second inequality we used\n the fact that the rank-one matrix uv\u22a4                                has only one nonzero singular value \u2225u\u2225\u2225v\u2225                                    .\n We will also need the following helper lemma.\n Lemma D.6. If the real number x satisfies x \u2264                                         A + B\u221ax, then we have x \u2264                          2A + B2.\n Proof. From the assumption, we have\n                                                                   \u221ax \u2212         B   2    \u2264   A + B2\n                                                                                2                      4 .\n Hence, we obtain                                      x \u2264       A + B2         4 + B     2   2    \u2264   2A + B2.\n Before proving Lemma D.2, we also present the following lemma that bounds the loss in each round.\n Lemma D.7. For any H \u2208                            Z, we have\n                            \u2113t(Bt) \u2264        4\u2113t(H) + 64L2                          H\u22252                                   H\u22252\n                                                                    1\u2225Wt \u2212         \u02c6    F \u2212     64L2  1\u2225Wt+1 \u2212            \u02c6    F + 2L2     1\u03b42t .\n                                                                     2                                             2\n Proof. By letting xt = \u02c6                  Bt, x = \u02c6      H \u225c       L1 (H \u2212        L1                                                 gt = \u02dc    Gt, wt = Wt in\n Lemma C.1, we obtain:                                                              2 I), gt = Gt \u225c               L 1 \u2207\u2113t(Bt), \u02dc\n(i) \u02c6  Bt \u2208     Z,\u02c6which means that \u2225                    \u02c6\n(ii) It holds that                                       Bt\u2225op \u2264        1.\n            \u27e8Gt, \u02c6  Bt \u2212      \u02c6\n                             H\u27e9     \u2264    1                H\u22252  F \u2212      1                    H\u22252  F + \u03c1        Gt\u22252   F + max{0, \u2212\u27e8Gt, \u02c6              Bt\u27e9}\u03b4t,   (75)\n                       \u2225  \u02dc             2\u03c1\u2225Wt \u2212           \u02c6            2\u03c1\u2225Wt+1 \u2212             \u02c6            2\u2225    \u02dc\n                         Gt\u2225F \u2264         \u2225Gt\u2225F + |\u27e8Gt, \u02c6           Bt\u27e9|\u2225St\u2225F .                                                                                   (76)\n First, note that \u2225St\u2225F \u2264                    3 by Definition 3.2 and |\u27e8Gt, \u02c6                   Bt\u27e9| \u2264      \u2225Gt\u2225\u2217\u2225        \u02c6\n(76), we get                              \u2225 \u02dc                                                                           Bt\u2225op \u2264        \u2225Gt\u2225\u2217. Together with\n                                            Gt\u2225F \u2264         \u2225Gt\u2225F + 3\u2225Gt\u2225\u2217                \u2264   4\u2225Gt\u2225\u2217        \u2264    16      \u2113t(Bt),                                 (77)\n                                                                                                               L1\n                                                               2\n where we used the fact that Gt =                             L1 \u2207\u2113t(Bt) and Lemma D.5 in the last inequality. Furthermore,\n since \u2113t is convex, we have\n                                  \u2113t(Bt) \u2212        \u2113t(H) \u2264        \u27e8\u2207\u2113t(Bt), Bt \u2212             H\u27e9    =    L1      2    \u27e8Gt, \u02c6  Bt \u2212      \u02c6\n                                                                                                           2                         H\u27e9,\n                                                                                     35", "md": "Proof. It is straightforward to verify the expression in (73). The first inequality in (74) follows from\nthe fact that $$\\|A\\|_F \\leq \\|A\\|_*$$ for any matrix $$A \\in Sd$$. For the second inequality, note that\n\n$$\n\\begin{aligned}\n\\|\\nabla \\ell_k(B)\\|_* &\\leq \\frac{1}{\\|s_k\\|^2} \\|s_k(w_k - B s_k)^T\\|_* + \\| (w_k - B s_k) s_k^T \\|_* \\\\\n&\\leq 2 = 2 \\ell_k(B),\n\\end{aligned}\n$$\n\nwhere in the first inequality we used the triangle inequality, and in the second inequality we used\nthe fact that the rank-one matrix $$uv^T$$ has only one nonzero singular value $$\\|u\\|\\|v\\|$$.\n\nWe will also need the following helper lemma.\n\nLemma D.6. If the real number x satisfies $$x \\leq A + B\\sqrt{x}$$, then we have $$x \\leq 2A + B^2$$.\n\nProof. From the assumption, we have\n\n$$\n\\sqrt{x} - B \\leq \\frac{A + B^2}{4}.\n$$\n\nHence, we obtain $$x \\leq \\frac{A + B^2}{4} + \\frac{B^2}{4} \\leq 2A + B^2$$.\n\nBefore proving Lemma D.2, we also present the following lemma that bounds the loss in each round.\n\nLemma D.7. For any $$H \\in Z$$, we have\n\n$$\n\\ell_t(B_t) \\leq 4\\ell_t(H) + 64L^2 \\|H\\|_2 \\left\\| \\frac{1}{\\|W_t - \\hat{F}\\|_2} - 64L^2 \\frac{1}{\\|W_{t+1} - \\hat{F}\\|_2} + 2L^2 \\frac{1}{\\delta_t^2} \\right\\}.\n$$\n\nProof. By letting $$x_t = \\hat{B}_t$$, $$x = \\hat{H} \\triangleq L1 (H - L1$$, $$g_t = \\tilde{G}_t$$, $$w_t = W_t$$ in\nLemma C.1, we obtain:\n\n(i) $$\\hat{B}_t \\in Z$$,\n(ii) It holds that $$\\|\\hat{B}_t\\|_{op} \\leq 1$$.\n\n$$\\langle G_t, \\hat{B}_t - \\hat{H} \\rangle \\leq \\frac{1}{\\|H\\|_2} - 1 \\|H\\|_2 + \\rho \\|G_t\\|_2 + \\max\\{0, -\\langle G_t, \\hat{B}_t \\rangle\\} \\delta_t$$,\n\n$$\\| \\tilde{G}_t \\|_F \\leq \\|G_t\\|_F + |\\langle G_t, \\hat{B}_t \\rangle| \\|S_t\\|_F$$.\n\nFirst, note that $$\\|S_t\\|_F \\leq 3$$ by Definition 3.2 and $$|\\langle G_t, \\hat{B}_t \\rangle| \\leq \\|G_t\\|_*\\|\\hat{B}_t\\|_{op} \\leq \\|G_t\\|_*$$. Together with\n\n$$\n\\| \\hat{B}_t \\|_{op} \\leq \\|G_t\\|_*.\n$$\n\nTogether with\n\n$$\n\\|G_t\\|_F \\leq \\|G_t\\|_F + 3\\|G_t\\|_* \\leq 4\\|G_t\\|_* \\leq 16 \\ell_t(B_t),\n$$\n\nwhere we used the fact that $$G_t = L1 \\nabla \\ell_t(B_t)$$ and Lemma D.5 in the last inequality. Furthermore,\nsince $$\\ell_t$$ is convex, we have\n\n$$\n\\ell_t(B_t) - \\ell_t(H) \\leq \\langle \\nabla \\ell_t(B_t), B_t - H \\rangle = L1 \\langle G_t, \\hat{B}_t - \\hat{H} \\rangle.\n$$", "images": [], "items": [{"type": "text", "value": "Proof. It is straightforward to verify the expression in (73). The first inequality in (74) follows from\nthe fact that $$\\|A\\|_F \\leq \\|A\\|_*$$ for any matrix $$A \\in Sd$$. For the second inequality, note that\n\n$$\n\\begin{aligned}\n\\|\\nabla \\ell_k(B)\\|_* &\\leq \\frac{1}{\\|s_k\\|^2} \\|s_k(w_k - B s_k)^T\\|_* + \\| (w_k - B s_k) s_k^T \\|_* \\\\\n&\\leq 2 = 2 \\ell_k(B),\n\\end{aligned}\n$$\n\nwhere in the first inequality we used the triangle inequality, and in the second inequality we used\nthe fact that the rank-one matrix $$uv^T$$ has only one nonzero singular value $$\\|u\\|\\|v\\|$$.\n\nWe will also need the following helper lemma.\n\nLemma D.6. If the real number x satisfies $$x \\leq A + B\\sqrt{x}$$, then we have $$x \\leq 2A + B^2$$.\n\nProof. From the assumption, we have\n\n$$\n\\sqrt{x} - B \\leq \\frac{A + B^2}{4}.\n$$\n\nHence, we obtain $$x \\leq \\frac{A + B^2}{4} + \\frac{B^2}{4} \\leq 2A + B^2$$.\n\nBefore proving Lemma D.2, we also present the following lemma that bounds the loss in each round.\n\nLemma D.7. For any $$H \\in Z$$, we have\n\n$$\n\\ell_t(B_t) \\leq 4\\ell_t(H) + 64L^2 \\|H\\|_2 \\left\\| \\frac{1}{\\|W_t - \\hat{F}\\|_2} - 64L^2 \\frac{1}{\\|W_{t+1} - \\hat{F}\\|_2} + 2L^2 \\frac{1}{\\delta_t^2} \\right\\}.\n$$\n\nProof. By letting $$x_t = \\hat{B}_t$$, $$x = \\hat{H} \\triangleq L1 (H - L1$$, $$g_t = \\tilde{G}_t$$, $$w_t = W_t$$ in\nLemma C.1, we obtain:\n\n(i) $$\\hat{B}_t \\in Z$$,\n(ii) It holds that $$\\|\\hat{B}_t\\|_{op} \\leq 1$$.\n\n$$\\langle G_t, \\hat{B}_t - \\hat{H} \\rangle \\leq \\frac{1}{\\|H\\|_2} - 1 \\|H\\|_2 + \\rho \\|G_t\\|_2 + \\max\\{0, -\\langle G_t, \\hat{B}_t \\rangle\\} \\delta_t$$,\n\n$$\\| \\tilde{G}_t \\|_F \\leq \\|G_t\\|_F + |\\langle G_t, \\hat{B}_t \\rangle| \\|S_t\\|_F$$.\n\nFirst, note that $$\\|S_t\\|_F \\leq 3$$ by Definition 3.2 and $$|\\langle G_t, \\hat{B}_t \\rangle| \\leq \\|G_t\\|_*\\|\\hat{B}_t\\|_{op} \\leq \\|G_t\\|_*$$. Together with\n\n$$\n\\| \\hat{B}_t \\|_{op} \\leq \\|G_t\\|_*.\n$$\n\nTogether with\n\n$$\n\\|G_t\\|_F \\leq \\|G_t\\|_F + 3\\|G_t\\|_* \\leq 4\\|G_t\\|_* \\leq 16 \\ell_t(B_t),\n$$\n\nwhere we used the fact that $$G_t = L1 \\nabla \\ell_t(B_t)$$ and Lemma D.5 in the last inequality. Furthermore,\nsince $$\\ell_t$$ is convex, we have\n\n$$\n\\ell_t(B_t) - \\ell_t(H) \\leq \\langle \\nabla \\ell_t(B_t), B_t - H \\rangle = L1 \\langle G_t, \\hat{B}_t - \\hat{H} \\rangle.\n$$", "md": "Proof. It is straightforward to verify the expression in (73). The first inequality in (74) follows from\nthe fact that $$\\|A\\|_F \\leq \\|A\\|_*$$ for any matrix $$A \\in Sd$$. For the second inequality, note that\n\n$$\n\\begin{aligned}\n\\|\\nabla \\ell_k(B)\\|_* &\\leq \\frac{1}{\\|s_k\\|^2} \\|s_k(w_k - B s_k)^T\\|_* + \\| (w_k - B s_k) s_k^T \\|_* \\\\\n&\\leq 2 = 2 \\ell_k(B),\n\\end{aligned}\n$$\n\nwhere in the first inequality we used the triangle inequality, and in the second inequality we used\nthe fact that the rank-one matrix $$uv^T$$ has only one nonzero singular value $$\\|u\\|\\|v\\|$$.\n\nWe will also need the following helper lemma.\n\nLemma D.6. If the real number x satisfies $$x \\leq A + B\\sqrt{x}$$, then we have $$x \\leq 2A + B^2$$.\n\nProof. From the assumption, we have\n\n$$\n\\sqrt{x} - B \\leq \\frac{A + B^2}{4}.\n$$\n\nHence, we obtain $$x \\leq \\frac{A + B^2}{4} + \\frac{B^2}{4} \\leq 2A + B^2$$.\n\nBefore proving Lemma D.2, we also present the following lemma that bounds the loss in each round.\n\nLemma D.7. For any $$H \\in Z$$, we have\n\n$$\n\\ell_t(B_t) \\leq 4\\ell_t(H) + 64L^2 \\|H\\|_2 \\left\\| \\frac{1}{\\|W_t - \\hat{F}\\|_2} - 64L^2 \\frac{1}{\\|W_{t+1} - \\hat{F}\\|_2} + 2L^2 \\frac{1}{\\delta_t^2} \\right\\}.\n$$\n\nProof. By letting $$x_t = \\hat{B}_t$$, $$x = \\hat{H} \\triangleq L1 (H - L1$$, $$g_t = \\tilde{G}_t$$, $$w_t = W_t$$ in\nLemma C.1, we obtain:\n\n(i) $$\\hat{B}_t \\in Z$$,\n(ii) It holds that $$\\|\\hat{B}_t\\|_{op} \\leq 1$$.\n\n$$\\langle G_t, \\hat{B}_t - \\hat{H} \\rangle \\leq \\frac{1}{\\|H\\|_2} - 1 \\|H\\|_2 + \\rho \\|G_t\\|_2 + \\max\\{0, -\\langle G_t, \\hat{B}_t \\rangle\\} \\delta_t$$,\n\n$$\\| \\tilde{G}_t \\|_F \\leq \\|G_t\\|_F + |\\langle G_t, \\hat{B}_t \\rangle| \\|S_t\\|_F$$.\n\nFirst, note that $$\\|S_t\\|_F \\leq 3$$ by Definition 3.2 and $$|\\langle G_t, \\hat{B}_t \\rangle| \\leq \\|G_t\\|_*\\|\\hat{B}_t\\|_{op} \\leq \\|G_t\\|_*$$. Together with\n\n$$\n\\| \\hat{B}_t \\|_{op} \\leq \\|G_t\\|_*.\n$$\n\nTogether with\n\n$$\n\\|G_t\\|_F \\leq \\|G_t\\|_F + 3\\|G_t\\|_* \\leq 4\\|G_t\\|_* \\leq 16 \\ell_t(B_t),\n$$\n\nwhere we used the fact that $$G_t = L1 \\nabla \\ell_t(B_t)$$ and Lemma D.5 in the last inequality. Furthermore,\nsince $$\\ell_t$$ is convex, we have\n\n$$\n\\ell_t(B_t) - \\ell_t(H) \\leq \\langle \\nabla \\ell_t(B_t), B_t - H \\rangle = L1 \\langle G_t, \\hat{B}_t - \\hat{H} \\rangle.\n$$"}]}, {"page": 36, "text": "                                      2                             2                                        2\n where we used Gt =                  L1 \u2207\u2113t(Bt), \u02c6       Bt \u225c      L1 (Bt     \u2212   L1               H \u225c\n                                                                                   2 I), and \u02c6              L1 (H\u2212       L1\n                                                                                                                          2 I). Therefore, by combining\n(75) and (77) we get\n                \u2113t(Bt) \u2212       \u2113t(H) \u2264         L21               H\u22252  F \u2212     L2 1                  H\u22252   F + \u03c1            Gt\u22252  F + L2     1                        (78)\n                                               8\u03c1 \u2225Wt \u2212          \u02c6            8\u03c1 \u2225Wt+1 \u2212             \u02c6            8L2  1\u2225  \u02dc               4 \u2225Gt\u2225\u2217\u03b4t\n                                          \u2264    L21               H\u22252  F \u2212     L2 1                  H\u22252   F + 32\u03c1\u2113t(Bt) + L1                   \u2113t(Bt)\u03b4t.             (79)\n                                               8\u03c1 \u2225Wt \u2212          \u02c6            8\u03c1 \u2225Wt+1 \u2212             \u02c6\n Note that \u2113t(Bt) appears on both sides of (79). By further applying Lemma D.6, we obtain\n                      \u2113t(Bt) \u2264        2\u2113t(H) + L2         1              H\u22252  F \u2212     L2 1                  H\u22252   F + 64\u03c1\u2113t(Bt) + L2              1\u03b42t .\n                                                       4\u03c1 \u2225Wt \u2212          \u02c6            4\u03c1 \u2225Wt+1 \u2212             \u02c6\n Since \u03c1 = 1/128, by rearranging and simplifying terms in the above inequality, we obtain\n                            \u2113t(Bt) \u2264        4\u2113t(H) + 64L2                          H\u22252                                   H\u22252   F + 2L2     1\u03b42t .\n                                                                    1\u2225Wt \u2212         \u02c6    F \u2212     64L2  1\u2225Wt+1 \u2212            \u02c6\n Proof of Lemma D.2. We let Ht = \u22072f(yt) for t = 0, 1, . . . , T \u2212                                                1. Thus, we get\n                         \u2113t(Bt) \u2264        4\u2113t(Ht) + 64L2                         Ht\u22252                                    Ht\u22252\n                                                                  1\u2225Wt \u2212         \u02c6     F \u2212     64L2   1\u2225Wt+1 \u2212           \u02c6     F + 2L2     1\u03b42 t\n                                     = 4\u2113t(Ht) + 64L2                           Ht\u22252                                    Ht+1\u22252\n                                          + 64L2        \u2225Wt+1 \u2212   1\u2225Wt \u2212  \u02c6      \u02c6     F \u2212     64L2   1\u2225Wt+1 \u2212           \u02c6         F + 2L2     1\u03b42 t\n                                                                         Ht+1\u22252                              Ht\u22252       .\n Furthermore, note that  \u2225Wt+1 \u2212          \u02c6          1                              F \u2212     \u2225Wt+1 \u2212          \u02c6     F\n                                          Ht+1\u22252                             Ht\u22252\n                    = (\u2225Wt+1 \u2212              \u02c6        F \u2212    \u2225Wt+1 \u2212           \u02c6     F\n                                           Ht+1\u2225F + \u2225Wt+1 \u2212                    \u02c6\n                                                                               Ht\u2225F )(\u2225Wt+1 \u2212                \u02c6\n                                                                                                            Ht+1\u2225F \u2212          \u2225Wt+1 \u2212           \u02c6\n                    \u2264    4 \u221a  d\u2225   \u02c6                             \u221a   d                                                                         Ht\u2225F )\n                                  Ht+1 \u2212         \u02c6\n                                                Ht\u2225F = 8         L1    \u2225Ht+1 \u2212         Ht\u2225F ,\n where in the last inequality we used the fact that \u02c6                                          Ht, \u02c6  Ht+1, Wt+1 \u2208               B\u221ad(0) and the triangle\n inequality. Therefore, we get\n \u2113t(Bt) \u2264        4\u2113t(Ht) + 64L2                          Ht\u22252                                    Ht+1\u22252                                      \u221a  d\u2225Ht+1 \u2212         Ht\u2225F .\n                                          1\u2225Wt \u2212          \u02c6     F \u2212    64L2   1\u2225Wt+1 \u2212           \u02c6          F + 2L2     1\u03b42t + 512L1\n By summing the above inequality from t = 0 to T \u2212                                            1, we get\n        T\u22121    \u2113t(Bt) \u2264        64L2                  H0\u22252             T\u22121    \u2113t(Ht) + 2L2           T\u22121    \u03b42                 \u221a   d T\u22121    \u2225Ht+1 \u2212        Ht\u2225F .\n         t=0                          1\u2225W0 \u2212          \u02c6     F +24      t=0                        1  t=0  2  t + 512L1               t=0\n Finally, we use the fact that W0 \u225c                           L1 (B0     \u2212  L12 I), and \u02c6    H0 \u225c        L1 (H0     \u2212   L1\n                                                                                                                         2 I) to obtain Lemma D.2.\n D.3          Proof of Lemma D.3\n By Assumption 2.2, we have \u2225wt \u2212                            Htst\u2225      = \u2225\u2207f(\u02dc       xt+1)     \u2212  \u2207f(yt)        \u2212  \u2207f(yt)(\u02dc      xt+1 \u2212      yt)\u2225     \u2264  L2    xt+1 \u2212\n yt\u22252. Thus,            \u2113t(Ht) = \u2225wt \u2212               Htst\u22252       \u2264    L22    xt+1 \u2212      yt\u22252 \u2264        (1 + \u03b11)2L2        2    xt+1 \u2212       yt\u22252,         2 \u2225\u02dc\n                                                \u2225st\u22252                   4 \u2225\u02dc         36                4\u03b22(1 \u2212        \u03b11)2 \u2225\u02c6", "md": "# Math Equations\n\nwhere we used Gt = $$\\frac{L1 \\nabla \\ell_t(Bt)}{\\hat{Bt} \\triangleq L1 (Bt - L1 \\frac{H}{2} I)},$$ and $$\\hat{H} \\triangleq L1 (H - L1 \\frac{H}{2} I)$$. Therefore, by combining (75) and (77) we get\n\n$$\n\\ell_t(Bt) - \\ell_t(H) \\leq \\frac{L2^2}{8} \\|H\\|_F^2 - L2^2 \\|H\\|_F^2 + \\rho \\|Gt\\|_F^2 + L2^2 1 \\leq \\frac{L2^2}{8} \\|H\\|_F^2 - L2^2 \\|H\\|_F^2 + 32\\rho \\ell_t(Bt) + L1 \\ell_t(Bt) \\delta_t. \\quad (79)\n$$\nNote that $$\\ell_t(Bt)$$ appears on both sides of (79). By further applying Lemma D.6, we obtain\n\n$$\n\\ell_t(Bt) \\leq 2\\ell_t(H) + L2^2 1 \\|H\\|_F^2 - L2^2 1 \\|H\\|_F^2 + 64\\rho \\ell_t(Bt) + L2^2 1 \\delta^2_t.\n$$\nSince $$\\rho = \\frac{1}{128}$$, by rearranging and simplifying terms in the above inequality, we obtain\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(H) + 64L2 \\|H\\|_F^2 \\|H\\|_F^2 + 2L2^2 1 \\delta^2_t.\n$$\nProof of Lemma D.2. We let $$Ht = \\nabla^2f(yt)$$ for $$t = 0, 1, ..., T - 1$$. Thus, we get\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht\\|_F^2 + 64L2 \\|Wt+1 - 1\\|Wt - 1\\| + 2L2^2 1 \\delta^2_t = 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht+1\\|_F^2 + 64L2 \\|Wt+1 - 1\\|Wt - 1\\| + 2L2^2 1 \\delta^2_t.\n$$\nFurthermore, note that\n\n$$\n\\|Wt+1 - 1\\|Ht+1\\|_F - \\|Wt+1 - 1\\|Ht\\|_F = 8L1 \\|Ht+1 - Ht\\|_F,\n$$\nwhere in the last inequality we used the fact that $$\\hat{Ht}, \\hat{Ht+1}, Wt+1 \\in B\\sqrt{d}(0)$$ and the triangle inequality. Therefore, we get\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht+1\\|_F^2 + 512L1 \\delta^2_t.\n$$\nBy summing the above inequality from $$t = 0$$ to $$T - 1$$, we get\n\n$$\n\\sum_{t=0}^{T-1} \\ell_t(Bt) \\leq 64L2 \\|H0\\|_F^2 + \\sum_{t=0}^{T-1} \\ell_t(Ht) + 2L2 \\sum_{t=0}^{T-1} \\delta^2 + 512L1 \\sqrt{d} \\sum_{t=0}^{T-1} \\|Ht+1 - Ht\\|_F.\n$$\nFinally, we use the fact that $$W0 \\triangleq L1 (B0 - L1^2 I)$$, and $$\\hat{H0} \\triangleq L1 (H0 - L1 \\frac{H0}{2} I)$$ to obtain Lemma D.2.\n\nProof of Lemma D.3\n\nBy Assumption 2.2, we have $$\\|wt - Htst\\| = \\|\\nabla f(\\tilde{xt+1}) - \\nabla f(yt) - \\nabla f(yt)(\\tilde{xt+1} - yt)\\| \\leq L2 \\|\\tilde{xt+1} - yt\\|_2$$. Thus, $$\\ell_t(Ht) = \\|wt - Htst\\|_2 \\leq L2^2 \\|\\tilde{xt+1} - yt\\|_2 \\leq (1 + \\alpha1)^2L2^2 \\|\\tilde{st}\\|_2 4 \\|\\tilde{36} 4\\beta2(1 - \\alpha1)^2 \\|\\hat{st}\\|_2$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "where we used Gt = $$\\frac{L1 \\nabla \\ell_t(Bt)}{\\hat{Bt} \\triangleq L1 (Bt - L1 \\frac{H}{2} I)},$$ and $$\\hat{H} \\triangleq L1 (H - L1 \\frac{H}{2} I)$$. Therefore, by combining (75) and (77) we get\n\n$$\n\\ell_t(Bt) - \\ell_t(H) \\leq \\frac{L2^2}{8} \\|H\\|_F^2 - L2^2 \\|H\\|_F^2 + \\rho \\|Gt\\|_F^2 + L2^2 1 \\leq \\frac{L2^2}{8} \\|H\\|_F^2 - L2^2 \\|H\\|_F^2 + 32\\rho \\ell_t(Bt) + L1 \\ell_t(Bt) \\delta_t. \\quad (79)\n$$\nNote that $$\\ell_t(Bt)$$ appears on both sides of (79). By further applying Lemma D.6, we obtain\n\n$$\n\\ell_t(Bt) \\leq 2\\ell_t(H) + L2^2 1 \\|H\\|_F^2 - L2^2 1 \\|H\\|_F^2 + 64\\rho \\ell_t(Bt) + L2^2 1 \\delta^2_t.\n$$\nSince $$\\rho = \\frac{1}{128}$$, by rearranging and simplifying terms in the above inequality, we obtain\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(H) + 64L2 \\|H\\|_F^2 \\|H\\|_F^2 + 2L2^2 1 \\delta^2_t.\n$$\nProof of Lemma D.2. We let $$Ht = \\nabla^2f(yt)$$ for $$t = 0, 1, ..., T - 1$$. Thus, we get\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht\\|_F^2 + 64L2 \\|Wt+1 - 1\\|Wt - 1\\| + 2L2^2 1 \\delta^2_t = 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht+1\\|_F^2 + 64L2 \\|Wt+1 - 1\\|Wt - 1\\| + 2L2^2 1 \\delta^2_t.\n$$\nFurthermore, note that\n\n$$\n\\|Wt+1 - 1\\|Ht+1\\|_F - \\|Wt+1 - 1\\|Ht\\|_F = 8L1 \\|Ht+1 - Ht\\|_F,\n$$\nwhere in the last inequality we used the fact that $$\\hat{Ht}, \\hat{Ht+1}, Wt+1 \\in B\\sqrt{d}(0)$$ and the triangle inequality. Therefore, we get\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht+1\\|_F^2 + 512L1 \\delta^2_t.\n$$\nBy summing the above inequality from $$t = 0$$ to $$T - 1$$, we get\n\n$$\n\\sum_{t=0}^{T-1} \\ell_t(Bt) \\leq 64L2 \\|H0\\|_F^2 + \\sum_{t=0}^{T-1} \\ell_t(Ht) + 2L2 \\sum_{t=0}^{T-1} \\delta^2 + 512L1 \\sqrt{d} \\sum_{t=0}^{T-1} \\|Ht+1 - Ht\\|_F.\n$$\nFinally, we use the fact that $$W0 \\triangleq L1 (B0 - L1^2 I)$$, and $$\\hat{H0} \\triangleq L1 (H0 - L1 \\frac{H0}{2} I)$$ to obtain Lemma D.2.\n\nProof of Lemma D.3\n\nBy Assumption 2.2, we have $$\\|wt - Htst\\| = \\|\\nabla f(\\tilde{xt+1}) - \\nabla f(yt) - \\nabla f(yt)(\\tilde{xt+1} - yt)\\| \\leq L2 \\|\\tilde{xt+1} - yt\\|_2$$. Thus, $$\\ell_t(Ht) = \\|wt - Htst\\|_2 \\leq L2^2 \\|\\tilde{xt+1} - yt\\|_2 \\leq (1 + \\alpha1)^2L2^2 \\|\\tilde{st}\\|_2 4 \\|\\tilde{36} 4\\beta2(1 - \\alpha1)^2 \\|\\hat{st}\\|_2$$.", "md": "where we used Gt = $$\\frac{L1 \\nabla \\ell_t(Bt)}{\\hat{Bt} \\triangleq L1 (Bt - L1 \\frac{H}{2} I)},$$ and $$\\hat{H} \\triangleq L1 (H - L1 \\frac{H}{2} I)$$. Therefore, by combining (75) and (77) we get\n\n$$\n\\ell_t(Bt) - \\ell_t(H) \\leq \\frac{L2^2}{8} \\|H\\|_F^2 - L2^2 \\|H\\|_F^2 + \\rho \\|Gt\\|_F^2 + L2^2 1 \\leq \\frac{L2^2}{8} \\|H\\|_F^2 - L2^2 \\|H\\|_F^2 + 32\\rho \\ell_t(Bt) + L1 \\ell_t(Bt) \\delta_t. \\quad (79)\n$$\nNote that $$\\ell_t(Bt)$$ appears on both sides of (79). By further applying Lemma D.6, we obtain\n\n$$\n\\ell_t(Bt) \\leq 2\\ell_t(H) + L2^2 1 \\|H\\|_F^2 - L2^2 1 \\|H\\|_F^2 + 64\\rho \\ell_t(Bt) + L2^2 1 \\delta^2_t.\n$$\nSince $$\\rho = \\frac{1}{128}$$, by rearranging and simplifying terms in the above inequality, we obtain\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(H) + 64L2 \\|H\\|_F^2 \\|H\\|_F^2 + 2L2^2 1 \\delta^2_t.\n$$\nProof of Lemma D.2. We let $$Ht = \\nabla^2f(yt)$$ for $$t = 0, 1, ..., T - 1$$. Thus, we get\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht\\|_F^2 + 64L2 \\|Wt+1 - 1\\|Wt - 1\\| + 2L2^2 1 \\delta^2_t = 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht+1\\|_F^2 + 64L2 \\|Wt+1 - 1\\|Wt - 1\\| + 2L2^2 1 \\delta^2_t.\n$$\nFurthermore, note that\n\n$$\n\\|Wt+1 - 1\\|Ht+1\\|_F - \\|Wt+1 - 1\\|Ht\\|_F = 8L1 \\|Ht+1 - Ht\\|_F,\n$$\nwhere in the last inequality we used the fact that $$\\hat{Ht}, \\hat{Ht+1}, Wt+1 \\in B\\sqrt{d}(0)$$ and the triangle inequality. Therefore, we get\n\n$$\n\\ell_t(Bt) \\leq 4\\ell_t(Ht) + 64L2 \\|Ht\\|_F^2 \\|Ht+1\\|_F^2 + 512L1 \\delta^2_t.\n$$\nBy summing the above inequality from $$t = 0$$ to $$T - 1$$, we get\n\n$$\n\\sum_{t=0}^{T-1} \\ell_t(Bt) \\leq 64L2 \\|H0\\|_F^2 + \\sum_{t=0}^{T-1} \\ell_t(Ht) + 2L2 \\sum_{t=0}^{T-1} \\delta^2 + 512L1 \\sqrt{d} \\sum_{t=0}^{T-1} \\|Ht+1 - Ht\\|_F.\n$$\nFinally, we use the fact that $$W0 \\triangleq L1 (B0 - L1^2 I)$$, and $$\\hat{H0} \\triangleq L1 (H0 - L1 \\frac{H0}{2} I)$$ to obtain Lemma D.2.\n\nProof of Lemma D.3\n\nBy Assumption 2.2, we have $$\\|wt - Htst\\| = \\|\\nabla f(\\tilde{xt+1}) - \\nabla f(yt) - \\nabla f(yt)(\\tilde{xt+1} - yt)\\| \\leq L2 \\|\\tilde{xt+1} - yt\\|_2$$. Thus, $$\\ell_t(Ht) = \\|wt - Htst\\|_2 \\leq L2^2 \\|\\tilde{xt+1} - yt\\|_2 \\leq (1 + \\alpha1)^2L2^2 \\|\\tilde{st}\\|_2 4 \\|\\tilde{36} 4\\beta2(1 - \\alpha1)^2 \\|\\hat{st}\\|_2$$."}]}, {"page": 37, "text": "where we used Lemma 3.2 in the last inequality. Also, Since ak \u2265                                        \u03b7k for all k \u2265        0, by (21) we get\n                            N\u22121    \u2225\u02c6xk+1 \u2212      yk\u22252 \u2264     N\u22121    a2k \u2225\u02c6xk+1 \u2212      yk\u22252 \u2264      1 \u2212 1 \u03c32 \u2225z0 \u2212      x\u2217\u22252.\n                                                                    \u03b72\n                             k=0                             k=0     k\nHence, we have\n                T\u22121   \u2113t(Ht) \u2264        (1 + \u03b11)2L2     2         \u2225\u02c6                                         2  N\u22121\n                                                                 xk+1 \u2212      yk\u22252 \u2264       (1 + \u03b11)2L2                \u2225\u02c6xk+1 \u2212      yk\u22252\n                t=0                  4\u03b22(1 \u2212      \u03b11)2    k\u2208B                            4\u03b22(1 \u2212       \u03b11)2    k=0\n                                                                                     \u2264   (1 + \u03b11)2L2      2\u2225z0 \u2212     x\u2217\u22252\nwhich proves the first inequality in (72).                                                 4\u03b22(1 \u2212      \u03b11)2(1 \u2212      \u03c32) ,\nFurthermore, by Assumption 2.2, we have\n    \u2225Ht+1 \u2212       Ht\u2225F =\u2225\u22072f(yt+1) \u2212                 \u22072f(yt)\u2225F \u2264         \u221a  d\u2225\u22072f(yt+1) \u2212           \u22072f(yt)\u2225op \u2264          \u221a  dL2\u2225yt+1 \u2212        yt\u2225.\nHence, by using the triangle inequality, we can bound\n              T\u22121   \u2225Ht+1 \u2212       Ht\u2225F \u2264        \u221a  dL2   N\u22121    \u2225yk+1 \u2212      yk\u2225    \u2264   \u221a  dL2C2       1 + log AN  A  1    \u2225z0 \u2212     x\u2217\u2225,\n               t=0                                        k=0\nwhere we used Lemma A.7 in the last inequality.\nD.4        Proof of Lemma D.4\nBefore presenting the proof of Lemma D.4, we start with a helper lemma that shows a lower bound\non A1.\nLemma D.8. We have A1 = \u02c6                      \u03b70 \u2265     min{\u03c30, \u03b12\u03b2   L1 }.\nProof. The equality A1 = \u02c6               \u03b70 is shown in the proof of Lemma A.2. To show the lower bound on \u02c6                                         \u03b70,\nwe use Lemma 3.2 and separate two cases. If 0 /                          \u2208  B, then we have \u02c6        \u03b70 = \u03b70 = \u03c30. Otherwise, if 0 \u2208                 B,\nthen we have\n                                             \u02c6                        \u03b12\u03b2\u2225\u02dc  x1 \u2212    y0\u2225\n                                            \u03b70 \u2265     \u2225\u2207f(\u02dc   x1) \u2212     \u2207f(y0) \u2212        Bk(\u02dc  x1 \u2212    y0)\u2225.\nMoreover, as shown in the proof of Lemma D.1, we have \u2225\u2207f(\u02dc                                           x1) \u2212     \u2207f(y0) \u2212        Bk(\u02dc  x1 \u2212    y0)\u2225    \u2264\nL1\u2225\u02dc  x1 \u2212    y0\u2225, which further implies that \u02c6                 \u03b70 \u2265    \u03b12\u03b2\n                                                                         L1 . This completes the proof.\nWe combine Lemma D.2 and Lemma D.3 to get\n                   \u2225wk \u2212      Bksk\u22252      =   T\u22121   \u2113 t(Bt) \u2264      256\u2225B0 \u2212        H0\u22252  F + C3L2     2\u2225z0 \u2212     x\u2217\u22252 + 2L2      1 T\u22121   \u03b42t\n             k\u2208B          \u2225sk\u22252                t=0                 + 512C2L1L2d              1 + log AN  A1      \u2225z0 \u2212     x\u2217\u2225.     t=0\n                                                                           37", "md": "# Math Equations\n\nwhere we used Lemma 3.2 in the last inequality. Also, Since \\(a_k \\geq \\eta_k\\) for all \\(k \\geq 0\\), by (21) we get\n\n$$\n\\begin{align*}\n\\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_k \\|_2 &\\leq \\sum_{k=0}^{N-1} a^2_k \\| \\hat{x}_{k+1} - y_k \\|_2 \\\\\n&\\leq \\frac{1 - 1}{\\sigma^2} \\| z_0 - x^* \\|_2.\n\\end{align*}\n$$\n\nHence, we have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(H_t) &\\leq (1 + \\alpha_1)^2 L^2 \\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_k \\|_2 \\\\\n&\\leq (1 + \\alpha_1)^2 L^2 \\sqrt{2} \\| z_0 - x^* \\|_2.\n\\end{align*}\n$$\n\nwhich proves the first inequality in (72). Furthermore, by Assumption 2.2, we have\n\n$$\n\\| H_{t+1} - H_t \\|_F = \\| \\nabla^2 f(y_{t+1}) - \\nabla^2 f(y_t) \\|_F \\leq \\sqrt{d} \\| \\nabla^2 f(y_{t+1}) - \\nabla^2 f(y_t) \\|_{op} \\leq \\sqrt{d} L^2 \\| y_{t+1} - y_t \\|.\n$$\n\nHence, by using the triangle inequality, we can bound\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\| H_{t+1} - H_t \\|_F &\\leq \\sqrt{d} L^2 \\sum_{k=0}^{N-1} \\| y_{k+1} - y_k \\| \\\\\n&\\leq \\sqrt{d} L^2 C^2 (1 + \\log A N) A^1 \\| z_0 - x^* \\|,\n\\end{align*}\n$$\n\nwhere we used Lemma A.7 in the last inequality.\n\nProof of Lemma D.4\n\nBefore presenting the proof of Lemma D.4, we start with a helper lemma that shows a lower bound on \\(A_1\\).\n\nLemma D.8. We have \\(A_1 = \\hat{\\eta}_0 \\geq \\min\\{\\sigma_0, \\alpha^2 \\beta L_1\\}\\).\n\nProof. The equality \\(A_1 = \\hat{\\eta}_0\\) is shown in the proof of Lemma A.2. To show the lower bound on \\(\\hat{\\eta}_0\\), we use Lemma 3.2 and separate two cases. If \\(0 \\notin B\\), then we have \\(\\hat{\\eta}_0 = \\eta_0 = \\sigma_0\\). Otherwise, if \\(0 \\in B\\), then we have\n\n$$\n\\begin{align*}\n\\hat{\\eta}_0 &\\geq \\left\\| \\nabla f(\\tilde{x}_1) - \\nabla f(y_0) - B_k(\\tilde{x}_1 - y_0) \\right\\| \\\\\n&\\geq \\alpha^2 \\beta \\| \\tilde{x}_1 - y_0 \\|_1.\n\\end{align*}\n$$\n\nMoreover, as shown in the proof of Lemma D.1, we have \\(\\left\\| \\nabla f(\\tilde{x}_1) - \\nabla f(y_0) - B_k(\\tilde{x}_1 - y_0) \\right\\| \\leq L_1 \\| \\tilde{x}_1 - y_0 \\|\\), which further implies that \\(\\hat{\\eta}_0 \\geq \\alpha^2 \\beta L_1\\). This completes the proof.\n\nWe combine Lemma D.2 and Lemma D.3 to get\n\n$$\n\\begin{align*}\n\\left\\| w_k - B_k s_k \\right\\|_2 &= \\sum_{k \\in B} \\left\\| s_k \\right\\|_2 \\\\\n&\\leq 256 \\| B_0 - H_0 \\|_F^2 + C_3 L^2 \\| z_0 - x^* \\|_2 + 2L^2 \\sum_{t=0}^{T-1} \\delta^2_t \\\\\n&\\leq 512 C^2 L_1 L_2 d (1 + \\log A N) A_1 \\| z_0 - x^* \\| + 2L^2 \\sum_{t=0}^{T-1} \\delta^2_t.\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "where we used Lemma 3.2 in the last inequality. Also, Since \\(a_k \\geq \\eta_k\\) for all \\(k \\geq 0\\), by (21) we get\n\n$$\n\\begin{align*}\n\\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_k \\|_2 &\\leq \\sum_{k=0}^{N-1} a^2_k \\| \\hat{x}_{k+1} - y_k \\|_2 \\\\\n&\\leq \\frac{1 - 1}{\\sigma^2} \\| z_0 - x^* \\|_2.\n\\end{align*}\n$$\n\nHence, we have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(H_t) &\\leq (1 + \\alpha_1)^2 L^2 \\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_k \\|_2 \\\\\n&\\leq (1 + \\alpha_1)^2 L^2 \\sqrt{2} \\| z_0 - x^* \\|_2.\n\\end{align*}\n$$\n\nwhich proves the first inequality in (72). Furthermore, by Assumption 2.2, we have\n\n$$\n\\| H_{t+1} - H_t \\|_F = \\| \\nabla^2 f(y_{t+1}) - \\nabla^2 f(y_t) \\|_F \\leq \\sqrt{d} \\| \\nabla^2 f(y_{t+1}) - \\nabla^2 f(y_t) \\|_{op} \\leq \\sqrt{d} L^2 \\| y_{t+1} - y_t \\|.\n$$\n\nHence, by using the triangle inequality, we can bound\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\| H_{t+1} - H_t \\|_F &\\leq \\sqrt{d} L^2 \\sum_{k=0}^{N-1} \\| y_{k+1} - y_k \\| \\\\\n&\\leq \\sqrt{d} L^2 C^2 (1 + \\log A N) A^1 \\| z_0 - x^* \\|,\n\\end{align*}\n$$\n\nwhere we used Lemma A.7 in the last inequality.\n\nProof of Lemma D.4\n\nBefore presenting the proof of Lemma D.4, we start with a helper lemma that shows a lower bound on \\(A_1\\).\n\nLemma D.8. We have \\(A_1 = \\hat{\\eta}_0 \\geq \\min\\{\\sigma_0, \\alpha^2 \\beta L_1\\}\\).\n\nProof. The equality \\(A_1 = \\hat{\\eta}_0\\) is shown in the proof of Lemma A.2. To show the lower bound on \\(\\hat{\\eta}_0\\), we use Lemma 3.2 and separate two cases. If \\(0 \\notin B\\), then we have \\(\\hat{\\eta}_0 = \\eta_0 = \\sigma_0\\). Otherwise, if \\(0 \\in B\\), then we have\n\n$$\n\\begin{align*}\n\\hat{\\eta}_0 &\\geq \\left\\| \\nabla f(\\tilde{x}_1) - \\nabla f(y_0) - B_k(\\tilde{x}_1 - y_0) \\right\\| \\\\\n&\\geq \\alpha^2 \\beta \\| \\tilde{x}_1 - y_0 \\|_1.\n\\end{align*}\n$$\n\nMoreover, as shown in the proof of Lemma D.1, we have \\(\\left\\| \\nabla f(\\tilde{x}_1) - \\nabla f(y_0) - B_k(\\tilde{x}_1 - y_0) \\right\\| \\leq L_1 \\| \\tilde{x}_1 - y_0 \\|\\), which further implies that \\(\\hat{\\eta}_0 \\geq \\alpha^2 \\beta L_1\\). This completes the proof.\n\nWe combine Lemma D.2 and Lemma D.3 to get\n\n$$\n\\begin{align*}\n\\left\\| w_k - B_k s_k \\right\\|_2 &= \\sum_{k \\in B} \\left\\| s_k \\right\\|_2 \\\\\n&\\leq 256 \\| B_0 - H_0 \\|_F^2 + C_3 L^2 \\| z_0 - x^* \\|_2 + 2L^2 \\sum_{t=0}^{T-1} \\delta^2_t \\\\\n&\\leq 512 C^2 L_1 L_2 d (1 + \\log A N) A_1 \\| z_0 - x^* \\| + 2L^2 \\sum_{t=0}^{T-1} \\delta^2_t.\n\\end{align*}\n$$", "md": "where we used Lemma 3.2 in the last inequality. Also, Since \\(a_k \\geq \\eta_k\\) for all \\(k \\geq 0\\), by (21) we get\n\n$$\n\\begin{align*}\n\\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_k \\|_2 &\\leq \\sum_{k=0}^{N-1} a^2_k \\| \\hat{x}_{k+1} - y_k \\|_2 \\\\\n&\\leq \\frac{1 - 1}{\\sigma^2} \\| z_0 - x^* \\|_2.\n\\end{align*}\n$$\n\nHence, we have\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\ell_t(H_t) &\\leq (1 + \\alpha_1)^2 L^2 \\sum_{k=0}^{N-1} \\| \\hat{x}_{k+1} - y_k \\|_2 \\\\\n&\\leq (1 + \\alpha_1)^2 L^2 \\sqrt{2} \\| z_0 - x^* \\|_2.\n\\end{align*}\n$$\n\nwhich proves the first inequality in (72). Furthermore, by Assumption 2.2, we have\n\n$$\n\\| H_{t+1} - H_t \\|_F = \\| \\nabla^2 f(y_{t+1}) - \\nabla^2 f(y_t) \\|_F \\leq \\sqrt{d} \\| \\nabla^2 f(y_{t+1}) - \\nabla^2 f(y_t) \\|_{op} \\leq \\sqrt{d} L^2 \\| y_{t+1} - y_t \\|.\n$$\n\nHence, by using the triangle inequality, we can bound\n\n$$\n\\begin{align*}\n\\sum_{t=0}^{T-1} \\| H_{t+1} - H_t \\|_F &\\leq \\sqrt{d} L^2 \\sum_{k=0}^{N-1} \\| y_{k+1} - y_k \\| \\\\\n&\\leq \\sqrt{d} L^2 C^2 (1 + \\log A N) A^1 \\| z_0 - x^* \\|,\n\\end{align*}\n$$\n\nwhere we used Lemma A.7 in the last inequality.\n\nProof of Lemma D.4\n\nBefore presenting the proof of Lemma D.4, we start with a helper lemma that shows a lower bound on \\(A_1\\).\n\nLemma D.8. We have \\(A_1 = \\hat{\\eta}_0 \\geq \\min\\{\\sigma_0, \\alpha^2 \\beta L_1\\}\\).\n\nProof. The equality \\(A_1 = \\hat{\\eta}_0\\) is shown in the proof of Lemma A.2. To show the lower bound on \\(\\hat{\\eta}_0\\), we use Lemma 3.2 and separate two cases. If \\(0 \\notin B\\), then we have \\(\\hat{\\eta}_0 = \\eta_0 = \\sigma_0\\). Otherwise, if \\(0 \\in B\\), then we have\n\n$$\n\\begin{align*}\n\\hat{\\eta}_0 &\\geq \\left\\| \\nabla f(\\tilde{x}_1) - \\nabla f(y_0) - B_k(\\tilde{x}_1 - y_0) \\right\\| \\\\\n&\\geq \\alpha^2 \\beta \\| \\tilde{x}_1 - y_0 \\|_1.\n\\end{align*}\n$$\n\nMoreover, as shown in the proof of Lemma D.1, we have \\(\\left\\| \\nabla f(\\tilde{x}_1) - \\nabla f(y_0) - B_k(\\tilde{x}_1 - y_0) \\right\\| \\leq L_1 \\| \\tilde{x}_1 - y_0 \\|\\), which further implies that \\(\\hat{\\eta}_0 \\geq \\alpha^2 \\beta L_1\\). This completes the proof.\n\nWe combine Lemma D.2 and Lemma D.3 to get\n\n$$\n\\begin{align*}\n\\left\\| w_k - B_k s_k \\right\\|_2 &= \\sum_{k \\in B} \\left\\| s_k \\right\\|_2 \\\\\n&\\leq 256 \\| B_0 - H_0 \\|_F^2 + C_3 L^2 \\| z_0 - x^* \\|_2 + 2L^2 \\sum_{t=0}^{T-1} \\delta^2_t \\\\\n&\\leq 512 C^2 L_1 L_2 d (1 + \\log A N) A_1 \\| z_0 - x^* \\| + 2L^2 \\sum_{t=0}^{T-1} \\delta^2_t.\n\\end{align*}\n$$"}]}, {"page": 38, "text": "Since \u03b4t = 1/(\u221at + 2 ln(t + 2)), we have\n              T\u22121   \u03b42t =   T+1       1              1             T+1       1                 1         ln12 \u2212     ln(T +11) \u2264         2.5.\n               t=0           t=2   t ln2 t \u2264     2 ln2 2 +       2        t ln2 tdt =      2 ln2 2 +\nHence, it further follows from (14) and Lemma 3.3 that\n                                   N5     \u2264   4C2 1  N\u22121    1\n                                   A2                       \u02c6\n                                                            \u03b72\n                                      N              k=0     k\n                                          \u2264   4C2 1(2 \u2212     \u03b22)   + 4C2    1(2 \u2212     \u03b22)          \u2225wk \u2212      Bksk\u22252\n                                               (1 \u2212    \u03b22)\u03c32  0       (1 \u2212    \u03b22)\u03b12  2\u03b22    k\u2208B          \u2225sk\u22252\n                                          \u2264   C6   + C7L2     1 + C8\u2225B0 \u2212          H0\u22252  F + C9L2     2\u2225z0 \u2212      x\u2217\u22252\n                                              \u03c320\nTo simplify the notation, define               + C10L1L2d           1 + log AN   A1      \u2225z0 \u2212    x\u2217\u2225.                                              (80)\n                  M = C6   \u03c320  + C7L2    1 + C8\u2225B0 \u2212          H0\u22252   F + C9L2     2\u2225z0 \u2212      x\u2217\u22252 + C10L1L2d\u2225z0 \u2212                x\u2217\u2225,\nand the inequality in (80) becomes N5                    A2N \u2264     M + C10L1L2d\u2225z0 \u2212                x\u2217\u2225   log ANA1 . Let A\u2217     N be the number\nthat achieves the equality\n                                            (A\u2217N5N)2 = M + C10L1L2d\u2225z0 \u2212                     x\u2217\u2225   log A\u2217A1N  ,\nand we can see that AN \u2265                  A\u2217N. Thus, we instead try to construct a lower bound on A\u2217                                N. If A\u2217   N \u2264   A1,\nthen log(A\u2217      N/A1) \u2264        0 and furthermore      N5                             1        \u221a  M\n                                                    (A\u2217 N)2 \u2264      M        \u21d2       AN     \u2264   N2.5 .                                               (81)\nOtherwise, assume that A\u2217                N > A1. Then log(A\u2217             N/A1) > 0 and we first show an upper bound on A\u2217                              N:\n                        N5                                                                                                1\n                     (A\u2217 N)2 = M + C10L1L2d\u2225z0 \u2212                      x\u2217\u2225   log A\u2217A1N   \u2265   M        \u21d2       A\u2217N \u2264     \u221a  M    N2.5.\nThis in turn leads to a lower bound on A\u2217                        N:\n    N5                                                                                                                   max{ L1  \u03b12\u03b2  , 1\n  (A\u2217 N)2 = M + C10L1L2d\u2225z0 \u2212                     x\u2217\u2225   log A\u2217 A1N   \u2264   M + C10L1L2d\u2225z0 \u2212                x\u2217\u2225   log                 \u221a  M \u03c30 }N2.5       ,\nwhere we also used the fact that A1 \u2265                         min{\u03c30, \u03b12\u03b2   L1 } (cf. Lemma D.8). Thus, we get\n                    1          1           1                                                       max{ L1  \u03b12\u03b2  , 1              1   2\n                  AN     \u2264   A\u2217 N   \u2264   N2.5       M + C10L1L2d\u2225z0 \u2212                 x\u2217\u2225   log                \u221a  M \u03c30 }N2.5              .          (82)\nCombining both cases in (81) and (82), we conclude the proof of Lemma D.4.\n                                                                            38", "md": "Since $$\\delta t = \\frac{1}{\\sqrt{t} + 2 \\ln(t + 2)}$$, we have\n\n\\[\n\\begin{aligned}\nT^{-1} \\delta^2 t &= T+1 \\int_{t=0}^{t=2} \\frac{1}{t \\ln^2 t} dt \\\\\n&\\leq 2 \\ln 2 + 2 \\int_{t=0}^{t=2} \\frac{1}{t \\ln^2 t} dt \\\\\n&= 2 \\ln 2 + \\ldots\n\\end{aligned}\n\\]\n\nHence, it further follows from (14) and Lemma 3.3 that\n\n\\[\n\\begin{aligned}\nN^5 &\\leq 4C^2_1 N^{-1} \\sum_{k=0}^{\\hat{\\eta}^2} \\\\\n&\\leq 4C^2_1(2 - \\beta^2) + 4C^2_1(2 - \\beta^2) \\lVert w_k - B_k s_k \\rVert^2 \\\\\n&\\leq C_6 + C_7 L^2_1 + C_8 \\lVert B_0 - H_0 \\rVert^2_F + C_9 L^2_2 \\lVert z_0 - x^* \\rVert^2\n\\end{aligned}\n\\]\n\nTo simplify the notation, define\n\n$$M = C_6 \\sigma^2_0 + C_7 L^2_1 + C_8 \\lVert B_0 - H_0 \\rVert^2_F + C_9 L^2_2 \\lVert z_0 - x^* \\rVert^2 + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert$$,\n\nand the inequality in (80) becomes\n\n$$N^5 A^2 N \\leq M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{AN}{A_1}$$.\n\nLet $A^* N$ be the number that achieves the equality\n\n$$(A^* N^5 N)^2 = M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1}$$,\n\nand we can see that $AN \\geq A^* N$. Thus, we instead try to construct a lower bound on $A^* N$. If $A^* N \\leq A_1$, then $\\log\\left(\\frac{A^* N}{A_1}\\right) \\leq 0$ and furthermore\n\n$$(A^* N)^2 \\leq M \\Rightarrow AN \\leq N^{2.5}$$.\n\nOtherwise, assume that $A^* N > A_1$. Then $\\log\\left(\\frac{A^* N}{A_1}\\right) > 0$ and we first show an upper bound on $A^* N$:\n\n$$(A^* N)^2 = M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1} \\geq M \\Rightarrow A^* N \\leq \\sqrt{M} N^{2.5}$$.\n\nThis in turn leads to a lower bound on $A^* N$:\n\n\\[\n\\begin{aligned}\n(A^* N)^2 &= M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1} \\\\\n&\\leq M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\sqrt{M \\sigma^0} N^{2.5} \\\\\n&\\leq \\ldots\n\\end{aligned}\n\\]\n\nCombining both cases in (81) and (82), we conclude the proof of Lemma D.4.", "images": [], "items": [{"type": "text", "value": "Since $$\\delta t = \\frac{1}{\\sqrt{t} + 2 \\ln(t + 2)}$$, we have\n\n\\[\n\\begin{aligned}\nT^{-1} \\delta^2 t &= T+1 \\int_{t=0}^{t=2} \\frac{1}{t \\ln^2 t} dt \\\\\n&\\leq 2 \\ln 2 + 2 \\int_{t=0}^{t=2} \\frac{1}{t \\ln^2 t} dt \\\\\n&= 2 \\ln 2 + \\ldots\n\\end{aligned}\n\\]\n\nHence, it further follows from (14) and Lemma 3.3 that\n\n\\[\n\\begin{aligned}\nN^5 &\\leq 4C^2_1 N^{-1} \\sum_{k=0}^{\\hat{\\eta}^2} \\\\\n&\\leq 4C^2_1(2 - \\beta^2) + 4C^2_1(2 - \\beta^2) \\lVert w_k - B_k s_k \\rVert^2 \\\\\n&\\leq C_6 + C_7 L^2_1 + C_8 \\lVert B_0 - H_0 \\rVert^2_F + C_9 L^2_2 \\lVert z_0 - x^* \\rVert^2\n\\end{aligned}\n\\]\n\nTo simplify the notation, define\n\n$$M = C_6 \\sigma^2_0 + C_7 L^2_1 + C_8 \\lVert B_0 - H_0 \\rVert^2_F + C_9 L^2_2 \\lVert z_0 - x^* \\rVert^2 + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert$$,\n\nand the inequality in (80) becomes\n\n$$N^5 A^2 N \\leq M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{AN}{A_1}$$.\n\nLet $A^* N$ be the number that achieves the equality\n\n$$(A^* N^5 N)^2 = M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1}$$,\n\nand we can see that $AN \\geq A^* N$. Thus, we instead try to construct a lower bound on $A^* N$. If $A^* N \\leq A_1$, then $\\log\\left(\\frac{A^* N}{A_1}\\right) \\leq 0$ and furthermore\n\n$$(A^* N)^2 \\leq M \\Rightarrow AN \\leq N^{2.5}$$.\n\nOtherwise, assume that $A^* N > A_1$. Then $\\log\\left(\\frac{A^* N}{A_1}\\right) > 0$ and we first show an upper bound on $A^* N$:\n\n$$(A^* N)^2 = M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1} \\geq M \\Rightarrow A^* N \\leq \\sqrt{M} N^{2.5}$$.\n\nThis in turn leads to a lower bound on $A^* N$:\n\n\\[\n\\begin{aligned}\n(A^* N)^2 &= M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1} \\\\\n&\\leq M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\sqrt{M \\sigma^0} N^{2.5} \\\\\n&\\leq \\ldots\n\\end{aligned}\n\\]\n\nCombining both cases in (81) and (82), we conclude the proof of Lemma D.4.", "md": "Since $$\\delta t = \\frac{1}{\\sqrt{t} + 2 \\ln(t + 2)}$$, we have\n\n\\[\n\\begin{aligned}\nT^{-1} \\delta^2 t &= T+1 \\int_{t=0}^{t=2} \\frac{1}{t \\ln^2 t} dt \\\\\n&\\leq 2 \\ln 2 + 2 \\int_{t=0}^{t=2} \\frac{1}{t \\ln^2 t} dt \\\\\n&= 2 \\ln 2 + \\ldots\n\\end{aligned}\n\\]\n\nHence, it further follows from (14) and Lemma 3.3 that\n\n\\[\n\\begin{aligned}\nN^5 &\\leq 4C^2_1 N^{-1} \\sum_{k=0}^{\\hat{\\eta}^2} \\\\\n&\\leq 4C^2_1(2 - \\beta^2) + 4C^2_1(2 - \\beta^2) \\lVert w_k - B_k s_k \\rVert^2 \\\\\n&\\leq C_6 + C_7 L^2_1 + C_8 \\lVert B_0 - H_0 \\rVert^2_F + C_9 L^2_2 \\lVert z_0 - x^* \\rVert^2\n\\end{aligned}\n\\]\n\nTo simplify the notation, define\n\n$$M = C_6 \\sigma^2_0 + C_7 L^2_1 + C_8 \\lVert B_0 - H_0 \\rVert^2_F + C_9 L^2_2 \\lVert z_0 - x^* \\rVert^2 + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert$$,\n\nand the inequality in (80) becomes\n\n$$N^5 A^2 N \\leq M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{AN}{A_1}$$.\n\nLet $A^* N$ be the number that achieves the equality\n\n$$(A^* N^5 N)^2 = M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1}$$,\n\nand we can see that $AN \\geq A^* N$. Thus, we instead try to construct a lower bound on $A^* N$. If $A^* N \\leq A_1$, then $\\log\\left(\\frac{A^* N}{A_1}\\right) \\leq 0$ and furthermore\n\n$$(A^* N)^2 \\leq M \\Rightarrow AN \\leq N^{2.5}$$.\n\nOtherwise, assume that $A^* N > A_1$. Then $\\log\\left(\\frac{A^* N}{A_1}\\right) > 0$ and we first show an upper bound on $A^* N$:\n\n$$(A^* N)^2 = M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1} \\geq M \\Rightarrow A^* N \\leq \\sqrt{M} N^{2.5}$$.\n\nThis in turn leads to a lower bound on $A^* N$:\n\n\\[\n\\begin{aligned}\n(A^* N)^2 &= M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\frac{A^* A_1 N}{A_1} \\\\\n&\\leq M + C_{10} L_1 L_2 d \\lVert z_0 - x^* \\rVert \\log \\sqrt{M \\sigma^0} N^{2.5} \\\\\n&\\leq \\ldots\n\\end{aligned}\n\\]\n\nCombining both cases in (81) and (82), we conclude the proof of Lemma D.4."}]}, {"page": 39, "text": "Subroutine 3 LinearSolver(A, b; \u03b1)\n  1: Input: A \u2208          Sd\n                          +, b \u2208     Rd, 0 < \u03b1 < 1\n  2: Initialize: s0 \u2190          0, r0 \u2190     b \u2212   As0, p0 \u2190        r0\n  3: for k = 0, 1, . . . do\n  4:      if \u2225rk\u22252 \u2264      \u03b1\u2225sk\u22252 then\n  5:         Return sk\n  6:      end if\n  7:      \u03b1k \u2190    \u27e8rk, Ark\u27e9/\u27e8Apk, Apk\u27e9\n  8:      sk+1 \u2190     sk + \u03b1kpk\n  9:      rk+1 \u2190     rk \u2212   \u03b1kApk\n10:       Compute and store Ark+1\n11:       \u03b2k \u2190    \u27e8rk+1, Ark+1\u27e9/\u27e8rk, Ark\u27e9\n12:       pk+1 \u2190      rk+1 + \u03b2kpk\n13:       Compute and store Apk+1 \u2190                  Ark+1 + \u03b2kApk\n14: end for\nE        Characterizing the Computational Cost\nIn this section, we first specify the implementation details of the LinearSolver oracle in Definition 3.1\nand the SEP oracle in Definition 3.2. Then in Section E.3, we present the proof of Theorem 4.2.\nE.1        Implementation of the LinearSolver Oracle\nWe implement the LinearSolver oracle by running the conjugate residual (CR) method [Saa03] to\nsolve the linear system As = b. In particular, we initialize the CR method with s0 = 0 and returns\nthe iterate sk once we achieve \u2225Ask \u2212                        b\u2225   \u2264   \u03b1\u2225sk\u2225. The following lemma provides the convergence\nguarantee of the CR method, which will be later used in the proof of Theorem 4.2.\nLemma E.1 ([Nem95, Chapter 12.4]). Let s\u2217                                  be any optimal solution of As\u2217                   = b and let {sk} be\nthe iterates generated by Subroutine 3. Then we have\n                                                \u2225rk\u22252 = \u2225Ask \u2212            b\u22252 \u2264      \u03bbmax(A)\u2225s\u2217\u22252          .\n                                                                                         (k + 1)2\nE.2        Implementation of SEP Oracle\nWe implement the SEP oracle in Definition 3.2 based on the classical Lanczos method with a random\nstart, where the initial vector is chosen randomly and uniformly from the unit sphere (see, e.g.,\n[Saa11; YTFUC21]). For completeness, the full algorithm is described in Subroutine 4.\nTo prove the correctness of our algorithm, we first recall a classical result in [KW92] on the\nconvergence behavior of the Lanczos method.\nProposition E.2 ([KW92, Theorem 4.2]). Consider a symmetric matrix W and let \u03bb1(W) and\n\u03bbd(W) denote its largest and smallest eigenvalues, respectively. Then after k iterations of the\nLanczos method with a random start, we find unit vectors u(1) and u(d) such that\n                      P(\u27e8Wu(1), u(1)\u27e9          \u2264   \u03bb1(W) \u2212        \u03f5(\u03bb1(W) \u2212          \u03bbd(W))) \u2264         1.648   \u221a  de\u2212\u221a\u03f5(2k\u22121),\n                      P(\u27e8Wu(d), u(d)\u27e9          \u2265   \u03bbd(W) + \u03f5(\u03bb1(W) \u2212                 \u03bbd(W))) \u2264         1.648   \u221a  de\u2212\u221a\u03f5(2k\u22121),\n                                                                            39", "md": "Subroutine 3 LinearSolver(A, b; \u03b1)\n\n1: Input: $$A \\in \\mathbb{S}^d_+, b \\in \\mathbb{R}^d, 0 < \\alpha < 1$$\n\n2: Initialize: $$s_0 \\leftarrow 0, r_0 \\leftarrow b - As_0, p_0 \\leftarrow r_0$$\n\n3: for $$k = 0, 1, ...$$ do\n\n4: if $$\\|r_k\\|_2 \\leq \\alpha\\|s_k\\|_2$$ then\n\n5: Return $$s_k$$\n\n6: end if\n\n7: $$\\alpha_k \\leftarrow \\frac{\\langle r_k, A r_k \\rangle}{\\langle A p_k, A p_k \\rangle}$$\n\n8: $$s_{k+1} \\leftarrow s_k + \\alpha_k p_k$$\n\n9: $$r_{k+1} \\leftarrow r_k - \\alpha_k A p_k$$\n\n10: Compute and store $$A r_{k+1}$$\n\n11: $$\\beta_k \\leftarrow \\frac{\\langle r_{k+1}, A r_{k+1} \\rangle}{\\langle r_k, A r_k \\rangle}$$\n\n12: $$p_{k+1} \\leftarrow r_{k+1} + \\beta_k p_k$$\n\n13: Compute and store $$A p_{k+1} \\leftarrow A r_{k+1} + \\beta_k A p_k$$\n\n14: end for\n\nE Characterizing the Computational Cost\n\nIn this section, we first specify the implementation details of the LinearSolver oracle in Definition 3.1 and the SEP oracle in Definition 3.2. Then in Section E.3, we present the proof of Theorem 4.2.\n\nE.1 Implementation of the LinearSolver Oracle\n\nWe implement the LinearSolver oracle by running the conjugate residual (CR) method [Saa03] to solve the linear system $$As = b$$. In particular, we initialize the CR method with $$s_0 = 0$$ and returns the iterate $$s_k$$ once we achieve $$\\|As_k - b\\| \\leq \\alpha\\|s_k\\|$$. The following lemma provides the convergence guarantee of the CR method, which will be later used in the proof of Theorem 4.2.\n\nLemma E.1 ([Nem95, Chapter 12.4]). Let $$s^*$$ be any optimal solution of $$As^* = b$$ and let {$$s_k$$} be the iterates generated by Subroutine 3. Then we have\n\n$$\\|r_k\\|_2 = \\|As_k - b\\|_2 \\leq \\frac{\\lambda_{\\text{max}}(A)\\|s^*\\|_2}{(k + 1)^2}$$\n\nE.2 Implementation of SEP Oracle\n\nWe implement the SEP oracle in Definition 3.2 based on the classical Lanczos method with a random start, where the initial vector is chosen randomly and uniformly from the unit sphere (see, e.g., [Saa11; YTFUC21]). For completeness, the full algorithm is described in Subroutine 4.\n\nTo prove the correctness of our algorithm, we first recall a classical result in [KW92] on the convergence behavior of the Lanczos method.\n\nProposition E.2 ([KW92, Theorem 4.2]). Consider a symmetric matrix $$W$$ and let $$\\lambda_1(W)$$ and $$\\lambda_d(W)$$ denote its largest and smallest eigenvalues, respectively. Then after $$k$$ iterations of the Lanczos method with a random start, we find unit vectors $$u(1)$$ and $$u(d)$$ such that\n\n$$P(\\langle Wu(1), u(1) \\rangle \\leq \\lambda_1(W) - \\epsilon(\\lambda_1(W) - \\lambda_d(W))) \\leq 1.648 \\sqrt{de^{-\\sqrt{\\epsilon}(2k-1)}}$$\n\n$$P(\\langle Wu(d), u(d) \\rangle \\geq \\lambda_d(W) + \\epsilon(\\lambda_1(W) - \\lambda_d(W))) \\leq 1.648 \\sqrt{de^{-\\sqrt{\\epsilon}(2k-1)}}$$\n\n39", "images": [], "items": [{"type": "text", "value": "Subroutine 3 LinearSolver(A, b; \u03b1)\n\n1: Input: $$A \\in \\mathbb{S}^d_+, b \\in \\mathbb{R}^d, 0 < \\alpha < 1$$\n\n2: Initialize: $$s_0 \\leftarrow 0, r_0 \\leftarrow b - As_0, p_0 \\leftarrow r_0$$\n\n3: for $$k = 0, 1, ...$$ do\n\n4: if $$\\|r_k\\|_2 \\leq \\alpha\\|s_k\\|_2$$ then\n\n5: Return $$s_k$$\n\n6: end if\n\n7: $$\\alpha_k \\leftarrow \\frac{\\langle r_k, A r_k \\rangle}{\\langle A p_k, A p_k \\rangle}$$\n\n8: $$s_{k+1} \\leftarrow s_k + \\alpha_k p_k$$\n\n9: $$r_{k+1} \\leftarrow r_k - \\alpha_k A p_k$$\n\n10: Compute and store $$A r_{k+1}$$\n\n11: $$\\beta_k \\leftarrow \\frac{\\langle r_{k+1}, A r_{k+1} \\rangle}{\\langle r_k, A r_k \\rangle}$$\n\n12: $$p_{k+1} \\leftarrow r_{k+1} + \\beta_k p_k$$\n\n13: Compute and store $$A p_{k+1} \\leftarrow A r_{k+1} + \\beta_k A p_k$$\n\n14: end for\n\nE Characterizing the Computational Cost\n\nIn this section, we first specify the implementation details of the LinearSolver oracle in Definition 3.1 and the SEP oracle in Definition 3.2. Then in Section E.3, we present the proof of Theorem 4.2.\n\nE.1 Implementation of the LinearSolver Oracle\n\nWe implement the LinearSolver oracle by running the conjugate residual (CR) method [Saa03] to solve the linear system $$As = b$$. In particular, we initialize the CR method with $$s_0 = 0$$ and returns the iterate $$s_k$$ once we achieve $$\\|As_k - b\\| \\leq \\alpha\\|s_k\\|$$. The following lemma provides the convergence guarantee of the CR method, which will be later used in the proof of Theorem 4.2.\n\nLemma E.1 ([Nem95, Chapter 12.4]). Let $$s^*$$ be any optimal solution of $$As^* = b$$ and let {$$s_k$$} be the iterates generated by Subroutine 3. Then we have\n\n$$\\|r_k\\|_2 = \\|As_k - b\\|_2 \\leq \\frac{\\lambda_{\\text{max}}(A)\\|s^*\\|_2}{(k + 1)^2}$$\n\nE.2 Implementation of SEP Oracle\n\nWe implement the SEP oracle in Definition 3.2 based on the classical Lanczos method with a random start, where the initial vector is chosen randomly and uniformly from the unit sphere (see, e.g., [Saa11; YTFUC21]). For completeness, the full algorithm is described in Subroutine 4.\n\nTo prove the correctness of our algorithm, we first recall a classical result in [KW92] on the convergence behavior of the Lanczos method.\n\nProposition E.2 ([KW92, Theorem 4.2]). Consider a symmetric matrix $$W$$ and let $$\\lambda_1(W)$$ and $$\\lambda_d(W)$$ denote its largest and smallest eigenvalues, respectively. Then after $$k$$ iterations of the Lanczos method with a random start, we find unit vectors $$u(1)$$ and $$u(d)$$ such that\n\n$$P(\\langle Wu(1), u(1) \\rangle \\leq \\lambda_1(W) - \\epsilon(\\lambda_1(W) - \\lambda_d(W))) \\leq 1.648 \\sqrt{de^{-\\sqrt{\\epsilon}(2k-1)}}$$\n\n$$P(\\langle Wu(d), u(d) \\rangle \\geq \\lambda_d(W) + \\epsilon(\\lambda_1(W) - \\lambda_d(W))) \\leq 1.648 \\sqrt{de^{-\\sqrt{\\epsilon}(2k-1)}}$$\n\n39", "md": "Subroutine 3 LinearSolver(A, b; \u03b1)\n\n1: Input: $$A \\in \\mathbb{S}^d_+, b \\in \\mathbb{R}^d, 0 < \\alpha < 1$$\n\n2: Initialize: $$s_0 \\leftarrow 0, r_0 \\leftarrow b - As_0, p_0 \\leftarrow r_0$$\n\n3: for $$k = 0, 1, ...$$ do\n\n4: if $$\\|r_k\\|_2 \\leq \\alpha\\|s_k\\|_2$$ then\n\n5: Return $$s_k$$\n\n6: end if\n\n7: $$\\alpha_k \\leftarrow \\frac{\\langle r_k, A r_k \\rangle}{\\langle A p_k, A p_k \\rangle}$$\n\n8: $$s_{k+1} \\leftarrow s_k + \\alpha_k p_k$$\n\n9: $$r_{k+1} \\leftarrow r_k - \\alpha_k A p_k$$\n\n10: Compute and store $$A r_{k+1}$$\n\n11: $$\\beta_k \\leftarrow \\frac{\\langle r_{k+1}, A r_{k+1} \\rangle}{\\langle r_k, A r_k \\rangle}$$\n\n12: $$p_{k+1} \\leftarrow r_{k+1} + \\beta_k p_k$$\n\n13: Compute and store $$A p_{k+1} \\leftarrow A r_{k+1} + \\beta_k A p_k$$\n\n14: end for\n\nE Characterizing the Computational Cost\n\nIn this section, we first specify the implementation details of the LinearSolver oracle in Definition 3.1 and the SEP oracle in Definition 3.2. Then in Section E.3, we present the proof of Theorem 4.2.\n\nE.1 Implementation of the LinearSolver Oracle\n\nWe implement the LinearSolver oracle by running the conjugate residual (CR) method [Saa03] to solve the linear system $$As = b$$. In particular, we initialize the CR method with $$s_0 = 0$$ and returns the iterate $$s_k$$ once we achieve $$\\|As_k - b\\| \\leq \\alpha\\|s_k\\|$$. The following lemma provides the convergence guarantee of the CR method, which will be later used in the proof of Theorem 4.2.\n\nLemma E.1 ([Nem95, Chapter 12.4]). Let $$s^*$$ be any optimal solution of $$As^* = b$$ and let {$$s_k$$} be the iterates generated by Subroutine 3. Then we have\n\n$$\\|r_k\\|_2 = \\|As_k - b\\|_2 \\leq \\frac{\\lambda_{\\text{max}}(A)\\|s^*\\|_2}{(k + 1)^2}$$\n\nE.2 Implementation of SEP Oracle\n\nWe implement the SEP oracle in Definition 3.2 based on the classical Lanczos method with a random start, where the initial vector is chosen randomly and uniformly from the unit sphere (see, e.g., [Saa11; YTFUC21]). For completeness, the full algorithm is described in Subroutine 4.\n\nTo prove the correctness of our algorithm, we first recall a classical result in [KW92] on the convergence behavior of the Lanczos method.\n\nProposition E.2 ([KW92, Theorem 4.2]). Consider a symmetric matrix $$W$$ and let $$\\lambda_1(W)$$ and $$\\lambda_d(W)$$ denote its largest and smallest eigenvalues, respectively. Then after $$k$$ iterations of the Lanczos method with a random start, we find unit vectors $$u(1)$$ and $$u(d)$$ such that\n\n$$P(\\langle Wu(1), u(1) \\rangle \\leq \\lambda_1(W) - \\epsilon(\\lambda_1(W) - \\lambda_d(W))) \\leq 1.648 \\sqrt{de^{-\\sqrt{\\epsilon}(2k-1)}}$$\n\n$$P(\\langle Wu(d), u(d) \\rangle \\geq \\lambda_d(W) + \\epsilon(\\lambda_1(W) - \\lambda_d(W))) \\leq 1.648 \\sqrt{de^{-\\sqrt{\\epsilon}(2k-1)}}$$\n\n39"}]}, {"page": 40, "text": "Subroutine 4 SEP(W; \u03b4, q)\n 1: Input: W \u2208          Sd, \u03b4 > 0, q \u2208     (0, 1)\n 2: Set the number of iterations N1 \u2190                min      log 11d          , d\n                                                                    q2 + 1  2\n 3: Run Lanczos method with a random start for N1 iterations to get u(1) and u(d) (cf. Proposition E.2)\n 4: Set \u02c6  \u03bb1 \u2190    \u27e8Wu(1), u(1)\u27e9      and \u02c6 \u03bbd \u2190    \u27e8Wu(d), u(d)\u27e9\n 5: Set \u02c6  \u03bbmax \u2190     max{\u02c6  \u03bb1, \u2212\u02c6 \u03bbd}\n 6: if \u02c6 \u03bbmax \u2264    1/2 then         # Case I: \u03b3 \u2264        1, which implies \u2225W\u2225op \u2264            1\n 7:      Return \u03b3 = 2\u02c6     \u03bbmax and S = 0\n 8: else if \u02c6  \u03bbmax \u2265     2 then        # Case II: \u03b3 > 1 and S defines a separating hyperplane\n 9:      if \u02c6\n            \u03bb1 > \u2212\u02c6   \u03bbd then\n10:         Return \u03b3 = 2\u02c6     \u03bbmax and S = 3u(1)(u(1))\u22a4\n11:      else\n12:         Return \u03b3 = 2\u02c6     \u03bbmax and S = \u22123u(d)(u(d))\u22a4\n13:      end if\n14: else       # 1 2 < \u02c6 \u03bb max < 2                                 \u221a1                    , d\n15:      Set the number of iterations N2 \u2190               min                  q2 + 1  2\n                                                                  4  2\u03b4 log 11d\n16:      Run Lanczos method with a random start for N2 iterations to get \u02dc                        u(1) and \u02dc   u(d) (cf. Proposition E.2)\n17:      Set \u02dc\u03bb1 \u2190    \u27e8W\u02dc  u(1) , \u02dc\n                                  u(1) \u27e9 and \u02dc \u03bbd \u2190    \u27e8W\u02dc  u(d)  , \u02dc\n18:      Set \u02dc\u03bbmax = max{\u02dc      \u03bb1, \u2212\u02dc \u03bbd}                         u(d)  \u27e9\n19:      if \u02dc\n            \u03bbmax \u2264     1 \u2212  \u03b4 then\n20:         Return \u03b3 = \u02dc     \u03bbmax + \u03b4 and S = 0\n21:      else if \u02dc \u03bb1 \u2265   \u2212\u02dc \u03bbd then\n22:         Return \u03b3 = \u02dc     \u03bbmax + \u03b4 and S = \u02dc       u(1) (\u02dc\n23:      else                                               u(1)  )\u22a4\n24:         Return \u03b3 = \u02dc     \u03bbmax + \u03b4 and S = \u2212\u02dc        u(d) (\u02dcu(d) )\u22a4\n25:      end if\n26: end if\nAs a corollary, to ensure that, with probability at least 1 \u2212                          q,\n  \u27e8Wu(1), u(1)\u27e9       > \u03bb1(W) \u2212         \u03f5(\u03bb1(W) \u2212        \u03bbd(W)) and \u27e8Wu(d), u(d)\u27e9               < \u03bbn(W) + \u03f5(\u03bb1(W) \u2212                 \u03bbd(W)),\nthe number of iterations can be bounded by \u23081                     4\u03f5\u22121/2 log(11d/q2) + 1          2\u2309.\nLemma E.3. Let \u03b3 and S be the output of SEP(W; \u03b4, q) in Subroutine 4. Then with probability at\nleast 1 \u2212    q, they satisfy one of the following properties:\n     \u2022 Case I: \u03b3 \u2264        1, then we have \u2225W\u2225op \u2264                 1;\n     \u2022 Case II: \u03b3 > 1, then we have \u2225W/\u03b3\u2225op \u2264                            1, \u2225S\u2225F = 3 and \u27e8S, W \u2212                 \u02c6\n        such that \u2225      \u02c6                                                                                       B\u27e9   \u2265   \u03b3 \u2212   1 for any \u02c6    B\n                         B\u2225op \u2264      1.\nProof. Note that in Subroutine 4, we first run the Lanczos method for                                    \u03f5\u22121/2 log 11d q2 + 1  2    iterations,\nwhere \u03f5 = 1     4. Thus, by Proposition E.2, with probability at least 1 \u2212                             q/2 we have\n                                   \u02c6\n                                   \u03bb1 \u225c    \u27e8Wu(1), u(1)\u27e9       \u2265  \u03bb1(W) \u2212        1                                                          (83)\n                                   \u02c6                                             4(\u03bb1(W) \u2212         \u03bbd(W)),\n                                  \u03bbd \u225c     \u27e8Wu(d), u(d)\u27e9       \u2264  \u03bbd(W) + 1      4(\u03bb1(W) \u2212         \u03bbd(W)).                                  (84)\n                                                                       40", "md": "Subroutine 4 SEP(W; \u03b4, q)\n\n|1:|Input: \\(W \\in S^d\\), \\(\\delta > 0\\), \\(q \\in (0, 1)\\)|\n|---|---|\n|2:|Set the number of iterations \\(N1 \\leftarrow \\min \\left( \\log 11d, \\frac{q^2 + 1}{2} \\right)\\)|\n|3:|Run Lanczos method with a random start for \\(N1\\) iterations to get \\(u(1)\\) and \\(u(d)\\) (cf. Proposition E.2)|\n|4:|Set \\(\\hat{\\lambda}_1 \\leftarrow \\langle Wu(1), u(1) \\rangle\\) and \\(\\hat{\\lambda}_d \\leftarrow \\langle Wu(d), u(d) \\rangle\\)|\n|5:|Set \\(\\hat{\\lambda}_{\\text{max}} \\leftarrow \\max\\{ \\hat{\\lambda}_1, -\\hat{\\lambda}_d \\}\\)|\n|6:|if \\(\\hat{\\lambda}_{\\text{max}} \\leq 1/2\\) then # Case I: \\(\\gamma \\leq 1\\), which implies \\(\\|W\\|_{\\text{op}} \\leq 1\\)|\n|7:|Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = 0\\)|\n|8:|else if \\(\\hat{\\lambda}_{\\text{max}} \\geq 2\\) then # Case II: \\(\\gamma > 1\\) and \\(S\\) defines a separating hyperplane|\n|9:|if \\(\\hat{\\lambda}_1 > -\\hat{\\lambda}_d\\) then|\n|10:|Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = 3u(1)(u(1))^{\\top}\\)|\n|11:|else|\n|12:|Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = -3u(d)(u(d))^{\\top}\\)|\n|13:|end if|\n|14:|else # \\(1/2 < \\hat{\\lambda}_{\\text{max}} < 2\\)|\n|15:|Set the number of iterations \\(N2 \\leftarrow \\min \\left( \\frac{4}{2\\delta} \\log 11d, q^2 + 1 \\right)\\)|\n|16:|Run Lanczos method with a random start for \\(N2\\) iterations to get \\(\\tilde{u}(1)\\) and \\(\\tilde{u}(d)\\) (cf. Proposition E.2)|\n|17:|Set \\(\\tilde{\\lambda}_1 \\leftarrow \\langle W\\tilde{u}(1), \\tilde{u}(1) \\rangle\\) and \\(\\tilde{\\lambda}_d \\leftarrow \\langle W\\tilde{u}(d), \\tilde{u}(d) \\rangle\\)|\n|18:|Set \\(\\tilde{\\lambda}_{\\text{max}} = \\max\\{ \\tilde{\\lambda}_1, -\\tilde{\\lambda}_d \\}\\)|\n|19:|if \\(\\tilde{\\lambda}_{\\text{max}} \\leq 1 - \\delta\\) then|\n|20:|Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = 0\\)|\n|21:|else if \\(\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d\\) then|\n|22:|Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = \\tilde{u}(1)(\\tilde{u}(1))^{\\top}\\)|\n|23:|else|\n|24:|Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = -\\tilde{u}(d)(\\tilde{u}(d))^{\\top}\\)|\n|25:|end if|\n|26:|end if|\n\nAs a corollary, to ensure that, with probability at least \\(1 - q\\),\n\n$$\\langle Wu(1), u(1) \\rangle > \\lambda_1(W) - \\epsilon(\\lambda_1(W) - \\lambda_d(W))$$\n$$\\text{and}$$\n$$\\langle Wu(d), u(d) \\rangle < \\lambda_n(W) + \\epsilon(\\lambda_1(W) - \\lambda_d(W)),$$\nthe number of iterations can be bounded by \\(\\lceil 1 - 4\\epsilon^{-1/2} \\log(11d/q^2) + 1/2 \\rceil\\).\n\nLemma E.3. Let \\(\\gamma\\) and \\(S\\) be the output of SEP(W; \u03b4, q) in Subroutine 4. Then with probability at least \\(1 - q\\), they satisfy one of the following properties:\n\n- Case I: \\(\\gamma \\leq 1\\), then we have \\(\\|W\\|_{\\text{op}} \\leq 1\\);\n- Case II: \\(\\gamma > 1\\), then we have \\(\\|W/\\gamma\\|_{\\text{op}} \\leq 1\\), \\(\\|S\\|_F = 3\\) and \\(\\langle S, W - \\hat{B} \\rangle \\geq \\gamma - 1\\) for any \\(\\hat{B}\\) such that \\(\\| \\hat{B} \\|_{\\text{op}} \\leq 1\\).\n\nProof. Note that in Subroutine 4, we first run the Lanczos method for \\(\\epsilon^{-1/2} \\log 11d q^2 + 1/2\\) iterations, where \\(\\epsilon = 1/4\\). Thus, by Proposition E.2, with probability at least \\(1 - q/2\\) we have\n\n$$\\hat{\\lambda}_1 \\triangleq \\langle Wu(1), u(1) \\rangle \\geq \\lambda_1(W) - \\frac{1}{4} ( \\lambda_1(W) - \\lambda_d(W) ) \\quad (83)$$\n$$\\text{and}$$\n$$\\lambda_d \\triangleq \\langle Wu(d), u(d) \\rangle \\leq \\lambda_d(W) + \\frac{1}{4} ( \\lambda_1(W) - \\lambda_d(W) ). \\quad (84)$$\n40", "images": [], "items": [{"type": "text", "value": "Subroutine 4 SEP(W; \u03b4, q)", "md": "Subroutine 4 SEP(W; \u03b4, q)"}, {"type": "table", "rows": [["1:", "Input: \\(W \\in S^d\\), \\(\\delta > 0\\), \\(q \\in (0, 1)\\)"], ["2:", "Set the number of iterations \\(N1 \\leftarrow \\min \\left( \\log 11d, \\frac{q^2 + 1}{2} \\right)\\)"], ["3:", "Run Lanczos method with a random start for \\(N1\\) iterations to get \\(u(1)\\) and \\(u(d)\\) (cf. Proposition E.2)"], ["4:", "Set \\(\\hat{\\lambda}_1 \\leftarrow \\langle Wu(1), u(1) \\rangle\\) and \\(\\hat{\\lambda}_d \\leftarrow \\langle Wu(d), u(d) \\rangle\\)"], ["5:", "Set \\(\\hat{\\lambda}_{\\text{max}} \\leftarrow \\max\\{ \\hat{\\lambda}_1, -\\hat{\\lambda}_d \\}\\)"], ["6:", "if \\(\\hat{\\lambda}_{\\text{max}} \\leq 1/2\\) then # Case I: \\(\\gamma \\leq 1\\), which implies \\(\\", "W\\", "_{\\text{op}} \\leq 1\\)"], ["7:", "Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = 0\\)"], ["8:", "else if \\(\\hat{\\lambda}_{\\text{max}} \\geq 2\\) then # Case II: \\(\\gamma > 1\\) and \\(S\\) defines a separating hyperplane"], ["9:", "if \\(\\hat{\\lambda}_1 > -\\hat{\\lambda}_d\\) then"], ["10:", "Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = 3u(1)(u(1))^{\\top}\\)"], ["11:", "else"], ["12:", "Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = -3u(d)(u(d))^{\\top}\\)"], ["13:", "end if"], ["14:", "else # \\(1/2 < \\hat{\\lambda}_{\\text{max}} < 2\\)"], ["15:", "Set the number of iterations \\(N2 \\leftarrow \\min \\left( \\frac{4}{2\\delta} \\log 11d, q^2 + 1 \\right)\\)"], ["16:", "Run Lanczos method with a random start for \\(N2\\) iterations to get \\(\\tilde{u}(1)\\) and \\(\\tilde{u}(d)\\) (cf. Proposition E.2)"], ["17:", "Set \\(\\tilde{\\lambda}_1 \\leftarrow \\langle W\\tilde{u}(1), \\tilde{u}(1) \\rangle\\) and \\(\\tilde{\\lambda}_d \\leftarrow \\langle W\\tilde{u}(d), \\tilde{u}(d) \\rangle\\)"], ["18:", "Set \\(\\tilde{\\lambda}_{\\text{max}} = \\max\\{ \\tilde{\\lambda}_1, -\\tilde{\\lambda}_d \\}\\)"], ["19:", "if \\(\\tilde{\\lambda}_{\\text{max}} \\leq 1 - \\delta\\) then"], ["20:", "Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = 0\\)"], ["21:", "else if \\(\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d\\) then"], ["22:", "Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = \\tilde{u}(1)(\\tilde{u}(1))^{\\top}\\)"], ["23:", "else"], ["24:", "Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = -\\tilde{u}(d)(\\tilde{u}(d))^{\\top}\\)"], ["25:", "end if"], ["26:", "end if"]], "md": "|1:|Input: \\(W \\in S^d\\), \\(\\delta > 0\\), \\(q \\in (0, 1)\\)|\n|---|---|\n|2:|Set the number of iterations \\(N1 \\leftarrow \\min \\left( \\log 11d, \\frac{q^2 + 1}{2} \\right)\\)|\n|3:|Run Lanczos method with a random start for \\(N1\\) iterations to get \\(u(1)\\) and \\(u(d)\\) (cf. Proposition E.2)|\n|4:|Set \\(\\hat{\\lambda}_1 \\leftarrow \\langle Wu(1), u(1) \\rangle\\) and \\(\\hat{\\lambda}_d \\leftarrow \\langle Wu(d), u(d) \\rangle\\)|\n|5:|Set \\(\\hat{\\lambda}_{\\text{max}} \\leftarrow \\max\\{ \\hat{\\lambda}_1, -\\hat{\\lambda}_d \\}\\)|\n|6:|if \\(\\hat{\\lambda}_{\\text{max}} \\leq 1/2\\) then # Case I: \\(\\gamma \\leq 1\\), which implies \\(\\|W\\|_{\\text{op}} \\leq 1\\)|\n|7:|Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = 0\\)|\n|8:|else if \\(\\hat{\\lambda}_{\\text{max}} \\geq 2\\) then # Case II: \\(\\gamma > 1\\) and \\(S\\) defines a separating hyperplane|\n|9:|if \\(\\hat{\\lambda}_1 > -\\hat{\\lambda}_d\\) then|\n|10:|Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = 3u(1)(u(1))^{\\top}\\)|\n|11:|else|\n|12:|Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = -3u(d)(u(d))^{\\top}\\)|\n|13:|end if|\n|14:|else # \\(1/2 < \\hat{\\lambda}_{\\text{max}} < 2\\)|\n|15:|Set the number of iterations \\(N2 \\leftarrow \\min \\left( \\frac{4}{2\\delta} \\log 11d, q^2 + 1 \\right)\\)|\n|16:|Run Lanczos method with a random start for \\(N2\\) iterations to get \\(\\tilde{u}(1)\\) and \\(\\tilde{u}(d)\\) (cf. Proposition E.2)|\n|17:|Set \\(\\tilde{\\lambda}_1 \\leftarrow \\langle W\\tilde{u}(1), \\tilde{u}(1) \\rangle\\) and \\(\\tilde{\\lambda}_d \\leftarrow \\langle W\\tilde{u}(d), \\tilde{u}(d) \\rangle\\)|\n|18:|Set \\(\\tilde{\\lambda}_{\\text{max}} = \\max\\{ \\tilde{\\lambda}_1, -\\tilde{\\lambda}_d \\}\\)|\n|19:|if \\(\\tilde{\\lambda}_{\\text{max}} \\leq 1 - \\delta\\) then|\n|20:|Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = 0\\)|\n|21:|else if \\(\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d\\) then|\n|22:|Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = \\tilde{u}(1)(\\tilde{u}(1))^{\\top}\\)|\n|23:|else|\n|24:|Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = -\\tilde{u}(d)(\\tilde{u}(d))^{\\top}\\)|\n|25:|end if|\n|26:|end if|", "isPerfectTable": false, "csv": "\"1:\",\"Input: \\(W \\in S^d\\), \\(\\delta > 0\\), \\(q \\in (0, 1)\\)\"\n\"2:\",\"Set the number of iterations \\(N1 \\leftarrow \\min \\left( \\log 11d, \\frac{q^2 + 1}{2} \\right)\\)\"\n\"3:\",\"Run Lanczos method with a random start for \\(N1\\) iterations to get \\(u(1)\\) and \\(u(d)\\) (cf. Proposition E.2)\"\n\"4:\",\"Set \\(\\hat{\\lambda}_1 \\leftarrow \\langle Wu(1), u(1) \\rangle\\) and \\(\\hat{\\lambda}_d \\leftarrow \\langle Wu(d), u(d) \\rangle\\)\"\n\"5:\",\"Set \\(\\hat{\\lambda}_{\\text{max}} \\leftarrow \\max\\{ \\hat{\\lambda}_1, -\\hat{\\lambda}_d \\}\\)\"\n\"6:\",\"if \\(\\hat{\\lambda}_{\\text{max}} \\leq 1/2\\) then # Case I: \\(\\gamma \\leq 1\\), which implies \\(\\\",\"W\\\",\"_{\\text{op}} \\leq 1\\)\"\n\"7:\",\"Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = 0\\)\"\n\"8:\",\"else if \\(\\hat{\\lambda}_{\\text{max}} \\geq 2\\) then # Case II: \\(\\gamma > 1\\) and \\(S\\) defines a separating hyperplane\"\n\"9:\",\"if \\(\\hat{\\lambda}_1 > -\\hat{\\lambda}_d\\) then\"\n\"10:\",\"Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = 3u(1)(u(1))^{\\top}\\)\"\n\"11:\",\"else\"\n\"12:\",\"Return \\(\\gamma = 2\\hat{\\lambda}_{\\text{max}}\\) and \\(S = -3u(d)(u(d))^{\\top}\\)\"\n\"13:\",\"end if\"\n\"14:\",\"else # \\(1/2 < \\hat{\\lambda}_{\\text{max}} < 2\\)\"\n\"15:\",\"Set the number of iterations \\(N2 \\leftarrow \\min \\left( \\frac{4}{2\\delta} \\log 11d, q^2 + 1 \\right)\\)\"\n\"16:\",\"Run Lanczos method with a random start for \\(N2\\) iterations to get \\(\\tilde{u}(1)\\) and \\(\\tilde{u}(d)\\) (cf. Proposition E.2)\"\n\"17:\",\"Set \\(\\tilde{\\lambda}_1 \\leftarrow \\langle W\\tilde{u}(1), \\tilde{u}(1) \\rangle\\) and \\(\\tilde{\\lambda}_d \\leftarrow \\langle W\\tilde{u}(d), \\tilde{u}(d) \\rangle\\)\"\n\"18:\",\"Set \\(\\tilde{\\lambda}_{\\text{max}} = \\max\\{ \\tilde{\\lambda}_1, -\\tilde{\\lambda}_d \\}\\)\"\n\"19:\",\"if \\(\\tilde{\\lambda}_{\\text{max}} \\leq 1 - \\delta\\) then\"\n\"20:\",\"Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = 0\\)\"\n\"21:\",\"else if \\(\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d\\) then\"\n\"22:\",\"Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = \\tilde{u}(1)(\\tilde{u}(1))^{\\top}\\)\"\n\"23:\",\"else\"\n\"24:\",\"Return \\(\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta\\) and \\(S = -\\tilde{u}(d)(\\tilde{u}(d))^{\\top}\\)\"\n\"25:\",\"end if\"\n\"26:\",\"end if\""}, {"type": "text", "value": "As a corollary, to ensure that, with probability at least \\(1 - q\\),\n\n$$\\langle Wu(1), u(1) \\rangle > \\lambda_1(W) - \\epsilon(\\lambda_1(W) - \\lambda_d(W))$$\n$$\\text{and}$$\n$$\\langle Wu(d), u(d) \\rangle < \\lambda_n(W) + \\epsilon(\\lambda_1(W) - \\lambda_d(W)),$$\nthe number of iterations can be bounded by \\(\\lceil 1 - 4\\epsilon^{-1/2} \\log(11d/q^2) + 1/2 \\rceil\\).\n\nLemma E.3. Let \\(\\gamma\\) and \\(S\\) be the output of SEP(W; \u03b4, q) in Subroutine 4. Then with probability at least \\(1 - q\\), they satisfy one of the following properties:\n\n- Case I: \\(\\gamma \\leq 1\\), then we have \\(\\|W\\|_{\\text{op}} \\leq 1\\);\n- Case II: \\(\\gamma > 1\\), then we have \\(\\|W/\\gamma\\|_{\\text{op}} \\leq 1\\), \\(\\|S\\|_F = 3\\) and \\(\\langle S, W - \\hat{B} \\rangle \\geq \\gamma - 1\\) for any \\(\\hat{B}\\) such that \\(\\| \\hat{B} \\|_{\\text{op}} \\leq 1\\).\n\nProof. Note that in Subroutine 4, we first run the Lanczos method for \\(\\epsilon^{-1/2} \\log 11d q^2 + 1/2\\) iterations, where \\(\\epsilon = 1/4\\). Thus, by Proposition E.2, with probability at least \\(1 - q/2\\) we have\n\n$$\\hat{\\lambda}_1 \\triangleq \\langle Wu(1), u(1) \\rangle \\geq \\lambda_1(W) - \\frac{1}{4} ( \\lambda_1(W) - \\lambda_d(W) ) \\quad (83)$$\n$$\\text{and}$$\n$$\\lambda_d \\triangleq \\langle Wu(d), u(d) \\rangle \\leq \\lambda_d(W) + \\frac{1}{4} ( \\lambda_1(W) - \\lambda_d(W) ). \\quad (84)$$\n40", "md": "As a corollary, to ensure that, with probability at least \\(1 - q\\),\n\n$$\\langle Wu(1), u(1) \\rangle > \\lambda_1(W) - \\epsilon(\\lambda_1(W) - \\lambda_d(W))$$\n$$\\text{and}$$\n$$\\langle Wu(d), u(d) \\rangle < \\lambda_n(W) + \\epsilon(\\lambda_1(W) - \\lambda_d(W)),$$\nthe number of iterations can be bounded by \\(\\lceil 1 - 4\\epsilon^{-1/2} \\log(11d/q^2) + 1/2 \\rceil\\).\n\nLemma E.3. Let \\(\\gamma\\) and \\(S\\) be the output of SEP(W; \u03b4, q) in Subroutine 4. Then with probability at least \\(1 - q\\), they satisfy one of the following properties:\n\n- Case I: \\(\\gamma \\leq 1\\), then we have \\(\\|W\\|_{\\text{op}} \\leq 1\\);\n- Case II: \\(\\gamma > 1\\), then we have \\(\\|W/\\gamma\\|_{\\text{op}} \\leq 1\\), \\(\\|S\\|_F = 3\\) and \\(\\langle S, W - \\hat{B} \\rangle \\geq \\gamma - 1\\) for any \\(\\hat{B}\\) such that \\(\\| \\hat{B} \\|_{\\text{op}} \\leq 1\\).\n\nProof. Note that in Subroutine 4, we first run the Lanczos method for \\(\\epsilon^{-1/2} \\log 11d q^2 + 1/2\\) iterations, where \\(\\epsilon = 1/4\\). Thus, by Proposition E.2, with probability at least \\(1 - q/2\\) we have\n\n$$\\hat{\\lambda}_1 \\triangleq \\langle Wu(1), u(1) \\rangle \\geq \\lambda_1(W) - \\frac{1}{4} ( \\lambda_1(W) - \\lambda_d(W) ) \\quad (83)$$\n$$\\text{and}$$\n$$\\lambda_d \\triangleq \\langle Wu(d), u(d) \\rangle \\leq \\lambda_d(W) + \\frac{1}{4} ( \\lambda_1(W) - \\lambda_d(W) ). \\quad (84)$$\n40"}]}, {"page": 41, "text": "Combining (83) and (84), we get\n                1                         \u03bb1 \u2212  \u02c6\n                                                \u03bbd    \u21d2     \u03bb1(W) \u2212    \u03bbd(W) \u2264    2(\u02c6\u03bb1 \u2212  \u02c6\n                2(\u03bb1(W) \u2212     \u03bbd(W)) \u2264    \u02c6                                               \u03bbd).\nBy plugging the above inequality back into (83) and (84), we further have\n                        \u03bb1(W) \u2264     \u02c6\n                                    \u03bb1 + 1                          \u03bb1 + 1  \u03bb 1 \u2212 \u02c6\n                        \u03bbd(W) \u2265     \u02c6     4(\u03bb1(W) \u2212    \u03bbd(W)) \u2264     \u02c6     2(\u02c6     \u03bbd),                     (85)\n                                    \u03bbd \u2212  1                         \u03bbd \u2212  1 \u03bb1 \u2212  \u02c6\nLet \u02c6                                     4(\u03bb1(W) \u2212    \u03bbd(W)) \u2265     \u02c6     2(\u02c6     \u03bbd).                     (86)\n    \u03bbmax = max{\u02c6   \u03bb1, \u2212\u02c6\u03bbd}. By (85) and (86), we can further bound the eigenvalues of W by\n        \u03bb1(W) \u2264    \u02c6\n                   \u03bbmax + 1      \u03bbmax = 2\u02c6 \u03bbmax    and    \u03bbd(W) \u2265    \u2212\u02c6\u03bbmax \u2212   1    \u03bbmax = \u22122\u02c6  \u03bbmax.\n                            2 \u00b7 2\u02c6                                              2 \u00b7 2\u02c6\nHence, we can see that \u2225W\u2225op = max{\u03bb1(W), \u2212\u03bbd(W)} \u2264                2\u02c6\u03bbmax. Now we distinguish three cases.\n  (a) If \u02c6\n         \u03bbmax \u2264   1                                                                      \u03bbmax \u2264   1 and S = 0.\n                  2, then we are in Case I and the ExtEvec oracle outputs \u03b3 = 2\u02c6\n      In this case, we indeed have \u2225W\u2225op \u2264        \u03b3 \u2264  1.\n (b) If \u02c6\u03bbmax \u2265    2, then we are in Case II. In addition, if \u02c6       \u03bb1 \u2265   \u2212\u02c6\u03bbd, then the ExtEvec oracle\n      returns \u03b3 = 2\u02c6  \u03bbmax and S = 3u(1)(u(1))\u22a4. Similarly, if \u2212\u02c6        \u03bbd > \u02c6\u03bb1, then the ExtEvec oracle\n      returns \u03b3 = 2\u02c6 \u03bbmax and S = \u22123u(d)(u(d))\u22a4. Without loss of generality, consider the case where\n      \u02c6\n      \u03bb1 \u2265  \u2212\u02c6\u03bbd. Since \u2225W\u2225op \u2264      2\u02c6\n                                      \u03bbmax = \u03b3, we have \u2225W/\u03b3\u2225op \u2264         1. Also, since u1 is a unit vector,\n      we have \u2225S\u2225F = 3\u2225u(1)\u22252 = 3. Finally, for any \u02c6       B such that \u2225\u02c6 B\u2225op \u2264    1, we have\n             \u27e8S, W \u2212   \u02c6\n                       B\u27e9  = 3(u(1))\u22a4Wu(1) \u2212     3(u(1))\u22a4  \u02c6\n                                                           Bu(1) \u2265   3\u02c6\n                                                                      \u03bbmax \u2212   3 \u2265 2\u02c6\u03bbmax \u2212  1 = \u03b3 \u2212   1,\n      where we used the fact that \u02c6    \u03bbmax \u2265  2 in the last inequality.\n  (c) If 1    \u03bbmax < 2, we continue to run the Lanczos method for a total number of           1\n         2 < \u02c6                                                                                 4\u03f5\u22121/2 log 11d\n      1   iterations, where \u03f5 = 1                                                                          q2 +\n      2                            8\u03b4. Thus, by Proposition E.2, with probability at least 1 \u2212          q/2 we\n      have\n                             \u02dc\n                             \u03bb1 \u225c  \u27e8W\u02dc u(1), \u02dc\n                                            u(1) \u27e9 \u2265 \u03bb1(W) \u2212     1                                         (87)\n                             \u02dc                                   8\u03b4(\u03bb1(W) \u2212    \u03bbd(W)),\n                             \u03bbd \u225c  \u27e8W\u02dc u(d), \u02dc\n                                            u(d) \u27e9\u2264  \u03bbd(W) + 1   8\u03b4(\u03bb1(W) \u2212    \u03bbd(W)).                     (88)\n      Let \u02dc\u03bbmax = max{\u02dc   \u03bb1, \u2212\u02dc\u03bbd}. Since we have \u03bb1(W) \u2264        2\u02c6\u03bbmax \u2264  4 and \u03bbd(W) \u2265      \u22122\u02c6 \u03bbmax \u2265   \u22124,\n      the above implies that \u02dc    \u03bb1 \u2265   \u03bb1(W) \u2212     \u03b4 and \u02dc \u03bbd \u2264   \u03bbd(W) + \u03b4. Hence, we can see that\n      \u2225W\u2225op = max{\u03bb1(W), \u2212\u03bbd(W)} \u2264             \u02c6\n                                               \u03bbmax + \u03b4. We further consider two subcases.\n      (c1) If \u02dc\n              \u03bbmax \u2264   1 \u2212  \u03b4, then we are in Case I and the ExtEvec oracle outputs \u03b3 = \u02dc        \u03bbmax + \u03b4 and\n            S = 0. In this case, we indeed have \u2225W\u2225op \u2264         \u03b3 \u2264  1.\n      (c2) If \u02dc\n              \u03bbmax > 1\u2212\u03b4, then we are in Case II. In addition, if \u02dc      \u03bb1 \u2265  \u2212\u02dc\u03bbd, then the ExtEvec oracle\n            returns \u03b3 = \u02dc  \u03bbmax + \u03b4 and S = \u02dc    u(1)(\u02dc\n                                                      u(1) )\u22a4. Similarly, if \u2212\u02dc \u03bbd > \u02dc \u03bb1, then the ExtEvec\n            oracle returns \u03b3 = \u02dc  \u03bbmax + \u03b4 and S = \u2212\u02dc   u(d) (\u02dc\n            the case where \u02dc  \u03bb1 \u2265  \u2212\u02dc\u03bbd. Since \u2225W\u2225op \u2264      \u02dcu(d))\u22a4. Without loss of generality, consider\n            since \u02dc                                          \u03bbmax + \u03b4 = \u03b3, we have \u2225W/\u03b3\u2225op \u2264          1. Also,\n            \u2225\u02c6    u(1) is a unit vector, we have \u2225S\u2225F = \u2225\u02dc      u(1) \u22252 = 1. Finally, for any \u02c6  B such that\n             B\u2225op \u2264   1, we have\n                       \u27e8S, W \u2212   \u02c6\n                                 B\u27e9  = (\u02dc\n                                        u(1) )\u22a4W\u02dc  u(1) \u2212 (\u02dc\n                                                           u(1) )\u22a4 \u02c6\n                                                                  B\u02dcu(1) \u2265  \u02dc\nThis completes the proof.                                                   \u03bbmax \u2212   1 = \u03b3 \u2212  1 \u2212  \u03b4.\n                                                      41", "md": "Combining (83) and (84), we get\n\n$$\n\\frac{1}{\\lambda_1 - \\hat{\\lambda}_d} \\leq \\lambda_1(W) - \\lambda_d(W) \\leq 2(\\hat{\\lambda}_1 - \\hat{\\lambda}_d)\n$$\n\nBy plugging the above inequality back into (83) and (84), we further have\n\n$$\n\\begin{align*}\n\\lambda_1(W) &\\leq \\hat{\\lambda}_1 + 1 \\\\\n\\lambda_d(W) &\\geq \\hat{\\lambda}_d - 1 \\\\\n4(\\lambda_1(W) - \\lambda_d(W)) &\\leq 2(\\hat{\\lambda}_1 - \\hat{\\lambda}_d) \\tag{85} \\\\\n\\end{align*}\n$$\n\nLet $\\hat{\\lambda}_{\\text{max}} = \\max\\{\\hat{\\lambda}_1, -\\hat{\\lambda}_d\\}$. By (85) and (86), we can further bound the eigenvalues of W by\n\n$$\n\\lambda_1(W) \\leq 2\\hat{\\lambda}_{\\text{max}} + 1 \\quad \\text{and} \\quad \\lambda_d(W) \\geq -2\\hat{\\lambda}_{\\text{max}}\n$$\n\nHence, we can see that $\\|W\\|_{\\text{op}} = \\max\\{\\lambda_1(W), -\\lambda_d(W)\\} \\leq 2\\hat{\\lambda}_{\\text{max}}$. Now we distinguish three cases.\n\n(a) If $\\hat{\\lambda}_{\\text{max}} \\leq \\frac{1}{2}$, then we are in Case I and the ExtEvec oracle outputs $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$. In this case, we indeed have $\\|W\\|_{\\text{op}} \\leq \\gamma \\leq 1$.\n\n(b) If $\\hat{\\lambda}_{\\text{max}} \\geq 2$, then we are in Case II. In addition, if $\\hat{\\lambda}_1 \\geq -\\hat{\\lambda}_d$, then the ExtEvec oracle returns $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$ and $S = 3u(1)(u(1))^T$. Similarly, if $-\\hat{\\lambda}_d > \\hat{\\lambda}_1$, then the ExtEvec oracle returns $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$ and $S = -3u(d)(u(d))^T$. Without loss of generality, consider the case where $\\hat{\\lambda}_1 \\geq -\\hat{\\lambda}_d$. Since $\\|W\\|_{\\text{op}} \\leq 2\\hat{\\lambda}_{\\text{max}} = \\gamma$, we have $\\|W/\\gamma\\|_{\\text{op}} \\leq 1$. Also, since $u_1$ is a unit vector, we have $\\|S\\|_F = 3\\|u(1)\\|^2 = 3$. Finally, for any $\\hat{B}$ such that $\\|\\hat{B}\\|_{\\text{op}} \\leq 1$, we have\n\n$$\n\\langle S, W - \\hat{B} \\rangle = 3(u(1))^TWu(1) - 3(u(1))^T\\hat{B}u(1) \\geq 3\\hat{\\lambda}_{\\text{max}} - 3 \\geq 2\\hat{\\lambda}_{\\text{max}} - 1 = \\gamma - 1\n$$\n\nwhere we used the fact that $\\hat{\\lambda}_{\\text{max}} \\geq 2$ in the last inequality.\n\n(c) If $\\frac{1}{2} < \\hat{\\lambda}_{\\text{max}} < 2$, we continue to run the Lanczos method for a total number of $\\frac{1}{4\\epsilon^{-1/2}\\log 11d}$ iterations, where $\\epsilon = \\frac{1}{8\\delta}$. Thus, by Proposition E.2, with probability at least $1 - \\frac{q}{2}$ we have\n\n$$\n\\begin{align*}\n\\tilde{\\lambda}_1 &\\geq \\langle W\\tilde{u}(1), \\tilde{u}(1) \\rangle \\geq \\lambda_1(W) - 1 \\\\\n\\tilde{\\lambda}_d &\\leq \\langle W\\tilde{u}(d), \\tilde{u}(d) \\rangle \\leq \\lambda_d(W) + 1 + 8\\delta(\\lambda_1(W) - \\lambda_d(W)) \\tag{88} \\\\\n\\end{align*}\n$$\n\nLet $\\tilde{\\lambda}_{\\text{max}} = \\max\\{\\tilde{\\lambda}_1, -\\tilde{\\lambda}_d}$. Since we have $\\lambda_1(W) \\leq 2\\hat{\\lambda}_{\\text{max}} \\leq 4$ and $\\lambda_d(W) \\geq -2\\hat{\\lambda}_{\\text{max}} \\geq -4$, the above implies that $\\tilde{\\lambda}_1 \\geq \\lambda_1(W) - \\delta$ and $\\tilde{\\lambda}_d \\leq \\lambda_d(W) + \\delta$. Hence, we can see that $\\|W\\|_{\\text{op}} = \\max\\{\\lambda_1(W), -\\lambda_d(W)\\} \\leq \\hat{\\lambda}_{\\text{max}} + \\delta$. We further consider two subcases.\n\n(c1) If $\\tilde{\\lambda}_{\\text{max}} \\leq 1 - \\delta$, then we are in Case I and the ExtEvec oracle outputs $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = 0$. In this case, we indeed have $\\|W\\|_{\\text{op}} \\leq \\gamma \\leq 1$.\n\n(c2) If $\\tilde{\\lambda}_{\\text{max}} > 1 - \\delta$, then we are in Case II. In addition, if $\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d$, then the ExtEvec oracle returns $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = \\tilde{u}(1)(\\tilde{u}(1))^T$. Similarly, if $-\\tilde{\\lambda}_d > \\tilde{\\lambda}_1$, then the ExtEvec oracle returns $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = -\\tilde{u}(d)(\\tilde{u}(d))^T$. Without loss of generality, consider the case where $\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d$. Since $\\|W\\|_{\\text{op}} \\leq \\tilde{\\lambda}_{\\text{max}} + \\delta = \\gamma$, we have $\\|W/\\gamma\\|_{\\text{op}} \\leq 1$. Also, since $\\tilde{u}(1)$ is a unit vector, we have $\\|S\\|_F = \\|\\tilde{u}(1)\\|^2 = 1$. Finally, for any $\\hat{B}$ such that $\\|\\hat{B}\\|_{\\text{op}} \\leq 1$, we have\n\n$$\n\\langle S, W - \\hat{B} \\rangle = (\\tilde{u}(1))^TW\\tilde{u}(1) - (\\tilde{u}(1))^T\\hat{B}\\tilde{u}(1) \\geq \\tilde{\\lambda}_{\\text{max}} - 1 = \\gamma - 1 - \\delta\n$$\n\nThis completes the proof.", "images": [], "items": [{"type": "text", "value": "Combining (83) and (84), we get\n\n$$\n\\frac{1}{\\lambda_1 - \\hat{\\lambda}_d} \\leq \\lambda_1(W) - \\lambda_d(W) \\leq 2(\\hat{\\lambda}_1 - \\hat{\\lambda}_d)\n$$\n\nBy plugging the above inequality back into (83) and (84), we further have\n\n$$\n\\begin{align*}\n\\lambda_1(W) &\\leq \\hat{\\lambda}_1 + 1 \\\\\n\\lambda_d(W) &\\geq \\hat{\\lambda}_d - 1 \\\\\n4(\\lambda_1(W) - \\lambda_d(W)) &\\leq 2(\\hat{\\lambda}_1 - \\hat{\\lambda}_d) \\tag{85} \\\\\n\\end{align*}\n$$\n\nLet $\\hat{\\lambda}_{\\text{max}} = \\max\\{\\hat{\\lambda}_1, -\\hat{\\lambda}_d\\}$. By (85) and (86), we can further bound the eigenvalues of W by\n\n$$\n\\lambda_1(W) \\leq 2\\hat{\\lambda}_{\\text{max}} + 1 \\quad \\text{and} \\quad \\lambda_d(W) \\geq -2\\hat{\\lambda}_{\\text{max}}\n$$\n\nHence, we can see that $\\|W\\|_{\\text{op}} = \\max\\{\\lambda_1(W), -\\lambda_d(W)\\} \\leq 2\\hat{\\lambda}_{\\text{max}}$. Now we distinguish three cases.\n\n(a) If $\\hat{\\lambda}_{\\text{max}} \\leq \\frac{1}{2}$, then we are in Case I and the ExtEvec oracle outputs $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$. In this case, we indeed have $\\|W\\|_{\\text{op}} \\leq \\gamma \\leq 1$.\n\n(b) If $\\hat{\\lambda}_{\\text{max}} \\geq 2$, then we are in Case II. In addition, if $\\hat{\\lambda}_1 \\geq -\\hat{\\lambda}_d$, then the ExtEvec oracle returns $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$ and $S = 3u(1)(u(1))^T$. Similarly, if $-\\hat{\\lambda}_d > \\hat{\\lambda}_1$, then the ExtEvec oracle returns $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$ and $S = -3u(d)(u(d))^T$. Without loss of generality, consider the case where $\\hat{\\lambda}_1 \\geq -\\hat{\\lambda}_d$. Since $\\|W\\|_{\\text{op}} \\leq 2\\hat{\\lambda}_{\\text{max}} = \\gamma$, we have $\\|W/\\gamma\\|_{\\text{op}} \\leq 1$. Also, since $u_1$ is a unit vector, we have $\\|S\\|_F = 3\\|u(1)\\|^2 = 3$. Finally, for any $\\hat{B}$ such that $\\|\\hat{B}\\|_{\\text{op}} \\leq 1$, we have\n\n$$\n\\langle S, W - \\hat{B} \\rangle = 3(u(1))^TWu(1) - 3(u(1))^T\\hat{B}u(1) \\geq 3\\hat{\\lambda}_{\\text{max}} - 3 \\geq 2\\hat{\\lambda}_{\\text{max}} - 1 = \\gamma - 1\n$$\n\nwhere we used the fact that $\\hat{\\lambda}_{\\text{max}} \\geq 2$ in the last inequality.\n\n(c) If $\\frac{1}{2} < \\hat{\\lambda}_{\\text{max}} < 2$, we continue to run the Lanczos method for a total number of $\\frac{1}{4\\epsilon^{-1/2}\\log 11d}$ iterations, where $\\epsilon = \\frac{1}{8\\delta}$. Thus, by Proposition E.2, with probability at least $1 - \\frac{q}{2}$ we have\n\n$$\n\\begin{align*}\n\\tilde{\\lambda}_1 &\\geq \\langle W\\tilde{u}(1), \\tilde{u}(1) \\rangle \\geq \\lambda_1(W) - 1 \\\\\n\\tilde{\\lambda}_d &\\leq \\langle W\\tilde{u}(d), \\tilde{u}(d) \\rangle \\leq \\lambda_d(W) + 1 + 8\\delta(\\lambda_1(W) - \\lambda_d(W)) \\tag{88} \\\\\n\\end{align*}\n$$\n\nLet $\\tilde{\\lambda}_{\\text{max}} = \\max\\{\\tilde{\\lambda}_1, -\\tilde{\\lambda}_d}$. Since we have $\\lambda_1(W) \\leq 2\\hat{\\lambda}_{\\text{max}} \\leq 4$ and $\\lambda_d(W) \\geq -2\\hat{\\lambda}_{\\text{max}} \\geq -4$, the above implies that $\\tilde{\\lambda}_1 \\geq \\lambda_1(W) - \\delta$ and $\\tilde{\\lambda}_d \\leq \\lambda_d(W) + \\delta$. Hence, we can see that $\\|W\\|_{\\text{op}} = \\max\\{\\lambda_1(W), -\\lambda_d(W)\\} \\leq \\hat{\\lambda}_{\\text{max}} + \\delta$. We further consider two subcases.\n\n(c1) If $\\tilde{\\lambda}_{\\text{max}} \\leq 1 - \\delta$, then we are in Case I and the ExtEvec oracle outputs $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = 0$. In this case, we indeed have $\\|W\\|_{\\text{op}} \\leq \\gamma \\leq 1$.\n\n(c2) If $\\tilde{\\lambda}_{\\text{max}} > 1 - \\delta$, then we are in Case II. In addition, if $\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d$, then the ExtEvec oracle returns $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = \\tilde{u}(1)(\\tilde{u}(1))^T$. Similarly, if $-\\tilde{\\lambda}_d > \\tilde{\\lambda}_1$, then the ExtEvec oracle returns $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = -\\tilde{u}(d)(\\tilde{u}(d))^T$. Without loss of generality, consider the case where $\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d$. Since $\\|W\\|_{\\text{op}} \\leq \\tilde{\\lambda}_{\\text{max}} + \\delta = \\gamma$, we have $\\|W/\\gamma\\|_{\\text{op}} \\leq 1$. Also, since $\\tilde{u}(1)$ is a unit vector, we have $\\|S\\|_F = \\|\\tilde{u}(1)\\|^2 = 1$. Finally, for any $\\hat{B}$ such that $\\|\\hat{B}\\|_{\\text{op}} \\leq 1$, we have\n\n$$\n\\langle S, W - \\hat{B} \\rangle = (\\tilde{u}(1))^TW\\tilde{u}(1) - (\\tilde{u}(1))^T\\hat{B}\\tilde{u}(1) \\geq \\tilde{\\lambda}_{\\text{max}} - 1 = \\gamma - 1 - \\delta\n$$\n\nThis completes the proof.", "md": "Combining (83) and (84), we get\n\n$$\n\\frac{1}{\\lambda_1 - \\hat{\\lambda}_d} \\leq \\lambda_1(W) - \\lambda_d(W) \\leq 2(\\hat{\\lambda}_1 - \\hat{\\lambda}_d)\n$$\n\nBy plugging the above inequality back into (83) and (84), we further have\n\n$$\n\\begin{align*}\n\\lambda_1(W) &\\leq \\hat{\\lambda}_1 + 1 \\\\\n\\lambda_d(W) &\\geq \\hat{\\lambda}_d - 1 \\\\\n4(\\lambda_1(W) - \\lambda_d(W)) &\\leq 2(\\hat{\\lambda}_1 - \\hat{\\lambda}_d) \\tag{85} \\\\\n\\end{align*}\n$$\n\nLet $\\hat{\\lambda}_{\\text{max}} = \\max\\{\\hat{\\lambda}_1, -\\hat{\\lambda}_d\\}$. By (85) and (86), we can further bound the eigenvalues of W by\n\n$$\n\\lambda_1(W) \\leq 2\\hat{\\lambda}_{\\text{max}} + 1 \\quad \\text{and} \\quad \\lambda_d(W) \\geq -2\\hat{\\lambda}_{\\text{max}}\n$$\n\nHence, we can see that $\\|W\\|_{\\text{op}} = \\max\\{\\lambda_1(W), -\\lambda_d(W)\\} \\leq 2\\hat{\\lambda}_{\\text{max}}$. Now we distinguish three cases.\n\n(a) If $\\hat{\\lambda}_{\\text{max}} \\leq \\frac{1}{2}$, then we are in Case I and the ExtEvec oracle outputs $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$. In this case, we indeed have $\\|W\\|_{\\text{op}} \\leq \\gamma \\leq 1$.\n\n(b) If $\\hat{\\lambda}_{\\text{max}} \\geq 2$, then we are in Case II. In addition, if $\\hat{\\lambda}_1 \\geq -\\hat{\\lambda}_d$, then the ExtEvec oracle returns $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$ and $S = 3u(1)(u(1))^T$. Similarly, if $-\\hat{\\lambda}_d > \\hat{\\lambda}_1$, then the ExtEvec oracle returns $\\gamma = 2\\hat{\\lambda}_{\\text{max}}$ and $S = -3u(d)(u(d))^T$. Without loss of generality, consider the case where $\\hat{\\lambda}_1 \\geq -\\hat{\\lambda}_d$. Since $\\|W\\|_{\\text{op}} \\leq 2\\hat{\\lambda}_{\\text{max}} = \\gamma$, we have $\\|W/\\gamma\\|_{\\text{op}} \\leq 1$. Also, since $u_1$ is a unit vector, we have $\\|S\\|_F = 3\\|u(1)\\|^2 = 3$. Finally, for any $\\hat{B}$ such that $\\|\\hat{B}\\|_{\\text{op}} \\leq 1$, we have\n\n$$\n\\langle S, W - \\hat{B} \\rangle = 3(u(1))^TWu(1) - 3(u(1))^T\\hat{B}u(1) \\geq 3\\hat{\\lambda}_{\\text{max}} - 3 \\geq 2\\hat{\\lambda}_{\\text{max}} - 1 = \\gamma - 1\n$$\n\nwhere we used the fact that $\\hat{\\lambda}_{\\text{max}} \\geq 2$ in the last inequality.\n\n(c) If $\\frac{1}{2} < \\hat{\\lambda}_{\\text{max}} < 2$, we continue to run the Lanczos method for a total number of $\\frac{1}{4\\epsilon^{-1/2}\\log 11d}$ iterations, where $\\epsilon = \\frac{1}{8\\delta}$. Thus, by Proposition E.2, with probability at least $1 - \\frac{q}{2}$ we have\n\n$$\n\\begin{align*}\n\\tilde{\\lambda}_1 &\\geq \\langle W\\tilde{u}(1), \\tilde{u}(1) \\rangle \\geq \\lambda_1(W) - 1 \\\\\n\\tilde{\\lambda}_d &\\leq \\langle W\\tilde{u}(d), \\tilde{u}(d) \\rangle \\leq \\lambda_d(W) + 1 + 8\\delta(\\lambda_1(W) - \\lambda_d(W)) \\tag{88} \\\\\n\\end{align*}\n$$\n\nLet $\\tilde{\\lambda}_{\\text{max}} = \\max\\{\\tilde{\\lambda}_1, -\\tilde{\\lambda}_d}$. Since we have $\\lambda_1(W) \\leq 2\\hat{\\lambda}_{\\text{max}} \\leq 4$ and $\\lambda_d(W) \\geq -2\\hat{\\lambda}_{\\text{max}} \\geq -4$, the above implies that $\\tilde{\\lambda}_1 \\geq \\lambda_1(W) - \\delta$ and $\\tilde{\\lambda}_d \\leq \\lambda_d(W) + \\delta$. Hence, we can see that $\\|W\\|_{\\text{op}} = \\max\\{\\lambda_1(W), -\\lambda_d(W)\\} \\leq \\hat{\\lambda}_{\\text{max}} + \\delta$. We further consider two subcases.\n\n(c1) If $\\tilde{\\lambda}_{\\text{max}} \\leq 1 - \\delta$, then we are in Case I and the ExtEvec oracle outputs $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = 0$. In this case, we indeed have $\\|W\\|_{\\text{op}} \\leq \\gamma \\leq 1$.\n\n(c2) If $\\tilde{\\lambda}_{\\text{max}} > 1 - \\delta$, then we are in Case II. In addition, if $\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d$, then the ExtEvec oracle returns $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = \\tilde{u}(1)(\\tilde{u}(1))^T$. Similarly, if $-\\tilde{\\lambda}_d > \\tilde{\\lambda}_1$, then the ExtEvec oracle returns $\\gamma = \\tilde{\\lambda}_{\\text{max}} + \\delta$ and $S = -\\tilde{u}(d)(\\tilde{u}(d))^T$. Without loss of generality, consider the case where $\\tilde{\\lambda}_1 \\geq -\\tilde{\\lambda}_d$. Since $\\|W\\|_{\\text{op}} \\leq \\tilde{\\lambda}_{\\text{max}} + \\delta = \\gamma$, we have $\\|W/\\gamma\\|_{\\text{op}} \\leq 1$. Also, since $\\tilde{u}(1)$ is a unit vector, we have $\\|S\\|_F = \\|\\tilde{u}(1)\\|^2 = 1$. Finally, for any $\\hat{B}$ such that $\\|\\hat{B}\\|_{\\text{op}} \\leq 1$, we have\n\n$$\n\\langle S, W - \\hat{B} \\rangle = (\\tilde{u}(1))^TW\\tilde{u}(1) - (\\tilde{u}(1))^T\\hat{B}\\tilde{u}(1) \\geq \\tilde{\\lambda}_{\\text{max}} - 1 = \\gamma - 1 - \\delta\n$$\n\nThis completes the proof."}]}, {"page": 42, "text": "E.3        Proof of Theorem 4.2\nWe divide the proof of Theorem 4.2 into the following three lemmas.\nLemma E.4. If we run Algorithm 1 as specified in Theorem 4.1 for N iterations, then the total\nnumber of line search steps can be bounded by 2N + log                                      1/\u03b2(\u03c30L1/\u03b12). As a corollary, the total\nnumber of gradient queries is bounded by 3N\u03f5 + log                               1/\u03b2( \u03c30L1\n                                                                                         \u03b12 ).\nProof. In our backtracking scheme, the number of steps in each iteration is given by log                                              1/\u03b2(\u03b7k/\u02c6   \u03b7k)+1.\nAlso note that \u03b7k+1 \u2264               \u02c6\n                                    \u03b7k/\u03b2 for all k \u2265          0. Thus, we have\n                            N\u22121      log  1/\u03b2   \u03b7k  + 1      = N + log        1/\u03b2  \u03c30   +   N\u22122   log1/\u03b2     \u03b7k+1\n                                                \u02c6\n                                                \u03b7k                                  \u02c6\n                                                                                   \u03b70                        \u02c6\n                            k=0                                                             k=0              \u03b7k+1\n                                                                                   \u03c30       N\u22122                  \u02c6\n                                                             \u2264   N + log1/\u03b2         \u02c6   +            log1/\u03b2      \u03b7k     + 1\n                                                                                   \u03b70                           \u02c6\n                                                                                            k=0                \u03b7k+1\n                                                             \u2264   2N \u2212      1 + log1/\u03b2      \u02c6 \u03c30\n                                                                                           \u03b7N\u22121\nFurthermore, since \u02c6           \u03b7k \u2265    \u03b12\u03b2/L1 for all k \u2265             0, we arrive at the conclusion.\nLemma E.5. The total number of matrix-vector product evaluations in the LinearSolver oracle is\nbounded by N\u03f5 + C11              \u221a  \u03c30L1 + C12           L1\u2225z0\u2212x\u2217\u22252      , where C11 and C12 are absolute constants.\n                                                                2\u03f5\nProof. Our proof loosely follows the strategy in [CHJJS22]. We first bound the number of steps\nrequired by Subroutine 3 before it terminates.\n                                                                                                                                \u03b1+1\nLemma E.6. Suppose A \u2ab0                        I. Then Subroutine 3 terminates after at most                                       \u03b1 \u03bbmax(A) \u2212          1\niterations.\nProof. Note that \u2225sk\u22252 \u2265                 \u2225s\u2217\u22252\u2212\u2225sk\u2212s\u2217\u22252. Also, since A \u2ab0                       I, we have \u2225sk\u2212s\u2217\u22252 \u2264               \u2225A(sk\u2212s\u2217)\u22252 =\n\u2225rk\u22252. Therefore, we have\n                 \u2225rk\u22252 \u2264      \u03b1\u2225sk\u22252          \u21d0       \u2225rk\u22252 \u2264       \u03b1\u2225s\u2217\u22252 \u2212       \u03b1\u2225rk\u22252         \u21d0       \u2225rk\u22252 \u2264       \u03b1 +\u03b11\u2225s\u2217\u22252.\nBy using Lemma E.1, we only need k \u2265                             \u03b1+1  \u03b1 \u03bbmax(A) \u2212           1 to achieve \u2225Ask \u2212              b\u2225   \u2264  \u03b1\u2225sk\u2225.\nMoreover, when the step size is smaller enough, we can show that Subroutine 3 will terminate in\none iteration.\n                                                                       \u03b1\nLemma E.7. Let A = I + \u03b7B. When \u03b7 \u2264                                  2L1 , Algorithm 3 terminates in one iteration.\nProof. From the update rule of Subroutine 3, we can compute that s1 = b\u22a4Ab                                          \u2225Ab\u22252  2 b, which implies\n                                                             \u2225A1/2b\u22252                         \u2225b\u2225               \u2225b\u2225\n                                 \u2225s1\u2225    = \u2225b\u2225     \u00b7  (A1/2b)\u22a4A(A1/2b) \u2265                  \u03bbmax(A) \u2265          1 + \u03b7L1     .\n                                                                            42", "md": "E.3 Proof of Theorem 4.2\n\nWe divide the proof of Theorem 4.2 into the following three lemmas.\n\nLemma E.4. If we run Algorithm 1 as specified in Theorem 4.1 for N iterations, then the total number of line search steps can be bounded by $$2N + \\log \\frac{1}{\\beta}(\\sigma_0L^{1/\\alpha^2}).$$ As a corollary, the total number of gradient queries is bounded by $$3N\\epsilon + \\log \\frac{1}{\\beta}(\\sigma_0L^{1/\\alpha^2}).$$\n\nProof. In our backtracking scheme, the number of steps in each iteration is given by $$\\log \\frac{1}{\\beta}(\\eta_k/\\hat{\\eta}_k)+1.$$ Also note that $$\\eta_{k+1} \\leq \\hat{\\eta}_k/\\beta$$ for all $$k \\geq 0$$. Thus, we have\n\n$$\n\\begin{aligned}\n\\sum_{k=0}^{N-1} \\log \\frac{1}{\\beta} \\frac{\\eta_k}{\\hat{\\eta}_k} + 1 &= N + \\log \\frac{1}{\\beta} \\sigma_0 + \\sum_{k=0}^{N-2} \\log \\frac{1}{\\beta} \\frac{\\eta_{k+1}}{\\hat{\\eta}_k} \\\\\n&\\leq N + \\log \\frac{1}{\\beta} \\hat{\\eta}_0 + \\sum_{k=0}^{N-2} \\log \\frac{1}{\\beta} \\frac{\\eta_k}{\\hat{\\eta}_{k+1}} + 1 \\\\\n&\\leq 2N - 1 + \\log \\frac{1}{\\beta} \\frac{\\hat{\\sigma}_0}{\\eta_{N-1}}\n\\end{aligned}\n$$\nFurthermore, since $$\\hat{\\eta}_k \\geq \\alpha^2\\beta/L_1$$ for all $$k \\geq 0$$, we arrive at the conclusion.\n\nLemma E.5. The total number of matrix-vector product evaluations in the LinearSolver oracle is bounded by $$N\\epsilon + C_{11} \\sqrt{\\sigma_0L_1} + C_{12} L_1 \\|z_0-x^*\\|_2$$, where $$C_{11}$$ and $$C_{12}$$ are absolute constants.\n\nProof. Our proof loosely follows the strategy in [CHJJS22]. We first bound the number of steps required by Subroutine 3 before it terminates.\n\nLemma E.6. Suppose $$A \\succeq I$$. Then Subroutine 3 terminates after at most $$\\alpha \\lambda_{\\text{max}}(A) - 1$$ iterations.\n\nProof. Note that $$\\|s_k\\|_2 \\geq \\|s^*\\|_2 - \\|s_k-s^*\\|_2$$. Also, since $$A \\succeq I$$, we have $$\\|s_k-s^*\\|_2 \\leq \\|A(s_k-s^*)\\|_2 = \\|r_k\\|_2$$. Therefore, we have\n\n$$\n\\begin{aligned}\n\\|r_k\\|_2 &\\leq \\alpha \\|s_k\\|_2 \\Leftrightarrow \\|r_k\\|_2 &\\leq \\alpha \\|s^*\\|_2 - \\alpha \\|r_k\\|_2 \\Leftrightarrow \\|r_k\\|_2 &\\leq \\alpha + \\alpha^2 \\|s^*\\|_2.\n\\end{aligned}\n$$\nBy using Lemma E.1, we only need $$k \\geq \\frac{\\alpha+1}{\\alpha \\lambda_{\\text{max}}(A) - 1}$$ to achieve $$\\|As_k - b\\| \\leq \\alpha \\|s_k$$. Moreover, when the step size is small enough, we can show that Subroutine 3 will terminate in one iteration.\n\nLemma E.7. Let $$A = I + \\eta B$$. When $$\\eta \\leq \\frac{1}{2L_1}$$, Algorithm 3 terminates in one iteration.\n\nProof. From the update rule of Subroutine 3, we can compute that $$s_1 = b^TAb/\\|Ab\\|_2^2 b$$, which implies\n\n$$\n\\|A^{1/2}b\\|_2 \\geq \\|b\\| \\cdot (A^{1/2}b)^T A (A^{1/2}b) \\geq \\lambda_{\\text{max}}(A) \\geq 1 + \\eta L_1.\n$$", "images": [], "items": [{"type": "text", "value": "E.3 Proof of Theorem 4.2\n\nWe divide the proof of Theorem 4.2 into the following three lemmas.\n\nLemma E.4. If we run Algorithm 1 as specified in Theorem 4.1 for N iterations, then the total number of line search steps can be bounded by $$2N + \\log \\frac{1}{\\beta}(\\sigma_0L^{1/\\alpha^2}).$$ As a corollary, the total number of gradient queries is bounded by $$3N\\epsilon + \\log \\frac{1}{\\beta}(\\sigma_0L^{1/\\alpha^2}).$$\n\nProof. In our backtracking scheme, the number of steps in each iteration is given by $$\\log \\frac{1}{\\beta}(\\eta_k/\\hat{\\eta}_k)+1.$$ Also note that $$\\eta_{k+1} \\leq \\hat{\\eta}_k/\\beta$$ for all $$k \\geq 0$$. Thus, we have\n\n$$\n\\begin{aligned}\n\\sum_{k=0}^{N-1} \\log \\frac{1}{\\beta} \\frac{\\eta_k}{\\hat{\\eta}_k} + 1 &= N + \\log \\frac{1}{\\beta} \\sigma_0 + \\sum_{k=0}^{N-2} \\log \\frac{1}{\\beta} \\frac{\\eta_{k+1}}{\\hat{\\eta}_k} \\\\\n&\\leq N + \\log \\frac{1}{\\beta} \\hat{\\eta}_0 + \\sum_{k=0}^{N-2} \\log \\frac{1}{\\beta} \\frac{\\eta_k}{\\hat{\\eta}_{k+1}} + 1 \\\\\n&\\leq 2N - 1 + \\log \\frac{1}{\\beta} \\frac{\\hat{\\sigma}_0}{\\eta_{N-1}}\n\\end{aligned}\n$$\nFurthermore, since $$\\hat{\\eta}_k \\geq \\alpha^2\\beta/L_1$$ for all $$k \\geq 0$$, we arrive at the conclusion.\n\nLemma E.5. The total number of matrix-vector product evaluations in the LinearSolver oracle is bounded by $$N\\epsilon + C_{11} \\sqrt{\\sigma_0L_1} + C_{12} L_1 \\|z_0-x^*\\|_2$$, where $$C_{11}$$ and $$C_{12}$$ are absolute constants.\n\nProof. Our proof loosely follows the strategy in [CHJJS22]. We first bound the number of steps required by Subroutine 3 before it terminates.\n\nLemma E.6. Suppose $$A \\succeq I$$. Then Subroutine 3 terminates after at most $$\\alpha \\lambda_{\\text{max}}(A) - 1$$ iterations.\n\nProof. Note that $$\\|s_k\\|_2 \\geq \\|s^*\\|_2 - \\|s_k-s^*\\|_2$$. Also, since $$A \\succeq I$$, we have $$\\|s_k-s^*\\|_2 \\leq \\|A(s_k-s^*)\\|_2 = \\|r_k\\|_2$$. Therefore, we have\n\n$$\n\\begin{aligned}\n\\|r_k\\|_2 &\\leq \\alpha \\|s_k\\|_2 \\Leftrightarrow \\|r_k\\|_2 &\\leq \\alpha \\|s^*\\|_2 - \\alpha \\|r_k\\|_2 \\Leftrightarrow \\|r_k\\|_2 &\\leq \\alpha + \\alpha^2 \\|s^*\\|_2.\n\\end{aligned}\n$$\nBy using Lemma E.1, we only need $$k \\geq \\frac{\\alpha+1}{\\alpha \\lambda_{\\text{max}}(A) - 1}$$ to achieve $$\\|As_k - b\\| \\leq \\alpha \\|s_k$$. Moreover, when the step size is small enough, we can show that Subroutine 3 will terminate in one iteration.\n\nLemma E.7. Let $$A = I + \\eta B$$. When $$\\eta \\leq \\frac{1}{2L_1}$$, Algorithm 3 terminates in one iteration.\n\nProof. From the update rule of Subroutine 3, we can compute that $$s_1 = b^TAb/\\|Ab\\|_2^2 b$$, which implies\n\n$$\n\\|A^{1/2}b\\|_2 \\geq \\|b\\| \\cdot (A^{1/2}b)^T A (A^{1/2}b) \\geq \\lambda_{\\text{max}}(A) \\geq 1 + \\eta L_1.\n$$", "md": "E.3 Proof of Theorem 4.2\n\nWe divide the proof of Theorem 4.2 into the following three lemmas.\n\nLemma E.4. If we run Algorithm 1 as specified in Theorem 4.1 for N iterations, then the total number of line search steps can be bounded by $$2N + \\log \\frac{1}{\\beta}(\\sigma_0L^{1/\\alpha^2}).$$ As a corollary, the total number of gradient queries is bounded by $$3N\\epsilon + \\log \\frac{1}{\\beta}(\\sigma_0L^{1/\\alpha^2}).$$\n\nProof. In our backtracking scheme, the number of steps in each iteration is given by $$\\log \\frac{1}{\\beta}(\\eta_k/\\hat{\\eta}_k)+1.$$ Also note that $$\\eta_{k+1} \\leq \\hat{\\eta}_k/\\beta$$ for all $$k \\geq 0$$. Thus, we have\n\n$$\n\\begin{aligned}\n\\sum_{k=0}^{N-1} \\log \\frac{1}{\\beta} \\frac{\\eta_k}{\\hat{\\eta}_k} + 1 &= N + \\log \\frac{1}{\\beta} \\sigma_0 + \\sum_{k=0}^{N-2} \\log \\frac{1}{\\beta} \\frac{\\eta_{k+1}}{\\hat{\\eta}_k} \\\\\n&\\leq N + \\log \\frac{1}{\\beta} \\hat{\\eta}_0 + \\sum_{k=0}^{N-2} \\log \\frac{1}{\\beta} \\frac{\\eta_k}{\\hat{\\eta}_{k+1}} + 1 \\\\\n&\\leq 2N - 1 + \\log \\frac{1}{\\beta} \\frac{\\hat{\\sigma}_0}{\\eta_{N-1}}\n\\end{aligned}\n$$\nFurthermore, since $$\\hat{\\eta}_k \\geq \\alpha^2\\beta/L_1$$ for all $$k \\geq 0$$, we arrive at the conclusion.\n\nLemma E.5. The total number of matrix-vector product evaluations in the LinearSolver oracle is bounded by $$N\\epsilon + C_{11} \\sqrt{\\sigma_0L_1} + C_{12} L_1 \\|z_0-x^*\\|_2$$, where $$C_{11}$$ and $$C_{12}$$ are absolute constants.\n\nProof. Our proof loosely follows the strategy in [CHJJS22]. We first bound the number of steps required by Subroutine 3 before it terminates.\n\nLemma E.6. Suppose $$A \\succeq I$$. Then Subroutine 3 terminates after at most $$\\alpha \\lambda_{\\text{max}}(A) - 1$$ iterations.\n\nProof. Note that $$\\|s_k\\|_2 \\geq \\|s^*\\|_2 - \\|s_k-s^*\\|_2$$. Also, since $$A \\succeq I$$, we have $$\\|s_k-s^*\\|_2 \\leq \\|A(s_k-s^*)\\|_2 = \\|r_k\\|_2$$. Therefore, we have\n\n$$\n\\begin{aligned}\n\\|r_k\\|_2 &\\leq \\alpha \\|s_k\\|_2 \\Leftrightarrow \\|r_k\\|_2 &\\leq \\alpha \\|s^*\\|_2 - \\alpha \\|r_k\\|_2 \\Leftrightarrow \\|r_k\\|_2 &\\leq \\alpha + \\alpha^2 \\|s^*\\|_2.\n\\end{aligned}\n$$\nBy using Lemma E.1, we only need $$k \\geq \\frac{\\alpha+1}{\\alpha \\lambda_{\\text{max}}(A) - 1}$$ to achieve $$\\|As_k - b\\| \\leq \\alpha \\|s_k$$. Moreover, when the step size is small enough, we can show that Subroutine 3 will terminate in one iteration.\n\nLemma E.7. Let $$A = I + \\eta B$$. When $$\\eta \\leq \\frac{1}{2L_1}$$, Algorithm 3 terminates in one iteration.\n\nProof. From the update rule of Subroutine 3, we can compute that $$s_1 = b^TAb/\\|Ab\\|_2^2 b$$, which implies\n\n$$\n\\|A^{1/2}b\\|_2 \\geq \\|b\\| \\cdot (A^{1/2}b)^T A (A^{1/2}b) \\geq \\lambda_{\\text{max}}(A) \\geq 1 + \\eta L_1.\n$$"}]}, {"page": 43, "text": "On the other hand, we also have\n                                   \u03b1           \u2225r1\u2225    \u2264   \u2225Ab \u2212     \u03b1 b\u2225   = \u03b7\u2225Bb\u2225        \u2264   \u03b7L1\u2225b\u2225.\nMoreover, when \u03b7 \u2264                2L1 , we have \u03b7L1 \u2264             1+\u03b7L1 , which implies that \u2225r1\u2225                  \u2264   \u03b1\u2225s1\u2225.\nNow we upper bound the total number of matrix-vector products in Algorithm 1. Note that at the\nk-th iteration, we use the LinearSolver oracle with A = I + \u03b7+Bk where \u03b7+ = \u03b7k\u03b2i. We can store                                               \u03b11\nthe vector Bkb at the beginning and reuse it to compute s1 when the step size \u03b7+ <                                                          2L1 . And\nwhen \u03b2i\u03b7       kL1 \u2265      \u03b11\n                           2 , it holds that\n                                                       1 + \u03b2i\u03b7     kL1 \u2264      \u03b11 + 2\u03b2i\u03b7kL1.\n                                                                                 \u03b11\nThus, at the k-th iteration, the number of matrix-vector products can be bounded by\n                                          MVk \u2264       1 +                          \u03b11 + 1     (1 + \u03b7k\u03b2iL1)\n                                                            i\u22650,\u03b7k\u03b2i\u2265      \u03b11         \u03b11\n                                                                          2L 1\n                                                  \u2264   1 +                       \u03b11 + 2        \u03b2i\u03b7  kL1\n                                                            i\u22650,\u03b7k\u03b2i\u2265      \u03b11      \u03b11\n                                                                          2L 1\nFurthermore, we can bound that                    \u2264   1 + \u03b11 + 2\u03b11      1 \u2212 1 \u221a\u03b2       \u03b7kL1.\n              N\u22121    \u221a\u03b7k \u2264      \u221a\u03c30 +      N\u22121    \u221a\u03b7k \u2264      \u221a\u03c30 +       \u221a1    N\u22122                             2(2 \u2212    \u221a\u03b2)\n                                                                            \u03b2            \u02c6                   \u221a\u03b2(1 \u2212       \u221a\u03b2)\n               k=0                          k=1                                k=0       \u03b7k \u2264    \u221a\u03c30 +                              AN\u22121\nNote that \u03f5 < f(xN\u22121) \u2212                 f(x\u2217) \u2264       \u2225z0\u2212x\u2217\u22252                                                           . Thus, we can bound\n                                                        2AN\u22121 . Hence, we have AN\u22121 \u2264                        \u2225z0\u2212x\u2217\u22252\n                                                                                                                 2\u03f5\nthe total number of matrix-vector product evaluations by\n                       N\u03f5\u22121                                        1                          \u221a2(2 \u2212     \u221a\u03b2)          L1\u2225z0 \u2212      x\u2217\u22252\n            MV =        k=0   MVk \u2264       N\u03f5 + \u03b11 + 2 \u03b11      1 \u2212   \u221a\u03b2           \u03c30L1 +          \u03b2(1 \u2212     \u221a\u03b2)                2\u03f5              ,\n                                       = N\u03f5 + C11           \u03c30L1 + C12            L1\u2225z0 \u2212  2\u03f5  x\u2217\u22252    ,\n                                                1                                 1     \u221a2(2\u2212\u221a\u03b2)\nwhere we define C11 = \u03b11+2             \u03b11    1\u2212\u221a\u03b2 and C12 = \u03b11+2         \u03b11    1\u2212\u221a\u03b2       \u03b2(1\u2212\u221a\u03b2).\nLemma E.8. The total number of matrix-vector product evaluations in the SEP oracle is bounded\nby O     N1.25(log N\u03f5)0.5 log           \u221a  dN\u03f5      .\n            \u03f5                               p\nProof. Note that we have Nt \u2264                          1           q2      2    in Subroutine 4, where \u03b4t = 1/(\u221at + 2 log(t + 2))\nand qt = p/(2.5(t + 1) log2(t + 1)). Thus, we have  4\u221a2\u03b4t log 44d   t + 1\n                              T\u22121            T\u22121   (t + 2)0.25 log0.5(t + 2)                      \u221a  44d(t + 1) log2(t + 1)\n                      N =     t=0   Nt \u2264     t=0                  2\u221a  2                log 2.5                  p                                    (89)\n                                         = O       N1.25       log N\u03f5 log      \u221a  dN\u03f5       .                                                        (90)\n                                                      \u03f5                            p\n                                                                            43", "md": "# Math Equations\n\nOn the other hand, we also have\n\n$$\n\\alpha \\|\\mathbf{r}_1\\| \\leq \\|Ab - \\alpha \\mathbf{b}\\| = \\eta\\|B\\mathbf{b}\\| \\leq \\eta L_1 \\|\\mathbf{b}\\|.\n$$\nMoreover, when $\\eta \\leq 2L_1$, we have $\\eta L_1 \\leq 1+\\eta L_1$, which implies that $\\|\\mathbf{r}_1\\| \\leq \\alpha\\|\\mathbf{s}_1\\|$.\n\nNow we upper bound the total number of matrix-vector products in Algorithm 1. Note that at the k-th iteration, we use the LinearSolver oracle with $A = I + \\eta+B_k$ where $\\eta+ = \\eta_k\\beta_i$. We can store $\\alpha_1$ the vector $B_k\\mathbf{b}$ at the beginning and reuse it to compute $\\mathbf{s}_1$ when the step size $\\eta+ < 2L_1$. And when $\\beta_i\\eta_kL_1 \\geq \\alpha_1/2$, it holds that $1 + \\beta_i\\eta_kL_1 \\leq \\alpha_1 + 2\\beta_i\\eta_kL_1$.\n\n$$\n\\begin{align*}\n\\text{MV}_k &\\leq 1 + \\sum_{i\\geq 0, \\eta_k\\beta_i\\geq \\alpha_1} \\left(1 + \\eta_k\\beta_iL_1\\right) \\\\\n&\\leq 1 + \\sum_{i\\geq 0, \\eta_k\\beta_i\\geq \\alpha_1} \\left(1 + 2\\alpha_1\\frac{1 - 1}{\\sqrt{\\beta}}\\eta_kL_1\\right) \\\\\n&\\leq 1 + \\alpha_1 + 2\\alpha_1\\frac{1 - 1}{\\sqrt{\\beta}}\\eta_kL_1.\n\\end{align*}\n$$\nFurthermore, we can bound that\n\n$$\n\\begin{align*}\n\\sum_{k=0}^{N-1} \\sqrt{\\eta_k} &\\leq \\sqrt{\\sigma_0} + \\sum_{k=1}^{N-2} \\sqrt{\\eta_k} \\\\\n&\\leq \\sqrt{\\sigma_0} + \\sqrt{1}^{N-2} \\frac{2(2 - \\sqrt{\\beta})}{\\sqrt{\\beta}(1 - \\sqrt{\\beta})} \\\\\n&\\leq \\sqrt{\\sigma_0} + \\frac{AN-1}{\\sqrt{\\beta}}.\n\\end{align*}\n$$\nNote that $\\epsilon < f(\\mathbf{x}_{N-1}) - f(\\mathbf{x}^*) \\leq \\|\\mathbf{z}_0-\\mathbf{x}^*\\|_2^2/2AN-1$. Hence, we have $AN-1 \\leq \\|\\mathbf{z}_0-\\mathbf{x}^*\\|_2^2/2\\epsilon$ the total number of matrix-vector product evaluations by\n\n$$\n\\begin{align*}\n\\text{MV} &= \\sum_{k=0}^{N\\epsilon-1} \\text{MV}_k \\\\\n&\\leq N\\epsilon + \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{2(2 - \\sqrt{\\beta})}}\\sigma_0L_1 + \\beta(1 - \\sqrt{\\beta})\\frac{2\\epsilon}{2} \\\\\n&= N\\epsilon + C_{11}\\sigma_0L_1 + C_{12}L_1\\|\\mathbf{z}_0 - 2\\epsilon\\mathbf{x}^*\\|_2^2,\n\\end{align*}\n$$\nwhere we define $C_{11} = \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{\\beta}}$ and $C_{12} = \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{\\beta}}\\beta(1 - \\sqrt{\\beta})$.\n\nLemma E.8. The total number of matrix-vector product evaluations in the SEP oracle is bounded by $O(N^{1.25}(\\log N\\epsilon)^{0.5}\\log\\sqrt{dN\\epsilon})$.\n\nProof. Note that we have $N_t \\leq \\frac{1}{\\sqrt{t} + 2\\log(t + 2)}$ in Subroutine 4, where $\\delta_t = 1/(\\sqrt{t} + 2\\log(t + 2))$ and $q_t = p/(2.5(t + 1)\\log^2(t + 1))$. Thus, we have\n\n$$\n\\begin{align*}\nN &= \\sum_{t=0}^{T-1} N_t \\\\\n&\\leq \\sum_{t=0}^{T-1} \\frac{4\\sqrt{2}\\delta_t\\log 44d(t + 1)}{(t + 2)^{0.25}\\log^{0.5}(t + 2)} \\\\\n&\\leq O(N^{1.25}\\log N\\epsilon\\log\\sqrt{dN\\epsilon}).\n\\end{align*}\n$$\n43", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "On the other hand, we also have\n\n$$\n\\alpha \\|\\mathbf{r}_1\\| \\leq \\|Ab - \\alpha \\mathbf{b}\\| = \\eta\\|B\\mathbf{b}\\| \\leq \\eta L_1 \\|\\mathbf{b}\\|.\n$$\nMoreover, when $\\eta \\leq 2L_1$, we have $\\eta L_1 \\leq 1+\\eta L_1$, which implies that $\\|\\mathbf{r}_1\\| \\leq \\alpha\\|\\mathbf{s}_1\\|$.\n\nNow we upper bound the total number of matrix-vector products in Algorithm 1. Note that at the k-th iteration, we use the LinearSolver oracle with $A = I + \\eta+B_k$ where $\\eta+ = \\eta_k\\beta_i$. We can store $\\alpha_1$ the vector $B_k\\mathbf{b}$ at the beginning and reuse it to compute $\\mathbf{s}_1$ when the step size $\\eta+ < 2L_1$. And when $\\beta_i\\eta_kL_1 \\geq \\alpha_1/2$, it holds that $1 + \\beta_i\\eta_kL_1 \\leq \\alpha_1 + 2\\beta_i\\eta_kL_1$.\n\n$$\n\\begin{align*}\n\\text{MV}_k &\\leq 1 + \\sum_{i\\geq 0, \\eta_k\\beta_i\\geq \\alpha_1} \\left(1 + \\eta_k\\beta_iL_1\\right) \\\\\n&\\leq 1 + \\sum_{i\\geq 0, \\eta_k\\beta_i\\geq \\alpha_1} \\left(1 + 2\\alpha_1\\frac{1 - 1}{\\sqrt{\\beta}}\\eta_kL_1\\right) \\\\\n&\\leq 1 + \\alpha_1 + 2\\alpha_1\\frac{1 - 1}{\\sqrt{\\beta}}\\eta_kL_1.\n\\end{align*}\n$$\nFurthermore, we can bound that\n\n$$\n\\begin{align*}\n\\sum_{k=0}^{N-1} \\sqrt{\\eta_k} &\\leq \\sqrt{\\sigma_0} + \\sum_{k=1}^{N-2} \\sqrt{\\eta_k} \\\\\n&\\leq \\sqrt{\\sigma_0} + \\sqrt{1}^{N-2} \\frac{2(2 - \\sqrt{\\beta})}{\\sqrt{\\beta}(1 - \\sqrt{\\beta})} \\\\\n&\\leq \\sqrt{\\sigma_0} + \\frac{AN-1}{\\sqrt{\\beta}}.\n\\end{align*}\n$$\nNote that $\\epsilon < f(\\mathbf{x}_{N-1}) - f(\\mathbf{x}^*) \\leq \\|\\mathbf{z}_0-\\mathbf{x}^*\\|_2^2/2AN-1$. Hence, we have $AN-1 \\leq \\|\\mathbf{z}_0-\\mathbf{x}^*\\|_2^2/2\\epsilon$ the total number of matrix-vector product evaluations by\n\n$$\n\\begin{align*}\n\\text{MV} &= \\sum_{k=0}^{N\\epsilon-1} \\text{MV}_k \\\\\n&\\leq N\\epsilon + \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{2(2 - \\sqrt{\\beta})}}\\sigma_0L_1 + \\beta(1 - \\sqrt{\\beta})\\frac{2\\epsilon}{2} \\\\\n&= N\\epsilon + C_{11}\\sigma_0L_1 + C_{12}L_1\\|\\mathbf{z}_0 - 2\\epsilon\\mathbf{x}^*\\|_2^2,\n\\end{align*}\n$$\nwhere we define $C_{11} = \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{\\beta}}$ and $C_{12} = \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{\\beta}}\\beta(1 - \\sqrt{\\beta})$.\n\nLemma E.8. The total number of matrix-vector product evaluations in the SEP oracle is bounded by $O(N^{1.25}(\\log N\\epsilon)^{0.5}\\log\\sqrt{dN\\epsilon})$.\n\nProof. Note that we have $N_t \\leq \\frac{1}{\\sqrt{t} + 2\\log(t + 2)}$ in Subroutine 4, where $\\delta_t = 1/(\\sqrt{t} + 2\\log(t + 2))$ and $q_t = p/(2.5(t + 1)\\log^2(t + 1))$. Thus, we have\n\n$$\n\\begin{align*}\nN &= \\sum_{t=0}^{T-1} N_t \\\\\n&\\leq \\sum_{t=0}^{T-1} \\frac{4\\sqrt{2}\\delta_t\\log 44d(t + 1)}{(t + 2)^{0.25}\\log^{0.5}(t + 2)} \\\\\n&\\leq O(N^{1.25}\\log N\\epsilon\\log\\sqrt{dN\\epsilon}).\n\\end{align*}\n$$\n43", "md": "On the other hand, we also have\n\n$$\n\\alpha \\|\\mathbf{r}_1\\| \\leq \\|Ab - \\alpha \\mathbf{b}\\| = \\eta\\|B\\mathbf{b}\\| \\leq \\eta L_1 \\|\\mathbf{b}\\|.\n$$\nMoreover, when $\\eta \\leq 2L_1$, we have $\\eta L_1 \\leq 1+\\eta L_1$, which implies that $\\|\\mathbf{r}_1\\| \\leq \\alpha\\|\\mathbf{s}_1\\|$.\n\nNow we upper bound the total number of matrix-vector products in Algorithm 1. Note that at the k-th iteration, we use the LinearSolver oracle with $A = I + \\eta+B_k$ where $\\eta+ = \\eta_k\\beta_i$. We can store $\\alpha_1$ the vector $B_k\\mathbf{b}$ at the beginning and reuse it to compute $\\mathbf{s}_1$ when the step size $\\eta+ < 2L_1$. And when $\\beta_i\\eta_kL_1 \\geq \\alpha_1/2$, it holds that $1 + \\beta_i\\eta_kL_1 \\leq \\alpha_1 + 2\\beta_i\\eta_kL_1$.\n\n$$\n\\begin{align*}\n\\text{MV}_k &\\leq 1 + \\sum_{i\\geq 0, \\eta_k\\beta_i\\geq \\alpha_1} \\left(1 + \\eta_k\\beta_iL_1\\right) \\\\\n&\\leq 1 + \\sum_{i\\geq 0, \\eta_k\\beta_i\\geq \\alpha_1} \\left(1 + 2\\alpha_1\\frac{1 - 1}{\\sqrt{\\beta}}\\eta_kL_1\\right) \\\\\n&\\leq 1 + \\alpha_1 + 2\\alpha_1\\frac{1 - 1}{\\sqrt{\\beta}}\\eta_kL_1.\n\\end{align*}\n$$\nFurthermore, we can bound that\n\n$$\n\\begin{align*}\n\\sum_{k=0}^{N-1} \\sqrt{\\eta_k} &\\leq \\sqrt{\\sigma_0} + \\sum_{k=1}^{N-2} \\sqrt{\\eta_k} \\\\\n&\\leq \\sqrt{\\sigma_0} + \\sqrt{1}^{N-2} \\frac{2(2 - \\sqrt{\\beta})}{\\sqrt{\\beta}(1 - \\sqrt{\\beta})} \\\\\n&\\leq \\sqrt{\\sigma_0} + \\frac{AN-1}{\\sqrt{\\beta}}.\n\\end{align*}\n$$\nNote that $\\epsilon < f(\\mathbf{x}_{N-1}) - f(\\mathbf{x}^*) \\leq \\|\\mathbf{z}_0-\\mathbf{x}^*\\|_2^2/2AN-1$. Hence, we have $AN-1 \\leq \\|\\mathbf{z}_0-\\mathbf{x}^*\\|_2^2/2\\epsilon$ the total number of matrix-vector product evaluations by\n\n$$\n\\begin{align*}\n\\text{MV} &= \\sum_{k=0}^{N\\epsilon-1} \\text{MV}_k \\\\\n&\\leq N\\epsilon + \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{2(2 - \\sqrt{\\beta})}}\\sigma_0L_1 + \\beta(1 - \\sqrt{\\beta})\\frac{2\\epsilon}{2} \\\\\n&= N\\epsilon + C_{11}\\sigma_0L_1 + C_{12}L_1\\|\\mathbf{z}_0 - 2\\epsilon\\mathbf{x}^*\\|_2^2,\n\\end{align*}\n$$\nwhere we define $C_{11} = \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{\\beta}}$ and $C_{12} = \\alpha_1 + 2\\alpha_1\\frac{1 - \\sqrt{\\beta}}{\\sqrt{\\beta}}\\beta(1 - \\sqrt{\\beta})$.\n\nLemma E.8. The total number of matrix-vector product evaluations in the SEP oracle is bounded by $O(N^{1.25}(\\log N\\epsilon)^{0.5}\\log\\sqrt{dN\\epsilon})$.\n\nProof. Note that we have $N_t \\leq \\frac{1}{\\sqrt{t} + 2\\log(t + 2)}$ in Subroutine 4, where $\\delta_t = 1/(\\sqrt{t} + 2\\log(t + 2))$ and $q_t = p/(2.5(t + 1)\\log^2(t + 1))$. Thus, we have\n\n$$\n\\begin{align*}\nN &= \\sum_{t=0}^{T-1} N_t \\\\\n&\\leq \\sum_{t=0}^{T-1} \\frac{4\\sqrt{2}\\delta_t\\log 44d(t + 1)}{(t + 2)^{0.25}\\log^{0.5}(t + 2)} \\\\\n&\\leq O(N^{1.25}\\log N\\epsilon\\log\\sqrt{dN\\epsilon}).\n\\end{align*}\n$$\n43"}]}, {"page": 44, "text": "F      Experiments\nIn our experiments, we consider the logistic regression problem. Below we provide more details\nabout the data generation scheme as well as the implementation of Nesterov\u2019s accelerated gradient\nmethod, BFGS, and our proposed A-QPNE algorithm.\nDataset generation. The dataset consists of n data points {(ai, yi)}n                            i=1, where ai \u2208       Rd is the i-th\nfeature vector and yi \u2208          {\u22121, 1} is its corresponding label. The labels {yi}n                   i=1 are generated by\n                                            yi = sign(\u27e8a\u2217   i , x\u2217\u27e9),     i = 1, 2, . . . , n,\nwhere a\u2217   i \u2208   Rd\u22121 and x\u2217        \u2208  Rd\u22121 are the underlying true feature vector and the underlying true\nparameter, respectively. Moreover, each entry of a\u2217                  i and x\u2217     is drawn independently according to the\nstandard normal distribution N(0, 1). Note that the true feature vectors {a\u2217                               i }n\nour dataset; instead, we generate {ai}n               i=1 by adding noises and appending an extra dimension toi=1 are not given in\n{a\u2217i }n\n      i=1. Specifically, we let ai = [a\u2217         i + ni + 1; 1]\u22a4     \u2208  Rd, where ni \u223c         N(0, \u03c32I) is the i.i.d. Gaussian\nnoise vector and 1 \u2208         Rd\u22121 denotes the all-one vector. In our experiment, we set n = 2, 000, d = 150\nand \u03c3 = 0.8.\nNAG. We implemented a monotone variant of the Nesterov accelerated gradient method as described\nin [Bec17, Section 10.7.4]. Moreover, we determine the step size using a backtracking line search\nscheme.\nBFGS. We implemented the classical BFGS algorithm, where the step size is determined by the\nMor\u00b4e\u2013Thuente line search scheme using an implementation by Diane O\u2019Leary1.\nA-QPNE (our method). We implemented our proposed A-QPNE method following the pseu-\ndocode in Algorithm 1, where the line search scheme is given in Subroutine 1 and the Hessian\napproximation update is given in Subroutine 2. Moreover, the implementations of the LinearSolver\noracle and the SEP oracle are given by Subroutines 3 and 4, respectively.\n   1 http://www.cs.umd.edu/users/oleary/software/\n                                                                   44", "md": "```markdown\n## Experiments\n\nIn our experiments, we consider the logistic regression problem. Below we provide more details\nabout the data generation scheme as well as the implementation of Nesterov\u2019s accelerated gradient\nmethod, BFGS, and our proposed A-QPNE algorithm.\n\n### Dataset generation\n\nThe dataset consists of n data points {(ai, yi)}i=1, where ai \u2208 \u211dd is the i-th\nfeature vector and yi \u2208 {\u22121, 1} is its corresponding label. The labels {yi}i=1 are generated by\nyi = sign(\u27e8a*i, x*\u27e9), i = 1, 2, . . . , n,\n\n$$a*i \u2208 \u211dd\u22121$$ and $$x* \u2208 \u211dd\u22121$$ are the underlying true feature vector and the underlying true\nparameter, respectively. Moreover, each entry of $$a*i$$ and $$x*$$ is drawn independently according to the\nstandard normal distribution N(0, 1). Note that the true feature vectors {a*i}i=1 are not given in\nour dataset; instead, we generate {ai}i=1 by adding noises and appending an extra dimension to\n{a*i}i=1. Specifically, we let ai = [a*i + ni + 1; 1]T \u2208 \u211dd, where ni \u223c N(0, \u03c32I) is the i.i.d. Gaussian\nnoise vector and 1 \u2208 \u211dd\u22121 denotes the all-one vector. In our experiment, we set n = 2,000, d = 150\nand \u03c3 = 0.8.\n\n### NAG\n\nWe implemented a monotone variant of the Nesterov accelerated gradient method as described\nin [Bec17, Section 10.7.4]. Moreover, we determine the step size using a backtracking line search\nscheme.\n\n### BFGS\n\nWe implemented the classical BFGS algorithm, where the step size is determined by the\nMor\u00e9\u2013Thuente line search scheme using an implementation by Diane O\u2019Leary1.\n\n### A-QPNE (our method)\n\nWe implemented our proposed A-QPNE method following the pseudocode in Algorithm 1, where the line search scheme is given in Subroutine 1 and the Hessian\napproximation update is given in Subroutine 2. Moreover, the implementations of the LinearSolver\noracle and the SEP oracle are given by Subroutines 3 and 4, respectively.\n\n1 http://www.cs.umd.edu/users/oleary/software/\n\n```", "images": [], "items": [{"type": "text", "value": "```markdown", "md": "```markdown"}, {"type": "heading", "lvl": 2, "value": "Experiments", "md": "## Experiments"}, {"type": "text", "value": "In our experiments, we consider the logistic regression problem. Below we provide more details\nabout the data generation scheme as well as the implementation of Nesterov\u2019s accelerated gradient\nmethod, BFGS, and our proposed A-QPNE algorithm.", "md": "In our experiments, we consider the logistic regression problem. Below we provide more details\nabout the data generation scheme as well as the implementation of Nesterov\u2019s accelerated gradient\nmethod, BFGS, and our proposed A-QPNE algorithm."}, {"type": "heading", "lvl": 3, "value": "Dataset generation", "md": "### Dataset generation"}, {"type": "text", "value": "The dataset consists of n data points {(ai, yi)}i=1, where ai \u2208 \u211dd is the i-th\nfeature vector and yi \u2208 {\u22121, 1} is its corresponding label. The labels {yi}i=1 are generated by\nyi = sign(\u27e8a*i, x*\u27e9), i = 1, 2, . . . , n,\n\n$$a*i \u2208 \u211dd\u22121$$ and $$x* \u2208 \u211dd\u22121$$ are the underlying true feature vector and the underlying true\nparameter, respectively. Moreover, each entry of $$a*i$$ and $$x*$$ is drawn independently according to the\nstandard normal distribution N(0, 1). Note that the true feature vectors {a*i}i=1 are not given in\nour dataset; instead, we generate {ai}i=1 by adding noises and appending an extra dimension to\n{a*i}i=1. Specifically, we let ai = [a*i + ni + 1; 1]T \u2208 \u211dd, where ni \u223c N(0, \u03c32I) is the i.i.d. Gaussian\nnoise vector and 1 \u2208 \u211dd\u22121 denotes the all-one vector. In our experiment, we set n = 2,000, d = 150\nand \u03c3 = 0.8.", "md": "The dataset consists of n data points {(ai, yi)}i=1, where ai \u2208 \u211dd is the i-th\nfeature vector and yi \u2208 {\u22121, 1} is its corresponding label. The labels {yi}i=1 are generated by\nyi = sign(\u27e8a*i, x*\u27e9), i = 1, 2, . . . , n,\n\n$$a*i \u2208 \u211dd\u22121$$ and $$x* \u2208 \u211dd\u22121$$ are the underlying true feature vector and the underlying true\nparameter, respectively. Moreover, each entry of $$a*i$$ and $$x*$$ is drawn independently according to the\nstandard normal distribution N(0, 1). Note that the true feature vectors {a*i}i=1 are not given in\nour dataset; instead, we generate {ai}i=1 by adding noises and appending an extra dimension to\n{a*i}i=1. Specifically, we let ai = [a*i + ni + 1; 1]T \u2208 \u211dd, where ni \u223c N(0, \u03c32I) is the i.i.d. Gaussian\nnoise vector and 1 \u2208 \u211dd\u22121 denotes the all-one vector. In our experiment, we set n = 2,000, d = 150\nand \u03c3 = 0.8."}, {"type": "heading", "lvl": 3, "value": "NAG", "md": "### NAG"}, {"type": "text", "value": "We implemented a monotone variant of the Nesterov accelerated gradient method as described\nin [Bec17, Section 10.7.4]. Moreover, we determine the step size using a backtracking line search\nscheme.", "md": "We implemented a monotone variant of the Nesterov accelerated gradient method as described\nin [Bec17, Section 10.7.4]. Moreover, we determine the step size using a backtracking line search\nscheme."}, {"type": "heading", "lvl": 3, "value": "BFGS", "md": "### BFGS"}, {"type": "text", "value": "We implemented the classical BFGS algorithm, where the step size is determined by the\nMor\u00e9\u2013Thuente line search scheme using an implementation by Diane O\u2019Leary1.", "md": "We implemented the classical BFGS algorithm, where the step size is determined by the\nMor\u00e9\u2013Thuente line search scheme using an implementation by Diane O\u2019Leary1."}, {"type": "heading", "lvl": 3, "value": "A-QPNE (our method)", "md": "### A-QPNE (our method)"}, {"type": "text", "value": "We implemented our proposed A-QPNE method following the pseudocode in Algorithm 1, where the line search scheme is given in Subroutine 1 and the Hessian\napproximation update is given in Subroutine 2. Moreover, the implementations of the LinearSolver\noracle and the SEP oracle are given by Subroutines 3 and 4, respectively.\n\n1 http://www.cs.umd.edu/users/oleary/software/\n\n```", "md": "We implemented our proposed A-QPNE method following the pseudocode in Algorithm 1, where the line search scheme is given in Subroutine 1 and the Hessian\napproximation update is given in Subroutine 2. Moreover, the implementations of the LinearSolver\noracle and the SEP oracle are given by Subroutines 3 and 4, respectively.\n\n1 http://www.cs.umd.edu/users/oleary/software/\n\n```"}]}], "job_id": "747883eb-5ad8-4d17-bacd-9de20e5a3e1f", "file_path": "./corpus/2306.02212.pdf"}