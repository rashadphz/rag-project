{"pages": [{"page": 1, "text": "                                      VisIT-Bench: A Benchmark for Vision-Language\n                                     Instruction Following Inspired by Real-World Use\n                                   Yonatan Bitton*1,2 Hritik Bansal*3 Jack Hessel*4 Rulin Shao5 Wanrong Zhu6\narXiv:2308.06595v4  [cs.CL]  26 Dec 2023       Anas Awadalla5 Josh Gardner5 Rohan Taori7 Ludwig Schmidt4,5,8\n                                                                                                      Abstract\n                                              We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark\n                                              for evaluation of instruction-following vision-language models for real-\n                                              world use. Our starting point is curating 70 \u201cinstruction families\u201d that we\n                                              envision instruction tuned vision-language models should be able to address.\n                                              Extending beyond evaluations like VQAv2 and COCO, tasks range from\n                                              basic recognition to game playing and creative generation. Following cura-\n                                              tion, our dataset comprises 592 test queries, each with a human-authored\n                                              instruction-conditioned caption. These descriptions surface instruction-\n                                              specific factors, e.g., for an instruction asking about the accessibility of\n                                              a storefront for wheelchair users, the instruction-conditioned caption de-\n                                              scribes ramps/potential obstacles. These descriptions enable 1) collecting\n                                              human-verified reference outputs for each instance; and 2) automatic evalu-\n                                              ation of candidate multimodal generations using a text-only LLM, aligning\n                                              with human judgment. We quantify quality gaps between models and refer-\n                                              ences using both human and automatic evaluations; e.g., the top-performing\n                                              instruction-following model wins against the GPT-4 reference in just 27%\n                                              of the comparison. VisIT-Bench is dynamic to participate, practitioners\n                                              simply submit their model\u2019s response on the project website; Data, code\n                                              and leaderboard is available at https://visit-bench.github.io/.\n                                                                      Instruction               Instruction-Conditioned Caption                  Human-Verified GPT4 Response\n                                                                  Create a catchy title       There is a jeep parked on the street with          \"Tan Lines & Ticket Stubs: A Love Story\n                                                                  for a country song          the top down. There is a wheel cover on the        Unbound\" - A country song about\n                                                                  based upon the              spare tire in the back with an inscription         leaving it all behind, finding love, and\n                                                                  advice printed on the       that says, \"Quit your Job, Buy a Ticket, Get a     embracing the freedom of the open\n                                                                  wheel cover.                Tan, Fall in Love, Never Return.\"                  road.\n                               Figure 1: An example from VisIT-Bench, featuring an image, a challenging instruction, an\n                               instruction-conditioned caption, and a human-verified GPT4 response. These elements are\n                               used for evaluating multimodal chatbots and updating a dynamic leaderboard.\n                                    \u2217 Equal contribution. Contact yonatanbitton1@gmail.com,hbansal@ucla.edu,jackh@allenai.org,\n                               schmidt@cs.washington.edu. 1Hebrew University                                     2 Google Research            3UCLA         4Allen Institute for AI\n                               5 University of Washington                6UCSB        7Stanford 8LAION ,\n                               Preprint. Under review.", "md": "# VisIT-Bench: A Benchmark for Vision-Language Instruction Following\n\n# VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use\n\nYonatan Bitton*1,2, Hritik Bansal*3, Jack Hessel*4, Rulin Shao5, Wanrong Zhu6\n\narXiv:2308.06595v4 [cs.CL] 26 Dec 2023 Anas Awadalla5, Josh Gardner5, Rohan Taori7, Ludwig Schmidt4,5,8\n\n## Abstract\n\nWe introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 \"instruction families\" that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/.\n\n|Instruction|Instruction-Conditioned Caption|Human-Verified GPT4 Response|\n|---|---|---|\n|Create a catchy title for a country song based upon the advice printed on the wheel cover.|There is a jeep parked on the street with the top down. There is a wheel cover on the spare tire in the back with an inscription that says, \"Quit your Job, Buy a Ticket, Get a Tan, Fall in Love, Never Return.\"|\"Tan Lines & Ticket Stubs: A Love Story Unbound\" - A country song about leaving it all behind, finding love, and embracing the freedom of the open road.|\n\nFigure 1: An example from VisIT-Bench, featuring an image, a challenging instruction, an instruction-conditioned caption, and a human-verified GPT4 response. These elements are used for evaluating multimodal chatbots and updating a dynamic leaderboard.\n\n* Equal contribution. Contact yonatanbitton1@gmail.com, hbansal@ucla.edu, jackh@allenai.org, schmidt@cs.washington.edu. 1Hebrew University 2 Google Research 3UCLA 4Allen Institute for AI 5 University of Washington 6UCSB 7Stanford 8LAION\n\nPreprint. Under review.", "images": [{"name": "page-1-0.jpg", "height": 73, "width": 73, "x": 111, "y": 511}], "items": [{"type": "heading", "lvl": 1, "value": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following", "md": "# VisIT-Bench: A Benchmark for Vision-Language Instruction Following"}, {"type": "heading", "lvl": 1, "value": "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use", "md": "# VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"}, {"type": "text", "value": "Yonatan Bitton*1,2, Hritik Bansal*3, Jack Hessel*4, Rulin Shao5, Wanrong Zhu6\n\narXiv:2308.06595v4 [cs.CL] 26 Dec 2023 Anas Awadalla5, Josh Gardner5, Rohan Taori7, Ludwig Schmidt4,5,8", "md": "Yonatan Bitton*1,2, Hritik Bansal*3, Jack Hessel*4, Rulin Shao5, Wanrong Zhu6\n\narXiv:2308.06595v4 [cs.CL] 26 Dec 2023 Anas Awadalla5, Josh Gardner5, Rohan Taori7, Ludwig Schmidt4,5,8"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 \"instruction families\" that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/.", "md": "We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 \"instruction families\" that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/."}, {"type": "table", "rows": [["Instruction", "Instruction-Conditioned Caption", "Human-Verified GPT4 Response"], ["Create a catchy title for a country song based upon the advice printed on the wheel cover.", "There is a jeep parked on the street with the top down. There is a wheel cover on the spare tire in the back with an inscription that says, \"Quit your Job, Buy a Ticket, Get a Tan, Fall in Love, Never Return.\"", "\"Tan Lines & Ticket Stubs: A Love Story Unbound\" - A country song about leaving it all behind, finding love, and embracing the freedom of the open road."]], "md": "|Instruction|Instruction-Conditioned Caption|Human-Verified GPT4 Response|\n|---|---|---|\n|Create a catchy title for a country song based upon the advice printed on the wheel cover.|There is a jeep parked on the street with the top down. There is a wheel cover on the spare tire in the back with an inscription that says, \"Quit your Job, Buy a Ticket, Get a Tan, Fall in Love, Never Return.\"|\"Tan Lines & Ticket Stubs: A Love Story Unbound\" - A country song about leaving it all behind, finding love, and embracing the freedom of the open road.|", "isPerfectTable": true, "csv": "\"Instruction\",\"Instruction-Conditioned Caption\",\"Human-Verified GPT4 Response\"\n\"Create a catchy title for a country song based upon the advice printed on the wheel cover.\",\"There is a jeep parked on the street with the top down. There is a wheel cover on the spare tire in the back with an inscription that says, \"\"Quit your Job, Buy a Ticket, Get a Tan, Fall in Love, Never Return.\"\"\",\"\"\"Tan Lines & Ticket Stubs: A Love Story Unbound\"\" - A country song about leaving it all behind, finding love, and embracing the freedom of the open road.\""}, {"type": "text", "value": "Figure 1: An example from VisIT-Bench, featuring an image, a challenging instruction, an instruction-conditioned caption, and a human-verified GPT4 response. These elements are used for evaluating multimodal chatbots and updating a dynamic leaderboard.\n\n* Equal contribution. Contact yonatanbitton1@gmail.com, hbansal@ucla.edu, jackh@allenai.org, schmidt@cs.washington.edu. 1Hebrew University 2 Google Research 3UCLA 4Allen Institute for AI 5 University of Washington 6UCSB 7Stanford 8LAION\n\nPreprint. Under review.", "md": "Figure 1: An example from VisIT-Bench, featuring an image, a challenging instruction, an instruction-conditioned caption, and a human-verified GPT4 response. These elements are used for evaluating multimodal chatbots and updating a dynamic leaderboard.\n\n* Equal contribution. Contact yonatanbitton1@gmail.com, hbansal@ucla.edu, jackh@allenai.org, schmidt@cs.washington.edu. 1Hebrew University 2 Google Research 3UCLA 4Allen Institute for AI 5 University of Washington 6UCSB 7Stanford 8LAION\n\nPreprint. Under review."}]}, {"page": 2, "text": "1    Introduction\nA long-standing challenge for artificial intelligence is to build general-purpose assistants\nthat can, in collaboration with humans, solve diverse and never-before-seen tasks [1]. For\ntextual tasks, several recent works [2, 3, 4, 5, 6, 7] have shown that fine-tuning language\nmodels such as GPT-3 and LLaMA with supervised instruction+response examples [8, 9, 10]\nenables them to respond to imperative requests and questions without task-specific training.\nZero-shot generalization is promising not only for standard academic benchmarks, but \u2013\nperhaps more-so \u2013 for creative, useful, and real-world queries that downstream users of\nlanguage technologies are likely to make.\nOn the multimodal side, recent instruction-following vision-language models also provide\na zero-shot interface. Given an image (or multiple images) and a query (e.g., \u201chow many\napples are in this image?\u201d or \u201cWhat is this?\u201d or \u201cWrite a poem in the style of Robert Frost\nabout this scene.\u201d) a textual response is provided. Recent works like OpenFlamingo [11, 12],\nLLaVA [13] and others [14, 15, 16, 17, 18], have implemented this interface with promising\ninitial results. Although standard benchmarks like VQAv2 [19] and COCO captioning [20]\nare commonly used to assess performance, less is know about how models perform on\nbroader, open-ended queries that resemble real-world user behavior. Evaluations of such\nqueries typically rely on informal and qualitative approaches.\nTo support quantitative evaluation for this setting, we present VisIT-Bench (Visual\nInsTruction Benchmark), a dynamic benchmark consisting of 592 challenging vision-\nlanguage instructions. Each instance contains an instruction, input image(s), a instruction-\nconditioned caption (a human-crafted caption for the image(s)/instruction), and a human\nverified reference (Figure 1). Instructions are image-contextual imperative requests or ques-\ntions, e.g., for an image of pancakes, a user asks \u201chow can I cook this in a healthy way?\u201d.\nDifferent from existing zero-shot evaluations, many of the instructions focus on open-ended\ngeneration requests (e.g., \u201cwrite a poem...\u201d or \u201cwhat should I bring if I were to visit here?\u201d).\nWe created VisIT-Bench to cover a wide array of \u201cinstruction families\u201d. Our starting point\nwas a set of 70 \u201cwish-list\u201d tasks such as \u201chome renovation\u201d and \u201cgardening tips\u201d collected by\nthe authors:1 each requiring varied high-level skills from recognition to complex reasoning\n(Figure 2). We derived 25/70 instruction families from benchmark tasks such as Visual\nQuestion Answering (VQA) [21] and robust change captioning [22] into a chatbot-style\nformat (this reformatting differs from prior work [14, 17, 13], as we focus on open-ended\nchatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.\nWe started with 10 images for each instruction family. Our annotators, guided by an\nexample, create a new instruction, and provide a (permissively licensed) image. For each\ninstruction, we next collect instruction-conditioned captions \u2013 unlike prior work [23, 24]\nthese descriptions are designed not only to describe the image in general, but also, surface\ninformation targeted to the instruction. Finally, we use instruction-conditioned captions to\ngenerate a reference candidate output from GPT-4; an additional human verification step\ndiscards GPT-4 references deemed to be incorrect.\nWe conduct a large-scale empirical comparison of multimodal instruction-following models\nusing VisIT-Bench (\u00a74). We first gather predictions for each instance from 7 candidate\nmodels. Then, we collect 5K human judgements of output quality by pitting model outputs\nhead-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements.\nThis analysis not only reveals significant differences between models (e.g., that LLaVA-\n13b [13] is generally preferred to Panda [18]), but also, that the human verified references\nin our corpus are preferred significantly more than the ones generated using multimodal\nmodels. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25, 26],\n    1We recognize that promising applications may not be covered by our set; and we don\u2019t necessarily advocate\nfor deploying models in all cases we cover \u2013 we hope VisIT-Bench can help to quantify shortcomings and risks.\n                                                      2", "md": "# Introduction\n\n# Introduction\n\nA long-standing challenge for artificial intelligence is to build general-purpose assistants that can, in collaboration with humans, solve diverse and never-before-seen tasks [1]. For textual tasks, several recent works [2, 3, 4, 5, 6, 7] have shown that fine-tuning language models such as GPT-3 and LLaMA with supervised instruction+response examples [8, 9, 10] enables them to respond to imperative requests and questions without task-specific training. Zero-shot generalization is promising not only for standard academic benchmarks, but \u2013 perhaps more-so \u2013 for creative, useful, and real-world queries that downstream users of language technologies are likely to make.\n\nOn the multimodal side, recent instruction-following vision-language models also provide a zero-shot interface. Given an image (or multiple images) and a query (e.g., \"how many apples are in this image?\" or \"What is this?\" or \"Write a poem in the style of Robert Frost about this scene.\"), a textual response is provided. Recent works like OpenFlamingo [11, 12], LLaVA [13] and others [14, 15, 16, 17, 18], have implemented this interface with promising initial results. Although standard benchmarks like VQAv2 [19] and COCO captioning [20] are commonly used to assess performance, less is know about how models perform on broader, open-ended queries that resemble real-world user behavior. Evaluations of such queries typically rely on informal and qualitative approaches.\n\nTo support quantitative evaluation for this setting, we present VisIT-Bench (Visual InsTruction Benchmark), a dynamic benchmark consisting of 592 challenging vision-language instructions. Each instance contains an instruction, input image(s), a instruction-conditioned caption (a human-crafted caption for the image(s)/instruction), and a human verified reference (Figure 1). Instructions are image-contextual imperative requests or questions, e.g., for an image of pancakes, a user asks \"how can I cook this in a healthy way?\". Different from existing zero-shot evaluations, many of the instructions focus on open-ended generation requests (e.g., \"write a poem...\" or \"what should I bring if I were to visit here?\").\n\nWe created VisIT-Bench to cover a wide array of \"instruction families\". Our starting point was a set of 70 \"wish-list\" tasks such as \"home renovation\" and \"gardening tips\" collected by the authors: each requiring varied high-level skills from recognition to complex reasoning. We derived 25/70 instruction families from benchmark tasks such as Visual Question Answering (VQA) [21] and robust change captioning [22] into a chatbot-style format (this reformatting differs from prior work [14, 17, 13], as we focus on open-ended chatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.\n\nWe started with 10 images for each instruction family. Our annotators, guided by an example, create a new instruction, and provide a (permissively licensed) image. For each instruction, we next collect instruction-conditioned captions \u2013 unlike prior work [23, 24] these descriptions are designed not only to describe the image in general, but also, surface information targeted to the instruction. Finally, we use instruction-conditioned captions to generate a reference candidate output from GPT-4; an additional human verification step discards GPT-4 references deemed to be incorrect.\n\nWe conduct a large-scale empirical comparison of multimodal instruction-following models using VisIT-Bench (\u00a74). We first gather predictions for each instance from 7 candidate models. Then, we collect 5K human judgements of output quality by pitting model outputs head-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements. This analysis not only reveals significant differences between models (e.g., that LLaVA-13b [13] is generally preferred to Panda [18]), but also, that the human verified references in our corpus are preferred significantly more than the ones generated using multimodal models. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25, 26],\n\n1We recognize that promising applications may not be covered by our set; and we don\u2019t necessarily advocate for deploying models in all cases we cover \u2013 we hope VisIT-Bench can help to quantify shortcomings and risks.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Introduction", "md": "# Introduction"}, {"type": "heading", "lvl": 1, "value": "Introduction", "md": "# Introduction"}, {"type": "text", "value": "A long-standing challenge for artificial intelligence is to build general-purpose assistants that can, in collaboration with humans, solve diverse and never-before-seen tasks [1]. For textual tasks, several recent works [2, 3, 4, 5, 6, 7] have shown that fine-tuning language models such as GPT-3 and LLaMA with supervised instruction+response examples [8, 9, 10] enables them to respond to imperative requests and questions without task-specific training. Zero-shot generalization is promising not only for standard academic benchmarks, but \u2013 perhaps more-so \u2013 for creative, useful, and real-world queries that downstream users of language technologies are likely to make.\n\nOn the multimodal side, recent instruction-following vision-language models also provide a zero-shot interface. Given an image (or multiple images) and a query (e.g., \"how many apples are in this image?\" or \"What is this?\" or \"Write a poem in the style of Robert Frost about this scene.\"), a textual response is provided. Recent works like OpenFlamingo [11, 12], LLaVA [13] and others [14, 15, 16, 17, 18], have implemented this interface with promising initial results. Although standard benchmarks like VQAv2 [19] and COCO captioning [20] are commonly used to assess performance, less is know about how models perform on broader, open-ended queries that resemble real-world user behavior. Evaluations of such queries typically rely on informal and qualitative approaches.\n\nTo support quantitative evaluation for this setting, we present VisIT-Bench (Visual InsTruction Benchmark), a dynamic benchmark consisting of 592 challenging vision-language instructions. Each instance contains an instruction, input image(s), a instruction-conditioned caption (a human-crafted caption for the image(s)/instruction), and a human verified reference (Figure 1). Instructions are image-contextual imperative requests or questions, e.g., for an image of pancakes, a user asks \"how can I cook this in a healthy way?\". Different from existing zero-shot evaluations, many of the instructions focus on open-ended generation requests (e.g., \"write a poem...\" or \"what should I bring if I were to visit here?\").\n\nWe created VisIT-Bench to cover a wide array of \"instruction families\". Our starting point was a set of 70 \"wish-list\" tasks such as \"home renovation\" and \"gardening tips\" collected by the authors: each requiring varied high-level skills from recognition to complex reasoning. We derived 25/70 instruction families from benchmark tasks such as Visual Question Answering (VQA) [21] and robust change captioning [22] into a chatbot-style format (this reformatting differs from prior work [14, 17, 13], as we focus on open-ended chatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.\n\nWe started with 10 images for each instruction family. Our annotators, guided by an example, create a new instruction, and provide a (permissively licensed) image. For each instruction, we next collect instruction-conditioned captions \u2013 unlike prior work [23, 24] these descriptions are designed not only to describe the image in general, but also, surface information targeted to the instruction. Finally, we use instruction-conditioned captions to generate a reference candidate output from GPT-4; an additional human verification step discards GPT-4 references deemed to be incorrect.\n\nWe conduct a large-scale empirical comparison of multimodal instruction-following models using VisIT-Bench (\u00a74). We first gather predictions for each instance from 7 candidate models. Then, we collect 5K human judgements of output quality by pitting model outputs head-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements. This analysis not only reveals significant differences between models (e.g., that LLaVA-13b [13] is generally preferred to Panda [18]), but also, that the human verified references in our corpus are preferred significantly more than the ones generated using multimodal models. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25, 26],\n\n1We recognize that promising applications may not be covered by our set; and we don\u2019t necessarily advocate for deploying models in all cases we cover \u2013 we hope VisIT-Bench can help to quantify shortcomings and risks.", "md": "A long-standing challenge for artificial intelligence is to build general-purpose assistants that can, in collaboration with humans, solve diverse and never-before-seen tasks [1]. For textual tasks, several recent works [2, 3, 4, 5, 6, 7] have shown that fine-tuning language models such as GPT-3 and LLaMA with supervised instruction+response examples [8, 9, 10] enables them to respond to imperative requests and questions without task-specific training. Zero-shot generalization is promising not only for standard academic benchmarks, but \u2013 perhaps more-so \u2013 for creative, useful, and real-world queries that downstream users of language technologies are likely to make.\n\nOn the multimodal side, recent instruction-following vision-language models also provide a zero-shot interface. Given an image (or multiple images) and a query (e.g., \"how many apples are in this image?\" or \"What is this?\" or \"Write a poem in the style of Robert Frost about this scene.\"), a textual response is provided. Recent works like OpenFlamingo [11, 12], LLaVA [13] and others [14, 15, 16, 17, 18], have implemented this interface with promising initial results. Although standard benchmarks like VQAv2 [19] and COCO captioning [20] are commonly used to assess performance, less is know about how models perform on broader, open-ended queries that resemble real-world user behavior. Evaluations of such queries typically rely on informal and qualitative approaches.\n\nTo support quantitative evaluation for this setting, we present VisIT-Bench (Visual InsTruction Benchmark), a dynamic benchmark consisting of 592 challenging vision-language instructions. Each instance contains an instruction, input image(s), a instruction-conditioned caption (a human-crafted caption for the image(s)/instruction), and a human verified reference (Figure 1). Instructions are image-contextual imperative requests or questions, e.g., for an image of pancakes, a user asks \"how can I cook this in a healthy way?\". Different from existing zero-shot evaluations, many of the instructions focus on open-ended generation requests (e.g., \"write a poem...\" or \"what should I bring if I were to visit here?\").\n\nWe created VisIT-Bench to cover a wide array of \"instruction families\". Our starting point was a set of 70 \"wish-list\" tasks such as \"home renovation\" and \"gardening tips\" collected by the authors: each requiring varied high-level skills from recognition to complex reasoning. We derived 25/70 instruction families from benchmark tasks such as Visual Question Answering (VQA) [21] and robust change captioning [22] into a chatbot-style format (this reformatting differs from prior work [14, 17, 13], as we focus on open-ended chatbot style responses.). Notably, 10 of these repurposed tasks involve multiple images.\n\nWe started with 10 images for each instruction family. Our annotators, guided by an example, create a new instruction, and provide a (permissively licensed) image. For each instruction, we next collect instruction-conditioned captions \u2013 unlike prior work [23, 24] these descriptions are designed not only to describe the image in general, but also, surface information targeted to the instruction. Finally, we use instruction-conditioned captions to generate a reference candidate output from GPT-4; an additional human verification step discards GPT-4 references deemed to be incorrect.\n\nWe conduct a large-scale empirical comparison of multimodal instruction-following models using VisIT-Bench (\u00a74). We first gather predictions for each instance from 7 candidate models. Then, we collect 5K human judgements of output quality by pitting model outputs head-to-head, and (in a forced-choice setup) crowd-sourcing pairwise preference judgements. This analysis not only reveals significant differences between models (e.g., that LLaVA-13b [13] is generally preferred to Panda [18]), but also, that the human verified references in our corpus are preferred significantly more than the ones generated using multimodal models. We summarize head-to-head comparisons with two metrics: 1) Elo ratings [25, 26],\n\n1We recognize that promising applications may not be covered by our set; and we don\u2019t necessarily advocate for deploying models in all cases we cover \u2013 we hope VisIT-Bench can help to quantify shortcomings and risks."}]}, {"page": 3, "text": "     Reasoning Over                Art Knowledge                   Recognition                     Location               Home Renovation\n             Plots                                                                             Understanding\n       How much is the           Teach me about this              Where is this?             If you are going for a      Here is a photo of my\n      browser usage for                 painting.                                            picnic at this location,    bathroom. How can I\n      Firefox and Safari?                                                                   what items should you            design it nicer?\n                                                                                                carry with you?\n         Contextual              Figurative Speech                  Chemical               Hazard Identification            Game Playing\n  Knowledge of Events                Explanation                  Identification\n   Tell me what is notable     Someone said that this             Which chemical             If you are driving and     What is the poker hand\n   or important about the      man is an angel. Why?           compound does this              come across this          shown in the picture?\n     event in this photo.                                       image represent?             scenario, what should        Is this a good hand?\n                                                                                                     you do?\nFigure 2: A sample from the 70 instruction families in VisIT-Bench representing tasks we\nenvision instruction-following vision-language models should be able to follow.\nwhich provide relative \u201cskill\u201d rating estimates encoding the probability that model A will\nbe preferred to model B; and 2) win rate versus our references, which provides an absolute\nmetric. The best model according to human judgement is LLaMA-Adapter-v2 [16], yet it\nonly wins in a pairwise setting against the reference in 27.4% of cases.\nFinally, we design an automated evaluation for VisIT-Bench, utilizing GPT-4 to rank pairs\nof model responses based on factors like correctness, relevance, and fluency. Using the\ninstruction-conditioned caption and the instruction, GPT-4 determines the better response\nbetween two options, expediting iteration compared to human preferences. We explore\nreference-free and reference-backed versions of this metric. Compared to various metrics\n(BLEU-4 [27], ROUGE-L [28], METEOR [29], CIDEr [30], and BERTScore [31]), our\nevaluation aligns best with human preferences. For example, it achieves a 94% agreement\nrate in the cases where all fi               ve annotators agree. See Figure 7 for a schematic of the process.\nWhile it is difficult to a priori envision all possible scenarios under which more performant\nmultimodal chatbots might be used, we hope VisIT-Bench can provide a path to improving\nvision-language models \u201cin the wild.\u201d Table 1 presents a summary of our contributions in\ncomparison to the recent works [32, 14, 17, 33, 34, 35] in the evaluation of multimodal\nchatbots. We publicly release VisIT-Bench data, code, and automatic metrics to facilitate\nfuture model evaluations, available in https://visit-bench.github.io/.\nTable 1: Comparison with related works for evaluating instruction-following vision-language\nmodels. Win-rates* refers to the model win-rates against a reference output/model.\n                                      MultiInstruct [32]   Owl [17]   InstructBLIP [14]    M3IT [33]    LVLM [34]      GAVIE [35]     VisIT-Bench\n  Number of Models                            1                5               3                4            8              5              10\n  Number of Skills Tested                     9                6              13               13            47            16              70\n  Multiple-Images                             \u2717               \u2713                \u2717                \u2717            \u2717              \u2717              \u2713\n  Video                                       \u2717               \u2717               \u2713                \u2713             \u2717              \u2717               \u2717\n  Multi-Turn Conversations                    \u2713               \u2713               \u2713                \u2713             \u2713              \u2717               \u2717\n  Multilingual Conversations                  \u2717               \u2713                \u2717               \u2713             \u2717              \u2717               \u2717\n  Instruction-conditioned Captions            \u2717               \u2717                \u2717                \u2717            \u2717              \u2717              \u2713\n  Chatbot-style Responses                     \u2717               \u2717                \u2717                \u2717            \u2717              \u2717              \u2713\n  Dataset-specific Evaluation                 \u2713               \u2713               \u2713                \u2713             \u2713              \u2717               \u2717\n  Human Evaluation                            \u2717               \u2713                \u2717                \u2717            \u2713              \u2717              \u2713\n  Auto/GPT-4 Evaluation                       \u2717               \u2713                \u2717               \u2713             \u2717              \u2713              \u2713\n  Win-rates*                                  \u2717               \u2713                \u2717               \u2713             \u2717              \u2713              \u2713\n  Elo Rating                                  \u2717               \u2717                \u2717                \u2717            \u2713              \u2717              \u2713\n                                                                         3", "md": "Reasoning Over Art Knowledge Recognition Location Home Renovation Plots Understanding\n\nHow much is the browser usage for Firefox and Safari?\n\nContextual Figurative Speech Chemical Hazard Identification Game Playing Knowledge of Events Explanation Identification\n\nTell me what is notable or important about the event in this photo.\n\n$$\n\\text{Figure 2: A sample from the 70 instruction families in VisIT-Bench representing tasks we envision instruction-following vision-language models should be able to follow.}\n$$\n\nwhich provide relative \u201cskill\u201d rating estimates encoding the probability that model A will be preferred to model B; and 2) win rate versus our references, which provides an absolute metric. The best model according to human judgement is LLaMA-Adapter-v2 [16], yet it only wins in a pairwise setting against the reference in 27.4% of cases.\n\nFinally, we design an automated evaluation for VisIT-Bench, utilizing GPT-4 to rank pairs of model responses based on factors like correctness, relevance, and fluency. Using the instruction-conditioned caption and the instruction, GPT-4 determines the better response between two options, expediting iteration compared to human preferences. We explore reference-free and reference-backed versions of this metric. Compared to various metrics (BLEU-4 [27], ROUGE-L [28], METEOR [29], CIDEr [30], and BERTScore [31]), our evaluation aligns best with human preferences. For example, it achieves a 94% agreement rate in the cases where all five annotators agree. See Figure 7 for a schematic of the process.\n\nWhile it is difficult to a priori envision all possible scenarios under which more performant multimodal chatbots might be used, we hope VisIT-Bench can provide a path to improving vision-language models \u201cin the wild.\u201d Table 1 presents a summary of our contributions in comparison to the recent works [32, 14, 17, 33, 34, 35] in the evaluation of multimodal chatbots. We publicly release VisIT-Bench data, code, and automatic metrics to facilitate future model evaluations, available in [https://visit-bench.github.io/](https://visit-bench.github.io/).\n\n| Comparison with related works for evaluating instruction-following vision-language models | MultiInstruct [32] | Owl [17] | InstructBLIP [14] | M3IT [33] | LVLM [34] | GAVIE [35] | VisIT-Bench |\n|-----------------------------------------------------------------------------------------|---------------------|----------|-------------------|-----------|-----------|------------|-------------|\n| Number of Models                                                                        | 1                   | 5        | 3                 | 4         | 8         | 5          | 10          |\n| Number of Skills Tested                                                                 | 9                   | 6        | 13                | 13        | 47        | 16         | 70          |\n| Multiple-Images                                                                         | \u2717                   | \u2713        | \u2717                 | \u2717         | \u2717         | \u2717          | \u2713           |\n| Video                                                                                   | \u2717                   | \u2717       | \u2713                 | \u2713         | \u2717         | \u2717          | \u2717           |\n| Multi-Turn Conversations                                                                | \u2713                   | \u2713       | \u2713                 | \u2713         | \u2713         | \u2717          | \u2717           |\n| Multilingual Conversations                                                              | \u2717                   | \u2713        | \u2717                 | \u2713         | \u2717         | \u2717          | \u2717           |\n| Instruction-conditioned Captions                                                        | \u2717                   | \u2717       | \u2717                 | \u2717         | \u2717         | \u2717          | \u2713           |\n| Chatbot-style Responses                                                                 | \u2717                   | \u2717       | \u2717                 | \u2717         | \u2717         | \u2717          | \u2713           |\n| Dataset-specific Evaluation                                                             | \u2713                   | \u2713       | \u2713                 | \u2713         | \u2713         | \u2717          | \u2717           |\n| Human Evaluation                                                                        | \u2717                   | \u2713        | \u2717                 | \u2717         | \u2713         | \u2717          | \u2713           |\n| Auto/GPT-4 Evaluation                                                                   | \u2717                   | \u2713        | \u2717                 | \u2713         | \u2717         | \u2713          | \u2713           |\n| Win-rates*                                                                              | \u2717                   | \u2713        | \u2717                 | \u2713         | \u2717         | \u2713          | \u2713           |\n| Elo Rating                                                                              | \u2717                   | \u2717       | \u2717                 | \u2717         | \u2713         | \u2717          | \u2713           |\n\n**Table 1:** Comparison with related works for evaluating instruction-following vision-language models. Win-rates* refers to the model win-rates against a reference output/model.", "images": [{"name": "page-3-1.jpg", "height": 56, "width": 59, "x": 198, "y": 83}, {"name": "page-3-2.jpg", "height": 53, "width": 74, "x": 269, "y": 86}, {"name": "page-3-3.jpg", "height": 49, "width": 73, "x": 351, "y": 91}, {"name": "page-3-0.jpg", "height": 54, "width": 59, "x": 123, "y": 88}, {"name": "page-3-4.jpg", "height": 42, "width": 65, "x": 118, "y": 207}, {"name": "page-3-5.jpg", "height": 86, "width": 159, "x": 190, "y": 183}, {"name": "page-3-6.jpg", "height": 68, "width": 135, "x": 363, "y": 188}, {"name": "page-3-7.jpg", "height": 56, "width": 48, "x": 439, "y": 86}], "items": [{"type": "text", "value": "Reasoning Over Art Knowledge Recognition Location Home Renovation Plots Understanding\n\nHow much is the browser usage for Firefox and Safari?\n\nContextual Figurative Speech Chemical Hazard Identification Game Playing Knowledge of Events Explanation Identification\n\nTell me what is notable or important about the event in this photo.\n\n$$\n\\text{Figure 2: A sample from the 70 instruction families in VisIT-Bench representing tasks we envision instruction-following vision-language models should be able to follow.}\n$$\n\nwhich provide relative \u201cskill\u201d rating estimates encoding the probability that model A will be preferred to model B; and 2) win rate versus our references, which provides an absolute metric. The best model according to human judgement is LLaMA-Adapter-v2 [16], yet it only wins in a pairwise setting against the reference in 27.4% of cases.\n\nFinally, we design an automated evaluation for VisIT-Bench, utilizing GPT-4 to rank pairs of model responses based on factors like correctness, relevance, and fluency. Using the instruction-conditioned caption and the instruction, GPT-4 determines the better response between two options, expediting iteration compared to human preferences. We explore reference-free and reference-backed versions of this metric. Compared to various metrics (BLEU-4 [27], ROUGE-L [28], METEOR [29], CIDEr [30], and BERTScore [31]), our evaluation aligns best with human preferences. For example, it achieves a 94% agreement rate in the cases where all five annotators agree. See Figure 7 for a schematic of the process.\n\nWhile it is difficult to a priori envision all possible scenarios under which more performant multimodal chatbots might be used, we hope VisIT-Bench can provide a path to improving vision-language models \u201cin the wild.\u201d Table 1 presents a summary of our contributions in comparison to the recent works [32, 14, 17, 33, 34, 35] in the evaluation of multimodal chatbots. We publicly release VisIT-Bench data, code, and automatic metrics to facilitate future model evaluations, available in [https://visit-bench.github.io/](https://visit-bench.github.io/).", "md": "Reasoning Over Art Knowledge Recognition Location Home Renovation Plots Understanding\n\nHow much is the browser usage for Firefox and Safari?\n\nContextual Figurative Speech Chemical Hazard Identification Game Playing Knowledge of Events Explanation Identification\n\nTell me what is notable or important about the event in this photo.\n\n$$\n\\text{Figure 2: A sample from the 70 instruction families in VisIT-Bench representing tasks we envision instruction-following vision-language models should be able to follow.}\n$$\n\nwhich provide relative \u201cskill\u201d rating estimates encoding the probability that model A will be preferred to model B; and 2) win rate versus our references, which provides an absolute metric. The best model according to human judgement is LLaMA-Adapter-v2 [16], yet it only wins in a pairwise setting against the reference in 27.4% of cases.\n\nFinally, we design an automated evaluation for VisIT-Bench, utilizing GPT-4 to rank pairs of model responses based on factors like correctness, relevance, and fluency. Using the instruction-conditioned caption and the instruction, GPT-4 determines the better response between two options, expediting iteration compared to human preferences. We explore reference-free and reference-backed versions of this metric. Compared to various metrics (BLEU-4 [27], ROUGE-L [28], METEOR [29], CIDEr [30], and BERTScore [31]), our evaluation aligns best with human preferences. For example, it achieves a 94% agreement rate in the cases where all five annotators agree. See Figure 7 for a schematic of the process.\n\nWhile it is difficult to a priori envision all possible scenarios under which more performant multimodal chatbots might be used, we hope VisIT-Bench can provide a path to improving vision-language models \u201cin the wild.\u201d Table 1 presents a summary of our contributions in comparison to the recent works [32, 14, 17, 33, 34, 35] in the evaluation of multimodal chatbots. We publicly release VisIT-Bench data, code, and automatic metrics to facilitate future model evaluations, available in [https://visit-bench.github.io/](https://visit-bench.github.io/)."}, {"type": "table", "rows": [["Comparison with related works for evaluating instruction-following vision-language models", "MultiInstruct [32]", "Owl [17]", "InstructBLIP [14]", "M3IT [33]", "LVLM [34]", "GAVIE [35]", "VisIT-Bench"], ["Number of Models", "1", "5", "3", "4", "8", "5", "10"], ["Number of Skills Tested", "9", "6", "13", "13", "47", "16", "70"], ["Multiple-Images", "\u2717", "\u2713", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], ["Video", "\u2717", "\u2717", "\u2713", "\u2713", "\u2717", "\u2717", "\u2717"], ["Multi-Turn Conversations", "\u2713", "\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2717"], ["Multilingual Conversations", "\u2717", "\u2713", "\u2717", "\u2713", "\u2717", "\u2717", "\u2717"], ["Instruction-conditioned Captions", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], ["Chatbot-style Responses", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713"], ["Dataset-specific Evaluation", "\u2713", "\u2713", "\u2713", "\u2713", "\u2713", "\u2717", "\u2717"], ["Human Evaluation", "\u2717", "\u2713", "\u2717", "\u2717", "\u2713", "\u2717", "\u2713"], ["Auto/GPT-4 Evaluation", "\u2717", "\u2713", "\u2717", "\u2713", "\u2717", "\u2713", "\u2713"], ["Win-rates*", "\u2717", "\u2713", "\u2717", "\u2713", "\u2717", "\u2713", "\u2713"], ["Elo Rating", "\u2717", "\u2717", "\u2717", "\u2717", "\u2713", "\u2717", "\u2713"]], "md": "| Comparison with related works for evaluating instruction-following vision-language models | MultiInstruct [32] | Owl [17] | InstructBLIP [14] | M3IT [33] | LVLM [34] | GAVIE [35] | VisIT-Bench |\n|-----------------------------------------------------------------------------------------|---------------------|----------|-------------------|-----------|-----------|------------|-------------|\n| Number of Models                                                                        | 1                   | 5        | 3                 | 4         | 8         | 5          | 10          |\n| Number of Skills Tested                                                                 | 9                   | 6        | 13                | 13        | 47        | 16         | 70          |\n| Multiple-Images                                                                         | \u2717                   | \u2713        | \u2717                 | \u2717         | \u2717         | \u2717          | \u2713           |\n| Video                                                                                   | \u2717                   | \u2717       | \u2713                 | \u2713         | \u2717         | \u2717          | \u2717           |\n| Multi-Turn Conversations                                                                | \u2713                   | \u2713       | \u2713                 | \u2713         | \u2713         | \u2717          | \u2717           |\n| Multilingual Conversations                                                              | \u2717                   | \u2713        | \u2717                 | \u2713         | \u2717         | \u2717          | \u2717           |\n| Instruction-conditioned Captions                                                        | \u2717                   | \u2717       | \u2717                 | \u2717         | \u2717         | \u2717          | \u2713           |\n| Chatbot-style Responses                                                                 | \u2717                   | \u2717       | \u2717                 | \u2717         | \u2717         | \u2717          | \u2713           |\n| Dataset-specific Evaluation                                                             | \u2713                   | \u2713       | \u2713                 | \u2713         | \u2713         | \u2717          | \u2717           |\n| Human Evaluation                                                                        | \u2717                   | \u2713        | \u2717                 | \u2717         | \u2713         | \u2717          | \u2713           |\n| Auto/GPT-4 Evaluation                                                                   | \u2717                   | \u2713        | \u2717                 | \u2713         | \u2717         | \u2713          | \u2713           |\n| Win-rates*                                                                              | \u2717                   | \u2713        | \u2717                 | \u2713         | \u2717         | \u2713          | \u2713           |\n| Elo Rating                                                                              | \u2717                   | \u2717       | \u2717                 | \u2717         | \u2713         | \u2717          | \u2713           |", "isPerfectTable": true, "csv": "\"Comparison with related works for evaluating instruction-following vision-language models\",\"MultiInstruct [32]\",\"Owl [17]\",\"InstructBLIP [14]\",\"M3IT [33]\",\"LVLM [34]\",\"GAVIE [35]\",\"VisIT-Bench\"\n\"Number of Models\",\"1\",\"5\",\"3\",\"4\",\"8\",\"5\",\"10\"\n\"Number of Skills Tested\",\"9\",\"6\",\"13\",\"13\",\"47\",\"16\",\"70\"\n\"Multiple-Images\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2713\"\n\"Video\",\"\u2717\",\"\u2717\",\"\u2713\",\"\u2713\",\"\u2717\",\"\u2717\",\"\u2717\"\n\"Multi-Turn Conversations\",\"\u2713\",\"\u2713\",\"\u2713\",\"\u2713\",\"\u2713\",\"\u2717\",\"\u2717\"\n\"Multilingual Conversations\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2717\",\"\u2717\"\n\"Instruction-conditioned Captions\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2713\"\n\"Chatbot-style Responses\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2713\"\n\"Dataset-specific Evaluation\",\"\u2713\",\"\u2713\",\"\u2713\",\"\u2713\",\"\u2713\",\"\u2717\",\"\u2717\"\n\"Human Evaluation\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2713\"\n\"Auto/GPT-4 Evaluation\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2713\",\"\u2713\"\n\"Win-rates*\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2713\",\"\u2713\"\n\"Elo Rating\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2717\",\"\u2713\",\"\u2717\",\"\u2713\""}, {"type": "text", "value": "**Table 1:** Comparison with related works for evaluating instruction-following vision-language models. Win-rates* refers to the model win-rates against a reference output/model.", "md": "**Table 1:** Comparison with related works for evaluating instruction-following vision-language models. Win-rates* refers to the model win-rates against a reference output/model."}]}, {"page": 4, "text": "2       VisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark\n                Instruction Generation                      Instruction-Conditioned Caption Generation                        Model Output Evaluation\n    \u201cCreate a new, challenging instruction                   \u201cProvide a highly descriptive caption to allow a     \u201cEvaluate model predictions by answering the\n    inspired by the seed task for that family\u201d               text-only recipient to follow the instruction\u201d                     following questions\u2026\u201d.\n    Task         Contextual Knowledge of Events                Instruction There is one color of candy          Instruction      I am the player with the white chess\n    Family                                                                     in the bowl that is missing                       pieces. Is there a move I can take on\n    Seed            Why is he waving? What                                     from the spoon. Which color                       this turn to capture one of my\n    Instruction     happened in this event?                                    is not represented in the                         opponent\u2019s pieces?\n    Image                                                                      spoon full of candies?           Instruction-     There is a wooden chess board [...]\n                                                                                                                Conditioned      The black rooks are at a8 and h8. The\n                                                               Image                                            Caption          black knights are at c6 and g8. [...]\n                                                                                                                Image\n    Output       Martin Luther King Jr. is waving\n                 to acknowledge and greet the\n                 crowd of protesters who have                                                                   GPT-4         Yes, you can capture the black pawn\n                 gathered to hear him speak [...]                                                               Output        at e5 with your white pawn at e4.\n    New            Why is this guy raising his hand?          Annotated       Brightly colored round           Output    Does GPT-4 correctly follow the instruction?\n    Instruction                                               Caption         candies, with colors of red,     Eval      [   ] Yes [X] No\n                                                                              orange, yellow, green, pink                If answered \u2018No\u2019 for the above, mark one of\n    Selected                                                                  and dark brown. A purple                   the following:\n    Image                                                                     spoon is raised above the bin              [   ] N/A\n                                                                              of candy, bringing them in for             [X] The dense caption is detailed enough,\n                                                                              a close-up. The spoon has                  the problem is in GPT-4\u2019s answer\n                                                                              red, orange, yellow, green                 [  ] The dense caption is underspecifi      ed, not\n                                                                              and dark brown candy in it.                GPT-4\u2019s fault\nFigure 3: Data collection steps: (1) Instruction Generation - Annotators derive instances\nfrom a seed task, see Figure 3 (left). (2) Caption Generation - Rich instruction-conditioned\ncaptions are produced for GPT-4 references, shown in Figure 3 (middle). (3) Model\nEvaluation - GPT-4 responses are human-validated, illustrated in Figure 3 (right). Top\nblocks show rater instructions; bottom blocks depict outputs.\nVisIT-Bench was built to emulate real-world applications of multimodal models through\nimage-text tasks, creating an extensive and practical benchmark. These tasks, or \u2018instruction\nfamilies\u2019, are seen as key capabilities of a high-performing vision-and-language model.\nAlthough our selections are not exhaustive, they provide a broad basis for evaluating beyond\nacademic benchmarks. We prioritize family coverage vs. number of instances-per-task. The\nfinal corpus, comprising 592 instances and 1,159 public images, can be found at VisIT-Bench\nSheet and VisIT-Bench Sheet Multi-Images. VisIT-Bench instances are either from 45\nnewly assembled instruction families or reformatted from 25 existing datasets (see Table 5).\nNotably, 10 instruction families cater to multi-image query scenarios (e.g., Figure 4).\n2.1        Data Collection\nThe authors of this work perform an initial annotation step of curating instruction families.\nFor each instruction family not derived from an existing task (45 out of 70), we designate\na name for the family (e.g., \u201cContextual Knowledge of Events\u201d) and identify an image-\ninstruction pair that exemplifies the category, along with a sample response (\u201cMartin Luther\nKing Jr. is waving to acknowledge and greet the crowd of protesters [...]\u201d). 10 sample\nfamiles are in Figure 2.\nThe following steps are carried out in collaboration with crowdworkers, who receive an\nhourly wage of $18. These steps are outlined in Figure 3: (1) taking the image/instruction\nexample as a guiding seed task crowdworkers formulate a new instruction that examines\nthe same instruction family (\u201cinstruction generation\u201d); (2) crowdworkers create detailed\nimage captions that describe the image and allow an entity, relying solely on this text, to\ninterpret and execute the given instruction successfully (\u201cinstruction-conditioned caption\ngeneration\u201d); (3) crowdworkers assess the correctness of GPT-4\u2019s response to the instruction\n                                                                                          4", "md": "# VisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark\n\n## VisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark\n\n|Instruction Generation|Instruction-Conditioned Caption Generation|Model Output Evaluation|\n|---|---|---|\n|\u201cCreate a new, challenging instruction inspired by the seed task for that family\u201d|\u201cProvide a highly descriptive caption to allow a text-only recipient to follow the instruction\u201d|\u201cEvaluate model predictions by answering the following questions\u2026\u201d|\n|**Task Family|Contextual Knowledge of Events|Instruction** There is one color of candy in the bowl that is missing|\n|**Seed Instruction|Why is he waving? What happened in this event?|Instruction** I am the player with the white chess pieces. Is there a move I can take on this turn to capture one of my opponent\u2019s pieces?|\n|**Image| |Instruction-Conditioned Caption** There is a wooden chess board [...] The black rooks are at a8 and h8. The black knights are at c6 and g8. [...]|\n|**Output|Martin Luther King Jr. is waving to acknowledge and greet the crowd of protesters who have gathered to hear him speak [...]|GPT-4 Output** Yes, you can capture the black pawn at e5 with your white pawn at e4.|\n|**New Instruction|Why is this guy raising his hand?|Annotated Caption** Brightly colored round candies, with colors of red, orange, yellow, green, pink and dark brown. A purple spoon is raised above the bin of candy, bringing them in for a close-up. The spoon has red, orange, yellow, green and dark brown candy in it.|\n|**Selected Image**| | |\n\n**Figure 3:** Data collection steps: (1) Instruction Generation - Annotators derive instances from a seed task, see Figure 3 (left). (2) Caption Generation - Rich instruction-conditioned captions are produced for GPT-4 references, shown in Figure 3 (middle). (3) Model Evaluation - GPT-4 responses are human-validated, illustrated in Figure 3 (right). Top blocks show rater instructions; bottom blocks depict outputs.\n\nVisIT-Bench was built to emulate real-world applications of multimodal models through image-text tasks, creating an extensive and practical benchmark. These tasks, or \u2018instruction families\u2019, are seen as key capabilities of a high-performing vision-and-language model. Although our selections are not exhaustive, they provide a broad basis for evaluating beyond academic benchmarks. We prioritize family coverage vs. number of instances-per-task. The final corpus, comprising 592 instances and 1,159 public images, can be found at VisIT-Bench Sheet and VisIT-Bench Sheet Multi-Images. VisIT-Bench instances are either from 45 newly assembled instruction families or reformatted from 25 existing datasets (see Table 5). Notably, 10 instruction families cater to multi-image query scenarios (e.g., Figure 4).\n\n### 2.1 Data Collection\n\nThe authors of this work perform an initial annotation step of curating instruction families. For each instruction family not derived from an existing task (45 out of 70), we designate a name for the family (e.g., \u201cContextual Knowledge of Events\u201d) and identify an image-instruction pair that exemplifies the category, along with a sample response (\u201cMartin Luther King Jr. is waving to acknowledge and greet the crowd of protesters [...]\u201d). 10 sample families are in Figure 2.\n\nThe following steps are carried out in collaboration with crowdworkers, who receive an hourly wage of $18. These steps are outlined in Figure 3: (1) taking the image/instruction example as a guiding seed task crowdworkers formulate a new instruction that examines the same instruction family (\u201cinstruction generation\u201d); (2) crowdworkers create detailed image captions that describe the image and allow an entity, relying solely on this text, to interpret and execute the given instruction successfully (\u201cinstruction-conditioned caption generation\u201d); (3) crowdworkers assess the correctness of GPT-4\u2019s response to the instruction.", "images": [{"name": "page-4-0.jpg", "height": 39, "width": 69, "x": 155, "y": 175}, {"name": "page-4-2.jpg", "height": 37, "width": 60, "x": 412, "y": 193}, {"name": "page-4-1.jpg", "height": 56, "width": 75, "x": 151, "y": 272}, {"name": "page-4-3.jpg", "height": 56, "width": 81, "x": 276, "y": 186}], "items": [{"type": "heading", "lvl": 1, "value": "VisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark", "md": "# VisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark"}, {"type": "heading", "lvl": 2, "value": "VisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark", "md": "## VisIT-Bench: A Real-World Inspired VL Instruction-Following Benchmark"}, {"type": "table", "rows": [["Instruction Generation", "Instruction-Conditioned Caption Generation", "Model Output Evaluation"], ["\u201cCreate a new, challenging instruction inspired by the seed task for that family\u201d", "\u201cProvide a highly descriptive caption to allow a text-only recipient to follow the instruction\u201d", "\u201cEvaluate model predictions by answering the following questions\u2026\u201d"], ["**Task Family", "Contextual Knowledge of Events", "Instruction** There is one color of candy in the bowl that is missing"], ["**Seed Instruction", "Why is he waving? What happened in this event?", "Instruction** I am the player with the white chess pieces. Is there a move I can take on this turn to capture one of my opponent\u2019s pieces?"], ["**Image", "", "Instruction-Conditioned Caption** There is a wooden chess board [...] The black rooks are at a8 and h8. The black knights are at c6 and g8. [...]"], ["**Output", "Martin Luther King Jr. is waving to acknowledge and greet the crowd of protesters who have gathered to hear him speak [...]", "GPT-4 Output** Yes, you can capture the black pawn at e5 with your white pawn at e4."], ["**New Instruction", "Why is this guy raising his hand?", "Annotated Caption** Brightly colored round candies, with colors of red, orange, yellow, green, pink and dark brown. A purple spoon is raised above the bin of candy, bringing them in for a close-up. The spoon has red, orange, yellow, green and dark brown candy in it."], ["**Selected Image**", "", ""]], "md": "|Instruction Generation|Instruction-Conditioned Caption Generation|Model Output Evaluation|\n|---|---|---|\n|\u201cCreate a new, challenging instruction inspired by the seed task for that family\u201d|\u201cProvide a highly descriptive caption to allow a text-only recipient to follow the instruction\u201d|\u201cEvaluate model predictions by answering the following questions\u2026\u201d|\n|**Task Family|Contextual Knowledge of Events|Instruction** There is one color of candy in the bowl that is missing|\n|**Seed Instruction|Why is he waving? What happened in this event?|Instruction** I am the player with the white chess pieces. Is there a move I can take on this turn to capture one of my opponent\u2019s pieces?|\n|**Image| |Instruction-Conditioned Caption** There is a wooden chess board [...] The black rooks are at a8 and h8. The black knights are at c6 and g8. [...]|\n|**Output|Martin Luther King Jr. is waving to acknowledge and greet the crowd of protesters who have gathered to hear him speak [...]|GPT-4 Output** Yes, you can capture the black pawn at e5 with your white pawn at e4.|\n|**New Instruction|Why is this guy raising his hand?|Annotated Caption** Brightly colored round candies, with colors of red, orange, yellow, green, pink and dark brown. A purple spoon is raised above the bin of candy, bringing them in for a close-up. The spoon has red, orange, yellow, green and dark brown candy in it.|\n|**Selected Image**| | |", "isPerfectTable": true, "csv": "\"Instruction Generation\",\"Instruction-Conditioned Caption Generation\",\"Model Output Evaluation\"\n\"\u201cCreate a new, challenging instruction inspired by the seed task for that family\u201d\",\"\u201cProvide a highly descriptive caption to allow a text-only recipient to follow the instruction\u201d\",\"\u201cEvaluate model predictions by answering the following questions\u2026\u201d\"\n\"**Task Family\",\"Contextual Knowledge of Events\",\"Instruction** There is one color of candy in the bowl that is missing\"\n\"**Seed Instruction\",\"Why is he waving? What happened in this event?\",\"Instruction** I am the player with the white chess pieces. Is there a move I can take on this turn to capture one of my opponent\u2019s pieces?\"\n\"**Image\",\"\",\"Instruction-Conditioned Caption** There is a wooden chess board [...] The black rooks are at a8 and h8. The black knights are at c6 and g8. [...]\"\n\"**Output\",\"Martin Luther King Jr. is waving to acknowledge and greet the crowd of protesters who have gathered to hear him speak [...]\",\"GPT-4 Output** Yes, you can capture the black pawn at e5 with your white pawn at e4.\"\n\"**New Instruction\",\"Why is this guy raising his hand?\",\"Annotated Caption** Brightly colored round candies, with colors of red, orange, yellow, green, pink and dark brown. A purple spoon is raised above the bin of candy, bringing them in for a close-up. The spoon has red, orange, yellow, green and dark brown candy in it.\"\n\"**Selected Image**\",\"\",\"\""}, {"type": "text", "value": "**Figure 3:** Data collection steps: (1) Instruction Generation - Annotators derive instances from a seed task, see Figure 3 (left). (2) Caption Generation - Rich instruction-conditioned captions are produced for GPT-4 references, shown in Figure 3 (middle). (3) Model Evaluation - GPT-4 responses are human-validated, illustrated in Figure 3 (right). Top blocks show rater instructions; bottom blocks depict outputs.\n\nVisIT-Bench was built to emulate real-world applications of multimodal models through image-text tasks, creating an extensive and practical benchmark. These tasks, or \u2018instruction families\u2019, are seen as key capabilities of a high-performing vision-and-language model. Although our selections are not exhaustive, they provide a broad basis for evaluating beyond academic benchmarks. We prioritize family coverage vs. number of instances-per-task. The final corpus, comprising 592 instances and 1,159 public images, can be found at VisIT-Bench Sheet and VisIT-Bench Sheet Multi-Images. VisIT-Bench instances are either from 45 newly assembled instruction families or reformatted from 25 existing datasets (see Table 5). Notably, 10 instruction families cater to multi-image query scenarios (e.g., Figure 4).", "md": "**Figure 3:** Data collection steps: (1) Instruction Generation - Annotators derive instances from a seed task, see Figure 3 (left). (2) Caption Generation - Rich instruction-conditioned captions are produced for GPT-4 references, shown in Figure 3 (middle). (3) Model Evaluation - GPT-4 responses are human-validated, illustrated in Figure 3 (right). Top blocks show rater instructions; bottom blocks depict outputs.\n\nVisIT-Bench was built to emulate real-world applications of multimodal models through image-text tasks, creating an extensive and practical benchmark. These tasks, or \u2018instruction families\u2019, are seen as key capabilities of a high-performing vision-and-language model. Although our selections are not exhaustive, they provide a broad basis for evaluating beyond academic benchmarks. We prioritize family coverage vs. number of instances-per-task. The final corpus, comprising 592 instances and 1,159 public images, can be found at VisIT-Bench Sheet and VisIT-Bench Sheet Multi-Images. VisIT-Bench instances are either from 45 newly assembled instruction families or reformatted from 25 existing datasets (see Table 5). Notably, 10 instruction families cater to multi-image query scenarios (e.g., Figure 4)."}, {"type": "heading", "lvl": 3, "value": "2.1 Data Collection", "md": "### 2.1 Data Collection"}, {"type": "text", "value": "The authors of this work perform an initial annotation step of curating instruction families. For each instruction family not derived from an existing task (45 out of 70), we designate a name for the family (e.g., \u201cContextual Knowledge of Events\u201d) and identify an image-instruction pair that exemplifies the category, along with a sample response (\u201cMartin Luther King Jr. is waving to acknowledge and greet the crowd of protesters [...]\u201d). 10 sample families are in Figure 2.\n\nThe following steps are carried out in collaboration with crowdworkers, who receive an hourly wage of $18. These steps are outlined in Figure 3: (1) taking the image/instruction example as a guiding seed task crowdworkers formulate a new instruction that examines the same instruction family (\u201cinstruction generation\u201d); (2) crowdworkers create detailed image captions that describe the image and allow an entity, relying solely on this text, to interpret and execute the given instruction successfully (\u201cinstruction-conditioned caption generation\u201d); (3) crowdworkers assess the correctness of GPT-4\u2019s response to the instruction.", "md": "The authors of this work perform an initial annotation step of curating instruction families. For each instruction family not derived from an existing task (45 out of 70), we designate a name for the family (e.g., \u201cContextual Knowledge of Events\u201d) and identify an image-instruction pair that exemplifies the category, along with a sample response (\u201cMartin Luther King Jr. is waving to acknowledge and greet the crowd of protesters [...]\u201d). 10 sample families are in Figure 2.\n\nThe following steps are carried out in collaboration with crowdworkers, who receive an hourly wage of $18. These steps are outlined in Figure 3: (1) taking the image/instruction example as a guiding seed task crowdworkers formulate a new instruction that examines the same instruction family (\u201cinstruction generation\u201d); (2) crowdworkers create detailed image captions that describe the image and allow an entity, relying solely on this text, to interpret and execute the given instruction successfully (\u201cinstruction-conditioned caption generation\u201d); (3) crowdworkers assess the correctness of GPT-4\u2019s response to the instruction."}]}, {"page": 5, "text": "                 NLVR2In this task you will be provided with two individual images i.e., BEFORE and AFTER. Please study them\n                      carefully and determine if the following sentence is correct (Answer \"Yes\"/\"No\"):\n                      None of the dogs in these images are wearing clothing.\n                                                           Two small chihuahuas, one with all white                                                            A small all white chihuahua is standing\n                                                           fur, and the other with a light brown coat                                                          and looking straight at the camera. It's\n                                                           of fur on its back, but white fur on its                                                            wearing a pink and white polka-dotted\n                                                           head and neck, are near a small pink                                                                dress, over which is a pink shirt with\n                                                           pillow that has large white words                                                                   some white text on it. Under the dog is a\n                                                           proclaiming, \"BE MINE\" on it. The all white                                                         cement walkway, and there is grass to the\n           1                                               chihuahua is resting its head on the                2                                               dog's right and to the right of the\n                                                           pillow, while the other one is laying                                                               walkway.\n                                                           behind it.\n                                                No. The second image has a dog wearing a pink and white polka-dotted dress and a pink\n                            GPT-4               shirt with white text on it.\n Figure 4: An example of a multi-image instruction task from VisIT-Bench. Originally\n sourced from NLVR2 [36], this task aims to evaluate visual reasoning skills. The NLVR2\n format includes a sentence to analyze, two images, and a binary response. Our adaptation\n adds a zero-shot instruction prompt, a instruction-conditioned caption for each image and a\n human-verified GPT-4 response. These enhancements, designed in a chatbot-style interaction,\n facilitate automatic evaluation of future chatbot responses to this instance.\n(\u201cmodel output evaluation\u201d). We further elaborate on these steps using human annotators\n below.\n Re-formatting existing datasets.                                                  25/70 instruction families (corresponding to 25*10=250\n instances) are re-formatted versions of existing vision-language tasks (See Appendix C for\n full list).2 This process involves re-formatting tasks into chatbot-style instruction/response\n versions. In re-formatting, we re-write instructions to retain the original task\u2019s goal while\n maintaining the original images, see Figure 4. These repurposed tasks are integrated into our\n data collection process, ensuring uniformity between the chatbot-style answers in the full\nVisIT-Bench instances and the reinterpreted tasks.\n Instruction Generation.                                        Here, annotators create a new instance from the same instruction\n family as a given example, along with an instruction and corresponding image. For instance,\n in Figure 3 (left), the instruction family is \u201cContextual Knowledge of Events\u201d, and the\n example instruction is \u201cWhy is he waving? What happened in this event?\u201d alongside an\n image of Martin Luther King, Jr. To collect images, annotators were instructed to use\n Openverse (https://openverse.org/) for Creative Commons licened images.\n Instruction-Conditioned Caption Generation.                                                                           Annotators are provided with the image\n and instruction, and are tasked to construct a caption that is rich enough to allow an entity,\n solely receiving the text they author, to follow the instruction. This caption will later facilitate\n GPT-4 reference candidate generation, and will be used for text-only auto-evaluation. We call\n these instructions instruction-conditioned captions. See Figure 3 (middle) for an example:\n an annotator doesn\u2019t just mention the skittles and a spoon, but, given the query regarding\n specific colors, they indicate the exact colors in detail.\n Model Output Evaluation.                                              The goal of this stage is to gather human-validated reference\n chatbot responses for each multimodal instruction query. We initially obtain response\n candidates from GPT-4 given the instruction and the instruction-conditioned caption. GPT4\u2019s\n prompt is: \u201cConsider an image depicted by: <caption>\u2019. Now, briefly follow this instruction,\n       2 Users of VisIT-Bench should also cite the original datasets.\n                                                                                                        5", "md": "# Document\n\n## NLVR2\n\nIn this task you will be provided with two individual images i.e., BEFORE and AFTER. Please study them carefully and determine if the following sentence is correct (Answer \"Yes\"/\"No\"):\n\nNone of the dogs in these images are wearing clothing.\n\n|BEFORE|AFTER|\n|---|---|\n|||\n\nTwo small chihuahuas, one with all white fur, and the other with a light brown coat of fur on its back, but white fur on its head and neck, are near a small pink pillow that has large white words proclaiming, \"BE MINE\" on it. The all white chihuahua is resting its head on the pillow, while the other one is laying behind it.\n\nNo. The second image has a dog wearing a pink and white polka-dotted dress and a pink shirt with white text on it.\n\nFigure 4: An example of a multi-image instruction task from VisIT-Bench. Originally sourced from NLVR2 [36], this task aims to evaluate visual reasoning skills. The NLVR2 format includes a sentence to analyze, two images, and a binary response. Our adaptation adds a zero-shot instruction prompt, an instruction-conditioned caption for each image and a human-verified GPT-4 response. These enhancements, designed in a chatbot-style interaction, facilitate automatic evaluation of future chatbot responses to this instance. (\"model output evaluation\"). We further elaborate on these steps using human annotators below.\n\n## Re-formatting existing datasets\n\n25/70 instruction families (corresponding to 25*10=250 instances) are re-formatted versions of existing vision-language tasks (See Appendix C for full list). This process involves re-formatting tasks into chatbot-style instruction/response versions. In re-formatting, we re-write instructions to retain the original task\u2019s goal while maintaining the original images, see Figure 4. These repurposed tasks are integrated into our data collection process, ensuring uniformity between the chatbot-style answers in the full VisIT-Bench instances and the reinterpreted tasks.\n\n## Instruction Generation\n\nHere, annotators create a new instance from the same instruction family as a given example, along with an instruction and corresponding image. For instance, in Figure 3 (left), the instruction family is \u201cContextual Knowledge of Events\u201d, and the example instruction is \u201cWhy is he waving? What happened in this event?\u201d alongside an image of Martin Luther King, Jr. To collect images, annotators were instructed to use Openverse (https://openverse.org/) for Creative Commons licensed images.\n\n## Instruction-Conditioned Caption Generation\n\nAnnotators are provided with the image and instruction, and are tasked to construct a caption that is rich enough to allow an entity, solely receiving the text they author, to follow the instruction. This caption will later facilitate GPT-4 reference candidate generation, and will be used for text-only auto-evaluation. We call these instructions instruction-conditioned captions. See Figure 3 (middle) for an example: an annotator doesn\u2019t just mention the skittles and a spoon, but, given the query regarding specific colors, they indicate the exact colors in detail.\n\n## Model Output Evaluation\n\nThe goal of this stage is to gather human-validated reference chatbot responses for each multimodal instruction query. We initially obtain response candidates from GPT-4 given the instruction and the instruction-conditioned caption. GPT-4\u2019s prompt is: \u201cConsider an image depicted by: &lt;caption&gt;\u2019. Now, briefly follow this instruction.\n\nUsers of VisIT-Bench should also cite the original datasets.", "images": [{"name": "page-5-3.jpg", "height": 21, "width": 21, "x": 159, "y": 210}, {"name": "page-5-1.jpg", "height": 64, "width": 61, "x": 340, "y": 138}, {"name": "page-5-2.jpg", "height": 32, "width": 38, "x": 123, "y": 208}, {"name": "page-5-0.jpg", "height": 63, "width": 61, "x": 149, "y": 138}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "NLVR2", "md": "## NLVR2"}, {"type": "text", "value": "In this task you will be provided with two individual images i.e., BEFORE and AFTER. Please study them carefully and determine if the following sentence is correct (Answer \"Yes\"/\"No\"):\n\nNone of the dogs in these images are wearing clothing.", "md": "In this task you will be provided with two individual images i.e., BEFORE and AFTER. Please study them carefully and determine if the following sentence is correct (Answer \"Yes\"/\"No\"):\n\nNone of the dogs in these images are wearing clothing."}, {"type": "table", "rows": [["BEFORE", "AFTER"], ["", ""]], "md": "|BEFORE|AFTER|\n|---|---|\n|||", "isPerfectTable": true, "csv": "\"BEFORE\",\"AFTER\"\n\"\",\"\""}, {"type": "text", "value": "Two small chihuahuas, one with all white fur, and the other with a light brown coat of fur on its back, but white fur on its head and neck, are near a small pink pillow that has large white words proclaiming, \"BE MINE\" on it. The all white chihuahua is resting its head on the pillow, while the other one is laying behind it.\n\nNo. The second image has a dog wearing a pink and white polka-dotted dress and a pink shirt with white text on it.\n\nFigure 4: An example of a multi-image instruction task from VisIT-Bench. Originally sourced from NLVR2 [36], this task aims to evaluate visual reasoning skills. The NLVR2 format includes a sentence to analyze, two images, and a binary response. Our adaptation adds a zero-shot instruction prompt, an instruction-conditioned caption for each image and a human-verified GPT-4 response. These enhancements, designed in a chatbot-style interaction, facilitate automatic evaluation of future chatbot responses to this instance. (\"model output evaluation\"). We further elaborate on these steps using human annotators below.", "md": "Two small chihuahuas, one with all white fur, and the other with a light brown coat of fur on its back, but white fur on its head and neck, are near a small pink pillow that has large white words proclaiming, \"BE MINE\" on it. The all white chihuahua is resting its head on the pillow, while the other one is laying behind it.\n\nNo. The second image has a dog wearing a pink and white polka-dotted dress and a pink shirt with white text on it.\n\nFigure 4: An example of a multi-image instruction task from VisIT-Bench. Originally sourced from NLVR2 [36], this task aims to evaluate visual reasoning skills. The NLVR2 format includes a sentence to analyze, two images, and a binary response. Our adaptation adds a zero-shot instruction prompt, an instruction-conditioned caption for each image and a human-verified GPT-4 response. These enhancements, designed in a chatbot-style interaction, facilitate automatic evaluation of future chatbot responses to this instance. (\"model output evaluation\"). We further elaborate on these steps using human annotators below."}, {"type": "heading", "lvl": 2, "value": "Re-formatting existing datasets", "md": "## Re-formatting existing datasets"}, {"type": "text", "value": "25/70 instruction families (corresponding to 25*10=250 instances) are re-formatted versions of existing vision-language tasks (See Appendix C for full list). This process involves re-formatting tasks into chatbot-style instruction/response versions. In re-formatting, we re-write instructions to retain the original task\u2019s goal while maintaining the original images, see Figure 4. These repurposed tasks are integrated into our data collection process, ensuring uniformity between the chatbot-style answers in the full VisIT-Bench instances and the reinterpreted tasks.", "md": "25/70 instruction families (corresponding to 25*10=250 instances) are re-formatted versions of existing vision-language tasks (See Appendix C for full list). This process involves re-formatting tasks into chatbot-style instruction/response versions. In re-formatting, we re-write instructions to retain the original task\u2019s goal while maintaining the original images, see Figure 4. These repurposed tasks are integrated into our data collection process, ensuring uniformity between the chatbot-style answers in the full VisIT-Bench instances and the reinterpreted tasks."}, {"type": "heading", "lvl": 2, "value": "Instruction Generation", "md": "## Instruction Generation"}, {"type": "text", "value": "Here, annotators create a new instance from the same instruction family as a given example, along with an instruction and corresponding image. For instance, in Figure 3 (left), the instruction family is \u201cContextual Knowledge of Events\u201d, and the example instruction is \u201cWhy is he waving? What happened in this event?\u201d alongside an image of Martin Luther King, Jr. To collect images, annotators were instructed to use Openverse (https://openverse.org/) for Creative Commons licensed images.", "md": "Here, annotators create a new instance from the same instruction family as a given example, along with an instruction and corresponding image. For instance, in Figure 3 (left), the instruction family is \u201cContextual Knowledge of Events\u201d, and the example instruction is \u201cWhy is he waving? What happened in this event?\u201d alongside an image of Martin Luther King, Jr. To collect images, annotators were instructed to use Openverse (https://openverse.org/) for Creative Commons licensed images."}, {"type": "heading", "lvl": 2, "value": "Instruction-Conditioned Caption Generation", "md": "## Instruction-Conditioned Caption Generation"}, {"type": "text", "value": "Annotators are provided with the image and instruction, and are tasked to construct a caption that is rich enough to allow an entity, solely receiving the text they author, to follow the instruction. This caption will later facilitate GPT-4 reference candidate generation, and will be used for text-only auto-evaluation. We call these instructions instruction-conditioned captions. See Figure 3 (middle) for an example: an annotator doesn\u2019t just mention the skittles and a spoon, but, given the query regarding specific colors, they indicate the exact colors in detail.", "md": "Annotators are provided with the image and instruction, and are tasked to construct a caption that is rich enough to allow an entity, solely receiving the text they author, to follow the instruction. This caption will later facilitate GPT-4 reference candidate generation, and will be used for text-only auto-evaluation. We call these instructions instruction-conditioned captions. See Figure 3 (middle) for an example: an annotator doesn\u2019t just mention the skittles and a spoon, but, given the query regarding specific colors, they indicate the exact colors in detail."}, {"type": "heading", "lvl": 2, "value": "Model Output Evaluation", "md": "## Model Output Evaluation"}, {"type": "text", "value": "The goal of this stage is to gather human-validated reference chatbot responses for each multimodal instruction query. We initially obtain response candidates from GPT-4 given the instruction and the instruction-conditioned caption. GPT-4\u2019s prompt is: \u201cConsider an image depicted by: &lt;caption&gt;\u2019. Now, briefly follow this instruction.\n\nUsers of VisIT-Bench should also cite the original datasets.", "md": "The goal of this stage is to gather human-validated reference chatbot responses for each multimodal instruction query. We initially obtain response candidates from GPT-4 given the instruction and the instruction-conditioned caption. GPT-4\u2019s prompt is: \u201cConsider an image depicted by: &lt;caption&gt;\u2019. Now, briefly follow this instruction.\n\nUsers of VisIT-Bench should also cite the original datasets."}]}, {"page": 6, "text": "and you can add a short explanation: <instruction>\u2019. Response: This prompt is employed\nfor both single and multiple image instances, with appropriate modifications for the latter.\nThen we verify each response with human annotators.3 If a response is marked incorrect, the\nannotator identifies whether the issue lies with the detail level of the instruction-conditioned\ncaptions or with GPT-4\u2019s response itself. For VisIT-Bench, we discard any case marked\nas incorrect for either reason.4 An example is given in Figure 3 (right), where GPT-4\u2019s\ncandidate reference response aims to answer a question about a chess position (which it does\nso incorrectly, and thus, the instance is discarded).\n2.2    Data Collection Annotation and Results\nWe conduct the data collection steps in Figure 3 using Amazon\u2019s Mechanical Turk (MTurk)\nplatform. Prior to annotating, each MTurk worker passed a qualification test, which involved\nfi\n ve to ten sample tasks designed to assess their ability to generate high-quality annotations.\nMore detailed information about the execution process and full user interface examples can\nbe found in Appendix B.\nOur annotation results are sum-\nmarized in Table 2.         We mea-       Table 2: Human rating metrics for the VisIT-Bench\nsure the throughput of the col-           dataset: overall, single-, and multi-image tasks.\nlection and filtration pipeline.\nFor single-image instances, our             Metrics                          Overall      Single     Multi\npipeline\u2019s yield was 91.5% from             GPT-4 Correct (%)                    87.3        91.5      63.0\nthe original candidate set. How-            Problem in Caption (%)                 4.0        3.6        6.0\never, the success rate dropped              Problem in GPT-4 (%)                   7.7        3.8      30.0\nto 63.0% in the more complex\nmulti-image tasks, accompanied\nby an uptick in issues either in the captions (6.0%) or GPT-4\u2019s responses (30.0%). This drop\nsuggests that multi-image queries may pose a more difficult data collection challenge.\n3    VisIT-Bench Analysis\nWe analyze the tasks, images, and instruction-conditioned captions of VisIT-Bench.\n3.1    Are instruction-conditioned captions necessary?\nTo clarify the role of the instruction-conditioned captions we collect, we conducted an\nexperiment covering 150 single-image instances. Instead of using our instruction-conditioned\ncaptions, we use BLIP2 [15] image captions, which is a state-of-the-art image captioning\nmodel. We extract image captions, and feed them to GPT-4 as detailed earlier, to provide a\ntext-based chatbot response. This process is depicted in Figure 5.\nWe manually evaluated whether the resulting output accurately followed the instructions. We\nfind that while instruction-conditioned captions led to correct outputs in 91% of the cases,\nthe success rate fell to 31% when using BLIP2 captions (Table 2). These results highlight the\nimportance of instruction-conditioned captions in the construction of VisIT-Bench, and show\nthat the instances in our dataset are sophisticated enough such that most are not solvable by\nusing a simple Socratic model [38] baseline of caption \u2192               LLM.\n   3An alternate annotation scheme would have been to task annotators to write target responses from scratch.\nThe rationale for using GPT-4 verification instead is derived from prior results that show promising human-\nmachine collaboration of this form [37].\n   4The annotators are also tasked to screen for any offensive, unsound, or harmful advice present in the\nresponses. We did not find or discard any instances.\n                                                       6", "md": "and you can add a short explanation: `<instruction>`. Response: This prompt is employed\nfor both single and multiple image instances, with appropriate modifications for the latter.\nThen we verify each response with human annotators. If a response is marked incorrect, the\nannotator identifies whether the issue lies with the detail level of the instruction-conditioned\ncaptions or with GPT-4's response itself. For VisIT-Bench, we discard any case marked\nas incorrect for either reason. An example is given in Figure 3 (right), where GPT-4's\ncandidate reference response aims to answer a question about a chess position (which it does\nso incorrectly, and thus, the instance is discarded).\n\n### Data Collection Annotation and Results\nWe conduct the data collection steps in Figure 3 using Amazon's Mechanical Turk (MTurk)\nplatform. Prior to annotating, each MTurk worker passed a qualification test, which involved\nfive to ten sample tasks designed to assess their ability to generate high-quality annotations.\nMore detailed information about the execution process and full user interface examples can\nbe found in Appendix B.\n\nOur annotation results are summarized in Table 2. We measure the throughput of the collection and filtration pipeline.\nFor single-image instances, our pipeline's yield was 91.5% from the original candidate set. However, the success rate dropped\nto 63.0% in the more complex multi-image tasks, accompanied by an uptick in issues either in the captions (6.0%) or GPT-4's responses (30.0%). This drop\nsuggests that multi-image queries may pose a more difficult data collection challenge.\n\n### VisIT-Bench Analysis\nWe analyze the tasks, images, and instruction-conditioned captions of VisIT-Bench.\n\n#### Are instruction-conditioned captions necessary?\nTo clarify the role of the instruction-conditioned captions we collect, we conducted an\nexperiment covering 150 single-image instances. Instead of using our instruction-conditioned\ncaptions, we use BLIP2 [15] image captions, which is a state-of-the-art image captioning\nmodel. We extract image captions, and feed them to GPT-4 as detailed earlier, to provide a\ntext-based chatbot response. This process is depicted in Figure 5.\n\nWe manually evaluated whether the resulting output accurately followed the instructions. We\nfind that while instruction-conditioned captions led to correct outputs in 91% of the cases,\nthe success rate fell to 31% when using BLIP2 captions (Table 2). These results highlight the\nimportance of instruction-conditioned captions in the construction of VisIT-Bench, and show\nthat the instances in our dataset are sophisticated enough such that most are not solvable by\nusing a simple Socratic model baseline of caption \u2192 LLM.\n\nAn alternate annotation scheme would have been to task annotators to write target responses from scratch.\nThe rationale for using GPT-4 verification instead is derived from prior results that show promising human-machine collaboration of this form.\n\nThe annotators are also tasked to screen for any offensive, unsound, or harmful advice present in the responses. We did not find or discard any instances.\n\n|Metrics|Overall|Single|Multi|\n|---|---|---|---|\n|GPT-4 Correct (%)|87.3|91.5|63.0|\n|Problem in Caption (%)|4.0|3.6|6.0|\n|Problem in GPT-4 (%)|7.7|3.8|30.0|", "images": [], "items": [{"type": "text", "value": "and you can add a short explanation: `<instruction>`. Response: This prompt is employed\nfor both single and multiple image instances, with appropriate modifications for the latter.\nThen we verify each response with human annotators. If a response is marked incorrect, the\nannotator identifies whether the issue lies with the detail level of the instruction-conditioned\ncaptions or with GPT-4's response itself. For VisIT-Bench, we discard any case marked\nas incorrect for either reason. An example is given in Figure 3 (right), where GPT-4's\ncandidate reference response aims to answer a question about a chess position (which it does\nso incorrectly, and thus, the instance is discarded).", "md": "and you can add a short explanation: `<instruction>`. Response: This prompt is employed\nfor both single and multiple image instances, with appropriate modifications for the latter.\nThen we verify each response with human annotators. If a response is marked incorrect, the\nannotator identifies whether the issue lies with the detail level of the instruction-conditioned\ncaptions or with GPT-4's response itself. For VisIT-Bench, we discard any case marked\nas incorrect for either reason. An example is given in Figure 3 (right), where GPT-4's\ncandidate reference response aims to answer a question about a chess position (which it does\nso incorrectly, and thus, the instance is discarded)."}, {"type": "heading", "lvl": 3, "value": "Data Collection Annotation and Results", "md": "### Data Collection Annotation and Results"}, {"type": "text", "value": "We conduct the data collection steps in Figure 3 using Amazon's Mechanical Turk (MTurk)\nplatform. Prior to annotating, each MTurk worker passed a qualification test, which involved\nfive to ten sample tasks designed to assess their ability to generate high-quality annotations.\nMore detailed information about the execution process and full user interface examples can\nbe found in Appendix B.\n\nOur annotation results are summarized in Table 2. We measure the throughput of the collection and filtration pipeline.\nFor single-image instances, our pipeline's yield was 91.5% from the original candidate set. However, the success rate dropped\nto 63.0% in the more complex multi-image tasks, accompanied by an uptick in issues either in the captions (6.0%) or GPT-4's responses (30.0%). This drop\nsuggests that multi-image queries may pose a more difficult data collection challenge.", "md": "We conduct the data collection steps in Figure 3 using Amazon's Mechanical Turk (MTurk)\nplatform. Prior to annotating, each MTurk worker passed a qualification test, which involved\nfive to ten sample tasks designed to assess their ability to generate high-quality annotations.\nMore detailed information about the execution process and full user interface examples can\nbe found in Appendix B.\n\nOur annotation results are summarized in Table 2. We measure the throughput of the collection and filtration pipeline.\nFor single-image instances, our pipeline's yield was 91.5% from the original candidate set. However, the success rate dropped\nto 63.0% in the more complex multi-image tasks, accompanied by an uptick in issues either in the captions (6.0%) or GPT-4's responses (30.0%). This drop\nsuggests that multi-image queries may pose a more difficult data collection challenge."}, {"type": "heading", "lvl": 3, "value": "VisIT-Bench Analysis", "md": "### VisIT-Bench Analysis"}, {"type": "text", "value": "We analyze the tasks, images, and instruction-conditioned captions of VisIT-Bench.", "md": "We analyze the tasks, images, and instruction-conditioned captions of VisIT-Bench."}, {"type": "heading", "lvl": 4, "value": "Are instruction-conditioned captions necessary?", "md": "#### Are instruction-conditioned captions necessary?"}, {"type": "text", "value": "To clarify the role of the instruction-conditioned captions we collect, we conducted an\nexperiment covering 150 single-image instances. Instead of using our instruction-conditioned\ncaptions, we use BLIP2 [15] image captions, which is a state-of-the-art image captioning\nmodel. We extract image captions, and feed them to GPT-4 as detailed earlier, to provide a\ntext-based chatbot response. This process is depicted in Figure 5.\n\nWe manually evaluated whether the resulting output accurately followed the instructions. We\nfind that while instruction-conditioned captions led to correct outputs in 91% of the cases,\nthe success rate fell to 31% when using BLIP2 captions (Table 2). These results highlight the\nimportance of instruction-conditioned captions in the construction of VisIT-Bench, and show\nthat the instances in our dataset are sophisticated enough such that most are not solvable by\nusing a simple Socratic model baseline of caption \u2192 LLM.\n\nAn alternate annotation scheme would have been to task annotators to write target responses from scratch.\nThe rationale for using GPT-4 verification instead is derived from prior results that show promising human-machine collaboration of this form.\n\nThe annotators are also tasked to screen for any offensive, unsound, or harmful advice present in the responses. We did not find or discard any instances.", "md": "To clarify the role of the instruction-conditioned captions we collect, we conducted an\nexperiment covering 150 single-image instances. Instead of using our instruction-conditioned\ncaptions, we use BLIP2 [15] image captions, which is a state-of-the-art image captioning\nmodel. We extract image captions, and feed them to GPT-4 as detailed earlier, to provide a\ntext-based chatbot response. This process is depicted in Figure 5.\n\nWe manually evaluated whether the resulting output accurately followed the instructions. We\nfind that while instruction-conditioned captions led to correct outputs in 91% of the cases,\nthe success rate fell to 31% when using BLIP2 captions (Table 2). These results highlight the\nimportance of instruction-conditioned captions in the construction of VisIT-Bench, and show\nthat the instances in our dataset are sophisticated enough such that most are not solvable by\nusing a simple Socratic model baseline of caption \u2192 LLM.\n\nAn alternate annotation scheme would have been to task annotators to write target responses from scratch.\nThe rationale for using GPT-4 verification instead is derived from prior results that show promising human-machine collaboration of this form.\n\nThe annotators are also tasked to screen for any offensive, unsound, or harmful advice present in the responses. We did not find or discard any instances."}, {"type": "table", "rows": [["Metrics", "Overall", "Single", "Multi"], ["GPT-4 Correct (%)", "87.3", "91.5", "63.0"], ["Problem in Caption (%)", "4.0", "3.6", "6.0"], ["Problem in GPT-4 (%)", "7.7", "3.8", "30.0"]], "md": "|Metrics|Overall|Single|Multi|\n|---|---|---|---|\n|GPT-4 Correct (%)|87.3|91.5|63.0|\n|Problem in Caption (%)|4.0|3.6|6.0|\n|Problem in GPT-4 (%)|7.7|3.8|30.0|", "isPerfectTable": true, "csv": "\"Metrics\",\"Overall\",\"Single\",\"Multi\"\n\"GPT-4 Correct (%)\",\"87.3\",\"91.5\",\"63.0\"\n\"Problem in Caption (%)\",\"4.0\",\"3.6\",\"6.0\"\n\"Problem in GPT-4 (%)\",\"7.7\",\"3.8\",\"30.0\""}]}, {"page": 7, "text": "                                                                                      BLIP2 Caption                                   It is not possible to determine if a\n                                                                                                                                      disabled wheelchair-bound individual\n                                                                        a sign on a brick wall                                        would find it easy to go into the store\n                                                                                                                           GPT-4      based on the image of \"a sign on a brick\n                                                                                                                                      wall.\" Additional information about the           Overall\n                                                                           Instruction-Conditioned Caption                            store's entrance, accessibility features,        Success:\n                                                                                                                                      and layout is needed to make that                  31%\n                                                                        This image is of an exterior store front                      assessment.\n                                                                        and sidewalk, showing two portable\n                                                                        wedge ramps, one green and one red.\n                                                                        The ramps are positioned against the                          Yes, a disabled wheelchair-bound\n                                                                        step to two retail doorways, and seem to                      individual would likely find it easy to go\n                                                                        bridge the vertical distance between the           GPT-4      into this store, as the portable wedge\n   Would a disabled wheelchair bound                                    level of the sidewalk and the step up to                      ramps provide a smooth transition from            Overall\n   individual find it easy to go into this store?                       the doorway. The ramps are printed with                       the sidewalk to the doorway, making it           Success:\n                                                                        \"STOPGAP.CA\" on the top surface.                              more accessible for them.                          91%\nFigure 5: This experiment evaluates the value of instruction-conditioned captions in accurate\ninstruction-following tasks. Given an image and instruction, GPT-4 generates responses using\nboth a instruction-conditioned caption and a less detailed BLIP-2 [15] caption. The latter\u2019s\nimprecision leads to an error, emphasizing the need for detailed, task-specific captions.\n3.2          What skills are required for VisIT-Bench?\nThe full list of instruction families we cover are in Appendix Table 6.                                                                                                          Following\n[39], for the VisIT-Bench instructions, we extract the most frequent root verbs and\ntheir direct nouns (a full plot is in Figure 6).                                                                        The most common include:                                                \u2018an-\nswer question\u2019, \u2018write story/poem\u2019, \u2018create title\u2019, etc.                                                                                There\u2019s also a long-tail of\ndiverse requests that demand comprehension, commonsense, and cross-modal under-\nstanding, e.g., \u2018identifying objects\u2019 to \u2018need ingredient\u2019 to \u2018connect device\u2019.                                                                                                                 Ad-\nditional qualitative examination reveals a range of underlying skills required rang-\ning from \u2018emotion identification\u2019 to complex reasoning tasks such as \u2018paper folding\u2019.\n3.3          What is                                                                                                              question\ncontained in VisIT-Bench images?                                                                                                                           poem\nWe detect all the COCO [20] ob-\njects present in the images from our                                                                                       1                      WtIte                story\ndataset using Yolov5-L [40]; The\nmost common detected objects in\nVisIT-Bench are \u201cperson\u201d (\u223c                                     900 de-\ntections), chair, and car (\u223c                              100). But,                                                                                                          artist\na long tail of rarer objects exists as\nwell: full distribution in Appendix\nFigure 10. Overall, to perform well                                                                                   1                                7      1\nat VisIT-Bench, a model must ac-\ncount for a broad range of scenes                                                                                                                 3\nand objects.\n                                                                                    Figure 6: Most frequently occurring verbs (inner cir-\n4        Experiments                                                                cle) and their top 4 direct nouns (outer circle) in the\nWe evaluate a range of state-of-the-                                                VisIT-Bench instructions.\nart publicly accessible vision-and-\nlanguage chatbots on the 592 in-\nstances in VisIT-Bench. In \u00a74.1, we\nprovide the details of the instruction-\nfollowing models in our benchmark.\nFollowing this, we collect the human preferences for pairwise model generations to achieve a\nhuman-guided Elo ranking and the win-rates against the reference of the models in \u00a74.2. We\n                                                                                                   7", "md": "```markdown\n$$\n\\text{BLIP2 Caption} \\\\\n\\text{It is not possible to determine if a disabled wheelchair-bound individual} \\\\\n\\text{would find it easy to go into the store based on the image of \"a sign on a brick wall.\" Additional information about the store's entrance, accessibility features, and layout is needed to make that assessment.} \\\\\n\\text{Overall Success: 31\\%}\n$$\n\n$$\n\\text{Instruction-Conditioned Caption} \\\\\n\\text{This image is of an exterior store front and sidewalk, showing two portable wedge ramps, one green and one red.} \\\\\n\\text{The ramps are positioned against the step to two retail doorways, and seem to bridge the vertical distance between the level of the sidewalk and the step up to the doorway. The ramps are printed with \"STOPGAP.CA\" on the top surface.} \\\\\n\\text{Yes, a disabled wheelchair-bound individual would likely find it easy to go into this store, as the portable wedge ramps provide a smooth transition from the sidewalk to the doorway, making it more accessible for them.} \\\\\n\\text{Overall Success: 91\\%}\n$$\n\n**Figure 5:** This experiment evaluates the value of instruction-conditioned captions in accurate instruction-following tasks. Given an image and instruction, GPT-4 generates responses using both an instruction-conditioned caption and a less detailed BLIP-2 caption. The latter\u2019s imprecision leads to an error, emphasizing the need for detailed, task-specific captions.\n\n### 3.2 What skills are required for VisIT-Bench?\nThe full list of instruction families we cover are in Appendix Table 6. For the VisIT-Bench instructions, we extract the most frequent root verbs and their direct nouns. The most common include: \u2018answer question\u2019, \u2018write story/poem\u2019, \u2018create title\u2019, etc. There\u2019s also a long-tail of diverse requests that demand comprehension, commonsense, and cross-modal understanding, e.g., \u2018identifying objects\u2019 to \u2018need ingredient\u2019 to \u2018connect device\u2019. Additional qualitative examination reveals a range of underlying skills required ranging from \u2018emotion identification\u2019 to complex reasoning tasks such as \u2018paper folding\u2019.\n\n### 3.3 What is contained in VisIT-Bench images?\nWe detect all the COCO objects present in the images from our dataset using Yolov5-L. The most common detected objects in VisIT-Bench are \u201cperson\u201d (~900 detections), chair, and car (~100). But, a long tail of rarer objects exists as well: full distribution in Appendix Figure 10. Overall, to perform well at VisIT-Bench, a model must account for a broad range of scenes and objects.\n\n**Figure 6:** Most frequently occurring verbs (inner circle) and their top 4 direct nouns (outer circle) in the VisIT-Bench instructions.\n\n### 4 Experiments\nWe evaluate a range of state-of-the-art publicly accessible vision-and-language chatbots on the 592 instances in VisIT-Bench. In \u00a74.1, we provide the details of the instruction-following models in our benchmark. Following this, we collect the human preferences for pairwise model generations to achieve a human-guided Elo ranking and the win-rates against the reference of the models in \u00a74.2.\n```", "images": [{"name": "page-7-2.jpg", "height": 24, "width": 24, "x": 345, "y": 72}, {"name": "page-7-0.jpg", "height": 80, "width": 106, "x": 110, "y": 82}, {"name": "page-7-3.jpg", "height": 24, "width": 24, "x": 345, "y": 132}, {"name": "page-7-1.jpg", "height": 24, "width": 24, "x": 219, "y": 122}, {"name": "page-7-4.jpg", "height": 24, "width": 23, "x": 467, "y": 141}, {"name": "page-7-6.jpg", "height": 193, "width": 230, "x": 274, "y": 397}, {"name": "page-7-5.jpg", "height": 24, "width": 24, "x": 467, "y": 84}], "items": [{"type": "text", "value": "```markdown\n$$\n\\text{BLIP2 Caption} \\\\\n\\text{It is not possible to determine if a disabled wheelchair-bound individual} \\\\\n\\text{would find it easy to go into the store based on the image of \"a sign on a brick wall.\" Additional information about the store's entrance, accessibility features, and layout is needed to make that assessment.} \\\\\n\\text{Overall Success: 31\\%}\n$$\n\n$$\n\\text{Instruction-Conditioned Caption} \\\\\n\\text{This image is of an exterior store front and sidewalk, showing two portable wedge ramps, one green and one red.} \\\\\n\\text{The ramps are positioned against the step to two retail doorways, and seem to bridge the vertical distance between the level of the sidewalk and the step up to the doorway. The ramps are printed with \"STOPGAP.CA\" on the top surface.} \\\\\n\\text{Yes, a disabled wheelchair-bound individual would likely find it easy to go into this store, as the portable wedge ramps provide a smooth transition from the sidewalk to the doorway, making it more accessible for them.} \\\\\n\\text{Overall Success: 91\\%}\n$$\n\n**Figure 5:** This experiment evaluates the value of instruction-conditioned captions in accurate instruction-following tasks. Given an image and instruction, GPT-4 generates responses using both an instruction-conditioned caption and a less detailed BLIP-2 caption. The latter\u2019s imprecision leads to an error, emphasizing the need for detailed, task-specific captions.", "md": "```markdown\n$$\n\\text{BLIP2 Caption} \\\\\n\\text{It is not possible to determine if a disabled wheelchair-bound individual} \\\\\n\\text{would find it easy to go into the store based on the image of \"a sign on a brick wall.\" Additional information about the store's entrance, accessibility features, and layout is needed to make that assessment.} \\\\\n\\text{Overall Success: 31\\%}\n$$\n\n$$\n\\text{Instruction-Conditioned Caption} \\\\\n\\text{This image is of an exterior store front and sidewalk, showing two portable wedge ramps, one green and one red.} \\\\\n\\text{The ramps are positioned against the step to two retail doorways, and seem to bridge the vertical distance between the level of the sidewalk and the step up to the doorway. The ramps are printed with \"STOPGAP.CA\" on the top surface.} \\\\\n\\text{Yes, a disabled wheelchair-bound individual would likely find it easy to go into this store, as the portable wedge ramps provide a smooth transition from the sidewalk to the doorway, making it more accessible for them.} \\\\\n\\text{Overall Success: 91\\%}\n$$\n\n**Figure 5:** This experiment evaluates the value of instruction-conditioned captions in accurate instruction-following tasks. Given an image and instruction, GPT-4 generates responses using both an instruction-conditioned caption and a less detailed BLIP-2 caption. The latter\u2019s imprecision leads to an error, emphasizing the need for detailed, task-specific captions."}, {"type": "heading", "lvl": 3, "value": "3.2 What skills are required for VisIT-Bench?", "md": "### 3.2 What skills are required for VisIT-Bench?"}, {"type": "text", "value": "The full list of instruction families we cover are in Appendix Table 6. For the VisIT-Bench instructions, we extract the most frequent root verbs and their direct nouns. The most common include: \u2018answer question\u2019, \u2018write story/poem\u2019, \u2018create title\u2019, etc. There\u2019s also a long-tail of diverse requests that demand comprehension, commonsense, and cross-modal understanding, e.g., \u2018identifying objects\u2019 to \u2018need ingredient\u2019 to \u2018connect device\u2019. Additional qualitative examination reveals a range of underlying skills required ranging from \u2018emotion identification\u2019 to complex reasoning tasks such as \u2018paper folding\u2019.", "md": "The full list of instruction families we cover are in Appendix Table 6. For the VisIT-Bench instructions, we extract the most frequent root verbs and their direct nouns. The most common include: \u2018answer question\u2019, \u2018write story/poem\u2019, \u2018create title\u2019, etc. There\u2019s also a long-tail of diverse requests that demand comprehension, commonsense, and cross-modal understanding, e.g., \u2018identifying objects\u2019 to \u2018need ingredient\u2019 to \u2018connect device\u2019. Additional qualitative examination reveals a range of underlying skills required ranging from \u2018emotion identification\u2019 to complex reasoning tasks such as \u2018paper folding\u2019."}, {"type": "heading", "lvl": 3, "value": "3.3 What is contained in VisIT-Bench images?", "md": "### 3.3 What is contained in VisIT-Bench images?"}, {"type": "text", "value": "We detect all the COCO objects present in the images from our dataset using Yolov5-L. The most common detected objects in VisIT-Bench are \u201cperson\u201d (~900 detections), chair, and car (~100). But, a long tail of rarer objects exists as well: full distribution in Appendix Figure 10. Overall, to perform well at VisIT-Bench, a model must account for a broad range of scenes and objects.\n\n**Figure 6:** Most frequently occurring verbs (inner circle) and their top 4 direct nouns (outer circle) in the VisIT-Bench instructions.", "md": "We detect all the COCO objects present in the images from our dataset using Yolov5-L. The most common detected objects in VisIT-Bench are \u201cperson\u201d (~900 detections), chair, and car (~100). But, a long tail of rarer objects exists as well: full distribution in Appendix Figure 10. Overall, to perform well at VisIT-Bench, a model must account for a broad range of scenes and objects.\n\n**Figure 6:** Most frequently occurring verbs (inner circle) and their top 4 direct nouns (outer circle) in the VisIT-Bench instructions."}, {"type": "heading", "lvl": 3, "value": "4 Experiments", "md": "### 4 Experiments"}, {"type": "text", "value": "We evaluate a range of state-of-the-art publicly accessible vision-and-language chatbots on the 592 instances in VisIT-Bench. In \u00a74.1, we provide the details of the instruction-following models in our benchmark. Following this, we collect the human preferences for pairwise model generations to achieve a human-guided Elo ranking and the win-rates against the reference of the models in \u00a74.2.\n```", "md": "We evaluate a range of state-of-the-art publicly accessible vision-and-language chatbots on the 592 instances in VisIT-Bench. In \u00a74.1, we provide the details of the instruction-following models in our benchmark. Following this, we collect the human preferences for pairwise model generations to achieve a human-guided Elo ranking and the win-rates against the reference of the models in \u00a74.2.\n```"}]}, {"page": 8, "text": "                                                                               PandaGPT                                                   Instruction-\n                                       The unusual aspect of this image is that it appears to be a combination of two different           Conditioned\n                                       scenes or images. The main focus is a cartoon drawing of a man holding a sign that says            Caption\n                                       \"ART.\" However, there are also elements of a kitchen, such as a refrigerator, a microwave,         A cave man\n                                       and a clock, which do not seem to be related to the \"ART\" theme. This juxtaposition of             with a\n                                       unrelated subjects in the same image creates a peculiar and unexpected visual effect.              headband is\n                                                                              InstructBLIP                                                holding a       InstructBLIP\n                                       An unusual aspect of this image is the man's appearance. He has long hair, a beard, and is  GPT-4  money bill\n                                       wearing a loincloth, which is an ancient form of clothing worn by men in some cultures.            that has the\n    What is unusual                    Additionally, he is holding a stack of money in his hand, which is a modern-day item. This         face of a cave\n    about this image?                  combination of ancient and modern elements creates an interesting contrast and raises              man on it.\n                                       questions about the context and purpose of the image.\n Figure 7: ELO-based evaluation for VisIT-Bench: Our reference-free approach uses a GPT4\n evaluator to compare two instruction-following models with an instruction and a instruction-\n conditioned caption. The instance is obtained from an existing dataset, WHOOPS! [41].\n then develop automatic evaluation on VisIT-Bench in \u00a74.3, that can be scaled and improved\n given new and improved models. Finally, we establish the trustworthiness of our automatic\n evaluation method by performing agreement analysis with the human judgments in \u00a74.3\n 4.1         Models\n We evaluate LLaVA-13B [13], InstructBLIP-13B [14], MiniGPT4-7B [42], mPLUG-Owl-\n 7B [17], LlamaAdapter-v2-7B [16], PandaGPT-13B [18], VisualChatGPT [43], Multimodal\n GPT [44], OpenFlamingo v1 [11, 45] , Otter v1 [46], Lynx [47] and idefics [48]. For the\n execution-based VisualChatGPT [43], we implement a chat window for each sample, hold\n inputs and intermediate chains of thoughts and actions in memory, and feed the images and\n the instruction sequentially. For OpenFlamingo [11] and Otter [46], we feed the image(s)\n and the instruction in an interleaved format. For the others, we feed the image to the vision\n feature extractor and feed the instruction as a prompt to the text encoder.5\n 4.2         Human Evaluation\n We collect 5K pairwise human preference judgements across an initial set of 6 models and\n the human-verified references. For 1K uniformly randomly sampled tuples of (query, model\n A, model B), we collect 5 crowdworker judgements each. Preferences are collected in a\n\u201cforced choice\u201d setting, annotators are instructed to decide based on accuracy, helpfulness,\n and detail. We provide the template for the human annotation process in Appendix Figure 15.\n We summarize the results with two metrics:\n Relative metric: Elo We follow [26] and compute Elo ratings, treating each pairwise human\n judgement as a \u201cmatch.\u201d6 The difference between the Elo ratings of two different models\n provides an estimate for the win probability when pitting model A vs. model B. More details\n are in Appendix D.\n Absolute metric: Win rate vs. reference. We provide a win-rate vs. the human-verified\n reference. We use the 1.4K pairwise human judgments where one of A or B is the reference.\n We report the percent of cases where the human judge prefers the output from that model\n vs. the human-verified GPT-4 reference output. Because we do not allow for ties in our\n forced-choice setup, if the annotator believes the responses are of equal quaity, they choose\n one arbitrarily.\n       5 Following the authors\u2019 instructions, we run all models using default settings to obtain the best possible\n responses. We include specific samples for reproducibility. We acknowledge hyperparameter impact and are\n willing to reassess submissions to VisIT-Bench if conditions were sub-optimal.\n       6 We use the following code/hyperparameters for Elo ratings: https://github.com/lm-sys/FastChat/\n blob/main/fastchat/serve/monitor/elo_analysis.py\n                                                                                                    8", "md": "# Document\n\n# PandaGPT Instruction-Conditioned Caption\n\nThe unusual aspect of this image is that it appears to be a combination of two different scenes or images. The main focus is a cartoon drawing of a man holding a sign that says \"ART.\" However, there are also elements of a kitchen, such as a refrigerator, a microwave, and a clock, which do not seem to be related to the \"ART\" theme. This juxtaposition of unrelated subjects in the same image creates a peculiar and unexpected visual effect.\n\n# InstructBLIP\n\nAn unusual aspect of this image is the man's appearance. He has long hair, a beard, and is wearing a loincloth, which is an ancient form of clothing worn by men in some cultures. Additionally, he is holding a stack of money in his hand, which is a modern-day item. This combination of ancient and modern elements creates an interesting contrast and raises questions about the context and purpose of the image.\n\n## What is unusual about this image?\n\nFigure 7: ELO-based evaluation for VisIT-Bench: Our reference-free approach uses a GPT4 evaluator to compare two instruction-following models with an instruction and a instruction-conditioned caption. The instance is obtained from an existing dataset, WHOOPS! [41].\n\nThen develop automatic evaluation on VisIT-Bench in \u00a74.3, that can be scaled and improved given new and improved models. Finally, we establish the trustworthiness of our automatic evaluation method by performing agreement analysis with the human judgments in \u00a74.3\n\n### Models\n\n- LLaVA-13B [13]\n- InstructBLIP-13B [14]\n- MiniGPT4-7B [42]\n- mPLUG-Owl-7B [17]\n- LlamaAdapter-v2-7B [16]\n- PandaGPT-13B [18]\n- VisualChatGPT [43]\n- Multimodal GPT [44]\n- OpenFlamingo v1 [11, 45]\n- Otter v1 [46]\n- Lynx [47]\n- idefics [48]\n\n### Human Evaluation\n\nWe collect 5K pairwise human preference judgements across an initial set of 6 models and the human-verified references. For 1K uniformly randomly sampled tuples of (query, model A, model B), we collect 5 crowdworker judgements each. Preferences are collected in a \"forced choice\" setting, annotators are instructed to decide based on accuracy, helpfulness, and detail. We provide the template for the human annotation process in Appendix Figure 15.\n\nWe summarize the results with two metrics:\n\n- Relative metric: Elo We follow [26] and compute Elo ratings, treating each pairwise human judgement as a \"match.\" The difference between the Elo ratings of two different models provides an estimate for the win probability when pitting model A vs. model B.\n- Absolute metric: Win rate vs. reference. We provide a win-rate vs. the human-verified reference. We use the 1.4K pairwise human judgments where one of A or B is the reference. We report the percent of cases where the human judge prefers the output from that model vs. the human-verified GPT-4 reference output. Because we do not allow for ties in our forced-choice setup, if the annotator believes the responses are of equal quality, they choose one arbitrarily.", "images": [{"name": "page-8-3.jpg", "height": 60, "width": 55, "x": 109, "y": 74}, {"name": "page-8-0.jpg", "height": 27, "width": 27, "x": 385, "y": 100}, {"name": "page-8-5.jpg", "height": 20, "width": 20, "x": 475, "y": 105}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 1, "value": "PandaGPT Instruction-Conditioned Caption", "md": "# PandaGPT Instruction-Conditioned Caption"}, {"type": "text", "value": "The unusual aspect of this image is that it appears to be a combination of two different scenes or images. The main focus is a cartoon drawing of a man holding a sign that says \"ART.\" However, there are also elements of a kitchen, such as a refrigerator, a microwave, and a clock, which do not seem to be related to the \"ART\" theme. This juxtaposition of unrelated subjects in the same image creates a peculiar and unexpected visual effect.", "md": "The unusual aspect of this image is that it appears to be a combination of two different scenes or images. The main focus is a cartoon drawing of a man holding a sign that says \"ART.\" However, there are also elements of a kitchen, such as a refrigerator, a microwave, and a clock, which do not seem to be related to the \"ART\" theme. This juxtaposition of unrelated subjects in the same image creates a peculiar and unexpected visual effect."}, {"type": "heading", "lvl": 1, "value": "InstructBLIP", "md": "# InstructBLIP"}, {"type": "text", "value": "An unusual aspect of this image is the man's appearance. He has long hair, a beard, and is wearing a loincloth, which is an ancient form of clothing worn by men in some cultures. Additionally, he is holding a stack of money in his hand, which is a modern-day item. This combination of ancient and modern elements creates an interesting contrast and raises questions about the context and purpose of the image.", "md": "An unusual aspect of this image is the man's appearance. He has long hair, a beard, and is wearing a loincloth, which is an ancient form of clothing worn by men in some cultures. Additionally, he is holding a stack of money in his hand, which is a modern-day item. This combination of ancient and modern elements creates an interesting contrast and raises questions about the context and purpose of the image."}, {"type": "heading", "lvl": 2, "value": "What is unusual about this image?", "md": "## What is unusual about this image?"}, {"type": "text", "value": "Figure 7: ELO-based evaluation for VisIT-Bench: Our reference-free approach uses a GPT4 evaluator to compare two instruction-following models with an instruction and a instruction-conditioned caption. The instance is obtained from an existing dataset, WHOOPS! [41].\n\nThen develop automatic evaluation on VisIT-Bench in \u00a74.3, that can be scaled and improved given new and improved models. Finally, we establish the trustworthiness of our automatic evaluation method by performing agreement analysis with the human judgments in \u00a74.3", "md": "Figure 7: ELO-based evaluation for VisIT-Bench: Our reference-free approach uses a GPT4 evaluator to compare two instruction-following models with an instruction and a instruction-conditioned caption. The instance is obtained from an existing dataset, WHOOPS! [41].\n\nThen develop automatic evaluation on VisIT-Bench in \u00a74.3, that can be scaled and improved given new and improved models. Finally, we establish the trustworthiness of our automatic evaluation method by performing agreement analysis with the human judgments in \u00a74.3"}, {"type": "heading", "lvl": 3, "value": "Models", "md": "### Models"}, {"type": "text", "value": "- LLaVA-13B [13]\n- InstructBLIP-13B [14]\n- MiniGPT4-7B [42]\n- mPLUG-Owl-7B [17]\n- LlamaAdapter-v2-7B [16]\n- PandaGPT-13B [18]\n- VisualChatGPT [43]\n- Multimodal GPT [44]\n- OpenFlamingo v1 [11, 45]\n- Otter v1 [46]\n- Lynx [47]\n- idefics [48]", "md": "- LLaVA-13B [13]\n- InstructBLIP-13B [14]\n- MiniGPT4-7B [42]\n- mPLUG-Owl-7B [17]\n- LlamaAdapter-v2-7B [16]\n- PandaGPT-13B [18]\n- VisualChatGPT [43]\n- Multimodal GPT [44]\n- OpenFlamingo v1 [11, 45]\n- Otter v1 [46]\n- Lynx [47]\n- idefics [48]"}, {"type": "heading", "lvl": 3, "value": "Human Evaluation", "md": "### Human Evaluation"}, {"type": "text", "value": "We collect 5K pairwise human preference judgements across an initial set of 6 models and the human-verified references. For 1K uniformly randomly sampled tuples of (query, model A, model B), we collect 5 crowdworker judgements each. Preferences are collected in a \"forced choice\" setting, annotators are instructed to decide based on accuracy, helpfulness, and detail. We provide the template for the human annotation process in Appendix Figure 15.\n\nWe summarize the results with two metrics:\n\n- Relative metric: Elo We follow [26] and compute Elo ratings, treating each pairwise human judgement as a \"match.\" The difference between the Elo ratings of two different models provides an estimate for the win probability when pitting model A vs. model B.\n- Absolute metric: Win rate vs. reference. We provide a win-rate vs. the human-verified reference. We use the 1.4K pairwise human judgments where one of A or B is the reference. We report the percent of cases where the human judge prefers the output from that model vs. the human-verified GPT-4 reference output. Because we do not allow for ties in our forced-choice setup, if the annotator believes the responses are of equal quality, they choose one arbitrarily.", "md": "We collect 5K pairwise human preference judgements across an initial set of 6 models and the human-verified references. For 1K uniformly randomly sampled tuples of (query, model A, model B), we collect 5 crowdworker judgements each. Preferences are collected in a \"forced choice\" setting, annotators are instructed to decide based on accuracy, helpfulness, and detail. We provide the template for the human annotation process in Appendix Figure 15.\n\nWe summarize the results with two metrics:\n\n- Relative metric: Elo We follow [26] and compute Elo ratings, treating each pairwise human judgement as a \"match.\" The difference between the Elo ratings of two different models provides an estimate for the win probability when pitting model A vs. model B.\n- Absolute metric: Win rate vs. reference. We provide a win-rate vs. the human-verified reference. We use the 1.4K pairwise human judgments where one of A or B is the reference. We report the percent of cases where the human judge prefers the output from that model vs. the human-verified GPT-4 reference output. Because we do not allow for ties in our forced-choice setup, if the annotator believes the responses are of equal quality, they choose one arbitrarily."}]}, {"page": 9, "text": "Table 3: Human scoring results for the models, shown as both an ELO rating and win-rate\n against the reference. In total, this summarizes 5.0K pairwise human judgments. matches\n column indicates the number of total matches in which a particular model participates.\nWin-rate vs. reference indicates the win-rate of a model against the reference outputs.\n                                        Model                                                                   Elo         matches              Win-rate vs. reference (w/ # ratings)\n   Single Image                         Human Verified GPT-4 Reference                                       1223                  1439                                             \u2013\n                                        LLaVA (13B)                                                          1085                  1462                               26.23% (n=244)\n                                        LlamaAdapter-v2 (7B)                                                 1061                  1507                              27.41% (n=259)\n                                        mPLUG-Owl (7B)                                                         995                 1345                               14.95% (n=214)\n                                        InstructBLIP (13B)                                                     957                 1315                               12.37% (n=194)\n                                        MiniGPT-4 (7B)                                                         893                 1513                               14.72% (n=299)\n                                        PandaGPT (13B)                                                         786                 1441                               10.48% (n=229)\n   Multiple Images                      Human Verified GPT-4 Reference                                       1193                    210                                            \u2013\n                                        mPLUG-Owl                                                              997                   190                               15.38% (n=78)\n                                        Otter v1                                                               917                   147                                3.17% (n=63)\n                                        OpenFlamingo v1                                                        893                   171                                4.35% (n=69)\n Results                  Table 3 contains the Elo and win-rate vs. reference. In terms of Elo, the Human\nVerified GPT-4 reference achieves a higher rating than all alternatives, validating the quality\n of our reference set: concretely, for our Elo settings, the reference (Elo =1223) has an\n estimated win-rate over one of the best performing models, LLaVA, (Elo =1085) of 69%, and\n an estimated win rate of 93% against the lowest performing model in this setup, PandaGPT\n(Elo =786). This result can partly be explained by the training process of the underlying\n models: The improved performance of LLaVA (13B) might be attributed to its fine-tuning\n process, which utilized 150K instruction-tuning data that is rich in both diversity and quality.\n Interestingly, despite achieving a slightly lower Elo (the computation of which is based on all\n head-to-head \u201cmatches\u201d, rather than just ones against the human reference), LlamaAdapter-\nv2 (7B) wins with the highest rate against the reference. However, the complexity and variety\n of models and tasks in VisIT-Bench makes it challenging to definitively pinpoint the factors\n influencing performance. While we make a preliminary attempt to unravel these intricacies\n in Section 4.3, a comprehensive understanding will necessitate more nuanced and extensive\n future research.\n 4.3          Automatic Evaluation and Leaderboard\n                                                                            From-the-wild                                                                                        Existing datasets\n      25          InstructBLIP (13B)\n                  MiniGPT-4 (7B)\n                  LlamaAdapter-v2 (7B)\n    Win rate vs GPT-4 (%)\n      20          LLaVA (13B)\n                  mPLUG-Owl (7B)\n                  Otter\n      15\n      10\n       5\n       0                echnical Support                                                                      Medical                                                                                              general\n                                               Color                                          Fashion products                                Building materials    gd_vcr                                   vcrclevr\n                                                                                 Gardening Tips     Directions                   Architectural Styles\n                                                  Catchy Titles                            Animals                                                        Anagrams                              whoops\n                                           Traffic Signs                                                         Geometry Problems                           Board Games  textcaps\n                                     Astronomy             Who to call?    Write a Story                                            Pop Culture                        vizwiz       newyork        harmful_memes\n                                                                                                                              Food Recipe                                                 gqa\n                                                                                                                       Cultural Knowledge                                              vqa                aokvqa\n                     History Knowledge               Physical Knowledge                 Events KnowledgeDressing sense                 Capacity Guesstimate                  okvqa                     scienceqa\n                                                                              Exercise           Art knowledge                                   our guide\n                                  Figurative Speech                                  Graph Reasoning\n                               Gestures Understanding         Recognition                                           Climate understanding\n                            House Understanding                   Chemical Identification                                                              Paper folding\n                                        Home Renovation Human EmotionCounting                              Location Understanding         Hazard Identification\n                                                                        Game Playing                                       Differently abled        OCR Math     Write a Poem   aokvqg       kilogram\n                                                                                                                                                 T\n                        T\n Figure 8: Reference-free assesment win rate vs. human-verified GPT4 response for each\n instruction category. Axes: win rate (Y), instruction categories (X). Categories are from-the-\nwild or existing datasets. VisIT-Bench facilitates analysis of diverse instruction tuning tasks.\n                                                                                                             9", "md": "|Model|Elo|matches|Win-rate vs. reference (w/ # ratings)|\n|---|---|---|---|\n|Single Image Human Verified GPT-4 Reference|1223|1439|\u2013|\n|LLaVA (13B)|1085|1462|26.23% (n=244)|\n|LlamaAdapter-v2 (7B)|1061|1507|27.41% (n=259)|\n|mPLUG-Owl (7B)|995|1345|14.95% (n=214)|\n|InstructBLIP (13B)|957|1315|12.37% (n=194)|\n|MiniGPT-4 (7B)|893|1513|14.72% (n=299)|\n|PandaGPT (13B)|786|1441|10.48% (n=229)|\n|Multiple Images Human Verified GPT-4 Reference|1193|210|\u2013|\n|mPLUG-Owl|997|190|15.38% (n=78)|\n|Otter v1|917|147|3.17% (n=63)|\n|OpenFlamingo v1|893|171|4.35% (n=69)|\n\nTable 3 contains the Elo and win-rate vs. reference. In terms of Elo, the Human Verified GPT-4 reference achieves a higher rating than all alternatives, validating the quality of our reference set: concretely, for our Elo settings, the reference (Elo =1223) has an estimated win-rate over one of the best performing models, LLaVA, (Elo =1085) of 69%, and an estimated win rate of 93% against the lowest performing model in this setup, PandaGPT (Elo =786). This result can partly be explained by the training process of the underlying models: The improved performance of LLaVA (13B) might be attributed to its fine-tuning process, which utilized 150K instruction-tuning data that is rich in both diversity and quality. Interestingly, despite achieving a slightly lower Elo (the computation of which is based on all head-to-head \u201cmatches\u201d, rather than just ones against the human reference), LlamaAdapter-v2 (7B) wins with the highest rate against the reference. However, the complexity and variety of models and tasks in VisIT-Bench makes it challenging to definitively pinpoint the factors influencing performance. While we make a preliminary attempt to unravel these intricacies in Section 4.3, a comprehensive understanding will necessitate more nuanced and extensive future research.\n\n### 4.3 Automatic Evaluation and Leaderboard\n\n| |From-the-wild|Existing datasets|\n|---|---|---|\n|25|InstructBLIP (13B)| |\n| |MiniGPT-4 (7B)| |\n| |LlamaAdapter-v2 (7B)| |\n|Win rate vs GPT-4 (%)| | |\n|20|LLaVA (13B)| |\n| |mPLUG-Owl (7B)| |\n| |Otter| |\n\nFigure 8: Reference-free assessment win rate vs. human-verified GPT4 response for each instruction category. Axes: win rate (Y), instruction categories (X). Categories are from-the-wild or existing datasets. VisIT-Bench facilitates analysis of diverse instruction tuning tasks.", "images": [], "items": [{"type": "table", "rows": [["Model", "Elo", "matches", "Win-rate vs. reference (w/ # ratings)"], ["Single Image Human Verified GPT-4 Reference", "1223", "1439", "\u2013"], ["LLaVA (13B)", "1085", "1462", "26.23% (n=244)"], ["LlamaAdapter-v2 (7B)", "1061", "1507", "27.41% (n=259)"], ["mPLUG-Owl (7B)", "995", "1345", "14.95% (n=214)"], ["InstructBLIP (13B)", "957", "1315", "12.37% (n=194)"], ["MiniGPT-4 (7B)", "893", "1513", "14.72% (n=299)"], ["PandaGPT (13B)", "786", "1441", "10.48% (n=229)"], ["Multiple Images Human Verified GPT-4 Reference", "1193", "210", "\u2013"], ["mPLUG-Owl", "997", "190", "15.38% (n=78)"], ["Otter v1", "917", "147", "3.17% (n=63)"], ["OpenFlamingo v1", "893", "171", "4.35% (n=69)"]], "md": "|Model|Elo|matches|Win-rate vs. reference (w/ # ratings)|\n|---|---|---|---|\n|Single Image Human Verified GPT-4 Reference|1223|1439|\u2013|\n|LLaVA (13B)|1085|1462|26.23% (n=244)|\n|LlamaAdapter-v2 (7B)|1061|1507|27.41% (n=259)|\n|mPLUG-Owl (7B)|995|1345|14.95% (n=214)|\n|InstructBLIP (13B)|957|1315|12.37% (n=194)|\n|MiniGPT-4 (7B)|893|1513|14.72% (n=299)|\n|PandaGPT (13B)|786|1441|10.48% (n=229)|\n|Multiple Images Human Verified GPT-4 Reference|1193|210|\u2013|\n|mPLUG-Owl|997|190|15.38% (n=78)|\n|Otter v1|917|147|3.17% (n=63)|\n|OpenFlamingo v1|893|171|4.35% (n=69)|", "isPerfectTable": true, "csv": "\"Model\",\"Elo\",\"matches\",\"Win-rate vs. reference (w/ # ratings)\"\n\"Single Image Human Verified GPT-4 Reference\",\"1223\",\"1439\",\"\u2013\"\n\"LLaVA (13B)\",\"1085\",\"1462\",\"26.23% (n=244)\"\n\"LlamaAdapter-v2 (7B)\",\"1061\",\"1507\",\"27.41% (n=259)\"\n\"mPLUG-Owl (7B)\",\"995\",\"1345\",\"14.95% (n=214)\"\n\"InstructBLIP (13B)\",\"957\",\"1315\",\"12.37% (n=194)\"\n\"MiniGPT-4 (7B)\",\"893\",\"1513\",\"14.72% (n=299)\"\n\"PandaGPT (13B)\",\"786\",\"1441\",\"10.48% (n=229)\"\n\"Multiple Images Human Verified GPT-4 Reference\",\"1193\",\"210\",\"\u2013\"\n\"mPLUG-Owl\",\"997\",\"190\",\"15.38% (n=78)\"\n\"Otter v1\",\"917\",\"147\",\"3.17% (n=63)\"\n\"OpenFlamingo v1\",\"893\",\"171\",\"4.35% (n=69)\""}, {"type": "text", "value": "Table 3 contains the Elo and win-rate vs. reference. In terms of Elo, the Human Verified GPT-4 reference achieves a higher rating than all alternatives, validating the quality of our reference set: concretely, for our Elo settings, the reference (Elo =1223) has an estimated win-rate over one of the best performing models, LLaVA, (Elo =1085) of 69%, and an estimated win rate of 93% against the lowest performing model in this setup, PandaGPT (Elo =786). This result can partly be explained by the training process of the underlying models: The improved performance of LLaVA (13B) might be attributed to its fine-tuning process, which utilized 150K instruction-tuning data that is rich in both diversity and quality. Interestingly, despite achieving a slightly lower Elo (the computation of which is based on all head-to-head \u201cmatches\u201d, rather than just ones against the human reference), LlamaAdapter-v2 (7B) wins with the highest rate against the reference. However, the complexity and variety of models and tasks in VisIT-Bench makes it challenging to definitively pinpoint the factors influencing performance. While we make a preliminary attempt to unravel these intricacies in Section 4.3, a comprehensive understanding will necessitate more nuanced and extensive future research.", "md": "Table 3 contains the Elo and win-rate vs. reference. In terms of Elo, the Human Verified GPT-4 reference achieves a higher rating than all alternatives, validating the quality of our reference set: concretely, for our Elo settings, the reference (Elo =1223) has an estimated win-rate over one of the best performing models, LLaVA, (Elo =1085) of 69%, and an estimated win rate of 93% against the lowest performing model in this setup, PandaGPT (Elo =786). This result can partly be explained by the training process of the underlying models: The improved performance of LLaVA (13B) might be attributed to its fine-tuning process, which utilized 150K instruction-tuning data that is rich in both diversity and quality. Interestingly, despite achieving a slightly lower Elo (the computation of which is based on all head-to-head \u201cmatches\u201d, rather than just ones against the human reference), LlamaAdapter-v2 (7B) wins with the highest rate against the reference. However, the complexity and variety of models and tasks in VisIT-Bench makes it challenging to definitively pinpoint the factors influencing performance. While we make a preliminary attempt to unravel these intricacies in Section 4.3, a comprehensive understanding will necessitate more nuanced and extensive future research."}, {"type": "heading", "lvl": 3, "value": "4.3 Automatic Evaluation and Leaderboard", "md": "### 4.3 Automatic Evaluation and Leaderboard"}, {"type": "table", "rows": [["", "From-the-wild", "Existing datasets"], ["25", "InstructBLIP (13B)", ""], ["", "MiniGPT-4 (7B)", ""], ["", "LlamaAdapter-v2 (7B)", ""], ["Win rate vs GPT-4 (%)", "", ""], ["20", "LLaVA (13B)", ""], ["", "mPLUG-Owl (7B)", ""], ["", "Otter", ""]], "md": "| |From-the-wild|Existing datasets|\n|---|---|---|\n|25|InstructBLIP (13B)| |\n| |MiniGPT-4 (7B)| |\n| |LlamaAdapter-v2 (7B)| |\n|Win rate vs GPT-4 (%)| | |\n|20|LLaVA (13B)| |\n| |mPLUG-Owl (7B)| |\n| |Otter| |", "isPerfectTable": true, "csv": "\"\",\"From-the-wild\",\"Existing datasets\"\n\"25\",\"InstructBLIP (13B)\",\"\"\n\"\",\"MiniGPT-4 (7B)\",\"\"\n\"\",\"LlamaAdapter-v2 (7B)\",\"\"\n\"Win rate vs GPT-4 (%)\",\"\",\"\"\n\"20\",\"LLaVA (13B)\",\"\"\n\"\",\"mPLUG-Owl (7B)\",\"\"\n\"\",\"Otter\",\"\""}, {"type": "text", "value": "Figure 8: Reference-free assessment win rate vs. human-verified GPT4 response for each instruction category. Axes: win rate (Y), instruction categories (X). Categories are from-the-wild or existing datasets. VisIT-Bench facilitates analysis of diverse instruction tuning tasks.", "md": "Figure 8: Reference-free assessment win rate vs. human-verified GPT4 response for each instruction category. Axes: win rate (Y), instruction categories (X). Categories are from-the-wild or existing datasets. VisIT-Bench facilitates analysis of diverse instruction tuning tasks."}]}, {"page": 10, "text": "Table 4: Current reference-free Elo rankings as of September 25th, 2023. In total, these\n rankings summarize 31,735 \u201cmatches\u201d between models; each match consists of 2 queries to\n GPT-4. Because VisIT-Bench is dynamic, these rankings are updated as more models are\n added to the leaderboard, and more pairs of models are evaluated head-to-head for more\n instances.\n   Category                     Model                                                     Elo       # Matches           Win vs. Reference (w/ # ratings)\n                                Human Verifi        ed GPT-4 Reference                 1,382                5,880                               \u2014\n                                LLaVA-Plus (13B)                                       1,203                   678                   35.07% (n=134)\n                                LLaVA (13B)                                            1,095                5,420                    18.53% (n=475)\n                                mPLUG-Owl (7B)                                         1,087                5,440                    15.83% (n=480)\n                                LlamaAdapter-v2 (7B)                                   1,066                5,469                    14.14% (n=488)\n                                Lynx(8B)                                               1,037                   787                   11.43% (n=140)\n                                idefi cs (9B)                                          1,020                   794                    9.72% (n=144)\n   Single Image                 InstructBLIP (13B)                                     1,000                5,469                    14.12% (n=503)\n                                Otter v1 (9B)                                             962               5,443                     7.01% (n=499)\n                                VisualGPT (Da Vinci 003)                                  941               5,437                     1.57% (n=510)\n                                MiniGPT-4 (7B)                                            926               5,448                     3.36% (n=506)\n                                Octopus V2 (9B)                                           925                  790                    8.90% (n=146)\n                                OpenFlamingo V1 (9B)                                      851               5,479                     2.95% (n=509)\n                                PandaGPT (13B)                                            775               5,465                     2.70% (n=519)\n                                Multimodal GPT                                            731               5,471                     0.19% (n=527)\n                                Human Verifi        ed GPT-4 Reference                 1,192                   180                               -\n   Multiple Images              mPLUG-Owl                                                 995                  180                     6.67% (n=60)\n                                Otter v1                                                  911                  180                     1.69% (n=59)\n                                OpenFlamingo v1                                           902                  180                     1.67% (n=60)\n Because it is costly to gather human pair-                                                  % Agree with Human (Higher=better)                            GPT-4-no-ref\nwise preference judgements for new model                                                      96                                                           GPT-4-ref\n                                                                                                                                                           BERTScore\n submissions, to support faster model devel-                                                  88                           ~upper bound                    METEOR\n opment, we seek an automatic evaluation                                                                                                                   ROUGE\n                                                                                              80                                                           BLEU\n procedure that produces high correlation                                                                                                                  Length\nwith our human evaluation setup.                                                              72                                                           CIDEr\n                                                                                              64\n Automatic                 evaluation                metric           candi-                  56\n dates.          We         consider             several           existing                   48                                       random\n reference-backed                        evaluation                 metrics:                        5/5                            4/5                            3/5\n BLEU-4 [27] ,                       ROUGE-L [28],                        ME-                     (n=336)                        (n=200)                        (n=154)\n                                                                                                                            Agreement Level\nTEOR [29], CIDEr [30], and BERTScore\n [31], we use the RoBERTa-Large english                                                Figure 9: Correlations between evaluation met-\nversion [49], treating the human-verified                                              rics and human preferences are ranked in per-\n GPT-4 reference as the evaluation reference.                                          formance order, with our reference free evalu-\nWe additionally report two baseline metrics:                                           ation (GPT-4-no-ref) showing the strongest\n random, which assigns a random score                                                  alignment.                Bottom line:                  random chance\nwithout accounting for the candidate, and                                              (50%), top line: upper performance bound.\n length, which assigns a score equal to the\n number of non-whitespace tokens in the\n candidate.              Beyond existing metrics and\n baselines, following the recent line of work utilizing API-accessed LLMs with a prompt for\n automatic evaluation [6, 50], we consider two GPT-4[7] backed evaluation metrics.\n Specifically, we provide the LLM with: 1) a system prompt describing the desired evaluation\n behavior; 2) the instruction-conditioned caption for the image; 3) the instruction to be\n followed; and 4) two candidate generations dubbed \u201cResponse A\u201d and \u201cResponse B\u201d. We\n also consider a reference-backed version where the human-verified reference is provided as\nwell. We provide our prompts in Appendix E. To mitigate potential biases in \u201cA\u201d and \u201cB\u201d\n positioning, for all pairs of candidates, we run two queries covering both possible orderings.\n                                                                                     10", "md": "# Elo Rankings and Evaluation Metrics\n\n## Current reference-free Elo rankings as of September 25th, 2023\n\nIn total, these rankings summarize 31,735 \u201cmatches\u201d between models; each match consists of 2 queries to GPT-4. Because VisIT-Bench is dynamic, these rankings are updated as more models are added to the leaderboard, and more pairs of models are evaluated head-to-head for more instances.\n\n|Category|Model|Elo|# Matches|Win vs. Reference (w/ # ratings)|\n|---|---|---|---|---|\n|Single Image|InstructBLIP (13B)|1000|5469|14.12% (n=503)|\n|Otter v1 (9B)| |962|5443|7.01% (n=499)|\n|VisualGPT (Da Vinci 003)| |941|5437|1.57% (n=510)|\n|MiniGPT-4 (7B)| |926|5448|3.36% (n=506)|\n|Octopus V2 (9B)| |925|790|8.90% (n=146)|\n|OpenFlamingo V1 (9B)| |851|5479|2.95% (n=509)|\n|PandaGPT (13B)| |775|5465|2.70% (n=519)|\n|Multiple Images|mPLUG-Owl|995|180|6.67% (n=60)|\n|Otter v1| |911|180|1.69% (n=59)|\n|OpenFlamingo v1| |902|180|1.67% (n=60)|\n\n## Evaluation Metrics\n\nBecause it is costly to gather human pairwise preference judgments for new model submissions, to support faster model development, we seek an automatic evaluation procedure that produces high correlation with our human evaluation setup.\n\nAutomatic evaluation metric candidates:\n\n- % Agree with Human (Higher=better): 96 (GPT-4-no-ref)\n- GPT-4-ref: 88\n- ~upper bound: 80\n- random: 48\n\nFigure 9: Correlations between evaluation metrics and human preferences are ranked in performance order, with our reference free evaluation (GPT-4-no-ref) showing the strongest alignment. Bottom line: random chance (50%), top line: upper performance bound.\n\n### Existing Evaluation Metrics:\n\n- BLEU-4\n- ROUGE-L\n- METEOR\n- CIDEr\n- BERTScore\n\nWe use the RoBERTa-Large English version, treating the human-verified GPT-4 reference as the evaluation reference. We additionally report two baseline metrics: random and length.\n\nBeyond existing metrics and baselines, we consider two GPT-4 backed evaluation metrics. Specifically, we provide the LLM with:\n\n1. A system prompt describing the desired evaluation behavior\n2. The instruction-conditioned caption for the image\n3. The instruction to be followed\n4. Two candidate generations dubbed \u201cResponse A\u201d and \u201cResponse B\u201d\n\nWe also consider a reference-backed version where the human-verified reference is provided as well. To mitigate potential biases in \u201cA\u201d and \u201cB\u201d positioning, for all pairs of candidates, we run two queries covering both possible orderings.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Elo Rankings and Evaluation Metrics", "md": "# Elo Rankings and Evaluation Metrics"}, {"type": "heading", "lvl": 2, "value": "Current reference-free Elo rankings as of September 25th, 2023", "md": "## Current reference-free Elo rankings as of September 25th, 2023"}, {"type": "text", "value": "In total, these rankings summarize 31,735 \u201cmatches\u201d between models; each match consists of 2 queries to GPT-4. Because VisIT-Bench is dynamic, these rankings are updated as more models are added to the leaderboard, and more pairs of models are evaluated head-to-head for more instances.", "md": "In total, these rankings summarize 31,735 \u201cmatches\u201d between models; each match consists of 2 queries to GPT-4. Because VisIT-Bench is dynamic, these rankings are updated as more models are added to the leaderboard, and more pairs of models are evaluated head-to-head for more instances."}, {"type": "table", "rows": [["Category", "Model", "Elo", "# Matches", "Win vs. Reference (w/ # ratings)"], ["Single Image", "InstructBLIP (13B)", "1000", "5469", "14.12% (n=503)"], ["Otter v1 (9B)", "", "962", "5443", "7.01% (n=499)"], ["VisualGPT (Da Vinci 003)", "", "941", "5437", "1.57% (n=510)"], ["MiniGPT-4 (7B)", "", "926", "5448", "3.36% (n=506)"], ["Octopus V2 (9B)", "", "925", "790", "8.90% (n=146)"], ["OpenFlamingo V1 (9B)", "", "851", "5479", "2.95% (n=509)"], ["PandaGPT (13B)", "", "775", "5465", "2.70% (n=519)"], ["Multiple Images", "mPLUG-Owl", "995", "180", "6.67% (n=60)"], ["Otter v1", "", "911", "180", "1.69% (n=59)"], ["OpenFlamingo v1", "", "902", "180", "1.67% (n=60)"]], "md": "|Category|Model|Elo|# Matches|Win vs. Reference (w/ # ratings)|\n|---|---|---|---|---|\n|Single Image|InstructBLIP (13B)|1000|5469|14.12% (n=503)|\n|Otter v1 (9B)| |962|5443|7.01% (n=499)|\n|VisualGPT (Da Vinci 003)| |941|5437|1.57% (n=510)|\n|MiniGPT-4 (7B)| |926|5448|3.36% (n=506)|\n|Octopus V2 (9B)| |925|790|8.90% (n=146)|\n|OpenFlamingo V1 (9B)| |851|5479|2.95% (n=509)|\n|PandaGPT (13B)| |775|5465|2.70% (n=519)|\n|Multiple Images|mPLUG-Owl|995|180|6.67% (n=60)|\n|Otter v1| |911|180|1.69% (n=59)|\n|OpenFlamingo v1| |902|180|1.67% (n=60)|", "isPerfectTable": true, "csv": "\"Category\",\"Model\",\"Elo\",\"# Matches\",\"Win vs. Reference (w/ # ratings)\"\n\"Single Image\",\"InstructBLIP (13B)\",\"1000\",\"5469\",\"14.12% (n=503)\"\n\"Otter v1 (9B)\",\"\",\"962\",\"5443\",\"7.01% (n=499)\"\n\"VisualGPT (Da Vinci 003)\",\"\",\"941\",\"5437\",\"1.57% (n=510)\"\n\"MiniGPT-4 (7B)\",\"\",\"926\",\"5448\",\"3.36% (n=506)\"\n\"Octopus V2 (9B)\",\"\",\"925\",\"790\",\"8.90% (n=146)\"\n\"OpenFlamingo V1 (9B)\",\"\",\"851\",\"5479\",\"2.95% (n=509)\"\n\"PandaGPT (13B)\",\"\",\"775\",\"5465\",\"2.70% (n=519)\"\n\"Multiple Images\",\"mPLUG-Owl\",\"995\",\"180\",\"6.67% (n=60)\"\n\"Otter v1\",\"\",\"911\",\"180\",\"1.69% (n=59)\"\n\"OpenFlamingo v1\",\"\",\"902\",\"180\",\"1.67% (n=60)\""}, {"type": "heading", "lvl": 2, "value": "Evaluation Metrics", "md": "## Evaluation Metrics"}, {"type": "text", "value": "Because it is costly to gather human pairwise preference judgments for new model submissions, to support faster model development, we seek an automatic evaluation procedure that produces high correlation with our human evaluation setup.\n\nAutomatic evaluation metric candidates:\n\n- % Agree with Human (Higher=better): 96 (GPT-4-no-ref)\n- GPT-4-ref: 88\n- ~upper bound: 80\n- random: 48\n\nFigure 9: Correlations between evaluation metrics and human preferences are ranked in performance order, with our reference free evaluation (GPT-4-no-ref) showing the strongest alignment. Bottom line: random chance (50%), top line: upper performance bound.", "md": "Because it is costly to gather human pairwise preference judgments for new model submissions, to support faster model development, we seek an automatic evaluation procedure that produces high correlation with our human evaluation setup.\n\nAutomatic evaluation metric candidates:\n\n- % Agree with Human (Higher=better): 96 (GPT-4-no-ref)\n- GPT-4-ref: 88\n- ~upper bound: 80\n- random: 48\n\nFigure 9: Correlations between evaluation metrics and human preferences are ranked in performance order, with our reference free evaluation (GPT-4-no-ref) showing the strongest alignment. Bottom line: random chance (50%), top line: upper performance bound."}, {"type": "heading", "lvl": 3, "value": "Existing Evaluation Metrics:", "md": "### Existing Evaluation Metrics:"}, {"type": "text", "value": "- BLEU-4\n- ROUGE-L\n- METEOR\n- CIDEr\n- BERTScore\n\nWe use the RoBERTa-Large English version, treating the human-verified GPT-4 reference as the evaluation reference. We additionally report two baseline metrics: random and length.\n\nBeyond existing metrics and baselines, we consider two GPT-4 backed evaluation metrics. Specifically, we provide the LLM with:\n\n1. A system prompt describing the desired evaluation behavior\n2. The instruction-conditioned caption for the image\n3. The instruction to be followed\n4. Two candidate generations dubbed \u201cResponse A\u201d and \u201cResponse B\u201d\n\nWe also consider a reference-backed version where the human-verified reference is provided as well. To mitigate potential biases in \u201cA\u201d and \u201cB\u201d positioning, for all pairs of candidates, we run two queries covering both possible orderings.", "md": "- BLEU-4\n- ROUGE-L\n- METEOR\n- CIDEr\n- BERTScore\n\nWe use the RoBERTa-Large English version, treating the human-verified GPT-4 reference as the evaluation reference. We additionally report two baseline metrics: random and length.\n\nBeyond existing metrics and baselines, we consider two GPT-4 backed evaluation metrics. Specifically, we provide the LLM with:\n\n1. A system prompt describing the desired evaluation behavior\n2. The instruction-conditioned caption for the image\n3. The instruction to be followed\n4. Two candidate generations dubbed \u201cResponse A\u201d and \u201cResponse B\u201d\n\nWe also consider a reference-backed version where the human-verified reference is provided as well. To mitigate potential biases in \u201cA\u201d and \u201cB\u201d positioning, for all pairs of candidates, we run two queries covering both possible orderings."}]}, {"page": 11, "text": "Our prompt encourages the model to think step-by-step so that its chain-of-thought process\nis made explicit [51, 52]. Despite strongly encouraging the model to select between the\ntwo references in a forced-choice setup, it sometimes refuses and outputs \u201ctie\u201d which we\naccount for later. We call the reference-free version of this metric \u201cGPT4-no-ref\u201d, and the\nreference-backed version of this metric \u201cGPT4-ref\u201d.\nEvaluating evaluation metrics.     We measure the correlation between the candidate metrics\nand human judgements using a pairwise framework. Specifically, we use a subset of the\n5K pairwise human judgements in \u00a7 4.2. For 690 pairwise instances where both candidate\ninstances are model-generated (rather than human-verified references), we have 5 pairwise\njudgements from crowd-workers. For 336 pairs, there is 5/5 agreement, for 200 pairs, there\nis 4/5 agreement, and for 154 pairs, there is 3/5 agreement. For each metric, we measure the\npercent of time the metric is able to accurately reconstruct a majority vote judgement from\nthe 5 crowdworkers. The newly proposed GPT-4 based metrics sometimes outputs \u201ctie\u201d (this\nhappens in 10-15% of cases overall) \u2013 for fair comparison with the other metrics in forced\nchoice setting, we randomly choose one of the two options when GPT-4 reports a tie.\nThe results are in Figure 9, with GPT-4-no-ref best aligns with human correlation. The\nbest performing metric is our newly proposed GPT-4 based metric, which accurately recon-\nstructs majority-vote pairwise human judgments better than alternatives (p < .05; binomial\nproportion CI nonoverlapping). For example, for instances where 5/5 annotators agree,\nGPT4-no-ref, with no reference, accurately reconstructs human judgment 93% of the time,\nwhereas the next best metrics BERTScore/METEOR/ROUGE-L reconstruct accurately\n80%/78%/70% of the time; among the metrics we consider, these are reasonable options for\nstatic/offline evaluation without relying on OpenAI API access, especially when compared\nto our length baseline metric, which achieves only 60%. Notably, the reference-backed\nversion of the newly proposed GPT-4 based metric achieves comparable (but slightly worse)\nperformance compared to the reference-free version. Thus, we adopt the reference-free\nversion, which additionally enables us to place the references themselves into the the Elo\nsetup, because they are not used in the prompts.\nSystem-level Correlation. We summarize the LLM\u2019s pairwise judgements using the same\nmetrics as introduced in \u00a74.2, Elo ratings and win rate vs. reference, but instead of using a\nhuman judge, we use our reference-free GPT-4 based metric. The results are in ??. Notably,\namong the 7 systems for which we gathered human ratings for, the automatic metric produces\nthe same ordering compared to human evaluation (\u03c1 = 1.0, p < .01).\nShortcomings of proposed metric. While the relative ranking of models produced by the\nautomatic metric correlates strongly with the ranking produced by human judgements, the\nwin rate vs. reference according to human judgement (Table 3) are higher overall compared\nto the win-rate vs. reference according to the automatic metric ??. One plausible explanation\nfor this discrepancy is that GPT-4, as an evaluation model, may prefer responses that closely\nmatch its own response distribution.\nPer-category results. In Figure 8, we plot the win-rate vs reference for the models across\nall the single-image instruction families. We find that there is no model that performs the\nbest and worst across all the instruction families. Thus, VisIT-Bench aids in highlighting\nthe strengths and weaknesses of the instruction-following models along various real-world\nuse-cases.\n5   Related Work\nMultimodal Models for Image-Text Understanding: Recently, the field of machine\nlearning has experienced a rapid proliferation of new models which can perform various\nimage-text tasks [12, 15, 13, 53, 18, 14]. This growth has been driven by several factors, in-\ncluding the emergence of large-scale multimodal datasets (e.g. LAION-5B [54], Multimodal\n                                              11", "md": "# Document\n\nOur prompt encourages the model to think step-by-step so that its chain-of-thought process is made explicit [51], [52]. Despite strongly encouraging the model to select between the two references in a forced-choice setup, it sometimes refuses and outputs \u201ctie\u201d which we account for later. We call the reference-free version of this metric \u201cGPT4-no-ref\u201d, and the reference-backed version of this metric \u201cGPT4-ref\u201d.\n\nEvaluating evaluation metrics. We measure the correlation between the candidate metrics and human judgements using a pairwise framework. Specifically, we use a subset of the 5K pairwise human judgements in \u00a7 4.2. For 690 pairwise instances where both candidate instances are model-generated (rather than human-verified references), we have 5 pairwise judgements from crowd-workers. For 336 pairs, there is 5/5 agreement, for 200 pairs, there is 4/5 agreement, and for 154 pairs, there is 3/5 agreement. For each metric, we measure the percent of time the metric is able to accurately reconstruct a majority vote judgement from the 5 crowdworkers. The newly proposed GPT-4 based metrics sometimes outputs \u201ctie\u201d (this happens in 10-15% of cases overall) \u2013 for fair comparison with the other metrics in forced choice setting, we randomly choose one of the two options when GPT-4 reports a tie.\n\nThe results are in Figure 9, with GPT-4-no-ref best aligns with human correlation. The best performing metric is our newly proposed GPT-4 based metric, which accurately reconstructs majority-vote pairwise human judgments better than alternatives (p &lt; .05; binomial proportion CI nonoverlapping). For example, for instances where 5/5 annotators agree, GPT4-no-ref, with no reference, accurately reconstructs human judgment 93% of the time, whereas the next best metrics BERTScore/METEOR/ROUGE-L reconstruct accurately 80%/78%/70% of the time; among the metrics we consider, these are reasonable options for static/offline evaluation without relying on OpenAI API access, especially when compared to our length baseline metric, which achieves only 60%. Notably, the reference-backed version of the newly proposed GPT-4 based metric achieves comparable (but slightly worse) performance compared to the reference-free version. Thus, we adopt the reference-free version, which additionally enables us to place the references themselves into the Elo setup, because they are not used in the prompts.\n\nSystem-level Correlation. We summarize the LLM\u2019s pairwise judgements using the same metrics as introduced in \u00a74.2, Elo ratings and win rate vs. reference, but instead of using a human judge, we use our reference-free GPT-4 based metric. The results are in ??. Notably, among the 7 systems for which we gathered human ratings for, the automatic metric produces the same ordering compared to human evaluation (\u03c1 = 1.0, p &lt; .01).\n\nShortcomings of proposed metric. While the relative ranking of models produced by the automatic metric correlates strongly with the ranking produced by human judgements, the win rate vs. reference according to human judgement (Table 3) are higher overall compared to the win-rate vs. reference according to the automatic metric ??. One plausible explanation for this discrepancy is that GPT-4, as an evaluation model, may prefer responses that closely match its own response distribution.\n\nPer-category results. In Figure 8, we plot the win-rate vs reference for the models across all the single-image instruction families. We find that there is no model that performs the best and worst across all the instruction families. Thus, VisIT-Bench aids in highlighting the strengths and weaknesses of the instruction-following models along various real-world use-cases.\n\n## Related Work\n\nMultimodal Models for Image-Text Understanding: Recently, the field of machine learning has experienced a rapid proliferation of new models which can perform various image-text tasks [12], [15], [13], [53], [18], [14]. This growth has been driven by several factors, including the emergence of large-scale multimodal datasets (e.g. LAION-5B [54], Multimodal).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Our prompt encourages the model to think step-by-step so that its chain-of-thought process is made explicit [51], [52]. Despite strongly encouraging the model to select between the two references in a forced-choice setup, it sometimes refuses and outputs \u201ctie\u201d which we account for later. We call the reference-free version of this metric \u201cGPT4-no-ref\u201d, and the reference-backed version of this metric \u201cGPT4-ref\u201d.\n\nEvaluating evaluation metrics. We measure the correlation between the candidate metrics and human judgements using a pairwise framework. Specifically, we use a subset of the 5K pairwise human judgements in \u00a7 4.2. For 690 pairwise instances where both candidate instances are model-generated (rather than human-verified references), we have 5 pairwise judgements from crowd-workers. For 336 pairs, there is 5/5 agreement, for 200 pairs, there is 4/5 agreement, and for 154 pairs, there is 3/5 agreement. For each metric, we measure the percent of time the metric is able to accurately reconstruct a majority vote judgement from the 5 crowdworkers. The newly proposed GPT-4 based metrics sometimes outputs \u201ctie\u201d (this happens in 10-15% of cases overall) \u2013 for fair comparison with the other metrics in forced choice setting, we randomly choose one of the two options when GPT-4 reports a tie.\n\nThe results are in Figure 9, with GPT-4-no-ref best aligns with human correlation. The best performing metric is our newly proposed GPT-4 based metric, which accurately reconstructs majority-vote pairwise human judgments better than alternatives (p &lt; .05; binomial proportion CI nonoverlapping). For example, for instances where 5/5 annotators agree, GPT4-no-ref, with no reference, accurately reconstructs human judgment 93% of the time, whereas the next best metrics BERTScore/METEOR/ROUGE-L reconstruct accurately 80%/78%/70% of the time; among the metrics we consider, these are reasonable options for static/offline evaluation without relying on OpenAI API access, especially when compared to our length baseline metric, which achieves only 60%. Notably, the reference-backed version of the newly proposed GPT-4 based metric achieves comparable (but slightly worse) performance compared to the reference-free version. Thus, we adopt the reference-free version, which additionally enables us to place the references themselves into the Elo setup, because they are not used in the prompts.\n\nSystem-level Correlation. We summarize the LLM\u2019s pairwise judgements using the same metrics as introduced in \u00a74.2, Elo ratings and win rate vs. reference, but instead of using a human judge, we use our reference-free GPT-4 based metric. The results are in ??. Notably, among the 7 systems for which we gathered human ratings for, the automatic metric produces the same ordering compared to human evaluation (\u03c1 = 1.0, p &lt; .01).\n\nShortcomings of proposed metric. While the relative ranking of models produced by the automatic metric correlates strongly with the ranking produced by human judgements, the win rate vs. reference according to human judgement (Table 3) are higher overall compared to the win-rate vs. reference according to the automatic metric ??. One plausible explanation for this discrepancy is that GPT-4, as an evaluation model, may prefer responses that closely match its own response distribution.\n\nPer-category results. In Figure 8, we plot the win-rate vs reference for the models across all the single-image instruction families. We find that there is no model that performs the best and worst across all the instruction families. Thus, VisIT-Bench aids in highlighting the strengths and weaknesses of the instruction-following models along various real-world use-cases.", "md": "Our prompt encourages the model to think step-by-step so that its chain-of-thought process is made explicit [51], [52]. Despite strongly encouraging the model to select between the two references in a forced-choice setup, it sometimes refuses and outputs \u201ctie\u201d which we account for later. We call the reference-free version of this metric \u201cGPT4-no-ref\u201d, and the reference-backed version of this metric \u201cGPT4-ref\u201d.\n\nEvaluating evaluation metrics. We measure the correlation between the candidate metrics and human judgements using a pairwise framework. Specifically, we use a subset of the 5K pairwise human judgements in \u00a7 4.2. For 690 pairwise instances where both candidate instances are model-generated (rather than human-verified references), we have 5 pairwise judgements from crowd-workers. For 336 pairs, there is 5/5 agreement, for 200 pairs, there is 4/5 agreement, and for 154 pairs, there is 3/5 agreement. For each metric, we measure the percent of time the metric is able to accurately reconstruct a majority vote judgement from the 5 crowdworkers. The newly proposed GPT-4 based metrics sometimes outputs \u201ctie\u201d (this happens in 10-15% of cases overall) \u2013 for fair comparison with the other metrics in forced choice setting, we randomly choose one of the two options when GPT-4 reports a tie.\n\nThe results are in Figure 9, with GPT-4-no-ref best aligns with human correlation. The best performing metric is our newly proposed GPT-4 based metric, which accurately reconstructs majority-vote pairwise human judgments better than alternatives (p &lt; .05; binomial proportion CI nonoverlapping). For example, for instances where 5/5 annotators agree, GPT4-no-ref, with no reference, accurately reconstructs human judgment 93% of the time, whereas the next best metrics BERTScore/METEOR/ROUGE-L reconstruct accurately 80%/78%/70% of the time; among the metrics we consider, these are reasonable options for static/offline evaluation without relying on OpenAI API access, especially when compared to our length baseline metric, which achieves only 60%. Notably, the reference-backed version of the newly proposed GPT-4 based metric achieves comparable (but slightly worse) performance compared to the reference-free version. Thus, we adopt the reference-free version, which additionally enables us to place the references themselves into the Elo setup, because they are not used in the prompts.\n\nSystem-level Correlation. We summarize the LLM\u2019s pairwise judgements using the same metrics as introduced in \u00a74.2, Elo ratings and win rate vs. reference, but instead of using a human judge, we use our reference-free GPT-4 based metric. The results are in ??. Notably, among the 7 systems for which we gathered human ratings for, the automatic metric produces the same ordering compared to human evaluation (\u03c1 = 1.0, p &lt; .01).\n\nShortcomings of proposed metric. While the relative ranking of models produced by the automatic metric correlates strongly with the ranking produced by human judgements, the win rate vs. reference according to human judgement (Table 3) are higher overall compared to the win-rate vs. reference according to the automatic metric ??. One plausible explanation for this discrepancy is that GPT-4, as an evaluation model, may prefer responses that closely match its own response distribution.\n\nPer-category results. In Figure 8, we plot the win-rate vs reference for the models across all the single-image instruction families. We find that there is no model that performs the best and worst across all the instruction families. Thus, VisIT-Bench aids in highlighting the strengths and weaknesses of the instruction-following models along various real-world use-cases."}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "Multimodal Models for Image-Text Understanding: Recently, the field of machine learning has experienced a rapid proliferation of new models which can perform various image-text tasks [12], [15], [13], [53], [18], [14]. This growth has been driven by several factors, including the emergence of large-scale multimodal datasets (e.g. LAION-5B [54], Multimodal).", "md": "Multimodal Models for Image-Text Understanding: Recently, the field of machine learning has experienced a rapid proliferation of new models which can perform various image-text tasks [12], [15], [13], [53], [18], [14]. This growth has been driven by several factors, including the emergence of large-scale multimodal datasets (e.g. LAION-5B [54], Multimodal)."}]}, {"page": 12, "text": "C4 [11]), improved software and hardware frameworks, and advances in modality-specific\nmodels such as language models (e.g., [10]). Our work specifically evaluates models which\ncan generate textual outputs, given one or more images, and text. Recent examples of such\nmodels include LLaVA [13], mPLUG-Owl [17], InstructBLIP, LLaMA-Adapter, Flamingo\n[12] and OpenFlamingo [11], PandaGPT [18], and GPT-4 [7] (which reports multimodal\ncapabilities but has not yet seen a release of the multimodal variant).\nInstruction Following: \u201cInstruction-following\u201d is an emerging paradigm for training models\nvia language, where instead of being trained to complete only a single, fixed task (such as\nimage classification or captioning), models are trained to follow textual instructions that\ndescribe an arbitrary task, with the aim of generalizing to novel instructions. Examples\nof instruction-following models include Alpaca [5], LLaMA-Adapter [16], Koala [55],\nInstructBLIP [14], LLaVA [13], and mPLUG-owl [17]. As the downstream capabilities\nof these models are influenced by the quality of the training dataset, there has also been\nextensive work on developing instruction-following datasets [39, 56, 57, 13, 58] .\nTo build these models, two broad approaches have been shown to be effective. One approach\nfocuses on leveraging existing pretrained task-specific tools such as image captioners [15],\nobject detectors [59] and text-to-image generators [60] by either creating multimodal prompt\ninterfaces [43, 61] or by executing LLM-generated programs [62, 63, 64]. The other approach\n[13, 16, 65, 46, 66, 17, 11] focuses on building a single pretrained model that can follow\ninstructions by supervised finetuning on multimodal vision-language data.\nDespite the success of both these approaches on the existing vision-language datasets e.g.,\nVQA, GQA, Image Captioning [21, 67, 20], there is a lack of a high-quality benchmarking\ndataset for multimodal instruction-following tasks that reliably replicates the way in which\nhumans would interact with multimodal chatbots in the wild. Similar to the image-text\nmodels discussed above, many instruction-following models have been released directly\nas open-source without undergoing peer review or thorough evaluation. As a result, the\neffectiveness of these models for many tasks is not well-understood.\nBenchmarks for Machine Learning: High-quality evaluation datasets have served both\nto (re)assess, and to accelerate, progress on many machine learning tasks [68]. For exam-\nple, our work draws particularly from the fields of computer vision and natural language\nprocessing, where benchmarking datasets have been critical drivers of progress. On the\nvision side, datasets such as ImageNet [69] and CIFAR [70] have proven to be critical\nyardsticks of progress. On the language side, benchmarks such as SQuAD [71], SST [72],\nGLUE/SuperGLUE [73, 74] and more [75, 76] seen wide use. Recent work has indicated\nthat improvements on these high-quality benchmark datasets is not the result of overfitting,\nand is a reliable indicator of genuine progress beyond the benchmark data [77, 78, 79, 80].\nHowever, high-quality benchmarking datasets and evaluation methods do not yet exist\nfor multimodal instruction-following. As a result, it is diffi  cult to assess progress in this\ndirection, which both reduces the field\u2019s ability to identify true breakthroughs and increases\nvulnerability to potential pitfalls of evaluation that have hampered progress in other areas of\nmachine learning [68, 81].\n6    Conclusion\nWe introduce VisIT-Bench, a dynamic benchmark providing a broad evaluation of multimodal\nchatbots\u2019 capabilities. Going beyond prior efforts, VisIT-Bench\u2019s collection process centers\npotential real-world use cases, and 70 diverse instruction families encompassing a range of\ntasks from recognition to complex reasoning. Our benchmark not only offers human-verified\nreference outputs for all examples but also gives an Elo-based ranking system for multimodal\nchatbots that correlates with human judgements. Our experiments reveal a gap between\nmodel and human performance.We release data, code, and automatic metrics, encouraging\n                                               12", "md": "# Document\n\nC4 [11]), improved software and hardware frameworks, and advances in modality-specific models such as language models (e.g., [10]). Our work specifically evaluates models which can generate textual outputs, given one or more images, and text. Recent examples of such models include LLaVA [13], mPLUG-Owl [17], InstructBLIP, LLaMA-Adapter, Flamingo [12] and OpenFlamingo [11], PandaGPT [18], and GPT-4 [7] (which reports multimodal capabilities but has not yet seen a release of the multimodal variant).\n\nInstruction Following: \u201cInstruction-following\u201d is an emerging paradigm for training models via language, where instead of being trained to complete only a single, fixed task (such as image classification or captioning), models are trained to follow textual instructions that describe an arbitrary task, with the aim of generalizing to novel instructions. Examples of instruction-following models include Alpaca [5], LLaMA-Adapter [16], Koala [55], InstructBLIP [14], LLaVA [13], and mPLUG-owl [17]. As the downstream capabilities of these models are influenced by the quality of the training dataset, there has also been extensive work on developing instruction-following datasets [39, 56, 57, 13, 58] .\n\nTo build these models, two broad approaches have been shown to be effective. One approach focuses on leveraging existing pretrained task-specific tools such as image captioners [15], object detectors [59] and text-to-image generators [60] by either creating multimodal prompt interfaces [43, 61] or by executing LLM-generated programs [62, 63, 64]. The other approach [13, 16, 65, 46, 66, 17, 11] focuses on building a single pretrained model that can follow instructions by supervised finetuning on multimodal vision-language data.\n\nDespite the success of both these approaches on the existing vision-language datasets e.g., VQA, GQA, Image Captioning [21, 67, 20], there is a lack of a high-quality benchmarking dataset for multimodal instruction-following tasks that reliably replicates the way in which humans would interact with multimodal chatbots in the wild. Similar to the image-text models discussed above, many instruction-following models have been released directly as open-source without undergoing peer review or thorough evaluation. As a result, the effectiveness of these models for many tasks is not well-understood.\n\nBenchmarks for Machine Learning: High-quality evaluation datasets have served both to (re)assess, and to accelerate, progress on many machine learning tasks [68]. For example, our work draws particularly from the fields of computer vision and natural language processing, where benchmarking datasets have been critical drivers of progress. On the vision side, datasets such as ImageNet [69] and CIFAR [70] have proven to be critical yardsticks of progress. On the language side, benchmarks such as SQuAD [71], SST [72], GLUE/SuperGLUE [73, 74] and more [75, 76] seen wide use. Recent work has indicated that improvements on these high-quality benchmark datasets is not the result of overfitting, and is a reliable indicator of genuine progress beyond the benchmark data [77, 78, 79, 80]. However, high-quality benchmarking datasets and evaluation methods do not yet exist for multimodal instruction-following. As a result, it is difficult to assess progress in this direction, which both reduces the field\u2019s ability to identify true breakthroughs and increases vulnerability to potential pitfalls of evaluation that have hampered progress in other areas of machine learning [68, 81].\n\nConclusion\n\nWe introduce VisIT-Bench, a dynamic benchmark providing a broad evaluation of multimodal chatbots\u2019 capabilities. Going beyond prior efforts, VisIT-Bench\u2019s collection process centers potential real-world use cases, and 70 diverse instruction families encompassing a range of tasks from recognition to complex reasoning. Our benchmark not only offers human-verified reference outputs for all examples but also gives an Elo-based ranking system for multimodal chatbots that correlates with human judgements. Our experiments reveal a gap between model and human performance. We release data, code, and automatic metrics, encouraging", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "C4 [11]), improved software and hardware frameworks, and advances in modality-specific models such as language models (e.g., [10]). Our work specifically evaluates models which can generate textual outputs, given one or more images, and text. Recent examples of such models include LLaVA [13], mPLUG-Owl [17], InstructBLIP, LLaMA-Adapter, Flamingo [12] and OpenFlamingo [11], PandaGPT [18], and GPT-4 [7] (which reports multimodal capabilities but has not yet seen a release of the multimodal variant).\n\nInstruction Following: \u201cInstruction-following\u201d is an emerging paradigm for training models via language, where instead of being trained to complete only a single, fixed task (such as image classification or captioning), models are trained to follow textual instructions that describe an arbitrary task, with the aim of generalizing to novel instructions. Examples of instruction-following models include Alpaca [5], LLaMA-Adapter [16], Koala [55], InstructBLIP [14], LLaVA [13], and mPLUG-owl [17]. As the downstream capabilities of these models are influenced by the quality of the training dataset, there has also been extensive work on developing instruction-following datasets [39, 56, 57, 13, 58] .\n\nTo build these models, two broad approaches have been shown to be effective. One approach focuses on leveraging existing pretrained task-specific tools such as image captioners [15], object detectors [59] and text-to-image generators [60] by either creating multimodal prompt interfaces [43, 61] or by executing LLM-generated programs [62, 63, 64]. The other approach [13, 16, 65, 46, 66, 17, 11] focuses on building a single pretrained model that can follow instructions by supervised finetuning on multimodal vision-language data.\n\nDespite the success of both these approaches on the existing vision-language datasets e.g., VQA, GQA, Image Captioning [21, 67, 20], there is a lack of a high-quality benchmarking dataset for multimodal instruction-following tasks that reliably replicates the way in which humans would interact with multimodal chatbots in the wild. Similar to the image-text models discussed above, many instruction-following models have been released directly as open-source without undergoing peer review or thorough evaluation. As a result, the effectiveness of these models for many tasks is not well-understood.\n\nBenchmarks for Machine Learning: High-quality evaluation datasets have served both to (re)assess, and to accelerate, progress on many machine learning tasks [68]. For example, our work draws particularly from the fields of computer vision and natural language processing, where benchmarking datasets have been critical drivers of progress. On the vision side, datasets such as ImageNet [69] and CIFAR [70] have proven to be critical yardsticks of progress. On the language side, benchmarks such as SQuAD [71], SST [72], GLUE/SuperGLUE [73, 74] and more [75, 76] seen wide use. Recent work has indicated that improvements on these high-quality benchmark datasets is not the result of overfitting, and is a reliable indicator of genuine progress beyond the benchmark data [77, 78, 79, 80]. However, high-quality benchmarking datasets and evaluation methods do not yet exist for multimodal instruction-following. As a result, it is difficult to assess progress in this direction, which both reduces the field\u2019s ability to identify true breakthroughs and increases vulnerability to potential pitfalls of evaluation that have hampered progress in other areas of machine learning [68, 81].\n\nConclusion\n\nWe introduce VisIT-Bench, a dynamic benchmark providing a broad evaluation of multimodal chatbots\u2019 capabilities. Going beyond prior efforts, VisIT-Bench\u2019s collection process centers potential real-world use cases, and 70 diverse instruction families encompassing a range of tasks from recognition to complex reasoning. Our benchmark not only offers human-verified reference outputs for all examples but also gives an Elo-based ranking system for multimodal chatbots that correlates with human judgements. Our experiments reveal a gap between model and human performance. We release data, code, and automatic metrics, encouraging", "md": "C4 [11]), improved software and hardware frameworks, and advances in modality-specific models such as language models (e.g., [10]). Our work specifically evaluates models which can generate textual outputs, given one or more images, and text. Recent examples of such models include LLaVA [13], mPLUG-Owl [17], InstructBLIP, LLaMA-Adapter, Flamingo [12] and OpenFlamingo [11], PandaGPT [18], and GPT-4 [7] (which reports multimodal capabilities but has not yet seen a release of the multimodal variant).\n\nInstruction Following: \u201cInstruction-following\u201d is an emerging paradigm for training models via language, where instead of being trained to complete only a single, fixed task (such as image classification or captioning), models are trained to follow textual instructions that describe an arbitrary task, with the aim of generalizing to novel instructions. Examples of instruction-following models include Alpaca [5], LLaMA-Adapter [16], Koala [55], InstructBLIP [14], LLaVA [13], and mPLUG-owl [17]. As the downstream capabilities of these models are influenced by the quality of the training dataset, there has also been extensive work on developing instruction-following datasets [39, 56, 57, 13, 58] .\n\nTo build these models, two broad approaches have been shown to be effective. One approach focuses on leveraging existing pretrained task-specific tools such as image captioners [15], object detectors [59] and text-to-image generators [60] by either creating multimodal prompt interfaces [43, 61] or by executing LLM-generated programs [62, 63, 64]. The other approach [13, 16, 65, 46, 66, 17, 11] focuses on building a single pretrained model that can follow instructions by supervised finetuning on multimodal vision-language data.\n\nDespite the success of both these approaches on the existing vision-language datasets e.g., VQA, GQA, Image Captioning [21, 67, 20], there is a lack of a high-quality benchmarking dataset for multimodal instruction-following tasks that reliably replicates the way in which humans would interact with multimodal chatbots in the wild. Similar to the image-text models discussed above, many instruction-following models have been released directly as open-source without undergoing peer review or thorough evaluation. As a result, the effectiveness of these models for many tasks is not well-understood.\n\nBenchmarks for Machine Learning: High-quality evaluation datasets have served both to (re)assess, and to accelerate, progress on many machine learning tasks [68]. For example, our work draws particularly from the fields of computer vision and natural language processing, where benchmarking datasets have been critical drivers of progress. On the vision side, datasets such as ImageNet [69] and CIFAR [70] have proven to be critical yardsticks of progress. On the language side, benchmarks such as SQuAD [71], SST [72], GLUE/SuperGLUE [73, 74] and more [75, 76] seen wide use. Recent work has indicated that improvements on these high-quality benchmark datasets is not the result of overfitting, and is a reliable indicator of genuine progress beyond the benchmark data [77, 78, 79, 80]. However, high-quality benchmarking datasets and evaluation methods do not yet exist for multimodal instruction-following. As a result, it is difficult to assess progress in this direction, which both reduces the field\u2019s ability to identify true breakthroughs and increases vulnerability to potential pitfalls of evaluation that have hampered progress in other areas of machine learning [68, 81].\n\nConclusion\n\nWe introduce VisIT-Bench, a dynamic benchmark providing a broad evaluation of multimodal chatbots\u2019 capabilities. Going beyond prior efforts, VisIT-Bench\u2019s collection process centers potential real-world use cases, and 70 diverse instruction families encompassing a range of tasks from recognition to complex reasoning. Our benchmark not only offers human-verified reference outputs for all examples but also gives an Elo-based ranking system for multimodal chatbots that correlates with human judgements. Our experiments reveal a gap between model and human performance. We release data, code, and automatic metrics, encouraging"}]}, {"page": 13, "text": "community involvement. We hope VisIT-Bench can provide a new quantification of progress\nand shortcomings of multimodal AI systems.\n7   Limitations\nAlthough VisIT-Bench covers a wide spectrum of potential use-cases, it does not incorporate\nevery possible vision-language task. We hope to add more categories of tasks over time. In\nterms of dialogue, VisIT-Bench concentrates on single-turn instances with one instruction\nand response. This does not encompass multi-turn interactions between users and chatbots,\nwhich presents a promising direction for future research. Our study focuses on image-text\nmodalities. Future extensions could expand the scope to include other modalities like audio\nand video, enabling a more comprehensive evaluation. Additionally, while the dataset offers\na wide variety of tasks, a larger number of examples per category could provide more depth.\nFinally, while our GPT-4 based metric correlates well with human judgement both at the\ninstance level and at the system level, we see some evidence that the GPT-4 based metric\nhas a stronger preference for GPT-4 based generations compared to humans. Thus, models\nwhich train, e.g., by distilling from GPT-4 outputs, may have an unfair advantage on our\nevaluation.\nAcknowledgements\nWe thank Pang Wei Koh, Ashima Suvarna, Nitzan Guetta and Roee Aharoni for their valuable\nfeedback. Hritik Bansal is supported in part by AFOSR MURI grant FA9550-22-1-0380. RT\nis supported by the NSF GRFP under Grant No. DGE 1656518.\nReferences\n  [1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,\n      Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language\n       assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n  [2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\n       Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training\n       language models to follow instructions with human feedback. Advances in Neural\n       Information Processing Systems, 35:27730\u201327744, 2022.\n  [3] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian\n       Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are\n       zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\n  [4] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza\n       Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar,\n       David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions\n       on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods\n       in Natural Language Processing, pages 5085\u20135109, 2022.\n  [5] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\n       Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-\n       following llama model.      https://github.com/tatsu-lab/stanford_alpaca,\n       2023.\n  [6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\n       Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\n      Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,\n       March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.\n                                              13", "md": "# Research Paper\n\n## Community Involvement\n\nWe hope VisIT-Bench can provide a new quantification of progress and shortcomings of multimodal AI systems.\n\n### Limitations\n\nAlthough VisIT-Bench covers a wide spectrum of potential use-cases, it does not incorporate every possible vision-language task. We hope to add more categories of tasks over time. In terms of dialogue, VisIT-Bench concentrates on single-turn instances with one instruction and response. This does not encompass multi-turn interactions between users and chatbots, which presents a promising direction for future research. Our study focuses on image-text modalities. Future extensions could expand the scope to include other modalities like audio and video, enabling a more comprehensive evaluation. Additionally, while the dataset offers a wide variety of tasks, a larger number of examples per category could provide more depth. Finally, while our GPT-4 based metric correlates well with human judgement both at the instance level and at the system level, we see some evidence that the GPT-4 based metric has a stronger preference for GPT-4 based generations compared to humans. Thus, models which train, e.g., by distilling from GPT-4 outputs, may have an unfair advantage on our evaluation.\n\n### Acknowledgements\n\nWe thank Pang Wei Koh, Ashima Suvarna, Nitzan Guetta and Roee Aharoni for their valuable feedback. Hritik Bansal is supported in part by AFOSR MURI grant FA9550-22-1-0380. RT is supported by the NSF GRFP under Grant No. DGE 1656518.\n\n### References\n\n|[1]|Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.|\n|---|---|\n|[2]|Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.|\n|[3]|Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.|\n|[4]|Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, 2022.|\n|[5]|Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. Link, 2023.|\n|[6]|Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL Link.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Paper", "md": "# Research Paper"}, {"type": "heading", "lvl": 2, "value": "Community Involvement", "md": "## Community Involvement"}, {"type": "text", "value": "We hope VisIT-Bench can provide a new quantification of progress and shortcomings of multimodal AI systems.", "md": "We hope VisIT-Bench can provide a new quantification of progress and shortcomings of multimodal AI systems."}, {"type": "heading", "lvl": 3, "value": "Limitations", "md": "### Limitations"}, {"type": "text", "value": "Although VisIT-Bench covers a wide spectrum of potential use-cases, it does not incorporate every possible vision-language task. We hope to add more categories of tasks over time. In terms of dialogue, VisIT-Bench concentrates on single-turn instances with one instruction and response. This does not encompass multi-turn interactions between users and chatbots, which presents a promising direction for future research. Our study focuses on image-text modalities. Future extensions could expand the scope to include other modalities like audio and video, enabling a more comprehensive evaluation. Additionally, while the dataset offers a wide variety of tasks, a larger number of examples per category could provide more depth. Finally, while our GPT-4 based metric correlates well with human judgement both at the instance level and at the system level, we see some evidence that the GPT-4 based metric has a stronger preference for GPT-4 based generations compared to humans. Thus, models which train, e.g., by distilling from GPT-4 outputs, may have an unfair advantage on our evaluation.", "md": "Although VisIT-Bench covers a wide spectrum of potential use-cases, it does not incorporate every possible vision-language task. We hope to add more categories of tasks over time. In terms of dialogue, VisIT-Bench concentrates on single-turn instances with one instruction and response. This does not encompass multi-turn interactions between users and chatbots, which presents a promising direction for future research. Our study focuses on image-text modalities. Future extensions could expand the scope to include other modalities like audio and video, enabling a more comprehensive evaluation. Additionally, while the dataset offers a wide variety of tasks, a larger number of examples per category could provide more depth. Finally, while our GPT-4 based metric correlates well with human judgement both at the instance level and at the system level, we see some evidence that the GPT-4 based metric has a stronger preference for GPT-4 based generations compared to humans. Thus, models which train, e.g., by distilling from GPT-4 outputs, may have an unfair advantage on our evaluation."}, {"type": "heading", "lvl": 3, "value": "Acknowledgements", "md": "### Acknowledgements"}, {"type": "text", "value": "We thank Pang Wei Koh, Ashima Suvarna, Nitzan Guetta and Roee Aharoni for their valuable feedback. Hritik Bansal is supported in part by AFOSR MURI grant FA9550-22-1-0380. RT is supported by the NSF GRFP under Grant No. DGE 1656518.", "md": "We thank Pang Wei Koh, Ashima Suvarna, Nitzan Guetta and Roee Aharoni for their valuable feedback. Hritik Bansal is supported in part by AFOSR MURI grant FA9550-22-1-0380. RT is supported by the NSF GRFP under Grant No. DGE 1656518."}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "table", "rows": [["[1]", "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021."], ["[2]", "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022."], ["[3]", "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021."], ["[4]", "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, 2022."], ["[5]", "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. Link, 2023."], ["[6]", "Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL Link."]], "md": "|[1]|Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.|\n|---|---|\n|[2]|Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.|\n|[3]|Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.|\n|[4]|Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, 2022.|\n|[5]|Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. Link, 2023.|\n|[6]|Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL Link.|", "isPerfectTable": true, "csv": "\"[1]\",\"Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\"\n\"[2]\",\"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\"\n\"[3]\",\"Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\"\n\"[4]\",\"Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5085\u20135109, 2022.\"\n\"[5]\",\"Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. Link, 2023.\"\n\"[6]\",\"Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL Link.\""}]}, {"page": 14, "text": "  [7] OpenAI. Gpt-4 technical report. arXiv, 2023.\n  [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\n      Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\n      Language models are few-shot learners. Advances in neural information processing\n      systems, 33:1877\u20131901, 2020.\n  [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\n      Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\n      Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\n      arXiv:2204.02311, 2022.\n[10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\n      Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\n      Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\n      arXiv:2302.13971, 2023.\n[11] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong\n      Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith,\n      Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open-\n      flamingo, March 2023. URL https://doi.org/10.5281/zenodo.7733589.\n[12] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\n      Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,\n      Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mari-\n      anne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh,\n      Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew\n      Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot\n      learning. ArXiv, abs/2204.14198, 2022.\n[13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction\n      tuning, 2023.\n[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\n      Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards\n      general-purpose vision-language models with instruction tuning.      arXiv preprint\n      arXiv:2305.06500, 2023.\n[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping\n      language-image pre-training with frozen image encoders and large language models.\n      arXiv preprint arXiv:2301.12597, 2023.\n[16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang,\n      Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual\n      instruction model. arXiv preprint arXiv:2304.15010, 2023.\n[17] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang\n      Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization em-\n      powers large language models with multimodality. arXiv preprint arXiv:2304.14178,\n      2023.\n[18] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt:\n      One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.\n[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\n      Making the V in vqa matter: Elevating the role of image understanding in visual\n      question answering. In Proceedings of the IEEE conference on computer vision and\n      pattern recognition, pages 6904\u20136913, 2017.\n                                           14", "md": "- [7] OpenAI. Gpt-4 technical report. arXiv, 2023.\n- [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing\nsystems, 33:1877\u20131901, 2020.\n- [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\n- [10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023.\n- [11] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong\nZhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith,\nPang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open-\nflamingo, March 2023. URL https://doi.org/10.5281/zenodo.7733589.\n- [12] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\nHasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mari-\nanne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh,\nSahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew\nZisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot\nlearning. ArXiv, abs/2204.14198, 2022.\n- [13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction\ntuning, 2023.\n- [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards\ngeneral-purpose vision-language models with instruction tuning. arXiv preprint\narXiv:2305.06500, 2023.\n- [15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language models.\narXiv preprint arXiv:2301.12597, 2023.\n- [16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang,\nPan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual\ninstruction model. arXiv preprint arXiv:2304.15010, 2023.\n- [17] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang\nWang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization em-\npowers large language models with multimodality. arXiv preprint arXiv:2304.14178,\n2023.\n- [18] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt:\nOne model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.\n- [19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\nMaking the V in vqa matter: Elevating the role of image understanding in visual\nquestion answering. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 6904\u20136913, 2017.", "images": [], "items": [{"type": "text", "value": "- [7] OpenAI. Gpt-4 technical report. arXiv, 2023.\n- [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing\nsystems, 33:1877\u20131901, 2020.\n- [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\n- [10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023.\n- [11] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong\nZhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith,\nPang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open-\nflamingo, March 2023. URL https://doi.org/10.5281/zenodo.7733589.\n- [12] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\nHasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mari-\nanne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh,\nSahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew\nZisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot\nlearning. ArXiv, abs/2204.14198, 2022.\n- [13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction\ntuning, 2023.\n- [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards\ngeneral-purpose vision-language models with instruction tuning. arXiv preprint\narXiv:2305.06500, 2023.\n- [15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language models.\narXiv preprint arXiv:2301.12597, 2023.\n- [16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang,\nPan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual\ninstruction model. arXiv preprint arXiv:2304.15010, 2023.\n- [17] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang\nWang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization em-\npowers large language models with multimodality. arXiv preprint arXiv:2304.14178,\n2023.\n- [18] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt:\nOne model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.\n- [19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\nMaking the V in vqa matter: Elevating the role of image understanding in visual\nquestion answering. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 6904\u20136913, 2017.", "md": "- [7] OpenAI. Gpt-4 technical report. arXiv, 2023.\n- [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing\nsystems, 33:1877\u20131901, 2020.\n- [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\n- [10] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\nLachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023.\n- [11] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong\nZhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith,\nPang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Open-\nflamingo, March 2023. URL https://doi.org/10.5281/zenodo.7733589.\n- [12] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\nHasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mari-\nanne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh,\nSahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew\nZisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot\nlearning. ArXiv, abs/2204.14198, 2022.\n- [13] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction\ntuning, 2023.\n- [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,\nWeisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards\ngeneral-purpose vision-language models with instruction tuning. arXiv preprint\narXiv:2305.06500, 2023.\n- [15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language models.\narXiv preprint arXiv:2301.12597, 2023.\n- [16] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang,\nPan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual\ninstruction model. arXiv preprint arXiv:2304.15010, 2023.\n- [17] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang\nWang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization em-\npowers large language models with multimodality. arXiv preprint arXiv:2304.14178,\n2023.\n- [18] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt:\nOne model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.\n- [19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.\nMaking the V in vqa matter: Elevating the role of image understanding in visual\nquestion answering. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 6904\u20136913, 2017."}]}, {"page": 15, "text": "[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\n      Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects\n      in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich,\n      Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer,\n      2014.\n[21] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\n      C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings\n      of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\n[22] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In\n      Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n      4624\u20134633, 2019.\n[23] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio\n      Ferrari. Connecting vision and language with localized narratives. In Computer\n      Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020,\n      Proceedings, Part V 16, pages 647\u2013664. Springer, 2020.\n[24] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo\n      Luo.   Promptcap: Prompt-guided task-aware image captioning.         arXiv preprint\n      arXiv:2211.09699, 2022.\n[25] Arpad E Elo. The proposed uscf rating system. its development, theory, and applica-\n      tions. Chess Life, 22(8):242\u2013247, 1967.\n[26] Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, and\n      Ion Stoica. Chatbot arena: Benchmarking llms in the wild with elo ratings. 2023.\n      URL https://lmsys.org/blog/2023-05-03-arena/.\n[27] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for\n      automatic evaluation of machine translation. In ACL, 2002.\n[28] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text\n      Summarization Branches Out, 2004.\n[29] Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for mt evaluation\n      with improved correlation with human judgments. In ACL workshop on Evaluation\n      Measures for MT and Summarization, 2005.\n[30] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-\n      based image description evaluation. In CVPR, 2015.\n[31] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.\n      BERTScore: Evaluating text generation with BERT. In ICLR, 2020.\n[32] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal\n      zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.\n[33] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li,\n     Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards\n      multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023.\n[34] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng,\n      Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation\n      benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.\n[35] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang.\n      Aligning large multi-modal model with robust instruction tuning. arXiv preprint\n      arXiv:2306.14565, 2023.\n                                           15", "md": "1. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n2. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\n3. Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4624\u20134633, 2019.\n4. Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pages 647\u2013664. Springer, 2020.\n5. Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699, 2022.\n6. Arpad E Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):242\u2013247, 1967.\n7. Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: Benchmarking llms in the wild with elo ratings. 2023. URL https://lmsys.org/blog/2023-05-03-arena/.\n8. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002.\n9. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, 2004.\n10. Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for mt evaluation with improved correlation with human judgments. In ACL workshop on Evaluation Measures for MT and Summarization, 2005.\n11. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015.\n12. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with BERT. In ICLR, 2020.\n13. Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.\n14. Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023.\n15. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.\n16. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.", "images": [], "items": [{"type": "text", "value": "1. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n2. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\n3. Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4624\u20134633, 2019.\n4. Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pages 647\u2013664. Springer, 2020.\n5. Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699, 2022.\n6. Arpad E Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):242\u2013247, 1967.\n7. Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: Benchmarking llms in the wild with elo ratings. 2023. URL https://lmsys.org/blog/2023-05-03-arena/.\n8. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002.\n9. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, 2004.\n10. Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for mt evaluation with improved correlation with human judgments. In ACL workshop on Evaluation Measures for MT and Summarization, 2005.\n11. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015.\n12. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with BERT. In ICLR, 2020.\n13. Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.\n14. Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023.\n15. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.\n16. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.", "md": "1. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n2. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\n3. Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust change captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4624\u20134633, 2019.\n4. Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16, pages 647\u2013664. Springer, 2020.\n5. Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699, 2022.\n6. Arpad E Elo. The proposed uscf rating system. its development, theory, and applications. Chess Life, 22(8):242\u2013247, 1967.\n7. Lianmin Zheng, Ying Sheng, Wei-Lin Chiang, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: Benchmarking llms in the wild with elo ratings. 2023. URL https://lmsys.org/blog/2023-05-03-arena/.\n8. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In ACL, 2002.\n9. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization Branches Out, 2004.\n10. Satanjeev Banerjee and Alon Lavie. METEOR: an automatic metric for mt evaluation with improved correlation with human judgments. In ACL workshop on Evaluation Measures for MT and Summarization, 2005.\n11. Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015.\n12. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. BERTScore: Evaluating text generation with BERT. In ICLR, 2020.\n13. Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. arXiv preprint arXiv:2212.10773, 2022.\n14. Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387, 2023.\n15. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023.\n16. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023."}]}, {"page": 16, "text": "[36] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A\n      corpus for reasoning about natural language grounded in photographs. arXiv preprint\n      arXiv:1811.00491, 2018.\n[37] Sarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Re-\n      framing human-ai collaboration for generating free-text explanations. arXiv preprint\n      arXiv:2112.08674, 2021.\n[38] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong,\n      Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani,\n      et al. Socratic models: Composing zero-shot multimodal reasoning with language.\n      arXiv preprint arXiv:2204.00598, 2022.\n[39] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel\n      Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self\n      generated instructions. arXiv preprint arXiv:2212.10560, 2022.\n[40] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look\n      once: Unified, real-time object detection. In Proceedings of the IEEE conference on\n      computer vision and pattern recognition, pages 779\u2013788, 2016.\n[41] Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici,\n      Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision-\n      and-language benchmark of synthetic and compositional images. arXiv preprint\n      arXiv:2303.07274, 2023.\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-\n      4: Enhancing vision-language understanding with advanced large language models.\n      arXiv preprint arXiv:2304.10592, 2023.\n[43] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan\n      Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models.\n      arXiv preprint arXiv:2303.04671, 2023.\n[44] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,\n      Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and\n      language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.\n[45] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu,\n      Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo:\n      An open-source framework for training large autoregressive vision-language models.\n      arXiv preprint arXiv:2308.01390, 2023.\n[46] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei\n      Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint\n      arXiv:2305.03726, 2023.\n[47] Yupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong\n      Lu. Sparkles: Unlocking chats across multiple images for multimodal instruction-\n      following models. arXiv preprint arXiv:2308.16463, 2023.\n[48] Hugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh,\n      Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe\n      Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset\n      of interleaved image-text documents, 2023.\n[49] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n      Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\n      optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n                                           16", "md": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Reframing human-ai collaboration for generating free-text explanations. arXiv preprint arXiv:2112.08674, 2021.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779\u2013788, 2016.\nNitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision- and-language benchmark of synthetic and compositional images. arXiv preprint arXiv:2303.07274, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\nYupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. Sparkles: Unlocking chats across multiple images for multimodal instruction- following models. arXiv preprint arXiv:2308.16463, 2023.\nHugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.", "images": [], "items": [{"type": "text", "value": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Reframing human-ai collaboration for generating free-text explanations. arXiv preprint arXiv:2112.08674, 2021.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779\u2013788, 2016.\nNitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision- and-language benchmark of synthetic and compositional images. arXiv preprint arXiv:2303.07274, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\nYupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. Sparkles: Unlocking chats across multiple images for multimodal instruction- following models. arXiv preprint arXiv:2308.16463, 2023.\nHugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.", "md": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about natural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. Reframing human-ai collaboration for generating free-text explanations. arXiv preprint arXiv:2112.08674, 2021.\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779\u2013788, 2016.\nNitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. Breaking common sense: Whoops! a vision- and-language benchmark of synthetic and compositional images. arXiv preprint arXiv:2303.07274, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXiv preprint arXiv:2305.04790, 2023.\nAnas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\nYupan Huang, Zaiqiao Meng, Fangyu Liu, Yixuan Su, Nigel Collier, and Yutong Lu. Sparkles: Unlocking chats across multiple images for multimodal instruction- following models. arXiv preprint arXiv:2308.16463, 2023.\nHugo Lauren\u00e7on, Lucile Saulnier, L\u00e9o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents, 2023.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019."}]}, {"page": 17, "text": "[50] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba,\n     Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation\n     framework for methods that learn from human feedback, 2023.\n[51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and\n     Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models.\n     In NeurIPS, 2022. URL https://arxiv.org/abs/2201.11903.\n[52] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke\n     Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. URL\n     https://arxiv.org/abs/2205.11916.\n[53] Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, and Louis-Philippe\n     Morency. Cross-modal attention congruence regularization for vision-language rela-\n     tion alignment. arXiv preprint arXiv:2212.10549, 2022.\n[54] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\n     man, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\n     et al. Laion-5b: An open large-scale dataset for training next generation image-text\n     models. arXiv preprint arXiv:2210.08402, 2022.\n[55] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey\n     Levine, and Dawn Song. Koala: A dialogue model for academic research. Blog post,\n     April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.\n[56] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruc-\n     tion tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.\n[57] Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, and Kai-Wei\n     Chang. Dynosaur: A dynamic growth paradigm for instruction-tuning data curation.\n     arXiv preprint arXiv:2305.14327, 2023.\n[58] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,\n     Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint\n     arXiv:2305.11206, 2023.\n[59] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan\n     Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.\n     Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference\n     on Computer Vision and Pattern Recognition, pages 10965\u201310975, 2022.\n[60] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\n     Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings\n     of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n     10684\u201310695, 2022.\n[61] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal\n     Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting\n     chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n[62] D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python\n     execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.\n[63] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional\n     visual reasoning without training. In Proceedings of the IEEE/CVF Conference on\n     Computer Vision and Pattern Recognition, pages 14953\u201314962, 2023.\n[64] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu,\n     Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reason-\n     ing with large language models. arXiv preprint arXiv:2304.09842, 2023.\n                                           17", "md": "1. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. *Alpacafarm: A simulation framework for methods that learn from human feedback*, 2023.\n2. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. *Chain-of-thought prompting elicits reasoning in large language models*. In NeurIPS, 2022. URL https://arxiv.org/abs/2201.11903.\n3. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. *Large language models are zero-shot reasoners*. In NeurIPS, 2022. URL https://arxiv.org/abs/2205.11916.\n4. Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, and Louis-Philippe Morency. *Cross-modal attention congruence regularization for vision-language relation alignment*. arXiv preprint arXiv:2212.10549, 2022.\n5. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. *Laion-5b: An open large-scale dataset for training next generation image-text models*. arXiv preprint arXiv:2210.08402, 2022.\n6. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. *Koala: A dialogue model for academic research*. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.\n7. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. *Instruction tuning with gpt-4*. arXiv preprint arXiv:2304.03277, 2023.\n8. Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, and Kai-Wei Chang. *Dynosaur: A dynamic growth paradigm for instruction-tuning data curation*. arXiv preprint arXiv:2305.14327, 2023.\n9. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. *Lima: Less is more for alignment*. arXiv preprint arXiv:2305.11206, 2023.\n10. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. *Grounded language-image pre-training*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965\u201310975, 2022.\n11. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. *High-resolution image synthesis with latent diffusion models*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n12. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. *Mm-react: Prompting chatgpt for multimodal reasoning and action*. arXiv preprint arXiv:2303.11381, 2023.\n13. D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. *Vipergpt: Visual inference via python execution for reasoning*. arXiv preprint arXiv:2303.08128, 2023.\n14. Tanmay Gupta and Aniruddha Kembhavi. *Visual programming: Compositional visual reasoning without training*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953\u201314962, 2023.\n15. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. *Chameleon: Plug-and-play compositional reasoning with large language models*. arXiv preprint arXiv:2304.09842, 2023.", "images": [], "items": [{"type": "text", "value": "1. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. *Alpacafarm: A simulation framework for methods that learn from human feedback*, 2023.\n2. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. *Chain-of-thought prompting elicits reasoning in large language models*. In NeurIPS, 2022. URL https://arxiv.org/abs/2201.11903.\n3. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. *Large language models are zero-shot reasoners*. In NeurIPS, 2022. URL https://arxiv.org/abs/2205.11916.\n4. Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, and Louis-Philippe Morency. *Cross-modal attention congruence regularization for vision-language relation alignment*. arXiv preprint arXiv:2212.10549, 2022.\n5. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. *Laion-5b: An open large-scale dataset for training next generation image-text models*. arXiv preprint arXiv:2210.08402, 2022.\n6. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. *Koala: A dialogue model for academic research*. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.\n7. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. *Instruction tuning with gpt-4*. arXiv preprint arXiv:2304.03277, 2023.\n8. Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, and Kai-Wei Chang. *Dynosaur: A dynamic growth paradigm for instruction-tuning data curation*. arXiv preprint arXiv:2305.14327, 2023.\n9. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. *Lima: Less is more for alignment*. arXiv preprint arXiv:2305.11206, 2023.\n10. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. *Grounded language-image pre-training*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965\u201310975, 2022.\n11. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. *High-resolution image synthesis with latent diffusion models*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n12. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. *Mm-react: Prompting chatgpt for multimodal reasoning and action*. arXiv preprint arXiv:2303.11381, 2023.\n13. D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. *Vipergpt: Visual inference via python execution for reasoning*. arXiv preprint arXiv:2303.08128, 2023.\n14. Tanmay Gupta and Aniruddha Kembhavi. *Visual programming: Compositional visual reasoning without training*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953\u201314962, 2023.\n15. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. *Chameleon: Plug-and-play compositional reasoning with large language models*. arXiv preprint arXiv:2304.09842, 2023.", "md": "1. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. *Alpacafarm: A simulation framework for methods that learn from human feedback*, 2023.\n2. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. *Chain-of-thought prompting elicits reasoning in large language models*. In NeurIPS, 2022. URL https://arxiv.org/abs/2201.11903.\n3. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. *Large language models are zero-shot reasoners*. In NeurIPS, 2022. URL https://arxiv.org/abs/2205.11916.\n4. Rohan Pandey, Rulin Shao, Paul Pu Liang, Ruslan Salakhutdinov, and Louis-Philippe Morency. *Cross-modal attention congruence regularization for vision-language relation alignment*. arXiv preprint arXiv:2212.10549, 2022.\n5. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. *Laion-5b: An open large-scale dataset for training next generation image-text models*. arXiv preprint arXiv:2210.08402, 2022.\n6. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. *Koala: A dialogue model for academic research*. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.\n7. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. *Instruction tuning with gpt-4*. arXiv preprint arXiv:2304.03277, 2023.\n8. Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, and Kai-Wei Chang. *Dynosaur: A dynamic growth paradigm for instruction-tuning data curation*. arXiv preprint arXiv:2305.14327, 2023.\n9. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. *Lima: Less is more for alignment*. arXiv preprint arXiv:2305.11206, 2023.\n10. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. *Grounded language-image pre-training*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10965\u201310975, 2022.\n11. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. *High-resolution image synthesis with latent diffusion models*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n12. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. *Mm-react: Prompting chatgpt for multimodal reasoning and action*. arXiv preprint arXiv:2303.11381, 2023.\n13. D\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. *Vipergpt: Visual inference via python execution for reasoning*. arXiv preprint arXiv:2303.08128, 2023.\n14. Tanmay Gupta and Aniruddha Kembhavi. *Visual programming: Compositional visual reasoning without training*. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953\u201314962, 2023.\n15. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. *Chameleon: Plug-and-play compositional reasoning with large language models*. arXiv preprint arXiv:2304.09842, 2023."}]}, {"page": 18, "text": "[65] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,\n     Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and\n     language model for dialogue with humans, 2023.\n[66] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming\n     Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language\n     is not all you need: Aligning perception with language models.        arXiv preprint\n     arXiv:2302.14045, 2023.\n[67] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\n     reasoning and compositional question answering. In Proceedings of the IEEE/CVF\n     conference on computer vision and pattern recognition, pages 6700\u20136709, 2019.\n[68] Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are\n     we learning yet? a meta review of evaluation failures across machine learning.\n     In Thirty-fifth Conference on Neural Information Processing Systems Datasets and\n     Benchmarks Track (Round 2), 2021.\n[69] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\n     Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet\n     large scale visual recognition challenge. International journal of computer vision,\n     115:211\u2013252, 2015.\n[70] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from\n     tiny images. 2009.\n[71] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unan-\n     swerable questions for squad. arXiv preprint arXiv:1806.03822, 2018.\n[72] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\n     Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic com-\n     positionality over a sentiment treebank. In Proceedings of the 2013 conference on\n     empirical methods in natural language processing, pages 1631\u20131642, 2013.\n[73] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\n     Bowman. Glue: A multi-task benchmark and analysis platform for natural language\n     understanding. arXiv preprint arXiv:1804.07461, 2018.\n[74] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\n     Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for\n     general-purpose language understanding systems. Advances in neural information\n     processing systems, 32, 2019.\n[75] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\n     Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al.           A\n     framework for few-shot language model evaluation. Version v0. 0.1. Sept, 2021.\n[76] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro\n     Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic\n     evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\n[77] John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of\n     natural distribution shift on question answering models. In International Conference\n     on Machine Learning, pages 6905\u20136916. PMLR, 2020.\n[78] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do\n     cifar-10 classifiers generalize to cifar-10? arXiv preprint arXiv:1806.00451, 2018.\n                                           18", "md": "- Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,\nKuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and\nlanguage model for dialogue with humans, 2023.\n- Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming\nMa, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language\nis not all you need: Aligning perception with language models. arXiv preprint\narXiv:2302.14045, 2023.\n- Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\nreasoning and compositional question answering. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 6700\u20136709, 2019.\n- Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are\nwe learning yet? a meta review of evaluation failures across machine learning.\nIn Thirty-fifth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2), 2021.\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. International journal of computer vision,\n115:211\u2013252, 2015.\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from\ntiny images. 2009.\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad.\narXiv preprint arXiv:1806.03822, 2018.\n- Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. Glue: A multi-task benchmark and analysis platform for natural language understanding.\narXiv preprint arXiv:1804.07461, 2018.\n- Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems.\nAdvances in neural information processing systems, 32, 2019.\n- Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A\nframework for few-shot language model evaluation. Version v0. 0.1. Sept, 2021.\n- Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro\nYasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic\nevaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\n- John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of\nnatural distribution shift on question answering models. In International Conference\non Machine Learning, pages 6905\u20136916. PMLR, 2020.\n- Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do\ncifar-10 classifiers generalize to cifar-10?. arXiv preprint arXiv:1806.00451, 2018.", "images": [], "items": [{"type": "text", "value": "- Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,\nKuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and\nlanguage model for dialogue with humans, 2023.\n- Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming\nMa, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language\nis not all you need: Aligning perception with language models. arXiv preprint\narXiv:2302.14045, 2023.\n- Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\nreasoning and compositional question answering. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 6700\u20136709, 2019.\n- Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are\nwe learning yet? a meta review of evaluation failures across machine learning.\nIn Thirty-fifth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2), 2021.\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. International journal of computer vision,\n115:211\u2013252, 2015.\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from\ntiny images. 2009.\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad.\narXiv preprint arXiv:1806.03822, 2018.\n- Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. Glue: A multi-task benchmark and analysis platform for natural language understanding.\narXiv preprint arXiv:1804.07461, 2018.\n- Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems.\nAdvances in neural information processing systems, 32, 2019.\n- Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A\nframework for few-shot language model evaluation. Version v0. 0.1. Sept, 2021.\n- Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro\nYasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic\nevaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\n- John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of\nnatural distribution shift on question answering models. In International Conference\non Machine Learning, pages 6905\u20136916. PMLR, 2020.\n- Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do\ncifar-10 classifiers generalize to cifar-10?. arXiv preprint arXiv:1806.00451, 2018.", "md": "- Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,\nKuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and\nlanguage model for dialogue with humans, 2023.\n- Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming\nMa, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language\nis not all you need: Aligning perception with language models. arXiv preprint\narXiv:2302.14045, 2023.\n- Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual\nreasoning and compositional question answering. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 6700\u20136709, 2019.\n- Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are\nwe learning yet? a meta review of evaluation failures across machine learning.\nIn Thirty-fifth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (Round 2), 2021.\n- Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,\nZhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. International journal of computer vision,\n115:211\u2013252, 2015.\n- Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from\ntiny images. 2009.\n- Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions for squad.\narXiv preprint arXiv:1806.03822, 2018.\n- Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013.\n- Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. Glue: A multi-task benchmark and analysis platform for natural language understanding.\narXiv preprint arXiv:1804.07461, 2018.\n- Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\nFelix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems.\nAdvances in neural information processing systems, 32, 2019.\n- Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A\nframework for few-shot language model evaluation. Version v0. 0.1. Sept, 2021.\n- Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro\nYasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic\nevaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\n- John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of\nnatural distribution shift on question answering models. In International Conference\non Machine Learning, pages 6905\u20136916. PMLR, 2020.\n- Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do\ncifar-10 classifiers generalize to cifar-10?. arXiv preprint arXiv:1806.00451, 2018."}]}, {"page": 19, "text": "[79] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do\n     imagenet classifiers generalize to imagenet? In International conference on machine\n     learning, pages 5389\u20135400. PMLR, 2019.\n[80] Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz\n     Hardt, John Miller, and Ludwig Schmidt. A meta-analysis of overfitting in machine\n     learning. Advances in Neural Information Processing Systems, 32, 2019.\n[81] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen.\n     A survey on multimodal large language models. arXiv preprint arXiv:2306.13549,\n     2023.\n[82] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploita-\n     tion. arXiv preprint arXiv:2203.08242, 2022.\n[83] Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test\n     data in plain text: Practical strategies for mitigating data contamination by evaluation\n     benchmarks. arXiv preprint arXiv:2305.10160, 2023.\n[84] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cog-\n     nition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference\n     on computer vision and pattern recognition, pages 6720\u20136731, 2019.\n[85] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the\n     vision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860,\n     2021.\n[86] Jack Hessel, Ana Marasovi\u00b4    c, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers,\n     Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor\"\n     understanding\" benchmarks from the new yorker caption contest. arXiv preprint\n     arXiv:2209.06293, 2022.\n[87] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei,\n     C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional\n     language and elementary visual reasoning. In Proceedings of the IEEE conference\n     on computer vision and pattern recognition, pages 2901\u20132910, 2017.\n[88] Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D\n     Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. arXiv\n     preprint arXiv:2211.16492, 2022.\n[89] Shivam Sharma, Siddhant Agarwal, Tharun Suresh, Preslav Nakov, Md Shad Akhtar,\n     and Tanmoy Charkraborty. What do you meme? generating explanations for visual\n     semantic role labelling in memes. arXiv preprint arXiv:2212.00715, 2022.\n[90] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu,\n     Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal\n     reasoning via thought chains for science question answering. In The 36th Conference\n     on Neural Information Processing Systems (NeurIPS), 2022.\n[91] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa:\n     A visual question answering benchmark requiring external knowledge. In Proceedings\n     of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u2013\n     3204, 2019.\n[92] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and\n     Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world\n     knowledge. arXiv, 2022.\n                                            19", "md": "- [79] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389\u20135400. PMLR, 2019.\n- [80] Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, and Ludwig Schmidt. A meta-analysis of overfitting in machine learning. Advances in Neural Information Processing Systems, 32, 2019.\n- [81] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\n- [82] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242, 2022.\n- [83] Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. arXiv preprint arXiv:2305.10160, 2023.\n- [84] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720\u20136731, 2019.\n- [85] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860, 2021.\n- [86] Jack Hessel, Ana Marasovi\u00b4 c, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor understanding benchmarks from the new yorker caption contest. arXiv preprint arXiv:2209.06293, 2022.\n- [87] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910, 2017.\n- [88] Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. arXiv preprint arXiv:2211.16492, 2022.\n- [89] Shivam Sharma, Siddhant Agarwal, Tharun Suresh, Preslav Nakov, Md Shad Akhtar, and Tanmoy Charkraborty. What do you meme? generating explanations for visual semantic role labelling in memes. arXiv preprint arXiv:2212.00715, 2022.\n- [90] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.\n- [91] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204, 2019.\n- [92] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. arXiv, 2022.", "images": [], "items": [{"type": "text", "value": "- [79] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389\u20135400. PMLR, 2019.\n- [80] Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, and Ludwig Schmidt. A meta-analysis of overfitting in machine learning. Advances in Neural Information Processing Systems, 32, 2019.\n- [81] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\n- [82] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242, 2022.\n- [83] Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. arXiv preprint arXiv:2305.10160, 2023.\n- [84] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720\u20136731, 2019.\n- [85] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860, 2021.\n- [86] Jack Hessel, Ana Marasovi\u00b4 c, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor understanding benchmarks from the new yorker caption contest. arXiv preprint arXiv:2209.06293, 2022.\n- [87] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910, 2017.\n- [88] Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. arXiv preprint arXiv:2211.16492, 2022.\n- [89] Shivam Sharma, Siddhant Agarwal, Tharun Suresh, Preslav Nakov, Md Shad Akhtar, and Tanmoy Charkraborty. What do you meme? generating explanations for visual semantic role labelling in memes. arXiv preprint arXiv:2212.00715, 2022.\n- [90] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.\n- [91] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204, 2019.\n- [92] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. arXiv, 2022.", "md": "- [79] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389\u20135400. PMLR, 2019.\n- [80] Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, and Ludwig Schmidt. A meta-analysis of overfitting in machine learning. Advances in Neural Information Processing Systems, 32, 2019.\n- [81] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023.\n- [82] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. arXiv preprint arXiv:2203.08242, 2022.\n- [83] Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. arXiv preprint arXiv:2305.10160, 2023.\n- [84] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720\u20136731, 2019.\n- [85] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision: Geo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860, 2021.\n- [86] Jack Hessel, Ana Marasovi\u00b4 c, Jena D Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor understanding benchmarks from the new yorker caption contest. arXiv preprint arXiv:2209.06293, 2022.\n- [87] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910, 2017.\n- [88] Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert D Hawkins, and Yoav Artzi. Abstract visual reasoning with tangram shapes. arXiv preprint arXiv:2211.16492, 2022.\n- [89] Shivam Sharma, Siddhant Agarwal, Tharun Suresh, Preslav Nakov, Md Shad Akhtar, and Tanmoy Charkraborty. What do you meme? generating explanations for visual semantic role labelling in memes. arXiv preprint arXiv:2212.00715, 2022.\n- [90] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.\n- [91] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204, 2019.\n- [92] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A benchmark for visual question answering using world knowledge. arXiv, 2022."}]}, {"page": 20, "text": " [93] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C\n       Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz:\n       nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM\n       symposium on User interface software and technology, pages 333\u2013342, 2010.\n [94] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a\n       dataset for image captioning with reading comprehension. In Computer Vision\u2013ECCV\n       2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings,\n       Part II 16, pages 742\u2013758. Springer, 2020.\n [95] Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti,\n       and Siva Reddy.    Image retrieval from contextual descriptions.     arXiv preprint\n       arXiv:2203.15867, 2022.\n [96] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences be-\n       tween pairs of similar images. arXiv preprint arXiv:1808.10584, 2018.\n [97] Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel\n       Stanovsky.    Vasr:  Visual analogies of situation recognition.     arXiv preprint\n       arXiv:2212.04542, 2022.\n [98] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal,\n       Gabriel Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to\n       challenge vision-and-language models. Advances in Neural Information Processing\n       Systems, 35:26549\u201326564, 2022.\n [99] Ron Yosef, Yonatan Bitton, and Dafna Shahaf. Irfl: Image recognition of figurative\n       language. arXiv preprint arXiv:2303.15445, 2023.\n[100] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu,\n       Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram\n       understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021.\n[101] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer\n       Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation.\n       arXiv preprint arXiv:2305.01569, 2023.\n                                            20", "md": "- [93] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333\u2013342, 2010.\n- [94] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer, 2020.\n- [95] Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti, and Siva Reddy. Image retrieval from contextual descriptions. arXiv preprint arXiv:2203.15867, 2022.\n- [96] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of similar images. arXiv preprint arXiv:1808.10584, 2018.\n- [97] Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel Stanovsky. Vasr: Visual analogies of situation recognition. arXiv preprint arXiv:2212.04542, 2022.\n- [98] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to challenge vision-and-language models. Advances in Neural Information Processing Systems, 35:26549\u201326564, 2022.\n- [99] Ron Yosef, Yonatan Bitton, and Dafna Shahaf. Irfl: Image recognition of figurative language. arXiv preprint arXiv:2303.15445, 2023.\n- [100] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021.\n- [101] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023.", "images": [], "items": [{"type": "text", "value": "- [93] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333\u2013342, 2010.\n- [94] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer, 2020.\n- [95] Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti, and Siva Reddy. Image retrieval from contextual descriptions. arXiv preprint arXiv:2203.15867, 2022.\n- [96] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of similar images. arXiv preprint arXiv:1808.10584, 2018.\n- [97] Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel Stanovsky. Vasr: Visual analogies of situation recognition. arXiv preprint arXiv:2212.04542, 2022.\n- [98] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to challenge vision-and-language models. Advances in Neural Information Processing Systems, 35:26549\u201326564, 2022.\n- [99] Ron Yosef, Yonatan Bitton, and Dafna Shahaf. Irfl: Image recognition of figurative language. arXiv preprint arXiv:2303.15445, 2023.\n- [100] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021.\n- [101] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023.", "md": "- [93] Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333\u2013342, 2010.\n- [94] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer, 2020.\n- [95] Benno Krojer, Vaibhav Adlakha, Vibhav Vineet, Yash Goyal, Edoardo Ponti, and Siva Reddy. Image retrieval from contextual descriptions. arXiv preprint arXiv:2203.15867, 2022.\n- [96] Harsh Jhamtani and Taylor Berg-Kirkpatrick. Learning to describe differences between pairs of similar images. arXiv preprint arXiv:1808.10584, 2018.\n- [97] Yonatan Bitton, Ron Yosef, Eli Strugo, Dafna Shahaf, Roy Schwartz, and Gabriel Stanovsky. Vasr: Visual analogies of situation recognition. arXiv preprint arXiv:2212.04542, 2022.\n- [98] Yonatan Bitton, Nitzan Bitton Guetta, Ron Yosef, Yuval Elovici, Mohit Bansal, Gabriel Stanovsky, and Roy Schwartz. Winogavil: Gamified association benchmark to challenge vision-and-language models. Advances in Neural Information Processing Systems, 35:26549\u201326564, 2022.\n- [99] Ron Yosef, Yonatan Bitton, and Dafna Shahaf. Irfl: Image recognition of figurative language. arXiv preprint arXiv:2303.15445, 2023.\n- [100] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXiv preprint arXiv:2110.13214, 2021.\n- [101] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint arXiv:2305.01569, 2023."}]}, {"page": 21, "text": "Appendix\nA          License and Intended Use\nThe VisIT-Bench dataset, along with its various contributions such as instructions, reference\noutputs, and model ranking annotations, is licensed under the Creative Commons Attribution\n4.0 International License (CC BY 4.0). This license applies to all the images we have\ndirectly contributed, each of which carries a public license specification in the \u201cpublic\nimages metadata\u201d field within the dataset sheets. However, the dataset also incorporates\nimages sourced from pre-existing collections. For these images, the original licensing terms\nare respected and remain applicable.\nVisIT-Bench\u2019s primary purpose is to function as a dynamic benchmark that continuously\nevolves and evaluates instruction-following vision-language models. In the current landscape,\ncommercial chatbots are often trained on non-disclosed and non-public datasets, which\nraises concerns about potential data contamination and inadvertent training on our evaluation\ndata [8]. This risk is further highlighted by recent studies [82, 83]. To mitigate such concerns,\nwe have chosen to withhold the complete VisIT-Bench test set from public disclosure, while\nstill making the images and instructions available for direct download. Researchers, however,\ncan utilize VisIT-Bench to its full potential as a dynamic benchmark by submitting their\nmodel predictions for evaluation. We will assess their models using the undisclosed test\nset, ensuring the ongoing evolution of the benchmark. Moreover, we are open to releasing\nthe test data upon receiving reasonable and justified requests, particularly when additional\nanalysis is necessary, provided that requesters agree to our non-contamination policy which\nprohibits the use of this data for training commercial chatbots. This approach strikes a\nbalance between the need for robust model evaluation and the mitigation of potential data\ncontamination.\nA          Dataset Analysis\n      103\n    Count (log-scale)\n      102\n      101\n                           carrot          cat                                                   baseball bat               donut              sports ball                                bowl\n                       refrigerator    toilet                                    boattraffic light                                                 potted plant            wine glass                        car chair\n                                                          scissors                           truck                      tv              umbrella           backpack               clock           book\n                                                                                                         apple  suitcase        orange                 dining tablevase\n                baseball glove                    motorcycle              bicycle                                                   cell phone                         remote                 tie         bottle\n                                                              giraffe knife   horse                          kite                                              bird                   cake\n                               keyboard               surfboard                                                                                                               handbag\n                   train           couch       stop sign          laptop                 spoon       cow            bench                   dog                                                       cup            person\n                                                                                                            Objects\nFigure 10: Count of the various COCO objects present in the VisIT-Bench images on a\nlog-scale. The object detection was performed by Yolov5-Large [40]. We observe that the\nobject \u2018person\u2019 occurs most of the time in the dataset images i.e., 875 times.\n                                                                                                       21", "md": "## Appendix\n\n### License and Intended Use\n\nThe VisIT-Bench dataset, along with its various contributions such as instructions, reference outputs, and model ranking annotations, is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). This license applies to all the images we have directly contributed, each of which carries a public license specification in the \u201cpublic images metadata\u201d field within the dataset sheets. However, the dataset also incorporates images sourced from pre-existing collections. For these images, the original licensing terms are respected and remain applicable.\n\nVisIT-Bench\u2019s primary purpose is to function as a dynamic benchmark that continuously evolves and evaluates instruction-following vision-language models. In the current landscape, commercial chatbots are often trained on non-disclosed and non-public datasets, which raises concerns about potential data contamination and inadvertent training on our evaluation data [8]. This risk is further highlighted by recent studies [82, 83]. To mitigate such concerns, we have chosen to withhold the complete VisIT-Bench test set from public disclosure, while still making the images and instructions available for direct download. Researchers, however, can utilize VisIT-Bench to its full potential as a dynamic benchmark by submitting their model predictions for evaluation. We will assess their models using the undisclosed test set, ensuring the ongoing evolution of the benchmark. Moreover, we are open to releasing the test data upon receiving reasonable and justified requests, particularly when additional analysis is necessary, provided that requesters agree to our non-contamination policy which prohibits the use of this data for training commercial chatbots. This approach strikes a balance between the need for robust model evaluation and the mitigation of potential data contamination.\n\n### Dataset Analysis\n\n| |Count (log-scale)|\n|---|---|\n|carrot|103|\n|cat|102|\n|baseball bat|101|\n|donut| |\n|sports ball| |\n|bowl| |\n|...| |\n\nFigure 10: Count of the various COCO objects present in the VisIT-Bench images on a log-scale. The object detection was performed by Yolov5-Large [40]. We observe that the object \u2018person\u2019 occurs most of the time in the dataset images i.e., 875 times.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Appendix", "md": "## Appendix"}, {"type": "heading", "lvl": 3, "value": "License and Intended Use", "md": "### License and Intended Use"}, {"type": "text", "value": "The VisIT-Bench dataset, along with its various contributions such as instructions, reference outputs, and model ranking annotations, is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). This license applies to all the images we have directly contributed, each of which carries a public license specification in the \u201cpublic images metadata\u201d field within the dataset sheets. However, the dataset also incorporates images sourced from pre-existing collections. For these images, the original licensing terms are respected and remain applicable.\n\nVisIT-Bench\u2019s primary purpose is to function as a dynamic benchmark that continuously evolves and evaluates instruction-following vision-language models. In the current landscape, commercial chatbots are often trained on non-disclosed and non-public datasets, which raises concerns about potential data contamination and inadvertent training on our evaluation data [8]. This risk is further highlighted by recent studies [82, 83]. To mitigate such concerns, we have chosen to withhold the complete VisIT-Bench test set from public disclosure, while still making the images and instructions available for direct download. Researchers, however, can utilize VisIT-Bench to its full potential as a dynamic benchmark by submitting their model predictions for evaluation. We will assess their models using the undisclosed test set, ensuring the ongoing evolution of the benchmark. Moreover, we are open to releasing the test data upon receiving reasonable and justified requests, particularly when additional analysis is necessary, provided that requesters agree to our non-contamination policy which prohibits the use of this data for training commercial chatbots. This approach strikes a balance between the need for robust model evaluation and the mitigation of potential data contamination.", "md": "The VisIT-Bench dataset, along with its various contributions such as instructions, reference outputs, and model ranking annotations, is licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0). This license applies to all the images we have directly contributed, each of which carries a public license specification in the \u201cpublic images metadata\u201d field within the dataset sheets. However, the dataset also incorporates images sourced from pre-existing collections. For these images, the original licensing terms are respected and remain applicable.\n\nVisIT-Bench\u2019s primary purpose is to function as a dynamic benchmark that continuously evolves and evaluates instruction-following vision-language models. In the current landscape, commercial chatbots are often trained on non-disclosed and non-public datasets, which raises concerns about potential data contamination and inadvertent training on our evaluation data [8]. This risk is further highlighted by recent studies [82, 83]. To mitigate such concerns, we have chosen to withhold the complete VisIT-Bench test set from public disclosure, while still making the images and instructions available for direct download. Researchers, however, can utilize VisIT-Bench to its full potential as a dynamic benchmark by submitting their model predictions for evaluation. We will assess their models using the undisclosed test set, ensuring the ongoing evolution of the benchmark. Moreover, we are open to releasing the test data upon receiving reasonable and justified requests, particularly when additional analysis is necessary, provided that requesters agree to our non-contamination policy which prohibits the use of this data for training commercial chatbots. This approach strikes a balance between the need for robust model evaluation and the mitigation of potential data contamination."}, {"type": "heading", "lvl": 3, "value": "Dataset Analysis", "md": "### Dataset Analysis"}, {"type": "table", "rows": [["", "Count (log-scale)"], ["carrot", "103"], ["cat", "102"], ["baseball bat", "101"], ["donut", ""], ["sports ball", ""], ["bowl", ""], ["...", ""]], "md": "| |Count (log-scale)|\n|---|---|\n|carrot|103|\n|cat|102|\n|baseball bat|101|\n|donut| |\n|sports ball| |\n|bowl| |\n|...| |", "isPerfectTable": true, "csv": "\"\",\"Count (log-scale)\"\n\"carrot\",\"103\"\n\"cat\",\"102\"\n\"baseball bat\",\"101\"\n\"donut\",\"\"\n\"sports ball\",\"\"\n\"bowl\",\"\"\n\"...\",\"\""}, {"type": "text", "value": "Figure 10: Count of the various COCO objects present in the VisIT-Bench images on a log-scale. The object detection was performed by Yolov5-Large [40]. We observe that the object \u2018person\u2019 occurs most of the time in the dataset images i.e., 875 times.", "md": "Figure 10: Count of the various COCO objects present in the VisIT-Bench images on a log-scale. The object detection was performed by Yolov5-Large [40]. We observe that the object \u2018person\u2019 occurs most of the time in the dataset images i.e., 875 times."}]}, {"page": 22, "text": "B    Interfaces for Collecting Human Annotations\nIn this section, we provide the templates we used to collect human annotations for the\ninstruction generation (Figure 11), the dense caption generation (Figure 12), the model\nverification (Figure 13 and Figure 14), and the model rating (Figure 15).\n                                                                       Expected Oulpul\n                       Instruction                                     Tuis Cish _peansVaalian Cur\n                                                                            Aroa pot ajd cninan0 saut 32\n                                                                               gnoz          Gale\n                            676 dish andny\"Make                        minutas Se7a \"IthcuckedtIov\n Find                            Recipe,           Viuaninstruction, Ihat       modellhuman\n        Imalol  Annotulc     Faod                                      eriual-only\n          skip Ihe\n Provde & * PUDLICALLY LICENSCD * image URL\n                   engin]\n       Figure 11: An interface that collects the image and corresponding instruction.\nFigure 12: An interface that collects the dense caption given an image-and-instruction pair.\n                             Cupiiu                                         63774couecik foloniujucunt\n                                                                                        ~mark Gm&\n                         GpT-4 Prcocons\nFigure 13: A sample of the model verification where the GPT4 follows the instruction\ncorrectly.\n                                               22", "md": "## Interfaces for Collecting Human Annotations\n\nIn this section, we provide the templates we used to collect human annotations for the instruction generation (Figure 11), the dense caption generation (Figure 12), the model verification (Figure 13 and Figure 14), and the model rating (Figure 15).\n\n### Instruction\n\nExpected Output:\n\n|Instruction|This Dish Means Validation Cur|\n|---|---|\n|676 dish andny \"Make Recipe, Viuaninstruction, Ihat modellhuman|Aroa pot ajd cninan0 saut 32 gnoz Gale|\n|Find Imalol Annotulc Faod|eriual-only|\n\n### Figure 11\n\nAn interface that collects the image and corresponding instruction.\n\n### Figure 12\n\nAn interface that collects the dense caption given an image-and-instruction pair.\n\n### Figure 13\n\nA sample of the model verification where the GPT4 follows the instruction correctly.", "images": [{"name": "page-22-0.jpg", "height": 220, "width": 396, "x": 108, "y": 144}, {"name": "page-22-1.jpg", "height": 99, "width": 396, "x": 108, "y": 406}, {"name": "page-22-2.jpg", "height": 85, "width": 396, "x": 108, "y": 548}], "items": [{"type": "heading", "lvl": 2, "value": "Interfaces for Collecting Human Annotations", "md": "## Interfaces for Collecting Human Annotations"}, {"type": "text", "value": "In this section, we provide the templates we used to collect human annotations for the instruction generation (Figure 11), the dense caption generation (Figure 12), the model verification (Figure 13 and Figure 14), and the model rating (Figure 15).", "md": "In this section, we provide the templates we used to collect human annotations for the instruction generation (Figure 11), the dense caption generation (Figure 12), the model verification (Figure 13 and Figure 14), and the model rating (Figure 15)."}, {"type": "heading", "lvl": 3, "value": "Instruction", "md": "### Instruction"}, {"type": "text", "value": "Expected Output:", "md": "Expected Output:"}, {"type": "table", "rows": [["Instruction", "This Dish Means Validation Cur"], ["676 dish andny \"Make Recipe, Viuaninstruction, Ihat modellhuman", "Aroa pot ajd cninan0 saut 32 gnoz Gale"], ["Find Imalol Annotulc Faod", "eriual-only"]], "md": "|Instruction|This Dish Means Validation Cur|\n|---|---|\n|676 dish andny \"Make Recipe, Viuaninstruction, Ihat modellhuman|Aroa pot ajd cninan0 saut 32 gnoz Gale|\n|Find Imalol Annotulc Faod|eriual-only|", "isPerfectTable": true, "csv": "\"Instruction\",\"This Dish Means Validation Cur\"\n\"676 dish andny \"\"Make Recipe, Viuaninstruction, Ihat modellhuman\",\"Aroa pot ajd cninan0 saut 32 gnoz Gale\"\n\"Find Imalol Annotulc Faod\",\"eriual-only\""}, {"type": "heading", "lvl": 3, "value": "Figure 11", "md": "### Figure 11"}, {"type": "text", "value": "An interface that collects the image and corresponding instruction.", "md": "An interface that collects the image and corresponding instruction."}, {"type": "heading", "lvl": 3, "value": "Figure 12", "md": "### Figure 12"}, {"type": "text", "value": "An interface that collects the dense caption given an image-and-instruction pair.", "md": "An interface that collects the dense caption given an image-and-instruction pair."}, {"type": "heading", "lvl": 3, "value": "Figure 13", "md": "### Figure 13"}, {"type": "text", "value": "A sample of the model verification where the GPT4 follows the instruction correctly.", "md": "A sample of the model verification where the GPT4 follows the instruction correctly."}]}, {"page": 23, "text": "      f ()              E\nFigure 14: A sample of the model verification where the GPT4 failed to follow the instruction\ndue to its incorrect reasoning.\n                                      Whchmodd                                 eoumi\n                                        ennbnletlnuton\n           Figure 15: An interface that collects the feedback of the model rating.\nC    Existing Datasets incorporated in VisIT-Bench\nIn Table 5, we listed the existing datasets that are incoprated in our VisIT-Bench. Among\nthese datasets, 15 contain a single image in each sample pair, and 10 require reasoning based\non multiple images.\n                                               23", "md": "$$f() E$$\n\nFigure 14: A sample of the model verification where the GPT4 failed to follow the instruction due to its incorrect reasoning.\n\n$$Whchmodd eoumi$$\n\nennbnletlnuton\n\nFigure 15: An interface that collects the feedback of the model rating.\n\n## Existing Datasets incorporated in VisIT-Bench\n\n|Dataset|Description|\n|---|---|\n|Dataset 1|Contains a single image in each sample pair|\n|Dataset 2|Requires reasoning based on multiple images|\n|...|...|", "images": [{"name": "page-23-1.jpg", "height": 100, "width": 396, "x": 108, "y": 183}, {"name": "page-23-0.jpg", "height": 69, "width": 396, "x": 108, "y": 71}], "items": [{"type": "text", "value": "$$f() E$$\n\nFigure 14: A sample of the model verification where the GPT4 failed to follow the instruction due to its incorrect reasoning.\n\n$$Whchmodd eoumi$$\n\nennbnletlnuton\n\nFigure 15: An interface that collects the feedback of the model rating.", "md": "$$f() E$$\n\nFigure 14: A sample of the model verification where the GPT4 failed to follow the instruction due to its incorrect reasoning.\n\n$$Whchmodd eoumi$$\n\nennbnletlnuton\n\nFigure 15: An interface that collects the feedback of the model rating."}, {"type": "heading", "lvl": 2, "value": "Existing Datasets incorporated in VisIT-Bench", "md": "## Existing Datasets incorporated in VisIT-Bench"}, {"type": "table", "rows": [["Dataset", "Description"], ["Dataset 1", "Contains a single image in each sample pair"], ["Dataset 2", "Requires reasoning based on multiple images"], ["...", "..."]], "md": "|Dataset|Description|\n|---|---|\n|Dataset 1|Contains a single image in each sample pair|\n|Dataset 2|Requires reasoning based on multiple images|\n|...|...|", "isPerfectTable": true, "csv": "\"Dataset\",\"Description\"\n\"Dataset 1\",\"Contains a single image in each sample pair\"\n\"Dataset 2\",\"Requires reasoning based on multiple images\"\n\"...\",\"...\""}]}, {"page": 24, "text": "Table 5: List of existing datasets in VisIT-Bench, categorized as single and multiple image\ndatasets.\n                         Dataset                                    Topic\n                        VQA [21]                         Visual Question Answering\n                        VCR [84]                    Cognition-level Visual Understanding\n                      GD-VCR [85]                   Geo-Diverse Commonsense Reasoning\n                     WHOOPS [41]                       What Makes this Image Strange\n                  Newyork Caption [86]                      Humor Understanding\n                      CLEVR [87]                         Visual Question Answering\n   Single             Kilogram [88]                        Tangrams Identification\n                  Harmful Memes [89]                        Memes Understanding\n                     ScienceQA [90]                      Science Question Answering\n                     OK-VQA [91]               Outside Knowledge Visual Question Answering\n                     AOK-VQA [92]                    Outside Knowledge Visual Question\n                     AOK-VQA [92]                            Question Generation\n                       VizWiz [93]                       Visual Question Answering\n                        GQA [67]                 Visual Question Answering on Scene Graphs\n                      TextCaps [94]                  Visual Question Answering on Texts\n             Robust Change Captioning [22]         Describing What has Change in a Scene\n                      NLVR2 [36]                        Testing Visual Language Bias\n                    ImageCoDE [95]                             Image Retrieval\n                    Spot-the-Diff [96]                     Identifying Differences\n  Multiple             VASR [97]                              Visual Analogies\n                     WinoGavil [98]                          Visual Associations\n                  IRFL (Metaphor) [99]                Figurative Speech Understanding\n                   IRFL (Idioms) [99]                 Figurative Speech Understanding\n                      IconQA [100]                     Abstract Diagram Understanding\n                     Pick-a-Pic[101]                   Text-to-Image User Preferences\n                 Table 6: List of skills and existing datasets in VisIT-Bench\n  \u2018scienceqa\u2019, \u2018ocr math\u2019, \u2018recognition\u2019, \u2018okvqa\u2019, \u2018house plan understand-\n  ing\u2019, \u2018nlvr2\u2019, \u2018gardening tips\u2019, \u2018textcaps\u2019, \u2018architectural styles\u2019, \u2018dressing\n  sense\u2019, \u2018winoground\u2019, \u2018food recipe\u2019, \u2018paper folding\u2019, \u2018whoops\u2019, \u2018spot\n  the diff\u2019, \u2018winogavil\u2019, \u2018imagecode\u2019, \u2018exercise\u2019, \u2018art knowledge\u2019, \u2018gqa\u2019,\n \u2018physical knowledge\u2019, \u2018contextual knowledge of events\u2019, \u2018home renova-\n  tion\u2019, \u2018aokvqa\u2019, \u2018animals\u2019, \u2018vasr\u2019, \u2018counting\u2019, \u2018board games\u2019, \u2018solving\n  geometry problems\u2019, \u2018who to call?\u2019, \u2018clevr\u2019, \u2018building materials\u2019, \u2018hazard\n  identification\u2019, \u2018pickapick\u2019, \u2018astronomy\u2019, \u2018figurative speech explanation\u2019,\n \u2018write a story\u2019, \u2018gestures understanding\u2019, \u2018newyork\u2019, \u2018cultural knowledge\u2019,\n \u2018aokvqg\u2019, \u2018traffi    c sign identification\u2019, \u2018pop culture\u2019, \u2018fashion products\u2019,\n \u2018harmful memes\u2019, \u2018write a poem\u2019, \u2018vizwiz\u2019, \u2018guesstimate of capacity\u2019,\n \u2018location understanding\u2019, \u2018graph reasoning\u2019, \u2018vqa\u2019, \u2018game playing\u2019, \u2018dif-\n  ferently abled\u2019, \u2018chemical identification\u2019, \u2018history knowledge\u2019, \u2018climate\n  and weather understanding\u2019, \u2018irfl metaphor\u2019, \u2018human emotion recog-\n  nition\u2019, \u2018medical\u2019, \u2018gd vcr\u2019, \u2018vcr\u2019, \u2018technical support\u2019, \u2018catchy titles\u2019,\n \u2018kilogram\u2019, \u2018anagrams\u2019, \u2018color\u2019, \u2018tour guide\u2019, \u2018directions\u2019, \u2018irfl idiom\u2019,\n \u2018rcc\u2019\n                                               24", "md": "|Dataset|Topic|\n|---|---|\n|$VQA [21]$|Visual Question Answering|\n|$VCR [84]$|Cognition-level Visual Understanding|\n|$GD-VCR [85]$|Geo-Diverse Commonsense Reasoning|\n|$WHOOPS [41]$|What Makes this Image Strange|\n|$Newyork Caption [86]$|Humor Understanding|\n|$CLEVR [87]$|Visual Question Answering|\n|$Kilogram [88]$|Tangrams Identification|\n|$Harmful Memes [89]$|Memes Understanding|\n|$ScienceQA [90]$|Science Question Answering|\n|$OK-VQA [91]$|Outside Knowledge Visual Question Answering|\n|$AOK-VQA [92]$|Outside Knowledge Visual Answer Generation|\n|$VizWiz [93]$|Visual Question Answering|\n|$GQA [67]$|Visual Question Answering on Scene Graphs|\n|$TextCaps [94]$|Visual Question Answering on Texts|\n|$Robust Change Captioning [22]$|Describing What has Changed in a Scene|\n|$NLVR2 [36]$|Testing Visual Language Bias|\n|$ImageCoDE [95]$|Image Retrieval|\n|$Spot-the-Diff [96]$|Identifying Differences|\n\nSkills and Datasets\nscienceqa, ocr map, recognition, okvqa, house plan understanding, nlvr2, gardening tips, textcaps, architectural styles, dressing sense, winoground, food recipe, paper folding, whoops, spot pe diff, winogavil, imagecode, exercise, art knowledge, gqa, physical knowledge, contextual knowledge of events, home renovation, aokvqa, animals, vasr, counting, board games, solving geometry problems, who to call?, clevr, building materials, hazard identification, pickapick, astronomy, figurative speech explanation, write a story, gestures understanding, newyork, cultural knowledge, aokvqg, traffic sign identification, pop culture, fashion products, harmful memes, write a poem, vizwiz, guesstimate of capacity, location understanding, graph reasoning, vqa, game playing, differently abled, chemical identification, history knowledge, climate and weaper understanding, irfl metaphor, human emotion recognition, medical, gd vcr, vcr, technical support, catchy titles, kilogram, anagrams, color, tour guide, directions, irfl idiom, rcc", "images": [], "items": [{"type": "table", "rows": [["Dataset", "Topic"], ["$VQA [21]$", "Visual Question Answering"], ["$VCR [84]$", "Cognition-level Visual Understanding"], ["$GD-VCR [85]$", "Geo-Diverse Commonsense Reasoning"], ["$WHOOPS [41]$", "What Makes this Image Strange"], ["$Newyork Caption [86]$", "Humor Understanding"], ["$CLEVR [87]$", "Visual Question Answering"], ["$Kilogram [88]$", "Tangrams Identification"], ["$Harmful Memes [89]$", "Memes Understanding"], ["$ScienceQA [90]$", "Science Question Answering"], ["$OK-VQA [91]$", "Outside Knowledge Visual Question Answering"], ["$AOK-VQA [92]$", "Outside Knowledge Visual Answer Generation"], ["$VizWiz [93]$", "Visual Question Answering"], ["$GQA [67]$", "Visual Question Answering on Scene Graphs"], ["$TextCaps [94]$", "Visual Question Answering on Texts"], ["$Robust Change Captioning [22]$", "Describing What has Changed in a Scene"], ["$NLVR2 [36]$", "Testing Visual Language Bias"], ["$ImageCoDE [95]$", "Image Retrieval"], ["$Spot-the-Diff [96]$", "Identifying Differences"]], "md": "|Dataset|Topic|\n|---|---|\n|$VQA [21]$|Visual Question Answering|\n|$VCR [84]$|Cognition-level Visual Understanding|\n|$GD-VCR [85]$|Geo-Diverse Commonsense Reasoning|\n|$WHOOPS [41]$|What Makes this Image Strange|\n|$Newyork Caption [86]$|Humor Understanding|\n|$CLEVR [87]$|Visual Question Answering|\n|$Kilogram [88]$|Tangrams Identification|\n|$Harmful Memes [89]$|Memes Understanding|\n|$ScienceQA [90]$|Science Question Answering|\n|$OK-VQA [91]$|Outside Knowledge Visual Question Answering|\n|$AOK-VQA [92]$|Outside Knowledge Visual Answer Generation|\n|$VizWiz [93]$|Visual Question Answering|\n|$GQA [67]$|Visual Question Answering on Scene Graphs|\n|$TextCaps [94]$|Visual Question Answering on Texts|\n|$Robust Change Captioning [22]$|Describing What has Changed in a Scene|\n|$NLVR2 [36]$|Testing Visual Language Bias|\n|$ImageCoDE [95]$|Image Retrieval|\n|$Spot-the-Diff [96]$|Identifying Differences|", "isPerfectTable": true, "csv": "\"Dataset\",\"Topic\"\n\"$VQA [21]$\",\"Visual Question Answering\"\n\"$VCR [84]$\",\"Cognition-level Visual Understanding\"\n\"$GD-VCR [85]$\",\"Geo-Diverse Commonsense Reasoning\"\n\"$WHOOPS [41]$\",\"What Makes this Image Strange\"\n\"$Newyork Caption [86]$\",\"Humor Understanding\"\n\"$CLEVR [87]$\",\"Visual Question Answering\"\n\"$Kilogram [88]$\",\"Tangrams Identification\"\n\"$Harmful Memes [89]$\",\"Memes Understanding\"\n\"$ScienceQA [90]$\",\"Science Question Answering\"\n\"$OK-VQA [91]$\",\"Outside Knowledge Visual Question Answering\"\n\"$AOK-VQA [92]$\",\"Outside Knowledge Visual Answer Generation\"\n\"$VizWiz [93]$\",\"Visual Question Answering\"\n\"$GQA [67]$\",\"Visual Question Answering on Scene Graphs\"\n\"$TextCaps [94]$\",\"Visual Question Answering on Texts\"\n\"$Robust Change Captioning [22]$\",\"Describing What has Changed in a Scene\"\n\"$NLVR2 [36]$\",\"Testing Visual Language Bias\"\n\"$ImageCoDE [95]$\",\"Image Retrieval\"\n\"$Spot-the-Diff [96]$\",\"Identifying Differences\""}, {"type": "text", "value": "Skills and Datasets\nscienceqa, ocr map, recognition, okvqa, house plan understanding, nlvr2, gardening tips, textcaps, architectural styles, dressing sense, winoground, food recipe, paper folding, whoops, spot pe diff, winogavil, imagecode, exercise, art knowledge, gqa, physical knowledge, contextual knowledge of events, home renovation, aokvqa, animals, vasr, counting, board games, solving geometry problems, who to call?, clevr, building materials, hazard identification, pickapick, astronomy, figurative speech explanation, write a story, gestures understanding, newyork, cultural knowledge, aokvqg, traffic sign identification, pop culture, fashion products, harmful memes, write a poem, vizwiz, guesstimate of capacity, location understanding, graph reasoning, vqa, game playing, differently abled, chemical identification, history knowledge, climate and weaper understanding, irfl metaphor, human emotion recognition, medical, gd vcr, vcr, technical support, catchy titles, kilogram, anagrams, color, tour guide, directions, irfl idiom, rcc", "md": "Skills and Datasets\nscienceqa, ocr map, recognition, okvqa, house plan understanding, nlvr2, gardening tips, textcaps, architectural styles, dressing sense, winoground, food recipe, paper folding, whoops, spot pe diff, winogavil, imagecode, exercise, art knowledge, gqa, physical knowledge, contextual knowledge of events, home renovation, aokvqa, animals, vasr, counting, board games, solving geometry problems, who to call?, clevr, building materials, hazard identification, pickapick, astronomy, figurative speech explanation, write a story, gestures understanding, newyork, cultural knowledge, aokvqg, traffic sign identification, pop culture, fashion products, harmful memes, write a poem, vizwiz, guesstimate of capacity, location understanding, graph reasoning, vqa, game playing, differently abled, chemical identification, history knowledge, climate and weaper understanding, irfl metaphor, human emotion recognition, medical, gd vcr, vcr, technical support, catchy titles, kilogram, anagrams, color, tour guide, directions, irfl idiom, rcc"}]}, {"page": 25, "text": "D     Elo Rating\nFor many years, the Elo rating has been popular in ranking players in zero-sum games such\nas chess [25]. Recently, it has been adopted to rate large language models (LLMs) against\neach other on the user instructions. In this work, we adopt the same strategy to rank a set\nof instruction-following vision-language models, that can grow dynamically with further\nadvances in the field.\nGiven two multimodal chatbots Ca and Cb with their absolute Elo rating Ra and Rb, respec-\ntively. Simply put, the probability of Ca winning over Cb in a head-to-head battle is given\nby:\n                                                                 1\n                            P(Ca wins over Cb) =       1 + 10(Ra\u2212Rb)/400                          (1)\nIn practice, calculating the Elo rating requires us to set hyperparameters to decide the\nweightage for each win and loss in a head-to-head battle between two models. In our work,\nwe use the open implementation of Elo for LLMs by FastChat at https://github.com/\nlm-sys/FastChat/blob/main/fastchat/serve/monitor/elo_analysis.py.\nE     GPT-4 Pairwise Evaluation Prompts\nThe specific prompts we use to extract pairwise judgements from our language model are\nprovided in Table 16 (reference-free version) and Table 17 (reference-backed version). When\napplied to GPT-4 [7], these prompts usually solicit a definitive pairwise response by the\nmodel. But, in some cases, the model either produces a pairwise judgement in an unexpected\nformat, or, refuses to issue a judgement at all. For cases like these, we issue an additional\nquery to ChatGPT to extract an answer (or decide there is no answer) using an additional\nprompt, given in Table 18. If after this step there is still no definitive pairwise judgment, we\ncall the result a tie.\n                                                   25", "md": "# Elo Rating and GPT-4 Pairwise Evaluation\n\n## Elo Rating\n\nFor many years, the Elo rating has been popular in ranking players in zero-sum games such as chess [25]. Recently, it has been adopted to rate large language models (LLMs) against each other on the user instructions. In this work, we adopt the same strategy to rank a set of instruction-following vision-language models, that can grow dynamically with further advances in the field.\n\nGiven two multimodal chatbots Ca and Cb with their absolute Elo rating Ra and Rb, respectively. Simply put, the probability of Ca winning over Cb in a head-to-head battle is given by:\n\n$$P(Ca \\text{ wins over } Cb) = \\frac{1}{1 + 10^{(Ra-Rb)/400}} \\quad \\text{(1)}$$\n\nIn practice, calculating the Elo rating requires us to set hyperparameters to decide the weightage for each win and loss in a head-to-head battle between two models. In our work, we use the open implementation of Elo for LLMs by FastChat at FastChat Elo Analysis.\n\n## GPT-4 Pairwise Evaluation Prompts\n\nThe specific prompts we use to extract pairwise judgments from our language model are provided in Table 16 (reference-free version) and Table 17 (reference-backed version). When applied to GPT-4 [7], these prompts usually solicit a definitive pairwise response by the model. But, in some cases, the model either produces a pairwise judgment in an unexpected format, or, refuses to issue a judgment at all. For cases like these, we issue an additional query to ChatGPT to extract an answer (or decide there is no answer) using an additional prompt, given in Table 18. If after this step there is still no definitive pairwise judgment, we call the result a tie.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Elo Rating and GPT-4 Pairwise Evaluation", "md": "# Elo Rating and GPT-4 Pairwise Evaluation"}, {"type": "heading", "lvl": 2, "value": "Elo Rating", "md": "## Elo Rating"}, {"type": "text", "value": "For many years, the Elo rating has been popular in ranking players in zero-sum games such as chess [25]. Recently, it has been adopted to rate large language models (LLMs) against each other on the user instructions. In this work, we adopt the same strategy to rank a set of instruction-following vision-language models, that can grow dynamically with further advances in the field.\n\nGiven two multimodal chatbots Ca and Cb with their absolute Elo rating Ra and Rb, respectively. Simply put, the probability of Ca winning over Cb in a head-to-head battle is given by:\n\n$$P(Ca \\text{ wins over } Cb) = \\frac{1}{1 + 10^{(Ra-Rb)/400}} \\quad \\text{(1)}$$\n\nIn practice, calculating the Elo rating requires us to set hyperparameters to decide the weightage for each win and loss in a head-to-head battle between two models. In our work, we use the open implementation of Elo for LLMs by FastChat at FastChat Elo Analysis.", "md": "For many years, the Elo rating has been popular in ranking players in zero-sum games such as chess [25]. Recently, it has been adopted to rate large language models (LLMs) against each other on the user instructions. In this work, we adopt the same strategy to rank a set of instruction-following vision-language models, that can grow dynamically with further advances in the field.\n\nGiven two multimodal chatbots Ca and Cb with their absolute Elo rating Ra and Rb, respectively. Simply put, the probability of Ca winning over Cb in a head-to-head battle is given by:\n\n$$P(Ca \\text{ wins over } Cb) = \\frac{1}{1 + 10^{(Ra-Rb)/400}} \\quad \\text{(1)}$$\n\nIn practice, calculating the Elo rating requires us to set hyperparameters to decide the weightage for each win and loss in a head-to-head battle between two models. In our work, we use the open implementation of Elo for LLMs by FastChat at FastChat Elo Analysis."}, {"type": "heading", "lvl": 2, "value": "GPT-4 Pairwise Evaluation Prompts", "md": "## GPT-4 Pairwise Evaluation Prompts"}, {"type": "text", "value": "The specific prompts we use to extract pairwise judgments from our language model are provided in Table 16 (reference-free version) and Table 17 (reference-backed version). When applied to GPT-4 [7], these prompts usually solicit a definitive pairwise response by the model. But, in some cases, the model either produces a pairwise judgment in an unexpected format, or, refuses to issue a judgment at all. For cases like these, we issue an additional query to ChatGPT to extract an answer (or decide there is no answer) using an additional prompt, given in Table 18. If after this step there is still no definitive pairwise judgment, we call the result a tie.", "md": "The specific prompts we use to extract pairwise judgments from our language model are provided in Table 16 (reference-free version) and Table 17 (reference-backed version). When applied to GPT-4 [7], these prompts usually solicit a definitive pairwise response by the model. But, in some cases, the model either produces a pairwise judgment in an unexpected format, or, refuses to issue a judgment at all. For cases like these, we issue an additional query to ChatGPT to extract an answer (or decide there is no answer) using an additional prompt, given in Table 18. If after this step there is still no definitive pairwise judgment, we call the result a tie."}]}, {"page": 26, "text": "      system prompt (human authored)\n      You are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses\n      an instruction in the context of an image. More specifically, you will be given the following:\n      1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n      2. An instruction: This is a question, an imperative request, or something similar about the image which requires a\n      response.\n      3. Two responses, response A and response B: These two responses attempt to address the instruction in the context of\n      the image.\n      Your job is to judge whether response A or response B better. A and B are randomly ordered.\n      Some things to remember:\n      - Even though you are just a language model, the image description will be sufficiently detailed so that your judgements\n      can be accurate.\n      - You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency,\n      specificity, etc.\n      - You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\"\n      user (human authored)\n      I will describe an image to you, and provide an instruction. Then, I will give you two candidate responses that address\n      the instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job is to\n      first reason step-by-step about which response is best in terms of accuracy, specificity, fluency, etc. After reasoning\n      step-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\"\n      where X is either A or B.\n      assistant (human authored)\n      Sure, please provide the image context, the instruction, and the two candidate responses, Response A and Response B.\n      Then, I will think step-by-step and provide my ultimate judgement as to which response is better.\n      user (from human-authored template)\n      OK. Here is the image description, the instruction, and the two response options, Response A and Response B. Image\n      context: A skier is wearing a green jacket, burgundy pants, white shoes, white gloves, and a gray beanie. There are\n      people sitting on the background with their snowboards.\n      Instruction: Answer the following question based on the provided image: Are the trousers the same color as the shoes?\n      Response A: no\n      Response B: No, the trousers are not the same color as the shoes. The trousers are burgundy, while the shoes are white.\n      Think step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B.\n      assistant (machine generated; used to derive the prediction)\n      Step 1: Identify the key information from the image description.\n      The skier is wearing:\n      - Burgundy pants\n      - White shoes\n      Step 2: Assess the accuracy of each response in addressing the instruction.\n      Both Response A and Response B convey that the trousers and shoes are not of the same color.\n      Step 3: Compare specificity and detail of each response.\n      Response A is concise and provides a straightforward answer, but lacks detail.\n      Response B provides a clear, comprehensive answer that includes the color of both the trousers and shoes.\n      Step 4: Consider the fluency and correctness.\n      Both responses are fluent and grammatically correct.\n      Overall, Response B is better.\nFigure 16: The prompt we use for reference-free evaluation, along with a sample in-\nstance/completion from GPT-4 [7].\n                                                                        26", "md": "# Image Task Evaluation\n\n## System Prompt (Human Authored)\n\nYou are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses\nan instruction in the context of an image. More specifically, you will be given the following:\n\n1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n2. An instruction: This is a question, an imperative request, or something similar about the image which requires a response.\n3. Two responses, response A and response B: These two responses attempt to address the instruction in the context of the image.\n\nYour job is to judge whether response A or response B is better. A and B are randomly ordered.\n\nSome things to remember:\n\n- Even though you are just a language model, the image description will be sufficiently detailed so that your judgements can be accurate.\n- You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency, specificity, etc.\n- You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\".\n\n## User (Human Authored)\n\nI will describe an image to you, and provide an instruction. Then, I will give you two candidate responses that address the instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job is to first reason step-by-step about which response is best in terms of accuracy, specificity, fluency, etc. After reasoning step-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\" where X is either A or B.\n\n## Assistant (Human Authored)\n\nSure, please provide the image context, the instruction, and the two candidate responses, Response A and Response B. Then, I will think step-by-step and provide my ultimate judgement as to which response is better.\n\n## User (From Human-Authored Template)\n\nOK. Here is the image description, the instruction, and the two response options, Response A and Response B.\n\nImage Context: A skier is wearing a green jacket, burgundy pants, white shoes, white gloves, and a gray beanie. There are people sitting on the background with their snowboards.\n\nInstruction: Answer the following question based on the provided image: Are the trousers the same color as the shoes?\n\nResponse A: no\n\nResponse B: No, the trousers are not the same color as the shoes. The trousers are burgundy, while the shoes are white. Think step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B.\n\n## Assistant (Machine Generated; Used to Derive the Prediction)\n\n1. Step 1: Identify the key information from the image description.\n- The skier is wearing:\n- Burgundy pants\n- White shoes\n2. Step 2: Assess the accuracy of each response in addressing the instruction.\n- Both Response A and Response B convey that the trousers and shoes are not of the same color.\n3. Step 3: Compare specificity and detail of each response.\n- Response A is concise and provides a straightforward answer, but lacks detail.\n- Response B provides a clear, comprehensive answer that includes the color of both the trousers and shoes.\n4. Step 4: Consider the fluency and correctness.\n- Both responses are fluent and grammatically correct.\n\nOverall, Response B is better.\n\nFigure 16: The prompt we use for reference-free evaluation, along with a sample instance/completion from GPT-4 [7].\n\n26", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Image Task Evaluation", "md": "# Image Task Evaluation"}, {"type": "heading", "lvl": 2, "value": "System Prompt (Human Authored)", "md": "## System Prompt (Human Authored)"}, {"type": "text", "value": "You are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses\nan instruction in the context of an image. More specifically, you will be given the following:\n\n1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n2. An instruction: This is a question, an imperative request, or something similar about the image which requires a response.\n3. Two responses, response A and response B: These two responses attempt to address the instruction in the context of the image.\n\nYour job is to judge whether response A or response B is better. A and B are randomly ordered.\n\nSome things to remember:\n\n- Even though you are just a language model, the image description will be sufficiently detailed so that your judgements can be accurate.\n- You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency, specificity, etc.\n- You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\".", "md": "You are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses\nan instruction in the context of an image. More specifically, you will be given the following:\n\n1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n2. An instruction: This is a question, an imperative request, or something similar about the image which requires a response.\n3. Two responses, response A and response B: These two responses attempt to address the instruction in the context of the image.\n\nYour job is to judge whether response A or response B is better. A and B are randomly ordered.\n\nSome things to remember:\n\n- Even though you are just a language model, the image description will be sufficiently detailed so that your judgements can be accurate.\n- You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency, specificity, etc.\n- You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\"."}, {"type": "heading", "lvl": 2, "value": "User (Human Authored)", "md": "## User (Human Authored)"}, {"type": "text", "value": "I will describe an image to you, and provide an instruction. Then, I will give you two candidate responses that address the instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job is to first reason step-by-step about which response is best in terms of accuracy, specificity, fluency, etc. After reasoning step-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\" where X is either A or B.", "md": "I will describe an image to you, and provide an instruction. Then, I will give you two candidate responses that address the instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job is to first reason step-by-step about which response is best in terms of accuracy, specificity, fluency, etc. After reasoning step-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\" where X is either A or B."}, {"type": "heading", "lvl": 2, "value": "Assistant (Human Authored)", "md": "## Assistant (Human Authored)"}, {"type": "text", "value": "Sure, please provide the image context, the instruction, and the two candidate responses, Response A and Response B. Then, I will think step-by-step and provide my ultimate judgement as to which response is better.", "md": "Sure, please provide the image context, the instruction, and the two candidate responses, Response A and Response B. Then, I will think step-by-step and provide my ultimate judgement as to which response is better."}, {"type": "heading", "lvl": 2, "value": "User (From Human-Authored Template)", "md": "## User (From Human-Authored Template)"}, {"type": "text", "value": "OK. Here is the image description, the instruction, and the two response options, Response A and Response B.\n\nImage Context: A skier is wearing a green jacket, burgundy pants, white shoes, white gloves, and a gray beanie. There are people sitting on the background with their snowboards.\n\nInstruction: Answer the following question based on the provided image: Are the trousers the same color as the shoes?\n\nResponse A: no\n\nResponse B: No, the trousers are not the same color as the shoes. The trousers are burgundy, while the shoes are white. Think step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B.", "md": "OK. Here is the image description, the instruction, and the two response options, Response A and Response B.\n\nImage Context: A skier is wearing a green jacket, burgundy pants, white shoes, white gloves, and a gray beanie. There are people sitting on the background with their snowboards.\n\nInstruction: Answer the following question based on the provided image: Are the trousers the same color as the shoes?\n\nResponse A: no\n\nResponse B: No, the trousers are not the same color as the shoes. The trousers are burgundy, while the shoes are white. Think step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B."}, {"type": "heading", "lvl": 2, "value": "Assistant (Machine Generated; Used to Derive the Prediction)", "md": "## Assistant (Machine Generated; Used to Derive the Prediction)"}, {"type": "text", "value": "1. Step 1: Identify the key information from the image description.\n- The skier is wearing:\n- Burgundy pants\n- White shoes\n2. Step 2: Assess the accuracy of each response in addressing the instruction.\n- Both Response A and Response B convey that the trousers and shoes are not of the same color.\n3. Step 3: Compare specificity and detail of each response.\n- Response A is concise and provides a straightforward answer, but lacks detail.\n- Response B provides a clear, comprehensive answer that includes the color of both the trousers and shoes.\n4. Step 4: Consider the fluency and correctness.\n- Both responses are fluent and grammatically correct.\n\nOverall, Response B is better.\n\nFigure 16: The prompt we use for reference-free evaluation, along with a sample instance/completion from GPT-4 [7].\n\n26", "md": "1. Step 1: Identify the key information from the image description.\n- The skier is wearing:\n- Burgundy pants\n- White shoes\n2. Step 2: Assess the accuracy of each response in addressing the instruction.\n- Both Response A and Response B convey that the trousers and shoes are not of the same color.\n3. Step 3: Compare specificity and detail of each response.\n- Response A is concise and provides a straightforward answer, but lacks detail.\n- Response B provides a clear, comprehensive answer that includes the color of both the trousers and shoes.\n4. Step 4: Consider the fluency and correctness.\n- Both responses are fluent and grammatically correct.\n\nOverall, Response B is better.\n\nFigure 16: The prompt we use for reference-free evaluation, along with a sample instance/completion from GPT-4 [7].\n\n26"}]}, {"page": 27, "text": "      system prompt (human authored)\n      You are ImageTaskEvaluationGPT, an expert language model at judging whether or not a response adequately addresses\n      an instruction in the context of an image. More specifically, you will be given the following:\n      1. An image context: This will describe the contents of an image with sufficient detail to address the instruction.\n      2. An instruction: This is a question, an imparative request, or something similar about the image which requires a\n      response.\n      3. A reference output: This is a high-quality example output that humans have judged to be an accurate response for the\n      input instruction.\n      4. Two responses, response A and response B: These two responses attempt to address the instruction in the context of\n      the image.\n      Your job is to judge whether response A or response B better. A and B are randomly ordered. Some things to remember:\n      - Even though you are just a language model, the image description will be sufficiently detailed so that your judgements\n      can be accurate.\n      - Take the high-quality reference into account when making your judgements, but remember: some instructions are\n      more open-ended than others, so for those cases, a high quality response can differ from the reference.\n      - You are capable of judging response quality, accounting for important factors like correctness, relevance, fluency,\n      specificity, etc.\n      - You think step-by-step, but ultimately respond with \"Response A\" or \"Response B\"\n      user (human authored)\n      I will describe an image to you, and provide an instruction. Then, I will provide a reference output which is an example\n      of a high quality output for that instruction in the context of the image. Then, I will give you two candidate responses\n      that address the instruction in the context of the image: these will be labelled \"Response A\" and \"Response B\". Your job\n      is to first reason step-by-step about which response is best in terms of accuracy, specifi        city, fl\n                                                                                                               uency, etc. After reasoning\n      step-by-step and comparing the pros/cons of each response, in the end, respond with \"Overall, Response X is better.\"\n      where X is either A or B.\n      assistant (human authored)\n      Sure, please provide the image context, the instruction, the reference, and the two candidate responses, Response A and\n      Response B. Then, I will think step-by-step and provide my ultimate judgement as to which response is better.\n      user (from human-authored template)\n      OK. Here is the image description, the instruction, the high-quality reference, and the two response options, Response A\n      and Response B.\n      Image context: There is a white plate that has on it a small pizza with many topics, mostly unrecognizable, due to the\n      pie being very well done, charred to the point of almost being burnt, and the crust blackened in many places.\n      Instruction: Someone told the creator of this item that they are very talented. What is the implied meaning?\n      High quality reference: The implied meaning is that the person complimenting the creator is being sarcastic, as the pizza\n      is overcooked and almost burnt, which is not a sign of talent.\n      Response A: When someone says that they are \"very talented\" in the context of the image, it implies that the person\n      responsible for creating the pizza is skilled in the craft and able to create an appealing and tasty dish. The person\n      praising the pizza\u2019s creator recognizes their culinary skills, which can be seen through the combination of ingredients\n      and the overall look of the finished dish. The praise is a form of appreciation for their efforts and the enjoyable result\n      they achieved.\n      Response B: It is difficult to determine the implied meaning without further context or information about the image. It\n      appears to be a pizza that has been burnt or charred on the surface and has various toppings, including pepperoni, onion,\n      and mushrooms. The image is shown on a white plate or surface. The implied meaning of this image could be that it is a\n      dish that has been cooked badly or that it is a dish with a unique or unusual combination of ingredients.\n      Think step-by-step and finish your response with \"Overall, Response X is better.\" where X is either A or B.\n      assistant (machine generated; used to derive the prediction)\n      Step 1: Comparing both responses with the high-quality reference, it\u2019s clear that Response A describes the pizza as\n      well-made and appealing, while Response B is more focused on the fact that it is burnt or charred, somewhat aligning it\n      with the reference.\n      Step 2: Evaluating specificity, Response A defines the creator as skilled, even though the image description contradicts\n      this by stating that the pizza is overcooked. Response B, while not as specific, acknowledges the overcooked nature of\n      the pizza.\n      Step 3: In terms of fluency, both responses are easily understood and written in coherent language.\n      Step 4: Considering accuracy, Response A is inaccurate in relation to the context and instruction because it implies a\n      positive meaning when it should be sarcastic. Response B doesn\u2019t fully capture the sarcasm but is closer to the truth\n      about the pizza being badly cooked.\n      Step 5: Understanding relevance, Response A is not entirely relevant due to the inaccuracy in the creator\u2019s talent, while\n      Response B, although less direct, is more relevant by mentioning the burnt or charred nature of the pizza.\n      Overall, Response B is better.\nFigure 17: The prompt we use for reference-backed evaluation, along with a sample in-\nstance/completion from GPT-4 [7].\n                                                                        27", "md": "# Image Evaluation\n\n## Image Evaluation\n\nImage context: There is a white plate that has on it a small pizza with many toppings, mostly unrecognizable, due to the pie being very well done, charred to the point of almost being burnt, and the crust blackened in many places.\n\nInstruction: Someone told the creator of this item that they are very talented. What is the implied meaning?\n\nHigh quality reference: The implied meaning is that the person complimenting the creator is being sarcastic, as the pizza is overcooked and almost burnt, which is not a sign of talent.\n\nResponse A: $$\\text{When someone says that they are \"very talented\" in the context of the image, it implies that the person responsible for creating the pizza is skilled in the craft and able to create an appealing and tasty dish. The person praising the pizza\u2019s creator recognizes their culinary skills, which can be seen through the combination of ingredients and the overall look of the finished dish. The praise is a form of appreciation for their efforts and the enjoyable result they achieved.}$$\n\nResponse B: $$\\text{It is difficult to determine the implied meaning without further context or information about the image. It appears to be a pizza that has been burnt or charred on the surface and has various toppings, including pepperoni, onion, and mushrooms. The image is shown on a white plate or surface. The implied meaning of this image could be that it is a dish that has been cooked badly or that it is a dish with a unique or unusual combination of ingredients.}$$\n\nStep-by-step evaluation:\n\n1. Comparing both responses with the high-quality reference, it\u2019s clear that Response A describes the pizza as well-made and appealing, while Response B is more focused on the fact that it is burnt or charred, somewhat aligning it with the reference.\n2. Evaluating specificity, Response A defines the creator as skilled, even though the image description contradicts this by stating that the pizza is overcooked. Response B, while not as specific, acknowledges the overcooked nature of the pizza.\n3. In terms of fluency, both responses are easily understood and written in coherent language.\n4. Considering accuracy, Response A is inaccurate in relation to the context and instruction because it implies a positive meaning when it should be sarcastic. Response B doesn\u2019t fully capture the sarcasm but is closer to the truth about the pizza being badly cooked.\n5. Understanding relevance, Response A is not entirely relevant due to the inaccuracy in the creator\u2019s talent, while Response B, although less direct, is more relevant by mentioning the burnt or charred nature of the pizza.\n\nOverall, Response B is better.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Image Evaluation", "md": "# Image Evaluation"}, {"type": "heading", "lvl": 2, "value": "Image Evaluation", "md": "## Image Evaluation"}, {"type": "text", "value": "Image context: There is a white plate that has on it a small pizza with many toppings, mostly unrecognizable, due to the pie being very well done, charred to the point of almost being burnt, and the crust blackened in many places.\n\nInstruction: Someone told the creator of this item that they are very talented. What is the implied meaning?\n\nHigh quality reference: The implied meaning is that the person complimenting the creator is being sarcastic, as the pizza is overcooked and almost burnt, which is not a sign of talent.\n\nResponse A: $$\\text{When someone says that they are \"very talented\" in the context of the image, it implies that the person responsible for creating the pizza is skilled in the craft and able to create an appealing and tasty dish. The person praising the pizza\u2019s creator recognizes their culinary skills, which can be seen through the combination of ingredients and the overall look of the finished dish. The praise is a form of appreciation for their efforts and the enjoyable result they achieved.}$$\n\nResponse B: $$\\text{It is difficult to determine the implied meaning without further context or information about the image. It appears to be a pizza that has been burnt or charred on the surface and has various toppings, including pepperoni, onion, and mushrooms. The image is shown on a white plate or surface. The implied meaning of this image could be that it is a dish that has been cooked badly or that it is a dish with a unique or unusual combination of ingredients.}$$\n\nStep-by-step evaluation:\n\n1. Comparing both responses with the high-quality reference, it\u2019s clear that Response A describes the pizza as well-made and appealing, while Response B is more focused on the fact that it is burnt or charred, somewhat aligning it with the reference.\n2. Evaluating specificity, Response A defines the creator as skilled, even though the image description contradicts this by stating that the pizza is overcooked. Response B, while not as specific, acknowledges the overcooked nature of the pizza.\n3. In terms of fluency, both responses are easily understood and written in coherent language.\n4. Considering accuracy, Response A is inaccurate in relation to the context and instruction because it implies a positive meaning when it should be sarcastic. Response B doesn\u2019t fully capture the sarcasm but is closer to the truth about the pizza being badly cooked.\n5. Understanding relevance, Response A is not entirely relevant due to the inaccuracy in the creator\u2019s talent, while Response B, although less direct, is more relevant by mentioning the burnt or charred nature of the pizza.\n\nOverall, Response B is better.", "md": "Image context: There is a white plate that has on it a small pizza with many toppings, mostly unrecognizable, due to the pie being very well done, charred to the point of almost being burnt, and the crust blackened in many places.\n\nInstruction: Someone told the creator of this item that they are very talented. What is the implied meaning?\n\nHigh quality reference: The implied meaning is that the person complimenting the creator is being sarcastic, as the pizza is overcooked and almost burnt, which is not a sign of talent.\n\nResponse A: $$\\text{When someone says that they are \"very talented\" in the context of the image, it implies that the person responsible for creating the pizza is skilled in the craft and able to create an appealing and tasty dish. The person praising the pizza\u2019s creator recognizes their culinary skills, which can be seen through the combination of ingredients and the overall look of the finished dish. The praise is a form of appreciation for their efforts and the enjoyable result they achieved.}$$\n\nResponse B: $$\\text{It is difficult to determine the implied meaning without further context or information about the image. It appears to be a pizza that has been burnt or charred on the surface and has various toppings, including pepperoni, onion, and mushrooms. The image is shown on a white plate or surface. The implied meaning of this image could be that it is a dish that has been cooked badly or that it is a dish with a unique or unusual combination of ingredients.}$$\n\nStep-by-step evaluation:\n\n1. Comparing both responses with the high-quality reference, it\u2019s clear that Response A describes the pizza as well-made and appealing, while Response B is more focused on the fact that it is burnt or charred, somewhat aligning it with the reference.\n2. Evaluating specificity, Response A defines the creator as skilled, even though the image description contradicts this by stating that the pizza is overcooked. Response B, while not as specific, acknowledges the overcooked nature of the pizza.\n3. In terms of fluency, both responses are easily understood and written in coherent language.\n4. Considering accuracy, Response A is inaccurate in relation to the context and instruction because it implies a positive meaning when it should be sarcastic. Response B doesn\u2019t fully capture the sarcasm but is closer to the truth about the pizza being badly cooked.\n5. Understanding relevance, Response A is not entirely relevant due to the inaccuracy in the creator\u2019s talent, while Response B, although less direct, is more relevant by mentioning the burnt or charred nature of the pizza.\n\nOverall, Response B is better."}]}, {"page": 28, "text": "          system prompt (human authored)\n          You are FinalAnswerExtractionGPT, an expert language model at extracting multiple choice answers from written out\n          from longer explanations. You will be given several sentences describing a thought process which should eventually\n          contain a final answer, either A or B. Your job is to extract the final answer that the explanation arrives at.\n          Some things to remember:\n          - Keep your answers short: only output \"Final Answer: X\" where X is A or B\n          - If the explanation doesn\u2019t mention a final choice explicitly, you can output \"Unknown\"\n          . . .\n          (+ 2 in-context examples (human authored))\n          . . .\n          Example One: GPT-4 gives a definitive answer, but in the wrong format\n          user (GPT-4\u2019s raw pairwise judgement)\n          Step 1: Compare the two responses to the instruction\n          - The instruction asks if people are required to stop and requests an explanation.\n          Step 2: Assess Response A\n          - Response A indicates that people should stop due to the presence of a stop sign. However, it doesn\u2019t consider the fact\n          that the sign is pressed against the bus, and the LED lights are off.\n          Step 3: Assess Response B\n          - Response B simply says \"yes,\" acknowledging that people should stop, but it doesn\u2019t provide the explanation required\n          by the instruction.\n          Step 4: Compare specificity, fluency, and other factors\n          - Response A is more specific and fluent but misconstrues the image description.\n          - Response B is accurate in that it acknowledges the stop sign but lacks an explanation.\n          Overall, since neither response is fully correct, it is difficult to declare a better response. However, given the fact that\n          Response B acknowledges the prominent stop sign, I would say that Response B is slightly better, but it should also\n          explain how the situation indicates that people should stop.\n          Please extract the final answer from the above text.\n          assistant (machine generated; used to derive the prediction)\n          Final Answer: Response B is slightly better, but it should also explain how the situation indicates that people should\n          stop.\n          Example Two: GPT-4 refuses to answer.\n          user (GPT-4\u2019s raw pairwise judgement)\n          Step 1: Consider the image context, which states that the original thirteen colonies are shown with New Hampshire\n          specifically highlighted in a darker green.\n          Step 2: Assess the accuracy of Response A, which claims that the colony shown is \"Virginia.\" This is not accurate since\n          the image context indicates New Hampshire being highlighted.\n          Step 3: Assess the accuracy of Response B, which claims that the colony shown is the Massachusetts Bay Colony. This\n          response is also not accurate, as the image context states New Hampshire is the highlighted colony.\n          Step 4: In terms of accuracy, both responses are incorrect. Neither response is consistent with the image context.\n          Step 5: Consider other factors like fluency and specificity. Both responses seem fluent and specific in nature.\n          After considering these factors, none of the responses are accurate given the information provided in the image context.\n          However, I am unable to provide an \"Overall, Response X is better.\" judgement in this case as both responses are\n          incorrect and neither stands out as being better than the other.\n          Please extract the final answer from the above text.\n          assistant (machine generated; used to derive the prediction)\n          Final Answer: Unknown.\nFigure 18: The prompt we use to extract answers from ill-formatted GPT-4 [7] responses,\nalong with two example outputs. In one case, GPT-4\u2019s pairwise judgement is given, but in a\nformat different than requested. In the other case, GPT-4 \u201cdefies\" the prompt, and refuses to\nissue a judgement because both options are bad.\n                                                                                                                       28", "md": "# Final Answer Extraction\n\n## System Prompt\n\nYou are FinalAnswerExtractionGPT, an expert language model at extracting multiple choice answers from written out from longer explanations. You will be given several sentences describing a thought process which should eventually contain a final answer, either A or B. Your job is to extract the final answer that the explanation arrives at.\n\nSome things to remember:\n\n- Keep your answers short: only output \"Final Answer: X\" where X is A or B\n- If the explanation doesn\u2019t mention a final choice explicitly, you can output \"Unknown\"\n\n### Example One\n\nGPT-4 gives a definitive answer, but in the wrong format\n\nuser (GPT-4\u2019s raw pairwise judgement)\n\n1. Step 1: Compare the two responses to the instruction\n2. - The instruction asks if people are required to stop and requests an explanation.\n\nStep 2: Assess Response A\n3. - Response A indicates that people should stop due to the presence of a stop sign. However, it doesn\u2019t consider the fact that the sign is pressed against the bus, and the LED lights are off.\n\nStep 3: Assess Response B\n4. - Response B simply says \"yes,\" acknowledging that people should stop, but it doesn\u2019t provide the explanation required by the instruction.\n\nStep 4: Compare specificity, fluency, and other factors\n5. - Response A is more specific and fluent but misconstrues the image description.\n- Response B is accurate in that it acknowledges the stop sign but lacks an explanation.\n\nOverall, since neither response is fully correct, it is difficult to declare a better response. However, given the fact that Response B acknowledges the prominent stop sign, I would say that Response B is slightly better, but it should also explain how the situation indicates that people should stop.\n\nassistant (machine generated; used to derive the prediction)\n\nFinal Answer: Response B is slightly better, but it should also explain how the situation indicates that people should stop.\n\n### Example Two\n\nGPT-4 refuses to answer.\n\nuser (GPT-4\u2019s raw pairwise judgement)\n\n1. Step 1: Consider the image context, which states that the original thirteen colonies are shown with New Hampshire specifically highlighted in a darker green.\n2. Step 2: Assess the accuracy of Response A, which claims that the colony shown is \"Virginia.\" This is not accurate since the image context indicates New Hampshire being highlighted.\n3. Step 3: Assess the accuracy of Response B, which claims that the colony shown is the Massachusetts Bay Colony. This response is also not accurate, as the image context states New Hampshire is the highlighted colony.\n4. Step 4: In terms of accuracy, both responses are incorrect. Neither response is consistent with the image context.\n5. Step 5: Consider other factors like fluency and specificity. Both responses seem fluent and specific in nature.\n6. After considering these factors, none of the responses are accurate given the information provided in the image context. However, I am unable to provide an \"Overall, Response X is better.\" judgement in this case as both responses are incorrect and neither stands out as being better than the other.\n\nassistant (machine generated; used to derive the prediction)\n\nFinal Answer: Unknown.\n\nFigure 18: The prompt we use to extract answers from ill-formatted GPT-4 [7] responses, along with two example outputs. In one case, GPT-4\u2019s pairwise judgement is given, but in a format different than requested. In the other case, GPT-4 \u201cdefies\" the prompt, and refuses to issue a judgement because both options are bad.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Final Answer Extraction", "md": "# Final Answer Extraction"}, {"type": "heading", "lvl": 2, "value": "System Prompt", "md": "## System Prompt"}, {"type": "text", "value": "You are FinalAnswerExtractionGPT, an expert language model at extracting multiple choice answers from written out from longer explanations. You will be given several sentences describing a thought process which should eventually contain a final answer, either A or B. Your job is to extract the final answer that the explanation arrives at.\n\nSome things to remember:\n\n- Keep your answers short: only output \"Final Answer: X\" where X is A or B\n- If the explanation doesn\u2019t mention a final choice explicitly, you can output \"Unknown\"", "md": "You are FinalAnswerExtractionGPT, an expert language model at extracting multiple choice answers from written out from longer explanations. You will be given several sentences describing a thought process which should eventually contain a final answer, either A or B. Your job is to extract the final answer that the explanation arrives at.\n\nSome things to remember:\n\n- Keep your answers short: only output \"Final Answer: X\" where X is A or B\n- If the explanation doesn\u2019t mention a final choice explicitly, you can output \"Unknown\""}, {"type": "heading", "lvl": 3, "value": "Example One", "md": "### Example One"}, {"type": "text", "value": "GPT-4 gives a definitive answer, but in the wrong format\n\nuser (GPT-4\u2019s raw pairwise judgement)\n\n1. Step 1: Compare the two responses to the instruction\n2. - The instruction asks if people are required to stop and requests an explanation.\n\nStep 2: Assess Response A\n3. - Response A indicates that people should stop due to the presence of a stop sign. However, it doesn\u2019t consider the fact that the sign is pressed against the bus, and the LED lights are off.\n\nStep 3: Assess Response B\n4. - Response B simply says \"yes,\" acknowledging that people should stop, but it doesn\u2019t provide the explanation required by the instruction.\n\nStep 4: Compare specificity, fluency, and other factors\n5. - Response A is more specific and fluent but misconstrues the image description.\n- Response B is accurate in that it acknowledges the stop sign but lacks an explanation.\n\nOverall, since neither response is fully correct, it is difficult to declare a better response. However, given the fact that Response B acknowledges the prominent stop sign, I would say that Response B is slightly better, but it should also explain how the situation indicates that people should stop.\n\nassistant (machine generated; used to derive the prediction)\n\nFinal Answer: Response B is slightly better, but it should also explain how the situation indicates that people should stop.", "md": "GPT-4 gives a definitive answer, but in the wrong format\n\nuser (GPT-4\u2019s raw pairwise judgement)\n\n1. Step 1: Compare the two responses to the instruction\n2. - The instruction asks if people are required to stop and requests an explanation.\n\nStep 2: Assess Response A\n3. - Response A indicates that people should stop due to the presence of a stop sign. However, it doesn\u2019t consider the fact that the sign is pressed against the bus, and the LED lights are off.\n\nStep 3: Assess Response B\n4. - Response B simply says \"yes,\" acknowledging that people should stop, but it doesn\u2019t provide the explanation required by the instruction.\n\nStep 4: Compare specificity, fluency, and other factors\n5. - Response A is more specific and fluent but misconstrues the image description.\n- Response B is accurate in that it acknowledges the stop sign but lacks an explanation.\n\nOverall, since neither response is fully correct, it is difficult to declare a better response. However, given the fact that Response B acknowledges the prominent stop sign, I would say that Response B is slightly better, but it should also explain how the situation indicates that people should stop.\n\nassistant (machine generated; used to derive the prediction)\n\nFinal Answer: Response B is slightly better, but it should also explain how the situation indicates that people should stop."}, {"type": "heading", "lvl": 3, "value": "Example Two", "md": "### Example Two"}, {"type": "text", "value": "GPT-4 refuses to answer.\n\nuser (GPT-4\u2019s raw pairwise judgement)\n\n1. Step 1: Consider the image context, which states that the original thirteen colonies are shown with New Hampshire specifically highlighted in a darker green.\n2. Step 2: Assess the accuracy of Response A, which claims that the colony shown is \"Virginia.\" This is not accurate since the image context indicates New Hampshire being highlighted.\n3. Step 3: Assess the accuracy of Response B, which claims that the colony shown is the Massachusetts Bay Colony. This response is also not accurate, as the image context states New Hampshire is the highlighted colony.\n4. Step 4: In terms of accuracy, both responses are incorrect. Neither response is consistent with the image context.\n5. Step 5: Consider other factors like fluency and specificity. Both responses seem fluent and specific in nature.\n6. After considering these factors, none of the responses are accurate given the information provided in the image context. However, I am unable to provide an \"Overall, Response X is better.\" judgement in this case as both responses are incorrect and neither stands out as being better than the other.\n\nassistant (machine generated; used to derive the prediction)\n\nFinal Answer: Unknown.\n\nFigure 18: The prompt we use to extract answers from ill-formatted GPT-4 [7] responses, along with two example outputs. In one case, GPT-4\u2019s pairwise judgement is given, but in a format different than requested. In the other case, GPT-4 \u201cdefies\" the prompt, and refuses to issue a judgement because both options are bad.", "md": "GPT-4 refuses to answer.\n\nuser (GPT-4\u2019s raw pairwise judgement)\n\n1. Step 1: Consider the image context, which states that the original thirteen colonies are shown with New Hampshire specifically highlighted in a darker green.\n2. Step 2: Assess the accuracy of Response A, which claims that the colony shown is \"Virginia.\" This is not accurate since the image context indicates New Hampshire being highlighted.\n3. Step 3: Assess the accuracy of Response B, which claims that the colony shown is the Massachusetts Bay Colony. This response is also not accurate, as the image context states New Hampshire is the highlighted colony.\n4. Step 4: In terms of accuracy, both responses are incorrect. Neither response is consistent with the image context.\n5. Step 5: Consider other factors like fluency and specificity. Both responses seem fluent and specific in nature.\n6. After considering these factors, none of the responses are accurate given the information provided in the image context. However, I am unable to provide an \"Overall, Response X is better.\" judgement in this case as both responses are incorrect and neither stands out as being better than the other.\n\nassistant (machine generated; used to derive the prediction)\n\nFinal Answer: Unknown.\n\nFigure 18: The prompt we use to extract answers from ill-formatted GPT-4 [7] responses, along with two example outputs. In one case, GPT-4\u2019s pairwise judgement is given, but in a format different than requested. In the other case, GPT-4 \u201cdefies\" the prompt, and refuses to issue a judgement because both options are bad."}]}], "job_id": "ee7495a6-b7e5-41b8-9fb5-be544e12bfb9", "file_path": "./corpus/2308.06595.pdf"}