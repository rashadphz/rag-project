{"pages": [{"page": 1, "text": "                             ELDEN: Exploration via Local Dependencies\n                                           Jiaheng Hu\u2217                                   Zizhao Wang\u2217\n                                  University of Texas at Austin                  University of Texas at Austin\n                                      jhu@cs.utexas.edu                          zizhao.wang@utexas.edu\narXiv:2310.08702v1  [cs.LG]  12 Oct 2023     Peter Stone\u2020                             Roberto Mart\u00edn-Mart\u00edn\u2020\n                               University of Texas at Austin, Sony AI               University of Texas at Austin\n                                     pstone@cs.utexas.edu                           robertomm@cs.utexas.edu\n                                                                   Abstract\n                              Tasks with large state space and sparse rewards present a longstanding challenge to\n                              reinforcement learning. In these tasks, an agent needs to explore the state space\n                              efficiently until it finds a reward. To deal with this problem, the community has\n                              proposed to augment the reward function with intrinsic reward, a bonus signal that\n                              encourages the agent to visit interesting states. In this work, we propose a new\n                              way of defining interesting states for environments with factored state spaces and\n                              complex chained dependencies, where an agent\u2019s actions may change the value\n                              of one entity that, in order, may affect the value of another entity. Our insight\n                              is that, in these environments, interesting states for exploration are states where\n                              the agent is uncertain whether (as opposed to how) entities such as the agent or\n                              objects have some influence on each other. We present ELDEN, Exploration via\n                              Local DepENdencies, a novel intrinsic reward that encourages the discovery of\n                              new interactions between entities. ELDEN utilizes a novel scheme \u2014 the partial\n                              derivative of the learned dynamics to model the local dependencies between entities\n                              accurately and computationally efficiently. The uncertainty of the predicted depen-\n                              dencies is then used as an intrinsic reward to encourage exploration toward new\n                              interactions. We evaluate the performance of ELDEN on four different domains\n                              with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In\n                              all domains, ELDEN correctly identifies local dependencies and learns successful\n                              policies, significantly outperforming previous state-of-the-art exploration methods.\n                    1    Introduction\n                    Reinforcement learning (RL) has achieved remarkable success in recent years in tasks where a\n                    well-shaped dense reward function is easy to define, such as playing video games [33, 18, 4] and\n                    controlling robots [9, 2, 15, 16]. However, for many real-world tasks, defining a dense reward function\n                    is non-trivial, yet a sparse reward function based on success or failure is directly available. For such\n                    reward functions, learning good policies is often challenging, as it requires efficient exploration of\n                    the state space.\n                    To address this challenge, RL researchers proposed the use of an intrinsic reward, an additional\n                    task-agnostic signal given to the agent for visiting interesting states. Intrinsic reward methods can be\n                    roughly classified into two main paradigms: curiosity [20, 25, 6] and empowerment [29, 27, 14] ,\n                    where the agent is rewarded either for visiting novel states or for obtaining maximal control over the\n                    environment, respectively.\n                        \u2217Equal contribution\n                        \u2020Equal supervision\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# ELDEN: Exploration via Local Dependencies\n\n# ELDEN: Exploration via Local Dependencies\n\nJiaheng Hu* University of Texas at Austin\n\nZizhao Wang* University of Texas at Austin\n\nEmail: jhu@cs.utexas.edu, zizhao.wang@utexas.edu\n\nPeter Stone\u2020 University of Texas at Austin, Sony AI\n\nRoberto Mart\u00edn-Mart\u00edn\u2020 University of Texas at Austin\n\nEmail: pstone@cs.utexas.edu, robertomm@cs.utexas.edu\n\n## Abstract\n\nTasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent\u2019s actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme \u2014 the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods.\n\n### Introduction\n\nReinforcement learning (RL) has achieved remarkable success in recent years in tasks where a well-shaped dense reward function is easy to define, such as playing video games and controlling robots. However, for many real-world tasks, defining a dense reward function is non-trivial, yet a sparse reward function based on success or failure is directly available. For such reward functions, learning good policies is often challenging, as it requires efficient exploration of the state space. To address this challenge, RL researchers proposed the use of an intrinsic reward, an additional task-agnostic signal given to the agent for visiting interesting states. Intrinsic reward methods can be roughly classified into two main paradigms: curiosity and empowerment, where the agent is rewarded either for visiting novel states or for obtaining maximal control over the environment, respectively.\n\n*Equal contribution\n\n\u2020Equal supervision\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "ELDEN: Exploration via Local Dependencies", "md": "# ELDEN: Exploration via Local Dependencies"}, {"type": "heading", "lvl": 1, "value": "ELDEN: Exploration via Local Dependencies", "md": "# ELDEN: Exploration via Local Dependencies"}, {"type": "text", "value": "Jiaheng Hu* University of Texas at Austin\n\nZizhao Wang* University of Texas at Austin\n\nEmail: jhu@cs.utexas.edu, zizhao.wang@utexas.edu\n\nPeter Stone\u2020 University of Texas at Austin, Sony AI\n\nRoberto Mart\u00edn-Mart\u00edn\u2020 University of Texas at Austin\n\nEmail: pstone@cs.utexas.edu, robertomm@cs.utexas.edu", "md": "Jiaheng Hu* University of Texas at Austin\n\nZizhao Wang* University of Texas at Austin\n\nEmail: jhu@cs.utexas.edu, zizhao.wang@utexas.edu\n\nPeter Stone\u2020 University of Texas at Austin, Sony AI\n\nRoberto Mart\u00edn-Mart\u00edn\u2020 University of Texas at Austin\n\nEmail: pstone@cs.utexas.edu, robertomm@cs.utexas.edu"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent\u2019s actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme \u2014 the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods.", "md": "Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent\u2019s actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between entities. ELDEN utilizes a novel scheme \u2014 the partial derivative of the learned dynamics to model the local dependencies between entities accurately and computationally efficiently. The uncertainty of the predicted dependencies is then used as an intrinsic reward to encourage exploration toward new interactions. We evaluate the performance of ELDEN on four different domains with complex dependencies, ranging from 2D grid worlds to 3D robotic tasks. In all domains, ELDEN correctly identifies local dependencies and learns successful policies, significantly outperforming previous state-of-the-art exploration methods."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "Reinforcement learning (RL) has achieved remarkable success in recent years in tasks where a well-shaped dense reward function is easy to define, such as playing video games and controlling robots. However, for many real-world tasks, defining a dense reward function is non-trivial, yet a sparse reward function based on success or failure is directly available. For such reward functions, learning good policies is often challenging, as it requires efficient exploration of the state space. To address this challenge, RL researchers proposed the use of an intrinsic reward, an additional task-agnostic signal given to the agent for visiting interesting states. Intrinsic reward methods can be roughly classified into two main paradigms: curiosity and empowerment, where the agent is rewarded either for visiting novel states or for obtaining maximal control over the environment, respectively.\n\n*Equal contribution\n\n\u2020Equal supervision\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Reinforcement learning (RL) has achieved remarkable success in recent years in tasks where a well-shaped dense reward function is easy to define, such as playing video games and controlling robots. However, for many real-world tasks, defining a dense reward function is non-trivial, yet a sparse reward function based on success or failure is directly available. For such reward functions, learning good policies is often challenging, as it requires efficient exploration of the state space. To address this challenge, RL researchers proposed the use of an intrinsic reward, an additional task-agnostic signal given to the agent for visiting interesting states. Intrinsic reward methods can be roughly classified into two main paradigms: curiosity and empowerment, where the agent is rewarded either for visiting novel states or for obtaining maximal control over the environment, respectively.\n\n*Equal contribution\n\n\u2020Equal supervision\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "                                                                 behaviors of different agents at different learning stages\n           the task requires\n  turning on the stove       to cook\n    the meatball      in the pot\n                                            Curiosity: interested in high prediction errors\n                                                          in entity movements                          ELDEN (ours): interested in novel local\n                                                                                                           dependencies between entities\n meatball\n   pot                  stove switch          Empowerment: interested in maximizing the\n                                             action's influence on environment entities\n                            stove\n Figure 1: (Left) In a kitchen task with multiple potential agent-object and object-object interactions, (Middle)\n for a curiosity-based agent interested in hard-to-predict entity motion, it will initially focus on exploring arm\n movement, then on pot and meatball manipulation, and finally keep rolling the meatball whose outcomes are\n challenging to predict. On the other hand, for an empowerment-based agent interested in maximizing the action\u2019s\n influence, it begins with controlling the arm and then learning to move the pot and meatball simultaneously, but\n it ignores the potential interaction between the stove and the meatball. (Right) ELDEN avoids those issues by\n identifying whether dependencies between entities happen and focusing the exploration on novel ones. After the\n agent learns that it can control the pot and meatball, it will move on to explore other potential interactions, e.g.,\nwhether the stove can influence the meatball. Hence it has a larger opportunity to learn this task, compared with\n a curiosity or empowerment-based agent.\nWhile these methods significantly improve exploration in some domains, there are cases where the\n aforementioned methods fail. Consider, for example, a kitchen environment with several objects\nwhere there are multiple potential agent-object and object-object interactions, and an agent is tasked\nwith putting a meatball in a pot and cooking it on the stove (Fig. 1). On the one hand, curiosity-driven\n methods will encourage the agent to explore the environment by visiting states where the exact\n outcome of an action is uncertain. Consequently, for each interactable object, the agent will exhaust\n any possible interaction until it can accurately predict every change in the object\u2019s state. As a result,\n such an exploration strategy can be inefficient, especially for environments with many objects. In the\n kitchen example, it is hard to predict how the meatball rolls in the pot, and thus the curiosity-driven\n agent would keep rolling it. On the other hand, for empowerment methods, the agent is encouraged\n to remain in states where it can influence as many states (objects) simultaneously as possible (e.g.\n holding the pot with the meatball inside). By doing so, however, it ignores object-object interactions\n that the action cannot directly control but indirectly induce, which can be the key to task completion.\n In the kitchen case, an empowerment-driven agent will therefore not be interested in placing the\n pot and the meatball on the stove, as it forfeits control of them by doing so, even though this action\n enables the stove to heat the meatball. Our main insight is that, in this type of environment, an\n intelligently exploring agent should be able to learn that it can use the pot to move the meatball after a\n few trials. Then, instead of spending time learning the complex meatball movement or different styles\n to manipulate the pot, it would move on to explore other modes of interacting with other objects, e.g.,\n putting the pot on the stove and switching on the stove.\n Following this motivation, we propose a new definition of interesting states \u2014 focusing on whether\n the environment entities (consisting of the agents and objects) can interact, rather than how exactly\n they interact. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward\n mechanism that models the local dependencies between entities in the scene (agent-object, object-\n object) and uses the uncertainty about the dependencies to guide exploration. By relaxing the curiosity\n signal from dynamics prediction to dependencies prediction, ELDEN implicitly biases the exploration\n toward states where novel interaction modes happen rather than states where the state value is novel\n but dependencies remain the same. Specifically, ELDEN trains an ensemble of dynamics models. In\n each model, the local dependencies between objects are modeled by the partial derivatives of state\n predictions w.r.t. the current state and action. Then, the local dependency uncertainty is measured as\n the variance across all dynamic models.\nWe evaluate ELDEN on discrete and continuous domains with multiple objects leading to many\n interaction modes and tasks with chained dependencies. Our results show that using a partial\n                                                                           2", "md": "# Exploration via Local Dependencies\n\n## Behaviors of Different Agents at Different Learning Stages\n\nThe task requires turning on the stove to cook the meatball in the pot.\n\nCuriosity: interested in high prediction errors in entity movements\n\nELDEN (ours): interested in novel local dependencies between entities\n\nEmpowerment: interested in maximizing the action's influence on environment entities\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{meatball} & \\text{pot} & \\text{stove switch} \\\\\n\\hline\n\\text{pot} & & \\text{stove} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 1: (Left) In a kitchen task with multiple potential agent-object and object-object interactions, (Middle) for a curiosity-based agent interested in hard-to-predict entity motion, it will initially focus on exploring arm movement, then on pot and meatball manipulation, and finally keep rolling the meatball whose outcomes are challenging to predict. On the other hand, for an empowerment-based agent interested in maximizing the action\u2019s influence, it begins with controlling the arm and then learning to move the pot and meatball simultaneously, but it ignores the potential interaction between the stove and the meatball. (Right) ELDEN avoids those issues by identifying whether dependencies between entities happen and focusing the exploration on novel ones. After the agent learns that it can control the pot and meatball, it will move on to explore other potential interactions, e.g., whether the stove can influence the meatball. Hence it has a larger opportunity to learn this task, compared with a curiosity or empowerment-based agent.\n\nWhile these methods significantly improve exploration in some domains, there are cases where the aforementioned methods fail. Consider, for example, a kitchen environment with several objects where there are multiple potential agent-object and object-object interactions, and an agent is tasked with putting a meatball in a pot and cooking it on the stove (Fig. 1). On the one hand, curiosity-driven methods will encourage the agent to explore the environment by visiting states where the exact outcome of an action is uncertain. Consequently, for each interactable object, the agent will exhaust any possible interaction until it can accurately predict every change in the object\u2019s state. As a result, such an exploration strategy can be inefficient, especially for environments with many objects. In the kitchen example, it is hard to predict how the meatball rolls in the pot, and thus the curiosity-driven agent would keep rolling it. On the other hand, for empowerment methods, the agent is encouraged to remain in states where it can influence as many states (objects) simultaneously as possible (e.g. holding the pot with the meatball inside). By doing so, however, it ignores object-object interactions that the action cannot directly control but indirectly induce, which can be the key to task completion. In the kitchen case, an empowerment-driven agent will therefore not be interested in placing the pot and the meatball on the stove, as it forfeits control of them by doing so, even though this action enables the stove to heat the meatball. Our main insight is that, in this type of environment, an intelligently exploring agent should be able to learn that it can use the pot to move the meatball after a few trials. Then, instead of spending time learning the complex meatball movement or different styles to manipulate the pot, it would move on to explore other modes of interacting with other objects, e.g., putting the pot on the stove and switching on the stove.\n\nFollowing this motivation, we propose a new definition of interesting states \u2014 focusing on whether the environment entities (consisting of the agents and objects) can interact, rather than how exactly they interact. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward mechanism that models the local dependencies between entities in the scene (agent-object, object-object) and uses the uncertainty about the dependencies to guide exploration. By relaxing the curiosity signal from dynamics prediction to dependencies prediction, ELDEN implicitly biases the exploration toward states where novel interaction modes happen rather than states where the state value is novel but dependencies remain the same. Specifically, ELDEN trains an ensemble of dynamics models. In each model, the local dependencies between objects are modeled by the partial derivatives of state predictions w.r.t. the current state and action. Then, the local dependency uncertainty is measured as the variance across all dynamic models.\n\nWe evaluate ELDEN on discrete and continuous domains with multiple objects leading to many interaction modes and tasks with chained dependencies. Our results show that using a partial", "images": [{"name": "page-2-37.jpg", "height": 35, "width": 35, "x": 414, "y": 83}, {"name": "page-2-38.jpg", "height": 35, "width": 35, "x": 462, "y": 83}, {"name": "page-2-36.jpg", "height": 35, "width": 35, "x": 365, "y": 83}, {"name": "page-2-35.jpg", "height": 35, "width": 35, "x": 317, "y": 83}, {"name": "page-2-34.jpg", "height": 35, "width": 35, "x": 269, "y": 83}, {"name": "page-2-33.jpg", "height": 35, "width": 35, "x": 221, "y": 83}, {"name": "page-2-0.jpg", "height": 104, "width": 104, "x": 108, "y": 110}], "items": [{"type": "heading", "lvl": 1, "value": "Exploration via Local Dependencies", "md": "# Exploration via Local Dependencies"}, {"type": "heading", "lvl": 2, "value": "Behaviors of Different Agents at Different Learning Stages", "md": "## Behaviors of Different Agents at Different Learning Stages"}, {"type": "text", "value": "The task requires turning on the stove to cook the meatball in the pot.\n\nCuriosity: interested in high prediction errors in entity movements\n\nELDEN (ours): interested in novel local dependencies between entities\n\nEmpowerment: interested in maximizing the action's influence on environment entities\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{meatball} & \\text{pot} & \\text{stove switch} \\\\\n\\hline\n\\text{pot} & & \\text{stove} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 1: (Left) In a kitchen task with multiple potential agent-object and object-object interactions, (Middle) for a curiosity-based agent interested in hard-to-predict entity motion, it will initially focus on exploring arm movement, then on pot and meatball manipulation, and finally keep rolling the meatball whose outcomes are challenging to predict. On the other hand, for an empowerment-based agent interested in maximizing the action\u2019s influence, it begins with controlling the arm and then learning to move the pot and meatball simultaneously, but it ignores the potential interaction between the stove and the meatball. (Right) ELDEN avoids those issues by identifying whether dependencies between entities happen and focusing the exploration on novel ones. After the agent learns that it can control the pot and meatball, it will move on to explore other potential interactions, e.g., whether the stove can influence the meatball. Hence it has a larger opportunity to learn this task, compared with a curiosity or empowerment-based agent.\n\nWhile these methods significantly improve exploration in some domains, there are cases where the aforementioned methods fail. Consider, for example, a kitchen environment with several objects where there are multiple potential agent-object and object-object interactions, and an agent is tasked with putting a meatball in a pot and cooking it on the stove (Fig. 1). On the one hand, curiosity-driven methods will encourage the agent to explore the environment by visiting states where the exact outcome of an action is uncertain. Consequently, for each interactable object, the agent will exhaust any possible interaction until it can accurately predict every change in the object\u2019s state. As a result, such an exploration strategy can be inefficient, especially for environments with many objects. In the kitchen example, it is hard to predict how the meatball rolls in the pot, and thus the curiosity-driven agent would keep rolling it. On the other hand, for empowerment methods, the agent is encouraged to remain in states where it can influence as many states (objects) simultaneously as possible (e.g. holding the pot with the meatball inside). By doing so, however, it ignores object-object interactions that the action cannot directly control but indirectly induce, which can be the key to task completion. In the kitchen case, an empowerment-driven agent will therefore not be interested in placing the pot and the meatball on the stove, as it forfeits control of them by doing so, even though this action enables the stove to heat the meatball. Our main insight is that, in this type of environment, an intelligently exploring agent should be able to learn that it can use the pot to move the meatball after a few trials. Then, instead of spending time learning the complex meatball movement or different styles to manipulate the pot, it would move on to explore other modes of interacting with other objects, e.g., putting the pot on the stove and switching on the stove.\n\nFollowing this motivation, we propose a new definition of interesting states \u2014 focusing on whether the environment entities (consisting of the agents and objects) can interact, rather than how exactly they interact. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward mechanism that models the local dependencies between entities in the scene (agent-object, object-object) and uses the uncertainty about the dependencies to guide exploration. By relaxing the curiosity signal from dynamics prediction to dependencies prediction, ELDEN implicitly biases the exploration toward states where novel interaction modes happen rather than states where the state value is novel but dependencies remain the same. Specifically, ELDEN trains an ensemble of dynamics models. In each model, the local dependencies between objects are modeled by the partial derivatives of state predictions w.r.t. the current state and action. Then, the local dependency uncertainty is measured as the variance across all dynamic models.\n\nWe evaluate ELDEN on discrete and continuous domains with multiple objects leading to many interaction modes and tasks with chained dependencies. Our results show that using a partial", "md": "The task requires turning on the stove to cook the meatball in the pot.\n\nCuriosity: interested in high prediction errors in entity movements\n\nELDEN (ours): interested in novel local dependencies between entities\n\nEmpowerment: interested in maximizing the action's influence on environment entities\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{meatball} & \\text{pot} & \\text{stove switch} \\\\\n\\hline\n\\text{pot} & & \\text{stove} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 1: (Left) In a kitchen task with multiple potential agent-object and object-object interactions, (Middle) for a curiosity-based agent interested in hard-to-predict entity motion, it will initially focus on exploring arm movement, then on pot and meatball manipulation, and finally keep rolling the meatball whose outcomes are challenging to predict. On the other hand, for an empowerment-based agent interested in maximizing the action\u2019s influence, it begins with controlling the arm and then learning to move the pot and meatball simultaneously, but it ignores the potential interaction between the stove and the meatball. (Right) ELDEN avoids those issues by identifying whether dependencies between entities happen and focusing the exploration on novel ones. After the agent learns that it can control the pot and meatball, it will move on to explore other potential interactions, e.g., whether the stove can influence the meatball. Hence it has a larger opportunity to learn this task, compared with a curiosity or empowerment-based agent.\n\nWhile these methods significantly improve exploration in some domains, there are cases where the aforementioned methods fail. Consider, for example, a kitchen environment with several objects where there are multiple potential agent-object and object-object interactions, and an agent is tasked with putting a meatball in a pot and cooking it on the stove (Fig. 1). On the one hand, curiosity-driven methods will encourage the agent to explore the environment by visiting states where the exact outcome of an action is uncertain. Consequently, for each interactable object, the agent will exhaust any possible interaction until it can accurately predict every change in the object\u2019s state. As a result, such an exploration strategy can be inefficient, especially for environments with many objects. In the kitchen example, it is hard to predict how the meatball rolls in the pot, and thus the curiosity-driven agent would keep rolling it. On the other hand, for empowerment methods, the agent is encouraged to remain in states where it can influence as many states (objects) simultaneously as possible (e.g. holding the pot with the meatball inside). By doing so, however, it ignores object-object interactions that the action cannot directly control but indirectly induce, which can be the key to task completion. In the kitchen case, an empowerment-driven agent will therefore not be interested in placing the pot and the meatball on the stove, as it forfeits control of them by doing so, even though this action enables the stove to heat the meatball. Our main insight is that, in this type of environment, an intelligently exploring agent should be able to learn that it can use the pot to move the meatball after a few trials. Then, instead of spending time learning the complex meatball movement or different styles to manipulate the pot, it would move on to explore other modes of interacting with other objects, e.g., putting the pot on the stove and switching on the stove.\n\nFollowing this motivation, we propose a new definition of interesting states \u2014 focusing on whether the environment entities (consisting of the agents and objects) can interact, rather than how exactly they interact. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward mechanism that models the local dependencies between entities in the scene (agent-object, object-object) and uses the uncertainty about the dependencies to guide exploration. By relaxing the curiosity signal from dynamics prediction to dependencies prediction, ELDEN implicitly biases the exploration toward states where novel interaction modes happen rather than states where the state value is novel but dependencies remain the same. Specifically, ELDEN trains an ensemble of dynamics models. In each model, the local dependencies between objects are modeled by the partial derivatives of state predictions w.r.t. the current state and action. Then, the local dependency uncertainty is measured as the variance across all dynamic models.\n\nWe evaluate ELDEN on discrete and continuous domains with multiple objects leading to many interaction modes and tasks with chained dependencies. Our results show that using a partial"}]}, {"page": 3, "text": "derivative-based extractor on dynamic models training allows us to accurately identify local connec-\ntivities between environment entities. Furthermore, the intrinsic reward derived from the identified\nlocal connectivities allows ELDEN to outperform state-of-the-art exploration methods (curiosity and\nempowerment-driven) on the tested domains.\n2    Related Work\nThe idea of using intrinsic reward to facilitate exploration in reinforcement learning is a long-studied\ntopic that can be dated back to Schmidhuber [25]. In this section, we first discuss two main classes of\nintrinsic reward related to ELDEN. Since ELDEN requires reasoning about the local dependencies\nbetween environment entities, we also discuss works that involve utilizing dependencies/causality in\nreinforcement learning.\n2.1   Curiosity-Driven Exploration\nCuriosity-driven exploration rewards an agent for visiting \u201cnovel\u201d states, where different methods\ndefine the \u201cnovelness\u201d of a state in different ways. For methods that utilize visit count to define state\nnovelty, Bellemare et al. [3] utilized a density model to estimate pseudo-count for each state, which\nis then used to derive intrinsic reward for the agent; Tang et al. [30] uses locality-sensitive hashing\ncode to convert high-dimensional states to hash codes such that the visit count for each hash code\ncan be explicitly kept track of. For methods that utilize predictiveness to define state novelty, Stadie\net al. [29] learns a forward dynamics model that operates on a learned latent space of the observation,\nand uses the prediction error as the intrinsic reward for exploration; Burda et al. [6] uses randomly\ninitialized networks to extract state features, where the agent is encouraged to visit states where\npredictions about the state features are inaccurate; Pathak et al. [21] utilizes disagreements within\nan ensemble of dynamic models as a signal for intrinsic reward, and directly backpropagates from\nthe reward signal to the policy parameters to improve exploration. Pathak et al. [20] incorporates\nempowerment into the curiosity-based method by learning an inverse dynamics model that maps\nfrom state to action. The inverse dynamics model defines a feature space for the states, where the\nprediction error of a forward dynamic model is measured in this feature space.\nHowever, learning accurate dynamics can be difficult and require a significant coverage of a (possibly\nlarge) state-action space; e.g., when a robot manipulates a block, it would have to experience multiple\naction-reaction pairs to be able to make accurate predictions [13]. Furthermore, prior curiosity-driven\nexploration methods can be derailed by the stochasticity in the dynamics, i.e., when the outcome of\nan action has large entropy (e.g., tossing a coin), so the agent would keep repeating it. ELDEN can\nbe considered a type of curiosity-driven exploration method. However, unlike previous works which\nonly consider improving knowledge about the dynamics, ELDEN explicitly considers knowledge\nabout local dependencies between environment entities, and utilizes it to encourage exploration. Thus,\nit avoids the need to learn an accurate dynamic model and is less sensitive to environmental noise and\nstochasticity.\n2.2   Empowerment-Driven Exploration\nEmpowerment-based exploration methods are based on a different understanding of what states\nshould be encouraged for task-agnostic exploration [35, 27, 8, 19]. Their main idea is that the\nmost interesting states to explore for any task are states where the agent has the most controllable\ndiversity about what the next state will be, i.e., states where there are multiple possible next states\nthat can be chosen by the agent. From those states, it is easier to fulfill any downstream task that\nrequires purposefully changing the state. Empowerment-based exploration methods reason about\nthe controllable elements in the environment (states that can be influenced by agent\u2019s actions), and\nencourage the agent to find states where this controllability is large, typically through some form of\nmutual information maximization between the agent\u2019s actions and the next states. In particular, Zhao\net al. [35] uses domain knowledge to divide up the state space into internal state and external state, and\nmaximize the mutual information between them. Seitzer et al. [27] measures the local dependencies\nbetween action and environment entities by estimating their conditional mutual information (CMI)\nand using it as intrinsic reward signal to encourage the agent to maximize the influence of action over\nenvironment entities.\n                                                     3", "md": "# Exploration Methods in Reinforcement Learning\n\n# Derivative-based Extractor on Dynamic Models Training\n\nThe derivative-based extractor on dynamic models training allows us to accurately identify local connectivities between environment entities. Furthermore, the intrinsic reward derived from the identified local connectivities allows ELDEN to outperform state-of-the-art exploration methods (curiosity and empowerment-driven) on the tested domains.\n\n## Related Work\n\nThe idea of using intrinsic reward to facilitate exploration in reinforcement learning is a long-studied topic that can be dated back to Schmidhuber [25]. In this section, we first discuss two main classes of intrinsic reward related to ELDEN. Since ELDEN requires reasoning about the local dependencies between environment entities, we also discuss works that involve utilizing dependencies/causality in reinforcement learning.\n\n### Curiosity-Driven Exploration\n\nCuriosity-driven exploration rewards an agent for visiting \u201cnovel\u201d states, where different methods define the \u201cnovelness\u201d of a state in different ways. For methods that utilize visit count to define state novelty, Bellemare et al. [3] utilized a density model to estimate pseudo-count for each state, which is then used to derive intrinsic reward for the agent; Tang et al. [30] uses locality-sensitive hashing code to convert high-dimensional states to hash codes such that the visit count for each hash code can be explicitly kept track of. For methods that utilize predictiveness to define state novelty, Stadie et al. [29] learns a forward dynamics model that operates on a learned latent space of the observation, and uses the prediction error as the intrinsic reward for exploration; Burda et al. [6] uses randomly initialized networks to extract state features, where the agent is encouraged to visit states where predictions about the state features are inaccurate; Pathak et al. [21] utilizes disagreements within an ensemble of dynamic models as a signal for intrinsic reward, and directly backpropagates from the reward signal to the policy parameters to improve exploration. Pathak et al. [20] incorporates empowerment into the curiosity-based method by learning an inverse dynamics model that maps from state to action. The inverse dynamics model defines a feature space for the states, where the prediction error of a forward dynamic model is measured in this feature space.\n\nHowever, learning accurate dynamics can be difficult and require a significant coverage of a (possibly large) state-action space; e.g., when a robot manipulates a block, it would have to experience multiple action-reaction pairs to be able to make accurate predictions [13]. Furthermore, prior curiosity-driven exploration methods can be derailed by the stochasticity in the dynamics, i.e., when the outcome of an action has large entropy (e.g., tossing a coin), so the agent would keep repeating it. ELDEN can be considered a type of curiosity-driven exploration method. However, unlike previous works which only consider improving knowledge about the dynamics, ELDEN explicitly considers knowledge about local dependencies between environment entities, and utilizes it to encourage exploration. Thus, it avoids the need to learn an accurate dynamic model and is less sensitive to environmental noise and stochasticity.\n\n### Empowerment-Driven Exploration\n\nEmpowerment-based exploration methods are based on a different understanding of what states should be encouraged for task-agnostic exploration [35, 27, 8, 19]. Their main idea is that the most interesting states to explore for any task are states where the agent has the most controllable diversity about what the next state will be, i.e., states where there are multiple possible next states that can be chosen by the agent. From those states, it is easier to fulfill any downstream task that requires purposefully changing the state. Empowerment-based exploration methods reason about the controllable elements in the environment (states that can be influenced by agent\u2019s actions), and encourage the agent to find states where this controllability is large, typically through some form of mutual information maximization between the agent\u2019s actions and the next states. In particular, Zhao et al. [35] uses domain knowledge to divide up the state space into internal state and external state, and maximize the mutual information between them. Seitzer et al. [27] measures the local dependencies between action and environment entities by estimating their conditional mutual information (CMI) and using it as intrinsic reward signal to encourage the agent to maximize the influence of action over environment entities.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Exploration Methods in Reinforcement Learning", "md": "# Exploration Methods in Reinforcement Learning"}, {"type": "heading", "lvl": 1, "value": "Derivative-based Extractor on Dynamic Models Training", "md": "# Derivative-based Extractor on Dynamic Models Training"}, {"type": "text", "value": "The derivative-based extractor on dynamic models training allows us to accurately identify local connectivities between environment entities. Furthermore, the intrinsic reward derived from the identified local connectivities allows ELDEN to outperform state-of-the-art exploration methods (curiosity and empowerment-driven) on the tested domains.", "md": "The derivative-based extractor on dynamic models training allows us to accurately identify local connectivities between environment entities. Furthermore, the intrinsic reward derived from the identified local connectivities allows ELDEN to outperform state-of-the-art exploration methods (curiosity and empowerment-driven) on the tested domains."}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "The idea of using intrinsic reward to facilitate exploration in reinforcement learning is a long-studied topic that can be dated back to Schmidhuber [25]. In this section, we first discuss two main classes of intrinsic reward related to ELDEN. Since ELDEN requires reasoning about the local dependencies between environment entities, we also discuss works that involve utilizing dependencies/causality in reinforcement learning.", "md": "The idea of using intrinsic reward to facilitate exploration in reinforcement learning is a long-studied topic that can be dated back to Schmidhuber [25]. In this section, we first discuss two main classes of intrinsic reward related to ELDEN. Since ELDEN requires reasoning about the local dependencies between environment entities, we also discuss works that involve utilizing dependencies/causality in reinforcement learning."}, {"type": "heading", "lvl": 3, "value": "Curiosity-Driven Exploration", "md": "### Curiosity-Driven Exploration"}, {"type": "text", "value": "Curiosity-driven exploration rewards an agent for visiting \u201cnovel\u201d states, where different methods define the \u201cnovelness\u201d of a state in different ways. For methods that utilize visit count to define state novelty, Bellemare et al. [3] utilized a density model to estimate pseudo-count for each state, which is then used to derive intrinsic reward for the agent; Tang et al. [30] uses locality-sensitive hashing code to convert high-dimensional states to hash codes such that the visit count for each hash code can be explicitly kept track of. For methods that utilize predictiveness to define state novelty, Stadie et al. [29] learns a forward dynamics model that operates on a learned latent space of the observation, and uses the prediction error as the intrinsic reward for exploration; Burda et al. [6] uses randomly initialized networks to extract state features, where the agent is encouraged to visit states where predictions about the state features are inaccurate; Pathak et al. [21] utilizes disagreements within an ensemble of dynamic models as a signal for intrinsic reward, and directly backpropagates from the reward signal to the policy parameters to improve exploration. Pathak et al. [20] incorporates empowerment into the curiosity-based method by learning an inverse dynamics model that maps from state to action. The inverse dynamics model defines a feature space for the states, where the prediction error of a forward dynamic model is measured in this feature space.\n\nHowever, learning accurate dynamics can be difficult and require a significant coverage of a (possibly large) state-action space; e.g., when a robot manipulates a block, it would have to experience multiple action-reaction pairs to be able to make accurate predictions [13]. Furthermore, prior curiosity-driven exploration methods can be derailed by the stochasticity in the dynamics, i.e., when the outcome of an action has large entropy (e.g., tossing a coin), so the agent would keep repeating it. ELDEN can be considered a type of curiosity-driven exploration method. However, unlike previous works which only consider improving knowledge about the dynamics, ELDEN explicitly considers knowledge about local dependencies between environment entities, and utilizes it to encourage exploration. Thus, it avoids the need to learn an accurate dynamic model and is less sensitive to environmental noise and stochasticity.", "md": "Curiosity-driven exploration rewards an agent for visiting \u201cnovel\u201d states, where different methods define the \u201cnovelness\u201d of a state in different ways. For methods that utilize visit count to define state novelty, Bellemare et al. [3] utilized a density model to estimate pseudo-count for each state, which is then used to derive intrinsic reward for the agent; Tang et al. [30] uses locality-sensitive hashing code to convert high-dimensional states to hash codes such that the visit count for each hash code can be explicitly kept track of. For methods that utilize predictiveness to define state novelty, Stadie et al. [29] learns a forward dynamics model that operates on a learned latent space of the observation, and uses the prediction error as the intrinsic reward for exploration; Burda et al. [6] uses randomly initialized networks to extract state features, where the agent is encouraged to visit states where predictions about the state features are inaccurate; Pathak et al. [21] utilizes disagreements within an ensemble of dynamic models as a signal for intrinsic reward, and directly backpropagates from the reward signal to the policy parameters to improve exploration. Pathak et al. [20] incorporates empowerment into the curiosity-based method by learning an inverse dynamics model that maps from state to action. The inverse dynamics model defines a feature space for the states, where the prediction error of a forward dynamic model is measured in this feature space.\n\nHowever, learning accurate dynamics can be difficult and require a significant coverage of a (possibly large) state-action space; e.g., when a robot manipulates a block, it would have to experience multiple action-reaction pairs to be able to make accurate predictions [13]. Furthermore, prior curiosity-driven exploration methods can be derailed by the stochasticity in the dynamics, i.e., when the outcome of an action has large entropy (e.g., tossing a coin), so the agent would keep repeating it. ELDEN can be considered a type of curiosity-driven exploration method. However, unlike previous works which only consider improving knowledge about the dynamics, ELDEN explicitly considers knowledge about local dependencies between environment entities, and utilizes it to encourage exploration. Thus, it avoids the need to learn an accurate dynamic model and is less sensitive to environmental noise and stochasticity."}, {"type": "heading", "lvl": 3, "value": "Empowerment-Driven Exploration", "md": "### Empowerment-Driven Exploration"}, {"type": "text", "value": "Empowerment-based exploration methods are based on a different understanding of what states should be encouraged for task-agnostic exploration [35, 27, 8, 19]. Their main idea is that the most interesting states to explore for any task are states where the agent has the most controllable diversity about what the next state will be, i.e., states where there are multiple possible next states that can be chosen by the agent. From those states, it is easier to fulfill any downstream task that requires purposefully changing the state. Empowerment-based exploration methods reason about the controllable elements in the environment (states that can be influenced by agent\u2019s actions), and encourage the agent to find states where this controllability is large, typically through some form of mutual information maximization between the agent\u2019s actions and the next states. In particular, Zhao et al. [35] uses domain knowledge to divide up the state space into internal state and external state, and maximize the mutual information between them. Seitzer et al. [27] measures the local dependencies between action and environment entities by estimating their conditional mutual information (CMI) and using it as intrinsic reward signal to encourage the agent to maximize the influence of action over environment entities.", "md": "Empowerment-based exploration methods are based on a different understanding of what states should be encouraged for task-agnostic exploration [35, 27, 8, 19]. Their main idea is that the most interesting states to explore for any task are states where the agent has the most controllable diversity about what the next state will be, i.e., states where there are multiple possible next states that can be chosen by the agent. From those states, it is easier to fulfill any downstream task that requires purposefully changing the state. Empowerment-based exploration methods reason about the controllable elements in the environment (states that can be influenced by agent\u2019s actions), and encourage the agent to find states where this controllability is large, typically through some form of mutual information maximization between the agent\u2019s actions and the next states. In particular, Zhao et al. [35] uses domain knowledge to divide up the state space into internal state and external state, and maximize the mutual information between them. Seitzer et al. [27] measures the local dependencies between action and environment entities by estimating their conditional mutual information (CMI) and using it as intrinsic reward signal to encourage the agent to maximize the influence of action over environment entities."}]}, {"page": 4, "text": "However, due to the diffi   culty in measuring the mutual information across a multi-step trajectory,\nexisting empowerment-based methods only measure 1-step empowerment, e.g., how much the agent\ndirectly influences variables. Thus they cannot detect dependencies between objects (e.g., indirect\ntool use such as using the stove to cook meals) and the environment\u2019s influence on the agent (e.g., the\nelevator can take the agent to the desired floor). Furthermore, since the objective of empowerment-\nbased methods is to maximize controllability, they tend to only control the easiest-to-manipulate\nobject to maximize its empowerment when there are multiple controllable variables.\nELDEN is closely related to Seitzer et al. [27], but differs from it in three main aspects: first, unlike\nSeitzer et al. [27], which only considers the interaction between action and environment entities,\nELDEN also considers the interaction between environment entities that are not locally dependant on\nthe action, which allows curiosity about indirect interaction between the agent and the environment\nentities to propagate through time during the RL training. Second, unlike Seitzer et al. [27], ELDEN\ntries to visit states with novel interactions, instead of maximizing controllability, thus avoiding the\ntendency to only interact with easy-to-control objects. Lastly, ELDEN estimates local dependencies\nthrough reasoning about the partial derivatives of learned dynamic models, which we empirically\nshow to be more accurate and computationally effi         cient compared to the CMI-based estimation in\nSeitzer et al. [27].\n2.3   Causality in Reinforcement Learning\nThe concept of incorporating causality in the training of a reinforcement learning agent has been\nutilized in many different forms. Wang et al. [31] demonstrates that incorporating causal dependencies\nbetween environment entities can greatly improve generalization to out-of-distribution states. Hu et al.\n[10] exploited causal dependencies between action dimensions and reward terms to reduce variance\nin the gradients and facilitate policy learning of mobile manipulation tasks. Pitis et al. [24] shows\nthat knowing the local causal dependencies between objects can facilitate policies to generalize to\nunseen states and actions. Pitis et al. [23] uses local dependencies to generate counterfactual samples\nin order to facilitate sample efficiency. Sontakke et al. [28] discovers causal factors in the dynamics\nof a given environment through a causal curiosity reward term.\nLike in Pitis et al. [23], ELDEN learns to predict the local connectivities between environment\nentities depending on the state and the action values. However, we do not generate counterfactuals\nbut create an intrinsic reward based on the local dependency that facilitates exploration with RL in\nsparse-reward setups.\n3    ELDEN: Exploration via Local Dependencies\nIn the section, we introduce ELDEN, which infers the local dependencies between environment\nentities and uses the uncertainty of dependencies as an intrinsic reward for tackling hard-exploration\nproblems. In Sec. 3.1, we formally define the problem setup of ELDEN. In Sec. 3.2, we discuss how\nELDEN uncovers local dependencies. In Sec. 3.3, we describe how ELDEN improves exploration\nwith the intrinsic reward.\n3.1   Problem Statement\nWe consider decision-making as a discrete-time Markov Decision Process (S, A, P, R), where S\nis a state space which we assume can factored as S = S1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SN, A is an action space, P is a\nMarkovian transition model, and R is a reward function. The goal of the RL agent is to optimize the\nparameters \u03b8 of a policy \u03c0\u03b8 such that the total expected return under \u03c0\u03b8 is maximized. Specifically,\nwe focus on cases where R is sparse, and therefore intelligent exploration is crucial to the discovery\nof optimal policies.\nLocal Causal Graph Model            We can model the transition at time step t as a Causal Graphical\nModel (CGM) [22] consisting of (1) nodes (St, At, St+1), (2) a directed graph G describing global\ndependencies between nodes, and (3) a conditional distribution p for each state variable at the next time\nstep, Snt+1. We assume the transition can be factorized as P(st+1|st, at) =  N        n=1 p(sn t+1|Pa(Sn t+1)),\nwhere the Pa(v) are parents of a node v in the causal graph G. For many environments, G can be\ndense or even fully connected, because whenever it is possible for Sj to depend on Si, no matter how\n                                                       4", "md": "However, due to the difficulty in measuring the mutual information across a multi-step trajectory, existing empowerment-based methods only measure 1-step empowerment, e.g., how much the agent directly influences variables. Thus they cannot detect dependencies between objects (e.g., indirect tool use such as using the stove to cook meals) and the environment\u2019s influence on the agent (e.g., the elevator can take the agent to the desired floor). Furthermore, since the objective of empowerment-based methods is to maximize controllability, they tend to only control the easiest-to-manipulate object to maximize its empowerment when there are multiple controllable variables.\n\nELDEN is closely related to Seitzer et al. [27], but differs from it in three main aspects: first, unlike Seitzer et al. [27], which only considers the interaction between action and environment entities, ELDEN also considers the interaction between environment entities that are not locally dependent on the action, which allows curiosity about indirect interaction between the agent and the environment entities to propagate through time during the RL training. Second, unlike Seitzer et al. [27], ELDEN tries to visit states with novel interactions, instead of maximizing controllability, thus avoiding the tendency to only interact with easy-to-control objects. Lastly, ELDEN estimates local dependencies through reasoning about the partial derivatives of learned dynamic models, which we empirically show to be more accurate and computationally efficient compared to the CMI-based estimation in Seitzer et al. [27].\n\n## 2.3 Causality in Reinforcement Learning\n\nThe concept of incorporating causality in the training of a reinforcement learning agent has been utilized in many different forms. Wang et al. [31] demonstrates that incorporating causal dependencies between environment entities can greatly improve generalization to out-of-distribution states. Hu et al. [10] exploited causal dependencies between action dimensions and reward terms to reduce variance in the gradients and facilitate policy learning of mobile manipulation tasks. Pitis et al. [24] shows that knowing the local causal dependencies between objects can facilitate policies to generalize to unseen states and actions. Pitis et al. [23] uses local dependencies to generate counterfactual samples in order to facilitate sample efficiency. Sontakke et al. [28] discovers causal factors in the dynamics of a given environment through a causal curiosity reward term.\n\nLike in Pitis et al. [23], ELDEN learns to predict the local connectivities between environment entities depending on the state and the action values. However, we do not generate counterfactuals but create an intrinsic reward based on the local dependency that facilitates exploration with RL in sparse-reward setups.\n\n## 3 ELDEN: Exploration via Local Dependencies\n\nIn the section, we introduce ELDEN, which infers the local dependencies between environment entities and uses the uncertainty of dependencies as an intrinsic reward for tackling hard-exploration problems. In Sec. 3.1, we formally define the problem setup of ELDEN. In Sec. 3.2, we discuss how ELDEN uncovers local dependencies. In Sec. 3.3, we describe how ELDEN improves exploration with the intrinsic reward.\n\n### 3.1 Problem Statement\n\nWe consider decision-making as a discrete-time Markov Decision Process (S, A, P, R), where S is a state space which we assume can factored as S = S1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SN, A is an action space, P is a Markovian transition model, and R is a reward function. The goal of the RL agent is to optimize the parameters \u03b8 of a policy \u03c0\u03b8 such that the total expected return under \u03c0\u03b8 is maximized. Specifically, we focus on cases where R is sparse, and therefore intelligent exploration is crucial to the discovery of optimal policies.\n\nLocal Causal Graph Model We can model the transition at time step t as a Causal Graphical Model (CGM) [22] consisting of (1) nodes (St, At, St+1), (2) a directed graph G describing global dependencies between nodes, and (3) a conditional distribution p for each state variable at the next time step, Snt+1. We assume the transition can be factorized as $$P(st+1|st, at) = \\prod_{n=1}^{N} p(s_{n}^{t+1}|Pa(S_{n}^{t+1}))$$, where the Pa(v) are parents of a node v in the causal graph G. For many environments, G can be dense or even fully connected, because whenever it is possible for Sj to depend on Si, no matter how", "images": [], "items": [{"type": "text", "value": "However, due to the difficulty in measuring the mutual information across a multi-step trajectory, existing empowerment-based methods only measure 1-step empowerment, e.g., how much the agent directly influences variables. Thus they cannot detect dependencies between objects (e.g., indirect tool use such as using the stove to cook meals) and the environment\u2019s influence on the agent (e.g., the elevator can take the agent to the desired floor). Furthermore, since the objective of empowerment-based methods is to maximize controllability, they tend to only control the easiest-to-manipulate object to maximize its empowerment when there are multiple controllable variables.\n\nELDEN is closely related to Seitzer et al. [27], but differs from it in three main aspects: first, unlike Seitzer et al. [27], which only considers the interaction between action and environment entities, ELDEN also considers the interaction between environment entities that are not locally dependent on the action, which allows curiosity about indirect interaction between the agent and the environment entities to propagate through time during the RL training. Second, unlike Seitzer et al. [27], ELDEN tries to visit states with novel interactions, instead of maximizing controllability, thus avoiding the tendency to only interact with easy-to-control objects. Lastly, ELDEN estimates local dependencies through reasoning about the partial derivatives of learned dynamic models, which we empirically show to be more accurate and computationally efficient compared to the CMI-based estimation in Seitzer et al. [27].", "md": "However, due to the difficulty in measuring the mutual information across a multi-step trajectory, existing empowerment-based methods only measure 1-step empowerment, e.g., how much the agent directly influences variables. Thus they cannot detect dependencies between objects (e.g., indirect tool use such as using the stove to cook meals) and the environment\u2019s influence on the agent (e.g., the elevator can take the agent to the desired floor). Furthermore, since the objective of empowerment-based methods is to maximize controllability, they tend to only control the easiest-to-manipulate object to maximize its empowerment when there are multiple controllable variables.\n\nELDEN is closely related to Seitzer et al. [27], but differs from it in three main aspects: first, unlike Seitzer et al. [27], which only considers the interaction between action and environment entities, ELDEN also considers the interaction between environment entities that are not locally dependent on the action, which allows curiosity about indirect interaction between the agent and the environment entities to propagate through time during the RL training. Second, unlike Seitzer et al. [27], ELDEN tries to visit states with novel interactions, instead of maximizing controllability, thus avoiding the tendency to only interact with easy-to-control objects. Lastly, ELDEN estimates local dependencies through reasoning about the partial derivatives of learned dynamic models, which we empirically show to be more accurate and computationally efficient compared to the CMI-based estimation in Seitzer et al. [27]."}, {"type": "heading", "lvl": 2, "value": "2.3 Causality in Reinforcement Learning", "md": "## 2.3 Causality in Reinforcement Learning"}, {"type": "text", "value": "The concept of incorporating causality in the training of a reinforcement learning agent has been utilized in many different forms. Wang et al. [31] demonstrates that incorporating causal dependencies between environment entities can greatly improve generalization to out-of-distribution states. Hu et al. [10] exploited causal dependencies between action dimensions and reward terms to reduce variance in the gradients and facilitate policy learning of mobile manipulation tasks. Pitis et al. [24] shows that knowing the local causal dependencies between objects can facilitate policies to generalize to unseen states and actions. Pitis et al. [23] uses local dependencies to generate counterfactual samples in order to facilitate sample efficiency. Sontakke et al. [28] discovers causal factors in the dynamics of a given environment through a causal curiosity reward term.\n\nLike in Pitis et al. [23], ELDEN learns to predict the local connectivities between environment entities depending on the state and the action values. However, we do not generate counterfactuals but create an intrinsic reward based on the local dependency that facilitates exploration with RL in sparse-reward setups.", "md": "The concept of incorporating causality in the training of a reinforcement learning agent has been utilized in many different forms. Wang et al. [31] demonstrates that incorporating causal dependencies between environment entities can greatly improve generalization to out-of-distribution states. Hu et al. [10] exploited causal dependencies between action dimensions and reward terms to reduce variance in the gradients and facilitate policy learning of mobile manipulation tasks. Pitis et al. [24] shows that knowing the local causal dependencies between objects can facilitate policies to generalize to unseen states and actions. Pitis et al. [23] uses local dependencies to generate counterfactual samples in order to facilitate sample efficiency. Sontakke et al. [28] discovers causal factors in the dynamics of a given environment through a causal curiosity reward term.\n\nLike in Pitis et al. [23], ELDEN learns to predict the local connectivities between environment entities depending on the state and the action values. However, we do not generate counterfactuals but create an intrinsic reward based on the local dependency that facilitates exploration with RL in sparse-reward setups."}, {"type": "heading", "lvl": 2, "value": "3 ELDEN: Exploration via Local Dependencies", "md": "## 3 ELDEN: Exploration via Local Dependencies"}, {"type": "text", "value": "In the section, we introduce ELDEN, which infers the local dependencies between environment entities and uses the uncertainty of dependencies as an intrinsic reward for tackling hard-exploration problems. In Sec. 3.1, we formally define the problem setup of ELDEN. In Sec. 3.2, we discuss how ELDEN uncovers local dependencies. In Sec. 3.3, we describe how ELDEN improves exploration with the intrinsic reward.", "md": "In the section, we introduce ELDEN, which infers the local dependencies between environment entities and uses the uncertainty of dependencies as an intrinsic reward for tackling hard-exploration problems. In Sec. 3.1, we formally define the problem setup of ELDEN. In Sec. 3.2, we discuss how ELDEN uncovers local dependencies. In Sec. 3.3, we describe how ELDEN improves exploration with the intrinsic reward."}, {"type": "heading", "lvl": 3, "value": "3.1 Problem Statement", "md": "### 3.1 Problem Statement"}, {"type": "text", "value": "We consider decision-making as a discrete-time Markov Decision Process (S, A, P, R), where S is a state space which we assume can factored as S = S1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SN, A is an action space, P is a Markovian transition model, and R is a reward function. The goal of the RL agent is to optimize the parameters \u03b8 of a policy \u03c0\u03b8 such that the total expected return under \u03c0\u03b8 is maximized. Specifically, we focus on cases where R is sparse, and therefore intelligent exploration is crucial to the discovery of optimal policies.\n\nLocal Causal Graph Model We can model the transition at time step t as a Causal Graphical Model (CGM) [22] consisting of (1) nodes (St, At, St+1), (2) a directed graph G describing global dependencies between nodes, and (3) a conditional distribution p for each state variable at the next time step, Snt+1. We assume the transition can be factorized as $$P(st+1|st, at) = \\prod_{n=1}^{N} p(s_{n}^{t+1}|Pa(S_{n}^{t+1}))$$, where the Pa(v) are parents of a node v in the causal graph G. For many environments, G can be dense or even fully connected, because whenever it is possible for Sj to depend on Si, no matter how", "md": "We consider decision-making as a discrete-time Markov Decision Process (S, A, P, R), where S is a state space which we assume can factored as S = S1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SN, A is an action space, P is a Markovian transition model, and R is a reward function. The goal of the RL agent is to optimize the parameters \u03b8 of a policy \u03c0\u03b8 such that the total expected return under \u03c0\u03b8 is maximized. Specifically, we focus on cases where R is sparse, and therefore intelligent exploration is crucial to the discovery of optimal policies.\n\nLocal Causal Graph Model We can model the transition at time step t as a Causal Graphical Model (CGM) [22] consisting of (1) nodes (St, At, St+1), (2) a directed graph G describing global dependencies between nodes, and (3) a conditional distribution p for each state variable at the next time step, Snt+1. We assume the transition can be factorized as $$P(st+1|st, at) = \\prod_{n=1}^{N} p(s_{n}^{t+1}|Pa(S_{n}^{t+1}))$$, where the Pa(v) are parents of a node v in the causal graph G. For many environments, G can be dense or even fully connected, because whenever it is possible for Sj to depend on Si, no matter how"}]}, {"page": 5, "text": "Algorithm 1 Training of ELDEN (on-policy)\n   1: Initialize the dynamics ensemble {f}M                             , policy \u03c0\u03b8, and replay buffer D.\n   2: for number of training iterations do\n   3:         Collect K environment transitions {st, at, rt,task, st+1}K with current policy \u03c0\u03b8\n   4:         for k = 1 . . . K do\n   5:               Gk,m = f m.compute_ G(sk                   t , ak t ) (Sec. 3.2)                              \u25b7  Compute the local dependency graph\n   6:               rkt,intrinsic = variance({Gk,m}M              m=1)\n   7:               rkt = rk   t,task + \u03b2 \u00b7 rk     t,intrinsic\n   8:         end for\n   9:         Update the policy \u03c0\u03b8 with {st, at, rt, st+1}K (Sec. 3.3)\n 10:          Add {st, at, st+1}K into the replay buffer D\n 11:          Sample a mini-batch B = {st, at, st+1}K                          buff from D\n 12:          Train the dynamics ensemble {f}M with B (Sec. 3.2)\n 13: end for\n unlikely, it is necessary to include the edge Si \u2192                                      Sj. However, in the real world, even if possible to\n interact, most entities are independent of each other most of the time. Following this observation,\nwe are interested in inferring local dependencies that are specific to (st, at), represented by a local\n causal graph Gt that is minimal by removing inactive edges in G.\n 3.2       Identifying Local Dependencies with Dynamics Partial Derivatives\n Based on the definition in Sec. 3.1, the key component of ELDEN is to accurately evaluate which\n potential dependencies between environment entities are locally active, i.e., identify the local causal\n graph Gt given (st, at). This identification requires answering a challenging question \u2014 whether\n entity i\u2019s value, Si            t = si    t is the cause of entity j to have value Sj                               t+1 = sj      t+1. ELDEN approaches\n it with the inspiration from the but-for test: the local dependency exists if Sj                                                        t+1 = sj      t+1 would not\n happen but for Si             t = si   t. In other words, we assess whether Sj                              t+1 would change if Si                  t has a different\nvalue. Notice that, since we focus on local dependencies, we only want to vary Si                                                                    t near its actual\nvalue si     t rather than trying all its possible values.\nTo this end, ELDEN utilizes partial derivatives to identify local dependencies, as they naturally\n capture the extent of change in Sj                        t+1 with respect to Si               t. Specifically, assuming the access of ground\n truth transition probability p (which we will relax later), ELDEN considers Sj                                                               t+1 = sj       t+1 locally\n depends on Si           t = si   t if                                 \u2202p(sj   t+1|st, at)           \u2265    \u03f5,                                                                (1)\n                                                                                 \u2202si  t\nwhere \u03f5 is a predefined threshold. A large partial derivative indicates that a slight change in Si                                                                     t will\n lead to a substantial change in Sj                       t+1, thus satisfying the but-for test.\nTo evaluate partial derivatives without the ground truth transition probability, ELDEN approximates\n p with a dynamics model f parameterized by a neural network \u02c6                                                    p(sj  t+1) = f(st, at) and trains f by\n maximizing the log-likelihood of \u02c6                          p(sj  t+1) (for notational simplicity, we omit the conditionals st, at\n in p in this section). Due to limited data and training errors, even when two environment entities are\n not locally dependent, there occasionally exists a large partial derivative between them. To reduce\n such false positives, ELDEN further applies regularization to suppress partial derivatives w.r.t. inputs\n that are not necessary for predicting sj                          t+1. The overall loss of dynamics training is\n                                                                                                         \u2202  \u02c6\n                                                                                                            p(sj\n                                                    Lf = \u2212         log \u02c6  p(sj  t+1) + \u03bb                      \u2202si t+1)      ,                                               (2)\n                                                                                                 i,j               t\nwhere \u03bb is the regularization coefficient.\n                                                                                        5", "md": "Algorithm 1 Training of ELDEN (on-policy)\n1. Initialize the dynamics ensemble {f}M, policy $$\\pi_{\\theta}$$, and replay buffer D.\n2. for number of training iterations do\n3. Collect K environment transitions {st, at, rt, task, st+1}K with current policy $$\\pi_{\\theta}$$\n4. for k = 1 . . . K do\n5. $$G_{k,m} = f_{m}.compute\\_G(s_{k}^{t}, a_{k}^{t})$$ (Sec. 3.2)  \u25b7 Compute the local dependency graph\n6. $$r_{k}^{t,intrinsic} = variance(\\{G_{k,m}\\}_{m=1}^{M})$$\n7. $$r_{k}^{t} = r_{k}^{t,task} + \\beta \\cdot r_{k}^{t,intrinsic}$$\n8. end for\n9. Update the policy $$\\pi_{\\theta}$$ with {st, at, rt, st+1}K (Sec. 3.3)\n10. Add {st, at, st+1}K into the replay buffer D\n11. Sample a mini-batch B = {st, at, st+1}K buff from D\n12. Train the dynamics ensemble {f}M with B (Sec. 3.2)\n13. end for\n\nUnlikely, it is necessary to include the edge $$S_{i} \\rightarrow S_{j}$$. However, in the real world, even if possible to interact, most entities are independent of each other most of the time. Following this observation, we are interested in inferring local dependencies that are specific to $$(s_{t}, a_{t})$$, represented by a local causal graph $$G_{t}$$ that is minimal by removing inactive edges in G.\n\n### 3.2 Identifying Local Dependencies with Dynamics Partial Derivatives\n\nBased on the definition in Sec. 3.1, the key component of ELDEN is to accurately evaluate which potential dependencies between environment entities are locally active, i.e., identify the local causal graph $$G_{t}$$ given $$(s_{t}, a_{t})$$. This identification requires answering a challenging question \u2014 whether entity i\u2019s value, $$S_{i}^{t} = s_{i}^{t}$$ is the cause of entity j to have value $$S_{j}^{t+1} = s_{j}^{t+1}$$. ELDEN approaches it with the inspiration from the but-for test: the local dependency exists if $$S_{j}^{t+1} = s_{j}^{t+1}$$ would not happen but for $$S_{i}^{t} = s_{i}^{t}$$. In other words, we assess whether $$S_{j}^{t+1}$$ would change if $$S_{i}^{t}$$ has a different value. Notice that, since we focus on local dependencies, we only want to vary $$S_{i}^{t}$$ near its actual value $$s_{i}^{t}$$ rather than trying all its possible values.\n\nTo this end, ELDEN utilizes partial derivatives to identify local dependencies, as they naturally capture the extent of change in $$S_{j}^{t+1}$$ with respect to $$S_{i}^{t}$$. Specifically, assuming the access of ground truth transition probability p (which we will relax later), ELDEN considers $$S_{j}^{t+1} = s_{j}^{t+1}$$ locally depends on $$S_{i}^{t} = s_{i}^{t}$$ if $$\\frac{\\partial p(s_{j}^{t+1}|s_{t}, a_{t})}{\\partial s_{i}^{t}} \\geq \\epsilon$$, where $$\\epsilon$$ is a predefined threshold. A large partial derivative indicates that a slight change in $$S_{i}^{t}$$ will lead to a substantial change in $$S_{j}^{t+1}$$, thus satisfying the but-for test.\n\nTo evaluate partial derivatives without the ground truth transition probability, ELDEN approximates p with a dynamics model f parameterized by a neural network $$\\hat{p}(s_{j}^{t+1}) = f(s_{t}, a_{t})$$ and trains f by maximizing the log-likelihood of $$\\hat{p}(s_{j}^{t+1})$$. Due to limited data and training errors, even when two environment entities are not locally dependent, there occasionally exists a large partial derivative between them. To reduce such false positives, ELDEN further applies regularization to suppress partial derivatives w.r.t. inputs that are not necessary for predicting $$s_{j}^{t+1}$$. The overall loss of dynamics training is $$L_{f} = -\\sum_{i,j} \\log \\hat{p}(s_{j}^{t+1}) + \\lambda \\frac{\\partial \\hat{p}(s_{j}^{t+1})}{\\partial s_{i}^{t+1}}$$, where $$\\lambda$$ is the regularization coefficient.", "images": [], "items": [{"type": "text", "value": "Algorithm 1 Training of ELDEN (on-policy)\n1. Initialize the dynamics ensemble {f}M, policy $$\\pi_{\\theta}$$, and replay buffer D.\n2. for number of training iterations do\n3. Collect K environment transitions {st, at, rt, task, st+1}K with current policy $$\\pi_{\\theta}$$\n4. for k = 1 . . . K do\n5. $$G_{k,m} = f_{m}.compute\\_G(s_{k}^{t}, a_{k}^{t})$$ (Sec. 3.2)  \u25b7 Compute the local dependency graph\n6. $$r_{k}^{t,intrinsic} = variance(\\{G_{k,m}\\}_{m=1}^{M})$$\n7. $$r_{k}^{t} = r_{k}^{t,task} + \\beta \\cdot r_{k}^{t,intrinsic}$$\n8. end for\n9. Update the policy $$\\pi_{\\theta}$$ with {st, at, rt, st+1}K (Sec. 3.3)\n10. Add {st, at, st+1}K into the replay buffer D\n11. Sample a mini-batch B = {st, at, st+1}K buff from D\n12. Train the dynamics ensemble {f}M with B (Sec. 3.2)\n13. end for\n\nUnlikely, it is necessary to include the edge $$S_{i} \\rightarrow S_{j}$$. However, in the real world, even if possible to interact, most entities are independent of each other most of the time. Following this observation, we are interested in inferring local dependencies that are specific to $$(s_{t}, a_{t})$$, represented by a local causal graph $$G_{t}$$ that is minimal by removing inactive edges in G.", "md": "Algorithm 1 Training of ELDEN (on-policy)\n1. Initialize the dynamics ensemble {f}M, policy $$\\pi_{\\theta}$$, and replay buffer D.\n2. for number of training iterations do\n3. Collect K environment transitions {st, at, rt, task, st+1}K with current policy $$\\pi_{\\theta}$$\n4. for k = 1 . . . K do\n5. $$G_{k,m} = f_{m}.compute\\_G(s_{k}^{t}, a_{k}^{t})$$ (Sec. 3.2)  \u25b7 Compute the local dependency graph\n6. $$r_{k}^{t,intrinsic} = variance(\\{G_{k,m}\\}_{m=1}^{M})$$\n7. $$r_{k}^{t} = r_{k}^{t,task} + \\beta \\cdot r_{k}^{t,intrinsic}$$\n8. end for\n9. Update the policy $$\\pi_{\\theta}$$ with {st, at, rt, st+1}K (Sec. 3.3)\n10. Add {st, at, st+1}K into the replay buffer D\n11. Sample a mini-batch B = {st, at, st+1}K buff from D\n12. Train the dynamics ensemble {f}M with B (Sec. 3.2)\n13. end for\n\nUnlikely, it is necessary to include the edge $$S_{i} \\rightarrow S_{j}$$. However, in the real world, even if possible to interact, most entities are independent of each other most of the time. Following this observation, we are interested in inferring local dependencies that are specific to $$(s_{t}, a_{t})$$, represented by a local causal graph $$G_{t}$$ that is minimal by removing inactive edges in G."}, {"type": "heading", "lvl": 3, "value": "3.2 Identifying Local Dependencies with Dynamics Partial Derivatives", "md": "### 3.2 Identifying Local Dependencies with Dynamics Partial Derivatives"}, {"type": "text", "value": "Based on the definition in Sec. 3.1, the key component of ELDEN is to accurately evaluate which potential dependencies between environment entities are locally active, i.e., identify the local causal graph $$G_{t}$$ given $$(s_{t}, a_{t})$$. This identification requires answering a challenging question \u2014 whether entity i\u2019s value, $$S_{i}^{t} = s_{i}^{t}$$ is the cause of entity j to have value $$S_{j}^{t+1} = s_{j}^{t+1}$$. ELDEN approaches it with the inspiration from the but-for test: the local dependency exists if $$S_{j}^{t+1} = s_{j}^{t+1}$$ would not happen but for $$S_{i}^{t} = s_{i}^{t}$$. In other words, we assess whether $$S_{j}^{t+1}$$ would change if $$S_{i}^{t}$$ has a different value. Notice that, since we focus on local dependencies, we only want to vary $$S_{i}^{t}$$ near its actual value $$s_{i}^{t}$$ rather than trying all its possible values.\n\nTo this end, ELDEN utilizes partial derivatives to identify local dependencies, as they naturally capture the extent of change in $$S_{j}^{t+1}$$ with respect to $$S_{i}^{t}$$. Specifically, assuming the access of ground truth transition probability p (which we will relax later), ELDEN considers $$S_{j}^{t+1} = s_{j}^{t+1}$$ locally depends on $$S_{i}^{t} = s_{i}^{t}$$ if $$\\frac{\\partial p(s_{j}^{t+1}|s_{t}, a_{t})}{\\partial s_{i}^{t}} \\geq \\epsilon$$, where $$\\epsilon$$ is a predefined threshold. A large partial derivative indicates that a slight change in $$S_{i}^{t}$$ will lead to a substantial change in $$S_{j}^{t+1}$$, thus satisfying the but-for test.\n\nTo evaluate partial derivatives without the ground truth transition probability, ELDEN approximates p with a dynamics model f parameterized by a neural network $$\\hat{p}(s_{j}^{t+1}) = f(s_{t}, a_{t})$$ and trains f by maximizing the log-likelihood of $$\\hat{p}(s_{j}^{t+1})$$. Due to limited data and training errors, even when two environment entities are not locally dependent, there occasionally exists a large partial derivative between them. To reduce such false positives, ELDEN further applies regularization to suppress partial derivatives w.r.t. inputs that are not necessary for predicting $$s_{j}^{t+1}$$. The overall loss of dynamics training is $$L_{f} = -\\sum_{i,j} \\log \\hat{p}(s_{j}^{t+1}) + \\lambda \\frac{\\partial \\hat{p}(s_{j}^{t+1})}{\\partial s_{i}^{t+1}}$$, where $$\\lambda$$ is the regularization coefficient.", "md": "Based on the definition in Sec. 3.1, the key component of ELDEN is to accurately evaluate which potential dependencies between environment entities are locally active, i.e., identify the local causal graph $$G_{t}$$ given $$(s_{t}, a_{t})$$. This identification requires answering a challenging question \u2014 whether entity i\u2019s value, $$S_{i}^{t} = s_{i}^{t}$$ is the cause of entity j to have value $$S_{j}^{t+1} = s_{j}^{t+1}$$. ELDEN approaches it with the inspiration from the but-for test: the local dependency exists if $$S_{j}^{t+1} = s_{j}^{t+1}$$ would not happen but for $$S_{i}^{t} = s_{i}^{t}$$. In other words, we assess whether $$S_{j}^{t+1}$$ would change if $$S_{i}^{t}$$ has a different value. Notice that, since we focus on local dependencies, we only want to vary $$S_{i}^{t}$$ near its actual value $$s_{i}^{t}$$ rather than trying all its possible values.\n\nTo this end, ELDEN utilizes partial derivatives to identify local dependencies, as they naturally capture the extent of change in $$S_{j}^{t+1}$$ with respect to $$S_{i}^{t}$$. Specifically, assuming the access of ground truth transition probability p (which we will relax later), ELDEN considers $$S_{j}^{t+1} = s_{j}^{t+1}$$ locally depends on $$S_{i}^{t} = s_{i}^{t}$$ if $$\\frac{\\partial p(s_{j}^{t+1}|s_{t}, a_{t})}{\\partial s_{i}^{t}} \\geq \\epsilon$$, where $$\\epsilon$$ is a predefined threshold. A large partial derivative indicates that a slight change in $$S_{i}^{t}$$ will lead to a substantial change in $$S_{j}^{t+1}$$, thus satisfying the but-for test.\n\nTo evaluate partial derivatives without the ground truth transition probability, ELDEN approximates p with a dynamics model f parameterized by a neural network $$\\hat{p}(s_{j}^{t+1}) = f(s_{t}, a_{t})$$ and trains f by maximizing the log-likelihood of $$\\hat{p}(s_{j}^{t+1})$$. Due to limited data and training errors, even when two environment entities are not locally dependent, there occasionally exists a large partial derivative between them. To reduce such false positives, ELDEN further applies regularization to suppress partial derivatives w.r.t. inputs that are not necessary for predicting $$s_{j}^{t+1}$$. The overall loss of dynamics training is $$L_{f} = -\\sum_{i,j} \\log \\hat{p}(s_{j}^{t+1}) + \\lambda \\frac{\\partial \\hat{p}(s_{j}^{t+1})}{\\partial s_{i}^{t+1}}$$, where $$\\lambda$$ is the regularization coefficient."}]}, {"page": 6, "text": "                                                                                                     butter\n                                                                                                 meatball\n                                                                                                         Initial State  Goal State\n           (a) Thawing                    (b) CarWash                 (c) Minecraft 2D                    (d) Kitchen\nFigure 2: We test ELDEN on three domains and four environments. (a) (b) Mini-behavior and\n(c) Minecraft 2D with discrete state spaces, where the agent has to achieve a series of temporally\nextended tasks with complex object interactions. (d) Robosuite, a robot table-top manipulation\nsimulation environment with continuous state spaces, where the robot needs to perform multiple\ninterdependent subtasks to finish the cooking task.\n3.3     ELDEN Policy Learning\nELDEN utilizes the local dependency identification described in Sec. 3.2 to improve exploration\nfor model-free RL. The key idea behind ELDEN is to encourage an agent to visit states where new\nlocal dependencies are likely to emerge. When the ground truth local dependencies are available,\nthe novelty of local dependencies between state variables can be measured by the magnitude of\nerror of the dependencies identified by our method. Unfortunately, in many cases, it is hard to\nmanually specify the ground truth local dependencies. Instead, ELDEN trains an ensemble of\ndynamics models, and measures dependency novelty by the variance of the local dependency graphs\nextracted independently from each of the dynamics models in the ensemble. Specifically, ELDEN\nfirst computes the variance of each edge in the graph and then uses the mean of the edge variance as\nthe graph variance. Finally, the calculated variance is used as the intrinsic reward and is scaled with a\ncoefficient \u03b2 that controls the magnitude of the exploration bonus, and added to the task reward rtask.\nELDEN is applicable to both on-policy and off-policy settings. We show the pseudo-code for on-\npolicy ELDEN in Algorithm 1. The off-policy version of ELDEN can be easily derived by updating\nthe policy with transitions sampled from the replay buffer in line 9 of Algorithm 1. Importantly, any\nmodel-free RL algorithm can be used for the policy update step.\n4     Experiments\nIn our experiments, we aim to answer two main questions: Q1: Is ELDEN able to accurately detect\nlocal dependencies between environment entities in factored state spaces (Sec. 4.1)? Q2: Does\nELDEN improve the performance of RL algorithms in sparse-reward environments with chained\ndependencies (Sec. 4.2)?\nEnvironments            As shown in Fig. 2, we evaluate ELDEN in four simulated environments with\ndifferent objects that have complex and chained dependencies: (1) CARWASH, (2) THAWING, (4) 2D\nMINECRAFT and (3) KITCHEN. Both CARWASH and THAWING are long-horizon household tasks in\ndiscrete gridworld from the Mini-BEHAVIOR Benchmark [12]. MINECRAFT 2D is an environment\nmodified from the one used by Andreas et al. [1], where the agent needs to master a complex\ntechnology tree to finish the task. KITCHEN is a continuous robot table-top manipulation domain\nimplemented in RoboSuite [36]. To complete tasks in these environments and receive the sparse\nreward, the agent has to conduct a series of actions that change not only the state of the interacted\nentities but also induce further interaction between interacted entities and others (e.g., interacting with\nthe stove switch that enact interaction between the stove and the cooking the meatball). The agents in\nall environments can select between a set of action primitives, a set of discrete actions that can be\napplied on each object, e.g., goTo(obj) or pick(obj). Notice that even with the action primitives,\nthese domains are still very hard to solve due to the presence of many interaction modes and the\ndifficulty in finding the correct (potentially long) sequence of interactions among many options that\nwill lead to task success. We provide further descriptions of each environment in Appendix Sec B.\n                                                                 6", "md": "# Document\n\nbutter\n\nmeatball\n\nInitial State &emsp; Goal State\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n(a) \\text{ Thawing} & (b) \\text{ CarWash} \\\\\n(c) \\text{ Minecraft 2D} & (d) \\text{ Kitchen} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 2: We test ELDEN on three domains and four environments. (a) (b) Mini-behavior and (c) Minecraft 2D with discrete state spaces, where the agent has to achieve a series of temporally extended tasks with complex object interactions. (d) Robosuite, a robot table-top manipulation simulation environment with continuous state spaces, where the robot needs to perform multiple interdependent subtasks to finish the cooking task.\n\n### 3.3 ELDEN Policy Learning\n\nELDEN utilizes the local dependency identification described in Sec. 3.2 to improve exploration for model-free RL. The key idea behind ELDEN is to encourage an agent to visit states where new local dependencies are likely to emerge. When the ground truth local dependencies are available, the novelty of local dependencies between state variables can be measured by the magnitude of error of the dependencies identified by our method. Unfortunately, in many cases, it is hard to manually specify the ground truth local dependencies. Instead, ELDEN trains an ensemble of dynamics models, and measures dependency novelty by the variance of the local dependency graphs extracted independently from each of the dynamics models in the ensemble. Specifically, ELDEN first computes the variance of each edge in the graph and then uses the mean of the edge variance as the graph variance. Finally, the calculated variance is used as the intrinsic reward and is scaled with a coefficient \u03b2 that controls the magnitude of the exploration bonus, and added to the task reward rtask. ELDEN is applicable to both on-policy and off-policy settings. We show the pseudo-code for on-policy ELDEN in Algorithm 1. The off-policy version of ELDEN can be easily derived by updating the policy with transitions sampled from the replay buffer in line 9 of Algorithm 1. Importantly, any model-free RL algorithm can be used for the policy update step.\n\n### 4 Experiments\n\nIn our experiments, we aim to answer two main questions: Q1: Is ELDEN able to accurately detect local dependencies between environment entities in factored state spaces (Sec. 4.1)? Q2: Does ELDEN improve the performance of RL algorithms in sparse-reward environments with chained dependencies (Sec. 4.2)?\n\nEnvironments As shown in Fig. 2, we evaluate ELDEN in four simulated environments with different objects that have complex and chained dependencies: (1) CARWASH, (2) THAWING, (4) 2D MINECRAFT and (3) KITCHEN. Both CARWASH and THAWING are long-horizon household tasks in discrete gridworld from the Mini-BEHAVIOR Benchmark [12]. MINECRAFT 2D is an environment modified from the one used by Andreas et al. [1], where the agent needs to master a complex technology tree to finish the task. KITCHEN is a continuous robot table-top manipulation domain implemented in RoboSuite [36]. To complete tasks in these environments and receive the sparse reward, the agent has to conduct a series of actions that change not only the state of the interacted entities but also induce further interaction between interacted entities and others (e.g., interacting with the stove switch that enact interaction between the stove and the cooking the meatball). The agents in all environments can select between a set of action primitives, a set of discrete actions that can be applied on each object, e.g., goTo(obj) or pick(obj). Notice that even with the action primitives, these domains are still very hard to solve due to the presence of many interaction modes and the difficulty in finding the correct (potentially long) sequence of interactions among many options that will lead to task success. We provide further descriptions of each environment in Appendix Sec B.", "images": [{"name": "page-6-2.jpg", "height": 27, "width": 27, "x": 303, "y": 73}, {"name": "page-6-1.jpg", "height": 87, "width": 87, "x": 209, "y": 73}, {"name": "page-6-3.jpg", "height": 40, "width": 27, "x": 303, "y": 112}, {"name": "page-6-0.jpg", "height": 87, "width": 87, "x": 116, "y": 73}, {"name": "page-6-8.jpg", "height": 92, "width": 92, "x": 490, "y": 72}, {"name": "page-6-7.jpg", "height": 92, "width": 92, "x": 397, "y": 72}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "butter\n\nmeatball\n\nInitial State &emsp; Goal State\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n(a) \\text{ Thawing} & (b) \\text{ CarWash} \\\\\n(c) \\text{ Minecraft 2D} & (d) \\text{ Kitchen} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 2: We test ELDEN on three domains and four environments. (a) (b) Mini-behavior and (c) Minecraft 2D with discrete state spaces, where the agent has to achieve a series of temporally extended tasks with complex object interactions. (d) Robosuite, a robot table-top manipulation simulation environment with continuous state spaces, where the robot needs to perform multiple interdependent subtasks to finish the cooking task.", "md": "butter\n\nmeatball\n\nInitial State &emsp; Goal State\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n(a) \\text{ Thawing} & (b) \\text{ CarWash} \\\\\n(c) \\text{ Minecraft 2D} & (d) \\text{ Kitchen} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 2: We test ELDEN on three domains and four environments. (a) (b) Mini-behavior and (c) Minecraft 2D with discrete state spaces, where the agent has to achieve a series of temporally extended tasks with complex object interactions. (d) Robosuite, a robot table-top manipulation simulation environment with continuous state spaces, where the robot needs to perform multiple interdependent subtasks to finish the cooking task."}, {"type": "heading", "lvl": 3, "value": "3.3 ELDEN Policy Learning", "md": "### 3.3 ELDEN Policy Learning"}, {"type": "text", "value": "ELDEN utilizes the local dependency identification described in Sec. 3.2 to improve exploration for model-free RL. The key idea behind ELDEN is to encourage an agent to visit states where new local dependencies are likely to emerge. When the ground truth local dependencies are available, the novelty of local dependencies between state variables can be measured by the magnitude of error of the dependencies identified by our method. Unfortunately, in many cases, it is hard to manually specify the ground truth local dependencies. Instead, ELDEN trains an ensemble of dynamics models, and measures dependency novelty by the variance of the local dependency graphs extracted independently from each of the dynamics models in the ensemble. Specifically, ELDEN first computes the variance of each edge in the graph and then uses the mean of the edge variance as the graph variance. Finally, the calculated variance is used as the intrinsic reward and is scaled with a coefficient \u03b2 that controls the magnitude of the exploration bonus, and added to the task reward rtask. ELDEN is applicable to both on-policy and off-policy settings. We show the pseudo-code for on-policy ELDEN in Algorithm 1. The off-policy version of ELDEN can be easily derived by updating the policy with transitions sampled from the replay buffer in line 9 of Algorithm 1. Importantly, any model-free RL algorithm can be used for the policy update step.", "md": "ELDEN utilizes the local dependency identification described in Sec. 3.2 to improve exploration for model-free RL. The key idea behind ELDEN is to encourage an agent to visit states where new local dependencies are likely to emerge. When the ground truth local dependencies are available, the novelty of local dependencies between state variables can be measured by the magnitude of error of the dependencies identified by our method. Unfortunately, in many cases, it is hard to manually specify the ground truth local dependencies. Instead, ELDEN trains an ensemble of dynamics models, and measures dependency novelty by the variance of the local dependency graphs extracted independently from each of the dynamics models in the ensemble. Specifically, ELDEN first computes the variance of each edge in the graph and then uses the mean of the edge variance as the graph variance. Finally, the calculated variance is used as the intrinsic reward and is scaled with a coefficient \u03b2 that controls the magnitude of the exploration bonus, and added to the task reward rtask. ELDEN is applicable to both on-policy and off-policy settings. We show the pseudo-code for on-policy ELDEN in Algorithm 1. The off-policy version of ELDEN can be easily derived by updating the policy with transitions sampled from the replay buffer in line 9 of Algorithm 1. Importantly, any model-free RL algorithm can be used for the policy update step."}, {"type": "heading", "lvl": 3, "value": "4 Experiments", "md": "### 4 Experiments"}, {"type": "text", "value": "In our experiments, we aim to answer two main questions: Q1: Is ELDEN able to accurately detect local dependencies between environment entities in factored state spaces (Sec. 4.1)? Q2: Does ELDEN improve the performance of RL algorithms in sparse-reward environments with chained dependencies (Sec. 4.2)?\n\nEnvironments As shown in Fig. 2, we evaluate ELDEN in four simulated environments with different objects that have complex and chained dependencies: (1) CARWASH, (2) THAWING, (4) 2D MINECRAFT and (3) KITCHEN. Both CARWASH and THAWING are long-horizon household tasks in discrete gridworld from the Mini-BEHAVIOR Benchmark [12]. MINECRAFT 2D is an environment modified from the one used by Andreas et al. [1], where the agent needs to master a complex technology tree to finish the task. KITCHEN is a continuous robot table-top manipulation domain implemented in RoboSuite [36]. To complete tasks in these environments and receive the sparse reward, the agent has to conduct a series of actions that change not only the state of the interacted entities but also induce further interaction between interacted entities and others (e.g., interacting with the stove switch that enact interaction between the stove and the cooking the meatball). The agents in all environments can select between a set of action primitives, a set of discrete actions that can be applied on each object, e.g., goTo(obj) or pick(obj). Notice that even with the action primitives, these domains are still very hard to solve due to the presence of many interaction modes and the difficulty in finding the correct (potentially long) sequence of interactions among many options that will lead to task success. We provide further descriptions of each environment in Appendix Sec B.", "md": "In our experiments, we aim to answer two main questions: Q1: Is ELDEN able to accurately detect local dependencies between environment entities in factored state spaces (Sec. 4.1)? Q2: Does ELDEN improve the performance of RL algorithms in sparse-reward environments with chained dependencies (Sec. 4.2)?\n\nEnvironments As shown in Fig. 2, we evaluate ELDEN in four simulated environments with different objects that have complex and chained dependencies: (1) CARWASH, (2) THAWING, (4) 2D MINECRAFT and (3) KITCHEN. Both CARWASH and THAWING are long-horizon household tasks in discrete gridworld from the Mini-BEHAVIOR Benchmark [12]. MINECRAFT 2D is an environment modified from the one used by Andreas et al. [1], where the agent needs to master a complex technology tree to finish the task. KITCHEN is a continuous robot table-top manipulation domain implemented in RoboSuite [36]. To complete tasks in these environments and receive the sparse reward, the agent has to conduct a series of actions that change not only the state of the interacted entities but also induce further interaction between interacted entities and others (e.g., interacting with the stove switch that enact interaction between the stove and the cooking the meatball). The agents in all environments can select between a set of action primitives, a set of discrete actions that can be applied on each object, e.g., goTo(obj) or pick(obj). Notice that even with the action primitives, these domains are still very hard to solve due to the presence of many interaction modes and the difficulty in finding the correct (potentially long) sequence of interactions among many options that will lead to task success. We provide further descriptions of each environment in Appendix Sec B."}]}, {"page": 7, "text": "         Table 1: Mean \u00b1 std. error of ROC AUC (\u2191) and F1 (\u2191) of local dependency prediction\n                                THAWING                             CARWASH                               KITCHEN\n                       ROC AUC                F1           ROC AUC                 F1           ROC AUC                F1\n      ELDEN           0.71 \u00b1 0.01       0.57 \u00b1 0.00        0.78 \u00b1 0.02       0.66 \u00b1 0.02       0.66 \u00b1 0.01        0.25 \u00b1 0.01\n        pCMI          0.55 \u00b1 0.01       0.60 \u00b1 0.00        0.73 \u00b1 0.02       0.78 \u00b1 0.01       0.60 \u00b1 0.00        0.28 \u00b1 0.00\n        Attn          0.65 \u00b1 0.04       0.63 \u00b1 0.01        0.66 \u00b1 0.01       0.55 \u00b1 0.03       0.51 \u00b1 0.01        0.22 \u00b1 0.02\n    Input Mask        0.50 \u00b1 0.00       0.40 \u00b1 0.00        0.50 \u00b1 0.00       0.32 \u00b1 0.01       0.50 \u00b1 0.00        0.08 \u00b1 0.00\n     Attn Mask        0.45 \u00b1 0.03       0.47 \u00b1 0.02        0.47 \u00b1 0.07       0.43 \u00b1 0.03       0.52 \u00b1 0.01        0.13 \u00b1 0.01\n Implementation Details               For discrete state or action spaces, the partial derivatives w.r.t. st/at\n are undefined. To address this issue, we use Mixup [34] to create synthetic inputs and labels by\n linearly combining pairs of inputs and labels, thus approximately changing the input space to be\n continuous. Compared to learning from discrete inputs only, dynamics models trained on such\n data generate partial derivatives that better reflect local dependencies, as shown in Sec 4.3.1. For\n the 2D MINECRAFT and KITCHEN environments where some local dependencies have complex\n preconditions and thus are hard to induce, we apply sample prioritization to dynamics learning, where\n the priority is measured as prediction error. In this way, the dynamics model gets aware of unknown\n interactions faster and guides the exploration more efficiently than not using prioritization. Further\n details are provided in the Appendix.\n 4.1    Evaluating the Detection of Local Dependencies\nWe compare the local dependencies extracted by ELDEN with the following baselines (see imple-\n mentation details in Appendix Sec. C):\n                                                                                                              p(sjt+1|st,at)\n \u2022 pCMI (point-wise conditional mutual information) [27, 31]: defined as log                                 p(sj         t,at). It\n   quantifies how likely it is that sj       t+1 depends on si      t.                                          t+1|st\\si\n \u2022 Attn (attention): Use the score between each entity pair computed by the attention modules inside\n   the dynamics model to quantify local dependencies.\n \u2022 Input Mask: we implement a learnable binary mask to the dynamics model that can zero out some\n   inputs conditioned on (st, at): f([st, at] \u2299              M(st, at)). During training, the mask is regularized to\n   use as few inputs as possible with L1 regularization, leading to a quantification of minimal local\n   dependencies.\n \u2022 Attn Mask: we implement a learnable mask to the dynamics model similar to the one in Input\n   Mask, but in this case, the mask is applied to the attention scores. The mask is regularized following\n   the method by Weiss et al. [32].\nWe train the dynamics model of each method with three random seeds on pre-collected transition data\n and evaluate their performance by predicting the local causal graph Gt for 50 unseen episodes based on\n the state-action pair (st, at). We compare their predictions with the ground truth local dependencies\n extracted from the simulator. In the three environments, many potential local dependencies are\n inactive most of the time, and thus only a small portion (\u2264                 3%) of the ground truth labels indicate the\n existence of local dependencies for a given entity pair. To account for such imbalance, we use the\n area under the receiver operating characteristic curve (ROC-AUC) and the best achievable F-score\n(F1) as evaluation metrics.\nThe results of the evaluation on the detection of local dependencies are summarized in Table 1.\n ELDEN outperforms all baselines in terms of ROC-AUC consistently across all environments\n(Q1). For the F1 score, pCMI performs best in most environments (especially in the more complex\n CARWASH and KITCHEN), but ELDEN performs comparably or achieves the second-best F1 scores\nwith much less computation: pCMI computation cost is N times higher than ELDEN, where N is the\n number of environment entities, and thus pCMI scales badly to environments with a large number of\n objects. Further evaluation details can be found in the Appendix.\n                                                                  7", "md": "|THAWING|ROC AUC|F1|ROC AUC|F1|ROC AUC|F1|\n|---|---|---|---|---|---|---|\n|ELDEN|$0.71 \\pm 0.01$|$0.57 \\pm 0.00$|$0.78 \\pm 0.02$|$0.66 \\pm 0.02$|$0.66 \\pm 0.01$|$0.25 \\pm 0.01$|\n|pCMI|$0.55 \\pm 0.01$|$0.60 \\pm 0.00$|$0.73 \\pm 0.02$|$0.78 \\pm 0.01$|$0.60 \\pm 0.00$|$0.28 \\pm 0.00$|\n|Attn|$0.65 \\pm 0.04$|$0.63 \\pm 0.01$|$0.66 \\pm 0.01$|$0.55 \\pm 0.03$|$0.51 \\pm 0.01$|$0.22 \\pm 0.02$|\n|Input Mask|$0.50 \\pm 0.00$|$0.40 \\pm 0.00$|$0.50 \\pm 0.00$|$0.32 \\pm 0.01$|$0.50 \\pm 0.00$|$0.08 \\pm 0.00$|\n|Attn Mask|$0.45 \\pm 0.03$|$0.47 \\pm 0.02$|$0.47 \\pm 0.07$|$0.43 \\pm 0.03$|$0.52 \\pm 0.01$|$0.13 \\pm 0.01$|\n\nImplementation Details: For discrete state or action spaces, the partial derivatives w.r.t. st/at are undefined. To address this issue, we use Mixup [34] to create synthetic inputs and labels by linearly combining pairs of inputs and labels, thus approximately changing the input space to be continuous. Compared to learning from discrete inputs only, dynamics models trained on such data generate partial derivatives that better reflect local dependencies, as shown in Sec 4.3.1. For the 2D MINECRAFT and KITCHEN environments where some local dependencies have complex preconditions and thus are hard to induce, we apply sample prioritization to dynamics learning, where the priority is measured as prediction error. In this way, the dynamics model gets aware of unknown interactions faster and guides the exploration more efficiently than not using prioritization. Further details are provided in the Appendix.\n\n### 4.1 Evaluating the Detection of Local Dependencies\n\nWe compare the local dependencies extracted by ELDEN with the following baselines (see implementation details in Appendix Sec. C):\n\n- pCMI (point-wise conditional mutual information) [27, 31]: defined as $\\log \\frac{p(s_{j,t+1}|s_{t},a_{t})}{p(s_{j,t+1}|s_{t}\\backslash s_{i})}$. It quantifies how likely it is that $s_{j,t+1}$ depends on $s_{i,t}$.\n- Attn (attention): Use the score between each entity pair computed by the attention modules inside the dynamics model to quantify local dependencies.\n- Input Mask: we implement a learnable binary mask to the dynamics model that can zero out some inputs conditioned on $(s_{t}, a_{t}): f([s_{t}, a_{t}] \\odot M(s_{t}, a_{t})).$ During training, the mask is regularized to use as few inputs as possible with L1 regularization, leading to a quantification of minimal local dependencies.\n- Attn Mask: we implement a learnable mask to the dynamics model similar to the one in Input Mask, but in this case, the mask is applied to the attention scores. The mask is regularized following the method by Weiss et al. [32].\n\nWe train the dynamics model of each method with three random seeds on pre-collected transition data and evaluate their performance by predicting the local causal graph $$G_{t}$$ for 50 unseen episodes based on the state-action pair $$(s_{t}, a_{t}).$$ We compare their predictions with the ground truth local dependencies extracted from the simulator. In the three environments, many potential local dependencies are inactive most of the time, and thus only a small portion ($$\\leq 3\\%$$) of the ground truth labels indicate the existence of local dependencies for a given entity pair. To account for such imbalance, we use the area under the receiver operating characteristic curve (ROC-AUC) and the best achievable F-score (F1) as evaluation metrics.\n\nThe results of the evaluation on the detection of local dependencies are summarized in Table 1. ELDEN outperforms all baselines in terms of ROC-AUC consistently across all environments (Q1). For the F1 score, pCMI performs best in most environments (especially in the more complex CARWASH and KITCHEN), but ELDEN performs comparably or achieves the second-best F1 scores with much less computation: pCMI computation cost is N times higher than ELDEN, where N is the number of environment entities, and thus pCMI scales badly to environments with a large number of objects. Further evaluation details can be found in the Appendix.", "images": [], "items": [{"type": "table", "rows": [["THAWING", "ROC AUC", "F1", "ROC AUC", "F1", "ROC AUC", "F1"], ["ELDEN", "$0.71 \\pm 0.01$", "$0.57 \\pm 0.00$", "$0.78 \\pm 0.02$", "$0.66 \\pm 0.02$", "$0.66 \\pm 0.01$", "$0.25 \\pm 0.01$"], ["pCMI", "$0.55 \\pm 0.01$", "$0.60 \\pm 0.00$", "$0.73 \\pm 0.02$", "$0.78 \\pm 0.01$", "$0.60 \\pm 0.00$", "$0.28 \\pm 0.00$"], ["Attn", "$0.65 \\pm 0.04$", "$0.63 \\pm 0.01$", "$0.66 \\pm 0.01$", "$0.55 \\pm 0.03$", "$0.51 \\pm 0.01$", "$0.22 \\pm 0.02$"], ["Input Mask", "$0.50 \\pm 0.00$", "$0.40 \\pm 0.00$", "$0.50 \\pm 0.00$", "$0.32 \\pm 0.01$", "$0.50 \\pm 0.00$", "$0.08 \\pm 0.00$"], ["Attn Mask", "$0.45 \\pm 0.03$", "$0.47 \\pm 0.02$", "$0.47 \\pm 0.07$", "$0.43 \\pm 0.03$", "$0.52 \\pm 0.01$", "$0.13 \\pm 0.01$"]], "md": "|THAWING|ROC AUC|F1|ROC AUC|F1|ROC AUC|F1|\n|---|---|---|---|---|---|---|\n|ELDEN|$0.71 \\pm 0.01$|$0.57 \\pm 0.00$|$0.78 \\pm 0.02$|$0.66 \\pm 0.02$|$0.66 \\pm 0.01$|$0.25 \\pm 0.01$|\n|pCMI|$0.55 \\pm 0.01$|$0.60 \\pm 0.00$|$0.73 \\pm 0.02$|$0.78 \\pm 0.01$|$0.60 \\pm 0.00$|$0.28 \\pm 0.00$|\n|Attn|$0.65 \\pm 0.04$|$0.63 \\pm 0.01$|$0.66 \\pm 0.01$|$0.55 \\pm 0.03$|$0.51 \\pm 0.01$|$0.22 \\pm 0.02$|\n|Input Mask|$0.50 \\pm 0.00$|$0.40 \\pm 0.00$|$0.50 \\pm 0.00$|$0.32 \\pm 0.01$|$0.50 \\pm 0.00$|$0.08 \\pm 0.00$|\n|Attn Mask|$0.45 \\pm 0.03$|$0.47 \\pm 0.02$|$0.47 \\pm 0.07$|$0.43 \\pm 0.03$|$0.52 \\pm 0.01$|$0.13 \\pm 0.01$|", "isPerfectTable": true, "csv": "\"THAWING\",\"ROC AUC\",\"F1\",\"ROC AUC\",\"F1\",\"ROC AUC\",\"F1\"\n\"ELDEN\",\"$0.71 \\pm 0.01$\",\"$0.57 \\pm 0.00$\",\"$0.78 \\pm 0.02$\",\"$0.66 \\pm 0.02$\",\"$0.66 \\pm 0.01$\",\"$0.25 \\pm 0.01$\"\n\"pCMI\",\"$0.55 \\pm 0.01$\",\"$0.60 \\pm 0.00$\",\"$0.73 \\pm 0.02$\",\"$0.78 \\pm 0.01$\",\"$0.60 \\pm 0.00$\",\"$0.28 \\pm 0.00$\"\n\"Attn\",\"$0.65 \\pm 0.04$\",\"$0.63 \\pm 0.01$\",\"$0.66 \\pm 0.01$\",\"$0.55 \\pm 0.03$\",\"$0.51 \\pm 0.01$\",\"$0.22 \\pm 0.02$\"\n\"Input Mask\",\"$0.50 \\pm 0.00$\",\"$0.40 \\pm 0.00$\",\"$0.50 \\pm 0.00$\",\"$0.32 \\pm 0.01$\",\"$0.50 \\pm 0.00$\",\"$0.08 \\pm 0.00$\"\n\"Attn Mask\",\"$0.45 \\pm 0.03$\",\"$0.47 \\pm 0.02$\",\"$0.47 \\pm 0.07$\",\"$0.43 \\pm 0.03$\",\"$0.52 \\pm 0.01$\",\"$0.13 \\pm 0.01$\""}, {"type": "text", "value": "Implementation Details: For discrete state or action spaces, the partial derivatives w.r.t. st/at are undefined. To address this issue, we use Mixup [34] to create synthetic inputs and labels by linearly combining pairs of inputs and labels, thus approximately changing the input space to be continuous. Compared to learning from discrete inputs only, dynamics models trained on such data generate partial derivatives that better reflect local dependencies, as shown in Sec 4.3.1. For the 2D MINECRAFT and KITCHEN environments where some local dependencies have complex preconditions and thus are hard to induce, we apply sample prioritization to dynamics learning, where the priority is measured as prediction error. In this way, the dynamics model gets aware of unknown interactions faster and guides the exploration more efficiently than not using prioritization. Further details are provided in the Appendix.", "md": "Implementation Details: For discrete state or action spaces, the partial derivatives w.r.t. st/at are undefined. To address this issue, we use Mixup [34] to create synthetic inputs and labels by linearly combining pairs of inputs and labels, thus approximately changing the input space to be continuous. Compared to learning from discrete inputs only, dynamics models trained on such data generate partial derivatives that better reflect local dependencies, as shown in Sec 4.3.1. For the 2D MINECRAFT and KITCHEN environments where some local dependencies have complex preconditions and thus are hard to induce, we apply sample prioritization to dynamics learning, where the priority is measured as prediction error. In this way, the dynamics model gets aware of unknown interactions faster and guides the exploration more efficiently than not using prioritization. Further details are provided in the Appendix."}, {"type": "heading", "lvl": 3, "value": "4.1 Evaluating the Detection of Local Dependencies", "md": "### 4.1 Evaluating the Detection of Local Dependencies"}, {"type": "text", "value": "We compare the local dependencies extracted by ELDEN with the following baselines (see implementation details in Appendix Sec. C):\n\n- pCMI (point-wise conditional mutual information) [27, 31]: defined as $\\log \\frac{p(s_{j,t+1}|s_{t},a_{t})}{p(s_{j,t+1}|s_{t}\\backslash s_{i})}$. It quantifies how likely it is that $s_{j,t+1}$ depends on $s_{i,t}$.\n- Attn (attention): Use the score between each entity pair computed by the attention modules inside the dynamics model to quantify local dependencies.\n- Input Mask: we implement a learnable binary mask to the dynamics model that can zero out some inputs conditioned on $(s_{t}, a_{t}): f([s_{t}, a_{t}] \\odot M(s_{t}, a_{t})).$ During training, the mask is regularized to use as few inputs as possible with L1 regularization, leading to a quantification of minimal local dependencies.\n- Attn Mask: we implement a learnable mask to the dynamics model similar to the one in Input Mask, but in this case, the mask is applied to the attention scores. The mask is regularized following the method by Weiss et al. [32].\n\nWe train the dynamics model of each method with three random seeds on pre-collected transition data and evaluate their performance by predicting the local causal graph $$G_{t}$$ for 50 unseen episodes based on the state-action pair $$(s_{t}, a_{t}).$$ We compare their predictions with the ground truth local dependencies extracted from the simulator. In the three environments, many potential local dependencies are inactive most of the time, and thus only a small portion ($$\\leq 3\\%$$) of the ground truth labels indicate the existence of local dependencies for a given entity pair. To account for such imbalance, we use the area under the receiver operating characteristic curve (ROC-AUC) and the best achievable F-score (F1) as evaluation metrics.\n\nThe results of the evaluation on the detection of local dependencies are summarized in Table 1. ELDEN outperforms all baselines in terms of ROC-AUC consistently across all environments (Q1). For the F1 score, pCMI performs best in most environments (especially in the more complex CARWASH and KITCHEN), but ELDEN performs comparably or achieves the second-best F1 scores with much less computation: pCMI computation cost is N times higher than ELDEN, where N is the number of environment entities, and thus pCMI scales badly to environments with a large number of objects. Further evaluation details can be found in the Appendix.", "md": "We compare the local dependencies extracted by ELDEN with the following baselines (see implementation details in Appendix Sec. C):\n\n- pCMI (point-wise conditional mutual information) [27, 31]: defined as $\\log \\frac{p(s_{j,t+1}|s_{t},a_{t})}{p(s_{j,t+1}|s_{t}\\backslash s_{i})}$. It quantifies how likely it is that $s_{j,t+1}$ depends on $s_{i,t}$.\n- Attn (attention): Use the score between each entity pair computed by the attention modules inside the dynamics model to quantify local dependencies.\n- Input Mask: we implement a learnable binary mask to the dynamics model that can zero out some inputs conditioned on $(s_{t}, a_{t}): f([s_{t}, a_{t}] \\odot M(s_{t}, a_{t})).$ During training, the mask is regularized to use as few inputs as possible with L1 regularization, leading to a quantification of minimal local dependencies.\n- Attn Mask: we implement a learnable mask to the dynamics model similar to the one in Input Mask, but in this case, the mask is applied to the attention scores. The mask is regularized following the method by Weiss et al. [32].\n\nWe train the dynamics model of each method with three random seeds on pre-collected transition data and evaluate their performance by predicting the local causal graph $$G_{t}$$ for 50 unseen episodes based on the state-action pair $$(s_{t}, a_{t}).$$ We compare their predictions with the ground truth local dependencies extracted from the simulator. In the three environments, many potential local dependencies are inactive most of the time, and thus only a small portion ($$\\leq 3\\%$$) of the ground truth labels indicate the existence of local dependencies for a given entity pair. To account for such imbalance, we use the area under the receiver operating characteristic curve (ROC-AUC) and the best achievable F-score (F1) as evaluation metrics.\n\nThe results of the evaluation on the detection of local dependencies are summarized in Table 1. ELDEN outperforms all baselines in terms of ROC-AUC consistently across all environments (Q1). For the F1 score, pCMI performs best in most environments (especially in the more complex CARWASH and KITCHEN), but ELDEN performs comparably or achieves the second-best F1 scores with much less computation: pCMI computation cost is N times higher than ELDEN, where N is the number of environment entities, and thus pCMI scales badly to environments with a large number of objects. Further evaluation details can be found in the Appendix."}]}, {"page": 8, "text": "         (a) Thawing                 (b) CarWash                    (c) Craft                  (d) Kitchen\nFigure 3: Learning curve of ELDEN (ours) compared to baseline approaches. Each method uses\nthree random seeds, and we show the mean \u00b1 std dev of the number of stages completed toward task\nsuccess. The stage count is normalized to [0, 1], where 1 corresponds to task completion. ELDEN\nlearns successful policies in all four test environments, and is the only method that succeeds in the\nCarWash, 2D Minecraft, and Kitchen environments with complex chained dependencies.\n4.2    Evaluating Exploration in Sparse-Reward RL Tasks\nThe ultimate goal of our method is to improve exploration for RL in sparse-reward setups. In\nthe second evaluation, we compare the performance of ELDEN against several state-of-the-art\nintrinsic-motivation exploration algorithms in reinforcement learning, including:\n\u2022 Disagreement [21]: the intrinsic reward is computed based on the variance of the predictions from\n   an ensemble of forward dynamics models.\n\u2022 Dynamics Curiosity [5]: intrinsic reward is computed based on the prediction error of a trained\n   forward dynamics model.\n\u2022 CAI (Causal Influence Detection) [27]: an empowerment-based method, where the agent is given\n   intrinsic reward for maximizing the number of state entities that depend on its action.\n\u2022 Vanilla PPO [26]: baseline without intrinsic reward that serves as control signal.\nWhile ELDEN can be used with any RL algorithm, in our experiments we use proximal policy\noptimization (PPO) [26], as well as with the baselines. To facilitate the introspection of the results,\nwe define manually a set of semantic stages representing internal progress toward task completion.\nThe stage definitions for each of the environments are described in detail in the Appendix. Notice\nthat these stages are not used by the agents during training and do not provide any additional reward;\nthey are only used to facilitate the analysis of the results.\nFig. 3 depicts the count of reached stages per episode during training for each task, normalized by\nthe number of stages to complete the task. In the normalized stage count, a value of 1 corresponds\nto successfully completing the task and it is the only stage where the learning agents receive sparse\ntask reward (not intrinsic). Fig. 3 indicates that ELDEN is able to learn successful policies in all\nfour environments. Importantly, in CARWASH, 2D MINECRAFT and KITCHEN, ELDEN is the only\nmethod that successfully learns to complete the task, demonstrating the advantage of ELDEN over\nthe baseline algorithms in tackling tasks with complex chained dependencies (Q2).\nThe normalized stage count of ELDEN in CARWASH, 2D MINECRAFT and KITCHEN does not\nconverge to 1 (completing the entire task in all episodes) mainly due to two reasons: First, in both\ntasks, the locations of the objects are randomly initialized at the start of each episode. For some\ninitialization (e.g. a target object is blocked by unmovable obstacles), the task is impossible to solve.\nSecond, in both tasks, two out of the three ELDEN training procedures with different random seeds\nconverge to succeeding most of the time, but the training process with one seed fails to find a good\npolicy, dragging down the mean value of the normalized stage count. This large variance in success\nis a current limitation of ELDEN.\nIn the relatively simple THAWING environment, we found ELDEN does not provide a significant\nadvantage over the other baseline methods. The Dynamics Curiosity baseline learns faster to achieve\nthe task indicating a better sample efficiency. This was rather expected: as with any exploration\nheuristic, ELDEN is not universally better than previous intrinsic reward methods \u2014 instead, it is\nbetter suited for a specific type of environment, where there are many complex and chained object\n                                                          8", "md": "# Document\n\n(a) Thawing (b) CarWash (c) Craft (d) Kitchen\n\nFigure 3: Learning curve of ELDEN (ours) compared to baseline approaches. Each method uses three random seeds, and we show the mean \u00b1 std dev of the number of stages completed toward task success. The stage count is normalized to [0, 1], where 1 corresponds to task completion. ELDEN learns successful policies in all four test environments, and is the only method that succeeds in the CarWash, 2D Minecraft, and Kitchen environments with complex chained dependencies.\n\n#### 4.2 Evaluating Exploration in Sparse-Reward RL Tasks\n\nThe ultimate goal of our method is to improve exploration for RL in sparse-reward setups. In the second evaluation, we compare the performance of ELDEN against several state-of-the-art intrinsic-motivation exploration algorithms in reinforcement learning, including:\n\n- Disagreement [21]: the intrinsic reward is computed based on the variance of the predictions from an ensemble of forward dynamics models.\n- Dynamics Curiosity [5]: intrinsic reward is computed based on the prediction error of a trained forward dynamics model.\n- CAI (Causal Influence Detection) [27]: an empowerment-based method, where the agent is given intrinsic reward for maximizing the number of state entities that depend on its action.\n- Vanilla PPO [26]: baseline without intrinsic reward that serves as control signal.\n\nWhile ELDEN can be used with any RL algorithm, in our experiments we use proximal policy optimization (PPO) [26], as well as with the baselines. To facilitate the introspection of the results, we define manually a set of semantic stages representing internal progress toward task completion. The stage definitions for each of the environments are described in detail in the Appendix. Notice that these stages are not used by the agents during training and do not provide any additional reward; they are only used to facilitate the analysis of the results.\n\nFig. 3 depicts the count of reached stages per episode during training for each task, normalized by the number of stages to complete the task. In the normalized stage count, a value of 1 corresponds to successfully completing the task and it is the only stage where the learning agents receive sparse task reward (not intrinsic). Fig. 3 indicates that ELDEN is able to learn successful policies in all four environments. Importantly, in CARWASH, 2D MINECRAFT and KITCHEN, ELDEN is the only method that successfully learns to complete the task, demonstrating the advantage of ELDEN over the baseline algorithms in tackling tasks with complex chained dependencies (Q2).\n\nThe normalized stage count of ELDEN in CARWASH, 2D MINECRAFT and KITCHEN does not converge to 1 (completing the entire task in all episodes) mainly due to two reasons: First, in both tasks, the locations of the objects are randomly initialized at the start of each episode. For some initialization (e.g. a target object is blocked by unmovable obstacles), the task is impossible to solve. Second, in both tasks, two out of the three ELDEN training procedures with different random seeds converge to succeeding most of the time, but the training process with one seed fails to find a good policy, dragging down the mean value of the normalized stage count. This large variance in success is a current limitation of ELDEN.\n\nIn the relatively simple THAWING environment, we found ELDEN does not provide a significant advantage over the other baseline methods. The Dynamics Curiosity baseline learns faster to achieve the task indicating a better sample efficiency. This was rather expected: as with any exploration heuristic, ELDEN is not universally better than previous intrinsic reward methods \u2014 instead, it is better suited for a specific type of environment, where there are many complex and chained object", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "(a) Thawing (b) CarWash (c) Craft (d) Kitchen\n\nFigure 3: Learning curve of ELDEN (ours) compared to baseline approaches. Each method uses three random seeds, and we show the mean \u00b1 std dev of the number of stages completed toward task success. The stage count is normalized to [0, 1], where 1 corresponds to task completion. ELDEN learns successful policies in all four test environments, and is the only method that succeeds in the CarWash, 2D Minecraft, and Kitchen environments with complex chained dependencies.", "md": "(a) Thawing (b) CarWash (c) Craft (d) Kitchen\n\nFigure 3: Learning curve of ELDEN (ours) compared to baseline approaches. Each method uses three random seeds, and we show the mean \u00b1 std dev of the number of stages completed toward task success. The stage count is normalized to [0, 1], where 1 corresponds to task completion. ELDEN learns successful policies in all four test environments, and is the only method that succeeds in the CarWash, 2D Minecraft, and Kitchen environments with complex chained dependencies."}, {"type": "heading", "lvl": 4, "value": "4.2 Evaluating Exploration in Sparse-Reward RL Tasks", "md": "#### 4.2 Evaluating Exploration in Sparse-Reward RL Tasks"}, {"type": "text", "value": "The ultimate goal of our method is to improve exploration for RL in sparse-reward setups. In the second evaluation, we compare the performance of ELDEN against several state-of-the-art intrinsic-motivation exploration algorithms in reinforcement learning, including:\n\n- Disagreement [21]: the intrinsic reward is computed based on the variance of the predictions from an ensemble of forward dynamics models.\n- Dynamics Curiosity [5]: intrinsic reward is computed based on the prediction error of a trained forward dynamics model.\n- CAI (Causal Influence Detection) [27]: an empowerment-based method, where the agent is given intrinsic reward for maximizing the number of state entities that depend on its action.\n- Vanilla PPO [26]: baseline without intrinsic reward that serves as control signal.\n\nWhile ELDEN can be used with any RL algorithm, in our experiments we use proximal policy optimization (PPO) [26], as well as with the baselines. To facilitate the introspection of the results, we define manually a set of semantic stages representing internal progress toward task completion. The stage definitions for each of the environments are described in detail in the Appendix. Notice that these stages are not used by the agents during training and do not provide any additional reward; they are only used to facilitate the analysis of the results.\n\nFig. 3 depicts the count of reached stages per episode during training for each task, normalized by the number of stages to complete the task. In the normalized stage count, a value of 1 corresponds to successfully completing the task and it is the only stage where the learning agents receive sparse task reward (not intrinsic). Fig. 3 indicates that ELDEN is able to learn successful policies in all four environments. Importantly, in CARWASH, 2D MINECRAFT and KITCHEN, ELDEN is the only method that successfully learns to complete the task, demonstrating the advantage of ELDEN over the baseline algorithms in tackling tasks with complex chained dependencies (Q2).\n\nThe normalized stage count of ELDEN in CARWASH, 2D MINECRAFT and KITCHEN does not converge to 1 (completing the entire task in all episodes) mainly due to two reasons: First, in both tasks, the locations of the objects are randomly initialized at the start of each episode. For some initialization (e.g. a target object is blocked by unmovable obstacles), the task is impossible to solve. Second, in both tasks, two out of the three ELDEN training procedures with different random seeds converge to succeeding most of the time, but the training process with one seed fails to find a good policy, dragging down the mean value of the normalized stage count. This large variance in success is a current limitation of ELDEN.\n\nIn the relatively simple THAWING environment, we found ELDEN does not provide a significant advantage over the other baseline methods. The Dynamics Curiosity baseline learns faster to achieve the task indicating a better sample efficiency. This was rather expected: as with any exploration heuristic, ELDEN is not universally better than previous intrinsic reward methods \u2014 instead, it is better suited for a specific type of environment, where there are many complex and chained object", "md": "The ultimate goal of our method is to improve exploration for RL in sparse-reward setups. In the second evaluation, we compare the performance of ELDEN against several state-of-the-art intrinsic-motivation exploration algorithms in reinforcement learning, including:\n\n- Disagreement [21]: the intrinsic reward is computed based on the variance of the predictions from an ensemble of forward dynamics models.\n- Dynamics Curiosity [5]: intrinsic reward is computed based on the prediction error of a trained forward dynamics model.\n- CAI (Causal Influence Detection) [27]: an empowerment-based method, where the agent is given intrinsic reward for maximizing the number of state entities that depend on its action.\n- Vanilla PPO [26]: baseline without intrinsic reward that serves as control signal.\n\nWhile ELDEN can be used with any RL algorithm, in our experiments we use proximal policy optimization (PPO) [26], as well as with the baselines. To facilitate the introspection of the results, we define manually a set of semantic stages representing internal progress toward task completion. The stage definitions for each of the environments are described in detail in the Appendix. Notice that these stages are not used by the agents during training and do not provide any additional reward; they are only used to facilitate the analysis of the results.\n\nFig. 3 depicts the count of reached stages per episode during training for each task, normalized by the number of stages to complete the task. In the normalized stage count, a value of 1 corresponds to successfully completing the task and it is the only stage where the learning agents receive sparse task reward (not intrinsic). Fig. 3 indicates that ELDEN is able to learn successful policies in all four environments. Importantly, in CARWASH, 2D MINECRAFT and KITCHEN, ELDEN is the only method that successfully learns to complete the task, demonstrating the advantage of ELDEN over the baseline algorithms in tackling tasks with complex chained dependencies (Q2).\n\nThe normalized stage count of ELDEN in CARWASH, 2D MINECRAFT and KITCHEN does not converge to 1 (completing the entire task in all episodes) mainly due to two reasons: First, in both tasks, the locations of the objects are randomly initialized at the start of each episode. For some initialization (e.g. a target object is blocked by unmovable obstacles), the task is impossible to solve. Second, in both tasks, two out of the three ELDEN training procedures with different random seeds converge to succeeding most of the time, but the training process with one seed fails to find a good policy, dragging down the mean value of the normalized stage count. This large variance in success is a current limitation of ELDEN.\n\nIn the relatively simple THAWING environment, we found ELDEN does not provide a significant advantage over the other baseline methods. The Dynamics Curiosity baseline learns faster to achieve the task indicating a better sample efficiency. This was rather expected: as with any exploration heuristic, ELDEN is not universally better than previous intrinsic reward methods \u2014 instead, it is better suited for a specific type of environment, where there are many complex and chained object"}]}, {"page": 9, "text": "   Table 2: Ablation of ELDEN on local dependency prediction (mean \u00b1 std. error of ROC AUC and F1)\n                            THAWING                           CARWASH                             KITCHEN\n  no Mixup &      ROC AUC (\u2191)           F1 (\u2191)       ROC AUC (\u2191)          F1 (\u2191)       ROC AUC (\u2191)           F1 (\u2191)\n     no Reg        0.48 \u00b1 0.01       0.42 \u00b1 0.01      0.44 \u00b1 0.00      0.27 \u00b1 0.01           N/A              N/A\n    no Reg,\n   i.e., \u03bb = 0     0.57 \u00b1 0.01       0.52 \u00b1 0.01      0.54 \u00b1 0.01      0.42 \u00b1 0.01       0.64 \u00b1 0.01      0.24 \u00b1 0.01\n   \u03bb = 10\u22121        0.68 \u00b1 0.00       0.57 \u00b1 0.00      0.73 \u00b1 0.01      0.58 \u00b1 0.02       0.55 \u00b1 0.00      0.14 \u00b1 0.00\n   \u03bb = 10\u22122        0.71 \u00b1 0.01       0.57 \u00b1 0.00      0.76 \u00b1 0.01      0.60 \u00b1 0.00       0.60 \u00b1 0.01      0.21 \u00b1 0.01\n   \u03bb = 10\u22123        0.64 \u00b1 0.01       0.55 \u00b1 0.01      0.78 \u00b1 0.02      0.66 \u00b1 0.02       0.65 \u00b1 0.00      0.24 \u00b1 0.01\n   \u03bb = 10\u22124        0.65 \u00b1 0.02       0.55 \u00b1 0.01      0.75 \u00b1 0.01      0.60 \u00b1 0.01       0.66 \u00b1 0.00      0.25 \u00b1 0.01\n   \u03bb = 10\u22125        0.63 \u00b1 0.00       0.53 \u00b1 0.01      0.72 \u00b1 0.00      0.57 \u00b1 0.00       0.65 \u00b1 0.01      0.24 \u00b1 0.01\ndependencies, not the case for THAWING. We provide additional experimental evaluations of the\nfailure cases of ELDEN in Appendix Sec E.\n4.3    Ablation Studies\nWe ablate different components of ELDEN to examine their importance to the overall methods.\n4.3.1    Ablations for Local Dependency Detection\nIn our ablation study on ELDEN for local dependency detection, we investigate the impact of each\ncomponent with the following variations:\n\u2022 No Mixup & No Reg: We disable the use of Mixup for discrete space prediction, and no partial\n  derivative regularization is applied in this case.\n\u2022 Different partial derivative regularization coefficients:             we test with different \u03bb values in\n  {0, 10\u22121, 10\u22122, 10\u22123, 10\u22124, 10\u22125}.\nAs shown in Table. 2, in Thawing and CarWash environments, partial derivative regularization with\nappropriate coefficients significantly improves ELDEN\u2019s detection of local dependencies, compared to\nno regularization (i.e., \u03bb = 0) or inappropriate \u03bb values. Furthermore, in discrete-state environments,\nMixup smooths the landscape of partial derivatives by providing synthesized continuous inputs as\nexemplified in Fig. 1(b) of Zhang et al. [34], thus facilitating local dependency prediction \u2014 even\nwhen compared to using Mixup without any regularization, not using Mixup leads to a noticeable\ndegradation in the prediction performance.\n4.3.2    Ablations for Task Learning\nNext, we examine how different components and hyperparameters of ELDEN affect task learning:\nAblation of Local Dependency Metrics We compare the exploration performance when using\ndifferent local dependency detection methods. Specifically, we compare with pCMI as it achieves the\nbest local dependency detection in Sec. 4.1. We present the comparison results between ELDEN and\npCMI in the Kitchen environment in Fig. 4(a) where both methods successfully learn to solve the\ntask. However, it is important to notice that the computation cost of pCMI is N times more than that\nof ELDEN (where N is the number of environment entities), and thus may not scale to environments\nwith a large number of entities.\nAblation of Dynamics Sample Prioritization We study the effectiveness of applying sample\nprioritization in dynamics model training. Specifically, we test ELDEN with and without prioritization\nin the Kitchen environment, and show the result in Fig. 4(b). We can see that ELDEN without\nprioritization fails to learn a useful policy. The reason is that some key entity interactions occur rather\nrarely before the agent masters them, e.g., frying meatball with butter. In such cases, the dynamics\nmodel needs to quickly learn that unknown dependencies appear so that it can bias the exploration\ntoward reproducing such dependencies. Sample prioritization helps the dynamics model learn such\n                                                         9", "md": "| |THAWING|CARWASH|KITCHEN|\n|---|---|---|---|\n|no Mixup & no Reg|ROC AUC (\u2191)|F1 (\u2191)|ROC AUC (\u2191)|F1 (\u2191)|ROC AUC (\u2191)|F1 (\u2191)|\n|no Reg|0.48 \u00b1 0.01|0.42 \u00b1 0.01|0.44 \u00b1 0.00|0.27 \u00b1 0.01|N/A|N/A|\n|no Reg, i.e., \u03bb = 0|0.57 \u00b1 0.01|0.52 \u00b1 0.01|0.54 \u00b1 0.01|0.42 \u00b1 0.01|0.64 \u00b1 0.01|0.24 \u00b1 0.01|\n|\u03bb = 10-1|0.68 \u00b1 0.00|0.57 \u00b1 0.00|0.73 \u00b1 0.01|0.58 \u00b1 0.02|0.55 \u00b1 0.00|0.14 \u00b1 0.00|\n|\u03bb = 10-2|0.71 \u00b1 0.01|0.57 \u00b1 0.00|0.76 \u00b1 0.01|0.60 \u00b1 0.00|0.60 \u00b1 0.01|0.21 \u00b1 0.01|\n|\u03bb = 10-3|0.64 \u00b1 0.01|0.55 \u00b1 0.01|0.78 \u00b1 0.02|0.66 \u00b1 0.02|0.65 \u00b1 0.00|0.24 \u00b1 0.01|\n|\u03bb = 10-4|0.65 \u00b1 0.02|0.55 \u00b1 0.01|0.75 \u00b1 0.01|0.60 \u00b1 0.01|0.66 \u00b1 0.00|0.25 \u00b1 0.01|\n|\u03bb = 10-5|0.63 \u00b1 0.00|0.53 \u00b1 0.01|0.72 \u00b1 0.00|0.57 \u00b1 0.00|0.65 \u00b1 0.01|0.24 \u00b1 0.01|\n\ndependencies, not the case for THAWING. We provide additional experimental evaluations of the failure cases of ELDEN in Appendix Sec E.\n\n### Ablation Studies\n\nWe ablate different components of ELDEN to examine their importance to the overall methods.\n\n#### Ablations for Local Dependency Detection\n\nIn our ablation study on ELDEN for local dependency detection, we investigate the impact of each component with the following variations:\n\n- No Mixup & No Reg: We disable the use of Mixup for discrete space prediction, and no partial derivative regularization is applied in this case.\n- Different partial derivative regularization coefficients: we test with different $\\lambda$ values in {0, 10-1, 10-2, 10-3, 10-4, 10-5}.\n\nAs shown in Table. 2, in Thawing and CarWash environments, partial derivative regularization with appropriate coefficients significantly improves ELDEN\u2019s detection of local dependencies, compared to no regularization (i.e., $$\\lambda = 0$$) or inappropriate $$\\lambda$$ values. Furthermore, in discrete-state environments, Mixup smooths the landscape of partial derivatives by providing synthesized continuous inputs as exemplified in Fig. 1(b) of Zhang et al. [34], thus facilitating local dependency prediction \u2014 even when compared to using Mixup without any regularization, not using Mixup leads to a noticeable degradation in the prediction performance.\n\n#### Ablations for Task Learning\n\nNext, we examine how different components and hyperparameters of ELDEN affect task learning:\n\nAblation of Local Dependency Metrics We compare the exploration performance when using different local dependency detection methods. Specifically, we compare with pCMI as it achieves the best local dependency detection in Sec. 4.1. We present the comparison results between ELDEN and pCMI in the Kitchen environment in Fig. 4(a) where both methods successfully learn to solve the task. However, it is important to notice that the computation cost of pCMI is N times more than that of ELDEN (where N is the number of environment entities), and thus may not scale to environments with a large number of entities.\n\nAblation of Dynamics Sample Prioritization We study the effectiveness of applying sample prioritization in dynamics model training. Specifically, we test ELDEN with and without prioritization in the Kitchen environment, and show the result in Fig. 4(b). We can see that ELDEN without prioritization fails to learn a useful policy. The reason is that some key entity interactions occur rather rarely before the agent masters them, e.g., frying meatball with butter. In such cases, the dynamics model needs to quickly learn that unknown dependencies appear so that it can bias the exploration toward reproducing such dependencies. Sample prioritization helps the dynamics model learn such.", "images": [], "items": [{"type": "table", "rows": [["", "THAWING", "CARWASH", "KITCHEN"], ["no Mixup & no Reg", "ROC AUC (\u2191)", "F1 (\u2191)", "ROC AUC (\u2191)", "F1 (\u2191)", "ROC AUC (\u2191)", "F1 (\u2191)"], ["no Reg", "0.48 \u00b1 0.01", "0.42 \u00b1 0.01", "0.44 \u00b1 0.00", "0.27 \u00b1 0.01", "N/A", "N/A"], ["no Reg, i.e., \u03bb = 0", "0.57 \u00b1 0.01", "0.52 \u00b1 0.01", "0.54 \u00b1 0.01", "0.42 \u00b1 0.01", "0.64 \u00b1 0.01", "0.24 \u00b1 0.01"], ["\u03bb = 10-1", "0.68 \u00b1 0.00", "0.57 \u00b1 0.00", "0.73 \u00b1 0.01", "0.58 \u00b1 0.02", "0.55 \u00b1 0.00", "0.14 \u00b1 0.00"], ["\u03bb = 10-2", "0.71 \u00b1 0.01", "0.57 \u00b1 0.00", "0.76 \u00b1 0.01", "0.60 \u00b1 0.00", "0.60 \u00b1 0.01", "0.21 \u00b1 0.01"], ["\u03bb = 10-3", "0.64 \u00b1 0.01", "0.55 \u00b1 0.01", "0.78 \u00b1 0.02", "0.66 \u00b1 0.02", "0.65 \u00b1 0.00", "0.24 \u00b1 0.01"], ["\u03bb = 10-4", "0.65 \u00b1 0.02", "0.55 \u00b1 0.01", "0.75 \u00b1 0.01", "0.60 \u00b1 0.01", "0.66 \u00b1 0.00", "0.25 \u00b1 0.01"], ["\u03bb = 10-5", "0.63 \u00b1 0.00", "0.53 \u00b1 0.01", "0.72 \u00b1 0.00", "0.57 \u00b1 0.00", "0.65 \u00b1 0.01", "0.24 \u00b1 0.01"]], "md": "| |THAWING|CARWASH|KITCHEN|\n|---|---|---|---|\n|no Mixup & no Reg|ROC AUC (\u2191)|F1 (\u2191)|ROC AUC (\u2191)|F1 (\u2191)|ROC AUC (\u2191)|F1 (\u2191)|\n|no Reg|0.48 \u00b1 0.01|0.42 \u00b1 0.01|0.44 \u00b1 0.00|0.27 \u00b1 0.01|N/A|N/A|\n|no Reg, i.e., \u03bb = 0|0.57 \u00b1 0.01|0.52 \u00b1 0.01|0.54 \u00b1 0.01|0.42 \u00b1 0.01|0.64 \u00b1 0.01|0.24 \u00b1 0.01|\n|\u03bb = 10-1|0.68 \u00b1 0.00|0.57 \u00b1 0.00|0.73 \u00b1 0.01|0.58 \u00b1 0.02|0.55 \u00b1 0.00|0.14 \u00b1 0.00|\n|\u03bb = 10-2|0.71 \u00b1 0.01|0.57 \u00b1 0.00|0.76 \u00b1 0.01|0.60 \u00b1 0.00|0.60 \u00b1 0.01|0.21 \u00b1 0.01|\n|\u03bb = 10-3|0.64 \u00b1 0.01|0.55 \u00b1 0.01|0.78 \u00b1 0.02|0.66 \u00b1 0.02|0.65 \u00b1 0.00|0.24 \u00b1 0.01|\n|\u03bb = 10-4|0.65 \u00b1 0.02|0.55 \u00b1 0.01|0.75 \u00b1 0.01|0.60 \u00b1 0.01|0.66 \u00b1 0.00|0.25 \u00b1 0.01|\n|\u03bb = 10-5|0.63 \u00b1 0.00|0.53 \u00b1 0.01|0.72 \u00b1 0.00|0.57 \u00b1 0.00|0.65 \u00b1 0.01|0.24 \u00b1 0.01|", "isPerfectTable": false, "csv": "\"\",\"THAWING\",\"CARWASH\",\"KITCHEN\"\n\"no Mixup & no Reg\",\"ROC AUC (\u2191)\",\"F1 (\u2191)\",\"ROC AUC (\u2191)\",\"F1 (\u2191)\",\"ROC AUC (\u2191)\",\"F1 (\u2191)\"\n\"no Reg\",\"0.48 \u00b1 0.01\",\"0.42 \u00b1 0.01\",\"0.44 \u00b1 0.00\",\"0.27 \u00b1 0.01\",\"N/A\",\"N/A\"\n\"no Reg, i.e., \u03bb = 0\",\"0.57 \u00b1 0.01\",\"0.52 \u00b1 0.01\",\"0.54 \u00b1 0.01\",\"0.42 \u00b1 0.01\",\"0.64 \u00b1 0.01\",\"0.24 \u00b1 0.01\"\n\"\u03bb = 10-1\",\"0.68 \u00b1 0.00\",\"0.57 \u00b1 0.00\",\"0.73 \u00b1 0.01\",\"0.58 \u00b1 0.02\",\"0.55 \u00b1 0.00\",\"0.14 \u00b1 0.00\"\n\"\u03bb = 10-2\",\"0.71 \u00b1 0.01\",\"0.57 \u00b1 0.00\",\"0.76 \u00b1 0.01\",\"0.60 \u00b1 0.00\",\"0.60 \u00b1 0.01\",\"0.21 \u00b1 0.01\"\n\"\u03bb = 10-3\",\"0.64 \u00b1 0.01\",\"0.55 \u00b1 0.01\",\"0.78 \u00b1 0.02\",\"0.66 \u00b1 0.02\",\"0.65 \u00b1 0.00\",\"0.24 \u00b1 0.01\"\n\"\u03bb = 10-4\",\"0.65 \u00b1 0.02\",\"0.55 \u00b1 0.01\",\"0.75 \u00b1 0.01\",\"0.60 \u00b1 0.01\",\"0.66 \u00b1 0.00\",\"0.25 \u00b1 0.01\"\n\"\u03bb = 10-5\",\"0.63 \u00b1 0.00\",\"0.53 \u00b1 0.01\",\"0.72 \u00b1 0.00\",\"0.57 \u00b1 0.00\",\"0.65 \u00b1 0.01\",\"0.24 \u00b1 0.01\""}, {"type": "text", "value": "dependencies, not the case for THAWING. We provide additional experimental evaluations of the failure cases of ELDEN in Appendix Sec E.", "md": "dependencies, not the case for THAWING. We provide additional experimental evaluations of the failure cases of ELDEN in Appendix Sec E."}, {"type": "heading", "lvl": 3, "value": "Ablation Studies", "md": "### Ablation Studies"}, {"type": "text", "value": "We ablate different components of ELDEN to examine their importance to the overall methods.", "md": "We ablate different components of ELDEN to examine their importance to the overall methods."}, {"type": "heading", "lvl": 4, "value": "Ablations for Local Dependency Detection", "md": "#### Ablations for Local Dependency Detection"}, {"type": "text", "value": "In our ablation study on ELDEN for local dependency detection, we investigate the impact of each component with the following variations:\n\n- No Mixup & No Reg: We disable the use of Mixup for discrete space prediction, and no partial derivative regularization is applied in this case.\n- Different partial derivative regularization coefficients: we test with different $\\lambda$ values in {0, 10-1, 10-2, 10-3, 10-4, 10-5}.\n\nAs shown in Table. 2, in Thawing and CarWash environments, partial derivative regularization with appropriate coefficients significantly improves ELDEN\u2019s detection of local dependencies, compared to no regularization (i.e., $$\\lambda = 0$$) or inappropriate $$\\lambda$$ values. Furthermore, in discrete-state environments, Mixup smooths the landscape of partial derivatives by providing synthesized continuous inputs as exemplified in Fig. 1(b) of Zhang et al. [34], thus facilitating local dependency prediction \u2014 even when compared to using Mixup without any regularization, not using Mixup leads to a noticeable degradation in the prediction performance.", "md": "In our ablation study on ELDEN for local dependency detection, we investigate the impact of each component with the following variations:\n\n- No Mixup & No Reg: We disable the use of Mixup for discrete space prediction, and no partial derivative regularization is applied in this case.\n- Different partial derivative regularization coefficients: we test with different $\\lambda$ values in {0, 10-1, 10-2, 10-3, 10-4, 10-5}.\n\nAs shown in Table. 2, in Thawing and CarWash environments, partial derivative regularization with appropriate coefficients significantly improves ELDEN\u2019s detection of local dependencies, compared to no regularization (i.e., $$\\lambda = 0$$) or inappropriate $$\\lambda$$ values. Furthermore, in discrete-state environments, Mixup smooths the landscape of partial derivatives by providing synthesized continuous inputs as exemplified in Fig. 1(b) of Zhang et al. [34], thus facilitating local dependency prediction \u2014 even when compared to using Mixup without any regularization, not using Mixup leads to a noticeable degradation in the prediction performance."}, {"type": "heading", "lvl": 4, "value": "Ablations for Task Learning", "md": "#### Ablations for Task Learning"}, {"type": "text", "value": "Next, we examine how different components and hyperparameters of ELDEN affect task learning:\n\nAblation of Local Dependency Metrics We compare the exploration performance when using different local dependency detection methods. Specifically, we compare with pCMI as it achieves the best local dependency detection in Sec. 4.1. We present the comparison results between ELDEN and pCMI in the Kitchen environment in Fig. 4(a) where both methods successfully learn to solve the task. However, it is important to notice that the computation cost of pCMI is N times more than that of ELDEN (where N is the number of environment entities), and thus may not scale to environments with a large number of entities.\n\nAblation of Dynamics Sample Prioritization We study the effectiveness of applying sample prioritization in dynamics model training. Specifically, we test ELDEN with and without prioritization in the Kitchen environment, and show the result in Fig. 4(b). We can see that ELDEN without prioritization fails to learn a useful policy. The reason is that some key entity interactions occur rather rarely before the agent masters them, e.g., frying meatball with butter. In such cases, the dynamics model needs to quickly learn that unknown dependencies appear so that it can bias the exploration toward reproducing such dependencies. Sample prioritization helps the dynamics model learn such.", "md": "Next, we examine how different components and hyperparameters of ELDEN affect task learning:\n\nAblation of Local Dependency Metrics We compare the exploration performance when using different local dependency detection methods. Specifically, we compare with pCMI as it achieves the best local dependency detection in Sec. 4.1. We present the comparison results between ELDEN and pCMI in the Kitchen environment in Fig. 4(a) where both methods successfully learn to solve the task. However, it is important to notice that the computation cost of pCMI is N times more than that of ELDEN (where N is the number of environment entities), and thus may not scale to environments with a large number of entities.\n\nAblation of Dynamics Sample Prioritization We study the effectiveness of applying sample prioritization in dynamics model training. Specifically, we test ELDEN with and without prioritization in the Kitchen environment, and show the result in Fig. 4(b). We can see that ELDEN without prioritization fails to learn a useful policy. The reason is that some key entity interactions occur rather rarely before the agent masters them, e.g., frying meatball with butter. In such cases, the dynamics model needs to quickly learn that unknown dependencies appear so that it can bias the exploration toward reproducing such dependencies. Sample prioritization helps the dynamics model learn such."}]}, {"page": 10, "text": "  (a) Dependency detection   (b) Sample prioritization    (c) Gradient thresholds    (d) Reward coefficients\nFigure 4: Ablation of ELDEN on task learning. Each curve uses three random seeds and shows\nthe mean \u00b1 std dev of the normalized stages. We found ELDEN to have moderate tolerance\ntowards hyperparameters. We found sample prioritization in dynamics learning to be crucial to the\nperformance of ELDEN.\ninfrequent dependencies quickly, making it critical in environments with novel and hard-to-induce\nlocal dependencies.\nAblation of Partial Derivative Threshold: The partial derivative threshold \u03f5 determines the depen-\ndency predictions. A threshold that is too large / too small will make all dependency predictions\nnegative / positive respectively, leading to deteriorated performance. In this section, we examine\nwhether our method is sensitive to the choice of threshold in the CarWash environment, where the\nresults are presented in Fig. 4(c). We observe that our method is relatively sensitive to the choice of\nthreshold, and an inappropriate threshold could cause catastrophic failure. A potential next step for\nELDEN is to automatically determine the partial derivative threshold.\nAblation of Intrinsic Reward Coefficient: The intrinsic reward coefficient controls the scale of the\nintrinsic reward relative to the task reward. We examine the effect of this coefficient by experimenting\nwith different values in the CarWash environment, where the results are presented in Fig. 4(d). We\nfind that our methods work well in a large range of the intrinsic reward coefficients (1 - 10), since the\ntask only gives sparse rewards and the intrinsic rewards are the only learning signal most of the time.\nThe only exceptions are (1) when the intrinsic reward coefficient is too large (e.g., 100), the intrinsic\nreward significantly surpasses the task reward, and (2) when the coefficient is too small (e.g., 0.1),\nthe episode intrinsic reward is too small (e.g., 0.03) for PPO to learn any useful policy.\n5    Limitations and Conclusion\nWe introduce ELDEN, a method for improving exploration in sparse reward reinforcement learning\ntasks. ELDEN identifies local dependencies between environment entities and uses the uncertainty\nabout such dependencies as an intrinsic reward to improve exploration. Experiments demonstrate that\nELDEN uncovers local dependencies more accurately compared to related methods, and significantly\noutperforms previous exploration methods in tasks with complex chained dependencies.\nHowever, ELDEN is not without limitations. First, ELDEN intentionally bias exploration towards\n\u201ccovering up the possible interactions between objects\u201d rather than \u201cbecoming an expert at manipu-\nlating a particular object\u201d. While such an inductive bias works well in many practical domains, it\nmay fail when facing tasks that require precise object interaction (e.g. rotating the meatball in the\npot to a specific orientation). A future direction to alleviate this problem and expand the scope of\nsolvable tasks is to combine ELDEN with dynamics curiosity and formulate a composite intrinsic\nreward. Second, as noted in the experiment section, the variance of ELDEN across different random\nseeds can be large, While the high variance is a general problem to Reinforcement Learning, finding\nways to further stabilize ELDEN can be an important direction for future work.\n                                                      10", "md": "# ELDEN Method\n\n## Dependency detection\n\nELDEN identifies local dependencies between environment entities and uses the uncertainty about such dependencies as an intrinsic reward to improve exploration.\n\n## Sample prioritization\n\nSample prioritization in dynamics learning is crucial to the performance of ELDEN.\n\n## Gradient thresholds\n\nThe partial derivative threshold \u03f5 determines the dependency predictions. A threshold that is too large / too small will make all dependency predictions negative / positive respectively, leading to deteriorated performance.\n\n## Reward coefficients\n\nThe intrinsic reward coefficient controls the scale of the intrinsic reward relative to the task reward.\n\n### Ablation of ELDEN on task learning\n\nEach curve uses three random seeds and shows the mean \u00b1 std dev of the normalized stages. ELDEN has moderate tolerance towards hyperparameters.\n\n### Ablation of Partial Derivative Threshold\n\nThe partial derivative threshold \u03f5 determines the dependency predictions. Our method is relatively sensitive to the choice of threshold, and an inappropriate threshold could cause catastrophic failure.\n\n### Ablation of Intrinsic Reward Coefficient\n\nWe find that our methods work well in a large range of the intrinsic reward coefficients (1 - 10). However, extreme values can significantly affect the learning process.\n\n## Limitations and Conclusion\n\nELDEN intentionally biases exploration towards uncovering possible interactions between objects rather than becoming an expert at manipulating a particular object. This bias may limit performance in tasks requiring precise object interaction.\n\nThe variance of ELDEN across different random seeds can be large, which is a general problem in Reinforcement Learning. Stabilizing ELDEN further can be an important direction for future work.\n\n### Figure 4: Ablation of ELDEN on task learning\n\nResults are presented in the following figures:\n\n- Fig. 4(c): Sensitivity to the choice of threshold in the CarWash environment.\n- Fig. 4(d): Effect of intrinsic reward coefficient in the CarWash environment.\n\n### References\n\n1. ELDEN: Exploration in Sparse Reward Reinforcement Learning Tasks\n2. Related methods in exploration and reinforcement learning", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "ELDEN Method", "md": "# ELDEN Method"}, {"type": "heading", "lvl": 2, "value": "Dependency detection", "md": "## Dependency detection"}, {"type": "text", "value": "ELDEN identifies local dependencies between environment entities and uses the uncertainty about such dependencies as an intrinsic reward to improve exploration.", "md": "ELDEN identifies local dependencies between environment entities and uses the uncertainty about such dependencies as an intrinsic reward to improve exploration."}, {"type": "heading", "lvl": 2, "value": "Sample prioritization", "md": "## Sample prioritization"}, {"type": "text", "value": "Sample prioritization in dynamics learning is crucial to the performance of ELDEN.", "md": "Sample prioritization in dynamics learning is crucial to the performance of ELDEN."}, {"type": "heading", "lvl": 2, "value": "Gradient thresholds", "md": "## Gradient thresholds"}, {"type": "text", "value": "The partial derivative threshold \u03f5 determines the dependency predictions. A threshold that is too large / too small will make all dependency predictions negative / positive respectively, leading to deteriorated performance.", "md": "The partial derivative threshold \u03f5 determines the dependency predictions. A threshold that is too large / too small will make all dependency predictions negative / positive respectively, leading to deteriorated performance."}, {"type": "heading", "lvl": 2, "value": "Reward coefficients", "md": "## Reward coefficients"}, {"type": "text", "value": "The intrinsic reward coefficient controls the scale of the intrinsic reward relative to the task reward.", "md": "The intrinsic reward coefficient controls the scale of the intrinsic reward relative to the task reward."}, {"type": "heading", "lvl": 3, "value": "Ablation of ELDEN on task learning", "md": "### Ablation of ELDEN on task learning"}, {"type": "text", "value": "Each curve uses three random seeds and shows the mean \u00b1 std dev of the normalized stages. ELDEN has moderate tolerance towards hyperparameters.", "md": "Each curve uses three random seeds and shows the mean \u00b1 std dev of the normalized stages. ELDEN has moderate tolerance towards hyperparameters."}, {"type": "heading", "lvl": 3, "value": "Ablation of Partial Derivative Threshold", "md": "### Ablation of Partial Derivative Threshold"}, {"type": "text", "value": "The partial derivative threshold \u03f5 determines the dependency predictions. Our method is relatively sensitive to the choice of threshold, and an inappropriate threshold could cause catastrophic failure.", "md": "The partial derivative threshold \u03f5 determines the dependency predictions. Our method is relatively sensitive to the choice of threshold, and an inappropriate threshold could cause catastrophic failure."}, {"type": "heading", "lvl": 3, "value": "Ablation of Intrinsic Reward Coefficient", "md": "### Ablation of Intrinsic Reward Coefficient"}, {"type": "text", "value": "We find that our methods work well in a large range of the intrinsic reward coefficients (1 - 10). However, extreme values can significantly affect the learning process.", "md": "We find that our methods work well in a large range of the intrinsic reward coefficients (1 - 10). However, extreme values can significantly affect the learning process."}, {"type": "heading", "lvl": 2, "value": "Limitations and Conclusion", "md": "## Limitations and Conclusion"}, {"type": "text", "value": "ELDEN intentionally biases exploration towards uncovering possible interactions between objects rather than becoming an expert at manipulating a particular object. This bias may limit performance in tasks requiring precise object interaction.\n\nThe variance of ELDEN across different random seeds can be large, which is a general problem in Reinforcement Learning. Stabilizing ELDEN further can be an important direction for future work.", "md": "ELDEN intentionally biases exploration towards uncovering possible interactions between objects rather than becoming an expert at manipulating a particular object. This bias may limit performance in tasks requiring precise object interaction.\n\nThe variance of ELDEN across different random seeds can be large, which is a general problem in Reinforcement Learning. Stabilizing ELDEN further can be an important direction for future work."}, {"type": "heading", "lvl": 3, "value": "Figure 4: Ablation of ELDEN on task learning", "md": "### Figure 4: Ablation of ELDEN on task learning"}, {"type": "text", "value": "Results are presented in the following figures:\n\n- Fig. 4(c): Sensitivity to the choice of threshold in the CarWash environment.\n- Fig. 4(d): Effect of intrinsic reward coefficient in the CarWash environment.", "md": "Results are presented in the following figures:\n\n- Fig. 4(c): Sensitivity to the choice of threshold in the CarWash environment.\n- Fig. 4(d): Effect of intrinsic reward coefficient in the CarWash environment."}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "text", "value": "1. ELDEN: Exploration in Sparse Reward Reinforcement Learning Tasks\n2. Related methods in exploration and reinforcement learning", "md": "1. ELDEN: Exploration in Sparse Reward Reinforcement Learning Tasks\n2. Related methods in exploration and reinforcement learning"}]}, {"page": 11, "text": "Acknowledgements        This work has taken place in the Robot Interactive Intelligence Lab (RobIn)\nand Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University\nof Texas at Austin. LARG research is supported in part by the National Science Foundation (FAIN-\n2019844, NRT-2125858), the Office of Naval Research (N00014-18-2243), Army Research Office\n(E2061621), Bosch, Lockheed Martin. Both LARG and RobIn are supported by Good Systems, a\nresearch grand challenge at the University of Texas at Austin. The views and conclusions contained\nin this document are those of the authors alone. Peter Stone serves as the Executive Director of\nSony AI America and receives financial compensation for this work. The terms of this arrangement\nhave been reviewed and approved by the University of Texas at Austin in accordance with its policy\non objectivity in research. We thank Bo Liu and Caleb Chuck for their valuable feedback on the\nmanuscript.\nReferences\n [1] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with\n      policy sketches. In International Conference on Machine Learning, pages 166\u2013175. PMLR,\n      2017.\n [2] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob Mc-\n      Grew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning\n      dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320,\n      2020.\n [3] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi\n      Munos.    Unifying count-based exploration and intrinsic motivation.      Advances in neural\n      information processing systems, 29, 2016.\n [4] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u02db       ebiak, Christy\n      Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large\n      scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n [5] Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros.\n      Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.\n [6] Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random\n      network distillation. arXiv preprint arXiv:1810.12894, 2018.\n [7] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan\n      Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample\n      efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.\n [8] Jongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh, Neal Wu, Mohammad Norouzi,\n      and Honglak Lee. Contingency-aware exploration in reinforcement learning. arXiv preprint\n      arXiv:1811.01483, 2018.\n [9] Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine.\n      Learning to walk via deep reinforcement learning. RSS, 2018.\n[10] Jiaheng Hu, Peter Stone, and Roberto Mart\u00edn-Mart\u00edn. Causal policy gradient for whole-body\n      mobile manipulation. arXiv preprint arXiv:2305.04866, 2023.\n[11] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.\n      arXiv preprint arXiv:1611.01144, 2016.\n[12] Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto\n      Mart\u00edn-Mart\u00edn. Mini-behavior: A procedurally generated benchmark for long-horizon decision-\n      making in embodied ai. arXiv preprint 2310.01824, 2023.\n[13] Alina Kloss, Stefan Schaal, and Jeannette Bohg. Combining learned and analytical models for\n      predicting action effects. arXiv preprint arXiv:1710.04102, 11, 2017.\n[14] Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. All else being equal be\n      empowered. In Advances in Artificial Life: 8th European Conference, ECAL 2005, Canterbury,\n     UK, September 5-9, 2005. Proceedings 8, pages 744\u2013753. Springer, 2005.\n                                                 11", "md": "# Acknowledgements\n\n## Acknowledgements\n\nThis work has taken place in the Robot Interactive Intelligence Lab (RobIn) and Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by the National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research (N00014-18-2243), Army Research Office (E2061621), Bosch, Lockheed Martin. Both LARG and RobIn are supported by Good Systems, a research grand challenge at the University of Texas at Austin. The views and conclusions contained in this document are those of the authors alone. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research. We thank Bo Liu and Caleb Chuck for their valuable feedback on the manuscript.\n\n## References\n\n1. Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, pages 166\u2013175. PMLR, 2017.\n2. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020.\n3. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.\n4. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u02db ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n5. Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.\n6. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\n7. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.\n8. Jongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh, Neal Wu, Mohammad Norouzi, and Honglak Lee. Contingency-aware exploration in reinforcement learning. arXiv preprint arXiv:1811.01483, 2018.\n9. Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via deep reinforcement learning. RSS, 2018.\n10. Jiaheng Hu, Peter Stone, and Roberto Mart\u00edn-Mart\u00edn. Causal policy gradient for whole-body mobile manipulation. arXiv preprint arXiv:2305.04866, 2023.\n11. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.\n12. Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto Mart\u00edn-Mart\u00edn. Mini-behavior: A procedurally generated benchmark for long-horizon decision-making in embodied ai. arXiv preprint 2310.01824, 2023.\n13. Alina Kloss, Stefan Schaal, and Jeannette Bohg. Combining learned and analytical models for predicting action effects. arXiv preprint arXiv:1710.04102, 11, 2017.\n14. Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. All else being equal be empowered. In Advances in Artificial Life: 8th European Conference, ECAL 2005, Canterbury, UK, September 5-9, 2005. Proceedings 8, pages 744\u2013753. Springer, 2005.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgements", "md": "# Acknowledgements"}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "This work has taken place in the Robot Interactive Intelligence Lab (RobIn) and Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by the National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research (N00014-18-2243), Army Research Office (E2061621), Bosch, Lockheed Martin. Both LARG and RobIn are supported by Good Systems, a research grand challenge at the University of Texas at Austin. The views and conclusions contained in this document are those of the authors alone. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research. We thank Bo Liu and Caleb Chuck for their valuable feedback on the manuscript.", "md": "This work has taken place in the Robot Interactive Intelligence Lab (RobIn) and Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory, The University of Texas at Austin. LARG research is supported in part by the National Science Foundation (FAIN-2019844, NRT-2125858), the Office of Naval Research (N00014-18-2243), Army Research Office (E2061621), Bosch, Lockheed Martin. Both LARG and RobIn are supported by Good Systems, a research grand challenge at the University of Texas at Austin. The views and conclusions contained in this document are those of the authors alone. Peter Stone serves as the Executive Director of Sony AI America and receives financial compensation for this work. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research. We thank Bo Liu and Caleb Chuck for their valuable feedback on the manuscript."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, pages 166\u2013175. PMLR, 2017.\n2. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020.\n3. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.\n4. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u02db ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n5. Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.\n6. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\n7. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.\n8. Jongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh, Neal Wu, Mohammad Norouzi, and Honglak Lee. Contingency-aware exploration in reinforcement learning. arXiv preprint arXiv:1811.01483, 2018.\n9. Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via deep reinforcement learning. RSS, 2018.\n10. Jiaheng Hu, Peter Stone, and Roberto Mart\u00edn-Mart\u00edn. Causal policy gradient for whole-body mobile manipulation. arXiv preprint arXiv:2305.04866, 2023.\n11. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.\n12. Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto Mart\u00edn-Mart\u00edn. Mini-behavior: A procedurally generated benchmark for long-horizon decision-making in embodied ai. arXiv preprint 2310.01824, 2023.\n13. Alina Kloss, Stefan Schaal, and Jeannette Bohg. Combining learned and analytical models for predicting action effects. arXiv preprint arXiv:1710.04102, 11, 2017.\n14. Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. All else being equal be empowered. In Advances in Artificial Life: 8th European Conference, ECAL 2005, Canterbury, UK, September 5-9, 2005. Proceedings 8, pages 744\u2013753. Springer, 2005.", "md": "1. Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, pages 166\u2013175. PMLR, 2017.\n2. OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3\u201320, 2020.\n3. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016.\n4. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys\u0142aw D\u02db ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n5. Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.\n6. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\n7. Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. arXiv preprint arXiv:1810.08272, 2018.\n8. Jongwook Choi, Yijie Guo, Marcin Moczulski, Junhyuk Oh, Neal Wu, Mohammad Norouzi, and Honglak Lee. Contingency-aware exploration in reinforcement learning. arXiv preprint arXiv:1811.01483, 2018.\n9. Tuomas Haarnoja, Sehoon Ha, Aurick Zhou, Jie Tan, George Tucker, and Sergey Levine. Learning to walk via deep reinforcement learning. RSS, 2018.\n10. Jiaheng Hu, Peter Stone, and Roberto Mart\u00edn-Mart\u00edn. Causal policy gradient for whole-body mobile manipulation. arXiv preprint arXiv:2305.04866, 2023.\n11. Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.\n12. Emily Jin, Jiaheng Hu, Zhuoyi Huang, Ruohan Zhang, Jiajun Wu, Li Fei-Fei, and Roberto Mart\u00edn-Mart\u00edn. Mini-behavior: A procedurally generated benchmark for long-horizon decision-making in embodied ai. arXiv preprint 2310.01824, 2023.\n13. Alina Kloss, Stefan Schaal, and Jeannette Bohg. Combining learned and analytical models for predicting action effects. arXiv preprint arXiv:1710.04102, 11, 2017.\n14. Alexander S Klyubin, Daniel Polani, and Chrystopher L Nehaniv. All else being equal be empowered. In Advances in Artificial Life: 8th European Conference, ECAL 2005, Canterbury, UK, September 5-9, 2005. Proceedings 8, pages 744\u2013753. Springer, 2005."}]}, {"page": 12, "text": "[15] Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\n     The International Journal of Robotics Research, 32(11):1238\u20131274, 2013.\n[16] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\n      visuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.\n[17] Francesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and\n      Michael Tschannen. Weakly-supervised disentanglement without compromises. In International\n     Conference on Machine Learning, pages 6348\u20136359. PMLR, 2020.\n[18] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\n      Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\n      Human-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\n[19] Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for\n      intrinsically motivated reinforcement learning. Advances in neural information processing\n      systems, 28, 2015.\n[20] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\n      by self-supervised prediction. In International conference on machine learning, pages 2778\u2013\n      2787. PMLR, 2017.\n[21] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagree-\n      ment. In International conference on machine learning, pages 5062\u20135071. PMLR, 2019.\n[22] Judea Pearl. Causal inference in statistics: An overview, 2009.\n[23] Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally\n      factored dynamics. Advances in Neural Information Processing Systems, 33:3976\u20133990, 2020.\n[24] Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based\n      counterfactual data augmentation. arXiv preprint arXiv:2210.11287, 2022.\n[25] J\u00fcrgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building\n      neural controllers. In Proc. of the international conference on simulation of adaptive behavior:\n     From animals to animats, pages 222\u2013227, 1991.\n[26] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\n      policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[27] Maximilian Seitzer, Bernhard Sch\u00f6lkopf, and Georg Martius. Causal influence detection for\n      improving efficiency in reinforcement learning. Advances in Neural Information Processing\n      Systems, 34:22905\u201322918, 2021.\n[28] Sumedh A Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Sch\u00f6lkopf. Causal curios-\n      ity: Rl agents discovering self-supervised experiments for causal representation learning. In\n      International conference on machine learning, pages 9848\u20139858. PMLR, 2021.\n[29] Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement\n      learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\n[30] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John\n      Schulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration\n      for deep reinforcement learning. Advances in neural information processing systems, 30, 2017.\n[31] Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. Causal dynamics learning for\n      task-independent state abstraction, 2022. URL https://arxiv.org/abs/2206.13452.\n[32] Martin Weiss, Nasim Rahaman, Francesco Locatello, Chris Pal, Yoshua Bengio, Bernhard\n      Sch\u00f6lkopf, Erran Li Li, and Nicolas Ballas. Neural attentive circuits. Advances in Neural\n      Information Processing Systems, 35:7741\u20137754, 2022.\n                                                 12", "md": "# References\n\n# References\n\n1. Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238\u20131274, 2013.\n2. Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.\n3. Francesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and\nMichael Tschannen. Weakly-supervised disentanglement without compromises. In International\nConference on Machine Learning, pages 6348\u20136359. PMLR, 2020.\n4. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\n5. Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for\nintrinsically motivated reinforcement learning. Advances in neural information processing\nsystems, 28, 2015.\n6. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International conference on machine learning, pages 2778\u2013\n2787. PMLR, 2017.\n7. Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.\nIn International conference on machine learning, pages 5062\u20135071. PMLR, 2019.\n8. Judea Pearl. Causal inference in statistics: An overview, 2009.\n9. Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally\nfactored dynamics. Advances in Neural Information Processing Systems, 33:3976\u20133990, 2020.\n10. Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based\ncounterfactual data augmentation. arXiv preprint arXiv:2210.11287, 2022.\n11. J\u00fcrgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building\nneural controllers. In Proc. of the international conference on simulation of adaptive behavior:\nFrom animals to animats, pages 222\u2013227, 1991.\n12. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n13. Maximilian Seitzer, Bernhard Sch\u00f6lkopf, and Georg Martius. Causal influence detection for\nimproving efficiency in reinforcement learning. Advances in Neural Information Processing\nSystems, 34:22905\u201322918, 2021.\n14. Sumedh A Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Sch\u00f6lkopf. Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning. In International conference on machine learning, pages 9848\u20139858. PMLR, 2021.\n15. Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement\nlearning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\n16. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John\nSchulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration\nfor deep reinforcement learning. Advances in neural information processing systems, 30, 2017.\n17. Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. Causal dynamics learning for\ntask-independent state abstraction, 2022. URL https://arxiv.org/abs/2206.13452.\n18. Martin Weiss, Nasim Rahaman, Francesco Locatello, Chris Pal, Yoshua Bengio, Bernhard\nSch\u00f6lkopf, Erran Li Li, and Nicolas Ballas. Neural attentive circuits. Advances in Neural\nInformation Processing Systems, 35:7741\u20137754, 2022.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "1. Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238\u20131274, 2013.\n2. Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.\n3. Francesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and\nMichael Tschannen. Weakly-supervised disentanglement without compromises. In International\nConference on Machine Learning, pages 6348\u20136359. PMLR, 2020.\n4. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\n5. Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for\nintrinsically motivated reinforcement learning. Advances in neural information processing\nsystems, 28, 2015.\n6. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International conference on machine learning, pages 2778\u2013\n2787. PMLR, 2017.\n7. Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.\nIn International conference on machine learning, pages 5062\u20135071. PMLR, 2019.\n8. Judea Pearl. Causal inference in statistics: An overview, 2009.\n9. Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally\nfactored dynamics. Advances in Neural Information Processing Systems, 33:3976\u20133990, 2020.\n10. Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based\ncounterfactual data augmentation. arXiv preprint arXiv:2210.11287, 2022.\n11. J\u00fcrgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building\nneural controllers. In Proc. of the international conference on simulation of adaptive behavior:\nFrom animals to animats, pages 222\u2013227, 1991.\n12. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n13. Maximilian Seitzer, Bernhard Sch\u00f6lkopf, and Georg Martius. Causal influence detection for\nimproving efficiency in reinforcement learning. Advances in Neural Information Processing\nSystems, 34:22905\u201322918, 2021.\n14. Sumedh A Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Sch\u00f6lkopf. Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning. In International conference on machine learning, pages 9848\u20139858. PMLR, 2021.\n15. Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement\nlearning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\n16. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John\nSchulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration\nfor deep reinforcement learning. Advances in neural information processing systems, 30, 2017.\n17. Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. Causal dynamics learning for\ntask-independent state abstraction, 2022. URL https://arxiv.org/abs/2206.13452.\n18. Martin Weiss, Nasim Rahaman, Francesco Locatello, Chris Pal, Yoshua Bengio, Bernhard\nSch\u00f6lkopf, Erran Li Li, and Nicolas Ballas. Neural attentive circuits. Advances in Neural\nInformation Processing Systems, 35:7741\u20137754, 2022.", "md": "1. Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey.\nThe International Journal of Robotics Research, 32(11):1238\u20131274, 2013.\n2. Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep\nvisuomotor policies. The Journal of Machine Learning Research, 17(1):1334\u20131373, 2016.\n3. Francesco Locatello, Ben Poole, Gunnar R\u00e4tsch, Bernhard Sch\u00f6lkopf, Olivier Bachem, and\nMichael Tschannen. Weakly-supervised disentanglement without compromises. In International\nConference on Machine Learning, pages 6348\u20136359. PMLR, 2020.\n4. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529\u2013533, 2015.\n5. Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for\nintrinsically motivated reinforcement learning. Advances in neural information processing\nsystems, 28, 2015.\n6. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International conference on machine learning, pages 2778\u2013\n2787. PMLR, 2017.\n7. Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.\nIn International conference on machine learning, pages 5062\u20135071. PMLR, 2019.\n8. Judea Pearl. Causal inference in statistics: An overview, 2009.\n9. Silviu Pitis, Elliot Creager, and Animesh Garg. Counterfactual data augmentation using locally\nfactored dynamics. Advances in Neural Information Processing Systems, 33:3976\u20133990, 2020.\n10. Silviu Pitis, Elliot Creager, Ajay Mandlekar, and Animesh Garg. Mocoda: Model-based\ncounterfactual data augmentation. arXiv preprint arXiv:2210.11287, 2022.\n11. J\u00fcrgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building\nneural controllers. In Proc. of the international conference on simulation of adaptive behavior:\nFrom animals to animats, pages 222\u2013227, 1991.\n12. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n13. Maximilian Seitzer, Bernhard Sch\u00f6lkopf, and Georg Martius. Causal influence detection for\nimproving efficiency in reinforcement learning. Advances in Neural Information Processing\nSystems, 34:22905\u201322918, 2021.\n14. Sumedh A Sontakke, Arash Mehrjou, Laurent Itti, and Bernhard Sch\u00f6lkopf. Causal curiosity: Rl agents discovering self-supervised experiments for causal representation learning. In International conference on machine learning, pages 9848\u20139858. PMLR, 2021.\n15. Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement\nlearning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\n16. Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John\nSchulman, Filip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration\nfor deep reinforcement learning. Advances in neural information processing systems, 30, 2017.\n17. Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, and Peter Stone. Causal dynamics learning for\ntask-independent state abstraction, 2022. URL https://arxiv.org/abs/2206.13452.\n18. Martin Weiss, Nasim Rahaman, Francesco Locatello, Chris Pal, Yoshua Bengio, Bernhard\nSch\u00f6lkopf, Erran Li Li, and Nicolas Ballas. Neural attentive circuits. Advances in Neural\nInformation Processing Systems, 35:7741\u20137754, 2022."}]}, {"page": 13, "text": "[33] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,\n      Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.\n      Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):\n      223\u2013228, 2022.\n[34] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\n      empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n[35] Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, and Wei Xu. Mutual information state\n      intrinsic control. arXiv preprint arXiv:2103.08107, 2021.\n[36] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular\n      simulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293,\n      2020.\n                                                13", "md": "[33] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,\nThomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.\nOutracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):\n223\u2013228, 2022.\n\n[34] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\n[35] Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, and Wei Xu. Mutual information state\nintrinsic control. arXiv preprint arXiv:2103.08107, 2021.\n\n[36] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular\nsimulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293,\n2020.", "images": [], "items": [{"type": "text", "value": "[33] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,\nThomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.\nOutracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):\n223\u2013228, 2022.\n\n[34] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\n[35] Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, and Wei Xu. Mutual information state\nintrinsic control. arXiv preprint arXiv:2103.08107, 2021.\n\n[36] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular\nsimulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293,\n2020.", "md": "[33] Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian,\nThomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al.\nOutracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):\n223\u2013228, 2022.\n\n[34] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\n[35] Rui Zhao, Yang Gao, Pieter Abbeel, Volker Tresp, and Wei Xu. Mutual information state\nintrinsic control. arXiv preprint arXiv:2103.08107, 2021.\n\n[36] Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\u00edn-Mart\u00edn. robosuite: A modular\nsimulation framework and benchmark for robot learning. arXiv preprint arXiv:2009.12293,\n2020."}]}, {"page": 14, "text": "A      ELDEN details\nAssumptions            We summarize our assumptions on the MDP as follows:\n1. The state space can be factored as S = S1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SN.\n2. The transition of each state factor is independent, i.e., the dynamics can be represented\n    P(st+1|st, at) =  N          n=1 p(sn   t+1|Pa(Sn    t+1)).\n3. There is no instantaneous dependency between state factors at the same time step t, i.e., no\n    dependency such as si          t \u2192   sjt for any i, j.\nFor assumption 1, factored state space is commonly employed in causality literature and applies\nto many simulated or robotics environments. In cases where low-level observations or partial\nobservability are present, disentangled representation or causal representation methods can be utilized\nto learn a factored state space [17]. When a factored state space is available, assumptions 2 and 3\ngenerally hold.\nNetwork Architecture                In Figure 5(a), the architecture of ELDEN for predicting each state factor\nsj\n t+1 is illustrated. The process consists of the following steps:\n1. Feature Extraction: For each input state factor si                t, ELDEN utilizes a separate multi-layer perception\n    (MLP) to extract its corresponding feature gi.\n2. Entity Interaction: ELDEN employs a multi-head self-attention module to model entity interactions\n    and generates a set of transformed features hi that incorporate information from other state factors.\n3. Prediction using Multi-Head Attention: With hj as the query, ELDEN utilizes a multi-head\n    attention module to compute the prediction \u02c6                   p(sj t+1|st, at) for each state factor. For continuous\n    state factor, \u02c6  p(sj t+1) is modeled as a normal distribution with the mean computed by the network\n    and a fixed variance equal to 1. For discrete factor, \u02c6                      p(sjt+1) is a categorical distribution with\n    network outputs as class probabilities.\nThroughout the prediction process, there are a total of N such networks in ELDEN, with each network\nresponsible for predicting a separate state factor sj                 t+1.\nThe training loss for the dynamics model is:\n                                  L = \u2212      log   N    \u02c6                                \u2202\u02c6p(sj t+1)                          (3)\n                                                  j=1  p(sj t+1|st, at) + \u03bb        i,j       \u2202si t      ,\nwhere \u03bb is the coefficient for partial derivative regularization.\nB      Environment Details\nIn this section, we provide a detailed description of the environment, including its semantic stages\nrepresenting internal progress toward task completion, state space, and action space. We also highlight\nthat while each task consists of multiple semantic stages, agents do not have access to this information.\nThe learning signal for agents is solely based on a sparse reward of 0 or 1, indicating whether the task\nhas been completed or not. Additionally, in each environment, the poses of all environment entities\nare randomly initialized for each episode.\nMeanwhile, as ELDEN focuses on exploring novel local dependencies between environment entities,\nin all environments, the action space consists of hard-coded skills to increase the probability of entity\ninteractions and bypass navigation challenges under sparse rewards. Extending ELDEN to explore\nlocal dependency and learn such skills simultaneously would be an important direction for future\nwork.\nThawing          As shown in Fig. 6(a), the Thawing environment consists of a sink, a refrigerator, and a\nfrozen fish. The task requires the agent to complete the following stages: (1) open the refrigerator, (2)\ntake the frozen fish out of the refrigerator, and (3) put the fish into the sink to thaw it. The discrete state\nspace consists of (i) the agent\u2019s position and direction, (ii) the positions of all environment entities,\n                                                                    14", "md": "## A ELDEN details\n\nAssumptions We summarize our assumptions on the MDP as follows:\n\n1. The state space can be factored as \\( S = S1 \\times \\ldots \\times SN \\).\n2. The transition of each state factor is independent, i.e., the dynamics can be represented as: $ P(s_{t+1}|s_t, a_t) = \\prod_{n=1}^{N} p(s_n^{t+1}|Pa(S_n^{t+1})). $\n3. There is no instantaneous dependency between state factors at the same time step \\( t \\), i.e., no dependency such as \\( s_i^t \\rightarrow s_j^t \\) for any \\( i, j \\).\n\nFor assumption 1, factored state space is commonly employed in causality literature and applies to many simulated or robotics environments. In cases where low-level observations or partial observability are present, disentangled representation or causal representation methods can be utilized to learn a factored state space [17]. When a factored state space is available, assumptions 2 and 3 generally hold.\n\nNetwork Architecture In Figure 5(a), the architecture of ELDEN for predicting each state factor \\( s_j^{t+1} \\) is illustrated. The process consists of the following steps:\n\n1. Feature Extraction: For each input state factor \\( s_i^t \\), ELDEN utilizes a separate multi-layer perception (MLP) to extract its corresponding feature \\( g_i \\).\n2. Entity Interaction: ELDEN employs a multi-head self-attention module to model entity interactions and generates a set of transformed features \\( h_i \\) that incorporate information from other state factors.\n3. Prediction using Multi-Head Attention: With \\( h_j \\) as the query, ELDEN utilizes a multi-head attention module to compute the prediction \\( \\hat{p}(s_j^{t+1}|s_t, a_t) \\) for each state factor. For continuous state factor, \\( \\hat{p}(s_j^{t+1}) \\) is modeled as a normal distribution with the mean computed by the network and a fixed variance equal to 1. For discrete factor, \\( \\hat{p}(s_j^{t+1}) \\) is a categorical distribution with network outputs as class probabilities.\n\nThroughout the prediction process, there are a total of \\( N \\) such networks in ELDEN, with each network responsible for predicting a separate state factor \\( s_j^{t+1} \\).\n\nThe training loss for the dynamics model is:\n\n$$ L = -\\sum_{j=1}^{N} \\log \\hat{p}(s_j^{t+1}|s_t, a_t) + \\lambda \\sum_{i,j} \\frac{\\partial s_i^t}{\\partial s_j^{t+1}} $$\n\nwhere \\( \\lambda \\) is the coefficient for partial derivative regularization.\n\n## B Environment Details\n\nIn this section, we provide a detailed description of the environment, including its semantic stages representing internal progress toward task completion, state space, and action space. We also highlight that while each task consists of multiple semantic stages, agents do not have access to this information. The learning signal for agents is solely based on a sparse reward of 0 or 1, indicating whether the task has been completed or not. Additionally, in each environment, the poses of all environment entities are randomly initialized for each episode.\n\nMeanwhile, as ELDEN focuses on exploring novel local dependencies between environment entities, in all environments, the action space consists of hard-coded skills to increase the probability of entity interactions and bypass navigation challenges under sparse rewards. Extending ELDEN to explore local dependency and learn such skills simultaneously would be an important direction for future work.\n\nThawing As shown in Fig. 6(a), the Thawing environment consists of a sink, a refrigerator, and a frozen fish. The task requires the agent to complete the following stages: (1) open the refrigerator, (2) take the frozen fish out of the refrigerator, and (3) put the fish into the sink to thaw it. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) the positions of all environment entities.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "A ELDEN details", "md": "## A ELDEN details"}, {"type": "text", "value": "Assumptions We summarize our assumptions on the MDP as follows:\n\n1. The state space can be factored as \\( S = S1 \\times \\ldots \\times SN \\).\n2. The transition of each state factor is independent, i.e., the dynamics can be represented as: $ P(s_{t+1}|s_t, a_t) = \\prod_{n=1}^{N} p(s_n^{t+1}|Pa(S_n^{t+1})). $\n3. There is no instantaneous dependency between state factors at the same time step \\( t \\), i.e., no dependency such as \\( s_i^t \\rightarrow s_j^t \\) for any \\( i, j \\).\n\nFor assumption 1, factored state space is commonly employed in causality literature and applies to many simulated or robotics environments. In cases where low-level observations or partial observability are present, disentangled representation or causal representation methods can be utilized to learn a factored state space [17]. When a factored state space is available, assumptions 2 and 3 generally hold.\n\nNetwork Architecture In Figure 5(a), the architecture of ELDEN for predicting each state factor \\( s_j^{t+1} \\) is illustrated. The process consists of the following steps:\n\n1. Feature Extraction: For each input state factor \\( s_i^t \\), ELDEN utilizes a separate multi-layer perception (MLP) to extract its corresponding feature \\( g_i \\).\n2. Entity Interaction: ELDEN employs a multi-head self-attention module to model entity interactions and generates a set of transformed features \\( h_i \\) that incorporate information from other state factors.\n3. Prediction using Multi-Head Attention: With \\( h_j \\) as the query, ELDEN utilizes a multi-head attention module to compute the prediction \\( \\hat{p}(s_j^{t+1}|s_t, a_t) \\) for each state factor. For continuous state factor, \\( \\hat{p}(s_j^{t+1}) \\) is modeled as a normal distribution with the mean computed by the network and a fixed variance equal to 1. For discrete factor, \\( \\hat{p}(s_j^{t+1}) \\) is a categorical distribution with network outputs as class probabilities.\n\nThroughout the prediction process, there are a total of \\( N \\) such networks in ELDEN, with each network responsible for predicting a separate state factor \\( s_j^{t+1} \\).\n\nThe training loss for the dynamics model is:\n\n$$ L = -\\sum_{j=1}^{N} \\log \\hat{p}(s_j^{t+1}|s_t, a_t) + \\lambda \\sum_{i,j} \\frac{\\partial s_i^t}{\\partial s_j^{t+1}} $$\n\nwhere \\( \\lambda \\) is the coefficient for partial derivative regularization.", "md": "Assumptions We summarize our assumptions on the MDP as follows:\n\n1. The state space can be factored as \\( S = S1 \\times \\ldots \\times SN \\).\n2. The transition of each state factor is independent, i.e., the dynamics can be represented as: $ P(s_{t+1}|s_t, a_t) = \\prod_{n=1}^{N} p(s_n^{t+1}|Pa(S_n^{t+1})). $\n3. There is no instantaneous dependency between state factors at the same time step \\( t \\), i.e., no dependency such as \\( s_i^t \\rightarrow s_j^t \\) for any \\( i, j \\).\n\nFor assumption 1, factored state space is commonly employed in causality literature and applies to many simulated or robotics environments. In cases where low-level observations or partial observability are present, disentangled representation or causal representation methods can be utilized to learn a factored state space [17]. When a factored state space is available, assumptions 2 and 3 generally hold.\n\nNetwork Architecture In Figure 5(a), the architecture of ELDEN for predicting each state factor \\( s_j^{t+1} \\) is illustrated. The process consists of the following steps:\n\n1. Feature Extraction: For each input state factor \\( s_i^t \\), ELDEN utilizes a separate multi-layer perception (MLP) to extract its corresponding feature \\( g_i \\).\n2. Entity Interaction: ELDEN employs a multi-head self-attention module to model entity interactions and generates a set of transformed features \\( h_i \\) that incorporate information from other state factors.\n3. Prediction using Multi-Head Attention: With \\( h_j \\) as the query, ELDEN utilizes a multi-head attention module to compute the prediction \\( \\hat{p}(s_j^{t+1}|s_t, a_t) \\) for each state factor. For continuous state factor, \\( \\hat{p}(s_j^{t+1}) \\) is modeled as a normal distribution with the mean computed by the network and a fixed variance equal to 1. For discrete factor, \\( \\hat{p}(s_j^{t+1}) \\) is a categorical distribution with network outputs as class probabilities.\n\nThroughout the prediction process, there are a total of \\( N \\) such networks in ELDEN, with each network responsible for predicting a separate state factor \\( s_j^{t+1} \\).\n\nThe training loss for the dynamics model is:\n\n$$ L = -\\sum_{j=1}^{N} \\log \\hat{p}(s_j^{t+1}|s_t, a_t) + \\lambda \\sum_{i,j} \\frac{\\partial s_i^t}{\\partial s_j^{t+1}} $$\n\nwhere \\( \\lambda \\) is the coefficient for partial derivative regularization."}, {"type": "heading", "lvl": 2, "value": "B Environment Details", "md": "## B Environment Details"}, {"type": "text", "value": "In this section, we provide a detailed description of the environment, including its semantic stages representing internal progress toward task completion, state space, and action space. We also highlight that while each task consists of multiple semantic stages, agents do not have access to this information. The learning signal for agents is solely based on a sparse reward of 0 or 1, indicating whether the task has been completed or not. Additionally, in each environment, the poses of all environment entities are randomly initialized for each episode.\n\nMeanwhile, as ELDEN focuses on exploring novel local dependencies between environment entities, in all environments, the action space consists of hard-coded skills to increase the probability of entity interactions and bypass navigation challenges under sparse rewards. Extending ELDEN to explore local dependency and learn such skills simultaneously would be an important direction for future work.\n\nThawing As shown in Fig. 6(a), the Thawing environment consists of a sink, a refrigerator, and a frozen fish. The task requires the agent to complete the following stages: (1) open the refrigerator, (2) take the frozen fish out of the refrigerator, and (3) put the fish into the sink to thaw it. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) the positions of all environment entities.", "md": "In this section, we provide a detailed description of the environment, including its semantic stages representing internal progress toward task completion, state space, and action space. We also highlight that while each task consists of multiple semantic stages, agents do not have access to this information. The learning signal for agents is solely based on a sparse reward of 0 or 1, indicating whether the task has been completed or not. Additionally, in each environment, the poses of all environment entities are randomly initialized for each episode.\n\nMeanwhile, as ELDEN focuses on exploring novel local dependencies between environment entities, in all environments, the action space consists of hard-coded skills to increase the probability of entity interactions and bypass navigation challenges under sparse rewards. Extending ELDEN to explore local dependency and learn such skills simultaneously would be an important direction for future work.\n\nThawing As shown in Fig. 6(a), the Thawing environment consists of a sink, a refrigerator, and a frozen fish. The task requires the agent to complete the following stages: (1) open the refrigerator, (2) take the frozen fish out of the refrigerator, and (3) put the fish into the sink to thaw it. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) the positions of all environment entities."}]}, {"page": 15, "text": "Figure 5: The dynamics model of each local dependency detection method. (a) The dynamics model\nof ELDEN for predicting sj      t+1. Notice that each network predicts sj      t+1 only, and there are N such\nnetworks in total, each responsible for predicting one state factor in st+1. For visual simplicity,\nthe \u201c\u00d7N\u201d symbol is only shown in (a). (b) pCMI computes p(sj                t+1|st, at) and p(sj  t+1|st \\ si t, at)\nby manually setting the binary mask M to different values, where \u2297                     represents element-wise\nmultiplication. (c) For Input Mask, M is learned to condition on (st, at) and is regularized to use\nas few inputs as possible. (d) For Attn Mask, M also conditions on (st, at) but is applied to the\nattention score in the self-attention module.\n(iii) the thawing status of the fish, and (iv) whether the refrigerator door is opened. The discrete\naction space consists of (i) moving to a specified environment entity, (ii) picking up / dropping down\nthe fish, and (iii) opening / closing the refrigerator door.\nCarWash        As shown in Fig. 6(b), the CarWash environment consists of a car, a sink, a bucket, a\nshelf, a rag, and a piece of soap. The task requires the agent to complete the following stages: (1)\ntake the rag off the shelf, (2) put it in the sink, (3) toggle the sink to soak the rag up, (4) clean the\ncar with the soaked rag, (5) take the soap off the self, and (6) clean the rag with the soap inside the\nbucket. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) the positions\nof all environment entities, (iii) the soak status of the rag, (iv) the cleanness of the rag and the car,\nand (iv) whether the sink is toggled. The discrete action space consists of (i) moving to a specified\nenvironment entity, (ii) picking up / dropping down the rag, (iii) toggling the sink, and (iii) picking\nup / dropping down the soap.\n2D Minecraft       As shown in Fig. 6(c), the environment has complex chained dependencies \u2014 to get\nthe gem, the agent needs to\n                                                        15", "md": "Figure 5: The dynamics model of each local dependency detection method.\n\n(a) The dynamics model of ELDEN for predicting $$s_{j}^{t+1}$$. Notice that each network predicts $$s_{j}^{t+1}$$ only, and there are N such networks in total, each responsible for predicting one state factor in $$s_{t+1}$$. For visual simplicity, the \u201c\u00d7N\u201d symbol is only shown in (a).\n\n(b) pCMI computes $$p(s_{j}^{t+1}|s_{t}, a_{t})$$ and $$p(s_{j}^{t+1}|s_{t} \\,|\\, s_{i}^{t}, a_{t})$$ by manually setting the binary mask M to different values, where \u2297 represents element-wise multiplication.\n\n(c) For Input Mask, M is learned to condition on $$(s_{t}, a_{t})$$ and is regularized to use as few inputs as possible.\n\n(d) For Attn Mask, M also conditions on $$(s_{t}, a_{t})$$ but is applied to the attention score in the self-attention module.\n\n(iii) the thawing status of the fish, and (iv) whether the refrigerator door is opened. The discrete action space consists of (i) moving to a specified environment entity, (ii) picking up / dropping down the fish, and (iii) opening / closing the refrigerator door.\n\nCarWash As shown in Fig. 6(b), the CarWash environment consists of a car, a sink, a bucket, a shelf, a rag, and a piece of soap. The task requires the agent to complete the following stages: (1) take the rag off the shelf, (2) put it in the sink, (3) toggle the sink to soak the rag up, (4) clean the car with the soaked rag, (5) take the soap off the self, and (6) clean the rag with the soap inside the bucket. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) the positions of all environment entities, (iii) the soak status of the rag, (iv) the cleanness of the rag and the car, and (iv) whether the sink is toggled. The discrete action space consists of (i) moving to a specified environment entity, (ii) picking up / dropping down the rag, (iii) toggling the sink, and (iii) picking up / dropping down the soap.\n\n2D Minecraft As shown in Fig. 6(c), the environment has complex chained dependencies \u2014 to get the gem, the agent needs to\n\n15", "images": [], "items": [{"type": "text", "value": "Figure 5: The dynamics model of each local dependency detection method.\n\n(a) The dynamics model of ELDEN for predicting $$s_{j}^{t+1}$$. Notice that each network predicts $$s_{j}^{t+1}$$ only, and there are N such networks in total, each responsible for predicting one state factor in $$s_{t+1}$$. For visual simplicity, the \u201c\u00d7N\u201d symbol is only shown in (a).\n\n(b) pCMI computes $$p(s_{j}^{t+1}|s_{t}, a_{t})$$ and $$p(s_{j}^{t+1}|s_{t} \\,|\\, s_{i}^{t}, a_{t})$$ by manually setting the binary mask M to different values, where \u2297 represents element-wise multiplication.\n\n(c) For Input Mask, M is learned to condition on $$(s_{t}, a_{t})$$ and is regularized to use as few inputs as possible.\n\n(d) For Attn Mask, M also conditions on $$(s_{t}, a_{t})$$ but is applied to the attention score in the self-attention module.\n\n(iii) the thawing status of the fish, and (iv) whether the refrigerator door is opened. The discrete action space consists of (i) moving to a specified environment entity, (ii) picking up / dropping down the fish, and (iii) opening / closing the refrigerator door.\n\nCarWash As shown in Fig. 6(b), the CarWash environment consists of a car, a sink, a bucket, a shelf, a rag, and a piece of soap. The task requires the agent to complete the following stages: (1) take the rag off the shelf, (2) put it in the sink, (3) toggle the sink to soak the rag up, (4) clean the car with the soaked rag, (5) take the soap off the self, and (6) clean the rag with the soap inside the bucket. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) the positions of all environment entities, (iii) the soak status of the rag, (iv) the cleanness of the rag and the car, and (iv) whether the sink is toggled. The discrete action space consists of (i) moving to a specified environment entity, (ii) picking up / dropping down the rag, (iii) toggling the sink, and (iii) picking up / dropping down the soap.\n\n2D Minecraft As shown in Fig. 6(c), the environment has complex chained dependencies \u2014 to get the gem, the agent needs to\n\n15", "md": "Figure 5: The dynamics model of each local dependency detection method.\n\n(a) The dynamics model of ELDEN for predicting $$s_{j}^{t+1}$$. Notice that each network predicts $$s_{j}^{t+1}$$ only, and there are N such networks in total, each responsible for predicting one state factor in $$s_{t+1}$$. For visual simplicity, the \u201c\u00d7N\u201d symbol is only shown in (a).\n\n(b) pCMI computes $$p(s_{j}^{t+1}|s_{t}, a_{t})$$ and $$p(s_{j}^{t+1}|s_{t} \\,|\\, s_{i}^{t}, a_{t})$$ by manually setting the binary mask M to different values, where \u2297 represents element-wise multiplication.\n\n(c) For Input Mask, M is learned to condition on $$(s_{t}, a_{t})$$ and is regularized to use as few inputs as possible.\n\n(d) For Attn Mask, M also conditions on $$(s_{t}, a_{t})$$ but is applied to the attention score in the self-attention module.\n\n(iii) the thawing status of the fish, and (iv) whether the refrigerator door is opened. The discrete action space consists of (i) moving to a specified environment entity, (ii) picking up / dropping down the fish, and (iii) opening / closing the refrigerator door.\n\nCarWash As shown in Fig. 6(b), the CarWash environment consists of a car, a sink, a bucket, a shelf, a rag, and a piece of soap. The task requires the agent to complete the following stages: (1) take the rag off the shelf, (2) put it in the sink, (3) toggle the sink to soak the rag up, (4) clean the car with the soaked rag, (5) take the soap off the self, and (6) clean the rag with the soap inside the bucket. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) the positions of all environment entities, (iii) the soak status of the rag, (iv) the cleanness of the rag and the car, and (iv) whether the sink is toggled. The discrete action space consists of (i) moving to a specified environment entity, (ii) picking up / dropping down the rag, (iii) toggling the sink, and (iii) picking up / dropping down the soap.\n\n2D Minecraft As shown in Fig. 6(c), the environment has complex chained dependencies \u2014 to get the gem, the agent needs to\n\n15"}]}, {"page": 16, "text": "                                                                             butter\n                                                                         meatball\n                                                                                Initial State                  Goal State\n           (a) Thawing                        (b) CarWash                                         (c) Kitchen\n                                                                                                 craft @\n                                                                                                   apply to\n                                                          (d) Minecraft 2D\n                                                    Figure 6: Environments.\n1. get across the river to reach the gem by\n      (a) collecting a unit of grass and crafting a rope,\n      (b) collecting a unit of wood and crafting a bridge with the rope,\n      (c) building the bridge on top of the river;\n2. collect the gem by\n      (a) collecting a unit of wood to craft a wood stick\n      (b) collecting another unit of wood and combining it with the stick to craft a wood pickaxe that\n           is required for collecting the stone,\n      (c) collecting a unit of wood and a unit of stone to craft a stick and then a stone pickaxe that is\n           required for collecting the gem,\n      (d) collecting the gem with the stone pickaxe.\nNotice that all crafting must be conducted at the crafting table. The discrete state space consists of (i)\nthe agent\u2019s position and direction, (ii) an inventory tracking the number of materials and tools that\nthe agent has, and (iii) the positions of all environment entities. The discrete action consists of (i)\npicking up / applying tools to (only effective when the agent faces an environment entity and has the\nnecessary tools to interact with it), (ii) crafting a specified tool (only effective when the agent has\nenough materials and faces the crafting table), and (iii) moving to a specified environment entity.\nKitchen        As shown in Fig. 6(d), in the kitchen environment, there are a robot arm (i.e., the agent), a\npiece of butter, a meatball, a pot, and a stove with its switch. The task requires the agent to complete\nthe following stages: (1) pick and place the butter into the pot, (2) pick and place the pot onto the\nstove, (3) turn on the stove to melt the butter in the pot, (4) pick and place the meatball into the pot\nto cook it, and (5) turn off the stove. Notice that melting the butter is a prerequisite for cooking the\nmeatball, otherwise, it will result in the meatball being overcooked and the task failing. The state\nspace is continuous, consisting of the pose of all objects, the melting status of the butter, and the\ncooking status of the meatball (whether it is raw, cooked, or overcooked). The action space is discrete,\nconsisting of hard-coded skills: moving to [butter, meatball, pot, pot handle, stove, stove switch],\ngrasping, dropping, and toggling the switch. Grasping and toggling are only applicable when the\nend-effector is close to the corresponding environment entities.\nC      Implementation of Local Dependency Detection\nBaselines        We give a detailed description of each baseline as follows:\n                                                                    16", "md": "# Document\n\n$$\\text{butter}$$\n\n$$\\text{meatball}$$\n\n|Initial State|Goal State|\n|---|---|\n|(a) Thawing|(b) CarWash|\n| |(c) Kitchen|\n\nFigure 6: Environments.\n\n1. get across the river to reach the gem by\n\ncollecting a unit of grass and crafting a rope,\n2. collecting a unit of wood and crafting a bridge with the rope,\n3. building the bridge on top of the river;\n\ncollect the gem by\n\ncollecting a unit of wood to craft a wood stick\ncollecting another unit of wood and combining it with the stick to craft a wood pickaxe that is required for collecting the stone,\ncollecting a unit of wood and a unit of stone to craft a stick and then a stone pickaxe that is required for collecting the gem,\ncollecting the gem with the stone pickaxe.\n\nNotice that all crafting must be conducted at the crafting table. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) an inventory tracking the number of materials and tools that the agent has, and (iii) the positions of all environment entities. The discrete action consists of (i) picking up / applying tools to (only effective when the agent faces an environment entity and has the necessary tools to interact with it), (ii) crafting a specified tool (only effective when the agent has enough materials and faces the crafting table), and (iii) moving to a specified environment entity.\n\nKitchen As shown in Fig. 6(d), in the kitchen environment, there are a robot arm (i.e., the agent), a piece of butter, a meatball, a pot, and a stove with its switch. The task requires the agent to complete the following stages: (1) pick and place the butter into the pot, (2) pick and place the pot onto the stove, (3) turn on the stove to melt the butter in the pot, (4) pick and place the meatball into the pot to cook it, and (5) turn off the stove. Notice that melting the butter is a prerequisite for cooking the meatball, otherwise, it will result in the meatball being overcooked and the task failing. The state space is continuous, consisting of the pose of all objects, the melting status of the butter, and the cooking status of the meatball (whether it is raw, cooked, or overcooked). The action space is discrete, consisting of hard-coded skills: moving to [butter, meatball, pot, pot handle, stove, stove switch], grasping, dropping, and toggling the switch. Grasping and toggling are only applicable when the end-effector is close to the corresponding environment entities.\n\nC Implementation of Local Dependency Detection Baselines We give a detailed description of each baseline as follows:\n\n16", "images": [{"name": "page-16-0.jpg", "height": 95, "width": 95, "x": 111, "y": 72}, {"name": "page-16-1.jpg", "height": 95, "width": 95, "x": 212, "y": 71}, {"name": "page-16-3.jpg", "height": 88, "width": 88, "x": 403, "y": 72}, {"name": "page-16-2.jpg", "height": 88, "width": 88, "x": 314, "y": 72}, {"name": "page-16-8.jpg", "height": 25, "width": 13, "x": 179, "y": 235}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "$$\\text{butter}$$\n\n$$\\text{meatball}$$", "md": "$$\\text{butter}$$\n\n$$\\text{meatball}$$"}, {"type": "table", "rows": [["Initial State", "Goal State"], ["(a) Thawing", "(b) CarWash"], ["", "(c) Kitchen"]], "md": "|Initial State|Goal State|\n|---|---|\n|(a) Thawing|(b) CarWash|\n| |(c) Kitchen|", "isPerfectTable": true, "csv": "\"Initial State\",\"Goal State\"\n\"(a) Thawing\",\"(b) CarWash\"\n\"\",\"(c) Kitchen\""}, {"type": "text", "value": "Figure 6: Environments.\n\n1. get across the river to reach the gem by\n\ncollecting a unit of grass and crafting a rope,\n2. collecting a unit of wood and crafting a bridge with the rope,\n3. building the bridge on top of the river;\n\ncollect the gem by\n\ncollecting a unit of wood to craft a wood stick\ncollecting another unit of wood and combining it with the stick to craft a wood pickaxe that is required for collecting the stone,\ncollecting a unit of wood and a unit of stone to craft a stick and then a stone pickaxe that is required for collecting the gem,\ncollecting the gem with the stone pickaxe.\n\nNotice that all crafting must be conducted at the crafting table. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) an inventory tracking the number of materials and tools that the agent has, and (iii) the positions of all environment entities. The discrete action consists of (i) picking up / applying tools to (only effective when the agent faces an environment entity and has the necessary tools to interact with it), (ii) crafting a specified tool (only effective when the agent has enough materials and faces the crafting table), and (iii) moving to a specified environment entity.\n\nKitchen As shown in Fig. 6(d), in the kitchen environment, there are a robot arm (i.e., the agent), a piece of butter, a meatball, a pot, and a stove with its switch. The task requires the agent to complete the following stages: (1) pick and place the butter into the pot, (2) pick and place the pot onto the stove, (3) turn on the stove to melt the butter in the pot, (4) pick and place the meatball into the pot to cook it, and (5) turn off the stove. Notice that melting the butter is a prerequisite for cooking the meatball, otherwise, it will result in the meatball being overcooked and the task failing. The state space is continuous, consisting of the pose of all objects, the melting status of the butter, and the cooking status of the meatball (whether it is raw, cooked, or overcooked). The action space is discrete, consisting of hard-coded skills: moving to [butter, meatball, pot, pot handle, stove, stove switch], grasping, dropping, and toggling the switch. Grasping and toggling are only applicable when the end-effector is close to the corresponding environment entities.\n\nC Implementation of Local Dependency Detection Baselines We give a detailed description of each baseline as follows:\n\n16", "md": "Figure 6: Environments.\n\n1. get across the river to reach the gem by\n\ncollecting a unit of grass and crafting a rope,\n2. collecting a unit of wood and crafting a bridge with the rope,\n3. building the bridge on top of the river;\n\ncollect the gem by\n\ncollecting a unit of wood to craft a wood stick\ncollecting another unit of wood and combining it with the stick to craft a wood pickaxe that is required for collecting the stone,\ncollecting a unit of wood and a unit of stone to craft a stick and then a stone pickaxe that is required for collecting the gem,\ncollecting the gem with the stone pickaxe.\n\nNotice that all crafting must be conducted at the crafting table. The discrete state space consists of (i) the agent\u2019s position and direction, (ii) an inventory tracking the number of materials and tools that the agent has, and (iii) the positions of all environment entities. The discrete action consists of (i) picking up / applying tools to (only effective when the agent faces an environment entity and has the necessary tools to interact with it), (ii) crafting a specified tool (only effective when the agent has enough materials and faces the crafting table), and (iii) moving to a specified environment entity.\n\nKitchen As shown in Fig. 6(d), in the kitchen environment, there are a robot arm (i.e., the agent), a piece of butter, a meatball, a pot, and a stove with its switch. The task requires the agent to complete the following stages: (1) pick and place the butter into the pot, (2) pick and place the pot onto the stove, (3) turn on the stove to melt the butter in the pot, (4) pick and place the meatball into the pot to cook it, and (5) turn off the stove. Notice that melting the butter is a prerequisite for cooking the meatball, otherwise, it will result in the meatball being overcooked and the task failing. The state space is continuous, consisting of the pose of all objects, the melting status of the butter, and the cooking status of the meatball (whether it is raw, cooked, or overcooked). The action space is discrete, consisting of hard-coded skills: moving to [butter, meatball, pot, pot handle, stove, stove switch], grasping, dropping, and toggling the switch. Grasping and toggling are only applicable when the end-effector is close to the corresponding environment entities.\n\nC Implementation of Local Dependency Detection Baselines We give a detailed description of each baseline as follows:\n\n16"}]}, {"page": 17, "text": "  \u2022 pCMI (point-wise conditional mutual information): it considers that the local dependency\n    sit \u2192  sjt+1 exists if their point-wise conditional mutual information is greater than a predefined\n                                          p(sj\n    threshold, i.e., pCMIi,j := log      p(sjt+1|st,at)\n                                                    t,at) \u2265  \u03f5. As shown in Fig. 5(b), to compute pCMIi,j,\n                                            t+1|st\\si\n    Wang et al. [31] uses a manually defined binary mask M \u2208              [0, 1]N to ignore some inputs when\n    predicting sj  t+1: (1) to compute p(sj    t+1|st, at), M uses all inputs (all its entries are set to 1),\n    and (2) to compute p(sj     t+1|st \\ sit, at), the entry for gi is set to 0. When evaluating the local\n    dependency, pCMI needs to compute p(sj           t+1|st \\ si t, at) for every i, and thus its computation\n    cost is N times larger than ELDEN. We also computes pCMI following Seitzer et al. [27], which\n    yields similar performance but is even more computationally expensive compared to the method\n    proposed by Wang et al. [31].\n  \u2022 Attn (attention): it uses the same architecture as ELDEN that is shown in Fig. 5(a). When\n    computing the overall attention score, it averages the attention score across all heads in each\n    module, then computes the likelihood of dependency si          t \u2192  sjt+1 as  N  k=1 cgi,hk \u00b7 chk,sjt+1 where\n    ca,b is the averaged score between the input a and the output b.\n  \u2022 Input Mask: as shown in Fig. 5(c), it also uses a binary mask M except that M is computed\n    from (st, at). During training, to only use necessary inputs for sj       t+1 prediction, M is regularized\n    with the L1 norm on its number of non-zero entries. The Gumbel reparameterization is used to\n    compute the gradient for the binary M [11].\n  \u2022 Attn Mask: as shown in Fig. 5(d), similar to Input Mask, a mask M of size N \u00d7 N is computed\n    from (st, at), but it is applied to the attention score. The mask is regularized with Stochastic\n    Kernel Modulated Dot-Product (SKMDP) proposed by Weiss et al. [32].\nFor modules that are shared by all methods, we use the same architecture for a fair comparison.\nData     For a fair comparison, when training each method, we use the same dataset collected by\na scripted policy, rather than let each method collect its own data, to avoid potential performance\ndifferences caused by data discrepancies. Specifically, we use a scripted policy to expose all potential\nlocal dependencies and collect 500K transitions in each environment.\nNotice that, in exploration with sparse reward experiments, the dynamics models are still trained\nonline, using the transition data collected on its own.\nHyperparameters          The hyperparameters used for evaluating local dependency detection of each\nmethod are provided in Table 3. Unless specified otherwise, the parameters are shared across all\nenvironments.\nD     Evaluating Exploration in Sparse-Reward RL Tasks\nD.1    Implementation\nDuring policy learning, all methods share the same PPO and training hyperparameters, provided in\nTable 4. The hyperparameters for dynamics model setup during policy learning are the same as in\nTable 3 unless specified otherwise.\nD.2    Success Rate Plots\nAs a supplementary to the normalized stage metric used in the main paper, we provide the success\nrate as an additional metric. The success rate learning curves of all methods in the three environments\nare shown in in Fig. 7. Again, ELDEN outperforms and performs comparably with all baselines.\nNotice that, in the CarWash and Kitchen environments, all baselines never succeed throughout the\ntraining (i.e., success rate = 0 for all episodes), leading to training curves that overlap with the x axis.\n                                                        17", "md": "- pCMI (point-wise conditional mutual information): it considers that the local dependency\n$sit \\rightarrow sj_{t+1}$ exists if their point-wise conditional mutual information is greater than a predefined threshold, i.e., $pCMI_{i,j} := \\log \\frac{p(s_{j_{t+1}}|s_t,a_t)}{p(s_{j_{t+1}}|s_t\\setminus s_i)} \\geq \\epsilon$. As shown in Fig. 5(b), to compute $pCMI_{i,j}$, Wang et al. [31] uses a manually defined binary mask $M \\in [0, 1]^N$ to ignore some inputs when predicting $s_{j_{t+1}}$: (1) to compute $p(s_{j_{t+1}}|s_t, a_t)$, $M$ uses all inputs (all its entries are set to 1), and (2) to compute $p(s_{j_{t+1}}|s_t \\setminus s_i, a_t)$, the entry for $s_i$ is set to 0. When evaluating the local dependency, $pCMI$ needs to compute $p(s_{j_{t+1}}|s_t \\setminus s_i, a_t)$ for every $i$, and thus its computation cost is $N$ times larger than ELDEN. We also compute $pCMI$ following Seitzer et al. [27], which yields similar performance but is even more computationally expensive compared to the method proposed by Wang et al. [31].\n- Attn (attention): it uses the same architecture as ELDEN that is shown in Fig. 5(a). When computing the overall attention score, it averages the attention score across all heads in each module, then computes the likelihood of dependency $s_i \\rightarrow s_{j_{t+1}}$ as $\\sum_{k=1}^{N} c_{gi,hk} \\cdot c_{hk,s_{j_{t+1}}}$ where $c_{a,b}$ is the averaged score between the input $a$ and the output $b$.\n- Input Mask: as shown in Fig. 5(c), it also uses a binary mask $M$ except that $M$ is computed from $(s_t, a_t)$. During training, to only use necessary inputs for $s_{j_{t+1}}$ prediction, $M$ is regularized with the L1 norm on its number of non-zero entries. The Gumbel reparameterization is used to compute the gradient for the binary $M$ [11].\n- Attn Mask: as shown in Fig. 5(d), similar to Input Mask, a mask $M$ of size $N \\times N$ is computed from $(s_t, a_t)$, but it is applied to the attention score. The mask is regularized with Stochastic Kernel Modulated Dot-Product (SKMDP) proposed by Weiss et al. [32].\n\nFor modules that are shared by all methods, we use the same architecture for a fair comparison.\n\nData: For a fair comparison, when training each method, we use the same dataset collected by a scripted policy, rather than let each method collect its own data, to avoid potential performance differences caused by data discrepancies. Specifically, we use a scripted policy to expose all potential local dependencies and collect 500K transitions in each environment.\n\nNotice that, in exploration with sparse reward experiments, the dynamics models are still trained online, using the transition data collected on its own.\n\nHyperparameters: The hyperparameters used for evaluating local dependency detection of each method are provided in Table 3. Unless specified otherwise, the parameters are shared across all environments.\n\n## Evaluating Exploration in Sparse-Reward RL Tasks\n\n### Implementation\n\nDuring policy learning, all methods share the same PPO and training hyperparameters, provided in Table 4. The hyperparameters for dynamics model setup during policy learning are the same as in Table 3 unless specified otherwise.\n\n### Success Rate Plots\n\nAs a supplementary to the normalized stage metric used in the main paper, we provide the success rate as an additional metric. The success rate learning curves of all methods in the three environments are shown in in Fig. 7. Again, ELDEN outperforms and performs comparably with all baselines.\n\nNotice that, in the CarWash and Kitchen environments, all baselines never succeed throughout the training (i.e., success rate = 0 for all episodes), leading to training curves that overlap with the x axis.", "images": [], "items": [{"type": "text", "value": "- pCMI (point-wise conditional mutual information): it considers that the local dependency\n$sit \\rightarrow sj_{t+1}$ exists if their point-wise conditional mutual information is greater than a predefined threshold, i.e., $pCMI_{i,j} := \\log \\frac{p(s_{j_{t+1}}|s_t,a_t)}{p(s_{j_{t+1}}|s_t\\setminus s_i)} \\geq \\epsilon$. As shown in Fig. 5(b), to compute $pCMI_{i,j}$, Wang et al. [31] uses a manually defined binary mask $M \\in [0, 1]^N$ to ignore some inputs when predicting $s_{j_{t+1}}$: (1) to compute $p(s_{j_{t+1}}|s_t, a_t)$, $M$ uses all inputs (all its entries are set to 1), and (2) to compute $p(s_{j_{t+1}}|s_t \\setminus s_i, a_t)$, the entry for $s_i$ is set to 0. When evaluating the local dependency, $pCMI$ needs to compute $p(s_{j_{t+1}}|s_t \\setminus s_i, a_t)$ for every $i$, and thus its computation cost is $N$ times larger than ELDEN. We also compute $pCMI$ following Seitzer et al. [27], which yields similar performance but is even more computationally expensive compared to the method proposed by Wang et al. [31].\n- Attn (attention): it uses the same architecture as ELDEN that is shown in Fig. 5(a). When computing the overall attention score, it averages the attention score across all heads in each module, then computes the likelihood of dependency $s_i \\rightarrow s_{j_{t+1}}$ as $\\sum_{k=1}^{N} c_{gi,hk} \\cdot c_{hk,s_{j_{t+1}}}$ where $c_{a,b}$ is the averaged score between the input $a$ and the output $b$.\n- Input Mask: as shown in Fig. 5(c), it also uses a binary mask $M$ except that $M$ is computed from $(s_t, a_t)$. During training, to only use necessary inputs for $s_{j_{t+1}}$ prediction, $M$ is regularized with the L1 norm on its number of non-zero entries. The Gumbel reparameterization is used to compute the gradient for the binary $M$ [11].\n- Attn Mask: as shown in Fig. 5(d), similar to Input Mask, a mask $M$ of size $N \\times N$ is computed from $(s_t, a_t)$, but it is applied to the attention score. The mask is regularized with Stochastic Kernel Modulated Dot-Product (SKMDP) proposed by Weiss et al. [32].\n\nFor modules that are shared by all methods, we use the same architecture for a fair comparison.\n\nData: For a fair comparison, when training each method, we use the same dataset collected by a scripted policy, rather than let each method collect its own data, to avoid potential performance differences caused by data discrepancies. Specifically, we use a scripted policy to expose all potential local dependencies and collect 500K transitions in each environment.\n\nNotice that, in exploration with sparse reward experiments, the dynamics models are still trained online, using the transition data collected on its own.\n\nHyperparameters: The hyperparameters used for evaluating local dependency detection of each method are provided in Table 3. Unless specified otherwise, the parameters are shared across all environments.", "md": "- pCMI (point-wise conditional mutual information): it considers that the local dependency\n$sit \\rightarrow sj_{t+1}$ exists if their point-wise conditional mutual information is greater than a predefined threshold, i.e., $pCMI_{i,j} := \\log \\frac{p(s_{j_{t+1}}|s_t,a_t)}{p(s_{j_{t+1}}|s_t\\setminus s_i)} \\geq \\epsilon$. As shown in Fig. 5(b), to compute $pCMI_{i,j}$, Wang et al. [31] uses a manually defined binary mask $M \\in [0, 1]^N$ to ignore some inputs when predicting $s_{j_{t+1}}$: (1) to compute $p(s_{j_{t+1}}|s_t, a_t)$, $M$ uses all inputs (all its entries are set to 1), and (2) to compute $p(s_{j_{t+1}}|s_t \\setminus s_i, a_t)$, the entry for $s_i$ is set to 0. When evaluating the local dependency, $pCMI$ needs to compute $p(s_{j_{t+1}}|s_t \\setminus s_i, a_t)$ for every $i$, and thus its computation cost is $N$ times larger than ELDEN. We also compute $pCMI$ following Seitzer et al. [27], which yields similar performance but is even more computationally expensive compared to the method proposed by Wang et al. [31].\n- Attn (attention): it uses the same architecture as ELDEN that is shown in Fig. 5(a). When computing the overall attention score, it averages the attention score across all heads in each module, then computes the likelihood of dependency $s_i \\rightarrow s_{j_{t+1}}$ as $\\sum_{k=1}^{N} c_{gi,hk} \\cdot c_{hk,s_{j_{t+1}}}$ where $c_{a,b}$ is the averaged score between the input $a$ and the output $b$.\n- Input Mask: as shown in Fig. 5(c), it also uses a binary mask $M$ except that $M$ is computed from $(s_t, a_t)$. During training, to only use necessary inputs for $s_{j_{t+1}}$ prediction, $M$ is regularized with the L1 norm on its number of non-zero entries. The Gumbel reparameterization is used to compute the gradient for the binary $M$ [11].\n- Attn Mask: as shown in Fig. 5(d), similar to Input Mask, a mask $M$ of size $N \\times N$ is computed from $(s_t, a_t)$, but it is applied to the attention score. The mask is regularized with Stochastic Kernel Modulated Dot-Product (SKMDP) proposed by Weiss et al. [32].\n\nFor modules that are shared by all methods, we use the same architecture for a fair comparison.\n\nData: For a fair comparison, when training each method, we use the same dataset collected by a scripted policy, rather than let each method collect its own data, to avoid potential performance differences caused by data discrepancies. Specifically, we use a scripted policy to expose all potential local dependencies and collect 500K transitions in each environment.\n\nNotice that, in exploration with sparse reward experiments, the dynamics models are still trained online, using the transition data collected on its own.\n\nHyperparameters: The hyperparameters used for evaluating local dependency detection of each method are provided in Table 3. Unless specified otherwise, the parameters are shared across all environments."}, {"type": "heading", "lvl": 2, "value": "Evaluating Exploration in Sparse-Reward RL Tasks", "md": "## Evaluating Exploration in Sparse-Reward RL Tasks"}, {"type": "heading", "lvl": 3, "value": "Implementation", "md": "### Implementation"}, {"type": "text", "value": "During policy learning, all methods share the same PPO and training hyperparameters, provided in Table 4. The hyperparameters for dynamics model setup during policy learning are the same as in Table 3 unless specified otherwise.", "md": "During policy learning, all methods share the same PPO and training hyperparameters, provided in Table 4. The hyperparameters for dynamics model setup during policy learning are the same as in Table 3 unless specified otherwise."}, {"type": "heading", "lvl": 3, "value": "Success Rate Plots", "md": "### Success Rate Plots"}, {"type": "text", "value": "As a supplementary to the normalized stage metric used in the main paper, we provide the success rate as an additional metric. The success rate learning curves of all methods in the three environments are shown in in Fig. 7. Again, ELDEN outperforms and performs comparably with all baselines.\n\nNotice that, in the CarWash and Kitchen environments, all baselines never succeed throughout the training (i.e., success rate = 0 for all episodes), leading to training curves that overlap with the x axis.", "md": "As a supplementary to the normalized stage metric used in the main paper, we provide the success rate as an additional metric. The success rate learning curves of all methods in the three environments are shown in in Fig. 7. Again, ELDEN outperforms and performs comparably with all baselines.\n\nNotice that, in the CarWash and Kitchen environments, all baselines never succeed throughout the training (i.e., success rate = 0 for all episodes), leading to training curves that overlap with the x axis."}]}, {"page": 18, "text": "Table 3: Parameters of the dynamics model training for local dependency detection experiments.\nParameters shared if not specified.\n                                             Name                                       Tasks\n                                                                        Thawing       CarWash      Kitchen\n           environment                  episode length                      20           100         100\n                                            grid size                       10            10         N/A\n                                           optimizer                                    Adam\n                                         learning rate                                3 \u00d7 10\u22124\n             training                      batch size                                     32\n                                     # of training batches                               500k\n                                      # of random seeds                                    3\n                                    mixup Beta parameter                    1             1          N/A\n                                     activation functions                               ReLU\n                                          {MLP}N   i=1                   [64, 64]     [64, 64]    [128, 128]\n                                       \u03bb annealing starts                  50k           50k         100k\n                                       \u03bb annealing ends                   100k          100k         200k\n             ELDEN                               # of heads                                4\n                                                   use bias                              False\n                            attention      key, query, value size           16            16          32\n                                                 output size                64            64         128\n                                               post attn MLP             [64, 64]     [64, 64]    [128, 128]\n                                     attention parameters                         same as ELDEN\n               Input             M regularization coefficient                         1 \u00d7 10\u22122\n               Mask           M regularization annealing starts            50k           50k         100k\n                              M regularization annealing ends             100k          100k         200k\n                                     attention parameters                         same as ELDEN\n               Attn                             signature size                            64\n               Mask         SKPMD           learnable bandwidth                          True\n                                          bandwidth initialization                         1\n             Table 4: Parameters of the Policy Learning. Parameters shared if not specified.\n                                            Name                                      Tasks\n                                                                       Thawing       CarWash     Kitchen\n                                          optimizer                                   Adam\n                                    activation functions                               Tanh\n                                        learning rate                               1 \u00d7 10\u22124\n                                          batch size                                    32\n                PPO                       clip ratio                                    0.1\n                                          MLP size                                  [128, 128]\n                                           GAE \u03bb                                       0.98\n                                         target steps                                   250\n                                           n steps                         60           600        100\n                                     # of environments                     20            20         80\n                                     # of random seeds                                   3\n                               intrinsic reward coefficient \u03b2                            1\n                           # of dynamics update per policy step                          1\n              training            dynamics learning rate                            1 \u00d7 10\u22125\n                                       ensemble size                                     5\n                               level of sample prioritization             N/A           N/A         0.5\n                                   mixup Beta parameter                    0.1          0.1        N/A\n                               partial derivative threshold \u03f5                       3 \u00d7 10\u22124\n                                                            18", "md": "|Name|Tasks|\n|---|---|\n|environment| |\n|episode length|$20$|$100$|$100$|\n|grid size|$10$|$10$|N/A|\n|optimizer|Adam|\n|learning rate|$3 \\times 10^{-4}$|\n|training| |\n|batch size|$32$|\n|# of training batches|$500k$|\n|# of random seeds|$3$|\n|mixup Beta parameter|$1$|$1$|N/A|\n|activation functions|ReLU|\n|{MLP}N i=1|$[64, 64]$|$[64, 64]$|$[128, 128]$|\n|\u03bb annealing starts|$50k$|$50k$|$100k$|\n|\u03bb annealing ends|$100k$|$100k$|$200k$|\n|ELDEN|# of heads|$4$|\n| |use bias|False|\n|attention|key, query, value size|$16$|$16$|$32$|\n| |output size|$64$|$64$|$128$|\n| |post attn MLP|$[64, 64]$|$[64, 64]$|$[128, 128]$|\n| |attention parameters|same as ELDEN|\n|Input|M regularization coefficient|$1 \\times 10^{-2}$|\n|Mask|M regularization annealing starts|$50k$|$50k$|$100k$|\n| |M regularization annealing ends|$100k$|$100k$|$200k$|\n| |attention parameters|same as ELDEN|\n|Attn|signature size|$64$|\n|Mask|SKPMD|learnable bandwidth|True|\n| |bandwidth initialization|$1$|\n\n|Name|Tasks|\n|---|---|\n|optimizer|Adam|\n|activation functions|Tanh|\n|learning rate|$1 \\times 10^{-4}$|\n|batch size|$32$|\n|PPO| |\n|clip ratio|$0.1$|\n|MLP size|$[128, 128]$|\n|GAE \u03bb|$0.98$|\n|target steps|$250$|\n|n steps|$60$|$600$|$100$|\n|# of environments|$20$|$20$|$80$|\n|# of random seeds|$3$|\n|intrinsic reward coefficient \u03b2|$1$|\n|# of dynamics update per policy step|$1$|\n|training|dynamics learning rate|$1 \\times 10^{-5}$|\n| |ensemble size|$5$|\n| |level of sample prioritization|N/A|N/A|$0.5$|\n| |mixup Beta parameter|$0.1$|$0.1$|N/A|\n| |partial derivative threshold \u03f5|$3 \\times 10^{-4}$|", "images": [], "items": [{"type": "table", "rows": [["Name", "Tasks"], ["environment", ""], ["episode length", "$20$", "$100$", "$100$"], ["grid size", "$10$", "$10$", "N/A"], ["optimizer", "Adam"], ["learning rate", "$3 \\times 10^{-4}$"], ["training", ""], ["batch size", "$32$"], ["# of training batches", "$500k$"], ["# of random seeds", "$3$"], ["mixup Beta parameter", "$1$", "$1$", "N/A"], ["activation functions", "ReLU"], ["{MLP}N i=1", "$[64, 64]$", "$[64, 64]$", "$[128, 128]$"], ["\u03bb annealing starts", "$50k$", "$50k$", "$100k$"], ["\u03bb annealing ends", "$100k$", "$100k$", "$200k$"], ["ELDEN", "# of heads", "$4$"], ["", "use bias", "False"], ["attention", "key, query, value size", "$16$", "$16$", "$32$"], ["", "output size", "$64$", "$64$", "$128$"], ["", "post attn MLP", "$[64, 64]$", "$[64, 64]$", "$[128, 128]$"], ["", "attention parameters", "same as ELDEN"], ["Input", "M regularization coefficient", "$1 \\times 10^{-2}$"], ["Mask", "M regularization annealing starts", "$50k$", "$50k$", "$100k$"], ["", "M regularization annealing ends", "$100k$", "$100k$", "$200k$"], ["", "attention parameters", "same as ELDEN"], ["Attn", "signature size", "$64$"], ["Mask", "SKPMD", "learnable bandwidth", "True"], ["", "bandwidth initialization", "$1$"]], "md": "|Name|Tasks|\n|---|---|\n|environment| |\n|episode length|$20$|$100$|$100$|\n|grid size|$10$|$10$|N/A|\n|optimizer|Adam|\n|learning rate|$3 \\times 10^{-4}$|\n|training| |\n|batch size|$32$|\n|# of training batches|$500k$|\n|# of random seeds|$3$|\n|mixup Beta parameter|$1$|$1$|N/A|\n|activation functions|ReLU|\n|{MLP}N i=1|$[64, 64]$|$[64, 64]$|$[128, 128]$|\n|\u03bb annealing starts|$50k$|$50k$|$100k$|\n|\u03bb annealing ends|$100k$|$100k$|$200k$|\n|ELDEN|# of heads|$4$|\n| |use bias|False|\n|attention|key, query, value size|$16$|$16$|$32$|\n| |output size|$64$|$64$|$128$|\n| |post attn MLP|$[64, 64]$|$[64, 64]$|$[128, 128]$|\n| |attention parameters|same as ELDEN|\n|Input|M regularization coefficient|$1 \\times 10^{-2}$|\n|Mask|M regularization annealing starts|$50k$|$50k$|$100k$|\n| |M regularization annealing ends|$100k$|$100k$|$200k$|\n| |attention parameters|same as ELDEN|\n|Attn|signature size|$64$|\n|Mask|SKPMD|learnable bandwidth|True|\n| |bandwidth initialization|$1$|", "isPerfectTable": false, "csv": "\"Name\",\"Tasks\"\n\"environment\",\"\"\n\"episode length\",\"$20$\",\"$100$\",\"$100$\"\n\"grid size\",\"$10$\",\"$10$\",\"N/A\"\n\"optimizer\",\"Adam\"\n\"learning rate\",\"$3 \\times 10^{-4}$\"\n\"training\",\"\"\n\"batch size\",\"$32$\"\n\"# of training batches\",\"$500k$\"\n\"# of random seeds\",\"$3$\"\n\"mixup Beta parameter\",\"$1$\",\"$1$\",\"N/A\"\n\"activation functions\",\"ReLU\"\n\"{MLP}N i=1\",\"$[64, 64]$\",\"$[64, 64]$\",\"$[128, 128]$\"\n\"\u03bb annealing starts\",\"$50k$\",\"$50k$\",\"$100k$\"\n\"\u03bb annealing ends\",\"$100k$\",\"$100k$\",\"$200k$\"\n\"ELDEN\",\"# of heads\",\"$4$\"\n\"\",\"use bias\",\"False\"\n\"attention\",\"key, query, value size\",\"$16$\",\"$16$\",\"$32$\"\n\"\",\"output size\",\"$64$\",\"$64$\",\"$128$\"\n\"\",\"post attn MLP\",\"$[64, 64]$\",\"$[64, 64]$\",\"$[128, 128]$\"\n\"\",\"attention parameters\",\"same as ELDEN\"\n\"Input\",\"M regularization coefficient\",\"$1 \\times 10^{-2}$\"\n\"Mask\",\"M regularization annealing starts\",\"$50k$\",\"$50k$\",\"$100k$\"\n\"\",\"M regularization annealing ends\",\"$100k$\",\"$100k$\",\"$200k$\"\n\"\",\"attention parameters\",\"same as ELDEN\"\n\"Attn\",\"signature size\",\"$64$\"\n\"Mask\",\"SKPMD\",\"learnable bandwidth\",\"True\"\n\"\",\"bandwidth initialization\",\"$1$\""}, {"type": "table", "rows": [["Name", "Tasks"], ["optimizer", "Adam"], ["activation functions", "Tanh"], ["learning rate", "$1 \\times 10^{-4}$"], ["batch size", "$32$"], ["PPO", ""], ["clip ratio", "$0.1$"], ["MLP size", "$[128, 128]$"], ["GAE \u03bb", "$0.98$"], ["target steps", "$250$"], ["n steps", "$60$", "$600$", "$100$"], ["# of environments", "$20$", "$20$", "$80$"], ["# of random seeds", "$3$"], ["intrinsic reward coefficient \u03b2", "$1$"], ["# of dynamics update per policy step", "$1$"], ["training", "dynamics learning rate", "$1 \\times 10^{-5}$"], ["", "ensemble size", "$5$"], ["", "level of sample prioritization", "N/A", "N/A", "$0.5$"], ["", "mixup Beta parameter", "$0.1$", "$0.1$", "N/A"], ["", "partial derivative threshold \u03f5", "$3 \\times 10^{-4}$"]], "md": "|Name|Tasks|\n|---|---|\n|optimizer|Adam|\n|activation functions|Tanh|\n|learning rate|$1 \\times 10^{-4}$|\n|batch size|$32$|\n|PPO| |\n|clip ratio|$0.1$|\n|MLP size|$[128, 128]$|\n|GAE \u03bb|$0.98$|\n|target steps|$250$|\n|n steps|$60$|$600$|$100$|\n|# of environments|$20$|$20$|$80$|\n|# of random seeds|$3$|\n|intrinsic reward coefficient \u03b2|$1$|\n|# of dynamics update per policy step|$1$|\n|training|dynamics learning rate|$1 \\times 10^{-5}$|\n| |ensemble size|$5$|\n| |level of sample prioritization|N/A|N/A|$0.5$|\n| |mixup Beta parameter|$0.1$|$0.1$|N/A|\n| |partial derivative threshold \u03f5|$3 \\times 10^{-4}$|", "isPerfectTable": false, "csv": "\"Name\",\"Tasks\"\n\"optimizer\",\"Adam\"\n\"activation functions\",\"Tanh\"\n\"learning rate\",\"$1 \\times 10^{-4}$\"\n\"batch size\",\"$32$\"\n\"PPO\",\"\"\n\"clip ratio\",\"$0.1$\"\n\"MLP size\",\"$[128, 128]$\"\n\"GAE \u03bb\",\"$0.98$\"\n\"target steps\",\"$250$\"\n\"n steps\",\"$60$\",\"$600$\",\"$100$\"\n\"# of environments\",\"$20$\",\"$20$\",\"$80$\"\n\"# of random seeds\",\"$3$\"\n\"intrinsic reward coefficient \u03b2\",\"$1$\"\n\"# of dynamics update per policy step\",\"$1$\"\n\"training\",\"dynamics learning rate\",\"$1 \\times 10^{-5}$\"\n\"\",\"ensemble size\",\"$5$\"\n\"\",\"level of sample prioritization\",\"N/A\",\"N/A\",\"$0.5$\"\n\"\",\"mixup Beta parameter\",\"$0.1$\",\"$0.1$\",\"N/A\"\n\"\",\"partial derivative threshold \u03f5\",\"$3 \\times 10^{-4}$\""}]}, {"page": 19, "text": "         (a) Thawing                 (b) CarWash                    (c) Craft                  (d) Kitchen\nFigure 7: Learning curve of ELDEN (ours) compared to baseline approaches (mean \u00b1 std dev of\nsuccess rate across three random seeds). For CarWash, Craft, and Kitchen, the success rates for all\nbaselines are zero throughout the training, overlapping with the x axis.\n       (a) a navigation task with the goal in green        (b) ELDEN performs worse than Dynamics Curiosity\n             Figure 8: We demonstrate a failure mode of our method on a navigation task.\nE     Failure Modes of ELDEN\nAs mentioned in the main paper, ELDEN may have limited advantages for tasks that require precise\ncontrol of a specific environment entity. One such example is navigation, where the agent needs to\nreach a very specific point in space that has no particular semantic meaning. We empirically examine\nthis statement in the Minigrid environment [7], where the agent needs to navigate to the green goal\npoint in an empty room through primitive actions (turn left, turn right, and move forward), as shown\nin Fig. 8(a). We compare ELDEN against Dynamics Curiosity and Vanilla PPO, and present the result\nin Fig. 8(b). Since this environment is relatively simple, all three methods are eventually able to solve\nthe task. However, the Dynamics Curiosity converges faster than ELDEN, showing that ELDEN is\nindeed not as capable as curiosity-driven explorations in tasks that focus on precise control rather than\nexploring dependencies between environment entities. The Vanilla PPO converges slowest, indicating\nthat even in the Empty environment, ELDEN still has advantages over purely random exploration.\nF     Compute Architecture\nThe experiments were conducted on machines of the following configurations:\n        \u2022 Nvidia 2080 Ti GPU; AMD Ryzen Threadripper 3970X 32-Core Processor\n        \u2022 Nvidia A40 GPU; Intel(R) Xeon(R) Gold 6342 CPU @2.80GHz\n        \u2022 Nvidia A100 GPU; Intel(R) Xeon(R) Gold 6342 CPU @2.80GHz\n                                                         19", "md": "```markdown\n(a) Thawing                 (b) CarWash                    (c) Craft                  (d) Kitchen\n\n$$\n\\text{Figure 7: Learning curve of ELDEN (ours) compared to baseline approaches (mean \u00b1 std dev of success rate across three random seeds). For CarWash, Craft, and Kitchen, the success rates for all baselines are zero throughout the training, overlapping with the x axis.}\n$$\n\n(a) a navigation task with the goal in green        (b) ELDEN performs worse than Dynamics Curiosity\n\n$$\n\\text{Figure 8: We demonstrate a failure mode of our method on a navigation task.}\n$$\n\n$$\n\\text{Failure Modes of ELDEN}\n$$\n\nAs mentioned in the main paper, ELDEN may have limited advantages for tasks that require precise control of a specific environment entity. One such example is navigation, where the agent needs to reach a very specific point in space that has no particular semantic meaning. We empirically examine this statement in the Minigrid environment [7], where the agent needs to navigate to the green goal point in an empty room through primitive actions (turn left, turn right, and move forward), as shown in Fig. 8(a). We compare ELDEN against Dynamics Curiosity and Vanilla PPO, and present the result in Fig. 8(b). Since this environment is relatively simple, all three methods are eventually able to solve the task. However, the Dynamics Curiosity converges faster than ELDEN, showing that ELDEN is indeed not as capable as curiosity-driven explorations in tasks that focus on precise control rather than exploring dependencies between environment entities. The Vanilla PPO converges slowest, indicating that even in the Empty environment, ELDEN still has advantages over purely random exploration.\n\n$$\n\\text{Compute Architecture}\n$$\n\nThe experiments were conducted on machines of the following configurations:\n| GPU             | CPU                               |\n|-----------------|-----------------------------------|\n| Nvidia 2080 Ti  | AMD Ryzen Threadripper 3970X      |\n| Nvidia A40      | Intel(R) Xeon(R) Gold 6342 @2.80GHz |\n| Nvidia A100     | Intel(R) Xeon(R) Gold 6342 @2.80GHz |\n\n19\n```", "images": [{"name": "page-19-0.jpg", "height": 98, "width": 98, "x": 159, "y": 236}], "items": [{"type": "text", "value": "```markdown\n(a) Thawing                 (b) CarWash                    (c) Craft                  (d) Kitchen\n\n$$\n\\text{Figure 7: Learning curve of ELDEN (ours) compared to baseline approaches (mean \u00b1 std dev of success rate across three random seeds). For CarWash, Craft, and Kitchen, the success rates for all baselines are zero throughout the training, overlapping with the x axis.}\n$$\n\n(a) a navigation task with the goal in green        (b) ELDEN performs worse than Dynamics Curiosity\n\n$$\n\\text{Figure 8: We demonstrate a failure mode of our method on a navigation task.}\n$$\n\n$$\n\\text{Failure Modes of ELDEN}\n$$\n\nAs mentioned in the main paper, ELDEN may have limited advantages for tasks that require precise control of a specific environment entity. One such example is navigation, where the agent needs to reach a very specific point in space that has no particular semantic meaning. We empirically examine this statement in the Minigrid environment [7], where the agent needs to navigate to the green goal point in an empty room through primitive actions (turn left, turn right, and move forward), as shown in Fig. 8(a). We compare ELDEN against Dynamics Curiosity and Vanilla PPO, and present the result in Fig. 8(b). Since this environment is relatively simple, all three methods are eventually able to solve the task. However, the Dynamics Curiosity converges faster than ELDEN, showing that ELDEN is indeed not as capable as curiosity-driven explorations in tasks that focus on precise control rather than exploring dependencies between environment entities. The Vanilla PPO converges slowest, indicating that even in the Empty environment, ELDEN still has advantages over purely random exploration.\n\n$$\n\\text{Compute Architecture}\n$$\n\nThe experiments were conducted on machines of the following configurations:", "md": "```markdown\n(a) Thawing                 (b) CarWash                    (c) Craft                  (d) Kitchen\n\n$$\n\\text{Figure 7: Learning curve of ELDEN (ours) compared to baseline approaches (mean \u00b1 std dev of success rate across three random seeds). For CarWash, Craft, and Kitchen, the success rates for all baselines are zero throughout the training, overlapping with the x axis.}\n$$\n\n(a) a navigation task with the goal in green        (b) ELDEN performs worse than Dynamics Curiosity\n\n$$\n\\text{Figure 8: We demonstrate a failure mode of our method on a navigation task.}\n$$\n\n$$\n\\text{Failure Modes of ELDEN}\n$$\n\nAs mentioned in the main paper, ELDEN may have limited advantages for tasks that require precise control of a specific environment entity. One such example is navigation, where the agent needs to reach a very specific point in space that has no particular semantic meaning. We empirically examine this statement in the Minigrid environment [7], where the agent needs to navigate to the green goal point in an empty room through primitive actions (turn left, turn right, and move forward), as shown in Fig. 8(a). We compare ELDEN against Dynamics Curiosity and Vanilla PPO, and present the result in Fig. 8(b). Since this environment is relatively simple, all three methods are eventually able to solve the task. However, the Dynamics Curiosity converges faster than ELDEN, showing that ELDEN is indeed not as capable as curiosity-driven explorations in tasks that focus on precise control rather than exploring dependencies between environment entities. The Vanilla PPO converges slowest, indicating that even in the Empty environment, ELDEN still has advantages over purely random exploration.\n\n$$\n\\text{Compute Architecture}\n$$\n\nThe experiments were conducted on machines of the following configurations:"}, {"type": "table", "rows": [["GPU", "CPU"], ["Nvidia 2080 Ti", "AMD Ryzen Threadripper 3970X"], ["Nvidia A40", "Intel(R) Xeon(R) Gold 6342 @2.80GHz"], ["Nvidia A100", "Intel(R) Xeon(R) Gold 6342 @2.80GHz"]], "md": "| GPU             | CPU                               |\n|-----------------|-----------------------------------|\n| Nvidia 2080 Ti  | AMD Ryzen Threadripper 3970X      |\n| Nvidia A40      | Intel(R) Xeon(R) Gold 6342 @2.80GHz |\n| Nvidia A100     | Intel(R) Xeon(R) Gold 6342 @2.80GHz |", "isPerfectTable": true, "csv": "\"GPU\",\"CPU\"\n\"Nvidia 2080 Ti\",\"AMD Ryzen Threadripper 3970X\"\n\"Nvidia A40\",\"Intel(R) Xeon(R) Gold 6342 @2.80GHz\"\n\"Nvidia A100\",\"Intel(R) Xeon(R) Gold 6342 @2.80GHz\""}, {"type": "text", "value": "19\n```", "md": "19\n```"}]}], "job_id": "7c5af290-5503-4777-8d88-2e7c228784c9", "file_path": "./corpus/2310.08702.pdf"}