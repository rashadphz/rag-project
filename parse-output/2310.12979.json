{"pages": [{"page": 1, "text": "                                           Predicting a Protein\u2019s Stability\n                                               under a Million Mutations\n                                     Jeffrey Ouyang-Zhang                             Daniel J. Diaz\n                                           UT Austin                                     UT Austin\n                                     jozhang@utexas.edu                         danny.diaz@utexas.edu\narXiv:2310.12979v2  [q-bio.BM]  30 Oct 2023\n                                        Adam R. Klivans                             Philipp Kr\u00e4henb\u00fchl\n                                            UT Austin                                     UT Austin\n                                   klivans@cs.utexas.edu                         philkr@cs.utexas.edu\n                                                                  Abstract\n                             Stabilizing proteins is a foundational step in protein engineering. However, the\n                             evolutionary pressure of all extant proteins makes identifying the scarce number of\n                             mutations that will improve thermodynamic stability challenging. Deep learning\n                             has recently emerged as a powerful tool for identifying promising mutations. Exist-\n                             ing approaches, however, are computationally expensive, as the number of model\n                             inferences scales with the number of mutations queried. Our main contribution is\n                             a simple, parallel decoding algorithm. Our Mutate Everything is capable of pre-\n                             dicting the effect of all single and double mutations in one forward pass. It is even\n                             versatile enough to predict higher-order mutations with minimal computational\n                             overhead. We build our Mutate Everything on top of ESM2 and AlphaFold, neither\n                             of which were trained to predict thermodynamic stability. We trained on the Mega-\n                             Scale cDNA proteolysis dataset and achieved state-of-the-art performance on single\n                             and higher-order mutations on S669, ProTherm, and ProteinGym datasets. Our\n                             code is available at https://github.com/jozhang97/MutateEverything.\n                    1    Introduction\n                    Protein engineering is the discipline of mutating a natural protein sequence to improve properties for\n                    industrial [5, 78] and pharmaceutical applications [2, 25, 41]. However, evolution simultaneously\n                    optimizes several properties of a protein within its native environment, resulting in proteins with\n                    marginal thermodynamic stability (\u223c      5-15 kcal/mol) [39] which become non-functional in an in-\n                    dustrial setting. Therefore, repurposing a natural protein for biotechnological applications usually\n                    begins with identifying non-deleterious mutations that stabilize the structure. With a stabilized\n                    structure, downstream engineering goals, which often require exploring destabilizing mutations,\n                    become tractable. Historically, this process has been bottlenecked by the requirements of extensive\n                    laboratory characterization of rational designs [33, 79] or directed evolution libraries [4, 12, 21, 22].\n                    The recent explosion in biological data [9, 29, 72, 73] has enabled deep learning frameworks to\n                    accelerate the identification of stabilizing mutations. A successfully stabilized protein often requires\n                    several mutations. However, current frameworks do not take into account epistatic interactions\n                    between multiple mutations. Thus, to significantly accelerate protein engineering, it is critical to\n                    efficiently navigate the epistatic landscape of higher-order mutations [10, 69]. However, due to its\n                    combinatorial nature, thorough exploration quickly becomes computationally prohibitive.\n                    In this paper, we introduce Mutate Everything to directly predict changes in thermodynamic stability\n                    (\u2206\u2206G) for all single and higher-order mutations jointly. Mutate Everything is a parallel decoding\n                    algorithm that works in conjunction with a sequence model for predicting thermodynamic stability.\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Predicting a Protein\u2019s Stability under a Million Mutations\n\n# Predicting a Protein\u2019s Stability under a Million Mutations\n\nJeffrey Ouyang-Zhang UT Austin jozhang@utexas.edu\n\nDaniel J. Diaz UT Austin danny.diaz@utexas.edu\n\nAdam R. Klivans UT Austin klivans@cs.utexas.edu\n\nPhilipp Kr\u00e4henb\u00fchl UT Austin philkr@cs.utexas.edu\n\narXiv:2310.12979v2 [q-bio.BM] 30 Oct 2023\n\n## Abstract\n\nStabilizing proteins is a foundational step in protein engineering. However, the evolutionary pressure of all extant proteins makes identifying the scarce number of mutations that will improve thermodynamic stability challenging. Deep learning has recently emerged as a powerful tool for identifying promising mutations. Existing approaches, however, are computationally expensive, as the number of model inferences scales with the number of mutations queried. Our main contribution is a simple, parallel decoding algorithm. Our Mutate Everything is capable of predicting the effect of all single and double mutations in one forward pass. It is even versatile enough to predict higher-order mutations with minimal computational overhead. We build our Mutate Everything on top of ESM2 and AlphaFold, neither of which were trained to predict thermodynamic stability. We trained on the Mega-Scale cDNA proteolysis dataset and achieved state-of-the-art performance on single and higher-order mutations on S669, ProTherm, and ProteinGym datasets. Our code is available at https://github.com/jozhang97/MutateEverything.\n\n### Introduction\n\nProtein engineering is the discipline of mutating a natural protein sequence to improve properties for industrial and pharmaceutical applications. However, evolution simultaneously optimizes several properties of a protein within its native environment, resulting in proteins with marginal thermodynamic stability (~ 5-15 kcal/mol) which become non-functional in an industrial setting. Therefore, repurposing a natural protein for biotechnological applications usually begins with identifying non-deleterious mutations that stabilize the structure. With a stabilized structure, downstream engineering goals, which often require exploring destabilizing mutations, become tractable. Historically, this process has been bottlenecked by the requirements of extensive laboratory characterization of rational designs or directed evolution libraries. The recent explosion in biological data has enabled deep learning frameworks to accelerate the identification of stabilizing mutations. A successfully stabilized protein often requires several mutations. However, current frameworks do not take into account epistatic interactions between multiple mutations. Thus, to significantly accelerate protein engineering, it is critical to efficiently navigate the epistatic landscape of higher-order mutations. However, due to its combinatorial nature, thorough exploration quickly becomes computationally prohibitive.\n\nIn this paper, we introduce Mutate Everything to directly predict changes in thermodynamic stability ($$\\Delta\\Delta G$$) for all single and higher-order mutations jointly. Mutate Everything is a parallel decoding algorithm that works in conjunction with a sequence model for predicting thermodynamic stability.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Predicting a Protein\u2019s Stability under a Million Mutations", "md": "# Predicting a Protein\u2019s Stability under a Million Mutations"}, {"type": "heading", "lvl": 1, "value": "Predicting a Protein\u2019s Stability under a Million Mutations", "md": "# Predicting a Protein\u2019s Stability under a Million Mutations"}, {"type": "text", "value": "Jeffrey Ouyang-Zhang UT Austin jozhang@utexas.edu\n\nDaniel J. Diaz UT Austin danny.diaz@utexas.edu\n\nAdam R. Klivans UT Austin klivans@cs.utexas.edu\n\nPhilipp Kr\u00e4henb\u00fchl UT Austin philkr@cs.utexas.edu\n\narXiv:2310.12979v2 [q-bio.BM] 30 Oct 2023", "md": "Jeffrey Ouyang-Zhang UT Austin jozhang@utexas.edu\n\nDaniel J. Diaz UT Austin danny.diaz@utexas.edu\n\nAdam R. Klivans UT Austin klivans@cs.utexas.edu\n\nPhilipp Kr\u00e4henb\u00fchl UT Austin philkr@cs.utexas.edu\n\narXiv:2310.12979v2 [q-bio.BM] 30 Oct 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Stabilizing proteins is a foundational step in protein engineering. However, the evolutionary pressure of all extant proteins makes identifying the scarce number of mutations that will improve thermodynamic stability challenging. Deep learning has recently emerged as a powerful tool for identifying promising mutations. Existing approaches, however, are computationally expensive, as the number of model inferences scales with the number of mutations queried. Our main contribution is a simple, parallel decoding algorithm. Our Mutate Everything is capable of predicting the effect of all single and double mutations in one forward pass. It is even versatile enough to predict higher-order mutations with minimal computational overhead. We build our Mutate Everything on top of ESM2 and AlphaFold, neither of which were trained to predict thermodynamic stability. We trained on the Mega-Scale cDNA proteolysis dataset and achieved state-of-the-art performance on single and higher-order mutations on S669, ProTherm, and ProteinGym datasets. Our code is available at https://github.com/jozhang97/MutateEverything.", "md": "Stabilizing proteins is a foundational step in protein engineering. However, the evolutionary pressure of all extant proteins makes identifying the scarce number of mutations that will improve thermodynamic stability challenging. Deep learning has recently emerged as a powerful tool for identifying promising mutations. Existing approaches, however, are computationally expensive, as the number of model inferences scales with the number of mutations queried. Our main contribution is a simple, parallel decoding algorithm. Our Mutate Everything is capable of predicting the effect of all single and double mutations in one forward pass. It is even versatile enough to predict higher-order mutations with minimal computational overhead. We build our Mutate Everything on top of ESM2 and AlphaFold, neither of which were trained to predict thermodynamic stability. We trained on the Mega-Scale cDNA proteolysis dataset and achieved state-of-the-art performance on single and higher-order mutations on S669, ProTherm, and ProteinGym datasets. Our code is available at https://github.com/jozhang97/MutateEverything."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "Protein engineering is the discipline of mutating a natural protein sequence to improve properties for industrial and pharmaceutical applications. However, evolution simultaneously optimizes several properties of a protein within its native environment, resulting in proteins with marginal thermodynamic stability (~ 5-15 kcal/mol) which become non-functional in an industrial setting. Therefore, repurposing a natural protein for biotechnological applications usually begins with identifying non-deleterious mutations that stabilize the structure. With a stabilized structure, downstream engineering goals, which often require exploring destabilizing mutations, become tractable. Historically, this process has been bottlenecked by the requirements of extensive laboratory characterization of rational designs or directed evolution libraries. The recent explosion in biological data has enabled deep learning frameworks to accelerate the identification of stabilizing mutations. A successfully stabilized protein often requires several mutations. However, current frameworks do not take into account epistatic interactions between multiple mutations. Thus, to significantly accelerate protein engineering, it is critical to efficiently navigate the epistatic landscape of higher-order mutations. However, due to its combinatorial nature, thorough exploration quickly becomes computationally prohibitive.\n\nIn this paper, we introduce Mutate Everything to directly predict changes in thermodynamic stability ($$\\Delta\\Delta G$$) for all single and higher-order mutations jointly. Mutate Everything is a parallel decoding algorithm that works in conjunction with a sequence model for predicting thermodynamic stability.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Protein engineering is the discipline of mutating a natural protein sequence to improve properties for industrial and pharmaceutical applications. However, evolution simultaneously optimizes several properties of a protein within its native environment, resulting in proteins with marginal thermodynamic stability (~ 5-15 kcal/mol) which become non-functional in an industrial setting. Therefore, repurposing a natural protein for biotechnological applications usually begins with identifying non-deleterious mutations that stabilize the structure. With a stabilized structure, downstream engineering goals, which often require exploring destabilizing mutations, become tractable. Historically, this process has been bottlenecked by the requirements of extensive laboratory characterization of rational designs or directed evolution libraries. The recent explosion in biological data has enabled deep learning frameworks to accelerate the identification of stabilizing mutations. A successfully stabilized protein often requires several mutations. However, current frameworks do not take into account epistatic interactions between multiple mutations. Thus, to significantly accelerate protein engineering, it is critical to efficiently navigate the epistatic landscape of higher-order mutations. However, due to its combinatorial nature, thorough exploration quickly becomes computationally prohibitive.\n\nIn this paper, we introduce Mutate Everything to directly predict changes in thermodynamic stability ($$\\Delta\\Delta G$$) for all single and higher-order mutations jointly. Mutate Everything is a parallel decoding algorithm that works in conjunction with a sequence model for predicting thermodynamic stability.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "                                                             K\n                                                P1K             AKVMGH\u2026           -1.0 kcal/mol\n                                                             P\n                           Mutate                                 E\n                         Everything             K3E           PA  VMGH\u2026           -0.5 kcal/mol\n                                                                  K   K   W\n  PAKVMGH\u2026                                     M5K             PAKV  G  \u2026         +1.0 kcal/mol\n                                               H7W\n                                                                      M    H\nFigure 1: Mutate Everything efficiently predicts \u2206\u2206G, the change in thermodynamic stability of\nfolding, for over a million mutations (e.g. all single, double mutations) in a single inference step.\nThis helps identify and prioritize stabilizing mutations (\u2206\u2206G < 0) in protein engineering efforts.\nThe notation for a mutation is AAfrom, pos, AAto (e.g. \u201cP1K\u201d mutates from P to K at position 1)\nPrior models for predicting thermodynamic stability work by producing an embedding or representa-\ntion of the input sequence. Our decoder takes this representation and uses a linear network to create\nfurther representations, one for every possible single mutation. These mutation-level representa-\ntions are then further aggregated to form representations for higher-order mutations. We feed these\nhigher-order representations into a lightweight multi-layer perception (MLP) head to output predicted\n\u2206\u2206G measurements. Since the mutation-level representations are computed only once, we are able\nto scale to millions of (higher-order) mutations, as the aggregation and MLP-head computations\nare inexpensive. As such, given a fixed computational budget, our model evaluates millions more\npotential mutations than prior methods.\nMutate Everything estimates the effects of all single and double mutations for a single protein in\nseconds on one GPU. This can efficiently compute the stability of all single and double mutants\nacross all \u223c20,000 proteins in the human proteome within a short time frame. To the best of our\nknowledge, Mutate Everything is the first tool that renders the computational analysis of double\nmutants across the entire proteome tractable. Mutate Everything can be used in conjunction with any\nmodel that generates representations for protein sequences. In this paper, we use AlphaFold [31]\nas a backbone architecture and fine-tune it on a dataset labeled by experimentally derived \u2206\u2206G\nmeasurements. This results in the first accurate model of stability prediction based on AlphaFold.\nWe evaluate the performance of our Mutate Everything on well-established benchmarks for predict-\ning experimentally-validated \u2206\u2206G measurements of mutations: ProTherm [48], S669 [54], and\nProteinGym [49]. On ProTherm high-order mutations (PTMul), our model achieves a Spearman of\n0.53, compared to 0.50 in the next best method. On S669 single mutations, our model achieves a\nSpearman of 0.56, outperforming the state-of-the-art at 0.53. On ProteinGym, the Mutate Every-\nthing outperforms state-of-the-art methods from 0.48 to 0.49. Our model makes finding the most\nstabilizing double mutations computationally tractable. We show on the double mutant subset of the\ncDNA-proteolysis dataset (cDNA2) [72], where only 3% of known mutations are stabilizing, that\nMutate Everything effectively ranks stabilizing mutations ahead of destabilizing ones.\n2    Related Works\n2.1   Protein Engineering\nProtein engineering is the process of mutating a natural protein to improve particular phenotypes,\nsuch as thermodynamic stability and function. The field has historically relied on rational design and\nstochastic methods, such as error-prone PCR [1], DNA shuffling [71], and directed evolution (DE)\n[4, 27], to identify gain-of-function mutations. Rational design is limited to proteins with solved\nstructures and requires an extensive understanding of biochemistry and specific knowledge of the\nparticular protein to select mutations highly likely to improve the target phenotypes [33, 79]. Directed\nevolution requires little to no knowledge of the protein and instead generates a library of protein\nvariants that are then screened for the target phenotypes (e.g. fluorescence brightness, antibiotic\n                                                   2", "md": "# Mutate Everything\n\n## Figure 1: Mutate Everything\n\nMutate Everything efficiently predicts $$\\Delta\\Delta G$$, the change in thermodynamic stability of folding, for over a million mutations (e.g. all single, double mutations) in a single inference step. This helps identify and prioritize stabilizing mutations ($$\\Delta\\Delta G < 0$$) in protein engineering efforts.\n\nThe notation for a mutation is AAfrom, pos, AAto (e.g. \u201cP1K\u201d mutates from P to K at position 1).\n\nPrior models for predicting thermodynamic stability work by producing an embedding or representation of the input sequence. Our decoder takes this representation and uses a linear network to create further representations, one for every possible single mutation. These mutation-level representations are then further aggregated to form representations for higher-order mutations. We feed these higher-order representations into a lightweight multi-layer perception (MLP) head to output predicted $$\\Delta\\Delta G$$ measurements. Since the mutation-level representations are computed only once, we are able to scale to millions of (higher-order) mutations, as the aggregation and MLP-head computations are inexpensive. As such, given a fixed computational budget, our model evaluates millions more potential mutations than prior methods.\n\nMutate Everything estimates the effects of all single and double mutations for a single protein in seconds on one GPU. This can efficiently compute the stability of all single and double mutants across all ~20,000 proteins in the human proteome within a short time frame. To the best of our knowledge, Mutate Everything is the first tool that renders the computational analysis of double mutants across the entire proteome tractable. Mutate Everything can be used in conjunction with any model that generates representations for protein sequences. In this paper, we use AlphaFold [31] as a backbone architecture and fine-tune it on a dataset labeled by experimentally derived $$\\Delta\\Delta G$$ measurements. This results in the first accurate model of stability prediction based on AlphaFold.\n\nWe evaluate the performance of our Mutate Everything on well-established benchmarks for predicting experimentally-validated $$\\Delta\\Delta G$$ measurements of mutations: ProTherm [48], S669 [54], and ProteinGym [49]. On ProTherm high-order mutations (PTMul), our model achieves a Spearman of 0.53, compared to 0.50 in the next best method. On S669 single mutations, our model achieves a Spearman of 0.56, outperforming the state-of-the-art at 0.53. On ProteinGym, the Mutate Everything outperforms state-of-the-art methods from 0.48 to 0.49. Our model makes finding the most stabilizing double mutations computationally tractable. We show on the double mutant subset of the cDNA-proteolysis dataset (cDNA2) [72], where only 3% of known mutations are stabilizing, that Mutate Everything effectively ranks stabilizing mutations ahead of destabilizing ones.\n\n### Related Works\n\n#### Protein Engineering\n\nProtein engineering is the process of mutating a natural protein to improve particular phenotypes, such as thermodynamic stability and function. The field has historically relied on rational design and stochastic methods, such as error-prone PCR [1], DNA shuffling [71], and directed evolution (DE) [4, 27], to identify gain-of-function mutations. Rational design is limited to proteins with solved structures and requires an extensive understanding of biochemistry and specific knowledge of the particular protein to select mutations highly likely to improve the target phenotypes [33, 79]. Directed evolution requires little to no knowledge of the protein and instead generates a library of protein variants that are then screened for the target phenotypes (e.g. fluorescence brightness, antibiotic).", "images": [{"name": "page-2-0.jpg", "height": 97, "width": 97, "x": 98, "y": 86}], "items": [{"type": "heading", "lvl": 1, "value": "Mutate Everything", "md": "# Mutate Everything"}, {"type": "heading", "lvl": 2, "value": "Figure 1: Mutate Everything", "md": "## Figure 1: Mutate Everything"}, {"type": "text", "value": "Mutate Everything efficiently predicts $$\\Delta\\Delta G$$, the change in thermodynamic stability of folding, for over a million mutations (e.g. all single, double mutations) in a single inference step. This helps identify and prioritize stabilizing mutations ($$\\Delta\\Delta G < 0$$) in protein engineering efforts.\n\nThe notation for a mutation is AAfrom, pos, AAto (e.g. \u201cP1K\u201d mutates from P to K at position 1).\n\nPrior models for predicting thermodynamic stability work by producing an embedding or representation of the input sequence. Our decoder takes this representation and uses a linear network to create further representations, one for every possible single mutation. These mutation-level representations are then further aggregated to form representations for higher-order mutations. We feed these higher-order representations into a lightweight multi-layer perception (MLP) head to output predicted $$\\Delta\\Delta G$$ measurements. Since the mutation-level representations are computed only once, we are able to scale to millions of (higher-order) mutations, as the aggregation and MLP-head computations are inexpensive. As such, given a fixed computational budget, our model evaluates millions more potential mutations than prior methods.\n\nMutate Everything estimates the effects of all single and double mutations for a single protein in seconds on one GPU. This can efficiently compute the stability of all single and double mutants across all ~20,000 proteins in the human proteome within a short time frame. To the best of our knowledge, Mutate Everything is the first tool that renders the computational analysis of double mutants across the entire proteome tractable. Mutate Everything can be used in conjunction with any model that generates representations for protein sequences. In this paper, we use AlphaFold [31] as a backbone architecture and fine-tune it on a dataset labeled by experimentally derived $$\\Delta\\Delta G$$ measurements. This results in the first accurate model of stability prediction based on AlphaFold.\n\nWe evaluate the performance of our Mutate Everything on well-established benchmarks for predicting experimentally-validated $$\\Delta\\Delta G$$ measurements of mutations: ProTherm [48], S669 [54], and ProteinGym [49]. On ProTherm high-order mutations (PTMul), our model achieves a Spearman of 0.53, compared to 0.50 in the next best method. On S669 single mutations, our model achieves a Spearman of 0.56, outperforming the state-of-the-art at 0.53. On ProteinGym, the Mutate Everything outperforms state-of-the-art methods from 0.48 to 0.49. Our model makes finding the most stabilizing double mutations computationally tractable. We show on the double mutant subset of the cDNA-proteolysis dataset (cDNA2) [72], where only 3% of known mutations are stabilizing, that Mutate Everything effectively ranks stabilizing mutations ahead of destabilizing ones.", "md": "Mutate Everything efficiently predicts $$\\Delta\\Delta G$$, the change in thermodynamic stability of folding, for over a million mutations (e.g. all single, double mutations) in a single inference step. This helps identify and prioritize stabilizing mutations ($$\\Delta\\Delta G < 0$$) in protein engineering efforts.\n\nThe notation for a mutation is AAfrom, pos, AAto (e.g. \u201cP1K\u201d mutates from P to K at position 1).\n\nPrior models for predicting thermodynamic stability work by producing an embedding or representation of the input sequence. Our decoder takes this representation and uses a linear network to create further representations, one for every possible single mutation. These mutation-level representations are then further aggregated to form representations for higher-order mutations. We feed these higher-order representations into a lightweight multi-layer perception (MLP) head to output predicted $$\\Delta\\Delta G$$ measurements. Since the mutation-level representations are computed only once, we are able to scale to millions of (higher-order) mutations, as the aggregation and MLP-head computations are inexpensive. As such, given a fixed computational budget, our model evaluates millions more potential mutations than prior methods.\n\nMutate Everything estimates the effects of all single and double mutations for a single protein in seconds on one GPU. This can efficiently compute the stability of all single and double mutants across all ~20,000 proteins in the human proteome within a short time frame. To the best of our knowledge, Mutate Everything is the first tool that renders the computational analysis of double mutants across the entire proteome tractable. Mutate Everything can be used in conjunction with any model that generates representations for protein sequences. In this paper, we use AlphaFold [31] as a backbone architecture and fine-tune it on a dataset labeled by experimentally derived $$\\Delta\\Delta G$$ measurements. This results in the first accurate model of stability prediction based on AlphaFold.\n\nWe evaluate the performance of our Mutate Everything on well-established benchmarks for predicting experimentally-validated $$\\Delta\\Delta G$$ measurements of mutations: ProTherm [48], S669 [54], and ProteinGym [49]. On ProTherm high-order mutations (PTMul), our model achieves a Spearman of 0.53, compared to 0.50 in the next best method. On S669 single mutations, our model achieves a Spearman of 0.56, outperforming the state-of-the-art at 0.53. On ProteinGym, the Mutate Everything outperforms state-of-the-art methods from 0.48 to 0.49. Our model makes finding the most stabilizing double mutations computationally tractable. We show on the double mutant subset of the cDNA-proteolysis dataset (cDNA2) [72], where only 3% of known mutations are stabilizing, that Mutate Everything effectively ranks stabilizing mutations ahead of destabilizing ones."}, {"type": "heading", "lvl": 3, "value": "Related Works", "md": "### Related Works"}, {"type": "heading", "lvl": 4, "value": "Protein Engineering", "md": "#### Protein Engineering"}, {"type": "text", "value": "Protein engineering is the process of mutating a natural protein to improve particular phenotypes, such as thermodynamic stability and function. The field has historically relied on rational design and stochastic methods, such as error-prone PCR [1], DNA shuffling [71], and directed evolution (DE) [4, 27], to identify gain-of-function mutations. Rational design is limited to proteins with solved structures and requires an extensive understanding of biochemistry and specific knowledge of the particular protein to select mutations highly likely to improve the target phenotypes [33, 79]. Directed evolution requires little to no knowledge of the protein and instead generates a library of protein variants that are then screened for the target phenotypes (e.g. fluorescence brightness, antibiotic).", "md": "Protein engineering is the process of mutating a natural protein to improve particular phenotypes, such as thermodynamic stability and function. The field has historically relied on rational design and stochastic methods, such as error-prone PCR [1], DNA shuffling [71], and directed evolution (DE) [4, 27], to identify gain-of-function mutations. Rational design is limited to proteins with solved structures and requires an extensive understanding of biochemistry and specific knowledge of the particular protein to select mutations highly likely to improve the target phenotypes [33, 79]. Directed evolution requires little to no knowledge of the protein and instead generates a library of protein variants that are then screened for the target phenotypes (e.g. fluorescence brightness, antibiotic)."}]}, {"page": 3, "text": "resistance, stability, activity) [4, 27]. The library can be generated via site-saturated mutagenesis\nof a handful of positions in the sequence [12] or DNA shuffl      ing, in which the gene is digested into\nrandom fragments and reassembled into full-length sequences [71]. After screening the library, the\nmost \"fit\" variant is then selected as the initial sequence for the next round of directed evolution. This\niterative process repeats until it obtains a protein variant with the desired phenotypes.\nMachine learning has demonstrated its ability to augment rational design and accelerate the stabi-\nlization of a variety of proteins [20, 28, 40, 52, 68]. Separately, machine learning-guided directed\nevolution (MLDE)[75] has been shown to improve the likelihood of obtaining the global fitness\nmaximum by 81-fold compared to traditional DE [76]. MLDE has accelerated the engineering of\nseveral proteins, such as the enantioselectivity of enzymes for kinetic resolution of epoxides [23]\nand the activity and expression of a glutathione transferase [46]. Mutate Everything empowers the\nexperimentalist to accelerate the stabilization of a protein for both rational design and MLDE.\n2.2   Machine Learning for Protein Structure Prediction\nRecent advances in machine learning have led to remarkable progress in protein structure prediction.\nAlphaFold [31] has demonstrated that deep learning is highly effective at predicting protein structures\nfrom a sequence by using evolutionary history via a multiple sequence alignment (MSA). AlphaFold\npasses the MSA and a pairwise representation of the sequence into Evoformer to capture the co-\nevolutionary patterns between residues. The Evoformer output is then processed by the Structure\nModule, which predicts the protein\u2019s structure. We challenge prior works postulating that AlphaFold\ncannot be used for stability prediction and show that fine-tuning these rich co-evolutionary and\nstructural features yield highly performant stability predictors [53].\nEvolutionary Scale Modeling (ESM) [38, 62] has shown that protein structure prediction can be\nperformed without MSAs and specialized architectures by leveraging large transformers pre-trained\non masked token prediction. Other works extended this masked pre-training framework, including\nMSA-Transformer [60] which incorporates a sequence\u2019s MSA as input, and Tranception [49], which\ndevelops a hybrid convolution and attention based autoregressive architecture. We show that fine-\ntuning these evolutionary representations excels at protein stability assessment without MSAs.\n2.3   Protein Stability Assessment\nTraditional approaches to protein stability assessment relied on a combination of physics-based\nmethods, statistical analysis, and traditional machine learning techniques. Physics-based tools, such\nas FoldX, Rosetta, and SDM, utilize energy functions and statistical patterns to assess how mutations\naffect a protein\u2019s stability [32, 67, 77]. DDGun [44] directly inferred \u2206\u2206G from heuristics, including\nBlosum substitution scores, differences in interaction energy with neighboring residues, and changes\nin hydrophobicity. Many traditional machine learning approaches used support vector machines and\ndecision trees with physics-based feature engineering [11, 14, 15, 17, 37, 66]. Others ensemble\nexisting machine learning methods [35, 56, 59, 63, 64].\nRecently, deep learning-based approaches have recently begun to outperform existing physics and\ntraditional machine learning approaches [13, 19, 34]. ACDC-NN [6] trains an asymmetric convolu-\ntional neural network for predicting forward and reverse mutation effects. Thermonet [36] voxelizes\nand feeds both the wild-type and mutant protein structures into a 3D-CNN to regress a \u2206\u2206G value.\nPROSTATA feeds the wild-type and mutant protein sequence into a pre-trained ESM2 model and then\nregresses a \u2206\u2206G value [74]. Stability Oracle takes the native and mutant amino acid type along with\nthe local structure as input and regresses \u2206\u2206G [18]. Several deep learning frameworks [64, 80, 81]\nmodel multiple mutations in addition to single mutations. In this paper, we develop a framework that\nmodels the protein to enable efficient enumeration of all mutation candidates.\n3    Preliminary\nProblem Setup. A protein w = (w1, ..., wL) is a sequence of amino acids wl \u2208                  AA, where\nAA = {A, C, ..., Y } are the 20 different amino acid types encoded by genetic information. Let\n\u00b5 = (p, t) denote a mutation substituting the amino acid wp at position p \u2208       [1, ..., L] to amino acid\ntype t \u2208  AA. Our goal is to determine the change in thermodynamic stability \u2206\u2206G \u2208         R for a protein\nw under a large number of mutation sets (M1, ..., MN) where a mutation set Mi = {\u00b5k}Ki         k=1.\n                                                     3", "md": "Resistance, stability, activity) [4, 27]. The library can be generated via site-saturated mutagenesis\nof a handful of positions in the sequence [12] or DNA shuffling, in which the gene is digested into\nrandom fragments and reassembled into full-length sequences [71]. After screening the library, the\nmost \"fit\" variant is then selected as the initial sequence for the next round of directed evolution. This\niterative process repeats until it obtains a protein variant with the desired phenotypes.\n\nMachine learning has demonstrated its ability to augment rational design and accelerate the stabilization of a variety of proteins [20, 28, 40, 52, 68]. Separately, machine learning-guided directed evolution (MLDE)[75] has been shown to improve the likelihood of obtaining the global fitness maximum by 81-fold compared to traditional DE [76]. MLDE has accelerated the engineering of several proteins, such as the enantioselectivity of enzymes for kinetic resolution of epoxides [23] and the activity and expression of a glutathione transferase [46]. Mutate Everything empowers the experimentalist to accelerate the stabilization of a protein for both rational design and MLDE.\n\n## Machine Learning for Protein Structure Prediction\n\nRecent advances in machine learning have led to remarkable progress in protein structure prediction.\nAlphaFold [31] has demonstrated that deep learning is highly effective at predicting protein structures\nfrom a sequence by using evolutionary history via a multiple sequence alignment (MSA). AlphaFold\npasses the MSA and a pairwise representation of the sequence into Evoformer to capture the co-evolutionary patterns between residues. The Evoformer output is then processed by the Structure\nModule, which predicts the protein\u2019s structure. We challenge prior works postulating that AlphaFold\ncannot be used for stability prediction and show that fine-tuning these rich co-evolutionary and\nstructural features yield highly performant stability predictors [53].\n\nEvolutionary Scale Modeling (ESM) [38, 62] has shown that protein structure prediction can be\nperformed without MSAs and specialized architectures by leveraging large transformers pre-trained\non masked token prediction. Other works extended this masked pre-training framework, including\nMSA-Transformer [60] which incorporates a sequence\u2019s MSA as input, and Tranception [49], which\ndevelops a hybrid convolution and attention based autoregressive architecture. We show that fine-tuning these evolutionary representations excels at protein stability assessment without MSAs.\n\n## Protein Stability Assessment\n\nTraditional approaches to protein stability assessment relied on a combination of physics-based\nmethods, statistical analysis, and traditional machine learning techniques. Physics-based tools, such\nas FoldX, Rosetta, and SDM, utilize energy functions and statistical patterns to assess how mutations\naffect a protein\u2019s stability [32, 67, 77]. DDGun [44] directly inferred $$\\Delta\\Delta G$$ from heuristics, including\nBlosum substitution scores, differences in interaction energy with neighboring residues, and changes\nin hydrophobicity. Many traditional machine learning approaches used support vector machines and\ndecision trees with physics-based feature engineering [11, 14, 15, 17, 37, 66]. Others ensemble\nexisting machine learning methods [35, 56, 59, 63, 64].\n\nRecently, deep learning-based approaches have recently begun to outperform existing physics and\ntraditional machine learning approaches [13, 19, 34]. ACDC-NN [6] trains an asymmetric convolutional neural network for predicting forward and reverse mutation effects. Thermonet [36] voxelizes\nand feeds both the wild-type and mutant protein structures into a 3D-CNN to regress a $$\\Delta\\Delta G$$ value.\nPROSTATA feeds the wild-type and mutant protein sequence into a pre-trained ESM2 model and then\nregresses a $$\\Delta\\Delta G$$ value [74]. Stability Oracle takes the native and mutant amino acid type along with\nthe local structure as input and regresses $$\\Delta\\Delta G$$ [18]. Several deep learning frameworks [64, 80, 81]\nmodel multiple mutations in addition to single mutations. In this paper, we develop a framework that\nmodels the protein to enable efficient enumeration of all mutation candidates.\n\n### Preliminary\n\nProblem Setup. A protein $$w = (w_1, ..., w_L)$$ is a sequence of amino acids $$w_l \\in AA$$, where\n$$AA = \\{A, C, ..., Y\\}$$ are the 20 different amino acid types encoded by genetic information. Let\n$$\\mu = (p, t)$$ denote a mutation substituting the amino acid $$w_p$$ at position $$p \\in [1, ..., L]$$ to amino acid\ntype $$t \\in AA$$. Our goal is to determine the change in thermodynamic stability $$\\Delta\\Delta G \\in R$$ for a protein\n$$w$$ under a large number of mutation sets $$(M_1, ..., M_N)$$ where a mutation set $$M_i = \\{\\mu_k\\}_{k=1}^K$$.", "images": [], "items": [{"type": "text", "value": "Resistance, stability, activity) [4, 27]. The library can be generated via site-saturated mutagenesis\nof a handful of positions in the sequence [12] or DNA shuffling, in which the gene is digested into\nrandom fragments and reassembled into full-length sequences [71]. After screening the library, the\nmost \"fit\" variant is then selected as the initial sequence for the next round of directed evolution. This\niterative process repeats until it obtains a protein variant with the desired phenotypes.\n\nMachine learning has demonstrated its ability to augment rational design and accelerate the stabilization of a variety of proteins [20, 28, 40, 52, 68]. Separately, machine learning-guided directed evolution (MLDE)[75] has been shown to improve the likelihood of obtaining the global fitness maximum by 81-fold compared to traditional DE [76]. MLDE has accelerated the engineering of several proteins, such as the enantioselectivity of enzymes for kinetic resolution of epoxides [23] and the activity and expression of a glutathione transferase [46]. Mutate Everything empowers the experimentalist to accelerate the stabilization of a protein for both rational design and MLDE.", "md": "Resistance, stability, activity) [4, 27]. The library can be generated via site-saturated mutagenesis\nof a handful of positions in the sequence [12] or DNA shuffling, in which the gene is digested into\nrandom fragments and reassembled into full-length sequences [71]. After screening the library, the\nmost \"fit\" variant is then selected as the initial sequence for the next round of directed evolution. This\niterative process repeats until it obtains a protein variant with the desired phenotypes.\n\nMachine learning has demonstrated its ability to augment rational design and accelerate the stabilization of a variety of proteins [20, 28, 40, 52, 68]. Separately, machine learning-guided directed evolution (MLDE)[75] has been shown to improve the likelihood of obtaining the global fitness maximum by 81-fold compared to traditional DE [76]. MLDE has accelerated the engineering of several proteins, such as the enantioselectivity of enzymes for kinetic resolution of epoxides [23] and the activity and expression of a glutathione transferase [46]. Mutate Everything empowers the experimentalist to accelerate the stabilization of a protein for both rational design and MLDE."}, {"type": "heading", "lvl": 2, "value": "Machine Learning for Protein Structure Prediction", "md": "## Machine Learning for Protein Structure Prediction"}, {"type": "text", "value": "Recent advances in machine learning have led to remarkable progress in protein structure prediction.\nAlphaFold [31] has demonstrated that deep learning is highly effective at predicting protein structures\nfrom a sequence by using evolutionary history via a multiple sequence alignment (MSA). AlphaFold\npasses the MSA and a pairwise representation of the sequence into Evoformer to capture the co-evolutionary patterns between residues. The Evoformer output is then processed by the Structure\nModule, which predicts the protein\u2019s structure. We challenge prior works postulating that AlphaFold\ncannot be used for stability prediction and show that fine-tuning these rich co-evolutionary and\nstructural features yield highly performant stability predictors [53].\n\nEvolutionary Scale Modeling (ESM) [38, 62] has shown that protein structure prediction can be\nperformed without MSAs and specialized architectures by leveraging large transformers pre-trained\non masked token prediction. Other works extended this masked pre-training framework, including\nMSA-Transformer [60] which incorporates a sequence\u2019s MSA as input, and Tranception [49], which\ndevelops a hybrid convolution and attention based autoregressive architecture. We show that fine-tuning these evolutionary representations excels at protein stability assessment without MSAs.", "md": "Recent advances in machine learning have led to remarkable progress in protein structure prediction.\nAlphaFold [31] has demonstrated that deep learning is highly effective at predicting protein structures\nfrom a sequence by using evolutionary history via a multiple sequence alignment (MSA). AlphaFold\npasses the MSA and a pairwise representation of the sequence into Evoformer to capture the co-evolutionary patterns between residues. The Evoformer output is then processed by the Structure\nModule, which predicts the protein\u2019s structure. We challenge prior works postulating that AlphaFold\ncannot be used for stability prediction and show that fine-tuning these rich co-evolutionary and\nstructural features yield highly performant stability predictors [53].\n\nEvolutionary Scale Modeling (ESM) [38, 62] has shown that protein structure prediction can be\nperformed without MSAs and specialized architectures by leveraging large transformers pre-trained\non masked token prediction. Other works extended this masked pre-training framework, including\nMSA-Transformer [60] which incorporates a sequence\u2019s MSA as input, and Tranception [49], which\ndevelops a hybrid convolution and attention based autoregressive architecture. We show that fine-tuning these evolutionary representations excels at protein stability assessment without MSAs."}, {"type": "heading", "lvl": 2, "value": "Protein Stability Assessment", "md": "## Protein Stability Assessment"}, {"type": "text", "value": "Traditional approaches to protein stability assessment relied on a combination of physics-based\nmethods, statistical analysis, and traditional machine learning techniques. Physics-based tools, such\nas FoldX, Rosetta, and SDM, utilize energy functions and statistical patterns to assess how mutations\naffect a protein\u2019s stability [32, 67, 77]. DDGun [44] directly inferred $$\\Delta\\Delta G$$ from heuristics, including\nBlosum substitution scores, differences in interaction energy with neighboring residues, and changes\nin hydrophobicity. Many traditional machine learning approaches used support vector machines and\ndecision trees with physics-based feature engineering [11, 14, 15, 17, 37, 66]. Others ensemble\nexisting machine learning methods [35, 56, 59, 63, 64].\n\nRecently, deep learning-based approaches have recently begun to outperform existing physics and\ntraditional machine learning approaches [13, 19, 34]. ACDC-NN [6] trains an asymmetric convolutional neural network for predicting forward and reverse mutation effects. Thermonet [36] voxelizes\nand feeds both the wild-type and mutant protein structures into a 3D-CNN to regress a $$\\Delta\\Delta G$$ value.\nPROSTATA feeds the wild-type and mutant protein sequence into a pre-trained ESM2 model and then\nregresses a $$\\Delta\\Delta G$$ value [74]. Stability Oracle takes the native and mutant amino acid type along with\nthe local structure as input and regresses $$\\Delta\\Delta G$$ [18]. Several deep learning frameworks [64, 80, 81]\nmodel multiple mutations in addition to single mutations. In this paper, we develop a framework that\nmodels the protein to enable efficient enumeration of all mutation candidates.", "md": "Traditional approaches to protein stability assessment relied on a combination of physics-based\nmethods, statistical analysis, and traditional machine learning techniques. Physics-based tools, such\nas FoldX, Rosetta, and SDM, utilize energy functions and statistical patterns to assess how mutations\naffect a protein\u2019s stability [32, 67, 77]. DDGun [44] directly inferred $$\\Delta\\Delta G$$ from heuristics, including\nBlosum substitution scores, differences in interaction energy with neighboring residues, and changes\nin hydrophobicity. Many traditional machine learning approaches used support vector machines and\ndecision trees with physics-based feature engineering [11, 14, 15, 17, 37, 66]. Others ensemble\nexisting machine learning methods [35, 56, 59, 63, 64].\n\nRecently, deep learning-based approaches have recently begun to outperform existing physics and\ntraditional machine learning approaches [13, 19, 34]. ACDC-NN [6] trains an asymmetric convolutional neural network for predicting forward and reverse mutation effects. Thermonet [36] voxelizes\nand feeds both the wild-type and mutant protein structures into a 3D-CNN to regress a $$\\Delta\\Delta G$$ value.\nPROSTATA feeds the wild-type and mutant protein sequence into a pre-trained ESM2 model and then\nregresses a $$\\Delta\\Delta G$$ value [74]. Stability Oracle takes the native and mutant amino acid type along with\nthe local structure as input and regresses $$\\Delta\\Delta G$$ [18]. Several deep learning frameworks [64, 80, 81]\nmodel multiple mutations in addition to single mutations. In this paper, we develop a framework that\nmodels the protein to enable efficient enumeration of all mutation candidates."}, {"type": "heading", "lvl": 3, "value": "Preliminary", "md": "### Preliminary"}, {"type": "text", "value": "Problem Setup. A protein $$w = (w_1, ..., w_L)$$ is a sequence of amino acids $$w_l \\in AA$$, where\n$$AA = \\{A, C, ..., Y\\}$$ are the 20 different amino acid types encoded by genetic information. Let\n$$\\mu = (p, t)$$ denote a mutation substituting the amino acid $$w_p$$ at position $$p \\in [1, ..., L]$$ to amino acid\ntype $$t \\in AA$$. Our goal is to determine the change in thermodynamic stability $$\\Delta\\Delta G \\in R$$ for a protein\n$$w$$ under a large number of mutation sets $$(M_1, ..., M_N)$$ where a mutation set $$M_i = \\{\\mu_k\\}_{k=1}^K$$.", "md": "Problem Setup. A protein $$w = (w_1, ..., w_L)$$ is a sequence of amino acids $$w_l \\in AA$$, where\n$$AA = \\{A, C, ..., Y\\}$$ are the 20 different amino acid types encoded by genetic information. Let\n$$\\mu = (p, t)$$ denote a mutation substituting the amino acid $$w_p$$ at position $$p \\in [1, ..., L]$$ to amino acid\ntype $$t \\in AA$$. Our goal is to determine the change in thermodynamic stability $$\\Delta\\Delta G \\in R$$ for a protein\n$$w$$ under a large number of mutation sets $$(M_1, ..., M_N)$$ where a mutation set $$M_i = \\{\\mu_k\\}_{k=1}^K$$."}]}, {"page": 4, "text": "         P                                                                                 MLP[V]                \u0394\u0394G                                      MLP[V]\n         A                                                                                                       for P1V\n         K                                                                                                                                                                                        \u0394\u0394G\n                                                                                                                                                                                                      for\n         V                                                                                 MLP[E]                \u0394\u0394G                                      MLP[E]                      MLP            P1V\n        M                                                                                                        for V4E                                                                             V4E\n         G             AlphaFold                                                                                                                                                                     H7K\n                                                                                                                 \u0394\u0394G\n                                                                                                                for H7K                                   MLP[K]\n                                                                                          MLP[K]\n         H\n                     (a) Backbone                                                    (b) Mutate Single                                                       (c) Mutate Multiple\nFigure 2: Mutate Everything decodes any mutation. The model takes a protein sequence of length\nL as input. (a) A pre-trained feature extractor (e.g. ESM [38], AlphaFold [31]) computes per-position\nfeatures for each amino acid in the sequence (yellow). Each token is reshaped by an MLP into an\narray of all possible mutations at that location. (b) A single mutation is directly decoded to its \u2206\u2206G.\n(c) For a mutation set containing multiple mutations, selected mutations are aggregated and decoded\nto obtain a predicted \u2206\u2206G.\nThe mutation sets include higher-order mutations (K > 1), which is beyond the scope of state-of-the-\nart single mutation predictors (K = 1) [18, 36, 74]. These higher-order mutations are strictly more\nchallenging than single mutations. The model must understand the effect of each single mutation and\nalso the epistatic interactions that arise between individual mutations. We introduce a method that\ncaptures these interactions for any mutation set and decodes a corresponding \u2206\u2206G.\nFurthermore, existing paradigms cannot efficiently scale to a large number of mutations. Higher-order\nmutation models [16, 35, 44, 64] take a protein and its mutant as input and predict the change in\nstability. These methods would need to enumerate and sort over a million model evaluations to find\nthe most stabilizing mutations for a small protein of 75 amino acids. Our model runs a fine-tuned\nbackbone only once and a lightweight decoder once for each mutation in parallel.\nFeature Extraction. We fine-tune AlphaFold and other pre-trained models to alleviate the scarcity of\nlabeled data. AlphaFold\u2019s Evoformer learns co-evolutionary patterns, such as residue interactions, and\nthe Structure Module learns structural information, such as solvent accessibility or residue-residue\ndistance. We demonstrate this is connected to a protein\u2019s stability and how mutations affect stability.\nEach element of the protein sequence is tokenized by its amino acid type wl and positionally encoded.\nThis produces a set of high-dimensional tokens for the transformer to take as input. The tokens\nare fed into a pre-trained model, which extracts per-position features (xl)L                                                                                          l=1 where xl \u2208                 Rd. The\ntransformer feature extractor is finetuned with our training objective. This is illustrated in Figure 2a.\n4         Methodology\nOur model uses per-position features (xl)L                                                     l=1 to determine the stability of a mutation set M                                                            . We\ndistinguish between single point mutations K = |M                                                                    | = 1, and higher-order mutations K > 1.\nBoth single and higher-order mutations share the same backbone architecture and encoding scheme,\nsee Figure 2b. For each mutation \u00b5 = (p, t) at position p to amino acid t, we compute a latent mutation\nrepresentation z(\u00b5) \u2208                            Rd, which captures the effect of the mutation on the protein. We decompose the\nmutation representation z(\u00b5) = f t(xp) + ht, where sequence-dependent features f t \u2208                                                                                                       {f A, ..., f Y }\nproject the per-position feature according to the mutated amino acid type, and sequence-independent\nfeatures ht \u2208                  {hA, ...hY } are embeddings unique to the amino acid types A, C, ..., Y . Intuitively, f t\ncaptures a contextualized feature at the position, and ht captures information about the mutated amino\nacid type. This mutation representation builds up a feature representing the amino acid substitution.\nDecoding single mutations. Figure 2b illustrates single mutation decoding. The change in thermo-\ndynamic stability \u2206\u2206G for a single mutation \u00b5 is read out from its mutation representation z(\u00b5). A\nlightweight linear head g1 : Rd \u2192                                             R decodes this representation into the change in thermodynamic\nstability: \u2206\u2206Gpr = g(z(\u00b5)).\n                                                                                                             4", "md": "# Mutate Everything\n\n## Mutate Everything\n\nThe model takes a protein sequence of length L as input.\n\n1. ### (a) Backbone\n\nA pre-trained feature extractor (e.g. ESM [38], AlphaFold [31]) computes per-position features for each amino acid in the sequence (yellow). Each token is reshaped by an MLP into an array of all possible mutations at that location.\n2. ### (b) Mutate Single\n\nA single mutation is directly decoded to its $\\Delta\\Delta G$.\n3. ### (c) Mutate Multiple\n\nFor a mutation set containing multiple mutations, selected mutations are aggregated and decoded to obtain a predicted $\\Delta\\Delta G$.\n\nThe mutation sets include higher-order mutations (K > 1), which is beyond the scope of state-of-the-art single mutation predictors (K = 1) [18, 36, 74]. These higher-order mutations are strictly more challenging than single mutations. The model must understand the effect of each single mutation and also the epistatic interactions that arise between individual mutations. We introduce a method that captures these interactions for any mutation set and decodes a corresponding $$\\Delta\\Delta G$$.\n\nFurthermore, existing paradigms cannot efficiently scale to a large number of mutations. Higher-order mutation models [16, 35, 44, 64] take a protein and its mutant as input and predict the change in stability. These methods would need to enumerate and sort over a million model evaluations to find the most stabilizing mutations for a small protein of 75 amino acids. Our model runs a fine-tuned backbone only once and a lightweight decoder once for each mutation in parallel.\n\n### Feature Extraction\n\nWe fine-tune AlphaFold and other pre-trained models to alleviate the scarcity of labeled data. AlphaFold\u2019s Evoformer learns co-evolutionary patterns, such as residue interactions, and the Structure Module learns structural information, such as solvent accessibility or residue-residue distance. We demonstrate this is connected to a protein\u2019s stability and how mutations affect stability.\n\nEach element of the protein sequence is tokenized by its amino acid type wl and positionally encoded. This produces a set of high-dimensional tokens for the transformer to take as input. The tokens are fed into a pre-trained model, which extracts per-position features $$x_{l}$$ where $$x_{l} \\in \\mathbb{R}^{d}$$. The transformer feature extractor is finetuned with our training objective. This is illustrated in Figure 2a.\n\n### Methodology\n\nOur model uses per-position features $$x_{l}$$ to determine the stability of a mutation set M. We distinguish between single point mutations K = |M| = 1, and higher-order mutations K > 1. Both single and higher-order mutations share the same backbone architecture and encoding scheme, see Figure 2b. For each mutation \u00b5 = (p, t) at position p to amino acid t, we compute a latent mutation representation $$z(\\mu) \\in \\mathbb{R}^{d}$$, which captures the effect of the mutation on the protein. We decompose the mutation representation $$z(\\mu) = f_{t}(x_{p}) + h_{t}$$, where sequence-dependent features $$f_{t} \\in \\{f_{A}, ..., f_{Y}\\}$$ project the per-position feature according to the mutated amino acid type, and sequence-independent features $$h_{t} \\in \\{h_{A}, ...h_{Y}\\}$$ are embeddings unique to the amino acid types A, C, ..., Y. Intuitively, $$f_{t}$$ captures a contextualized feature at the position, and $$h_{t}$$ captures information about the mutated amino acid type. This mutation representation builds up a feature representing the amino acid substitution.\n\nDecoding single mutations. Figure 2b illustrates single mutation decoding. The change in thermodynamic stability $$\\Delta\\Delta G$$ for a single mutation \u00b5 is read out from its mutation representation $$z(\\mu)$$. A lightweight linear head $$g_{1} : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$$ decodes this representation into the change in thermodynamic stability: $$\\Delta\\Delta G_{pr} = g(z(\\mu))$$.", "images": [{"name": "page-4-0.jpg", "height": 42, "width": 127, "x": 153, "y": 88}], "items": [{"type": "heading", "lvl": 1, "value": "Mutate Everything", "md": "# Mutate Everything"}, {"type": "heading", "lvl": 2, "value": "Mutate Everything", "md": "## Mutate Everything"}, {"type": "text", "value": "The model takes a protein sequence of length L as input.\n\n1. ### (a) Backbone\n\nA pre-trained feature extractor (e.g. ESM [38], AlphaFold [31]) computes per-position features for each amino acid in the sequence (yellow). Each token is reshaped by an MLP into an array of all possible mutations at that location.\n2. ### (b) Mutate Single\n\nA single mutation is directly decoded to its $\\Delta\\Delta G$.\n3. ### (c) Mutate Multiple\n\nFor a mutation set containing multiple mutations, selected mutations are aggregated and decoded to obtain a predicted $\\Delta\\Delta G$.\n\nThe mutation sets include higher-order mutations (K > 1), which is beyond the scope of state-of-the-art single mutation predictors (K = 1) [18, 36, 74]. These higher-order mutations are strictly more challenging than single mutations. The model must understand the effect of each single mutation and also the epistatic interactions that arise between individual mutations. We introduce a method that captures these interactions for any mutation set and decodes a corresponding $$\\Delta\\Delta G$$.\n\nFurthermore, existing paradigms cannot efficiently scale to a large number of mutations. Higher-order mutation models [16, 35, 44, 64] take a protein and its mutant as input and predict the change in stability. These methods would need to enumerate and sort over a million model evaluations to find the most stabilizing mutations for a small protein of 75 amino acids. Our model runs a fine-tuned backbone only once and a lightweight decoder once for each mutation in parallel.", "md": "The model takes a protein sequence of length L as input.\n\n1. ### (a) Backbone\n\nA pre-trained feature extractor (e.g. ESM [38], AlphaFold [31]) computes per-position features for each amino acid in the sequence (yellow). Each token is reshaped by an MLP into an array of all possible mutations at that location.\n2. ### (b) Mutate Single\n\nA single mutation is directly decoded to its $\\Delta\\Delta G$.\n3. ### (c) Mutate Multiple\n\nFor a mutation set containing multiple mutations, selected mutations are aggregated and decoded to obtain a predicted $\\Delta\\Delta G$.\n\nThe mutation sets include higher-order mutations (K > 1), which is beyond the scope of state-of-the-art single mutation predictors (K = 1) [18, 36, 74]. These higher-order mutations are strictly more challenging than single mutations. The model must understand the effect of each single mutation and also the epistatic interactions that arise between individual mutations. We introduce a method that captures these interactions for any mutation set and decodes a corresponding $$\\Delta\\Delta G$$.\n\nFurthermore, existing paradigms cannot efficiently scale to a large number of mutations. Higher-order mutation models [16, 35, 44, 64] take a protein and its mutant as input and predict the change in stability. These methods would need to enumerate and sort over a million model evaluations to find the most stabilizing mutations for a small protein of 75 amino acids. Our model runs a fine-tuned backbone only once and a lightweight decoder once for each mutation in parallel."}, {"type": "heading", "lvl": 3, "value": "Feature Extraction", "md": "### Feature Extraction"}, {"type": "text", "value": "We fine-tune AlphaFold and other pre-trained models to alleviate the scarcity of labeled data. AlphaFold\u2019s Evoformer learns co-evolutionary patterns, such as residue interactions, and the Structure Module learns structural information, such as solvent accessibility or residue-residue distance. We demonstrate this is connected to a protein\u2019s stability and how mutations affect stability.\n\nEach element of the protein sequence is tokenized by its amino acid type wl and positionally encoded. This produces a set of high-dimensional tokens for the transformer to take as input. The tokens are fed into a pre-trained model, which extracts per-position features $$x_{l}$$ where $$x_{l} \\in \\mathbb{R}^{d}$$. The transformer feature extractor is finetuned with our training objective. This is illustrated in Figure 2a.", "md": "We fine-tune AlphaFold and other pre-trained models to alleviate the scarcity of labeled data. AlphaFold\u2019s Evoformer learns co-evolutionary patterns, such as residue interactions, and the Structure Module learns structural information, such as solvent accessibility or residue-residue distance. We demonstrate this is connected to a protein\u2019s stability and how mutations affect stability.\n\nEach element of the protein sequence is tokenized by its amino acid type wl and positionally encoded. This produces a set of high-dimensional tokens for the transformer to take as input. The tokens are fed into a pre-trained model, which extracts per-position features $$x_{l}$$ where $$x_{l} \\in \\mathbb{R}^{d}$$. The transformer feature extractor is finetuned with our training objective. This is illustrated in Figure 2a."}, {"type": "heading", "lvl": 3, "value": "Methodology", "md": "### Methodology"}, {"type": "text", "value": "Our model uses per-position features $$x_{l}$$ to determine the stability of a mutation set M. We distinguish between single point mutations K = |M| = 1, and higher-order mutations K > 1. Both single and higher-order mutations share the same backbone architecture and encoding scheme, see Figure 2b. For each mutation \u00b5 = (p, t) at position p to amino acid t, we compute a latent mutation representation $$z(\\mu) \\in \\mathbb{R}^{d}$$, which captures the effect of the mutation on the protein. We decompose the mutation representation $$z(\\mu) = f_{t}(x_{p}) + h_{t}$$, where sequence-dependent features $$f_{t} \\in \\{f_{A}, ..., f_{Y}\\}$$ project the per-position feature according to the mutated amino acid type, and sequence-independent features $$h_{t} \\in \\{h_{A}, ...h_{Y}\\}$$ are embeddings unique to the amino acid types A, C, ..., Y. Intuitively, $$f_{t}$$ captures a contextualized feature at the position, and $$h_{t}$$ captures information about the mutated amino acid type. This mutation representation builds up a feature representing the amino acid substitution.\n\nDecoding single mutations. Figure 2b illustrates single mutation decoding. The change in thermodynamic stability $$\\Delta\\Delta G$$ for a single mutation \u00b5 is read out from its mutation representation $$z(\\mu)$$. A lightweight linear head $$g_{1} : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$$ decodes this representation into the change in thermodynamic stability: $$\\Delta\\Delta G_{pr} = g(z(\\mu))$$.", "md": "Our model uses per-position features $$x_{l}$$ to determine the stability of a mutation set M. We distinguish between single point mutations K = |M| = 1, and higher-order mutations K > 1. Both single and higher-order mutations share the same backbone architecture and encoding scheme, see Figure 2b. For each mutation \u00b5 = (p, t) at position p to amino acid t, we compute a latent mutation representation $$z(\\mu) \\in \\mathbb{R}^{d}$$, which captures the effect of the mutation on the protein. We decompose the mutation representation $$z(\\mu) = f_{t}(x_{p}) + h_{t}$$, where sequence-dependent features $$f_{t} \\in \\{f_{A}, ..., f_{Y}\\}$$ project the per-position feature according to the mutated amino acid type, and sequence-independent features $$h_{t} \\in \\{h_{A}, ...h_{Y}\\}$$ are embeddings unique to the amino acid types A, C, ..., Y. Intuitively, $$f_{t}$$ captures a contextualized feature at the position, and $$h_{t}$$ captures information about the mutated amino acid type. This mutation representation builds up a feature representing the amino acid substitution.\n\nDecoding single mutations. Figure 2b illustrates single mutation decoding. The change in thermodynamic stability $$\\Delta\\Delta G$$ for a single mutation \u00b5 is read out from its mutation representation $$z(\\mu)$$. A lightweight linear head $$g_{1} : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$$ decodes this representation into the change in thermodynamic stability: $$\\Delta\\Delta G_{pr} = g(z(\\mu))$$."}]}, {"page": 5, "text": "   P                                                                           A       C      D              Y          Head               \u0394\u0394G       for\n  A                                                                                    C      D              Y                              P1C:V4D\n   K                                                                           A       C      D              Y\n                                                     Amino Acid                A       C      D              Y          Head               \u0394\u0394G       for\n                                                                               A\n  V                                                   Expansion                A       C      D              Y                          M5Y:G6A:H7C\n  M           AlphaFold                                                        A       C      D              Y\n  G                                                                                           D                         Head               \u0394\u0394G  H7Y  for\n                                                                                       C\n                                                                               A                             Y\n  H\nFigure 3: Mutate Everything decodes many mutations. Each token output by the backbone xl is\nexpanded to 20 tokens z(\u00b5), each corresponding to one unique amino acid type (yellow with amino\nacid types). For each mutation set M, we first aggregate the features corresponding to each mutation\u2019s\nposition and amino acid being mutated to (arrows). A lightweight head g then decodes the aggregated\nfeatures to obtain the predicted \u2206\u2206G. These heads are illustrated in Figure 2b and Figure 2c. Parallel\nfeature aggregation and decoding enable efficient stability prediction for millions of mutations.\nDecoding higher-order mutations. Figure 2c illustrates higher-order mutation decoding. The\n\u2206\u2206G for a higher-order mutation M = {\u00b5k}K                                 k=1 is read out from its K corresponding single point\nmutation representations {z(\u00b5k)}K                       k=1. Mutate Everything aggregates the representations for each\nsingle point mutation within a higher-order mutation. By aggregating the single mutations in the\nlatent space, the model learns to account for the interactions between them. We aggregate with\nsummation for its simplicity. A lightweight head g : Rd \u2192                                      R then decodes the higher-order mutation\n\u2206\u2206Gpr = g( k z(\u00b5k)). This formulation allows us to seamlessly parameterize any higher-order\nmutation. For higher-order mutations (e.g. K > 1), we empirically find that it is beneficial to predict\nthe residual to the sum of single mutation \u2206\u2206Gs from our single mutation decoder (see Table 5d).\nDecoding a million mutations in one forward pass. This decoding scheme efficiently compute\n\u2206\u2206G for millions of mutations, whether single or higher-order (see Figure 3). Our insight is to\nprecompute all mutation features and reuse them for all predictions. First, we compute all L\u00d720\nmutation representations Z = {z((p, t)) : p \u2208                                 [1, ..., L], t \u2208      AA}. This expands each token xl to\n20 representations that each capture the effect of mutating to one specific amino acid (including the\nwild-type amino acid). The backbone and this expansion are executed once for a protein sequence.\nA lightweight head decodes \u2206\u2206G for any higher-order mutation (a mutation set). For each mutation\nset, we index and add the relevant mutation representations in Z. Index and sum operations are\nparallelized with outer-summation for K = 2 and the \u201cgather\u201d operation for K > 2. Finally, the\nlightweight decoder g computes the \u2206\u2206G for each mutation set in parallel.\nTraining. Our model is fine-tuned using supervised learning. We optimize the Huber loss.\n                                                                              N\n                        L(w, (Mi, \u2206\u2206Gexp)N          i      i=1) = 1     N    i=1   ||\u2206\u2206Gpr(w, Mi) \u2212                  \u2206\u2206Gexp||H i                          (1)\nwhere w is a protein sequence and (Mi, \u2206\u2206Gexp)N                                      i=1 are the mutation sets for this protein with\n                                                                              i\nexperimentally validated \u2206\u2206G. This approach efficiently backpropagates the loss through all labeled\nmutations for a given protein in a single pass, unlike prior work which backpropagates through\nonly one mutation in a single pass. We train on single and double mutants with on average 2000\nmutations for each protein. When training on double mutants, we learn the residual to the sum of\nthe experimental (ground truth) \u2206\u2206G for the constituent single mutations. Note that we use the\nexperimental \u2206\u2206G of the single mutants to train double mutants but during testing, we use the\nmodel\u2019s predicted \u2206\u2206G of the single mutants to infer double mutant \u2206\u2206G. Thus, we do not need\nexperimental data for single effects to predict combined effects.\nOnly 3% of the double mutants in our training set are stabilizing (\u2206\u2206G < -0.5 kcal/mol [8]). To\nimprove our performance on stabilizing mutations, we subsample the number of destabilizing double\nmutations to at most 8 times the number of stabilizing double mutations.\n                                                                                5", "md": "P                                                                           A       C      D              Y          Head               $$\\Delta\\Delta G$$       for\nA                                                                                    C      D              Y                              P1C:V4D\nK                                                                           A       C      D              Y\nAmino Acid                A       C      D              Y          Head               $$\\Delta\\Delta G$$       for\nA\nV                                                   Expansion                A       C      D              Y                          M5Y:G6A:H7C\nM           AlphaFold                                                        A       C      D              Y\nG                                                                                           D                         Head               $$\\Delta\\Delta G$$  H7Y  for\nC\nA                             Y\nH\n\nFigure 3: Mutate Everything decodes many mutations. Each token output by the backbone xl is\nexpanded to 20 tokens z(\u00b5), each corresponding to one unique amino acid type (yellow with amino\nacid types). For each mutation set M, we first aggregate the features corresponding to each mutation\u2019s\nposition and amino acid being mutated to (arrows). A lightweight head g then decodes the aggregated\nfeatures to obtain the predicted $$\\Delta\\Delta G$$. These heads are illustrated in Figure 2b and Figure 2c. Parallel\nfeature aggregation and decoding enable efficient stability prediction for millions of mutations.\nDecoding higher-order mutations. Figure 2c illustrates higher-order mutation decoding. The\n$$\\Delta\\Delta G$$ for a higher-order mutation M = {\u00b5k}K                                 k=1 is read out from its K corresponding single point\nmutation representations {z(\u00b5k)}K                       k=1. Mutate Everything aggregates the representations for each\nsingle point mutation within a higher-order mutation. By aggregating the single mutations in the\nlatent space, the model learns to account for the interactions between them. We aggregate with\nsummation for its simplicity. A lightweight head g : Rd \u2192                                      R then decodes the higher-order mutation\n$$\\Delta\\Delta G_{pr} = g(\\sum_{k} z(\u00b5_k))$$. This formulation allows us to seamlessly parameterize any higher-order\nmutation. For higher-order mutations (e.g. K > 1), we empirically find that it is beneficial to predict\nthe residual to the sum of single mutation $$\\Delta\\Delta G$$s from our single mutation decoder (see Table 5d).\nDecoding a million mutations in one forward pass. This decoding scheme efficiently compute\n$$\\Delta\\Delta G$$ for millions of mutations, whether single or higher-order (see Figure 3). Our insight is to\nprecompute all mutation features and reuse them for all predictions. First, we compute all L\u00d720\nmutation representations Z = {z((p, t)) : p \u2208                                 [1, ..., L], t \u2208      AA}. This expands each token xl to\n20 representations that each capture the effect of mutating to one specific amino acid (including the\nwild-type amino acid). The backbone and this expansion are executed once for a protein sequence.\nA lightweight head decodes $$\\Delta\\Delta G$$ for any higher-order mutation (a mutation set). For each mutation\nset, we index and add the relevant mutation representations in Z. Index and sum operations are\nparallelized with outer-summation for K = 2 and the \u201cgather\u201d operation for K > 2. Finally, the\nlightweight decoder g computes the $$\\Delta\\Delta G$$ for each mutation set in parallel.\nTraining. Our model is fine-tuned using supervised learning. We optimize the Huber loss.\nN\nL(w, (Mi, $$\\Delta\\Delta G_{exp}$$)N          i      i=1) = 1     N    i=1   ||$$\\Delta\\Delta G_{pr}(w, Mi) \u2212                  $$\\Delta\\Delta G_{exp}$$||H i                          (1)\nwhere w is a protein sequence and (Mi, $$\\Delta\\Delta G_{exp}$$)N                                      i=1 are the mutation sets for this protein with\ni\nexperimentally validated $$\\Delta\\Delta G$$. This approach efficiently backpropagates the loss through all labeled\nmutations for a given protein in a single pass, unlike prior work which backpropagates through\nonly one mutation in a single pass. We train on single and double mutants with on average 2000\nmutations for each protein. When training on double mutants, we learn the residual to the sum of\nthe experimental (ground truth) $$\\Delta\\Delta G$$ for the constituent single mutations. Note that we use the\nexperimental $$\\Delta\\Delta G$$ of the single mutants to train double mutants but during testing, we use the\nmodel\u2019s predicted $$\\Delta\\Delta G$$ of the single mutants to infer double mutant $$\\Delta\\Delta G$$. Thus, we do not need\nexperimental data for single effects to predict combined effects.\nOnly 3% of the double mutants in our training set are stabilizing ($$\\Delta\\Delta G$$ < -0.5 kcal/mol [8]). To\nimprove our performance on stabilizing mutations, we subsample the number of destabilizing double\nmutations to at most 8 times the number of stabilizing double mutations.\n5", "images": [{"name": "page-5-0.jpg", "height": 52, "width": 156, "x": 147, "y": 99}], "items": [{"type": "text", "value": "P                                                                           A       C      D              Y          Head               $$\\Delta\\Delta G$$       for\nA                                                                                    C      D              Y                              P1C:V4D\nK                                                                           A       C      D              Y\nAmino Acid                A       C      D              Y          Head               $$\\Delta\\Delta G$$       for\nA\nV                                                   Expansion                A       C      D              Y                          M5Y:G6A:H7C\nM           AlphaFold                                                        A       C      D              Y\nG                                                                                           D                         Head               $$\\Delta\\Delta G$$  H7Y  for\nC\nA                             Y\nH\n\nFigure 3: Mutate Everything decodes many mutations. Each token output by the backbone xl is\nexpanded to 20 tokens z(\u00b5), each corresponding to one unique amino acid type (yellow with amino\nacid types). For each mutation set M, we first aggregate the features corresponding to each mutation\u2019s\nposition and amino acid being mutated to (arrows). A lightweight head g then decodes the aggregated\nfeatures to obtain the predicted $$\\Delta\\Delta G$$. These heads are illustrated in Figure 2b and Figure 2c. Parallel\nfeature aggregation and decoding enable efficient stability prediction for millions of mutations.\nDecoding higher-order mutations. Figure 2c illustrates higher-order mutation decoding. The\n$$\\Delta\\Delta G$$ for a higher-order mutation M = {\u00b5k}K                                 k=1 is read out from its K corresponding single point\nmutation representations {z(\u00b5k)}K                       k=1. Mutate Everything aggregates the representations for each\nsingle point mutation within a higher-order mutation. By aggregating the single mutations in the\nlatent space, the model learns to account for the interactions between them. We aggregate with\nsummation for its simplicity. A lightweight head g : Rd \u2192                                      R then decodes the higher-order mutation\n$$\\Delta\\Delta G_{pr} = g(\\sum_{k} z(\u00b5_k))$$. This formulation allows us to seamlessly parameterize any higher-order\nmutation. For higher-order mutations (e.g. K > 1), we empirically find that it is beneficial to predict\nthe residual to the sum of single mutation $$\\Delta\\Delta G$$s from our single mutation decoder (see Table 5d).\nDecoding a million mutations in one forward pass. This decoding scheme efficiently compute\n$$\\Delta\\Delta G$$ for millions of mutations, whether single or higher-order (see Figure 3). Our insight is to\nprecompute all mutation features and reuse them for all predictions. First, we compute all L\u00d720\nmutation representations Z = {z((p, t)) : p \u2208                                 [1, ..., L], t \u2208      AA}. This expands each token xl to\n20 representations that each capture the effect of mutating to one specific amino acid (including the\nwild-type amino acid). The backbone and this expansion are executed once for a protein sequence.\nA lightweight head decodes $$\\Delta\\Delta G$$ for any higher-order mutation (a mutation set). For each mutation\nset, we index and add the relevant mutation representations in Z. Index and sum operations are\nparallelized with outer-summation for K = 2 and the \u201cgather\u201d operation for K > 2. Finally, the\nlightweight decoder g computes the $$\\Delta\\Delta G$$ for each mutation set in parallel.\nTraining. Our model is fine-tuned using supervised learning. We optimize the Huber loss.\nN\nL(w, (Mi, $$\\Delta\\Delta G_{exp}$$)N          i      i=1) = 1     N    i=1   ||$$\\Delta\\Delta G_{pr}(w, Mi) \u2212                  $$\\Delta\\Delta G_{exp}$$||H i                          (1)\nwhere w is a protein sequence and (Mi, $$\\Delta\\Delta G_{exp}$$)N                                      i=1 are the mutation sets for this protein with\ni\nexperimentally validated $$\\Delta\\Delta G$$. This approach efficiently backpropagates the loss through all labeled\nmutations for a given protein in a single pass, unlike prior work which backpropagates through\nonly one mutation in a single pass. We train on single and double mutants with on average 2000\nmutations for each protein. When training on double mutants, we learn the residual to the sum of\nthe experimental (ground truth) $$\\Delta\\Delta G$$ for the constituent single mutations. Note that we use the\nexperimental $$\\Delta\\Delta G$$ of the single mutants to train double mutants but during testing, we use the\nmodel\u2019s predicted $$\\Delta\\Delta G$$ of the single mutants to infer double mutant $$\\Delta\\Delta G$$. Thus, we do not need\nexperimental data for single effects to predict combined effects.\nOnly 3% of the double mutants in our training set are stabilizing ($$\\Delta\\Delta G$$ < -0.5 kcal/mol [8]). To\nimprove our performance on stabilizing mutations, we subsample the number of destabilizing double\nmutations to at most 8 times the number of stabilizing double mutations.\n5", "md": "P                                                                           A       C      D              Y          Head               $$\\Delta\\Delta G$$       for\nA                                                                                    C      D              Y                              P1C:V4D\nK                                                                           A       C      D              Y\nAmino Acid                A       C      D              Y          Head               $$\\Delta\\Delta G$$       for\nA\nV                                                   Expansion                A       C      D              Y                          M5Y:G6A:H7C\nM           AlphaFold                                                        A       C      D              Y\nG                                                                                           D                         Head               $$\\Delta\\Delta G$$  H7Y  for\nC\nA                             Y\nH\n\nFigure 3: Mutate Everything decodes many mutations. Each token output by the backbone xl is\nexpanded to 20 tokens z(\u00b5), each corresponding to one unique amino acid type (yellow with amino\nacid types). For each mutation set M, we first aggregate the features corresponding to each mutation\u2019s\nposition and amino acid being mutated to (arrows). A lightweight head g then decodes the aggregated\nfeatures to obtain the predicted $$\\Delta\\Delta G$$. These heads are illustrated in Figure 2b and Figure 2c. Parallel\nfeature aggregation and decoding enable efficient stability prediction for millions of mutations.\nDecoding higher-order mutations. Figure 2c illustrates higher-order mutation decoding. The\n$$\\Delta\\Delta G$$ for a higher-order mutation M = {\u00b5k}K                                 k=1 is read out from its K corresponding single point\nmutation representations {z(\u00b5k)}K                       k=1. Mutate Everything aggregates the representations for each\nsingle point mutation within a higher-order mutation. By aggregating the single mutations in the\nlatent space, the model learns to account for the interactions between them. We aggregate with\nsummation for its simplicity. A lightweight head g : Rd \u2192                                      R then decodes the higher-order mutation\n$$\\Delta\\Delta G_{pr} = g(\\sum_{k} z(\u00b5_k))$$. This formulation allows us to seamlessly parameterize any higher-order\nmutation. For higher-order mutations (e.g. K > 1), we empirically find that it is beneficial to predict\nthe residual to the sum of single mutation $$\\Delta\\Delta G$$s from our single mutation decoder (see Table 5d).\nDecoding a million mutations in one forward pass. This decoding scheme efficiently compute\n$$\\Delta\\Delta G$$ for millions of mutations, whether single or higher-order (see Figure 3). Our insight is to\nprecompute all mutation features and reuse them for all predictions. First, we compute all L\u00d720\nmutation representations Z = {z((p, t)) : p \u2208                                 [1, ..., L], t \u2208      AA}. This expands each token xl to\n20 representations that each capture the effect of mutating to one specific amino acid (including the\nwild-type amino acid). The backbone and this expansion are executed once for a protein sequence.\nA lightweight head decodes $$\\Delta\\Delta G$$ for any higher-order mutation (a mutation set). For each mutation\nset, we index and add the relevant mutation representations in Z. Index and sum operations are\nparallelized with outer-summation for K = 2 and the \u201cgather\u201d operation for K > 2. Finally, the\nlightweight decoder g computes the $$\\Delta\\Delta G$$ for each mutation set in parallel.\nTraining. Our model is fine-tuned using supervised learning. We optimize the Huber loss.\nN\nL(w, (Mi, $$\\Delta\\Delta G_{exp}$$)N          i      i=1) = 1     N    i=1   ||$$\\Delta\\Delta G_{pr}(w, Mi) \u2212                  $$\\Delta\\Delta G_{exp}$$||H i                          (1)\nwhere w is a protein sequence and (Mi, $$\\Delta\\Delta G_{exp}$$)N                                      i=1 are the mutation sets for this protein with\ni\nexperimentally validated $$\\Delta\\Delta G$$. This approach efficiently backpropagates the loss through all labeled\nmutations for a given protein in a single pass, unlike prior work which backpropagates through\nonly one mutation in a single pass. We train on single and double mutants with on average 2000\nmutations for each protein. When training on double mutants, we learn the residual to the sum of\nthe experimental (ground truth) $$\\Delta\\Delta G$$ for the constituent single mutations. Note that we use the\nexperimental $$\\Delta\\Delta G$$ of the single mutants to train double mutants but during testing, we use the\nmodel\u2019s predicted $$\\Delta\\Delta G$$ of the single mutants to infer double mutant $$\\Delta\\Delta G$$. Thus, we do not need\nexperimental data for single effects to predict combined effects.\nOnly 3% of the double mutants in our training set are stabilizing ($$\\Delta\\Delta G$$ < -0.5 kcal/mol [8]). To\nimprove our performance on stabilizing mutations, we subsample the number of destabilizing double\nmutations to at most 8 times the number of stabilizing double mutations.\n5"}]}, {"page": 6, "text": "  Method                          Stabilizing rs     nDCG          DetPr         AUC          MCC\n  Mean                                 0.00           0.02          0.03         0.50          0.00\n  MSA                                  0.04           0.06          0.02         0.60          0.01\n  ESM2 [38]                            0.04           0.08          0.02         0.65          0.00\n  PROSTATA [74] (Additive)             0.08           0.15          0.05         0.75          0.05\n  Mutate Everything (Additive)      0.08(0.03)     0.25(0.01)    0.10(0.01)   0.81(0.01)    0.25(0.02)\n  Mutate Everything (Ours)          0.14(0.02)     0.43 (0.02) 0.16 (0.01) 0.84 (0.01) 0.27 (0.01)\nTable 1: Stabilizing Double Mutation Results on cDNA2. The metrics evaluate the model\u2019s\nperformance on stabilizing mutations. Our additive baseline naively adds predicted single mutation\n\u2206\u2206Gs. Our model is state-of-the-art at finding stabilizing mutations. The proteins in cDNA2 val\nhave at most 36% sequence similarity to those in the training set. Parenthesis indicates standard error\nacross 7 training runs.\n5    Results\n5.1   Implementation Details\nOur primary feature extractor is AlphaFold, implemented in OpenFold [3, 31]. The multiple sequence\nalignment (MSA) is computed using Colabfold [43]. Sequence features from the Evoformer and\nStructure Module are aggregated as input to the decoder. An adapter maps the backbone hidden\ndimension to D = 128. Our amino acid projections and single mutation decoders are implemented\nwith a linear layer. The higher-order mutation decoder transforms the previous embedding with a\n2-layer MLP. These representations are aggregated and fed into a 3-layer MLP to predict \u2206\u2206G.\nWe train on the cDNA display proteolysis dataset [72], which leverages a high throughput proteolysis\nscreen enabled by next generation sequencing (NGS) to extract noisy \u2206\u2206G values for 100 mini-\nproteins totaling over 100,000 single and double mutations. To evaluate generalization on unseen\nproteins, we remove proteins from our training set that have high sequence similarity using MMSeqs2\n[70] to any protein in our evaluation benchmarks. The short protein lengths (< 100 amino acids) keep\nthe memory requirement small for the higher-order decoder. See Section A.1 for more information\nabout cDNA and other datasets.\nWe fine-tune a pre-trained backbone on single mutations for 20 epochs. Then, we finetune the model\non both single and double mutations for 100 epochs using a cosine learning rate schedule with 10\nwarmup epochs. We use a batch size of 3 proteins due to the high memory requirements of AlphaFold.\nWe use a learning rate of 3\u00d710\u22124 and weight decay of 0.5. Training takes 6 hours on 3 A100 GPUs.\n5.2   Finding Stabilizing Double Mutations\nWe evaluate our model\u2019s ability to find the stabilizing mutations on cDNA2. cDNA2 is our validation\nsplit of the cDNA double mutations dataset, consisting of 18 mini-proteins totaling 22,000 double\nmutations. The proteins in the validation set have at most 36% homology with those in the cDNA\ntraining set. Of these mutations, only 198 or 0.8% are stabilizing with \u2206\u2206G < \u22120.5 kcal/mol [8].\nLike protein engineering, this scenario closely resembles the challenge of identifying a small number\nof stabilizing mutations amidst a much larger pool of destabilizing or neutral mutations.\nWe evaluate different methods on stabilizing mutations in Table 1. Stabilizing rs is the Spearman\ncoefficient on the experimentally stabilizing subset introduced in [65]. Normalized discounted\ncumulative gain (nDCG) [30] measures the quality of the ranked mutation list by taking into account\nits experimentally validated \u2206\u2206G and position in the list [59]. Detection Precision (DetPr) is the\nproportion of experimentally validated stabilizing mutations among the top K = 30 predictions. We\nadapt previous state-of-the-art single mutation predictors by naively adding both single mutation\npredictions. A detailed description of the metrics is found in Section A.2.\nOur model demonstrates exceptional performance in prioritizing stabilizing double mutations over\ndestabilizing ones, achieving a significantly higher normalized discounted cumulative gain of 0.43\ncompared to 0.25, as well as a superior detection precision of 0.16 compared to 0.10. Our model\nadditionally improves classification metrics Matthews Correlation Coefficient (MCC) and Area under\nPrecision-Recall Curve (AUC) by 0.02 and 0.03, respectively.\n                                                    6", "md": "# Stabilizing Double Mutation Results\n\n## Table 1: Stabilizing Double Mutation Results on cDNA2\n\n|Method|Stabilizing rs|nDCG|DetPr|AUC|MCC|\n|---|---|---|---|---|---|\n|Mean|0.00|0.02|0.03|0.50|0.00|\n|MSA|0.04|0.06|0.02|0.60|0.01|\n|ESM2 [38]|0.04|0.08|0.02|0.65|0.00|\n|PROSTATA [74] (Additive)|0.08|0.15|0.05|0.75|0.05|\n|Mutate Everything (Additive)|0.08(0.03)|0.25(0.01)|0.10(0.01)|0.81(0.01)|0.25(0.02)|\n|Mutate Everything (Ours)|0.14(0.02)|0.43(0.02)|0.16(0.01)|0.84(0.01)|0.27(0.01)|\n\n## Results\n\n### Implementation Details\n\nOur primary feature extractor is AlphaFold, implemented in OpenFold [3, 31]. The multiple sequence alignment (MSA) is computed using Colabfold [43]. Sequence features from the Evoformer and Structure Module are aggregated as input to the decoder. An adapter maps the backbone hidden dimension to D = 128. Our amino acid projections and single mutation decoders are implemented with a linear layer. The higher-order mutation decoder transforms the previous embedding with a 2-layer MLP. These representations are aggregated and fed into a 3-layer MLP to predict $$\\Delta\\Delta G$$.\n\nWe train on the cDNA display proteolysis dataset [72], which leverages a high throughput proteolysis screen enabled by next generation sequencing (NGS) to extract noisy $$\\Delta\\Delta G$$ values for 100 mini-proteins totaling over 100,000 single and double mutations. To evaluate generalization on unseen proteins, we remove proteins from our training set that have high sequence similarity using MMSeqs2 [70] to any protein in our evaluation benchmarks. The short protein lengths (< 100 amino acids) keep the memory requirement small for the higher-order decoder. See Section A.1 for more information about cDNA and other datasets.\n\n### Finding Stabilizing Double Mutations\n\nWe evaluate our model\u2019s ability to find the stabilizing mutations on cDNA2. cDNA2 is our validation split of the cDNA double mutations dataset, consisting of 18 mini-proteins totaling 22,000 double mutations. The proteins in the validation set have at most 36% homology with those in the cDNA training set. Of these mutations, only 198 or 0.8% are stabilizing with $$\\Delta\\Delta G < -0.5$$ kcal/mol [8].\n\nLike protein engineering, this scenario closely resembles the challenge of identifying a small number of stabilizing mutations amidst a much larger pool of destabilizing or neutral mutations.\n\n### Evaluation Metrics\n\nWe evaluate different methods on stabilizing mutations in Table 1. Stabilizing rs is the Spearman coefficient on the experimentally stabilizing subset introduced in [65]. Normalized discounted cumulative gain (nDCG) [30] measures the quality of the ranked mutation list by taking into account its experimentally validated $$\\Delta\\Delta G$$ and position in the list [59]. Detection Precision (DetPr) is the proportion of experimentally validated stabilizing mutations among the top $$K = 30$$ predictions.\n\nOur model demonstrates exceptional performance in prioritizing stabilizing double mutations over destabilizing ones, achieving a significantly higher normalized discounted cumulative gain of 0.43 compared to 0.25, as well as a superior detection precision of 0.16 compared to 0.10. Our model additionally improves classification metrics Matthews Correlation Coefficient (MCC) and Area under Precision-Recall Curve (AUC) by 0.02 and 0.03, respectively.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Stabilizing Double Mutation Results", "md": "# Stabilizing Double Mutation Results"}, {"type": "heading", "lvl": 2, "value": "Table 1: Stabilizing Double Mutation Results on cDNA2", "md": "## Table 1: Stabilizing Double Mutation Results on cDNA2"}, {"type": "table", "rows": [["Method", "Stabilizing rs", "nDCG", "DetPr", "AUC", "MCC"], ["Mean", "0.00", "0.02", "0.03", "0.50", "0.00"], ["MSA", "0.04", "0.06", "0.02", "0.60", "0.01"], ["ESM2 [38]", "0.04", "0.08", "0.02", "0.65", "0.00"], ["PROSTATA [74] (Additive)", "0.08", "0.15", "0.05", "0.75", "0.05"], ["Mutate Everything (Additive)", "0.08(0.03)", "0.25(0.01)", "0.10(0.01)", "0.81(0.01)", "0.25(0.02)"], ["Mutate Everything (Ours)", "0.14(0.02)", "0.43(0.02)", "0.16(0.01)", "0.84(0.01)", "0.27(0.01)"]], "md": "|Method|Stabilizing rs|nDCG|DetPr|AUC|MCC|\n|---|---|---|---|---|---|\n|Mean|0.00|0.02|0.03|0.50|0.00|\n|MSA|0.04|0.06|0.02|0.60|0.01|\n|ESM2 [38]|0.04|0.08|0.02|0.65|0.00|\n|PROSTATA [74] (Additive)|0.08|0.15|0.05|0.75|0.05|\n|Mutate Everything (Additive)|0.08(0.03)|0.25(0.01)|0.10(0.01)|0.81(0.01)|0.25(0.02)|\n|Mutate Everything (Ours)|0.14(0.02)|0.43(0.02)|0.16(0.01)|0.84(0.01)|0.27(0.01)|", "isPerfectTable": true, "csv": "\"Method\",\"Stabilizing rs\",\"nDCG\",\"DetPr\",\"AUC\",\"MCC\"\n\"Mean\",\"0.00\",\"0.02\",\"0.03\",\"0.50\",\"0.00\"\n\"MSA\",\"0.04\",\"0.06\",\"0.02\",\"0.60\",\"0.01\"\n\"ESM2 [38]\",\"0.04\",\"0.08\",\"0.02\",\"0.65\",\"0.00\"\n\"PROSTATA [74] (Additive)\",\"0.08\",\"0.15\",\"0.05\",\"0.75\",\"0.05\"\n\"Mutate Everything (Additive)\",\"0.08(0.03)\",\"0.25(0.01)\",\"0.10(0.01)\",\"0.81(0.01)\",\"0.25(0.02)\"\n\"Mutate Everything (Ours)\",\"0.14(0.02)\",\"0.43(0.02)\",\"0.16(0.01)\",\"0.84(0.01)\",\"0.27(0.01)\""}, {"type": "heading", "lvl": 2, "value": "Results", "md": "## Results"}, {"type": "heading", "lvl": 3, "value": "Implementation Details", "md": "### Implementation Details"}, {"type": "text", "value": "Our primary feature extractor is AlphaFold, implemented in OpenFold [3, 31]. The multiple sequence alignment (MSA) is computed using Colabfold [43]. Sequence features from the Evoformer and Structure Module are aggregated as input to the decoder. An adapter maps the backbone hidden dimension to D = 128. Our amino acid projections and single mutation decoders are implemented with a linear layer. The higher-order mutation decoder transforms the previous embedding with a 2-layer MLP. These representations are aggregated and fed into a 3-layer MLP to predict $$\\Delta\\Delta G$$.\n\nWe train on the cDNA display proteolysis dataset [72], which leverages a high throughput proteolysis screen enabled by next generation sequencing (NGS) to extract noisy $$\\Delta\\Delta G$$ values for 100 mini-proteins totaling over 100,000 single and double mutations. To evaluate generalization on unseen proteins, we remove proteins from our training set that have high sequence similarity using MMSeqs2 [70] to any protein in our evaluation benchmarks. The short protein lengths (< 100 amino acids) keep the memory requirement small for the higher-order decoder. See Section A.1 for more information about cDNA and other datasets.", "md": "Our primary feature extractor is AlphaFold, implemented in OpenFold [3, 31]. The multiple sequence alignment (MSA) is computed using Colabfold [43]. Sequence features from the Evoformer and Structure Module are aggregated as input to the decoder. An adapter maps the backbone hidden dimension to D = 128. Our amino acid projections and single mutation decoders are implemented with a linear layer. The higher-order mutation decoder transforms the previous embedding with a 2-layer MLP. These representations are aggregated and fed into a 3-layer MLP to predict $$\\Delta\\Delta G$$.\n\nWe train on the cDNA display proteolysis dataset [72], which leverages a high throughput proteolysis screen enabled by next generation sequencing (NGS) to extract noisy $$\\Delta\\Delta G$$ values for 100 mini-proteins totaling over 100,000 single and double mutations. To evaluate generalization on unseen proteins, we remove proteins from our training set that have high sequence similarity using MMSeqs2 [70] to any protein in our evaluation benchmarks. The short protein lengths (< 100 amino acids) keep the memory requirement small for the higher-order decoder. See Section A.1 for more information about cDNA and other datasets."}, {"type": "heading", "lvl": 3, "value": "Finding Stabilizing Double Mutations", "md": "### Finding Stabilizing Double Mutations"}, {"type": "text", "value": "We evaluate our model\u2019s ability to find the stabilizing mutations on cDNA2. cDNA2 is our validation split of the cDNA double mutations dataset, consisting of 18 mini-proteins totaling 22,000 double mutations. The proteins in the validation set have at most 36% homology with those in the cDNA training set. Of these mutations, only 198 or 0.8% are stabilizing with $$\\Delta\\Delta G < -0.5$$ kcal/mol [8].\n\nLike protein engineering, this scenario closely resembles the challenge of identifying a small number of stabilizing mutations amidst a much larger pool of destabilizing or neutral mutations.", "md": "We evaluate our model\u2019s ability to find the stabilizing mutations on cDNA2. cDNA2 is our validation split of the cDNA double mutations dataset, consisting of 18 mini-proteins totaling 22,000 double mutations. The proteins in the validation set have at most 36% homology with those in the cDNA training set. Of these mutations, only 198 or 0.8% are stabilizing with $$\\Delta\\Delta G < -0.5$$ kcal/mol [8].\n\nLike protein engineering, this scenario closely resembles the challenge of identifying a small number of stabilizing mutations amidst a much larger pool of destabilizing or neutral mutations."}, {"type": "heading", "lvl": 3, "value": "Evaluation Metrics", "md": "### Evaluation Metrics"}, {"type": "text", "value": "We evaluate different methods on stabilizing mutations in Table 1. Stabilizing rs is the Spearman coefficient on the experimentally stabilizing subset introduced in [65]. Normalized discounted cumulative gain (nDCG) [30] measures the quality of the ranked mutation list by taking into account its experimentally validated $$\\Delta\\Delta G$$ and position in the list [59]. Detection Precision (DetPr) is the proportion of experimentally validated stabilizing mutations among the top $$K = 30$$ predictions.\n\nOur model demonstrates exceptional performance in prioritizing stabilizing double mutations over destabilizing ones, achieving a significantly higher normalized discounted cumulative gain of 0.43 compared to 0.25, as well as a superior detection precision of 0.16 compared to 0.10. Our model additionally improves classification metrics Matthews Correlation Coefficient (MCC) and Area under Precision-Recall Curve (AUC) by 0.02 and 0.03, respectively.", "md": "We evaluate different methods on stabilizing mutations in Table 1. Stabilizing rs is the Spearman coefficient on the experimentally stabilizing subset introduced in [65]. Normalized discounted cumulative gain (nDCG) [30] measures the quality of the ranked mutation list by taking into account its experimentally validated $$\\Delta\\Delta G$$ and position in the list [59]. Detection Precision (DetPr) is the proportion of experimentally validated stabilizing mutations among the top $$K = 30$$ predictions.\n\nOur model demonstrates exceptional performance in prioritizing stabilizing double mutations over destabilizing ones, achieving a significantly higher normalized discounted cumulative gain of 0.43 compared to 0.25, as well as a superior detection precision of 0.16 compared to 0.10. Our model additionally improves classification metrics Matthews Correlation Coefficient (MCC) and Area under Precision-Recall Curve (AUC) by 0.02 and 0.03, respectively."}]}, {"page": 7, "text": "    Method                                          rs            AUC              MCC           RMSE \u2193          Stabilizing rs\n    Mean (cDNA)                                   0.00             0.50            0.00             2.42               0.00\n    Mean (ProTherm)                               0.00             0.50            0.00             2.26               0.00\n    MSA                                           0.07             0.51            -0.05            N/A                0.02\n    ESM2 [62]                                     0.05             0.51            0.04             N/A               -0.01\n    FoldX [67]                                    0.41               -                -             2.95                 -\n    DDGun [44]                                    0.25             0.63            0.16             2.21               0.17\n    DDGun3D [44]                                  0.26             0.64            0.18             2.24               0.17\n    PROSTATA [74] (Additive)                      0.21             0.60            0.05             2.25               0.00\n    Mutate Everything (Additive) 0.50(0.02) 0.76(0.01) 0.37(0.02) 2.02(0.03)                                       0.20(0.01)\n    Mutate Everything (Ours)                  0.53(0.01) 0.78(0.01) 0.43(0.01) 2.04(0.01)                          0.19(0.01)\nTable 2: Multiple Mutation Results on PTMul. Our additive baseline naively adds predicted single\nmutation \u2206\u2206Gs. Our model presents a strong mutation assessor. The proteins in PTMul have at\nmost 35% sequence similarity to those in the training set. Parenthesis is standard error across 7 runs.\nTo the best of our knowledge, Mutate Everything is the first                        106         Mutate Everything (AF)\nwork that models all double mutations in a computationally                          105         Mutate Everything (ESM)\ntractable manner. Figure 4 shows the runtime of several                                         PROSTATA\n                                                                                    Runtime (sec)\nmethods on a protein of 317 amino acids on an A100 GPU.                             104\nThe dashed line indicates the transition from evaluating                            103\nsingle mutants to double mutants. Mutate Everything                                 102\npredicts \u2206\u2206G for all single and double mutations in one\npass of the model. It runs in 0.6 seconds using an ESM2                             101\nbackbone, and 12.1 seconds on an AlphaFold backbone.                                100\nPROSTATA [74] also uses the ESM2 backbone but takes                                             102          104          106\na protein sequence and its mutated sequence as input and                                              Number of Mutations\noutputs the change in stability. PROSTATA takes 306\nhours to evaluate all double mutations with a batch size of                      Figure 4:        Runtime analysis.              Our\n768. On an 8 GPU node, this will take 1.5 days.                                  model\u2019s runtime is constant.\n5.3     Higher-order Mutation Results\nWe evaluate our model for predicting changes in thermodynamic stability for higher-order mutations\nin ProTherm. ProTherm is a database of thermodynamic (\u2206\u2206G) and thermostability (\u2206Tm) ex-\nperimental characterization of protein mutations curated from the literature. We consider a subset\nof this database that contains \u2206\u2206G values for higher-order mutations, named ProTherm Multiple\n(PTMul). PTMul contains 858 mutations [44]. We keep 846 experimentally validated \u2206\u2206G for\nhigher-order mutants, including 536 doubles and 183 triples, after removing 4 duplicate and 8\nambiguous mutations. PTMul proteins have at most 35% homology to the proteins used in training.\nTable 2 shows our results on PTMul. A description of baselines is provided in Section A.3. Our\nadditive baselines naively add the sum of single mutation \u2206\u2206Gs. This baseline rigorously evaluates\nour model\u2019s ability to learn epistasis, or the non-additive interactions when combining two mutations.\nMean always predicts the global test dataset mean statistic. ESM and MSA perform poorly when\ncomparing mutations across proteins. On ProTherm, Mutate Everything achieves the state-of-the-art\nperformance of 0.53 Spearman correlation rs, compared to 0.50 of our additive baseline. A breakdown\nof PTMul performance on double mutations and more (> 2) mutations is found in Table 6.\nWhile other methods also handle multiple mutations, they usually adopt a mutation- or protein-level\ntrain and test split. This results in data leakage since the same protein or a homolog will have\nmutations in both the training and testing sets, leading to inflated reported metrics [16, 18, 35, 64].\nIn our study, we use sequence similarity-based splitting where our training proteins have at most 35%\nhomology to those in PTMul. To fairly evaluate generalization to new proteins, we exclude these\ninflated comparisons from our study.\n                                                                   7", "md": "|Method|rs|AUC|MCC|RMSE \u2193|Stabilizing rs|\n|---|---|---|---|---|---|\n|Mean (cDNA)|0.00|0.50|0.00|2.42|0.00|\n|Mean (ProTherm)|0.00|0.50|0.00|2.26|0.00|\n|MSA|0.07|0.51|-0.05|N/A|0.02|\n|ESM2 [62]|0.05|0.51|0.04|N/A|-0.01|\n|FoldX [67]|0.41|-|-|2.95|-|\n|DDGun [44]|0.25|0.63|0.16|2.21|0.17|\n|DDGun3D [44]|0.26|0.64|0.18|2.24|0.17|\n|PROSTATA [74] (Additive)|0.21|0.60|0.05|2.25|0.00|\n|Mutate Everything (Additive)|0.50(0.02)|0.76(0.01)|0.37(0.02)|2.02(0.03)|0.20(0.01)|\n|Mutate Everything (Ours)|0.53(0.01)|0.78(0.01)|0.43(0.01)|2.04(0.01)|0.19(0.01)|\n\nTable 2: Multiple Mutation Results on PTMul. Our additive baseline naively adds predicted single mutation \u2206\u2206Gs. Our model presents a strong mutation assessor. The proteins in PTMul have at most 35% sequence similarity to those in the training set. Parenthesis is standard error across 7 runs.\n\nTo the best of our knowledge, Mutate Everything is the first work that models all double mutations in a computationally tractable manner. Figure 4 shows the runtime of several methods on a protein of 317 amino acids on an A100 GPU. The dashed line indicates the transition from evaluating single mutants to double mutants. Mutate Everything predicts \u2206\u2206G for all single and double mutations in one pass of the model. It runs in 0.6 seconds using an ESM2 backbone, and 12.1 seconds on an AlphaFold backbone. PROSTATA [74] also uses the ESM2 backbone but takes a protein sequence and its mutated sequence as input and outputs the change in stability. PROSTATA takes 306 hours to evaluate all double mutations with a batch size of 768. On an 8 GPU node, this will take 1.5 days.\n\n5.3 Higher-order Mutation Results\n\nWe evaluate our model for predicting changes in thermodynamic stability for higher-order mutations in ProTherm. ProTherm is a database of thermodynamic (\u2206\u2206G) and thermostability (\u2206Tm) experimental characterization of protein mutations curated from the literature. We consider a subset of this database that contains \u2206\u2206G values for higher-order mutations, named ProTherm Multiple (PTMul). PTMul contains 858 mutations [44]. We keep 846 experimentally validated \u2206\u2206G for higher-order mutants, including 536 doubles and 183 triples, after removing 4 duplicate and 8 ambiguous mutations. PTMul proteins have at most 35% homology to the proteins used in training. Table 2 shows our results on PTMul. A description of baselines is provided in Section A.3. Our additive baselines naively add the sum of single mutation \u2206\u2206Gs. This baseline rigorously evaluates our model\u2019s ability to learn epistasis, or the non-additive interactions when combining two mutations. Mean always predicts the global test dataset mean statistic. ESM and MSA perform poorly when comparing mutations across proteins. On ProTherm, Mutate Everything achieves the state-of-the-art performance of 0.53 Spearman correlation rs, compared to 0.50 of our additive baseline. A breakdown of PTMul performance on double mutations and more (> 2) mutations is found in Table 6.\n\nWhile other methods also handle multiple mutations, they usually adopt a mutation- or protein-level train and test split. This results in data leakage since the same protein or a homolog will have mutations in both the training and testing sets, leading to inflated reported metrics [16, 18, 35, 64]. In our study, we use sequence similarity-based splitting where our training proteins have at most 35% homology to those in PTMul. To fairly evaluate generalization to new proteins, we exclude these inflated comparisons from our study.", "images": [], "items": [{"type": "table", "rows": [["Method", "rs", "AUC", "MCC", "RMSE \u2193", "Stabilizing rs"], ["Mean (cDNA)", "0.00", "0.50", "0.00", "2.42", "0.00"], ["Mean (ProTherm)", "0.00", "0.50", "0.00", "2.26", "0.00"], ["MSA", "0.07", "0.51", "-0.05", "N/A", "0.02"], ["ESM2 [62]", "0.05", "0.51", "0.04", "N/A", "-0.01"], ["FoldX [67]", "0.41", "-", "-", "2.95", "-"], ["DDGun [44]", "0.25", "0.63", "0.16", "2.21", "0.17"], ["DDGun3D [44]", "0.26", "0.64", "0.18", "2.24", "0.17"], ["PROSTATA [74] (Additive)", "0.21", "0.60", "0.05", "2.25", "0.00"], ["Mutate Everything (Additive)", "0.50(0.02)", "0.76(0.01)", "0.37(0.02)", "2.02(0.03)", "0.20(0.01)"], ["Mutate Everything (Ours)", "0.53(0.01)", "0.78(0.01)", "0.43(0.01)", "2.04(0.01)", "0.19(0.01)"]], "md": "|Method|rs|AUC|MCC|RMSE \u2193|Stabilizing rs|\n|---|---|---|---|---|---|\n|Mean (cDNA)|0.00|0.50|0.00|2.42|0.00|\n|Mean (ProTherm)|0.00|0.50|0.00|2.26|0.00|\n|MSA|0.07|0.51|-0.05|N/A|0.02|\n|ESM2 [62]|0.05|0.51|0.04|N/A|-0.01|\n|FoldX [67]|0.41|-|-|2.95|-|\n|DDGun [44]|0.25|0.63|0.16|2.21|0.17|\n|DDGun3D [44]|0.26|0.64|0.18|2.24|0.17|\n|PROSTATA [74] (Additive)|0.21|0.60|0.05|2.25|0.00|\n|Mutate Everything (Additive)|0.50(0.02)|0.76(0.01)|0.37(0.02)|2.02(0.03)|0.20(0.01)|\n|Mutate Everything (Ours)|0.53(0.01)|0.78(0.01)|0.43(0.01)|2.04(0.01)|0.19(0.01)|", "isPerfectTable": true, "csv": "\"Method\",\"rs\",\"AUC\",\"MCC\",\"RMSE \u2193\",\"Stabilizing rs\"\n\"Mean (cDNA)\",\"0.00\",\"0.50\",\"0.00\",\"2.42\",\"0.00\"\n\"Mean (ProTherm)\",\"0.00\",\"0.50\",\"0.00\",\"2.26\",\"0.00\"\n\"MSA\",\"0.07\",\"0.51\",\"-0.05\",\"N/A\",\"0.02\"\n\"ESM2 [62]\",\"0.05\",\"0.51\",\"0.04\",\"N/A\",\"-0.01\"\n\"FoldX [67]\",\"0.41\",\"-\",\"-\",\"2.95\",\"-\"\n\"DDGun [44]\",\"0.25\",\"0.63\",\"0.16\",\"2.21\",\"0.17\"\n\"DDGun3D [44]\",\"0.26\",\"0.64\",\"0.18\",\"2.24\",\"0.17\"\n\"PROSTATA [74] (Additive)\",\"0.21\",\"0.60\",\"0.05\",\"2.25\",\"0.00\"\n\"Mutate Everything (Additive)\",\"0.50(0.02)\",\"0.76(0.01)\",\"0.37(0.02)\",\"2.02(0.03)\",\"0.20(0.01)\"\n\"Mutate Everything (Ours)\",\"0.53(0.01)\",\"0.78(0.01)\",\"0.43(0.01)\",\"2.04(0.01)\",\"0.19(0.01)\""}, {"type": "text", "value": "Table 2: Multiple Mutation Results on PTMul. Our additive baseline naively adds predicted single mutation \u2206\u2206Gs. Our model presents a strong mutation assessor. The proteins in PTMul have at most 35% sequence similarity to those in the training set. Parenthesis is standard error across 7 runs.\n\nTo the best of our knowledge, Mutate Everything is the first work that models all double mutations in a computationally tractable manner. Figure 4 shows the runtime of several methods on a protein of 317 amino acids on an A100 GPU. The dashed line indicates the transition from evaluating single mutants to double mutants. Mutate Everything predicts \u2206\u2206G for all single and double mutations in one pass of the model. It runs in 0.6 seconds using an ESM2 backbone, and 12.1 seconds on an AlphaFold backbone. PROSTATA [74] also uses the ESM2 backbone but takes a protein sequence and its mutated sequence as input and outputs the change in stability. PROSTATA takes 306 hours to evaluate all double mutations with a batch size of 768. On an 8 GPU node, this will take 1.5 days.\n\n5.3 Higher-order Mutation Results\n\nWe evaluate our model for predicting changes in thermodynamic stability for higher-order mutations in ProTherm. ProTherm is a database of thermodynamic (\u2206\u2206G) and thermostability (\u2206Tm) experimental characterization of protein mutations curated from the literature. We consider a subset of this database that contains \u2206\u2206G values for higher-order mutations, named ProTherm Multiple (PTMul). PTMul contains 858 mutations [44]. We keep 846 experimentally validated \u2206\u2206G for higher-order mutants, including 536 doubles and 183 triples, after removing 4 duplicate and 8 ambiguous mutations. PTMul proteins have at most 35% homology to the proteins used in training. Table 2 shows our results on PTMul. A description of baselines is provided in Section A.3. Our additive baselines naively add the sum of single mutation \u2206\u2206Gs. This baseline rigorously evaluates our model\u2019s ability to learn epistasis, or the non-additive interactions when combining two mutations. Mean always predicts the global test dataset mean statistic. ESM and MSA perform poorly when comparing mutations across proteins. On ProTherm, Mutate Everything achieves the state-of-the-art performance of 0.53 Spearman correlation rs, compared to 0.50 of our additive baseline. A breakdown of PTMul performance on double mutations and more (> 2) mutations is found in Table 6.\n\nWhile other methods also handle multiple mutations, they usually adopt a mutation- or protein-level train and test split. This results in data leakage since the same protein or a homolog will have mutations in both the training and testing sets, leading to inflated reported metrics [16, 18, 35, 64]. In our study, we use sequence similarity-based splitting where our training proteins have at most 35% homology to those in PTMul. To fairly evaluate generalization to new proteins, we exclude these inflated comparisons from our study.", "md": "Table 2: Multiple Mutation Results on PTMul. Our additive baseline naively adds predicted single mutation \u2206\u2206Gs. Our model presents a strong mutation assessor. The proteins in PTMul have at most 35% sequence similarity to those in the training set. Parenthesis is standard error across 7 runs.\n\nTo the best of our knowledge, Mutate Everything is the first work that models all double mutations in a computationally tractable manner. Figure 4 shows the runtime of several methods on a protein of 317 amino acids on an A100 GPU. The dashed line indicates the transition from evaluating single mutants to double mutants. Mutate Everything predicts \u2206\u2206G for all single and double mutations in one pass of the model. It runs in 0.6 seconds using an ESM2 backbone, and 12.1 seconds on an AlphaFold backbone. PROSTATA [74] also uses the ESM2 backbone but takes a protein sequence and its mutated sequence as input and outputs the change in stability. PROSTATA takes 306 hours to evaluate all double mutations with a batch size of 768. On an 8 GPU node, this will take 1.5 days.\n\n5.3 Higher-order Mutation Results\n\nWe evaluate our model for predicting changes in thermodynamic stability for higher-order mutations in ProTherm. ProTherm is a database of thermodynamic (\u2206\u2206G) and thermostability (\u2206Tm) experimental characterization of protein mutations curated from the literature. We consider a subset of this database that contains \u2206\u2206G values for higher-order mutations, named ProTherm Multiple (PTMul). PTMul contains 858 mutations [44]. We keep 846 experimentally validated \u2206\u2206G for higher-order mutants, including 536 doubles and 183 triples, after removing 4 duplicate and 8 ambiguous mutations. PTMul proteins have at most 35% homology to the proteins used in training. Table 2 shows our results on PTMul. A description of baselines is provided in Section A.3. Our additive baselines naively add the sum of single mutation \u2206\u2206Gs. This baseline rigorously evaluates our model\u2019s ability to learn epistasis, or the non-additive interactions when combining two mutations. Mean always predicts the global test dataset mean statistic. ESM and MSA perform poorly when comparing mutations across proteins. On ProTherm, Mutate Everything achieves the state-of-the-art performance of 0.53 Spearman correlation rs, compared to 0.50 of our additive baseline. A breakdown of PTMul performance on double mutations and more (> 2) mutations is found in Table 6.\n\nWhile other methods also handle multiple mutations, they usually adopt a mutation- or protein-level train and test split. This results in data leakage since the same protein or a homolog will have mutations in both the training and testing sets, leading to inflated reported metrics [16, 18, 35, 64]. In our study, we use sequence similarity-based splitting where our training proteins have at most 35% homology to those in PTMul. To fairly evaluate generalization to new proteins, we exclude these inflated comparisons from our study."}]}, {"page": 8, "text": "                                                 Forward                                Reverse\n  Method                             rs    AUC      MCC       RMSE \u2193        rs    AUC      MCC       RMSE \u2193\n  Structure-based Methods\n  mCSM [57]                        0.37     0.66     0.13       1.53      0.24     0.61     0.10       2.33\n  I-Mutant3.0 [11]                 0.35     0.64     0.08       1.53      0.17     0.59     0.06       2.35\n  DUET [56]                        0.42     0.68     0.19       1.52      0.26     0.63     0.12       2.16\n  FoldX [67]                       0.27     0.62     0.14       2.35      0.32     0.67     0.20       2.54\n  MAESTRO [35]                     0.46     0.69     0.26       1.45      0.22     0.61     0.11       2.12\n  PopMusic [17]                    0.42     0.69     0.22       1.51      0.24     0.62     0.12       2.10\n  SDM [77]                         0.39     0.67     0.21       1.67      0.14     0.59     0.12       2.15\n  INPS3D [66]                      0.44     0.70     0.20       1.49      0.36     0.69     0.23       1.78\n  Dynamut [63]                     0.38     0.68     0.20       1.59      0.37     0.67     0.17       1.69\n  ThermoNet [36]                   0.38     0.69     0.21       1.62      0.35     0.66     0.18       1.66\n  PremPS [14]                      0.42     0.66     0.20       1.50      0.43     0.66     0.22       1.49\n  DDGun3D [44]                     0.43     0.71     0.27       1.60      0.41     0.71     0.23       1.61\n  ACDC-NN [7]                      0.46     0.73     0.23       1.49      0.45     0.72     0.22       1.50\n  Stability Oracle [18]            0.53     0.75     0.34       1.44      0.53     0.75     0.32       1.43\n  Sequence-based Methods\n  MuPro [15]                       0.27     0.61     0.03       1.60      0.22     0.64     0.08       2.41\n  I-Mutant3.0-Seq [11]             0.34     0.67     0.25       1.54      0.23     0.61     0.05       2.25\n  DDGun [44]                       0.43     0.72     0.24       1.73      0.41     0.71     0.23       1.76\n  ACDC-NN-Seq [55]                 0.44     0.72     0.26       1.52      0.44     0.72     0.23       1.52\n  INPS-Seq [66]                    0.44     0.72     0.25       1.52      0.44     0.72     0.21       1.53\n  PROSTATA [74]                    0.50     0.73     0.28       1.44      0.50     0.73     0.29       1.44\n  Mutate Everything (Ours)         0.56     0.76     0.35       1.38      0.49     0.73     0.30       1.48\nTable 3: Comparisons on Single Mutation Thermodynamic Stability prediction in S669. We\nreport regression and classification metrics on the forward and reverse datasets for structure-based\napproaches (top block) and sequence-based approaches (bottom block). r refers to Spearman corre-\nlation coeffi cient. Our training proteins have at most 30% sequence similarity with those in S669.\nMutate Everything is state-of-the-art on S669 forward.\n5.4   Single Mutation Results\nWe additionally validate our model\u2019s ability to predict changes in thermodynamic stability under\nsingle mutations. We compare Mutate Everything to prior works on the newly introduced S669\ndataset, which was established to address the data leakage issue and enable fair comparisons between\nexisting methods [54]. We also evaluate the commonly studied reverse dataset setting, in which the\nmodel takes the mutant sequence as input and predicts \u2206\u2206G for mutating back to the wild-type\nsequence. The experimental value is obtained by negating the original \u2206\u2206G value going from\nthe wild-type sequence to the mutant sequence. S669 is well curated and extensive, containing 94\nproteins totaling 669 mutations [54]. S669 proteins have at most 30% sequence similarity with those\nin our training set to ensure separation between the training and validation sets.\nTable 3 shows our comparisons with prior works. Mutate Everything is state-of-the-art on S669,\nachieving a Spearman correlation of 0.56, where the prior art obtained 0.53. Mutate Everything\noutperforms the existing sequence model by a large margin, with a 6-point gain on Spearman\ncorrelation and a 3-point improvement on AUC. Our method is even as powerful as the latest\nstructure-based methods which have access to atom coordinates.\nMutate Everything is competitive on the reverse dataset evaluation, in which the original and mutated\nsequences are flipped and \u2206\u2206G negated. Our performance drops on reverse mutations because\nreverse mutations are out of distribution for our model. We did not perform data augmentation to\ntrain on reversed mutations as commonly done in the literature [18, 36, 74]. We found that it was\nbeneficial to bias our reverse predictions using \u2206\u2206G to the amino acid found in the original sequence.\nWe found the performance to be similar across 5 runs (< 0.01 standard error).\n                                                       8", "md": "# Thermodynamic Stability Prediction Comparison\n\n## Table 3: Comparisons on Single Mutation Thermodynamic Stability prediction in S669\n\nWe report regression and classification metrics on the forward and reverse datasets for structure-based approaches (top block) and sequence-based approaches (bottom block). r refers to Spearman correlation coefficient. Our training proteins have at most 30% sequence similarity with those in S669. Mutate Everything is state-of-the-art on S669 forward.\n\n|Method|rs|AUC|MCC|RMSE \u2193|rs|AUC|MCC|RMSE \u2193|\n|---|---|---|---|---|---|---|---|---|\n|Structure-based Methods| | | | | | | | |\n|mCSM [57]|0.37|0.66|0.13|1.53|0.24|0.61|0.10|2.33|\n|I-Mutant3.0 [11]|0.35|0.64|0.08|1.53|0.17|0.59|0.06|2.35|\n|DUET [56]|0.42|0.68|0.19|1.52|0.26|0.63|0.12|2.16|\n|FoldX [67]|0.27|0.62|0.14|2.35|0.32|0.67|0.20|2.54|\n|MAESTRO [35]|0.46|0.69|0.26|1.45|0.22|0.61|0.11|2.12|\n|PopMusic [17]|0.42|0.69|0.22|1.51|0.24|0.62|0.12|2.10|\n|SDM [77]|0.39|0.67|0.21|1.67|0.14|0.59|0.12|2.15|\n|INPS3D [66]|0.44|0.70|0.20|1.49|0.36|0.69|0.23|1.78|\n|Dynamut [63]|0.38|0.68|0.20|1.59|0.37|0.67|0.17|1.69|\n|ThermoNet [36]|0.38|0.69|0.21|1.62|0.35|0.66|0.18|1.66|\n|PremPS [14]|0.42|0.66|0.20|1.50|0.43|0.66|0.22|1.49|\n|DDGun3D [44]|0.43|0.71|0.27|1.60|0.41|0.71|0.23|1.61|\n|ACDC-NN [7]|0.46|0.73|0.23|1.49|0.45|0.72|0.22|1.50|\n|Stability Oracle [18]|0.53|0.75|0.34|1.44|0.53|0.75|0.32|1.43|\n|Sequence-based Methods| | | | | | | | |\n|MuPro [15]|0.27|0.61|0.03|1.60|0.22|0.64|0.08|2.41|\n|I-Mutant3.0-Seq [11]|0.34|0.67|0.25|1.54|0.23|0.61|0.05|2.25|\n|DDGun [44]|0.43|0.72|0.24|1.73|0.41|0.71|0.23|1.76|\n|ACDC-NN-Seq [55]|0.44|0.72|0.26|1.52|0.44|0.72|0.23|1.52|\n|INPS-Seq [66]|0.44|0.72|0.25|1.52|0.44|0.72|0.21|1.53|\n|PROSTATA [74]|0.50|0.73|0.28|1.44|0.50|0.73|0.29|1.44|\n|Mutate Everything (Ours)|0.56|0.76|0.35|1.38|0.49|0.73|0.30|1.48|\n\n### 5.4 Single Mutation Results\n\nWe additionally validate our model\u2019s ability to predict changes in thermodynamic stability under single mutations. We compare Mutate Everything to prior works on the newly introduced S669 dataset, which was established to address the data leakage issue and enable fair comparisons between existing methods [54]. We also evaluate the commonly studied reverse dataset setting, in which the model takes the mutant sequence as input and predicts $$\\Delta\\Delta G$$ for mutating back to the wild-type sequence. The experimental value is obtained by negating the original $$\\Delta\\Delta G$$ value going from the wild-type sequence to the mutant sequence. S669 is well curated and extensive, containing 94 proteins totaling 669 mutations [54]. S669 proteins have at most 30% sequence similarity with those in our training set to ensure separation between the training and validation sets.\n\nTable 3 shows our comparisons with prior works. Mutate Everything is state-of-the-art on S669, achieving a Spearman correlation of 0.56, where the prior art obtained 0.53. Mutate Everything outperforms the existing sequence model by a large margin, with a 6-point gain on Spearman correlation and a 3-point improvement on AUC. Our method is even as powerful as the latest structure-based methods which have access to atom coordinates.\n\nMutate Everything is competitive on the reverse dataset evaluation, in which the original and mutated sequences are flipped and $$\\Delta\\Delta G$$ negated. Our performance drops on reverse mutations because reverse mutations are out of distribution for our model. We did not perform data augmentation to train on reversed mutations as commonly done in the literature [18, 36, 74]. We found that it was beneficial to bias our reverse predictions using $$\\Delta\\Delta G$$ to the amino acid found in the original sequence. We found the performance to be similar across 5 runs (&lt; 0.01 standard error).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Thermodynamic Stability Prediction Comparison", "md": "# Thermodynamic Stability Prediction Comparison"}, {"type": "heading", "lvl": 2, "value": "Table 3: Comparisons on Single Mutation Thermodynamic Stability prediction in S669", "md": "## Table 3: Comparisons on Single Mutation Thermodynamic Stability prediction in S669"}, {"type": "text", "value": "We report regression and classification metrics on the forward and reverse datasets for structure-based approaches (top block) and sequence-based approaches (bottom block). r refers to Spearman correlation coefficient. Our training proteins have at most 30% sequence similarity with those in S669. Mutate Everything is state-of-the-art on S669 forward.", "md": "We report regression and classification metrics on the forward and reverse datasets for structure-based approaches (top block) and sequence-based approaches (bottom block). r refers to Spearman correlation coefficient. Our training proteins have at most 30% sequence similarity with those in S669. Mutate Everything is state-of-the-art on S669 forward."}, {"type": "table", "rows": [["Method", "rs", "AUC", "MCC", "RMSE \u2193", "rs", "AUC", "MCC", "RMSE \u2193"], ["Structure-based Methods", "", "", "", "", "", "", "", ""], ["mCSM [57]", "0.37", "0.66", "0.13", "1.53", "0.24", "0.61", "0.10", "2.33"], ["I-Mutant3.0 [11]", "0.35", "0.64", "0.08", "1.53", "0.17", "0.59", "0.06", "2.35"], ["DUET [56]", "0.42", "0.68", "0.19", "1.52", "0.26", "0.63", "0.12", "2.16"], ["FoldX [67]", "0.27", "0.62", "0.14", "2.35", "0.32", "0.67", "0.20", "2.54"], ["MAESTRO [35]", "0.46", "0.69", "0.26", "1.45", "0.22", "0.61", "0.11", "2.12"], ["PopMusic [17]", "0.42", "0.69", "0.22", "1.51", "0.24", "0.62", "0.12", "2.10"], ["SDM [77]", "0.39", "0.67", "0.21", "1.67", "0.14", "0.59", "0.12", "2.15"], ["INPS3D [66]", "0.44", "0.70", "0.20", "1.49", "0.36", "0.69", "0.23", "1.78"], ["Dynamut [63]", "0.38", "0.68", "0.20", "1.59", "0.37", "0.67", "0.17", "1.69"], ["ThermoNet [36]", "0.38", "0.69", "0.21", "1.62", "0.35", "0.66", "0.18", "1.66"], ["PremPS [14]", "0.42", "0.66", "0.20", "1.50", "0.43", "0.66", "0.22", "1.49"], ["DDGun3D [44]", "0.43", "0.71", "0.27", "1.60", "0.41", "0.71", "0.23", "1.61"], ["ACDC-NN [7]", "0.46", "0.73", "0.23", "1.49", "0.45", "0.72", "0.22", "1.50"], ["Stability Oracle [18]", "0.53", "0.75", "0.34", "1.44", "0.53", "0.75", "0.32", "1.43"], ["Sequence-based Methods", "", "", "", "", "", "", "", ""], ["MuPro [15]", "0.27", "0.61", "0.03", "1.60", "0.22", "0.64", "0.08", "2.41"], ["I-Mutant3.0-Seq [11]", "0.34", "0.67", "0.25", "1.54", "0.23", "0.61", "0.05", "2.25"], ["DDGun [44]", "0.43", "0.72", "0.24", "1.73", "0.41", "0.71", "0.23", "1.76"], ["ACDC-NN-Seq [55]", "0.44", "0.72", "0.26", "1.52", "0.44", "0.72", "0.23", "1.52"], ["INPS-Seq [66]", "0.44", "0.72", "0.25", "1.52", "0.44", "0.72", "0.21", "1.53"], ["PROSTATA [74]", "0.50", "0.73", "0.28", "1.44", "0.50", "0.73", "0.29", "1.44"], ["Mutate Everything (Ours)", "0.56", "0.76", "0.35", "1.38", "0.49", "0.73", "0.30", "1.48"]], "md": "|Method|rs|AUC|MCC|RMSE \u2193|rs|AUC|MCC|RMSE \u2193|\n|---|---|---|---|---|---|---|---|---|\n|Structure-based Methods| | | | | | | | |\n|mCSM [57]|0.37|0.66|0.13|1.53|0.24|0.61|0.10|2.33|\n|I-Mutant3.0 [11]|0.35|0.64|0.08|1.53|0.17|0.59|0.06|2.35|\n|DUET [56]|0.42|0.68|0.19|1.52|0.26|0.63|0.12|2.16|\n|FoldX [67]|0.27|0.62|0.14|2.35|0.32|0.67|0.20|2.54|\n|MAESTRO [35]|0.46|0.69|0.26|1.45|0.22|0.61|0.11|2.12|\n|PopMusic [17]|0.42|0.69|0.22|1.51|0.24|0.62|0.12|2.10|\n|SDM [77]|0.39|0.67|0.21|1.67|0.14|0.59|0.12|2.15|\n|INPS3D [66]|0.44|0.70|0.20|1.49|0.36|0.69|0.23|1.78|\n|Dynamut [63]|0.38|0.68|0.20|1.59|0.37|0.67|0.17|1.69|\n|ThermoNet [36]|0.38|0.69|0.21|1.62|0.35|0.66|0.18|1.66|\n|PremPS [14]|0.42|0.66|0.20|1.50|0.43|0.66|0.22|1.49|\n|DDGun3D [44]|0.43|0.71|0.27|1.60|0.41|0.71|0.23|1.61|\n|ACDC-NN [7]|0.46|0.73|0.23|1.49|0.45|0.72|0.22|1.50|\n|Stability Oracle [18]|0.53|0.75|0.34|1.44|0.53|0.75|0.32|1.43|\n|Sequence-based Methods| | | | | | | | |\n|MuPro [15]|0.27|0.61|0.03|1.60|0.22|0.64|0.08|2.41|\n|I-Mutant3.0-Seq [11]|0.34|0.67|0.25|1.54|0.23|0.61|0.05|2.25|\n|DDGun [44]|0.43|0.72|0.24|1.73|0.41|0.71|0.23|1.76|\n|ACDC-NN-Seq [55]|0.44|0.72|0.26|1.52|0.44|0.72|0.23|1.52|\n|INPS-Seq [66]|0.44|0.72|0.25|1.52|0.44|0.72|0.21|1.53|\n|PROSTATA [74]|0.50|0.73|0.28|1.44|0.50|0.73|0.29|1.44|\n|Mutate Everything (Ours)|0.56|0.76|0.35|1.38|0.49|0.73|0.30|1.48|", "isPerfectTable": true, "csv": "\"Method\",\"rs\",\"AUC\",\"MCC\",\"RMSE \u2193\",\"rs\",\"AUC\",\"MCC\",\"RMSE \u2193\"\n\"Structure-based Methods\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"mCSM [57]\",\"0.37\",\"0.66\",\"0.13\",\"1.53\",\"0.24\",\"0.61\",\"0.10\",\"2.33\"\n\"I-Mutant3.0 [11]\",\"0.35\",\"0.64\",\"0.08\",\"1.53\",\"0.17\",\"0.59\",\"0.06\",\"2.35\"\n\"DUET [56]\",\"0.42\",\"0.68\",\"0.19\",\"1.52\",\"0.26\",\"0.63\",\"0.12\",\"2.16\"\n\"FoldX [67]\",\"0.27\",\"0.62\",\"0.14\",\"2.35\",\"0.32\",\"0.67\",\"0.20\",\"2.54\"\n\"MAESTRO [35]\",\"0.46\",\"0.69\",\"0.26\",\"1.45\",\"0.22\",\"0.61\",\"0.11\",\"2.12\"\n\"PopMusic [17]\",\"0.42\",\"0.69\",\"0.22\",\"1.51\",\"0.24\",\"0.62\",\"0.12\",\"2.10\"\n\"SDM [77]\",\"0.39\",\"0.67\",\"0.21\",\"1.67\",\"0.14\",\"0.59\",\"0.12\",\"2.15\"\n\"INPS3D [66]\",\"0.44\",\"0.70\",\"0.20\",\"1.49\",\"0.36\",\"0.69\",\"0.23\",\"1.78\"\n\"Dynamut [63]\",\"0.38\",\"0.68\",\"0.20\",\"1.59\",\"0.37\",\"0.67\",\"0.17\",\"1.69\"\n\"ThermoNet [36]\",\"0.38\",\"0.69\",\"0.21\",\"1.62\",\"0.35\",\"0.66\",\"0.18\",\"1.66\"\n\"PremPS [14]\",\"0.42\",\"0.66\",\"0.20\",\"1.50\",\"0.43\",\"0.66\",\"0.22\",\"1.49\"\n\"DDGun3D [44]\",\"0.43\",\"0.71\",\"0.27\",\"1.60\",\"0.41\",\"0.71\",\"0.23\",\"1.61\"\n\"ACDC-NN [7]\",\"0.46\",\"0.73\",\"0.23\",\"1.49\",\"0.45\",\"0.72\",\"0.22\",\"1.50\"\n\"Stability Oracle [18]\",\"0.53\",\"0.75\",\"0.34\",\"1.44\",\"0.53\",\"0.75\",\"0.32\",\"1.43\"\n\"Sequence-based Methods\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"MuPro [15]\",\"0.27\",\"0.61\",\"0.03\",\"1.60\",\"0.22\",\"0.64\",\"0.08\",\"2.41\"\n\"I-Mutant3.0-Seq [11]\",\"0.34\",\"0.67\",\"0.25\",\"1.54\",\"0.23\",\"0.61\",\"0.05\",\"2.25\"\n\"DDGun [44]\",\"0.43\",\"0.72\",\"0.24\",\"1.73\",\"0.41\",\"0.71\",\"0.23\",\"1.76\"\n\"ACDC-NN-Seq [55]\",\"0.44\",\"0.72\",\"0.26\",\"1.52\",\"0.44\",\"0.72\",\"0.23\",\"1.52\"\n\"INPS-Seq [66]\",\"0.44\",\"0.72\",\"0.25\",\"1.52\",\"0.44\",\"0.72\",\"0.21\",\"1.53\"\n\"PROSTATA [74]\",\"0.50\",\"0.73\",\"0.28\",\"1.44\",\"0.50\",\"0.73\",\"0.29\",\"1.44\"\n\"Mutate Everything (Ours)\",\"0.56\",\"0.76\",\"0.35\",\"1.38\",\"0.49\",\"0.73\",\"0.30\",\"1.48\""}, {"type": "heading", "lvl": 3, "value": "5.4 Single Mutation Results", "md": "### 5.4 Single Mutation Results"}, {"type": "text", "value": "We additionally validate our model\u2019s ability to predict changes in thermodynamic stability under single mutations. We compare Mutate Everything to prior works on the newly introduced S669 dataset, which was established to address the data leakage issue and enable fair comparisons between existing methods [54]. We also evaluate the commonly studied reverse dataset setting, in which the model takes the mutant sequence as input and predicts $$\\Delta\\Delta G$$ for mutating back to the wild-type sequence. The experimental value is obtained by negating the original $$\\Delta\\Delta G$$ value going from the wild-type sequence to the mutant sequence. S669 is well curated and extensive, containing 94 proteins totaling 669 mutations [54]. S669 proteins have at most 30% sequence similarity with those in our training set to ensure separation between the training and validation sets.\n\nTable 3 shows our comparisons with prior works. Mutate Everything is state-of-the-art on S669, achieving a Spearman correlation of 0.56, where the prior art obtained 0.53. Mutate Everything outperforms the existing sequence model by a large margin, with a 6-point gain on Spearman correlation and a 3-point improvement on AUC. Our method is even as powerful as the latest structure-based methods which have access to atom coordinates.\n\nMutate Everything is competitive on the reverse dataset evaluation, in which the original and mutated sequences are flipped and $$\\Delta\\Delta G$$ negated. Our performance drops on reverse mutations because reverse mutations are out of distribution for our model. We did not perform data augmentation to train on reversed mutations as commonly done in the literature [18, 36, 74]. We found that it was beneficial to bias our reverse predictions using $$\\Delta\\Delta G$$ to the amino acid found in the original sequence. We found the performance to be similar across 5 runs (&lt; 0.01 standard error).", "md": "We additionally validate our model\u2019s ability to predict changes in thermodynamic stability under single mutations. We compare Mutate Everything to prior works on the newly introduced S669 dataset, which was established to address the data leakage issue and enable fair comparisons between existing methods [54]. We also evaluate the commonly studied reverse dataset setting, in which the model takes the mutant sequence as input and predicts $$\\Delta\\Delta G$$ for mutating back to the wild-type sequence. The experimental value is obtained by negating the original $$\\Delta\\Delta G$$ value going from the wild-type sequence to the mutant sequence. S669 is well curated and extensive, containing 94 proteins totaling 669 mutations [54]. S669 proteins have at most 30% sequence similarity with those in our training set to ensure separation between the training and validation sets.\n\nTable 3 shows our comparisons with prior works. Mutate Everything is state-of-the-art on S669, achieving a Spearman correlation of 0.56, where the prior art obtained 0.53. Mutate Everything outperforms the existing sequence model by a large margin, with a 6-point gain on Spearman correlation and a 3-point improvement on AUC. Our method is even as powerful as the latest structure-based methods which have access to atom coordinates.\n\nMutate Everything is competitive on the reverse dataset evaluation, in which the original and mutated sequences are flipped and $$\\Delta\\Delta G$$ negated. Our performance drops on reverse mutations because reverse mutations are out of distribution for our model. We did not perform data augmentation to train on reversed mutations as commonly done in the literature [18, 36, 74]. We found that it was beneficial to bias our reverse predictions using $$\\Delta\\Delta G$$ to the amino acid found in the original sequence. We found the performance to be similar across 5 runs (&lt; 0.01 standard error)."}]}, {"page": 9, "text": "                                                      ProteinGym-Stability             ProteinGym\n      Model                                             rs     AUC      MCC        rs     AUC      MCC\n      DeepSequence [61]                               0.44     0.74      0.34     0.43     0.74     0.35\n      EVE [24]                                        0.47     0.75      0.36     0.46     0.76     0.36\n      ESM1v [42]                                      0.41     0.72      0.31     0.41     0.74     0.33\n      Progen2 [47]                                    0.45     0.74      0.35     0.42     0.74     0.34\n      MSA Transformer [60]                            0.50     0.77      0.38     0.44     0.75     0.35\n      Tranception [49]                                0.40     0.71      0.31     0.40     0.73     0.32\n      Tranception+MSA                                 0.45     0.74      0.35     0.45     0.76     0.36\n      TranceptEVE [50]                                0.50     0.77      0.38     0.48     0.77     0.38\n      MSA                                             0.41     0.72      0.32     0.39     0.72     0.31\n      Mutate Everything                               0.52     0.78      0.39     0.38     0.72     0.30\n      Mutate Everything +MSA                          0.52     0.78      0.41     0.44     0.75     0.36\n      Mutate Everything +MSA+Tranception              0.53     0.79      0.42     0.49     0.78     0.39\nTable 4: Comparisons on mutation fitness prediction in ProteinGym. We report ranking and\nclassification metrics on both a subset of ProteinGym with stability-like phenotypes and the entire\nProteinGym dataset. Our stability model can generalize to all phenotypes, outperforming existing\nmethods on stability-like phenotypes and performing similarly to existing methods otherwise. The\nmaximum homology between our training set and ProteinGym proteins is 43%.\n5.5   Generalization to other Phenotypes\nWe show that stability predictors generalize to other phenotypes as well. ProteinGym substitutions\nis a dataset of 87 proteins with 1.6 million mutation sets labeled with a generic fitness score, such as\nactivity, growth, or expression. The mutation sets have up to 20 substitutions and 14 proteins contain\nhigher-order mutations. The ProteinGym-Stability split contains 7 proteins and 26,000 mutations with\nfitness labels that are known to correlate with thermodynamic stability (thermostability, abundance,\nexpression). ProteinGym proteins have at most 45% sequence similarity with those in our training\nset. Due to memory constraints, we truncate proteins to at most 2000 amino acids.\nTable 4 compares our work with previous works. Our MSA baseline uses the empirical amino acid\ndistribution in the MSA at a given position to compute a likelihood ratio. The MSA retrieval prior\nweights the scores using the per-position first-order statistics in a multiple sequence alignment. On\nProteinGym-Stability, Mutate Everything obtains a Spearman correlation of 0.52, compared to 0.50\nof the next best method, TranceptEVE. On the one protein with labeled stability, Mutate Everything\nobtains 0.51 vs 0.33 of Tranception with MSA retrieval [51]. On the full ProteinGym benchmark, we\noutperform the strongest methods by ensembling our predictions with Tranception [49]. Ensembling\nour method with evolutionary models by averaging scores gains 0.11 points on ProteinGym and 0.01\npoints on ProteinGym-Stability, suggesting that our model learns complementary information to the\nevolutionary models. For comparison, TransEVE ensembles an evolutionary model (Tranception)\nand a family-based model (EVE) for a 0.03 point gain [50]. We found the performance to be similar\nacross 5 runs (< 0.01 standard error). An in-depth study on how protein stability related to function\ncan be found in [26].\n5.6   Ablation Studies\nWe ablate several architectural design choices in Table 5. Core architectural designs are evaluated on\nS669. Architectural designs for only multiple mutations are evaluated on cDNA2.\nBackbone. We analyze the effect of different feature extractors on single mutation stability assess-\nment in Table 5a. ESM2 [38] takes only sequence as input whereas MSA-Transformer [60] and\nAlphaFold [31] take the sequence and its MSA as input. Both ESM2 and MSA-Transformer are\ntrained on masking residue modeling while AlphaFold is also trained on structure prediction. MSA\ninput improves the stability prediction performance and AlphaFold\u2019s architecture and training further\nimprove stability prediction.\n                                                       9", "md": "# Protein Stability Prediction\n\n## Table 4: Comparisons on mutation fitness prediction in ProteinGym\n\n|Model|rs|AUC|MCC|rs|AUC|MCC|\n|---|---|---|---|---|---|---|\n|DeepSequence [61]|0.44|0.74|0.34|0.43|0.74|0.35|\n|EVE [24]|0.47|0.75|0.36|0.46|0.76|0.36|\n|ESM1v [42]|0.41|0.72|0.31|0.41|0.74|0.33|\n|Progen2 [47]|0.45|0.74|0.35|0.42|0.74|0.34|\n|MSA Transformer [60]|0.50|0.77|0.38|0.44|0.75|0.35|\n|Tranception [49]|0.40|0.71|0.31|0.40|0.73|0.32|\n|Tranception+MSA|0.45|0.74|0.35|0.45|0.76|0.36|\n|TranceptEVE [50]|0.50|0.77|0.38|0.48|0.77|0.38|\n|MSA|0.41|0.72|0.32|0.39|0.72|0.31|\n|Mutate Everything|0.52|0.78|0.39|0.38|0.72|0.30|\n|Mutate Everything +MSA|0.52|0.78|0.41|0.44|0.75|0.36|\n|Mutate Everything +MSA+Tranception|0.53|0.79|0.42|0.49|0.78|0.39|\n\nOur stability model can generalize to all phenotypes, outperforming existing methods on stability-like phenotypes and performing similarly to existing methods otherwise. The maximum homology between our training set and ProteinGym proteins is 43%.\n\n### 5.5 Generalization to other Phenotypes\n\nWe show that stability predictors generalize to other phenotypes as well. ProteinGym substitutions is a dataset of 87 proteins with 1.6 million mutation sets labeled with a generic fitness score, such as activity, growth, or expression. The mutation sets have up to 20 substitutions and 14 proteins contain higher-order mutations. The ProteinGym-Stability split contains 7 proteins and 26,000 mutations with fitness labels that are known to correlate with thermodynamic stability (thermostability, abundance, expression). ProteinGym proteins have at most 45% sequence similarity with those in our training set. Due to memory constraints, we truncate proteins to at most 2000 amino acids.\n\n### Table 4: Comparison with Previous Works\n\nOur MSA baseline uses the empirical amino acid distribution in the MSA at a given position to compute a likelihood ratio. The MSA retrieval prior weights the scores using the per-position first-order statistics in a multiple sequence alignment. On ProteinGym-Stability, Mutate Everything obtains a Spearman correlation of 0.52, compared to 0.50 of the next best method, TranceptEVE. On the one protein with labeled stability, Mutate Everything obtains 0.51 vs 0.33 of Tranception with MSA retrieval [51]. On the full ProteinGym benchmark, we outperform the strongest methods by ensembling our predictions with Tranception [49]. Ensembling our method with evolutionary models by averaging scores gains 0.11 points on ProteinGym and 0.01 points on ProteinGym-Stability, suggesting that our model learns complementary information to the evolutionary models. For comparison, TransEVE ensembles an evolutionary model (Tranception) and a family-based model (EVE) for a 0.03 point gain [50]. We found the performance to be similar across 5 runs (&lt; 0.01 standard error). An in-depth study on how protein stability related to function can be found in [26].\n\n### 5.6 Ablation Studies\n\nWe ablate several architectural design choices in Table 5. Core architectural designs are evaluated on S669. Architectural designs for only multiple mutations are evaluated on cDNA2.\n\nBackbone. We analyze the effect of different feature extractors on single mutation stability assessment in Table 5a. ESM2 [38] takes only sequence as input whereas MSA-Transformer [60] and AlphaFold [31] take the sequence and its MSA as input. Both ESM2 and MSA-Transformer are trained on masking residue modeling while AlphaFold is also trained on structure prediction. MSA input improves the stability prediction performance and AlphaFold\u2019s architecture and training further improve stability prediction.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Protein Stability Prediction", "md": "# Protein Stability Prediction"}, {"type": "heading", "lvl": 2, "value": "Table 4: Comparisons on mutation fitness prediction in ProteinGym", "md": "## Table 4: Comparisons on mutation fitness prediction in ProteinGym"}, {"type": "table", "rows": [["Model", "rs", "AUC", "MCC", "rs", "AUC", "MCC"], ["DeepSequence [61]", "0.44", "0.74", "0.34", "0.43", "0.74", "0.35"], ["EVE [24]", "0.47", "0.75", "0.36", "0.46", "0.76", "0.36"], ["ESM1v [42]", "0.41", "0.72", "0.31", "0.41", "0.74", "0.33"], ["Progen2 [47]", "0.45", "0.74", "0.35", "0.42", "0.74", "0.34"], ["MSA Transformer [60]", "0.50", "0.77", "0.38", "0.44", "0.75", "0.35"], ["Tranception [49]", "0.40", "0.71", "0.31", "0.40", "0.73", "0.32"], ["Tranception+MSA", "0.45", "0.74", "0.35", "0.45", "0.76", "0.36"], ["TranceptEVE [50]", "0.50", "0.77", "0.38", "0.48", "0.77", "0.38"], ["MSA", "0.41", "0.72", "0.32", "0.39", "0.72", "0.31"], ["Mutate Everything", "0.52", "0.78", "0.39", "0.38", "0.72", "0.30"], ["Mutate Everything +MSA", "0.52", "0.78", "0.41", "0.44", "0.75", "0.36"], ["Mutate Everything +MSA+Tranception", "0.53", "0.79", "0.42", "0.49", "0.78", "0.39"]], "md": "|Model|rs|AUC|MCC|rs|AUC|MCC|\n|---|---|---|---|---|---|---|\n|DeepSequence [61]|0.44|0.74|0.34|0.43|0.74|0.35|\n|EVE [24]|0.47|0.75|0.36|0.46|0.76|0.36|\n|ESM1v [42]|0.41|0.72|0.31|0.41|0.74|0.33|\n|Progen2 [47]|0.45|0.74|0.35|0.42|0.74|0.34|\n|MSA Transformer [60]|0.50|0.77|0.38|0.44|0.75|0.35|\n|Tranception [49]|0.40|0.71|0.31|0.40|0.73|0.32|\n|Tranception+MSA|0.45|0.74|0.35|0.45|0.76|0.36|\n|TranceptEVE [50]|0.50|0.77|0.38|0.48|0.77|0.38|\n|MSA|0.41|0.72|0.32|0.39|0.72|0.31|\n|Mutate Everything|0.52|0.78|0.39|0.38|0.72|0.30|\n|Mutate Everything +MSA|0.52|0.78|0.41|0.44|0.75|0.36|\n|Mutate Everything +MSA+Tranception|0.53|0.79|0.42|0.49|0.78|0.39|", "isPerfectTable": true, "csv": "\"Model\",\"rs\",\"AUC\",\"MCC\",\"rs\",\"AUC\",\"MCC\"\n\"DeepSequence [61]\",\"0.44\",\"0.74\",\"0.34\",\"0.43\",\"0.74\",\"0.35\"\n\"EVE [24]\",\"0.47\",\"0.75\",\"0.36\",\"0.46\",\"0.76\",\"0.36\"\n\"ESM1v [42]\",\"0.41\",\"0.72\",\"0.31\",\"0.41\",\"0.74\",\"0.33\"\n\"Progen2 [47]\",\"0.45\",\"0.74\",\"0.35\",\"0.42\",\"0.74\",\"0.34\"\n\"MSA Transformer [60]\",\"0.50\",\"0.77\",\"0.38\",\"0.44\",\"0.75\",\"0.35\"\n\"Tranception [49]\",\"0.40\",\"0.71\",\"0.31\",\"0.40\",\"0.73\",\"0.32\"\n\"Tranception+MSA\",\"0.45\",\"0.74\",\"0.35\",\"0.45\",\"0.76\",\"0.36\"\n\"TranceptEVE [50]\",\"0.50\",\"0.77\",\"0.38\",\"0.48\",\"0.77\",\"0.38\"\n\"MSA\",\"0.41\",\"0.72\",\"0.32\",\"0.39\",\"0.72\",\"0.31\"\n\"Mutate Everything\",\"0.52\",\"0.78\",\"0.39\",\"0.38\",\"0.72\",\"0.30\"\n\"Mutate Everything +MSA\",\"0.52\",\"0.78\",\"0.41\",\"0.44\",\"0.75\",\"0.36\"\n\"Mutate Everything +MSA+Tranception\",\"0.53\",\"0.79\",\"0.42\",\"0.49\",\"0.78\",\"0.39\""}, {"type": "text", "value": "Our stability model can generalize to all phenotypes, outperforming existing methods on stability-like phenotypes and performing similarly to existing methods otherwise. The maximum homology between our training set and ProteinGym proteins is 43%.", "md": "Our stability model can generalize to all phenotypes, outperforming existing methods on stability-like phenotypes and performing similarly to existing methods otherwise. The maximum homology between our training set and ProteinGym proteins is 43%."}, {"type": "heading", "lvl": 3, "value": "5.5 Generalization to other Phenotypes", "md": "### 5.5 Generalization to other Phenotypes"}, {"type": "text", "value": "We show that stability predictors generalize to other phenotypes as well. ProteinGym substitutions is a dataset of 87 proteins with 1.6 million mutation sets labeled with a generic fitness score, such as activity, growth, or expression. The mutation sets have up to 20 substitutions and 14 proteins contain higher-order mutations. The ProteinGym-Stability split contains 7 proteins and 26,000 mutations with fitness labels that are known to correlate with thermodynamic stability (thermostability, abundance, expression). ProteinGym proteins have at most 45% sequence similarity with those in our training set. Due to memory constraints, we truncate proteins to at most 2000 amino acids.", "md": "We show that stability predictors generalize to other phenotypes as well. ProteinGym substitutions is a dataset of 87 proteins with 1.6 million mutation sets labeled with a generic fitness score, such as activity, growth, or expression. The mutation sets have up to 20 substitutions and 14 proteins contain higher-order mutations. The ProteinGym-Stability split contains 7 proteins and 26,000 mutations with fitness labels that are known to correlate with thermodynamic stability (thermostability, abundance, expression). ProteinGym proteins have at most 45% sequence similarity with those in our training set. Due to memory constraints, we truncate proteins to at most 2000 amino acids."}, {"type": "heading", "lvl": 3, "value": "Table 4: Comparison with Previous Works", "md": "### Table 4: Comparison with Previous Works"}, {"type": "text", "value": "Our MSA baseline uses the empirical amino acid distribution in the MSA at a given position to compute a likelihood ratio. The MSA retrieval prior weights the scores using the per-position first-order statistics in a multiple sequence alignment. On ProteinGym-Stability, Mutate Everything obtains a Spearman correlation of 0.52, compared to 0.50 of the next best method, TranceptEVE. On the one protein with labeled stability, Mutate Everything obtains 0.51 vs 0.33 of Tranception with MSA retrieval [51]. On the full ProteinGym benchmark, we outperform the strongest methods by ensembling our predictions with Tranception [49]. Ensembling our method with evolutionary models by averaging scores gains 0.11 points on ProteinGym and 0.01 points on ProteinGym-Stability, suggesting that our model learns complementary information to the evolutionary models. For comparison, TransEVE ensembles an evolutionary model (Tranception) and a family-based model (EVE) for a 0.03 point gain [50]. We found the performance to be similar across 5 runs (&lt; 0.01 standard error). An in-depth study on how protein stability related to function can be found in [26].", "md": "Our MSA baseline uses the empirical amino acid distribution in the MSA at a given position to compute a likelihood ratio. The MSA retrieval prior weights the scores using the per-position first-order statistics in a multiple sequence alignment. On ProteinGym-Stability, Mutate Everything obtains a Spearman correlation of 0.52, compared to 0.50 of the next best method, TranceptEVE. On the one protein with labeled stability, Mutate Everything obtains 0.51 vs 0.33 of Tranception with MSA retrieval [51]. On the full ProteinGym benchmark, we outperform the strongest methods by ensembling our predictions with Tranception [49]. Ensembling our method with evolutionary models by averaging scores gains 0.11 points on ProteinGym and 0.01 points on ProteinGym-Stability, suggesting that our model learns complementary information to the evolutionary models. For comparison, TransEVE ensembles an evolutionary model (Tranception) and a family-based model (EVE) for a 0.03 point gain [50]. We found the performance to be similar across 5 runs (&lt; 0.01 standard error). An in-depth study on how protein stability related to function can be found in [26]."}, {"type": "heading", "lvl": 3, "value": "5.6 Ablation Studies", "md": "### 5.6 Ablation Studies"}, {"type": "text", "value": "We ablate several architectural design choices in Table 5. Core architectural designs are evaluated on S669. Architectural designs for only multiple mutations are evaluated on cDNA2.\n\nBackbone. We analyze the effect of different feature extractors on single mutation stability assessment in Table 5a. ESM2 [38] takes only sequence as input whereas MSA-Transformer [60] and AlphaFold [31] take the sequence and its MSA as input. Both ESM2 and MSA-Transformer are trained on masking residue modeling while AlphaFold is also trained on structure prediction. MSA input improves the stability prediction performance and AlphaFold\u2019s architecture and training further improve stability prediction.", "md": "We ablate several architectural design choices in Table 5. Core architectural designs are evaluated on S669. Architectural designs for only multiple mutations are evaluated on cDNA2.\n\nBackbone. We analyze the effect of different feature extractors on single mutation stability assessment in Table 5a. ESM2 [38] takes only sequence as input whereas MSA-Transformer [60] and AlphaFold [31] take the sequence and its MSA as input. Both ESM2 and MSA-Transformer are trained on masking residue modeling while AlphaFold is also trained on structure prediction. MSA input improves the stability prediction performance and AlphaFold\u2019s architecture and training further improve stability prediction."}]}, {"page": 10, "text": "      backbone                       rs       AUC               aggregator                 nDCG      DetPr     AUC\n      ESM2 [38]                      0.47     0.72              outer prod. + flat.        0.39      0.15      0.82\n      MSA-Transformer [60]           0.53     0.75              product                    0.42      0.14      0.85\n      AlphaFold (scratch)            0.36     0.65              sum                        0.43      0.16      0.84\n      AlphaFold [31]                 0.56     0.76              whiten + sum               0.25      0.12      0.80\n    (a) Backbone on S669. MSA inputs improve                 (b) Mutation Aggregation on cDNA2. Higher-\n    performance. A pre-trained AlphaFold model               order mutations aggregate single mutation features.\n    performs the strongest.                                  Adding these features is competitive.\n           backbone opt         rs       AUC                      higher model         nDCG      DetPr     AUC\n           freeze               0.48     0.72                     direct               0.40      0.13      0.85\n           finetune             0.56     0.76                     multiply             0.17      0.08      0.77\n                                                                  add                  0.43      0.16      0.84\n    (c) Backbone Optimization on S669. Fine-                 (d) Higher-order Modeling on cDNA2. We model\n    tuning the AlphaFold backbone improves per-              the \u2206\u2206G of the double mutation as a residual com-\n    formance.                                                bined with the \u2206\u2206G of its single mutation.\n      Table 5: Mutate Everything ablation experiments. Default Settings are marked in grey.\nBackbone Optimization We try fine-tuning and freezing the AlphaFold backbone during stability fine-\ntuning in Table 5c. We find that fine-tuning the backbone improves stability prediction performance.\nAggregation The technique to aggregate mutation representations for higher-order mutations is\nablated in Table 5b. To control for feature dimension size, we reduce the head dimension when\naggregating with outer product and flattening. Aggregating with product and summation performs\nsimilarly on double mutations. We add the features for a natural extension to more mutations.\nHigher Order Modeling In the direct prediction model, the multi-mutant head directly predicts\n\u2206\u2206G. In the multiply and add models, the multi-mutant head learns how interactions among single\nmutations affect the higher-order mutation\u2019s \u2206\u2206G. In these models, the multi-mutant head output\nlearns an additive bias or multiplicative scaling to the sum of single mutation \u2206\u2206Gs. In Table 5d,\nwe show that learning a bias performs the strongest among these three options.\n6    Limitations\nFirst, our training dataset contains biases that may affect model performance. The training set contains\nonly small proteins, which may limit performance on larger ones. Our model may exhibit biases\ntowards certain types of mutations due to the data imbalance in our training set. Second, the limited\navailability of experimental stability data poses a challenge for in-silico evaluation. Evaluation on\nlarger and more diverse datasets is necessary to fully assess the generalizability of our model. In the\nfuture, we hope that high-throughput experimental assays will enable more rigorous evaluation and\nfurther improvements in protein stability prediction.\n7    Conclusion\nWe present a method that efficiently scales thermodynamic stability prediction from single mutations\nto higher-order mutations. Our key insight is that the effects of mutations on the same protein are\ncorrelated. Thus, for a target protein, it suffices to run a deep backbone once and decode the effect of\nall mutations simultaneously using a shallow decoder. With the AlphaFold model as our backbone,\nour method outperforms existing methods on a variety of single and multiple mutation benchmarks.\nOur method scales to millions of mutations with minimal computational overhead and runs in a\nfraction of the time it would take prior works.\n8    Acknowledgements\nThis work is supported by the NSF AI Institute for Foundations of Machine Learning (IFML).\n                                                            10", "md": "# Document\n\n## Table 5: Mutate Everything ablation experiments. Default Settings are marked in grey.\n\n|backbone|rs|AUC|aggregator|nDCG|DetPr|AUC|\n|---|---|---|---|---|---|---|\n|ESM2 [38]|0.47|0.72|outer prod. + flat.|0.39|0.15|0.82|\n|MSA-Transformer [60]|0.53|0.75|product|0.42|0.14|0.85|\n|AlphaFold (scratch)|0.36|0.65|sum|0.43|0.16|0.84|\n|AlphaFold [31]|0.56|0.76|whiten + sum|0.25|0.12|0.80|\n\n## Backbone Optimization\n\nWe try fine-tuning and freezing the AlphaFold backbone during stability fine-tuning in Table 5c. We find that fine-tuning the backbone improves stability prediction performance.\n\n## Aggregation\n\nThe technique to aggregate mutation representations for higher-order mutations is ablated in Table 5b. To control for feature dimension size, we reduce the head dimension when aggregating with outer product and flattening. Aggregating with product and summation performs similarly on double mutations. We add the features for a natural extension to more mutations.\n\n## Higher Order Modeling\n\nIn the direct prediction model, the multi-mutant head directly predicts $$\\Delta\\Delta G$$. In the multiply and add models, the multi-mutant head learns how interactions among single mutations affect the higher-order mutation\u2019s $$\\Delta\\Delta G$$. In these models, the multi-mutant head output learns an additive bias or multiplicative scaling to the sum of single mutation $$\\Delta\\Delta G$$. In Table 5d, we show that learning a bias performs the strongest among these three options.\n\n## Limitations\n\nFirst, our training dataset contains biases that may affect model performance. The training set contains only small proteins, which may limit performance on larger ones. Our model may exhibit biases towards certain types of mutations due to the data imbalance in our training set. Second, the limited availability of experimental stability data poses a challenge for in-silico evaluation. Evaluation on larger and more diverse datasets is necessary to fully assess the generalizability of our model. In the future, we hope that high-throughput experimental assays will enable more rigorous evaluation and further improvements in protein stability prediction.\n\n## Conclusion\n\nWe present a method that efficiently scales thermodynamic stability prediction from single mutations to higher-order mutations. Our key insight is that the effects of mutations on the same protein are correlated. Thus, for a target protein, it suffices to run a deep backbone once and decode the effect of all mutations simultaneously using a shallow decoder. With the AlphaFold model as our backbone, our method outperforms existing methods on a variety of single and multiple mutation benchmarks. Our method scales to millions of mutations with minimal computational overhead and runs in a fraction of the time it would take prior works.\n\n## Acknowledgements\n\nThis work is supported by the NSF AI Institute for Foundations of Machine Learning (IFML).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Table 5: Mutate Everything ablation experiments. Default Settings are marked in grey.", "md": "## Table 5: Mutate Everything ablation experiments. Default Settings are marked in grey."}, {"type": "table", "rows": [["backbone", "rs", "AUC", "aggregator", "nDCG", "DetPr", "AUC"], ["ESM2 [38]", "0.47", "0.72", "outer prod. + flat.", "0.39", "0.15", "0.82"], ["MSA-Transformer [60]", "0.53", "0.75", "product", "0.42", "0.14", "0.85"], ["AlphaFold (scratch)", "0.36", "0.65", "sum", "0.43", "0.16", "0.84"], ["AlphaFold [31]", "0.56", "0.76", "whiten + sum", "0.25", "0.12", "0.80"]], "md": "|backbone|rs|AUC|aggregator|nDCG|DetPr|AUC|\n|---|---|---|---|---|---|---|\n|ESM2 [38]|0.47|0.72|outer prod. + flat.|0.39|0.15|0.82|\n|MSA-Transformer [60]|0.53|0.75|product|0.42|0.14|0.85|\n|AlphaFold (scratch)|0.36|0.65|sum|0.43|0.16|0.84|\n|AlphaFold [31]|0.56|0.76|whiten + sum|0.25|0.12|0.80|", "isPerfectTable": true, "csv": "\"backbone\",\"rs\",\"AUC\",\"aggregator\",\"nDCG\",\"DetPr\",\"AUC\"\n\"ESM2 [38]\",\"0.47\",\"0.72\",\"outer prod. + flat.\",\"0.39\",\"0.15\",\"0.82\"\n\"MSA-Transformer [60]\",\"0.53\",\"0.75\",\"product\",\"0.42\",\"0.14\",\"0.85\"\n\"AlphaFold (scratch)\",\"0.36\",\"0.65\",\"sum\",\"0.43\",\"0.16\",\"0.84\"\n\"AlphaFold [31]\",\"0.56\",\"0.76\",\"whiten + sum\",\"0.25\",\"0.12\",\"0.80\""}, {"type": "heading", "lvl": 2, "value": "Backbone Optimization", "md": "## Backbone Optimization"}, {"type": "text", "value": "We try fine-tuning and freezing the AlphaFold backbone during stability fine-tuning in Table 5c. We find that fine-tuning the backbone improves stability prediction performance.", "md": "We try fine-tuning and freezing the AlphaFold backbone during stability fine-tuning in Table 5c. We find that fine-tuning the backbone improves stability prediction performance."}, {"type": "heading", "lvl": 2, "value": "Aggregation", "md": "## Aggregation"}, {"type": "text", "value": "The technique to aggregate mutation representations for higher-order mutations is ablated in Table 5b. To control for feature dimension size, we reduce the head dimension when aggregating with outer product and flattening. Aggregating with product and summation performs similarly on double mutations. We add the features for a natural extension to more mutations.", "md": "The technique to aggregate mutation representations for higher-order mutations is ablated in Table 5b. To control for feature dimension size, we reduce the head dimension when aggregating with outer product and flattening. Aggregating with product and summation performs similarly on double mutations. We add the features for a natural extension to more mutations."}, {"type": "heading", "lvl": 2, "value": "Higher Order Modeling", "md": "## Higher Order Modeling"}, {"type": "text", "value": "In the direct prediction model, the multi-mutant head directly predicts $$\\Delta\\Delta G$$. In the multiply and add models, the multi-mutant head learns how interactions among single mutations affect the higher-order mutation\u2019s $$\\Delta\\Delta G$$. In these models, the multi-mutant head output learns an additive bias or multiplicative scaling to the sum of single mutation $$\\Delta\\Delta G$$. In Table 5d, we show that learning a bias performs the strongest among these three options.", "md": "In the direct prediction model, the multi-mutant head directly predicts $$\\Delta\\Delta G$$. In the multiply and add models, the multi-mutant head learns how interactions among single mutations affect the higher-order mutation\u2019s $$\\Delta\\Delta G$$. In these models, the multi-mutant head output learns an additive bias or multiplicative scaling to the sum of single mutation $$\\Delta\\Delta G$$. In Table 5d, we show that learning a bias performs the strongest among these three options."}, {"type": "heading", "lvl": 2, "value": "Limitations", "md": "## Limitations"}, {"type": "text", "value": "First, our training dataset contains biases that may affect model performance. The training set contains only small proteins, which may limit performance on larger ones. Our model may exhibit biases towards certain types of mutations due to the data imbalance in our training set. Second, the limited availability of experimental stability data poses a challenge for in-silico evaluation. Evaluation on larger and more diverse datasets is necessary to fully assess the generalizability of our model. In the future, we hope that high-throughput experimental assays will enable more rigorous evaluation and further improvements in protein stability prediction.", "md": "First, our training dataset contains biases that may affect model performance. The training set contains only small proteins, which may limit performance on larger ones. Our model may exhibit biases towards certain types of mutations due to the data imbalance in our training set. Second, the limited availability of experimental stability data poses a challenge for in-silico evaluation. Evaluation on larger and more diverse datasets is necessary to fully assess the generalizability of our model. In the future, we hope that high-throughput experimental assays will enable more rigorous evaluation and further improvements in protein stability prediction."}, {"type": "heading", "lvl": 2, "value": "Conclusion", "md": "## Conclusion"}, {"type": "text", "value": "We present a method that efficiently scales thermodynamic stability prediction from single mutations to higher-order mutations. Our key insight is that the effects of mutations on the same protein are correlated. Thus, for a target protein, it suffices to run a deep backbone once and decode the effect of all mutations simultaneously using a shallow decoder. With the AlphaFold model as our backbone, our method outperforms existing methods on a variety of single and multiple mutation benchmarks. Our method scales to millions of mutations with minimal computational overhead and runs in a fraction of the time it would take prior works.", "md": "We present a method that efficiently scales thermodynamic stability prediction from single mutations to higher-order mutations. Our key insight is that the effects of mutations on the same protein are correlated. Thus, for a target protein, it suffices to run a deep backbone once and decode the effect of all mutations simultaneously using a shallow decoder. With the AlphaFold model as our backbone, our method outperforms existing methods on a variety of single and multiple mutation benchmarks. Our method scales to millions of mutations with minimal computational overhead and runs in a fraction of the time it would take prior works."}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "This work is supported by the NSF AI Institute for Foundations of Machine Learning (IFML).", "md": "This work is supported by the NSF AI Institute for Foundations of Machine Learning (IFML)."}]}, {"page": 11, "text": "References\n [1] Randomization of genes by pcr mutagenesis. Genome research 2(1), 28\u201333 (1992) 2\n [2] Adams,        J.P.,   Brown,      M.J.,    Diaz-Rodriguez,         A.,    Lloyd,     R.C.,     Roiban,     G.D.:       Biocataly-\n       sis:    A Pharma Perspective. Advanced Synthesis and Catalysis 361(11),                                 2421\u20132432 (2019).\n       https://doi.org/10.1002/adsc.201900424 1\n [3] Ahdritz, G., Bouatta, N., Kadyan, S., Xia, Q., Gerecke, W., O\u2019Donnell, T.J., Berenberg, D., Fisk, I.,\n       Zanichelli, N., Zhang, B., Nowaczynski, A., Wang, B., Stepniewska-Dziubinska, M.M., Zhang, S.,\n       Ojewole, A., Guney, M.E., Biderman, S., Watkins, A.M., Ra, S., Lorenzo, P.R., Nivon, L., Weitzner, B.,\n       Ban, Y.E.A., Sorger, P.K., Mostaque, E., Zhang, Z., Bonneau, R., AlQuraishi, M.: Openfold: Retraining\n       alphafold2 yields new insights into its learning mechanisms and capacity for generalization. bioRxiv (2022).\n       https://doi.org/10.1101/2022.11.20.517210 6\n [4] Arnold, F.H.: Design by directed evolution. Accounts of chemical research 31(3), 125\u2013131 (1998) 1, 2, 3\n [5] Bell, E.L., Finnigan, W., France, S.P., Green, A.P., Hayes, M.A., Hepworth, L.J., Lovelock, S.L., Niikura,\n       H., Osuna, S., Romero, E., Ryan, K.S., Turner, N.J., Flitsch, S.L.: Biocatalysis. Nature Reviews Methods\n       Primers 1(1), 1\u201321 (2021). https://doi.org/10.1038/s43586-021-00044-z 1\n [6] Benevenuta, S., Pancotti, C., Fariselli, P., Birolo, G., Sanavia, T.: An antisymmetric neural network to\n       predict free energy changes in protein variants. Journal of Physics D: Applied Physics 54(24), 245403\n      (2021) 3\n [7] Benevenuta, S., Pancotti, C., Fariselli, P., Birolo, G., Sanavia, T.: An antisymmetric neural network to\n       predict free energy changes in protein variants. Journal of Physics D: Applied Physics 54(24), 245403\n      (2021) 8\n [8] Benevenuta, S., Birolo, G., Sanavia, T., Capriotti, E., Fariselli, P.: Challenges in predicting stabilizing\n       variations: An exploration. Frontiers in Molecular Biosciences 9, 1075570 (2023) 5, 6, 17\n [9] Br\u00fcckner, A., Polge, C., Lentze, N., Auerbach, D., Schlattner, U.: Yeast two-hybrid, a powerful tool for\n       systems biology. International journal of molecular sciences 10(6), 2763\u20132788 (2009) 1\n[10] Cadet, F., Saavedra, E., Syren, P.O., Gontero, B.: Machine learning, epistasis, and protein engineering:\n       From sequence-structure-function relationships to regulation of metabolic pathways. Frontiers in Molecular\n       Biosciences 9, 1098289 (2022) 1\n[11] Capriotti, E., Fariselli, P., Casadio, R.: I-mutant2. 0: predicting stability changes upon mutation from the\n       protein sequence or structure. Nucleic acids research 33(suppl_2), W306\u2013W310 (2005) 3, 8\n[12] Chen, K., Arnold, F.H.: Tuning the activity of an enzyme for unusual environments: sequential random\n       mutagenesis of subtilisin e for catalysis in dimethylformamide. Proceedings of the National Academy of\n       Sciences 90(12), 5618\u20135622 (1993) 1, 3\n[13] Chen, T., Gong, C., Diaz, D.J., Chen, X., Wells, J.T., Wang, Z., Ellington, A., Dimakis, A., Klivans, A.,\n       et al.: Hotprotein: A novel framework for protein thermostability prediction and editing. In: The Eleventh\n       International Conference on Learning Representations (2022) 3\n[14] Chen, Y., Lu, H., Zhang, N., Zhu, Z., Wang, S., Li, M.: Premps: Predicting the impact of missense\n       mutations on protein stability. PLoS computational biology 16(12), e1008543 (2020) 3, 8\n[15] Cheng, J., Randall, A., Baldi, P.: Prediction of protein stability changes for single-site mutations using\n       support vector machines. Proteins: Structure, Function, and Bioinformatics 62(4), 1125\u20131132 (2006) 3, 8\n[16] Dehghanpoor, R., Ricks, E., Hursh, K., Gunderson, S., Farhoodi, R., Haspel, N., Hutchinson, B., Jagodzin-\n       ski, F.: Predicting the effect of single and multiple mutations on protein structural stability. Molecules\n       23(2), 251 (2018) 4, 7\n[17] Dehouck, Y., Kwasigroch, J.M., Gilis, D., Rooman, M.: Popmusic 2.1: a web server for the estimation of\n       protein stability changes upon mutation and sequence optimality. BMC bioinformatics 12(1), 1\u201312 (2011)\n       3, 8\n[18] Diaz, D.J., Gong, C., Ouyang-Zhang, J., Loy, J.M., Wells, J.T., Yang, D., Ellington, A.J., Dimakis, A.,\n       Klivans, A.R.: Stability oracle: A structure-based graph-transformer for identifying stabilizing mutations\n      (2023). https://doi.org/10.1101/2023.05.15.540857 3, 4, 7, 8, 16, 17, 18, 19\n[19] Diaz, D.J., Kulikova, A.V., Ellington, A.D., Wilke, C.O.: Using machine learning to predict the effects and\n       consequences of mutations in proteins. Current Opinion in Structural Biology 78, 102518 (2023) 3\n                                                                  11", "md": "# References\n\n## References\n\n1. Randomization of genes by PCR mutagenesis. Genome Research 2(1), 28\u201333 (1992)\n2. Adams, J.P., Brown, M.J., Diaz-Rodriguez, A., Lloyd, R.C., Roiban, G.D.: Biocatalysis: A Pharma Perspective. Advanced Synthesis and Catalysis 361(11), 2421\u20132432 (2019). Link\n3. Ahdritz, G., Bouatta, N., Kadyan, S., Xia, Q., Gerecke, W., O\u2019Donnell, T.J., Berenberg, D., Fisk, I., Zanichelli, N., Zhang, B., Nowaczynski, A., Wang, B., Stepniewska-Dziubinska, M.M., Zhang, S., Ojewole, A., Guney, M.E., Biderman, S., Watkins, A.M., Ra, S., Lorenzo, P.R., Nivon, L., Weitzner, B., Ban, Y.E.A., Sorger, P.K., Mostaque, E., Zhang, Z., Bonneau, R., AlQuraishi, M.: Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization. bioRxiv (2022). Link\n4. Arnold, F.H.: Design by directed evolution. Accounts of Chemical Research 31(3), 125\u2013131 (1998)\n5. Bell, E.L., Finnigan, W., France, S.P., Green, A.P., Hayes, M.A., Hepworth, L.J., Lovelock, S.L., Niikura, H., Osuna, S., Romero, E., Ryan, K.S., Turner, N.J., Flitsch, S.L.: Biocatalysis. Nature Reviews Methods Primers 1(1), 1\u201321 (2021). Link\n6. Benevenuta, S., Pancotti, C., Fariselli, P., Birolo, G., Sanavia, T.: An antisymmetric neural network to predict free energy changes in protein variants. Journal of Physics D: Applied Physics 54(24), 245403 (2021)\n7. Benevenuta, S., Pancotti, C., Fariselli, P., Birolo, G., Sanavia, T.: An antisymmetric neural network to predict free energy changes in protein variants. Journal of Physics D: Applied Physics 54(24), 245403 (2021)\n8. Benevenuta, S., Birolo, G., Sanavia, T., Capriotti, E., Fariselli, P.: Challenges in predicting stabilizing variations: An exploration. Frontiers in Molecular Biosciences 9, 1075570 (2023)\n9. Br\u00fcckner, A., Polge, C., Lentze, N., Auerbach, D., Schlattner, U.: Yeast two-hybrid, a powerful tool for systems biology. International Journal of Molecular Sciences 10(6), 2763\u20132788 (2009)\n10. Cadet, F., Saavedra, E., Syren, P.O., Gontero, B.: Machine learning, epistasis, and protein engineering: From sequence-structure-function relationships to regulation of metabolic pathways. Frontiers in Molecular Biosciences 9, 1098289 (2022)\n11. Capriotti, E., Fariselli, P., Casadio, R.: I-mutant2.0: predicting stability changes upon mutation from the protein sequence or structure. Nucleic Acids Research 33(suppl_2), W306\u2013W310 (2005)\n12. Chen, K., Arnold, F.H.: Tuning the activity of an enzyme for unusual environments: sequential random mutagenesis of subtilisin E for catalysis in dimethylformamide. Proceedings of the National Academy of Sciences 90(12), 5618\u20135622 (1993)\n13. Chen, T., Gong, C., Diaz, D.J., Chen, X., Wells, J.T., Wang, Z., Ellington, A., Dimakis, A., Klivans, A., et al.: Hotprotein: A novel framework for protein thermostability prediction and editing. In: The Eleventh International Conference on Learning Representations (2022)\n14. Chen, Y., Lu, H., Zhang, N., Zhu, Z., Wang, S., Li, M.: Premps: Predicting the impact of missense mutations on protein stability. PLoS Computational Biology 16(12), e1008543 (2020)\n15. Cheng, J., Randall, A., Baldi, P.: Prediction of protein stability changes for single-site mutations using support vector machines. Proteins: Structure, Function, and Bioinformatics 62(4), 1125\u20131132 (2006)\n16. Dehghanpoor, R., Ricks, E., Hursh, K., Gunderson, S., Farhoodi, R., Haspel, N., Hutchinson, B., Jagodzinski, F.: Predicting the effect of single and multiple mutations on protein structural stability. Molecules 23(2), 251 (2018)\n17. Dehouck, Y., Kwasigroch, J.M., Gilis, D., Rooman, M.: Popmusic 2.1: a web server for the estimation of protein stability changes upon mutation and sequence optimality. BMC Bioinformatics 12(1), 1\u201312 (2011)\n18. Diaz, D.J., Gong, C., Ouyang-Zhang, J., Loy, J.M., Wells, J.T., Yang, D., Ellington, A.J., Dimakis, A., Klivans, A.R.: Stability oracle: A structure-based graph-transformer for identifying stabilizing mutations (2023). Link\n19. Diaz, D.J., Kulikova, A.V., Ellington, A.D., Wilke, C.O.: Using machine learning to predict the effects and consequences of mutations in proteins. Current Opinion in Structural Biology 78, 102518 (2023)", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. Randomization of genes by PCR mutagenesis. Genome Research 2(1), 28\u201333 (1992)\n2. Adams, J.P., Brown, M.J., Diaz-Rodriguez, A., Lloyd, R.C., Roiban, G.D.: Biocatalysis: A Pharma Perspective. Advanced Synthesis and Catalysis 361(11), 2421\u20132432 (2019). Link\n3. Ahdritz, G., Bouatta, N., Kadyan, S., Xia, Q., Gerecke, W., O\u2019Donnell, T.J., Berenberg, D., Fisk, I., Zanichelli, N., Zhang, B., Nowaczynski, A., Wang, B., Stepniewska-Dziubinska, M.M., Zhang, S., Ojewole, A., Guney, M.E., Biderman, S., Watkins, A.M., Ra, S., Lorenzo, P.R., Nivon, L., Weitzner, B., Ban, Y.E.A., Sorger, P.K., Mostaque, E., Zhang, Z., Bonneau, R., AlQuraishi, M.: Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization. bioRxiv (2022). Link\n4. Arnold, F.H.: Design by directed evolution. Accounts of Chemical Research 31(3), 125\u2013131 (1998)\n5. Bell, E.L., Finnigan, W., France, S.P., Green, A.P., Hayes, M.A., Hepworth, L.J., Lovelock, S.L., Niikura, H., Osuna, S., Romero, E., Ryan, K.S., Turner, N.J., Flitsch, S.L.: Biocatalysis. Nature Reviews Methods Primers 1(1), 1\u201321 (2021). Link\n6. Benevenuta, S., Pancotti, C., Fariselli, P., Birolo, G., Sanavia, T.: An antisymmetric neural network to predict free energy changes in protein variants. Journal of Physics D: Applied Physics 54(24), 245403 (2021)\n7. Benevenuta, S., Pancotti, C., Fariselli, P., Birolo, G., Sanavia, T.: An antisymmetric neural network to predict free energy changes in protein variants. Journal of Physics D: Applied Physics 54(24), 245403 (2021)\n8. Benevenuta, S., Birolo, G., Sanavia, T., Capriotti, E., Fariselli, P.: Challenges in predicting stabilizing variations: An exploration. Frontiers in Molecular Biosciences 9, 1075570 (2023)\n9. Br\u00fcckner, A., Polge, C., Lentze, N., Auerbach, D., Schlattner, U.: Yeast two-hybrid, a powerful tool for systems biology. International Journal of Molecular Sciences 10(6), 2763\u20132788 (2009)\n10. Cadet, F., Saavedra, E., Syren, P.O., Gontero, B.: Machine learning, epistasis, and protein engineering: From sequence-structure-function relationships to regulation of metabolic pathways. Frontiers in Molecular Biosciences 9, 1098289 (2022)\n11. Capriotti, E., Fariselli, P., Casadio, R.: I-mutant2.0: predicting stability changes upon mutation from the protein sequence or structure. Nucleic Acids Research 33(suppl_2), W306\u2013W310 (2005)\n12. Chen, K., Arnold, F.H.: Tuning the activity of an enzyme for unusual environments: sequential random mutagenesis of subtilisin E for catalysis in dimethylformamide. Proceedings of the National Academy of Sciences 90(12), 5618\u20135622 (1993)\n13. Chen, T., Gong, C., Diaz, D.J., Chen, X., Wells, J.T., Wang, Z., Ellington, A., Dimakis, A., Klivans, A., et al.: Hotprotein: A novel framework for protein thermostability prediction and editing. In: The Eleventh International Conference on Learning Representations (2022)\n14. Chen, Y., Lu, H., Zhang, N., Zhu, Z., Wang, S., Li, M.: Premps: Predicting the impact of missense mutations on protein stability. PLoS Computational Biology 16(12), e1008543 (2020)\n15. Cheng, J., Randall, A., Baldi, P.: Prediction of protein stability changes for single-site mutations using support vector machines. Proteins: Structure, Function, and Bioinformatics 62(4), 1125\u20131132 (2006)\n16. Dehghanpoor, R., Ricks, E., Hursh, K., Gunderson, S., Farhoodi, R., Haspel, N., Hutchinson, B., Jagodzinski, F.: Predicting the effect of single and multiple mutations on protein structural stability. Molecules 23(2), 251 (2018)\n17. Dehouck, Y., Kwasigroch, J.M., Gilis, D., Rooman, M.: Popmusic 2.1: a web server for the estimation of protein stability changes upon mutation and sequence optimality. BMC Bioinformatics 12(1), 1\u201312 (2011)\n18. Diaz, D.J., Gong, C., Ouyang-Zhang, J., Loy, J.M., Wells, J.T., Yang, D., Ellington, A.J., Dimakis, A., Klivans, A.R.: Stability oracle: A structure-based graph-transformer for identifying stabilizing mutations (2023). Link\n19. Diaz, D.J., Kulikova, A.V., Ellington, A.D., Wilke, C.O.: Using machine learning to predict the effects and consequences of mutations in proteins. Current Opinion in Structural Biology 78, 102518 (2023)", "md": "1. Randomization of genes by PCR mutagenesis. Genome Research 2(1), 28\u201333 (1992)\n2. Adams, J.P., Brown, M.J., Diaz-Rodriguez, A., Lloyd, R.C., Roiban, G.D.: Biocatalysis: A Pharma Perspective. Advanced Synthesis and Catalysis 361(11), 2421\u20132432 (2019). Link\n3. Ahdritz, G., Bouatta, N., Kadyan, S., Xia, Q., Gerecke, W., O\u2019Donnell, T.J., Berenberg, D., Fisk, I., Zanichelli, N., Zhang, B., Nowaczynski, A., Wang, B., Stepniewska-Dziubinska, M.M., Zhang, S., Ojewole, A., Guney, M.E., Biderman, S., Watkins, A.M., Ra, S., Lorenzo, P.R., Nivon, L., Weitzner, B., Ban, Y.E.A., Sorger, P.K., Mostaque, E., Zhang, Z., Bonneau, R., AlQuraishi, M.: Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization. bioRxiv (2022). Link\n4. Arnold, F.H.: Design by directed evolution. Accounts of Chemical Research 31(3), 125\u2013131 (1998)\n5. Bell, E.L., Finnigan, W., France, S.P., Green, A.P., Hayes, M.A., Hepworth, L.J., Lovelock, S.L., Niikura, H., Osuna, S., Romero, E., Ryan, K.S., Turner, N.J., Flitsch, S.L.: Biocatalysis. Nature Reviews Methods Primers 1(1), 1\u201321 (2021). Link\n6. Benevenuta, S., Pancotti, C., Fariselli, P., Birolo, G., Sanavia, T.: An antisymmetric neural network to predict free energy changes in protein variants. Journal of Physics D: Applied Physics 54(24), 245403 (2021)\n7. Benevenuta, S., Pancotti, C., Fariselli, P., Birolo, G., Sanavia, T.: An antisymmetric neural network to predict free energy changes in protein variants. Journal of Physics D: Applied Physics 54(24), 245403 (2021)\n8. Benevenuta, S., Birolo, G., Sanavia, T., Capriotti, E., Fariselli, P.: Challenges in predicting stabilizing variations: An exploration. Frontiers in Molecular Biosciences 9, 1075570 (2023)\n9. Br\u00fcckner, A., Polge, C., Lentze, N., Auerbach, D., Schlattner, U.: Yeast two-hybrid, a powerful tool for systems biology. International Journal of Molecular Sciences 10(6), 2763\u20132788 (2009)\n10. Cadet, F., Saavedra, E., Syren, P.O., Gontero, B.: Machine learning, epistasis, and protein engineering: From sequence-structure-function relationships to regulation of metabolic pathways. Frontiers in Molecular Biosciences 9, 1098289 (2022)\n11. Capriotti, E., Fariselli, P., Casadio, R.: I-mutant2.0: predicting stability changes upon mutation from the protein sequence or structure. Nucleic Acids Research 33(suppl_2), W306\u2013W310 (2005)\n12. Chen, K., Arnold, F.H.: Tuning the activity of an enzyme for unusual environments: sequential random mutagenesis of subtilisin E for catalysis in dimethylformamide. Proceedings of the National Academy of Sciences 90(12), 5618\u20135622 (1993)\n13. Chen, T., Gong, C., Diaz, D.J., Chen, X., Wells, J.T., Wang, Z., Ellington, A., Dimakis, A., Klivans, A., et al.: Hotprotein: A novel framework for protein thermostability prediction and editing. In: The Eleventh International Conference on Learning Representations (2022)\n14. Chen, Y., Lu, H., Zhang, N., Zhu, Z., Wang, S., Li, M.: Premps: Predicting the impact of missense mutations on protein stability. PLoS Computational Biology 16(12), e1008543 (2020)\n15. Cheng, J., Randall, A., Baldi, P.: Prediction of protein stability changes for single-site mutations using support vector machines. Proteins: Structure, Function, and Bioinformatics 62(4), 1125\u20131132 (2006)\n16. Dehghanpoor, R., Ricks, E., Hursh, K., Gunderson, S., Farhoodi, R., Haspel, N., Hutchinson, B., Jagodzinski, F.: Predicting the effect of single and multiple mutations on protein structural stability. Molecules 23(2), 251 (2018)\n17. Dehouck, Y., Kwasigroch, J.M., Gilis, D., Rooman, M.: Popmusic 2.1: a web server for the estimation of protein stability changes upon mutation and sequence optimality. BMC Bioinformatics 12(1), 1\u201312 (2011)\n18. Diaz, D.J., Gong, C., Ouyang-Zhang, J., Loy, J.M., Wells, J.T., Yang, D., Ellington, A.J., Dimakis, A., Klivans, A.R.: Stability oracle: A structure-based graph-transformer for identifying stabilizing mutations (2023). Link\n19. Diaz, D.J., Kulikova, A.V., Ellington, A.D., Wilke, C.O.: Using machine learning to predict the effects and consequences of mutations in proteins. Current Opinion in Structural Biology 78, 102518 (2023)"}]}, {"page": 12, "text": "[20] d\u2019Oelsnitz, S., Diaz, D.J., Acosta, D.J., Schechter, M.W., Minus, M.B., Howard, J.R., Do, H., Loy, J., Alper,\n     H., Ellington, A.D.: Synthetic microbial sensing and biosynthesis of amaryllidaceae alkaloids. bioRxiv pp.\n     2023\u201304 (2023) 3\n[21] d\u2019Oelsnitz, S., Kim, W., Burkholder, N.T., Javanmardi, K., Thyer, R., Zhang, Y., Alper, H.S., Ellington,\n     A.D.: Using fungible biosensors to evolve improved alkaloid biosyntheses. Nature Chemical Biology 18(9),\n     981\u2013989 (2022) 1\n[22] Ellefson, J.W., Gollihar, J., Shroff, R., Shivram, H., Iyer, V.R., Ellington, A.D.: Synthetic evolutionary\n     origin of a proofreading reverse transcriptase. Science 352(6293), 1590\u20131593 (2016) 1\n[23] Feng, X., Sanchis, J., Reetz, M.T., Rabitz, H.: Enhancing the efficiency of directed evolution in focused\n     enzyme libraries by the adaptive substituent reordering algorithm. Chemistry\u2013A European Journal 18(18),\n     5646\u20135654 (2012) 3\n[24] Frazer, J., Notin, P., Dias, M., Gomez, A., Min, J.K., Brock, K., Gal, Y., Marks, D.S.: Disease variant\n     prediction with deep generative models of evolutionary data. Nature 599(7883), 91\u201395 (2021) 9\n[25] Gebauer, M., Skerra, A.: Engineered protein scaffolds as next-generation therapeutics. Annual Review of\n     Pharmacology and Toxicology 60, 391\u2013415 (2020). https://doi.org/10.1146/annurev-pharmtox-010818-\n     021118 1\n[26] Gerasimavicius, L., Livesey, B.J., Marsh, J.A.: Correspondence between functional scores from deep\n     mutational scans and predicted effects on protein stability. bioRxiv pp. 2023\u201302 (2023) 9\n[27] Giver, L., Gershenson, A., Freskgard, P.O., Arnold, F.H.: Directed evolution of a thermostable esterase.\n     Proceedings of the National Academy of Sciences 95(22), 12809\u201312813 (1998) 2, 3\n[28] Hie, B.L., Shanker, V.R., Xu, D., Bruun, T.U., Weidenbacher, P.A., Tang, S., Wu, W., Pak, J.E., Kim, P.S.:\n     Efficient evolution of human antibodies from general protein language models. Nature Biotechnology\n     (2023) 3\n[29] Jaroszewicz, W., Morcinek-Or\u0142owska, J., Pierzynowska, K., Gaffke, L., W\u02db     egrzyn, G.: Phage display and\n     other peptide display technologies. FEMS Microbiology Reviews 46(2), fuab052 (2022) 1\n[30] J\u00e4rvelin, K., Kek\u00e4l\u00e4inen, J.: Cumulated gain-based evaluation of ir techniques. ACM Transactions on\n     Information Systems (TOIS) 20(4), 422\u2013446 (2002) 6, 17\n[31] Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates,\n     R., \u017d\u00eddek, A., Potapenko, A., et al.: Highly accurate protein structure prediction with alphafold. Nature\n     596(7873), 583\u2013589 (2021) 2, 3, 4, 6, 9, 10\n[32] Kellogg, E.H., Leaver-Fay, A., Baker, D.: Role of conformational sampling in computing mutation-induced\n     changes in protein structure and stability. Proteins: Structure, Function, and Bioinformatics 79(3), 830\u2013838\n     (2011) 3\n[33] Klabunde, T., Petrassi, H.M., Oza, V.B., Raman, P., Kelly, J.W., Sacchettini, J.C.: Rational design of potent\n     human transthyretin amyloid disease inhibitors. Nature structural biology 7(4), 312\u2013321 (2000) 1, 2\n[34] Kouba, P., Kohout, P., Haddadi, F., Bushuiev, A., Samusevich, R., Sedlar, J., Damborsky, J., Pluskal, T.,\n     Sivic, J., Mazurenko, S.: Machine learning-guided protein engineering. ACS Catalysis 13, 13863\u201313895\n     (2023) 3\n[35] Laimer, J., Hofer, H., Fritz, M., Wegenkittl, S., Lackner, P.: Maestro-multi agent stability prediction upon\n     point mutations. BMC bioinformatics 16(1), 1\u201313 (2015) 3, 4, 7, 8\n[36] Li, B., Yang, Y.T., Capra, J.A., Gerstein, M.B.: Predicting changes in protein thermodynamic stability\n     upon point mutation with deep 3d convolutional neural networks. PLoS computational biology 16(11),\n     e1008291 (2020) 3, 4, 8\n[37] Li, G., Panday, S.K., Alexov, E.: Saafec-seq: a sequence-based method for predicting the effect of single\n     point mutations on protein thermodynamic stability. International journal of molecular sciences 22(2), 606\n     (2021) 3\n[38] Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., dos Santos Costa, A., Fazel-Zarandi,\n     M., Sercu, T., Candido, S., et al.: Language models of protein sequences at the scale of evolution enable\n     accurate structure prediction. bioRxiv (2022) 3, 4, 6, 9, 10, 18\n                                                       12", "md": "1. d\u2019Oelsnitz, S., Diaz, D.J., Acosta, D.J., Schechter, M.W., Minus, M.B., Howard, J.R., Do, H., Loy, J., Alper, H., Ellington, A.D.: Synthetic microbial sensing and biosynthesis of amaryllidaceae alkaloids. bioRxiv pp. 2023\u201304 (2023)\n2. d\u2019Oelsnitz, S., Kim, W., Burkholder, N.T., Javanmardi, K., Thyer, R., Zhang, Y., Alper, H.S., Ellington, A.D.: Using fungible biosensors to evolve improved alkaloid biosyntheses. Nature Chemical Biology 18(9), 981\u2013989 (2022)\n3. Ellefson, J.W., Gollihar, J., Shroff, R., Shivram, H., Iyer, V.R., Ellington, A.D.: Synthetic evolutionary origin of a proofreading reverse transcriptase. Science 352(6293), 1590\u20131593 (2016)\n4. Feng, X., Sanchis, J., Reetz, M.T., Rabitz, H.: Enhancing the efficiency of directed evolution in focused enzyme libraries by the adaptive substituent reordering algorithm. Chemistry\u2013A European Journal 18(18), 5646\u20135654 (2012)\n5. Frazer, J., Notin, P., Dias, M., Gomez, A., Min, J.K., Brock, K., Gal, Y., Marks, D.S.: Disease variant prediction with deep generative models of evolutionary data. Nature 599(7883), 91\u201395 (2021)\n6. Gebauer, M., Skerra, A.: Engineered protein scaffolds as next-generation therapeutics. Annual Review of Pharmacology and Toxicology 60, 391\u2013415 (2020). https://doi.org/10.1146/annurev-pharmtox-010818-021118\n7. Gerasimavicius, L., Livesey, B.J., Marsh, J.A.: Correspondence between functional scores from deep mutational scans and predicted effects on protein stability. bioRxiv pp. 2023\u201302 (2023)\n8. Giver, L., Gershenson, A., Freskgard, P.O., Arnold, F.H.: Directed evolution of a thermostable esterase. Proceedings of the National Academy of Sciences 95(22), 12809\u201312813 (1998)\n9. Hie, B.L., Shanker, V.R., Xu, D., Bruun, T.U., Weidenbacher, P.A., Tang, S., Wu, W., Pak, J.E., Kim, P.S.: Efficient evolution of human antibodies from general protein language models. Nature Biotechnology (2023)\n10. Jaroszewicz, W., Morcinek-Or\u0142owska, J., Pierzynowska, K., Gaffke, L., W\u02db egrzyn, G.: Phage display and other peptide display technologies. FEMS Microbiology Reviews 46(2), fuab052 (2022)", "images": [], "items": [{"type": "text", "value": "1. d\u2019Oelsnitz, S., Diaz, D.J., Acosta, D.J., Schechter, M.W., Minus, M.B., Howard, J.R., Do, H., Loy, J., Alper, H., Ellington, A.D.: Synthetic microbial sensing and biosynthesis of amaryllidaceae alkaloids. bioRxiv pp. 2023\u201304 (2023)\n2. d\u2019Oelsnitz, S., Kim, W., Burkholder, N.T., Javanmardi, K., Thyer, R., Zhang, Y., Alper, H.S., Ellington, A.D.: Using fungible biosensors to evolve improved alkaloid biosyntheses. Nature Chemical Biology 18(9), 981\u2013989 (2022)\n3. Ellefson, J.W., Gollihar, J., Shroff, R., Shivram, H., Iyer, V.R., Ellington, A.D.: Synthetic evolutionary origin of a proofreading reverse transcriptase. Science 352(6293), 1590\u20131593 (2016)\n4. Feng, X., Sanchis, J., Reetz, M.T., Rabitz, H.: Enhancing the efficiency of directed evolution in focused enzyme libraries by the adaptive substituent reordering algorithm. Chemistry\u2013A European Journal 18(18), 5646\u20135654 (2012)\n5. Frazer, J., Notin, P., Dias, M., Gomez, A., Min, J.K., Brock, K., Gal, Y., Marks, D.S.: Disease variant prediction with deep generative models of evolutionary data. Nature 599(7883), 91\u201395 (2021)\n6. Gebauer, M., Skerra, A.: Engineered protein scaffolds as next-generation therapeutics. Annual Review of Pharmacology and Toxicology 60, 391\u2013415 (2020). https://doi.org/10.1146/annurev-pharmtox-010818-021118\n7. Gerasimavicius, L., Livesey, B.J., Marsh, J.A.: Correspondence between functional scores from deep mutational scans and predicted effects on protein stability. bioRxiv pp. 2023\u201302 (2023)\n8. Giver, L., Gershenson, A., Freskgard, P.O., Arnold, F.H.: Directed evolution of a thermostable esterase. Proceedings of the National Academy of Sciences 95(22), 12809\u201312813 (1998)\n9. Hie, B.L., Shanker, V.R., Xu, D., Bruun, T.U., Weidenbacher, P.A., Tang, S., Wu, W., Pak, J.E., Kim, P.S.: Efficient evolution of human antibodies from general protein language models. Nature Biotechnology (2023)\n10. Jaroszewicz, W., Morcinek-Or\u0142owska, J., Pierzynowska, K., Gaffke, L., W\u02db egrzyn, G.: Phage display and other peptide display technologies. FEMS Microbiology Reviews 46(2), fuab052 (2022)", "md": "1. d\u2019Oelsnitz, S., Diaz, D.J., Acosta, D.J., Schechter, M.W., Minus, M.B., Howard, J.R., Do, H., Loy, J., Alper, H., Ellington, A.D.: Synthetic microbial sensing and biosynthesis of amaryllidaceae alkaloids. bioRxiv pp. 2023\u201304 (2023)\n2. d\u2019Oelsnitz, S., Kim, W., Burkholder, N.T., Javanmardi, K., Thyer, R., Zhang, Y., Alper, H.S., Ellington, A.D.: Using fungible biosensors to evolve improved alkaloid biosyntheses. Nature Chemical Biology 18(9), 981\u2013989 (2022)\n3. Ellefson, J.W., Gollihar, J., Shroff, R., Shivram, H., Iyer, V.R., Ellington, A.D.: Synthetic evolutionary origin of a proofreading reverse transcriptase. Science 352(6293), 1590\u20131593 (2016)\n4. Feng, X., Sanchis, J., Reetz, M.T., Rabitz, H.: Enhancing the efficiency of directed evolution in focused enzyme libraries by the adaptive substituent reordering algorithm. Chemistry\u2013A European Journal 18(18), 5646\u20135654 (2012)\n5. Frazer, J., Notin, P., Dias, M., Gomez, A., Min, J.K., Brock, K., Gal, Y., Marks, D.S.: Disease variant prediction with deep generative models of evolutionary data. Nature 599(7883), 91\u201395 (2021)\n6. Gebauer, M., Skerra, A.: Engineered protein scaffolds as next-generation therapeutics. Annual Review of Pharmacology and Toxicology 60, 391\u2013415 (2020). https://doi.org/10.1146/annurev-pharmtox-010818-021118\n7. Gerasimavicius, L., Livesey, B.J., Marsh, J.A.: Correspondence between functional scores from deep mutational scans and predicted effects on protein stability. bioRxiv pp. 2023\u201302 (2023)\n8. Giver, L., Gershenson, A., Freskgard, P.O., Arnold, F.H.: Directed evolution of a thermostable esterase. Proceedings of the National Academy of Sciences 95(22), 12809\u201312813 (1998)\n9. Hie, B.L., Shanker, V.R., Xu, D., Bruun, T.U., Weidenbacher, P.A., Tang, S., Wu, W., Pak, J.E., Kim, P.S.: Efficient evolution of human antibodies from general protein language models. Nature Biotechnology (2023)\n10. Jaroszewicz, W., Morcinek-Or\u0142owska, J., Pierzynowska, K., Gaffke, L., W\u02db egrzyn, G.: Phage display and other peptide display technologies. FEMS Microbiology Reviews 46(2), fuab052 (2022)"}]}, {"page": 13, "text": "[39] Loell, K., Nanda, V.: Marginal protein stability drives subcellular proteome isoelectric point. Proceedings\n      of the National Academy of Sciences 115(46), 11778\u201311783 (2018) 1\n[40] Lu, H., Diaz, D.J., Czarnecki, N.J., Zhu, C., Kim, W., Shroff, R., Acosta, D.J., Alexander, B.R., Cole,\n      H.O., Zhang, Y., et al.: Machine learning-aided engineering of hydrolases for pet depolymerization. Nature\n      604(7907), 662\u2013667 (2022) 3\n[41] Meghwanshi, G.K., Kaur, N., Verma, S., Dabi, N.K., Vashishtha, A., Charan, P.D., Purohit, P., Bhandari,\n      H.S., Bhojak, N., Kumar, R.: Enzymes for pharmaceutical and therapeutic applications. Biotechnology\n      and Applied Biochemistry 67(4), 586\u2013601 (2020). https://doi.org/10.1002/bab.1919 1\n[42] Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., Rives, A.: Language models enable zero-shot prediction\n      of the effects of mutations on protein function. Advances in Neural Information Processing Systems 34,\n      29287\u201329303 (2021) 9, 18\n[43] Mirdita, M., Sch\u00fctze, K., Moriwaki, Y., Heo, L., Ovchinnikov, S., Steinegger, M.: Colabfold: making\n      protein folding accessible to all. Nature methods 19(6), 679\u2013682 (2022) 6\n[44] Montanucci, L., Capriotti, E., Frank, Y., Ben-Tal, N., Fariselli, P.: Ddgun: an untrained method for the\n      prediction of protein stability changes upon single and multiple point variations. BMC bioinformatics 20,\n     1\u201310 (2019) 3, 4, 7, 8, 16, 18\n[45] Montanucci, L., Savojardo, C., Martelli, P.L., Casadio, R., Fariselli, P.: On the biases in predictions of\n      protein stability changes upon variations: the inps test case. Bioinformatics 35(14), 2525\u20132527 (2019) 16\n[46] Musdal, Y., Govindarajan, S., Mannervik, B.: Exploring sequence-function space of a poplar glutathione\n      transferase using designed information-rich gene variants. Protein Engineering, Design and Selection 30(8),\n      543\u2013549 (2017) 3\n[47] Nijkamp, E., Ruffolo, J., Weinstein, E.N., Naik, N., Madani, A.: Progen2: exploring the boundaries of\n      protein language models. arXiv preprint arXiv:2206.13517 (2022) 9\n[48] Nikam, R., Kulandaisamy, A., Harini, K., Sharma, D., Gromiha, M.M.: Prothermdb: thermodynamic\n      database for proteins and mutants revisited after 15 years. Nucleic acids research 49(D1), D420\u2013D424\n      (2021) 2, 16\n[49] Notin, P., Dias, M., Frazer, J., Hurtado, J.M., Gomez, A.N., Marks, D., Gal, Y.: Tranception: protein fitness\n      prediction with autoregressive transformers and inference-time retrieval. In: International Conference on\n      Machine Learning. pp. 16990\u201317017. PMLR (2022) 2, 3, 9, 16, 18\n[50] Notin, P.M., Van Niekerk, L., Kollasch, A.W., Ritter, D., Gal, Y., Marks, D.: Trancepteve: Combining\n      family-specific and family-agnostic models of protein sequences for improved fitness prediction. bioRxiv\n      pp. 2022\u201312 (2022) 9\n[51] Nutschel, C., Fulton, A., Zimmermann, O., Schwaneberg, U., Jaeger, K.E., Gohlke, H.: Systematically\n      scrutinizing the impact of substitution sites on thermostability and detergent tolerance for bacillus subtilis\n      lipase a. Journal of chemical information and modeling 60(3), 1568\u20131584 (2020) 9\n[52] Paik, I., Ngo, P.H., Shroff, R., Diaz, D.J., Maranhao, A.C., Walker, D.J., Bhadra, S., Ellington, A.D.:\n      Improved bst dna polymerase variants derived via a machine learning approach. Biochemistry (2021) 3\n[53] Pak, M.A., Markhieva, K.A., Novikova, M.S., Petrov, D.S., Vorobyev, I.S., Maksimova, E.S., Kondrashov,\n      F.A., Ivankov, D.N.: Using alphafold to predict the impact of single mutations on protein stability and\n      function. Plos one 18(3), e0282689 (2023) 3\n[54] Pancotti, C., Benevenuta, S., Birolo, G., Alberini, V., Repetto, V., Sanavia, T., Capriotti, E., Fariselli, P.:\n      Predicting protein stability changes upon single-point mutation: a thorough comparison of the available\n      tools on a new dataset. Briefings in Bioinformatics 23(2), bbab555 (2022) 2, 8, 16, 17\n[55] Pancotti, C., Benevenuta, S., Repetto, V., Birolo, G., Capriotti, E., Sanavia, T., Fariselli, P.: A deep-learning\n      sequence-based method to predict protein stability changes upon genetic variations. Genes 12(6), 911\n      (2021) 8\n[56] Pires, D.E., Ascher, D.B., Blundell, T.L.: Duet: a server for predicting effects of mutations on protein\n      stability using an integrated computational approach. Nucleic acids research 42(W1), W314\u2013W319 (2014)\n      3, 8\n[57] Pires, D.E., Ascher, D.B., Blundell, T.L.: mcsm: predicting the effects of mutations in proteins using\n      graph-based signatures. Bioinformatics 30(3), 335\u2013342 (2014) 8\n                                                         13", "md": "1. Loell, K., Nanda, V.: Marginal protein stability drives subcellular proteome isoelectric point. Proceedings of the National Academy of Sciences 115(46), 11778\u201311783 (2018) 1\n2. Lu, H., Diaz, D.J., Czarnecki, N.J., Zhu, C., Kim, W., Shroff, R., Acosta, D.J., Alexander, B.R., Cole, H.O., Zhang, Y., et al.: Machine learning-aided engineering of hydrolases for pet depolymerization. Nature 604(7907), 662\u2013667 (2022) 3\n3. Meghwanshi, G.K., Kaur, N., Verma, S., Dabi, N.K., Vashishtha, A., Charan, P.D., Purohit, P., Bhandari, H.S., Bhojak, N., Kumar, R.: Enzymes for pharmaceutical and therapeutic applications. Biotechnology and Applied Biochemistry 67(4), 586\u2013601 (2020). https://doi.org/10.1002/bab.1919 1\n4. Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., Rives, A.: Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in Neural Information Processing Systems 34, 29287\u201329303 (2021) 9, 18\n5. Mirdita, M., Sch\u00fctze, K., Moriwaki, Y., Heo, L., Ovchinnikov, S., Steinegger, M.: Colabfold: making protein folding accessible to all. Nature methods 19(6), 679\u2013682 (2022) 6\n6. Montanucci, L., Capriotti, E., Frank, Y., Ben-Tal, N., Fariselli, P.: Ddgun: an untrained method for the prediction of protein stability changes upon single and multiple point variations. BMC bioinformatics 20, 1\u201310 (2019) 3, 4, 7, 8, 16, 18\n7. Montanucci, L., Savojardo, C., Martelli, P.L., Casadio, R., Fariselli, P.: On the biases in predictions of protein stability changes upon variations: the inps test case. Bioinformatics 35(14), 2525\u20132527 (2019) 16\n8. Musdal, Y., Govindarajan, S., Mannervik, B.: Exploring sequence-function space of a poplar glutathione transferase using designed information-rich gene variants. Protein Engineering, Design and Selection 30(8), 543\u2013549 (2017) 3\n9. Nijkamp, E., Ruffolo, J., Weinstein, E.N., Naik, N., Madani, A.: Progen2: exploring the boundaries of protein language models. arXiv preprint arXiv:2206.13517 (2022) 9\n10. Nikam, R., Kulandaisamy, A., Harini, K., Sharma, D., Gromiha, M.M.: Prothermdb: thermodynamic database for proteins and mutants revisited after 15 years. Nucleic acids research 49(D1), D420\u2013D424 (2021) 2, 16\n11. Notin, P., Dias, M., Frazer, J., Hurtado, J.M., Gomez, A.N., Marks, D., Gal, Y.: Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In: International Conference on Machine Learning. pp. 16990\u201317017. PMLR (2022) 2, 3, 9, 16, 18\n12. Notin, P.M., Van Niekerk, L., Kollasch, A.W., Ritter, D., Gal, Y., Marks, D.: Trancepteve: Combining family-specific and family-agnostic models of protein sequences for improved fitness prediction. bioRxiv pp. 2022\u201312 (2022) 9\n13. Nutschel, C., Fulton, A., Zimmermann, O., Schwaneberg, U., Jaeger, K.E., Gohlke, H.: Systematically scrutinizing the impact of substitution sites on thermostability and detergent tolerance for bacillus subtilis lipase a. Journal of chemical information and modeling 60(3), 1568\u20131584 (2020) 9\n14. Paik, I., Ngo, P.H., Shroff, R., Diaz, D.J., Maranhao, A.C., Walker, D.J., Bhadra, S., Ellington, A.D.: Improved bst dna polymerase variants derived via a machine learning approach. Biochemistry (2021) 3\n15. Pak, M.A., Markhieva, K.A., Novikova, M.S., Petrov, D.S., Vorobyev, I.S., Maksimova, E.S., Kondrashov, F.A., Ivankov, D.N.: Using alphafold to predict the impact of single mutations on protein stability and function. Plos one 18(3), e0282689 (2023) 3\n16. Pancotti, C., Benevenuta, S., Birolo, G., Alberini, V., Repetto, V., Sanavia, T., Capriotti, E., Fariselli, P.: Predicting protein stability changes upon single-point mutation: a thorough comparison of the available tools on a new dataset. Briefings in Bioinformatics 23(2), bbab555 (2022) 2, 8, 16, 17\n17. Pancotti, C., Benevenuta, S., Repetto, V., Birolo, G., Capriotti, E., Sanavia, T., Fariselli, P.: A deep-learning sequence-based method to predict protein stability changes upon genetic variations. Genes 12(6), 911 (2021) 8\n18. Pires, D.E., Ascher, D.B., Blundell, T.L.: Duet: a server for predicting effects of mutations on protein stability using an integrated computational approach. Nucleic acids research 42(W1), W314\u2013W319 (2014) 3, 8\n19. Pires, D.E., Ascher, D.B., Blundell, T.L.: mcsm: predicting the effects of mutations in proteins using graph-based signatures. Bioinformatics 30(3), 335\u2013342 (2014) 8", "images": [], "items": [{"type": "text", "value": "1. Loell, K., Nanda, V.: Marginal protein stability drives subcellular proteome isoelectric point. Proceedings of the National Academy of Sciences 115(46), 11778\u201311783 (2018) 1\n2. Lu, H., Diaz, D.J., Czarnecki, N.J., Zhu, C., Kim, W., Shroff, R., Acosta, D.J., Alexander, B.R., Cole, H.O., Zhang, Y., et al.: Machine learning-aided engineering of hydrolases for pet depolymerization. Nature 604(7907), 662\u2013667 (2022) 3\n3. Meghwanshi, G.K., Kaur, N., Verma, S., Dabi, N.K., Vashishtha, A., Charan, P.D., Purohit, P., Bhandari, H.S., Bhojak, N., Kumar, R.: Enzymes for pharmaceutical and therapeutic applications. Biotechnology and Applied Biochemistry 67(4), 586\u2013601 (2020). https://doi.org/10.1002/bab.1919 1\n4. Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., Rives, A.: Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in Neural Information Processing Systems 34, 29287\u201329303 (2021) 9, 18\n5. Mirdita, M., Sch\u00fctze, K., Moriwaki, Y., Heo, L., Ovchinnikov, S., Steinegger, M.: Colabfold: making protein folding accessible to all. Nature methods 19(6), 679\u2013682 (2022) 6\n6. Montanucci, L., Capriotti, E., Frank, Y., Ben-Tal, N., Fariselli, P.: Ddgun: an untrained method for the prediction of protein stability changes upon single and multiple point variations. BMC bioinformatics 20, 1\u201310 (2019) 3, 4, 7, 8, 16, 18\n7. Montanucci, L., Savojardo, C., Martelli, P.L., Casadio, R., Fariselli, P.: On the biases in predictions of protein stability changes upon variations: the inps test case. Bioinformatics 35(14), 2525\u20132527 (2019) 16\n8. Musdal, Y., Govindarajan, S., Mannervik, B.: Exploring sequence-function space of a poplar glutathione transferase using designed information-rich gene variants. Protein Engineering, Design and Selection 30(8), 543\u2013549 (2017) 3\n9. Nijkamp, E., Ruffolo, J., Weinstein, E.N., Naik, N., Madani, A.: Progen2: exploring the boundaries of protein language models. arXiv preprint arXiv:2206.13517 (2022) 9\n10. Nikam, R., Kulandaisamy, A., Harini, K., Sharma, D., Gromiha, M.M.: Prothermdb: thermodynamic database for proteins and mutants revisited after 15 years. Nucleic acids research 49(D1), D420\u2013D424 (2021) 2, 16\n11. Notin, P., Dias, M., Frazer, J., Hurtado, J.M., Gomez, A.N., Marks, D., Gal, Y.: Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In: International Conference on Machine Learning. pp. 16990\u201317017. PMLR (2022) 2, 3, 9, 16, 18\n12. Notin, P.M., Van Niekerk, L., Kollasch, A.W., Ritter, D., Gal, Y., Marks, D.: Trancepteve: Combining family-specific and family-agnostic models of protein sequences for improved fitness prediction. bioRxiv pp. 2022\u201312 (2022) 9\n13. Nutschel, C., Fulton, A., Zimmermann, O., Schwaneberg, U., Jaeger, K.E., Gohlke, H.: Systematically scrutinizing the impact of substitution sites on thermostability and detergent tolerance for bacillus subtilis lipase a. Journal of chemical information and modeling 60(3), 1568\u20131584 (2020) 9\n14. Paik, I., Ngo, P.H., Shroff, R., Diaz, D.J., Maranhao, A.C., Walker, D.J., Bhadra, S., Ellington, A.D.: Improved bst dna polymerase variants derived via a machine learning approach. Biochemistry (2021) 3\n15. Pak, M.A., Markhieva, K.A., Novikova, M.S., Petrov, D.S., Vorobyev, I.S., Maksimova, E.S., Kondrashov, F.A., Ivankov, D.N.: Using alphafold to predict the impact of single mutations on protein stability and function. Plos one 18(3), e0282689 (2023) 3\n16. Pancotti, C., Benevenuta, S., Birolo, G., Alberini, V., Repetto, V., Sanavia, T., Capriotti, E., Fariselli, P.: Predicting protein stability changes upon single-point mutation: a thorough comparison of the available tools on a new dataset. Briefings in Bioinformatics 23(2), bbab555 (2022) 2, 8, 16, 17\n17. Pancotti, C., Benevenuta, S., Repetto, V., Birolo, G., Capriotti, E., Sanavia, T., Fariselli, P.: A deep-learning sequence-based method to predict protein stability changes upon genetic variations. Genes 12(6), 911 (2021) 8\n18. Pires, D.E., Ascher, D.B., Blundell, T.L.: Duet: a server for predicting effects of mutations on protein stability using an integrated computational approach. Nucleic acids research 42(W1), W314\u2013W319 (2014) 3, 8\n19. Pires, D.E., Ascher, D.B., Blundell, T.L.: mcsm: predicting the effects of mutations in proteins using graph-based signatures. Bioinformatics 30(3), 335\u2013342 (2014) 8", "md": "1. Loell, K., Nanda, V.: Marginal protein stability drives subcellular proteome isoelectric point. Proceedings of the National Academy of Sciences 115(46), 11778\u201311783 (2018) 1\n2. Lu, H., Diaz, D.J., Czarnecki, N.J., Zhu, C., Kim, W., Shroff, R., Acosta, D.J., Alexander, B.R., Cole, H.O., Zhang, Y., et al.: Machine learning-aided engineering of hydrolases for pet depolymerization. Nature 604(7907), 662\u2013667 (2022) 3\n3. Meghwanshi, G.K., Kaur, N., Verma, S., Dabi, N.K., Vashishtha, A., Charan, P.D., Purohit, P., Bhandari, H.S., Bhojak, N., Kumar, R.: Enzymes for pharmaceutical and therapeutic applications. Biotechnology and Applied Biochemistry 67(4), 586\u2013601 (2020). https://doi.org/10.1002/bab.1919 1\n4. Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., Rives, A.: Language models enable zero-shot prediction of the effects of mutations on protein function. Advances in Neural Information Processing Systems 34, 29287\u201329303 (2021) 9, 18\n5. Mirdita, M., Sch\u00fctze, K., Moriwaki, Y., Heo, L., Ovchinnikov, S., Steinegger, M.: Colabfold: making protein folding accessible to all. Nature methods 19(6), 679\u2013682 (2022) 6\n6. Montanucci, L., Capriotti, E., Frank, Y., Ben-Tal, N., Fariselli, P.: Ddgun: an untrained method for the prediction of protein stability changes upon single and multiple point variations. BMC bioinformatics 20, 1\u201310 (2019) 3, 4, 7, 8, 16, 18\n7. Montanucci, L., Savojardo, C., Martelli, P.L., Casadio, R., Fariselli, P.: On the biases in predictions of protein stability changes upon variations: the inps test case. Bioinformatics 35(14), 2525\u20132527 (2019) 16\n8. Musdal, Y., Govindarajan, S., Mannervik, B.: Exploring sequence-function space of a poplar glutathione transferase using designed information-rich gene variants. Protein Engineering, Design and Selection 30(8), 543\u2013549 (2017) 3\n9. Nijkamp, E., Ruffolo, J., Weinstein, E.N., Naik, N., Madani, A.: Progen2: exploring the boundaries of protein language models. arXiv preprint arXiv:2206.13517 (2022) 9\n10. Nikam, R., Kulandaisamy, A., Harini, K., Sharma, D., Gromiha, M.M.: Prothermdb: thermodynamic database for proteins and mutants revisited after 15 years. Nucleic acids research 49(D1), D420\u2013D424 (2021) 2, 16\n11. Notin, P., Dias, M., Frazer, J., Hurtado, J.M., Gomez, A.N., Marks, D., Gal, Y.: Tranception: protein fitness prediction with autoregressive transformers and inference-time retrieval. In: International Conference on Machine Learning. pp. 16990\u201317017. PMLR (2022) 2, 3, 9, 16, 18\n12. Notin, P.M., Van Niekerk, L., Kollasch, A.W., Ritter, D., Gal, Y., Marks, D.: Trancepteve: Combining family-specific and family-agnostic models of protein sequences for improved fitness prediction. bioRxiv pp. 2022\u201312 (2022) 9\n13. Nutschel, C., Fulton, A., Zimmermann, O., Schwaneberg, U., Jaeger, K.E., Gohlke, H.: Systematically scrutinizing the impact of substitution sites on thermostability and detergent tolerance for bacillus subtilis lipase a. Journal of chemical information and modeling 60(3), 1568\u20131584 (2020) 9\n14. Paik, I., Ngo, P.H., Shroff, R., Diaz, D.J., Maranhao, A.C., Walker, D.J., Bhadra, S., Ellington, A.D.: Improved bst dna polymerase variants derived via a machine learning approach. Biochemistry (2021) 3\n15. Pak, M.A., Markhieva, K.A., Novikova, M.S., Petrov, D.S., Vorobyev, I.S., Maksimova, E.S., Kondrashov, F.A., Ivankov, D.N.: Using alphafold to predict the impact of single mutations on protein stability and function. Plos one 18(3), e0282689 (2023) 3\n16. Pancotti, C., Benevenuta, S., Birolo, G., Alberini, V., Repetto, V., Sanavia, T., Capriotti, E., Fariselli, P.: Predicting protein stability changes upon single-point mutation: a thorough comparison of the available tools on a new dataset. Briefings in Bioinformatics 23(2), bbab555 (2022) 2, 8, 16, 17\n17. Pancotti, C., Benevenuta, S., Repetto, V., Birolo, G., Capriotti, E., Sanavia, T., Fariselli, P.: A deep-learning sequence-based method to predict protein stability changes upon genetic variations. Genes 12(6), 911 (2021) 8\n18. Pires, D.E., Ascher, D.B., Blundell, T.L.: Duet: a server for predicting effects of mutations on protein stability using an integrated computational approach. Nucleic acids research 42(W1), W314\u2013W319 (2014) 3, 8\n19. Pires, D.E., Ascher, D.B., Blundell, T.L.: mcsm: predicting the effects of mutations in proteins using graph-based signatures. Bioinformatics 30(3), 335\u2013342 (2014) 8"}]}, {"page": 14, "text": "[58] Pucci, F., Schwersensky, M., Rooman, M.: Artificial intelligence challenges for predicting the impact of\n      mutations on protein stability. Current opinion in structural biology 72, 161\u2013168 (2022) 17\n[59] Qiu, Y., Wei, G.W.: Persistent spectral theory-guided protein engineering. Nature Computational Science\n      pp. 1\u201315 (2023) 3, 6, 17\n[60] Rao, R.M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel, P., Sercu, T., Rives, A.: Msa transformer. In:\n      International Conference on Machine Learning. pp. 8844\u20138856. PMLR (2021) 3, 9, 10\n[61] Riesselman, A.J., Ingraham, J.B., Marks, D.S.: Deep generative models of genetic variation capture the\n      effects of mutations. Nature methods 15(10), 816\u2013822 (2018) 9\n[62] Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., Fergus,\n      R.: Biological structure and function emerge from scaling unsupervised learning to 250 million protein\n      sequences. PNAS (2019). https://doi.org/10.1101/622803 3, 7\n[63] Rodrigues, C.H., Pires, D.E., Ascher, D.B.: Dynamut: predicting the impact of mutations on protein\n      conformation, flexibility and stability. Nucleic acids research 46(W1), W350\u2013W355 (2018) 3, 8\n[64] Rodrigues, C.H., Pires, D.E., Ascher, D.B.: Dynamut2: Assessing changes in stability and flexibility upon\n      single and multiple point missense mutations. Protein Science 30(1), 60\u201369 (2021) 3, 4, 7\n[65] Sanavia, T., Birolo, G., Montanucci, L., Turina, P., Capriotti, E., Fariselli, P.: Limitations and\n      challenges in protein stability prediction upon genome variations: towards future applications in\n      precision medicine. Computational and Structural Biotechnology Journal 18, 1968\u20131979 (2020).\n      https://doi.org/10.1016/j.csbj.2020.07.011, https://doi.org/10.1016/j.csbj.2020.07.011 6, 17\n[66] Savojardo, C., Fariselli, P., Martelli, P.L., Casadio, R.: Inps-md: a web server to predict stability of protein\n     variants from sequence and structure. Bioinformatics 32(16), 2542\u20132544 (2016) 3, 8\n[67] Schymkowitz, J., Borg, J., Stricher, F., Nys, R., Rousseau, F., Serrano, L.: The foldx web server: an online\n      force field. Nucleic acids research 33(suppl_2), W382\u2013W388 (2005) 3, 7, 8, 18\n[68] Shroff, R., Cole, A.W., Diaz, D.J., Morrow, B.R., Donnell, I., Annapareddy, A., Gollihar, J., Ellington,\n     A.D., Thyer, R.: Discovery of novel gain-of-function mutations guided by structure-based deep learning.\n     ACS synthetic biology 9(11), 2927\u20132935 (2020) 3\n[69] Starr, T.N., Thornton, J.W.: Epistasis in protein evolution. Protein science 25(7), 1204\u20131218 (2016) 1\n[70] Steinegger, M., Soding, J.: Mmseqs2 enables sensitive protein sequence searching for the analysis of\n      massive data sets. Nature biotechnology 35(11), 1026\u20131028 (2017) 6\n[71] Stemmer, W.P.: Rapid evolution of a protein in vitro by dna shuffling. Nature 370(6488), 389\u2013391 (1994)\n      2, 3\n[72] Tsuboyama, K., Dauparas, J., Chen, J., Laine, E., Mohseni Behbahani, Y., Weinstein, J.J., Mangan, N.M.,\n      Ovchinnikov, S., Rocklin, G.J.: Mega-scale experimental analysis of protein folding stability in biology\n      and protein design. bioRxiv pp. 2022\u201312 (2022) 1, 2, 6, 16\n[73] Tung, J.W., Heydari, K., Tirouvanziam, R., Sahaf, B., Parks, D.R., Herzenberg, L.A., Herzenberg, L.A.:\n      Modern flow cytometry: a practical approach. Clinics in laboratory medicine 27(3), 453\u2013468 (2007) 1\n[74] Umerenkov, D., Shashkova, T.I., Strashnov, P.V., Nikolaev, F., Sindeeva, M., Ivanisenko, N.V., Kardymon,\n      O.L.: Prostata: Protein stability assessment using transformers. bioRxiv pp. 2022\u201312 (2022) 3, 4, 6, 7, 8,\n     18, 19\n[75] Wittmann, B.J., Johnston, K.E., Wu, Z., Arnold, F.H.: Advances in machine learning for directed evolution.\n      Current opinion in structural biology 69, 11\u201318 (2021) 3\n[76] Wittmann, B.J., Yue, Y., Arnold, F.H.: Informed training set design enables effi       cient machine learning-\n      assisted directed protein evolution. Cell Systems 12(11), 1026\u20131045 (2021) 3\n[77] Worth, C.L., Preissner, R., Blundell, T.L.: Sdm\u2014a server for predicting effects of mutations on protein\n      stability and malfunction. Nucleic acids research 39(suppl_2), W215\u2013W222 (2011) 3, 8\n[78] Wu, S., Snajdrova, R., Moore, J.C., Baldenius, K., Bornscheuer, U.T.: Biocatalysis: Enzymatic Syn-\n      thesis for Industrial Applications. Angewandte Chemie - International Edition 60(1), 88\u2013119 (2021).\n      https://doi.org/10.1002/anie.202006648 1\n                                                         14", "md": "# References\n\n# References\n\n|#|Authors|Title|Journal/Conference|Pages|Year|\n|---|---|---|---|---|---|\n|58|Pucci, F., Schwersensky, M., Rooman, M.|Artificial intelligence challenges for predicting the impact of mutations on protein stability|Current opinion in structural biology|161\u2013168|2022|\n|59|Qiu, Y., Wei, G.W.|Persistent spectral theory-guided protein engineering|Nature Computational Science|1\u201315|2023|\n|60|Rao, R.M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel, P., Sercu, T., Rives, A.|Msa transformer|International Conference on Machine Learning|8844\u20138856|2021|\n|61|Riesselman, A.J., Ingraham, J.B., Marks, D.S.|Deep generative models of genetic variation capture the effects of mutations|Nature methods|816\u2013822|2018|\n|62|Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., Fergus, R.|Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences|PNAS|N/A|2019|\n|63|Rodrigues, C.H., Pires, D.E., Ascher, D.B.|Dynamut: predicting the impact of mutations on protein conformation, flexibility and stability|Nucleic acids research|W350\u2013W355|2018|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "table", "rows": [["#", "Authors", "Title", "Journal/Conference", "Pages", "Year"], ["58", "Pucci, F., Schwersensky, M., Rooman, M.", "Artificial intelligence challenges for predicting the impact of mutations on protein stability", "Current opinion in structural biology", "161\u2013168", "2022"], ["59", "Qiu, Y., Wei, G.W.", "Persistent spectral theory-guided protein engineering", "Nature Computational Science", "1\u201315", "2023"], ["60", "Rao, R.M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel, P., Sercu, T., Rives, A.", "Msa transformer", "International Conference on Machine Learning", "8844\u20138856", "2021"], ["61", "Riesselman, A.J., Ingraham, J.B., Marks, D.S.", "Deep generative models of genetic variation capture the effects of mutations", "Nature methods", "816\u2013822", "2018"], ["62", "Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., Fergus, R.", "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences", "PNAS", "N/A", "2019"], ["63", "Rodrigues, C.H., Pires, D.E., Ascher, D.B.", "Dynamut: predicting the impact of mutations on protein conformation, flexibility and stability", "Nucleic acids research", "W350\u2013W355", "2018"]], "md": "|#|Authors|Title|Journal/Conference|Pages|Year|\n|---|---|---|---|---|---|\n|58|Pucci, F., Schwersensky, M., Rooman, M.|Artificial intelligence challenges for predicting the impact of mutations on protein stability|Current opinion in structural biology|161\u2013168|2022|\n|59|Qiu, Y., Wei, G.W.|Persistent spectral theory-guided protein engineering|Nature Computational Science|1\u201315|2023|\n|60|Rao, R.M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel, P., Sercu, T., Rives, A.|Msa transformer|International Conference on Machine Learning|8844\u20138856|2021|\n|61|Riesselman, A.J., Ingraham, J.B., Marks, D.S.|Deep generative models of genetic variation capture the effects of mutations|Nature methods|816\u2013822|2018|\n|62|Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., Fergus, R.|Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences|PNAS|N/A|2019|\n|63|Rodrigues, C.H., Pires, D.E., Ascher, D.B.|Dynamut: predicting the impact of mutations on protein conformation, flexibility and stability|Nucleic acids research|W350\u2013W355|2018|", "isPerfectTable": true, "csv": "\"#\",\"Authors\",\"Title\",\"Journal/Conference\",\"Pages\",\"Year\"\n\"58\",\"Pucci, F., Schwersensky, M., Rooman, M.\",\"Artificial intelligence challenges for predicting the impact of mutations on protein stability\",\"Current opinion in structural biology\",\"161\u2013168\",\"2022\"\n\"59\",\"Qiu, Y., Wei, G.W.\",\"Persistent spectral theory-guided protein engineering\",\"Nature Computational Science\",\"1\u201315\",\"2023\"\n\"60\",\"Rao, R.M., Liu, J., Verkuil, R., Meier, J., Canny, J., Abbeel, P., Sercu, T., Rives, A.\",\"Msa transformer\",\"International Conference on Machine Learning\",\"8844\u20138856\",\"2021\"\n\"61\",\"Riesselman, A.J., Ingraham, J.B., Marks, D.S.\",\"Deep generative models of genetic variation capture the effects of mutations\",\"Nature methods\",\"816\u2013822\",\"2018\"\n\"62\",\"Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C.L., Ma, J., Fergus, R.\",\"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences\",\"PNAS\",\"N/A\",\"2019\"\n\"63\",\"Rodrigues, C.H., Pires, D.E., Ascher, D.B.\",\"Dynamut: predicting the impact of mutations on protein conformation, flexibility and stability\",\"Nucleic acids research\",\"W350\u2013W355\",\"2018\""}]}, {"page": 15, "text": "[79] Yeung, N., Lin, Y.W., Gao, Y.G., Zhao, X., Russell, B.S., Lei, L., Miner, K.D., Robinson, H., Lu, Y.:\n      Rational design of a structural and functional nitric oxide reductase. Nature 462(7276), 1079\u20131082 (2009)\n     1, 2\n[80] Zhou, B., Lv, O., Yi, K., Xiong, X., Tan, P., Hong, L., Wang, Y.G.: Accurate and definite mutational effect\n      prediction with lightweight equivariant graph neural networks. arXiv preprint arXiv:2304.08299 (2023) 3\n[81] Zhou, Y., Pan, Q., Pires, D.E., Rodrigues, C.H., Ascher, D.B.: Ddmut: predicting effects of mutations on\n      protein stability using deep learning. Nucleic Acids Research p. gkad472 (2023) 3\n                                                         15", "md": "[79] Yeung, N., Lin, Y.W., Gao, Y.G., Zhao, X., Russell, B.S., Lei, L., Miner, K.D., Robinson, H., Lu, Y.:\nRational design of a structural and functional nitric oxide reductase. Nature 462(7276), 1079\u20131082 (2009)\n1, 2\n\n[80] Zhou, B., Lv, O., Yi, K., Xiong, X., Tan, P., Hong, L., Wang, Y.G.: Accurate and definite mutational effect\nprediction with lightweight equivariant graph neural networks. arXiv preprint arXiv:2304.08299 (2023) 3\n\n[81] Zhou, Y., Pan, Q., Pires, D.E., Rodrigues, C.H., Ascher, D.B.: Ddmut: predicting effects of mutations on\nprotein stability using deep learning. Nucleic Acids Research p. gkad472 (2023) 3\n15", "images": [], "items": [{"type": "text", "value": "[79] Yeung, N., Lin, Y.W., Gao, Y.G., Zhao, X., Russell, B.S., Lei, L., Miner, K.D., Robinson, H., Lu, Y.:\nRational design of a structural and functional nitric oxide reductase. Nature 462(7276), 1079\u20131082 (2009)\n1, 2\n\n[80] Zhou, B., Lv, O., Yi, K., Xiong, X., Tan, P., Hong, L., Wang, Y.G.: Accurate and definite mutational effect\nprediction with lightweight equivariant graph neural networks. arXiv preprint arXiv:2304.08299 (2023) 3\n\n[81] Zhou, Y., Pan, Q., Pires, D.E., Rodrigues, C.H., Ascher, D.B.: Ddmut: predicting effects of mutations on\nprotein stability using deep learning. Nucleic Acids Research p. gkad472 (2023) 3\n15", "md": "[79] Yeung, N., Lin, Y.W., Gao, Y.G., Zhao, X., Russell, B.S., Lei, L., Miner, K.D., Robinson, H., Lu, Y.:\nRational design of a structural and functional nitric oxide reductase. Nature 462(7276), 1079\u20131082 (2009)\n1, 2\n\n[80] Zhou, B., Lv, O., Yi, K., Xiong, X., Tan, P., Hong, L., Wang, Y.G.: Accurate and definite mutational effect\nprediction with lightweight equivariant graph neural networks. arXiv preprint arXiv:2304.08299 (2023) 3\n\n[81] Zhou, Y., Pan, Q., Pires, D.E., Rodrigues, C.H., Ascher, D.B.: Ddmut: predicting effects of mutations on\nprotein stability using deep learning. Nucleic Acids Research p. gkad472 (2023) 3\n15"}]}, {"page": 16, "text": "A     Experimental Setting\nWe elaborate on our experimental protocol, including the datasets used for training and testing, the\nevaluation metrics, and the baselines that calibrate our models.\nA.1     Datasets\nA fundamental principle in machine learning is to partition data so that the model is not evaluated on\nthe same data it was trained on. This is essential to assess the model\u2019s ability to generalize beyond its\ntraining data (in our case, to new proteins). Protein sequences are regarded as the same protein when\nthey have large sequence similarity (e.g. > 50%), even when they are not exactly the same string.\nThe impact of training on protein sequences similar to those in the test set has received considerable\nattention [18, 45, 54]. We list the datasets we use below and report the maximum sequence similarity\nof our training set with each test set. This ensures a clear separation between our training and test\nsets.\ncDNA proteolysis [72] is a large scale dataset containing mutant proteins with \u2206\u2206G measurements.\nThe dataset is generated by cDNA display proteolysis, a massively parallel technique that measures\nthermodynamic folding stability for millions of protein variants [72]. The cDNA proteins average\n56.1 amino acids in length with a maximum length of 72 and a minimum length of 30 amino acids.\nThe mean MSA depth is 7797 with a standard deviation of 6282. The maximum depth is 23525 and\nthe minimum depth is 5. The mutations contain either one or two substitutions on a small protein. We\nfollow the train-val split introduced in [18] and additionally filter out proteins in our training set that\nare similar to those in our evaluation datasets. Specifically, we train on 116 proteins with 213,000\ntotal mutations, of which 97,000 are double mutants and 117,000 are single mutants. We hold out\ncDNA2, a validation set of 18 mini-proteins with 22,000 total double mutations. The proteins in our\ntraining set have at most 36% sequence similarity to those in cDNA2. A comprehensive analysis of\nthe dataset, experimental assay, and filtering criteria are found in their paper [1].\nProTherm [48] contains experimentally measured thermodynamic data \u2206\u2206G for protein mutants.\nWe use PTMul, a split containing only higher-order mutations curated by DDGun [44]. There are\n88 proteins with 846 total mutations in PTMul. The proteins in our training set have at most 35%\nsequence similarity to those in PTMul.\nS669 [54] is a manually curated dataset of mutant proteins and their \u2206\u2206G measurements. S669\ncontains mutations with one substitution. There are 94 proteins with 669 total single mutations in\nS669. The proteins in our training set have at most 30% sequence similarity to those in S669.\nProteinGym [49] is a curated set of proteins in which mutations are assessed for some fitness\nphenotype. These fitness phenotypes vary drastically, from cell growth to expression to fluorescence.\nThe fitness values are always normalized so a higher value indicates stronger fi        tness. For classification\nmetrics, the fitness values are binarized by some threshold either defined manually or arbitrarily at\nthe median value. We also consider a subset of the proteins in ProteinGym in which fitness correlates\nhighly with thermodynamic stability. These phenotypes include thermostability, expression, etc.\nThere are 87 proteins totaling over 1.6 million (potentially higher-order) mutations. The proteins in\nour training set have at most 45% sequence similarity to those in ProteinGym.\nT2837 [18] is an aggregate dataset with mutant proteins and their \u2206\u2206G measurements. T2837\ncontains mutations with one substitution. There are 129 proteins with 2837 total single mutations in\nT2837. The proteins in our training set have at most 34% sequence similarity to those in T2837.\nA.2     Metrics\nWe evaluate our model\u2019s ability to assess mutations in ProTherm, S669, and ProteinGym. Additionally,\nwe evaluate our model\u2019s ability to detect stabilizing mutations in cDNA2.\nA.2.1     Mutation Assessment\nThe model predicts the \u2206\u2206G measurement under all mutations with experimental data. The models\nare evaluated by the following regression and classification metrics. The classification metrics are\n                                                        16", "md": "# Experimental Setting\n\n## Experimental Setting\n\nWe elaborate on our experimental protocol, including the datasets used for training and testing, the evaluation metrics, and the baselines that calibrate our models.\n\n### A.1 Datasets\n\nA fundamental principle in machine learning is to partition data so that the model is not evaluated on the same data it was trained on. This is essential to assess the model\u2019s ability to generalize beyond its training data (in our case, to new proteins). Protein sequences are regarded as the same protein when they have large sequence similarity (e.g. &gt; 50%), even when they are not exactly the same string.\n\nThe impact of training on protein sequences similar to those in the test set has received considerable attention [18, 45, 54]. We list the datasets we use below and report the maximum sequence similarity of our training set with each test set. This ensures a clear separation between our training and test sets.\n\n#### cDNA proteolysis [72]\n\ncDNA proteolysis is a large scale dataset containing mutant proteins with $$\\Delta\\Delta G$$ measurements. The dataset is generated by cDNA display proteolysis, a massively parallel technique that measures thermodynamic folding stability for millions of protein variants. The cDNA proteins average 56.1 amino acids in length with a maximum length of 72 and a minimum length of 30 amino acids. The mean MSA depth is 7797 with a standard deviation of 6282. The maximum depth is 23525 and the minimum depth is 5. The mutations contain either one or two substitutions on a small protein. We follow the train-val split introduced in [18] and additionally filter out proteins in our training set that are similar to those in our evaluation datasets. Specifically, we train on 116 proteins with 213,000 total mutations, of which 97,000 are double mutants and 117,000 are single mutants. We hold out cDNA2, a validation set of 18 mini-proteins with 22,000 total double mutations. The proteins in our training set have at most 36% sequence similarity to those in cDNA2. A comprehensive analysis of the dataset, experimental assay, and filtering criteria are found in their paper [1].\n\n#### ProTherm [48]\n\nProTherm contains experimentally measured thermodynamic data $$\\Delta\\Delta G$$ for protein mutants. We use PTMul, a split containing only higher-order mutations curated by DDGun [44]. There are 88 proteins with 846 total mutations in PTMul. The proteins in our training set have at most 35% sequence similarity to those in PTMul.\n\n#### S669 [54]\n\nS669 is a manually curated dataset of mutant proteins and their $$\\Delta\\Delta G$$ measurements. S669 contains mutations with one substitution. There are 94 proteins with 669 total single mutations in S669. The proteins in our training set have at most 30% sequence similarity to those in S669.\n\n#### ProteinGym [49]\n\nProteinGym is a curated set of proteins in which mutations are assessed for some fitness phenotype. These fitness phenotypes vary drastically, from cell growth to expression to fluorescence. The fitness values are always normalized so a higher value indicates stronger fitness. For classification metrics, the fitness values are binarized by some threshold either defined manually or arbitrarily at the median value. We also consider a subset of the proteins in ProteinGym in which fitness correlates highly with thermodynamic stability. These phenotypes include thermostability, expression, etc. There are 87 proteins totaling over 1.6 million (potentially higher-order) mutations. The proteins in our training set have at most 45% sequence similarity to those in ProteinGym.\n\n#### T2837 [18]\n\nT2837 is an aggregate dataset with mutant proteins and their $$\\Delta\\Delta G$$ measurements. T2837 contains mutations with one substitution. There are 129 proteins with 2837 total single mutations in T2837. The proteins in our training set have at most 34% sequence similarity to those in T2837.\n\n### A.2 Metrics\n\nWe evaluate our model\u2019s ability to assess mutations in ProTherm, S669, and ProteinGym. Additionally, we evaluate our model\u2019s ability to detect stabilizing mutations in cDNA2.\n\n#### A.2.1 Mutation Assessment\n\nThe model predicts the $$\\Delta\\Delta G$$ measurement under all mutations with experimental data. The models are evaluated by the following regression and classification metrics. The classification metrics are\n\n|Regression Metrics|Classification Metrics|\n|---|---|\n|Insert regression metrics here|Insert classification metrics here|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Experimental Setting", "md": "# Experimental Setting"}, {"type": "heading", "lvl": 2, "value": "Experimental Setting", "md": "## Experimental Setting"}, {"type": "text", "value": "We elaborate on our experimental protocol, including the datasets used for training and testing, the evaluation metrics, and the baselines that calibrate our models.", "md": "We elaborate on our experimental protocol, including the datasets used for training and testing, the evaluation metrics, and the baselines that calibrate our models."}, {"type": "heading", "lvl": 3, "value": "A.1 Datasets", "md": "### A.1 Datasets"}, {"type": "text", "value": "A fundamental principle in machine learning is to partition data so that the model is not evaluated on the same data it was trained on. This is essential to assess the model\u2019s ability to generalize beyond its training data (in our case, to new proteins). Protein sequences are regarded as the same protein when they have large sequence similarity (e.g. &gt; 50%), even when they are not exactly the same string.\n\nThe impact of training on protein sequences similar to those in the test set has received considerable attention [18, 45, 54]. We list the datasets we use below and report the maximum sequence similarity of our training set with each test set. This ensures a clear separation between our training and test sets.", "md": "A fundamental principle in machine learning is to partition data so that the model is not evaluated on the same data it was trained on. This is essential to assess the model\u2019s ability to generalize beyond its training data (in our case, to new proteins). Protein sequences are regarded as the same protein when they have large sequence similarity (e.g. &gt; 50%), even when they are not exactly the same string.\n\nThe impact of training on protein sequences similar to those in the test set has received considerable attention [18, 45, 54]. We list the datasets we use below and report the maximum sequence similarity of our training set with each test set. This ensures a clear separation between our training and test sets."}, {"type": "heading", "lvl": 4, "value": "cDNA proteolysis [72]", "md": "#### cDNA proteolysis [72]"}, {"type": "text", "value": "cDNA proteolysis is a large scale dataset containing mutant proteins with $$\\Delta\\Delta G$$ measurements. The dataset is generated by cDNA display proteolysis, a massively parallel technique that measures thermodynamic folding stability for millions of protein variants. The cDNA proteins average 56.1 amino acids in length with a maximum length of 72 and a minimum length of 30 amino acids. The mean MSA depth is 7797 with a standard deviation of 6282. The maximum depth is 23525 and the minimum depth is 5. The mutations contain either one or two substitutions on a small protein. We follow the train-val split introduced in [18] and additionally filter out proteins in our training set that are similar to those in our evaluation datasets. Specifically, we train on 116 proteins with 213,000 total mutations, of which 97,000 are double mutants and 117,000 are single mutants. We hold out cDNA2, a validation set of 18 mini-proteins with 22,000 total double mutations. The proteins in our training set have at most 36% sequence similarity to those in cDNA2. A comprehensive analysis of the dataset, experimental assay, and filtering criteria are found in their paper [1].", "md": "cDNA proteolysis is a large scale dataset containing mutant proteins with $$\\Delta\\Delta G$$ measurements. The dataset is generated by cDNA display proteolysis, a massively parallel technique that measures thermodynamic folding stability for millions of protein variants. The cDNA proteins average 56.1 amino acids in length with a maximum length of 72 and a minimum length of 30 amino acids. The mean MSA depth is 7797 with a standard deviation of 6282. The maximum depth is 23525 and the minimum depth is 5. The mutations contain either one or two substitutions on a small protein. We follow the train-val split introduced in [18] and additionally filter out proteins in our training set that are similar to those in our evaluation datasets. Specifically, we train on 116 proteins with 213,000 total mutations, of which 97,000 are double mutants and 117,000 are single mutants. We hold out cDNA2, a validation set of 18 mini-proteins with 22,000 total double mutations. The proteins in our training set have at most 36% sequence similarity to those in cDNA2. A comprehensive analysis of the dataset, experimental assay, and filtering criteria are found in their paper [1]."}, {"type": "heading", "lvl": 4, "value": "ProTherm [48]", "md": "#### ProTherm [48]"}, {"type": "text", "value": "ProTherm contains experimentally measured thermodynamic data $$\\Delta\\Delta G$$ for protein mutants. We use PTMul, a split containing only higher-order mutations curated by DDGun [44]. There are 88 proteins with 846 total mutations in PTMul. The proteins in our training set have at most 35% sequence similarity to those in PTMul.", "md": "ProTherm contains experimentally measured thermodynamic data $$\\Delta\\Delta G$$ for protein mutants. We use PTMul, a split containing only higher-order mutations curated by DDGun [44]. There are 88 proteins with 846 total mutations in PTMul. The proteins in our training set have at most 35% sequence similarity to those in PTMul."}, {"type": "heading", "lvl": 4, "value": "S669 [54]", "md": "#### S669 [54]"}, {"type": "text", "value": "S669 is a manually curated dataset of mutant proteins and their $$\\Delta\\Delta G$$ measurements. S669 contains mutations with one substitution. There are 94 proteins with 669 total single mutations in S669. The proteins in our training set have at most 30% sequence similarity to those in S669.", "md": "S669 is a manually curated dataset of mutant proteins and their $$\\Delta\\Delta G$$ measurements. S669 contains mutations with one substitution. There are 94 proteins with 669 total single mutations in S669. The proteins in our training set have at most 30% sequence similarity to those in S669."}, {"type": "heading", "lvl": 4, "value": "ProteinGym [49]", "md": "#### ProteinGym [49]"}, {"type": "text", "value": "ProteinGym is a curated set of proteins in which mutations are assessed for some fitness phenotype. These fitness phenotypes vary drastically, from cell growth to expression to fluorescence. The fitness values are always normalized so a higher value indicates stronger fitness. For classification metrics, the fitness values are binarized by some threshold either defined manually or arbitrarily at the median value. We also consider a subset of the proteins in ProteinGym in which fitness correlates highly with thermodynamic stability. These phenotypes include thermostability, expression, etc. There are 87 proteins totaling over 1.6 million (potentially higher-order) mutations. The proteins in our training set have at most 45% sequence similarity to those in ProteinGym.", "md": "ProteinGym is a curated set of proteins in which mutations are assessed for some fitness phenotype. These fitness phenotypes vary drastically, from cell growth to expression to fluorescence. The fitness values are always normalized so a higher value indicates stronger fitness. For classification metrics, the fitness values are binarized by some threshold either defined manually or arbitrarily at the median value. We also consider a subset of the proteins in ProteinGym in which fitness correlates highly with thermodynamic stability. These phenotypes include thermostability, expression, etc. There are 87 proteins totaling over 1.6 million (potentially higher-order) mutations. The proteins in our training set have at most 45% sequence similarity to those in ProteinGym."}, {"type": "heading", "lvl": 4, "value": "T2837 [18]", "md": "#### T2837 [18]"}, {"type": "text", "value": "T2837 is an aggregate dataset with mutant proteins and their $$\\Delta\\Delta G$$ measurements. T2837 contains mutations with one substitution. There are 129 proteins with 2837 total single mutations in T2837. The proteins in our training set have at most 34% sequence similarity to those in T2837.", "md": "T2837 is an aggregate dataset with mutant proteins and their $$\\Delta\\Delta G$$ measurements. T2837 contains mutations with one substitution. There are 129 proteins with 2837 total single mutations in T2837. The proteins in our training set have at most 34% sequence similarity to those in T2837."}, {"type": "heading", "lvl": 3, "value": "A.2 Metrics", "md": "### A.2 Metrics"}, {"type": "text", "value": "We evaluate our model\u2019s ability to assess mutations in ProTherm, S669, and ProteinGym. Additionally, we evaluate our model\u2019s ability to detect stabilizing mutations in cDNA2.", "md": "We evaluate our model\u2019s ability to assess mutations in ProTherm, S669, and ProteinGym. Additionally, we evaluate our model\u2019s ability to detect stabilizing mutations in cDNA2."}, {"type": "heading", "lvl": 4, "value": "A.2.1 Mutation Assessment", "md": "#### A.2.1 Mutation Assessment"}, {"type": "text", "value": "The model predicts the $$\\Delta\\Delta G$$ measurement under all mutations with experimental data. The models are evaluated by the following regression and classification metrics. The classification metrics are", "md": "The model predicts the $$\\Delta\\Delta G$$ measurement under all mutations with experimental data. The models are evaluated by the following regression and classification metrics. The classification metrics are"}, {"type": "table", "rows": [["Regression Metrics", "Classification Metrics"], ["Insert regression metrics here", "Insert classification metrics here"]], "md": "|Regression Metrics|Classification Metrics|\n|---|---|\n|Insert regression metrics here|Insert classification metrics here|", "isPerfectTable": true, "csv": "\"Regression Metrics\",\"Classification Metrics\"\n\"Insert regression metrics here\",\"Insert classification metrics here\""}]}, {"page": 17, "text": "useful in applications where it is unnecessary to provide exact \u2206\u2206G and a binary stabilizing or\nde-stabilizing label (or third label, neutral) is sufficient.\nFollowing the literature, we additionally evaluate reverse mutations (in contrast to the standard\n\u201cforward\u201d mutations) where the \u201cto\u201d and \u201cfrom\u201d amino acids are flipped (e.g. P1K to K1P). The\nmodel takes the mutant sequence as input and is expected to output the negation of the original \u2206\u2206G\nvalue when mutating to the original native amino acid.\nSpearman correlation coefficient (rs) evaluates the model\u2019s ability to rank the mutations by their\n\u2206\u2206G values. This metric considers only the relative ordering between mutations and disregards\nthe predicted values. Spearman has its limitations because our datasets contain predominantly\ndestabilizing mutations (\u2206\u2206G > 0). Spearman applied on these datasets overwhelmingly measures\nthe model\u2019s ability to rank destabilizing mutations which does not directly translate to identifying the\nmost stabilizing mutations [8, 58].\nRoot Mean Square Error (RMSE) provides a measure of how closely the predicted measurements\nalign with the true measurements.\nArea Under the receiver operating characteristic Curve (AUC) is a popular classification metric\nthat focuses on positives. The receiver operating characteristic curve is created by plotting precision\nagainst recall at various classification thresholds. Precision is calculated as the ratio of true positives\nto the sum of true positives and false positives. Recall is calculated as the ratio of true positives to the\nsum of true positives and false negatives.\nMatthew correlation coefficient (MCC) evaluates a model\u2019s classification performance under\nimbalanced datasets. It takes into account true positive, true negative, false positive, and false negative\npredictions to measure the overall performance of a model.\nA.2.2    Mutation Detection\nThe regression metrics for mutation assessment are not directly suitable for protein engineering\nwhere the goal is to find stabilizing mutations. In addition to the classification metrics, we evaluate\nour models using per-protein metrics which evaluate the model\u2019s ability to detect a few stabilizing\nmutations from a large number of possible mutations. These metrics are applied to heavily annotated\nproteins (\u223c   1000 \u2206\u2206G measurements) with randomly sampled mutations.\nStabilizing Spearman correlation coefficient rs is the Spearman coefficient calculated on the subset\nof experimentally stabilizing mutations. This mimics the case in protein engineering where there are\ntoo many stabilizing mutations to test in vitro and the goal is to prioritize the most stabilizing ones.\nThis metric is used in [18, 65].\nNormalized Discounted Cumulative Gain (nDCG) [30] quantifies the quality of a ranking among\nmutations by considering both the experimental \u2206\u2206G and predicted rank of each mutation. Specif-\nically, nDCG computes the sum of experimentally stabilizing \u2206\u2206Gs weighted inversely by their\nranks. This metric rewards a model that ranks the best mutations first. We truncate the ranking at\nK = 30 mutations to approximate the number of in-vitro experiments that can typically be run in\nparallel. This metric is found in [59].\nDetection Precision (DetPr) measures the fraction of experimentally stabilizing mutations among\nthe top K = 30 predicted mutations by the model. This simulates the success rate a protein engineer\nmight expect when deploying this model. K is set to approximate the number of in-vitro experiments\nthat can typically be run in parallel.\nA.3    Baselines\nSeveral baselines calibrate performance across datasets. A subset of the critical baselines is introduced\nbriefly (see [54] for more).\nMean is a simple baseline in which the mean measurement is predicted for all mutations. This\nbaseline is not conditioned on the protein or mutation and instead uses only the dataset statistic.\nMultiple Sequence Alignment (MSA) leverages the statistics from a set of similar protein sequences\nto rank mutations. Similar protein sequences are fetched, aligned, and indexed at a mutation position.\n                                                      17", "md": "# Document\n\nuseful in applications where it is unnecessary to provide exact $$\\Delta\\Delta G$$ and a binary stabilizing or de-stabilizing label (or third label, neutral) is sufficient.\n\nFollowing the literature, we additionally evaluate reverse mutations (in contrast to the standard \"forward\" mutations) where the \"to\" and \"from\" amino acids are flipped (e.g. P1K to K1P). The model takes the mutant sequence as input and is expected to output the negation of the original $$\\Delta\\Delta G$$ value when mutating to the original native amino acid.\n\nSpearman correlation coefficient (rs) evaluates the model's ability to rank the mutations by their $$\\Delta\\Delta G$$ values. This metric considers only the relative ordering between mutations and disregards the predicted values. Spearman has its limitations because our datasets contain predominantly destabilizing mutations ($$\\Delta\\Delta G > 0$$). Spearman applied on these datasets overwhelmingly measures the model's ability to rank destabilizing mutations which does not directly translate to identifying the most stabilizing mutations [8, 58].\n\nRoot Mean Square Error (RMSE) provides a measure of how closely the predicted measurements align with the true measurements.\n\nArea Under the receiver operating characteristic Curve (AUC) is a popular classification metric that focuses on positives. The receiver operating characteristic curve is created by plotting precision against recall at various classification thresholds. Precision is calculated as the ratio of true positives to the sum of true positives and false positives. Recall is calculated as the ratio of true positives to the sum of true positives and false negatives.\n\nMatthew correlation coefficient (MCC) evaluates a model's classification performance under imbalanced datasets. It takes into account true positive, true negative, false positive, and false negative predictions to measure the overall performance of a model.\n\n## Mutation Detection\n\nThe regression metrics for mutation assessment are not directly suitable for protein engineering where the goal is to find stabilizing mutations. In addition to the classification metrics, we evaluate our models using per-protein metrics which evaluate the model's ability to detect a few stabilizing mutations from a large number of possible mutations. These metrics are applied to heavily annotated proteins (~1000 $$\\Delta\\Delta G$$ measurements) with randomly sampled mutations.\n\nStabilizing Spearman correlation coefficient rs is the Spearman coefficient calculated on the subset of experimentally stabilizing mutations. This mimics the case in protein engineering where there are too many stabilizing mutations to test in vitro and the goal is to prioritize the most stabilizing ones. This metric is used in [18, 65].\n\nNormalized Discounted Cumulative Gain (nDCG) [30] quantifies the quality of a ranking among mutations by considering both the experimental $$\\Delta\\Delta G$$ and predicted rank of each mutation. Specifically, nDCG computes the sum of experimentally stabilizing $$\\Delta\\Delta G$$s weighted inversely by their ranks. This metric rewards a model that ranks the best mutations first. We truncate the ranking at K = 30 mutations to approximate the number of in-vitro experiments that can typically be run in parallel. This metric is found in [59].\n\nDetection Precision (DetPr) measures the fraction of experimentally stabilizing mutations among the top K = 30 predicted mutations by the model. This simulates the success rate a protein engineer might expect when deploying this model. K is set to approximate the number of in-vitro experiments that can typically be run in parallel.\n\n## Baselines\n\nSeveral baselines calibrate performance across datasets. A subset of the critical baselines is introduced briefly (see [54] for more).\n\nMean is a simple baseline in which the mean measurement is predicted for all mutations. This baseline is not conditioned on the protein or mutation and instead uses only the dataset statistic.\n\nMultiple Sequence Alignment (MSA) leverages the statistics from a set of similar protein sequences to rank mutations. Similar protein sequences are fetched, aligned, and indexed at a mutation position.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "useful in applications where it is unnecessary to provide exact $$\\Delta\\Delta G$$ and a binary stabilizing or de-stabilizing label (or third label, neutral) is sufficient.\n\nFollowing the literature, we additionally evaluate reverse mutations (in contrast to the standard \"forward\" mutations) where the \"to\" and \"from\" amino acids are flipped (e.g. P1K to K1P). The model takes the mutant sequence as input and is expected to output the negation of the original $$\\Delta\\Delta G$$ value when mutating to the original native amino acid.\n\nSpearman correlation coefficient (rs) evaluates the model's ability to rank the mutations by their $$\\Delta\\Delta G$$ values. This metric considers only the relative ordering between mutations and disregards the predicted values. Spearman has its limitations because our datasets contain predominantly destabilizing mutations ($$\\Delta\\Delta G > 0$$). Spearman applied on these datasets overwhelmingly measures the model's ability to rank destabilizing mutations which does not directly translate to identifying the most stabilizing mutations [8, 58].\n\nRoot Mean Square Error (RMSE) provides a measure of how closely the predicted measurements align with the true measurements.\n\nArea Under the receiver operating characteristic Curve (AUC) is a popular classification metric that focuses on positives. The receiver operating characteristic curve is created by plotting precision against recall at various classification thresholds. Precision is calculated as the ratio of true positives to the sum of true positives and false positives. Recall is calculated as the ratio of true positives to the sum of true positives and false negatives.\n\nMatthew correlation coefficient (MCC) evaluates a model's classification performance under imbalanced datasets. It takes into account true positive, true negative, false positive, and false negative predictions to measure the overall performance of a model.", "md": "useful in applications where it is unnecessary to provide exact $$\\Delta\\Delta G$$ and a binary stabilizing or de-stabilizing label (or third label, neutral) is sufficient.\n\nFollowing the literature, we additionally evaluate reverse mutations (in contrast to the standard \"forward\" mutations) where the \"to\" and \"from\" amino acids are flipped (e.g. P1K to K1P). The model takes the mutant sequence as input and is expected to output the negation of the original $$\\Delta\\Delta G$$ value when mutating to the original native amino acid.\n\nSpearman correlation coefficient (rs) evaluates the model's ability to rank the mutations by their $$\\Delta\\Delta G$$ values. This metric considers only the relative ordering between mutations and disregards the predicted values. Spearman has its limitations because our datasets contain predominantly destabilizing mutations ($$\\Delta\\Delta G > 0$$). Spearman applied on these datasets overwhelmingly measures the model's ability to rank destabilizing mutations which does not directly translate to identifying the most stabilizing mutations [8, 58].\n\nRoot Mean Square Error (RMSE) provides a measure of how closely the predicted measurements align with the true measurements.\n\nArea Under the receiver operating characteristic Curve (AUC) is a popular classification metric that focuses on positives. The receiver operating characteristic curve is created by plotting precision against recall at various classification thresholds. Precision is calculated as the ratio of true positives to the sum of true positives and false positives. Recall is calculated as the ratio of true positives to the sum of true positives and false negatives.\n\nMatthew correlation coefficient (MCC) evaluates a model's classification performance under imbalanced datasets. It takes into account true positive, true negative, false positive, and false negative predictions to measure the overall performance of a model."}, {"type": "heading", "lvl": 2, "value": "Mutation Detection", "md": "## Mutation Detection"}, {"type": "text", "value": "The regression metrics for mutation assessment are not directly suitable for protein engineering where the goal is to find stabilizing mutations. In addition to the classification metrics, we evaluate our models using per-protein metrics which evaluate the model's ability to detect a few stabilizing mutations from a large number of possible mutations. These metrics are applied to heavily annotated proteins (~1000 $$\\Delta\\Delta G$$ measurements) with randomly sampled mutations.\n\nStabilizing Spearman correlation coefficient rs is the Spearman coefficient calculated on the subset of experimentally stabilizing mutations. This mimics the case in protein engineering where there are too many stabilizing mutations to test in vitro and the goal is to prioritize the most stabilizing ones. This metric is used in [18, 65].\n\nNormalized Discounted Cumulative Gain (nDCG) [30] quantifies the quality of a ranking among mutations by considering both the experimental $$\\Delta\\Delta G$$ and predicted rank of each mutation. Specifically, nDCG computes the sum of experimentally stabilizing $$\\Delta\\Delta G$$s weighted inversely by their ranks. This metric rewards a model that ranks the best mutations first. We truncate the ranking at K = 30 mutations to approximate the number of in-vitro experiments that can typically be run in parallel. This metric is found in [59].\n\nDetection Precision (DetPr) measures the fraction of experimentally stabilizing mutations among the top K = 30 predicted mutations by the model. This simulates the success rate a protein engineer might expect when deploying this model. K is set to approximate the number of in-vitro experiments that can typically be run in parallel.", "md": "The regression metrics for mutation assessment are not directly suitable for protein engineering where the goal is to find stabilizing mutations. In addition to the classification metrics, we evaluate our models using per-protein metrics which evaluate the model's ability to detect a few stabilizing mutations from a large number of possible mutations. These metrics are applied to heavily annotated proteins (~1000 $$\\Delta\\Delta G$$ measurements) with randomly sampled mutations.\n\nStabilizing Spearman correlation coefficient rs is the Spearman coefficient calculated on the subset of experimentally stabilizing mutations. This mimics the case in protein engineering where there are too many stabilizing mutations to test in vitro and the goal is to prioritize the most stabilizing ones. This metric is used in [18, 65].\n\nNormalized Discounted Cumulative Gain (nDCG) [30] quantifies the quality of a ranking among mutations by considering both the experimental $$\\Delta\\Delta G$$ and predicted rank of each mutation. Specifically, nDCG computes the sum of experimentally stabilizing $$\\Delta\\Delta G$$s weighted inversely by their ranks. This metric rewards a model that ranks the best mutations first. We truncate the ranking at K = 30 mutations to approximate the number of in-vitro experiments that can typically be run in parallel. This metric is found in [59].\n\nDetection Precision (DetPr) measures the fraction of experimentally stabilizing mutations among the top K = 30 predicted mutations by the model. This simulates the success rate a protein engineer might expect when deploying this model. K is set to approximate the number of in-vitro experiments that can typically be run in parallel."}, {"type": "heading", "lvl": 2, "value": "Baselines", "md": "## Baselines"}, {"type": "text", "value": "Several baselines calibrate performance across datasets. A subset of the critical baselines is introduced briefly (see [54] for more).\n\nMean is a simple baseline in which the mean measurement is predicted for all mutations. This baseline is not conditioned on the protein or mutation and instead uses only the dataset statistic.\n\nMultiple Sequence Alignment (MSA) leverages the statistics from a set of similar protein sequences to rank mutations. Similar protein sequences are fetched, aligned, and indexed at a mutation position.", "md": "Several baselines calibrate performance across datasets. A subset of the critical baselines is introduced briefly (see [54] for more).\n\nMean is a simple baseline in which the mean measurement is predicted for all mutations. This baseline is not conditioned on the protein or mutation and instead uses only the dataset statistic.\n\nMultiple Sequence Alignment (MSA) leverages the statistics from a set of similar protein sequences to rank mutations. Similar protein sequences are fetched, aligned, and indexed at a mutation position."}]}, {"page": 18, "text": "This yields an amino acid frequency count normalized into a probability distribution. The score is\ndefined as the likelihood ratio between the mutated and native amino acid types. While calibrated for\na given position and protein, it may not be well-calibrated across proteins.\nESM [38] also computes the likelihood ratio between a mutated and native amino acid type [42].\nUnlike MSA, the probability distribution is decoded from ESM2 [38], a model pre-trained on masked\namino acid prediction. Like MSA, ESM may not be well-calibrated across proteins.\nDDGun [44] uses a linear combination of scores derived from evolutionary and structural features to\ncompute \u2206\u2206G.\nFoldX [67] leverages an empirical force field to estimate the change in stability. It models the Van\nDer Waals interactions, solvation and hydrogen bonding interactions, changes in entropy, and other\nphysical values.\nPROSTATA [74] is a sequence model for stability prediction built on the ESM2 model. It takes\nnative and mutant protein sequences as input and produces a \u2206\u2206G value.\nStability Oracle [18] is a structure-based graph-transformer model for stability prediction.\nTranception [49] is a specialized autoregressive transformer that uses grouped attention to encourage\nspecialization across heads.\nB     Additional Results\nB.1    Higher-order Mutation Results on PTMul Breakdown\nProTherm Multiple (PTMul) contains 846 mutations, of which 536 are double mutations and 310\nmutations with more than two mutations. We breakdown the performance on PTMul into double\nmutations and higher-order mutations (greater than 2) in Table 6. While our model performs similarly\nagainst the additive baseline on double mutations, Mutate Everything outperforms the additive\nbaseline on higher-order mutations.\n                                          Double Mutations                        More Mutations\n   Method                           rs    AUC      MCC       RMSE \u2193        rs    AUC      MCC       RMSE \u2193\n   DDGun                          0.28     0.63     0.22       2.23      0.15     0.57     0.12       2.19\n   DDGun3D                        0.29     0.61     0.17       2.25      0.19     0.60     0.17       2.20\n   Additive Baseline              0.52     0.76     0.30       1.95      0.49     0.79     0.45       2.14\n   Mutate Everything (ours)       0.50     0.75     0.34       2.08      0.60     0.86     0.58       1.99\nTable 6: Results on PTMul splits containing double mutations and more than 2 mutations.\nMutate Everything outperforms existing methods, especially when there are multiple mutations.\n                                                       18", "md": "This yields an amino acid frequency count normalized into a probability distribution. The score is defined as the likelihood ratio between the mutated and native amino acid types. While calibrated for a given position and protein, it may not be well-calibrated across proteins.\n\nESM [38] also computes the likelihood ratio between a mutated and native amino acid type [42]. Unlike MSA, the probability distribution is decoded from ESM2 [38], a model pre-trained on masked amino acid prediction. Like MSA, ESM may not be well-calibrated across proteins.\n\nDDGun [44] uses a linear combination of scores derived from evolutionary and structural features to compute $$\\Delta\\Delta G$$.\n\nFoldX [67] leverages an empirical force field to estimate the change in stability. It models the Van Der Waals interactions, solvation and hydrogen bonding interactions, changes in entropy, and other physical values.\n\nPROSTATA [74] is a sequence model for stability prediction built on the ESM2 model. It takes native and mutant protein sequences as input and produces a $$\\Delta\\Delta G$$ value.\n\nStability Oracle [18] is a structure-based graph-transformer model for stability prediction.\n\nTranception [49] is a specialized autoregressive transformer that uses grouped attention to encourage specialization across heads.\n\n### Additional Results\n\n#### Higher-order Mutation Results on PTMul Breakdown\n\nProTherm Multiple (PTMul) contains 846 mutations, of which 536 are double mutations and 310 mutations with more than two mutations. We breakdown the performance on PTMul into double mutations and higher-order mutations (greater than 2) in Table 6. While our model performs similarly against the additive baseline on double mutations, Mutate Everything outperforms the additive baseline on higher-order mutations.\n\n| Method              | rs   | AUC  | MCC  | RMSE \u2193 | rs   | AUC  | MCC  | RMSE \u2193 |\n|---------------------|------|------|------|--------|------|------|------|--------|\n| DDGun               | 0.28 | 0.63 | 0.22 | 2.23   | 0.15 | 0.57 | 0.12 | 2.19   |\n| DDGun3D             | 0.29 | 0.61 | 0.17 | 2.25   | 0.19 | 0.60 | 0.17 | 2.20   |\n| Additive Baseline   | 0.52 | 0.76 | 0.30 | 1.95   | 0.49 | 0.79 | 0.45 | 2.14   |\n| Mutate Everything   | 0.50 | 0.75 | 0.34 | 2.08   | 0.60 | 0.86 | 0.58 | 1.99   |\n\nTable 6: Results on PTMul splits containing double mutations and more than 2 mutations.\n\nMutate Everything outperforms existing methods, especially when there are multiple mutations.", "images": [], "items": [{"type": "text", "value": "This yields an amino acid frequency count normalized into a probability distribution. The score is defined as the likelihood ratio between the mutated and native amino acid types. While calibrated for a given position and protein, it may not be well-calibrated across proteins.\n\nESM [38] also computes the likelihood ratio between a mutated and native amino acid type [42]. Unlike MSA, the probability distribution is decoded from ESM2 [38], a model pre-trained on masked amino acid prediction. Like MSA, ESM may not be well-calibrated across proteins.\n\nDDGun [44] uses a linear combination of scores derived from evolutionary and structural features to compute $$\\Delta\\Delta G$$.\n\nFoldX [67] leverages an empirical force field to estimate the change in stability. It models the Van Der Waals interactions, solvation and hydrogen bonding interactions, changes in entropy, and other physical values.\n\nPROSTATA [74] is a sequence model for stability prediction built on the ESM2 model. It takes native and mutant protein sequences as input and produces a $$\\Delta\\Delta G$$ value.\n\nStability Oracle [18] is a structure-based graph-transformer model for stability prediction.\n\nTranception [49] is a specialized autoregressive transformer that uses grouped attention to encourage specialization across heads.", "md": "This yields an amino acid frequency count normalized into a probability distribution. The score is defined as the likelihood ratio between the mutated and native amino acid types. While calibrated for a given position and protein, it may not be well-calibrated across proteins.\n\nESM [38] also computes the likelihood ratio between a mutated and native amino acid type [42]. Unlike MSA, the probability distribution is decoded from ESM2 [38], a model pre-trained on masked amino acid prediction. Like MSA, ESM may not be well-calibrated across proteins.\n\nDDGun [44] uses a linear combination of scores derived from evolutionary and structural features to compute $$\\Delta\\Delta G$$.\n\nFoldX [67] leverages an empirical force field to estimate the change in stability. It models the Van Der Waals interactions, solvation and hydrogen bonding interactions, changes in entropy, and other physical values.\n\nPROSTATA [74] is a sequence model for stability prediction built on the ESM2 model. It takes native and mutant protein sequences as input and produces a $$\\Delta\\Delta G$$ value.\n\nStability Oracle [18] is a structure-based graph-transformer model for stability prediction.\n\nTranception [49] is a specialized autoregressive transformer that uses grouped attention to encourage specialization across heads."}, {"type": "heading", "lvl": 3, "value": "Additional Results", "md": "### Additional Results"}, {"type": "heading", "lvl": 4, "value": "Higher-order Mutation Results on PTMul Breakdown", "md": "#### Higher-order Mutation Results on PTMul Breakdown"}, {"type": "text", "value": "ProTherm Multiple (PTMul) contains 846 mutations, of which 536 are double mutations and 310 mutations with more than two mutations. We breakdown the performance on PTMul into double mutations and higher-order mutations (greater than 2) in Table 6. While our model performs similarly against the additive baseline on double mutations, Mutate Everything outperforms the additive baseline on higher-order mutations.", "md": "ProTherm Multiple (PTMul) contains 846 mutations, of which 536 are double mutations and 310 mutations with more than two mutations. We breakdown the performance on PTMul into double mutations and higher-order mutations (greater than 2) in Table 6. While our model performs similarly against the additive baseline on double mutations, Mutate Everything outperforms the additive baseline on higher-order mutations."}, {"type": "table", "rows": [["Method", "rs", "AUC", "MCC", "RMSE \u2193", "rs", "AUC", "MCC", "RMSE \u2193"], ["DDGun", "0.28", "0.63", "0.22", "2.23", "0.15", "0.57", "0.12", "2.19"], ["DDGun3D", "0.29", "0.61", "0.17", "2.25", "0.19", "0.60", "0.17", "2.20"], ["Additive Baseline", "0.52", "0.76", "0.30", "1.95", "0.49", "0.79", "0.45", "2.14"], ["Mutate Everything", "0.50", "0.75", "0.34", "2.08", "0.60", "0.86", "0.58", "1.99"]], "md": "| Method              | rs   | AUC  | MCC  | RMSE \u2193 | rs   | AUC  | MCC  | RMSE \u2193 |\n|---------------------|------|------|------|--------|------|------|------|--------|\n| DDGun               | 0.28 | 0.63 | 0.22 | 2.23   | 0.15 | 0.57 | 0.12 | 2.19   |\n| DDGun3D             | 0.29 | 0.61 | 0.17 | 2.25   | 0.19 | 0.60 | 0.17 | 2.20   |\n| Additive Baseline   | 0.52 | 0.76 | 0.30 | 1.95   | 0.49 | 0.79 | 0.45 | 2.14   |\n| Mutate Everything   | 0.50 | 0.75 | 0.34 | 2.08   | 0.60 | 0.86 | 0.58 | 1.99   |", "isPerfectTable": true, "csv": "\"Method\",\"rs\",\"AUC\",\"MCC\",\"RMSE \u2193\",\"rs\",\"AUC\",\"MCC\",\"RMSE \u2193\"\n\"DDGun\",\"0.28\",\"0.63\",\"0.22\",\"2.23\",\"0.15\",\"0.57\",\"0.12\",\"2.19\"\n\"DDGun3D\",\"0.29\",\"0.61\",\"0.17\",\"2.25\",\"0.19\",\"0.60\",\"0.17\",\"2.20\"\n\"Additive Baseline\",\"0.52\",\"0.76\",\"0.30\",\"1.95\",\"0.49\",\"0.79\",\"0.45\",\"2.14\"\n\"Mutate Everything\",\"0.50\",\"0.75\",\"0.34\",\"2.08\",\"0.60\",\"0.86\",\"0.58\",\"1.99\""}, {"type": "text", "value": "Table 6: Results on PTMul splits containing double mutations and more than 2 mutations.\n\nMutate Everything outperforms existing methods, especially when there are multiple mutations.", "md": "Table 6: Results on PTMul splits containing double mutations and more than 2 mutations.\n\nMutate Everything outperforms existing methods, especially when there are multiple mutations."}]}, {"page": 19, "text": "                                                                 Forward                                             Reverse\n   Method                                        rs       AUC         MCC         RMSE \u2193             rs      AUC         MCC         RMSE \u2193\n   PROSTATA-IFML [18, 74]                      0.53        0.75        0.31          1.75          0.53       0.75        0.32          1.75\n   Stability Oracle [18]                       0.62        0.81        0.39          1.65          0.62       0.81        0.39          1.65\n   Mutate Everything (Ours)                    0.65        0.79        0.39          1.59          0.52       0.72        0.28          1.75\n Table 7: Comparisons on Single Mutation Thermodynamic Stability prediction in T2837. We\n report regression and classification metrics on the forward and reverse datasets. rs refers to Spearman\n correlation coefficient. Our training proteins have at most 34% sequence similarity with those in\n T2837. Mutate Everything is state-of-the-art on T2837 forward.\n       Test Set               #Mutations         rp         rs       RMSE \u2193       MCC        AUROC        Precision     Recall     Accuracy\n       T2837 Orig             2835            0.6274      0.6472      1.5877      0.3888      0.7956       0.5615       0.4791      0.7894\n       T2837 TR               2835            0.5301      0.5208      1.7563      0.2802      0.7229       0.8297       0.8572      0.7517\n       T2837 Orig + TR        5670            0.7229      0.7282      1.6741      0.5482      0.8433       0.7765       0.7701      0.7705\n Table 8: Mutate Everything regression and classification metrics on T2837 and its reverse datasets.\n TR refers to Thermodynamic Reversibility. rp is Pearson correlation coeffi                                        cient. rs is Spearman\n correlation coefficient. Mutations that failed our data pipeline are excluded.\n B.2      Single Mutation Results on T2837\nWe include our results on the newly proposed T2837 dataset in Table 7 [18]. Mutate Everything\n achieves a Spearman correlation of 0.65 compared to 0.62 of Stability Oracle, outperforming existing\n methods on the forward dataset. PROSTATA-IFML [18] fine-tunes PROSTATA [74] on the cDNA\n proteolysis dataset for more direct comparison. Mutate Everything performs competitively with other\n sequence models on the reverse dataset even without training on reverse mutations. We show our\n performance on additional metrics for T2837 in Table 8.\n                                                                        19", "md": "|Method|rs|AUC|MCC|RMSE \u2193|rs|AUC|MCC|RMSE \u2193|\n|---|---|---|---|---|---|---|---|---|\n|PROSTATA-IFML [18, 74]|0.53|0.75|0.31|1.75|0.53|0.75|0.32|1.75|\n|Stability Oracle [18]|0.62|0.81|0.39|1.65|0.62|0.81|0.39|1.65|\n|Mutate Everything (Ours)|0.65|0.79|0.39|1.59|0.52|0.72|0.28|1.75|\n\nTable 7: Comparisons on Single Mutation Thermodynamic Stability prediction in T2837. We report regression and classification metrics on the forward and reverse datasets. rs refers to Spearman correlation coefficient. Our training proteins have at most 34% sequence similarity with those in T2837. Mutate Everything is state-of-the-art on T2837 forward.\n\n|Test Set|#Mutations|rp|rs|RMSE \u2193|MCC|AUROC|Precision|Recall|Accuracy|\n|---|---|---|---|---|---|---|---|---|---|\n|T2837 Orig|2835|0.6274|0.6472|1.5877|0.3888|0.7956|0.5615|0.4791|0.7894|\n|T2837 TR|2835|0.5301|0.5208|1.7563|0.2802|0.7229|0.8297|0.8572|0.7517|\n|T2837 Orig + TR|5670|0.7229|0.7282|1.6741|0.5482|0.8433|0.7765|0.7701|0.7705|\n\nTable 8: Mutate Everything regression and classification metrics on T2837 and its reverse datasets. TR refers to Thermodynamic Reversibility. rp is Pearson correlation coefficient. rs is Spearman correlation coefficient. Mutations that failed our data pipeline are excluded.\n\nB.2 Single Mutation Results on T2837\n\nWe include our results on the newly proposed T2837 dataset in Table 7 [18]. Mutate Everything achieves a Spearman correlation of 0.65 compared to 0.62 of Stability Oracle, outperforming existing methods on the forward dataset. PROSTATA-IFML [18] fine-tunes PROSTATA [74] on the cDNA proteolysis dataset for more direct comparison. Mutate Everything performs competitively with other sequence models on the reverse dataset even without training on reverse mutations. We show our performance on additional metrics for T2837 in Table 8.", "images": [], "items": [{"type": "table", "rows": [["Method", "rs", "AUC", "MCC", "RMSE \u2193", "rs", "AUC", "MCC", "RMSE \u2193"], ["PROSTATA-IFML [18, 74]", "0.53", "0.75", "0.31", "1.75", "0.53", "0.75", "0.32", "1.75"], ["Stability Oracle [18]", "0.62", "0.81", "0.39", "1.65", "0.62", "0.81", "0.39", "1.65"], ["Mutate Everything (Ours)", "0.65", "0.79", "0.39", "1.59", "0.52", "0.72", "0.28", "1.75"]], "md": "|Method|rs|AUC|MCC|RMSE \u2193|rs|AUC|MCC|RMSE \u2193|\n|---|---|---|---|---|---|---|---|---|\n|PROSTATA-IFML [18, 74]|0.53|0.75|0.31|1.75|0.53|0.75|0.32|1.75|\n|Stability Oracle [18]|0.62|0.81|0.39|1.65|0.62|0.81|0.39|1.65|\n|Mutate Everything (Ours)|0.65|0.79|0.39|1.59|0.52|0.72|0.28|1.75|", "isPerfectTable": true, "csv": "\"Method\",\"rs\",\"AUC\",\"MCC\",\"RMSE \u2193\",\"rs\",\"AUC\",\"MCC\",\"RMSE \u2193\"\n\"PROSTATA-IFML [18, 74]\",\"0.53\",\"0.75\",\"0.31\",\"1.75\",\"0.53\",\"0.75\",\"0.32\",\"1.75\"\n\"Stability Oracle [18]\",\"0.62\",\"0.81\",\"0.39\",\"1.65\",\"0.62\",\"0.81\",\"0.39\",\"1.65\"\n\"Mutate Everything (Ours)\",\"0.65\",\"0.79\",\"0.39\",\"1.59\",\"0.52\",\"0.72\",\"0.28\",\"1.75\""}, {"type": "text", "value": "Table 7: Comparisons on Single Mutation Thermodynamic Stability prediction in T2837. We report regression and classification metrics on the forward and reverse datasets. rs refers to Spearman correlation coefficient. Our training proteins have at most 34% sequence similarity with those in T2837. Mutate Everything is state-of-the-art on T2837 forward.", "md": "Table 7: Comparisons on Single Mutation Thermodynamic Stability prediction in T2837. We report regression and classification metrics on the forward and reverse datasets. rs refers to Spearman correlation coefficient. Our training proteins have at most 34% sequence similarity with those in T2837. Mutate Everything is state-of-the-art on T2837 forward."}, {"type": "table", "rows": [["Test Set", "#Mutations", "rp", "rs", "RMSE \u2193", "MCC", "AUROC", "Precision", "Recall", "Accuracy"], ["T2837 Orig", "2835", "0.6274", "0.6472", "1.5877", "0.3888", "0.7956", "0.5615", "0.4791", "0.7894"], ["T2837 TR", "2835", "0.5301", "0.5208", "1.7563", "0.2802", "0.7229", "0.8297", "0.8572", "0.7517"], ["T2837 Orig + TR", "5670", "0.7229", "0.7282", "1.6741", "0.5482", "0.8433", "0.7765", "0.7701", "0.7705"]], "md": "|Test Set|#Mutations|rp|rs|RMSE \u2193|MCC|AUROC|Precision|Recall|Accuracy|\n|---|---|---|---|---|---|---|---|---|---|\n|T2837 Orig|2835|0.6274|0.6472|1.5877|0.3888|0.7956|0.5615|0.4791|0.7894|\n|T2837 TR|2835|0.5301|0.5208|1.7563|0.2802|0.7229|0.8297|0.8572|0.7517|\n|T2837 Orig + TR|5670|0.7229|0.7282|1.6741|0.5482|0.8433|0.7765|0.7701|0.7705|", "isPerfectTable": true, "csv": "\"Test Set\",\"#Mutations\",\"rp\",\"rs\",\"RMSE \u2193\",\"MCC\",\"AUROC\",\"Precision\",\"Recall\",\"Accuracy\"\n\"T2837 Orig\",\"2835\",\"0.6274\",\"0.6472\",\"1.5877\",\"0.3888\",\"0.7956\",\"0.5615\",\"0.4791\",\"0.7894\"\n\"T2837 TR\",\"2835\",\"0.5301\",\"0.5208\",\"1.7563\",\"0.2802\",\"0.7229\",\"0.8297\",\"0.8572\",\"0.7517\"\n\"T2837 Orig + TR\",\"5670\",\"0.7229\",\"0.7282\",\"1.6741\",\"0.5482\",\"0.8433\",\"0.7765\",\"0.7701\",\"0.7705\""}, {"type": "text", "value": "Table 8: Mutate Everything regression and classification metrics on T2837 and its reverse datasets. TR refers to Thermodynamic Reversibility. rp is Pearson correlation coefficient. rs is Spearman correlation coefficient. Mutations that failed our data pipeline are excluded.\n\nB.2 Single Mutation Results on T2837\n\nWe include our results on the newly proposed T2837 dataset in Table 7 [18]. Mutate Everything achieves a Spearman correlation of 0.65 compared to 0.62 of Stability Oracle, outperforming existing methods on the forward dataset. PROSTATA-IFML [18] fine-tunes PROSTATA [74] on the cDNA proteolysis dataset for more direct comparison. Mutate Everything performs competitively with other sequence models on the reverse dataset even without training on reverse mutations. We show our performance on additional metrics for T2837 in Table 8.", "md": "Table 8: Mutate Everything regression and classification metrics on T2837 and its reverse datasets. TR refers to Thermodynamic Reversibility. rp is Pearson correlation coefficient. rs is Spearman correlation coefficient. Mutations that failed our data pipeline are excluded.\n\nB.2 Single Mutation Results on T2837\n\nWe include our results on the newly proposed T2837 dataset in Table 7 [18]. Mutate Everything achieves a Spearman correlation of 0.65 compared to 0.62 of Stability Oracle, outperforming existing methods on the forward dataset. PROSTATA-IFML [18] fine-tunes PROSTATA [74] on the cDNA proteolysis dataset for more direct comparison. Mutate Everything performs competitively with other sequence models on the reverse dataset even without training on reverse mutations. We show our performance on additional metrics for T2837 in Table 8."}]}], "job_id": "8a4ae534-8838-4177-b23a-979e56bc517c", "file_path": "./corpus/2310.12979.pdf"}