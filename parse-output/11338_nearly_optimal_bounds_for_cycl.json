{"pages": [{"page": 1, "text": "        Nearly Optimal Bounds for Cyclic Forgetting\n                    Halyun Jeong                                      Mark Kong\n        University of California Los Angeles             University of California Los Angeles\n               hajeong@math.ucla.edu                             markkong@ucla.edu\n                      Deanna Needell                              William Swartworth\n           University of California Los Angeles                Carnegie Mellon University\n                  deanna@math.ucla.edu                         wswartwo@andrew.cmu.edu\n                                             Rachel Ward\n                                     University of Texas at Austin\n                                       rward@math.utexas.edu\n                                              Abstract\n         We provide theoretical bounds on the forgetting quantity in the continual learning\n         setting for linear tasks, where each round of learning corresponds to projecting\n         onto a linear subspace. For a cyclic task ordering on T tasks repeated m times each,\n         we prove the best known upper bound of O(T 2/m) on the forgetting. Notably, our\n         bound holds uniformly over all choices of tasks and is independent of the ambient\n         dimension. Our main technical contribution is a characterization of the union of\n         all numerical ranges of products of T (real or complex) projections as a sinusoidal\n         spiral, which may be of independent interest.\n1    Introduction\nWhile machine learning often focuses on learning from a static dataset, one is often interested in\nacquiring data on the fly and adapting to the present context. This is known throughout the literature\nas \u201ccontinual learning\u201d or \u201clifelong learning\u201d [Wan+23; CL18; Par+19; TM95].\nOne challenge of continual learning is \u201ccatastrophic forgetting\u201d [Had+20; VT19; Kem+18]: A model\nmay learn useful information in context A but then lose much of this knowledge when adapting to\ncontext B. If context A is not representative of future contexts, then this is a correct adjustment for\ndistribution shift. However, if contexts similar to A arise repeatedly, this may be undesirable..\nBuilding on prior work, we aim to better understand the catastrophic forgetting phenomenon when\nfaced with a series of linear regression problems that are revisited in cyclic order.\nIn machine learning, many data sets display cyclic or periodic patterns. These patterns often arise\nfrom factors such as the \u201cday of the week effect\u201d [QR22] observed in financial or search engine data\n[Yan+22]. Additionally, the methods of cyclic alternating projections by Von Neumann [Von49] and\nHalperin [Hal62] are well-studied methods for solving linear systems [Kar37]. Equivalently, one can\nthink of this as studying residual bounds for cyclic block Kaczmarz-type algorithms. Our work can\nbe thought of as studying the worst-case forgetting of these popular methods. [Evr+22] mentions\nthis perspective, but leaves open the problem of obtaining tight convergence bounds. In particular,\nthe dimension dependence on the ambient data in the bounds of [Evr+22] could be problematic\nsince high-dimensional data is so ubiquitous in machine learning. In contrast, our new bound for the\nworst-case forgetting does not scale at all with the dimension of the ambient data.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Research Paper\n\n# Nearly Optimal Bounds for Cyclic Forgetting\n\nHalyun Jeong University of California Los Angeles - hajeong@math.ucla.edu\n\nMark Kong University of California Los Angeles - markkong@ucla.edu\n\nDeanna Needell University of California Los Angeles - deanna@math.ucla.edu\n\nWilliam Swartworth Carnegie Mellon University - wswartwo@andrew.cmu.edu\n\nRachel Ward University of Texas at Austin - rward@math.utexas.edu\n\n## Abstract\n\nWe provide theoretical bounds on the forgetting quantity in the continual learning setting for linear tasks, where each round of learning corresponds to projecting onto a linear subspace. For a cyclic task ordering on T tasks repeated m times each, we prove the best known upper bound of $$O(T^2/m)$$ on the forgetting. Notably, our bound holds uniformly over all choices of tasks and is independent of the ambient dimension. Our main technical contribution is a characterization of the union of all numerical ranges of products of T (real or complex) projections as a sinusoidal spiral, which may be of independent interest.\n\n## Introduction\n\nWhile machine learning often focuses on learning from a static dataset, one is often interested in acquiring data on the fly and adapting to the present context. This is known throughout the literature as \u201ccontinual learning\u201d or \u201clifelong learning\u201d [Wan+23; CL18; Par+19; TM95].\n\nOne challenge of continual learning is \u201ccatastrophic forgetting\u201d [Had+20; VT19; Kem+18]: A model may learn useful information in context A but then lose much of this knowledge when adapting to context B. If context A is not representative of future contexts, then this is a correct adjustment for distribution shift. However, if contexts similar to A arise repeatedly, this may be undesirable.\n\nBuilding on prior work, we aim to better understand the catastrophic forgetting phenomenon when faced with a series of linear regression problems that are revisited in cyclic order.\n\nIn machine learning, many data sets display cyclic or periodic patterns. These patterns often arise from factors such as the \u201cday of the week effect\u201d [QR22] observed in financial or search engine data [Yan+22]. Additionally, the methods of cyclic alternating projections by Von Neumann [Von49] and Halperin [Hal62] are well-studied methods for solving linear systems [Kar37]. Equivalently, one can think of this as studying residual bounds for cyclic block Kaczmarz-type algorithms. Our work can be thought of as studying the worst-case forgetting of these popular methods. [Evr+22] mentions this perspective, but leaves open the problem of obtaining tight convergence bounds. In particular, the dimension dependence on the ambient data in the bounds of [Evr+22] could be problematic since high-dimensional data is so ubiquitous in machine learning. In contrast, our new bound for the worst-case forgetting does not scale at all with the dimension of the ambient data.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Paper", "md": "# Research Paper"}, {"type": "heading", "lvl": 1, "value": "Nearly Optimal Bounds for Cyclic Forgetting", "md": "# Nearly Optimal Bounds for Cyclic Forgetting"}, {"type": "text", "value": "Halyun Jeong University of California Los Angeles - hajeong@math.ucla.edu\n\nMark Kong University of California Los Angeles - markkong@ucla.edu\n\nDeanna Needell University of California Los Angeles - deanna@math.ucla.edu\n\nWilliam Swartworth Carnegie Mellon University - wswartwo@andrew.cmu.edu\n\nRachel Ward University of Texas at Austin - rward@math.utexas.edu", "md": "Halyun Jeong University of California Los Angeles - hajeong@math.ucla.edu\n\nMark Kong University of California Los Angeles - markkong@ucla.edu\n\nDeanna Needell University of California Los Angeles - deanna@math.ucla.edu\n\nWilliam Swartworth Carnegie Mellon University - wswartwo@andrew.cmu.edu\n\nRachel Ward University of Texas at Austin - rward@math.utexas.edu"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "We provide theoretical bounds on the forgetting quantity in the continual learning setting for linear tasks, where each round of learning corresponds to projecting onto a linear subspace. For a cyclic task ordering on T tasks repeated m times each, we prove the best known upper bound of $$O(T^2/m)$$ on the forgetting. Notably, our bound holds uniformly over all choices of tasks and is independent of the ambient dimension. Our main technical contribution is a characterization of the union of all numerical ranges of products of T (real or complex) projections as a sinusoidal spiral, which may be of independent interest.", "md": "We provide theoretical bounds on the forgetting quantity in the continual learning setting for linear tasks, where each round of learning corresponds to projecting onto a linear subspace. For a cyclic task ordering on T tasks repeated m times each, we prove the best known upper bound of $$O(T^2/m)$$ on the forgetting. Notably, our bound holds uniformly over all choices of tasks and is independent of the ambient dimension. Our main technical contribution is a characterization of the union of all numerical ranges of products of T (real or complex) projections as a sinusoidal spiral, which may be of independent interest."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "While machine learning often focuses on learning from a static dataset, one is often interested in acquiring data on the fly and adapting to the present context. This is known throughout the literature as \u201ccontinual learning\u201d or \u201clifelong learning\u201d [Wan+23; CL18; Par+19; TM95].\n\nOne challenge of continual learning is \u201ccatastrophic forgetting\u201d [Had+20; VT19; Kem+18]: A model may learn useful information in context A but then lose much of this knowledge when adapting to context B. If context A is not representative of future contexts, then this is a correct adjustment for distribution shift. However, if contexts similar to A arise repeatedly, this may be undesirable.\n\nBuilding on prior work, we aim to better understand the catastrophic forgetting phenomenon when faced with a series of linear regression problems that are revisited in cyclic order.\n\nIn machine learning, many data sets display cyclic or periodic patterns. These patterns often arise from factors such as the \u201cday of the week effect\u201d [QR22] observed in financial or search engine data [Yan+22]. Additionally, the methods of cyclic alternating projections by Von Neumann [Von49] and Halperin [Hal62] are well-studied methods for solving linear systems [Kar37]. Equivalently, one can think of this as studying residual bounds for cyclic block Kaczmarz-type algorithms. Our work can be thought of as studying the worst-case forgetting of these popular methods. [Evr+22] mentions this perspective, but leaves open the problem of obtaining tight convergence bounds. In particular, the dimension dependence on the ambient data in the bounds of [Evr+22] could be problematic since high-dimensional data is so ubiquitous in machine learning. In contrast, our new bound for the worst-case forgetting does not scale at all with the dimension of the ambient data.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "While machine learning often focuses on learning from a static dataset, one is often interested in acquiring data on the fly and adapting to the present context. This is known throughout the literature as \u201ccontinual learning\u201d or \u201clifelong learning\u201d [Wan+23; CL18; Par+19; TM95].\n\nOne challenge of continual learning is \u201ccatastrophic forgetting\u201d [Had+20; VT19; Kem+18]: A model may learn useful information in context A but then lose much of this knowledge when adapting to context B. If context A is not representative of future contexts, then this is a correct adjustment for distribution shift. However, if contexts similar to A arise repeatedly, this may be undesirable.\n\nBuilding on prior work, we aim to better understand the catastrophic forgetting phenomenon when faced with a series of linear regression problems that are revisited in cyclic order.\n\nIn machine learning, many data sets display cyclic or periodic patterns. These patterns often arise from factors such as the \u201cday of the week effect\u201d [QR22] observed in financial or search engine data [Yan+22]. Additionally, the methods of cyclic alternating projections by Von Neumann [Von49] and Halperin [Hal62] are well-studied methods for solving linear systems [Kar37]. Equivalently, one can think of this as studying residual bounds for cyclic block Kaczmarz-type algorithms. Our work can be thought of as studying the worst-case forgetting of these popular methods. [Evr+22] mentions this perspective, but leaves open the problem of obtaining tight convergence bounds. In particular, the dimension dependence on the ambient data in the bounds of [Evr+22] could be problematic since high-dimensional data is so ubiquitous in machine learning. In contrast, our new bound for the worst-case forgetting does not scale at all with the dimension of the ambient data.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "1.1    Problem Setup\nThe learning algorithm we consider fits a linear map to a set of data points of the form (\u20d7                      x, y) with\n\u20d7\nx \u2208  Rd, y \u2208    R.\nOur data is split into \u201ctasks\u201d, which are lists of elements from our dataset (with repeats allowed).\nThese tasks are fed to our learning algorithm one at a time. Our algorithm updates its prediction after\nbeing given each task.\nWe assemble the rt inputs of the tth task into a matrix Xt \u2208               Rrt,d, where each row is an input vector\n\u20d7\nxT , and their corresponding outputs into a column vector \u20d7                yt \u2208    Rrt. We aim to to find a column\nvector \u20d7w \u2208   Rd such that Xt \u20d7    w = \u20d7 yt for all t.\nThe learning algorithm we consider applies a block Kaczmarz update at each step. Explicitly, we\ninitialize \u20d7w0 \u2208     Rd anywhere, and for t \u2208           Z\u22651 we set \u20d7    wt to be the projection of \u20d7       wt\u22121 onto the\nsolution space Pt for the tth regression problem. That is,\n                                       \u20d7\n                                       wt+1 = \u20d7   wt + X+  t+1(\u20d7 yt+1 \u2212    Xt+1 \u20d7 wt)\nwhere + denotes the Moore-Penrose pseudoinverse.\nGeometrically, the set of vectors \u20d7      w \u2208    Rd such that the map Rd \u2192           R: \u20d7 x  \u2192  \u20d7\n                                                                                               xT \u20d7w agrees with all the\ndata in task t is an affine subspace Pt. This algorithm obtains wt+1 by projecting wt onto Pt+1. The\nset of vectors in Rd satisfying all tasks is P :=  t Pt. The algorithm never leaves the maximal affine\nhyperplane through \u20d7      w0 perpendicular to P, so if the wt converge to a solution then it converges to\nthe solution closest to w0..\nIt is well-known that running gradient descent to interpolation with quadratic loss on task t starting\nfrom wt produces wt+1.\nTypically, projecting onto the solution space for task t moves \u20d7                wi\u22121 off the solution space for task\nt \u2212  1. We wish to understand the amount of information \u201cforgotten\u201d over the course of training.\nFollowing [Evr+22], we define the forgetting for a sequence of tasks and initialization S at iteration\nn as                                      FS(n) := 1        n  \u2225Xt \u20d7wn \u2212    \u20d7\n                                                       n  t=1               yt\u22252 .\nThat is, FS(n) is the average loss of wn on all previous tasks (including task n), where each task is\nweighted equally regardless of how many data points it has. As mentioned in the introduction, large\nforgetting is not always undesirable, but \u201cforgetting\u201d in humans is not always undesirable either, so\nthe name \u201cforgetting\u201d not imply an inappropriately negative connotation.\nIn this work, we focus on cyclic task orderings. That is, we have T tasks (X1, \u20d7                  y1), . . . , (XT , \u20d7\n                                                                                                                    yT ) we\nvisit cyclically, i.e., Xt = Xt mod T +1 and \u20d7         yt = \u20d7 yt mod T +1 for all t. Then the sequence of iterates\nconverges to a cycle of length T in the sense that, for any t \u2208              Z\u22650, wt, wt+T , wt+2T , . . . converges,\neither linearly or after a finite number of iterations; We justify this in the supplementary material.\nIn particular, if there is at least one simultaneous solution to all tasks, then the limit must be a solution;\nWe justify a generalization of this in the supplmentary material, so we get linear convergence to a\nsolution which implies linear convergence of forgetting.\nHowever, this does not guarantee that, for any collection of T tasks in m cycles, the forgetting can\nbe bounded above by O(c\u2212m) for some constant c, even after normalizing the tasks in the way we\ndescribe later: The choice of tasks that give the largest forgetting for one value of m may not be the\ntasks that give the largest forgetting for another value of m. Here we ask whether we can obtain a\nworst-case bound on FS(mT) that only depends on the number of tasks T and the number of cycles\nm.\nWe focus on bounding the forgetting after a whole number of cycles; The forgetting in the middle of\na cycle is a weighted average of the losses at each task, and therefore can be treated in the same way\nas getting a bound without normalizing the tasks. For a quick bound, the weights are close to 1, so\nwe may rescale the tasks to get all coefficients to be 1 or less without increasing our bound much.\nThe forgetting for T tasks after m cycles is\n                              1    mT   \u2225Xt \u20d7wmT \u2212     \u20d7              T\n                                                       yt\u22252 = 1           \u2225Xt \u20d7wmT \u2212      \u20d7\n                             mT    t=1                            T  t=1                  yt\u22252 .                          (1)\n                                                              2", "md": "## 1.1 Problem Setup\n\nThe learning algorithm we consider fits a linear map to a set of data points of the form ($$\\vec{x}, y$$) with $$\\vec{x} \\in \\mathbb{R}^d$$, $$y \\in \\mathbb{R}$$. Our data is split into \u201ctasks\u201d, which are lists of elements from our dataset (with repeats allowed). These tasks are fed to our learning algorithm one at a time. Our algorithm updates its prediction after being given each task.\n\nWe assemble the $$rt$$ inputs of the $$t$$th task into a matrix $$X_t \\in \\mathbb{R}^{rt \\times d}$$, where each row is an input vector $$\\vec{x}^T$$, and their corresponding outputs into a column vector $$\\vec{y}_t \\in \\mathbb{R}^{rt}$$. We aim to find a column vector $$\\vec{w} \\in \\mathbb{R}^d$$ such that $$X_t\\vec{w} = \\vec{y}_t$$ for all $$t$$.\n\nThe learning algorithm we consider applies a block Kaczmarz update at each step. Explicitly, we initialize $$\\vec{w}_0 \\in \\mathbb{R}^d$$ anywhere, and for $$t \\in \\mathbb{Z}_{\\geq 1}$$ we set $$\\vec{w}_t$$ to be the projection of $$\\vec{w}_{t-1}$$ onto the solution space $$P_t$$ for the $$t$$th regression problem. That is,\n\n$$\\vec{w}_{t+1} = \\vec{w}_t + X^+_{t+1}(\\vec{y}_{t+1} - X_{t+1}\\vec{w}_t)$$\n\nwhere $$+$$ denotes the Moore-Penrose pseudoinverse.\n\nGeometrically, the set of vectors $$\\vec{w} \\in \\mathbb{R}^d$$ such that the map $$\\mathbb{R}^d \\rightarrow \\mathbb{R} : \\vec{x} \\rightarrow \\vec{x}^T\\vec{w}$$ agrees with all the data in task $$t$$ is an affine subspace $$P_t$$. This algorithm obtains $$\\vec{w}_{t+1}$$ by projecting $$\\vec{w}_t$$ onto $$P_{t+1}$$. The set of vectors in $$\\mathbb{R}^d$$ satisfying all tasks is $$P := \\sum_t P_t$$. The algorithm never leaves the maximal affine hyperplane through $$\\vec{w}_0$$ perpendicular to $$P$$, so if the $$\\vec{w}_t$$ converge to a solution then it converges to the solution closest to $$\\vec{w}_0$$.\n\nIt is well-known that running gradient descent to interpolation with quadratic loss on task $$t$$ starting from $$\\vec{w}_t$$ produces $$\\vec{w}_{t+1}$$. Typically, projecting onto the solution space for task $$t$$ moves $$\\vec{w}_{i-1}$$ off the solution space for task $$t - 1$$. We wish to understand the amount of information \u201cforgotten\u201d over the course of training.\n\nFollowing [Evr+22], we define the forgetting for a sequence of tasks and initialization $$S$$ at iteration $$n$$ as\n\n$$F_S(n) := \\frac{1}{n} \\sum_{t=1}^{n} \\lVert X_t\\vec{w}_n - \\vec{y}_t \\rVert^2$$\n\nThat is, $$F_S(n)$$ is the average loss of $$\\vec{w}_n$$ on all previous tasks (including task $$n$$), where each task is weighted equally regardless of how many data points it has. As mentioned in the introduction, large forgetting is not always undesirable, but \u201cforgetting\u201d in humans is not always undesirable either, so the name \u201cforgetting\u201d does not imply an inappropriately negative connotation.\n\nIn this work, we focus on cyclic task orderings. That is, we have $$T$$ tasks $$(X_1, \\vec{y}_1), . . . , (X_T, \\vec{y}_T)$$ we visit cyclically, i.e., $$X_t = X_{t \\mod T + 1}$$ and $$\\vec{y}_t = \\vec{y}_{t \\mod T + 1}$$ for all $$t$$. Then the sequence of iterates converges to a cycle of length $$T$$ in the sense that, for any $$t \\in \\mathbb{Z}_{\\geq 0}$$, $$\\vec{w}_t, \\vec{w}_{t+T}, \\vec{w}_{t+2T}, . . .$$ converges, either linearly or after a finite number of iterations; We justify this in the supplementary material.\n\nIn particular, if there is at least one simultaneous solution to all tasks, then the limit must be a solution; We justify a generalization of this in the supplmentary material, so we get linear convergence to a solution which implies linear convergence of forgetting.\n\nHowever, this does not guarantee that, for any collection of $$T$$ tasks in $$m$$ cycles, the forgetting can be bounded above by $$O(c^{-m})$$ for some constant $$c$$, even after normalizing the tasks in the way we describe later: The choice of tasks that give the largest forgetting for one value of $$m$$ may not be the tasks that give the largest forgetting for another value of $$m$$. Here we ask whether we can obtain a worst-case bound on $$F_S(mT)$$ that only depends on the number of tasks $$T$$ and the number of cycles $$m$$.\n\nWe focus on bounding the forgetting after a whole number of cycles; The forgetting in the middle of a cycle is a weighted average of the losses at each task, and therefore can be treated in the same way as getting a bound without normalizing the tasks. For a quick bound, the weights are close to 1, so we may rescale the tasks to get all coefficients to be 1 or less without increasing our bound much. The forgetting for $$T$$ tasks after $$m$$ cycles is\n\n$$\\frac{1}{mT} \\sum_{t=1}^{mT} \\lVert X_t\\vec{w}_{mT} - \\vec{y}_t \\rVert^2 = \\frac{1}{mT} \\sum_{t=1}^{T} \\sum_{t=1}^{m} \\lVert X_t\\vec{w}_{mT} - \\vec{y}_t \\rVert^2$$\n\n(1)", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "1.1 Problem Setup", "md": "## 1.1 Problem Setup"}, {"type": "text", "value": "The learning algorithm we consider fits a linear map to a set of data points of the form ($$\\vec{x}, y$$) with $$\\vec{x} \\in \\mathbb{R}^d$$, $$y \\in \\mathbb{R}$$. Our data is split into \u201ctasks\u201d, which are lists of elements from our dataset (with repeats allowed). These tasks are fed to our learning algorithm one at a time. Our algorithm updates its prediction after being given each task.\n\nWe assemble the $$rt$$ inputs of the $$t$$th task into a matrix $$X_t \\in \\mathbb{R}^{rt \\times d}$$, where each row is an input vector $$\\vec{x}^T$$, and their corresponding outputs into a column vector $$\\vec{y}_t \\in \\mathbb{R}^{rt}$$. We aim to find a column vector $$\\vec{w} \\in \\mathbb{R}^d$$ such that $$X_t\\vec{w} = \\vec{y}_t$$ for all $$t$$.\n\nThe learning algorithm we consider applies a block Kaczmarz update at each step. Explicitly, we initialize $$\\vec{w}_0 \\in \\mathbb{R}^d$$ anywhere, and for $$t \\in \\mathbb{Z}_{\\geq 1}$$ we set $$\\vec{w}_t$$ to be the projection of $$\\vec{w}_{t-1}$$ onto the solution space $$P_t$$ for the $$t$$th regression problem. That is,\n\n$$\\vec{w}_{t+1} = \\vec{w}_t + X^+_{t+1}(\\vec{y}_{t+1} - X_{t+1}\\vec{w}_t)$$\n\nwhere $$+$$ denotes the Moore-Penrose pseudoinverse.\n\nGeometrically, the set of vectors $$\\vec{w} \\in \\mathbb{R}^d$$ such that the map $$\\mathbb{R}^d \\rightarrow \\mathbb{R} : \\vec{x} \\rightarrow \\vec{x}^T\\vec{w}$$ agrees with all the data in task $$t$$ is an affine subspace $$P_t$$. This algorithm obtains $$\\vec{w}_{t+1}$$ by projecting $$\\vec{w}_t$$ onto $$P_{t+1}$$. The set of vectors in $$\\mathbb{R}^d$$ satisfying all tasks is $$P := \\sum_t P_t$$. The algorithm never leaves the maximal affine hyperplane through $$\\vec{w}_0$$ perpendicular to $$P$$, so if the $$\\vec{w}_t$$ converge to a solution then it converges to the solution closest to $$\\vec{w}_0$$.\n\nIt is well-known that running gradient descent to interpolation with quadratic loss on task $$t$$ starting from $$\\vec{w}_t$$ produces $$\\vec{w}_{t+1}$$. Typically, projecting onto the solution space for task $$t$$ moves $$\\vec{w}_{i-1}$$ off the solution space for task $$t - 1$$. We wish to understand the amount of information \u201cforgotten\u201d over the course of training.\n\nFollowing [Evr+22], we define the forgetting for a sequence of tasks and initialization $$S$$ at iteration $$n$$ as\n\n$$F_S(n) := \\frac{1}{n} \\sum_{t=1}^{n} \\lVert X_t\\vec{w}_n - \\vec{y}_t \\rVert^2$$\n\nThat is, $$F_S(n)$$ is the average loss of $$\\vec{w}_n$$ on all previous tasks (including task $$n$$), where each task is weighted equally regardless of how many data points it has. As mentioned in the introduction, large forgetting is not always undesirable, but \u201cforgetting\u201d in humans is not always undesirable either, so the name \u201cforgetting\u201d does not imply an inappropriately negative connotation.\n\nIn this work, we focus on cyclic task orderings. That is, we have $$T$$ tasks $$(X_1, \\vec{y}_1), . . . , (X_T, \\vec{y}_T)$$ we visit cyclically, i.e., $$X_t = X_{t \\mod T + 1}$$ and $$\\vec{y}_t = \\vec{y}_{t \\mod T + 1}$$ for all $$t$$. Then the sequence of iterates converges to a cycle of length $$T$$ in the sense that, for any $$t \\in \\mathbb{Z}_{\\geq 0}$$, $$\\vec{w}_t, \\vec{w}_{t+T}, \\vec{w}_{t+2T}, . . .$$ converges, either linearly or after a finite number of iterations; We justify this in the supplementary material.\n\nIn particular, if there is at least one simultaneous solution to all tasks, then the limit must be a solution; We justify a generalization of this in the supplmentary material, so we get linear convergence to a solution which implies linear convergence of forgetting.\n\nHowever, this does not guarantee that, for any collection of $$T$$ tasks in $$m$$ cycles, the forgetting can be bounded above by $$O(c^{-m})$$ for some constant $$c$$, even after normalizing the tasks in the way we describe later: The choice of tasks that give the largest forgetting for one value of $$m$$ may not be the tasks that give the largest forgetting for another value of $$m$$. Here we ask whether we can obtain a worst-case bound on $$F_S(mT)$$ that only depends on the number of tasks $$T$$ and the number of cycles $$m$$.\n\nWe focus on bounding the forgetting after a whole number of cycles; The forgetting in the middle of a cycle is a weighted average of the losses at each task, and therefore can be treated in the same way as getting a bound without normalizing the tasks. For a quick bound, the weights are close to 1, so we may rescale the tasks to get all coefficients to be 1 or less without increasing our bound much. The forgetting for $$T$$ tasks after $$m$$ cycles is\n\n$$\\frac{1}{mT} \\sum_{t=1}^{mT} \\lVert X_t\\vec{w}_{mT} - \\vec{y}_t \\rVert^2 = \\frac{1}{mT} \\sum_{t=1}^{T} \\sum_{t=1}^{m} \\lVert X_t\\vec{w}_{mT} - \\vec{y}_t \\rVert^2$$\n\n(1)", "md": "The learning algorithm we consider fits a linear map to a set of data points of the form ($$\\vec{x}, y$$) with $$\\vec{x} \\in \\mathbb{R}^d$$, $$y \\in \\mathbb{R}$$. Our data is split into \u201ctasks\u201d, which are lists of elements from our dataset (with repeats allowed). These tasks are fed to our learning algorithm one at a time. Our algorithm updates its prediction after being given each task.\n\nWe assemble the $$rt$$ inputs of the $$t$$th task into a matrix $$X_t \\in \\mathbb{R}^{rt \\times d}$$, where each row is an input vector $$\\vec{x}^T$$, and their corresponding outputs into a column vector $$\\vec{y}_t \\in \\mathbb{R}^{rt}$$. We aim to find a column vector $$\\vec{w} \\in \\mathbb{R}^d$$ such that $$X_t\\vec{w} = \\vec{y}_t$$ for all $$t$$.\n\nThe learning algorithm we consider applies a block Kaczmarz update at each step. Explicitly, we initialize $$\\vec{w}_0 \\in \\mathbb{R}^d$$ anywhere, and for $$t \\in \\mathbb{Z}_{\\geq 1}$$ we set $$\\vec{w}_t$$ to be the projection of $$\\vec{w}_{t-1}$$ onto the solution space $$P_t$$ for the $$t$$th regression problem. That is,\n\n$$\\vec{w}_{t+1} = \\vec{w}_t + X^+_{t+1}(\\vec{y}_{t+1} - X_{t+1}\\vec{w}_t)$$\n\nwhere $$+$$ denotes the Moore-Penrose pseudoinverse.\n\nGeometrically, the set of vectors $$\\vec{w} \\in \\mathbb{R}^d$$ such that the map $$\\mathbb{R}^d \\rightarrow \\mathbb{R} : \\vec{x} \\rightarrow \\vec{x}^T\\vec{w}$$ agrees with all the data in task $$t$$ is an affine subspace $$P_t$$. This algorithm obtains $$\\vec{w}_{t+1}$$ by projecting $$\\vec{w}_t$$ onto $$P_{t+1}$$. The set of vectors in $$\\mathbb{R}^d$$ satisfying all tasks is $$P := \\sum_t P_t$$. The algorithm never leaves the maximal affine hyperplane through $$\\vec{w}_0$$ perpendicular to $$P$$, so if the $$\\vec{w}_t$$ converge to a solution then it converges to the solution closest to $$\\vec{w}_0$$.\n\nIt is well-known that running gradient descent to interpolation with quadratic loss on task $$t$$ starting from $$\\vec{w}_t$$ produces $$\\vec{w}_{t+1}$$. Typically, projecting onto the solution space for task $$t$$ moves $$\\vec{w}_{i-1}$$ off the solution space for task $$t - 1$$. We wish to understand the amount of information \u201cforgotten\u201d over the course of training.\n\nFollowing [Evr+22], we define the forgetting for a sequence of tasks and initialization $$S$$ at iteration $$n$$ as\n\n$$F_S(n) := \\frac{1}{n} \\sum_{t=1}^{n} \\lVert X_t\\vec{w}_n - \\vec{y}_t \\rVert^2$$\n\nThat is, $$F_S(n)$$ is the average loss of $$\\vec{w}_n$$ on all previous tasks (including task $$n$$), where each task is weighted equally regardless of how many data points it has. As mentioned in the introduction, large forgetting is not always undesirable, but \u201cforgetting\u201d in humans is not always undesirable either, so the name \u201cforgetting\u201d does not imply an inappropriately negative connotation.\n\nIn this work, we focus on cyclic task orderings. That is, we have $$T$$ tasks $$(X_1, \\vec{y}_1), . . . , (X_T, \\vec{y}_T)$$ we visit cyclically, i.e., $$X_t = X_{t \\mod T + 1}$$ and $$\\vec{y}_t = \\vec{y}_{t \\mod T + 1}$$ for all $$t$$. Then the sequence of iterates converges to a cycle of length $$T$$ in the sense that, for any $$t \\in \\mathbb{Z}_{\\geq 0}$$, $$\\vec{w}_t, \\vec{w}_{t+T}, \\vec{w}_{t+2T}, . . .$$ converges, either linearly or after a finite number of iterations; We justify this in the supplementary material.\n\nIn particular, if there is at least one simultaneous solution to all tasks, then the limit must be a solution; We justify a generalization of this in the supplmentary material, so we get linear convergence to a solution which implies linear convergence of forgetting.\n\nHowever, this does not guarantee that, for any collection of $$T$$ tasks in $$m$$ cycles, the forgetting can be bounded above by $$O(c^{-m})$$ for some constant $$c$$, even after normalizing the tasks in the way we describe later: The choice of tasks that give the largest forgetting for one value of $$m$$ may not be the tasks that give the largest forgetting for another value of $$m$$. Here we ask whether we can obtain a worst-case bound on $$F_S(mT)$$ that only depends on the number of tasks $$T$$ and the number of cycles $$m$$.\n\nWe focus on bounding the forgetting after a whole number of cycles; The forgetting in the middle of a cycle is a weighted average of the losses at each task, and therefore can be treated in the same way as getting a bound without normalizing the tasks. For a quick bound, the weights are close to 1, so we may rescale the tasks to get all coefficients to be 1 or less without increasing our bound much. The forgetting for $$T$$ tasks after $$m$$ cycles is\n\n$$\\frac{1}{mT} \\sum_{t=1}^{mT} \\lVert X_t\\vec{w}_{mT} - \\vec{y}_t \\rVert^2 = \\frac{1}{mT} \\sum_{t=1}^{T} \\sum_{t=1}^{m} \\lVert X_t\\vec{w}_{mT} - \\vec{y}_t \\rVert^2$$\n\n(1)"}]}, {"page": 3, "text": "as each of the T tasks is visited equally often.\nThis can be made arbitrarily large by scaling the data points, so we describe normalizations in the\nnext section. Given these normalizations, [Evr+22] showed that such a worst-case bound is possible,\nbut the bounds they provided are not tight. Here, we improve on the forgetting bounds from [Evr+22]\nand, as a byproduct, show the union of the numerical ranges of all products of T orthogonal (real or\ncomplex) projections forms a sinusoidal spiral.\n1.1.1        Normalizations\nFollowing [Evr+22], we restrict to the setting where P is nonempty (i.e., the union of all tasks\nis consistent with a single linear function) and let \u20d7                              w\u2217     \u2208   P  . So every data point (\u20d7               x, y) satisfies\n\u20d7\nxT \u20d7 w\u2217   = y, and the planes Pi we project onto all pass through \u20d7                                 w\u2217.\nUnlike [Evr+22], we translate the setup so w\u2217                              = 0. This modifies each data point (\u20d7                      x, y) by setting\ny = 0 but preserving \u20d7            x. This translates the Pi to pass through 0, so they are subspaces, and Pi is the\nkernel of Xi, and the loss of task t at \u20d7                   w is now \u2225Xt \u20d7        w\u22252, turning forgetting into.\n                                        FS(n) = 1            n                                  n   X   t    ( \u20d7     2\n                                                        n   t=1  \u2225Xt \u20d7  w\u22252 = 1      n       \u00d7t=1             w)\nwhere the product is the categorical product. Thus, for a fixed collection of tasks X1, X2, . . . , Xn,\nabusing notation to let Pt denote the projection onto the plane Pt, the largest forgetting over all\n                                 \uf8eb     n        \uf8f6     n           2\ninitializations \u20d7      w0 is     \uf8ed   \u00d7t=1   Xt  \uf8f8n   t=1  Pt        .\nRescaling \u20d7      w0 rescales all iterates, and hence the forgetting, by the same amount. So we normalize\n\u2225 \u20d7\n  w0\u2225     = 1. Similarly, rescaling all tasks Xt by the same scalar scales each term in the forgetting by\nthat factor, so we normalize so that maxt\u2225Xt\u2225                              = 1.\nAs \u2225    \u20d7\n       wt\u2225    decreases in t, this implies the loss of \u20d7                 wn at task t is \u2225Xt \u20d7         wn\u22252 \u2264        1, and hence the forgetting\nis at most 1.\nIn the cyclic setting, for fixed T and m, these normalizations make the domain of optimization (the\nspace of T       -tuples of tasks with maxt\u2225Xt\u2225                     \u2264   1 and initializations \u20d7         w0 \u2208     Sd\u22121) compact, so forgetting\nattains a maximum, which is what we bound in this paper.\nIn the supplementary, we describe a tighter normalization that gives the same forgetting bounds.\n1.2       Observations\nIn this section, we describe some observations that help us get our bound.\nAs noted in the previous section, using our normalization, there is a trivial bound on the forgetting of\n1.\nThe data matrices Xt only affect the iterates \u20d7                        wi via their rowspan (i.e., the span of the data points in\nthe task), so we may replace Xi by any other data matrix with the same rowspan, or equivalently the\nsame kernel, without affecting the iterates subject to our normalization constraint. As \u2225Xi\u2225                                                        \u2264    1, it\nis a contraction, so the norm of the image of any \u20d7                             w is at most its distance to the kernel, and this is\nrealized when Xi is a projection away from its kernel. Therefore, the worst-case bound is not affected\nby restricting Xi to be orthogonal projections with kernel Pi.\nWith this restriction, for any \u20d7             w, \u2225Xi \u20d7    w\u2225    is the distance from \u20d7          w to Pi. Therefore, the forgetting can be\ninterpreted as the average distance to each Pi over all previous tasks (weighted equally).\n1.3      Prior Work\nBounds on forgetting.                   [Evr+22] considered forgetting bounds both for random task orderings and\ncyclic task orderings. However their bound for cyclic task orderings is not tight. After m cycles of T\n                                                                              3", "md": "# Math Equations and Text\n\nAs each of the T tasks is visited equally often. This can be made arbitrarily large by scaling the data points, so we describe normalizations in the next section. Given these normalizations, [Evr+22] showed that such a worst-case bound is possible, but the bounds they provided are not tight. Here, we improve on the forgetting bounds from [Evr+22] and, as a byproduct, show the union of the numerical ranges of all products of T orthogonal (real or complex) projections forms a sinusoidal spiral.\n\n### 1.1.1 Normalizations\n\nFollowing [Evr+22], we restrict to the setting where P is nonempty (i.e., the union of all tasks is consistent with a single linear function) and let $\\vec{w}^*$ &in; P. So every data point ($\\vec{x}, y$) satisfies $\\vec{x}^T \\vec{w}^* = y$, and the planes Pi we project onto all pass through $\\vec{w}^*$.\n\nUnlike [Evr+22], we translate the setup so $\\vec{w}^* = 0$. This modifies each data point ($\\vec{x}, y$) by setting $y = 0$ but preserving $\\vec{x}$. This translates the Pi to pass through 0, so they are subspaces, and Pi is the kernel of Xi, and the loss of task t at $\\vec{w}$ is now $\\|X_t \\vec{w}\\|^2$, turning forgetting into:\n\n$FS(n) = \\frac{1}{n} \\sum_{t=1}^{n} \\|X_t \\vec{w}\\|^2 = \\frac{1}{n} \\times \\prod_{t=1}^{n} w$\n\nwhere the product is the categorical product. Thus, for a fixed collection of tasks X1, X2, ..., Xn, abusing notation to let Pt denote the projection onto the plane Pt, the largest forgetting over all initializations $\\vec{w}_0$ is:\n\n$\\left(\\prod_{t=1}^{n} X_t\\right)^n \\times \\prod_{t=1}^{n} P_t$\n\nRescaling $\\vec{w}_0$ rescales all iterates, and hence the forgetting, by the same amount. So we normalize $\\|\\vec{w}_0\\| = 1$. Similarly, rescaling all tasks Xt by the same scalar scales each term in the forgetting by that factor, so we normalize so that $\\max_t \\|X_t\\| = 1$.\n\nAs $\\|\\vec{w}_t\\|$ decreases in t, this implies the loss of $\\vec{w}_n$ at task t is $\\|X_t \\vec{w}_n\\|^2 \\leq 1$, and hence the forgetting is at most 1.\n\n### 1.2 Observations\n\nIn this section, we describe some observations that help us get our bound. As noted in the previous section, using our normalization, there is a trivial bound on the forgetting of 1. The data matrices Xt only affect the iterates $\\vec{w}_i$ via their rowspan (i.e., the span of the data points in the task), so we may replace Xi by any other data matrix with the same rowspan, or equivalently the same kernel, without affecting the iterates subject to our normalization constraint. As $\\|X_i\\| \\leq 1$, it is a contraction, so the norm of the image of any $\\vec{w}$ is at most its distance to the kernel, and this is realized when Xi is a projection away from its kernel. Therefore, the worst-case bound is not affected by restricting Xi to be orthogonal projections with kernel Pi. With this restriction, for any $\\vec{w}$, $\\|X_i \\vec{w}\\|$ is the distance from $\\vec{w}$ to Pi. Therefore, the forgetting can be interpreted as the average distance to each Pi over all previous tasks (weighted equally).\n\n### 1.3 Prior Work\n\nBounds on forgetting. [Evr+22] considered forgetting bounds both for random task orderings and cyclic task orderings. However their bound for cyclic task orderings is not tight. After m cycles of T", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "As each of the T tasks is visited equally often. This can be made arbitrarily large by scaling the data points, so we describe normalizations in the next section. Given these normalizations, [Evr+22] showed that such a worst-case bound is possible, but the bounds they provided are not tight. Here, we improve on the forgetting bounds from [Evr+22] and, as a byproduct, show the union of the numerical ranges of all products of T orthogonal (real or complex) projections forms a sinusoidal spiral.", "md": "As each of the T tasks is visited equally often. This can be made arbitrarily large by scaling the data points, so we describe normalizations in the next section. Given these normalizations, [Evr+22] showed that such a worst-case bound is possible, but the bounds they provided are not tight. Here, we improve on the forgetting bounds from [Evr+22] and, as a byproduct, show the union of the numerical ranges of all products of T orthogonal (real or complex) projections forms a sinusoidal spiral."}, {"type": "heading", "lvl": 3, "value": "1.1.1 Normalizations", "md": "### 1.1.1 Normalizations"}, {"type": "text", "value": "Following [Evr+22], we restrict to the setting where P is nonempty (i.e., the union of all tasks is consistent with a single linear function) and let $\\vec{w}^*$ &in; P. So every data point ($\\vec{x}, y$) satisfies $\\vec{x}^T \\vec{w}^* = y$, and the planes Pi we project onto all pass through $\\vec{w}^*$.\n\nUnlike [Evr+22], we translate the setup so $\\vec{w}^* = 0$. This modifies each data point ($\\vec{x}, y$) by setting $y = 0$ but preserving $\\vec{x}$. This translates the Pi to pass through 0, so they are subspaces, and Pi is the kernel of Xi, and the loss of task t at $\\vec{w}$ is now $\\|X_t \\vec{w}\\|^2$, turning forgetting into:\n\n$FS(n) = \\frac{1}{n} \\sum_{t=1}^{n} \\|X_t \\vec{w}\\|^2 = \\frac{1}{n} \\times \\prod_{t=1}^{n} w$\n\nwhere the product is the categorical product. Thus, for a fixed collection of tasks X1, X2, ..., Xn, abusing notation to let Pt denote the projection onto the plane Pt, the largest forgetting over all initializations $\\vec{w}_0$ is:\n\n$\\left(\\prod_{t=1}^{n} X_t\\right)^n \\times \\prod_{t=1}^{n} P_t$\n\nRescaling $\\vec{w}_0$ rescales all iterates, and hence the forgetting, by the same amount. So we normalize $\\|\\vec{w}_0\\| = 1$. Similarly, rescaling all tasks Xt by the same scalar scales each term in the forgetting by that factor, so we normalize so that $\\max_t \\|X_t\\| = 1$.\n\nAs $\\|\\vec{w}_t\\|$ decreases in t, this implies the loss of $\\vec{w}_n$ at task t is $\\|X_t \\vec{w}_n\\|^2 \\leq 1$, and hence the forgetting is at most 1.", "md": "Following [Evr+22], we restrict to the setting where P is nonempty (i.e., the union of all tasks is consistent with a single linear function) and let $\\vec{w}^*$ &in; P. So every data point ($\\vec{x}, y$) satisfies $\\vec{x}^T \\vec{w}^* = y$, and the planes Pi we project onto all pass through $\\vec{w}^*$.\n\nUnlike [Evr+22], we translate the setup so $\\vec{w}^* = 0$. This modifies each data point ($\\vec{x}, y$) by setting $y = 0$ but preserving $\\vec{x}$. This translates the Pi to pass through 0, so they are subspaces, and Pi is the kernel of Xi, and the loss of task t at $\\vec{w}$ is now $\\|X_t \\vec{w}\\|^2$, turning forgetting into:\n\n$FS(n) = \\frac{1}{n} \\sum_{t=1}^{n} \\|X_t \\vec{w}\\|^2 = \\frac{1}{n} \\times \\prod_{t=1}^{n} w$\n\nwhere the product is the categorical product. Thus, for a fixed collection of tasks X1, X2, ..., Xn, abusing notation to let Pt denote the projection onto the plane Pt, the largest forgetting over all initializations $\\vec{w}_0$ is:\n\n$\\left(\\prod_{t=1}^{n} X_t\\right)^n \\times \\prod_{t=1}^{n} P_t$\n\nRescaling $\\vec{w}_0$ rescales all iterates, and hence the forgetting, by the same amount. So we normalize $\\|\\vec{w}_0\\| = 1$. Similarly, rescaling all tasks Xt by the same scalar scales each term in the forgetting by that factor, so we normalize so that $\\max_t \\|X_t\\| = 1$.\n\nAs $\\|\\vec{w}_t\\|$ decreases in t, this implies the loss of $\\vec{w}_n$ at task t is $\\|X_t \\vec{w}_n\\|^2 \\leq 1$, and hence the forgetting is at most 1."}, {"type": "heading", "lvl": 3, "value": "1.2 Observations", "md": "### 1.2 Observations"}, {"type": "text", "value": "In this section, we describe some observations that help us get our bound. As noted in the previous section, using our normalization, there is a trivial bound on the forgetting of 1. The data matrices Xt only affect the iterates $\\vec{w}_i$ via their rowspan (i.e., the span of the data points in the task), so we may replace Xi by any other data matrix with the same rowspan, or equivalently the same kernel, without affecting the iterates subject to our normalization constraint. As $\\|X_i\\| \\leq 1$, it is a contraction, so the norm of the image of any $\\vec{w}$ is at most its distance to the kernel, and this is realized when Xi is a projection away from its kernel. Therefore, the worst-case bound is not affected by restricting Xi to be orthogonal projections with kernel Pi. With this restriction, for any $\\vec{w}$, $\\|X_i \\vec{w}\\|$ is the distance from $\\vec{w}$ to Pi. Therefore, the forgetting can be interpreted as the average distance to each Pi over all previous tasks (weighted equally).", "md": "In this section, we describe some observations that help us get our bound. As noted in the previous section, using our normalization, there is a trivial bound on the forgetting of 1. The data matrices Xt only affect the iterates $\\vec{w}_i$ via their rowspan (i.e., the span of the data points in the task), so we may replace Xi by any other data matrix with the same rowspan, or equivalently the same kernel, without affecting the iterates subject to our normalization constraint. As $\\|X_i\\| \\leq 1$, it is a contraction, so the norm of the image of any $\\vec{w}$ is at most its distance to the kernel, and this is realized when Xi is a projection away from its kernel. Therefore, the worst-case bound is not affected by restricting Xi to be orthogonal projections with kernel Pi. With this restriction, for any $\\vec{w}$, $\\|X_i \\vec{w}\\|$ is the distance from $\\vec{w}$ to Pi. Therefore, the forgetting can be interpreted as the average distance to each Pi over all previous tasks (weighted equally)."}, {"type": "heading", "lvl": 3, "value": "1.3 Prior Work", "md": "### 1.3 Prior Work"}, {"type": "text", "value": "Bounds on forgetting. [Evr+22] considered forgetting bounds both for random task orderings and cyclic task orderings. However their bound for cyclic task orderings is not tight. After m cycles of T", "md": "Bounds on forgetting. [Evr+22] considered forgetting bounds both for random task orderings and cyclic task orderings. However their bound for cyclic task orderings is not tight. After m cycles of T"}]}, {"page": 4, "text": " tasks, they show an upper bound of min                        \u221a T 2     2mT       on the forgetting, leaving a gap with their\n lower bound of \u2126( T 2      mT )                                  mT , T 2d\n A key step of their proof is bounding \u2225Am\u20d7                              Am+1\u20d7 2 where A is a product of T projections\n                                                              u\u22252 \u2212                u                       \u221a\n and u is a unit vector. Their dimension-independent bound gives a T 2/                                       mT dependence on the\n forgetting after m cycles. Their other bound improves the m dependence, but only uses that A is a\n contraction. This forces a linear dependence on the dimension d, as this is possible if the supremum\n is taken over all contractions. Indeed, for any m a multiple of d, one can have\n by choosing A to be an operator that maps     \u2225Am\u20d7   u\u22252 \u2212      Am+1\u20d7 2 \u2265 u          \u2126(d/m)\n                                             \u20d7e1  \u2192   \u20d7e2  \u2192   . . .  \u2192  \u20d7ed  \u2192   (1 \u2212     d   e1,\n                                                                                          m)\u20d7\n and taking \u20d7    u = \u20d7e1, where \u20d7ei denotes the ith standard unit vector.\n [RZ23] bound           Am\u20d7    u \u2212   Am+1\u20d7    u  2 for A a product of projections. However they are primarily\n interested in the asymptotics with respect to m. They supply two proofs which rely on either the\n bound in [BS16] or in [Cro08], and hence do not avoid the exponential dependence in T                                          .\n [Lin+23] study forgetting from a different perspective. They assume Gaussian i.i.d. tasks ( called\n features in their paper) at each iteration and explicitly compute the expected forgetting and general-\n ization error. We analyze the worst-case forgetting without any assumptions on the distribution of\n tasks, so our work is quite different from [Lin+23].\n As mentioned in [Evr+22], the iterate wt can be equivalently obtained by running the (block)\n Kaczmarz method [NT14] with the cyclic row selection rule for solving a consistent system yt =\n Xtw, \u2200t \u2208        [T ]. However, most id=MK]of the convergence analyses of Kaczmarz-type methods\n [Elf80; NT14; SV09] bound the distance from the least-norm solution, whereas in our setting we\n interested in the residual errors. Moreover such analyses depend on the condition number of the\n system, so are unlikely to adapt to our setting, since our bounds hold for all choices of tasks (i.e.\n blocks), with no condition number dependence.\n Bounds on the numerical range.                      Lemma 5.1 in [Cro08] gives a bound on the numerical range of\n any linear transformation A expressible as a product of T projections. This, combined with other\n results in that same work, yields a bound on                      Am\u20d7   u \u2212    Am+1\u20d7    u  2 . When combined with the results\n of [Evr+22], the result is a bound of the form O(f(T                        )/m) on the forgetting. However, the function\n f(T   ) turns out to be exponential when applying the results of [Cro08] directly. This is due to Lemma\n 5.1 of [Cro08] being loose. There they show that the numerical range of A is contained in a region of\n C with a boundary point at one where the tangent lines at 1 have exponential slope. In this paper, we\n give the smallest region that is guaranteed to contain the numerical range of A given only T                                       .\n [BS16] also gives a bound on the numerical range of a product of projections. Specifically they\n show that the numerical range is contained in a Stoltz domain with the angle at 1 depending on a\n quantity called the Friedrich\u2019s number of the (subspaces associated to the) projections. They then\n combine their bound with that of [Cro08] to obtain somewhat tighter control over the numerical\n range. However the Friedrich\u2019s number for T subspaces can be made arbitrarily close to 1 so for our\n purposes their bound will not yield better results than the bounds in [Cro08].\n 1.4     Results\nWe give a nearly optimal bound on the forgetting for cyclic task orderings, removing the dimension\n dependence of [Evr+22]and avoiding the exponential dependence on T that would arise from applying\n results in [Cro07].\n Theorem 1.                                          sup   FS(mT      ) \u2264    O   T 2       .\n                                                      S                             m\n where S ranges over all (task, initialization) pairs where the tasks are given by matrices\n X1, X2, . . . , XT with maxt \u2225Xt\u2225                \u2264   1 and \u20d7  w0 \u2208    4Rd with |\u20d7   w0| = 1.", "md": "# Math Equations and Text\n\ntasks, they show an upper bound of min $$\\sqrt{T}^2 \\frac{2mT}{mT}$$ on the forgetting, leaving a gap with their lower bound of $$\\Omega\\left(\\frac{T^2}{mT}\\right)$$.\n\nA key step of their proof is bounding $$\\|Am\\vec{u} - Am+1\\vec{u}\\|^2$$ where A is a product of T projections and u is a unit vector. Their dimension-independent bound gives a $$\\frac{T^2}{mT}$$ dependence on the forgetting after m cycles. Their other bound improves the m dependence, but only uses that A is a contraction. This forces a linear dependence on the dimension d, as this is possible if the supremum is taken over all contractions. Indeed, for any m a multiple of d, one can have by choosing A to be an operator that maps $$\\vec{e1} \\rightarrow \\vec{e2} \\rightarrow ... \\rightarrow \\vec{ed} \\rightarrow (1 - \\frac{d}{m})\\vec{e1}$$ and taking $$\\vec{u} = \\vec{e1}$$, where $$\\vec{ei}$$ denotes the ith standard unit vector.\n\n[RZ23] bound $$Am\\vec{u} - Am+1\\vec{u}^2$$ for A a product of projections. However, they are primarily interested in the asymptotics with respect to m. They supply two proofs which rely on either the bound in [BS16] or in [Cro08], and hence do not avoid the exponential dependence in T.\n\n[Lin+23] study forgetting from a different perspective. They assume Gaussian i.i.d. tasks (called features in their paper) at each iteration and explicitly compute the expected forgetting and generalization error. We analyze the worst-case forgetting without any assumptions on the distribution of tasks, so our work is quite different from [Lin+23].\n\nAs mentioned in [Evr+22], the iterate $$w_t$$ can be equivalently obtained by running the (block) Kaczmarz method [NT14] with the cyclic row selection rule for solving a consistent system $$y_t = X_tw, \\forall t \\in [T]$$. However, most of the convergence analyses of Kaczmarz-type methods [Elf80; NT14; SV09] bound the distance from the least-norm solution, whereas in our setting we are interested in the residual errors. Moreover, such analyses depend on the condition number of the system, so are unlikely to adapt to our setting, since our bounds hold for all choices of tasks (i.e. blocks), with no condition number dependence.\n\nBounds on the numerical range. Lemma 5.1 in [Cro08] gives a bound on the numerical range of any linear transformation A expressible as a product of T projections. This, combined with other results in that same work, yields a bound on $$Am\\vec{u} - Am+1\\vec{u}^2$$. When combined with the results of [Evr+22], the result is a bound of the form $$O\\left(\\frac{f(T)}{m}\\right)$$ on the forgetting. However, the function $$f(T)$$ turns out to be exponential when applying the results of [Cro08] directly. This is due to Lemma 5.1 of [Cro08] being loose. There they show that the numerical range of A is contained in a region of C with a boundary point at one where the tangent lines at 1 have exponential slope. In this paper, we give the smallest region that is guaranteed to contain the numerical range of A given only T.\n\n[BS16] also gives a bound on the numerical range of a product of projections. Specifically, they show that the numerical range is contained in a Stoltz domain with the angle at 1 depending on a quantity called the Friedrich\u2019s number of the (subspaces associated to the) projections. They then combine their bound with that of [Cro08] to obtain somewhat tighter control over the numerical range. However, the Friedrich\u2019s number for T subspaces can be made arbitrarily close to 1 so for our purposes their bound will not yield better results than the bounds in [Cro08].\n\n### Results\n\nWe give a nearly optimal bound on the forgetting for cyclic task orderings, removing the dimension dependence of [Evr+22] and avoiding the exponential dependence on T that would arise from applying results in [Cro07].\n\nTheorem 1: $$\\sup_{S} FS(mT) \\leq O\\left(\\frac{T^2}{m}\\right)$$, where S ranges over all (task, initialization) pairs where the tasks are given by matrices $$X_1, X_2, ..., X_T$$ with $$\\max_t \\|X_t\\| \\leq 1$$ and $$\\vec{w_0} \\in \\mathbb{R}^d$$ with $$|\\vec{w_0}| = 1$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "tasks, they show an upper bound of min $$\\sqrt{T}^2 \\frac{2mT}{mT}$$ on the forgetting, leaving a gap with their lower bound of $$\\Omega\\left(\\frac{T^2}{mT}\\right)$$.\n\nA key step of their proof is bounding $$\\|Am\\vec{u} - Am+1\\vec{u}\\|^2$$ where A is a product of T projections and u is a unit vector. Their dimension-independent bound gives a $$\\frac{T^2}{mT}$$ dependence on the forgetting after m cycles. Their other bound improves the m dependence, but only uses that A is a contraction. This forces a linear dependence on the dimension d, as this is possible if the supremum is taken over all contractions. Indeed, for any m a multiple of d, one can have by choosing A to be an operator that maps $$\\vec{e1} \\rightarrow \\vec{e2} \\rightarrow ... \\rightarrow \\vec{ed} \\rightarrow (1 - \\frac{d}{m})\\vec{e1}$$ and taking $$\\vec{u} = \\vec{e1}$$, where $$\\vec{ei}$$ denotes the ith standard unit vector.\n\n[RZ23] bound $$Am\\vec{u} - Am+1\\vec{u}^2$$ for A a product of projections. However, they are primarily interested in the asymptotics with respect to m. They supply two proofs which rely on either the bound in [BS16] or in [Cro08], and hence do not avoid the exponential dependence in T.\n\n[Lin+23] study forgetting from a different perspective. They assume Gaussian i.i.d. tasks (called features in their paper) at each iteration and explicitly compute the expected forgetting and generalization error. We analyze the worst-case forgetting without any assumptions on the distribution of tasks, so our work is quite different from [Lin+23].\n\nAs mentioned in [Evr+22], the iterate $$w_t$$ can be equivalently obtained by running the (block) Kaczmarz method [NT14] with the cyclic row selection rule for solving a consistent system $$y_t = X_tw, \\forall t \\in [T]$$. However, most of the convergence analyses of Kaczmarz-type methods [Elf80; NT14; SV09] bound the distance from the least-norm solution, whereas in our setting we are interested in the residual errors. Moreover, such analyses depend on the condition number of the system, so are unlikely to adapt to our setting, since our bounds hold for all choices of tasks (i.e. blocks), with no condition number dependence.\n\nBounds on the numerical range. Lemma 5.1 in [Cro08] gives a bound on the numerical range of any linear transformation A expressible as a product of T projections. This, combined with other results in that same work, yields a bound on $$Am\\vec{u} - Am+1\\vec{u}^2$$. When combined with the results of [Evr+22], the result is a bound of the form $$O\\left(\\frac{f(T)}{m}\\right)$$ on the forgetting. However, the function $$f(T)$$ turns out to be exponential when applying the results of [Cro08] directly. This is due to Lemma 5.1 of [Cro08] being loose. There they show that the numerical range of A is contained in a region of C with a boundary point at one where the tangent lines at 1 have exponential slope. In this paper, we give the smallest region that is guaranteed to contain the numerical range of A given only T.\n\n[BS16] also gives a bound on the numerical range of a product of projections. Specifically, they show that the numerical range is contained in a Stoltz domain with the angle at 1 depending on a quantity called the Friedrich\u2019s number of the (subspaces associated to the) projections. They then combine their bound with that of [Cro08] to obtain somewhat tighter control over the numerical range. However, the Friedrich\u2019s number for T subspaces can be made arbitrarily close to 1 so for our purposes their bound will not yield better results than the bounds in [Cro08].", "md": "tasks, they show an upper bound of min $$\\sqrt{T}^2 \\frac{2mT}{mT}$$ on the forgetting, leaving a gap with their lower bound of $$\\Omega\\left(\\frac{T^2}{mT}\\right)$$.\n\nA key step of their proof is bounding $$\\|Am\\vec{u} - Am+1\\vec{u}\\|^2$$ where A is a product of T projections and u is a unit vector. Their dimension-independent bound gives a $$\\frac{T^2}{mT}$$ dependence on the forgetting after m cycles. Their other bound improves the m dependence, but only uses that A is a contraction. This forces a linear dependence on the dimension d, as this is possible if the supremum is taken over all contractions. Indeed, for any m a multiple of d, one can have by choosing A to be an operator that maps $$\\vec{e1} \\rightarrow \\vec{e2} \\rightarrow ... \\rightarrow \\vec{ed} \\rightarrow (1 - \\frac{d}{m})\\vec{e1}$$ and taking $$\\vec{u} = \\vec{e1}$$, where $$\\vec{ei}$$ denotes the ith standard unit vector.\n\n[RZ23] bound $$Am\\vec{u} - Am+1\\vec{u}^2$$ for A a product of projections. However, they are primarily interested in the asymptotics with respect to m. They supply two proofs which rely on either the bound in [BS16] or in [Cro08], and hence do not avoid the exponential dependence in T.\n\n[Lin+23] study forgetting from a different perspective. They assume Gaussian i.i.d. tasks (called features in their paper) at each iteration and explicitly compute the expected forgetting and generalization error. We analyze the worst-case forgetting without any assumptions on the distribution of tasks, so our work is quite different from [Lin+23].\n\nAs mentioned in [Evr+22], the iterate $$w_t$$ can be equivalently obtained by running the (block) Kaczmarz method [NT14] with the cyclic row selection rule for solving a consistent system $$y_t = X_tw, \\forall t \\in [T]$$. However, most of the convergence analyses of Kaczmarz-type methods [Elf80; NT14; SV09] bound the distance from the least-norm solution, whereas in our setting we are interested in the residual errors. Moreover, such analyses depend on the condition number of the system, so are unlikely to adapt to our setting, since our bounds hold for all choices of tasks (i.e. blocks), with no condition number dependence.\n\nBounds on the numerical range. Lemma 5.1 in [Cro08] gives a bound on the numerical range of any linear transformation A expressible as a product of T projections. This, combined with other results in that same work, yields a bound on $$Am\\vec{u} - Am+1\\vec{u}^2$$. When combined with the results of [Evr+22], the result is a bound of the form $$O\\left(\\frac{f(T)}{m}\\right)$$ on the forgetting. However, the function $$f(T)$$ turns out to be exponential when applying the results of [Cro08] directly. This is due to Lemma 5.1 of [Cro08] being loose. There they show that the numerical range of A is contained in a region of C with a boundary point at one where the tangent lines at 1 have exponential slope. In this paper, we give the smallest region that is guaranteed to contain the numerical range of A given only T.\n\n[BS16] also gives a bound on the numerical range of a product of projections. Specifically, they show that the numerical range is contained in a Stoltz domain with the angle at 1 depending on a quantity called the Friedrich\u2019s number of the (subspaces associated to the) projections. They then combine their bound with that of [Cro08] to obtain somewhat tighter control over the numerical range. However, the Friedrich\u2019s number for T subspaces can be made arbitrarily close to 1 so for our purposes their bound will not yield better results than the bounds in [Cro08]."}, {"type": "heading", "lvl": 3, "value": "Results", "md": "### Results"}, {"type": "text", "value": "We give a nearly optimal bound on the forgetting for cyclic task orderings, removing the dimension dependence of [Evr+22] and avoiding the exponential dependence on T that would arise from applying results in [Cro07].\n\nTheorem 1: $$\\sup_{S} FS(mT) \\leq O\\left(\\frac{T^2}{m}\\right)$$, where S ranges over all (task, initialization) pairs where the tasks are given by matrices $$X_1, X_2, ..., X_T$$ with $$\\max_t \\|X_t\\| \\leq 1$$ and $$\\vec{w_0} \\in \\mathbb{R}^d$$ with $$|\\vec{w_0}| = 1$$.", "md": "We give a nearly optimal bound on the forgetting for cyclic task orderings, removing the dimension dependence of [Evr+22] and avoiding the exponential dependence on T that would arise from applying results in [Cro07].\n\nTheorem 1: $$\\sup_{S} FS(mT) \\leq O\\left(\\frac{T^2}{m}\\right)$$, where S ranges over all (task, initialization) pairs where the tasks are given by matrices $$X_1, X_2, ..., X_T$$ with $$\\max_t \\|X_t\\| \\leq 1$$ and $$\\vec{w_0} \\in \\mathbb{R}^d$$ with $$|\\vec{w_0}| = 1$$."}]}, {"page": 5, "text": "To prove this, [Evr+22] show that, if A denotes the linear map given by one cycle, letting \u2206m(\u20d7                              u) :=\n\u2225Am\u20d7   u\u2225  \u2212   \u2225Am+1\u20d7    u\u2225,\n                       sup   FS(mT) \u2264        T \u2212 2  1  \u00b7\u20d7max             u) \u2264    T \u2212 2  1\u2225Am(1 \u2212       A)\u2225.\n                         S                              u:\u2225u\u2225=1 \u2206m(\u20d7\nWe provide a geometric interpretation of their proof in the supplementary material. [CP17] showed\nthat there exists a constant Q \u2264           1 +   \u221a  2 such that\n                                                 \u2225p(A)\u2225     \u2264  Q     sup    |p(z)|\n                                                                  z\u2208W (A)\nfor all polynomials p. Our approach is to determine  A W(A) where A ranges over all products\nof k complex orthogonal projections. As this includes the class of products of k real orthogonal\nprojections, combining these inequalities gives our bound.\nWe will show in Section 3 that the problem of bounding the numerical range of the product of k \u2212                                   1\ncomplex orthogonal projections is equivalent to understanding the range of the product\n                       P(\u20d7v0, . . . ,\u20d7vk\u22121) := \u27e8\u20d7v0,\u20d7v1\u27e9     \u27e8\u20d7v1,\u20d7v2\u27e9  . . . \u27e8\u20d7vk\u22122,\u20d7vk\u22121\u27e9    \u27e8\u20d7vk\u22121,\u20d7v0\u27e9   ,                   (2)\nwhere the \u20d7vi range over complex vectors in Cd of norm 1. We give a complete description of the\nrange of P as the filled sinusoidal spiral\n                                         rei\u03b8 : cos(\u03c0/k)r\u22121/k \u2265             cos(\u03b8 \u2212  k  \u03c0 ).\nThis shape has a simple geometric description. Let Pk be the convex hull of the kth roots of unity.\nThen the sinusoidal spiral above is the image of Pk under the map z  \u2192                          zk.\nThe main challenge in computing P\u2019s range \u0393k is in computing its boundary since a topological\nargument (given in the supplementary material) shows that \u0393k is simply connected. To compute\nthe boundary, we first observe that it suffices to consider input vectors in C2. To see this, P maps a\nsequence of vectors to the cyclic product of pairwise inner products, so if any of the input vectors is\nnot coplanar with its neighbors, then projecting it onto the plane spanned by its neighbors and scaling\nit to have unit norm increases the magnitude of the corresponding factors. So it suffi                           ces to consider\nthe case when all vectors are coplanar, which is equivalent to considering input vectors in C2. In this\ncase, P is a smooth map from the (real) manifold (S1)n \u2282                       Cn to the (real) manifold C, so any input\nthat gets sent to a boundary point of \u0393k has singular Jacobian. That is, the directional derivative in all\ndirections tangent to the domain must be parallel. It turns out that these algebraic conditions can be\nmanipulated to characterize all critical points and critical values of P, and the computation is made\nsimpler by using quaternions.\nAs a side effect of our proof, we also fully describe the tuples \u20d7v0, . . . ,\u20d7vk\u22121) that map to boundary\npoints of the image of P and show that they naturally correspond (up to multiplying each \u20d7vt by a\ncomplex unit \u03d5t) to certain quaternionic roots of unity.\nUsing this result we obtain the following bound on the increments between consecutive powers of A\nwhen A is a product of projections. We believe this result may be independently useful.\nLemma 2. For any linear map A: Cn \u2192                      Cn expressible as a product of k complex projections,\n                                       \u2225(I \u2212    A)Am\u2225       \u2264  (C + ok,m\u2192\u221e(1)) k         m,\nwhere C is an absolute constant which can be taken to be 0.4.\nThis lemma, with the inequality of [Evr+22], immediately implies Theorem 1.\nIn particular, for fixed T, we obtain an asymptotically optimal bound in terms of iteration count m,\nindependent of dimension. In contrast, the dimension-independent bound given in [Evr+22] was\nsub-optimal by a factor of \u221am.\nFinally, . So our results can be interpreted as giving a bound for this setting as well.\n                                                                  5", "md": "To prove this, $$\\text{[Evr+22]}$$ show that, if A denotes the linear map given by one cycle, letting $$\\Delta_m(\\vec{u}) := \\|A^m \\vec{u}\\| - \\|A^{m+1} \\vec{u}\\|,$$\n$$\\sup_{\\substack{\\vec{u}: \\|u\\|=1}} \\Delta_m(\\vec{u}) \\leq T - 2\\sqrt{1} \\cdot \\max_{\\vec{u}: \\|u\\|=1} \\Delta_m(1 - A)\\|.$$\n\nWe provide a geometric interpretation of their proof in the supplementary material. $$\\text{[CP17]}$$ showed that there exists a constant $$Q \\leq 1 + \\sqrt{2}$$ such that\n$$\\|p(A)\\| \\leq Q \\sup_{z\\in W(A)} |p(z)|$$\nfor all polynomials p. Our approach is to determine $$A W(A)$$ where A ranges over all products of k complex orthogonal projections. As this includes the class of products of k real orthogonal projections, combining these inequalities gives our bound.\n\nWe will show in Section 3 that the problem of bounding the numerical range of the product of $$k-1$$ complex orthogonal projections is equivalent to understanding the range of the product\n$$P(\\vec{v}_0, ..., \\vec{v}_{k-1}) := \\langle \\vec{v}_0, \\vec{v}_1 \\rangle \\langle \\vec{v}_1, \\vec{v}_2 \\rangle ... \\langle \\vec{v}_{k-2}, \\vec{v}_{k-1} \\rangle \\langle \\vec{v}_{k-1}, \\vec{v}_0 \\rangle, \\quad (2)$$\nwhere the $$\\vec{v}_i$$ range over complex vectors in $$\\mathbb{C}^d$$ of norm 1. We give a complete description of the range of P as the filled sinusoidal spiral\n$$re^{i\\theta}: \\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} \\geq \\cos(\\theta - k\\pi).$$\nThis shape has a simple geometric description. Let $$P_k$$ be the convex hull of the kth roots of unity. Then the sinusoidal spiral above is the image of $$P_k$$ under the map $$z \\mapsto z^k$$.\n\nThe main challenge in computing P's range $$\\Gamma_k$$ is in computing its boundary since a topological argument (given in the supplementary material) shows that $$\\Gamma_k$$ is simply connected. To compute the boundary, we first observe that it suffices to consider input vectors in $$\\mathbb{C}^2$$. To see this, P maps a sequence of vectors to the cyclic product of pairwise inner products, so if any of the input vectors is not coplanar with its neighbors, then projecting it onto the plane spanned by its neighbors and scaling it to have unit norm increases the magnitude of the corresponding factors. So it suffices to consider the case when all vectors are coplanar, which is equivalent to considering input vectors in $$\\mathbb{C}^2$$. In this case, P is a smooth map from the (real) manifold $$(S^1)^n \\subset \\mathbb{C}^n$$ to the (real) manifold $$\\mathbb{C}$$, so any input that gets sent to a boundary point of $$\\Gamma_k$$ has singular Jacobian. That is, the directional derivative in all directions tangent to the domain must be parallel. It turns out that these algebraic conditions can be manipulated to characterize all critical points and critical values of P, and the computation is made simpler by using quaternions.\n\nAs a side effect of our proof, we also fully describe the tuples $$\\vec{v}_0, ..., \\vec{v}_{k-1}$$ that map to boundary points of the image of P and show that they naturally correspond (up to multiplying each $$\\vec{v}_t$$ by a complex unit $$\\phi_t$$) to certain quaternionic roots of unity.\n\nUsing this result we obtain the following bound on the increments between consecutive powers of A when A is a product of projections. We believe this result may be independently useful.\n\nLemma 2. For any linear map $$A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n$$ expressible as a product of k complex projections,\n$$\\|(I - A)A^m\\| \\leq (C + o_{k,m\\to\\infty}(1))^k m,$$\nwhere C is an absolute constant which can be taken to be 0.4.\n\nThis lemma, with the inequality of $$\\text{[Evr+22]}$$, immediately implies Theorem 1.\n\nIn particular, for fixed T, we obtain an asymptotically optimal bound in terms of iteration count m, independent of dimension. In contrast, the dimension-independent bound given in $$\\text{[Evr+22]}$$ was sub-optimal by a factor of $$\\sqrt{m}$$.\n\nFinally, . So our results can be interpreted as giving a bound for this setting as well.", "images": [], "items": [{"type": "text", "value": "To prove this, $$\\text{[Evr+22]}$$ show that, if A denotes the linear map given by one cycle, letting $$\\Delta_m(\\vec{u}) := \\|A^m \\vec{u}\\| - \\|A^{m+1} \\vec{u}\\|,$$\n$$\\sup_{\\substack{\\vec{u}: \\|u\\|=1}} \\Delta_m(\\vec{u}) \\leq T - 2\\sqrt{1} \\cdot \\max_{\\vec{u}: \\|u\\|=1} \\Delta_m(1 - A)\\|.$$\n\nWe provide a geometric interpretation of their proof in the supplementary material. $$\\text{[CP17]}$$ showed that there exists a constant $$Q \\leq 1 + \\sqrt{2}$$ such that\n$$\\|p(A)\\| \\leq Q \\sup_{z\\in W(A)} |p(z)|$$\nfor all polynomials p. Our approach is to determine $$A W(A)$$ where A ranges over all products of k complex orthogonal projections. As this includes the class of products of k real orthogonal projections, combining these inequalities gives our bound.\n\nWe will show in Section 3 that the problem of bounding the numerical range of the product of $$k-1$$ complex orthogonal projections is equivalent to understanding the range of the product\n$$P(\\vec{v}_0, ..., \\vec{v}_{k-1}) := \\langle \\vec{v}_0, \\vec{v}_1 \\rangle \\langle \\vec{v}_1, \\vec{v}_2 \\rangle ... \\langle \\vec{v}_{k-2}, \\vec{v}_{k-1} \\rangle \\langle \\vec{v}_{k-1}, \\vec{v}_0 \\rangle, \\quad (2)$$\nwhere the $$\\vec{v}_i$$ range over complex vectors in $$\\mathbb{C}^d$$ of norm 1. We give a complete description of the range of P as the filled sinusoidal spiral\n$$re^{i\\theta}: \\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} \\geq \\cos(\\theta - k\\pi).$$\nThis shape has a simple geometric description. Let $$P_k$$ be the convex hull of the kth roots of unity. Then the sinusoidal spiral above is the image of $$P_k$$ under the map $$z \\mapsto z^k$$.\n\nThe main challenge in computing P's range $$\\Gamma_k$$ is in computing its boundary since a topological argument (given in the supplementary material) shows that $$\\Gamma_k$$ is simply connected. To compute the boundary, we first observe that it suffices to consider input vectors in $$\\mathbb{C}^2$$. To see this, P maps a sequence of vectors to the cyclic product of pairwise inner products, so if any of the input vectors is not coplanar with its neighbors, then projecting it onto the plane spanned by its neighbors and scaling it to have unit norm increases the magnitude of the corresponding factors. So it suffices to consider the case when all vectors are coplanar, which is equivalent to considering input vectors in $$\\mathbb{C}^2$$. In this case, P is a smooth map from the (real) manifold $$(S^1)^n \\subset \\mathbb{C}^n$$ to the (real) manifold $$\\mathbb{C}$$, so any input that gets sent to a boundary point of $$\\Gamma_k$$ has singular Jacobian. That is, the directional derivative in all directions tangent to the domain must be parallel. It turns out that these algebraic conditions can be manipulated to characterize all critical points and critical values of P, and the computation is made simpler by using quaternions.\n\nAs a side effect of our proof, we also fully describe the tuples $$\\vec{v}_0, ..., \\vec{v}_{k-1}$$ that map to boundary points of the image of P and show that they naturally correspond (up to multiplying each $$\\vec{v}_t$$ by a complex unit $$\\phi_t$$) to certain quaternionic roots of unity.\n\nUsing this result we obtain the following bound on the increments between consecutive powers of A when A is a product of projections. We believe this result may be independently useful.\n\nLemma 2. For any linear map $$A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n$$ expressible as a product of k complex projections,\n$$\\|(I - A)A^m\\| \\leq (C + o_{k,m\\to\\infty}(1))^k m,$$\nwhere C is an absolute constant which can be taken to be 0.4.\n\nThis lemma, with the inequality of $$\\text{[Evr+22]}$$, immediately implies Theorem 1.\n\nIn particular, for fixed T, we obtain an asymptotically optimal bound in terms of iteration count m, independent of dimension. In contrast, the dimension-independent bound given in $$\\text{[Evr+22]}$$ was sub-optimal by a factor of $$\\sqrt{m}$$.\n\nFinally, . So our results can be interpreted as giving a bound for this setting as well.", "md": "To prove this, $$\\text{[Evr+22]}$$ show that, if A denotes the linear map given by one cycle, letting $$\\Delta_m(\\vec{u}) := \\|A^m \\vec{u}\\| - \\|A^{m+1} \\vec{u}\\|,$$\n$$\\sup_{\\substack{\\vec{u}: \\|u\\|=1}} \\Delta_m(\\vec{u}) \\leq T - 2\\sqrt{1} \\cdot \\max_{\\vec{u}: \\|u\\|=1} \\Delta_m(1 - A)\\|.$$\n\nWe provide a geometric interpretation of their proof in the supplementary material. $$\\text{[CP17]}$$ showed that there exists a constant $$Q \\leq 1 + \\sqrt{2}$$ such that\n$$\\|p(A)\\| \\leq Q \\sup_{z\\in W(A)} |p(z)|$$\nfor all polynomials p. Our approach is to determine $$A W(A)$$ where A ranges over all products of k complex orthogonal projections. As this includes the class of products of k real orthogonal projections, combining these inequalities gives our bound.\n\nWe will show in Section 3 that the problem of bounding the numerical range of the product of $$k-1$$ complex orthogonal projections is equivalent to understanding the range of the product\n$$P(\\vec{v}_0, ..., \\vec{v}_{k-1}) := \\langle \\vec{v}_0, \\vec{v}_1 \\rangle \\langle \\vec{v}_1, \\vec{v}_2 \\rangle ... \\langle \\vec{v}_{k-2}, \\vec{v}_{k-1} \\rangle \\langle \\vec{v}_{k-1}, \\vec{v}_0 \\rangle, \\quad (2)$$\nwhere the $$\\vec{v}_i$$ range over complex vectors in $$\\mathbb{C}^d$$ of norm 1. We give a complete description of the range of P as the filled sinusoidal spiral\n$$re^{i\\theta}: \\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} \\geq \\cos(\\theta - k\\pi).$$\nThis shape has a simple geometric description. Let $$P_k$$ be the convex hull of the kth roots of unity. Then the sinusoidal spiral above is the image of $$P_k$$ under the map $$z \\mapsto z^k$$.\n\nThe main challenge in computing P's range $$\\Gamma_k$$ is in computing its boundary since a topological argument (given in the supplementary material) shows that $$\\Gamma_k$$ is simply connected. To compute the boundary, we first observe that it suffices to consider input vectors in $$\\mathbb{C}^2$$. To see this, P maps a sequence of vectors to the cyclic product of pairwise inner products, so if any of the input vectors is not coplanar with its neighbors, then projecting it onto the plane spanned by its neighbors and scaling it to have unit norm increases the magnitude of the corresponding factors. So it suffices to consider the case when all vectors are coplanar, which is equivalent to considering input vectors in $$\\mathbb{C}^2$$. In this case, P is a smooth map from the (real) manifold $$(S^1)^n \\subset \\mathbb{C}^n$$ to the (real) manifold $$\\mathbb{C}$$, so any input that gets sent to a boundary point of $$\\Gamma_k$$ has singular Jacobian. That is, the directional derivative in all directions tangent to the domain must be parallel. It turns out that these algebraic conditions can be manipulated to characterize all critical points and critical values of P, and the computation is made simpler by using quaternions.\n\nAs a side effect of our proof, we also fully describe the tuples $$\\vec{v}_0, ..., \\vec{v}_{k-1}$$ that map to boundary points of the image of P and show that they naturally correspond (up to multiplying each $$\\vec{v}_t$$ by a complex unit $$\\phi_t$$) to certain quaternionic roots of unity.\n\nUsing this result we obtain the following bound on the increments between consecutive powers of A when A is a product of projections. We believe this result may be independently useful.\n\nLemma 2. For any linear map $$A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n$$ expressible as a product of k complex projections,\n$$\\|(I - A)A^m\\| \\leq (C + o_{k,m\\to\\infty}(1))^k m,$$\nwhere C is an absolute constant which can be taken to be 0.4.\n\nThis lemma, with the inequality of $$\\text{[Evr+22]}$$, immediately implies Theorem 1.\n\nIn particular, for fixed T, we obtain an asymptotically optimal bound in terms of iteration count m, independent of dimension. In contrast, the dimension-independent bound given in $$\\text{[Evr+22]}$$ was sub-optimal by a factor of $$\\sqrt{m}$$.\n\nFinally, . So our results can be interpreted as giving a bound for this setting as well."}]}, {"page": 6, "text": "                                                          variable name              element of\n                                                                 a, b, z                    C            .\n                                                                  \u20d7v, \u20d7\n                                                                      w                    C2\n                                                                  p, q                      H\n                                   Figure 1: The boundaries of \u0393m for m = 22, 23, . . . , 27.\n 1.5       Notation for proof\n We use C and H to denote the complex numbers and the quaternions. While the previous sections\n considered real vector spaces, our proof works by obtaining a bound over all complex projections.\n So for sections 2 and 3, all vector spaces will be over C. In these sections, we use the following\n guidelines for variable names:\n Our complex inner products are always conjugate linear in the first argument, so \u27e8\u20d7v, \u20d7                                            w\u27e9   = \u20d7v\u2217   w\u20d7where\n v\u2217   denotes the conjugate transpose of \u20d7v. The argument of a complex number is taken to be in (\u2212\u03c0, \u03c0].\n The numerical range of a matrix A \u2208                         Cd\u00d7d is defined by\n 2       Bounding the range of P                   W    (A) = {\u20d7     z\u2217A\u20d7   z|\u20d7z \u2208    Cd, \u2225\u20d7   z\u2225   = 1}.\n In this section, we fully describe the range of P given by (2) and obtain a full description of the\n inputs that map to boundary points. We will use the following notation throughout the next argument.\n Definition 3. For (a, b) \u2208                 C2, we define (a, b)\u22a5              := (b, \u2212a).\n The following identity is a straightforward consequence of the definition.\n                                                     \u20d7v, \u20d7\n Proposition 4. For \u20d7v, \u20d7            w \u2208     C2,         w\u22a5       = \u2212\u27e8\u20d7v\u22a5, \u20d7     w\u27e9.\n Our first theorem shows that the range of P is a filled sinusoidal spiral. This result is of mathematical\n interest on its own, and will also be applied to cyclic forgetting bounds in the next section.\n We begin with a lemma characterizing the critical points of P treated as a smooth map R2k \u2192                                                              R2.\n We defer the proof to the supplementary material.\n Lemma 5. For any critical value c \u2208                                 C of P      , there exists a sequence of vectors (a0, b0) =\n(1, 0), (a1, b1), . . . , (ak\u22121, bk\u22121) such that P                      ((am, bm)k\u22121     m=0) = c. Furthermore, there exist \u03b1, \u03b2 \u2208                          C\n such that\n          1. \u27e8(am, bm), (am+1, bm+1)\u27e9                      = \u03b1 for all m\n          2. \u27e8(am, bm)\u22a5, (am+1, bm+1)\u27e9                        = \u03b2\u03c9m\u22121 for all m\n (where indices are treated mod k).                                 Furthermore, any sequence ((am, bm)k\u22121                           m=0) attaining\n P  (((am, bm)k\u22121     m=0)) = c can be obtained by taking a sequence satisfying the above conditions\n and multiplying each vector by a complex unit.\n                                                                               6", "md": "# Math Equations and Text\n\nvariable name element of\n\n$$\n\\begin{array}{cc}\n\\text{a, b, z} & \\in \\mathbb{C} \\\\\n\\vec{v}, \\vec{w} & \\in \\mathbb{C}^2 \\\\\np, q & \\in \\mathbb{H}\n\\end{array}\n$$\nFigure 1: The boundaries of $\\Gamma_m$ for $m = 22, 23, ..., 27$.\n\n1.5 Notation for proof\n\nWe use $\\mathbb{C}$ and $\\mathbb{H}$ to denote the complex numbers and the quaternions. While the previous sections considered real vector spaces, our proof works by obtaining a bound over all complex projections. So for sections 2 and 3, all vector spaces will be over $\\mathbb{C}$. In these sections, we use the following guidelines for variable names:\n\nOur complex inner products are always conjugate linear in the first argument, so $\\langle \\vec{v}, \\vec{w} \\rangle = \\vec{v}^* \\vec{w}$ where $\\vec{v}^*$ denotes the conjugate transpose of $\\vec{v}$. The argument of a complex number is taken to be in $(-\\pi, \\pi]$.\n\nThe numerical range of a matrix $A \\in \\mathbb{C}^{d \\times d}$ is defined by\n\n$$\nW(A) = \\{ \\vec{z}^* A \\vec{z} | \\vec{z} \\in \\mathbb{C}^d, \\| \\vec{z} \\| = 1 \\}.\n$$\nIn this section, we fully describe the range of $P$ given by (2) and obtain a full description of the inputs that map to boundary points. We will use the following notation throughout the next argument.\n\nDefinition 3. For $(a, b) \\in \\mathbb{C}^2$, we define $(a, b)^\\perp := (b, -a)$.\n\nThe following identity is a straightforward consequence of the definition.\n\n$$\n\\text{Proposition 4. For } \\vec{v}, \\vec{w} \\in \\mathbb{C}^2, \\vec{w}^\\perp = -\\langle \\vec{v}^\\perp, \\vec{w} \\rangle.\n$$\nOur first theorem shows that the range of $P$ is a filled sinusoidal spiral. This result is of mathematical interest on its own, and will also be applied to cyclic forgetting bounds in the next section.\n\nWe begin with a lemma characterizing the critical points of $P$ treated as a smooth map $\\mathbb{R}^{2k} \\rightarrow \\mathbb{R}^2$. We defer the proof to the supplementary material.\n\nLemma 5. For any critical value $c \\in \\mathbb{C}$ of $P$, there exists a sequence of vectors $(a_0, b_0) = (1, 0), (a_1, b_1), ..., (a_{k-1}, b_{k-1})$ such that $P((a_m, b_m)_{m=0}^{k-1}) = c$. Furthermore, there exist $\\alpha, \\beta \\in \\mathbb{C}$ such that\n\n1. $\\langle (a_m, b_m), (a_{m+1}, b_{m+1}) \\rangle = \\alpha$ for all $m$\n2. $\\langle (a_m, b_m)^\\perp, (a_{m+1}, b_{m+1}) \\rangle = \\beta \\omega^{m-1}$ for all $m$ (where indices are treated mod $k$).\n\nFurthermore, any sequence $((a_m, b_m)_{m=0}^{k-1})$ attaining $P(((a_m, b_m)_{m=0}^{k-1})) = c$ can be obtained by taking a sequence satisfying the above conditions and multiplying each vector by a complex unit.", "images": [{"name": "page-6-0.jpg", "height": 124, "width": 182, "x": 215, "y": 128}], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "variable name element of\n\n$$\n\\begin{array}{cc}\n\\text{a, b, z} & \\in \\mathbb{C} \\\\\n\\vec{v}, \\vec{w} & \\in \\mathbb{C}^2 \\\\\np, q & \\in \\mathbb{H}\n\\end{array}\n$$\nFigure 1: The boundaries of $\\Gamma_m$ for $m = 22, 23, ..., 27$.\n\n1.5 Notation for proof\n\nWe use $\\mathbb{C}$ and $\\mathbb{H}$ to denote the complex numbers and the quaternions. While the previous sections considered real vector spaces, our proof works by obtaining a bound over all complex projections. So for sections 2 and 3, all vector spaces will be over $\\mathbb{C}$. In these sections, we use the following guidelines for variable names:\n\nOur complex inner products are always conjugate linear in the first argument, so $\\langle \\vec{v}, \\vec{w} \\rangle = \\vec{v}^* \\vec{w}$ where $\\vec{v}^*$ denotes the conjugate transpose of $\\vec{v}$. The argument of a complex number is taken to be in $(-\\pi, \\pi]$.\n\nThe numerical range of a matrix $A \\in \\mathbb{C}^{d \\times d}$ is defined by\n\n$$\nW(A) = \\{ \\vec{z}^* A \\vec{z} | \\vec{z} \\in \\mathbb{C}^d, \\| \\vec{z} \\| = 1 \\}.\n$$\nIn this section, we fully describe the range of $P$ given by (2) and obtain a full description of the inputs that map to boundary points. We will use the following notation throughout the next argument.\n\nDefinition 3. For $(a, b) \\in \\mathbb{C}^2$, we define $(a, b)^\\perp := (b, -a)$.\n\nThe following identity is a straightforward consequence of the definition.\n\n$$\n\\text{Proposition 4. For } \\vec{v}, \\vec{w} \\in \\mathbb{C}^2, \\vec{w}^\\perp = -\\langle \\vec{v}^\\perp, \\vec{w} \\rangle.\n$$\nOur first theorem shows that the range of $P$ is a filled sinusoidal spiral. This result is of mathematical interest on its own, and will also be applied to cyclic forgetting bounds in the next section.\n\nWe begin with a lemma characterizing the critical points of $P$ treated as a smooth map $\\mathbb{R}^{2k} \\rightarrow \\mathbb{R}^2$. We defer the proof to the supplementary material.\n\nLemma 5. For any critical value $c \\in \\mathbb{C}$ of $P$, there exists a sequence of vectors $(a_0, b_0) = (1, 0), (a_1, b_1), ..., (a_{k-1}, b_{k-1})$ such that $P((a_m, b_m)_{m=0}^{k-1}) = c$. Furthermore, there exist $\\alpha, \\beta \\in \\mathbb{C}$ such that\n\n1. $\\langle (a_m, b_m), (a_{m+1}, b_{m+1}) \\rangle = \\alpha$ for all $m$\n2. $\\langle (a_m, b_m)^\\perp, (a_{m+1}, b_{m+1}) \\rangle = \\beta \\omega^{m-1}$ for all $m$ (where indices are treated mod $k$).\n\nFurthermore, any sequence $((a_m, b_m)_{m=0}^{k-1})$ attaining $P(((a_m, b_m)_{m=0}^{k-1})) = c$ can be obtained by taking a sequence satisfying the above conditions and multiplying each vector by a complex unit.", "md": "variable name element of\n\n$$\n\\begin{array}{cc}\n\\text{a, b, z} & \\in \\mathbb{C} \\\\\n\\vec{v}, \\vec{w} & \\in \\mathbb{C}^2 \\\\\np, q & \\in \\mathbb{H}\n\\end{array}\n$$\nFigure 1: The boundaries of $\\Gamma_m$ for $m = 22, 23, ..., 27$.\n\n1.5 Notation for proof\n\nWe use $\\mathbb{C}$ and $\\mathbb{H}$ to denote the complex numbers and the quaternions. While the previous sections considered real vector spaces, our proof works by obtaining a bound over all complex projections. So for sections 2 and 3, all vector spaces will be over $\\mathbb{C}$. In these sections, we use the following guidelines for variable names:\n\nOur complex inner products are always conjugate linear in the first argument, so $\\langle \\vec{v}, \\vec{w} \\rangle = \\vec{v}^* \\vec{w}$ where $\\vec{v}^*$ denotes the conjugate transpose of $\\vec{v}$. The argument of a complex number is taken to be in $(-\\pi, \\pi]$.\n\nThe numerical range of a matrix $A \\in \\mathbb{C}^{d \\times d}$ is defined by\n\n$$\nW(A) = \\{ \\vec{z}^* A \\vec{z} | \\vec{z} \\in \\mathbb{C}^d, \\| \\vec{z} \\| = 1 \\}.\n$$\nIn this section, we fully describe the range of $P$ given by (2) and obtain a full description of the inputs that map to boundary points. We will use the following notation throughout the next argument.\n\nDefinition 3. For $(a, b) \\in \\mathbb{C}^2$, we define $(a, b)^\\perp := (b, -a)$.\n\nThe following identity is a straightforward consequence of the definition.\n\n$$\n\\text{Proposition 4. For } \\vec{v}, \\vec{w} \\in \\mathbb{C}^2, \\vec{w}^\\perp = -\\langle \\vec{v}^\\perp, \\vec{w} \\rangle.\n$$\nOur first theorem shows that the range of $P$ is a filled sinusoidal spiral. This result is of mathematical interest on its own, and will also be applied to cyclic forgetting bounds in the next section.\n\nWe begin with a lemma characterizing the critical points of $P$ treated as a smooth map $\\mathbb{R}^{2k} \\rightarrow \\mathbb{R}^2$. We defer the proof to the supplementary material.\n\nLemma 5. For any critical value $c \\in \\mathbb{C}$ of $P$, there exists a sequence of vectors $(a_0, b_0) = (1, 0), (a_1, b_1), ..., (a_{k-1}, b_{k-1})$ such that $P((a_m, b_m)_{m=0}^{k-1}) = c$. Furthermore, there exist $\\alpha, \\beta \\in \\mathbb{C}$ such that\n\n1. $\\langle (a_m, b_m), (a_{m+1}, b_{m+1}) \\rangle = \\alpha$ for all $m$\n2. $\\langle (a_m, b_m)^\\perp, (a_{m+1}, b_{m+1}) \\rangle = \\beta \\omega^{m-1}$ for all $m$ (where indices are treated mod $k$).\n\nFurthermore, any sequence $((a_m, b_m)_{m=0}^{k-1})$ attaining $P(((a_m, b_m)_{m=0}^{k-1})) = c$ can be obtained by taking a sequence satisfying the above conditions and multiplying each vector by a complex unit."}]}, {"page": 7, "text": "Given this characterization of P                  \u2019s critical points, our next result computes the boundary points of P                                     \u2019s\nimage, from which we obtain the entire image of P as a consequence.\nTheorem 6. Let P               (\u20d7v0, . . . , \u20d7vk\u22121) = \u27e8\u20d7v0,        \u20d7v1\u27e9  \u27e8\u20d7v1,  \u20d7v2\u27e9  . . . \u27e8\u20d7vk\u22122,    \u20d7vk\u22121\u27e9    \u27e8\u20d7vk\u22121,     \u20d7v0\u27e9  and let \u0393k be the\nrange of P as each \u20d7vi ranges over unit vectors in Cd. Then\n                         \u0393k =         rei\u03b8| cos      \u03c0 k     r\u22121/k \u2265        cos    \u03b8 \u2212   k  \u03c0      , r \u2265    0, \u03b8 \u2208     [0, 2\u03c0]       ,\nwhich is a filled sinusoidal spiral.\nProof. We show this result for vectors in C2. In the supplementary material, we show that increasing\nthe dimension does not affect the range, so considering vectors in C2 is sufficient. The result is trivial\nfor k \u2264      2, so we will assume k \u2265                  3.\nThe domain of P is compact, so \u0393k is also compact. P is also smooth (as a function R4 \u2192                                                            R2), so\nits boundary consists of critical values. We will give a complete description of the critical values and\ncritical points and show that the outermost critical values form a sinusoidal spiral, which is enough to\nshow that \u0393k is contained in the claimed region. We defer showing that \u0393k contains the entire filled\nspiral to the supplementary material.\nFix a critical value c of P and let ai, bi be as in Lemma 5 above. As (am, bm) and (am, bm)\u22a5                                                              are\northogonal, they form a basis for C2, so letting \u03b1 = a0, \u03b2 = b0, we get a recurrence relation\n                                                       (a1, b1) = \u03b1(1, 0) + \u03b2(1, 0)\u22a5\n                                                   (a2, b2) = \u03b1(a1, b1) + \u03b2\u03c9(a1, b1)\u22a5\n                                                  (a3, b3) = \u03b1(a2, b2) + \u03b2\u03c92(a2, b2)\u22a5\n                                                                               .\n                                                                               .\n                                                                               .\n                                   (ak\u22121, bk\u22121) = \u03b1(ak\u22122, bk\u22122) + \u03b2\u03c9k\u22121(ak\u22122, bk\u22122)\u22a5\n                                 (1, 0) = (a0, b0) = \u03b1(ak\u22121, bk\u22121) + \u03b2\u03c9k(ak\u22121, bk\u22121)\u22a5.\nRepresenting each vector (a, b) \u2208                        C2 as a quaternion via (a, b)  \u2192                      a + bj, it follows from direct\ncomputation that (a, b)\u22a5                   \u2192    j(a + bj) and, for \u03d5 \u2208                  C, \u03d5(a, b)  \u2192           \u03d5(a + bj) = (a + bj)\u03d5. In\nparticular, multiplication on the left by a complex number is unambiguous.\nOur recurrence relation can be expressed as\n                                                       a1 + b1j = (\u03b1 + \u03b2j)(1 + 0j)\n                                                    a2 + b2j = (\u03b1 + \u03b2\u03c9j)(a1 + b1j)\n                                                   a3 + b3j = (\u03b1 + \u03b2\u03c92j)(a2 + b2j)\n                                                                               .\n                                                                               .\n                                                                               .\n                                         ak\u22121 + bk\u22121j = (\u03b1 + \u03b2\u03c9k\u22122j)(ak\u22122 + bk\u22122j)\n                                                 1 + 0j = (\u03b1 + \u03b2\u03c9k)(ak\u22121 + bk\u22121j).\nIn other words, the vectors must be the partial products\n                                (am, bm) = (\u03b1 + \u03b2\u03c9m\u22121j)(\u03b1 + \u03b2\u03c9m\u22121j) . . . (\u03b1 + \u03b2\u03c90j)\nfor m = 0, . . . , k \u2212          1 (which is the empty product of m = 0), where \u03b1, \u03b2 satisfy the equation\n                                       1 = (\u03b1 + \u03b2\u03c9k\u22121j)(\u03b1 + \u03b2\u03c9k\u22122j) . . . (\u03b1 + \u03b2\u03c90j).\nWe now show that any sequence of vectors (a0, b0), . . . , (ak\u22121, bk\u22121) in a solution to the above\nsystem of equations can be obtained by slightly modifying a solution in the case \u03c9 = 1 (possibly\nwith a different value of k).\nLet \u03b6k be a (not necessarily primitive) complex kth root of unity. Consider the sequence\n                            (a0, b0), (a1\u03b6k, b1\u03b6k), (a2\u03b62              k, b2\u03b62  k), . . . , (ak\u22121\u03b6k\u22121   k     , bk\u22121\u03b6k\u22121).k\n                                                                              7", "md": "Given this characterization of P's critical points, our next result computes the boundary points of P's image, from which we obtain the entire image of P as a consequence.\n\nTheorem 6. Let $$P(\\vec{v}_0, ..., \\vec{v}_{k-1}) = \\langle \\vec{v}_0, \\vec{v}_1 \\rangle \\langle \\vec{v}_1, \\vec{v}_2 \\rangle ... \\langle \\vec{v}_{k-2}, \\vec{v}_{k-1} \\rangle \\langle \\vec{v}_{k-1}, \\vec{v}_0 \\rangle$$ and let \u0393k be the range of P as each $\\vec{v}_i$ ranges over unit vectors in Cd. Then\n\n$$\n\\Gamma_k = \\{re^{i\\theta} \\, | \\, \\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} \\geq \\cos\\theta - \\frac{k\\pi}{k}, r \\geq 0, \\theta \\in [0, 2\\pi]\\},\n$$\n\nwhich is a filled sinusoidal spiral.\n\nProof. We show this result for vectors in C2. In the supplementary material, we show that increasing the dimension does not affect the range, so considering vectors in C2 is sufficient. The result is trivial for $k \\leq 2$, so we will assume $k \\geq 3$.\n\nThe domain of P is compact, so \u0393k is also compact. P is also smooth (as a function $R^4 \\rightarrow R^2$), so its boundary consists of critical values. We will give a complete description of the critical values and critical points and show that the outermost critical values form a sinusoidal spiral, which is enough to show that \u0393k is contained in the claimed region. We defer showing that \u0393k contains the entire filled spiral to the supplementary material.\n\nFix a critical value c of P and let $a_i, b_i$ be as in Lemma 5 above. As $(a_m, b_m)$ and $(a_m, b_m)^\\perp$ are orthogonal, they form a basis for C2, so letting $\\alpha = a_0, \\beta = b_0$, we get a recurrence relation\n\n$$\n\\begin{align*}\n(a_1, b_1) & = \\alpha(1, 0) + \\beta(1, 0)^\\perp \\\\\n(a_2, b_2) & = \\alpha(a_1, b_1) + \\beta\\omega(a_1, b_1)^\\perp \\\\\n(a_3, b_3) & = \\alpha(a_2, b_2) + \\beta\\omega^2(a_2, b_2)^\\perp \\\\\n& \\ldots \\\\\n(a_{k-1}, b_{k-1}) & = \\alpha(a_{k-2}, b_{k-2}) + \\beta\\omega^{k-1}(a_{k-2}, b_{k-2})^\\perp \\\\\n(1, 0) & = (a_0, b_0) = \\alpha(a_{k-1}, b_{k-1}) + \\beta\\omega^k(a_{k-1}, b_{k-1})^\\perp.\n\\end{align*}\n$$\n\nRepresenting each vector $(a, b) \\in C2$ as a quaternion via $(a, b) \\rightarrow a + bj$, it follows from direct computation that $(a, b)^\\perp \\rightarrow j(a + bj)$ and, for $\\phi \\in C$, $\\phi(a, b) \\rightarrow \\phi(a + bj) = (a + bj)\\phi$. In particular, multiplication on the left by a complex number is unambiguous.\n\nOur recurrence relation can be expressed as\n\n$$\n\\begin{align*}\na_1 + b_1j & = (\\alpha + \\beta j)(1 + 0j) \\\\\na_2 + b_2j & = (\\alpha + \\beta\\omega j)(a_1 + b_1j) \\\\\na_3 + b_3j & = (\\alpha + \\beta\\omega^2 j)(a_2 + b_2j) \\\\\n& \\ldots \\\\\na_{k-1} + b_{k-1}j & = (\\alpha + \\beta\\omega^{k-2}j)(a_{k-2} + b_{k-2}j) \\\\\n1 + 0j & = (\\alpha + \\beta\\omega^k)(a_{k-1} + b_{k-1}j).\n\\end{align*}\n$$\n\nIn other words, the vectors must be the partial products\n\n$$\n(am, bm) = (\\alpha + \\beta\\omega^{m-1}j)(\\alpha + \\beta\\omega^{m-1}j) \\ldots (\\alpha + \\beta\\omega^0j)\n$$\n\nfor $m = 0, ..., k - 1$ (which is the empty product of $m = 0$), where $\\alpha, \\beta$ satisfy the equation\n\n$$\n1 = (\\alpha + \\beta\\omega^{k-1}j)(\\alpha + \\beta\\omega^{k-2}j) \\ldots (\\alpha + \\beta\\omega^0j).\n$$\n\nWe now show that any sequence of vectors $(a_0, b_0), ..., (a_{k-1}, b_{k-1})$ in a solution to the above system of equations can be obtained by slightly modifying a solution in the case $\\omega = 1$ (possibly with a different value of $k$).\n\nLet $\\zeta_k$ be a (not necessarily primitive) complex $k$th root of unity. Consider the sequence\n\n$$(a_0, b_0), (a_1\\zeta_k, b_1\\zeta_k), (a_2\\zeta_k^2, b_2\\zeta_k^2), ..., (a_{k-1}\\zeta_k^{k-1}, b_{k-1}\\zeta_k^{k-1}).$$", "images": [], "items": [{"type": "text", "value": "Given this characterization of P's critical points, our next result computes the boundary points of P's image, from which we obtain the entire image of P as a consequence.\n\nTheorem 6. Let $$P(\\vec{v}_0, ..., \\vec{v}_{k-1}) = \\langle \\vec{v}_0, \\vec{v}_1 \\rangle \\langle \\vec{v}_1, \\vec{v}_2 \\rangle ... \\langle \\vec{v}_{k-2}, \\vec{v}_{k-1} \\rangle \\langle \\vec{v}_{k-1}, \\vec{v}_0 \\rangle$$ and let \u0393k be the range of P as each $\\vec{v}_i$ ranges over unit vectors in Cd. Then\n\n$$\n\\Gamma_k = \\{re^{i\\theta} \\, | \\, \\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} \\geq \\cos\\theta - \\frac{k\\pi}{k}, r \\geq 0, \\theta \\in [0, 2\\pi]\\},\n$$\n\nwhich is a filled sinusoidal spiral.\n\nProof. We show this result for vectors in C2. In the supplementary material, we show that increasing the dimension does not affect the range, so considering vectors in C2 is sufficient. The result is trivial for $k \\leq 2$, so we will assume $k \\geq 3$.\n\nThe domain of P is compact, so \u0393k is also compact. P is also smooth (as a function $R^4 \\rightarrow R^2$), so its boundary consists of critical values. We will give a complete description of the critical values and critical points and show that the outermost critical values form a sinusoidal spiral, which is enough to show that \u0393k is contained in the claimed region. We defer showing that \u0393k contains the entire filled spiral to the supplementary material.\n\nFix a critical value c of P and let $a_i, b_i$ be as in Lemma 5 above. As $(a_m, b_m)$ and $(a_m, b_m)^\\perp$ are orthogonal, they form a basis for C2, so letting $\\alpha = a_0, \\beta = b_0$, we get a recurrence relation\n\n$$\n\\begin{align*}\n(a_1, b_1) & = \\alpha(1, 0) + \\beta(1, 0)^\\perp \\\\\n(a_2, b_2) & = \\alpha(a_1, b_1) + \\beta\\omega(a_1, b_1)^\\perp \\\\\n(a_3, b_3) & = \\alpha(a_2, b_2) + \\beta\\omega^2(a_2, b_2)^\\perp \\\\\n& \\ldots \\\\\n(a_{k-1}, b_{k-1}) & = \\alpha(a_{k-2}, b_{k-2}) + \\beta\\omega^{k-1}(a_{k-2}, b_{k-2})^\\perp \\\\\n(1, 0) & = (a_0, b_0) = \\alpha(a_{k-1}, b_{k-1}) + \\beta\\omega^k(a_{k-1}, b_{k-1})^\\perp.\n\\end{align*}\n$$\n\nRepresenting each vector $(a, b) \\in C2$ as a quaternion via $(a, b) \\rightarrow a + bj$, it follows from direct computation that $(a, b)^\\perp \\rightarrow j(a + bj)$ and, for $\\phi \\in C$, $\\phi(a, b) \\rightarrow \\phi(a + bj) = (a + bj)\\phi$. In particular, multiplication on the left by a complex number is unambiguous.\n\nOur recurrence relation can be expressed as\n\n$$\n\\begin{align*}\na_1 + b_1j & = (\\alpha + \\beta j)(1 + 0j) \\\\\na_2 + b_2j & = (\\alpha + \\beta\\omega j)(a_1 + b_1j) \\\\\na_3 + b_3j & = (\\alpha + \\beta\\omega^2 j)(a_2 + b_2j) \\\\\n& \\ldots \\\\\na_{k-1} + b_{k-1}j & = (\\alpha + \\beta\\omega^{k-2}j)(a_{k-2} + b_{k-2}j) \\\\\n1 + 0j & = (\\alpha + \\beta\\omega^k)(a_{k-1} + b_{k-1}j).\n\\end{align*}\n$$\n\nIn other words, the vectors must be the partial products\n\n$$\n(am, bm) = (\\alpha + \\beta\\omega^{m-1}j)(\\alpha + \\beta\\omega^{m-1}j) \\ldots (\\alpha + \\beta\\omega^0j)\n$$\n\nfor $m = 0, ..., k - 1$ (which is the empty product of $m = 0$), where $\\alpha, \\beta$ satisfy the equation\n\n$$\n1 = (\\alpha + \\beta\\omega^{k-1}j)(\\alpha + \\beta\\omega^{k-2}j) \\ldots (\\alpha + \\beta\\omega^0j).\n$$\n\nWe now show that any sequence of vectors $(a_0, b_0), ..., (a_{k-1}, b_{k-1})$ in a solution to the above system of equations can be obtained by slightly modifying a solution in the case $\\omega = 1$ (possibly with a different value of $k$).\n\nLet $\\zeta_k$ be a (not necessarily primitive) complex $k$th root of unity. Consider the sequence\n\n$$(a_0, b_0), (a_1\\zeta_k, b_1\\zeta_k), (a_2\\zeta_k^2, b_2\\zeta_k^2), ..., (a_{k-1}\\zeta_k^{k-1}, b_{k-1}\\zeta_k^{k-1}).$$", "md": "Given this characterization of P's critical points, our next result computes the boundary points of P's image, from which we obtain the entire image of P as a consequence.\n\nTheorem 6. Let $$P(\\vec{v}_0, ..., \\vec{v}_{k-1}) = \\langle \\vec{v}_0, \\vec{v}_1 \\rangle \\langle \\vec{v}_1, \\vec{v}_2 \\rangle ... \\langle \\vec{v}_{k-2}, \\vec{v}_{k-1} \\rangle \\langle \\vec{v}_{k-1}, \\vec{v}_0 \\rangle$$ and let \u0393k be the range of P as each $\\vec{v}_i$ ranges over unit vectors in Cd. Then\n\n$$\n\\Gamma_k = \\{re^{i\\theta} \\, | \\, \\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} \\geq \\cos\\theta - \\frac{k\\pi}{k}, r \\geq 0, \\theta \\in [0, 2\\pi]\\},\n$$\n\nwhich is a filled sinusoidal spiral.\n\nProof. We show this result for vectors in C2. In the supplementary material, we show that increasing the dimension does not affect the range, so considering vectors in C2 is sufficient. The result is trivial for $k \\leq 2$, so we will assume $k \\geq 3$.\n\nThe domain of P is compact, so \u0393k is also compact. P is also smooth (as a function $R^4 \\rightarrow R^2$), so its boundary consists of critical values. We will give a complete description of the critical values and critical points and show that the outermost critical values form a sinusoidal spiral, which is enough to show that \u0393k is contained in the claimed region. We defer showing that \u0393k contains the entire filled spiral to the supplementary material.\n\nFix a critical value c of P and let $a_i, b_i$ be as in Lemma 5 above. As $(a_m, b_m)$ and $(a_m, b_m)^\\perp$ are orthogonal, they form a basis for C2, so letting $\\alpha = a_0, \\beta = b_0$, we get a recurrence relation\n\n$$\n\\begin{align*}\n(a_1, b_1) & = \\alpha(1, 0) + \\beta(1, 0)^\\perp \\\\\n(a_2, b_2) & = \\alpha(a_1, b_1) + \\beta\\omega(a_1, b_1)^\\perp \\\\\n(a_3, b_3) & = \\alpha(a_2, b_2) + \\beta\\omega^2(a_2, b_2)^\\perp \\\\\n& \\ldots \\\\\n(a_{k-1}, b_{k-1}) & = \\alpha(a_{k-2}, b_{k-2}) + \\beta\\omega^{k-1}(a_{k-2}, b_{k-2})^\\perp \\\\\n(1, 0) & = (a_0, b_0) = \\alpha(a_{k-1}, b_{k-1}) + \\beta\\omega^k(a_{k-1}, b_{k-1})^\\perp.\n\\end{align*}\n$$\n\nRepresenting each vector $(a, b) \\in C2$ as a quaternion via $(a, b) \\rightarrow a + bj$, it follows from direct computation that $(a, b)^\\perp \\rightarrow j(a + bj)$ and, for $\\phi \\in C$, $\\phi(a, b) \\rightarrow \\phi(a + bj) = (a + bj)\\phi$. In particular, multiplication on the left by a complex number is unambiguous.\n\nOur recurrence relation can be expressed as\n\n$$\n\\begin{align*}\na_1 + b_1j & = (\\alpha + \\beta j)(1 + 0j) \\\\\na_2 + b_2j & = (\\alpha + \\beta\\omega j)(a_1 + b_1j) \\\\\na_3 + b_3j & = (\\alpha + \\beta\\omega^2 j)(a_2 + b_2j) \\\\\n& \\ldots \\\\\na_{k-1} + b_{k-1}j & = (\\alpha + \\beta\\omega^{k-2}j)(a_{k-2} + b_{k-2}j) \\\\\n1 + 0j & = (\\alpha + \\beta\\omega^k)(a_{k-1} + b_{k-1}j).\n\\end{align*}\n$$\n\nIn other words, the vectors must be the partial products\n\n$$\n(am, bm) = (\\alpha + \\beta\\omega^{m-1}j)(\\alpha + \\beta\\omega^{m-1}j) \\ldots (\\alpha + \\beta\\omega^0j)\n$$\n\nfor $m = 0, ..., k - 1$ (which is the empty product of $m = 0$), where $\\alpha, \\beta$ satisfy the equation\n\n$$\n1 = (\\alpha + \\beta\\omega^{k-1}j)(\\alpha + \\beta\\omega^{k-2}j) \\ldots (\\alpha + \\beta\\omega^0j).\n$$\n\nWe now show that any sequence of vectors $(a_0, b_0), ..., (a_{k-1}, b_{k-1})$ in a solution to the above system of equations can be obtained by slightly modifying a solution in the case $\\omega = 1$ (possibly with a different value of $k$).\n\nLet $\\zeta_k$ be a (not necessarily primitive) complex $k$th root of unity. Consider the sequence\n\n$$(a_0, b_0), (a_1\\zeta_k, b_1\\zeta_k), (a_2\\zeta_k^2, b_2\\zeta_k^2), ..., (a_{k-1}\\zeta_k^{k-1}, b_{k-1}\\zeta_k^{k-1}).$$"}]}, {"page": 8, "text": "This also satisfies the two conditions in Lemma 5, so we can run the same logic as above. The\ncorresponding change in the resulting system is that this multiplies \u03b1, \u03b2 by \u03b6k and \u03c9 by \u03b62                   k, so by\nmultiplying each vector by these complex units we may freely modify \u03c9 by \u03b62                    k for any kth root of\nunity \u03b6k, or equivalently any power of e2\u03c0i/k.\nIf   k   is  odd,    this   is   enough     to   show     that   any    nonzero     critical   value    can    be   ob-\ntained    by    \u03c9    =      1,   and    furthermore      that   any    critical   point   can    be   expressed      as\n(a0, b0), (a1\u03b6k, b1\u03b6k), (a2\u03b62   k, b2\u03b62k), . . . , (ak\u22121\u03b6k\u22121  , bk\u22121\u03b6k\u22121) for some kth root of unity \u03b6k.\n                                                         k            k\nIf k is even, let \u03b62k be a 2kth root of unity. Consider the sequence\n   (a0, b0), (a1\u03b62k, b1\u03b62k), (a2\u03b62    2k, b2\u03b622k), . . . , (ak\u22121\u03b6k\u22121\n                                       (a0\u03b6k2k, b0\u03b6k2k), (a1\u03b6k+1 2k , bk\u22121\u03b6k\u221212k ),                 , bk\u22121\u03b62k\u22121).\n                                                               2k , b1\u03b6k+1\n                                                                         2k ), . . . , (ak\u22121\u03b62k\u22121\n                                                                                              2k            2k\nThis satisfies the conditions in Lemma 5 for 2k in place of k. The corresponding values of \u03b1, \u03b2 in the\nresulting system are the same as in the original multiplied by \u03b62k, and the resulting value of \u03c9 is the\nsame as the original multiplied by \u03b62      2k. As \u03b62 2k can be chosen to be \u03c9\u22121, we can reduce to the case\n                   1 = (\u03b62k(\u03b1 + \u03b2\u03c92k\u22121j))(\u03b62k(\u03b1 + \u03b2\u03c92k\u22122j)) . . . (\u03b62k(\u03b1 + \u03b2\u03c90j))\nwhere \u03c9 = 1, or equivalently                  1 = (\u03b62k(\u03b1 + \u03b2j))2k.\nRemark 7. These reductions can also be interpreted using quaternions, which we demonstrate in the\nsupplementary material.\nFor any integer m, the mth roots of unity in H are precisely those numbers whose real parts are the\nsame as those of the real parts of the mth roots of unity in C. So (\u03b1 + \u03b2j)m = 1 if and only if \u03b1 lies\non one of the vertical diagonals of the m-gon whose vertices are the mth roots of unity (where the\nvertical diagonals are those with constant real part, including sides of the n-gon).\nTherefore, the values of \u03b1 in the phase-shifted sequences are vertical diagonals in the regular k-gon\nif k is odd and every other vertical diagonal in the regular 2k-gon if k is even, where the vertices are\nthe corresponding roots of unity. Inverting the phase shift gives that, regardless of whether k is even\nor odd, the values of \u03b1 in the solutions to\n                              1 = (\u03b1 + \u03b2\u03c9k\u22121j)(\u03b1 + \u03b2\u03c9k\u22122j) . . . (\u03b1 + \u03b2\u03c90j)\nform the diagonals (including the sides) of the k-gon whose vertices are the kth roots of unity.\nAs the inner product of consecutive vectors at such a sequence is \u03b1, the value of P at such a sequence\nis \u03b1k. Therefore the critical values of f are then kth powers of these diagonals. Furthermore, when\nk is odd the critical points are sequences of the form (am, bm) = \u03d5m(\u03b1 + \u03b2j)m where \u03b1 + \u03b2j is a\nquaternionic kth root of unity and \u03d5m are units in C; When k is even, the critical points are sequences\nof the form (am, bm) = \u03d5m(\u03b1 + \u03b2j)m where \u03b1 + \u03b2j are quaternionic 2kth roots of unity and \u03d5m\nare units in C,\nBy the maximum principle for holomorphic maps, the outer boundary of \u0393k is obtained by taking\nthe kth powers of the sides (where each side maps to the same curve). Using polar coordinates, the\nresulting curve is the sinusoidal spiral cos( \u03c0     k )r\u22121/k = cos( \u03b8\u2212\u03c0  k ).\nThe same proof shows that all other critical points other than those that P sends to 0 arise from the\nother diagonals of the polygons and their corresponding critical values are their kth powers, which\nare also sinusoidal spirals. A computation shows that a sequence of vectors that P sends to 0 is a\ncritical point if and only if at least two of the inner products are 0.\n3    Application to cyclic forgetting bounds\nAs an immediate application of Theorem 6, we can control the numerical range of a product of k\northogonal projections.\nCorollary 8. Let A = PkPk\u22121 . . . P1 be a product of k orthogonal complex projections Cd \u2192                          Cd.\nThen W(A) \u2286       \u0393k+1.\n                                                           8", "md": "This also satisfies the two conditions in Lemma 5, so we can run the same logic as above. The corresponding change in the resulting system is that this multiplies $$\\alpha, \\beta$$ by $$\\zeta_k$$ and $$\\omega$$ by $$\\zeta_{2k}$$, so by multiplying each vector by these complex units we may freely modify $$\\omega$$ by $$\\zeta_{2k}$$ for any kth root of unity $$\\zeta_k$$, or equivalently any power of $$e^{2\\pi i/k}$$.\n\nIf $$k$$ is odd, this is enough to show that any nonzero critical value can be obtained by $$\\omega = 1$$, and furthermore that any critical point can be expressed as $$(a_0, b_0), (a_1\\zeta_k, b_1\\zeta_k), (a_2\\zeta_{2k}, b_2\\zeta_{2k}), ..., (a_{k-1}\\zeta_{k-1}, b_{k-1}\\zeta_{k-1})$$ for some kth root of unity $$\\zeta_k$$.\n\nIf $$k$$ is even, let $$\\zeta_{2k}$$ be a 2kth root of unity. Consider the sequence $$(a_0, b_0), (a_1\\zeta_{2k}, b_1\\zeta_{2k}), (a_2\\zeta_{2 2k}, b_2\\zeta_{22k}), ..., (a_{k-1}\\zeta_{k-1}, b_{k-1}\\zeta_{k-1})$$.\n\nThis satisfies the conditions in Lemma 5 for 2k in place of k. The corresponding values of $$\\alpha, \\beta$$ in the resulting system are the same as in the original multiplied by $$\\zeta_{2k}$$, and the resulting value of $$\\omega$$ is the same as the original multiplied by $$\\zeta_{2 2k}$$. As $$\\zeta_{2 2k}$$ can be chosen to be $$\\omega^{-1}$$, we can reduce to the case $$1 = (\\zeta_{2k}(\\alpha + \\beta\\omega^{2k-1}j))(\\zeta_{2k}(\\alpha + \\beta\\omega^{2k-2}j)) ... (\\zeta_{2k}(\\alpha + \\beta\\omega^0j))$$ where $$\\omega = 1$$, or equivalently $$1 = (\\zeta_{2k}(\\alpha + \\beta j))^2k$$.\n\nRemark 7. These reductions can also be interpreted using quaternions, which we demonstrate in the supplementary material.\n\nFor any integer $$m$$, the mth roots of unity in $$\\mathbb{H}$$ are precisely those numbers whose real parts are the same as those of the real parts of the mth roots of unity in $$\\mathbb{C}$$. So $$(\\alpha + \\beta j)^m = 1$$ if and only if $$\\alpha$$ lies on one of the vertical diagonals of the m-gon whose vertices are the mth roots of unity (where the vertical diagonals are those with constant real part, including sides of the n-gon).\n\nTherefore, the values of $$\\alpha$$ in the phase-shifted sequences are vertical diagonals in the regular k-gon if k is odd and every other vertical diagonal in the regular 2k-gon if k is even, where the vertices are the corresponding roots of unity. Inverting the phase shift gives that, regardless of whether k is even or odd, the values of $$\\alpha$$ in the solutions to $$1 = (\\alpha + \\beta\\omega^{k-1}j)(\\alpha + \\beta\\omega^{k-2}j) ... (\\alpha + \\beta\\omega^0j)$$ form the diagonals (including the sides) of the k-gon whose vertices are the kth roots of unity.\n\nAs the inner product of consecutive vectors at such a sequence is $$\\alpha$$, the value of $$P$$ at such a sequence is $$\\alpha^k$$. Therefore the critical values of $$f$$ are then kth powers of these diagonals. Furthermore, when $$k$$ is odd the critical points are sequences of the form $$(a_m, b_m) = \\phi_m(\\alpha + \\beta j)^m$$ where $$\\alpha + \\beta j$$ is a quaternionic kth root of unity and $$\\phi_m$$ are units in $$\\mathbb{C}$$. When $$k$$ is even, the critical points are sequences of the form $$(a_m, b_m) = \\phi_m(\\alpha + \\beta j)^m$$ where $$\\alpha + \\beta j$$ are quaternionic 2kth roots of unity and $$\\phi_m$$ are units in $$\\mathbb{C}$$.\n\nBy the maximum principle for holomorphic maps, the outer boundary of $$\\Gamma_k$$ is obtained by taking the kth powers of the sides (where each side maps to the same curve). Using polar coordinates, the resulting curve is the sinusoidal spiral $$\\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} = \\cos\\left(\\theta - \\frac{\\pi}{k}\\right)$$.\n\nThe same proof shows that all other critical points other than those that $$P$$ sends to 0 arise from the other diagonals of the polygons and their corresponding critical values are their kth powers, which are also sinusoidal spirals. A computation shows that a sequence of vectors that $$P$$ sends to 0 is a critical point if and only if at least two of the inner products are 0.\n\n### Application to cyclic forgetting bounds\n\nAs an immediate application of Theorem 6, we can control the numerical range of a product of k orthogonal projections.\n\nCorollary 8. Let $$A = P_kP_{k-1} ... P_1$$ be a product of k orthogonal complex projections $$\\mathbb{C}^d \\rightarrow \\mathbb{C}^d$$. Then $$W(A) \\subseteq \\Gamma_{k+1}$$.", "images": [], "items": [{"type": "text", "value": "This also satisfies the two conditions in Lemma 5, so we can run the same logic as above. The corresponding change in the resulting system is that this multiplies $$\\alpha, \\beta$$ by $$\\zeta_k$$ and $$\\omega$$ by $$\\zeta_{2k}$$, so by multiplying each vector by these complex units we may freely modify $$\\omega$$ by $$\\zeta_{2k}$$ for any kth root of unity $$\\zeta_k$$, or equivalently any power of $$e^{2\\pi i/k}$$.\n\nIf $$k$$ is odd, this is enough to show that any nonzero critical value can be obtained by $$\\omega = 1$$, and furthermore that any critical point can be expressed as $$(a_0, b_0), (a_1\\zeta_k, b_1\\zeta_k), (a_2\\zeta_{2k}, b_2\\zeta_{2k}), ..., (a_{k-1}\\zeta_{k-1}, b_{k-1}\\zeta_{k-1})$$ for some kth root of unity $$\\zeta_k$$.\n\nIf $$k$$ is even, let $$\\zeta_{2k}$$ be a 2kth root of unity. Consider the sequence $$(a_0, b_0), (a_1\\zeta_{2k}, b_1\\zeta_{2k}), (a_2\\zeta_{2 2k}, b_2\\zeta_{22k}), ..., (a_{k-1}\\zeta_{k-1}, b_{k-1}\\zeta_{k-1})$$.\n\nThis satisfies the conditions in Lemma 5 for 2k in place of k. The corresponding values of $$\\alpha, \\beta$$ in the resulting system are the same as in the original multiplied by $$\\zeta_{2k}$$, and the resulting value of $$\\omega$$ is the same as the original multiplied by $$\\zeta_{2 2k}$$. As $$\\zeta_{2 2k}$$ can be chosen to be $$\\omega^{-1}$$, we can reduce to the case $$1 = (\\zeta_{2k}(\\alpha + \\beta\\omega^{2k-1}j))(\\zeta_{2k}(\\alpha + \\beta\\omega^{2k-2}j)) ... (\\zeta_{2k}(\\alpha + \\beta\\omega^0j))$$ where $$\\omega = 1$$, or equivalently $$1 = (\\zeta_{2k}(\\alpha + \\beta j))^2k$$.\n\nRemark 7. These reductions can also be interpreted using quaternions, which we demonstrate in the supplementary material.\n\nFor any integer $$m$$, the mth roots of unity in $$\\mathbb{H}$$ are precisely those numbers whose real parts are the same as those of the real parts of the mth roots of unity in $$\\mathbb{C}$$. So $$(\\alpha + \\beta j)^m = 1$$ if and only if $$\\alpha$$ lies on one of the vertical diagonals of the m-gon whose vertices are the mth roots of unity (where the vertical diagonals are those with constant real part, including sides of the n-gon).\n\nTherefore, the values of $$\\alpha$$ in the phase-shifted sequences are vertical diagonals in the regular k-gon if k is odd and every other vertical diagonal in the regular 2k-gon if k is even, where the vertices are the corresponding roots of unity. Inverting the phase shift gives that, regardless of whether k is even or odd, the values of $$\\alpha$$ in the solutions to $$1 = (\\alpha + \\beta\\omega^{k-1}j)(\\alpha + \\beta\\omega^{k-2}j) ... (\\alpha + \\beta\\omega^0j)$$ form the diagonals (including the sides) of the k-gon whose vertices are the kth roots of unity.\n\nAs the inner product of consecutive vectors at such a sequence is $$\\alpha$$, the value of $$P$$ at such a sequence is $$\\alpha^k$$. Therefore the critical values of $$f$$ are then kth powers of these diagonals. Furthermore, when $$k$$ is odd the critical points are sequences of the form $$(a_m, b_m) = \\phi_m(\\alpha + \\beta j)^m$$ where $$\\alpha + \\beta j$$ is a quaternionic kth root of unity and $$\\phi_m$$ are units in $$\\mathbb{C}$$. When $$k$$ is even, the critical points are sequences of the form $$(a_m, b_m) = \\phi_m(\\alpha + \\beta j)^m$$ where $$\\alpha + \\beta j$$ are quaternionic 2kth roots of unity and $$\\phi_m$$ are units in $$\\mathbb{C}$$.\n\nBy the maximum principle for holomorphic maps, the outer boundary of $$\\Gamma_k$$ is obtained by taking the kth powers of the sides (where each side maps to the same curve). Using polar coordinates, the resulting curve is the sinusoidal spiral $$\\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} = \\cos\\left(\\theta - \\frac{\\pi}{k}\\right)$$.\n\nThe same proof shows that all other critical points other than those that $$P$$ sends to 0 arise from the other diagonals of the polygons and their corresponding critical values are their kth powers, which are also sinusoidal spirals. A computation shows that a sequence of vectors that $$P$$ sends to 0 is a critical point if and only if at least two of the inner products are 0.", "md": "This also satisfies the two conditions in Lemma 5, so we can run the same logic as above. The corresponding change in the resulting system is that this multiplies $$\\alpha, \\beta$$ by $$\\zeta_k$$ and $$\\omega$$ by $$\\zeta_{2k}$$, so by multiplying each vector by these complex units we may freely modify $$\\omega$$ by $$\\zeta_{2k}$$ for any kth root of unity $$\\zeta_k$$, or equivalently any power of $$e^{2\\pi i/k}$$.\n\nIf $$k$$ is odd, this is enough to show that any nonzero critical value can be obtained by $$\\omega = 1$$, and furthermore that any critical point can be expressed as $$(a_0, b_0), (a_1\\zeta_k, b_1\\zeta_k), (a_2\\zeta_{2k}, b_2\\zeta_{2k}), ..., (a_{k-1}\\zeta_{k-1}, b_{k-1}\\zeta_{k-1})$$ for some kth root of unity $$\\zeta_k$$.\n\nIf $$k$$ is even, let $$\\zeta_{2k}$$ be a 2kth root of unity. Consider the sequence $$(a_0, b_0), (a_1\\zeta_{2k}, b_1\\zeta_{2k}), (a_2\\zeta_{2 2k}, b_2\\zeta_{22k}), ..., (a_{k-1}\\zeta_{k-1}, b_{k-1}\\zeta_{k-1})$$.\n\nThis satisfies the conditions in Lemma 5 for 2k in place of k. The corresponding values of $$\\alpha, \\beta$$ in the resulting system are the same as in the original multiplied by $$\\zeta_{2k}$$, and the resulting value of $$\\omega$$ is the same as the original multiplied by $$\\zeta_{2 2k}$$. As $$\\zeta_{2 2k}$$ can be chosen to be $$\\omega^{-1}$$, we can reduce to the case $$1 = (\\zeta_{2k}(\\alpha + \\beta\\omega^{2k-1}j))(\\zeta_{2k}(\\alpha + \\beta\\omega^{2k-2}j)) ... (\\zeta_{2k}(\\alpha + \\beta\\omega^0j))$$ where $$\\omega = 1$$, or equivalently $$1 = (\\zeta_{2k}(\\alpha + \\beta j))^2k$$.\n\nRemark 7. These reductions can also be interpreted using quaternions, which we demonstrate in the supplementary material.\n\nFor any integer $$m$$, the mth roots of unity in $$\\mathbb{H}$$ are precisely those numbers whose real parts are the same as those of the real parts of the mth roots of unity in $$\\mathbb{C}$$. So $$(\\alpha + \\beta j)^m = 1$$ if and only if $$\\alpha$$ lies on one of the vertical diagonals of the m-gon whose vertices are the mth roots of unity (where the vertical diagonals are those with constant real part, including sides of the n-gon).\n\nTherefore, the values of $$\\alpha$$ in the phase-shifted sequences are vertical diagonals in the regular k-gon if k is odd and every other vertical diagonal in the regular 2k-gon if k is even, where the vertices are the corresponding roots of unity. Inverting the phase shift gives that, regardless of whether k is even or odd, the values of $$\\alpha$$ in the solutions to $$1 = (\\alpha + \\beta\\omega^{k-1}j)(\\alpha + \\beta\\omega^{k-2}j) ... (\\alpha + \\beta\\omega^0j)$$ form the diagonals (including the sides) of the k-gon whose vertices are the kth roots of unity.\n\nAs the inner product of consecutive vectors at such a sequence is $$\\alpha$$, the value of $$P$$ at such a sequence is $$\\alpha^k$$. Therefore the critical values of $$f$$ are then kth powers of these diagonals. Furthermore, when $$k$$ is odd the critical points are sequences of the form $$(a_m, b_m) = \\phi_m(\\alpha + \\beta j)^m$$ where $$\\alpha + \\beta j$$ is a quaternionic kth root of unity and $$\\phi_m$$ are units in $$\\mathbb{C}$$. When $$k$$ is even, the critical points are sequences of the form $$(a_m, b_m) = \\phi_m(\\alpha + \\beta j)^m$$ where $$\\alpha + \\beta j$$ are quaternionic 2kth roots of unity and $$\\phi_m$$ are units in $$\\mathbb{C}$$.\n\nBy the maximum principle for holomorphic maps, the outer boundary of $$\\Gamma_k$$ is obtained by taking the kth powers of the sides (where each side maps to the same curve). Using polar coordinates, the resulting curve is the sinusoidal spiral $$\\cos\\left(\\frac{\\pi}{k}\\right)r^{-1/k} = \\cos\\left(\\theta - \\frac{\\pi}{k}\\right)$$.\n\nThe same proof shows that all other critical points other than those that $$P$$ sends to 0 arise from the other diagonals of the polygons and their corresponding critical values are their kth powers, which are also sinusoidal spirals. A computation shows that a sequence of vectors that $$P$$ sends to 0 is a critical point if and only if at least two of the inner products are 0."}, {"type": "heading", "lvl": 3, "value": "Application to cyclic forgetting bounds", "md": "### Application to cyclic forgetting bounds"}, {"type": "text", "value": "As an immediate application of Theorem 6, we can control the numerical range of a product of k orthogonal projections.\n\nCorollary 8. Let $$A = P_kP_{k-1} ... P_1$$ be a product of k orthogonal complex projections $$\\mathbb{C}^d \\rightarrow \\mathbb{C}^d$$. Then $$W(A) \\subseteq \\Gamma_{k+1}$$.", "md": "As an immediate application of Theorem 6, we can control the numerical range of a product of k orthogonal projections.\n\nCorollary 8. Let $$A = P_kP_{k-1} ... P_1$$ be a product of k orthogonal complex projections $$\\mathbb{C}^d \\rightarrow \\mathbb{C}^d$$. Then $$W(A) \\subseteq \\Gamma_{k+1}$$."}]}, {"page": 9, "text": "Proof. We first observe a general fact. Let P : Cd \u2192                          Cd be an orthogonal projection and let \u20d7                 z \u2208  Cd.\n               P\u20d7z\nLet \u20d7 q =    \u2225P\u20d7 z\u2225. Then                                P\u20d7 z = \u20d7  q\u27e8\u20d7q, \u20d7\nNow let \u20d7    z be an arbitrary unit vector and let                       z\u27e9  = \u20d7 q\u20d7q\u2217\u20d7 z.\n                                                       \u20d7         PiPi\u22121 . . . P1\u20d7     z\nIterating the fact stated above,                  (\u20d7   qi =    \u2225PiPi\u22121 . . . P1\u20d7      z\u2225  .\n                                                   qi\u20d7q\u2217          q1\u20d7q\u2217   z = Pi . . . P1\u20d7    z,\n                                                        i ) . . . (\u20d7   1)\u20d7\nfor all i \u2208  \u20d7  [k]. Hence\n             z\u2217A\u20d7   z = \u20d7  z\u2217(\u20d7 qk\u20d7q\u2217    qk\u22121\u20d7  q\u2217              q1\u20d7q\u2217   z = \u27e8\u20d7  z, \u20d7qk\u27e9  \u27e8\u20d7qk, \u20d7qk\u22121\u27e9    . . . \u27e8\u20d7\n                                                                                                               q2, \u20d7q1\u27e9   \u27e8\u20d7\nso W    (A) \u2286      \u0393k+1.             k)(\u20d7         k\u22121) . . . (\u20d7      1)\u20d7                                                   q1, \u20d7z\u27e9  ,\nIn the supplementary material, we use our characterization of P \u22121(\u2202\u0393k+1) to characterize the pairs\n(A, \u20d7 z) such that \u20d7    z\u2217A\u20d7  z \u2208    \u2202\u0393k+1.\nWe now bound \u2225Am(I \u2212                   A)\u2225    uniformly over all linear maps A: Cn \u2192                         Cn that can be expressed\nas a product of k complex projections by applying the above bound on the numerical range into the\nresult of [CP17] that there exists a constant Q \u2264                       1 +    \u221a  2 such that\n                                                      \u2225p(A)\u2225      \u2264   Q     sup     |p(z)|\n                                                                         z\u2208W (A)\nfor all polynomials p. We defer the calculation to the supplementary material.\nLemma 9.                               sup   |zm(1 \u2212       z)| \u2264     k        4                            .\n                                      z\u2208\u0393k                          m   4   e\u03c02 + ok,m\u2192\u221e(1)\nA more precise version of this bound in which                         e\u03c02 can be improved in terms of k                m will be given in\nthe supplementary material.\nThis, combined with Crouzeix\u2019s inequality, gives Lemma 2, which we restate below more precisely,\nand hence Theorem 1.\nLemma 2. For any linear map A: Cn \u2192                            Cn expressible as a product of k complex projections,\n                                       \u2225Am(I \u2212        A)\u2225    \u2264    k        4                            Q\n                                                                  m      e\u03c02 + ok,m\u2192\u221e(1)\nwhere Q is the constant of Crouzeix\u2019s inequality as presented above.\n4      Real projections\nIn the application to forgetting, all relevant projections are real projections. Therefore one may ask\nwhether our bound can be improved by restricting to products of k real projections.\nAs we show in the supplementary material, the union of the numerical ranges is the same even if we\nrestrict to real orthogonal projections R4 \u2192                        R4, and it is possible to get the maximal asymptotic\ndecay rate on         Am \u2212       Am+1      , but any real or complex sequence of orthogonal projections attaining\na point on \u2202\u0393k in its numerical range can be orthogonally decomposed into invariant (under all\nprojections) subspaces U \u2295                V where \u2225Am\u20d7         u\u2225   \u2212    Am+1\u20d7     u    decays quickly on U. In particular, any\nindecomposable such sequence must have quickly decaying forgetting.\n5      Future Work\nIt is not clear that this yields a completely optimal bound for forgetting. Is it possible to improve\nO(T 2/m) to O(T/m) matching the lower bound of [Evr+22]?\nFinally one might wonder how rare the worst-case bound is. For example if one visits the T tasks\ncyclically, but in a random order is it possible to beat the O(T 2/m) bound, perhaps with some weak\ndependence on the ambient dimension?\n                                                                        9", "md": "# Math Equations and Text\n\n## Proof\n\nWe first observe a general fact. Let \\( P : \\mathbb{C}^d \\rightarrow \\mathbb{C}^d \\) be an orthogonal projection and let \\( \\vec{z} \\in \\mathbb{C}^d \\).\n\nLet \\( \\vec{q} = \\|\\|P\\vec{z}\\|\\|. \\) Then \\( P\\vec{z} = \\vec{q}\\langle\\vec{q}, \\vec{z}\\rangle \\).\n\nNow let \\( \\vec{z} \\) be an arbitrary unit vector and let \\( \\langle z \\rangle = \\vec{q}\\vec{q}^*\\vec{z} \\).\n\nIterating the fact stated above, \\( (\\vec{q}_i = \\|\\|P_iP_{i-1}...P_1\\vec{z}\\|\\| \\).\n\nFor all \\( i \\in [k] \\). Hence\n\n$$ z^*Az = \\vec{z}^*(\\vec{q}_k\\vec{q}^*...\\vec{q}_1\\vec{q}^*\\vec{z} = \\langle\\vec{z}, \\vec{q}_k\\rangle\\langle\\vec{q}_k, \\vec{q}_{k-1}\\rangle...\\langle\\vec{q}_2, \\vec{q}_1\\rangle\\langle\\vec{q}_1, \\vec{z}\\rangle, $$\nso \\( W(A) \\subseteq \\Gamma_{k+1} \\).\n\nIn the supplementary material, we use our characterization of \\( P^{-1}(\\partial\\Gamma_{k+1}) \\) to characterize the pairs \\( (A, \\vec{z}) \\) such that \\( \\vec{z}^*A\\vec{z} \\in \\partial\\Gamma_{k+1} \\).\n\nWe now bound \\( \\|Am(I - A)\\| \\) uniformly over all linear maps \\( A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n \\) that can be expressed as a product of \\( k \\) complex projections by applying the above bound on the numerical range into the result of [CP17] that there exists a constant \\( Q \\leq 1 + \\sqrt{2} \\) such that\n\n$$ \\|p(A)\\| \\leq Q \\sup_{z \\in W(A)} |p(z)| $$\nfor all polynomials \\( p \\). We defer the calculation to the supplementary material.\n\n### Lemma 9\n\n\\( \\sup_{z \\in \\Gamma_k} |z^m(1 - z)| \\leq k^4e^{\\pi^2 + o_{k,m\\rightarrow\\infty}(1)} \\).\n\nA more precise version of this bound in which \\( e^{\\pi^2} \\) can be improved in terms of \\( k \\) and \\( m \\) will be given in the supplementary material.\n\nThis, combined with Crouzeix\u2019s inequality, gives Lemma 2, which we restate below more precisely, and hence Theorem 1.\n\n### Lemma 2\n\nFor any linear map \\( A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n \\) expressible as a product of \\( k \\) complex projections,\n\n$$ \\|Am(I - A)\\| \\leq k^4Qe^{\\pi^2 + o_{k,m\\rightarrow\\infty}(1)} $$\nwhere \\( Q \\) is the constant of Crouzeix\u2019s inequality as presented above.\n\n### Real projections\n\nIn the application to forgetting, all relevant projections are real projections. Therefore one may ask whether our bound can be improved by restricting to products of \\( k \\) real projections.\n\nAs we show in the supplementary material, the union of the numerical ranges is the same even if we restrict to real orthogonal projections \\( \\mathbb{R}^4 \\rightarrow \\mathbb{R}^4 \\), and it is possible to get the maximal asymptotic decay rate on \\( Am - Am+1 \\), but any real or complex sequence of orthogonal projections attaining a point on \\( \\partial\\Gamma_k \\) in its numerical range can be orthogonally decomposed into invariant (under all projections) subspaces \\( U \\oplus V \\) where \\( \\|Am\\vec{u} - Am+1\\vec{u} \\) decays quickly on \\( U \\). In particular, any indecomposable such sequence must have quickly decaying forgetting.\n\n### Future Work\n\nIt is not clear that this yields a completely optimal bound for forgetting. Is it possible to improve \\( O(T^2/m) \\) to \\( O(T/m) \\) matching the lower bound of [Evr+22]?\n\nFinally one might wonder how rare the worst-case bound is. For example if one visits the \\( T \\) tasks cyclically, but in a random order is it possible to beat the \\( O(T^2/m) \\) bound, perhaps with some weak dependence on the ambient dimension?", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Proof", "md": "## Proof"}, {"type": "text", "value": "We first observe a general fact. Let \\( P : \\mathbb{C}^d \\rightarrow \\mathbb{C}^d \\) be an orthogonal projection and let \\( \\vec{z} \\in \\mathbb{C}^d \\).\n\nLet \\( \\vec{q} = \\|\\|P\\vec{z}\\|\\|. \\) Then \\( P\\vec{z} = \\vec{q}\\langle\\vec{q}, \\vec{z}\\rangle \\).\n\nNow let \\( \\vec{z} \\) be an arbitrary unit vector and let \\( \\langle z \\rangle = \\vec{q}\\vec{q}^*\\vec{z} \\).\n\nIterating the fact stated above, \\( (\\vec{q}_i = \\|\\|P_iP_{i-1}...P_1\\vec{z}\\|\\| \\).\n\nFor all \\( i \\in [k] \\). Hence\n\n$$ z^*Az = \\vec{z}^*(\\vec{q}_k\\vec{q}^*...\\vec{q}_1\\vec{q}^*\\vec{z} = \\langle\\vec{z}, \\vec{q}_k\\rangle\\langle\\vec{q}_k, \\vec{q}_{k-1}\\rangle...\\langle\\vec{q}_2, \\vec{q}_1\\rangle\\langle\\vec{q}_1, \\vec{z}\\rangle, $$\nso \\( W(A) \\subseteq \\Gamma_{k+1} \\).\n\nIn the supplementary material, we use our characterization of \\( P^{-1}(\\partial\\Gamma_{k+1}) \\) to characterize the pairs \\( (A, \\vec{z}) \\) such that \\( \\vec{z}^*A\\vec{z} \\in \\partial\\Gamma_{k+1} \\).\n\nWe now bound \\( \\|Am(I - A)\\| \\) uniformly over all linear maps \\( A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n \\) that can be expressed as a product of \\( k \\) complex projections by applying the above bound on the numerical range into the result of [CP17] that there exists a constant \\( Q \\leq 1 + \\sqrt{2} \\) such that\n\n$$ \\|p(A)\\| \\leq Q \\sup_{z \\in W(A)} |p(z)| $$\nfor all polynomials \\( p \\). We defer the calculation to the supplementary material.", "md": "We first observe a general fact. Let \\( P : \\mathbb{C}^d \\rightarrow \\mathbb{C}^d \\) be an orthogonal projection and let \\( \\vec{z} \\in \\mathbb{C}^d \\).\n\nLet \\( \\vec{q} = \\|\\|P\\vec{z}\\|\\|. \\) Then \\( P\\vec{z} = \\vec{q}\\langle\\vec{q}, \\vec{z}\\rangle \\).\n\nNow let \\( \\vec{z} \\) be an arbitrary unit vector and let \\( \\langle z \\rangle = \\vec{q}\\vec{q}^*\\vec{z} \\).\n\nIterating the fact stated above, \\( (\\vec{q}_i = \\|\\|P_iP_{i-1}...P_1\\vec{z}\\|\\| \\).\n\nFor all \\( i \\in [k] \\). Hence\n\n$$ z^*Az = \\vec{z}^*(\\vec{q}_k\\vec{q}^*...\\vec{q}_1\\vec{q}^*\\vec{z} = \\langle\\vec{z}, \\vec{q}_k\\rangle\\langle\\vec{q}_k, \\vec{q}_{k-1}\\rangle...\\langle\\vec{q}_2, \\vec{q}_1\\rangle\\langle\\vec{q}_1, \\vec{z}\\rangle, $$\nso \\( W(A) \\subseteq \\Gamma_{k+1} \\).\n\nIn the supplementary material, we use our characterization of \\( P^{-1}(\\partial\\Gamma_{k+1}) \\) to characterize the pairs \\( (A, \\vec{z}) \\) such that \\( \\vec{z}^*A\\vec{z} \\in \\partial\\Gamma_{k+1} \\).\n\nWe now bound \\( \\|Am(I - A)\\| \\) uniformly over all linear maps \\( A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n \\) that can be expressed as a product of \\( k \\) complex projections by applying the above bound on the numerical range into the result of [CP17] that there exists a constant \\( Q \\leq 1 + \\sqrt{2} \\) such that\n\n$$ \\|p(A)\\| \\leq Q \\sup_{z \\in W(A)} |p(z)| $$\nfor all polynomials \\( p \\). We defer the calculation to the supplementary material."}, {"type": "heading", "lvl": 3, "value": "Lemma 9", "md": "### Lemma 9"}, {"type": "text", "value": "\\( \\sup_{z \\in \\Gamma_k} |z^m(1 - z)| \\leq k^4e^{\\pi^2 + o_{k,m\\rightarrow\\infty}(1)} \\).\n\nA more precise version of this bound in which \\( e^{\\pi^2} \\) can be improved in terms of \\( k \\) and \\( m \\) will be given in the supplementary material.\n\nThis, combined with Crouzeix\u2019s inequality, gives Lemma 2, which we restate below more precisely, and hence Theorem 1.", "md": "\\( \\sup_{z \\in \\Gamma_k} |z^m(1 - z)| \\leq k^4e^{\\pi^2 + o_{k,m\\rightarrow\\infty}(1)} \\).\n\nA more precise version of this bound in which \\( e^{\\pi^2} \\) can be improved in terms of \\( k \\) and \\( m \\) will be given in the supplementary material.\n\nThis, combined with Crouzeix\u2019s inequality, gives Lemma 2, which we restate below more precisely, and hence Theorem 1."}, {"type": "heading", "lvl": 3, "value": "Lemma 2", "md": "### Lemma 2"}, {"type": "text", "value": "For any linear map \\( A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n \\) expressible as a product of \\( k \\) complex projections,\n\n$$ \\|Am(I - A)\\| \\leq k^4Qe^{\\pi^2 + o_{k,m\\rightarrow\\infty}(1)} $$\nwhere \\( Q \\) is the constant of Crouzeix\u2019s inequality as presented above.", "md": "For any linear map \\( A: \\mathbb{C}^n \\rightarrow \\mathbb{C}^n \\) expressible as a product of \\( k \\) complex projections,\n\n$$ \\|Am(I - A)\\| \\leq k^4Qe^{\\pi^2 + o_{k,m\\rightarrow\\infty}(1)} $$\nwhere \\( Q \\) is the constant of Crouzeix\u2019s inequality as presented above."}, {"type": "heading", "lvl": 3, "value": "Real projections", "md": "### Real projections"}, {"type": "text", "value": "In the application to forgetting, all relevant projections are real projections. Therefore one may ask whether our bound can be improved by restricting to products of \\( k \\) real projections.\n\nAs we show in the supplementary material, the union of the numerical ranges is the same even if we restrict to real orthogonal projections \\( \\mathbb{R}^4 \\rightarrow \\mathbb{R}^4 \\), and it is possible to get the maximal asymptotic decay rate on \\( Am - Am+1 \\), but any real or complex sequence of orthogonal projections attaining a point on \\( \\partial\\Gamma_k \\) in its numerical range can be orthogonally decomposed into invariant (under all projections) subspaces \\( U \\oplus V \\) where \\( \\|Am\\vec{u} - Am+1\\vec{u} \\) decays quickly on \\( U \\). In particular, any indecomposable such sequence must have quickly decaying forgetting.", "md": "In the application to forgetting, all relevant projections are real projections. Therefore one may ask whether our bound can be improved by restricting to products of \\( k \\) real projections.\n\nAs we show in the supplementary material, the union of the numerical ranges is the same even if we restrict to real orthogonal projections \\( \\mathbb{R}^4 \\rightarrow \\mathbb{R}^4 \\), and it is possible to get the maximal asymptotic decay rate on \\( Am - Am+1 \\), but any real or complex sequence of orthogonal projections attaining a point on \\( \\partial\\Gamma_k \\) in its numerical range can be orthogonally decomposed into invariant (under all projections) subspaces \\( U \\oplus V \\) where \\( \\|Am\\vec{u} - Am+1\\vec{u} \\) decays quickly on \\( U \\). In particular, any indecomposable such sequence must have quickly decaying forgetting."}, {"type": "heading", "lvl": 3, "value": "Future Work", "md": "### Future Work"}, {"type": "text", "value": "It is not clear that this yields a completely optimal bound for forgetting. Is it possible to improve \\( O(T^2/m) \\) to \\( O(T/m) \\) matching the lower bound of [Evr+22]?\n\nFinally one might wonder how rare the worst-case bound is. For example if one visits the \\( T \\) tasks cyclically, but in a random order is it possible to beat the \\( O(T^2/m) \\) bound, perhaps with some weak dependence on the ambient dimension?", "md": "It is not clear that this yields a completely optimal bound for forgetting. Is it possible to improve \\( O(T^2/m) \\) to \\( O(T/m) \\) matching the lower bound of [Evr+22]?\n\nFinally one might wonder how rare the worst-case bound is. For example if one visits the \\( T \\) tasks cyclically, but in a random order is it possible to beat the \\( O(T^2/m) \\) bound, perhaps with some weak dependence on the ambient dimension?"}]}, {"page": 10, "text": "6   Acknowledgment\nThe authors would like to thank Itay Evron for interesting discussions about [Evr+22].\nReferences\n[Kar37]      Stefan Karczmarz. \u201cAngenaherte auflosung von systemen linearer glei-chungen\u201d. In:\n             Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat. (1937), pp. 355\u2013357.\n[Von49]      John Von Neumann. \u201cOn rings of operators. Reduction theory\u201d. In: Annals of Mathemat-\n             ics (1949), pp. 401\u2013485.\n[Hal62]      Israel Halperin. \u201cThe product of projection operators\u201d. In: Acta Sci. Math.(Szeged) 23.1\n             (1962), pp. 96\u201399.\n[Elf80]      Tommy Elfving. \u201cBlock-iterative methods for consistent and inconsistent linear equa-\n             tions\u201d. In: Numerische Mathematik 35 (1980), pp. 1\u201312.\n[TM95]       Sebastian Thrun and Tom M Mitchell. \u201cLifelong robot learning\u201d. In: Robotics and\n             autonomous systems 15.1-2 (1995), pp. 25\u201346.\n[Cro07]      Michel Crouzeix. \u201cNumerical range and functional calculus in Hilbert space\u201d. In: Journal\n             of Functional Analysis 244.2 (2007), pp. 668\u2013690.\n[Cro08]      Michel Crouzeix. \u201cA functional calculus based on the numerical range: applications\u201d.\n             In: Linear and Multilinear Algebra 56.1-2 (2008), pp. 81\u2013103.\n[SV09]       Thomas Strohmer and Roman Vershynin. \u201cA randomized Kaczmarz algorithm with\n             exponential convergence\u201d. In: Journal of Fourier Analysis and Applications 15.2 (2009),\n             p. 262.\n[NT14]       Deanna Needell and Joel A Tropp. \u201cPaved with good intentions: analysis of a randomized\n             block Kaczmarz method\u201d. In: Linear Algebra and its Applications 441 (2014), pp. 199\u2013\n             221.\n[BS16]       Catalin Badea and David Seifert. \u201cRitt operators and convergence in the method of\n             alternating projections\u201d. In: Journal of Approximation Theory 205 (2016), pp. 133\u2013148.\n[CP17]       Michel Crouzeix and C\u00e9sar Palencia. \u201cThe numerical range is a (1+2)-spectral set\u201d. In:\n             SIAM Journal on Matrix Analysis and Applications 38.2 (2017), pp. 649\u2013655.\n[CL18]       Zhiyuan Chen and Bing Liu. \u201cLifelong machine learning\u201d. In: Synthesis Lectures on\n             Artificial Intelligence and Machine Learning 12.3 (2018), pp. 1\u2013207.\n[Kem+18]     Ronald Kemker et al. \u201cMeasuring catastrophic forgetting in neural networks\u201d. In: Pro-\n             ceedings of the AAAI conference on artificial intelligence. Vol. 32. 1. 2018.\n[Par+19]     German I Parisi et al. \u201cContinual lifelong learning with neural networks: A review\u201d. In:\n             Neural networks 113 (2019), pp. 54\u201371.\n[VT19]       Gido M Van de Ven and Andreas S Tolias. \u201cThree scenarios for continual learning\u201d. In:\n             arXiv preprint arXiv:1904.07734 (2019).\n[Had+20]     Raia Hadsell et al. \u201cEmbracing change: Continual learning in deep neural networks\u201d. In:\n             Trends in cognitive sciences 24.12 (2020), pp. 1028\u20131040.\n[Evr+22]     Itay Evron et al. \u201cHow catastrophic can catastrophic forgetting be in linear regression?\u201d\n             In: Conference on Learning Theory. PMLR. 2022, pp. 4028\u20134079.\n[QR22]       Chao Qin and Daniel Russo. \u201cAdaptivity and confounding in multi-armed bandit experi-\n             ments\u201d. In: arXiv preprint arXiv:2202.09036 (2022).\n[Yan+22]     Yingxiang Yang et al. \u201cFourier Learning with Cyclical Data\u201d. In: International Confer-\n             ence on Machine Learning. PMLR. 2022, pp. 25280\u201325301.\n[Lin+23]     Sen Lin et al. \u201cTheory on Forgetting and Generalization of Continual Learning\u201d. In:\n             arXiv preprint arXiv:2302.05836 (2023).\n[RZ23]       Simeon Reich and Rafa\u0142 Zalas. \u201cPolynomial estimates for the method of cyclic projec-\n             tions in Hilbert spaces\u201d. In: Numerical Algorithms (2023), pp. 1\u201326.\n[Wan+23]     Liyuan Wang et al. \u201cA comprehensive survey of continual learning: Theory, method and\n             application\u201d. In: arXiv preprint arXiv:2302.00487 (2023).\n                                                  10", "md": "# Acknowledgment and References\n\n## Acknowledgment\n\nThe authors would like to thank Itay Evron for interesting discussions about $$[Evr+22]$$.\n\n## References\n\n|[Kar37]|Stefan Karczmarz. \"Angenaherte auflosung von systemen linearer glei-chungen\". In: Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat. (1937), pp. 355\u2013357.|\n|---|---|\n|[Von49]|John Von Neumann. \"On rings of operators. Reduction theory\". In: Annals of Mathematics (1949), pp. 401\u2013485.|\n|[Hal62]|Israel Halperin. \"The product of projection operators\". In: Acta Sci. Math.(Szeged) 23.1 (1962), pp. 96\u201399.|\n|[Elf80]|Tommy Elfving. \"Block-iterative methods for consistent and inconsistent linear equations\". In: Numerische Mathematik 35 (1980), pp. 1\u201312.|\n|[TM95]|Sebastian Thrun and Tom M Mitchell. \"Lifelong robot learning\". In: Robotics and autonomous systems 15.1-2 (1995), pp. 25\u201346.|\n|[Cro07]|Michel Crouzeix. \"Numerical range and functional calculus in Hilbert space\". In: Journal of Functional Analysis 244.2 (2007), pp. 668\u2013690.|\n|[Cro08]|Michel Crouzeix. \"A functional calculus based on the numerical range: applications\". In: Linear and Multilinear Algebra 56.1-2 (2008), pp. 81\u2013103.|\n|[SV09]|Thomas Strohmer and Roman Vershynin. \"A randomized Kaczmarz algorithm with exponential convergence\". In: Journal of Fourier Analysis and Applications 15.2 (2009), p. 262.|\n|[NT14]|Deanna Needell and Joel A Tropp. \"Paved with good intentions: analysis of a randomized block Kaczmarz method\". In: Linear Algebra and its Applications 441 (2014), pp. 199\u2013221.|\n|[BS16]|Catalin Badea and David Seifert. \"Ritt operators and convergence in the method of alternating projections\". In: Journal of Approximation Theory 205 (2016), pp. 133\u2013148.|\n|[CP17]|Michel Crouzeix and C\u00e9sar Palencia. \"The numerical range is a (1+2)-spectral set\". In: SIAM Journal on Matrix Analysis and Applications 38.2 (2017), pp. 649\u2013655.|\n|[CL18]|Zhiyuan Chen and Bing Liu. \"Lifelong machine learning\". In: Synthesis Lectures on Artificial Intelligence and Machine Learning 12.3 (2018), pp. 1\u2013207.|\n|[Kem+18]|Ronald Kemker et al. \"Measuring catastrophic forgetting in neural networks\". In: Proceedings of the AAAI conference on artificial intelligence. Vol. 32. 1. 2018.|\n|[Par+19]|German I Parisi et al. \"Continual lifelong learning with neural networks: A review\". In: Neural networks 113 (2019), pp. 54\u201371.|\n|[VT19]|Gido M Van de Ven and Andreas S Tolias. \"Three scenarios for continual learning\". In: arXiv preprint arXiv:1904.07734 (2019).|\n|[Had+20]|Raia Hadsell et al. \"Embracing change: Continual learning in deep neural networks\". In: Trends in cognitive sciences 24.12 (2020), pp. 1028\u20131040.|\n|[Evr+22]|Itay Evron et al. \"How catastrophic can catastrophic forgetting be in linear regression?\". In: Conference on Learning Theory. PMLR. 2022, pp. 4028\u20134079.|\n|[QR22]|Chao Qin and Daniel Russo. \"Adaptivity and confounding in multi-armed bandit experiments\". In: arXiv preprint arXiv:2202.09036 (2022).|\n|[Yan+22]|Yingxiang Yang et al. \"Fourier Learning with Cyclical Data\". In: International Conference on Machine Learning. PMLR. 2022, pp. 25280\u201325301.|\n|[Lin+23]|Sen Lin et al. \"Theory on Forgetting and Generalization of Continual Learning\". In: arXiv preprint arXiv:2302.05836 (2023).|\n|[RZ23]|Simeon Reich and Rafa\u0142 Zalas. \"Polynomial estimates for the method of cyclic projections in Hilbert spaces\". In: Numerical Algorithms (2023), pp. 1\u201326.|\n|[Wan+23]|Liyuan Wang et al. \"A comprehensive survey of continual learning: Theory, method and application\". In: arXiv preprint arXiv:2302.00487 (2023).|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgment and References", "md": "# Acknowledgment and References"}, {"type": "heading", "lvl": 2, "value": "Acknowledgment", "md": "## Acknowledgment"}, {"type": "text", "value": "The authors would like to thank Itay Evron for interesting discussions about $$[Evr+22]$$.", "md": "The authors would like to thank Itay Evron for interesting discussions about $$[Evr+22]$$."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "table", "rows": [["[Kar37]", "Stefan Karczmarz. \"Angenaherte auflosung von systemen linearer glei-chungen\". In: Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat. (1937), pp. 355\u2013357."], ["[Von49]", "John Von Neumann. \"On rings of operators. Reduction theory\". In: Annals of Mathematics (1949), pp. 401\u2013485."], ["[Hal62]", "Israel Halperin. \"The product of projection operators\". In: Acta Sci. Math.(Szeged) 23.1 (1962), pp. 96\u201399."], ["[Elf80]", "Tommy Elfving. \"Block-iterative methods for consistent and inconsistent linear equations\". In: Numerische Mathematik 35 (1980), pp. 1\u201312."], ["[TM95]", "Sebastian Thrun and Tom M Mitchell. \"Lifelong robot learning\". In: Robotics and autonomous systems 15.1-2 (1995), pp. 25\u201346."], ["[Cro07]", "Michel Crouzeix. \"Numerical range and functional calculus in Hilbert space\". In: Journal of Functional Analysis 244.2 (2007), pp. 668\u2013690."], ["[Cro08]", "Michel Crouzeix. \"A functional calculus based on the numerical range: applications\". In: Linear and Multilinear Algebra 56.1-2 (2008), pp. 81\u2013103."], ["[SV09]", "Thomas Strohmer and Roman Vershynin. \"A randomized Kaczmarz algorithm with exponential convergence\". In: Journal of Fourier Analysis and Applications 15.2 (2009), p. 262."], ["[NT14]", "Deanna Needell and Joel A Tropp. \"Paved with good intentions: analysis of a randomized block Kaczmarz method\". In: Linear Algebra and its Applications 441 (2014), pp. 199\u2013221."], ["[BS16]", "Catalin Badea and David Seifert. \"Ritt operators and convergence in the method of alternating projections\". In: Journal of Approximation Theory 205 (2016), pp. 133\u2013148."], ["[CP17]", "Michel Crouzeix and C\u00e9sar Palencia. \"The numerical range is a (1+2)-spectral set\". In: SIAM Journal on Matrix Analysis and Applications 38.2 (2017), pp. 649\u2013655."], ["[CL18]", "Zhiyuan Chen and Bing Liu. \"Lifelong machine learning\". In: Synthesis Lectures on Artificial Intelligence and Machine Learning 12.3 (2018), pp. 1\u2013207."], ["[Kem+18]", "Ronald Kemker et al. \"Measuring catastrophic forgetting in neural networks\". In: Proceedings of the AAAI conference on artificial intelligence. Vol. 32. 1. 2018."], ["[Par+19]", "German I Parisi et al. \"Continual lifelong learning with neural networks: A review\". In: Neural networks 113 (2019), pp. 54\u201371."], ["[VT19]", "Gido M Van de Ven and Andreas S Tolias. \"Three scenarios for continual learning\". In: arXiv preprint arXiv:1904.07734 (2019)."], ["[Had+20]", "Raia Hadsell et al. \"Embracing change: Continual learning in deep neural networks\". In: Trends in cognitive sciences 24.12 (2020), pp. 1028\u20131040."], ["[Evr+22]", "Itay Evron et al. \"How catastrophic can catastrophic forgetting be in linear regression?\". In: Conference on Learning Theory. PMLR. 2022, pp. 4028\u20134079."], ["[QR22]", "Chao Qin and Daniel Russo. \"Adaptivity and confounding in multi-armed bandit experiments\". In: arXiv preprint arXiv:2202.09036 (2022)."], ["[Yan+22]", "Yingxiang Yang et al. \"Fourier Learning with Cyclical Data\". In: International Conference on Machine Learning. PMLR. 2022, pp. 25280\u201325301."], ["[Lin+23]", "Sen Lin et al. \"Theory on Forgetting and Generalization of Continual Learning\". In: arXiv preprint arXiv:2302.05836 (2023)."], ["[RZ23]", "Simeon Reich and Rafa\u0142 Zalas. \"Polynomial estimates for the method of cyclic projections in Hilbert spaces\". In: Numerical Algorithms (2023), pp. 1\u201326."], ["[Wan+23]", "Liyuan Wang et al. \"A comprehensive survey of continual learning: Theory, method and application\". In: arXiv preprint arXiv:2302.00487 (2023)."]], "md": "|[Kar37]|Stefan Karczmarz. \"Angenaherte auflosung von systemen linearer glei-chungen\". In: Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat. (1937), pp. 355\u2013357.|\n|---|---|\n|[Von49]|John Von Neumann. \"On rings of operators. Reduction theory\". In: Annals of Mathematics (1949), pp. 401\u2013485.|\n|[Hal62]|Israel Halperin. \"The product of projection operators\". In: Acta Sci. Math.(Szeged) 23.1 (1962), pp. 96\u201399.|\n|[Elf80]|Tommy Elfving. \"Block-iterative methods for consistent and inconsistent linear equations\". In: Numerische Mathematik 35 (1980), pp. 1\u201312.|\n|[TM95]|Sebastian Thrun and Tom M Mitchell. \"Lifelong robot learning\". In: Robotics and autonomous systems 15.1-2 (1995), pp. 25\u201346.|\n|[Cro07]|Michel Crouzeix. \"Numerical range and functional calculus in Hilbert space\". In: Journal of Functional Analysis 244.2 (2007), pp. 668\u2013690.|\n|[Cro08]|Michel Crouzeix. \"A functional calculus based on the numerical range: applications\". In: Linear and Multilinear Algebra 56.1-2 (2008), pp. 81\u2013103.|\n|[SV09]|Thomas Strohmer and Roman Vershynin. \"A randomized Kaczmarz algorithm with exponential convergence\". In: Journal of Fourier Analysis and Applications 15.2 (2009), p. 262.|\n|[NT14]|Deanna Needell and Joel A Tropp. \"Paved with good intentions: analysis of a randomized block Kaczmarz method\". In: Linear Algebra and its Applications 441 (2014), pp. 199\u2013221.|\n|[BS16]|Catalin Badea and David Seifert. \"Ritt operators and convergence in the method of alternating projections\". In: Journal of Approximation Theory 205 (2016), pp. 133\u2013148.|\n|[CP17]|Michel Crouzeix and C\u00e9sar Palencia. \"The numerical range is a (1+2)-spectral set\". In: SIAM Journal on Matrix Analysis and Applications 38.2 (2017), pp. 649\u2013655.|\n|[CL18]|Zhiyuan Chen and Bing Liu. \"Lifelong machine learning\". In: Synthesis Lectures on Artificial Intelligence and Machine Learning 12.3 (2018), pp. 1\u2013207.|\n|[Kem+18]|Ronald Kemker et al. \"Measuring catastrophic forgetting in neural networks\". In: Proceedings of the AAAI conference on artificial intelligence. Vol. 32. 1. 2018.|\n|[Par+19]|German I Parisi et al. \"Continual lifelong learning with neural networks: A review\". In: Neural networks 113 (2019), pp. 54\u201371.|\n|[VT19]|Gido M Van de Ven and Andreas S Tolias. \"Three scenarios for continual learning\". In: arXiv preprint arXiv:1904.07734 (2019).|\n|[Had+20]|Raia Hadsell et al. \"Embracing change: Continual learning in deep neural networks\". In: Trends in cognitive sciences 24.12 (2020), pp. 1028\u20131040.|\n|[Evr+22]|Itay Evron et al. \"How catastrophic can catastrophic forgetting be in linear regression?\". In: Conference on Learning Theory. PMLR. 2022, pp. 4028\u20134079.|\n|[QR22]|Chao Qin and Daniel Russo. \"Adaptivity and confounding in multi-armed bandit experiments\". In: arXiv preprint arXiv:2202.09036 (2022).|\n|[Yan+22]|Yingxiang Yang et al. \"Fourier Learning with Cyclical Data\". In: International Conference on Machine Learning. PMLR. 2022, pp. 25280\u201325301.|\n|[Lin+23]|Sen Lin et al. \"Theory on Forgetting and Generalization of Continual Learning\". In: arXiv preprint arXiv:2302.05836 (2023).|\n|[RZ23]|Simeon Reich and Rafa\u0142 Zalas. \"Polynomial estimates for the method of cyclic projections in Hilbert spaces\". In: Numerical Algorithms (2023), pp. 1\u201326.|\n|[Wan+23]|Liyuan Wang et al. \"A comprehensive survey of continual learning: Theory, method and application\". In: arXiv preprint arXiv:2302.00487 (2023).|", "isPerfectTable": true, "csv": "\"[Kar37]\",\"Stefan Karczmarz. \"\"Angenaherte auflosung von systemen linearer glei-chungen\"\". In: Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat. (1937), pp. 355\u2013357.\"\n\"[Von49]\",\"John Von Neumann. \"\"On rings of operators. Reduction theory\"\". In: Annals of Mathematics (1949), pp. 401\u2013485.\"\n\"[Hal62]\",\"Israel Halperin. \"\"The product of projection operators\"\". In: Acta Sci. Math.(Szeged) 23.1 (1962), pp. 96\u201399.\"\n\"[Elf80]\",\"Tommy Elfving. \"\"Block-iterative methods for consistent and inconsistent linear equations\"\". In: Numerische Mathematik 35 (1980), pp. 1\u201312.\"\n\"[TM95]\",\"Sebastian Thrun and Tom M Mitchell. \"\"Lifelong robot learning\"\". In: Robotics and autonomous systems 15.1-2 (1995), pp. 25\u201346.\"\n\"[Cro07]\",\"Michel Crouzeix. \"\"Numerical range and functional calculus in Hilbert space\"\". In: Journal of Functional Analysis 244.2 (2007), pp. 668\u2013690.\"\n\"[Cro08]\",\"Michel Crouzeix. \"\"A functional calculus based on the numerical range: applications\"\". In: Linear and Multilinear Algebra 56.1-2 (2008), pp. 81\u2013103.\"\n\"[SV09]\",\"Thomas Strohmer and Roman Vershynin. \"\"A randomized Kaczmarz algorithm with exponential convergence\"\". In: Journal of Fourier Analysis and Applications 15.2 (2009), p. 262.\"\n\"[NT14]\",\"Deanna Needell and Joel A Tropp. \"\"Paved with good intentions: analysis of a randomized block Kaczmarz method\"\". In: Linear Algebra and its Applications 441 (2014), pp. 199\u2013221.\"\n\"[BS16]\",\"Catalin Badea and David Seifert. \"\"Ritt operators and convergence in the method of alternating projections\"\". In: Journal of Approximation Theory 205 (2016), pp. 133\u2013148.\"\n\"[CP17]\",\"Michel Crouzeix and C\u00e9sar Palencia. \"\"The numerical range is a (1+2)-spectral set\"\". In: SIAM Journal on Matrix Analysis and Applications 38.2 (2017), pp. 649\u2013655.\"\n\"[CL18]\",\"Zhiyuan Chen and Bing Liu. \"\"Lifelong machine learning\"\". In: Synthesis Lectures on Artificial Intelligence and Machine Learning 12.3 (2018), pp. 1\u2013207.\"\n\"[Kem+18]\",\"Ronald Kemker et al. \"\"Measuring catastrophic forgetting in neural networks\"\". In: Proceedings of the AAAI conference on artificial intelligence. Vol. 32. 1. 2018.\"\n\"[Par+19]\",\"German I Parisi et al. \"\"Continual lifelong learning with neural networks: A review\"\". In: Neural networks 113 (2019), pp. 54\u201371.\"\n\"[VT19]\",\"Gido M Van de Ven and Andreas S Tolias. \"\"Three scenarios for continual learning\"\". In: arXiv preprint arXiv:1904.07734 (2019).\"\n\"[Had+20]\",\"Raia Hadsell et al. \"\"Embracing change: Continual learning in deep neural networks\"\". In: Trends in cognitive sciences 24.12 (2020), pp. 1028\u20131040.\"\n\"[Evr+22]\",\"Itay Evron et al. \"\"How catastrophic can catastrophic forgetting be in linear regression?\"\". In: Conference on Learning Theory. PMLR. 2022, pp. 4028\u20134079.\"\n\"[QR22]\",\"Chao Qin and Daniel Russo. \"\"Adaptivity and confounding in multi-armed bandit experiments\"\". In: arXiv preprint arXiv:2202.09036 (2022).\"\n\"[Yan+22]\",\"Yingxiang Yang et al. \"\"Fourier Learning with Cyclical Data\"\". In: International Conference on Machine Learning. PMLR. 2022, pp. 25280\u201325301.\"\n\"[Lin+23]\",\"Sen Lin et al. \"\"Theory on Forgetting and Generalization of Continual Learning\"\". In: arXiv preprint arXiv:2302.05836 (2023).\"\n\"[RZ23]\",\"Simeon Reich and Rafa\u0142 Zalas. \"\"Polynomial estimates for the method of cyclic projections in Hilbert spaces\"\". In: Numerical Algorithms (2023), pp. 1\u201326.\"\n\"[Wan+23]\",\"Liyuan Wang et al. \"\"A comprehensive survey of continual learning: Theory, method and application\"\". In: arXiv preprint arXiv:2302.00487 (2023).\""}]}], "job_id": "a86c847f-fdae-4864-b1f6-d8c2aa365b63", "file_path": "./corpus/11338_nearly_optimal_bounds_for_cycl.pdf"}