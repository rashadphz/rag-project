{"pages": [{"page": 1, "text": "                             FAMO: Fast Adaptive Multitask Optimization\n                                                \u2020 Bo Liu, \u2021Yihao Feng, \u2020,\u00a7Peter Stone, \u2020Qiang Liu\n                                     \u2020The University of Texas at Austin, \u2021Salesforce AI Research, \u00a7Sony AI\n                                    {bliu, pstone, lqiang}@cs.utexas.edu, yihaof@salesforce.com\narXiv:2306.03792v3  [cs.LG]  30 Oct 2023                               Abstract\n                                One of the grand enduring goals of AI is to create generalist agents that can learn\n                                multiple different tasks from diverse data via multitask learning (MTL). However,\n                                in practice, applying gradient descent (GD) on the average loss across all tasks\n                                may yield poor multitask performance due to severe under-optimization of certain\n                                tasks. Previous approaches that manipulate task gradients for a more balanced\n                                loss decrease require storing and computing all task gradients (O(k) space and\n                                time where k is the number of tasks), limiting their use in large-scale scenarios.\n                                In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a dy-\n                                namic weighting method that decreases task losses in a balanced way using O(1)\n                                space and time. We conduct an extensive set of experiments covering multi-task\n                                supervised and reinforcement learning problems. Our results indicate that FAMO\n                                achieves comparable or superior performance to state-of-the-art gradient manipula-\n                                tion techniques while offering significant improvements in space and computational\n                                efficiency. Code is available at https://github.com/Cranial-XIX/FAMO.\n                     1    Introduction\n                     Large models trained on diverse data have advanced both computer vision [20] and natural language\n                     processing [4], paving the way for generalist agents capable of multitask learning (MTL) [5]. Given\n                     the substantial size of these models, it is crucial to design MTL methods that are effective in terms of\n                     task performance and efficient in terms of space and time complexities for managing training costs\n                     and environmental impacts. This work explores such methods through the lens of optimization.\n                     Perhaps the most intuitive way of solving an MTL problem is to optimize the average loss across\n                     all tasks. However, in practice, doing so can lead to models with poor multitask performance: a\n                     subset of tasks are severely under-optimized. A major reason behind such optimization failure is that\n                     a subset of tasks are under-optimized because the average gradient constantly results in small (or\n                     even negative) progress on these tasks (see details in Section 2).\n                     To mitigate this problem, gradient manipulation methods [43, 25, 7, 24] compute a new update vector\n                     in place of the gradient to the average loss, such that all task losses decrease in a more balanced\n                     way. The new update vector is often determined by solving an additional optimization problem that\n                     involves all task gradients. While these approaches exhibit improved performance, they become\n                     computationally expensive when the number of tasks and the model size are large [41]. This is\n                     because they require computing and storing all task gradients at each iteration, thus demanding\n                     O(k) space and time complexities, not to mention the overhead introduced by solving the additional\n                     optimization problem. In contrast, the average gradient can be efficiently computed in O(1) space\n                     and time per iteration because one can first average the task losses and then take the gradient of the\n                     average loss.1 To this end, we ask the following question:\n                         1Here, we refer to the situation where a single data x can be used to compute all task losses.\n                     37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# FAMO: Fast Adaptive Multitask Optimization\n\n## FAMO: Fast Adaptive Multitask Optimization\n\n\u2020 Bo Liu, \u2021Yihao Feng, \u2020,\u00a7Peter Stone, \u2020Qiang Liu\n\n\u2020The University of Texas at Austin, \u2021Salesforce AI Research, \u00a7Sony AI\n\n{bliu, pstone, lqiang}@cs.utexas.edu, yihaof@salesforce.com\n\narXiv:2306.03792v3 [cs.LG] 30 Oct 2023\n\n### Abstract\n\nOne of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients (O(k) space and time where k is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a dynamic weighting method that decreases task losses in a balanced way using O(1) space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation techniques while offering significant improvements in space and computational efficiency. Code is available at https://github.com/Cranial-XIX/FAMO.\n\n#### 1 Introduction\n\nLarge models trained on diverse data have advanced both computer vision [20] and natural language processing [4], paving the way for generalist agents capable of multitask learning (MTL) [5]. Given the substantial size of these models, it is crucial to design MTL methods that are effective in terms of task performance and efficient in terms of space and time complexities for managing training costs and environmental impacts. This work explores such methods through the lens of optimization.\n\nPerhaps the most intuitive way of solving an MTL problem is to optimize the average loss across all tasks. However, in practice, doing so can lead to models with poor multitask performance: a subset of tasks are severely under-optimized. A major reason behind such optimization failure is that a subset of tasks are under-optimized because the average gradient constantly results in small (or even negative) progress on these tasks (see details in Section 2).\n\nTo mitigate this problem, gradient manipulation methods [43, 25, 7, 24] compute a new update vector in place of the gradient to the average loss, such that all task losses decrease in a more balanced way. The new update vector is often determined by solving an additional optimization problem that involves all task gradients. While these approaches exhibit improved performance, they become computationally expensive when the number of tasks and the model size are large [41]. This is because they require computing and storing all task gradients at each iteration, thus demanding O(k) space and time complexities, not to mention the overhead introduced by solving the additional optimization problem. In contrast, the average gradient can be efficiently computed in O(1) space and time per iteration because one can first average the task losses and then take the gradient of the average loss.1 To this end, we ask the following question:\n\n1Here, we refer to the situation where a single data x can be used to compute all task losses.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "FAMO: Fast Adaptive Multitask Optimization", "md": "# FAMO: Fast Adaptive Multitask Optimization"}, {"type": "heading", "lvl": 2, "value": "FAMO: Fast Adaptive Multitask Optimization", "md": "## FAMO: Fast Adaptive Multitask Optimization"}, {"type": "text", "value": "\u2020 Bo Liu, \u2021Yihao Feng, \u2020,\u00a7Peter Stone, \u2020Qiang Liu\n\n\u2020The University of Texas at Austin, \u2021Salesforce AI Research, \u00a7Sony AI\n\n{bliu, pstone, lqiang}@cs.utexas.edu, yihaof@salesforce.com\n\narXiv:2306.03792v3 [cs.LG] 30 Oct 2023", "md": "\u2020 Bo Liu, \u2021Yihao Feng, \u2020,\u00a7Peter Stone, \u2020Qiang Liu\n\n\u2020The University of Texas at Austin, \u2021Salesforce AI Research, \u00a7Sony AI\n\n{bliu, pstone, lqiang}@cs.utexas.edu, yihaof@salesforce.com\n\narXiv:2306.03792v3 [cs.LG] 30 Oct 2023"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients (O(k) space and time where k is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a dynamic weighting method that decreases task losses in a balanced way using O(1) space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation techniques while offering significant improvements in space and computational efficiency. Code is available at https://github.com/Cranial-XIX/FAMO.", "md": "One of the grand enduring goals of AI is to create generalist agents that can learn multiple different tasks from diverse data via multitask learning (MTL). However, in practice, applying gradient descent (GD) on the average loss across all tasks may yield poor multitask performance due to severe under-optimization of certain tasks. Previous approaches that manipulate task gradients for a more balanced loss decrease require storing and computing all task gradients (O(k) space and time where k is the number of tasks), limiting their use in large-scale scenarios. In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a dynamic weighting method that decreases task losses in a balanced way using O(1) space and time. We conduct an extensive set of experiments covering multi-task supervised and reinforcement learning problems. Our results indicate that FAMO achieves comparable or superior performance to state-of-the-art gradient manipulation techniques while offering significant improvements in space and computational efficiency. Code is available at https://github.com/Cranial-XIX/FAMO."}, {"type": "heading", "lvl": 4, "value": "1 Introduction", "md": "#### 1 Introduction"}, {"type": "text", "value": "Large models trained on diverse data have advanced both computer vision [20] and natural language processing [4], paving the way for generalist agents capable of multitask learning (MTL) [5]. Given the substantial size of these models, it is crucial to design MTL methods that are effective in terms of task performance and efficient in terms of space and time complexities for managing training costs and environmental impacts. This work explores such methods through the lens of optimization.\n\nPerhaps the most intuitive way of solving an MTL problem is to optimize the average loss across all tasks. However, in practice, doing so can lead to models with poor multitask performance: a subset of tasks are severely under-optimized. A major reason behind such optimization failure is that a subset of tasks are under-optimized because the average gradient constantly results in small (or even negative) progress on these tasks (see details in Section 2).\n\nTo mitigate this problem, gradient manipulation methods [43, 25, 7, 24] compute a new update vector in place of the gradient to the average loss, such that all task losses decrease in a more balanced way. The new update vector is often determined by solving an additional optimization problem that involves all task gradients. While these approaches exhibit improved performance, they become computationally expensive when the number of tasks and the model size are large [41]. This is because they require computing and storing all task gradients at each iteration, thus demanding O(k) space and time complexities, not to mention the overhead introduced by solving the additional optimization problem. In contrast, the average gradient can be efficiently computed in O(1) space and time per iteration because one can first average the task losses and then take the gradient of the average loss.1 To this end, we ask the following question:\n\n1Here, we refer to the situation where a single data x can be used to compute all task losses.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Large models trained on diverse data have advanced both computer vision [20] and natural language processing [4], paving the way for generalist agents capable of multitask learning (MTL) [5]. Given the substantial size of these models, it is crucial to design MTL methods that are effective in terms of task performance and efficient in terms of space and time complexities for managing training costs and environmental impacts. This work explores such methods through the lens of optimization.\n\nPerhaps the most intuitive way of solving an MTL problem is to optimize the average loss across all tasks. However, in practice, doing so can lead to models with poor multitask performance: a subset of tasks are severely under-optimized. A major reason behind such optimization failure is that a subset of tasks are under-optimized because the average gradient constantly results in small (or even negative) progress on these tasks (see details in Section 2).\n\nTo mitigate this problem, gradient manipulation methods [43, 25, 7, 24] compute a new update vector in place of the gradient to the average loss, such that all task losses decrease in a more balanced way. The new update vector is often determined by solving an additional optimization problem that involves all task gradients. While these approaches exhibit improved performance, they become computationally expensive when the number of tasks and the model size are large [41]. This is because they require computing and storing all task gradients at each iteration, thus demanding O(k) space and time complexities, not to mention the overhead introduced by solving the additional optimization problem. In contrast, the average gradient can be efficiently computed in O(1) space and time per iteration because one can first average the task losses and then take the gradient of the average loss.1 To this end, we ask the following question:\n\n1Here, we refer to the situation where a single data x can be used to compute all task losses.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": " AvB    Loss Landscape              contour           L2 contour            Time Comsumption (Sec.)\n                                                                                       AGrau CAGtauNashWn.\n         Pareto Front          Initial Point                                     MGD                     FANO\n         Adam         Ck MGDA               PCGrad            CAGrad          NashMTL             FAMO\nFigure 1: Top left: The loss landscape, and individual task losses of a toy 2-task learning problem (\u2605represents\nthe minimum of task losses). Top right: the runtime of different MTL methods for 50000 steps. Bottom:\nthe loss trajectories of different MTL methods. ADAM fails in 1 out of 5 runs to reach the Pareto front due to\nCG. FAMO decreases task losses in a balanced way and is the only method matching the O(1) space/time\ncomplexity of ADAM. Experimental details and analysis are provided in Section 5.1.\n  (Q) Is it possible to design a multi-task learning optimizer that ensures a balanced reduction in\n               losses across all tasks while utilizing O(1) space and time per iteration?\nIn this work, we present Fast Adaptive Multitask Optimization (FAMO), a simple yet effective\nadaptive task weighting method to address the above question. On the one hand, FAMO is designed\nto ensure that all tasks are optimized with approximately similar progress. On the other hand, FAMO\nleverages the loss history to update the task weighting, hence bypassing the necessity of computing\nall task gradients. To summarize, our contributions are:\n       1. We introduce FAMO, an MTL optimizer that decreases task losses approximately at equal\n          rates while using only O(1) space and time per iteration.\n       2. We demonstrate that FAMO performs comparably to or better than existing gradient manip-\n          ulation methods on a wide range of standard MTL benchmarks, in terms of standard MTL\n          metrics, while being significantly computationally cheaper.\n2    Background\nIn this section, we provide the formal definition of multitask learning, then discuss its optimization\nchallenge, and provide a brief overview of the gradient manipulation methods.\nMultitask Learning (MTL)           MTL considers optimizing a single model with parameter \u03b8 \u2208          Rm that\ncan perform k \u2265    2 tasks well, where each task is associated with a loss function \u2113i(\u03b8) \u2236       Rm \u2192    R\u22650.2\nThen, it is common to optimize the average loss across all tasks:\n                                                             k\n                                        min                 \u2211  \u2113i(\u03b8)}.                                       (1)\n                                        \u03b8\u2208Rm {\u21130(\u03b8) \u2236= 1  k i=1\nOptimization Challenge         Directly optimizing (1) can result in severe under-optimization of a subset\nof tasks. A major reason behind this optimization challenge is the \u201cgeneralized\" conflicting gradient\nphenomenon, which we explain in the following. At any time step t, assume one updates the model\n    2In this work, we assume \u2200  i, \u2113i(\u03b8) \u2265 0, which is true for typical loss functions including mean square and\ncross-entropy losses. Note that one can always transform \u2113i to be non-negative if a loss lower bound is known.\n                                                       2", "md": "AvB Loss Landscape contour L2 contour Time Consumption (Sec.)\n\n| Pareto Front | Initial Point | MGD | FAMO |\n|--------------|---------------|-----|------|\n| Adam         | MGDA          | PCGrad | CAGrad |\n|              |               | NashMTL | FAMO |\n\n$$\\text{Figure 1: Top left: The loss landscape, and individual task losses of a toy 2-task learning problem (\u2605 represents the minimum of task losses). Top right: the runtime of different MTL methods for 50000 steps. Bottom: the loss trajectories of different MTL methods. ADAM fails in 1 out of 5 runs to reach the Pareto front due to CG. FAMO decreases task losses in a balanced way and is the only method matching the O(1) space/time complexity of ADAM. Experimental details and analysis are provided in Section 5.1.}$$\n\n(Q) Is it possible to design a multi-task learning optimizer that ensures a balanced reduction in losses across all tasks while utilizing O(1) space and time per iteration?\n\nIn this work, we present Fast Adaptive Multitask Optimization (FAMO), a simple yet effective adaptive task weighting method to address the above question. On the one hand, FAMO is designed to ensure that all tasks are optimized with approximately similar progress. On the other hand, FAMO leverages the loss history to update the task weighting, hence bypassing the necessity of computing all task gradients. To summarize, our contributions are:\n1. We introduce FAMO, an MTL optimizer that decreases task losses approximately at equal rates while using only O(1) space and time per iteration.\n2. We demonstrate that FAMO performs comparably to or better than existing gradient manipulation methods on a wide range of standard MTL benchmarks, in terms of standard MTL metrics, while being significantly computationally cheaper.\n\n## Background\n\nIn this section, we provide the formal definition of multitask learning, then discuss its optimization challenge, and provide a brief overview of the gradient manipulation methods.\n\n**Multitask Learning (MTL)**\nMTL considers optimizing a single model with parameter $$\\theta \\in \\mathbb{R}^m$$ that can perform $$k \\geq 2$$ tasks well, where each task is associated with a loss function $$\\ell_i(\\theta) \\colon \\mathbb{R}^m \\rightarrow \\mathbb{R}_{\\geq 0}$$. Then, it is common to optimize the average loss across all tasks:\n$$\n\\min_{\\theta \\in \\mathbb{R}^m} \\left\\{ \\ell_0(\\theta) := \\frac{1}{k} \\sum_{i=1}^{k} \\ell_i(\\theta) \\right\\} \\quad (1)\n$$\n\n**Optimization Challenge**\nDirectly optimizing (1) can result in severe under-optimization of a subset of tasks. A major reason behind this optimization challenge is the \"generalized\" conflicting gradient phenomenon, which we explain in the following. At any time step $$t$$, assume one updates the model. In this work, we assume $$\\forall i, \\ell_i(\\theta) \\geq 0$$, which is true for typical loss functions including mean square and cross-entropy losses. Note that one can always transform $$\\ell_i$$ to be non-negative if a loss lower bound is known.", "images": [{"name": "page-2-0.jpg", "height": 189, "width": 396, "x": 108, "y": 72}], "items": [{"type": "text", "value": "AvB Loss Landscape contour L2 contour Time Consumption (Sec.)", "md": "AvB Loss Landscape contour L2 contour Time Consumption (Sec.)"}, {"type": "table", "rows": [["Pareto Front", "Initial Point", "MGD", "FAMO"], ["Adam", "MGDA", "PCGrad", "CAGrad"], ["", "", "NashMTL", "FAMO"]], "md": "| Pareto Front | Initial Point | MGD | FAMO |\n|--------------|---------------|-----|------|\n| Adam         | MGDA          | PCGrad | CAGrad |\n|              |               | NashMTL | FAMO |", "isPerfectTable": true, "csv": "\"Pareto Front\",\"Initial Point\",\"MGD\",\"FAMO\"\n\"Adam\",\"MGDA\",\"PCGrad\",\"CAGrad\"\n\"\",\"\",\"NashMTL\",\"FAMO\""}, {"type": "text", "value": "$$\\text{Figure 1: Top left: The loss landscape, and individual task losses of a toy 2-task learning problem (\u2605 represents the minimum of task losses). Top right: the runtime of different MTL methods for 50000 steps. Bottom: the loss trajectories of different MTL methods. ADAM fails in 1 out of 5 runs to reach the Pareto front due to CG. FAMO decreases task losses in a balanced way and is the only method matching the O(1) space/time complexity of ADAM. Experimental details and analysis are provided in Section 5.1.}$$\n\n(Q) Is it possible to design a multi-task learning optimizer that ensures a balanced reduction in losses across all tasks while utilizing O(1) space and time per iteration?\n\nIn this work, we present Fast Adaptive Multitask Optimization (FAMO), a simple yet effective adaptive task weighting method to address the above question. On the one hand, FAMO is designed to ensure that all tasks are optimized with approximately similar progress. On the other hand, FAMO leverages the loss history to update the task weighting, hence bypassing the necessity of computing all task gradients. To summarize, our contributions are:\n1. We introduce FAMO, an MTL optimizer that decreases task losses approximately at equal rates while using only O(1) space and time per iteration.\n2. We demonstrate that FAMO performs comparably to or better than existing gradient manipulation methods on a wide range of standard MTL benchmarks, in terms of standard MTL metrics, while being significantly computationally cheaper.", "md": "$$\\text{Figure 1: Top left: The loss landscape, and individual task losses of a toy 2-task learning problem (\u2605 represents the minimum of task losses). Top right: the runtime of different MTL methods for 50000 steps. Bottom: the loss trajectories of different MTL methods. ADAM fails in 1 out of 5 runs to reach the Pareto front due to CG. FAMO decreases task losses in a balanced way and is the only method matching the O(1) space/time complexity of ADAM. Experimental details and analysis are provided in Section 5.1.}$$\n\n(Q) Is it possible to design a multi-task learning optimizer that ensures a balanced reduction in losses across all tasks while utilizing O(1) space and time per iteration?\n\nIn this work, we present Fast Adaptive Multitask Optimization (FAMO), a simple yet effective adaptive task weighting method to address the above question. On the one hand, FAMO is designed to ensure that all tasks are optimized with approximately similar progress. On the other hand, FAMO leverages the loss history to update the task weighting, hence bypassing the necessity of computing all task gradients. To summarize, our contributions are:\n1. We introduce FAMO, an MTL optimizer that decreases task losses approximately at equal rates while using only O(1) space and time per iteration.\n2. We demonstrate that FAMO performs comparably to or better than existing gradient manipulation methods on a wide range of standard MTL benchmarks, in terms of standard MTL metrics, while being significantly computationally cheaper."}, {"type": "heading", "lvl": 2, "value": "Background", "md": "## Background"}, {"type": "text", "value": "In this section, we provide the formal definition of multitask learning, then discuss its optimization challenge, and provide a brief overview of the gradient manipulation methods.\n\n**Multitask Learning (MTL)**\nMTL considers optimizing a single model with parameter $$\\theta \\in \\mathbb{R}^m$$ that can perform $$k \\geq 2$$ tasks well, where each task is associated with a loss function $$\\ell_i(\\theta) \\colon \\mathbb{R}^m \\rightarrow \\mathbb{R}_{\\geq 0}$$. Then, it is common to optimize the average loss across all tasks:\n$$\n\\min_{\\theta \\in \\mathbb{R}^m} \\left\\{ \\ell_0(\\theta) := \\frac{1}{k} \\sum_{i=1}^{k} \\ell_i(\\theta) \\right\\} \\quad (1)\n$$\n\n**Optimization Challenge**\nDirectly optimizing (1) can result in severe under-optimization of a subset of tasks. A major reason behind this optimization challenge is the \"generalized\" conflicting gradient phenomenon, which we explain in the following. At any time step $$t$$, assume one updates the model. In this work, we assume $$\\forall i, \\ell_i(\\theta) \\geq 0$$, which is true for typical loss functions including mean square and cross-entropy losses. Note that one can always transform $$\\ell_i$$ to be non-negative if a loss lower bound is known.", "md": "In this section, we provide the formal definition of multitask learning, then discuss its optimization challenge, and provide a brief overview of the gradient manipulation methods.\n\n**Multitask Learning (MTL)**\nMTL considers optimizing a single model with parameter $$\\theta \\in \\mathbb{R}^m$$ that can perform $$k \\geq 2$$ tasks well, where each task is associated with a loss function $$\\ell_i(\\theta) \\colon \\mathbb{R}^m \\rightarrow \\mathbb{R}_{\\geq 0}$$. Then, it is common to optimize the average loss across all tasks:\n$$\n\\min_{\\theta \\in \\mathbb{R}^m} \\left\\{ \\ell_0(\\theta) := \\frac{1}{k} \\sum_{i=1}^{k} \\ell_i(\\theta) \\right\\} \\quad (1)\n$$\n\n**Optimization Challenge**\nDirectly optimizing (1) can result in severe under-optimization of a subset of tasks. A major reason behind this optimization challenge is the \"generalized\" conflicting gradient phenomenon, which we explain in the following. At any time step $$t$$, assume one updates the model. In this work, we assume $$\\forall i, \\ell_i(\\theta) \\geq 0$$, which is true for typical loss functions including mean square and cross-entropy losses. Note that one can always transform $$\\ell_i$$ to be non-negative if a loss lower bound is known."}]}, {"page": 3, "text": "Algorithm 1 Fast Adaptive Multitask Optimization (FAMO)\n   1: Input: Initial parameter \u03b80, task losses {\u2113i}k                                      i=1 (ensure that \u2113i \u2265                  \u03f5 > 0, for instance, by \u2113i \u2190\n        \u2113i \u2212    \u2113\u2217i + \u03f5, \u2113\u2217    i = inf\u03b8 \u2113i(\u03b8)), learning rate \u03b1 and \u03b2, and decay \u03b3 (= 0.001 by default).\n   2: \u03be1 \u2190        0.            // initialize the task logits to all zeros\n   3: for t = 1 \u2236          T do\n   4:        Compute zt = Softmax(\u03bet), e.g.,                                 zi,t =     \u2211k   exp(\u03bei,t)             .\n   5:        Update the model parameters:                              k                    i=1 exp(\u03bei,t)                        k\n                                             \u03b8t+1 = \u03b8t \u2212          \u03b1        (ct   zi,t   )\u2207\u2113i,t, where ct = (                    \u2211     zi,t  )  \u22121.\n                                                                     \u2211           \u2113i,t                                           i=1   \u2113i,t\n   6:        Update the logits for task weighting:                   i=1                       \u23a1 \u2207\u22baz1,t(\u03bet)          \u23a4 \u22ba  \u23a1  log \u21131,t \u2212       log \u21131,t+1       \u23a4\n                                                                                               \u23a2                     \u23a5    \u23a2                                    \u23a5\n                             \u03bet+1 = \u03bet \u2212         \u03b2(\u03b4t + \u03b3\u03bet) where \u03b4t =                        \u23a2          \u22ee          \u23a5    \u23a2                  \u22ee                 \u23a5  .\n                                                                                                                     \u23a5    \u23a2                                    \u23a5\n                                                                                               \u23a2                                                               \u23a5\n                                                                                               \u23a2 \u2207\u22bazk,t(\u03bet)          \u23a5    \u23a2log \u2113k,t \u2212         log \u2113k,t+1.\n   7: end for                                                                                  \u23a3                     \u23a6    \u23a3                                    \u23a6\n parameter using a gradient descent style iterative update: \u03b8t+1 = \u03b8t \u2212                                                    \u03b1dt where \u03b1 is the step size and\n dt is the update at time t. Then, we say that conflicting gradients (CG) [24, 43] happens if\n                                                   \u2203i, \u2113i(\u03b8t+1) \u2212              \u2113i(\u03b8t) \u2248        \u2212\u03b1\u2207\u2113i(\u03b8t)\u22badt > 0.\n In other words, certain task\u2019s loss is increasing. CG often occurs during optimization and is not\n inherently detrimental. However, it becomes undesirable when a subset of tasks persistently undergoes\n under-optimization due to CG. In a more general sense, it is not desirable if a subset of tasks has\n much slower learning progress compared to the rest of the tasks (even if all task losses are decreasing).\nThis very phenomenon, which we call the \u201cgeneralized\" conflicting gradient, has spurred previous\n research to mitigate it at each optimization stage [43].\n Gradient Manipulation Methods                                      Gradient manipulation methods aim to decrease all task losses\n in a more balanced way by finding a new update dt at each step. dt is usually a convex combination\n of task gradients, and therefore the name gradient manipulation (denote \u2207\u2113i,t = \u2207\u03b8\u2113i(\u03b8t) for short):\n                                      \u23a1 \u2207\u2113\u22ba       \u23a4 \u22ba                                   \u23a1  w1,t   \u23a4\n                                      \u23a2       1,t \u23a5                                     \u23a2         \u23a5\n                                      \u23a2      \u22ee    \u23a5                                     \u23a2     \u22ee   \u23a5\n                             dt =     \u23a2           \u23a5    wt,       where wt =             \u23a2         \u23a5   = f(\u2207\u21131,t,          . . . , \u2207\u2113k,t) \u2208        Sk.                (2)\n                                      \u23a2           \u23a5                                     \u23a2         \u23a5\n                                      \u23a2 \u2207\u2113\u22ba  k,t  \u23a5                                     \u23a3 wk,t    \u23a6\n Here, Sk = {w \u2208               Rk     \u23a3           \u23a6\n                                  \u22650 \u2223   w\u22ba1 = 1} is the probabilistic simplex, and wt is the task weighting across all\n tasks. Please refer to Appendix A for details of fi                                      ve state-of-the-art gradient manipulation methods\n(MGDA, PCGRAD, CAGRAD, IMTL-G, NASHMTL) and their corresponding f. Note that existing\n gradient manipulation methods require computing and storing k task gradients before applying f to\n compute dt, which often involves solving an additional optimization problem. As a result, we say\n these methods require at least O(k) space and time complexity, which makes them slow and memory\n inefficient when k and model size m are large.\n 3       Fast Adaptive Multitask Optimization (FAMO)\n In this section, we introduce FAMO that addresses question Q, which involves two main ideas:\n           1. At each step, decrease all task losses at an equal rate as much as possible (Section 3.1).\n           2. Amortize the computation in 1. over time (Section 3.2).\n                                                                                         3", "md": "# Fast Adaptive Multitask Optimization (FAMO)\n\n## Algorithm 1 Fast Adaptive Multitask Optimization (FAMO)\n\nInput: Initial parameter \u03b80, task losses {\u2113i}ki=1 (ensure that \u2113i \u2265 \u03f5 > 0, for instance, by \u2113i \u2190 \u2113i - \u2113*i + \u03f5, \u2113*i = inf\u03b8 \u2113i(\u03b8)), learning rate \u03b1 and \u03b2, and decay \u03b3 (= 0.001 by default).\n\n\u03be1 \u2190 0. // initialize the task logits to all zeros\n\n1. for t = 1 : T do\n2. Compute zt = Softmax(\u03bet), e.g., zi,t = \u2211ki=1 exp(\u03bei,t).\n3. Update the model parameters: \u03b8t+1 = \u03b8t - \u03b1(ct zi,t)\u2207\u2113i,t, where ct = (\u2211i=1 zi,t)-1.\n4. Update the logits for task weighting:\n5. $\n\\begin{align*}\n\u03bet+1 & = \u03bet - \u03b2(\u03b4t + \u03b3\u03bet) \\text{ where } \u03b4t = \\begin{bmatrix} \\nabla z1,t(\u03bet) \\\\ \\vdots \\\\ \\nabla zk,t(\u03bet</sub) \\end{bmatrix}^T \\begin{bmatrix} \\log \u21131,t - \\log \u21131,t+1 \\\\ \\vdots \\\\ \\log \u2113k,t - \\log \u2113k,t+1 \\end{bmatrix}.\n\\end{align*}\n$\n\nParameter using a gradient descent style iterative update: \u03b8t+1 = \u03b8t - \u03b1dt where \u03b1 is the step size and dt is the update at time t. Then, we say that conflicting gradients (CG) [24, 43] happens if \u2203i, \u2113i(\u03b8t+1) - \u2113i(\u03b8t</sub) \u2248 -\u03b1\u2207\u2113i(\u03b8t)Tdt > 0.\n\nIn other words, certain task\u2019s loss is increasing. CG often occurs during optimization and is not inherently detrimental. However, it becomes undesirable when a subset of tasks persistently undergoes under-optimization due to CG. In a more general sense, it is not desirable if a subset of tasks has much slower learning progress compared to the rest of the tasks (even if all task losses are decreasing).\n\nThis very phenomenon, which we call the \u201cgeneralized\" conflicting gradient, has spurred previous research to mitigate it at each optimization stage [43].\n\n### Gradient Manipulation Methods\n\nGradient manipulation methods aim to decrease all task losses in a more balanced way by finding a new update dt at each step. dt is usually a convex combination of task gradients, and therefore the name gradient manipulation (denote \u2207\u2113i,t = \u2207\u03b8\u2113i(\u03b8t for short):\n\n$$\n\\begin{align*}\ndt & = \\begin{bmatrix} \\nabla \u21131,t^T \\\\ \\vdots \\\\ \\nabla \u2113k,t^T \\end{bmatrix} \\begin{bmatrix} w1,t \\\\ \\vdots \\\\ wk,t \\end{bmatrix}, \\text{ where } wt = f(\u2207\u21131,t, ..., \u2207\u2113k,t) \u2208 Sk.\n\\end{align*}\n$$\nHere, Sk = {w \u2208 Rk \u22650 \u2223 wT1 = 1} is the probabilistic simplex, and wt is the task weighting across all tasks. Please refer to Appendix A for details of five state-of-the-art gradient manipulation methods (MGDA, PCGRAD, CAGRAD, IMTL-G, NASHMTL) and their corresponding f. Note that existing gradient manipulation methods require computing and storing k task gradients before applying f to compute dt, which often involves solving an additional optimization problem. As a result, we say these methods require at least O(k) space and time complexity, which makes them slow and memory inefficient when k and model size m are large.\n\n### Fast Adaptive Multitask Optimization (FAMO)\n\nIn this section, we introduce FAMO that addresses question Q, which involves two main ideas:\n\n1. At each step, decrease all task losses at an equal rate as much as possible (Section 3.1).\n2. Amortize the computation in 1. over time (Section 3.2).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Fast Adaptive Multitask Optimization (FAMO)", "md": "# Fast Adaptive Multitask Optimization (FAMO)"}, {"type": "heading", "lvl": 2, "value": "Algorithm 1 Fast Adaptive Multitask Optimization (FAMO)", "md": "## Algorithm 1 Fast Adaptive Multitask Optimization (FAMO)"}, {"type": "text", "value": "Input: Initial parameter \u03b80, task losses {\u2113i}ki=1 (ensure that \u2113i \u2265 \u03f5 > 0, for instance, by \u2113i \u2190 \u2113i - \u2113*i + \u03f5, \u2113*i = inf\u03b8 \u2113i(\u03b8)), learning rate \u03b1 and \u03b2, and decay \u03b3 (= 0.001 by default).\n\n\u03be1 \u2190 0. // initialize the task logits to all zeros\n\n1. for t = 1 : T do\n2. Compute zt = Softmax(\u03bet), e.g., zi,t = \u2211ki=1 exp(\u03bei,t).\n3. Update the model parameters: \u03b8t+1 = \u03b8t - \u03b1(ct zi,t)\u2207\u2113i,t, where ct = (\u2211i=1 zi,t)-1.\n4. Update the logits for task weighting:\n5. $\n\\begin{align*}\n\u03bet+1 & = \u03bet - \u03b2(\u03b4t + \u03b3\u03bet) \\text{ where } \u03b4t = \\begin{bmatrix} \\nabla z1,t(\u03bet) \\\\ \\vdots \\\\ \\nabla zk,t(\u03bet</sub) \\end{bmatrix}^T \\begin{bmatrix} \\log \u21131,t - \\log \u21131,t+1 \\\\ \\vdots \\\\ \\log \u2113k,t - \\log \u2113k,t+1 \\end{bmatrix}.\n\\end{align*}\n$\n\nParameter using a gradient descent style iterative update: \u03b8t+1 = \u03b8t - \u03b1dt where \u03b1 is the step size and dt is the update at time t. Then, we say that conflicting gradients (CG) [24, 43] happens if \u2203i, \u2113i(\u03b8t+1) - \u2113i(\u03b8t</sub) \u2248 -\u03b1\u2207\u2113i(\u03b8t)Tdt > 0.\n\nIn other words, certain task\u2019s loss is increasing. CG often occurs during optimization and is not inherently detrimental. However, it becomes undesirable when a subset of tasks persistently undergoes under-optimization due to CG. In a more general sense, it is not desirable if a subset of tasks has much slower learning progress compared to the rest of the tasks (even if all task losses are decreasing).\n\nThis very phenomenon, which we call the \u201cgeneralized\" conflicting gradient, has spurred previous research to mitigate it at each optimization stage [43].", "md": "Input: Initial parameter \u03b80, task losses {\u2113i}ki=1 (ensure that \u2113i \u2265 \u03f5 > 0, for instance, by \u2113i \u2190 \u2113i - \u2113*i + \u03f5, \u2113*i = inf\u03b8 \u2113i(\u03b8)), learning rate \u03b1 and \u03b2, and decay \u03b3 (= 0.001 by default).\n\n\u03be1 \u2190 0. // initialize the task logits to all zeros\n\n1. for t = 1 : T do\n2. Compute zt = Softmax(\u03bet), e.g., zi,t = \u2211ki=1 exp(\u03bei,t).\n3. Update the model parameters: \u03b8t+1 = \u03b8t - \u03b1(ct zi,t)\u2207\u2113i,t, where ct = (\u2211i=1 zi,t)-1.\n4. Update the logits for task weighting:\n5. $\n\\begin{align*}\n\u03bet+1 & = \u03bet - \u03b2(\u03b4t + \u03b3\u03bet) \\text{ where } \u03b4t = \\begin{bmatrix} \\nabla z1,t(\u03bet) \\\\ \\vdots \\\\ \\nabla zk,t(\u03bet</sub) \\end{bmatrix}^T \\begin{bmatrix} \\log \u21131,t - \\log \u21131,t+1 \\\\ \\vdots \\\\ \\log \u2113k,t - \\log \u2113k,t+1 \\end{bmatrix}.\n\\end{align*}\n$\n\nParameter using a gradient descent style iterative update: \u03b8t+1 = \u03b8t - \u03b1dt where \u03b1 is the step size and dt is the update at time t. Then, we say that conflicting gradients (CG) [24, 43] happens if \u2203i, \u2113i(\u03b8t+1) - \u2113i(\u03b8t</sub) \u2248 -\u03b1\u2207\u2113i(\u03b8t)Tdt > 0.\n\nIn other words, certain task\u2019s loss is increasing. CG often occurs during optimization and is not inherently detrimental. However, it becomes undesirable when a subset of tasks persistently undergoes under-optimization due to CG. In a more general sense, it is not desirable if a subset of tasks has much slower learning progress compared to the rest of the tasks (even if all task losses are decreasing).\n\nThis very phenomenon, which we call the \u201cgeneralized\" conflicting gradient, has spurred previous research to mitigate it at each optimization stage [43]."}, {"type": "heading", "lvl": 3, "value": "Gradient Manipulation Methods", "md": "### Gradient Manipulation Methods"}, {"type": "text", "value": "Gradient manipulation methods aim to decrease all task losses in a more balanced way by finding a new update dt at each step. dt is usually a convex combination of task gradients, and therefore the name gradient manipulation (denote \u2207\u2113i,t = \u2207\u03b8\u2113i(\u03b8t for short):\n\n$$\n\\begin{align*}\ndt & = \\begin{bmatrix} \\nabla \u21131,t^T \\\\ \\vdots \\\\ \\nabla \u2113k,t^T \\end{bmatrix} \\begin{bmatrix} w1,t \\\\ \\vdots \\\\ wk,t \\end{bmatrix}, \\text{ where } wt = f(\u2207\u21131,t, ..., \u2207\u2113k,t) \u2208 Sk.\n\\end{align*}\n$$\nHere, Sk = {w \u2208 Rk \u22650 \u2223 wT1 = 1} is the probabilistic simplex, and wt is the task weighting across all tasks. Please refer to Appendix A for details of five state-of-the-art gradient manipulation methods (MGDA, PCGRAD, CAGRAD, IMTL-G, NASHMTL) and their corresponding f. Note that existing gradient manipulation methods require computing and storing k task gradients before applying f to compute dt, which often involves solving an additional optimization problem. As a result, we say these methods require at least O(k) space and time complexity, which makes them slow and memory inefficient when k and model size m are large.", "md": "Gradient manipulation methods aim to decrease all task losses in a more balanced way by finding a new update dt at each step. dt is usually a convex combination of task gradients, and therefore the name gradient manipulation (denote \u2207\u2113i,t = \u2207\u03b8\u2113i(\u03b8t for short):\n\n$$\n\\begin{align*}\ndt & = \\begin{bmatrix} \\nabla \u21131,t^T \\\\ \\vdots \\\\ \\nabla \u2113k,t^T \\end{bmatrix} \\begin{bmatrix} w1,t \\\\ \\vdots \\\\ wk,t \\end{bmatrix}, \\text{ where } wt = f(\u2207\u21131,t, ..., \u2207\u2113k,t) \u2208 Sk.\n\\end{align*}\n$$\nHere, Sk = {w \u2208 Rk \u22650 \u2223 wT1 = 1} is the probabilistic simplex, and wt is the task weighting across all tasks. Please refer to Appendix A for details of five state-of-the-art gradient manipulation methods (MGDA, PCGRAD, CAGRAD, IMTL-G, NASHMTL) and their corresponding f. Note that existing gradient manipulation methods require computing and storing k task gradients before applying f to compute dt, which often involves solving an additional optimization problem. As a result, we say these methods require at least O(k) space and time complexity, which makes them slow and memory inefficient when k and model size m are large."}, {"type": "heading", "lvl": 3, "value": "Fast Adaptive Multitask Optimization (FAMO)", "md": "### Fast Adaptive Multitask Optimization (FAMO)"}, {"type": "text", "value": "In this section, we introduce FAMO that addresses question Q, which involves two main ideas:\n\n1. At each step, decrease all task losses at an equal rate as much as possible (Section 3.1).\n2. Amortize the computation in 1. over time (Section 3.2).", "md": "In this section, we introduce FAMO that addresses question Q, which involves two main ideas:\n\n1. At each step, decrease all task losses at an equal rate as much as possible (Section 3.1).\n2. Amortize the computation in 1. over time (Section 3.2)."}]}, {"page": 4, "text": " 3.1       Balanced Rate of Loss Improvement\nAt time t, assume we perform the update \u03b8t+1 = \u03b8t \u2212                                            \u03b1dt, we define the rate of improvement for task\n i as\n                                                                    ri(\u03b1,     dt) = \u2113i,t \u2212          \u2113i,t+1    .3                                                    (3)\n                                                                                                 \u2113i,t\n FAMO then seeks an update dt that results in the largest worst-case improvement rate across all tasks\n( 1\n  2  \u2225dt\u2225     is subtracted to prevent an under-specified optimization problem where the objective can be\n infinitely large):\n                                                              max                1                                                                                  (4)\n                                                             dt\u2208Rm min  i\u2208[k]    \u03b1  ri(\u03b1,      dt) \u2212      1\nWhen the step size \u03b1 is small, using Taylor approximation, the problem (4) can be approximated by         2 \u2225dt\u22252.\n                                        max                 \u2207\u2113\u22ba  i,tdt    \u2212    1                                     \u22badt \u2212      1                                   (5)\n                                       dt\u2208Rm min  i\u2208[K]         \u2113i,t           2 \u2225dt\u22252 = (\u2207           log \u2113i,t)                 2  \u2225dt\u22252.\n Instead of solving the primal problem in (5) where d \u2208                                           Rm (m can be millions if \u03b8 is the parameter of\n a neural network), we consider its dual problem:\n Proposition 3.1. The dual objective of (5) is\n                                                                                                                 \u23a1 \u2207   log \u2113\u22ba       \u23a4\n                                                                                                                 \u23a2              1,t \u23a5\n                                                                     1                                           \u23a2         \u22ee        \u23a5\n                                             z\u2217                                           where Jt =             \u23a2                  \u23a5  ,                            (6)\n                                               t \u2208   arg min         2  \u2225Jtz\u22252,                                  \u23a2                  \u23a5\n                                                         z\u2208Sk                                                    \u23a2 \u2207   log \u2113\u22ba  k,t  \u23a5\n where z\u2217      t = [z\u2217      t,i] is the optimal combination weights of the gradients, and the optimal update     \u23a3                  \u23a6\n direction is d\u2217        t = Jtz\u2217     t .\n Proof.\n                                       max                                   \u22bad \u2212      1\n                                       d\u2208Rm min i\u2208[k]    (\u2207  k log \u2113i,t)               2  \u2225d\u22252\n                                    = max        z\u2208Sk (    \u2211     zi\u2207     log \u2113i,t)     \u22bad \u2212      1\n                                       d\u2208Rm min            i=1                                   2  \u2225d\u22252\n                                    =  min                 \u2211 k   zi\u2207     log \u2113i,t)     \u22bad \u2212      1                   (strong duality)\nWrite g(d,          z) = (      \u2211k     z\u2208Sk maxd\u2208Rm (      i=1  \u22bad \u2212     1                       2  \u2225d\u22252\n                                    i=1 zi\u2207      log \u2113i,t)               2  \u2225d\u22252, then by setting  k\n                                                         \u2202g                 \u21d2            d\u2217   =   \u2211    zi\u2207     log \u2113i,t.\n Plugging in d\u2217             back, we have                \u2202d = 0                                   i=1k                         2\n                      max                                   \u22bad \u2212     1                        1    \u2211     zi\u2207     log \u2113i,t\u2225        = min        1\n                      d\u2208Rm min i\u2208[k]   (\u2207    log \u2113i,t)               2  \u2225d\u22252 = min   z\u2208Sk     2  \u2225 i=1                                z\u2208Sk     2  \u2225Jtz\u22252.\nAt the optimum, we have d\u2217                        t = Jtz\u2217     t .\nThe dual problem in (6) can be viewed as optimizing the log objective of the multiple gradient descent\n algorithm (MGDA) [9, 35]. Similar to MGDA, (6) only involves a decision variable of dimension\n k \u226a      m. Furthermore, if the optimal combination weights z\u2217                                                  t is an interior point of Sk, then the\n improvement rates ri(\u03b1,                      d\u2217t ) of the different tasks i equal, as we show in the following result.\n      3 To avoid division by zero, in practice we add a small constant (e.g., 1e \u2212                                               8) to all losses. For the ease of\n notation (e.g., \u2113i(\u22c5) \u2190              \u2113i(\u22c5) + 1e \u2212        8, we omit it throughout the paper.\n                                                                                         4", "md": "3.1 Balanced Rate of Loss Improvement\n\nAt time \\( t \\), assume we perform the update \\( \\theta_{t+1} = \\theta_{t} - \\alpha d_{t} \\), we define the rate of improvement for task \\( i \\) as\n\n$$\nr_{i}(\\alpha, dt) = \\frac{\\ell_{i,t} - \\ell_{i,t+1}}{\\ell_{i,t}} \\quad (3)\n$$\nFAMO then seeks an update \\( d_{t} \\) that results in the largest worst-case improvement rate across all tasks\n\n\\( \\left( \\frac{1}{2} \\lVert dt \\rVert \\right) \\) is subtracted to prevent an under-specified optimization problem where the objective can be infinitely large:\n\n$$\n\\max_{dt \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( \\alpha r_{i}(\\alpha, dt) - \\frac{1}{2} \\right) \\quad (4)\n$$\nWhen the step size \\( \\alpha \\) is small, using Taylor approximation, the problem (4) can be approximated by \\( \\frac{1}{2} \\lVert dt \\rVert^{2} \\).\n\n$$\n\\max_{dt \\in \\mathbb{R}^{m}} \\min_{i \\in [K]} \\left( \\nabla \\ell_{i}^{T} dt - \\frac{1}{2} dt^{T} dt - \\frac{1}{2} \\right) \\quad (5)\n$$\n$$\n= \\left( \\nabla \\log \\ell_{i,t} \\right)^{T} \\frac{1}{2} \\lVert dt \\rVert^{2}.\n$$\nInstead of solving the primal problem in (5) where \\( d \\in \\mathbb{R}^{m} \\) (\\( m \\) can be millions if \\( \\theta \\) is the parameter of a neural network), we consider its dual problem:\n\nProposition 3.1. The dual objective of (5) is\n\n$$\nz^{*}_{t} \\in \\arg \\min_{z \\in S_{k}} \\left( \\frac{1}{2} \\lVert J_{t}z \\rVert^{2} \\right), \\quad \\text{where} \\quad J_{t} = \\begin{bmatrix} \\nabla \\log \\ell_{1,t} \\\\ \\vdots \\\\ \\nabla \\log \\ell_{k,t} \\end{bmatrix} \\quad (6)\n$$\nwhere \\( z^{*}_{t} = [z^{*}_{t,i}] \\) is the optimal combination weights of the gradients, and the optimal update direction is \\( d^{*}_{t} = J_{t}z^{*}_{t} \\).\n\nProof.\n\n$$\n\\begin{align*}\n&\\max_{d \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( (\\nabla^{k} \\log \\ell_{i,t})^{T} dt - \\frac{1}{2} dt^{T} dt \\right) \\\\\n&= \\max_{z \\in S_{k}} \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2} \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t}^{T} dt - \\frac{1}{2} \\quad \\text{(strong duality)} \\\\\n&= \\min_{z \\in S_{k}} \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2}\n\\end{align*}\n$$\nWrite \\( g(d, z) = \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2} \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t}^{T} dt \\), then by setting \\( \\frac{\\partial g}{\\partial d} = 0 \\), we have \\( d^{*} = \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\).\n\nPlugging in \\( d^{*} \\) back, we have\n\n$$\n\\max_{d \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( (\\nabla \\log \\ell_{i,t})^{T} dt - \\frac{1}{2} dt^{T} dt \\right) = \\min_{z \\in S_{k}} \\lVert J_{t}z \\rVert^{2}.\n$$\nAt the optimum, we have \\( d^{*}_{t} = J_{t}z^{*}_{t} \\).\n\nThe dual problem in (6) can be viewed as optimizing the log objective of the multiple gradient descent algorithm (MGDA). Similar to MGDA, (6) only involves a decision variable of dimension \\( k \\ll m \\). Furthermore, if the optimal combination weights \\( z^{*}_{t} \\) is an interior point of \\( S_{k} \\), then the improvement rates \\( r_{i}(\\alpha, d^{*}_{t}) \\) of the different tasks \\( i \\) equal, as we show in the following result.\n\nTo avoid division by zero, in practice we add a small constant (e.g., \\( 1e^{-8} \\)) to all losses. For the ease of notation (e.g., \\( \\ell_{i}(\\cdot) \\leftarrow \\ell_{i}(\\cdot) + 1e^{-8} \\)), we omit it throughout the paper.", "images": [], "items": [{"type": "text", "value": "3.1 Balanced Rate of Loss Improvement\n\nAt time \\( t \\), assume we perform the update \\( \\theta_{t+1} = \\theta_{t} - \\alpha d_{t} \\), we define the rate of improvement for task \\( i \\) as\n\n$$\nr_{i}(\\alpha, dt) = \\frac{\\ell_{i,t} - \\ell_{i,t+1}}{\\ell_{i,t}} \\quad (3)\n$$\nFAMO then seeks an update \\( d_{t} \\) that results in the largest worst-case improvement rate across all tasks\n\n\\( \\left( \\frac{1}{2} \\lVert dt \\rVert \\right) \\) is subtracted to prevent an under-specified optimization problem where the objective can be infinitely large:\n\n$$\n\\max_{dt \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( \\alpha r_{i}(\\alpha, dt) - \\frac{1}{2} \\right) \\quad (4)\n$$\nWhen the step size \\( \\alpha \\) is small, using Taylor approximation, the problem (4) can be approximated by \\( \\frac{1}{2} \\lVert dt \\rVert^{2} \\).\n\n$$\n\\max_{dt \\in \\mathbb{R}^{m}} \\min_{i \\in [K]} \\left( \\nabla \\ell_{i}^{T} dt - \\frac{1}{2} dt^{T} dt - \\frac{1}{2} \\right) \\quad (5)\n$$\n$$\n= \\left( \\nabla \\log \\ell_{i,t} \\right)^{T} \\frac{1}{2} \\lVert dt \\rVert^{2}.\n$$\nInstead of solving the primal problem in (5) where \\( d \\in \\mathbb{R}^{m} \\) (\\( m \\) can be millions if \\( \\theta \\) is the parameter of a neural network), we consider its dual problem:\n\nProposition 3.1. The dual objective of (5) is\n\n$$\nz^{*}_{t} \\in \\arg \\min_{z \\in S_{k}} \\left( \\frac{1}{2} \\lVert J_{t}z \\rVert^{2} \\right), \\quad \\text{where} \\quad J_{t} = \\begin{bmatrix} \\nabla \\log \\ell_{1,t} \\\\ \\vdots \\\\ \\nabla \\log \\ell_{k,t} \\end{bmatrix} \\quad (6)\n$$\nwhere \\( z^{*}_{t} = [z^{*}_{t,i}] \\) is the optimal combination weights of the gradients, and the optimal update direction is \\( d^{*}_{t} = J_{t}z^{*}_{t} \\).\n\nProof.\n\n$$\n\\begin{align*}\n&\\max_{d \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( (\\nabla^{k} \\log \\ell_{i,t})^{T} dt - \\frac{1}{2} dt^{T} dt \\right) \\\\\n&= \\max_{z \\in S_{k}} \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2} \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t}^{T} dt - \\frac{1}{2} \\quad \\text{(strong duality)} \\\\\n&= \\min_{z \\in S_{k}} \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2}\n\\end{align*}\n$$\nWrite \\( g(d, z) = \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2} \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t}^{T} dt \\), then by setting \\( \\frac{\\partial g}{\\partial d} = 0 \\), we have \\( d^{*} = \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\).\n\nPlugging in \\( d^{*} \\) back, we have\n\n$$\n\\max_{d \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( (\\nabla \\log \\ell_{i,t})^{T} dt - \\frac{1}{2} dt^{T} dt \\right) = \\min_{z \\in S_{k}} \\lVert J_{t}z \\rVert^{2}.\n$$\nAt the optimum, we have \\( d^{*}_{t} = J_{t}z^{*}_{t} \\).\n\nThe dual problem in (6) can be viewed as optimizing the log objective of the multiple gradient descent algorithm (MGDA). Similar to MGDA, (6) only involves a decision variable of dimension \\( k \\ll m \\). Furthermore, if the optimal combination weights \\( z^{*}_{t} \\) is an interior point of \\( S_{k} \\), then the improvement rates \\( r_{i}(\\alpha, d^{*}_{t}) \\) of the different tasks \\( i \\) equal, as we show in the following result.\n\nTo avoid division by zero, in practice we add a small constant (e.g., \\( 1e^{-8} \\)) to all losses. For the ease of notation (e.g., \\( \\ell_{i}(\\cdot) \\leftarrow \\ell_{i}(\\cdot) + 1e^{-8} \\)), we omit it throughout the paper.", "md": "3.1 Balanced Rate of Loss Improvement\n\nAt time \\( t \\), assume we perform the update \\( \\theta_{t+1} = \\theta_{t} - \\alpha d_{t} \\), we define the rate of improvement for task \\( i \\) as\n\n$$\nr_{i}(\\alpha, dt) = \\frac{\\ell_{i,t} - \\ell_{i,t+1}}{\\ell_{i,t}} \\quad (3)\n$$\nFAMO then seeks an update \\( d_{t} \\) that results in the largest worst-case improvement rate across all tasks\n\n\\( \\left( \\frac{1}{2} \\lVert dt \\rVert \\right) \\) is subtracted to prevent an under-specified optimization problem where the objective can be infinitely large:\n\n$$\n\\max_{dt \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( \\alpha r_{i}(\\alpha, dt) - \\frac{1}{2} \\right) \\quad (4)\n$$\nWhen the step size \\( \\alpha \\) is small, using Taylor approximation, the problem (4) can be approximated by \\( \\frac{1}{2} \\lVert dt \\rVert^{2} \\).\n\n$$\n\\max_{dt \\in \\mathbb{R}^{m}} \\min_{i \\in [K]} \\left( \\nabla \\ell_{i}^{T} dt - \\frac{1}{2} dt^{T} dt - \\frac{1}{2} \\right) \\quad (5)\n$$\n$$\n= \\left( \\nabla \\log \\ell_{i,t} \\right)^{T} \\frac{1}{2} \\lVert dt \\rVert^{2}.\n$$\nInstead of solving the primal problem in (5) where \\( d \\in \\mathbb{R}^{m} \\) (\\( m \\) can be millions if \\( \\theta \\) is the parameter of a neural network), we consider its dual problem:\n\nProposition 3.1. The dual objective of (5) is\n\n$$\nz^{*}_{t} \\in \\arg \\min_{z \\in S_{k}} \\left( \\frac{1}{2} \\lVert J_{t}z \\rVert^{2} \\right), \\quad \\text{where} \\quad J_{t} = \\begin{bmatrix} \\nabla \\log \\ell_{1,t} \\\\ \\vdots \\\\ \\nabla \\log \\ell_{k,t} \\end{bmatrix} \\quad (6)\n$$\nwhere \\( z^{*}_{t} = [z^{*}_{t,i}] \\) is the optimal combination weights of the gradients, and the optimal update direction is \\( d^{*}_{t} = J_{t}z^{*}_{t} \\).\n\nProof.\n\n$$\n\\begin{align*}\n&\\max_{d \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( (\\nabla^{k} \\log \\ell_{i,t})^{T} dt - \\frac{1}{2} dt^{T} dt \\right) \\\\\n&= \\max_{z \\in S_{k}} \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2} \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t}^{T} dt - \\frac{1}{2} \\quad \\text{(strong duality)} \\\\\n&= \\min_{z \\in S_{k}} \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2}\n\\end{align*}\n$$\nWrite \\( g(d, z) = \\left( \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\right)^{T} dt - \\frac{1}{2} \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t}^{T} dt \\), then by setting \\( \\frac{\\partial g}{\\partial d} = 0 \\), we have \\( d^{*} = \\sum_{i=1}^{k} z_{i} \\nabla \\log \\ell_{i,t} \\).\n\nPlugging in \\( d^{*} \\) back, we have\n\n$$\n\\max_{d \\in \\mathbb{R}^{m}} \\min_{i \\in [k]} \\left( (\\nabla \\log \\ell_{i,t})^{T} dt - \\frac{1}{2} dt^{T} dt \\right) = \\min_{z \\in S_{k}} \\lVert J_{t}z \\rVert^{2}.\n$$\nAt the optimum, we have \\( d^{*}_{t} = J_{t}z^{*}_{t} \\).\n\nThe dual problem in (6) can be viewed as optimizing the log objective of the multiple gradient descent algorithm (MGDA). Similar to MGDA, (6) only involves a decision variable of dimension \\( k \\ll m \\). Furthermore, if the optimal combination weights \\( z^{*}_{t} \\) is an interior point of \\( S_{k} \\), then the improvement rates \\( r_{i}(\\alpha, d^{*}_{t}) \\) of the different tasks \\( i \\) equal, as we show in the following result.\n\nTo avoid division by zero, in practice we add a small constant (e.g., \\( 1e^{-8} \\)) to all losses. For the ease of notation (e.g., \\( \\ell_{i}(\\cdot) \\leftarrow \\ell_{i}(\\cdot) + 1e^{-8} \\)), we omit it throughout the paper."}]}, {"page": 5, "text": " Proposition 3.2. Assume {\u2113i}k                         i=1 are smooth and the optimal weights z\u2217                                t in (6) is an interior point of\n Sk, then                                                 \u2200   i \u2260   j \u2208   [k],             r\u2217i (d\u2217  t ) = r\u2217   j (d\u2217  t ),\n where r\u2217      i (d\u2217 t ) = lim\u03b1\u21920 1          \u03b1  ri(\u03b1,     d\u2217 t ).\n Proof. Consider the Lagrangian form of (6)\n                    L(z,      \u03bb,  \u00b5) = 1         \u2211 k   zi\u2207     log \u2113i,t\u2225     2  + \u03bb(     \u2211 k   zi \u2212    1) \u2212     \u2211k    \u00b5izi, where \u2200i,               \u00b5i \u2265     0.            (7)\n                                            2  \u2225i=1                                      i=1                    i=1\nWhen z\u2217           reaches the optimum, we have \u2202L(z,                                  \u03bb,  \u00b5)/\u2202z = 0, recall that d\u2217                  t = Jtz\u2217     t , then\n                                                                                    \u23a1 \u2207   log \u2113\u22ba       \u23a4\n                                                                                    \u23a2             1,t  \u23a5\n                                                                                    \u23a2         \u22ee        \u23a5\n                        J\u22ba t Jtz\u2217      = \u2212\u00b5 \u2212       \u03bb,      where Jt =              \u23a2                  \u23a5        \u21d2            J\u22bat d\u2217 t = \u2212(\u00b5 + \u03bb).\n                                                                                    \u23a2                  \u23a5\n                                                                                    \u23a2 \u2207   log \u2113\u22ba  k,t  \u23a5\nWhen z\u2217       t is an interior point of Sk, we know that \u00b5 = 0. Hence J\u22ba            \u23a3                  \u23a6             t d\u2217 t = \u2212\u03bb. This means,\n                      \u2200i \u2260     j,           lim      1              t ) = \u2207      log \u2113\u22ba   i,td\u2217 t = \u2207      log \u2113\u22ba  j,td\u2217  t = lim\u03b1\u21920     1                t ).\n                                            \u03b1\u21920     \u03b1   ri(\u03b1,     d\u2217                                                                     \u03b1   rj(\u03b1,     d\u2217\n 3.2       Fast Approximation by Amortizing over Time\n Instead of fully solving (6) at each optimization step, FAMO performs a single-step gradient descent\n on z, which amortizes the computation over the optimization trajectory:\n                                zt+1 = zt \u2212         \u03b1z    \u02dc                                 1      k                            2                                          (8)\n                                                         \u03b4,       where \u02dc      \u03b4 = \u2207z             \u2211    zi,t\u2207     log \u2113i,t\u2225         = J\u22ba  t Jtzt.\n But then, note that                                     \u23a1 log \u21131,t \u2212        log \u21131,t+1     2 \u23a4\u2225i=1\n                                                     1   \u23a2                                    \u23a5\n                                                         \u23a2                  \u22ee                 \u23a5  \u2248   J\u22bat dt = J\u22ba     t Jtzt,                                               (9)\n                                                         \u23a2                                    \u23a5\n                                                     \u03b1   \u23a2 log \u2113k,t \u2212        log \u2113k,t+1       \u23a5\n                                                         \u23a3                                    \u23a6\n so we can use the change in log losses to approximate the gradient.\n In practice, to ensure that z always stays in Sk, we re-parameterize z by \u03be and let zt = Softmax(\u03bet),\nwhere \u03bet \u2208          RK are the unconstrained softmax logits. Consequently, we have the following approxi-\n mate update on \u03be from (8):\n                                                                                   \u23a1 \u2207\u22baz1,t(\u03be)          \u23a4 \u22ba  \u23a1 log \u21131,t \u2212        log \u21131,t+1       \u23a4\n                                                                                   \u23a2                    \u23a5    \u23a2                                    \u23a5\n                               \u03bet+1 = \u03bet \u2212         \u03b2\u03b4,         where \u03b4 =           \u23a2          \u22ee         \u23a5    \u23a2                  \u22ee                 \u23a5 .                    (10)\n                                                                                   \u23a2                    \u23a5    \u23a2                                    \u23a5\n                                                                                   \u23a2 \u2207\u22bazk,t(\u03be)          \u23a5    \u23a2 log \u2113k,t \u2212        log \u2113k,t+1       \u23a5\n                                                                                   \u23a3                    \u23a6    \u23a3                                    \u23a6\n Remark: While it is possible to perform gradient descent on z for other gradient manipulation\n methods in principle, we will demonstrate in Appendix B that not all such updates can be easily\n approximated using the change in losses.\n 3.3       Practical Implementation\nTo facilitate practical implementation, we present two modifications to the update in (10).\n Re-normalization                      The suggested update above is a convex combination of the gradients of the log\n loss, e.g.,\n                                                        d\u2217   =    k    zi,t\u2207     log \u2113i,t =        k   (  zi,t   )\u2207\u2113i,t.\n                                                                 \u2211                                \u2211       \u2113i,t\nWhen \u2113i,t is small, the multiplicative coeffi                    i=1                cient zi,t    i=1\n                                                                                               \u2113i,t can be quite large and result in unstable\n optimization. Therefore, we propose to multiply d\u2217                                          by a constant ct, such that ctd\u2217                         can be written as\n a convex combination of the task gradients just as in other gradient manipulation algorithms (see (2)\n and we provide the corresponding definition of w in the following):\n                           ct = (     \u2211 k   zi,t   ) \u22121       and        dt = ctd\u2217         =  \u2211 k   wi\u2207\u2113i,t,           where wi = ct              zi,t   .               (11)\n                                      i=1   \u2113i,t                                         5    i=1                                                 \u2113i,t", "md": "# Math Equations and Text\n\n## Proposition 3.2\n\nAssume {\u2113i}i=1k are smooth and the optimal weights z*t in (6) is an interior point of Sk, then \u2200 i \u2260 j \u2208 [k], r*i(d*t) = r*j(d*t), where\n\n$$r*i(d*t) = \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} r_i(\u03b1, d*t)$$\nProof: Consider the Lagrangian form of (6)\n\n$$L(z, \u03bb, \u03bc) = \\frac{1}{2} \\sum_{i=1}^{k} zi\u2207 log \u2113i,t^2 + \u03bb(\\sum_{i=1}^{k} zi - 1) - \\sum_{i=1}^{k} \u03bcizi, \\text{where} \\forall i, \u03bci \u2265 0. (7)$$\nWhen z*t reaches the optimum, we have \u2202L(z, \u03bb, \u03bc)/\u2202z = 0, recall that d*t = Jtz*t, then\n\n$$J^T_t Jtz*t = -\u03bc - \u03bb, \\text{where} J_t = \\begin{bmatrix} \\nabla log \u2113^T_1,t \\\\ \\vdots \\\\ \\nabla log \u2113^T_k,t \\end{bmatrix} \\Rightarrow J^T_t d*t = -(\u03bc + \u03bb).$$\nWhen z*t is an interior point of Sk, we know that \u03bc = 0. Hence JTt d*t = -\u03bb. This means,\n\n$$\\forall i \u2260 j, \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} ri(\u03b1, d*t) = \\nabla log \u2113^T_i,td*t = \\nabla log \u2113^T_j,td*t = \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} rj(\u03b1, d*t).$$\n\n## 3.2 Fast Approximation by Amortizing over Time\n\nInstead of fully solving (6) at each optimization step, FAMO performs a single-step gradient descent on z, which amortizes the computation over the optimization trajectory:\n\n$$zt+1 = zt - \u03b1z\u02dc\u03b4, \\text{where} \u02dc\u03b4 = \\nabla_z \\sum zi,t\u2207 log \u2113i,t^2 = J^T_t Jtzt.$$\nBut then, note that\n\n$$\\begin{bmatrix} log \u21131,t - log \u21131,t+1 \\\\ \\vdots \\\\ log \u2113k,t - log \u2113k,t+1 \\end{bmatrix}^2 \u2248 J^T_t dt = J^T_t Jtzt, (9)$$\nso we can use the change in log losses to approximate the gradient.\n\nIn practice, to ensure that z always stays in Sk, we re-parameterize z by \u03be and let zt = Softmax(\u03bet), where \u03bet \u2208 RK are the unconstrained softmax logits. Consequently, we have the following approximate update on \u03be from (8):\n\n$$\\begin{align*} \u03bet+1 & = \u03bet - \u03b2\u03b4, \\text{where} \u03b4 = \\begin{bmatrix} \\nabla^Tz1,t(\u03be) \\\\ \\vdots \\\\ \\nabla^Tzk,t(\u03be) \\end{bmatrix}^T \\begin{bmatrix} log \u21131,t - log \u21131,t+1 \\\\ \\vdots \\\\ log \u2113k,t - log \u2113k,t+1 \\end{bmatrix} \\end{align*}$$\n\n## Remark:\n\nWhile it is possible to perform gradient descent on z for other gradient manipulation methods in principle, we will demonstrate in Appendix B that not all such updates can be easily approximated using the change in losses.\n\n## 3.3 Practical Implementation\n\nTo facilitate practical implementation, we present two modifications to the update in (10).\n\nRe-normalization: The suggested update above is a convex combination of the gradients of the log loss, e.g.,\n\n$$d* = \\sum_{i=1}^{k} zi,t\u2207 log \u2113i,t = \\sum_{i=1}^{k} (\\frac{zi,t}{\\sum_{i=1}^{k} \u2113i,t})\u2207\u2113i,t.$$\nWhen \u2113i,t is small, the multiplicative coefficient zi,t/\u2113i,t can be quite large and result in unstable optimization. Therefore, we propose to multiply d* by a constant ct, such that ctd* can be written as a convex combination of the task gradients just as in other gradient manipulation algorithms (see (2) and we provide the corresponding definition of w in the following):\n\n$$ct = (\\sum_{i=1}^{k} zi,t)^{-1} \\text{and} dt = ctd* = \\sum_{i=1}^{k} wi\u2207\u2113i,t, \\text{where} wi = ct zi,t / \u2113i,t.$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Proposition 3.2", "md": "## Proposition 3.2"}, {"type": "text", "value": "Assume {\u2113i}i=1k are smooth and the optimal weights z*t in (6) is an interior point of Sk, then \u2200 i \u2260 j \u2208 [k], r*i(d*t) = r*j(d*t), where\n\n$$r*i(d*t) = \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} r_i(\u03b1, d*t)$$\nProof: Consider the Lagrangian form of (6)\n\n$$L(z, \u03bb, \u03bc) = \\frac{1}{2} \\sum_{i=1}^{k} zi\u2207 log \u2113i,t^2 + \u03bb(\\sum_{i=1}^{k} zi - 1) - \\sum_{i=1}^{k} \u03bcizi, \\text{where} \\forall i, \u03bci \u2265 0. (7)$$\nWhen z*t reaches the optimum, we have \u2202L(z, \u03bb, \u03bc)/\u2202z = 0, recall that d*t = Jtz*t, then\n\n$$J^T_t Jtz*t = -\u03bc - \u03bb, \\text{where} J_t = \\begin{bmatrix} \\nabla log \u2113^T_1,t \\\\ \\vdots \\\\ \\nabla log \u2113^T_k,t \\end{bmatrix} \\Rightarrow J^T_t d*t = -(\u03bc + \u03bb).$$\nWhen z*t is an interior point of Sk, we know that \u03bc = 0. Hence JTt d*t = -\u03bb. This means,\n\n$$\\forall i \u2260 j, \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} ri(\u03b1, d*t) = \\nabla log \u2113^T_i,td*t = \\nabla log \u2113^T_j,td*t = \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} rj(\u03b1, d*t).$$", "md": "Assume {\u2113i}i=1k are smooth and the optimal weights z*t in (6) is an interior point of Sk, then \u2200 i \u2260 j \u2208 [k], r*i(d*t) = r*j(d*t), where\n\n$$r*i(d*t) = \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} r_i(\u03b1, d*t)$$\nProof: Consider the Lagrangian form of (6)\n\n$$L(z, \u03bb, \u03bc) = \\frac{1}{2} \\sum_{i=1}^{k} zi\u2207 log \u2113i,t^2 + \u03bb(\\sum_{i=1}^{k} zi - 1) - \\sum_{i=1}^{k} \u03bcizi, \\text{where} \\forall i, \u03bci \u2265 0. (7)$$\nWhen z*t reaches the optimum, we have \u2202L(z, \u03bb, \u03bc)/\u2202z = 0, recall that d*t = Jtz*t, then\n\n$$J^T_t Jtz*t = -\u03bc - \u03bb, \\text{where} J_t = \\begin{bmatrix} \\nabla log \u2113^T_1,t \\\\ \\vdots \\\\ \\nabla log \u2113^T_k,t \\end{bmatrix} \\Rightarrow J^T_t d*t = -(\u03bc + \u03bb).$$\nWhen z*t is an interior point of Sk, we know that \u03bc = 0. Hence JTt d*t = -\u03bb. This means,\n\n$$\\forall i \u2260 j, \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} ri(\u03b1, d*t) = \\nabla log \u2113^T_i,td*t = \\nabla log \u2113^T_j,td*t = \\lim_{\u03b1\u21920} \\frac{1}{\u03b1} rj(\u03b1, d*t).$$"}, {"type": "heading", "lvl": 2, "value": "3.2 Fast Approximation by Amortizing over Time", "md": "## 3.2 Fast Approximation by Amortizing over Time"}, {"type": "text", "value": "Instead of fully solving (6) at each optimization step, FAMO performs a single-step gradient descent on z, which amortizes the computation over the optimization trajectory:\n\n$$zt+1 = zt - \u03b1z\u02dc\u03b4, \\text{where} \u02dc\u03b4 = \\nabla_z \\sum zi,t\u2207 log \u2113i,t^2 = J^T_t Jtzt.$$\nBut then, note that\n\n$$\\begin{bmatrix} log \u21131,t - log \u21131,t+1 \\\\ \\vdots \\\\ log \u2113k,t - log \u2113k,t+1 \\end{bmatrix}^2 \u2248 J^T_t dt = J^T_t Jtzt, (9)$$\nso we can use the change in log losses to approximate the gradient.\n\nIn practice, to ensure that z always stays in Sk, we re-parameterize z by \u03be and let zt = Softmax(\u03bet), where \u03bet \u2208 RK are the unconstrained softmax logits. Consequently, we have the following approximate update on \u03be from (8):\n\n$$\\begin{align*} \u03bet+1 & = \u03bet - \u03b2\u03b4, \\text{where} \u03b4 = \\begin{bmatrix} \\nabla^Tz1,t(\u03be) \\\\ \\vdots \\\\ \\nabla^Tzk,t(\u03be) \\end{bmatrix}^T \\begin{bmatrix} log \u21131,t - log \u21131,t+1 \\\\ \\vdots \\\\ log \u2113k,t - log \u2113k,t+1 \\end{bmatrix} \\end{align*}$$", "md": "Instead of fully solving (6) at each optimization step, FAMO performs a single-step gradient descent on z, which amortizes the computation over the optimization trajectory:\n\n$$zt+1 = zt - \u03b1z\u02dc\u03b4, \\text{where} \u02dc\u03b4 = \\nabla_z \\sum zi,t\u2207 log \u2113i,t^2 = J^T_t Jtzt.$$\nBut then, note that\n\n$$\\begin{bmatrix} log \u21131,t - log \u21131,t+1 \\\\ \\vdots \\\\ log \u2113k,t - log \u2113k,t+1 \\end{bmatrix}^2 \u2248 J^T_t dt = J^T_t Jtzt, (9)$$\nso we can use the change in log losses to approximate the gradient.\n\nIn practice, to ensure that z always stays in Sk, we re-parameterize z by \u03be and let zt = Softmax(\u03bet), where \u03bet \u2208 RK are the unconstrained softmax logits. Consequently, we have the following approximate update on \u03be from (8):\n\n$$\\begin{align*} \u03bet+1 & = \u03bet - \u03b2\u03b4, \\text{where} \u03b4 = \\begin{bmatrix} \\nabla^Tz1,t(\u03be) \\\\ \\vdots \\\\ \\nabla^Tzk,t(\u03be) \\end{bmatrix}^T \\begin{bmatrix} log \u21131,t - log \u21131,t+1 \\\\ \\vdots \\\\ log \u2113k,t - log \u2113k,t+1 \\end{bmatrix} \\end{align*}$$"}, {"type": "heading", "lvl": 2, "value": "Remark:", "md": "## Remark:"}, {"type": "text", "value": "While it is possible to perform gradient descent on z for other gradient manipulation methods in principle, we will demonstrate in Appendix B that not all such updates can be easily approximated using the change in losses.", "md": "While it is possible to perform gradient descent on z for other gradient manipulation methods in principle, we will demonstrate in Appendix B that not all such updates can be easily approximated using the change in losses."}, {"type": "heading", "lvl": 2, "value": "3.3 Practical Implementation", "md": "## 3.3 Practical Implementation"}, {"type": "text", "value": "To facilitate practical implementation, we present two modifications to the update in (10).\n\nRe-normalization: The suggested update above is a convex combination of the gradients of the log loss, e.g.,\n\n$$d* = \\sum_{i=1}^{k} zi,t\u2207 log \u2113i,t = \\sum_{i=1}^{k} (\\frac{zi,t}{\\sum_{i=1}^{k} \u2113i,t})\u2207\u2113i,t.$$\nWhen \u2113i,t is small, the multiplicative coefficient zi,t/\u2113i,t can be quite large and result in unstable optimization. Therefore, we propose to multiply d* by a constant ct, such that ctd* can be written as a convex combination of the task gradients just as in other gradient manipulation algorithms (see (2) and we provide the corresponding definition of w in the following):\n\n$$ct = (\\sum_{i=1}^{k} zi,t)^{-1} \\text{and} dt = ctd* = \\sum_{i=1}^{k} wi\u2207\u2113i,t, \\text{where} wi = ct zi,t / \u2113i,t.$$", "md": "To facilitate practical implementation, we present two modifications to the update in (10).\n\nRe-normalization: The suggested update above is a convex combination of the gradients of the log loss, e.g.,\n\n$$d* = \\sum_{i=1}^{k} zi,t\u2207 log \u2113i,t = \\sum_{i=1}^{k} (\\frac{zi,t}{\\sum_{i=1}^{k} \u2113i,t})\u2207\u2113i,t.$$\nWhen \u2113i,t is small, the multiplicative coefficient zi,t/\u2113i,t can be quite large and result in unstable optimization. Therefore, we propose to multiply d* by a constant ct, such that ctd* can be written as a convex combination of the task gradients just as in other gradient manipulation algorithms (see (2) and we provide the corresponding definition of w in the following):\n\n$$ct = (\\sum_{i=1}^{k} zi,t)^{-1} \\text{and} dt = ctd* = \\sum_{i=1}^{k} wi\u2207\u2113i,t, \\text{where} wi = ct zi,t / \u2113i,t.$$"}]}, {"page": 6, "text": " Regularization                   As we are amortizing the computation over time and the loss objective {\u2113i(\u22c5)}s\n are changing dynamically, it makes sense to focus more on the recent updates of \u03be [46]. To this end,\nwe put a decay term on w such that the resulting \u03bet is an exponential moving average of its gradient\n updates:\n                          \u03bet+1 = \u03bet \u2212         \u03b2(\u03b4t + \u03b3\u03bet) = \u2212\u03b2(\u03b4t + (1 \u2212                        \u03b2\u03b3)\u03b4t\u22121 + (1 \u2212               \u03b2\u03b3)2\u03b4t\u22122 + .           . . ).                 (12)\nWe provide the complete FAMO algorithm in Algorithm 1 and its pseudocode in Appendix C.\n 3.4       The Continuous Limit of FAMO\n One way to characterize FAMO\u2019s behavior is to understand the stationary points of the continuous-\n time limit of FAMO (i.e. when step sizes (\u03b1,                                       \u03b2) shrink to zero). From Algorithm 1, one can derive\n the following non-autonomous dynamical system (assuming {\u2113i} are all smooth):                                        \u23a1 \u2207\u22baz1,t(\u03bet)          \u23a4\n                                        \u03b8\u02d9                            Jtzt                                            \u23a2                     \u23a5\n                                    [                                                                                 \u23a2           \u22ee         \u23a5  .                           (13)\n                                         \u02d9                 AtJ\u22ba                                                       \u23a2                     \u23a5\n                                        \u03be ] = \u2212ct [                t Jtzt + \u03b3     ct \u03bet]    , where At =              \u23a2 \u2207\u22bazk,t(\u03bet)          \u23a5\n(13) reaches its stationary points (or fixed points) when (note that ct > 0)                                          \u23a3                     \u23a6\n                                     \u03b8\u02d9                                                                               k\n                                  [   \u02d9                  \u21d2       Jtzt = 0 and \u03bet = 0  \u21d2                              \u2211    \u2207   log \u2113i,t = 0.                                (14)\nTherefore, the minimum points of \u2211k  \u03be ] = 0                       i=1 log \u2113i(\u03b8) are all stationary points of (13).  i=1\n 4       Related Work\n In this section, we summarize existing methods that tackle learning challenges in multitask learning\n(MTL). The general idea of most existing works is to encourage positive knowledge transfer by sharing\n parameters while decreasing any potential negative knowledge transfer (a.k.a, interference) during\n learning. There are three major ways of doing so: task grouping, designing network architectures\n specifically for MTL, and designing multitask optimization methods.\n Task Grouping                     Task grouping refers to grouping K tasks into N < K clusters and learning N\n models for each cluster. The key is estimating the amount of positive knowledge transfer incurred by\n grouping certain tasks together and then identifying which tasks should be grouped [39, 45, 38, 36,\n11].\n Multitask Architecture                            Novel neural architectures for MTL include hard-parameter-sharing\n methods, which decompose a neural network into task-specific modules and a shared feature extractor\n using manually designed heuristics [21, 29, 2], and soft-parameter-sharing methods, which learn\nwhich parameters to share [30, 34, 12, 27]. Recent studies extend neural architecture search for MTL\n by learning where to branch a network to have task-specific modules [14, 3].\n Multitask Optimization                           The most relevant approach to our method is MTL optimization via task\n balancing. These methods dynamically re-weight all task losses to mitigate the conflicting gradient\n issue [40, 43]. The simplest form of gradient manipulation is to re-weight the task losses based on\n manually designed criteria [6, 13, 18], but these methods are often heuristic and lack theoretical\n support. Gradient manipulation methods [35, 43, 25, 7, 16, 24, 32, 26, 47] propose to form a new\n update vector at each optimization by linearly combining task gradients. The local improvements\n across all tasks using the new update can often be explicitly analyzed, making these methods better\n understood in terms of convergence. However, it has been observed that gradient manipulation\n methods are often slow in practice, which may outweigh their performance benefits [22]. By contrast,\n FAMO is designed to match the performance of these methods while remaining effi                                                                           cient in terms\n of memory and computation. Another recent work proposes to sample random task weights at\n each optimization step for MTL [23], which is also computationally effi                                                               cient. However, we will\n demonstrate empirically that FAMO performs better than this method.\n                                                                                         6", "md": "Regularization\n\nAs we are amortizing the computation over time and the loss objective $$\\{\\ell_i(\\cdot)\\}$$ are changing dynamically, it makes sense to focus more on the recent updates of $$\\xi$$ [46]. To this end, we put a decay term on $$w$$ such that the resulting $$\\xi_t$$ is an exponential moving average of its gradient updates:\n\n$$\n\\xi_{t+1} = \\xi_t - \\beta(\\delta_t + \\gamma\\xi_t) = -\\beta(\\delta_t + (1 - \\beta\\gamma)\\delta_{t-1} + (1 - \\beta\\gamma)^2\\delta_{t-2} + ...)\n$$\n(12)\n\nWe provide the complete FAMO algorithm in Algorithm 1 and its pseudocode in Appendix C.\n\n### 3.4 The Continuous Limit of FAMO\n\nOne way to characterize FAMO\u2019s behavior is to understand the stationary points of the continuous-time limit of FAMO (i.e. when step sizes $$(\\alpha, \\beta)$$ shrink to zero). From Algorithm 1, one can derive the following non-autonomous dynamical system (assuming $$\\{\\ell_i\\}$$ are all smooth):\n\n$$\n\\begin{bmatrix}\n\\dot{\\theta} \\\\\n\\dot{\\xi}\n\\end{bmatrix} = -c_t \\begin{bmatrix}\nJ_tz_t \\\\\nAtJ^{\\top}_t\n\\end{bmatrix} + \\gamma c_t \\xi_t\n$$\nwhere $$A_t = \\sum_k \\nabla^{\\top}z_k,t(\\xi_t)$$.\n\nEquation (13) reaches its stationary points (or fixed points) when (note that $$c_t > 0$$):\n\n$$\n\\begin{aligned}\n\\dot{\\theta} &\\Rightarrow J_tz_t = 0 \\text{ and } \\xi_t = 0 \\\\\n\\dot{\\xi} &\\Rightarrow \\sum_k \\nabla \\log \\ell_{i,t} = 0\n\\end{aligned}\n$$\n(14)\n\nTherefore, the minimum points of $$\\sum_k \\xi = 0 \\log \\ell_i(\\theta)$$ are all stationary points of (13).\n\n### 4 Related Work\n\nIn this section, we summarize existing methods that tackle learning challenges in multitask learning (MTL). The general idea of most existing works is to encourage positive knowledge transfer by sharing parameters while decreasing any potential negative knowledge transfer (a.k.a, interference) during learning. There are three major ways of doing so: task grouping, designing network architectures specifically for MTL, and designing multitask optimization methods.\n\n#### Task Grouping\n\nTask grouping refers to grouping $$K$$ tasks into $$N < K$$ clusters and learning $$N$$ models for each cluster. The key is estimating the amount of positive knowledge transfer incurred by grouping certain tasks together and then identifying which tasks should be grouped [39, 45, 38, 36, 11].\n\n#### Multitask Architecture\n\nNovel neural architectures for MTL include hard-parameter-sharing methods, which decompose a neural network into task-specific modules and a shared feature extractor using manually designed heuristics [21, 29, 2], and soft-parameter-sharing methods, which learn which parameters to share [30, 34, 12, 27]. Recent studies extend neural architecture search for MTL by learning where to branch a network to have task-specific modules [14, 3].\n\n#### Multitask Optimization\n\nThe most relevant approach to our method is MTL optimization via task balancing. These methods dynamically re-weight all task losses to mitigate the conflicting gradient issue [40, 43]. The simplest form of gradient manipulation is to re-weight the task losses based on manually designed criteria [6, 13, 18], but these methods are often heuristic and lack theoretical support. Gradient manipulation methods [35, 43, 25, 7, 16, 24, 32, 26, 47] propose to form a new update vector at each optimization by linearly combining task gradients. The local improvements across all tasks using the new update can often be explicitly analyzed, making these methods better understood in terms of convergence. However, it has been observed that gradient manipulation methods are often slow in practice, which may outweigh their performance benefits [22]. By contrast, FAMO is designed to match the performance of these methods while remaining efficient in terms of memory and computation. Another recent work proposes to sample random task weights at each optimization step for MTL [23], which is also computationally efficient. However, we will demonstrate empirically that FAMO performs better than this method.", "images": [], "items": [{"type": "text", "value": "Regularization\n\nAs we are amortizing the computation over time and the loss objective $$\\{\\ell_i(\\cdot)\\}$$ are changing dynamically, it makes sense to focus more on the recent updates of $$\\xi$$ [46]. To this end, we put a decay term on $$w$$ such that the resulting $$\\xi_t$$ is an exponential moving average of its gradient updates:\n\n$$\n\\xi_{t+1} = \\xi_t - \\beta(\\delta_t + \\gamma\\xi_t) = -\\beta(\\delta_t + (1 - \\beta\\gamma)\\delta_{t-1} + (1 - \\beta\\gamma)^2\\delta_{t-2} + ...)\n$$\n(12)\n\nWe provide the complete FAMO algorithm in Algorithm 1 and its pseudocode in Appendix C.", "md": "Regularization\n\nAs we are amortizing the computation over time and the loss objective $$\\{\\ell_i(\\cdot)\\}$$ are changing dynamically, it makes sense to focus more on the recent updates of $$\\xi$$ [46]. To this end, we put a decay term on $$w$$ such that the resulting $$\\xi_t$$ is an exponential moving average of its gradient updates:\n\n$$\n\\xi_{t+1} = \\xi_t - \\beta(\\delta_t + \\gamma\\xi_t) = -\\beta(\\delta_t + (1 - \\beta\\gamma)\\delta_{t-1} + (1 - \\beta\\gamma)^2\\delta_{t-2} + ...)\n$$\n(12)\n\nWe provide the complete FAMO algorithm in Algorithm 1 and its pseudocode in Appendix C."}, {"type": "heading", "lvl": 3, "value": "3.4 The Continuous Limit of FAMO", "md": "### 3.4 The Continuous Limit of FAMO"}, {"type": "text", "value": "One way to characterize FAMO\u2019s behavior is to understand the stationary points of the continuous-time limit of FAMO (i.e. when step sizes $$(\\alpha, \\beta)$$ shrink to zero). From Algorithm 1, one can derive the following non-autonomous dynamical system (assuming $$\\{\\ell_i\\}$$ are all smooth):\n\n$$\n\\begin{bmatrix}\n\\dot{\\theta} \\\\\n\\dot{\\xi}\n\\end{bmatrix} = -c_t \\begin{bmatrix}\nJ_tz_t \\\\\nAtJ^{\\top}_t\n\\end{bmatrix} + \\gamma c_t \\xi_t\n$$\nwhere $$A_t = \\sum_k \\nabla^{\\top}z_k,t(\\xi_t)$$.\n\nEquation (13) reaches its stationary points (or fixed points) when (note that $$c_t > 0$$):\n\n$$\n\\begin{aligned}\n\\dot{\\theta} &\\Rightarrow J_tz_t = 0 \\text{ and } \\xi_t = 0 \\\\\n\\dot{\\xi} &\\Rightarrow \\sum_k \\nabla \\log \\ell_{i,t} = 0\n\\end{aligned}\n$$\n(14)\n\nTherefore, the minimum points of $$\\sum_k \\xi = 0 \\log \\ell_i(\\theta)$$ are all stationary points of (13).", "md": "One way to characterize FAMO\u2019s behavior is to understand the stationary points of the continuous-time limit of FAMO (i.e. when step sizes $$(\\alpha, \\beta)$$ shrink to zero). From Algorithm 1, one can derive the following non-autonomous dynamical system (assuming $$\\{\\ell_i\\}$$ are all smooth):\n\n$$\n\\begin{bmatrix}\n\\dot{\\theta} \\\\\n\\dot{\\xi}\n\\end{bmatrix} = -c_t \\begin{bmatrix}\nJ_tz_t \\\\\nAtJ^{\\top}_t\n\\end{bmatrix} + \\gamma c_t \\xi_t\n$$\nwhere $$A_t = \\sum_k \\nabla^{\\top}z_k,t(\\xi_t)$$.\n\nEquation (13) reaches its stationary points (or fixed points) when (note that $$c_t > 0$$):\n\n$$\n\\begin{aligned}\n\\dot{\\theta} &\\Rightarrow J_tz_t = 0 \\text{ and } \\xi_t = 0 \\\\\n\\dot{\\xi} &\\Rightarrow \\sum_k \\nabla \\log \\ell_{i,t} = 0\n\\end{aligned}\n$$\n(14)\n\nTherefore, the minimum points of $$\\sum_k \\xi = 0 \\log \\ell_i(\\theta)$$ are all stationary points of (13)."}, {"type": "heading", "lvl": 3, "value": "4 Related Work", "md": "### 4 Related Work"}, {"type": "text", "value": "In this section, we summarize existing methods that tackle learning challenges in multitask learning (MTL). The general idea of most existing works is to encourage positive knowledge transfer by sharing parameters while decreasing any potential negative knowledge transfer (a.k.a, interference) during learning. There are three major ways of doing so: task grouping, designing network architectures specifically for MTL, and designing multitask optimization methods.", "md": "In this section, we summarize existing methods that tackle learning challenges in multitask learning (MTL). The general idea of most existing works is to encourage positive knowledge transfer by sharing parameters while decreasing any potential negative knowledge transfer (a.k.a, interference) during learning. There are three major ways of doing so: task grouping, designing network architectures specifically for MTL, and designing multitask optimization methods."}, {"type": "heading", "lvl": 4, "value": "Task Grouping", "md": "#### Task Grouping"}, {"type": "text", "value": "Task grouping refers to grouping $$K$$ tasks into $$N < K$$ clusters and learning $$N$$ models for each cluster. The key is estimating the amount of positive knowledge transfer incurred by grouping certain tasks together and then identifying which tasks should be grouped [39, 45, 38, 36, 11].", "md": "Task grouping refers to grouping $$K$$ tasks into $$N < K$$ clusters and learning $$N$$ models for each cluster. The key is estimating the amount of positive knowledge transfer incurred by grouping certain tasks together and then identifying which tasks should be grouped [39, 45, 38, 36, 11]."}, {"type": "heading", "lvl": 4, "value": "Multitask Architecture", "md": "#### Multitask Architecture"}, {"type": "text", "value": "Novel neural architectures for MTL include hard-parameter-sharing methods, which decompose a neural network into task-specific modules and a shared feature extractor using manually designed heuristics [21, 29, 2], and soft-parameter-sharing methods, which learn which parameters to share [30, 34, 12, 27]. Recent studies extend neural architecture search for MTL by learning where to branch a network to have task-specific modules [14, 3].", "md": "Novel neural architectures for MTL include hard-parameter-sharing methods, which decompose a neural network into task-specific modules and a shared feature extractor using manually designed heuristics [21, 29, 2], and soft-parameter-sharing methods, which learn which parameters to share [30, 34, 12, 27]. Recent studies extend neural architecture search for MTL by learning where to branch a network to have task-specific modules [14, 3]."}, {"type": "heading", "lvl": 4, "value": "Multitask Optimization", "md": "#### Multitask Optimization"}, {"type": "text", "value": "The most relevant approach to our method is MTL optimization via task balancing. These methods dynamically re-weight all task losses to mitigate the conflicting gradient issue [40, 43]. The simplest form of gradient manipulation is to re-weight the task losses based on manually designed criteria [6, 13, 18], but these methods are often heuristic and lack theoretical support. Gradient manipulation methods [35, 43, 25, 7, 16, 24, 32, 26, 47] propose to form a new update vector at each optimization by linearly combining task gradients. The local improvements across all tasks using the new update can often be explicitly analyzed, making these methods better understood in terms of convergence. However, it has been observed that gradient manipulation methods are often slow in practice, which may outweigh their performance benefits [22]. By contrast, FAMO is designed to match the performance of these methods while remaining efficient in terms of memory and computation. Another recent work proposes to sample random task weights at each optimization step for MTL [23], which is also computationally efficient. However, we will demonstrate empirically that FAMO performs better than this method.", "md": "The most relevant approach to our method is MTL optimization via task balancing. These methods dynamically re-weight all task losses to mitigate the conflicting gradient issue [40, 43]. The simplest form of gradient manipulation is to re-weight the task losses based on manually designed criteria [6, 13, 18], but these methods are often heuristic and lack theoretical support. Gradient manipulation methods [35, 43, 25, 7, 16, 24, 32, 26, 47] propose to form a new update vector at each optimization by linearly combining task gradients. The local improvements across all tasks using the new update can often be explicitly analyzed, making these methods better understood in terms of convergence. However, it has been observed that gradient manipulation methods are often slow in practice, which may outweigh their performance benefits [22]. By contrast, FAMO is designed to match the performance of these methods while remaining efficient in terms of memory and computation. Another recent work proposes to sample random task weights at each optimization step for MTL [23], which is also computationally efficient. However, we will demonstrate empirically that FAMO performs better than this method."}]}, {"page": 7, "text": "5    Empirical Results\nWe conduct experiments to answer the following question:\nHow does FAMO perform in terms of space/time complexities and standard MTL metrics against\nprior MTL optimizers on standard benchmarks (e.g., supervised and reinforcement MTL problems)?\nIn the following, we first use a toy 2-task problem to demonstrate how FAMO mitigates CG while\nbeing efficient. Then we show that FAMO performs comparably or even better than state-of-the-\nart gradient manipulation methods on standard multitask supervised and reinforcement learning\nbenchmarks. In addition, FAMO requires significantly lower computation time when K is large\ncompared to other methods. Lastly, we conduct an ablation study on how robust FAMO is to \u03b3. Each\nsubsection first details the experimental setup and then analyzes the results.\n5.1   A Toy 2-Task Example\n            Figure 2: The average loss L0 and the two task losses L1 and L2 for the toy example.\nTo better understand the optimization trajectory of FAMO, we adopt the same 2D multitask op-\ntimization problem from NASHMTL [32] to visualize how FAMO balances different loss ob-\njectives.  The model parameter \u03b8 = (\u03b81,\u03b82) \u2208           R2.    The two tasks\u2019 objectives and their sur-\nface plots are provided in Appendix D and Figure 2. We compare FAMO against ADAM [19],\nMGDA [35], PCGRAD [43], CAGRAD [24], and NASHMTL [32]. We then pick 5 initial points\n\u03b8init \u2208{(\u22128.5,7.5),(\u22128.5,5),(0,0),(9,9),(10,\u22128)} and plot the corresponding optimization trajec-\ntories with different methods in Figure 1. Note that the toy example is constructed such that naively\napplying ADAM on the average loss can cause the failure of optimization for task 1.\nFindings: From Figure 1, we observe that FAMO, like all other gradient manipulation methods,\nmitigates the CG and reaches the Pareto front for all fi   ve runs. In the meantime, FAMO performs\nsimilarly to NASHMTL and achieves a balanced loss decrease even when the two task losses are\nimproperly scaled. Finally, as shown in the top-right of the plot, FAMO behaves similarly to ADAM\nin terms of the training time, which is 25\u00d7 faster than NASHMTL.\n5.2   MTL Performance\nMultitask Supervised Learning.          We consider four supervised benchmarks commonly used in\nprior MTL research [24, 27, 32, 33]: NYU-v2 [31] (3 tasks), CityScapes [8] (2 tasks), QM-9 [1] (11\ntasks), and CelebA [28] (40 tasks). Specifically, NYU-v2 is an indoor scene dataset consisting of\n1449 RGBD images and dense per-pixel labeling with 13 classes. The learning objectives include\nimage segmentation, depth prediction, and surface normal prediction based on any scene image.\nCityScapes dataset is similar to NYU-v2 but contains 5000 street-view RGBD images with per-pixel\nannotations. QM-9 dataset is a widely used benchmark in graph neural network learning. It consists\nof >130K molecules represented as graphs annotated with node and edge features. We follow the\nsame experimental setting used in NASHMTL [32], where the learning objective is to predict 11\nproperties of molecules. We use 110K molecules from the QM9 example in PyTorch Geometric [10],\n10K molecules for validation, and the rest of 10K molecules for testing. The characteristic of this\ndataset is that the 11 properties are at different scales, posing a challenge for task balancing in MTL.\nLastly, CelebA dataset contains 200K face images of 10K different celebrities, and each face image\nis provided with 40 facial binary attributes. Therefore, CelebA can be viewed as a 40-task MTL\nproblem. Different from NYU-v2, CityScapes, and QM-9, the number of tasks (K) in CelebA is\nmuch larger, hence posing a challenge to learning efficiency.\n                                                     7", "md": "# Empirical Results\n\n## Empirical Results\n\nWe conduct experiments to answer the following question: How does FAMO perform in terms of space/time complexities and standard MTL metrics against prior MTL optimizers on standard benchmarks (e.g., supervised and reinforcement MTL problems)? In the following, we first use a toy 2-task problem to demonstrate how FAMO mitigates CG while being efficient. Then we show that FAMO performs comparably or even better than state-of-the-art gradient manipulation methods on standard multitask supervised and reinforcement learning benchmarks. In addition, FAMO requires significantly lower computation time when K is large compared to other methods. Lastly, we conduct an ablation study on how robust FAMO is to \u03b3. Each subsection first details the experimental setup and then analyzes the results.\n\n### A Toy 2-Task Example\n\nFigure 2: The average loss L0 and the two task losses L1 and L2 for the toy example.\n\nTo better understand the optimization trajectory of FAMO, we adopt the same 2D multitask optimization problem from NASHMTL [32] to visualize how FAMO balances different loss objectives. The model parameter $$\\theta = (\\theta_1, \\theta_2) \\in \\mathbb{R}^2$$. The two tasks' objectives and their surface plots are provided in Appendix D and Figure 2. We compare FAMO against ADAM [19], MGDA [35], PCGRAD [43], CAGRAD [24], and NASHMTL [32]. We then pick 5 initial points $$\\theta_{init} \\in \\{(-8.5,7.5), (-8.5,5), (0,0), (9,9), (10,-8)\\}$$ and plot the corresponding optimization trajectories with different methods in Figure 1. Note that the toy example is constructed such that naively applying ADAM on the average loss can cause the failure of optimization for task 1.\n\nFindings: From Figure 1, we observe that FAMO, like all other gradient manipulation methods, mitigates the CG and reaches the Pareto front for all five runs. In the meantime, FAMO performs similarly to NASHMTL and achieves a balanced loss decrease even when the two task losses are improperly scaled. Finally, as shown in the top-right of the plot, FAMO behaves similarly to ADAM in terms of the training time, which is 25\u00d7 faster than NASHMTL.\n\n### MTL Performance\n\nMultitask Supervised Learning. We consider four supervised benchmarks commonly used in prior MTL research [24, 27, 32, 33]: NYU-v2 [31] (3 tasks), CityScapes [8] (2 tasks), QM-9 [1] (11 tasks), and CelebA [28] (40 tasks). Specifically, NYU-v2 is an indoor scene dataset consisting of 1449 RGBD images and dense per-pixel labeling with 13 classes. The learning objectives include image segmentation, depth prediction, and surface normal prediction based on any scene image. CityScapes dataset is similar to NYU-v2 but contains 5000 street-view RGBD images with per-pixel annotations. QM-9 dataset is a widely used benchmark in graph neural network learning. It consists of >130K molecules represented as graphs annotated with node and edge features. We follow the same experimental setting used in NASHMTL [32], where the learning objective is to predict 11 properties of molecules. We use 110K molecules from the QM9 example in PyTorch Geometric [10], 10K molecules for validation, and the rest of 10K molecules for testing. The characteristic of this dataset is that the 11 properties are at different scales, posing a challenge for task balancing in MTL. Lastly, CelebA dataset contains 200K face images of 10K different celebrities, and each face image is provided with 40 facial binary attributes. Therefore, CelebA can be viewed as a 40-task MTL problem. Different from NYU-v2, CityScapes, and QM-9, the number of tasks (K) in CelebA is much larger, hence posing a challenge to learning efficiency.", "images": [{"name": "page-7-0.jpg", "height": 93, "width": 317, "x": 147, "y": 244}], "items": [{"type": "heading", "lvl": 1, "value": "Empirical Results", "md": "# Empirical Results"}, {"type": "heading", "lvl": 2, "value": "Empirical Results", "md": "## Empirical Results"}, {"type": "text", "value": "We conduct experiments to answer the following question: How does FAMO perform in terms of space/time complexities and standard MTL metrics against prior MTL optimizers on standard benchmarks (e.g., supervised and reinforcement MTL problems)? In the following, we first use a toy 2-task problem to demonstrate how FAMO mitigates CG while being efficient. Then we show that FAMO performs comparably or even better than state-of-the-art gradient manipulation methods on standard multitask supervised and reinforcement learning benchmarks. In addition, FAMO requires significantly lower computation time when K is large compared to other methods. Lastly, we conduct an ablation study on how robust FAMO is to \u03b3. Each subsection first details the experimental setup and then analyzes the results.", "md": "We conduct experiments to answer the following question: How does FAMO perform in terms of space/time complexities and standard MTL metrics against prior MTL optimizers on standard benchmarks (e.g., supervised and reinforcement MTL problems)? In the following, we first use a toy 2-task problem to demonstrate how FAMO mitigates CG while being efficient. Then we show that FAMO performs comparably or even better than state-of-the-art gradient manipulation methods on standard multitask supervised and reinforcement learning benchmarks. In addition, FAMO requires significantly lower computation time when K is large compared to other methods. Lastly, we conduct an ablation study on how robust FAMO is to \u03b3. Each subsection first details the experimental setup and then analyzes the results."}, {"type": "heading", "lvl": 3, "value": "A Toy 2-Task Example", "md": "### A Toy 2-Task Example"}, {"type": "text", "value": "Figure 2: The average loss L0 and the two task losses L1 and L2 for the toy example.\n\nTo better understand the optimization trajectory of FAMO, we adopt the same 2D multitask optimization problem from NASHMTL [32] to visualize how FAMO balances different loss objectives. The model parameter $$\\theta = (\\theta_1, \\theta_2) \\in \\mathbb{R}^2$$. The two tasks' objectives and their surface plots are provided in Appendix D and Figure 2. We compare FAMO against ADAM [19], MGDA [35], PCGRAD [43], CAGRAD [24], and NASHMTL [32]. We then pick 5 initial points $$\\theta_{init} \\in \\{(-8.5,7.5), (-8.5,5), (0,0), (9,9), (10,-8)\\}$$ and plot the corresponding optimization trajectories with different methods in Figure 1. Note that the toy example is constructed such that naively applying ADAM on the average loss can cause the failure of optimization for task 1.\n\nFindings: From Figure 1, we observe that FAMO, like all other gradient manipulation methods, mitigates the CG and reaches the Pareto front for all five runs. In the meantime, FAMO performs similarly to NASHMTL and achieves a balanced loss decrease even when the two task losses are improperly scaled. Finally, as shown in the top-right of the plot, FAMO behaves similarly to ADAM in terms of the training time, which is 25\u00d7 faster than NASHMTL.", "md": "Figure 2: The average loss L0 and the two task losses L1 and L2 for the toy example.\n\nTo better understand the optimization trajectory of FAMO, we adopt the same 2D multitask optimization problem from NASHMTL [32] to visualize how FAMO balances different loss objectives. The model parameter $$\\theta = (\\theta_1, \\theta_2) \\in \\mathbb{R}^2$$. The two tasks' objectives and their surface plots are provided in Appendix D and Figure 2. We compare FAMO against ADAM [19], MGDA [35], PCGRAD [43], CAGRAD [24], and NASHMTL [32]. We then pick 5 initial points $$\\theta_{init} \\in \\{(-8.5,7.5), (-8.5,5), (0,0), (9,9), (10,-8)\\}$$ and plot the corresponding optimization trajectories with different methods in Figure 1. Note that the toy example is constructed such that naively applying ADAM on the average loss can cause the failure of optimization for task 1.\n\nFindings: From Figure 1, we observe that FAMO, like all other gradient manipulation methods, mitigates the CG and reaches the Pareto front for all five runs. In the meantime, FAMO performs similarly to NASHMTL and achieves a balanced loss decrease even when the two task losses are improperly scaled. Finally, as shown in the top-right of the plot, FAMO behaves similarly to ADAM in terms of the training time, which is 25\u00d7 faster than NASHMTL."}, {"type": "heading", "lvl": 3, "value": "MTL Performance", "md": "### MTL Performance"}, {"type": "text", "value": "Multitask Supervised Learning. We consider four supervised benchmarks commonly used in prior MTL research [24, 27, 32, 33]: NYU-v2 [31] (3 tasks), CityScapes [8] (2 tasks), QM-9 [1] (11 tasks), and CelebA [28] (40 tasks). Specifically, NYU-v2 is an indoor scene dataset consisting of 1449 RGBD images and dense per-pixel labeling with 13 classes. The learning objectives include image segmentation, depth prediction, and surface normal prediction based on any scene image. CityScapes dataset is similar to NYU-v2 but contains 5000 street-view RGBD images with per-pixel annotations. QM-9 dataset is a widely used benchmark in graph neural network learning. It consists of >130K molecules represented as graphs annotated with node and edge features. We follow the same experimental setting used in NASHMTL [32], where the learning objective is to predict 11 properties of molecules. We use 110K molecules from the QM9 example in PyTorch Geometric [10], 10K molecules for validation, and the rest of 10K molecules for testing. The characteristic of this dataset is that the 11 properties are at different scales, posing a challenge for task balancing in MTL. Lastly, CelebA dataset contains 200K face images of 10K different celebrities, and each face image is provided with 40 facial binary attributes. Therefore, CelebA can be viewed as a 40-task MTL problem. Different from NYU-v2, CityScapes, and QM-9, the number of tasks (K) in CelebA is much larger, hence posing a challenge to learning efficiency.", "md": "Multitask Supervised Learning. We consider four supervised benchmarks commonly used in prior MTL research [24, 27, 32, 33]: NYU-v2 [31] (3 tasks), CityScapes [8] (2 tasks), QM-9 [1] (11 tasks), and CelebA [28] (40 tasks). Specifically, NYU-v2 is an indoor scene dataset consisting of 1449 RGBD images and dense per-pixel labeling with 13 classes. The learning objectives include image segmentation, depth prediction, and surface normal prediction based on any scene image. CityScapes dataset is similar to NYU-v2 but contains 5000 street-view RGBD images with per-pixel annotations. QM-9 dataset is a widely used benchmark in graph neural network learning. It consists of >130K molecules represented as graphs annotated with node and edge features. We follow the same experimental setting used in NASHMTL [32], where the learning objective is to predict 11 properties of molecules. We use 110K molecules from the QM9 example in PyTorch Geometric [10], 10K molecules for validation, and the rest of 10K molecules for testing. The characteristic of this dataset is that the 11 properties are at different scales, posing a challenge for task balancing in MTL. Lastly, CelebA dataset contains 200K face images of 10K different celebrities, and each face image is provided with 40 facial binary attributes. Therefore, CelebA can be viewed as a 40-task MTL problem. Different from NYU-v2, CityScapes, and QM-9, the number of tasks (K) in CelebA is much larger, hence posing a challenge to learning efficiency."}]}, {"page": 8, "text": "                    Segmentation                Depth                         Surface Normal\n Method                                                           Angle Dist \u2193            Within t\u25cb \u2191         MR \u2193    \u2206m% \u2193\n                 mIoU \u2191    Pix Acc \u2191    Abs Err \u2193    Rel Err \u2193   Mean    Median     11.25     22.5      30\n STL               38.30       63.76       0.6754      0.2780    25.01     19.21    30.14   57.20    69.15\n LS                39.29       65.33       0.5493      0.2263    28.15     23.96    22.09   47.50    61.08     8.89        5.59\n SI                38.45       64.27       0.5354      0.2201    27.60     23.37    22.53   48.57    62.32     7.89        4.39\n RLW               37.17       63.77       0.5759      0.2410    28.27     24.18    22.26   47.05    60.62    11.22        7.78\n DWA               39.11       65.31       0.5510      0.2285    27.61     23.18    24.17   50.18    62.39     7.67        3.57\n UW                36.87       63.17       0.5446      0.2260    27.04     22.61    23.54   49.05    63.65     7.44        4.05\n MGDA              30.47       59.90       0.6070      0.2555    24.88     19.45    29.18   56.88    69.36     6.00        1.38\n PCGRAD            38.06       64.64       0.5550      0.2325    27.41     22.80    23.86   49.83    63.14     8.00        3.97\n GRADDROP          39.39       65.12       0.5455      0.2279    27.48     22.96    23.38   49.44    62.87     7.00        3.58\n CAGRAD            39.79       65.49       0.5486      0.2250    26.31     21.58    25.61   52.36    65.58     4.56        0.20\n IMTL-G            39.35       65.60       0.5426      0.2256    26.02     21.19    26.20   53.13    66.24     3.78       -0.76\n NASHMTL           40.13       65.93       0.5261      0.2171    25.26     20.08    28.40   55.47    68.15     2.11       -4.04\n FAMO              38.88       64.90       0.5474      0.2194    25.06     19.57    29.21   56.61    68.98     3.44       -4.10\nTable 1: Results on NYU-v2 dataset (3 tasks). Each experiment is repeated over 3 random seeds and the mean is\nreported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n Method            \u00b5      \u03b1    \u03f5HOMO    \u03f5LUMO     \u27e8R2\u27e9    ZPVE        U0       U        H       G       cv   MR \u2193     \u2206m% \u2193\n                                                         MAE \u2193\n STL            0.07    0.18     60.6     53.9    0.50      4.53    58.8     64.2    63.8     66.2   0.07\n LS             0.11    0.33     73.6     89.7    5.20    14.06    143.4    144.2   144.6    140.3   0.13     6.45        177.6\n SI             0.31    0.35    149.8    135.7    1.00      4.51    55.3     55.8    55.8     55.3   0.11     3.55         77.8\n RLW            0.11    0.34     76.9     92.8    5.87    15.47    156.3    157.1   157.6    153.0   0.14     8.00        203.8\n DWA            0.11    0.33     74.1     90.6    5.09    13.99    142.3    143.0   143.4    139.3   0.13     6.27        175.3\n UW             0.39    0.43    166.2    155.8    1.07      4.99    66.4     66.8    66.8     66.2   0.12     4.91        108.0\n MGDA           0.22    0.37    126.8    104.6    3.23      5.69    88.4     89.4    89.3     88.0   0.12     5.91        120.5\n PCGRAD         0.11    0.29     75.9     88.3    3.94      9.15   116.4    116.8   117.2    114.5   0.11     4.73        125.7\n CAGRAD         0.12    0.32     83.5     94.8    3.22      6.93   114.0    114.3   114.5    112.3   0.12     5.45        112.8\n IMTL-G         0.14    0.29     98.3     93.9    1.75      5.70   101.4    102.4   102.0    100.1   0.10     4.36         77.2\n NASHMTL        0.10    0.25     82.9     81.9    2.43      5.38    74.5     75.0    75.1     74.2   0.09     2.09         62.0\n FAMO           0.15    0.30     94.0     95.2    1.63      4.95   70.82     71.2    71.2     70.3   0.10     3.27         58.5\nTable 2: Results on QM-9 dataset (11 tasks). Each experiment is repeated over 3 random seeds and the mean is\nreported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\nWe compare FAMO against 11 MTL optimization methods and a single-task learning baseline: (1)\nSingle task learning (STL), training an independent model (\u03b8 for each task; (2) Linear scalarization\n(LS) baseline that minimizes L0; (3) Scale-invariant (SI) baseline that minimizes \u2211k log Lk(\u03b8), as\nSI is invariant to any scalar multiplication of task losses; (4) Dynamic Weight Average (DWA) [27],\na heuristic for adjusting task weights based on rates of loss changes; (5) Uncertainty Weighting\n(UW) [18] uses task uncertainty as a proxy to adjust task weights; (6) Random Loss Weighting\n(RLW) [23] that samples task weighting whose log-probabilities follow the normal distribution;\n(7) MGDA [35] that finds the equal descent direction for each task; (8) PCGRAD [43] proposes\nto project each task gradient to the normal plan of that of other tasks and combining them together\nin the end; (9) CAGRAD [24] optimizes the average loss while explicitly controls the minimum\ndecrease across tasks; (10) IMTL-G [25] finds the update direction with equal projections on task\ngradients; (11) GRADDROP [7] that randomly dropout certain dimensions of the task gradients based\non how much they conflict; (12) NASHMTL [32] formulates MTL as a bargaining game and finds\nthe solution to the game that benefits all tasks. For FAMO, we choose the best hyperparameter\n\u03b3 \u2208  {0.0001,0.001,0.01} based on the validation loss. Specifically, we choose \u03b3 equals 0.01 for the\nCityScapes dataset and 0.001 for the rest of the datasets. See Appendix E for results with error bars.\nEvaluations: We consider two metrics [32] for MTL: 1) \u2206m%, the average per-task performance\ndrop of a method m relative to the STL baseline denoted as b: \u2206m% =                               1     k=1(\u22121)\u03b4k(Mm,k \u2212\nMb,k)/Mb,k \u00d7 100, where Mb,k and Mm,k are the STL and m\u2019s value for metric Mk. \u03b4k = 1 (or 0) if   K \u2211K\nthe Mk is higher (or lower) the better. 2) Mean Rank (MR): the average rank of each method across\ntasks. For instance, if a method ranks first for every task, MR will be 1.\nFindings: Results on the four benchmark datasets are provided in Table 1, 2 and 3. We observe that\nFAMO performs consistently well across different supervised learning MTL benchmarks compared\n                                                                8", "md": "|Method|mIoU \u2191|Pix Acc \u2191|Abs Err \u2193|Rel Err \u2193|Mean Angle|Median Angle|11.25\u00b0|22.5\u00b0|30\u00b0|MR \u2193|\u2206m% \u2193|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|STL|38.30|63.76|0.6754|0.2780|25.01|19.21|30.14|57.20|69.15| | |\n|LS|39.29|65.33|0.5493|0.2263|28.15|23.96|22.09|47.50|61.08|8.89|5.59|\n|SI|38.45|64.27|0.5354|0.2201|27.60|23.37|22.53|48.57|62.32|7.89|4.39|\n|RLW|37.17|63.77|0.5759|0.2410|28.27|24.18|22.26|47.05|60.62|11.22|7.78|\n|DWA|39.11|65.31|0.5510|0.2285|27.61|23.18|24.17|50.18|62.39|7.67|3.57|\n|UW|36.87|63.17|0.5446|0.2260|27.04|22.61|23.54|49.05|63.65|7.44|4.05|\n|MGDA|30.47|59.90|0.6070|0.2555|24.88|19.45|29.18|56.88|69.36|6.00|1.38|\n|PCGRAD|38.06|64.64|0.5550|0.2325|27.41|22.80|23.86|49.83|63.14|8.00|3.97|\n|GRADDROP|39.39|65.12|0.5455|0.2279|27.48|22.96|23.38|49.44|62.87|7.00|3.58|\n|CAGRAD|39.79|65.49|0.5486|0.2250|26.31|21.58|25.61|52.36|65.58|4.56|0.20|\n|IMTL-G|39.35|65.60|0.5426|0.2256|26.02|21.19|26.20|53.13|66.24|3.78|-0.76|\n|NASHMTL|40.13|65.93|0.5261|0.2171|25.26|20.08|28.40|55.47|68.15|2.11|-4.04|\n|FAMO|38.88|64.90|0.5474|0.2194|25.06|19.57|29.21|56.61|68.98|3.44|-4.10|\n\nTable 1: Results on NYU-v2 dataset (3 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n\n|Method|\u00b5|\u03b1|\u03f5HOMO|\u03f5LUMO|\u27e8R\u00b2\u27e9|ZPVE|U\u2080|U|H|G|cv|MR \u2193|\u2206m% \u2193|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|STL|0.07|0.18|60.6|53.9|0.50|4.53|58.8|64.2|63.8|66.2|0.07| | |\n|LS|0.11|0.33|73.6|89.7|5.20|14.06|143.4|144.2|144.6|140.3|0.13|6.45|177.6|\n|SI|0.31|0.35|149.8|135.7|1.00|4.51|55.3|55.8|55.8|55.3|0.11|3.55|77.8|\n|RLW|0.11|0.34|76.9|92.8|5.87|15.47|156.3|157.1|157.6|153.0|0.14|8.00|203.8|\n|DWA|0.11|0.33|74.1|90.6|5.09|13.99|142.3|143.0|143.4|139.3|0.13|6.27|175.3|\n|UW|0.39|0.43|166.2|155.8|1.07|4.99|66.4|66.8|66.8|66.2|0.12|4.91|108.0|\n|MGDA|0.22|0.37|126.8|104.6|3.23|5.69|88.4|89.4|89.3|88.0|0.12|5.91|120.5|\n|PCGRAD|0.11|0.29|75.9|88.3|3.94|9.15|116.4|116.8|117.2|114.5|0.11|4.73|125.7|\n|CAGRAD|0.12|0.32|83.5|94.8|3.22|6.93|114.0|114.3|114.5|112.3|0.12|5.45|112.8|\n|IMTL-G|0.14|0.29|98.3|93.9|1.75|5.70|101.4|102.4|102.0|100.1|0.10|4.36|77.2|\n|NASHMTL|0.10|0.25|82.9|81.9|2.43|5.38|74.5|75.0|75.1|74.2|0.09|2.09|62.0|\n|FAMO|0.15|0.30|94.0|95.2|1.63|4.95|70.82|71.2|71.2|70.3|0.10|3.27|58.5|\n\nTable 2: Results on QM-9 dataset (11 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n\nWe compare FAMO against 11 MTL optimization methods and a single-task learning baseline: (1) Single task learning (STL), training an independent model (\u03b8 for each task; (2) Linear scalarization (LS) baseline that minimizes L0; (3) Scale-invariant (SI) baseline that minimizes \u2211k log Lk(\u03b8), as SI is invariant to any scalar multiplication of task losses; (4) Dynamic Weight Average (DWA) [27], a heuristic for adjusting task weights based on rates of loss changes; (5) Uncertainty Weighting (UW) [18] uses task uncertainty as a proxy to adjust task weights; (6) Random Loss Weighting (RLW) [23] that samples task weighting whose log-probabilities follow the normal distribution; (7) MGDA [35] that finds the equal descent direction for each task; (8) PCGRAD [43] proposes to project each task gradient to the normal plan of that of other tasks and combining them together in the end; (9) CAGRAD [24] optimizes the average loss while explicitly controls the minimum decrease across tasks; (10) IMTL-G [25] finds the update direction with equal projections on task gradients; (11) GRADDROP [7] that randomly dropout certain dimensions of the task gradients based on how much they conflict; (12) NASHMTL [32] formulates MTL as a bargaining gameand finds the solution to the game that benefits all tasks. For FAMO, we choose the best hyperparameter \u03b3 \u2208 {0.0001, 0.001, 0.01} based on the validation loss. Specifically, we choose \u03b3 equals 0.01 for the CityScapes dataset and 0.001 for the rest of the datasets. See Appendix E for results with error bars.\n\nEvaluations: We consider two metrics [32] for MTL:\n\n1. \u2206m%, the average per-task performance drop of a method m relative to the STL baseline denoted as b:\n$\\Delta m\\% = \\frac{1}{K} \\sum_{k=1}^{K} (-1)^{\\delta_k} \\frac{M_{m,k} - M_{b,k}}{M_{b,k}} \\times 100$\nwhere \\(M_{b,k}\\) and \\(M_{m,k}\\) are the STL and m\u2019s value for metric \\(M_k\\). \\(\\delta_k = 1\\) (or 0) if the \\(M_k\\) is higher (or lower) the better.\n2. Mean Rank (MR): the average rank of each method across tasks. For instance, if a method ranks first for every task, MR will be 1.\n\nFindings: Results on the four benchmark datasets are provided in Table 1, 2 and 3. We observe that FAMO performs consistently well across different supervised learning MTL benchmarks compared.", "images": [], "items": [{"type": "table", "rows": [["Method", "mIoU \u2191", "Pix Acc \u2191", "Abs Err \u2193", "Rel Err \u2193", "Mean Angle", "Median Angle", "11.25\u00b0", "22.5\u00b0", "30\u00b0", "MR \u2193", "\u2206m% \u2193"], ["STL", "38.30", "63.76", "0.6754", "0.2780", "25.01", "19.21", "30.14", "57.20", "69.15", "", ""], ["LS", "39.29", "65.33", "0.5493", "0.2263", "28.15", "23.96", "22.09", "47.50", "61.08", "8.89", "5.59"], ["SI", "38.45", "64.27", "0.5354", "0.2201", "27.60", "23.37", "22.53", "48.57", "62.32", "7.89", "4.39"], ["RLW", "37.17", "63.77", "0.5759", "0.2410", "28.27", "24.18", "22.26", "47.05", "60.62", "11.22", "7.78"], ["DWA", "39.11", "65.31", "0.5510", "0.2285", "27.61", "23.18", "24.17", "50.18", "62.39", "7.67", "3.57"], ["UW", "36.87", "63.17", "0.5446", "0.2260", "27.04", "22.61", "23.54", "49.05", "63.65", "7.44", "4.05"], ["MGDA", "30.47", "59.90", "0.6070", "0.2555", "24.88", "19.45", "29.18", "56.88", "69.36", "6.00", "1.38"], ["PCGRAD", "38.06", "64.64", "0.5550", "0.2325", "27.41", "22.80", "23.86", "49.83", "63.14", "8.00", "3.97"], ["GRADDROP", "39.39", "65.12", "0.5455", "0.2279", "27.48", "22.96", "23.38", "49.44", "62.87", "7.00", "3.58"], ["CAGRAD", "39.79", "65.49", "0.5486", "0.2250", "26.31", "21.58", "25.61", "52.36", "65.58", "4.56", "0.20"], ["IMTL-G", "39.35", "65.60", "0.5426", "0.2256", "26.02", "21.19", "26.20", "53.13", "66.24", "3.78", "-0.76"], ["NASHMTL", "40.13", "65.93", "0.5261", "0.2171", "25.26", "20.08", "28.40", "55.47", "68.15", "2.11", "-4.04"], ["FAMO", "38.88", "64.90", "0.5474", "0.2194", "25.06", "19.57", "29.21", "56.61", "68.98", "3.44", "-4.10"]], "md": "|Method|mIoU \u2191|Pix Acc \u2191|Abs Err \u2193|Rel Err \u2193|Mean Angle|Median Angle|11.25\u00b0|22.5\u00b0|30\u00b0|MR \u2193|\u2206m% \u2193|\n|---|---|---|---|---|---|---|---|---|---|---|---|\n|STL|38.30|63.76|0.6754|0.2780|25.01|19.21|30.14|57.20|69.15| | |\n|LS|39.29|65.33|0.5493|0.2263|28.15|23.96|22.09|47.50|61.08|8.89|5.59|\n|SI|38.45|64.27|0.5354|0.2201|27.60|23.37|22.53|48.57|62.32|7.89|4.39|\n|RLW|37.17|63.77|0.5759|0.2410|28.27|24.18|22.26|47.05|60.62|11.22|7.78|\n|DWA|39.11|65.31|0.5510|0.2285|27.61|23.18|24.17|50.18|62.39|7.67|3.57|\n|UW|36.87|63.17|0.5446|0.2260|27.04|22.61|23.54|49.05|63.65|7.44|4.05|\n|MGDA|30.47|59.90|0.6070|0.2555|24.88|19.45|29.18|56.88|69.36|6.00|1.38|\n|PCGRAD|38.06|64.64|0.5550|0.2325|27.41|22.80|23.86|49.83|63.14|8.00|3.97|\n|GRADDROP|39.39|65.12|0.5455|0.2279|27.48|22.96|23.38|49.44|62.87|7.00|3.58|\n|CAGRAD|39.79|65.49|0.5486|0.2250|26.31|21.58|25.61|52.36|65.58|4.56|0.20|\n|IMTL-G|39.35|65.60|0.5426|0.2256|26.02|21.19|26.20|53.13|66.24|3.78|-0.76|\n|NASHMTL|40.13|65.93|0.5261|0.2171|25.26|20.08|28.40|55.47|68.15|2.11|-4.04|\n|FAMO|38.88|64.90|0.5474|0.2194|25.06|19.57|29.21|56.61|68.98|3.44|-4.10|", "isPerfectTable": true, "csv": "\"Method\",\"mIoU \u2191\",\"Pix Acc \u2191\",\"Abs Err \u2193\",\"Rel Err \u2193\",\"Mean Angle\",\"Median Angle\",\"11.25\u00b0\",\"22.5\u00b0\",\"30\u00b0\",\"MR \u2193\",\"\u2206m% \u2193\"\n\"STL\",\"38.30\",\"63.76\",\"0.6754\",\"0.2780\",\"25.01\",\"19.21\",\"30.14\",\"57.20\",\"69.15\",\"\",\"\"\n\"LS\",\"39.29\",\"65.33\",\"0.5493\",\"0.2263\",\"28.15\",\"23.96\",\"22.09\",\"47.50\",\"61.08\",\"8.89\",\"5.59\"\n\"SI\",\"38.45\",\"64.27\",\"0.5354\",\"0.2201\",\"27.60\",\"23.37\",\"22.53\",\"48.57\",\"62.32\",\"7.89\",\"4.39\"\n\"RLW\",\"37.17\",\"63.77\",\"0.5759\",\"0.2410\",\"28.27\",\"24.18\",\"22.26\",\"47.05\",\"60.62\",\"11.22\",\"7.78\"\n\"DWA\",\"39.11\",\"65.31\",\"0.5510\",\"0.2285\",\"27.61\",\"23.18\",\"24.17\",\"50.18\",\"62.39\",\"7.67\",\"3.57\"\n\"UW\",\"36.87\",\"63.17\",\"0.5446\",\"0.2260\",\"27.04\",\"22.61\",\"23.54\",\"49.05\",\"63.65\",\"7.44\",\"4.05\"\n\"MGDA\",\"30.47\",\"59.90\",\"0.6070\",\"0.2555\",\"24.88\",\"19.45\",\"29.18\",\"56.88\",\"69.36\",\"6.00\",\"1.38\"\n\"PCGRAD\",\"38.06\",\"64.64\",\"0.5550\",\"0.2325\",\"27.41\",\"22.80\",\"23.86\",\"49.83\",\"63.14\",\"8.00\",\"3.97\"\n\"GRADDROP\",\"39.39\",\"65.12\",\"0.5455\",\"0.2279\",\"27.48\",\"22.96\",\"23.38\",\"49.44\",\"62.87\",\"7.00\",\"3.58\"\n\"CAGRAD\",\"39.79\",\"65.49\",\"0.5486\",\"0.2250\",\"26.31\",\"21.58\",\"25.61\",\"52.36\",\"65.58\",\"4.56\",\"0.20\"\n\"IMTL-G\",\"39.35\",\"65.60\",\"0.5426\",\"0.2256\",\"26.02\",\"21.19\",\"26.20\",\"53.13\",\"66.24\",\"3.78\",\"-0.76\"\n\"NASHMTL\",\"40.13\",\"65.93\",\"0.5261\",\"0.2171\",\"25.26\",\"20.08\",\"28.40\",\"55.47\",\"68.15\",\"2.11\",\"-4.04\"\n\"FAMO\",\"38.88\",\"64.90\",\"0.5474\",\"0.2194\",\"25.06\",\"19.57\",\"29.21\",\"56.61\",\"68.98\",\"3.44\",\"-4.10\""}, {"type": "text", "value": "Table 1: Results on NYU-v2 dataset (3 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.", "md": "Table 1: Results on NYU-v2 dataset (3 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance."}, {"type": "table", "rows": [["Method", "\u00b5", "\u03b1", "\u03f5HOMO", "\u03f5LUMO", "\u27e8R\u00b2\u27e9", "ZPVE", "U\u2080", "U", "H", "G", "cv", "MR \u2193", "\u2206m% \u2193"], ["STL", "0.07", "0.18", "60.6", "53.9", "0.50", "4.53", "58.8", "64.2", "63.8", "66.2", "0.07", "", ""], ["LS", "0.11", "0.33", "73.6", "89.7", "5.20", "14.06", "143.4", "144.2", "144.6", "140.3", "0.13", "6.45", "177.6"], ["SI", "0.31", "0.35", "149.8", "135.7", "1.00", "4.51", "55.3", "55.8", "55.8", "55.3", "0.11", "3.55", "77.8"], ["RLW", "0.11", "0.34", "76.9", "92.8", "5.87", "15.47", "156.3", "157.1", "157.6", "153.0", "0.14", "8.00", "203.8"], ["DWA", "0.11", "0.33", "74.1", "90.6", "5.09", "13.99", "142.3", "143.0", "143.4", "139.3", "0.13", "6.27", "175.3"], ["UW", "0.39", "0.43", "166.2", "155.8", "1.07", "4.99", "66.4", "66.8", "66.8", "66.2", "0.12", "4.91", "108.0"], ["MGDA", "0.22", "0.37", "126.8", "104.6", "3.23", "5.69", "88.4", "89.4", "89.3", "88.0", "0.12", "5.91", "120.5"], ["PCGRAD", "0.11", "0.29", "75.9", "88.3", "3.94", "9.15", "116.4", "116.8", "117.2", "114.5", "0.11", "4.73", "125.7"], ["CAGRAD", "0.12", "0.32", "83.5", "94.8", "3.22", "6.93", "114.0", "114.3", "114.5", "112.3", "0.12", "5.45", "112.8"], ["IMTL-G", "0.14", "0.29", "98.3", "93.9", "1.75", "5.70", "101.4", "102.4", "102.0", "100.1", "0.10", "4.36", "77.2"], ["NASHMTL", "0.10", "0.25", "82.9", "81.9", "2.43", "5.38", "74.5", "75.0", "75.1", "74.2", "0.09", "2.09", "62.0"], ["FAMO", "0.15", "0.30", "94.0", "95.2", "1.63", "4.95", "70.82", "71.2", "71.2", "70.3", "0.10", "3.27", "58.5"]], "md": "|Method|\u00b5|\u03b1|\u03f5HOMO|\u03f5LUMO|\u27e8R\u00b2\u27e9|ZPVE|U\u2080|U|H|G|cv|MR \u2193|\u2206m% \u2193|\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n|STL|0.07|0.18|60.6|53.9|0.50|4.53|58.8|64.2|63.8|66.2|0.07| | |\n|LS|0.11|0.33|73.6|89.7|5.20|14.06|143.4|144.2|144.6|140.3|0.13|6.45|177.6|\n|SI|0.31|0.35|149.8|135.7|1.00|4.51|55.3|55.8|55.8|55.3|0.11|3.55|77.8|\n|RLW|0.11|0.34|76.9|92.8|5.87|15.47|156.3|157.1|157.6|153.0|0.14|8.00|203.8|\n|DWA|0.11|0.33|74.1|90.6|5.09|13.99|142.3|143.0|143.4|139.3|0.13|6.27|175.3|\n|UW|0.39|0.43|166.2|155.8|1.07|4.99|66.4|66.8|66.8|66.2|0.12|4.91|108.0|\n|MGDA|0.22|0.37|126.8|104.6|3.23|5.69|88.4|89.4|89.3|88.0|0.12|5.91|120.5|\n|PCGRAD|0.11|0.29|75.9|88.3|3.94|9.15|116.4|116.8|117.2|114.5|0.11|4.73|125.7|\n|CAGRAD|0.12|0.32|83.5|94.8|3.22|6.93|114.0|114.3|114.5|112.3|0.12|5.45|112.8|\n|IMTL-G|0.14|0.29|98.3|93.9|1.75|5.70|101.4|102.4|102.0|100.1|0.10|4.36|77.2|\n|NASHMTL|0.10|0.25|82.9|81.9|2.43|5.38|74.5|75.0|75.1|74.2|0.09|2.09|62.0|\n|FAMO|0.15|0.30|94.0|95.2|1.63|4.95|70.82|71.2|71.2|70.3|0.10|3.27|58.5|", "isPerfectTable": true, "csv": "\"Method\",\"\u00b5\",\"\u03b1\",\"\u03f5HOMO\",\"\u03f5LUMO\",\"\u27e8R\u00b2\u27e9\",\"ZPVE\",\"U\u2080\",\"U\",\"H\",\"G\",\"cv\",\"MR \u2193\",\"\u2206m% \u2193\"\n\"STL\",\"0.07\",\"0.18\",\"60.6\",\"53.9\",\"0.50\",\"4.53\",\"58.8\",\"64.2\",\"63.8\",\"66.2\",\"0.07\",\"\",\"\"\n\"LS\",\"0.11\",\"0.33\",\"73.6\",\"89.7\",\"5.20\",\"14.06\",\"143.4\",\"144.2\",\"144.6\",\"140.3\",\"0.13\",\"6.45\",\"177.6\"\n\"SI\",\"0.31\",\"0.35\",\"149.8\",\"135.7\",\"1.00\",\"4.51\",\"55.3\",\"55.8\",\"55.8\",\"55.3\",\"0.11\",\"3.55\",\"77.8\"\n\"RLW\",\"0.11\",\"0.34\",\"76.9\",\"92.8\",\"5.87\",\"15.47\",\"156.3\",\"157.1\",\"157.6\",\"153.0\",\"0.14\",\"8.00\",\"203.8\"\n\"DWA\",\"0.11\",\"0.33\",\"74.1\",\"90.6\",\"5.09\",\"13.99\",\"142.3\",\"143.0\",\"143.4\",\"139.3\",\"0.13\",\"6.27\",\"175.3\"\n\"UW\",\"0.39\",\"0.43\",\"166.2\",\"155.8\",\"1.07\",\"4.99\",\"66.4\",\"66.8\",\"66.8\",\"66.2\",\"0.12\",\"4.91\",\"108.0\"\n\"MGDA\",\"0.22\",\"0.37\",\"126.8\",\"104.6\",\"3.23\",\"5.69\",\"88.4\",\"89.4\",\"89.3\",\"88.0\",\"0.12\",\"5.91\",\"120.5\"\n\"PCGRAD\",\"0.11\",\"0.29\",\"75.9\",\"88.3\",\"3.94\",\"9.15\",\"116.4\",\"116.8\",\"117.2\",\"114.5\",\"0.11\",\"4.73\",\"125.7\"\n\"CAGRAD\",\"0.12\",\"0.32\",\"83.5\",\"94.8\",\"3.22\",\"6.93\",\"114.0\",\"114.3\",\"114.5\",\"112.3\",\"0.12\",\"5.45\",\"112.8\"\n\"IMTL-G\",\"0.14\",\"0.29\",\"98.3\",\"93.9\",\"1.75\",\"5.70\",\"101.4\",\"102.4\",\"102.0\",\"100.1\",\"0.10\",\"4.36\",\"77.2\"\n\"NASHMTL\",\"0.10\",\"0.25\",\"82.9\",\"81.9\",\"2.43\",\"5.38\",\"74.5\",\"75.0\",\"75.1\",\"74.2\",\"0.09\",\"2.09\",\"62.0\"\n\"FAMO\",\"0.15\",\"0.30\",\"94.0\",\"95.2\",\"1.63\",\"4.95\",\"70.82\",\"71.2\",\"71.2\",\"70.3\",\"0.10\",\"3.27\",\"58.5\""}, {"type": "text", "value": "Table 2: Results on QM-9 dataset (11 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n\nWe compare FAMO against 11 MTL optimization methods and a single-task learning baseline: (1) Single task learning (STL), training an independent model (\u03b8 for each task; (2) Linear scalarization (LS) baseline that minimizes L0; (3) Scale-invariant (SI) baseline that minimizes \u2211k log Lk(\u03b8), as SI is invariant to any scalar multiplication of task losses; (4) Dynamic Weight Average (DWA) [27], a heuristic for adjusting task weights based on rates of loss changes; (5) Uncertainty Weighting (UW) [18] uses task uncertainty as a proxy to adjust task weights; (6) Random Loss Weighting (RLW) [23] that samples task weighting whose log-probabilities follow the normal distribution; (7) MGDA [35] that finds the equal descent direction for each task; (8) PCGRAD [43] proposes to project each task gradient to the normal plan of that of other tasks and combining them together in the end; (9) CAGRAD [24] optimizes the average loss while explicitly controls the minimum decrease across tasks; (10) IMTL-G [25] finds the update direction with equal projections on task gradients; (11) GRADDROP [7] that randomly dropout certain dimensions of the task gradients based on how much they conflict; (12) NASHMTL [32] formulates MTL as a bargaining gameand finds the solution to the game that benefits all tasks. For FAMO, we choose the best hyperparameter \u03b3 \u2208 {0.0001, 0.001, 0.01} based on the validation loss. Specifically, we choose \u03b3 equals 0.01 for the CityScapes dataset and 0.001 for the rest of the datasets. See Appendix E for results with error bars.\n\nEvaluations: We consider two metrics [32] for MTL:\n\n1. \u2206m%, the average per-task performance drop of a method m relative to the STL baseline denoted as b:\n$\\Delta m\\% = \\frac{1}{K} \\sum_{k=1}^{K} (-1)^{\\delta_k} \\frac{M_{m,k} - M_{b,k}}{M_{b,k}} \\times 100$\nwhere \\(M_{b,k}\\) and \\(M_{m,k}\\) are the STL and m\u2019s value for metric \\(M_k\\). \\(\\delta_k = 1\\) (or 0) if the \\(M_k\\) is higher (or lower) the better.\n2. Mean Rank (MR): the average rank of each method across tasks. For instance, if a method ranks first for every task, MR will be 1.\n\nFindings: Results on the four benchmark datasets are provided in Table 1, 2 and 3. We observe that FAMO performs consistently well across different supervised learning MTL benchmarks compared.", "md": "Table 2: Results on QM-9 dataset (11 tasks). Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n\nWe compare FAMO against 11 MTL optimization methods and a single-task learning baseline: (1) Single task learning (STL), training an independent model (\u03b8 for each task; (2) Linear scalarization (LS) baseline that minimizes L0; (3) Scale-invariant (SI) baseline that minimizes \u2211k log Lk(\u03b8), as SI is invariant to any scalar multiplication of task losses; (4) Dynamic Weight Average (DWA) [27], a heuristic for adjusting task weights based on rates of loss changes; (5) Uncertainty Weighting (UW) [18] uses task uncertainty as a proxy to adjust task weights; (6) Random Loss Weighting (RLW) [23] that samples task weighting whose log-probabilities follow the normal distribution; (7) MGDA [35] that finds the equal descent direction for each task; (8) PCGRAD [43] proposes to project each task gradient to the normal plan of that of other tasks and combining them together in the end; (9) CAGRAD [24] optimizes the average loss while explicitly controls the minimum decrease across tasks; (10) IMTL-G [25] finds the update direction with equal projections on task gradients; (11) GRADDROP [7] that randomly dropout certain dimensions of the task gradients based on how much they conflict; (12) NASHMTL [32] formulates MTL as a bargaining gameand finds the solution to the game that benefits all tasks. For FAMO, we choose the best hyperparameter \u03b3 \u2208 {0.0001, 0.001, 0.01} based on the validation loss. Specifically, we choose \u03b3 equals 0.01 for the CityScapes dataset and 0.001 for the rest of the datasets. See Appendix E for results with error bars.\n\nEvaluations: We consider two metrics [32] for MTL:\n\n1. \u2206m%, the average per-task performance drop of a method m relative to the STL baseline denoted as b:\n$\\Delta m\\% = \\frac{1}{K} \\sum_{k=1}^{K} (-1)^{\\delta_k} \\frac{M_{m,k} - M_{b,k}}{M_{b,k}} \\times 100$\nwhere \\(M_{b,k}\\) and \\(M_{m,k}\\) are the STL and m\u2019s value for metric \\(M_k\\). \\(\\delta_k = 1\\) (or 0) if the \\(M_k\\) is higher (or lower) the better.\n2. Mean Rank (MR): the average rank of each method across tasks. For instance, if a method ranks first for every task, MR will be 1.\n\nFindings: Results on the four benchmark datasets are provided in Table 1, 2 and 3. We observe that FAMO performs consistently well across different supervised learning MTL benchmarks compared."}]}, {"page": 9, "text": "                                               CityScapes                                       CelebA\n  Method              Segmentation                 Depth\n                   mIoU \u2191     Pix Acc \u2191    Abs Err \u2193    Rel Err \u2193   MR \u2193      \u2206m% \u2193       MR \u2193      \u2206m% \u2193\n  STL                74.01        93.16       0.0125       27.77\n  LS                 70.95        91.73       0.0161       33.83      6.50        14.11     4.15         6.28\n  SI                 70.95        91.73       0.0161       33.83      9.25        14.11     7.20         7.83\n  RLW                74.57        93.41       0.0158       47.79      9.25        24.38     1.46         5.22\n  DWA                75.24        93.52       0.0160       44.37      6.50        21.45     3.20         6.95\n  UW                 72.02        92.85       0.0140       30.13      6.00         5.89     3.23         5.78\n  MGDA               68.84        91.54       0.0309       33.50      9.75        44.14    14.85        10.93\n  PCGRAD             75.13        93.48       0.0154       42.07      6.75        18.29     3.17         6.65\n  GRADDROP           75.27        93.53       0.0157       47.54      6.00        23.73     3.29         7.80\n  CAGRAD             75.16        93.48       0.0141       37.60      5.75        11.64     2.48         6.20\n  IMTL-G             75.33        93.49       0.0135       38.41      4.00        11.10     0.84         4.67\n  NASHMTL            75.41        93.66       0.0129       35.02      2.00         6.82     2.84         4.97\n  FAMO               74.54        93.29       0.0145       32.59      6.25         8.13     1.21         4.72\n Table 3: Results on CityScapes (2 tasks) and CelebA (40 tasks) datasets. Each experiment is repeated over 3\n random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main\n metrics for MTL performance.\n to other gradient manipulation methods. In particular, it achieves state-of-the-art results in terms of\n\u2206m% on the NYU-v2 and QM-9 datasets.\n Multitask Reinforcement Learning.           We further apply FAMO to multitask reinforcement learning\n (MTRL) problems as MTRL often suffers more from conflicting gradients due to the stochastic nature\n of reinforcement learning [43]. Following CAGRAD [24], we apply FAMO on the MetaWorld [44]\n MT10 benchmark, which consists of 10 robot manipulation tasks with different reward functions.\n Following [37], we use Soft Actor-Critic (SAC) [15] as the underlying RL algorithm, and compare\n against baseline methods including LS (SAC with a shared model) [44], Soft Modularization [42]\n (an MTL network that routes different modules in a shared model to form different policies), PC-\n GRAD [43], CAGRAD and NASHMTL [32]. The experimental setting and hyperparameters all match\n exactly with those in CAGRAD. For NASHMTL, we report the results of applying the NASHMTL\n update once per {1,50,100} iterations.4 The results for all methods are provided in Table 5.2.\n            NushMTL             (Su)       (1OQ)\n FAMO(?      0,OCI)         (0.(I)E        (0,0I)         Method                                Success \u2191\n      1,0                                                                                    (mean \u00b1 stderr)\n  9                                                       LS (lower bound)                      0.49 \u00b10.07\n     0.75                                                 STL (proxy for upper bound)           0.90 \u00b10.03\n                                                          PCGRAD [43]                           0.72 \u00b10.02\n                                                          SOFT MODULARIZATION [42]              0.73 \u00b10.04\n     0.25                                                 CAGRAD                                0.83 \u00b10.05\n  4                                                       NASHMTL [32] (every 1)                0.91 \u00b10.03\n                              IM                  2M      NASHMTL [32] (every 50)               0.85 \u00b10.02\n                       Training Steps                     NASHMTL [32] (every 100)              0.87 \u00b10.03\n                                                          NASHMTL (ours) (every 1)              0.80 \u00b10.13\n                    11.0                                  NASHMTL (ours) (every 50)             0.76 \u00b10.10\n 4                                                        NASHMTL (ours) (every 100)            0.80 \u00b10.12\n                             2.0    1.5     1.,0          UW [18]                               0.77 \u00b10.05\n              1.0                                         FAMO (ours)                           0.83 \u00b10.05\n                                                         Table 4: MTRL results (averaged over 10 runs) on the\n      Figure 3: Training Success Rate and Time.          Metaworld-10 benchmark.\n    4We could not reproduce the MTRL results of NASHMTL exactly, so we report both the results from the\n original paper and our reproduced results.\n                                                        9", "md": "# OCR Text\n\n## Results on CityScapes and CelebA datasets\n\n|Method|Segmentation mIoU \u2191|Pix Acc \u2191|Abs Err \u2193|Rel Err \u2193|MR \u2193|\u2206m% \u2193|MR \u2193|\u2206m% \u2193|\n|---|---|---|---|---|---|---|---|---|\n|STL|74.01|93.16|0.0125|27.77| | | | |\n|LS|70.95|91.73|0.0161|33.83|6.50|14.11|4.15|6.28|\n|SI|70.95|91.73|0.0161|33.83|9.25|14.11|7.20|7.83|\n|RLW|74.57|93.41|0.0158|47.79|9.25|24.38|1.46|5.22|\n|DWA|75.24|93.52|0.0160|44.37|6.50|21.45|3.20|6.95|\n|UW|72.02|92.85|0.0140|30.13|6.00|5.89|3.23|5.78|\n|MGDA|68.84|91.54|0.0309|33.50|9.75|44.14|14.85|10.93|\n|PCGRAD|75.13|93.48|0.0154|42.07|6.75|18.29|3.17|6.65|\n|GRADDROP|75.27|93.53|0.0157|47.54|6.00|23.73|3.29|7.80|\n|CAGRAD|75.16|93.48|0.0141|37.60|5.75|11.64|2.48|6.20|\n|IMTL-G|75.33|93.49|0.0135|38.41|4.00|11.10|0.84|4.67|\n|NASHMTL|75.41|93.66|0.0129|35.02|2.00|6.82|2.84|4.97|\n|FAMO|74.54|93.29|0.0145|32.59|6.25|8.13|1.21|4.72|\n\nTable 3: Results on CityScapes (2 tasks) and CelebA (40 tasks) datasets. Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n\nIt achieves state-of-the-art results in terms of \u2206m% on the NYU-v2 and QM-9 datasets.\n\n## Multitask Reinforcement Learning\n\nWe further apply FAMO to multitask reinforcement learning (MTRL) problems as MTRL often suffers more from conflicting gradients due to the stochastic nature of reinforcement learning. Following CAGRAD, we apply FAMO on the MetaWorld MT10 benchmark, which consists of 10 robot manipulation tasks with different reward functions. We use Soft Actor-Critic (SAC) as the underlying RL algorithm and compare against baseline methods including LS (SAC with a shared model), Soft Modularization (an MTL network that routes different modules in a shared model to form different policies), PCGRAD, CAGRAD, and NASHMTL. The experimental setting and hyperparameters all match exactly with those in CAGRAD. For NASHMTL, we report the results of applying the NASHMTL update once per {1,50,100} iterations. The results for all methods are provided in Table 5.2.\n\n|Method|Success \u2191 (mean \u00b1 stderr)|\n|---|---|\n|LS (lower bound)|0.49 \u00b10.07|\n|STL (proxy for upper bound)|0.90 \u00b10.03|\n|PCGRAD|0.72 \u00b10.02|\n|SOFT MODULARIZATION|0.73 \u00b10.04|\n|CAGRAD|0.83 \u00b10.05|\n|NASHMTL (every 1)|0.91 \u00b10.03|\n|NASHMTL (every 50)|0.85 \u00b10.02|\n|NASHMTL (every 100)|0.87 \u00b10.03|\n|NASHMTL (ours) (every 1)|0.80 \u00b10.13|\n|NASHMTL (ours) (every 50)|0.76 \u00b10.10|\n|NASHMTL (ours) (every 100)|0.80 \u00b10.12|\n|UW|0.77 \u00b10.05|\n|FAMO (ours)|0.83 \u00b10.05|\n\nTable 4: MTRL results (averaged over 10 runs) on the Metaworld-10 benchmark.\n\nFigure 3: Training Success Rate and Time.\n\nWe could not reproduce the MTRL results of NASHMTL exactly, so we report both the results from the original paper and our reproduced results.", "images": [{"name": "page-9-0.jpg", "height": 197, "width": 195, "x": 108, "y": 467}], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "heading", "lvl": 2, "value": "Results on CityScapes and CelebA datasets", "md": "## Results on CityScapes and CelebA datasets"}, {"type": "table", "rows": [["Method", "Segmentation mIoU \u2191", "Pix Acc \u2191", "Abs Err \u2193", "Rel Err \u2193", "MR \u2193", "\u2206m% \u2193", "MR \u2193", "\u2206m% \u2193"], ["STL", "74.01", "93.16", "0.0125", "27.77", "", "", "", ""], ["LS", "70.95", "91.73", "0.0161", "33.83", "6.50", "14.11", "4.15", "6.28"], ["SI", "70.95", "91.73", "0.0161", "33.83", "9.25", "14.11", "7.20", "7.83"], ["RLW", "74.57", "93.41", "0.0158", "47.79", "9.25", "24.38", "1.46", "5.22"], ["DWA", "75.24", "93.52", "0.0160", "44.37", "6.50", "21.45", "3.20", "6.95"], ["UW", "72.02", "92.85", "0.0140", "30.13", "6.00", "5.89", "3.23", "5.78"], ["MGDA", "68.84", "91.54", "0.0309", "33.50", "9.75", "44.14", "14.85", "10.93"], ["PCGRAD", "75.13", "93.48", "0.0154", "42.07", "6.75", "18.29", "3.17", "6.65"], ["GRADDROP", "75.27", "93.53", "0.0157", "47.54", "6.00", "23.73", "3.29", "7.80"], ["CAGRAD", "75.16", "93.48", "0.0141", "37.60", "5.75", "11.64", "2.48", "6.20"], ["IMTL-G", "75.33", "93.49", "0.0135", "38.41", "4.00", "11.10", "0.84", "4.67"], ["NASHMTL", "75.41", "93.66", "0.0129", "35.02", "2.00", "6.82", "2.84", "4.97"], ["FAMO", "74.54", "93.29", "0.0145", "32.59", "6.25", "8.13", "1.21", "4.72"]], "md": "|Method|Segmentation mIoU \u2191|Pix Acc \u2191|Abs Err \u2193|Rel Err \u2193|MR \u2193|\u2206m% \u2193|MR \u2193|\u2206m% \u2193|\n|---|---|---|---|---|---|---|---|---|\n|STL|74.01|93.16|0.0125|27.77| | | | |\n|LS|70.95|91.73|0.0161|33.83|6.50|14.11|4.15|6.28|\n|SI|70.95|91.73|0.0161|33.83|9.25|14.11|7.20|7.83|\n|RLW|74.57|93.41|0.0158|47.79|9.25|24.38|1.46|5.22|\n|DWA|75.24|93.52|0.0160|44.37|6.50|21.45|3.20|6.95|\n|UW|72.02|92.85|0.0140|30.13|6.00|5.89|3.23|5.78|\n|MGDA|68.84|91.54|0.0309|33.50|9.75|44.14|14.85|10.93|\n|PCGRAD|75.13|93.48|0.0154|42.07|6.75|18.29|3.17|6.65|\n|GRADDROP|75.27|93.53|0.0157|47.54|6.00|23.73|3.29|7.80|\n|CAGRAD|75.16|93.48|0.0141|37.60|5.75|11.64|2.48|6.20|\n|IMTL-G|75.33|93.49|0.0135|38.41|4.00|11.10|0.84|4.67|\n|NASHMTL|75.41|93.66|0.0129|35.02|2.00|6.82|2.84|4.97|\n|FAMO|74.54|93.29|0.0145|32.59|6.25|8.13|1.21|4.72|", "isPerfectTable": true, "csv": "\"Method\",\"Segmentation mIoU \u2191\",\"Pix Acc \u2191\",\"Abs Err \u2193\",\"Rel Err \u2193\",\"MR \u2193\",\"\u2206m% \u2193\",\"MR \u2193\",\"\u2206m% \u2193\"\n\"STL\",\"74.01\",\"93.16\",\"0.0125\",\"27.77\",\"\",\"\",\"\",\"\"\n\"LS\",\"70.95\",\"91.73\",\"0.0161\",\"33.83\",\"6.50\",\"14.11\",\"4.15\",\"6.28\"\n\"SI\",\"70.95\",\"91.73\",\"0.0161\",\"33.83\",\"9.25\",\"14.11\",\"7.20\",\"7.83\"\n\"RLW\",\"74.57\",\"93.41\",\"0.0158\",\"47.79\",\"9.25\",\"24.38\",\"1.46\",\"5.22\"\n\"DWA\",\"75.24\",\"93.52\",\"0.0160\",\"44.37\",\"6.50\",\"21.45\",\"3.20\",\"6.95\"\n\"UW\",\"72.02\",\"92.85\",\"0.0140\",\"30.13\",\"6.00\",\"5.89\",\"3.23\",\"5.78\"\n\"MGDA\",\"68.84\",\"91.54\",\"0.0309\",\"33.50\",\"9.75\",\"44.14\",\"14.85\",\"10.93\"\n\"PCGRAD\",\"75.13\",\"93.48\",\"0.0154\",\"42.07\",\"6.75\",\"18.29\",\"3.17\",\"6.65\"\n\"GRADDROP\",\"75.27\",\"93.53\",\"0.0157\",\"47.54\",\"6.00\",\"23.73\",\"3.29\",\"7.80\"\n\"CAGRAD\",\"75.16\",\"93.48\",\"0.0141\",\"37.60\",\"5.75\",\"11.64\",\"2.48\",\"6.20\"\n\"IMTL-G\",\"75.33\",\"93.49\",\"0.0135\",\"38.41\",\"4.00\",\"11.10\",\"0.84\",\"4.67\"\n\"NASHMTL\",\"75.41\",\"93.66\",\"0.0129\",\"35.02\",\"2.00\",\"6.82\",\"2.84\",\"4.97\"\n\"FAMO\",\"74.54\",\"93.29\",\"0.0145\",\"32.59\",\"6.25\",\"8.13\",\"1.21\",\"4.72\""}, {"type": "text", "value": "Table 3: Results on CityScapes (2 tasks) and CelebA (40 tasks) datasets. Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n\nIt achieves state-of-the-art results in terms of \u2206m% on the NYU-v2 and QM-9 datasets.", "md": "Table 3: Results on CityScapes (2 tasks) and CelebA (40 tasks) datasets. Each experiment is repeated over 3 random seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n\nIt achieves state-of-the-art results in terms of \u2206m% on the NYU-v2 and QM-9 datasets."}, {"type": "heading", "lvl": 2, "value": "Multitask Reinforcement Learning", "md": "## Multitask Reinforcement Learning"}, {"type": "text", "value": "We further apply FAMO to multitask reinforcement learning (MTRL) problems as MTRL often suffers more from conflicting gradients due to the stochastic nature of reinforcement learning. Following CAGRAD, we apply FAMO on the MetaWorld MT10 benchmark, which consists of 10 robot manipulation tasks with different reward functions. We use Soft Actor-Critic (SAC) as the underlying RL algorithm and compare against baseline methods including LS (SAC with a shared model), Soft Modularization (an MTL network that routes different modules in a shared model to form different policies), PCGRAD, CAGRAD, and NASHMTL. The experimental setting and hyperparameters all match exactly with those in CAGRAD. For NASHMTL, we report the results of applying the NASHMTL update once per {1,50,100} iterations. The results for all methods are provided in Table 5.2.", "md": "We further apply FAMO to multitask reinforcement learning (MTRL) problems as MTRL often suffers more from conflicting gradients due to the stochastic nature of reinforcement learning. Following CAGRAD, we apply FAMO on the MetaWorld MT10 benchmark, which consists of 10 robot manipulation tasks with different reward functions. We use Soft Actor-Critic (SAC) as the underlying RL algorithm and compare against baseline methods including LS (SAC with a shared model), Soft Modularization (an MTL network that routes different modules in a shared model to form different policies), PCGRAD, CAGRAD, and NASHMTL. The experimental setting and hyperparameters all match exactly with those in CAGRAD. For NASHMTL, we report the results of applying the NASHMTL update once per {1,50,100} iterations. The results for all methods are provided in Table 5.2."}, {"type": "table", "rows": [["Method", "Success \u2191 (mean \u00b1 stderr)"], ["LS (lower bound)", "0.49 \u00b10.07"], ["STL (proxy for upper bound)", "0.90 \u00b10.03"], ["PCGRAD", "0.72 \u00b10.02"], ["SOFT MODULARIZATION", "0.73 \u00b10.04"], ["CAGRAD", "0.83 \u00b10.05"], ["NASHMTL (every 1)", "0.91 \u00b10.03"], ["NASHMTL (every 50)", "0.85 \u00b10.02"], ["NASHMTL (every 100)", "0.87 \u00b10.03"], ["NASHMTL (ours) (every 1)", "0.80 \u00b10.13"], ["NASHMTL (ours) (every 50)", "0.76 \u00b10.10"], ["NASHMTL (ours) (every 100)", "0.80 \u00b10.12"], ["UW", "0.77 \u00b10.05"], ["FAMO (ours)", "0.83 \u00b10.05"]], "md": "|Method|Success \u2191 (mean \u00b1 stderr)|\n|---|---|\n|LS (lower bound)|0.49 \u00b10.07|\n|STL (proxy for upper bound)|0.90 \u00b10.03|\n|PCGRAD|0.72 \u00b10.02|\n|SOFT MODULARIZATION|0.73 \u00b10.04|\n|CAGRAD|0.83 \u00b10.05|\n|NASHMTL (every 1)|0.91 \u00b10.03|\n|NASHMTL (every 50)|0.85 \u00b10.02|\n|NASHMTL (every 100)|0.87 \u00b10.03|\n|NASHMTL (ours) (every 1)|0.80 \u00b10.13|\n|NASHMTL (ours) (every 50)|0.76 \u00b10.10|\n|NASHMTL (ours) (every 100)|0.80 \u00b10.12|\n|UW|0.77 \u00b10.05|\n|FAMO (ours)|0.83 \u00b10.05|", "isPerfectTable": true, "csv": "\"Method\",\"Success \u2191 (mean \u00b1 stderr)\"\n\"LS (lower bound)\",\"0.49 \u00b10.07\"\n\"STL (proxy for upper bound)\",\"0.90 \u00b10.03\"\n\"PCGRAD\",\"0.72 \u00b10.02\"\n\"SOFT MODULARIZATION\",\"0.73 \u00b10.04\"\n\"CAGRAD\",\"0.83 \u00b10.05\"\n\"NASHMTL (every 1)\",\"0.91 \u00b10.03\"\n\"NASHMTL (every 50)\",\"0.85 \u00b10.02\"\n\"NASHMTL (every 100)\",\"0.87 \u00b10.03\"\n\"NASHMTL (ours) (every 1)\",\"0.80 \u00b10.13\"\n\"NASHMTL (ours) (every 50)\",\"0.76 \u00b10.10\"\n\"NASHMTL (ours) (every 100)\",\"0.80 \u00b10.12\"\n\"UW\",\"0.77 \u00b10.05\"\n\"FAMO (ours)\",\"0.83 \u00b10.05\""}, {"type": "text", "value": "Table 4: MTRL results (averaged over 10 runs) on the Metaworld-10 benchmark.\n\nFigure 3: Training Success Rate and Time.\n\nWe could not reproduce the MTRL results of NASHMTL exactly, so we report both the results from the original paper and our reproduced results.", "md": "Table 4: MTRL results (averaged over 10 runs) on the Metaworld-10 benchmark.\n\nFigure 3: Training Success Rate and Time.\n\nWe could not reproduce the MTRL results of NASHMTL exactly, so we report both the results from the original paper and our reproduced results."}]}, {"page": 10, "text": "Findings: From Table 5.2, we observe that FAMO performs comparably to CAGRAD and out-\nperforms PCGRAD and the average gradient descent baselines by a large margin. FAMO also\noutperforms NASHMTL based on our implementation. Moreover, FAMO is significantly faster than\nNASHMTL, even when it is applied once every 100 steps.\n5.3   MTL Efficiency (Training Time Comparison)\nFigure 4 provides the FAMO\u2019s average training time per epoch against that of the baseline methods.\n                    MGDA           PCGrid           CAGrd           LMTL G           VishMTL           FAMO\n   CltyScapes (2 tasks)          NYU-VZ (3 tasks)             QM-9 (9 tasks)             CelebA (40 tasks)\nFigure 4: Average training time per epoch for different MTL optimization methods. We report the relative\ntraining time of a method to that of the linear scalarization (LS) method (which uses the average gradient).\nFindings: From the figure, we observe that FAMO introduces negligible overhead across all\nbenchmark datasets compared to the LS method, which is, in theory, the lower bound for computation\ntime. In contrast, methods like NASHMTL have much longer training time compared to FAMO.\nMore importantly, the computation cost of these methods scales with the number of tasks. In addition,\nnote that these methods also take at least O(K) space to store the task gradients, which is implausible\nfor large models in the many-task setting (i.e., when m = \u2223\u03b8\u2223       and K are large).\n5.4   Ablation on \u03b3\nIn this section, we provide the ablation study on the regularization coefficient \u03b3 in Figure 5.\n                            ULI\n         CityScapes                   NYU-v2                        OM-9                        CelebA\nFigure 5: Ablation over \u03b3: we plot the performance of FAMO (in terms of \u2206m% using different values of \u03b3\nfrom {0.0001, 0.001, 0.01} on the four supervised MTL benchmarks.\nFindings: From Figure 5, we can observe that choosing the right regularization coeffi             cient can be\ncrucial. But except for CityScapes, FAMO performs reasonably well using all different \u03b3s. The\nproblem with CityScapes is that one of the task losses is close to 0 at the very beginning, hence small\nchanges in task weighting can result in very different loss improvement. Therefore we conjecture that\nusing a larger \u03b3, in this case, can help stabilize MTL.\n6    Conclusion and Limitations\nIn this work, we introduce FAMO, a fast optimization method for multitask learning (MTL) that\nmitigates the conflicting gradients using O(1) space and time. As multitasking large models gain\nmore attention, we believe designing efficient but effective optimizers like FAMO for MTL is\ncrucial. FAMO balances task losses by ensuring each task\u2019s loss decreases approximately at an\nequal rate. Empirically, we observe that FAMO can achieve competitive performance against the\nstate-of-the-art MTL gradient manipulation methods. One limitation of FAMO is its dependency on\nthe regularization parameter \u03b3, which is introduced due to the stochastic update of the task weighting\nlogits w. Future work can investigate a more principled way of determining \u03b3.\n                                                       10", "md": "# MTL Efficiency and Ablation Study\n\n## MTL Efficiency (Training Time Comparison)\n\nFigure 4 provides the FAMO\u2019s average training time per epoch against that of the baseline methods.\n\n| |MGDA|PCGrid|CAGrd|LMTL G|VishMTL|FAMO|\n|---|---|---|---|---|---|---|\n|CityScapes (2 tasks)| |NYU-VZ (3 tasks)| |QM-9 (9 tasks)|CelebA (40 tasks)| |\n\nFindings: From the figure, we observe that FAMO introduces negligible overhead across all benchmark datasets compared to the LS method, which is, in theory, the lower bound for computation time. In contrast, methods like NASHMTL have much longer training time compared to FAMO. More importantly, the computation cost of these methods scales with the number of tasks. In addition, note that these methods also take at least O(K) space to store the task gradients, which is implausible for large models in the many-task setting (i.e., when m = $$\\mid\\theta\\mid$$ and K are large).\n\n## Ablation on $$\\gamma$$\n\nIn this section, we provide the ablation study on the regularization coefficient $$\\gamma$$ in Figure 5.\n\n| |CityScapes|NYU-v2|OM-9|CelebA|\n|---|---|---|---|---|\n| |ULI| | | |\n\nFindings: From Figure 5, we can observe that choosing the right regularization coefficient can be crucial. But except for CityScapes, FAMO performs reasonably well using all different $$\\gamma$$s. The problem with CityScapes is that one of the task losses is close to 0 at the very beginning, hence small changes in task weighting can result in very different loss improvement. Therefore we conjecture that using a larger $$\\gamma$$, in this case, can help stabilize MTL.\n\n## Conclusion and Limitations\n\nIn this work, we introduce FAMO, a fast optimization method for multitask learning (MTL) that mitigates the conflicting gradients using O(1) space and time. As multitasking large models gain more attention, we believe designing efficient but effective optimizers like FAMO for MTL is crucial. FAMO balances task losses by ensuring each task\u2019s loss decreases approximately at an equal rate. Empirically, we observe that FAMO can achieve competitive performance against the state-of-the-art MTL gradient manipulation methods. One limitation of FAMO is its dependency on the regularization parameter $$\\gamma$$, which is introduced due to the stochastic update of the task weighting logits w. Future work can investigate a more principled way of determining $$\\gamma$$.", "images": [{"name": "page-10-0.jpg", "height": 112, "width": 396, "x": 108, "y": 171}, {"name": "page-10-1.jpg", "height": 72, "width": 397, "x": 108, "y": 435}], "items": [{"type": "heading", "lvl": 1, "value": "MTL Efficiency and Ablation Study", "md": "# MTL Efficiency and Ablation Study"}, {"type": "heading", "lvl": 2, "value": "MTL Efficiency (Training Time Comparison)", "md": "## MTL Efficiency (Training Time Comparison)"}, {"type": "text", "value": "Figure 4 provides the FAMO\u2019s average training time per epoch against that of the baseline methods.", "md": "Figure 4 provides the FAMO\u2019s average training time per epoch against that of the baseline methods."}, {"type": "table", "rows": [["", "MGDA", "PCGrid", "CAGrd", "LMTL G", "VishMTL", "FAMO"], ["CityScapes (2 tasks)", "", "NYU-VZ (3 tasks)", "", "QM-9 (9 tasks)", "CelebA (40 tasks)", ""]], "md": "| |MGDA|PCGrid|CAGrd|LMTL G|VishMTL|FAMO|\n|---|---|---|---|---|---|---|\n|CityScapes (2 tasks)| |NYU-VZ (3 tasks)| |QM-9 (9 tasks)|CelebA (40 tasks)| |", "isPerfectTable": true, "csv": "\"\",\"MGDA\",\"PCGrid\",\"CAGrd\",\"LMTL G\",\"VishMTL\",\"FAMO\"\n\"CityScapes (2 tasks)\",\"\",\"NYU-VZ (3 tasks)\",\"\",\"QM-9 (9 tasks)\",\"CelebA (40 tasks)\",\"\""}, {"type": "text", "value": "Findings: From the figure, we observe that FAMO introduces negligible overhead across all benchmark datasets compared to the LS method, which is, in theory, the lower bound for computation time. In contrast, methods like NASHMTL have much longer training time compared to FAMO. More importantly, the computation cost of these methods scales with the number of tasks. In addition, note that these methods also take at least O(K) space to store the task gradients, which is implausible for large models in the many-task setting (i.e., when m = $$\\mid\\theta\\mid$$ and K are large).", "md": "Findings: From the figure, we observe that FAMO introduces negligible overhead across all benchmark datasets compared to the LS method, which is, in theory, the lower bound for computation time. In contrast, methods like NASHMTL have much longer training time compared to FAMO. More importantly, the computation cost of these methods scales with the number of tasks. In addition, note that these methods also take at least O(K) space to store the task gradients, which is implausible for large models in the many-task setting (i.e., when m = $$\\mid\\theta\\mid$$ and K are large)."}, {"type": "heading", "lvl": 2, "value": "Ablation on $$\\gamma$$", "md": "## Ablation on $$\\gamma$$"}, {"type": "text", "value": "In this section, we provide the ablation study on the regularization coefficient $$\\gamma$$ in Figure 5.", "md": "In this section, we provide the ablation study on the regularization coefficient $$\\gamma$$ in Figure 5."}, {"type": "table", "rows": [["", "CityScapes", "NYU-v2", "OM-9", "CelebA"], ["", "ULI", "", "", ""]], "md": "| |CityScapes|NYU-v2|OM-9|CelebA|\n|---|---|---|---|---|\n| |ULI| | | |", "isPerfectTable": true, "csv": "\"\",\"CityScapes\",\"NYU-v2\",\"OM-9\",\"CelebA\"\n\"\",\"ULI\",\"\",\"\",\"\""}, {"type": "text", "value": "Findings: From Figure 5, we can observe that choosing the right regularization coefficient can be crucial. But except for CityScapes, FAMO performs reasonably well using all different $$\\gamma$$s. The problem with CityScapes is that one of the task losses is close to 0 at the very beginning, hence small changes in task weighting can result in very different loss improvement. Therefore we conjecture that using a larger $$\\gamma$$, in this case, can help stabilize MTL.", "md": "Findings: From Figure 5, we can observe that choosing the right regularization coefficient can be crucial. But except for CityScapes, FAMO performs reasonably well using all different $$\\gamma$$s. The problem with CityScapes is that one of the task losses is close to 0 at the very beginning, hence small changes in task weighting can result in very different loss improvement. Therefore we conjecture that using a larger $$\\gamma$$, in this case, can help stabilize MTL."}, {"type": "heading", "lvl": 2, "value": "Conclusion and Limitations", "md": "## Conclusion and Limitations"}, {"type": "text", "value": "In this work, we introduce FAMO, a fast optimization method for multitask learning (MTL) that mitigates the conflicting gradients using O(1) space and time. As multitasking large models gain more attention, we believe designing efficient but effective optimizers like FAMO for MTL is crucial. FAMO balances task losses by ensuring each task\u2019s loss decreases approximately at an equal rate. Empirically, we observe that FAMO can achieve competitive performance against the state-of-the-art MTL gradient manipulation methods. One limitation of FAMO is its dependency on the regularization parameter $$\\gamma$$, which is introduced due to the stochastic update of the task weighting logits w. Future work can investigate a more principled way of determining $$\\gamma$$.", "md": "In this work, we introduce FAMO, a fast optimization method for multitask learning (MTL) that mitigates the conflicting gradients using O(1) space and time. As multitasking large models gain more attention, we believe designing efficient but effective optimizers like FAMO for MTL is crucial. FAMO balances task losses by ensuring each task\u2019s loss decreases approximately at an equal rate. Empirically, we observe that FAMO can achieve competitive performance against the state-of-the-art MTL gradient manipulation methods. One limitation of FAMO is its dependency on the regularization parameter $$\\gamma$$, which is introduced due to the stochastic update of the task weighting logits w. Future work can investigate a more principled way of determining $$\\gamma$$."}]}, {"page": 11, "text": "References\n [1] L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in\n     the chemical universe database GDB-13. J. Am. Chem. Soc., 131:8732, 2009.\n [2] Felix JS Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel C Alexander, and Jorge Cardoso.\n     Stochastic filter groups for multi-task cnns: Learning specialist and generalist convolution\n     kernels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n     1385\u20131394, 2019.\n [3] David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. Automated\n     search for resource-efficient branched multi-task networks. arXiv preprint arXiv:2008.10292,\n     2020.\n [4] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\n     Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\n     intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n [5] Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\n [6] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gra-\n     dient normalization for adaptive loss balancing in deep multitask networks. In International\n     Conference on Machine Learning, pages 794\u2013803. PMLR, 2018.\n [7] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai,\n     and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign\n     dropout. arXiv preprint arXiv:2010.06808, 2020.\n [8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\n     Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\n     urban scene understanding. In Proceedings of the IEEE conference on computer vision and\n     pattern recognition, pages 3213\u20133223, 2016.\n [9] Jean-Antoine D\u00e9sid\u00e9ri. Multiple-gradient descent algorithm (mgda) for multiobjective opti-\n     mization. Comptes Rendus Mathematique, 350(5-6):313\u2013318, 2012.\n[10] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.\n     arXiv preprint arXiv:1903.02428, 2019.\n[11] Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Effi       ciently\n     identifying task groupings for multi-task learning. Advances in Neural Information Processing\n     Systems, 34:27503\u201327516, 2021.\n[12] Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and Wei Liu. Mtl-nas: Task-agnostic\n     neural architecture search towards general-purpose multi-task learning. In Proceedings of the\n     IEEE/CVF Conference on computer vision and pattern recognition, pages 11543\u201311552, 2020.\n[13] Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task\n     prioritization for multitask learning. In Proceedings of the European conference on computer\n     vision (ECCV), pages 270\u2013287, 2018.\n[14] Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning.\n     In International Conference on Machine Learning, pages 3854\u20133863. PMLR, 2020.\n[15] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-\n     policy maximum entropy deep reinforcement learning with a stochastic actor. In International\n     Conference on Machine Learning, pages 1861\u20131870. PMLR, 2018.\n[16] Adri\u00e1n Javaloy and Isabel Valera. Rotograd: Dynamic gradient homogenization for multi-task\n     learning. arXiv preprint arXiv:2103.02631, 2021.\n[17] Alexandr Katrutsa, Daniil Merkulov, Nurislam Tursynbek, and Ivan Oseledets. Follow the\n     bisector: a simple method for multi-objective optimization. arXiv preprint arXiv:2007.06937,\n     2020.\n                                                11", "md": "# References\n\n## References\n\n1. L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in\nthe chemical universe database GDB-13. J. Am. Chem. Soc., 131:8732, 2009.\n2. Felix JS Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel C Alexander, and Jorge Cardoso.\nStochastic filter groups for multi-task cnns: Learning specialist and generalist convolution\nkernels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n1385\u20131394, 2019.\n3. David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. Automated\nsearch for resource-efficient branched multi-task networks. arXiv preprint arXiv:2008.10292,\n2020.\n4. S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n5. Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\n6. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning, pages 794\u2013803. PMLR, 2018.\n7. Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai,\nand Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign\ndropout. arXiv preprint arXiv:2010.06808, 2020.\n8. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3213\u20133223, 2016.\n9. Jean-Antoine D\u00e9sid\u00e9ri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Comptes Rendus Mathematique, 350(5-6):313\u2013318, 2012.\n10. Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.\n11. Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings for multi-task learning. Advances in Neural Information Processing Systems, 34:27503\u201327516, 2021.\n12. Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and Wei Liu. Mtl-nas: Task-agnostic neural architecture search towards general-purpose multi-task learning. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 11543\u201311552, 2020.\n13. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioritization for multitask learning. In Proceedings of the European conference on computer vision (ECCV), pages 270\u2013287, 2018.\n14. Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In International Conference on Machine Learning, pages 3854\u20133863. PMLR, 2020.\n15. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861\u20131870. PMLR, 2018.\n16. Adri\u00e1n Javaloy and Isabel Valera. Rotograd: Dynamic gradient homogenization for multi-task learning. arXiv preprint arXiv:2103.02631, 2021.\n17. Alexandr Katrutsa, Daniil Merkulov, Nurislam Tursynbek, and Ivan Oseledets. Follow the bisector: a simple method for multi-objective optimization. arXiv preprint arXiv:2007.06937, 2020.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in\nthe chemical universe database GDB-13. J. Am. Chem. Soc., 131:8732, 2009.\n2. Felix JS Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel C Alexander, and Jorge Cardoso.\nStochastic filter groups for multi-task cnns: Learning specialist and generalist convolution\nkernels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n1385\u20131394, 2019.\n3. David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. Automated\nsearch for resource-efficient branched multi-task networks. arXiv preprint arXiv:2008.10292,\n2020.\n4. S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n5. Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\n6. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning, pages 794\u2013803. PMLR, 2018.\n7. Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai,\nand Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign\ndropout. arXiv preprint arXiv:2010.06808, 2020.\n8. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3213\u20133223, 2016.\n9. Jean-Antoine D\u00e9sid\u00e9ri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Comptes Rendus Mathematique, 350(5-6):313\u2013318, 2012.\n10. Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.\n11. Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings for multi-task learning. Advances in Neural Information Processing Systems, 34:27503\u201327516, 2021.\n12. Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and Wei Liu. Mtl-nas: Task-agnostic neural architecture search towards general-purpose multi-task learning. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 11543\u201311552, 2020.\n13. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioritization for multitask learning. In Proceedings of the European conference on computer vision (ECCV), pages 270\u2013287, 2018.\n14. Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In International Conference on Machine Learning, pages 3854\u20133863. PMLR, 2020.\n15. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861\u20131870. PMLR, 2018.\n16. Adri\u00e1n Javaloy and Isabel Valera. Rotograd: Dynamic gradient homogenization for multi-task learning. arXiv preprint arXiv:2103.02631, 2021.\n17. Alexandr Katrutsa, Daniil Merkulov, Nurislam Tursynbek, and Ivan Oseledets. Follow the bisector: a simple method for multi-objective optimization. arXiv preprint arXiv:2007.06937, 2020.", "md": "1. L. C. Blum and J.-L. Reymond. 970 million druglike small molecules for virtual screening in\nthe chemical universe database GDB-13. J. Am. Chem. Soc., 131:8732, 2009.\n2. Felix JS Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel C Alexander, and Jorge Cardoso.\nStochastic filter groups for multi-task cnns: Learning specialist and generalist convolution\nkernels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n1385\u20131394, 2019.\n3. David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. Automated\nsearch for resource-efficient branched multi-task networks. arXiv preprint arXiv:2008.10292,\n2020.\n4. S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n5. Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\n6. Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning, pages 794\u2013803. PMLR, 2018.\n7. Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai,\nand Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign\ndropout. arXiv preprint arXiv:2010.06808, 2020.\n8. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3213\u20133223, 2016.\n9. Jean-Antoine D\u00e9sid\u00e9ri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Comptes Rendus Mathematique, 350(5-6):313\u2013318, 2012.\n10. Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428, 2019.\n11. Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Efficiently identifying task groupings for multi-task learning. Advances in Neural Information Processing Systems, 34:27503\u201327516, 2021.\n12. Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and Wei Liu. Mtl-nas: Task-agnostic neural architecture search towards general-purpose multi-task learning. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 11543\u201311552, 2020.\n13. Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li Fei-Fei. Dynamic task prioritization for multitask learning. In Proceedings of the European conference on computer vision (ECCV), pages 270\u2013287, 2018.\n14. Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In International Conference on Machine Learning, pages 3854\u20133863. PMLR, 2020.\n15. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861\u20131870. PMLR, 2018.\n16. Adri\u00e1n Javaloy and Isabel Valera. Rotograd: Dynamic gradient homogenization for multi-task learning. arXiv preprint arXiv:2103.02631, 2021.\n17. Alexandr Katrutsa, Daniil Merkulov, Nurislam Tursynbek, and Ivan Oseledets. Follow the bisector: a simple method for multi-objective optimization. arXiv preprint arXiv:2007.06937, 2020."}]}, {"page": 12, "text": "[18] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh\n     losses for scene geometry and semantics. In Proceedings of the IEEE conference on computer\n     vision and pattern recognition, pages 7482\u20137491, 2018.\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n     arXiv:1412.6980, 2014.\n[20] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\n     Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\n     preprint arXiv:2304.02643, 2023.\n[21] Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-,\n     and high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE\n     conference on computer vision and pattern recognition, pages 6129\u20136138, 2017.\n[22] Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and Pawan K Mudigonda.\n     In defense of the unitary scalarization for deep multi-task learning.     Advances in Neural\n     Information Processing Systems, 35:12169\u201312183, 2022.\n[23] Baijiong Lin, Feiyang Ye, and Yu Zhang. A closer look at loss weighting in multi-task learning.\n     arXiv preprint arXiv:2111.10603, 2021.\n[24] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent\n     for multi-task learning. Advances in Neural Information Processing Systems, 34:18878\u201318890,\n     2021.\n[25] Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin\n     Liao, and Wayne Zhang. Towards impartial multi-task learning. In International Conference on\n     Learning Representations, 2020.\n[26] Shikun Liu, Stephen James, Andrew J Davison, and Edward Johns. Auto-lambda: Disentangling\n     dynamic task relationships. arXiv preprint arXiv:2202.03091, 2022.\n[27] Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention.\n     In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n     pages 1871\u20131880, 2019.\n[28] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\n     wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.\n[29] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S Yu. Learning multiple tasks\n     with multilinear relationship networks. Advances in neural information processing systems, 30,\n     2017.\n[30] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks\n     for multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern\n     recognition, pages 3994\u20134003, 2016.\n[31] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and\n     support inference from rgbd images. In ECCV, 2012.\n[32] Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik,\n     and Ethan Fetaya. Multi-task learning as a bargaining game. arXiv preprint arXiv:2202.01017,\n     2022.\n[33] Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A Zuluaga. Improved\n     optimization strategies for deep multi-task networks. arXiv preprint arXiv:2109.11678, 2021.\n[34] Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S\u00f8gaard. Latent multi-\n     task architecture learning. In Proceedings of the AAAI Conference on Artificial Intelligence,\n     volume 33, pages 4822\u20134829, 2019.\n[35] Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. arXiv\n     preprint arXiv:1810.04650, 2018.\n                                                 12", "md": "# Research Papers\n\n# List of Research Papers\n\n1. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh\nlosses for scene geometry and semantics. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7482\u20137491, 2018.\n2. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n3. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n4. Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-,\nand high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 6129\u20136138, 2017.\n5. Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and Pawan K Mudigonda.\nIn defense of the unitary scalarization for deep multi-task learning. Advances in Neural\nInformation Processing Systems, 35:12169\u201312183, 2022.\n6. Baijiong Lin, Feiyang Ye, and Yu Zhang. A closer look at loss weighting in multi-task learning.\narXiv preprint arXiv:2111.10603, 2021.\n7. Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent\nfor multi-task learning. Advances in Neural Information Processing Systems, 34:18878\u201318890, 2021.\n8. Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin\nLiao, and Wayne Zhang. Towards impartial multi-task learning. In International Conference on\nLearning Representations, 2020.\n9. Shikun Liu, Stephen James, Andrew J Davison, and Edward Johns. Auto-lambda: Disentangling\ndynamic task relationships. arXiv preprint arXiv:2202.03091, 2022.\n10. Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 1871\u20131880, 2019.\n11. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\nwild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.\n12. Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S Yu. Learning multiple tasks\nwith multilinear relationship networks. Advances in neural information processing systems, 30,\n2017.\n13. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks\nfor multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3994\u20134003, 2016.\n14. Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and\nsupport inference from rgbd images. In ECCV, 2012.\n15. Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik,\nand Ethan Fetaya. Multi-task learning as a bargaining game. arXiv preprint arXiv:2202.01017,\n2022.\n16. Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A Zuluaga. Improved\noptimization strategies for deep multi-task networks. arXiv preprint arXiv:2109.11678, 2021.\n17. Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S\u00f8gaard. Latent multi-\ntask architecture learning. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 4822\u20134829, 2019.\n18. Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. arXiv\npreprint arXiv:1810.04650, 2018.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Papers", "md": "# Research Papers"}, {"type": "heading", "lvl": 1, "value": "List of Research Papers", "md": "# List of Research Papers"}, {"type": "text", "value": "1. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh\nlosses for scene geometry and semantics. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7482\u20137491, 2018.\n2. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n3. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n4. Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-,\nand high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 6129\u20136138, 2017.\n5. Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and Pawan K Mudigonda.\nIn defense of the unitary scalarization for deep multi-task learning. Advances in Neural\nInformation Processing Systems, 35:12169\u201312183, 2022.\n6. Baijiong Lin, Feiyang Ye, and Yu Zhang. A closer look at loss weighting in multi-task learning.\narXiv preprint arXiv:2111.10603, 2021.\n7. Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent\nfor multi-task learning. Advances in Neural Information Processing Systems, 34:18878\u201318890, 2021.\n8. Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin\nLiao, and Wayne Zhang. Towards impartial multi-task learning. In International Conference on\nLearning Representations, 2020.\n9. Shikun Liu, Stephen James, Andrew J Davison, and Edward Johns. Auto-lambda: Disentangling\ndynamic task relationships. arXiv preprint arXiv:2202.03091, 2022.\n10. Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 1871\u20131880, 2019.\n11. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\nwild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.\n12. Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S Yu. Learning multiple tasks\nwith multilinear relationship networks. Advances in neural information processing systems, 30,\n2017.\n13. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks\nfor multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3994\u20134003, 2016.\n14. Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and\nsupport inference from rgbd images. In ECCV, 2012.\n15. Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik,\nand Ethan Fetaya. Multi-task learning as a bargaining game. arXiv preprint arXiv:2202.01017,\n2022.\n16. Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A Zuluaga. Improved\noptimization strategies for deep multi-task networks. arXiv preprint arXiv:2109.11678, 2021.\n17. Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S\u00f8gaard. Latent multi-\ntask architecture learning. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 4822\u20134829, 2019.\n18. Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. arXiv\npreprint arXiv:1810.04650, 2018.", "md": "1. Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh\nlosses for scene geometry and semantics. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7482\u20137491, 2018.\n2. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n3. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n4. Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-,\nand high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 6129\u20136138, 2017.\n5. Vitaly Kurin, Alessandro De Palma, Ilya Kostrikov, Shimon Whiteson, and Pawan K Mudigonda.\nIn defense of the unitary scalarization for deep multi-task learning. Advances in Neural\nInformation Processing Systems, 35:12169\u201312183, 2022.\n6. Baijiong Lin, Feiyang Ye, and Yu Zhang. A closer look at loss weighting in multi-task learning.\narXiv preprint arXiv:2111.10603, 2021.\n7. Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent\nfor multi-task learning. Advances in Neural Information Processing Systems, 34:18878\u201318890, 2021.\n8. Liyang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin\nLiao, and Wayne Zhang. Towards impartial multi-task learning. In International Conference on\nLearning Representations, 2020.\n9. Shikun Liu, Stephen James, Andrew J Davison, and Edward Johns. Auto-lambda: Disentangling\ndynamic task relationships. arXiv preprint arXiv:2202.03091, 2022.\n10. Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 1871\u20131880, 2019.\n11. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\nwild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015.\n12. Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S Yu. Learning multiple tasks\nwith multilinear relationship networks. Advances in neural information processing systems, 30,\n2017.\n13. Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks\nfor multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 3994\u20134003, 2016.\n14. Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and\nsupport inference from rgbd images. In ECCV, 2012.\n15. Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal Chechik,\nand Ethan Fetaya. Multi-task learning as a bargaining game. arXiv preprint arXiv:2202.01017,\n2022.\n16. Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria A Zuluaga. Improved\noptimization strategies for deep multi-task networks. arXiv preprint arXiv:2109.11678, 2021.\n17. Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S\u00f8gaard. Latent multi-\ntask architecture learning. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 4822\u20134829, 2019.\n18. Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. arXiv\npreprint arXiv:1810.04650, 2018."}]}, {"page": 13, "text": "[36] Jiayi Shen, Xiantong Zhen, Marcel Worring, and Ling Shao. Variational multi-task learning with\n     gumbel-softmax priors. Advances in Neural Information Processing Systems, 34:21031\u201321042,\n     2021.\n[37] Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with\n     context-based representations. arXiv preprint arXiv:2102.06177, 2021.\n[38] Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.\n     Which tasks should be learned together in multi-task learning? In International Conference on\n     Machine Learning, pages 9120\u20139132. PMLR, 2020.\n[39] Sebastian Thrun and Joseph O\u2019Sullivan. Discovering structure in multiple learning tasks: The\n     tc algorithm. In ICML, volume 96, pages 489\u2013497, 1996.\n[40] Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin\n     Dai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. IEEE\n     Transactions on Pattern Analysis and Machine Intelligence, 2021.\n[41] Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. Do current\n     multi-task optimization methods in deep learning even help? Advances in Neural Information\n     Processing Systems, 35:13597\u201313609, 2022.\n[42] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with\n     soft modularization. arXiv preprint arXiv:2003.13661, 2020.\n[43] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\n     Gradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020.\n[44] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and\n     Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement\n     learning. In Conference on Robot Learning, pages 1094\u20131100. PMLR, 2020.\n[45] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\n     Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE\n     conference on computer vision and pattern recognition, pages 3712\u20133722, 2018.\n[46] Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu. On\n     the convergence of stochastic multi-objective gradient manipulation and beyond. Advances in\n     Neural Information Processing Systems, 35:38103\u201338115, 2022.\n[47] Shijie Zhu, Hui Zhao, Pengjie Wang, Hongbo Deng, Jian Xu, and Bo Zheng.               Gradient\n     deconfliction via orthogonal projections onto subspaces for multi-task learning.\n                                                 13", "md": "- Jiayi Shen, Xiantong Zhen, Marcel Worring, and Ling Shao. Variational multi-task learning with\ngumbel-softmax priors. Advances in Neural Information Processing Systems, 34:21031\u201321042,\n2021.\n- Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with\ncontext-based representations. arXiv preprint arXiv:2102.06177, 2021.\n- Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.\nWhich tasks should be learned together in multi-task learning? In International Conference on\nMachine Learning, pages 9120\u20139132. PMLR, 2020.\n- Sebastian Thrun and Joseph O\u2019Sullivan. Discovering structure in multiple learning tasks: The\ntc algorithm. In ICML, volume 96, pages 489\u2013497, 1996.\n- Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin\nDai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2021.\n- Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. Do current\nmulti-task optimization methods in deep learning even help? Advances in Neural Information\nProcessing Systems, 35:13597\u201313609, 2022.\n- Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with\nsoft modularization. arXiv preprint arXiv:2003.13661, 2020.\n- Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\nGradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020.\n- Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and\nSergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement\nlearning. In Conference on Robot Learning, pages 1094\u20131100. PMLR, 2020.\n- Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\nSavarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 3712\u20133722, 2018.\n- Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu. On\nthe convergence of stochastic multi-objective gradient manipulation and beyond. Advances in\nNeural Information Processing Systems, 35:38103\u201338115, 2022.\n- Shijie Zhu, Hui Zhao, Pengjie Wang, Hongbo Deng, Jian Xu, and Bo Zheng. Gradient\ndeconfliction via orthogonal projections onto subspaces for multi-task learning.", "images": [], "items": [{"type": "text", "value": "- Jiayi Shen, Xiantong Zhen, Marcel Worring, and Ling Shao. Variational multi-task learning with\ngumbel-softmax priors. Advances in Neural Information Processing Systems, 34:21031\u201321042,\n2021.\n- Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with\ncontext-based representations. arXiv preprint arXiv:2102.06177, 2021.\n- Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.\nWhich tasks should be learned together in multi-task learning? In International Conference on\nMachine Learning, pages 9120\u20139132. PMLR, 2020.\n- Sebastian Thrun and Joseph O\u2019Sullivan. Discovering structure in multiple learning tasks: The\ntc algorithm. In ICML, volume 96, pages 489\u2013497, 1996.\n- Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin\nDai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2021.\n- Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. Do current\nmulti-task optimization methods in deep learning even help? Advances in Neural Information\nProcessing Systems, 35:13597\u201313609, 2022.\n- Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with\nsoft modularization. arXiv preprint arXiv:2003.13661, 2020.\n- Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\nGradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020.\n- Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and\nSergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement\nlearning. In Conference on Robot Learning, pages 1094\u20131100. PMLR, 2020.\n- Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\nSavarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 3712\u20133722, 2018.\n- Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu. On\nthe convergence of stochastic multi-objective gradient manipulation and beyond. Advances in\nNeural Information Processing Systems, 35:38103\u201338115, 2022.\n- Shijie Zhu, Hui Zhao, Pengjie Wang, Hongbo Deng, Jian Xu, and Bo Zheng. Gradient\ndeconfliction via orthogonal projections onto subspaces for multi-task learning.", "md": "- Jiayi Shen, Xiantong Zhen, Marcel Worring, and Ling Shao. Variational multi-task learning with\ngumbel-softmax priors. Advances in Neural Information Processing Systems, 34:21031\u201321042,\n2021.\n- Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with\ncontext-based representations. arXiv preprint arXiv:2102.06177, 2021.\n- Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.\nWhich tasks should be learned together in multi-task learning? In International Conference on\nMachine Learning, pages 9120\u20139132. PMLR, 2020.\n- Sebastian Thrun and Joseph O\u2019Sullivan. Discovering structure in multiple learning tasks: The\ntc algorithm. In ICML, volume 96, pages 489\u2013497, 1996.\n- Simon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin\nDai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2021.\n- Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. Do current\nmulti-task optimization methods in deep learning even help? Advances in Neural Information\nProcessing Systems, 35:13597\u201313609, 2022.\n- Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-task reinforcement learning with\nsoft modularization. arXiv preprint arXiv:2003.13661, 2020.\n- Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn.\nGradient surgery for multi-task learning. arXiv preprint arXiv:2001.06782, 2020.\n- Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and\nSergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement\nlearning. In Conference on Robot Learning, pages 1094\u20131100. PMLR, 2020.\n- Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\nSavarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 3712\u20133722, 2018.\n- Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu Zhu. On\nthe convergence of stochastic multi-objective gradient manipulation and beyond. Advances in\nNeural Information Processing Systems, 35:38103\u201338115, 2022.\n- Shijie Zhu, Hui Zhao, Pengjie Wang, Hongbo Deng, Jian Xu, and Bo Zheng. Gradient\ndeconfliction via orthogonal projections onto subspaces for multi-task learning."}]}, {"page": 14, "text": "A      Gradient Manipulation Methods\nIn this section, we provide a brief overview of representative gradient manipulation methods in\nmultitask/multiobjective optimization. Specifically, we will also discuss the connections among these\nmethods.\nMultiple Gradient Descent Algorithm (MGDA) [9, 35]                                   The MGDA algorithm is one of the\nearliest gradient manipulation methods for multitask learning. In MGDA, the per step update dt is\nfound by solving\n                                                  max              i,td \u2212   1\n                                                  d\u2208Rm min\n                                                         i\u2208[k] \u2207\u2113\u22ba          2\u2225d\u22252.\nAs a result, the solution d\u2217             of MGDA optimizes the \u201cworst improvement\" across all tasks or\nequivalently seeks an equal descent across all task losses as much as possible. But in practice,\nMGDA suffers from slow convergence since the update d\u2217                             can be very small. For instance, if one\ntask has a very small loss scale, the progress of all other tasks will be bounded by the progress on this\ntask. Note that the original objective in (6) is similar to the MGDA objective in the sense that we\ncan view optimizing (6) as optimizing the log of the task losses. Hence, when we compare FAMO\nagainst MGDA, one can regard FAMO as balancing the rate of loss improvement while MGDA\nbalances the absolute improvement across task losses.\nProjecting Gradient Descent (PCGRAD) [43]                          PCGRAD initializes vi         PC = \u2207\u2113i,t, then for each task\ni, PCGRAD loops over all task j \u2260               i (in a random order, which is crucial as mentioned in [43]) and\nremoves the \u201cconflict\"\n                                  vi                 PC  \u22ba\u2207\u2113j,t   \u2207\u2113j,t     if    vi   \u22ba\u2207\u2113j,t < 0.\n                                    PC \u2190   viPC \u2212   vi\u2225\u2113j,t\u22252                       PC\nIn the end, PCGRAD produces dt = 1                 k \u2211k i=1 viPC. Due to the construction, PCGRAD will also help\nimprove the \u201cworst improvement\" across all tasks since the \u201cconflicts\" have been removed. However,\ndue to the stochastic iterative procedural of this algorithm, it is hard to understand PCGRAD from a\nfirst principle approach.\nConflict-averse Gradient Descent (CAGRAD) [24]                              dt is found by solving\n                                   max               i,td    s.t.   \u2225d \u2212   \u2207\u21130,t\u2225    \u2264 c\u2225\u2207\u21130,t\u2225.\n                                   d\u2208Rm min\n                                          i\u2208[k]  \u2207\u2113\u22ba\nHere, \u21130,t = 1   k \u2211k  i=1 \u2113i,t. CAGRAD seeks an update dt that optimizes the \u201cworst improvement\" as\nmuch as possible, conditioned on that the update still decreases the average loss. By controlling the\nhyperparameter c, CAGRAD can recover MGDA (c \u2192                             \u221e) and the vanilla averaged gradient descent\n(c \u2192   0). Due to the extra constraint, CAGRAD provably converges to the stationary points of \u21130 when\n0 \u2264  c < 1.\nImpartial Multi-Task Learning (IMTL-G) [25]                              IMTL-G finds dt such that it shares the same\ncosine similarity with any task gradients:\n              \u2200i \u2260   j,   d\u22bat   \u2207\u2113i,t         t  \u2207\u2113j,t         and dt =      \u2211k  wi,t\u2207\u2113i,t, for some wt \u2208            Sk.\n                              \u2225\u2207\u2113i,t\u2225    = d\u22ba   \u2225\u2207\u2113j,t\u2225,                     i=1\nThe constraint that dt = \u2211k         i=1 wi,t\u2207\u2113i,t is for preventing the problem from being under-determined.\nFrom the above equation, we can see that IMTL-G ignores the \u201csize\" of each task gradient and only\ncares about the \u201cdirection\". As a result, one can think of IMTL-G as a variant of MGDA that\napplies to the normalized gradients. By doing so, IMTL-G does not suffer from the straggler effect\ndue to slow objectives. Furthermore, one can view IMTL-G as the equal angle descent, which is\nalso proposed in Katrutsa et al. [17], where the objective is to find d such that\n                                       \u2200i \u2260   j,        cos(d,   \u2207\u2113i,t) = cos(d,       \u2207\u2113j,t).\n                                                                  14", "md": "# Gradient Manipulation Methods\n\n## Gradient Manipulation Methods\n\nIn this section, we provide a brief overview of representative gradient manipulation methods in multitask/multiobjective optimization. Specifically, we will also discuss the connections among these methods.\n\n### Multiple Gradient Descent Algorithm (MGDA) [9, 35]\n\nThe MGDA algorithm is one of the earliest gradient manipulation methods for multitask learning. In MGDA, the per step update \\(d_t\\) is found by solving\n\n$$\n\\max_{d \\in \\mathbb{R}^m} \\min_{i \\in [k]} \\nabla \\ell_i^T d - \\frac{1}{2} \\|d\\|^2.\n$$\n\nAs a result, the solution \\(d^*\\) of MGDA optimizes the \"worst improvement\" across all tasks or equivalently seeks an equal descent across all task losses as much as possible. But in practice, MGDA suffers from slow convergence since the update \\(d^*\\) can be very small. For instance, if one task has a very small loss scale, the progress of all other tasks will be bounded by the progress on this task. Note that the original objective in (6) is similar to the MGDA objective in the sense that we can view optimizing (6) as optimizing the log of the task losses. Hence, when we compare FAMO against MGDA, one can regard FAMO as balancing the rate of loss improvement while MGDA balances the absolute improvement across task losses.\n\n### Projecting Gradient Descent (PCGRAD) [43]\n\nPCGRAD initializes \\(v_i^{PC} = \\nabla \\ell_{i,t}\\), then for each task \\(i\\), PCGRAD loops over all task \\(j \\neq i\\) (in a random order, which is crucial as mentioned in [43]) and removes the \"conflict\"\n\n$$\nv_i^{PC} \\leftarrow v_i^{PC} - v_i \\frac{v_i^{PC^T} \\nabla \\ell_{j,t}}{\\|\\nabla \\ell_{j,t}\\|^2} v_i^{PC}.\n$$\n\nIn the end, PCGRAD produces \\(d_t = \\frac{1}{k} \\sum_{i=1}^{k} v_i^{PC}\\). Due to the construction, PCGRAD will also help improve the \"worst improvement\" across all tasks since the \"conflicts\" have been removed. However, due to the stochastic iterative procedural of this algorithm, it is hard to understand PCGRAD from a first principle approach.\n\n### Conflict-averse Gradient Descent (CAGRAD) [24]\n\n\\(d_t\\) is found by solving\n\n$$\n\\max_{d \\in \\mathbb{R}^m} \\min_{i \\in [k]} \\nabla \\ell^T d \\quad \\text{s.t.} \\quad \\|d - \\nabla \\ell_{0,t}\\| \\leq c\\|\\nabla \\ell_{0,t}\\|.\n$$\n\nHere, \\(\\ell_{0,t} = \\frac{1}{k} \\sum_{i=1}^{k} \\ell_{i,t}\\). CAGRAD seeks an update \\(d_t\\) that optimizes the \"worst improvement\" as much as possible, conditioned on that the update still decreases the average loss. By controlling the hyperparameter \\(c\\), CAGRAD can recover MGDA (\\(c \\rightarrow \\infty\\)) and the vanilla averaged gradient descent (\\(c \\rightarrow 0\\)). Due to the extra constraint, CAGRAD provably converges to the stationary points of \\(\\ell_0\\) when \\(0 \\leq c < 1\\).\n\n### Impartial Multi-Task Learning (IMTL-G) [25]\n\nIMTL-G finds \\(d_t\\) such that it shares the same cosine similarity with any task gradients:\n\n$$\n\\forall i \\neq j, \\quad d_t^T \\nabla \\ell_{i,t} = d_t^T \\nabla \\ell_{j,t} \\quad \\text{and} \\quad d_t = \\sum_{i=1}^{k} w_{i,t} \\nabla \\ell_{i,t}, \\text{ for some } w_t \\in S_k.\n$$\n\nThe constraint that \\(d_t = \\sum_{i=1}^{k} w_{i,t} \\nabla \\ell_{i,t}\\) is for preventing the problem from being under-determined. From the above equation, we can see that IMTL-G ignores the \"size\" of each task gradient and only cares about the \"direction\". As a result, one can think of IMTL-G as a variant of MGDA that applies to the normalized gradients. By doing so, IMTL-G does not suffer from the straggler effect due to slow objectives. Furthermore, one can view IMTL-G as the equal angle descent, which is also proposed in Katrutsa et al. [17], where the objective is to find \\(d\\) such that\n\n$$\n\\forall i \\neq j, \\quad \\cos(d, \\nabla \\ell_{i,t}) = \\cos(d, \\nabla \\ell_{j,t}).\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Gradient Manipulation Methods", "md": "# Gradient Manipulation Methods"}, {"type": "heading", "lvl": 2, "value": "Gradient Manipulation Methods", "md": "## Gradient Manipulation Methods"}, {"type": "text", "value": "In this section, we provide a brief overview of representative gradient manipulation methods in multitask/multiobjective optimization. Specifically, we will also discuss the connections among these methods.", "md": "In this section, we provide a brief overview of representative gradient manipulation methods in multitask/multiobjective optimization. Specifically, we will also discuss the connections among these methods."}, {"type": "heading", "lvl": 3, "value": "Multiple Gradient Descent Algorithm (MGDA) [9, 35]", "md": "### Multiple Gradient Descent Algorithm (MGDA) [9, 35]"}, {"type": "text", "value": "The MGDA algorithm is one of the earliest gradient manipulation methods for multitask learning. In MGDA, the per step update \\(d_t\\) is found by solving\n\n$$\n\\max_{d \\in \\mathbb{R}^m} \\min_{i \\in [k]} \\nabla \\ell_i^T d - \\frac{1}{2} \\|d\\|^2.\n$$\n\nAs a result, the solution \\(d^*\\) of MGDA optimizes the \"worst improvement\" across all tasks or equivalently seeks an equal descent across all task losses as much as possible. But in practice, MGDA suffers from slow convergence since the update \\(d^*\\) can be very small. For instance, if one task has a very small loss scale, the progress of all other tasks will be bounded by the progress on this task. Note that the original objective in (6) is similar to the MGDA objective in the sense that we can view optimizing (6) as optimizing the log of the task losses. Hence, when we compare FAMO against MGDA, one can regard FAMO as balancing the rate of loss improvement while MGDA balances the absolute improvement across task losses.", "md": "The MGDA algorithm is one of the earliest gradient manipulation methods for multitask learning. In MGDA, the per step update \\(d_t\\) is found by solving\n\n$$\n\\max_{d \\in \\mathbb{R}^m} \\min_{i \\in [k]} \\nabla \\ell_i^T d - \\frac{1}{2} \\|d\\|^2.\n$$\n\nAs a result, the solution \\(d^*\\) of MGDA optimizes the \"worst improvement\" across all tasks or equivalently seeks an equal descent across all task losses as much as possible. But in practice, MGDA suffers from slow convergence since the update \\(d^*\\) can be very small. For instance, if one task has a very small loss scale, the progress of all other tasks will be bounded by the progress on this task. Note that the original objective in (6) is similar to the MGDA objective in the sense that we can view optimizing (6) as optimizing the log of the task losses. Hence, when we compare FAMO against MGDA, one can regard FAMO as balancing the rate of loss improvement while MGDA balances the absolute improvement across task losses."}, {"type": "heading", "lvl": 3, "value": "Projecting Gradient Descent (PCGRAD) [43]", "md": "### Projecting Gradient Descent (PCGRAD) [43]"}, {"type": "text", "value": "PCGRAD initializes \\(v_i^{PC} = \\nabla \\ell_{i,t}\\), then for each task \\(i\\), PCGRAD loops over all task \\(j \\neq i\\) (in a random order, which is crucial as mentioned in [43]) and removes the \"conflict\"\n\n$$\nv_i^{PC} \\leftarrow v_i^{PC} - v_i \\frac{v_i^{PC^T} \\nabla \\ell_{j,t}}{\\|\\nabla \\ell_{j,t}\\|^2} v_i^{PC}.\n$$\n\nIn the end, PCGRAD produces \\(d_t = \\frac{1}{k} \\sum_{i=1}^{k} v_i^{PC}\\). Due to the construction, PCGRAD will also help improve the \"worst improvement\" across all tasks since the \"conflicts\" have been removed. However, due to the stochastic iterative procedural of this algorithm, it is hard to understand PCGRAD from a first principle approach.", "md": "PCGRAD initializes \\(v_i^{PC} = \\nabla \\ell_{i,t}\\), then for each task \\(i\\), PCGRAD loops over all task \\(j \\neq i\\) (in a random order, which is crucial as mentioned in [43]) and removes the \"conflict\"\n\n$$\nv_i^{PC} \\leftarrow v_i^{PC} - v_i \\frac{v_i^{PC^T} \\nabla \\ell_{j,t}}{\\|\\nabla \\ell_{j,t}\\|^2} v_i^{PC}.\n$$\n\nIn the end, PCGRAD produces \\(d_t = \\frac{1}{k} \\sum_{i=1}^{k} v_i^{PC}\\). Due to the construction, PCGRAD will also help improve the \"worst improvement\" across all tasks since the \"conflicts\" have been removed. However, due to the stochastic iterative procedural of this algorithm, it is hard to understand PCGRAD from a first principle approach."}, {"type": "heading", "lvl": 3, "value": "Conflict-averse Gradient Descent (CAGRAD) [24]", "md": "### Conflict-averse Gradient Descent (CAGRAD) [24]"}, {"type": "text", "value": "\\(d_t\\) is found by solving\n\n$$\n\\max_{d \\in \\mathbb{R}^m} \\min_{i \\in [k]} \\nabla \\ell^T d \\quad \\text{s.t.} \\quad \\|d - \\nabla \\ell_{0,t}\\| \\leq c\\|\\nabla \\ell_{0,t}\\|.\n$$\n\nHere, \\(\\ell_{0,t} = \\frac{1}{k} \\sum_{i=1}^{k} \\ell_{i,t}\\). CAGRAD seeks an update \\(d_t\\) that optimizes the \"worst improvement\" as much as possible, conditioned on that the update still decreases the average loss. By controlling the hyperparameter \\(c\\), CAGRAD can recover MGDA (\\(c \\rightarrow \\infty\\)) and the vanilla averaged gradient descent (\\(c \\rightarrow 0\\)). Due to the extra constraint, CAGRAD provably converges to the stationary points of \\(\\ell_0\\) when \\(0 \\leq c < 1\\).", "md": "\\(d_t\\) is found by solving\n\n$$\n\\max_{d \\in \\mathbb{R}^m} \\min_{i \\in [k]} \\nabla \\ell^T d \\quad \\text{s.t.} \\quad \\|d - \\nabla \\ell_{0,t}\\| \\leq c\\|\\nabla \\ell_{0,t}\\|.\n$$\n\nHere, \\(\\ell_{0,t} = \\frac{1}{k} \\sum_{i=1}^{k} \\ell_{i,t}\\). CAGRAD seeks an update \\(d_t\\) that optimizes the \"worst improvement\" as much as possible, conditioned on that the update still decreases the average loss. By controlling the hyperparameter \\(c\\), CAGRAD can recover MGDA (\\(c \\rightarrow \\infty\\)) and the vanilla averaged gradient descent (\\(c \\rightarrow 0\\)). Due to the extra constraint, CAGRAD provably converges to the stationary points of \\(\\ell_0\\) when \\(0 \\leq c < 1\\)."}, {"type": "heading", "lvl": 3, "value": "Impartial Multi-Task Learning (IMTL-G) [25]", "md": "### Impartial Multi-Task Learning (IMTL-G) [25]"}, {"type": "text", "value": "IMTL-G finds \\(d_t\\) such that it shares the same cosine similarity with any task gradients:\n\n$$\n\\forall i \\neq j, \\quad d_t^T \\nabla \\ell_{i,t} = d_t^T \\nabla \\ell_{j,t} \\quad \\text{and} \\quad d_t = \\sum_{i=1}^{k} w_{i,t} \\nabla \\ell_{i,t}, \\text{ for some } w_t \\in S_k.\n$$\n\nThe constraint that \\(d_t = \\sum_{i=1}^{k} w_{i,t} \\nabla \\ell_{i,t}\\) is for preventing the problem from being under-determined. From the above equation, we can see that IMTL-G ignores the \"size\" of each task gradient and only cares about the \"direction\". As a result, one can think of IMTL-G as a variant of MGDA that applies to the normalized gradients. By doing so, IMTL-G does not suffer from the straggler effect due to slow objectives. Furthermore, one can view IMTL-G as the equal angle descent, which is also proposed in Katrutsa et al. [17], where the objective is to find \\(d\\) such that\n\n$$\n\\forall i \\neq j, \\quad \\cos(d, \\nabla \\ell_{i,t}) = \\cos(d, \\nabla \\ell_{j,t}).\n$$", "md": "IMTL-G finds \\(d_t\\) such that it shares the same cosine similarity with any task gradients:\n\n$$\n\\forall i \\neq j, \\quad d_t^T \\nabla \\ell_{i,t} = d_t^T \\nabla \\ell_{j,t} \\quad \\text{and} \\quad d_t = \\sum_{i=1}^{k} w_{i,t} \\nabla \\ell_{i,t}, \\text{ for some } w_t \\in S_k.\n$$\n\nThe constraint that \\(d_t = \\sum_{i=1}^{k} w_{i,t} \\nabla \\ell_{i,t}\\) is for preventing the problem from being under-determined. From the above equation, we can see that IMTL-G ignores the \"size\" of each task gradient and only cares about the \"direction\". As a result, one can think of IMTL-G as a variant of MGDA that applies to the normalized gradients. By doing so, IMTL-G does not suffer from the straggler effect due to slow objectives. Furthermore, one can view IMTL-G as the equal angle descent, which is also proposed in Katrutsa et al. [17], where the objective is to find \\(d\\) such that\n\n$$\n\\forall i \\neq j, \\quad \\cos(d, \\nabla \\ell_{i,t}) = \\cos(d, \\nabla \\ell_{j,t}).\n$$"}]}, {"page": 15, "text": " NASHMTL[32]                       NASHMTL finds dt by solving a bargaining game treating the local improvement\n of each task loss as the utility for each task:\n                                                                                       k\n                                                                       max            \u2211    log (\u2207\u2113\u22ba      i,td).\n                                                                  d\u2208Rm,\u2225d\u2225\u22641         i=1\n Note that the objective of NASHMTL implicitly assumes that there exists d such that \u2200                                                                      i, \u2207\u2113\u22ba  i,td > 0\n(otherwise we reach the Pareto front). It is easy to see that\n                      max      \u2211 k   log (\u2207\u2113\u22ba      i,td) = max           \u2211 k   log\u27e8      \u2207\u2113i,t                           \u2211 k   log cos      (\u2207\u2113i,t,      d).\n                      \u2225d\u2225\u22641                                                                                     \u2225d\u2225\u22641\n                               i=1                              \u2225d\u2225\u22641    i=1           \u2225\u2207\u2113i,t\u2225       , d\u27e9   = max        i=1\nTherefore, due to the log, NASHMTL also ignores the \u201csize\" of task gradients and only cares about\n                                                                              \u2207\u2113i,t\n their \u201cdirections\". Moreover, denote ui =                                   \u2225\u2207\u2113i,t\u2225    . Then, according to the KKT condition, we know:\n                                             ui                           \u03b1 \u2265     0            \u21d2                 d = 1               1\n                                      \u2211i    u\u22bai d \u2212     \u03b1d = 0,                                                         \u03b1 \u2211   i   u\u22ba i d  ui.\n Consider when k = 2, if we take the equal angle descent direction: d\u2220                                                         = (u1 + u2)/2 (note that as u1\n and u2 are normalized, their bisector is just their average). Then it is easy to check that\n           d\u2220     = 1                  2                               2                                                1(u1 + u2)           = u\u22ba   2(u1 + u2)        .\n                      \u03b1  (  u\u22ba 1(u1 + u2)           u1 +     u\u22ba 2(u1 + u2)          u2), where \u03b1 = u\u22ba                          4                            4\nAs a result, we can see that when k = 2, NASHMTL is equivalent to IMTL-G (or the equal angle\n descent). However, when k > 2, this is not in general true.\n Remark Note that all of these gradient manipulation methods require computing and storing K task\n gradients before applying f to compute dt, which often involves solving an additional optimization\n problem. Hence, these methods can be slow for large K and large model sizes.\n B        Amortizing other Gradient Manipulation Methods\nAlthough FAMO uses iterative update on w, it is not immediately clear whether we can apply the\n same amortization easily on other existing gradient manipulation methods. In this section, we discuss\n such possibilities and point out the challenges.\n Amortizing MGDA                           This is almost the same as in FAMO, except that MGDA acts on the original\n task losses while FAMO acts on the log of task losses.\n Amortizing PCGRAD                            For PCGRAD, finding the final update vector requires iteratively projecting\n one task gradient to the other, so there is no straightforward way of bypassing the computation of\n task gradients.\n Amortizing IMTL-G                             The task weighting in IMTL-G is computed by a series of matrix-matrix\n and matrix-vector products using task gradients [25]. Hence, it is also hard to amortize its computation\n over time.\nTherefore, we focus on deriving the amortization for CAGRAD and NASHMTL.\n Amortizing CAGRAD                             For CAGRAD, the dual objective is\n                                                             min                        wg0 + c\u2225gw\u2225\u2225g0\u2225,                                                                  (15)\n                                                             w\u2208Sk F      (w) = g\u22ba\nwhere g0 = \u2207\u21130,t and gw = \u2211k                         i=1 wi\u2207\u2113i. Denote\u23a1                  \u2207\u2113\u22ba       \u23a4\n                                                                                      \u23a2       1,t  \u23a5\n                                                                                      \u23a2      \u22ee     \u23a5\n                                                                              G =     \u23a2            \u23a5  .\n                                                                                      \u23a2            \u23a5\n                                                                                      \u23a2  \u2207\u2113\u22ba  k,t  \u23a5\n                                                                                      \u23a3 15         \u23a6", "md": "```markdown\nNASHMTL[32]\nNASHMTL finds dt by solving a bargaining game treating the local improvement\nof each task loss as the utility for each task:\n\n$$\n\\max_{d\\in\\mathbb{R}^m, \\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log(\\nabla\\ell_i^{\\top}d).\n$$\n\nNote that the objective of NASHMTL implicitly assumes that there exists \\(d\\) such that \\(\\forall i, \\nabla\\ell_i^{\\top}d > 0\\) (otherwise we reach the Pareto front). It is easy to see that\n\n$$\n\\max_{\\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log(\\nabla\\ell_i^{\\top}d) = \\max_{\\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log\\left\\langle \\nabla\\ell_i, \\sum_{i=1}^{k} \\log\\cos(\\nabla\\ell_i, d)\\right\\rangle = \\max_{i=1}^{k} \\frac{\\nabla\\ell_i^{\\top}}{\\|\\nabla\\ell_i\\|}d.\n$$\n\nTherefore, due to the log, NASHMTL also ignores the \"size\" of task gradients and only cares about their \"directions\". Moreover, denote \\(u_i = \\frac{\\nabla\\ell_i^{\\top}}{\\|\\nabla\\ell_i\\|}\\). Then, according to the KKT condition, we know:\n\n$$\nu_i \\alpha \\geq 0 \\Rightarrow d = \\frac{1}{\\sum_{i} u_i^{\\top}d u_i}.\n$$\n\nConsider when \\(k = 2\\), if we take the equal angle descent direction: \\(d^{\\angle} = (u_1 + u_2)/2\\) (note that as \\(u_1\\) and \\(u_2\\) are normalized, their bisector is just their average). Then it is easy to check that\n\n$$\nd^{\\angle} = \\frac{1}{2} \\alpha \\left(u_1^{\\top}(u_1 + u_2) + u_2^{\\top}(u_1 + u_2)\\right) = \\frac{1}{4} \\alpha u_1^{\\top}(u_1 + u_2) + \\frac{1}{4} \\alpha u_2^{\\top}(u_1 + u_2),\n$$\n\nwhere \\(\\alpha = \\frac{u_1^{\\top}u_2}{4}\\).\n\nAs a result, we can see that when \\(k = 2\\), NASHMTL is equivalent to IMTL-G (or the equal angle descent). However, when \\(k > 2\\), this is not in general true.\n\n**Remark:** Note that all of these gradient manipulation methods require computing and storing \\(K\\) task gradients before applying \\(f\\) to compute \\(d\\), which often involves solving an additional optimization problem. Hence, these methods can be slow for large \\(K\\) and large model sizes.\n\n**Amortizing other Gradient Manipulation Methods**\n\nAlthough FAMO uses iterative update on \\(w\\), it is not immediately clear whether we can apply the same amortization easily on other existing gradient manipulation methods. In this section, we discuss such possibilities and point out the challenges.\n\n**Amortizing MGDA**\n\nThis is almost the same as in FAMO, except that MGDA acts on the original task losses while FAMO acts on the log of task losses.\n\n**Amortizing PCGRAD**\n\nFor PCGRAD, finding the final update vector requires iteratively projecting one task gradient to the other, so there is no straightforward way of bypassing the computation of task gradients.\n\n**Amortizing IMTL-G**\n\nThe task weighting in IMTL-G is computed by a series of matrix-matrix and matrix-vector products using task gradients [25]. Hence, it is also hard to amortize its computation over time.\n\nTherefore, we focus on deriving the amortization for CAGRAD and NASHMTL.\n\n**Amortizing CAGRAD**\n\nFor CAGRAD, the dual objective is\n\n$$\n\\min_{w\\in S_k} F(w) = g_0^{\\top}w + c\\|g_w\\|\\|g_0\\|,\n$$\n\nwhere \\(g_0 = \\nabla\\ell_0,t\\) and \\(g_w = \\sum_{i=1}^{k} w_i\\nabla\\ell_i\\). Denote\n\n$$\nG = \\begin{bmatrix}\n\\nabla\\ell_1^{\\top} \\\\\n\\vdots \\\\\n\\nabla\\ell_k^{\\top}\n\\end{bmatrix}.\n$$\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nNASHMTL[32]\nNASHMTL finds dt by solving a bargaining game treating the local improvement\nof each task loss as the utility for each task:\n\n$$\n\\max_{d\\in\\mathbb{R}^m, \\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log(\\nabla\\ell_i^{\\top}d).\n$$\n\nNote that the objective of NASHMTL implicitly assumes that there exists \\(d\\) such that \\(\\forall i, \\nabla\\ell_i^{\\top}d > 0\\) (otherwise we reach the Pareto front). It is easy to see that\n\n$$\n\\max_{\\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log(\\nabla\\ell_i^{\\top}d) = \\max_{\\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log\\left\\langle \\nabla\\ell_i, \\sum_{i=1}^{k} \\log\\cos(\\nabla\\ell_i, d)\\right\\rangle = \\max_{i=1}^{k} \\frac{\\nabla\\ell_i^{\\top}}{\\|\\nabla\\ell_i\\|}d.\n$$\n\nTherefore, due to the log, NASHMTL also ignores the \"size\" of task gradients and only cares about their \"directions\". Moreover, denote \\(u_i = \\frac{\\nabla\\ell_i^{\\top}}{\\|\\nabla\\ell_i\\|}\\). Then, according to the KKT condition, we know:\n\n$$\nu_i \\alpha \\geq 0 \\Rightarrow d = \\frac{1}{\\sum_{i} u_i^{\\top}d u_i}.\n$$\n\nConsider when \\(k = 2\\), if we take the equal angle descent direction: \\(d^{\\angle} = (u_1 + u_2)/2\\) (note that as \\(u_1\\) and \\(u_2\\) are normalized, their bisector is just their average). Then it is easy to check that\n\n$$\nd^{\\angle} = \\frac{1}{2} \\alpha \\left(u_1^{\\top}(u_1 + u_2) + u_2^{\\top}(u_1 + u_2)\\right) = \\frac{1}{4} \\alpha u_1^{\\top}(u_1 + u_2) + \\frac{1}{4} \\alpha u_2^{\\top}(u_1 + u_2),\n$$\n\nwhere \\(\\alpha = \\frac{u_1^{\\top}u_2}{4}\\).\n\nAs a result, we can see that when \\(k = 2\\), NASHMTL is equivalent to IMTL-G (or the equal angle descent). However, when \\(k > 2\\), this is not in general true.\n\n**Remark:** Note that all of these gradient manipulation methods require computing and storing \\(K\\) task gradients before applying \\(f\\) to compute \\(d\\), which often involves solving an additional optimization problem. Hence, these methods can be slow for large \\(K\\) and large model sizes.\n\n**Amortizing other Gradient Manipulation Methods**\n\nAlthough FAMO uses iterative update on \\(w\\), it is not immediately clear whether we can apply the same amortization easily on other existing gradient manipulation methods. In this section, we discuss such possibilities and point out the challenges.\n\n**Amortizing MGDA**\n\nThis is almost the same as in FAMO, except that MGDA acts on the original task losses while FAMO acts on the log of task losses.\n\n**Amortizing PCGRAD**\n\nFor PCGRAD, finding the final update vector requires iteratively projecting one task gradient to the other, so there is no straightforward way of bypassing the computation of task gradients.\n\n**Amortizing IMTL-G**\n\nThe task weighting in IMTL-G is computed by a series of matrix-matrix and matrix-vector products using task gradients [25]. Hence, it is also hard to amortize its computation over time.\n\nTherefore, we focus on deriving the amortization for CAGRAD and NASHMTL.\n\n**Amortizing CAGRAD**\n\nFor CAGRAD, the dual objective is\n\n$$\n\\min_{w\\in S_k} F(w) = g_0^{\\top}w + c\\|g_w\\|\\|g_0\\|,\n$$\n\nwhere \\(g_0 = \\nabla\\ell_0,t\\) and \\(g_w = \\sum_{i=1}^{k} w_i\\nabla\\ell_i\\). Denote\n\n$$\nG = \\begin{bmatrix}\n\\nabla\\ell_1^{\\top} \\\\\n\\vdots \\\\\n\\nabla\\ell_k^{\\top}\n\\end{bmatrix}.\n$$\n```", "md": "```markdown\nNASHMTL[32]\nNASHMTL finds dt by solving a bargaining game treating the local improvement\nof each task loss as the utility for each task:\n\n$$\n\\max_{d\\in\\mathbb{R}^m, \\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log(\\nabla\\ell_i^{\\top}d).\n$$\n\nNote that the objective of NASHMTL implicitly assumes that there exists \\(d\\) such that \\(\\forall i, \\nabla\\ell_i^{\\top}d > 0\\) (otherwise we reach the Pareto front). It is easy to see that\n\n$$\n\\max_{\\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log(\\nabla\\ell_i^{\\top}d) = \\max_{\\|d\\|\\leq 1} \\sum_{i=1}^{k} \\log\\left\\langle \\nabla\\ell_i, \\sum_{i=1}^{k} \\log\\cos(\\nabla\\ell_i, d)\\right\\rangle = \\max_{i=1}^{k} \\frac{\\nabla\\ell_i^{\\top}}{\\|\\nabla\\ell_i\\|}d.\n$$\n\nTherefore, due to the log, NASHMTL also ignores the \"size\" of task gradients and only cares about their \"directions\". Moreover, denote \\(u_i = \\frac{\\nabla\\ell_i^{\\top}}{\\|\\nabla\\ell_i\\|}\\). Then, according to the KKT condition, we know:\n\n$$\nu_i \\alpha \\geq 0 \\Rightarrow d = \\frac{1}{\\sum_{i} u_i^{\\top}d u_i}.\n$$\n\nConsider when \\(k = 2\\), if we take the equal angle descent direction: \\(d^{\\angle} = (u_1 + u_2)/2\\) (note that as \\(u_1\\) and \\(u_2\\) are normalized, their bisector is just their average). Then it is easy to check that\n\n$$\nd^{\\angle} = \\frac{1}{2} \\alpha \\left(u_1^{\\top}(u_1 + u_2) + u_2^{\\top}(u_1 + u_2)\\right) = \\frac{1}{4} \\alpha u_1^{\\top}(u_1 + u_2) + \\frac{1}{4} \\alpha u_2^{\\top}(u_1 + u_2),\n$$\n\nwhere \\(\\alpha = \\frac{u_1^{\\top}u_2}{4}\\).\n\nAs a result, we can see that when \\(k = 2\\), NASHMTL is equivalent to IMTL-G (or the equal angle descent). However, when \\(k > 2\\), this is not in general true.\n\n**Remark:** Note that all of these gradient manipulation methods require computing and storing \\(K\\) task gradients before applying \\(f\\) to compute \\(d\\), which often involves solving an additional optimization problem. Hence, these methods can be slow for large \\(K\\) and large model sizes.\n\n**Amortizing other Gradient Manipulation Methods**\n\nAlthough FAMO uses iterative update on \\(w\\), it is not immediately clear whether we can apply the same amortization easily on other existing gradient manipulation methods. In this section, we discuss such possibilities and point out the challenges.\n\n**Amortizing MGDA**\n\nThis is almost the same as in FAMO, except that MGDA acts on the original task losses while FAMO acts on the log of task losses.\n\n**Amortizing PCGRAD**\n\nFor PCGRAD, finding the final update vector requires iteratively projecting one task gradient to the other, so there is no straightforward way of bypassing the computation of task gradients.\n\n**Amortizing IMTL-G**\n\nThe task weighting in IMTL-G is computed by a series of matrix-matrix and matrix-vector products using task gradients [25]. Hence, it is also hard to amortize its computation over time.\n\nTherefore, we focus on deriving the amortization for CAGRAD and NASHMTL.\n\n**Amortizing CAGRAD**\n\nFor CAGRAD, the dual objective is\n\n$$\n\\min_{w\\in S_k} F(w) = g_0^{\\top}w + c\\|g_w\\|\\|g_0\\|,\n$$\n\nwhere \\(g_0 = \\nabla\\ell_0,t\\) and \\(g_w = \\sum_{i=1}^{k} w_i\\nabla\\ell_i\\). Denote\n\n$$\nG = \\begin{bmatrix}\n\\nabla\\ell_1^{\\top} \\\\\n\\vdots \\\\\n\\nabla\\ell_k^{\\top}\n\\end{bmatrix}.\n$$\n```"}]}, {"page": 16, "text": " Now, if we take the gradient with respect to w in (15), we have:\n                                                     \u2202F\n                                                     \u2202w = G\u22bag0 + c \u2225g0\u2225    \u2225gw\u2225G\u22bagw.                                  (16)\n As a result, in order to approximate this gradient, one can separately estimate:\n                                                     G\u22bag0 \u2248      \u2113(\u03b8) \u2212    \u2113(\u03b8 \u2212   \u03b1g0)\n                                                                             \u03b1\n                                                    G\u22bagw \u2248       \u2113(\u03b8) \u2212    \u2113(\u03b8 \u2212   \u03b1gw).                              (17)\n                                                      \u2225g0\u2225   \u2248  \u221a   1\u22baG\u22bag0   \u03b1\n                                                     \u2225gw\u2225    \u2248  \u221aw\u22baG\u22bagw\n Once all these are estimated, one can combine them together to perform a single update on w. But\n note that this will require 3 forward and backward passes through the model, making it harder to\n implement in practice.\n Amortizing NASHMTL                      Per derivation from NASHMTL [32], the objective is to solve for w:\n One can therefore form an objective:                        G\u22baGw = 1 \u2298         w.           2                        (18)\n                                                 min                                         2.                       (19)\n                                                   w F    (w) = \u2225G\u22baGw \u2212            1 \u2298   w\u2225\n Taking the derivative of F with respect to w, we have\n                           \u2202F\n                           \u2202w = 2G\u22baG(G\u22bagw \u2212                 1 \u2298   w) + 2(G\u22bagw \u2212           1 \u2298   w) \u2298    (w \u2299     w).  (20)\n Therefore, to approximate the gradient of w, one needs to first estimate\n                                                G\u22bagw \u2248       L(\u03b8) \u2212     L(\u03b8 \u2212    \u03b1gw)      = \u03b7.                       (21)\n                                                                          \u03b1\n Then we estimate\n                                   G\u22baG(\u03b7 \u2212        1 \u2298  w) \u2248     L(\u03b8) \u2212     L(\u03b8 \u2212    \u03b1G(\u03b7 \u2212       1 \u2298  w)).            (22)\n                                                                                     \u03b1\n Again, this results in 3 forward and backward passes through the model, let alone the overhead of\n resetting the model back to \u03b8 (requires a copy of the original weights).\n In short, though it is possible to derive fast approximation algorithm to approximate the gradient\n update on w for some of the existing gradient manipulation methods, it often involves much more\n complicated computation compared to that of FAMO.\n C      FAMO Pseudocode in PyTorch\nWe provide the pseudocode for FAMO in Algorithm 2. To use FAMO, one just first compute the\n task losses, call get_weighted_loss to get the weighted loss, and do the normal backpropagation\n through the weighted loss. After that, one call update to update the task weighting.\n D      Toy Example\nWe provide the task objectives for the toy example in the following. The model parameter \u03b8 =\n (\u03b81,  \u03b82) \u2208   R2 and the task objectives are L1 and L2:\n        L1  (\u03b8) = 0.1 \u22c5    (c1(\u03b8)f1(\u03b8) + c2(\u03b8)g1(\u03b8)) and L2(\u03b8) = c1(\u03b8)f2(\u03b8) + c2(\u03b8)g2(\u03b8), where\n        f1(\u03b8) = log (       max(\u22230.5(\u2212\u03b81 \u2212          7) \u2212   tanh    (\u2212\u03b82)\u2223, 0.000005)) + 6,\n        f2(\u03b8) = log (       max(\u22230.5(\u2212\u03b81 + 3) \u2212            tanh    (\u2212\u03b82) + 2\u2223, 0.000005)) + 6,\n         g1(\u03b8) = ((\u2212\u03b81 + 7)2 + 0.1 \u2217              (\u2212\u03b82 \u2212    8)2)/10 \u2212       20,\n         g2(\u03b8) = ((\u2212\u03b81 \u2212         7)2 + 0.1 \u2217      (\u2212\u03b82 \u2212    8)2)/10 \u2212       20,\n         c1(\u03b8) = max(tanh            (0.5 \u2217   \u03b82), 0) and c2(\u03b8) = max(tanh                    (\u22120.5 \u2217     \u03b82), 0).\n                                                                       16", "md": "# Math Equations and Text\n\nNow, if we take the gradient with respect to w in (15), we have:\n\n$$\n\\frac{\\partial F}{\\partial w} = G^{\\top}g_0 + c \\frac{\\|g_0\\|}{\\|g_w\\|}G^{\\top}g_w. \\quad (16)\n$$\n\nAs a result, in order to approximate this gradient, one can separately estimate:\n\n$$\n\\begin{align*}\nG^{\\top}g_0 &\\approx \\frac{\\ell(\\theta) - \\ell(\\theta - \\alpha g_0)}{\\alpha}, \\\\\nG^{\\top}g_w &\\approx \\frac{\\ell(\\theta) - \\ell(\\theta - \\alpha g_w)}{\\alpha}, \\quad (17) \\\\\n\\|g_0\\| &\\approx \\sqrt{1^{\\top}G^{\\top}g_0 \\alpha}, \\\\\n\\|g_w\\| &\\approx \\sqrt{w^{\\top}G^{\\top}g_w}.\n\\end{align*}\n$$\n\nOnce all these are estimated, one can combine them together to perform a single update on w. But note that this will require 3 forward and backward passes through the model, making it harder to implement in practice.\n\nAmortizing NASHMTL Per derivation from NASHMTL [32], the objective is to solve for w:\n\n$$\n\\min_w F(w) = \\|G^{\\top}Gw - \\frac{1}{w}\\|^2. \\quad (18)\n$$\n$$\n\\min_w F(w) = \\left\\|G^{\\top}Gw - \\frac{1}{w}\\right\\|^2. \\quad (19)\n$$\n\nTaking the derivative of F with respect to w, we have:\n\n$$\n\\frac{\\partial F}{\\partial w} = 2G^{\\top}G(G^{\\top}g_w - \\frac{1}{w}) + 2(G^{\\top}g_w - \\frac{1}{w}) \\oslash (w \\odot w). \\quad (20)\n$$\n\nTherefore, to approximate the gradient of w, one needs to first estimate:\n\n$$\nG^{\\top}g_w \\approx \\frac{L(\\theta) - L(\\theta - \\alpha g_w)}{\\alpha} = \\eta. \\quad (21)\n$$\n\nThen we estimate:\n\n$$\nG^{\\top}G(\\eta - \\frac{1}{w}) \\approx L(\\theta) - L(\\theta - \\alpha G(\\eta - \\frac{1}{w})). \\quad (22)\n$$\n\nAgain, this results in 3 forward and backward passes through the model, let alone the overhead of resetting the model back to \u03b8 (requires a copy of the original weights).\n\nIn short, though it is possible to derive fast approximation algorithm to approximate the gradient update on w for some of the existing gradient manipulation methods, it often involves much more complicated computation compared to that of FAMO.\n\nC FAMO Pseudocode in PyTorch\n\nWe provide the pseudocode for FAMO in Algorithm 2. To use FAMO, one just first compute the task losses, call get_weighted_loss to get the weighted loss, and do the normal backpropagation through the weighted loss. After that, one call update to update the task weighting.\n\nD Toy Example\n\nWe provide the task objectives for the toy example in the following. The model parameter \u03b8 = (\u03b81, \u03b82) \u2208 \u211d^2 and the task objectives are L1 and L2:\n\n$$\n\\begin{align*}\nL1(\\theta) &= 0.1 \\cdot (c1(\\theta)f1(\\theta) + c2(\\theta)g1(\\theta)), \\\\\nL2(\\theta) &= c1(\\theta)f2(\\theta) + c2(\\theta)g2(\\theta),\n\\end{align*}\n$$\nwhere\n\n$$\n\\begin{align*}\nf1(\\theta) &= \\log\\left(\\max\\left(|0.5(-\\theta_1 - 7) - \\tanh(-\\theta_2)|, 0.000005\\right)\\right) + 6, \\\\\nf2(\\theta) &= \\log\\left(\\max\\left(|0.5(-\\theta_1 + 3) - \\tanh(-\\theta_2) + 2|, 0.000005\\right)\\right) + 6, \\\\\ng1(\\theta) &= \\left((- \\theta_1 + 7)^2 + 0.1 \\cdot (-\\theta_2 - 8)^2\\right)/10 - 20, \\\\\ng2(\\theta) &= \\left((- \\theta_1 - 7)^2 + 0.1 \\cdot (-\\theta_2 - 8)^2\\right)/10 - 20, \\\\\nc1(\\theta) &= \\max\\left(\\tanh(0.5 \\cdot \\theta_2), 0\\right), \\\\\nc2(\\theta) &= \\max\\left(\\tanh(-0.5 \\cdot \\theta_2), 0\\right).\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "Now, if we take the gradient with respect to w in (15), we have:\n\n$$\n\\frac{\\partial F}{\\partial w} = G^{\\top}g_0 + c \\frac{\\|g_0\\|}{\\|g_w\\|}G^{\\top}g_w. \\quad (16)\n$$\n\nAs a result, in order to approximate this gradient, one can separately estimate:\n\n$$\n\\begin{align*}\nG^{\\top}g_0 &\\approx \\frac{\\ell(\\theta) - \\ell(\\theta - \\alpha g_0)}{\\alpha}, \\\\\nG^{\\top}g_w &\\approx \\frac{\\ell(\\theta) - \\ell(\\theta - \\alpha g_w)}{\\alpha}, \\quad (17) \\\\\n\\|g_0\\| &\\approx \\sqrt{1^{\\top}G^{\\top}g_0 \\alpha}, \\\\\n\\|g_w\\| &\\approx \\sqrt{w^{\\top}G^{\\top}g_w}.\n\\end{align*}\n$$\n\nOnce all these are estimated, one can combine them together to perform a single update on w. But note that this will require 3 forward and backward passes through the model, making it harder to implement in practice.\n\nAmortizing NASHMTL Per derivation from NASHMTL [32], the objective is to solve for w:\n\n$$\n\\min_w F(w) = \\|G^{\\top}Gw - \\frac{1}{w}\\|^2. \\quad (18)\n$$\n$$\n\\min_w F(w) = \\left\\|G^{\\top}Gw - \\frac{1}{w}\\right\\|^2. \\quad (19)\n$$\n\nTaking the derivative of F with respect to w, we have:\n\n$$\n\\frac{\\partial F}{\\partial w} = 2G^{\\top}G(G^{\\top}g_w - \\frac{1}{w}) + 2(G^{\\top}g_w - \\frac{1}{w}) \\oslash (w \\odot w). \\quad (20)\n$$\n\nTherefore, to approximate the gradient of w, one needs to first estimate:\n\n$$\nG^{\\top}g_w \\approx \\frac{L(\\theta) - L(\\theta - \\alpha g_w)}{\\alpha} = \\eta. \\quad (21)\n$$\n\nThen we estimate:\n\n$$\nG^{\\top}G(\\eta - \\frac{1}{w}) \\approx L(\\theta) - L(\\theta - \\alpha G(\\eta - \\frac{1}{w})). \\quad (22)\n$$\n\nAgain, this results in 3 forward and backward passes through the model, let alone the overhead of resetting the model back to \u03b8 (requires a copy of the original weights).\n\nIn short, though it is possible to derive fast approximation algorithm to approximate the gradient update on w for some of the existing gradient manipulation methods, it often involves much more complicated computation compared to that of FAMO.\n\nC FAMO Pseudocode in PyTorch\n\nWe provide the pseudocode for FAMO in Algorithm 2. To use FAMO, one just first compute the task losses, call get_weighted_loss to get the weighted loss, and do the normal backpropagation through the weighted loss. After that, one call update to update the task weighting.\n\nD Toy Example\n\nWe provide the task objectives for the toy example in the following. The model parameter \u03b8 = (\u03b81, \u03b82) \u2208 \u211d^2 and the task objectives are L1 and L2:\n\n$$\n\\begin{align*}\nL1(\\theta) &= 0.1 \\cdot (c1(\\theta)f1(\\theta) + c2(\\theta)g1(\\theta)), \\\\\nL2(\\theta) &= c1(\\theta)f2(\\theta) + c2(\\theta)g2(\\theta),\n\\end{align*}\n$$\nwhere\n\n$$\n\\begin{align*}\nf1(\\theta) &= \\log\\left(\\max\\left(|0.5(-\\theta_1 - 7) - \\tanh(-\\theta_2)|, 0.000005\\right)\\right) + 6, \\\\\nf2(\\theta) &= \\log\\left(\\max\\left(|0.5(-\\theta_1 + 3) - \\tanh(-\\theta_2) + 2|, 0.000005\\right)\\right) + 6, \\\\\ng1(\\theta) &= \\left((- \\theta_1 + 7)^2 + 0.1 \\cdot (-\\theta_2 - 8)^2\\right)/10 - 20, \\\\\ng2(\\theta) &= \\left((- \\theta_1 - 7)^2 + 0.1 \\cdot (-\\theta_2 - 8)^2\\right)/10 - 20, \\\\\nc1(\\theta) &= \\max\\left(\\tanh(0.5 \\cdot \\theta_2), 0\\right), \\\\\nc2(\\theta) &= \\max\\left(\\tanh(-0.5 \\cdot \\theta_2), 0\\right).\n\\end{align*}\n$$", "md": "Now, if we take the gradient with respect to w in (15), we have:\n\n$$\n\\frac{\\partial F}{\\partial w} = G^{\\top}g_0 + c \\frac{\\|g_0\\|}{\\|g_w\\|}G^{\\top}g_w. \\quad (16)\n$$\n\nAs a result, in order to approximate this gradient, one can separately estimate:\n\n$$\n\\begin{align*}\nG^{\\top}g_0 &\\approx \\frac{\\ell(\\theta) - \\ell(\\theta - \\alpha g_0)}{\\alpha}, \\\\\nG^{\\top}g_w &\\approx \\frac{\\ell(\\theta) - \\ell(\\theta - \\alpha g_w)}{\\alpha}, \\quad (17) \\\\\n\\|g_0\\| &\\approx \\sqrt{1^{\\top}G^{\\top}g_0 \\alpha}, \\\\\n\\|g_w\\| &\\approx \\sqrt{w^{\\top}G^{\\top}g_w}.\n\\end{align*}\n$$\n\nOnce all these are estimated, one can combine them together to perform a single update on w. But note that this will require 3 forward and backward passes through the model, making it harder to implement in practice.\n\nAmortizing NASHMTL Per derivation from NASHMTL [32], the objective is to solve for w:\n\n$$\n\\min_w F(w) = \\|G^{\\top}Gw - \\frac{1}{w}\\|^2. \\quad (18)\n$$\n$$\n\\min_w F(w) = \\left\\|G^{\\top}Gw - \\frac{1}{w}\\right\\|^2. \\quad (19)\n$$\n\nTaking the derivative of F with respect to w, we have:\n\n$$\n\\frac{\\partial F}{\\partial w} = 2G^{\\top}G(G^{\\top}g_w - \\frac{1}{w}) + 2(G^{\\top}g_w - \\frac{1}{w}) \\oslash (w \\odot w). \\quad (20)\n$$\n\nTherefore, to approximate the gradient of w, one needs to first estimate:\n\n$$\nG^{\\top}g_w \\approx \\frac{L(\\theta) - L(\\theta - \\alpha g_w)}{\\alpha} = \\eta. \\quad (21)\n$$\n\nThen we estimate:\n\n$$\nG^{\\top}G(\\eta - \\frac{1}{w}) \\approx L(\\theta) - L(\\theta - \\alpha G(\\eta - \\frac{1}{w})). \\quad (22)\n$$\n\nAgain, this results in 3 forward and backward passes through the model, let alone the overhead of resetting the model back to \u03b8 (requires a copy of the original weights).\n\nIn short, though it is possible to derive fast approximation algorithm to approximate the gradient update on w for some of the existing gradient manipulation methods, it often involves much more complicated computation compared to that of FAMO.\n\nC FAMO Pseudocode in PyTorch\n\nWe provide the pseudocode for FAMO in Algorithm 2. To use FAMO, one just first compute the task losses, call get_weighted_loss to get the weighted loss, and do the normal backpropagation through the weighted loss. After that, one call update to update the task weighting.\n\nD Toy Example\n\nWe provide the task objectives for the toy example in the following. The model parameter \u03b8 = (\u03b81, \u03b82) \u2208 \u211d^2 and the task objectives are L1 and L2:\n\n$$\n\\begin{align*}\nL1(\\theta) &= 0.1 \\cdot (c1(\\theta)f1(\\theta) + c2(\\theta)g1(\\theta)), \\\\\nL2(\\theta) &= c1(\\theta)f2(\\theta) + c2(\\theta)g2(\\theta),\n\\end{align*}\n$$\nwhere\n\n$$\n\\begin{align*}\nf1(\\theta) &= \\log\\left(\\max\\left(|0.5(-\\theta_1 - 7) - \\tanh(-\\theta_2)|, 0.000005\\right)\\right) + 6, \\\\\nf2(\\theta) &= \\log\\left(\\max\\left(|0.5(-\\theta_1 + 3) - \\tanh(-\\theta_2) + 2|, 0.000005\\right)\\right) + 6, \\\\\ng1(\\theta) &= \\left((- \\theta_1 + 7)^2 + 0.1 \\cdot (-\\theta_2 - 8)^2\\right)/10 - 20, \\\\\ng2(\\theta) &= \\left((- \\theta_1 - 7)^2 + 0.1 \\cdot (-\\theta_2 - 8)^2\\right)/10 - 20, \\\\\nc1(\\theta) &= \\max\\left(\\tanh(0.5 \\cdot \\theta_2), 0\\right), \\\\\nc2(\\theta) &= \\max\\left(\\tanh(-0.5 \\cdot \\theta_2), 0\\right).\n\\end{align*}\n$$"}]}, {"page": 17, "text": "Algorithm 2 Implementation of FAMO in PyTorch-like Pseudocode\nclass FAMO:\ndef __init__(self, num_tasks, min_losses, \u03b1=0.025, \u03b3=0.001):\n     # min_losses       (num_tasks,) the loss lower bound for each task.\n     self.min_losses = min_losses\n     self.xi = torch.tensor([0.0] * num_tasks, requires_grad=True)\n     self.xi_opt = torch.optim.Adam([self.xi], lr=\u03b1, weight_decay=\u03b3)\ndef get_weighted_loss(self, losses):\n     # losses      (num_tasks,)\n     z = F.softmax(self.xi, -1)\n     D = losses - self.min_losses + 1e-8\n     c = 1 / (z / D).sum().detach()\n     loss = (c * D.log() * z).sum()\n     return loss\ndef update(self, prev_losses, curr_losses):\n     # prev_losses        (num_tasks,)\n     # curr_losses        (num_tasks,)\n     delta = (prev_losses - self.min_losses + 1e-8).log() -\n                (curr_losses - self.min_losses + 1e-8).log()\n     with torch.enable_grad():\n           d = torch.autograd.grad(F.softmax(self.xi, -1),\n                                            self.xi,\n                                            grad_outputs=delta.detach())[0]\n     self.xi_opt.zero_grad()\n     self.xi.grad = d\n     self.xi_opt.step\nE    Experimental Results with Error Bars\nWe followed the exact experimental setup from NASHMTL [32]. Therefore, the numbers for baseline\nmethods are taken from their original paper. In the following, we provide FAMO\u2019s result with error\nbars.\n                                                  17", "md": "Algorithm 2 Implementation of FAMO in PyTorch-like Pseudocode\n\nclass FAMO:\n\ndef __init__(self, num_tasks, min_losses, \u03b1=0.025, \u03b3=0.001):\n\n# min_losses       (num_tasks,) the loss lower bound for each task.\n\nself.min_losses = min_losses\n\nself.xi = torch.tensor([0.0] * num_tasks, requires_grad=True)\n\nself.xi_opt = torch.optim.Adam([self.xi], lr=\u03b1, weight_decay=\u03b3)\n\ndef get_weighted_loss(self, losses):\n\n# losses      (num_tasks,)\n\n$$z = F.softmax(self.xi, -1)$$\n\n$$D = losses - self.min_losses + 1e-8$$\n\n$$c = 1 / (z / D).sum().detach()$$\n\n$$loss = (c * D.log() * z).sum()$$\n\nreturn loss\n\ndef update(self, prev_losses, curr_losses):\n\n# prev_losses        (num_tasks,)\n\n# curr_losses        (num_tasks,)\n\n$$\\delta = (prev_losses - self.min_losses + 1e-8).log() - (curr_losses - self.min_losses + 1e-8).log()$$\n\n$$\\text{with torch.enable_grad():}$$\n\n$$d = torch.autograd.grad(F.softmax(self.xi, -1), self.xi, grad\\_outputs=\\delta.detach())[0]$$\n\nself.xi_opt.zero_grad()\n\nself.xi.grad = d\n\nself.xi_opt.step\n\nE    Experimental Results with Error Bars\n\nWe followed the exact experimental setup from NASHMTL [32]. Therefore, the numbers for baseline methods are taken from their original paper. In the following, we provide FAMO\u2019s result with error bars.\n\n17", "images": [], "items": [{"type": "text", "value": "Algorithm 2 Implementation of FAMO in PyTorch-like Pseudocode\n\nclass FAMO:\n\ndef __init__(self, num_tasks, min_losses, \u03b1=0.025, \u03b3=0.001):", "md": "Algorithm 2 Implementation of FAMO in PyTorch-like Pseudocode\n\nclass FAMO:\n\ndef __init__(self, num_tasks, min_losses, \u03b1=0.025, \u03b3=0.001):"}, {"type": "heading", "lvl": 1, "value": "min_losses       (num_tasks,) the loss lower bound for each task.", "md": "# min_losses       (num_tasks,) the loss lower bound for each task."}, {"type": "text", "value": "self.min_losses = min_losses\n\nself.xi = torch.tensor([0.0] * num_tasks, requires_grad=True)\n\nself.xi_opt = torch.optim.Adam([self.xi], lr=\u03b1, weight_decay=\u03b3)\n\ndef get_weighted_loss(self, losses):", "md": "self.min_losses = min_losses\n\nself.xi = torch.tensor([0.0] * num_tasks, requires_grad=True)\n\nself.xi_opt = torch.optim.Adam([self.xi], lr=\u03b1, weight_decay=\u03b3)\n\ndef get_weighted_loss(self, losses):"}, {"type": "heading", "lvl": 1, "value": "losses      (num_tasks,)", "md": "# losses      (num_tasks,)"}, {"type": "text", "value": "$$z = F.softmax(self.xi, -1)$$\n\n$$D = losses - self.min_losses + 1e-8$$\n\n$$c = 1 / (z / D).sum().detach()$$\n\n$$loss = (c * D.log() * z).sum()$$\n\nreturn loss\n\ndef update(self, prev_losses, curr_losses):", "md": "$$z = F.softmax(self.xi, -1)$$\n\n$$D = losses - self.min_losses + 1e-8$$\n\n$$c = 1 / (z / D).sum().detach()$$\n\n$$loss = (c * D.log() * z).sum()$$\n\nreturn loss\n\ndef update(self, prev_losses, curr_losses):"}, {"type": "heading", "lvl": 1, "value": "prev_losses        (num_tasks,)", "md": "# prev_losses        (num_tasks,)"}, {"type": "heading", "lvl": 1, "value": "curr_losses        (num_tasks,)", "md": "# curr_losses        (num_tasks,)"}, {"type": "text", "value": "$$\\delta = (prev_losses - self.min_losses + 1e-8).log() - (curr_losses - self.min_losses + 1e-8).log()$$\n\n$$\\text{with torch.enable_grad():}$$\n\n$$d = torch.autograd.grad(F.softmax(self.xi, -1), self.xi, grad\\_outputs=\\delta.detach())[0]$$\n\nself.xi_opt.zero_grad()\n\nself.xi.grad = d\n\nself.xi_opt.step\n\nE    Experimental Results with Error Bars\n\nWe followed the exact experimental setup from NASHMTL [32]. Therefore, the numbers for baseline methods are taken from their original paper. In the following, we provide FAMO\u2019s result with error bars.\n\n17", "md": "$$\\delta = (prev_losses - self.min_losses + 1e-8).log() - (curr_losses - self.min_losses + 1e-8).log()$$\n\n$$\\text{with torch.enable_grad():}$$\n\n$$d = torch.autograd.grad(F.softmax(self.xi, -1), self.xi, grad\\_outputs=\\delta.detach())[0]$$\n\nself.xi_opt.zero_grad()\n\nself.xi.grad = d\n\nself.xi_opt.step\n\nE    Experimental Results with Error Bars\n\nWe followed the exact experimental setup from NASHMTL [32]. Therefore, the numbers for baseline methods are taken from their original paper. In the following, we provide FAMO\u2019s result with error bars.\n\n17"}]}, {"page": 18, "text": "                         Segmentation                  Depth                          Surface Normal\n  Method                                                                  Angle Dist \u2193             Within t\u25cb  \u2191         \u2206m% \u2193\n                      mIoU \u2191     Pix Acc \u2191    Abs Err \u2193     Rel Err \u2193    Mean     Median     11.25     22.5        30\n  FAMO (mean)           38.88        64.90       0.5474       0.2194     25.06      19.57    29.21    56.61    68.98        -4.10\n  FAMO (stderr)         \u00b10.54         \u00b10.21      \u00b10.0016     \u00b10.0026     \u00b10.06      \u00b10.09    \u00b10.17    \u00b10.19     \u00b10.14       \u00b10.39\nTable 5: Results on NYU-v2 dataset (3 tasks). Each experiment is repeated over 3 random seeds and the mean is\nreported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n  Method                 \u00b5          \u03b1   \u03f5HOMO    \u03f5LUMO       \u27e8R2\u27e9MAE \u2193ZPVE       U0       U       H       G         cv    \u2206m% \u2193\n  FAMO (mean)          0.15      0.30     94.0     95.2      1.63      4.95   70.82     71.2    71.2    70.3      0.10        58.5\n  FAMO (stderr)     \u00b10.0046   \u00b10.0070   \u00b13.074   \u00b12.413   \u00b10.0211   \u00b10.0871    \u00b12.17   \u00b12.19   \u00b12.19   \u00b12.21   \u00b10.0026       \u00b13.26\nTable 6: Results on QM-9 dataset (11 tasks). Each experiment is repeated over 3 random seeds and the mean is\nreported. The best average result is marked in bold. MR and \u2206m% are the main metrics for MTL performance.\n                                                               CityScapes                             CelebA\n                     Method                Segmentation                Depth\n                                        mIoU \u2191    Pix Acc \u2191    Abs Err \u2193    Rel Err \u2193   \u2206m% \u2193        \u2206m% \u2193\n                     FAMO (mean)          74.54        93.29      0.0145       32.59         8.13        1.21\n                     FAMO (stderr)        \u00b10.11        \u00b10.04     \u00b10.0009        \u00b11.06       \u00b11.98       \u00b10.24\nTable 7: Results on CityScapes (2 tasks) and CelebA (40 tasks) datasets. Each experiment is repeated over 3\nrandom seeds and the mean is reported. The best average result is marked in bold. MR and \u2206m% are the main\nmetrics for MTL performance.\n                                                                 18", "md": "|Segmentation|Depth|Surface Normal|\n|---|---|---|\n|Method| | |\n| |mIoU \u2191|Pix Acc \u2191|Abs Err \u2193|Rel Err \u2193|Mean|Median|11.25|22.5|30|\n|FAMO (mean)|38.88|64.90|0.5474|0.2194|25.06|19.57|29.21|56.61|68.98|-4.10|\n|FAMO (stderr)|\u00b10.54|\u00b10.21|\u00b10.0016|\u00b10.0026|\u00b10.06|\u00b10.09|\u00b10.17|\u00b10.19|\u00b10.14|\u00b10.39|\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\mu & \\alpha & \\epsilon_{\\text{HOMO}} & \\epsilon_{\\text{LUMO}} & \\langle R^2 \\rangle \\text{MAE} & \\text{ZPVE} & U_0 & U & H & G & \\text{cv} & \\Delta m\\% \\downarrow \\\\\n\\hline\n\\text{FAMO (mean)} & 0.15 & 0.30 & 94.0 & 95.2 & 1.63 & 4.95 & 70.82 & 71.2 & 71.2 & 70.3 & 0.10 & 58.5 \\\\\n\\text{FAMO (stderr)} & \\pm0.0046 & \\pm0.0070 & \\pm3.074 & \\pm2.413 & \\pm0.0211 & \\pm0.0871 & \\pm2.17 & \\pm2.19 & \\pm2.19 & \\pm2.21 & \\pm0.0026 & \\pm3.26 \\\\\n\\hline\n\\end{array}\n$$\n\n|Method|Segmentation|Depth|\n|---|---|---|\n| |mIoU \u2191|Pix Acc \u2191|Abs Err \u2193|Rel Err \u2193|\u0394m% \u2193|\u0394m% \u2193|\n|FAMO (mean)|74.54|93.29|0.0145|32.59|8.13|1.21|\n|FAMO (stderr)|\u00b10.11|\u00b10.04|\u00b10.0009|\u00b11.06|\u00b11.98|\u00b10.24|", "images": [], "items": [{"type": "table", "rows": [["Segmentation", "Depth", "Surface Normal"], ["Method", "", ""], ["", "mIoU \u2191", "Pix Acc \u2191", "Abs Err \u2193", "Rel Err \u2193", "Mean", "Median", "11.25", "22.5", "30"], ["FAMO (mean)", "38.88", "64.90", "0.5474", "0.2194", "25.06", "19.57", "29.21", "56.61", "68.98", "-4.10"], ["FAMO (stderr)", "\u00b10.54", "\u00b10.21", "\u00b10.0016", "\u00b10.0026", "\u00b10.06", "\u00b10.09", "\u00b10.17", "\u00b10.19", "\u00b10.14", "\u00b10.39"]], "md": "|Segmentation|Depth|Surface Normal|\n|---|---|---|\n|Method| | |\n| |mIoU \u2191|Pix Acc \u2191|Abs Err \u2193|Rel Err \u2193|Mean|Median|11.25|22.5|30|\n|FAMO (mean)|38.88|64.90|0.5474|0.2194|25.06|19.57|29.21|56.61|68.98|-4.10|\n|FAMO (stderr)|\u00b10.54|\u00b10.21|\u00b10.0016|\u00b10.0026|\u00b10.06|\u00b10.09|\u00b10.17|\u00b10.19|\u00b10.14|\u00b10.39|", "isPerfectTable": false, "csv": "\"Segmentation\",\"Depth\",\"Surface Normal\"\n\"Method\",\"\",\"\"\n\"\",\"mIoU \u2191\",\"Pix Acc \u2191\",\"Abs Err \u2193\",\"Rel Err \u2193\",\"Mean\",\"Median\",\"11.25\",\"22.5\",\"30\"\n\"FAMO (mean)\",\"38.88\",\"64.90\",\"0.5474\",\"0.2194\",\"25.06\",\"19.57\",\"29.21\",\"56.61\",\"68.98\",\"-4.10\"\n\"FAMO (stderr)\",\"\u00b10.54\",\"\u00b10.21\",\"\u00b10.0016\",\"\u00b10.0026\",\"\u00b10.06\",\"\u00b10.09\",\"\u00b10.17\",\"\u00b10.19\",\"\u00b10.14\",\"\u00b10.39\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\mu & \\alpha & \\epsilon_{\\text{HOMO}} & \\epsilon_{\\text{LUMO}} & \\langle R^2 \\rangle \\text{MAE} & \\text{ZPVE} & U_0 & U & H & G & \\text{cv} & \\Delta m\\% \\downarrow \\\\\n\\hline\n\\text{FAMO (mean)} & 0.15 & 0.30 & 94.0 & 95.2 & 1.63 & 4.95 & 70.82 & 71.2 & 71.2 & 70.3 & 0.10 & 58.5 \\\\\n\\text{FAMO (stderr)} & \\pm0.0046 & \\pm0.0070 & \\pm3.074 & \\pm2.413 & \\pm0.0211 & \\pm0.0871 & \\pm2.17 & \\pm2.19 & \\pm2.19 & \\pm2.21 & \\pm0.0026 & \\pm3.26 \\\\\n\\hline\n\\end{array}\n$$", "md": "$$\n\\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\mu & \\alpha & \\epsilon_{\\text{HOMO}} & \\epsilon_{\\text{LUMO}} & \\langle R^2 \\rangle \\text{MAE} & \\text{ZPVE} & U_0 & U & H & G & \\text{cv} & \\Delta m\\% \\downarrow \\\\\n\\hline\n\\text{FAMO (mean)} & 0.15 & 0.30 & 94.0 & 95.2 & 1.63 & 4.95 & 70.82 & 71.2 & 71.2 & 70.3 & 0.10 & 58.5 \\\\\n\\text{FAMO (stderr)} & \\pm0.0046 & \\pm0.0070 & \\pm3.074 & \\pm2.413 & \\pm0.0211 & \\pm0.0871 & \\pm2.17 & \\pm2.19 & \\pm2.19 & \\pm2.21 & \\pm0.0026 & \\pm3.26 \\\\\n\\hline\n\\end{array}\n$$"}, {"type": "table", "rows": [["Method", "Segmentation", "Depth"], ["", "mIoU \u2191", "Pix Acc \u2191", "Abs Err \u2193", "Rel Err \u2193", "\u0394m% \u2193", "\u0394m% \u2193"], ["FAMO (mean)", "74.54", "93.29", "0.0145", "32.59", "8.13", "1.21"], ["FAMO (stderr)", "\u00b10.11", "\u00b10.04", "\u00b10.0009", "\u00b11.06", "\u00b11.98", "\u00b10.24"]], "md": "|Method|Segmentation|Depth|\n|---|---|---|\n| |mIoU \u2191|Pix Acc \u2191|Abs Err \u2193|Rel Err \u2193|\u0394m% \u2193|\u0394m% \u2193|\n|FAMO (mean)|74.54|93.29|0.0145|32.59|8.13|1.21|\n|FAMO (stderr)|\u00b10.11|\u00b10.04|\u00b10.0009|\u00b11.06|\u00b11.98|\u00b10.24|", "isPerfectTable": false, "csv": "\"Method\",\"Segmentation\",\"Depth\"\n\"\",\"mIoU \u2191\",\"Pix Acc \u2191\",\"Abs Err \u2193\",\"Rel Err \u2193\",\"\u0394m% \u2193\",\"\u0394m% \u2193\"\n\"FAMO (mean)\",\"74.54\",\"93.29\",\"0.0145\",\"32.59\",\"8.13\",\"1.21\"\n\"FAMO (stderr)\",\"\u00b10.11\",\"\u00b10.04\",\"\u00b10.0009\",\"\u00b11.06\",\"\u00b11.98\",\"\u00b10.24\""}]}], "job_id": "24ea0a90-4044-4e65-ba7a-ffd38d639f0c", "file_path": "./corpus/2306.03792.pdf"}