{"pages": [{"page": 1, "text": "                              AdANNS: A Framework for Adaptive Semantic Search\n                                      Aniket Rege\u2217\u2020              Aditya Kusupati\u2217\u2020\u22c4                  Sharan Ranjit S\u2020               Alan Fan\u2020  Qingqing Cao\u2020,\n                                                                      Sham Kakade\u2021                 Prateek Jain\u22c4             Ali Farhadi\u2020\n                                                       \u2020University of Washington, \u22c4Google Research, \u2021Harvard University\n                                                        {kusupati,ali}@cs.washington.edu, prajain@google.com\narXiv:2305.19435v2  [cs.LG]  18 Oct 2023                                                           Abstract\n                                            Web-scale search systems learn an encoder to embed a given query which is then\n                                            hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve\n                                            similar data points. To accurately capture tail queries and data points, learned\n                                            representations typically are rigid, high-dimensional vectors that are generally\n                                            used as-is in the entire ANNS pipeline and can lead to computationally expensive\n                                            retrieval. In this paper, we argue that instead of rigid representations, different\n                                            stages of ANNS can leverage adaptive representations of varying capacities to\n                                            achieve signifi       cantly better accuracy-compute trade-offs, i.e., stages of ANNS that\n                                            can get away with more approximate computation should use a lower-capacity\n                                            representation of the same data point. To this end, we introduce AdANNS                                            ,\n                                            a novel ANNS design framework that explicitly leverages the flexibility of Ma-\n                                            tryoshka Representations [31]. We demonstrate state-of-the-art accuracy-compute\n                                            trade-offs using novel AdANNS-based key ANNS building blocks like search\n                                            data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For exam-\n                                            ple on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the\n                                            rigid representations-based IVF [48] at the same compute budget; and matches\n                                            accuracy while being up to 90\u00d7 faster in wall-clock time. For Natural Questions,\n                                            32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline [13]\n                                            constructed using rigid representations \u2013 same accuracy at half the cost! We further\n                                            show that the gains from AdANNS translate to modern-day composite ANNS\n                                            indices that combine search structures and quantization. Finally, we demonstrate\n                                            that AdANNS can enable inference-time adaptivity for compute-aware search\n                                            on ANNS indices built non-adaptively on matryoshka representations. Code is\n                                            open-sourced at https://github.com/RAIVNLab/AdANNS.\n                              1      Introduction\n                              Semantic search [24] on learned representations [40, 41, 50] is a major component in retrieval\n                              pipelines [4, 9]. In its simplest form, semantic search methods learn a neural network to embed\n                              queries as well as a large number (N) of data points in a d-dimensional vector space. For a given query,\n                              the nearest (in embedding space) point is retrieved using either an exact search or using approximate\n                              nearest neighbor search (ANNS) [21] which is now indispensable for real-time large-scale retrieval.\n                              Existing semantic search methods learn fixed or rigid representations (RRs) which are used as is\n                              in all the stages of ANNS (data structures for data pruning and quantization for cheaper distance\n                              computation; see Section 2). That is, while ANNS indices allow a variety of parameters for searching\n                              the design space to optimize the accuracy-compute trade-off, the provided data dimensionality is\n                              typically assumed to be an immutable parameter. To make it concrete, let us consider inverted file\n                              index (IVF) [48], a popular web-scale ANNS technique [16]. IVF has two stages (Section 3) during\n                              inference: (a) cluster mapping: mapping the query to a cluster of data points [36], and (b) linear\n                                   \u2217Equal contribution.\n                              37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# AdANNS: A Framework for Adaptive Semantic Search\n\n# AdANNS: A Framework for Adaptive Semantic Search\n\nAniket Rege*\u2020, Aditya Kusupati*\u2020\u22c4, Sharan Ranjit S\u2020, Alan Fan\u2020, Qingqing Cao\u2020, Sham Kakade\u2021, Prateek Jain\u22c4, Ali Farhadi\u2020\n\n*University of Washington, \u22c4Google Research, \u2021Harvard University\n\n{kusupati,ali}@cs.washington.edu, prajain@google.com\n\narXiv:2305.19435v2 [cs.LG] 18 Oct 2023\n\n## Abstract\n\nWeb-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations [31]. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the rigid representations-based IVF [48] at the same compute budget; and matches accuracy while being up to 90\u00d7 faster in wall-clock time. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline [13] constructed using rigid representations \u2013 same accuracy at half the cost! We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on Matryoshka representations. Code is open-sourced at https://github.com/RAIVNLab/AdANNS.\n\n## 1 Introduction\n\nSemantic search [24] on learned representations [40, 41, 50] is a major component in retrieval pipelines [4, 9]. In its simplest form, semantic search methods learn a neural network to embed queries as well as a large number (N) of data points in a d-dimensional vector space. For a given query, the nearest (in embedding space) point is retrieved using either an exact search or using approximate nearest neighbor search (ANNS) [21] which is now indispensable for real-time large-scale retrieval. Existing semantic search methods learn fixed or rigid representations (RRs) which are used as is in all the stages of ANNS (data structures for data pruning and quantization for cheaper distance computation; see Section 2). That is, while ANNS indices allow a variety of parameters for searching the design space to optimize the accuracy-compute trade-off, the provided data dimensionality is typically assumed to be an immutable parameter. To make it concrete, let us consider inverted file index (IVF) [48], a popular web-scale ANNS technique [16]. IVF has two stages (Section 3) during inference: (a) cluster mapping: mapping the query to a cluster of data points [36], and (b) linear\n\n*Equal contribution.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "AdANNS: A Framework for Adaptive Semantic Search", "md": "# AdANNS: A Framework for Adaptive Semantic Search"}, {"type": "heading", "lvl": 1, "value": "AdANNS: A Framework for Adaptive Semantic Search", "md": "# AdANNS: A Framework for Adaptive Semantic Search"}, {"type": "text", "value": "Aniket Rege*\u2020, Aditya Kusupati*\u2020\u22c4, Sharan Ranjit S\u2020, Alan Fan\u2020, Qingqing Cao\u2020, Sham Kakade\u2021, Prateek Jain\u22c4, Ali Farhadi\u2020\n\n*University of Washington, \u22c4Google Research, \u2021Harvard University\n\n{kusupati,ali}@cs.washington.edu, prajain@google.com\n\narXiv:2305.19435v2 [cs.LG] 18 Oct 2023", "md": "Aniket Rege*\u2020, Aditya Kusupati*\u2020\u22c4, Sharan Ranjit S\u2020, Alan Fan\u2020, Qingqing Cao\u2020, Sham Kakade\u2021, Prateek Jain\u22c4, Ali Farhadi\u2020\n\n*University of Washington, \u22c4Google Research, \u2021Harvard University\n\n{kusupati,ali}@cs.washington.edu, prajain@google.com\n\narXiv:2305.19435v2 [cs.LG] 18 Oct 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations [31]. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the rigid representations-based IVF [48] at the same compute budget; and matches accuracy while being up to 90\u00d7 faster in wall-clock time. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline [13] constructed using rigid representations \u2013 same accuracy at half the cost! We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on Matryoshka representations. Code is open-sourced at https://github.com/RAIVNLab/AdANNS.", "md": "Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations [31]. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the rigid representations-based IVF [48] at the same compute budget; and matches accuracy while being up to 90\u00d7 faster in wall-clock time. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline [13] constructed using rigid representations \u2013 same accuracy at half the cost! We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on Matryoshka representations. Code is open-sourced at https://github.com/RAIVNLab/AdANNS."}, {"type": "heading", "lvl": 2, "value": "1 Introduction", "md": "## 1 Introduction"}, {"type": "text", "value": "Semantic search [24] on learned representations [40, 41, 50] is a major component in retrieval pipelines [4, 9]. In its simplest form, semantic search methods learn a neural network to embed queries as well as a large number (N) of data points in a d-dimensional vector space. For a given query, the nearest (in embedding space) point is retrieved using either an exact search or using approximate nearest neighbor search (ANNS) [21] which is now indispensable for real-time large-scale retrieval. Existing semantic search methods learn fixed or rigid representations (RRs) which are used as is in all the stages of ANNS (data structures for data pruning and quantization for cheaper distance computation; see Section 2). That is, while ANNS indices allow a variety of parameters for searching the design space to optimize the accuracy-compute trade-off, the provided data dimensionality is typically assumed to be an immutable parameter. To make it concrete, let us consider inverted file index (IVF) [48], a popular web-scale ANNS technique [16]. IVF has two stages (Section 3) during inference: (a) cluster mapping: mapping the query to a cluster of data points [36], and (b) linear\n\n*Equal contribution.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "Semantic search [24] on learned representations [40, 41, 50] is a major component in retrieval pipelines [4, 9]. In its simplest form, semantic search methods learn a neural network to embed queries as well as a large number (N) of data points in a d-dimensional vector space. For a given query, the nearest (in embedding space) point is retrieved using either an exact search or using approximate nearest neighbor search (ANNS) [21] which is now indispensable for real-time large-scale retrieval. Existing semantic search methods learn fixed or rigid representations (RRs) which are used as is in all the stages of ANNS (data structures for data pruning and quantization for cheaper distance computation; see Section 2). That is, while ANNS indices allow a variety of parameters for searching the design space to optimize the accuracy-compute trade-off, the provided data dimensionality is typically assumed to be an immutable parameter. To make it concrete, let us consider inverted file index (IVF) [48], a popular web-scale ANNS technique [16]. IVF has two stages (Section 3) during inference: (a) cluster mapping: mapping the query to a cluster of data points [36], and (b) linear\n\n*Equal contribution.\n\n37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "   70.5\n  Top-1 Accuracy (%)                                       Top-1 Accuracy (%)\n   70.0          ~90\u00d7 real-world speed-up                   46                                ~3% gain\n   69.5     ~1.5% gain                                      44             2\u00d7 cheaper\n                                                            42\n   69.0                               AdANNS-IVF                                           AdANNS-OPQ\n                                      Rigid-IVF             40                             Rigid-OPQ\n       0.05                0.5   1                  10          32            48            64           96\n              Search Latency/Query (ms)                                Compute Budget (Bytes)\n         (a) Image retrieval on ImageNet-1K.                  (b) Passage retrieval on Natural Questions.\nFigure 1: AdANNS helps design search data structures and quantization methods with better\naccuracy-compute trade-offs than the existing solutions. In particular, (a) AdANNS-IVF improves on\nstandard IVF by up to 1.5% in accuracy while being 90\u00d7 faster in deployment and (b) AdANNS-OPQ\nis as accurate as the baseline at half the cost! Rigid-IVF and Rigid-OPQ are standard techniques that\nare built on rigid representations (RRs) while AdANNS uses matryoshka representations (MRs) [31].\nscan: distance computation w.r.t all points in the retrieved cluster to find the nearest neighbor (NN).\nStandard IVF utilizes the same high-dimensional RR for both phases, which can be sub-optimal.\nWhy the sub-optimality? Imagine one needs to partition a dataset into k clusters for IVF and the\ndimensionality of the data is d \u2013 IVF uses full d representation to partition into k clusters. However,\nsuppose we have an alternate approach that somehow projects the data in d/2 dimensions and learns\n2k clusters. Note that the storage and computation to find the nearest cluster remains the same in both\ncases, i.e., when we have k clusters of d dimensions or 2k clusters of d/2 dimensions. 2k clusters\ncan provide significantly more refined partitioning, but the distances computed between queries and\nclusters could be significantly more inaccurate after projection to d/2 dimensions.\nSo, if we can find a mechanism to obtain a d/2-dimensional representation of points that can accurately\napproximate the topology/distances of d-dimensional representation, then we can potentially build\nsignificantly better ANNS structure that utilizes different capacity representations for the cluster\nmapping and linear scan phases of IVF. But how do we find such adaptive representations? These\ndesired adaptive representations should be cheap to obtain and still ensure distance preservation\nacross dimensionality. Post-hoc dimensionality reduction techniques like SVD [14] and random\nprojections [25] on high-dimensional RRs are potential candidates, but our experiments indicate that\nin practice they are highly inaccurate and do not preserve distances well enough (Figure 2).\nInstead, we identify that the recently proposed Matryoshka Representations (MRs) [31] satisfy\nthe specifications for adaptive representations. Matryoshka representations pack information in a\nhierarchical nested manner, i.e., the first m-dimensions of the d-dimensional MR form an accurate\nlow-dimensional representation while being aware of the information in the higher dimensions.\nThis allows us to deploy MRs in two major and novel ways as part of ANNS: (a) low-dimensional\nrepresentations for accuracy-compute optimal clustering and quantization, and (b) high-dimensional\nrepresentations for precise re-ranking when feasible.\nTo this effort, we introduce AdANNS            , a novel design framework for semantic search that uses\nmatryoshka representation-based adaptive representations across different stages of ANNS to ensure\nsignificantly better accuracy-compute trade-off than the state-of-the-art baselines.\nTypical ANNS systems have two key components: (a) search data structure to store datapoints, (b)\ndistance computation to map a given query to points in the data structure. Through AdANNS, we\naddress both these components and significantly improve their performance. In particular, we first\npropose AdANNS-IVF (Section 4.1) which tackles the first component of ANNS systems. AdANNS-\nIVF uses standard full-precision computations but uses adaptive representations for different IVF\nstages. On ImageNet 1-NN image retrieval (Figure 1a), AdANNS-IVF is up to 1.5% more accurate\nfor the compute budget and 90\u00d7 cheaper in deployment for the same accuracy as IVF.\n                                                       2", "md": "# AdANNS Design Framework\n\n## AdANNS Design Framework\n\nAdaptive Approximate Nearest Neighbor Search (AdANNS) is a novel design framework for semantic search that utilizes matryoshka representation-based adaptive representations to achieve better accuracy-compute trade-offs compared to existing solutions.\n\n### Components of ANNS Systems\n\nANNS systems typically consist of two key components:\n\n1. Search data structure to store datapoints\n2. Distance computation to map a given query to points in the data structure\n\nThrough AdANNS, both these components are addressed and their performance is significantly improved.\n\n### AdANNS-IVF\n\nAdANNS-IVF focuses on the first component of ANNS systems. It utilizes adaptive representations for different stages of Inverted File (IVF) search. On ImageNet 1-NN image retrieval, AdANNS-IVF achieves up to 1.5% higher accuracy for the same compute budget and is 90 times cheaper in deployment compared to IVF.\n\n### Mathematical Insights\n\nAdANNS leverages Matryoshka Representations (MRs) which pack information in a hierarchical nested manner. This allows for accurate low-dimensional representations while preserving information from higher dimensions.\n\n### Trade-offs and Benefits\n\nBy using adaptive representations, AdANNS ensures better accuracy-compute trade-offs than traditional ANNS structures. It offers improved clustering, quantization, and re-ranking capabilities for semantic search tasks.\n\n### Experimental Results\n\nExperimental results indicate that post-hoc dimensionality reduction techniques like SVD and random projections are inaccurate for ANNS tasks. In contrast, MRs provide adaptive representations that meet the requirements for efficient and accurate search operations.\n\n### Conclusion\n\nAdANNS presents a promising approach to enhancing the performance of ANNS systems through the use of matryoshka representation-based adaptive representations.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "AdANNS Design Framework", "md": "# AdANNS Design Framework"}, {"type": "heading", "lvl": 2, "value": "AdANNS Design Framework", "md": "## AdANNS Design Framework"}, {"type": "text", "value": "Adaptive Approximate Nearest Neighbor Search (AdANNS) is a novel design framework for semantic search that utilizes matryoshka representation-based adaptive representations to achieve better accuracy-compute trade-offs compared to existing solutions.", "md": "Adaptive Approximate Nearest Neighbor Search (AdANNS) is a novel design framework for semantic search that utilizes matryoshka representation-based adaptive representations to achieve better accuracy-compute trade-offs compared to existing solutions."}, {"type": "heading", "lvl": 3, "value": "Components of ANNS Systems", "md": "### Components of ANNS Systems"}, {"type": "text", "value": "ANNS systems typically consist of two key components:\n\n1. Search data structure to store datapoints\n2. Distance computation to map a given query to points in the data structure\n\nThrough AdANNS, both these components are addressed and their performance is significantly improved.", "md": "ANNS systems typically consist of two key components:\n\n1. Search data structure to store datapoints\n2. Distance computation to map a given query to points in the data structure\n\nThrough AdANNS, both these components are addressed and their performance is significantly improved."}, {"type": "heading", "lvl": 3, "value": "AdANNS-IVF", "md": "### AdANNS-IVF"}, {"type": "text", "value": "AdANNS-IVF focuses on the first component of ANNS systems. It utilizes adaptive representations for different stages of Inverted File (IVF) search. On ImageNet 1-NN image retrieval, AdANNS-IVF achieves up to 1.5% higher accuracy for the same compute budget and is 90 times cheaper in deployment compared to IVF.", "md": "AdANNS-IVF focuses on the first component of ANNS systems. It utilizes adaptive representations for different stages of Inverted File (IVF) search. On ImageNet 1-NN image retrieval, AdANNS-IVF achieves up to 1.5% higher accuracy for the same compute budget and is 90 times cheaper in deployment compared to IVF."}, {"type": "heading", "lvl": 3, "value": "Mathematical Insights", "md": "### Mathematical Insights"}, {"type": "text", "value": "AdANNS leverages Matryoshka Representations (MRs) which pack information in a hierarchical nested manner. This allows for accurate low-dimensional representations while preserving information from higher dimensions.", "md": "AdANNS leverages Matryoshka Representations (MRs) which pack information in a hierarchical nested manner. This allows for accurate low-dimensional representations while preserving information from higher dimensions."}, {"type": "heading", "lvl": 3, "value": "Trade-offs and Benefits", "md": "### Trade-offs and Benefits"}, {"type": "text", "value": "By using adaptive representations, AdANNS ensures better accuracy-compute trade-offs than traditional ANNS structures. It offers improved clustering, quantization, and re-ranking capabilities for semantic search tasks.", "md": "By using adaptive representations, AdANNS ensures better accuracy-compute trade-offs than traditional ANNS structures. It offers improved clustering, quantization, and re-ranking capabilities for semantic search tasks."}, {"type": "heading", "lvl": 3, "value": "Experimental Results", "md": "### Experimental Results"}, {"type": "text", "value": "Experimental results indicate that post-hoc dimensionality reduction techniques like SVD and random projections are inaccurate for ANNS tasks. In contrast, MRs provide adaptive representations that meet the requirements for efficient and accurate search operations.", "md": "Experimental results indicate that post-hoc dimensionality reduction techniques like SVD and random projections are inaccurate for ANNS tasks. In contrast, MRs provide adaptive representations that meet the requirements for efficient and accurate search operations."}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "AdANNS presents a promising approach to enhancing the performance of ANNS systems through the use of matryoshka representation-based adaptive representations.", "md": "AdANNS presents a promising approach to enhancing the performance of ANNS systems through the use of matryoshka representation-based adaptive representations."}]}, {"page": 3, "text": "We then propose AdANNS-OPQ (Section 4.2) which addresses the second component by using\nAdANNS-based quantization (OPQ [13]) \u2013 here we use exhaustive search overall points. AdANNS-\n OPQ is as accurate as the baseline OPQ on RRs while being at least 2\u00d7 faster on Natural Ques-\n tions [32] 1-NN passage retrieval (Figure 1b). Finally, we combine the two techniques to obtain\nAdANNS-IVFOPQ (Section 4.3) which is more accurate while being much cheaper \u2013 up to 8\u00d7\n\u2013 than the traditional IVFOPQ [24] index. To demonstrate generality of our technique, we adapt\nAdANNS to DiskANN [22] which provides interesting accuracy-compute tradeoff; see Table 1.\nWhile MR already has multi-granular representations, careful integration with ANNS building blocks\n is critical to obtain a practical method and is our main contribution. In fact, Kusupati et al. [31]\n proposed a simple adaptive retrieval setup that uses smaller-dimensional MR for shortlisting in re-\n trieval followed by precise re-ranking with a higher-dimensional MR. Such techniques, unfortunately,\n cannot be scaled to industrial systems as they require forming a new index for every shortlisting\n provided by low-dimensional MR. Ensuring that the method aligns well with the modern-day\nANNS pipelines is important as they already have mechanisms to handle real-world constraints like\n load-balancing [16] and random access from disk [22]. So, AdANNS is a step towards making the\n abstraction of adaptive search and retrieval feasible at the web-scale.\n Through extensive experimentation, we also show that AdANNS generalizes across search data\n structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers)\nwhile still translating the theoretical gains to latency reductions in deployment. While we have mainly\n focused on IVF and OPQ-based ANNS in this work, AdANNS also blends well with other ANNS\n pipelines. We also show that AdANNS can enable compute-aware elastic search on prebuilt indices\nwithout making any modifications (Section 5.1); note that this is in contrast to AdANNS-IVF that\n builds the index explicitly utilizing \u201cadaptivity\u201d in representations. Finally, we provide an extensive\n analysis on the alignment of matryoshka representation for better semantic search (Section 5.2).\nWe make the following key contributions:\n \u2022 We introduce AdANNS          , a novel framework for semantic search that leverages matryoshka\n   representations for designing ANNS systems with better accuracy-compute trade-offs.\n \u2022 AdANNS powered search data structure (AdANNS-IVF) and quantization (AdANNS-OPQ) show\n   a significant improvement in accuracy-compute tradeoff compared to existing solutions.\n \u2022 AdANNS generalizes to modern-day composite ANNS indices and can also enable compute-aware\n   elastic search during inference with no modifications.\n 2   Related Work\nApproximate nearest neighbour search (ANNS) is a paradigm to come as close as possible [7] to\n retrieving the \u201ctrue\u201d nearest neighbor (NN) without the exorbitant search costs associated with\n exhaustive search [21, 52]. The \u201capproximate\u201d nature comes from data pruning as well as the cheaper\n distance computation that enable real-time web-scale search. In its naive form, NN-search has a\n complexity of O(dN); d is the data dimensionality used for distance computation and N is the size\n of the database. ANNS employs each of these approximations to reduce the linear dependence on the\n dimensionality (cheaper distance computation) and data points visited during search (data pruning).\n Cheaper distance computation. From a bird\u2019s eye view, cheaper distance computation is always\n obtained through dimensionality reduction (quantization included). PCA and SVD [14, 26] can\n reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy.\n On the other hand, quantization-based techniques [6, 15] like (optimized) product quantization\n ((O)PQ) [13, 23] have proved extremely crucial for relatively accurate yet cheap distance computation\n and simultaneously reduce the memory overhead significantly. Another naive solution is to indepen-\n dently train the representation function with varying low-dimensional information bottlenecks [31]\nwhich is rarely used due to the costs of maintaining multiple models and databases.\n Data pruning. Enabled by various data structures, data pruning reduces the number of data points\n visited as part of the search. This is often achieved through hashing [8, 46], trees [3, 12, 16, 48]\n and graphs [22, 38]. More recently there have been efforts towards end-to-end learning of the\n search data structures [17, 29, 30]. However, web-scale ANNS indices are often constructed on rigid\n d-dimensional real vectors using the aforementioned data structures that assist with the real-time\n search. For a more comprehensive review of ANNS structures please refer to [5, 34, 51].\n                                                   3", "md": "We then propose AdANNS-OPQ (Section 4.2) which addresses the second component by using AdANNS-based quantization (OPQ [13]) \u2013 here we use exhaustive search overall points. AdANNS-OPQ is as accurate as the baseline OPQ on RRs while being at least 2\u00d7 faster on Natural Questions [32] 1-NN passage retrieval (Figure 1b). Finally, we combine the two techniques to obtain AdANNS-IVFOPQ (Section 4.3) which is more accurate while being much cheaper \u2013 up to 8\u00d7 \u2013 than the traditional IVFOPQ [24] index. To demonstrate generality of our technique, we adapt AdANNS to DiskANN [22] which provides interesting accuracy-compute tradeoff; see Table 1.\nWhile MR already has multi-granular representations, careful integration with ANNS building blocks is critical to obtain a practical method and is our main contribution. In fact, Kusupati et al. [31] proposed a simple adaptive retrieval setup that uses smaller-dimensional MR for shortlisting in retrieval followed by precise re-ranking with a higher-dimensional MR. Such techniques, unfortunately, cannot be scaled to industrial systems as they require forming a new index for every shortlisting provided by low-dimensional MR. Ensuring that the method aligns well with the modern-day ANNS pipelines is important as they already have mechanisms to handle real-world constraints like load-balancing [16] and random access from disk [22]. So, AdANNS is a step towards making the abstraction of adaptive search and retrieval feasible at the web-scale.\nThrough extensive experimentation, we also show that AdANNS generalizes across search data structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers) while still translating the theoretical gains to latency reductions in deployment. While we have mainly focused on IVF and OPQ-based ANNS in this work, AdANNS also blends well with other ANNS pipelines. We also show that AdANNS can enable compute-aware elastic search on prebuilt indices without making any modifications (Section 5.1); note that this is in contrast to AdANNS-IVF that builds the index explicitly utilizing \u201cadaptivity\u201d in representations. Finally, we provide an extensive analysis on the alignment of matryoshka representation for better semantic search (Section 5.2).\n\nWe make the following key contributions:\n- We introduce AdANNS, a novel framework for semantic search that leverages matryoshka representations for designing ANNS systems with better accuracy-compute trade-offs.\n- AdANNS powered search data structure (AdANNS-IVF) and quantization (AdANNS-OPQ) show a significant improvement in accuracy-compute tradeoff compared to existing solutions.\n- AdANNS generalizes to modern-day composite ANNS indices and can also enable compute-aware elastic search during inference with no modifications.\n\n## Related Work\nApproximate nearest neighbour search (ANNS) is a paradigm to come as close as possible [7] to retrieving the \u201ctrue\u201d nearest neighbor (NN) without the exorbitant search costs associated with exhaustive search [21, 52]. The \u201capproximate\u201d nature comes from data pruning as well as the cheaper distance computation that enable real-time web-scale search. In its naive form, NN-search has a complexity of O(dN); d is the data dimensionality used for distance computation and N is the size of the database. ANNS employs each of these approximations to reduce the linear dependence on the dimensionality (cheaper distance computation) and data points visited during search (data pruning).\n\n**Cheaper distance computation.** From a bird\u2019s eye view, cheaper distance computation is always obtained through dimensionality reduction (quantization included). PCA and SVD [14, 26] can reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy. On the other hand, quantization-based techniques [6, 15] like (optimized) product quantization ((O)PQ) [13, 23] have proved extremely crucial for relatively accurate yet cheap distance computation and simultaneously reduce the memory overhead significantly. Another naive solution is to independently train the representation function with varying low-dimensional information bottlenecks [31] which is rarely used due to the costs of maintaining multiple models and databases.\n\n**Data pruning.** Enabled by various data structures, data pruning reduces the number of data points visited as part of the search. This is often achieved through hashing [8, 46], trees [3, 12, 16, 48] and graphs [22, 38]. More recently there have been efforts towards end-to-end learning of the search data structures [17, 29, 30]. However, web-scale ANNS indices are often constructed on rigid d-dimensional real vectors using the aforementioned data structures that assist with the real-time search. For a more comprehensive review of ANNS structures please refer to [5, 34, 51].", "images": [], "items": [{"type": "text", "value": "We then propose AdANNS-OPQ (Section 4.2) which addresses the second component by using AdANNS-based quantization (OPQ [13]) \u2013 here we use exhaustive search overall points. AdANNS-OPQ is as accurate as the baseline OPQ on RRs while being at least 2\u00d7 faster on Natural Questions [32] 1-NN passage retrieval (Figure 1b). Finally, we combine the two techniques to obtain AdANNS-IVFOPQ (Section 4.3) which is more accurate while being much cheaper \u2013 up to 8\u00d7 \u2013 than the traditional IVFOPQ [24] index. To demonstrate generality of our technique, we adapt AdANNS to DiskANN [22] which provides interesting accuracy-compute tradeoff; see Table 1.\nWhile MR already has multi-granular representations, careful integration with ANNS building blocks is critical to obtain a practical method and is our main contribution. In fact, Kusupati et al. [31] proposed a simple adaptive retrieval setup that uses smaller-dimensional MR for shortlisting in retrieval followed by precise re-ranking with a higher-dimensional MR. Such techniques, unfortunately, cannot be scaled to industrial systems as they require forming a new index for every shortlisting provided by low-dimensional MR. Ensuring that the method aligns well with the modern-day ANNS pipelines is important as they already have mechanisms to handle real-world constraints like load-balancing [16] and random access from disk [22]. So, AdANNS is a step towards making the abstraction of adaptive search and retrieval feasible at the web-scale.\nThrough extensive experimentation, we also show that AdANNS generalizes across search data structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers) while still translating the theoretical gains to latency reductions in deployment. While we have mainly focused on IVF and OPQ-based ANNS in this work, AdANNS also blends well with other ANNS pipelines. We also show that AdANNS can enable compute-aware elastic search on prebuilt indices without making any modifications (Section 5.1); note that this is in contrast to AdANNS-IVF that builds the index explicitly utilizing \u201cadaptivity\u201d in representations. Finally, we provide an extensive analysis on the alignment of matryoshka representation for better semantic search (Section 5.2).\n\nWe make the following key contributions:\n- We introduce AdANNS, a novel framework for semantic search that leverages matryoshka representations for designing ANNS systems with better accuracy-compute trade-offs.\n- AdANNS powered search data structure (AdANNS-IVF) and quantization (AdANNS-OPQ) show a significant improvement in accuracy-compute tradeoff compared to existing solutions.\n- AdANNS generalizes to modern-day composite ANNS indices and can also enable compute-aware elastic search during inference with no modifications.", "md": "We then propose AdANNS-OPQ (Section 4.2) which addresses the second component by using AdANNS-based quantization (OPQ [13]) \u2013 here we use exhaustive search overall points. AdANNS-OPQ is as accurate as the baseline OPQ on RRs while being at least 2\u00d7 faster on Natural Questions [32] 1-NN passage retrieval (Figure 1b). Finally, we combine the two techniques to obtain AdANNS-IVFOPQ (Section 4.3) which is more accurate while being much cheaper \u2013 up to 8\u00d7 \u2013 than the traditional IVFOPQ [24] index. To demonstrate generality of our technique, we adapt AdANNS to DiskANN [22] which provides interesting accuracy-compute tradeoff; see Table 1.\nWhile MR already has multi-granular representations, careful integration with ANNS building blocks is critical to obtain a practical method and is our main contribution. In fact, Kusupati et al. [31] proposed a simple adaptive retrieval setup that uses smaller-dimensional MR for shortlisting in retrieval followed by precise re-ranking with a higher-dimensional MR. Such techniques, unfortunately, cannot be scaled to industrial systems as they require forming a new index for every shortlisting provided by low-dimensional MR. Ensuring that the method aligns well with the modern-day ANNS pipelines is important as they already have mechanisms to handle real-world constraints like load-balancing [16] and random access from disk [22]. So, AdANNS is a step towards making the abstraction of adaptive search and retrieval feasible at the web-scale.\nThrough extensive experimentation, we also show that AdANNS generalizes across search data structures, distance approximations, modalities (text & image), and encoders (CNNs & Transformers) while still translating the theoretical gains to latency reductions in deployment. While we have mainly focused on IVF and OPQ-based ANNS in this work, AdANNS also blends well with other ANNS pipelines. We also show that AdANNS can enable compute-aware elastic search on prebuilt indices without making any modifications (Section 5.1); note that this is in contrast to AdANNS-IVF that builds the index explicitly utilizing \u201cadaptivity\u201d in representations. Finally, we provide an extensive analysis on the alignment of matryoshka representation for better semantic search (Section 5.2).\n\nWe make the following key contributions:\n- We introduce AdANNS, a novel framework for semantic search that leverages matryoshka representations for designing ANNS systems with better accuracy-compute trade-offs.\n- AdANNS powered search data structure (AdANNS-IVF) and quantization (AdANNS-OPQ) show a significant improvement in accuracy-compute tradeoff compared to existing solutions.\n- AdANNS generalizes to modern-day composite ANNS indices and can also enable compute-aware elastic search during inference with no modifications."}, {"type": "heading", "lvl": 2, "value": "Related Work", "md": "## Related Work"}, {"type": "text", "value": "Approximate nearest neighbour search (ANNS) is a paradigm to come as close as possible [7] to retrieving the \u201ctrue\u201d nearest neighbor (NN) without the exorbitant search costs associated with exhaustive search [21, 52]. The \u201capproximate\u201d nature comes from data pruning as well as the cheaper distance computation that enable real-time web-scale search. In its naive form, NN-search has a complexity of O(dN); d is the data dimensionality used for distance computation and N is the size of the database. ANNS employs each of these approximations to reduce the linear dependence on the dimensionality (cheaper distance computation) and data points visited during search (data pruning).\n\n**Cheaper distance computation.** From a bird\u2019s eye view, cheaper distance computation is always obtained through dimensionality reduction (quantization included). PCA and SVD [14, 26] can reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy. On the other hand, quantization-based techniques [6, 15] like (optimized) product quantization ((O)PQ) [13, 23] have proved extremely crucial for relatively accurate yet cheap distance computation and simultaneously reduce the memory overhead significantly. Another naive solution is to independently train the representation function with varying low-dimensional information bottlenecks [31] which is rarely used due to the costs of maintaining multiple models and databases.\n\n**Data pruning.** Enabled by various data structures, data pruning reduces the number of data points visited as part of the search. This is often achieved through hashing [8, 46], trees [3, 12, 16, 48] and graphs [22, 38]. More recently there have been efforts towards end-to-end learning of the search data structures [17, 29, 30]. However, web-scale ANNS indices are often constructed on rigid d-dimensional real vectors using the aforementioned data structures that assist with the real-time search. For a more comprehensive review of ANNS structures please refer to [5, 34, 51].", "md": "Approximate nearest neighbour search (ANNS) is a paradigm to come as close as possible [7] to retrieving the \u201ctrue\u201d nearest neighbor (NN) without the exorbitant search costs associated with exhaustive search [21, 52]. The \u201capproximate\u201d nature comes from data pruning as well as the cheaper distance computation that enable real-time web-scale search. In its naive form, NN-search has a complexity of O(dN); d is the data dimensionality used for distance computation and N is the size of the database. ANNS employs each of these approximations to reduce the linear dependence on the dimensionality (cheaper distance computation) and data points visited during search (data pruning).\n\n**Cheaper distance computation.** From a bird\u2019s eye view, cheaper distance computation is always obtained through dimensionality reduction (quantization included). PCA and SVD [14, 26] can reduce dimensionality and preserve distances only to a limited extent without sacrificing accuracy. On the other hand, quantization-based techniques [6, 15] like (optimized) product quantization ((O)PQ) [13, 23] have proved extremely crucial for relatively accurate yet cheap distance computation and simultaneously reduce the memory overhead significantly. Another naive solution is to independently train the representation function with varying low-dimensional information bottlenecks [31] which is rarely used due to the costs of maintaining multiple models and databases.\n\n**Data pruning.** Enabled by various data structures, data pruning reduces the number of data points visited as part of the search. This is often achieved through hashing [8, 46], trees [3, 12, 16, 48] and graphs [22, 38]. More recently there have been efforts towards end-to-end learning of the search data structures [17, 29, 30]. However, web-scale ANNS indices are often constructed on rigid d-dimensional real vectors using the aforementioned data structures that assist with the real-time search. For a more comprehensive review of ANNS structures please refer to [5, 34, 51]."}]}, {"page": 4, "text": "Composite indices. ANNS pipelines often benefit from the complementary nature of various building\nblocks [24, 42]. In practice, often the data structures (coarse-quantizer) like IVF [48] and HNSW [37]\nare combined with cheaper distance alternatives like PQ [23] (fine-quantizer) for massive speed-ups\nin web-scale search. While the data structures are built on d-dimensional real vectors, past works\nconsistently show that PQ can be safely used for distance computation during search time. As\nevident in modern web-scale ANNS systems like DiskANN [22], the data structures are built on\nd-dimensional real vectors but work with PQ vectors (32 \u2212        64-byte) for fast distance computations.\nANNS benchmark datasets. Despite the Herculean advances in representation learning [19, 42],\nANNS progress is often only benchmarked on fixed representation vectors provided for about a\ndozen million to billion scale datasets [1, 47] with limited access to the raw data. This resulted in\nthe improvement of algorithmic design for rigid representations (RRs) that are often not specifically\ndesigned for search. All the existing ANNS methods work with the assumption of using the provided\nd-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade-\noff in the first place. Note that the lack of raw-image and text-based benchmarks led us to using\nImageNet-1K [45] (1.3M images, 50K queries) and Natural Questions [32] (21M passages, 3.6K\nqueries) for experimentation. While not billion-scale, the results observed on ImageNet often translate\nto real-world progress [28], and Natural Questions is one of the largest question answering datasets\nbenchmarked for dense passage retrieval [27], making our results generalizable and widely applicable.\nIn this paper, we investigate the utility of adaptive representations \u2013 embeddings of different dimen-\nsionalities having similar semantic information \u2013 in improving the design of ANNS algorithms. This\nhelps in transitioning out of restricted construction and inference on rigid representations for ANNS.\nTo this end, we extensively use Matryoshka Representations (MRs) [31] which have desired adaptive\nproperties in-built. To the best of our knowledge, this is the first work that improves accuracy-compute\ntrade-off in ANNS by leveraging adaptive representations on different phases of construction and\ninference for ANNS data structures.\n3    Problem Setup, Notation, and Preliminaries\nThe problem setup of approximate nearest neighbor search (ANNS) [21] consists of a database of N\ndata points, [x1, x2, . . . , xN], and a query, q, where the goal is to \u201capproximately\u201d retrieve the nearest\ndata point to the query. Both the database and query are embedded to Rd using a representation\nfunction \u03d5 : X \u2192     Rd, often a neural network that can be learned through various representation\nlearning paradigms [2, 19, 20, 40, 42].\nMatryoshka Representations (MRs).           The d-dimensional representations from \u03d5 can have a nested\nstructure like Matryoshka Representations (MRs) [31] in-built \u2013 \u03d5MR(d). Matryoshka Representation\nLearning (MRL) learns these nested representations with a simple strategy of optimizing the same\ntraining objective at varying dimensionalities. These granularities are ordered such that the lowest\nrepresentation size forms a prefix for the higher-dimensional representations. So, high-dimensional\nMR inherently contains low-dimensional representations of varying granularities that can be accessed\nfor free \u2013 first m-dimensions (m \u2208       [d]) ie., \u03d5MR(d)[1 : m] from the d-dimensional MR form an\nm-dimensional representation which is as accurate as its independently trained rigid representation\n(RR) counterpart \u2013 \u03d5RR(m). Training an encoder with MRL does not involve any overhead or\nhyperparameter tuning and works seamlessly across modalities, training objectives, and architectures.\nInverted File Index (IVF).        IVF [48] is an ANNS data structure used in web-scale search sys-\ntems [16] owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction\ninvolves clustering (coarse quantization through k-means) [36] on d-dimensional representation that\nresults in an inverted file list [53] of all the data points in each cluster. During search, d-dimensional\nquery representation is assigned to the most relevant cluster (Ci; i \u2208     [k]) by finding the closest cen-\ntroid (\u00b5i) using an appropriate distance metric (L2 or cosine). This is followed by an exhaustive linear\nsearch across all data points in the cluster which gives the closest NN (see Figure 5 in Appendix A\nfor IVF overview). Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within\neach cluster [16]. Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.\nOptimized Product Quantization (OPQ).              Product Quantization (PQ) [23] works by splitting a\nd-dimensional real vector into m sub-vectors and quantizing each sub-vector with an independent 2b\n                                                     4", "md": "Composite indices. ANNS pipelines often benefit from the complementary nature of various building blocks [24, 42]. In practice, often the data structures (coarse-quantizer) like IVF [48] and HNSW [37] are combined with cheaper distance alternatives like PQ [23] (fine-quantizer) for massive speed-ups in web-scale search. While the data structures are built on d-dimensional real vectors, past works consistently show that PQ can be safely used for distance computation during search time. As evident in modern web-scale ANNS systems like DiskANN [22], the data structures are built on d-dimensional real vectors but work with PQ vectors (32 \u2212 64-byte) for fast distance computations.\n\nANNS benchmark datasets. Despite the Herculean advances in representation learning [19, 42], ANNS progress is often only benchmarked on fixed representation vectors provided for about a dozen million to billion scale datasets [1, 47] with limited access to the raw data. This resulted in the improvement of algorithmic design for rigid representations (RRs) that are often not specifically designed for search. All the existing ANNS methods work with the assumption of using the provided d-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade-off in the first place. Note that the lack of raw-image and text-based benchmarks led us to using ImageNet-1K [45] (1.3M images, 50K queries) and Natural Questions [32] (21M passages, 3.6K queries) for experimentation. While not billion-scale, the results observed on ImageNet often translate to real-world progress [28], and Natural Questions is one of the largest question answering datasets benchmarked for dense passage retrieval [27], making our results generalizable and widely applicable.\n\nIn this paper, we investigate the utility of adaptive representations \u2013 embeddings of different dimensionalities having similar semantic information \u2013 in improving the design of ANNS algorithms. This helps in transitioning out of restricted construction and inference on rigid representations for ANNS. To this end, we extensively use Matryoshka Representations (MRs) [31] which have desired adaptive properties in-built. To the best of our knowledge, this is the first work that improves accuracy-compute trade-off in ANNS by leveraging adaptive representations on different phases of construction and inference for ANNS data structures.\n\n### Problem Setup, Notation, and Preliminaries\n\nThe problem setup of approximate nearest neighbor search (ANNS) [21] consists of a database of N data points, [x1, x2, . . . , xN], and a query, q, where the goal is to \u201capproximately\u201d retrieve the nearest data point to the query. Both the database and query are embedded to Rd using a representation function \u03d5 : X \u2192 Rd, often a neural network that can be learned through various representation learning paradigms [2, 19, 20, 40, 42].\n\nMatryoshka Representations (MRs). The d-dimensional representations from \u03d5 can have a nested structure like Matryoshka Representations (MRs) [31] in-built \u2013 \u03d5MR(d). Matryoshka Representation Learning (MRL) learns these nested representations with a simple strategy of optimizing the same training objective at varying dimensionalities. These granularities are ordered such that the lowest representation size forms a prefix for the higher-dimensional representations. So, high-dimensional MR inherently contains low-dimensional representations of varying granularities that can be accessed for free \u2013 first m-dimensions (m \u2208 [d]) ie., \u03d5MR(d)[1 : m] from the d-dimensional MR form an m-dimensional representation which is as accurate as its independently trained rigid representation (RR) counterpart \u2013 \u03d5RR(m). Training an encoder with MRL does not involve any overhead or hyperparameter tuning and works seamlessly across modalities, training objectives, and architectures.\n\nInverted File Index (IVF). IVF [48] is an ANNS data structure used in web-scale search systems [16] owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction involves clustering (coarse quantization through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, d-dimensional query representation is assigned to the most relevant cluster (Ci; i \u2208 [k]) by finding the closest centroid (\u00b5i) using an appropriate distance metric (L2 or cosine). This is followed by an exhaustive linear search across all data points in the cluster which gives the closest NN. Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within each cluster. Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.\n\nOptimized Product Quantization (OPQ). Product Quantization (PQ) [23] works by splitting a d-dimensional real vector into m sub-vectors and quantizing each sub-vector with an independent 2b", "images": [], "items": [{"type": "text", "value": "Composite indices. ANNS pipelines often benefit from the complementary nature of various building blocks [24, 42]. In practice, often the data structures (coarse-quantizer) like IVF [48] and HNSW [37] are combined with cheaper distance alternatives like PQ [23] (fine-quantizer) for massive speed-ups in web-scale search. While the data structures are built on d-dimensional real vectors, past works consistently show that PQ can be safely used for distance computation during search time. As evident in modern web-scale ANNS systems like DiskANN [22], the data structures are built on d-dimensional real vectors but work with PQ vectors (32 \u2212 64-byte) for fast distance computations.\n\nANNS benchmark datasets. Despite the Herculean advances in representation learning [19, 42], ANNS progress is often only benchmarked on fixed representation vectors provided for about a dozen million to billion scale datasets [1, 47] with limited access to the raw data. This resulted in the improvement of algorithmic design for rigid representations (RRs) that are often not specifically designed for search. All the existing ANNS methods work with the assumption of using the provided d-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade-off in the first place. Note that the lack of raw-image and text-based benchmarks led us to using ImageNet-1K [45] (1.3M images, 50K queries) and Natural Questions [32] (21M passages, 3.6K queries) for experimentation. While not billion-scale, the results observed on ImageNet often translate to real-world progress [28], and Natural Questions is one of the largest question answering datasets benchmarked for dense passage retrieval [27], making our results generalizable and widely applicable.\n\nIn this paper, we investigate the utility of adaptive representations \u2013 embeddings of different dimensionalities having similar semantic information \u2013 in improving the design of ANNS algorithms. This helps in transitioning out of restricted construction and inference on rigid representations for ANNS. To this end, we extensively use Matryoshka Representations (MRs) [31] which have desired adaptive properties in-built. To the best of our knowledge, this is the first work that improves accuracy-compute trade-off in ANNS by leveraging adaptive representations on different phases of construction and inference for ANNS data structures.", "md": "Composite indices. ANNS pipelines often benefit from the complementary nature of various building blocks [24, 42]. In practice, often the data structures (coarse-quantizer) like IVF [48] and HNSW [37] are combined with cheaper distance alternatives like PQ [23] (fine-quantizer) for massive speed-ups in web-scale search. While the data structures are built on d-dimensional real vectors, past works consistently show that PQ can be safely used for distance computation during search time. As evident in modern web-scale ANNS systems like DiskANN [22], the data structures are built on d-dimensional real vectors but work with PQ vectors (32 \u2212 64-byte) for fast distance computations.\n\nANNS benchmark datasets. Despite the Herculean advances in representation learning [19, 42], ANNS progress is often only benchmarked on fixed representation vectors provided for about a dozen million to billion scale datasets [1, 47] with limited access to the raw data. This resulted in the improvement of algorithmic design for rigid representations (RRs) that are often not specifically designed for search. All the existing ANNS methods work with the assumption of using the provided d-dimensional representation which might not be Pareto-optimal for the accuracy-compute trade-off in the first place. Note that the lack of raw-image and text-based benchmarks led us to using ImageNet-1K [45] (1.3M images, 50K queries) and Natural Questions [32] (21M passages, 3.6K queries) for experimentation. While not billion-scale, the results observed on ImageNet often translate to real-world progress [28], and Natural Questions is one of the largest question answering datasets benchmarked for dense passage retrieval [27], making our results generalizable and widely applicable.\n\nIn this paper, we investigate the utility of adaptive representations \u2013 embeddings of different dimensionalities having similar semantic information \u2013 in improving the design of ANNS algorithms. This helps in transitioning out of restricted construction and inference on rigid representations for ANNS. To this end, we extensively use Matryoshka Representations (MRs) [31] which have desired adaptive properties in-built. To the best of our knowledge, this is the first work that improves accuracy-compute trade-off in ANNS by leveraging adaptive representations on different phases of construction and inference for ANNS data structures."}, {"type": "heading", "lvl": 3, "value": "Problem Setup, Notation, and Preliminaries", "md": "### Problem Setup, Notation, and Preliminaries"}, {"type": "text", "value": "The problem setup of approximate nearest neighbor search (ANNS) [21] consists of a database of N data points, [x1, x2, . . . , xN], and a query, q, where the goal is to \u201capproximately\u201d retrieve the nearest data point to the query. Both the database and query are embedded to Rd using a representation function \u03d5 : X \u2192 Rd, often a neural network that can be learned through various representation learning paradigms [2, 19, 20, 40, 42].\n\nMatryoshka Representations (MRs). The d-dimensional representations from \u03d5 can have a nested structure like Matryoshka Representations (MRs) [31] in-built \u2013 \u03d5MR(d). Matryoshka Representation Learning (MRL) learns these nested representations with a simple strategy of optimizing the same training objective at varying dimensionalities. These granularities are ordered such that the lowest representation size forms a prefix for the higher-dimensional representations. So, high-dimensional MR inherently contains low-dimensional representations of varying granularities that can be accessed for free \u2013 first m-dimensions (m \u2208 [d]) ie., \u03d5MR(d)[1 : m] from the d-dimensional MR form an m-dimensional representation which is as accurate as its independently trained rigid representation (RR) counterpart \u2013 \u03d5RR(m). Training an encoder with MRL does not involve any overhead or hyperparameter tuning and works seamlessly across modalities, training objectives, and architectures.\n\nInverted File Index (IVF). IVF [48] is an ANNS data structure used in web-scale search systems [16] owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction involves clustering (coarse quantization through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, d-dimensional query representation is assigned to the most relevant cluster (Ci; i \u2208 [k]) by finding the closest centroid (\u00b5i) using an appropriate distance metric (L2 or cosine). This is followed by an exhaustive linear search across all data points in the cluster which gives the closest NN. Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within each cluster. Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.\n\nOptimized Product Quantization (OPQ). Product Quantization (PQ) [23] works by splitting a d-dimensional real vector into m sub-vectors and quantizing each sub-vector with an independent 2b", "md": "The problem setup of approximate nearest neighbor search (ANNS) [21] consists of a database of N data points, [x1, x2, . . . , xN], and a query, q, where the goal is to \u201capproximately\u201d retrieve the nearest data point to the query. Both the database and query are embedded to Rd using a representation function \u03d5 : X \u2192 Rd, often a neural network that can be learned through various representation learning paradigms [2, 19, 20, 40, 42].\n\nMatryoshka Representations (MRs). The d-dimensional representations from \u03d5 can have a nested structure like Matryoshka Representations (MRs) [31] in-built \u2013 \u03d5MR(d). Matryoshka Representation Learning (MRL) learns these nested representations with a simple strategy of optimizing the same training objective at varying dimensionalities. These granularities are ordered such that the lowest representation size forms a prefix for the higher-dimensional representations. So, high-dimensional MR inherently contains low-dimensional representations of varying granularities that can be accessed for free \u2013 first m-dimensions (m \u2208 [d]) ie., \u03d5MR(d)[1 : m] from the d-dimensional MR form an m-dimensional representation which is as accurate as its independently trained rigid representation (RR) counterpart \u2013 \u03d5RR(m). Training an encoder with MRL does not involve any overhead or hyperparameter tuning and works seamlessly across modalities, training objectives, and architectures.\n\nInverted File Index (IVF). IVF [48] is an ANNS data structure used in web-scale search systems [16] owing to its simplicity, minimal compute overhead, and high accuracy. IVF construction involves clustering (coarse quantization through k-means) [36] on d-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, d-dimensional query representation is assigned to the most relevant cluster (Ci; i \u2208 [k]) by finding the closest centroid (\u00b5i) using an appropriate distance metric (L2 or cosine). This is followed by an exhaustive linear search across all data points in the cluster which gives the closest NN. Lastly, IVF can scale to web-scale by utilizing a hierarchical IVF structure within each cluster. Table 2 in Appendix A describes the retrieval formula for multiple variants of IVF.\n\nOptimized Product Quantization (OPQ). Product Quantization (PQ) [23] works by splitting a d-dimensional real vector into m sub-vectors and quantizing each sub-vector with an independent 2b"}]}, {"page": 5, "text": "length codebook across the database. After PQ, each d-dimensional vector can be represented by a\ncompact m \u00d7 b bit vector; we make each vector m bytes long by fixing b = 8. During search time,\ndistance computation between the query vector and PQ database is extremely efficient with only m\ncodebook lookups. The generality of PQ encompasses scalar/vector quantization [15, 36] as special\ncases. However, PQ can be further improved by rotating the d-dimensional space appropriately to\nmaximize distance preservation after PQ. Optimized Product Quantization (OPQ) [13] achieves this\nby learning an orthonormal projection matrix R that rotates the d-dimensional space to be more\namenable to PQ. OPQ shows consistent gains over PQ across a variety of ANNS tasks and has\nbecome the default choice in standard composite indices [22, 24].\nDatasets.     We evaluate the ANNS algorithms while changing the representations used for the search\nthus making it impossible to evaluate on the usual benchmarks [1]. Hence we experiment with two\npublic datasets: (a) ImageNet-1K [45] dataset on the task of image retrieval \u2013 where the goal is to\nretrieve images from a database (1.3M image train set) belonging to the same class as the query\nimage (50K image validation set) and (b) Natural Questions (NQ) [32] dataset on the task of question\nanswering through dense passage retrieval \u2013 where the goal is to retrieve the relevant passage from a\ndatabase (21M Wikipedia passages) for a query (3.6K questions).\nMetrics     Performance of ANNS is often measured using recall score [22], k-recall@N \u2013 recall of\nthe exact NN across search complexities which denotes the recall of k \u201ctrue\u201d NN when N data points\nare retrieved. However, the presence of labels allows us to compute 1-NN (top-1) accuracy. Top-1\naccuracy is a harder and more fine-grained metric that correlates well with typical retrieval metrics\nlike recall and mean average precision (mAP@k). Even though we report top-1 accuracy by default\nduring experimentation, we discuss other metrics in Appendix C. Finally, we measure the compute\noverhead of ANNS using MFLOPS/query and also provide wall-clock times (see Appendix B.1).\nEncoders.      For ImageNet, we encode both the database and query set using a ResNet50 (\u03d5I) [19]\ntrained on ImageNet-1K. For NQ, we encode both the passages in the database and the questions in\nthe query set using a BERT-Base (\u03d5N) [10] model fine-tuned on NQ for dense passage retrieval [27].\nWe use the trained ResNet50 models with varying representation sizes (d = [8, 16, . . . , 2048]; default\nbeing 2048) as suggested by Kusupati et al. [31] alongside the MRL-ResNet50 models trained with\nMRL for the same dimensionalities. The RR and MR models are trained to ensure the supervised\none-vs-all classification accuracy across all data dimensionalities is nearly the same \u2013 1-NN accuracy\nof 2048-d RR and MR models are 71.19% and 70.97% respectively on ImageNet-1K. Independently\ntrained models, \u03d5RR(d), output d = [8, 16 . . . , 2048] dimensional RRs while a single MRL-ResNet50\n                   I\nmodel, \u03d5MR(d)    , outputs a d = 2048-dimensional MR that contains all the 9 granularities.\n          I\nWe also train BERT-Base models in a similar vein as the aforementioned ResNet50 models. The\nkey difference is that we take a pre-trained BERT-Base model and fine-tune on NQ as suggested\nby Karpukhin et al. [27] with varying (5) representation sizes (bottlenecks) (d = [48, 96, . . . , 768];\ndefault being 768) to obtain \u03d5RR(d)N      that creates RRs for the NQ dataset. To get the MRL-BERT-\nBase model, we fine-tune a pre-trained BERT-Base encoder on the NQ train dataset using the MRL\nobjective with the same granularities as RRs to obtain \u03d5MR(d)   N      which contains all fi  ve granularities.\nAkin to ResNet50 models, the RR and MR BERT-Base models on NQ are built to have similar 1-NN\naccuracy for 768-d of 52.2% and 51.5% respectively. More implementation details can be found in\nAppendix B and additional experiment-specific information is provided at the appropriate places.\n4    AdANNS \u2013 Adaptive ANNS\nIn this section, we present our proposed AdANNS              framework that exploits the inherent flexibility\nof matryoshka representations to improve the accuracy-compute trade-off for semantic search com-\nponents. Standard ANNS pipeline can be split into two key components: (a) search data structure\nthat indexes and stores data points, (b) query-point computation method that outputs (approximate)\ndistance between a given query and data point. For example, standard IVFOPQ [24] method uses\nan IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient\ndistance computation between the query and the data points during the linear scan.\n                                                        5", "md": "# AdANNS Framework\n\n## AdANNS \u2013 Adaptive ANNS\n\nIn this section, we present our proposed AdANNS framework that exploits the inherent flexibility of matryoshka representations to improve the accuracy-compute trade-off for semantic search components. Standard ANNS pipeline can be split into two key components: (a) search data structure that indexes and stores data points, (b) query-point computation method that outputs (approximate) distance between a given query and data point. For example, standard IVFOPQ method uses an IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient distance computation between the query and the data points during the linear scan.\n\n### 5", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "AdANNS Framework", "md": "# AdANNS Framework"}, {"type": "heading", "lvl": 2, "value": "AdANNS \u2013 Adaptive ANNS", "md": "## AdANNS \u2013 Adaptive ANNS"}, {"type": "text", "value": "In this section, we present our proposed AdANNS framework that exploits the inherent flexibility of matryoshka representations to improve the accuracy-compute trade-off for semantic search components. Standard ANNS pipeline can be split into two key components: (a) search data structure that indexes and stores data points, (b) query-point computation method that outputs (approximate) distance between a given query and data point. For example, standard IVFOPQ method uses an IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient distance computation between the query and the data points during the linear scan.", "md": "In this section, we present our proposed AdANNS framework that exploits the inherent flexibility of matryoshka representations to improve the accuracy-compute trade-off for semantic search components. Standard ANNS pipeline can be split into two key components: (a) search data structure that indexes and stores data points, (b) query-point computation method that outputs (approximate) distance between a given query and data point. For example, standard IVFOPQ method uses an IVF structure to index points on full-precision vectors and then relies on OPQ for more efficient distance computation between the query and the data points during the linear scan."}, {"type": "heading", "lvl": 3, "value": "5", "md": "### 5"}]}, {"page": 6, "text": " Below, we show that AdANNS can be applied to both the above-mentioned ANNS components\n and provides significant gains on the computation-accuracy tradeoff curve. In particular, we present\n AdANNS-IVF which is AdANNS version of the standard IVF index structure [48], and the closely\n related ScaNN structure [16]. We also present AdANNS-OPQ which introduces representation adap-\n tivity in the OPQ, an industry-default quantization. Then, in Section 4.3 we further demonstrate the\n combination of the two techniques to get AdANNS-IVFOPQ \u2013 an AdANNS version of IVFOPQ [24]\n\u2013 and AdANNS-DiskANN, a similar variant of DiskANN [22]. Overall, our experiments show that\n AdANNS-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on\n RRs and AdANNS-OPQ is as accurate as the OPQ on RRs while being significantly cheaper.\n 4.1   AdANNS-IVF\n Recall from Section 1 that IVF has\n a clustering and a linear scan phase,          70.0\n where both phase use same dimen-             Top-1 Accuracy (%)\n sional rigid representation.       Now,                                                   AdANNS-IVF\n AdANNS-IVF allows the clustering               69.5                                       AdANNS-IVF-D\n phase to use the first dc dimensions                                                      MG-IVF-RR\n of the given matryoshka represen-              69.0                                       MG-IVF-SVD\n tation (MR).     Similarly, the linear                                                    IVF-MR\n scan within each cluster uses ds di-                                                      IVF-RR\n mensions, where again ds represents            68.5\n top ds coordinates from MR. Note\n that setting dc = ds results in non-\n adaptive regular IVF. Intuitively, we              0.1                   0.5        1                     5\n would set dc \u226a      ds, so that instead                               MFLOPS/Query\n of clustering with a high-dimensional\n representation, we can approximate it      Figure 2: 1-NN accuracy on ImageNet retrieval shows\n accurately with a low-dimensional em-      that AdANNS-IVF achieves near-optimal accuracy-compute\n bedding of size dc followed by a lin-      trade-off compared across various rigid and adaptive base-\n ear scan with a higher ds-dimensional      lines. Both adaptive variants of MR and RR significantly\n representation. Intuitively, this helps    outperform their rigid counterparts (IVF-XX) while post-hoc\n in the smooth search of design space       compression on RR using SVD for adaptivity falls short.\n for state-of-the-art accuracy-compute\n trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve\n which is critical in several practical settings.\n Our experiments on regular IVF with MRs and RRs (IVF-MR & IVF-RR) of varying dimensionali-\n ties and IVF configurations (# clusters, # probes) show that (Figure 2) matryoshka representations\n result in a significantly better accuracy-compute trade-off. We further studied and found that learned\n lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher-\n dimensional embeddings (see Appendix E for more results).\n AdANNS utilizes d-dimensional matryoshka representation to get accurate dc and ds dimensional\n vectors at no extra compute cost. The resulting AdANNS-IVF provides a much better accuracy-\n compute trade-off (Figure 2) on ImageNet-1K retrieval compared to IVF-MR, IVF-RR, and MG-\n IVF-RR \u2013 multi-granular IVF with rigid representations (akin to AdANNS without MR) \u2013 a strong\n baseline that uses dc and ds dimensional RRs. Finally, we exhaustively search the design space of\n IVF by varying dc, ds \u2208    [8, 16, . . . , 2048] and the number of clusters k \u2208  [8, 16, . . . , 2048]. Please\n see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G.\n Empirical results.      Figure 2 shows that AdANNS-IVF outperforms the baselines across all\n accuracy-compute settings for ImageNet-1K retrieval. AdANNS-IVF results in 10\u00d7 lower compute\n for the best accuracy of the extremely expensive MG-IVF-RR and non-adaptive IVF-MR. Specifi-\n cally, as shown in Figure 1a, AdANNS-IVF is up to 1.5% more accurate for the same compute and\n has up to 100\u00d7 lesser FLOPS/query (90\u00d7 real-world speed-up!) than the status quo ANNS on rigid\n representations (IVF-RR). We filter out points for the sake of presentation and encourage the reader\n to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched.\n The advantage of AdANNS for construction of search structures is evident from the improvements\n in IVF (AdANNS-IVF) and can be easily extended to other ANNS structures like ScaNN [16] and\n                                                      6", "md": "Below, we show that AdANNS can be applied to both the above-mentioned ANNS components\nand provides significant gains on the computation-accuracy tradeoff curve. In particular, we present\nAdANNS-IVF which is AdANNS version of the standard IVF index structure [48], and the closely\nrelated ScaNN structure [16]. We also present AdANNS-OPQ which introduces representation adaptivity in the OPQ, an industry-default quantization. Then, in Section 4.3 we further demonstrate the combination of the two techniques to get AdANNS-IVFOPQ \u2013 an AdANNS version of IVFOPQ [24] \u2013 and AdANNS-DiskANN, a similar variant of DiskANN [22]. Overall, our experiments show that AdANNS-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on RRs and AdANNS-OPQ is as accurate as the OPQ on RRs while being significantly cheaper.\n\n### AdANNS-IVF\nRecall from Section 1 that IVF has a clustering and a linear scan phase, where both phases use the same dimensional rigid representation. Now,\n\n| Top-1 Accuracy (%) |                   |\n|---------------------|-------------------|\n|        70.0         | AdANNS-IVF        |\n|        69.5         | AdANNS-IVF-D      |\n|        69.0         | MG-IVF-RR         |\n|        68.5         | IVF-RR            |\n\n$$\n\\begin{array}{cccc}\n0.1 & 0.5 & 1 & 5 \\\\\n\\end{array}\n$$\n\n**Figure 2:** 1-NN accuracy on ImageNet retrieval shows that AdANNS-IVF achieves near-optimal accuracy-compute trade-off compared across various rigid and adaptive baselines. Both adaptive variants of MR and RR significantly outperform their rigid counterparts (IVF-XX) while post-hoc compression on RR using SVD for adaptivity falls short.\n\nAdANNS-IVF allows the clustering phase to use the first dc dimensions of the given matryoshka representation (MR). Similarly, the linear scan within each cluster uses ds dimensions, where again ds represents top ds coordinates from MR. Note that setting dc = ds results in non-adaptive regular IVF. Intuitively, we would set dc \u226a ds, so that instead of clustering with a high-dimensional representation, we can approximate it accurately with a low-dimensional embedding of size dc followed by a linear scan with a higher ds-dimensional representation. Intuitively, this helps in the smooth search of design space for state-of-the-art accuracy-compute trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve which is critical in several practical settings.\n\nOur experiments on regular IVF with MRs and RRs (IVF-MR & IVF-RR) of varying dimensionalities and IVF configurations (# clusters, # probes) show that matryoshka representations result in a significantly better accuracy-compute trade-off. We further studied and found that learned lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher-dimensional embeddings (see Appendix E for more results).\n\nAdANNS utilizes d-dimensional matryoshka representation to get accurate dc and ds dimensional vectors at no extra compute cost. The resulting AdANNS-IVF provides a much better accuracy-compute trade-off on ImageNet-1K retrieval compared to IVF-MR, IVF-RR, and MG-IVF-RR \u2013 multi-granular IVF with rigid representations (akin to AdANNS without MR) \u2013 a strong baseline that uses dc and ds dimensional RRs. Finally, we exhaustively search the design space of IVF by varying dc, ds \u2208 [8, 16, ..., 2048] and the number of clusters k \u2208 [8, 16, ..., 2048]. Please see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G.\n\nEmpirical results. Figure 2 shows that AdANNS-IVF outperforms the baselines across all accuracy-compute settings for ImageNet-1K retrieval. AdANNS-IVF results in 10\u00d7 lower compute for the best accuracy of the extremely expensive MG-IVF-RR and non-adaptive IVF-MR. Specifically, as shown in Figure 1a, AdANNS-IVF is up to 1.5% more accurate for the same compute and has up to 100\u00d7 lesser FLOPS/query (90\u00d7 real-world speed-up!) than the status quo ANNS on rigid representations (IVF-RR). We filter out points for the sake of presentation and encourage the reader to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched.\n\nThe advantage of AdANNS for construction of search structures is evident from the improvements in IVF (AdANNS-IVF) and can be easily extended to other ANNS structures like ScaNN [16] and.", "images": [], "items": [{"type": "text", "value": "Below, we show that AdANNS can be applied to both the above-mentioned ANNS components\nand provides significant gains on the computation-accuracy tradeoff curve. In particular, we present\nAdANNS-IVF which is AdANNS version of the standard IVF index structure [48], and the closely\nrelated ScaNN structure [16]. We also present AdANNS-OPQ which introduces representation adaptivity in the OPQ, an industry-default quantization. Then, in Section 4.3 we further demonstrate the combination of the two techniques to get AdANNS-IVFOPQ \u2013 an AdANNS version of IVFOPQ [24] \u2013 and AdANNS-DiskANN, a similar variant of DiskANN [22]. Overall, our experiments show that AdANNS-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on RRs and AdANNS-OPQ is as accurate as the OPQ on RRs while being significantly cheaper.", "md": "Below, we show that AdANNS can be applied to both the above-mentioned ANNS components\nand provides significant gains on the computation-accuracy tradeoff curve. In particular, we present\nAdANNS-IVF which is AdANNS version of the standard IVF index structure [48], and the closely\nrelated ScaNN structure [16]. We also present AdANNS-OPQ which introduces representation adaptivity in the OPQ, an industry-default quantization. Then, in Section 4.3 we further demonstrate the combination of the two techniques to get AdANNS-IVFOPQ \u2013 an AdANNS version of IVFOPQ [24] \u2013 and AdANNS-DiskANN, a similar variant of DiskANN [22]. Overall, our experiments show that AdANNS-IVF is significantly more accuracy-compute optimal compared to the IVF indices built on RRs and AdANNS-OPQ is as accurate as the OPQ on RRs while being significantly cheaper."}, {"type": "heading", "lvl": 3, "value": "AdANNS-IVF", "md": "### AdANNS-IVF"}, {"type": "text", "value": "Recall from Section 1 that IVF has a clustering and a linear scan phase, where both phases use the same dimensional rigid representation. Now,", "md": "Recall from Section 1 that IVF has a clustering and a linear scan phase, where both phases use the same dimensional rigid representation. Now,"}, {"type": "table", "rows": [["Top-1 Accuracy (%)", ""], ["70.0", "AdANNS-IVF"], ["69.5", "AdANNS-IVF-D"], ["69.0", "MG-IVF-RR"], ["68.5", "IVF-RR"]], "md": "| Top-1 Accuracy (%) |                   |\n|---------------------|-------------------|\n|        70.0         | AdANNS-IVF        |\n|        69.5         | AdANNS-IVF-D      |\n|        69.0         | MG-IVF-RR         |\n|        68.5         | IVF-RR            |", "isPerfectTable": true, "csv": "\"Top-1 Accuracy (%)\",\"\"\n\"70.0\",\"AdANNS-IVF\"\n\"69.5\",\"AdANNS-IVF-D\"\n\"69.0\",\"MG-IVF-RR\"\n\"68.5\",\"IVF-RR\""}, {"type": "text", "value": "$$\n\\begin{array}{cccc}\n0.1 & 0.5 & 1 & 5 \\\\\n\\end{array}\n$$\n\n**Figure 2:** 1-NN accuracy on ImageNet retrieval shows that AdANNS-IVF achieves near-optimal accuracy-compute trade-off compared across various rigid and adaptive baselines. Both adaptive variants of MR and RR significantly outperform their rigid counterparts (IVF-XX) while post-hoc compression on RR using SVD for adaptivity falls short.\n\nAdANNS-IVF allows the clustering phase to use the first dc dimensions of the given matryoshka representation (MR). Similarly, the linear scan within each cluster uses ds dimensions, where again ds represents top ds coordinates from MR. Note that setting dc = ds results in non-adaptive regular IVF. Intuitively, we would set dc \u226a ds, so that instead of clustering with a high-dimensional representation, we can approximate it accurately with a low-dimensional embedding of size dc followed by a linear scan with a higher ds-dimensional representation. Intuitively, this helps in the smooth search of design space for state-of-the-art accuracy-compute trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve which is critical in several practical settings.\n\nOur experiments on regular IVF with MRs and RRs (IVF-MR & IVF-RR) of varying dimensionalities and IVF configurations (# clusters, # probes) show that matryoshka representations result in a significantly better accuracy-compute trade-off. We further studied and found that learned lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher-dimensional embeddings (see Appendix E for more results).\n\nAdANNS utilizes d-dimensional matryoshka representation to get accurate dc and ds dimensional vectors at no extra compute cost. The resulting AdANNS-IVF provides a much better accuracy-compute trade-off on ImageNet-1K retrieval compared to IVF-MR, IVF-RR, and MG-IVF-RR \u2013 multi-granular IVF with rigid representations (akin to AdANNS without MR) \u2013 a strong baseline that uses dc and ds dimensional RRs. Finally, we exhaustively search the design space of IVF by varying dc, ds \u2208 [8, 16, ..., 2048] and the number of clusters k \u2208 [8, 16, ..., 2048]. Please see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G.\n\nEmpirical results. Figure 2 shows that AdANNS-IVF outperforms the baselines across all accuracy-compute settings for ImageNet-1K retrieval. AdANNS-IVF results in 10\u00d7 lower compute for the best accuracy of the extremely expensive MG-IVF-RR and non-adaptive IVF-MR. Specifically, as shown in Figure 1a, AdANNS-IVF is up to 1.5% more accurate for the same compute and has up to 100\u00d7 lesser FLOPS/query (90\u00d7 real-world speed-up!) than the status quo ANNS on rigid representations (IVF-RR). We filter out points for the sake of presentation and encourage the reader to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched.\n\nThe advantage of AdANNS for construction of search structures is evident from the improvements in IVF (AdANNS-IVF) and can be easily extended to other ANNS structures like ScaNN [16] and.", "md": "$$\n\\begin{array}{cccc}\n0.1 & 0.5 & 1 & 5 \\\\\n\\end{array}\n$$\n\n**Figure 2:** 1-NN accuracy on ImageNet retrieval shows that AdANNS-IVF achieves near-optimal accuracy-compute trade-off compared across various rigid and adaptive baselines. Both adaptive variants of MR and RR significantly outperform their rigid counterparts (IVF-XX) while post-hoc compression on RR using SVD for adaptivity falls short.\n\nAdANNS-IVF allows the clustering phase to use the first dc dimensions of the given matryoshka representation (MR). Similarly, the linear scan within each cluster uses ds dimensions, where again ds represents top ds coordinates from MR. Note that setting dc = ds results in non-adaptive regular IVF. Intuitively, we would set dc \u226a ds, so that instead of clustering with a high-dimensional representation, we can approximate it accurately with a low-dimensional embedding of size dc followed by a linear scan with a higher ds-dimensional representation. Intuitively, this helps in the smooth search of design space for state-of-the-art accuracy-compute trade-off. Furthermore, this can provide a precise operating point on accuracy-compute tradeoff curve which is critical in several practical settings.\n\nOur experiments on regular IVF with MRs and RRs (IVF-MR & IVF-RR) of varying dimensionalities and IVF configurations (# clusters, # probes) show that matryoshka representations result in a significantly better accuracy-compute trade-off. We further studied and found that learned lower-dimensional representations offer better accuracy-compute trade-offs for IVF than higher-dimensional embeddings (see Appendix E for more results).\n\nAdANNS utilizes d-dimensional matryoshka representation to get accurate dc and ds dimensional vectors at no extra compute cost. The resulting AdANNS-IVF provides a much better accuracy-compute trade-off on ImageNet-1K retrieval compared to IVF-MR, IVF-RR, and MG-IVF-RR \u2013 multi-granular IVF with rigid representations (akin to AdANNS without MR) \u2013 a strong baseline that uses dc and ds dimensional RRs. Finally, we exhaustively search the design space of IVF by varying dc, ds \u2208 [8, 16, ..., 2048] and the number of clusters k \u2208 [8, 16, ..., 2048]. Please see Appendix E for more details. For IVF experiments on the NQ dataset, please refer to Appendix G.\n\nEmpirical results. Figure 2 shows that AdANNS-IVF outperforms the baselines across all accuracy-compute settings for ImageNet-1K retrieval. AdANNS-IVF results in 10\u00d7 lower compute for the best accuracy of the extremely expensive MG-IVF-RR and non-adaptive IVF-MR. Specifically, as shown in Figure 1a, AdANNS-IVF is up to 1.5% more accurate for the same compute and has up to 100\u00d7 lesser FLOPS/query (90\u00d7 real-world speed-up!) than the status quo ANNS on rigid representations (IVF-RR). We filter out points for the sake of presentation and encourage the reader to check out Figure 8 in Appendix E for an expansive plot of all the configurations searched.\n\nThe advantage of AdANNS for construction of search structures is evident from the improvements in IVF (AdANNS-IVF) and can be easily extended to other ANNS structures like ScaNN [16] and."}]}, {"page": 7, "text": " HNSW [38]. For example, HNSW consists of multiple layers with graphs of NSW graphs [37] of\n increasing complexity. AdANNS can be adopted to HNSW, where the construction of each level can\n be powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general,\nAdANNS provides fine-grained control over compute overhead (storage, working memory, inference,\n and construction cost) during construction and inference while providing the best possible accuracy.\n 4.2  AdANNS-OPQ\n Standard Product Quantization (PQ) essentially performs block-wise vector quantization via cluster-\n ing. For example, suppose we need 32-byte PQ compressed vectors from the given 2048 dimensional\n representations. Then, we can chunk the representations in m = 32 equal blocks/sub-vectors of 64-d\n each, and each sub-vector space is clustered into 28 = 256 partitions. That is, the representation of\n each point is essentially cluster-id for each block. Optimized PQ (OPQ) [13] further refines this idea,\n by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top\n of the rotated representations. In ANNS, OPQ is used extensively to compress vectors and improves\n approximate distance computation primarily due to significantly lower memory overhead than storing\n full-precision data points IVF.\nAdANNS-OPQ utilizes MR representations to apply OPQ on lower-dimensional representations.\nThat is, for a given quantization budget, AdANNS allows using top ds \u226a     d dimensions from MR and\n then computing clusters with ds/m-dimensional blocks where m is the number of blocks. Depending\n on ds and m, we have further flexibility of trading-off dimensionality/capacity for increasing the\n number of clusters to meet the given quantization budget. AdANNS-OPQ tries multiple ds, m, and\n number of clusters for a fixed quantization budget to obtain the best performing configuration.\nWe experimented with 8 \u2212    128 byte OPQ budgets for both ImageNet and Natural Questions retrieval\nwith an exhaustive search on the quantized vectors. We compare AdANNS-OPQ which uses MRs of\nvarying granularities to the baseline OPQ built on the highest dimensional RRs. We also evaluate\n OPQ vectors obtained projection using SVD [14] on top of the highest-dimensional RRs.\n Empirical results.    Figures 3 and 1b show that AdANNS-OPQ significantly outperforms \u2013 up to\n4% accuracy gain \u2013 the baselines (OPQ on RRs) across compute budgets on both ImageNet and NQ.\n In particular, AdANNS-OPQ tends to match the accuracy of a 64-byte (a typical choice in ANNS)\n OPQ baseline with only a 32-byte budget. This results in a 2\u00d7 reduction in both storage and compute\n FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D).\nWe only report the best AdANNS-OPQ for each budget typically obtained through a much lower-\n dimensional MR (128 & 192; much faster to build as well) than the highest-dimensional MR (2048\n& 768) for ImageNet and NQ respectively (see Appendix G for more details). At the same time, we\n     71                                                  70\n   Top-1 Accuracy (%)                                   Top-1 Accuracy (%)\n     70                                                  69\n                                                         68\n     69                                                  67\n     68                           AdANNS-OPQ             66\n                                  OPQ-RR                 65                        AdANNS-IVFOPQ\n     67                           OPQ-RR-SVD                                       Rigid-IVFOPQ\n        16                  32                 64            8           16           32           64\n               Compute Budget (Bytes)                              Compute Budget (Bytes)\n Figure 3: AdANNS-OPQ matches the accuracy           Figure 4: Combining the gains of AdANNS for\n of 64-byte OPQ on RR using only 32-bytes for        IVF and OPQ leads to better IVFOPQ compos-\n ImageNet retrieval. AdANNS provides large           ite indices. On ImageNet retrieval, AdANNS-\n gains at lower compute budgets and saturates to     IVFOPQ is 8\u00d7 cheaper for the same accuracy\n baseline performance for larger budgets.            and provides 1 - 4% gains over IVFOPQ on RRs.\n                                                   7", "md": "HNSW [38]. For example, HNSW consists of multiple layers with graphs of NSW graphs [37] of\nincreasing complexity. AdANNS can be adopted to HNSW, where the construction of each level can\nbe powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general,\nAdANNS provides fine-grained control over compute overhead (storage, working memory, inference,\nand construction cost) during construction and inference while providing the best possible accuracy.\n\n$$AdANNS-OPQ$$\n\nStandard Product Quantization (PQ) essentially performs block-wise vector quantization via clustering. For example, suppose we need 32-byte PQ compressed vectors from the given 2048 dimensional representations. Then, we can chunk the representations in $$m = 32$$ equal blocks/sub-vectors of 64-d each, and each sub-vector space is clustered into $$2^8 = 256$$ partitions. That is, the representation of each point is essentially cluster-id for each block. Optimized PQ (OPQ) [13] further refines this idea, by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top of the rotated representations. In ANNS, OPQ is used extensively to compress vectors and improves approximate distance computation primarily due to significantly lower memory overhead than storing full-precision data points IVF.\n\nAdANNS-OPQ utilizes MR representations to apply OPQ on lower-dimensional representations. That is, for a given quantization budget, AdANNS allows using top $$d_s \\ll d$$ dimensions from MR and then computing clusters with $$d_s/m$$-dimensional blocks where $$m$$ is the number of blocks. Depending on $$d_s$$ and $$m$$, we have further flexibility of trading-off dimensionality/capacity for increasing the number of clusters to meet the given quantization budget. AdANNS-OPQ tries multiple $$d_s$$, $$m$$, and number of clusters for a fixed quantization budget to obtain the best performing configuration.\n\nWe experimented with 8 - 128 byte OPQ budgets for both ImageNet and Natural Questions retrieval with an exhaustive search on the quantized vectors. We compare AdANNS-OPQ which uses MRs of varying granularities to the baseline OPQ built on the highest dimensional RRs. We also evaluate OPQ vectors obtained projection using SVD [14] on top of the highest-dimensional RRs.\n\nEmpirical results. Figures 3 and 1b show that AdANNS-OPQ significantly outperforms - up to 4% accuracy gain - the baselines (OPQ on RRs) across compute budgets on both ImageNet and NQ. In particular, AdANNS-OPQ tends to match the accuracy of a 64-byte (a typical choice in ANNS) OPQ baseline with only a 32-byte budget. This results in a 2\u00d7 reduction in both storage and compute FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D).\n\nWe only report the best AdANNS-OPQ for each budget typically obtained through a much lower-dimensional MR (128 & 192; much faster to build as well) than the highest-dimensional MR (2048 & 768) for ImageNet and NQ respectively (see Appendix G for more details). At the same time,\n\n|Top-1 Accuracy (%)|Top-1 Accuracy (%)|\n|---|---|\n|70|69|\n|69|67|\n|68|66|\n|AdANNS-OPQ|AdANNS-IVFOPQ|\n|OPQ-RR|Rigid-IVFOPQ|\n\nFigure 3: AdANNS-OPQ matches the accuracy of 64-byte OPQ on RR using only 32-bytes for ImageNet retrieval. AdANNS provides large gains at lower compute budgets and saturates to baseline performance for larger budgets.\n\nFigure 4: Combining the gains of AdANNS for IVF and OPQ leads to better IVFOPQ composite indices. On ImageNet retrieval, AdANNS-IVFOPQ is 8\u00d7 cheaper for the same accuracy and provides 1 - 4% gains over IVFOPQ on RRs.", "images": [], "items": [{"type": "text", "value": "HNSW [38]. For example, HNSW consists of multiple layers with graphs of NSW graphs [37] of\nincreasing complexity. AdANNS can be adopted to HNSW, where the construction of each level can\nbe powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general,\nAdANNS provides fine-grained control over compute overhead (storage, working memory, inference,\nand construction cost) during construction and inference while providing the best possible accuracy.\n\n$$AdANNS-OPQ$$\n\nStandard Product Quantization (PQ) essentially performs block-wise vector quantization via clustering. For example, suppose we need 32-byte PQ compressed vectors from the given 2048 dimensional representations. Then, we can chunk the representations in $$m = 32$$ equal blocks/sub-vectors of 64-d each, and each sub-vector space is clustered into $$2^8 = 256$$ partitions. That is, the representation of each point is essentially cluster-id for each block. Optimized PQ (OPQ) [13] further refines this idea, by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top of the rotated representations. In ANNS, OPQ is used extensively to compress vectors and improves approximate distance computation primarily due to significantly lower memory overhead than storing full-precision data points IVF.\n\nAdANNS-OPQ utilizes MR representations to apply OPQ on lower-dimensional representations. That is, for a given quantization budget, AdANNS allows using top $$d_s \\ll d$$ dimensions from MR and then computing clusters with $$d_s/m$$-dimensional blocks where $$m$$ is the number of blocks. Depending on $$d_s$$ and $$m$$, we have further flexibility of trading-off dimensionality/capacity for increasing the number of clusters to meet the given quantization budget. AdANNS-OPQ tries multiple $$d_s$$, $$m$$, and number of clusters for a fixed quantization budget to obtain the best performing configuration.\n\nWe experimented with 8 - 128 byte OPQ budgets for both ImageNet and Natural Questions retrieval with an exhaustive search on the quantized vectors. We compare AdANNS-OPQ which uses MRs of varying granularities to the baseline OPQ built on the highest dimensional RRs. We also evaluate OPQ vectors obtained projection using SVD [14] on top of the highest-dimensional RRs.\n\nEmpirical results. Figures 3 and 1b show that AdANNS-OPQ significantly outperforms - up to 4% accuracy gain - the baselines (OPQ on RRs) across compute budgets on both ImageNet and NQ. In particular, AdANNS-OPQ tends to match the accuracy of a 64-byte (a typical choice in ANNS) OPQ baseline with only a 32-byte budget. This results in a 2\u00d7 reduction in both storage and compute FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D).\n\nWe only report the best AdANNS-OPQ for each budget typically obtained through a much lower-dimensional MR (128 & 192; much faster to build as well) than the highest-dimensional MR (2048 & 768) for ImageNet and NQ respectively (see Appendix G for more details). At the same time,", "md": "HNSW [38]. For example, HNSW consists of multiple layers with graphs of NSW graphs [37] of\nincreasing complexity. AdANNS can be adopted to HNSW, where the construction of each level can\nbe powered by appropriate dimensionalities for an optimal accuracy-compute trade-off. In general,\nAdANNS provides fine-grained control over compute overhead (storage, working memory, inference,\nand construction cost) during construction and inference while providing the best possible accuracy.\n\n$$AdANNS-OPQ$$\n\nStandard Product Quantization (PQ) essentially performs block-wise vector quantization via clustering. For example, suppose we need 32-byte PQ compressed vectors from the given 2048 dimensional representations. Then, we can chunk the representations in $$m = 32$$ equal blocks/sub-vectors of 64-d each, and each sub-vector space is clustered into $$2^8 = 256$$ partitions. That is, the representation of each point is essentially cluster-id for each block. Optimized PQ (OPQ) [13] further refines this idea, by first rotating the representations using a learned orthogonal matrix, and then applying PQ on top of the rotated representations. In ANNS, OPQ is used extensively to compress vectors and improves approximate distance computation primarily due to significantly lower memory overhead than storing full-precision data points IVF.\n\nAdANNS-OPQ utilizes MR representations to apply OPQ on lower-dimensional representations. That is, for a given quantization budget, AdANNS allows using top $$d_s \\ll d$$ dimensions from MR and then computing clusters with $$d_s/m$$-dimensional blocks where $$m$$ is the number of blocks. Depending on $$d_s$$ and $$m$$, we have further flexibility of trading-off dimensionality/capacity for increasing the number of clusters to meet the given quantization budget. AdANNS-OPQ tries multiple $$d_s$$, $$m$$, and number of clusters for a fixed quantization budget to obtain the best performing configuration.\n\nWe experimented with 8 - 128 byte OPQ budgets for both ImageNet and Natural Questions retrieval with an exhaustive search on the quantized vectors. We compare AdANNS-OPQ which uses MRs of varying granularities to the baseline OPQ built on the highest dimensional RRs. We also evaluate OPQ vectors obtained projection using SVD [14] on top of the highest-dimensional RRs.\n\nEmpirical results. Figures 3 and 1b show that AdANNS-OPQ significantly outperforms - up to 4% accuracy gain - the baselines (OPQ on RRs) across compute budgets on both ImageNet and NQ. In particular, AdANNS-OPQ tends to match the accuracy of a 64-byte (a typical choice in ANNS) OPQ baseline with only a 32-byte budget. This results in a 2\u00d7 reduction in both storage and compute FLOPS which translates to significant gains in real-world web-scale deployment (see Appendix D).\n\nWe only report the best AdANNS-OPQ for each budget typically obtained through a much lower-dimensional MR (128 & 192; much faster to build as well) than the highest-dimensional MR (2048 & 768) for ImageNet and NQ respectively (see Appendix G for more details). At the same time,"}, {"type": "table", "rows": [["Top-1 Accuracy (%)", "Top-1 Accuracy (%)"], ["70", "69"], ["69", "67"], ["68", "66"], ["AdANNS-OPQ", "AdANNS-IVFOPQ"], ["OPQ-RR", "Rigid-IVFOPQ"]], "md": "|Top-1 Accuracy (%)|Top-1 Accuracy (%)|\n|---|---|\n|70|69|\n|69|67|\n|68|66|\n|AdANNS-OPQ|AdANNS-IVFOPQ|\n|OPQ-RR|Rigid-IVFOPQ|", "isPerfectTable": true, "csv": "\"Top-1 Accuracy (%)\",\"Top-1 Accuracy (%)\"\n\"70\",\"69\"\n\"69\",\"67\"\n\"68\",\"66\"\n\"AdANNS-OPQ\",\"AdANNS-IVFOPQ\"\n\"OPQ-RR\",\"Rigid-IVFOPQ\""}, {"type": "text", "value": "Figure 3: AdANNS-OPQ matches the accuracy of 64-byte OPQ on RR using only 32-bytes for ImageNet retrieval. AdANNS provides large gains at lower compute budgets and saturates to baseline performance for larger budgets.\n\nFigure 4: Combining the gains of AdANNS for IVF and OPQ leads to better IVFOPQ composite indices. On ImageNet retrieval, AdANNS-IVFOPQ is 8\u00d7 cheaper for the same accuracy and provides 1 - 4% gains over IVFOPQ on RRs.", "md": "Figure 3: AdANNS-OPQ matches the accuracy of 64-byte OPQ on RR using only 32-bytes for ImageNet retrieval. AdANNS provides large gains at lower compute budgets and saturates to baseline performance for larger budgets.\n\nFigure 4: Combining the gains of AdANNS for IVF and OPQ leads to better IVFOPQ composite indices. On ImageNet retrieval, AdANNS-IVFOPQ is 8\u00d7 cheaper for the same accuracy and provides 1 - 4% gains over IVFOPQ on RRs."}]}, {"page": 8, "text": " note that building compressed OPQ vectors on projected RRs using SVD to the smaller dimensions\n (or using low-dimensional RRs, see Appendix D) as the optimal AdANNS-OPQ does not help in\n improving the accuracy. The significant gains we observe in AdANNS-OPQ are purely due to better\n information packing in MRs \u2013 we hypothesize that packing the most important information in the\n initial coordinates results in a better PQ quantization than RRs where the information is uniformly\n distributed across all the dimensions [31, 49]. See Appendix D for more details and experiments.\n 4.3  AdANNS for Composite Indices\nWe now extend AdANNS to composite indices [24] which put together two main ANNS building\n blocks \u2013 search structures and quantization \u2013 together to obtain effi cient web-scale ANNS indices\n used in practice. A simple instantiation of a composite index would be the combination of IVF and\n OPQ \u2013 IVFOPQ \u2013 where the clustering in IVF happens with full-precision real vectors but the linear\n scan within each cluster is approximated using OPQ-compressed variants of the representation \u2013\n since often the full-precision vectors of the database cannot fit in RAM. Contemporary ANNS indices\n like DiskANN [22] make this a default choice where they build the search graph with a full-precision\n vector and approximate the distance computations during search with an OPQ-compressed vector to\n obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then\n re-ranked to form the final list using their full-precision vectors fetched from the disk. AdANNS is\n naturally suited to this shortlist-rerank framework: we use a low-d MR for forming index, where we\n could tune AdANNS parameters according to the accuracy-compute trade-off of the graph and OPQ\n vectors. We then use a high-d MR for re-ranking.\n Empirical results.    Figure 4 shows     Table 1: AdANNS-DiskANN using a 16-d MR + re-ranking\n that AdANNS-IVFOPQ is 1 \u2212          4%    with the 2048-d MR outperforms DiskANN built on 2048-d\n better than the baseline at all the      RR at half the compute cost on ImageNet retrieval.\n PQ compute budgets. Furthermore,\nAdANNS-IVFOPQ has the same ac-                                           RR-2048      AdANNS\n curacy as the baselines at 8\u00d7 lower            PQ Budget (Bytes)           32            16\n overhead. With DiskANN, AdANNS\n accelerates shortlist generation by us-        Top-1 Accuracy (%)         70.37        70.56\n ing low-dimensional representations            mAP@10 (%)                 62.46        64.70\n and recoups the accuracy by re-                Precision@40 (%)           65.65        68.25\n ranking with the highest-dimensional\n MR at negligible cost. Table 1 shows that AdANNS-DiskANN is more accurate than the baseline for\n both 1-NN and ranking performance at only half the cost. Using low-dimensional representations\n further speeds up inference in AdANNS-DiskANN (see Appendix F).\n These results show the generality of AdANNS and its broad applicability across a variety of ANNS\n indices built on top of the base building blocks. Currently, AdANNS piggybacks on typical ANNS\n pipelines for their inherent accounting of the real-world system constraints [16, 22, 25]. However,\nwe believe that AdANNS\u2019s flexibility and significantly better accuracy-compute trade-off can be\n further informed by real-world deployment constraints. We leave this high-potential line of work that\n requires extensive study to future research.\n 5   Further Analysis and Discussion\n 5.1  Compute-aware Elastic Search During Inference\nAdANNS search structures cater to many specific large-scale use scenarios that need to satisfy precise\n resource constraints during construction as well as inference. However, in many cases, construction\n and storage of the indices are not the bottlenecks or the user is unable to search the design space.\n In these settings, AdANNS-D enables adaptive inference through accurate yet cheaper distance\n computation using the low-dimensional prefix of matryoshka representation. Akin to composite\n indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low-\n dimensional MR for faster distance computation on ANNS structure built non-adaptively with a\n high-dimensional MR without any modifications to the existing index.\n Empirical results.    Figure 2 shows that for a given compute budget using IVF on ImageNet-1K\n retrieval, AdANNS-IVF is better than AdANNS-IVF-D due to the explicit control during the building\n                                                   8", "md": "note that building compressed OPQ vectors on projected RRs using SVD to the smaller dimensions (or using low-dimensional RRs, see Appendix D) as the optimal AdANNS-OPQ does not help in improving the accuracy. The significant gains we observe in AdANNS-OPQ are purely due to better information packing in MRs \u2013 we hypothesize that packing the most important information in the initial coordinates results in a better PQ quantization than RRs where the information is uniformly distributed across all the dimensions [31, 49]. See Appendix D for more details and experiments.\n\n#### AdANNS for Composite Indices\n\nWe now extend AdANNS to composite indices [24] which put together two main ANNS building blocks \u2013 search structures and quantization \u2013 together to obtain efficient web-scale ANNS indices used in practice. A simple instantiation of a composite index would be the combination of IVF and OPQ \u2013 IVFOPQ \u2013 where the clustering in IVF happens with full-precision real vectors but the linear scan within each cluster is approximated using OPQ-compressed variants of the representation \u2013 since often the full-precision vectors of the database cannot fit in RAM. Contemporary ANNS indices like DiskANN [22] make this a default choice where they build the search graph with a full-precision vector and approximate the distance computations during search with an OPQ-compressed vector to obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then re-ranked to form the final list using their full-precision vectors fetched from the disk. AdANNS is naturally suited to this shortlist-rerank framework: we use a low-d MR for forming index, where we could tune AdANNS parameters according to the accuracy-compute trade-off of the graph and OPQ vectors. We then use a high-d MR for re-ranking.\n\nEmpirical results. Figure 4 shows\n\n**Table 1: AdANNS-DiskANN using a 16-d MR + re-ranking**\n| |RR-2048|AdANNS|\n|---|---|---|\n|PQ Budget (Bytes)|32|16|\n|Top-1 Accuracy (%)|70.37|70.56|\n|mAP@10 (%)|62.46|64.70|\n|Precision@40 (%)|65.65|68.25|\n\nthat AdANNS-IVFOPQ is $$1 - 4\\%$$ with the 2048-d MR outperforms DiskANN built on 2048-d RR at half the compute cost on ImageNet retrieval.\n\nAdANNS-IVFOPQ has the same accuracy as the baselines at 8\u00d7 lower overhead. With DiskANN, AdANNS accelerates shortlist generation by using low-dimensional representations and recoups the accuracy by re-ranking with the highest-dimensional MR at negligible cost. Table 1 shows that AdANNS-DiskANN is more accurate than the baseline for both 1-NN and ranking performance at only half the cost. Using low-dimensional representations further speeds up inference in AdANNS-DiskANN (see Appendix F).\n\nThese results show the generality of AdANNS and its broad applicability across a variety of ANNS indices built on top of the base building blocks. Currently, AdANNS piggybacks on typical ANNS pipelines for their inherent accounting of the real-world system constraints [16, 22, 25]. However, we believe that AdANNS\u2019s flexibility and significantly better accuracy-compute trade-off can be further informed by real-world deployment constraints. We leave this high-potential line of work that requires extensive study to future research.\n\n#### Further Analysis and Discussion\n\n##### Compute-aware Elastic Search During Inference\n\nAdANNS search structures cater to many specific large-scale use scenarios that need to satisfy precise resource constraints during construction as well as inference. However, in many cases, construction and storage of the indices are not the bottlenecks or the user is unable to search the design space. In these settings, AdANNS-D enables adaptive inference through accurate yet cheaper distance computation using the low-dimensional prefix of matryoshka representation. Akin to composite indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low-dimensional MR for faster distance computation on ANNS structure built non-adaptively with a high-dimensional MR without any modifications to the existing index.\n\nEmpirical results. Figure 2 shows that for a given compute budget using IVF on ImageNet-1K retrieval, AdANNS-IVF is better than AdANNS-IVF-D due to the explicit control during the building", "images": [], "items": [{"type": "text", "value": "note that building compressed OPQ vectors on projected RRs using SVD to the smaller dimensions (or using low-dimensional RRs, see Appendix D) as the optimal AdANNS-OPQ does not help in improving the accuracy. The significant gains we observe in AdANNS-OPQ are purely due to better information packing in MRs \u2013 we hypothesize that packing the most important information in the initial coordinates results in a better PQ quantization than RRs where the information is uniformly distributed across all the dimensions [31, 49]. See Appendix D for more details and experiments.", "md": "note that building compressed OPQ vectors on projected RRs using SVD to the smaller dimensions (or using low-dimensional RRs, see Appendix D) as the optimal AdANNS-OPQ does not help in improving the accuracy. The significant gains we observe in AdANNS-OPQ are purely due to better information packing in MRs \u2013 we hypothesize that packing the most important information in the initial coordinates results in a better PQ quantization than RRs where the information is uniformly distributed across all the dimensions [31, 49]. See Appendix D for more details and experiments."}, {"type": "heading", "lvl": 4, "value": "AdANNS for Composite Indices", "md": "#### AdANNS for Composite Indices"}, {"type": "text", "value": "We now extend AdANNS to composite indices [24] which put together two main ANNS building blocks \u2013 search structures and quantization \u2013 together to obtain efficient web-scale ANNS indices used in practice. A simple instantiation of a composite index would be the combination of IVF and OPQ \u2013 IVFOPQ \u2013 where the clustering in IVF happens with full-precision real vectors but the linear scan within each cluster is approximated using OPQ-compressed variants of the representation \u2013 since often the full-precision vectors of the database cannot fit in RAM. Contemporary ANNS indices like DiskANN [22] make this a default choice where they build the search graph with a full-precision vector and approximate the distance computations during search with an OPQ-compressed vector to obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then re-ranked to form the final list using their full-precision vectors fetched from the disk. AdANNS is naturally suited to this shortlist-rerank framework: we use a low-d MR for forming index, where we could tune AdANNS parameters according to the accuracy-compute trade-off of the graph and OPQ vectors. We then use a high-d MR for re-ranking.\n\nEmpirical results. Figure 4 shows\n\n**Table 1: AdANNS-DiskANN using a 16-d MR + re-ranking**", "md": "We now extend AdANNS to composite indices [24] which put together two main ANNS building blocks \u2013 search structures and quantization \u2013 together to obtain efficient web-scale ANNS indices used in practice. A simple instantiation of a composite index would be the combination of IVF and OPQ \u2013 IVFOPQ \u2013 where the clustering in IVF happens with full-precision real vectors but the linear scan within each cluster is approximated using OPQ-compressed variants of the representation \u2013 since often the full-precision vectors of the database cannot fit in RAM. Contemporary ANNS indices like DiskANN [22] make this a default choice where they build the search graph with a full-precision vector and approximate the distance computations during search with an OPQ-compressed vector to obtain a very small shortlist of retrieved datapoints. In DiskANN, the shortlist of data points is then re-ranked to form the final list using their full-precision vectors fetched from the disk. AdANNS is naturally suited to this shortlist-rerank framework: we use a low-d MR for forming index, where we could tune AdANNS parameters according to the accuracy-compute trade-off of the graph and OPQ vectors. We then use a high-d MR for re-ranking.\n\nEmpirical results. Figure 4 shows\n\n**Table 1: AdANNS-DiskANN using a 16-d MR + re-ranking**"}, {"type": "table", "rows": [["", "RR-2048", "AdANNS"], ["PQ Budget (Bytes)", "32", "16"], ["Top-1 Accuracy (%)", "70.37", "70.56"], ["mAP@10 (%)", "62.46", "64.70"], ["Precision@40 (%)", "65.65", "68.25"]], "md": "| |RR-2048|AdANNS|\n|---|---|---|\n|PQ Budget (Bytes)|32|16|\n|Top-1 Accuracy (%)|70.37|70.56|\n|mAP@10 (%)|62.46|64.70|\n|Precision@40 (%)|65.65|68.25|", "isPerfectTable": true, "csv": "\"\",\"RR-2048\",\"AdANNS\"\n\"PQ Budget (Bytes)\",\"32\",\"16\"\n\"Top-1 Accuracy (%)\",\"70.37\",\"70.56\"\n\"mAP@10 (%)\",\"62.46\",\"64.70\"\n\"Precision@40 (%)\",\"65.65\",\"68.25\""}, {"type": "text", "value": "that AdANNS-IVFOPQ is $$1 - 4\\%$$ with the 2048-d MR outperforms DiskANN built on 2048-d RR at half the compute cost on ImageNet retrieval.\n\nAdANNS-IVFOPQ has the same accuracy as the baselines at 8\u00d7 lower overhead. With DiskANN, AdANNS accelerates shortlist generation by using low-dimensional representations and recoups the accuracy by re-ranking with the highest-dimensional MR at negligible cost. Table 1 shows that AdANNS-DiskANN is more accurate than the baseline for both 1-NN and ranking performance at only half the cost. Using low-dimensional representations further speeds up inference in AdANNS-DiskANN (see Appendix F).\n\nThese results show the generality of AdANNS and its broad applicability across a variety of ANNS indices built on top of the base building blocks. Currently, AdANNS piggybacks on typical ANNS pipelines for their inherent accounting of the real-world system constraints [16, 22, 25]. However, we believe that AdANNS\u2019s flexibility and significantly better accuracy-compute trade-off can be further informed by real-world deployment constraints. We leave this high-potential line of work that requires extensive study to future research.", "md": "that AdANNS-IVFOPQ is $$1 - 4\\%$$ with the 2048-d MR outperforms DiskANN built on 2048-d RR at half the compute cost on ImageNet retrieval.\n\nAdANNS-IVFOPQ has the same accuracy as the baselines at 8\u00d7 lower overhead. With DiskANN, AdANNS accelerates shortlist generation by using low-dimensional representations and recoups the accuracy by re-ranking with the highest-dimensional MR at negligible cost. Table 1 shows that AdANNS-DiskANN is more accurate than the baseline for both 1-NN and ranking performance at only half the cost. Using low-dimensional representations further speeds up inference in AdANNS-DiskANN (see Appendix F).\n\nThese results show the generality of AdANNS and its broad applicability across a variety of ANNS indices built on top of the base building blocks. Currently, AdANNS piggybacks on typical ANNS pipelines for their inherent accounting of the real-world system constraints [16, 22, 25]. However, we believe that AdANNS\u2019s flexibility and significantly better accuracy-compute trade-off can be further informed by real-world deployment constraints. We leave this high-potential line of work that requires extensive study to future research."}, {"type": "heading", "lvl": 4, "value": "Further Analysis and Discussion", "md": "#### Further Analysis and Discussion"}, {"type": "heading", "lvl": 5, "value": "Compute-aware Elastic Search During Inference", "md": "##### Compute-aware Elastic Search During Inference"}, {"type": "text", "value": "AdANNS search structures cater to many specific large-scale use scenarios that need to satisfy precise resource constraints during construction as well as inference. However, in many cases, construction and storage of the indices are not the bottlenecks or the user is unable to search the design space. In these settings, AdANNS-D enables adaptive inference through accurate yet cheaper distance computation using the low-dimensional prefix of matryoshka representation. Akin to composite indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low-dimensional MR for faster distance computation on ANNS structure built non-adaptively with a high-dimensional MR without any modifications to the existing index.\n\nEmpirical results. Figure 2 shows that for a given compute budget using IVF on ImageNet-1K retrieval, AdANNS-IVF is better than AdANNS-IVF-D due to the explicit control during the building", "md": "AdANNS search structures cater to many specific large-scale use scenarios that need to satisfy precise resource constraints during construction as well as inference. However, in many cases, construction and storage of the indices are not the bottlenecks or the user is unable to search the design space. In these settings, AdANNS-D enables adaptive inference through accurate yet cheaper distance computation using the low-dimensional prefix of matryoshka representation. Akin to composite indices (Section 4.3) that use PQ vectors for cheaper distance computation, we can use the low-dimensional MR for faster distance computation on ANNS structure built non-adaptively with a high-dimensional MR without any modifications to the existing index.\n\nEmpirical results. Figure 2 shows that for a given compute budget using IVF on ImageNet-1K retrieval, AdANNS-IVF is better than AdANNS-IVF-D due to the explicit control during the building"}]}, {"page": 9, "text": "of the ANNS structure which is expected. However, the interesting observation is that AdANNS-D\nmatches or outperforms the IVF indices built with MRs of varying capacities for ImageNet retrieval.\nHowever, these methods are applicable in specific scenarios of deployment. Obtaining optimal\nAdANNS search structure (highly accurate) or even the best IVF-MR index relies on a relatively\nexpensive design search but delivers indices that fit the storage, memory, compute, and accuracy\nconstraints all at once. On the other hand AdANNS-D does not require a precisely built ANNS index\nbut can enable compute-aware search during inference. AdANNS-D is a great choice for setups that\ncan afford only one single database/index but need to cater to varying deployment constraints, e.g.,\none task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query.\n5.2   Why MRs over RRs?\nQuite a few of the gains from AdANNS are owing to the quality and capabilities of matryoshka\nrepresentations. So, we conducted extensive analysis to understand why matryoshka representations\nseem to be more aligned for semantic search than the status-quo rigid representations.\nDifficulty of NN search. Relative contrast (Cr) [18] is inversely proportional to the diffi     culty of\nnearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that MRs have\nbetter Cr than RRs across dimensionalities, further supporting that matryoshka representations are\nmore aligned (easier) for NN search than existing rigid representations for the same accuracy. More\ndetails and analysis about this experiment can be found in Appendix H.2.\nClustering distributions. We also investigate the potential deviation in clustering distributions\nfor MRs across dimensionalities compared to RRs. Unlike the RRs where the information is\nuniformly diffused across dimensions [49], MRs have hierarchical information packing. Figure 11 in\nAppendix E.3 shows that matryoshka representations result in clusters similar (measured by total\nvariation distance [33]) to that of rigid representations and do not result in any unusual artifacts.\nRobustness. Figure 9 in Appendix E shows that MRs continue to be better than RRs even for out-of-\ndistribution (OOD) image queries (ImageNetV2 [44]) using ANNS. It also shows that the highest\ndata dimensionality need not always be the most robust which is further supported by the higher\nrecall using lower dimensions. Further details about this experiment can be found in Appendix E.1.\nGenerality across encoders. IVF-MR consistently has higher accuracy than IVF-RR across dimen-\nsionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and\nBERT-Base on NQ). We find that our observations on better alignment of MRs for NN search hold\nacross neural network architectures, ResNet18/34/101 [19] and ConvNeXt-Tiny [35]. Appendix H.3\ndelves deep into the experimentation done using various neural architectures on ImageNet-1K.\nRecall score analysis. Analysis of recall score (see Appendix C) in Appendix H.1 shows that for\na similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search\ncomplexities for IVF and HNSW on ImageNet-1K. Across the board, MRs have higher recall scores\nand top-1 accuracy pointing to easier \u201csearchability\u201d and thus suitability of matryoshka representations\nfor ANNS. Larger-scale experiments and further analysis can be found in Appendix H.\nThrough these analyses, we argue that matryoshka representations are better suited for semantic\nsearch than rigid representations, thus making them an ideal choice for AdANNS.\n5.3   Search for AdANNS Hyperparameters\nChoosing the optimal hyperparameters for AdANNS, such as dc, ds, m, # clusters, # probes, is an\ninteresting and open problem that requires more rigorous examination. As the ANNS index is formed\nonce and used for potentially billions of queries with massive implications for cost, latency and\nqueries-per-second, a hyperparameter search for the best index is generally an acceptable industry\npractice [22, 38]. The Faiss library [24] provides guidelines2 to choose the appropriate index for a\nspecific problem, including memory constraints, database size, and the need for exact results. There\nhave been efforts at automating the search for optimal indexing parameters, such as Autofaiss3, which\nmaximizes recall given compute constraints.\n   2https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n   3https://github.com/criteo/autofaiss\n                                                    9", "md": "of the ANNS structure which is expected. However, the interesting observation is that AdANNS-D\nmatches or outperforms the IVF indices built with MRs of varying capacities for ImageNet retrieval.\nHowever, these methods are applicable in specific scenarios of deployment. Obtaining optimal\nAdANNS search structure (highly accurate) or even the best IVF-MR index relies on a relatively\nexpensive design search but delivers indices that fit the storage, memory, compute, and accuracy\nconstraints all at once. On the other hand AdANNS-D does not require a precisely built ANNS index\nbut can enable compute-aware search during inference. AdANNS-D is a great choice for setups that\ncan afford only one single database/index but need to cater to varying deployment constraints, e.g.,\none task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query.\n\n### 5.2   Why MRs over RRs?\n\nQuite a few of the gains from AdANNS are owing to the quality and capabilities of matryoshka\nrepresentations. So, we conducted extensive analysis to understand why matryoshka representations\nseem to be more aligned for semantic search than the status-quo rigid representations.\n\nDifficulty of NN search. Relative contrast (Cr) [18] is inversely proportional to the difficulty of\nnearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that MRs have\nbetter Cr than RRs across dimensionalities, further supporting that matryoshka representations are\nmore aligned (easier) for NN search than existing rigid representations for the same accuracy. More\ndetails and analysis about this experiment can be found in Appendix H.2.\n\nClustering distributions. We also investigate the potential deviation in clustering distributions\nfor MRs across dimensionalities compared to RRs. Unlike the RRs where the information is\nuniformly diffused across dimensions [49], MRs have hierarchical information packing. Figure 11 in\nAppendix E.3 shows that matryoshka representations result in clusters similar (measured by total\nvariation distance [33]) to that of rigid representations and do not result in any unusual artifacts.\n\nRobustness. Figure 9 in Appendix E shows that MRs continue to be better than RRs even for out-of-\ndistribution (OOD) image queries (ImageNetV2 [44]) using ANNS. It also shows that the highest\ndata dimensionality need not always be the most robust which is further supported by the higher\nrecall using lower dimensions. Further details about this experiment can be found in Appendix E.1.\n\nGenerality across encoders. IVF-MR consistently has higher accuracy than IVF-RR across dimensionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and\nBERT-Base on NQ). We find that our observations on better alignment of MRs for NN search hold\nacross neural network architectures, ResNet18/34/101 [19] and ConvNeXt-Tiny [35]. Appendix H.3\ndelves deep into the experimentation done using various neural architectures on ImageNet-1K.\n\nRecall score analysis. Analysis of recall score (see Appendix C) in Appendix H.1 shows that for\na similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search\ncomplexities for IVF and HNSW on ImageNet-1K. Across the board, MRs have higher recall scores\nand top-1 accuracy pointing to easier \u201csearchability\u201d and thus suitability of matryoshka representations\nfor ANNS. Larger-scale experiments and further analysis can be found in Appendix H.\n\nThrough these analyses, we argue that matryoshka representations are better suited for semantic\nsearch than rigid representations, thus making them an ideal choice for AdANNS.\n\n### 5.3   Search for AdANNS Hyperparameters\n\nChoosing the optimal hyperparameters for AdANNS, such as dc, ds, m, # clusters, # probes, is an\ninteresting and open problem that requires more rigorous examination. As the ANNS index is formed\nonce and used for potentially billions of queries with massive implications for cost, latency and\nqueries-per-second, a hyperparameter search for the best index is generally an acceptable industry\npractice [22, 38]. The Faiss library [24] provides guidelines to choose the appropriate index for a\nspecific problem, including memory constraints, database size, and the need for exact results. There\nhave been efforts at automating the search for optimal indexing parameters, such as Autofaiss, which\nmaximizes recall given compute constraints.\n\nReferences:\n- Guidelines to choose an index\n- Autofaiss", "images": [], "items": [{"type": "text", "value": "of the ANNS structure which is expected. However, the interesting observation is that AdANNS-D\nmatches or outperforms the IVF indices built with MRs of varying capacities for ImageNet retrieval.\nHowever, these methods are applicable in specific scenarios of deployment. Obtaining optimal\nAdANNS search structure (highly accurate) or even the best IVF-MR index relies on a relatively\nexpensive design search but delivers indices that fit the storage, memory, compute, and accuracy\nconstraints all at once. On the other hand AdANNS-D does not require a precisely built ANNS index\nbut can enable compute-aware search during inference. AdANNS-D is a great choice for setups that\ncan afford only one single database/index but need to cater to varying deployment constraints, e.g.,\none task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query.", "md": "of the ANNS structure which is expected. However, the interesting observation is that AdANNS-D\nmatches or outperforms the IVF indices built with MRs of varying capacities for ImageNet retrieval.\nHowever, these methods are applicable in specific scenarios of deployment. Obtaining optimal\nAdANNS search structure (highly accurate) or even the best IVF-MR index relies on a relatively\nexpensive design search but delivers indices that fit the storage, memory, compute, and accuracy\nconstraints all at once. On the other hand AdANNS-D does not require a precisely built ANNS index\nbut can enable compute-aware search during inference. AdANNS-D is a great choice for setups that\ncan afford only one single database/index but need to cater to varying deployment constraints, e.g.,\none task requires 70% accuracy while another task has a compute budget of 1 MFLOPS/query."}, {"type": "heading", "lvl": 3, "value": "5.2   Why MRs over RRs?", "md": "### 5.2   Why MRs over RRs?"}, {"type": "text", "value": "Quite a few of the gains from AdANNS are owing to the quality and capabilities of matryoshka\nrepresentations. So, we conducted extensive analysis to understand why matryoshka representations\nseem to be more aligned for semantic search than the status-quo rigid representations.\n\nDifficulty of NN search. Relative contrast (Cr) [18] is inversely proportional to the difficulty of\nnearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that MRs have\nbetter Cr than RRs across dimensionalities, further supporting that matryoshka representations are\nmore aligned (easier) for NN search than existing rigid representations for the same accuracy. More\ndetails and analysis about this experiment can be found in Appendix H.2.\n\nClustering distributions. We also investigate the potential deviation in clustering distributions\nfor MRs across dimensionalities compared to RRs. Unlike the RRs where the information is\nuniformly diffused across dimensions [49], MRs have hierarchical information packing. Figure 11 in\nAppendix E.3 shows that matryoshka representations result in clusters similar (measured by total\nvariation distance [33]) to that of rigid representations and do not result in any unusual artifacts.\n\nRobustness. Figure 9 in Appendix E shows that MRs continue to be better than RRs even for out-of-\ndistribution (OOD) image queries (ImageNetV2 [44]) using ANNS. It also shows that the highest\ndata dimensionality need not always be the most robust which is further supported by the higher\nrecall using lower dimensions. Further details about this experiment can be found in Appendix E.1.\n\nGenerality across encoders. IVF-MR consistently has higher accuracy than IVF-RR across dimensionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and\nBERT-Base on NQ). We find that our observations on better alignment of MRs for NN search hold\nacross neural network architectures, ResNet18/34/101 [19] and ConvNeXt-Tiny [35]. Appendix H.3\ndelves deep into the experimentation done using various neural architectures on ImageNet-1K.\n\nRecall score analysis. Analysis of recall score (see Appendix C) in Appendix H.1 shows that for\na similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search\ncomplexities for IVF and HNSW on ImageNet-1K. Across the board, MRs have higher recall scores\nand top-1 accuracy pointing to easier \u201csearchability\u201d and thus suitability of matryoshka representations\nfor ANNS. Larger-scale experiments and further analysis can be found in Appendix H.\n\nThrough these analyses, we argue that matryoshka representations are better suited for semantic\nsearch than rigid representations, thus making them an ideal choice for AdANNS.", "md": "Quite a few of the gains from AdANNS are owing to the quality and capabilities of matryoshka\nrepresentations. So, we conducted extensive analysis to understand why matryoshka representations\nseem to be more aligned for semantic search than the status-quo rigid representations.\n\nDifficulty of NN search. Relative contrast (Cr) [18] is inversely proportional to the difficulty of\nnearest neighbor search on a given database. On ImageNet-1K, Figure 14 shows that MRs have\nbetter Cr than RRs across dimensionalities, further supporting that matryoshka representations are\nmore aligned (easier) for NN search than existing rigid representations for the same accuracy. More\ndetails and analysis about this experiment can be found in Appendix H.2.\n\nClustering distributions. We also investigate the potential deviation in clustering distributions\nfor MRs across dimensionalities compared to RRs. Unlike the RRs where the information is\nuniformly diffused across dimensions [49], MRs have hierarchical information packing. Figure 11 in\nAppendix E.3 shows that matryoshka representations result in clusters similar (measured by total\nvariation distance [33]) to that of rigid representations and do not result in any unusual artifacts.\n\nRobustness. Figure 9 in Appendix E shows that MRs continue to be better than RRs even for out-of-\ndistribution (OOD) image queries (ImageNetV2 [44]) using ANNS. It also shows that the highest\ndata dimensionality need not always be the most robust which is further supported by the higher\nrecall using lower dimensions. Further details about this experiment can be found in Appendix E.1.\n\nGenerality across encoders. IVF-MR consistently has higher accuracy than IVF-RR across dimensionalities despite having similar accuracies with exact NN search (for ResNet50 on ImageNet and\nBERT-Base on NQ). We find that our observations on better alignment of MRs for NN search hold\nacross neural network architectures, ResNet18/34/101 [19] and ConvNeXt-Tiny [35]. Appendix H.3\ndelves deep into the experimentation done using various neural architectures on ImageNet-1K.\n\nRecall score analysis. Analysis of recall score (see Appendix C) in Appendix H.1 shows that for\na similar top-1 accuracy, lower-dimensional representations have better 1-Recall@1 across search\ncomplexities for IVF and HNSW on ImageNet-1K. Across the board, MRs have higher recall scores\nand top-1 accuracy pointing to easier \u201csearchability\u201d and thus suitability of matryoshka representations\nfor ANNS. Larger-scale experiments and further analysis can be found in Appendix H.\n\nThrough these analyses, we argue that matryoshka representations are better suited for semantic\nsearch than rigid representations, thus making them an ideal choice for AdANNS."}, {"type": "heading", "lvl": 3, "value": "5.3   Search for AdANNS Hyperparameters", "md": "### 5.3   Search for AdANNS Hyperparameters"}, {"type": "text", "value": "Choosing the optimal hyperparameters for AdANNS, such as dc, ds, m, # clusters, # probes, is an\ninteresting and open problem that requires more rigorous examination. As the ANNS index is formed\nonce and used for potentially billions of queries with massive implications for cost, latency and\nqueries-per-second, a hyperparameter search for the best index is generally an acceptable industry\npractice [22, 38]. The Faiss library [24] provides guidelines to choose the appropriate index for a\nspecific problem, including memory constraints, database size, and the need for exact results. There\nhave been efforts at automating the search for optimal indexing parameters, such as Autofaiss, which\nmaximizes recall given compute constraints.\n\nReferences:\n- Guidelines to choose an index\n- Autofaiss", "md": "Choosing the optimal hyperparameters for AdANNS, such as dc, ds, m, # clusters, # probes, is an\ninteresting and open problem that requires more rigorous examination. As the ANNS index is formed\nonce and used for potentially billions of queries with massive implications for cost, latency and\nqueries-per-second, a hyperparameter search for the best index is generally an acceptable industry\npractice [22, 38]. The Faiss library [24] provides guidelines to choose the appropriate index for a\nspecific problem, including memory constraints, database size, and the need for exact results. There\nhave been efforts at automating the search for optimal indexing parameters, such as Autofaiss, which\nmaximizes recall given compute constraints.\n\nReferences:\n- Guidelines to choose an index\n- Autofaiss"}]}, {"page": 10, "text": " In case of AdANNS, we suggest starting at the best configurations of MRs followed by a local\n design space search to lead to near-optimal AdANNS configurations (e.g. use IVF-MR to bootstrap\n AdANNS-IVF). We also share some observations during the course of our experiments:\n 1. AdANNS-IVF: Top-1 accuracy generally improves (with diminishing returns after a point) with\n    increasing dimensionality of clustering (dc) and search (ds), as we show on ImageNet variants and\n    with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-d MRs matches\n    the performance of high-d MRs as they likely contain similar amounts of useful information,\n    making the increased compute cost not worth the marginal gains. Increasing # probes naturally\n    boosts performance (Appendix, Figure 10a). Lastly, it is generally accepted that a good starting\n    point for the # clusters k is    ND/2, where ND is the number of indexable items [39]. k = \u221aND\n    is the optimal choice of k from a FLOPS computation perspective as can be seen in Appendix B.1.\n 2. AdANNS-OPQ: we observe that for a fixed compute budget in bytes (m), the top-1 accuracy\n    reaches a peak at d < dmax (Appendix, Table 4). We hypothesize that the better performance of\n    AdANNS-OPQ at d < dmax is due to the curse of dimensionality, i.e. it is easier to learn PQ\n    codebooks on smaller embeddings with similar amounts of information. We find that using an\n    MR with d = 4 \u00d7 m is a good starting point on ImageNet and NQ. We also suggest using an 8-bit\n    (256-length) codebook for OPQ as the default for each of the sub-block quantizer.\n 3. AdANNS-DiskANN: Our observations with DiskANN are consistent with other indexing struc-\n    tures, i.e. the optimal graph construction dimensionality d < dmax (Appendix, Figure 12). A\n    careful study of DiskANN on different datasets is required for more general guidelines to choose\n    graph construction and OPQ dimensionality d.\n 5.4   Limitations\n AdANNS\u2019s core focus is to improve the design of the existing ANNS pipelines. To use AdANNS\n on a corpus, we need to back-fill [43] the MRs of the data \u2013 a significant yet a one-time overhead.\nWe also notice that high-dimensional MRs start to degrade in performance when optimizing also for\n an extremely low-dimensional granularity (e.g., < 24-d for NQ) \u2013 otherwise is it quite easy to have\n comparable accuracies with both RRs and MRs. Lastly, the existing dense representations can only\n in theory be converted to MRs with an auto-encoder-style non-linear transformation. We believe\n most of these limitations form excellent future work to improve AdANNS further.\n 6    Conclusions\nWe proposed a novel framework, AdANNS                , that leverages adaptive representations for different\n phases of ANNS pipelines to improve the accuracy-compute tradeoff. AdANNS utilizes the inherent\n flexibility of matryoshka representations [31] to design better ANNS building blocks than the standard\n ones which use the rigid representation in each phase. AdANNS achieves SOTA accuracy-compute\n trade-off for the two main ANNS building blocks: search data structures (AdANNS-IVF) and\n quantization (AdANNS-OPQ). The combination of AdANNS-based building blocks leads to the\n construction of better real-world composite ANNS indices \u2013 with as much as 8\u00d7 reduction in cost at\n the same accuracy as strong baselines \u2013 while also enabling compute-aware elastic search. Finally, we\n note that combining AdANNS with elastic encoders [11] enables truly adaptive large-scale retrieval.\n Acknowledgments\nWe are grateful to Kaifeng Chen, Venkata Sailesh Sanampudi, Sanjiv Kumar, Harsha Vardhan\n Simhadri, Gantavya Bhatt, Matthijs Douze and Matthew Wallingford for helpful discussions and\n feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part\n of the paper\u2019s large-scale experimentation is supported through a research GCP credit award from\n Google Cloud and Google Research. Sham Kakade acknowledges funding from the ONR award\n N00014-22-1-2377 and NSF award CCF-2212841. Ali Farhadi acknowledges funding from the NSF\n awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and\n gifts from Allen Institute for Artificial Intelligence and Google.\n                                                      10", "md": "# AdANNS Observations and Limitations\n\n# AdANNS Observations and Limitations\n\nIn case of AdANNS, we suggest starting at the best configurations of MRs followed by a local design space search to lead to near-optimal AdANNS configurations (e.g. use IVF-MR to bootstrap AdANNS-IVF). We also share some observations during the course of our experiments:\n\n## 1. AdANNS-IVF\n\nTop-1 accuracy generally improves (with diminishing returns after a point) with increasing dimensionality of clustering ($$d_c$$) and search ($$d_s$$), as we show on ImageNet variants and with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-d MRs matches the performance of high-d MRs as they likely contain similar amounts of useful information, making the increased compute cost not worth the marginal gains. Increasing # probes naturally boosts performance (Appendix, Figure 10a). Lastly, it is generally accepted that a good starting point for the # clusters $$k$$ is $$\\frac{ND}{2}$$, where $$ND$$ is the number of indexable items [39]. $$k = \\sqrt{ND}$$ is the optimal choice of $$k$$ from a FLOPS computation perspective as can be seen in Appendix B.1.\n\n## 2. AdANNS-OPQ\n\nWe observe that for a fixed compute budget in bytes ($$m$$), the top-1 accuracy reaches a peak at $$d < d_{\\text{max}}$$ (Appendix, Table 4). We hypothesize that the better performance of AdANNS-OPQ at $$d < d_{\\text{max}}$$ is due to the curse of dimensionality, i.e. it is easier to learn PQ codebooks on smaller embeddings with similar amounts of information. We find that using an MR with $$d = 4 \\times m$$ is a good starting point on ImageNet and NQ. We also suggest using an 8-bit (256-length) codebook for OPQ as the default for each of the sub-block quantizer.\n\n## 3. AdANNS-DiskANN\n\nOur observations with DiskANN are consistent with other indexing structures, i.e. the optimal graph construction dimensionality $$d < d_{\\text{max}}$$ (Appendix, Figure 12). A careful study of DiskANN on different datasets is required for more general guidelines to choose graph construction and OPQ dimensionality $$d$$.\n\n## 5.4 Limitations\n\nAdANNS\u2019s core focus is to improve the design of the existing ANNS pipelines. To use AdANNS on a corpus, we need to back-fill [43] the MRs of the data \u2013 a significant yet a one-time overhead. We also notice that high-dimensional MRs start to degrade in performance when optimizing also for an extremely low-dimensional granularity (e.g., < 24-d for NQ) \u2013 otherwise is it quite easy to have comparable accuracies with both RRs and MRs. Lastly, the existing dense representations can only in theory be converted to MRs with an auto-encoder-style non-linear transformation. We believe most of these limitations form excellent future work to improve AdANNS further.\n\n## 6 Conclusions\n\nWe proposed a novel framework, AdANNS, that leverages adaptive representations for different phases of ANNS pipelines to improve the accuracy-compute tradeoff. AdANNS utilizes the inherent flexibility of matryoshka representations [31] to design better ANNS building blocks than the standard ones which use the rigid representation in each phase. AdANNS achieves SOTA accuracy-compute trade-off for the two main ANNS building blocks: search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). The combination of AdANNS-based building blocks leads to the construction of better real-world composite ANNS indices \u2013 with as much as 8\u00d7 reduction in cost at the same accuracy as strong baselines \u2013 while also enabling compute-aware elastic search. Finally, we note that combining AdANNS with elastic encoders [11] enables truly adaptive large-scale retrieval.\n\n## Acknowledgments\n\nWe are grateful to Kaifeng Chen, Venkata Sailesh Sanampudi, Sanjiv Kumar, Harsha Vardhan Simhadri, Gantavya Bhatt, Matthijs Douze and Matthew Wallingford for helpful discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part of the paper\u2019s large-scale experimentation is supported through a research GCP credit award from Google Cloud and Google Research. Sham Kakade acknowledges funding from the ONR award N00014-22-1-2377 and NSF award CCF-2212841. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence and Google.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "AdANNS Observations and Limitations", "md": "# AdANNS Observations and Limitations"}, {"type": "heading", "lvl": 1, "value": "AdANNS Observations and Limitations", "md": "# AdANNS Observations and Limitations"}, {"type": "text", "value": "In case of AdANNS, we suggest starting at the best configurations of MRs followed by a local design space search to lead to near-optimal AdANNS configurations (e.g. use IVF-MR to bootstrap AdANNS-IVF). We also share some observations during the course of our experiments:", "md": "In case of AdANNS, we suggest starting at the best configurations of MRs followed by a local design space search to lead to near-optimal AdANNS configurations (e.g. use IVF-MR to bootstrap AdANNS-IVF). We also share some observations during the course of our experiments:"}, {"type": "heading", "lvl": 2, "value": "1. AdANNS-IVF", "md": "## 1. AdANNS-IVF"}, {"type": "text", "value": "Top-1 accuracy generally improves (with diminishing returns after a point) with increasing dimensionality of clustering ($$d_c$$) and search ($$d_s$$), as we show on ImageNet variants and with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-d MRs matches the performance of high-d MRs as they likely contain similar amounts of useful information, making the increased compute cost not worth the marginal gains. Increasing # probes naturally boosts performance (Appendix, Figure 10a). Lastly, it is generally accepted that a good starting point for the # clusters $$k$$ is $$\\frac{ND}{2}$$, where $$ND$$ is the number of indexable items [39]. $$k = \\sqrt{ND}$$ is the optimal choice of $$k$$ from a FLOPS computation perspective as can be seen in Appendix B.1.", "md": "Top-1 accuracy generally improves (with diminishing returns after a point) with increasing dimensionality of clustering ($$d_c$$) and search ($$d_s$$), as we show on ImageNet variants and with multiple encoders in the Appendix (Figures 9 and 15). Clustering with low-d MRs matches the performance of high-d MRs as they likely contain similar amounts of useful information, making the increased compute cost not worth the marginal gains. Increasing # probes naturally boosts performance (Appendix, Figure 10a). Lastly, it is generally accepted that a good starting point for the # clusters $$k$$ is $$\\frac{ND}{2}$$, where $$ND$$ is the number of indexable items [39]. $$k = \\sqrt{ND}$$ is the optimal choice of $$k$$ from a FLOPS computation perspective as can be seen in Appendix B.1."}, {"type": "heading", "lvl": 2, "value": "2. AdANNS-OPQ", "md": "## 2. AdANNS-OPQ"}, {"type": "text", "value": "We observe that for a fixed compute budget in bytes ($$m$$), the top-1 accuracy reaches a peak at $$d < d_{\\text{max}}$$ (Appendix, Table 4). We hypothesize that the better performance of AdANNS-OPQ at $$d < d_{\\text{max}}$$ is due to the curse of dimensionality, i.e. it is easier to learn PQ codebooks on smaller embeddings with similar amounts of information. We find that using an MR with $$d = 4 \\times m$$ is a good starting point on ImageNet and NQ. We also suggest using an 8-bit (256-length) codebook for OPQ as the default for each of the sub-block quantizer.", "md": "We observe that for a fixed compute budget in bytes ($$m$$), the top-1 accuracy reaches a peak at $$d < d_{\\text{max}}$$ (Appendix, Table 4). We hypothesize that the better performance of AdANNS-OPQ at $$d < d_{\\text{max}}$$ is due to the curse of dimensionality, i.e. it is easier to learn PQ codebooks on smaller embeddings with similar amounts of information. We find that using an MR with $$d = 4 \\times m$$ is a good starting point on ImageNet and NQ. We also suggest using an 8-bit (256-length) codebook for OPQ as the default for each of the sub-block quantizer."}, {"type": "heading", "lvl": 2, "value": "3. AdANNS-DiskANN", "md": "## 3. AdANNS-DiskANN"}, {"type": "text", "value": "Our observations with DiskANN are consistent with other indexing structures, i.e. the optimal graph construction dimensionality $$d < d_{\\text{max}}$$ (Appendix, Figure 12). A careful study of DiskANN on different datasets is required for more general guidelines to choose graph construction and OPQ dimensionality $$d$$.", "md": "Our observations with DiskANN are consistent with other indexing structures, i.e. the optimal graph construction dimensionality $$d < d_{\\text{max}}$$ (Appendix, Figure 12). A careful study of DiskANN on different datasets is required for more general guidelines to choose graph construction and OPQ dimensionality $$d$$."}, {"type": "heading", "lvl": 2, "value": "5.4 Limitations", "md": "## 5.4 Limitations"}, {"type": "text", "value": "AdANNS\u2019s core focus is to improve the design of the existing ANNS pipelines. To use AdANNS on a corpus, we need to back-fill [43] the MRs of the data \u2013 a significant yet a one-time overhead. We also notice that high-dimensional MRs start to degrade in performance when optimizing also for an extremely low-dimensional granularity (e.g., < 24-d for NQ) \u2013 otherwise is it quite easy to have comparable accuracies with both RRs and MRs. Lastly, the existing dense representations can only in theory be converted to MRs with an auto-encoder-style non-linear transformation. We believe most of these limitations form excellent future work to improve AdANNS further.", "md": "AdANNS\u2019s core focus is to improve the design of the existing ANNS pipelines. To use AdANNS on a corpus, we need to back-fill [43] the MRs of the data \u2013 a significant yet a one-time overhead. We also notice that high-dimensional MRs start to degrade in performance when optimizing also for an extremely low-dimensional granularity (e.g., < 24-d for NQ) \u2013 otherwise is it quite easy to have comparable accuracies with both RRs and MRs. Lastly, the existing dense representations can only in theory be converted to MRs with an auto-encoder-style non-linear transformation. We believe most of these limitations form excellent future work to improve AdANNS further."}, {"type": "heading", "lvl": 2, "value": "6 Conclusions", "md": "## 6 Conclusions"}, {"type": "text", "value": "We proposed a novel framework, AdANNS, that leverages adaptive representations for different phases of ANNS pipelines to improve the accuracy-compute tradeoff. AdANNS utilizes the inherent flexibility of matryoshka representations [31] to design better ANNS building blocks than the standard ones which use the rigid representation in each phase. AdANNS achieves SOTA accuracy-compute trade-off for the two main ANNS building blocks: search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). The combination of AdANNS-based building blocks leads to the construction of better real-world composite ANNS indices \u2013 with as much as 8\u00d7 reduction in cost at the same accuracy as strong baselines \u2013 while also enabling compute-aware elastic search. Finally, we note that combining AdANNS with elastic encoders [11] enables truly adaptive large-scale retrieval.", "md": "We proposed a novel framework, AdANNS, that leverages adaptive representations for different phases of ANNS pipelines to improve the accuracy-compute tradeoff. AdANNS utilizes the inherent flexibility of matryoshka representations [31] to design better ANNS building blocks than the standard ones which use the rigid representation in each phase. AdANNS achieves SOTA accuracy-compute trade-off for the two main ANNS building blocks: search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). The combination of AdANNS-based building blocks leads to the construction of better real-world composite ANNS indices \u2013 with as much as 8\u00d7 reduction in cost at the same accuracy as strong baselines \u2013 while also enabling compute-aware elastic search. Finally, we note that combining AdANNS with elastic encoders [11] enables truly adaptive large-scale retrieval."}, {"type": "heading", "lvl": 2, "value": "Acknowledgments", "md": "## Acknowledgments"}, {"type": "text", "value": "We are grateful to Kaifeng Chen, Venkata Sailesh Sanampudi, Sanjiv Kumar, Harsha Vardhan Simhadri, Gantavya Bhatt, Matthijs Douze and Matthew Wallingford for helpful discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part of the paper\u2019s large-scale experimentation is supported through a research GCP credit award from Google Cloud and Google Research. Sham Kakade acknowledges funding from the ONR award N00014-22-1-2377 and NSF award CCF-2212841. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence and Google.", "md": "We are grateful to Kaifeng Chen, Venkata Sailesh Sanampudi, Sanjiv Kumar, Harsha Vardhan Simhadri, Gantavya Bhatt, Matthijs Douze and Matthew Wallingford for helpful discussions and feedback. Aditya Kusupati also thanks Tom Duerig and Rahul Sukthankar for their support. Part of the paper\u2019s large-scale experimentation is supported through a research GCP credit award from Google Cloud and Google Research. Sham Kakade acknowledges funding from the ONR award N00014-22-1-2377 and NSF award CCF-2212841. Ali Farhadi acknowledges funding from the NSF awards IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543, and gifts from Allen Institute for Artificial Intelligence and Google."}]}, {"page": 11, "text": "References\n [1] M. Aum\u00fcller, E. Bernhardsson, and A. Faithfull. Ann-benchmarks: A benchmarking tool for\n      approximate nearest neighbor algorithms. Information Systems, 87:101374, 2020.\n [2] Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceed-\n      ings of ICML workshop on unsupervised and transfer learning, pages 17\u201336. JMLR Workshop\n      and Conference Proceedings, 2012.\n [3] E. Bernhardsson.     Annoy: Approximate Nearest Neighbors in C++/Python, 2018.             URL\n      https://pypi.org/project/annoy/. Python package version 1.13.0.\n [4] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer\n      networks and ISDN systems, 30(1-7):107\u2013117, 1998.\n [5] D. Cai. A revisit of hashing algorithms for approximate nearest neighbor search. IEEE\n     Transactions on Knowledge and Data Engineering, 33(6):2337\u20132348, 2021. doi: 10.1109/\n      TKDE.2019.2953897.\n [6] T. Chen, L. Li, and Y. Sun. Differentiable product quantization for end-to-end embedding\n      compression. In International Conference on Machine Learning, pages 1617\u20131626. PMLR,\n      2020.\n [7] K. L. Clarkson. An algorithm for approximate closest-point queries. In Proceedings of the tenth\n      annual symposium on Computational geometry, pages 160\u2013164, 1994.\n [8] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based\n      on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational\n      geometry, pages 253\u2013262, 2004.\n [9] J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the 2nd\n     ACM International Conference on Web Search and Data Mining (WSDM), volume 10, 2009.\n[10] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\n      transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[11] Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hannaneh,\n      S. Kakade, A. Farhadi, and P. Jain. Matformer: Nested transformer for elastic inference. arXiv\n      preprint arxiv:2310.07707, 2023.\n[12] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for finding best matches in\n      logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209\u2013\n      226, 1977.\n[13] T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest\n      neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n      Recognition, pages 2946\u20132953, 2013.\n[14] G. Golub and W. Kahan. Calculating the singular values and pseudo-inverse of a matrix. Journal\n      of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2):\n      205\u2013224, 1965.\n[15] R. Gray. Vector quantization. IEEE Assp Magazine, 1(2):4\u201329, 1984.\n[16] R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating\n      large-scale inference with anisotropic vector quantization. In International Conference on\n      Machine Learning, pages 3887\u20133896. PMLR, 2020.\n[17] N. Gupta, P. H. Chen, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. End-to-end learning to index and\n      search in large output spaces. arXiv preprint arXiv:2210.08410, 2022.\n[18] J. He, S. Kumar, and S.-F. Chang. On the difficulty of nearest neighbor search. In International\n      Conference on Machine Learning (ICML), 2012.\n                                                 11", "md": "# References\n\n## References\n\n1. M. Aum\u00fcller, E. Bernhardsson, and A. Faithfull. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 87:101374, 2020.\n2. Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceedings of ICML workshop on unsupervised and transfer learning, pages 17\u201336. JMLR Workshop and Conference Proceedings, 2012.\n3. E. Bernhardsson. Annoy: Approximate Nearest Neighbors in C++/Python, 2018. URL https://pypi.org/project/annoy/. Python package version 1.13.0.\n4. S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107\u2013117, 1998.\n5. D. Cai. A revisit of hashing algorithms for approximate nearest neighbor search. IEEE Transactions on Knowledge and Data Engineering, 33(6):2337\u20132348, 2021. doi: 10.1109/TKDE.2019.2953897.\n6. T. Chen, L. Li, and Y. Sun. Differentiable product quantization for end-to-end embedding compression. In International Conference on Machine Learning, pages 1617\u20131626. PMLR, 2020.\n7. K. L. Clarkson. An algorithm for approximate closest-point queries. In Proceedings of the tenth annual symposium on Computational geometry, pages 160\u2013164, 1994.\n8. M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253\u2013262, 2004.\n9. J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the 2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10, 2009.\n10. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n11. Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hannaneh, S. Kakade, A. Farhadi, and P. Jain. Matformer: Nested transformer for elastic inference. arXiv preprint arxiv:2310.07707, 2023.\n12. J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209\u2013226, 1977.\n13. T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2946\u20132953, 2013.\n14. G. Golub and W. Kahan. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2):205\u2013224, 1965.\n15. R. Gray. Vector quantization. IEEE Assp Magazine, 1(2):4\u201329, 1984.\n16. R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887\u20133896. PMLR, 2020.\n17. N. Gupta, P. H. Chen, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. End-to-end learning to index and search in large output spaces. arXiv preprint arXiv:2210.08410, 2022.\n18. J. He, S. Kumar, and S.-F. Chang. On the difficulty of nearest neighbor search. In International Conference on Machine Learning (ICML), 2012.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. M. Aum\u00fcller, E. Bernhardsson, and A. Faithfull. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 87:101374, 2020.\n2. Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceedings of ICML workshop on unsupervised and transfer learning, pages 17\u201336. JMLR Workshop and Conference Proceedings, 2012.\n3. E. Bernhardsson. Annoy: Approximate Nearest Neighbors in C++/Python, 2018. URL https://pypi.org/project/annoy/. Python package version 1.13.0.\n4. S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107\u2013117, 1998.\n5. D. Cai. A revisit of hashing algorithms for approximate nearest neighbor search. IEEE Transactions on Knowledge and Data Engineering, 33(6):2337\u20132348, 2021. doi: 10.1109/TKDE.2019.2953897.\n6. T. Chen, L. Li, and Y. Sun. Differentiable product quantization for end-to-end embedding compression. In International Conference on Machine Learning, pages 1617\u20131626. PMLR, 2020.\n7. K. L. Clarkson. An algorithm for approximate closest-point queries. In Proceedings of the tenth annual symposium on Computational geometry, pages 160\u2013164, 1994.\n8. M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253\u2013262, 2004.\n9. J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the 2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10, 2009.\n10. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n11. Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hannaneh, S. Kakade, A. Farhadi, and P. Jain. Matformer: Nested transformer for elastic inference. arXiv preprint arxiv:2310.07707, 2023.\n12. J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209\u2013226, 1977.\n13. T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2946\u20132953, 2013.\n14. G. Golub and W. Kahan. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2):205\u2013224, 1965.\n15. R. Gray. Vector quantization. IEEE Assp Magazine, 1(2):4\u201329, 1984.\n16. R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887\u20133896. PMLR, 2020.\n17. N. Gupta, P. H. Chen, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. End-to-end learning to index and search in large output spaces. arXiv preprint arXiv:2210.08410, 2022.\n18. J. He, S. Kumar, and S.-F. Chang. On the difficulty of nearest neighbor search. In International Conference on Machine Learning (ICML), 2012.", "md": "1. M. Aum\u00fcller, E. Bernhardsson, and A. Faithfull. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. Information Systems, 87:101374, 2020.\n2. Y. Bengio. Deep learning of representations for unsupervised and transfer learning. In Proceedings of ICML workshop on unsupervised and transfer learning, pages 17\u201336. JMLR Workshop and Conference Proceedings, 2012.\n3. E. Bernhardsson. Annoy: Approximate Nearest Neighbors in C++/Python, 2018. URL https://pypi.org/project/annoy/. Python package version 1.13.0.\n4. S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems, 30(1-7):107\u2013117, 1998.\n5. D. Cai. A revisit of hashing algorithms for approximate nearest neighbor search. IEEE Transactions on Knowledge and Data Engineering, 33(6):2337\u20132348, 2021. doi: 10.1109/TKDE.2019.2953897.\n6. T. Chen, L. Li, and Y. Sun. Differentiable product quantization for end-to-end embedding compression. In International Conference on Machine Learning, pages 1617\u20131626. PMLR, 2020.\n7. K. L. Clarkson. An algorithm for approximate closest-point queries. In Proceedings of the tenth annual symposium on Computational geometry, pages 160\u2013164, 1994.\n8. M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. Locality-sensitive hashing scheme based on p-stable distributions. In Proceedings of the twentieth annual symposium on Computational geometry, pages 253\u2013262, 2004.\n9. J. Dean. Challenges in building large-scale information retrieval systems. In Keynote of the 2nd ACM International Conference on Web Search and Data Mining (WSDM), volume 10, 2009.\n10. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n11. Devvrit, S. Kudugunta, A. Kusupati, T. Dettmers, K. Chen, I. Dhillon, Y. Tsvetkov, H. Hannaneh, S. Kakade, A. Farhadi, and P. Jain. Matformer: Nested transformer for elastic inference. arXiv preprint arxiv:2310.07707, 2023.\n12. J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software (TOMS), 3(3):209\u2013226, 1977.\n13. T. Ge, K. He, Q. Ke, and J. Sun. Optimized product quantization for approximate nearest neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2946\u20132953, 2013.\n14. G. Golub and W. Kahan. Calculating the singular values and pseudo-inverse of a matrix. Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, 2(2):205\u2013224, 1965.\n15. R. Gray. Vector quantization. IEEE Assp Magazine, 1(2):4\u201329, 1984.\n16. R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, pages 3887\u20133896. PMLR, 2020.\n17. N. Gupta, P. H. Chen, H.-F. Yu, C.-J. Hsieh, and I. S. Dhillon. End-to-end learning to index and search in large output spaces. arXiv preprint arXiv:2210.08410, 2022.\n18. J. He, S. Kumar, and S.-F. Chang. On the difficulty of nearest neighbor search. In International Conference on Machine Learning (ICML), 2012."}]}, {"page": 12, "text": "[19] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\n     Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013\n     778, 2016.\n[20] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\n     representation learning. In Proceedings of the IEEE/CVF conference on computer vision and\n     pattern recognition, pages 9729\u20139738, 2020.\n[21] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of\n     dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,\n     pages 604\u2013613, 1998.\n[22] S. Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi.\n     Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in\n     Neural Information Processing Systems, 32, 2019.\n[23] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE\n     transactions on pattern analysis and machine intelligence, 33(1):117\u2013128, 2010.\n[24] J. Johnson, M. Douze, and H. J\u00e9gou.        Billion-scale similarity search with GPUs.      IEEE\n     Transactions on Big Data, 7(3):535\u2013547, 2019.\n[25] W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:\n     189\u2013206, 1984.\n[26] I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments.\n     Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering\n     Sciences, 374(2065):20150202, 2016.\n[27] V. Karpukhin, B. O\u02d8  guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense\n     passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n[28] S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings\n     of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671,\n     2019.\n[29] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures.\n     In Proceedings of the 2018 international conference on management of data, pages 489\u2013504,\n     2018.\n[30] A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain, S. Kakade,\n     and A. Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes. Advances\n     in Neural Information Processing Systems, 34:23900\u201323913, 2021.\n[31] A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder,\n     K. Chen, S. Kakade, P. Jain, and A. Farhadi. Matryoshka representation learning. In Advances\n     in Neural Information Processing Systems, December 2022.\n[32] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein,\n     I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering\n     research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.\n[33] D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical\n     Soc., 2017.\n[34] W. Li, Y. Zhang, Y. Sun, W. Wang, W. Zhang, and X. Lin. Approximate nearest neighbor search\n     on high dimensional data\u2014experiments, analyses, and improvement. IEEE Transactions on\n     Knowledge and Data Engineering, 2020.\n[35] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In\n     Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n     11976\u201311986, 2022.\n[36] S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):\n     129\u2013137, 1982.\n                                                  12", "md": "# References\n\n# References\n\n1. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n2. K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.\n3. P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604\u2013613, 1998.\n4. S. Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems, 32, 2019.\n5. H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117\u2013128, 2010.\n6. J. Johnson, M. Douze, and H. J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.\n7. W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:189\u2013206, 1984.\n8. I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016.\n9. V. Karpukhin, B. O\u02d8 guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n10. S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671, 2019.\n11. T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489\u2013504, 2018.\n12. A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain, S. Kakade, and A. Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes. Advances in Neural Information Processing Systems, 34:23900\u201323913, 2021.\n13. A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, and A. Farhadi. Matryoshka representation learning. In Advances in Neural Information Processing Systems, December 2022.\n14. T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.\n15. D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017.\n16. W. Li, Y. Zhang, Y. Sun, W. Wang, W. Zhang, and X. Lin. Approximate nearest neighbor search on high dimensional data\u2014experiments, analyses, and improvement. IEEE Transactions on Knowledge and Data Engineering, 2020.\n17. Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986, 2022.\n18. S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129\u2013137, 1982.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "1. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n2. K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.\n3. P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604\u2013613, 1998.\n4. S. Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems, 32, 2019.\n5. H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117\u2013128, 2010.\n6. J. Johnson, M. Douze, and H. J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.\n7. W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:189\u2013206, 1984.\n8. I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016.\n9. V. Karpukhin, B. O\u02d8 guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n10. S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671, 2019.\n11. T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489\u2013504, 2018.\n12. A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain, S. Kakade, and A. Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes. Advances in Neural Information Processing Systems, 34:23900\u201323913, 2021.\n13. A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, and A. Farhadi. Matryoshka representation learning. In Advances in Neural Information Processing Systems, December 2022.\n14. T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.\n15. D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017.\n16. W. Li, Y. Zhang, Y. Sun, W. Wang, W. Zhang, and X. Lin. Approximate nearest neighbor search on high dimensional data\u2014experiments, analyses, and improvement. IEEE Transactions on Knowledge and Data Engineering, 2020.\n17. Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986, 2022.\n18. S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129\u2013137, 1982.", "md": "1. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\n2. K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9729\u20139738, 2020.\n3. P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604\u2013613, 1998.\n4. S. Jayaram Subramanya, F. Devvrit, H. V. Simhadri, R. Krishnawamy, and R. Kadekodi. Diskann: Fast accurate billion-point nearest neighbor search on a single node. Advances in Neural Information Processing Systems, 32, 2019.\n5. H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117\u2013128, 2010.\n6. J. Johnson, M. Douze, and H. J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019.\n7. W. B. Johnson. Extensions of lipschitz mappings into a hilbert space. Contemp. Math., 26:189\u2013206, 1984.\n8. I. T. Jolliffe and J. Cadima. Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202, 2016.\n9. V. Karpukhin, B. O\u02d8 guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020.\n10. S. Kornblith, J. Shlens, and Q. V. Le. Do better imagenet models transfer better? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2661\u20132671, 2019.\n11. T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In Proceedings of the 2018 international conference on management of data, pages 489\u2013504, 2018.\n12. A. Kusupati, M. Wallingford, V. Ramanujan, R. Somani, J. S. Park, K. Pillutla, P. Jain, S. Kakade, and A. Farhadi. Llc: Accurate, multi-purpose learnt low-dimensional binary codes. Advances in Neural Information Processing Systems, 34:23900\u201323913, 2021.\n13. A. Kusupati, G. Bhatt, A. Rege, M. Wallingford, A. Sinha, V. Ramanujan, W. Howard-Snyder, K. Chen, S. Kakade, P. Jain, and A. Farhadi. Matryoshka representation learning. In Advances in Neural Information Processing Systems, December 2022.\n14. T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453\u2013466, 2019.\n15. D. A. Levin and Y. Peres. Markov chains and mixing times, volume 107. American Mathematical Soc., 2017.\n16. W. Li, Y. Zhang, Y. Sun, W. Wang, W. Zhang, and X. Lin. Approximate nearest neighbor search on high dimensional data\u2014experiments, analyses, and improvement. IEEE Transactions on Knowledge and Data Engineering, 2020.\n17. Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11976\u201311986, 2022.\n18. S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129\u2013137, 1982."}]}, {"page": 13, "text": "[37] Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor\n      algorithm based on navigable small world graphs. Information Systems, 45:61\u201368, 2014.\n[38] Y. A. Malkov and D. Yashunin. Efficient and robust approximate nearest neighbor search using\n      hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis & Machine\n      Intelligence, 42(04):824\u2013836, 2020.\n[39] K. Mardia, J. Kent, and J. Bibby. Multivariate analysis. Probability and Mathematical Statistics,\n     1979.\n[40] P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https:\n     //blog.google/products/search/search-language-understanding-bert/.\n[41] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W.\n      Kim, C. Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint\n      arXiv:2201.10005, 2022.\n[42] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n      P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\n      In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n[43] V. Ramanujan, P. K. A. Vasu, A. Farhadi, O. Tuzel, and H. Pouransari. Forward compatible\n      training for representation learning. arXiv preprint arXiv:2112.02805, 2021.\n[44] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to\n      imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\n[45] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\n      A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\n      journal of computer vision, 115(3):211\u2013252, 2015.\n[46] R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate\n      Reasoning, 50(7):969\u2013978, 2009.\n[47] H. V. Simhadri, G. Williams, M. Aum\u00fcller, M. Douze, A. Babenko, D. Baranchuk, Q. Chen,\n      L. Hosseini, R. Krishnaswamy, G. Srinivasa, et al. Results of the neurips\u201921 challenge on\n      billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2205.03763, 2022.\n[48] J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos.\n      In Computer Vision, IEEE International Conference on, volume 3, pages 1470\u20131470. IEEE\n      Computer Society, 2003.\n[49] D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient\n      descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\n[50] C. Waldburger. As search needs evolve, microsoft makes ai tools for better search available\n      to researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft.\n      com/ai/bing-vector-search/.\n[51] M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental comparison\n      of graph-based approximate nearest neighbor search. Proceedings of the VLDB Endowment, 14\n      (11):1964\u20131978, 2021.\n[52] R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity-\n      search methods in high-dimensional spaces. In VLDB, volume 98, pages 194\u2013205, 1998.\n[53] I. H. Witten, I. H. Witten, A. Moffat, T. C. Bell, T. C. Bell, E. Fox, and T. C. Bell. Managing\n      gigabytes: compressing and indexing documents and images. Morgan Kaufmann, 1999.\n                                                   13", "md": "# References\n\n# References\n\n1. Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor algorithm based on navigable small world graphs. Information Systems, 45:61\u201368, 2014.\n2. Y. A. Malkov and D. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis & Machine Intelligence, 42(04):824\u2013836, 2020.\n3. K. Mardia, J. Kent, and J. Bibby. Multivariate analysis. Probability and Mathematical Statistics, 1979.\n4. P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https://blog.google/products/search/search-language-understanding-bert/.\n5. A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.\n6. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n7. V. Ramanujan, P. K. A. Vasu, A. Farhadi, O. Tuzel, and H. Pouransari. Forward compatible training for representation learning. arXiv preprint arXiv:2112.02805, 2021.\n8. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\n9. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.\n10. R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969\u2013978, 2009.\n11. H. V. Simhadri, G. Williams, M. Aum\u00fcller, M. Douze, A. Babenko, D. Baranchuk, Q. Chen, L. Hosseini, R. Krishnaswamy, G. Srinivasa, et al. Results of the neurips\u201921 challenge on billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2205.03763, 2022.\n12. J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pages 1470\u20131470. IEEE Computer Society, 2003.\n13. D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\n14. C. Waldburger. As search needs evolve, microsoft makes ai tools for better search available to researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft.com/ai/bing-vector-search/.\n15. M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. Proceedings of the VLDB Endowment, 14 (11):1964\u20131978, 2021.\n16. R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces. In VLDB, volume 98, pages 194\u2013205, 1998.\n17. I. H. Witten, I. H. Witten, A. Moffat, T. C. Bell, T. C. Bell, E. Fox, and T. C. Bell. Managing gigabytes: compressing and indexing documents and images. Morgan Kaufmann, 1999.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "1. Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor algorithm based on navigable small world graphs. Information Systems, 45:61\u201368, 2014.\n2. Y. A. Malkov and D. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis & Machine Intelligence, 42(04):824\u2013836, 2020.\n3. K. Mardia, J. Kent, and J. Bibby. Multivariate analysis. Probability and Mathematical Statistics, 1979.\n4. P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https://blog.google/products/search/search-language-understanding-bert/.\n5. A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.\n6. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n7. V. Ramanujan, P. K. A. Vasu, A. Farhadi, O. Tuzel, and H. Pouransari. Forward compatible training for representation learning. arXiv preprint arXiv:2112.02805, 2021.\n8. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\n9. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.\n10. R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969\u2013978, 2009.\n11. H. V. Simhadri, G. Williams, M. Aum\u00fcller, M. Douze, A. Babenko, D. Baranchuk, Q. Chen, L. Hosseini, R. Krishnaswamy, G. Srinivasa, et al. Results of the neurips\u201921 challenge on billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2205.03763, 2022.\n12. J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pages 1470\u20131470. IEEE Computer Society, 2003.\n13. D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\n14. C. Waldburger. As search needs evolve, microsoft makes ai tools for better search available to researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft.com/ai/bing-vector-search/.\n15. M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. Proceedings of the VLDB Endowment, 14 (11):1964\u20131978, 2021.\n16. R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces. In VLDB, volume 98, pages 194\u2013205, 1998.\n17. I. H. Witten, I. H. Witten, A. Moffat, T. C. Bell, T. C. Bell, E. Fox, and T. C. Bell. Managing gigabytes: compressing and indexing documents and images. Morgan Kaufmann, 1999.", "md": "1. Y. Malkov, A. Ponomarenko, A. Logvinov, and V. Krylov. Approximate nearest neighbor algorithm based on navigable small world graphs. Information Systems, 45:61\u201368, 2014.\n2. Y. A. Malkov and D. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis & Machine Intelligence, 42(04):824\u2013836, 2020.\n3. K. Mardia, J. Kent, and J. Bibby. Multivariate analysis. Probability and Mathematical Statistics, 1979.\n4. P. Nayak. Understanding searches better than ever before. Google AI Blog, 2019. URL https://blog.google/products/search/search-language-understanding-bert/.\n5. A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy, et al. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005, 2022.\n6. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n7. V. Ramanujan, P. K. A. Vasu, A. Farhadi, O. Tuzel, and H. Pouransari. Forward compatible training for representation learning. arXiv preprint arXiv:2112.02805, 2021.\n8. B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\n9. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015.\n10. R. Salakhutdinov and G. Hinton. Semantic hashing. International Journal of Approximate Reasoning, 50(7):969\u2013978, 2009.\n11. H. V. Simhadri, G. Williams, M. Aum\u00fcller, M. Douze, A. Babenko, D. Baranchuk, Q. Chen, L. Hosseini, R. Krishnaswamy, G. Srinivasa, et al. Results of the neurips\u201921 challenge on billion-scale approximate nearest neighbor search. arXiv preprint arXiv:2205.03763, 2022.\n12. J. Sivic and A. Zisserman. Video google: A text retrieval approach to object matching in videos. In Computer Vision, IEEE International Conference on, volume 3, pages 1470\u20131470. IEEE Computer Society, 2003.\n13. D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):2822\u20132878, 2018.\n14. C. Waldburger. As search needs evolve, microsoft makes ai tools for better search available to researchers and developers. Microsoft AI Blog, 2019. URL https://blogs.microsoft.com/ai/bing-vector-search/.\n15. M. Wang, X. Xu, Q. Yue, and Y. Wang. A comprehensive survey and experimental comparison of graph-based approximate nearest neighbor search. Proceedings of the VLDB Endowment, 14 (11):1964\u20131978, 2021.\n16. R. Weber, H.-J. Schek, and S. Blott. A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces. In VLDB, volume 98, pages 194\u2013205, 1998.\n17. I. H. Witten, I. H. Witten, A. Moffat, T. C. Bell, T. C. Bell, E. Fox, and T. C. Bell. Managing gigabytes: compressing and indexing documents and images. Morgan Kaufmann, 1999."}]}, {"page": 14, "text": "A     AdANNS Framework\nAlgorithm 1 AdANNS-IVF Psuedocode\n# Index database to construct clusters and build inverted file system\ndef adannsConstruction(database, d_cluster, num_clusters):\n     # Slice database with cluster construction dim (d_cluster)\n     xb = database[:d_cluster]\n     cluster_centroids = constructClusters(xb, num_clusters)\n     return cluster_centroids\ndef adannsInference(queries, centroids, d_shortlist, d_search, num_probes,\n      k):\n     # Slice queries and centroids with cluster shortlist dim (d_shortlist)\n     xq = queries[:d_shortlist]\n     xc = centroids[:d_shortlist]\n     for q in queries:\n          # compute distance of query from each cluster centroid\n          candidate_distances = computeDistances(q, xc)\n          # sort cluster candidates by distance and choose small number to\n                probe\n          cluster_candidates = sortAscending(candidate_distances)[:num_probes]\n          database_candidates = getClusterMembers(cluster_candidates)\n          # Linear Scan all shortlisted clusters with search dim (d_search)\n          k_nearest_neighbors[q] = linearScan(q, database_candidates, d_search,\n                 k)\n     return k_nearest_neighbors\n      ANNS                                                                                    ANNS\n  Construction                               \ud835\udf19: \ud835\udc4b\u2192  \u211d!                                    Inference\n    Database                                                                                  Query\n                        Constru ct Clus\n                                      ters with \u211ddc               Select Closest Cluster\n        \u00b5 1                                        \u00b5i                                           \u00b5k\n      \ud835\udc4b \ud835\udf16C1                                    \ud835\udc4b\ud835\udf16C i                                      \ud835\udc4b\ud835\udf16C k\n                                                Linear                  Top k\n                                              Scan with               Relevant\n                                                  \u211dds                Data Points\nFigure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases.\nAdaptive representations can be utilized effectively in the decoupled components of clustering and\nsearching for a better accuracy-compute trade-off (AdANNS-IVF).\n                                                    14", "md": "A     AdANNS Framework\nAlgorithm 1 AdANNS-IVF Psuedocode\n# Index database to construct clusters and build inverted file system\ndef adannsConstruction(database, d_cluster, num_clusters):\n# Slice database with cluster construction dim (d_cluster)\nxb = database[:d_cluster]\ncluster_centroids = constructClusters(xb, num_clusters)\nreturn cluster_centroids\ndef adannsInference(queries, centroids, d_shortlist, d_search, num_probes,\nk):\n# Slice queries and centroids with cluster shortlist dim (d_shortlist)\nxq = queries[:d_shortlist]\nxc = centroids[:d_shortlist]\nfor q in queries:\n# compute distance of query from each cluster centroid\ncandidate_distances = computeDistances(q, xc)\n# sort cluster candidates by distance and choose small number to\nprobe\ncluster_candidates = sortAscending(candidate_distances)[:num_probes]\ndatabase_candidates = getClusterMembers(cluster_candidates)\n# Linear Scan all shortlisted clusters with search dim (d_search)\nk_nearest_neighbors[q] = linearScan(q, database_candidates, d_search,\nk)\nreturn k_nearest_neighbors\nANNS                                                                                    ANNS\nConstruction                               $$\\varphi: X \\rightarrow \\mathbb{R}^d$$                                    Inference\nDatabase                                                                                  Query\nConstru ct Clus\nters with $$\\mathbb{R}^d_c$$               Select Closest Cluster\n\u00b5 1                                        \u00b5i                                           \u00b5k\n$$X \\in C_1$$                                    $$X \\in C_i$$                                      $$X \\in C_k$$\nLinear                  Top k\nScan with               Relevant\n$$\\mathbb{R}^d_s$$                Data Points\nFigure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases.\nAdaptive representations can be utilized effectively in the decoupled components of clustering and\nsearching for a better accuracy-compute trade-off (AdANNS-IVF).\n14", "images": [], "items": [{"type": "text", "value": "A     AdANNS Framework\nAlgorithm 1 AdANNS-IVF Psuedocode", "md": "A     AdANNS Framework\nAlgorithm 1 AdANNS-IVF Psuedocode"}, {"type": "heading", "lvl": 1, "value": "Index database to construct clusters and build inverted file system", "md": "# Index database to construct clusters and build inverted file system"}, {"type": "text", "value": "def adannsConstruction(database, d_cluster, num_clusters):", "md": "def adannsConstruction(database, d_cluster, num_clusters):"}, {"type": "heading", "lvl": 1, "value": "Slice database with cluster construction dim (d_cluster)", "md": "# Slice database with cluster construction dim (d_cluster)"}, {"type": "text", "value": "xb = database[:d_cluster]\ncluster_centroids = constructClusters(xb, num_clusters)\nreturn cluster_centroids\ndef adannsInference(queries, centroids, d_shortlist, d_search, num_probes,\nk):", "md": "xb = database[:d_cluster]\ncluster_centroids = constructClusters(xb, num_clusters)\nreturn cluster_centroids\ndef adannsInference(queries, centroids, d_shortlist, d_search, num_probes,\nk):"}, {"type": "heading", "lvl": 1, "value": "Slice queries and centroids with cluster shortlist dim (d_shortlist)", "md": "# Slice queries and centroids with cluster shortlist dim (d_shortlist)"}, {"type": "text", "value": "xq = queries[:d_shortlist]\nxc = centroids[:d_shortlist]\nfor q in queries:", "md": "xq = queries[:d_shortlist]\nxc = centroids[:d_shortlist]\nfor q in queries:"}, {"type": "heading", "lvl": 1, "value": "compute distance of query from each cluster centroid", "md": "# compute distance of query from each cluster centroid"}, {"type": "text", "value": "candidate_distances = computeDistances(q, xc)", "md": "candidate_distances = computeDistances(q, xc)"}, {"type": "heading", "lvl": 1, "value": "sort cluster candidates by distance and choose small number to", "md": "# sort cluster candidates by distance and choose small number to"}, {"type": "text", "value": "probe\ncluster_candidates = sortAscending(candidate_distances)[:num_probes]\ndatabase_candidates = getClusterMembers(cluster_candidates)", "md": "probe\ncluster_candidates = sortAscending(candidate_distances)[:num_probes]\ndatabase_candidates = getClusterMembers(cluster_candidates)"}, {"type": "heading", "lvl": 1, "value": "Linear Scan all shortlisted clusters with search dim (d_search)", "md": "# Linear Scan all shortlisted clusters with search dim (d_search)"}, {"type": "text", "value": "k_nearest_neighbors[q] = linearScan(q, database_candidates, d_search,\nk)\nreturn k_nearest_neighbors\nANNS                                                                                    ANNS\nConstruction                               $$\\varphi: X \\rightarrow \\mathbb{R}^d$$                                    Inference\nDatabase                                                                                  Query\nConstru ct Clus\nters with $$\\mathbb{R}^d_c$$               Select Closest Cluster\n\u00b5 1                                        \u00b5i                                           \u00b5k\n$$X \\in C_1$$                                    $$X \\in C_i$$                                      $$X \\in C_k$$\nLinear                  Top k\nScan with               Relevant\n$$\\mathbb{R}^d_s$$                Data Points\nFigure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases.\nAdaptive representations can be utilized effectively in the decoupled components of clustering and\nsearching for a better accuracy-compute trade-off (AdANNS-IVF).\n14", "md": "k_nearest_neighbors[q] = linearScan(q, database_candidates, d_search,\nk)\nreturn k_nearest_neighbors\nANNS                                                                                    ANNS\nConstruction                               $$\\varphi: X \\rightarrow \\mathbb{R}^d$$                                    Inference\nDatabase                                                                                  Query\nConstru ct Clus\nters with $$\\mathbb{R}^d_c$$               Select Closest Cluster\n\u00b5 1                                        \u00b5i                                           \u00b5k\n$$X \\in C_1$$                                    $$X \\in C_i$$                                      $$X \\in C_k$$\nLinear                  Top k\nScan with               Relevant\n$$\\mathbb{R}^d_s$$                Data Points\nFigure 5: The schematic of inverted file index (IVF) outlaying the construction and inference phases.\nAdaptive representations can be utilized effectively in the decoupled components of clustering and\nsearching for a better accuracy-compute trade-off (AdANNS-IVF).\n14"}]}, {"page": 15, "text": "Table 2: Mathematical formulae of the retrieval phase across various methods built on IVF. See\nSection 3 for notations.\n        Method                                                   Retrieval Formula during Inference\n       IVF-RR                         arg min j\u2208Ch(q) \u2225\u03d5RR(d)(q) \u2212   \u03d5RR(d)(xj)\u2225, s.t. h(q) = arg minh \u2225\u03d5RR(d)(q) \u2212      \u00b5RR(d)\u2225\n                                                                                                                           h\n       IVF-MR                        arg min j\u2208Ch(q) \u2225\u03d5MR(d)(q) \u2212    \u03d5MR(d)(xj)\u2225, s.t. h(q) = arg minh \u2225\u03d5MR(d)(q) \u2212       \u00b5MR(d)\u2225\n                                                                                                                           h\n    AdANNS-IVF                     arg min j\u2208Ch(q) \u2225\u03d5MR(ds)(q) \u2212    \u03d5MR(ds)(xj)\u2225, s.t. h(q) = arg minh \u2225\u03d5MR(dc)(q) \u2212       \u00b5MR(dc)\u2225\n                                                                                                                            h\n     MG-IVF-RR                      arg min j\u2208Ch(q) \u2225\u03d5RR(ds)(q) \u2212   \u03d5RR(ds)(xj)\u2225, s.t. h(q) = arg minh \u2225\u03d5RR(dc)(q) \u2212      \u00b5RR(dc)\u2225\n                                                                                                                            h\n  AdANNS-IVF-D          arg min  j\u2208Ch(q) \u2225\u03d5MR(d)(q)[1 : \u02c6d] \u2212 \u03d5MR(d)(xj)[1 : \u02c6 d]\u2225, s.t. h(q) = arg minh \u2225\u03d5MR(d)(q)[1 : \u02c6  d] \u2212 \u00b5MR(d)  [1 : \u02c6\n       IVFOPQ                           arg min  j\u2208Ch(q) \u2225\u03d5PQ(m,b)(q) \u2212   \u03d5PQ(m,b)(xj)\u2225, s.t. h(q) = arg minh \u2225\u03d5(q) \u2212      \u00b5h\u2225    h         d]\u2225\nB      Training and Compute Costs\nA bulk of our ANNS experimentation was written with Faiss [24], a library for effi                                         cient similarity\nsearch and clustering. AdANNS was implemented from scratch (Algorithm 1) due to difficulty in\ndecoupling clustering and linear scan with Faiss, with code available at https://github.com/\nRAIVNLab/AdANNS. We also provide a version of AdANNS with Faiss optimizations with the\nrestriction that Dc \u2265          Ds as a limitation of the current implementation, which can be further optimized.\nAll ANNS experiments (AdANNS-IVF, MG-IVF-RR, IVF-MR, IVF-RR, HNSW, HNSWOPQ,\nIVFOPQ) were run on an Intel Xeon 2.20GHz CPU with 12 cores. Exact Search (Flat L2, PQ, OPQ)\nand DiskANN experiments were run with CUDA 11.0 on a A100-SXM4 NVIDIA GPU with 40G\nRAM. The wall-clock inference times quoted in Figure 1a and Table 3 are reported on CPU with\nFaiss optimizations, and are averaged over three inference runs for ImageNet-1K retrieval.\nTable 3: Comparison of AdANNS-IVF and Rigid-IVF wall-clock inference times for ImageNet-1K\nretrieval. AdANNS-IVF has up to \u223c                      1.5% gain over Rigid-IVF for a fixed search latency per query.\n                                  AdANNS-IVF                                                 Rigid-IVF\n                 Top-1        Search Latency/Query (ms)                    Top-1        Search Latency/Query (ms)\n                 70.02                         0.03                        68.51                         0.02\n                 70.08                         0.06                        68.54                         0.05\n                 70.19                         0.06                        68.74                         0.08\n                 70.36                         0.88                        69.20                         0.86\n                 70.60                         5.57                        70.13                         5.67\nDPR [27] on NQ [32].                 We follow the setup on the DPR repo4: the Wikipedia corpus has 21 million\npassages and Natural Questions dataset for open-domain QA settings. The training set contains\n79,168 question and answer pairs, the dev set has 8,757 pairs and the test set has 3,610 pairs.\nB.1      Inference Compute Cost\nWe evaluate inference compute costs for IVF in MegaFLOPS per query (MFLOPS/query) as shown\nin Figures 2, 10a, and 8 as follows:\n                                                         C = dsk + npdsND        k\nwhere dc is the cluster construction embedding dimensionality, ds is the embedding dim used for\nlinear scan within each probed cluster, which is controlled by # of search probes np. Finally, k\nis the number of clusters |Ci| indexed over database of size ND. The default setting in this work,\nunless otherwise stated, is np = 1, k = 1024, ND = 1281167 (ImageNet-1K trainset). Vanilla IVF\nsupports only dc = ds, while AdANNS-IVF provides flexibility via decoupling clustering and search\n(Section 4). AdANNS-IVF-D is a special case of AdANNS-IVF with the flexibility restricted to\ninference, i.e., dc is a fixed high-dimensional MR.\n     4https://github.com/facebookresearch/DPR\n                                                                       15", "md": "|Method|Retrieval Formula during Inference|\n|---|---|\n|IVF-RR|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{RR}(d)(q) - \\phi_{RR}(d)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{RR}(d)(q) - \\mu_{RR}(d) \\right\\|_h$|\n|IVF-MR|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{MR}(d)(q) - \\phi_{MR}(d)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{MR}(d)(q) - \\mu_{MR}(d) \\right\\|_h$|\n|AdANNS-IVF|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{MR}(ds)(q) - \\phi_{MR}(ds)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{MR}(dc)(q) - \\mu_{MR}(dc) \\right\\|_h$|\n|MG-IVF-RR|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{RR}(ds)(q) - \\phi_{RR}(ds)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{RR}(dc)(q) - \\mu_{RR}(dc) \\right\\|_h$|\n|AdANNS-IVF-D|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{MR}(d)(q)[1 : \\hat{d}] - \\phi_{MR}(d)(x_j)[1 : \\hat{d}] \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{MR}(d)(q)[1 : \\hat{d}] - \\mu_{MR}(d)[1 : \\hat{d}] \\right\\|$|\n|IVFOPQ|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{PQ}(m,b)(q) - \\phi_{PQ}(m,b)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi(q) - \\mu_h \\right\\|_h$|\n\nTraining and Compute Costs\n\nA bulk of our ANNS experimentation was written with Faiss [24], a library for efficient similarity search and clustering. AdANNS was implemented from scratch (Algorithm 1) due to difficulty in decoupling clustering and linear scan with Faiss, with code available at https://github.com/RAIVNLab/AdANNS. We also provide a version of AdANNS with Faiss optimizations with the restriction that \\(Dc \\ge Ds\\) as a limitation of the current implementation, which can be further optimized.\n\nAll ANNS experiments (AdANNS-IVF, MG-IVF-RR, IVF-MR, IVF-RR, HNSW, HNSWOPQ, IVFOPQ) were run on an Intel Xeon 2.20GHz CPU with 12 cores. Exact Search (Flat L2, PQ, OPQ) and DiskANN experiments were run with CUDA 11.0 on a A100-SXM4 NVIDIA GPU with 40G RAM. The wall-clock inference times quoted in Figure 1a and Table 3 are reported on CPU with Faiss optimizations, and are averaged over three inference runs for ImageNet-1K retrieval.\n\n|AdANNS-IVF|Rigid-IVF|\n|---|---|\n|Top-1|Search Latency/Query (ms)|\n|70.02|0.03|\n|70.08|0.06|\n|70.19|0.06|\n|70.36|0.88|\n|70.60|5.57|\n\nDPR [27] on NQ [32]\n\nWe follow the setup on the DPR repo: the Wikipedia corpus has 21 million passages and Natural Questions dataset for open-domain QA settings. The training set contains 79,168 question and answer pairs, the dev set has 8,757 pairs and the test set has 3,610 pairs.\n\nInference Compute Cost\n\nWe evaluate inference compute costs for IVF in MegaFLOPS per query (MFLOPS/query) as shown in Figures 2, 10a, and 8 as follows:\n\n$$C = dsk + npdsNDk$$\n\nwhere \\(dc\\) is the cluster construction embedding dimensionality, \\(ds\\) is the embedding dim used for linear scan within each probed cluster, which is controlled by # of search probes np. Finally, \\(k\\) is the number of clusters \\(|Ci|\\) indexed over database of size \\(ND\\). The default setting in this work, unless otherwise stated, is np = 1, k = 1024, ND = 1281167 (ImageNet-1K trainset). Vanilla IVF supports only \\(dc = ds\\), while AdANNS-IVF provides flexibility via decoupling clustering and search (Section 4). AdANNS-IVF-D is a special case of AdANNS-IVF with the flexibility restricted to inference, i.e., \\(dc\\) is a fixed high-dimensional MR.", "images": [], "items": [{"type": "table", "rows": [["Method", "Retrieval Formula during Inference"], ["IVF-RR", "$\\arg \\min_{j\\in Ch(q)} \\left\\", "\\phi_{RR}(d)(q) - \\phi_{RR}(d)(x_j) \\right\\", ", s.t. h(q) = \\arg \\min_h \\left\\", "\\phi_{RR}(d)(q) - \\mu_{RR}(d) \\right\\", "_h$"], ["IVF-MR", "$\\arg \\min_{j\\in Ch(q)} \\left\\", "\\phi_{MR}(d)(q) - \\phi_{MR}(d)(x_j) \\right\\", ", s.t. h(q) = \\arg \\min_h \\left\\", "\\phi_{MR}(d)(q) - \\mu_{MR}(d) \\right\\", "_h$"], ["AdANNS-IVF", "$\\arg \\min_{j\\in Ch(q)} \\left\\", "\\phi_{MR}(ds)(q) - \\phi_{MR}(ds)(x_j) \\right\\", ", s.t. h(q) = \\arg \\min_h \\left\\", "\\phi_{MR}(dc)(q) - \\mu_{MR}(dc) \\right\\", "_h$"], ["MG-IVF-RR", "$\\arg \\min_{j\\in Ch(q)} \\left\\", "\\phi_{RR}(ds)(q) - \\phi_{RR}(ds)(x_j) \\right\\", ", s.t. h(q) = \\arg \\min_h \\left\\", "\\phi_{RR}(dc)(q) - \\mu_{RR}(dc) \\right\\", "_h$"], ["AdANNS-IVF-D", "$\\arg \\min_{j\\in Ch(q)} \\left\\", "\\phi_{MR}(d)(q)[1 : \\hat{d}] - \\phi_{MR}(d)(x_j)[1 : \\hat{d}] \\right\\", ", s.t. h(q) = \\arg \\min_h \\left\\", "\\phi_{MR}(d)(q)[1 : \\hat{d}] - \\mu_{MR}(d)[1 : \\hat{d}] \\right\\", "$"], ["IVFOPQ", "$\\arg \\min_{j\\in Ch(q)} \\left\\", "\\phi_{PQ}(m,b)(q) - \\phi_{PQ}(m,b)(x_j) \\right\\", ", s.t. h(q) = \\arg \\min_h \\left\\", "\\phi(q) - \\mu_h \\right\\", "_h$"]], "md": "|Method|Retrieval Formula during Inference|\n|---|---|\n|IVF-RR|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{RR}(d)(q) - \\phi_{RR}(d)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{RR}(d)(q) - \\mu_{RR}(d) \\right\\|_h$|\n|IVF-MR|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{MR}(d)(q) - \\phi_{MR}(d)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{MR}(d)(q) - \\mu_{MR}(d) \\right\\|_h$|\n|AdANNS-IVF|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{MR}(ds)(q) - \\phi_{MR}(ds)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{MR}(dc)(q) - \\mu_{MR}(dc) \\right\\|_h$|\n|MG-IVF-RR|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{RR}(ds)(q) - \\phi_{RR}(ds)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{RR}(dc)(q) - \\mu_{RR}(dc) \\right\\|_h$|\n|AdANNS-IVF-D|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{MR}(d)(q)[1 : \\hat{d}] - \\phi_{MR}(d)(x_j)[1 : \\hat{d}] \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi_{MR}(d)(q)[1 : \\hat{d}] - \\mu_{MR}(d)[1 : \\hat{d}] \\right\\|$|\n|IVFOPQ|$\\arg \\min_{j\\in Ch(q)} \\left\\| \\phi_{PQ}(m,b)(q) - \\phi_{PQ}(m,b)(x_j) \\right\\|, s.t. h(q) = \\arg \\min_h \\left\\| \\phi(q) - \\mu_h \\right\\|_h$|", "isPerfectTable": false, "csv": "\"Method\",\"Retrieval Formula during Inference\"\n\"IVF-RR\",\"$\\arg \\min_{j\\in Ch(q)} \\left\\\",\"\\phi_{RR}(d)(q) - \\phi_{RR}(d)(x_j) \\right\\\",\", s.t. h(q) = \\arg \\min_h \\left\\\",\"\\phi_{RR}(d)(q) - \\mu_{RR}(d) \\right\\\",\"_h$\"\n\"IVF-MR\",\"$\\arg \\min_{j\\in Ch(q)} \\left\\\",\"\\phi_{MR}(d)(q) - \\phi_{MR}(d)(x_j) \\right\\\",\", s.t. h(q) = \\arg \\min_h \\left\\\",\"\\phi_{MR}(d)(q) - \\mu_{MR}(d) \\right\\\",\"_h$\"\n\"AdANNS-IVF\",\"$\\arg \\min_{j\\in Ch(q)} \\left\\\",\"\\phi_{MR}(ds)(q) - \\phi_{MR}(ds)(x_j) \\right\\\",\", s.t. h(q) = \\arg \\min_h \\left\\\",\"\\phi_{MR}(dc)(q) - \\mu_{MR}(dc) \\right\\\",\"_h$\"\n\"MG-IVF-RR\",\"$\\arg \\min_{j\\in Ch(q)} \\left\\\",\"\\phi_{RR}(ds)(q) - \\phi_{RR}(ds)(x_j) \\right\\\",\", s.t. h(q) = \\arg \\min_h \\left\\\",\"\\phi_{RR}(dc)(q) - \\mu_{RR}(dc) \\right\\\",\"_h$\"\n\"AdANNS-IVF-D\",\"$\\arg \\min_{j\\in Ch(q)} \\left\\\",\"\\phi_{MR}(d)(q)[1 : \\hat{d}] - \\phi_{MR}(d)(x_j)[1 : \\hat{d}] \\right\\\",\", s.t. h(q) = \\arg \\min_h \\left\\\",\"\\phi_{MR}(d)(q)[1 : \\hat{d}] - \\mu_{MR}(d)[1 : \\hat{d}] \\right\\\",\"$\"\n\"IVFOPQ\",\"$\\arg \\min_{j\\in Ch(q)} \\left\\\",\"\\phi_{PQ}(m,b)(q) - \\phi_{PQ}(m,b)(x_j) \\right\\\",\", s.t. h(q) = \\arg \\min_h \\left\\\",\"\\phi(q) - \\mu_h \\right\\\",\"_h$\""}, {"type": "text", "value": "Training and Compute Costs\n\nA bulk of our ANNS experimentation was written with Faiss [24], a library for efficient similarity search and clustering. AdANNS was implemented from scratch (Algorithm 1) due to difficulty in decoupling clustering and linear scan with Faiss, with code available at https://github.com/RAIVNLab/AdANNS. We also provide a version of AdANNS with Faiss optimizations with the restriction that \\(Dc \\ge Ds\\) as a limitation of the current implementation, which can be further optimized.\n\nAll ANNS experiments (AdANNS-IVF, MG-IVF-RR, IVF-MR, IVF-RR, HNSW, HNSWOPQ, IVFOPQ) were run on an Intel Xeon 2.20GHz CPU with 12 cores. Exact Search (Flat L2, PQ, OPQ) and DiskANN experiments were run with CUDA 11.0 on a A100-SXM4 NVIDIA GPU with 40G RAM. The wall-clock inference times quoted in Figure 1a and Table 3 are reported on CPU with Faiss optimizations, and are averaged over three inference runs for ImageNet-1K retrieval.", "md": "Training and Compute Costs\n\nA bulk of our ANNS experimentation was written with Faiss [24], a library for efficient similarity search and clustering. AdANNS was implemented from scratch (Algorithm 1) due to difficulty in decoupling clustering and linear scan with Faiss, with code available at https://github.com/RAIVNLab/AdANNS. We also provide a version of AdANNS with Faiss optimizations with the restriction that \\(Dc \\ge Ds\\) as a limitation of the current implementation, which can be further optimized.\n\nAll ANNS experiments (AdANNS-IVF, MG-IVF-RR, IVF-MR, IVF-RR, HNSW, HNSWOPQ, IVFOPQ) were run on an Intel Xeon 2.20GHz CPU with 12 cores. Exact Search (Flat L2, PQ, OPQ) and DiskANN experiments were run with CUDA 11.0 on a A100-SXM4 NVIDIA GPU with 40G RAM. The wall-clock inference times quoted in Figure 1a and Table 3 are reported on CPU with Faiss optimizations, and are averaged over three inference runs for ImageNet-1K retrieval."}, {"type": "table", "rows": [["AdANNS-IVF", "Rigid-IVF"], ["Top-1", "Search Latency/Query (ms)"], ["70.02", "0.03"], ["70.08", "0.06"], ["70.19", "0.06"], ["70.36", "0.88"], ["70.60", "5.57"]], "md": "|AdANNS-IVF|Rigid-IVF|\n|---|---|\n|Top-1|Search Latency/Query (ms)|\n|70.02|0.03|\n|70.08|0.06|\n|70.19|0.06|\n|70.36|0.88|\n|70.60|5.57|", "isPerfectTable": true, "csv": "\"AdANNS-IVF\",\"Rigid-IVF\"\n\"Top-1\",\"Search Latency/Query (ms)\"\n\"70.02\",\"0.03\"\n\"70.08\",\"0.06\"\n\"70.19\",\"0.06\"\n\"70.36\",\"0.88\"\n\"70.60\",\"5.57\""}, {"type": "text", "value": "DPR [27] on NQ [32]\n\nWe follow the setup on the DPR repo: the Wikipedia corpus has 21 million passages and Natural Questions dataset for open-domain QA settings. The training set contains 79,168 question and answer pairs, the dev set has 8,757 pairs and the test set has 3,610 pairs.\n\nInference Compute Cost\n\nWe evaluate inference compute costs for IVF in MegaFLOPS per query (MFLOPS/query) as shown in Figures 2, 10a, and 8 as follows:\n\n$$C = dsk + npdsNDk$$\n\nwhere \\(dc\\) is the cluster construction embedding dimensionality, \\(ds\\) is the embedding dim used for linear scan within each probed cluster, which is controlled by # of search probes np. Finally, \\(k\\) is the number of clusters \\(|Ci|\\) indexed over database of size \\(ND\\). The default setting in this work, unless otherwise stated, is np = 1, k = 1024, ND = 1281167 (ImageNet-1K trainset). Vanilla IVF supports only \\(dc = ds\\), while AdANNS-IVF provides flexibility via decoupling clustering and search (Section 4). AdANNS-IVF-D is a special case of AdANNS-IVF with the flexibility restricted to inference, i.e., \\(dc\\) is a fixed high-dimensional MR.", "md": "DPR [27] on NQ [32]\n\nWe follow the setup on the DPR repo: the Wikipedia corpus has 21 million passages and Natural Questions dataset for open-domain QA settings. The training set contains 79,168 question and answer pairs, the dev set has 8,757 pairs and the test set has 3,610 pairs.\n\nInference Compute Cost\n\nWe evaluate inference compute costs for IVF in MegaFLOPS per query (MFLOPS/query) as shown in Figures 2, 10a, and 8 as follows:\n\n$$C = dsk + npdsNDk$$\n\nwhere \\(dc\\) is the cluster construction embedding dimensionality, \\(ds\\) is the embedding dim used for linear scan within each probed cluster, which is controlled by # of search probes np. Finally, \\(k\\) is the number of clusters \\(|Ci|\\) indexed over database of size \\(ND\\). The default setting in this work, unless otherwise stated, is np = 1, k = 1024, ND = 1281167 (ImageNet-1K trainset). Vanilla IVF supports only \\(dc = ds\\), while AdANNS-IVF provides flexibility via decoupling clustering and search (Section 4). AdANNS-IVF-D is a special case of AdANNS-IVF with the flexibility restricted to inference, i.e., \\(dc\\) is a fixed high-dimensional MR."}]}, {"page": 16, "text": " C    Evaluation Metrics\n In this work, we primarily use top-1 accuracy (i.e. 1-Nearest Neighbor), recall@k, corrected mean\n average precision (mAP@k) [30] and k-Recall@N (recall score), which are defined over all queries\n Q over indexed database of size ND as:\n                               top-1 =   Q correct_pred@1\n                                                 |Q|\n                          Recall@k =     Q correct_pred@k     \u2217  num_classes\n                                                 |Q|                |ND|\nwhere correct_pred@k is the number of k-NN with correctly predicted labels for a given query. As\n noted in Section 3, k-Recall@N is the overlap between k exact search nearest neighbors (considered\n as ground truth) and the top N retrieved documents. As Faiss [24] supports a maximum of 2048-\n NN while searching the indexed database, we report 40-Recall@2048 in Figure 13. Also note\n that for ImageNet-1K, which constitutes a bulk of the experimentation in this work, |Q| = 50000,\n |ND| = 1281167 and num_classes = 1000. For ImageNetv2 [44], |Q| = 10000 and num_classes\n = 1000, and for ImageNet-4K [31], |Q| = 210100, |ND| = 4202000 and num_classes = 4202. For\n NQ [32], |Q| = 3610 and |ND| = 21015324. As NQ consists of question-answer pairs (instance-\n level), num_classes = 3610 for the test set.\n                                                        70\n      Top-1 Accuracy (%)\n       70                                              Top-1 Accuracy (%)\n       68                                               68\n       66                           AdANNS-OPQ          66                      AdANNS-IVF+OPQ\n                                    OPQ-MR-2048                                 IVF+OPQ-MR-2048\n       64                           Rigid-OPQ                                   Rigid-IVF+OPQ\n           8           16           32          64      64  8           16           32          64\n                 Compute Budget (Bytes)                           Compute Budget (Bytes)\n        (a) Exact Search + OPQ on ImageNet-1K                (b) IVF + OPQ on ImageNet-1K\n                                                        70\n     Top-1 Accuracy (%)                                Top-1 Accuracy (%)\n       70.6\n                                                        68\n       70.2\n                                                        66\n       69.8                AdANNS-DiskANN+OPQ                                AdANNS-HNSW+OPQ\n                           DiskANN+OPQ-MR-2048          64                   HNSW+OPQ-MR-2048\n       69.4                Rigid-DiskANN+OPQ                                 Rigid-HNSW+OPQ\n             8          16          32          64          8           16           32          64\n                  Compute Budget (Bytes)                          Compute Budget (Bytes)\n         (c) DiskANN + OPQ on ImageNet-1K                   (d) HNSW + OPQ on ImageNet-1K\n Figure 6: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared\n to MR and Rigid baselines models on ImageNet-1K and Natural Questions.\n                                                   16", "md": "## Evaluation Metrics\n\nIn this work, we primarily use top-1 accuracy (i.e. 1-Nearest Neighbor), recall@k, corrected mean average precision (mAP@k) [30] and k-Recall@N (recall score), which are defined over all queries Q over indexed database of size ND as:\n\n$$\n\\text{top-1} = \\frac{Q \\text{ correct\\_pred@1}}{|Q|} \\\\\n\\text{Recall@k} = \\frac{Q \\text{ correct\\_pred@k} \\times \\text{num\\_classes}}{|Q| \\times |ND|}\n$$\n\nwhere correct_pred@k is the number of k-NN with correctly predicted labels for a given query. As noted in Section 3, k-Recall@N is the overlap between k exact search nearest neighbors (considered as ground truth) and the top N retrieved documents. As Faiss [24] supports a maximum of 2048-NN while searching the indexed database, we report 40-Recall@2048 in Figure 13. Also note that for ImageNet-1K, which constitutes a bulk of the experimentation in this work, |Q| = 50000, |ND| = 1281167 and num_classes = 1000. For ImageNetv2 [44], |Q| = 10000 and num_classes = 1000, and for ImageNet-4K [31], |Q| = 210100, |ND| = 4202000 and num_classes = 4202. For NQ [32], |Q| = 3610 and |ND| = 21015324. As NQ consists of question-answer pairs (instance-level), num_classes = 3610 for the test set.\n\n### Top-1 Accuracy (%)\n\n| |AdANNS-OPQ|AdANNS-IVF+OPQ|\n|---|---|---|\n|OPQ-MR-2048|68|64|\n|Rigid-OPQ|66|64|\n\n(a) Exact Search + OPQ on ImageNet-1K\n\n### Top-1 Accuracy (%)\n\n| |AdANNS-DiskANN+OPQ|AdANNS-HNSW+OPQ|\n|---|---|---|\n|DiskANN+OPQ-MR-2048|70.2|69.4|\n|Rigid-DiskANN+OPQ|70.6|69.8|\n\n(b) IVF + OPQ on ImageNet-1K\n\nFigure 6: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared to MR and Rigid baselines models on ImageNet-1K and Natural Questions.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Evaluation Metrics", "md": "## Evaluation Metrics"}, {"type": "text", "value": "In this work, we primarily use top-1 accuracy (i.e. 1-Nearest Neighbor), recall@k, corrected mean average precision (mAP@k) [30] and k-Recall@N (recall score), which are defined over all queries Q over indexed database of size ND as:\n\n$$\n\\text{top-1} = \\frac{Q \\text{ correct\\_pred@1}}{|Q|} \\\\\n\\text{Recall@k} = \\frac{Q \\text{ correct\\_pred@k} \\times \\text{num\\_classes}}{|Q| \\times |ND|}\n$$\n\nwhere correct_pred@k is the number of k-NN with correctly predicted labels for a given query. As noted in Section 3, k-Recall@N is the overlap between k exact search nearest neighbors (considered as ground truth) and the top N retrieved documents. As Faiss [24] supports a maximum of 2048-NN while searching the indexed database, we report 40-Recall@2048 in Figure 13. Also note that for ImageNet-1K, which constitutes a bulk of the experimentation in this work, |Q| = 50000, |ND| = 1281167 and num_classes = 1000. For ImageNetv2 [44], |Q| = 10000 and num_classes = 1000, and for ImageNet-4K [31], |Q| = 210100, |ND| = 4202000 and num_classes = 4202. For NQ [32], |Q| = 3610 and |ND| = 21015324. As NQ consists of question-answer pairs (instance-level), num_classes = 3610 for the test set.", "md": "In this work, we primarily use top-1 accuracy (i.e. 1-Nearest Neighbor), recall@k, corrected mean average precision (mAP@k) [30] and k-Recall@N (recall score), which are defined over all queries Q over indexed database of size ND as:\n\n$$\n\\text{top-1} = \\frac{Q \\text{ correct\\_pred@1}}{|Q|} \\\\\n\\text{Recall@k} = \\frac{Q \\text{ correct\\_pred@k} \\times \\text{num\\_classes}}{|Q| \\times |ND|}\n$$\n\nwhere correct_pred@k is the number of k-NN with correctly predicted labels for a given query. As noted in Section 3, k-Recall@N is the overlap between k exact search nearest neighbors (considered as ground truth) and the top N retrieved documents. As Faiss [24] supports a maximum of 2048-NN while searching the indexed database, we report 40-Recall@2048 in Figure 13. Also note that for ImageNet-1K, which constitutes a bulk of the experimentation in this work, |Q| = 50000, |ND| = 1281167 and num_classes = 1000. For ImageNetv2 [44], |Q| = 10000 and num_classes = 1000, and for ImageNet-4K [31], |Q| = 210100, |ND| = 4202000 and num_classes = 4202. For NQ [32], |Q| = 3610 and |ND| = 21015324. As NQ consists of question-answer pairs (instance-level), num_classes = 3610 for the test set."}, {"type": "heading", "lvl": 3, "value": "Top-1 Accuracy (%)", "md": "### Top-1 Accuracy (%)"}, {"type": "table", "rows": [["", "AdANNS-OPQ", "AdANNS-IVF+OPQ"], ["OPQ-MR-2048", "68", "64"], ["Rigid-OPQ", "66", "64"]], "md": "| |AdANNS-OPQ|AdANNS-IVF+OPQ|\n|---|---|---|\n|OPQ-MR-2048|68|64|\n|Rigid-OPQ|66|64|", "isPerfectTable": true, "csv": "\"\",\"AdANNS-OPQ\",\"AdANNS-IVF+OPQ\"\n\"OPQ-MR-2048\",\"68\",\"64\"\n\"Rigid-OPQ\",\"66\",\"64\""}, {"type": "text", "value": "(a) Exact Search + OPQ on ImageNet-1K", "md": "(a) Exact Search + OPQ on ImageNet-1K"}, {"type": "heading", "lvl": 3, "value": "Top-1 Accuracy (%)", "md": "### Top-1 Accuracy (%)"}, {"type": "table", "rows": [["", "AdANNS-DiskANN+OPQ", "AdANNS-HNSW+OPQ"], ["DiskANN+OPQ-MR-2048", "70.2", "69.4"], ["Rigid-DiskANN+OPQ", "70.6", "69.8"]], "md": "| |AdANNS-DiskANN+OPQ|AdANNS-HNSW+OPQ|\n|---|---|---|\n|DiskANN+OPQ-MR-2048|70.2|69.4|\n|Rigid-DiskANN+OPQ|70.6|69.8|", "isPerfectTable": true, "csv": "\"\",\"AdANNS-DiskANN+OPQ\",\"AdANNS-HNSW+OPQ\"\n\"DiskANN+OPQ-MR-2048\",\"70.2\",\"69.4\"\n\"Rigid-DiskANN+OPQ\",\"70.6\",\"69.8\""}, {"type": "text", "value": "(b) IVF + OPQ on ImageNet-1K\n\nFigure 6: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared to MR and Rigid baselines models on ImageNet-1K and Natural Questions.", "md": "(b) IVF + OPQ on ImageNet-1K\n\nFigure 6: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared to MR and Rigid baselines models on ImageNet-1K and Natural Questions."}]}, {"page": 17, "text": "D    AdANNS-OPQ\nIn this section, we take a deeper dive into the quantization characteristics of MR. In this work, we\nrestrict our focus to optimized product quantization (OPQ) [13], which adds a learned space rotation\nand dimensionality permutation to PQ\u2019s sub-vector quantization to learn more optimal PQ codes. We\ncompare OPQ to vanilla PQ on ImageNet in Table 4, and observe large gains at larger embedding\ndimensionalities, which agrees with the findings of Jayaram Subramanya et al. [22].\nWe perform a study of composite OPQ m \u00d7 b indices on ImageNet-1K across compression compute\nbudgets m (where b = 8, i.e. 1 Byte), i.e. Exact Search with OPQ, IVF+OPQ, HNSW+OPQ, and\nDiskANN+OPQ, as seen in Figure 6. It is evident from these results:\n1. Learning OPQ codebooks with AdANNS (Figure 6a) provides a 1-5% gain in top-1 accuracy\n   over rigid representations at low compute budgets (\u2264      32 Bytes). AdANNS-OPQ saturates to\n   Rigid-OPQ performance at low compression (\u2265        64 Bytes).\n2. For IVF, learning clusters with MRs instead of RRs (Figure 6b) provides substantial gains (1-\n   4%). In contrast to Exact-OPQ, using AdANNS for learning OPQ codebooks does not provide\n   substantial top-1 accuracy gains over MR with d = 2048 (highest), though it is still slightly better\n   or equal to MR-2048 at all compute budgets. This further supports that IVF performance generally\n   scales with embedding dimensionality, which is consistent with our findings on ImageNet across\n   robustness variants and encoders (See Figures 9 and 15 respectively).\n3. Note that in contrast to Exact, IVF, and HNSW coarse quantizers, DiskANN inherently re-ranks\n   the retrieved shortlist with high-precision embeddings (d = 2048), which is reflected in its high\n   top-1 accuracy. We find that AdANNS with 8-byte OPQ (Figure 6c) matches the top-1 accuracy\n   of rigid representations using 32-byte OPQ, for a 4\u00d7 cost reduction for the same accuracy. Also\n   note that using AdANNS provides large gains over using MR-2048 at high compression (1.5%),\n   highlighting the necessity of AdANNS\u2019s flexibility for high-precision retrieval at low compute\n   budgets.\n4. Our findings on the HNSW-OPQ composite index (Figure 6d) are consistent with all other indices,\n   i.e. HNSW graphs constructed with AdANNS OPQ codebooks provide significant gains over RR\n   and MR, especially at high compression (\u2264     32 Bytes).\nOPQ on NQ dataset\n                                                       40\n       45\n      Top-1 Accuracy (%)                              Top-1 Accuracy (%)\n       40                                              35\n       35                                              30\n       30                                              25\n       25                         AdANNS-OPQ           20                      AdANNS-IVF+OPQ\n       20                         OPQ-MR-768                                   IVF+OPQ-MR-768\n                                  Rigid-OPQ            15                      Rigid-IVF+OPQ\n           8      16     32     48     64     96           8      16     32     48      64     96\n                Compute Budget (Bytes)                           Compute Budget (Bytes)\n     (a) Exact Search + OPQ on Natural Questions          (b) IVF + OPQ on Natural Questions\nFigure 7: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared\nto MR and Rigid baselines models on Natural Questions.\nOur observations on ImageNet with ResNet-50 MR across search structures also extend to the Natural\nQuestions dataset with Dense Passage Retriever (DPR with BERT-Base MR embeddings). We note\nthat AdANNS provides gains over RR-768 embeddings for both Exact Search and IVF with OPQ\n(Figure 7a and 7b). We find that similar to ImageNet (Figure 15) IVF performance on Natural\nQuestions generally scales with dimensionality. AdANNS thus reduces to MR-768 performance\nfor M \u2265   16. See Appendix G for a more in-depth discussion of AdANNS with DPR on Natural\nQuestions.\n                                                  17", "md": "In this section, we take a deeper dive into the quantization characteristics of MR. In this work, we\nrestrict our focus to optimized product quantization (OPQ) [13], which adds a learned space rotation\nand dimensionality permutation to PQ\u2019s sub-vector quantization to learn more optimal PQ codes. We\ncompare OPQ to vanilla PQ on ImageNet in Table 4, and observe large gains at larger embedding\ndimensionalities, which agrees with the findings of Jayaram Subramanya et al. [22].\n\nWe perform a study of composite OPQ m \u00d7 b indices on ImageNet-1K across compression compute\nbudgets m (where b = 8, i.e. 1 Byte), i.e. Exact Search with OPQ, IVF+OPQ, HNSW+OPQ, and\nDiskANN+OPQ, as seen in Figure 6. It is evident from these results:\n\n1. Learning OPQ codebooks with AdANNS (Figure 6a) provides a 1-5% gain in top-1 accuracy\nover rigid representations at low compute budgets (\u2264 32 Bytes). AdANNS-OPQ saturates to\nRigid-OPQ performance at low compression (\u2265 64 Bytes).\n2. For IVF, learning clusters with MRs instead of RRs (Figure 6b) provides substantial gains (1-4%). In contrast to Exact-OPQ, using AdANNS for learning OPQ codebooks does not provide\nsubstantial top-1 accuracy gains over MR with d = 2048 (highest), though it is still slightly better\nor equal to MR-2048 at all compute budgets. This further supports that IVF performance generally\nscales with embedding dimensionality, which is consistent with our findings on ImageNet across\nrobustness variants and encoders (See Figures 9 and 15 respectively).\n3. Note that in contrast to Exact, IVF, and HNSW coarse quantizers, DiskANN inherently re-ranks\nthe retrieved shortlist with high-precision embeddings (d = 2048), which is reflected in its high\ntop-1 accuracy. We find that AdANNS with 8-byte OPQ (Figure 6c) matches the top-1 accuracy\nof rigid representations using 32-byte OPQ, for a 4\u00d7 cost reduction for the same accuracy. Also\nnote that using AdANNS provides large gains over using MR-2048 at high compression (1.5%),\nhighlighting the necessity of AdANNS\u2019s flexibility for high-precision retrieval at low compute\nbudgets.\n4. Our findings on the HNSW-OPQ composite index (Figure 6d) are consistent with all other indices,\ni.e. HNSW graphs constructed with AdANNS OPQ codebooks provide significant gains over RR\nand MR, especially at high compression (\u2264 32 Bytes).\n\nOPQ on NQ dataset\n\n| |8|16|32|48|64|96|\n|---|---|---|---|---|---|---|\n|Top-1 Accuracy (%)|40|35|30|25|20|AdANNS-OPQ|\n| | | | | | |OPQ-MR-768|\n| | | | | | |Rigid-OPQ|\n| |Compute Budget (Bytes)| | | | | |\n| |AdANNS-IVF+OPQ|IVF+OPQ-MR-768|Rigid-IVF+OPQ| | | |\n\nFigure 7: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared\nto MR and Rigid baselines models on Natural Questions.\n\nOur observations on ImageNet with ResNet-50 MR across search structures also extend to the Natural\nQuestions dataset with Dense Passage Retriever (DPR with BERT-Base MR embeddings). We note\nthat AdANNS provides gains over RR-768 embeddings for both Exact Search and IVF with OPQ\n(Figure 7a and 7b). We find that similar to ImageNet (Figure 15) IVF performance on Natural\nQuestions generally scales with dimensionality. AdANNS thus reduces to MR-768 performance\nfor M \u2265 16. See Appendix G for a more in-depth discussion of AdANNS with DPR on Natural\nQuestions.\n\n17", "images": [], "items": [{"type": "text", "value": "In this section, we take a deeper dive into the quantization characteristics of MR. In this work, we\nrestrict our focus to optimized product quantization (OPQ) [13], which adds a learned space rotation\nand dimensionality permutation to PQ\u2019s sub-vector quantization to learn more optimal PQ codes. We\ncompare OPQ to vanilla PQ on ImageNet in Table 4, and observe large gains at larger embedding\ndimensionalities, which agrees with the findings of Jayaram Subramanya et al. [22].\n\nWe perform a study of composite OPQ m \u00d7 b indices on ImageNet-1K across compression compute\nbudgets m (where b = 8, i.e. 1 Byte), i.e. Exact Search with OPQ, IVF+OPQ, HNSW+OPQ, and\nDiskANN+OPQ, as seen in Figure 6. It is evident from these results:\n\n1. Learning OPQ codebooks with AdANNS (Figure 6a) provides a 1-5% gain in top-1 accuracy\nover rigid representations at low compute budgets (\u2264 32 Bytes). AdANNS-OPQ saturates to\nRigid-OPQ performance at low compression (\u2265 64 Bytes).\n2. For IVF, learning clusters with MRs instead of RRs (Figure 6b) provides substantial gains (1-4%). In contrast to Exact-OPQ, using AdANNS for learning OPQ codebooks does not provide\nsubstantial top-1 accuracy gains over MR with d = 2048 (highest), though it is still slightly better\nor equal to MR-2048 at all compute budgets. This further supports that IVF performance generally\nscales with embedding dimensionality, which is consistent with our findings on ImageNet across\nrobustness variants and encoders (See Figures 9 and 15 respectively).\n3. Note that in contrast to Exact, IVF, and HNSW coarse quantizers, DiskANN inherently re-ranks\nthe retrieved shortlist with high-precision embeddings (d = 2048), which is reflected in its high\ntop-1 accuracy. We find that AdANNS with 8-byte OPQ (Figure 6c) matches the top-1 accuracy\nof rigid representations using 32-byte OPQ, for a 4\u00d7 cost reduction for the same accuracy. Also\nnote that using AdANNS provides large gains over using MR-2048 at high compression (1.5%),\nhighlighting the necessity of AdANNS\u2019s flexibility for high-precision retrieval at low compute\nbudgets.\n4. Our findings on the HNSW-OPQ composite index (Figure 6d) are consistent with all other indices,\ni.e. HNSW graphs constructed with AdANNS OPQ codebooks provide significant gains over RR\nand MR, especially at high compression (\u2264 32 Bytes).\n\nOPQ on NQ dataset", "md": "In this section, we take a deeper dive into the quantization characteristics of MR. In this work, we\nrestrict our focus to optimized product quantization (OPQ) [13], which adds a learned space rotation\nand dimensionality permutation to PQ\u2019s sub-vector quantization to learn more optimal PQ codes. We\ncompare OPQ to vanilla PQ on ImageNet in Table 4, and observe large gains at larger embedding\ndimensionalities, which agrees with the findings of Jayaram Subramanya et al. [22].\n\nWe perform a study of composite OPQ m \u00d7 b indices on ImageNet-1K across compression compute\nbudgets m (where b = 8, i.e. 1 Byte), i.e. Exact Search with OPQ, IVF+OPQ, HNSW+OPQ, and\nDiskANN+OPQ, as seen in Figure 6. It is evident from these results:\n\n1. Learning OPQ codebooks with AdANNS (Figure 6a) provides a 1-5% gain in top-1 accuracy\nover rigid representations at low compute budgets (\u2264 32 Bytes). AdANNS-OPQ saturates to\nRigid-OPQ performance at low compression (\u2265 64 Bytes).\n2. For IVF, learning clusters with MRs instead of RRs (Figure 6b) provides substantial gains (1-4%). In contrast to Exact-OPQ, using AdANNS for learning OPQ codebooks does not provide\nsubstantial top-1 accuracy gains over MR with d = 2048 (highest), though it is still slightly better\nor equal to MR-2048 at all compute budgets. This further supports that IVF performance generally\nscales with embedding dimensionality, which is consistent with our findings on ImageNet across\nrobustness variants and encoders (See Figures 9 and 15 respectively).\n3. Note that in contrast to Exact, IVF, and HNSW coarse quantizers, DiskANN inherently re-ranks\nthe retrieved shortlist with high-precision embeddings (d = 2048), which is reflected in its high\ntop-1 accuracy. We find that AdANNS with 8-byte OPQ (Figure 6c) matches the top-1 accuracy\nof rigid representations using 32-byte OPQ, for a 4\u00d7 cost reduction for the same accuracy. Also\nnote that using AdANNS provides large gains over using MR-2048 at high compression (1.5%),\nhighlighting the necessity of AdANNS\u2019s flexibility for high-precision retrieval at low compute\nbudgets.\n4. Our findings on the HNSW-OPQ composite index (Figure 6d) are consistent with all other indices,\ni.e. HNSW graphs constructed with AdANNS OPQ codebooks provide significant gains over RR\nand MR, especially at high compression (\u2264 32 Bytes).\n\nOPQ on NQ dataset"}, {"type": "table", "rows": [["", "8", "16", "32", "48", "64", "96"], ["Top-1 Accuracy (%)", "40", "35", "30", "25", "20", "AdANNS-OPQ"], ["", "", "", "", "", "", "OPQ-MR-768"], ["", "", "", "", "", "", "Rigid-OPQ"], ["", "Compute Budget (Bytes)", "", "", "", "", ""], ["", "AdANNS-IVF+OPQ", "IVF+OPQ-MR-768", "Rigid-IVF+OPQ", "", "", ""]], "md": "| |8|16|32|48|64|96|\n|---|---|---|---|---|---|---|\n|Top-1 Accuracy (%)|40|35|30|25|20|AdANNS-OPQ|\n| | | | | | |OPQ-MR-768|\n| | | | | | |Rigid-OPQ|\n| |Compute Budget (Bytes)| | | | | |\n| |AdANNS-IVF+OPQ|IVF+OPQ-MR-768|Rigid-IVF+OPQ| | | |", "isPerfectTable": true, "csv": "\"\",\"8\",\"16\",\"32\",\"48\",\"64\",\"96\"\n\"Top-1 Accuracy (%)\",\"40\",\"35\",\"30\",\"25\",\"20\",\"AdANNS-OPQ\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"OPQ-MR-768\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"Rigid-OPQ\"\n\"\",\"Compute Budget (Bytes)\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"AdANNS-IVF+OPQ\",\"IVF+OPQ-MR-768\",\"Rigid-IVF+OPQ\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 7: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared\nto MR and Rigid baselines models on Natural Questions.\n\nOur observations on ImageNet with ResNet-50 MR across search structures also extend to the Natural\nQuestions dataset with Dense Passage Retriever (DPR with BERT-Base MR embeddings). We note\nthat AdANNS provides gains over RR-768 embeddings for both Exact Search and IVF with OPQ\n(Figure 7a and 7b). We find that similar to ImageNet (Figure 15) IVF performance on Natural\nQuestions generally scales with dimensionality. AdANNS thus reduces to MR-768 performance\nfor M \u2265 16. See Appendix G for a more in-depth discussion of AdANNS with DPR on Natural\nQuestions.\n\n17", "md": "Figure 7: Top-1 Accuracy of AdANNS composite indices with OPQ distance computation compared\nto MR and Rigid baselines models on Natural Questions.\n\nOur observations on ImageNet with ResNet-50 MR across search structures also extend to the Natural\nQuestions dataset with Dense Passage Retriever (DPR with BERT-Base MR embeddings). We note\nthat AdANNS provides gains over RR-768 embeddings for both Exact Search and IVF with OPQ\n(Figure 7a and 7b). We find that similar to ImageNet (Figure 15) IVF performance on Natural\nQuestions generally scales with dimensionality. AdANNS thus reduces to MR-768 performance\nfor M \u2265 16. See Appendix G for a more in-depth discussion of AdANNS with DPR on Natural\nQuestions.\n\n17"}]}, {"page": 18, "text": "Table 4: Comparison of PQ-MR with OPQ-MR for exact search on ImageNet-1K across embedding\ndimensionality d \u2208     {8, 16, ..., 2048} quantized to m \u2208    {8, 16, 32, 64} bytes. OPQ shows large\ngains over vanilla PQ at larger embedding dimensionalities d \u2265      128. Entries with the highest top-1\naccuracy for a given (d, m) tuple are bolded.\n               Config                    PQ                              OPQ\n              d      m     Top-1    mAP@10        P@100      Top-1     mAP@10       P@100\n              8       8    62.18       56.71       61.23     62.22       56.70       61.23\n              16      8    67.91       62.85       67.21     67.88       62.96       67.21\n                     16    67.85       62.95       67.21     67.96       62.94       67.21\n                      8    68.80       63.62       67.86     68.91       63.63       67.86\n              32     16    69.57       64.22       68.12     69.47       64.20       68.12\n                     32    69.44       64.20       68.12     69.47       64.23       68.12\n                      8    68.39       63.40       67.47     68.38       63.42       67.60\n              64     16    69.77       64.43       68.25     69.95       64.55       68.38\n                     32    70.13       64.67       68.38     70.05       64.65       68.38\n                     64    70.12       64.69       68.42     70.18       64.70       68.38\n                      8    67.27       61.99       65.78     68.40       63.11       67.34\n             128     16    69.51       64.32       68.12     69.78       64.56       68.38\n                     32    70.27       64.72       68.51     70.60       64.97       68.51\n                     64    70.61       64.93       68.49     70.65       64.98       68.51\n                      8    66.06       60.44       64.09     67.90       62.69       66.95\n             256     16    68.56       63.33       66.95     69.92       64.71       68.51\n                     32    70.08       64.83       68.38     70.59       65.15       68.64\n                     64    70.48       64.98       68.55     70.69       65.09       68.64\n                      8    65.09       59.03       62.53     67.51       62.12       66.56\n             512     16    67.68       62.11       65.39     69.67       64.53       68.38\n                     32    69.51       64.01       67.34     70.44       65.11       68.64\n                     64    70.53       65.02       68.52     70.72       65.17       68.64\n                      8    64.58       58.26       61.75     67.26       62.07       66.56\n            1024     16    66.84       61.07       64.09     69.34       64.23       68.12\n                     32    68.71       62.92       66.04     70.43       65.03       68.64\n                     64    69.88       64.35       67.68     70.81       65.19       68.64\n                      8    62.19       56.11       59.80     66.89       61.69       66.30\n            2048     16    65.99       60.27       63.18     69.25       64.09       67.99\n                     32    67.99       62.04       64.74     70.39       64.97       68.51\n                     64    69.20       63.46       66.40     70.57       65.15       68.51\nE    AdANNS-IVF\nInverted file index (IVF) [48] is a simple yet powerful ANNS data structure used in web-scale search\nsystems [16]. IVF construction involves clustering (coarse quantization often through k-means) [36]\non d-dimensional representation that results in an inverted file list [53] of all the data points in each\ncluster. During search, the d-dimensional query representation is first assigned to the closest clusters\n(# probes, typically set to 1) and then an exhaustive linear scan happens within each cluster to obtain\nthe nearest neighbors. As seen in Figure 9, IVF top-1 accuracy scales logarithmically with increasing\nrepresentation dimensionality d on ImageNet-1K/V2/4K. The learned low-d representations thus\nprovide better accuracy-compute trade-offs compared to high-d representations, thus furthering the\ncase for usage of AdANNS with IVF.\nOur proposed adaptive variant of IVF, AdANNS-IVF, decouples the clustering, with dc dimensions,\nand the linear scan within each cluster, with ds dimensions \u2013 setting dc = ds results in non-\nadaptive vanilla IVF. This helps in the smooth search of design space for the optimal accuracy-\ncompute trade-off. A naive instantiation yet strong baseline would be to use explicitly trained\n                                                   18", "md": "|Config|PQ|OPQ|\n|---|---|---|\n|d|m|Top-1|mAP@10|P@100|Top-1|mAP@10|P@100|\n|8|8|62.18|56.71|61.23|62.22|56.70|61.23|\n|16|8|67.91|62.85|67.21|67.88|62.96|67.21|\n| |16|67.85|62.95|67.21|67.96|62.94|67.21|\n| |8|68.80|63.62|67.86|68.91|63.63|67.86|\n|32|16|69.57|64.22|68.12|69.47|64.20|68.12|\n| |32|69.44|64.20|68.12|69.47|64.23|68.12|\n| |8|68.39|63.40|67.47|68.38|63.42|67.60|\n|64|16|69.77|64.43|68.25|69.95|64.55|68.38|\n| |32|70.13|64.67|68.38|70.05|64.65|68.38|\n| |64|70.12|64.69|68.42|70.18|64.70|68.38|\n| |8|67.27|61.99|65.78|68.40|63.11|67.34|\n|128|16|69.51|64.32|68.12|69.78|64.56|68.38|\n| |32|70.27|64.72|68.51|70.60|64.97|68.51|\n| |64|70.61|64.93|68.49|70.65|64.98|68.51|\n| |8|66.06|60.44|64.09|67.90|62.69|66.95|\n|256|16|68.56|63.33|66.95|69.92|64.71|68.51|\n| |32|70.08|64.83|68.38|70.59|65.15|68.64|\n| |64|70.48|64.98|68.55|70.69|65.09|68.64|\n| |8|65.09|59.03|62.53|67.51|62.12|66.56|\n|512|16|67.68|62.11|65.39|69.67|64.53|68.38|\n| |32|69.51|64.01|67.34|70.44|65.11|68.64|\n| |64|70.53|65.02|68.52|70.72|65.17|68.64|\n| |8|64.58|58.26|61.75|67.26|62.07|66.56|\n|1024|16|66.84|61.07|64.09|69.34|64.23|68.12|\n| |32|68.71|62.92|66.04|70.43|65.03|68.64|\n| |64|69.88|64.35|67.68|70.81|65.19|68.64|\n| |8|62.19|56.11|59.80|66.89|61.69|66.30|\n|2048|16|65.99|60.27|63.18|69.25|64.09|67.99|\n| |32|67.99|62.04|64.74|70.39|64.97|68.51|\n| |64|69.20|63.46|66.40|70.57|65.15|68.51|\n\nTable 4: Comparison of PQ-MR with OPQ-MR for exact search on ImageNet-1K across embedding dimensionality $$d \\in \\{8, 16, ..., 2048\\}$$ quantized to $$m \\in \\{8, 16, 32, 64\\}$$ bytes. OPQ shows large gains over vanilla PQ at larger embedding dimensionalities $$d \\geq 128$$. Entries with the highest top-1 accuracy for a given $$(d, m)$$ tuple are bolded.\n\nE AdANNS-IVF\n\nInverted file index (IVF) [48] is a simple yet powerful ANNS data structure used in web-scale search systems [16]. IVF construction involves clustering (coarse quantization often through k-means) [36] on $$d$$-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, the $$d$$-dimensional query representation is first assigned to the closest clusters (# probes, typically set to 1) and then an exhaustive linear scan happens within each cluster to obtain the nearest neighbors. As seen in Figure 9, IVF top-1 accuracy scales logarithmically with increasing representation dimensionality $$d$$ on ImageNet-1K/V2/4K. The learned low-$$d$$ representations thus provide better accuracy-compute trade-offs compared to high-$$d$$ representations, thus furthering the case for usage of AdANNS with IVF.\n\nOur proposed adaptive variant of IVF, AdANNS-IVF, decouples the clustering, with $$d_c$$ dimensions, and the linear scan within each cluster, with $$d_s$$ dimensions \u2013 setting $$d_c = d_s$$ results in non-adaptive vanilla IVF. This helps in the smooth search of design space for the optimal accuracy-compute trade-off. A naive instantiation yet strong baseline would be to use explicitly trained", "images": [], "items": [{"type": "table", "rows": [["Config", "PQ", "OPQ"], ["d", "m", "Top-1", "mAP@10", "P@100", "Top-1", "mAP@10", "P@100"], ["8", "8", "62.18", "56.71", "61.23", "62.22", "56.70", "61.23"], ["16", "8", "67.91", "62.85", "67.21", "67.88", "62.96", "67.21"], ["", "16", "67.85", "62.95", "67.21", "67.96", "62.94", "67.21"], ["", "8", "68.80", "63.62", "67.86", "68.91", "63.63", "67.86"], ["32", "16", "69.57", "64.22", "68.12", "69.47", "64.20", "68.12"], ["", "32", "69.44", "64.20", "68.12", "69.47", "64.23", "68.12"], ["", "8", "68.39", "63.40", "67.47", "68.38", "63.42", "67.60"], ["64", "16", "69.77", "64.43", "68.25", "69.95", "64.55", "68.38"], ["", "32", "70.13", "64.67", "68.38", "70.05", "64.65", "68.38"], ["", "64", "70.12", "64.69", "68.42", "70.18", "64.70", "68.38"], ["", "8", "67.27", "61.99", "65.78", "68.40", "63.11", "67.34"], ["128", "16", "69.51", "64.32", "68.12", "69.78", "64.56", "68.38"], ["", "32", "70.27", "64.72", "68.51", "70.60", "64.97", "68.51"], ["", "64", "70.61", "64.93", "68.49", "70.65", "64.98", "68.51"], ["", "8", "66.06", "60.44", "64.09", "67.90", "62.69", "66.95"], ["256", "16", "68.56", "63.33", "66.95", "69.92", "64.71", "68.51"], ["", "32", "70.08", "64.83", "68.38", "70.59", "65.15", "68.64"], ["", "64", "70.48", "64.98", "68.55", "70.69", "65.09", "68.64"], ["", "8", "65.09", "59.03", "62.53", "67.51", "62.12", "66.56"], ["512", "16", "67.68", "62.11", "65.39", "69.67", "64.53", "68.38"], ["", "32", "69.51", "64.01", "67.34", "70.44", "65.11", "68.64"], ["", "64", "70.53", "65.02", "68.52", "70.72", "65.17", "68.64"], ["", "8", "64.58", "58.26", "61.75", "67.26", "62.07", "66.56"], ["1024", "16", "66.84", "61.07", "64.09", "69.34", "64.23", "68.12"], ["", "32", "68.71", "62.92", "66.04", "70.43", "65.03", "68.64"], ["", "64", "69.88", "64.35", "67.68", "70.81", "65.19", "68.64"], ["", "8", "62.19", "56.11", "59.80", "66.89", "61.69", "66.30"], ["2048", "16", "65.99", "60.27", "63.18", "69.25", "64.09", "67.99"], ["", "32", "67.99", "62.04", "64.74", "70.39", "64.97", "68.51"], ["", "64", "69.20", "63.46", "66.40", "70.57", "65.15", "68.51"]], "md": "|Config|PQ|OPQ|\n|---|---|---|\n|d|m|Top-1|mAP@10|P@100|Top-1|mAP@10|P@100|\n|8|8|62.18|56.71|61.23|62.22|56.70|61.23|\n|16|8|67.91|62.85|67.21|67.88|62.96|67.21|\n| |16|67.85|62.95|67.21|67.96|62.94|67.21|\n| |8|68.80|63.62|67.86|68.91|63.63|67.86|\n|32|16|69.57|64.22|68.12|69.47|64.20|68.12|\n| |32|69.44|64.20|68.12|69.47|64.23|68.12|\n| |8|68.39|63.40|67.47|68.38|63.42|67.60|\n|64|16|69.77|64.43|68.25|69.95|64.55|68.38|\n| |32|70.13|64.67|68.38|70.05|64.65|68.38|\n| |64|70.12|64.69|68.42|70.18|64.70|68.38|\n| |8|67.27|61.99|65.78|68.40|63.11|67.34|\n|128|16|69.51|64.32|68.12|69.78|64.56|68.38|\n| |32|70.27|64.72|68.51|70.60|64.97|68.51|\n| |64|70.61|64.93|68.49|70.65|64.98|68.51|\n| |8|66.06|60.44|64.09|67.90|62.69|66.95|\n|256|16|68.56|63.33|66.95|69.92|64.71|68.51|\n| |32|70.08|64.83|68.38|70.59|65.15|68.64|\n| |64|70.48|64.98|68.55|70.69|65.09|68.64|\n| |8|65.09|59.03|62.53|67.51|62.12|66.56|\n|512|16|67.68|62.11|65.39|69.67|64.53|68.38|\n| |32|69.51|64.01|67.34|70.44|65.11|68.64|\n| |64|70.53|65.02|68.52|70.72|65.17|68.64|\n| |8|64.58|58.26|61.75|67.26|62.07|66.56|\n|1024|16|66.84|61.07|64.09|69.34|64.23|68.12|\n| |32|68.71|62.92|66.04|70.43|65.03|68.64|\n| |64|69.88|64.35|67.68|70.81|65.19|68.64|\n| |8|62.19|56.11|59.80|66.89|61.69|66.30|\n|2048|16|65.99|60.27|63.18|69.25|64.09|67.99|\n| |32|67.99|62.04|64.74|70.39|64.97|68.51|\n| |64|69.20|63.46|66.40|70.57|65.15|68.51|", "isPerfectTable": false, "csv": "\"Config\",\"PQ\",\"OPQ\"\n\"d\",\"m\",\"Top-1\",\"mAP@10\",\"P@100\",\"Top-1\",\"mAP@10\",\"P@100\"\n\"8\",\"8\",\"62.18\",\"56.71\",\"61.23\",\"62.22\",\"56.70\",\"61.23\"\n\"16\",\"8\",\"67.91\",\"62.85\",\"67.21\",\"67.88\",\"62.96\",\"67.21\"\n\"\",\"16\",\"67.85\",\"62.95\",\"67.21\",\"67.96\",\"62.94\",\"67.21\"\n\"\",\"8\",\"68.80\",\"63.62\",\"67.86\",\"68.91\",\"63.63\",\"67.86\"\n\"32\",\"16\",\"69.57\",\"64.22\",\"68.12\",\"69.47\",\"64.20\",\"68.12\"\n\"\",\"32\",\"69.44\",\"64.20\",\"68.12\",\"69.47\",\"64.23\",\"68.12\"\n\"\",\"8\",\"68.39\",\"63.40\",\"67.47\",\"68.38\",\"63.42\",\"67.60\"\n\"64\",\"16\",\"69.77\",\"64.43\",\"68.25\",\"69.95\",\"64.55\",\"68.38\"\n\"\",\"32\",\"70.13\",\"64.67\",\"68.38\",\"70.05\",\"64.65\",\"68.38\"\n\"\",\"64\",\"70.12\",\"64.69\",\"68.42\",\"70.18\",\"64.70\",\"68.38\"\n\"\",\"8\",\"67.27\",\"61.99\",\"65.78\",\"68.40\",\"63.11\",\"67.34\"\n\"128\",\"16\",\"69.51\",\"64.32\",\"68.12\",\"69.78\",\"64.56\",\"68.38\"\n\"\",\"32\",\"70.27\",\"64.72\",\"68.51\",\"70.60\",\"64.97\",\"68.51\"\n\"\",\"64\",\"70.61\",\"64.93\",\"68.49\",\"70.65\",\"64.98\",\"68.51\"\n\"\",\"8\",\"66.06\",\"60.44\",\"64.09\",\"67.90\",\"62.69\",\"66.95\"\n\"256\",\"16\",\"68.56\",\"63.33\",\"66.95\",\"69.92\",\"64.71\",\"68.51\"\n\"\",\"32\",\"70.08\",\"64.83\",\"68.38\",\"70.59\",\"65.15\",\"68.64\"\n\"\",\"64\",\"70.48\",\"64.98\",\"68.55\",\"70.69\",\"65.09\",\"68.64\"\n\"\",\"8\",\"65.09\",\"59.03\",\"62.53\",\"67.51\",\"62.12\",\"66.56\"\n\"512\",\"16\",\"67.68\",\"62.11\",\"65.39\",\"69.67\",\"64.53\",\"68.38\"\n\"\",\"32\",\"69.51\",\"64.01\",\"67.34\",\"70.44\",\"65.11\",\"68.64\"\n\"\",\"64\",\"70.53\",\"65.02\",\"68.52\",\"70.72\",\"65.17\",\"68.64\"\n\"\",\"8\",\"64.58\",\"58.26\",\"61.75\",\"67.26\",\"62.07\",\"66.56\"\n\"1024\",\"16\",\"66.84\",\"61.07\",\"64.09\",\"69.34\",\"64.23\",\"68.12\"\n\"\",\"32\",\"68.71\",\"62.92\",\"66.04\",\"70.43\",\"65.03\",\"68.64\"\n\"\",\"64\",\"69.88\",\"64.35\",\"67.68\",\"70.81\",\"65.19\",\"68.64\"\n\"\",\"8\",\"62.19\",\"56.11\",\"59.80\",\"66.89\",\"61.69\",\"66.30\"\n\"2048\",\"16\",\"65.99\",\"60.27\",\"63.18\",\"69.25\",\"64.09\",\"67.99\"\n\"\",\"32\",\"67.99\",\"62.04\",\"64.74\",\"70.39\",\"64.97\",\"68.51\"\n\"\",\"64\",\"69.20\",\"63.46\",\"66.40\",\"70.57\",\"65.15\",\"68.51\""}, {"type": "text", "value": "Table 4: Comparison of PQ-MR with OPQ-MR for exact search on ImageNet-1K across embedding dimensionality $$d \\in \\{8, 16, ..., 2048\\}$$ quantized to $$m \\in \\{8, 16, 32, 64\\}$$ bytes. OPQ shows large gains over vanilla PQ at larger embedding dimensionalities $$d \\geq 128$$. Entries with the highest top-1 accuracy for a given $$(d, m)$$ tuple are bolded.\n\nE AdANNS-IVF\n\nInverted file index (IVF) [48] is a simple yet powerful ANNS data structure used in web-scale search systems [16]. IVF construction involves clustering (coarse quantization often through k-means) [36] on $$d$$-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, the $$d$$-dimensional query representation is first assigned to the closest clusters (# probes, typically set to 1) and then an exhaustive linear scan happens within each cluster to obtain the nearest neighbors. As seen in Figure 9, IVF top-1 accuracy scales logarithmically with increasing representation dimensionality $$d$$ on ImageNet-1K/V2/4K. The learned low-$$d$$ representations thus provide better accuracy-compute trade-offs compared to high-$$d$$ representations, thus furthering the case for usage of AdANNS with IVF.\n\nOur proposed adaptive variant of IVF, AdANNS-IVF, decouples the clustering, with $$d_c$$ dimensions, and the linear scan within each cluster, with $$d_s$$ dimensions \u2013 setting $$d_c = d_s$$ results in non-adaptive vanilla IVF. This helps in the smooth search of design space for the optimal accuracy-compute trade-off. A naive instantiation yet strong baseline would be to use explicitly trained", "md": "Table 4: Comparison of PQ-MR with OPQ-MR for exact search on ImageNet-1K across embedding dimensionality $$d \\in \\{8, 16, ..., 2048\\}$$ quantized to $$m \\in \\{8, 16, 32, 64\\}$$ bytes. OPQ shows large gains over vanilla PQ at larger embedding dimensionalities $$d \\geq 128$$. Entries with the highest top-1 accuracy for a given $$(d, m)$$ tuple are bolded.\n\nE AdANNS-IVF\n\nInverted file index (IVF) [48] is a simple yet powerful ANNS data structure used in web-scale search systems [16]. IVF construction involves clustering (coarse quantization often through k-means) [36] on $$d$$-dimensional representation that results in an inverted file list [53] of all the data points in each cluster. During search, the $$d$$-dimensional query representation is first assigned to the closest clusters (# probes, typically set to 1) and then an exhaustive linear scan happens within each cluster to obtain the nearest neighbors. As seen in Figure 9, IVF top-1 accuracy scales logarithmically with increasing representation dimensionality $$d$$ on ImageNet-1K/V2/4K. The learned low-$$d$$ representations thus provide better accuracy-compute trade-offs compared to high-$$d$$ representations, thus furthering the case for usage of AdANNS with IVF.\n\nOur proposed adaptive variant of IVF, AdANNS-IVF, decouples the clustering, with $$d_c$$ dimensions, and the linear scan within each cluster, with $$d_s$$ dimensions \u2013 setting $$d_c = d_s$$ results in non-adaptive vanilla IVF. This helps in the smooth search of design space for the optimal accuracy-compute trade-off. A naive instantiation yet strong baseline would be to use explicitly trained"}]}, {"page": 19, "text": "                        71\n                        70\n                      Top-1 Accuracy(%)\n                        69\n                        68                                                                                              AdANNS-IVF\n                        67                                                                                              AdANNS-IVF-D\n                                                                                                                        MG-IVF-RR\n                        66                                                                                              MG-IVF-SVD\n                        65                                                                                              IVF-MR\n                                                                                                                        IVF-RR\n                        64                      0.1                          1                         10                        100\n                                                                        MFLOPS/Query\nFigure 8: Top-1 accuracy vs. compute cost per query of AdANNS-IVF compared to IVF-MR,\nIVF-RR and MG-IVF-RR baselines on ImageNet-1K.\ndc and ds dimensional rigid representations (called MG-IVF-RR, for multi-granular IVF with\nrigid representations). We also examine the setting of adaptively choosing low-dimensional MR\nto linear scan the shortlisted clusters built with high-dimensional MR, i.e. AdANNS-IVF-D, as\nseen in Table 5. As seen in Figure 8, AdANNS-IVF provides pareto-optimal accuracy-compute\ntradeoff across inference compute. This figure is a more exhaustive indication of AdANNS-IVF\nbehavior compared to baselines than Figures 1a and 2. AdANNS-IVF is evaluated for all possible\ntuples of dc, ds, k = |C| \u2208                      {8, 16, . . . , 2048}. AdANNS-IVF-D is evaluated for a pre-built IVF\nindex with dc = 2048 and ds \u2208                                {8, . . . , 2048}. MG-IVF-RR configurations are evaluated for\ndc \u2208      {8, . . . , ds}, ds \u2208           {32, . . . , 2048} and k = 1024 clusters. A study over additional k values\nis omitted due to high compute cost. Finally, IVF-MR and IVF-RR configurations are evaluated\nfor dc = ds \u2208              {8, 16, . . . , 2048} and k \u2208                   {256, . . . , 8192}. Note that for a fair comparison, we\nuse np = 1 across all configurations. We discuss the inference compute for these settings in\nAppendix B.1.\nE.1       Robustness\n        70                                                    60                                                   30\n       Top-1 Accuracy(%)                                      58                                                  Top-1 Accuracy(%)\n                                                             Top-1 Accuracy(%)\n        68                                                    56                                                   25\n        66\n        64                                     IVF-MR         54                                    IVF-MR         20\n        62                                     IVF-RR         52                                    IVF-RR         15\n                                               Exact-MR                                             Exact-MR                                             IVF-MR\n        60                                     Exact-RR       50                                    Exact-RR       10                                    Exact-MR\n             8    16   32   64   128   256  512  1024 2048         8    16   32   64   128   256  512  1024 2048       8    16    32   64   128  256   512  1024 2048\n                         Representation Size                                   Representation Size                                 Representation Size\n                  (a) ImageNet-1K                                        (b) ImageNetV2                                      (c) ImageNet-4K\nFigure 9: Top-1 Accuracy variation of IVF-MR of ImageNet 1K, ImageNetV2 and ImageNet-4K.\nRR baselines are omitted on ImageNet-4K due to high compute cost.\nAs shown in Figure 9, we examined the clustering capabilities of MRs on both in-distribution (ID)\nqueries via ImageNet-1K and out-of-distribution (OOD) queries via ImageNetV2 [44], as well as\non larger-scale ImageNet-4K [31]. For ID queries on ImageNet-1K (Figure 9a), IVF-MR is at least\nas accurate as Exact-RR for d \u2264                            256 with a single search probe, demonstrating the quality of in-\ndistribution low-d clustering with MR. On OOD queries (Figure 9b), we observe that IVF-MR is on\naverage 2% more robust than IVF-RR across all cluster construction and linear scan dimensionalities\nd. It is also notable that clustering with MRs followed by linear scan with # probes = 1 is more robust\nthan exact search with RR embeddings across all d \u2264                                            2048, indicating the adaptability of MRs to\ndistribution shifts during inference. As seen in Table 5, on ImageNetV2 AdANNS-IVF-D is the best\n                                                                                   19", "md": "# Document\n\n## Top-1 Accuracy vs. Compute Cost per Query\n\nFigure 8: Top-1 accuracy vs. compute cost per query of AdANNS-IVF compared to IVF-MR, IVF-RR and MG-IVF-RR baselines on ImageNet-1K.\n\ndc and ds dimensional rigid representations (called MG-IVF-RR, for multi-granular IVF with rigid representations). We also examine the setting of adaptively choosing low-dimensional MR to linear scan the shortlisted clusters built with high-dimensional MR, i.e. AdANNS-IVF-D, as seen in Table 5. As seen in Figure 8, AdANNS-IVF provides pareto-optimal accuracy-compute tradeoff across inference compute. This figure is a more exhaustive indication of AdANNS-IVF behavior compared to baselines than Figures 1a and 2. AdANNS-IVF is evaluated for all possible tuples of $$dc, ds, k = |C| \\in \\{8, 16, ..., 2048\\}$$. AdANNS-IVF-D is evaluated for a pre-built IVF index with $$dc = 2048$$ and $$ds \\in \\{8, ..., 2048\\}$$. MG-IVF-RR configurations are evaluated for $$dc \\in \\{8, ..., ds\\}$$, $$ds \\in \\{32, ..., 2048\\}$$ and $$k = 1024$$ clusters. A study over additional $$k$$ values is omitted due to high compute cost. Finally, IVF-MR and IVF-RR configurations are evaluated for $$dc = ds \\in \\{8, 16, ..., 2048\\}$$ and $$k \\in \\{256, ..., 8192\\}$$. Note that for a fair comparison, we use $$np = 1$$ across all configurations. We discuss the inference compute for these settings in Appendix B.1.\n\n## Robustness\n\nFigure 9: Top-1 Accuracy variation of IVF-MR of ImageNet 1K, ImageNetV2 and ImageNet-4K. RR baselines are omitted on ImageNet-4K due to high compute cost.\n\nAs shown in Figure 9, we examined the clustering capabilities of MRs on both in-distribution (ID) queries via ImageNet-1K and out-of-distribution (OOD) queries via ImageNetV2 [44], as well as on larger-scale ImageNet-4K [31]. For ID queries on ImageNet-1K (Figure 9a), IVF-MR is at least as accurate as Exact-RR for $$d \\leq 256$$ with a single search probe, demonstrating the quality of in-distribution low-d clustering with MR. On OOD queries (Figure 9b), we observe that IVF-MR is on average 2% more robust than IVF-RR across all cluster construction and linear scan dimensionalities $$d$$. It is also notable that clustering with MRs followed by linear scan with # probes = 1 is more robust than exact search with RR embeddings across all $$d \\leq 2048$$, indicating the adaptability of MRs to distribution shifts during inference. As seen in Table 5, on ImageNetV2 AdANNS-IVF-D is the best.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Top-1 Accuracy vs. Compute Cost per Query", "md": "## Top-1 Accuracy vs. Compute Cost per Query"}, {"type": "text", "value": "Figure 8: Top-1 accuracy vs. compute cost per query of AdANNS-IVF compared to IVF-MR, IVF-RR and MG-IVF-RR baselines on ImageNet-1K.\n\ndc and ds dimensional rigid representations (called MG-IVF-RR, for multi-granular IVF with rigid representations). We also examine the setting of adaptively choosing low-dimensional MR to linear scan the shortlisted clusters built with high-dimensional MR, i.e. AdANNS-IVF-D, as seen in Table 5. As seen in Figure 8, AdANNS-IVF provides pareto-optimal accuracy-compute tradeoff across inference compute. This figure is a more exhaustive indication of AdANNS-IVF behavior compared to baselines than Figures 1a and 2. AdANNS-IVF is evaluated for all possible tuples of $$dc, ds, k = |C| \\in \\{8, 16, ..., 2048\\}$$. AdANNS-IVF-D is evaluated for a pre-built IVF index with $$dc = 2048$$ and $$ds \\in \\{8, ..., 2048\\}$$. MG-IVF-RR configurations are evaluated for $$dc \\in \\{8, ..., ds\\}$$, $$ds \\in \\{32, ..., 2048\\}$$ and $$k = 1024$$ clusters. A study over additional $$k$$ values is omitted due to high compute cost. Finally, IVF-MR and IVF-RR configurations are evaluated for $$dc = ds \\in \\{8, 16, ..., 2048\\}$$ and $$k \\in \\{256, ..., 8192\\}$$. Note that for a fair comparison, we use $$np = 1$$ across all configurations. We discuss the inference compute for these settings in Appendix B.1.", "md": "Figure 8: Top-1 accuracy vs. compute cost per query of AdANNS-IVF compared to IVF-MR, IVF-RR and MG-IVF-RR baselines on ImageNet-1K.\n\ndc and ds dimensional rigid representations (called MG-IVF-RR, for multi-granular IVF with rigid representations). We also examine the setting of adaptively choosing low-dimensional MR to linear scan the shortlisted clusters built with high-dimensional MR, i.e. AdANNS-IVF-D, as seen in Table 5. As seen in Figure 8, AdANNS-IVF provides pareto-optimal accuracy-compute tradeoff across inference compute. This figure is a more exhaustive indication of AdANNS-IVF behavior compared to baselines than Figures 1a and 2. AdANNS-IVF is evaluated for all possible tuples of $$dc, ds, k = |C| \\in \\{8, 16, ..., 2048\\}$$. AdANNS-IVF-D is evaluated for a pre-built IVF index with $$dc = 2048$$ and $$ds \\in \\{8, ..., 2048\\}$$. MG-IVF-RR configurations are evaluated for $$dc \\in \\{8, ..., ds\\}$$, $$ds \\in \\{32, ..., 2048\\}$$ and $$k = 1024$$ clusters. A study over additional $$k$$ values is omitted due to high compute cost. Finally, IVF-MR and IVF-RR configurations are evaluated for $$dc = ds \\in \\{8, 16, ..., 2048\\}$$ and $$k \\in \\{256, ..., 8192\\}$$. Note that for a fair comparison, we use $$np = 1$$ across all configurations. We discuss the inference compute for these settings in Appendix B.1."}, {"type": "heading", "lvl": 2, "value": "Robustness", "md": "## Robustness"}, {"type": "text", "value": "Figure 9: Top-1 Accuracy variation of IVF-MR of ImageNet 1K, ImageNetV2 and ImageNet-4K. RR baselines are omitted on ImageNet-4K due to high compute cost.\n\nAs shown in Figure 9, we examined the clustering capabilities of MRs on both in-distribution (ID) queries via ImageNet-1K and out-of-distribution (OOD) queries via ImageNetV2 [44], as well as on larger-scale ImageNet-4K [31]. For ID queries on ImageNet-1K (Figure 9a), IVF-MR is at least as accurate as Exact-RR for $$d \\leq 256$$ with a single search probe, demonstrating the quality of in-distribution low-d clustering with MR. On OOD queries (Figure 9b), we observe that IVF-MR is on average 2% more robust than IVF-RR across all cluster construction and linear scan dimensionalities $$d$$. It is also notable that clustering with MRs followed by linear scan with # probes = 1 is more robust than exact search with RR embeddings across all $$d \\leq 2048$$, indicating the adaptability of MRs to distribution shifts during inference. As seen in Table 5, on ImageNetV2 AdANNS-IVF-D is the best.", "md": "Figure 9: Top-1 Accuracy variation of IVF-MR of ImageNet 1K, ImageNetV2 and ImageNet-4K. RR baselines are omitted on ImageNet-4K due to high compute cost.\n\nAs shown in Figure 9, we examined the clustering capabilities of MRs on both in-distribution (ID) queries via ImageNet-1K and out-of-distribution (OOD) queries via ImageNetV2 [44], as well as on larger-scale ImageNet-4K [31]. For ID queries on ImageNet-1K (Figure 9a), IVF-MR is at least as accurate as Exact-RR for $$d \\leq 256$$ with a single search probe, demonstrating the quality of in-distribution low-d clustering with MR. On OOD queries (Figure 9b), we observe that IVF-MR is on average 2% more robust than IVF-RR across all cluster construction and linear scan dimensionalities $$d$$. It is also notable that clustering with MRs followed by linear scan with # probes = 1 is more robust than exact search with RR embeddings across all $$d \\leq 2048$$, indicating the adaptability of MRs to distribution shifts during inference. As seen in Table 5, on ImageNetV2 AdANNS-IVF-D is the best."}]}, {"page": 20, "text": "Table 5: Top-1 Accuracy of AdANNS-IVF-D on out-of-distribution queries from ImageNetV2\ncompared to both IVF and Exact Search with MR and RR embeddings. Note that for AdANNS-\nIVF-D, the dimensionality used to build clusters dc = 2048.\n                  d        AdANNS-IVF-D                  IVF-MR           Exact-MR            IVF-RR           Exact-RR\n                  8                 53.51                  50.44              50.41              49.03             48.79\n                 16                 57.32                  56.35              56.64              55.04             55.08\n                 32                 57.32                  57.64              57.96              56.06             56.69\n                 64                 57.85                  58.01              58.94              56.84             57.37\n                128                 58.02                  58.09              59.13              56.14             57.17\n                256                 58.01                  58.33              59.18              55.60             57.09\n                512                 58.03                  57.84              59.40              55.46             57.12\n               1024                 57.66                  57.58              59.11              54.80             57.53\n               2048                 58.04                  58.04              59.63              56.17             57.84\nconfiguration for d \u2264           16, and is similarly accurate to IVF-MR at all other d. AdANNS-IVF-D with\nd = 128 is able to match its own accuracy with d = 2048, a 16\u00d7 compute gain during inference.\nThis demonstrates the potential of AdANNS to adaptively search pre-indexed clustering structures.\nOn 4-million scale ImageNet-4K (Figure 9c), we observe similar accuracy trends of IVF-MR\ncompared to Exact-MR as in ImageNet-1K (Figure 9a) and ImageNetV2 (Figure 9b). We omit\nbaseline IVF-RR and Exact-RR experiments due to high compute cost at larger scale.\nE.2      IVF-MR Ablations\n         30                                       ~100\u00d7 Compute             100.0\n                                                                             97.5\n                                                                             95.0\n         25                                                                1-Recall@N\n        Top-1 (%)                                                            92.5\n         20                                             IVF 1-Probe          90.0                                           1-Recall@1\n                                                        IVF 2-Probe          87.5                                           1-Recall@2\n         15                                             IVF 4-Probe          85.0                                           1-Recall@4\n                                                                                                                            1-Recall@5\n                                                        IVF 8-Probe          82.5                                           1-Recall@10\n         100.1           1           10          100    Exact1000            80.0    8    16     32    64    128   256   512   1024  2048\n                                MFLOPS/Query                                                       Representation Size\n                    (a) 4K Search Probes (np)                                                (b) Centroid Recall\nFigure 10: Ablations on IVF-MR Clustering: a) Analysis of accuracy-compute tradeoff with in-\ncreasing IVF-MR search probes np on ImageNet-4K compared to Exact-MR and b) k-Recall@N on\nImageNet-1K cluster centroids across representation sizes d. Cluster centroids retrieved with highest\nembedding dim d = 2048 were considered ground-truth centroids.\nAs seen in Figure 10a, IVF-MR can match the accuracy of Exact Search on ImageNet-4K with\n\u223c   100\u00d7 less compute. We also explored the capability of MRs at retrieving cluster centroids with\nlow-d compared to a ground truth of 2048-d with k-Recall@N, as seen in Figure 10b. MRs were able\nto saturate to near-perfect 1-Recall@N for d \u2265                       32 and N \u2265         4, indicating the potential of AdANNS\nat matching exact search performance with less than 10 search probes np.\nE.3      Clustering Distribution\nWe examined the distribution of learnt clusters across embedding dimensionalities d for both MR\nand RR models, as seen in Figure 11. We observe IVF-MR to have less variance than IVF-RR at\nd \u2208   {8, 16}, and slightly higher variance for d \u2265                     32, while IVF-MR outperforms IVF-RR in top-1\nacross all d (Figure 9a). This indicates that although MR learns clusters that are less uniformly\ndistributed than RR at high d, the quality of learnt clustering is superior to RR across all d. Note that a\nuniform distribution is N/k data points per cluster, i.e. \u223c                        1250 for ImageNet-1K with k = 1024. We\nquantitatively evaluate the proximity of the MR and RR clustering distributions with Total Variation\n                                                                      20", "md": "|d|AdANNS-IVF-D|IVF-MR|Exact-MR|IVF-RR|Exact-RR|\n|---|---|---|---|---|---|\n|8|53.51|50.44|50.41|49.03|48.79|\n|16|57.32|56.35|56.64|55.04|55.08|\n|32|57.32|57.64|57.96|56.06|56.69|\n|64|57.85|58.01|58.94|56.84|57.37|\n|128|58.02|58.09|59.13|56.14|57.17|\n|256|58.01|58.33|59.18|55.60|57.09|\n|512|58.03|57.84|59.40|55.46|57.12|\n|1024|57.66|57.58|59.11|54.80|57.53|\n|2048|58.04|58.04|59.63|56.17|57.84|\n\nConfiguration for $$d \\leq 16$$, and is similarly accurate to IVF-MR at all other d. AdANNS-IVF-D with $$d = 128$$ is able to match its own accuracy with $$d = 2048$$, a 16\u00d7 compute gain during inference. This demonstrates the potential of AdANNS to adaptively search pre-indexed clustering structures.\n\nOn 4-million scale ImageNet-4K (Figure 9c), we observe similar accuracy trends of IVF-MR compared to Exact-MR as in ImageNet-1K (Figure 9a) and ImageNetV2 (Figure 9b). We omit baseline IVF-RR and Exact-RR experiments due to high compute cost at larger scale.\n\n### IVF-MR Ablations\n\n| |Top-1 (%)|\n|---|---|\n|IVF 1-Probe|100.0|\n|IVF 2-Probe|97.5|\n|IVF 4-Probe|95.0|\n|IVF 8-Probe|92.5|\n\nAs seen in Figure 10a, IVF-MR can match the accuracy of Exact Search on ImageNet-4K with \u223c 100\u00d7 less compute. We also explored the capability of MRs at retrieving cluster centroids with low d compared to a ground truth of 2048-d with k-Recall@N, as seen in Figure 10b. MRs were able to saturate to near-perfect 1-Recall@N for $$d \\geq 32$$ and $$N \\geq 4$$, indicating the potential of AdANNS at matching exact search performance with less than 10 search probes np.\n\n### Clustering Distribution\n\nWe examined the distribution of learnt clusters across embedding dimensionalities d for both MR and RR models, as seen in Figure 11. We observe IVF-MR to have less variance than IVF-RR at $$d \\in \\{8, 16\\}$$, and slightly higher variance for $$d \\geq 32$$, while IVF-MR outperforms IVF-RR in top-1 across all d (Figure 9a). This indicates that although MR learns clusters that are less uniformly distributed than RR at high d, the quality of learnt clustering is superior to RR across all d. Note that a uniform distribution is $$N/k$$ data points per cluster, i.e. \u223c 1250 for ImageNet-1K with $$k = 1024$$. We quantitatively evaluate the proximity of the MR and RR clustering distributions with Total Variation.", "images": [], "items": [{"type": "table", "rows": [["d", "AdANNS-IVF-D", "IVF-MR", "Exact-MR", "IVF-RR", "Exact-RR"], ["8", "53.51", "50.44", "50.41", "49.03", "48.79"], ["16", "57.32", "56.35", "56.64", "55.04", "55.08"], ["32", "57.32", "57.64", "57.96", "56.06", "56.69"], ["64", "57.85", "58.01", "58.94", "56.84", "57.37"], ["128", "58.02", "58.09", "59.13", "56.14", "57.17"], ["256", "58.01", "58.33", "59.18", "55.60", "57.09"], ["512", "58.03", "57.84", "59.40", "55.46", "57.12"], ["1024", "57.66", "57.58", "59.11", "54.80", "57.53"], ["2048", "58.04", "58.04", "59.63", "56.17", "57.84"]], "md": "|d|AdANNS-IVF-D|IVF-MR|Exact-MR|IVF-RR|Exact-RR|\n|---|---|---|---|---|---|\n|8|53.51|50.44|50.41|49.03|48.79|\n|16|57.32|56.35|56.64|55.04|55.08|\n|32|57.32|57.64|57.96|56.06|56.69|\n|64|57.85|58.01|58.94|56.84|57.37|\n|128|58.02|58.09|59.13|56.14|57.17|\n|256|58.01|58.33|59.18|55.60|57.09|\n|512|58.03|57.84|59.40|55.46|57.12|\n|1024|57.66|57.58|59.11|54.80|57.53|\n|2048|58.04|58.04|59.63|56.17|57.84|", "isPerfectTable": true, "csv": "\"d\",\"AdANNS-IVF-D\",\"IVF-MR\",\"Exact-MR\",\"IVF-RR\",\"Exact-RR\"\n\"8\",\"53.51\",\"50.44\",\"50.41\",\"49.03\",\"48.79\"\n\"16\",\"57.32\",\"56.35\",\"56.64\",\"55.04\",\"55.08\"\n\"32\",\"57.32\",\"57.64\",\"57.96\",\"56.06\",\"56.69\"\n\"64\",\"57.85\",\"58.01\",\"58.94\",\"56.84\",\"57.37\"\n\"128\",\"58.02\",\"58.09\",\"59.13\",\"56.14\",\"57.17\"\n\"256\",\"58.01\",\"58.33\",\"59.18\",\"55.60\",\"57.09\"\n\"512\",\"58.03\",\"57.84\",\"59.40\",\"55.46\",\"57.12\"\n\"1024\",\"57.66\",\"57.58\",\"59.11\",\"54.80\",\"57.53\"\n\"2048\",\"58.04\",\"58.04\",\"59.63\",\"56.17\",\"57.84\""}, {"type": "text", "value": "Configuration for $$d \\leq 16$$, and is similarly accurate to IVF-MR at all other d. AdANNS-IVF-D with $$d = 128$$ is able to match its own accuracy with $$d = 2048$$, a 16\u00d7 compute gain during inference. This demonstrates the potential of AdANNS to adaptively search pre-indexed clustering structures.\n\nOn 4-million scale ImageNet-4K (Figure 9c), we observe similar accuracy trends of IVF-MR compared to Exact-MR as in ImageNet-1K (Figure 9a) and ImageNetV2 (Figure 9b). We omit baseline IVF-RR and Exact-RR experiments due to high compute cost at larger scale.", "md": "Configuration for $$d \\leq 16$$, and is similarly accurate to IVF-MR at all other d. AdANNS-IVF-D with $$d = 128$$ is able to match its own accuracy with $$d = 2048$$, a 16\u00d7 compute gain during inference. This demonstrates the potential of AdANNS to adaptively search pre-indexed clustering structures.\n\nOn 4-million scale ImageNet-4K (Figure 9c), we observe similar accuracy trends of IVF-MR compared to Exact-MR as in ImageNet-1K (Figure 9a) and ImageNetV2 (Figure 9b). We omit baseline IVF-RR and Exact-RR experiments due to high compute cost at larger scale."}, {"type": "heading", "lvl": 3, "value": "IVF-MR Ablations", "md": "### IVF-MR Ablations"}, {"type": "table", "rows": [["", "Top-1 (%)"], ["IVF 1-Probe", "100.0"], ["IVF 2-Probe", "97.5"], ["IVF 4-Probe", "95.0"], ["IVF 8-Probe", "92.5"]], "md": "| |Top-1 (%)|\n|---|---|\n|IVF 1-Probe|100.0|\n|IVF 2-Probe|97.5|\n|IVF 4-Probe|95.0|\n|IVF 8-Probe|92.5|", "isPerfectTable": true, "csv": "\"\",\"Top-1 (%)\"\n\"IVF 1-Probe\",\"100.0\"\n\"IVF 2-Probe\",\"97.5\"\n\"IVF 4-Probe\",\"95.0\"\n\"IVF 8-Probe\",\"92.5\""}, {"type": "text", "value": "As seen in Figure 10a, IVF-MR can match the accuracy of Exact Search on ImageNet-4K with \u223c 100\u00d7 less compute. We also explored the capability of MRs at retrieving cluster centroids with low d compared to a ground truth of 2048-d with k-Recall@N, as seen in Figure 10b. MRs were able to saturate to near-perfect 1-Recall@N for $$d \\geq 32$$ and $$N \\geq 4$$, indicating the potential of AdANNS at matching exact search performance with less than 10 search probes np.", "md": "As seen in Figure 10a, IVF-MR can match the accuracy of Exact Search on ImageNet-4K with \u223c 100\u00d7 less compute. We also explored the capability of MRs at retrieving cluster centroids with low d compared to a ground truth of 2048-d with k-Recall@N, as seen in Figure 10b. MRs were able to saturate to near-perfect 1-Recall@N for $$d \\geq 32$$ and $$N \\geq 4$$, indicating the potential of AdANNS at matching exact search performance with less than 10 search probes np."}, {"type": "heading", "lvl": 3, "value": "Clustering Distribution", "md": "### Clustering Distribution"}, {"type": "text", "value": "We examined the distribution of learnt clusters across embedding dimensionalities d for both MR and RR models, as seen in Figure 11. We observe IVF-MR to have less variance than IVF-RR at $$d \\in \\{8, 16\\}$$, and slightly higher variance for $$d \\geq 32$$, while IVF-MR outperforms IVF-RR in top-1 across all d (Figure 9a). This indicates that although MR learns clusters that are less uniformly distributed than RR at high d, the quality of learnt clustering is superior to RR across all d. Note that a uniform distribution is $$N/k$$ data points per cluster, i.e. \u223c 1250 for ImageNet-1K with $$k = 1024$$. We quantitatively evaluate the proximity of the MR and RR clustering distributions with Total Variation.", "md": "We examined the distribution of learnt clusters across embedding dimensionalities d for both MR and RR models, as seen in Figure 11. We observe IVF-MR to have less variance than IVF-RR at $$d \\in \\{8, 16\\}$$, and slightly higher variance for $$d \\geq 32$$, while IVF-MR outperforms IVF-RR in top-1 across all d (Figure 9a). This indicates that although MR learns clusters that are less uniformly distributed than RR at high d, the quality of learnt clustering is superior to RR across all d. Note that a uniform distribution is $$N/k$$ data points per cluster, i.e. \u223c 1250 for ImageNet-1K with $$k = 1024$$. We quantitatively evaluate the proximity of the MR and RR clustering distributions with Total Variation."}]}, {"page": 21, "text": "    300                            dTV=0.00078                                          dTV=0.00027           350                            dTV=0.00016\n    250                        dTV, 2048=0.00128         300                        dTV, 2048=0.00121         300                        dTV, 2048=0.00095\n   Number of cells                                      Number of cells                                      Number of cells\n    200                                   MR-8           250                                  MR-16           250                                  MR-32\n    150                                   RR-8           200                                  RR-16           200                                  RR-32\n                                                         150                                                  150\n    100                                                  100                                                  100\n     50                                                   50                                                   50\n       0   0     500    1000    1500    2000   2500         0   0     500    1000    1500   2000    2500        0    0     500    1000    1500   2000    2500\n            Number of data points per cell                       Number of data points per cell                       Number of data points per cell\n    350                            dTV=0.00037           350                            dTV=0.00061           400                            dTV=0.00068\n    300                        dTV, 2048=0.00078         300                        dTV, 2048=0.00069                                    dTV, 2048=0.00064\n   Number of cells                                      Number of cells                                      Number of cells\n    250                                  MR-64           250                                 MR-128           300                                 MR-256\n    200                                  RR-64           200                                 RR-128           200                                 RR-256\n    150                                                  150\n    100                                                  100                                                  100\n     50                                                   50\n       0   0     500    1000    1500    2000   2500         0   0     500    1000    1500   2000    2500        0    0     500    1000    1500   2000    2500\n            Number of data points per cell                       Number of data points per cell                       Number of data points per cell\n                                   dTV=0.00076           400                            dTV=0.00075           400                            dTV=0.00054\n    350                        dTV, 2048=0.00065                                    dTV, 2048=0.00059                                    dTV, 2048=0.00054\n    300\n   Number of cells                                      Number of cells                                      Number of cells\n                                        MR-512           300                               MR-1024            300                               MR-2048\n    250                                 RR-512                                             RR-1024                                              RR-2048\n    200                                                  200                                                  200\n    150\n    100                                                  100                                                  100\n     50\n       0   0     500    1000    1500    2000   2500         0   0     500    1000    1500   2000    2500        0    0     500    1000    1500   2000    2500\n            Number of data points per cell                       Number of data points per cell                       Number of data points per cell\nFigure 11: Clustering distributions for IVF-MR and IVF-RR across embedding dimensionality d on\nImageNet-1K. An IVF-MR and IVF-RR clustered with d = 16 embeddings is denoted by MR-16\nand RR-16 respectively.\nDistance [33], which is defined over two discrete probability distributions p, q over [n] as follows:\n                                                         dT V (p, q) = 1       2  i\u2208[n]  |pi \u2212     qi|\nWe also compute dT V,2048(MR-d) = dT V (MR-d, RR-2048), which evaluates the total variation dis-\ntance of a given low-d MR from high-d RR-2048. We observe a monotonically decreasing dT V,2048\nwith increasing d, which demonstrates that MR clustering distributions get closer to RR-2048 as we\nincrease the embedding dimensionality d. We observe in Figure 11 that dT V (MR-d, RR-d) \u223c                                                            7e \u2212     4\nfor d \u2208      {8, 256, . . . , 2048} and \u223c               3e \u2212     4 for d \u2208      {16, 32, 64}. These findings agree with the top-1\nimprovement of MR over RR as shown in Figure 9a, where there are smaller improvements for\nd \u2208    {16, 32, 64} (smaller dT V ) and larger improvements for d \u2208                                        {8, 256, . . . , 2048} (larger dT V ).\nThese results demonstrate a correlation between top-1 performance of IVF-MR and the quality of\nclusters learnt with MR.\nF       AdANNS-DiskANN\nDiskANN is a state-of-the-art graph-based ANNS index capable of serving queries from both RAM\nand SSD. DiskANN builds a greedy best-first graph with OPQ distance computation, with compressed\nvectors stored in memory. The index and full-precision vectors are stored on the SSD. During search,\n                                                                              21", "md": "# Clustering Distributions for IVF-MR and IVF-RR\n\n## Clustering distributions for IVF-MR and IVF-RR across embedding dimensionality d on ImageNet-1K\n\nAn IVF-MR and IVF-RR clustered with d = 16 embeddings is denoted by MR-16 and RR-16 respectively.\n\n### Distance [33]\n\nDistance [33], which is defined over two discrete probability distributions \\( p, q \\) over [n] as follows:\n\n$$ d_{TV}(p, q) = \\frac{1}{2} \\sum_{i\\in[n]} |p_i - q_i| $$\n\nWe also compute \\( d_{TV,2048}(MR-d) = d_{TV}(MR-d, RR-2048) \\), which evaluates the total variation distance of a given low-d MR from high-d RR-2048. We observe a monotonically decreasing \\( d_{TV,2048} \\) with increasing d, which demonstrates that MR clustering distributions get closer to RR-2048 as we increase the embedding dimensionality d. We observe in Figure 11 that \\( d_{TV}(MR-d, RR-d) \\approx 7e-4 \\) for \\( d \\in \\{8, 256, ..., 2048\\} \\) and \\( \\approx 3e-4 \\) for \\( d \\in \\{16, 32, 64\\} \\). These findings agree with the top-1 improvement of MR over RR as shown in Figure 9a, where there are smaller improvements for \\( d \\in \\{16, 32, 64\\} \\) (smaller \\( d_{TV} \\)) and larger improvements for \\( d \\in \\{8, 256, ..., 2048\\} \\) (larger \\( d_{TV} \\)).\n\nThese results demonstrate a correlation between top-1 performance of IVF-MR and the quality of clusters learnt with MR.\n\n### AdANNS-DiskANN\n\nDiskANN is a state-of-the-art graph-based ANNS index capable of serving queries from both RAM and SSD. DiskANN builds a greedy best-first graph with OPQ distance computation, with compressed vectors stored in memory. The index and full-precision vectors are stored on the SSD. During search.\n\n21", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Clustering Distributions for IVF-MR and IVF-RR", "md": "# Clustering Distributions for IVF-MR and IVF-RR"}, {"type": "heading", "lvl": 2, "value": "Clustering distributions for IVF-MR and IVF-RR across embedding dimensionality d on ImageNet-1K", "md": "## Clustering distributions for IVF-MR and IVF-RR across embedding dimensionality d on ImageNet-1K"}, {"type": "text", "value": "An IVF-MR and IVF-RR clustered with d = 16 embeddings is denoted by MR-16 and RR-16 respectively.", "md": "An IVF-MR and IVF-RR clustered with d = 16 embeddings is denoted by MR-16 and RR-16 respectively."}, {"type": "heading", "lvl": 3, "value": "Distance [33]", "md": "### Distance [33]"}, {"type": "text", "value": "Distance [33], which is defined over two discrete probability distributions \\( p, q \\) over [n] as follows:\n\n$$ d_{TV}(p, q) = \\frac{1}{2} \\sum_{i\\in[n]} |p_i - q_i| $$\n\nWe also compute \\( d_{TV,2048}(MR-d) = d_{TV}(MR-d, RR-2048) \\), which evaluates the total variation distance of a given low-d MR from high-d RR-2048. We observe a monotonically decreasing \\( d_{TV,2048} \\) with increasing d, which demonstrates that MR clustering distributions get closer to RR-2048 as we increase the embedding dimensionality d. We observe in Figure 11 that \\( d_{TV}(MR-d, RR-d) \\approx 7e-4 \\) for \\( d \\in \\{8, 256, ..., 2048\\} \\) and \\( \\approx 3e-4 \\) for \\( d \\in \\{16, 32, 64\\} \\). These findings agree with the top-1 improvement of MR over RR as shown in Figure 9a, where there are smaller improvements for \\( d \\in \\{16, 32, 64\\} \\) (smaller \\( d_{TV} \\)) and larger improvements for \\( d \\in \\{8, 256, ..., 2048\\} \\) (larger \\( d_{TV} \\)).\n\nThese results demonstrate a correlation between top-1 performance of IVF-MR and the quality of clusters learnt with MR.", "md": "Distance [33], which is defined over two discrete probability distributions \\( p, q \\) over [n] as follows:\n\n$$ d_{TV}(p, q) = \\frac{1}{2} \\sum_{i\\in[n]} |p_i - q_i| $$\n\nWe also compute \\( d_{TV,2048}(MR-d) = d_{TV}(MR-d, RR-2048) \\), which evaluates the total variation distance of a given low-d MR from high-d RR-2048. We observe a monotonically decreasing \\( d_{TV,2048} \\) with increasing d, which demonstrates that MR clustering distributions get closer to RR-2048 as we increase the embedding dimensionality d. We observe in Figure 11 that \\( d_{TV}(MR-d, RR-d) \\approx 7e-4 \\) for \\( d \\in \\{8, 256, ..., 2048\\} \\) and \\( \\approx 3e-4 \\) for \\( d \\in \\{16, 32, 64\\} \\). These findings agree with the top-1 improvement of MR over RR as shown in Figure 9a, where there are smaller improvements for \\( d \\in \\{16, 32, 64\\} \\) (smaller \\( d_{TV} \\)) and larger improvements for \\( d \\in \\{8, 256, ..., 2048\\} \\) (larger \\( d_{TV} \\)).\n\nThese results demonstrate a correlation between top-1 performance of IVF-MR and the quality of clusters learnt with MR."}, {"type": "heading", "lvl": 3, "value": "AdANNS-DiskANN", "md": "### AdANNS-DiskANN"}, {"type": "text", "value": "DiskANN is a state-of-the-art graph-based ANNS index capable of serving queries from both RAM and SSD. DiskANN builds a greedy best-first graph with OPQ distance computation, with compressed vectors stored in memory. The index and full-precision vectors are stored on the SSD. During search.\n\n21", "md": "DiskANN is a state-of-the-art graph-based ANNS index capable of serving queries from both RAM and SSD. DiskANN builds a greedy best-first graph with OPQ distance computation, with compressed vectors stored in memory. The index and full-precision vectors are stored on the SSD. During search.\n\n21"}]}, {"page": 22, "text": "Table 6: Wall clock search latency (\u00b5s) of AdANNS-DiskANN across graph construction dimension-\nality d \u2208   {8, 16, . . . , 2048} and compute budget in terms of OPQ budget M \u2208                  {8, 16, 32, 48, 64}.\nSearch latency is fairly consistent across fixed embedding dimensionality D.\n                              d       M=8       M=16        M=32       M=48        M=64\n                              8        495         -           -           -          -\n                              16       555        571          -           -          -\n                              32       669        655         653          -          -\n                              64       864        855         843         844        848\n                             128      1182       1311        1156       1161        2011\n                             256      1923       1779        1744       2849        1818\n                             512      2802       3272        3423       2780        3171\n                            1024      5127       5456        5724       4683        5087\n                            2048      9907       9833       10205      10183        9329\nwhen a query\u2019s neighbor shortlist is fetched from the SSD, its full-precision vector is also fetched\nin a single disk read. This enables efficient and fast distance computation with PQ on a large initial\nshortlist of candidate nearest neighbors in RAM followed by a high-precision re-ranking with full-\nprecision vectors fetched from the SSD on a much smaller shortlist. The experiments carried out in\nthis work primarily utilize a DiskANN graph index built in-memory5 with OPQ distance computation.\nAs with IVF, DiskANN is also well\nsuited to the flexibility provided by               70.2\nAdANNS as we demonstrate on both\nImageNet and NQ that the optimal                  Top-1 Accuracy (%)\nPQ codebook for a given compute                     70.0\nbudget is learnt with a smaller em-                 69.8\nbedding dimensionality d (see Fig-\nures 6c and      7a). We demonstrate                69.6\nthe capability of AdANNS-DiskANN\nwith a compute budget of m                 \u2208        69.4\n{32, 64} in Table 1.          We tabulate                                              32-Byte\nthe search time latency of AdANNS-                  69.2                               48-Byte\nDiskANN in microseconds (\u00b5s) in\nTable 6, which grows linearly with                  69.0                               64-Byte\ngraph construction dimensionality d.                       32         64         128        256        512       1024\nWe also examine DiskANN-MR with                                             Representation Size\nSSD graph indices on ImageNet-1K\nacross OPQ budgets for distance com-            Figure 12: DiskANN-MR with SSD indices for ImageNet-\nputation mdc \u2208      {32, 48, 64}, as seen       1K retrieval, with compute budgets mdisk                 = mdc       \u2208\nin Figure 12.        With SSD indices,          {32, 48, 64} across graph and OPQ codebook construction\nwe store PQ-compressed vectors on               dimensionalities d \u2208       {32, . . . , 1024}. Note that this does\ndisk with mdisk = mdc, which es-                not use any re-ranking after obtaining OPQ based shortlist.\nsentially disables DiskANN\u2019s implicit\nhigh-precision re-ranking.          We ob-\nserve similar trends to other composite ANNS indices on ImageNet, where the optimal dim for\nfixed OPQ budget is not the highest dim (d = 1024 with fp32 embeddings is current highest dim\nsupported by DiskANN which stores vectors in 4KB sectors on disk). This provides further motiva-\ntion for AdANNS-DiskANN, which leverages MRs to provide flexible access to the optimal dim\nfor quantization and thus enables similar Top-1 accuracy to Rigid DiskANN for up to 1/4 the cost\n(Figure 6c).\nG     AdANNS on Natural Questions\nIn addition to image retrieval on ImageNet, we also experiment with dense passage retrieval (DPR) on\nNatural Questions. As shown in Figure 6, MR representations are 1 \u2212                 10% more accurate than their\n    5https://github.com/microsoft/DiskANN\n                                                          22", "md": "|d|M=8|M=16|M=32|M=48|M=64|\n|---|---|---|---|---|---|\n|8|495|-|-|-|-|\n|16|555|571|-|-|-|\n|32|669|655|653|-|-|\n|64|864|855|843|844|848|\n|128|1182|1311|1156|1161|2011|\n|256|1923|1779|1744|2849|1818|\n|512|2802|3272|3423|2780|3171|\n|1024|5127|5456|5724|4683|5087|\n|2048|9907|9833|10205|10183|9329|\n\nWhen a query\u2019s neighbor shortlist is fetched from the SSD, its full-precision vector is also fetched in a single disk read. This enables efficient and fast distance computation with PQ on a large initial shortlist of candidate nearest neighbors in RAM followed by a high-precision re-ranking with full-precision vectors fetched from the SSD on a much smaller shortlist. The experiments carried out in this work primarily utilize a DiskANN graph index built in-memory with OPQ distance computation. As with IVF, DiskANN is also well suited to the flexibility provided by AdANNS as we demonstrate on both ImageNet and NQ that the optimal PQ codebook for a given compute budget is learnt with a smaller embedding dimensionality d (see Figures 6c and 7a). We demonstrate the capability of AdANNS-DiskANN with a compute budget of $$m \\in \\{32, 64\\}$$ in Table 1. We tabulate the search time latency of AdANNS-DiskANN in microseconds (\u00b5s) in Table 6, which grows linearly with graph construction dimensionality d.\n\nWe also examine DiskANN-MR with SSD graph indices on ImageNet-1K across OPQ budgets for distance computation $$mdc \\in \\{32, 48, 64\\}$$, as seen in Figure 12. With SSD indices, we store PQ-compressed vectors on disk with $$mdisk = mdc$$, which essentially disables DiskANN\u2019s implicit high-precision re-ranking. We observe similar trends to other composite ANNS indices on ImageNet, where the optimal dim for fixed OPQ budget is not the highest dim ($$d = 1024$$ with fp32 embeddings is current highest dim supported by DiskANN which stores vectors in 4KB sectors on disk). This provides further motivation for AdANNS-DiskANN, which leverages MRs to provide flexible access to the optimal dim for quantization and thus enables similar Top-1 accuracy to Rigid DiskANN for up to 1/4 the cost (Figure 6c).\n\n### AdANNS on Natural Questions\n\nIn addition to image retrieval on ImageNet, we also experiment with dense passage retrieval (DPR) on Natural Questions. As shown in Figure 6, MR representations are 1 - 10% more accurate than their\n\n5https://github.com/microsoft/DiskANN", "images": [], "items": [{"type": "table", "rows": [["d", "M=8", "M=16", "M=32", "M=48", "M=64"], ["8", "495", "-", "-", "-", "-"], ["16", "555", "571", "-", "-", "-"], ["32", "669", "655", "653", "-", "-"], ["64", "864", "855", "843", "844", "848"], ["128", "1182", "1311", "1156", "1161", "2011"], ["256", "1923", "1779", "1744", "2849", "1818"], ["512", "2802", "3272", "3423", "2780", "3171"], ["1024", "5127", "5456", "5724", "4683", "5087"], ["2048", "9907", "9833", "10205", "10183", "9329"]], "md": "|d|M=8|M=16|M=32|M=48|M=64|\n|---|---|---|---|---|---|\n|8|495|-|-|-|-|\n|16|555|571|-|-|-|\n|32|669|655|653|-|-|\n|64|864|855|843|844|848|\n|128|1182|1311|1156|1161|2011|\n|256|1923|1779|1744|2849|1818|\n|512|2802|3272|3423|2780|3171|\n|1024|5127|5456|5724|4683|5087|\n|2048|9907|9833|10205|10183|9329|", "isPerfectTable": true, "csv": "\"d\",\"M=8\",\"M=16\",\"M=32\",\"M=48\",\"M=64\"\n\"8\",\"495\",\"-\",\"-\",\"-\",\"-\"\n\"16\",\"555\",\"571\",\"-\",\"-\",\"-\"\n\"32\",\"669\",\"655\",\"653\",\"-\",\"-\"\n\"64\",\"864\",\"855\",\"843\",\"844\",\"848\"\n\"128\",\"1182\",\"1311\",\"1156\",\"1161\",\"2011\"\n\"256\",\"1923\",\"1779\",\"1744\",\"2849\",\"1818\"\n\"512\",\"2802\",\"3272\",\"3423\",\"2780\",\"3171\"\n\"1024\",\"5127\",\"5456\",\"5724\",\"4683\",\"5087\"\n\"2048\",\"9907\",\"9833\",\"10205\",\"10183\",\"9329\""}, {"type": "text", "value": "When a query\u2019s neighbor shortlist is fetched from the SSD, its full-precision vector is also fetched in a single disk read. This enables efficient and fast distance computation with PQ on a large initial shortlist of candidate nearest neighbors in RAM followed by a high-precision re-ranking with full-precision vectors fetched from the SSD on a much smaller shortlist. The experiments carried out in this work primarily utilize a DiskANN graph index built in-memory with OPQ distance computation. As with IVF, DiskANN is also well suited to the flexibility provided by AdANNS as we demonstrate on both ImageNet and NQ that the optimal PQ codebook for a given compute budget is learnt with a smaller embedding dimensionality d (see Figures 6c and 7a). We demonstrate the capability of AdANNS-DiskANN with a compute budget of $$m \\in \\{32, 64\\}$$ in Table 1. We tabulate the search time latency of AdANNS-DiskANN in microseconds (\u00b5s) in Table 6, which grows linearly with graph construction dimensionality d.\n\nWe also examine DiskANN-MR with SSD graph indices on ImageNet-1K across OPQ budgets for distance computation $$mdc \\in \\{32, 48, 64\\}$$, as seen in Figure 12. With SSD indices, we store PQ-compressed vectors on disk with $$mdisk = mdc$$, which essentially disables DiskANN\u2019s implicit high-precision re-ranking. We observe similar trends to other composite ANNS indices on ImageNet, where the optimal dim for fixed OPQ budget is not the highest dim ($$d = 1024$$ with fp32 embeddings is current highest dim supported by DiskANN which stores vectors in 4KB sectors on disk). This provides further motivation for AdANNS-DiskANN, which leverages MRs to provide flexible access to the optimal dim for quantization and thus enables similar Top-1 accuracy to Rigid DiskANN for up to 1/4 the cost (Figure 6c).", "md": "When a query\u2019s neighbor shortlist is fetched from the SSD, its full-precision vector is also fetched in a single disk read. This enables efficient and fast distance computation with PQ on a large initial shortlist of candidate nearest neighbors in RAM followed by a high-precision re-ranking with full-precision vectors fetched from the SSD on a much smaller shortlist. The experiments carried out in this work primarily utilize a DiskANN graph index built in-memory with OPQ distance computation. As with IVF, DiskANN is also well suited to the flexibility provided by AdANNS as we demonstrate on both ImageNet and NQ that the optimal PQ codebook for a given compute budget is learnt with a smaller embedding dimensionality d (see Figures 6c and 7a). We demonstrate the capability of AdANNS-DiskANN with a compute budget of $$m \\in \\{32, 64\\}$$ in Table 1. We tabulate the search time latency of AdANNS-DiskANN in microseconds (\u00b5s) in Table 6, which grows linearly with graph construction dimensionality d.\n\nWe also examine DiskANN-MR with SSD graph indices on ImageNet-1K across OPQ budgets for distance computation $$mdc \\in \\{32, 48, 64\\}$$, as seen in Figure 12. With SSD indices, we store PQ-compressed vectors on disk with $$mdisk = mdc$$, which essentially disables DiskANN\u2019s implicit high-precision re-ranking. We observe similar trends to other composite ANNS indices on ImageNet, where the optimal dim for fixed OPQ budget is not the highest dim ($$d = 1024$$ with fp32 embeddings is current highest dim supported by DiskANN which stores vectors in 4KB sectors on disk). This provides further motivation for AdANNS-DiskANN, which leverages MRs to provide flexible access to the optimal dim for quantization and thus enables similar Top-1 accuracy to Rigid DiskANN for up to 1/4 the cost (Figure 6c)."}, {"type": "heading", "lvl": 3, "value": "AdANNS on Natural Questions", "md": "### AdANNS on Natural Questions"}, {"type": "text", "value": "In addition to image retrieval on ImageNet, we also experiment with dense passage retrieval (DPR) on Natural Questions. As shown in Figure 6, MR representations are 1 - 10% more accurate than their\n\n5https://github.com/microsoft/DiskANN", "md": "In addition to image retrieval on ImageNet, we also experiment with dense passage retrieval (DPR) on Natural Questions. As shown in Figure 6, MR representations are 1 - 10% more accurate than their\n\n5https://github.com/microsoft/DiskANN"}]}, {"page": 23, "text": "RR counterparts across PQ compute budgets with Exact Search + OPQ on NQ. We also demonstrate\nthat IVF-MR is 1 \u2212         2.5% better than IVF-RR for Precision@k, k \u2208                   {1, 5, 20, 100, 200}. Note that\non NQ, IVF loses \u223c          10% accuracy compared to exact search, even with the RR-768 baseline. We\nhypothesize the weak performance of IVF owing to poor clusterability of the BERT-Base embeddings\nfine-tuned on the NQ dataset. A more thorough exploration of AdANNS-IVF on NQ is an immediate\nfuture work and is in progress.\nH      Ablations\nH.1     Recall Score Analysis\n                                                                    100\n                 100.0\n                                                                     98\n                  97.5\n                40-Recall@2048 (%)                                   96\n                  95.0                                             1-Recall@1 (%)\n                  92.5                                   d8          94                                    d8\n                                                          16         92                                     16\n                  90.0                                    32                                                32\n                                                          64         90                                     64\n                  87.5                                    128                                               128\n                                                          256        88                                     256\n                  85.0                                    512                                               512\n                                                          1024       86                                     1024\n                  82.5                                    2048                                              2048\n                      1          2         4             10            1          2          4             10\n                               IVF Search Probes (np)                            IVF Search Probes (np)\n                 100                                                100\n                   90                                                95\n                40-Recall@2048 (%)\n                                                                     90\n                   80                                    d         1-Recall@1 (%)                          d\n                                                          8          85                                     8\n                                                          16                                                16\n                   70                                     32         80                                     32\n                                                          64                                                64\n                                                          128        75                                     128\n                   60                                     256                                               256\n                                                          512        70                                     512\n                   50                                     1024                                              1024\n                                                          2048       65                                     2048\n                     1       2       4         10      20              1       2       4         10      20\n                         HNSW Search Probes (efSearch)                     HNSW Search Probes (efSearch)\n                 100                                                100\n                   95                                                95\n                40-Recall@2048 (%)\n                   90                                              1-Recall@1 (%)\n                                                                     90\n                   85\n                                                                     85\n                   80\n                   75                               MR-8             80                               MR-8\n                                                    MR-64                                             MR-64\n                   70                               MR-256           75                               MR-256\n                                                    MR-2048                                           MR-2048\n                   651         2          4            10            701          2         4             10\n                              IVF Search Probes (np)                             IVF Search Probes (np)\nFigure 13: k-Recall@N of d-dimensional MR for IVF and HNSW with increasing search probes\nnp on ImageNet-1K and ImageNet-4K. On ImageNet-4K, we restrict our study to IVF-MR with\nd \u2208    {8, 64, 256, 2048}. Other embedding dimensionalities, HNSW-MR and RR baselines are\nomitted due to high compute cost. We observe that trends from ImageNet-1K with increasing d and\nnp extend to ImageNet-4K, which is 4\u00d7 larger.\n                                                               23", "md": "# Math Equations and Tables\n\nRR counterparts across PQ compute budgets with Exact Search + OPQ on NQ. We also demonstrate that IVF-MR is 1 - 2.5% better than IVF-RR for Precision@k, k \u2208 {1, 5, 20, 100, 200}. Note that on NQ, IVF loses \u223c 10% accuracy compared to exact search, even with the RR-768 baseline. We hypothesize the weak performance of IVF owing to poor clusterability of the BERT-Base embeddings fine-tuned on the NQ dataset. A more thorough exploration of AdANNS-IVF on NQ is an immediate future work and is in progress.\n\n## Ablations\n\n### Recall Score Analysis\n\n|IVF Search Probes (np)|1|2|4|10|\n|---|---|---|---|---|\n|40-Recall@2048 (%)|100.0|97.5|95.0|92.5|\n|1-Recall@1 (%)|94|92|90|88|\n\n|HNSW Search Probes (efSearch)|1|2|4|10|20|\n|---|---|---|---|---|---|\n|40-Recall@2048 (%)|100|95|90|85| |\n|1-Recall@1 (%)|80|75|70|65| |\n\nFigure 13: k-Recall@N of d-dimensional MR for IVF and HNSW with increasing search probes np on ImageNet-1K and ImageNet-4K. On ImageNet-4K, we restrict our study to IVF-MR with d \u2208 {8, 64, 256, 2048}. Other embedding dimensionalities, HNSW-MR and RR baselines are omitted due to high compute cost. We observe that trends from ImageNet-1K with increasing d and np extend to ImageNet-4K, which is 4\u00d7 larger.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Tables", "md": "# Math Equations and Tables"}, {"type": "text", "value": "RR counterparts across PQ compute budgets with Exact Search + OPQ on NQ. We also demonstrate that IVF-MR is 1 - 2.5% better than IVF-RR for Precision@k, k \u2208 {1, 5, 20, 100, 200}. Note that on NQ, IVF loses \u223c 10% accuracy compared to exact search, even with the RR-768 baseline. We hypothesize the weak performance of IVF owing to poor clusterability of the BERT-Base embeddings fine-tuned on the NQ dataset. A more thorough exploration of AdANNS-IVF on NQ is an immediate future work and is in progress.", "md": "RR counterparts across PQ compute budgets with Exact Search + OPQ on NQ. We also demonstrate that IVF-MR is 1 - 2.5% better than IVF-RR for Precision@k, k \u2208 {1, 5, 20, 100, 200}. Note that on NQ, IVF loses \u223c 10% accuracy compared to exact search, even with the RR-768 baseline. We hypothesize the weak performance of IVF owing to poor clusterability of the BERT-Base embeddings fine-tuned on the NQ dataset. A more thorough exploration of AdANNS-IVF on NQ is an immediate future work and is in progress."}, {"type": "heading", "lvl": 2, "value": "Ablations", "md": "## Ablations"}, {"type": "heading", "lvl": 3, "value": "Recall Score Analysis", "md": "### Recall Score Analysis"}, {"type": "table", "rows": [["IVF Search Probes (np)", "1", "2", "4", "10"], ["40-Recall@2048 (%)", "100.0", "97.5", "95.0", "92.5"], ["1-Recall@1 (%)", "94", "92", "90", "88"]], "md": "|IVF Search Probes (np)|1|2|4|10|\n|---|---|---|---|---|\n|40-Recall@2048 (%)|100.0|97.5|95.0|92.5|\n|1-Recall@1 (%)|94|92|90|88|", "isPerfectTable": true, "csv": "\"IVF Search Probes (np)\",\"1\",\"2\",\"4\",\"10\"\n\"40-Recall@2048 (%)\",\"100.0\",\"97.5\",\"95.0\",\"92.5\"\n\"1-Recall@1 (%)\",\"94\",\"92\",\"90\",\"88\""}, {"type": "table", "rows": [["HNSW Search Probes (efSearch)", "1", "2", "4", "10", "20"], ["40-Recall@2048 (%)", "100", "95", "90", "85", ""], ["1-Recall@1 (%)", "80", "75", "70", "65", ""]], "md": "|HNSW Search Probes (efSearch)|1|2|4|10|20|\n|---|---|---|---|---|---|\n|40-Recall@2048 (%)|100|95|90|85| |\n|1-Recall@1 (%)|80|75|70|65| |", "isPerfectTable": true, "csv": "\"HNSW Search Probes (efSearch)\",\"1\",\"2\",\"4\",\"10\",\"20\"\n\"40-Recall@2048 (%)\",\"100\",\"95\",\"90\",\"85\",\"\"\n\"1-Recall@1 (%)\",\"80\",\"75\",\"70\",\"65\",\"\""}, {"type": "text", "value": "Figure 13: k-Recall@N of d-dimensional MR for IVF and HNSW with increasing search probes np on ImageNet-1K and ImageNet-4K. On ImageNet-4K, we restrict our study to IVF-MR with d \u2208 {8, 64, 256, 2048}. Other embedding dimensionalities, HNSW-MR and RR baselines are omitted due to high compute cost. We observe that trends from ImageNet-1K with increasing d and np extend to ImageNet-4K, which is 4\u00d7 larger.", "md": "Figure 13: k-Recall@N of d-dimensional MR for IVF and HNSW with increasing search probes np on ImageNet-1K and ImageNet-4K. On ImageNet-4K, we restrict our study to IVF-MR with d \u2208 {8, 64, 256, 2048}. Other embedding dimensionalities, HNSW-MR and RR baselines are omitted due to high compute cost. We observe that trends from ImageNet-1K with increasing d and np extend to ImageNet-4K, which is 4\u00d7 larger."}]}, {"page": 24, "text": "                    22.5                                                           MR\n                    20.0\n                   Relative Contrast                                               RR\n                    17.5\n                    15.0\n                    12.5\n                    10.0\n                      7.5\n                      5.0\n                      2.5\n                             8      16     32     64     128     256    512    1024   2048\n                                             Representation Size\n Figure 14: Relative contrast of varying capacity MRs and RRs on ImageNet-1K corroborating the\n findings of He et al. [18].\n In this section we also examine the variation of k-Recall@N with by probing a larger search space\nwith IVF and HNSW indices. For IVF, search probes represent the number of clusters shortlisted for\n linear scan during inference. For HNSW, search quality is controlled by the efSearch parameter [38],\nwhich represents the closest neighbors to query q at level lc of the graph and is analogous to number\n of search probes in IVF. As seen in Figure 13, general trends show a) an intuitive increase in recall\nwith increasing search probes np) for fixed search probes, b) a decrease in recall with increasing\n search dimensionality d c) similar trends in ImageNet-1K and 4\u00d7 larger ImageNet-4K.\n H.2   Relative Contrast\nWe utilize Relative Contrast [18] to capture the difficulty of nearest neighbors search with IVF-MR\n compared to IVF-RR. For a given database X = {xi \u2208          Rd, i = 1, . . . , ND}, a query q \u2208 Rd, and a\n distance metric D(., .) we compute relative contrast Cr as a measure of the difficulty in finding the\n1-nearest neighbor (1-NN) for a query q in database X as follows:\n 1. Compute Dq   min = min                                                                       nn \u2208  X\n                        i=1...n D(q, xi), i.e. the distance of query q to its nearest neighbor xq\n 2. Compute Dq   mean = Ex[D(q, x)] as the average distance of query q from all database points\n    x \u2208  X\n 3. Relative Contrast of a given query Cq    r = Dq  mean  , which is a measure of how separable the\n                                                    Dqmin\n    query\u2019s nearest neighbor xq nn is from an average point in the database x\n 4. Compute an expectation over all queries for Relative Contrast over the entire database as\n                                              Cr = Eq[Dq   mean]\n                                                     Eq[Dq min]\n It is evident that Cr captures the difficulty of Nearest Neighbor Search in database X, as a Cr \u223c        1\n indicates that for an average query, its nearest neighbor is almost equidistant from a random point in\n the database. As demonstrated in Figure 14, MRs have higher Rc than RR Embeddings for an Exact\n Search on ImageNet-1K for all d \u2265       16. This result implies that a portion of MR\u2019s improvement\n over RR for 1-NN retrieval across all embedding dimensionalities d [31] is due to a higher average\n separability of the MR 1-NN from a random database point.\n H.3   Generality across Encoders\nWe perform an ablation over the representation function \u03d5 : X \u2192         Rd learnt via a backbone neural\n network (primarily ResNet50 in this work), as detailed in Section 3. We also train MRL models [31]\n \u03d5MR(d) on ResNet18/34/101 [19] that are as accurate as their independently trained RR baseline\n models \u03d5RR(d), where d is the default max representation size of each architecture. We also train\n                                                    24", "md": "# Document\n\n## Relative Contrast\n\nWe utilize Relative Contrast [18] to capture the difficulty of nearest neighbors search with IVF-MR compared to IVF-RR. For a given database \\(X = \\{x_i \\in \\mathbb{R}^d, i = 1, ..., N_D\\}\\), a query \\(q \\in \\mathbb{R}^d\\), and a distance metric \\(D(., .)\\) we compute relative contrast \\(C_r\\) as a measure of the difficulty in finding the 1-nearest neighbor (1-NN) for a query \\(q\\) in database \\(X\\) as follows:\n\n1. Compute \\(D_{q_{\\text{min}}} = \\min_{i=1...n} D(q, x_i)\\), i.e. the distance of query \\(q\\) to its nearest neighbor \\(x_{q_{\\text{NN}}}\\)\n2. Compute \\(D_{q_{\\text{mean}}} = E_x[D(q, x)]\\) as the average distance of query \\(q\\) from all database points \\(x \\in X\\)\n3. Relative Contrast of a given query \\(C_{q_r} = \\frac{D_{q_{\\text{mean}}}}{D_{q_{\\text{min}}}}\\), which is a measure of how separable the query\u2019s nearest neighbor \\(x_{q_{\\text{NN}}}\\) is from an average point in the database \\(x\\)\n4. Compute an expectation over all queries for Relative Contrast over the entire database as \\(C_r = E_q[D_{q_{\\text{mean}}}] / E_q[D_{q_{\\text{min}}}]\\)\n\nIt is evident that \\(C_r\\) captures the difficulty of Nearest Neighbor Search in database \\(X\\), as a \\(C_r \\sim 1\\) indicates that for an average query, its nearest neighbor is almost equidistant from a random point in the database. As demonstrated in Figure 14, MRs have higher \\(R_c\\) than RR Embeddings for an Exact Search on ImageNet-1K for all \\(d \\ge 16\\). This result implies that a portion of MR\u2019s improvement over RR for 1-NN retrieval across all embedding dimensionalities \\(d\\) [31] is due to a higher average separability of the MR 1-NN from a random database point.\n\n### Generality across Encoders\n\nWe perform an ablation over the representation function \\(\\phi : X \\rightarrow \\mathbb{R}^d\\) learnt via a backbone neural network (primarily ResNet50 in this work), as detailed in Section 3. We also train MRL models [31] \\(\\phi_{\\text{MR}}(d)\\) on ResNet18/34/101 [19] that are as accurate as their independently trained RR baseline models \\(\\phi_{\\text{RR}}(d)\\), where \\(d\\) is the default max representation size of each architecture. We also train", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Relative Contrast", "md": "## Relative Contrast"}, {"type": "text", "value": "We utilize Relative Contrast [18] to capture the difficulty of nearest neighbors search with IVF-MR compared to IVF-RR. For a given database \\(X = \\{x_i \\in \\mathbb{R}^d, i = 1, ..., N_D\\}\\), a query \\(q \\in \\mathbb{R}^d\\), and a distance metric \\(D(., .)\\) we compute relative contrast \\(C_r\\) as a measure of the difficulty in finding the 1-nearest neighbor (1-NN) for a query \\(q\\) in database \\(X\\) as follows:\n\n1. Compute \\(D_{q_{\\text{min}}} = \\min_{i=1...n} D(q, x_i)\\), i.e. the distance of query \\(q\\) to its nearest neighbor \\(x_{q_{\\text{NN}}}\\)\n2. Compute \\(D_{q_{\\text{mean}}} = E_x[D(q, x)]\\) as the average distance of query \\(q\\) from all database points \\(x \\in X\\)\n3. Relative Contrast of a given query \\(C_{q_r} = \\frac{D_{q_{\\text{mean}}}}{D_{q_{\\text{min}}}}\\), which is a measure of how separable the query\u2019s nearest neighbor \\(x_{q_{\\text{NN}}}\\) is from an average point in the database \\(x\\)\n4. Compute an expectation over all queries for Relative Contrast over the entire database as \\(C_r = E_q[D_{q_{\\text{mean}}}] / E_q[D_{q_{\\text{min}}}]\\)\n\nIt is evident that \\(C_r\\) captures the difficulty of Nearest Neighbor Search in database \\(X\\), as a \\(C_r \\sim 1\\) indicates that for an average query, its nearest neighbor is almost equidistant from a random point in the database. As demonstrated in Figure 14, MRs have higher \\(R_c\\) than RR Embeddings for an Exact Search on ImageNet-1K for all \\(d \\ge 16\\). This result implies that a portion of MR\u2019s improvement over RR for 1-NN retrieval across all embedding dimensionalities \\(d\\) [31] is due to a higher average separability of the MR 1-NN from a random database point.", "md": "We utilize Relative Contrast [18] to capture the difficulty of nearest neighbors search with IVF-MR compared to IVF-RR. For a given database \\(X = \\{x_i \\in \\mathbb{R}^d, i = 1, ..., N_D\\}\\), a query \\(q \\in \\mathbb{R}^d\\), and a distance metric \\(D(., .)\\) we compute relative contrast \\(C_r\\) as a measure of the difficulty in finding the 1-nearest neighbor (1-NN) for a query \\(q\\) in database \\(X\\) as follows:\n\n1. Compute \\(D_{q_{\\text{min}}} = \\min_{i=1...n} D(q, x_i)\\), i.e. the distance of query \\(q\\) to its nearest neighbor \\(x_{q_{\\text{NN}}}\\)\n2. Compute \\(D_{q_{\\text{mean}}} = E_x[D(q, x)]\\) as the average distance of query \\(q\\) from all database points \\(x \\in X\\)\n3. Relative Contrast of a given query \\(C_{q_r} = \\frac{D_{q_{\\text{mean}}}}{D_{q_{\\text{min}}}}\\), which is a measure of how separable the query\u2019s nearest neighbor \\(x_{q_{\\text{NN}}}\\) is from an average point in the database \\(x\\)\n4. Compute an expectation over all queries for Relative Contrast over the entire database as \\(C_r = E_q[D_{q_{\\text{mean}}}] / E_q[D_{q_{\\text{min}}}]\\)\n\nIt is evident that \\(C_r\\) captures the difficulty of Nearest Neighbor Search in database \\(X\\), as a \\(C_r \\sim 1\\) indicates that for an average query, its nearest neighbor is almost equidistant from a random point in the database. As demonstrated in Figure 14, MRs have higher \\(R_c\\) than RR Embeddings for an Exact Search on ImageNet-1K for all \\(d \\ge 16\\). This result implies that a portion of MR\u2019s improvement over RR for 1-NN retrieval across all embedding dimensionalities \\(d\\) [31] is due to a higher average separability of the MR 1-NN from a random database point."}, {"type": "heading", "lvl": 3, "value": "Generality across Encoders", "md": "### Generality across Encoders"}, {"type": "text", "value": "We perform an ablation over the representation function \\(\\phi : X \\rightarrow \\mathbb{R}^d\\) learnt via a backbone neural network (primarily ResNet50 in this work), as detailed in Section 3. We also train MRL models [31] \\(\\phi_{\\text{MR}}(d)\\) on ResNet18/34/101 [19] that are as accurate as their independently trained RR baseline models \\(\\phi_{\\text{RR}}(d)\\), where \\(d\\) is the default max representation size of each architecture. We also train", "md": "We perform an ablation over the representation function \\(\\phi : X \\rightarrow \\mathbb{R}^d\\) learnt via a backbone neural network (primarily ResNet50 in this work), as detailed in Section 3. We also train MRL models [31] \\(\\phi_{\\text{MR}}(d)\\) on ResNet18/34/101 [19] that are as accurate as their independently trained RR baseline models \\(\\phi_{\\text{RR}}(d)\\), where \\(d\\) is the default max representation size of each architecture. We also train"}]}, {"page": 25, "text": "MRL with a ConvNeXt-Tiny backbone with [d] = {48, 96, 192, 384, 786}. MR-768 has a top-1\naccuracy of 79.45% compared to independently trained publicly available RR-768 baseline with\ntop-1 accuracy 82.1% (Code and RR model available on the official repo6). We note that this training\nhad no hyperparameter tuning whatsoever, and this gap can be closed with additional model training\neffort. We then compare clustering the MRs via IVF-MR with k = 2048, np = 1 on ImageNet-1K to\nExact-MR, which is shown in Figure 15. IVF-MR shows similar trends across backbones compared\nto Exact-MR, i.e. a maximum top-1 accuracy drop of \u223c                     1.6% for a single search probe. This suggests\nthe clustering capabilities of MR extend beyond an inductive bias of \u03d5MR(d) \u2208                              ResNet50, though\nwe leave a detailed exploration for future work.\n                               75\n                              Top-1 Accuracy (%)\n                               70\n                               65\n                               60\n                                                                                          ResNet18\n                                                                                          ResNet32\n                               55                                                         ResNet50\n                                                                         IVF-MR           ResNet101\n                                                                         Exact-MR         ConvNeXt-Tiny\n                               50      8      16      32     64      128    256     512     1024   2048\n                                                      Representation Size\nFigure 15: Top-1 Accuracy variation of IVF-MR on ImageNet-1K with different embedding rep-\nresentation function \u03d5MR(d) (see Section 3), where \u03d5 \u2208                     {ResNet18/34/101, ConvNeXt-Tiny}. We\nobserve similar trends between IVF-MR and Exact-MR on ResNet18/34/101 when compared to\nResNet50 (Figure 9a) which is the default in all experiments in this work.\n    6 https://github.com/facebookresearch/ConvNeXt\n                                                                25", "md": "MRL with a ConvNeXt-Tiny backbone with [d] = {48, 96, 192, 384, 786}. MR-768 has a top-1 accuracy of 79.45% compared to independently trained publicly available RR-768 baseline with top-1 accuracy 82.1% (Code and RR model available on the official repo6). We note that this training had no hyperparameter tuning whatsoever, and this gap can be closed with additional model training effort. We then compare clustering the MRs via IVF-MR with k = 2048, np = 1 on ImageNet-1K to Exact-MR, which is shown in Figure 15. IVF-MR shows similar trends across backbones compared to Exact-MR, i.e. a maximum top-1 accuracy drop of $$\\sim$$ 1.6% for a single search probe. This suggests the clustering capabilities of MR extend beyond an inductive bias of $$\\phi_{MR}(d) \\in$$ ResNet50, though we leave a detailed exploration for future work.\n\n| |Representation Size|\n|---|---|\n|ResNet18|8|\n|ResNet32|16|\n|ResNet50|32|\n|ResNet101|64|\n|ConvNeXt-Tiny|128|\n| |256|\n| |512|\n| |1024|\n| |2048|\n\nFigure 15: Top-1 Accuracy variation of IVF-MR on ImageNet-1K with different embedding representation function $$\\phi_{MR}(d)$$ (see Section 3), where $$\\phi \\in$$ {ResNet18/34/101, ConvNeXt-Tiny}. We observe similar trends between IVF-MR and Exact-MR on ResNet18/34/101 when compared to ResNet50 (Figure 9a) which is the default in all experiments in this work.\n\n6 https://github.com/facebookresearch/ConvNeXt", "images": [], "items": [{"type": "text", "value": "MRL with a ConvNeXt-Tiny backbone with [d] = {48, 96, 192, 384, 786}. MR-768 has a top-1 accuracy of 79.45% compared to independently trained publicly available RR-768 baseline with top-1 accuracy 82.1% (Code and RR model available on the official repo6). We note that this training had no hyperparameter tuning whatsoever, and this gap can be closed with additional model training effort. We then compare clustering the MRs via IVF-MR with k = 2048, np = 1 on ImageNet-1K to Exact-MR, which is shown in Figure 15. IVF-MR shows similar trends across backbones compared to Exact-MR, i.e. a maximum top-1 accuracy drop of $$\\sim$$ 1.6% for a single search probe. This suggests the clustering capabilities of MR extend beyond an inductive bias of $$\\phi_{MR}(d) \\in$$ ResNet50, though we leave a detailed exploration for future work.", "md": "MRL with a ConvNeXt-Tiny backbone with [d] = {48, 96, 192, 384, 786}. MR-768 has a top-1 accuracy of 79.45% compared to independently trained publicly available RR-768 baseline with top-1 accuracy 82.1% (Code and RR model available on the official repo6). We note that this training had no hyperparameter tuning whatsoever, and this gap can be closed with additional model training effort. We then compare clustering the MRs via IVF-MR with k = 2048, np = 1 on ImageNet-1K to Exact-MR, which is shown in Figure 15. IVF-MR shows similar trends across backbones compared to Exact-MR, i.e. a maximum top-1 accuracy drop of $$\\sim$$ 1.6% for a single search probe. This suggests the clustering capabilities of MR extend beyond an inductive bias of $$\\phi_{MR}(d) \\in$$ ResNet50, though we leave a detailed exploration for future work."}, {"type": "table", "rows": [["", "Representation Size"], ["ResNet18", "8"], ["ResNet32", "16"], ["ResNet50", "32"], ["ResNet101", "64"], ["ConvNeXt-Tiny", "128"], ["", "256"], ["", "512"], ["", "1024"], ["", "2048"]], "md": "| |Representation Size|\n|---|---|\n|ResNet18|8|\n|ResNet32|16|\n|ResNet50|32|\n|ResNet101|64|\n|ConvNeXt-Tiny|128|\n| |256|\n| |512|\n| |1024|\n| |2048|", "isPerfectTable": true, "csv": "\"\",\"Representation Size\"\n\"ResNet18\",\"8\"\n\"ResNet32\",\"16\"\n\"ResNet50\",\"32\"\n\"ResNet101\",\"64\"\n\"ConvNeXt-Tiny\",\"128\"\n\"\",\"256\"\n\"\",\"512\"\n\"\",\"1024\"\n\"\",\"2048\""}, {"type": "text", "value": "Figure 15: Top-1 Accuracy variation of IVF-MR on ImageNet-1K with different embedding representation function $$\\phi_{MR}(d)$$ (see Section 3), where $$\\phi \\in$$ {ResNet18/34/101, ConvNeXt-Tiny}. We observe similar trends between IVF-MR and Exact-MR on ResNet18/34/101 when compared to ResNet50 (Figure 9a) which is the default in all experiments in this work.\n\n6 https://github.com/facebookresearch/ConvNeXt", "md": "Figure 15: Top-1 Accuracy variation of IVF-MR on ImageNet-1K with different embedding representation function $$\\phi_{MR}(d)$$ (see Section 3), where $$\\phi \\in$$ {ResNet18/34/101, ConvNeXt-Tiny}. We observe similar trends between IVF-MR and Exact-MR on ResNet18/34/101 when compared to ResNet50 (Figure 9a) which is the default in all experiments in this work.\n\n6 https://github.com/facebookresearch/ConvNeXt"}]}], "job_id": "df22c0cf-f8d4-43da-b9d8-faf8dc381559", "file_path": "./corpus/2305.19435.pdf"}