{"pages": [{"page": 1, "text": "                     Improving Multimodal Datasets with Image Captioning\n                               Thao Nguyen1                    Samir Yitzhak Gadre2                       Gabriel Ilharco1\n                                               Sewoong Oh1,3                    Ludwig Schmidt1,4         ,5\narXiv:2307.10350v2  [cs.LG]  26 Oct 2023                               Abstract\n                       Massive web datasets play a key role in the success of large vision-language models like\n                   CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to\n                   reduce noise often come at the expense of data diversity. Our work focuses on caption quality\n                   as one major source of noise, and studies how generated captions can increase the utility of\n                  web-scraped datapoints with nondescript text. Through exploring different mixing strategies for\n                   raw and generated captions, we outperform the best filtering method proposed by the DataComp\n                   benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M\n                   image-text pairs. Our best approach is also 2\u00d7 better at Flickr and MS-COCO retrieval. We then\n                   analyze what makes synthetic captions an effective source of text supervision. In experimenting\n                  with different image captioning models, we also demonstrate that the performance of a model\n                   on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of\n                   the utility of the captions it generates for multimodal training. Finally, our experiments with\n                   using generated captions at DataComp\u2019s large scale (1.28B image-text pairs) offer insights into\n                   the limitations of synthetic text, as well as the importance of image curation with increasing\n                   training data quantity. The generated captions used in our experiments are now available on\n                   HuggingFace6.\n           1      Introduction\n           Pre-training large multimodal models on image-text pairs sourced from the web has become a\n           standard approach to obtaining high performance on vision tasks [3, 24, 37, 40]. However, raw web\n           data can be noisy or uninformative (Figure 1). Many existing data preprocessing efforts revolve\n           around human-defined heuristics based on image and text content separately\u2014e.g., caption length,\n           presence of nouns, sentence complexity, image aspect ratio, minimum image size [8, 10, 46, 47]\u2014or\n           the reliability of the data source [14]. More complex filtering approaches target poorly aligned\n           image-text pairs, by using trained CLIP models [40] to rank the cosine similarity score between\n           image and text embeddings [46], or ensuring mentions of image objects in the captions [47]. These\n           approaches discard between 60% to 90% of the initial data collected, regardless of whether the\n           images themselves are suitable for training.\n           In this work, we seek to restore the utility of such discarded examples with the help of synthetic\n           captions. To do so, we leverage the DataComp benchmark [18], where initial data processing is kept\n           to a minimum, i.e. only filtering out NSFW examples and train-test overlap. This allows us to\n               1University of Washington.          Correspondence to thaottn@cs.washington.edu                       2Columbia University\n               3Google Research            4Allen Institute for Artificial Intelligence          5LAION\n             6 https://huggingface.co/datasets/thaottn/DataComp_medium_pool_BLIP2_captions,\n               https://huggingface.co/datasets/thaottn/DataComp_large_pool_BLIP2_captions\n                                                                            1", "md": "# Improving Multimodal Datasets with Image Captioning\n\n## Improving Multimodal Datasets with Image Captioning\n\nThao Nguyen1, Samir Yitzhak Gadre2, Gabriel Ilharco1\n\nSewoong Oh1,3, Ludwig Schmidt1,4,5\n\narXiv:2307.10350v2 [cs.LG] 26 Oct 2023\n\n### Abstract\n\nMassive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2\u00d7 better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp\u2019s large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity. The generated captions used in our experiments are now available on HuggingFace.\n\n### Introduction\n\nPre-training large multimodal models on image-text pairs sourced from the web has become a standard approach to obtaining high performance on vision tasks [3, 24, 37, 40]. However, raw web data can be noisy or uninformative. Many existing data preprocessing efforts revolve around human-defined heuristics based on image and text content separately\u2014e.g., caption length, presence of nouns, sentence complexity, image aspect ratio, minimum image size\u2014or the reliability of the data source. More complex filtering approaches target poorly aligned image-text pairs, by using trained CLIP models to rank the cosine similarity score between image and text embeddings, or ensuring mentions of image objects in the captions. These approaches discard between 60% to 90% of the initial data collected, regardless of whether the images themselves are suitable for training.\n\nIn this work, we seek to restore the utility of such discarded examples with the help of synthetic captions. To do so, we leverage the DataComp benchmark, where initial data processing is kept to a minimum, i.e. only filtering out NSFW examples and train-test overlap. This allows us to\n\n1University of Washington. Correspondence to thaottn@cs.washington.edu 2Columbia University\n\n3Google Research 4Allen Institute for Artificial Intelligence 5LAION\n\n6 DataComp Medium Pool BLIP2 Captions, DataComp Large Pool BLIP2 Captions", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Improving Multimodal Datasets with Image Captioning", "md": "# Improving Multimodal Datasets with Image Captioning"}, {"type": "heading", "lvl": 2, "value": "Improving Multimodal Datasets with Image Captioning", "md": "## Improving Multimodal Datasets with Image Captioning"}, {"type": "text", "value": "Thao Nguyen1, Samir Yitzhak Gadre2, Gabriel Ilharco1\n\nSewoong Oh1,3, Ludwig Schmidt1,4,5\n\narXiv:2307.10350v2 [cs.LG] 26 Oct 2023", "md": "Thao Nguyen1, Samir Yitzhak Gadre2, Gabriel Ilharco1\n\nSewoong Oh1,3, Ludwig Schmidt1,4,5\n\narXiv:2307.10350v2 [cs.LG] 26 Oct 2023"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2\u00d7 better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp\u2019s large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity. The generated captions used in our experiments are now available on HuggingFace.", "md": "Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2\u00d7 better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp\u2019s large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity. The generated captions used in our experiments are now available on HuggingFace."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "Pre-training large multimodal models on image-text pairs sourced from the web has become a standard approach to obtaining high performance on vision tasks [3, 24, 37, 40]. However, raw web data can be noisy or uninformative. Many existing data preprocessing efforts revolve around human-defined heuristics based on image and text content separately\u2014e.g., caption length, presence of nouns, sentence complexity, image aspect ratio, minimum image size\u2014or the reliability of the data source. More complex filtering approaches target poorly aligned image-text pairs, by using trained CLIP models to rank the cosine similarity score between image and text embeddings, or ensuring mentions of image objects in the captions. These approaches discard between 60% to 90% of the initial data collected, regardless of whether the images themselves are suitable for training.\n\nIn this work, we seek to restore the utility of such discarded examples with the help of synthetic captions. To do so, we leverage the DataComp benchmark, where initial data processing is kept to a minimum, i.e. only filtering out NSFW examples and train-test overlap. This allows us to\n\n1University of Washington. Correspondence to thaottn@cs.washington.edu 2Columbia University\n\n3Google Research 4Allen Institute for Artificial Intelligence 5LAION\n\n6 DataComp Medium Pool BLIP2 Captions, DataComp Large Pool BLIP2 Captions", "md": "Pre-training large multimodal models on image-text pairs sourced from the web has become a standard approach to obtaining high performance on vision tasks [3, 24, 37, 40]. However, raw web data can be noisy or uninformative. Many existing data preprocessing efforts revolve around human-defined heuristics based on image and text content separately\u2014e.g., caption length, presence of nouns, sentence complexity, image aspect ratio, minimum image size\u2014or the reliability of the data source. More complex filtering approaches target poorly aligned image-text pairs, by using trained CLIP models to rank the cosine similarity score between image and text embeddings, or ensuring mentions of image objects in the captions. These approaches discard between 60% to 90% of the initial data collected, regardless of whether the images themselves are suitable for training.\n\nIn this work, we seek to restore the utility of such discarded examples with the help of synthetic captions. To do so, we leverage the DataComp benchmark, where initial data processing is kept to a minimum, i.e. only filtering out NSFW examples and train-test overlap. This allows us to\n\n1University of Washington. Correspondence to thaottn@cs.washington.edu 2Columbia University\n\n3Google Research 4Allen Institute for Artificial Intelligence 5LAION\n\n6 DataComp Medium Pool BLIP2 Captions, DataComp Large Pool BLIP2 Captions"}]}, {"page": 2, "text": "                   Origin:\n                   Inaq\"\n                   Gener\n                     aree\n Figure 1: Raw captions crawled from the web contain significant noise; cosine similarity filtering\n helps reduce noise but discards many images that are useful for training. Here we show some images\n that would be filtered out if only the top 30% examples from the candidate pool with highest image-text\n cosine similarities are used for training. In these pairs, captions generated by BLIP2 tend to be more faithful\n to the respective images compared to raw captions obtained from the Internet. In Appendix A, we show 20\n other samples drawn completely at random from the discarded pool.\n perform controlled experiments on the raw Common Crawl data and bypass subjective human-design\n choices that may be employed in the creation of other datasets (e.g., LAION-5B [46]). We study\n several image captioning models and find that recent releases (e.g., BLIP2 [30] and OpenCLIP-CoCa\n [38]) can generate captions that improve CLIP training and lead to a significant boost in zero-shot\n performance over existing data curation methods. In particular, at the medium scale (128M samples\n seen), training on the entire candidate pool with synthetic captions is sufficient to outperform\n common filtering baselines that are applied to raw data (e.g., selecting top 30% examples with\n highest image-text cosine similarity based on OpenAI\u2019s CLIP-ViT/L14). Section 5 describes our\n experiments with a variety of mixing strategies to combine signals from both raw and synthetic text.\nTo explain the performance benefits of synthetic captions, we measure caption noise and diversity in\n various training sets, and demonstrate the importance of both factors in achieving good performance.\nWhile existing data filtering methods are effective at reducing noise, they also hurt the diversity of\n the original training data in the process (e.g., by reducing concept coverage). Synthetic captions\n help alleviate this drop in diversity by increasing the number of useful captions available for training.\n In Section 6, we analyze various properties of caption data, as well as specific advantages of training\nwith synthetic captions (e.g., improved retrieval capabilities).\n Remarkably, our empirical investigation in Section 4 shows that choosing a captioning model to\n yield competitive downstream performance is non-trivial, as better performance on image captioning\n benchmarks does not necessarily mean better generated captions for CLIP training. We also note\n that while this work focuses on the quality of captions used in multimodal training, image quality is\n another equally important topic of study. As the size of the data pool we experiment with grows, we\n start to observe changes in the relative importance of text quality versus image quality in building a\n good pre-training dataset. We comment on this in Section 7.\nTo summarize, our findings serve as a first step towards improving the quality of web-scale datasets\n                                                       2", "md": "# Document\n\n## Origin:\n\nInaq\" Gener aree\n\nFigure 1: Raw captions crawled from the web contain significant noise; cosine similarity filtering helps reduce noise but discards many images that are useful for training. Here we show some images that would be filtered out if only the top 30% examples from the candidate pool with highest image-text cosine similarities are used for training. In these pairs, captions generated by BLIP2 tend to be more faithful to the respective images compared to raw captions obtained from the Internet. In Appendix A, we show 20 other samples drawn completely at random from the discarded pool.\n\nperform controlled experiments on the raw Common Crawl data and bypass subjective human-design choices that may be employed in the creation of other datasets (e.g., LAION-5B [46]). We study several image captioning models and find that recent releases (e.g., BLIP2 [30] and OpenCLIP-CoCa [38]) can generate captions that improve CLIP training and lead to a significant boost in zero-shot performance over existing data curation methods. In particular, at the medium scale (128M samples seen), training on the entire candidate pool with synthetic captions is sufficient to outperform common filtering baselines that are applied to raw data (e.g., selecting top 30% examples with highest image-text cosine similarity based on OpenAI\u2019s CLIP-ViT/L14). Section 5 describes our experiments with a variety of mixing strategies to combine signals from both raw and synthetic text.\n\nTo explain the performance benefits of synthetic captions, we measure caption noise and diversity in various training sets, and demonstrate the importance of both factors in achieving good performance. While existing data filtering methods are effective at reducing noise, they also hurt the diversity of the original training data in the process (e.g., by reducing concept coverage). Synthetic captions help alleviate this drop in diversity by increasing the number of useful captions available for training. In Section 6, we analyze various properties of caption data, as well as specific advantages of training with synthetic captions (e.g., improved retrieval capabilities).\n\nRemarkably, our empirical investigation in Section 4 shows that choosing a captioning model to yield competitive downstream performance is non-trivial, as better performance on image captioning benchmarks does not necessarily mean better generated captions for CLIP training. We also note that while this work focuses on the quality of captions used in multimodal training, image quality is another equally important topic of study. As the size of the data pool we experiment with grows, we start to observe changes in the relative importance of text quality versus image quality in building a good pre-training dataset. We comment on this in Section 7.\n\nTo summarize, our findings serve as a first step towards improving the quality of web-scale datasets.", "images": [{"name": "page-2-4.jpg", "height": 50, "width": 50, "x": 306, "y": 135}, {"name": "page-2-5.jpg", "height": 51, "width": 91, "x": 85, "y": 75}, {"name": "page-2-3.jpg", "height": 50, "width": 66, "x": 89, "y": 194}, {"name": "page-2-1.jpg", "height": 50, "width": 66, "x": 298, "y": 194}, {"name": "page-2-2.jpg", "height": 50, "width": 62, "x": 300, "y": 76}, {"name": "page-2-0.jpg", "height": 50, "width": 66, "x": 89, "y": 135}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Origin:", "md": "## Origin:"}, {"type": "text", "value": "Inaq\" Gener aree\n\nFigure 1: Raw captions crawled from the web contain significant noise; cosine similarity filtering helps reduce noise but discards many images that are useful for training. Here we show some images that would be filtered out if only the top 30% examples from the candidate pool with highest image-text cosine similarities are used for training. In these pairs, captions generated by BLIP2 tend to be more faithful to the respective images compared to raw captions obtained from the Internet. In Appendix A, we show 20 other samples drawn completely at random from the discarded pool.\n\nperform controlled experiments on the raw Common Crawl data and bypass subjective human-design choices that may be employed in the creation of other datasets (e.g., LAION-5B [46]). We study several image captioning models and find that recent releases (e.g., BLIP2 [30] and OpenCLIP-CoCa [38]) can generate captions that improve CLIP training and lead to a significant boost in zero-shot performance over existing data curation methods. In particular, at the medium scale (128M samples seen), training on the entire candidate pool with synthetic captions is sufficient to outperform common filtering baselines that are applied to raw data (e.g., selecting top 30% examples with highest image-text cosine similarity based on OpenAI\u2019s CLIP-ViT/L14). Section 5 describes our experiments with a variety of mixing strategies to combine signals from both raw and synthetic text.\n\nTo explain the performance benefits of synthetic captions, we measure caption noise and diversity in various training sets, and demonstrate the importance of both factors in achieving good performance. While existing data filtering methods are effective at reducing noise, they also hurt the diversity of the original training data in the process (e.g., by reducing concept coverage). Synthetic captions help alleviate this drop in diversity by increasing the number of useful captions available for training. In Section 6, we analyze various properties of caption data, as well as specific advantages of training with synthetic captions (e.g., improved retrieval capabilities).\n\nRemarkably, our empirical investigation in Section 4 shows that choosing a captioning model to yield competitive downstream performance is non-trivial, as better performance on image captioning benchmarks does not necessarily mean better generated captions for CLIP training. We also note that while this work focuses on the quality of captions used in multimodal training, image quality is another equally important topic of study. As the size of the data pool we experiment with grows, we start to observe changes in the relative importance of text quality versus image quality in building a good pre-training dataset. We comment on this in Section 7.\n\nTo summarize, our findings serve as a first step towards improving the quality of web-scale datasets.", "md": "Inaq\" Gener aree\n\nFigure 1: Raw captions crawled from the web contain significant noise; cosine similarity filtering helps reduce noise but discards many images that are useful for training. Here we show some images that would be filtered out if only the top 30% examples from the candidate pool with highest image-text cosine similarities are used for training. In these pairs, captions generated by BLIP2 tend to be more faithful to the respective images compared to raw captions obtained from the Internet. In Appendix A, we show 20 other samples drawn completely at random from the discarded pool.\n\nperform controlled experiments on the raw Common Crawl data and bypass subjective human-design choices that may be employed in the creation of other datasets (e.g., LAION-5B [46]). We study several image captioning models and find that recent releases (e.g., BLIP2 [30] and OpenCLIP-CoCa [38]) can generate captions that improve CLIP training and lead to a significant boost in zero-shot performance over existing data curation methods. In particular, at the medium scale (128M samples seen), training on the entire candidate pool with synthetic captions is sufficient to outperform common filtering baselines that are applied to raw data (e.g., selecting top 30% examples with highest image-text cosine similarity based on OpenAI\u2019s CLIP-ViT/L14). Section 5 describes our experiments with a variety of mixing strategies to combine signals from both raw and synthetic text.\n\nTo explain the performance benefits of synthetic captions, we measure caption noise and diversity in various training sets, and demonstrate the importance of both factors in achieving good performance. While existing data filtering methods are effective at reducing noise, they also hurt the diversity of the original training data in the process (e.g., by reducing concept coverage). Synthetic captions help alleviate this drop in diversity by increasing the number of useful captions available for training. In Section 6, we analyze various properties of caption data, as well as specific advantages of training with synthetic captions (e.g., improved retrieval capabilities).\n\nRemarkably, our empirical investigation in Section 4 shows that choosing a captioning model to yield competitive downstream performance is non-trivial, as better performance on image captioning benchmarks does not necessarily mean better generated captions for CLIP training. We also note that while this work focuses on the quality of captions used in multimodal training, image quality is another equally important topic of study. As the size of the data pool we experiment with grows, we start to observe changes in the relative importance of text quality versus image quality in building a good pre-training dataset. We comment on this in Section 7.\n\nTo summarize, our findings serve as a first step towards improving the quality of web-scale datasets."}]}, {"page": 3, "text": " via the use of synthetic captions. In the process, we offer insights on several research directions:\n\u2022 What are the considerations for choosing a captioning model? We find that specializing a pre-\n   trained network towards image captioning via fine-tuning, and optimizing for high CIDEr score on\n   standard benchmarks in general, end up producing captions that are less effective for multimodal\n   training. Reference-free captioning metrics (e.g., CLIP-S [21]) more reliably reflect the training\n   quality of the generated captions.\n\u2022 How to combine signals from multiple sources of captions? We investigate different strategies for\n   filtering and mixing raw and synthetic captions. This leads to performance gains on DataComp\n   benchmark at small (12.8M pool size), medium (128M pool size) and large (1.28B pool size)\n   scales, compared to existing approaches that utilize only raw data. On ImageNet, the performance\n   benefits diminish with scale. On retrieval tasks, however, the gains are significant across all scales.\n\u2022 What makes synthetic captions effective? Our analysis of text properties shows that on an individual\n   level, synthetic captions are less noisy and contain more visual information. However, at the\n   population level, synthetic captions are less diverse than raw captions. Consequently, using both\n   sources of captions helps improve the overall caption quality, measured in terms of text diversity\n   as well as image-text alignment.\n\u2022 How do benefits of synthetic captions scale? Unlike what was found in the original DataComp\n   experiments, given access to generated captions, the best filtering approach differs across scales.\n   Experimenting with data quantities ranging from 12.8M to 1.28B also allows us to observe some\n   limitations of synthetic captions. We posit that image-based filtering, as well as the diversity gap\n   between model-generated and web-scraped captions, play an increasingly important role in large\n   data regimes.\n More broadly, our results have important implications for future work as additional progress (captured\n by the right metric) in image captioning can further enhance the quality of text used for vision-\n language pre-training. Moreover, the effectiveness of synthetic captions unlocks another massive\n source of training data: uncaptioned web images from Common Crawl. This can ultimately empower\n more large-scale multimodal training by improving the availability of properly aligned and sufficiently\n diverse image-text data.\n 2    Related work\n Synthetic data.      Previous work has explored using synthetic data to create new datasets or\n augment existing ones [12, 15, 19, 25, 36, 41, 56, inter alia].       Closer to our work, Azizi et al.\n [5], Bansal and Grover [6], He et al. [20] use image generation models to create synthetic images\n for classification tasks. In the context of CLIP, Santurkar et al. [44] show that a model trained on\n synthetic captions can outperform a model trained on human-provided captions. The captions were\n generated procedurally for the 120K images in the MS-COCO training set [11] using multi-object\n image labels verified by Mechanical Turk workers, which would be difficult to obtain for web-scale\n datasets like LAION-5B [46] or CommonPool [18] that are about four orders of magnitude larger.\n Most similar to our work is the LAION-COCO dataset [45], containing 600M image-text pairs\n from LAION-5B [46] with synthetic captions generated using BLIP [29] and ranked using CLIP\n models [23, 40]. While [45] heavily filters the raw data pool before generating captions, we work\n with uncurated web datasets. In addition, the generated captions provided by LAION-COCO still\n                                                    3", "md": "# Research Directions on Synthetic Captions\n\n## Research Directions on Synthetic Captions\n\nvia the use of synthetic captions. In the process, we offer insights on several research directions:\n\n- What are the considerations for choosing a captioning model? We find that specializing a pre-trained network towards image captioning via fine-tuning, and optimizing for high CIDEr score on standard benchmarks in general, end up producing captions that are less effective for multimodal training. Reference-free captioning metrics (e.g., CLIP-S [21]) more reliably reflect the training quality of the generated captions.\n- How to combine signals from multiple sources of captions? We investigate different strategies for filtering and mixing raw and synthetic captions. This leads to performance gains on DataComp benchmark at small (12.8M pool size), medium (128M pool size) and large (1.28B pool size) scales, compared to existing approaches that utilize only raw data. On ImageNet, the performance benefits diminish with scale. On retrieval tasks, however, the gains are significant across all scales.\n- What makes synthetic captions effective? Our analysis of text properties shows that on an individual level, synthetic captions are less noisy and contain more visual information. However, at the population level, synthetic captions are less diverse than raw captions. Consequently, using both sources of captions helps improve the overall caption quality, measured in terms of text diversity as well as image-text alignment.\n- How do benefits of synthetic captions scale? Unlike what was found in the original DataComp experiments, given access to generated captions, the best filtering approach differs across scales. Experimenting with data quantities ranging from 12.8M to 1.28B also allows us to observe some limitations of synthetic captions. We posit that image-based filtering, as well as the diversity gap between model-generated and web-scraped captions, play an increasingly important role in large data regimes.\n\nMore broadly, our results have important implications for future work as additional progress (captured by the right metric) in image captioning can further enhance the quality of text used for vision-language pre-training. Moreover, the effectiveness of synthetic captions unlocks another massive source of training data: uncaptioned web images from Common Crawl. This can ultimately empower more large-scale multimodal training by improving the availability of properly aligned and sufficiently diverse image-text data.\n\n### Related work\n\nSynthetic data. Previous work has explored using synthetic data to create new datasets or augment existing ones [12, 15, 19, 25, 36, 41, inter alia]. Closer to our work, Azizi et al. [5], Bansal and Grover [6], He et al. [20] use image generation models to create synthetic images for classification tasks. In the context of CLIP, Santurkar et al. [44] show that a model trained on synthetic captions can outperform a model trained on human-provided captions. The captions were generated procedurally for the 120K images in the MS-COCO training set [11] using multi-object image labels verified by Mechanical Turk workers, which would be difficult to obtain for web-scale datasets like LAION-5B [46] or CommonPool [18] that are about four orders of magnitude larger. Most similar to our work is the LAION-COCO dataset [45], containing 600M image-text pairs from LAION-5B [46] with synthetic captions generated using BLIP [29] and ranked using CLIP models [23, 40]. While [45] heavily filters the raw data pool before generating captions, we work with uncurated web datasets. In addition, the generated captions provided by LAION-COCO still", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Directions on Synthetic Captions", "md": "# Research Directions on Synthetic Captions"}, {"type": "heading", "lvl": 2, "value": "Research Directions on Synthetic Captions", "md": "## Research Directions on Synthetic Captions"}, {"type": "text", "value": "via the use of synthetic captions. In the process, we offer insights on several research directions:\n\n- What are the considerations for choosing a captioning model? We find that specializing a pre-trained network towards image captioning via fine-tuning, and optimizing for high CIDEr score on standard benchmarks in general, end up producing captions that are less effective for multimodal training. Reference-free captioning metrics (e.g., CLIP-S [21]) more reliably reflect the training quality of the generated captions.\n- How to combine signals from multiple sources of captions? We investigate different strategies for filtering and mixing raw and synthetic captions. This leads to performance gains on DataComp benchmark at small (12.8M pool size), medium (128M pool size) and large (1.28B pool size) scales, compared to existing approaches that utilize only raw data. On ImageNet, the performance benefits diminish with scale. On retrieval tasks, however, the gains are significant across all scales.\n- What makes synthetic captions effective? Our analysis of text properties shows that on an individual level, synthetic captions are less noisy and contain more visual information. However, at the population level, synthetic captions are less diverse than raw captions. Consequently, using both sources of captions helps improve the overall caption quality, measured in terms of text diversity as well as image-text alignment.\n- How do benefits of synthetic captions scale? Unlike what was found in the original DataComp experiments, given access to generated captions, the best filtering approach differs across scales. Experimenting with data quantities ranging from 12.8M to 1.28B also allows us to observe some limitations of synthetic captions. We posit that image-based filtering, as well as the diversity gap between model-generated and web-scraped captions, play an increasingly important role in large data regimes.\n\nMore broadly, our results have important implications for future work as additional progress (captured by the right metric) in image captioning can further enhance the quality of text used for vision-language pre-training. Moreover, the effectiveness of synthetic captions unlocks another massive source of training data: uncaptioned web images from Common Crawl. This can ultimately empower more large-scale multimodal training by improving the availability of properly aligned and sufficiently diverse image-text data.", "md": "via the use of synthetic captions. In the process, we offer insights on several research directions:\n\n- What are the considerations for choosing a captioning model? We find that specializing a pre-trained network towards image captioning via fine-tuning, and optimizing for high CIDEr score on standard benchmarks in general, end up producing captions that are less effective for multimodal training. Reference-free captioning metrics (e.g., CLIP-S [21]) more reliably reflect the training quality of the generated captions.\n- How to combine signals from multiple sources of captions? We investigate different strategies for filtering and mixing raw and synthetic captions. This leads to performance gains on DataComp benchmark at small (12.8M pool size), medium (128M pool size) and large (1.28B pool size) scales, compared to existing approaches that utilize only raw data. On ImageNet, the performance benefits diminish with scale. On retrieval tasks, however, the gains are significant across all scales.\n- What makes synthetic captions effective? Our analysis of text properties shows that on an individual level, synthetic captions are less noisy and contain more visual information. However, at the population level, synthetic captions are less diverse than raw captions. Consequently, using both sources of captions helps improve the overall caption quality, measured in terms of text diversity as well as image-text alignment.\n- How do benefits of synthetic captions scale? Unlike what was found in the original DataComp experiments, given access to generated captions, the best filtering approach differs across scales. Experimenting with data quantities ranging from 12.8M to 1.28B also allows us to observe some limitations of synthetic captions. We posit that image-based filtering, as well as the diversity gap between model-generated and web-scraped captions, play an increasingly important role in large data regimes.\n\nMore broadly, our results have important implications for future work as additional progress (captured by the right metric) in image captioning can further enhance the quality of text used for vision-language pre-training. Moreover, the effectiveness of synthetic captions unlocks another massive source of training data: uncaptioned web images from Common Crawl. This can ultimately empower more large-scale multimodal training by improving the availability of properly aligned and sufficiently diverse image-text data."}, {"type": "heading", "lvl": 3, "value": "Related work", "md": "### Related work"}, {"type": "text", "value": "Synthetic data. Previous work has explored using synthetic data to create new datasets or augment existing ones [12, 15, 19, 25, 36, 41, inter alia]. Closer to our work, Azizi et al. [5], Bansal and Grover [6], He et al. [20] use image generation models to create synthetic images for classification tasks. In the context of CLIP, Santurkar et al. [44] show that a model trained on synthetic captions can outperform a model trained on human-provided captions. The captions were generated procedurally for the 120K images in the MS-COCO training set [11] using multi-object image labels verified by Mechanical Turk workers, which would be difficult to obtain for web-scale datasets like LAION-5B [46] or CommonPool [18] that are about four orders of magnitude larger. Most similar to our work is the LAION-COCO dataset [45], containing 600M image-text pairs from LAION-5B [46] with synthetic captions generated using BLIP [29] and ranked using CLIP models [23, 40]. While [45] heavily filters the raw data pool before generating captions, we work with uncurated web datasets. In addition, the generated captions provided by LAION-COCO still", "md": "Synthetic data. Previous work has explored using synthetic data to create new datasets or augment existing ones [12, 15, 19, 25, 36, 41, inter alia]. Closer to our work, Azizi et al. [5], Bansal and Grover [6], He et al. [20] use image generation models to create synthetic images for classification tasks. In the context of CLIP, Santurkar et al. [44] show that a model trained on synthetic captions can outperform a model trained on human-provided captions. The captions were generated procedurally for the 120K images in the MS-COCO training set [11] using multi-object image labels verified by Mechanical Turk workers, which would be difficult to obtain for web-scale datasets like LAION-5B [46] or CommonPool [18] that are about four orders of magnitude larger. Most similar to our work is the LAION-COCO dataset [45], containing 600M image-text pairs from LAION-5B [46] with synthetic captions generated using BLIP [29] and ranked using CLIP models [23, 40]. While [45] heavily filters the raw data pool before generating captions, we work with uncurated web datasets. In addition, the generated captions provided by LAION-COCO still"}]}, {"page": 4, "text": "significantly lag behind the corresponding web-crawled captions when it comes to yielding good\nCLIP performance\u2014we provide empirical evidence and address this gap in Appendix G.\nImage captioning.       Building models able to generate captions from images has been a long-\nstanding subject of research [13, 27, 28, 31, 52, 53, inter alia]. More recently, models like BLIP2\n[29, 30], Flamingo [3], and CoCa [38, 55] have made significant progress on this task. It is worth\nnoting that the training data for BLIP [29] and BLIP2 [30] contains synthetic captions, as the\nauthors find that this helps boost the captioning ability of the resulting model compared to training\non just noisy web data. Zhu et al. [57] couple large language models with image captioning models\nto generate more enriched image descriptions. We expect that as these image captioning systems\nbecome more capable, the impact of using synthetic data will bring larger improvements over existing\nnoisy image-text datasets.\nImproving image-text datasets.         Given the importance of the pre-training data for multimodal\nnetworks [17, 18, 33], several authors have proposed techniques for improving the quality of image-\ntext datasets. Radenovic et al. [39] propose a filtering technique called Complexity, Action, and\nText-spotting (CAT), designed to select only informative image-text pairs. Cao et al. [9] filter out\nsamples that contain text regions in the image and advocate for the benefits of increasing the number\nof samples given a fixed compute budget. Instead of discarding all text-spotting examples, Maini\net al. [32] proposes masking out the text part in the image and only removing image-text pairs in\nwhich the masked image contains no useful visual features. Abbas et al. [1] identify and remove\nsamples that are semantically similar to each other. Many image-text datasets also have their own\npreprocessing techniques, often not fully disclosed [10, 14, 24, 37, 40, 46]. All of these filtering\napproaches are complementary to the use of synthetic captions proposed by this work.\nConcurrent to our work, Fan et al. [16] present a form of data augmentation for training CLIP models\nwhere the captions are rewritten by a large language model. However, the rewriting process assumes\naccess to some raw text and is not conditioned on the images, which may limit its effectiveness\nwhen the original captions are not descriptive (e.g., see Figure 1). In contrast, our work uses image\ncaptioning models, which are able to generate relevant captions for images regardless of the original\ntext associated with them. We also work with raw Common Crawl data instead of preprocessed\ndatasets to study the trade-offs between raw and generated captions in a systematic manner. Finally,\nGadre et al. [18] introduces DataComp, a benchmark for designing better pre-training datasets for\nCLIP, which we use in experiments throughout the paper.\n3    Experiment setup\nData.    Most of our experiments involve the CommonPool provided by the DataComp benchmark\n[18]. CommonPool contains image-text pairs sourced from Common Crawl dumps between 2014 and\n2022, deduplicated and randomly shuffled. The small, medium, and large scales of the benchmark\ncontain 12.8M, 128M and 1.28B candidate pairs respectively. Data preprocessing is kept to a\nminimum, involving only NSFW filtering, evaluation set deduplication, and face blurring, to allow\nmaximum flexibility for dataset design. We also experiment with LAION-COCO [45] and discuss in\nAppendix G why it is not ideal for studying how to improve the quality of raw training data.\n                                                  4", "md": "significantly lag behind the corresponding web-crawled captions when it comes to yielding good CLIP performance\u2014we provide empirical evidence and address this gap in Appendix G.\n\nImage captioning. Building models able to generate captions from images has been a long-standing subject of research [13, 27, 28, 31, 52, 53, inter alia]. More recently, models like BLIP2 [29, 30], Flamingo [3], and CoCa [38, 55] have made significant progress on this task. It is worth noting that the training data for BLIP [29] and BLIP2 [30] contains synthetic captions, as the authors find that this helps boost the captioning ability of the resulting model compared to training on just noisy web data. Zhu et al. [57] couple large language models with image captioning models to generate more enriched image descriptions. We expect that as these image captioning systems become more capable, the impact of using synthetic data will bring larger improvements over existing noisy image-text datasets.\n\nImproving image-text datasets. Given the importance of the pre-training data for multimodal networks [17, 18, 33], several authors have proposed techniques for improving the quality of image-text datasets. Radenovic et al. [39] propose a filtering technique called Complexity, Action, and Text-spotting (CAT), designed to select only informative image-text pairs. Cao et al. [9] filter out samples that contain text regions in the image and advocate for the benefits of increasing the number of samples given a fixed compute budget. Instead of discarding all text-spotting examples, Maini et al. [32] proposes masking out the text part in the image and only removing image-text pairs in which the masked image contains no useful visual features. Abbas et al. [1] identify and remove samples that are semantically similar to each other. Many image-text datasets also have their own preprocessing techniques, often not fully disclosed [10, 14, 24, 37, 40, 46]. All of these filtering approaches are complementary to the use of synthetic captions proposed by this work.\n\nConcurrent to our work, Fan et al. [16] present a form of data augmentation for training CLIP models where the captions are rewritten by a large language model. However, the rewriting process assumes access to some raw text and is not conditioned on the images, which may limit its effectiveness when the original captions are not descriptive (e.g., see Figure 1). In contrast, our work uses image captioning models, which are able to generate relevant captions for images regardless of the original text associated with them. We also work with raw Common Crawl data instead of preprocessed datasets to study the trade-offs between raw and generated captions in a systematic manner. Finally, Gadre et al. [18] introduces DataComp, a benchmark for designing better pre-training datasets for CLIP, which we use in experiments throughout the paper.\n\n### Experiment setup\n\nData. Most of our experiments involve the CommonPool provided by the DataComp benchmark [18]. CommonPool contains image-text pairs sourced from Common Crawl dumps between 2014 and 2022, deduplicated and randomly shuffled. The small, medium, and large scales of the benchmark contain 12.8M, 128M and 1.28B candidate pairs respectively. Data preprocessing is kept to a minimum, involving only NSFW filtering, evaluation set deduplication, and face blurring, to allow maximum flexibility for dataset design. We also experiment with LAION-COCO [45] and discuss in Appendix G why it is not ideal for studying how to improve the quality of raw training data.\n\n1. Reference", "images": [], "items": [{"type": "text", "value": "significantly lag behind the corresponding web-crawled captions when it comes to yielding good CLIP performance\u2014we provide empirical evidence and address this gap in Appendix G.\n\nImage captioning. Building models able to generate captions from images has been a long-standing subject of research [13, 27, 28, 31, 52, 53, inter alia]. More recently, models like BLIP2 [29, 30], Flamingo [3], and CoCa [38, 55] have made significant progress on this task. It is worth noting that the training data for BLIP [29] and BLIP2 [30] contains synthetic captions, as the authors find that this helps boost the captioning ability of the resulting model compared to training on just noisy web data. Zhu et al. [57] couple large language models with image captioning models to generate more enriched image descriptions. We expect that as these image captioning systems become more capable, the impact of using synthetic data will bring larger improvements over existing noisy image-text datasets.\n\nImproving image-text datasets. Given the importance of the pre-training data for multimodal networks [17, 18, 33], several authors have proposed techniques for improving the quality of image-text datasets. Radenovic et al. [39] propose a filtering technique called Complexity, Action, and Text-spotting (CAT), designed to select only informative image-text pairs. Cao et al. [9] filter out samples that contain text regions in the image and advocate for the benefits of increasing the number of samples given a fixed compute budget. Instead of discarding all text-spotting examples, Maini et al. [32] proposes masking out the text part in the image and only removing image-text pairs in which the masked image contains no useful visual features. Abbas et al. [1] identify and remove samples that are semantically similar to each other. Many image-text datasets also have their own preprocessing techniques, often not fully disclosed [10, 14, 24, 37, 40, 46]. All of these filtering approaches are complementary to the use of synthetic captions proposed by this work.\n\nConcurrent to our work, Fan et al. [16] present a form of data augmentation for training CLIP models where the captions are rewritten by a large language model. However, the rewriting process assumes access to some raw text and is not conditioned on the images, which may limit its effectiveness when the original captions are not descriptive (e.g., see Figure 1). In contrast, our work uses image captioning models, which are able to generate relevant captions for images regardless of the original text associated with them. We also work with raw Common Crawl data instead of preprocessed datasets to study the trade-offs between raw and generated captions in a systematic manner. Finally, Gadre et al. [18] introduces DataComp, a benchmark for designing better pre-training datasets for CLIP, which we use in experiments throughout the paper.", "md": "significantly lag behind the corresponding web-crawled captions when it comes to yielding good CLIP performance\u2014we provide empirical evidence and address this gap in Appendix G.\n\nImage captioning. Building models able to generate captions from images has been a long-standing subject of research [13, 27, 28, 31, 52, 53, inter alia]. More recently, models like BLIP2 [29, 30], Flamingo [3], and CoCa [38, 55] have made significant progress on this task. It is worth noting that the training data for BLIP [29] and BLIP2 [30] contains synthetic captions, as the authors find that this helps boost the captioning ability of the resulting model compared to training on just noisy web data. Zhu et al. [57] couple large language models with image captioning models to generate more enriched image descriptions. We expect that as these image captioning systems become more capable, the impact of using synthetic data will bring larger improvements over existing noisy image-text datasets.\n\nImproving image-text datasets. Given the importance of the pre-training data for multimodal networks [17, 18, 33], several authors have proposed techniques for improving the quality of image-text datasets. Radenovic et al. [39] propose a filtering technique called Complexity, Action, and Text-spotting (CAT), designed to select only informative image-text pairs. Cao et al. [9] filter out samples that contain text regions in the image and advocate for the benefits of increasing the number of samples given a fixed compute budget. Instead of discarding all text-spotting examples, Maini et al. [32] proposes masking out the text part in the image and only removing image-text pairs in which the masked image contains no useful visual features. Abbas et al. [1] identify and remove samples that are semantically similar to each other. Many image-text datasets also have their own preprocessing techniques, often not fully disclosed [10, 14, 24, 37, 40, 46]. All of these filtering approaches are complementary to the use of synthetic captions proposed by this work.\n\nConcurrent to our work, Fan et al. [16] present a form of data augmentation for training CLIP models where the captions are rewritten by a large language model. However, the rewriting process assumes access to some raw text and is not conditioned on the images, which may limit its effectiveness when the original captions are not descriptive (e.g., see Figure 1). In contrast, our work uses image captioning models, which are able to generate relevant captions for images regardless of the original text associated with them. We also work with raw Common Crawl data instead of preprocessed datasets to study the trade-offs between raw and generated captions in a systematic manner. Finally, Gadre et al. [18] introduces DataComp, a benchmark for designing better pre-training datasets for CLIP, which we use in experiments throughout the paper."}, {"type": "heading", "lvl": 3, "value": "Experiment setup", "md": "### Experiment setup"}, {"type": "text", "value": "Data. Most of our experiments involve the CommonPool provided by the DataComp benchmark [18]. CommonPool contains image-text pairs sourced from Common Crawl dumps between 2014 and 2022, deduplicated and randomly shuffled. The small, medium, and large scales of the benchmark contain 12.8M, 128M and 1.28B candidate pairs respectively. Data preprocessing is kept to a minimum, involving only NSFW filtering, evaluation set deduplication, and face blurring, to allow maximum flexibility for dataset design. We also experiment with LAION-COCO [45] and discuss in Appendix G why it is not ideal for studying how to improve the quality of raw training data.\n\n1. Reference", "md": "Data. Most of our experiments involve the CommonPool provided by the DataComp benchmark [18]. CommonPool contains image-text pairs sourced from Common Crawl dumps between 2014 and 2022, deduplicated and randomly shuffled. The small, medium, and large scales of the benchmark contain 12.8M, 128M and 1.28B candidate pairs respectively. Data preprocessing is kept to a minimum, involving only NSFW filtering, evaluation set deduplication, and face blurring, to allow maximum flexibility for dataset design. We also experiment with LAION-COCO [45] and discuss in Appendix G why it is not ideal for studying how to improve the quality of raw training data.\n\n1. Reference"}]}, {"page": 5, "text": "Captioning models.       We experiment with BLIP [29] and BLIP2 [30] using HuggingFace\u2019s Trans-\nformers framework. Both models were pre-trained on 129M image-text pairs from the web including\nMS-COCO [11] and LAION-400M [46], in addition to the bootstrapped version of the web data with\nsynthetic captions generated by BLIP\u2019s captioner. We also look at OpenCLIP-CoCa [23, 38], which\nwas trained on LAION-2B [46]. For each architecture, we experiment with both the pre-trained\nmodel and the one that has been fine-tuned on MS-COCO. Caption generation uses top-K sampling\nwith K = 50, minimum caption length 5, and maximum caption length 40.\nTraining.    Given CommonPool data of a particular scale, we generate synthetic captions for the\nimages in the pool using the captioning models described above. Then we train a CLIP model on\nthe resulting image-text datasets, using ViT-B/32 as the image encoder for the small and medium\nscales, and ViT-B/16 for the large scale. Following DataComp\u2019s setup [18], the compute budget,\narchitecture and hyperparameters for each scale are fixed in order to isolate data quality as the main\nfactor influencing performance. Given a candidate pool of N image-text pairs, the CLIP model is\nthen trained with N samples seen in total. Refer to Appendix B for more details.\nEvaluation.     We adopt DataComp\u2019s zero-shot evaluation suite and report both ImageNet accuracy\nand the average accuracy over 38 classification and retrieval tasks proposed by the benchmark [18].\nWe also pay particular attention to retrieval performance on Flickr30K [54] and MS-COCO [11].\nThe retrieval score reported is the average of text-to-image Recall@1 and image-to-text Recall@1.\nUnless specified otherwise, in the subsequent sections, \u201cCLIP score filtering\u201d or \u201ctop x%\u201d refers to\nselecting top x% examples from the initial training set, based on the cosine similarity between image\nand text embeddings output by OpenAI\u2019s CLIP ViT-L/14 model [40], and \u201cBLIP2\u201d refers to captions\ngenerated by BLIP2, using top-K sampling with softmax temperature 0.75, which we have found to\nyield the best downstream performance compared to other sampling temperatures (see Appendix C).\n4    Impact of model specialization on captions generated for multi-\n     modal training\nGiven the abundance of image captioning models to choose from, a natural question to ask is: does\nperformance on standard image captioning benchmarks correlate with how useful the generated\ncaptions are as text supervision for CLIP training?\nIn particular, CIDEr [51], together with other reference-based metrics like SPICE [4] and BLEU-4\n[35], has been widely adopted as a yardstick for determining state-of-the-art on image captioning\nbenchmarks [3, 22, 29, 30, 55]. Consequently, previous work [29, 30, 55] also experiments with\nfine-tuning captioning models on MS-COCO and obtains competitive CIDEr scores on popular\nevaluation sets like NoCaps [2].\nWe compare the utility of synthetic captions produced by BLIP2 and OpenCLIP-CoCa with and\nwithout fine-tuning on MS-COCO, by training CLIP on the generated captions and evaluating the\ntrained model on ImageNet classification and Flickr retrieval (Table 1). Fine-tuned captioning models\nproduce captions that boost the retrieval capabilities of CLIP, but hurts its ImageNet classification\nperformance. We hypothesize that fine-tuning on MS-COCO reduces the diversity of the generated\ntext, as evidenced by the lower number of unique trigrams across 1M caption samples (Table 1).\n                                                   5", "md": "# Impact of model specialization on captions generated for multi-modal training\n\nCaptioning models. We experiment with BLIP [29] and BLIP2 [30] using HuggingFace\u2019s Transformers framework. Both models were pre-trained on 129M image-text pairs from the web including MS-COCO [11] and LAION-400M [46], in addition to the bootstrapped version of the web data with synthetic captions generated by BLIP\u2019s captioner. We also look at OpenCLIP-CoCa [23, 38], which was trained on LAION-2B [46]. For each architecture, we experiment with both the pre-trained model and the one that has been fine-tuned on MS-COCO. Caption generation uses top-K sampling with K = 50, minimum caption length 5, and maximum caption length 40.\n\nTraining. Given CommonPool data of a particular scale, we generate synthetic captions for the images in the pool using the captioning models described above. Then we train a CLIP model on the resulting image-text datasets, using ViT-B/32 as the image encoder for the small and medium scales, and ViT-B/16 for the large scale. Following DataComp\u2019s setup [18], the compute budget, architecture and hyperparameters for each scale are fixed in order to isolate data quality as the main factor influencing performance. Given a candidate pool of N image-text pairs, the CLIP model is then trained with N samples seen in total. Refer to Appendix B for more details.\n\nEvaluation. We adopt DataComp\u2019s zero-shot evaluation suite and report both ImageNet accuracy and the average accuracy over 38 classification and retrieval tasks proposed by the benchmark [18]. We also pay particular attention to retrieval performance on Flickr30K [54] and MS-COCO [11]. The retrieval score reported is the average of text-to-image Recall@1 and image-to-text Recall@1. Unless specified otherwise, in the subsequent sections, \u201cCLIP score filtering\u201d or \u201ctop x%\u201d refers to selecting top x% examples from the initial training set, based on the cosine similarity between image and text embeddings output by OpenAI\u2019s CLIP ViT-L/14 model [40], and \u201cBLIP2\u201d refers to captions generated by BLIP2, using top-K sampling with softmax temperature 0.75, which we have found to yield the best downstream performance compared to other sampling temperatures (see Appendix C).\n\n#### Impact of model specialization on captions generated for multi-modal training\n\nGiven the abundance of image captioning models to choose from, a natural question to ask is: does performance on standard image captioning benchmarks correlate with how useful the generated captions are as text supervision for CLIP training?\n\nIn particular, CIDEr [51], together with other reference-based metrics like SPICE [4] and BLEU-4 [35], has been widely adopted as a yardstick for determining state-of-the-art on image captioning benchmarks [3, 22, 29, 30, 55]. Consequently, previous work [29, 30, 55] also experiments with fine-tuning captioning models on MS-COCO and obtains competitive CIDEr scores on popular evaluation sets like NoCaps [2].\n\nWe compare the utility of synthetic captions produced by BLIP2 and OpenCLIP-CoCa with and without fine-tuning on MS-COCO, by training CLIP on the generated captions and evaluating the trained model on ImageNet classification and Flickr retrieval (Table 1). Fine-tuned captioning models produce captions that boost the retrieval capabilities of CLIP, but hurts its ImageNet classification performance. We hypothesize that fine-tuning on MS-COCO reduces the diversity of the generated text, as evidenced by the lower number of unique trigrams across 1M caption samples (Table 1).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Impact of model specialization on captions generated for multi-modal training", "md": "# Impact of model specialization on captions generated for multi-modal training"}, {"type": "text", "value": "Captioning models. We experiment with BLIP [29] and BLIP2 [30] using HuggingFace\u2019s Transformers framework. Both models were pre-trained on 129M image-text pairs from the web including MS-COCO [11] and LAION-400M [46], in addition to the bootstrapped version of the web data with synthetic captions generated by BLIP\u2019s captioner. We also look at OpenCLIP-CoCa [23, 38], which was trained on LAION-2B [46]. For each architecture, we experiment with both the pre-trained model and the one that has been fine-tuned on MS-COCO. Caption generation uses top-K sampling with K = 50, minimum caption length 5, and maximum caption length 40.\n\nTraining. Given CommonPool data of a particular scale, we generate synthetic captions for the images in the pool using the captioning models described above. Then we train a CLIP model on the resulting image-text datasets, using ViT-B/32 as the image encoder for the small and medium scales, and ViT-B/16 for the large scale. Following DataComp\u2019s setup [18], the compute budget, architecture and hyperparameters for each scale are fixed in order to isolate data quality as the main factor influencing performance. Given a candidate pool of N image-text pairs, the CLIP model is then trained with N samples seen in total. Refer to Appendix B for more details.\n\nEvaluation. We adopt DataComp\u2019s zero-shot evaluation suite and report both ImageNet accuracy and the average accuracy over 38 classification and retrieval tasks proposed by the benchmark [18]. We also pay particular attention to retrieval performance on Flickr30K [54] and MS-COCO [11]. The retrieval score reported is the average of text-to-image Recall@1 and image-to-text Recall@1. Unless specified otherwise, in the subsequent sections, \u201cCLIP score filtering\u201d or \u201ctop x%\u201d refers to selecting top x% examples from the initial training set, based on the cosine similarity between image and text embeddings output by OpenAI\u2019s CLIP ViT-L/14 model [40], and \u201cBLIP2\u201d refers to captions generated by BLIP2, using top-K sampling with softmax temperature 0.75, which we have found to yield the best downstream performance compared to other sampling temperatures (see Appendix C).", "md": "Captioning models. We experiment with BLIP [29] and BLIP2 [30] using HuggingFace\u2019s Transformers framework. Both models were pre-trained on 129M image-text pairs from the web including MS-COCO [11] and LAION-400M [46], in addition to the bootstrapped version of the web data with synthetic captions generated by BLIP\u2019s captioner. We also look at OpenCLIP-CoCa [23, 38], which was trained on LAION-2B [46]. For each architecture, we experiment with both the pre-trained model and the one that has been fine-tuned on MS-COCO. Caption generation uses top-K sampling with K = 50, minimum caption length 5, and maximum caption length 40.\n\nTraining. Given CommonPool data of a particular scale, we generate synthetic captions for the images in the pool using the captioning models described above. Then we train a CLIP model on the resulting image-text datasets, using ViT-B/32 as the image encoder for the small and medium scales, and ViT-B/16 for the large scale. Following DataComp\u2019s setup [18], the compute budget, architecture and hyperparameters for each scale are fixed in order to isolate data quality as the main factor influencing performance. Given a candidate pool of N image-text pairs, the CLIP model is then trained with N samples seen in total. Refer to Appendix B for more details.\n\nEvaluation. We adopt DataComp\u2019s zero-shot evaluation suite and report both ImageNet accuracy and the average accuracy over 38 classification and retrieval tasks proposed by the benchmark [18]. We also pay particular attention to retrieval performance on Flickr30K [54] and MS-COCO [11]. The retrieval score reported is the average of text-to-image Recall@1 and image-to-text Recall@1. Unless specified otherwise, in the subsequent sections, \u201cCLIP score filtering\u201d or \u201ctop x%\u201d refers to selecting top x% examples from the initial training set, based on the cosine similarity between image and text embeddings output by OpenAI\u2019s CLIP ViT-L/14 model [40], and \u201cBLIP2\u201d refers to captions generated by BLIP2, using top-K sampling with softmax temperature 0.75, which we have found to yield the best downstream performance compared to other sampling temperatures (see Appendix C)."}, {"type": "heading", "lvl": 4, "value": "Impact of model specialization on captions generated for multi-modal training", "md": "#### Impact of model specialization on captions generated for multi-modal training"}, {"type": "text", "value": "Given the abundance of image captioning models to choose from, a natural question to ask is: does performance on standard image captioning benchmarks correlate with how useful the generated captions are as text supervision for CLIP training?\n\nIn particular, CIDEr [51], together with other reference-based metrics like SPICE [4] and BLEU-4 [35], has been widely adopted as a yardstick for determining state-of-the-art on image captioning benchmarks [3, 22, 29, 30, 55]. Consequently, previous work [29, 30, 55] also experiments with fine-tuning captioning models on MS-COCO and obtains competitive CIDEr scores on popular evaluation sets like NoCaps [2].\n\nWe compare the utility of synthetic captions produced by BLIP2 and OpenCLIP-CoCa with and without fine-tuning on MS-COCO, by training CLIP on the generated captions and evaluating the trained model on ImageNet classification and Flickr retrieval (Table 1). Fine-tuned captioning models produce captions that boost the retrieval capabilities of CLIP, but hurts its ImageNet classification performance. We hypothesize that fine-tuning on MS-COCO reduces the diversity of the generated text, as evidenced by the lower number of unique trigrams across 1M caption samples (Table 1).", "md": "Given the abundance of image captioning models to choose from, a natural question to ask is: does performance on standard image captioning benchmarks correlate with how useful the generated captions are as text supervision for CLIP training?\n\nIn particular, CIDEr [51], together with other reference-based metrics like SPICE [4] and BLEU-4 [35], has been widely adopted as a yardstick for determining state-of-the-art on image captioning benchmarks [3, 22, 29, 30, 55]. Consequently, previous work [29, 30, 55] also experiments with fine-tuning captioning models on MS-COCO and obtains competitive CIDEr scores on popular evaluation sets like NoCaps [2].\n\nWe compare the utility of synthetic captions produced by BLIP2 and OpenCLIP-CoCa with and without fine-tuning on MS-COCO, by training CLIP on the generated captions and evaluating the trained model on ImageNet classification and Flickr retrieval (Table 1). Fine-tuned captioning models produce captions that boost the retrieval capabilities of CLIP, but hurts its ImageNet classification performance. We hypothesize that fine-tuning on MS-COCO reduces the diversity of the generated text, as evidenced by the lower number of unique trigrams across 1M caption samples (Table 1)."}]}, {"page": 6, "text": "  Captioning model           NoCaps         CLIP-S      Cosine       No. of unique      ImageNet     Flickr\n                             CIDEr [51]     [21]        similarity   trigrams           accuracy     retrieval\n  BLIP, ViT-L/16             113.2*         0.698       0.231        2.82 \u00d7 106         0.207        0.498\n  (finetuned)\n  BLIP2, ViT-g               80.6           0.737       0.251        2.72 \u00d7 106         0.281        0.507\n  BLIP2, ViT-g               119.7*         0.711       0.235        1.97 \u00d7 106         0.227        0.549\n  (finetuned)\n  OpenCLIP-CoCa,             0.354*         0.752       0.260        4.45 \u00d7 106         0.321        0.395\n  ViT-L/14\n  OpenCLIP-CoCa,             106.5*         0.702       0.232        1.81 \u00d7 106         0.252        0.542\n  ViT-L/14 (finetuned)\nTable 1: CIDEr score does not reliably predict how effective a captioning model is at generating\n synthetic captions for multimodal pre-training; fine-tuning image captioning models leads to\n lower ImageNet accuracy when training CLIP on the generated captions. * indicates numbers\n obtained from previous work and from contacting the authors. We fix the architecture and compare captions\n generated from captioning models with and without fine-tuning on MS-COCO [11] as sources of text\n supervision for CLIP. Fine-tuning pre-trained networks on the task of image captioning ends up producing\n synthetic captions that are worse for training CLIP (see ImageNet accuracy), possibly due to reduced text\n diversity. On the contrary, retrieval performance is higher when using captions generated by fine-tuned\n models.\n Notably, captioning models that are not fine-tuned have very poor CIDEr scores; going with this\n metric would have suggested that these models are not suitable for caption generation at all.\nWhile many image captioning metrics like CIDEr, SPICE and BLEU-4 emphasize similarity between\n generated captions and reference captions provided by humans, prior work has also proposed\n reference-free metrics\u2014for example, CLIP-S [21], which uses a trained CLIP model to assess the\n compatibility between an image and the generated caption. We compute CLIP-S for the medium\n candidate pool with different synthetic captions and find that this metric is more reflective of the\n ImageNet performance trend. Fine-tuned captioning models produce captions that have lower\n CLIP-S and image-text cosine similarity in general.\n Since BLIP2 (no fine-tuning) produces sufficiently good text supervision for CLIP to do well on\n both ImageNet classification and Flickr retrieval, we use it as the captioning model of choice in\n subsequent experiments that look at how to combine raw and synthetic captions.\n 5    Filtering raw and synthetic captions\n Here we explore in more detail different ways of filtering and combining raw and generated captions\n at the medium scale of DataComp [18]:\n\u2022 No filtering: we train on the entire, unmodified pool (i.e., 128M samples).\n\u2022 CLIP score filtering: we select the top x% of examples with highest image-text cosine similarity.\n\u2022 CLIP score intersect with ImageNet1k clustering: Gadre et al. [18] propose clustering image\n   embeddings and only selecting images whose cluster center is a nearest neighbor to an image from\n                                                       6", "md": "|Captioning model|NoCaps CIDEr [51]|CLIP-S [21]|Cosine similarity|No. of unique trigrams|ImageNet accuracy|Flickr retrieval|\n|---|---|---|---|---|---|---|\n|BLIP, ViT-L/16 (finetuned)|113.2*|0.698|0.231|2.82 \u00d7 10^6|0.207|0.498|\n|BLIP2, ViT-g|80.6|0.737|0.251|2.72 \u00d7 10^6|0.281|0.507|\n|BLIP2, ViT-g (finetuned)|119.7*|0.711|0.235|1.97 \u00d7 10^6|0.227|0.549|\n|OpenCLIP-CoCa, ViT-L/14|0.354*|0.752|0.260|4.45 \u00d7 10^6|0.321|0.395|\n|OpenCLIP-CoCa, ViT-L/14 (finetuned)|106.5*|0.702|0.232|1.81 \u00d7 10^6|0.252|0.542|\n\nTable 1: CIDEr score does not reliably predict how effective a captioning model is at generating synthetic captions for multimodal pre-training; fine-tuning image captioning models leads to lower ImageNet accuracy when training CLIP on the generated captions. * indicates numbers obtained from previous work and from contacting the authors. We fix the architecture and compare captions generated from captioning models with and without fine-tuning on MS-COCO [11] as sources of text supervision for CLIP. Fine-tuning pre-trained networks on the task of image captioning ends up producing synthetic captions that are worse for training CLIP (see ImageNet accuracy), possibly due to reduced text diversity. On the contrary, retrieval performance is higher when using captions generated by fine-tuned models.\n\nNotably, captioning models that are not fine-tuned have very poor CIDEr scores; going with this metric would have suggested that these models are not suitable for caption generation at all.\n\nWhile many image captioning metrics like CIDEr, SPICE and BLEU-4 emphasize similarity between generated captions and reference captions provided by humans, prior work has also proposed reference-free metrics\u2014for example, CLIP-S [21], which uses a trained CLIP model to assess the compatibility between an image and the generated caption. We compute CLIP-S for the medium candidate pool with different synthetic captions and find that this metric is more reflective of the ImageNet performance trend. Fine-tuned captioning models produce captions that have lower CLIP-S and image-text cosine similarity in general.\n\nSince BLIP2 (no fine-tuning) produces sufficiently good text supervision for CLIP to do well on both ImageNet classification and Flickr retrieval, we use it as the captioning model of choice in subsequent experiments that look at how to combine raw and synthetic captions.\n\n5 Filtering raw and synthetic captions\n\nHere we explore in more detail different ways of filtering and combining raw and generated captions at the medium scale of DataComp [18]:\n\n- No filtering: we train on the entire, unmodified pool (i.e., 128M samples).\n- CLIP score filtering: we select the top x% of examples with highest image-text cosine similarity.\n- CLIP score intersect with ImageNet1k clustering: Gadre et al. [18] propose clustering image embeddings and only selecting images whose cluster center is a nearest neighbor to an image from", "images": [], "items": [{"type": "table", "rows": [["Captioning model", "NoCaps CIDEr [51]", "CLIP-S [21]", "Cosine similarity", "No. of unique trigrams", "ImageNet accuracy", "Flickr retrieval"], ["BLIP, ViT-L/16 (finetuned)", "113.2*", "0.698", "0.231", "2.82 \u00d7 10^6", "0.207", "0.498"], ["BLIP2, ViT-g", "80.6", "0.737", "0.251", "2.72 \u00d7 10^6", "0.281", "0.507"], ["BLIP2, ViT-g (finetuned)", "119.7*", "0.711", "0.235", "1.97 \u00d7 10^6", "0.227", "0.549"], ["OpenCLIP-CoCa, ViT-L/14", "0.354*", "0.752", "0.260", "4.45 \u00d7 10^6", "0.321", "0.395"], ["OpenCLIP-CoCa, ViT-L/14 (finetuned)", "106.5*", "0.702", "0.232", "1.81 \u00d7 10^6", "0.252", "0.542"]], "md": "|Captioning model|NoCaps CIDEr [51]|CLIP-S [21]|Cosine similarity|No. of unique trigrams|ImageNet accuracy|Flickr retrieval|\n|---|---|---|---|---|---|---|\n|BLIP, ViT-L/16 (finetuned)|113.2*|0.698|0.231|2.82 \u00d7 10^6|0.207|0.498|\n|BLIP2, ViT-g|80.6|0.737|0.251|2.72 \u00d7 10^6|0.281|0.507|\n|BLIP2, ViT-g (finetuned)|119.7*|0.711|0.235|1.97 \u00d7 10^6|0.227|0.549|\n|OpenCLIP-CoCa, ViT-L/14|0.354*|0.752|0.260|4.45 \u00d7 10^6|0.321|0.395|\n|OpenCLIP-CoCa, ViT-L/14 (finetuned)|106.5*|0.702|0.232|1.81 \u00d7 10^6|0.252|0.542|", "isPerfectTable": true, "csv": "\"Captioning model\",\"NoCaps CIDEr [51]\",\"CLIP-S [21]\",\"Cosine similarity\",\"No. of unique trigrams\",\"ImageNet accuracy\",\"Flickr retrieval\"\n\"BLIP, ViT-L/16 (finetuned)\",\"113.2*\",\"0.698\",\"0.231\",\"2.82 \u00d7 10^6\",\"0.207\",\"0.498\"\n\"BLIP2, ViT-g\",\"80.6\",\"0.737\",\"0.251\",\"2.72 \u00d7 10^6\",\"0.281\",\"0.507\"\n\"BLIP2, ViT-g (finetuned)\",\"119.7*\",\"0.711\",\"0.235\",\"1.97 \u00d7 10^6\",\"0.227\",\"0.549\"\n\"OpenCLIP-CoCa, ViT-L/14\",\"0.354*\",\"0.752\",\"0.260\",\"4.45 \u00d7 10^6\",\"0.321\",\"0.395\"\n\"OpenCLIP-CoCa, ViT-L/14 (finetuned)\",\"106.5*\",\"0.702\",\"0.232\",\"1.81 \u00d7 10^6\",\"0.252\",\"0.542\""}, {"type": "text", "value": "Table 1: CIDEr score does not reliably predict how effective a captioning model is at generating synthetic captions for multimodal pre-training; fine-tuning image captioning models leads to lower ImageNet accuracy when training CLIP on the generated captions. * indicates numbers obtained from previous work and from contacting the authors. We fix the architecture and compare captions generated from captioning models with and without fine-tuning on MS-COCO [11] as sources of text supervision for CLIP. Fine-tuning pre-trained networks on the task of image captioning ends up producing synthetic captions that are worse for training CLIP (see ImageNet accuracy), possibly due to reduced text diversity. On the contrary, retrieval performance is higher when using captions generated by fine-tuned models.\n\nNotably, captioning models that are not fine-tuned have very poor CIDEr scores; going with this metric would have suggested that these models are not suitable for caption generation at all.\n\nWhile many image captioning metrics like CIDEr, SPICE and BLEU-4 emphasize similarity between generated captions and reference captions provided by humans, prior work has also proposed reference-free metrics\u2014for example, CLIP-S [21], which uses a trained CLIP model to assess the compatibility between an image and the generated caption. We compute CLIP-S for the medium candidate pool with different synthetic captions and find that this metric is more reflective of the ImageNet performance trend. Fine-tuned captioning models produce captions that have lower CLIP-S and image-text cosine similarity in general.\n\nSince BLIP2 (no fine-tuning) produces sufficiently good text supervision for CLIP to do well on both ImageNet classification and Flickr retrieval, we use it as the captioning model of choice in subsequent experiments that look at how to combine raw and synthetic captions.\n\n5 Filtering raw and synthetic captions\n\nHere we explore in more detail different ways of filtering and combining raw and generated captions at the medium scale of DataComp [18]:\n\n- No filtering: we train on the entire, unmodified pool (i.e., 128M samples).\n- CLIP score filtering: we select the top x% of examples with highest image-text cosine similarity.\n- CLIP score intersect with ImageNet1k clustering: Gadre et al. [18] propose clustering image embeddings and only selecting images whose cluster center is a nearest neighbor to an image from", "md": "Table 1: CIDEr score does not reliably predict how effective a captioning model is at generating synthetic captions for multimodal pre-training; fine-tuning image captioning models leads to lower ImageNet accuracy when training CLIP on the generated captions. * indicates numbers obtained from previous work and from contacting the authors. We fix the architecture and compare captions generated from captioning models with and without fine-tuning on MS-COCO [11] as sources of text supervision for CLIP. Fine-tuning pre-trained networks on the task of image captioning ends up producing synthetic captions that are worse for training CLIP (see ImageNet accuracy), possibly due to reduced text diversity. On the contrary, retrieval performance is higher when using captions generated by fine-tuned models.\n\nNotably, captioning models that are not fine-tuned have very poor CIDEr scores; going with this metric would have suggested that these models are not suitable for caption generation at all.\n\nWhile many image captioning metrics like CIDEr, SPICE and BLEU-4 emphasize similarity between generated captions and reference captions provided by humans, prior work has also proposed reference-free metrics\u2014for example, CLIP-S [21], which uses a trained CLIP model to assess the compatibility between an image and the generated caption. We compute CLIP-S for the medium candidate pool with different synthetic captions and find that this metric is more reflective of the ImageNet performance trend. Fine-tuned captioning models produce captions that have lower CLIP-S and image-text cosine similarity in general.\n\nSince BLIP2 (no fine-tuning) produces sufficiently good text supervision for CLIP to do well on both ImageNet classification and Flickr retrieval, we use it as the captioning model of choice in subsequent experiments that look at how to combine raw and synthetic captions.\n\n5 Filtering raw and synthetic captions\n\nHere we explore in more detail different ways of filtering and combining raw and generated captions at the medium scale of DataComp [18]:\n\n- No filtering: we train on the entire, unmodified pool (i.e., 128M samples).\n- CLIP score filtering: we select the top x% of examples with highest image-text cosine similarity.\n- CLIP score intersect with ImageNet1k clustering: Gadre et al. [18] propose clustering image embeddings and only selecting images whose cluster center is a nearest neighbor to an image from"}]}, {"page": 7, "text": "        38.0                                                 BLIP2 (top 50%)                                      32.5\n        37.5                                               +Raw (50%, filtered)\n        37.0                                      BLIP2 (top 50%)             Raw (top 30%)                       30.0\n        36.5                                                                +BLIP2 (70%, filtered)                27.5\n       Average accuracy                               Raw (top 30%)                                             ImageNet Accuracy\n        36.0                                        +BLIP2 (70%)          BLIP2 (top 75%                          25.0\n        35.5                 BLIP2                                        intersect IN1k)\n        35.0                 (no filtering)                                                                       22.5\n        34.5                                                                                                      20.0\n        34.0                                                                                                                        Raw\n        33.5                                                                                                      17.5              BLIP2\n                                                                                                                                    Top raw+remaining BLIP2\n        33.0        Raw (top 30%)                              Raw (top 30%                                       15.0              Top raw+remaining BLIP2 (filtered)\n                                                               intersect IN1k)\n        32.5\n           27.0     27.5     28.0     28.5    29.0     29.5     30.0     30.5     31.0     31.5     32.0                        20        40          60         80     100\n                                           ImageNet accuracy                                                                         CLIP score filtering %\n Figure 2: At the 128M scale of DataComp, we obtain improvement on ImageNet and average\n accuracies compared to the best filtering method on raw data, by using a mixture of raw\n and synthetic captions, selecting only image-text pairs with cosine similarity above a certain\n threshold. (Left) We visualize how various data filtering strategies perform at medium scale, on ImageNet\n and across 38 tasks. Including BLIP2 captions in the training data significantly outperforms competitive\n baselines from DataComp trained on only raw text [18]. (Right) As we vary the percentage of top examples\n chosen from the pool (based on CLIP score), we see consistent benefits from (i) using BLIP2 captions for\n samples that would be discarded otherwise, (ii) applying the same filtering threshold to new image-text pairs\n containing BLIP2 captions to keep noise level low. The exact accuracy numbers can be found in Appendix D.\n     ImageNet1k. The authors then find the intersection between this set of examples and those that\n     are in the top x% based on CLIP score. This is the best baseline using raw captions on DataComp.\n\u2022 Combining raw and synthetic captions: we use raw captions for the top x% of examples based\n     on CLIP score. For the remaining images (that would otherwise be filtered out), we generate\n     corresponding BLIP2 captions and add them back to the training pool. We also experiment with\n     filtering these additional image-text pairs with the same cosine similarity threshold set in the first\n     step (i.e., BLIP2 (x%, filtered) in Figure 2).\n In Appendix D, we investigate other baselines and report how well each approach does with varying\n cosine similarity thresholds. Figure 2 (left) shows the relative performance of select baselines (the\n degree of CLIP score filtering has been tuned and only the best accuracy is plotted). We find that\n the best performance at medium scale, measured by either ImageNet or average accuracy, is achieved\n by mixing raw and synthetic captions, subject to a cosine similarity threshold. Besides, including\n BLIP2 captions in the training pool also improves retrieval performance by more than 2\u00d7 (Table 2).\n In the right plot of Figure 2, we compare ImageNet performance at various filtering thresholds\n for methods that involve only one source of captions and those that involve both. We observe\n that given image-raw-text pairs filtered with certain cosine similarity threshold (blue line), adding\n BLIP2 captions for some (red line) or all of the remaining images (green line) always helps. It is\nworth noting that as we lower the threshold and include more raw captions in the training mix, the\n performance starts to become lower than using just synthetic captions (orange line). Overall we find\n that filtering is still a necessary step even when using synthetic captions that are supposedly more\n relevant to the training images.\n                                                                                                 7", "md": "# DataComp Results\n\n## DataComp Results\n\nFigure 2: At the 128M scale of DataComp, we obtain improvement on ImageNet and average accuracies compared to the best filtering method on raw data, by using a mixture of raw and synthetic captions, selecting only image-text pairs with cosine similarity above a certain threshold. (Left) We visualize how various data filtering strategies perform at medium scale, on ImageNet and across 38 tasks. Including BLIP2 captions in the training data significantly outperforms competitive baselines from DataComp trained on only raw text [18]. (Right) As we vary the percentage of top examples chosen from the pool (based on CLIP score), we see consistent benefits from (i) using BLIP2 captions for samples that would be discarded otherwise, (ii) applying the same filtering threshold to new image-text pairs containing BLIP2 captions to keep noise level low. The exact accuracy numbers can be found in Appendix D.\n\nImageNet1k. The authors then find the intersection between this set of examples and those that are in the top x% based on CLIP score. This is the best baseline using raw captions on DataComp.\n\n- Combining raw and synthetic captions: we use raw captions for the top x% of examples based on CLIP score. For the remaining images (that would otherwise be filtered out), we generate corresponding BLIP2 captions and add them back to the training pool. We also experiment with filtering these additional image-text pairs with the same cosine similarity threshold set in the first step (i.e., BLIP2 (x%, filtered) in Figure 2).\n\nIn Appendix D, we investigate other baselines and report how well each approach does with varying cosine similarity thresholds. Figure 2 (left) shows the relative performance of select baselines (the degree of CLIP score filtering has been tuned and only the best accuracy is plotted). We find that the best performance at medium scale, measured by either ImageNet or average accuracy, is achieved by mixing raw and synthetic captions, subject to a cosine similarity threshold. Besides, including BLIP2 captions in the training pool also improves retrieval performance by more than 2\u00d7 (Table 2).\n\nIn the right plot of Figure 2, we compare ImageNet performance at various filtering thresholds for methods that involve only one source of captions and those that involve both. We observe that given image-raw-text pairs filtered with certain cosine similarity threshold (blue line), adding BLIP2 captions for some (red line) or all of the remaining images (green line) always helps. It is worth noting that as we lower the threshold and include more raw captions in the training mix, the performance starts to become lower than using just synthetic captions (orange line). Overall we find that filtering is still a necessary step even when using synthetic captions that are supposedly more relevant to the training images.\n\n### Appendix D\n\nTable 2: Performance with varying cosine similarity thresholds\n\n|Cosine Similarity Threshold|Accuracy|\n|---|---|\n|0.1|85%|\n|0.2|90%|\n|0.3|92%|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "DataComp Results", "md": "# DataComp Results"}, {"type": "heading", "lvl": 2, "value": "DataComp Results", "md": "## DataComp Results"}, {"type": "text", "value": "Figure 2: At the 128M scale of DataComp, we obtain improvement on ImageNet and average accuracies compared to the best filtering method on raw data, by using a mixture of raw and synthetic captions, selecting only image-text pairs with cosine similarity above a certain threshold. (Left) We visualize how various data filtering strategies perform at medium scale, on ImageNet and across 38 tasks. Including BLIP2 captions in the training data significantly outperforms competitive baselines from DataComp trained on only raw text [18]. (Right) As we vary the percentage of top examples chosen from the pool (based on CLIP score), we see consistent benefits from (i) using BLIP2 captions for samples that would be discarded otherwise, (ii) applying the same filtering threshold to new image-text pairs containing BLIP2 captions to keep noise level low. The exact accuracy numbers can be found in Appendix D.\n\nImageNet1k. The authors then find the intersection between this set of examples and those that are in the top x% based on CLIP score. This is the best baseline using raw captions on DataComp.\n\n- Combining raw and synthetic captions: we use raw captions for the top x% of examples based on CLIP score. For the remaining images (that would otherwise be filtered out), we generate corresponding BLIP2 captions and add them back to the training pool. We also experiment with filtering these additional image-text pairs with the same cosine similarity threshold set in the first step (i.e., BLIP2 (x%, filtered) in Figure 2).\n\nIn Appendix D, we investigate other baselines and report how well each approach does with varying cosine similarity thresholds. Figure 2 (left) shows the relative performance of select baselines (the degree of CLIP score filtering has been tuned and only the best accuracy is plotted). We find that the best performance at medium scale, measured by either ImageNet or average accuracy, is achieved by mixing raw and synthetic captions, subject to a cosine similarity threshold. Besides, including BLIP2 captions in the training pool also improves retrieval performance by more than 2\u00d7 (Table 2).\n\nIn the right plot of Figure 2, we compare ImageNet performance at various filtering thresholds for methods that involve only one source of captions and those that involve both. We observe that given image-raw-text pairs filtered with certain cosine similarity threshold (blue line), adding BLIP2 captions for some (red line) or all of the remaining images (green line) always helps. It is worth noting that as we lower the threshold and include more raw captions in the training mix, the performance starts to become lower than using just synthetic captions (orange line). Overall we find that filtering is still a necessary step even when using synthetic captions that are supposedly more relevant to the training images.", "md": "Figure 2: At the 128M scale of DataComp, we obtain improvement on ImageNet and average accuracies compared to the best filtering method on raw data, by using a mixture of raw and synthetic captions, selecting only image-text pairs with cosine similarity above a certain threshold. (Left) We visualize how various data filtering strategies perform at medium scale, on ImageNet and across 38 tasks. Including BLIP2 captions in the training data significantly outperforms competitive baselines from DataComp trained on only raw text [18]. (Right) As we vary the percentage of top examples chosen from the pool (based on CLIP score), we see consistent benefits from (i) using BLIP2 captions for samples that would be discarded otherwise, (ii) applying the same filtering threshold to new image-text pairs containing BLIP2 captions to keep noise level low. The exact accuracy numbers can be found in Appendix D.\n\nImageNet1k. The authors then find the intersection between this set of examples and those that are in the top x% based on CLIP score. This is the best baseline using raw captions on DataComp.\n\n- Combining raw and synthetic captions: we use raw captions for the top x% of examples based on CLIP score. For the remaining images (that would otherwise be filtered out), we generate corresponding BLIP2 captions and add them back to the training pool. We also experiment with filtering these additional image-text pairs with the same cosine similarity threshold set in the first step (i.e., BLIP2 (x%, filtered) in Figure 2).\n\nIn Appendix D, we investigate other baselines and report how well each approach does with varying cosine similarity thresholds. Figure 2 (left) shows the relative performance of select baselines (the degree of CLIP score filtering has been tuned and only the best accuracy is plotted). We find that the best performance at medium scale, measured by either ImageNet or average accuracy, is achieved by mixing raw and synthetic captions, subject to a cosine similarity threshold. Besides, including BLIP2 captions in the training pool also improves retrieval performance by more than 2\u00d7 (Table 2).\n\nIn the right plot of Figure 2, we compare ImageNet performance at various filtering thresholds for methods that involve only one source of captions and those that involve both. We observe that given image-raw-text pairs filtered with certain cosine similarity threshold (blue line), adding BLIP2 captions for some (red line) or all of the remaining images (green line) always helps. It is worth noting that as we lower the threshold and include more raw captions in the training mix, the performance starts to become lower than using just synthetic captions (orange line). Overall we find that filtering is still a necessary step even when using synthetic captions that are supposedly more relevant to the training images."}, {"type": "heading", "lvl": 3, "value": "Appendix D", "md": "### Appendix D"}, {"type": "text", "value": "Table 2: Performance with varying cosine similarity thresholds", "md": "Table 2: Performance with varying cosine similarity thresholds"}, {"type": "table", "rows": [["Cosine Similarity Threshold", "Accuracy"], ["0.1", "85%"], ["0.2", "90%"], ["0.3", "92%"]], "md": "|Cosine Similarity Threshold|Accuracy|\n|---|---|\n|0.1|85%|\n|0.2|90%|\n|0.3|92%|", "isPerfectTable": true, "csv": "\"Cosine Similarity Threshold\",\"Accuracy\"\n\"0.1\",\"85%\"\n\"0.2\",\"90%\"\n\"0.3\",\"92%\""}]}, {"page": 8, "text": "                   0.200                                                                           0.30\n                   0.175                                                                           0.25\n                   0.150\n                                                                                                   0.20\n                 Frequency                                                                        Frequency                                                                      BLIP2\n                   0.125\n                   0.100                                                                           0.15                                                                          BLIP2 (top 30%)\n                                                                                                                                                                                 Raw\n                   0.075                                                                           0.10                                                                          Raw (top 30%)\n                   0.050                                                                                                                                                         Raw (top 30%) +\n                   0.025                                                                           0.05                                                                          BLIP2 (70%, filtered)\n                   0.000 100                            101                             102        0.00   10  1                                             100\n                                         Number of words                                                          Grounding ratio\nFigure 3: Individual synthetic captions can contain more information (especially visual one)\nthan raw captions. We calculate the number of words and the fraction of those being visual tokens in\neach caption for different training sets. Individual BLIP2 captions tend to yield higher numbers on these\ntwo metrics compared to individual web-crawled captions, suggesting that on a caption-per-caption basis,\nsynthetic data may contain richer information.\n6            What makes synthetic captions effective?\n6.1             Defining caption quality\nAs seen from sample images in Figure 1, web-scraped                                                                                    Raw (no filtering)                                               13.2\ntext may not contain specific visual information (e.g.,                                                                                Raw (top 30% intersect IN1k)                                     18.2\n\u201cItalien - Ligurien\u201d) or may not reflect the content of the\nimage (e.g., \u201cImage Not Found\u201d). We seek to understand                                                                                 Raw (top 30%)                                                    19.7\nhow generated captions can help overcome these issues.                                                                                 Raw (top 30%) + BLIP2 (70%,                                      38.0\nTo approximate the richness of information conveyed                                                                                    filtered)\nin the text data, we take a 1M random subset from                                                                                      BLIP2 (top 75% intersect IN1k)                                   38.9\neach training set and measure the number of words,                                                                                     BLIP2 (top 50%)                                                  40.1\nas well as the grounding ratio [49] (i.e., the fraction of                                                                             Raw (top 30%) + BLIP2 (70%)                                      40.5\ntokens that describe visual concepts, with the vocabulary                                                                              BLIP2 (no filtering)                                             41.7\ndefined by MS-COCO), in the corresponding captions.                                                                                Table 2: Training on generated captions\nIn Figure 3, we observe that synthetic captions and raw                                                                             substantially boosts retrieval capabili-\ncaptions follow different distributions, with the former                                                                            ties of the resulting CLIP models. Here\ngenerally containing more words (left pane) and more                                                                               we report the average text-to-image and image-\nvisual tokens (right pane) per sample. Performing CLIP                                                                              to-text retrieval performance across both MS-\nscore filtering on raw captions leads to improvements                                                                               COCO and Flickr for different data filtering\non both of these properties; so does mixing raw and                                                                                 baselines.                More specific breakdown can be\n                                                                                                                                    found in Appendix Figure 9. Overall, we ob-\nsynthetic captions. Regarding the issue of poor image-                                                                              serve a 2\u00d7 improvement at the medium scale\ntext alignment, we approximate the alignment using                                                                                  of DataComp when synthetic captions are in-\ncosine similarity between image and text embeddings                                                                                 cluded in the training set.\nfrom CLIP, and find that web-crawled captions indeed\nhave lower similarities overall compared to model-generated ones (Figure 4).\nThe analyses above measure properties of individual captions. We next aim to capture a single\ndiversity metric over all text in the training set. We again select a random subset, the size of which\n                                                                                                                 8", "md": "# Document\n\n$$\\begin{array}{cc}\n\\text{0.200} & \\text{0.30} \\\\\n\\text{0.175} & \\text{0.25} \\\\\n\\text{0.150} & \\\\\n& \\text{0.20} \\\\\n\\text{Frequency} & \\text{Frequency} & \\text{BLIP2} \\\\\n\\text{0.125} & & \\\\\n\\text{0.100} & \\text{0.15} & \\text{BLIP2 (top 30%)} \\\\\n\\text{0.075} & \\text{0.10} & \\text{Raw (top 30%)} \\\\\n\\text{0.050} & & \\text{Raw (top 30%) +} \\\\\n\\text{0.025} & \\text{0.05} & \\text{BLIP2 (70%, filtered)} \\\\\n\\text{0.000} & 100 & 101 & 102 & 0.00 & 10 & 1 & 100 \\\\\n& \\text{Number of words} & & \\text{Grounding ratio}\n\\end{array}$$\n\nFigure 3: Individual synthetic captions can contain more information (especially visual one) than raw captions. We calculate the number of words and the fraction of those being visual tokens in each caption for different training sets. Individual BLIP2 captions tend to yield higher numbers on these two metrics compared to individual web-crawled captions, suggesting that on a caption-per-caption basis, synthetic data may contain richer information.\n\n6 What makes synthetic captions effective?\n\n6.1 Defining caption quality\n\nAs seen from sample images in Figure 1, web-scraped text may not contain specific visual information (e.g., \"Italien - Ligurien\") or may not reflect the content of the image (e.g., \"Image Not Found\"). We seek to understand how generated captions can help overcome these issues. To approximate the richness of information conveyed in the text data, we take a 1M random subset from each training set and measure the number of words, as well as the grounding ratio [49] (i.e., the fraction of tokens that describe visual concepts, with the vocabulary defined by MS-COCO), in the corresponding captions. In Figure 3, we observe that synthetic captions and raw captions follow different distributions, with the former generally containing more words (left pane) and more visual tokens (right pane) per sample. Performing CLIP score filtering on raw captions leads to improvements on both of these properties; so does mixing raw and synthetic captions. Regarding the issue of poor image-text alignment, we approximate the alignment using cosine similarity between image and text embeddings from CLIP, and find that web-crawled captions indeed have lower similarities overall compared to model-generated ones (Figure 4).\n\nThe analyses above measure properties of individual captions. We next aim to capture a single diversity metric over all text in the training set. We again select a random subset, the size of which\n\n|Raw (no filtering)|13.2|\n|---|---|\n|Raw (top 30% intersect IN1k)|18.2|\n|Raw (top 30%)|19.7|\n|Raw (top 30%) + BLIP2 (70%, filtered)|38.0|\n|BLIP2 (top 75% intersect IN1k)|38.9|\n|BLIP2 (top 50%)|40.1|\n|Raw (top 30%) + BLIP2 (70%)|40.5|\n|BLIP2 (no filtering)|41.7|\n\nTable 2: Training on generated captions substantially boosts retrieval capabilities of the resulting CLIP models. Here we report the average text-to-image and image-to-text retrieval performance across both MS-COCO and Flickr for different data filtering baselines. More specific breakdown can be found in Appendix Figure 9. Overall, we observe a 2\u00d7 improvement at the medium scale of DataComp when synthetic captions are included in the training set.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "$$\\begin{array}{cc}\n\\text{0.200} & \\text{0.30} \\\\\n\\text{0.175} & \\text{0.25} \\\\\n\\text{0.150} & \\\\\n& \\text{0.20} \\\\\n\\text{Frequency} & \\text{Frequency} & \\text{BLIP2} \\\\\n\\text{0.125} & & \\\\\n\\text{0.100} & \\text{0.15} & \\text{BLIP2 (top 30%)} \\\\\n\\text{0.075} & \\text{0.10} & \\text{Raw (top 30%)} \\\\\n\\text{0.050} & & \\text{Raw (top 30%) +} \\\\\n\\text{0.025} & \\text{0.05} & \\text{BLIP2 (70%, filtered)} \\\\\n\\text{0.000} & 100 & 101 & 102 & 0.00 & 10 & 1 & 100 \\\\\n& \\text{Number of words} & & \\text{Grounding ratio}\n\\end{array}$$\n\nFigure 3: Individual synthetic captions can contain more information (especially visual one) than raw captions. We calculate the number of words and the fraction of those being visual tokens in each caption for different training sets. Individual BLIP2 captions tend to yield higher numbers on these two metrics compared to individual web-crawled captions, suggesting that on a caption-per-caption basis, synthetic data may contain richer information.\n\n6 What makes synthetic captions effective?\n\n6.1 Defining caption quality\n\nAs seen from sample images in Figure 1, web-scraped text may not contain specific visual information (e.g., \"Italien - Ligurien\") or may not reflect the content of the image (e.g., \"Image Not Found\"). We seek to understand how generated captions can help overcome these issues. To approximate the richness of information conveyed in the text data, we take a 1M random subset from each training set and measure the number of words, as well as the grounding ratio [49] (i.e., the fraction of tokens that describe visual concepts, with the vocabulary defined by MS-COCO), in the corresponding captions. In Figure 3, we observe that synthetic captions and raw captions follow different distributions, with the former generally containing more words (left pane) and more visual tokens (right pane) per sample. Performing CLIP score filtering on raw captions leads to improvements on both of these properties; so does mixing raw and synthetic captions. Regarding the issue of poor image-text alignment, we approximate the alignment using cosine similarity between image and text embeddings from CLIP, and find that web-crawled captions indeed have lower similarities overall compared to model-generated ones (Figure 4).\n\nThe analyses above measure properties of individual captions. We next aim to capture a single diversity metric over all text in the training set. We again select a random subset, the size of which", "md": "$$\\begin{array}{cc}\n\\text{0.200} & \\text{0.30} \\\\\n\\text{0.175} & \\text{0.25} \\\\\n\\text{0.150} & \\\\\n& \\text{0.20} \\\\\n\\text{Frequency} & \\text{Frequency} & \\text{BLIP2} \\\\\n\\text{0.125} & & \\\\\n\\text{0.100} & \\text{0.15} & \\text{BLIP2 (top 30%)} \\\\\n\\text{0.075} & \\text{0.10} & \\text{Raw (top 30%)} \\\\\n\\text{0.050} & & \\text{Raw (top 30%) +} \\\\\n\\text{0.025} & \\text{0.05} & \\text{BLIP2 (70%, filtered)} \\\\\n\\text{0.000} & 100 & 101 & 102 & 0.00 & 10 & 1 & 100 \\\\\n& \\text{Number of words} & & \\text{Grounding ratio}\n\\end{array}$$\n\nFigure 3: Individual synthetic captions can contain more information (especially visual one) than raw captions. We calculate the number of words and the fraction of those being visual tokens in each caption for different training sets. Individual BLIP2 captions tend to yield higher numbers on these two metrics compared to individual web-crawled captions, suggesting that on a caption-per-caption basis, synthetic data may contain richer information.\n\n6 What makes synthetic captions effective?\n\n6.1 Defining caption quality\n\nAs seen from sample images in Figure 1, web-scraped text may not contain specific visual information (e.g., \"Italien - Ligurien\") or may not reflect the content of the image (e.g., \"Image Not Found\"). We seek to understand how generated captions can help overcome these issues. To approximate the richness of information conveyed in the text data, we take a 1M random subset from each training set and measure the number of words, as well as the grounding ratio [49] (i.e., the fraction of tokens that describe visual concepts, with the vocabulary defined by MS-COCO), in the corresponding captions. In Figure 3, we observe that synthetic captions and raw captions follow different distributions, with the former generally containing more words (left pane) and more visual tokens (right pane) per sample. Performing CLIP score filtering on raw captions leads to improvements on both of these properties; so does mixing raw and synthetic captions. Regarding the issue of poor image-text alignment, we approximate the alignment using cosine similarity between image and text embeddings from CLIP, and find that web-crawled captions indeed have lower similarities overall compared to model-generated ones (Figure 4).\n\nThe analyses above measure properties of individual captions. We next aim to capture a single diversity metric over all text in the training set. We again select a random subset, the size of which"}, {"type": "table", "rows": [["Raw (no filtering)", "13.2"], ["Raw (top 30% intersect IN1k)", "18.2"], ["Raw (top 30%)", "19.7"], ["Raw (top 30%) + BLIP2 (70%, filtered)", "38.0"], ["BLIP2 (top 75% intersect IN1k)", "38.9"], ["BLIP2 (top 50%)", "40.1"], ["Raw (top 30%) + BLIP2 (70%)", "40.5"], ["BLIP2 (no filtering)", "41.7"]], "md": "|Raw (no filtering)|13.2|\n|---|---|\n|Raw (top 30% intersect IN1k)|18.2|\n|Raw (top 30%)|19.7|\n|Raw (top 30%) + BLIP2 (70%, filtered)|38.0|\n|BLIP2 (top 75% intersect IN1k)|38.9|\n|BLIP2 (top 50%)|40.1|\n|Raw (top 30%) + BLIP2 (70%)|40.5|\n|BLIP2 (no filtering)|41.7|", "isPerfectTable": true, "csv": "\"Raw (no filtering)\",\"13.2\"\n\"Raw (top 30% intersect IN1k)\",\"18.2\"\n\"Raw (top 30%)\",\"19.7\"\n\"Raw (top 30%) + BLIP2 (70%, filtered)\",\"38.0\"\n\"BLIP2 (top 75% intersect IN1k)\",\"38.9\"\n\"BLIP2 (top 50%)\",\"40.1\"\n\"Raw (top 30%) + BLIP2 (70%)\",\"40.5\"\n\"BLIP2 (no filtering)\",\"41.7\""}, {"type": "text", "value": "Table 2: Training on generated captions substantially boosts retrieval capabilities of the resulting CLIP models. Here we report the average text-to-image and image-to-text retrieval performance across both MS-COCO and Flickr for different data filtering baselines. More specific breakdown can be found in Appendix Figure 9. Overall, we observe a 2\u00d7 improvement at the medium scale of DataComp when synthetic captions are included in the training set.", "md": "Table 2: Training on generated captions substantially boosts retrieval capabilities of the resulting CLIP models. Here we report the average text-to-image and image-to-text retrieval performance across both MS-COCO and Flickr for different data filtering baselines. More specific breakdown can be found in Appendix Figure 9. Overall, we observe a 2\u00d7 improvement at the medium scale of DataComp when synthetic captions are included in the training set."}]}, {"page": 9, "text": "                                                                         BLIP2           Figure 4:            Generated captions overall exhibit\n       0.08                                                              Raw             higher image-text alignment than raw captions;\n       0.06                                                                              this indicates that the former is less noisy as\n      Density                                                                            a training source. We randomly sample 1% of the\n       0.04                                                                             128M candidate pool and given the same set of images,\n                                                                                         compare the cosine similarity distribution between raw\n       0.02                                                                              caption data and BLIP2 caption data. We find that\n                                                                                         overall BLIP2 captions have much higher image-text\n       0.00      0.1      0.0       0.1       0.2       0.3       0.4      0.5           cosine similarity (mean similarity 0.251 vs 0.208).\n                      CLIP ViT-L/14 image-text cosine similarity\n scales with the training set size, and calculate the number of unique trigrams across all captions in\n the subset. With this diversity metric, we find that BLIP2 captions actually lag behind raw captions\n(Figure 5). Using only the top 30% raw captions (based on CLIP score) is even more detrimental.\nWe summarize these different aspects of caption quality in a noise versus diversity framework (Figure\n 5), which also offers some intuition for our best baseline uncovered in Section 5. CLIP score filtering\n that has been commonly adopted in prior work [18, 46] is effective at improving performance on\n raw data by by removing noisy examples (i.e., those with poor image-text alignment). However,\n this procedure also lowers diversity (note: Figure 5 only provides a measure of text diversity, but\n image diversity is affected as well). By generating synthetic captions for the images that would be\n discarded otherwise, and subsequently only using pairs where the image-text similarities still meet\n the threshold, we manage to keep the overall noise level similarly low, while adding more diversity\n to the training pool. Progress along both axes enables further performance improvement compared\n to just filtering raw data.\n       0.29                                                                                          Figure 5: Combining raw and synthetic\n       0.28                           more diverse                      >                            captions subject to a cosine similarity\n      Average image-text cosine similarity\n                raw (top 30%)                 raw (top 30%)+BLIP2 (70%,filtered)            30       threshold helps reduce noise level while\n       0.27     acc=27.3%                     acc=31.7%                        ^            28       boosting data diversity, both of which are\n                                                                                                     essential for achieving good performance.\n       0.26                                                                                      Imagenet accuracy\n                                                                                            26       In this plot, circle size denotes the relative size of\n       0.25                                                                   less noise             the resulting training set. While removing noisy\n                                  BLIP2 (no filter)                                                  image-text pairs, CLIP score filtering also low-\n       0.24                       acc=28.1%                                                 24       ers the diversity of the caption set substantially,\n       0.23                                                                                 22       as measured by the number of unique trigrams\n       0.22                                                                                          in the pool. Adding more useful training data\n                                                               raw (no filter)              20       by using BLIP2 captions for filtered out images,\n       0.21                                                    acc=17.6%                             while respecting the existing CLIP score thresh-\n       0.20                                                                                 18       old, helps overcome this limitation and improves\n             2                              3                     4                5                 the training data quality along both axes.\n                   No. of unique trigrams in captions (log scale)1e6\n 6.2         Performance analysis\nAfter diving deeper into properties of synthetic captions, we next analyze the training implications of\n these captions in more detail. We examine two models, one trained using only raw captions and the\n other using only BLIP2 captions, with both training sets having been filtered with CLIP score for\n top 30% pairs, and achieving similar performance on ImageNet (27.3% vs 27.5%). Averaged across\n38 evaluation tasks, training on generated captions offers a 2.8% improvement. We break down\n                                                                                           9", "md": "```markdown\nBLIP2 Figure 4: Generated captions overall exhibit\n0.08 Raw higher image-text alignment than raw captions; this indicates that the former is less noisy as a training source. We randomly sample 1% of the 128M candidate pool and given the same set of images, compare the cosine similarity distribution between raw caption data and BLIP2 caption data. We find that overall BLIP2 captions have much higher image-text cosine similarity (mean similarity $$0.251$$ vs $$0.208$$).\n\nCLIP ViT-L/14 image-text cosine similarity scales with the training set size, and calculate the number of unique trigrams across all captions in the subset. With this diversity metric, we find that BLIP2 captions actually lag behind raw captions (Figure 5). Using only the top 30% raw captions (based on CLIP score) is even more detrimental. We summarize these different aspects of caption quality in a noise versus diversity framework (Figure 5), which also offers some intuition for our best baseline uncovered in Section 5. CLIP score filtering that has been commonly adopted in prior work [18, 46] is effective at improving performance on raw data by removing noisy examples (i.e., those with poor image-text alignment). However, this procedure also lowers diversity (note: Figure 5 only provides a measure of text diversity, but image diversity is affected as well). By generating synthetic captions for the images that would be discarded otherwise, and subsequently only using pairs where the image-text similarities still meet the threshold, we manage to keep the overall noise level similarly low, while adding more diversity to the training pool. Progress along both axes enables further performance improvement compared to just filtering raw data.\n\nFigure 5: Combining raw and synthetic captions subject to a cosine similarity threshold helps reduce noise level while boosting data diversity, both of which are essential for achieving good performance.\nAverage image-text cosine similarity\nraw (top 30%) acc=27.3%\nraw (top 30%)+BLIP2 (70%,filtered) acc=31.7%\nBLIP2 (no filter) acc=28.1%\nraw (no filter) acc=17.6%\nImagenet accuracy\nIn this plot, circle size denotes the relative size of the resulting training set. While removing noisy image-text pairs, CLIP score filtering also lowers the diversity of the caption set substantially, as measured by the number of unique trigrams in the pool. Adding more useful training data by using BLIP2 captions for filtered out images, while respecting the existing CLIP score threshold, helps overcome this limitation and improves the training data quality along both axes.\n\nNo. of unique trigrams in captions (log scale)1e6\n\nPerformance analysis\nAfter diving deeper into properties of synthetic captions, we next analyze the training implications of these captions in more detail. We examine two models, one trained using only raw captions and the other using only BLIP2 captions, with both training sets having been filtered with CLIP score for top 30% pairs, and achieving similar performance on ImageNet (27.3% vs 27.5%). Averaged across 38 evaluation tasks, training on generated captions offers a 2.8% improvement. We break down\n```\n```latex\n$$\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{Average image-text cosine similarity} & \\text{raw (top 30%)} & \\text{raw (top 30%)+BLIP2 (70%,filtered)} & 30 & \\text{threshold helps reduce noise level while} \\\\\n\\text{acc=27.3\\%} & \\text{acc=31.7\\%} & & 28 & \\text{boosting data diversity, both of which are} \\\\\n& & & & \\text{essential for achieving good performance.} \\\\\n\\hline\n\\text{BLIP2 (no filter)} & \\text{acc=28.1\\%} & & & \\\\\n\\hline\n\\text{raw (no filter)} & \\text{acc=17.6\\%} & & & \\\\\n\\hline\n\\end{array}$$\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nBLIP2 Figure 4: Generated captions overall exhibit\n0.08 Raw higher image-text alignment than raw captions; this indicates that the former is less noisy as a training source. We randomly sample 1% of the 128M candidate pool and given the same set of images, compare the cosine similarity distribution between raw caption data and BLIP2 caption data. We find that overall BLIP2 captions have much higher image-text cosine similarity (mean similarity $$0.251$$ vs $$0.208$$).\n\nCLIP ViT-L/14 image-text cosine similarity scales with the training set size, and calculate the number of unique trigrams across all captions in the subset. With this diversity metric, we find that BLIP2 captions actually lag behind raw captions (Figure 5). Using only the top 30% raw captions (based on CLIP score) is even more detrimental. We summarize these different aspects of caption quality in a noise versus diversity framework (Figure 5), which also offers some intuition for our best baseline uncovered in Section 5. CLIP score filtering that has been commonly adopted in prior work [18, 46] is effective at improving performance on raw data by removing noisy examples (i.e., those with poor image-text alignment). However, this procedure also lowers diversity (note: Figure 5 only provides a measure of text diversity, but image diversity is affected as well). By generating synthetic captions for the images that would be discarded otherwise, and subsequently only using pairs where the image-text similarities still meet the threshold, we manage to keep the overall noise level similarly low, while adding more diversity to the training pool. Progress along both axes enables further performance improvement compared to just filtering raw data.\n\nFigure 5: Combining raw and synthetic captions subject to a cosine similarity threshold helps reduce noise level while boosting data diversity, both of which are essential for achieving good performance.\nAverage image-text cosine similarity\nraw (top 30%) acc=27.3%\nraw (top 30%)+BLIP2 (70%,filtered) acc=31.7%\nBLIP2 (no filter) acc=28.1%\nraw (no filter) acc=17.6%\nImagenet accuracy\nIn this plot, circle size denotes the relative size of the resulting training set. While removing noisy image-text pairs, CLIP score filtering also lowers the diversity of the caption set substantially, as measured by the number of unique trigrams in the pool. Adding more useful training data by using BLIP2 captions for filtered out images, while respecting the existing CLIP score threshold, helps overcome this limitation and improves the training data quality along both axes.\n\nNo. of unique trigrams in captions (log scale)1e6\n\nPerformance analysis\nAfter diving deeper into properties of synthetic captions, we next analyze the training implications of these captions in more detail. We examine two models, one trained using only raw captions and the other using only BLIP2 captions, with both training sets having been filtered with CLIP score for top 30% pairs, and achieving similar performance on ImageNet (27.3% vs 27.5%). Averaged across 38 evaluation tasks, training on generated captions offers a 2.8% improvement. We break down\n```\n```latex\n$$\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{Average image-text cosine similarity} & \\text{raw (top 30%)} & \\text{raw (top 30%)+BLIP2 (70%,filtered)} & 30 & \\text{threshold helps reduce noise level while} \\\\\n\\text{acc=27.3\\%} & \\text{acc=31.7\\%} & & 28 & \\text{boosting data diversity, both of which are} \\\\\n& & & & \\text{essential for achieving good performance.} \\\\\n\\hline\n\\text{BLIP2 (no filter)} & \\text{acc=28.1\\%} & & & \\\\\n\\hline\n\\text{raw (no filter)} & \\text{acc=17.6\\%} & & & \\\\\n\\hline\n\\end{array}$$\n```", "md": "```markdown\nBLIP2 Figure 4: Generated captions overall exhibit\n0.08 Raw higher image-text alignment than raw captions; this indicates that the former is less noisy as a training source. We randomly sample 1% of the 128M candidate pool and given the same set of images, compare the cosine similarity distribution between raw caption data and BLIP2 caption data. We find that overall BLIP2 captions have much higher image-text cosine similarity (mean similarity $$0.251$$ vs $$0.208$$).\n\nCLIP ViT-L/14 image-text cosine similarity scales with the training set size, and calculate the number of unique trigrams across all captions in the subset. With this diversity metric, we find that BLIP2 captions actually lag behind raw captions (Figure 5). Using only the top 30% raw captions (based on CLIP score) is even more detrimental. We summarize these different aspects of caption quality in a noise versus diversity framework (Figure 5), which also offers some intuition for our best baseline uncovered in Section 5. CLIP score filtering that has been commonly adopted in prior work [18, 46] is effective at improving performance on raw data by removing noisy examples (i.e., those with poor image-text alignment). However, this procedure also lowers diversity (note: Figure 5 only provides a measure of text diversity, but image diversity is affected as well). By generating synthetic captions for the images that would be discarded otherwise, and subsequently only using pairs where the image-text similarities still meet the threshold, we manage to keep the overall noise level similarly low, while adding more diversity to the training pool. Progress along both axes enables further performance improvement compared to just filtering raw data.\n\nFigure 5: Combining raw and synthetic captions subject to a cosine similarity threshold helps reduce noise level while boosting data diversity, both of which are essential for achieving good performance.\nAverage image-text cosine similarity\nraw (top 30%) acc=27.3%\nraw (top 30%)+BLIP2 (70%,filtered) acc=31.7%\nBLIP2 (no filter) acc=28.1%\nraw (no filter) acc=17.6%\nImagenet accuracy\nIn this plot, circle size denotes the relative size of the resulting training set. While removing noisy image-text pairs, CLIP score filtering also lowers the diversity of the caption set substantially, as measured by the number of unique trigrams in the pool. Adding more useful training data by using BLIP2 captions for filtered out images, while respecting the existing CLIP score threshold, helps overcome this limitation and improves the training data quality along both axes.\n\nNo. of unique trigrams in captions (log scale)1e6\n\nPerformance analysis\nAfter diving deeper into properties of synthetic captions, we next analyze the training implications of these captions in more detail. We examine two models, one trained using only raw captions and the other using only BLIP2 captions, with both training sets having been filtered with CLIP score for top 30% pairs, and achieving similar performance on ImageNet (27.3% vs 27.5%). Averaged across 38 evaluation tasks, training on generated captions offers a 2.8% improvement. We break down\n```\n```latex\n$$\\begin{array}{|c|c|c|c|c|}\n\\hline\n\\text{Average image-text cosine similarity} & \\text{raw (top 30%)} & \\text{raw (top 30%)+BLIP2 (70%,filtered)} & 30 & \\text{threshold helps reduce noise level while} \\\\\n\\text{acc=27.3\\%} & \\text{acc=31.7\\%} & & 28 & \\text{boosting data diversity, both of which are} \\\\\n& & & & \\text{essential for achieving good performance.} \\\\\n\\hline\n\\text{BLIP2 (no filter)} & \\text{acc=28.1\\%} & & & \\\\\n\\hline\n\\text{raw (no filter)} & \\text{acc=17.6\\%} & & & \\\\\n\\hline\n\\end{array}$$\n```"}]}, {"page": 10, "text": "         40                                                                                                                                                                                                                                                                             39.2\n         30\n       Accuracy difference                                                                                                                                                                                                                                                 21.2  22.5\n         20\n                                                                                                                                                                                                                                                             12.6  13.9\n         10                                                                                                                                                                                                                                           10.4\n           0                                                -2.1   -2.0   -1.5   -1.2  -0.9   -0.6   -0.6   -0.2   -0.1    0.0    0.2    0.3    0.4    0.5   0.8    1.0    1.1    1.2    1.8    1.9    3.3   3.3    3.4    4.3    4.3    4.4    6.2\n                                              -6.6   -4.9\n         10              -11.0  -9.7   -8.9\n                              cars                       stl10                           cifar10                                                                          vtab/dtd              sun397                                            gtsrb                             mnist\n                   voc2007       food101      vtab/pets         country211                                      wilds/fmow                  vtab/pcam                                                                    objectnet   imagenet-r                          vtab/svhn\n                                                                                                  imagenetv2                 imagenet1k           imagenet-a                         imagenet-o\n                                                                   misc/winogavil         fgvc_aircraft              renderedsst2wilds/iwildcam         vtab/flowers        vtab/cifar100              vtab/resisc45                             vtab/eurosat\n                                                                                                                                                                                               fairness/geode\n                                                                                                                                                                                                           vtab/caltech101\n                                                  wilds/camelyon17   vtab/clevr_count_all      fairness/dollar_street                                                                                                   imagenet_sketch              retrieval/flickr_1k_test\n            vtab/clevr_closest_object_distance                                                                                          vtab/kitti_closest_vehicle_distance                                                         retrieval/mscoco_2014_5k_test\n Figure 6: Given similar ImageNet accuracy, training with generated captions improves perfor-\n mance on 23 out of 38 tasks compared to training with raw captions, especially on ImageNet\n distribution shifts, text recognition and retrieval tasks. We compare performance on each task of\n the DataComp benchmark between training with only BLIP2 captions and training with only raw captions;\n both datasets have been filtered with CLIP score to select the top 30% examples. Even though the two\n training sets yield similar ImageNet accuracy (\u223c27%), using generated captions leads to 2.8% improvement\n on average accuracy, including minor gains on ImageNet distribution shifts and significant gains on MNIST,\n SVHN, Flickr and MS-COCO retrieval.\n performance difference between the two models on individual tasks (Figure 6), and observe that\n BLIP2 captions also perform better on ImageNet-derived distribution shifts and text recognition\n(e.g., MNIST, SVHN). Notably, among the tasks with the biggest performance gains are Flickr\n and MS-COCO retrieval. We provide a similar analysis in Appendix Figure 10, where expanding a\n filtered raw dataset with additional images and their BLIP2 captions improves CLIP performance\n on 30 out of 38 tasks.\nThe two models compared above share similar ImageNet accuracy but may not be trained on the\n same images. In Figure 7, we fix the set of training samples to be the top 30% with highest cosine\n similarity between image and raw text. Replacing the raw captions with BLIP2 captions increases\n retrieval performance on Flickr and MS-COCO by more than 1.5\u00d7 (first two columns of each task).\nWe include retrieval performance of training on the entire pool with BLIP2 captions (generated using\n either the pre-trained or the fine-tuned captioning model), as well as that of training on a mixture\n of raw and BLIP2 captions, to demonstrate the consistent gains that synthetic captions offer.\n 7              Performance at scale\nWe next apply select baselines described in Section 5 to a wider range of candidate pool sizes,\n ranging from 12.8M to 1.28B samples. In particular, we examine training on the entire pool with\n only raw captions or only BLIP2 captions, CLIP score filtering, using the intersection of top CLIP\n score examples and examples that lie in clusters close to ImageNet train set, as well as mixing raw\n and synthetic captions\u2014our best baseline from the medium scale. The filtering percentage for each\n method is tuned on the medium scale candidate pool and then applied to experiments at other scales.\n Given a starting pool of N samples, we limit the training budget to N steps. The 400M and 1.28B\n                                                                                                                                                   10", "md": "# Document\n\n$$\n\\begin{array}{cc}\n40 & 39.2 \\\\\n30 & \\\\\n\\text{Accuracy difference} & 21.2 \\quad 22.5 \\\\\n20 & \\\\\n& 12.6 \\quad 13.9 \\\\\n10 & 10.4 \\\\\n0 & -2.1 \\quad -2.0 \\quad -1.5 \\quad -1.2 \\quad -0.9 \\quad -0.6 \\quad -0.6 \\quad -0.2 \\quad -0.1 \\quad 0.0 \\quad 0.2 \\quad 0.3 \\quad 0.4 \\quad 0.5 \\quad 0.8 \\quad 1.0 \\quad 1.1 \\quad 1.2 \\quad 1.8 \\quad 1.9 \\quad 3.3 \\quad 3.3 \\quad 3.4 \\quad 4.3 \\quad 4.3 \\quad 4.4 \\quad 6.2 \\\\\n& -6.6 \\quad -4.9 \\\\\n10 & -11.0 \\quad -9.7 \\quad -8.9 \\\\\n\\end{array}\n$$\n\nFigure 6: Given similar ImageNet accuracy, training with generated captions improves performance on 23 out of 38 tasks compared to training with raw captions, especially on ImageNet distribution shifts, text recognition and retrieval tasks. We compare performance on each task of the DataComp benchmark between training with only BLIP2 captions and training with only raw captions; both datasets have been filtered with CLIP score to select the top 30% examples. Even though the two training sets yield similar ImageNet accuracy (\u223c27%), using generated captions leads to 2.8% improvement on average accuracy, including minor gains on ImageNet distribution shifts and significant gains on MNIST, SVHN, Flickr and MS-COCO retrieval.\n\nPerformance difference between the two models on individual tasks (Figure 6), and observe that BLIP2 captions also perform better on ImageNet-derived distribution shifts and text recognition (e.g., MNIST, SVHN). Notably, among the tasks with the biggest performance gains are Flickr and MS-COCO retrieval. We provide a similar analysis in Appendix Figure 10, where expanding a filtered raw dataset with additional images and their BLIP2 captions improves CLIP performance on 30 out of 38 tasks.\n\nThe two models compared above share similar ImageNet accuracy but may not be trained on the same images. In Figure 7, we fix the set of training samples to be the top 30% with highest cosine similarity between image and raw text. Replacing the raw captions with BLIP2 captions increases retrieval performance on Flickr and MS-COCO by more than 1.5\u00d7 (first two columns of each task). We include retrieval performance of training on the entire pool with BLIP2 captions (generated using either the pre-trained or the fine-tuned captioning model), as well as that of training on a mixture of raw and BLIP2 captions, to demonstrate the consistent gains that synthetic captions offer.\n\n7 Performance at scale We next apply select baselines described in Section 5 to a wider range of candidate pool sizes, ranging from 12.8M to 1.28B samples. In particular, we examine training on the entire pool with only raw captions or only BLIP2 captions, CLIP score filtering, using the intersection of top CLIP score examples and examples that lie in clusters close to ImageNet train set, as well as mixing raw and synthetic captions\u2014our best baseline from the medium scale. The filtering percentage for each method is tuned on the medium scale candidate pool and then applied to experiments at other scales. Given a starting pool of N samples, we limit the training budget to N steps. The 400M and 1.28B 10", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "$$\n\\begin{array}{cc}\n40 & 39.2 \\\\\n30 & \\\\\n\\text{Accuracy difference} & 21.2 \\quad 22.5 \\\\\n20 & \\\\\n& 12.6 \\quad 13.9 \\\\\n10 & 10.4 \\\\\n0 & -2.1 \\quad -2.0 \\quad -1.5 \\quad -1.2 \\quad -0.9 \\quad -0.6 \\quad -0.6 \\quad -0.2 \\quad -0.1 \\quad 0.0 \\quad 0.2 \\quad 0.3 \\quad 0.4 \\quad 0.5 \\quad 0.8 \\quad 1.0 \\quad 1.1 \\quad 1.2 \\quad 1.8 \\quad 1.9 \\quad 3.3 \\quad 3.3 \\quad 3.4 \\quad 4.3 \\quad 4.3 \\quad 4.4 \\quad 6.2 \\\\\n& -6.6 \\quad -4.9 \\\\\n10 & -11.0 \\quad -9.7 \\quad -8.9 \\\\\n\\end{array}\n$$\n\nFigure 6: Given similar ImageNet accuracy, training with generated captions improves performance on 23 out of 38 tasks compared to training with raw captions, especially on ImageNet distribution shifts, text recognition and retrieval tasks. We compare performance on each task of the DataComp benchmark between training with only BLIP2 captions and training with only raw captions; both datasets have been filtered with CLIP score to select the top 30% examples. Even though the two training sets yield similar ImageNet accuracy (\u223c27%), using generated captions leads to 2.8% improvement on average accuracy, including minor gains on ImageNet distribution shifts and significant gains on MNIST, SVHN, Flickr and MS-COCO retrieval.\n\nPerformance difference between the two models on individual tasks (Figure 6), and observe that BLIP2 captions also perform better on ImageNet-derived distribution shifts and text recognition (e.g., MNIST, SVHN). Notably, among the tasks with the biggest performance gains are Flickr and MS-COCO retrieval. We provide a similar analysis in Appendix Figure 10, where expanding a filtered raw dataset with additional images and their BLIP2 captions improves CLIP performance on 30 out of 38 tasks.\n\nThe two models compared above share similar ImageNet accuracy but may not be trained on the same images. In Figure 7, we fix the set of training samples to be the top 30% with highest cosine similarity between image and raw text. Replacing the raw captions with BLIP2 captions increases retrieval performance on Flickr and MS-COCO by more than 1.5\u00d7 (first two columns of each task). We include retrieval performance of training on the entire pool with BLIP2 captions (generated using either the pre-trained or the fine-tuned captioning model), as well as that of training on a mixture of raw and BLIP2 captions, to demonstrate the consistent gains that synthetic captions offer.\n\n7 Performance at scale We next apply select baselines described in Section 5 to a wider range of candidate pool sizes, ranging from 12.8M to 1.28B samples. In particular, we examine training on the entire pool with only raw captions or only BLIP2 captions, CLIP score filtering, using the intersection of top CLIP score examples and examples that lie in clusters close to ImageNet train set, as well as mixing raw and synthetic captions\u2014our best baseline from the medium scale. The filtering percentage for each method is tuned on the medium scale candidate pool and then applied to experiments at other scales. Given a starting pool of N samples, we limit the training budget to N steps. The 400M and 1.28B 10", "md": "$$\n\\begin{array}{cc}\n40 & 39.2 \\\\\n30 & \\\\\n\\text{Accuracy difference} & 21.2 \\quad 22.5 \\\\\n20 & \\\\\n& 12.6 \\quad 13.9 \\\\\n10 & 10.4 \\\\\n0 & -2.1 \\quad -2.0 \\quad -1.5 \\quad -1.2 \\quad -0.9 \\quad -0.6 \\quad -0.6 \\quad -0.2 \\quad -0.1 \\quad 0.0 \\quad 0.2 \\quad 0.3 \\quad 0.4 \\quad 0.5 \\quad 0.8 \\quad 1.0 \\quad 1.1 \\quad 1.2 \\quad 1.8 \\quad 1.9 \\quad 3.3 \\quad 3.3 \\quad 3.4 \\quad 4.3 \\quad 4.3 \\quad 4.4 \\quad 6.2 \\\\\n& -6.6 \\quad -4.9 \\\\\n10 & -11.0 \\quad -9.7 \\quad -8.9 \\\\\n\\end{array}\n$$\n\nFigure 6: Given similar ImageNet accuracy, training with generated captions improves performance on 23 out of 38 tasks compared to training with raw captions, especially on ImageNet distribution shifts, text recognition and retrieval tasks. We compare performance on each task of the DataComp benchmark between training with only BLIP2 captions and training with only raw captions; both datasets have been filtered with CLIP score to select the top 30% examples. Even though the two training sets yield similar ImageNet accuracy (\u223c27%), using generated captions leads to 2.8% improvement on average accuracy, including minor gains on ImageNet distribution shifts and significant gains on MNIST, SVHN, Flickr and MS-COCO retrieval.\n\nPerformance difference between the two models on individual tasks (Figure 6), and observe that BLIP2 captions also perform better on ImageNet-derived distribution shifts and text recognition (e.g., MNIST, SVHN). Notably, among the tasks with the biggest performance gains are Flickr and MS-COCO retrieval. We provide a similar analysis in Appendix Figure 10, where expanding a filtered raw dataset with additional images and their BLIP2 captions improves CLIP performance on 30 out of 38 tasks.\n\nThe two models compared above share similar ImageNet accuracy but may not be trained on the same images. In Figure 7, we fix the set of training samples to be the top 30% with highest cosine similarity between image and raw text. Replacing the raw captions with BLIP2 captions increases retrieval performance on Flickr and MS-COCO by more than 1.5\u00d7 (first two columns of each task). We include retrieval performance of training on the entire pool with BLIP2 captions (generated using either the pre-trained or the fine-tuned captioning model), as well as that of training on a mixture of raw and BLIP2 captions, to demonstrate the consistent gains that synthetic captions offer.\n\n7 Performance at scale We next apply select baselines described in Section 5 to a wider range of candidate pool sizes, ranging from 12.8M to 1.28B samples. In particular, we examine training on the entire pool with only raw captions or only BLIP2 captions, CLIP score filtering, using the intersection of top CLIP score examples and examples that lie in clusters close to ImageNet train set, as well as mixing raw and synthetic captions\u2014our best baseline from the medium scale. The filtering percentage for each method is tuned on the medium scale candidate pool and then applied to experiments at other scales. Given a starting pool of N samples, we limit the training budget to N steps. The 400M and 1.28B 10"}]}, {"page": 11, "text": "                                                            Raw (top 30%)                                   Figure 7: Synthetic captions display a clear\n                                                            BLIP2 (top 30% raw data)                        advantage over raw captions on retrieval tasks.\n        0.5                                                 Raw (top 30%)+BLIP2 (70%, filtered)            We highlight the superior performance on Flickr and\n                                                            BLIP2 (no filtering)\n                                                            BLIP2-finetuned (no filtering)                  MS-COCO retrieval obtained from training CLIP\n        0.4                                                                                                 on captions generated by BLIP2 (pre-trained model\n       Retrieval score                                                                                      or model that has been fine-tuned on MS-COCO),\n        0.3                                                                                                 compared to training on raw captions. In particular,\n                                                                                                            the first two columns of each task represent two\n        0.2                                                                                                 models trained on the same set of images (i.e., those\n                                                                                                           whose cosine similarity between image and raw text\n                                                                                                            embeddings are in the top 30%), just with different\n        0.1                                                                                                 captions. This suggests that substantial gains on\n                                                                                                            retrieval tasks can be obtained solely by using better\n        0.0                    Flickr                                   MS-COCO                             aligned captions.\n scales use the large training settings from DataComp (see [18]).\nWe focus on ImageNet classification and Flickr retrieval performance (note: MS-COCO training\n set was included in BLIP2\u2019s pre-training data so we have excluded MS-COCO retrieval from this\n comparison). At larger data quantity regimes, using synthetic captions continues to substantially\n outperform existing raw-text filtering baselines at retrieval (Figure 8, right plot). On ImageNet,\n however, adding BLIP2 captions to the training mix sees diminishing returns: Raw (top 30%\n intersect IN1k) + BLIP2 (remaining 70%, filtered, intersect IN1k) outperforms Data-\n Comp\u2019s best baseline trained on raw data, Raw (top 30% intersect IN1k), by 2.5% at 400M\n scale and 1.2% at 1.28B scale (Figure 8, left plot).\nTo give some intuition for this result, we offer two candidate hypotheses:\n\u2022 As noted in Section 6, both caption noise and diversity are important considerations for performance.\n     Noise level, measured by average image-text cosine similarity, stays about the same across all\n     scales for each training distribution. In contrast, the diversity gap between model-generated text\n     and web-scraped text may become more significant with increasing data quantities. We repeat\n     the caption quality analyses from Section 6 with varying random subset size, and find that when\n     using the number of unique nouns and unique trigrams as proxies for text diversity, generated\n     captions exhibit a worse scaling trend than raw captions (Appendix Figure 12).\n\u2022 Image quality becomes increasingly important at larger scales:\n    (i) from 12.8M to 128M scale, training on the entire candidate pool with BLIP2 captions outperforms\n     competitive filtering baselines done on raw data (e.g., Raw (top 30%)). This is not the case for\n     larger scales.\n    (ii) starting from 128M scale, baselines that also curate image content (i.e., intersection of top\n     CLIP score examples and those that lie in clusters close to the ImageNet1k train set) consistently\n     outperform baselines that involve only CLIP score filtering, using either raw or BLIP2 captions.\n Exact performance numbers can be found in Appendix D, Table 4. Overall, we find that given\n a fixed training budget, making more datapoints useful by carefully replacing noisy raw captions\nwith synthetic captions\u2014e.g., Raw (top 30%) + BLIP2 (70%, filtered) versus Raw (top\n30%)\u2014still offers classification and retrieval gains across all scales. However, for synthetic captions\n                                                                                                       11", "md": "# Synthetic Captions vs Raw Captions\n\n## Figure 7: Synthetic captions display a clear advantage over raw captions on retrieval tasks\n\nRetrieval score\n\n| |Flickr|MS-COCO|\n|---|---|---|\n|Raw (top 30%)|0.5|0.4|\n|BLIP2 (top 30% raw data)|0.5|0.4|\n|Raw (top 30%)+BLIP2 (70%, filtered)|0.5|0.4|\n|BLIP2 (no filtering)|0.5|0.4|\n|BLIP2-finetuned (no filtering)|0.5|0.4|\n\nscales use the large training settings from DataComp (see [18]).\n\nWe focus on ImageNet classification and Flickr retrieval performance (note: MS-COCO training set was included in BLIP2\u2019s pre-training data so we have excluded MS-COCO retrieval from this comparison). At larger data quantity regimes, using synthetic captions continues to substantially outperform existing raw-text filtering baselines at retrieval (Figure 8, right plot). On ImageNet, however, adding BLIP2 captions to the training mix sees diminishing returns: Raw (top 30% intersect IN1k) + BLIP2 (remaining 70%, filtered, intersect IN1k) outperforms DataComp\u2019s best baseline trained on raw data, Raw (top 30% intersect IN1k), by 2.5% at 400M scale and 1.2% at 1.28B scale (Figure 8, left plot).\n\nTo give some intuition for this result, we offer two candidate hypotheses:\n\n1. As noted in Section 6, both caption noise and diversity are important considerations for performance.\n- Noise level, measured by average image-text cosine similarity, stays about the same across all scales for each training distribution. In contrast, the diversity gap between model-generated text and web-scraped text may become more significant with increasing data quantities. We repeat the caption quality analyses from Section 6 with varying random subset size, and find that when using the number of unique nouns and unique trigrams as proxies for text diversity, generated captions exhibit a worse scaling trend than raw captions (Appendix Figure 12).\n2. Image quality becomes increasingly important at larger scales:\n- from 12.8M to 128M scale, training on the entire candidate pool with BLIP2 captions outperforms competitive filtering baselines done on raw data (e.g., Raw (top 30%)). This is not the case for larger scales.\n- starting from 128M scale, baselines that also curate image content (i.e., intersection of top CLIP score examples and those that lie in clusters close to the ImageNet1k train set) consistently outperform baselines that involve only CLIP score filtering, using either raw or BLIP2 captions.\n\nExact performance numbers can be found in Appendix D, Table 4. Overall, we find that given a fixed training budget, making more datapoints useful by carefully replacing noisy raw captions with synthetic captions\u2014e.g., Raw (top 30%) + BLIP2 (70%, filtered) versus Raw (top 30%)\u2014still offers classification and retrieval gains across all scales. However, for synthetic captions", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Synthetic Captions vs Raw Captions", "md": "# Synthetic Captions vs Raw Captions"}, {"type": "heading", "lvl": 2, "value": "Figure 7: Synthetic captions display a clear advantage over raw captions on retrieval tasks", "md": "## Figure 7: Synthetic captions display a clear advantage over raw captions on retrieval tasks"}, {"type": "text", "value": "Retrieval score", "md": "Retrieval score"}, {"type": "table", "rows": [["", "Flickr", "MS-COCO"], ["Raw (top 30%)", "0.5", "0.4"], ["BLIP2 (top 30% raw data)", "0.5", "0.4"], ["Raw (top 30%)+BLIP2 (70%, filtered)", "0.5", "0.4"], ["BLIP2 (no filtering)", "0.5", "0.4"], ["BLIP2-finetuned (no filtering)", "0.5", "0.4"]], "md": "| |Flickr|MS-COCO|\n|---|---|---|\n|Raw (top 30%)|0.5|0.4|\n|BLIP2 (top 30% raw data)|0.5|0.4|\n|Raw (top 30%)+BLIP2 (70%, filtered)|0.5|0.4|\n|BLIP2 (no filtering)|0.5|0.4|\n|BLIP2-finetuned (no filtering)|0.5|0.4|", "isPerfectTable": true, "csv": "\"\",\"Flickr\",\"MS-COCO\"\n\"Raw (top 30%)\",\"0.5\",\"0.4\"\n\"BLIP2 (top 30% raw data)\",\"0.5\",\"0.4\"\n\"Raw (top 30%)+BLIP2 (70%, filtered)\",\"0.5\",\"0.4\"\n\"BLIP2 (no filtering)\",\"0.5\",\"0.4\"\n\"BLIP2-finetuned (no filtering)\",\"0.5\",\"0.4\""}, {"type": "text", "value": "scales use the large training settings from DataComp (see [18]).\n\nWe focus on ImageNet classification and Flickr retrieval performance (note: MS-COCO training set was included in BLIP2\u2019s pre-training data so we have excluded MS-COCO retrieval from this comparison). At larger data quantity regimes, using synthetic captions continues to substantially outperform existing raw-text filtering baselines at retrieval (Figure 8, right plot). On ImageNet, however, adding BLIP2 captions to the training mix sees diminishing returns: Raw (top 30% intersect IN1k) + BLIP2 (remaining 70%, filtered, intersect IN1k) outperforms DataComp\u2019s best baseline trained on raw data, Raw (top 30% intersect IN1k), by 2.5% at 400M scale and 1.2% at 1.28B scale (Figure 8, left plot).\n\nTo give some intuition for this result, we offer two candidate hypotheses:\n\n1. As noted in Section 6, both caption noise and diversity are important considerations for performance.\n- Noise level, measured by average image-text cosine similarity, stays about the same across all scales for each training distribution. In contrast, the diversity gap between model-generated text and web-scraped text may become more significant with increasing data quantities. We repeat the caption quality analyses from Section 6 with varying random subset size, and find that when using the number of unique nouns and unique trigrams as proxies for text diversity, generated captions exhibit a worse scaling trend than raw captions (Appendix Figure 12).\n2. Image quality becomes increasingly important at larger scales:\n- from 12.8M to 128M scale, training on the entire candidate pool with BLIP2 captions outperforms competitive filtering baselines done on raw data (e.g., Raw (top 30%)). This is not the case for larger scales.\n- starting from 128M scale, baselines that also curate image content (i.e., intersection of top CLIP score examples and those that lie in clusters close to the ImageNet1k train set) consistently outperform baselines that involve only CLIP score filtering, using either raw or BLIP2 captions.\n\nExact performance numbers can be found in Appendix D, Table 4. Overall, we find that given a fixed training budget, making more datapoints useful by carefully replacing noisy raw captions with synthetic captions\u2014e.g., Raw (top 30%) + BLIP2 (70%, filtered) versus Raw (top 30%)\u2014still offers classification and retrieval gains across all scales. However, for synthetic captions", "md": "scales use the large training settings from DataComp (see [18]).\n\nWe focus on ImageNet classification and Flickr retrieval performance (note: MS-COCO training set was included in BLIP2\u2019s pre-training data so we have excluded MS-COCO retrieval from this comparison). At larger data quantity regimes, using synthetic captions continues to substantially outperform existing raw-text filtering baselines at retrieval (Figure 8, right plot). On ImageNet, however, adding BLIP2 captions to the training mix sees diminishing returns: Raw (top 30% intersect IN1k) + BLIP2 (remaining 70%, filtered, intersect IN1k) outperforms DataComp\u2019s best baseline trained on raw data, Raw (top 30% intersect IN1k), by 2.5% at 400M scale and 1.2% at 1.28B scale (Figure 8, left plot).\n\nTo give some intuition for this result, we offer two candidate hypotheses:\n\n1. As noted in Section 6, both caption noise and diversity are important considerations for performance.\n- Noise level, measured by average image-text cosine similarity, stays about the same across all scales for each training distribution. In contrast, the diversity gap between model-generated text and web-scraped text may become more significant with increasing data quantities. We repeat the caption quality analyses from Section 6 with varying random subset size, and find that when using the number of unique nouns and unique trigrams as proxies for text diversity, generated captions exhibit a worse scaling trend than raw captions (Appendix Figure 12).\n2. Image quality becomes increasingly important at larger scales:\n- from 12.8M to 128M scale, training on the entire candidate pool with BLIP2 captions outperforms competitive filtering baselines done on raw data (e.g., Raw (top 30%)). This is not the case for larger scales.\n- starting from 128M scale, baselines that also curate image content (i.e., intersection of top CLIP score examples and those that lie in clusters close to the ImageNet1k train set) consistently outperform baselines that involve only CLIP score filtering, using either raw or BLIP2 captions.\n\nExact performance numbers can be found in Appendix D, Table 4. Overall, we find that given a fixed training budget, making more datapoints useful by carefully replacing noisy raw captions with synthetic captions\u2014e.g., Raw (top 30%) + BLIP2 (70%, filtered) versus Raw (top 30%)\u2014still offers classification and retrieval gains across all scales. However, for synthetic captions"}]}, {"page": 12, "text": "                  Raw captions (no filter)                                                             80          Raw captions (no filter)\n       60         Raw captions (top 30%)                                                                           Raw captions (top 30%)\n                  Raw (top 30% intersect IN1k)                                                         70          Raw (top 30% intersect IN1k)\n                  Raw (top 30%) + BLIP2 (70%, filtered)                                                            Raw (top 30%) + BLIP2 (70%, filtered)\n       50         BLIP2 captions (no filter)                                                           60          BLIP2 captions (no filter)\n                  BLIP2 captions (top 50%)                                                            Flickr retrieval score\n      ImageNet accuracy                                                                                            BLIP2 captions (top 50%)\n       40         BLIP2 (top 75% intersect IN1k)                                                       50          BLIP2 (top 75% intersect IN1k)\n       30                                                                                              40\n                                                                                                       30\n       20\n                                                                                                       20\n       10                                                                                              10\n        0        12.8M            40M            128M            400M            1.28B                   0       12.8M            40M             128M    400M  1.28B\nFigure 8: With access to generated captions, we find that the best data filtering method for\nImageNet classification varies with the scale of the candidate pool; however, when it comes to\nretrieval, training on synthetic captions is beneficial across all scales. We apply select baselines\nfrom Section 5 to a range of candidate pool sizes, and find that the best method on Flickr retrieval always\ninvolves synthetic captions (right plot). On ImageNet (left plot), selecting meaningful images (e.g., those that\nlie close to the ImageNet train set in the embedding space) becomes increasingly important at larger scales\n(see dotted versus striked columns). As the data pool size increases, using BLIP2 captions seems to yield\ndiminishing returns, possibly due to the saturation of text diversity obtained from image captioning models.\nto continue to perform competitively on ImageNet at larger data regimes, we need to start paying\nattention to image content, as well as enhancing the diversity of the generated text.\n8         Conclusion\nIn this work, we demonstrate the effectiveness of synthetic captions in improving caption quality for\nmultimodal training, as well as enhancing certain capabilities of the resulting model (e.g., retrieval).\nNotably, we find that fine-tuning general-purpose models towards the task of image captioning\nactually makes them less effective at producing good captions for CLIP training. Our experiments\nwith various data pool sizes, ranging from 12.8M to 1.28B image-text pairs, show that including\ngenerated captions in the training data can be highly effective at small and medium scales. However,\nwith larger data quantities, the diversity gap between model-generated and web-scraped text begin\nto hinder performance gains, and it also becomes increasingly harder to obtain state-of-the-art\nImageNet accuracy by just improving text supervision alone.\nLimitations.                    Our experiments do not involve an exhaustive list of image captioning systems\ncurrently available. Given a captioning model of sufficient capability\u2014i.e., it can generate captions\nfor training CLIP to reach a good performance\u2014a major theme of our work is understanding how\nto combine signals from both raw and synthetic captions, as well as the differences between these\ntwo sources of text. We note that even with improved caption quality, multimodal web datasets\nmay still contain harmful stereotypes, some of which have been extensively discussed in prior work\n[7]. In Appendix H, we conduct some preliminary investigation on the change in race and gender\nbias between training on only raw web-crawled text and training on synthetic captions. Besides,\ngenerated captions also inherit biases from the captioning models, and using these captions to train\n                                                                                              12", "md": "# Document\n\n## Raw captions (no filter)\n\n80 Raw captions (no filter)\n\n60 Raw captions (top 30%)\n\nRaw (top 30% intersect IN1k) 70 Raw (top 30% intersect IN1k)\n\nRaw (top 30%) + BLIP2 (70%, filtered) Raw (top 30%) + BLIP2 (70%, filtered)\n\nBLIP2 captions (no filter) 60 BLIP2 captions (no filter)\n\nBLIP2 captions (top 50%) Flickr retrieval score\n\nBLIP2 (top 75% intersect IN1k) 50 BLIP2 (top 75% intersect IN1k)\n\n### ImageNet accuracy\n\nBLIP2 captions (top 50%)\n\n| |12.8M|40M|128M|400M|1.28B|\n|---|---|---|---|---|---|\n|0|12.8M|40M|128M|400M|1.28B|\n\n### Figure 8:\n\nWith access to generated captions, we find that the best data filtering method for ImageNet classification varies with the scale of the candidate pool; however, when it comes to retrieval, training on synthetic captions is beneficial across all scales. We apply select baselines from Section 5 to a range of candidate pool sizes, and find that the best method on Flickr retrieval always involves synthetic captions (right plot). On ImageNet (left plot), selecting meaningful images (e.g., those that lie close to the ImageNet train set in the embedding space) becomes increasingly important at larger scales (see dotted versus striked columns). As the data pool size increases, using BLIP2 captions seems to yield diminishing returns, possibly due to the saturation of text diversity obtained from image captioning models. To continue to perform competitively on ImageNet at larger data regimes, we need to start paying attention to image content, as well as enhancing the diversity of the generated text.\n\n### Conclusion\n\nIn this work, we demonstrate the effectiveness of synthetic captions in improving caption quality for multimodal training, as well as enhancing certain capabilities of the resulting model (e.g., retrieval). Notably, we find that fine-tuning general-purpose models towards the task of image captioning actually makes them less effective at producing good captions for CLIP training. Our experiments with various data pool sizes, ranging from 12.8M to 1.28B image-text pairs, show that including generated captions in the training data can be highly effective at small and medium scales. However, with larger data quantities, the diversity gap between model-generated and web-scraped text begin to hinder performance gains, and it also becomes increasingly harder to obtain state-of-the-art ImageNet accuracy by just improving text supervision alone.\n\n### Limitations\n\nOur experiments do not involve an exhaustive list of image captioning systems currently available. Given a captioning model of sufficient capability\u2014i.e., it can generate captions for training CLIP to reach a good performance\u2014a major theme of our work is understanding how to combine signals from both raw and synthetic captions, as well as the differences between these two sources of text. We note that even with improved caption quality, multimodal web datasets may still contain harmful stereotypes, some of which have been extensively discussed in prior work [7]. In Appendix H, we conduct some preliminary investigation on the change in race and gender bias between training on only raw web-crawled text and training on synthetic captions. Besides, generated captions also inherit biases from the captioning models, and using these captions to train", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Raw captions (no filter)", "md": "## Raw captions (no filter)"}, {"type": "text", "value": "80 Raw captions (no filter)\n\n60 Raw captions (top 30%)\n\nRaw (top 30% intersect IN1k) 70 Raw (top 30% intersect IN1k)\n\nRaw (top 30%) + BLIP2 (70%, filtered) Raw (top 30%) + BLIP2 (70%, filtered)\n\nBLIP2 captions (no filter) 60 BLIP2 captions (no filter)\n\nBLIP2 captions (top 50%) Flickr retrieval score\n\nBLIP2 (top 75% intersect IN1k) 50 BLIP2 (top 75% intersect IN1k)", "md": "80 Raw captions (no filter)\n\n60 Raw captions (top 30%)\n\nRaw (top 30% intersect IN1k) 70 Raw (top 30% intersect IN1k)\n\nRaw (top 30%) + BLIP2 (70%, filtered) Raw (top 30%) + BLIP2 (70%, filtered)\n\nBLIP2 captions (no filter) 60 BLIP2 captions (no filter)\n\nBLIP2 captions (top 50%) Flickr retrieval score\n\nBLIP2 (top 75% intersect IN1k) 50 BLIP2 (top 75% intersect IN1k)"}, {"type": "heading", "lvl": 3, "value": "ImageNet accuracy", "md": "### ImageNet accuracy"}, {"type": "text", "value": "BLIP2 captions (top 50%)", "md": "BLIP2 captions (top 50%)"}, {"type": "table", "rows": [["", "12.8M", "40M", "128M", "400M", "1.28B"], ["0", "12.8M", "40M", "128M", "400M", "1.28B"]], "md": "| |12.8M|40M|128M|400M|1.28B|\n|---|---|---|---|---|---|\n|0|12.8M|40M|128M|400M|1.28B|", "isPerfectTable": true, "csv": "\"\",\"12.8M\",\"40M\",\"128M\",\"400M\",\"1.28B\"\n\"0\",\"12.8M\",\"40M\",\"128M\",\"400M\",\"1.28B\""}, {"type": "heading", "lvl": 3, "value": "Figure 8:", "md": "### Figure 8:"}, {"type": "text", "value": "With access to generated captions, we find that the best data filtering method for ImageNet classification varies with the scale of the candidate pool; however, when it comes to retrieval, training on synthetic captions is beneficial across all scales. We apply select baselines from Section 5 to a range of candidate pool sizes, and find that the best method on Flickr retrieval always involves synthetic captions (right plot). On ImageNet (left plot), selecting meaningful images (e.g., those that lie close to the ImageNet train set in the embedding space) becomes increasingly important at larger scales (see dotted versus striked columns). As the data pool size increases, using BLIP2 captions seems to yield diminishing returns, possibly due to the saturation of text diversity obtained from image captioning models. To continue to perform competitively on ImageNet at larger data regimes, we need to start paying attention to image content, as well as enhancing the diversity of the generated text.", "md": "With access to generated captions, we find that the best data filtering method for ImageNet classification varies with the scale of the candidate pool; however, when it comes to retrieval, training on synthetic captions is beneficial across all scales. We apply select baselines from Section 5 to a range of candidate pool sizes, and find that the best method on Flickr retrieval always involves synthetic captions (right plot). On ImageNet (left plot), selecting meaningful images (e.g., those that lie close to the ImageNet train set in the embedding space) becomes increasingly important at larger scales (see dotted versus striked columns). As the data pool size increases, using BLIP2 captions seems to yield diminishing returns, possibly due to the saturation of text diversity obtained from image captioning models. To continue to perform competitively on ImageNet at larger data regimes, we need to start paying attention to image content, as well as enhancing the diversity of the generated text."}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "In this work, we demonstrate the effectiveness of synthetic captions in improving caption quality for multimodal training, as well as enhancing certain capabilities of the resulting model (e.g., retrieval). Notably, we find that fine-tuning general-purpose models towards the task of image captioning actually makes them less effective at producing good captions for CLIP training. Our experiments with various data pool sizes, ranging from 12.8M to 1.28B image-text pairs, show that including generated captions in the training data can be highly effective at small and medium scales. However, with larger data quantities, the diversity gap between model-generated and web-scraped text begin to hinder performance gains, and it also becomes increasingly harder to obtain state-of-the-art ImageNet accuracy by just improving text supervision alone.", "md": "In this work, we demonstrate the effectiveness of synthetic captions in improving caption quality for multimodal training, as well as enhancing certain capabilities of the resulting model (e.g., retrieval). Notably, we find that fine-tuning general-purpose models towards the task of image captioning actually makes them less effective at producing good captions for CLIP training. Our experiments with various data pool sizes, ranging from 12.8M to 1.28B image-text pairs, show that including generated captions in the training data can be highly effective at small and medium scales. However, with larger data quantities, the diversity gap between model-generated and web-scraped text begin to hinder performance gains, and it also becomes increasingly harder to obtain state-of-the-art ImageNet accuracy by just improving text supervision alone."}, {"type": "heading", "lvl": 3, "value": "Limitations", "md": "### Limitations"}, {"type": "text", "value": "Our experiments do not involve an exhaustive list of image captioning systems currently available. Given a captioning model of sufficient capability\u2014i.e., it can generate captions for training CLIP to reach a good performance\u2014a major theme of our work is understanding how to combine signals from both raw and synthetic captions, as well as the differences between these two sources of text. We note that even with improved caption quality, multimodal web datasets may still contain harmful stereotypes, some of which have been extensively discussed in prior work [7]. In Appendix H, we conduct some preliminary investigation on the change in race and gender bias between training on only raw web-crawled text and training on synthetic captions. Besides, generated captions also inherit biases from the captioning models, and using these captions to train", "md": "Our experiments do not involve an exhaustive list of image captioning systems currently available. Given a captioning model of sufficient capability\u2014i.e., it can generate captions for training CLIP to reach a good performance\u2014a major theme of our work is understanding how to combine signals from both raw and synthetic captions, as well as the differences between these two sources of text. We note that even with improved caption quality, multimodal web datasets may still contain harmful stereotypes, some of which have been extensively discussed in prior work [7]. In Appendix H, we conduct some preliminary investigation on the change in race and gender bias between training on only raw web-crawled text and training on synthetic captions. Besides, generated captions also inherit biases from the captioning models, and using these captions to train"}]}, {"page": 13, "text": "the next generation of models can amplify the biases. The risks from using model outputs to replace\nhuman annotations have been studied in simplified settings in [48, 50].\nFuture work.      Our findings motivate a number of interesting future directions. One concrete\nquestion is improving the diversity of generated captions at large scale, such as by varying the\nsoftmax temperature (we only experiment with T = 0.75 at this scale, chosen based on our ablation\nstudy at the medium scale), or by combining synthetic caption data from multiple image captioning\nsystems. Another direction is proposing new algorithms to combine information from raw and\ngenerated captions, beyond what we already investigated in Section 5 and Appendix D. Future work\ncould also explore using text-to-image generation [34, 42, 43] to create synthetic training images for\nconcepts that are underrepresented in existing captions, in order to boost data diversity and close\nknowledge gaps in the resulting model.\nAcknowledgements\nWe thank Stability AI for the generous assistance with compute resources. We are grateful to Josh\nGardner and Simon Kornblith for providing feedback on the manuscript. We also thank Maciej\nKilian, Anas Awadalla, Alex Fang, Dhruba Ghosh and Jonathan Hayase for helpful discussions while\nworking on this paper. SYG is supported by a NSF Graduate Research Fellowship. This work is\nsupported in part by Open Philanthropy, the Allen Institute for AI, and NSF grants DMS-2134012\nand CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML).\nReferences\n [1] A. Abbas, K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos. Semdedup: Data-efficient\n     learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.\n [2] H. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee,\n     and P. Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF\n     international conference on computer vision, pages 8948\u20138957, 2019.\n [3] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\n     M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in\n     Neural Information Processing Systems, 35:23716\u201323736, 2022.\n [4] P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice: Semantic propositional image\n     caption evaluation. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\n     The Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages 382\u2013398. Springer, 2016.\n [5] S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet. Synthetic data from diffusion\n     models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023.\n [6] H. Bansal and A. Grover. Leaving reality to imagination: Robust classification via generated\n     datasets. arXiv preprint arXiv:2302.02503, 2023.\n [7] A. Birhane, V. U. Prabhu, and E. Kahembwe. Multimodal datasets: misogyny, pornography,\n     and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.\n                                                  13", "md": "the next generation of models can amplify the biases. The risks from using model outputs to replace\nhuman annotations have been studied in simplified settings in [48], [50].\n\nFuture work. Our findings motivate a number of interesting future directions. One concrete\nquestion is improving the diversity of generated captions at large scale, such as by varying the\nsoftmax temperature (we only experiment with $$T = 0.75$$ at this scale, chosen based on our ablation\nstudy at the medium scale), or by combining synthetic caption data from multiple image captioning\nsystems. Another direction is proposing new algorithms to combine information from raw and\ngenerated captions, beyond what we already investigated in Section 5 and Appendix D. Future work\ncould also explore using text-to-image generation [34], [42], [43] to create synthetic training images for\nconcepts that are underrepresented in existing captions, in order to boost data diversity and close\nknowledge gaps in the resulting model.\n\nAcknowledgements\n\nWe thank Stability AI for the generous assistance with compute resources. We are grateful to Josh\nGardner and Simon Kornblith for providing feedback on the manuscript. We also thank Maciej\nKilian, Anas Awadalla, Alex Fang, Dhruba Ghosh and Jonathan Hayase for helpful discussions while\nworking on this paper. SYG is supported by a NSF Graduate Research Fellowship. This work is\nsupported in part by Open Philanthropy, the Allen Institute for AI, and NSF grants DMS-2134012\nand CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML).\n\n### References\n\n1.  A. Abbas, K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos. Semdedup: Data-efficient\nlearning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.\n2.  H. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee,\nand P. Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 8948\u20138957, 2019.\n3.  J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736, 2022.\n4.  P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice: Semantic propositional image\ncaption evaluation. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages 382\u2013398. Springer, 2016.\n5.  S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet. Synthetic data from diffusion\nmodels improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023.\n6.  H. Bansal and A. Grover. Leaving reality to imagination: Robust classification via generated\ndatasets. arXiv preprint arXiv:2302.02503, 2023.\n7.  A. Birhane, V. U. Prabhu, and E. Kahembwe. Multimodal datasets: misogyny, pornography,\nand malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.", "images": [], "items": [{"type": "text", "value": "the next generation of models can amplify the biases. The risks from using model outputs to replace\nhuman annotations have been studied in simplified settings in [48], [50].\n\nFuture work. Our findings motivate a number of interesting future directions. One concrete\nquestion is improving the diversity of generated captions at large scale, such as by varying the\nsoftmax temperature (we only experiment with $$T = 0.75$$ at this scale, chosen based on our ablation\nstudy at the medium scale), or by combining synthetic caption data from multiple image captioning\nsystems. Another direction is proposing new algorithms to combine information from raw and\ngenerated captions, beyond what we already investigated in Section 5 and Appendix D. Future work\ncould also explore using text-to-image generation [34], [42], [43] to create synthetic training images for\nconcepts that are underrepresented in existing captions, in order to boost data diversity and close\nknowledge gaps in the resulting model.\n\nAcknowledgements\n\nWe thank Stability AI for the generous assistance with compute resources. We are grateful to Josh\nGardner and Simon Kornblith for providing feedback on the manuscript. We also thank Maciej\nKilian, Anas Awadalla, Alex Fang, Dhruba Ghosh and Jonathan Hayase for helpful discussions while\nworking on this paper. SYG is supported by a NSF Graduate Research Fellowship. This work is\nsupported in part by Open Philanthropy, the Allen Institute for AI, and NSF grants DMS-2134012\nand CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML).", "md": "the next generation of models can amplify the biases. The risks from using model outputs to replace\nhuman annotations have been studied in simplified settings in [48], [50].\n\nFuture work. Our findings motivate a number of interesting future directions. One concrete\nquestion is improving the diversity of generated captions at large scale, such as by varying the\nsoftmax temperature (we only experiment with $$T = 0.75$$ at this scale, chosen based on our ablation\nstudy at the medium scale), or by combining synthetic caption data from multiple image captioning\nsystems. Another direction is proposing new algorithms to combine information from raw and\ngenerated captions, beyond what we already investigated in Section 5 and Appendix D. Future work\ncould also explore using text-to-image generation [34], [42], [43] to create synthetic training images for\nconcepts that are underrepresented in existing captions, in order to boost data diversity and close\nknowledge gaps in the resulting model.\n\nAcknowledgements\n\nWe thank Stability AI for the generous assistance with compute resources. We are grateful to Josh\nGardner and Simon Kornblith for providing feedback on the manuscript. We also thank Maciej\nKilian, Anas Awadalla, Alex Fang, Dhruba Ghosh and Jonathan Hayase for helpful discussions while\nworking on this paper. SYG is supported by a NSF Graduate Research Fellowship. This work is\nsupported in part by Open Philanthropy, the Allen Institute for AI, and NSF grants DMS-2134012\nand CCF-2019844 as a part of NSF Institute for Foundations of Machine Learning (IFML)."}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "text", "value": "1.  A. Abbas, K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos. Semdedup: Data-efficient\nlearning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.\n2.  H. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee,\nand P. Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 8948\u20138957, 2019.\n3.  J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736, 2022.\n4.  P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice: Semantic propositional image\ncaption evaluation. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages 382\u2013398. Springer, 2016.\n5.  S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet. Synthetic data from diffusion\nmodels improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023.\n6.  H. Bansal and A. Grover. Leaving reality to imagination: Robust classification via generated\ndatasets. arXiv preprint arXiv:2302.02503, 2023.\n7.  A. Birhane, V. U. Prabhu, and E. Kahembwe. Multimodal datasets: misogyny, pornography,\nand malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021.", "md": "1.  A. Abbas, K. Tirumala, D. Simig, S. Ganguli, and A. S. Morcos. Semdedup: Data-efficient\nlearning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023.\n2.  H. Agrawal, K. Desai, Y. Wang, X. Chen, R. Jain, M. Johnson, D. Batra, D. Parikh, S. Lee,\nand P. Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF\ninternational conference on computer vision, pages 8948\u20138957, 2019.\n3.  J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736, 2022.\n4.  P. Anderson, B. Fernando, M. Johnson, and S. Gould. Spice: Semantic propositional image\ncaption evaluation. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part V 14, pages 382\u2013398. Springer, 2016.\n5.  S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, and D. J. Fleet. Synthetic data from diffusion\nmodels improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023.\n6.  H. Bansal and A. Grover. Leaving reality to imagination: Robust classification via generated\ndatasets. arXiv preprint arXiv:2302.02503, 2023.\n7.  A. Birhane, V. U. Prabhu, and E. Kahembwe. Multimodal datasets: misogyny, pornography,\nand malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021."}]}, {"page": 14, "text": " [8] M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim. Coyo-700m: Image-text pair dataset.\n    https://github.com/kakaobrain/coyo-dataset, 2022.\n [9] L. Cao, B. Zhang, C. Chen, Y. Yang, X. Du, W. Zhang, Z. Lu, and Y. Zheng.              Less is\n    more: Removing text-regions improves clip training efficiency and robustness. arXiv preprint\n    arXiv:2305.05095, 2023.\n[10] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut. Conceptual 12m: Pushing web-scale\n    image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF\n    Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\n[11] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll\u00e1r, and C. L. Zitnick. Microsoft\n    coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\n[12] C. M. de Melo, A. Torralba, L. Guibas, J. DiCarlo, R. Chellappa, and J. Hodgins. Next-\n    generation deep learning based on simulators and synthetic data. Trends in cognitive sciences,\n    2021.\n[13] K. Desai and J. Johnson. Virtex: Learning visual representations from textual annotations. In\n    Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages\n    11162\u201311173, 2021.\n[14] K. Desai, G. Kaul, Z. Aysola, and J. Johnson. Redcaps: Web-curated image-text data created\n    by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.\n[15] A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt,\n    D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In\n    Proceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015.\n[16] L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian. Improving clip training with language\n    rewrites. arXiv preprint arXiv:2305.20088, 2023.\n[17] A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data\n    determines distributional robustness in contrastive language image pre-training (clip).      In\n    International Conference on Machine Learning, pages 6216\u20136234. PMLR, 2022.\n[18] S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman,\n    D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets.\n    arXiv preprint arXiv:2304.14108, 2023.\n[19] C. Gan, J. Schwartz, S. Alter, D. Mrowca, M. Schrimpf, J. Traer, J. De Freitas, J. Kubilius,\n    A. Bhandwaldar, N. Haber, et al. Threedworld: A platform for interactive multi-modal physical\n    simulation. arXiv preprint arXiv:2007.04954, 2020.\n[20] R. He, S. Sun, X. Yu, C. Xue, W. Zhang, P. Torr, S. Bai, and X. Qi. Is synthetic data from\n    generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022.\n[21] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi. Clipscore: A reference-free\n    evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\n[22] X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang. Scaling up vision-language\n                                                14", "md": "# References\n\n# List of References\n\n1. M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim. Coyo-700m: Image-text pair dataset.\nLink, 2022.\n2. L. Cao, B. Zhang, C. Chen, Y. Yang, X. Du, W. Zhang, Z. Lu, and Y. Zheng. Less is more: Removing text-regions improves clip training efficiency and robustness. arXiv preprint arXiv:2305.05095, 2023.\n3. S. Changpinyo, P. Sharma, N. Ding, and R. Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\n4. X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\n5. C. M. de Melo, A. Torralba, L. Guibas, J. DiCarlo, R. Chellappa, and J. Hodgins. Next-generation deep learning based on simulators and synthetic data. Trends in cognitive sciences, 2021.\n6. K. Desai and J. Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11162\u201311173, 2021.\n7. K. Desai, G. Kaul, Z. Aysola, and J. Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.\n8. A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015.\n9. L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian. Improving clip training with language rewrites. arXiv preprint arXiv:2305.20088, 2023.\n10. A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pages 6216\u20136234. PMLR, 2022.\n11. S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\n12. C. Gan, J. Schwartz, S. Alter, D. Mrowca, M. Schrimpf, J. Traer, J. De Freitas, J. Kubilius, A. Bhandwaldar, N. Haber, et al. Threedworld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.\n13. R. He, S. Sun, X. Yu, C. Xue, W. Zhang, P. Torr, S. Bai, and X. Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022.\n14. J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\n15. X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang. Scaling up vision-language\n$14$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "1. M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim. Coyo-700m: Image-text pair dataset.\nLink, 2022.\n2. L. Cao, B. Zhang, C. Chen, Y. Yang, X. Du, W. Zhang, Z. Lu, and Y. Zheng. Less is more: Removing text-regions improves clip training efficiency and robustness. arXiv preprint arXiv:2305.05095, 2023.\n3. S. Changpinyo, P. Sharma, N. Ding, and R. Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\n4. X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\n5. C. M. de Melo, A. Torralba, L. Guibas, J. DiCarlo, R. Chellappa, and J. Hodgins. Next-generation deep learning based on simulators and synthetic data. Trends in cognitive sciences, 2021.\n6. K. Desai and J. Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11162\u201311173, 2021.\n7. K. Desai, G. Kaul, Z. Aysola, and J. Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.\n8. A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015.\n9. L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian. Improving clip training with language rewrites. arXiv preprint arXiv:2305.20088, 2023.\n10. A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pages 6216\u20136234. PMLR, 2022.\n11. S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\n12. C. Gan, J. Schwartz, S. Alter, D. Mrowca, M. Schrimpf, J. Traer, J. De Freitas, J. Kubilius, A. Bhandwaldar, N. Haber, et al. Threedworld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.\n13. R. He, S. Sun, X. Yu, C. Xue, W. Zhang, P. Torr, S. Bai, and X. Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022.\n14. J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\n15. X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang. Scaling up vision-language\n$14$", "md": "1. M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim. Coyo-700m: Image-text pair dataset.\nLink, 2022.\n2. L. Cao, B. Zhang, C. Chen, Y. Yang, X. Du, W. Zhang, Z. Lu, and Y. Zheng. Less is more: Removing text-regions improves clip training efficiency and robustness. arXiv preprint arXiv:2305.05095, 2023.\n3. S. Changpinyo, P. Sharma, N. Ding, and R. Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\n4. X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015.\n5. C. M. de Melo, A. Torralba, L. Guibas, J. DiCarlo, R. Chellappa, and J. Hodgins. Next-generation deep learning based on simulators and synthetic data. Trends in cognitive sciences, 2021.\n6. K. Desai and J. Johnson. Virtex: Learning visual representations from textual annotations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11162\u201311173, 2021.\n7. K. Desai, G. Kaul, Z. Aysola, and J. Johnson. Redcaps: Web-curated image-text data created by the people, for the people. arXiv preprint arXiv:2111.11431, 2021.\n8. A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. Van Der Smagt, D. Cremers, and T. Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 2758\u20132766, 2015.\n9. L. Fan, D. Krishnan, P. Isola, D. Katabi, and Y. Tian. Improving clip training with language rewrites. arXiv preprint arXiv:2305.20088, 2023.\n10. A. Fang, G. Ilharco, M. Wortsman, Y. Wan, V. Shankar, A. Dave, and L. Schmidt. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pages 6216\u20136234. PMLR, 2022.\n11. S. Y. Gadre, G. Ilharco, A. Fang, J. Hayase, G. Smyrnis, T. Nguyen, R. Marten, M. Wortsman, D. Ghosh, J. Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023.\n12. C. Gan, J. Schwartz, S. Alter, D. Mrowca, M. Schrimpf, J. Traer, J. De Freitas, J. Kubilius, A. Bhandwaldar, N. Haber, et al. Threedworld: A platform for interactive multi-modal physical simulation. arXiv preprint arXiv:2007.04954, 2020.\n13. R. He, S. Sun, X. Yu, C. Xue, W. Zhang, P. Torr, S. Bai, and X. Qi. Is synthetic data from generative models ready for image recognition? arXiv preprint arXiv:2210.07574, 2022.\n14. J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\n15. X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, and L. Wang. Scaling up vision-language\n$14$"}]}, {"page": 15, "text": "     pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer\n    Vision and Pattern Recognition, pages 17980\u201317989, 2022.\n[23] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar,\n     H. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL\n     https://doi.org/10.5281/zenodo.5143773.\n[24] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and\n     T. Duerig.  Scaling up visual and vision-language representation learning with noisy text\n     supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\n[25] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick.\n     Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\n    Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910,\n     2017.\n[26] K. K\u00e4rkk\u00e4inen and J. Joo. Fairface: Face attribute dataset for balanced race, gender, and age.\n    arXiv preprint arXiv:1908.04913, 2019.\n[27] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions.\n     In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n     3128\u20133137, 2015.\n[28] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural language models. In International\n    conference on machine learning, pages 595\u2013603. PMLR, 2014.\n[29] J. Li, D. Li, C. Xiong, and S. Hoi.    Blip: Bootstrapping language-image pre-training for\n     unified vision-language understanding and generation. In International Conference on Machine\n    Learning, pages 12888\u201312900. PMLR, 2022.\n[30] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\n     frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[31] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar:\n     Object-semantics aligned pre-training for vision-language tasks. In Computer Vision\u2013ECCV\n    2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX\n    16, pages 121\u2013137. Springer, 2020.\n[32] P. Maini, S. Goyal, Z. C. Lipton, J. Z. Kolter, and A. Raghunathan. T-mars: Improving visual\n     representations by circumventing text feature learning. arXiv preprint arXiv:2307.03132, 2023.\n[33] T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt. Quality not quantity: On the\n     interaction between dataset design and robustness of clip. arXiv preprint arXiv:2208.05516,\n     2022.\n[34] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and\n     M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion\n     models. arXiv preprint arXiv:2112.10741, 2021.\n[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation\n     of machine translation.   In Proceedings of the 40th annual meeting of the Association for\n    Computational Linguistics, pages 311\u2013318, 2002.\n                                                 15", "md": "[23] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar,\nH. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL\nhttps://doi.org/10.5281/zenodo.5143773.\n[24] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and\nT. Duerig.  Scaling up visual and vision-language representation learning with noisy text\nsupervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\n[25] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick.\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910,\n2017.\n[26] K. K\u00e4rkk\u00e4inen and J. Joo. Fairface: Face attribute dataset for balanced race, gender, and age.\narXiv preprint arXiv:1908.04913, 2019.\n[27] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n3128\u20133137, 2015.\n[28] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural language models. In International\nconference on machine learning, pages 595\u2013603. PMLR, 2014.\n[29] J. Li, D. Li, C. Xiong, and S. Hoi.    Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022.\n[30] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[31] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar:\nObject-semantics aligned pre-training for vision-language tasks. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX\n16, pages 121\u2013137. Springer, 2020.\n[32] P. Maini, S. Goyal, Z. C. Lipton, J. Z. Kolter, and A. Raghunathan. T-mars: Improving visual\nrepresentations by circumventing text feature learning. arXiv preprint arXiv:2307.03132, 2023.\n[33] T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt. Quality not quantity: On the\ninteraction between dataset design and robustness of clip. arXiv preprint arXiv:2208.05516,\n2022.\n[34] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and\nM. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion\nmodels. arXiv preprint arXiv:2112.10741, 2021.\n[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation\nof machine translation.   In Proceedings of the 40th annual meeting of the Association for\nComputational Linguistics, pages 311\u2013318, 2002.", "images": [], "items": [{"type": "text", "value": "[23] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar,\nH. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL\nhttps://doi.org/10.5281/zenodo.5143773.\n[24] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and\nT. Duerig.  Scaling up visual and vision-language representation learning with noisy text\nsupervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\n[25] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick.\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910,\n2017.\n[26] K. K\u00e4rkk\u00e4inen and J. Joo. Fairface: Face attribute dataset for balanced race, gender, and age.\narXiv preprint arXiv:1908.04913, 2019.\n[27] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n3128\u20133137, 2015.\n[28] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural language models. In International\nconference on machine learning, pages 595\u2013603. PMLR, 2014.\n[29] J. Li, D. Li, C. Xiong, and S. Hoi.    Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022.\n[30] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[31] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar:\nObject-semantics aligned pre-training for vision-language tasks. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX\n16, pages 121\u2013137. Springer, 2020.\n[32] P. Maini, S. Goyal, Z. C. Lipton, J. Z. Kolter, and A. Raghunathan. T-mars: Improving visual\nrepresentations by circumventing text feature learning. arXiv preprint arXiv:2307.03132, 2023.\n[33] T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt. Quality not quantity: On the\ninteraction between dataset design and robustness of clip. arXiv preprint arXiv:2208.05516,\n2022.\n[34] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and\nM. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion\nmodels. arXiv preprint arXiv:2112.10741, 2021.\n[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation\nof machine translation.   In Proceedings of the 40th annual meeting of the Association for\nComputational Linguistics, pages 311\u2013318, 2002.", "md": "[23] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar,\nH. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. Openclip, July 2021. URL\nhttps://doi.org/10.5281/zenodo.5143773.\n[24] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and\nT. Duerig.  Scaling up visual and vision-language representation learning with noisy text\nsupervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\n[25] J. Johnson, B. Hariharan, L. Van Der Maaten, L. Fei-Fei, C. Lawrence Zitnick, and R. Girshick.\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 2901\u20132910,\n2017.\n[26] K. K\u00e4rkk\u00e4inen and J. Joo. Fairface: Face attribute dataset for balanced race, gender, and age.\narXiv preprint arXiv:1908.04913, 2019.\n[27] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n3128\u20133137, 2015.\n[28] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal neural language models. In International\nconference on machine learning, pages 595\u2013603. PMLR, 2014.\n[29] J. Li, D. Li, C. Xiong, and S. Hoi.    Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022.\n[30] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[31] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, et al. Oscar:\nObject-semantics aligned pre-training for vision-language tasks. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXX\n16, pages 121\u2013137. Springer, 2020.\n[32] P. Maini, S. Goyal, Z. C. Lipton, J. Z. Kolter, and A. Raghunathan. T-mars: Improving visual\nrepresentations by circumventing text feature learning. arXiv preprint arXiv:2307.03132, 2023.\n[33] T. Nguyen, G. Ilharco, M. Wortsman, S. Oh, and L. Schmidt. Quality not quantity: On the\ninteraction between dataset design and robustness of clip. arXiv preprint arXiv:2208.05516,\n2022.\n[34] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and\nM. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion\nmodels. arXiv preprint arXiv:2112.10741, 2021.\n[35] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation\nof machine translation.   In Proceedings of the 40th annual meeting of the Association for\nComputational Linguistics, pages 311\u2013318, 2002."}]}, {"page": 16, "text": "[36] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko. Visda: The visual\n     domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.\n[37] H. Pham, Z. Dai, G. Ghiasi, K. Kawaguchi, H. Liu, A. W. Yu, J. Yu, Y.-T. Chen, M.-T. Luong,\n     Y. Wu, et al. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050,\n     2021.\n[38] G. Puccetti, M. Kilian, and R. Beaumont.         Training contrastive captioners.     URL https:\n     //laion.ai/blog/coca/.\n[39] F. Radenovic, A. Dubey, A. Kadian, T. Mihaylov, S. Vandenhende, Y. Patel, Y. Wen, V. Ra-\n     manathan, and D. Mahajan. Filtering, distillation, and hard negatives for vision-language\n     pre-training. arXiv preprint arXiv:2301.02280, 2023.\n[40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\n     P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\n     In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n[41] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from\n     computer games. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\n     The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 102\u2013118. Springer, 2016.\n[42] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\n     with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision\n     and Pattern Recognition, pages 10684\u201310695, 2022.\n[43] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-\n     tijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models\n     with deep language understanding. Advances in Neural Information Processing Systems, 35:\n     36479\u201336494, 2022.\n[44] S. Santurkar, Y. Dubois, R. Taori, P. Liang, and T. Hashimoto. Is a caption worth a thousand\n     images? a controlled study for representation learning. arXiv preprint arXiv:2207.07635, 2022.\n[45] C. Schuhmann, A. K\u00f6pf, R. Vencu, T. Coombes, and R. Beaumont. Laion coco: 600m synthetic\n     captions from laion2b-en. URL https://laion.ai/blog/laion-coco/.\n[46] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\n     A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training\n     next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n[47] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed,\n     image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\n     Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n     2556\u20132565, 2018.\n[48] I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. Model dementia:\n     Generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.\n[49] H. Tan and M. Bansal. Vokenization: Improving language understanding with contextualized,\n     visual-grounded supervision. arXiv preprint arXiv:2010.06775, 2020.\n                                                   16", "md": "- [36] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.\n- [37] H. Pham, Z. Dai, G. Ghiasi, K. Kawaguchi, H. Liu, A. W. Yu, J. Yu, Y.-T. Chen, M.-T. Luong, Y. Wu, et al. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050, 2021.\n- [38] G. Puccetti, M. Kilian, and R. Beaumont. Training contrastive captioners. URL https://laion.ai/blog/coca/.\n- [39] F. Radenovic, A. Dubey, A. Kadian, T. Mihaylov, S. Vandenhende, Y. Patel, Y. Wen, V. Ramanathan, and D. Mahajan. Filtering, distillation, and hard negatives for vision-language pre-training. arXiv preprint arXiv:2301.02280, 2023.\n- [40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n- [41] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 102\u2013118. Springer, 2016.\n- [42] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n- [43] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35: 36479\u201336494, 2022.\n- [44] S. Santurkar, Y. Dubois, R. Taori, P. Liang, and T. Hashimoto. Is a caption worth a thousand images? a controlled study for representation learning. arXiv preprint arXiv:2207.07635, 2022.\n- [45] C. Schuhmann, A. K\u00f6pf, R. Vencu, T. Coombes, and R. Beaumont. Laion coco: 600m synthetic captions from laion2b-en. URL https://laion.ai/blog/laion-coco/.\n- [46] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n- [47] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.\n- [48] I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. Model dementia: Generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.\n- [49] H. Tan and M. Bansal. Vokenization: Improving language understanding with contextualized, visual-grounded supervision. arXiv preprint arXiv:2010.06775, 2020.", "images": [], "items": [{"type": "text", "value": "- [36] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.\n- [37] H. Pham, Z. Dai, G. Ghiasi, K. Kawaguchi, H. Liu, A. W. Yu, J. Yu, Y.-T. Chen, M.-T. Luong, Y. Wu, et al. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050, 2021.\n- [38] G. Puccetti, M. Kilian, and R. Beaumont. Training contrastive captioners. URL https://laion.ai/blog/coca/.\n- [39] F. Radenovic, A. Dubey, A. Kadian, T. Mihaylov, S. Vandenhende, Y. Patel, Y. Wen, V. Ramanathan, and D. Mahajan. Filtering, distillation, and hard negatives for vision-language pre-training. arXiv preprint arXiv:2301.02280, 2023.\n- [40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n- [41] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 102\u2013118. Springer, 2016.\n- [42] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n- [43] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35: 36479\u201336494, 2022.\n- [44] S. Santurkar, Y. Dubois, R. Taori, P. Liang, and T. Hashimoto. Is a caption worth a thousand images? a controlled study for representation learning. arXiv preprint arXiv:2207.07635, 2022.\n- [45] C. Schuhmann, A. K\u00f6pf, R. Vencu, T. Coombes, and R. Beaumont. Laion coco: 600m synthetic captions from laion2b-en. URL https://laion.ai/blog/laion-coco/.\n- [46] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n- [47] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.\n- [48] I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. Model dementia: Generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.\n- [49] H. Tan and M. Bansal. Vokenization: Improving language understanding with contextualized, visual-grounded supervision. arXiv preprint arXiv:2010.06775, 2020.", "md": "- [36] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.\n- [37] H. Pham, Z. Dai, G. Ghiasi, K. Kawaguchi, H. Liu, A. W. Yu, J. Yu, Y.-T. Chen, M.-T. Luong, Y. Wu, et al. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050, 2021.\n- [38] G. Puccetti, M. Kilian, and R. Beaumont. Training contrastive captioners. URL https://laion.ai/blog/coca/.\n- [39] F. Radenovic, A. Dubey, A. Kadian, T. Mihaylov, S. Vandenhende, Y. Patel, Y. Wen, V. Ramanathan, and D. Mahajan. Filtering, distillation, and hard negatives for vision-language pre-training. arXiv preprint arXiv:2301.02280, 2023.\n- [40] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n- [41] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 102\u2013118. Springer, 2016.\n- [42] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n- [43] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35: 36479\u201336494, 2022.\n- [44] S. Santurkar, Y. Dubois, R. Taori, P. Liang, and T. Hashimoto. Is a caption worth a thousand images? a controlled study for representation learning. arXiv preprint arXiv:2207.07635, 2022.\n- [45] C. Schuhmann, A. K\u00f6pf, R. Vencu, T. Coombes, and R. Beaumont. Laion coco: 600m synthetic captions from laion2b-en. URL https://laion.ai/blog/laion-coco/.\n- [46] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n- [47] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, 2018.\n- [48] I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, and R. Anderson. Model dementia: Generated data makes models forget. arXiv preprint arXiv:2305.17493, 2023.\n- [49] H. Tan and M. Bansal. Vokenization: Improving language understanding with contextualized, visual-grounded supervision. arXiv preprint arXiv:2010.06775, 2020."}]}, {"page": 17, "text": "[50] R. Taori and T. B. Hashimoto. Data feedback loops: Model-driven amplification of dataset\n     biases. arXiv preprint arXiv:2209.03942, 2022.\n[51] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description\n     evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition,\n     pages 4566\u20134575, 2015.\n[52] P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Ofa:\n     Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning\n     framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.\n[53] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. Simvlm: Simple visual language\n     model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.\n[54] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations:\n     New similarity metrics for semantic inference over event descriptions. Transactions of the\n    Association for Computational Linguistics, 2:67\u201378, 2014.\n[55] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive\n     captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\n[56] J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou. Structured3d: A large photo-realistic\n     dataset for structured 3d modeling. In Computer Vision\u2013ECCV 2020: 16th European Conference,\n    Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX 16, pages 519\u2013535. Springer, 2020.\n[57] D. Zhu, J. Chen, K. Haydarov, X. Shen, W. Zhang, and M. Elhoseiny.              Chatgpt asks,\n     blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint\n    arXiv:2303.06594, 2023.\n                                                 17", "md": "# References\n\n## References\n\n1. R. Taori and T. B. Hashimoto. Data feedback loops: Model-driven amplification of dataset biases. arXiv preprint arXiv:2209.03942, 2022.\n2. R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.\n3. P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.\n4. Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.\n5. P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.\n6. J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\n7. J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou. Structured3d: A large photo-realistic dataset for structured 3d modeling. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX 16, pages 519\u2013535. Springer, 2020.\n8. D. Zhu, J. Chen, K. Haydarov, X. Shen, W. Zhang, and M. Elhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint arXiv:2303.06594, 2023.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. R. Taori and T. B. Hashimoto. Data feedback loops: Model-driven amplification of dataset biases. arXiv preprint arXiv:2209.03942, 2022.\n2. R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.\n3. P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.\n4. Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.\n5. P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.\n6. J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\n7. J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou. Structured3d: A large photo-realistic dataset for structured 3d modeling. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX 16, pages 519\u2013535. Springer, 2020.\n8. D. Zhu, J. Chen, K. Haydarov, X. Shen, W. Zhang, and M. Elhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint arXiv:2303.06594, 2023.", "md": "1. R. Taori and T. B. Hashimoto. Data feedback loops: Model-driven amplification of dataset biases. arXiv preprint arXiv:2209.03942, 2022.\n2. R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4566\u20134575, 2015.\n3. P. Wang, A. Yang, R. Men, J. Lin, S. Bai, Z. Li, J. Ma, C. Zhou, J. Zhou, and H. Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340. PMLR, 2022.\n4. Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021.\n5. P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:67\u201378, 2014.\n6. J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\n7. J. Zheng, J. Zhang, J. Li, R. Tang, S. Gao, and Z. Zhou. Structured3d: A large photo-realistic dataset for structured 3d modeling. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IX 16, pages 519\u2013535. Springer, 2020.\n8. D. Zhu, J. Chen, K. Haydarov, X. Shen, W. Zhang, and M. Elhoseiny. Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions. arXiv preprint arXiv:2303.06594, 2023."}]}, {"page": 18, "text": "A        More examples of image-text pairs (no cherry picking)\n                                                     Raw: 2003 Mercedes-Benz C240 sedan, Leather, MUST BE\n                                                    SEEN - $6199\n                                                     BLIP (finetuned): a couple of cars parked in a parking lot with\n                                                    trees and cars\n                                                     BLIP2: 2002 mercedes-benz c-class for sale\n                                                     BLIP2 (finetuned): a blue mercedes benz car parked in a parking\n                                                    lot next to yellow cars\n                                                     OpenCLIP-CoCa: find used 2 0 0 1 mercedes benz c 2 4 0 base\n                                                    sedan 4 door 2 5 l for 2 0 0 1 mercedes benz c 2\n                                                     OpenCLIP-CoCa (finetuned): a blue mercedes parked on the\nside of a road .\n                                                     Raw: Gaziburma \u00dcnal is one of Gespeicherte Orte von Can.\n                                                     BLIP (finetuned): dozens of trays of different types of treats at\n                                                    a food stand\n                                                     BLIP2: some trays of pastries and a knife\n                                                     BLIP2 (finetuned): there are many trays filled with food items\nfrom the store\nOpenCLIP-CoCa: baklava , sweets , pastries\nOpenCLIP-CoCa (finetuned): there are trays full of different types of food .\n                                                     Raw: Open Virgin of Kazan, Wooden Egg with Stand, Blue\n      NORE INFO                                      BLIP (finetuned): a gray and white logo with the words more\n                                                    info in a rectangular shape\n                                                     BLIP2: a white button with the word more info\nBLIP2 (finetuned): more information is shown on a white button with an orange background\nOpenCLIP-CoCa: home - page - button . png\nOpenCLIP-CoCa (finetuned): a picture of a close up of a text message\n                                                     Raw: hair oil\n                                                     BLIP (finetuned): smiling blonde woman blow drying hair in\n                                                    a salon while getting a mani\n                                                     BLIP2:          hair     stylist    using      hair     spray     in   beauty      sa-\n                                                    lon\n                                                     BLIP2 (finetuned): a person is using a hairdryer to blow dry\n                                                    a long blonde hair\n                                                     OpenCLIP-CoCa:               female hairdresser styling a long blond\n                                                    hair     with     hairspray        in   a    beauty      salon     .      concept       :\n                                                    hair     care     ,   hair     straightening        ,   hair     color    correction\n                                                     .\n                                                     OpenCLIP-CoCa (finetuned): a person is spraying a hair dryer\n                                                    on a long blonde hair .\n                                                                     18", "md": "# Image-Text Pairs Examples\n\n## More examples of image-text pairs (no cherry picking)\n\n|Raw:|2003 Mercedes-Benz C240 sedan, Leather, MUST BE SEEN - $6199|\n|---|---|\n|BLIP (finetuned):|a couple of cars parked in a parking lot with trees and cars|\n|BLIP2:|2002 mercedes-benz c-class for sale|\n|BLIP2 (finetuned):|a blue mercedes benz car parked in a parking lot next to yellow cars|\n|OpenCLIP-CoCa:|find used 2001 mercedes benz c240 base sedan 4 door 2.5L for 2001 mercedes benz c|\n|OpenCLIP-CoCa (finetuned):|a blue mercedes parked on the side of a road.|\n\n|Raw:|Gaziburma \u00dcnal is one of Gespeicherte Orte von Can.|\n|---|---|\n|BLIP (finetuned):|dozens of trays of different types of treats at a food stand|\n|BLIP2:|some trays of pastries and a knife|\n|BLIP2 (finetuned):|there are many trays filled with food items from the store|\n|OpenCLIP-CoCa:|baklava, sweets, pastries|\n|OpenCLIP-CoCa (finetuned):|there are trays full of different types of food.|\n\n|Raw:|Open Virgin of Kazan, Wooden Egg with Stand, Blue|\n|---|---|\n|NORE INFO| |\n|BLIP (finetuned):|a gray and white logo with the words more info in a rectangular shape|\n|BLIP2:|a white button with the word more info|\n|BLIP2 (finetuned):|more information is shown on a white button with an orange background|\n|OpenCLIP-CoCa:|home - page - button . png|\n|OpenCLIP-CoCa (finetuned):|a picture of a close up of a text message|\n\n|Raw:|hair oil|\n|---|---|\n|BLIP (finetuned):|smiling blonde woman blow drying hair in a salon while getting a mani|\n|BLIP2:|hair stylist using hair spray in beauty salon|\n|BLIP2 (finetuned):|a person is using a hairdryer to blow dry a long blonde hair|\n|OpenCLIP-CoCa:|female hairdresser styling a long blond hair with hairspray in a beauty salon. concept: hair care, hair straightening, hair color correction.|\n|OpenCLIP-CoCa (finetuned):|a person is spraying a hair dryer on a long blonde hair.|", "images": [{"name": "page-18-2.jpg", "height": 39, "width": 160, "x": 74, "y": 385}, {"name": "page-18-0.jpg", "height": 119, "width": 160, "x": 74, "y": 101}, {"name": "page-18-1.jpg", "height": 61, "width": 160, "x": 74, "y": 254}, {"name": "page-18-3.jpg", "height": 160, "width": 160, "x": 74, "y": 498}], "items": [{"type": "heading", "lvl": 1, "value": "Image-Text Pairs Examples", "md": "# Image-Text Pairs Examples"}, {"type": "heading", "lvl": 2, "value": "More examples of image-text pairs (no cherry picking)", "md": "## More examples of image-text pairs (no cherry picking)"}, {"type": "table", "rows": [["Raw:", "2003 Mercedes-Benz C240 sedan, Leather, MUST BE SEEN - $6199"], ["BLIP (finetuned):", "a couple of cars parked in a parking lot with trees and cars"], ["BLIP2:", "2002 mercedes-benz c-class for sale"], ["BLIP2 (finetuned):", "a blue mercedes benz car parked in a parking lot next to yellow cars"], ["OpenCLIP-CoCa:", "find used 2001 mercedes benz c240 base sedan 4 door 2.5L for 2001 mercedes benz c"], ["OpenCLIP-CoCa (finetuned):", "a blue mercedes parked on the side of a road."]], "md": "|Raw:|2003 Mercedes-Benz C240 sedan, Leather, MUST BE SEEN - $6199|\n|---|---|\n|BLIP (finetuned):|a couple of cars parked in a parking lot with trees and cars|\n|BLIP2:|2002 mercedes-benz c-class for sale|\n|BLIP2 (finetuned):|a blue mercedes benz car parked in a parking lot next to yellow cars|\n|OpenCLIP-CoCa:|find used 2001 mercedes benz c240 base sedan 4 door 2.5L for 2001 mercedes benz c|\n|OpenCLIP-CoCa (finetuned):|a blue mercedes parked on the side of a road.|", "isPerfectTable": true, "csv": "\"Raw:\",\"2003 Mercedes-Benz C240 sedan, Leather, MUST BE SEEN - $6199\"\n\"BLIP (finetuned):\",\"a couple of cars parked in a parking lot with trees and cars\"\n\"BLIP2:\",\"2002 mercedes-benz c-class for sale\"\n\"BLIP2 (finetuned):\",\"a blue mercedes benz car parked in a parking lot next to yellow cars\"\n\"OpenCLIP-CoCa:\",\"find used 2001 mercedes benz c240 base sedan 4 door 2.5L for 2001 mercedes benz c\"\n\"OpenCLIP-CoCa (finetuned):\",\"a blue mercedes parked on the side of a road.\""}, {"type": "table", "rows": [["Raw:", "Gaziburma \u00dcnal is one of Gespeicherte Orte von Can."], ["BLIP (finetuned):", "dozens of trays of different types of treats at a food stand"], ["BLIP2:", "some trays of pastries and a knife"], ["BLIP2 (finetuned):", "there are many trays filled with food items from the store"], ["OpenCLIP-CoCa:", "baklava, sweets, pastries"], ["OpenCLIP-CoCa (finetuned):", "there are trays full of different types of food."]], "md": "|Raw:|Gaziburma \u00dcnal is one of Gespeicherte Orte von Can.|\n|---|---|\n|BLIP (finetuned):|dozens of trays of different types of treats at a food stand|\n|BLIP2:|some trays of pastries and a knife|\n|BLIP2 (finetuned):|there are many trays filled with food items from the store|\n|OpenCLIP-CoCa:|baklava, sweets, pastries|\n|OpenCLIP-CoCa (finetuned):|there are trays full of different types of food.|", "isPerfectTable": true, "csv": "\"Raw:\",\"Gaziburma \u00dcnal is one of Gespeicherte Orte von Can.\"\n\"BLIP (finetuned):\",\"dozens of trays of different types of treats at a food stand\"\n\"BLIP2:\",\"some trays of pastries and a knife\"\n\"BLIP2 (finetuned):\",\"there are many trays filled with food items from the store\"\n\"OpenCLIP-CoCa:\",\"baklava, sweets, pastries\"\n\"OpenCLIP-CoCa (finetuned):\",\"there are trays full of different types of food.\""}, {"type": "table", "rows": [["Raw:", "Open Virgin of Kazan, Wooden Egg with Stand, Blue"], ["NORE INFO", ""], ["BLIP (finetuned):", "a gray and white logo with the words more info in a rectangular shape"], ["BLIP2:", "a white button with the word more info"], ["BLIP2 (finetuned):", "more information is shown on a white button with an orange background"], ["OpenCLIP-CoCa:", "home - page - button . png"], ["OpenCLIP-CoCa (finetuned):", "a picture of a close up of a text message"]], "md": "|Raw:|Open Virgin of Kazan, Wooden Egg with Stand, Blue|\n|---|---|\n|NORE INFO| |\n|BLIP (finetuned):|a gray and white logo with the words more info in a rectangular shape|\n|BLIP2:|a white button with the word more info|\n|BLIP2 (finetuned):|more information is shown on a white button with an orange background|\n|OpenCLIP-CoCa:|home - page - button . png|\n|OpenCLIP-CoCa (finetuned):|a picture of a close up of a text message|", "isPerfectTable": true, "csv": "\"Raw:\",\"Open Virgin of Kazan, Wooden Egg with Stand, Blue\"\n\"NORE INFO\",\"\"\n\"BLIP (finetuned):\",\"a gray and white logo with the words more info in a rectangular shape\"\n\"BLIP2:\",\"a white button with the word more info\"\n\"BLIP2 (finetuned):\",\"more information is shown on a white button with an orange background\"\n\"OpenCLIP-CoCa:\",\"home - page - button . png\"\n\"OpenCLIP-CoCa (finetuned):\",\"a picture of a close up of a text message\""}, {"type": "table", "rows": [["Raw:", "hair oil"], ["BLIP (finetuned):", "smiling blonde woman blow drying hair in a salon while getting a mani"], ["BLIP2:", "hair stylist using hair spray in beauty salon"], ["BLIP2 (finetuned):", "a person is using a hairdryer to blow dry a long blonde hair"], ["OpenCLIP-CoCa:", "female hairdresser styling a long blond hair with hairspray in a beauty salon. concept: hair care, hair straightening, hair color correction."], ["OpenCLIP-CoCa (finetuned):", "a person is spraying a hair dryer on a long blonde hair."]], "md": "|Raw:|hair oil|\n|---|---|\n|BLIP (finetuned):|smiling blonde woman blow drying hair in a salon while getting a mani|\n|BLIP2:|hair stylist using hair spray in beauty salon|\n|BLIP2 (finetuned):|a person is using a hairdryer to blow dry a long blonde hair|\n|OpenCLIP-CoCa:|female hairdresser styling a long blond hair with hairspray in a beauty salon. concept: hair care, hair straightening, hair color correction.|\n|OpenCLIP-CoCa (finetuned):|a person is spraying a hair dryer on a long blonde hair.|", "isPerfectTable": true, "csv": "\"Raw:\",\"hair oil\"\n\"BLIP (finetuned):\",\"smiling blonde woman blow drying hair in a salon while getting a mani\"\n\"BLIP2:\",\"hair stylist using hair spray in beauty salon\"\n\"BLIP2 (finetuned):\",\"a person is using a hairdryer to blow dry a long blonde hair\"\n\"OpenCLIP-CoCa:\",\"female hairdresser styling a long blond hair with hairspray in a beauty salon. concept: hair care, hair straightening, hair color correction.\"\n\"OpenCLIP-CoCa (finetuned):\",\"a person is spraying a hair dryer on a long blonde hair.\""}]}, {"page": 19, "text": "                                                     Raw: Italien - Ligurien\n                                                     BLIP (finetuned): beige colored building with tan accents and\n                                                     palm trees on both sides of walkway\n                                                     BLIP2: house in villa marina, a villa with many rooms and\n                                                     palm trees\n                                                     BLIP2 (finetuned): a park with lots of trees and benches in\n                                                     front of a large building\n                                                     OpenCLIP-CoCa: residence - villa - maria - di - san - giovanni\n                                                    - near - the - sea - in - taormina\nOpenCLIP-CoCa (finetuned): a picture of a large building with a bunch of palm trees .\n                                                     Raw: 3 formas de pedir la mano de tu novia - wikiHow\n                                                     BLIP (finetuned): crates stacked up in a pile on top of each\n                                                    other\n                                                     BLIP2: the building contains five floors of wooden planks\n                                                     BLIP2 (finetuned): a big pile of wooden planks stacked together\n                                                     OpenCLIP-CoCa: the cost of wood pallets\n                                                     OpenCLIP-CoCa (finetuned): a large pile of wooden pallets\nmounted to a wall .\n                                                     Raw: lutz\n                                                     BLIP (finetuned): blond haired man in black suit looking at\n                                                    camera\n                                                     BLIP2: a man sitting on a chair with a blank background\n                                                     BLIP2 (finetuned): a man sitting in a chair with a lapel button\n                                                    in front\n                                                     OpenCLIP-CoCa: actor tilda swinton is pictured during a press\n                                                    conference for the film \u2019 a dangerous method \u2019 at the 2 0 1 1\n                                                    toronto film festival\nOpenCLIP-CoCa (finetuned): a person sitting on a chair wearing a suit and tie .\n                                                     Raw: image8.JPG\n                                                     BLIP (finetuned): members of a school play soccer in a gym-\n                                                    nasium with a crowd\n                                                     BLIP2:        a large crowd of kids perform during a dance\n                                                    show\n                                                     BLIP2 (finetuned): a group of young children standing on the\n                                                    basketball court\n                                                     OpenCLIP-CoCa: kid dressed in white standing in a gym area\n                                                     .\n                                                     OpenCLIP-CoCa (finetuned): a group of kids on the gym floor\n                                                    with fans on the floor .\n                                                                     19", "md": "# OCR Text\n\nRaw: Italien - Ligurien\n\n- BLIP (finetuned): beige colored building with tan accents and palm trees on both sides of walkway\n- BLIP2: house in villa marina, a villa with many rooms and palm trees\n- BLIP2 (finetuned): a park with lots of trees and benches in front of a large building\n- OpenCLIP-CoCa: residence - villa - maria - di - san - giovanni - near - the - sea - in - taormina\n- OpenCLIP-CoCa (finetuned): $a\\ picture\\ of\\ a\\ large\\ building\\ with\\ a\\ bunch\\ of\\ palm\\ trees.$\n\nRaw: 3 formas de pedir la mano de tu novia - wikiHow\n\n- BLIP (finetuned): crates stacked up in a pile on top of each other\n- BLIP2: the building contains five floors of wooden planks\n- BLIP2 (finetuned): a big pile of wooden planks stacked together\n- OpenCLIP-CoCa: the cost of wood pallets\n- OpenCLIP-CoCa (finetuned): $a\\ large\\ pile\\ of\\ wooden\\ pallets\\ mounted\\ to\\ a\\ wall.$\n\nRaw: lutz\n\n- BLIP (finetuned): blond haired man in black suit looking at camera\n- BLIP2: a man sitting on a chair with a blank background\n- BLIP2 (finetuned): a man sitting in a chair with a lapel button in front\n- OpenCLIP-CoCa: actor tilda swinton is pictured during a press conference for the film \u2019 a dangerous method \u2019 at the 2 0 1 1 toronto film festival\n- OpenCLIP-CoCa (finetuned): $a\\ person\\ sitting\\ on\\ a\\ chair\\ wearing\\ a\\ suit\\ and\\ tie.$\n\nRaw: image8.JPG\n\n- BLIP (finetuned): members of a school play soccer in a gymnasium with a crowd\n- BLIP2: a large crowd of kids perform during a dance show\n- BLIP2 (finetuned): a group of young children standing on the basketball court\n- OpenCLIP-CoCa: kid dressed in white standing in a gym area\n- OpenCLIP-CoCa (finetuned): $a\\ group\\ of\\ kids\\ on\\ the\\ gym\\ floor\\ with\\ fans\\ on\\ the\\ floor.$", "images": [{"name": "page-19-0.jpg", "height": 120, "width": 160, "x": 74, "y": 70}, {"name": "page-19-1.jpg", "height": 90, "width": 160, "x": 74, "y": 230}, {"name": "page-19-2.jpg", "height": 111, "width": 160, "x": 74, "y": 351}, {"name": "page-19-3.jpg", "height": 206, "width": 155, "x": 76, "y": 498}], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "text", "value": "Raw: Italien - Ligurien\n\n- BLIP (finetuned): beige colored building with tan accents and palm trees on both sides of walkway\n- BLIP2: house in villa marina, a villa with many rooms and palm trees\n- BLIP2 (finetuned): a park with lots of trees and benches in front of a large building\n- OpenCLIP-CoCa: residence - villa - maria - di - san - giovanni - near - the - sea - in - taormina\n- OpenCLIP-CoCa (finetuned): $a\\ picture\\ of\\ a\\ large\\ building\\ with\\ a\\ bunch\\ of\\ palm\\ trees.$\n\nRaw: 3 formas de pedir la mano de tu novia - wikiHow\n\n- BLIP (finetuned): crates stacked up in a pile on top of each other\n- BLIP2: the building contains five floors of wooden planks\n- BLIP2 (finetuned): a big pile of wooden planks stacked together\n- OpenCLIP-CoCa: the cost of wood pallets\n- OpenCLIP-CoCa (finetuned): $a\\ large\\ pile\\ of\\ wooden\\ pallets\\ mounted\\ to\\ a\\ wall.$\n\nRaw: lutz\n\n- BLIP (finetuned): blond haired man in black suit looking at camera\n- BLIP2: a man sitting on a chair with a blank background\n- BLIP2 (finetuned): a man sitting in a chair with a lapel button in front\n- OpenCLIP-CoCa: actor tilda swinton is pictured during a press conference for the film \u2019 a dangerous method \u2019 at the 2 0 1 1 toronto film festival\n- OpenCLIP-CoCa (finetuned): $a\\ person\\ sitting\\ on\\ a\\ chair\\ wearing\\ a\\ suit\\ and\\ tie.$\n\nRaw: image8.JPG\n\n- BLIP (finetuned): members of a school play soccer in a gymnasium with a crowd\n- BLIP2: a large crowd of kids perform during a dance show\n- BLIP2 (finetuned): a group of young children standing on the basketball court\n- OpenCLIP-CoCa: kid dressed in white standing in a gym area\n- OpenCLIP-CoCa (finetuned): $a\\ group\\ of\\ kids\\ on\\ the\\ gym\\ floor\\ with\\ fans\\ on\\ the\\ floor.$", "md": "Raw: Italien - Ligurien\n\n- BLIP (finetuned): beige colored building with tan accents and palm trees on both sides of walkway\n- BLIP2: house in villa marina, a villa with many rooms and palm trees\n- BLIP2 (finetuned): a park with lots of trees and benches in front of a large building\n- OpenCLIP-CoCa: residence - villa - maria - di - san - giovanni - near - the - sea - in - taormina\n- OpenCLIP-CoCa (finetuned): $a\\ picture\\ of\\ a\\ large\\ building\\ with\\ a\\ bunch\\ of\\ palm\\ trees.$\n\nRaw: 3 formas de pedir la mano de tu novia - wikiHow\n\n- BLIP (finetuned): crates stacked up in a pile on top of each other\n- BLIP2: the building contains five floors of wooden planks\n- BLIP2 (finetuned): a big pile of wooden planks stacked together\n- OpenCLIP-CoCa: the cost of wood pallets\n- OpenCLIP-CoCa (finetuned): $a\\ large\\ pile\\ of\\ wooden\\ pallets\\ mounted\\ to\\ a\\ wall.$\n\nRaw: lutz\n\n- BLIP (finetuned): blond haired man in black suit looking at camera\n- BLIP2: a man sitting on a chair with a blank background\n- BLIP2 (finetuned): a man sitting in a chair with a lapel button in front\n- OpenCLIP-CoCa: actor tilda swinton is pictured during a press conference for the film \u2019 a dangerous method \u2019 at the 2 0 1 1 toronto film festival\n- OpenCLIP-CoCa (finetuned): $a\\ person\\ sitting\\ on\\ a\\ chair\\ wearing\\ a\\ suit\\ and\\ tie.$\n\nRaw: image8.JPG\n\n- BLIP (finetuned): members of a school play soccer in a gymnasium with a crowd\n- BLIP2: a large crowd of kids perform during a dance show\n- BLIP2 (finetuned): a group of young children standing on the basketball court\n- OpenCLIP-CoCa: kid dressed in white standing in a gym area\n- OpenCLIP-CoCa (finetuned): $a\\ group\\ of\\ kids\\ on\\ the\\ gym\\ floor\\ with\\ fans\\ on\\ the\\ floor.$"}]}, {"page": 20, "text": "Raw: Nautica NAPTYR005\nBLIP (finetuned): navitta mens stainless steel bracelet watch\nwith blue dial\nBLIP2: nautica men\u2019s chronograph watch\nBLIP2 (finetuned): nautica men\u2019s men\u2019s chronograph black dial\nstainless steel bracelet watch\nOpenCLIP-CoCa: nautica newport chronograph n 2 2 0 0 3 g\nOpenCLIP-CoCa (finetuned): a mans black watch is shown\nwith red and blue accents\nRaw: Women Personality Creative Christmas Hat Face Expres-\nsion Gold Earring Funny Cartoon Ear Stud Jewelry Accessories\nGift Hot\nBLIP (finetuned): red and gold tone emojt earring\nBLIP2: kawaii santa emoticuos en la cabeza\nBLIP2 (finetuned): a pair of emoji earrings with faces and hats\nOpenCLIP-CoCa: best christmas gift for her new arrivals emoji\nearrings christmas emoji earrings\nOpenCLIP-CoCa (finetuned): a pair of gold earrings with a\nsmiley face and a christmas hat .\nRaw: 10840 SW 126th St photo067\nBLIP (finetuned): overview of a large backyard with a swimming\npool and patio\nBLIP2: 3344 sw 7th st, palm beach\nBLIP2 (finetuned): a house with a pool from above, with a yard\nOpenCLIP-CoCa: home for sale in country club shores west\npalm beach florida\nOpenCLIP-CoCa (finetuned): aerial image of a pool that has\na little bit of shade by the side .\nRaw:      Greenberg    Weathered     Marble     Plush   Ivory   Area\nRug\nBLIP (finetuned): grey rug with a text home on it by a ta-\nble\nBLIP2: a grey area rug on a wooden floor\nBLIP2     (finetuned):     a   white   coffee  table   with   a  sign\nsaying home on it.          it is sitting on a cream colored\nrug\nOpenCLIP-CoCa: rugs and carpets in hyderabad : buy online\nat best price in ...\nOpenCLIP-CoCa (finetuned): a rug is shown in a living room\nwith a chair .\n             20", "md": "# OCR Text\n\n## Raw: Nautica NAPTYR005\n\nBLIP (finetuned): navitta mens stainless steel bracelet watch with blue dial\n\nBLIP2: nautica men\u2019s chronograph watch\n\nBLIP2 (finetuned): nautica men\u2019s men\u2019s chronograph black dial stainless steel bracelet watch\n\nOpenCLIP-CoCa: nautica newport chronograph n 2 2 0 0 3 g\n\nOpenCLIP-CoCa (finetuned): a mans black watch is shown with red and blue accents\n\n## Raw: Women Personality Creative Christmas Hat Face Expression Gold Earring Funny Cartoon Ear Stud Jewelry Accessories Gift Hot\n\nBLIP (finetuned): red and gold tone emojt earring\n\nBLIP2: kawaii santa emoticuos en la cabeza\n\nBLIP2 (finetuned): a pair of emoji earrings with faces and hats\n\nOpenCLIP-CoCa: best christmas gift for her new arrivals emoji earrings christmas emoji earrings\n\nOpenCLIP-CoCa (finetuned): a pair of gold earrings with a smiley face and a Christmas hat\n\n## Raw: 10840 SW 126th St photo067\n\nBLIP (finetuned): overview of a large backyard with a swimming pool and patio\n\nBLIP2: 3344 SW 7th St, Palm Beach\n\nBLIP2 (finetuned): a house with a pool from above, with a yard\n\nOpenCLIP-CoCa: home for sale in Country Club Shores West Palm Beach Florida\n\nOpenCLIP-CoCa (finetuned): aerial image of a pool that has a little bit of shade by the side\n\n## Raw: Greenberg Weathered Marble Plush Ivory Area Rug\n\nBLIP (finetuned): grey rug with a text home on it by a table\n\nBLIP2: a grey area rug on a wooden floor\n\nBLIP2 (finetuned): a white coffee table with a sign saying home on it. It is sitting on a cream colored rug\n\nOpenCLIP-CoCa: rugs and carpets in Hyderabad: buy online at best price in ...\n\nOpenCLIP-CoCa (finetuned): a rug is shown in a living room with a chair", "images": [{"name": "page-20-1.jpg", "height": 106, "width": 160, "x": 74, "y": 399}, {"name": "page-20-2.jpg", "height": 160, "width": 160, "x": 74, "y": 531}, {"name": "page-20-0.jpg", "height": 321, "width": 160, "x": 74, "y": 56}], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "heading", "lvl": 2, "value": "Raw: Nautica NAPTYR005", "md": "## Raw: Nautica NAPTYR005"}, {"type": "text", "value": "BLIP (finetuned): navitta mens stainless steel bracelet watch with blue dial\n\nBLIP2: nautica men\u2019s chronograph watch\n\nBLIP2 (finetuned): nautica men\u2019s men\u2019s chronograph black dial stainless steel bracelet watch\n\nOpenCLIP-CoCa: nautica newport chronograph n 2 2 0 0 3 g\n\nOpenCLIP-CoCa (finetuned): a mans black watch is shown with red and blue accents", "md": "BLIP (finetuned): navitta mens stainless steel bracelet watch with blue dial\n\nBLIP2: nautica men\u2019s chronograph watch\n\nBLIP2 (finetuned): nautica men\u2019s men\u2019s chronograph black dial stainless steel bracelet watch\n\nOpenCLIP-CoCa: nautica newport chronograph n 2 2 0 0 3 g\n\nOpenCLIP-CoCa (finetuned): a mans black watch is shown with red and blue accents"}, {"type": "heading", "lvl": 2, "value": "Raw: Women Personality Creative Christmas Hat Face Expression Gold Earring Funny Cartoon Ear Stud Jewelry Accessories Gift Hot", "md": "## Raw: Women Personality Creative Christmas Hat Face Expression Gold Earring Funny Cartoon Ear Stud Jewelry Accessories Gift Hot"}, {"type": "text", "value": "BLIP (finetuned): red and gold tone emojt earring\n\nBLIP2: kawaii santa emoticuos en la cabeza\n\nBLIP2 (finetuned): a pair of emoji earrings with faces and hats\n\nOpenCLIP-CoCa: best christmas gift for her new arrivals emoji earrings christmas emoji earrings\n\nOpenCLIP-CoCa (finetuned): a pair of gold earrings with a smiley face and a Christmas hat", "md": "BLIP (finetuned): red and gold tone emojt earring\n\nBLIP2: kawaii santa emoticuos en la cabeza\n\nBLIP2 (finetuned): a pair of emoji earrings with faces and hats\n\nOpenCLIP-CoCa: best christmas gift for her new arrivals emoji earrings christmas emoji earrings\n\nOpenCLIP-CoCa (finetuned): a pair of gold earrings with a smiley face and a Christmas hat"}, {"type": "heading", "lvl": 2, "value": "Raw: 10840 SW 126th St photo067", "md": "## Raw: 10840 SW 126th St photo067"}, {"type": "text", "value": "BLIP (finetuned): overview of a large backyard with a swimming pool and patio\n\nBLIP2: 3344 SW 7th St, Palm Beach\n\nBLIP2 (finetuned): a house with a pool from above, with a yard\n\nOpenCLIP-CoCa: home for sale in Country Club Shores West Palm Beach Florida\n\nOpenCLIP-CoCa (finetuned): aerial image of a pool that has a little bit of shade by the side", "md": "BLIP (finetuned): overview of a large backyard with a swimming pool and patio\n\nBLIP2: 3344 SW 7th St, Palm Beach\n\nBLIP2 (finetuned): a house with a pool from above, with a yard\n\nOpenCLIP-CoCa: home for sale in Country Club Shores West Palm Beach Florida\n\nOpenCLIP-CoCa (finetuned): aerial image of a pool that has a little bit of shade by the side"}, {"type": "heading", "lvl": 2, "value": "Raw: Greenberg Weathered Marble Plush Ivory Area Rug", "md": "## Raw: Greenberg Weathered Marble Plush Ivory Area Rug"}, {"type": "text", "value": "BLIP (finetuned): grey rug with a text home on it by a table\n\nBLIP2: a grey area rug on a wooden floor\n\nBLIP2 (finetuned): a white coffee table with a sign saying home on it. It is sitting on a cream colored rug\n\nOpenCLIP-CoCa: rugs and carpets in Hyderabad: buy online at best price in ...\n\nOpenCLIP-CoCa (finetuned): a rug is shown in a living room with a chair", "md": "BLIP (finetuned): grey rug with a text home on it by a table\n\nBLIP2: a grey area rug on a wooden floor\n\nBLIP2 (finetuned): a white coffee table with a sign saying home on it. It is sitting on a cream colored rug\n\nOpenCLIP-CoCa: rugs and carpets in Hyderabad: buy online at best price in ...\n\nOpenCLIP-CoCa (finetuned): a rug is shown in a living room with a chair"}]}, {"page": 21, "text": "                                                     Raw: productivity, productivity, productivity\n                                                     BLIP (finetuned): drivers guide to the truck industry\n                                                     BLIP2: buy and sell truck parts\n      The Trucking                                   BLIP2 (finetuned): a white truck with a cover on it drives along\n         Industry                                   a highway\n                                                     OpenCLIP-CoCa:                how the trucking industry is chang-\n                                                    ing\n                                                     OpenCLIP-CoCa (finetuned): there are some trucks on the road\n                                                     .\n                                                     Raw: Amigas\n                                                     BLIP (finetuned): crowd of people outside a wedding ceremony\n                                                    near several trees\n                                                     BLIP2: a wedding ceremony in the middle of the street\n                                                     BLIP2 (finetuned): a black and white photograph of a number\n                                                    of women in prom dresses\n                                                     OpenCLIP-CoCa: 2 0 1 3 0 8 0 5 _ wedding _ carlenan _ 0\n                                                    0 3\n                                                     OpenCLIP-CoCa (finetuned): a group of people hugging and\ntalking in a group\n                                                     Raw: Automne hiver enfants manteau et pantalon ensemble\n                                                    capuche veste de Ski et pantalon gar\u00e7on fille coupe-vent imper-\n                                                    m\u00e9able en plein air camping randonn\u00e9e\n                                                     BLIP (finetuned): a man wearing a red and blue jacket and a\n                                                     pair of pants and a pair of sneakers\n                                                     BLIP2: the arctic light hooded jacket and pants set\n                                                     BLIP2 (finetuned): the colors of the jacket match the pant color\n                                                    of the jacket\n                                                     OpenCLIP-CoCa: the arctic light 2 0 1 7 children \u2019s clothing\n                                                    sets winter kids ski suits sets windproof waterproof warm jackets\ncoats pants boys set\nOpenCLIP-CoCa (finetuned): a child standing in their ski wear and a jacket and pants\n                                                      Raw: Der Lieferumfang\n                                                      BLIP (finetuned): there are several electronics laid out on the\n                                                      table ready to be used\n                                                      BLIP2: samsung galaxy s10e review | a quick tour of the sam-\n                                                      sung galaxy s10e\n                                                      BLIP2 (finetuned): wireless charging case and remote control,\n                                                      both packaged in the box\n                                                      OpenCLIP-CoCa: best - wireless - chargers - for - samsung\n                                                     - galaxy - note - 8 - s 8 - and - iphone - 8\nOpenCLIP-CoCa (finetuned): a set of various electronic items sitting on a table .\n                                                                     21", "md": "# Text and Equations\n\nRaw: productivity, productivity, productivity\n\nBLIP (finetuned): drivers guide to the truck industry\n\nBLIP2: buy and sell truck parts\n\nThe Trucking Industry\n\nBLIP2 (finetuned): a white truck with a cover on it drives along a highway\n\nOpenCLIP-CoCa: how the trucking industry is changing\n\nOpenCLIP-CoCa (finetuned): there are some trucks on the road.\n\nRaw: Amigas\n\nBLIP (finetuned): crowd of people outside a wedding ceremony near several trees\n\nBLIP2: a wedding ceremony in the middle of the street\n\nBLIP2 (finetuned): a black and white photograph of a number of women in prom dresses\n\nOpenCLIP-CoCa: 2 0 1 3 0 8 0 5 _ wedding _ carlenan _ 0 0 3\n\nOpenCLIP-CoCa (finetuned): a group of people hugging and talking in a group\n\nRaw: Automne hiver enfants manteau et pantalon ensemble capuche veste de Ski et pantalon gar\u00e7on fille coupe-vent imperm\u00e9able en plein air camping randonn\u00e9e\n\nBLIP (finetuned): a man wearing a red and blue jacket and a pair of pants and a pair of sneakers\n\nBLIP2: the arctic light hooded jacket and pants set\n\nBLIP2 (finetuned): the colors of the jacket match the pant color of the jacket\n\nOpenCLIP-CoCa: the arctic light 2 0 1 7 children \u2019s clothing sets winter kids ski suits sets windproof waterproof warm jackets coats pants boys set\n\nOpenCLIP-CoCa (finetuned): a child standing in their ski wear and a jacket and pants\n\nRaw: Der Lieferumfang\n\nBLIP (finetuned): there are several electronics laid out on the table ready to be used\n\nBLIP2: samsung galaxy s10e review | a quick tour of the samsung galaxy s10e\n\nBLIP2 (finetuned): wireless charging case and remote control, both packaged in the box\n\nOpenCLIP-CoCa: best - wireless - chargers - for - samsung - galaxy - note - 8 - s 8 - and - iphone - 8\n\nOpenCLIP-CoCa (finetuned): a set of various electronic items sitting on a table.\n\n21", "images": [{"name": "page-21-0.jpg", "height": 106, "width": 160, "x": 74, "y": 95}, {"name": "page-21-1.jpg", "height": 107, "width": 160, "x": 74, "y": 226}, {"name": "page-21-2.jpg", "height": 155, "width": 155, "x": 76, "y": 360}, {"name": "page-21-3.jpg", "height": 110, "width": 164, "x": 74, "y": 559}], "items": [{"type": "heading", "lvl": 1, "value": "Text and Equations", "md": "# Text and Equations"}, {"type": "text", "value": "Raw: productivity, productivity, productivity\n\nBLIP (finetuned): drivers guide to the truck industry\n\nBLIP2: buy and sell truck parts\n\nThe Trucking Industry\n\nBLIP2 (finetuned): a white truck with a cover on it drives along a highway\n\nOpenCLIP-CoCa: how the trucking industry is changing\n\nOpenCLIP-CoCa (finetuned): there are some trucks on the road.\n\nRaw: Amigas\n\nBLIP (finetuned): crowd of people outside a wedding ceremony near several trees\n\nBLIP2: a wedding ceremony in the middle of the street\n\nBLIP2 (finetuned): a black and white photograph of a number of women in prom dresses\n\nOpenCLIP-CoCa: 2 0 1 3 0 8 0 5 _ wedding _ carlenan _ 0 0 3\n\nOpenCLIP-CoCa (finetuned): a group of people hugging and talking in a group\n\nRaw: Automne hiver enfants manteau et pantalon ensemble capuche veste de Ski et pantalon gar\u00e7on fille coupe-vent imperm\u00e9able en plein air camping randonn\u00e9e\n\nBLIP (finetuned): a man wearing a red and blue jacket and a pair of pants and a pair of sneakers\n\nBLIP2: the arctic light hooded jacket and pants set\n\nBLIP2 (finetuned): the colors of the jacket match the pant color of the jacket\n\nOpenCLIP-CoCa: the arctic light 2 0 1 7 children \u2019s clothing sets winter kids ski suits sets windproof waterproof warm jackets coats pants boys set\n\nOpenCLIP-CoCa (finetuned): a child standing in their ski wear and a jacket and pants\n\nRaw: Der Lieferumfang\n\nBLIP (finetuned): there are several electronics laid out on the table ready to be used\n\nBLIP2: samsung galaxy s10e review | a quick tour of the samsung galaxy s10e\n\nBLIP2 (finetuned): wireless charging case and remote control, both packaged in the box\n\nOpenCLIP-CoCa: best - wireless - chargers - for - samsung - galaxy - note - 8 - s 8 - and - iphone - 8\n\nOpenCLIP-CoCa (finetuned): a set of various electronic items sitting on a table.\n\n21", "md": "Raw: productivity, productivity, productivity\n\nBLIP (finetuned): drivers guide to the truck industry\n\nBLIP2: buy and sell truck parts\n\nThe Trucking Industry\n\nBLIP2 (finetuned): a white truck with a cover on it drives along a highway\n\nOpenCLIP-CoCa: how the trucking industry is changing\n\nOpenCLIP-CoCa (finetuned): there are some trucks on the road.\n\nRaw: Amigas\n\nBLIP (finetuned): crowd of people outside a wedding ceremony near several trees\n\nBLIP2: a wedding ceremony in the middle of the street\n\nBLIP2 (finetuned): a black and white photograph of a number of women in prom dresses\n\nOpenCLIP-CoCa: 2 0 1 3 0 8 0 5 _ wedding _ carlenan _ 0 0 3\n\nOpenCLIP-CoCa (finetuned): a group of people hugging and talking in a group\n\nRaw: Automne hiver enfants manteau et pantalon ensemble capuche veste de Ski et pantalon gar\u00e7on fille coupe-vent imperm\u00e9able en plein air camping randonn\u00e9e\n\nBLIP (finetuned): a man wearing a red and blue jacket and a pair of pants and a pair of sneakers\n\nBLIP2: the arctic light hooded jacket and pants set\n\nBLIP2 (finetuned): the colors of the jacket match the pant color of the jacket\n\nOpenCLIP-CoCa: the arctic light 2 0 1 7 children \u2019s clothing sets winter kids ski suits sets windproof waterproof warm jackets coats pants boys set\n\nOpenCLIP-CoCa (finetuned): a child standing in their ski wear and a jacket and pants\n\nRaw: Der Lieferumfang\n\nBLIP (finetuned): there are several electronics laid out on the table ready to be used\n\nBLIP2: samsung galaxy s10e review | a quick tour of the samsung galaxy s10e\n\nBLIP2 (finetuned): wireless charging case and remote control, both packaged in the box\n\nOpenCLIP-CoCa: best - wireless - chargers - for - samsung - galaxy - note - 8 - s 8 - and - iphone - 8\n\nOpenCLIP-CoCa (finetuned): a set of various electronic items sitting on a table.\n\n21"}]}, {"page": 22, "text": "                               Raw: Autozone\n                               BLIP (finetuned): racing track with a line of seats and a sky\n                               background\n                               BLIP2: a photo of a grand prix race track, under a blue sky\n                               BLIP2 (finetuned): the circuit track is empty, but the sun beams\n                               into the sky\n                               OpenCLIP-CoCa: circuit of the americas\n                               OpenCLIP-CoCa (finetuned): a red and white pole next to a\nracing track\n             Hello             Raw: 2016.07.01 Nametags with Pronouns - Avery 5392_non-\n             my name i5        branded\n                               BLIP (finetuned): there are no pictures here to provide a cap-\n                               tion for\n                               BLIP2: hello, my name is name, my pronouns are pronouns\n             niv pronjuns Jre  BLIP2 (finetuned): a blue and white label with a blue and white\n                               text\n                               OpenCLIP-CoCa: 1 5 + hello my name is names pronuncia-\n                               tions and meanings\n                               OpenCLIP-CoCa (finetuned): hello my name is , my pronouns\nare .\n                               Raw: Women long sleeve t shirt 2015 Fashion shirts woman\n                               Full Comfortable leisure fashion womens long sleeve tops\n       Qooio                   BLIP (finetuned): the qaoo loading logo is shown above the qaoo\n                               loading logo\n                               BLIP2: qoo10 loading logo on white\n                               BLIP2 (finetuned): a picture of an image of a phone screen\n            Loading            showing a loading sign\n                               OpenCLIP-CoCa: loading _ 1 1 1 1 2 0 _ 0 1 . png\n                               OpenCLIP-CoCa (finetuned): a light grey font and a dark grey\n                               font with a large white background\n                               Raw: 1173x1500 Awesome Adult Coloring Pages Printable Zen-\n                               tangle Design\n                               BLIP (finetuned): chinese dragon coloring pages dragon color-\n                               ing pages for adults to print coloring pages\n                               BLIP2: dragon coloring pages with large and large dragon\n                               BLIP2 (finetuned): a circle with a dragon on it in the cen-\n                               ter\n                               OpenCLIP-CoCa: the 2 5 best chinese dragon drawing ideas on\n                               pinterest chinese\n                               OpenCLIP-CoCa (finetuned): a chinese dragon looks like a\n                               dragon from the movie the karate kid\n                                            22", "md": "# OCR Text\n\nRaw: Autozone\n\nBLIP (finetuned): racing track with a line of seats and a sky background\n\nBLIP2: a photo of a grand prix race track, under a blue sky\n\nBLIP2 (finetuned): the circuit track is empty, but the sun beams into the sky\n\nOpenCLIP-CoCa: circuit of the americas\n\nOpenCLIP-CoCa (finetuned): a red and white pole next to a racing track\n\nHello Raw: 2016.07.01 Nametags with Pronouns - Avery 5392_non-branded\n\nBLIP (finetuned): there are no pictures here to provide a caption for\n\nBLIP2: hello, my name is name, my pronouns are pronouns\n\nBLIP2 (finetuned): a blue and white label with a blue and white text\n\nOpenCLIP-CoCa: 1 5 + hello my name is names pronunciations and meanings\n\nOpenCLIP-CoCa (finetuned): hello my name is , my pronouns are .\n\nRaw: Women long sleeve t shirt 2015 Fashion shirts woman Full Comfortable leisure fashion womens long sleeve tops\n\nBLIP (finetuned): the qaoo loading logo is shown above the qaoo loading logo\n\nBLIP2: qoo10 loading logo on white\n\nBLIP2 (finetuned): a picture of an image of a phone screen showing a loading sign\n\nOpenCLIP-CoCa: loading _ 1 1 1 1 2 0 _ 0 1 . png\n\nOpenCLIP-CoCa (finetuned): a light grey font and a dark grey font with a large white background\n\nRaw: 1173x1500 Awesome Adult Coloring Pages Printable Zentangle Design\n\nBLIP (finetuned): chinese dragon coloring pages dragon coloring pages for adults to print coloring pages\n\nBLIP2: dragon coloring pages with large and large dragon\n\nBLIP2 (finetuned): a circle with a dragon on it in the center\n\nOpenCLIP-CoCa: the 2 5 best chinese dragon drawing ideas on pinterest chinese\n\nOpenCLIP-CoCa (finetuned): a chinese dragon looks like a dragon from the movie the karate kid", "images": [{"name": "page-22-0.jpg", "height": 90, "width": 160, "x": 74, "y": 86}, {"name": "page-22-1.jpg", "height": 119, "width": 160, "x": 74, "y": 218}, {"name": "page-22-2.jpg", "height": 346, "width": 160, "x": 74, "y": 362}], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "text", "value": "Raw: Autozone\n\nBLIP (finetuned): racing track with a line of seats and a sky background\n\nBLIP2: a photo of a grand prix race track, under a blue sky\n\nBLIP2 (finetuned): the circuit track is empty, but the sun beams into the sky\n\nOpenCLIP-CoCa: circuit of the americas\n\nOpenCLIP-CoCa (finetuned): a red and white pole next to a racing track\n\nHello Raw: 2016.07.01 Nametags with Pronouns - Avery 5392_non-branded\n\nBLIP (finetuned): there are no pictures here to provide a caption for\n\nBLIP2: hello, my name is name, my pronouns are pronouns\n\nBLIP2 (finetuned): a blue and white label with a blue and white text\n\nOpenCLIP-CoCa: 1 5 + hello my name is names pronunciations and meanings\n\nOpenCLIP-CoCa (finetuned): hello my name is , my pronouns are .\n\nRaw: Women long sleeve t shirt 2015 Fashion shirts woman Full Comfortable leisure fashion womens long sleeve tops\n\nBLIP (finetuned): the qaoo loading logo is shown above the qaoo loading logo\n\nBLIP2: qoo10 loading logo on white\n\nBLIP2 (finetuned): a picture of an image of a phone screen showing a loading sign\n\nOpenCLIP-CoCa: loading _ 1 1 1 1 2 0 _ 0 1 . png\n\nOpenCLIP-CoCa (finetuned): a light grey font and a dark grey font with a large white background\n\nRaw: 1173x1500 Awesome Adult Coloring Pages Printable Zentangle Design\n\nBLIP (finetuned): chinese dragon coloring pages dragon coloring pages for adults to print coloring pages\n\nBLIP2: dragon coloring pages with large and large dragon\n\nBLIP2 (finetuned): a circle with a dragon on it in the center\n\nOpenCLIP-CoCa: the 2 5 best chinese dragon drawing ideas on pinterest chinese\n\nOpenCLIP-CoCa (finetuned): a chinese dragon looks like a dragon from the movie the karate kid", "md": "Raw: Autozone\n\nBLIP (finetuned): racing track with a line of seats and a sky background\n\nBLIP2: a photo of a grand prix race track, under a blue sky\n\nBLIP2 (finetuned): the circuit track is empty, but the sun beams into the sky\n\nOpenCLIP-CoCa: circuit of the americas\n\nOpenCLIP-CoCa (finetuned): a red and white pole next to a racing track\n\nHello Raw: 2016.07.01 Nametags with Pronouns - Avery 5392_non-branded\n\nBLIP (finetuned): there are no pictures here to provide a caption for\n\nBLIP2: hello, my name is name, my pronouns are pronouns\n\nBLIP2 (finetuned): a blue and white label with a blue and white text\n\nOpenCLIP-CoCa: 1 5 + hello my name is names pronunciations and meanings\n\nOpenCLIP-CoCa (finetuned): hello my name is , my pronouns are .\n\nRaw: Women long sleeve t shirt 2015 Fashion shirts woman Full Comfortable leisure fashion womens long sleeve tops\n\nBLIP (finetuned): the qaoo loading logo is shown above the qaoo loading logo\n\nBLIP2: qoo10 loading logo on white\n\nBLIP2 (finetuned): a picture of an image of a phone screen showing a loading sign\n\nOpenCLIP-CoCa: loading _ 1 1 1 1 2 0 _ 0 1 . png\n\nOpenCLIP-CoCa (finetuned): a light grey font and a dark grey font with a large white background\n\nRaw: 1173x1500 Awesome Adult Coloring Pages Printable Zentangle Design\n\nBLIP (finetuned): chinese dragon coloring pages dragon coloring pages for adults to print coloring pages\n\nBLIP2: dragon coloring pages with large and large dragon\n\nBLIP2 (finetuned): a circle with a dragon on it in the center\n\nOpenCLIP-CoCa: the 2 5 best chinese dragon drawing ideas on pinterest chinese\n\nOpenCLIP-CoCa (finetuned): a chinese dragon looks like a dragon from the movie the karate kid"}]}, {"page": 23, "text": "B     Experiment details\nRefer to Appendices M and N of the DataComp benchmark [18] for training and evaluation details.\nTo summarize, both small and medium scales use ViT-B/32 as the image encoder for CLIP, in\naddition to fixing the hyperparameters used for training: learning rate 5e-4, 500 warmup steps, batch\nsize 4096, AdamW optimizer \u03b22 = 0.98. Large scale training uses the same hyperparameters, but\nwith batch size 8192 and ViT-B/16 as the image encoder.\nUsing DataComp infrastructure and the AWS EC2 cloud, a small model takes 4 A100 hours to\ntrain, while medium requires 40 A100 hours and large utilizes 960 A100 hours. We additionally\nreport CLIP ViT-L/14 and BLIP2 (OPT 2.7B backbone) inference costs. Recall that we run both\nof these models on the DataComp\u2019s large pool to curate the datasets used in this paper. For the\nCLIP model, we measure throughput at 490 samples per second on a single A100. For BLIP2, we\nget 75 samples per second on the same hardware. Hence, for the large pool of 1.28B samples, we\nspend 725 A100 hours computing CLIP features and 4,740 A100 hours generating BLIP2 captions.\nWhile the annotation cost (i.e., BLIP2 caption and CLIP score generation) is 6\u00d7 larger than a single\ntraining run proposed by the DataComp benchmark (which is equivalent to doing one pass through\nthe entire candidate pool), this additional cost can be easily amortized with more training epochs\nover the final training set, as well as with training different downstream models on the improved\ndataset. For reference, OpenAI trained various CLIP models on the same set of 400M curated\nimage-text pairs; the best performing model was trained on 256 GPUs for 2 weeks, totalling about\n86,000 GPU hours 7. This scale of training is common among existing large vision models. Future\nwork could explore the option of adaptively allocating compute to CLIP training and synthetic\ncaption annotation given a fixed compute budget.\n7 https://openai.com/research/clip\n                                                 23", "md": "# Experiment Details\n\nRefer to Appendices M and N of the DataComp benchmark [18] for training and evaluation details.\n\nTo summarize, both small and medium scales use ViT-B/32 as the image encoder for CLIP, in addition to fixing the hyperparameters used for training: learning rate 5e-4, 500 warmup steps, batch size 4096, AdamW optimizer $$\\beta_2 = 0.98$$. Large scale training uses the same hyperparameters, but with batch size 8192 and ViT-B/16 as the image encoder.\n\nUsing DataComp infrastructure and the AWS EC2 cloud, a small model takes 4 A100 hours to train, while medium requires 40 A100 hours and large utilizes 960 A100 hours. We additionally report CLIP ViT-L/14 and BLIP2 (OPT 2.7B backbone) inference costs. Recall that we run both of these models on the DataComp\u2019s large pool to curate the datasets used in this paper. For the CLIP model, we measure throughput at 490 samples per second on a single A100. For BLIP2, we get 75 samples per second on the same hardware. Hence, for the large pool of 1.28B samples, we spend 725 A100 hours computing CLIP features and 4,740 A100 hours generating BLIP2 captions.\n\nWhile the annotation cost (i.e., BLIP2 caption and CLIP score generation) is 6\u00d7 larger than a single training run proposed by the DataComp benchmark (which is equivalent to doing one pass through the entire candidate pool), this additional cost can be easily amortized with more training epochs over the final training set, as well as with training different downstream models on the improved dataset. For reference, OpenAI trained various CLIP models on the same set of 400M curated image-text pairs; the best performing model was trained on 256 GPUs for 2 weeks, totalling about 86,000 GPU hours. This scale of training is common among existing large vision models. Future work could explore the option of adaptively allocating compute to CLIP training and synthetic caption annotation given a fixed compute budget.\n\nReference: OpenAI Research - CLIP", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Experiment Details", "md": "# Experiment Details"}, {"type": "text", "value": "Refer to Appendices M and N of the DataComp benchmark [18] for training and evaluation details.\n\nTo summarize, both small and medium scales use ViT-B/32 as the image encoder for CLIP, in addition to fixing the hyperparameters used for training: learning rate 5e-4, 500 warmup steps, batch size 4096, AdamW optimizer $$\\beta_2 = 0.98$$. Large scale training uses the same hyperparameters, but with batch size 8192 and ViT-B/16 as the image encoder.\n\nUsing DataComp infrastructure and the AWS EC2 cloud, a small model takes 4 A100 hours to train, while medium requires 40 A100 hours and large utilizes 960 A100 hours. We additionally report CLIP ViT-L/14 and BLIP2 (OPT 2.7B backbone) inference costs. Recall that we run both of these models on the DataComp\u2019s large pool to curate the datasets used in this paper. For the CLIP model, we measure throughput at 490 samples per second on a single A100. For BLIP2, we get 75 samples per second on the same hardware. Hence, for the large pool of 1.28B samples, we spend 725 A100 hours computing CLIP features and 4,740 A100 hours generating BLIP2 captions.\n\nWhile the annotation cost (i.e., BLIP2 caption and CLIP score generation) is 6\u00d7 larger than a single training run proposed by the DataComp benchmark (which is equivalent to doing one pass through the entire candidate pool), this additional cost can be easily amortized with more training epochs over the final training set, as well as with training different downstream models on the improved dataset. For reference, OpenAI trained various CLIP models on the same set of 400M curated image-text pairs; the best performing model was trained on 256 GPUs for 2 weeks, totalling about 86,000 GPU hours. This scale of training is common among existing large vision models. Future work could explore the option of adaptively allocating compute to CLIP training and synthetic caption annotation given a fixed compute budget.\n\nReference: OpenAI Research - CLIP", "md": "Refer to Appendices M and N of the DataComp benchmark [18] for training and evaluation details.\n\nTo summarize, both small and medium scales use ViT-B/32 as the image encoder for CLIP, in addition to fixing the hyperparameters used for training: learning rate 5e-4, 500 warmup steps, batch size 4096, AdamW optimizer $$\\beta_2 = 0.98$$. Large scale training uses the same hyperparameters, but with batch size 8192 and ViT-B/16 as the image encoder.\n\nUsing DataComp infrastructure and the AWS EC2 cloud, a small model takes 4 A100 hours to train, while medium requires 40 A100 hours and large utilizes 960 A100 hours. We additionally report CLIP ViT-L/14 and BLIP2 (OPT 2.7B backbone) inference costs. Recall that we run both of these models on the DataComp\u2019s large pool to curate the datasets used in this paper. For the CLIP model, we measure throughput at 490 samples per second on a single A100. For BLIP2, we get 75 samples per second on the same hardware. Hence, for the large pool of 1.28B samples, we spend 725 A100 hours computing CLIP features and 4,740 A100 hours generating BLIP2 captions.\n\nWhile the annotation cost (i.e., BLIP2 caption and CLIP score generation) is 6\u00d7 larger than a single training run proposed by the DataComp benchmark (which is equivalent to doing one pass through the entire candidate pool), this additional cost can be easily amortized with more training epochs over the final training set, as well as with training different downstream models on the improved dataset. For reference, OpenAI trained various CLIP models on the same set of 400M curated image-text pairs; the best performing model was trained on 256 GPUs for 2 weeks, totalling about 86,000 GPU hours. This scale of training is common among existing large vision models. Future work could explore the option of adaptively allocating compute to CLIP training and synthetic caption annotation given a fixed compute budget.\n\nReference: OpenAI Research - CLIP"}]}, {"page": 24, "text": "C              Temperature ablations\n                   Captioning model                                  Metric                                          T=0.5                        T=0.75                 T=1.0                      T=1.5\n                   BLIP (finetuned)                                  ImageNet accuracy                               -                            0.207                  0.212                       -\n                                                                     Average accuracy                                -                            0.303                  0.312                       -\n                   BLIP2                                             ImageNet accuracy                               0.212                        0.281                  0.280                       0.251\n                                                                     Average accuracy                                0.300                        0.357                  0.353                       0.332\n                   BLIP2 (finetuned) ImageNet accuracy                                                               -                            0.227                  0.234                       0.221\n                                                                     Average accuracy                                -                            0.325                  0.326                       0.311\n                   OpenCLIP-CoCa                                     ImageNet accuracy                               0.306                        0.321                  0.314                       -\n                                                                     Average accuracy                                0.366                        0.371                  0.370                       -\n                   OpenCLIP-CoCa                                     ImageNet accuracy                               -                            0.252                  0.264                       0.262\n                   (finetuned)                                       Average accuracy                                -                            0.364                  0.374                       0.364\nTable 3: Performance on ImageNet and averaged across 38 tasks when training on the captions generated\nby captioning models in Table 1, with different softmax temperatures. We find that T = 0.75 and T = 1.0\ngenerally lead to good performance for CLIP training.\nD              More filtering baselines\n       50                                         Raw (top 30%)              BLIP2 (top 50%)                                 32.5                           BLIP2                      BLIP2 (top 50%)\n                               BLIP2             +BLIP2 (70%)                                        Raw (top 30%)                                                 Raw (top 30%)                                Raw (top 30%)\n                                                                                                    +BLIP2 (70%,                                                   +BLIP2 (70%)             BLIP2 (top 75%     +BLIP2 (70%,\n                                                                           BLIP2 (top 75%            filtered)               30.0                                                           intersect IN1k)     filtered)\n       45                                                                  intersect IN1k)\n                                                                                                                             27.5\n                                                                                                                            MS-COCO retrieval\n      Flickr retrieval\n       40                                                                                                                    25.0\n       35                                                                                                                    22.5\n                                                                                                                             20.0\n       30\n                                                                                                                             17.5\n       25          Raw (top 30%)                                         Raw (top 30%                                        15.0           Raw (top 30%)                                Raw (top 30%\n                                                                         intersect IN1k)                                                                                                 intersect IN1k)\n                               28                    29                    30                    31                                                     28             29                  30                31\n                                                   ImageNet accuracy                                                                                                ImageNet accuracy\nFigure 9: Retrieval performance on Flickr (left) and MS-COCO (right) versus ImageNet accuracy for select\nbaselines. Similar to the findings in Figure 2, we find that using only BLIP2 captions or mixing them with\nraw captions in the training data significantly boosts retrieval performance.\n                                                                                                                    24", "md": "# Document\n\n## Temperature ablations\n\n|Captioning model|Metric|T=0.5|T=0.75|T=1.0|T=1.5|\n|---|---|---|---|---|---|\n|BLIP (finetuned)|ImageNet accuracy|-|0.207|0.212|-|\n| |Average accuracy|-|0.303|0.312|-|\n|BLIP2|ImageNet accuracy|0.212|0.281|0.280|0.251|\n| |Average accuracy|0.300|0.357|0.353|0.332|\n|BLIP2 (finetuned)|ImageNet accuracy|-|0.227|0.234|0.221|\n| |Average accuracy|-|0.325|0.326|0.311|\n|OpenCLIP-CoCa|ImageNet accuracy|0.306|0.321|0.314|-|\n| |Average accuracy|0.366|0.371|0.370|-|\n|OpenCLIP-CoCa (finetuned)|ImageNet accuracy|-|0.252|0.264|0.262|\n| |Average accuracy|-|0.364|0.374|0.364|\n\nTable 3: Performance on ImageNet and averaged across 38 tasks when training on the captions generated by captioning models in Table 1, with different softmax temperatures. We find that $$T = 0.75$$ and $$T = 1.0$$ generally lead to good performance for CLIP training.\n\n## More filtering baselines\n\n| |Raw (top 30%)|BLIP2 (top 50%)| |BLIP2 +BLIP2 (70%)| |Raw (top 30%)|Raw (top 30%)|\n|---|---|---|---|---|---|---|---|\n|BLIP2| | |BLIP2 (top 75% intersect IN1k)| |BLIP2 (top 50% filtered)|30.0| |\n| | |27.5| | | | | |\n| |MS-COCO retrieval| | | | | | |\n|Flickr retrieval| | | | | | | |\n| | | | | | | | |\n| |Raw (top 30%)|Raw (top 30% intersect IN1k)| | | |Raw (top 30%)|Raw (top 30% intersect IN1k)|\n| |28|29|30|31| |28|29|30|31|\n| |ImageNet accuracy| | | | |ImageNet accuracy| | | |\n\nFigure 9: Retrieval performance on Flickr (left) and MS-COCO (right) versus ImageNet accuracy for select baselines. Similar to the findings in Figure 2, we find that using only BLIP2 captions or mixing them with raw captions in the training data significantly boosts retrieval performance.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Temperature ablations", "md": "## Temperature ablations"}, {"type": "table", "rows": [["Captioning model", "Metric", "T=0.5", "T=0.75", "T=1.0", "T=1.5"], ["BLIP (finetuned)", "ImageNet accuracy", "-", "0.207", "0.212", "-"], ["", "Average accuracy", "-", "0.303", "0.312", "-"], ["BLIP2", "ImageNet accuracy", "0.212", "0.281", "0.280", "0.251"], ["", "Average accuracy", "0.300", "0.357", "0.353", "0.332"], ["BLIP2 (finetuned)", "ImageNet accuracy", "-", "0.227", "0.234", "0.221"], ["", "Average accuracy", "-", "0.325", "0.326", "0.311"], ["OpenCLIP-CoCa", "ImageNet accuracy", "0.306", "0.321", "0.314", "-"], ["", "Average accuracy", "0.366", "0.371", "0.370", "-"], ["OpenCLIP-CoCa (finetuned)", "ImageNet accuracy", "-", "0.252", "0.264", "0.262"], ["", "Average accuracy", "-", "0.364", "0.374", "0.364"]], "md": "|Captioning model|Metric|T=0.5|T=0.75|T=1.0|T=1.5|\n|---|---|---|---|---|---|\n|BLIP (finetuned)|ImageNet accuracy|-|0.207|0.212|-|\n| |Average accuracy|-|0.303|0.312|-|\n|BLIP2|ImageNet accuracy|0.212|0.281|0.280|0.251|\n| |Average accuracy|0.300|0.357|0.353|0.332|\n|BLIP2 (finetuned)|ImageNet accuracy|-|0.227|0.234|0.221|\n| |Average accuracy|-|0.325|0.326|0.311|\n|OpenCLIP-CoCa|ImageNet accuracy|0.306|0.321|0.314|-|\n| |Average accuracy|0.366|0.371|0.370|-|\n|OpenCLIP-CoCa (finetuned)|ImageNet accuracy|-|0.252|0.264|0.262|\n| |Average accuracy|-|0.364|0.374|0.364|", "isPerfectTable": true, "csv": "\"Captioning model\",\"Metric\",\"T=0.5\",\"T=0.75\",\"T=1.0\",\"T=1.5\"\n\"BLIP (finetuned)\",\"ImageNet accuracy\",\"-\",\"0.207\",\"0.212\",\"-\"\n\"\",\"Average accuracy\",\"-\",\"0.303\",\"0.312\",\"-\"\n\"BLIP2\",\"ImageNet accuracy\",\"0.212\",\"0.281\",\"0.280\",\"0.251\"\n\"\",\"Average accuracy\",\"0.300\",\"0.357\",\"0.353\",\"0.332\"\n\"BLIP2 (finetuned)\",\"ImageNet accuracy\",\"-\",\"0.227\",\"0.234\",\"0.221\"\n\"\",\"Average accuracy\",\"-\",\"0.325\",\"0.326\",\"0.311\"\n\"OpenCLIP-CoCa\",\"ImageNet accuracy\",\"0.306\",\"0.321\",\"0.314\",\"-\"\n\"\",\"Average accuracy\",\"0.366\",\"0.371\",\"0.370\",\"-\"\n\"OpenCLIP-CoCa (finetuned)\",\"ImageNet accuracy\",\"-\",\"0.252\",\"0.264\",\"0.262\"\n\"\",\"Average accuracy\",\"-\",\"0.364\",\"0.374\",\"0.364\""}, {"type": "text", "value": "Table 3: Performance on ImageNet and averaged across 38 tasks when training on the captions generated by captioning models in Table 1, with different softmax temperatures. We find that $$T = 0.75$$ and $$T = 1.0$$ generally lead to good performance for CLIP training.", "md": "Table 3: Performance on ImageNet and averaged across 38 tasks when training on the captions generated by captioning models in Table 1, with different softmax temperatures. We find that $$T = 0.75$$ and $$T = 1.0$$ generally lead to good performance for CLIP training."}, {"type": "heading", "lvl": 2, "value": "More filtering baselines", "md": "## More filtering baselines"}, {"type": "table", "rows": [["", "Raw (top 30%)", "BLIP2 (top 50%)", "", "BLIP2 +BLIP2 (70%)", "", "Raw (top 30%)", "Raw (top 30%)"], ["BLIP2", "", "", "BLIP2 (top 75% intersect IN1k)", "", "BLIP2 (top 50% filtered)", "30.0", ""], ["", "", "27.5", "", "", "", "", ""], ["", "MS-COCO retrieval", "", "", "", "", "", ""], ["Flickr retrieval", "", "", "", "", "", "", ""], ["", "", "", "", "", "", "", ""], ["", "Raw (top 30%)", "Raw (top 30% intersect IN1k)", "", "", "", "Raw (top 30%)", "Raw (top 30% intersect IN1k)"], ["", "28", "29", "30", "31", "", "28", "29", "30", "31"], ["", "ImageNet accuracy", "", "", "", "", "ImageNet accuracy", "", "", ""]], "md": "| |Raw (top 30%)|BLIP2 (top 50%)| |BLIP2 +BLIP2 (70%)| |Raw (top 30%)|Raw (top 30%)|\n|---|---|---|---|---|---|---|---|\n|BLIP2| | |BLIP2 (top 75% intersect IN1k)| |BLIP2 (top 50% filtered)|30.0| |\n| | |27.5| | | | | |\n| |MS-COCO retrieval| | | | | | |\n|Flickr retrieval| | | | | | | |\n| | | | | | | | |\n| |Raw (top 30%)|Raw (top 30% intersect IN1k)| | | |Raw (top 30%)|Raw (top 30% intersect IN1k)|\n| |28|29|30|31| |28|29|30|31|\n| |ImageNet accuracy| | | | |ImageNet accuracy| | | |", "isPerfectTable": false, "csv": "\"\",\"Raw (top 30%)\",\"BLIP2 (top 50%)\",\"\",\"BLIP2 +BLIP2 (70%)\",\"\",\"Raw (top 30%)\",\"Raw (top 30%)\"\n\"BLIP2\",\"\",\"\",\"BLIP2 (top 75% intersect IN1k)\",\"\",\"BLIP2 (top 50% filtered)\",\"30.0\",\"\"\n\"\",\"\",\"27.5\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"MS-COCO retrieval\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"Flickr retrieval\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"Raw (top 30%)\",\"Raw (top 30% intersect IN1k)\",\"\",\"\",\"\",\"Raw (top 30%)\",\"Raw (top 30% intersect IN1k)\"\n\"\",\"28\",\"29\",\"30\",\"31\",\"\",\"28\",\"29\",\"30\",\"31\"\n\"\",\"ImageNet accuracy\",\"\",\"\",\"\",\"\",\"ImageNet accuracy\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 9: Retrieval performance on Flickr (left) and MS-COCO (right) versus ImageNet accuracy for select baselines. Similar to the findings in Figure 2, we find that using only BLIP2 captions or mixing them with raw captions in the training data significantly boosts retrieval performance.", "md": "Figure 9: Retrieval performance on Flickr (left) and MS-COCO (right) versus ImageNet accuracy for select baselines. Similar to the findings in Figure 2, we find that using only BLIP2 captions or mixing them with raw captions in the training data significantly boosts retrieval performance."}]}, {"page": 25, "text": "   Baseline                                                                           Training       ImageNet   Average\n                                                                                      set size       accuracy   accuracy\n                                 small scale (12.8M candidate pool, 12.8M training steps)\n   Raw captions (no filtering)                                                        12.8M*         0.025*     0.132*\n   BLIP2 captions (no fi   ltering)                                                   12.8M          0.076      0.200\n   Raw captions (top 30%)                                                             3.8M*          0.051*     0.173*\n   BLIP2 captions (top 50%)                                                           6.4M           0.080      0.203\n   Raw captions (top 30% intersect IN1k)                                              1.4M*          0.039*     0.144*\n   BLIP2 captions (top 75% intersect IN1k)                                            2.4M           0.073      0.192\n   Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k            2.2M           0.045      0.153\n   Raw captions (top 30%) + BLIP2 captions (70%, filtered)                            8.4M           0.076      0.197\n                                 medium scale (128M candidate pool, 128M training steps)\n   Raw captions (no filtering)                                                        128M*          0.176*     0.258*\n   BLIP2 captions (no fi   ltering)                                                   128M           0.281      0.357\n   Top BLIP2 captions across all temperatures (no filtering)                          128M           0.293      0.368\n   Raw captions (top 30%)                                                             38M*           0.273*     0.328*\n   BLIP2 captions (top 50%)                                                           64.1M          0.302      0.370\n   Raw captions (top 30% intersect IN1k)                                              14.0M*         0.297*     0.328*\n   BLIP2 captions (top 75% intersect IN1k)                                            23.6M          0.306      0.360\n   Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k            22.2M          0.281      0.314\n   Raw captions (top 30%) + BLIP2 captions (70%, filtered)                            83.6M          0.317      0.368\n   BLIP2 captions (top 50%) + Raw captions (50%, filtered)                            75.3M          0.310      0.376\n                                 large scale (1.28B candidate pool, 1.28B training steps)\n   Raw captions (no filtering)                                                        1.28B*         0.459*     0.437*\n   BLIP2 captions (no fi   ltering)                                                   1.28B          0.487      0.505\n   Raw captions (top 30%)                                                             384M*          0.578*     0.529*\n   BLIP2 captions (top 50%)                                                           641M           0.526      0.522\n   Raw captions (top 30% intersect IN1k)                                              140M*          0.631*     0.537*\n   BLIP2 captions (top 75% intersect IN1k)                                            237M           0.533      0.527\n   Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k            222M           0.643      0.549\n   Raw captions (top 30%) + BLIP2 captions (70%, filtered)                            834M           0.598      0.551\nTable 4: Performance for select baselines at small, medium, and large scales of DataComp. * indicates\nnumbers obtained from the original paper [18]. Underlined numbers are best-performing baselines from\nthe DataComp benchmark, trained on only raw web-crawled captions. Bolded numbers are the updated\nbest-performing methods after comparing with baselines involving synthetic captions. In general, given a\nfixed training budget, it is helpful to include more samples in the training pool by carefully replacing noisy\nraw captions with synthetic captions (i.e., Raw (top 30%) + BLIP2 (70%, filtered) versus Raw (top\n30%)). We experiment with many more filtering and mixing methods at the medium scale and report how\nthe performance varies with CLIP score filtering threshold, see Table 5.\n                                                               25", "md": "|Baseline|Training set size|ImageNet accuracy|Average accuracy|\n|---|---|---|---|\n|small scale (12.8M candidate pool, 12.8M training steps)| | | |\n|Raw captions (no filtering)|12.8M*|0.025*|0.132*|\n|BLIP2 captions (no filtering)|12.8M|0.076|0.200|\n|Raw captions (top 30%)|3.8M*|0.051*|0.173*|\n|BLIP2 captions (top 50%)|6.4M|0.080|0.203|\n|Raw captions (top 30% intersect IN1k)|1.4M*|0.039*|0.144*|\n|BLIP2 captions (top 75% intersect IN1k)|2.4M|0.073|0.192|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k|2.2M|0.045|0.153|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered)|8.4M|0.076|0.197|\n|medium scale (128M candidate pool, 128M training steps)| | | |\n|Raw captions (no filtering)|128M*|0.176*|0.258*|\n|BLIP2 captions (no filtering)|128M|0.281|0.357|\n|Top BLIP2 captions across all temperatures (no filtering)|128M|0.293|0.368|\n|Raw captions (top 30%)|38M*|0.273*|0.328*|\n|BLIP2 captions (top 50%)|64.1M|0.302|0.370|\n|Raw captions (top 30% intersect IN1k)|14.0M*|0.297*|0.328*|\n|BLIP2 captions (top 75% intersect IN1k)|23.6M|0.306|0.360|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k|22.2M|0.281|0.314|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered)|83.6M|0.317|0.368|\n|BLIP2 captions (top 50%) + Raw captions (50%, filtered)|75.3M|0.310|0.376|\n|large scale (1.28B candidate pool, 1.28B training steps)| | | |\n|Raw captions (no filtering)|1.28B*|0.459*|0.437*|\n|BLIP2 captions (no filtering)|1.28B|0.487|0.505|\n|Raw captions (top 30%)|384M*|0.578*|0.529*|\n|BLIP2 captions (top 50%)|641M|0.526|0.522|\n|Raw captions (top 30% intersect IN1k)|140M*|0.631*|0.537*|\n|BLIP2 captions (top 75% intersect IN1k)|237M|0.533|0.527|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k|222M|0.643|0.549|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered)|834M|0.598|0.551|\n\nTable 4: Performance for select baselines at small, medium, and large scales of DataComp. * indicates numbers obtained from the original paper [18]. Underlined numbers are best-performing baselines from the DataComp benchmark, trained on only raw web-crawled captions. Bolded numbers are the updated best-performing methods after comparing with baselines involving synthetic captions. In general, given a fixed training budget, it is helpful to include more samples in the training pool by carefully replacing noisy raw captions with synthetic captions (i.e., Raw (top 30%) + BLIP2 (70%, filtered) versus Raw (top 30%)). We experiment with many more filtering and mixing methods at the medium scale and report how the performance varies with CLIP score filtering threshold, see Table 5.\n\n25", "images": [], "items": [{"type": "table", "rows": [["Baseline", "Training set size", "ImageNet accuracy", "Average accuracy"], ["small scale (12.8M candidate pool, 12.8M training steps)", "", "", ""], ["Raw captions (no filtering)", "12.8M*", "0.025*", "0.132*"], ["BLIP2 captions (no filtering)", "12.8M", "0.076", "0.200"], ["Raw captions (top 30%)", "3.8M*", "0.051*", "0.173*"], ["BLIP2 captions (top 50%)", "6.4M", "0.080", "0.203"], ["Raw captions (top 30% intersect IN1k)", "1.4M*", "0.039*", "0.144*"], ["BLIP2 captions (top 75% intersect IN1k)", "2.4M", "0.073", "0.192"], ["Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k", "2.2M", "0.045", "0.153"], ["Raw captions (top 30%) + BLIP2 captions (70%, filtered)", "8.4M", "0.076", "0.197"], ["medium scale (128M candidate pool, 128M training steps)", "", "", ""], ["Raw captions (no filtering)", "128M*", "0.176*", "0.258*"], ["BLIP2 captions (no filtering)", "128M", "0.281", "0.357"], ["Top BLIP2 captions across all temperatures (no filtering)", "128M", "0.293", "0.368"], ["Raw captions (top 30%)", "38M*", "0.273*", "0.328*"], ["BLIP2 captions (top 50%)", "64.1M", "0.302", "0.370"], ["Raw captions (top 30% intersect IN1k)", "14.0M*", "0.297*", "0.328*"], ["BLIP2 captions (top 75% intersect IN1k)", "23.6M", "0.306", "0.360"], ["Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k", "22.2M", "0.281", "0.314"], ["Raw captions (top 30%) + BLIP2 captions (70%, filtered)", "83.6M", "0.317", "0.368"], ["BLIP2 captions (top 50%) + Raw captions (50%, filtered)", "75.3M", "0.310", "0.376"], ["large scale (1.28B candidate pool, 1.28B training steps)", "", "", ""], ["Raw captions (no filtering)", "1.28B*", "0.459*", "0.437*"], ["BLIP2 captions (no filtering)", "1.28B", "0.487", "0.505"], ["Raw captions (top 30%)", "384M*", "0.578*", "0.529*"], ["BLIP2 captions (top 50%)", "641M", "0.526", "0.522"], ["Raw captions (top 30% intersect IN1k)", "140M*", "0.631*", "0.537*"], ["BLIP2 captions (top 75% intersect IN1k)", "237M", "0.533", "0.527"], ["Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k", "222M", "0.643", "0.549"], ["Raw captions (top 30%) + BLIP2 captions (70%, filtered)", "834M", "0.598", "0.551"]], "md": "|Baseline|Training set size|ImageNet accuracy|Average accuracy|\n|---|---|---|---|\n|small scale (12.8M candidate pool, 12.8M training steps)| | | |\n|Raw captions (no filtering)|12.8M*|0.025*|0.132*|\n|BLIP2 captions (no filtering)|12.8M|0.076|0.200|\n|Raw captions (top 30%)|3.8M*|0.051*|0.173*|\n|BLIP2 captions (top 50%)|6.4M|0.080|0.203|\n|Raw captions (top 30% intersect IN1k)|1.4M*|0.039*|0.144*|\n|BLIP2 captions (top 75% intersect IN1k)|2.4M|0.073|0.192|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k|2.2M|0.045|0.153|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered)|8.4M|0.076|0.197|\n|medium scale (128M candidate pool, 128M training steps)| | | |\n|Raw captions (no filtering)|128M*|0.176*|0.258*|\n|BLIP2 captions (no filtering)|128M|0.281|0.357|\n|Top BLIP2 captions across all temperatures (no filtering)|128M|0.293|0.368|\n|Raw captions (top 30%)|38M*|0.273*|0.328*|\n|BLIP2 captions (top 50%)|64.1M|0.302|0.370|\n|Raw captions (top 30% intersect IN1k)|14.0M*|0.297*|0.328*|\n|BLIP2 captions (top 75% intersect IN1k)|23.6M|0.306|0.360|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k|22.2M|0.281|0.314|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered)|83.6M|0.317|0.368|\n|BLIP2 captions (top 50%) + Raw captions (50%, filtered)|75.3M|0.310|0.376|\n|large scale (1.28B candidate pool, 1.28B training steps)| | | |\n|Raw captions (no filtering)|1.28B*|0.459*|0.437*|\n|BLIP2 captions (no filtering)|1.28B|0.487|0.505|\n|Raw captions (top 30%)|384M*|0.578*|0.529*|\n|BLIP2 captions (top 50%)|641M|0.526|0.522|\n|Raw captions (top 30% intersect IN1k)|140M*|0.631*|0.537*|\n|BLIP2 captions (top 75% intersect IN1k)|237M|0.533|0.527|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k|222M|0.643|0.549|\n|Raw captions (top 30%) + BLIP2 captions (70%, filtered)|834M|0.598|0.551|", "isPerfectTable": true, "csv": "\"Baseline\",\"Training set size\",\"ImageNet accuracy\",\"Average accuracy\"\n\"small scale (12.8M candidate pool, 12.8M training steps)\",\"\",\"\",\"\"\n\"Raw captions (no filtering)\",\"12.8M*\",\"0.025*\",\"0.132*\"\n\"BLIP2 captions (no filtering)\",\"12.8M\",\"0.076\",\"0.200\"\n\"Raw captions (top 30%)\",\"3.8M*\",\"0.051*\",\"0.173*\"\n\"BLIP2 captions (top 50%)\",\"6.4M\",\"0.080\",\"0.203\"\n\"Raw captions (top 30% intersect IN1k)\",\"1.4M*\",\"0.039*\",\"0.144*\"\n\"BLIP2 captions (top 75% intersect IN1k)\",\"2.4M\",\"0.073\",\"0.192\"\n\"Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k\",\"2.2M\",\"0.045\",\"0.153\"\n\"Raw captions (top 30%) + BLIP2 captions (70%, filtered)\",\"8.4M\",\"0.076\",\"0.197\"\n\"medium scale (128M candidate pool, 128M training steps)\",\"\",\"\",\"\"\n\"Raw captions (no filtering)\",\"128M*\",\"0.176*\",\"0.258*\"\n\"BLIP2 captions (no filtering)\",\"128M\",\"0.281\",\"0.357\"\n\"Top BLIP2 captions across all temperatures (no filtering)\",\"128M\",\"0.293\",\"0.368\"\n\"Raw captions (top 30%)\",\"38M*\",\"0.273*\",\"0.328*\"\n\"BLIP2 captions (top 50%)\",\"64.1M\",\"0.302\",\"0.370\"\n\"Raw captions (top 30% intersect IN1k)\",\"14.0M*\",\"0.297*\",\"0.328*\"\n\"BLIP2 captions (top 75% intersect IN1k)\",\"23.6M\",\"0.306\",\"0.360\"\n\"Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k\",\"22.2M\",\"0.281\",\"0.314\"\n\"Raw captions (top 30%) + BLIP2 captions (70%, filtered)\",\"83.6M\",\"0.317\",\"0.368\"\n\"BLIP2 captions (top 50%) + Raw captions (50%, filtered)\",\"75.3M\",\"0.310\",\"0.376\"\n\"large scale (1.28B candidate pool, 1.28B training steps)\",\"\",\"\",\"\"\n\"Raw captions (no filtering)\",\"1.28B*\",\"0.459*\",\"0.437*\"\n\"BLIP2 captions (no filtering)\",\"1.28B\",\"0.487\",\"0.505\"\n\"Raw captions (top 30%)\",\"384M*\",\"0.578*\",\"0.529*\"\n\"BLIP2 captions (top 50%)\",\"641M\",\"0.526\",\"0.522\"\n\"Raw captions (top 30% intersect IN1k)\",\"140M*\",\"0.631*\",\"0.537*\"\n\"BLIP2 captions (top 75% intersect IN1k)\",\"237M\",\"0.533\",\"0.527\"\n\"Raw captions (top 30%) + BLIP2 captions (70%, filtered), intersect IN1k\",\"222M\",\"0.643\",\"0.549\"\n\"Raw captions (top 30%) + BLIP2 captions (70%, filtered)\",\"834M\",\"0.598\",\"0.551\""}, {"type": "text", "value": "Table 4: Performance for select baselines at small, medium, and large scales of DataComp. * indicates numbers obtained from the original paper [18]. Underlined numbers are best-performing baselines from the DataComp benchmark, trained on only raw web-crawled captions. Bolded numbers are the updated best-performing methods after comparing with baselines involving synthetic captions. In general, given a fixed training budget, it is helpful to include more samples in the training pool by carefully replacing noisy raw captions with synthetic captions (i.e., Raw (top 30%) + BLIP2 (70%, filtered) versus Raw (top 30%)). We experiment with many more filtering and mixing methods at the medium scale and report how the performance varies with CLIP score filtering threshold, see Table 5.\n\n25", "md": "Table 4: Performance for select baselines at small, medium, and large scales of DataComp. * indicates numbers obtained from the original paper [18]. Underlined numbers are best-performing baselines from the DataComp benchmark, trained on only raw web-crawled captions. Bolded numbers are the updated best-performing methods after comparing with baselines involving synthetic captions. In general, given a fixed training budget, it is helpful to include more samples in the training pool by carefully replacing noisy raw captions with synthetic captions (i.e., Raw (top 30%) + BLIP2 (70%, filtered) versus Raw (top 30%)). We experiment with many more filtering and mixing methods at the medium scale and report how the performance varies with CLIP score filtering threshold, see Table 5.\n\n25"}]}, {"page": 26, "text": "     CLIP score filtering                  10%             20%              30%             50%             75%     90%\n                                                      Cosine similarity threshold\n     Raw captions                          0.295           0.266            0.243           0.203           0.160   0.129\n     BLIP2 captions                        0.315           0.292            0.277           0.251           0.217   0.187\n                                                           Only raw captions\n     Training set size                     12.8M*          25.7M*           38.4M*          64.1M*          96.1M*  115M*\n     ImageNet accuracy                     0.198*          0.260*           0.273*          0.254*          0.212*  0.188*\n     Average accuracy                      0.277*          0.322*           0.328*          0.315*          0.285*  0.266*\n                                                         Only BLIP2 captions\n     Training set size                     12.8M           25.6M            38.5M           64.1M           96.0M   115M\n     ImageNet accuracy                     0.146           0.249            0.275           0.302           0.300   0.293\n     Average accuracy                      0.254           0.333            0.356           0.370           0.365   0.366\n                  Only BLIP2 captions, for top % based on cosine similarity of image and raw text\n     Training set size                     12.8M           25.7M            38.4M           64.1M           96.1M   115M\n     ImageNet accuracy                     0.192           0.245            0.261           0.266           0.267   0.276\n     Average accuracy                      0.280           0.330            0.346           0.342           0.349   0.356\n                         Raw captions for top % + BLIP2 captions for the remaining examples\n     Training set size                     128M            128M             128M            128M            128M    128M\n     ImageNet accuracy                     0.286           0.296            0.297           0.286           0.250   0.215\n     Average accuracy                      0.360           0.357            0.365           0.349           0.323   0.293\n                        Raw captions for top % + BLIP2 captions for the remaining examples,\n                                         subject to the same cosine similarity threshold\n     Training set size                     30.5M           59.5M            83.6M           114M            127M    128M\n     ImageNet accuracy                     0.267           0.310            0.317           0.296           0.251   0.212\n     Average accuracy                      0.343           0.372            0.368           0.352           0.313   0.285\n                         BLIP2 captions for top % + raw captions for the remaining examples,\n                                         subject to the same cosine similarity threshold\n     Training set size                     17.1M           32.8M            47.7M           75.3M           105M    121M\n     ImageNet accuracy                     0.212           0.272            0.298           0.310           0.298   0.285\n     Average accuracy                      0.305           0.353            0.367           0.376           0.375   0.355\n          Concatenate raw & BLIP2 captions for top % + BLIP2 captions for the remaining examples,\n                                         subject to the same cosine similarity threshold\n     Training set size                     30.5M           59.5M            83.6M           114M            127M    128M\n     ImageNet accuracy                     0.250           0.287            0.299           0.286           0.269   0.262\n     Average accuracy                      0.336           0.368            0.367           0.359           0.340   0.337\n                                         Top % raw captions + top % BLIP2 captions\n     Training set size                     25.6M           51.3M            76.9M           128M            -       -\n     ImageNet accuracy                     0.238           0.285            0.297           0.300           -       -\n     Average accuracy                      0.318           0.358            0.366           0.356           -       -\n                          BLIP2 captions - top % intersect with examples from IN1k clustering\n     Training set size                     -               -                10.0M           16.4M           23.6M   27.1M\n     ImageNet accuracy                     -               -                0.243           0.289           0.306   0.301\n     Average accuracy                      -               -                0.310           0.343           0.360   0.344\nTable 5: Summary of how various filtering and mixing strategies perform on ImageNet and on average across\n38 evaluation tasks in DataComp, given a 128M candidate pool (medium scale). * indicates numbers obtained\nfrom [18]. Note that all resulting training sets are trained for a fixed number of steps (128M samples seen)\nand other training variables (e.g., architecture, hyperparameters) are kept constant. Synthetic captions are\ngenerated using pre-trained BLIP2 model with top-K sampling (K = 50) and softmax temperature 0.75. We\nfind that at this scale, approaches that yield the best ImageNet and average accuracies leverage a combination\nof raw and synthetic captions.\n                                                                     26", "md": "# Filtered and Mixed Strategies Performance Summary\n\n## Summary of how various filtering and mixing strategies perform on ImageNet and on average across 38 evaluation tasks in DataComp, given a 128M candidate pool (medium scale).\n\n|CLIP score filtering|10%|20%|30%|50%|75%|90%|\n|---|---|---|---|---|---|---|\n|Cosine similarity threshold|Raw captions|BLIP2 captions| | | | |\n| |0.295|0.315| | | | |\n| | |0.266|0.292| | | |\n| | | |0.243|0.277| | |\n| | | | |0.203|0.251| |\n| | | | | |0.160|0.217|\n| | | | | | |0.129|0.187|\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M* & 25.7M* & 38.4M* & 64.1M* & 96.1M* & 115M* \\\\\n& \\text{ImageNet accuracy} & 0.198* & 0.260* & 0.273* & 0.254* & 0.212* & 0.188* \\\\\n& \\text{Average accuracy} & 0.277* & 0.322* & 0.328* & 0.315* & 0.285* & 0.266* \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M & 25.6M & 38.5M & 64.1M & 96.0M & 115M \\\\\n& \\text{ImageNet accuracy} & 0.146 & 0.249 & 0.275 & 0.302 & 0.300 & 0.293 \\\\\n& \\text{Average accuracy} & 0.254 & 0.333 & 0.356 & 0.370 & 0.365 & 0.366 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M & 25.7M & 38.4M & 64.1M & 96.1M & 115M \\\\\n& \\text{ImageNet accuracy} & 0.192 & 0.245 & 0.261 & 0.266 & 0.267 & 0.276 \\\\\n& \\text{Average accuracy} & 0.280 & 0.330 & 0.346 & 0.342 & 0.349 & 0.356 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 128M & 128M & 128M & 128M & 128M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.286 & 0.296 & 0.297 & 0.286 & 0.250 & 0.215 \\\\\n& \\text{Average accuracy} & 0.360 & 0.357 & 0.365 & 0.349 & 0.323 & 0.293 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 30.5M & 59.5M & 83.6M & 114M & 127M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.267 & 0.310 & 0.317 & 0.296 & 0.251 & 0.212 \\\\\n& \\text{Average accuracy} & 0.343 & 0.372 & 0.368 & 0.352 & 0.313 & 0.285 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 17.1M & 32.8M & 47.7M & 75.3M & 105M & 121M \\\\\n& \\text{ImageNet accuracy} & 0.212 & 0.272 & 0.298 & 0.310 & 0.298 & 0.285 \\\\\n& \\text{Average accuracy} & 0.305 & 0.353 & 0.367 & 0.376 & 0.375 & 0.355 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 30.5M & 59.5M & 83.6M & 114M & 127M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.250 & 0.287 & 0.299 & 0.286 & 0.269 & 0.262 \\\\\n& \\text{Average accuracy} & 0.336 & 0.368 & 0.367 & 0.359 & 0.340 & 0.337 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 25.6M & 51.3M & 76.9M & 128M & - & - \\\\\n& \\text{ImageNet accuracy} & 0.238 & 0.285 & 0.297 & 0.300 & - & - \\\\\n& \\text{Average accuracy} & 0.318 & 0.358 & 0.366 & 0.356 & - & - \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & - & - & 10.0M & 16.4M & 23.6M & 27.1M \\\\\n& \\text{ImageNet accuracy} & - & - & 0.243 & 0.289 & 0.306 & 0.301 \\\\\n& \\text{Average accuracy} & - & - & 0.310 & 0.343 & 0.360 & 0.344 \\\\\n\\hline\n\\end{array}\n$$\n\n* indicates numbers obtained from [18]. Note that all resulting training sets are trained for a fixed number of steps (128M samples seen) and other training variables (e.g., architecture, hyperparameters) are kept constant. Synthetic captions are generated using pre-trained BLIP2 model with top-K sampling (K = 50) and softmax temperature 0.75. We find that at this scale, approaches that yield the best ImageNet and average accuracies leverage a combination of raw and synthetic captions.\n\nTable 5: Summary of how various filtering and mixing strategies perform on ImageNet and on average across 38 evaluation tasks in DataComp, given a 128M candidate pool (medium scale).\n\n26", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Filtered and Mixed Strategies Performance Summary", "md": "# Filtered and Mixed Strategies Performance Summary"}, {"type": "heading", "lvl": 2, "value": "Summary of how various filtering and mixing strategies perform on ImageNet and on average across 38 evaluation tasks in DataComp, given a 128M candidate pool (medium scale).", "md": "## Summary of how various filtering and mixing strategies perform on ImageNet and on average across 38 evaluation tasks in DataComp, given a 128M candidate pool (medium scale)."}, {"type": "table", "rows": [["CLIP score filtering", "10%", "20%", "30%", "50%", "75%", "90%"], ["Cosine similarity threshold", "Raw captions", "BLIP2 captions", "", "", "", ""], ["", "0.295", "0.315", "", "", "", ""], ["", "", "0.266", "0.292", "", "", ""], ["", "", "", "0.243", "0.277", "", ""], ["", "", "", "", "0.203", "0.251", ""], ["", "", "", "", "", "0.160", "0.217"], ["", "", "", "", "", "", "0.129", "0.187"]], "md": "|CLIP score filtering|10%|20%|30%|50%|75%|90%|\n|---|---|---|---|---|---|---|\n|Cosine similarity threshold|Raw captions|BLIP2 captions| | | | |\n| |0.295|0.315| | | | |\n| | |0.266|0.292| | | |\n| | | |0.243|0.277| | |\n| | | | |0.203|0.251| |\n| | | | | |0.160|0.217|\n| | | | | | |0.129|0.187|", "isPerfectTable": false, "csv": "\"CLIP score filtering\",\"10%\",\"20%\",\"30%\",\"50%\",\"75%\",\"90%\"\n\"Cosine similarity threshold\",\"Raw captions\",\"BLIP2 captions\",\"\",\"\",\"\",\"\"\n\"\",\"0.295\",\"0.315\",\"\",\"\",\"\",\"\"\n\"\",\"\",\"0.266\",\"0.292\",\"\",\"\",\"\"\n\"\",\"\",\"\",\"0.243\",\"0.277\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"0.203\",\"0.251\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"0.160\",\"0.217\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"0.129\",\"0.187\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M* & 25.7M* & 38.4M* & 64.1M* & 96.1M* & 115M* \\\\\n& \\text{ImageNet accuracy} & 0.198* & 0.260* & 0.273* & 0.254* & 0.212* & 0.188* \\\\\n& \\text{Average accuracy} & 0.277* & 0.322* & 0.328* & 0.315* & 0.285* & 0.266* \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M & 25.6M & 38.5M & 64.1M & 96.0M & 115M \\\\\n& \\text{ImageNet accuracy} & 0.146 & 0.249 & 0.275 & 0.302 & 0.300 & 0.293 \\\\\n& \\text{Average accuracy} & 0.254 & 0.333 & 0.356 & 0.370 & 0.365 & 0.366 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M & 25.7M & 38.4M & 64.1M & 96.1M & 115M \\\\\n& \\text{ImageNet accuracy} & 0.192 & 0.245 & 0.261 & 0.266 & 0.267 & 0.276 \\\\\n& \\text{Average accuracy} & 0.280 & 0.330 & 0.346 & 0.342 & 0.349 & 0.356 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 128M & 128M & 128M & 128M & 128M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.286 & 0.296 & 0.297 & 0.286 & 0.250 & 0.215 \\\\\n& \\text{Average accuracy} & 0.360 & 0.357 & 0.365 & 0.349 & 0.323 & 0.293 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 30.5M & 59.5M & 83.6M & 114M & 127M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.267 & 0.310 & 0.317 & 0.296 & 0.251 & 0.212 \\\\\n& \\text{Average accuracy} & 0.343 & 0.372 & 0.368 & 0.352 & 0.313 & 0.285 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 17.1M & 32.8M & 47.7M & 75.3M & 105M & 121M \\\\\n& \\text{ImageNet accuracy} & 0.212 & 0.272 & 0.298 & 0.310 & 0.298 & 0.285 \\\\\n& \\text{Average accuracy} & 0.305 & 0.353 & 0.367 & 0.376 & 0.375 & 0.355 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 30.5M & 59.5M & 83.6M & 114M & 127M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.250 & 0.287 & 0.299 & 0.286 & 0.269 & 0.262 \\\\\n& \\text{Average accuracy} & 0.336 & 0.368 & 0.367 & 0.359 & 0.340 & 0.337 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 25.6M & 51.3M & 76.9M & 128M & - & - \\\\\n& \\text{ImageNet accuracy} & 0.238 & 0.285 & 0.297 & 0.300 & - & - \\\\\n& \\text{Average accuracy} & 0.318 & 0.358 & 0.366 & 0.356 & - & - \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & - & - & 10.0M & 16.4M & 23.6M & 27.1M \\\\\n& \\text{ImageNet accuracy} & - & - & 0.243 & 0.289 & 0.306 & 0.301 \\\\\n& \\text{Average accuracy} & - & - & 0.310 & 0.343 & 0.360 & 0.344 \\\\\n\\hline\n\\end{array}\n$$\n\n* indicates numbers obtained from [18]. Note that all resulting training sets are trained for a fixed number of steps (128M samples seen) and other training variables (e.g., architecture, hyperparameters) are kept constant. Synthetic captions are generated using pre-trained BLIP2 model with top-K sampling (K = 50) and softmax temperature 0.75. We find that at this scale, approaches that yield the best ImageNet and average accuracies leverage a combination of raw and synthetic captions.\n\nTable 5: Summary of how various filtering and mixing strategies perform on ImageNet and on average across 38 evaluation tasks in DataComp, given a 128M candidate pool (medium scale).\n\n26", "md": "$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M* & 25.7M* & 38.4M* & 64.1M* & 96.1M* & 115M* \\\\\n& \\text{ImageNet accuracy} & 0.198* & 0.260* & 0.273* & 0.254* & 0.212* & 0.188* \\\\\n& \\text{Average accuracy} & 0.277* & 0.322* & 0.328* & 0.315* & 0.285* & 0.266* \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M & 25.6M & 38.5M & 64.1M & 96.0M & 115M \\\\\n& \\text{ImageNet accuracy} & 0.146 & 0.249 & 0.275 & 0.302 & 0.300 & 0.293 \\\\\n& \\text{Average accuracy} & 0.254 & 0.333 & 0.356 & 0.370 & 0.365 & 0.366 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 12.8M & 25.7M & 38.4M & 64.1M & 96.1M & 115M \\\\\n& \\text{ImageNet accuracy} & 0.192 & 0.245 & 0.261 & 0.266 & 0.267 & 0.276 \\\\\n& \\text{Average accuracy} & 0.280 & 0.330 & 0.346 & 0.342 & 0.349 & 0.356 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 128M & 128M & 128M & 128M & 128M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.286 & 0.296 & 0.297 & 0.286 & 0.250 & 0.215 \\\\\n& \\text{Average accuracy} & 0.360 & 0.357 & 0.365 & 0.349 & 0.323 & 0.293 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 30.5M & 59.5M & 83.6M & 114M & 127M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.267 & 0.310 & 0.317 & 0.296 & 0.251 & 0.212 \\\\\n& \\text{Average accuracy} & 0.343 & 0.372 & 0.368 & 0.352 & 0.313 & 0.285 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 17.1M & 32.8M & 47.7M & 75.3M & 105M & 121M \\\\\n& \\text{ImageNet accuracy} & 0.212 & 0.272 & 0.298 & 0.310 & 0.298 & 0.285 \\\\\n& \\text{Average accuracy} & 0.305 & 0.353 & 0.367 & 0.376 & 0.375 & 0.355 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 30.5M & 59.5M & 83.6M & 114M & 127M & 128M \\\\\n& \\text{ImageNet accuracy} & 0.250 & 0.287 & 0.299 & 0.286 & 0.269 & 0.262 \\\\\n& \\text{Average accuracy} & 0.336 & 0.368 & 0.367 & 0.359 & 0.340 & 0.337 \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & 25.6M & 51.3M & 76.9M & 128M & - & - \\\\\n& \\text{ImageNet accuracy} & 0.238 & 0.285 & 0.297 & 0.300 & - & - \\\\\n& \\text{Average accuracy} & 0.318 & 0.358 & 0.366 & 0.356 & - & - \\\\\n\\hline\n\\end{array}\n$$\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n& \\text{Training set size} & - & - & 10.0M & 16.4M & 23.6M & 27.1M \\\\\n& \\text{ImageNet accuracy} & - & - & 0.243 & 0.289 & 0.306 & 0.301 \\\\\n& \\text{Average accuracy} & - & - & 0.310 & 0.343 & 0.360 & 0.344 \\\\\n\\hline\n\\end{array}\n$$\n\n* indicates numbers obtained from [18]. Note that all resulting training sets are trained for a fixed number of steps (128M samples seen) and other training variables (e.g., architecture, hyperparameters) are kept constant. Synthetic captions are generated using pre-trained BLIP2 model with top-K sampling (K = 50) and softmax temperature 0.75. We find that at this scale, approaches that yield the best ImageNet and average accuracies leverage a combination of raw and synthetic captions.\n\nTable 5: Summary of how various filtering and mixing strategies perform on ImageNet and on average across 38 evaluation tasks in DataComp, given a 128M candidate pool (medium scale).\n\n26"}]}, {"page": 27, "text": "E               Synthetic caption analysis\n                                                                                                                                                                                                                                                            21.4 21.5\n        20\n        15                                                                                                                                                                                                                                     13.4   15.1\n     Accuracy difference\n        105                                                          0.0  0.2  0.4  0.5  0.6  0.8  1.3  2.1  2.1  2.3  2.4                 3.5  3.9  4.1  4.2  4.3  4.4  4.4  4.7  5.6  6.4  6.4  6.5  6.8  6.9  7.6  8.1\n          0\n                                                       -0.1   -0.1\n                                           -1.6  -1.5\n          5                         -3.0\n                        -7.0  -6.7\n        10                  cars                                                                                                 stl10            cifar10    vtab/dtd              gtsrb                                            sun397              mnist\n                  voc2007          vtab/pcam                                                                    vtab/pets           food101                        objectnet                      vtab/svhn\n                                                                  country211                                                                                                              imagenet-r\n                                                            wilds/fmow                           imagenet-a                            imagenetv2                      imagenet1k   imagenet-o\n                                                   renderedsst2                     fgvc_aircraft      vtab/flowers                                                                                        vtab/cifar100vtab/resisc45vtab/eurosat\n                                                                           wilds/iwildcam\n                                                                     misc/winogavil                                                                                                                fairness/geode\n                     wilds/camelyon17 vtab/clevr_count_all                        fairness/dollar_street                                      imagenet_sketch                                                  vtab/caltech101                 retrieval/flickr_1k_test\n                  vtab/kitti_closest_vehicle_distance                                        vtab/clevr_closest_object_distance                                                                                          retrieval/mscoco_2014_5k_test\nFigure 10: We find that expanding a training set of filtered raw data by using BLIP2 captions\nfor some of the discarded images improves performance on 30 out of 38 evaluation tasks, in\naddition to boosting average accuracy by 4%. We compare performance on each task between training\non the top 30% of examples with raw captions (based on CLIP score) and training on the same set of examples\nbut with the addition of BLIP2 captions for the remaining 70% images, filtered by the same CLIP score\nthreshold. In Table 4, we have shown that adding BLIP2 captions improves ImageNet accuracy by 4.4% and\naverage accuracy by 4%. With this breakdown, we find that the performance improvement applies to most of\nthe tasks in the evaluation set, especially retrieval.\nWe also investigate whether there are system-                                                                                                   100              living things\n                                                                                                                                                                 non-living things\natic differences in training with raw versus\ngenerated text when it comes to recognizing                                                                                                       80\ncertain object categories. To do so, we exam-\nine two CLIP models that perform similarly on                                                                                                  BLIP2 captions class accuracy\nImageNet (i.e., \u00b10.2%): one trained on only                                                                                                       60\nraw captions and one trained on only generated\ncaptions, both training sets have been filtered\nwith CLIP score ranking to select the top 30%                                                                                                     40\nimage-text pairs. In Figure 11, we analyze per-\nformance on each ImageNet class, categorized\nas either \u2018living\u2019 or \u2018non-living\u2019 thing based                                                                                                    20\non where the classname synset is located in\nthe WordNet hierarchy. We observe that class-\nwise classification performances are scattered                                                                                                     0\nevenly around the y = x line, indicating that                                                                                                             0                      20                     40                      60                      80\n                                                                                                                                                                                            Raw captions class accuracy\ncompared to web-crawled captions, synthetic                                                                                           Figure 11: We break down per-class performance on\ncaptions do not exhibit a particular disadvan-                                                                                        ImageNet, between a CLIP model trained on only raw\ntage on either \u2018living\u2019 or \u2018non-living\u2019 concepts.                                                                                     captions and one trained on only synthetic captions with\n                                                                                                                                      similar overall ImageNet accuracy. We find no systematic\n                                                                                                                                      trends in the performance of either model when it comes\n                                                                                                                                      to classifying \u2018living\u2019 or \u2018non-living\u2019 things.\n                                                                                                                                        27", "md": "E Synthetic caption analysis 21.4 21.5 20 15 13.4 15.1 Accuracy difference 105 0.0 0.2 0.4 0.5 0.6 0.8 1.3 2.1 2.1 2.3 2.4 3.5 3.9 4.1 4.2 4.3 4.4 4.4 4.7 5.6 6.4 6.4 6.5 6.8 6.9 7.6 8.1 0 -0.1 -0.1 -1.6 -1.5 5 -3.0 -7.0 -6.7 10 cars stl10 cifar10 vtab/dtd gtsrb sun397 mnist voc2007 vtab/pcam vtab/pets food101 objectnet vtab/svhn country211 imagenet-r wilds/fmow imagenet-a imagenetv2 imagenet1k imagenet-o renderedsst2 fgvc_aircraft vtab/flowers vtab/cifar100vtab/resisc45vtab/eurosat wilds/iwildcam misc/winogavil fairness/geode wilds/camelyon17 vtab/clevr_count_all fairness/dollar_street imagenet_sketch vtab/caltech101 retrieval/flickr_1k_test vtab/kitti_closest_vehicle_distance vtab/clevr_closest_object_distance retrieval/mscoco_2014_5k_test\n\nFigure 10: We find that expanding a training set of filtered raw data by using BLIP2 captions for some of the discarded images improves performance on 30 out of 38 evaluation tasks, in addition to boosting average accuracy by 4%. We compare performance on each task between training on the top 30% of examples with raw captions (based on CLIP score) and training on the same set of examples but with the addition of BLIP2 captions for the remaining 70% images, filtered by the same CLIP score threshold. In Table 4, we have shown that adding BLIP2 captions improves ImageNet accuracy by 4.4% and average accuracy by 4%. With this breakdown, we find that the performance improvement applies to most of the tasks in the evaluation set, especially retrieval. We also investigate whether there are systematic differences in training with raw versus generated text when it comes to recognizing certain object categories. To do so, we examine two CLIP models that perform similarly on ImageNet (i.e., \u00b10.2%): one trained on only raw captions and one trained on only generated captions, both training sets have been filtered with CLIP score ranking to select the top 30% image-text pairs. In Figure 11, we analyze performance on each ImageNet class, categorized as either \u2018living\u2019 or \u2018non-living\u2019 thing based on where the classname synset is located in the WordNet hierarchy. We observe that class-wise classification performances are scattered evenly around the y = x line, indicating that compared to web-crawled captions, synthetic captions do not exhibit a particular disadvantage on either \u2018living\u2019 or \u2018non-living\u2019 concepts. 27\n\nBLIP2 captions class accuracy 100 living things non-living things 80 60 40 20 0 0 20 40 60 80 Raw captions class accuracy Figure 11: We break down per-class performance on ImageNet, between a CLIP model trained on only raw captions and one trained on only synthetic captions with similar overall ImageNet accuracy. We find no systematic trends in the performance of either model when it comes to classifying \u2018living\u2019 or \u2018non-living\u2019 things.", "images": [], "items": [{"type": "text", "value": "E Synthetic caption analysis 21.4 21.5 20 15 13.4 15.1 Accuracy difference 105 0.0 0.2 0.4 0.5 0.6 0.8 1.3 2.1 2.1 2.3 2.4 3.5 3.9 4.1 4.2 4.3 4.4 4.4 4.7 5.6 6.4 6.4 6.5 6.8 6.9 7.6 8.1 0 -0.1 -0.1 -1.6 -1.5 5 -3.0 -7.0 -6.7 10 cars stl10 cifar10 vtab/dtd gtsrb sun397 mnist voc2007 vtab/pcam vtab/pets food101 objectnet vtab/svhn country211 imagenet-r wilds/fmow imagenet-a imagenetv2 imagenet1k imagenet-o renderedsst2 fgvc_aircraft vtab/flowers vtab/cifar100vtab/resisc45vtab/eurosat wilds/iwildcam misc/winogavil fairness/geode wilds/camelyon17 vtab/clevr_count_all fairness/dollar_street imagenet_sketch vtab/caltech101 retrieval/flickr_1k_test vtab/kitti_closest_vehicle_distance vtab/clevr_closest_object_distance retrieval/mscoco_2014_5k_test\n\nFigure 10: We find that expanding a training set of filtered raw data by using BLIP2 captions for some of the discarded images improves performance on 30 out of 38 evaluation tasks, in addition to boosting average accuracy by 4%. We compare performance on each task between training on the top 30% of examples with raw captions (based on CLIP score) and training on the same set of examples but with the addition of BLIP2 captions for the remaining 70% images, filtered by the same CLIP score threshold. In Table 4, we have shown that adding BLIP2 captions improves ImageNet accuracy by 4.4% and average accuracy by 4%. With this breakdown, we find that the performance improvement applies to most of the tasks in the evaluation set, especially retrieval. We also investigate whether there are systematic differences in training with raw versus generated text when it comes to recognizing certain object categories. To do so, we examine two CLIP models that perform similarly on ImageNet (i.e., \u00b10.2%): one trained on only raw captions and one trained on only generated captions, both training sets have been filtered with CLIP score ranking to select the top 30% image-text pairs. In Figure 11, we analyze performance on each ImageNet class, categorized as either \u2018living\u2019 or \u2018non-living\u2019 thing based on where the classname synset is located in the WordNet hierarchy. We observe that class-wise classification performances are scattered evenly around the y = x line, indicating that compared to web-crawled captions, synthetic captions do not exhibit a particular disadvantage on either \u2018living\u2019 or \u2018non-living\u2019 concepts. 27\n\nBLIP2 captions class accuracy 100 living things non-living things 80 60 40 20 0 0 20 40 60 80 Raw captions class accuracy Figure 11: We break down per-class performance on ImageNet, between a CLIP model trained on only raw captions and one trained on only synthetic captions with similar overall ImageNet accuracy. We find no systematic trends in the performance of either model when it comes to classifying \u2018living\u2019 or \u2018non-living\u2019 things.", "md": "E Synthetic caption analysis 21.4 21.5 20 15 13.4 15.1 Accuracy difference 105 0.0 0.2 0.4 0.5 0.6 0.8 1.3 2.1 2.1 2.3 2.4 3.5 3.9 4.1 4.2 4.3 4.4 4.4 4.7 5.6 6.4 6.4 6.5 6.8 6.9 7.6 8.1 0 -0.1 -0.1 -1.6 -1.5 5 -3.0 -7.0 -6.7 10 cars stl10 cifar10 vtab/dtd gtsrb sun397 mnist voc2007 vtab/pcam vtab/pets food101 objectnet vtab/svhn country211 imagenet-r wilds/fmow imagenet-a imagenetv2 imagenet1k imagenet-o renderedsst2 fgvc_aircraft vtab/flowers vtab/cifar100vtab/resisc45vtab/eurosat wilds/iwildcam misc/winogavil fairness/geode wilds/camelyon17 vtab/clevr_count_all fairness/dollar_street imagenet_sketch vtab/caltech101 retrieval/flickr_1k_test vtab/kitti_closest_vehicle_distance vtab/clevr_closest_object_distance retrieval/mscoco_2014_5k_test\n\nFigure 10: We find that expanding a training set of filtered raw data by using BLIP2 captions for some of the discarded images improves performance on 30 out of 38 evaluation tasks, in addition to boosting average accuracy by 4%. We compare performance on each task between training on the top 30% of examples with raw captions (based on CLIP score) and training on the same set of examples but with the addition of BLIP2 captions for the remaining 70% images, filtered by the same CLIP score threshold. In Table 4, we have shown that adding BLIP2 captions improves ImageNet accuracy by 4.4% and average accuracy by 4%. With this breakdown, we find that the performance improvement applies to most of the tasks in the evaluation set, especially retrieval. We also investigate whether there are systematic differences in training with raw versus generated text when it comes to recognizing certain object categories. To do so, we examine two CLIP models that perform similarly on ImageNet (i.e., \u00b10.2%): one trained on only raw captions and one trained on only generated captions, both training sets have been filtered with CLIP score ranking to select the top 30% image-text pairs. In Figure 11, we analyze performance on each ImageNet class, categorized as either \u2018living\u2019 or \u2018non-living\u2019 thing based on where the classname synset is located in the WordNet hierarchy. We observe that class-wise classification performances are scattered evenly around the y = x line, indicating that compared to web-crawled captions, synthetic captions do not exhibit a particular disadvantage on either \u2018living\u2019 or \u2018non-living\u2019 concepts. 27\n\nBLIP2 captions class accuracy 100 living things non-living things 80 60 40 20 0 0 20 40 60 80 Raw captions class accuracy Figure 11: We break down per-class performance on ImageNet, between a CLIP model trained on only raw captions and one trained on only synthetic captions with similar overall ImageNet accuracy. We find no systematic trends in the performance of either model when it comes to classifying \u2018living\u2019 or \u2018non-living\u2019 things."}]}, {"page": 28, "text": "F        Performance at Scale\n    6  1e6    BLIP2                                                                      1.2  1e8    BLIP2\n              Raw                                                                                    Raw\n    5                                                                                    1.0\n   Number of unique nouns                                                               Number of unique trigrams\n    4                                                                                    0.8\n    3                                                                                    0.6\n    2                                                                                    0.4\n    1                                                                                    0.2\n    0                                                                                    0.0\n                     105                      106                      107                               105               106        107\n                                Number of samples                                                                  Number of samples\nFigure 12: Our simple analyses of text properties suggest that the text diversity provided by\nsynthetic captions may not scale as well as that of raw captions scraped from the Internet. We\nmeasure the number of unique nouns and unique trigrams in random subsets of BLIP2 and raw captions of\nvarious sizes. We observe that on both metrics, the scaling trend for generated captions is worse than that of\nraw captions. This increasing gap in data diversity may impact the performance benefits we can expect to\nobtain from using synthetic captions, when dealing with a larger scale of training data.\nG         Experiments with LAION-COCO\nOur experiments with synthetic captions are partly inspired by the release of LAION-COCO dataset\n[45], which used BLIP [29] with various hyperparameter settings to caption LAION-5B data [46],\nand then selected the top synthetic caption for each image based on the cosine similarity output\nby OpenAI\u2019s CLIPs [40]. We pick a random set of 100M samples from LAION-COCO and train\non this set using DataComp\u2019s medium scale configuration (i.e., 128M steps), with either only the\nraw captions or only the top BLIP captions that come with the dataset. We find that training on\nBLIP captions significantly lags behind training on raw captions, measured by both ImageNet and\naverage accuracies (Figure 13). Consequently, a natural question to ask is how much of this gap can\nbe overcome with progress in image captioning models, e.g. the release of BLIP2.\nWe proceed to generating BLIP2 captions for the same set of 100M images, using only one configura-\ntion from the original hyperparameter grid in [45] due to compute constraints. Despite the lack of\nhyperparameter tuning, the new BLIP2 captions manage to close the previous ImageNet performance\ngap by 75% and come close to the average accuracy obtained from training on raw captions (see\ntable in Figure 13). Since raw data in LAION was already filtered with a CLIP score threshold of\n0.28 during the dataset construction, we next experiment with applying the same filtering to BLIP2\ncaptions, in order to control for noise quality in the caption data. On the resulting 41M images,\nusing BLIP2 captions is about as effective as using raw captions (-0.7% ImageNet accuracy and\n+0.2% average accuracy).\nWe note that LAION is considered a curated web dataset, with heavy cosine similarity filtering\nbeing one of the preprocessing steps. This in turn leads to approximately 90% of the raw data\n                                                                                28", "md": "# Performance at Scale\n\n## Performance at Scale\n\n| |BLIP2| |BLIP2|\n|---|---|---|---|\n|Raw| |Raw| |\n|Number of unique nouns|1.2|Number of unique trigrams|1.0|\n|4| |0.8| |\n|3| |0.6| |\n|2| |0.4| |\n|1| |0.2| |\n|0| |0.0| |\n| |105|106|107| |105|106|107|\n| |Number of samples| | | |Number of samples| | | |\n\nFigure 12: Our simple analyses of text properties suggest that the text diversity provided by synthetic captions may not scale as well as that of raw captions scraped from the Internet. We measure the number of unique nouns and unique trigrams in random subsets of BLIP2 and raw captions of various sizes. We observe that on both metrics, the scaling trend for generated captions is worse than that of raw captions. This increasing gap in data diversity may impact the performance benefits we can expect to obtain from using synthetic captions, when dealing with a larger scale of training data.\n\n## Experiments with LAION-COCO\n\nOur experiments with synthetic captions are partly inspired by the release of LAION-COCO dataset [45], which used BLIP [29] with various hyperparameter settings to caption LAION-5B data [46], and then selected the top synthetic caption for each image based on the cosine similarity output by OpenAI\u2019s CLIPs [40]. We pick a random set of 100M samples from LAION-COCO and train on this set using DataComp\u2019s medium scale configuration (i.e., 128M steps), with either only the raw captions or only the top BLIP captions that come with the dataset. We find that training on BLIP captions significantly lags behind training on raw captions, measured by both ImageNet and average accuracies (Figure 13). Consequently, a natural question to ask is how much of this gap can be overcome with progress in image captioning models, e.g. the release of BLIP2.\n\nWe proceed to generating BLIP2 captions for the same set of 100M images, using only one configuration from the original hyperparameter grid in [45] due to compute constraints. Despite the lack of hyperparameter tuning, the new BLIP2 captions manage to close the previous ImageNet performance gap by 75% and come close to the average accuracy obtained from training on raw captions (see table in Figure 13). Since raw data in LAION was already filtered with a CLIP score threshold of 0.28 during the dataset construction, we next experiment with applying the same filtering to BLIP2 captions, in order to control for noise quality in the caption data. On the resulting 41M images, using BLIP2 captions is about as effective as using raw captions (-0.7% ImageNet accuracy and +0.2% average accuracy).\n\nWe note that LAION is considered a curated web dataset, with heavy cosine similarity filtering being one of the preprocessing steps. This in turn leads to approximately 90% of the raw data", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Performance at Scale", "md": "# Performance at Scale"}, {"type": "heading", "lvl": 2, "value": "Performance at Scale", "md": "## Performance at Scale"}, {"type": "table", "rows": [["", "BLIP2", "", "BLIP2"], ["Raw", "", "Raw", ""], ["Number of unique nouns", "1.2", "Number of unique trigrams", "1.0"], ["4", "", "0.8", ""], ["3", "", "0.6", ""], ["2", "", "0.4", ""], ["1", "", "0.2", ""], ["0", "", "0.0", ""], ["", "105", "106", "107", "", "105", "106", "107"], ["", "Number of samples", "", "", "", "Number of samples", "", "", ""]], "md": "| |BLIP2| |BLIP2|\n|---|---|---|---|\n|Raw| |Raw| |\n|Number of unique nouns|1.2|Number of unique trigrams|1.0|\n|4| |0.8| |\n|3| |0.6| |\n|2| |0.4| |\n|1| |0.2| |\n|0| |0.0| |\n| |105|106|107| |105|106|107|\n| |Number of samples| | | |Number of samples| | | |", "isPerfectTable": false, "csv": "\"\",\"BLIP2\",\"\",\"BLIP2\"\n\"Raw\",\"\",\"Raw\",\"\"\n\"Number of unique nouns\",\"1.2\",\"Number of unique trigrams\",\"1.0\"\n\"4\",\"\",\"0.8\",\"\"\n\"3\",\"\",\"0.6\",\"\"\n\"2\",\"\",\"0.4\",\"\"\n\"1\",\"\",\"0.2\",\"\"\n\"0\",\"\",\"0.0\",\"\"\n\"\",\"105\",\"106\",\"107\",\"\",\"105\",\"106\",\"107\"\n\"\",\"Number of samples\",\"\",\"\",\"\",\"Number of samples\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 12: Our simple analyses of text properties suggest that the text diversity provided by synthetic captions may not scale as well as that of raw captions scraped from the Internet. We measure the number of unique nouns and unique trigrams in random subsets of BLIP2 and raw captions of various sizes. We observe that on both metrics, the scaling trend for generated captions is worse than that of raw captions. This increasing gap in data diversity may impact the performance benefits we can expect to obtain from using synthetic captions, when dealing with a larger scale of training data.", "md": "Figure 12: Our simple analyses of text properties suggest that the text diversity provided by synthetic captions may not scale as well as that of raw captions scraped from the Internet. We measure the number of unique nouns and unique trigrams in random subsets of BLIP2 and raw captions of various sizes. We observe that on both metrics, the scaling trend for generated captions is worse than that of raw captions. This increasing gap in data diversity may impact the performance benefits we can expect to obtain from using synthetic captions, when dealing with a larger scale of training data."}, {"type": "heading", "lvl": 2, "value": "Experiments with LAION-COCO", "md": "## Experiments with LAION-COCO"}, {"type": "text", "value": "Our experiments with synthetic captions are partly inspired by the release of LAION-COCO dataset [45], which used BLIP [29] with various hyperparameter settings to caption LAION-5B data [46], and then selected the top synthetic caption for each image based on the cosine similarity output by OpenAI\u2019s CLIPs [40]. We pick a random set of 100M samples from LAION-COCO and train on this set using DataComp\u2019s medium scale configuration (i.e., 128M steps), with either only the raw captions or only the top BLIP captions that come with the dataset. We find that training on BLIP captions significantly lags behind training on raw captions, measured by both ImageNet and average accuracies (Figure 13). Consequently, a natural question to ask is how much of this gap can be overcome with progress in image captioning models, e.g. the release of BLIP2.\n\nWe proceed to generating BLIP2 captions for the same set of 100M images, using only one configuration from the original hyperparameter grid in [45] due to compute constraints. Despite the lack of hyperparameter tuning, the new BLIP2 captions manage to close the previous ImageNet performance gap by 75% and come close to the average accuracy obtained from training on raw captions (see table in Figure 13). Since raw data in LAION was already filtered with a CLIP score threshold of 0.28 during the dataset construction, we next experiment with applying the same filtering to BLIP2 captions, in order to control for noise quality in the caption data. On the resulting 41M images, using BLIP2 captions is about as effective as using raw captions (-0.7% ImageNet accuracy and +0.2% average accuracy).\n\nWe note that LAION is considered a curated web dataset, with heavy cosine similarity filtering being one of the preprocessing steps. This in turn leads to approximately 90% of the raw data", "md": "Our experiments with synthetic captions are partly inspired by the release of LAION-COCO dataset [45], which used BLIP [29] with various hyperparameter settings to caption LAION-5B data [46], and then selected the top synthetic caption for each image based on the cosine similarity output by OpenAI\u2019s CLIPs [40]. We pick a random set of 100M samples from LAION-COCO and train on this set using DataComp\u2019s medium scale configuration (i.e., 128M steps), with either only the raw captions or only the top BLIP captions that come with the dataset. We find that training on BLIP captions significantly lags behind training on raw captions, measured by both ImageNet and average accuracies (Figure 13). Consequently, a natural question to ask is how much of this gap can be overcome with progress in image captioning models, e.g. the release of BLIP2.\n\nWe proceed to generating BLIP2 captions for the same set of 100M images, using only one configuration from the original hyperparameter grid in [45] due to compute constraints. Despite the lack of hyperparameter tuning, the new BLIP2 captions manage to close the previous ImageNet performance gap by 75% and come close to the average accuracy obtained from training on raw captions (see table in Figure 13). Since raw data in LAION was already filtered with a CLIP score threshold of 0.28 during the dataset construction, we next experiment with applying the same filtering to BLIP2 captions, in order to control for noise quality in the caption data. On the resulting 41M images, using BLIP2 captions is about as effective as using raw captions (-0.7% ImageNet accuracy and +0.2% average accuracy).\n\nWe note that LAION is considered a curated web dataset, with heavy cosine similarity filtering being one of the preprocessing steps. This in turn leads to approximately 90% of the raw data"}]}, {"page": 29, "text": "       45          BLIP1 captions (no filtering)\n                   BLIP2 captions (no filtering)\n       40          Raw captions (no filtering)                                                                                ImageNet  Average   Dataset\n                   BLIP2 captions (filter threshold=0.28)\n       35          Raw captions (filter threshold=0.28)                                                                       accuracy  accuracy  size\n      ImageNet Accuracy                                                                                 BLIP                  20.5      29.0      104M\n       30\n       25                                                                                               BLIP2                 29.3      34.6      104M\n       20                                                                                               Raw                   32.3      34.9      104M\n       15                                                                                               BLIP2 (CLIP           26.8      32.4      41M\n       10                                                                                               score \u2265        0.28)\n        5                                                                                               Raw          (CLIP    27.5      32.2      41M\n        0                 ImageNet                                     Average                          score \u2265        0.28)\nFigure 13: BLIP2 significantly closes the performance gap between BLIP captions and raw\ncaptions on LAION-COCO; when controlled for noise level, the performance difference between\nusing BLIP2 and using raw captions is actually negligible. We use BLIP2 [30] to generate captions\nfor 100M random samples from the LAION-COCO dataset [45], which already come with corresponding\nBLIP [29] captions. We find that advances in the BLIP model family help generated captions close the gap\nwith raw captions, as measured by the zero-shot performance of CLIP trained on the captions. After applying\na cosine similarity threshold of 0.28 to the BLIP2 training pool, just like how LAION data was originally\ncurated, we find that using either raw captions or synthetic captions for the resulting set of training examples\nmakes little difference (hatched columns).\nfrom Common Crawl to be discarded, according to Schuhmann et al. [46]. Since LAION only\nretains about 10% of the original candidate pool, similar experiments in DataComp [18] have shown\nthat further CLIP score filtering on these top examples will only hurt performance. In addition,\ngiven that the selected raw captions are already relatively clean (measured via image-text cosine\nsimilarity), and there is no record of datapoints that were filtered out for further experimentation,\nwe find LAION-COCO to be an unsuitable benchmark for studying the utility of synthetic captions.\nOur experiments here mainly seek to demonstrate that progress in image captioning models (e.g.,\nthe BLIP model family) can translate to better text supervision for CLIP training that rivals the\neffectiveness of using raw captions.\nH           Fairness implications of using synthetic captions\nWe examine zero-shot classification accuracy of predicting race and gender from face images in the\nFairface dataset [26], for a model trained on only filtered raw captions, one trained on only filtered\nsynthetic captions, and one trained on both. We acknowledge that there are limitations to these\nevaluations as race and gender should not be considered fixed categories.\nWith Fairface, we find that using synthetic captions improves the classification performance on the\ndisadvantaged group (e.g. female) significantly, and reduces the performance gap between male and\nfemale groups while still boosting the overall performance on all race categories (Table 6). We leave\nmore extensive study of the fairness implications of using synthetic data (including and beyond\ngender biases) to future work.\n                                                                                                29", "md": "# Document\n\n## BLIP2 significantly closes the performance gap between BLIP captions and raw captions on LAION-COCO\n\nFigure 13: BLIP2 significantly closes the performance gap between BLIP captions and raw captions on LAION-COCO; when controlled for noise level, the performance difference between using BLIP2 and using raw captions is actually negligible. We use BLIP2 [30] to generate captions for 100M random samples from the LAION-COCO dataset [45], which already come with corresponding BLIP [29] captions. We find that advances in the BLIP model family help generated captions close the gap with raw captions, as measured by the zero-shot performance of CLIP trained on the captions. After applying a cosine similarity threshold of 0.28 to the BLIP2 training pool, just like how LAION data was originally curated, we find that using either raw captions or synthetic captions for the resulting set of training examples makes little difference (hatched columns).\n\nfrom Common Crawl to be discarded, according to Schuhmann et al. [46]. Since LAION only retains about 10% of the original candidate pool, similar experiments in DataComp [18] have shown that further CLIP score filtering on these top examples will only hurt performance. In addition, given that the selected raw captions are already relatively clean (measured via image-text cosine similarity), and there is no record of datapoints that were filtered out for further experimentation, we find LAION-COCO to be an unsuitable benchmark for studying the utility of synthetic captions. Our experiments here mainly seek to demonstrate that progress in image captioning models (e.g., the BLIP model family) can translate to better text supervision for CLIP training that rivals the effectiveness of using raw captions.\n\n## Fairness implications of using synthetic captions\n\nWe examine zero-shot classification accuracy of predicting race and gender from face images in the Fairface dataset [26], for a model trained on only filtered raw captions, one trained on only filtered synthetic captions, and one trained on both. We acknowledge that there are limitations to these evaluations as race and gender should not be considered fixed categories.\n\nWith Fairface, we find that using synthetic captions improves the classification performance on the disadvantaged group (e.g. female) significantly, and reduces the performance gap between male and female groups while still boosting the overall performance on all race categories (Table 6). We leave more extensive study of the fairness implications of using synthetic data (including and beyond gender biases) to future work.\n\n### Table 6\n\n| |Filtered Raw Captions|Filtered Synthetic Captions|Both|\n|---|---|---|---|\n|Classification Performance on Disadvantaged Group (e.g. female)|High|Significantly Improved| |\n|Performance Gap between Male and Female Groups|Large|Reduced| |\n|Overall Performance on All Race Categories| |Boosted| |", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "BLIP2 significantly closes the performance gap between BLIP captions and raw captions on LAION-COCO", "md": "## BLIP2 significantly closes the performance gap between BLIP captions and raw captions on LAION-COCO"}, {"type": "text", "value": "Figure 13: BLIP2 significantly closes the performance gap between BLIP captions and raw captions on LAION-COCO; when controlled for noise level, the performance difference between using BLIP2 and using raw captions is actually negligible. We use BLIP2 [30] to generate captions for 100M random samples from the LAION-COCO dataset [45], which already come with corresponding BLIP [29] captions. We find that advances in the BLIP model family help generated captions close the gap with raw captions, as measured by the zero-shot performance of CLIP trained on the captions. After applying a cosine similarity threshold of 0.28 to the BLIP2 training pool, just like how LAION data was originally curated, we find that using either raw captions or synthetic captions for the resulting set of training examples makes little difference (hatched columns).\n\nfrom Common Crawl to be discarded, according to Schuhmann et al. [46]. Since LAION only retains about 10% of the original candidate pool, similar experiments in DataComp [18] have shown that further CLIP score filtering on these top examples will only hurt performance. In addition, given that the selected raw captions are already relatively clean (measured via image-text cosine similarity), and there is no record of datapoints that were filtered out for further experimentation, we find LAION-COCO to be an unsuitable benchmark for studying the utility of synthetic captions. Our experiments here mainly seek to demonstrate that progress in image captioning models (e.g., the BLIP model family) can translate to better text supervision for CLIP training that rivals the effectiveness of using raw captions.", "md": "Figure 13: BLIP2 significantly closes the performance gap between BLIP captions and raw captions on LAION-COCO; when controlled for noise level, the performance difference between using BLIP2 and using raw captions is actually negligible. We use BLIP2 [30] to generate captions for 100M random samples from the LAION-COCO dataset [45], which already come with corresponding BLIP [29] captions. We find that advances in the BLIP model family help generated captions close the gap with raw captions, as measured by the zero-shot performance of CLIP trained on the captions. After applying a cosine similarity threshold of 0.28 to the BLIP2 training pool, just like how LAION data was originally curated, we find that using either raw captions or synthetic captions for the resulting set of training examples makes little difference (hatched columns).\n\nfrom Common Crawl to be discarded, according to Schuhmann et al. [46]. Since LAION only retains about 10% of the original candidate pool, similar experiments in DataComp [18] have shown that further CLIP score filtering on these top examples will only hurt performance. In addition, given that the selected raw captions are already relatively clean (measured via image-text cosine similarity), and there is no record of datapoints that were filtered out for further experimentation, we find LAION-COCO to be an unsuitable benchmark for studying the utility of synthetic captions. Our experiments here mainly seek to demonstrate that progress in image captioning models (e.g., the BLIP model family) can translate to better text supervision for CLIP training that rivals the effectiveness of using raw captions."}, {"type": "heading", "lvl": 2, "value": "Fairness implications of using synthetic captions", "md": "## Fairness implications of using synthetic captions"}, {"type": "text", "value": "We examine zero-shot classification accuracy of predicting race and gender from face images in the Fairface dataset [26], for a model trained on only filtered raw captions, one trained on only filtered synthetic captions, and one trained on both. We acknowledge that there are limitations to these evaluations as race and gender should not be considered fixed categories.\n\nWith Fairface, we find that using synthetic captions improves the classification performance on the disadvantaged group (e.g. female) significantly, and reduces the performance gap between male and female groups while still boosting the overall performance on all race categories (Table 6). We leave more extensive study of the fairness implications of using synthetic data (including and beyond gender biases) to future work.", "md": "We examine zero-shot classification accuracy of predicting race and gender from face images in the Fairface dataset [26], for a model trained on only filtered raw captions, one trained on only filtered synthetic captions, and one trained on both. We acknowledge that there are limitations to these evaluations as race and gender should not be considered fixed categories.\n\nWith Fairface, we find that using synthetic captions improves the classification performance on the disadvantaged group (e.g. female) significantly, and reduces the performance gap between male and female groups while still boosting the overall performance on all race categories (Table 6). We leave more extensive study of the fairness implications of using synthetic data (including and beyond gender biases) to future work."}, {"type": "heading", "lvl": 3, "value": "Table 6", "md": "### Table 6"}, {"type": "table", "rows": [["", "Filtered Raw Captions", "Filtered Synthetic Captions", "Both"], ["Classification Performance on Disadvantaged Group (e.g. female)", "High", "Significantly Improved", ""], ["Performance Gap between Male and Female Groups", "Large", "Reduced", ""], ["Overall Performance on All Race Categories", "", "Boosted", ""]], "md": "| |Filtered Raw Captions|Filtered Synthetic Captions|Both|\n|---|---|---|---|\n|Classification Performance on Disadvantaged Group (e.g. female)|High|Significantly Improved| |\n|Performance Gap between Male and Female Groups|Large|Reduced| |\n|Overall Performance on All Race Categories| |Boosted| |", "isPerfectTable": true, "csv": "\"\",\"Filtered Raw Captions\",\"Filtered Synthetic Captions\",\"Both\"\n\"Classification Performance on Disadvantaged Group (e.g. female)\",\"High\",\"Significantly Improved\",\"\"\n\"Performance Gap between Male and Female Groups\",\"Large\",\"Reduced\",\"\"\n\"Overall Performance on All Race Categories\",\"\",\"Boosted\",\"\""}]}, {"page": 30, "text": "   Gender     Model                                                         Race\n                                        Black     White    Indian    Latino/      Middle       South East  East\n                                                                     Hispanic     Eastern      Asian       Asian\n              Raw (top 30%)             93.0      88.8     91.2      90.8         92.3         85.3        81.3\n   Male       BLIP2 (top 30%)           87.2      73.7     77.2      74.9         78.6         72.0        64.0\n              Raw    (top   30%)    +   90.5      75.0     79.7      79.4         81.1         72.4        65.3\n              BLIP2 (70%, filtered)\n              Raw (top 30%)             20.3      47.1     35.1      42.0         40.9         44.9        56.8\n   Female     BLIP2 (top 30%)           36.9      70.8     57.9      67.5         67.4         64.1        78.4\n              Raw    (top   30%)    +   32.9      74.8     56.5      66.3         67.9         67.8        81.9\n              BLIP2 (70%, filtered)\n              Raw (top 30%)             56.7      68.0     63.2      66.4         66.6         65.1        69.1\n   Overall    BLIP2 (top 30%)           62.1      72.3     67.6      71.2         73.0         68.1        71.2\n              Raw    (top   30%)    +   61.7      74.9     68.1      72.9         74.5         70.1        73.6\n              BLIP2 (70%, filtered)\nTable 6: Using synthetic captions improves classification performance on Fairface for the minority group\n(i.e., female) across all race categories.\n                                                          30", "md": "|Gender|Model|Race|\n|---|---|---|\n| |Raw (top 30%)|Black|White|Indian|Latino/Hispanic|Middle Eastern|South East Asian|East Asian|\n| |93.0|88.8|91.2|90.8|92.3|85.3|81.3|\n|Male|BLIP2 (top 30%)|87.2|73.7|77.2|74.9|78.6|72.0|64.0|\n| |Raw (top 30%) + BLIP2 (70%, filtered)|90.5|75.0|79.7|79.4|81.1|72.4|65.3|\n| |Raw (top 30%)|20.3|47.1|35.1|42.0|40.9|44.9|56.8|\n|Female|BLIP2 (top 30%)|36.9|70.8|57.9|67.5|67.4|64.1|78.4|\n| |Raw (top 30%) + BLIP2 (70%, filtered)|32.9|74.8|56.5|66.3|67.9|67.8|81.9|\n| |Raw (top 30%)|56.7|68.0|63.2|66.4|66.6|65.1|69.1|\n|Overall|BLIP2 (top 30%)|62.1|72.3|67.6|71.2|73.0|68.1|71.2|\n| |Raw (top 30%) + BLIP2 (70%, filtered)|61.7|74.9|68.1|72.9|74.5|70.1|73.6|\n\nTable 6: Using synthetic captions improves classification performance on Fairface for the minority group (i.e., female) across all race categories.\n\n$$30$$", "images": [], "items": [{"type": "table", "rows": [["Gender", "Model", "Race"], ["", "Raw (top 30%)", "Black", "White", "Indian", "Latino/Hispanic", "Middle Eastern", "South East Asian", "East Asian"], ["", "93.0", "88.8", "91.2", "90.8", "92.3", "85.3", "81.3"], ["Male", "BLIP2 (top 30%)", "87.2", "73.7", "77.2", "74.9", "78.6", "72.0", "64.0"], ["", "Raw (top 30%) + BLIP2 (70%, filtered)", "90.5", "75.0", "79.7", "79.4", "81.1", "72.4", "65.3"], ["", "Raw (top 30%)", "20.3", "47.1", "35.1", "42.0", "40.9", "44.9", "56.8"], ["Female", "BLIP2 (top 30%)", "36.9", "70.8", "57.9", "67.5", "67.4", "64.1", "78.4"], ["", "Raw (top 30%) + BLIP2 (70%, filtered)", "32.9", "74.8", "56.5", "66.3", "67.9", "67.8", "81.9"], ["", "Raw (top 30%)", "56.7", "68.0", "63.2", "66.4", "66.6", "65.1", "69.1"], ["Overall", "BLIP2 (top 30%)", "62.1", "72.3", "67.6", "71.2", "73.0", "68.1", "71.2"], ["", "Raw (top 30%) + BLIP2 (70%, filtered)", "61.7", "74.9", "68.1", "72.9", "74.5", "70.1", "73.6"]], "md": "|Gender|Model|Race|\n|---|---|---|\n| |Raw (top 30%)|Black|White|Indian|Latino/Hispanic|Middle Eastern|South East Asian|East Asian|\n| |93.0|88.8|91.2|90.8|92.3|85.3|81.3|\n|Male|BLIP2 (top 30%)|87.2|73.7|77.2|74.9|78.6|72.0|64.0|\n| |Raw (top 30%) + BLIP2 (70%, filtered)|90.5|75.0|79.7|79.4|81.1|72.4|65.3|\n| |Raw (top 30%)|20.3|47.1|35.1|42.0|40.9|44.9|56.8|\n|Female|BLIP2 (top 30%)|36.9|70.8|57.9|67.5|67.4|64.1|78.4|\n| |Raw (top 30%) + BLIP2 (70%, filtered)|32.9|74.8|56.5|66.3|67.9|67.8|81.9|\n| |Raw (top 30%)|56.7|68.0|63.2|66.4|66.6|65.1|69.1|\n|Overall|BLIP2 (top 30%)|62.1|72.3|67.6|71.2|73.0|68.1|71.2|\n| |Raw (top 30%) + BLIP2 (70%, filtered)|61.7|74.9|68.1|72.9|74.5|70.1|73.6|", "isPerfectTable": false, "csv": "\"Gender\",\"Model\",\"Race\"\n\"\",\"Raw (top 30%)\",\"Black\",\"White\",\"Indian\",\"Latino/Hispanic\",\"Middle Eastern\",\"South East Asian\",\"East Asian\"\n\"\",\"93.0\",\"88.8\",\"91.2\",\"90.8\",\"92.3\",\"85.3\",\"81.3\"\n\"Male\",\"BLIP2 (top 30%)\",\"87.2\",\"73.7\",\"77.2\",\"74.9\",\"78.6\",\"72.0\",\"64.0\"\n\"\",\"Raw (top 30%) + BLIP2 (70%, filtered)\",\"90.5\",\"75.0\",\"79.7\",\"79.4\",\"81.1\",\"72.4\",\"65.3\"\n\"\",\"Raw (top 30%)\",\"20.3\",\"47.1\",\"35.1\",\"42.0\",\"40.9\",\"44.9\",\"56.8\"\n\"Female\",\"BLIP2 (top 30%)\",\"36.9\",\"70.8\",\"57.9\",\"67.5\",\"67.4\",\"64.1\",\"78.4\"\n\"\",\"Raw (top 30%) + BLIP2 (70%, filtered)\",\"32.9\",\"74.8\",\"56.5\",\"66.3\",\"67.9\",\"67.8\",\"81.9\"\n\"\",\"Raw (top 30%)\",\"56.7\",\"68.0\",\"63.2\",\"66.4\",\"66.6\",\"65.1\",\"69.1\"\n\"Overall\",\"BLIP2 (top 30%)\",\"62.1\",\"72.3\",\"67.6\",\"71.2\",\"73.0\",\"68.1\",\"71.2\"\n\"\",\"Raw (top 30%) + BLIP2 (70%, filtered)\",\"61.7\",\"74.9\",\"68.1\",\"72.9\",\"74.5\",\"70.1\",\"73.6\""}, {"type": "text", "value": "Table 6: Using synthetic captions improves classification performance on Fairface for the minority group (i.e., female) across all race categories.\n\n$$30$$", "md": "Table 6: Using synthetic captions improves classification performance on Fairface for the minority group (i.e., female) across all race categories.\n\n$$30$$"}]}], "job_id": "cfa0a3dc-7376-427b-8ee4-034064e75a5e", "file_path": "./corpus/2307.10350.pdf"}