{"pages": [{"page": 1, "text": "                                Solving Linear Inverse Problems Provably via\n                            Posterior Sampling with Latent Diffusion Models\n                                             Litu Rout    \u2217    Negin Raoof      \u2020   Giannis Daras       \u2021\n                          Constantine Caramanis            \u00a7   Alexandros G. Dimakis            \u00b6   Sanjay Shakkottai        \u2016\n                                                   The University of Texas at Austin\n                                                                     Abstract\n                     We present the first framework to solve linear inverse problems leveraging pre-trained                  latent\n                diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply topixel-space\n                diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a\n                linear model setting. The algorithmic insight obtained from our analysis extends to more general\n                settings often considered in practice. Experimentally, we outperform previously proposed posterior\n                sampling algorithms in a wide variety of problems including random inpainting, block inpainting,\n                denoising, deblurring, destriping, and super-resolution.\n         1      Introduction\n         We study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising,\n         inpainting, compressed sensing and super-resolution. There are two classes of approaches for inverse\n         problems: supervised methods where a restoration model is trained to solve the task at hand [35,\n         37, 52, 30], and unsupervised methods that use the prior learned by a generative model to guide the\n         restoration process [49, 38, 5, 32, 11, 26]; see also the survey of Ongie et al. [34] and references therein.\n             The second family of unsupervised methods has gained popularity because: (i) general-domain\n         foundation generative models have become widely available, (ii) unsupervised methods do not require\n         any training to solve inverse problems and leverage the massive data and compute investment of\n         pre-trained models and (iii) generative models                  sample    from the posterior-distribution, mitigating\narXiv:2307.00619v1 [cs.LG] 2 Jul 2023certain pitfalls of likelihood-maximization methods such as bias in the reconstructions [33, 24] and\n         regression to the mean [23, 22].\n             Diffusion models have emerged as a powerful new approach to generative modeling [44, 45, 46, 20,\n         28, 18, 51]. This family of generative models works by first corrupting the data distribution                              p0(x  0)\n         using an It\u00f4 Stochastic Differential Equation (SDE),                  dx  =  f(x, t)dt   +   g(t)dw, and then by learning\n         the score-function,      \u2207 x tlog  pt(x t), at all levels   t, using Denoising Score Matching (DSM) [21, 50]. The\n         seminal result of Anderson [1] shows that we can reverse the corruption process, i.e., start with noise\n         and then sample from the data distribution, by running another It\u00f4 SDE. The SDE that corrupts\n            \u2217litu.rout@utexas.edu\n            \u2020neginmr@utexas.edu\n            \u2021giannisdaras@utexas.edu\n            \u00a7constantine@utexas.edu\n            \u00b6dimakis@austin.utexas.edu\n            \u2016sanjay.shakkottai@utexas.edu                                 1", "md": "# Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\n\n## Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models\n\nLitu Rout \u2217 Negin Raoof \u2020 Giannis Daras \u2021\n\nConstantine Caramanis \u00a7 Alexandros G. Dimakis \u00b6 Sanjay Shakkottai \u2016\n\nThe University of Texas at Austin\n\n### Abstract\n\nWe present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.\n\n### Introduction\n\nWe study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process; see also the survey of Ongie et al. and references therein.\n\nThe second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\n\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution p0(x0) using an It\u00f4 Stochastic Differential Equation (SDE), dx = f(x, t)dt + g(t)dw, and then by learning the score-function, \u2207xt log pt(xt), at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\u00f4 SDE. The SDE that corrupts\n\nContact:\n\n- litu.rout@utexas.edu\n- neginmr@utexas.edu\n- giannisdaras@utexas.edu\n- constantine@utexas.edu\n- dimakis@austin.utexas.edu\n- sanjay.shakkottai@utexas.edu", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models", "md": "# Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models"}, {"type": "heading", "lvl": 2, "value": "Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models", "md": "## Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models"}, {"type": "text", "value": "Litu Rout \u2217 Negin Raoof \u2020 Giannis Daras \u2021\n\nConstantine Caramanis \u00a7 Alexandros G. Dimakis \u00b6 Sanjay Shakkottai \u2016\n\nThe University of Texas at Austin", "md": "Litu Rout \u2217 Negin Raoof \u2020 Giannis Daras \u2021\n\nConstantine Caramanis \u00a7 Alexandros G. Dimakis \u00b6 Sanjay Shakkottai \u2016\n\nThe University of Texas at Austin"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "We present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.", "md": "We present the first framework to solve linear inverse problems leveraging pre-trained latent diffusion models. Previously proposed algorithms (such as DPS and DDRM) only apply to pixel-space diffusion models. We theoretically analyze our algorithm showing provable sample recovery in a linear model setting. The algorithmic insight obtained from our analysis extends to more general settings often considered in practice. Experimentally, we outperform previously proposed posterior sampling algorithms in a wide variety of problems including random inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "We study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process; see also the survey of Ongie et al. and references therein.\n\nThe second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\n\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution p0(x0) using an It\u00f4 Stochastic Differential Equation (SDE), dx = f(x, t)dt + g(t)dw, and then by learning the score-function, \u2207xt log pt(xt), at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\u00f4 SDE. The SDE that corrupts\n\nContact:\n\n- litu.rout@utexas.edu\n- neginmr@utexas.edu\n- giannisdaras@utexas.edu\n- constantine@utexas.edu\n- dimakis@austin.utexas.edu\n- sanjay.shakkottai@utexas.edu", "md": "We study the use of pre-trained latent diffusion models to solve linear inverse problems such as denoising, inpainting, compressed sensing, and super-resolution. There are two classes of approaches for inverse problems: supervised methods where a restoration model is trained to solve the task at hand, and unsupervised methods that use the prior learned by a generative model to guide the restoration process; see also the survey of Ongie et al. and references therein.\n\nThe second family of unsupervised methods has gained popularity because: (i) general-domain foundation generative models have become widely available, (ii) unsupervised methods do not require any training to solve inverse problems and leverage the massive data and compute investment of pre-trained models and (iii) generative models sample from the posterior-distribution, mitigating certain pitfalls of likelihood-maximization methods such as bias in the reconstructions and regression to the mean.\n\nDiffusion models have emerged as a powerful new approach to generative modeling. This family of generative models works by first corrupting the data distribution p0(x0) using an It\u00f4 Stochastic Differential Equation (SDE), dx = f(x, t)dt + g(t)dw, and then by learning the score-function, \u2207xt log pt(xt), at all levels t, using Denoising Score Matching (DSM). The seminal result of Anderson shows that we can reverse the corruption process, i.e., start with noise and then sample from the data distribution, by running another It\u00f4 SDE. The SDE that corrupts\n\nContact:\n\n- litu.rout@utexas.edu\n- neginmr@utexas.edu\n- giannisdaras@utexas.edu\n- constantine@utexas.edu\n- dimakis@austin.utexas.edu\n- sanjay.shakkottai@utexas.edu"}]}, {"page": 2, "text": "                              SOP\nFigure 1: Overall pipeline of our proposed framework from left to right. Given an image (left) and a\nuser defined mask (center), our algorithm inpaints the masked region (right). The known part of the\nimages are unaltered (see Appendix B for web demo and image sources).\n                                                                 2", "md": "# SOP\n\nFigure 1: Overall pipeline of our proposed framework from left to right. Given an image (left) and a\nuser defined mask (center), our algorithm inpaints the masked region (right). The known part of the\nimages are unaltered (see Appendix B for web demo and image sources).\n\n$$2$$", "images": [{"name": "img_p1_1", "height": 435, "width": 435}, {"name": "img_p1_2", "height": 435, "width": 435}, {"name": "img_p1_3", "height": 435, "width": 435}, {"name": "img_p1_4", "height": 435, "width": 435}, {"name": "img_p1_5", "height": 435, "width": 435}, {"name": "img_p1_6", "height": 435, "width": 435}, {"name": "img_p1_7", "height": 435, "width": 435}, {"name": "img_p1_8", "height": 435, "width": 435}, {"name": "img_p1_9", "height": 435, "width": 435}, {"name": "img_p1_10", "height": 435, "width": 435}, {"name": "img_p1_11", "height": 433, "width": 435}, {"name": "img_p1_12", "height": 435, "width": 435}], "items": [{"type": "heading", "lvl": 1, "value": "SOP", "md": "# SOP"}, {"type": "text", "value": "Figure 1: Overall pipeline of our proposed framework from left to right. Given an image (left) and a\nuser defined mask (center), our algorithm inpaints the masked region (right). The known part of the\nimages are unaltered (see Appendix B for web demo and image sources).\n\n$$2$$", "md": "Figure 1: Overall pipeline of our proposed framework from left to right. Given an image (left) and a\nuser defined mask (center), our algorithm inpaints the masked region (right). The known part of the\nimages are unaltered (see Appendix B for web demo and image sources).\n\n$$2$$"}]}, {"page": 3, "text": "the data is often termed as Forward SDE and its reverse as Reverse SDE [46]. The latter depends\non the score-function         \u2207 x tlog  pt(x t)  that we learn through DSM. In [8, 9], the authors provided a\nnon-asymptotic analysis for the sampling of diffusion models when the score-function is only learned\napproximately.\n     The success of diffusion models sparked the interest to investigate how we can use them to solve\ninverse problems. Song et al. [46] showed that given measurements                        y  =  Ax  0 +  \u03c3 yn, we can provably\nsample from the distribution             p0 (x 0|y)  by running a modified Reverse SDE that depends on the\nunconditional score        \u2207 x tlog  pt(x t) and the term       \u2207 x tlog  p(y|x  t). The latter term captures how much\nthe current iterate explains the measurements and it is intractable even for linear inverse problems\nwithout assumptions on the distribution              p0(x  0) [11, 14]. To deal with the intractability of the problem,\na series of approximation algorithms have been developed [22, 11, 2, 13, 26, 10, 6, 43, 12, 27] for\nsolving (linear and non-linear) inverse problems with diffusion models.                           These algorithms use pre-\ntrained diffusion models as flexible priors for the data distribution to effectively solve problems such as\ninpainting, deblurring, super-resolution among others.\n     Recently, diffusion models have been generalized to learn to invert non-Markovian and non-linear\ncorruption processes [16, 15, 3]. One instance of this generalization is the family of Latent Diffusion\nModels (LDMs) [39]. LDMs project the data into some latent space,                      z0  =  E(x  0), perform the diffusion in\nthe latent space and use a decoder,          D(z  0 ), to move back to the pixel space. LDMs power state-of-the-art\nfoundation models such as Stable Diffusion [39] and have enabled a wide-range of applications across\nmany data modalities including images [39], video [4], audio [29] and medical domain distributions\n(e.g., for MRI and proteins) [36, 48]. Unfortunately, none of the existing algorithms for solving inverse\nproblems works with Latent Diffusion Models.                     Hence, to use a foundation model, such as Stable\nDiffusion, for some inverse problem, one needs to perform finetuning for each task of interest.\n     In this paper, we present the first framework to solve general inverse problems with pre-trained\nlatent   diffusion models. Our main idea is to extend DPS by adding an extra gradient update step to\nguide the diffusion process to sample latents for which the decoding-encoding map is not lossy. By\nharnessing the power of available foundation models, we are able to outperform previous approaches\nwithout finetuning across a wide range of problems (see Figure 1 and 2).\nOur contributions are as follows:\n   (i) We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve linear\n        inverse problem when the degradation operator is known.\n  (ii) We theoretically analyze our algorithm and show provable sample recovery in a linear model\n        setting with two-step diffusion processes.\n (iii) We achieve a new state-of-the-art for solving inverse problems with latent diffusion models, out-\n        performing previous approaches for inpainting, block inpainting, denoising, deblurring, destriping,\n        and super-resolution.      1\n2      Background and Method\nNotation:       Bold lower-case      x, bold upper-case       X, and normal lower case           x denote a vector, a matrix,\nand a scalar variable, respectively. We denote by                   \u2299  element-wise multiplication.            D(x)    represents\na diagonal matrix with entries            x.   We use     E(.)  for the encoder and         D(.)   for the decoder.       E\u266fp   is a\npushforward measure of          p, i.e., for every    x \u2208  p, the sample     E(x)    is a sample from      E\u266fp. We use arrows\nin Section 3 to distinguish random variables of the forward (\u2192) and the reverse process (\u2190).\n    1The source code is available at:   https://github.com/LituRout/PSLD          and a web application for image inpainting is\navailable at:  https://huggingface.co/spaces/PSLD/PSLD.           3", "md": "# Document\n\nThe data is often termed as Forward SDE and its reverse as Reverse SDE [46]. The latter depends on the score-function $$\\nabla_x t \\log p_t(x_t)$$ that we learn through DSM. In [8, 9], the authors provided a non-asymptotic analysis for the sampling of diffusion models when the score-function is only learned approximately.\n\nThe success of diffusion models sparked the interest to investigate how we can use them to solve inverse problems. Song et al. [46] showed that given measurements $$y = Ax_0 + \\sigma yn$$, we can provably sample from the distribution $$p_0(x_0|y)$$ by running a modified Reverse SDE that depends on the unconditional score $$\\nabla_x t \\log p_t(x_t)$$ and the term $$\\nabla_x t \\log p(y|x_t)$$. The latter term captures how much the current iterate explains the measurements and it is intractable even for linear inverse problems without assumptions on the distribution $$p_0(x_0)$$ [11, 14]. To deal with the intractability of the problem, a series of approximation algorithms have been developed [22, 11, 2, 13, 26, 10, 6, 43, 12, 27] for solving (linear and non-linear) inverse problems with diffusion models. These algorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively solve problems such as inpainting, deblurring, super-resolution among others.\n\nRecently, diffusion models have been generalized to learn to invert non-Markovian and non-linear corruption processes [16, 15, 3]. One instance of this generalization is the family of Latent Diffusion Models (LDMs) [39]. LDMs project the data into some latent space, $$z_0 = E(x_0)$$, perform the diffusion in the latent space and use a decoder, $$D(z_0)$$, to move back to the pixel space. LDMs power state-of-the-art foundation models such as Stable Diffusion [39] and have enabled a wide-range of applications across many data modalities including images [39], video [4], audio [29] and medical domain distributions (e.g., for MRI and proteins) [36, 48]. Unfortunately, none of the existing algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to use a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform finetuning for each task of interest.\n\nIn this paper, we present the first framework to solve general inverse problems with pre-trained latent diffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the diffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing the power of available foundation models, we are able to outperform previous approaches without finetuning across a wide range of problems.\n\nOur contributions are as follows:\n\n1. We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve linear inverse problem when the degradation operator is known.\n2. We theoretically analyze our algorithm and show provable sample recovery in a linear model setting with two-step diffusion processes.\n3. We achieve a new state-of-the-art for solving inverse problems with latent diffusion models, out-performing previous approaches for inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.\n\n## Background and Method\n\nNotation: Bold lower-case $$x$$, bold upper-case $$X$$, and normal lower case $$x$$ denote a vector, a matrix, and a scalar variable, respectively. We denote by $$\\odot$$ element-wise multiplication. $$D(x)$$ represents a diagonal matrix with entries $$x$$. We use $$E(.)$$ for the encoder and $$D(.)$$ for the decoder. $$E^\\#p$$ is a pushforward measure of $$p$$, i.e., for every $$x \\in p$$, the sample $$E(x)$$ is a sample from $$E^\\#p$$. We use arrows in Section 3 to distinguish random variables of the forward (\u2192) and the reverse process (\u2190).\n\n1The source code is available at: https://github.com/LituRout/PSLD and a web application for image inpainting is available at: https://huggingface.co/spaces/PSLD/PSLD.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "The data is often termed as Forward SDE and its reverse as Reverse SDE [46]. The latter depends on the score-function $$\\nabla_x t \\log p_t(x_t)$$ that we learn through DSM. In [8, 9], the authors provided a non-asymptotic analysis for the sampling of diffusion models when the score-function is only learned approximately.\n\nThe success of diffusion models sparked the interest to investigate how we can use them to solve inverse problems. Song et al. [46] showed that given measurements $$y = Ax_0 + \\sigma yn$$, we can provably sample from the distribution $$p_0(x_0|y)$$ by running a modified Reverse SDE that depends on the unconditional score $$\\nabla_x t \\log p_t(x_t)$$ and the term $$\\nabla_x t \\log p(y|x_t)$$. The latter term captures how much the current iterate explains the measurements and it is intractable even for linear inverse problems without assumptions on the distribution $$p_0(x_0)$$ [11, 14]. To deal with the intractability of the problem, a series of approximation algorithms have been developed [22, 11, 2, 13, 26, 10, 6, 43, 12, 27] for solving (linear and non-linear) inverse problems with diffusion models. These algorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively solve problems such as inpainting, deblurring, super-resolution among others.\n\nRecently, diffusion models have been generalized to learn to invert non-Markovian and non-linear corruption processes [16, 15, 3]. One instance of this generalization is the family of Latent Diffusion Models (LDMs) [39]. LDMs project the data into some latent space, $$z_0 = E(x_0)$$, perform the diffusion in the latent space and use a decoder, $$D(z_0)$$, to move back to the pixel space. LDMs power state-of-the-art foundation models such as Stable Diffusion [39] and have enabled a wide-range of applications across many data modalities including images [39], video [4], audio [29] and medical domain distributions (e.g., for MRI and proteins) [36, 48]. Unfortunately, none of the existing algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to use a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform finetuning for each task of interest.\n\nIn this paper, we present the first framework to solve general inverse problems with pre-trained latent diffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the diffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing the power of available foundation models, we are able to outperform previous approaches without finetuning across a wide range of problems.\n\nOur contributions are as follows:\n\n1. We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve linear inverse problem when the degradation operator is known.\n2. We theoretically analyze our algorithm and show provable sample recovery in a linear model setting with two-step diffusion processes.\n3. We achieve a new state-of-the-art for solving inverse problems with latent diffusion models, out-performing previous approaches for inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution.", "md": "The data is often termed as Forward SDE and its reverse as Reverse SDE [46]. The latter depends on the score-function $$\\nabla_x t \\log p_t(x_t)$$ that we learn through DSM. In [8, 9], the authors provided a non-asymptotic analysis for the sampling of diffusion models when the score-function is only learned approximately.\n\nThe success of diffusion models sparked the interest to investigate how we can use them to solve inverse problems. Song et al. [46] showed that given measurements $$y = Ax_0 + \\sigma yn$$, we can provably sample from the distribution $$p_0(x_0|y)$$ by running a modified Reverse SDE that depends on the unconditional score $$\\nabla_x t \\log p_t(x_t)$$ and the term $$\\nabla_x t \\log p(y|x_t)$$. The latter term captures how much the current iterate explains the measurements and it is intractable even for linear inverse problems without assumptions on the distribution $$p_0(x_0)$$ [11, 14]. To deal with the intractability of the problem, a series of approximation algorithms have been developed [22, 11, 2, 13, 26, 10, 6, 43, 12, 27] for solving (linear and non-linear) inverse problems with diffusion models. These algorithms use pre-trained diffusion models as flexible priors for the data distribution to effectively solve problems such as inpainting, deblurring, super-resolution among others.\n\nRecently, diffusion models have been generalized to learn to invert non-Markovian and non-linear corruption processes [16, 15, 3]. One instance of this generalization is the family of Latent Diffusion Models (LDMs) [39]. LDMs project the data into some latent space, $$z_0 = E(x_0)$$, perform the diffusion in the latent space and use a decoder, $$D(z_0)$$, to move back to the pixel space. LDMs power state-of-the-art foundation models such as Stable Diffusion [39] and have enabled a wide-range of applications across many data modalities including images [39], video [4], audio [29] and medical domain distributions (e.g., for MRI and proteins) [36, 48]. Unfortunately, none of the existing algorithms for solving inverse problems works with Latent Diffusion Models. Hence, to use a foundation model, such as Stable Diffusion, for some inverse problem, one needs to perform finetuning for each task of interest.\n\nIn this paper, we present the first framework to solve general inverse problems with pre-trained latent diffusion models. Our main idea is to extend DPS by adding an extra gradient update step to guide the diffusion process to sample latents for which the decoding-encoding map is not lossy. By harnessing the power of available foundation models, we are able to outperform previous approaches without finetuning across a wide range of problems.\n\nOur contributions are as follows:\n\n1. We show how to use Latent Diffusion Models models (such as Stable Diffusion) to solve linear inverse problem when the degradation operator is known.\n2. We theoretically analyze our algorithm and show provable sample recovery in a linear model setting with two-step diffusion processes.\n3. We achieve a new state-of-the-art for solving inverse problems with latent diffusion models, out-performing previous approaches for inpainting, block inpainting, denoising, deblurring, destriping, and super-resolution."}, {"type": "heading", "lvl": 2, "value": "Background and Method", "md": "## Background and Method"}, {"type": "text", "value": "Notation: Bold lower-case $$x$$, bold upper-case $$X$$, and normal lower case $$x$$ denote a vector, a matrix, and a scalar variable, respectively. We denote by $$\\odot$$ element-wise multiplication. $$D(x)$$ represents a diagonal matrix with entries $$x$$. We use $$E(.)$$ for the encoder and $$D(.)$$ for the decoder. $$E^\\#p$$ is a pushforward measure of $$p$$, i.e., for every $$x \\in p$$, the sample $$E(x)$$ is a sample from $$E^\\#p$$. We use arrows in Section 3 to distinguish random variables of the forward (\u2192) and the reverse process (\u2190).\n\n1The source code is available at: https://github.com/LituRout/PSLD and a web application for image inpainting is available at: https://huggingface.co/spaces/PSLD/PSLD.", "md": "Notation: Bold lower-case $$x$$, bold upper-case $$X$$, and normal lower case $$x$$ denote a vector, a matrix, and a scalar variable, respectively. We denote by $$\\odot$$ element-wise multiplication. $$D(x)$$ represents a diagonal matrix with entries $$x$$. We use $$E(.)$$ for the encoder and $$D(.)$$ for the decoder. $$E^\\#p$$ is a pushforward measure of $$p$$, i.e., for every $$x \\in p$$, the sample $$E(x)$$ is a sample from $$E^\\#p$$. We use arrows in Section 3 to distinguish random variables of the forward (\u2192) and the reverse process (\u2190).\n\n1The source code is available at: https://github.com/LituRout/PSLD and a web application for image inpainting is available at: https://huggingface.co/spaces/PSLD/PSLD."}]}, {"page": 4, "text": "     The standard diffusion modeling framework involves training a network,                          s\u03b8 (xt , t), to learn the\nscore-function,     \u2207 x tlog  pt(x t), at all levels  t, of a stochastic process described by an It\u00f4 SDE:\n                                                 dx  =  f(x, t)dt   +  g(t)dw,                                               (1)\n     where    w  is the standard Wiener process. To generate samples from the trained model, one can\nrun the (unconditional) Reverse SDE, where the score-function is approximated by the trained neural\nnetwork.     Given measurements          y  =   Ax  0 +  \u03c3y n, one can sample from the distribution              p 0(x 0 |y) by\nrunning the conditional Reverse SDE given by:\n                     dx  =  ( f(x, t)\u2212    g2 (t) (\u2207    log p (x  ) +  \u2207    log  p(y|x   ))) dt +  g(t)dw.                    (2)\n                                                    xt       t   t       xt            t\n     As mentioned,      \u2207 x tlog p(y|x  t )is intractable for general inverse problems. One of the most effective\napproximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:\n                         p(y|x   )\u2248   p(y|x    =  E[x   |x  ]) = N  (y;  \u03bc =  AE[x    |x  ],\u03a3 =   \u03c3 2I).                     (3)\n                               t             0         0   t                         0   t          y\n     Essentially, DPS substitutes the unknown clean image                   x 0 with its conditional expectation given\nthe noisy input,     E[x  0|x t]. Under this approximation, the term            p(y|x  t)  becomes tractable.\n     The theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze\nDPS in a linear model setting where the data distribution lives in a low-dimensional subspace, and show\nthat DPS actually samples from             p(x 0 |y) (Section 3.2). Then, we provide an            algorithm    (Section 2.1)\nand its   analysis   to sample from      p(x  0|y)  using latent diffusion models (Section 3.3). Importantly, our\nanalysis suggests that our algorithm enjoys the same theoretical guarantees while avoiding the curse\nof ambient dimension observed in pixel-space diffusion models including DPS. Using experiments\n(Section 4), we show that our algorithm allows us to use powerful foundation models and solve linear\ninverse problems, outperforming previous unsupervised approaches without the need for finetuning.\n2.1      Method\nIn Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model\ns \u03b8(z t, t)to predict the score      \u2207 z tlog p t(zt), of a diffusion process:\n                                                 dz  =  f(z, t)dt   + g(t)dw,                                                (4)\nwhere    z 0 =  E(x  0) for some encoder function         E(\u00b7) : R  d \u2192  R k . During sampling, we start with           z T, we\nrun the Reverse Diffusion Process and then we obtain a clean image by passing                      z 0 \u223c  p0 (z0 |zT ) through\na decoder     D  :R k  \u2192  R d .\n     Although Latent Diffusion Models underlie some of the most powerful foundation models for image\ngeneration, existing algorithms for solving inverse problems with diffusion models do not apply for\nLDMs. The most natural extension of the DPS idea would be to approximate                            p(y|z  t) with:\n                                            p(y|z  ) \u2248  p(y|x    =   D (E[z   |z  ])),                                       (5)\n                                                   t            0            0   t\ni.e., to approximate the unknown clean image             x 0 with the decoded version of the conditional expectation\nof the clean latent     z 0 given the noisy latent      z t. However, as we show experimentally in Section 4, this\nidea does not work. The failure of the \u201cvanilla\u201d extension of the DPS algorithm for latent diffusion\nmodels should not come as a surprise. The fundamental reason is that the encoder is a many-to-one\nmapping. Simply put, there are many latents                z 0 that correspond to encoded versions of images that\nexplain the measurements. Taking the gradient of the density given by                     (5) could be pulling     z t towards\nany of these latents       z0 , potentially in different directions. On the other hand, the score-function is\npulling   z t towards a specific     z 0 that corresponds to the4best denoised version of              zt.", "md": "The standard diffusion modeling framework involves training a network, $s_{\\theta}(x_{t}, t)$, to learn the score-function, $\\nabla_{x_{t}} \\log p_{t}(x_{t})$, at all levels $t$, of a stochastic process described by an It\u00f4 SDE:\n\n$$\ndx = f(x, t)dt + g(t)dw \\hspace{20pt} (1)\n$$\nwhere $w$ is the standard Wiener process. To generate samples from the trained model, one can run the (unconditional) Reverse SDE, where the score-function is approximated by the trained neural network. Given measurements $y = Ax_{0} + \\sigma_{y}n$, one can sample from the distribution $p_{0}(x_{0}|y)$ by running the conditional Reverse SDE given by:\n\n$$\ndx = \\left( f(x, t) - g^{2}(t) \\left( \\nabla \\log p(x_{t}) + \\nabla \\log p(y|x_{t}) \\right) \\right) dt + g(t)dw \\hspace{20pt} (2)\n$$\nAs mentioned, $\\nabla_{x_{t}} \\log p(y|x_{t})$ is intractable for general inverse problems. One of the most effective approximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:\n\n$$\np(y|x_{t}) \\approx p(y|x_{t} = E[x_{t}|x_{0}]) = N(y; \\mu = AE[x_{t}|x_{0}], \\Sigma = \\sigma^{2}I) \\hspace{20pt} (3)\n$$\nEssentially, DPS substitutes the unknown clean image $x_{0}$ with its conditional expectation given the noisy input, $E[x_{0}|x_{t}]$. Under this approximation, the term $p(y|x_{t})$ becomes tractable.\n\nThe theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze DPS in a linear model setting where the data distribution lives in a low-dimensional subspace, and show that DPS actually samples from $p(x_{0}|y)$ (Section 3.2). Then, we provide an algorithm (Section 2.1) and its analysis to sample from $p(x_{0}|y)$ using latent diffusion models (Section 3.3). Importantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while avoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS. Using experiments (Section 4), we show that our algorithm allows us to use powerful foundation models and solve linear inverse problems, outperforming previous unsupervised approaches without the need for finetuning.\n\n## 2.1 Method\n\nIn Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model $s_{\\theta}(z_{t}, t)$ to predict the score $\\nabla_{z_{t}} \\log p_{t}(z_{t})$, of a diffusion process:\n\n$$\ndz = f(z, t)dt + g(t)dw \\hspace{20pt} (4)\n$$\nwhere $z_{0} = E(x_{0})$ for some encoder function $E(\\cdot) : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{k}$. During sampling, we start with $z_{T}$, we run the Reverse Diffusion Process and then we obtain a clean image by passing $z_{0} \\sim p_{0}(z_{0}|z_{T})$ through a decoder $D : \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{d}$.\n\nAlthough Latent Diffusion Models underlie some of the most powerful foundation models for image generation, existing algorithms for solving inverse problems with diffusion models do not apply for LDMs. The most natural extension of the DPS idea would be to approximate $p(y|z_{t})$ with:\n\n$$\np(y|z_{t}) \\approx p(y|x_{0} = D(E[z_{t}|z_{0}])) \\hspace{20pt} (5)\n$$\ni.e., to approximate the unknown clean image $x_{0}$ with the decoded version of the conditional expectation of the clean latent $z_{0}$ given the noisy latent $z_{t}$. However, as we show experimentally in Section 4, this idea does not work. The failure of the \u201cvanilla\u201d extension of the DPS algorithm for latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a many-to-one mapping. Simply put, there are many latents $z_{0}$ that correspond to encoded versions of images that explain the measurements. Taking the gradient of the density given by (5) could be pulling $z_{t}$ towards any of these latents $z_{0}$, potentially in different directions. On the other hand, the score-function is pulling $z_{t}$ towards a specific $z_{0}$ that corresponds to the best denoised version of $z_{t}$.", "images": [], "items": [{"type": "text", "value": "The standard diffusion modeling framework involves training a network, $s_{\\theta}(x_{t}, t)$, to learn the score-function, $\\nabla_{x_{t}} \\log p_{t}(x_{t})$, at all levels $t$, of a stochastic process described by an It\u00f4 SDE:\n\n$$\ndx = f(x, t)dt + g(t)dw \\hspace{20pt} (1)\n$$\nwhere $w$ is the standard Wiener process. To generate samples from the trained model, one can run the (unconditional) Reverse SDE, where the score-function is approximated by the trained neural network. Given measurements $y = Ax_{0} + \\sigma_{y}n$, one can sample from the distribution $p_{0}(x_{0}|y)$ by running the conditional Reverse SDE given by:\n\n$$\ndx = \\left( f(x, t) - g^{2}(t) \\left( \\nabla \\log p(x_{t}) + \\nabla \\log p(y|x_{t}) \\right) \\right) dt + g(t)dw \\hspace{20pt} (2)\n$$\nAs mentioned, $\\nabla_{x_{t}} \\log p(y|x_{t})$ is intractable for general inverse problems. One of the most effective approximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:\n\n$$\np(y|x_{t}) \\approx p(y|x_{t} = E[x_{t}|x_{0}]) = N(y; \\mu = AE[x_{t}|x_{0}], \\Sigma = \\sigma^{2}I) \\hspace{20pt} (3)\n$$\nEssentially, DPS substitutes the unknown clean image $x_{0}$ with its conditional expectation given the noisy input, $E[x_{0}|x_{t}]$. Under this approximation, the term $p(y|x_{t})$ becomes tractable.\n\nThe theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze DPS in a linear model setting where the data distribution lives in a low-dimensional subspace, and show that DPS actually samples from $p(x_{0}|y)$ (Section 3.2). Then, we provide an algorithm (Section 2.1) and its analysis to sample from $p(x_{0}|y)$ using latent diffusion models (Section 3.3). Importantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while avoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS. Using experiments (Section 4), we show that our algorithm allows us to use powerful foundation models and solve linear inverse problems, outperforming previous unsupervised approaches without the need for finetuning.", "md": "The standard diffusion modeling framework involves training a network, $s_{\\theta}(x_{t}, t)$, to learn the score-function, $\\nabla_{x_{t}} \\log p_{t}(x_{t})$, at all levels $t$, of a stochastic process described by an It\u00f4 SDE:\n\n$$\ndx = f(x, t)dt + g(t)dw \\hspace{20pt} (1)\n$$\nwhere $w$ is the standard Wiener process. To generate samples from the trained model, one can run the (unconditional) Reverse SDE, where the score-function is approximated by the trained neural network. Given measurements $y = Ax_{0} + \\sigma_{y}n$, one can sample from the distribution $p_{0}(x_{0}|y)$ by running the conditional Reverse SDE given by:\n\n$$\ndx = \\left( f(x, t) - g^{2}(t) \\left( \\nabla \\log p(x_{t}) + \\nabla \\log p(y|x_{t}) \\right) \\right) dt + g(t)dw \\hspace{20pt} (2)\n$$\nAs mentioned, $\\nabla_{x_{t}} \\log p(y|x_{t})$ is intractable for general inverse problems. One of the most effective approximation methods is the DPS algorithm proposed by Chung et al. [11]. DPS assumes that:\n\n$$\np(y|x_{t}) \\approx p(y|x_{t} = E[x_{t}|x_{0}]) = N(y; \\mu = AE[x_{t}|x_{0}], \\Sigma = \\sigma^{2}I) \\hspace{20pt} (3)\n$$\nEssentially, DPS substitutes the unknown clean image $x_{0}$ with its conditional expectation given the noisy input, $E[x_{0}|x_{t}]$. Under this approximation, the term $p(y|x_{t})$ becomes tractable.\n\nThe theoretical properties of the DPS algorithm are not well understood. In this paper, we analyze DPS in a linear model setting where the data distribution lives in a low-dimensional subspace, and show that DPS actually samples from $p(x_{0}|y)$ (Section 3.2). Then, we provide an algorithm (Section 2.1) and its analysis to sample from $p(x_{0}|y)$ using latent diffusion models (Section 3.3). Importantly, our analysis suggests that our algorithm enjoys the same theoretical guarantees while avoiding the curse of ambient dimension observed in pixel-space diffusion models including DPS. Using experiments (Section 4), we show that our algorithm allows us to use powerful foundation models and solve linear inverse problems, outperforming previous unsupervised approaches without the need for finetuning."}, {"type": "heading", "lvl": 2, "value": "2.1 Method", "md": "## 2.1 Method"}, {"type": "text", "value": "In Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model $s_{\\theta}(z_{t}, t)$ to predict the score $\\nabla_{z_{t}} \\log p_{t}(z_{t})$, of a diffusion process:\n\n$$\ndz = f(z, t)dt + g(t)dw \\hspace{20pt} (4)\n$$\nwhere $z_{0} = E(x_{0})$ for some encoder function $E(\\cdot) : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{k}$. During sampling, we start with $z_{T}$, we run the Reverse Diffusion Process and then we obtain a clean image by passing $z_{0} \\sim p_{0}(z_{0}|z_{T})$ through a decoder $D : \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{d}$.\n\nAlthough Latent Diffusion Models underlie some of the most powerful foundation models for image generation, existing algorithms for solving inverse problems with diffusion models do not apply for LDMs. The most natural extension of the DPS idea would be to approximate $p(y|z_{t})$ with:\n\n$$\np(y|z_{t}) \\approx p(y|x_{0} = D(E[z_{t}|z_{0}])) \\hspace{20pt} (5)\n$$\ni.e., to approximate the unknown clean image $x_{0}$ with the decoded version of the conditional expectation of the clean latent $z_{0}$ given the noisy latent $z_{t}$. However, as we show experimentally in Section 4, this idea does not work. The failure of the \u201cvanilla\u201d extension of the DPS algorithm for latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a many-to-one mapping. Simply put, there are many latents $z_{0}$ that correspond to encoded versions of images that explain the measurements. Taking the gradient of the density given by (5) could be pulling $z_{t}$ towards any of these latents $z_{0}$, potentially in different directions. On the other hand, the score-function is pulling $z_{t}$ towards a specific $z_{0}$ that corresponds to the best denoised version of $z_{t}$.", "md": "In Latent Diffusion Models, the diffusion occurs in the latent space. Specifically, we train a model $s_{\\theta}(z_{t}, t)$ to predict the score $\\nabla_{z_{t}} \\log p_{t}(z_{t})$, of a diffusion process:\n\n$$\ndz = f(z, t)dt + g(t)dw \\hspace{20pt} (4)\n$$\nwhere $z_{0} = E(x_{0})$ for some encoder function $E(\\cdot) : \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{k}$. During sampling, we start with $z_{T}$, we run the Reverse Diffusion Process and then we obtain a clean image by passing $z_{0} \\sim p_{0}(z_{0}|z_{T})$ through a decoder $D : \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{d}$.\n\nAlthough Latent Diffusion Models underlie some of the most powerful foundation models for image generation, existing algorithms for solving inverse problems with diffusion models do not apply for LDMs. The most natural extension of the DPS idea would be to approximate $p(y|z_{t})$ with:\n\n$$\np(y|z_{t}) \\approx p(y|x_{0} = D(E[z_{t}|z_{0}])) \\hspace{20pt} (5)\n$$\ni.e., to approximate the unknown clean image $x_{0}$ with the decoded version of the conditional expectation of the clean latent $z_{0}$ given the noisy latent $z_{t}$. However, as we show experimentally in Section 4, this idea does not work. The failure of the \u201cvanilla\u201d extension of the DPS algorithm for latent diffusion models should not come as a surprise. The fundamental reason is that the encoder is a many-to-one mapping. Simply put, there are many latents $z_{0}$ that correspond to encoded versions of images that explain the measurements. Taking the gradient of the density given by (5) could be pulling $z_{t}$ towards any of these latents $z_{0}$, potentially in different directions. On the other hand, the score-function is pulling $z_{t}$ towards a specific $z_{0}$ that corresponds to the best denoised version of $z_{t}$."}]}, {"page": 5, "text": "  Algorithm 1:        DPS                                     Algorithm 2:        PSLD\n    Input:   T ,y, \u03b6T  , {\u02dc  T                                                     T        T       T             \u2217\n                          \u03c3 }   ,s                              Input:  T,  y, {\u03b7 }   , {\u03b3}    ,{\u02dc\n                                                                                                 \u03c3 }    ,E,D,Ax    ,A,s\n  1 x T \u223c T  (0,I)  i=1    i i=1   \u03b8                          1 zT  \u223c N(0,I)     i i=1    i i=1    ii=1           0     \u03b8\n  2 for  i= T  \u2212 1 to 0 do                                    2 for i = T \u2212  1 to 0 do\n  3      \u02c6\n         s \u2190  s (x , i)                                       3      \u02c6\n               \u03b8   i                                                 s \u2190  s\u03b8(zi, i)\n  4      \u02c6      \u221a1                                                         \u221a1\n         x  \u2190       (x + (1 \u2212  \u00af\n                               \u03b1 )\u02c6s)                         4      \u02c6\n                                                                     z  \u2190      (z  + (1\u2212  \u00af\n          0      \u00af    i         i                                     0          i        \u03b1i)\u02c6s)\n                 \u03b1                                                           \u00af\n                   i                                                         \u03b1i\n  5      z \u223c N(0,I)                 \u221a                         5      \u03f5\u223c N(0,I)                 \u221a\n                  \u221a\u03b1  (1\u2212\u00af                                                   \u221a\n                         \u03b1   )        \u00af\n                                      \u03b1   \u03b2                                    \u03b1 (1\u2212\u00af\n  6      x\u2032    \u2190     i    i\u22121  x +     i\u22121 i \u02c6                        \u2032         i   \u03b1 i\u22121)       \u03b1\u00afi\u22121\u03b2i\n                                             x  +\u02dc\n                                                 \u03c3  z         6      z    \u2190               z  +          \u02c6\n          i\u22121         1\u2212\u00af       i             0    i                                       i            z0 + \u02dc\u03c3i\u03f5\n                        \u03b1             1\u2212\u00af\n                          i             \u03b1i                            i\u22121        1\u2212\u00af\u03b1i           1\u2212\u00af\u03b1i\n  7      x     \u2190 x \u2032   \u2212 \u03b6 \u2207   \u2225y \u2212 A( \u02c6x )\u22252                 7      z\u2032\u2032  \u2190  z\u2032   \u2212 \u03b7 \u2207   \u2225y  \u2212 A(D( \u02c6z ))\u22252\n          i\u22121      i\u22121    i  xi          0  2                         i\u22121     i\u22121     i z i            0   2\n  8 end                                                       8      zi\u22121 \u2190\n  9 return   \u02c6                                                         \u2032\u2032                     T    \u2217          T           2\n             x0                                                       z    \u2212  \u03b3 \u2207   \u2225\u02c6\n                                                                                     z  \u2212 E(A   Ax   + (I \u2212 A   A)D( \u02c6z))\u2225\n                                                              9 end    i\u22121     i  zi  0            0                  0   2\n                                                            10  return  D( \u02c6z)\n                                                                            0\n    To address this problem, we propose an extra term that penalizes latents that are not fixed-points\nof the composition of the decoder-function with the encoder-function. Specifically, we approximate the\nintractable    \u2207  log p(y|z  t) with:                                                                              2\n             \u2207    log p(y|z   ) =  \u2207    p(y|x    =  D  (E[z   |z  ]))+\u03b3    \u2207    ||E[z  |z ] \u2212 E(D(E[z      |z  ]))|| .         (6)\n               zt            t     \ufe38  zt       0   \ufe37\ufe37        0   t  \ufe38    t \ufe38 zt       0   t    \ufe37\ufe37         0   t    \ufe38\n                                         DPS vanilla extension                         \u201cgoodness\u201d of  z0\nWe refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide\nthe diffusion process towards latents such that: i) they explain the measurements when passed through\nthe decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful\nto make sure that the generated sample remains on the manifold of real data. However, it does not\npenalize the reverse SDE for generating other latents              z 0 as long as   D(z  0 ) lies on the manifold of natural\nimages. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the\nmask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem\nby introducing the following gluing objective. In words, the gluing objective penalizes decoded images\nhaving a discontinuity at the boundary of the mask.\n                 \u2207  ztlog  p(y|z  t) =  \u2207 z tp(y|x  0 =  D(E[z   0 |zt]))\n                                        \ufe38              \ufe37\ufe37               \ufe38\n                                             DPS vanilla extension\n                                                \u2223\u2223                                                           \u2223\u22232\n                                                \u2223\u2223                  T     \u2217             T                    \u2223\u2223\n                                    +  \u03b3 t\u2207  zt \u2223\u2223E[z 0|zt] \u2212 E(A     Ax  0 + (I   \u2212 A    A)D(E[z    0 |zt]))\u2223\u2223  .             (7)\n                                          \ufe38                                 \ufe37\ufe37                                 \ufe38\n                                                                      \u201cgluing\u201d of z0\nThe gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-\nmatching update, and the gluing update point to the same optima in the latent space. We refer to\nthis approximation (7) as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we\nprovide an analysis of these gradient updates, along with the associated algorithms.\n3      Theoretical Results\nAs discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse\nprocesses, each governed by It\u00f4 SDEs. For implementation purposes, these SDEs are discretized over\na finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward\u221a             \u221a\n                           \u2212\u2192         \u2212\u2192                                             \u2212\u2192                    \u2212\nprocess starts from        x 0 \u223c   p(x 0 ) and gradually adds noise, i.e.,            x t+1  =     1  \u2212  \u03b2t\u2192x t +     \u03b2t\u03f5  where\n                                                                                                               \u2190\u2212\n\u03b2t \u2208  [0, 1] and   \u03b2 t\u2265   \u03b2t\u22121  for  t = 0, .\u221a. . , T\u2212  1 . The reverse process is initialized with            x  T \u223c N    (0, I d)\n                   \u2190\u2212             \u2190\u2212                                            \u2190\u2212\nand generates      x  t\u22121 =   \u03bc\u03b8 ( x t, t) +    \u03b2t\u03f5. In the last step,     \u03bc \u03b8( x  1,1)  is displayed without the noise.\n                                                                 5", "md": "# Document\n\n## Algorithm 1: DPS\n\nInput: \\(T, y, \\zeta_T, \\{\\tilde{T}, \\sigma\\}, s\\)\n\n|1|\\(x_T \\sim T(0, I)\\)|i=1|\\(\\theta_i\\)|\n|---|---|---|---|\n| | |2|for \\(i = T - 1\\) to 0 do|\n|3|\\(\\hat{s} \\leftarrow s(x, i)\\)|\\(\\theta_i\\)|\\(s \\leftarrow s_{\\theta}(z_i, i)\\)|\n|4|\\(x \\leftarrow x + (1 - \\bar{\\alpha})\\hat{s}\\)|\\(\\hat{x} \\leftarrow (x + (1 - \\bar{\\alpha})\\hat{s})\\)| |\n|5|\\(z \\sim N(0, I)\\)| | |\n|6|\\(x' \\leftarrow x_{i-1} + x_{i-1}\\hat{x} + \\tilde{\\sigma}z\\)| | |\n|7|\\(x \\leftarrow x' - \\zeta \\nabla \\|y - A(\\hat{x})\\|^2\\)| | |\n|8|end| | |\n|9|return \\(\\hat{x}_0\\)| | |\n\n## Algorithm 2: PSLD\n\nInput: \\(T, y, \\{\\eta\\}, \\{\\gamma\\}, \\{\\tilde{\\sigma}\\}, E, D, A_x, A, s\\)\n\n|1|\\(z_T \\sim N(0, I)\\)|i=1|\\(\\theta_i\\)|\n|---|---|---|---|\n| | |2|for \\(i = T - 1\\) to 0 do|\n|3|\\(\\hat{s} \\leftarrow s_{\\theta}(z_i, i)\\)| | |\n|4|\\(z \\leftarrow z + (1 - \\bar{\\alpha}_i)\\hat{s}\\)| | |\n|5|\\(\\epsilon \\sim N(0, I)\\)| | |\n|6|\\(z \\leftarrow z + z_0 + \\tilde{\\sigma}_i\\epsilon\\)| | |\n|7|\\(z'' \\leftarrow z' - \\eta \\nabla \\|y - A(D(\\hat{z}))\\|^2\\)| | |\n|8|end| | |\n|9|return \\(D(\\hat{z}_0)\\)| | |\n\nTo address this problem, we propose an extra term that penalizes latents that are not fixed-points of the composition of the decoder-function with the encoder-function. Specifically, we approximate the intractable \\(\\nabla \\log p(y|z_t)\\) with:\n\n$$\\nabla \\log p(y|z_t) = \\nabla p(y|x_0 = D(E[z_0|z_t])) + \\gamma \\nabla ||E[z_0|z_t] - E(D(E[z_0|z_t]))||.$$\n\nWe refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide the diffusion process towards latents such that: i) they explain the measurements when passed through the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful to make sure that the generated sample remains on the manifold of real data. However, it does not penalize the reverse SDE for generating other latents \\(z_0\\) as long as \\(D(z_0)\\) lies on the manifold of natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem by introducing the following gluing objective. In words, the gluing objective penalizes decoded images having a discontinuity at the boundary of the mask.\n\n$$\\nabla z_t \\log p(y|z_t) = \\nabla z_t p(y|x_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla z_t ||E[z_0|z_t] - E(A_x Ax_0 + (I - A_x A)D(E[z_0|z_t]))||.$$\n\nThe gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-matching update, and the gluing update point to the same optima in the latent space. We refer to this approximation as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we provide an analysis of these gradient updates, along with the associated algorithms.\n\n## Theoretical Results\n\nAs discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse processes, each governed by It\u00f4 SDEs. For implementation purposes, these SDEs are discretized over a finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward process starts from \\(x_0 \\sim p(x_0)\\) and gradually adds noise, i.e., \\(x_{t+1} = (1 - \\beta_t)x_t + \\beta_t\\epsilon\\) where \\(\\beta_t \\in [0, 1]\\) and \\(\\beta_t \\geq \\beta_{t-1}\\) for \\(t = 0, ..., T-1\\). The reverse process is initialized with \\(x_T \\sim N(0, I_d)\\) and generates \\(x_{t-1} = \\mu_{\\theta}(x_t, t) + \\beta_t\\epsilon\\). In the last step, \\(\\mu_{\\theta}(x_1, 1)\\) is displayed without the noise.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Algorithm 1: DPS", "md": "## Algorithm 1: DPS"}, {"type": "text", "value": "Input: \\(T, y, \\zeta_T, \\{\\tilde{T}, \\sigma\\}, s\\)", "md": "Input: \\(T, y, \\zeta_T, \\{\\tilde{T}, \\sigma\\}, s\\)"}, {"type": "table", "rows": [["1", "\\(x_T \\sim T(0, I)\\)", "i=1", "\\(\\theta_i\\)"], ["", "", "2", "for \\(i = T - 1\\) to 0 do"], ["3", "\\(\\hat{s} \\leftarrow s(x, i)\\)", "\\(\\theta_i\\)", "\\(s \\leftarrow s_{\\theta}(z_i, i)\\)"], ["4", "\\(x \\leftarrow x + (1 - \\bar{\\alpha})\\hat{s}\\)", "\\(\\hat{x} \\leftarrow (x + (1 - \\bar{\\alpha})\\hat{s})\\)", ""], ["5", "\\(z \\sim N(0, I)\\)", "", ""], ["6", "\\(x' \\leftarrow x_{i-1} + x_{i-1}\\hat{x} + \\tilde{\\sigma}z\\)", "", ""], ["7", "\\(x \\leftarrow x' - \\zeta \\nabla \\", "y - A(\\hat{x})\\", "^2\\)", "", ""], ["8", "end", "", ""], ["9", "return \\(\\hat{x}_0\\)", "", ""]], "md": "|1|\\(x_T \\sim T(0, I)\\)|i=1|\\(\\theta_i\\)|\n|---|---|---|---|\n| | |2|for \\(i = T - 1\\) to 0 do|\n|3|\\(\\hat{s} \\leftarrow s(x, i)\\)|\\(\\theta_i\\)|\\(s \\leftarrow s_{\\theta}(z_i, i)\\)|\n|4|\\(x \\leftarrow x + (1 - \\bar{\\alpha})\\hat{s}\\)|\\(\\hat{x} \\leftarrow (x + (1 - \\bar{\\alpha})\\hat{s})\\)| |\n|5|\\(z \\sim N(0, I)\\)| | |\n|6|\\(x' \\leftarrow x_{i-1} + x_{i-1}\\hat{x} + \\tilde{\\sigma}z\\)| | |\n|7|\\(x \\leftarrow x' - \\zeta \\nabla \\|y - A(\\hat{x})\\|^2\\)| | |\n|8|end| | |\n|9|return \\(\\hat{x}_0\\)| | |", "isPerfectTable": false, "csv": "\"1\",\"\\(x_T \\sim T(0, I)\\)\",\"i=1\",\"\\(\\theta_i\\)\"\n\"\",\"\",\"2\",\"for \\(i = T - 1\\) to 0 do\"\n\"3\",\"\\(\\hat{s} \\leftarrow s(x, i)\\)\",\"\\(\\theta_i\\)\",\"\\(s \\leftarrow s_{\\theta}(z_i, i)\\)\"\n\"4\",\"\\(x \\leftarrow x + (1 - \\bar{\\alpha})\\hat{s}\\)\",\"\\(\\hat{x} \\leftarrow (x + (1 - \\bar{\\alpha})\\hat{s})\\)\",\"\"\n\"5\",\"\\(z \\sim N(0, I)\\)\",\"\",\"\"\n\"6\",\"\\(x' \\leftarrow x_{i-1} + x_{i-1}\\hat{x} + \\tilde{\\sigma}z\\)\",\"\",\"\"\n\"7\",\"\\(x \\leftarrow x' - \\zeta \\nabla \\\",\"y - A(\\hat{x})\\\",\"^2\\)\",\"\",\"\"\n\"8\",\"end\",\"\",\"\"\n\"9\",\"return \\(\\hat{x}_0\\)\",\"\",\"\""}, {"type": "heading", "lvl": 2, "value": "Algorithm 2: PSLD", "md": "## Algorithm 2: PSLD"}, {"type": "text", "value": "Input: \\(T, y, \\{\\eta\\}, \\{\\gamma\\}, \\{\\tilde{\\sigma}\\}, E, D, A_x, A, s\\)", "md": "Input: \\(T, y, \\{\\eta\\}, \\{\\gamma\\}, \\{\\tilde{\\sigma}\\}, E, D, A_x, A, s\\)"}, {"type": "table", "rows": [["1", "\\(z_T \\sim N(0, I)\\)", "i=1", "\\(\\theta_i\\)"], ["", "", "2", "for \\(i = T - 1\\) to 0 do"], ["3", "\\(\\hat{s} \\leftarrow s_{\\theta}(z_i, i)\\)", "", ""], ["4", "\\(z \\leftarrow z + (1 - \\bar{\\alpha}_i)\\hat{s}\\)", "", ""], ["5", "\\(\\epsilon \\sim N(0, I)\\)", "", ""], ["6", "\\(z \\leftarrow z + z_0 + \\tilde{\\sigma}_i\\epsilon\\)", "", ""], ["7", "\\(z'' \\leftarrow z' - \\eta \\nabla \\", "y - A(D(\\hat{z}))\\", "^2\\)", "", ""], ["8", "end", "", ""], ["9", "return \\(D(\\hat{z}_0)\\)", "", ""]], "md": "|1|\\(z_T \\sim N(0, I)\\)|i=1|\\(\\theta_i\\)|\n|---|---|---|---|\n| | |2|for \\(i = T - 1\\) to 0 do|\n|3|\\(\\hat{s} \\leftarrow s_{\\theta}(z_i, i)\\)| | |\n|4|\\(z \\leftarrow z + (1 - \\bar{\\alpha}_i)\\hat{s}\\)| | |\n|5|\\(\\epsilon \\sim N(0, I)\\)| | |\n|6|\\(z \\leftarrow z + z_0 + \\tilde{\\sigma}_i\\epsilon\\)| | |\n|7|\\(z'' \\leftarrow z' - \\eta \\nabla \\|y - A(D(\\hat{z}))\\|^2\\)| | |\n|8|end| | |\n|9|return \\(D(\\hat{z}_0)\\)| | |", "isPerfectTable": false, "csv": "\"1\",\"\\(z_T \\sim N(0, I)\\)\",\"i=1\",\"\\(\\theta_i\\)\"\n\"\",\"\",\"2\",\"for \\(i = T - 1\\) to 0 do\"\n\"3\",\"\\(\\hat{s} \\leftarrow s_{\\theta}(z_i, i)\\)\",\"\",\"\"\n\"4\",\"\\(z \\leftarrow z + (1 - \\bar{\\alpha}_i)\\hat{s}\\)\",\"\",\"\"\n\"5\",\"\\(\\epsilon \\sim N(0, I)\\)\",\"\",\"\"\n\"6\",\"\\(z \\leftarrow z + z_0 + \\tilde{\\sigma}_i\\epsilon\\)\",\"\",\"\"\n\"7\",\"\\(z'' \\leftarrow z' - \\eta \\nabla \\\",\"y - A(D(\\hat{z}))\\\",\"^2\\)\",\"\",\"\"\n\"8\",\"end\",\"\",\"\"\n\"9\",\"return \\(D(\\hat{z}_0)\\)\",\"\",\"\""}, {"type": "text", "value": "To address this problem, we propose an extra term that penalizes latents that are not fixed-points of the composition of the decoder-function with the encoder-function. Specifically, we approximate the intractable \\(\\nabla \\log p(y|z_t)\\) with:\n\n$$\\nabla \\log p(y|z_t) = \\nabla p(y|x_0 = D(E[z_0|z_t])) + \\gamma \\nabla ||E[z_0|z_t] - E(D(E[z_0|z_t]))||.$$\n\nWe refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide the diffusion process towards latents such that: i) they explain the measurements when passed through the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful to make sure that the generated sample remains on the manifold of real data. However, it does not penalize the reverse SDE for generating other latents \\(z_0\\) as long as \\(D(z_0)\\) lies on the manifold of natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem by introducing the following gluing objective. In words, the gluing objective penalizes decoded images having a discontinuity at the boundary of the mask.\n\n$$\\nabla z_t \\log p(y|z_t) = \\nabla z_t p(y|x_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla z_t ||E[z_0|z_t] - E(A_x Ax_0 + (I - A_x A)D(E[z_0|z_t]))||.$$\n\nThe gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-matching update, and the gluing update point to the same optima in the latent space. We refer to this approximation as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we provide an analysis of these gradient updates, along with the associated algorithms.", "md": "To address this problem, we propose an extra term that penalizes latents that are not fixed-points of the composition of the decoder-function with the encoder-function. Specifically, we approximate the intractable \\(\\nabla \\log p(y|z_t)\\) with:\n\n$$\\nabla \\log p(y|z_t) = \\nabla p(y|x_0 = D(E[z_0|z_t])) + \\gamma \\nabla ||E[z_0|z_t] - E(D(E[z_0|z_t]))||.$$\n\nWe refer to this approximation as Goodness Modified Latent DPS (GML-DPS). Intuitively, we guide the diffusion process towards latents such that: i) they explain the measurements when passed through the decoder, and ii) they are fixed points of the decoder-encoder composition. The latter is useful to make sure that the generated sample remains on the manifold of real data. However, it does not penalize the reverse SDE for generating other latents \\(z_0\\) as long as \\(D(z_0)\\) lies on the manifold of natural images. Even in the linear case (see Section 3), this can lead to inconsistency at the boundary of the mask in the pixel space. The linear theory in Section 3 suggests that we can circumvent this problem by introducing the following gluing objective. In words, the gluing objective penalizes decoded images having a discontinuity at the boundary of the mask.\n\n$$\\nabla z_t \\log p(y|z_t) = \\nabla z_t p(y|x_0 = D(E[z_0|z_t])) + \\gamma_t \\nabla z_t ||E[z_0|z_t] - E(A_x Ax_0 + (I - A_x A)D(E[z_0|z_t]))||.$$\n\nThe gluing objective is critical for our algorithm as it ensures that the denoising update, measurement-matching update, and the gluing update point to the same optima in the latent space. We refer to this approximation as Posterior Sampling with Latent Diffusion (PSLD). In the next Section 3, we provide an analysis of these gradient updates, along with the associated algorithms."}, {"type": "heading", "lvl": 2, "value": "Theoretical Results", "md": "## Theoretical Results"}, {"type": "text", "value": "As discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse processes, each governed by It\u00f4 SDEs. For implementation purposes, these SDEs are discretized over a finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward process starts from \\(x_0 \\sim p(x_0)\\) and gradually adds noise, i.e., \\(x_{t+1} = (1 - \\beta_t)x_t + \\beta_t\\epsilon\\) where \\(\\beta_t \\in [0, 1]\\) and \\(\\beta_t \\geq \\beta_{t-1}\\) for \\(t = 0, ..., T-1\\). The reverse process is initialized with \\(x_T \\sim N(0, I_d)\\) and generates \\(x_{t-1} = \\mu_{\\theta}(x_t, t) + \\beta_t\\epsilon\\). In the last step, \\(\\mu_{\\theta}(x_1, 1)\\) is displayed without the noise.", "md": "As discussed in Section 2, diffusion models consist of two stochastic processes: the forward and reverse processes, each governed by It\u00f4 SDEs. For implementation purposes, these SDEs are discretized over a finite number of (time) steps, and the diffusion takes place using a transition kernel. The forward process starts from \\(x_0 \\sim p(x_0)\\) and gradually adds noise, i.e., \\(x_{t+1} = (1 - \\beta_t)x_t + \\beta_t\\epsilon\\) where \\(\\beta_t \\in [0, 1]\\) and \\(\\beta_t \\geq \\beta_{t-1}\\) for \\(t = 0, ..., T-1\\). The reverse process is initialized with \\(x_T \\sim N(0, I_d)\\) and generates \\(x_{t-1} = \\mu_{\\theta}(x_t, t) + \\beta_t\\epsilon\\). In the last step, \\(\\mu_{\\theta}(x_1, 1)\\) is displayed without the noise."}]}, {"page": 6, "text": "                                                                                       \u2212\u2192   \u2212\u2192\n    In this section, we consider the diffusion discretized to two steps ({x              0, x 1}), and a Gaussian transition\nkernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures\nessential components of complex diffusion processes without raising unnecessary complications in the\nanalysis. We provide a principled analysis of            Algorithm 1        and   Algorithm 2       in a linear model setting\nwith this two-step diffusion process under assumptions that guarantee exact reconstruction is possible\nin principle. A main result of our work is to prove that in this setting we can solve inverse problems\nperfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory. In\nSection 4, we then show that these algorithmic ideas are much more general, and apply to large-scale\n                                                                                        \u2212\u2192  \u2212\u2192        \u2212\u2192\nreal-world applications of diffusion models that use multiple steps ({x                   0,x 1, \u00b7 \u00b7 \u00b7,xT }, where   T  = 1000),\nand moreover do not satisfy the recoverability assumptions. We provide post-processing details of\nAlgorithm 2        in Appendix B.1. All proofs are given in Appendix A.\n3.1     Problem Setup\nThe goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse\nproblems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion\nprocesses in a linear model setting similar to [40, 7], where the images (x                     \u2212\u21920 \u2208  R d ) reside in a linear\n                           \u2212\u2192       \u2212\u2192           d\u00d7l  \u2212\u2192        l\nsubspace of the form       x 0 =  Sw \u2212\u21920,S \u2208   R     ,w  0 \u2208  R  . Here,   S is a tall thin matrix with       rank(S) =      l\u2264  d\nthat lifts any latent vector         w  0 \u223c N    (0,I  l) to the image space with ambient dimension                    d.  Given\n                                  \u2212\u2192                     l\u00d7d          l                                          \u2212\u2192\nthe measurements         y  =  Ax   0 +  \u03c3 yn,  A \u2208   R     , n  \u2208  R  , the goal is to sample from          p 0(x 0|y)   using a\npre-trained latent diffusion model. In the inpainting task, the measurement operator                            A   is such that\nA T A  is a diagonal matrix        D(m), where       m   is the masking vector with elements set to 1 where data\nis observed and 0 where data is masked (see Appendix A for further details). Recall that in latent\ndiffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder\n(VAE). Following the common practice [39], we consider a setting where the latent vector of the VAE\nis k-dimensional and the latent distribution is a standard Gaussian                    N  (0, I k). Our analysis shows that\nthe proposed      Algorithm 2        provably solves inverse problems under the following assumptions.\nAssumption 3.1.          The columns of the data generating model                 S  are orthonormal, i.e.,      S T S  =  Il.\nAssumption 3.2.          The measurement operator             A  satisfies  (AS)  T (AS)    \u227b  0.\n    These assumptions have previously appeared, e.g., [40]. While                      Assumption 3.1          is mild and can\nbe relaxed at the expense of (standard) mathematical complications,                       Assumption 3.2         indicates that\n(AS)  T (AS)    is a positive definite matrix. The latter ensures that there is enough energy left in the\nmeasurements for perfect reconstruction. More precisely, any subset of                    l coordinates exactly determines\n                                               \u2212\u2192\nthe remaining      (d \u2212  l) coordinates of     x 0. The underlying assumption is that there              exists  a solution and\nit is unique    [40]. Thus, the theoretical question becomes how close the recovered sample is to this\ngroundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors\nand prove that the generated samples are close to this posterior in distribution. However, this does not\nguarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works [40, 7],\nwe analyze posterior sampling in a two-step diffusion model and answer a fundamental question:                                Can\na pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?\n3.2     Posterior Sampling using Pixel-space Diffusion Model        \u2190\u2212\nWe first consider the reverse process, starting with                x 1 \u223c N   (0,  Id), and borrow a result from [40] to\n                             \u2190\u2212                                                                            \u2212\u2192\nshow that the sample         x 0 generated by the reverse process is a valid image from                 p(x  0).\n                                                                 6", "md": "# Diffusion Models\n\n## In this section, we consider the diffusion discretized to two steps ($$x_0, x_1$$), and a Gaussian transition kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures essential components of complex diffusion processes without raising unnecessary complications in the analysis. We provide a principled analysis of Algorithm 1 and Algorithm 2 in a linear model setting with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible in principle. A main result of our work is to prove that in this setting we can solve inverse problems perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory. In Section 4, we then show that these algorithmic ideas are much more general, and apply to large-scale real-world applications of diffusion models that use multiple steps ($$x_0, x_1, \\ldots, x_T$$, where $$T = 1000$$), and moreover do not satisfy the recoverability assumptions. We provide post-processing details of Algorithm 2 in Appendix B.1. All proofs are given in Appendix A.\n\n### 3.1 Problem Setup\n\nThe goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion processes in a linear model setting similar to [40, 7], where the images ($$\\mathbf{x}_0 \\in \\mathbb{R}^d$$) reside in a linear subspace of the form $$\\mathbf{x}_0 = S\\mathbf{w}_0, S \\in \\mathbb{R}^{d \\times l}, \\mathbf{w}_0 \\in \\mathbb{R}^l$$. Here, $$S$$ is a tall thin matrix with $$\\text{rank}(S) = l \\leq d$$ that lifts any latent vector $$\\mathbf{w}_0 \\sim \\mathcal{N}(0, I_l)$$ to the image space with ambient dimension $$d$$. Given the measurements $$\\mathbf{y} = A\\mathbf{x}_0 + \\sigma \\mathbf{n}, A \\in \\mathbb{R}^{l \\times d}, \\mathbf{n} \\in \\mathbb{R}$$, the goal is to sample from $$p_0(\\mathbf{x}_0 | \\mathbf{y})$$ using a pre-trained latent diffusion model. In the inpainting task, the measurement operator $$A$$ is such that $$A^T A$$ is a diagonal matrix $$D(m)$$, where $$m$$ is the masking vector with elements set to 1 where data is observed and 0 where data is masked (see Appendix A for further details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder (VAE). Following the common practice [39], we consider a setting where the latent vector of the VAE is $$k$$-dimensional and the latent distribution is a standard Gaussian $$\\mathcal{N}(0, I_k)$$. Our analysis shows that the proposed Algorithm 2 provably solves inverse problems under the following assumptions.\n\nAssumption 3.1. The columns of the data generating model $$S$$ are orthonormal, i.e., $$S^T S = I_l$$.\n\nAssumption 3.2. The measurement operator $$A$$ satisfies $$(AS)^T (AS) \\succ 0$$.\n\nThese assumptions have previously appeared, e.g., [40]. While Assumption 3.1 is mild and can be relaxed at the expense of (standard) mathematical complications, Assumption 3.2 indicates that $$(AS)^T (AS)$$ is a positive definite matrix. The latter ensures that there is enough energy left in the measurements for perfect reconstruction. More precisely, any subset of $$l$$ coordinates exactly determines the remaining $$(d - l)$$ coordinates of $$\\mathbf{x}_0$$. The underlying assumption is that there exists a solution and it is unique [40]. Thus, the theoretical question becomes how close the recovered sample is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors and prove that the generated samples are close to this posterior in distribution. However, this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works [40, 7], we analyze posterior sampling in a two-step diffusion model and answer a fundamental question: Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?\n\n### 3.2 Posterior Sampling using Pixel-space Diffusion Model\n\nWe first consider the reverse process, starting with $$x_1 \\sim \\mathcal{N}(0, I_d)$$, and borrow a result from [40] to show that the sample $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Diffusion Models", "md": "# Diffusion Models"}, {"type": "heading", "lvl": 2, "value": "In this section, we consider the diffusion discretized to two steps ($$x_0, x_1$$), and a Gaussian transition kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures essential components of complex diffusion processes without raising unnecessary complications in the analysis. We provide a principled analysis of Algorithm 1 and Algorithm 2 in a linear model setting with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible in principle. A main result of our work is to prove that in this setting we can solve inverse problems perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory. In Section 4, we then show that these algorithmic ideas are much more general, and apply to large-scale real-world applications of diffusion models that use multiple steps ($$x_0, x_1, \\ldots, x_T$$, where $$T = 1000$$), and moreover do not satisfy the recoverability assumptions. We provide post-processing details of Algorithm 2 in Appendix B.1. All proofs are given in Appendix A.", "md": "## In this section, we consider the diffusion discretized to two steps ($$x_0, x_1$$), and a Gaussian transition kernel that arises from the Ornstein-Uhlenbeck (OU) process. We choose this setup because it captures essential components of complex diffusion processes without raising unnecessary complications in the analysis. We provide a principled analysis of Algorithm 1 and Algorithm 2 in a linear model setting with this two-step diffusion process under assumptions that guarantee exact reconstruction is possible in principle. A main result of our work is to prove that in this setting we can solve inverse problems perfectly. As we show, this requires some novel algorithmic ideas that are suggested by our theory. In Section 4, we then show that these algorithmic ideas are much more general, and apply to large-scale real-world applications of diffusion models that use multiple steps ($$x_0, x_1, \\ldots, x_T$$, where $$T = 1000$$), and moreover do not satisfy the recoverability assumptions. We provide post-processing details of Algorithm 2 in Appendix B.1. All proofs are given in Appendix A."}, {"type": "heading", "lvl": 3, "value": "3.1 Problem Setup", "md": "### 3.1 Problem Setup"}, {"type": "text", "value": "The goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion processes in a linear model setting similar to [40, 7], where the images ($$\\mathbf{x}_0 \\in \\mathbb{R}^d$$) reside in a linear subspace of the form $$\\mathbf{x}_0 = S\\mathbf{w}_0, S \\in \\mathbb{R}^{d \\times l}, \\mathbf{w}_0 \\in \\mathbb{R}^l$$. Here, $$S$$ is a tall thin matrix with $$\\text{rank}(S) = l \\leq d$$ that lifts any latent vector $$\\mathbf{w}_0 \\sim \\mathcal{N}(0, I_l)$$ to the image space with ambient dimension $$d$$. Given the measurements $$\\mathbf{y} = A\\mathbf{x}_0 + \\sigma \\mathbf{n}, A \\in \\mathbb{R}^{l \\times d}, \\mathbf{n} \\in \\mathbb{R}$$, the goal is to sample from $$p_0(\\mathbf{x}_0 | \\mathbf{y})$$ using a pre-trained latent diffusion model. In the inpainting task, the measurement operator $$A$$ is such that $$A^T A$$ is a diagonal matrix $$D(m)$$, where $$m$$ is the masking vector with elements set to 1 where data is observed and 0 where data is masked (see Appendix A for further details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder (VAE). Following the common practice [39], we consider a setting where the latent vector of the VAE is $$k$$-dimensional and the latent distribution is a standard Gaussian $$\\mathcal{N}(0, I_k)$$. Our analysis shows that the proposed Algorithm 2 provably solves inverse problems under the following assumptions.\n\nAssumption 3.1. The columns of the data generating model $$S$$ are orthonormal, i.e., $$S^T S = I_l$$.\n\nAssumption 3.2. The measurement operator $$A$$ satisfies $$(AS)^T (AS) \\succ 0$$.\n\nThese assumptions have previously appeared, e.g., [40]. While Assumption 3.1 is mild and can be relaxed at the expense of (standard) mathematical complications, Assumption 3.2 indicates that $$(AS)^T (AS)$$ is a positive definite matrix. The latter ensures that there is enough energy left in the measurements for perfect reconstruction. More precisely, any subset of $$l$$ coordinates exactly determines the remaining $$(d - l)$$ coordinates of $$\\mathbf{x}_0$$. The underlying assumption is that there exists a solution and it is unique [40]. Thus, the theoretical question becomes how close the recovered sample is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors and prove that the generated samples are close to this posterior in distribution. However, this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works [40, 7], we analyze posterior sampling in a two-step diffusion model and answer a fundamental question: Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?", "md": "The goal is to show that posterior sampling algorithms (such as DPS) can provably solve inverse problems in a perfectly recoverable setting. To show exact recovery, we analyze two-step diffusion processes in a linear model setting similar to [40, 7], where the images ($$\\mathbf{x}_0 \\in \\mathbb{R}^d$$) reside in a linear subspace of the form $$\\mathbf{x}_0 = S\\mathbf{w}_0, S \\in \\mathbb{R}^{d \\times l}, \\mathbf{w}_0 \\in \\mathbb{R}^l$$. Here, $$S$$ is a tall thin matrix with $$\\text{rank}(S) = l \\leq d$$ that lifts any latent vector $$\\mathbf{w}_0 \\sim \\mathcal{N}(0, I_l)$$ to the image space with ambient dimension $$d$$. Given the measurements $$\\mathbf{y} = A\\mathbf{x}_0 + \\sigma \\mathbf{n}, A \\in \\mathbb{R}^{l \\times d}, \\mathbf{n} \\in \\mathbb{R}$$, the goal is to sample from $$p_0(\\mathbf{x}_0 | \\mathbf{y})$$ using a pre-trained latent diffusion model. In the inpainting task, the measurement operator $$A$$ is such that $$A^T A$$ is a diagonal matrix $$D(m)$$, where $$m$$ is the masking vector with elements set to 1 where data is observed and 0 where data is masked (see Appendix A for further details). Recall that in latent diffusion models, the diffusion takes place in the latent space of a pre-trained Variational Autoencoder (VAE). Following the common practice [39], we consider a setting where the latent vector of the VAE is $$k$$-dimensional and the latent distribution is a standard Gaussian $$\\mathcal{N}(0, I_k)$$. Our analysis shows that the proposed Algorithm 2 provably solves inverse problems under the following assumptions.\n\nAssumption 3.1. The columns of the data generating model $$S$$ are orthonormal, i.e., $$S^T S = I_l$$.\n\nAssumption 3.2. The measurement operator $$A$$ satisfies $$(AS)^T (AS) \\succ 0$$.\n\nThese assumptions have previously appeared, e.g., [40]. While Assumption 3.1 is mild and can be relaxed at the expense of (standard) mathematical complications, Assumption 3.2 indicates that $$(AS)^T (AS)$$ is a positive definite matrix. The latter ensures that there is enough energy left in the measurements for perfect reconstruction. More precisely, any subset of $$l$$ coordinates exactly determines the remaining $$(d - l)$$ coordinates of $$\\mathbf{x}_0$$. The underlying assumption is that there exists a solution and it is unique [40]. Thus, the theoretical question becomes how close the recovered sample is to this groundtruth sample from the true posterior. Alternatively, one may consider other types of posteriors and prove that the generated samples are close to this posterior in distribution. However, this does not guarantee that the exact groundtruth sample is recovered. Therefore, motivated by prior works [40, 7], we analyze posterior sampling in a two-step diffusion model and answer a fundamental question: Can a pre-trained latent diffusion model provably solve inverse problems in a perfectly recoverable setting?"}, {"type": "heading", "lvl": 3, "value": "3.2 Posterior Sampling using Pixel-space Diffusion Model", "md": "### 3.2 Posterior Sampling using Pixel-space Diffusion Model"}, {"type": "text", "value": "We first consider the reverse process, starting with $$x_1 \\sim \\mathcal{N}(0, I_d)$$, and borrow a result from [40] to show that the sample $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$.", "md": "We first consider the reverse process, starting with $$x_1 \\sim \\mathcal{N}(0, I_d)$$, and borrow a result from [40] to show that the sample $$x_0$$ generated by the reverse process is a valid image from $$p(x_0)$$."}]}, {"page": 7, "text": " Theorem 3.3        (Generative Modeling using Diffusion in Pixel Space, [40]).                 Suppose    Assumption 3.1\n holds. Let                \u2217                      [ \u2225    (\u2212\u2192  \u2212\u2192   \u2212       )        (    (        ))\u22252 ]\n                         \u03b8   = arg min    E\u2212\u2192\u2212\u2192     \u2225\u02dc             \u2192    \u2212\u2192           \u2212\u2192   \u2212\u2192  \u2212\u2192    \u2225\n                                       \u03b8    x 0,\u03f5    \u03bc 1  x 1(x 0,  \u03f5 ),x 0  \u2212  \u03bc \u03b8  x 1  x 0, \u03f5         .\n                                             (\u2212\u2192  (\u2212\u2192   \u2212  ))           (        )\n                                                        \u2192     :     \u2212\u2192   \u2212\u2192  \u2212\u2192                                              \u2217\n For a fixed variance       \u03b2 >    0, if \u03bc \u03b8  x 1  x 0,  \u03f5     =   \u03b8x 1  x 0,  \u03f5  , then the closed-form solution           \u03b8   is\n \u221a            T                                           \u221a                                                 (\u2212\u2192  )\n   1 \u2212  \u03b2SS     , which after normalization by          1/   1\u2212   \u03b2  recovers the true subspace of        p  x 0  .\n                                          \u2190\u2212                                                                           \u2212\u2192\n     Though this establishes that         x 0  generated by the reverse process is a valid image from               p(x  0), it is\n                                                            \u2212\u2192\n not necessarily a sample from the posterior              p(x 0 |y) that satisfies the measurements. To accomplish\n this we perform one additional step of gradient descent for every step of the reverse process. This gives\n us Algorithm 1, the DPS algorithm. The next theorem shows that the reverse SDE guided by these\n measurements (3) recovers the true underlying sample                 2.\n Theorem 3.4        (Posterior Sampling using Diffusion in Pixel Space).                  Suppose    Assumption 3.1          and\nAssumption 3.2          hold. Let us denote by        \u03c3 j,\u2200j  = 1, . . . , r, the singular values of     (AS)  T (AS)   and\n                           \u2217                      [ \u2225    (\u2212\u2192  \u2212\u2192   \u2212       )        (    (        ))\u22252 ]\n                         \u03b8   = arg min    E \u2212\u2192\u2212\u2192    \u2225\u02dc             \u2192    \u2212\u2192           \u2212\u2192   \u2212\u2192  \u2212\u2192    \u2225\n                                       \u03b8 \u2212\u2192 x 0,\u03f5 \u2212\u2192 \u03bc 1  x 1(x 0,  \u03f5 ),x 0  \u2212  \u03bc \u03b8  x 1  x 0, \u03f5         .            j\nGiven a partially known image            x 0 \u223c  p(x 0), a fixed variance     \u03b2 >   0, there exists a step size      \u03b6i  = 1/2\u03c3   j\n                                   \u2212\u2192                                                                                \u2212\u2192\n for all the coordinates of        x 0 such that    Algorithm 1         samples from the true posterior           p(x  0|y)  and\n                                                           \u2190\u2212     \u2212\u2192\nexactly recovers the groundtruth sample, i.e.,             x 0 =  x 0.\n 3.3     Posterior Sampling using Latent Diffusion Model\n In this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7),\n displayed in    Algorithm        2. We consider the case where the latent distribution of the VAE is in the\n same space as the latent distribution of the data generating model, i.e.,                 k =   l, and normalize     \u03b3i = 1  (as\n this is immaterial in the linear setting). In           Proposition 3.5, we provide analytical solutions for the\n encoder and the decoder of the VAE.\n Proposition 3.5          (Variational Autoencoder).           Suppose     Assumption 3.1           holds.    For an encoder\n E : R d \u2192   R k  and a decoder     D  : R k \u2192   R d, denote by    L(\u03d5, \u03c9)    the training objective of VAE:\n                                                  [ \u2225       \u2212\u2192             \u2212\u2192 \u22252 ]\n                  arg minL(\u03d5, \u03c9) :=       E \u2212\u2192      \u2225D(E(x   0 ;\u03d5);\u03c9)   \u2212 x  0\u2225    +  \u03bbKL   (E\u266fp,N    (0, I k)),\n                       \u03d5,\u03c9                  x 0\u223cp                              2\n                                   \u2212\u2192           T \u2212\u2192           \u2190\u2212           \u2190\u2212\nthen the combination of         E(x  0;\u03d5) =   S   x 0 and   D(z  0; \u03c9) =   Sz 0 is a minimizer of      L  (\u03d5, \u03c9).\n                                 \u2212\u2192           T \u2212\u2192                                                  \u2217\n     Using the encoder        E(x  0;\u03d5) =   S   x\u22170, we can use the analytical solution   (\u2212\u2192 )   \u03b8   of the LDM obtained\n in Theorem 3.3.           To verify that      \u03b8   recovers the true subspace           p  x 0 , we compose the decoder\n    \u2190\u2212             \u2190\u2212                                                     \u2190\u2212         (  \u2217\u2190\u2212 )        (   \u2190\u2212  )       \u2190\u2212\n D(z  0; \u03c9) =    Sz 0  with the generator of the LDM, i.e.,               x 0 =   D   \u03b8  z 1   =  D    Ik z1   =   Sz 1 .  Since\n \u2190\u2212                                                                                     \u2190\u2212                           \u2212\u2192\n z 1 \u223c N   (0, I k) and   S  is the data generating model, this shows that              x 0 is a sample from      p(x  0). Thus\n we have the following.\n Theorem 3.6         (Generative Modeling using Diffusion in Latent Space).                    Suppose     Assumption 3.1\n holds. Let the optimal solution of the latent diffusion model be\n                           \u2217                       [\u2225    (\u2212                )        (   (        ))\u22252 ]\n                                                          \u2192   \u2212\u2192   \u2212\n                                                                   \u2192    \u2212\u2192           \u2212\u2192   \u2212\u2192  \u2212\n                          \u03b8   = arg min    E \u2212\u2192\u2212\u2192   \u2225\u03bc\u02dc1  z 1(z 0,  \u03f5 ),z 0  \u2212  \u03bc \u03b8  z 1  z0 ,\u2192\u03f5   \u2225    .\n                                       \u03b8     z0,\u03f5\n    2While the DPS Algorithm [11] uses a scalar step size     \u03b6iat each step, this does not suffice for exact recovery. However,\n by generalizing to allow a different step size per coordinate, we can show sample recovery. Thus, in this section, we denote\n  j\n \u03b6i to be the step size at step  iand coordinate   j, 1 \u2264 j \u2264 r. Also note that the step index    iis vacuous in this section, as\n we consider a two-step diffusion process (i.e.,  i is always \u20191\u2019).\n                                                                 7", "md": "# Math Equations\n\n## Theorem 3.3 (Generative Modeling using Diffusion in Pixel Space, [40])\n\nSuppose Assumption 3.1 holds. Let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\tilde{\\rightarrow} - \\rightarrow \\right\\|\n$$\nFor a fixed variance $\\beta > 0$, if $\\mu_{\\theta} x_1 x_0, \\epsilon = \\theta x_1 x_0, \\epsilon$, then the closed-form solution $\\theta$ is $\\sqrt{1 - \\beta SS_T}$, which after normalization by $1/\\sqrt{1 - \\beta}$ recovers the true subspace of $p(x_0)$.\n\nThough this establishes that $x_0$ generated by the reverse process is a valid image from $p(x_0)$, it is not necessarily a sample from the posterior $p(x_0 | y)$ that satisfies the measurements. To accomplish this we perform one additional step of gradient descent for every step of the reverse process. This gives us Algorithm 1, the DPS algorithm. The next theorem shows that the reverse SDE guided by these measurements recovers the true underlying sample.\n\n## Theorem 3.4 (Posterior Sampling using Diffusion in Pixel Space)\n\nSuppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $\\sigma_j, \\forall j = 1, ..., r$, the singular values of $(AS)^T (AS)$ and\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\tilde{\\rightarrow} - \\rightarrow \\right\\|\n$$\nGiven a partially known image $x_0 \\sim p(x_0)$, a fixed variance $\\beta > 0$, there exists a step size $\\zeta_i = 1/2\\sigma_j$ for all the coordinates of $x_0$ such that Algorithm 1 samples from the true posterior $p(x_0 | y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = x_0$.\n\n## 3.3 Posterior Sampling using Latent Diffusion Model\n\nIn this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7), displayed in Algorithm 2. We consider the case where the latent distribution of the VAE is in the same space as the latent distribution of the data generating model, i.e., $k = l$, and normalize $\\gamma_i = 1$ (as this is immaterial in the linear setting). In Proposition 3.5, we provide analytical solutions for the encoder and the decoder of the VAE.\n\n## Proposition 3.5 (Variational Autoencoder)\n\nSuppose Assumption 3.1 holds. For an encoder $E: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ and a decoder $D: \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$, denote by $L(\\phi, \\omega)$ the training objective of VAE:\n\n$$\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{\\rightarrow} \\left\\| D(E(x_0; \\phi); \\omega) - x_0 \\right\\|_2 + \\lambda \\text{KL}(E_{\\#}p_N(0, I_k))\n$$\nthen the combination of $E(x_0; \\phi) = S x_0$ and $D(z_0; \\omega) = Sz_0$ is a minimizer of $L(\\phi, \\omega)$.\n\nUsing the encoder $E(x_0; \\phi) = S x^*_0$, we can use the analytical solution $\\theta$ of the LDM obtained in Theorem 3.3. To verify that $\\theta$ recovers the true subspace $p(x_0)$, we compose the decoder $D(z_0; \\omega) = Sz_0$ with the generator of the LDM, i.e., $x_0 = D \\theta z_1 = D I_k z_1 = Sz_1$. Since $z_1 \\sim N(0, I_k)$ and $S$ is the data generating model, this shows that $x_0$ is a sample from $p(x_0)$. Thus we have the following.\n\n## Theorem 3.6 (Generative Modeling using Diffusion in Latent Space)\n\nSuppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\epsilon \\right\\|\n$$\nWhile the DPS Algorithm uses a scalar step size $\\zeta_i$ at each step, this does not suffice for exact recovery. However, by generalizing to allow a different step size per coordinate, we can show sample recovery. Thus, in this section, we denote $\\zeta_i$ to be the step size at step $i$ and coordinate $j$, $1 \\leq j \\leq r$. Also note that the step index $i$ is vacuous in this section, as we consider a two-step diffusion process (i.e., $i$ is always '1').", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "heading", "lvl": 2, "value": "Theorem 3.3 (Generative Modeling using Diffusion in Pixel Space, [40])", "md": "## Theorem 3.3 (Generative Modeling using Diffusion in Pixel Space, [40])"}, {"type": "text", "value": "Suppose Assumption 3.1 holds. Let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\tilde{\\rightarrow} - \\rightarrow \\right\\|\n$$\nFor a fixed variance $\\beta > 0$, if $\\mu_{\\theta} x_1 x_0, \\epsilon = \\theta x_1 x_0, \\epsilon$, then the closed-form solution $\\theta$ is $\\sqrt{1 - \\beta SS_T}$, which after normalization by $1/\\sqrt{1 - \\beta}$ recovers the true subspace of $p(x_0)$.\n\nThough this establishes that $x_0$ generated by the reverse process is a valid image from $p(x_0)$, it is not necessarily a sample from the posterior $p(x_0 | y)$ that satisfies the measurements. To accomplish this we perform one additional step of gradient descent for every step of the reverse process. This gives us Algorithm 1, the DPS algorithm. The next theorem shows that the reverse SDE guided by these measurements recovers the true underlying sample.", "md": "Suppose Assumption 3.1 holds. Let\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\tilde{\\rightarrow} - \\rightarrow \\right\\|\n$$\nFor a fixed variance $\\beta > 0$, if $\\mu_{\\theta} x_1 x_0, \\epsilon = \\theta x_1 x_0, \\epsilon$, then the closed-form solution $\\theta$ is $\\sqrt{1 - \\beta SS_T}$, which after normalization by $1/\\sqrt{1 - \\beta}$ recovers the true subspace of $p(x_0)$.\n\nThough this establishes that $x_0$ generated by the reverse process is a valid image from $p(x_0)$, it is not necessarily a sample from the posterior $p(x_0 | y)$ that satisfies the measurements. To accomplish this we perform one additional step of gradient descent for every step of the reverse process. This gives us Algorithm 1, the DPS algorithm. The next theorem shows that the reverse SDE guided by these measurements recovers the true underlying sample."}, {"type": "heading", "lvl": 2, "value": "Theorem 3.4 (Posterior Sampling using Diffusion in Pixel Space)", "md": "## Theorem 3.4 (Posterior Sampling using Diffusion in Pixel Space)"}, {"type": "text", "value": "Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $\\sigma_j, \\forall j = 1, ..., r$, the singular values of $(AS)^T (AS)$ and\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\tilde{\\rightarrow} - \\rightarrow \\right\\|\n$$\nGiven a partially known image $x_0 \\sim p(x_0)$, a fixed variance $\\beta > 0$, there exists a step size $\\zeta_i = 1/2\\sigma_j$ for all the coordinates of $x_0$ such that Algorithm 1 samples from the true posterior $p(x_0 | y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = x_0$.", "md": "Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $\\sigma_j, \\forall j = 1, ..., r$, the singular values of $(AS)^T (AS)$ and\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\tilde{\\rightarrow} - \\rightarrow \\right\\|\n$$\nGiven a partially known image $x_0 \\sim p(x_0)$, a fixed variance $\\beta > 0$, there exists a step size $\\zeta_i = 1/2\\sigma_j$ for all the coordinates of $x_0$ such that Algorithm 1 samples from the true posterior $p(x_0 | y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = x_0$."}, {"type": "heading", "lvl": 2, "value": "3.3 Posterior Sampling using Latent Diffusion Model", "md": "## 3.3 Posterior Sampling using Latent Diffusion Model"}, {"type": "text", "value": "In this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7), displayed in Algorithm 2. We consider the case where the latent distribution of the VAE is in the same space as the latent distribution of the data generating model, i.e., $k = l$, and normalize $\\gamma_i = 1$ (as this is immaterial in the linear setting). In Proposition 3.5, we provide analytical solutions for the encoder and the decoder of the VAE.", "md": "In this section, we analyze two approximations: GML-DPS based on (6), and PSLD based on (7), displayed in Algorithm 2. We consider the case where the latent distribution of the VAE is in the same space as the latent distribution of the data generating model, i.e., $k = l$, and normalize $\\gamma_i = 1$ (as this is immaterial in the linear setting). In Proposition 3.5, we provide analytical solutions for the encoder and the decoder of the VAE."}, {"type": "heading", "lvl": 2, "value": "Proposition 3.5 (Variational Autoencoder)", "md": "## Proposition 3.5 (Variational Autoencoder)"}, {"type": "text", "value": "Suppose Assumption 3.1 holds. For an encoder $E: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ and a decoder $D: \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$, denote by $L(\\phi, \\omega)$ the training objective of VAE:\n\n$$\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{\\rightarrow} \\left\\| D(E(x_0; \\phi); \\omega) - x_0 \\right\\|_2 + \\lambda \\text{KL}(E_{\\#}p_N(0, I_k))\n$$\nthen the combination of $E(x_0; \\phi) = S x_0$ and $D(z_0; \\omega) = Sz_0$ is a minimizer of $L(\\phi, \\omega)$.\n\nUsing the encoder $E(x_0; \\phi) = S x^*_0$, we can use the analytical solution $\\theta$ of the LDM obtained in Theorem 3.3. To verify that $\\theta$ recovers the true subspace $p(x_0)$, we compose the decoder $D(z_0; \\omega) = Sz_0$ with the generator of the LDM, i.e., $x_0 = D \\theta z_1 = D I_k z_1 = Sz_1$. Since $z_1 \\sim N(0, I_k)$ and $S$ is the data generating model, this shows that $x_0$ is a sample from $p(x_0)$. Thus we have the following.", "md": "Suppose Assumption 3.1 holds. For an encoder $E: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ and a decoder $D: \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$, denote by $L(\\phi, \\omega)$ the training objective of VAE:\n\n$$\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{\\rightarrow} \\left\\| D(E(x_0; \\phi); \\omega) - x_0 \\right\\|_2 + \\lambda \\text{KL}(E_{\\#}p_N(0, I_k))\n$$\nthen the combination of $E(x_0; \\phi) = S x_0$ and $D(z_0; \\omega) = Sz_0$ is a minimizer of $L(\\phi, \\omega)$.\n\nUsing the encoder $E(x_0; \\phi) = S x^*_0$, we can use the analytical solution $\\theta$ of the LDM obtained in Theorem 3.3. To verify that $\\theta$ recovers the true subspace $p(x_0)$, we compose the decoder $D(z_0; \\omega) = Sz_0$ with the generator of the LDM, i.e., $x_0 = D \\theta z_1 = D I_k z_1 = Sz_1$. Since $z_1 \\sim N(0, I_k)$ and $S$ is the data generating model, this shows that $x_0$ is a sample from $p(x_0)$. Thus we have the following."}, {"type": "heading", "lvl": 2, "value": "Theorem 3.6 (Generative Modeling using Diffusion in Latent Space)", "md": "## Theorem 3.6 (Generative Modeling using Diffusion in Latent Space)"}, {"type": "text", "value": "Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\epsilon \\right\\|\n$$\nWhile the DPS Algorithm uses a scalar step size $\\zeta_i$ at each step, this does not suffice for exact recovery. However, by generalizing to allow a different step size per coordinate, we can show sample recovery. Thus, in this section, we denote $\\zeta_i$ to be the step size at step $i$ and coordinate $j$, $1 \\leq j \\leq r$. Also note that the step index $i$ is vacuous in this section, as we consider a two-step diffusion process (i.e., $i$ is always '1').", "md": "Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n\n$$\n\\theta^* = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\epsilon \\right\\|\n$$\nWhile the DPS Algorithm uses a scalar step size $\\zeta_i$ at each step, this does not suffice for exact recovery. However, by generalizing to allow a different step size per coordinate, we can show sample recovery. Thus, in this section, we denote $\\zeta_i$ to be the step size at step $i$ and coordinate $j$, $1 \\leq j \\leq r$. Also note that the step index $i$ is vacuous in this section, as we consider a two-step diffusion process (i.e., $i$ is always '1')."}]}, {"page": 8, "text": "                                               ( \u2212  (        ))            (       )\n                                                 \u2192    \u2212\n                                                      \u2192   \u2212\u2192           \u2212\u2192   \u2212\u2192  \u2212\u2192\n For a fixed variance        \u03b2  >   0, if   \u03bc\u03b8   z1   z0,  \u03f5     :=   \u03b8z 1  z 0, \u03f5  , then the closed-form solution is\n  \u2217     \u221a                                                  \u221a  1                                                 (\u2190\u2212    )      \u2190\u2212\n \u03b8   =    1 \u2212  \u03b2I k , which after normalization by           1\u2212\u03b2   and composition with the decoder          D   z 0; \u03c9  =   Sz 0\n                                        (\u2212\u2192 )\n recovers the true subspace of        p  x 0 .\n     With this optimal       \u03b8 \u2217, we can now prove exact sample recovery using GML-DPS (6).\n Theorem 3.7         (Posterior Sampling using Goodness Modified Latent DPS).                       Let  Assumptions 3.1\nand   3.2   hold. Let   \u03c3 j,\u2200j  = 1, . . . , r, denote the singular values of       (AS)  T (AS), and let\n                           \u2217                       [\u2225    (\u2212                )       (    (        ))\u22252 ]\n                                                          \u2192   \u2212\u2192   \u2212\n                                                                   \u2192    \u2212\u2192          \u2212\u2192   \u2212\u2192   \u2212\n                          \u03b8  = arg min     E\u2212\u2192\u2212\u2192    \u2225\u03bc\u02dc1  z 1(z 0, \u03f5  ),z 0  \u2212  \u03bc \u03b8  z1   z0 ,\u2192\u03f5   \u2225    .\n                                       \u03b8 \u2212\u2192 z 0,\u03f5  \u2212\u2192\nGiven a partially known image            x 0 \u223c  p(x 0 ), any fixed variance      \u03b2  \u2208 (0, 1), then with the (unique) step\n        j                                                                                                                  \u2212\u2192\n size \u03b7   = 1/2\u03c3   j, j = 1, 2, . . . , r, the GML-DPS Algorithm (6) samples from the true posterior                     p(x 0|y)\n        i                                                       \u2190\u2212     \u2212\u2192\nand exactly recovers the groundtruth sample, i.e.,              x 0 =  x 0.\n     Theorem 3.7         shows that GML-DPS (6) recovers the true sample using an LDM. This approach,\n however, requires the step size          \u03b7 to be chosen      coordinate-wise      in a specific manner. Also, multiple\n natural images could have the same measurements in the pixel space. This is a reasonable concern for\n LDMs due to one-to-many mappings of the decoder. Note that the                          goodness objective      (Section 2.1)\n                                                                                                                  \u2190\u2212\n cannot help in this scenario because it assigns uniform probability to many of these latents                     z 1 for which\n      \u2223\u2223\u2190\u2212  \u2190\u2212             \u2190\u2212  \u2190\u2212     \u2223\u22232\n \u2207 \u2190\u2212 \u2223\u2223z 0(z 1)]\u2212 E(D(z     0(z 1))) \u2223\u2223  = 0. These challenges motivate the           gluing objective    in  Theorem 3.8.\n   z1\n This is crucial for two reasons. First, we show that it helps recover the true sample even when the step\n size \u03b7  is chosen arbitrarily. Second, it assigns all the probability mass to the desired (unique) solution\n in the pixel space.\n Theorem 3.8        (Posterior Sampling using Diffusion in Latent Space).                Let  Assumptions 3.1          and   3.2\n hold. Let   \u03c3 j,\u2200j  = 1, . . . , rdenote the singular values of        (AS)  T (AS)    and let\n                           \u2217                       [\u2225    (\u2212                )       (    (        ))\u22252 ]\n                                                          \u2192   \u2212\u2192   \u2212\n                                                                   \u2192    \u2212\u2192          \u2212\u2192   \u2212\u2192   \u2212\n                          \u03b8  = arg min     E\u2212\u2192\u2212\u2192    \u2225\u03bc\u02dc1  z 1(z 0, \u03f5  ),z 0  \u2212  \u03bc \u03b8  z1   z0 ,\u2192\u03f5   \u2225    .\n                                       \u03b8 \u2212\u2192 z 0,\u03f5  \u2212\u2192\nGiven a partially known image            x 0 \u223c  p(x  0), any fixed variance       \u03b2 \u2208  (0, 1), and any positive step sizes\n \u03b7j, j = 1,  2, . . . , r, the PSLD Algorithm 2 samples from the true posterior               p(x\u2212\u21920|y) and exactly recovers\n  i                                   \u2190\u2212     \u2212\u2192\nthe groundtruth sample, i.e.,         x 0 =  x 0.\n     The important distinction between             Theorem 3.7         and  Theorem 3.8         is that the former requires\n the  exact  step size while the latter works for any finite step size. Combining denoising, measurement-\n consistency (with a scalar        \u03b7), and gluing updates, we have\n                             \u2225                     \u2225           \u2225                                                         \u2225 2\n     \u2190\u2212       \u2217\u2190\u2212        \u2190\u2212  \u2225      \u2190\u2212  \u2190\u2212         \u22252      \u2190\u2212  \u2225\u2190\u2212  \u2190\u2212           T   \u2212\u2192               T        \u2190\u2212  \u2190\u2212    \u2225\n     z 0 =  \u03b8  z 1 \u2212\u03b7\u2207   z 1  AD(z   0 (z 1)) \u2212  y  2 \u2212 \u2207  z 1 \u2225z 0(z 1) \u2212 E(A     Ax  0 + (I  d \u2212 A    A)D(z   0 (z 1)))\u2225 2 .\n When    \u03b7  is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution\n \u2212\u2192                                                                                                      \u2190\u2212   \u2190\u2212       \u2212\u2192\n z0 . When the reverse SDE generates the exact same groundtruth sample, i.e.,                         D(z  1 (z0 )) =  x 0, then\n the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the\n reverse SDE to recover the true underlying sample irrespective of the value of                      \u03b7.\n     We draw the following key insights from our               Theorem 3.8:        Curse of ambient dimension:                 In\n order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement\n error needs to be computed in the             d-dimensional ambient space. Therefore, DPS algorithm suffers\n from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent\n space, and therefore avoids the curse of ambient dimension.                   Large-scale foundation model:                 We\n propose a posterior sampling algorithm which offers the provision to use large-scale foundation models,\n and it provably solves general linear inverse problems.                 Robustness to measurement step:                     The\n gluing objective makes our algorithm robust to the choice of step size                     \u03b7. Furthermore, it allows the\n                                                                   \u2212\u2192\n same (scalar) step size across all the coordinates of          8  x 0.", "md": "For a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} z_1 z_0, \\epsilon := \\theta z_1 z_0, \\epsilon$$, then the closed-form solution is\n\n$$\n\\theta^* = 1 - \\beta I_k\n$$\nwhich after normalization by $$1-\\beta$$ and composition with the decoder $$D z_0; \\omega = Sz_0$$ recovers the true subspace of $$p x_0$$.\n\nWith this optimal $$\\theta^*$$, we can now prove exact sample recovery using GML-DPS (6).\n\nTheorem 3.7 (Posterior Sampling using Goodness Modified Latent DPS). Let Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$, denote the singular values of $$(AS)^T (AS)$$, and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu \\tilde{z}_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\rightarrow \\epsilon \\right\\|_{\\theta \\rightarrow z_0, \\epsilon}\n$$\n\nGiven a partially known image $$x_0 \\sim p(x_0)$$, any fixed variance $$\\beta \\in (0, 1)$$, then with the (unique) step size $$\\eta = 1/2\\sigma_j, j = 1, 2, ..., r$$, the GML-DPS Algorithm (6) samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nTheorem 3.7 shows that GML-DPS (6) recovers the true sample using an LDM. This approach, however, requires the step size $$\\eta$$ to be chosen coordinate-wise in a specific manner. Also, multiple natural images could have the same measurements in the pixel space. This is a reasonable concern for LDMs due to one-to-many mappings of the decoder. Note that the goodness objective cannot help in this scenario because it assigns uniform probability to many of these latents $$z_1$$ for which $$\\left\\| \\nabla_{z_0(z_1)} - E(D(z_0(z_1))) \\right\\|_2 = 0$$. These challenges motivate the gluing objective in Theorem 3.8.\n\nTheorem 3.8 (Posterior Sampling using Diffusion in Latent Space). Let Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$ denote the singular values of $$(AS)^T (AS)$$, and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu \\tilde{z}_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\rightarrow \\epsilon \\right\\|_{\\theta \\rightarrow z_0, \\epsilon}\n$$\n\nGiven a partially known image $$x_0 \\sim p(x_0)$$, any fixed variance $$\\beta \\in (0, 1)$$, and any positive step sizes $$\\eta_j, j = 1, 2, ..., r$$, the PSLD Algorithm 2 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nThe important distinction between Theorem 3.7 and Theorem 3.8 is that the former requires the exact step size while the latter works for any finite step size. Combining denoising, measurement-consistency (with a scalar $$\\eta$$), and gluing updates, we have\n\n$$\n\\left\\| \\theta^* \\leftarrow \\tilde{\\theta} \\leftarrow \\theta \\right\\|_2 \\leftarrow \\left\\| z_0 = \\theta z_1 - \\eta \\nabla z_1 AD(z_0(z_1)) - y \\right\\|_2 - \\nabla z_1 \\left\\| z_0(z_1) - E(AAx_0 + (I_d - AA)D(z_0(z_1))) \\right\\|_2\n$$\n\nWhen $$\\eta$$ is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution $$z_0$$. When the reverse SDE generates the exact same groundtruth sample, i.e., $$D(z_1(z_0)) = x_0$$, then the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the reverse SDE to recover the true underlying sample irrespective of the value of $$\\eta$$.\n\nWe draw the following key insights from our Theorem 3.8:\n\n- Curse of ambient dimension: In order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error needs to be computed in the $$d$$-dimensional ambient space. Therefore, DPS algorithm suffers from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space, and therefore avoids the curse of ambient dimension.\n\n- Large-scale foundation model: We propose a posterior sampling algorithm which offers the provision to use large-scale foundation models, and it provably solves general linear inverse problems.\n\n- Robustness to measurement step: The gluing objective makes our algorithm robust to the choice of step size $$\\eta$$. Furthermore, it allows the same (scalar) step size across all the coordinates of $$x_0$$.", "images": [], "items": [{"type": "text", "value": "For a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} z_1 z_0, \\epsilon := \\theta z_1 z_0, \\epsilon$$, then the closed-form solution is\n\n$$\n\\theta^* = 1 - \\beta I_k\n$$\nwhich after normalization by $$1-\\beta$$ and composition with the decoder $$D z_0; \\omega = Sz_0$$ recovers the true subspace of $$p x_0$$.\n\nWith this optimal $$\\theta^*$$, we can now prove exact sample recovery using GML-DPS (6).\n\nTheorem 3.7 (Posterior Sampling using Goodness Modified Latent DPS). Let Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$, denote the singular values of $$(AS)^T (AS)$$, and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu \\tilde{z}_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\rightarrow \\epsilon \\right\\|_{\\theta \\rightarrow z_0, \\epsilon}\n$$\n\nGiven a partially known image $$x_0 \\sim p(x_0)$$, any fixed variance $$\\beta \\in (0, 1)$$, then with the (unique) step size $$\\eta = 1/2\\sigma_j, j = 1, 2, ..., r$$, the GML-DPS Algorithm (6) samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nTheorem 3.7 shows that GML-DPS (6) recovers the true sample using an LDM. This approach, however, requires the step size $$\\eta$$ to be chosen coordinate-wise in a specific manner. Also, multiple natural images could have the same measurements in the pixel space. This is a reasonable concern for LDMs due to one-to-many mappings of the decoder. Note that the goodness objective cannot help in this scenario because it assigns uniform probability to many of these latents $$z_1$$ for which $$\\left\\| \\nabla_{z_0(z_1)} - E(D(z_0(z_1))) \\right\\|_2 = 0$$. These challenges motivate the gluing objective in Theorem 3.8.\n\nTheorem 3.8 (Posterior Sampling using Diffusion in Latent Space). Let Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$ denote the singular values of $$(AS)^T (AS)$$, and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu \\tilde{z}_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\rightarrow \\epsilon \\right\\|_{\\theta \\rightarrow z_0, \\epsilon}\n$$\n\nGiven a partially known image $$x_0 \\sim p(x_0)$$, any fixed variance $$\\beta \\in (0, 1)$$, and any positive step sizes $$\\eta_j, j = 1, 2, ..., r$$, the PSLD Algorithm 2 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nThe important distinction between Theorem 3.7 and Theorem 3.8 is that the former requires the exact step size while the latter works for any finite step size. Combining denoising, measurement-consistency (with a scalar $$\\eta$$), and gluing updates, we have\n\n$$\n\\left\\| \\theta^* \\leftarrow \\tilde{\\theta} \\leftarrow \\theta \\right\\|_2 \\leftarrow \\left\\| z_0 = \\theta z_1 - \\eta \\nabla z_1 AD(z_0(z_1)) - y \\right\\|_2 - \\nabla z_1 \\left\\| z_0(z_1) - E(AAx_0 + (I_d - AA)D(z_0(z_1))) \\right\\|_2\n$$\n\nWhen $$\\eta$$ is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution $$z_0$$. When the reverse SDE generates the exact same groundtruth sample, i.e., $$D(z_1(z_0)) = x_0$$, then the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the reverse SDE to recover the true underlying sample irrespective of the value of $$\\eta$$.\n\nWe draw the following key insights from our Theorem 3.8:\n\n- Curse of ambient dimension: In order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error needs to be computed in the $$d$$-dimensional ambient space. Therefore, DPS algorithm suffers from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space, and therefore avoids the curse of ambient dimension.\n\n- Large-scale foundation model: We propose a posterior sampling algorithm which offers the provision to use large-scale foundation models, and it provably solves general linear inverse problems.\n\n- Robustness to measurement step: The gluing objective makes our algorithm robust to the choice of step size $$\\eta$$. Furthermore, it allows the same (scalar) step size across all the coordinates of $$x_0$$.", "md": "For a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta} z_1 z_0, \\epsilon := \\theta z_1 z_0, \\epsilon$$, then the closed-form solution is\n\n$$\n\\theta^* = 1 - \\beta I_k\n$$\nwhich after normalization by $$1-\\beta$$ and composition with the decoder $$D z_0; \\omega = Sz_0$$ recovers the true subspace of $$p x_0$$.\n\nWith this optimal $$\\theta^*$$, we can now prove exact sample recovery using GML-DPS (6).\n\nTheorem 3.7 (Posterior Sampling using Goodness Modified Latent DPS). Let Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$, denote the singular values of $$(AS)^T (AS)$$, and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu \\tilde{z}_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\rightarrow \\epsilon \\right\\|_{\\theta \\rightarrow z_0, \\epsilon}\n$$\n\nGiven a partially known image $$x_0 \\sim p(x_0)$$, any fixed variance $$\\beta \\in (0, 1)$$, then with the (unique) step size $$\\eta = 1/2\\sigma_j, j = 1, 2, ..., r$$, the GML-DPS Algorithm (6) samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nTheorem 3.7 shows that GML-DPS (6) recovers the true sample using an LDM. This approach, however, requires the step size $$\\eta$$ to be chosen coordinate-wise in a specific manner. Also, multiple natural images could have the same measurements in the pixel space. This is a reasonable concern for LDMs due to one-to-many mappings of the decoder. Note that the goodness objective cannot help in this scenario because it assigns uniform probability to many of these latents $$z_1$$ for which $$\\left\\| \\nabla_{z_0(z_1)} - E(D(z_0(z_1))) \\right\\|_2 = 0$$. These challenges motivate the gluing objective in Theorem 3.8.\n\nTheorem 3.8 (Posterior Sampling using Diffusion in Latent Space). Let Assumptions 3.1 and 3.2 hold. Let $$\\sigma_j, \\forall j = 1, ..., r$$ denote the singular values of $$(AS)^T (AS)$$, and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E_{\\rightarrow \\rightarrow} \\left\\| \\mu \\tilde{z}_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\rightarrow \\epsilon \\right\\|_{\\theta \\rightarrow z_0, \\epsilon}\n$$\n\nGiven a partially known image $$x_0 \\sim p(x_0)$$, any fixed variance $$\\beta \\in (0, 1)$$, and any positive step sizes $$\\eta_j, j = 1, 2, ..., r$$, the PSLD Algorithm 2 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nThe important distinction between Theorem 3.7 and Theorem 3.8 is that the former requires the exact step size while the latter works for any finite step size. Combining denoising, measurement-consistency (with a scalar $$\\eta$$), and gluing updates, we have\n\n$$\n\\left\\| \\theta^* \\leftarrow \\tilde{\\theta} \\leftarrow \\theta \\right\\|_2 \\leftarrow \\left\\| z_0 = \\theta z_1 - \\eta \\nabla z_1 AD(z_0(z_1)) - y \\right\\|_2 - \\nabla z_1 \\left\\| z_0(z_1) - E(AAx_0 + (I_d - AA)D(z_0(z_1))) \\right\\|_2\n$$\n\nWhen $$\\eta$$ is chosen arbitrarily, then the third term guides the reverse SDE towards the optimal solution $$z_0$$. When the reverse SDE generates the exact same groundtruth sample, i.e., $$D(z_1(z_0)) = x_0$$, then the third term becomes zero. For all other samples, it penalizes the reverse SDE. Thus, it forces the reverse SDE to recover the true underlying sample irrespective of the value of $$\\eta$$.\n\nWe draw the following key insights from our Theorem 3.8:\n\n- Curse of ambient dimension: In order to run posterior sampling using diffusion in the pixel space, the gradient of the measurement error needs to be computed in the $$d$$-dimensional ambient space. Therefore, DPS algorithm suffers from the curse of ambient dimension. On the other hand, our algorithm uses diffusion in the latent space, and therefore avoids the curse of ambient dimension.\n\n- Large-scale foundation model: We propose a posterior sampling algorithm which offers the provision to use large-scale foundation models, and it provably solves general linear inverse problems.\n\n- Robustness to measurement step: The gluing objective makes our algorithm robust to the choice of step size $$\\eta$$. Furthermore, it allows the same (scalar) step size across all the coordinates of $$x_0$$."}]}, {"page": 9, "text": "Table 1: Quantitative inpainting results on FFHQ           256  validation set [25, 11]. We use Stable Diffusion\nv-1.5 and the measurement operators as in DPS [11]. As shown, our PSLD model outperforms DPS\nsince it is able to leverage the power of the Stable Diffusion foundation model.\n                       Inpaint (random)          Inpaint (box)             SR (4\u00d7)            Gaussian Deblur\n  Method             FID (\u2193)    LPIPS (\u2193)    FID (\u2193)   LPIPS (\u2193)     FID (\u2193)   LPIPS (\u2193)    FID (\u2193)    LPIPS (\u2193)\n  PSLD (Ours)        21.34      0.096        43.11     0.167         34.28     0.201        41.53      0.221\n  DPS [11]           33.48      0.212        35.14     0.216         39.35     0.214        44.05      0.257\n  DDRM [26]          69.71      0.587        42.93     0.204         62.15     0.294        74.92      0.332\n  MCG [13]           29.26      0.286        40.11     0.309         87.64     0.520        101.2      0.340\n  PnP-ADMM [6]       123.6      0.692        151.9     0.406         66.52     0.353        90.42      0.441\n  Score-SDE [47]     76.54      0.612        60.06     0.331         96.72     0.563        109.0      0.403\n  ADMM-TV            181.5      0.463        68.94     0.322         110.6     0.428        186.7      0.507\n4     Experimental Evaluation\nWe experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct\nour experiments on a subset of the FFHQ dataset [25] (downscaled to                256  \u00d7  256 3, denoted by FFHQ\n256).   For out-of-distribution, we use images from the web and ImageNet dataset [17] (resized to\n256  \u00d7  256, denoted by ImageNet 256). To make a fair comparison, we use the same validation subset\nand follow the same masking strategy as the baseline DPS [11]. It is important to note that our main\ncontribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with\ntwo pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple\nsubsets of the LAION dataset [41, 42]; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on\nthe FFHQ     256  dataset [39]. The DPS model is similarly trained from scratch for 1M steps using 49k\nFFHQ     256  images, which excludes the first 1K images used as validation set.\n    Inverse Problems.         We experiment with the following task-specific measurement operators from\nthe baseline DPS [11]: (i) Box inpainting uses a mask of size 128\u00d7128 at the center. (ii) Random\ninpainting chooses a drop probability uniformly at random between                 (0.2,0.8)   and applies this drop\nprobability to all the pixels. (iii) Super-resolution downsamples images at           4\u00d7  scale. (iv) Gaussian blur\nconvolves images with a Gaussian blur kernel. (v) Motion blur convolves images with a motion blur\nkernel. We also experiment with these additional operators from RePaint [31]: (vi) Super-resolution\ndownsamples images at         2\u00d7,  3\u00d7, and    4\u00d7  scale.  (vii) Denoising has Gaussian noise with           \u03c3 = 0.05.\n(viii) Destriping has vertical and horizontal stripes in the input images.\n    Evaluation.        We compare the performance of our\nPSLD algorithm with the state-of-the-art DPS algorithm                Table 2: Quantitative super-resolution (us-\n[11] on random inpainting, box inpainting, denoising, Gaus-           ing measurement operator from [31]) re-\nsian deblur, motion deblur, arbitrary masking, and super-             sults on FFHQ     256  validation samples [25,\nresolution tasks. We show that PSLD outperforms DPS,                  11]. We use PSLD with Stable Diffusion.\nboth in-distribution and out-of-distribution datasets, using          Table shows LPIPS (\u2193).\nthe Stable Diffusion v-1.5 model pre-trained on the LAION\ndataset. We also test PSLD with LDM-VQ-4 trained on                       Method      PSLD (Ours)        DPS [11]\nFFHQ     256, to compare with DPS trained on the same                     2\u00d7          0.185              0.220\ndata distribution. Note that the LDM-v4 is a latent-based                 3\u00d7          0.220              0.247\nmodel released prior to Stable Diffusion. Therefore, it does              4\u00d7          0.233              0.291\nnot match the performance of Stable Diffusion in solving\ninverse problems. However, it shows the general applicabil-\n   3https://www.kaggle.com/datasets/denislukovnikov/ffhq256-images-only\n                                                           9", "md": "# Quantitative Inpainting Results\n\n## Table 1: Quantitative Inpainting Results on FFHQ 256 Validation Set\n\nWe use Stable Diffusion v-1.5 and the measurement operators as in DPS [11]. As shown, our PSLD model outperforms DPS since it is able to leverage the power of the Stable Diffusion foundation model.\n\n|Method|Inpaint (random) FID (\u2193)|Inpaint (random) LPIPS (\u2193)|Inpaint (box) FID (\u2193)|Inpaint (box) LPIPS (\u2193)|SR (4\u00d7) FID (\u2193)|SR (4\u00d7) LPIPS (\u2193)|Gaussian Deblur FID (\u2193)|Gaussian Deblur LPIPS (\u2193)|\n|---|---|---|---|---|---|---|---|---|\n|PSLD (Ours)|21.34|0.096|43.11|0.167|34.28|0.201|41.53|0.221|\n|DPS [11]|33.48|0.212|35.14|0.216|39.35|0.214|44.05|0.257|\n|DDRM [26]|69.71|0.587|42.93|0.204|62.15|0.294|74.92|0.332|\n|MCG [13]|29.26|0.286|40.11|0.309|87.64|0.520|101.2|0.340|\n|PnP-ADMM [6]|123.6|0.692|151.9|0.406|66.52|0.353|90.42|0.441|\n|Score-SDE [47]|76.54|0.612|60.06|0.331|96.72|0.563|109.0|0.403|\n|ADMM-TV|181.5|0.463|68.94|0.322|110.6|0.428|186.7|0.507|\n\n## Experimental Evaluation\n\nWe experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct our experiments on a subset of the FFHQ dataset [25] (downscaled to 256 x 256, denoted by FFHQ 256). For out-of-distribution, we use images from the web and ImageNet dataset [17] (resized to 256 x 256, denoted by ImageNet 256). To make a fair comparison, we use the same validation subset and follow the same masking strategy as the baseline DPS [11]. It is important to note that our main contribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple subsets of the LAION dataset [41, 42]; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on the FFHQ 256 dataset [39]. The DPS model is similarly trained from scratch for 1M steps using 49k FFHQ 256 images, which excludes the first 1K images used as validation set.\n\n### Inverse Problems\n\nWe experiment with the following task-specific measurement operators from the baseline DPS [11]:\n\n1. Box inpainting uses a mask of size 128x128 at the center.\n2. Random inpainting chooses a drop probability uniformly at random between (0.2, 0.8) and applies this drop probability to all the pixels.\n3. Super-resolution downsamples images at 4x scale.\n4. Gaussian blur convolves images with a Gaussian blur kernel.\n5. Motion blur convolves images with a motion blur kernel.\n\nWe also experiment with these additional operators from RePaint [31]:\n\n1. Super-resolution downsamples images at 2x, 3x, and 4x scale.\n2. Denoising has Gaussian noise with \u03c3 = 0.05.\n3. Destriping has vertical and horizontal stripes in the input images.\n\n### Evaluation\n\nWe compare the performance of our PSLD algorithm with the state-of-the-art DPS algorithm on random inpainting, box inpainting, denoising, Gaussian deblur, motion deblur, arbitrary masking, and super-resolution tasks. We show that PSLD outperforms DPS, both in-distribution and out-of-distribution datasets, using the Stable Diffusion v-1.5 model pre-trained on the LAION dataset. We also test PSLD with LDM-VQ-4 trained on FFHQ 256, to compare with DPS trained on the same data distribution. Note that the LDM-v4 is a latent-based model released prior to Stable Diffusion. Therefore, it does not match the performance of Stable Diffusion in solving inverse problems. However, it shows the general applicability.\n\n## Table 2: Quantitative Super-Resolution Results on FFHQ 256 Validation Samples\n\nWe use PSLD with Stable Diffusion. Table shows LPIPS (\u2193).\n\n|Method|PSLD (Ours)|DPS [11]|\n|---|---|---|\n|2x|0.185|0.220|\n|3x|0.220|0.247|\n|4x|0.233|0.291|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Quantitative Inpainting Results", "md": "# Quantitative Inpainting Results"}, {"type": "heading", "lvl": 2, "value": "Table 1: Quantitative Inpainting Results on FFHQ 256 Validation Set", "md": "## Table 1: Quantitative Inpainting Results on FFHQ 256 Validation Set"}, {"type": "text", "value": "We use Stable Diffusion v-1.5 and the measurement operators as in DPS [11]. As shown, our PSLD model outperforms DPS since it is able to leverage the power of the Stable Diffusion foundation model.", "md": "We use Stable Diffusion v-1.5 and the measurement operators as in DPS [11]. As shown, our PSLD model outperforms DPS since it is able to leverage the power of the Stable Diffusion foundation model."}, {"type": "table", "rows": [["Method", "Inpaint (random) FID (\u2193)", "Inpaint (random) LPIPS (\u2193)", "Inpaint (box) FID (\u2193)", "Inpaint (box) LPIPS (\u2193)", "SR (4\u00d7) FID (\u2193)", "SR (4\u00d7) LPIPS (\u2193)", "Gaussian Deblur FID (\u2193)", "Gaussian Deblur LPIPS (\u2193)"], ["PSLD (Ours)", "21.34", "0.096", "43.11", "0.167", "34.28", "0.201", "41.53", "0.221"], ["DPS [11]", "33.48", "0.212", "35.14", "0.216", "39.35", "0.214", "44.05", "0.257"], ["DDRM [26]", "69.71", "0.587", "42.93", "0.204", "62.15", "0.294", "74.92", "0.332"], ["MCG [13]", "29.26", "0.286", "40.11", "0.309", "87.64", "0.520", "101.2", "0.340"], ["PnP-ADMM [6]", "123.6", "0.692", "151.9", "0.406", "66.52", "0.353", "90.42", "0.441"], ["Score-SDE [47]", "76.54", "0.612", "60.06", "0.331", "96.72", "0.563", "109.0", "0.403"], ["ADMM-TV", "181.5", "0.463", "68.94", "0.322", "110.6", "0.428", "186.7", "0.507"]], "md": "|Method|Inpaint (random) FID (\u2193)|Inpaint (random) LPIPS (\u2193)|Inpaint (box) FID (\u2193)|Inpaint (box) LPIPS (\u2193)|SR (4\u00d7) FID (\u2193)|SR (4\u00d7) LPIPS (\u2193)|Gaussian Deblur FID (\u2193)|Gaussian Deblur LPIPS (\u2193)|\n|---|---|---|---|---|---|---|---|---|\n|PSLD (Ours)|21.34|0.096|43.11|0.167|34.28|0.201|41.53|0.221|\n|DPS [11]|33.48|0.212|35.14|0.216|39.35|0.214|44.05|0.257|\n|DDRM [26]|69.71|0.587|42.93|0.204|62.15|0.294|74.92|0.332|\n|MCG [13]|29.26|0.286|40.11|0.309|87.64|0.520|101.2|0.340|\n|PnP-ADMM [6]|123.6|0.692|151.9|0.406|66.52|0.353|90.42|0.441|\n|Score-SDE [47]|76.54|0.612|60.06|0.331|96.72|0.563|109.0|0.403|\n|ADMM-TV|181.5|0.463|68.94|0.322|110.6|0.428|186.7|0.507|", "isPerfectTable": true, "csv": "\"Method\",\"Inpaint (random) FID (\u2193)\",\"Inpaint (random) LPIPS (\u2193)\",\"Inpaint (box) FID (\u2193)\",\"Inpaint (box) LPIPS (\u2193)\",\"SR (4\u00d7) FID (\u2193)\",\"SR (4\u00d7) LPIPS (\u2193)\",\"Gaussian Deblur FID (\u2193)\",\"Gaussian Deblur LPIPS (\u2193)\"\n\"PSLD (Ours)\",\"21.34\",\"0.096\",\"43.11\",\"0.167\",\"34.28\",\"0.201\",\"41.53\",\"0.221\"\n\"DPS [11]\",\"33.48\",\"0.212\",\"35.14\",\"0.216\",\"39.35\",\"0.214\",\"44.05\",\"0.257\"\n\"DDRM [26]\",\"69.71\",\"0.587\",\"42.93\",\"0.204\",\"62.15\",\"0.294\",\"74.92\",\"0.332\"\n\"MCG [13]\",\"29.26\",\"0.286\",\"40.11\",\"0.309\",\"87.64\",\"0.520\",\"101.2\",\"0.340\"\n\"PnP-ADMM [6]\",\"123.6\",\"0.692\",\"151.9\",\"0.406\",\"66.52\",\"0.353\",\"90.42\",\"0.441\"\n\"Score-SDE [47]\",\"76.54\",\"0.612\",\"60.06\",\"0.331\",\"96.72\",\"0.563\",\"109.0\",\"0.403\"\n\"ADMM-TV\",\"181.5\",\"0.463\",\"68.94\",\"0.322\",\"110.6\",\"0.428\",\"186.7\",\"0.507\""}, {"type": "heading", "lvl": 2, "value": "Experimental Evaluation", "md": "## Experimental Evaluation"}, {"type": "text", "value": "We experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct our experiments on a subset of the FFHQ dataset [25] (downscaled to 256 x 256, denoted by FFHQ 256). For out-of-distribution, we use images from the web and ImageNet dataset [17] (resized to 256 x 256, denoted by ImageNet 256). To make a fair comparison, we use the same validation subset and follow the same masking strategy as the baseline DPS [11]. It is important to note that our main contribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple subsets of the LAION dataset [41, 42]; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on the FFHQ 256 dataset [39]. The DPS model is similarly trained from scratch for 1M steps using 49k FFHQ 256 images, which excludes the first 1K images used as validation set.", "md": "We experiment with in-distribution and out-of-distribution datasets. For in-distribution, we conduct our experiments on a subset of the FFHQ dataset [25] (downscaled to 256 x 256, denoted by FFHQ 256). For out-of-distribution, we use images from the web and ImageNet dataset [17] (resized to 256 x 256, denoted by ImageNet 256). To make a fair comparison, we use the same validation subset and follow the same masking strategy as the baseline DPS [11]. It is important to note that our main contribution is an algorithm that can leverage any latent diffusion model. We test our algorithm with two pre-trained latent diffusion models: (i) the Stable Diffusion model that is trained on multiple subsets of the LAION dataset [41, 42]; and (ii) the Latent Diffusion model (LDM-VQ-4) trained on the FFHQ 256 dataset [39]. The DPS model is similarly trained from scratch for 1M steps using 49k FFHQ 256 images, which excludes the first 1K images used as validation set."}, {"type": "heading", "lvl": 3, "value": "Inverse Problems", "md": "### Inverse Problems"}, {"type": "text", "value": "We experiment with the following task-specific measurement operators from the baseline DPS [11]:\n\n1. Box inpainting uses a mask of size 128x128 at the center.\n2. Random inpainting chooses a drop probability uniformly at random between (0.2, 0.8) and applies this drop probability to all the pixels.\n3. Super-resolution downsamples images at 4x scale.\n4. Gaussian blur convolves images with a Gaussian blur kernel.\n5. Motion blur convolves images with a motion blur kernel.\n\nWe also experiment with these additional operators from RePaint [31]:\n\n1. Super-resolution downsamples images at 2x, 3x, and 4x scale.\n2. Denoising has Gaussian noise with \u03c3 = 0.05.\n3. Destriping has vertical and horizontal stripes in the input images.", "md": "We experiment with the following task-specific measurement operators from the baseline DPS [11]:\n\n1. Box inpainting uses a mask of size 128x128 at the center.\n2. Random inpainting chooses a drop probability uniformly at random between (0.2, 0.8) and applies this drop probability to all the pixels.\n3. Super-resolution downsamples images at 4x scale.\n4. Gaussian blur convolves images with a Gaussian blur kernel.\n5. Motion blur convolves images with a motion blur kernel.\n\nWe also experiment with these additional operators from RePaint [31]:\n\n1. Super-resolution downsamples images at 2x, 3x, and 4x scale.\n2. Denoising has Gaussian noise with \u03c3 = 0.05.\n3. Destriping has vertical and horizontal stripes in the input images."}, {"type": "heading", "lvl": 3, "value": "Evaluation", "md": "### Evaluation"}, {"type": "text", "value": "We compare the performance of our PSLD algorithm with the state-of-the-art DPS algorithm on random inpainting, box inpainting, denoising, Gaussian deblur, motion deblur, arbitrary masking, and super-resolution tasks. We show that PSLD outperforms DPS, both in-distribution and out-of-distribution datasets, using the Stable Diffusion v-1.5 model pre-trained on the LAION dataset. We also test PSLD with LDM-VQ-4 trained on FFHQ 256, to compare with DPS trained on the same data distribution. Note that the LDM-v4 is a latent-based model released prior to Stable Diffusion. Therefore, it does not match the performance of Stable Diffusion in solving inverse problems. However, it shows the general applicability.", "md": "We compare the performance of our PSLD algorithm with the state-of-the-art DPS algorithm on random inpainting, box inpainting, denoising, Gaussian deblur, motion deblur, arbitrary masking, and super-resolution tasks. We show that PSLD outperforms DPS, both in-distribution and out-of-distribution datasets, using the Stable Diffusion v-1.5 model pre-trained on the LAION dataset. We also test PSLD with LDM-VQ-4 trained on FFHQ 256, to compare with DPS trained on the same data distribution. Note that the LDM-v4 is a latent-based model released prior to Stable Diffusion. Therefore, it does not match the performance of Stable Diffusion in solving inverse problems. However, it shows the general applicability."}, {"type": "heading", "lvl": 2, "value": "Table 2: Quantitative Super-Resolution Results on FFHQ 256 Validation Samples", "md": "## Table 2: Quantitative Super-Resolution Results on FFHQ 256 Validation Samples"}, {"type": "text", "value": "We use PSLD with Stable Diffusion. Table shows LPIPS (\u2193).", "md": "We use PSLD with Stable Diffusion. Table shows LPIPS (\u2193)."}, {"type": "table", "rows": [["Method", "PSLD (Ours)", "DPS [11]"], ["2x", "0.185", "0.220"], ["3x", "0.220", "0.247"], ["4x", "0.233", "0.291"]], "md": "|Method|PSLD (Ours)|DPS [11]|\n|---|---|---|\n|2x|0.185|0.220|\n|3x|0.220|0.247|\n|4x|0.233|0.291|", "isPerfectTable": true, "csv": "\"Method\",\"PSLD (Ours)\",\"DPS [11]\"\n\"2x\",\"0.185\",\"0.220\"\n\"3x\",\"0.220\",\"0.247\"\n\"4x\",\"0.233\",\"0.291\""}]}, {"page": 10, "text": "        (a) Input            (b) Groundtruth         (c) Comm. Serv. 1        (d) Comm. Serv. 2           (e) PSLD (Ours)\nFigure 2: Inpainting results in general domain images from the web (see Appendix B for image sources).\nOur model compared to state-of-art commercial inpainting services that leverage the same foundation\nmodel (Stable Diffusion v-1.5).\nity of our framework to leverage an LDM in posterior sampling. Since Stable Diffusion v-1.5 is trained\nwith an image resolution of          512   \u00d7 512, we apply the forward operator after upsampling inputs to\n512  \u00d7512, run posterior sampling at           512 \u00d7  512, and then downsample images to the original                256  \u00d7  256\nresolution for a fair comparison with DPS. We observed a similar performance while applying the\nmasking operator at       256  \u00d7256    and upscaling to     512   \u00d7512    before running PSLD. More implementation\ndetails are provided in Appendix B.1.\n    Metrics.         We use the commonly used Learned Perceptual Image Patch Similarity (LPIPS),\nPeak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM), and Fr\u00e9chet Inception\nDistance   4 (FID) metrics for quantitative evaluation.\n    Results.      Figure 2 shows the inpainting results on out-of-distribution samples. This experiment\nwas performed on commercial platforms that use (to the best of our knowledge) Stable diffusion and\nadditional proprietary models. This evaluation was performed on models deployed in May 2023 and\nmay change as commercial providers improve their platforms.\n    The qualitative advantage of PSLD is clearly demonstrated in Figures 2, 3, 4, 15 and 16.                                  In\nFigure 5, we compare PSLD and DPS in random inpainting task for varying percentage of dropped\npixels. Quantitatively, PSLD outperforms DPS in commonly used metrics: LPIPS, PSNR, and SSIM.\n    In our PSLD algorithm, we use Stable Diffusion v1.5 model and (zero-shot) test it on inverse\nproblems. Table 1 compares the quantitative results of PSLD with related works on random inpainting,\nbox inpainting, super-resolution, and Gaussian deblur tasks. PSLD significantly outperforms previous\napproaches on the relatively easier random inpainting task, and it is better or comparable on harder tasks.\n   4https://github.com/mseitzer/pytorch-fid                    10", "md": "# Inpainting Results\n\n## Results\n\nFigure 2 shows the inpainting results on out-of-distribution samples. This experiment was performed on commercial platforms that use (to the best of our knowledge) Stable diffusion and additional proprietary models. This evaluation was performed on models deployed in May 2023 and may change as commercial providers improve their platforms.\n\n### Qualitative Advantage of PSLD\n\nThe qualitative advantage of PSLD is clearly demonstrated in Figures 2, 3, 4, 15 and 16.\n\n### Quantitative Comparison\n\nIn Figure 5, we compare PSLD and DPS in random inpainting task for varying percentage of dropped pixels. Quantitatively, PSLD outperforms DPS in commonly used metrics: LPIPS, PSNR, and SSIM.\n\n### PSLD Algorithm\n\nIn our PSLD algorithm, we use Stable Diffusion v1.5 model and (zero-shot) test it on inverse problems. Table 1 compares the quantitative results of PSLD with related works on random inpainting, box inpainting, super-resolution, and Gaussian deblur tasks. PSLD significantly outperforms previous approaches on the relatively easier random inpainting task, and it is better or comparable on harder tasks.\n\n### Metrics\n\nWe use the commonly used Learned Perceptual Image Patch Similarity (LPIPS), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM), and Fr\u00e9chet Inception Distance (FID) metrics for quantitative evaluation.\n\n### Implementation Details\n\nSince Stable Diffusion v-1.5 is trained with an image resolution of 512 x 512, we apply the forward operator after upsampling inputs to 512 x 512, run posterior sampling at 512 x 512, and then downsample images to the original 256 x 256 resolution for a fair comparison with DPS. We observed a similar performance while applying the masking operator at 256 x 256 and upscaling to 512 x 512 before running PSLD. More implementation details are provided in Appendix B.1.\n\n### Reference\n\n4. https://github.com/mseitzer/pytorch-fid", "images": [{"name": "img_p9_1", "height": 276, "width": 275}, {"name": "img_p9_2", "height": 276, "width": 275}, {"name": "img_p9_3", "height": 275, "width": 275}, {"name": "img_p9_4", "height": 275, "width": 275}, {"name": "img_p9_5", "height": 275, "width": 275}, {"name": "img_p9_6", "height": 275, "width": 275}, {"name": "img_p9_7", "height": 275, "width": 275}, {"name": "img_p9_8", "height": 275, "width": 275}, {"name": "img_p9_9", "height": 276, "width": 275}, {"name": "img_p9_10", "height": 275, "width": 275}, {"name": "img_p9_11", "height": 275, "width": 275}, {"name": "img_p9_12", "height": 275, "width": 275}, {"name": "img_p9_13", "height": 275, "width": 275}, {"name": "img_p9_14", "height": 276, "width": 275}, {"name": "img_p9_15", "height": 275, "width": 275}], "items": [{"type": "heading", "lvl": 1, "value": "Inpainting Results", "md": "# Inpainting Results"}, {"type": "heading", "lvl": 2, "value": "Results", "md": "## Results"}, {"type": "text", "value": "Figure 2 shows the inpainting results on out-of-distribution samples. This experiment was performed on commercial platforms that use (to the best of our knowledge) Stable diffusion and additional proprietary models. This evaluation was performed on models deployed in May 2023 and may change as commercial providers improve their platforms.", "md": "Figure 2 shows the inpainting results on out-of-distribution samples. This experiment was performed on commercial platforms that use (to the best of our knowledge) Stable diffusion and additional proprietary models. This evaluation was performed on models deployed in May 2023 and may change as commercial providers improve their platforms."}, {"type": "heading", "lvl": 3, "value": "Qualitative Advantage of PSLD", "md": "### Qualitative Advantage of PSLD"}, {"type": "text", "value": "The qualitative advantage of PSLD is clearly demonstrated in Figures 2, 3, 4, 15 and 16.", "md": "The qualitative advantage of PSLD is clearly demonstrated in Figures 2, 3, 4, 15 and 16."}, {"type": "heading", "lvl": 3, "value": "Quantitative Comparison", "md": "### Quantitative Comparison"}, {"type": "text", "value": "In Figure 5, we compare PSLD and DPS in random inpainting task for varying percentage of dropped pixels. Quantitatively, PSLD outperforms DPS in commonly used metrics: LPIPS, PSNR, and SSIM.", "md": "In Figure 5, we compare PSLD and DPS in random inpainting task for varying percentage of dropped pixels. Quantitatively, PSLD outperforms DPS in commonly used metrics: LPIPS, PSNR, and SSIM."}, {"type": "heading", "lvl": 3, "value": "PSLD Algorithm", "md": "### PSLD Algorithm"}, {"type": "text", "value": "In our PSLD algorithm, we use Stable Diffusion v1.5 model and (zero-shot) test it on inverse problems. Table 1 compares the quantitative results of PSLD with related works on random inpainting, box inpainting, super-resolution, and Gaussian deblur tasks. PSLD significantly outperforms previous approaches on the relatively easier random inpainting task, and it is better or comparable on harder tasks.", "md": "In our PSLD algorithm, we use Stable Diffusion v1.5 model and (zero-shot) test it on inverse problems. Table 1 compares the quantitative results of PSLD with related works on random inpainting, box inpainting, super-resolution, and Gaussian deblur tasks. PSLD significantly outperforms previous approaches on the relatively easier random inpainting task, and it is better or comparable on harder tasks."}, {"type": "heading", "lvl": 3, "value": "Metrics", "md": "### Metrics"}, {"type": "text", "value": "We use the commonly used Learned Perceptual Image Patch Similarity (LPIPS), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM), and Fr\u00e9chet Inception Distance (FID) metrics for quantitative evaluation.", "md": "We use the commonly used Learned Perceptual Image Patch Similarity (LPIPS), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM), and Fr\u00e9chet Inception Distance (FID) metrics for quantitative evaluation."}, {"type": "heading", "lvl": 3, "value": "Implementation Details", "md": "### Implementation Details"}, {"type": "text", "value": "Since Stable Diffusion v-1.5 is trained with an image resolution of 512 x 512, we apply the forward operator after upsampling inputs to 512 x 512, run posterior sampling at 512 x 512, and then downsample images to the original 256 x 256 resolution for a fair comparison with DPS. We observed a similar performance while applying the masking operator at 256 x 256 and upscaling to 512 x 512 before running PSLD. More implementation details are provided in Appendix B.1.", "md": "Since Stable Diffusion v-1.5 is trained with an image resolution of 512 x 512, we apply the forward operator after upsampling inputs to 512 x 512, run posterior sampling at 512 x 512, and then downsample images to the original 256 x 256 resolution for a fair comparison with DPS. We observed a similar performance while applying the masking operator at 256 x 256 and upscaling to 512 x 512 before running PSLD. More implementation details are provided in Appendix B.1."}, {"type": "heading", "lvl": 3, "value": "Reference", "md": "### Reference"}, {"type": "text", "value": "4. https://github.com/mseitzer/pytorch-fid", "md": "4. https://github.com/mseitzer/pytorch-fid"}]}, {"page": 11, "text": "Table 3: Quantitative inpainting results on FFHQ            256  validation set [25, 11]. We use the      latent diffusion\n(LDM-VQ-4) trained on FFHQ            256. Note that in this experiment PSLD and DPS use diffusion models\ntrained on the same dataset. As shown, PSLD with LDM-VQ-4 as diffusion model outperforms DPS\nin box inpainting and has comparable performance in random inpainting.\n                                        Inpaint (random)                            Inpaint (box)\n            Method            PSNR (\u2191)      SSIM (\u2191)      LPIPS (\u2193)     PSNR (\u2191)      SSIM (\u2191)      LPIPS (\u2193)\n            PSLD (Ours)       30.31         0.851         0.221         24.22         0.819         0.158\n            DPS [11]          29.49         0.844         0.212         23.39         0.798         0.214\nTable 4: Quantitative results of random inpainting and denoising on FFHQ                      256  [25, 11] using Stable\nDiffusion v-1.5. Note that DPS is trained on FFHQ                 256. The results show that our method PSLD\ngeneralizes well to out-of-distribution samples even without finetuning.\n                             Random inpaint + denoise       \u03c3 = 0.00    Random inpaint + denoise       \u03c3 = 0.05\n           Method            PSNR (\u2191)      SSIM (\u2191)      LPIPS (\u2193)      PSNR (\u2191)      SSIM (\u2191)      LPIPS (\u2193)\n           PSLD (Ours)       34.02          0.951        0.083          33.71         0.943         0.096\n           DPS [11]          31.41         0.884         0.171          29.49         0.844         0.212\n                                                                                            Ce              Ce\n       Input       (b) Groundtruth (c) DPS [9]   (d) PSLD (Ours)      Input       (b) Groundtruth (c) DPS [9]   (d) PSLD (Ours)\nFigure 3:   Left panel:    Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion\nv-1.5.  Notice the text in the top row and the facial expression in the bottom row.                Right panel:      Block\n(128 \u00d7  128) inpainting, using the LDM-VQ-4 model trained on FFHQ            256  [25]. Notice the glasses in the top row\nand eyes in the bottom row.\nTable 4 draws a comparison between PSLD and the strongest baseline (among the compared methods)\non out-of-distribution images. Table 2 shows the super-resolution results using nearest-neighbor kernels\nfrom [31] on FFHQ 256 validation dataset. Observe that PSLD outperforms state-of-the-art methods\nacross diverse tasks and standard evaluation metrics.\n    In Table 3, we compare PSLD (using LDM-VQ-4) and DPS on random and box inpainting tasks with\nthe same operating resolution (256\u00d7          256) and training distributions (FFHQ 256). Although the LDM\nmodel exceeds DPS performance in box inpainting, it is comparable in random inpainting. As expected,\nusing a more powerful pre-trained model such as Stable Diffusion is beneficial in reconstruction\u2013see\nTable 1. This highlights the significance of our PSLD algorithm that has the provision to incorporate\na powerful foundation model with no extra training costs for solving inverse problems. Importantly,\nPSLD uses latent-based diffusion, and thus it avoids the curse of ambient dimension (Theorem 3.8),\nwhile still achieving comparable results to the state-of-the-art method DPS [11] that has been trained\non the same dataset. Additional experimental evaluation is provided in Appendix B.\n                                                            11", "md": "|Method|Inpaint (random)|Inpaint (box)|\n|---|---|---|\n| |PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|PSLD (Ours)|30.31|0.851|0.221|24.22|0.819|0.158|\n|DPS [11]|29.49|0.844|0.212|23.39|0.798|0.214|\n\n|Method|Random inpaint + denoise \u03c3 = 0.00|Random inpaint + denoise \u03c3 = 0.05|\n|---|---|---|\n| |PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|PSLD (Ours)|34.02|0.951|0.083|33.71|0.943|0.096|\n|DPS [11]|31.41|0.884|0.171|29.49|0.844|0.212|\n\nLeft panel: Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion v-1.5. Notice the text in the top row and the facial expression in the bottom row.\n\nRight panel: Block (128 \u00d7 128) inpainting, using the LDM-VQ-4 model trained on FFHQ 256 [25]. Notice the glasses in the top row and eyes in the bottom row.\n\n**Table 4: Comparison between PSLD and the strongest baseline on out-of-distribution images**\n\nTable 2 shows the super-resolution results using nearest-neighbor kernels from [31] on FFHQ 256 validation dataset. PSLD outperforms state-of-the-art methods across diverse tasks and standard evaluation metrics.\n\nIn Table 3, PSLD (using LDM-VQ-4) and DPS are compared on random and box inpainting tasks with the same operating resolution (256\u00d7 256) and training distributions (FFHQ 256). The LDM model exceeds DPS performance in box inpainting but is comparable in random inpainting. Using a more powerful pre-trained model such as Stable Diffusion is beneficial in reconstruction (see Table 1). This highlights the significance of our PSLD algorithm that can incorporate a powerful foundation model with no extra training costs for solving inverse problems. PSLD uses latent-based diffusion, avoiding the curse of ambient dimension (Theorem 3.8), while achieving comparable results to the state-of-the-art method DPS [11] trained on the same dataset. Additional experimental evaluation is provided in Appendix B.", "images": [{"name": "img_p10_1", "height": 390, "width": 1462}], "items": [{"type": "table", "rows": [["Method", "Inpaint (random)", "Inpaint (box)"], ["", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)"], ["PSLD (Ours)", "30.31", "0.851", "0.221", "24.22", "0.819", "0.158"], ["DPS [11]", "29.49", "0.844", "0.212", "23.39", "0.798", "0.214"]], "md": "|Method|Inpaint (random)|Inpaint (box)|\n|---|---|---|\n| |PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|PSLD (Ours)|30.31|0.851|0.221|24.22|0.819|0.158|\n|DPS [11]|29.49|0.844|0.212|23.39|0.798|0.214|", "isPerfectTable": false, "csv": "\"Method\",\"Inpaint (random)\",\"Inpaint (box)\"\n\"\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\"\n\"PSLD (Ours)\",\"30.31\",\"0.851\",\"0.221\",\"24.22\",\"0.819\",\"0.158\"\n\"DPS [11]\",\"29.49\",\"0.844\",\"0.212\",\"23.39\",\"0.798\",\"0.214\""}, {"type": "table", "rows": [["Method", "Random inpaint + denoise \u03c3 = 0.00", "Random inpaint + denoise \u03c3 = 0.05"], ["", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)", "PSNR (\u2191)", "SSIM (\u2191)", "LPIPS (\u2193)"], ["PSLD (Ours)", "34.02", "0.951", "0.083", "33.71", "0.943", "0.096"], ["DPS [11]", "31.41", "0.884", "0.171", "29.49", "0.844", "0.212"]], "md": "|Method|Random inpaint + denoise \u03c3 = 0.00|Random inpaint + denoise \u03c3 = 0.05|\n|---|---|---|\n| |PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|PSNR (\u2191)|SSIM (\u2191)|LPIPS (\u2193)|\n|PSLD (Ours)|34.02|0.951|0.083|33.71|0.943|0.096|\n|DPS [11]|31.41|0.884|0.171|29.49|0.844|0.212|", "isPerfectTable": false, "csv": "\"Method\",\"Random inpaint + denoise \u03c3 = 0.00\",\"Random inpaint + denoise \u03c3 = 0.05\"\n\"\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\",\"PSNR (\u2191)\",\"SSIM (\u2191)\",\"LPIPS (\u2193)\"\n\"PSLD (Ours)\",\"34.02\",\"0.951\",\"0.083\",\"33.71\",\"0.943\",\"0.096\"\n\"DPS [11]\",\"31.41\",\"0.884\",\"0.171\",\"29.49\",\"0.844\",\"0.212\""}, {"type": "text", "value": "Left panel: Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion v-1.5. Notice the text in the top row and the facial expression in the bottom row.\n\nRight panel: Block (128 \u00d7 128) inpainting, using the LDM-VQ-4 model trained on FFHQ 256 [25]. Notice the glasses in the top row and eyes in the bottom row.\n\n**Table 4: Comparison between PSLD and the strongest baseline on out-of-distribution images**\n\nTable 2 shows the super-resolution results using nearest-neighbor kernels from [31] on FFHQ 256 validation dataset. PSLD outperforms state-of-the-art methods across diverse tasks and standard evaluation metrics.\n\nIn Table 3, PSLD (using LDM-VQ-4) and DPS are compared on random and box inpainting tasks with the same operating resolution (256\u00d7 256) and training distributions (FFHQ 256). The LDM model exceeds DPS performance in box inpainting but is comparable in random inpainting. Using a more powerful pre-trained model such as Stable Diffusion is beneficial in reconstruction (see Table 1). This highlights the significance of our PSLD algorithm that can incorporate a powerful foundation model with no extra training costs for solving inverse problems. PSLD uses latent-based diffusion, avoiding the curse of ambient dimension (Theorem 3.8), while achieving comparable results to the state-of-the-art method DPS [11] trained on the same dataset. Additional experimental evaluation is provided in Appendix B.", "md": "Left panel: Random Inpainting on images from FFHQ 256 [25] using PSLD with Stable Diffusion v-1.5. Notice the text in the top row and the facial expression in the bottom row.\n\nRight panel: Block (128 \u00d7 128) inpainting, using the LDM-VQ-4 model trained on FFHQ 256 [25]. Notice the glasses in the top row and eyes in the bottom row.\n\n**Table 4: Comparison between PSLD and the strongest baseline on out-of-distribution images**\n\nTable 2 shows the super-resolution results using nearest-neighbor kernels from [31] on FFHQ 256 validation dataset. PSLD outperforms state-of-the-art methods across diverse tasks and standard evaluation metrics.\n\nIn Table 3, PSLD (using LDM-VQ-4) and DPS are compared on random and box inpainting tasks with the same operating resolution (256\u00d7 256) and training distributions (FFHQ 256). The LDM model exceeds DPS performance in box inpainting but is comparable in random inpainting. Using a more powerful pre-trained model such as Stable Diffusion is beneficial in reconstruction (see Table 1). This highlights the significance of our PSLD algorithm that can incorporate a powerful foundation model with no extra training costs for solving inverse problems. PSLD uses latent-based diffusion, avoiding the curse of ambient dimension (Theorem 3.8), while achieving comparable results to the state-of-the-art method DPS [11] trained on the same dataset. Additional experimental evaluation is provided in Appendix B."}]}, {"page": 12, "text": "           (a) Input                  (b) Groundtruth                    (c) DPS [11]                 (d) PSLD (Ours)\nFigure 4: Inpainting (random and box) results on out-of-distribution samples,                   256\u00d7256     (see Appendix B\nfor image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model.\n    0.30         DPS                           32                                        0.90\n    0.25        PSLD\n    0.20                                    2  30                                     1  0.85\n    0.15                                       28         DPS                            0.80         DPS\n    0.10                                       26        PSLD                            0.75         PSLD\n          20        40       60        80         20        40         60        80           20        40       60        80\n          Percentage of dropped pixels             Percentage of dropped pixels                Percentage of dropped pixels\nFigure 5: Comparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as the\npercentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.\n5     Conclusion\nIn this paper, we leverage latent diffusion models to solve general linear inverse problems.                             While\npreviously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to\nuse the image prior learned by latent-based foundation generative models. We provide a principled\nanalysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to\ndesign a modified objective (goodness and gluing). This leads to our algorithm \u2013 Posterior Sampling\nwith Latent Diffusion (PSLD) \u2013 that experimentally outperforms state-of-art baselines on a wide variety\nof tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.\nLimitations.      Our evaluation is based on Stable Diffusion which was trained on the LAION dataset.\nBiases in this dataset and foundation model will be implicitly affecting our algorithm. Our method\ncan work with any LDM and we expect new foundation models trained on better datasets like [19]\nto mitigate these issues. Second, we have not explored how to use latent-based foundation models to\nsolve non-linear inverse problems. Our method builds on the DPS approximation (which performs well\non non-linear inverse problems), and hence we believe our method can also be similarly extended.\n                                                              12", "md": "# Document\n\n## (a) Input\n\n## (b) Groundtruth\n\n## (c) DPS [11]\n\n## (d) PSLD (Ours)\n\nFigure 4: Inpainting (random and box) results on out-of-distribution samples, 256\u00d7256 (see Appendix B for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model.\n\n| |DPS| |PSLD|\n|---|---|---|---|\n|0.30|32|0.90| |\n|0.25|PSLD| | |\n|0.20|2|30|1|0.85|\n|0.15|28|DPS|0.80|DPS|\n|0.10|26|PSLD|0.75|PSLD|\n| |20|40|60|80|20|40|60|80|20|40|60|80|\n| |Percentage of dropped pixels| |Percentage of dropped pixels| |Percentage of dropped pixels|\n\nFigure 5: Comparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as the percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.\n\n### Conclusion\n\nIn this paper, we leverage latent diffusion models to solve general linear inverse problems. While previously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to use the image prior learned by latent-based foundation generative models. We provide a principled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to design a modified objective (goodness and gluing). This leads to our algorithm \u2013 Posterior Sampling with Latent Diffusion (PSLD) \u2013 that experimentally outperforms state-of-art baselines on a wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.\n\nLimitations. Our evaluation is based on Stable Diffusion which was trained on the LAION dataset. Biases in this dataset and foundation model will be implicitly affecting our algorithm. Our method can work with any LDM and we expect new foundation models trained on better datasets like [19] to mitigate these issues. Second, we have not explored how to use latent-based foundation models to solve non-linear inverse problems. Our method builds on the DPS approximation (which performs well on non-linear inverse problems), and hence we believe our method can also be similarly extended.\n\n12", "images": [{"name": "img_p11_1", "height": 348, "width": 348}, {"name": "img_p11_2", "height": 348, "width": 348}, {"name": "img_p11_3", "height": 348, "width": 348}, {"name": "img_p11_4", "height": 348, "width": 348}, {"name": "g_d433_img_p11_5"}, {"name": "g_d433_img_p11_6"}, {"name": "img_p11_7", "height": 348, "width": 348}, {"name": "g_d433_img_p11_8"}, {"name": "img_p11_9", "height": 297, "width": 464}, {"name": "img_p11_10", "height": 289, "width": 464}, {"name": "img_p11_11", "height": 289, "width": 464}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "(a) Input", "md": "## (a) Input"}, {"type": "heading", "lvl": 2, "value": "(b) Groundtruth", "md": "## (b) Groundtruth"}, {"type": "heading", "lvl": 2, "value": "(c) DPS [11]", "md": "## (c) DPS [11]"}, {"type": "heading", "lvl": 2, "value": "(d) PSLD (Ours)", "md": "## (d) PSLD (Ours)"}, {"type": "text", "value": "Figure 4: Inpainting (random and box) results on out-of-distribution samples, 256\u00d7256 (see Appendix B for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model.", "md": "Figure 4: Inpainting (random and box) results on out-of-distribution samples, 256\u00d7256 (see Appendix B for image sources). We use PSLD with Stable Diffusion v-1.5 as generative foundation model."}, {"type": "table", "rows": [["", "DPS", "", "PSLD"], ["0.30", "32", "0.90", ""], ["0.25", "PSLD", "", ""], ["0.20", "2", "30", "1", "0.85"], ["0.15", "28", "DPS", "0.80", "DPS"], ["0.10", "26", "PSLD", "0.75", "PSLD"], ["", "20", "40", "60", "80", "20", "40", "60", "80", "20", "40", "60", "80"], ["", "Percentage of dropped pixels", "", "Percentage of dropped pixels", "", "Percentage of dropped pixels"]], "md": "| |DPS| |PSLD|\n|---|---|---|---|\n|0.30|32|0.90| |\n|0.25|PSLD| | |\n|0.20|2|30|1|0.85|\n|0.15|28|DPS|0.80|DPS|\n|0.10|26|PSLD|0.75|PSLD|\n| |20|40|60|80|20|40|60|80|20|40|60|80|\n| |Percentage of dropped pixels| |Percentage of dropped pixels| |Percentage of dropped pixels|", "isPerfectTable": false, "csv": "\"\",\"DPS\",\"\",\"PSLD\"\n\"0.30\",\"32\",\"0.90\",\"\"\n\"0.25\",\"PSLD\",\"\",\"\"\n\"0.20\",\"2\",\"30\",\"1\",\"0.85\"\n\"0.15\",\"28\",\"DPS\",\"0.80\",\"DPS\"\n\"0.10\",\"26\",\"PSLD\",\"0.75\",\"PSLD\"\n\"\",\"20\",\"40\",\"60\",\"80\",\"20\",\"40\",\"60\",\"80\",\"20\",\"40\",\"60\",\"80\"\n\"\",\"Percentage of dropped pixels\",\"\",\"Percentage of dropped pixels\",\"\",\"Percentage of dropped pixels\""}, {"type": "text", "value": "Figure 5: Comparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as the percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS.", "md": "Figure 5: Comparing DPS and PSLD performance in random inpainting on FFHQ 256 [25, 11], as the percentage of masked pixels increases. PSLD with Stable Diffusion outperforms DPS."}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "In this paper, we leverage latent diffusion models to solve general linear inverse problems. While previously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to use the image prior learned by latent-based foundation generative models. We provide a principled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to design a modified objective (goodness and gluing). This leads to our algorithm \u2013 Posterior Sampling with Latent Diffusion (PSLD) \u2013 that experimentally outperforms state-of-art baselines on a wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.\n\nLimitations. Our evaluation is based on Stable Diffusion which was trained on the LAION dataset. Biases in this dataset and foundation model will be implicitly affecting our algorithm. Our method can work with any LDM and we expect new foundation models trained on better datasets like [19] to mitigate these issues. Second, we have not explored how to use latent-based foundation models to solve non-linear inverse problems. Our method builds on the DPS approximation (which performs well on non-linear inverse problems), and hence we believe our method can also be similarly extended.\n\n12", "md": "In this paper, we leverage latent diffusion models to solve general linear inverse problems. While previously proposed approaches only apply to pixel-space diffusion models, our algorithm allows us to use the image prior learned by latent-based foundation generative models. We provide a principled analysis of our algorithm in a linear two-step diffusion setting, and use insights from this analysis to design a modified objective (goodness and gluing). This leads to our algorithm \u2013 Posterior Sampling with Latent Diffusion (PSLD) \u2013 that experimentally outperforms state-of-art baselines on a wide variety of tasks including random inpainting, block inpainting, denoising, destriping, and super-resolution.\n\nLimitations. Our evaluation is based on Stable Diffusion which was trained on the LAION dataset. Biases in this dataset and foundation model will be implicitly affecting our algorithm. Our method can work with any LDM and we expect new foundation models trained on better datasets like [19] to mitigate these issues. Second, we have not explored how to use latent-based foundation models to solve non-linear inverse problems. Our method builds on the DPS approximation (which performs well on non-linear inverse problems), and hence we believe our method can also be similarly extended.\n\n12"}]}, {"page": 13, "text": "Acknowledgements\nThis research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods\nCCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital,\nWireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin\nMachine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering.\nLitu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in\nEngineering.      Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS\n012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace\nteam for providing us GPU support for the demo of our work.\nReferences\n  [1]   Brian D.O. Anderson. \u201cReverse-time diffusion equation models\u201d. In:                      Stochastic Processes and\n       their Applications      12.3 (1982), pp. 313\u2013326 (page 1).\n  [2]   Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir.\n       \u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\u201d. In:                           International\n        Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 3).\n  [3]   Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Gold-\n        blum, Jonas Geiping, and Tom Goldstein. \u201cCold Diffusion: Inverting arbitrary image transforms\n        without noise\u201d. In:     arXiv preprint arXiv:2208.09392           (2022) (page 3).\n  [4]   Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\n        Fidler, and Karsten Kreis. \u201cAlign your latents: High-resolution video synthesis with latent\n        diffusion models\u201d. In:    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n        Recognition. 2023, pp. 22563\u201322575 (page 3).\n  [5]   Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \u201cCompressed sensing using\n        generative models\u201d. In:      International Conference on Machine Learning. PMLR. 2017, pp. 537\u2013546\n       (page 1).\n  [6]   Stanley H Chan, Xiran Wang, and Omar A Elgendy. \u201cPlug-and-play ADMM for image restoration:\n        Fixed-point convergence and applications\u201d. In:              IEEE Transactions on Computational Imaging\n        3.1 (2016), pp. 84\u201398 (pages 3, 9, 30).\n  [7]   Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \u201cScore Approximation, Estimation\n        and Distribution Recovery of Diffusion Models on Low-Dimensional Data\u201d. In:                          arXiv preprint\n       arXiv:2302.07194        (2023) (page 6).\n  [8]   Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \u201cSampling is\n        as easy as learning the score: theory for diffusion models with minimal data assumptions\u201d. In:\n       arXiv preprint arXiv:2209.11215            (2022) (page 3).\n  [9]   Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \u201cRestoration-Degradation Beyond\n        Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\u201d. In:                            arXiv preprint\n       arXiv:2303.03384        (2023) (page 3).\n[10]    Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \u201cIlvr: Con-\n        ditioning method for denoising diffusion probabilistic models\u201d. In:             arXiv preprint arXiv:2108.02938\n       (2021) (page 3).                                       13", "md": "# Acknowledgements and References\n\n## Acknowledgements\n\nThis research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.\n\n## References\n\n1. Brian D.O. Anderson. \u201cReverse-time diffusion equation models\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\u2013326 (page 1).\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 3).\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \u201cCold Diffusion: Inverting arbitrary image transforms without noise\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 3).\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \u201cAlign your latents: High-resolution video synthesis with latent diffusion models\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\u201322575 (page 3).\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \u201cCompressed sensing using generative models\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\u2013546 (page 1).\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\u201398 (pages 3, 9, 30).\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (page 6).\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 3).\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 3).\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \u201cIlvr: Conditioning method for denoising diffusion probabilistic models\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 3).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgements and References", "md": "# Acknowledgements and References"}, {"type": "heading", "lvl": 2, "value": "Acknowledgements", "md": "## Acknowledgements"}, {"type": "text", "value": "This research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work.", "md": "This research has been supported by NSF Grants 2019844, 2112471, AF 1901292, CNS 2148141, Tripods CCF 1934932, the Texas Advanced Computing Center (TACC) and research gifts by Western Digital, Wireless Networking and Communications Group (WNCG) Industrial Affiliates Program, UT Austin Machine Learning Lab (MLL), Cisco and the Stanly P. Finch Centennial Professorship in Engineering. Litu Rout has been supported by the Ju-Nam and Pearl Chew Endowed Presidential Fellowship in Engineering. Giannis Daras has been supported by the Onassis Fellowship (Scholarship ID: F ZS 012-1/2022-2023), the Bodossaki Fellowship and the Leventis Fellowship. We thank the HuggingFace team for providing us GPU support for the demo of our work."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. Brian D.O. Anderson. \u201cReverse-time diffusion equation models\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\u2013326 (page 1).\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 3).\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \u201cCold Diffusion: Inverting arbitrary image transforms without noise\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 3).\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \u201cAlign your latents: High-resolution video synthesis with latent diffusion models\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\u201322575 (page 3).\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \u201cCompressed sensing using generative models\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\u2013546 (page 1).\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\u201398 (pages 3, 9, 30).\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (page 6).\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 3).\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 3).\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \u201cIlvr: Conditioning method for denoising diffusion probabilistic models\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 3).", "md": "1. Brian D.O. Anderson. \u201cReverse-time diffusion equation models\u201d. In: Stochastic Processes and their Applications 12.3 (1982), pp. 313\u2013326 (page 1).\n2. Marius Arvinte, Ajil Jalal, Giannis Daras, Eric Price, Alex Dimakis, and Jonathan I Tamir. \u201cSingle-Shot Adaptation using Score-Based Models for MRI Reconstruction\u201d. In: International Society for Magnetic Resonance in Medicine, Annual Meeting. 2022 (page 3).\n3. Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. \u201cCold Diffusion: Inverting arbitrary image transforms without noise\u201d. In: arXiv preprint arXiv:2208.09392 (2022) (page 3).\n4. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. \u201cAlign your latents: High-resolution video synthesis with latent diffusion models\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 22563\u201322575 (page 3).\n5. Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. \u201cCompressed sensing using generative models\u201d. In: International Conference on Machine Learning. PMLR. 2017, pp. 537\u2013546 (page 1).\n6. Stanley H Chan, Xiran Wang, and Omar A Elgendy. \u201cPlug-and-play ADMM for image restoration: Fixed-point convergence and applications\u201d. In: IEEE Transactions on Computational Imaging 3.1 (2016), pp. 84\u201398 (pages 3, 9, 30).\n7. Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. \u201cScore Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data\u201d. In: arXiv preprint arXiv:2302.07194 (2023) (page 6).\n8. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. \u201cSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions\u201d. In: arXiv preprint arXiv:2209.11215 (2022) (page 3).\n9. Sitan Chen, Giannis Daras, and Alexandros G Dimakis. \u201cRestoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers\u201d. In: arXiv preprint arXiv:2303.03384 (2023) (page 3).\n10. Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. \u201cIlvr: Conditioning method for denoising diffusion probabilistic models\u201d. In: arXiv preprint arXiv:2108.02938 (2021) (page 3)."}]}, {"page": 14, "text": "[11]  Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong\n      Chul Ye. \u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\u201d. In:                         The Eleventh\n      International Conference on Learning Representations. 2023.                     url:  https://openreview.net/\n      forum?id=OnD9zGAGT0k            (pages 1, 3, 4, 7, 9, 11, 12, 20, 22, 28\u201334).\n[12]  Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \u201cDirect Diffusion Bridge using Data Consis-\n      tency for Inverse Problems\u201d. In:          arXiv preprint arXiv:2305.19809            (2023) (page 3).\n[13]  Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \u201cImproving Diffusion Models\n      for Inverse Problems using Manifold Constraints\u201d. In:               Advances in Neural Information Processing\n      Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022.                             url:\n      https://openreview.net/forum?id=nJJjv0JDJju                        (pages 3, 9, 30).\n[14]  Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \u201cScore-guided\n      intermediate layer optimization: Fast langevin mixing for inverse problem\u201d. In:                        arXiv preprint\n      arXiv:2206.09104        (2022) (page 3).\n[15]  Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar.\n      \u201cSoft diffusion: Score matching for general corruptions\u201d. In:                  arXiv preprint arXiv:2209.05442\n      (2022) (page 3).\n[16]  Mauricio Delbracio and Peyman Milanfar. \u201cInversion by direct iteration: An alternative to\n      denoising diffusion for image restoration\u201d. In:            arXiv preprint arXiv:2303.11435            (2023) (page 3).\n[17]  Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \u201cImagenet: A large-\n      scale hierarchical image database\u201d. In:            2009 IEEE conference on computer vision and pattern\n      recognition. Ieee. 2009, pp. 248\u2013255 (pages 9, 24, 28, 30\u201332).\n[18]  Prafulla Dhariwal and Alexander Nichol. \u201cDiffusion models beat gans on image synthesis\u201d. In:\n      Advances in Neural Information Processing Systems                  34 (2021), pp. 8780\u20138794 (page 1).\n[19]  Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\n      Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. \u201cDataComp:\n      In search of the next generation of multimodal datasets\u201d. In:                  arXiv preprint arXiv:2304.14108\n      (2023) (page 12).\n[20]  Jonathan Ho, Ajay Jain, and Pieter Abbeel. \u201cDenoising diffusion probabilistic models\u201d. In:\n      Advances in Neural Information Processing Systems                  33 (2020), pp. 6840\u20136851 (page 1).\n[21]  Aapo Hyv\u00e4rinen and Peter Dayan. \u201cEstimation of non-normalized statistical models by score\n      matching.\u201d In:      Journal of Machine Learning Research              6.4 (2005) (page 1).\n[22]  Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir.\n      \u201cRobust compressed sensing mri with deep generative priors\u201d. In:                 Advances in Neural Information\n      Processing Systems       34 (2021), pp. 14938\u201314954 (pages 1, 3).\n[23]  Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price. \u201cInstance-optimal com-\n      pressed sensing via posterior sampling\u201d. In:            arXiv preprint arXiv:2106.11438            (2021) (page 1).\n[24]  Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price. \u201cFairness for\n      Image Generation with Uncertain Sensitive Attributes\u201d. In:                 Proceedings of the 38th International\n      Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of\n      Machine Learning Research. PMLR, 18\u201324 Jul 2021, pp. 4721\u20134732.                      url:   https://proceedings.\n      mlr.press/v139/jalal21b.html                (page 1).\n[25]  Tero Karras, Samuli Laine, and Timo Aila. \u201cA style-based generator architecture for generative\n      adversarial networks\u201d. In:         Proceedings of the IEEE/CVF conference on computer vision and\n      pattern recognition. 2019, pp. 4401\u20134410 (pages 9, 11, 12, 28\u201330).\n                                                              14", "md": "# References\n\n# References\n\n[11] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\u201d. In: The Eleventh International Conference on Learning Representations. 2023. Link (pages 1, 3, 4, 7, 9, 11, 12, 20, 22, 28\u201334).\n\n[12] Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 3).\n\n[13] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. Link (pages 3, 9, 30).\n\n[14] Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 3).\n\n[15] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar. \u201cSoft diffusion: Score matching for general corruptions\u201d. In: arXiv preprint arXiv:2209.05442 (2022) (page 3).\n\n[16] Mauricio Delbracio and Peyman Milanfar. \u201cInversion by direct iteration: An alternative to denoising diffusion for image restoration\u201d. In: arXiv preprint arXiv:2303.11435 (2023) (page 3).\n\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \u201cImagenet: A large-scale hierarchical image database\u201d. In: 2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\u2013255 (pages 9, 24, 28, 30\u201332).\n\n[18] Prafulla Dhariwal and Alexander Nichol. \u201cDiffusion models beat gans on image synthesis\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 8780\u20138794 (page 1).\n\n[19] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. \u201cDataComp: In search of the next generation of multimodal datasets\u201d. In: arXiv preprint arXiv:2304.14108 (2023) (page 12).\n\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. \u201cDenoising diffusion probabilistic models\u201d. In: Advances in Neural Information Processing Systems 33 (2020), pp. 6840\u20136851 (page 1).\n\n[21] Aapo Hyv\u00e4rinen and Peter Dayan. \u201cEstimation of non-normalized statistical models by score matching.\u201d In: Journal of Machine Learning Research 6.4 (2005) (page 1).\n\n[22] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. \u201cRobust compressed sensing mri with deep generative priors\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 14938\u201314954 (pages 1, 3).\n\n[23] Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price. \u201cInstance-optimal compressed sensing via posterior sampling\u201d. In: arXiv preprint arXiv:2106.11438 (2021) (page 1).\n\n[24] Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price. \u201cFairness for Image Generation with Uncertain Sensitive Attributes\u201d. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\u201324 Jul 2021, pp. 4721\u20134732. Link (page 1).\n\n[25] Tero Karras, Samuli Laine, and Timo Aila. \u201cA style-based generator architecture for generative adversarial networks\u201d. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 4401\u20134410 (pages 9, 11, 12, 28\u201330).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "[11] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\u201d. In: The Eleventh International Conference on Learning Representations. 2023. Link (pages 1, 3, 4, 7, 9, 11, 12, 20, 22, 28\u201334).\n\n[12] Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 3).\n\n[13] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. Link (pages 3, 9, 30).\n\n[14] Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 3).\n\n[15] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar. \u201cSoft diffusion: Score matching for general corruptions\u201d. In: arXiv preprint arXiv:2209.05442 (2022) (page 3).\n\n[16] Mauricio Delbracio and Peyman Milanfar. \u201cInversion by direct iteration: An alternative to denoising diffusion for image restoration\u201d. In: arXiv preprint arXiv:2303.11435 (2023) (page 3).\n\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \u201cImagenet: A large-scale hierarchical image database\u201d. In: 2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\u2013255 (pages 9, 24, 28, 30\u201332).\n\n[18] Prafulla Dhariwal and Alexander Nichol. \u201cDiffusion models beat gans on image synthesis\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 8780\u20138794 (page 1).\n\n[19] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. \u201cDataComp: In search of the next generation of multimodal datasets\u201d. In: arXiv preprint arXiv:2304.14108 (2023) (page 12).\n\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. \u201cDenoising diffusion probabilistic models\u201d. In: Advances in Neural Information Processing Systems 33 (2020), pp. 6840\u20136851 (page 1).\n\n[21] Aapo Hyv\u00e4rinen and Peter Dayan. \u201cEstimation of non-normalized statistical models by score matching.\u201d In: Journal of Machine Learning Research 6.4 (2005) (page 1).\n\n[22] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. \u201cRobust compressed sensing mri with deep generative priors\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 14938\u201314954 (pages 1, 3).\n\n[23] Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price. \u201cInstance-optimal compressed sensing via posterior sampling\u201d. In: arXiv preprint arXiv:2106.11438 (2021) (page 1).\n\n[24] Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price. \u201cFairness for Image Generation with Uncertain Sensitive Attributes\u201d. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\u201324 Jul 2021, pp. 4721\u20134732. Link (page 1).\n\n[25] Tero Karras, Samuli Laine, and Timo Aila. \u201cA style-based generator architecture for generative adversarial networks\u201d. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 4401\u20134410 (pages 9, 11, 12, 28\u201330).", "md": "[11] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. \u201cDiffusion Posterior Sampling for General Noisy Inverse Problems\u201d. In: The Eleventh International Conference on Learning Representations. 2023. Link (pages 1, 3, 4, 7, 9, 11, 12, 20, 22, 28\u201334).\n\n[12] Hyungjin Chung, Jeongsol Kim, and Jong Chul Ye. \u201cDirect Diffusion Bridge using Data Consistency for Inverse Problems\u201d. In: arXiv preprint arXiv:2305.19809 (2023) (page 3).\n\n[13] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. \u201cImproving Diffusion Models for Inverse Problems using Manifold Constraints\u201d. In: Advances in Neural Information Processing Systems. Ed. by Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho. 2022. Link (pages 3, 9, 30).\n\n[14] Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. \u201cScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem\u201d. In: arXiv preprint arXiv:2206.09104 (2022) (page 3).\n\n[15] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar. \u201cSoft diffusion: Score matching for general corruptions\u201d. In: arXiv preprint arXiv:2209.05442 (2022) (page 3).\n\n[16] Mauricio Delbracio and Peyman Milanfar. \u201cInversion by direct iteration: An alternative to denoising diffusion for image restoration\u201d. In: arXiv preprint arXiv:2303.11435 (2023) (page 3).\n\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \u201cImagenet: A large-scale hierarchical image database\u201d. In: 2009 IEEE conference on computer vision and pattern recognition. Ieee. 2009, pp. 248\u2013255 (pages 9, 24, 28, 30\u201332).\n\n[18] Prafulla Dhariwal and Alexander Nichol. \u201cDiffusion models beat gans on image synthesis\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 8780\u20138794 (page 1).\n\n[19] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. \u201cDataComp: In search of the next generation of multimodal datasets\u201d. In: arXiv preprint arXiv:2304.14108 (2023) (page 12).\n\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. \u201cDenoising diffusion probabilistic models\u201d. In: Advances in Neural Information Processing Systems 33 (2020), pp. 6840\u20136851 (page 1).\n\n[21] Aapo Hyv\u00e4rinen and Peter Dayan. \u201cEstimation of non-normalized statistical models by score matching.\u201d In: Journal of Machine Learning Research 6.4 (2005) (page 1).\n\n[22] Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir. \u201cRobust compressed sensing mri with deep generative priors\u201d. In: Advances in Neural Information Processing Systems 34 (2021), pp. 14938\u201314954 (pages 1, 3).\n\n[23] Ajil Jalal, Sushrut Karmalkar, Alexandros G Dimakis, and Eric Price. \u201cInstance-optimal compressed sensing via posterior sampling\u201d. In: arXiv preprint arXiv:2106.11438 (2021) (page 1).\n\n[24] Ajil Jalal, Sushrut Karmalkar, Jessica Hoffmann, Alex Dimakis, and Eric Price. \u201cFairness for Image Generation with Uncertain Sensitive Attributes\u201d. In: Proceedings of the 38th International Conference on Machine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings of Machine Learning Research. PMLR, 18\u201324 Jul 2021, pp. 4721\u20134732. Link (page 1).\n\n[25] Tero Karras, Samuli Laine, and Timo Aila. \u201cA style-based generator architecture for generative adversarial networks\u201d. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 4401\u20134410 (pages 9, 11, 12, 28\u201330)."}]}, {"page": 15, "text": "[26]   Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. \u201cDenoising Diffusion Restoration\n       Models\u201d. In:     Advances in Neural Information Processing Systems                 (pages 1, 3, 9, 30).\n[27]   Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad. \u201cGSURE-Based Diffusion Model\n       Training with Corrupted Data\u201d. In:            arXiv preprint arXiv:2305.13128           (2023) (page 3).\n[28]   Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. \u201cSoft truncation:\n       A universal training technique of score-based diffusion model for high precision score estimation\u201d.\n       In: International Conference on Machine Learning. PMLR. 2022, pp. 11201\u201311228 (page 1).\n[29]   Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\n       Mark D Plumbley. \u201cAudioldm: Text-to-audio generation with latent diffusion models\u201d. In:                          arXiv\n       preprint arXiv:2301.12503         (2023) (page 3).\n[30]   Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang. \u201cCoherent Semantic Attention for Image\n       Inpainting\u201d. In:     2019 IEEE/CVF International Conference on Computer Vision (ICCV)                             (Oct.\n       2019).   doi:  10.1109/iccv.2019.00427.            url:   http://dx.doi.org/10.1109/ICCV.2019.00427\n       (page 1).\n[31]   Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van\n       Gool. \u201cRepaint: Inpainting using denoising diffusion probabilistic models\u201d. In:                   Proceedings of the\n       IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 11461\u201311471\n       (pages 9, 11, 33).\n[32]   Gary Mataev, Peyman Milanfar, and Michael Elad. \u201cDeepRED: Deep image prior powered\n       by RED\u201d. In:       Proceedings of the IEEE/CVF International Conference on Computer Vision\n      Workshops. 2019, pp. 0\u20130 (page 1).\n[33]   Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. \u201cPulse: Self-\n       supervised photo upsampling via latent space exploration of generative models\u201d. In:                    Proceedings of\n       the ieee/cvf conference on computer vision and pattern recognition. 2020, pp. 2437\u20132445 (page 1).\n[34]   Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis,\n       and Rebecca Willett. \u201cDeep learning techniques for inverse problems in imaging\u201d. In:                             IEEE\n       Journal on Selected Areas in Information Theory                1.1 (2020), pp. 39\u201356 (page 1).\n[35]   Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. \u201cContext\n       encoders: Feature learning by inpainting\u201d. In:             Proceedings of the IEEE conference on computer\n       vision and pattern recognition. 2016, pp. 2536\u20132544 (page 1).\n[36]   Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez,\n       Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso. \u201cBrain imaging generation with\n       latent diffusion models\u201d. In:      Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI\n       2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings.\n       Springer. 2022, pp. 117\u2013126 (page 3).\n[37]   Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and\n       Daniel Cohen-Or. \u201cEncoding in Style: a StyleGAN Encoder for Image-to-Image Translation\u201d. In:\n       arXiv preprint arXiv:2008.00951           (2020) (page 1).\n[38]   Yaniv Romano, Michael Elad, and Peyman Milanfar. \u201cThe little engine that could: Regularization\n       by denoising (RED)\u201d. In:        SIAM Journal on Imaging Sciences            10.4 (2017), pp. 1804\u20131844 (page 1).\n[39]   Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. \u201cHigh-\n       resolution image synthesis with latent diffusion models\u201d. In:                   Proceedings of the IEEE/CVF\n       Conference on Computer Vision and Pattern Recognition. 2022, pp. 10684\u201310695 (pages 3, 6, 9).\n                                                              15", "md": "|Reference|Authors|Title|Publication Details|\n|---|---|---|---|\n|[26]|Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song|Denoising Diffusion Restoration Models|Advances in Neural Information Processing Systems (pages 1, 3, 9, 30)|\n|[27]|Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad|GSURE-Based Diffusion Model Training with Corrupted Data|arXiv preprint arXiv:2305.13128 (2023) (page 3)|\n|[28]|Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon|Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation|International Conference on Machine Learning. PMLR. 2022, pp. 11201\u201311228 (page 1)|\n|[29]|Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley|Audioldm: Text-to-audio generation with latent diffusion models|arXiv preprint arXiv:2301.12503 (2023) (page 3)|\n|[30]|Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang|Coherent Semantic Attention for Image Inpainting|2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). doi: 10.1109/iccv.2019.00427. url: http://dx.doi.org/10.1109/ICCV.2019.00427 (page 1)|\n|[31]|Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool|Repaint: Inpainting using denoising diffusion probabilistic models|Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 11461\u201311471 (pages 9, 11, 33)|\n|[32]|Gary Mataev, Peyman Milanfar, and Michael Elad|DeepRED: Deep image prior powered by RED|Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019, pp. 0\u20130 (page 1)|\n|[33]|Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin|Pulse: Self-supervised photo upsampling via latent space exploration of generative models|Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2020, pp. 2437\u20132445 (page 1)|\n|[34]|Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett|Deep learning techniques for inverse problems in imaging|IEEE Journal on Selected Areas in Information Theory 1.1 (2020), pp. 39\u201356 (page 1)|\n|[35]|Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros|Context encoders: Feature learning by inpainting|Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 2536\u20132544 (page 1)|\n|[36]|Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso|Brain imaging generation with latent diffusion models|Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer. 2022, pp. 117\u2013126 (page 3)|\n|[37]|Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or|Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation|arXiv preprint arXiv:2008.00951 (2020) (page 1)|\n|[38]|Yaniv Romano, Michael Elad, and Peyman Milanfar|The little engine that could: Regularization by denoising (RED)|SIAM Journal on Imaging Sciences 10.4 (2017), pp. 1804\u20131844 (page 1)|\n|[39]|Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer|High-resolution image synthesis with latent diffusion models|Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10684\u201310695 (pages 3, 6, 9)|", "images": [], "items": [{"type": "table", "rows": [["Reference", "Authors", "Title", "Publication Details"], ["[26]", "Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song", "Denoising Diffusion Restoration Models", "Advances in Neural Information Processing Systems (pages 1, 3, 9, 30)"], ["[27]", "Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad", "GSURE-Based Diffusion Model Training with Corrupted Data", "arXiv preprint arXiv:2305.13128 (2023) (page 3)"], ["[28]", "Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon", "Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation", "International Conference on Machine Learning. PMLR. 2022, pp. 11201\u201311228 (page 1)"], ["[29]", "Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley", "Audioldm: Text-to-audio generation with latent diffusion models", "arXiv preprint arXiv:2301.12503 (2023) (page 3)"], ["[30]", "Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang", "Coherent Semantic Attention for Image Inpainting", "2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). doi: 10.1109/iccv.2019.00427. url: http://dx.doi.org/10.1109/ICCV.2019.00427 (page 1)"], ["[31]", "Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool", "Repaint: Inpainting using denoising diffusion probabilistic models", "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 11461\u201311471 (pages 9, 11, 33)"], ["[32]", "Gary Mataev, Peyman Milanfar, and Michael Elad", "DeepRED: Deep image prior powered by RED", "Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019, pp. 0\u20130 (page 1)"], ["[33]", "Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin", "Pulse: Self-supervised photo upsampling via latent space exploration of generative models", "Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2020, pp. 2437\u20132445 (page 1)"], ["[34]", "Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett", "Deep learning techniques for inverse problems in imaging", "IEEE Journal on Selected Areas in Information Theory 1.1 (2020), pp. 39\u201356 (page 1)"], ["[35]", "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros", "Context encoders: Feature learning by inpainting", "Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 2536\u20132544 (page 1)"], ["[36]", "Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso", "Brain imaging generation with latent diffusion models", "Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer. 2022, pp. 117\u2013126 (page 3)"], ["[37]", "Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or", "Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation", "arXiv preprint arXiv:2008.00951 (2020) (page 1)"], ["[38]", "Yaniv Romano, Michael Elad, and Peyman Milanfar", "The little engine that could: Regularization by denoising (RED)", "SIAM Journal on Imaging Sciences 10.4 (2017), pp. 1804\u20131844 (page 1)"], ["[39]", "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer", "High-resolution image synthesis with latent diffusion models", "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10684\u201310695 (pages 3, 6, 9)"]], "md": "|Reference|Authors|Title|Publication Details|\n|---|---|---|---|\n|[26]|Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song|Denoising Diffusion Restoration Models|Advances in Neural Information Processing Systems (pages 1, 3, 9, 30)|\n|[27]|Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad|GSURE-Based Diffusion Model Training with Corrupted Data|arXiv preprint arXiv:2305.13128 (2023) (page 3)|\n|[28]|Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon|Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation|International Conference on Machine Learning. PMLR. 2022, pp. 11201\u201311228 (page 1)|\n|[29]|Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley|Audioldm: Text-to-audio generation with latent diffusion models|arXiv preprint arXiv:2301.12503 (2023) (page 3)|\n|[30]|Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang|Coherent Semantic Attention for Image Inpainting|2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). doi: 10.1109/iccv.2019.00427. url: http://dx.doi.org/10.1109/ICCV.2019.00427 (page 1)|\n|[31]|Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool|Repaint: Inpainting using denoising diffusion probabilistic models|Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 11461\u201311471 (pages 9, 11, 33)|\n|[32]|Gary Mataev, Peyman Milanfar, and Michael Elad|DeepRED: Deep image prior powered by RED|Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019, pp. 0\u20130 (page 1)|\n|[33]|Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin|Pulse: Self-supervised photo upsampling via latent space exploration of generative models|Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2020, pp. 2437\u20132445 (page 1)|\n|[34]|Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett|Deep learning techniques for inverse problems in imaging|IEEE Journal on Selected Areas in Information Theory 1.1 (2020), pp. 39\u201356 (page 1)|\n|[35]|Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros|Context encoders: Feature learning by inpainting|Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 2536\u20132544 (page 1)|\n|[36]|Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso|Brain imaging generation with latent diffusion models|Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer. 2022, pp. 117\u2013126 (page 3)|\n|[37]|Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or|Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation|arXiv preprint arXiv:2008.00951 (2020) (page 1)|\n|[38]|Yaniv Romano, Michael Elad, and Peyman Milanfar|The little engine that could: Regularization by denoising (RED)|SIAM Journal on Imaging Sciences 10.4 (2017), pp. 1804\u20131844 (page 1)|\n|[39]|Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer|High-resolution image synthesis with latent diffusion models|Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10684\u201310695 (pages 3, 6, 9)|", "isPerfectTable": true, "csv": "\"Reference\",\"Authors\",\"Title\",\"Publication Details\"\n\"[26]\",\"Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song\",\"Denoising Diffusion Restoration Models\",\"Advances in Neural Information Processing Systems (pages 1, 3, 9, 30)\"\n\"[27]\",\"Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael Elad\",\"GSURE-Based Diffusion Model Training with Corrupted Data\",\"arXiv preprint arXiv:2305.13128 (2023) (page 3)\"\n\"[28]\",\"Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon\",\"Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation\",\"International Conference on Machine Learning. PMLR. 2022, pp. 11201\u201311228 (page 1)\"\n\"[29]\",\"Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley\",\"Audioldm: Text-to-audio generation with latent diffusion models\",\"arXiv preprint arXiv:2301.12503 (2023) (page 3)\"\n\"[30]\",\"Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang\",\"Coherent Semantic Attention for Image Inpainting\",\"2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). doi: 10.1109/iccv.2019.00427. url: http://dx.doi.org/10.1109/ICCV.2019.00427 (page 1)\"\n\"[31]\",\"Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool\",\"Repaint: Inpainting using denoising diffusion probabilistic models\",\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 11461\u201311471 (pages 9, 11, 33)\"\n\"[32]\",\"Gary Mataev, Peyman Milanfar, and Michael Elad\",\"DeepRED: Deep image prior powered by RED\",\"Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 2019, pp. 0\u20130 (page 1)\"\n\"[33]\",\"Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin\",\"Pulse: Self-supervised photo upsampling via latent space exploration of generative models\",\"Proceedings of the ieee/cvf conference on computer vision and pattern recognition. 2020, pp. 2437\u20132445 (page 1)\"\n\"[34]\",\"Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, and Rebecca Willett\",\"Deep learning techniques for inverse problems in imaging\",\"IEEE Journal on Selected Areas in Information Theory 1.1 (2020), pp. 39\u201356 (page 1)\"\n\"[35]\",\"Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros\",\"Context encoders: Feature learning by inpainting\",\"Proceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 2536\u20132544 (page 1)\"\n\"[36]\",\"Walter HL Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F Da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, and M Jorge Cardoso\",\"Brain imaging generation with latent diffusion models\",\"Deep Generative Models: Second MICCAI Workshop, DGM4MICCAI 2022, Held in Conjunction with MICCAI 2022, Singapore, September 22, 2022, Proceedings. Springer. 2022, pp. 117\u2013126 (page 3)\"\n\"[37]\",\"Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or\",\"Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation\",\"arXiv preprint arXiv:2008.00951 (2020) (page 1)\"\n\"[38]\",\"Yaniv Romano, Michael Elad, and Peyman Milanfar\",\"The little engine that could: Regularization by denoising (RED)\",\"SIAM Journal on Imaging Sciences 10.4 (2017), pp. 1804\u20131844 (page 1)\"\n\"[39]\",\"Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer\",\"High-resolution image synthesis with latent diffusion models\",\"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022, pp. 10684\u201310695 (pages 3, 6, 9)\""}]}, {"page": 16, "text": "[40]   Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai. \u201cA Theoretical\n       Justification for Image Inpainting using Denoising Diffusion Probabilistic Models\u201d. In:                          arXiv\n       preprint arXiv:2302.01217         (2023) (pages 6, 7).\n[41]   Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\n       Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.                     LAION-400M: Open Dataset\n       of CLIP-Filtered 400 Million Image-Text Pairs. 2021. arXiv:                  2111.02114 [cs.CV]          (page 9).\n[42]   Christoph Schuhmann et al.          LAION-5B: An open large-scale dataset for training next generation\n       image-text models. 2022. arXiv:         2210.08402 [cs.CV]          (page 9).\n[43]   Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. \u201cPseudoinverse-guided diffusion\n       models for inverse problems\u201d. In:          International Conference on Learning Representations. 2023\n       (page 3).\n[44]   Yang Song and Stefano Ermon. \u201cGenerative modeling by estimating gradients of the data\n       distribution\u201d. In:    Advances in Neural Information Processing Systems                  32 (2019) (page 1).\n[45]   Yang Song and Stefano Ermon. \u201cImproved techniques for training score-based generative models\u201d.\n       In: Advances in neural information processing systems                33 (2020), pp. 12438\u201312448 (page 1).\n[46]   Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\n       Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In:\n       International Conference on Learning Representations. 2021 (pages 1, 3).\n[47]   Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\n       Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In:\n       International Conference on Learning Representations                 (pages 9, 30).\n[48]   Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion\n       models from human brain activity\u201d. In:            Proceedings of the IEEE/CVF Conference on Computer\n      Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3).\n[49]   Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play priors\n       for model based reconstruction\u201d. In:           2013 IEEE Global Conference on Signal and Information\n       Processing. IEEE. 2013, pp. 945\u2013948 (page 1).\n[50]   Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In:                           Neural\n       computation     23.7 (2011), pp. 1661\u20131674 (page 1).\n[51]   Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini,\n       Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and editbench:\n       Advancing and evaluating text-guided image inpainting\u201d. In:                     Proceedings of the IEEE/CVF\n       Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369 (pages 1, 23).\n[52]   Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form Image\n       Inpainting With Gated Convolution\u201d. In:            2019 IEEE/CVF International Conference on Computer\n      Vision (ICCV)        (Oct. 2019).     doi:  10.1109/iccv.2019.00457.              url:   http://dx.doi.org/10.\n       1109/ICCV.2019.00457           (page 1).\n                                                              16", "md": "|[40]|Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai. \u201cA Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models\u201d. In: arXiv preprint arXiv:2302.01217 (2023) (pages 6, 7).|\n|---|---|\n|[41]|Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. 2021. arXiv: 2111.02114 [cs.CV] (page 9).|\n|[42]|Christoph Schuhmann et al. LAION-5B: An open large-scale dataset for training next generation image-text models. 2022. arXiv: 2210.08402 [cs.CV] (page 9).|\n|[43]|Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. \u201cPseudoinverse-guided diffusion models for inverse problems\u201d. In: International Conference on Learning Representations. 2023 (page 3).|\n|[44]|Yang Song and Stefano Ermon. \u201cGenerative modeling by estimating gradients of the data distribution\u201d. In: Advances in Neural Information Processing Systems 32 (2019) (page 1).|\n|[45]|Yang Song and Stefano Ermon. \u201cImproved techniques for training score-based generative models\u201d. In: Advances in neural information processing systems 33 (2020), pp. 12438\u201312448 (page 1).|\n|[46]|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In: International Conference on Learning Representations. 2021 (pages 1, 3).|\n|[47]|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In: International Conference on Learning Representations (pages 9, 30).|\n|[48]|Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion models from human brain activity\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3).|\n|[49]|Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play priors for model based reconstruction\u201d. In: 2013 IEEE Global Conference on Signal and Information Processing. IEEE. 2013, pp. 945\u2013948 (page 1).|\n|[50]|Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In: Neural computation 23.7 (2011), pp. 1661\u20131674 (page 1).|\n|[51]|Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and editbench: Advancing and evaluating text-guided image inpainting\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369 (pages 1, 23).|\n|[52]|Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form Image Inpainting With Gated Convolution\u201d. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). doi: 10.1109/iccv.2019.00457. url: http://dx.doi.org/10.1109/ICCV.2019.00457 (page 1).|", "images": [], "items": [{"type": "table", "rows": [["[40]", "Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai. \u201cA Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models\u201d. In: arXiv preprint arXiv:2302.01217 (2023) (pages 6, 7)."], ["[41]", "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. 2021. arXiv: 2111.02114 [cs.CV] (page 9)."], ["[42]", "Christoph Schuhmann et al. LAION-5B: An open large-scale dataset for training next generation image-text models. 2022. arXiv: 2210.08402 [cs.CV] (page 9)."], ["[43]", "Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. \u201cPseudoinverse-guided diffusion models for inverse problems\u201d. In: International Conference on Learning Representations. 2023 (page 3)."], ["[44]", "Yang Song and Stefano Ermon. \u201cGenerative modeling by estimating gradients of the data distribution\u201d. In: Advances in Neural Information Processing Systems 32 (2019) (page 1)."], ["[45]", "Yang Song and Stefano Ermon. \u201cImproved techniques for training score-based generative models\u201d. In: Advances in neural information processing systems 33 (2020), pp. 12438\u201312448 (page 1)."], ["[46]", "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In: International Conference on Learning Representations. 2021 (pages 1, 3)."], ["[47]", "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In: International Conference on Learning Representations (pages 9, 30)."], ["[48]", "Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion models from human brain activity\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3)."], ["[49]", "Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play priors for model based reconstruction\u201d. In: 2013 IEEE Global Conference on Signal and Information Processing. IEEE. 2013, pp. 945\u2013948 (page 1)."], ["[50]", "Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In: Neural computation 23.7 (2011), pp. 1661\u20131674 (page 1)."], ["[51]", "Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and editbench: Advancing and evaluating text-guided image inpainting\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369 (pages 1, 23)."], ["[52]", "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form Image Inpainting With Gated Convolution\u201d. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). doi: 10.1109/iccv.2019.00457. url: http://dx.doi.org/10.1109/ICCV.2019.00457 (page 1)."]], "md": "|[40]|Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai. \u201cA Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models\u201d. In: arXiv preprint arXiv:2302.01217 (2023) (pages 6, 7).|\n|---|---|\n|[41]|Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. 2021. arXiv: 2111.02114 [cs.CV] (page 9).|\n|[42]|Christoph Schuhmann et al. LAION-5B: An open large-scale dataset for training next generation image-text models. 2022. arXiv: 2210.08402 [cs.CV] (page 9).|\n|[43]|Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. \u201cPseudoinverse-guided diffusion models for inverse problems\u201d. In: International Conference on Learning Representations. 2023 (page 3).|\n|[44]|Yang Song and Stefano Ermon. \u201cGenerative modeling by estimating gradients of the data distribution\u201d. In: Advances in Neural Information Processing Systems 32 (2019) (page 1).|\n|[45]|Yang Song and Stefano Ermon. \u201cImproved techniques for training score-based generative models\u201d. In: Advances in neural information processing systems 33 (2020), pp. 12438\u201312448 (page 1).|\n|[46]|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In: International Conference on Learning Representations. 2021 (pages 1, 3).|\n|[47]|Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In: International Conference on Learning Representations (pages 9, 30).|\n|[48]|Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion models from human brain activity\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3).|\n|[49]|Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play priors for model based reconstruction\u201d. In: 2013 IEEE Global Conference on Signal and Information Processing. IEEE. 2013, pp. 945\u2013948 (page 1).|\n|[50]|Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In: Neural computation 23.7 (2011), pp. 1661\u20131674 (page 1).|\n|[51]|Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and editbench: Advancing and evaluating text-guided image inpainting\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369 (pages 1, 23).|\n|[52]|Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form Image Inpainting With Gated Convolution\u201d. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). doi: 10.1109/iccv.2019.00457. url: http://dx.doi.org/10.1109/ICCV.2019.00457 (page 1).|", "isPerfectTable": true, "csv": "\"[40]\",\"Litu Rout, Advait Parulekar, Constantine Caramanis, and Sanjay Shakkottai. \u201cA Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models\u201d. In: arXiv preprint arXiv:2302.01217 (2023) (pages 6, 7).\"\n\"[41]\",\"Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. 2021. arXiv: 2111.02114 [cs.CV] (page 9).\"\n\"[42]\",\"Christoph Schuhmann et al. LAION-5B: An open large-scale dataset for training next generation image-text models. 2022. arXiv: 2210.08402 [cs.CV] (page 9).\"\n\"[43]\",\"Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. \u201cPseudoinverse-guided diffusion models for inverse problems\u201d. In: International Conference on Learning Representations. 2023 (page 3).\"\n\"[44]\",\"Yang Song and Stefano Ermon. \u201cGenerative modeling by estimating gradients of the data distribution\u201d. In: Advances in Neural Information Processing Systems 32 (2019) (page 1).\"\n\"[45]\",\"Yang Song and Stefano Ermon. \u201cImproved techniques for training score-based generative models\u201d. In: Advances in neural information processing systems 33 (2020), pp. 12438\u201312448 (page 1).\"\n\"[46]\",\"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In: International Conference on Learning Representations. 2021 (pages 1, 3).\"\n\"[47]\",\"Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. \u201cScore-Based Generative Modeling through Stochastic Differential Equations\u201d. In: International Conference on Learning Representations (pages 9, 30).\"\n\"[48]\",\"Yu Takagi and Shinji Nishimoto. \u201cHigh-resolution image reconstruction with latent diffusion models from human brain activity\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 14453\u201314463 (page 3).\"\n\"[49]\",\"Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. \u201cPlug-and-play priors for model based reconstruction\u201d. In: 2013 IEEE Global Conference on Signal and Information Processing. IEEE. 2013, pp. 945\u2013948 (page 1).\"\n\"[50]\",\"Pascal Vincent. \u201cA connection between score matching and denoising autoencoders\u201d. In: Neural computation 23.7 (2011), pp. 1661\u20131674 (page 1).\"\n\"[51]\",\"Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. \u201cImagen editor and editbench: Advancing and evaluating text-guided image inpainting\u201d. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 18359\u201318369 (pages 1, 23).\"\n\"[52]\",\"Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. \u201cFree-Form Image Inpainting With Gated Convolution\u201d. In: 2019 IEEE/CVF International Conference on Computer Vision (ICCV) (Oct. 2019). doi: 10.1109/iccv.2019.00457. url: http://dx.doi.org/10.1109/ICCV.2019.00457 (page 1).\""}]}, {"page": 17, "text": "A       Technical Proofs\nNotation and Measurement Matrix.                       We elaborate on the structure of the measurement matrix\nA \u2208    Rl\u00d7d . In our setting, we are considering linear inverse problems.                     Thus, this matrix is a pixel\nselector and consists of a subset of the rows from the               d \u00d7  d  identity matrix (the rows that are present\n                                                                                     \u2212\u2192        d\ncorrespond to the indices of the selected pixels from the image                      x 0 \u2208  R   ).  Given this structure, it\nimmediately follows that         A T A  is a  d \u00d7  d matrix that has the interpretation of a pixel selection                mask.\nSpecifically,   A T A  is a  d \u00d7  d diagonal matrix       D(m), where the elements of            m   are set to 1 where data\n(pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that\nthe first  k  coordinates are known.\n    The rest of this section contains proofs of all the theorems and propositions presented in the main\nbody of the paper.         For clarity, we restate the theorems more formally with precise mathematical\ndetails.\nA.1      Proof of Theorem 3.4\nTheorem A.1           (Posterior Sampling using Diffusion in Pixel Space).                     Suppose     Assumption 3.1\nand   Assumption 3.2           hold.   Let us denote by       \u03c3  =   {\u03c3  } k    the singular values of       (AS)   T (AS), i.e.\n                                                                        j  j=1\n(AS)  T (AS) =     U\u03a3V    T  :=  U D(\u03c3)V     T ,U   \u2208  R k\u00d7k , V  \u2208  R k\u00d7k  and\n                           \u2217                      [ \u2225   ( \u2212\u2192  \u2212\u2192   \u2212\u2192   \u2212\u2192 )        (\u2212\u2192  (\u2212\u2192   \u2212\u2192 ))\u22252 ]\n                                            \u2212\u2192 \u2212    \u2225                                               \u2225\n                         \u03b8   = arg minE        \u2192     \u02dc\n           \u2212\u2192         \u2212\u2192              \u03b8     x0, \u03f5    \u03bc 1  x 1(x 0,\u2212\u2192\u03f5 ),x 0  \u2212  \u03bc \u03b8  x 1  x 0, \u03f5         .\nSuppose    x 0  \u223c  p(x  0). Given measurements           y  =  Ax   0 and a fixed variance        \u03b2  \u2208  (0, 1), there exists a\n                    5                                    T           j            k                                      \u2212\u2192\nmatrix step size       \u03b6 = (1/2)(SU)D(\u03b6         i)(SU)     ,\u03b6 i=  {\u03b6 i  = 1/\u03c3  j }j=1  for all the coordinates of        x0  such\n                                                                   \u2212\u2192\nthat  Algorithm 1       samples from the true posterior          p(x 0|y)  and exactly recovers the groundtruth sample,\n      \u2190\u2212     \u2212\u2192\ni.e., x 0 =  x 0.                             \u2190\u2212      \u2212\u2192           \u2190\u2212\n    Proof.    Our goal is to show that        x 0 =   x 0, where   x 0  is returned by     Algorithm 1. Recall that the\n                                   \u2190\u2212\nreverse process starts with        x 1 \u223c N    (0, Id ) and generates the following:\n                              \u2190\u2212       \u2217\u2190\u2212             \u2225   \u2190\u2212  \u2190\u2212        \u22252\n                              x  0=   \u03b8  x 1 \u2212 \u03b6\u2207   \u2190\u2212 \u2225Ax   0(x 1) \u2212  y \u2225\n                                                    x1 \u2225                 \u222522\n                                       \u2217 \u2190\u2212            \u2225       T \u2190\u2212      \u2225\n                                  =   \u03b8  x 1 \u2212 \u03b6\u2207\u2190\u2212 x1 \u2225ASS      x 1 \u2212  y\u2225 2\n                                          T\u2190\u2212         (       T )T  (       T \u2190\u2212      )\n                                  =   SS   x  1\u2212  2\u03b6    ASS          ASS      x1 \u2212  y\n                                          T\u2190\u2212             T   T (       T \u2190\u2212      )\n                                  =   SS   x  1\u2212  2\u03b6SS     A      ASS     x 1 \u2212  y\n                                          T\u2190\u2212             T   T       T \u2190\u2212            T   T\n                                  =   SS   x  1\u2212  2\u03b6SS     A   ASS      x1  + 2\u03b6SS      A   y\n                                          T\u2190\u2212             T   T       T \u2190\u2212            T   T   \u2212\u2192\n                                  =   SS   x  1\u2212  2\u03b6SS     A   ASS      x1  + 2\u03b6SS      A   Ax  0\n                                          T\u2190\u2212             T   T       T \u2190\u2212            T   T     \u2212\u2192\n                                  =   SS   x  1\u2212  2\u03b6SS     A   ASS      x1  + 2\u03b6SS      A   ASz   0.\nNow, we use the singular value decomposition of                 (AS)   T (AS)   with left singular vectors in        U   \u2208  R k\u00d7k ,\nright singular vectors in      V  \u2208  R k\u00d7k  , and singular values      \u03c3 = [\u03c3  1, . . . ,k\u03c3]in \u03a3 =   D(\u03c3). Thus, the above\n   5We use the term \u2018step size\u2019 in a more general way than is normally used.               In this case, the step size is a \u2018pre-\nconditioning\u2019 positive definite matrix, whose eigenvalue magnitudes correspond to the scalar step sizes per coordinate\nalong an appropriately rotated basis.      This general form is needed and with carefully selected (unique) eigenvalues;\notherwise the DPS algorithm fails to converge to the groundtruth sample. We will later see that for our PSLD Algorithm\nin Theorem 3.8, we can revert to the commonly used notion of step size (a single scalar), as any finite step size (including\na single scalar common across all coordinates) suffices for proving recovery.\n                                                                17", "md": "# Technical Proofs\n\n## Technical Proofs\n\nNotation and Measurement Matrix. We elaborate on the structure of the measurement matrix $$A \\in \\mathbb{R}^{l \\times d}$$. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the $$d \\times d$$ identity matrix (the rows that are present correspond to the indices of the selected pixels from the image $$x_0 \\in \\mathbb{R}^d$$). Given this structure, it immediately follows that $$A^TA$$ is a $$d \\times d$$ matrix that has the interpretation of a pixel selection mask. Specifically, $$A^TA$$ is a $$d \\times d$$ diagonal matrix $$D(m)$$, where the elements of $$m$$ are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first $$k$$ coordinates are known.\n\nThe rest of this section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details.\n\n### Proof of Theorem 3.4\n\nTheorem A.1 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma = \\{\\sigma_j\\}_{j=1}^k$$ the singular values of $$(AS)^T(AS)$$, i.e. $$(AS)^T(AS) = U\\Sigma V^T := UD(\\sigma)V^T, U \\in \\mathbb{R}^{k \\times k}, V \\in \\mathbb{R}^{k \\times k}$$ and\n\n$$\\theta = \\arg \\min_{\\theta} E\\left[\\|x_0 - \\theta x_1\\|^2\\right]$$\nSuppose $$x_0 \\sim p(x_0)$$. Given measurements $$y = Ax_0$$ and a fixed variance $$\\beta \\in (0, 1)$$, there exists a matrix step size $$\\zeta = (1/2)(SU)D(\\zeta_i)(SU)$$, $$\\zeta_i = \\{1/\\sigma_j\\}_{j=1}$$ for all the coordinates of $$x_0$$ such that Algorithm 1 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nProof. Our goal is to show that $$x_0 = x_0$$, where $$x_0$$ is returned by Algorithm 1. Recall that the reverse process starts with $$x_1 \\sim \\mathcal{N}(0, I_d)$$ and generates the following:\n\n$$\\begin{align*}\nx_0 &= \\theta x_1 - \\zeta \\nabla \\left\\|Ax_0(x_1) - y\\right\\|^2 \\\\\n&= \\theta x_1 - \\zeta \\nabla x_1 \\left\\|ASSx_1 - y\\right\\|^2 \\\\\n&= SSx_1 - 2\\zeta ASS^T ASSx_1 - y \\\\\n&= SSx_1 - 2\\zeta SSA^T ASSx_1 - y \\\\\n&= SSx_1 - 2\\zeta SSA^T Ax_0.\n\\end{align*}$$\nNow, we use the singular value decomposition of $$(AS)^T(AS)$$ with left singular vectors in $$U \\in \\mathbb{R}^{k \\times k}$$, right singular vectors in $$V \\in \\mathbb{R}^{k \\times k}$$, and singular values $$\\sigma = [\\sigma_1, ..., k\\sigma]$$ in $$\\Sigma = D(\\sigma)$$. Thus, the above...", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Technical Proofs", "md": "# Technical Proofs"}, {"type": "heading", "lvl": 2, "value": "Technical Proofs", "md": "## Technical Proofs"}, {"type": "text", "value": "Notation and Measurement Matrix. We elaborate on the structure of the measurement matrix $$A \\in \\mathbb{R}^{l \\times d}$$. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the $$d \\times d$$ identity matrix (the rows that are present correspond to the indices of the selected pixels from the image $$x_0 \\in \\mathbb{R}^d$$). Given this structure, it immediately follows that $$A^TA$$ is a $$d \\times d$$ matrix that has the interpretation of a pixel selection mask. Specifically, $$A^TA$$ is a $$d \\times d$$ diagonal matrix $$D(m)$$, where the elements of $$m$$ are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first $$k$$ coordinates are known.\n\nThe rest of this section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details.", "md": "Notation and Measurement Matrix. We elaborate on the structure of the measurement matrix $$A \\in \\mathbb{R}^{l \\times d}$$. In our setting, we are considering linear inverse problems. Thus, this matrix is a pixel selector and consists of a subset of the rows from the $$d \\times d$$ identity matrix (the rows that are present correspond to the indices of the selected pixels from the image $$x_0 \\in \\mathbb{R}^d$$). Given this structure, it immediately follows that $$A^TA$$ is a $$d \\times d$$ matrix that has the interpretation of a pixel selection mask. Specifically, $$A^TA$$ is a $$d \\times d$$ diagonal matrix $$D(m)$$, where the elements of $$m$$ are set to 1 where data (pixel) is observed and 0 where data (pixel) is masked. Without the loss of generality, we suppose that the first $$k$$ coordinates are known.\n\nThe rest of this section contains proofs of all the theorems and propositions presented in the main body of the paper. For clarity, we restate the theorems more formally with precise mathematical details."}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 3.4", "md": "### Proof of Theorem 3.4"}, {"type": "text", "value": "Theorem A.1 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma = \\{\\sigma_j\\}_{j=1}^k$$ the singular values of $$(AS)^T(AS)$$, i.e. $$(AS)^T(AS) = U\\Sigma V^T := UD(\\sigma)V^T, U \\in \\mathbb{R}^{k \\times k}, V \\in \\mathbb{R}^{k \\times k}$$ and\n\n$$\\theta = \\arg \\min_{\\theta} E\\left[\\|x_0 - \\theta x_1\\|^2\\right]$$\nSuppose $$x_0 \\sim p(x_0)$$. Given measurements $$y = Ax_0$$ and a fixed variance $$\\beta \\in (0, 1)$$, there exists a matrix step size $$\\zeta = (1/2)(SU)D(\\zeta_i)(SU)$$, $$\\zeta_i = \\{1/\\sigma_j\\}_{j=1}$$ for all the coordinates of $$x_0$$ such that Algorithm 1 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nProof. Our goal is to show that $$x_0 = x_0$$, where $$x_0$$ is returned by Algorithm 1. Recall that the reverse process starts with $$x_1 \\sim \\mathcal{N}(0, I_d)$$ and generates the following:\n\n$$\\begin{align*}\nx_0 &= \\theta x_1 - \\zeta \\nabla \\left\\|Ax_0(x_1) - y\\right\\|^2 \\\\\n&= \\theta x_1 - \\zeta \\nabla x_1 \\left\\|ASSx_1 - y\\right\\|^2 \\\\\n&= SSx_1 - 2\\zeta ASS^T ASSx_1 - y \\\\\n&= SSx_1 - 2\\zeta SSA^T ASSx_1 - y \\\\\n&= SSx_1 - 2\\zeta SSA^T Ax_0.\n\\end{align*}$$\nNow, we use the singular value decomposition of $$(AS)^T(AS)$$ with left singular vectors in $$U \\in \\mathbb{R}^{k \\times k}$$, right singular vectors in $$V \\in \\mathbb{R}^{k \\times k}$$, and singular values $$\\sigma = [\\sigma_1, ..., k\\sigma]$$ in $$\\Sigma = D(\\sigma)$$. Thus, the above...", "md": "Theorem A.1 (Posterior Sampling using Diffusion in Pixel Space). Suppose Assumption 3.1 and Assumption 3.2 hold. Let us denote by $$\\sigma = \\{\\sigma_j\\}_{j=1}^k$$ the singular values of $$(AS)^T(AS)$$, i.e. $$(AS)^T(AS) = U\\Sigma V^T := UD(\\sigma)V^T, U \\in \\mathbb{R}^{k \\times k}, V \\in \\mathbb{R}^{k \\times k}$$ and\n\n$$\\theta = \\arg \\min_{\\theta} E\\left[\\|x_0 - \\theta x_1\\|^2\\right]$$\nSuppose $$x_0 \\sim p(x_0)$$. Given measurements $$y = Ax_0$$ and a fixed variance $$\\beta \\in (0, 1)$$, there exists a matrix step size $$\\zeta = (1/2)(SU)D(\\zeta_i)(SU)$$, $$\\zeta_i = \\{1/\\sigma_j\\}_{j=1}$$ for all the coordinates of $$x_0$$ such that Algorithm 1 samples from the true posterior $$p(x_0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x_0 = x_0$$.\n\nProof. Our goal is to show that $$x_0 = x_0$$, where $$x_0$$ is returned by Algorithm 1. Recall that the reverse process starts with $$x_1 \\sim \\mathcal{N}(0, I_d)$$ and generates the following:\n\n$$\\begin{align*}\nx_0 &= \\theta x_1 - \\zeta \\nabla \\left\\|Ax_0(x_1) - y\\right\\|^2 \\\\\n&= \\theta x_1 - \\zeta \\nabla x_1 \\left\\|ASSx_1 - y\\right\\|^2 \\\\\n&= SSx_1 - 2\\zeta ASS^T ASSx_1 - y \\\\\n&= SSx_1 - 2\\zeta SSA^T ASSx_1 - y \\\\\n&= SSx_1 - 2\\zeta SSA^T Ax_0.\n\\end{align*}$$\nNow, we use the singular value decomposition of $$(AS)^T(AS)$$ with left singular vectors in $$U \\in \\mathbb{R}^{k \\times k}$$, right singular vectors in $$V \\in \\mathbb{R}^{k \\times k}$$, and singular values $$\\sigma = [\\sigma_1, ..., k\\sigma]$$ in $$\\Sigma = D(\\sigma)$$. Thus, the above..."}]}, {"page": 18, "text": "expression becomes\n            \u2190\u2212        T \u2190\u2212                T  T \u2190\u2212                T \u2212\n                                                                   \u2192\n            x 0 = SS    x 1\u2212  2\u03b6SU\u03a3V        S  x 1 + 2\u03b6SU\u03a3V        z0\n                      T \u2190\u2212                T  T \u2190\u2212                T \u2212\n                                                                   \u2192\n                = SS    x 1\u2212  2\u03b6SU\u03a3V        S  x 1 + 2\u03b6SU\u03a3V        z0\n                      T \u2190\u2212                         T          T  T \u2190\u2212                          T         T \u2212\n                                                                                                           \u2192\n                = SS    x 1\u2212  2(SU)D(\u03b6     i)(SU)    SU\u03a3V       S  x 1 + 2(SU)D(\u03b6     i)(SU)    SU\u03a3V       z0\n                (i)     \u2190\u2212                                         \u2190\u2212                                      \u2212\n                      T                         T  T          T  T                         T  T          T \u2192\n                =  SS   x 1 \u2212 2(SU)D(\u03b6     i)U   S   SU\u03a3V       S  x 1 + 2(SU)D(\u03b6     i)U   S   SU\u03a3V       z0\n                (ii)   T\u2190\u2212                      T        T  T \u2190\u2212                      T       T \u2212\n                                                                                                \u2192\n                =  SS    x 1\u2212  2(SU)D(\u03b6     i)U   U\u03a3U      S  x 1 + 2(SU)D(\u03b6     i)U   U\u03a3U      z 0\n                      T \u2190\u2212                       T   T \u2190\u2212                       T \u2212\n                                                                                  \u2192\n                = SS    x 1\u2212  2(SU)D(\u03b6     i)\u03a3U    S   x1 + 2(SU)D(\u03b6      i)\u03a3U    z0\n                      T \u2190\u2212                          T   T\u2190\u2212                          T \u2212\n                                                                                       \u2192\n                = SS    x 1\u2212  2SU D(\u03b6    i)D(\u03c3)U      S  x 1 + 2SU D(\u03b6    i)D(\u03c3)U      z 0\n                      T \u2190\u2212                         T  T\u2190\u2212                          T\u2212\u2192\n                = SS    x 1\u2212  2SU D(\u03b6    i\u2299  \u03c3)U    S   x 1+ 2SU D(\u03b6     i\u2299  \u03c3)U    z 0,\nwhere (i) is due to   Assumption 3.1        and (ii) uses  Assumption 3.2. By choosing           \u03b6j as half the inverse\n                                                                                                  i\nof the non-zero singular values of       (AS)  T (AS), i.e.,  \u03b6j = 1/2\u03c3  i \u2200i = 1, . . . , k, we obtain\n                                     \u2190\u2212        T\u2190\u2212            Ti T \u2190\u2212           T \u2212\u2192\n                                     x 0=  SS    x 1\u2212 SU U      S  x 1 + SU U     z0\n                                               T\u2190\u2212         T\u2190\u2212       \u2212\n                                                                     \u2192     \u2212\u2192\n                                        =  SS    x 1\u2212 SS    x 1 +Sz   0 =  x 0,\nwhich completes the statement of the theorem.                                                                          \u25a1\nA.2      Proof of Proposition 3.5\nProposition A.2         (Variational Autoencoder).         Suppose    Assumption 3.1         holds.   For an encoder\nE  :R d \u2192   R k and a decoder    D  : R k \u2192  R d , denote by   L(\u03d5, \u03c9)   the training objective of VAE:\n                                               [\u2225      \u2212\u2192             \u2212\u2192 \u22252]\n                 arg minL(\u03d5, \u03c9) :=     E \u2212\u2192     \u2225D(E(x   0;\u03d5);\u03c9)   \u2212 x 0 \u2225   +  \u03bbKL   (E\u266fp,N   (0,I k )),\n                      \u03d5,\u03c9       \u2212\u2192       x0\u223cpT\u2212\u2192           \u2190\u2212          \u2190\u2212 2\nthen the combination of      E(x  0;\u03d5) =   S  x 0 and  D(z  0 ;\u03c9) =  Sz  0 is a minimizer of    L  (\u03d5, \u03c9).\n                                           \u2212\u2192          T\u2212\u2192                         \u2190\u2212           \u2190\u2212\nProof.   To show that the encoder       E(x 0 ;\u03d5) =  S  x 0 and the decoder     D(z  0;\u03c9) =   Sz 0 minimize the VAE\ntraining objective    L(\u03d5, \u03c9), we begin with the first part of the loss, which is also called           reconstruction\n                                          \u2212\u2192          T \u2212\u2192           \u2190\u2212          \u2190\u2212\nerror  L recon (\u03d5, \u03c9). Substituting    E(x  0;\u03d5) =   S  x 0 and  D(z  0;\u03c9) =   Sz  0, we have\n                                                           [\u2225       \u2212\u2192            \u2212\u2192 \u22252]\n                                 L recon (\u03d5, \u03c9) :=  E\u2212\u2192     \u2225 D(E(x  0 ;\u03d5);\u03c9)  \u2212 x  0\u2225\n                                                     x 0\u223cp[ \u2225                    \u22252 ] 2\n                                                            \u2225     T\u2212\u2192         \u2212\u2192 \u2225\n                                                =  E\u2212\u2192      \u2225D(S    x 0;\u03c9)  \u2212 x 0\u2225\n                                                     x0\u223cp [ \u2225               \u22252]    2\n                                                            \u2225    T \u2212\u2192    \u2212\u2192\u2225\n                                                =  E\u2212\u2192      \u2225SS    x0 \u2212 x  0\u2225\n                        \u2212\u2192                           x0\u223cp                    2\nUsing the fact that     x 0 lives in a linear subspace, we arrive at\n                                                          [\u2225                   \u22252]\n                                                           \u2225        \u2212\u2192      \u2212\n                                L       (\u03d5, \u03c9) =  E\u2212\u2192      \u2225SS  T Sz   \u2212 Sz \u2192\u2225 \u2225\n                                  recon             x0\u223cp             0       0  2\n                                               (i)              [ \u2225  \u2212\u2192      \u2212\u2192 \u22252]\n                                                    \u2212             \u2225             \u2225\n                                               =  E \u2192              Sz 0 \u2212 Sz   0    = 0,\n                                                    z0 \u223cN(0,I k)                 2\nwhere (i) is due to      Assumption 3.1.         Now, we analyze the distribution loss.            Note that the KL-\ndivergence between two Gaussian distributions with moments                 (\u03bc 1, \u03c31) and   (\u03bc2, \u03c32) is given by\n                                                              (  \u03c32 )    \u03c3 2 + (\u03bc 1 \u2212\u03bc  2)2    1\n                        KL(N    (\u03bc 1, \u03c31),N  (\u03bc 2, \u03c32)) = log    \u03c3    +    1    2\u03c3 2        \u2212  2 .\n                                                           18     1                2", "md": "expression becomes\n$$\n\\begin{aligned}\nx_0 &= SSx_1 - 2\\zeta SU\\Sigma V Sx_1 + 2\\zeta SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)(SU)SU\\Sigma V Sx_1 + 2(SU)D(\\zeta_i)(SU)SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)U SU\\Sigma V Sx_1 + 2(SU)D(\\zeta_i)U SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)\\Sigma U Sx_1 + 2(SU)D(\\zeta_i)\\Sigma U z_0 \\\\\n&= SSx_1 - 2SUD(\\zeta_i)D(\\sigma)U Sx_1 + 2SUD(\\zeta_i)D(\\sigma)U z_0 \\\\\n&= SSx_1 - 2SUD(\\zeta_i \\odot \\sigma)U Sx_1 + 2SUD(\\zeta_i \\odot \\sigma)U z_0,\n\\end{aligned}\n$$\nwhere (i) is due to Assumption 3.1 and (ii) uses Assumption 3.2. By choosing $\\zeta_j$ as half the inverse of the non-zero singular values of $(AS)^T(AS)$, i.e., $\\zeta_j = 1/2\\sigma_i$ for all $i = 1, ..., k$, we obtain\n$$\n\\begin{aligned}\nx_0 &= SSx_1 - SUU Sx_1 + SUU z_0 \\\\\n&= SSx_1 - SSx_1 + Sz_0 = x_0,\n\\end{aligned}\n$$\nwhich completes the statement of the theorem. \u25a1\n\n### Proof of Proposition 3.5\n\n**Proposition A.2 (Variational Autoencoder):** Suppose Assumption 3.1 holds. For an encoder $E: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ and a decoder $D: \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$, denote by $L(\\phi, \\omega)$ the training objective of VAE:\n$$\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{\\rightarrow x_0 \\sim p_T} \\left\\| D(E(x_0;\\phi);\\omega) - x_0 \\right\\| + \\lambda \\text{KL} \\left( E_{\\#p_N}(0, I_k) \\right),\n$$\nthen the combination of $E(x_0;\\phi) = Sx_0$ and $D(z_0;\\omega) = Sz_0$ is a minimizer of $L(\\phi, \\omega)$.\n\n**Proof:** To show that the encoder $E(x_0;\\phi) = Sx_0$ and the decoder $D(z_0;\\omega) = Sz_0$ minimize the VAE training objective $L(\\phi, \\omega)$, we begin with the first part of the loss, which is also called reconstruction error $L_{\\text{recon}}(\\phi, \\omega)$. Substituting $E(x_0;\\phi) = Sx_0$ and $D(z_0;\\omega) = Sz_0$, we have\n$$\n\\begin{aligned}\nL_{\\text{recon}}(\\phi, \\omega) &:= E_{\\rightarrow x_0 \\sim p} \\left\\| D(E(x_0;\\phi);\\omega) - x_0 \\right\\| \\\\\n&= E_{\\rightarrow x_0 \\sim p} \\left\\| D(Sx_0;\\omega) - x_0 \\right\\| \\\\\n&= E_{\\rightarrow x_0 \\sim p} \\left\\| SSx_0 - x_0 \\right\\| \\\\\n&= E_{x_0 \\sim p} \\left\\| SS^T Sz_0 - Sz_0 \\right\\| = 0,\n\\end{aligned}\n$$\nwhere (i) is due to Assumption 3.1. Now, we analyze the distribution loss. Note that the KL-divergence between two Gaussian distributions with moments $(\\mu_1, \\sigma_1)$ and $(\\mu_2, \\sigma_2)$ is given by\n$$\n\\text{KL}(N(\\mu_1, \\sigma_1), N(\\mu_2, \\sigma_2)) = \\log \\left( \\frac{\\sigma_2}{\\sigma_1} + \\frac{(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\right).\n$$", "images": [], "items": [{"type": "text", "value": "expression becomes\n$$\n\\begin{aligned}\nx_0 &= SSx_1 - 2\\zeta SU\\Sigma V Sx_1 + 2\\zeta SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)(SU)SU\\Sigma V Sx_1 + 2(SU)D(\\zeta_i)(SU)SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)U SU\\Sigma V Sx_1 + 2(SU)D(\\zeta_i)U SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)\\Sigma U Sx_1 + 2(SU)D(\\zeta_i)\\Sigma U z_0 \\\\\n&= SSx_1 - 2SUD(\\zeta_i)D(\\sigma)U Sx_1 + 2SUD(\\zeta_i)D(\\sigma)U z_0 \\\\\n&= SSx_1 - 2SUD(\\zeta_i \\odot \\sigma)U Sx_1 + 2SUD(\\zeta_i \\odot \\sigma)U z_0,\n\\end{aligned}\n$$\nwhere (i) is due to Assumption 3.1 and (ii) uses Assumption 3.2. By choosing $\\zeta_j$ as half the inverse of the non-zero singular values of $(AS)^T(AS)$, i.e., $\\zeta_j = 1/2\\sigma_i$ for all $i = 1, ..., k$, we obtain\n$$\n\\begin{aligned}\nx_0 &= SSx_1 - SUU Sx_1 + SUU z_0 \\\\\n&= SSx_1 - SSx_1 + Sz_0 = x_0,\n\\end{aligned}\n$$\nwhich completes the statement of the theorem. \u25a1", "md": "expression becomes\n$$\n\\begin{aligned}\nx_0 &= SSx_1 - 2\\zeta SU\\Sigma V Sx_1 + 2\\zeta SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)(SU)SU\\Sigma V Sx_1 + 2(SU)D(\\zeta_i)(SU)SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)U SU\\Sigma V Sx_1 + 2(SU)D(\\zeta_i)U SU\\Sigma V z_0 \\\\\n&= SSx_1 - 2(SU)D(\\zeta_i)\\Sigma U Sx_1 + 2(SU)D(\\zeta_i)\\Sigma U z_0 \\\\\n&= SSx_1 - 2SUD(\\zeta_i)D(\\sigma)U Sx_1 + 2SUD(\\zeta_i)D(\\sigma)U z_0 \\\\\n&= SSx_1 - 2SUD(\\zeta_i \\odot \\sigma)U Sx_1 + 2SUD(\\zeta_i \\odot \\sigma)U z_0,\n\\end{aligned}\n$$\nwhere (i) is due to Assumption 3.1 and (ii) uses Assumption 3.2. By choosing $\\zeta_j$ as half the inverse of the non-zero singular values of $(AS)^T(AS)$, i.e., $\\zeta_j = 1/2\\sigma_i$ for all $i = 1, ..., k$, we obtain\n$$\n\\begin{aligned}\nx_0 &= SSx_1 - SUU Sx_1 + SUU z_0 \\\\\n&= SSx_1 - SSx_1 + Sz_0 = x_0,\n\\end{aligned}\n$$\nwhich completes the statement of the theorem. \u25a1"}, {"type": "heading", "lvl": 3, "value": "Proof of Proposition 3.5", "md": "### Proof of Proposition 3.5"}, {"type": "text", "value": "**Proposition A.2 (Variational Autoencoder):** Suppose Assumption 3.1 holds. For an encoder $E: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ and a decoder $D: \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$, denote by $L(\\phi, \\omega)$ the training objective of VAE:\n$$\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{\\rightarrow x_0 \\sim p_T} \\left\\| D(E(x_0;\\phi);\\omega) - x_0 \\right\\| + \\lambda \\text{KL} \\left( E_{\\#p_N}(0, I_k) \\right),\n$$\nthen the combination of $E(x_0;\\phi) = Sx_0$ and $D(z_0;\\omega) = Sz_0$ is a minimizer of $L(\\phi, \\omega)$.\n\n**Proof:** To show that the encoder $E(x_0;\\phi) = Sx_0$ and the decoder $D(z_0;\\omega) = Sz_0$ minimize the VAE training objective $L(\\phi, \\omega)$, we begin with the first part of the loss, which is also called reconstruction error $L_{\\text{recon}}(\\phi, \\omega)$. Substituting $E(x_0;\\phi) = Sx_0$ and $D(z_0;\\omega) = Sz_0$, we have\n$$\n\\begin{aligned}\nL_{\\text{recon}}(\\phi, \\omega) &:= E_{\\rightarrow x_0 \\sim p} \\left\\| D(E(x_0;\\phi);\\omega) - x_0 \\right\\| \\\\\n&= E_{\\rightarrow x_0 \\sim p} \\left\\| D(Sx_0;\\omega) - x_0 \\right\\| \\\\\n&= E_{\\rightarrow x_0 \\sim p} \\left\\| SSx_0 - x_0 \\right\\| \\\\\n&= E_{x_0 \\sim p} \\left\\| SS^T Sz_0 - Sz_0 \\right\\| = 0,\n\\end{aligned}\n$$\nwhere (i) is due to Assumption 3.1. Now, we analyze the distribution loss. Note that the KL-divergence between two Gaussian distributions with moments $(\\mu_1, \\sigma_1)$ and $(\\mu_2, \\sigma_2)$ is given by\n$$\n\\text{KL}(N(\\mu_1, \\sigma_1), N(\\mu_2, \\sigma_2)) = \\log \\left( \\frac{\\sigma_2}{\\sigma_1} + \\frac{(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\right).\n$$", "md": "**Proposition A.2 (Variational Autoencoder):** Suppose Assumption 3.1 holds. For an encoder $E: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ and a decoder $D: \\mathbb{R}^k \\rightarrow \\mathbb{R}^d$, denote by $L(\\phi, \\omega)$ the training objective of VAE:\n$$\n\\arg \\min_{\\phi, \\omega} L(\\phi, \\omega) := E_{\\rightarrow x_0 \\sim p_T} \\left\\| D(E(x_0;\\phi);\\omega) - x_0 \\right\\| + \\lambda \\text{KL} \\left( E_{\\#p_N}(0, I_k) \\right),\n$$\nthen the combination of $E(x_0;\\phi) = Sx_0$ and $D(z_0;\\omega) = Sz_0$ is a minimizer of $L(\\phi, \\omega)$.\n\n**Proof:** To show that the encoder $E(x_0;\\phi) = Sx_0$ and the decoder $D(z_0;\\omega) = Sz_0$ minimize the VAE training objective $L(\\phi, \\omega)$, we begin with the first part of the loss, which is also called reconstruction error $L_{\\text{recon}}(\\phi, \\omega)$. Substituting $E(x_0;\\phi) = Sx_0$ and $D(z_0;\\omega) = Sz_0$, we have\n$$\n\\begin{aligned}\nL_{\\text{recon}}(\\phi, \\omega) &:= E_{\\rightarrow x_0 \\sim p} \\left\\| D(E(x_0;\\phi);\\omega) - x_0 \\right\\| \\\\\n&= E_{\\rightarrow x_0 \\sim p} \\left\\| D(Sx_0;\\omega) - x_0 \\right\\| \\\\\n&= E_{\\rightarrow x_0 \\sim p} \\left\\| SSx_0 - x_0 \\right\\| \\\\\n&= E_{x_0 \\sim p} \\left\\| SS^T Sz_0 - Sz_0 \\right\\| = 0,\n\\end{aligned}\n$$\nwhere (i) is due to Assumption 3.1. Now, we analyze the distribution loss. Note that the KL-divergence between two Gaussian distributions with moments $(\\mu_1, \\sigma_1)$ and $(\\mu_2, \\sigma_2)$ is given by\n$$\n\\text{KL}(N(\\mu_1, \\sigma_1), N(\\mu_2, \\sigma_2)) = \\log \\left( \\frac{\\sigma_2}{\\sigma_1} + \\frac{(\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\right).\n$$"}]}, {"page": 19, "text": "Since  E (x 0) =  S T x 0=   ST Sz 0 =  z 0, the distribution loss becomes:\n                       L dist (\u03d5) :=  KL(E\u266fp,N      (0,I k)) =  KL(N(0,I      k), N (0, I k)) = 0.\nA.3     Proof of Theorem 3.6\nTheorem A.3         (Generative Modeling using Diffusion in Latent Space).                Suppose   Assumption 3.1\nholds. Let the optimal solution of the latent diffusion model be\n                         \u2217                     [\u2225    (\u2212                )       (   (       ))\u22252 ]\n                                                      \u2192   \u2212\u2192   \u2212\n                                                               \u2192    \u2212\u2192          \u2212\u2192  \u2212\u2192   \u2212\n                        \u03b8  = arg min    E\u2212\u2192\u2212\u2192   \u2225 \u02dc                                      \u2192    \u2225\n                                    \u03b8    z 0,\u03f5    \u03bc1  z 1(z 0, \u03f5 ), z0   \u2212 \u03bc \u03b8  z1  z 0, \u03f5        .\n                                            ( \u2212  (       ))           (        )\n                                              \u2192   \u2212\n                                                  \u2192   \u2212\u2192           \u2212\u2192  \u2212\u2192   \u2212\u2192\nFor a fixed variance       \u03b2  >   0, if  \u03bc\u03b8   z1  z 0, \u03f5     :=  \u03b8z  1 z 0 ,\u03f5   , then the closed-form solution is\n  \u2217   \u221a                                                \u221a  1                                              (\u2190\u2212    )       \u2190\u2212\n\u03b8   =    1 \u2212  \u03b2I k, which after normalization by         1\u2212\u03b2  and composition with the decoder         D  z 0;\u03c9   :=  Sz 0\n                                     (\u2212\u2192 )\nrecovers the true subspace of      p  x 0 .\n    Proof.   In latent diffusion models, the training is performed in the latent space of a pre-trained\nVAE. If the VAE is chosen from          Proposition 3.5, then the training objective becomes:\n                               [\u2225    \u2212   (                    )       (    (           ))\u22252 ]\n                  minE   \u2212\u2192\u2212\u2192   \u2225 \u02dc  \u2192       \u2212\u2192   \u2212\u2192      \u2212\u2192           \u2212\u2192      \u2212\u2192   \u2212\u2192   \u2225\n                         x0, \u03f5   \u03bc 1(z 1  E(x  0), \u03f5 ),E(x  0)  \u2212  \u03bc\u03b8  z 1  E(x  0), \u03f5\n                    \u03b8             [\u2225     \u2212\u2192 (\u2212\u2192   \u2212       )       (   (        ))\u22252]\n                             \u2212     \u2225              \u2192    \u2212\u2192          \u2212\u2192  \u2212\u2192   \u2212\u2192   \u2225\n                        =  E \u2192 \u2212\u2192    \u02dc\n                             z0,\u03f5    \u03bc1 (z1   z0, \u03f5 ), z0   \u2212 \u03bc \u03b8  z1  z 0, \u03f5\n                                  [\u2225 \u2212         (   (        ))\u22252]            [\u2225            (       )\u2225 2]\n                                     \u2192          \u2212\u2192  \u2212\u2192  \u2212\u2192                      \u2212\n                                                                                \u2192       \u2212\n                                                                                        \u2192   \u2212\n                             \u2212     \u2225                          \u2225               \u2225             \u2192   \u2212\u2192  \u2225\n                             \u2192 \u2212\u2192                                      \u2212\n                                                                       \u2192  \u2212\n                        =  E         z0 \u2212  \u03bc \u03b8  z1  z 0, \u03f5        =  E    \u2192     z0 \u2212  \u03b8z 1  z 0, \u03f5\n                             z0,\u03f5                                      z0 ,\u03f5\n                                  [ \u2225        (    \u221a           \u221a      )\u2225 2 ]\n                             \u2212      \u2225\u2212\u2192        \u2212\u2192                 \u2212\u2192   \u2225\n                        =  E \u2192 \u2212\u2192   \u2225z 0\u2212  \u03b8   z 0  1 \u2212  \u03b2  +    \u03b2 \u03f5   \u2225\n                             z0,\u03f5      [\u2211k   (\u2212\u2192       T ( \u2212\u2192 \u221a           \u2212\u2192 \u221a   )) 2]\n                        =     \u2212 E             z 0,i\u2212 \u03b8 i   z0   1 \u2212  \u03b2 +   \u03f5    \u03b2       ,\n                              \u2192\n                           \u2212  z0\u223cp      i=1\n                           \u2192\u03f5\u223cN(0,I k )\nwhere   \u03b8T  denotes the    ith row of matrix    \u03b8. The solution of this regression problem is given by           6\n         i\n\u03b8i\u2217 = xE0,\u03f5[( z0 \u221a 1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2 ) (z 0\u221a 1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2 )T ]\u22121  Ex 0,\u03f5[ z0,i(z 0\u221a  1 \u2212  \u03b2 +  \u03f5\u221a \u03b2 )]\n    = xE0,\u03f5[( z0 \u221a 1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2 ) (z 0\u221a 1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2 )T ]\u22121  Ex 0,\u03f5[ z0,i(z 0\u221a  1 \u2212  \u03b2 +  \u03f5\u221a \u03b2 )]\n    = xE0,\u03f5[( E(x 0 )\u221a 1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2 ) (E(x  0)\u221a  1 \u2212 \u03b2  +  \u03f5\u221a \u03b2 ) T] \u22121 E x0,\u03f5 [E(x 0 )i( E(x 0)\u221a  1 \u2212\u03b2   +  \u03f5\u221a  \u03b2)]\n    = zE0,\u03f5[( E(Sz  0)\u221a  1 \u2212  \u03b2 +  \u03f5\u221a \u03b2 ) ( E(Sz  0)\u221a  1 \u2212  \u03b2 +  \u03f5\u221a \u03b2 ) T] \u22121 E z0,\u03f5[E(Sz   0)i( E(Sz  0)\u221a  1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2)]\n    = zE0,\u03f5[( S TSz  0\u221a 1\u2212   \u03b2  +\u03f5  \u221a \u03b2 ) (S T Sz 0\u221a  1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2) T ]\u22121 E z0,\u03f5 [(S T Sz 0)i (S T Sz 0\u221a  1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2)]\n   6For ease of notation, we drop the forward arrow in the rest of this proof.\n                                                            19", "md": "Since  $$E(x_0) = STx_0 = STSz_0 = z_0$$, the distribution loss becomes:\n$$\nL_{dist}(\\phi) := KL(E_{\\#p},N(0,I_k)) = KL(N(0,I_k), N(0, I_k)) = 0.\n$$\n\n### Proof of Theorem 3.6\n\n**Theorem A.3** (Generative Modeling using Diffusion in Latent Space). Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n$$\n\\theta^* = \\arg \\min_{\\theta} E[\\|(\\tilde{z}_1 - \\mu_1(z_1(z_0, \\epsilon)), z_0 - \\mu_{\\theta}z_1(z_0, \\epsilon)\\|^2].\n$$\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta}z_1(z_0, \\epsilon) := \\theta z_1(z_0, \\epsilon)$$, then the closed-form solution is\n$$\n\\theta^* = (1 - \\beta I_k)^{-\\frac{1}{2}},\n$$\nwhich after normalization by $$1-\\beta$$ and composition with the decoder $$Dz_0;\\omega := Sz_0$$ recovers the true subspace of $$p x_0$$.\n\n**Proof.** In latent diffusion models, the training is performed in the latent space of a pre-trained VAE. If the VAE is chosen from Proposition 3.5, then the training objective becomes:\n$$\n\\min_{\\theta} E[\\|(\\tilde{z}_1 - \\mu_1(z_1E(x_0), \\epsilon), E(x_0) - \\mu_{\\theta}z_1E(x_0), \\epsilon)\\|^2] = E_{z_0, \\epsilon}[\\|z_0 - \\mu_{\\theta}z_1z_0, \\epsilon\\|^2] = E_{z_0, \\epsilon}[\\|z_0 - \\theta z_1z_0, \\epsilon\\|^2].\n$$\nThe solution of this regression problem is given by\n$$\n\\theta_i^* = \\mathbb{E}_{z_0, \\epsilon} \\left[ \\left( z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}} \\right) \\left( z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}} \\right)^T \\right]^{-1} \\mathbb{E}_{x_0, \\epsilon} [z_{0,i}(z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}})],\n$$\nfor ease of notation, we drop the forward arrow in the rest of this proof.", "images": [], "items": [{"type": "text", "value": "Since  $$E(x_0) = STx_0 = STSz_0 = z_0$$, the distribution loss becomes:\n$$\nL_{dist}(\\phi) := KL(E_{\\#p},N(0,I_k)) = KL(N(0,I_k), N(0, I_k)) = 0.\n$$", "md": "Since  $$E(x_0) = STx_0 = STSz_0 = z_0$$, the distribution loss becomes:\n$$\nL_{dist}(\\phi) := KL(E_{\\#p},N(0,I_k)) = KL(N(0,I_k), N(0, I_k)) = 0.\n$$"}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 3.6", "md": "### Proof of Theorem 3.6"}, {"type": "text", "value": "**Theorem A.3** (Generative Modeling using Diffusion in Latent Space). Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n$$\n\\theta^* = \\arg \\min_{\\theta} E[\\|(\\tilde{z}_1 - \\mu_1(z_1(z_0, \\epsilon)), z_0 - \\mu_{\\theta}z_1(z_0, \\epsilon)\\|^2].\n$$\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta}z_1(z_0, \\epsilon) := \\theta z_1(z_0, \\epsilon)$$, then the closed-form solution is\n$$\n\\theta^* = (1 - \\beta I_k)^{-\\frac{1}{2}},\n$$\nwhich after normalization by $$1-\\beta$$ and composition with the decoder $$Dz_0;\\omega := Sz_0$$ recovers the true subspace of $$p x_0$$.\n\n**Proof.** In latent diffusion models, the training is performed in the latent space of a pre-trained VAE. If the VAE is chosen from Proposition 3.5, then the training objective becomes:\n$$\n\\min_{\\theta} E[\\|(\\tilde{z}_1 - \\mu_1(z_1E(x_0), \\epsilon), E(x_0) - \\mu_{\\theta}z_1E(x_0), \\epsilon)\\|^2] = E_{z_0, \\epsilon}[\\|z_0 - \\mu_{\\theta}z_1z_0, \\epsilon\\|^2] = E_{z_0, \\epsilon}[\\|z_0 - \\theta z_1z_0, \\epsilon\\|^2].\n$$\nThe solution of this regression problem is given by\n$$\n\\theta_i^* = \\mathbb{E}_{z_0, \\epsilon} \\left[ \\left( z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}} \\right) \\left( z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}} \\right)^T \\right]^{-1} \\mathbb{E}_{x_0, \\epsilon} [z_{0,i}(z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}})],\n$$\nfor ease of notation, we drop the forward arrow in the rest of this proof.", "md": "**Theorem A.3** (Generative Modeling using Diffusion in Latent Space). Suppose Assumption 3.1 holds. Let the optimal solution of the latent diffusion model be\n$$\n\\theta^* = \\arg \\min_{\\theta} E[\\|(\\tilde{z}_1 - \\mu_1(z_1(z_0, \\epsilon)), z_0 - \\mu_{\\theta}z_1(z_0, \\epsilon)\\|^2].\n$$\nFor a fixed variance $$\\beta > 0$$, if $$\\mu_{\\theta}z_1(z_0, \\epsilon) := \\theta z_1(z_0, \\epsilon)$$, then the closed-form solution is\n$$\n\\theta^* = (1 - \\beta I_k)^{-\\frac{1}{2}},\n$$\nwhich after normalization by $$1-\\beta$$ and composition with the decoder $$Dz_0;\\omega := Sz_0$$ recovers the true subspace of $$p x_0$$.\n\n**Proof.** In latent diffusion models, the training is performed in the latent space of a pre-trained VAE. If the VAE is chosen from Proposition 3.5, then the training objective becomes:\n$$\n\\min_{\\theta} E[\\|(\\tilde{z}_1 - \\mu_1(z_1E(x_0), \\epsilon), E(x_0) - \\mu_{\\theta}z_1E(x_0), \\epsilon)\\|^2] = E_{z_0, \\epsilon}[\\|z_0 - \\mu_{\\theta}z_1z_0, \\epsilon\\|^2] = E_{z_0, \\epsilon}[\\|z_0 - \\theta z_1z_0, \\epsilon\\|^2].\n$$\nThe solution of this regression problem is given by\n$$\n\\theta_i^* = \\mathbb{E}_{z_0, \\epsilon} \\left[ \\left( z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}} \\right) \\left( z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}} \\right)^T \\right]^{-1} \\mathbb{E}_{x_0, \\epsilon} [z_{0,i}(z_0 \\sqrt{1 - \\beta + \\epsilon \\sqrt{\\beta}})],\n$$\nfor ease of notation, we drop the forward arrow in the rest of this proof."}]}, {"page": 20, "text": "Using   Assumption 3.1, the above expression simplifies to\n\u03b8i\u2217 =  zE,\u03f5[( z0 \u221a 1 \u2212  \u03b2 +  \u03f5 \u221a \u03b2 ) (z 0\u221a  1 \u2212 \u03b2  +  \u03f5\u221a  \u03b2) T ]\u22121 E z0,\u03f5 [(z 0)i( z0\u221a  1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2 )]\n        0\n    =   E  [(1  \u2212\u03b2)z   0z T +  z0\u03f5 T\u221a  \u03b2(1  \u2212  \u03b2) +\u03f5z   T \u221a \u03b2(1  \u2212  \u03b2) +  \u03b2\u03f5\u03f5  T] \u22121 E z ,\u03f5[ (z0 )i(z 0\u221a  1 \u2212  \u03b2 +  \u03f5\u221a  \u03b2)]\n       z0,\u03f5              0                              0                               0\n    =  [( (1 \u2212  \u03b2)  E   [z0z T ] +  E   [z 0\u03f5T ] \u221a \u03b2(1  \u2212  \u03b2) +   E  [ \u03f5z T] \u221a  \u03b2(1 \u2212  \u03b2) +\u03b2     E  [\u03f5\u03f5 T ])] \u22121\n                   z0,\u03f5      0     z 0,\u03f5                         z0,\u03f5    0                      z0,\u03f5\n                                               \u00d7  E z0,\u03f5[ (z0 )i(z 0\u221a  1 \u2212\u03b2   +  \u03f5\u221a  \u03b2 )]\n    =  [( (1 \u2212  \u03b2)I k +  E [z 0]E  [\u03f5]T \u221a \u03b2(1  \u2212  \u03b2) +  E  [\u03f5]E  [z0]T \u221a  \u03b2(1  \u2212 \u03b2) +\u03b2I    k)] \u22121\n                         z0      \u03f5                      [\u03f5    z0(   \u221a             \u221a    )]\n                                               \u00d7  E z0,\u03f5  (z0 )i z 0   1 \u2212\u03b2   +  \u03f5   \u03b2    ,\nwhere the last step uses the fact that         z 0 and   \u03f5 are independent Gaussian random vectors with zero\nmean and unit covariance. Simplifying further, we arrive at\n                             \u2217                        \u22121        [       (   \u221a             \u221a   )]\n                           \u03b8i  = [(1 \u2212  \u03b2)I  k+  \u03b2I k ]   E z0,\u03f5 (z 0)i  z 0   1 \u2212 \u03b2  +  \u03f5  \u03b2\n                               =  Ez 0,\u03f5[(z 0)i( z 0\u221a 1 \u2212\u03b2    + \u03f5 \u221a \u03b2 )]\n                               =  Ez 0[ (z0)iz 0\u221a  1 \u2212 \u03b2 ] +  E z0,\u03f5[ (z0)i\u03f5 \u221a \u03b2 ]\n                               =  Ez 0[ (z0)iz 0\u221a  1 \u2212 \u03b2 ] +  E z0[(z 0)i]E \u03f5 [\u03f5]\u221a  \u03b2.\nThe final step follows from independence of           z 0 and  \u03f5. Since   z 0 and  \u03f5 are also   N  (0, Ik ), we get\n                            \u2217         [         \u221a        ]    [           \u221a                  ]T\n                           \u03b8i  =  Ez 0  (z0)iz 0   1 \u2212 \u03b2   =   0, . . . ,0, 1\u2212   \u03b2, 0, . . . ,0 ,\nwhere the    ith coordinate is   \u221a 1 \u2212  \u03b2  and zero everywhere else. Therefore, stacking all the rows together,\nwe get   \u03b8\u2217 =  \u221a  1 \u2212  \u03b2I  , which after normalization by        1/ \u221a 1 \u2212  \u03b2  gives the desired result.\n                          k   \u2217                                    \u2212\u2192       (\u2212\u2192 )\n    Next, we show that      \u03b8   recovers the true subspace of      x 0 \u223c  p  x 0 . When composed with the decoder\n                                                  \u2190\u2212        ( \u2217 \u2190\u2212)       (   \u2190\u2212 )      \u2190\u2212          \u2190\u2212\nof VAE, the generator of the LDM gives            x 0 =  D   \u03b8  z1   = D   I kz 1  =  Sz  1. Since  z 1 \u223c N   (0, Ik ), this\ncompletes the statement of the theorem.                                                                                   \u25a1\nA.4      Proof of Theorem 3.7\nRecall that the the latent-space GML-DPS (6) algorithm (based on the pixel-space DPS algorithm\n[11]) has three key steps. In the first step, it uses the normalized closed-form solution obtained in\nTheorem 3.6        to perform one step of      denoising    by the reverse SDE. In the second step, it runs one\nstep of gradient descent to satisfy the         measurements      in the pixel space. Finally, it takes one step of\ngradient descent on the      goodness   objective, which acts as a regularizer to ensure that the reconstructed\nimage lies on the data manifold.\n    This can be formalized as:        \u2190\u2212\u2032     \u2217\u2190\u2212            \u2225      \u2190\u2212  \u2190\u2212        \u22252\n                                      z 0 =  \u03b8 z 1 \u2212  \u03b7\u2207\u2190\u2212z1 \u2225AD(z    0(z1 ))\u2212  y \u2225 2;                                   (8)\n                                                      \u2223\u2223                 \u2223\u22232\n                                                      \u2223\u2223\u2190\u2212          \u2190\u2212   \u2223\u2223\n                                      \u2190\u2212              \u2223\u2223 \u2032            \u2032  \u2223\u2223\n                                      z 0 = arg min   \u2223\u2223z0  \u2212 E(D(z   0))\u2223\u2223  ,                                           (9)\n                                                  \u2190\u2212\u2032                      2\n                                                  z0\nIn practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis\nhowever, we analyze the exact system of equations above, as (9) has a closed-form solution in the linear\nsetting.                                                     20", "md": "# Math Equations\n\nUsing Assumption 3.1, the above expression simplifies to\n\n$$\n\\theta_i^* = zE,\\epsilon\\left[(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})^T\\right]^{-1}Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= E\\left[(1 - \\beta)z_0z^T + z_0\\epsilon^T\\sqrt{\\beta}(1 - \\beta) + \\epsilon z^T\\sqrt{\\beta}(1 - \\beta) + \\beta\\epsilon\\epsilon^T\\right]^{-1}Ez,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= \\left[(1 - \\beta)E[z_0z^T] + E[z_0\\epsilon^T]\\sqrt{\\beta}(1 - \\beta) + E[\\epsilon z^T]\\sqrt{\\beta}(1 - \\beta) + \\beta E[\\epsilon\\epsilon^T]\\right]^{-1} \\times Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= \\left[(1 - \\beta)I_k + E[z_0]E[\\epsilon]^T\\sqrt{\\beta}(1 - \\beta) + E[\\epsilon]E[z_0]^T\\sqrt{\\beta}(1 - \\beta) + \\beta I_k\\right]^{-1} \\times Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= Ez_0(z_0)^i z_0\\sqrt{1 - \\beta} + \\epsilon\\beta\n$$\n\n$$\n= Ez_0[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})] + Ez_0[(z_0)^i]E[\\epsilon][\\epsilon]\\sqrt{\\beta}\n$$\n\nThe final step follows from independence of z_0 and \\epsilon. Since z_0 and \\epsilon are also N(0, I_k), we get\n\n$$\n\\theta_i^* = Ez_0(z_0)^i z_0\\sqrt{1 - \\beta} = 0, ..., 0, 1 - \\beta, 0, ..., 0,\n$$\n\nwhere the ith coordinate is \\sqrt{1 - \\beta} and zero everywhere else. Therefore, stacking all the rows together, we get $\\theta^* = \\sqrt{1 - \\beta}I_k$, which after normalization by $\\frac{1}{\\sqrt{1 - \\beta}}$ gives the desired result.\n\nNext, we show that $\\theta$ recovers the true subspace of $x_0 \\sim p_{x_0}$. When composed with the decoder of VAE, the generator of the LDM gives $x_0 = D\\theta z_1 = DI_kz_1 = Sz_1$. Since $z_1 \\sim N(0, I_k)$, this completes the statement of the theorem. $\\blacksquare$\n\n### Proof of Theorem 3.7\n\nRecall that the latent-space GML-DPS (6) algorithm (based on the pixel-space DPS algorithm [11]) has three key steps. In the first step, it uses the normalized closed-form solution obtained in Theorem 3.6 to perform one step of denoising by the reverse SDE. In the second step, it runs one step of gradient descent to satisfy the measurements in the pixel space. Finally, it takes one step of gradient descent on the goodness objective, which acts as a regularizer to ensure that the reconstructed image lies on the data manifold.\n\nThis can be formalized as:\n\n$$\nz_0 = \\theta z_1 - \\eta\\nabla_{z_1}\\left\\|AD(z_0(z_1)) - y\\right\\|_2; \\quad (8)\n$$\n\n$$\nz_0 = \\arg\\min_{z_0}\\left\\|z_0 - E(D(z_0))\\right\\|_2; \\quad (9)\n$$\n\nIn practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis however, we analyze the exact system of equations above, as (9) has a closed-form solution in the linear setting. 20", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Using Assumption 3.1, the above expression simplifies to\n\n$$\n\\theta_i^* = zE,\\epsilon\\left[(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})^T\\right]^{-1}Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= E\\left[(1 - \\beta)z_0z^T + z_0\\epsilon^T\\sqrt{\\beta}(1 - \\beta) + \\epsilon z^T\\sqrt{\\beta}(1 - \\beta) + \\beta\\epsilon\\epsilon^T\\right]^{-1}Ez,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= \\left[(1 - \\beta)E[z_0z^T] + E[z_0\\epsilon^T]\\sqrt{\\beta}(1 - \\beta) + E[\\epsilon z^T]\\sqrt{\\beta}(1 - \\beta) + \\beta E[\\epsilon\\epsilon^T]\\right]^{-1} \\times Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= \\left[(1 - \\beta)I_k + E[z_0]E[\\epsilon]^T\\sqrt{\\beta}(1 - \\beta) + E[\\epsilon]E[z_0]^T\\sqrt{\\beta}(1 - \\beta) + \\beta I_k\\right]^{-1} \\times Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= Ez_0(z_0)^i z_0\\sqrt{1 - \\beta} + \\epsilon\\beta\n$$\n\n$$\n= Ez_0[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})] + Ez_0[(z_0)^i]E[\\epsilon][\\epsilon]\\sqrt{\\beta}\n$$\n\nThe final step follows from independence of z_0 and \\epsilon. Since z_0 and \\epsilon are also N(0, I_k), we get\n\n$$\n\\theta_i^* = Ez_0(z_0)^i z_0\\sqrt{1 - \\beta} = 0, ..., 0, 1 - \\beta, 0, ..., 0,\n$$\n\nwhere the ith coordinate is \\sqrt{1 - \\beta} and zero everywhere else. Therefore, stacking all the rows together, we get $\\theta^* = \\sqrt{1 - \\beta}I_k$, which after normalization by $\\frac{1}{\\sqrt{1 - \\beta}}$ gives the desired result.\n\nNext, we show that $\\theta$ recovers the true subspace of $x_0 \\sim p_{x_0}$. When composed with the decoder of VAE, the generator of the LDM gives $x_0 = D\\theta z_1 = DI_kz_1 = Sz_1$. Since $z_1 \\sim N(0, I_k)$, this completes the statement of the theorem. $\\blacksquare$", "md": "Using Assumption 3.1, the above expression simplifies to\n\n$$\n\\theta_i^* = zE,\\epsilon\\left[(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})^T\\right]^{-1}Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= E\\left[(1 - \\beta)z_0z^T + z_0\\epsilon^T\\sqrt{\\beta}(1 - \\beta) + \\epsilon z^T\\sqrt{\\beta}(1 - \\beta) + \\beta\\epsilon\\epsilon^T\\right]^{-1}Ez,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= \\left[(1 - \\beta)E[z_0z^T] + E[z_0\\epsilon^T]\\sqrt{\\beta}(1 - \\beta) + E[\\epsilon z^T]\\sqrt{\\beta}(1 - \\beta) + \\beta E[\\epsilon\\epsilon^T]\\right]^{-1} \\times Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= \\left[(1 - \\beta)I_k + E[z_0]E[\\epsilon]^T\\sqrt{\\beta}(1 - \\beta) + E[\\epsilon]E[z_0]^T\\sqrt{\\beta}(1 - \\beta) + \\beta I_k\\right]^{-1} \\times Ez_0,\\epsilon[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})]\n$$\n\n$$\n= Ez_0(z_0)^i z_0\\sqrt{1 - \\beta} + \\epsilon\\beta\n$$\n\n$$\n= Ez_0[(z_0)^i(z_0\\sqrt{1 - \\beta} + \\epsilon\\sqrt{\\beta})] + Ez_0[(z_0)^i]E[\\epsilon][\\epsilon]\\sqrt{\\beta}\n$$\n\nThe final step follows from independence of z_0 and \\epsilon. Since z_0 and \\epsilon are also N(0, I_k), we get\n\n$$\n\\theta_i^* = Ez_0(z_0)^i z_0\\sqrt{1 - \\beta} = 0, ..., 0, 1 - \\beta, 0, ..., 0,\n$$\n\nwhere the ith coordinate is \\sqrt{1 - \\beta} and zero everywhere else. Therefore, stacking all the rows together, we get $\\theta^* = \\sqrt{1 - \\beta}I_k$, which after normalization by $\\frac{1}{\\sqrt{1 - \\beta}}$ gives the desired result.\n\nNext, we show that $\\theta$ recovers the true subspace of $x_0 \\sim p_{x_0}$. When composed with the decoder of VAE, the generator of the LDM gives $x_0 = D\\theta z_1 = DI_kz_1 = Sz_1$. Since $z_1 \\sim N(0, I_k)$, this completes the statement of the theorem. $\\blacksquare$"}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 3.7", "md": "### Proof of Theorem 3.7"}, {"type": "text", "value": "Recall that the latent-space GML-DPS (6) algorithm (based on the pixel-space DPS algorithm [11]) has three key steps. In the first step, it uses the normalized closed-form solution obtained in Theorem 3.6 to perform one step of denoising by the reverse SDE. In the second step, it runs one step of gradient descent to satisfy the measurements in the pixel space. Finally, it takes one step of gradient descent on the goodness objective, which acts as a regularizer to ensure that the reconstructed image lies on the data manifold.\n\nThis can be formalized as:\n\n$$\nz_0 = \\theta z_1 - \\eta\\nabla_{z_1}\\left\\|AD(z_0(z_1)) - y\\right\\|_2; \\quad (8)\n$$\n\n$$\nz_0 = \\arg\\min_{z_0}\\left\\|z_0 - E(D(z_0))\\right\\|_2; \\quad (9)\n$$\n\nIn practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis however, we analyze the exact system of equations above, as (9) has a closed-form solution in the linear setting. 20", "md": "Recall that the latent-space GML-DPS (6) algorithm (based on the pixel-space DPS algorithm [11]) has three key steps. In the first step, it uses the normalized closed-form solution obtained in Theorem 3.6 to perform one step of denoising by the reverse SDE. In the second step, it runs one step of gradient descent to satisfy the measurements in the pixel space. Finally, it takes one step of gradient descent on the goodness objective, which acts as a regularizer to ensure that the reconstructed image lies on the data manifold.\n\nThis can be formalized as:\n\n$$\nz_0 = \\theta z_1 - \\eta\\nabla_{z_1}\\left\\|AD(z_0(z_1)) - y\\right\\|_2; \\quad (8)\n$$\n\n$$\nz_0 = \\arg\\min_{z_0}\\left\\|z_0 - E(D(z_0))\\right\\|_2; \\quad (9)\n$$\n\nIn practice, solving (9) can be difficult, and can be approximated via gradient descent. In our analysis however, we analyze the exact system of equations above, as (9) has a closed-form solution in the linear setting. 20"}]}, {"page": 21, "text": "Theorem A.4          (Posterior Sampling using Goodness Modified Latent DPS).                     Suppose    Assump-\ntions 3.1    and  Assumption 3.2        hold. Denote by    \u03c3  =  {\u03c3  }k    the singular values of    (AS)  T(AS), i.e.,\n(AS)  T (AS) =    U\u03a3U    T := U D(\u03c3)U     T ,U  \u2208  R k\u00d7k , and let  j j=1\n                         \u2217                     [\u2225    (\u2212               )      (    (       ))\u22252 ]\n                                                      \u2192   \u2212\u2192  \u2212\n                                                              \u2192    \u2212\u2192         \u2212\u2192   \u2212\u2192  \u2212\n                        \u03b8  = arg min   E \u2212\u2192\u2212\u2192   \u2225\u03bc\u02dc1  z1 (z0 , \u03f5 ),z0  \u2212  \u03bc \u03b8  z1  z 0,\u2192\u03f5   \u2225   .\n           \u2212\u2192       \u2212\u2192              \u03b8    z0, \u03f5              \u2212\u2192                                2\nSuppose    x 0 \u223c  p(x 0). Given measurements        y  = Ax  0  and any fixed variance      \u03b2 \u2208  (0,1), then with the\n(unique) step size    \u03b7  = (1/2)U D(\u03b7    i)U  T ,\u03b7i =  {\u03b7 j = 1/2\u03c3  j}k   , the GML-DPS algorithm (6) samples\n                               \u2212\u2192                         i           j=1                        \u2190\u2212     \u2212\u2192\nfrom the true posterior     p(x  0|y) and exactly recovers the groundtruth sample, i.e.,         x 0 =  x 0.\n     Proof.  We start with the measurement consistency update (8) and then show that the solution\nobtained from (8) is already a minimizer of (9). Therefore, we have\n                                   \u2190\u2212                    \u2225                    \u22252\n                                    \u2032      \u2217\u2190\u2212        \u2190\u2212 \u2225      \u2190\u2212  \u2190\u2212        \u2225\n                                   z0 =  \u03b8  z 1 \u2212 \u03b7\u2207  z1  AD(z    0(z1)) \u2212  y  2\n                                            \u2190\u2212           \u2225        \u2190\u2212       \u22252\n                                      =  I kz 1\u2212  \u03b7\u2207  \u2190\u2212 \u2225AD(I   kz 1)\u2212  y \u2225\n                                                      z1                     2\n                                         \u2190\u2212           \u2225     \u2190\u2212      \u22252\n                                      =  z 1 \u2212 \u03b7\u2207  \u2190\u2212 \u2225 ASz  1 )\u2212y  \u2225\n                                         \u2190\u2212        z1 \u2225     \u2190\u2212      \u222522\n                                      =  z 1 \u2212 \u03b7\u2207\u2190\u2212z1 \u2225 ASz  1 )\u2212y  \u2225 2\n                                      (i) \u2190\u2212           \u2225    \u2190\u2212      \u22252\n                                       =  z1 \u2212  \u03b7\u2207  \u2190\u2212 \u2225ASz   1\u2212  y \u2225\n                                                    z1    (          2 )\n                                         \u2190\u2212         T   T       \u2190\u2212\n                                      =  z 1 \u2212 2\u03b7S    A     ASz  1 \u2212y\n                                         \u2190\u2212         T   T     \u2190\u2212         T   T\n                                      =  z 1 \u2212 2\u03b7S    A   ASz   1+ 2\u03b7S     A  y\n                                         \u2190\u2212         T   T     \u2190\u2212         T   T   \u2212\u2192\n                                      =  z 1 \u2212 2\u03b7S    A   ASz   1+ 2\u03b7S     A  Ax  0\n                                         \u2190\u2212         T   T     \u2190\u2212         T   T    \u2212\u2192\n                                      =  z 1 \u2212 2\u03b7S    A   ASz   1+ 2\u03b7S     A  ASz   0,\nwhere (i) is due to    Assumption 3.1. By         Assumption 3.2,        (AS)  T (AS)   is a positive definite matrix\nand can be written as      U\u03a3U    T :\n                          \u2190\u2212\u2032   \u2190\u2212               T\u2190\u2212              T \u2212\n                                                                    \u2192\n                          z0 =  z 1 \u2212 2\u03b7U\u03a3U       z 1 + 2\u03b7U\u03a3U       z0\n                                \u2190\u2212                  T        T\u2190\u2212                  T       T \u2212\u2192\n                             =  z 1 \u2212 2U D(\u03b7    i)U   U\u03a3U     z 1 + 2U D(\u03b7   i)U    U\u03a3U     z 0\n                                \u2190\u2212                    T\u2190\u2212                    T \u2212\u2192\n                             =  z 1 \u2212 2U D(\u03b7    i)\u03a3U    z1 + 2U D(\u03b7    i)\u03a3U    z0\n                                \u2190\u2212                         T\u2190\u2212                         T\u2212\u2192\n                             =  z   \u2212 2U D(\u03b7     )D(\u03c3)U      z  + 2U D(\u03b7    )D(\u03c3)U      z\n                                \u2190\u22121             i        T \u2190\u2212 1             i       T \u2212   0\n                                                                                      \u2192\n                             =  z 1 \u2212 2U D(\u03b7    i\u2299  \u03c3)U    z 1+ 2U D(\u03b7    i \u2299 \u03c3)U     z0.\nSince   \u03b7i = 1/2\u03c3  j, the above expression further simplifies to\n         j                             \u2190\u2212     \u2190\u2212           \u2190\u2212          \u2212\n                                                                       \u2192      \u2212\n                                         \u2032               T            T       \u2192\n                         \u2190\u2212\u2032           z 0 =  z1 \u2212  U U    z1 +  U U   z 0 =  z0 .\nNext, we show that       z   is already a minimizer of (9). This is a direct consequence of the encoder-\n                           0                     \u2190\u2212             \u2190\u2212     \u2190\u2212             \u2223\u2223\u2190\u2212          \u2190\u2212   \u2223\u22232\n                                                  \u2032         T     \u2032      \u2032            \u2223\u2223  \u2032           \u2032  \u2223\u2223\ndecoder architecture of the VAE:           E(D(z  0 )) =  S   Sz 0  =  z 0.  Hence,   \u2223\u2223z 0\u2212 E(D(z    0))\u2223\u2223  = 0, and\n                         \u2190\u2212           \u2223\u2223\u2190\u2212          \u2190\u2212   \u2223\u22232\n                 \u2190\u2212                   \u2223\u2223                 \u2223\u2223     \u2212\n                          \u2032        \u2190\u2212    \u2032           \u2032          \u2192\nconsequently     z 0 =   z0 \u2212  \u03b3\u2207   \u2032 \u2223\u2223z0 \u2212 E(D(z   0 ))\u2223\u2223  =  z 0.  Thus, the reconstructed sample becomes\n\u2190\u2212        \u2190\u2212        \u2212\u2192    \u2212\u2192       z0\nx 0 =  D(z  0) =  Sz 0 =  x 0.\n                         \u2223\u2223                \u2223\u22232\n                          \u2190\u2212          \u2190\u2212                    \u2190\u2212\n                         \u2223\u2223 \u2032           \u2032  \u2223\u2223                 \u2032\n     Furthermore, as     \u2223\u2223z0\u2212 E(D(z    0))\u2223\u2223  = 0  for all z0 , it is evident that the goodness objective cannot\nrectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6)\nrequires the exact step size to sample from the posterior.                                                            \u25a1\n     Beyond the linear setting, we also refer to Table 5 for experiments supporting this result.\n                                                           21", "md": "Theorem A.4 (Posterior Sampling using Goodness Modified Latent DPS). Suppose Assumptions 3.1 and Assumption 3.2 hold. Denote by $$\\sigma = \\{\\sigma\\}_k$$ the singular values of $$(AS)^T(AS)$$, i.e., $$(AS)^T(AS) = U\u03a3U^T := U D(\\sigma)U^T, U \\in \\mathbb{R}^{k\u00d7k}$$, and let $$j j=1$$\n$$\\theta = \\arg \\min_{\\theta z0, \\epsilon} E[\\| \\mu\u02dc1 z1(z0, \\epsilon), z0 - \\mu_{\\theta} z1 z0, \\epsilon \\|_2].$$\nSuppose $$x0 \\sim p(x0)$$. Given measurements $$y = Ax0$$ and any fixed variance $$\\beta \\in (0,1)$$, then with the (unique) step size $$\\eta = (1/2)U D(\\eta_i)U^T, \\eta_i = \\{\\eta_j = 1/2\\sigma_j\\}_k$$, the GML-DPS algorithm (6) samples from the true posterior $$p(x0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x0 = x0$$.\n\nProof. We start with the measurement consistency update (8) and then show that the solution obtained from (8) is already a minimizer of (9). Therefore, we have\n\n$$\n\\begin{align*}\nz0 & = \\theta z1 - \\eta \\nabla z1 AD(z0(z1)) - y^2 \\\\\n& = I_k z1 - \\eta \\nabla \\|AD(I_k z1) - y\\|^2 \\\\\n& = z1 - \\eta \\nabla \\|ASz1 - y\\|^2 \\\\\n& = z1 - \\eta \\nabla z1 \\|ASz1 - y\\|^2 \\\\\n& = z1 - \\eta \\nabla \\|ASz1 - y\\|^2 \\\\\n& = z1 - 2\\eta S_A ASz1 - y \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A y \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A Ax0 \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A ASz0,\n\\end{align*}\n$$\n\nwhere (i) is due to Assumption 3.1. By Assumption 3.2, $$(AS)^T(AS)$$ is a positive definite matrix and can be written as $$U\u03a3U^T$$:\n\n$$\n\\begin{align*}\nz0 & = z1 - 2\\eta U\u03a3U z1 + 2\\eta U\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)U U\u03a3U z1 + 2U D(\\eta_i)U U\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)\u03a3U z1 + 2U D(\\eta_i)\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)D(\\sigma)U z1 + 2U D(\\eta_i)D(\\sigma)U z0 \\\\\n& = z1 - 2U D(\\eta_i\u2299\\sigma)U z1 + 2U D(\\eta_i\u2299\\sigma)U z0.\n\\end{align*}\n$$\n\nSince $$\\eta_i = 1/2\\sigma_j$$, the above expression further simplifies to\n\n$$\n\\begin{align*}\nz0 & = z1 - UU z1 + UU z0 = z0.\n\\end{align*}\n$$\n\nNext, we show that $$z0$$ is already a minimizer of (9). This is a direct consequence of the encoder-decoder architecture of the VAE: $$E(D(z0)) = Sz0 = z0$$. Hence, $$|z0 - E(D(z0))| = 0$$, and consequently $$z0 = z0$$. Thus, the reconstructed sample becomes $$x0 = D(z0) = Sz0 = x0$$.\n\nFurthermore, as $$|z0 - E(D(z0))| = 0$$ for all $$z0$$, it is evident that the goodness objective cannot rectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6) requires the exact step size to sample from the posterior.\n\nBeyond the linear setting, we also refer to Table 5 for experiments supporting this result.", "images": [], "items": [{"type": "text", "value": "Theorem A.4 (Posterior Sampling using Goodness Modified Latent DPS). Suppose Assumptions 3.1 and Assumption 3.2 hold. Denote by $$\\sigma = \\{\\sigma\\}_k$$ the singular values of $$(AS)^T(AS)$$, i.e., $$(AS)^T(AS) = U\u03a3U^T := U D(\\sigma)U^T, U \\in \\mathbb{R}^{k\u00d7k}$$, and let $$j j=1$$\n$$\\theta = \\arg \\min_{\\theta z0, \\epsilon} E[\\| \\mu\u02dc1 z1(z0, \\epsilon), z0 - \\mu_{\\theta} z1 z0, \\epsilon \\|_2].$$\nSuppose $$x0 \\sim p(x0)$$. Given measurements $$y = Ax0$$ and any fixed variance $$\\beta \\in (0,1)$$, then with the (unique) step size $$\\eta = (1/2)U D(\\eta_i)U^T, \\eta_i = \\{\\eta_j = 1/2\\sigma_j\\}_k$$, the GML-DPS algorithm (6) samples from the true posterior $$p(x0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x0 = x0$$.\n\nProof. We start with the measurement consistency update (8) and then show that the solution obtained from (8) is already a minimizer of (9). Therefore, we have\n\n$$\n\\begin{align*}\nz0 & = \\theta z1 - \\eta \\nabla z1 AD(z0(z1)) - y^2 \\\\\n& = I_k z1 - \\eta \\nabla \\|AD(I_k z1) - y\\|^2 \\\\\n& = z1 - \\eta \\nabla \\|ASz1 - y\\|^2 \\\\\n& = z1 - \\eta \\nabla z1 \\|ASz1 - y\\|^2 \\\\\n& = z1 - \\eta \\nabla \\|ASz1 - y\\|^2 \\\\\n& = z1 - 2\\eta S_A ASz1 - y \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A y \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A Ax0 \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A ASz0,\n\\end{align*}\n$$\n\nwhere (i) is due to Assumption 3.1. By Assumption 3.2, $$(AS)^T(AS)$$ is a positive definite matrix and can be written as $$U\u03a3U^T$$:\n\n$$\n\\begin{align*}\nz0 & = z1 - 2\\eta U\u03a3U z1 + 2\\eta U\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)U U\u03a3U z1 + 2U D(\\eta_i)U U\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)\u03a3U z1 + 2U D(\\eta_i)\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)D(\\sigma)U z1 + 2U D(\\eta_i)D(\\sigma)U z0 \\\\\n& = z1 - 2U D(\\eta_i\u2299\\sigma)U z1 + 2U D(\\eta_i\u2299\\sigma)U z0.\n\\end{align*}\n$$\n\nSince $$\\eta_i = 1/2\\sigma_j$$, the above expression further simplifies to\n\n$$\n\\begin{align*}\nz0 & = z1 - UU z1 + UU z0 = z0.\n\\end{align*}\n$$\n\nNext, we show that $$z0$$ is already a minimizer of (9). This is a direct consequence of the encoder-decoder architecture of the VAE: $$E(D(z0)) = Sz0 = z0$$. Hence, $$|z0 - E(D(z0))| = 0$$, and consequently $$z0 = z0$$. Thus, the reconstructed sample becomes $$x0 = D(z0) = Sz0 = x0$$.\n\nFurthermore, as $$|z0 - E(D(z0))| = 0$$ for all $$z0$$, it is evident that the goodness objective cannot rectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6) requires the exact step size to sample from the posterior.\n\nBeyond the linear setting, we also refer to Table 5 for experiments supporting this result.", "md": "Theorem A.4 (Posterior Sampling using Goodness Modified Latent DPS). Suppose Assumptions 3.1 and Assumption 3.2 hold. Denote by $$\\sigma = \\{\\sigma\\}_k$$ the singular values of $$(AS)^T(AS)$$, i.e., $$(AS)^T(AS) = U\u03a3U^T := U D(\\sigma)U^T, U \\in \\mathbb{R}^{k\u00d7k}$$, and let $$j j=1$$\n$$\\theta = \\arg \\min_{\\theta z0, \\epsilon} E[\\| \\mu\u02dc1 z1(z0, \\epsilon), z0 - \\mu_{\\theta} z1 z0, \\epsilon \\|_2].$$\nSuppose $$x0 \\sim p(x0)$$. Given measurements $$y = Ax0$$ and any fixed variance $$\\beta \\in (0,1)$$, then with the (unique) step size $$\\eta = (1/2)U D(\\eta_i)U^T, \\eta_i = \\{\\eta_j = 1/2\\sigma_j\\}_k$$, the GML-DPS algorithm (6) samples from the true posterior $$p(x0|y)$$ and exactly recovers the groundtruth sample, i.e., $$x0 = x0$$.\n\nProof. We start with the measurement consistency update (8) and then show that the solution obtained from (8) is already a minimizer of (9). Therefore, we have\n\n$$\n\\begin{align*}\nz0 & = \\theta z1 - \\eta \\nabla z1 AD(z0(z1)) - y^2 \\\\\n& = I_k z1 - \\eta \\nabla \\|AD(I_k z1) - y\\|^2 \\\\\n& = z1 - \\eta \\nabla \\|ASz1 - y\\|^2 \\\\\n& = z1 - \\eta \\nabla z1 \\|ASz1 - y\\|^2 \\\\\n& = z1 - \\eta \\nabla \\|ASz1 - y\\|^2 \\\\\n& = z1 - 2\\eta S_A ASz1 - y \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A y \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A Ax0 \\\\\n& = z1 - 2\\eta S_A ASz1 + 2\\eta S_A ASz0,\n\\end{align*}\n$$\n\nwhere (i) is due to Assumption 3.1. By Assumption 3.2, $$(AS)^T(AS)$$ is a positive definite matrix and can be written as $$U\u03a3U^T$$:\n\n$$\n\\begin{align*}\nz0 & = z1 - 2\\eta U\u03a3U z1 + 2\\eta U\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)U U\u03a3U z1 + 2U D(\\eta_i)U U\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)\u03a3U z1 + 2U D(\\eta_i)\u03a3U z0 \\\\\n& = z1 - 2U D(\\eta_i)D(\\sigma)U z1 + 2U D(\\eta_i)D(\\sigma)U z0 \\\\\n& = z1 - 2U D(\\eta_i\u2299\\sigma)U z1 + 2U D(\\eta_i\u2299\\sigma)U z0.\n\\end{align*}\n$$\n\nSince $$\\eta_i = 1/2\\sigma_j$$, the above expression further simplifies to\n\n$$\n\\begin{align*}\nz0 & = z1 - UU z1 + UU z0 = z0.\n\\end{align*}\n$$\n\nNext, we show that $$z0$$ is already a minimizer of (9). This is a direct consequence of the encoder-decoder architecture of the VAE: $$E(D(z0)) = Sz0 = z0$$. Hence, $$|z0 - E(D(z0))| = 0$$, and consequently $$z0 = z0$$. Thus, the reconstructed sample becomes $$x0 = D(z0) = Sz0 = x0$$.\n\nFurthermore, as $$|z0 - E(D(z0))| = 0$$ for all $$z0$$, it is evident that the goodness objective cannot rectify the error incurred in the measurement update (8). For this reason, GML-DPS algorithm (6) requires the exact step size to sample from the posterior.\n\nBeyond the linear setting, we also refer to Table 5 for experiments supporting this result."}]}, {"page": 22, "text": "A.5      Proof of Theorem 3.8\nDifferent from GML-DPS, PSLD             Algorithm 2       replaces the goodness objective (6) with the gluing\nobjective (7), which can be formalized as:\n                            \u2190\u2212\u2032     \u2217\u2190\u2212           \u2225      \u2190\u2212  \u2190\u2212        \u22252\n                            z   =  \u03b8  z1 \u2212  \u03b7\u2207 \u2190\u2212 \u2225 AD(z   0(z 1)) \u2212 y \u2225  ;                                          (10)\n                              0             \u2223\u2223  z1                      2                \u2223\u22232\n                                            \u2223\u2223\u2190\u2212                                    \u2190\u2212   \u2223\u2223\n                            \u2190\u2212              \u2223\u2223 \u2032         T   \u2212\u2192             T         \u2032  \u2223\u2223\n                            z 0 = arg min   \u2223\u2223z0 \u2212 E(A     Az 0 + (I d \u2212 A    A)D(z   0))\u2223\u2223  .                       (11)\n                                        \u2190\u2212\u2032                                                2\n                                        z0\nWe again remind that solving the minimization problem (11) is hard in general, and can be                 approximated\nby gradient descent as typically followed in practice [11]. However, in a linear model setting, (11) has a\nclosed-form solution which we derive to prove exact recovery.\nTheorem A.5        (Posterior Sampling using Diffusion in Latent Space).            Let Assumptions 3.1         and  3.2\nhold. Let   \u03c3 j,\u2200j = 1, . . . , rdenote the singular values of     (AS)  T (AS)   and let\n                         \u2217                     [\u2225    (\u2212               )       (   (       ))\u22252 ]\n                                                      \u2192   \u2212\u2192  \u2212\n                                                              \u2192    \u2212\u2192          \u2212\u2192  \u2212\u2192   \u2212\n                        \u03b8  = arg min    E\u2212\u2192\u2212\u2192   \u2225 \u02dc                                     \u2192    \u2225\n                                    \u03b8    z 0,\u03f5    \u03bc1  z 1(z 0, \u03f5 ),z 0  \u2212  \u03bc\u03b8  z 1  z0, \u03f5        .\n           \u2212\u2192       \u2212\u2192                                      \u2212\u2192\nSuppose    x 0 \u223c  p(x 0). Given measurements         y =  Ax  0, any fixed variance     \u03b2  \u2208 (0,1), and any positive\nstep sizes  \u03b7ij, j= 1,2, . . . , r, the PSLD Algorithm 2 samples from the true posterior          p(x\u2212\u21920|y) and exactly\n                                              \u2190\u2212    \u2212\u2192\nrecovers the groundtruth sample, i.e.,        x0 =  x 0.\n     Proof.  Following the proof in Appendix A.4, we have\n                                   \u2190\u2212\u2032     \u2217\u2190\u2212           \u2225      \u2190\u2212  \u2190\u2212        \u22252\n                                   z   =  \u03b8  z1 \u2212  \u03b7\u2207  \u2190\u2212\u2225 AD(z   0(z 1)) \u2212 y \u2225\n                                     0      \u2190\u2212         z1\u2225      \u2190\u2212       \u22252     2\n                                       =  Ik z1 \u2212  \u03b7\u2207\u2190\u2212  \u2225 AD(z   1) \u2212 y \u2225\n                                                      z1                  2\n                                          \u2190\u2212           \u2225    \u2190\u2212      \u22252\n                                       =  z1 \u2212  \u03b7\u2207\u2190\u2212z1 \u2225ASz   1\u2212  y \u22252\n                                          \u2190\u2212         T   T     \u2190\u2212\n                                       =  z1 \u2212  2\u03b7S   A   (ASz   1 \u2212 y)\n                                          \u2190\u2212                  \u2190\u2212                   \u2212\n                                       =  z1 \u2212  2\u03b7S  TA  TASz   1 + 2\u03b7S  T A T ASz \u21920\n                                          \u2190\u2212                  \u2190\u2212                   \u2212\n                                       =  z1 \u2212  2\u03b7S  TA  TASz   1 + 2\u03b7S  T A T ASz \u21920.\nWe use the above expression to derive a closed-form solution to the minimization problem (11):\n                          \u2225                                             \u22252\n                          \u2225\u2190\u2212                                       \u2190\u2212  \u2225\n               0  =  \u2207 \u2190\u2212 \u2225z \u2032\u2212 S  T (A T ASz \u2212\u2192 + (I   \u2212 A  T A)Sz  \u2032 )\u2225\n                        \u2032 \u2225  0                 0      d              0  \u2225\n                       z0 \u2225                                              2\u22252\n                          \u2225\u2190\u2212                \u2212                        \u2190\u2212  \u2225\n                  =  \u2207 \u2190\u2212 \u2225z \u2032\u2212 S  T A T ASz \u21920 \u2212 S  T(I d \u2212 A T A)Sz   \u2032)\u2225\n                        \u2032 \u2225  0                                          0 \u2225\n                       z0 \u2225                                                 2 \u22252\n                          \u2225\u2190\u2212\u2032     T   T     \u2212          \u2190\u2212                \u2190\u2212  \u2225\n                  =  \u2207\u2190\u2212  \u2225z  \u2212 S    A   ASz \u21920 \u2212 S  TSz  \u2032 \u2212 S T A T ASz  \u2032 )\u2225\n                        \u2032 \u2225  0                            0                0  \u2225\n                       z0                                                      2\n                          \u2225                                                   \u22252\n                          \u2225\u2190\u2212\u2032     T   T     \u2212          \u2190\u2212                \u2190\u2212  \u2225\n                  =  \u2207 \u2190\u2212 \u2225z  \u2212 S    A   ASz \u21920 \u2212 S  TSz  \u2032 + S T A T ASz  \u2032 )\u2225\n                        \u2032 \u2225  0                            0                0  \u2225\n                      (z0                         ) ( \u2190\u2212               \u2212       2   \u2190\u2212               \u2190\u2212  )\n                  = 2   Ik \u2212 S  TS  +  ST A T AS      z\u2032 \u2212 S  TA  TASz \u2192 0 \u2212 S T Sz  \u2032+  S T A TASz   \u2032)\n                                                       0                            0                 0\n                  = 2S  TA  TAS   ( ST A T ASz \u2190\u2212\u2032\u2212 S  T A TASz \u2212\u21920))  ,\n                                                 0\nwhere the last step is due to      Assumption 3.1. Thus, we have\n                                         \u2223\u2223\u2190\u2212                                    \u2190\u2212   \u2223\u22232\n                         \u2190\u2212              \u2223\u2223 \u2032         T   \u2212\u2192             T         \u2032  \u2223\u2223     \u2212\u2192\n                         z 0 = arg min   \u2223\u2223z0 \u2212 E(A    Az  0 + (I d \u2212 A    A)D(z   0))\u2223\u2223  =  z0 ,\n                                     \u2190\u2212\u2032 \u2223\u2223                                           \u2223\u22232\n                                     z0                    22", "md": "Proof of Theorem 3.8\n\nDifferent from GML-DPS, PSLD Algorithm 2 replaces the goodness objective (6) with the gluing objective (7), which can be formalized as:\n\n$$\nz^{\\prime} = \\theta z_1 - \\eta \\nabla \\left\\| AD(z_0(z_1)) - y \\right\\|; \\quad (10)\n$$\n$$\nz_0 = \\arg \\min \\left\\| z_0 - E(Az_0 + (I_d - AA)D(z_0)) \\right\\|; \\quad (11)\n$$\nWe again remind that solving the minimization problem (11) is hard in general, and can be approximated by gradient descent as typically followed in practice [11]. However, in a linear model setting, (11) has a closed-form solution which we derive to prove exact recovery.\n\nTheorem A.5 (Posterior Sampling using Diffusion in Latent Space)\n\nLet Assumptions 3.1 and 3.2 hold. Let $\\sigma_j, \\forall j = 1, . . . , r$ denote the singular values of $(AS)^T(AS)$ and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E\\left[ \\left\\| \\tilde{z} - \\mu_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\epsilon \\right] \\right.\n$$\nSuppose $x_0 \\sim p(x_0)$. Given measurements $y = Ax_0$, any fixed variance $\\beta \\in (0,1)$, and any positive step sizes $\\eta_{ij}, j=1,2,...,r$, the PSLD Algorithm 2 samples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = x_0$.\n\nProof. Following the proof in Appendix A.4, we have\n\n$$\nz^{\\prime} = \\theta z_1 - \\eta \\nabla \\left\\| AD(z_0(z_1)) - y \\right\\|\n$$\n$$\n= I_k z_1 - \\eta \\nabla \\left\\| AD(z_1) - y \\right\\|\n$$\n$$\n= z_1 - \\eta \\nabla z_1 \\left\\| ASz_1 - y \\right\\|^2\n$$\n$$\n= z_1 - 2\\eta S^T A (ASz_1 - y)\n$$\n$$\n= z_1 - 2\\eta S^T A TASz_1 + 2\\eta S^T A^T ASz_0\n$$\n$$\n= z_1 - 2\\eta S^T A TASz_1 + 2\\eta S^T A^T ASz_0.\n$$\nWe use the above expression to derive a closed-form solution to the minimization problem (11):\n\n$$\nz_0 = \\nabla \\left\\| z^{\\prime} - S^T (A^T ASz_0 + (I - A^T A)z^{\\prime}) \\right\\|\n$$\n$$\n= \\nabla \\left\\| z^{\\prime} - S^T A^T ASz_0 - S^T(I_d - A^T A)z^{\\prime} \\right\\|\n$$\n$$\n= \\nabla \\left\\| z^{\\prime} - S^T A^T ASz_0 - S^Tz^{\\prime} + S^T A^T ASz^{\\prime} \\right\\|\n$$\n$$\n= 2I_k - S^T S + ST A^T AS (z^{\\prime} - S^T A TASz_0 - S^Tz^{\\prime} + S^T A TASz^{\\prime})\n$$\n$$\n= 2S^T A TAS (ST A^T ASz^{\\prime} - S^T A TASz_0).\n$$\nWhere the last step is due to Assumption 3.1. Thus, we have\n\n$$\nz_0 = \\arg \\min \\left\\| z_0 - E(AAz_0 + (I_d - AA)D(z_0)) \\right\\| = z_0,\n$$\n$$\nz_0 = z_0.\n$$", "images": [], "items": [{"type": "text", "value": "Proof of Theorem 3.8\n\nDifferent from GML-DPS, PSLD Algorithm 2 replaces the goodness objective (6) with the gluing objective (7), which can be formalized as:\n\n$$\nz^{\\prime} = \\theta z_1 - \\eta \\nabla \\left\\| AD(z_0(z_1)) - y \\right\\|; \\quad (10)\n$$\n$$\nz_0 = \\arg \\min \\left\\| z_0 - E(Az_0 + (I_d - AA)D(z_0)) \\right\\|; \\quad (11)\n$$\nWe again remind that solving the minimization problem (11) is hard in general, and can be approximated by gradient descent as typically followed in practice [11]. However, in a linear model setting, (11) has a closed-form solution which we derive to prove exact recovery.\n\nTheorem A.5 (Posterior Sampling using Diffusion in Latent Space)\n\nLet Assumptions 3.1 and 3.2 hold. Let $\\sigma_j, \\forall j = 1, . . . , r$ denote the singular values of $(AS)^T(AS)$ and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E\\left[ \\left\\| \\tilde{z} - \\mu_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\epsilon \\right] \\right.\n$$\nSuppose $x_0 \\sim p(x_0)$. Given measurements $y = Ax_0$, any fixed variance $\\beta \\in (0,1)$, and any positive step sizes $\\eta_{ij}, j=1,2,...,r$, the PSLD Algorithm 2 samples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = x_0$.\n\nProof. Following the proof in Appendix A.4, we have\n\n$$\nz^{\\prime} = \\theta z_1 - \\eta \\nabla \\left\\| AD(z_0(z_1)) - y \\right\\|\n$$\n$$\n= I_k z_1 - \\eta \\nabla \\left\\| AD(z_1) - y \\right\\|\n$$\n$$\n= z_1 - \\eta \\nabla z_1 \\left\\| ASz_1 - y \\right\\|^2\n$$\n$$\n= z_1 - 2\\eta S^T A (ASz_1 - y)\n$$\n$$\n= z_1 - 2\\eta S^T A TASz_1 + 2\\eta S^T A^T ASz_0\n$$\n$$\n= z_1 - 2\\eta S^T A TASz_1 + 2\\eta S^T A^T ASz_0.\n$$\nWe use the above expression to derive a closed-form solution to the minimization problem (11):\n\n$$\nz_0 = \\nabla \\left\\| z^{\\prime} - S^T (A^T ASz_0 + (I - A^T A)z^{\\prime}) \\right\\|\n$$\n$$\n= \\nabla \\left\\| z^{\\prime} - S^T A^T ASz_0 - S^T(I_d - A^T A)z^{\\prime} \\right\\|\n$$\n$$\n= \\nabla \\left\\| z^{\\prime} - S^T A^T ASz_0 - S^Tz^{\\prime} + S^T A^T ASz^{\\prime} \\right\\|\n$$\n$$\n= 2I_k - S^T S + ST A^T AS (z^{\\prime} - S^T A TASz_0 - S^Tz^{\\prime} + S^T A TASz^{\\prime})\n$$\n$$\n= 2S^T A TAS (ST A^T ASz^{\\prime} - S^T A TASz_0).\n$$\nWhere the last step is due to Assumption 3.1. Thus, we have\n\n$$\nz_0 = \\arg \\min \\left\\| z_0 - E(AAz_0 + (I_d - AA)D(z_0)) \\right\\| = z_0,\n$$\n$$\nz_0 = z_0.\n$$", "md": "Proof of Theorem 3.8\n\nDifferent from GML-DPS, PSLD Algorithm 2 replaces the goodness objective (6) with the gluing objective (7), which can be formalized as:\n\n$$\nz^{\\prime} = \\theta z_1 - \\eta \\nabla \\left\\| AD(z_0(z_1)) - y \\right\\|; \\quad (10)\n$$\n$$\nz_0 = \\arg \\min \\left\\| z_0 - E(Az_0 + (I_d - AA)D(z_0)) \\right\\|; \\quad (11)\n$$\nWe again remind that solving the minimization problem (11) is hard in general, and can be approximated by gradient descent as typically followed in practice [11]. However, in a linear model setting, (11) has a closed-form solution which we derive to prove exact recovery.\n\nTheorem A.5 (Posterior Sampling using Diffusion in Latent Space)\n\nLet Assumptions 3.1 and 3.2 hold. Let $\\sigma_j, \\forall j = 1, . . . , r$ denote the singular values of $(AS)^T(AS)$ and let\n\n$$\n\\theta = \\arg \\min_{\\theta} E\\left[ \\left\\| \\tilde{z} - \\mu_1 z_1(z_0, \\epsilon), z_0 - \\mu_{\\theta} z_1 z_0, \\epsilon \\right] \\right.\n$$\nSuppose $x_0 \\sim p(x_0)$. Given measurements $y = Ax_0$, any fixed variance $\\beta \\in (0,1)$, and any positive step sizes $\\eta_{ij}, j=1,2,...,r$, the PSLD Algorithm 2 samples from the true posterior $p(x_0|y)$ and exactly recovers the groundtruth sample, i.e., $x_0 = x_0$.\n\nProof. Following the proof in Appendix A.4, we have\n\n$$\nz^{\\prime} = \\theta z_1 - \\eta \\nabla \\left\\| AD(z_0(z_1)) - y \\right\\|\n$$\n$$\n= I_k z_1 - \\eta \\nabla \\left\\| AD(z_1) - y \\right\\|\n$$\n$$\n= z_1 - \\eta \\nabla z_1 \\left\\| ASz_1 - y \\right\\|^2\n$$\n$$\n= z_1 - 2\\eta S^T A (ASz_1 - y)\n$$\n$$\n= z_1 - 2\\eta S^T A TASz_1 + 2\\eta S^T A^T ASz_0\n$$\n$$\n= z_1 - 2\\eta S^T A TASz_1 + 2\\eta S^T A^T ASz_0.\n$$\nWe use the above expression to derive a closed-form solution to the minimization problem (11):\n\n$$\nz_0 = \\nabla \\left\\| z^{\\prime} - S^T (A^T ASz_0 + (I - A^T A)z^{\\prime}) \\right\\|\n$$\n$$\n= \\nabla \\left\\| z^{\\prime} - S^T A^T ASz_0 - S^T(I_d - A^T A)z^{\\prime} \\right\\|\n$$\n$$\n= \\nabla \\left\\| z^{\\prime} - S^T A^T ASz_0 - S^Tz^{\\prime} + S^T A^T ASz^{\\prime} \\right\\|\n$$\n$$\n= 2I_k - S^T S + ST A^T AS (z^{\\prime} - S^T A TASz_0 - S^Tz^{\\prime} + S^T A TASz^{\\prime})\n$$\n$$\n= 2S^T A TAS (ST A^T ASz^{\\prime} - S^T A TASz_0).\n$$\nWhere the last step is due to Assumption 3.1. Thus, we have\n\n$$\nz_0 = \\arg \\min \\left\\| z_0 - E(AAz_0 + (I_d - AA)D(z_0)) \\right\\| = z_0,\n$$\n$$\nz_0 = z_0.\n$$"}]}, {"page": 23, "text": "                    \u2190\u2212        \u2190\u2212          \u2212\n                                          \u2192         \u2212\u2192     \u2212\u2192\nwhich produces      x 0 =  D(z  0 ) =  D(z  0) =  Sz  0 =  x 0.                                                               \u25a1\n    It is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the\nchoice of the step size     \u03b7, whereas GML-DPS requires the step size to be exactly                  \u03b7 = (1/2)U D(\u03b7       )U  T .\nB      Additional Experiments                                                                                             i\nB.1      Implementation Details\nFor inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing\n                                                                                        \u2190\u2212       T   \u2212\u2192            T        \u2190\u2212\nobjective) that are consistent with the known portions of the image, i.e.,              x 0 =  A   Ax  0+(I  d \u2212A    A)D(z    0).\nThis is different from the DPS sampler, which generates the whole image which may not match the\nobservations exactly. In other words, in the last of step of our algorithm, the observations are glued onto\nthe corresponding parts of the generated image, leaving the unmasked portions untouched [51]. This\nsometimes creates edge effects which are then removed by post-processing the glued image through the\nencoder and decoder of the SD model, i.e. running one last step of our algorithm. Figure 2 illustrates\nthat gluing the observations in commercial services still leads to visually inconsistent results (e.g. head\nin top row) unlike our method.\n    For all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is\n                                                         \u2190\u2212        \u2190\u2212\nnot needed, as there is no box inpainting, i.e.,         x 0 =  D(z  0 ). Furthermore, we use the same measurement\noperator    A  and its transpose      A  T  as provided by the DPS code repository              7. However, since Stable\nDiffusion v1.5 generates images of size        512\u00d7512     resolution and DPS operates at          256\u00d7256, we adjust the\nsize of the kernels used in PSLD to ensure that both the methods use the same amount of information\nwhile sampling from the posterior. During evaluation, we downsample PSLD generated images from\n512  \u00d7  512  to 256  \u00d7  256   to compare with DPS at the same resolution.\nPSLD (Stable Diffusion-V1.5 ):                 We run     Algorithm 2        with Stable Diffusion version 1.5 as the\nfoundation model      8 . We use a fixed      \u03b7 = 1   and  \u03b3  = 0.1. Since we study posterior sampling of images\nwithout conditioning on       text  inputs, we pass an empty string to the Stable Diffusion foundation model,\nwhich accepts texts as an input argument. For better performance, we recommend using the latest\npretrained weights.\nPSLD (LDM-VQ-4 ):                This is the same sampling algorithm as before but with a different latent\ndiffusion model, LDM-VQ-4           9  , which contains pretrained weights for FFHQ 256                  10  and large-scale\ntext-to-image generative model         11 . We keep the hyperparameters same (\u03b7               = 1  and   \u03b3 = 0.1). For each\ntask, we provide hyper-parameter details in our codebase                 12. Although we have tested our framework\nwith these two latent-diffusion-models, one may experiment with other latent-diffusion-models available\nin the same repository.\nDPS:    We use the original source code provided by the authors                 13.\nOOD images are sourced online:\n   1. Figure 1: the original images are generated by Stable Diffusion v-2.1                   14.\n   2. Figure 2 first row: Walking example from the web.\n   3. Figure 2 second row, Obama-Biden image from the web.\n   7https://github.com/DPS2022/diffusion-posterior-sampling/blob/main/guided_diffusion/measurements.py\n   8https://huggingface.co/runwayml/stable-diffusion-v1-5\n   9https://github.com/CompVis/latent-diffusion\n  10https://ommer-lab.com/files/latent-diffusion/ffhq.zip\n  11https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n  12https://github.com/LituRout/PSLD\n  13https://github.com/DPS2022/diffusion-posterior-sampling\n  14https://huggingface.co/spaces/stabilityai/stable-diffusion23", "md": "$$x_0 = D(z_0) = D(z_0) = Sz_0 = x_0.$$\n\nIt is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the choice of the step size $\\eta$, whereas GML-DPS requires the step size to be exactly $\\eta = \\left(\\frac{1}{2}\\right)UD(\\eta)U^T$.\n\n## Additional Experiments\n\n### Implementation Details\n\nFor inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing objective) that are consistent with the known portions of the image, i.e., $$x_0 = A Ax_0 + (I_d - A A)D(z_0)$$. This is different from the DPS sampler, which generates the whole image which may not match the observations exactly. In other words, in the last step of our algorithm, the observations are glued onto the corresponding parts of the generated image, leaving the unmasked portions untouched [51]. This sometimes creates edge effects which are then removed by post-processing the glued image through the encoder and decoder of the SD model, i.e. running one last step of our algorithm. Figure 2 illustrates that gluing the observations in commercial services still leads to visually inconsistent results (e.g. head in top row) unlike our method.\n\nFor all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is not needed, as there is no box inpainting, i.e., $$x_0 = D(z_0)$$. Furthermore, we use the same measurement operator $A$ and its transpose $A^T$ as provided by the DPS code repository [7]. However, since Stable Diffusion v1.5 generates images of size $512\\times512$ resolution and DPS operates at $256\\times256$, we adjust the size of the kernels used in PSLD to ensure that both the methods use the same amount of information while sampling from the posterior. During evaluation, we downsample PSLD generated images from $512\\times512$ to $256\\times256$ to compare with DPS at the same resolution.\n\nPSLD (Stable Diffusion-V1.5): We run Algorithm 2 with Stable Diffusion version 1.5 as the foundation model [8]. We use a fixed $\\eta = 1$ and $\\gamma = 0.1$. Since we study posterior sampling of images without conditioning on text inputs, we pass an empty string to the Stable Diffusion foundation model, which accepts texts as an input argument. For better performance, we recommend using the latest pretrained weights.\n\nPSLD (LDM-VQ-4): This is the same sampling algorithm as before but with a different latent diffusion model, LDM-VQ-4 [9], which contains pretrained weights for FFHQ 256 [10] and large-scale text-to-image generative model [11]. We keep the hyperparameters same ($\\eta = 1$ and $\\gamma = 0.1$). For each task, we provide hyper-parameter details in our codebase [12]. Although we have tested our framework with these two latent diffusion models, one may experiment with other latent diffusion models available in the same repository.\n\nDPS: We use the original source code provided by the authors [13].\n\nOOD images are sourced online:\n\n1. Figure 1: the original images are generated by Stable Diffusion v-2.1 [14].\n2. Figure 2 first row: Walking example from the web.\n3. Figure 2 second row, Obama-Biden image from the web.\n\nReferences:\n\n[7] https://github.com/DPS2022/diffusion-posterior-sampling/blob/main/guided_diffusion/measurements.py\n\n[8] https://huggingface.co/runwayml/stable-diffusion-v1-5\n\n[9] https://github.com/CompVis/latent-diffusion\n\n[10] https://ommer-lab.com/files/latent-diffusion/ffhq.zip\n\n[11] https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n\n[12] https://github.com/LituRout/PSLD\n\n[13] https://github.com/DPS2022/diffusion-posterior-sampling\n\n[14] https://huggingface.co/spaces/stabilityai/stable-diffusion23", "images": [], "items": [{"type": "text", "value": "$$x_0 = D(z_0) = D(z_0) = Sz_0 = x_0.$$\n\nIt is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the choice of the step size $\\eta$, whereas GML-DPS requires the step size to be exactly $\\eta = \\left(\\frac{1}{2}\\right)UD(\\eta)U^T$.", "md": "$$x_0 = D(z_0) = D(z_0) = Sz_0 = x_0.$$\n\nIt is worth highlighting that PSLD exactly recovers the groundtruth sample irrespective of the choice of the step size $\\eta$, whereas GML-DPS requires the step size to be exactly $\\eta = \\left(\\frac{1}{2}\\right)UD(\\eta)U^T$."}, {"type": "heading", "lvl": 2, "value": "Additional Experiments", "md": "## Additional Experiments"}, {"type": "heading", "lvl": 3, "value": "Implementation Details", "md": "### Implementation Details"}, {"type": "text", "value": "For inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing objective) that are consistent with the known portions of the image, i.e., $$x_0 = A Ax_0 + (I_d - A A)D(z_0)$$. This is different from the DPS sampler, which generates the whole image which may not match the observations exactly. In other words, in the last step of our algorithm, the observations are glued onto the corresponding parts of the generated image, leaving the unmasked portions untouched [51]. This sometimes creates edge effects which are then removed by post-processing the glued image through the encoder and decoder of the SD model, i.e. running one last step of our algorithm. Figure 2 illustrates that gluing the observations in commercial services still leads to visually inconsistent results (e.g. head in top row) unlike our method.\n\nFor all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is not needed, as there is no box inpainting, i.e., $$x_0 = D(z_0)$$. Furthermore, we use the same measurement operator $A$ and its transpose $A^T$ as provided by the DPS code repository [7]. However, since Stable Diffusion v1.5 generates images of size $512\\times512$ resolution and DPS operates at $256\\times256$, we adjust the size of the kernels used in PSLD to ensure that both the methods use the same amount of information while sampling from the posterior. During evaluation, we downsample PSLD generated images from $512\\times512$ to $256\\times256$ to compare with DPS at the same resolution.\n\nPSLD (Stable Diffusion-V1.5): We run Algorithm 2 with Stable Diffusion version 1.5 as the foundation model [8]. We use a fixed $\\eta = 1$ and $\\gamma = 0.1$. Since we study posterior sampling of images without conditioning on text inputs, we pass an empty string to the Stable Diffusion foundation model, which accepts texts as an input argument. For better performance, we recommend using the latest pretrained weights.\n\nPSLD (LDM-VQ-4): This is the same sampling algorithm as before but with a different latent diffusion model, LDM-VQ-4 [9], which contains pretrained weights for FFHQ 256 [10] and large-scale text-to-image generative model [11]. We keep the hyperparameters same ($\\eta = 1$ and $\\gamma = 0.1$). For each task, we provide hyper-parameter details in our codebase [12]. Although we have tested our framework with these two latent diffusion models, one may experiment with other latent diffusion models available in the same repository.\n\nDPS: We use the original source code provided by the authors [13].\n\nOOD images are sourced online:\n\n1. Figure 1: the original images are generated by Stable Diffusion v-2.1 [14].\n2. Figure 2 first row: Walking example from the web.\n3. Figure 2 second row, Obama-Biden image from the web.\n\nReferences:\n\n[7] https://github.com/DPS2022/diffusion-posterior-sampling/blob/main/guided_diffusion/measurements.py\n\n[8] https://huggingface.co/runwayml/stable-diffusion-v1-5\n\n[9] https://github.com/CompVis/latent-diffusion\n\n[10] https://ommer-lab.com/files/latent-diffusion/ffhq.zip\n\n[11] https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n\n[12] https://github.com/LituRout/PSLD\n\n[13] https://github.com/DPS2022/diffusion-posterior-sampling\n\n[14] https://huggingface.co/spaces/stabilityai/stable-diffusion23", "md": "For inpainting tasks, we note that the PSLD sampler generates missing parts (by design of our gluing objective) that are consistent with the known portions of the image, i.e., $$x_0 = A Ax_0 + (I_d - A A)D(z_0)$$. This is different from the DPS sampler, which generates the whole image which may not match the observations exactly. In other words, in the last step of our algorithm, the observations are glued onto the corresponding parts of the generated image, leaving the unmasked portions untouched [51]. This sometimes creates edge effects which are then removed by post-processing the glued image through the encoder and decoder of the SD model, i.e. running one last step of our algorithm. Figure 2 illustrates that gluing the observations in commercial services still leads to visually inconsistent results (e.g. head in top row) unlike our method.\n\nFor all other tasks, such as motion deblur, Gaussian deblur, and super-resolution, this last step is not needed, as there is no box inpainting, i.e., $$x_0 = D(z_0)$$. Furthermore, we use the same measurement operator $A$ and its transpose $A^T$ as provided by the DPS code repository [7]. However, since Stable Diffusion v1.5 generates images of size $512\\times512$ resolution and DPS operates at $256\\times256$, we adjust the size of the kernels used in PSLD to ensure that both the methods use the same amount of information while sampling from the posterior. During evaluation, we downsample PSLD generated images from $512\\times512$ to $256\\times256$ to compare with DPS at the same resolution.\n\nPSLD (Stable Diffusion-V1.5): We run Algorithm 2 with Stable Diffusion version 1.5 as the foundation model [8]. We use a fixed $\\eta = 1$ and $\\gamma = 0.1$. Since we study posterior sampling of images without conditioning on text inputs, we pass an empty string to the Stable Diffusion foundation model, which accepts texts as an input argument. For better performance, we recommend using the latest pretrained weights.\n\nPSLD (LDM-VQ-4): This is the same sampling algorithm as before but with a different latent diffusion model, LDM-VQ-4 [9], which contains pretrained weights for FFHQ 256 [10] and large-scale text-to-image generative model [11]. We keep the hyperparameters same ($\\eta = 1$ and $\\gamma = 0.1$). For each task, we provide hyper-parameter details in our codebase [12]. Although we have tested our framework with these two latent diffusion models, one may experiment with other latent diffusion models available in the same repository.\n\nDPS: We use the original source code provided by the authors [13].\n\nOOD images are sourced online:\n\n1. Figure 1: the original images are generated by Stable Diffusion v-2.1 [14].\n2. Figure 2 first row: Walking example from the web.\n3. Figure 2 second row, Obama-Biden image from the web.\n\nReferences:\n\n[7] https://github.com/DPS2022/diffusion-posterior-sampling/blob/main/guided_diffusion/measurements.py\n\n[8] https://huggingface.co/runwayml/stable-diffusion-v1-5\n\n[9] https://github.com/CompVis/latent-diffusion\n\n[10] https://ommer-lab.com/files/latent-diffusion/ffhq.zip\n\n[11] https://ommer-lab.com/files/latent-diffusion/nitro/txt2img-f8-large/model.ckpt\n\n[12] https://github.com/LituRout/PSLD\n\n[13] https://github.com/DPS2022/diffusion-posterior-sampling\n\n[14] https://huggingface.co/spaces/stabilityai/stable-diffusion23"}]}, {"page": 24, "text": "                                                       PSLD Image Inpainting\n                                                 Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n                                        Given an image (square size preferred) anduser defined mask, click on Inpaint to generate missing parts_\n     Upload                                                          Outputl\n                          9 -\n    Number of diffusion steps(e B;2001                      200      Outputz\n    Gluing factor (eB: le-1)\n             Ie,&\n    Gluing kernel size\n    Gluing kernel sigma (e g. 7)\n    Measurement factor (e,g; 1)\n     Your prompt (leave empty for posterior sampling)Inpaintl\nFigure 6: Results from the web application of our PSLD algorithm,                      512  \u00d7512. The original image (1)\nis generated by Stable Diffusion v-2.1 with the prompt,\u201cA dinner date between a robot couple during\nsunset\u201d.\n    4. Figure 2 third row, Fisherman from ImageNet 256 [17].\n    5. Figure 4 first row: Racoon image from the web.\n    6. Figure 4 second row: Fisherman from ImageNet 256 [17].\n    7. Figure 15: Celebrity face from the web.\nB.2      Additional Experimental Evaluation\nHere, we provide additional results to support our theoretical claims on various inverse problems.\n    Figures 6, 7, 8, and 9 show the inpainting results of user defined masks obtained from our PSLD\ninpainting web demo. Note that the foundation model used in this demo is a generic model. For better\nperformance on specific images, we recommend finetuning the foundation model on this class and then\nrunning posterior sampling using our web demo:                  https://huggingface.co/spaces/PSLD/PSLD.\n    Figure 10 and 11 illustrate         super-resolution         (4\u00d7) of in-distribution samples from the validation\nset of FFHQ 256. Observe that the samples generated by DPS are far from the groundtruth sample.\nOn the other hand, the samples generated by PSLD closely capture the perceptual quality of the\ngroundtruth sample. In other words, one may identify (b) and (c) as images of two different individuals,\nwhereas (b) and (d) of the same individual. We attribute this                  photorealism    of our method to the power\nof Stable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder\nin the gluing objective.                                        24", "md": "# PSLD Image Inpainting\n\n## PSLD Image Inpainting\n\nImage inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user-defined mask, click on Inpaint to generate missing parts.\n\n|Upload|Output|\n|---|---|\n|Number of diffusion steps (e.g. 200)|200|\n|Gluing factor (e.g. 1e-1)|Outputz|\n|Gluing kernel size| |\n|Gluing kernel sigma (e.g. 7)| |\n|Measurement factor (e.g. 1)| |\n|Your prompt (leave empty for posterior sampling)|Inpaintl|\n\nFigure 6: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A dinner date between a robot couple during sunset\".\n\n4. Figure 2 third row, Fisherman from ImageNet 256 [17].\n\n5. Figure 4 first row: Racoon image from the web.\n\n6. Figure 4 second row: Fisherman from ImageNet 256 [17].\n\n7. Figure 15: Celebrity face from the web.\n\n### Additional Experimental Evaluation\n\nHere, we provide additional results to support our theoretical claims on various inverse problems.\n\nFigures 6, 7, 8, and 9 show the inpainting results of user-defined masks obtained from our PSLD inpainting web demo. Note that the foundation model used in this demo is a generic model. For better performance on specific images, we recommend finetuning the foundation model on this class and then running posterior sampling using our web demo: https://huggingface.co/spaces/PSLD/PSLD.\n\nFigure 10 and 11 illustrate super-resolution (4x) of in-distribution samples from the validation set of FFHQ 256. Observe that the samples generated by DPS are far from the ground truth sample. On the other hand, the samples generated by PSLD closely capture the perceptual quality of the ground truth sample. In other words, one may identify (b) and (c) as images of two different individuals, whereas (b) and (d) of the same individual. We attribute this photorealism of our method to the power of Stable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder in the gluing objective.", "images": [{"name": "img_p23_1", "height": 945, "width": 1448}], "items": [{"type": "heading", "lvl": 1, "value": "PSLD Image Inpainting", "md": "# PSLD Image Inpainting"}, {"type": "heading", "lvl": 2, "value": "PSLD Image Inpainting", "md": "## PSLD Image Inpainting"}, {"type": "text", "value": "Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user-defined mask, click on Inpaint to generate missing parts.", "md": "Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user-defined mask, click on Inpaint to generate missing parts."}, {"type": "table", "rows": [["Upload", "Output"], ["Number of diffusion steps (e.g. 200)", "200"], ["Gluing factor (e.g. 1e-1)", "Outputz"], ["Gluing kernel size", ""], ["Gluing kernel sigma (e.g. 7)", ""], ["Measurement factor (e.g. 1)", ""], ["Your prompt (leave empty for posterior sampling)", "Inpaintl"]], "md": "|Upload|Output|\n|---|---|\n|Number of diffusion steps (e.g. 200)|200|\n|Gluing factor (e.g. 1e-1)|Outputz|\n|Gluing kernel size| |\n|Gluing kernel sigma (e.g. 7)| |\n|Measurement factor (e.g. 1)| |\n|Your prompt (leave empty for posterior sampling)|Inpaintl|", "isPerfectTable": true, "csv": "\"Upload\",\"Output\"\n\"Number of diffusion steps (e.g. 200)\",\"200\"\n\"Gluing factor (e.g. 1e-1)\",\"Outputz\"\n\"Gluing kernel size\",\"\"\n\"Gluing kernel sigma (e.g. 7)\",\"\"\n\"Measurement factor (e.g. 1)\",\"\"\n\"Your prompt (leave empty for posterior sampling)\",\"Inpaintl\""}, {"type": "text", "value": "Figure 6: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A dinner date between a robot couple during sunset\".\n\n4. Figure 2 third row, Fisherman from ImageNet 256 [17].\n\n5. Figure 4 first row: Racoon image from the web.\n\n6. Figure 4 second row: Fisherman from ImageNet 256 [17].\n\n7. Figure 15: Celebrity face from the web.", "md": "Figure 6: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A dinner date between a robot couple during sunset\".\n\n4. Figure 2 third row, Fisherman from ImageNet 256 [17].\n\n5. Figure 4 first row: Racoon image from the web.\n\n6. Figure 4 second row: Fisherman from ImageNet 256 [17].\n\n7. Figure 15: Celebrity face from the web."}, {"type": "heading", "lvl": 3, "value": "Additional Experimental Evaluation", "md": "### Additional Experimental Evaluation"}, {"type": "text", "value": "Here, we provide additional results to support our theoretical claims on various inverse problems.\n\nFigures 6, 7, 8, and 9 show the inpainting results of user-defined masks obtained from our PSLD inpainting web demo. Note that the foundation model used in this demo is a generic model. For better performance on specific images, we recommend finetuning the foundation model on this class and then running posterior sampling using our web demo: https://huggingface.co/spaces/PSLD/PSLD.\n\nFigure 10 and 11 illustrate super-resolution (4x) of in-distribution samples from the validation set of FFHQ 256. Observe that the samples generated by DPS are far from the ground truth sample. On the other hand, the samples generated by PSLD closely capture the perceptual quality of the ground truth sample. In other words, one may identify (b) and (c) as images of two different individuals, whereas (b) and (d) of the same individual. We attribute this photorealism of our method to the power of Stable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder in the gluing objective.", "md": "Here, we provide additional results to support our theoretical claims on various inverse problems.\n\nFigures 6, 7, 8, and 9 show the inpainting results of user-defined masks obtained from our PSLD inpainting web demo. Note that the foundation model used in this demo is a generic model. For better performance on specific images, we recommend finetuning the foundation model on this class and then running posterior sampling using our web demo: https://huggingface.co/spaces/PSLD/PSLD.\n\nFigure 10 and 11 illustrate super-resolution (4x) of in-distribution samples from the validation set of FFHQ 256. Observe that the samples generated by DPS are far from the ground truth sample. On the other hand, the samples generated by PSLD closely capture the perceptual quality of the ground truth sample. In other words, one may identify (b) and (c) as images of two different individuals, whereas (b) and (d) of the same individual. We attribute this photorealism of our method to the power of Stable Diffusion foundation model and the ability to use the knowledge of the VAE encoder-decoder in the gluing objective."}]}, {"page": 25, "text": "                                                       PSLD Image Inpainting\n                                                 Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n                                        Given an image (square size preferred) anduser defined mask, click on Inpaint to generate missing parts:\n     Upload                                                          Outputl\n    Number of diffusion steps (eg: 200)                     200      Output2\n    Gluing factor (e g; le-1)\n    Gluing kernel size (e.g. 15}\n    Gluing kernel sigma (eg:\n              (2.&\n    Measurement factor\n     Your prompt (leave empty for posterior sampling)Inpaint!\nFigure 7: Results from the web application of our PSLD algorithm,                       512  \u00d7512. The original image (1)\nis generated by Stable Diffusion v-2.1 with the prompt,\u201cA panda wearing a spiderman costume\u201d.\n                                                                25", "md": "## PSLD Image Inpainting\n\nImage inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user defined mask, click on Inpaint to generate missing parts:\n\n|Upload|Output|\n|---|---|\n|Number of diffusion steps (e.g. 200)|200|\n|Gluing factor (e.g. 1e-1)| |\n|Gluing kernel size (e.g. 15)| |\n|Gluing kernel sigma (e.g. 2)| |\n|Measurement factor| |\n|Your prompt (leave empty for posterior sampling)|Inpaint!|\n\nFigure 7: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A panda wearing a spiderman costume\".", "images": [{"name": "img_p24_1", "height": 940, "width": 1448}], "items": [{"type": "heading", "lvl": 2, "value": "PSLD Image Inpainting", "md": "## PSLD Image Inpainting"}, {"type": "text", "value": "Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user defined mask, click on Inpaint to generate missing parts:", "md": "Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user defined mask, click on Inpaint to generate missing parts:"}, {"type": "table", "rows": [["Upload", "Output"], ["Number of diffusion steps (e.g. 200)", "200"], ["Gluing factor (e.g. 1e-1)", ""], ["Gluing kernel size (e.g. 15)", ""], ["Gluing kernel sigma (e.g. 2)", ""], ["Measurement factor", ""], ["Your prompt (leave empty for posterior sampling)", "Inpaint!"]], "md": "|Upload|Output|\n|---|---|\n|Number of diffusion steps (e.g. 200)|200|\n|Gluing factor (e.g. 1e-1)| |\n|Gluing kernel size (e.g. 15)| |\n|Gluing kernel sigma (e.g. 2)| |\n|Measurement factor| |\n|Your prompt (leave empty for posterior sampling)|Inpaint!|", "isPerfectTable": true, "csv": "\"Upload\",\"Output\"\n\"Number of diffusion steps (e.g. 200)\",\"200\"\n\"Gluing factor (e.g. 1e-1)\",\"\"\n\"Gluing kernel size (e.g. 15)\",\"\"\n\"Gluing kernel sigma (e.g. 2)\",\"\"\n\"Measurement factor\",\"\"\n\"Your prompt (leave empty for posterior sampling)\",\"Inpaint!\""}, {"type": "text", "value": "Figure 7: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A panda wearing a spiderman costume\".", "md": "Figure 7: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A panda wearing a spiderman costume\"."}]}, {"page": 26, "text": "                                                        PSLD Image Inpainting\n                                                  Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n                                        Given =an image (square size preferred) anduser defined mask, click on Inpaint to generate missing parts:\n    Upload                                                            Outputl\n   Number of diffusion steps (eg: 200)                       200      Output2\n   Gluing factor (e g 1e-1)\n   Gluingkernel size (e g. 151\n   Gluing kernel sigma (eg. 7)\n   Measurement factor (e g. 1)\n    Your prompt (leave empty for posterior sampling)Inpaint!\nFigure 8: Results from the web application of our PSLD algorithm,                          512  \u00d7512. The original image (1)\nis generated by Stable Diffusion v-2.1 with the prompt,\u201cA teddy bear showing stop sign at the traffic\u201d.\n                                                                  26", "md": "## PSLD Image Inpainting\n\nImage inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user-defined mask, click on Inpaint to generate missing parts:\n\n|Upload|Output|\n|---|---|\n|Number of diffusion steps (e.g. 200)|200|\n|Gluing factor (e.g. 1e-1)| |\n|Gluing kernel size (e.g. 151)| |\n|Gluing kernel sigma (e.g. 7)| |\n|Measurement factor (e.g. 1)| |\n|Your prompt (leave empty for posterior sampling)|Inpaint!|\n\nFigure 8: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A teddy bear showing stop sign at the traffic\".", "images": [{"name": "img_p25_1", "height": 938, "width": 1448}], "items": [{"type": "heading", "lvl": 2, "value": "PSLD Image Inpainting", "md": "## PSLD Image Inpainting"}, {"type": "text", "value": "Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user-defined mask, click on Inpaint to generate missing parts:", "md": "Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user-defined mask, click on Inpaint to generate missing parts:"}, {"type": "table", "rows": [["Upload", "Output"], ["Number of diffusion steps (e.g. 200)", "200"], ["Gluing factor (e.g. 1e-1)", ""], ["Gluing kernel size (e.g. 151)", ""], ["Gluing kernel sigma (e.g. 7)", ""], ["Measurement factor (e.g. 1)", ""], ["Your prompt (leave empty for posterior sampling)", "Inpaint!"]], "md": "|Upload|Output|\n|---|---|\n|Number of diffusion steps (e.g. 200)|200|\n|Gluing factor (e.g. 1e-1)| |\n|Gluing kernel size (e.g. 151)| |\n|Gluing kernel sigma (e.g. 7)| |\n|Measurement factor (e.g. 1)| |\n|Your prompt (leave empty for posterior sampling)|Inpaint!|", "isPerfectTable": true, "csv": "\"Upload\",\"Output\"\n\"Number of diffusion steps (e.g. 200)\",\"200\"\n\"Gluing factor (e.g. 1e-1)\",\"\"\n\"Gluing kernel size (e.g. 151)\",\"\"\n\"Gluing kernel sigma (e.g. 7)\",\"\"\n\"Measurement factor (e.g. 1)\",\"\"\n\"Your prompt (leave empty for posterior sampling)\",\"Inpaint!\""}, {"type": "text", "value": "Figure 8: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A teddy bear showing stop sign at the traffic\".", "md": "Figure 8: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A teddy bear showing stop sign at the traffic\"."}]}, {"page": 27, "text": "                                                       PSLD Image Inpainting\n                                                 Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n                                       Given an image (square size preferred) anduser defined mask, click on Inpaint to generate missing parts:\n    Upload                                                           Outputl\n   Number of diffusion steps (e g. 200)                     200      Outputz\n   Gluing factor(e.&:le-1)\n   Gluing kernel size (e-g: 15}\n   Gluing kernel sigma (e.g.7)\n   Measurement factor (e.g: 1)\n    Your prompt (leave empty for posterior sampling)Inpaint!\nFigure 9: Results from the web application of our PSLD algorithm,                        512  \u00d7512. The original image (1)\nis generated by Stable Diffusion v-2.1 with the prompt,\u201cA cute dog playing with a toy teddy bear on\nthe lawn\u201d.\n                                                                 27", "md": "## PSLD Image Inpainting\n\nImage inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user defined mask, click on Inpaint to generate missing parts:\n\n|Upload|Output|\n|---|---|\n|Number of diffusion steps (e.g. 200)|200|\n|Gluing factor (e.g. 1e-1)| |\n|Gluing kernel size (e.g. 15)| |\n|Gluing kernel sigma (e.g. 7)| |\n|Measurement factor (e.g. 1)| |\n|Your prompt (leave empty for posterior sampling)|Inpaint!|\n\nFigure 9: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A cute dog playing with a toy teddy bear on the lawn\".\n\n27", "images": [{"name": "img_p26_1", "height": 940, "width": 1448}], "items": [{"type": "heading", "lvl": 2, "value": "PSLD Image Inpainting", "md": "## PSLD Image Inpainting"}, {"type": "text", "value": "Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user defined mask, click on Inpaint to generate missing parts:", "md": "Image inpainting by Posterior Sampling with Latent Diffusion (PSLD)\n\nGiven an image (square size preferred) and user defined mask, click on Inpaint to generate missing parts:"}, {"type": "table", "rows": [["Upload", "Output"], ["Number of diffusion steps (e.g. 200)", "200"], ["Gluing factor (e.g. 1e-1)", ""], ["Gluing kernel size (e.g. 15)", ""], ["Gluing kernel sigma (e.g. 7)", ""], ["Measurement factor (e.g. 1)", ""], ["Your prompt (leave empty for posterior sampling)", "Inpaint!"]], "md": "|Upload|Output|\n|---|---|\n|Number of diffusion steps (e.g. 200)|200|\n|Gluing factor (e.g. 1e-1)| |\n|Gluing kernel size (e.g. 15)| |\n|Gluing kernel sigma (e.g. 7)| |\n|Measurement factor (e.g. 1)| |\n|Your prompt (leave empty for posterior sampling)|Inpaint!|", "isPerfectTable": true, "csv": "\"Upload\",\"Output\"\n\"Number of diffusion steps (e.g. 200)\",\"200\"\n\"Gluing factor (e.g. 1e-1)\",\"\"\n\"Gluing kernel size (e.g. 15)\",\"\"\n\"Gluing kernel sigma (e.g. 7)\",\"\"\n\"Measurement factor (e.g. 1)\",\"\"\n\"Your prompt (leave empty for posterior sampling)\",\"Inpaint!\""}, {"type": "text", "value": "Figure 9: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A cute dog playing with a toy teddy bear on the lawn\".\n\n27", "md": "Figure 9: Results from the web application of our PSLD algorithm, 512 x 512. The original image (1) is generated by Stable Diffusion v-2.1 with the prompt, \"A cute dog playing with a toy teddy bear on the lawn\".\n\n27"}]}, {"page": 28, "text": "                            (a) Input                                          (b) Groundtruth\n                          (c) DPS [11]                                        (d) PSLD (Ours)\n        Figure 10: Super-resolution results on images from FFHQ 256 [25, 11] (in distribution).\n     In addition, we test on out-of-distribution samples from ImageNet [17] validation set. Figure 12\nand Figure 13 show the results in        motion deblur      and   Gaussian deblur, respectively. By leveraging\nthe foundation model Stable Diffusion v1.5, our PSLD method clearly outperforms DPS [11] in the\ngeneral domain. Further, Figures 14, 15, and 16 show reconstruction of general domain samples for\nrandom inpainting,         super-resolution, and        destriping     tasks, respectively. In all these tasks, the\nsamples generated by PSLD are closer to the groundtruth sample than the ones generated by DPS.\nTable 5 shows the quantitative results.\n                                                           28", "md": "", "images": [{"name": "img_p27_1", "height": 64, "width": 64}, {"name": "img_p27_2", "height": 652, "width": 652}, {"name": "img_p27_3", "height": 256, "width": 256}, {"name": "img_p27_4", "height": 256, "width": 256}], "items": []}, {"page": 29, "text": "         (a) Input                             (b) Groundtruth\n        (c) DPS [11]                           (d) PSLD (Ours)\nFigure 11: Super-resolution results on FFHQ 256 [25, 11] (in distribution).\n                                29", "md": "$$\n\\begin{array}{cc}\n\\text{(a) Input} & \\text{(b) Groundtruth} \\\\\n\\text{(c) DPS [11]} & \\text{(d) PSLD (Ours)}\n\\end{array}\n$$\n\nFigure 11: Super-resolution results on FFHQ 256 [25, 11] (in distribution).\n29", "images": [{"name": "img_p28_1", "height": 64, "width": 64}, {"name": "img_p28_2", "height": 652, "width": 652}, {"name": "img_p28_3", "height": 256, "width": 256}, {"name": "img_p28_4", "height": 256, "width": 256}], "items": [{"type": "text", "value": "$$\n\\begin{array}{cc}\n\\text{(a) Input} & \\text{(b) Groundtruth} \\\\\n\\text{(c) DPS [11]} & \\text{(d) PSLD (Ours)}\n\\end{array}\n$$\n\nFigure 11: Super-resolution results on FFHQ 256 [25, 11] (in distribution).\n29", "md": "$$\n\\begin{array}{cc}\n\\text{(a) Input} & \\text{(b) Groundtruth} \\\\\n\\text{(c) DPS [11]} & \\text{(d) PSLD (Ours)}\n\\end{array}\n$$\n\nFigure 11: Super-resolution results on FFHQ 256 [25, 11] (in distribution).\n29"}]}, {"page": 30, "text": "       (a) Input          (b) Groundtruth        (c) DPS [11]       (d) PSLD (Ours)\n          Figure 12: Motion deblur results on ImageNet 256 [17] (out-of-distribution).\nTable 5: Quantitative random inpainting results on FFHQ256validation set [25, 11]. We use Stable\nDiffusion (v1.5) trained on LAION.\n                        Inpaint (random)        SR (4\u00d7)          Gaussian Deblur\n    Method            PSNR (\u2191)  SSIM (\u2191)  PSNR (\u2191)   SSIM (\u2191)  PSNR (\u2191)  SSIM (\u2191)\n    PSLD (Ours)       30.31     0.851     30.73      0.867     30.10     0.843\n    GML-DPS (Ours)    29.49     0.844     29.77      0.860     29.21     0.820\n    DPS [11]          25.23     0.851     25.67      0.852     24.25     0.811\n    DDRM [26]         9.19      0.319     25.36      0.835     23.36     0.767\n    MCG [13]          21.57     0.751     20.05      0.559     6.72      0.051\n    PnP-ADMM [6]      8.41      0.325     26.55      0.865     24.93     0.812\n    Score-SDE [47]    13.52     0.437     17.62      0.617     7.12      0.109\n    ADMM-TV           22.03     0.784     23.86      0.803     22.37     0.801\n                                          30", "md": "```markdown\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\text{Inpaint (random)} & \\text{SR (4\u00d7)} & \\text{Gaussian Deblur} \\\\\n\\hline\n\\text{PSLD (Ours)} & 30.31 & 0.851 & 30.73 & 0.867 & 30.10 & 0.843 \\\\\n\\text{GML-DPS (Ours)} & 29.49 & 0.844 & 29.77 & 0.860 & 29.21 & 0.820 \\\\\n\\text{DPS [11]} & 25.23 & 0.851 & 25.67 & 0.852 & 24.25 & 0.811 \\\\\n\\text{DDRM [26]} & 9.19 & 0.319 & 25.36 & 0.835 & 23.36 & 0.767 \\\\\n\\text{MCG [13]} & 21.57 & 0.751 & 20.05 & 0.559 & 6.72 & 0.051 \\\\\n\\text{PnP-ADMM [6]} & 8.41 & 0.325 & 26.55 & 0.865 & 24.93 & 0.812 \\\\\n\\text{Score-SDE [47]} & 13.52 & 0.437 & 17.62 & 0.617 & 7.12 & 0.109 \\\\\n\\text{ADMM-TV} & 22.03 & 0.784 & 23.86 & 0.803 & 22.37 & 0.801 \\\\\n\\hline\n\\end{array}\n$$\n```", "images": [{"name": "img_p29_1", "height": 348, "width": 348}, {"name": "img_p29_2", "height": 348, "width": 348}, {"name": "img_p29_3", "height": 348, "width": 348}, {"name": "img_p29_4", "height": 348, "width": 348}, {"name": "img_p29_5", "height": 348, "width": 348}, {"name": "img_p29_6", "height": 348, "width": 348}, {"name": "img_p29_7", "height": 348, "width": 348}, {"name": "img_p29_8", "height": 348, "width": 348}, {"name": "img_p29_9", "height": 348, "width": 348}, {"name": "img_p29_10", "height": 348, "width": 348}, {"name": "img_p29_11", "height": 348, "width": 348}, {"name": "img_p29_12", "height": 348, "width": 348}], "items": [{"type": "text", "value": "```markdown\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\text{Inpaint (random)} & \\text{SR (4\u00d7)} & \\text{Gaussian Deblur} \\\\\n\\hline\n\\text{PSLD (Ours)} & 30.31 & 0.851 & 30.73 & 0.867 & 30.10 & 0.843 \\\\\n\\text{GML-DPS (Ours)} & 29.49 & 0.844 & 29.77 & 0.860 & 29.21 & 0.820 \\\\\n\\text{DPS [11]} & 25.23 & 0.851 & 25.67 & 0.852 & 24.25 & 0.811 \\\\\n\\text{DDRM [26]} & 9.19 & 0.319 & 25.36 & 0.835 & 23.36 & 0.767 \\\\\n\\text{MCG [13]} & 21.57 & 0.751 & 20.05 & 0.559 & 6.72 & 0.051 \\\\\n\\text{PnP-ADMM [6]} & 8.41 & 0.325 & 26.55 & 0.865 & 24.93 & 0.812 \\\\\n\\text{Score-SDE [47]} & 13.52 & 0.437 & 17.62 & 0.617 & 7.12 & 0.109 \\\\\n\\text{ADMM-TV} & 22.03 & 0.784 & 23.86 & 0.803 & 22.37 & 0.801 \\\\\n\\hline\n\\end{array}\n$$\n```", "md": "```markdown\n$$\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\text{Method} & \\text{Inpaint (random)} & \\text{SR (4\u00d7)} & \\text{Gaussian Deblur} \\\\\n\\hline\n\\text{PSLD (Ours)} & 30.31 & 0.851 & 30.73 & 0.867 & 30.10 & 0.843 \\\\\n\\text{GML-DPS (Ours)} & 29.49 & 0.844 & 29.77 & 0.860 & 29.21 & 0.820 \\\\\n\\text{DPS [11]} & 25.23 & 0.851 & 25.67 & 0.852 & 24.25 & 0.811 \\\\\n\\text{DDRM [26]} & 9.19 & 0.319 & 25.36 & 0.835 & 23.36 & 0.767 \\\\\n\\text{MCG [13]} & 21.57 & 0.751 & 20.05 & 0.559 & 6.72 & 0.051 \\\\\n\\text{PnP-ADMM [6]} & 8.41 & 0.325 & 26.55 & 0.865 & 24.93 & 0.812 \\\\\n\\text{Score-SDE [47]} & 13.52 & 0.437 & 17.62 & 0.617 & 7.12 & 0.109 \\\\\n\\text{ADMM-TV} & 22.03 & 0.784 & 23.86 & 0.803 & 22.37 & 0.801 \\\\\n\\hline\n\\end{array}\n$$\n```"}]}, {"page": 31, "text": "(a) Input              (b) Groundtruth              (c) DPS [11]            (d) PSLD (Ours)\n   Figure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution).\n                                           31", "md": "(a) Input              (b) Groundtruth              (c) DPS [11]            (d) PSLD (Ours)\n\nFigure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution).\n\n31", "images": [{"name": "img_p30_1", "height": 348, "width": 348}, {"name": "img_p30_2", "height": 348, "width": 348}, {"name": "img_p30_3", "height": 348, "width": 348}, {"name": "img_p30_4", "height": 348, "width": 348}, {"name": "img_p30_5", "height": 348, "width": 348}, {"name": "img_p30_6", "height": 348, "width": 348}, {"name": "img_p30_7", "height": 348, "width": 348}, {"name": "img_p30_8", "height": 348, "width": 348}, {"name": "img_p30_9", "height": 348, "width": 348}, {"name": "img_p30_10", "height": 348, "width": 348}, {"name": "img_p30_11", "height": 348, "width": 348}, {"name": "img_p30_12", "height": 348, "width": 348}], "items": [{"type": "text", "value": "(a) Input              (b) Groundtruth              (c) DPS [11]            (d) PSLD (Ours)\n\nFigure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution).\n\n31", "md": "(a) Input              (b) Groundtruth              (c) DPS [11]            (d) PSLD (Ours)\n\nFigure 13: Gaussian deblur results on ImageNet 256 [17] (out-of-distribution).\n\n31"}]}, {"page": 32, "text": "                              white com                                               (Qtcon\n(a) Input              (b) Groundtruth              (c) DPS [11]            (d) PSLD (Ours)\n Figure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).\n                                           32", "md": "# Random inpainting results on ImageNet 256 (out-of-distribution)\n\n(a) Input\n(b) Groundtruth\n(c) DPS [11]\n(d) PSLD (Ours)\nFigure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).\n\n$$32$$", "images": [{"name": "img_p31_1", "height": 348, "width": 348}, {"name": "img_p31_2", "height": 348, "width": 348}, {"name": "img_p31_3", "height": 348, "width": 348}, {"name": "img_p31_4", "height": 348, "width": 348}, {"name": "img_p31_5", "height": 348, "width": 348}, {"name": "img_p31_6", "height": 348, "width": 348}, {"name": "img_p31_7", "height": 348, "width": 348}, {"name": "img_p31_8", "height": 348, "width": 348}, {"name": "img_p31_9", "height": 348, "width": 348}, {"name": "img_p31_10", "height": 348, "width": 348}, {"name": "img_p31_11", "height": 348, "width": 348}, {"name": "img_p31_12", "height": 348, "width": 348}, {"name": "img_p31_13", "height": 348, "width": 348}, {"name": "img_p31_14", "height": 348, "width": 348}, {"name": "img_p31_15", "height": 348, "width": 348}, {"name": "img_p31_16", "height": 348, "width": 348}, {"name": "img_p31_17", "height": 348, "width": 348}, {"name": "img_p31_18", "height": 348, "width": 348}, {"name": "img_p31_19", "height": 348, "width": 348}, {"name": "img_p31_20", "height": 348, "width": 348}], "items": [{"type": "heading", "lvl": 1, "value": "Random inpainting results on ImageNet 256 (out-of-distribution)", "md": "# Random inpainting results on ImageNet 256 (out-of-distribution)"}, {"type": "text", "value": "(a) Input\n(b) Groundtruth\n(c) DPS [11]\n(d) PSLD (Ours)\nFigure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).\n\n$$32$$", "md": "(a) Input\n(b) Groundtruth\n(c) DPS [11]\n(d) PSLD (Ours)\nFigure 14: Random inpainting results on ImageNet 256 [17] (out-of-distribution).\n\n$$32$$"}]}, {"page": 33, "text": "          (a) Input                (b) Groundtruth                (c) DPS [11]              (d) PSLD (Ours)\nFigure 15: Super-resolution (using nearest neighbor kernel from [31]) results on out-of-distribution\nsamples from the web,      256 \u00d7256   (see Table 2 for LPIPS of these images).\n                                                         33", "md": "```markdown\n(a) Input                (b) Groundtruth                (c) DPS [11]              (d) PSLD (Ours)\n$$Figure 15: Super-resolution (using nearest neighbor kernel from [31]) results on out-of-distribution\nsamples from the web,      256 \\times 256$$   (see Table 2 for LPIPS of these images).\n33\n```", "images": [{"name": "img_p32_1", "height": 348, "width": 348}, {"name": "img_p32_2", "height": 348, "width": 348}, {"name": "img_p32_3", "height": 348, "width": 348}, {"name": "img_p32_4", "height": 348, "width": 348}, {"name": "img_p32_5", "height": 348, "width": 348}, {"name": "img_p32_6", "height": 348, "width": 348}, {"name": "img_p32_7", "height": 348, "width": 348}, {"name": "img_p32_8", "height": 348, "width": 348}, {"name": "img_p32_9", "height": 348, "width": 348}, {"name": "img_p32_10", "height": 348, "width": 348}, {"name": "img_p32_11", "height": 348, "width": 348}, {"name": "img_p32_12", "height": 348, "width": 348}], "items": [{"type": "text", "value": "```markdown\n(a) Input                (b) Groundtruth                (c) DPS [11]              (d) PSLD (Ours)\n$$Figure 15: Super-resolution (using nearest neighbor kernel from [31]) results on out-of-distribution\nsamples from the web,      256 \\times 256$$   (see Table 2 for LPIPS of these images).\n33\n```", "md": "```markdown\n(a) Input                (b) Groundtruth                (c) DPS [11]              (d) PSLD (Ours)\n$$Figure 15: Super-resolution (using nearest neighbor kernel from [31]) results on out-of-distribution\nsamples from the web,      256 \\times 256$$   (see Table 2 for LPIPS of these images).\n33\n```"}]}, {"page": 34, "text": "          (a) Input               (b) Groundtruth                (c) DPS [11]              (d) PSLD (Ours)\nFigure 16: Destriping results on out-of-distribution samples from the web,              256  \u00d7 256. (Top row)\nHorizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (Bottom row) Vertical destriping:\nLPIPS of PSLD=0.255, DPS [11]=0.597.\n                                                        34", "md": "```markdown\n(a) Input               (b) Groundtruth                (c) DPS [11]              (d) PSLD (Ours)\nFigure 16: Destriping results on out-of-distribution samples from the web,              256  \u00d7 256. (Top row)\nHorizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (Bottom row) Vertical destriping:\nLPIPS of PSLD=0.255, DPS [11]=0.597.\n34\n```", "images": [{"name": "img_p33_1", "height": 348, "width": 348}, {"name": "img_p33_2", "height": 348, "width": 348}, {"name": "img_p33_3", "height": 348, "width": 348}, {"name": "img_p33_4", "height": 348, "width": 348}, {"name": "img_p33_5", "height": 348, "width": 348}, {"name": "img_p33_6", "height": 348, "width": 348}, {"name": "img_p33_7", "height": 348, "width": 348}, {"name": "img_p33_8", "height": 348, "width": 348}], "items": [{"type": "text", "value": "```markdown\n(a) Input               (b) Groundtruth                (c) DPS [11]              (d) PSLD (Ours)\nFigure 16: Destriping results on out-of-distribution samples from the web,              256  \u00d7 256. (Top row)\nHorizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (Bottom row) Vertical destriping:\nLPIPS of PSLD=0.255, DPS [11]=0.597.\n34\n```", "md": "```markdown\n(a) Input               (b) Groundtruth                (c) DPS [11]              (d) PSLD (Ours)\nFigure 16: Destriping results on out-of-distribution samples from the web,              256  \u00d7 256. (Top row)\nHorizontal destriping: LPIPS of PSLD=0.244 and DPS [11]=0.613. (Bottom row) Vertical destriping:\nLPIPS of PSLD=0.255, DPS [11]=0.597.\n34\n```"}]}], "job_id": "741e48d3-56da-4e24-9fa0-3c7dbd5785e0", "file_path": "./corpus/2307.00619.pdf"}