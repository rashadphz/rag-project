{"pages": [{"page": 1, "text": "                                          Streaming PCA for Markovian Data\n                                              Syamantak Kumar*1 and Purnamrita Sarkar \u20202\n                           2     1 Department of Computer Science, University of Texas at Austin\n                            Department of Statistics and Data Sciences, University of Texas at Austin\narXiv:2305.02456v2  [math.ST]  16 Jun 2023\n                                                                         Abstract\n                                Since its inception in 1982, Oja\u2019s algorithm has become an established method for streaming\n                            principle component analysis (PCA). We study the problem of streaming PCA, where the data-\n                            points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to\n                            estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This\n                            setting has implications in scenarios where data can solely be sampled from a Markov Chain Monte\n                            Carlo (MCMC) type algorithm, and the objective is to perform inference on parameters of the\n                            stationary distribution. Most convergence guarantees for Oja\u2019s algorithm in the literature assume\n                            that the data-points are sampled IID. For data streams with Markovian dependence, one typically\n                            downsamples the data to get a \"nearly\" independent data stream. In this paper, we obtain the first\n                            sharp rate for Oja\u2019s algorithm on the entire data, where we remove the logarithmic dependence on\n                            the sample size, n, resulting from throwing data away in downsampling strategies.\n                     1     Introduction\n                     Streaming Principal Component Analysis (PCA) is an important and well studied problem where the\n                     principal eigenvector of the sample covariance matrix of a dataset is computed one data-point at a\n                     time. One of the most popular algorithms for streaming PCA was introduced by Erikki Oja in 1982\n                     [28, 29]. Most existing analyses of Oja\u2019s algorithm are done when the data is sampled IID.\n                          However, in many practical applications, the data-points are dependent and are sampled from an\n                     MCMC process converging to a target stationary distribution. This naturally arises in the context of\n                     token algorithms for Federated PCA settings [10, 11, 12] with multiple machines communicating via\n                     a fixed and connected graph topology. Each machine contains an arbitrary fraction of data-points\n                     and the goal is to design a streaming algorithm that respects this topology and returns the principal\n                     component of the whole dataset. This is typically achieved using a Metropolis-Hastings scheme that\n                     uses local information to design the transition matrix of a Markov chain with any desired stationary\n                     distribution. The stationary distribution, \u03c0, of the random walk is chosen so that the distribution of\n                     the samples under \u03c0 matches the uniform distribution over data-points. Governed by this Markov\n                     chain, a random walker then travels the network of machines and samples one data-point at a time\n                     from the current machine, and computes the update. However, even under the stationary distribution,\n                     the data-points are dependent, which deviates from the IID setup. Our goal is to obtain a sharp\n                     analysis of the sin2 error of the estimated vector w.r.t true top eigenvector of the unknown covariance\n                     matrix in the Markovian setting.\n                     Estimating the first principal component with streaming PCA                      : Let Xt be a mean zero d\n                     dimensional vector with covariance matrix \u03a3, and let \u03b7t be a decaying learning rate. The update rule\n                     of Oja\u2019s algorithm is given as -\n                                                     wt \u2190    (I + \u03b7tXtXT                         wt                              (1)\n                        *syamantak@utexas.edu                              t )wt\u22121, wt \u2190       \u2225wt\u22252\n                        \u2020purna.sarkar@utexas.edu\n                                                                             1", "md": "# Streaming PCA for Markovian Data\n\n# Streaming PCA for Markovian Data\n\nSyamantak Kumar*1 and Purnamrita Sarkar \u20202\n\n1 Department of Computer Science, University of Texas at Austin\n\n2 Department of Statistics and Data Sciences, University of Texas at Austin\n\narXiv:2305.02456v2 [math.ST] 16 Jun 2023\n\n## Abstract\n\nSince its inception in 1982, Oja\u2019s algorithm has become an established method for streaming principal component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in scenarios where data can solely be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the objective is to perform inference on parameters of the stationary distribution. Most convergence guarantees for Oja\u2019s algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a \"nearly\" independent data stream. In this paper, we obtain the first sharp rate for Oja\u2019s algorithm on the entire data, where we remove the logarithmic dependence on the sample size, n, resulting from throwing data away in downsampling strategies.\n\n## Introduction\n\nStreaming Principal Component Analysis (PCA) is an important and well studied problem where the principal eigenvector of the sample covariance matrix of a dataset is computed one data-point at a time. One of the most popular algorithms for streaming PCA was introduced by Erikki Oja in 1982 [28, 29]. Most existing analyses of Oja\u2019s algorithm are done when the data is sampled IID.\n\nHowever, in many practical applications, the data-points are dependent and are sampled from an MCMC process converging to a target stationary distribution. This naturally arises in the context of token algorithms for Federated PCA settings [10, 11, 12] with multiple machines communicating via a fixed and connected graph topology. Each machine contains an arbitrary fraction of data-points and the goal is to design a streaming algorithm that respects this topology and returns the principal component of the whole dataset. This is typically achieved using a Metropolis-Hastings scheme that uses local information to design the transition matrix of a Markov chain with any desired stationary distribution. The stationary distribution, \u03c0, of the random walk is chosen so that the distribution of the samples under \u03c0 matches the uniform distribution over data-points. Governed by this Markov chain, a random walker then travels the network of machines and samples one data-point at a time from the current machine, and computes the update. However, even under the stationary distribution, the data-points are dependent, which deviates from the IID setup. Our goal is to obtain a sharp analysis of the sin2 error of the estimated vector w.r.t true top eigenvector of the unknown covariance matrix in the Markovian setting.\n\nEstimating the first principal component with streaming PCA: Let Xt be a mean zero d-dimensional vector with covariance matrix \u03a3, and let \u03b7t be a decaying learning rate. The update rule of Oja\u2019s algorithm is given as -\n\n$$w_t \\leftarrow (I + \\eta_t X_t X^T_t) w_t$$\n\n*syamantak@utexas.edu\n\n\u2020purna.sarkar@utexas.edu", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Streaming PCA for Markovian Data", "md": "# Streaming PCA for Markovian Data"}, {"type": "heading", "lvl": 1, "value": "Streaming PCA for Markovian Data", "md": "# Streaming PCA for Markovian Data"}, {"type": "text", "value": "Syamantak Kumar*1 and Purnamrita Sarkar \u20202\n\n1 Department of Computer Science, University of Texas at Austin\n\n2 Department of Statistics and Data Sciences, University of Texas at Austin\n\narXiv:2305.02456v2 [math.ST] 16 Jun 2023", "md": "Syamantak Kumar*1 and Purnamrita Sarkar \u20202\n\n1 Department of Computer Science, University of Texas at Austin\n\n2 Department of Statistics and Data Sciences, University of Texas at Austin\n\narXiv:2305.02456v2 [math.ST] 16 Jun 2023"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Since its inception in 1982, Oja\u2019s algorithm has become an established method for streaming principal component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in scenarios where data can solely be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the objective is to perform inference on parameters of the stationary distribution. Most convergence guarantees for Oja\u2019s algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a \"nearly\" independent data stream. In this paper, we obtain the first sharp rate for Oja\u2019s algorithm on the entire data, where we remove the logarithmic dependence on the sample size, n, resulting from throwing data away in downsampling strategies.", "md": "Since its inception in 1982, Oja\u2019s algorithm has become an established method for streaming principal component analysis (PCA). We study the problem of streaming PCA, where the data-points are sampled from an irreducible, aperiodic, and reversible Markov chain. Our goal is to estimate the top eigenvector of the unknown covariance matrix of the stationary distribution. This setting has implications in scenarios where data can solely be sampled from a Markov Chain Monte Carlo (MCMC) type algorithm, and the objective is to perform inference on parameters of the stationary distribution. Most convergence guarantees for Oja\u2019s algorithm in the literature assume that the data-points are sampled IID. For data streams with Markovian dependence, one typically downsamples the data to get a \"nearly\" independent data stream. In this paper, we obtain the first sharp rate for Oja\u2019s algorithm on the entire data, where we remove the logarithmic dependence on the sample size, n, resulting from throwing data away in downsampling strategies."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Streaming Principal Component Analysis (PCA) is an important and well studied problem where the principal eigenvector of the sample covariance matrix of a dataset is computed one data-point at a time. One of the most popular algorithms for streaming PCA was introduced by Erikki Oja in 1982 [28, 29]. Most existing analyses of Oja\u2019s algorithm are done when the data is sampled IID.\n\nHowever, in many practical applications, the data-points are dependent and are sampled from an MCMC process converging to a target stationary distribution. This naturally arises in the context of token algorithms for Federated PCA settings [10, 11, 12] with multiple machines communicating via a fixed and connected graph topology. Each machine contains an arbitrary fraction of data-points and the goal is to design a streaming algorithm that respects this topology and returns the principal component of the whole dataset. This is typically achieved using a Metropolis-Hastings scheme that uses local information to design the transition matrix of a Markov chain with any desired stationary distribution. The stationary distribution, \u03c0, of the random walk is chosen so that the distribution of the samples under \u03c0 matches the uniform distribution over data-points. Governed by this Markov chain, a random walker then travels the network of machines and samples one data-point at a time from the current machine, and computes the update. However, even under the stationary distribution, the data-points are dependent, which deviates from the IID setup. Our goal is to obtain a sharp analysis of the sin2 error of the estimated vector w.r.t true top eigenvector of the unknown covariance matrix in the Markovian setting.\n\nEstimating the first principal component with streaming PCA: Let Xt be a mean zero d-dimensional vector with covariance matrix \u03a3, and let \u03b7t be a decaying learning rate. The update rule of Oja\u2019s algorithm is given as -\n\n$$w_t \\leftarrow (I + \\eta_t X_t X^T_t) w_t$$\n\n*syamantak@utexas.edu\n\n\u2020purna.sarkar@utexas.edu", "md": "Streaming Principal Component Analysis (PCA) is an important and well studied problem where the principal eigenvector of the sample covariance matrix of a dataset is computed one data-point at a time. One of the most popular algorithms for streaming PCA was introduced by Erikki Oja in 1982 [28, 29]. Most existing analyses of Oja\u2019s algorithm are done when the data is sampled IID.\n\nHowever, in many practical applications, the data-points are dependent and are sampled from an MCMC process converging to a target stationary distribution. This naturally arises in the context of token algorithms for Federated PCA settings [10, 11, 12] with multiple machines communicating via a fixed and connected graph topology. Each machine contains an arbitrary fraction of data-points and the goal is to design a streaming algorithm that respects this topology and returns the principal component of the whole dataset. This is typically achieved using a Metropolis-Hastings scheme that uses local information to design the transition matrix of a Markov chain with any desired stationary distribution. The stationary distribution, \u03c0, of the random walk is chosen so that the distribution of the samples under \u03c0 matches the uniform distribution over data-points. Governed by this Markov chain, a random walker then travels the network of machines and samples one data-point at a time from the current machine, and computes the update. However, even under the stationary distribution, the data-points are dependent, which deviates from the IID setup. Our goal is to obtain a sharp analysis of the sin2 error of the estimated vector w.r.t true top eigenvector of the unknown covariance matrix in the Markovian setting.\n\nEstimating the first principal component with streaming PCA: Let Xt be a mean zero d-dimensional vector with covariance matrix \u03a3, and let \u03b7t be a decaying learning rate. The update rule of Oja\u2019s algorithm is given as -\n\n$$w_t \\leftarrow (I + \\eta_t X_t X^T_t) w_t$$\n\n*syamantak@utexas.edu\n\n\u2020purna.sarkar@utexas.edu"}]}, {"page": 2, "text": "where wt is the estimate of v1 and \u03b7t is the step-size at timestep t. We aim to analyse the sin2 error\nof Oja\u2019s iterate at timestep t, defined as 1 \u2212                \u27e8wt, v1\u27e92, where v1 is the top eigenvector of \u03a3.\nStreaming PCA in the IID setting:                      For an IID data stream with E [Xi] = 0 and E                      XiXT  i  = \u03a3,\nthere has been a lot of work on determining the non-asymptotic convergence rates for Oja\u2019s algorithm\nand its various adaptations [15, 1, 3, 37, 13, 14, 25, 20, 24]. Amongst these, [15], [1] and [14] match\nthe optimal offline sample complexity bound, suggested by the independent and identically distributed\n(IID) version of Theorem 1 (See Theorem 1.1 in [15]).\n     We consider Oja\u2019s algorithm in the setting where the data is generated from a reversible,\nirreducible, and aperiodic Markov chain with stationary distribution \u03c0. We denote by E\u03c0[.] the\nexpectation under the stationary distribution. In this setting our goal is to estimate the principal\neigenvector of E\u03c0          XiXT   i   . As in the IID setting, E\u03c0[Xi] = 0. The challenge is that the data, even\nwhen it reaches stationarity, is dependent. Here the degree of dependence is captured by the second\neigenvalue in the magnitude of the transition matrix P (denoted as |\u03bb2(P)|) of the Markov chain.\nThis is closely related to the mixing time of a Markov chain [19], denoted as \u03c4mix, which is the time\nafter which the conditional distribution of a state is close in total variational distance to its stationary\ndistribution, \u03c0 (See Section 2.1).\n     Our contribution: Using a series of approximations, we obtain an optimal error rate for the sin2\nerror, which is worse by a factor of 1/(1 \u2212                 |\u03bb2(P)|) from the corresponding error rate of the IID case.\nPrevious work [3] has established rates worse by a poly-logarithmic factor by using downsampling,\ni.e. applying the update on every kth datapoint. In Figure 1, we compare Oja\u2019s algorithm with\nits downsampled and offl            ine variants (see Section 6 for more details on setup). We see that Oja\u2019s\nalgorithm performs significantly better than the downsampled variant, and similarly to the offl                                     ine\nvariant where for the ith data point we compute the eigenvector of the sample covariance matrix of\nall data-points up-to i. Our work provides a concrete and novel result that explains these observations.\nIn Table 1, we compare our bounds with related analyses of Oja\u2019s algorithm. The last row shows that\nwe are the first to obtain an error whose main term is free of logarithmic dependence on n or d for\nstreaming PCA in the Markovian case.\n     We break the logarithmic barrier in previous work by considering a series of approximations\nof finer granularity which uses reversibility of the Markov chain and standard mixing conditions\nof irreducible and aperiodic Markov chains. Our rates are comparable to the recent work of [27]\n(Proposition 1) that establishes an offline error analysis for estimating the principal component of the\nempirical covariance matrix of Markovian data by using a Matrix Bernstein inequality. Our results\nalso imply a linearly convergent decentralized algorithm for streaming PCA in a distributed setting.\nAs a simple byproduct of our theoretical result, we also obtain a rate for Oja\u2019s algorithm applied on\ndownsampled data, which is worse by a factor of log n, as shown in Figure 1. To our knowledge, this\nis the first work that analyzes the Markovian streaming PCA problem without any downsampling that\nmatches the error of the offline algorithm.\n     The crux of our analysis uses the mixing properties of the Markov chain. Strong mixing intuitively\nsays that the conditional distribution of a state s in timestep k given the starting state is exponentially\nclose to the stationary distribution of s, the closeness being measured using the total variation distance.\nAll previous work on Markovian data exploits this property by conditioning on states many time steps\nbefore. However, it is crucial to a) adaptively find how far to look back and b) bound the error of\nthe sequence of matrices we ignore between the current state and the state we are conditioning on.\nObserve that these two components are related. Looking back too far makes the dependence very\nsmall but increases the error resulting from approximating a larger matrix product of intermediate\nmatrices. We present a fine analysis that balances these two parts and then uses spectral theory to\nbound the second part within a factor of a variance parameter that characterizes the variability of the\nmatrices and shows up in the analysis of [15, 27].\n     Related work on streaming PCA and online matrix decomposition on Markovian data:\nAmongst recent work, [3] is very relevant to our setting, since it analyzes Oja\u2019s algorithm with\nMarkovian Data samples. Inspired by the ideas of [8], the authors propose a downsampled version\nof Oja\u2019s algorithm to reduce dependence amongst samples and provide a Stochastic Differential\n                                                                     2", "md": "# Document\n\nwhere \\(w_t\\) is the estimate of \\(v_1\\) and \\(\\eta_t\\) is the step-size at timestep \\(t\\). We aim to analyse the sin2 error of Oja\u2019s iterate at timestep \\(t\\), defined as \\(1 - \\langle w_t, v_1 \\rangle^2\\), where \\(v_1\\) is the top eigenvector of \\(\\Sigma\\).\n\nStreaming PCA in the IID setting: For an IID data stream with \\(E[X_i] = 0\\) and \\(E[X_iX_i^T] = \\Sigma\\), there has been a lot of work on determining the non-asymptotic convergence rates for Oja\u2019s algorithm and its various adaptations [15, 1, 3, 37, 13, 14, 25, 20, 24]. Amongst these, [15], [1] and [14] match the optimal offline sample complexity bound, suggested by the independent and identically distributed (IID) version of Theorem 1 (See Theorem 1.1 in [15]).\n\nWe consider Oja\u2019s algorithm in the setting where the data is generated from a reversible, irreducible, and aperiodic Markov chain with stationary distribution \\(\\pi\\). We denote by \\(E_\\pi[.]\\) the expectation under the stationary distribution. In this setting our goal is to estimate the principal eigenvector of \\(E_\\pi[X_iX_i^T]\\). As in the IID setting, \\(E_\\pi[X_i] = 0\\). The challenge is that the data, even when it reaches stationarity, is dependent. Here the degree of dependence is captured by the second eigenvalue in the magnitude of the transition matrix \\(P\\) (denoted as \\(|\\lambda_2(P)|\\) of the Markov chain. This is closely related to the mixing time of a Markov chain [19], denoted as \\(\\tau_{\\text{mix}}\\), which is the time after which the conditional distribution of a state is close in total variational distance to its stationary distribution, \\(\\pi\\) (See Section 2.1).\n\nOur contribution: Using a series of approximations, we obtain an optimal error rate for the sin2 error, which is worse by a factor of \\(\\frac{1}{1 - |\\lambda_2(P)|}\\) from the corresponding error rate of the IID case. Previous work [3] has established rates worse by a poly-logarithmic factor by using downsampling, i.e. applying the update on every \\(k\\)th datapoint. In Figure 1, we compare Oja\u2019s algorithm with its downsampled and offline variants (see Section 6 for more details on setup). We see that Oja\u2019s algorithm performs significantly better than the downsampled variant, and similarly to the offline variant where for the \\(i\\)th data point we compute the eigenvector of the sample covariance matrix of all data-points up-to \\(i\\). Our work provides a concrete and novel result that explains these observations.\n\nIn Table 1, we compare our bounds with related analyses of Oja\u2019s algorithm. The last row shows that we are the first to obtain an error whose main term is free of logarithmic dependence on \\(n\\) or \\(d\\) for streaming PCA in the Markovian case.\n\nWe break the logarithmic barrier in previous work by considering a series of approximations of finer granularity which uses reversibility of the Markov chain and standard mixing conditions of irreducible and aperiodic Markov chains. Our rates are comparable to the recent work of [27] (Proposition 1) that establishes an offline error analysis for estimating the principal component of the empirical covariance matrix of Markovian data by using a Matrix Bernstein inequality. Our results also imply a linearly convergent decentralized algorithm for streaming PCA in a distributed setting.\n\nAs a simple byproduct of our theoretical result, we also obtain a rate for Oja\u2019s algorithm applied on downsampled data, which is worse by a factor of \\(\\log n\\), as shown in Figure 1. To our knowledge, this is the first work that analyzes the Markovian streaming PCA problem without any downsampling that matches the error of the offline algorithm.\n\nThe crux of our analysis uses the mixing properties of the Markov chain. Strong mixing intuitively says that the conditional distribution of a state \\(s\\) in timestep \\(k\\) given the starting state is exponentially close to the stationary distribution of \\(s\\), the closeness being measured using the total variation distance. All previous work on Markovian data exploits this property by conditioning on states many time steps before. However, it is crucial to a) adaptively find how far to look back and b) bound the error of the sequence of matrices we ignore between the current state and the state we are conditioning on. Observe that these two components are related. Looking back too far makes the dependence very small but increases the error resulting from approximating a larger matrix product of intermediate matrices. We present a fine analysis that balances these two parts and then uses spectral theory to bound the second part within a factor of a variance parameter that characterizes the variability of the matrices and shows up in the analysis of [15, 27].\n\nRelated work on streaming PCA and online matrix decomposition on Markovian data: Amongst recent work, [3] is very relevant to our setting, since it analyzes Oja\u2019s algorithm with Markovian Data samples. Inspired by the ideas of [8], the authors propose a downsampled version of Oja\u2019s algorithm to reduce dependence amongst samples and provide a Stochastic Differential", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "where \\(w_t\\) is the estimate of \\(v_1\\) and \\(\\eta_t\\) is the step-size at timestep \\(t\\). We aim to analyse the sin2 error of Oja\u2019s iterate at timestep \\(t\\), defined as \\(1 - \\langle w_t, v_1 \\rangle^2\\), where \\(v_1\\) is the top eigenvector of \\(\\Sigma\\).\n\nStreaming PCA in the IID setting: For an IID data stream with \\(E[X_i] = 0\\) and \\(E[X_iX_i^T] = \\Sigma\\), there has been a lot of work on determining the non-asymptotic convergence rates for Oja\u2019s algorithm and its various adaptations [15, 1, 3, 37, 13, 14, 25, 20, 24]. Amongst these, [15], [1] and [14] match the optimal offline sample complexity bound, suggested by the independent and identically distributed (IID) version of Theorem 1 (See Theorem 1.1 in [15]).\n\nWe consider Oja\u2019s algorithm in the setting where the data is generated from a reversible, irreducible, and aperiodic Markov chain with stationary distribution \\(\\pi\\). We denote by \\(E_\\pi[.]\\) the expectation under the stationary distribution. In this setting our goal is to estimate the principal eigenvector of \\(E_\\pi[X_iX_i^T]\\). As in the IID setting, \\(E_\\pi[X_i] = 0\\). The challenge is that the data, even when it reaches stationarity, is dependent. Here the degree of dependence is captured by the second eigenvalue in the magnitude of the transition matrix \\(P\\) (denoted as \\(|\\lambda_2(P)|\\) of the Markov chain. This is closely related to the mixing time of a Markov chain [19], denoted as \\(\\tau_{\\text{mix}}\\), which is the time after which the conditional distribution of a state is close in total variational distance to its stationary distribution, \\(\\pi\\) (See Section 2.1).\n\nOur contribution: Using a series of approximations, we obtain an optimal error rate for the sin2 error, which is worse by a factor of \\(\\frac{1}{1 - |\\lambda_2(P)|}\\) from the corresponding error rate of the IID case. Previous work [3] has established rates worse by a poly-logarithmic factor by using downsampling, i.e. applying the update on every \\(k\\)th datapoint. In Figure 1, we compare Oja\u2019s algorithm with its downsampled and offline variants (see Section 6 for more details on setup). We see that Oja\u2019s algorithm performs significantly better than the downsampled variant, and similarly to the offline variant where for the \\(i\\)th data point we compute the eigenvector of the sample covariance matrix of all data-points up-to \\(i\\). Our work provides a concrete and novel result that explains these observations.\n\nIn Table 1, we compare our bounds with related analyses of Oja\u2019s algorithm. The last row shows that we are the first to obtain an error whose main term is free of logarithmic dependence on \\(n\\) or \\(d\\) for streaming PCA in the Markovian case.\n\nWe break the logarithmic barrier in previous work by considering a series of approximations of finer granularity which uses reversibility of the Markov chain and standard mixing conditions of irreducible and aperiodic Markov chains. Our rates are comparable to the recent work of [27] (Proposition 1) that establishes an offline error analysis for estimating the principal component of the empirical covariance matrix of Markovian data by using a Matrix Bernstein inequality. Our results also imply a linearly convergent decentralized algorithm for streaming PCA in a distributed setting.\n\nAs a simple byproduct of our theoretical result, we also obtain a rate for Oja\u2019s algorithm applied on downsampled data, which is worse by a factor of \\(\\log n\\), as shown in Figure 1. To our knowledge, this is the first work that analyzes the Markovian streaming PCA problem without any downsampling that matches the error of the offline algorithm.\n\nThe crux of our analysis uses the mixing properties of the Markov chain. Strong mixing intuitively says that the conditional distribution of a state \\(s\\) in timestep \\(k\\) given the starting state is exponentially close to the stationary distribution of \\(s\\), the closeness being measured using the total variation distance. All previous work on Markovian data exploits this property by conditioning on states many time steps before. However, it is crucial to a) adaptively find how far to look back and b) bound the error of the sequence of matrices we ignore between the current state and the state we are conditioning on. Observe that these two components are related. Looking back too far makes the dependence very small but increases the error resulting from approximating a larger matrix product of intermediate matrices. We present a fine analysis that balances these two parts and then uses spectral theory to bound the second part within a factor of a variance parameter that characterizes the variability of the matrices and shows up in the analysis of [15, 27].\n\nRelated work on streaming PCA and online matrix decomposition on Markovian data: Amongst recent work, [3] is very relevant to our setting, since it analyzes Oja\u2019s algorithm with Markovian Data samples. Inspired by the ideas of [8], the authors propose a downsampled version of Oja\u2019s algorithm to reduce dependence amongst samples and provide a Stochastic Differential", "md": "where \\(w_t\\) is the estimate of \\(v_1\\) and \\(\\eta_t\\) is the step-size at timestep \\(t\\). We aim to analyse the sin2 error of Oja\u2019s iterate at timestep \\(t\\), defined as \\(1 - \\langle w_t, v_1 \\rangle^2\\), where \\(v_1\\) is the top eigenvector of \\(\\Sigma\\).\n\nStreaming PCA in the IID setting: For an IID data stream with \\(E[X_i] = 0\\) and \\(E[X_iX_i^T] = \\Sigma\\), there has been a lot of work on determining the non-asymptotic convergence rates for Oja\u2019s algorithm and its various adaptations [15, 1, 3, 37, 13, 14, 25, 20, 24]. Amongst these, [15], [1] and [14] match the optimal offline sample complexity bound, suggested by the independent and identically distributed (IID) version of Theorem 1 (See Theorem 1.1 in [15]).\n\nWe consider Oja\u2019s algorithm in the setting where the data is generated from a reversible, irreducible, and aperiodic Markov chain with stationary distribution \\(\\pi\\). We denote by \\(E_\\pi[.]\\) the expectation under the stationary distribution. In this setting our goal is to estimate the principal eigenvector of \\(E_\\pi[X_iX_i^T]\\). As in the IID setting, \\(E_\\pi[X_i] = 0\\). The challenge is that the data, even when it reaches stationarity, is dependent. Here the degree of dependence is captured by the second eigenvalue in the magnitude of the transition matrix \\(P\\) (denoted as \\(|\\lambda_2(P)|\\) of the Markov chain. This is closely related to the mixing time of a Markov chain [19], denoted as \\(\\tau_{\\text{mix}}\\), which is the time after which the conditional distribution of a state is close in total variational distance to its stationary distribution, \\(\\pi\\) (See Section 2.1).\n\nOur contribution: Using a series of approximations, we obtain an optimal error rate for the sin2 error, which is worse by a factor of \\(\\frac{1}{1 - |\\lambda_2(P)|}\\) from the corresponding error rate of the IID case. Previous work [3] has established rates worse by a poly-logarithmic factor by using downsampling, i.e. applying the update on every \\(k\\)th datapoint. In Figure 1, we compare Oja\u2019s algorithm with its downsampled and offline variants (see Section 6 for more details on setup). We see that Oja\u2019s algorithm performs significantly better than the downsampled variant, and similarly to the offline variant where for the \\(i\\)th data point we compute the eigenvector of the sample covariance matrix of all data-points up-to \\(i\\). Our work provides a concrete and novel result that explains these observations.\n\nIn Table 1, we compare our bounds with related analyses of Oja\u2019s algorithm. The last row shows that we are the first to obtain an error whose main term is free of logarithmic dependence on \\(n\\) or \\(d\\) for streaming PCA in the Markovian case.\n\nWe break the logarithmic barrier in previous work by considering a series of approximations of finer granularity which uses reversibility of the Markov chain and standard mixing conditions of irreducible and aperiodic Markov chains. Our rates are comparable to the recent work of [27] (Proposition 1) that establishes an offline error analysis for estimating the principal component of the empirical covariance matrix of Markovian data by using a Matrix Bernstein inequality. Our results also imply a linearly convergent decentralized algorithm for streaming PCA in a distributed setting.\n\nAs a simple byproduct of our theoretical result, we also obtain a rate for Oja\u2019s algorithm applied on downsampled data, which is worse by a factor of \\(\\log n\\), as shown in Figure 1. To our knowledge, this is the first work that analyzes the Markovian streaming PCA problem without any downsampling that matches the error of the offline algorithm.\n\nThe crux of our analysis uses the mixing properties of the Markov chain. Strong mixing intuitively says that the conditional distribution of a state \\(s\\) in timestep \\(k\\) given the starting state is exponentially close to the stationary distribution of \\(s\\), the closeness being measured using the total variation distance. All previous work on Markovian data exploits this property by conditioning on states many time steps before. However, it is crucial to a) adaptively find how far to look back and b) bound the error of the sequence of matrices we ignore between the current state and the state we are conditioning on. Observe that these two components are related. Looking back too far makes the dependence very small but increases the error resulting from approximating a larger matrix product of intermediate matrices. We present a fine analysis that balances these two parts and then uses spectral theory to bound the second part within a factor of a variance parameter that characterizes the variability of the matrices and shows up in the analysis of [15, 27].\n\nRelated work on streaming PCA and online matrix decomposition on Markovian data: Amongst recent work, [3] is very relevant to our setting, since it analyzes Oja\u2019s algorithm with Markovian Data samples. Inspired by the ideas of [8], the authors propose a downsampled version of Oja\u2019s algorithm to reduce dependence amongst samples and provide a Stochastic Differential"}]}, {"page": 3, "text": "       Paper          Markov?        Online?       Log-free             sin2 error rate               Sample-Complexity\n                                                  main-term\n                                        Y              Y                 O      V                          O      V\n                                                                              gap2 1n                           gap2 1\u03f5\n    Jain et al.            N                                                 V log(d) 1                       V log(d) 1\n        [15]                            N              N                O     gap2    n                  O      gap2   \u03f5\n    Chen et al.            Y            Y              N                        -                 O      G              G\n         [3]                                                                                           gap2 1\u03f5 log2  gap2 1\u03f5\n  Neeman et al.            Y            N              N          O       V log d2\u2212\u03c0 4              O      V log  d2\u2212\u03c04\n                                                                        (1\u2212|\u03bb2(P )|) gap2 1\n        [27]                                                                              n              (1\u2212|\u03bb2(P )|) gap2 1\u03f5\n   Theorem 1               Y            Y              Y           O    (1\u2212|\u03bb2(PV)|) gap2 1         O             V\n                                                                                          n              (1\u2212|\u03bb2(P )|) gap2 1\u03f5\nTable 1: Comparison of sin2 error rates and sample complexities. Here gap := (\u03bb1 \u2212                        \u03bb2), where\n\u03bb1, \u03bb2 are the top 2 eigenvalues of \u03a3 and the sample complexities represent the number of samples\nrequired to achieve sin2 error at most \u03f5. We note that [1] and [14] also match the online sample\ncomplexity bound provided in [15]. Further, for the offline algorithm with IID data, [16] removes the\nlog (d) factor in exchange for a constant probability of success for large enough n.\nEquation (SDE) based analysis to achieve a sample complexity of O                      G      \u03f5 log2        G     \u03f5\n                                                                                   (\u03bb1\u2212\u03bb2)2 1          (\u03bb1\u2212\u03bb2)2 1\nfor sin2 error smaller than \u03f5, where G is a variance parameter. We obtain a similar rate in Corollary\n1 through our techniques. However, comparing with Theorem 1, we observe that downsampling\nleads to an extra O (log (n)) factor. It is important to point out that [3] provides an analysis for\nestimating top k principal components, whereas this paper focuses on obtaining a sharp rate for\nthe first principal component.          [21] consider the harder problem of online non-negative matrix\nfactorization for Markovian data. Their analysis establishes asymptotic convergence of error, but\ndoes not provide a rate.\n     Stochastic Optimization with Markovian\nData : Markovian models are often considered\nin Reinforcement Learning and Linear Dynamic\nSystems[2, 5, 9, 30, 4, 34, 18, 23].              There\nhave been many notable nonasymptotic bounds\nfor stochastic gradient descent (SGD) methods                                                                      alconthm\nfor general convex and nonconvex functions\nwith Markovian data [8, 31, 6, 7, 10, 38, 33].\nThe convergence rates (sample complexities)\nobtained in these works apply to more general\nproblems but do not exploit the matrix product\nstructure inherent to Oja\u2019s algorithm. In this\nwork, we develop novel techniques to show\nthat a sharper analysis is possible for the PCA\nobjective. The paper is organized as follows.\nSection 2 contains the problem setup and\npreliminaries about Markov Chains. Section 3\ncontains Theorem 1. We present a sketch of the              Figure 1:     Comparison of Oja\u2019s algorithm with\nmain technical tools in Section 4, intermediate             downsampled and offline variants. The X-axis represents\ntheorems needed for the main theorem in                     the sample size and the Y axis represents the sin2 error\nSection 5 and conclude with simulations in                  of each algorithm\u2019s estimate of the leading eigenvector.\nSection 6.                                                  The experimental setup is available in Section 6.\n                                                           3", "md": "# Document\n\n## Table 1: Comparison of sin2 error rates and sample complexities\n\n|Paper|Markov?|Online?|Log-free main-term|sin2 error rate|Sample-Complexity|\n|---|---|---|---|---|---|\n|Jain et al. [15]|N|N|O|gap2 1n|gap2 1\u03f5|\n|Chen et al. [3]|Y|Y|N|-|gap2 1\u03f5 log2 gap2 1\u03f5|\n|Neeman et al. [27]|Y|N|O|V log d2\u2212\u03c0 4|(1\u2212|\u03bb2(P)|) gap2 1n|\n|Theorem 1|Y|Y|O|(1\u2212|\u03bb2(PV)|) gap2 1n|(1\u2212|\u03bb2(P)|) gap2 1\u03f5|\n\nHere gap := (\u03bb1 \u2212 \u03bb2), where \u03bb1, \u03bb2 are the top 2 eigenvalues of \u03a3 and the sample complexities represent the number of samples required to achieve sin2 error at most \u03f5. We note that [1] and [14] also match the online sample complexity bound provided in [15]. Further, for the offline algorithm with IID data, [16] removes the log (d) factor in exchange for a constant probability of success for large enough n.\n\nEquation (SDE) based analysis to achieve a sample complexity of O G \u03f5 log2 G \u03f5 (\u03bb1\u2212\u03bb2)2 1 (\u03bb1\u2212\u03bb2)2 1 for sin2 error smaller than \u03f5, where G is a variance parameter. We obtain a similar rate in Corollary 1 through our techniques. However, comparing with Theorem 1, we observe that downsampling leads to an extra O (log (n)) factor. It is important to point out that [3] provides an analysis for estimating top k principal components, whereas this paper focuses on obtaining a sharp rate for the first principal component. [21] consider the harder problem of online non-negative matrix factorization for Markovian data. Their analysis establishes asymptotic convergence of error, but does not provide a rate.\n\n### Stochastic Optimization with Markovian Data\n\nMarkovian models are often considered in Reinforcement Learning and Linear Dynamic Systems[2, 5, 9, 30, 4, 34, 18, 23]. There have been many notable nonasymptotic bounds for stochastic gradient descent (SGD) methods for general convex and nonconvex functions with Markovian data [8, 31, 6, 7, 10, 38, 33]. The convergence rates (sample complexities) obtained in these works apply to more general problems but do not exploit the matrix product structure inherent to Oja\u2019s algorithm. In this work, we develop novel techniques to show that a sharper analysis is possible for the PCA objective. The paper is organized as follows.\n\nSection 2 contains the problem setup and preliminaries about Markov Chains. Section 3 contains Theorem 1. We present a sketch of the main technical tools in Section 4, intermediate theorems needed for the main theorem in Section 5 and conclude with simulations in Section 6.\n\nFigure 1: Comparison of Oja\u2019s algorithm with downsampled and offline variants. The X-axis represents the sample size and the Y axis represents the sin2 error of each algorithm\u2019s estimate of the leading eigenvector. The experimental setup is available in Section 6.", "images": [{"name": "page-3-0.jpg", "height": 191, "width": 238, "x": 306, "y": 432}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Table 1: Comparison of sin2 error rates and sample complexities", "md": "## Table 1: Comparison of sin2 error rates and sample complexities"}, {"type": "table", "rows": [["Paper", "Markov?", "Online?", "Log-free main-term", "sin2 error rate", "Sample-Complexity"], ["Jain et al. [15]", "N", "N", "O", "gap2 1n", "gap2 1\u03f5"], ["Chen et al. [3]", "Y", "Y", "N", "-", "gap2 1\u03f5 log2 gap2 1\u03f5"], ["Neeman et al. [27]", "Y", "N", "O", "V log d2\u2212\u03c0 4", "(1\u2212", "\u03bb2(P)", ") gap2 1n"], ["Theorem 1", "Y", "Y", "O", "(1\u2212", "\u03bb2(PV)", ") gap2 1n", "(1\u2212", "\u03bb2(P)", ") gap2 1\u03f5"]], "md": "|Paper|Markov?|Online?|Log-free main-term|sin2 error rate|Sample-Complexity|\n|---|---|---|---|---|---|\n|Jain et al. [15]|N|N|O|gap2 1n|gap2 1\u03f5|\n|Chen et al. [3]|Y|Y|N|-|gap2 1\u03f5 log2 gap2 1\u03f5|\n|Neeman et al. [27]|Y|N|O|V log d2\u2212\u03c0 4|(1\u2212|\u03bb2(P)|) gap2 1n|\n|Theorem 1|Y|Y|O|(1\u2212|\u03bb2(PV)|) gap2 1n|(1\u2212|\u03bb2(P)|) gap2 1\u03f5|", "isPerfectTable": false, "csv": "\"Paper\",\"Markov?\",\"Online?\",\"Log-free main-term\",\"sin2 error rate\",\"Sample-Complexity\"\n\"Jain et al. [15]\",\"N\",\"N\",\"O\",\"gap2 1n\",\"gap2 1\u03f5\"\n\"Chen et al. [3]\",\"Y\",\"Y\",\"N\",\"-\",\"gap2 1\u03f5 log2 gap2 1\u03f5\"\n\"Neeman et al. [27]\",\"Y\",\"N\",\"O\",\"V log d2\u2212\u03c0 4\",\"(1\u2212\",\"\u03bb2(P)\",\") gap2 1n\"\n\"Theorem 1\",\"Y\",\"Y\",\"O\",\"(1\u2212\",\"\u03bb2(PV)\",\") gap2 1n\",\"(1\u2212\",\"\u03bb2(P)\",\") gap2 1\u03f5\""}, {"type": "text", "value": "Here gap := (\u03bb1 \u2212 \u03bb2), where \u03bb1, \u03bb2 are the top 2 eigenvalues of \u03a3 and the sample complexities represent the number of samples required to achieve sin2 error at most \u03f5. We note that [1] and [14] also match the online sample complexity bound provided in [15]. Further, for the offline algorithm with IID data, [16] removes the log (d) factor in exchange for a constant probability of success for large enough n.\n\nEquation (SDE) based analysis to achieve a sample complexity of O G \u03f5 log2 G \u03f5 (\u03bb1\u2212\u03bb2)2 1 (\u03bb1\u2212\u03bb2)2 1 for sin2 error smaller than \u03f5, where G is a variance parameter. We obtain a similar rate in Corollary 1 through our techniques. However, comparing with Theorem 1, we observe that downsampling leads to an extra O (log (n)) factor. It is important to point out that [3] provides an analysis for estimating top k principal components, whereas this paper focuses on obtaining a sharp rate for the first principal component. [21] consider the harder problem of online non-negative matrix factorization for Markovian data. Their analysis establishes asymptotic convergence of error, but does not provide a rate.", "md": "Here gap := (\u03bb1 \u2212 \u03bb2), where \u03bb1, \u03bb2 are the top 2 eigenvalues of \u03a3 and the sample complexities represent the number of samples required to achieve sin2 error at most \u03f5. We note that [1] and [14] also match the online sample complexity bound provided in [15]. Further, for the offline algorithm with IID data, [16] removes the log (d) factor in exchange for a constant probability of success for large enough n.\n\nEquation (SDE) based analysis to achieve a sample complexity of O G \u03f5 log2 G \u03f5 (\u03bb1\u2212\u03bb2)2 1 (\u03bb1\u2212\u03bb2)2 1 for sin2 error smaller than \u03f5, where G is a variance parameter. We obtain a similar rate in Corollary 1 through our techniques. However, comparing with Theorem 1, we observe that downsampling leads to an extra O (log (n)) factor. It is important to point out that [3] provides an analysis for estimating top k principal components, whereas this paper focuses on obtaining a sharp rate for the first principal component. [21] consider the harder problem of online non-negative matrix factorization for Markovian data. Their analysis establishes asymptotic convergence of error, but does not provide a rate."}, {"type": "heading", "lvl": 3, "value": "Stochastic Optimization with Markovian Data", "md": "### Stochastic Optimization with Markovian Data"}, {"type": "text", "value": "Markovian models are often considered in Reinforcement Learning and Linear Dynamic Systems[2, 5, 9, 30, 4, 34, 18, 23]. There have been many notable nonasymptotic bounds for stochastic gradient descent (SGD) methods for general convex and nonconvex functions with Markovian data [8, 31, 6, 7, 10, 38, 33]. The convergence rates (sample complexities) obtained in these works apply to more general problems but do not exploit the matrix product structure inherent to Oja\u2019s algorithm. In this work, we develop novel techniques to show that a sharper analysis is possible for the PCA objective. The paper is organized as follows.\n\nSection 2 contains the problem setup and preliminaries about Markov Chains. Section 3 contains Theorem 1. We present a sketch of the main technical tools in Section 4, intermediate theorems needed for the main theorem in Section 5 and conclude with simulations in Section 6.\n\nFigure 1: Comparison of Oja\u2019s algorithm with downsampled and offline variants. The X-axis represents the sample size and the Y axis represents the sin2 error of each algorithm\u2019s estimate of the leading eigenvector. The experimental setup is available in Section 6.", "md": "Markovian models are often considered in Reinforcement Learning and Linear Dynamic Systems[2, 5, 9, 30, 4, 34, 18, 23]. There have been many notable nonasymptotic bounds for stochastic gradient descent (SGD) methods for general convex and nonconvex functions with Markovian data [8, 31, 6, 7, 10, 38, 33]. The convergence rates (sample complexities) obtained in these works apply to more general problems but do not exploit the matrix product structure inherent to Oja\u2019s algorithm. In this work, we develop novel techniques to show that a sharper analysis is possible for the PCA objective. The paper is organized as follows.\n\nSection 2 contains the problem setup and preliminaries about Markov Chains. Section 3 contains Theorem 1. We present a sketch of the main technical tools in Section 4, intermediate theorems needed for the main theorem in Section 5 and conclude with simulations in Section 6.\n\nFigure 1: Comparison of Oja\u2019s algorithm with downsampled and offline variants. The X-axis represents the sample size and the Y axis represents the sin2 error of each algorithm\u2019s estimate of the leading eigenvector. The experimental setup is available in Section 6."}]}, {"page": 4, "text": "2        Problem Setup and Preliminaries\nThis section presents the problem setup and outlines important properties of the Markov chain that\nwill be utilized subsequently. We assume that:\nAssumption 1. The Markov chain is irreducible, aperiodic, reversible, and starts in stationarity,\nwith state distribution \u03c01.\n      Such a Markov chain can arise in various situations, for e.g., while performing random walks on\nexpander graphs which are used extensively in fields such as computer networks, error-correcting\ncodes, and pseudorandom generators. Each state s of the Markov chain is associated with a distribution\nD(s) over d-dimensional vectors with mean \u00b5s \u2208                                        Rd and covariance matrix \u03a3s \u2208                           Rd\u00d7d.\nFor a random walk s1, s2, \u00b7 \u00b7 \u00b7 st on C, we defi                                ne the sequence of random variables X1, X2 \u00b7 \u00b7 \u00b7 Xt,\nwhere conditioned on the state si, Xi \u223c                            D (si).We represent the mean as \u00b5 := Es\u223c\u03c0 [\u00b5s] =  s \u03c0s\u00b5s\nand the covariance matrix as \u03a3 \u2208                          Rd\u00d7d, which, for i \u2208                 [n] can be expressed as:\n                \u03a3 := Esi\u223c\u03c0ED(si)                   (Xi \u2212       \u00b5) (Xi \u2212         \u00b5)T       = Es\u223c\u03c0 [\u03a3s] + Es\u223c\u03c0                      \u00b5s\u00b5T   s    \u2212   \u00b5\u00b5T\nIn this work, we assume \u00b5 = 0, which is a common assumption in the IID setting (see [15, 1]). While\nit may be possible to extend our analysis to the non-zero mean case, it is out of the scope of this paper.\nTherefore, \u03a3 = Esi\u223c\u03c0ED(si)                           XiXT    i     for i \u2208      [n]\n      Let the eigenvalues of \u03a3 be denoted as \u03bb1 > \u03bb2 \u2265                                                  \u03bb3 \u00b7 \u00b7 \u00b7 \u03bbd. Let v1 denote the leading\neigenvector of \u03a3 and V\u22a5                     denote the Rd\u00d7(d\u22121) matrix with the remaining eigenvectors as columns.\nWe proceed under the following standard assumptions for i \u2208                                                [n], (see for eg. [14]).\nAssumption 2. \u2225Esi\u223c\u03c0ED(si)[(XiXT                                i \u2212     \u03a3)2]\u22252 \u2264          V.\nAssumption 3. \u2225XiXT                    i \u2212     \u03a3\u22252 \u2264        M with probability 1.\nAssumption 3 also implies \u2225\u03a3s + \u00b5s\u00b5T                                 s \u2212      \u03a3\u22252 \u2264         M with probability 1. WLOG, we assume\nM + \u03bb1 \u2265             1. We use E [.] := Es\u223c\u03c0ED(s) [.] to denote the expectation over state s \u223c                                                    \u03c0 and over the\nstate-specific distributions D (.), unless otherwise specified. Define the matrix product\n                                Bt :=        I + \u03b7tXtXT          t       I + \u03b7tXt\u22121XT            t\u22121     . . .   I + \u03b71X1XT           1                             (2)\nUnrolling the recursion in 1, the output of Oja\u2019s algorithm at timestep t is given as wt = Btw0/ \u2225Btw0\u22252.\nIn this work, \u2225.\u22252 denotes the Euclidean L2 norm for vectors and the operator norm for matrices\nunless otherwise specified. I denotes the identity matrix.\n2.1         Markov chain mixing times\nNow we will discuss some well-known properties of an irreducible, aperiodic, and reversible Markov\nchain (also see [19]). Let |\u03bb2 (P                         ) | denote the second largest absolute eigenvalue of the Markov\nchain; let the state-distribution of the Markov chain at timestep t with s1 = x be P t(x, .). For any\ntwo probability distributions \u03bd1 and \u03bd2, recall that the total variational distance is T                                                           V (\u03bd1, \u03bd2) :=\n\u2225\u03bd1 \u2212       \u03bd2\u2225T V :=          1      x\u2208\u2126|\u03bd1(x) \u2212             \u03bd2(x)|. The distance from \u03c0 at the tth timestep is defined as\ndmix(t) := sup           x\u2208\u2126   2 T  V (P t(x, .), \u03c0). For irreducible and aperiodic Markov chains, by Theorem 4.9\nin [19], we have dmix(t) \u2264                    C exp(\u2212ct) for some C, c > 0. The mixing time of the Markov chain is\ndefined as:\n                                                           \u03c4mix(\u03f5) := inf{t : dmix(t) \u2264                     \u03f5}                                                      (3)\nAs in [19], we will denote \u03c4mix := \u03c4mix                                1   . Then, we have \u03c4mix(\u03f5) \u2264                      \u2308log2(1/\u03f5)\u2309          \u03c4mix. It is worth\n                                                                       4\nmentioning the useful relationship between dmix (.) and \u03c4mix, given as dmix (l\u03c4mix) \u2264                                                        2\u2212l           \u2200l \u2208   N0.\nThese results about mixing time are valid for general irreducible and aperiodic Markov chains. A\nreversible Markov chain satisfies \u2200                             x, y \u2208        \u2126, \u03c0 (x) P (x, y) = \u03c0 (y) P (y, x). For a reversible,\nirreducible, and aperiodic Markov chain, the gap 1 \u2212                                        |\u03bb2(P      )|, is inversely proportional to \u03c4mix [19].\n     1The last assumption may be eliminated by observing an initial burn-in period of \u03c4mix.\n                                                                                    4", "md": "# Problem Setup and Preliminaries\n\n## Problem Setup and Preliminaries\n\nThis section presents the problem setup and outlines important properties of the Markov chain that will be utilized subsequently. We assume that:\n\nAssumption 1. The Markov chain is irreducible, aperiodic, reversible, and starts in stationarity, with state distribution \u03c01.\n\nSuch a Markov chain can arise in various situations, for e.g., while performing random walks on expander graphs which are used extensively in fields such as computer networks, error-correcting codes, and pseudorandom generators. Each state s of the Markov chain is associated with a distribution D(s) over d-dimensional vectors with mean $$\\mu_s \\in \\mathbb{R}^d$$ and covariance matrix $$\\Sigma_s \\in \\mathbb{R}^{d \\times d}$$. For a random walk s1, s2, ..., st on C, we define the sequence of random variables X1, X2, ..., Xt, where conditioned on the state si, Xi ~ D(si). We represent the mean as $$\\mu := E_{s \\sim \\pi}[\\mu_s] = \\sum_s \\pi_s \\mu_s$$ and the covariance matrix as $$\\Sigma \\in \\mathbb{R}^{d \\times d}$$, which, for i \u2208 [n] can be expressed as:\n\n$$\\Sigma := E_{s_i \\sim \\pi}E[D(s_i)](X_i - \\mu)(X_i - \\mu)^T = E_{s \\sim \\pi}[\\Sigma_s] + E_{s \\sim \\pi}[\\mu_s \\mu_s^T] - \\mu \\mu^T$$\nIn this work, we assume $$\\mu = 0$$, which is a common assumption in the IID setting (see [15, 1]). While it may be possible to extend our analysis to the non-zero mean case, it is out of the scope of this paper. Therefore, $$\\Sigma = E_{s_i \\sim \\pi}D(s_i)X_iX_i^T$$ for i \u2208 [n].\n\nLet the eigenvalues of $$\\Sigma$$ be denoted as $$\\lambda_1 > \\lambda_2 \\geq \\lambda_3 \\geq ... \\geq \\lambda_d$$. Let v1 denote the leading eigenvector of $$\\Sigma$$ and V\u22a5 denote the $$\\mathbb{R}^{d \\times (d-1)}$$ matrix with the remaining eigenvectors as columns.\n\nWe proceed under the following standard assumptions for i \u2208 [n], (see for e.g. [14]).\n\nAssumption 2. $$\\|E_{s_i \\sim \\pi}E[D(s_i)]((X_iX_i^T - \\Sigma)^2)\\|_2 \\leq V$$.\n\nAssumption 3. $$\\|X_iX_i^T - \\Sigma\\|_2 \\leq M$$ with probability 1.\n\nAssumption 3 also implies $$\\|\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma\\|_2 \\leq M$$ with probability 1. WLOG, we assume $$M + \\lambda_1 \\geq 1$$. We use $$E[.] := E_{s \\sim \\pi}E[D(s)][.]$$ to denote the expectation over state s \u223c \u03c0 and over the state-specific distributions D(.), unless otherwise specified. Define the matrix product\n\n$$B_t := I + \\eta_tX_tX_t^T I + \\eta_tX_{t-1}X_{t-1}^T ... I + \\eta_1X_1X_1^T$$\nUnrolling the recursion in 1, the output of Oja\u2019s algorithm at timestep t is given as $$w_t = B_tw_0/ \\|B_tw_0\\|_2$$.\n\nIn this work, $$\\|.\\|_2$$ denotes the Euclidean L2 norm for vectors and the operator norm for matrices unless otherwise specified. I denotes the identity matrix.\n\n### Markov chain mixing times\n\nNow we will discuss some well-known properties of an irreducible, aperiodic, and reversible Markov chain (also see [19]). Let $$|\\lambda_2(P)|$$ denote the second largest absolute eigenvalue of the Markov chain; let the state-distribution of the Markov chain at timestep t with s1 = x be Pt(x, .). For any two probability distributions \u03bd1 and \u03bd2, recall that the total variational distance is $$TV(\\nu_1, \\nu_2) := \\| \\nu_1 - \\nu_2 \\|_{TV} := \\frac{1}{2} \\sum_{x \\in \\Omega} |\\nu_1(x) - \\nu_2(x)|$$. The distance from \u03c0 at the tth timestep is defined as $$d_{\\text{mix}}(t) := \\sup_{x \\in \\Omega} 2TV(P^t(x, .), \\pi)$$. For irreducible and aperiodic Markov chains, by Theorem 4.9 in [19], we have $$d_{\\text{mix}}(t) \\leq C \\exp(-ct)$$ for some C, c > 0. The mixing time of the Markov chain is defined as:\n\n$$\\tau_{\\text{mix}}(\\epsilon) := \\inf\\{t : d_{\\text{mix}}(t) \\leq \\epsilon\\}$$\nAs in [19], we will denote $$\\tau_{\\text{mix}} := \\tau_{\\text{mix}}^1$$. Then, we have $$\\tau_{\\text{mix}}(\\epsilon) \\leq \\lceil \\log_2(1/\\epsilon) \\rceil \\tau_{\\text{mix}}$$. It is worth mentioning the useful relationship between $$d_{\\text{mix}}(.)$$ and $$\\tau_{\\text{mix}}$$, given as $$d_{\\text{mix}}(l\\tau_{\\text{mix}}) \\leq 2^{-l}$$ for all $$l \\in \\mathbb{N}_0$$.\n\nThese results about mixing time are valid for general irreducible and aperiodic Markov chains. A reversible Markov chain satisfies $$\\forall x, y \\in \\Omega, \\pi(x) P(x, y) = \\pi(y) P(y, x)$$. For a reversible, irreducible, and aperiodic Markov chain, the gap $$1 - |\\lambda_2(P)|$$, is inversely proportional to $$\\tau_{\\text{mix}}$$ [19].\n\nThe last assumption may be eliminated by observing an initial burn-in period of $$\\tau_{\\text{mix}}/4$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Problem Setup and Preliminaries", "md": "# Problem Setup and Preliminaries"}, {"type": "heading", "lvl": 2, "value": "Problem Setup and Preliminaries", "md": "## Problem Setup and Preliminaries"}, {"type": "text", "value": "This section presents the problem setup and outlines important properties of the Markov chain that will be utilized subsequently. We assume that:\n\nAssumption 1. The Markov chain is irreducible, aperiodic, reversible, and starts in stationarity, with state distribution \u03c01.\n\nSuch a Markov chain can arise in various situations, for e.g., while performing random walks on expander graphs which are used extensively in fields such as computer networks, error-correcting codes, and pseudorandom generators. Each state s of the Markov chain is associated with a distribution D(s) over d-dimensional vectors with mean $$\\mu_s \\in \\mathbb{R}^d$$ and covariance matrix $$\\Sigma_s \\in \\mathbb{R}^{d \\times d}$$. For a random walk s1, s2, ..., st on C, we define the sequence of random variables X1, X2, ..., Xt, where conditioned on the state si, Xi ~ D(si). We represent the mean as $$\\mu := E_{s \\sim \\pi}[\\mu_s] = \\sum_s \\pi_s \\mu_s$$ and the covariance matrix as $$\\Sigma \\in \\mathbb{R}^{d \\times d}$$, which, for i \u2208 [n] can be expressed as:\n\n$$\\Sigma := E_{s_i \\sim \\pi}E[D(s_i)](X_i - \\mu)(X_i - \\mu)^T = E_{s \\sim \\pi}[\\Sigma_s] + E_{s \\sim \\pi}[\\mu_s \\mu_s^T] - \\mu \\mu^T$$\nIn this work, we assume $$\\mu = 0$$, which is a common assumption in the IID setting (see [15, 1]). While it may be possible to extend our analysis to the non-zero mean case, it is out of the scope of this paper. Therefore, $$\\Sigma = E_{s_i \\sim \\pi}D(s_i)X_iX_i^T$$ for i \u2208 [n].\n\nLet the eigenvalues of $$\\Sigma$$ be denoted as $$\\lambda_1 > \\lambda_2 \\geq \\lambda_3 \\geq ... \\geq \\lambda_d$$. Let v1 denote the leading eigenvector of $$\\Sigma$$ and V\u22a5 denote the $$\\mathbb{R}^{d \\times (d-1)}$$ matrix with the remaining eigenvectors as columns.\n\nWe proceed under the following standard assumptions for i \u2208 [n], (see for e.g. [14]).\n\nAssumption 2. $$\\|E_{s_i \\sim \\pi}E[D(s_i)]((X_iX_i^T - \\Sigma)^2)\\|_2 \\leq V$$.\n\nAssumption 3. $$\\|X_iX_i^T - \\Sigma\\|_2 \\leq M$$ with probability 1.\n\nAssumption 3 also implies $$\\|\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma\\|_2 \\leq M$$ with probability 1. WLOG, we assume $$M + \\lambda_1 \\geq 1$$. We use $$E[.] := E_{s \\sim \\pi}E[D(s)][.]$$ to denote the expectation over state s \u223c \u03c0 and over the state-specific distributions D(.), unless otherwise specified. Define the matrix product\n\n$$B_t := I + \\eta_tX_tX_t^T I + \\eta_tX_{t-1}X_{t-1}^T ... I + \\eta_1X_1X_1^T$$\nUnrolling the recursion in 1, the output of Oja\u2019s algorithm at timestep t is given as $$w_t = B_tw_0/ \\|B_tw_0\\|_2$$.\n\nIn this work, $$\\|.\\|_2$$ denotes the Euclidean L2 norm for vectors and the operator norm for matrices unless otherwise specified. I denotes the identity matrix.", "md": "This section presents the problem setup and outlines important properties of the Markov chain that will be utilized subsequently. We assume that:\n\nAssumption 1. The Markov chain is irreducible, aperiodic, reversible, and starts in stationarity, with state distribution \u03c01.\n\nSuch a Markov chain can arise in various situations, for e.g., while performing random walks on expander graphs which are used extensively in fields such as computer networks, error-correcting codes, and pseudorandom generators. Each state s of the Markov chain is associated with a distribution D(s) over d-dimensional vectors with mean $$\\mu_s \\in \\mathbb{R}^d$$ and covariance matrix $$\\Sigma_s \\in \\mathbb{R}^{d \\times d}$$. For a random walk s1, s2, ..., st on C, we define the sequence of random variables X1, X2, ..., Xt, where conditioned on the state si, Xi ~ D(si). We represent the mean as $$\\mu := E_{s \\sim \\pi}[\\mu_s] = \\sum_s \\pi_s \\mu_s$$ and the covariance matrix as $$\\Sigma \\in \\mathbb{R}^{d \\times d}$$, which, for i \u2208 [n] can be expressed as:\n\n$$\\Sigma := E_{s_i \\sim \\pi}E[D(s_i)](X_i - \\mu)(X_i - \\mu)^T = E_{s \\sim \\pi}[\\Sigma_s] + E_{s \\sim \\pi}[\\mu_s \\mu_s^T] - \\mu \\mu^T$$\nIn this work, we assume $$\\mu = 0$$, which is a common assumption in the IID setting (see [15, 1]). While it may be possible to extend our analysis to the non-zero mean case, it is out of the scope of this paper. Therefore, $$\\Sigma = E_{s_i \\sim \\pi}D(s_i)X_iX_i^T$$ for i \u2208 [n].\n\nLet the eigenvalues of $$\\Sigma$$ be denoted as $$\\lambda_1 > \\lambda_2 \\geq \\lambda_3 \\geq ... \\geq \\lambda_d$$. Let v1 denote the leading eigenvector of $$\\Sigma$$ and V\u22a5 denote the $$\\mathbb{R}^{d \\times (d-1)}$$ matrix with the remaining eigenvectors as columns.\n\nWe proceed under the following standard assumptions for i \u2208 [n], (see for e.g. [14]).\n\nAssumption 2. $$\\|E_{s_i \\sim \\pi}E[D(s_i)]((X_iX_i^T - \\Sigma)^2)\\|_2 \\leq V$$.\n\nAssumption 3. $$\\|X_iX_i^T - \\Sigma\\|_2 \\leq M$$ with probability 1.\n\nAssumption 3 also implies $$\\|\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma\\|_2 \\leq M$$ with probability 1. WLOG, we assume $$M + \\lambda_1 \\geq 1$$. We use $$E[.] := E_{s \\sim \\pi}E[D(s)][.]$$ to denote the expectation over state s \u223c \u03c0 and over the state-specific distributions D(.), unless otherwise specified. Define the matrix product\n\n$$B_t := I + \\eta_tX_tX_t^T I + \\eta_tX_{t-1}X_{t-1}^T ... I + \\eta_1X_1X_1^T$$\nUnrolling the recursion in 1, the output of Oja\u2019s algorithm at timestep t is given as $$w_t = B_tw_0/ \\|B_tw_0\\|_2$$.\n\nIn this work, $$\\|.\\|_2$$ denotes the Euclidean L2 norm for vectors and the operator norm for matrices unless otherwise specified. I denotes the identity matrix."}, {"type": "heading", "lvl": 3, "value": "Markov chain mixing times", "md": "### Markov chain mixing times"}, {"type": "text", "value": "Now we will discuss some well-known properties of an irreducible, aperiodic, and reversible Markov chain (also see [19]). Let $$|\\lambda_2(P)|$$ denote the second largest absolute eigenvalue of the Markov chain; let the state-distribution of the Markov chain at timestep t with s1 = x be Pt(x, .). For any two probability distributions \u03bd1 and \u03bd2, recall that the total variational distance is $$TV(\\nu_1, \\nu_2) := \\| \\nu_1 - \\nu_2 \\|_{TV} := \\frac{1}{2} \\sum_{x \\in \\Omega} |\\nu_1(x) - \\nu_2(x)|$$. The distance from \u03c0 at the tth timestep is defined as $$d_{\\text{mix}}(t) := \\sup_{x \\in \\Omega} 2TV(P^t(x, .), \\pi)$$. For irreducible and aperiodic Markov chains, by Theorem 4.9 in [19], we have $$d_{\\text{mix}}(t) \\leq C \\exp(-ct)$$ for some C, c > 0. The mixing time of the Markov chain is defined as:\n\n$$\\tau_{\\text{mix}}(\\epsilon) := \\inf\\{t : d_{\\text{mix}}(t) \\leq \\epsilon\\}$$\nAs in [19], we will denote $$\\tau_{\\text{mix}} := \\tau_{\\text{mix}}^1$$. Then, we have $$\\tau_{\\text{mix}}(\\epsilon) \\leq \\lceil \\log_2(1/\\epsilon) \\rceil \\tau_{\\text{mix}}$$. It is worth mentioning the useful relationship between $$d_{\\text{mix}}(.)$$ and $$\\tau_{\\text{mix}}$$, given as $$d_{\\text{mix}}(l\\tau_{\\text{mix}}) \\leq 2^{-l}$$ for all $$l \\in \\mathbb{N}_0$$.\n\nThese results about mixing time are valid for general irreducible and aperiodic Markov chains. A reversible Markov chain satisfies $$\\forall x, y \\in \\Omega, \\pi(x) P(x, y) = \\pi(y) P(y, x)$$. For a reversible, irreducible, and aperiodic Markov chain, the gap $$1 - |\\lambda_2(P)|$$, is inversely proportional to $$\\tau_{\\text{mix}}$$ [19].\n\nThe last assumption may be eliminated by observing an initial burn-in period of $$\\tau_{\\text{mix}}/4$$.", "md": "Now we will discuss some well-known properties of an irreducible, aperiodic, and reversible Markov chain (also see [19]). Let $$|\\lambda_2(P)|$$ denote the second largest absolute eigenvalue of the Markov chain; let the state-distribution of the Markov chain at timestep t with s1 = x be Pt(x, .). For any two probability distributions \u03bd1 and \u03bd2, recall that the total variational distance is $$TV(\\nu_1, \\nu_2) := \\| \\nu_1 - \\nu_2 \\|_{TV} := \\frac{1}{2} \\sum_{x \\in \\Omega} |\\nu_1(x) - \\nu_2(x)|$$. The distance from \u03c0 at the tth timestep is defined as $$d_{\\text{mix}}(t) := \\sup_{x \\in \\Omega} 2TV(P^t(x, .), \\pi)$$. For irreducible and aperiodic Markov chains, by Theorem 4.9 in [19], we have $$d_{\\text{mix}}(t) \\leq C \\exp(-ct)$$ for some C, c > 0. The mixing time of the Markov chain is defined as:\n\n$$\\tau_{\\text{mix}}(\\epsilon) := \\inf\\{t : d_{\\text{mix}}(t) \\leq \\epsilon\\}$$\nAs in [19], we will denote $$\\tau_{\\text{mix}} := \\tau_{\\text{mix}}^1$$. Then, we have $$\\tau_{\\text{mix}}(\\epsilon) \\leq \\lceil \\log_2(1/\\epsilon) \\rceil \\tau_{\\text{mix}}$$. It is worth mentioning the useful relationship between $$d_{\\text{mix}}(.)$$ and $$\\tau_{\\text{mix}}$$, given as $$d_{\\text{mix}}(l\\tau_{\\text{mix}}) \\leq 2^{-l}$$ for all $$l \\in \\mathbb{N}_0$$.\n\nThese results about mixing time are valid for general irreducible and aperiodic Markov chains. A reversible Markov chain satisfies $$\\forall x, y \\in \\Omega, \\pi(x) P(x, y) = \\pi(y) P(y, x)$$. For a reversible, irreducible, and aperiodic Markov chain, the gap $$1 - |\\lambda_2(P)|$$, is inversely proportional to $$\\tau_{\\text{mix}}$$ [19].\n\nThe last assumption may be eliminated by observing an initial burn-in period of $$\\tau_{\\text{mix}}/4$$."}]}, {"page": 5, "text": "3      Main Results\nIn this section, we present our main result, a near-optimal convergence rate for Oja\u2019s algorithm on\nMarkovian data. As a corollary, we also establish a rate of convergence for Oja\u2019s algorithm applied on\ndownsampled data, where every kth data-point is considered. Supplement S.5 contains comprehensive\nproofs of Theorem 1 and Corollary 1 while the proof of Proposition 1 can be found in Supplement\nSection S.2.\n                                                                                  \u03b1\nTheorem 1. Fix a \u03b4 \u2208         (0, 1) and let the step-sizes be \u03b7i :=        (\u03bb1\u2212\u03bb2)(\u03b2+i) with \u03b70 \u2264        1\n                                                                                                         e, \u03b1 > 2. Under\nassumptions 1, 2 and 3, for sufficiently large number of samples n such that                     log(n1        log \u03b2 1  ,\n                                                                                                      \u03b7n ) >        \u03b70\n                                                                                         V        1\n                             1000\u03b12 max         \u03c4mix log    \u03b71   (M + \u03bb1)2 ,        1\u2212|\u03bb2(P )| +\u03bb2\n                                                             0                            100\n                     \u03b2 :=                                                     \u03b4\nthe output wn of Oja\u2019s algorithm (1) satisfies    (\u03bb1 \u2212   \u03bb2)2 log     1 +   200\n               2 \u2264   C log    1         2\u03b2     2\u03b1                                                                       \u03c4mix   \u03b72  2\n                               \u03b4                                     C1V                  1                                     n\n1 \u2212    wTn v1             \u03b42         d    n         +                                     n + C2M (M + \u03bb1)2                  n2\n                                                       (\u03bb1 \u2212    \u03bb2)2 (1 \u2212    |\u03bb2 (P) |)              (\u03bb1 \u2212   \u03bb2)3\nwith probability atleast (1 \u2212        \u03b4). Here C is an absolute constant and\n                                    C1 := \u03b12 (3 + 7|\u03bb2 (P) |), C2 := 35\u03b13\n                                                    2\u03b1 \u2212    1                     \u03b1 \u2212   1\n     Next, we compare the rate of convergence proposed in Theorem 1 with the offl                           ine algorithm\nhaving access to the entire dataset {Xi}n         i=1 using a recent result from [27]. Here, the authors extend\nthe Matrix Bernstein inequality [35, 32], to Markovian random matrices. Their setup is much like\nours except that the matrix at any state is fixed, i.e., there is no data distribution D(s) as in our setup.\nHowever, it is easy to extend their result to our setting by observing that conditioned on the state\nsequence, the matrices XiXT          i , i \u2208  [n] are independent under our model, and we can push in the\nexpectation over the state-specific distributions, D(s), whenever required. Therefore, we have the\nfollowing result -\nProposition 1 (Theorem 2.2 of [27]+Wedin\u2019s theorem). Fix \u03b4 \u2208                     (0, 1). Consider an irreducible and\naperiodic Markov chain. Under assumptions 2 and 3, with probability 1 \u2212                      \u03b4, the leading eigenvector\n\u02c6\nv of  n  i=1 XiXT   i /n satisfies  d2\u2212\u03c04     1 + |\u03bb2 (P)|                     \uf8eb        M log      d2\u2212\u03c0 4         \uf8f62\n 1 \u2212    \u02c6       2 \u2264   C\u2032  V log       \u03b4                                        \uf8ed                      \u03b4           \uf8f8    . 1\n        vT v1           1   (\u03bb1 \u2212   \u03bb2)2        1 \u2212  |\u03bb2 (P)|      .n1+ C\u2032   2    (\u03bb1 \u2212   \u03bb2) (1 \u2212    |\u03bb2 (P) |)        n2(4)\nfor absolute constants C\u2032      1 and C\u2032  2.\n     Observe that Theorem 1 matches the leading term                            V\n                                                                     (\u03bb1\u2212\u03bb2)2(1\u2212|\u03bb2(P )|) in Eq 1 except the log(d)\nterm. We believe, much like the IID case (also see the remark in [15]), this logarithmic term in [27]\u2019s\nresult is removable for large n and a constant probability of success.\nRemark 1. (Comparison with IID algorithm) Fix a \u03b4 \u2208                         (0, 1). If the data-points {Xi}n        i=1 are\nsampled IID from the stationary distribution \u03c0, then under assumptions 2 and 3, using Theorem 4.1\nfrom [15], we have that the output wn of Oja\u2019s algorithm 1 satisfies -\n                                  2 \u2264    C log    1        \u03b2\u2032    2\u03b1                 \u03b1 \u20322V             1\n                   1 \u2212    wTn v1              \u03b42  \u03b4     d     n       +   (2\u03b1\u2032 \u2212    1) (\u03bb1 \u2212   \u03bb2)2   n                   (5)\n                                                              5", "md": "### Main Results\n\nIn this section, we present our main result, a near-optimal convergence rate for Oja\u2019s algorithm on Markovian data. As a corollary, we also establish a rate of convergence for Oja\u2019s algorithm applied on downsampled data, where every kth data-point is considered. Supplement S.5 contains comprehensive proofs of Theorem 1 and Corollary 1 while the proof of Proposition 1 can be found in Supplement Section S.2.\n\nTheorem 1. Fix a \u03b4 \u2208 (0, 1) and let the step-sizes be \u03b7i := (\u03bb1\u2212\u03bb2)(\u03b2+i) with \u03b70 \u2264 e, \u03b1 > 2. Under assumptions 1, 2 and 3, for sufficiently large number of samples n such that $$\\log\\left(n^{\\frac{1}{\\log\\beta}}\\right) > \\eta_0$$, the output wn of Oja\u2019s algorithm (1) satisfies\n\n$$\n\\begin{align*}\n2 &\\leq C \\log n \\frac{1}{2\\beta^2\\alpha} \\tau_{\\text{mix}}\\eta_2^2 \\\\\n&\\quad + C_1V \\frac{1}{n} + C_2M(M + \\lambda_1)^2 \\frac{1}{n^2} (\\lambda_1 - \\lambda_2)^2(1 - |\\lambda_2(P)|)(\\lambda_1 - \\lambda_2)^3\n\\end{align*}\n$$\nwith probability at least (1 - \u03b4). Here C is an absolute constant and\n\nC1 := \u03b12(3 + 7|\u03bb2(P)|), C2 := 35\u03b13(2\u03b1 - 1)(\u03b1 - 1)\n\nNext, we compare the rate of convergence proposed in Theorem 1 with the offline algorithm having access to the entire dataset {Xi}ni=1 using a recent result from [27]. Here, the authors extend the Matrix Bernstein inequality [35, 32], to Markovian random matrices. Their setup is much like ours except that the matrix at any state is fixed, i.e., there is no data distribution D(s) as in our setup. However, it is easy to extend their result to our setting by observing that conditioned on the state sequence, the matrices XiXTi, i \u2208 [n] are independent under our model, and we can push in the expectation over the state-specific distributions, D(s), whenever required. Therefore, we have the following result -\n\nProposition 1 (Theorem 2.2 of [27]+Wedin\u2019s theorem). Fix \u03b4 \u2208 (0, 1). Consider an irreducible and aperiodic Markov chain. Under assumptions 2 and 3, with probability 1 - \u03b4, the leading eigenvector v\u02c6 of \u03a3i=1XiXTi/n satisfies\n\n$$\nd^2 - \\pi^4 \\leq 1 + |\\lambda_2(P)| \\left(\\frac{M \\log d^2 - \\pi^4}{\\delta}\\right)^2\n$$\nfor absolute constants C'1 and C'2.\n\nObserve that Theorem 1 matches the leading term (\u03bb1\u2212\u03bb2)2(1-|\u03bb2(P)|) in Eq 1 except the log(d) term. We believe, much like the IID case (also see the remark in [15]), this logarithmic term in [27]\u2019s result is removable for large n and a constant probability of success.\n\nRemark 1. (Comparison with IID algorithm) Fix a \u03b4 \u2208 (0, 1). If the data-points {Xi}ni=1 are sampled IID from the stationary distribution \u03c0, then under assumptions 2 and 3, using Theorem 4.1 from [15], we have that the output wn of Oja\u2019s algorithm 1 satisfies -\n\n$$\n2 \\leq C \\log n \\frac{1}{\\beta'^2\\alpha} \\alpha'^2V \\frac{1}{n} + (2\\alpha' - 1)(\\lambda_1 - \\lambda_2)^2n\n$$", "images": [], "items": [{"type": "heading", "lvl": 3, "value": "Main Results", "md": "### Main Results"}, {"type": "text", "value": "In this section, we present our main result, a near-optimal convergence rate for Oja\u2019s algorithm on Markovian data. As a corollary, we also establish a rate of convergence for Oja\u2019s algorithm applied on downsampled data, where every kth data-point is considered. Supplement S.5 contains comprehensive proofs of Theorem 1 and Corollary 1 while the proof of Proposition 1 can be found in Supplement Section S.2.\n\nTheorem 1. Fix a \u03b4 \u2208 (0, 1) and let the step-sizes be \u03b7i := (\u03bb1\u2212\u03bb2)(\u03b2+i) with \u03b70 \u2264 e, \u03b1 > 2. Under assumptions 1, 2 and 3, for sufficiently large number of samples n such that $$\\log\\left(n^{\\frac{1}{\\log\\beta}}\\right) > \\eta_0$$, the output wn of Oja\u2019s algorithm (1) satisfies\n\n$$\n\\begin{align*}\n2 &\\leq C \\log n \\frac{1}{2\\beta^2\\alpha} \\tau_{\\text{mix}}\\eta_2^2 \\\\\n&\\quad + C_1V \\frac{1}{n} + C_2M(M + \\lambda_1)^2 \\frac{1}{n^2} (\\lambda_1 - \\lambda_2)^2(1 - |\\lambda_2(P)|)(\\lambda_1 - \\lambda_2)^3\n\\end{align*}\n$$\nwith probability at least (1 - \u03b4). Here C is an absolute constant and\n\nC1 := \u03b12(3 + 7|\u03bb2(P)|), C2 := 35\u03b13(2\u03b1 - 1)(\u03b1 - 1)\n\nNext, we compare the rate of convergence proposed in Theorem 1 with the offline algorithm having access to the entire dataset {Xi}ni=1 using a recent result from [27]. Here, the authors extend the Matrix Bernstein inequality [35, 32], to Markovian random matrices. Their setup is much like ours except that the matrix at any state is fixed, i.e., there is no data distribution D(s) as in our setup. However, it is easy to extend their result to our setting by observing that conditioned on the state sequence, the matrices XiXTi, i \u2208 [n] are independent under our model, and we can push in the expectation over the state-specific distributions, D(s), whenever required. Therefore, we have the following result -\n\nProposition 1 (Theorem 2.2 of [27]+Wedin\u2019s theorem). Fix \u03b4 \u2208 (0, 1). Consider an irreducible and aperiodic Markov chain. Under assumptions 2 and 3, with probability 1 - \u03b4, the leading eigenvector v\u02c6 of \u03a3i=1XiXTi/n satisfies\n\n$$\nd^2 - \\pi^4 \\leq 1 + |\\lambda_2(P)| \\left(\\frac{M \\log d^2 - \\pi^4}{\\delta}\\right)^2\n$$\nfor absolute constants C'1 and C'2.\n\nObserve that Theorem 1 matches the leading term (\u03bb1\u2212\u03bb2)2(1-|\u03bb2(P)|) in Eq 1 except the log(d) term. We believe, much like the IID case (also see the remark in [15]), this logarithmic term in [27]\u2019s result is removable for large n and a constant probability of success.\n\nRemark 1. (Comparison with IID algorithm) Fix a \u03b4 \u2208 (0, 1). If the data-points {Xi}ni=1 are sampled IID from the stationary distribution \u03c0, then under assumptions 2 and 3, using Theorem 4.1 from [15], we have that the output wn of Oja\u2019s algorithm 1 satisfies -\n\n$$\n2 \\leq C \\log n \\frac{1}{\\beta'^2\\alpha} \\alpha'^2V \\frac{1}{n} + (2\\alpha' - 1)(\\lambda_1 - \\lambda_2)^2n\n$$", "md": "In this section, we present our main result, a near-optimal convergence rate for Oja\u2019s algorithm on Markovian data. As a corollary, we also establish a rate of convergence for Oja\u2019s algorithm applied on downsampled data, where every kth data-point is considered. Supplement S.5 contains comprehensive proofs of Theorem 1 and Corollary 1 while the proof of Proposition 1 can be found in Supplement Section S.2.\n\nTheorem 1. Fix a \u03b4 \u2208 (0, 1) and let the step-sizes be \u03b7i := (\u03bb1\u2212\u03bb2)(\u03b2+i) with \u03b70 \u2264 e, \u03b1 > 2. Under assumptions 1, 2 and 3, for sufficiently large number of samples n such that $$\\log\\left(n^{\\frac{1}{\\log\\beta}}\\right) > \\eta_0$$, the output wn of Oja\u2019s algorithm (1) satisfies\n\n$$\n\\begin{align*}\n2 &\\leq C \\log n \\frac{1}{2\\beta^2\\alpha} \\tau_{\\text{mix}}\\eta_2^2 \\\\\n&\\quad + C_1V \\frac{1}{n} + C_2M(M + \\lambda_1)^2 \\frac{1}{n^2} (\\lambda_1 - \\lambda_2)^2(1 - |\\lambda_2(P)|)(\\lambda_1 - \\lambda_2)^3\n\\end{align*}\n$$\nwith probability at least (1 - \u03b4). Here C is an absolute constant and\n\nC1 := \u03b12(3 + 7|\u03bb2(P)|), C2 := 35\u03b13(2\u03b1 - 1)(\u03b1 - 1)\n\nNext, we compare the rate of convergence proposed in Theorem 1 with the offline algorithm having access to the entire dataset {Xi}ni=1 using a recent result from [27]. Here, the authors extend the Matrix Bernstein inequality [35, 32], to Markovian random matrices. Their setup is much like ours except that the matrix at any state is fixed, i.e., there is no data distribution D(s) as in our setup. However, it is easy to extend their result to our setting by observing that conditioned on the state sequence, the matrices XiXTi, i \u2208 [n] are independent under our model, and we can push in the expectation over the state-specific distributions, D(s), whenever required. Therefore, we have the following result -\n\nProposition 1 (Theorem 2.2 of [27]+Wedin\u2019s theorem). Fix \u03b4 \u2208 (0, 1). Consider an irreducible and aperiodic Markov chain. Under assumptions 2 and 3, with probability 1 - \u03b4, the leading eigenvector v\u02c6 of \u03a3i=1XiXTi/n satisfies\n\n$$\nd^2 - \\pi^4 \\leq 1 + |\\lambda_2(P)| \\left(\\frac{M \\log d^2 - \\pi^4}{\\delta}\\right)^2\n$$\nfor absolute constants C'1 and C'2.\n\nObserve that Theorem 1 matches the leading term (\u03bb1\u2212\u03bb2)2(1-|\u03bb2(P)|) in Eq 1 except the log(d) term. We believe, much like the IID case (also see the remark in [15]), this logarithmic term in [27]\u2019s result is removable for large n and a constant probability of success.\n\nRemark 1. (Comparison with IID algorithm) Fix a \u03b4 \u2208 (0, 1). If the data-points {Xi}ni=1 are sampled IID from the stationary distribution \u03c0, then under assumptions 2 and 3, using Theorem 4.1 from [15], we have that the output wn of Oja\u2019s algorithm 1 satisfies -\n\n$$\n2 \\leq C \\log n \\frac{1}{\\beta'^2\\alpha} \\alpha'^2V \\frac{1}{n} + (2\\alpha' - 1)(\\lambda_1 - \\lambda_2)^2n\n$$"}]}, {"page": 6, "text": "      The leading term of Theorem 1 is worse by a factor of                                        1\u2212|\u03bb2(P1)|      . Further, it has an additive lower\n                          log2(n)\norder term O                  n2         due to the covariance between data-points in the Markovian case.\nCorollary 1. (Downsampled Oja\u2019s algorithm) Fix a \u03b4 \u2208                                              (0, 1). If Oja\u2019s algorithm is applied on the\ndownsampled data-stream with every kth data-point, where k := \u03c4mix                                                      \u03b72n    then under the conditions\nof Theorem 1 with appropriately modified \u03b1 and \u03b2, the output wn satisfies\n1 \u2212     wT  n v1    2 \u2264 1              2\u03b2\u03c4mix log (n)           2\u03b1                                                                                                              2\n           C log        \u03b4       d                                      +       C1V\u03c4mix           log (n)      + C2M (M + \u03bb1)2                      log2 (n) \u03c4mix           \u03b72n\n                  \u03b42                             n                          (\u03bb1 \u2212       \u03bb2)2         n                   (\u03bb1 \u2212      \u03bb2)3                         n2\nwith probability atleast (1 \u2212                     \u03b4). Here C is an absolute constant and C1 := 30\u03b12                               2\u03b1\u22121     , C2 := 35\u03b13     \u03b1\u22121 .\nRemark 2. Data downsampling to reduce dependence amongst samples has been suggested in recent\nwork [26, 22, 3]. In Corollary 1, we establish that the rate obtained is sub-optimal compared to\nTheorem 1 by a log (n) factor. We prove this by a simple yet elegant observation: the downsampled\ndata stream can be considered to be drawn from a Markov chain with transition kernel P k (., .) since\neach data-point is k steps away from the previous one. For sufficiently large k, this implies that the\nmixing time of this chain is \u0398 (1). These new parameters are used to select the modified values of\n\u03b1, \u03b2 according to Lemma S.12 in the Supplement.\n      The proof of Theorem 1 follows the same general recipe as in [15] for obtaining a bound on the\nsin2 error. However, the original proof techniques heavily rely on the IID setting. We carry out a\nrefined analysis for each step under the Markovian data model by a careful control of error terms\narising out of dependence. The first step involves obtaining a high-probability bound on the sin2\nerror, by noting that Oja\u2019s algorithm on n data-points can be viewed as a single iteration of the power\nmethod on Bn. Therefore, fixing a \u03b4 \u2208                               (0, 1) using Lemma 3.1 from [15], we have with probability\nat least (1 \u2212         \u03b4),                                                             1     Tr     V T            n V\u22a5\n                                             sin2 (wn, v1) \u2264              C log  \u03b4    \u03b4          vT  \u22a5   BnBT  n v1         ,                                         (6)\n                                                                                                    1 BnBT\nwhere C is an absolute constant. The numerator is bounded by first bounding its expectation (see\nTheorem 3) and then using Markov\u2019s inequality. To bound the denominator, similar to [15], we will\nuse Chebyshev\u2019s inequality. Theorem 4 provides a lower bound for the expectation E                                                                  vT1 BnBT     n v1     .\nChebyshev\u2019s inequality also requires upper-bounding the variance of E                                                    vT1 BnBT      n v1     , which requires\nus to bound E                vT1 BnBT     n v1    2     (see Theorem 5).\n4        Main Technical Tools\nIn this section, we provide a sketch of the main arguments used in our proof.\n      Warm-up with downsampled Oja\u2019s algorithm: We start with the simple downsampled Oja\u2019s\nalgorithm to build intuition. Here, one applies Oja\u2019s update rule (Eq 1) to every kth data-point, for\na suitably chosen k. For k = \u2308L\u03c4mix log n\u2309, the total variation distance between any consecutive\ndata-points in the downsampled data stream is O(n\u2212L). As we show in Corollary 1, the error of this\nalgorithm is similar to the error of Oja\u2019s algorithm applied to n/k data-points in the IID setting, i.e.,\nO(V\u03c4mix log n/n).\n      We will take E              vT1 BnBT      n v1      as an example. Let us introduce some notation.\n                            Bj,i :=         I + \u03b7jXjXT          j       I + \u03b7j\u22121Xj\u22121XT               j\u22121      . . .   I + \u03b7iXiXT         i                            (7)\nWe peel this quantity one matrix at a time from the inside. Note that for a reversible Markov chain,\nstandard results imply (see Lemma 1) that the mixing conditions apply to the conditional distribution\nof a state given another state k steps in the \u201cfuture\u201d (see Supplement section S.3 for a proof). Recall\ndmix(k) from Section 2.1.\n                                                                                    6", "md": "The leading term of Theorem 1 is worse by a factor of $$1 - |\\lambda_2(P_1)|$$. Further, it has an additive lower order term $$O\\left(\\frac{n^2}{\\log^2(n)}\\right)$$ due to the covariance between data-points in the Markovian case.\n\n**Corollary 1.** (Downsampled Oja\u2019s algorithm) Fix a $$\\delta \\in (0, 1)$$. If Oja\u2019s algorithm is applied on the downsampled data-stream with every $$k$$th data-point, where $$k := \\tau_{\\text{mix}} \\cdot \\frac{\\eta^2n}{\\log(n)}$$ then under the conditions of Theorem 1 with appropriately modified $$\\alpha$$ and $$\\beta$$, the output $$w_n$$ satisfies\n\n$$1 - \\left(w_n^T v_1\\right)^2 \\leq 1 + 2\\beta\\tau_{\\text{mix}} \\log(n) + 2\\alpha \\cdot \\frac{C \\log(\\delta)}{\\delta^2} + C_1V\\tau_{\\text{mix}} \\log(n) + C_2M(M + \\lambda_1)^2 \\log^2(n) \\tau_{\\text{mix}} \\frac{\\eta^2n}{(\\lambda_1 - \\lambda_2)^2n(\\lambda_1 - \\lambda_2)^3n^2}$$\n\nwith probability at least $$(1 - \\delta)$$. Here $$C$$ is an absolute constant and $$C_1 := 30\\alpha^2 \\cdot \\frac{2\\alpha - 1}{\\alpha}, C_2 := 35\\alpha^3 \\cdot \\frac{\\alpha}{\\alpha - 1}$$.\n\n**Remark 2.** Data downsampling to reduce dependence amongst samples has been suggested in recent work [26, 22, 3]. In Corollary 1, we establish that the rate obtained is sub-optimal compared to Theorem 1 by a $$\\log(n)$$ factor. We prove this by a simple yet elegant observation: the downsampled data stream can be considered to be drawn from a Markov chain with transition kernel $$P^k(., .)$$ since each data-point is $$k$$ steps away from the previous one. For sufficiently large $$k$$, this implies that the mixing time of this chain is $$\\Theta(1)$$. These new parameters are used to select the modified values of $$\\alpha$$, $$\\beta$$ according to Lemma S.12 in the Supplement.\n\nThe proof of Theorem 1 follows the same general recipe as in [15] for obtaining a bound on the $$\\sin^2$$ error. However, the original proof techniques heavily rely on the IID setting. We carry out a refined analysis for each step under the Markovian data model by a careful control of error terms arising out of dependence. The first step involves obtaining a high-probability bound on the $$\\sin^2$$ error, by noting that Oja\u2019s algorithm on $$n$$ data-points can be viewed as a single iteration of the power method on $$B_n$$. Therefore, fixing a $$\\delta \\in (0, 1)$$ using Lemma 3.1 from [15], we have with probability at least $$(1 - \\delta)$$,\n\n$$\\sin^2(w_n, v_1) \\leq C \\log(\\delta) \\cdot \\frac{1}{\\delta} \\cdot \\frac{\\text{Tr}(V^T_n V^\\perp)}{v^T_\\perp B_n B^T_n v_1}$$\n\nwhere $$C$$ is an absolute constant. The numerator is bounded by first bounding its expectation (see Theorem 3) and then using Markov\u2019s inequality. To bound the denominator, similar to [15], we will use Chebyshev\u2019s inequality. Theorem 4 provides a lower bound for the expectation $$E[v^T_1 B_n B^T_n v_1]$$. Chebyshev\u2019s inequality also requires upper-bounding the variance of $$E[v^T_1 B_n B^T_n v_1]$$, which requires us to bound $$E[v^T_1 B_n B^T_n v_1]^2$$ (see Theorem 5).\n\n**Main Technical Tools**\n\nIn this section, we provide a sketch of the main arguments used in our proof.\n\nWarm-up with downsampled Oja\u2019s algorithm: We start with the simple downsampled Oja\u2019s algorithm to build intuition. Here, one applies Oja\u2019s update rule (Eq 1) to every $$k$$th data-point, for a suitably chosen $$k$$. For $$k = \\lceil L\\tau_{\\text{mix}} \\log n \\rceil$$, the total variation distance between any consecutive data-points in the downsampled data stream is $$O(n^{-L})$$. As we show in Corollary 1, the error of this algorithm is similar to the error of Oja\u2019s algorithm applied to $$n/k$$ data-points in the IID setting, i.e., $$O(V\\tau_{\\text{mix}} \\log n/n)$$.\n\nWe will take $$E[v^T_1 B_n B^T_n v_1]$$ as an example. Let us introduce some notation.\n\n$$B_{j,i} := I + \\eta_j X_j X^T_j I + \\eta_{j-1} X_{j-1} X^T_{j-1} \\ldots I + \\eta_i X_i X^T_i$$\n\nWe peel this quantity one matrix at a time from the inside. Note that for a reversible Markov chain, standard results imply (see Lemma 1) that the mixing conditions apply to the conditional distribution of a state given another state $$k$$ steps in the \u201cfuture\u201d (see Supplement section S.3 for a proof). Recall $$d_{\\text{mix}}(k)$$ from Section 2.1.", "images": [], "items": [{"type": "text", "value": "The leading term of Theorem 1 is worse by a factor of $$1 - |\\lambda_2(P_1)|$$. Further, it has an additive lower order term $$O\\left(\\frac{n^2}{\\log^2(n)}\\right)$$ due to the covariance between data-points in the Markovian case.\n\n**Corollary 1.** (Downsampled Oja\u2019s algorithm) Fix a $$\\delta \\in (0, 1)$$. If Oja\u2019s algorithm is applied on the downsampled data-stream with every $$k$$th data-point, where $$k := \\tau_{\\text{mix}} \\cdot \\frac{\\eta^2n}{\\log(n)}$$ then under the conditions of Theorem 1 with appropriately modified $$\\alpha$$ and $$\\beta$$, the output $$w_n$$ satisfies\n\n$$1 - \\left(w_n^T v_1\\right)^2 \\leq 1 + 2\\beta\\tau_{\\text{mix}} \\log(n) + 2\\alpha \\cdot \\frac{C \\log(\\delta)}{\\delta^2} + C_1V\\tau_{\\text{mix}} \\log(n) + C_2M(M + \\lambda_1)^2 \\log^2(n) \\tau_{\\text{mix}} \\frac{\\eta^2n}{(\\lambda_1 - \\lambda_2)^2n(\\lambda_1 - \\lambda_2)^3n^2}$$\n\nwith probability at least $$(1 - \\delta)$$. Here $$C$$ is an absolute constant and $$C_1 := 30\\alpha^2 \\cdot \\frac{2\\alpha - 1}{\\alpha}, C_2 := 35\\alpha^3 \\cdot \\frac{\\alpha}{\\alpha - 1}$$.\n\n**Remark 2.** Data downsampling to reduce dependence amongst samples has been suggested in recent work [26, 22, 3]. In Corollary 1, we establish that the rate obtained is sub-optimal compared to Theorem 1 by a $$\\log(n)$$ factor. We prove this by a simple yet elegant observation: the downsampled data stream can be considered to be drawn from a Markov chain with transition kernel $$P^k(., .)$$ since each data-point is $$k$$ steps away from the previous one. For sufficiently large $$k$$, this implies that the mixing time of this chain is $$\\Theta(1)$$. These new parameters are used to select the modified values of $$\\alpha$$, $$\\beta$$ according to Lemma S.12 in the Supplement.\n\nThe proof of Theorem 1 follows the same general recipe as in [15] for obtaining a bound on the $$\\sin^2$$ error. However, the original proof techniques heavily rely on the IID setting. We carry out a refined analysis for each step under the Markovian data model by a careful control of error terms arising out of dependence. The first step involves obtaining a high-probability bound on the $$\\sin^2$$ error, by noting that Oja\u2019s algorithm on $$n$$ data-points can be viewed as a single iteration of the power method on $$B_n$$. Therefore, fixing a $$\\delta \\in (0, 1)$$ using Lemma 3.1 from [15], we have with probability at least $$(1 - \\delta)$$,\n\n$$\\sin^2(w_n, v_1) \\leq C \\log(\\delta) \\cdot \\frac{1}{\\delta} \\cdot \\frac{\\text{Tr}(V^T_n V^\\perp)}{v^T_\\perp B_n B^T_n v_1}$$\n\nwhere $$C$$ is an absolute constant. The numerator is bounded by first bounding its expectation (see Theorem 3) and then using Markov\u2019s inequality. To bound the denominator, similar to [15], we will use Chebyshev\u2019s inequality. Theorem 4 provides a lower bound for the expectation $$E[v^T_1 B_n B^T_n v_1]$$. Chebyshev\u2019s inequality also requires upper-bounding the variance of $$E[v^T_1 B_n B^T_n v_1]$$, which requires us to bound $$E[v^T_1 B_n B^T_n v_1]^2$$ (see Theorem 5).\n\n**Main Technical Tools**\n\nIn this section, we provide a sketch of the main arguments used in our proof.\n\nWarm-up with downsampled Oja\u2019s algorithm: We start with the simple downsampled Oja\u2019s algorithm to build intuition. Here, one applies Oja\u2019s update rule (Eq 1) to every $$k$$th data-point, for a suitably chosen $$k$$. For $$k = \\lceil L\\tau_{\\text{mix}} \\log n \\rceil$$, the total variation distance between any consecutive data-points in the downsampled data stream is $$O(n^{-L})$$. As we show in Corollary 1, the error of this algorithm is similar to the error of Oja\u2019s algorithm applied to $$n/k$$ data-points in the IID setting, i.e., $$O(V\\tau_{\\text{mix}} \\log n/n)$$.\n\nWe will take $$E[v^T_1 B_n B^T_n v_1]$$ as an example. Let us introduce some notation.\n\n$$B_{j,i} := I + \\eta_j X_j X^T_j I + \\eta_{j-1} X_{j-1} X^T_{j-1} \\ldots I + \\eta_i X_i X^T_i$$\n\nWe peel this quantity one matrix at a time from the inside. Note that for a reversible Markov chain, standard results imply (see Lemma 1) that the mixing conditions apply to the conditional distribution of a state given another state $$k$$ steps in the \u201cfuture\u201d (see Supplement section S.3 for a proof). Recall $$d_{\\text{mix}}(k)$$ from Section 2.1.", "md": "The leading term of Theorem 1 is worse by a factor of $$1 - |\\lambda_2(P_1)|$$. Further, it has an additive lower order term $$O\\left(\\frac{n^2}{\\log^2(n)}\\right)$$ due to the covariance between data-points in the Markovian case.\n\n**Corollary 1.** (Downsampled Oja\u2019s algorithm) Fix a $$\\delta \\in (0, 1)$$. If Oja\u2019s algorithm is applied on the downsampled data-stream with every $$k$$th data-point, where $$k := \\tau_{\\text{mix}} \\cdot \\frac{\\eta^2n}{\\log(n)}$$ then under the conditions of Theorem 1 with appropriately modified $$\\alpha$$ and $$\\beta$$, the output $$w_n$$ satisfies\n\n$$1 - \\left(w_n^T v_1\\right)^2 \\leq 1 + 2\\beta\\tau_{\\text{mix}} \\log(n) + 2\\alpha \\cdot \\frac{C \\log(\\delta)}{\\delta^2} + C_1V\\tau_{\\text{mix}} \\log(n) + C_2M(M + \\lambda_1)^2 \\log^2(n) \\tau_{\\text{mix}} \\frac{\\eta^2n}{(\\lambda_1 - \\lambda_2)^2n(\\lambda_1 - \\lambda_2)^3n^2}$$\n\nwith probability at least $$(1 - \\delta)$$. Here $$C$$ is an absolute constant and $$C_1 := 30\\alpha^2 \\cdot \\frac{2\\alpha - 1}{\\alpha}, C_2 := 35\\alpha^3 \\cdot \\frac{\\alpha}{\\alpha - 1}$$.\n\n**Remark 2.** Data downsampling to reduce dependence amongst samples has been suggested in recent work [26, 22, 3]. In Corollary 1, we establish that the rate obtained is sub-optimal compared to Theorem 1 by a $$\\log(n)$$ factor. We prove this by a simple yet elegant observation: the downsampled data stream can be considered to be drawn from a Markov chain with transition kernel $$P^k(., .)$$ since each data-point is $$k$$ steps away from the previous one. For sufficiently large $$k$$, this implies that the mixing time of this chain is $$\\Theta(1)$$. These new parameters are used to select the modified values of $$\\alpha$$, $$\\beta$$ according to Lemma S.12 in the Supplement.\n\nThe proof of Theorem 1 follows the same general recipe as in [15] for obtaining a bound on the $$\\sin^2$$ error. However, the original proof techniques heavily rely on the IID setting. We carry out a refined analysis for each step under the Markovian data model by a careful control of error terms arising out of dependence. The first step involves obtaining a high-probability bound on the $$\\sin^2$$ error, by noting that Oja\u2019s algorithm on $$n$$ data-points can be viewed as a single iteration of the power method on $$B_n$$. Therefore, fixing a $$\\delta \\in (0, 1)$$ using Lemma 3.1 from [15], we have with probability at least $$(1 - \\delta)$$,\n\n$$\\sin^2(w_n, v_1) \\leq C \\log(\\delta) \\cdot \\frac{1}{\\delta} \\cdot \\frac{\\text{Tr}(V^T_n V^\\perp)}{v^T_\\perp B_n B^T_n v_1}$$\n\nwhere $$C$$ is an absolute constant. The numerator is bounded by first bounding its expectation (see Theorem 3) and then using Markov\u2019s inequality. To bound the denominator, similar to [15], we will use Chebyshev\u2019s inequality. Theorem 4 provides a lower bound for the expectation $$E[v^T_1 B_n B^T_n v_1]$$. Chebyshev\u2019s inequality also requires upper-bounding the variance of $$E[v^T_1 B_n B^T_n v_1]$$, which requires us to bound $$E[v^T_1 B_n B^T_n v_1]^2$$ (see Theorem 5).\n\n**Main Technical Tools**\n\nIn this section, we provide a sketch of the main arguments used in our proof.\n\nWarm-up with downsampled Oja\u2019s algorithm: We start with the simple downsampled Oja\u2019s algorithm to build intuition. Here, one applies Oja\u2019s update rule (Eq 1) to every $$k$$th data-point, for a suitably chosen $$k$$. For $$k = \\lceil L\\tau_{\\text{mix}} \\log n \\rceil$$, the total variation distance between any consecutive data-points in the downsampled data stream is $$O(n^{-L})$$. As we show in Corollary 1, the error of this algorithm is similar to the error of Oja\u2019s algorithm applied to $$n/k$$ data-points in the IID setting, i.e., $$O(V\\tau_{\\text{mix}} \\log n/n)$$.\n\nWe will take $$E[v^T_1 B_n B^T_n v_1]$$ as an example. Let us introduce some notation.\n\n$$B_{j,i} := I + \\eta_j X_j X^T_j I + \\eta_{j-1} X_{j-1} X^T_{j-1} \\ldots I + \\eta_i X_i X^T_i$$\n\nWe peel this quantity one matrix at a time from the inside. Note that for a reversible Markov chain, standard results imply (see Lemma 1) that the mixing conditions apply to the conditional distribution of a state given another state $$k$$ steps in the \u201cfuture\u201d (see Supplement section S.3 for a proof). Recall $$d_{\\text{mix}}(k)$$ from Section 2.1."}]}, {"page": 7, "text": "Lemma 1. Under Assumption 1,                          1                |P (Zt = s|Zt+k = t) \u2212                 \u03c0 (s)| = dmix (k).\n                                                      2 sup\n                                                          t\u2208\u2126     s\nIt will be helpful to explain our analysis by comparing it with the IID setting. For this reason, we will\nuse EIID[.] to denote the expectation under the IID data model.\n\u03b1n,1 := E          vT1 BnBT    n v1      = E      vT1 Bn,2       I + \u03b71\u03a3 + \u03b71(X1XT               1 \u2212     \u03a3)       I + \u03b71\u03a3 + \u03b71(X1XT               1 \u2212     \u03a3)   T BT   n,2v1\n        = E       vT1 Bn,2 (I + \u03b71\u03a3)2 BT             n,2v1      + 2\u03b71T1 + \u03b72         1T2,                                                                 (8)\nwhere the first term is smaller than (1 + \u03b71\u03bb1)2\u03b1n,2. We define T1 and T2 as follows. T1 :=\nE    vT1 Bn,2 (I + \u03b71\u03a3)              X1XT     1 \u2212    \u03a3     BT n,2v1     , and T2 := E           vT1 Bn,2       X1XT     1 \u2212    \u03a3   2 BT   n,2v1     .\n      For the IID setting, the second term is zero, and the third term can be bounded as follows:\nEIID     vT1 Bn,2       X1XT    1 \u2212     \u03a3   2 BT  n,2v1       = EIID       vT1 Bn,2E           X1XT    1 \u2212     \u03a3   2    BT n,2v1      \u2264    VEIID       vT1 Bn,2BT     n,2v1\nLet us denote the IID version of \u03b1n,i by \u03b1IID                      n,i = EIID[vT      1 Bn,iBT     n,iv1]. The final recursion for the IID\ncase becomes: \u03b1IID        n,1 \u2264     (1 + 2\u03b71\u03bb1 + \u03b72          1    \u03bb21 + V      )\u03b1IIDn,1. So, for our Markovian data model, the hope\nis that the cross term T1 (which has a multiplicative factor of \u03b71) is O(\u03b71) and T2 is O(\u03b72                                                  1). We will\nstart with the T1 term, which is zero in the IID setting.\n      We hope to reduce the product Bn,2(X1XT                            1 \u2212     \u03a3) into a product of nearly independent matrices.\nOne hope is that if instead of Bn,2, we had Bn,2+k for some suitably large integer k, then using\n(reverse) mixing properties of the Markov chain, we could argue using Lemma 1 that E[X1XT                                                               1 \u2212\n\u03a3|s1+k, . . . , sn] is very close to zero.The following lemma formally bounds the deviation of the\nlength-k matrix product from identity.\nLemma 2. Let Assumption 3 hold. If \u2200i \u2208                                    [n], \u03b7iki (M + \u03bb1) \u2264                 \u03f5, \u03f5 \u2208      (0, 1) and \u03b7i forms a\nnon-increasing sequence then \u2200                      m \u2264      n \u2212    kn,\n                              \u2225Bm+km\u22121,m \u2212               I\u22252 \u2264      (1 + \u03f5) km\u03b7m (M + \u03bb1) and                                                             (9)\n                                                                m+km\u22121         \u03b7tXtXT     t       \u2264   k2 m\u03b72  m (M + \u03bb1)2                               (10)\n                               Bm+km\u22121,m \u2212               I \u2212        t=m                        2\n      Lemma 2 bounds the norm of the matrix product Bt+k                                   t\u22121,t at two levels. The first result provides\na coarse bound, approximating linear and higher-order terms. The second result provides a finer\nbound, preserving the linear term and approximating quadratic and higher-order terms. The proofs\ninvolve a straightforward combinatorial expansion of Bt+k                                     t\u22121,t and are deferred to the Supplement\nsection S.3.\n      Approximating  k+1           i=2 (I + \u03b7iXiXT          i ) requires \u03b71k to be small. Since this is a recursive argument,\nwe would need \u03b7ik to be small for i = 1, . . . n, which is satisfied by the strong condition \u03b71k is small.\nTo obtain a tight analysis, we choose k adaptively. We set ki = \u03c4mix(\u03b72                                         i ) (see definition in Eq 3).\n      As we will show in detail in the Supplement, Lemma 2 Eq 10 along with the adaptive choice of\nki gives us a sharp error bound. Using it, we can bound T1 (see Eq 8) as:\n         k+1           \uf8ee                                                                                                                         \uf8f9\nT1 \u2264            \u03b7jE    \uf8ef                          XjXT         (I + \u03b71\u03a3) (X1X1 \u2212                 \u03a3) |Xk+2, . . . , Xn             BT             \uf8fa\n         j=2           \uf8f0vT 1 Bn,k+2 E                     j                        T1,j                                              n,k+2v1     \uf8fb   + O(\u03b72     1k2 1)\u03b1n,k+2\nNaively bounding the T1,j term by O(1) leads to the same rate as downsampled Oja\u2019s algorithm.\n      In the following lemma, we will establish that, indeed, T1,j has a much smaller norm. The novelty\nof our bound is not just in using the mixing properties of the Markov chain but also in teasing out the\nvariance parameter V. We will state the lemma, in a slightly more general form as -\n                                                                              7", "md": "## Lemma 1\n\nUnder Assumption 1,\n\n$$|P(Z_t = s|Z_{t+k} = t) - \\pi(s)| = d_{\\text{mix}}(k).$$\n\nIt will be helpful to explain our analysis by comparing it with the IID setting. For this reason, we will use EIID[.] to denote the expectation under the IID data model.\n\n$$\\alpha_{n,1} := E[v^T_1 B_nB^T_n v_1] = E[v^T_1 B_{n,2} (I + \\eta_1\\Sigma + \\eta_1(X_1X^T_1 - \\Sigma))(I + \\eta_1\\Sigma + \\eta_1(X_1X^T_1 - \\Sigma))^T B^T_{n,2}v_1 + 2\\eta_1T_1 + \\eta_21T_2] \\quad (8)$$\n\nwhere the first term is smaller than $$(1 + \\eta_1\\lambda_1)^2\\alpha_{n,2}$$. We define $$T_1$$ and $$T_2$$ as follows. $$T_1 := E[v^T_1 B_{n,2} (I + \\eta_1\\Sigma)X_1X^T_1 - \\Sigma)B^T_{n,2}v_1$$, and $$T_2 := E[v^T_1 B_{n,2}X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1$$.\n\nFor the IID setting, the second term is zero, and the third term can be bounded as follows:\n\n$$E_{\\text{IID}}[v^T_1 B_{n,2}X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1] = E_{\\text{IID}}[v^T_1 B_{n,2}E[X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1] \\leq VE_{\\text{IID}}[v^T_1 B_{n,2}B^T_{n,2}v_1]$$\n\nLet us denote the IID version of $$\\alpha_{n,i}$$ by $$\\alpha_{\\text{IID},n,i} = E_{\\text{IID}}[v^T_1 B_{n,i}B^T_{n,i}v_1]$$. The final recursion for the IID case becomes: $$\\alpha_{\\text{IID},n,1} \\leq (1 + 2\\eta_1\\lambda_1 + \\eta_21\\lambda_1^2 + V)\\alpha_{\\text{IID},n,1}$$. So, for our Markovian data model, the hope is that the cross term $$T_1$$ (which has a multiplicative factor of $$\\eta_1$$) is $$O(\\eta_1)$$ and $$T_2$$ is $$O(\\eta_21)$$. We will start with the $$T_1$$ term, which is zero in the IID setting.\n\nWe hope to reduce the product $$B_{n,2}(X_1X^T_1 - \\Sigma)$$ into a product of nearly independent matrices. One hope is that if instead of $$B_{n,2}$$, we had $$B_{n,2+k}$$ for some suitably large integer $$k$$, then using (reverse) mixing properties of the Markov chain, we could argue using Lemma 1 that $$E[X_1X^T_1 - \\Sigma|s_{1+k},...,s_n]$$ is very close to zero. The following lemma formally bounds the deviation of the length-k matrix product from identity.\n\n## Lemma 2\n\nLet Assumption 3 hold. If $$\\forall i \\in [n], \\eta_iki(M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence then $$\\forall m \\leq n - kn,$$\n\n$$\\|B_{m+k,m-1} - I\\|_2 \\leq (1 + \\epsilon)k\\eta_m(M + \\lambda_1)$$ and\n\n$$\\|B_{m+k,m-1}\\eta_tX_tX^T_t \\leq k^2\\eta^2_m(M + \\lambda_1)^2$$. (10)\n\nLemma 2 bounds the norm of the matrix product $$B_{t+k}t-1,t$$ at two levels. The first result provides a coarse bound, approximating linear and higher-order terms. The second result provides a finer bound, preserving the linear term and approximating quadratic and higher-order terms. The proofs involve a straightforward combinatorial expansion of $$B_{t+k}t-1,t$$ and are deferred to the Supplement section S.3.\n\nApproximating $$\\sum_{i=2}^{k+1}(I + \\eta_iX_iX^T_i)$$ requires $$\\eta_1k$$ to be small. Since this is a recursive argument, we would need $$\\eta_ik$$ to be small for $$i = 1, . . . n$$, which is satisfied by the strong condition $$\\eta_1k$$ is small.\n\nTo obtain a tight analysis, we choose $$k$$ adaptively. We set $$k_i = \\tau_{\\text{mix}}(\\eta^2_i)$$ (see definition in Eq 3).\n\nAs we will show in detail in the Supplement, Lemma 2 Eq 10 along with the adaptive choice of $$k_i$$ gives us a sharp error bound. Using it, we can bound $$T_1$$ (see Eq 8) as:\n\n$$T_1 \\leq \\sum_{j=2}^{k+1} \\eta_jE[X_jX^T_j(I + \\eta_1\\Sigma)(X_1X^T_1 - \\Sigma)|X_{k+2},...,X_n]B^T_{n,k+2}E_{n,k+2}v_1 + O(\\eta^2_1k^2_1)\\alpha_{n,k+2}$$\n\nNaively bounding the $$T_{1,j}$$ term by $$O(1)$$ leads to the same rate as downsampled Oja\u2019s algorithm.\n\nIn the following lemma, we will establish that, indeed, $$T_{1,j}$$ has a much smaller norm. The novelty of our bound is not just in using the mixing properties of the Markov chain but also in teasing out the variance parameter $$V$$. We will state the lemma, in a slightly more general form as -\n\n$$T1 \\leq \\sum_{j=2}^{k+1} \\eta_j E[X_j X^T_j (I + \\eta_1 \\Sigma) (X_1 X^T_1 - \\Sigma) |X_{k+2},...,X_n] B^T_{n,k+2} E_{n,k+2} v_1 + O(\\eta^2_1 k^2_1) \\alpha_{n,k+2}$$\n\nNaively bounding the $$T_{1,j}$$ term by $$O(1)$$ leads to the same rate as downsample", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Lemma 1", "md": "## Lemma 1"}, {"type": "text", "value": "Under Assumption 1,\n\n$$|P(Z_t = s|Z_{t+k} = t) - \\pi(s)| = d_{\\text{mix}}(k).$$\n\nIt will be helpful to explain our analysis by comparing it with the IID setting. For this reason, we will use EIID[.] to denote the expectation under the IID data model.\n\n$$\\alpha_{n,1} := E[v^T_1 B_nB^T_n v_1] = E[v^T_1 B_{n,2} (I + \\eta_1\\Sigma + \\eta_1(X_1X^T_1 - \\Sigma))(I + \\eta_1\\Sigma + \\eta_1(X_1X^T_1 - \\Sigma))^T B^T_{n,2}v_1 + 2\\eta_1T_1 + \\eta_21T_2] \\quad (8)$$\n\nwhere the first term is smaller than $$(1 + \\eta_1\\lambda_1)^2\\alpha_{n,2}$$. We define $$T_1$$ and $$T_2$$ as follows. $$T_1 := E[v^T_1 B_{n,2} (I + \\eta_1\\Sigma)X_1X^T_1 - \\Sigma)B^T_{n,2}v_1$$, and $$T_2 := E[v^T_1 B_{n,2}X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1$$.\n\nFor the IID setting, the second term is zero, and the third term can be bounded as follows:\n\n$$E_{\\text{IID}}[v^T_1 B_{n,2}X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1] = E_{\\text{IID}}[v^T_1 B_{n,2}E[X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1] \\leq VE_{\\text{IID}}[v^T_1 B_{n,2}B^T_{n,2}v_1]$$\n\nLet us denote the IID version of $$\\alpha_{n,i}$$ by $$\\alpha_{\\text{IID},n,i} = E_{\\text{IID}}[v^T_1 B_{n,i}B^T_{n,i}v_1]$$. The final recursion for the IID case becomes: $$\\alpha_{\\text{IID},n,1} \\leq (1 + 2\\eta_1\\lambda_1 + \\eta_21\\lambda_1^2 + V)\\alpha_{\\text{IID},n,1}$$. So, for our Markovian data model, the hope is that the cross term $$T_1$$ (which has a multiplicative factor of $$\\eta_1$$) is $$O(\\eta_1)$$ and $$T_2$$ is $$O(\\eta_21)$$. We will start with the $$T_1$$ term, which is zero in the IID setting.\n\nWe hope to reduce the product $$B_{n,2}(X_1X^T_1 - \\Sigma)$$ into a product of nearly independent matrices. One hope is that if instead of $$B_{n,2}$$, we had $$B_{n,2+k}$$ for some suitably large integer $$k$$, then using (reverse) mixing properties of the Markov chain, we could argue using Lemma 1 that $$E[X_1X^T_1 - \\Sigma|s_{1+k},...,s_n]$$ is very close to zero. The following lemma formally bounds the deviation of the length-k matrix product from identity.", "md": "Under Assumption 1,\n\n$$|P(Z_t = s|Z_{t+k} = t) - \\pi(s)| = d_{\\text{mix}}(k).$$\n\nIt will be helpful to explain our analysis by comparing it with the IID setting. For this reason, we will use EIID[.] to denote the expectation under the IID data model.\n\n$$\\alpha_{n,1} := E[v^T_1 B_nB^T_n v_1] = E[v^T_1 B_{n,2} (I + \\eta_1\\Sigma + \\eta_1(X_1X^T_1 - \\Sigma))(I + \\eta_1\\Sigma + \\eta_1(X_1X^T_1 - \\Sigma))^T B^T_{n,2}v_1 + 2\\eta_1T_1 + \\eta_21T_2] \\quad (8)$$\n\nwhere the first term is smaller than $$(1 + \\eta_1\\lambda_1)^2\\alpha_{n,2}$$. We define $$T_1$$ and $$T_2$$ as follows. $$T_1 := E[v^T_1 B_{n,2} (I + \\eta_1\\Sigma)X_1X^T_1 - \\Sigma)B^T_{n,2}v_1$$, and $$T_2 := E[v^T_1 B_{n,2}X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1$$.\n\nFor the IID setting, the second term is zero, and the third term can be bounded as follows:\n\n$$E_{\\text{IID}}[v^T_1 B_{n,2}X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1] = E_{\\text{IID}}[v^T_1 B_{n,2}E[X_1X^T_1 - \\Sigma)^2B^T_{n,2}v_1] \\leq VE_{\\text{IID}}[v^T_1 B_{n,2}B^T_{n,2}v_1]$$\n\nLet us denote the IID version of $$\\alpha_{n,i}$$ by $$\\alpha_{\\text{IID},n,i} = E_{\\text{IID}}[v^T_1 B_{n,i}B^T_{n,i}v_1]$$. The final recursion for the IID case becomes: $$\\alpha_{\\text{IID},n,1} \\leq (1 + 2\\eta_1\\lambda_1 + \\eta_21\\lambda_1^2 + V)\\alpha_{\\text{IID},n,1}$$. So, for our Markovian data model, the hope is that the cross term $$T_1$$ (which has a multiplicative factor of $$\\eta_1$$) is $$O(\\eta_1)$$ and $$T_2$$ is $$O(\\eta_21)$$. We will start with the $$T_1$$ term, which is zero in the IID setting.\n\nWe hope to reduce the product $$B_{n,2}(X_1X^T_1 - \\Sigma)$$ into a product of nearly independent matrices. One hope is that if instead of $$B_{n,2}$$, we had $$B_{n,2+k}$$ for some suitably large integer $$k$$, then using (reverse) mixing properties of the Markov chain, we could argue using Lemma 1 that $$E[X_1X^T_1 - \\Sigma|s_{1+k},...,s_n]$$ is very close to zero. The following lemma formally bounds the deviation of the length-k matrix product from identity."}, {"type": "heading", "lvl": 2, "value": "Lemma 2", "md": "## Lemma 2"}, {"type": "text", "value": "Let Assumption 3 hold. If $$\\forall i \\in [n], \\eta_iki(M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence then $$\\forall m \\leq n - kn,$$\n\n$$\\|B_{m+k,m-1} - I\\|_2 \\leq (1 + \\epsilon)k\\eta_m(M + \\lambda_1)$$ and\n\n$$\\|B_{m+k,m-1}\\eta_tX_tX^T_t \\leq k^2\\eta^2_m(M + \\lambda_1)^2$$. (10)\n\nLemma 2 bounds the norm of the matrix product $$B_{t+k}t-1,t$$ at two levels. The first result provides a coarse bound, approximating linear and higher-order terms. The second result provides a finer bound, preserving the linear term and approximating quadratic and higher-order terms. The proofs involve a straightforward combinatorial expansion of $$B_{t+k}t-1,t$$ and are deferred to the Supplement section S.3.\n\nApproximating $$\\sum_{i=2}^{k+1}(I + \\eta_iX_iX^T_i)$$ requires $$\\eta_1k$$ to be small. Since this is a recursive argument, we would need $$\\eta_ik$$ to be small for $$i = 1, . . . n$$, which is satisfied by the strong condition $$\\eta_1k$$ is small.\n\nTo obtain a tight analysis, we choose $$k$$ adaptively. We set $$k_i = \\tau_{\\text{mix}}(\\eta^2_i)$$ (see definition in Eq 3).\n\nAs we will show in detail in the Supplement, Lemma 2 Eq 10 along with the adaptive choice of $$k_i$$ gives us a sharp error bound. Using it, we can bound $$T_1$$ (see Eq 8) as:\n\n$$T_1 \\leq \\sum_{j=2}^{k+1} \\eta_jE[X_jX^T_j(I + \\eta_1\\Sigma)(X_1X^T_1 - \\Sigma)|X_{k+2},...,X_n]B^T_{n,k+2}E_{n,k+2}v_1 + O(\\eta^2_1k^2_1)\\alpha_{n,k+2}$$\n\nNaively bounding the $$T_{1,j}$$ term by $$O(1)$$ leads to the same rate as downsampled Oja\u2019s algorithm.\n\nIn the following lemma, we will establish that, indeed, $$T_{1,j}$$ has a much smaller norm. The novelty of our bound is not just in using the mixing properties of the Markov chain but also in teasing out the variance parameter $$V$$. We will state the lemma, in a slightly more general form as -\n\n$$T1 \\leq \\sum_{j=2}^{k+1} \\eta_j E[X_j X^T_j (I + \\eta_1 \\Sigma) (X_1 X^T_1 - \\Sigma) |X_{k+2},...,X_n] B^T_{n,k+2} E_{n,k+2} v_1 + O(\\eta^2_1 k^2_1) \\alpha_{n,k+2}$$\n\nNaively bounding the $$T_{1,j}$$ term by $$O(1)$$ leads to the same rate as downsample", "md": "Let Assumption 3 hold. If $$\\forall i \\in [n], \\eta_iki(M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence then $$\\forall m \\leq n - kn,$$\n\n$$\\|B_{m+k,m-1} - I\\|_2 \\leq (1 + \\epsilon)k\\eta_m(M + \\lambda_1)$$ and\n\n$$\\|B_{m+k,m-1}\\eta_tX_tX^T_t \\leq k^2\\eta^2_m(M + \\lambda_1)^2$$. (10)\n\nLemma 2 bounds the norm of the matrix product $$B_{t+k}t-1,t$$ at two levels. The first result provides a coarse bound, approximating linear and higher-order terms. The second result provides a finer bound, preserving the linear term and approximating quadratic and higher-order terms. The proofs involve a straightforward combinatorial expansion of $$B_{t+k}t-1,t$$ and are deferred to the Supplement section S.3.\n\nApproximating $$\\sum_{i=2}^{k+1}(I + \\eta_iX_iX^T_i)$$ requires $$\\eta_1k$$ to be small. Since this is a recursive argument, we would need $$\\eta_ik$$ to be small for $$i = 1, . . . n$$, which is satisfied by the strong condition $$\\eta_1k$$ is small.\n\nTo obtain a tight analysis, we choose $$k$$ adaptively. We set $$k_i = \\tau_{\\text{mix}}(\\eta^2_i)$$ (see definition in Eq 3).\n\nAs we will show in detail in the Supplement, Lemma 2 Eq 10 along with the adaptive choice of $$k_i$$ gives us a sharp error bound. Using it, we can bound $$T_1$$ (see Eq 8) as:\n\n$$T_1 \\leq \\sum_{j=2}^{k+1} \\eta_jE[X_jX^T_j(I + \\eta_1\\Sigma)(X_1X^T_1 - \\Sigma)|X_{k+2},...,X_n]B^T_{n,k+2}E_{n,k+2}v_1 + O(\\eta^2_1k^2_1)\\alpha_{n,k+2}$$\n\nNaively bounding the $$T_{1,j}$$ term by $$O(1)$$ leads to the same rate as downsampled Oja\u2019s algorithm.\n\nIn the following lemma, we will establish that, indeed, $$T_{1,j}$$ has a much smaller norm. The novelty of our bound is not just in using the mixing properties of the Markov chain but also in teasing out the variance parameter $$V$$. We will state the lemma, in a slightly more general form as -\n\n$$T1 \\leq \\sum_{j=2}^{k+1} \\eta_j E[X_j X^T_j (I + \\eta_1 \\Sigma) (X_1 X^T_1 - \\Sigma) |X_{k+2},...,X_n] B^T_{n,k+2} E_{n,k+2} v_1 + O(\\eta^2_1 k^2_1) \\alpha_{n,k+2}$$\n\nNaively bounding the $$T_{1,j}$$ term by $$O(1)$$ leads to the same rate as downsample"}]}, {"page": 8, "text": "Lemma 3. Under Assumptions 1, 2 and 3, for i < j \u2264                              i + ki,\n       E      XiXT   i \u2212    \u03a3    SX    jXT j |si+ki, . . . sn       2 \u2264      |\u03bb2 (P   )|j\u2212i V + 8\u03b72     i M (M + \u03bb1)             \u2225S\u22252\nwhere ki is as defined in Lemma S.12 and S is a constant symmetric positive semi-definite matrix.\n     Lemma 3 bounds the norm of the covariance between matrices                                        XiXT    i \u2212    \u03a3    S and XjXT       j .\nIn particular, this implies that the norm of T1,j decays as |\u03bb2 (P                             )|j\u22121. The proof uses a spectral\nargument that replaces a coarse approximation by a sum of ki O(1) terms to sum of k exponentially\ndecaying terms, thereby removing the dependence on ki, which can be as large as log(n). The proof\nis deferred to the Supplement section S.4. The details can be found in Supplement section S.4.\n     Let {c1, c2, c3, c4} be positive constants for ease of notation. Coming back to Eq 8, we can\n                                                         c1|\u03bb2(P     )|V\nbound T1 as follows: T1 \u2264              \u03b1n,k+2        \u03b71  1 \u2212   |\u03bb2(P    )| + c2\u03b72   1k2 1   . A similar argument can be applied\nto bound T2 as: T2 \u2264            \u03b1n,k+2      V + c3\u03b71k2      1  . Putting everything together in 8, we have\n\u03b1n,1 \u2264       (1 + \u03b71\u03bb1)2 + V             \u03b1n,2     +         c1|\u03bb2 (P    ) |     V\u03b72 1\u03b1n,k+2        +                 c4\u03b73 1k2 1\u03b1n,k+2\n                Recursion for IID setting                 1 \u2212    |\u03bb2 (P   )|                             Error due to approximation of matrix product\n                                                          Error due to Markovian dependence\nRecursing on this inequality gives us our bound on E                          vT         n v1     (Theorem 2). We are now ready\n                                                                               1 BnBT\nto present all our accompanying theorems.\n5      Intermediate Theorems for Convergence Analysis\nIn this section, we present our accompanying theorems which are used to obtain the main result in\nTheorem 1. But before doing so, we will need to establish some notation. Let ki := \u03c4mix                                            \u03b72i  , and\n                                                    \u03b1                                                                                1\nthe step-sizes be set as \u03b7i :=              (\u03bb1\u2212\u03bb2)(\u03b2+i) with \u03b1, \u03b2 as defined in Theorem 1. Let \u03f5 :=                                100. As\nshown in Lemma S.12 in Supplement Section S.3 our choice of step-sizes satisfy, \u2200i \u2208                                          [n],\n  C.1 \u03b7iki (M + \u03bb1) \u2264              \u03f5              C.2 (Slow decay) \u03b7i \u2264               \u03b7i\u2212ki \u2264      (1 + 2\u03f5) \u03b7i \u2264        2\u03b7i\nFurther, we define scalar variables -\n                      r := 2 (1 + \u03f5) kn\u03b7n (M + \u03bb1) ,                        \u03b6k,t := 40kt+1 (M + \u03bb1)2\n                 \u03c8k,t := 6M           1 + 3k2                           ,        V\u2032 := 1 + (3 + 4\u03f5) |\u03bb2 (P              ) |V             (11)\n                                                t+1 (M + \u03bb1)2                                     1 \u2212   |\u03bb2 (P   )|\nand recall the definitions of Bt and Bj,i in Eqs 2 and 7, respectively. We are now ready to present the\ntheoretical results needed to prove our main result. For simplicity of notation, we present versions of\n                                             \u03b1\nthe results by using \u03b7i :=           (\u03bb1\u2212\u03bb2)(\u03b2+i) with \u03b1, \u03b2 as defined in Theorem 1. However, these theorems\nare in fact valid under more general step-size schedules. We state and prove the more general versions\nin the Supplement Section S.4.\nTheorem 2. Under Assumptions 1, 2 and 3, for all n > kn, and \u03b7i satisfying C.1 and C.2, we have:\nE   vT1 BnBT    n v1    \u2264   (1 + r)2 exp        n\u2212kn       2\u03b7t\u03bb1 + \u03b72     t   V\u2032 + \u03bb2   1   + \u03b73  t \u03c8k,t       .\n                                                   t=1\n     The three primary differences with the IID case are a) the (1 + r)2 term, which arises since the\nrecursion sketched in Section 4 leaves out the last kn terms which are bounded by (1 + r)2; (b) the\n                          1\nnew factor of        1\u2212|\u03bb2(P )| with V due to the Markovian dependence between terms; and c) the extra\nlower order term \u03b73        t \u03c8k,t arising from the use of Lemmas 2 and 3.\n                                                                      8", "md": "# Math Equations and Text\n\n## Lemma 3\n\nUnder Assumptions 1, 2 and 3, for i &lt; j \u2264 i + ki,\n\n$$\nE[X_iX^T_i | s_{i+ki}, \\ldots, s_n]^2 \\leq |\\lambda_2(P)|^{j-i} V + 8\\eta^2_i M(M + \\lambda_1) \\|\\mathbf{S}\\|^2\n$$\nwhere ki is as defined in Lemma S.12 and S is a constant symmetric positive semi-definite matrix.\n\nLemma 3 bounds the norm of the covariance between matrices $X_iX^T_i$ and $X_jX^T_j$.\n\nIn particular, this implies that the norm of T1,j decays as $|\\lambda_2(P)|^{j-1}$. The proof uses a spectral argument that replaces a coarse approximation by a sum of ki O(1) terms to sum of k exponentially decaying terms, thereby removing the dependence on ki, which can be as large as log(n). The proof is deferred to the Supplement section S.4. The details can be found in Supplement section S.4.\n\nLet {c1, c2, c3, c4} be positive constants for ease of notation. Coming back to Eq 8, we can bound T1 as follows: $T1 \\leq c1|\\lambda_2(P)|^V \\alpha n,k+2 \\eta_1 (1 - |\\lambda_2(P)| + c2\\eta_2 k^2_1$. A similar argument can be applied to bound T2 as: $T2 \\leq \\alpha n,k+2 V + c3\\eta_1 k^2_1$. Putting everything together in 8, we have:\n\n$$\n\\alpha n,1 \\leq (1 + \\eta_1\\lambda_1)^2 + V\\alpha n,2 + c1|\\lambda_2(P)|^V\\eta_2\\alpha n,k+2 + c4\\eta_3 k^2_1\\alpha n,k+2\n$$\nRecursion for IID setting\n\nError due to approximation of matrix product\n\nError due to Markovian dependence\n\nRecursing on this inequality gives us our bound on $E[v^T_n B_n B^T_n v_1]$ (Theorem 2). We are now ready to present all our accompanying theorems.\n\n## Intermediate Theorems for Convergence Analysis\n\nIn this section, we present our accompanying theorems which are used to obtain the main result in Theorem 1. But before doing so, we will need to establish some notation. Let $k_i := \\tau_{\\text{mix}} \\eta_2^i$, and the step-sizes be set as $\\eta_i := \\frac{\\alpha}{(\\lambda_1-\\lambda_2)(\\beta+i)}$ with $\\alpha, \\beta$ as defined in Theorem 1. Let $\\epsilon := 100$. As shown in Lemma S.12 in Supplement Section S.3 our choice of step-sizes satisfy, $\\forall i \\in [n],$\n\n1. $\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$\n2. (Slow decay) $\\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i$\n\nFurther, we define scalar variables -\n\n$$\nr := 2(1 + \\epsilon) kn\\eta_n (M + \\lambda_1), \\quad \\zeta_{k,t} := 40k t+1 (M + \\lambda_1)^2\n$$\n$$\n\\psi_{k,t} := 6M \\frac{1 + 3k^2}{t+1 (M + \\lambda_1)^2}, \\quad V' := 1 + (3 + 4\\epsilon) |\\lambda_2(P)| V (11)\n$$\nand recall the definitions of $B_t$ and $B_{j,i}$ in Eqs 2 and 7, respectively. We are now ready to present the theoretical results needed to prove our main result. For simplicity of notation, we present versions of the results by using $\\eta_i := \\frac{\\alpha}{(\\lambda_1-\\lambda_2)(\\beta+i)}$ with $\\alpha, \\beta$ as defined in Theorem 1. However, these theorems are in fact valid under more general step-size schedules. We state and prove the more general versions in the Supplement Section S.4.\n\n## Theorem 2\n\nUnder Assumptions 1, 2 and 3, for all $n > k_n$, and $\\eta_i$ satisfying C.1 and C.2, we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1] \\leq (1 + r)^2 \\exp\\left(\\sum_{t=1}^{n-k_n} 2\\eta_t\\lambda_1 + \\eta^2_t V' + \\lambda_2 1 + \\eta_3 t \\psi_{k,t}\\right)\n$$\nThe three primary differences with the IID case are a) the $(1 + r)^2$ term, which arises since the recursion sketched in Section 4 leaves out the last $k_n$ terms which are bounded by $(1 + r)^2$; b) the new factor of $1-|\\lambda_2(P)|$ with $V$ due to the Markovian dependence between terms; and c) the extra lower order term $\\eta_3 t \\psi_{k,t}$ arising from the use of Lemmas 2 and 3.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "heading", "lvl": 2, "value": "Lemma 3", "md": "## Lemma 3"}, {"type": "text", "value": "Under Assumptions 1, 2 and 3, for i &lt; j \u2264 i + ki,\n\n$$\nE[X_iX^T_i | s_{i+ki}, \\ldots, s_n]^2 \\leq |\\lambda_2(P)|^{j-i} V + 8\\eta^2_i M(M + \\lambda_1) \\|\\mathbf{S}\\|^2\n$$\nwhere ki is as defined in Lemma S.12 and S is a constant symmetric positive semi-definite matrix.\n\nLemma 3 bounds the norm of the covariance between matrices $X_iX^T_i$ and $X_jX^T_j$.\n\nIn particular, this implies that the norm of T1,j decays as $|\\lambda_2(P)|^{j-1}$. The proof uses a spectral argument that replaces a coarse approximation by a sum of ki O(1) terms to sum of k exponentially decaying terms, thereby removing the dependence on ki, which can be as large as log(n). The proof is deferred to the Supplement section S.4. The details can be found in Supplement section S.4.\n\nLet {c1, c2, c3, c4} be positive constants for ease of notation. Coming back to Eq 8, we can bound T1 as follows: $T1 \\leq c1|\\lambda_2(P)|^V \\alpha n,k+2 \\eta_1 (1 - |\\lambda_2(P)| + c2\\eta_2 k^2_1$. A similar argument can be applied to bound T2 as: $T2 \\leq \\alpha n,k+2 V + c3\\eta_1 k^2_1$. Putting everything together in 8, we have:\n\n$$\n\\alpha n,1 \\leq (1 + \\eta_1\\lambda_1)^2 + V\\alpha n,2 + c1|\\lambda_2(P)|^V\\eta_2\\alpha n,k+2 + c4\\eta_3 k^2_1\\alpha n,k+2\n$$\nRecursion for IID setting\n\nError due to approximation of matrix product\n\nError due to Markovian dependence\n\nRecursing on this inequality gives us our bound on $E[v^T_n B_n B^T_n v_1]$ (Theorem 2). We are now ready to present all our accompanying theorems.", "md": "Under Assumptions 1, 2 and 3, for i &lt; j \u2264 i + ki,\n\n$$\nE[X_iX^T_i | s_{i+ki}, \\ldots, s_n]^2 \\leq |\\lambda_2(P)|^{j-i} V + 8\\eta^2_i M(M + \\lambda_1) \\|\\mathbf{S}\\|^2\n$$\nwhere ki is as defined in Lemma S.12 and S is a constant symmetric positive semi-definite matrix.\n\nLemma 3 bounds the norm of the covariance between matrices $X_iX^T_i$ and $X_jX^T_j$.\n\nIn particular, this implies that the norm of T1,j decays as $|\\lambda_2(P)|^{j-1}$. The proof uses a spectral argument that replaces a coarse approximation by a sum of ki O(1) terms to sum of k exponentially decaying terms, thereby removing the dependence on ki, which can be as large as log(n). The proof is deferred to the Supplement section S.4. The details can be found in Supplement section S.4.\n\nLet {c1, c2, c3, c4} be positive constants for ease of notation. Coming back to Eq 8, we can bound T1 as follows: $T1 \\leq c1|\\lambda_2(P)|^V \\alpha n,k+2 \\eta_1 (1 - |\\lambda_2(P)| + c2\\eta_2 k^2_1$. A similar argument can be applied to bound T2 as: $T2 \\leq \\alpha n,k+2 V + c3\\eta_1 k^2_1$. Putting everything together in 8, we have:\n\n$$\n\\alpha n,1 \\leq (1 + \\eta_1\\lambda_1)^2 + V\\alpha n,2 + c1|\\lambda_2(P)|^V\\eta_2\\alpha n,k+2 + c4\\eta_3 k^2_1\\alpha n,k+2\n$$\nRecursion for IID setting\n\nError due to approximation of matrix product\n\nError due to Markovian dependence\n\nRecursing on this inequality gives us our bound on $E[v^T_n B_n B^T_n v_1]$ (Theorem 2). We are now ready to present all our accompanying theorems."}, {"type": "heading", "lvl": 2, "value": "Intermediate Theorems for Convergence Analysis", "md": "## Intermediate Theorems for Convergence Analysis"}, {"type": "text", "value": "In this section, we present our accompanying theorems which are used to obtain the main result in Theorem 1. But before doing so, we will need to establish some notation. Let $k_i := \\tau_{\\text{mix}} \\eta_2^i$, and the step-sizes be set as $\\eta_i := \\frac{\\alpha}{(\\lambda_1-\\lambda_2)(\\beta+i)}$ with $\\alpha, \\beta$ as defined in Theorem 1. Let $\\epsilon := 100$. As shown in Lemma S.12 in Supplement Section S.3 our choice of step-sizes satisfy, $\\forall i \\in [n],$\n\n1. $\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$\n2. (Slow decay) $\\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i$\n\nFurther, we define scalar variables -\n\n$$\nr := 2(1 + \\epsilon) kn\\eta_n (M + \\lambda_1), \\quad \\zeta_{k,t} := 40k t+1 (M + \\lambda_1)^2\n$$\n$$\n\\psi_{k,t} := 6M \\frac{1 + 3k^2}{t+1 (M + \\lambda_1)^2}, \\quad V' := 1 + (3 + 4\\epsilon) |\\lambda_2(P)| V (11)\n$$\nand recall the definitions of $B_t$ and $B_{j,i}$ in Eqs 2 and 7, respectively. We are now ready to present the theoretical results needed to prove our main result. For simplicity of notation, we present versions of the results by using $\\eta_i := \\frac{\\alpha}{(\\lambda_1-\\lambda_2)(\\beta+i)}$ with $\\alpha, \\beta$ as defined in Theorem 1. However, these theorems are in fact valid under more general step-size schedules. We state and prove the more general versions in the Supplement Section S.4.", "md": "In this section, we present our accompanying theorems which are used to obtain the main result in Theorem 1. But before doing so, we will need to establish some notation. Let $k_i := \\tau_{\\text{mix}} \\eta_2^i$, and the step-sizes be set as $\\eta_i := \\frac{\\alpha}{(\\lambda_1-\\lambda_2)(\\beta+i)}$ with $\\alpha, \\beta$ as defined in Theorem 1. Let $\\epsilon := 100$. As shown in Lemma S.12 in Supplement Section S.3 our choice of step-sizes satisfy, $\\forall i \\in [n],$\n\n1. $\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$\n2. (Slow decay) $\\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i$\n\nFurther, we define scalar variables -\n\n$$\nr := 2(1 + \\epsilon) kn\\eta_n (M + \\lambda_1), \\quad \\zeta_{k,t} := 40k t+1 (M + \\lambda_1)^2\n$$\n$$\n\\psi_{k,t} := 6M \\frac{1 + 3k^2}{t+1 (M + \\lambda_1)^2}, \\quad V' := 1 + (3 + 4\\epsilon) |\\lambda_2(P)| V (11)\n$$\nand recall the definitions of $B_t$ and $B_{j,i}$ in Eqs 2 and 7, respectively. We are now ready to present the theoretical results needed to prove our main result. For simplicity of notation, we present versions of the results by using $\\eta_i := \\frac{\\alpha}{(\\lambda_1-\\lambda_2)(\\beta+i)}$ with $\\alpha, \\beta$ as defined in Theorem 1. However, these theorems are in fact valid under more general step-size schedules. We state and prove the more general versions in the Supplement Section S.4."}, {"type": "heading", "lvl": 2, "value": "Theorem 2", "md": "## Theorem 2"}, {"type": "text", "value": "Under Assumptions 1, 2 and 3, for all $n > k_n$, and $\\eta_i$ satisfying C.1 and C.2, we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1] \\leq (1 + r)^2 \\exp\\left(\\sum_{t=1}^{n-k_n} 2\\eta_t\\lambda_1 + \\eta^2_t V' + \\lambda_2 1 + \\eta_3 t \\psi_{k,t}\\right)\n$$\nThe three primary differences with the IID case are a) the $(1 + r)^2$ term, which arises since the recursion sketched in Section 4 leaves out the last $k_n$ terms which are bounded by $(1 + r)^2$; b) the new factor of $1-|\\lambda_2(P)|$ with $V$ due to the Markovian dependence between terms; and c) the extra lower order term $\\eta_3 t \\psi_{k,t}$ arising from the use of Lemmas 2 and 3.", "md": "Under Assumptions 1, 2 and 3, for all $n > k_n$, and $\\eta_i$ satisfying C.1 and C.2, we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1] \\leq (1 + r)^2 \\exp\\left(\\sum_{t=1}^{n-k_n} 2\\eta_t\\lambda_1 + \\eta^2_t V' + \\lambda_2 1 + \\eta_3 t \\psi_{k,t}\\right)\n$$\nThe three primary differences with the IID case are a) the $(1 + r)^2$ term, which arises since the recursion sketched in Section 4 leaves out the last $k_n$ terms which are bounded by $(1 + r)^2$; b) the new factor of $1-|\\lambda_2(P)|$ with $V$ due to the Markovian dependence between terms; and c) the extra lower order term $\\eta_3 t \\psi_{k,t}$ arising from the use of Lemmas 2 and 3."}]}, {"page": 9, "text": " Theorem 3. Let u := min {t : t \u2208                         [n], t \u2212     kt \u2265     0}. Under Assumptions 1, 2 and 3, for all n > u,\n and \u03b7i satisfying C.1 and C.2,\n E    Tr     V T                       \u2264   (1 + 5\u03f5) exp                 n     2\u03b7t\u03bb2 + \u03b72                V\u2032 + \u03bb2        + \u03b73\n               \u22a5   BnBT    n V\u22a5                                     t=u+1                     t\u2212kt                 1         t\u2212kt\u03c8k,t\n                                               \u00d7      d +        n     (V\u2032 + \u03b7t\u03c8k,t) C\u2032          k,t\u03b72 t\u2212kt exp              t     2\u03b7i (\u03bb1 \u2212       \u03bb2)\n                                                             t=u+1                                                       i=u+1\n where C\u2032     k,t :=       1 +      \u03b4     exp (2\u03bb1        u  i=1 \u03b7j).\n                                  200\n       Here, the difference is mainly in the new variable u, arising since the recursion stops at u, not 1.\n(1 + 5\u03f5) represents the approximation of the first u terms.\n Theorem 4. Under Assumptions 1, 2 and 3, for all n > kn, \u03b7i satisfying C.1 and C.2, and\n s := 2r +           \u03b4                          vT1 BnBT     n v1     \u2265   (1 \u2212     s) exp      n\u2212kn      2\u03b7t\u03bb1 \u2212        n\u2212kn     4\u03b72 t \u03bb21     .\n                  1000, we have: E                                                                t=1                    t=1\n       This differs from its IID counterpart by a multiplicative factor of (1 \u2212                                         s) for the same reason as\n before, which also makes the sums go up to (n \u2212                                kn) instead of n. Note that for sufficiently large n\n                                        log(n)\n (Lemma S.13), r = O                       n        is very small and \u03b4 \u2208              (0, 1). Therefore, (1 \u2212               s) \u2248    1 as large n.\n Theorem 5. Under Assumptions 1, 2 and 3, for all n > kn, and \u03b7i satisfying C.1 and C.2, we have:\n E      vT1 BnBT     n v1   2     \u2264   (1 + r)4 exp          n\u2212kn       4\u03b7t\u03bb1 +        n\u2212kn    \u03b72t \u03b6k,t     .\n                                                                t=1                    t=1\n       The differences are similar to the last theorems involving v1. Surprisingly, for this, the coarse\n approximation suffices, leading to an absence of the V term in the bound. Having established these\n results, the final step is to substitute them into Eq 6 and follow the proof recipe described earlier. This\n requires significant calculations and is deferred to the Supplement Section S.5.\n 6        Experimental Validation\n In this section, we present some simple experiments to validate our theoretical results. For more\n detailed experiments, see the Supplement. We design a Markov chain with |\u2126|= 10 states, where the\n transition matrix entries Pij equal \u03c1/(|\u2126|\u22121) for i \u0338= j and 1 \u2212                                     \u03c1 for i = j. Smaller values of \u03c1 lead\n to larger mixing times. It can be verified that the stationary distribution \u03c0 = U (\u2126) is uniform over\n the state-space and |\u03bb2 (P               ) |\u2248    (1 \u2212    \u03c1). We set \u03c1 = 0.2 for Figures 1 and 2a, and vary it in Figure 2b.\n Each point in the plot is averaged over 20 random runs over different Markov chains, datasets, and\n initialization.\n       Each state s \u2208           \u2126   is associated with D(s) := Bernoulli(ps) distribution. We set d = 1000 and\n select ps \u223c        U (0, 0.05) at the start of each random run. The covariance matrix, \u03a3s, for each state is\n set as \u03a3s (i, j) = exp (\u2212|i \u2212                  j|cs) \u03c3i\u03c3j where cs := 1 + 9                        s\u22121       , \u03c3i := 5i\u2212\u03b2. We start with the\n stationary distribution \u03c0, and for each state si, we draw IID samples Zi \u223c                        |\u2126|\u22121              D (si). We standardize Zi\n such that all components have zero mean and unit variance under the state distribution, D (si). We\n                                                                                         1\n then generate the sample data-point for PCA as Xi = \u03a3                                   2                                                XiXT    i    = \u03a3i\n and E[Xi] = 0d. The step sizes for Oja\u2019s algorithm are set as \u03b7i =                      i Zi. By construction, ED(si)    \u03b1\n 1\u2212|\u03bb2(P5)|. For the downsampled variant, every 10th data-point is considered, and \u03b2 is accordingly              (\u03b2+i)(\u03bb1\u2212\u03bb2) for \u03b1 = 5, \u03b2 =\n divided by 10. For the offl                   ine algorithm, we recompute the leading eigenvector of the sample\n covariance matrix of data-points seen so far.\n       Figure 1 compares the performance of different algorithms for the Bernoulli distribution. Here,\n we are checking if the results obtained in Theorem 1, Proposition 1, and Corollary 1 are reflected in\n the experiments. The experimental results demonstrate that Oja\u2019s algorithm performs significantly\n                                                                               9", "md": "## Theorem 3\n\nLet \\( u := \\min \\{t : t \\in [n], t - kt \\geq 0\\} \\). Under Assumptions 1, 2 and 3, for all \\( n > u \\) and \\( \\eta_i \\) satisfying C.1 and C.2, we have:\n\n$$\n\\begin{align*}\nE\\left[\\text{Tr} V^T_\\perp B_n B^T_n V^\\perp\\right]_{t=u+1} &\\leq (1 + 5\\epsilon) \\exp\\left(\\frac{n}{2}\\eta_t\\lambda_2 + \\eta^2 V' + \\lambda^2 + \\eta^3\\right) \\\\\n&\\times d + n(V' + \\eta_t\\psi_{k,t})C'_{k,t}\\eta^2_{t-kt}\\exp\\left(\\frac{t}{2}\\eta_i(\\lambda_1 - \\lambda_2)\\right)\n\\end{align*}\n$$\nwhere \\( C'_{k,t} := 1 + \\frac{\\delta}{200}\\exp\\left(2\\lambda_1\\sum_{i=1}^{u}\\eta_j\\right) \\).\n\nHere, the difference is mainly in the new variable \\( u \\), arising since the recursion stops at \\( u \\), not 1.\n\n\\((1 + 5\\epsilon)\\) represents the approximation of the first \\( u \\) terms.\n\n## Theorem 4\n\nUnder Assumptions 1, 2 and 3, for all \\( n > kn \\), \\( \\eta_i \\) satisfying C.1 and C.2, and \\( s := 2r + \\frac{\\delta}{1000} \\), we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1]_{t=1} \\geq (1 - s) \\exp\\left(\\frac{n-kn}{2}\\eta_t\\lambda_1 - \\frac{n-kn}{4}\\eta^2_t\\lambda^2_1\\right)\n$$\nThis differs from its IID counterpart by a multiplicative factor of \\( (1 - s) \\) for the same reason as before, which also makes the sums go up to \\( (n - kn) \\) instead of \\( n \\).\n\n## Theorem 5\n\nUnder Assumptions 1, 2 and 3, for all \\( n > kn \\), and \\( \\eta_i \\) satisfying C.1 and C.2, we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1]^2_{t=1} \\leq (1 + r)^4 \\exp\\left(\\frac{n-kn}{4}\\eta_t\\lambda_1 + \\frac{n-kn}{\\eta^2_t}\\zeta_{k,t}\\right)\n$$\nThe differences are similar to the last theorems involving \\( v_1 \\). Surprisingly, for this, the coarse approximation suffices, leading to an absence of the \\( V \\) term in the bound.\n\n## Experimental Validation\n\nIn this section, we present some simple experiments to validate our theoretical results. For more detailed experiments, see the Supplement.\n\nWe design a Markov chain with \\( |\\Omega| = 10 \\) states, where the transition matrix entries \\( P_{ij} \\) equal \\( \\rho/(|\\Omega|-1) \\) for \\( i \\neq j \\) and \\( 1 - \\rho \\) for \\( i = j \\). Smaller values of \\( \\rho \\) lead to larger mixing times.\n\nIt can be verified that the stationary distribution \\( \\pi = U(\\Omega) \\) is uniform over the state-space and \\( |\\lambda_2(P)| \\approx (1 - \\rho) \\). We set \\( \\rho = 0.2 \\) for Figures 1 and 2a, and vary it in Figure 2b.\n\nEach point in the plot is averaged over 20 random runs over different Markov chains, datasets, and initialization.\n\nEach state \\( s \\in \\Omega \\) is associated with \\( D(s) := \\text{Bernoulli}(p_s) \\) distribution. We set \\( d = 1000 \\) and select \\( p_s \\sim U(0, 0.05) \\) at the start of each random run.\n\nThe covariance matrix, \\( \\Sigma_s \\), for each state is set as \\( \\Sigma_s(i, j) = \\exp(-|i - j|c_s)\\sigma_i\\sigma_j \\) where \\( c_s := 1 + 9(s-1) \\), \\( \\sigma_i := 5i^{-\\beta} \\).\n\nWe start with the stationary distribution \\( \\pi \\), and for each state \\( s_i \\), we draw IID samples \\( Z_i \\sim \\frac{1}{|\\Omega|-1} D(s_i) \\). We standardize \\( Z_i \\) such that all components have zero mean and unit variance under the state distribution, \\( D(s_i) \\).\n\nThen we generate the sample data-point for PCA as \\( X_i = \\Sigma^{1/2}_i X^T_i = \\Sigma_i \\) and \\( E[X_i] = 0_d \\). The step sizes for Oja's algorithm are set as \\( \\eta_i = iZ_i \\).\n\nBy construction, \\( E[D(s_i)] \\approx \\frac{1}{1-|\\lambda_2(P^5)|} \\). For the downsampled variant, every 10th data-point is considered, and \\( \\beta \\) is accordingly divided by 10.\n\nFor the offline algorithm, we recompute the leading eigenvector of the sample covariance matrix of data-points seen so far.\n\nFigure 1 compares the performance of different algorithms for the Bernoulli distribution. Here, we are checking if the results obtained in Theorem 1, Proposition 1, and Corollary 1 are reflected in the experiments. The experimental results demonstrate that Oja's algorithm performs significantly.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Theorem 3", "md": "## Theorem 3"}, {"type": "text", "value": "Let \\( u := \\min \\{t : t \\in [n], t - kt \\geq 0\\} \\). Under Assumptions 1, 2 and 3, for all \\( n > u \\) and \\( \\eta_i \\) satisfying C.1 and C.2, we have:\n\n$$\n\\begin{align*}\nE\\left[\\text{Tr} V^T_\\perp B_n B^T_n V^\\perp\\right]_{t=u+1} &\\leq (1 + 5\\epsilon) \\exp\\left(\\frac{n}{2}\\eta_t\\lambda_2 + \\eta^2 V' + \\lambda^2 + \\eta^3\\right) \\\\\n&\\times d + n(V' + \\eta_t\\psi_{k,t})C'_{k,t}\\eta^2_{t-kt}\\exp\\left(\\frac{t}{2}\\eta_i(\\lambda_1 - \\lambda_2)\\right)\n\\end{align*}\n$$\nwhere \\( C'_{k,t} := 1 + \\frac{\\delta}{200}\\exp\\left(2\\lambda_1\\sum_{i=1}^{u}\\eta_j\\right) \\).\n\nHere, the difference is mainly in the new variable \\( u \\), arising since the recursion stops at \\( u \\), not 1.\n\n\\((1 + 5\\epsilon)\\) represents the approximation of the first \\( u \\) terms.", "md": "Let \\( u := \\min \\{t : t \\in [n], t - kt \\geq 0\\} \\). Under Assumptions 1, 2 and 3, for all \\( n > u \\) and \\( \\eta_i \\) satisfying C.1 and C.2, we have:\n\n$$\n\\begin{align*}\nE\\left[\\text{Tr} V^T_\\perp B_n B^T_n V^\\perp\\right]_{t=u+1} &\\leq (1 + 5\\epsilon) \\exp\\left(\\frac{n}{2}\\eta_t\\lambda_2 + \\eta^2 V' + \\lambda^2 + \\eta^3\\right) \\\\\n&\\times d + n(V' + \\eta_t\\psi_{k,t})C'_{k,t}\\eta^2_{t-kt}\\exp\\left(\\frac{t}{2}\\eta_i(\\lambda_1 - \\lambda_2)\\right)\n\\end{align*}\n$$\nwhere \\( C'_{k,t} := 1 + \\frac{\\delta}{200}\\exp\\left(2\\lambda_1\\sum_{i=1}^{u}\\eta_j\\right) \\).\n\nHere, the difference is mainly in the new variable \\( u \\), arising since the recursion stops at \\( u \\), not 1.\n\n\\((1 + 5\\epsilon)\\) represents the approximation of the first \\( u \\) terms."}, {"type": "heading", "lvl": 2, "value": "Theorem 4", "md": "## Theorem 4"}, {"type": "text", "value": "Under Assumptions 1, 2 and 3, for all \\( n > kn \\), \\( \\eta_i \\) satisfying C.1 and C.2, and \\( s := 2r + \\frac{\\delta}{1000} \\), we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1]_{t=1} \\geq (1 - s) \\exp\\left(\\frac{n-kn}{2}\\eta_t\\lambda_1 - \\frac{n-kn}{4}\\eta^2_t\\lambda^2_1\\right)\n$$\nThis differs from its IID counterpart by a multiplicative factor of \\( (1 - s) \\) for the same reason as before, which also makes the sums go up to \\( (n - kn) \\) instead of \\( n \\).", "md": "Under Assumptions 1, 2 and 3, for all \\( n > kn \\), \\( \\eta_i \\) satisfying C.1 and C.2, and \\( s := 2r + \\frac{\\delta}{1000} \\), we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1]_{t=1} \\geq (1 - s) \\exp\\left(\\frac{n-kn}{2}\\eta_t\\lambda_1 - \\frac{n-kn}{4}\\eta^2_t\\lambda^2_1\\right)\n$$\nThis differs from its IID counterpart by a multiplicative factor of \\( (1 - s) \\) for the same reason as before, which also makes the sums go up to \\( (n - kn) \\) instead of \\( n \\)."}, {"type": "heading", "lvl": 2, "value": "Theorem 5", "md": "## Theorem 5"}, {"type": "text", "value": "Under Assumptions 1, 2 and 3, for all \\( n > kn \\), and \\( \\eta_i \\) satisfying C.1 and C.2, we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1]^2_{t=1} \\leq (1 + r)^4 \\exp\\left(\\frac{n-kn}{4}\\eta_t\\lambda_1 + \\frac{n-kn}{\\eta^2_t}\\zeta_{k,t}\\right)\n$$\nThe differences are similar to the last theorems involving \\( v_1 \\). Surprisingly, for this, the coarse approximation suffices, leading to an absence of the \\( V \\) term in the bound.", "md": "Under Assumptions 1, 2 and 3, for all \\( n > kn \\), and \\( \\eta_i \\) satisfying C.1 and C.2, we have:\n\n$$\nE[v^T_1 B_n B^T_n v_1]^2_{t=1} \\leq (1 + r)^4 \\exp\\left(\\frac{n-kn}{4}\\eta_t\\lambda_1 + \\frac{n-kn}{\\eta^2_t}\\zeta_{k,t}\\right)\n$$\nThe differences are similar to the last theorems involving \\( v_1 \\). Surprisingly, for this, the coarse approximation suffices, leading to an absence of the \\( V \\) term in the bound."}, {"type": "heading", "lvl": 2, "value": "Experimental Validation", "md": "## Experimental Validation"}, {"type": "text", "value": "In this section, we present some simple experiments to validate our theoretical results. For more detailed experiments, see the Supplement.\n\nWe design a Markov chain with \\( |\\Omega| = 10 \\) states, where the transition matrix entries \\( P_{ij} \\) equal \\( \\rho/(|\\Omega|-1) \\) for \\( i \\neq j \\) and \\( 1 - \\rho \\) for \\( i = j \\). Smaller values of \\( \\rho \\) lead to larger mixing times.\n\nIt can be verified that the stationary distribution \\( \\pi = U(\\Omega) \\) is uniform over the state-space and \\( |\\lambda_2(P)| \\approx (1 - \\rho) \\). We set \\( \\rho = 0.2 \\) for Figures 1 and 2a, and vary it in Figure 2b.\n\nEach point in the plot is averaged over 20 random runs over different Markov chains, datasets, and initialization.\n\nEach state \\( s \\in \\Omega \\) is associated with \\( D(s) := \\text{Bernoulli}(p_s) \\) distribution. We set \\( d = 1000 \\) and select \\( p_s \\sim U(0, 0.05) \\) at the start of each random run.\n\nThe covariance matrix, \\( \\Sigma_s \\), for each state is set as \\( \\Sigma_s(i, j) = \\exp(-|i - j|c_s)\\sigma_i\\sigma_j \\) where \\( c_s := 1 + 9(s-1) \\), \\( \\sigma_i := 5i^{-\\beta} \\).\n\nWe start with the stationary distribution \\( \\pi \\), and for each state \\( s_i \\), we draw IID samples \\( Z_i \\sim \\frac{1}{|\\Omega|-1} D(s_i) \\). We standardize \\( Z_i \\) such that all components have zero mean and unit variance under the state distribution, \\( D(s_i) \\).\n\nThen we generate the sample data-point for PCA as \\( X_i = \\Sigma^{1/2}_i X^T_i = \\Sigma_i \\) and \\( E[X_i] = 0_d \\). The step sizes for Oja's algorithm are set as \\( \\eta_i = iZ_i \\).\n\nBy construction, \\( E[D(s_i)] \\approx \\frac{1}{1-|\\lambda_2(P^5)|} \\). For the downsampled variant, every 10th data-point is considered, and \\( \\beta \\) is accordingly divided by 10.\n\nFor the offline algorithm, we recompute the leading eigenvector of the sample covariance matrix of data-points seen so far.\n\nFigure 1 compares the performance of different algorithms for the Bernoulli distribution. Here, we are checking if the results obtained in Theorem 1, Proposition 1, and Corollary 1 are reflected in the experiments. The experimental results demonstrate that Oja's algorithm performs significantly.", "md": "In this section, we present some simple experiments to validate our theoretical results. For more detailed experiments, see the Supplement.\n\nWe design a Markov chain with \\( |\\Omega| = 10 \\) states, where the transition matrix entries \\( P_{ij} \\) equal \\( \\rho/(|\\Omega|-1) \\) for \\( i \\neq j \\) and \\( 1 - \\rho \\) for \\( i = j \\). Smaller values of \\( \\rho \\) lead to larger mixing times.\n\nIt can be verified that the stationary distribution \\( \\pi = U(\\Omega) \\) is uniform over the state-space and \\( |\\lambda_2(P)| \\approx (1 - \\rho) \\). We set \\( \\rho = 0.2 \\) for Figures 1 and 2a, and vary it in Figure 2b.\n\nEach point in the plot is averaged over 20 random runs over different Markov chains, datasets, and initialization.\n\nEach state \\( s \\in \\Omega \\) is associated with \\( D(s) := \\text{Bernoulli}(p_s) \\) distribution. We set \\( d = 1000 \\) and select \\( p_s \\sim U(0, 0.05) \\) at the start of each random run.\n\nThe covariance matrix, \\( \\Sigma_s \\), for each state is set as \\( \\Sigma_s(i, j) = \\exp(-|i - j|c_s)\\sigma_i\\sigma_j \\) where \\( c_s := 1 + 9(s-1) \\), \\( \\sigma_i := 5i^{-\\beta} \\).\n\nWe start with the stationary distribution \\( \\pi \\), and for each state \\( s_i \\), we draw IID samples \\( Z_i \\sim \\frac{1}{|\\Omega|-1} D(s_i) \\). We standardize \\( Z_i \\) such that all components have zero mean and unit variance under the state distribution, \\( D(s_i) \\).\n\nThen we generate the sample data-point for PCA as \\( X_i = \\Sigma^{1/2}_i X^T_i = \\Sigma_i \\) and \\( E[X_i] = 0_d \\). The step sizes for Oja's algorithm are set as \\( \\eta_i = iZ_i \\).\n\nBy construction, \\( E[D(s_i)] \\approx \\frac{1}{1-|\\lambda_2(P^5)|} \\). For the downsampled variant, every 10th data-point is considered, and \\( \\beta \\) is accordingly divided by 10.\n\nFor the offline algorithm, we recompute the leading eigenvector of the sample covariance matrix of data-points seen so far.\n\nFigure 1 compares the performance of different algorithms for the Bernoulli distribution. Here, we are checking if the results obtained in Theorem 1, Proposition 1, and Corollary 1 are reflected in the experiments. The experimental results demonstrate that Oja's algorithm performs significantly."}]}, {"page": 10, "text": "                 (a) Variation of sin2 error with \u03b2                     (b) Variation of sin2 error with |\u03bb2 (P) |\n                  Figure 2: X axis represents the sample size, and Y axis represents the sin2 error.\nbetter than the downsampled version, consistent with the theoretical results. It also shows that Oja\u2019s\nalgorithm performs similarly to the offline algorithm, which is also confirmed by our theoretical results\nand that of [27]. Figure 2a compares the performance of Oja\u2019s algorithm for different covariance\nmatrices. Smaller values of \u03b2 decrease the eigengap \u03bb1 \u2212                 \u03bb2, and hence lead to a slower convergence.\nFigure 2b confirms that smaller values of \u03c1 (larger values of |\u03bb2 (P) |) also worsen the rate, which\nmatches with our theoretical results.\n7      Conclusion\nWe have considered the problem of streaming PCA for Markovian data, which has implications\nin various settings like decentralized optimization, reinforcement learning, etc. The analysis of\nstreaming algorithms in such settings has seen a renewed surge of interest in recent years. However,\nthe dependence between data-points makes it difficult to obtain sharp bounds. We provide, to our\nknowledge, the first sharp bound for obtaining the first principal component from a Markovian data\nstream that breaks the logarithmic barrier present in the analysis done for downsampled data. We\nbelieve that the theoretical tools that we have developed in this paper would enable one to obtain\nsharp bounds for other dependent data settings, learning top k principal components, and online\ninference algorithms with updates involving products of matrices.\n8      Acknowledgements\nWe gratefully acknowledge NSF grants 2217069 and DMS 2109155. We are also grateful to Rachel\nWard and Bobby Shi for valuable discussions.\nReferences\n  [1] Zeyuan Allen-Zhu and Yuanzhi Li. First effi                cient convergence for streaming k-pca: a global,\n       gap-free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of\n       Computer Science (FOCS), pages 487\u2013492. IEEE, 2017.\n  [2] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference\n       learning with linear function approximation. CoRR, abs/1806.02450, 2018.\n  [3] Minshuo Chen, Lin Yang, Mengdi Wang, and Tuo Zhao. Dimensionality reduction for stationary\n       time series via stochastic nonconvex optimization. Advances in Neural Information Processing\n       Systems, 31, 2018.\n                                                              10", "md": "# Document\n\n## (a) Variation of $$\\sin^2$$ error with $$\\beta$$\n\n## (b) Variation of $$\\sin^2$$ error with $$|\\lambda_2(P)|$$\n\nFigure 2: X axis represents the sample size, and Y axis represents the $$\\sin^2$$ error.\n\nBetter than the downsampled version, consistent with the theoretical results. It also shows that Oja\u2019s\nalgorithm performs similarly to the offline algorithm, which is also confirmed by our theoretical results\nand that of [27]. Figure 2a compares the performance of Oja\u2019s algorithm for different covariance\nmatrices. Smaller values of $$\\beta$$ decrease the eigengap $$\\lambda_1 - \\lambda_2$$, and hence lead to a slower convergence.\nFigure 2b confirms that smaller values of $$\\rho$$ (larger values of $$|\\lambda_2(P)|$$) also worsen the rate, which\nmatches with our theoretical results.\n\n### Conclusion\n\nWe have considered the problem of streaming PCA for Markovian data, which has implications\nin various settings like decentralized optimization, reinforcement learning, etc. The analysis of\nstreaming algorithms in such settings has seen a renewed surge of interest in recent years. However,\nthe dependence between data-points makes it difficult to obtain sharp bounds. We provide, to our\nknowledge, the first sharp bound for obtaining the first principal component from a Markovian data\nstream that breaks the logarithmic barrier present in the analysis done for downsampled data. We\nbelieve that the theoretical tools that we have developed in this paper would enable one to obtain\nsharp bounds for other dependent data settings, learning top $$k$$ principal components, and online\ninference algorithms with updates involving products of matrices.\n\n### Acknowledgements\n\nWe gratefully acknowledge NSF grants 2217069 and DMS 2109155. We are also grateful to Rachel\nWard and Bobby Shi for valuable discussions.\n\n### References\n\n1. Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global,\ngap-free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of\nComputer Science (FOCS), pages 487\u2013492. IEEE, 2017.\n2. Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference\nlearning with linear function approximation. CoRR, abs/1806.02450, 2018.\n3. Minshuo Chen, Lin Yang, Mengdi Wang, and Tuo Zhao. Dimensionality reduction for stationary\ntime series via stochastic nonconvex optimization. Advances in Neural Information Processing\nSystems, 31, 2018.", "images": [{"name": "page-10-0.jpg", "height": 143, "width": 179, "x": 125, "y": 71}, {"name": "page-10-1.jpg", "height": 143, "width": 179, "x": 306, "y": 71}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "(a) Variation of $$\\sin^2$$ error with $$\\beta$$", "md": "## (a) Variation of $$\\sin^2$$ error with $$\\beta$$"}, {"type": "heading", "lvl": 2, "value": "(b) Variation of $$\\sin^2$$ error with $$|\\lambda_2(P)|$$", "md": "## (b) Variation of $$\\sin^2$$ error with $$|\\lambda_2(P)|$$"}, {"type": "text", "value": "Figure 2: X axis represents the sample size, and Y axis represents the $$\\sin^2$$ error.\n\nBetter than the downsampled version, consistent with the theoretical results. It also shows that Oja\u2019s\nalgorithm performs similarly to the offline algorithm, which is also confirmed by our theoretical results\nand that of [27]. Figure 2a compares the performance of Oja\u2019s algorithm for different covariance\nmatrices. Smaller values of $$\\beta$$ decrease the eigengap $$\\lambda_1 - \\lambda_2$$, and hence lead to a slower convergence.\nFigure 2b confirms that smaller values of $$\\rho$$ (larger values of $$|\\lambda_2(P)|$$) also worsen the rate, which\nmatches with our theoretical results.", "md": "Figure 2: X axis represents the sample size, and Y axis represents the $$\\sin^2$$ error.\n\nBetter than the downsampled version, consistent with the theoretical results. It also shows that Oja\u2019s\nalgorithm performs similarly to the offline algorithm, which is also confirmed by our theoretical results\nand that of [27]. Figure 2a compares the performance of Oja\u2019s algorithm for different covariance\nmatrices. Smaller values of $$\\beta$$ decrease the eigengap $$\\lambda_1 - \\lambda_2$$, and hence lead to a slower convergence.\nFigure 2b confirms that smaller values of $$\\rho$$ (larger values of $$|\\lambda_2(P)|$$) also worsen the rate, which\nmatches with our theoretical results."}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "We have considered the problem of streaming PCA for Markovian data, which has implications\nin various settings like decentralized optimization, reinforcement learning, etc. The analysis of\nstreaming algorithms in such settings has seen a renewed surge of interest in recent years. However,\nthe dependence between data-points makes it difficult to obtain sharp bounds. We provide, to our\nknowledge, the first sharp bound for obtaining the first principal component from a Markovian data\nstream that breaks the logarithmic barrier present in the analysis done for downsampled data. We\nbelieve that the theoretical tools that we have developed in this paper would enable one to obtain\nsharp bounds for other dependent data settings, learning top $$k$$ principal components, and online\ninference algorithms with updates involving products of matrices.", "md": "We have considered the problem of streaming PCA for Markovian data, which has implications\nin various settings like decentralized optimization, reinforcement learning, etc. The analysis of\nstreaming algorithms in such settings has seen a renewed surge of interest in recent years. However,\nthe dependence between data-points makes it difficult to obtain sharp bounds. We provide, to our\nknowledge, the first sharp bound for obtaining the first principal component from a Markovian data\nstream that breaks the logarithmic barrier present in the analysis done for downsampled data. We\nbelieve that the theoretical tools that we have developed in this paper would enable one to obtain\nsharp bounds for other dependent data settings, learning top $$k$$ principal components, and online\ninference algorithms with updates involving products of matrices."}, {"type": "heading", "lvl": 3, "value": "Acknowledgements", "md": "### Acknowledgements"}, {"type": "text", "value": "We gratefully acknowledge NSF grants 2217069 and DMS 2109155. We are also grateful to Rachel\nWard and Bobby Shi for valuable discussions.", "md": "We gratefully acknowledge NSF grants 2217069 and DMS 2109155. We are also grateful to Rachel\nWard and Bobby Shi for valuable discussions."}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "text", "value": "1. Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global,\ngap-free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of\nComputer Science (FOCS), pages 487\u2013492. IEEE, 2017.\n2. Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference\nlearning with linear function approximation. CoRR, abs/1806.02450, 2018.\n3. Minshuo Chen, Lin Yang, Mengdi Wang, and Tuo Zhao. Dimensionality reduction for stationary\ntime series via stochastic nonconvex optimization. Advances in Neural Information Processing\nSystems, 31, 2018.", "md": "1. Zeyuan Allen-Zhu and Yuanzhi Li. First efficient convergence for streaming k-pca: a global,\ngap-free, and near-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of\nComputer Science (FOCS), pages 487\u2013492. IEEE, 2017.\n2. Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference\nlearning with linear function approximation. CoRR, abs/1806.02450, 2018.\n3. Minshuo Chen, Lin Yang, Mengdi Wang, and Tuo Zhao. Dimensionality reduction for stationary\ntime series via stochastic nonconvex optimization. Advances in Neural Information Processing\nSystems, 31, 2018."}]}, {"page": 11, "text": " [4] Shuhang Chen, Adithya Devraj, Ana Busic, and Sean Meyn. Explicit mean-square error bounds\n     for monte-carlo and linear stochastic approximation. In International Conference on Artificial\n     Intelligence and Statistics, pages 4173\u20134183. PMLR, 2020.\n [5] Thinh T. Doan, Lam M. Nguyen, Nhan H. Pham, and Justin Romberg. Convergence rates of\n     accelerated markov gradient descent with applications in reinforcement learning, 2020.\n [6] Thinh T. Doan, Lam M. Nguyen, Nhan H. Pham, and Justin Romberg. Finite-time analysis of\n     stochastic gradient descent under markov randomness, 2020.\n [7] Ron Dorfman and Kfir Yehuda Levy. Adapting to mixing time in stochastic optimization with\n     markovian data. In International Conference on Machine Learning, pages 5429\u20135446. PMLR,\n     2022.\n [8] John C. Duchi, Alekh Agarwal, Mikael Johansson, and Michael I. Jordan. Ergodic mirror\n     descent. SIAM J. Optim., 22(4):1549\u20131578, 2012.\n [9] Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Hoi-To Wai. On the\n     stability of random matrix product with markovian noise: Application to linear stochastic\n     approximation and td learning. In Conference on Learning Theory, pages 1711\u20131752. PMLR,\n     2021.\n[10] Mathieu Even. Stochastic gradient descent under markovian sampling schemes, 2023.\n[11] Andreas Grammenos, Rodrigo Mendoza-Smith, Cecilia Mascolo, and Jon Crowcroft. Federated\n     PCA with adaptive rank estimation. CoRR, abs/1907.08059, 2019.\n[12] Anne Hartebrodt, Reza Nasirigerdeh, David B. Blumenthal, and Richard R\u00f6ttger. Federated\n     principal component analysis for genome-wide association studies. In 2021 IEEE International\n     Conference on Data Mining (ICDM), pages 1090\u20131095, 2021.\n[13] Amelia Henriksen and Rachel Ward. AdaOja: Adaptive Learning Rates for Streaming PCA.\n     arXiv e-prints, page arXiv:1905.12115, May 2019.\n[14] De Huang, Jonathan Niles-Weed, and Rachel Ward. Streaming k-pca: Efficient guarantees for\n     oja\u2019s algorithm, beyond rank-one updates. CoRR, abs/2102.03646, 2021.\n[15] Prateek Jain, Chi Jin, Sham Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming pca:\n     Matching matrix bernstein and near-optimal finite sample guarantees for oja\u2019s algorithm. In\n     Proceedings of The 29th Conference on Learning Theory (COLT), June 2016.\n[16] Chi Jin, Sham M Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Robust\n     shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector\n     computation. arXiv preprint arXiv:1510.08896, 2015.\n[17] L\u00e1szl\u00f3 Kozma. Inequalities cheat sheet, 2018. PDF file.\n[18] Harold J Kushner and G George Yin. Applications in signal processing, communications, and\n     adaptive control. Stochastic Approximation and Recursive Algorithms and Applications, pages\n     63\u201393, 2003.\n[19] David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American\n     Mathematical Soc., 2017.\n[20] Robert Lunde, Purnamrita Sarkar, and Rachel Ward. Bootstrapping the error of oja\u2019s algorithm.\n     Advances in Neural Information Processing Systems, 34:6240\u20136252, 2021.\n[21] Hanbaek Lyu, Deanna Needell, and Laura Balzano. Online matrix factorization for Markovian\n     data and applications to Network Dictionary Learning. arXiv e-prints, page arXiv:1911.01931,\n     November 2019.\n                                                11", "md": "# References\n\n# List of References\n\n1. Shuhang Chen, Adithya Devraj, Ana Busic, and Sean Meyn. Explicit mean-square error bounds\nfor monte-carlo and linear stochastic approximation. In International Conference on Artificial\nIntelligence and Statistics, pages 4173\u20134183. PMLR, 2020.\n2. Thinh T. Doan, Lam M. Nguyen, Nhan H. Pham, and Justin Romberg. Convergence rates of\naccelerated markov gradient descent with applications in reinforcement learning, 2020.\n3. Thinh T. Doan, Lam M. Nguyen, Nhan H. Pham, and Justin Romberg. Finite-time analysis of\nstochastic gradient descent under markov randomness, 2020.\n4. Ron Dorfman and Kfir Yehuda Levy. Adapting to mixing time in stochastic optimization with\nmarkovian data. In International Conference on Machine Learning, pages 5429\u20135446. PMLR, 2022.\n5. John C. Duchi, Alekh Agarwal, Mikael Johansson, and Michael I. Jordan. Ergodic mirror\ndescent. SIAM J. Optim., 22(4):1549\u20131578, 2012.\n6. Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Hoi-To Wai. On the\nstability of random matrix product with markovian noise: Application to linear stochastic\napproximation and td learning. In Conference on Learning Theory, pages 1711\u20131752. PMLR, 2021.\n7. Mathieu Even. Stochastic gradient descent under markovian sampling schemes, 2023.\n8. Andreas Grammenos, Rodrigo Mendoza-Smith, Cecilia Mascolo, and Jon Crowcroft. Federated\nPCA with adaptive rank estimation. CoRR, abs/1907.08059, 2019.\n9. Anne Hartebrodt, Reza Nasirigerdeh, David B. Blumenthal, and Richard R\u00f6ttger. Federated\nprincipal component analysis for genome-wide association studies. In 2021 IEEE International\nConference on Data Mining (ICDM), pages 1090\u20131095, 2021.\n10. Amelia Henriksen and Rachel Ward. AdaOja: Adaptive Learning Rates for Streaming PCA.\narXiv e-prints, page arXiv:1905.12115, May 2019.\n11. De Huang, Jonathan Niles-Weed, and Rachel Ward. Streaming k-pca: Efficient guarantees for\noja\u2019s algorithm, beyond rank-one updates. CoRR, abs/2102.03646, 2021.\n12. Prateek Jain, Chi Jin, Sham Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming pca:\nMatching matrix bernstein and near-optimal finite sample guarantees for oja\u2019s algorithm. In\nProceedings of The 29th Conference on Learning Theory (COLT), June 2016.\n13. Chi Jin, Sham M Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Robust\nshift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector\ncomputation. arXiv preprint arXiv:1510.08896, 2015.\n14. L\u00e1szl\u00f3 Kozma. Inequalities cheat sheet, 2018. PDF file.\n15. Harold J Kushner and G George Yin. Applications in signal processing, communications, and\nadaptive control. Stochastic Approximation and Recursive Algorithms and Applications, pages\n63\u201393, 2003.\n16. David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American\nMathematical Soc., 2017.\n17. Robert Lunde, Purnamrita Sarkar, and Rachel Ward. Bootstrapping the error of oja\u2019s algorithm.\nAdvances in Neural Information Processing Systems, 34:6240\u20136252, 2021.\n18. Hanbaek Lyu, Deanna Needell, and Laura Balzano. Online matrix factorization for Markovian\ndata and applications to Network Dictionary Learning. arXiv e-prints, page arXiv:1911.01931,\nNovember 2019.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "1. Shuhang Chen, Adithya Devraj, Ana Busic, and Sean Meyn. Explicit mean-square error bounds\nfor monte-carlo and linear stochastic approximation. In International Conference on Artificial\nIntelligence and Statistics, pages 4173\u20134183. PMLR, 2020.\n2. Thinh T. Doan, Lam M. Nguyen, Nhan H. Pham, and Justin Romberg. Convergence rates of\naccelerated markov gradient descent with applications in reinforcement learning, 2020.\n3. Thinh T. Doan, Lam M. Nguyen, Nhan H. Pham, and Justin Romberg. Finite-time analysis of\nstochastic gradient descent under markov randomness, 2020.\n4. Ron Dorfman and Kfir Yehuda Levy. Adapting to mixing time in stochastic optimization with\nmarkovian data. In International Conference on Machine Learning, pages 5429\u20135446. PMLR, 2022.\n5. John C. Duchi, Alekh Agarwal, Mikael Johansson, and Michael I. Jordan. Ergodic mirror\ndescent. SIAM J. Optim., 22(4):1549\u20131578, 2012.\n6. Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Hoi-To Wai. On the\nstability of random matrix product with markovian noise: Application to linear stochastic\napproximation and td learning. In Conference on Learning Theory, pages 1711\u20131752. PMLR, 2021.\n7. Mathieu Even. Stochastic gradient descent under markovian sampling schemes, 2023.\n8. Andreas Grammenos, Rodrigo Mendoza-Smith, Cecilia Mascolo, and Jon Crowcroft. Federated\nPCA with adaptive rank estimation. CoRR, abs/1907.08059, 2019.\n9. Anne Hartebrodt, Reza Nasirigerdeh, David B. Blumenthal, and Richard R\u00f6ttger. Federated\nprincipal component analysis for genome-wide association studies. In 2021 IEEE International\nConference on Data Mining (ICDM), pages 1090\u20131095, 2021.\n10. Amelia Henriksen and Rachel Ward. AdaOja: Adaptive Learning Rates for Streaming PCA.\narXiv e-prints, page arXiv:1905.12115, May 2019.\n11. De Huang, Jonathan Niles-Weed, and Rachel Ward. Streaming k-pca: Efficient guarantees for\noja\u2019s algorithm, beyond rank-one updates. CoRR, abs/2102.03646, 2021.\n12. Prateek Jain, Chi Jin, Sham Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming pca:\nMatching matrix bernstein and near-optimal finite sample guarantees for oja\u2019s algorithm. In\nProceedings of The 29th Conference on Learning Theory (COLT), June 2016.\n13. Chi Jin, Sham M Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Robust\nshift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector\ncomputation. arXiv preprint arXiv:1510.08896, 2015.\n14. L\u00e1szl\u00f3 Kozma. Inequalities cheat sheet, 2018. PDF file.\n15. Harold J Kushner and G George Yin. Applications in signal processing, communications, and\nadaptive control. Stochastic Approximation and Recursive Algorithms and Applications, pages\n63\u201393, 2003.\n16. David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American\nMathematical Soc., 2017.\n17. Robert Lunde, Purnamrita Sarkar, and Rachel Ward. Bootstrapping the error of oja\u2019s algorithm.\nAdvances in Neural Information Processing Systems, 34:6240\u20136252, 2021.\n18. Hanbaek Lyu, Deanna Needell, and Laura Balzano. Online matrix factorization for Markovian\ndata and applications to Network Dictionary Learning. arXiv e-prints, page arXiv:1911.01931,\nNovember 2019.", "md": "1. Shuhang Chen, Adithya Devraj, Ana Busic, and Sean Meyn. Explicit mean-square error bounds\nfor monte-carlo and linear stochastic approximation. In International Conference on Artificial\nIntelligence and Statistics, pages 4173\u20134183. PMLR, 2020.\n2. Thinh T. Doan, Lam M. Nguyen, Nhan H. Pham, and Justin Romberg. Convergence rates of\naccelerated markov gradient descent with applications in reinforcement learning, 2020.\n3. Thinh T. Doan, Lam M. Nguyen, Nhan H. Pham, and Justin Romberg. Finite-time analysis of\nstochastic gradient descent under markov randomness, 2020.\n4. Ron Dorfman and Kfir Yehuda Levy. Adapting to mixing time in stochastic optimization with\nmarkovian data. In International Conference on Machine Learning, pages 5429\u20135446. PMLR, 2022.\n5. John C. Duchi, Alekh Agarwal, Mikael Johansson, and Michael I. Jordan. Ergodic mirror\ndescent. SIAM J. Optim., 22(4):1549\u20131578, 2012.\n6. Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, and Hoi-To Wai. On the\nstability of random matrix product with markovian noise: Application to linear stochastic\napproximation and td learning. In Conference on Learning Theory, pages 1711\u20131752. PMLR, 2021.\n7. Mathieu Even. Stochastic gradient descent under markovian sampling schemes, 2023.\n8. Andreas Grammenos, Rodrigo Mendoza-Smith, Cecilia Mascolo, and Jon Crowcroft. Federated\nPCA with adaptive rank estimation. CoRR, abs/1907.08059, 2019.\n9. Anne Hartebrodt, Reza Nasirigerdeh, David B. Blumenthal, and Richard R\u00f6ttger. Federated\nprincipal component analysis for genome-wide association studies. In 2021 IEEE International\nConference on Data Mining (ICDM), pages 1090\u20131095, 2021.\n10. Amelia Henriksen and Rachel Ward. AdaOja: Adaptive Learning Rates for Streaming PCA.\narXiv e-prints, page arXiv:1905.12115, May 2019.\n11. De Huang, Jonathan Niles-Weed, and Rachel Ward. Streaming k-pca: Efficient guarantees for\noja\u2019s algorithm, beyond rank-one updates. CoRR, abs/2102.03646, 2021.\n12. Prateek Jain, Chi Jin, Sham Kakade, Praneeth Netrapalli, and Aaron Sidford. Streaming pca:\nMatching matrix bernstein and near-optimal finite sample guarantees for oja\u2019s algorithm. In\nProceedings of The 29th Conference on Learning Theory (COLT), June 2016.\n13. Chi Jin, Sham M Kakade, Cameron Musco, Praneeth Netrapalli, and Aaron Sidford. Robust\nshift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector\ncomputation. arXiv preprint arXiv:1510.08896, 2015.\n14. L\u00e1szl\u00f3 Kozma. Inequalities cheat sheet, 2018. PDF file.\n15. Harold J Kushner and G George Yin. Applications in signal processing, communications, and\nadaptive control. Stochastic Approximation and Recursive Algorithms and Applications, pages\n63\u201393, 2003.\n16. David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American\nMathematical Soc., 2017.\n17. Robert Lunde, Purnamrita Sarkar, and Rachel Ward. Bootstrapping the error of oja\u2019s algorithm.\nAdvances in Neural Information Processing Systems, 34:6240\u20136252, 2021.\n18. Hanbaek Lyu, Deanna Needell, and Laura Balzano. Online matrix factorization for Markovian\ndata and applications to Network Dictionary Learning. arXiv e-prints, page arXiv:1911.01931,\nNovember 2019."}]}, {"page": 12, "text": "[22] Shaocong Ma, Ziyi Chen, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Data sampling affects the\n     complexity of online sgd over dependent data. In Uncertainty in Artificial Intelligence, pages\n     1296\u20131305. PMLR, 2022.\n[23] Abdelkader Mokkadem. Mixing properties of arma processes. Stochastic Processes and their\n     Applications, 29(2):309\u2013315, 1988.\n[24] Jean-Marie Monnez. Stochastic approximation of eigenvectors and eigenvalues of the q-\n     symmetric expectation of a random matrix. Communications in Statistics-Theory and Methods,\n     pages 1\u201315, 2022.\n[25] Nikos Mouzakis and Eric Price. Spectral guarantees for adversarial streaming pca, 2022.\n[26] Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares\n     regression with markovian data: Fundamental limits and algorithms. Advances in neural\n     information processing systems, 33:16666\u201316676, 2020.\n[27] Joe Neeman, Bobby Shi, and Rachel Ward. Concentration inequalities for sums of markov\n     dependent random matrices, 2023.\n[28] Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical\n     Biology, 15(3):267\u2013273, November 1982.\n[29] Erkki Oja and Juha Karhunen. On stochastic approximation of the eigenvectors and eigenvalues\n     of the expectation of a random matrix. Journal of mathematical analysis and applications,\n     106(1):69\u201384, 1985.\n[30] Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation\n     andtd learning. In Conference on Learning Theory, pages 2803\u20132830. PMLR, 2019.\n[31] Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. Advances in neural\n     information processing systems, 31, 2018.\n[32] Joel A Tropp.    User-friendly tail bounds for sums of random matrices.       Foundations of\n     computational mathematics, 12:389\u2013434, 2012.\n[33] Lan V Truong. Generalization error bounds on deep learning with markov datasets. Advances\n     in Neural Information Processing Systems, 35:23452\u201323462, 2022.\n[34] John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function\n     approximation. In M.C. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural\n     Information Processing Systems, volume 9. MIT Press, 1996.\n[35] Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv\n     preprint arXiv:1011.3027, 2010.\n[36] Per-\u00c5ke Wedin. Perturbation bounds in connection with singular value decomposition. BIT\n     Numerical Mathematics, 12:99\u2013111, 1972.\n[37] Puyudi Yang, Cho-Jui Hsieh, and Jane-Ling Wang. History pca: A new algorithm for streaming\n     pca. arXiv preprint arXiv:1802.05447, 2018.\n[38] Ingvar Ziemann and Stephen Tu. Learning with little mixing. In S. Koyejo, S. Mohamed,\n     A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information\n     Processing Systems, volume 35, pages 4626\u20134637. Curran Associates, Inc., 2022.\n                                                12", "md": "# References\n\n## List of References\n\n|[22]|Shaocong Ma, Ziyi Chen, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Data sampling affects the complexity of online sgd over dependent data. In Uncertainty in Artificial Intelligence, pages 1296\u20131305. PMLR, 2022.|\n|---|---|\n|[23]|Abdelkader Mokkadem. Mixing properties of arma processes. Stochastic Processes and their Applications, 29(2):309\u2013315, 1988.|\n|[24]|Jean-Marie Monnez. Stochastic approximation of eigenvectors and eigenvalues of the q-symmetric expectation of a random matrix. Communications in Statistics-Theory and Methods, pages 1\u201315, 2022.|\n|[25]|Nikos Mouzakis and Eric Price. Spectral guarantees for adversarial streaming pca, 2022.|\n|[26]|Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression with markovian data: Fundamental limits and algorithms. Advances in neural information processing systems, 33:16666\u201316676, 2020.|\n|[27]|Joe Neeman, Bobby Shi, and Rachel Ward. Concentration inequalities for sums of markov dependent random matrices, 2023.|\n|[28]|Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical Biology, 15(3):267\u2013273, November 1982.|\n|[29]|Erkki Oja and Juha Karhunen. On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix. Journal of mathematical analysis and applications, 106(1):69\u201384, 1985.|\n|[30]|Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning. In Conference on Learning Theory, pages 2803\u20132830. PMLR, 2019.|\n|[31]|Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. Advances in neural information processing systems, 31, 2018.|\n|[32]|Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12:389\u2013434, 2012.|\n|[33]|Lan V Truong. Generalization error bounds on deep learning with markov datasets. Advances in Neural Information Processing Systems, 35:23452\u201323462, 2022.|\n|[34]|John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation. In M.C. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9. MIT Press, 1996.|\n|[35]|Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.|\n|[36]|Per-\u00c5ke Wedin. Perturbation bounds in connection with singular value decomposition. BIT Numerical Mathematics, 12:99\u2013111, 1972.|\n|[37]|Puyudi Yang, Cho-Jui Hsieh, and Jane-Ling Wang. History pca: A new algorithm for streaming pca. arXiv preprint arXiv:1802.05447, 2018.|\n|[38]|Ingvar Ziemann and Stephen Tu. Learning with little mixing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 4626\u20134637. Curran Associates, Inc., 2022.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "List of References", "md": "## List of References"}, {"type": "table", "rows": [["[22]", "Shaocong Ma, Ziyi Chen, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Data sampling affects the complexity of online sgd over dependent data. In Uncertainty in Artificial Intelligence, pages 1296\u20131305. PMLR, 2022."], ["[23]", "Abdelkader Mokkadem. Mixing properties of arma processes. Stochastic Processes and their Applications, 29(2):309\u2013315, 1988."], ["[24]", "Jean-Marie Monnez. Stochastic approximation of eigenvectors and eigenvalues of the q-symmetric expectation of a random matrix. Communications in Statistics-Theory and Methods, pages 1\u201315, 2022."], ["[25]", "Nikos Mouzakis and Eric Price. Spectral guarantees for adversarial streaming pca, 2022."], ["[26]", "Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression with markovian data: Fundamental limits and algorithms. Advances in neural information processing systems, 33:16666\u201316676, 2020."], ["[27]", "Joe Neeman, Bobby Shi, and Rachel Ward. Concentration inequalities for sums of markov dependent random matrices, 2023."], ["[28]", "Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical Biology, 15(3):267\u2013273, November 1982."], ["[29]", "Erkki Oja and Juha Karhunen. On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix. Journal of mathematical analysis and applications, 106(1):69\u201384, 1985."], ["[30]", "Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning. In Conference on Learning Theory, pages 2803\u20132830. PMLR, 2019."], ["[31]", "Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. Advances in neural information processing systems, 31, 2018."], ["[32]", "Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12:389\u2013434, 2012."], ["[33]", "Lan V Truong. Generalization error bounds on deep learning with markov datasets. Advances in Neural Information Processing Systems, 35:23452\u201323462, 2022."], ["[34]", "John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation. In M.C. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9. MIT Press, 1996."], ["[35]", "Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010."], ["[36]", "Per-\u00c5ke Wedin. Perturbation bounds in connection with singular value decomposition. BIT Numerical Mathematics, 12:99\u2013111, 1972."], ["[37]", "Puyudi Yang, Cho-Jui Hsieh, and Jane-Ling Wang. History pca: A new algorithm for streaming pca. arXiv preprint arXiv:1802.05447, 2018."], ["[38]", "Ingvar Ziemann and Stephen Tu. Learning with little mixing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 4626\u20134637. Curran Associates, Inc., 2022."]], "md": "|[22]|Shaocong Ma, Ziyi Chen, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Data sampling affects the complexity of online sgd over dependent data. In Uncertainty in Artificial Intelligence, pages 1296\u20131305. PMLR, 2022.|\n|---|---|\n|[23]|Abdelkader Mokkadem. Mixing properties of arma processes. Stochastic Processes and their Applications, 29(2):309\u2013315, 1988.|\n|[24]|Jean-Marie Monnez. Stochastic approximation of eigenvectors and eigenvalues of the q-symmetric expectation of a random matrix. Communications in Statistics-Theory and Methods, pages 1\u201315, 2022.|\n|[25]|Nikos Mouzakis and Eric Price. Spectral guarantees for adversarial streaming pca, 2022.|\n|[26]|Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression with markovian data: Fundamental limits and algorithms. Advances in neural information processing systems, 33:16666\u201316676, 2020.|\n|[27]|Joe Neeman, Bobby Shi, and Rachel Ward. Concentration inequalities for sums of markov dependent random matrices, 2023.|\n|[28]|Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical Biology, 15(3):267\u2013273, November 1982.|\n|[29]|Erkki Oja and Juha Karhunen. On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix. Journal of mathematical analysis and applications, 106(1):69\u201384, 1985.|\n|[30]|Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning. In Conference on Learning Theory, pages 2803\u20132830. PMLR, 2019.|\n|[31]|Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. Advances in neural information processing systems, 31, 2018.|\n|[32]|Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12:389\u2013434, 2012.|\n|[33]|Lan V Truong. Generalization error bounds on deep learning with markov datasets. Advances in Neural Information Processing Systems, 35:23452\u201323462, 2022.|\n|[34]|John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation. In M.C. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9. MIT Press, 1996.|\n|[35]|Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.|\n|[36]|Per-\u00c5ke Wedin. Perturbation bounds in connection with singular value decomposition. BIT Numerical Mathematics, 12:99\u2013111, 1972.|\n|[37]|Puyudi Yang, Cho-Jui Hsieh, and Jane-Ling Wang. History pca: A new algorithm for streaming pca. arXiv preprint arXiv:1802.05447, 2018.|\n|[38]|Ingvar Ziemann and Stephen Tu. Learning with little mixing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 4626\u20134637. Curran Associates, Inc., 2022.|", "isPerfectTable": true, "csv": "\"[22]\",\"Shaocong Ma, Ziyi Chen, Yi Zhou, Kaiyi Ji, and Yingbin Liang. Data sampling affects the complexity of online sgd over dependent data. In Uncertainty in Artificial Intelligence, pages 1296\u20131305. PMLR, 2022.\"\n\"[23]\",\"Abdelkader Mokkadem. Mixing properties of arma processes. Stochastic Processes and their Applications, 29(2):309\u2013315, 1988.\"\n\"[24]\",\"Jean-Marie Monnez. Stochastic approximation of eigenvectors and eigenvalues of the q-symmetric expectation of a random matrix. Communications in Statistics-Theory and Methods, pages 1\u201315, 2022.\"\n\"[25]\",\"Nikos Mouzakis and Eric Price. Spectral guarantees for adversarial streaming pca, 2022.\"\n\"[26]\",\"Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression with markovian data: Fundamental limits and algorithms. Advances in neural information processing systems, 33:16666\u201316676, 2020.\"\n\"[27]\",\"Joe Neeman, Bobby Shi, and Rachel Ward. Concentration inequalities for sums of markov dependent random matrices, 2023.\"\n\"[28]\",\"Erkki Oja. Simplified neuron model as a principal component analyzer. Journal of Mathematical Biology, 15(3):267\u2013273, November 1982.\"\n\"[29]\",\"Erkki Oja and Juha Karhunen. On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix. Journal of mathematical analysis and applications, 106(1):69\u201384, 1985.\"\n\"[30]\",\"Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning. In Conference on Learning Theory, pages 2803\u20132830. PMLR, 2019.\"\n\"[31]\",\"Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. Advances in neural information processing systems, 31, 2018.\"\n\"[32]\",\"Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of computational mathematics, 12:389\u2013434, 2012.\"\n\"[33]\",\"Lan V Truong. Generalization error bounds on deep learning with markov datasets. Advances in Neural Information Processing Systems, 35:23452\u201323462, 2022.\"\n\"[34]\",\"John Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation. In M.C. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems, volume 9. MIT Press, 1996.\"\n\"[35]\",\"Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint arXiv:1011.3027, 2010.\"\n\"[36]\",\"Per-\u00c5ke Wedin. Perturbation bounds in connection with singular value decomposition. BIT Numerical Mathematics, 12:99\u2013111, 1972.\"\n\"[37]\",\"Puyudi Yang, Cho-Jui Hsieh, and Jane-Ling Wang. History pca: A new algorithm for streaming pca. arXiv preprint arXiv:1802.05447, 2018.\"\n\"[38]\",\"Ingvar Ziemann and Stephen Tu. Learning with little mixing. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 4626\u20134637. Curran Associates, Inc., 2022.\""}]}, {"page": 13, "text": "Supplement\nThe Supplement is organized as follows -\n      \u2022 Section S.1 introduces notation that will be useful for concise representation.\n      \u2022 Section S.2 provides the proof of Proposition 1.\n      \u2022 Section S.3 contains useful intermediate results which are used in subsequent proofs of our\n         main results.\n      \u2022 Section S.4 proves bounds on v1BnBT                n vT                   n V T\n                                                               1 and V\u22a5BnBT           \u22a5   (Theorems 2, 3, 4 and 5).\n      \u2022 Section S.5 puts everything together and provides proofs of our main result - Theorem 1, along\n         with Corollary 1.\n      \u2022 Section S.6 provides additional experiments to further support our claims.\nS.1        Notation and assumptions\nFor conciseness, we define the stochastic function A : \u2126                   \u2192    Rd\u00d7d which maps each state variable of\nthe Markov chain to a (d \u00d7 d) positive semi-definite symmetric matrix as\n                                                       A (st) := XtXT      t\nWhere Xt \u223c        D (st) is drawn from the distribution corresponding to the state at timestep st. All the\ntheoretical results are derived under Assumptions 1, 2 and 3.\nS.2        Offline PCA with Markovian Data\nIn this section, we prove Proposition 1. We note that [27] considers Fj (sj) to be random only with\nrespect to the states. Therefore, we first show that their results generalize to our setting as well, using\nFj (sj) := A (sj) \u2212         \u03a3. From Eq (5) in [27], we have\n     n         \u03b8                         2          \uf8eb   n          \u03b8                         1          \u03b8                      \uf8f6\n  j=1   exp      2 (A (sj) \u2212     \u03a3)      F   = Tr   \uf8ed  j=1  exp     2 (A (sj) \u2212      \u03a3)     j=n  exp      2 (A (sj) \u2212     \u03a3)    \uf8f8\n                                                             \uf8eb    n                     \uf8f6\n                                             = vec (Id)T     \uf8ed  j=1  exp (\u03b8H (sj))      \uf8f8   vec (Id)\nwhere H (sj) := 1       2 [(A (sj) \u2212     \u03a3) \u2297    Id + Id \u2297     (A (sj) \u2212     \u03a3)]. Noting that conditioned on the state\nsequence, the matrices A (si) , i \u2208                [n] are independent under our model, we can push in the\nexpectation over the state-specific distributions inside. Let E\u03c0 denote the expectation over the\nstationary state-sequence of the Markov chain, and ED denote the distribution over states. Therefore,\n          \uf8ee    n         \u03b8                          2 \uf8f9          \uf8ee               \uf8eb   n                                 \uf8f6             \uf8f9\nE\u03c0ED      \uf8ef        exp                                \uf8fa          \uf8f0vec (Id)T      \uf8ed       ED(sj) [exp (\u03b8H (sj))]        \uf8f8   vec (Id)\uf8fb\n          \uf8f0   j=1          2 (A (sj) \u2212      \u03a3)      F \uf8fb  = E\u03c0                       j=1\nDefining the multiplication operator             E\u03b8 j h   (x) = ED(x) [exp (\u03b8Hj (x))] h (x) for any vector-valued\nfunction h, we note that Eq (8) from [27] holds for our case as well.\nNext, we adapt Proposition 5.3 from [27] for our setting. Specifically, we have the following\nlemma -\n                                                                 13", "md": "# Supplement\n\n## Supplement\n\n- Section S.1 introduces notation that will be useful for concise representation.\n- Section S.2 provides the proof of Proposition 1.\n- Section S.3 contains useful intermediate results which are used in subsequent proofs of our main results.\n- Section S.4 proves bounds on $v_1B_nB^T_n$ and $V^\\perp B_nB^T_\\perp$ (Theorems 2, 3, 4 and 5).\n- Section S.5 puts everything together and provides proofs of our main result - Theorem 1, along with Corollary 1.\n- Section S.6 provides additional experiments to further support our claims.\n\n### S.1 Notation and assumptions\n\nFor conciseness, we define the stochastic function $$A : \\Omega \\rightarrow \\mathbb{R}^{d \\times d}$$ which maps each state variable of the Markov chain to a $$(d \\times d)$$ positive semi-definite symmetric matrix as\n\n$$A(s_t) := X_tX^T_t$$\n\nWhere $$X_t \\sim D(s_t)$$ is drawn from the distribution corresponding to the state at timestep $$s_t$$. All the theoretical results are derived under Assumptions 1, 2 and 3.\n\n### S.2 Offline PCA with Markovian Data\n\nIn this section, we prove Proposition 1. We note that [27] considers $$F_j(s_j)$$ to be random only with respect to the states. Therefore, we first show that their results generalize to our setting as well, using $$F_j(s_j) := A(s_j) - \\Sigma$$. From Eq (5) in [27], we have\n\n$$\n\\begin{align*}\n&\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) F \\\\\n&= \\text{Tr}\\left(\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) \\sum_{j=n} \\exp\\left(2(A(s_j) - \\Sigma)\\right)\\right) \\\\\n&= \\text{vec}(Id)^T \\left(\\sum_{j=1}^{n} \\exp(\\theta H(s_j))\\right) \\text{vec}(Id)\n\\end{align*}\n$$\nwhere $$H(s_j) := \\frac{1}{2}\\left[(A(s_j) - \\Sigma) \\otimes Id + Id \\otimes (A(s_j) - \\Sigma)\\right]$$. Noting that conditioned on the state sequence, the matrices $$A(s_i), i \\in [n]$$ are independent under our model, we can push in the expectation over the state-specific distributions inside. Let $$E_\\pi$$ denote the expectation over the stationary state-sequence of the Markov chain, and $$E_D$$ denote the distribution over states. Therefore,\n\n$$\nE_\\pi E_D \\left[\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) F\\right] = E_\\pi \\sum_{j=1}^{n} E_D \\left[\\exp(\\theta H(s_j))\\right]\n$$\nDefining the multiplication operator $$E_\\theta^j h(x) = E_D(x) \\left[\\exp(\\theta H_j(x))\\right] h(x)$$ for any vector-valued function $$h$$, we note that Eq (8) from [27] holds for our case as well.\n\nNext, we adapt Proposition 5.3 from [27] for our setting. Specifically, we have the following lemma -\n\n13", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Supplement", "md": "# Supplement"}, {"type": "heading", "lvl": 2, "value": "Supplement", "md": "## Supplement"}, {"type": "text", "value": "- Section S.1 introduces notation that will be useful for concise representation.\n- Section S.2 provides the proof of Proposition 1.\n- Section S.3 contains useful intermediate results which are used in subsequent proofs of our main results.\n- Section S.4 proves bounds on $v_1B_nB^T_n$ and $V^\\perp B_nB^T_\\perp$ (Theorems 2, 3, 4 and 5).\n- Section S.5 puts everything together and provides proofs of our main result - Theorem 1, along with Corollary 1.\n- Section S.6 provides additional experiments to further support our claims.", "md": "- Section S.1 introduces notation that will be useful for concise representation.\n- Section S.2 provides the proof of Proposition 1.\n- Section S.3 contains useful intermediate results which are used in subsequent proofs of our main results.\n- Section S.4 proves bounds on $v_1B_nB^T_n$ and $V^\\perp B_nB^T_\\perp$ (Theorems 2, 3, 4 and 5).\n- Section S.5 puts everything together and provides proofs of our main result - Theorem 1, along with Corollary 1.\n- Section S.6 provides additional experiments to further support our claims."}, {"type": "heading", "lvl": 3, "value": "S.1 Notation and assumptions", "md": "### S.1 Notation and assumptions"}, {"type": "text", "value": "For conciseness, we define the stochastic function $$A : \\Omega \\rightarrow \\mathbb{R}^{d \\times d}$$ which maps each state variable of the Markov chain to a $$(d \\times d)$$ positive semi-definite symmetric matrix as\n\n$$A(s_t) := X_tX^T_t$$\n\nWhere $$X_t \\sim D(s_t)$$ is drawn from the distribution corresponding to the state at timestep $$s_t$$. All the theoretical results are derived under Assumptions 1, 2 and 3.", "md": "For conciseness, we define the stochastic function $$A : \\Omega \\rightarrow \\mathbb{R}^{d \\times d}$$ which maps each state variable of the Markov chain to a $$(d \\times d)$$ positive semi-definite symmetric matrix as\n\n$$A(s_t) := X_tX^T_t$$\n\nWhere $$X_t \\sim D(s_t)$$ is drawn from the distribution corresponding to the state at timestep $$s_t$$. All the theoretical results are derived under Assumptions 1, 2 and 3."}, {"type": "heading", "lvl": 3, "value": "S.2 Offline PCA with Markovian Data", "md": "### S.2 Offline PCA with Markovian Data"}, {"type": "text", "value": "In this section, we prove Proposition 1. We note that [27] considers $$F_j(s_j)$$ to be random only with respect to the states. Therefore, we first show that their results generalize to our setting as well, using $$F_j(s_j) := A(s_j) - \\Sigma$$. From Eq (5) in [27], we have\n\n$$\n\\begin{align*}\n&\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) F \\\\\n&= \\text{Tr}\\left(\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) \\sum_{j=n} \\exp\\left(2(A(s_j) - \\Sigma)\\right)\\right) \\\\\n&= \\text{vec}(Id)^T \\left(\\sum_{j=1}^{n} \\exp(\\theta H(s_j))\\right) \\text{vec}(Id)\n\\end{align*}\n$$\nwhere $$H(s_j) := \\frac{1}{2}\\left[(A(s_j) - \\Sigma) \\otimes Id + Id \\otimes (A(s_j) - \\Sigma)\\right]$$. Noting that conditioned on the state sequence, the matrices $$A(s_i), i \\in [n]$$ are independent under our model, we can push in the expectation over the state-specific distributions inside. Let $$E_\\pi$$ denote the expectation over the stationary state-sequence of the Markov chain, and $$E_D$$ denote the distribution over states. Therefore,\n\n$$\nE_\\pi E_D \\left[\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) F\\right] = E_\\pi \\sum_{j=1}^{n} E_D \\left[\\exp(\\theta H(s_j))\\right]\n$$\nDefining the multiplication operator $$E_\\theta^j h(x) = E_D(x) \\left[\\exp(\\theta H_j(x))\\right] h(x)$$ for any vector-valued function $$h$$, we note that Eq (8) from [27] holds for our case as well.\n\nNext, we adapt Proposition 5.3 from [27] for our setting. Specifically, we have the following lemma -\n\n13", "md": "In this section, we prove Proposition 1. We note that [27] considers $$F_j(s_j)$$ to be random only with respect to the states. Therefore, we first show that their results generalize to our setting as well, using $$F_j(s_j) := A(s_j) - \\Sigma$$. From Eq (5) in [27], we have\n\n$$\n\\begin{align*}\n&\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) F \\\\\n&= \\text{Tr}\\left(\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) \\sum_{j=n} \\exp\\left(2(A(s_j) - \\Sigma)\\right)\\right) \\\\\n&= \\text{vec}(Id)^T \\left(\\sum_{j=1}^{n} \\exp(\\theta H(s_j))\\right) \\text{vec}(Id)\n\\end{align*}\n$$\nwhere $$H(s_j) := \\frac{1}{2}\\left[(A(s_j) - \\Sigma) \\otimes Id + Id \\otimes (A(s_j) - \\Sigma)\\right]$$. Noting that conditioned on the state sequence, the matrices $$A(s_i), i \\in [n]$$ are independent under our model, we can push in the expectation over the state-specific distributions inside. Let $$E_\\pi$$ denote the expectation over the stationary state-sequence of the Markov chain, and $$E_D$$ denote the distribution over states. Therefore,\n\n$$\nE_\\pi E_D \\left[\\sum_{j=1}^{n} \\exp\\left(2(A(s_j) - \\Sigma)\\right) F\\right] = E_\\pi \\sum_{j=1}^{n} E_D \\left[\\exp(\\theta H(s_j))\\right]\n$$\nDefining the multiplication operator $$E_\\theta^j h(x) = E_D(x) \\left[\\exp(\\theta H_j(x))\\right] h(x)$$ for any vector-valued function $$h$$, we note that Eq (8) from [27] holds for our case as well.\n\nNext, we adapt Proposition 5.3 from [27] for our setting. Specifically, we have the following lemma -\n\n13"}]}, {"page": 14, "text": "                                                            1\nLemma S.1. Consider the operator H (x) :=                   2 [(A (x) \u2212    \u03a3) \u2297   Id + Id \u2297    (A (x) \u2212   \u03a3)]. Then,\nunder assumptions 3 and 2 and the definition of \u03a3, we have,\n    1. E\u03c0ED(x) [H (x)] = 0\n    2. H (x) \u2aaf     MI\n    3.   E\u03c0ED(x)      H (x)2     2 \u2264   V\nProof. The proof follows by using the same arguments as Proposition 5.3 from [27] and using\nthe expectation E\u03c0ED(x) over both the state sequence and the distribution over states, along with\nassumptions 3 and 2.\nFinally, to prove Bernstein\u2019s inequality, we prove that Lemma 6.7 from [27] holds for our case. To\nnote this, we start with equation (57) in their work. We have, using Lemma S.1,\n  v2, E\u03c0ED(x) [exp (\u03b8H (x))] v1            =    v2, E\u03c0ED(x) [exp (\u03b8H (x))] v1         \u221e   \u03b8k\n                                           =     v2,    I + E\u03c0ED(x) [H (x)] +        k=2   k! E\u03c0ED(x)      H (x)k     v1\n                                           =                  v2     \u221e   \u03b8k              H (x)k       v1\n                                               \u27e8v2, v1\u27e9  +          k=2\u221e k! E\u03c0ED(x)\n                                                                           \u03b8k\n                                           \u2264  |\u27e8v2, v1\u27e9|    1 + V     k=2  k! Mk\u22122\nTherefore, Eq (60) from [27] follows. The other bounds in the proof of Lemma 6.7 from [27] follow\nsimilarly. Therefore, we have the following version of Theorem 2.2 from [27] -\nProposition S.1. Under assumptions 2 and 3, we have\n              \uf8eb    1   n                        \uf8f6                  \uf8eb                t2/ 32                \uf8f6\n           P  \uf8ed    n      A (sj) \u2212    \u03a3     \u2265  t\uf8f8  \u2264   d2\u2212\u03c0 4 exp  \uf8ed  1+|\u03bb2(P )|        \u03c02   8/\u03c0          \uf8f8\n                     j=1                 2                            1\u2212|\u03bb2(P )|nV +     1\u2212|\u03bb2(P )|Mt\n     The proof of Proposition 1 now follows by converting the tail bound into a high probability bound\nand using Wedin\u2019s theorem [36]. See proof of Theorem 1.1 in [15] for details.\nS.3       Useful Results\nThis section presents some useful lemmas and their proofs that are subsequently used in our proofs.\nLemma S.2. (Reverse mixing) Consider a reversible, irreducible, and aperiodic Markov chain\nstarted from the stationary distribution. Then,\n                             1           |P (Zt = s|Zt+k = t) \u2212        \u03c0 (s)| = dmix (k)\n                             2 sup\n                               t\u2208\u2126    s\nProof. Let the transition probabilities of the Markov chain be represented as P(x|y) := P(Zt+1 =\nx|Zt = y). Consider the time-reversed chain Yi := Zn\u2212i+1 for i = 1, 2, . . . n. Then,\n                  P (Yl = sl|Yl\u22121 = sl\u22121, Yl\u22122 = sl\u22122 . . . Y1 = s1)\n                      = P (Zn\u2212l+1 = sl|Zn\u2212l+2 = sl\u22121, Zn\u2212l+3 = sl\u22122, . . . Zn = s1)\n                      = P (Zn\u2212l+1 = sl|Zn\u2212l+2 = sl\u22121)               using Lemma S.6\n                      = P (Zn\u2212l+1 = sl, Zn\u2212l+2 = sl\u22121)\n                                  P (Zn\u2212l+2 = sl\u22121)\n                      = \u03c0 (sl) P(sl\u22121|sl)\n                               \u03c0 (sl\u22121)\n                      = P(sl|sl\u22121)       using reversibility\n                                                          14", "md": "# Math Equations and Lemmas\n\n## Lemma S.1\n\nConsider the operator $$H (x) := 2 [(A (x) - \\Sigma) \\otimes Id + Id \\otimes (A (x) - \\Sigma)].$$ Then, under assumptions 3 and 2 and the definition of \u03a3, we have:\n\n1. $E_{\\pi}E_{D(x)} [H (x)] = 0$\n2. $H (x) \\preceq MI$\n3. $E_{\\pi}E_{D(x)} [H (x)^2] \\leq V$\n\nProof: The proof follows by using the same arguments as Proposition 5.3 from [27] and using the expectation $$E_{\\pi}E_{D(x)}$$ over both the state sequence and the distribution over states, along with assumptions 3 and 2.\n\n## Proposition S.1\n\nUnder assumptions 2 and 3, we have:\n\n$$\nP\\left(\\left. \\left(\\frac{1}{n} \\sum_{j=1}^{n} A(s_j) - \\Sigma\\right) \\geq t \\right| \\right) \\leq d^2 - \\pi^4 \\exp\\left(\\frac{t^2}{32}\\right) \\left(1 + \\left|\\lambda_2(P)\\right|\\right)^{\\frac{\\pi^2}{8\\pi}}$$\n\nThe proof of Proposition 1 now follows by converting the tail bound into a high probability bound and using Wedin\u2019s theorem [36]. See proof of Theorem 1.1 in [15] for details.\n\n## Useful Results\n\n### Lemma S.2 (Reverse mixing)\n\nConsider a reversible, irreducible, and aperiodic Markov chain started from the stationary distribution. Then:\n\n$$\n\\frac{1}{2} \\sup_{t \\in \\Omega} \\left|P(Z_t = s | Z_{t+k} = t) - \\pi(s)\\right| = d_{\\text{mix}}(k)\n$$\nProof: Let the transition probabilities of the Markov chain be represented as $$P(x|y) := P(Z_{t+1} = x | Z_t = y)$$. Consider the time-reversed chain $$Y_i := Z_{n-i+1}$$ for $$i = 1, 2, ... n$$. Then,\n\n1. $P(Y_l = s_l | Y_{l-1} = s_{l-1}, Y_{l-2} = s_{l-2}, ..., Y_1 = s_1)$\n2. $= P(Z_{n-l+1} = s_l | Z_{n-l+2} = s_{l-1}, Z_{n-l+3} = s_{l-2}, ..., Z_n = s_1)$\n3. $= P(Z_{n-l+1} = s_l | Z_{n-l+2} = s_{l-1})$ using Lemma S.6\n4. $= P(Z_{n-l+1} = s_l, Z_{n-l+2} = s_{l-1}) / P(Z_{n-l+2} = s_{l-1})$\n5. $= \\pi(s_l) P(s_{l-1} | s_l) / \\pi(s_{l-1})$ using reversibility\n6. $= P(s_l | s_{l-1})$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Lemmas", "md": "# Math Equations and Lemmas"}, {"type": "heading", "lvl": 2, "value": "Lemma S.1", "md": "## Lemma S.1"}, {"type": "text", "value": "Consider the operator $$H (x) := 2 [(A (x) - \\Sigma) \\otimes Id + Id \\otimes (A (x) - \\Sigma)].$$ Then, under assumptions 3 and 2 and the definition of \u03a3, we have:\n\n1. $E_{\\pi}E_{D(x)} [H (x)] = 0$\n2. $H (x) \\preceq MI$\n3. $E_{\\pi}E_{D(x)} [H (x)^2] \\leq V$\n\nProof: The proof follows by using the same arguments as Proposition 5.3 from [27] and using the expectation $$E_{\\pi}E_{D(x)}$$ over both the state sequence and the distribution over states, along with assumptions 3 and 2.", "md": "Consider the operator $$H (x) := 2 [(A (x) - \\Sigma) \\otimes Id + Id \\otimes (A (x) - \\Sigma)].$$ Then, under assumptions 3 and 2 and the definition of \u03a3, we have:\n\n1. $E_{\\pi}E_{D(x)} [H (x)] = 0$\n2. $H (x) \\preceq MI$\n3. $E_{\\pi}E_{D(x)} [H (x)^2] \\leq V$\n\nProof: The proof follows by using the same arguments as Proposition 5.3 from [27] and using the expectation $$E_{\\pi}E_{D(x)}$$ over both the state sequence and the distribution over states, along with assumptions 3 and 2."}, {"type": "heading", "lvl": 2, "value": "Proposition S.1", "md": "## Proposition S.1"}, {"type": "text", "value": "Under assumptions 2 and 3, we have:\n\n$$\nP\\left(\\left. \\left(\\frac{1}{n} \\sum_{j=1}^{n} A(s_j) - \\Sigma\\right) \\geq t \\right| \\right) \\leq d^2 - \\pi^4 \\exp\\left(\\frac{t^2}{32}\\right) \\left(1 + \\left|\\lambda_2(P)\\right|\\right)^{\\frac{\\pi^2}{8\\pi}}$$\n\nThe proof of Proposition 1 now follows by converting the tail bound into a high probability bound and using Wedin\u2019s theorem [36]. See proof of Theorem 1.1 in [15] for details.", "md": "Under assumptions 2 and 3, we have:\n\n$$\nP\\left(\\left. \\left(\\frac{1}{n} \\sum_{j=1}^{n} A(s_j) - \\Sigma\\right) \\geq t \\right| \\right) \\leq d^2 - \\pi^4 \\exp\\left(\\frac{t^2}{32}\\right) \\left(1 + \\left|\\lambda_2(P)\\right|\\right)^{\\frac{\\pi^2}{8\\pi}}$$\n\nThe proof of Proposition 1 now follows by converting the tail bound into a high probability bound and using Wedin\u2019s theorem [36]. See proof of Theorem 1.1 in [15] for details."}, {"type": "heading", "lvl": 2, "value": "Useful Results", "md": "## Useful Results"}, {"type": "heading", "lvl": 3, "value": "Lemma S.2 (Reverse mixing)", "md": "### Lemma S.2 (Reverse mixing)"}, {"type": "text", "value": "Consider a reversible, irreducible, and aperiodic Markov chain started from the stationary distribution. Then:\n\n$$\n\\frac{1}{2} \\sup_{t \\in \\Omega} \\left|P(Z_t = s | Z_{t+k} = t) - \\pi(s)\\right| = d_{\\text{mix}}(k)\n$$\nProof: Let the transition probabilities of the Markov chain be represented as $$P(x|y) := P(Z_{t+1} = x | Z_t = y)$$. Consider the time-reversed chain $$Y_i := Z_{n-i+1}$$ for $$i = 1, 2, ... n$$. Then,\n\n1. $P(Y_l = s_l | Y_{l-1} = s_{l-1}, Y_{l-2} = s_{l-2}, ..., Y_1 = s_1)$\n2. $= P(Z_{n-l+1} = s_l | Z_{n-l+2} = s_{l-1}, Z_{n-l+3} = s_{l-2}, ..., Z_n = s_1)$\n3. $= P(Z_{n-l+1} = s_l | Z_{n-l+2} = s_{l-1})$ using Lemma S.6\n4. $= P(Z_{n-l+1} = s_l, Z_{n-l+2} = s_{l-1}) / P(Z_{n-l+2} = s_{l-1})$\n5. $= \\pi(s_l) P(s_{l-1} | s_l) / \\pi(s_{l-1})$ using reversibility\n6. $= P(s_l | s_{l-1})$", "md": "Consider a reversible, irreducible, and aperiodic Markov chain started from the stationary distribution. Then:\n\n$$\n\\frac{1}{2} \\sup_{t \\in \\Omega} \\left|P(Z_t = s | Z_{t+k} = t) - \\pi(s)\\right| = d_{\\text{mix}}(k)\n$$\nProof: Let the transition probabilities of the Markov chain be represented as $$P(x|y) := P(Z_{t+1} = x | Z_t = y)$$. Consider the time-reversed chain $$Y_i := Z_{n-i+1}$$ for $$i = 1, 2, ... n$$. Then,\n\n1. $P(Y_l = s_l | Y_{l-1} = s_{l-1}, Y_{l-2} = s_{l-2}, ..., Y_1 = s_1)$\n2. $= P(Z_{n-l+1} = s_l | Z_{n-l+2} = s_{l-1}, Z_{n-l+3} = s_{l-2}, ..., Z_n = s_1)$\n3. $= P(Z_{n-l+1} = s_l | Z_{n-l+2} = s_{l-1})$ using Lemma S.6\n4. $= P(Z_{n-l+1} = s_l, Z_{n-l+2} = s_{l-1}) / P(Z_{n-l+2} = s_{l-1})$\n5. $= \\pi(s_l) P(s_{l-1} | s_l) / \\pi(s_{l-1})$ using reversibility\n6. $= P(s_l | s_{l-1})$"}]}, {"page": 15, "text": "This proves that Yn is an irreducible Markov chain with the same transition probabilities as the\noriginal Markov chain. The irreducibility of Yn follows from the original Markov chain being\nirreducible. Therefore,\nThen,                  P (Zt = s1|Zt+k = s2) = P (Yn+1\u2212t = s1|Yn+1\u2212t\u2212k = s2)                                    (S.12)\n1            |P (Zt = s|Zt+k = t) \u2212        \u03c0 (s)| = 1             |P (Yn+1\u2212t = s|Yn+1\u2212t\u2212k = t) \u2212            \u03c0 (s)| = dmix (k)\n2 sup                                                 2 sup\n   t\u2208\u2126   s                                              t\u2208\u2126    s\nwhere the last inequality follows from the forward mixing properties of the Markov chain.\nLemma S.3. Let Cj,i =  i           t=j(I + Zt) for i \u2264        j \u2264   n, where Zt \u2208       Rd\u00d7d are symmetric PSD\nmatrices. Let U \u2208      Rd\u00d7d\u2032. Then,\nProof.                          Tr   U T Cj,i+1CT  j,i+1U     \u2264  Tr   U T Cj,iCT j,iU\n         Tr   U T Cj,iCT j,iU   = Tr    U T Cj,i+1(I + 2Zi + Z2      i )CTj,i+1U\n                                = Tr    U T Cj,i+1CT   j,i+1U    + Tr    U T Cj,i+1(2Zi + Z2    i )CTj,i+1U\nSince Zi and Z2    i are both PSD, the second term on the RHS is always positive. This yields the\nproof.\nLemma S.4. Let Bt =  1          i=t(I + Zi), where Zi \u2208       Rd\u00d7d are symmetric PSD matrices.\nProof.                                  Tr   Bn\u22121BT    n\u22121   \u2264   Tr   BnBT  n\nTr   BnBT  n   = Tr     (I + Zn)Bn\u22121BT      n\u22121(I + Zn)                                        + Tr    ZnBn\u22121BT\n               = Tr     Bn\u22121BT   n\u22121    + Tr    ZnBn\u22121BT     n\u22121   + Tr    Bn\u22121BT    n\u22121Zn                          n\u22121Zn\n               = Tr     Bn\u22121BT   n\u22121    + 2 Tr    BTn\u22121ZnBn\u22121        + Tr    BT n\u22121Z2 nBn\u22121\nSince Zn and Z2    n are both PSD, the last two terms on the RHS are always positive. This yields the\nproof.\nLemma S.5. Consider matrices X \u2208              Rd\u00d7d\u2032 and A \u2208       Rd\u00d7d. Then,\n                                        Tr   XT AX       \u2264   \u2225A\u22252Tr     XT X\nProof. For a matrix Z \u2208       Rd\u00d7d, let the singular values be denoted as :\n                                  \u03c3max (Z) = \u03c31 (Z) \u2265         \u03c32 (Z) . . . \u2265  \u03c3d (Z)\nUsing Von-Neumann\u2019s trace inequality, we have\n                                   Tr   XT AX       =    Tr  AXXT\n                                                         d\n                                                    \u2264   i=1 \u03c3i (A) \u03c3id   XXT\n                                                    \u2264   \u03c3max (A)    i=1  \u03c3i  XXT\n                                                    = \u2225A\u22252Tr       XXT\n                                                    = \u2225A\u22252Tr       XT X\n                                                          15", "md": "This proves that Yn is an irreducible Markov chain with the same transition probabilities as the original Markov chain. The irreducibility of Yn follows from the original Markov chain being irreducible. Therefore,\n\nThen, $$P(Z_t = s_1 | Z_{t+k} = s_2) = P(Y_{n+1-t} = s_1 | Y_{n+1-t-k} = s_2)$$ (S.12)\n\n$|P(Z_t = s | Z_{t+k} = t) - \\pi(s)| = |P(Y_{n+1-t} = s | Y_{n+1-t-k} = t) - \\pi(s)| = d_{\\text{mix}}(k)$\n$\\frac{1}{2} \\sup_{t \\in \\Omega, s}$\n\nwhere the last inequality follows from the forward mixing properties of the Markov chain.\n\nLemma S.3. Let $$C_{j,i} = \\sum_{t=j}^{i} (I + Z_t)$$ for $$i \\leq j \\leq n$$, where $$Z_t \\in \\mathbb{R}^{d \\times d}$$ are symmetric PSD matrices. Let $$U \\in \\mathbb{R}^{d \\times d'}$$. Then,\n\nProof. $$\\text{Tr } U^T C_{j,i+1} C^T_{j,i+1} U \\leq \\text{Tr } U^T C_{j,i} C^T_{j,i} U$$\n\n$$\\text{Tr } U^T C_{j,i} C^T_{j,i} U = \\text{Tr } U^T C_{j,i+1}(I + 2Z_i + Z^2_i) C^T_{j,i+1} U$$\n\n$$= \\text{Tr } U^T C_{j,i+1} C^T_{j,i+1} U + \\text{Tr } U^T C_{j,i+1}(2Z_i + Z^2_i) C^T_{j,i+1} U$$\n\nSince $$Z_i$$ and $$Z^2_i$$ are both PSD, the second term on the RHS is always positive. This yields the proof.\n\nLemma S.4. Let $$B_t = \\sum_{i=t} (I + Z_i)$$, where $$Z_i \\in \\mathbb{R}^{d \\times d}$$ are symmetric PSD matrices.\n\nProof. $$\\text{Tr } B_{n-1} B^T_{n-1} \\leq \\text{Tr } B_n B^T_n$$\n\n$$\\text{Tr } B_n B^T_n = \\text{Tr } (I + Z_n) B_{n-1} B^T_{n-1} (I + Z_n) + \\text{Tr } Z_n B_{n-1} B^T_{n-1}$$\n\n$$= \\text{Tr } B_{n-1} B^T_{n-1} + \\text{Tr } B_{n-1} B^T_{n-1} Z_n + \\text{Tr } B_{n-1} B^T_{n-1} Z_n$$\n\nSince $$Z_n$$ and $$Z^2_n$$ are both PSD, the last two terms on the RHS are always positive. This yields the proof.\n\nLemma S.5. Consider matrices $$X \\in \\mathbb{R}^{d \\times d'}$$ and $$A \\in \\mathbb{R}^{d \\times d}$$. Then,\n\n$$\\text{Tr } X^T A X \\leq ||A||^2 \\text{Tr } X^T X$$\n\nProof. For a matrix $$Z \\in \\mathbb{R}^{d \\times d}$$, let the singular values be denoted as:\n\n$$\\sigma_{\\text{max}}(Z) = \\sigma_1(Z) \\geq \\sigma_2(Z) \\geq ... \\geq \\sigma_d(Z)$$\n\nUsing Von-Neumann\u2019s trace inequality, we have\n\n$$\\text{Tr } X^T A X = \\text{Tr } A X X^T$$\n\n$$\\leq \\sum_{i=1}^{d} \\sigma_i(A) \\sigma_{d-i} X X^T$$\n\n$$\\leq \\sigma_{\\text{max}}(A) \\sum_{i=1} \\sigma_i X X^T$$\n\n$$= ||A||^2 \\text{Tr } X X^T$$\n\n$$= ||A||^2 \\text{Tr } X^T X$$", "images": [], "items": [{"type": "text", "value": "This proves that Yn is an irreducible Markov chain with the same transition probabilities as the original Markov chain. The irreducibility of Yn follows from the original Markov chain being irreducible. Therefore,\n\nThen, $$P(Z_t = s_1 | Z_{t+k} = s_2) = P(Y_{n+1-t} = s_1 | Y_{n+1-t-k} = s_2)$$ (S.12)\n\n$|P(Z_t = s | Z_{t+k} = t) - \\pi(s)| = |P(Y_{n+1-t} = s | Y_{n+1-t-k} = t) - \\pi(s)| = d_{\\text{mix}}(k)$\n$\\frac{1}{2} \\sup_{t \\in \\Omega, s}$\n\nwhere the last inequality follows from the forward mixing properties of the Markov chain.\n\nLemma S.3. Let $$C_{j,i} = \\sum_{t=j}^{i} (I + Z_t)$$ for $$i \\leq j \\leq n$$, where $$Z_t \\in \\mathbb{R}^{d \\times d}$$ are symmetric PSD matrices. Let $$U \\in \\mathbb{R}^{d \\times d'}$$. Then,\n\nProof. $$\\text{Tr } U^T C_{j,i+1} C^T_{j,i+1} U \\leq \\text{Tr } U^T C_{j,i} C^T_{j,i} U$$\n\n$$\\text{Tr } U^T C_{j,i} C^T_{j,i} U = \\text{Tr } U^T C_{j,i+1}(I + 2Z_i + Z^2_i) C^T_{j,i+1} U$$\n\n$$= \\text{Tr } U^T C_{j,i+1} C^T_{j,i+1} U + \\text{Tr } U^T C_{j,i+1}(2Z_i + Z^2_i) C^T_{j,i+1} U$$\n\nSince $$Z_i$$ and $$Z^2_i$$ are both PSD, the second term on the RHS is always positive. This yields the proof.\n\nLemma S.4. Let $$B_t = \\sum_{i=t} (I + Z_i)$$, where $$Z_i \\in \\mathbb{R}^{d \\times d}$$ are symmetric PSD matrices.\n\nProof. $$\\text{Tr } B_{n-1} B^T_{n-1} \\leq \\text{Tr } B_n B^T_n$$\n\n$$\\text{Tr } B_n B^T_n = \\text{Tr } (I + Z_n) B_{n-1} B^T_{n-1} (I + Z_n) + \\text{Tr } Z_n B_{n-1} B^T_{n-1}$$\n\n$$= \\text{Tr } B_{n-1} B^T_{n-1} + \\text{Tr } B_{n-1} B^T_{n-1} Z_n + \\text{Tr } B_{n-1} B^T_{n-1} Z_n$$\n\nSince $$Z_n$$ and $$Z^2_n$$ are both PSD, the last two terms on the RHS are always positive. This yields the proof.\n\nLemma S.5. Consider matrices $$X \\in \\mathbb{R}^{d \\times d'}$$ and $$A \\in \\mathbb{R}^{d \\times d}$$. Then,\n\n$$\\text{Tr } X^T A X \\leq ||A||^2 \\text{Tr } X^T X$$\n\nProof. For a matrix $$Z \\in \\mathbb{R}^{d \\times d}$$, let the singular values be denoted as:\n\n$$\\sigma_{\\text{max}}(Z) = \\sigma_1(Z) \\geq \\sigma_2(Z) \\geq ... \\geq \\sigma_d(Z)$$\n\nUsing Von-Neumann\u2019s trace inequality, we have\n\n$$\\text{Tr } X^T A X = \\text{Tr } A X X^T$$\n\n$$\\leq \\sum_{i=1}^{d} \\sigma_i(A) \\sigma_{d-i} X X^T$$\n\n$$\\leq \\sigma_{\\text{max}}(A) \\sum_{i=1} \\sigma_i X X^T$$\n\n$$= ||A||^2 \\text{Tr } X X^T$$\n\n$$= ||A||^2 \\text{Tr } X^T X$$", "md": "This proves that Yn is an irreducible Markov chain with the same transition probabilities as the original Markov chain. The irreducibility of Yn follows from the original Markov chain being irreducible. Therefore,\n\nThen, $$P(Z_t = s_1 | Z_{t+k} = s_2) = P(Y_{n+1-t} = s_1 | Y_{n+1-t-k} = s_2)$$ (S.12)\n\n$|P(Z_t = s | Z_{t+k} = t) - \\pi(s)| = |P(Y_{n+1-t} = s | Y_{n+1-t-k} = t) - \\pi(s)| = d_{\\text{mix}}(k)$\n$\\frac{1}{2} \\sup_{t \\in \\Omega, s}$\n\nwhere the last inequality follows from the forward mixing properties of the Markov chain.\n\nLemma S.3. Let $$C_{j,i} = \\sum_{t=j}^{i} (I + Z_t)$$ for $$i \\leq j \\leq n$$, where $$Z_t \\in \\mathbb{R}^{d \\times d}$$ are symmetric PSD matrices. Let $$U \\in \\mathbb{R}^{d \\times d'}$$. Then,\n\nProof. $$\\text{Tr } U^T C_{j,i+1} C^T_{j,i+1} U \\leq \\text{Tr } U^T C_{j,i} C^T_{j,i} U$$\n\n$$\\text{Tr } U^T C_{j,i} C^T_{j,i} U = \\text{Tr } U^T C_{j,i+1}(I + 2Z_i + Z^2_i) C^T_{j,i+1} U$$\n\n$$= \\text{Tr } U^T C_{j,i+1} C^T_{j,i+1} U + \\text{Tr } U^T C_{j,i+1}(2Z_i + Z^2_i) C^T_{j,i+1} U$$\n\nSince $$Z_i$$ and $$Z^2_i$$ are both PSD, the second term on the RHS is always positive. This yields the proof.\n\nLemma S.4. Let $$B_t = \\sum_{i=t} (I + Z_i)$$, where $$Z_i \\in \\mathbb{R}^{d \\times d}$$ are symmetric PSD matrices.\n\nProof. $$\\text{Tr } B_{n-1} B^T_{n-1} \\leq \\text{Tr } B_n B^T_n$$\n\n$$\\text{Tr } B_n B^T_n = \\text{Tr } (I + Z_n) B_{n-1} B^T_{n-1} (I + Z_n) + \\text{Tr } Z_n B_{n-1} B^T_{n-1}$$\n\n$$= \\text{Tr } B_{n-1} B^T_{n-1} + \\text{Tr } B_{n-1} B^T_{n-1} Z_n + \\text{Tr } B_{n-1} B^T_{n-1} Z_n$$\n\nSince $$Z_n$$ and $$Z^2_n$$ are both PSD, the last two terms on the RHS are always positive. This yields the proof.\n\nLemma S.5. Consider matrices $$X \\in \\mathbb{R}^{d \\times d'}$$ and $$A \\in \\mathbb{R}^{d \\times d}$$. Then,\n\n$$\\text{Tr } X^T A X \\leq ||A||^2 \\text{Tr } X^T X$$\n\nProof. For a matrix $$Z \\in \\mathbb{R}^{d \\times d}$$, let the singular values be denoted as:\n\n$$\\sigma_{\\text{max}}(Z) = \\sigma_1(Z) \\geq \\sigma_2(Z) \\geq ... \\geq \\sigma_d(Z)$$\n\nUsing Von-Neumann\u2019s trace inequality, we have\n\n$$\\text{Tr } X^T A X = \\text{Tr } A X X^T$$\n\n$$\\leq \\sum_{i=1}^{d} \\sigma_i(A) \\sigma_{d-i} X X^T$$\n\n$$\\leq \\sigma_{\\text{max}}(A) \\sum_{i=1} \\sigma_i X X^T$$\n\n$$= ||A||^2 \\text{Tr } X X^T$$\n\n$$= ||A||^2 \\text{Tr } X^T X$$"}]}, {"page": 16, "text": "Lemma S.6. Given the Markov property in a Markov chain, the reverse Markov property holds, i.e\nProof.              P (Zt = s|Zt+1 = w, Zt+2 = st+2 . . . Zn = sn) = P (Zt = s|Zt+1 = w)\n               P (Zt = s|Zt+1 = w, Zt+2 = st+2 . . . Zn = sn)\n                       = P (Zt = s, Zt+1 = w, Zt+2 = st+2 . . . Zn = sn)\n                                   P (Zt+1 = t, Zt+2 = st+2 . . . Zn = sn)\n                       = P (Zt = s, Zt+1 = w) P (Zt+2 = st+2 . . . Zn = sn|Zt = s, Zt+1 = w)\n                                         P (Zt+1 = w) P (Zt+2 = st+2 . . . Zn = sn|Zt+1 = w)\n                       = P (Zt = s, Zt+1 = w) P (Zt+2 = st+2 . . . Zn = sn|Zt+1 = w)\n                                   P (Zt+1 = w) P (Zt+2 = st+2 . . . Zn = sn|Zt+1 = w)\n                       = P (Zt = s, Zt+1 = w)\n                                   P (Zt+1 = w)\n                       = P (Zt = s|Zt+1 = w)\nS.3.1          Proof of Lemma 2\nNow we are ready to provide a proof of Lemma 2.\nProof of Lemma 2. Without loss of generality, we prove the statement for m = 1. For convenience\nof notation, we denote k := k1. Note that,\n                    k                            r\n     Bk,1 =        r=0   (i1,i2...ir)\u2208Gr       j=1   \u03b7ij  A(sij    ), Gr = {(i1, . . . , ir) \u2208              {1, . . . , N}r : i1 < \u00b7 \u00b7 \u00b7 < ir}\nwith the convention that                  \u03d5 = I. Therefore, since \u03b7i forms a non-increasing sequence and |Gr|=                                       kr  ,\nwe have,\n                             \u2225Bk,1 \u2212       I\u22252 =           k                           r    \u03b7ij  A(sij    )\n                                                        r=1     (i1,i2...ir)\u2208Gr      j=1                      2\n                                                  \u2264      k                             r    \u03b7ij  A(sij    )\n                                                       r=1   (i1,i2...ir)\u2208Gr         j=1                      2\n                                                  \u2264      k    k  r         r    \u03b7i     (M + \u03bb1)r\n                                                       r=1                i=1\n                                                  \u2264      k    kr        r   \u03b7i      (M + \u03bb1)r\n                                                       r=1    r!      i=1\n                                                  \u2264      k    kr    1 (M + \u03bb1)r\n                                                       r=1    r! \u03b7r\n                                                  \u2264    exp (k\u03b71 (M + \u03bb1)) \u2212                   1\n                                                  \u2264    k\u03b71 (M + \u03bb1) (1 + k\u03b71 (M + \u03bb1)) using S.14\nwhere we have used the assumptions that \u2225A(s)\u22252\u2264  \u2264    (1 + \u03f5) k\u03b71 (M + \u03bb1)        \u2225A(s)\u2212\u03a3\u2225+\u2225\u03a3\u22252= (M + \u03bb1), k\u03b71 (M + \u03bb1) <                      (S.13)\n1 and the useful result that\n                                                        ex \u2264     1 + x + x2, x \u2208             [0, 1.79]                                          (S.14)\n                                                                               16", "md": "Lemma $$S.6$$. Given the Markov property in a Markov chain, the reverse Markov property holds, i.e\n\nProof.\n\n\\[\n\\begin{align*}\n& P(Z_t = s | Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n) = P(Z_t = s | Z_{t+1} = w) \\\\\n& = \\frac{P(Z_t = s, Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n)}{P(Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n)} \\\\\n& = \\frac{P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_t = s, Z_{t+1} = w)}{P(Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w)} \\\\\n& = P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w) \\\\\n& = P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w) \\\\\n& = P(Z_t = s, Z_{t+1} = w) \\\\\n& = P(Z_t = s | Z_{t+1} = w)\n\\end{align*}\n\\]\n\n$$S.3.1$$ Proof of Lemma 2\n\nNow we are ready to provide a proof of Lemma 2.\n\nProof of Lemma 2. Without loss of generality, we prove the statement for $$m = 1$$. For convenience of notation, we denote $$k := k_1$$. Note that,\n\n\\[\nB_{k,1} = \\sum_{r=0}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}), \\quad G_r = \\{(i_1, \\ldots, i_r) \\in \\{1, \\ldots, N\\}^r : i_1 < \\ldots < i_r\\}\n\\]\n\nwith the convention that $$\\phi = I$$. Therefore, since $$\\eta_i$$ forms a non-increasing sequence and $$|G_r| = k^r$$, we have,\n\n\\[\n\\begin{align*}\n& \\|B_{k,1} - I\\|_2 = \\left\\| \\sum_{r=1}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}) \\right\\|_2 \\\\\n& \\leq \\sum_{r=1}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}) \\\\\n& \\leq \\sum_{r=1}^{k} k^r \\sum_{i=1}^{r} \\eta_i (M + \\lambda_1)^r \\\\\n& \\leq \\sum_{r=1}^{k} k^r r! \\sum_{i=1}^{r} \\eta_i (M + \\lambda_1)^r \\\\\n& \\leq \\sum_{r=1}^{k} k^r r! \\frac{1}{(M + \\lambda_1)^r} \\eta_r \\\\\n& \\leq \\exp(k \\eta_1 (M + \\lambda_1)) - 1 \\\\\n& \\leq k \\eta_1 (M + \\lambda_1) (1 + k \\eta_1 (M + \\lambda_1)) \\text{ using } S.14\n\\end{align*}\n\\]\n\nwhere we have used the assumptions that $$\\|A(s)\\|_2 \\leq (1 + \\epsilon) k \\eta_1 (M + \\lambda_1)$$, $$\\|A(s) - \\Sigma\\| + \\| \\Sigma \\|_2 = (M + \\lambda_1)$$, $$k \\eta_1 (M + \\lambda_1) < 1$$ and the useful result that\n\n\\[\n\\exp(x) \\leq 1 + x + x^2, \\quad x \\in [0, 1.79] \\quad (S.14)\n\\]", "images": [], "items": [{"type": "text", "value": "Lemma $$S.6$$. Given the Markov property in a Markov chain, the reverse Markov property holds, i.e\n\nProof.\n\n\\[\n\\begin{align*}\n& P(Z_t = s | Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n) = P(Z_t = s | Z_{t+1} = w) \\\\\n& = \\frac{P(Z_t = s, Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n)}{P(Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n)} \\\\\n& = \\frac{P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_t = s, Z_{t+1} = w)}{P(Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w)} \\\\\n& = P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w) \\\\\n& = P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w) \\\\\n& = P(Z_t = s, Z_{t+1} = w) \\\\\n& = P(Z_t = s | Z_{t+1} = w)\n\\end{align*}\n\\]\n\n$$S.3.1$$ Proof of Lemma 2\n\nNow we are ready to provide a proof of Lemma 2.\n\nProof of Lemma 2. Without loss of generality, we prove the statement for $$m = 1$$. For convenience of notation, we denote $$k := k_1$$. Note that,\n\n\\[\nB_{k,1} = \\sum_{r=0}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}), \\quad G_r = \\{(i_1, \\ldots, i_r) \\in \\{1, \\ldots, N\\}^r : i_1 < \\ldots < i_r\\}\n\\]\n\nwith the convention that $$\\phi = I$$. Therefore, since $$\\eta_i$$ forms a non-increasing sequence and $$|G_r| = k^r$$, we have,\n\n\\[\n\\begin{align*}\n& \\|B_{k,1} - I\\|_2 = \\left\\| \\sum_{r=1}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}) \\right\\|_2 \\\\\n& \\leq \\sum_{r=1}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}) \\\\\n& \\leq \\sum_{r=1}^{k} k^r \\sum_{i=1}^{r} \\eta_i (M + \\lambda_1)^r \\\\\n& \\leq \\sum_{r=1}^{k} k^r r! \\sum_{i=1}^{r} \\eta_i (M + \\lambda_1)^r \\\\\n& \\leq \\sum_{r=1}^{k} k^r r! \\frac{1}{(M + \\lambda_1)^r} \\eta_r \\\\\n& \\leq \\exp(k \\eta_1 (M + \\lambda_1)) - 1 \\\\\n& \\leq k \\eta_1 (M + \\lambda_1) (1 + k \\eta_1 (M + \\lambda_1)) \\text{ using } S.14\n\\end{align*}\n\\]\n\nwhere we have used the assumptions that $$\\|A(s)\\|_2 \\leq (1 + \\epsilon) k \\eta_1 (M + \\lambda_1)$$, $$\\|A(s) - \\Sigma\\| + \\| \\Sigma \\|_2 = (M + \\lambda_1)$$, $$k \\eta_1 (M + \\lambda_1) < 1$$ and the useful result that\n\n\\[\n\\exp(x) \\leq 1 + x + x^2, \\quad x \\in [0, 1.79] \\quad (S.14)\n\\]", "md": "Lemma $$S.6$$. Given the Markov property in a Markov chain, the reverse Markov property holds, i.e\n\nProof.\n\n\\[\n\\begin{align*}\n& P(Z_t = s | Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n) = P(Z_t = s | Z_{t+1} = w) \\\\\n& = \\frac{P(Z_t = s, Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n)}{P(Z_{t+1} = w, Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n)} \\\\\n& = \\frac{P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_t = s, Z_{t+1} = w)}{P(Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w)} \\\\\n& = P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w) \\\\\n& = P(Z_t = s, Z_{t+1} = w) P(Z_{t+2} = s_{t+2}, \\ldots, Z_n = s_n | Z_{t+1} = w) \\\\\n& = P(Z_t = s, Z_{t+1} = w) \\\\\n& = P(Z_t = s | Z_{t+1} = w)\n\\end{align*}\n\\]\n\n$$S.3.1$$ Proof of Lemma 2\n\nNow we are ready to provide a proof of Lemma 2.\n\nProof of Lemma 2. Without loss of generality, we prove the statement for $$m = 1$$. For convenience of notation, we denote $$k := k_1$$. Note that,\n\n\\[\nB_{k,1} = \\sum_{r=0}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}), \\quad G_r = \\{(i_1, \\ldots, i_r) \\in \\{1, \\ldots, N\\}^r : i_1 < \\ldots < i_r\\}\n\\]\n\nwith the convention that $$\\phi = I$$. Therefore, since $$\\eta_i$$ forms a non-increasing sequence and $$|G_r| = k^r$$, we have,\n\n\\[\n\\begin{align*}\n& \\|B_{k,1} - I\\|_2 = \\left\\| \\sum_{r=1}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}) \\right\\|_2 \\\\\n& \\leq \\sum_{r=1}^{k} \\sum_{(i_1,i_2,\\ldots,i_r) \\in G_r} \\prod_{j=1}^{r} \\eta_{ij} A(s_{ij}) \\\\\n& \\leq \\sum_{r=1}^{k} k^r \\sum_{i=1}^{r} \\eta_i (M + \\lambda_1)^r \\\\\n& \\leq \\sum_{r=1}^{k} k^r r! \\sum_{i=1}^{r} \\eta_i (M + \\lambda_1)^r \\\\\n& \\leq \\sum_{r=1}^{k} k^r r! \\frac{1}{(M + \\lambda_1)^r} \\eta_r \\\\\n& \\leq \\exp(k \\eta_1 (M + \\lambda_1)) - 1 \\\\\n& \\leq k \\eta_1 (M + \\lambda_1) (1 + k \\eta_1 (M + \\lambda_1)) \\text{ using } S.14\n\\end{align*}\n\\]\n\nwhere we have used the assumptions that $$\\|A(s)\\|_2 \\leq (1 + \\epsilon) k \\eta_1 (M + \\lambda_1)$$, $$\\|A(s) - \\Sigma\\| + \\| \\Sigma \\|_2 = (M + \\lambda_1)$$, $$k \\eta_1 (M + \\lambda_1) < 1$$ and the useful result that\n\n\\[\n\\exp(x) \\leq 1 + x + x^2, \\quad x \\in [0, 1.79] \\quad (S.14)\n\\]"}]}, {"page": 17, "text": "This completes the proof for (a).\nFor part (b), we have\n       Bk,1 \u2212         I \u2212       k   \u03b7tA (st)            =        k                             r    \u03b7i j A(sij     )\n                              t=1                    2         r=2    (i1,i2...ir)\u2208Gr        j=1                       2\n                                                        \u2264      k                               r    \u03b7ij  A(sij     )\n                                                             r=2    (i1,i2...ir)\u2208Gr          j=2                       2\n                                                        \u2264      k     k r           r   \u03b7i      (M + \u03bb1)r\n                                                             r=2                 i=2\n                                                        \u2264      k    kr         r    \u03b7i     (M + \u03bb1)r\n                                                             r=2     r!      i=2\n                                                        \u2264      k    kr     1 (M + \u03bb1)r\n                                                             r=2     r! \u03b7r\n                                                        \u2264    exp (k\u03b71 (M + \u03bb1)) \u2212                     1 \u2212    k\u03b71 (M + \u03bb1)\n                                                        \u2264    k2\u03b72  1 (M + \u03bb1)2 using S.14 along with k\u03b71 (M + \u03bb1) < 1                 (S.15)\nwhich completes the proof.\nS.3.2           Proof of Lemma 3\nBefore proving Lemma 3, we will need the following lemma.\nLemma S.7. For arbitrary matrices Mi \u2208                                    Rd\u00d7d, i \u2208          [n] and Q \u2208           Rn\u00d7n, we have\nwhere \u2225.\u22252 denotes the spectral norm.    x,y\u2208[n]     Q (x, y) MxM T            y    2  \u2264    \u2225Q\u22252        x\u2208[n]    MxM T      x    2\nProof. Define matrix X \u2208                       Rd\u00d7nd as X :=                M1         M2        . . .     Mn      . We note that\n                        \u2225X\u22252 =              \u03bbmax (XXT )\uf8eb                         \uf8f6\n                                   =     \u03bbmax          \uf8ed  x\u2208[n]    MxM T      x  \uf8f8\nThen, we have,                     =           x\u2208[n]    MxM T     x     2  since     x\u2208[n]    MxM T      x is a symmetric matrix\n     x,y\u2208[n]     Q (x, y) MxM T           y = X (Q \u2297              Id\u00d7d) XT , where \u2297                   denotes the kronecker product\n                                               \u2264   \u2225X\u22252    2 \u2225Q \u2297        Id\u00d7d\u22252 using submultiplicativity of the spectral norm\nwhich completes our proof.                     = \u2225X\u22252      2 \u2225Q\u22252 since \u2225A \u2297                   B\u22252 = \u2225A\u22252\u2225B\u22252\n                                                                                   17", "md": "This completes the proof for (a).\n\nFor part (b), we have\n\n$$\n\\begin{align*}\n&B_{k,1} - \\sum_{t=1}^{k} I - \\sum_{r=2}^{k} \\eta_t A(s_t) \\\\\n&= \\sum_{r=2}^{k} \\left( \\sum_{(i_1,i_2...i_r) \\in G_r} \\sum_{j=1}^{2} \\right) \\eta_{ij} A(s_{ij}) \\\\\n&\\leq \\sum_{r=2}^{k} \\left( \\sum_{(i_1,i_2...i_r) \\in G_r} \\sum_{j=2}^{2} \\right) \\eta_{ij} A(s_{ij}) \\\\\n&\\leq \\sum_{r=2}^{k} \\sum_{i=2}^{k} r \\eta_i (M + \\lambda_1)^r \\\\\n&\\leq \\sum_{r=2}^{k} \\frac{k!}{r!} \\sum_{i=2}^{k} r \\eta_i (M + \\lambda_1)^r \\\\\n&\\leq \\sum_{r=2}^{k} \\frac{k!}{r!} \\eta_r (M + \\lambda_1)^r \\\\\n&\\leq \\exp(k\\eta_1(M + \\lambda_1)) - \\frac{1 - k\\eta_1(M + \\lambda_1)}{k^2\\eta_2(M + \\lambda_1)^2} \\text{ using S.14 along with } k\\eta_1(M + \\lambda_1) < 1 \\text{ (S.15)}\n\\end{align*}\n$$\nwhich completes the proof.\n\nS.3.2 Proof of Lemma 3\n\nBefore proving Lemma 3, we will need the following lemma.\n\nLemma S.7. For arbitrary matrices $M_i \\in \\mathbb{R}^{d \\times d}, i \\in [n]$ and $Q \\in \\mathbb{R}^{n \\times n}$, we have\n\n$$\n\\|Q(x, y) M_xM_y^T\\|_2 \\leq \\|Q\\|_2 \\|M_xM_y^T\\|_2 \\quad \\forall x,y \\in [n]\n$$\nProof. Define matrix $X \\in \\mathbb{R}^{d \\times nd}$ as $X := [M_1 \\ M_2 \\ ... \\ M_n]$. We note that\n\n$$\n\\|X\\|_2 = \\lambda_{\\text{max}}(XX^T) = \\lambda_{\\text{max}} \\left( \\sum_{x \\in [n]} M_xM_x^T \\right)\n$$\nThen, we have\n\n$$\n\\begin{align*}\n&= \\sum_{x \\in [n]} \\|M_xM_x^T\\|_2 \\text{ since } \\sum_{x \\in [n]} M_xM_x^T \\text{ is a symmetric matrix} \\\\\n&= \\sum_{x,y \\in [n]} Q(x, y) M_xM_y^T = X(Q \\otimes \\text{Id}_{d \\times d})X^T \\text{, where } \\otimes \\text{ denotes the kronecker product} \\\\\n&\\leq \\|X\\|_2 \\|Q \\otimes \\text{Id}_{d \\times d}\\|_2 \\text{ using submultiplicativity of the spectral norm} \\\\\n&= \\|X\\|_2 \\|Q\\|_2 \\text{ since } \\|A \\otimes B\\|_2 = \\|A\\|_2\\|B\\|_2\n\\end{align*}\n$$\nwhich completes our proof.", "images": [], "items": [{"type": "text", "value": "This completes the proof for (a).\n\nFor part (b), we have\n\n$$\n\\begin{align*}\n&B_{k,1} - \\sum_{t=1}^{k} I - \\sum_{r=2}^{k} \\eta_t A(s_t) \\\\\n&= \\sum_{r=2}^{k} \\left( \\sum_{(i_1,i_2...i_r) \\in G_r} \\sum_{j=1}^{2} \\right) \\eta_{ij} A(s_{ij}) \\\\\n&\\leq \\sum_{r=2}^{k} \\left( \\sum_{(i_1,i_2...i_r) \\in G_r} \\sum_{j=2}^{2} \\right) \\eta_{ij} A(s_{ij}) \\\\\n&\\leq \\sum_{r=2}^{k} \\sum_{i=2}^{k} r \\eta_i (M + \\lambda_1)^r \\\\\n&\\leq \\sum_{r=2}^{k} \\frac{k!}{r!} \\sum_{i=2}^{k} r \\eta_i (M + \\lambda_1)^r \\\\\n&\\leq \\sum_{r=2}^{k} \\frac{k!}{r!} \\eta_r (M + \\lambda_1)^r \\\\\n&\\leq \\exp(k\\eta_1(M + \\lambda_1)) - \\frac{1 - k\\eta_1(M + \\lambda_1)}{k^2\\eta_2(M + \\lambda_1)^2} \\text{ using S.14 along with } k\\eta_1(M + \\lambda_1) < 1 \\text{ (S.15)}\n\\end{align*}\n$$\nwhich completes the proof.\n\nS.3.2 Proof of Lemma 3\n\nBefore proving Lemma 3, we will need the following lemma.\n\nLemma S.7. For arbitrary matrices $M_i \\in \\mathbb{R}^{d \\times d}, i \\in [n]$ and $Q \\in \\mathbb{R}^{n \\times n}$, we have\n\n$$\n\\|Q(x, y) M_xM_y^T\\|_2 \\leq \\|Q\\|_2 \\|M_xM_y^T\\|_2 \\quad \\forall x,y \\in [n]\n$$\nProof. Define matrix $X \\in \\mathbb{R}^{d \\times nd}$ as $X := [M_1 \\ M_2 \\ ... \\ M_n]$. We note that\n\n$$\n\\|X\\|_2 = \\lambda_{\\text{max}}(XX^T) = \\lambda_{\\text{max}} \\left( \\sum_{x \\in [n]} M_xM_x^T \\right)\n$$\nThen, we have\n\n$$\n\\begin{align*}\n&= \\sum_{x \\in [n]} \\|M_xM_x^T\\|_2 \\text{ since } \\sum_{x \\in [n]} M_xM_x^T \\text{ is a symmetric matrix} \\\\\n&= \\sum_{x,y \\in [n]} Q(x, y) M_xM_y^T = X(Q \\otimes \\text{Id}_{d \\times d})X^T \\text{, where } \\otimes \\text{ denotes the kronecker product} \\\\\n&\\leq \\|X\\|_2 \\|Q \\otimes \\text{Id}_{d \\times d}\\|_2 \\text{ using submultiplicativity of the spectral norm} \\\\\n&= \\|X\\|_2 \\|Q\\|_2 \\text{ since } \\|A \\otimes B\\|_2 = \\|A\\|_2\\|B\\|_2\n\\end{align*}\n$$\nwhich completes our proof.", "md": "This completes the proof for (a).\n\nFor part (b), we have\n\n$$\n\\begin{align*}\n&B_{k,1} - \\sum_{t=1}^{k} I - \\sum_{r=2}^{k} \\eta_t A(s_t) \\\\\n&= \\sum_{r=2}^{k} \\left( \\sum_{(i_1,i_2...i_r) \\in G_r} \\sum_{j=1}^{2} \\right) \\eta_{ij} A(s_{ij}) \\\\\n&\\leq \\sum_{r=2}^{k} \\left( \\sum_{(i_1,i_2...i_r) \\in G_r} \\sum_{j=2}^{2} \\right) \\eta_{ij} A(s_{ij}) \\\\\n&\\leq \\sum_{r=2}^{k} \\sum_{i=2}^{k} r \\eta_i (M + \\lambda_1)^r \\\\\n&\\leq \\sum_{r=2}^{k} \\frac{k!}{r!} \\sum_{i=2}^{k} r \\eta_i (M + \\lambda_1)^r \\\\\n&\\leq \\sum_{r=2}^{k} \\frac{k!}{r!} \\eta_r (M + \\lambda_1)^r \\\\\n&\\leq \\exp(k\\eta_1(M + \\lambda_1)) - \\frac{1 - k\\eta_1(M + \\lambda_1)}{k^2\\eta_2(M + \\lambda_1)^2} \\text{ using S.14 along with } k\\eta_1(M + \\lambda_1) < 1 \\text{ (S.15)}\n\\end{align*}\n$$\nwhich completes the proof.\n\nS.3.2 Proof of Lemma 3\n\nBefore proving Lemma 3, we will need the following lemma.\n\nLemma S.7. For arbitrary matrices $M_i \\in \\mathbb{R}^{d \\times d}, i \\in [n]$ and $Q \\in \\mathbb{R}^{n \\times n}$, we have\n\n$$\n\\|Q(x, y) M_xM_y^T\\|_2 \\leq \\|Q\\|_2 \\|M_xM_y^T\\|_2 \\quad \\forall x,y \\in [n]\n$$\nProof. Define matrix $X \\in \\mathbb{R}^{d \\times nd}$ as $X := [M_1 \\ M_2 \\ ... \\ M_n]$. We note that\n\n$$\n\\|X\\|_2 = \\lambda_{\\text{max}}(XX^T) = \\lambda_{\\text{max}} \\left( \\sum_{x \\in [n]} M_xM_x^T \\right)\n$$\nThen, we have\n\n$$\n\\begin{align*}\n&= \\sum_{x \\in [n]} \\|M_xM_x^T\\|_2 \\text{ since } \\sum_{x \\in [n]} M_xM_x^T \\text{ is a symmetric matrix} \\\\\n&= \\sum_{x,y \\in [n]} Q(x, y) M_xM_y^T = X(Q \\otimes \\text{Id}_{d \\times d})X^T \\text{, where } \\otimes \\text{ denotes the kronecker product} \\\\\n&\\leq \\|X\\|_2 \\|Q \\otimes \\text{Id}_{d \\times d}\\|_2 \\text{ using submultiplicativity of the spectral norm} \\\\\n&= \\|X\\|_2 \\|Q\\|_2 \\text{ since } \\|A \\otimes B\\|_2 = \\|A\\|_2\\|B\\|_2\n\\end{align*}\n$$\nwhich completes our proof."}]}, {"page": 18, "text": "Proof of Lemma 3. We denote ki := k for convenience of notation. By using reversibility (see S.12),\nwe know that the time-reversed process is also a Markov chain with the same transition probabilities.\nThen, for i < j \u2264        i + k and any m,\n           P(si = s, sj = t|si+k = u) = P(si = s|sj = t)P(sj = t|si+k = u)\n                                                  (i)\n                                                  = P j\u2212i(t, s)P i+k\u2212j(u, t)\n                                                  = P(sm = s|sm\u2212j+i = t)P(sm\u2212j+i = t|sm\u2212k = u)\nStep (i) uses reversibility. Therefore,           = P(sm = s, sm\u2212j+i = t|sm\u2212k = u)                                          (S.16)\nE [(A (si) \u2212      \u03a3) SA (sj) |si+k, . . . sn] =         s,t   \u03a3s + \u00b5s\u00b5T    s \u2212   \u03a3    S   \u03a3t + \u00b5t\u00b5T    t   P(si = s, sj = t|si+k, . . . sn)\n                          using Lemma S.6 =             s,t   \u03a3s + \u00b5s\u00b5T    s \u2212   \u03a3    S   \u03a3t + \u00b5t\u00b5T    t   P(si = s, sj = t|si+k)\n                               using Eq S.16 =          s,t   \u03a3s + \u00b5s\u00b5T    s \u2212   \u03a3    S   \u03a3t + \u00b5t\u00b5T    t   P(sm = s, sm\u2212j+i = t|sm\u2212k = u)\n                                                   = E [(A (sm) \u2212         \u03a3) SA (sm\u2212j+i) |sm\u2212k]\n                                                   = E [(A (sj) \u2212        \u03a3) SA (si) |sj\u2212k] setting m := j\nTherefore, without loss of generality, we proceed with the second form.\n       \u2225E [(A (sj) \u2212       \u03a3) SA (si) |sj\u2212k = x0]\u22252\n        \u2264  \u2225E [(A (sj) \u2212       \u03a3) S\u03a3|sj\u2212k = x0]\u22252          + \u2225E [(A (sj) \u2212        \u03a3) S (A (si) \u2212      \u03a3) |sj\u2212k = x0]\u22252\n                                  T1                                                         T2\n    T1 := \u2225E [(A (sj) \u2212         \u03a3) S\u03a3|sj\u2212k = x0]\u22252\n        =   E    ED(sj) [(A (sj) \u2212        \u03a3)] |sj\u2212k = x0        S\u03a3     2\n        =   E       \u03a3s j + \u00b5sj\u00b5T   sj \u2212   \u03a3    |sj\u2212k = x0    S\u03a3 S\u03a3     2\n        =   s\u2208\u2126    P k(s  j\u2212k, s)    \u03a3s + \u00b5s\u00b5T    s \u2212    \u03a3         2       + E\u03c0       \u03a3s + \u00b5s\u00b5T\n        \u2264    s\u2208\u2126    P k(sj\u2212k, s) \u2212       \u03c0 (s)     \u03a3s + \u00b5s\u00b5T    s \u2212   \u03a3                      =0    s \u2212   \u03a3      2 \u2225S\u22252 \u2225\u03a3\u22252\n        = \u03bb1 \u2225S\u22252          s\u2208\u2126    P k(sj\u2212k, s) \u2212       \u03c0 (s)     \u03a3s + \u00b5s\u00b5T    s \u2212   \u03a3     2\n        \u2264  \u03bb1 \u2225S\u22252 M       s\u2208\u2126    P k(sj\u2212k, s) \u2212      \u03c0 (s)\n        \u2264  2\u03bb1 \u2225S\u22252 Mdmix (ki+1)\n        \u2264  2\u03b72 i M\u03bb1 \u2225S\u22252                                                                                                   (S.17)\n                                                                 18", "md": "Proof of Lemma 3. We denote \\(k_i := k\\) for convenience of notation. By using reversibility (see S.12), we know that the time-reversed process is also a Markov chain with the same transition probabilities. Then, for \\(i < j \\leq i + k\\) and any \\(m\\),\n\n$$\n\\begin{aligned}\n& P(s_i = s, s_j = t | s_{i+k} = u) \\\\\n& = P(s_i = s | s_j = t) P(s_j = t | s_{i+k} = u) \\quad \\text{(i)} \\\\\n& = P_{j-i}(t, s) P_{i+k-j}(u, t) \\\\\n& = P(s_m = s | s_{m-j+i} = t) P(s_{m-j+i} = t | s_{m-k} = u)\n\\end{aligned}\n$$\n\nStep (i) uses reversibility. Therefore,\n\n$$\n\\begin{aligned}\n& = P(s_m = s, s_{m-j+i} = t | s_{m-k} = u) \\quad \\text{(S.16)}\n\\end{aligned}\n$$\n\n\\[\nE \\left[ (A(s_i) - \\Sigma) S A(s_j) | s_{i+k}, \\ldots, s_n \\right] = \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_i = s, s_j = t | s_{i+k}, \\ldots, s_n)\n\\]\n\nusing Lemma S.6\n\n\\[\n= \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_i = s, s_j = t | s_i+k)\n\\]\n\nusing Eq S.16\n\n\\[\n= \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_m = s, s_{m-j+i} = t | s_{m-k} = u)\n\\]\n\n\\[\n= E \\left[ (A(s_m) - \\Sigma) S A(s_{m-j+i}) | s_{m-k} \\right]\n\\]\n\n\\[\n= E \\left[ (A(s_j) - \\Sigma) S A(s_i) | s_{j-k} \\right] \\quad \\text{setting } m := j\n\\]\n\nTherefore, without loss of generality, we proceed with the second form.\n\n\\[\n\\|E[(A(s_j) - \\Sigma) S A(s_i) | s_{j-k} = x_0]\\|_2 \\leq \\|E[(A(s_j) - \\Sigma) S\\Sigma | s_{j-k} = x_0]\\|_2 + \\|E[(A(s_j) - \\Sigma) S (A(s_i) - \\Sigma) | s_{j-k} = x_0]\\|_2\n\\]\n\n\\(T_1 := \\|E[(A(s_j) - \\Sigma) S\\Sigma | s_{j-k} = x_0]\\|_2\\)\n\n\\[\n\\begin{aligned}\n& = E \\left[ \\mathbb{E}(s_j) [(A(s_j) - \\Sigma)] | s_{j-k} = x_0 \\right] S\\Sigma^2 \\\\\n& = E \\left[ \\Sigma_{s_j} + \\mu_{s_j} \\mu_{s_j}^T - \\Sigma | s_{j-k} = x_0 \\right] S\\Sigma^2 \\\\\n& = \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 + E_\\pi (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 \\\\\n& \\leq \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 = 0 - \\Sigma^2 \\|S\\|^2 \\| \\Sigma \\| ^2 \\\\\n& = \\lambda_1 \\|S\\|^2 \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 \\\\\n& \\leq \\lambda_1 \\|S\\|^2 M \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) \\\\\n& \\leq 2\\lambda_1 \\|S\\|^2 M \\text{d}_{\\text{mix}}(k_i+1) \\\\\n& \\leq 2\\eta^2 i M \\lambda_1 \\|S\\|^2 \\quad \\text{(S.17)}\n\\end{aligned}\n\\]", "images": [], "items": [{"type": "text", "value": "Proof of Lemma 3. We denote \\(k_i := k\\) for convenience of notation. By using reversibility (see S.12), we know that the time-reversed process is also a Markov chain with the same transition probabilities. Then, for \\(i < j \\leq i + k\\) and any \\(m\\),\n\n$$\n\\begin{aligned}\n& P(s_i = s, s_j = t | s_{i+k} = u) \\\\\n& = P(s_i = s | s_j = t) P(s_j = t | s_{i+k} = u) \\quad \\text{(i)} \\\\\n& = P_{j-i}(t, s) P_{i+k-j}(u, t) \\\\\n& = P(s_m = s | s_{m-j+i} = t) P(s_{m-j+i} = t | s_{m-k} = u)\n\\end{aligned}\n$$\n\nStep (i) uses reversibility. Therefore,\n\n$$\n\\begin{aligned}\n& = P(s_m = s, s_{m-j+i} = t | s_{m-k} = u) \\quad \\text{(S.16)}\n\\end{aligned}\n$$\n\n\\[\nE \\left[ (A(s_i) - \\Sigma) S A(s_j) | s_{i+k}, \\ldots, s_n \\right] = \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_i = s, s_j = t | s_{i+k}, \\ldots, s_n)\n\\]\n\nusing Lemma S.6\n\n\\[\n= \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_i = s, s_j = t | s_i+k)\n\\]\n\nusing Eq S.16\n\n\\[\n= \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_m = s, s_{m-j+i} = t | s_{m-k} = u)\n\\]\n\n\\[\n= E \\left[ (A(s_m) - \\Sigma) S A(s_{m-j+i}) | s_{m-k} \\right]\n\\]\n\n\\[\n= E \\left[ (A(s_j) - \\Sigma) S A(s_i) | s_{j-k} \\right] \\quad \\text{setting } m := j\n\\]\n\nTherefore, without loss of generality, we proceed with the second form.\n\n\\[\n\\|E[(A(s_j) - \\Sigma) S A(s_i) | s_{j-k} = x_0]\\|_2 \\leq \\|E[(A(s_j) - \\Sigma) S\\Sigma | s_{j-k} = x_0]\\|_2 + \\|E[(A(s_j) - \\Sigma) S (A(s_i) - \\Sigma) | s_{j-k} = x_0]\\|_2\n\\]\n\n\\(T_1 := \\|E[(A(s_j) - \\Sigma) S\\Sigma | s_{j-k} = x_0]\\|_2\\)\n\n\\[\n\\begin{aligned}\n& = E \\left[ \\mathbb{E}(s_j) [(A(s_j) - \\Sigma)] | s_{j-k} = x_0 \\right] S\\Sigma^2 \\\\\n& = E \\left[ \\Sigma_{s_j} + \\mu_{s_j} \\mu_{s_j}^T - \\Sigma | s_{j-k} = x_0 \\right] S\\Sigma^2 \\\\\n& = \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 + E_\\pi (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 \\\\\n& \\leq \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 = 0 - \\Sigma^2 \\|S\\|^2 \\| \\Sigma \\| ^2 \\\\\n& = \\lambda_1 \\|S\\|^2 \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 \\\\\n& \\leq \\lambda_1 \\|S\\|^2 M \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) \\\\\n& \\leq 2\\lambda_1 \\|S\\|^2 M \\text{d}_{\\text{mix}}(k_i+1) \\\\\n& \\leq 2\\eta^2 i M \\lambda_1 \\|S\\|^2 \\quad \\text{(S.17)}\n\\end{aligned}\n\\]", "md": "Proof of Lemma 3. We denote \\(k_i := k\\) for convenience of notation. By using reversibility (see S.12), we know that the time-reversed process is also a Markov chain with the same transition probabilities. Then, for \\(i < j \\leq i + k\\) and any \\(m\\),\n\n$$\n\\begin{aligned}\n& P(s_i = s, s_j = t | s_{i+k} = u) \\\\\n& = P(s_i = s | s_j = t) P(s_j = t | s_{i+k} = u) \\quad \\text{(i)} \\\\\n& = P_{j-i}(t, s) P_{i+k-j}(u, t) \\\\\n& = P(s_m = s | s_{m-j+i} = t) P(s_{m-j+i} = t | s_{m-k} = u)\n\\end{aligned}\n$$\n\nStep (i) uses reversibility. Therefore,\n\n$$\n\\begin{aligned}\n& = P(s_m = s, s_{m-j+i} = t | s_{m-k} = u) \\quad \\text{(S.16)}\n\\end{aligned}\n$$\n\n\\[\nE \\left[ (A(s_i) - \\Sigma) S A(s_j) | s_{i+k}, \\ldots, s_n \\right] = \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_i = s, s_j = t | s_{i+k}, \\ldots, s_n)\n\\]\n\nusing Lemma S.6\n\n\\[\n= \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_i = s, s_j = t | s_i+k)\n\\]\n\nusing Eq S.16\n\n\\[\n= \\sum_{s,t} \\Sigma_s + \\mu_s \\mu_s^T - \\Sigma S \\Sigma_t + \\mu_t \\mu_t^T P(s_m = s, s_{m-j+i} = t | s_{m-k} = u)\n\\]\n\n\\[\n= E \\left[ (A(s_m) - \\Sigma) S A(s_{m-j+i}) | s_{m-k} \\right]\n\\]\n\n\\[\n= E \\left[ (A(s_j) - \\Sigma) S A(s_i) | s_{j-k} \\right] \\quad \\text{setting } m := j\n\\]\n\nTherefore, without loss of generality, we proceed with the second form.\n\n\\[\n\\|E[(A(s_j) - \\Sigma) S A(s_i) | s_{j-k} = x_0]\\|_2 \\leq \\|E[(A(s_j) - \\Sigma) S\\Sigma | s_{j-k} = x_0]\\|_2 + \\|E[(A(s_j) - \\Sigma) S (A(s_i) - \\Sigma) | s_{j-k} = x_0]\\|_2\n\\]\n\n\\(T_1 := \\|E[(A(s_j) - \\Sigma) S\\Sigma | s_{j-k} = x_0]\\|_2\\)\n\n\\[\n\\begin{aligned}\n& = E \\left[ \\mathbb{E}(s_j) [(A(s_j) - \\Sigma)] | s_{j-k} = x_0 \\right] S\\Sigma^2 \\\\\n& = E \\left[ \\Sigma_{s_j} + \\mu_{s_j} \\mu_{s_j}^T - \\Sigma | s_{j-k} = x_0 \\right] S\\Sigma^2 \\\\\n& = \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 + E_\\pi (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 \\\\\n& \\leq \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 = 0 - \\Sigma^2 \\|S\\|^2 \\| \\Sigma \\| ^2 \\\\\n& = \\lambda_1 \\|S\\|^2 \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) (\\Sigma_s + \\mu_s \\mu_s^T - \\Sigma)^2 \\\\\n& \\leq \\lambda_1 \\|S\\|^2 M \\sum_{s \\in \\Omega} P_k(s_{j-k}, s) - \\pi(s) \\\\\n& \\leq 2\\lambda_1 \\|S\\|^2 M \\text{d}_{\\text{mix}}(k_i+1) \\\\\n& \\leq 2\\eta^2 i M \\lambda_1 \\|S\\|^2 \\quad \\text{(S.17)}\n\\end{aligned}\n\\]"}]}, {"page": 19, "text": "T2 = \u2225E [(A (sj) \u2212           \u03a3) S (A (si) \u2212         \u03a3) |sj\u2212k = x0]\u22252\n     =     x,y\u2208\u2126   P (sj = x, si = y|sj\u2212k = x0) ED(x) [A (x) \u2212                        \u03a3] SED(y) [A (y) \u2212           \u03a3]    2 using independence of\n         D (x) and D (y) conditioned on x and y                                               1    1\n     =    x,y\u2208\u2126    P (sj = x, si = y|sj\u2212k = x0)                  \u03a3x + \u00b5x\u00b5T     x \u2212    \u03a3    S  2 S  2   \u03a3y + \u00b5y\u00b5T      y \u2212   \u03a3\n                                                                             Wx                               W Ty                 2\n     =     x,y\u2208\u2126   P (sj = x|si = y) P (si = y|sj\u2212k = x0) WxW T                        y   2   using the Markov property\n     =     x,y\u2208\u2126   P j\u2212i (y, x) P i\u2212j+k (x0, y) WxW T              y    2                         \u03c0 (x) P i\u2212j+k (x0, y) WxW T\n     =     x,y\u2208\u2126     P j\u2212i (y, x) \u2212       \u03c0 (x)     P i\u2212j+k (x0, y) WxW T          y +    x,y\u2208\u2126 \u03c0 (x) Wx            P i\u2212j+k (x0, y) W T  y    2\n     =     x,y\u2208\u2126     P j\u2212i (y, x) \u2212       \u03c0 (x)     P i\u2212j+k (x0, y) WxW T          y +    x\u2208\u2126     =0          y\u2208\u2126                             y   2\n =    x,y\u2208\u2126      P j\u2212i (y, x) \u2212       \u03c0 (x)     P i\u2212j+k (x0, y) WxW T          y    2\n \u2264     x,y\u2208\u2126     P j\u2212i (y, x) \u2212       \u03c0 (x)      P j\u2212i+k (x0, y) \u2212          \u03c0 (y)    WxW T    y    2 +    x,y\u2208\u2126     P j\u2212i (y, x) \u2212       \u03c0 (x)     \u03c0 (y) WxW T  y  2\n                                                  T21                                                                                  T22\n                                                                                                                                      (S.18)\nFor T21, we have,\n                   T21 \u2264     x,y\u2208\u2126     P j\u2212i (y, x) \u2212       \u03c0 (x)     P i\u2212j+k (x0, y) \u2212         \u03c0 (y)      WxW T    y    2\n                         \u2264   \u2225S\u22252 M2        y\u2208\u2126    P i\u2212j+k (x0, y) \u2212         \u03c0 (y)    x\u2208\u2126     P j\u2212i (y, x) \u2212       \u03c0 (x)\n                         \u2264   2 \u2225S\u22252 M2dmix (j \u2212             i) y\u2208\u2126    P i\u2212j+k (x0, y) \u2212         \u03c0 (y)\n                         \u2264   4 \u2225S\u22252 M2dmix (j \u2212             i) dmix (i \u2212    j + k)\n                                               \u2212    j\u2212i     \u2212   i\u2212j+k\n                         \u2264   4 \u2225S\u22252 M22             \u03c4mix  2      \u03c4mix\n                                                    j\u2212i+i\u2212j+k\n                         \u2264   8 \u2225S\u22252 M22        \u2212     k  \u03c4mix        since \u2200a, b \u230aa\u230b        + \u230ab\u230b    \u2265   \u230aa + b\u230b     \u2212  1\n                         \u2264   8 \u2225S\u22252 M22        \u2212    \u03c4mix   \u2264   8 \u2225S\u22252 M2dmix (k) \u2264             8\u03b72 i M2 \u2225S\u22252                          (S.19)\n                                                                      19", "md": "# Math Equations\n\n$$T2 = \\|\\mathbb{E} \\left[(A(s_j) - \\Sigma) S (A(s_i) - \\Sigma) | s_{j-k} = x_0\\right]\\|^2$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x, s_i = y | s_{j-k} = x_0) \\mathbb{E}[D(x) [A(x) - \\Sigma] SED(y) [A(y) - \\Sigma]]^2$$ using independence of\n\n$$D(x)$$ and $$D(y)$$ conditioned on $$x$$ and $$y$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x, s_i = y | s_{j-k} = x_0) (\\Sigma_x + \\mu_x\\mu_x^T)(x - \\Sigma) S^2 (\\Sigma_y + \\mu_y\\mu_y^T)(y - \\Sigma) W_x W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x | s_i = y) P(s_i = y | s_{j-k} = x_0) W_x W^T_y$$ using the Markov property\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) P_{i-j+k}(x0, y) W_x W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y + \\sum_{x,y \\in \\Omega} \\pi(x) W_x P_{i-j+k}(x0, y) W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y + \\sum_{x \\in \\Omega} = 0 \\sum_{y \\in \\Omega} y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y$$\n\n$$\\leq \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{j-i+k}(x0, y) - \\pi(y) W_x W^T_y + \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) \\pi(y) W_x W^T_y$$\n\n$$T21$$\n\nFor $$T21$$, we have,\n\n$$T21 \\leq \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) - \\pi(y) W_x W^T_y$$\n\n$$\\leq \\|\\mathbb{S}\\|^2 M^2 \\sum_{y \\in \\Omega} P_{i-j+k}(x0, y) - \\pi(y) \\sum_{x \\in \\Omega} P_{j-i}(y, x) - \\pi(x)$$\n\n$$\\leq 2 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(j - i) \\sum_{y \\in \\Omega} P_{i-j+k}(x0, y) - \\pi(y)$$\n\n$$\\leq 4 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(j - i) d_{\\text{mix}}(i - j + k)$$\n\n$$\\leq 4 \\|\\mathbb{S}\\|^2 M^2 2 \\tau_{\\text{mix}}^2 \\frac{j-i+i-j+k}{j-i+i-j+k}$$\n\n$$\\leq 8 \\|\\mathbb{S}\\|^2 M^2 - k \\tau_{\\text{mix}} \\leq 8 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(k) \\leq 8\\eta^2 i M^2 \\|\\mathbb{S}\\|^2$$\n\n$$19$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "$$T2 = \\|\\mathbb{E} \\left[(A(s_j) - \\Sigma) S (A(s_i) - \\Sigma) | s_{j-k} = x_0\\right]\\|^2$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x, s_i = y | s_{j-k} = x_0) \\mathbb{E}[D(x) [A(x) - \\Sigma] SED(y) [A(y) - \\Sigma]]^2$$ using independence of\n\n$$D(x)$$ and $$D(y)$$ conditioned on $$x$$ and $$y$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x, s_i = y | s_{j-k} = x_0) (\\Sigma_x + \\mu_x\\mu_x^T)(x - \\Sigma) S^2 (\\Sigma_y + \\mu_y\\mu_y^T)(y - \\Sigma) W_x W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x | s_i = y) P(s_i = y | s_{j-k} = x_0) W_x W^T_y$$ using the Markov property\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) P_{i-j+k}(x0, y) W_x W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y + \\sum_{x,y \\in \\Omega} \\pi(x) W_x P_{i-j+k}(x0, y) W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y + \\sum_{x \\in \\Omega} = 0 \\sum_{y \\in \\Omega} y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y$$\n\n$$\\leq \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{j-i+k}(x0, y) - \\pi(y) W_x W^T_y + \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) \\pi(y) W_x W^T_y$$\n\n$$T21$$\n\nFor $$T21$$, we have,\n\n$$T21 \\leq \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) - \\pi(y) W_x W^T_y$$\n\n$$\\leq \\|\\mathbb{S}\\|^2 M^2 \\sum_{y \\in \\Omega} P_{i-j+k}(x0, y) - \\pi(y) \\sum_{x \\in \\Omega} P_{j-i}(y, x) - \\pi(x)$$\n\n$$\\leq 2 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(j - i) \\sum_{y \\in \\Omega} P_{i-j+k}(x0, y) - \\pi(y)$$\n\n$$\\leq 4 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(j - i) d_{\\text{mix}}(i - j + k)$$\n\n$$\\leq 4 \\|\\mathbb{S}\\|^2 M^2 2 \\tau_{\\text{mix}}^2 \\frac{j-i+i-j+k}{j-i+i-j+k}$$\n\n$$\\leq 8 \\|\\mathbb{S}\\|^2 M^2 - k \\tau_{\\text{mix}} \\leq 8 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(k) \\leq 8\\eta^2 i M^2 \\|\\mathbb{S}\\|^2$$\n\n$$19$$", "md": "$$T2 = \\|\\mathbb{E} \\left[(A(s_j) - \\Sigma) S (A(s_i) - \\Sigma) | s_{j-k} = x_0\\right]\\|^2$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x, s_i = y | s_{j-k} = x_0) \\mathbb{E}[D(x) [A(x) - \\Sigma] SED(y) [A(y) - \\Sigma]]^2$$ using independence of\n\n$$D(x)$$ and $$D(y)$$ conditioned on $$x$$ and $$y$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x, s_i = y | s_{j-k} = x_0) (\\Sigma_x + \\mu_x\\mu_x^T)(x - \\Sigma) S^2 (\\Sigma_y + \\mu_y\\mu_y^T)(y - \\Sigma) W_x W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P(s_j = x | s_i = y) P(s_i = y | s_{j-k} = x_0) W_x W^T_y$$ using the Markov property\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) P_{i-j+k}(x0, y) W_x W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y + \\sum_{x,y \\in \\Omega} \\pi(x) W_x P_{i-j+k}(x0, y) W^T_y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y + \\sum_{x \\in \\Omega} = 0 \\sum_{y \\in \\Omega} y$$\n\n$$= \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) W_x W^T_y$$\n\n$$\\leq \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{j-i+k}(x0, y) - \\pi(y) W_x W^T_y + \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) \\pi(y) W_x W^T_y$$\n\n$$T21$$\n\nFor $$T21$$, we have,\n\n$$T21 \\leq \\sum_{x,y \\in \\Omega} P_{j-i}(y, x) - \\pi(x) P_{i-j+k}(x0, y) - \\pi(y) W_x W^T_y$$\n\n$$\\leq \\|\\mathbb{S}\\|^2 M^2 \\sum_{y \\in \\Omega} P_{i-j+k}(x0, y) - \\pi(y) \\sum_{x \\in \\Omega} P_{j-i}(y, x) - \\pi(x)$$\n\n$$\\leq 2 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(j - i) \\sum_{y \\in \\Omega} P_{i-j+k}(x0, y) - \\pi(y)$$\n\n$$\\leq 4 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(j - i) d_{\\text{mix}}(i - j + k)$$\n\n$$\\leq 4 \\|\\mathbb{S}\\|^2 M^2 2 \\tau_{\\text{mix}}^2 \\frac{j-i+i-j+k}{j-i+i-j+k}$$\n\n$$\\leq 8 \\|\\mathbb{S}\\|^2 M^2 - k \\tau_{\\text{mix}} \\leq 8 \\|\\mathbb{S}\\|^2 M^2 d_{\\text{mix}}(k) \\leq 8\\eta^2 i M^2 \\|\\mathbb{S}\\|^2$$\n\n$$19$$"}]}, {"page": 20, "text": " For T22, we have,\n T22 =        x,y\u2208\u2126      P j\u2212i (y, x) \u2212         \u03c0 (x)      \u03c0 (y) WxW T        y    2\n       =                  P j\u2212i (y, x) \u2212         \u03c0 (x)         \u03c0 (y)           \u03c0 (x)Wx                \u03c0 (y)W T    y\n             x,y\u2208\u2126                     \u03c0 (x)                                                                              2\n                          P j\u2212i (y, x) \u2212         \u03c0 (x)                                                                    1                      1\n       =     x,y\u2208\u2126                     \u03c0 (x)                   \u03c0 (y)           \u03c0 (x)      \u03a3x + \u00b5x\u00b5T       x \u2212    \u03a3     S  2          \u03c0 (y)S      2   \u03a3y + \u00b5y\u00b5T     y \u2212    \u03a3  2\n       (i)\n        \u2264   \u2225Q\u22252      E\u03c0x\u2208\u2126    \u03c0 (x)      \u03a3x + \u00b5x\u00b5T       x \u2212    \u03a3     S    \u03a3x + \u00b5x\u00b5T       x \u2212     \u03a3      2\n       = \u2225Q\u22252                   \u03a3x + \u00b5x\u00b5T        x \u2212    \u03a3    S     \u03a3x2+ \u00b5x\u00b5T       x \u2212    \u03a3        2\n       \u2264    \u2225Q\u22252 \u2225S\u22252          E\u03c0         \u03a3x + \u00b5x\u00b5T       x \u2212    \u03a3          2\n       \u2264    V \u2225Q\u22252 \u2225S\u22252                                                                                                                              (S.20)            1\n Step (i) uses Lemma S.7 with Q(y, x) := (P i\u2212j(y,x)\u2212\u03c0(x))                \u221a   \u03c0(x)               \u03c0 (y) and Mx =                \u03c0 (x)      \u03a3x + \u00b5x\u00b5T        x \u2212  \u03a3   S  2 .\n Let\u2019s now bound \u2225Q\u22252. Let \u03a0 := diag (\u03c0) \u2208                         1     R\u2126\u00d7\u2126       and t := j \u2212          i. Then, we have\n                                                       Q = \u03a0       2   P t \u2212     11T \u03a0       \u03a0\u22121   2\n                                                                   1                     1            1\n                                                           = \u03a0     2 P t\u03a0\u22121    2 \u2212    \u03a02 11T \u03a02\n Now, since we have a reversible Markov chain, \u03a0P = P T \u03a0. Therefore,\n                                                         1                     1\n                                                      \u03a0  2 P   \u03a0\u22121  2 = \u03a02 \u03a0\u22121P T \u03a0\u03a0\u22121                 2\n                                                                        = \u03a0\u22121     2 P T \u03a02   1\n                                                                                    1\nTherefore, P is similar to the self-adjoint matrix \u03a0                                2 P  \u03a0\u22121   2 and their eigenvalues are real and the\n                                          1                                                      1\n same. Further note that \u03a0                2 1 is the leading eigenvector of \u03a02 P                      \u03a0\u22121   2 with eigenvalue 1 since\n                                        1                1            1\n                                     \u03a0  2 P  \u03a0\u22121   2 \u03a02 1 = \u03a02 P      1    1\n                                                              = \u03a0     2 1 since P is a stochastic matrix\n Now,\n                                                                  1                      1            1\n                                              \u2225Q\u22252 =          \u03a0   2 P t\u03a0\u22121     2 \u2212 t  \u03a02 11T \u03a02           2\n                                                                     1                        1            1\n                                                        =        \u03a0   2 P 1\u03a0\u22121   2     \u2212 t \u03a0   2 11T \u03a02        2\n                                                        \u2264     \u03bb2     \u03a0   2 P  \u03a0\u22121   2\n                                                        = |\u03bb2 (P       )|t\nwhere |\u03bb2 (.) | denotes the second-largest eigenvalue in magnitude. Therefore, using S.17, S.19 and\n S.20, we have\n            E [(A (si) \u2212        \u03a3) SA (sj) |si+k, ...sn] \u2264                   |\u03bb2 (P     )|j\u2212i V + 8\u03b72       i M2 + 2\u03b72       i M\u03bb1         \u2225S\u22252\n Hence proved.                                                         \u2264     |\u03bb2 (P     )|j\u2212i V + 8\u03b72       i M (M + \u03bb1)               \u2225S\u22252\n                                                                              20", "md": "# Math Equations\n\nFor T22, we have,\n\n$$\nT22 = \\sum_{x,y\\in\\Omega} P_{j-i}(y, x) - \\frac{\\pi(x)}{\\pi(y)} W_x W^T y^2\n$$\n$$\n= \\sum_{x,y\\in\\Omega} P_{j-i}(y, x) - \\frac{\\pi(x)}{\\pi(y)} \\frac{\\pi(x)W_x}{\\pi(y)W^T y}\n$$\n$$\n= \\sum_{x,y\\in\\Omega} \\frac{P_{j-i}(y, x) - \\pi(x)}{\\pi(x)} \\pi(y) \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma S^2 \\right) \\pi(y)S^2 \\Sigma_y + \\mu_y\\mu_y^T - \\Sigma^2\n$$\n$$\n\\leq \\|Q\\|_2 E_{\\pi_x\\in\\Omega} \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma S \\right) \\Sigma_x^2 + \\mu_x\\mu_x^T - \\Sigma^2\n$$\n$$\n\\leq \\|Q\\|_2 \\|\\Sigma\\|_2 E_{\\pi} \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma^2 \\right)\n$$\n$$\n\\leq V \\|Q\\|_2 \\|\\Sigma\\|_2 \\quad \\text{(S.20)}\n$$\nStep (i) uses Lemma S.7 with $Q(y, x) := (P_{i-j}(y,x)-\\pi(x))\\sqrt{\\pi(x)}\\frac{\\pi(y)}{\\pi(x)}$ and $M_x = \\pi(x) \\Sigma_x + \\mu_x\\mu_x^T x - \\Sigma S^2$.\n\nLet\u2019s now bound $\\|Q\\|_2$. Let $\\Pi := \\text{diag}(\\pi) \\in \\mathbb{R}^{\\Omega\\times\\Omega}$ and $t := j - i$. Then, we have\n\n$$\nQ = \\Pi^2 P_t - 11^T \\Pi \\Pi^{-1}^2\n$$\n$$\n= \\Pi^2 P_t \\Pi^{-1}^2 - \\Pi^2 11^T \\Pi^2\n$$\nNow, since we have a reversible Markov chain, $\\Pi P = P^T \\Pi$. Therefore,\n\n$$\n\\Pi^2 P \\Pi^{-1}^2 = \\Pi^2 \\Pi^{-1} P^T \\Pi \\Pi^{-1}^2\n$$\n$$\n= \\Pi^{-1}^2 P^T \\Pi^2\n$$\nTherefore, $P$ is similar to the self-adjoint matrix $\\Pi^2 P \\Pi^{-1}^2$ and their eigenvalues are real and the same. Further note that $\\Pi^2 1$ is the leading eigenvector of $\\Pi^2 P \\Pi^{-1}^2$ with eigenvalue 1 since $\\Pi^2 P 1 = \\Pi^2 P 1$ since $P$ is a stochastic matrix.\n\nNow,\n\n$$\n\\|Q\\|_2 = \\Pi^2 P_t \\Pi^{-1}^2 - t \\Pi^2 11^T \\Pi^2\n$$\n$$\n= \\Pi^2 P 1 \\Pi^{-1}^2 - t \\Pi^2 11^T \\Pi^2\n$$\n$$\n\\leq \\lambda_2 \\Pi^2 P \\Pi^{-1}^2\n$$\n$$\n= |\\lambda_2(P)|^t\n$$\nwhere $|\\lambda_2(.)|$ denotes the second-largest eigenvalue in magnitude. Therefore, using S.17, S.19 and S.20, we have\n\n$$\nE \\left[ (A(s_i) - \\Sigma) S A(s_j) | s_{i+k}, ..., s_n \\right] \\leq | \\lambda_2(P) |^{j-i} V + 8\\eta_i^2 M^2 + 2\\eta_i^2 M\\lambda_1 \\|\\Sigma\\|_2\n$$\nHence proved.\n\n$$\n\\leq | \\lambda_2(P) |^{j-i} V + 8\\eta_i^2 M (M + \\lambda_1) \\|\\Sigma\\|_2 \\quad (20)\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "For T22, we have,\n\n$$\nT22 = \\sum_{x,y\\in\\Omega} P_{j-i}(y, x) - \\frac{\\pi(x)}{\\pi(y)} W_x W^T y^2\n$$\n$$\n= \\sum_{x,y\\in\\Omega} P_{j-i}(y, x) - \\frac{\\pi(x)}{\\pi(y)} \\frac{\\pi(x)W_x}{\\pi(y)W^T y}\n$$\n$$\n= \\sum_{x,y\\in\\Omega} \\frac{P_{j-i}(y, x) - \\pi(x)}{\\pi(x)} \\pi(y) \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma S^2 \\right) \\pi(y)S^2 \\Sigma_y + \\mu_y\\mu_y^T - \\Sigma^2\n$$\n$$\n\\leq \\|Q\\|_2 E_{\\pi_x\\in\\Omega} \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma S \\right) \\Sigma_x^2 + \\mu_x\\mu_x^T - \\Sigma^2\n$$\n$$\n\\leq \\|Q\\|_2 \\|\\Sigma\\|_2 E_{\\pi} \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma^2 \\right)\n$$\n$$\n\\leq V \\|Q\\|_2 \\|\\Sigma\\|_2 \\quad \\text{(S.20)}\n$$\nStep (i) uses Lemma S.7 with $Q(y, x) := (P_{i-j}(y,x)-\\pi(x))\\sqrt{\\pi(x)}\\frac{\\pi(y)}{\\pi(x)}$ and $M_x = \\pi(x) \\Sigma_x + \\mu_x\\mu_x^T x - \\Sigma S^2$.\n\nLet\u2019s now bound $\\|Q\\|_2$. Let $\\Pi := \\text{diag}(\\pi) \\in \\mathbb{R}^{\\Omega\\times\\Omega}$ and $t := j - i$. Then, we have\n\n$$\nQ = \\Pi^2 P_t - 11^T \\Pi \\Pi^{-1}^2\n$$\n$$\n= \\Pi^2 P_t \\Pi^{-1}^2 - \\Pi^2 11^T \\Pi^2\n$$\nNow, since we have a reversible Markov chain, $\\Pi P = P^T \\Pi$. Therefore,\n\n$$\n\\Pi^2 P \\Pi^{-1}^2 = \\Pi^2 \\Pi^{-1} P^T \\Pi \\Pi^{-1}^2\n$$\n$$\n= \\Pi^{-1}^2 P^T \\Pi^2\n$$\nTherefore, $P$ is similar to the self-adjoint matrix $\\Pi^2 P \\Pi^{-1}^2$ and their eigenvalues are real and the same. Further note that $\\Pi^2 1$ is the leading eigenvector of $\\Pi^2 P \\Pi^{-1}^2$ with eigenvalue 1 since $\\Pi^2 P 1 = \\Pi^2 P 1$ since $P$ is a stochastic matrix.\n\nNow,\n\n$$\n\\|Q\\|_2 = \\Pi^2 P_t \\Pi^{-1}^2 - t \\Pi^2 11^T \\Pi^2\n$$\n$$\n= \\Pi^2 P 1 \\Pi^{-1}^2 - t \\Pi^2 11^T \\Pi^2\n$$\n$$\n\\leq \\lambda_2 \\Pi^2 P \\Pi^{-1}^2\n$$\n$$\n= |\\lambda_2(P)|^t\n$$\nwhere $|\\lambda_2(.)|$ denotes the second-largest eigenvalue in magnitude. Therefore, using S.17, S.19 and S.20, we have\n\n$$\nE \\left[ (A(s_i) - \\Sigma) S A(s_j) | s_{i+k}, ..., s_n \\right] \\leq | \\lambda_2(P) |^{j-i} V + 8\\eta_i^2 M^2 + 2\\eta_i^2 M\\lambda_1 \\|\\Sigma\\|_2\n$$\nHence proved.\n\n$$\n\\leq | \\lambda_2(P) |^{j-i} V + 8\\eta_i^2 M (M + \\lambda_1) \\|\\Sigma\\|_2 \\quad (20)\n$$", "md": "For T22, we have,\n\n$$\nT22 = \\sum_{x,y\\in\\Omega} P_{j-i}(y, x) - \\frac{\\pi(x)}{\\pi(y)} W_x W^T y^2\n$$\n$$\n= \\sum_{x,y\\in\\Omega} P_{j-i}(y, x) - \\frac{\\pi(x)}{\\pi(y)} \\frac{\\pi(x)W_x}{\\pi(y)W^T y}\n$$\n$$\n= \\sum_{x,y\\in\\Omega} \\frac{P_{j-i}(y, x) - \\pi(x)}{\\pi(x)} \\pi(y) \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma S^2 \\right) \\pi(y)S^2 \\Sigma_y + \\mu_y\\mu_y^T - \\Sigma^2\n$$\n$$\n\\leq \\|Q\\|_2 E_{\\pi_x\\in\\Omega} \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma S \\right) \\Sigma_x^2 + \\mu_x\\mu_x^T - \\Sigma^2\n$$\n$$\n\\leq \\|Q\\|_2 \\|\\Sigma\\|_2 E_{\\pi} \\left( \\Sigma_x + \\mu_x\\mu_x^T - \\Sigma^2 \\right)\n$$\n$$\n\\leq V \\|Q\\|_2 \\|\\Sigma\\|_2 \\quad \\text{(S.20)}\n$$\nStep (i) uses Lemma S.7 with $Q(y, x) := (P_{i-j}(y,x)-\\pi(x))\\sqrt{\\pi(x)}\\frac{\\pi(y)}{\\pi(x)}$ and $M_x = \\pi(x) \\Sigma_x + \\mu_x\\mu_x^T x - \\Sigma S^2$.\n\nLet\u2019s now bound $\\|Q\\|_2$. Let $\\Pi := \\text{diag}(\\pi) \\in \\mathbb{R}^{\\Omega\\times\\Omega}$ and $t := j - i$. Then, we have\n\n$$\nQ = \\Pi^2 P_t - 11^T \\Pi \\Pi^{-1}^2\n$$\n$$\n= \\Pi^2 P_t \\Pi^{-1}^2 - \\Pi^2 11^T \\Pi^2\n$$\nNow, since we have a reversible Markov chain, $\\Pi P = P^T \\Pi$. Therefore,\n\n$$\n\\Pi^2 P \\Pi^{-1}^2 = \\Pi^2 \\Pi^{-1} P^T \\Pi \\Pi^{-1}^2\n$$\n$$\n= \\Pi^{-1}^2 P^T \\Pi^2\n$$\nTherefore, $P$ is similar to the self-adjoint matrix $\\Pi^2 P \\Pi^{-1}^2$ and their eigenvalues are real and the same. Further note that $\\Pi^2 1$ is the leading eigenvector of $\\Pi^2 P \\Pi^{-1}^2$ with eigenvalue 1 since $\\Pi^2 P 1 = \\Pi^2 P 1$ since $P$ is a stochastic matrix.\n\nNow,\n\n$$\n\\|Q\\|_2 = \\Pi^2 P_t \\Pi^{-1}^2 - t \\Pi^2 11^T \\Pi^2\n$$\n$$\n= \\Pi^2 P 1 \\Pi^{-1}^2 - t \\Pi^2 11^T \\Pi^2\n$$\n$$\n\\leq \\lambda_2 \\Pi^2 P \\Pi^{-1}^2\n$$\n$$\n= |\\lambda_2(P)|^t\n$$\nwhere $|\\lambda_2(.)|$ denotes the second-largest eigenvalue in magnitude. Therefore, using S.17, S.19 and S.20, we have\n\n$$\nE \\left[ (A(s_i) - \\Sigma) S A(s_j) | s_{i+k}, ..., s_n \\right] \\leq | \\lambda_2(P) |^{j-i} V + 8\\eta_i^2 M^2 + 2\\eta_i^2 M\\lambda_1 \\|\\Sigma\\|_2\n$$\nHence proved.\n\n$$\n\\leq | \\lambda_2(P) |^{j-i} V + 8\\eta_i^2 M (M + \\lambda_1) \\|\\Sigma\\|_2 \\quad (20)\n$$"}]}, {"page": 21, "text": "Lemma S.8. Let \u2200i \u2208              [n], \u03b7iki (M + \u03bb1) \u2264            \u03f5, \u03f5 \u2208   (0, 1) and \u03b7i forms a non-increasing sequence.\nSet ki := \u03c4mix         \u03b3\u03b72 i   , \u03b3 \u2208    (0, 1]. Then for constant matrix U \u2208                      Rd\u00d7d\u2032, and constant positive\nsemi-definite matrix G \u2208            Rd\u00d7d, i \u2264       j \u2264   n, j \u2212     i \u2265   ki, we have\n  E   Tr    U T Bj,i+1G (Ai \u2212           \u03a3) BT  j,i+1U\n           \u2264   \u03b7i+1\u2225G\u22252          2V |\u03bb2 (P)|                        2\u03b3 (1 + 8\u03f5) +          2 + (1 + \u03f5)2         k2\n                                1 \u2212   |\u03bb2 (P)| + \u03b7i+1M                                                           i+1 (M + \u03bb1)2\n                     \u00d7 E     Tr    U T Bj,i+ki+1BT      j,i+ki+1U\nwhere Bj,i is defined in 7.\nProof. For the convenience of notation, we denote ki+1 := k. Let Bj,i+1 = Bj,i+k (I + R), then\n E   Tr  \uf8ee U T Bj,i+1G (Ai \u2212          \u03a3) BT   j,i+1U       =   \uf8f9        \uf8ee                                                           \uf8f9\n         \uf8ef                                                    \uf8fa         \uf8ef        U T Bj,i+kG (Ai \u2212          \u03a3) RT BT                \uf8fa\n      E  \uf8f0Tr     U T Bj,i+kG (Ai \u2212 T1       \u03a3) BT   j,i+kU     \uf8fb  + E   \uf8f0Tr                           T2                j,i+kU      \uf8fb  +\n         \uf8ee                                                        \uf8f9        \uf8ee                                                             \uf8f9\n      E  \uf8ef       U T Bj,i+kRG (Ai \u2212            \u03a3) BT              \uf8fa        \uf8ef       U T Bj,i+kRG (Ai \u2212             \u03a3) RT BT               \uf8fa\n         \uf8f0Tr                         T3               j,i+kU      \uf8fb  + E   \uf8f0Tr                            T4                  j,i+kU     \uf8fb\n                                                                                                                                    (S.21)\nWe will now bound each of the terms E [T1] , E [T2] , E [T3] and E [T4].\n            E [T1] = E        Tr   U T Bj,i+kG (Ai \u2212           \u03a3) BT  j,i+kU\n                     = E      E    Tr    U T Bj,i+kG (Ai \u2212          \u03a3) BT  j,i+kU       si+k, . . . sj\u22121, sj\n                     = E      Tr     U T Bj,i+kG E          (Ai \u2212     \u03a3)   si+k, . . . sj\u22121, sj       BT j,i+kU\n                     = E      Tr     U T Bj,i+kG E          (Ai \u2212     \u03a3)   si+k     BT j,i+kU          using Lemma S.6\nNow, using Lemma 1, we have,\n           E     (Ai \u2212    \u03a3)    si+k      2  =     s\u2208\u2126  P k(si+k, s) (Ai \u2212          \u03a3)   2\n                                             =     s\u2208\u2126    P k(s   i+k, s) \u2212    \u03c0 (s)     (Ai \u2212    \u03a3) + E\u03c0 [(Ai \u2212   =0     \u03a3)]    2\n                                             =     s\u2208\u2126    P k(s   i+k, s) \u2212    \u03c0 (s)     (Ai \u2212    \u03a3)    2\n                                             \u2264   M   s\u2208\u2126    P k(si+k, s) \u2212        \u03c0 (s)\n                                             \u2264   2Mdmix (ki+1)\nwhere we have used Lemma S.5. Therefore,     \u2264   2\u03b3\u03b72 i+1M                                                                          (S.22)\n                                |E [T1]| \u2264     \u03b3\u03b72 i+1M\u2225G\u22252E        21  Tr    U T Bj,i+kBT     j,i+kU                               (S.23)", "md": "# Math Equations\n\nLemma S.8. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$$, $$\\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence. Set $$k_i := \\tau_{\\text{mix}}^{\\gamma} \\eta_i^2$$, $$\\gamma \\in (0, 1]$$. Then for constant matrix $$U \\in \\mathbb{R}^{d \\times d'}$$, and constant positive semi-definite matrix $$G \\in \\mathbb{R}^{d \\times d}$$, $$i \\leq j \\leq n$$, $$j - i \\geq k_i$$, we have\n\n$$\n\\begin{align*}\n&E \\text{Tr} \\left( U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U \\right) \\\\\n&\\leq \\eta_{i+1} \\|G\\|_2^2 2V |\\lambda_2(P)| 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 \\\\\n&\\times E \\text{Tr} \\left( U^T B_{j,i+k+1}B^T_{j,i+k+1}U \\right)\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. For the convenience of notation, we denote $$k_{i+1} := k$$. Let $$B_{j,i+1} = B_{j,i+k} (I + R)$$, then\n\n$$\n\\begin{align*}\n&E \\text{Tr} \\left( U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U \\right) \\\\\n&= E \\left( \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) R^T B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}RG (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}RG (A_i - \\Sigma) R^T B^T_{j,i+k}U \\right) \\right)\n\\end{align*}\n$$\nWe will now bound each of the terms $$E[T1]$$, $$E[T2]$$, $$E[T3]$$ and $$E[T4]$$.\n\n$$E[T1] = E \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right)$$\n\n$$\n\\begin{align*}\n&= E \\left( E \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&= E \\text{Tr} \\left( U^T B_{j,i+k}G E(A_i - \\Sigma) \\right) \\\\\n&= E \\text{Tr} \\left( U^T B_{j,i+k}G E(A_i - \\Sigma) \\right) \\text{ using Lemma S.6}\n\\end{align*}\n$$\nNow, using Lemma 1, we have,\n\n$$\n\\begin{align*}\n&E(A_i - \\Sigma) \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) (A_i - \\Sigma)^2 \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) (A_i - \\Sigma) + E_{\\pi}[(A_i - \\Sigma)]^2 \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) (A_i - \\Sigma)^2 \\\\\n&\\leq M \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) \\\\\n&\\leq 2M \\text{d}_{\\text{mix}}(k_{i+1})\n\\end{align*}\n$$\nTherefore, $$|E[T1]| \\leq 2\\gamma \\eta_{i+1} M$$\n\n$$\n\\begin{align*}\n&\\leq |E[T1]| \\leq \\gamma \\eta_{i+1} M \\|G\\|_2 E \\left( 2\\text{Tr} \\left( U^T B_{j,i+k}B^T_{j,i+k}U \\right) \\right)\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Lemma S.8. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$$, $$\\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence. Set $$k_i := \\tau_{\\text{mix}}^{\\gamma} \\eta_i^2$$, $$\\gamma \\in (0, 1]$$. Then for constant matrix $$U \\in \\mathbb{R}^{d \\times d'}$$, and constant positive semi-definite matrix $$G \\in \\mathbb{R}^{d \\times d}$$, $$i \\leq j \\leq n$$, $$j - i \\geq k_i$$, we have\n\n$$\n\\begin{align*}\n&E \\text{Tr} \\left( U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U \\right) \\\\\n&\\leq \\eta_{i+1} \\|G\\|_2^2 2V |\\lambda_2(P)| 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 \\\\\n&\\times E \\text{Tr} \\left( U^T B_{j,i+k+1}B^T_{j,i+k+1}U \\right)\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. For the convenience of notation, we denote $$k_{i+1} := k$$. Let $$B_{j,i+1} = B_{j,i+k} (I + R)$$, then\n\n$$\n\\begin{align*}\n&E \\text{Tr} \\left( U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U \\right) \\\\\n&= E \\left( \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) R^T B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}RG (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}RG (A_i - \\Sigma) R^T B^T_{j,i+k}U \\right) \\right)\n\\end{align*}\n$$\nWe will now bound each of the terms $$E[T1]$$, $$E[T2]$$, $$E[T3]$$ and $$E[T4]$$.\n\n$$E[T1] = E \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right)$$\n\n$$\n\\begin{align*}\n&= E \\left( E \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&= E \\text{Tr} \\left( U^T B_{j,i+k}G E(A_i - \\Sigma) \\right) \\\\\n&= E \\text{Tr} \\left( U^T B_{j,i+k}G E(A_i - \\Sigma) \\right) \\text{ using Lemma S.6}\n\\end{align*}\n$$\nNow, using Lemma 1, we have,\n\n$$\n\\begin{align*}\n&E(A_i - \\Sigma) \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) (A_i - \\Sigma)^2 \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) (A_i - \\Sigma) + E_{\\pi}[(A_i - \\Sigma)]^2 \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) (A_i - \\Sigma)^2 \\\\\n&\\leq M \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) \\\\\n&\\leq 2M \\text{d}_{\\text{mix}}(k_{i+1})\n\\end{align*}\n$$\nTherefore, $$|E[T1]| \\leq 2\\gamma \\eta_{i+1} M$$\n\n$$\n\\begin{align*}\n&\\leq |E[T1]| \\leq \\gamma \\eta_{i+1} M \\|G\\|_2 E \\left( 2\\text{Tr} \\left( U^T B_{j,i+k}B^T_{j,i+k}U \\right) \\right)\n\\end{align*}\n$$", "md": "Lemma S.8. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$$, $$\\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence. Set $$k_i := \\tau_{\\text{mix}}^{\\gamma} \\eta_i^2$$, $$\\gamma \\in (0, 1]$$. Then for constant matrix $$U \\in \\mathbb{R}^{d \\times d'}$$, and constant positive semi-definite matrix $$G \\in \\mathbb{R}^{d \\times d}$$, $$i \\leq j \\leq n$$, $$j - i \\geq k_i$$, we have\n\n$$\n\\begin{align*}\n&E \\text{Tr} \\left( U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U \\right) \\\\\n&\\leq \\eta_{i+1} \\|G\\|_2^2 2V |\\lambda_2(P)| 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 \\\\\n&\\times E \\text{Tr} \\left( U^T B_{j,i+k+1}B^T_{j,i+k+1}U \\right)\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. For the convenience of notation, we denote $$k_{i+1} := k$$. Let $$B_{j,i+1} = B_{j,i+k} (I + R)$$, then\n\n$$\n\\begin{align*}\n&E \\text{Tr} \\left( U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U \\right) \\\\\n&= E \\left( \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) R^T B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}RG (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&+ E \\left( \\text{Tr} \\left( U^T B_{j,i+k}RG (A_i - \\Sigma) R^T B^T_{j,i+k}U \\right) \\right)\n\\end{align*}\n$$\nWe will now bound each of the terms $$E[T1]$$, $$E[T2]$$, $$E[T3]$$ and $$E[T4]$$.\n\n$$E[T1] = E \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right)$$\n\n$$\n\\begin{align*}\n&= E \\left( E \\text{Tr} \\left( U^T B_{j,i+k}G (A_i - \\Sigma) B^T_{j,i+k}U \\right) \\right) \\\\\n&= E \\text{Tr} \\left( U^T B_{j,i+k}G E(A_i - \\Sigma) \\right) \\\\\n&= E \\text{Tr} \\left( U^T B_{j,i+k}G E(A_i - \\Sigma) \\right) \\text{ using Lemma S.6}\n\\end{align*}\n$$\nNow, using Lemma 1, we have,\n\n$$\n\\begin{align*}\n&E(A_i - \\Sigma) \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) (A_i - \\Sigma)^2 \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) (A_i - \\Sigma) + E_{\\pi}[(A_i - \\Sigma)]^2 \\\\\n&= \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) (A_i - \\Sigma)^2 \\\\\n&\\leq M \\sum_{s \\in \\Omega} P^k(s_{i+k}, s) - \\pi(s) \\\\\n&\\leq 2M \\text{d}_{\\text{mix}}(k_{i+1})\n\\end{align*}\n$$\nTherefore, $$|E[T1]| \\leq 2\\gamma \\eta_{i+1} M$$\n\n$$\n\\begin{align*}\n&\\leq |E[T1]| \\leq \\gamma \\eta_{i+1} M \\|G\\|_2 E \\left( 2\\text{Tr} \\left( U^T B_{j,i+k}B^T_{j,i+k}U \\right) \\right)\n\\end{align*}\n$$"}]}, {"page": 22, "text": "We will now bound E [T2]. Let R0 :=  i+k\u22121            \u2113=i+1 \u03b7\u2113A\u2113. Using Lemma 2 we have\nThen,                                    \u2225R \u2212     R0\u22252 \u2264     \u03b72i+1k2 i+1 (M + \u03bb1)2\nE[T2] = E       Tr   U T Bj,i+kG (Ai \u2212         \u03a3) RT BT   j,i+kU\n        = E     Tr   U T Bj,i+kG (Ai \u2212         \u03a3) RT  0 BTj,i+kU       + E    Tr    U T Bj,i+kG (Ai \u2212         \u03a3) (R \u2212     R0)T BT   j,i+kU\n        = E     Tr   U T Bj,i+kGE[(Ai \u2212          \u03a3) RT  0 |si+k, . . . sj\u22121, sj]BT   j,i+kU      +\nUsing Lemma 3 with S := I we have,                 E   Tr    U T Bj,i+kG (Ai \u2212         \u03a3) (R \u2212     R0)T BT   j,i+kU\n     \u2225E   (Ai \u2212    \u03a3) RT  0 |si+k, . . . , sj  \u22252 \u2264   i+k\u22121    \u03b7\u2113   |\u03bb2 (P)|\u2113\u2212i V + 8\u03b3\u03b72       i+1M (M + \u03bb1)\n                                                       \u2113=i+1\n                                                   \u2264  \u03b7i+1V       |\u03bb2 (P)|               i+1ki+1M (M + \u03bb1)                (S.24)\nTherefore,                                                     1 \u2212   |\u03bb2 (P)| + 8\u03b3\u03b73\n|E [T2] |\n \u2264  \u2225G\u22252      \u03b7i+1    V |\u03bb2 (P)|               i+1ki+1M (M + \u03bb1) + \u03b72            i+1k2 i+1M (M + \u03bb1)2              E   Tr   U T Bj,i+kBT    j,i+kU\n                     1 \u2212  |\u03bb2 (P)| + 8\u03b3\u03b73\n = \u03b7i+1\u2225G\u22252           V |\u03bb2 (P)|               i+1ki+1M (M + \u03bb1) + \u03b7i+1k2              i+1M (M + \u03bb1)2              E   Tr   U T Bj,i+kBT    j,i+kU\n                     1 \u2212  |\u03bb2 (P)| + 8\u03b3\u03b72                                                                                 (S.25)\nSimilarly using Lemma 3 with S := G,\n|E [T3]| \u2264    \u03b7i+1\u2225G\u22252          V |\u03bb2 (P)|              i+1ki+1M (M + \u03bb1) + \u03b7i+1k2               i+1M (M + \u03bb1)2             E   Tr    U T Bj,i+kBT  j,i+kU\n                              1 \u2212   |\u03bb2 (P)| + 8\u03b3\u03b72                                                                       (S.26)\nFinally,\n   |E [T4]| \u2264    M\u2225G\u22252\u2225R\u22252        2E    Tr   U T Bj,i+kBT    j,i+kU\n              \u2264  (1 + \u03f5)2 \u03b72  i+1k2 i+1M (M + \u03bb1)2 \u2225G\u22252E                Tr    U T Bj,i+kBT    j,i+kU       using Lemma 2  (S.27)\nTherefore, using Eqs S.23, S.25, S.26, S.27 along with S.21, we have\n  E   Tr   U T Bj,i+1G (Ai \u2212         \u03a3) BT j,i+1U\n \u2264  \u03b7i+1\u2225G\u22252         2V |\u03bb2 (P)|                      2\u03b3 + 16\u03b3\u03b7i+1ki+1 (M + \u03bb1) +                  2 + (1 + \u03f5)2       k2i+1 (M + \u03bb1)2\n                     1 \u2212  |\u03bb2 (P)| + \u03b7i+1M\n \u2264  \u03b7i+1\u2225G\u22252         2V |\u03bb2 (P)|                      2\u03b3 (1 + 8\u03f5) +        2 + (1 + \u03f5)2       k2       \u00d7 E    Tr    U T Bj,i+kBT    j,i+kU\n                     1 \u2212  |\u03bb2 (P)| + \u03b7i+1M                                                      i+1 (M + \u03bb1)2\nwhere in the last line we used \u03b7i+1ki+1 (M + \u03bb1) \u2264              22    \u03f5. Hence proved.                 \u00d7 E    Tr    U T Bj,i+kBT    j,i+kU", "md": "We will now bound $$E[T2]$$. Let $$R_0 := i+k-1 \\ell=i+1 \\eta_{\\ell}A_{\\ell}$$. Using Lemma 2 we have\nThen, $$\\|R - R_0\\|_2 \\leq \\eta^2 i+1k^2 i+1 (M + \\lambda_1)^2$$\n$$E[T2] = E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) R^T B^T_{j,i+k}U$$\n$$= E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) R^T_0 B^T_{j,i+k}U + E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) (R - R_0)^T B^T_{j,i+k}U$$\n$$= E \\text{Tr} U^T B_{j,i+k}GE[(A_i - \\Sigma) R^T_0 |s_{i+k}, . . . s_{j-1}, s_j]B^T_{j,i+k}U +$$\nUsing Lemma 3 with $$S := I$$ we have, $$E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) (R - R_0)^T B^T_{j,i+k}U$$\n$$\\|E (A_i - \\Sigma) R^T_0 |s_{i+k}, . . . , s_j \\|_2 \\leq i+k-1 \\eta_{\\ell} |\\lambda_2 (P)|_{\\ell-i} V + 8\\gamma \\eta^2 i+1M (M + \\lambda_1)$$\n$$\\leq \\eta_{i+1}V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) \\text{ (S.24)}$$\nTherefore, $$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^3 |E[T2]|$$\n$$\\leq \\|G\\|_2 \\eta_{i+1} V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta^2 i+1k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^3$$\n$$= \\eta_{i+1}\\|G\\|_2 V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta_{i+1}k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^2 \\text{ (S.25)}$$\nSimilarly using Lemma 3 with $$S := G$$,\n$$|E[T3]| \\leq \\eta_{i+1}\\|G\\|_2 V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta_{i+1}k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^2 \\text{ (S.26)}$$\nFinally,\n$$|E[T4]| \\leq M\\|G\\|_2\\|R\\|_2 2E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$\\leq (1 + \\epsilon)^2 \\eta^2 i+1k^2 i+1M (M + \\lambda_1)^2 \\|G\\|_2E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U \\text{ using Lemma 2 (S.27)}$$\nTherefore, using Eqs S.23, S.25, S.26, S.27 along with S.21, we have\n$$E \\text{Tr} U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U$$\n$$\\leq \\eta_{i+1}\\|G\\|_2 2V |\\lambda_2 (P)| 2\\gamma + 16\\gamma \\eta_{i+1}k i+1 (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 k^2 i+1 (M + \\lambda_1)^2$$\n$$1 - |\\lambda_2 (P)| + \\eta_{i+1}M$$\n$$\\leq \\eta_{i+1}\\|G\\|_2 2V |\\lambda_2 (P)| 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 \\times E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + \\eta_{i+1}M i+1 (M + \\lambda_1)^2$$\nwhere in the last line we used $$\\eta_{i+1}k i+1 (M + \\lambda_1) \\leq 22 \\epsilon$$. Hence proved. $$\\times E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$", "images": [], "items": [{"type": "text", "value": "We will now bound $$E[T2]$$. Let $$R_0 := i+k-1 \\ell=i+1 \\eta_{\\ell}A_{\\ell}$$. Using Lemma 2 we have\nThen, $$\\|R - R_0\\|_2 \\leq \\eta^2 i+1k^2 i+1 (M + \\lambda_1)^2$$\n$$E[T2] = E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) R^T B^T_{j,i+k}U$$\n$$= E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) R^T_0 B^T_{j,i+k}U + E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) (R - R_0)^T B^T_{j,i+k}U$$\n$$= E \\text{Tr} U^T B_{j,i+k}GE[(A_i - \\Sigma) R^T_0 |s_{i+k}, . . . s_{j-1}, s_j]B^T_{j,i+k}U +$$\nUsing Lemma 3 with $$S := I$$ we have, $$E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) (R - R_0)^T B^T_{j,i+k}U$$\n$$\\|E (A_i - \\Sigma) R^T_0 |s_{i+k}, . . . , s_j \\|_2 \\leq i+k-1 \\eta_{\\ell} |\\lambda_2 (P)|_{\\ell-i} V + 8\\gamma \\eta^2 i+1M (M + \\lambda_1)$$\n$$\\leq \\eta_{i+1}V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) \\text{ (S.24)}$$\nTherefore, $$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^3 |E[T2]|$$\n$$\\leq \\|G\\|_2 \\eta_{i+1} V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta^2 i+1k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^3$$\n$$= \\eta_{i+1}\\|G\\|_2 V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta_{i+1}k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^2 \\text{ (S.25)}$$\nSimilarly using Lemma 3 with $$S := G$$,\n$$|E[T3]| \\leq \\eta_{i+1}\\|G\\|_2 V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta_{i+1}k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^2 \\text{ (S.26)}$$\nFinally,\n$$|E[T4]| \\leq M\\|G\\|_2\\|R\\|_2 2E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$\\leq (1 + \\epsilon)^2 \\eta^2 i+1k^2 i+1M (M + \\lambda_1)^2 \\|G\\|_2E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U \\text{ using Lemma 2 (S.27)}$$\nTherefore, using Eqs S.23, S.25, S.26, S.27 along with S.21, we have\n$$E \\text{Tr} U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U$$\n$$\\leq \\eta_{i+1}\\|G\\|_2 2V |\\lambda_2 (P)| 2\\gamma + 16\\gamma \\eta_{i+1}k i+1 (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 k^2 i+1 (M + \\lambda_1)^2$$\n$$1 - |\\lambda_2 (P)| + \\eta_{i+1}M$$\n$$\\leq \\eta_{i+1}\\|G\\|_2 2V |\\lambda_2 (P)| 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 \\times E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + \\eta_{i+1}M i+1 (M + \\lambda_1)^2$$\nwhere in the last line we used $$\\eta_{i+1}k i+1 (M + \\lambda_1) \\leq 22 \\epsilon$$. Hence proved. $$\\times E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$", "md": "We will now bound $$E[T2]$$. Let $$R_0 := i+k-1 \\ell=i+1 \\eta_{\\ell}A_{\\ell}$$. Using Lemma 2 we have\nThen, $$\\|R - R_0\\|_2 \\leq \\eta^2 i+1k^2 i+1 (M + \\lambda_1)^2$$\n$$E[T2] = E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) R^T B^T_{j,i+k}U$$\n$$= E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) R^T_0 B^T_{j,i+k}U + E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) (R - R_0)^T B^T_{j,i+k}U$$\n$$= E \\text{Tr} U^T B_{j,i+k}GE[(A_i - \\Sigma) R^T_0 |s_{i+k}, . . . s_{j-1}, s_j]B^T_{j,i+k}U +$$\nUsing Lemma 3 with $$S := I$$ we have, $$E \\text{Tr} U^T B_{j,i+k}G (A_i - \\Sigma) (R - R_0)^T B^T_{j,i+k}U$$\n$$\\|E (A_i - \\Sigma) R^T_0 |s_{i+k}, . . . , s_j \\|_2 \\leq i+k-1 \\eta_{\\ell} |\\lambda_2 (P)|_{\\ell-i} V + 8\\gamma \\eta^2 i+1M (M + \\lambda_1)$$\n$$\\leq \\eta_{i+1}V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) \\text{ (S.24)}$$\nTherefore, $$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^3 |E[T2]|$$\n$$\\leq \\|G\\|_2 \\eta_{i+1} V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta^2 i+1k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^3$$\n$$= \\eta_{i+1}\\|G\\|_2 V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta_{i+1}k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^2 \\text{ (S.25)}$$\nSimilarly using Lemma 3 with $$S := G$$,\n$$|E[T3]| \\leq \\eta_{i+1}\\|G\\|_2 V |\\lambda_2 (P)|_{i+1} i+1k i+1M (M + \\lambda_1) + \\eta_{i+1}k^2 i+1M (M + \\lambda_1)^2 E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + 8\\gamma \\eta^2 \\text{ (S.26)}$$\nFinally,\n$$|E[T4]| \\leq M\\|G\\|_2\\|R\\|_2 2E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$\\leq (1 + \\epsilon)^2 \\eta^2 i+1k^2 i+1M (M + \\lambda_1)^2 \\|G\\|_2E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U \\text{ using Lemma 2 (S.27)}$$\nTherefore, using Eqs S.23, S.25, S.26, S.27 along with S.21, we have\n$$E \\text{Tr} U^T B_{j,i+1}G (A_i - \\Sigma) B^T_{j,i+1}U$$\n$$\\leq \\eta_{i+1}\\|G\\|_2 2V |\\lambda_2 (P)| 2\\gamma + 16\\gamma \\eta_{i+1}k i+1 (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 k^2 i+1 (M + \\lambda_1)^2$$\n$$1 - |\\lambda_2 (P)| + \\eta_{i+1}M$$\n$$\\leq \\eta_{i+1}\\|G\\|_2 2V |\\lambda_2 (P)| 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 \\times E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$\n$$1 - |\\lambda_2 (P)| + \\eta_{i+1}M i+1 (M + \\lambda_1)^2$$\nwhere in the last line we used $$\\eta_{i+1}k i+1 (M + \\lambda_1) \\leq 22 \\epsilon$$. Hence proved. $$\\times E \\text{Tr} U^T B_{j,i+k}B^T_{j,i+k}U$$"}]}, {"page": 23, "text": "Lemma S.9. Let \u2200i \u2208              [n], \u03b7iki (M + \u03bb1) \u2264            \u03f5, \u03f5 \u2208   (0, 1) and \u03b7i forms a non-increasing sequence.\nSet ki := \u03c4mix         \u03b3\u03b72i   , \u03b3 \u2208    (0, 1]. Then for constant matrices U \u2208                     Rd\u00d7d\u2032, G \u2208         Rd\u00d7d, i \u2264        j \u2264\nn, j \u2212    i \u2265   ki, we have\n E    Tr     U T Bj,i+1G (Ai \u2212          \u03a3)2 BT   j,i+1U\n             \u2264    V + \u03b7i+1M2 (2\u03b3\u03b7i+1 + (1 + \u03f5) (2 + \u03f5 (1 + \u03f5)) ki+1 (M + \u03bb1))                                   \u2225G\u22252 E       Tr    U T Bj,i+ki+1BT  j,i+ki+1U\nwhere Bj,i is defined in 7.\nProof. For convenience of notation, we denote ki+1 := k. Let Bj,i+1 = Bj,i+k (I + R), then\nE   Tr  \uf8ee  U T Bj,i+1G (Ai \u2212          \u03a3)2 BT   j,i+1U        =   \uf8f9        \uf8ee                                                             \uf8f9\n                                                               \uf8fa          \uf8ef                                                             \uf8fa\n        \uf8ef                                                        \uf8fa        \uf8ef                                                             \uf8fa\n     E  \uf8ef       U T Bj,i+kG (Ai \u2212           \u03a3)2 BT   j,i+kU      \uf8fb  + E   \uf8f0Tr      U T Bj,i+kG (Ai \u2212          \u03a3)2 RT BT     j,i+kU      \uf8fb  +\n        \uf8f0Tr                         T1                                                                   T2\n        \uf8ee                                                          \uf8f9         \uf8ee                                                               \uf8f9\n        \uf8ef                                                          \uf8fa         \uf8ef                                                               \uf8fa\n     E  \uf8ef       U T Bj,i+kRG (Ai \u2212             \u03a3)2 BT  j,i+kU      \uf8fa         \uf8ef        U T Bj,i+kRG (Ai \u2212            \u03a3)2 RT BT     j,i+kU     \uf8fa\n        \uf8f0Tr                          T3                            \uf8fb  + E    \uf8f0Tr                             T4                              \uf8fb\nWe will now bound each of the terms E [T1] , E [T2] , E [T3] and E [T4].\nSince     E\u03c0     (At \u2212     \u03a3)2     2 \u2264    V, therefore\n           E [T1] = E        Tr    U T Bj,i+kG (Ai \u2212           \u03a3)2 BT   j,i+kU\n                     = E     E    Tr    U T Bj,i+kG (Ai \u2212           \u03a3)2 BT   j,i+kU       si+k, . . . sj\u22121, sj\n                     = E     Tr     U T Bj,i+kG E          (Ai \u2212     \u03a3)2    si+k, . . . sj\u22121, sj       BT j,i+kU\n                     = E     Tr     U T Bj,i+kG E          (Ai \u2212     \u03a3)2    si+k     BT j,i+kU         using Lemma S.6\n                     (i)\n                     \u2264    V + 2dmix (k) M2             \u2225G\u22252 E       Tr    U T Bj,i+k BT     j,i+kU\nwhere in (i), we used similar steps as S.22 to get\n                         E     (Ai \u2212    \u03a3)2    si+k       2 \u2264    E\u03c0      (Ai \u2212    \u03a3)2      2 + 2dmix (k) M2                         (S.28)\nNext, using Lemma 2 we have that\nTherefore,                                 \u2225R\u22252 \u2264       (1 + \u03f5) ki+1\u03b7i+1 (M + \u03bb1) .                                                 (S.29)\n                E [T2] = E        Tr    U T Bj,i+kG (Ai \u2212           \u03a3)2 RT BT     j,i+kU\nSimilarly,                \u2264  (1 + \u03f5) ki+1\u03b7i+1M2 (M + \u03bb1) \u2225G\u22252 E                         Tr    U T Bj,i+kBT     j,i+kU\n                E [T3] = E        Tr    U T Bj,i+kRG (Ai \u2212             \u03a3)2 BT  j,i+kU\n                          \u2264  (1 + \u03f5) ki+1\u03b7i+1M2 (M + \u03bb1) \u2225G\u22252 E     23                  Tr    U T Bj,i+kBT     j,i+kU", "md": "Lemma S.9. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence. Set $$k_i := \\tau_{\\text{mix}} \\gamma \\eta_{2i}$$, $$\\gamma \\in (0, 1]$$. Then for constant matrices $$U \\in \\mathbb{R}^{d \\times d'}, G \\in \\mathbb{R}^{d \\times d}, i \\leq j \\leq n, j - i \\geq k_i$$, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E} \\text{Tr} \\, U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\\\\n&\\leq V + \\eta_{i+1}M^2 (2\\gamma \\eta_{i+1} + (1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) k_{i+1} (M + \\lambda_1)) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k_{i+1}}B^T_{j,i+k_{i+1}}U\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. For convenience of notation, we denote $$k_{i+1} := k$$. Let $$B_{j,i+1} = B_{j,i+k} (I + R)$$, then\n\n$$\n\\begin{align*}\n&\\mathbb{E} \\text{Tr} \\left[ U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\right] \\\\\n&= \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\right] + \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\right] \\\\\n&+ \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 B^T_{j,i+k}U \\right] + \\mathbb{E} \\left[ \\text{Tr} \\, T_4 \\right]\n\\end{align*}\n$$\nWe will now bound each of the terms $$\\mathbb{E} [T1]$$, $$\\mathbb{E} [T2]$$, $$\\mathbb{E} [T3]$$ and $$\\mathbb{E} [T4]$$. Since $$\\mathbb{E}_{\\pi} (A_t - \\Sigma)^2 \\leq V$$, therefore\n\n$$\n\\begin{align*}\n\\mathbb{E} [T1] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\\\\n&= \\mathbb{E} \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\, \\text{si+k, . . . sj-1, sj} \\\\\n&= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G \\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k, . . . sj-1, sj} B^T_{j,i+k}U \\\\\n&= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G \\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k} B^T_{j,i+k}U \\, \\text{using Lemma S.6} \\\\\n&\\leq V + 2d_{\\text{mix}}(k) M^2 \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$\nwhere in (i), we used similar steps as S.22 to get\n\n$$\n\\begin{align*}\n\\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k}^2 &\\leq \\mathbb{E}_{\\pi} (A_i - \\Sigma)^2 + 2d_{\\text{mix}}(k) M^2 \\quad \\text{(S.28)}\n\\end{align*}\n$$\nNext, using Lemma 2 we have that\n\nTherefore, $$\\lVert R \\rVert^2 \\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} (M + \\lambda_1)$$ (S.29)\n\n$$\n\\begin{align*}\n\\mathbb{E} [T2] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\\\\n&\\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} M^2 (M + \\lambda_1) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$\n$$\n\\begin{align*}\n\\mathbb{E} [T3] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 B^T_{j,i+k}U \\\\\n&\\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} M^2 (M + \\lambda_1) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Lemma S.9. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence. Set $$k_i := \\tau_{\\text{mix}} \\gamma \\eta_{2i}$$, $$\\gamma \\in (0, 1]$$. Then for constant matrices $$U \\in \\mathbb{R}^{d \\times d'}, G \\in \\mathbb{R}^{d \\times d}, i \\leq j \\leq n, j - i \\geq k_i$$, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E} \\text{Tr} \\, U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\\\\n&\\leq V + \\eta_{i+1}M^2 (2\\gamma \\eta_{i+1} + (1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) k_{i+1} (M + \\lambda_1)) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k_{i+1}}B^T_{j,i+k_{i+1}}U\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. For convenience of notation, we denote $$k_{i+1} := k$$. Let $$B_{j,i+1} = B_{j,i+k} (I + R)$$, then\n\n$$\n\\begin{align*}\n&\\mathbb{E} \\text{Tr} \\left[ U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\right] \\\\\n&= \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\right] + \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\right] \\\\\n&+ \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 B^T_{j,i+k}U \\right] + \\mathbb{E} \\left[ \\text{Tr} \\, T_4 \\right]\n\\end{align*}\n$$\nWe will now bound each of the terms $$\\mathbb{E} [T1]$$, $$\\mathbb{E} [T2]$$, $$\\mathbb{E} [T3]$$ and $$\\mathbb{E} [T4]$$. Since $$\\mathbb{E}_{\\pi} (A_t - \\Sigma)^2 \\leq V$$, therefore\n\n$$\n\\begin{align*}\n\\mathbb{E} [T1] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\\\\n&= \\mathbb{E} \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\, \\text{si+k, . . . sj-1, sj} \\\\\n&= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G \\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k, . . . sj-1, sj} B^T_{j,i+k}U \\\\\n&= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G \\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k} B^T_{j,i+k}U \\, \\text{using Lemma S.6} \\\\\n&\\leq V + 2d_{\\text{mix}}(k) M^2 \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$\nwhere in (i), we used similar steps as S.22 to get\n\n$$\n\\begin{align*}\n\\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k}^2 &\\leq \\mathbb{E}_{\\pi} (A_i - \\Sigma)^2 + 2d_{\\text{mix}}(k) M^2 \\quad \\text{(S.28)}\n\\end{align*}\n$$\nNext, using Lemma 2 we have that\n\nTherefore, $$\\lVert R \\rVert^2 \\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} (M + \\lambda_1)$$ (S.29)\n\n$$\n\\begin{align*}\n\\mathbb{E} [T2] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\\\\n&\\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} M^2 (M + \\lambda_1) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$\n$$\n\\begin{align*}\n\\mathbb{E} [T3] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 B^T_{j,i+k}U \\\\\n&\\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} M^2 (M + \\lambda_1) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$", "md": "Lemma S.9. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence. Set $$k_i := \\tau_{\\text{mix}} \\gamma \\eta_{2i}$$, $$\\gamma \\in (0, 1]$$. Then for constant matrices $$U \\in \\mathbb{R}^{d \\times d'}, G \\in \\mathbb{R}^{d \\times d}, i \\leq j \\leq n, j - i \\geq k_i$$, we have\n\n$$\n\\begin{align*}\n&\\mathbb{E} \\text{Tr} \\, U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\\\\n&\\leq V + \\eta_{i+1}M^2 (2\\gamma \\eta_{i+1} + (1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) k_{i+1} (M + \\lambda_1)) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k_{i+1}}B^T_{j,i+k_{i+1}}U\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. For convenience of notation, we denote $$k_{i+1} := k$$. Let $$B_{j,i+1} = B_{j,i+k} (I + R)$$, then\n\n$$\n\\begin{align*}\n&\\mathbb{E} \\text{Tr} \\left[ U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\right] \\\\\n&= \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\right] + \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\right] \\\\\n&+ \\mathbb{E} \\left[ \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 B^T_{j,i+k}U \\right] + \\mathbb{E} \\left[ \\text{Tr} \\, T_4 \\right]\n\\end{align*}\n$$\nWe will now bound each of the terms $$\\mathbb{E} [T1]$$, $$\\mathbb{E} [T2]$$, $$\\mathbb{E} [T3]$$ and $$\\mathbb{E} [T4]$$. Since $$\\mathbb{E}_{\\pi} (A_t - \\Sigma)^2 \\leq V$$, therefore\n\n$$\n\\begin{align*}\n\\mathbb{E} [T1] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\\\\n&= \\mathbb{E} \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 B^T_{j,i+k}U \\, \\text{si+k, . . . sj-1, sj} \\\\\n&= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G \\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k, . . . sj-1, sj} B^T_{j,i+k}U \\\\\n&= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G \\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k} B^T_{j,i+k}U \\, \\text{using Lemma S.6} \\\\\n&\\leq V + 2d_{\\text{mix}}(k) M^2 \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$\nwhere in (i), we used similar steps as S.22 to get\n\n$$\n\\begin{align*}\n\\mathbb{E} (A_i - \\Sigma)^2 \\, \\text{si+k}^2 &\\leq \\mathbb{E}_{\\pi} (A_i - \\Sigma)^2 + 2d_{\\text{mix}}(k) M^2 \\quad \\text{(S.28)}\n\\end{align*}\n$$\nNext, using Lemma 2 we have that\n\nTherefore, $$\\lVert R \\rVert^2 \\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} (M + \\lambda_1)$$ (S.29)\n\n$$\n\\begin{align*}\n\\mathbb{E} [T2] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}G (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\\\\n&\\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} M^2 (M + \\lambda_1) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$\n$$\n\\begin{align*}\n\\mathbb{E} [T3] &= \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 B^T_{j,i+k}U \\\\\n&\\leq (1 + \\epsilon) k_{i+1} \\eta_{i+1} M^2 (M + \\lambda_1) \\lVert G \\rVert^2 \\mathbb{E} \\text{Tr} \\, U^T B_{j,i+k} B^T_{j,i+k}U\n\\end{align*}\n$$"}]}, {"page": 24, "text": "Finally, using the bound on \u2225R\u22252 from Eq S.29, we have:\nE [T4] = E        Tr    U T Bj,i+kRG (Ai \u2212            \u03a3)2 RT BT     j,i+kU\n         \u2264   (1 + \u03f5)2 k2   i+1\u03b72  i+1M2 (M + \u03bb1)2 \u2225G\u22252 E                   Tr    U T Bj,i+kBT     j,i+kU\n         \u2264   \u03f5 (1 + \u03f5)2 ki+1\u03b7i+1M2 (M + \u03bb1) \u2225G\u22252 E                          Tr   U T Bj,i+kBT      j,i+kU        using \u2200i, \u03b7iki (M + \u03bb1) \u2264               c\nTherefore,\n E    Tr     U T Bj,i+1G (Ai \u2212          \u03a3)2 BT   j,i+1U\n (i)\n \u2264     V + \u03b7i+1       2\u03b3\u03b7i+1M2 + (1 + \u03f5) (2 + \u03f5 (1 + \u03f5)) ki+1M2 (M + \u03bb1)                                   \u2225G\u22252 E       Tr    U T Bj,i+kBT     j,i+kU\n =    V + \u03b7i+1M2 (2\u03b3\u03b7i+1 + (1 + \u03f5) (2 + \u03f5 (1 + \u03f5)) ki+1 (M + \u03bb1))                                   \u2225G\u22252 E       Tr    U T Bj,i+kBT     j,i+kU\nwhere in (i), we used dmix (k) = dmix (ki+1) \u2264                     \u03b3\u03b72 i+1. Hence proved.\nLemma S.10. Let \u2200i \u2208              [n], \u03b7iki (M + \u03bb1) \u2264            \u03f5, \u03f5 \u2208   (0, 1) and step-sizes \u03b7i forms a non-increasing\nsequence. Further, let the step-sizes follow a slow-decay property, i.e, \u2200i, \u03b7i \u2264                                   \u03b7i\u2212ki \u2264      2\u03b7i. Set\nki := \u03c4mix        \u03b3\u03b72i   , \u03b3 \u2208     (0, 1]. Let G \u2208          Rd\u00d7d be a constant positive semi-definite matrix, and\nPt := Tr       Bt\u22121BT    t\u22121G(At \u2212        \u03a3)    , then,\nE [Pt] \u2264     \u03b7t\u2212kt       2V |\u03bb2 (P)|                          2\u03b3 (1 + 8\u03f5) +          2 + (1 + \u03f5)2        k2t (M + \u03bb1)2             \u2225G\u22252 E       Tr    Bt\u2212ktBT   t\u2212kt\n                        1 \u2212    |\u03bb2 (P)| + \u03b7t\u2212ktM\nwhere Bt is defined in 2.\nProof. Let Bt = (I +\uf8eeR) Bt\u2212kt with \u2225R\u22252\u2264                        \uf8f9r. Then, \uf8ee                                                \uf8f9\n    E [Pt] = E      \uf8ef        Bt\u2212ktBT                            \uf8fa         \uf8ef       Bt\u2212ktBT                                  \uf8fa\n                    \uf8f0Tr                 t\u2212ktG(At \u2212         \u03a3)   \uf8fb  + E    \uf8f0Tr                 t\u2212ktRT G(At \u2212          \u03a3)    \uf8fb\n                                         Pt,1                                                    Pt,2\n                          \uf8ee                                             \uf8f9         \uf8ee                                                  \uf8f9\n                          \uf8ef                                             \uf8fa         \uf8ef       Bt\u2212ktBT                                    \uf8fa\n                   + E    \uf8f0Tr     Bt\u2212ktBT     t\u2212ktG(At \u2212        \u03a3)R     \uf8fb   + E   \uf8f0Tr                 t\u2212ktRT G(At \u2212          \u03a3)R     \uf8fb\n                                                Pt,3                                                      Pt,4\nLet\u2019s consider each of the terms above. Using Von-Neumann\u2019s trace inequality and S.25, we have,\nE [Pt,1] = E        Tr   Bt\u2212ktBT     t\u2212ktE [G(At \u2212         \u03a3)|s1, s2, . . . , st\u2212kt]\n           \u2264   E    Tr   Bt\u2212ktBT     t\u2212ktGE [(At \u2212         \u03a3)|st\u2212kt]\n           \u2264   \u2225GE [(At \u2212        \u03a3)|st\u2212kt]\u22252 E         Tr    Bt\u2212ktBT     t\u2212kt\n           \u2264   2Mdmix (kt) \u2225G\u22252 E              Tr    Bt\u2212ktBT    t\u2212kt       using S.22\n           \u2264   2\u03b3\u03b72  t M \u2225G\u22252 E         Tr    Bt\u2212ktBT     t\u2212kt\nE [Pt,2] = E    E   Tr   Bt\u2212ktBT     t\u2212kt, E     RT G(At \u2212        \u03a3)U|s1, s2, . . . , st\u2212kt\n           \u2264          RT G(At \u2212        \u03a3)|s1, s2, . . . , st\u2212kt        2 E    Tr   Bt\u2212ktBT     t\u2212kt\n           =    E     RT G(At \u2212        \u03a3)|st\u2212kt        2 E    Tr    Bt\u2212ktBT    t\u2212kt\n           \u2264   \u03b7t\u2212kt \u2225G\u22252            V |\u03bb2 (P)|                t\u2212ktktM (M + \u03bb1) + \u03b7t\u2212ktk2                 t M (M + \u03bb1)2              E   Tr    Bt\u2212ktBT    t\u2212kt     using S.2\n                                   1 \u2212   |\u03bb2 (P)| + 8\u03b3\u03b72\n                                                                    24", "md": "Finally, using the bound on \u2225R\u22252 from Eq S.29, we have:\n\n$$\n\\begin{align*}\nE[T4] &amp;= E \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\\\\n&amp;\\leq (1 + \\epsilon)^2 k^2_{i+1}\\eta^2_{i+1}M^2(M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\\\\n&amp\\leq \\epsilon (1 + \\epsilon)^2 k_{i+1}\\eta_{i+1}M^2(M + \\lambda_1) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\text{ using } \\forall i, \\eta_ik_i(M + \\lambda_1) \\leq c\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\begin{align*}\nE \\text{Tr} \\, U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\quad (i) &amp;\\leq V + \\eta_{i+1}2\\gamma\\eta_{i+1}M^2 + (1 + \\epsilon)(2 + \\epsilon(1 + \\epsilon))k_{i+1}M^2(M + \\lambda_1) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\\\\n&amp= V + \\eta_{i+1}M^2(2\\gamma\\eta_{i+1} + (1 + \\epsilon)(2 + \\epsilon(1 + \\epsilon))k_{i+1}(M + \\lambda_1)) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U\n\\end{align*}\n$$\nwhere in (i), we used $dmix(k) = dmix(k_{i+1}) \\leq \\gamma\\eta^2_{i+1}$. Hence proved.\n\nLemma S.10. Let $\\forall i \\in [n]$, $\\eta_ik_i(M + \\lambda_1) \\leq \\epsilon$, $\\epsilon \\in (0, 1)$ and step-sizes $\\eta_i$ forms a non-increasing sequence. Further, let the step-sizes follow a slow-decay property, i.e, $\\forall i, \\eta_i \\leq \\eta_{i-k_i} \\leq 2\\eta_i$. Set $k_i := \\tau_{\\text{mix}}\\gamma\\eta^2_i$, $\\gamma \\in (0, 1]$. Let $G \\in \\mathbb{R}^{d \\times d}$ be a constant positive semi-definite matrix, and $P_t := \\text{Tr} \\, B_{t-1}B^T_{t-1}G(A_t - \\Sigma)$, then,\n\n$$\n\\begin{align*}\nE[P_t] &amp;\\leq \\eta_{t-k_t}2V \\lvert \\lambda_2(P) \\rvert^2 \\gamma(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t(M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\quad \\times \\left(1 - \\lvert \\lambda_2(P) \\rvert + \\eta_{t-k_t}M\\right)\n\\end{align*}\n$$\n\nProof. Let $B_t = (I + [R]) B_{t-k_t}$ with $\\lVert R \\rVert^2 \\leq r$. Then,\n\n$$\n\\begin{align*}\nE[P_t] &amp;= E \\left[ \\begin{array}{c} B_{t-k_t}B^T_{t-k_t} \\\\ \\text{Tr} \\, G(A_t - \\Sigma) \\end{array} \\right] + E \\left[ \\begin{array}{c} \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G(A_t - \\Sigma) \\\\ \\text{Tr} \\, R^T G(A_t - \\Sigma) \\end{array} \\right] \\\\\n&amp= \\begin{array}{c} E \\left[ \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\right] \\\\ P_{t,1} \\end{array} + E \\begin{array}{c} \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G(A_t - \\Sigma)R \\\\ \\text{Tr} \\, R^T G(A_t - \\Sigma)R \\end{array}\n\\end{align*}\n$$\n\nLet\u2019s consider each of the terms above. Using Von-Neumann\u2019s trace inequality and S.25, we have,\n\n$$\n\\begin{align*}\nE[P_{t,1}] &amp;= E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}E[G(A_t - \\Sigma) \\, | \\, s_1, s_2, . . . , s_{t-k_t}] \\\\\n&amp\\leq E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G E[(A_t - \\Sigma) \\, | \\, s_{t-k_t}] \\\\\n&amp\\leq \\lVert G E[(A_t - \\Sigma) \\, | \\, s_{t-k_t}] \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\leq 2Mdmix(k_t) \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\text{ using S.22} \\\\\n&amp\\leq 2\\gamma\\eta^2_t M \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\nE[P_{t,2}] &amp;= E \\left[ E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\right] , E \\left[ R^T G(A_t - \\Sigma)U \\, | \\, s_1, s_2, . . . , s_{t-k_t} \\right] \\\\\n&amp\\leq R^T G(A_t - \\Sigma) \\, | \\, s_1, s_2, . . . , s_{t-k_t} \\, 2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp= E \\left[ R^T G(A_t - \\Sigma) \\, | \\, s_{t-k_t} \\right] \\, 2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\leq \\eta_{t-k_t} \\lVert G \\rVert^2 V \\lvert \\lambda_2(P) \\rvert^{t-k_t}M(M + \\lambda_1) + \\eta_{t-k_t}k^2_t M(M + \\lambda_1)^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\text{ using S.2} \\\\\n&amp\\quad \\times \\left(1 - \\lvert \\lambda_2(P) \\rvert + 8\\gamma\\eta^2_{24}\\right)\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Finally, using the bound on \u2225R\u22252 from Eq S.29, we have:\n\n$$\n\\begin{align*}\nE[T4] &amp;= E \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\\\\n&amp;\\leq (1 + \\epsilon)^2 k^2_{i+1}\\eta^2_{i+1}M^2(M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\\\\n&amp\\leq \\epsilon (1 + \\epsilon)^2 k_{i+1}\\eta_{i+1}M^2(M + \\lambda_1) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\text{ using } \\forall i, \\eta_ik_i(M + \\lambda_1) \\leq c\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\begin{align*}\nE \\text{Tr} \\, U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\quad (i) &amp;\\leq V + \\eta_{i+1}2\\gamma\\eta_{i+1}M^2 + (1 + \\epsilon)(2 + \\epsilon(1 + \\epsilon))k_{i+1}M^2(M + \\lambda_1) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\\\\n&amp= V + \\eta_{i+1}M^2(2\\gamma\\eta_{i+1} + (1 + \\epsilon)(2 + \\epsilon(1 + \\epsilon))k_{i+1}(M + \\lambda_1)) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U\n\\end{align*}\n$$\nwhere in (i), we used $dmix(k) = dmix(k_{i+1}) \\leq \\gamma\\eta^2_{i+1}$. Hence proved.\n\nLemma S.10. Let $\\forall i \\in [n]$, $\\eta_ik_i(M + \\lambda_1) \\leq \\epsilon$, $\\epsilon \\in (0, 1)$ and step-sizes $\\eta_i$ forms a non-increasing sequence. Further, let the step-sizes follow a slow-decay property, i.e, $\\forall i, \\eta_i \\leq \\eta_{i-k_i} \\leq 2\\eta_i$. Set $k_i := \\tau_{\\text{mix}}\\gamma\\eta^2_i$, $\\gamma \\in (0, 1]$. Let $G \\in \\mathbb{R}^{d \\times d}$ be a constant positive semi-definite matrix, and $P_t := \\text{Tr} \\, B_{t-1}B^T_{t-1}G(A_t - \\Sigma)$, then,\n\n$$\n\\begin{align*}\nE[P_t] &amp;\\leq \\eta_{t-k_t}2V \\lvert \\lambda_2(P) \\rvert^2 \\gamma(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t(M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\quad \\times \\left(1 - \\lvert \\lambda_2(P) \\rvert + \\eta_{t-k_t}M\\right)\n\\end{align*}\n$$\n\nProof. Let $B_t = (I + [R]) B_{t-k_t}$ with $\\lVert R \\rVert^2 \\leq r$. Then,\n\n$$\n\\begin{align*}\nE[P_t] &amp;= E \\left[ \\begin{array}{c} B_{t-k_t}B^T_{t-k_t} \\\\ \\text{Tr} \\, G(A_t - \\Sigma) \\end{array} \\right] + E \\left[ \\begin{array}{c} \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G(A_t - \\Sigma) \\\\ \\text{Tr} \\, R^T G(A_t - \\Sigma) \\end{array} \\right] \\\\\n&amp= \\begin{array}{c} E \\left[ \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\right] \\\\ P_{t,1} \\end{array} + E \\begin{array}{c} \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G(A_t - \\Sigma)R \\\\ \\text{Tr} \\, R^T G(A_t - \\Sigma)R \\end{array}\n\\end{align*}\n$$\n\nLet\u2019s consider each of the terms above. Using Von-Neumann\u2019s trace inequality and S.25, we have,\n\n$$\n\\begin{align*}\nE[P_{t,1}] &amp;= E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}E[G(A_t - \\Sigma) \\, | \\, s_1, s_2, . . . , s_{t-k_t}] \\\\\n&amp\\leq E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G E[(A_t - \\Sigma) \\, | \\, s_{t-k_t}] \\\\\n&amp\\leq \\lVert G E[(A_t - \\Sigma) \\, | \\, s_{t-k_t}] \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\leq 2Mdmix(k_t) \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\text{ using S.22} \\\\\n&amp\\leq 2\\gamma\\eta^2_t M \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\nE[P_{t,2}] &amp;= E \\left[ E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\right] , E \\left[ R^T G(A_t - \\Sigma)U \\, | \\, s_1, s_2, . . . , s_{t-k_t} \\right] \\\\\n&amp\\leq R^T G(A_t - \\Sigma) \\, | \\, s_1, s_2, . . . , s_{t-k_t} \\, 2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp= E \\left[ R^T G(A_t - \\Sigma) \\, | \\, s_{t-k_t} \\right] \\, 2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\leq \\eta_{t-k_t} \\lVert G \\rVert^2 V \\lvert \\lambda_2(P) \\rvert^{t-k_t}M(M + \\lambda_1) + \\eta_{t-k_t}k^2_t M(M + \\lambda_1)^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\text{ using S.2} \\\\\n&amp\\quad \\times \\left(1 - \\lvert \\lambda_2(P) \\rvert + 8\\gamma\\eta^2_{24}\\right)\n\\end{align*}\n$$", "md": "Finally, using the bound on \u2225R\u22252 from Eq S.29, we have:\n\n$$\n\\begin{align*}\nE[T4] &amp;= E \\text{Tr} \\, U^T B_{j,i+k}RG (A_i - \\Sigma)^2 R^T B^T_{j,i+k}U \\\\\n&amp;\\leq (1 + \\epsilon)^2 k^2_{i+1}\\eta^2_{i+1}M^2(M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\\\\n&amp\\leq \\epsilon (1 + \\epsilon)^2 k_{i+1}\\eta_{i+1}M^2(M + \\lambda_1) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\text{ using } \\forall i, \\eta_ik_i(M + \\lambda_1) \\leq c\n\\end{align*}\n$$\nTherefore,\n\n$$\n\\begin{align*}\nE \\text{Tr} \\, U^T B_{j,i+1}G (A_i - \\Sigma)^2 B^T_{j,i+1}U \\quad (i) &amp;\\leq V + \\eta_{i+1}2\\gamma\\eta_{i+1}M^2 + (1 + \\epsilon)(2 + \\epsilon(1 + \\epsilon))k_{i+1}M^2(M + \\lambda_1) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U \\\\\n&amp= V + \\eta_{i+1}M^2(2\\gamma\\eta_{i+1} + (1 + \\epsilon)(2 + \\epsilon(1 + \\epsilon))k_{i+1}(M + \\lambda_1)) \\lVert G \\rVert^2 E \\text{Tr} \\, U^T B_{j,i+k}B^T_{j,i+k}U\n\\end{align*}\n$$\nwhere in (i), we used $dmix(k) = dmix(k_{i+1}) \\leq \\gamma\\eta^2_{i+1}$. Hence proved.\n\nLemma S.10. Let $\\forall i \\in [n]$, $\\eta_ik_i(M + \\lambda_1) \\leq \\epsilon$, $\\epsilon \\in (0, 1)$ and step-sizes $\\eta_i$ forms a non-increasing sequence. Further, let the step-sizes follow a slow-decay property, i.e, $\\forall i, \\eta_i \\leq \\eta_{i-k_i} \\leq 2\\eta_i$. Set $k_i := \\tau_{\\text{mix}}\\gamma\\eta^2_i$, $\\gamma \\in (0, 1]$. Let $G \\in \\mathbb{R}^{d \\times d}$ be a constant positive semi-definite matrix, and $P_t := \\text{Tr} \\, B_{t-1}B^T_{t-1}G(A_t - \\Sigma)$, then,\n\n$$\n\\begin{align*}\nE[P_t] &amp;\\leq \\eta_{t-k_t}2V \\lvert \\lambda_2(P) \\rvert^2 \\gamma(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t(M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\quad \\times \\left(1 - \\lvert \\lambda_2(P) \\rvert + \\eta_{t-k_t}M\\right)\n\\end{align*}\n$$\n\nProof. Let $B_t = (I + [R]) B_{t-k_t}$ with $\\lVert R \\rVert^2 \\leq r$. Then,\n\n$$\n\\begin{align*}\nE[P_t] &amp;= E \\left[ \\begin{array}{c} B_{t-k_t}B^T_{t-k_t} \\\\ \\text{Tr} \\, G(A_t - \\Sigma) \\end{array} \\right] + E \\left[ \\begin{array}{c} \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G(A_t - \\Sigma) \\\\ \\text{Tr} \\, R^T G(A_t - \\Sigma) \\end{array} \\right] \\\\\n&amp= \\begin{array}{c} E \\left[ \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\right] \\\\ P_{t,1} \\end{array} + E \\begin{array}{c} \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G(A_t - \\Sigma)R \\\\ \\text{Tr} \\, R^T G(A_t - \\Sigma)R \\end{array}\n\\end{align*}\n$$\n\nLet\u2019s consider each of the terms above. Using Von-Neumann\u2019s trace inequality and S.25, we have,\n\n$$\n\\begin{align*}\nE[P_{t,1}] &amp;= E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}E[G(A_t - \\Sigma) \\, | \\, s_1, s_2, . . . , s_{t-k_t}] \\\\\n&amp\\leq E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}G E[(A_t - \\Sigma) \\, | \\, s_{t-k_t}] \\\\\n&amp\\leq \\lVert G E[(A_t - \\Sigma) \\, | \\, s_{t-k_t}] \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\leq 2Mdmix(k_t) \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\text{ using S.22} \\\\\n&amp\\leq 2\\gamma\\eta^2_t M \\lVert G \\rVert^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t}\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\nE[P_{t,2}] &amp;= E \\left[ E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\right] , E \\left[ R^T G(A_t - \\Sigma)U \\, | \\, s_1, s_2, . . . , s_{t-k_t} \\right] \\\\\n&amp\\leq R^T G(A_t - \\Sigma) \\, | \\, s_1, s_2, . . . , s_{t-k_t} \\, 2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp= E \\left[ R^T G(A_t - \\Sigma) \\, | \\, s_{t-k_t} \\right] \\, 2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\\\\n&amp\\leq \\eta_{t-k_t} \\lVert G \\rVert^2 V \\lvert \\lambda_2(P) \\rvert^{t-k_t}M(M + \\lambda_1) + \\eta_{t-k_t}k^2_t M(M + \\lambda_1)^2 E \\text{Tr} \\, B_{t-k_t}B^T_{t-k_t} \\text{ using S.2} \\\\\n&amp\\quad \\times \\left(1 - \\lvert \\lambda_2(P) \\rvert + 8\\gamma\\eta^2_{24}\\right)\n\\end{align*}\n$$"}]}, {"page": 25, "text": " E [Pt,3] \u2264        \u03b7t\u2212kt \u2225G\u22252              V |\u03bb2 (P       )|              t\u2212ktktM (M + \u03bb1) + \u03b7t\u2212ktk2                        t M (M + \u03bb1)2                 E    Tr     Bt\u2212ktBT      t\u2212kt\n                                          1 \u2212    |\u03bb2 (P    )| + 8\u03b3\u03b72\n                    using similar steps as E [Pt,2]\n E [Pt,4] = E           Tr    Bt\u2212ktBT       t\u2212ktRT G(At \u2212              \u03a3)R\n              \u2264    r2M \u2225G\u22252 E              Tr     Bt\u2212ktBT      t\u2212kt\n              \u2264    (1 + \u03f5)2 \u03b72     t\u2212kt+1k2     t M (M + \u03bb1)2 \u2225G\u22252 E                      Tr     Bt\u2212ktBT       t\u2212kt        using Lemma 2\n              \u2264    (1 + \u03f5)2 \u03b72     t\u2212ktk2   t M (M + \u03bb1)2 \u2225G\u22252 E                      Tr     Bt\u2212ktBT      t\u2212kt\n Therefore we have,\n E [Pt]\n  \u2264   \u03b7t\u2212kt         2V |\u03bb2 (P        )|               2\u03b3\u03b7t + 16\u03b3\u03b72         t\u2212ktkt (M + \u03bb1) +                   2 + (1 + \u03f5)2            \u03b7t\u2212ktk2    t (M + \u03bb1)2                 \u2225G\u22252\n                   1 \u2212    |\u03bb2 (P     )| + M\n                          \u00d7 E      Tr     Bt\u2212ktBT      t\u2212kt\n  (i)                2V |\u03bb2 (P       )|                        2\u03b3 + 16\u03b3\u03b7tkt (M + \u03bb1) +                         2 + (1 + \u03f5)2            k2                            \u2225G\u22252\n  \u2264    \u03b7t\u2212kt        1 \u2212    |\u03bb2 (P     )| + \u03b7t\u2212ktM                                                                                        t (M + \u03bb1)2\n                          \u00d7 E      Tr     Bt\u2212ktBT      t\u2212kt\n  (ii)                2V |\u03bb2 (P       )|\n   \u2264    \u03b7t\u2212kt        1 \u2212    |\u03bb2 (P     )| + \u03b7t\u2212ktM              2\u03b3 (1 + 8\u03f5) +              2 + (1 + \u03f5)2            k2t (M + \u03bb1)2                 \u2225G\u22252 E        Tr     Bt\u2212ktBT      t\u2212kt\n where in (i) we used 2\u03b7t\u2212kt \u2264                     \u03b7t \u2264     \u03b7t\u2212kt along with \u03b7tkt (M + \u03bb1) \u2264                          \u03f5 in (ii). Hence proved.\n Lemma S.11. Let \u2200i \u2208                   [n], \u03b7iki (M + \u03bb1) \u2264                 \u03f5, \u03f5 \u2208    (0, 1) and \u03b7i forms a non-increasing sequence.\n Set ki := \u03c4mix          \u03b3\u03b72 i    , \u03b3 \u2208    (0, 1]. Let U \u2208          Rd\u00d7d be a constant matrix and Qt := Tr                              Bt\u22121BT     t\u22121(At \u2212         \u03a3)U(At \u2212           \u03a3)  .\nFurther, let the decay of the step-sizes be slow such that \u2200i, \u03b7i \u2264                                         \u03b7i\u2212ki \u2264       2\u03b7i. Then\n E [Qt] \u2264         V + \u03b7t\u2212kt+1M2 (2\u03b3\u03b7t + 2 (1 + \u03f5) (1 + \u03f5 (1 + \u03f5)) kt (M + \u03bb1))                                                 \u2225U\u22252 E         Tr     Bt\u2212ktBT      t\u2212kt\n where Bt is defined in 2.\n Proof. Let Bt = (I + R) Bt\u2212kt with \u2225R\u22252\u2264                                   r. Then,\n                    \uf8ee                                                                 \uf8f9          \uf8ee                                                                       \uf8f9\n E [Qt] = E         \uf8ef        Bt\u2212ktBT                                                  \uf8fa          \uf8ef         Bt\u2212ktBT                                                       \uf8fa\n                    \uf8f0Tr                    t\u2212kt(At \u2212         \u03a3)U(At \u2212           \u03a3)    \uf8fb   + E    \uf8f0Tr                    t\u2212ktRT (At \u2212            \u03a3)U(At \u2212           \u03a3)    \uf8fb\n                                                   Qt,1                                                                            Qt,2\n                          \uf8ee                                                                     \uf8f9          \uf8ee                                                                          \uf8f9\n                          \uf8ef                                                                     \uf8fa          \uf8ef        RBt\u2212ktBT                                                          \uf8fa\n                   + E    \uf8f0Tr       RBt\u2212ktBT        t\u2212kt(At \u2212         \u03a3)U(At \u2212           \u03a3)     \uf8fb  + E     \uf8f0Tr                       t\u2212ktRT (At \u2212           \u03a3)U(At \u2212           \u03a3)     \uf8fb\n                                                           Qt,3                                                                               Qt,4\n Let\u2019s consider each of the terms above. Using Von-Neumann\u2019s trace inequality and noting that\n  E\u03c0       (At \u2212      \u03a3)2      2 \u2264     V, we have\n                    E [Qt,1] = E           Tr     Bt\u2212ktBT      t\u2212ktE [(At \u2212           \u03a3)U(At \u2212           \u03a3)|s1, s2, . . . , st\u2212kt]\n                                  = E      Tr     Bt\u2212ktBT      t\u2212ktE [(At \u2212           \u03a3)U(At \u2212           \u03a3)|st\u2212kt]\n                                  \u2264   \u2225E [(At \u2212  E      \u03a3)U(At \u2212          \u03a3)|st\u2212kt]\u22252 E             Tr     Bt\u2212ktBT      t\u2212kt\n                                  \u2264   \u2225U\u22252             (At \u2212      \u03a3)2|st\u2212kt            2 E    Tr     Bt\u2212ktBT      t\u2212kt         using S.28\n                                  \u2264   \u2225U\u22252        V + 2dmix (kt) M2                 E    Tr    Bt\u2212ktBT       t\u2212kt\n                                  \u2264   \u2225U\u22252        V + 2\u03b3\u03b72      t M2       E    Tr     Bt\u2212ktBT      t\u2212kt\n                                                                                25", "md": "$$E [P_{t,3}] \\leq \\eta t - k_t \\lVert G \\rVert^2 V \\lvert \\lambda_2 (P) \\rvert t - k_t k_t M (M + \\lambda_1) + \\eta t - k_t k^2_t t M (M + \\lambda_1)^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n$$\\frac{1}{1 - \\lvert \\lambda_2 (P) \\rvert + 8\\gamma \\eta^2}$$\n\nusing similar steps as $$E [P_{t,2}]$$\n\n$$E [P_{t,4}] = E \\text{Tr} B_{t-k_t} B^T_{t-k_t} R^T G(At - \\Sigma)R$$\n\n$$\\leq r^2 M \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n$$\\leq (1 + \\epsilon)^2 \\eta^2 t - k_t + 1 k^2_t t M (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$ using Lemma 2\n\n$$\\leq (1 + \\epsilon)^2 \\eta^2 t - k_t k^2_t t M (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nTherefore we have,\n\n$$E [P_t] \\leq \\eta t - k_t 2V \\lvert \\lambda_2 (P) \\rvert 2\\gamma \\eta t + 16\\gamma \\eta^2 t - k_t k_t (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 \\eta t - k_t k^2_t t (M + \\lambda_1)^2 \\lVert G \\rVert^2$$\n\n$$\\frac{1}{1 - \\lvert \\lambda_2 (P) \\rvert + M} \\times E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n(i) $$2V \\lvert \\lambda_2 (P) \\rvert 2\\gamma + 16\\gamma \\eta t k_t (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 k^2 \\lVert G \\rVert^2$$\n\n$$\\leq \\eta t - k_t 1 - \\lvert \\lambda_2 (P) \\rvert + \\eta t - k_t M 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 t (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nwhere in (i) we used $$2\\eta t - k_t \\leq \\eta t \\leq \\eta t - k_t$$ along with $$\\eta t k_t (M + \\lambda_1) \\leq \\epsilon$$ in (ii). Hence proved.\n\nLemma S.11. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence.\n\nSet $$k_i := \\tau \\text{mix} \\gamma \\eta^2 i$$, $$\\gamma \\in (0, 1]$$. Let $$U \\in \\mathbb{R}^{d \\times d}$$ be a constant matrix and $$Q_t := \\text{Tr} B_{t-1} B^T_{t-1}(A_t - \\Sigma)U(A_t - \\Sigma)$$. Further, let the decay of the step-sizes be slow such that $$\\forall i, \\eta_i \\leq \\eta_{i-k_i} \\leq 2\\eta_i$$. Then\n\n$$E [Q_t] \\leq V + \\eta t - k_t + 1 M^2 (2\\gamma \\eta t + 2 (1 + \\epsilon) (1 + \\epsilon (1 + \\epsilon)) k_t (M + \\lambda_1)) \\lVert U \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nwhere $$B_t$$ is defined in 2.\n\nProof. Let $$B_t = (I + R) B_{t-k_t}$$ with $$\\lVert R \\rVert^2 \\leq r$$. Then,\n\n$$\\begin{align*}\nE [Q_t] & = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} (A_t - \\Sigma)U(A_t - \\Sigma) \\right] + E \\left[ \\text{Tr} R B_{t-k_t} B^T_{t-k_t} (A_t - \\Sigma)U(A_t - \\Sigma) \\right] \\\\\n& = Q_{t,1} + Q_{t,2} + Q_{t,3} + Q_{t,4}\n\\end{align*}$$\n\nLet\u2019s consider each of the terms above. Using Von-Neumann\u2019s trace inequality and noting that $$E_{\\pi} (A_t - \\Sigma)^2 \\leq V$$, we have\n\n$$\\begin{align*}\nE [Q_{t,1}] & = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} E [(A_t - \\Sigma)U(A_t - \\Sigma) \\lvert s_1, s_2, . . . , s_{t-k_t}] \\right] \\\\\n& = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} E [(A_t - \\Sigma)U(A_t - \\Sigma) \\lvert s_{t-k_t}] \\right] \\\\\n& \\leq \\lVert E [(A_t - E \\Sigma)U(A_t - E \\Sigma) \\lvert s_{t-k_t}] \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\\\\n& \\leq \\lVert U \\rVert^2 (A_t - \\Sigma)^2 \\lvert s_{t-k_t} \\rvert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\text{ using S.28} \\\\\n& \\leq \\lVert U \\rVert^2 V + 2d_{\\text{mix}} (k_t) M^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\\\\n& \\leq \\lVert U \\rVert^2 V + 2\\gamma \\eta^2 t M^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}\n\\end{align*}$$", "images": [], "items": [{"type": "text", "value": "$$E [P_{t,3}] \\leq \\eta t - k_t \\lVert G \\rVert^2 V \\lvert \\lambda_2 (P) \\rvert t - k_t k_t M (M + \\lambda_1) + \\eta t - k_t k^2_t t M (M + \\lambda_1)^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n$$\\frac{1}{1 - \\lvert \\lambda_2 (P) \\rvert + 8\\gamma \\eta^2}$$\n\nusing similar steps as $$E [P_{t,2}]$$\n\n$$E [P_{t,4}] = E \\text{Tr} B_{t-k_t} B^T_{t-k_t} R^T G(At - \\Sigma)R$$\n\n$$\\leq r^2 M \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n$$\\leq (1 + \\epsilon)^2 \\eta^2 t - k_t + 1 k^2_t t M (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$ using Lemma 2\n\n$$\\leq (1 + \\epsilon)^2 \\eta^2 t - k_t k^2_t t M (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nTherefore we have,\n\n$$E [P_t] \\leq \\eta t - k_t 2V \\lvert \\lambda_2 (P) \\rvert 2\\gamma \\eta t + 16\\gamma \\eta^2 t - k_t k_t (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 \\eta t - k_t k^2_t t (M + \\lambda_1)^2 \\lVert G \\rVert^2$$\n\n$$\\frac{1}{1 - \\lvert \\lambda_2 (P) \\rvert + M} \\times E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n(i) $$2V \\lvert \\lambda_2 (P) \\rvert 2\\gamma + 16\\gamma \\eta t k_t (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 k^2 \\lVert G \\rVert^2$$\n\n$$\\leq \\eta t - k_t 1 - \\lvert \\lambda_2 (P) \\rvert + \\eta t - k_t M 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 t (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nwhere in (i) we used $$2\\eta t - k_t \\leq \\eta t \\leq \\eta t - k_t$$ along with $$\\eta t k_t (M + \\lambda_1) \\leq \\epsilon$$ in (ii). Hence proved.\n\nLemma S.11. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence.\n\nSet $$k_i := \\tau \\text{mix} \\gamma \\eta^2 i$$, $$\\gamma \\in (0, 1]$$. Let $$U \\in \\mathbb{R}^{d \\times d}$$ be a constant matrix and $$Q_t := \\text{Tr} B_{t-1} B^T_{t-1}(A_t - \\Sigma)U(A_t - \\Sigma)$$. Further, let the decay of the step-sizes be slow such that $$\\forall i, \\eta_i \\leq \\eta_{i-k_i} \\leq 2\\eta_i$$. Then\n\n$$E [Q_t] \\leq V + \\eta t - k_t + 1 M^2 (2\\gamma \\eta t + 2 (1 + \\epsilon) (1 + \\epsilon (1 + \\epsilon)) k_t (M + \\lambda_1)) \\lVert U \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nwhere $$B_t$$ is defined in 2.\n\nProof. Let $$B_t = (I + R) B_{t-k_t}$$ with $$\\lVert R \\rVert^2 \\leq r$$. Then,\n\n$$\\begin{align*}\nE [Q_t] & = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} (A_t - \\Sigma)U(A_t - \\Sigma) \\right] + E \\left[ \\text{Tr} R B_{t-k_t} B^T_{t-k_t} (A_t - \\Sigma)U(A_t - \\Sigma) \\right] \\\\\n& = Q_{t,1} + Q_{t,2} + Q_{t,3} + Q_{t,4}\n\\end{align*}$$\n\nLet\u2019s consider each of the terms above. Using Von-Neumann\u2019s trace inequality and noting that $$E_{\\pi} (A_t - \\Sigma)^2 \\leq V$$, we have\n\n$$\\begin{align*}\nE [Q_{t,1}] & = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} E [(A_t - \\Sigma)U(A_t - \\Sigma) \\lvert s_1, s_2, . . . , s_{t-k_t}] \\right] \\\\\n& = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} E [(A_t - \\Sigma)U(A_t - \\Sigma) \\lvert s_{t-k_t}] \\right] \\\\\n& \\leq \\lVert E [(A_t - E \\Sigma)U(A_t - E \\Sigma) \\lvert s_{t-k_t}] \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\\\\n& \\leq \\lVert U \\rVert^2 (A_t - \\Sigma)^2 \\lvert s_{t-k_t} \\rvert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\text{ using S.28} \\\\\n& \\leq \\lVert U \\rVert^2 V + 2d_{\\text{mix}} (k_t) M^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\\\\n& \\leq \\lVert U \\rVert^2 V + 2\\gamma \\eta^2 t M^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}\n\\end{align*}$$", "md": "$$E [P_{t,3}] \\leq \\eta t - k_t \\lVert G \\rVert^2 V \\lvert \\lambda_2 (P) \\rvert t - k_t k_t M (M + \\lambda_1) + \\eta t - k_t k^2_t t M (M + \\lambda_1)^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n$$\\frac{1}{1 - \\lvert \\lambda_2 (P) \\rvert + 8\\gamma \\eta^2}$$\n\nusing similar steps as $$E [P_{t,2}]$$\n\n$$E [P_{t,4}] = E \\text{Tr} B_{t-k_t} B^T_{t-k_t} R^T G(At - \\Sigma)R$$\n\n$$\\leq r^2 M \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n$$\\leq (1 + \\epsilon)^2 \\eta^2 t - k_t + 1 k^2_t t M (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$ using Lemma 2\n\n$$\\leq (1 + \\epsilon)^2 \\eta^2 t - k_t k^2_t t M (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nTherefore we have,\n\n$$E [P_t] \\leq \\eta t - k_t 2V \\lvert \\lambda_2 (P) \\rvert 2\\gamma \\eta t + 16\\gamma \\eta^2 t - k_t k_t (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 \\eta t - k_t k^2_t t (M + \\lambda_1)^2 \\lVert G \\rVert^2$$\n\n$$\\frac{1}{1 - \\lvert \\lambda_2 (P) \\rvert + M} \\times E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\n(i) $$2V \\lvert \\lambda_2 (P) \\rvert 2\\gamma + 16\\gamma \\eta t k_t (M + \\lambda_1) + 2 + (1 + \\epsilon)^2 k^2 \\lVert G \\rVert^2$$\n\n$$\\leq \\eta t - k_t 1 - \\lvert \\lambda_2 (P) \\rvert + \\eta t - k_t M 2\\gamma (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2 t (M + \\lambda_1)^2 \\lVert G \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nwhere in (i) we used $$2\\eta t - k_t \\leq \\eta t \\leq \\eta t - k_t$$ along with $$\\eta t k_t (M + \\lambda_1) \\leq \\epsilon$$ in (ii). Hence proved.\n\nLemma S.11. Let $$\\forall i \\in [n], \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon, \\epsilon \\in (0, 1)$$ and $$\\eta_i$$ forms a non-increasing sequence.\n\nSet $$k_i := \\tau \\text{mix} \\gamma \\eta^2 i$$, $$\\gamma \\in (0, 1]$$. Let $$U \\in \\mathbb{R}^{d \\times d}$$ be a constant matrix and $$Q_t := \\text{Tr} B_{t-1} B^T_{t-1}(A_t - \\Sigma)U(A_t - \\Sigma)$$. Further, let the decay of the step-sizes be slow such that $$\\forall i, \\eta_i \\leq \\eta_{i-k_i} \\leq 2\\eta_i$$. Then\n\n$$E [Q_t] \\leq V + \\eta t - k_t + 1 M^2 (2\\gamma \\eta t + 2 (1 + \\epsilon) (1 + \\epsilon (1 + \\epsilon)) k_t (M + \\lambda_1)) \\lVert U \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}$$\n\nwhere $$B_t$$ is defined in 2.\n\nProof. Let $$B_t = (I + R) B_{t-k_t}$$ with $$\\lVert R \\rVert^2 \\leq r$$. Then,\n\n$$\\begin{align*}\nE [Q_t] & = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} (A_t - \\Sigma)U(A_t - \\Sigma) \\right] + E \\left[ \\text{Tr} R B_{t-k_t} B^T_{t-k_t} (A_t - \\Sigma)U(A_t - \\Sigma) \\right] \\\\\n& = Q_{t,1} + Q_{t,2} + Q_{t,3} + Q_{t,4}\n\\end{align*}$$\n\nLet\u2019s consider each of the terms above. Using Von-Neumann\u2019s trace inequality and noting that $$E_{\\pi} (A_t - \\Sigma)^2 \\leq V$$, we have\n\n$$\\begin{align*}\nE [Q_{t,1}] & = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} E [(A_t - \\Sigma)U(A_t - \\Sigma) \\lvert s_1, s_2, . . . , s_{t-k_t}] \\right] \\\\\n& = E \\left[ \\text{Tr} B_{t-k_t} B^T_{t-k_t} E [(A_t - \\Sigma)U(A_t - \\Sigma) \\lvert s_{t-k_t}] \\right] \\\\\n& \\leq \\lVert E [(A_t - E \\Sigma)U(A_t - E \\Sigma) \\lvert s_{t-k_t}] \\rVert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\\\\n& \\leq \\lVert U \\rVert^2 (A_t - \\Sigma)^2 \\lvert s_{t-k_t} \\rvert^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\text{ using S.28} \\\\\n& \\leq \\lVert U \\rVert^2 V + 2d_{\\text{mix}} (k_t) M^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t} \\\\\n& \\leq \\lVert U \\rVert^2 V + 2\\gamma \\eta^2 t M^2 E \\text{Tr} B_{t-k_t} B^T_{t-k_t}\n\\end{align*}$$"}]}, {"page": 26, "text": " E [Qt,2] = E        Tr    Bt\u2212ktBT     t\u2212ktE      RT (At \u2212       \u03a3)U(At \u2212        \u03a3)|s1, s2, . . . , st\u2212kt\n             \u2264    E    RT (At \u2212       \u03a3)U(At \u2212         \u03a3)|st\u2212kt        2 E    Tr    Bt\u2212ktBT     t\u2212kt\n             \u2264   (1 + \u03f5) \u03b7t\u2212kt+1ktM2 (M + \u03bb1) \u2225U\u22252 E                          Tr    Bt\u2212ktBT     t\u2212kt         using Lemma 2\n E [Qt,3] \u2264      (1 + \u03f5) \u03b7t\u2212kt+1ktM2 (M + \u03bb1) \u2225U\u22252 E                          Tr    Bt\u2212ktBT     t\u2212kt       using a similar argument as Qt,2\n E [Qt,4] = E        Tr    RBt\u2212ktBT       t\u2212ktRT (At \u2212         \u03a3)U(At \u2212         \u03a3)\n             = E     Tr    Bt\u2212ktBT     t\u2212ktRT (At \u2212         \u03a3)U(At \u2212         \u03a3)R\n             \u2264   r2 \u2225U\u22252 M2E            Tr    Bt\u2212ktBT     t\u2212kt\n             \u2264   (1 + \u03f5)2 \u03b72   t\u2212kt+1k2    t M2 (M + \u03bb1)2 \u2225U\u22252 E                  Tr    Bt\u2212ktBT     t\u2212kt       using Lemma 2\n Therefore, we have\n E\u2264[Qt] V + \u03b7t\u2212kt+1          2\u03b3\u03b7tM2 + 2 (1 + \u03f5) ktM2 (M + \u03bb1) + (1 + \u03f5)2 \u03b7t\u2212kt+1k2                                 t M2 (M + \u03bb1)2                 \u2225U\u22252 E       Tr  Bt\u2212ktBT  t\u2212kt\n  (i)\n  \u2264     V + \u03b7t\u2212kt+1M2  2\u03b3\u03b7t + 2 (1 + \u03f5) kt (M + \u03bb1) + 2\u03f5 (1 + \u03f5)2 kt (M + \u03bb1)                                               \u2225U\u22252 E       Tr    Bt\u2212ktBT     t\u2212kt\n  =    V + \u03b7t\u2212kt+1M2 (2\u03b3\u03b7t + 2 (1 + \u03f5) (1 + \u03f5 (1 + \u03f5)) kt (M + \u03bb1))                                    \u2225U\u22252 E        Tr    Bt\u2212ktBT    t\u2212kt\n In (i), we used the slow-decay assumption on \u03b7i mentioned in the lemma statement along with\n \u03b7iki (M + \u03bb1) \u2264           \u03f5. Hence proved.\n Lemma S.12. (Learning Rate Schedule) Fix any \u03b4 \u2208                               (0, 1). Set ki := \u03c4mix          \u03b72i  . Suppose the step\n sizes are set such that\n                                                       \u03b7i =                \u03b1\n Define the linear function                                    (\u03bb1 \u2212     \u03bb2) (\u03b2 + i)\n                 1                       \u2200i \u2208   [n], f (i) := 1     \u03b7i   = (\u03bb1 \u2212      \u03bb2)\u03b1(\u03b2 + i),\nWith \u03f5 :=       100 and \u03bek,t, \u03b6k,t, V\u2032, Vk,t defined in S.49, set \u03b1 > 2, f (0) \u2265                         e, m := 200 and\n \u03b2 := 600 max            \u03c4mix log (f (0)) (M + \u03bb1) \u03b1              , 5\u03c4mix log (f (0)) (M + \u03bb1)2 \u03b12                 ,            V\u2032 + 5\u03bb2    1   \u03b12\n then we have                           \u03bb1 \u2212    \u03bb2                       3 (\u03bb1 \u2212     \u03bb2)2 log      1 + \u03b4  m          300 (\u03bb1 \u2212      \u03bb2)2 log       1 + \u03b4 m\n     1. \u03b7iki (M + \u03bb1) \u2264             \u03f5\n     2. \u2200i,n\u03b7i \u2264       \u03b7i\u2212ki \u2264      (1 + 2\u03f5) \u03b7i \u2264        2\u03b7i (slow-decay)\n     3.   i=1    Vk,i + \u03b6k,i + 4\u03bb2      1   \u03b72i \u2264    log    1 + \u03b4 m\n     4.    n   (V\u2032 + \u03bek,i) \u03b72    i\u2212ki exp        \u2212      n    2\u03b7j (\u03bb1 \u2212       \u03bb2)      \u2264\n          i=1                                       j=i+1\n                       2 (1 + 10\u03f5) \u03b12                    V\u2032         1       24 (1 + 10\u03f5) \u03b13              M (M + \u03bb1)2            k2n\n                               2\u03b1 \u2212    1           (\u03bb1 \u2212    \u03bb2)2    n +             (\u03b1 \u2212    1)              (\u03bb1 \u2212    \u03bb2)3       n2\n                                                                       26", "md": "E [Q_{t,2}] = E \\left[ \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t E \\right] \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) | s_1, s_2, . . . , s_{t-k_t} \\\\\n\\leq E \\, \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) | s_{t-k_t}^2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\\\\n\\leq (1 + \\epsilon) \\eta_{t-k_t+1} k_t M^2 (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\quad \\text{using Lemma 2} \\\\\nE [Q_{t,3}] \\leq (1 + \\epsilon) \\eta_{t-k_t+1} k_t M^2 (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\quad \\text{using a similar argument as } Q_{t,2} \\\\\nE [Q_{t,4}] = E \\, \\text{Tr} \\, R B_t^{-k_t} B_t^T - k_t \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) \\\\\n= E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) R \\\\\n\\leq r^2 \\left\\| U \\right\\|_2 M^2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\\\\n\\leq (1 + \\epsilon)^2 \\eta^2_{t-k_t+1} k^2_t M^2 (M + \\lambda_1)^2 \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\quad \\text{using Lemma 2} \\\\\n\\text{Therefore, we have} \\\\\nE \\leq [Q_t] V + \\eta_{t-k_t+1} 2\\gamma \\eta_t M^2 + 2 (1 + \\epsilon) k_t M^2 (M + \\lambda_1) + (1 + \\epsilon)^2 \\eta_{t-k_t+1} k^2_t t M^2 (M + \\lambda_1)^2 \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n(i) \\\\\n\\leq V + \\eta_{t-k_t+1} M^2 2\\gamma \\eta_t + 2 (1 + \\epsilon) k_t (M + \\lambda_1) + 2\\epsilon (1 + \\epsilon)^2 k_t (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n= V + \\eta_{t-k_t+1} M^2 (2\\gamma \\eta_t + 2 (1 + \\epsilon) (1 + \\epsilon (1 + \\epsilon)) k_t (M + \\lambda_1)) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n\\text{In (i), we used the slow-decay assumption on } \\eta_i \\text{ mentioned in the lemma statement along with} \\\\\n\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon. \\text{Hence proved.} \\\\\n\\text{Lemma S.12. (Learning Rate Schedule) Fix any } \\delta \\in (0, 1). \\text{Set } k_i := \\tau \\text{mix} \\eta^2_i. \\text{Suppose the step} \\\\\n\\text{sizes are set such that} \\\\\n\\eta_i = \\alpha \\\\\n\\text{Define the linear function} \\\\\n\\frac{1}{\\eta_i} = \\frac{( \\lambda_1 - \\lambda_2) ( \\beta + i)}{1} \\quad \\forall i \\in [n], f (i) := \\frac{1}{\\eta_i} = ( \\lambda_1 - \\lambda_2) \\alpha ( \\beta + i), \\\\\n\\text{With } \\epsilon := 100 \\text{ and } \\xi_{k,t}, \\zeta_{k,t}, V', V_{k,t} \\text{ defined in S.49, set } \\alpha > 2, f (0) \\geq e, m := 200 \\text{ and} \\\\\n\\beta := 600 \\max \\left( \\tau \\text{mix} \\log (f (0)) (M + \\lambda_1) \\alpha, 5 \\tau \\text{mix} \\log (f (0)) (M + \\lambda_1)^2 \\alpha^2, V' + 5 \\lambda_2 \\frac{1}{\\alpha^2} \\right) \\\\\n\\text{then we have} \\\\\n\\lambda_1 - \\lambda_2 \\frac{3 ( \\lambda_1 - \\lambda_2)^2 \\log (1 + \\delta) m}{300 ( \\lambda_1 - \\lambda_2)^2 \\log (1 + \\delta) m} \\\\\n1. \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon \\\\\n2. \\forall i, n \\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i \\text{ (slow-decay)} \\\\\n3. \\sum_{i=1}^{n} V_{k,i} + \\zeta_{k,i} + 4 \\lambda_2 \\frac{1}{\\eta^2_i} \\leq \\log (1 + \\delta) m \\\\\n4. \\sum_{i=1}^{n} \\left( V' + \\xi_{k,i} \\right) \\eta^2_{i-k_i} \\exp \\left( - \\sum_{j=i+1}^{n} 2 \\eta_j ( \\lambda_1 - \\lambda_2) \\right) \\leq \\\\\n\\frac{2 (1 + 10\\epsilon) \\alpha^2 V' \\frac{1}{24 (1 + 10\\epsilon) \\alpha^3} M (M + \\lambda_1)^2 k^2_n}{2\\alpha - 1 ( \\lambda_1 - \\lambda_2)^2 n + ( \\alpha - 1) ( \\lambda_1 - \\lambda_2)^3 n^2} \\\\\n26", "images": [], "items": [{"type": "text", "value": "E [Q_{t,2}] = E \\left[ \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t E \\right] \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) | s_1, s_2, . . . , s_{t-k_t} \\\\\n\\leq E \\, \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) | s_{t-k_t}^2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\\\\n\\leq (1 + \\epsilon) \\eta_{t-k_t+1} k_t M^2 (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\quad \\text{using Lemma 2} \\\\\nE [Q_{t,3}] \\leq (1 + \\epsilon) \\eta_{t-k_t+1} k_t M^2 (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\quad \\text{using a similar argument as } Q_{t,2} \\\\\nE [Q_{t,4}] = E \\, \\text{Tr} \\, R B_t^{-k_t} B_t^T - k_t \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) \\\\\n= E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) R \\\\\n\\leq r^2 \\left\\| U \\right\\|_2 M^2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\\\\n\\leq (1 + \\epsilon)^2 \\eta^2_{t-k_t+1} k^2_t M^2 (M + \\lambda_1)^2 \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\quad \\text{using Lemma 2} \\\\\n\\text{Therefore, we have} \\\\\nE \\leq [Q_t] V + \\eta_{t-k_t+1} 2\\gamma \\eta_t M^2 + 2 (1 + \\epsilon) k_t M^2 (M + \\lambda_1) + (1 + \\epsilon)^2 \\eta_{t-k_t+1} k^2_t t M^2 (M + \\lambda_1)^2 \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n(i) \\\\\n\\leq V + \\eta_{t-k_t+1} M^2 2\\gamma \\eta_t + 2 (1 + \\epsilon) k_t (M + \\lambda_1) + 2\\epsilon (1 + \\epsilon)^2 k_t (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n= V + \\eta_{t-k_t+1} M^2 (2\\gamma \\eta_t + 2 (1 + \\epsilon) (1 + \\epsilon (1 + \\epsilon)) k_t (M + \\lambda_1)) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n\\text{In (i), we used the slow-decay assumption on } \\eta_i \\text{ mentioned in the lemma statement along with} \\\\\n\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon. \\text{Hence proved.} \\\\\n\\text{Lemma S.12. (Learning Rate Schedule) Fix any } \\delta \\in (0, 1). \\text{Set } k_i := \\tau \\text{mix} \\eta^2_i. \\text{Suppose the step} \\\\\n\\text{sizes are set such that} \\\\\n\\eta_i = \\alpha \\\\\n\\text{Define the linear function} \\\\\n\\frac{1}{\\eta_i} = \\frac{( \\lambda_1 - \\lambda_2) ( \\beta + i)}{1} \\quad \\forall i \\in [n], f (i) := \\frac{1}{\\eta_i} = ( \\lambda_1 - \\lambda_2) \\alpha ( \\beta + i), \\\\\n\\text{With } \\epsilon := 100 \\text{ and } \\xi_{k,t}, \\zeta_{k,t}, V', V_{k,t} \\text{ defined in S.49, set } \\alpha > 2, f (0) \\geq e, m := 200 \\text{ and} \\\\\n\\beta := 600 \\max \\left( \\tau \\text{mix} \\log (f (0)) (M + \\lambda_1) \\alpha, 5 \\tau \\text{mix} \\log (f (0)) (M + \\lambda_1)^2 \\alpha^2, V' + 5 \\lambda_2 \\frac{1}{\\alpha^2} \\right) \\\\\n\\text{then we have} \\\\\n\\lambda_1 - \\lambda_2 \\frac{3 ( \\lambda_1 - \\lambda_2)^2 \\log (1 + \\delta) m}{300 ( \\lambda_1 - \\lambda_2)^2 \\log (1 + \\delta) m} \\\\\n1. \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon \\\\\n2. \\forall i, n \\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i \\text{ (slow-decay)} \\\\\n3. \\sum_{i=1}^{n} V_{k,i} + \\zeta_{k,i} + 4 \\lambda_2 \\frac{1}{\\eta^2_i} \\leq \\log (1 + \\delta) m \\\\\n4. \\sum_{i=1}^{n} \\left( V' + \\xi_{k,i} \\right) \\eta^2_{i-k_i} \\exp \\left( - \\sum_{j=i+1}^{n} 2 \\eta_j ( \\lambda_1 - \\lambda_2) \\right) \\leq \\\\\n\\frac{2 (1 + 10\\epsilon) \\alpha^2 V' \\frac{1}{24 (1 + 10\\epsilon) \\alpha^3} M (M + \\lambda_1)^2 k^2_n}{2\\alpha - 1 ( \\lambda_1 - \\lambda_2)^2 n + ( \\alpha - 1) ( \\lambda_1 - \\lambda_2)^3 n^2} \\\\\n26", "md": "E [Q_{t,2}] = E \\left[ \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t E \\right] \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) | s_1, s_2, . . . , s_{t-k_t} \\\\\n\\leq E \\, \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) | s_{t-k_t}^2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\\\\n\\leq (1 + \\epsilon) \\eta_{t-k_t+1} k_t M^2 (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\quad \\text{using Lemma 2} \\\\\nE [Q_{t,3}] \\leq (1 + \\epsilon) \\eta_{t-k_t+1} k_t M^2 (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\quad \\text{using a similar argument as } Q_{t,2} \\\\\nE [Q_{t,4}] = E \\, \\text{Tr} \\, R B_t^{-k_t} B_t^T - k_t \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) \\\\\n= E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\text{RT} \\left( A_t - \\Sigma \\right) U \\left( A_t - \\Sigma \\right) R \\\\\n\\leq r^2 \\left\\| U \\right\\|_2 M^2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T - k_t \\\\\n\\leq (1 + \\epsilon)^2 \\eta^2_{t-k_t+1} k^2_t M^2 (M + \\lambda_1)^2 \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\quad \\text{using Lemma 2} \\\\\n\\text{Therefore, we have} \\\\\nE \\leq [Q_t] V + \\eta_{t-k_t+1} 2\\gamma \\eta_t M^2 + 2 (1 + \\epsilon) k_t M^2 (M + \\lambda_1) + (1 + \\epsilon)^2 \\eta_{t-k_t+1} k^2_t t M^2 (M + \\lambda_1)^2 \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n(i) \\\\\n\\leq V + \\eta_{t-k_t+1} M^2 2\\gamma \\eta_t + 2 (1 + \\epsilon) k_t (M + \\lambda_1) + 2\\epsilon (1 + \\epsilon)^2 k_t (M + \\lambda_1) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n= V + \\eta_{t-k_t+1} M^2 (2\\gamma \\eta_t + 2 (1 + \\epsilon) (1 + \\epsilon (1 + \\epsilon)) k_t (M + \\lambda_1)) \\left\\| U \\right\\|_2 E \\, \\text{Tr} \\, B_t^{-k_t} B_t^T \\\\\n\\text{In (i), we used the slow-decay assumption on } \\eta_i \\text{ mentioned in the lemma statement along with} \\\\\n\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon. \\text{Hence proved.} \\\\\n\\text{Lemma S.12. (Learning Rate Schedule) Fix any } \\delta \\in (0, 1). \\text{Set } k_i := \\tau \\text{mix} \\eta^2_i. \\text{Suppose the step} \\\\\n\\text{sizes are set such that} \\\\\n\\eta_i = \\alpha \\\\\n\\text{Define the linear function} \\\\\n\\frac{1}{\\eta_i} = \\frac{( \\lambda_1 - \\lambda_2) ( \\beta + i)}{1} \\quad \\forall i \\in [n], f (i) := \\frac{1}{\\eta_i} = ( \\lambda_1 - \\lambda_2) \\alpha ( \\beta + i), \\\\\n\\text{With } \\epsilon := 100 \\text{ and } \\xi_{k,t}, \\zeta_{k,t}, V', V_{k,t} \\text{ defined in S.49, set } \\alpha > 2, f (0) \\geq e, m := 200 \\text{ and} \\\\\n\\beta := 600 \\max \\left( \\tau \\text{mix} \\log (f (0)) (M + \\lambda_1) \\alpha, 5 \\tau \\text{mix} \\log (f (0)) (M + \\lambda_1)^2 \\alpha^2, V' + 5 \\lambda_2 \\frac{1}{\\alpha^2} \\right) \\\\\n\\text{then we have} \\\\\n\\lambda_1 - \\lambda_2 \\frac{3 ( \\lambda_1 - \\lambda_2)^2 \\log (1 + \\delta) m}{300 ( \\lambda_1 - \\lambda_2)^2 \\log (1 + \\delta) m} \\\\\n1. \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon \\\\\n2. \\forall i, n \\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i \\text{ (slow-decay)} \\\\\n3. \\sum_{i=1}^{n} V_{k,i} + \\zeta_{k,i} + 4 \\lambda_2 \\frac{1}{\\eta^2_i} \\leq \\log (1 + \\delta) m \\\\\n4. \\sum_{i=1}^{n} \\left( V' + \\xi_{k,i} \\right) \\eta^2_{i-k_i} \\exp \\left( - \\sum_{j=i+1}^{n} 2 \\eta_j ( \\lambda_1 - \\lambda_2) \\right) \\leq \\\\\n\\frac{2 (1 + 10\\epsilon) \\alpha^2 V' \\frac{1}{24 (1 + 10\\epsilon) \\alpha^3} M (M + \\lambda_1)^2 k^2_n}{2\\alpha - 1 ( \\lambda_1 - \\lambda_2)^2 n + ( \\alpha - 1) ( \\lambda_1 - \\lambda_2)^3 n^2} \\\\\n26"}]}, {"page": 27, "text": "Proof. We use the following inequalities -\n               t  \u03b72                      \u03b12                        Using        1          \u221e         1                             (S.30)\n             j=i    j \u2264    (\u03bb1 \u2212    \u03bb2)2 (\u03b2 + i \u2212        1)                   x + 1 \u2264      i=1   (x + i)2 \u2264       x1\n               t  \u03b7j \u2265     (\u03bb1 \u2212 \u03b1  \u03bb2) log     t + \u03b2i++1\u03b2                                                                          (S.31)\n             j=i\n               t  \u03b7j \u2264     (\u03bb1 \u2212 \u03b1  \u03bb2) log       i +t\u03b2+\u2212\u03b2    1                                                                     (S.32)\n             j=i\n               t\n              (j + \u03b2)\u2113         \u2264   (t + \u03b2 + 1)\u2113+1 \u2212          (i + \u03b2)\u2113+1       \u2264   (t + \u03b2 + 1)\u2113+1         \u2200  \u2113 > 0                   (S.33)\n             j=i                                    \u2113 + 1                                \u2113 + 1\nFor the first result, we observe that f(x) = log(x)                x     is a decreasing function of x for x \u2265                  e. Using\nproperties of the mixing time (see Section 2.1 in the manuscript), we have\n   ki := \u03c4mix      \u03b72i   \u2264     2\u03c4mix             1      = 4\u03c4mix              (\u03b2 + i) (\u03bb1 \u2212         \u03bb2)      = 4\u03c4mix\n                             log (2) log        \u03b72i         log (2) log                   \u03b1                     log (2) log (f (i))\n                                                                                                                                    (S.34)\nfor \u03b7i < 1. For i \u2265         0                 f (i) \u2265     f (0) = \u03b2 (\u03bb1 \u2212    \u03b1    \u03bb2)    \u2265  e\nTherefore,\n             \u03b7iki (M + \u03bb1) \u2264            4\u03c4mix (M + \u03bb1)                     \u03b1                    (\u03b2 + i) (\u03bb1 \u2212          \u03bb2)\n                                               log (2)          (\u03b2 + i) (\u03bb1 \u2212       \u03bb2) log                   \u03b1\n                                    = 4\u03c4mix (M + \u03bb1)            log (f (i))\n                                               log (2)              f (i)\n                                    \u2264   4\u03c4mix (M + \u03bb1)          log (f (0))\n                                               log (2)              f (0)\nFrom the assumptions mentioned in the Lemma statement, we have\n                                log (f (0))                \u03f5 log (2)                      log (2)\nTherefore,                          f (0)        <   4\u03c4mix (M + \u03bb1) =            400\u03c4mix (M + \u03bb1)                                   (S.35)\n                                                     \u2200  i, \u03b7iki (M + \u03bb1) \u2264           \u03f5                                              (S.36)\nFor the second result, we note that \u2200i \u2208                 [n],\n                                                     \u03b7i\u2212ki    =       \u03b2 + i\n                                                       \u03b7i         \u03b2 + i \u2212     ki\n                                                              = 1 +           ki\n                                                              = 1 +     \u03b2 + i1\u2212     ki\n                                                                         \u03b2+i\n                                                                          ki \u2212    1\n                                                                    27", "md": "# Math Equations\n\nProof. We use the following inequalities -\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\leq (\\lambda_{1} - \\lambda_{2})^{2} (\\beta + i - 1) \\quad \\text{(S.30)}\n$$\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\geq (\\lambda_{1} - \\alpha \\lambda_{2}) \\log t + \\beta i++1\\beta \\quad \\text{(S.31)}\n$$\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\leq (\\lambda_{1} - \\alpha \\lambda_{2}) \\log i + t\\beta+-\\beta 1 \\quad \\text{(S.32)}\n$$\n\n$$\n(t(j + \\beta)^{\\ell} \\leq (t + \\beta + 1)^{\\ell+1} - (i + \\beta)^{\\ell+1} \\leq (t + \\beta + 1)^{\\ell+1} \\quad \\forall \\ell > 0 \\quad \\text{(S.33)}\n$$\n\nFor the first result, we observe that \\( f(x) = \\log(x) \\frac{x}{e} \\) is a decreasing function of x for \\( x \\geq e \\). Using properties of the mixing time (see Section 2.1 in the manuscript), we have\n\n$$\nk_{i} := \\tau_{\\text{mix}} \\eta_{2i} \\leq 2\\tau_{\\text{mix}} 1 = 4\\tau_{\\text{mix}} (\\beta + i) (\\lambda_{1} - \\lambda_{2}) = 4\\tau_{\\text{mix}} \\log(2) \\log \\eta_{2i} \\log(2) \\log \\alpha \\log(2) \\log(f(i)) \\quad \\text{(S.34)}\n$$\n\nFor \\( \\eta_{i} < 1 \\), for \\( i \\geq 0 \\), \\( f(i) \\geq f(0) = \\beta (\\lambda_{1} - \\alpha \\lambda_{2}) \\geq e \\). Therefore,\n\n$$\n\\eta_{i} k_{i} (M + \\lambda_{1}) \\leq 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\alpha (\\beta + i) (\\lambda_{1} - \\lambda_{2}) \\log(2) (\\beta + i) (\\lambda_{1} - \\lambda_{2}) \\log \\alpha = 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\log(f(i)) \\log(2) f(i) \\leq 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\log(f(0)) \\log(2) f(0) \\quad \\text{(S.35)}\n$$\n\nFrom the assumptions mentioned in the Lemma statement, we have\n\n$$\n\\log(f(0)) < \\epsilon \\log(2) \\log(2) \\quad \\text{Therefore,} \\quad f(0) < 4\\tau_{\\text{mix}} (M + \\lambda_{1}) = 400\\tau_{\\text{mix}} (M + \\lambda_{1}) \\quad \\text{(S.36)}\n$$\n\nFor the second result, we note that \\( \\forall i \\in [n] \\),\n\n$$\n\\frac{\\eta_{i}-k_{i}}{\\eta_{i}} = \\beta + i \\frac{\\beta + i - k_{i}}{k_{i}} = 1 + \\frac{k_{i}}{\\beta + i} = 1 + \\frac{\\beta + i}{1 - k_{i}} \\quad \\text{(S.37)}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Proof. We use the following inequalities -\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\leq (\\lambda_{1} - \\lambda_{2})^{2} (\\beta + i - 1) \\quad \\text{(S.30)}\n$$\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\geq (\\lambda_{1} - \\alpha \\lambda_{2}) \\log t + \\beta i++1\\beta \\quad \\text{(S.31)}\n$$\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\leq (\\lambda_{1} - \\alpha \\lambda_{2}) \\log i + t\\beta+-\\beta 1 \\quad \\text{(S.32)}\n$$\n\n$$\n(t(j + \\beta)^{\\ell} \\leq (t + \\beta + 1)^{\\ell+1} - (i + \\beta)^{\\ell+1} \\leq (t + \\beta + 1)^{\\ell+1} \\quad \\forall \\ell > 0 \\quad \\text{(S.33)}\n$$\n\nFor the first result, we observe that \\( f(x) = \\log(x) \\frac{x}{e} \\) is a decreasing function of x for \\( x \\geq e \\). Using properties of the mixing time (see Section 2.1 in the manuscript), we have\n\n$$\nk_{i} := \\tau_{\\text{mix}} \\eta_{2i} \\leq 2\\tau_{\\text{mix}} 1 = 4\\tau_{\\text{mix}} (\\beta + i) (\\lambda_{1} - \\lambda_{2}) = 4\\tau_{\\text{mix}} \\log(2) \\log \\eta_{2i} \\log(2) \\log \\alpha \\log(2) \\log(f(i)) \\quad \\text{(S.34)}\n$$\n\nFor \\( \\eta_{i} < 1 \\), for \\( i \\geq 0 \\), \\( f(i) \\geq f(0) = \\beta (\\lambda_{1} - \\alpha \\lambda_{2}) \\geq e \\). Therefore,\n\n$$\n\\eta_{i} k_{i} (M + \\lambda_{1}) \\leq 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\alpha (\\beta + i) (\\lambda_{1} - \\lambda_{2}) \\log(2) (\\beta + i) (\\lambda_{1} - \\lambda_{2}) \\log \\alpha = 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\log(f(i)) \\log(2) f(i) \\leq 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\log(f(0)) \\log(2) f(0) \\quad \\text{(S.35)}\n$$\n\nFrom the assumptions mentioned in the Lemma statement, we have\n\n$$\n\\log(f(0)) < \\epsilon \\log(2) \\log(2) \\quad \\text{Therefore,} \\quad f(0) < 4\\tau_{\\text{mix}} (M + \\lambda_{1}) = 400\\tau_{\\text{mix}} (M + \\lambda_{1}) \\quad \\text{(S.36)}\n$$\n\nFor the second result, we note that \\( \\forall i \\in [n] \\),\n\n$$\n\\frac{\\eta_{i}-k_{i}}{\\eta_{i}} = \\beta + i \\frac{\\beta + i - k_{i}}{k_{i}} = 1 + \\frac{k_{i}}{\\beta + i} = 1 + \\frac{\\beta + i}{1 - k_{i}} \\quad \\text{(S.37)}\n$$", "md": "Proof. We use the following inequalities -\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\leq (\\lambda_{1} - \\lambda_{2})^{2} (\\beta + i - 1) \\quad \\text{(S.30)}\n$$\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\geq (\\lambda_{1} - \\alpha \\lambda_{2}) \\log t + \\beta i++1\\beta \\quad \\text{(S.31)}\n$$\n\n$$\n\\sum_{j=i}^{t} \\eta_{j} \\leq (\\lambda_{1} - \\alpha \\lambda_{2}) \\log i + t\\beta+-\\beta 1 \\quad \\text{(S.32)}\n$$\n\n$$\n(t(j + \\beta)^{\\ell} \\leq (t + \\beta + 1)^{\\ell+1} - (i + \\beta)^{\\ell+1} \\leq (t + \\beta + 1)^{\\ell+1} \\quad \\forall \\ell > 0 \\quad \\text{(S.33)}\n$$\n\nFor the first result, we observe that \\( f(x) = \\log(x) \\frac{x}{e} \\) is a decreasing function of x for \\( x \\geq e \\). Using properties of the mixing time (see Section 2.1 in the manuscript), we have\n\n$$\nk_{i} := \\tau_{\\text{mix}} \\eta_{2i} \\leq 2\\tau_{\\text{mix}} 1 = 4\\tau_{\\text{mix}} (\\beta + i) (\\lambda_{1} - \\lambda_{2}) = 4\\tau_{\\text{mix}} \\log(2) \\log \\eta_{2i} \\log(2) \\log \\alpha \\log(2) \\log(f(i)) \\quad \\text{(S.34)}\n$$\n\nFor \\( \\eta_{i} < 1 \\), for \\( i \\geq 0 \\), \\( f(i) \\geq f(0) = \\beta (\\lambda_{1} - \\alpha \\lambda_{2}) \\geq e \\). Therefore,\n\n$$\n\\eta_{i} k_{i} (M + \\lambda_{1}) \\leq 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\alpha (\\beta + i) (\\lambda_{1} - \\lambda_{2}) \\log(2) (\\beta + i) (\\lambda_{1} - \\lambda_{2}) \\log \\alpha = 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\log(f(i)) \\log(2) f(i) \\leq 4\\tau_{\\text{mix}} (M + \\lambda_{1}) \\log(f(0)) \\log(2) f(0) \\quad \\text{(S.35)}\n$$\n\nFrom the assumptions mentioned in the Lemma statement, we have\n\n$$\n\\log(f(0)) < \\epsilon \\log(2) \\log(2) \\quad \\text{Therefore,} \\quad f(0) < 4\\tau_{\\text{mix}} (M + \\lambda_{1}) = 400\\tau_{\\text{mix}} (M + \\lambda_{1}) \\quad \\text{(S.36)}\n$$\n\nFor the second result, we note that \\( \\forall i \\in [n] \\),\n\n$$\n\\frac{\\eta_{i}-k_{i}}{\\eta_{i}} = \\beta + i \\frac{\\beta + i - k_{i}}{k_{i}} = 1 + \\frac{k_{i}}{\\beta + i} = 1 + \\frac{\\beta + i}{1 - k_{i}} \\quad \\text{(S.37)}\n$$"}]}, {"page": 28, "text": "Consider the fraction \u03b2+i       ki . We can simplify it as :\n                                            \u03b2 + i                             \u03b2 + i\n                                                     \u2265    log (2)           (\u03b2+i)(\u03bb1\u2212\u03bb2)\n                                               ki          4\u03c4mix    log             \u03b1\n                                                     =        \u03b1 log (2)              f (i)\n                                                          4\u03c4mix (\u03bb1 \u2212      \u03bb2)   log (f (i))\n                                                              \u03b1 log (2)              f (0)\n                                                     \u2265    4\u03c4mix (\u03bb1 \u2212      \u03bb2)   log (f (0))\n                                                     \u2265    1\n                                                          \u03f5 from S.35\n                                           x\nwhere we used the fact that             log(x) is an increasing function for x \u2265                e. Therefore, we have that\n                                                \u03b7i\u2212ki    \u2264   1 +    1  1\n                                                   \u03b7i    =      1   \u03f5 \u2212   1\n                                                             1 \u2212    \u03f5\nFor the third result, we note that                       \u2264   1 + 2\u03f5 for \u03f5 \u2208       (0, 0.1)\n      \u03b6k,t := 40kt+1 (M + \u03bb1)2 ,\n      \u03bek,t := 2\u03b7tM          3 + 9k2   t+1 (M + \u03bb1)2\n            \u2264   24\u03b7tM        k2t+1 (M + \u03bb1)2            since (M + \u03bb1) \u2265             1 WLOG\nTherefore,  \u2264   24\u03f5 (1 + \u03f5) kt+1 (M + \u03bb1)2 since \u03b7t \u2264                    (1 + 2\u03f5) \u03b7t+1 and \u03b7t+1kt+1 (M + \u03bb1) \u2264                  \u03f5\n                    n                                                 n                                  n\n                   i=1    Vk,i + \u03b6k,i      \u03b72i =     V\u2032 + 5\u03bb2    1   i=1  \u03b72i + 41 (M + \u03bb1)2            i=1  \u03b72i ki+1\n                                                (i)                    n                                n\n                                                \u2264     V\u2032 + 5\u03bb2             \u03b72  +45 (M + \u03bb1)2                \u03b72                  (S.37)\n                                                                 1          i                                 i+1ki+1\nwhere (i) follows from the slow decay property of \u03b7i.                i=1T1                             i=1     T2\nFor T1, using S.30 we have,\n                                                        T1 \u2264     (\u03bb1 \u2212  \u03b12\u03bb2)2 \u03b2                                                (S.38)\nFor T2, substituting the value of ki from S.34 for \u03b7i < 1 we have,\n    T2 :=      n   \u03b72i+1ki+1 \u2264        4\u03c4mix       n                    \u03b1                  2  log    (\u03bb1 \u2212      \u03bb2) (\u03b2 + i + 1)\n              i=1                    log (2)    i=1      (\u03bb1 \u2212    \u03bb2) (\u03b2 + i + 1)                                   \u03b1           (S.39)\n                                 = 4\u03c4mix          n   log (f (i + 1))                                                           (S.40)\n                                     log (2)    i=1      f (i + 1)2\n                                                                    28", "md": "Consider the fraction $$\\frac{\\beta + i}{ki}$$. We can simplify it as:\n\n$$\n\\frac{\\beta + i}{ki} \\geq \\frac{\\log(2)}{4\\tau_{\\text{mix}}} = \\frac{\\alpha \\log(2) f(i)}{4\\tau_{\\text{mix}}(\\lambda_1 - \\lambda_2) \\log(f(i))} \\geq \\frac{4\\tau_{\\text{mix}}(\\lambda_1 - \\lambda_2) \\log(f(0))}{\\alpha \\log(2) f(0)} \\geq 1 \\quad \\text{for} \\quad \\epsilon \\quad \\text{from S.35}\n$$\n\nwhere we used the fact that $$\\log(x)$$ is an increasing function for $$x \\geq e$$. Therefore, we have that\n\n$$\n\\frac{\\eta_i - ki}{\\eta_i} \\leq 1 + \\frac{1}{1 - \\epsilon}\n$$\n\nFor the third result, we note that\n\n$$\n\\zeta_{k,t} := \\frac{40kt+1}{(M + \\lambda_1)^2}, \\quad \\xi_{k,t} := \\frac{2\\eta_tM}{3 + 9k^2t+1(M + \\lambda_1)^2} \\leq 24\\eta_tM \\frac{k^2t+1}{(M + \\lambda_1)^2} \\quad \\text{since} \\quad (M + \\lambda_1) \\geq 1 \\quad \\text{WLOG}\n$$\n\nTherefore,\n\n$$\n\\leq 24\\epsilon(1 + \\epsilon)k^{t+1}(M + \\lambda_1)^2 \\quad \\text{since} \\quad \\eta_t \\leq (1 + 2\\epsilon)\\eta_{t+1} \\quad \\text{and} \\quad \\eta_{t+1}k^{t+1}(M + \\lambda_1) \\leq \\epsilon\n$$\n\n$$\n\\sum_{i=1}^{n} (V_{k,i} + \\zeta_{k,i})\\eta_{2i} = V' + 5\\lambda_2 \\leq \\sum_{i=1}^{n} \\eta_{2i} + 45(M + \\lambda_1)^2 \\eta_{2i}k^{i+1} \\quad \\text{(S.37)}\n$$\n\nwhere $$(i)$$ follows from the slow decay property of $$\\eta_i$$. For $$T1$$, using S.30 we have,\n\n$$\nT1 \\leq (\\lambda_1 - \\alpha^2\\lambda_2)^2 \\beta \\quad \\text{(S.38)}\n$$\n\nFor $$T2$$, substituting the value of $$k_i$$ from S.34 for $$\\eta_i < 1$$ we have,\n\n$$\nT2 := \\sum_{i=1}^{n} \\eta_{2i+1}k^{i+1} \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2)} \\sum_{i=1}^{n} \\alpha^2 \\log(\\lambda_1 - \\lambda_2)(\\beta + i + 1) \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2)} \\sum_{i=1}^{n} \\log(f(i + 1)) \\quad \\text{(S.40)}\n$$", "images": [], "items": [{"type": "text", "value": "Consider the fraction $$\\frac{\\beta + i}{ki}$$. We can simplify it as:\n\n$$\n\\frac{\\beta + i}{ki} \\geq \\frac{\\log(2)}{4\\tau_{\\text{mix}}} = \\frac{\\alpha \\log(2) f(i)}{4\\tau_{\\text{mix}}(\\lambda_1 - \\lambda_2) \\log(f(i))} \\geq \\frac{4\\tau_{\\text{mix}}(\\lambda_1 - \\lambda_2) \\log(f(0))}{\\alpha \\log(2) f(0)} \\geq 1 \\quad \\text{for} \\quad \\epsilon \\quad \\text{from S.35}\n$$\n\nwhere we used the fact that $$\\log(x)$$ is an increasing function for $$x \\geq e$$. Therefore, we have that\n\n$$\n\\frac{\\eta_i - ki}{\\eta_i} \\leq 1 + \\frac{1}{1 - \\epsilon}\n$$\n\nFor the third result, we note that\n\n$$\n\\zeta_{k,t} := \\frac{40kt+1}{(M + \\lambda_1)^2}, \\quad \\xi_{k,t} := \\frac{2\\eta_tM}{3 + 9k^2t+1(M + \\lambda_1)^2} \\leq 24\\eta_tM \\frac{k^2t+1}{(M + \\lambda_1)^2} \\quad \\text{since} \\quad (M + \\lambda_1) \\geq 1 \\quad \\text{WLOG}\n$$\n\nTherefore,\n\n$$\n\\leq 24\\epsilon(1 + \\epsilon)k^{t+1}(M + \\lambda_1)^2 \\quad \\text{since} \\quad \\eta_t \\leq (1 + 2\\epsilon)\\eta_{t+1} \\quad \\text{and} \\quad \\eta_{t+1}k^{t+1}(M + \\lambda_1) \\leq \\epsilon\n$$\n\n$$\n\\sum_{i=1}^{n} (V_{k,i} + \\zeta_{k,i})\\eta_{2i} = V' + 5\\lambda_2 \\leq \\sum_{i=1}^{n} \\eta_{2i} + 45(M + \\lambda_1)^2 \\eta_{2i}k^{i+1} \\quad \\text{(S.37)}\n$$\n\nwhere $$(i)$$ follows from the slow decay property of $$\\eta_i$$. For $$T1$$, using S.30 we have,\n\n$$\nT1 \\leq (\\lambda_1 - \\alpha^2\\lambda_2)^2 \\beta \\quad \\text{(S.38)}\n$$\n\nFor $$T2$$, substituting the value of $$k_i$$ from S.34 for $$\\eta_i < 1$$ we have,\n\n$$\nT2 := \\sum_{i=1}^{n} \\eta_{2i+1}k^{i+1} \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2)} \\sum_{i=1}^{n} \\alpha^2 \\log(\\lambda_1 - \\lambda_2)(\\beta + i + 1) \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2)} \\sum_{i=1}^{n} \\log(f(i + 1)) \\quad \\text{(S.40)}\n$$", "md": "Consider the fraction $$\\frac{\\beta + i}{ki}$$. We can simplify it as:\n\n$$\n\\frac{\\beta + i}{ki} \\geq \\frac{\\log(2)}{4\\tau_{\\text{mix}}} = \\frac{\\alpha \\log(2) f(i)}{4\\tau_{\\text{mix}}(\\lambda_1 - \\lambda_2) \\log(f(i))} \\geq \\frac{4\\tau_{\\text{mix}}(\\lambda_1 - \\lambda_2) \\log(f(0))}{\\alpha \\log(2) f(0)} \\geq 1 \\quad \\text{for} \\quad \\epsilon \\quad \\text{from S.35}\n$$\n\nwhere we used the fact that $$\\log(x)$$ is an increasing function for $$x \\geq e$$. Therefore, we have that\n\n$$\n\\frac{\\eta_i - ki}{\\eta_i} \\leq 1 + \\frac{1}{1 - \\epsilon}\n$$\n\nFor the third result, we note that\n\n$$\n\\zeta_{k,t} := \\frac{40kt+1}{(M + \\lambda_1)^2}, \\quad \\xi_{k,t} := \\frac{2\\eta_tM}{3 + 9k^2t+1(M + \\lambda_1)^2} \\leq 24\\eta_tM \\frac{k^2t+1}{(M + \\lambda_1)^2} \\quad \\text{since} \\quad (M + \\lambda_1) \\geq 1 \\quad \\text{WLOG}\n$$\n\nTherefore,\n\n$$\n\\leq 24\\epsilon(1 + \\epsilon)k^{t+1}(M + \\lambda_1)^2 \\quad \\text{since} \\quad \\eta_t \\leq (1 + 2\\epsilon)\\eta_{t+1} \\quad \\text{and} \\quad \\eta_{t+1}k^{t+1}(M + \\lambda_1) \\leq \\epsilon\n$$\n\n$$\n\\sum_{i=1}^{n} (V_{k,i} + \\zeta_{k,i})\\eta_{2i} = V' + 5\\lambda_2 \\leq \\sum_{i=1}^{n} \\eta_{2i} + 45(M + \\lambda_1)^2 \\eta_{2i}k^{i+1} \\quad \\text{(S.37)}\n$$\n\nwhere $$(i)$$ follows from the slow decay property of $$\\eta_i$$. For $$T1$$, using S.30 we have,\n\n$$\nT1 \\leq (\\lambda_1 - \\alpha^2\\lambda_2)^2 \\beta \\quad \\text{(S.38)}\n$$\n\nFor $$T2$$, substituting the value of $$k_i$$ from S.34 for $$\\eta_i < 1$$ we have,\n\n$$\nT2 := \\sum_{i=1}^{n} \\eta_{2i+1}k^{i+1} \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2)} \\sum_{i=1}^{n} \\alpha^2 \\log(\\lambda_1 - \\lambda_2)(\\beta + i + 1) \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2)} \\sum_{i=1}^{n} \\log(f(i + 1)) \\quad \\text{(S.40)}\n$$"}]}, {"page": 29, "text": "Note that f (i) is a linear function of i and \u2200i f (i +11) \u2212           f (i) = \u03bb1\u2212\u03bb2\u03b1   . We observe that g(x) =\nlog(x)\n  x2    is a decreasing function of x for x \u2265        e 2 \u223c  1.65. Therefore,\n                           \u03bb1 \u2212     \u03bb2     n   log (f (i + 1))    \u2264     f(n+1)   log (x)  dx\n                                 \u03b1        i=1     f (i + 1)2           f(1)        x2\nSubstituting in S.40 we have,\n   T2 \u2264    4\u03c4mix          \u03b1           f(n+1)  log (x)   dx\n          log (2)     \u03bb1 \u2212\u03b1 \u03bb2     \uf8ebf(1) log (x) x2          f(n) \uf8f6\n      = 4\u03c4mix                      \uf8ed\u2212               + 1           \uf8f8\n          log (2)     \u03bb1 \u2212  \u03bb2                x        x     f(1)\n      \u2264    4\u03c4mix          \u03b1         log (f (1))    +     1\n          log (2)     \u03bb1 \u2212  \u03bb2           f (1)        f (1)\n      \u2264    8\u03c4mix          \u03b1         log (f (1))\n          log (2)     \u03bb1 \u2212  \u03bb2           f (1)\n      \u2264    8\u03c4mix          \u03b1         log (f (0))       since log (x)    is a decreasing function of x for x \u2265        e\n          log (2)     \u03bb1 \u2212  \u03bb2           f (0)                  x\nPutting everything together in S.37 and using the bounds on \u03b2, f (0) mentioned in the lemma\nstatement, we have,\n   n   Vk,i + \u03b6k,i    \u03b72i \u2264  460 (M + \u03bb1)2 \u03c4mix              \u03b1        log (f (0))   +        \u03b12          V\u2032 + 5\u03bb2  1\n i=1                                                     \u03bb1 \u2212  \u03bb2        f (0)         (\u03bb1 \u2212   \u03bb2)2 \u03b2\n                          = 460\u03c4mix log (f (0))            \u03b12        (M + \u03bb1)2 +              \u03b12          V\u2032 + 5\u03bb2 1\n                          \u2264  log    1 + \u03b4m           (\u03bb1 \u2212   \u03bb2)2 \u03b2                     (\u03bb1 \u2212  \u03bb2)2 \u03b2\nFinally, for the last result we first note that\n                     \u03bek,t := 2\u03b7tM       3 + 9k2 t+1 (M + \u03bb1)2\nTherefore,                \u2264  24\u03b7tM      k2t+1 (M + \u03bb1)2        since (M + \u03bb1) \u2265        1 WLOG\n  n                            \uf8eb        n                    \uf8f6\n i=1 (V\u2032 + \u03bek,i) \u03b72  i\u2212ki exp n\uf8ed\u2212    j=i+1  2\u03b7j (\u03bb1 \u2212   \uf8eb\u03bb2) \uf8f8  n                     \uf8f6\n             \u2264   (1 + 2\u03f5)2   i=1 (V\u2032 + \u03bek,i) \u03b72  i exp  \uf8ed\u2212   j=i+1   2\u03b7j (\u03bb1 \u2212   \u03bb2)  \uf8f8\n                             n                        \uf8eb        n                    \uf8f6\n             \u2264   (1 + 5\u03f5)   i=1 (V\u2032 + \u03bek,i) \u03b72 i exp  \uf8ed\u2212    j=i+1  2\u03b7j (\u03bb1 \u2212    \u03bb2) \uf8f8   since \u03f5 \u2208   (0, 0.1)\n                           \uf8ee   n              \uf8eb       n                    \uf8f6       n               \uf8eb        n                  \uf8f6\uf8f9\n             = (1 + 5\u03f5)    \uf8f0      V\u2032\u03b72        \uf8ed\u2212          2\u03b7j (\u03bb1 \u2212    \u03bb2) \uf8f8   +       \u03bek,i\u03b72      \uf8ed\u2212           2\u03b7j (\u03bb1 \u2212  \u03bb2) \uf8f8\uf8fb\n                                       i exp                                                i exp\n                              i=1                  j=i+1                          i=1                    j=i+1\n                                                                                                                (S.41)\n                                                          29", "md": "Note that \\( f(i) \\) is a linear function of \\( i \\) and \\( \\forall i f(i + 1) - f(i) = \\lambda_1 - \\lambda_2 \\alpha \\). We observe that \\( g(x) = \\frac{\\log(x)}{x^2} \\) is a decreasing function of \\( x \\) for \\( x \\geq e^2 \\approx 1.65 \\). Therefore,\n\n$$\n\\lambda_1 - \\lambda_2 \\sum_{i=1}^{n} \\frac{\\log(f(i + 1))}{f(i + 1)^2} \\leq \\int_{1}^{n+1} \\frac{\\log(x)}{x^2} dx\n$$\n\nSubstituting in S.40 we have,\n\n$$\nT2 \\leq \\frac{4\\tau_{\\text{mix}}}{\\alpha} \\int_{1}^{n+1} \\frac{\\log(x)}{x} dx \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\alpha \\lambda_2} \\left( \\frac{f(1) \\log(x)}{x^2} - \\frac{f(n)}{x} + 1 \\right)\n$$\n\n$$\n\\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(1)) + 1 \\right) \\leq \\frac{8\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(1)) \\right) \\leq \\frac{8\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(0)) \\right) \\text{ since } \\log(x) \\text{ is a decreasing function of } x \\text{ for } x \\geq e\n$$\n\nPutting everything together in S.37 and using the bounds on \\( \\beta, f(0) \\) mentioned in the lemma statement, we have,\n\n$$\n\\sum_{i=1}^{n} V_{k,i} + \\zeta_{k,i} \\eta^2_i \\leq 460(M + \\lambda_1)^2 \\tau_{\\text{mix}} \\frac{\\alpha \\log(f(0)) + \\alpha^2 V' + 5\\lambda_2}{\\lambda_1 - \\lambda_2 f(0)(\\lambda_1 - \\lambda_2)^2 \\beta}\n$$\n\n$$\n= 460\\tau_{\\text{mix}} \\log(f(0)) \\frac{\\alpha^2 (M + \\lambda_1)^2 + \\alpha^2 V' + 5\\lambda_2}{(\\lambda_1 - \\lambda_2)^2 \\beta} \\leq \\log\\left(1 + \\delta_m\\right) \\frac{(\\lambda_1 - \\lambda_2)^2 \\beta}{(\\lambda_1 - \\lambda_2)^2 \\beta}\n$$\n\nFinally, for the last result we first note that\n\n$$\n\\xi_{k,t} := 2\\eta_t M^3 + 9k^2 t+1 (M + \\lambda_1)^2\n$$\n\nTherefore,\n\n$$\n\\leq 24\\eta_t M^{k^2t+1} (M + \\lambda_1)^2 \\text{ since } (M + \\lambda_1) \\geq 1 \\text{ WLOG}\n$$\n\n$$\n\\leq (1 + 2\\epsilon)^2 \\sum_{i=1}^{n} (V' + \\xi_{k,i}) \\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right)\n$$\n\n$$\n\\leq (1 + 5\\epsilon) \\sum_{i=1}^{n} (V' + \\xi_{k,i}) \\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\text{ since } \\epsilon \\in (0, 0.1)\n$$\n\n$$\n= (1 + 5\\epsilon) \\left[ \\sum_{i=1}^{n} V'\\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) + \\sum_{i=1}^{n} \\xi_{k,i}\\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\right]\n$$", "images": [], "items": [{"type": "text", "value": "Note that \\( f(i) \\) is a linear function of \\( i \\) and \\( \\forall i f(i + 1) - f(i) = \\lambda_1 - \\lambda_2 \\alpha \\). We observe that \\( g(x) = \\frac{\\log(x)}{x^2} \\) is a decreasing function of \\( x \\) for \\( x \\geq e^2 \\approx 1.65 \\). Therefore,\n\n$$\n\\lambda_1 - \\lambda_2 \\sum_{i=1}^{n} \\frac{\\log(f(i + 1))}{f(i + 1)^2} \\leq \\int_{1}^{n+1} \\frac{\\log(x)}{x^2} dx\n$$\n\nSubstituting in S.40 we have,\n\n$$\nT2 \\leq \\frac{4\\tau_{\\text{mix}}}{\\alpha} \\int_{1}^{n+1} \\frac{\\log(x)}{x} dx \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\alpha \\lambda_2} \\left( \\frac{f(1) \\log(x)}{x^2} - \\frac{f(n)}{x} + 1 \\right)\n$$\n\n$$\n\\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(1)) + 1 \\right) \\leq \\frac{8\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(1)) \\right) \\leq \\frac{8\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(0)) \\right) \\text{ since } \\log(x) \\text{ is a decreasing function of } x \\text{ for } x \\geq e\n$$\n\nPutting everything together in S.37 and using the bounds on \\( \\beta, f(0) \\) mentioned in the lemma statement, we have,\n\n$$\n\\sum_{i=1}^{n} V_{k,i} + \\zeta_{k,i} \\eta^2_i \\leq 460(M + \\lambda_1)^2 \\tau_{\\text{mix}} \\frac{\\alpha \\log(f(0)) + \\alpha^2 V' + 5\\lambda_2}{\\lambda_1 - \\lambda_2 f(0)(\\lambda_1 - \\lambda_2)^2 \\beta}\n$$\n\n$$\n= 460\\tau_{\\text{mix}} \\log(f(0)) \\frac{\\alpha^2 (M + \\lambda_1)^2 + \\alpha^2 V' + 5\\lambda_2}{(\\lambda_1 - \\lambda_2)^2 \\beta} \\leq \\log\\left(1 + \\delta_m\\right) \\frac{(\\lambda_1 - \\lambda_2)^2 \\beta}{(\\lambda_1 - \\lambda_2)^2 \\beta}\n$$\n\nFinally, for the last result we first note that\n\n$$\n\\xi_{k,t} := 2\\eta_t M^3 + 9k^2 t+1 (M + \\lambda_1)^2\n$$\n\nTherefore,\n\n$$\n\\leq 24\\eta_t M^{k^2t+1} (M + \\lambda_1)^2 \\text{ since } (M + \\lambda_1) \\geq 1 \\text{ WLOG}\n$$\n\n$$\n\\leq (1 + 2\\epsilon)^2 \\sum_{i=1}^{n} (V' + \\xi_{k,i}) \\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right)\n$$\n\n$$\n\\leq (1 + 5\\epsilon) \\sum_{i=1}^{n} (V' + \\xi_{k,i}) \\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\text{ since } \\epsilon \\in (0, 0.1)\n$$\n\n$$\n= (1 + 5\\epsilon) \\left[ \\sum_{i=1}^{n} V'\\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) + \\sum_{i=1}^{n} \\xi_{k,i}\\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\right]\n$$", "md": "Note that \\( f(i) \\) is a linear function of \\( i \\) and \\( \\forall i f(i + 1) - f(i) = \\lambda_1 - \\lambda_2 \\alpha \\). We observe that \\( g(x) = \\frac{\\log(x)}{x^2} \\) is a decreasing function of \\( x \\) for \\( x \\geq e^2 \\approx 1.65 \\). Therefore,\n\n$$\n\\lambda_1 - \\lambda_2 \\sum_{i=1}^{n} \\frac{\\log(f(i + 1))}{f(i + 1)^2} \\leq \\int_{1}^{n+1} \\frac{\\log(x)}{x^2} dx\n$$\n\nSubstituting in S.40 we have,\n\n$$\nT2 \\leq \\frac{4\\tau_{\\text{mix}}}{\\alpha} \\int_{1}^{n+1} \\frac{\\log(x)}{x} dx \\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\alpha \\lambda_2} \\left( \\frac{f(1) \\log(x)}{x^2} - \\frac{f(n)}{x} + 1 \\right)\n$$\n\n$$\n\\leq \\frac{4\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(1)) + 1 \\right) \\leq \\frac{8\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(1)) \\right) \\leq \\frac{8\\tau_{\\text{mix}}}{\\log(2) \\lambda_1 - \\lambda_2} \\left( \\alpha \\log(f(0)) \\right) \\text{ since } \\log(x) \\text{ is a decreasing function of } x \\text{ for } x \\geq e\n$$\n\nPutting everything together in S.37 and using the bounds on \\( \\beta, f(0) \\) mentioned in the lemma statement, we have,\n\n$$\n\\sum_{i=1}^{n} V_{k,i} + \\zeta_{k,i} \\eta^2_i \\leq 460(M + \\lambda_1)^2 \\tau_{\\text{mix}} \\frac{\\alpha \\log(f(0)) + \\alpha^2 V' + 5\\lambda_2}{\\lambda_1 - \\lambda_2 f(0)(\\lambda_1 - \\lambda_2)^2 \\beta}\n$$\n\n$$\n= 460\\tau_{\\text{mix}} \\log(f(0)) \\frac{\\alpha^2 (M + \\lambda_1)^2 + \\alpha^2 V' + 5\\lambda_2}{(\\lambda_1 - \\lambda_2)^2 \\beta} \\leq \\log\\left(1 + \\delta_m\\right) \\frac{(\\lambda_1 - \\lambda_2)^2 \\beta}{(\\lambda_1 - \\lambda_2)^2 \\beta}\n$$\n\nFinally, for the last result we first note that\n\n$$\n\\xi_{k,t} := 2\\eta_t M^3 + 9k^2 t+1 (M + \\lambda_1)^2\n$$\n\nTherefore,\n\n$$\n\\leq 24\\eta_t M^{k^2t+1} (M + \\lambda_1)^2 \\text{ since } (M + \\lambda_1) \\geq 1 \\text{ WLOG}\n$$\n\n$$\n\\leq (1 + 2\\epsilon)^2 \\sum_{i=1}^{n} (V' + \\xi_{k,i}) \\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right)\n$$\n\n$$\n\\leq (1 + 5\\epsilon) \\sum_{i=1}^{n} (V' + \\xi_{k,i}) \\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\text{ since } \\epsilon \\in (0, 0.1)\n$$\n\n$$\n= (1 + 5\\epsilon) \\left[ \\sum_{i=1}^{n} V'\\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) + \\sum_{i=1}^{n} \\xi_{k,i}\\eta^2_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\right]\n$$"}]}, {"page": 30, "text": "Let\u2019s define     \uf8eb        n                       \uf8f6                n                          n                      n\ng (i) := exp     \uf8ed\u2212            2\u03b7j (\u03bb1 \u2212     \u03bb2)  \uf8f8  , T3 :=           \u03b72                         \u03b73                     \u03b73\n                                                                        i g (i) , T4 :=            i g (i) , T5 :=         i k2\n                                                                                                                              i g (i) ,\n                       j=i+1                                      i=1                        i=1                    i=1\nNote that since kn \u2265         ki,    T5 =     n   \u03b73i k2                 n  \u03b73\nThen,                                       i=1       i g (i) \u2264   k2n i=1    i g (i) = k2  nT4\n  n                                \uf8eb        n                      \uf8f6\n i=1  (V\u2032 + \u03bek,i) \u03b72   i\u2212ki exp    \uf8ed\u2212    j=i+1   2\u03b7j (\u03bb1 \u2212     \u03bb2) \uf8f8   \u2264   (1 + 5\u03f5)     V\u2032T3 + 24M (M + \u03bb1)2 T5\n                                                                       \u2264   (1 + 5\u03f5)     V\u2032T3 + 24M (M + \u03bb1)2 k2         nT4\n                                                                                                                        (S.42)\n                              i+\u03b2+1    2\u03b1   . Noting that       \u03b2+1   2  \u2264     \u03b2+1   3   \u2264  2, we have\nUsing S.31, g (i) \u2264          n+\u03b2+1                               \u03b2               \u03b2\n                              n            \uf8eb         n                      \uf8f6\n                   T3 :=    i=1  \u03b72i exp   \uf8ed\u22122     j=i+1   \u03b7j (\u03bb1 \u2212    \u03bb2)  \uf8f8\n                        =         \u03b1       2    n        1          i + \u03b2 + 1      2\u03b1\n                              \u03bb1 \u2212   \u03bb2       i=1  (\u03b2 + i)2        n + \u03b2 + 1\n                        \u2264         \u03b1       2  \u03b2 + 1 2           n           1            i + \u03b2 + 1      2\u03b1\n                              \u03bb1 \u2212   \u03bb2            \u03b2         i=1   (\u03b2 + i + 1)2         n + \u03b2 + 1\n                        =         \u03b1       2  \u03b2 + 1 2           n           1            i + \u03b2 + 1      2\u03b1\n                              \u03bb1 \u2212   \u03bb2            \u03b2         i=1   (\u03b2 + i + 1)2         n + \u03b2 + 1\n                        \u2264  2         \u03b1       2           1            n   (i + \u03b2 + 1)2\u03b1\u22122\n                                \u03bb1 \u2212    \u03bb2      (n + \u03b2 + 1)2\u03b1        i=1\n                        \u2264       2            \u03b1       2          1          n + \u03b2 + 2 2\u03b1           using S.33\n                            2\u03b1 \u2212   1     \u03bb1 \u2212   \u03bb2       (n + \u03b2 + 2)         n + \u03b2 + 1\n                        =       2            \u03b1       2          1            1 +         1        2\u03b1                    (S.43)\nand similarly,              2\u03b1 \u2212n  1     \u03bb1 \u2212\uf8eb  \u03bb2      n(n + \u03b2 + 2)          \uf8f6    n + \u03b2 + 1\n                      T4 :=    i=1  \u03b73i exp  \uf8ed\u22122     j=i+1   \u03b7j (\u03bb1 \u2212    \u03bb2)  \uf8f8\n                          =          \u03b1       3    n        1         i + \u03b2 + 1      2\u03b1\n                                 \u03bb1 \u2212   \u03bb2       i=1  (\u03b2 + i)3       n + \u03b2 + 1\n                          \u2264          \u03b1       3  \u03b2 + 1 3          n           1             i + \u03b2 + 1     2\u03b1\n                                 \u03bb1 \u2212   \u03bb2            \u03b2         i=1  (\u03b2 + i + 1)3         n + \u03b2 + 1\n                          =          \u03b1       3  \u03b2 + 1 3          n           1             i + \u03b2 + 1     2\u03b1\n                                 \u03bb1 \u2212   \u03bb2            \u03b2         i=1  (\u03b2 + i + 1)2         n + \u03b2 + 1\n                          \u2264   2        \u03b1       3            1            n   (i + \u03b2 + 1)2\u03b1\u22123\n                                   \u03bb1 \u2212   \u03bb2       (n + \u03b2 + 1)2\u03b1        i=1\n                                                                30", "md": "Let\u2019s define\n\n$$\ng(i) := \\exp\\left(-\\frac{1}{2}\\eta j(\\lambda_1 - \\lambda_2)\\right), \\quad T3 := \\sum_{i=1}^{n} \\eta^2 i g(i), \\quad T4 := \\sum_{i=1}^{n} i g(i), \\quad T5 := \\sum_{i=1}^{n} i k^2 i g(i),\n$$\nNote that since \\(k_n \\geq k_i\\), \\(T5 = \\sum_{i=1}^{n} \\eta^3 i k^2\\)\n\nThen,\n\n$$\n\\sum_{i=1}^{n} i g(i) \\leq k^2 \\sum_{i=1}^{n} i g(i) = k^2 nT4\n$$\n$$\n\\sum_{i=1}^{n} (V' + \\xi k_i) \\eta^2 i - k_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta j(\\lambda_1 - \\lambda_2)\\right) \\leq (1 + 5\\epsilon) V'T3 + 24M(M + \\lambda_1)^2 T5 \\leq (1 + 5\\epsilon) V'T3 + 24M(M + \\lambda_1)^2 k^2 nT4 \\quad (S.42)\n$$\nwhere \\(i+\\beta+1 \\leq 2\\alpha\\). Noting that \\(\\beta+1 \\leq 3 \\leq 2\\), we have\n\n$$\n\\begin{align*}\nT3 &:= \\sum_{i=1}^{n+ \\beta + 1} \\eta^2 i \\exp\\left(-2\\sum_{j=i+1}^{n} \\eta j(\\lambda_1 - \\lambda_2)\\right) \\\\\n&= \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\sum_{i=1}^{n} \\frac{1}{(i + \\beta + 1)^{2\\alpha}} \\\\\n&\\leq \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\frac{1}{\\beta + 1} \\frac{1}{2^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&= \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\frac{1}{\\beta + 1} \\frac{1}{2^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&\\leq 2\\alpha^2 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{(n + \\beta + 1)^{2\\alpha}} \\sum_{i=1}^{n} \\\\\n&\\leq 2\\alpha^2 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{1 + \\frac{1}{2\\alpha}}^{2\\alpha} \\quad \\text{(S.43)}\n\\end{align*}\n$$\nand similarly,\n\n$$\n\\begin{align*}\nT4 &:= \\sum_{i=1}^{n} \\eta^3 i \\exp\\left(-2\\sum_{j=i+1}^{n} \\eta j(\\lambda_1 - \\lambda_2)\\right) \\\\\n&= \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\sum_{i=1}^{n} \\frac{1}{(i + \\beta + 1)^{2\\alpha}} \\\\\n&\\leq \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\frac{1}{\\beta + 1} \\frac{1}{3^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&= \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\frac{1}{\\beta + 1} \\frac{1}{3^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&\\leq 2\\alpha^3 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{(n + \\beta + 1)^{2\\alpha}} \\sum_{i=1}^{n} \\\\\n&\\leq 2\\alpha^3 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{1 + \\frac{1}{2\\alpha}}^{2\\alpha} \\quad (S.44)\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Let\u2019s define\n\n$$\ng(i) := \\exp\\left(-\\frac{1}{2}\\eta j(\\lambda_1 - \\lambda_2)\\right), \\quad T3 := \\sum_{i=1}^{n} \\eta^2 i g(i), \\quad T4 := \\sum_{i=1}^{n} i g(i), \\quad T5 := \\sum_{i=1}^{n} i k^2 i g(i),\n$$\nNote that since \\(k_n \\geq k_i\\), \\(T5 = \\sum_{i=1}^{n} \\eta^3 i k^2\\)\n\nThen,\n\n$$\n\\sum_{i=1}^{n} i g(i) \\leq k^2 \\sum_{i=1}^{n} i g(i) = k^2 nT4\n$$\n$$\n\\sum_{i=1}^{n} (V' + \\xi k_i) \\eta^2 i - k_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta j(\\lambda_1 - \\lambda_2)\\right) \\leq (1 + 5\\epsilon) V'T3 + 24M(M + \\lambda_1)^2 T5 \\leq (1 + 5\\epsilon) V'T3 + 24M(M + \\lambda_1)^2 k^2 nT4 \\quad (S.42)\n$$\nwhere \\(i+\\beta+1 \\leq 2\\alpha\\). Noting that \\(\\beta+1 \\leq 3 \\leq 2\\), we have\n\n$$\n\\begin{align*}\nT3 &:= \\sum_{i=1}^{n+ \\beta + 1} \\eta^2 i \\exp\\left(-2\\sum_{j=i+1}^{n} \\eta j(\\lambda_1 - \\lambda_2)\\right) \\\\\n&= \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\sum_{i=1}^{n} \\frac{1}{(i + \\beta + 1)^{2\\alpha}} \\\\\n&\\leq \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\frac{1}{\\beta + 1} \\frac{1}{2^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&= \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\frac{1}{\\beta + 1} \\frac{1}{2^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&\\leq 2\\alpha^2 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{(n + \\beta + 1)^{2\\alpha}} \\sum_{i=1}^{n} \\\\\n&\\leq 2\\alpha^2 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{1 + \\frac{1}{2\\alpha}}^{2\\alpha} \\quad \\text{(S.43)}\n\\end{align*}\n$$\nand similarly,\n\n$$\n\\begin{align*}\nT4 &:= \\sum_{i=1}^{n} \\eta^3 i \\exp\\left(-2\\sum_{j=i+1}^{n} \\eta j(\\lambda_1 - \\lambda_2)\\right) \\\\\n&= \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\sum_{i=1}^{n} \\frac{1}{(i + \\beta + 1)^{2\\alpha}} \\\\\n&\\leq \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\frac{1}{\\beta + 1} \\frac{1}{3^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&= \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\frac{1}{\\beta + 1} \\frac{1}{3^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&\\leq 2\\alpha^3 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{(n + \\beta + 1)^{2\\alpha}} \\sum_{i=1}^{n} \\\\\n&\\leq 2\\alpha^3 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{1 + \\frac{1}{2\\alpha}}^{2\\alpha} \\quad (S.44)\n\\end{align*}\n$$", "md": "Let\u2019s define\n\n$$\ng(i) := \\exp\\left(-\\frac{1}{2}\\eta j(\\lambda_1 - \\lambda_2)\\right), \\quad T3 := \\sum_{i=1}^{n} \\eta^2 i g(i), \\quad T4 := \\sum_{i=1}^{n} i g(i), \\quad T5 := \\sum_{i=1}^{n} i k^2 i g(i),\n$$\nNote that since \\(k_n \\geq k_i\\), \\(T5 = \\sum_{i=1}^{n} \\eta^3 i k^2\\)\n\nThen,\n\n$$\n\\sum_{i=1}^{n} i g(i) \\leq k^2 \\sum_{i=1}^{n} i g(i) = k^2 nT4\n$$\n$$\n\\sum_{i=1}^{n} (V' + \\xi k_i) \\eta^2 i - k_i \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta j(\\lambda_1 - \\lambda_2)\\right) \\leq (1 + 5\\epsilon) V'T3 + 24M(M + \\lambda_1)^2 T5 \\leq (1 + 5\\epsilon) V'T3 + 24M(M + \\lambda_1)^2 k^2 nT4 \\quad (S.42)\n$$\nwhere \\(i+\\beta+1 \\leq 2\\alpha\\). Noting that \\(\\beta+1 \\leq 3 \\leq 2\\), we have\n\n$$\n\\begin{align*}\nT3 &:= \\sum_{i=1}^{n+ \\beta + 1} \\eta^2 i \\exp\\left(-2\\sum_{j=i+1}^{n} \\eta j(\\lambda_1 - \\lambda_2)\\right) \\\\\n&= \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\sum_{i=1}^{n} \\frac{1}{(i + \\beta + 1)^{2\\alpha}} \\\\\n&\\leq \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\frac{1}{\\beta + 1} \\frac{1}{2^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&= \\alpha^2 \\frac{\\lambda_1 - \\lambda_2}{2} \\frac{1}{\\beta + 1} \\frac{1}{2^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&\\leq 2\\alpha^2 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{(n + \\beta + 1)^{2\\alpha}} \\sum_{i=1}^{n} \\\\\n&\\leq 2\\alpha^2 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{1 + \\frac{1}{2\\alpha}}^{2\\alpha} \\quad \\text{(S.43)}\n\\end{align*}\n$$\nand similarly,\n\n$$\n\\begin{align*}\nT4 &:= \\sum_{i=1}^{n} \\eta^3 i \\exp\\left(-2\\sum_{j=i+1}^{n} \\eta j(\\lambda_1 - \\lambda_2)\\right) \\\\\n&= \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\sum_{i=1}^{n} \\frac{1}{(i + \\beta + 1)^{2\\alpha}} \\\\\n&\\leq \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\frac{1}{\\beta + 1} \\frac{1}{3^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&= \\alpha^3 \\frac{\\lambda_1 - \\lambda_2}{3} \\frac{1}{\\beta + 1} \\frac{1}{3^{n}} \\sum_{i=1}^{n} (i + \\beta + 1)^{2\\alpha} \\\\\n&\\leq 2\\alpha^3 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{(n + \\beta + 1)^{2\\alpha}} \\sum_{i=1}^{n} \\\\\n&\\leq 2\\alpha^3 \\frac{1}{\\lambda_1 - \\lambda_2} \\frac{1}{1 + \\frac{1}{2\\alpha}}^{2\\alpha} \\quad (S.44)\n\\end{align*}\n$$"}]}, {"page": 31, "text": "                       \u2264       1             \u03b1        3            1            n + \u03b2 + 2         2\u03b1     using S.33\n                            \u03b1 \u2212   1     \u03bb1 \u2212     \u03bb2       (n + \u03b2 + 2)2            n + \u03b2 + 1\n                       =       1             \u03b1        3            1             1 +          1         2\u03b1                            (S.44)\nUsing S.36, we have         \u03b1 \u2212   1     \u03bb1 \u2212     \u03bb2       (n + \u03b2 + 2)2                  n + \u03b2 + 1\n                                    \u03b1\nTherefore, using [17]         n + \u03b2 + 1 = \u03b7n (\u03bb1 \u2212              \u03bb2) \u2264     \u03b7n\u03bb1 \u2264      \u03b7nkn\u03bb1 \u2264        \u03f5 \u2264    0.1                      (S.45)\n                                       1         2\u03b1 (i)            1          (ii)             4\u03b1\n                          1 +    n + \u03b2 + 1             \u2264              2\u03b1       \u2264   1 +    n + \u03b2 + 1 \u2264         1 + 4\u03f5                  (S.46)\n                                      2\u03b1                    1 \u2212    n+\u03b2+1                           1\nwhere (i) follows since            n+\u03b2+1 < 1 by S.45 and (ii) follows since                      1\u2212x \u2264     1 + 2x for x \u2208         [0, 12].\n     Using S.46 with S.43, we have\n                            T3 \u2264         2              \u03b1       2            1            1 +         4\u03b1\n                                     2\u03b1 \u2212    1     \u03bb1 \u2212    \u03bb2        (n + \u03b2 + 2)                 n + \u03b2 + 1\n                                \u2264    2 (1 + 4\u03f5)             \u03b1        2           1                                                    (S.47)\nUsing S.46 with S.44, we have          2\u03b1 \u2212    1        \u03bb1 \u2212    \u03bb2       (n + \u03b2 + 2)\nLet                                      T4 \u2264     1\u03b1+\u22124\u03f5  1     \u03bb1 \u2212 \u03b1  \u03bb2   3    (n + \u03b2 +12)2                                        (S.48)\n                                     C1 := 2 (1 + 10\u03f5) \u03b12            , C2 := 24 (1 + 10\u03f5) \u03b13             ,\n                                                     2\u03b1 \u2212     1                        (\u03b1 \u2212    1)\nPutting together S.47, S.48 in S.42 and using the definition of ki in S.34 we have\n                         (1 + 5\u03f5) V\u2032T3 \u2264          2 (1 + 5\u03f5) (1 + 4\u03f5)                  \u03b1       2           V\u2032\n                                                           2\u03b1 \u2212    1              \u03bb1 \u2212    \u03bb2        (n + \u03b2 + 2)\n                                              \u2264   2 (1 + 10\u03f5) \u03b12               V\u2032         1\nand similarly,                                          2\u03b1 \u2212    1       (\u03bb1 \u2212     \u03bb2)2   n since \u03f5 \u2264       0.05\n              24 (1 + 5\u03f5) M (M + \u03bb1)2 k2                nT4 \u2264      24 (1 + 5\u03f5) (1 + 4\u03f5) \u03b13           M (M + \u03bb1)2            k2n\nTherefore from S.42, we have                                                   \u03b1 \u2212    1                 (\u03bb1 \u2212     \u03bb2)3      n2\n   n                                  \uf8eb         n                         \uf8f6                  V\u2032         1          M (M + \u03bb1)2            k2n\n i=1   (V\u2032 + \u03bek,i) \u03b72    i\u2212ki exp     \uf8ed\u2212     j=i+1    2\u03b7j (\u03bb1 \u2212      \u03bb2)  \uf8f8   \u2264   C1   (\u03bb1 \u2212    \u03bb2)2    n + C2        (\u03bb1 \u2212     \u03bb2)3      n2\nHence proved.\n                                                                      31", "md": "# Math Equations\n\n$$\\leq \\frac{1}{\\alpha^3} \\frac{1}{n + \\beta + 2} \\frac{2\\alpha}{\\text{using S.33}}$$\n\n$$\\frac{\\alpha - 1}{\\lambda_1 - \\lambda_2} \\left( \\frac{n + \\beta + 2}{n + \\beta + 1} \\right)^2 = \\frac{1}{\\alpha^3} \\frac{1}{1 + \\frac{1}{2\\alpha}} \\quad \\text{(S.44)}$$\n\nUsing S.36, we have $$\\frac{\\alpha - 1}{\\lambda_1 - \\lambda_2} \\left( \\frac{n + \\beta + 2}{n + \\beta + 1} \\right)^2$$\n\nTherefore, using [17] $$n + \\beta + 1 = \\eta n (\\lambda_1 - \\lambda_2) \\leq \\eta n \\lambda_1 \\leq \\eta n k n \\lambda_1 \\leq \\epsilon \\leq 0.1 \\quad \\text{(S.45)}$$\n\n$$\\frac{1 + n + \\beta + 1}{2\\alpha} \\leq \\frac{1}{1 - n + \\beta + 1} \\leq \\frac{1 + n + \\beta + 1}{1 + 4\\epsilon} \\quad \\text{(S.46)}$$\n\nwhere (i) follows since $$n + \\beta + 1 < 1$$ by S.45 and (ii) follows since $$1 - x \\leq 1 + 2x$$ for $$x \\in [0, \\frac{1}{2}]$$.\n\nUsing S.46 with S.43, we have $$T3 \\leq \\frac{2}{2\\alpha - 1} \\frac{\\alpha^2}{2} \\frac{1}{1 + 4\\alpha} \\leq 2(1 + 4\\epsilon) \\frac{\\alpha^2}{2} \\frac{1}{1} \\quad \\text{(S.47)}$$\n\nUsing S.46 with S.44, we have $$T4 \\leq \\frac{1}{\\alpha} + 4\\epsilon \\frac{1}{\\lambda_1 - \\alpha \\lambda_2}^3 \\frac{n + \\beta + 2}{2} \\quad \\text{(S.48)}$$\n\n$$C1 := 2(1 + 10\\epsilon) \\alpha^2, \\quad C2 := 24(1 + 10\\epsilon) \\alpha^3, \\quad \\frac{2}{2\\alpha - 1} (\\alpha - 1)$$\n\nPutting together S.47, S.48 in S.42 and using the definition of $$k_i$$ in S.34 we have\n\n$$(1 + 5\\epsilon) V' T3 \\leq 2(1 + 5\\epsilon)(1 + 4\\epsilon) \\frac{\\alpha^2}{2} V' \\frac{2\\alpha - 1}{\\lambda_1 - \\lambda_2} (n + \\beta + 2) \\leq 2(1 + 10\\epsilon) \\alpha^2 V'$$\n\nand similarly, $$\\frac{2}{2\\alpha - 1} (\\lambda_1 - \\lambda_2)^2 n$$ since $$\\epsilon \\leq 0.05$$\n\n$$24(1 + 5\\epsilon) M(M + \\lambda_1)^2 k^2 n T4 \\leq 24(1 + 5\\epsilon)(1 + 4\\epsilon) \\alpha^3 M(M + \\lambda_1)^2 k^2 n$$\n\nTherefore from S.42, we have $$\\frac{\\alpha - 1}{(\\lambda_1 - \\lambda_2)^3 n^2} \\sum_{i=1}^{n} (V' + \\xi k_i) \\eta^2 i - k_i \\exp \\left( - \\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right) \\leq C1 (\\lambda_1 - \\lambda_2)^2 n + C2 (\\lambda_1 - \\lambda_2)^3 n^2$$\n\nHence proved.\n\n31", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "$$\\leq \\frac{1}{\\alpha^3} \\frac{1}{n + \\beta + 2} \\frac{2\\alpha}{\\text{using S.33}}$$\n\n$$\\frac{\\alpha - 1}{\\lambda_1 - \\lambda_2} \\left( \\frac{n + \\beta + 2}{n + \\beta + 1} \\right)^2 = \\frac{1}{\\alpha^3} \\frac{1}{1 + \\frac{1}{2\\alpha}} \\quad \\text{(S.44)}$$\n\nUsing S.36, we have $$\\frac{\\alpha - 1}{\\lambda_1 - \\lambda_2} \\left( \\frac{n + \\beta + 2}{n + \\beta + 1} \\right)^2$$\n\nTherefore, using [17] $$n + \\beta + 1 = \\eta n (\\lambda_1 - \\lambda_2) \\leq \\eta n \\lambda_1 \\leq \\eta n k n \\lambda_1 \\leq \\epsilon \\leq 0.1 \\quad \\text{(S.45)}$$\n\n$$\\frac{1 + n + \\beta + 1}{2\\alpha} \\leq \\frac{1}{1 - n + \\beta + 1} \\leq \\frac{1 + n + \\beta + 1}{1 + 4\\epsilon} \\quad \\text{(S.46)}$$\n\nwhere (i) follows since $$n + \\beta + 1 < 1$$ by S.45 and (ii) follows since $$1 - x \\leq 1 + 2x$$ for $$x \\in [0, \\frac{1}{2}]$$.\n\nUsing S.46 with S.43, we have $$T3 \\leq \\frac{2}{2\\alpha - 1} \\frac{\\alpha^2}{2} \\frac{1}{1 + 4\\alpha} \\leq 2(1 + 4\\epsilon) \\frac{\\alpha^2}{2} \\frac{1}{1} \\quad \\text{(S.47)}$$\n\nUsing S.46 with S.44, we have $$T4 \\leq \\frac{1}{\\alpha} + 4\\epsilon \\frac{1}{\\lambda_1 - \\alpha \\lambda_2}^3 \\frac{n + \\beta + 2}{2} \\quad \\text{(S.48)}$$\n\n$$C1 := 2(1 + 10\\epsilon) \\alpha^2, \\quad C2 := 24(1 + 10\\epsilon) \\alpha^3, \\quad \\frac{2}{2\\alpha - 1} (\\alpha - 1)$$\n\nPutting together S.47, S.48 in S.42 and using the definition of $$k_i$$ in S.34 we have\n\n$$(1 + 5\\epsilon) V' T3 \\leq 2(1 + 5\\epsilon)(1 + 4\\epsilon) \\frac{\\alpha^2}{2} V' \\frac{2\\alpha - 1}{\\lambda_1 - \\lambda_2} (n + \\beta + 2) \\leq 2(1 + 10\\epsilon) \\alpha^2 V'$$\n\nand similarly, $$\\frac{2}{2\\alpha - 1} (\\lambda_1 - \\lambda_2)^2 n$$ since $$\\epsilon \\leq 0.05$$\n\n$$24(1 + 5\\epsilon) M(M + \\lambda_1)^2 k^2 n T4 \\leq 24(1 + 5\\epsilon)(1 + 4\\epsilon) \\alpha^3 M(M + \\lambda_1)^2 k^2 n$$\n\nTherefore from S.42, we have $$\\frac{\\alpha - 1}{(\\lambda_1 - \\lambda_2)^3 n^2} \\sum_{i=1}^{n} (V' + \\xi k_i) \\eta^2 i - k_i \\exp \\left( - \\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right) \\leq C1 (\\lambda_1 - \\lambda_2)^2 n + C2 (\\lambda_1 - \\lambda_2)^3 n^2$$\n\nHence proved.\n\n31", "md": "$$\\leq \\frac{1}{\\alpha^3} \\frac{1}{n + \\beta + 2} \\frac{2\\alpha}{\\text{using S.33}}$$\n\n$$\\frac{\\alpha - 1}{\\lambda_1 - \\lambda_2} \\left( \\frac{n + \\beta + 2}{n + \\beta + 1} \\right)^2 = \\frac{1}{\\alpha^3} \\frac{1}{1 + \\frac{1}{2\\alpha}} \\quad \\text{(S.44)}$$\n\nUsing S.36, we have $$\\frac{\\alpha - 1}{\\lambda_1 - \\lambda_2} \\left( \\frac{n + \\beta + 2}{n + \\beta + 1} \\right)^2$$\n\nTherefore, using [17] $$n + \\beta + 1 = \\eta n (\\lambda_1 - \\lambda_2) \\leq \\eta n \\lambda_1 \\leq \\eta n k n \\lambda_1 \\leq \\epsilon \\leq 0.1 \\quad \\text{(S.45)}$$\n\n$$\\frac{1 + n + \\beta + 1}{2\\alpha} \\leq \\frac{1}{1 - n + \\beta + 1} \\leq \\frac{1 + n + \\beta + 1}{1 + 4\\epsilon} \\quad \\text{(S.46)}$$\n\nwhere (i) follows since $$n + \\beta + 1 < 1$$ by S.45 and (ii) follows since $$1 - x \\leq 1 + 2x$$ for $$x \\in [0, \\frac{1}{2}]$$.\n\nUsing S.46 with S.43, we have $$T3 \\leq \\frac{2}{2\\alpha - 1} \\frac{\\alpha^2}{2} \\frac{1}{1 + 4\\alpha} \\leq 2(1 + 4\\epsilon) \\frac{\\alpha^2}{2} \\frac{1}{1} \\quad \\text{(S.47)}$$\n\nUsing S.46 with S.44, we have $$T4 \\leq \\frac{1}{\\alpha} + 4\\epsilon \\frac{1}{\\lambda_1 - \\alpha \\lambda_2}^3 \\frac{n + \\beta + 2}{2} \\quad \\text{(S.48)}$$\n\n$$C1 := 2(1 + 10\\epsilon) \\alpha^2, \\quad C2 := 24(1 + 10\\epsilon) \\alpha^3, \\quad \\frac{2}{2\\alpha - 1} (\\alpha - 1)$$\n\nPutting together S.47, S.48 in S.42 and using the definition of $$k_i$$ in S.34 we have\n\n$$(1 + 5\\epsilon) V' T3 \\leq 2(1 + 5\\epsilon)(1 + 4\\epsilon) \\frac{\\alpha^2}{2} V' \\frac{2\\alpha - 1}{\\lambda_1 - \\lambda_2} (n + \\beta + 2) \\leq 2(1 + 10\\epsilon) \\alpha^2 V'$$\n\nand similarly, $$\\frac{2}{2\\alpha - 1} (\\lambda_1 - \\lambda_2)^2 n$$ since $$\\epsilon \\leq 0.05$$\n\n$$24(1 + 5\\epsilon) M(M + \\lambda_1)^2 k^2 n T4 \\leq 24(1 + 5\\epsilon)(1 + 4\\epsilon) \\alpha^3 M(M + \\lambda_1)^2 k^2 n$$\n\nTherefore from S.42, we have $$\\frac{\\alpha - 1}{(\\lambda_1 - \\lambda_2)^3 n^2} \\sum_{i=1}^{n} (V' + \\xi k_i) \\eta^2 i - k_i \\exp \\left( - \\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right) \\leq C1 (\\lambda_1 - \\lambda_2)^2 n + C2 (\\lambda_1 - \\lambda_2)^3 n^2$$\n\nHence proved.\n\n31"}]}, {"page": 32, "text": "S.4         Proofs : Convergence Analysis of Oja\u2019s Algorithm for Markovian\n            Data\nIn this section, we present proofs of Theorems 2, 3, 4 and 5. We state versions of these theorems that\nare valid under more general conditions on the step sizes. Specifically, for the following, we only     1\nrequire a sequence of non-increasing step-sizes which satisfy, for \u03f5 :=                               100, \u2200i \u2208     [n] -\n  C.1 \u03b7iki (M + \u03bb1) \u2264              \u03f5              C.2 (Slow decay) \u03b7i \u2264               \u03b7i\u2212ki \u2264      (1 + 2\u03f5) \u03b7i \u2264        2\u03b7i\nThe version of these theorems stated in the main manuscript are obtained by plugging in the step-sizes\n                     \u03b1\nas \u03b7i :=     (\u03bb1\u2212\u03bb2)(\u03b2+i) for the values of \u03b1, \u03b2 provided in Lemma S.12. Before starting with the proofs,\nwe define the following scalar variables -\n                           r := 2 (1 + \u03f5) kn\u03b7n (M + \u03bb1) ,                        \u03b6k,t := 40kt+1 (M + \u03bb1)2\n                      \u03c8k,t := 6M           1 + 3k2   t+1 (M + \u03bb1)2           ,        \u03bek,t := \u03b7t\u2212kt\u03c8k,t\n                         V\u2032 := 1 + (3 + 4\u03f5) |\u03bb2 (P             ) | V,                  Vk,t := V\u2032 + \u03bb2       1 + \u03bek,t                 (S.49)\n                                         1 \u2212    |\u03bb2 (P   )|\n     The basic idea behind these proofs is illustrated in Figure S.1, where we are trying to approximate\nthe matrix product by conditioning back in time just the right amount, to balance the tradeoff between\nthe advantage of the mixing decay and the norm of the product of matrices.\n                                                                     (I+nx *                      1+M2\n                                                                                                 XXI\nFigure S.1: If we could replace the intermediate products (white matrices) by I, the conditional expectation of\nthe noise matrix X1XT       1 \u2212    \u03a3 conditioned on the grey matrices would be nearly zero.\nTheorem 2. (General Version) Under Assumptions 1, 2 and 3, for all n > kn, and any decaying\nstep-size schedule \u03b7i satisfying C.1 and C.2, we have:\n                E    vT1 Bn,1BT    n,1v1     \u2264   (1 + r)2 exp        n\u2212kn       2\u03b7t\u03bb1 + \u03b72     t   V\u2032 + \u03bb2   1 + \u03bek,t\n                                                                        t=1\nwhere Bj,i is defined in 7.\nProof. Define \u03b1n,t := E              Tr    vT1 Bn,tBT    n,tv1      = E      vT1 Bn,tBT   n,tv1     , i \u2264  t \u2264   n. Then, we have\nvT1 Bn,tBT    n,tv1 = vT    1 Bn,t+1(I + \u03b7t\u03a3)2BT            n,t+1v1 + 2\u03b7t           vT1 Bn,t+1(I + \u03b7t\u03a3)(At \u2212  Pn,t         \u03a3)BT   n,t+1v1\n                      + \u03b72  t   vT1 Bn,t+1(At \u2212    Qn,t  \u03a3)2BT   n,t+1v1                                                              (S.50)\n                      \u2264   vT1 Bj,t+1BT     j,t+1v1((1 + \u03b7t\u03bb1)2) + \u03b72           t Qn,t + 2\u03b7tPn,t\nUsing Lemma S.8 with U = v1, G = (I + \u03b7t\u03a3) , \u03b3 = 1 and noting that E\u03c0 [At \u2212                                               \u03a3] = 0, along\nwith observing that \u03b1n,t+k            t+1 \u2264    \u03b1n,t+kt from Lemma S.3, we have\n|E [Pn,t]| \u2264      \u03b7t+1 (1 + \u03b7t\u03bb1)            2V |\u03bb2 (P      )|                   2 + 16\u03f5 +         2 + (1 + \u03f5)2         k2t+1 (M + \u03bb1)2       \u03b1n,t+kt\n                                             1 \u2212   |\u03bb2 (P   )| + \u03b7t+1M\n                                                                      32", "md": "S.4 Proofs: Convergence Analysis of Oja\u2019s Algorithm for Markovian Data\n\nIn this section, we present proofs of Theorems 2, 3, 4 and 5. We state versions of these theorems that are valid under more general conditions on the step sizes. Specifically, for the following, we only require a sequence of non-increasing step-sizes which satisfy, for $$\\epsilon := 100, \\forall i \\in [n]$$ -\n\nC.1 $$\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$$\n\nC.2 (Slow decay) $$\\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i$$\n\nThe version of these theorems stated in the main manuscript are obtained by plugging in the step-sizes as $$\\eta_i := \\frac{(\\lambda_1 - \\lambda_2)(\\beta+i)}{\\alpha}$$ for the values of $$\\alpha, \\beta$$ provided in Lemma S.12. Before starting with the proofs, we define the following scalar variables -\n\n$$r := 2(1 + \\epsilon) k_n \\eta_n (M + \\lambda_1)$$, $$\\zeta_{k,t} := \\frac{40k t+1}{(M + \\lambda_1)^2}$$\n\n$$\\psi_{k,t} := \\frac{6M}{1 + 3k^2 t+1}(M + \\lambda_1)^2$$, $$\\xi_{k,t} := \\eta_{t-k_t} \\psi_{k,t}$$\n\n$$V' := 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V$$, $$V_{k,t} := V' + \\lambda_2 \\frac{1 + \\xi_{k,t}}{1 - |\\lambda_2 (P)|}$$\n\nThe basic idea behind these proofs is illustrated in Figure S.1, where we are trying to approximate the matrix product by conditioning back in time just the right amount, to balance the tradeoff between the advantage of the mixing decay and the norm of the product of matrices.\n\nTheorem 2. (General Version) Under Assumptions 1, 2 and 3, for all $$n > k_n$$, and any decaying step-size schedule $$\\eta_i$$ satisfying C.1 and C.2, we have:\n\n$$\n\\begin{align*}\nE v^T_1 B_{n,1} B^T_{n,1} v_1 &\\leq (1 + r)^2 \\exp\\left(\\sum_{t=1}^{n-k_n} 2\\eta_t \\lambda_1 + \\eta^2_t V' + \\lambda_2 \\frac{1 + \\xi_{k,t}}{t}\\right)\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. Define $$\\alpha_{n,t} := E\\left[\\text{Tr} v^T_1 B_{n,t} B^T_{n,t} v_1\\right] = E v^T_1 B_{n,t} B^T_{n,t} v_1$$, $$i \\leq t \\leq n$$. Then, we have\n\n$$\n\\begin{align*}\nv^T_1 B_{n,t} B^T_{n,t} v_1 &= v^T_1 B_{n,t+1}(I + \\eta_t \\Sigma)^2 B^T_{n,t+1} v_1 + 2\\eta_t v^T_1 B_{n,t+1}(I + \\eta_t \\Sigma)(A_t - P_{n,t} \\Sigma) B^T_{n,t+1} v_1 \\\\\n&\\quad + \\eta^2_t v^T_1 B_{n,t+1}(A_t - Q_{n,t} \\Sigma)^2 B^T_{n,t+1} v_1 \\quad \\text{(S.50)} \\\\\n&\\leq v^T_1 B_{j,t+1} B^T_{j,t+1} v_1((1 + \\eta_t \\lambda_1)^2) + \\eta^2_t Q_{n,t} + 2\\eta_t P_{n,t}\n\\end{align*}\n$$\nUsing Lemma S.8 with $$U = v_1$$, $$G = (I + \\eta_t \\Sigma)$$, $$\\gamma = 1$$ and noting that $$E_{\\pi}[A_t - \\Sigma] = 0$$, along with observing that $$\\alpha_{n,t+k} \\leq \\alpha_{n,t+kt}$$ from Lemma S.3, we have\n\n$$\n\\begin{align*}\n|E [P_{n,t}]| &\\leq \\eta_{t+1} (1 + \\eta_t \\lambda_1)^2 V |\\lambda_2 (P)|^2 + 16\\epsilon + 2 + (1 + \\epsilon)^2 k^2 t+1 (M + \\lambda_1)^2 \\alpha_{n,t+kt} \\\\\n&\\quad + \\eta_{t+1} M\n\\end{align*}\n$$", "images": [{"name": "page-32-0.jpg", "height": 93, "width": 238, "x": 187, "y": 332}], "items": [{"type": "text", "value": "S.4 Proofs: Convergence Analysis of Oja\u2019s Algorithm for Markovian Data\n\nIn this section, we present proofs of Theorems 2, 3, 4 and 5. We state versions of these theorems that are valid under more general conditions on the step sizes. Specifically, for the following, we only require a sequence of non-increasing step-sizes which satisfy, for $$\\epsilon := 100, \\forall i \\in [n]$$ -\n\nC.1 $$\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$$\n\nC.2 (Slow decay) $$\\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i$$\n\nThe version of these theorems stated in the main manuscript are obtained by plugging in the step-sizes as $$\\eta_i := \\frac{(\\lambda_1 - \\lambda_2)(\\beta+i)}{\\alpha}$$ for the values of $$\\alpha, \\beta$$ provided in Lemma S.12. Before starting with the proofs, we define the following scalar variables -\n\n$$r := 2(1 + \\epsilon) k_n \\eta_n (M + \\lambda_1)$$, $$\\zeta_{k,t} := \\frac{40k t+1}{(M + \\lambda_1)^2}$$\n\n$$\\psi_{k,t} := \\frac{6M}{1 + 3k^2 t+1}(M + \\lambda_1)^2$$, $$\\xi_{k,t} := \\eta_{t-k_t} \\psi_{k,t}$$\n\n$$V' := 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V$$, $$V_{k,t} := V' + \\lambda_2 \\frac{1 + \\xi_{k,t}}{1 - |\\lambda_2 (P)|}$$\n\nThe basic idea behind these proofs is illustrated in Figure S.1, where we are trying to approximate the matrix product by conditioning back in time just the right amount, to balance the tradeoff between the advantage of the mixing decay and the norm of the product of matrices.\n\nTheorem 2. (General Version) Under Assumptions 1, 2 and 3, for all $$n > k_n$$, and any decaying step-size schedule $$\\eta_i$$ satisfying C.1 and C.2, we have:\n\n$$\n\\begin{align*}\nE v^T_1 B_{n,1} B^T_{n,1} v_1 &\\leq (1 + r)^2 \\exp\\left(\\sum_{t=1}^{n-k_n} 2\\eta_t \\lambda_1 + \\eta^2_t V' + \\lambda_2 \\frac{1 + \\xi_{k,t}}{t}\\right)\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. Define $$\\alpha_{n,t} := E\\left[\\text{Tr} v^T_1 B_{n,t} B^T_{n,t} v_1\\right] = E v^T_1 B_{n,t} B^T_{n,t} v_1$$, $$i \\leq t \\leq n$$. Then, we have\n\n$$\n\\begin{align*}\nv^T_1 B_{n,t} B^T_{n,t} v_1 &= v^T_1 B_{n,t+1}(I + \\eta_t \\Sigma)^2 B^T_{n,t+1} v_1 + 2\\eta_t v^T_1 B_{n,t+1}(I + \\eta_t \\Sigma)(A_t - P_{n,t} \\Sigma) B^T_{n,t+1} v_1 \\\\\n&\\quad + \\eta^2_t v^T_1 B_{n,t+1}(A_t - Q_{n,t} \\Sigma)^2 B^T_{n,t+1} v_1 \\quad \\text{(S.50)} \\\\\n&\\leq v^T_1 B_{j,t+1} B^T_{j,t+1} v_1((1 + \\eta_t \\lambda_1)^2) + \\eta^2_t Q_{n,t} + 2\\eta_t P_{n,t}\n\\end{align*}\n$$\nUsing Lemma S.8 with $$U = v_1$$, $$G = (I + \\eta_t \\Sigma)$$, $$\\gamma = 1$$ and noting that $$E_{\\pi}[A_t - \\Sigma] = 0$$, along with observing that $$\\alpha_{n,t+k} \\leq \\alpha_{n,t+kt}$$ from Lemma S.3, we have\n\n$$\n\\begin{align*}", "md": "S.4 Proofs: Convergence Analysis of Oja\u2019s Algorithm for Markovian Data\n\nIn this section, we present proofs of Theorems 2, 3, 4 and 5. We state versions of these theorems that are valid under more general conditions on the step sizes. Specifically, for the following, we only require a sequence of non-increasing step-sizes which satisfy, for $$\\epsilon := 100, \\forall i \\in [n]$$ -\n\nC.1 $$\\eta_i k_i (M + \\lambda_1) \\leq \\epsilon$$\n\nC.2 (Slow decay) $$\\eta_i \\leq \\eta_{i-k_i} \\leq (1 + 2\\epsilon) \\eta_i \\leq 2\\eta_i$$\n\nThe version of these theorems stated in the main manuscript are obtained by plugging in the step-sizes as $$\\eta_i := \\frac{(\\lambda_1 - \\lambda_2)(\\beta+i)}{\\alpha}$$ for the values of $$\\alpha, \\beta$$ provided in Lemma S.12. Before starting with the proofs, we define the following scalar variables -\n\n$$r := 2(1 + \\epsilon) k_n \\eta_n (M + \\lambda_1)$$, $$\\zeta_{k,t} := \\frac{40k t+1}{(M + \\lambda_1)^2}$$\n\n$$\\psi_{k,t} := \\frac{6M}{1 + 3k^2 t+1}(M + \\lambda_1)^2$$, $$\\xi_{k,t} := \\eta_{t-k_t} \\psi_{k,t}$$\n\n$$V' := 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V$$, $$V_{k,t} := V' + \\lambda_2 \\frac{1 + \\xi_{k,t}}{1 - |\\lambda_2 (P)|}$$\n\nThe basic idea behind these proofs is illustrated in Figure S.1, where we are trying to approximate the matrix product by conditioning back in time just the right amount, to balance the tradeoff between the advantage of the mixing decay and the norm of the product of matrices.\n\nTheorem 2. (General Version) Under Assumptions 1, 2 and 3, for all $$n > k_n$$, and any decaying step-size schedule $$\\eta_i$$ satisfying C.1 and C.2, we have:\n\n$$\n\\begin{align*}\nE v^T_1 B_{n,1} B^T_{n,1} v_1 &\\leq (1 + r)^2 \\exp\\left(\\sum_{t=1}^{n-k_n} 2\\eta_t \\lambda_1 + \\eta^2_t V' + \\lambda_2 \\frac{1 + \\xi_{k,t}}{t}\\right)\n\\end{align*}\n$$\nwhere $$B_{j,i}$$ is defined in 7.\n\nProof. Define $$\\alpha_{n,t} := E\\left[\\text{Tr} v^T_1 B_{n,t} B^T_{n,t} v_1\\right] = E v^T_1 B_{n,t} B^T_{n,t} v_1$$, $$i \\leq t \\leq n$$. Then, we have\n\n$$\n\\begin{align*}\nv^T_1 B_{n,t} B^T_{n,t} v_1 &= v^T_1 B_{n,t+1}(I + \\eta_t \\Sigma)^2 B^T_{n,t+1} v_1 + 2\\eta_t v^T_1 B_{n,t+1}(I + \\eta_t \\Sigma)(A_t - P_{n,t} \\Sigma) B^T_{n,t+1} v_1 \\\\\n&\\quad + \\eta^2_t v^T_1 B_{n,t+1}(A_t - Q_{n,t} \\Sigma)^2 B^T_{n,t+1} v_1 \\quad \\text{(S.50)} \\\\\n&\\leq v^T_1 B_{j,t+1} B^T_{j,t+1} v_1((1 + \\eta_t \\lambda_1)^2) + \\eta^2_t Q_{n,t} + 2\\eta_t P_{n,t}\n\\end{align*}\n$$\nUsing Lemma S.8 with $$U = v_1$$, $$G = (I + \\eta_t \\Sigma)$$, $$\\gamma = 1$$ and noting that $$E_{\\pi}[A_t - \\Sigma] = 0$$, along with observing that $$\\alpha_{n,t+k} \\leq \\alpha_{n,t+kt}$$ from Lemma S.3, we have\n\n$$\n\\begin{align*}"}, {"type": "table", "rows": [["E [P_{n,t}]", "&\\leq \\eta_{t+1} (1 + \\eta_t \\lambda_1)^2 V", "\\lambda_2 (P)"]], "md": "|E [P_{n,t}]| &\\leq \\eta_{t+1} (1 + \\eta_t \\lambda_1)^2 V |\\lambda_2 (P)|^2 + 16\\epsilon + 2 + (1 + \\epsilon)^2 k^2 t+1 (M + \\lambda_1)^2 \\alpha_{n,t+kt} \\\\", "isPerfectTable": true, "csv": "\"E [P_{n,t}]\",\"&\\leq \\eta_{t+1} (1 + \\eta_t \\lambda_1)^2 V\",\"\\lambda_2 (P)\""}, {"type": "text", "value": "&\\quad + \\eta_{t+1} M\n\\end{align*}\n$$", "md": "&\\quad + \\eta_{t+1} M\n\\end{align*}\n$$"}]}, {"page": 33, "text": "We note that \u2200i, ki \u2265         1, therefore, using the assumption in S.49, 1 + \u03b7t\u03bb1 \u2264                   1 + \u03b7tkt (M + \u03bb1) \u2264\n1 + \u03f5.\nNext, using Lemma S.9 with U = v1, G = I, \u03b3 = 1 and noting that                                   E\u03c0     (At \u2212    \u03a3)2     2 \u2264     V\nalong with observing that \u03b1n,t+k            t+1 \u2264    \u03b1n,t+kt using Lemma S.3, we have\n      |E [Qn,t]| \u2264       V + \u03b7t+1M2 (2\u03b7t+1 + (1 + \u03f5) (2 + \u03f5 (1 + \u03f5)) kt+1 (M + \u03bb1))                            \u03b1n,t+kt\n                    \u2264    V + 2\u03f5\u03b7t+1M + \u03b7t+1M2 ((1 + \u03f5) (2 + \u03f5 (1 + \u03f5)) kt+1 (M + \u03bb1))                               \u03b1n,t+kt\nwhere in the last line, we used \u03b7t+1M \u2264                 \u03b7t+1 (M + \u03bb1) \u2264          \u03b7t+1kt+1 (M + \u03bb1) \u2264            \u03f5.\nThen from S.50 for n \u2212           kt \u2265   t \u2265   1,\n     \u03b1n,t \u2264    (1 + \u03b7t\u03bb1)2 \u03b1n,t+1 +           1 + (3 + 4\u03f5) |\u03bb2 (P) |            V\u03b72 t \u03b1n,t+kt + Ck,t\u03b73      t \u03b1n,t+kt       (S.51)\nwhere Ck,t is defined as                              1 \u2212   |\u03bb2 (P)|\nCk,t := M         4 (1 + \u03f5) (1 + 8\u03f5) + 2\u03f5 + kt+1 (M + \u03bb1)                   (1 + \u03f5) (2 + \u03f5 (1 + \u03f5)) M + 2              2 + (1 + \u03f5)2  kt+1 (M + \u03bb1)\n       (i)\n       \u2264   M     4 (1 + \u03f5) (1 + 8\u03f5) + 2\u03f5 +            (1 + \u03f5) (2 + \u03f5 (1 + \u03f5)) + 2           2 + (1 + \u03f5)2          k2\n       = M       4 + 38\u03f5 + 32\u03f52 +          6 + 2\u03f5 + (1 + \u03f5)2 (1 + 2\u03f5)             k2                               t+1 (M + \u03bb1)2\nwhere in (i) we used M \u2264             kt+1 (M + \u03bb1).                                t+1 (M + \u03bb1)2\nThen recalling the definition of \u03bek,t in S.49, and noting that \u03b1n,t+k                          t \u2264     \u03b1n,t+1 using Lemma\nS.3 we have from S.51,\n              \u03b1n,t \u2264    (1 + \u03b7t\u03bb1)2 \u03b1n,t+1 +           1 + (3 + 4\u03f5) |\u03bb2 (P) |              V + \u03bek,t      \u03b72t \u03b1n,t+kt\n                                                                 1 \u2212   |\u03bb2 (P)|\n                    =     1 + 2\u03b7t\u03bb1 + \u03b72      t   1 + (3 + 4\u03f5) |\u03bb2 (P) |             V + \u03bb2   1 + \u03bek,t        \u03b1n,t+1\nTherefore using this recursion, we have,                   1 \u2212   |\u03bb2 (P)|\n   \u03b1n,1 \u2264     \u03b1n,n\u2212kn+1 exp          2\u03bb1   n\u2212kn   \u03b7t +   n\u2212kn   \u03b72t   1 + (3 + 4\u03f5) |\u03bb2 (P) |              V + \u03bb2  1 + \u03bek,t\n                                            t=1           t=1                   1 \u2212  |\u03bb2 (P)|\nLet Bn,n\u2212kn+1 = I + R\u2032, where \u2225R\u2032\u2225\u2264                    r a.s.\n                      \u03b1n,n\u2212kn+1 = E          vT1 Bn,n\u2212kn+1BT       n,n\u2212kn+1v1\n                                      = E    vT       + E     vT                      + E     vT\n                                               1 v1            1 (R\u2032 + R\u2032T )v1                 1 R\u2032R\u2032T v1\nUsing Lemma 2 we have                 \u2264  1 + 2r + r2\n                                r \u2264   (1 + \u03f5) kn\u03b7n\u2212kn+1 (M + \u03bb1)\n                                   \u2264  (1 + \u03f5) kn\u03b7n\u2212kn (M + \u03bb1)\nTherefore,                         \u2264  2 (1 + \u03f5) kn\u03b7n (M + \u03bb1) since \u03b7n\u2212kn \u2264                    2\u03b7n\n \u03b1n,1 \u2264      1 + 2r + r2       exp     2\u03bb1   n\u2212kn   \u03b7t +   n\u2212kn   \u03b72t   1 + (3 + 4\u03f5) |\u03bb2 (P) |              V + \u03bb2  1 + \u03bek,t\nHence proved.                                 t=1           t=1                   1 \u2212  |\u03bb2 (P)|\n                                                                 33", "md": "# Math Equations\n\nWe note that $$\\forall i, k_i \\geq 1$$, therefore, using the assumption in S.49, $$1 + \\eta_t \\lambda_1 \\leq 1 + \\eta_t k_t (M + \\lambda_1) \\leq 1 + \\epsilon$$.\n\nNext, using Lemma S.9 with $$U = v_1$$, $$G = I$$, $$\\gamma = 1$$ and noting that $$E_{\\pi} (A_t - \\Sigma)^2 \\leq V$$ along with observing that $$\\alpha_{n,t+k}^{t+1} \\leq \\alpha_{n,t+kt}$$ using Lemma S.3, we have\n\n$$\n\\begin{align*}\n|E[Q_{n,t}]| &\\leq V + \\eta_{t+1}M^2 (2\\eta_{t+1} + (1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) k_{t+1} (M + \\lambda_1)) \\alpha_{n,t+kt} \\\\\n&\\leq V + 2\\epsilon \\eta_{t+1}M + \\eta_{t+1}M^2 ((1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) k_{t+1} (M + \\lambda_1)) \\alpha_{n,t+kt}\n\\end{align*}\n$$\nwhere in the last line, we used $$\\eta_{t+1}M \\leq \\eta_{t+1} (M + \\lambda_1) \\leq \\eta_{t+1}k_{t+1} (M + \\lambda_1) \\leq \\epsilon$$.\n\nThen from S.50 for $$n - kt \\geq t \\geq 1$$,\n\n$$\n\\alpha_{n,t} \\leq (1 + \\eta_t \\lambda_1)^2 \\alpha_{n,t+1} + 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V\\eta^2 t \\alpha_{n,t+kt} + C_{k,t}\\eta^3 t \\alpha_{n,t+kt} \\text{(S.51)}\n$$\nwhere $$C_{k,t}$$ is defined as\n\n$$\n\\begin{align*}\nC_{k,t} &:= M^4 (1 + \\epsilon) (1 + 8\\epsilon) + 2\\epsilon + k_{t+1} (M + \\lambda_1) (1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) M + 2^2 + (1 + \\epsilon)^2 k_{t+1} (M + \\lambda_1) \\\\\n&\\leq M^4 + 38\\epsilon + 32\\epsilon^2 + 6 + 2\\epsilon + (1 + \\epsilon)^2 (1 + 2\\epsilon) k^2 t+1 (M + \\lambda_1)^2\n\\end{align*}\n$$\nwhere in (i) we used $$M \\leq k_{t+1} (M + \\lambda_1)$$.\n\nThen recalling the definition of $$\\xi_{k,t}$$ in S.49, and noting that $$\\alpha_{n,t+k}^{t} \\leq \\alpha_{n,t+1}$$ using Lemma S.3 we have from S.51,\n\n$$\n\\begin{align*}\n\\alpha_{n,t} &\\leq (1 + \\eta_t \\lambda_1)^2 \\alpha_{n,t+1} + 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\xi_{k,t} \\eta^2 t \\alpha_{n,t+kt} \\\\\n&= 1 + 2\\eta_t \\lambda_1 + \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\alpha_{n,t+1}\n\\end{align*}\n$$\nTherefore using this recursion, we have,\n\n$$\n\\alpha_{n,1} \\leq \\alpha_{n,n-kn+1} \\exp \\left(2\\lambda_1 n-kn \\eta_t + n-kn \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\right)\n$$\nLet $$B_{n,n-kn+1} = I + R'$$, where $$\\|R'\\| \\leq r$$ a.s.\n\n$$\n\\begin{align*}\n\\alpha_{n,n-kn+1} &= E[v_1^T B_{n,n-kn+1} B^T_{n,n-kn+1} v_1] \\\\\n&= E[v_1^T + E[v_1^T + E[v_1^T\n\\end{align*}\n$$\nUsing Lemma 2 we have $$r \\leq 1 + 2r + r^2$$\n\n$$\nr \\leq (1 + \\epsilon) kn\\eta_{n-kn+1} (M + \\lambda_1) \\leq (1 + \\epsilon) kn\\eta_{n-kn} (M + \\lambda_1)\n$$\nTherefore, $$r \\leq 2 (1 + \\epsilon) kn\\eta_n (M + \\lambda_1)$$ since $$\\eta_{n-kn} \\leq 2\\eta_n$$\n\n$$\n\\alpha_{n,1} \\leq 1 + 2r + r^2 \\exp \\left(2\\lambda_1 n-kn \\eta_t + n-kn \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\right)\n$$\nHence proved.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "We note that $$\\forall i, k_i \\geq 1$$, therefore, using the assumption in S.49, $$1 + \\eta_t \\lambda_1 \\leq 1 + \\eta_t k_t (M + \\lambda_1) \\leq 1 + \\epsilon$$.\n\nNext, using Lemma S.9 with $$U = v_1$$, $$G = I$$, $$\\gamma = 1$$ and noting that $$E_{\\pi} (A_t - \\Sigma)^2 \\leq V$$ along with observing that $$\\alpha_{n,t+k}^{t+1} \\leq \\alpha_{n,t+kt}$$ using Lemma S.3, we have\n\n$$\n\\begin{align*}", "md": "We note that $$\\forall i, k_i \\geq 1$$, therefore, using the assumption in S.49, $$1 + \\eta_t \\lambda_1 \\leq 1 + \\eta_t k_t (M + \\lambda_1) \\leq 1 + \\epsilon$$.\n\nNext, using Lemma S.9 with $$U = v_1$$, $$G = I$$, $$\\gamma = 1$$ and noting that $$E_{\\pi} (A_t - \\Sigma)^2 \\leq V$$ along with observing that $$\\alpha_{n,t+k}^{t+1} \\leq \\alpha_{n,t+kt}$$ using Lemma S.3, we have\n\n$$\n\\begin{align*}"}, {"type": "table", "rows": [["E[Q_{n,t}]"]], "md": "|E[Q_{n,t}]| &\\leq V + \\eta_{t+1}M^2 (2\\eta_{t+1} + (1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) k_{t+1} (M + \\lambda_1)) \\alpha_{n,t+kt} \\\\", "isPerfectTable": true, "csv": "\"E[Q_{n,t}]\""}, {"type": "text", "value": "&\\leq V + 2\\epsilon \\eta_{t+1}M + \\eta_{t+1}M^2 ((1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) k_{t+1} (M + \\lambda_1)) \\alpha_{n,t+kt}\n\\end{align*}\n$$\nwhere in the last line, we used $$\\eta_{t+1}M \\leq \\eta_{t+1} (M + \\lambda_1) \\leq \\eta_{t+1}k_{t+1} (M + \\lambda_1) \\leq \\epsilon$$.\n\nThen from S.50 for $$n - kt \\geq t \\geq 1$$,\n\n$$\n\\alpha_{n,t} \\leq (1 + \\eta_t \\lambda_1)^2 \\alpha_{n,t+1} + 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V\\eta^2 t \\alpha_{n,t+kt} + C_{k,t}\\eta^3 t \\alpha_{n,t+kt} \\text{(S.51)}\n$$\nwhere $$C_{k,t}$$ is defined as\n\n$$\n\\begin{align*}\nC_{k,t} &:= M^4 (1 + \\epsilon) (1 + 8\\epsilon) + 2\\epsilon + k_{t+1} (M + \\lambda_1) (1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) M + 2^2 + (1 + \\epsilon)^2 k_{t+1} (M + \\lambda_1) \\\\\n&\\leq M^4 + 38\\epsilon + 32\\epsilon^2 + 6 + 2\\epsilon + (1 + \\epsilon)^2 (1 + 2\\epsilon) k^2 t+1 (M + \\lambda_1)^2\n\\end{align*}\n$$\nwhere in (i) we used $$M \\leq k_{t+1} (M + \\lambda_1)$$.\n\nThen recalling the definition of $$\\xi_{k,t}$$ in S.49, and noting that $$\\alpha_{n,t+k}^{t} \\leq \\alpha_{n,t+1}$$ using Lemma S.3 we have from S.51,\n\n$$\n\\begin{align*}\n\\alpha_{n,t} &\\leq (1 + \\eta_t \\lambda_1)^2 \\alpha_{n,t+1} + 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\xi_{k,t} \\eta^2 t \\alpha_{n,t+kt} \\\\\n&= 1 + 2\\eta_t \\lambda_1 + \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\alpha_{n,t+1}\n\\end{align*}\n$$\nTherefore using this recursion, we have,\n\n$$\n\\alpha_{n,1} \\leq \\alpha_{n,n-kn+1} \\exp \\left(2\\lambda_1 n-kn \\eta_t + n-kn \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\right)\n$$\nLet $$B_{n,n-kn+1} = I + R'$$, where $$\\|R'\\| \\leq r$$ a.s.\n\n$$\n\\begin{align*}\n\\alpha_{n,n-kn+1} &= E[v_1^T B_{n,n-kn+1} B^T_{n,n-kn+1} v_1] \\\\\n&= E[v_1^T + E[v_1^T + E[v_1^T\n\\end{align*}\n$$\nUsing Lemma 2 we have $$r \\leq 1 + 2r + r^2$$\n\n$$\nr \\leq (1 + \\epsilon) kn\\eta_{n-kn+1} (M + \\lambda_1) \\leq (1 + \\epsilon) kn\\eta_{n-kn} (M + \\lambda_1)\n$$\nTherefore, $$r \\leq 2 (1 + \\epsilon) kn\\eta_n (M + \\lambda_1)$$ since $$\\eta_{n-kn} \\leq 2\\eta_n$$\n\n$$\n\\alpha_{n,1} \\leq 1 + 2r + r^2 \\exp \\left(2\\lambda_1 n-kn \\eta_t + n-kn \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\right)\n$$\nHence proved.", "md": "&\\leq V + 2\\epsilon \\eta_{t+1}M + \\eta_{t+1}M^2 ((1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) k_{t+1} (M + \\lambda_1)) \\alpha_{n,t+kt}\n\\end{align*}\n$$\nwhere in the last line, we used $$\\eta_{t+1}M \\leq \\eta_{t+1} (M + \\lambda_1) \\leq \\eta_{t+1}k_{t+1} (M + \\lambda_1) \\leq \\epsilon$$.\n\nThen from S.50 for $$n - kt \\geq t \\geq 1$$,\n\n$$\n\\alpha_{n,t} \\leq (1 + \\eta_t \\lambda_1)^2 \\alpha_{n,t+1} + 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V\\eta^2 t \\alpha_{n,t+kt} + C_{k,t}\\eta^3 t \\alpha_{n,t+kt} \\text{(S.51)}\n$$\nwhere $$C_{k,t}$$ is defined as\n\n$$\n\\begin{align*}\nC_{k,t} &:= M^4 (1 + \\epsilon) (1 + 8\\epsilon) + 2\\epsilon + k_{t+1} (M + \\lambda_1) (1 + \\epsilon) (2 + \\epsilon (1 + \\epsilon)) M + 2^2 + (1 + \\epsilon)^2 k_{t+1} (M + \\lambda_1) \\\\\n&\\leq M^4 + 38\\epsilon + 32\\epsilon^2 + 6 + 2\\epsilon + (1 + \\epsilon)^2 (1 + 2\\epsilon) k^2 t+1 (M + \\lambda_1)^2\n\\end{align*}\n$$\nwhere in (i) we used $$M \\leq k_{t+1} (M + \\lambda_1)$$.\n\nThen recalling the definition of $$\\xi_{k,t}$$ in S.49, and noting that $$\\alpha_{n,t+k}^{t} \\leq \\alpha_{n,t+1}$$ using Lemma S.3 we have from S.51,\n\n$$\n\\begin{align*}\n\\alpha_{n,t} &\\leq (1 + \\eta_t \\lambda_1)^2 \\alpha_{n,t+1} + 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\xi_{k,t} \\eta^2 t \\alpha_{n,t+kt} \\\\\n&= 1 + 2\\eta_t \\lambda_1 + \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\alpha_{n,t+1}\n\\end{align*}\n$$\nTherefore using this recursion, we have,\n\n$$\n\\alpha_{n,1} \\leq \\alpha_{n,n-kn+1} \\exp \\left(2\\lambda_1 n-kn \\eta_t + n-kn \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\right)\n$$\nLet $$B_{n,n-kn+1} = I + R'$$, where $$\\|R'\\| \\leq r$$ a.s.\n\n$$\n\\begin{align*}\n\\alpha_{n,n-kn+1} &= E[v_1^T B_{n,n-kn+1} B^T_{n,n-kn+1} v_1] \\\\\n&= E[v_1^T + E[v_1^T + E[v_1^T\n\\end{align*}\n$$\nUsing Lemma 2 we have $$r \\leq 1 + 2r + r^2$$\n\n$$\nr \\leq (1 + \\epsilon) kn\\eta_{n-kn+1} (M + \\lambda_1) \\leq (1 + \\epsilon) kn\\eta_{n-kn} (M + \\lambda_1)\n$$\nTherefore, $$r \\leq 2 (1 + \\epsilon) kn\\eta_n (M + \\lambda_1)$$ since $$\\eta_{n-kn} \\leq 2\\eta_n$$\n\n$$\n\\alpha_{n,1} \\leq 1 + 2r + r^2 \\exp \\left(2\\lambda_1 n-kn \\eta_t + n-kn \\eta^2 t 1 + (3 + 4\\epsilon) |\\lambda_2 (P)| V + \\lambda^2 1 + \\xi_{k,t} \\right)\n$$\nHence proved."}]}, {"page": 34, "text": "Theorem 3. (General Version) Let u := min {t : t \u2208                             [n], t \u2212    kt \u2265    0}. Under Assumptions 1, 2\nand 3, for all n > u, and any decaying step-size \u03b7i satisfying C.1 and C.2, we have,\n E    Tr    V T                    \u2264   (1 + 5\u03f5) exp            n     2\u03b7i\u03bb2 +       V\u2032 + \u03bb2   1 + \u03bek,i     \u03b72i\u2212ki\n              \u22a5  BnBT   n V\u22a5                 \uf8eb            n i=u+1                                   \uf8eb      i                         \uf8f6\uf8f6\n                                          \u00d7  \uf8edd +      i=u+1    (V\u2032 + \u03bek,i) C\u2032    k,i\u03b72 i\u2212ki exp    \uf8ed  j=u+1     2\u03b7j (\u03bb1 \u2212      \u03bb2)  \uf8f8\uf8f8\nwhere C\u2032    k,t := exp       2\u03bb1    u  j=1 (\u03b7j \u2212     \u03b7t\u2212u+j) +  t\u2212u      j=1 \u03b72  j   Vk,j \u2212    Vk,j+u         and Bt is defined in 2.\nProof. For t \u2264        n, let\n             \u03b1t := \u03b1t,1 = E          vT1 BtBT   t v1    = E     Tr    vT1 BtBT   t v1     , as defined in Theorem 2\n             \u03b2t := E       Tr   V T\u22a5  BtBT  t V\u22a5\nNote that \u03b1t + \u03b2t = Tr             BtBT   t    by definition. Then,\nTr    BtBT  t V\u22a5V T  \u22a5     = Tr     Bt\u22121BT     t\u22121(I + \u03b7t\u03a3)V\u22a5V T        \u22a5  (I + \u03b7t\u03a3)        + \u03b7t Tr      BT t\u22121(I + \u03b7t\u03a3)V\u22a5V T         \u22a5  (At \u2212    \u03a3)Bt\u22121\n                                  + \u03b7t Tr      BT t\u22121(At \u2212       \u03a3)V\u22a5V T   \u22a5  (I + \u03b7t\u03a3)Bt\u22121           + \u03b72  t Tr    Bt\u22121BT    t\u22121(At \u2212       \u03a3)V\u22a5V T   \u22a5  (At \u2212   \u03a3)\n                           \u2264   (1 + \u03b7t\u03bb2)2 Tr         Bt\u22121BT    t\u22121V\u22a5V T    \u22a5    + 2\u03b7t Tr       Bt\u22121BT    t\u22121(I + \u03b7t\u03a3)V\u22a5V T         \u22a5  (At \u2212     \u03a3)\n                                                                                                                       Pt\n                                  + \u03b72  t Tr   Bt\u22121BT     t\u22121(At \u2212    Qt \u03a3)V\u22a5V T   \u22a5  (At \u2212     \u03a3)\nLet Bt\u22121 = (I + R) Bt\u2212kt with \u2225R\u22252\u2264                          r. Using Lemma S.10 with G = (I + \u03b7t\u03a3) V\u22a5V T                            \u22a5    =\nV\u22a5(I + \u03b7t\u039b\u22a5)V T        \u22a5  , \u03b3 = 1, where \u039b\u22a5          is a d \u2212    1 \u00d7 d \u2212      1 diagonal matrix of eigenvalues \u03bb2, . . . , \u03bbd\nof \u03a3, and noting that          V\u22a5V T   \u22a5    2 = 1,\nE [Pt] \u2264     (1 + \u03b7t\u03bb1) \u03b7t\u2212kt            2V |\u03bb2 (P)|                          2 (1 + 8\u03f5) +         2 + (1 + \u03f5)2        k2t (M + \u03bb1)2             (\u03b1t\u2212kt + \u03b2t\u2212kt)\n                                         1 \u2212   |\u03bb2 (P)| + \u03b7t\u2212ktM\n         \u2264   (1 + \u03f5) \u03b7t\u2212kt          2V |\u03bb2 (P)|                          2 (1 + 8\u03f5) +         2 + (1 + \u03f5)2        k2t (M + \u03bb1)2             (\u03b1t\u2212kt + \u03b2t\u2212kt)\n                                   1 \u2212   |\u03bb2 (P)| + \u03b7t\u2212ktM\nwhere in the last line, we used \u03b7t\u03bb1 \u2264                 \u03b7tkt (M + \u03bb1) \u2264           \u03f5.\nUsing Lemma S.11 with U = V\u22a5V T                  \u22a5  , \u03b3 = 1,\nE [Qt] \u2264  (i)  V + \u03b7t\u2212kt+1M2 (2\u03b7t + 2 (1 + \u03f5) (1 + \u03f5 (1 + \u03f5)) kt (M + \u03bb1))                                 (\u03b1t\u2212kt + \u03b2t\u2212kt)\n          \u2264     V + 2\u03f5\u03b7tM + 2\u03b7t\u2212kt+1M2 ((1 + \u03f5) (1 + \u03f5 (1 + \u03f5)) kt (M + \u03bb1))                                     (\u03b1t\u2212kt + \u03b2t\u2212kt)\n          (ii)\n          \u2264      V + 2\u03f5\u03b7tM + 2\u03b7t\u2212kt+1M                     (1 + \u03f5) (1 + \u03f5 (1 + \u03f5)) k2        t (M + \u03bb1)2             (\u03b1t\u2212kt + \u03b2t\u2212kt)\nwhere in (i) we used \u2200i, \u03b7iM \u2264                     \u03b7iki (M + \u03bb1) \u2264             \u03f5 and in (ii) we used M \u2264                 kt (M + \u03bb1).\n                                                                    34", "md": "Theorem 3. (General Version) Let \\( u := \\min \\{t : t \\in [n], t - kt \\geq 0\\} \\). Under Assumptions 1, 2 and 3, for all \\( n > u \\), and any decaying step-size \\( \\eta_i \\) satisfying C.1 and C.2, we have,\n\n\\[\nE \\text{Tr} V^\\top \\perp B_n B^\\top_n V^\\perp \\leq (1 + 5\\epsilon) \\exp\\left( n \\frac{2\\eta_i\\lambda_2 + V' + \\lambda_2}{1 + \\xi_{k,i}} \\eta^2_{i-k_i} \\right)\n\\]\n\\[\n\\times \\left( d + \\sum_{i=u+1} \\left( V' + \\xi_{k,i} \\right) C'_{k,i}\\eta^2_{i-k_i} \\exp\\left( \\sum_{j=u+1} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right) \\right)\n\\]\n\nwhere \\( C'_{k,t} := \\exp\\left( 2\\lambda_1 \\sum_{j=1}^u (\\eta_j - \\eta_{t-u+j}) + \\sum_{j=1}^{t-u} \\eta^2_j V_{k,j} - V_{k,j+u} \\right) \\) and \\( B_t \\) is defined in 2.\n\n**Proof.** For \\( t \\leq n \\), let\n\n\\[\n\\alpha_t := \\alpha_{t,1} = E v^\\top_1 B_t B^\\top_t v_1 = E \\text{Tr} v^\\top_1 B_t B^\\top_t v_1\n\\]\n\n\\[\n\\beta_t := E \\text{Tr} V^\\top_\\perp B_t B^\\top_t V^\\perp\n\\]\n\nNote that \\( \\alpha_t + \\beta_t = \\text{Tr} B_t B^\\top_t \\) by definition. Then,\n\n\\[\n\\begin{aligned}\n\\text{Tr} B_t B^\\top_t V^\\perp V^\\top_\\perp & = \\text{Tr} B_{t-1} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (I + \\eta_t\\Sigma) \\\\\n& \\quad + \\eta_t \\text{Tr} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma)B_{t-1} \\\\\n& \\quad + \\eta_t \\text{Tr} B^\\top_{t-1}(A_t - \\Sigma)V^\\perp V^\\top_\\perp (I + \\eta_t\\Sigma)B_{t-1} \\\\\n& \\leq (1 + \\eta_t\\lambda_2)^2 \\text{Tr} B_{t-1} B^\\top_{t-1} V^\\perp V^\\top_\\perp \\\\\n& \\quad + 2\\eta_t \\text{Tr} B_{t-1} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma) \\\\\n& \\quad + \\eta^2_t \\text{Tr} B_{t-1} B^\\top_{t-1}(A_t - \\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma)\n\\end{aligned}\n\\]\n\nLet \\( B_{t-1} = (I + R) B_t - k_t \\) with \\( \\|R\\|_2 \\leq r \\). Using Lemma S.10 with \\( G = (I + \\eta_t\\Sigma) V^\\perp V^\\top_\\perp = V^\\perp(I + \\eta_t\\Lambda^\\perp)V^\\top_\\perp \\), \\( \\gamma = 1 \\), where \\( \\Lambda^\\perp \\) is a \\( d - 1 \\times d - 1 \\) diagonal matrix of eigenvalues \\( \\lambda_2, \\ldots, \\lambda_d \\) of \\( \\Sigma \\), and noting that \\( V^\\perp V^\\top_\\perp = 1 \\),\n\n\\[\nE[P_t] \\leq (1 + \\eta_t\\lambda_1) \\eta^{t-k_t}_t 2V|\\lambda_2(P)|^2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\]\n\n\\[\n\\leq (1 + \\epsilon) \\eta^{t-k_t}_t 2V|\\lambda_2(P)|^2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\]\n\nwhere in the last line, we used \\( \\eta_t\\lambda_1 \\leq \\eta_t k_t (M + \\lambda_1) \\leq \\epsilon \\).\n\nUsing Lemma S.11 with \\( U = V^\\perp V^\\top_\\perp \\), \\( \\gamma = 1 \\),\n\n\\[\n\\begin{aligned}\nE[Q_t] & \\leq V + \\eta^{t-k_t+1}_t M^2 \\left( 2\\eta_t + 2(1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k_t (M + \\lambda_1) \\right) (\\alpha_{t-k_t} + \\beta_{t-k_t}) \\\\\n& \\leq V + 2\\epsilon\\eta_t M + 2\\eta^{t-k_t+1}_t M^2 \\left( (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k_t (M + \\lambda_1) \\right) (\\alpha_{t-k_t} + \\beta_{t-k_t}) \\\\\n& \\leq V + 2\\epsilon\\eta_t M + 2\\eta^{t-k_t+1}_t M (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\end{aligned}\n\\]\n\nwhere in (i) we used \\( \\forall i, \\eta_i M \\leq \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon \\) and in (ii) we used \\( M \\leq k_t (M + \\lambda_1) \\).", "images": [], "items": [{"type": "text", "value": "Theorem 3. (General Version) Let \\( u := \\min \\{t : t \\in [n], t - kt \\geq 0\\} \\). Under Assumptions 1, 2 and 3, for all \\( n > u \\), and any decaying step-size \\( \\eta_i \\) satisfying C.1 and C.2, we have,\n\n\\[\nE \\text{Tr} V^\\top \\perp B_n B^\\top_n V^\\perp \\leq (1 + 5\\epsilon) \\exp\\left( n \\frac{2\\eta_i\\lambda_2 + V' + \\lambda_2}{1 + \\xi_{k,i}} \\eta^2_{i-k_i} \\right)\n\\]\n\\[\n\\times \\left( d + \\sum_{i=u+1} \\left( V' + \\xi_{k,i} \\right) C'_{k,i}\\eta^2_{i-k_i} \\exp\\left( \\sum_{j=u+1} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right) \\right)\n\\]\n\nwhere \\( C'_{k,t} := \\exp\\left( 2\\lambda_1 \\sum_{j=1}^u (\\eta_j - \\eta_{t-u+j}) + \\sum_{j=1}^{t-u} \\eta^2_j V_{k,j} - V_{k,j+u} \\right) \\) and \\( B_t \\) is defined in 2.\n\n**Proof.** For \\( t \\leq n \\), let\n\n\\[\n\\alpha_t := \\alpha_{t,1} = E v^\\top_1 B_t B^\\top_t v_1 = E \\text{Tr} v^\\top_1 B_t B^\\top_t v_1\n\\]\n\n\\[\n\\beta_t := E \\text{Tr} V^\\top_\\perp B_t B^\\top_t V^\\perp\n\\]\n\nNote that \\( \\alpha_t + \\beta_t = \\text{Tr} B_t B^\\top_t \\) by definition. Then,\n\n\\[\n\\begin{aligned}\n\\text{Tr} B_t B^\\top_t V^\\perp V^\\top_\\perp & = \\text{Tr} B_{t-1} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (I + \\eta_t\\Sigma) \\\\\n& \\quad + \\eta_t \\text{Tr} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma)B_{t-1} \\\\\n& \\quad + \\eta_t \\text{Tr} B^\\top_{t-1}(A_t - \\Sigma)V^\\perp V^\\top_\\perp (I + \\eta_t\\Sigma)B_{t-1} \\\\\n& \\leq (1 + \\eta_t\\lambda_2)^2 \\text{Tr} B_{t-1} B^\\top_{t-1} V^\\perp V^\\top_\\perp \\\\\n& \\quad + 2\\eta_t \\text{Tr} B_{t-1} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma) \\\\\n& \\quad + \\eta^2_t \\text{Tr} B_{t-1} B^\\top_{t-1}(A_t - \\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma)\n\\end{aligned}\n\\]\n\nLet \\( B_{t-1} = (I + R) B_t - k_t \\) with \\( \\|R\\|_2 \\leq r \\). Using Lemma S.10 with \\( G = (I + \\eta_t\\Sigma) V^\\perp V^\\top_\\perp = V^\\perp(I + \\eta_t\\Lambda^\\perp)V^\\top_\\perp \\), \\( \\gamma = 1 \\), where \\( \\Lambda^\\perp \\) is a \\( d - 1 \\times d - 1 \\) diagonal matrix of eigenvalues \\( \\lambda_2, \\ldots, \\lambda_d \\) of \\( \\Sigma \\), and noting that \\( V^\\perp V^\\top_\\perp = 1 \\),\n\n\\[\nE[P_t] \\leq (1 + \\eta_t\\lambda_1) \\eta^{t-k_t}_t 2V|\\lambda_2(P)|^2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\]\n\n\\[\n\\leq (1 + \\epsilon) \\eta^{t-k_t}_t 2V|\\lambda_2(P)|^2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\]\n\nwhere in the last line, we used \\( \\eta_t\\lambda_1 \\leq \\eta_t k_t (M + \\lambda_1) \\leq \\epsilon \\).\n\nUsing Lemma S.11 with \\( U = V^\\perp V^\\top_\\perp \\), \\( \\gamma = 1 \\),\n\n\\[\n\\begin{aligned}\nE[Q_t] & \\leq V + \\eta^{t-k_t+1}_t M^2 \\left( 2\\eta_t + 2(1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k_t (M + \\lambda_1) \\right) (\\alpha_{t-k_t} + \\beta_{t-k_t}) \\\\\n& \\leq V + 2\\epsilon\\eta_t M + 2\\eta^{t-k_t+1}_t M^2 \\left( (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k_t (M + \\lambda_1) \\right) (\\alpha_{t-k_t} + \\beta_{t-k_t}) \\\\\n& \\leq V + 2\\epsilon\\eta_t M + 2\\eta^{t-k_t+1}_t M (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\end{aligned}\n\\]\n\nwhere in (i) we used \\( \\forall i, \\eta_i M \\leq \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon \\) and in (ii) we used \\( M \\leq k_t (M + \\lambda_1) \\).", "md": "Theorem 3. (General Version) Let \\( u := \\min \\{t : t \\in [n], t - kt \\geq 0\\} \\). Under Assumptions 1, 2 and 3, for all \\( n > u \\), and any decaying step-size \\( \\eta_i \\) satisfying C.1 and C.2, we have,\n\n\\[\nE \\text{Tr} V^\\top \\perp B_n B^\\top_n V^\\perp \\leq (1 + 5\\epsilon) \\exp\\left( n \\frac{2\\eta_i\\lambda_2 + V' + \\lambda_2}{1 + \\xi_{k,i}} \\eta^2_{i-k_i} \\right)\n\\]\n\\[\n\\times \\left( d + \\sum_{i=u+1} \\left( V' + \\xi_{k,i} \\right) C'_{k,i}\\eta^2_{i-k_i} \\exp\\left( \\sum_{j=u+1} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right) \\right)\n\\]\n\nwhere \\( C'_{k,t} := \\exp\\left( 2\\lambda_1 \\sum_{j=1}^u (\\eta_j - \\eta_{t-u+j}) + \\sum_{j=1}^{t-u} \\eta^2_j V_{k,j} - V_{k,j+u} \\right) \\) and \\( B_t \\) is defined in 2.\n\n**Proof.** For \\( t \\leq n \\), let\n\n\\[\n\\alpha_t := \\alpha_{t,1} = E v^\\top_1 B_t B^\\top_t v_1 = E \\text{Tr} v^\\top_1 B_t B^\\top_t v_1\n\\]\n\n\\[\n\\beta_t := E \\text{Tr} V^\\top_\\perp B_t B^\\top_t V^\\perp\n\\]\n\nNote that \\( \\alpha_t + \\beta_t = \\text{Tr} B_t B^\\top_t \\) by definition. Then,\n\n\\[\n\\begin{aligned}\n\\text{Tr} B_t B^\\top_t V^\\perp V^\\top_\\perp & = \\text{Tr} B_{t-1} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (I + \\eta_t\\Sigma) \\\\\n& \\quad + \\eta_t \\text{Tr} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma)B_{t-1} \\\\\n& \\quad + \\eta_t \\text{Tr} B^\\top_{t-1}(A_t - \\Sigma)V^\\perp V^\\top_\\perp (I + \\eta_t\\Sigma)B_{t-1} \\\\\n& \\leq (1 + \\eta_t\\lambda_2)^2 \\text{Tr} B_{t-1} B^\\top_{t-1} V^\\perp V^\\top_\\perp \\\\\n& \\quad + 2\\eta_t \\text{Tr} B_{t-1} B^\\top_{t-1}(I + \\eta_t\\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma) \\\\\n& \\quad + \\eta^2_t \\text{Tr} B_{t-1} B^\\top_{t-1}(A_t - \\Sigma)V^\\perp V^\\top_\\perp (A_t - \\Sigma)\n\\end{aligned}\n\\]\n\nLet \\( B_{t-1} = (I + R) B_t - k_t \\) with \\( \\|R\\|_2 \\leq r \\). Using Lemma S.10 with \\( G = (I + \\eta_t\\Sigma) V^\\perp V^\\top_\\perp = V^\\perp(I + \\eta_t\\Lambda^\\perp)V^\\top_\\perp \\), \\( \\gamma = 1 \\), where \\( \\Lambda^\\perp \\) is a \\( d - 1 \\times d - 1 \\) diagonal matrix of eigenvalues \\( \\lambda_2, \\ldots, \\lambda_d \\) of \\( \\Sigma \\), and noting that \\( V^\\perp V^\\top_\\perp = 1 \\),\n\n\\[\nE[P_t] \\leq (1 + \\eta_t\\lambda_1) \\eta^{t-k_t}_t 2V|\\lambda_2(P)|^2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\]\n\n\\[\n\\leq (1 + \\epsilon) \\eta^{t-k_t}_t 2V|\\lambda_2(P)|^2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\]\n\nwhere in the last line, we used \\( \\eta_t\\lambda_1 \\leq \\eta_t k_t (M + \\lambda_1) \\leq \\epsilon \\).\n\nUsing Lemma S.11 with \\( U = V^\\perp V^\\top_\\perp \\), \\( \\gamma = 1 \\),\n\n\\[\n\\begin{aligned}\nE[Q_t] & \\leq V + \\eta^{t-k_t+1}_t M^2 \\left( 2\\eta_t + 2(1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k_t (M + \\lambda_1) \\right) (\\alpha_{t-k_t} + \\beta_{t-k_t}) \\\\\n& \\leq V + 2\\epsilon\\eta_t M + 2\\eta^{t-k_t+1}_t M^2 \\left( (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k_t (M + \\lambda_1) \\right) (\\alpha_{t-k_t} + \\beta_{t-k_t}) \\\\\n& \\leq V + 2\\epsilon\\eta_t M + 2\\eta^{t-k_t+1}_t M (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon)) k^2_t (M + \\lambda_1)^2 (\\alpha_{t-k_t} + \\beta_{t-k_t})\n\\end{aligned}\n\\]\n\nwhere in (i) we used \\( \\forall i, \\eta_i M \\leq \\eta_i k_i (M + \\lambda_1) \\leq \\epsilon \\) and in (ii) we used \\( M \\leq k_t (M + \\lambda_1) \\)."}]}, {"page": 35, "text": "Putting everything together, we have,\nE   Tr    BtBT   t V\u22a5V T  \u22a5\n   \u2264   (1 + \u03b7t\u03bb2)2 \u03b2t\u22121\n        + 2 (1 + \u03f5) \u03b7t\u03b7t\u2212kt             2V |\u03bb2 (P)|                          2 (1 + 8\u03f5) +         2 + (1 + \u03f5)2         k2                        (\u03b1t\u2212kt + \u03b2t\u2212kt)\n                                        1 \u2212   |\u03bb2 (P)| + \u03b7t\u2212ktM                                                         t (M + \u03bb1)2\n         + \u03b72  t   V + 2\u03f5\u03b7tM + 2\u03b7t\u2212kt+1M                     (1 + \u03f5) (1 + \u03f5 (1 + \u03f5)) k2        t (M + \u03bb1)2             (\u03b1t\u2212kt + \u03b2t\u2212kt)\n   \u2264   (1 + \u03b7t\u03bb2)2 \u03b2t\u22121\n        + 2 (1 + \u03f5) \u03b72               2V |\u03bb2 (P)|                          2 (1 + 8\u03f5) +         2 + (1 + \u03f5)2         k2                        (\u03b1t\u2212kt + \u03b2t\u2212kt)\n                           t\u2212kt      1 \u2212   |\u03bb2 (P)| + \u03b7t\u2212ktM                                                         t (M + \u03bb1)2\n         + \u03b72  t\u2212kt     V + 2\u03f5\u03b7tM + 2\u03b7t\u2212kt+1M                    (1 + \u03f5) (1 + \u03f5 (1 + \u03f5)) k2         t (M + \u03bb1)2            (\u03b1t\u2212kt + \u03b2t\u2212kt)\n   \u2264   (1 + \u03b7t\u03bb2)2 \u03b2t\u22121 + \u03b72                  1 + (3 + 4\u03f5) |\u03bb2 (P) |                V + \u03bek,t       (\u03b1t\u2212kt + \u03b2t\u2212kt)\n                                     t\u2212kt               1 \u2212   |\u03bb2 (P)|\nwhere \u03bek,t is as defined in S.49. Therefore using Lemma S.4,\nE    Tr    BtBT  t V\u22a5V T  \u22a5      \u2264      1 + 2\u03b7t\u03bb2 + \u03b72      t\u2212kt     1 + (3 + 4\u03f5) |\u03bb2 (P) |                V + \u03bb2   2 + \u03bek,t         \u03b2t\u22121\n                                                                               1 \u2212   |\u03bb2 (P)|\n                                 + \u03b72          1 + (3 + 4\u03f5) |\u03bb2 (P) |                V + \u03bek,t        \u03b1 t\u22121                          (S.52)\n                                      t\u2212kt               1 \u2212   |\u03bb2 (P)|\nLet \u03c7\u03f5 := 1 + 4\u03f5 (1 + \u03f5)             1 + \u03f5 + \u03f52        \u2264  1.05. From Theorem 2 denoting                                                1 + \u03f5 + \u03f52        = \u03c7\u03f5,\nrk,t := 1 + 4 (1 + \u03f5) \u03b7t\u22121kt\u22121 (M + \u03bb1) + 4 (1 + c)2 \u03b72                         t\u22121k2  t\u22121 (M + \u03bb1)2 \u2264           1 + 4\u03f5 (1 + \u03f5)     (S.53)\nwe have,\n     \u03b1t\u22121 \u2264      rk,t exp       2\u03bb1   t\u2212kt\u22121    \u03b7t +   t\u2212kt\u22121    \u03b72i   1 + (3 + 4\u03f5) |\u03bb2 (P) |                 V + \u03bb2   1 + \u03bek,i\n                                        i=1               i=1                     1 \u2212   |\u03bb2 (P)|\nNow, we note the definition of Vk,t and V\u2032 as mentioned in S.49 -\n                                    Vk,t :=     1 + (3 + 4\u03f5) |\u03bb2 (P) |               V + \u03bb2   1 + \u03bek,t\n                                                         1 \u2212   |\u03bb2 (P)|\nTherefore using S.52,                     = V\u2032 + \u03bb2     1 + \u03bek,t\n\u03b2t \u2264      1 + 2\u03b7t\u03bb2 + \u03b72      t\u2212ktVk,t      \u03b2t\u22121 + \u03b72    t\u2212ktrk,t (V\u2032 + \u03bek,t) exp             2\u03bb1   t\u2212kt\u22121     \u03b7i +   t\u2212kt\u22121    \u03b72i Vk,i\n                                                                                                       i=1              i=1\nRecursing on the above inequality for u < t \u2264                      n where u = min {i : i \u2208             [n], i \u2212   ki \u2265    0}, we have,\n\u03b2n \u2264    \u03b2u exp        2    n     \u03b7i\u03bb2 +        n     Vk,i\u03b72 i\u2212ki\n                        i=u+1               i=u+1\n                 n                                         \uf8eb      n                                    \uf8f6        \uf8ebi\u2212ki                           \uf8f6\n          +   i=u+1    rk,i (V\u2032 + \u03bek,i) \u03b72     i\u2212ki exp    \uf8ed  j=i+1      2\u03b7j\u03bb2 + Vk,j\u03b72       j\u2212kj     \uf8f8  exp   \uf8ed   j=1   2\u03b7j\u03bb1 + Vk,j\u03b72       j\uf8f8\n     \u2264  exp          n    2\u03b7i\u03bb2 + Vk,i\u03b72      i\u2212ki\n                 i=u+1\n              \uf8eb              n                                         \uf8eb  i\u2212ki                                   i                                    \uf8f6\uf8f6\n          \u00d7   \uf8ed\u03b2u +      i=u+1    rk,i (V\u2032 + \u03bek,i) \u03b72     i\u2212ki exp  35 \uf8ed  j=1   (2\u03b7j\u03bb1 + Vk,j\u03b72       j ) \u2212  j=u+1       2\u03b7j\u03bb2 + Vk,j\u03b72      j\u2212kj     \uf8f8\uf8f8", "md": "Putting everything together, we have,\n\n$$\nE \\leq Tr \\cdot BtBT \\cdot t V^{\\perp}V T^{\\perp} \\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + 2(1 + \\epsilon) \\eta_t \\eta_{t-k_t} \\cdot 2|V| \\cdot |\\lambda^2 (P)| \\cdot \\frac{2(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2}{k^2} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t}) \\cdot \\frac{1 - |\\lambda^2 (P)| + \\eta_{t-k_t}M}{t(M + \\lambda_1)^2} + \\eta^2_t V + 2\\epsilon \\eta_t M + 2\\eta_{t-k_t+1}M \\cdot (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon))k^2 \\cdot \\frac{t(M + \\lambda_1)^2}{(\\alpha_t - k_t + \\beta_{t-k_t})}\n$$\n\n$$\n\\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + 2(1 + \\epsilon) \\eta^2_{t-k_t} \\cdot 2|V| \\cdot |\\lambda^2 (P)| \\cdot \\frac{2(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2}{k^2} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t}) \\cdot \\frac{1 - |\\lambda^2 (P)| + \\eta_{t-k_t}M}{t(M + \\lambda_1)^2} + \\eta^2_{t-k_t} V + 2\\epsilon \\eta_t M + 2\\eta_{t-k_t+1}M \\cdot (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon))k^2 \\cdot \\frac{t(M + \\lambda_1)^2}{(\\alpha_t - k_t + \\beta_{t-k_t})}\n$$\n\n$$\n\\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + \\eta^2 \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\xi_{k,t}} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t})\n$$\n\nwhere $\\xi_{k,t}$ is as defined in S.49. Therefore using Lemma S.4,\n\n$$\nE \\leq Tr \\cdot BtBT \\cdot t V^{\\perp}V T^{\\perp} \\leq 1 + 2\\eta_t \\lambda^2 + \\eta^2_{t-k_t} \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} + \\xi_{k,t} \\cdot \\beta_{t-1} \\cdot \\frac{1 - |\\lambda^2 (P)|}{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|} + \\eta^2 \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\xi_{k,t}} \\cdot \\alpha_{t-1} \\quad (S.52)\n$$\n\nLet $\\chi_{\\epsilon} := \\frac{1 + 4\\epsilon(1 + \\epsilon)}{1 + \\epsilon + \\epsilon^2} \\leq 1.05$. From Theorem 2 denoting $\\chi_{\\epsilon} = \\chi_{\\epsilon}$,\n\n$$\nr_{k,t} := 1 + 4(1 + \\epsilon) \\eta_{t-1}k_{t-1}(M + \\lambda_1) + 4(1 + c)^2 \\eta^2_{t-1}k^2_{t-1}(M + \\lambda_1)^2 \\leq 1 + 4\\epsilon(1 + \\epsilon) \\quad (S.53)\n$$\n\nwe have,\n\n$$\n\\alpha_{t-1} \\leq r_{k,t} \\exp \\left(2\\lambda_1 t-k_{t-1} \\eta_t + t-k_{t-1} \\eta^2\\right) \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} \\cdot \\frac{1 + \\xi_{k,i}}{1 - |\\lambda^2 (P)|}\n$$\n\nNow, we note the definition of $V_{k,t}$ and $V'$ as mentioned in S.49 -\n\n$$\nV_{k,t} := \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} \\cdot \\frac{1 + \\xi_{k,t}}{1 - |\\lambda^2 (P)|}\n$$\n\nTherefore using S.52,\n\n$$\nV_{k,t} = V' + \\lambda^2 \\cdot \\frac{1 + \\xi_{k,t}}{1 - |\\lambda^2 (P)|}\n$$\n\n$$\n\\beta_t \\leq 1 + 2\\eta_t \\lambda^2 + \\eta^2_{t-k_t}V_{k,t} \\beta_{t-1} + \\eta^2_{t-k_t}r_{k,t}(V' + \\xi_{k,t}) \\exp \\left(2\\lambda_1 t-k_{t-1} \\eta_i + t-k_{t-1} \\eta^2_i\\right) \\quad (S.54)\n$$\n\nRecursing on the above inequality for $u < t \\leq n$ where $u = \\min \\{i : i \\in [n], i - k_i \\geq 0\\}$, we have,\n\n$$\n\\beta_n \\leq \\beta_u \\exp \\left(2 \\sum_{i=u+1}^{n} \\eta_i \\lambda^2 + \\sum_{i=u+1}^{n} V_{k,i} \\eta^2_{i-k_i}\\right) + \\sum_{i=u+1}^{n} r_{k,i}(V' + \\xi_{k,i}) \\eta^2_{i-k_i} \\exp \\left(2 \\sum_{j=i+1}^{n} \\eta_j \\lambda^2 + V_{k,j} \\eta^2_{j-k_j}\\right) \\exp \\left(\\sum_{j=1}^{i} 2\\eta_j \\lambda_1 + V_{k,j} \\eta^2_j\\right)\n$$", "images": [], "items": [{"type": "text", "value": "Putting everything together, we have,\n\n$$\nE \\leq Tr \\cdot BtBT \\cdot t V^{\\perp}V T^{\\perp} \\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + 2(1 + \\epsilon) \\eta_t \\eta_{t-k_t} \\cdot 2|V| \\cdot |\\lambda^2 (P)| \\cdot \\frac{2(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2}{k^2} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t}) \\cdot \\frac{1 - |\\lambda^2 (P)| + \\eta_{t-k_t}M}{t(M + \\lambda_1)^2} + \\eta^2_t V + 2\\epsilon \\eta_t M + 2\\eta_{t-k_t+1}M \\cdot (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon))k^2 \\cdot \\frac{t(M + \\lambda_1)^2}{(\\alpha_t - k_t + \\beta_{t-k_t})}\n$$\n\n$$\n\\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + 2(1 + \\epsilon) \\eta^2_{t-k_t} \\cdot 2|V| \\cdot |\\lambda^2 (P)| \\cdot \\frac{2(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2}{k^2} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t}) \\cdot \\frac{1 - |\\lambda^2 (P)| + \\eta_{t-k_t}M}{t(M + \\lambda_1)^2} + \\eta^2_{t-k_t} V + 2\\epsilon \\eta_t M + 2\\eta_{t-k_t+1}M \\cdot (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon))k^2 \\cdot \\frac{t(M + \\lambda_1)^2}{(\\alpha_t - k_t + \\beta_{t-k_t})}\n$$\n\n$$\n\\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + \\eta^2 \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\xi_{k,t}} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t})\n$$\n\nwhere $\\xi_{k,t}$ is as defined in S.49. Therefore using Lemma S.4,\n\n$$\nE \\leq Tr \\cdot BtBT \\cdot t V^{\\perp}V T^{\\perp} \\leq 1 + 2\\eta_t \\lambda^2 + \\eta^2_{t-k_t} \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} + \\xi_{k,t} \\cdot \\beta_{t-1} \\cdot \\frac{1 - |\\lambda^2 (P)|}{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|} + \\eta^2 \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\xi_{k,t}} \\cdot \\alpha_{t-1} \\quad (S.52)\n$$\n\nLet $\\chi_{\\epsilon} := \\frac{1 + 4\\epsilon(1 + \\epsilon)}{1 + \\epsilon + \\epsilon^2} \\leq 1.05$. From Theorem 2 denoting $\\chi_{\\epsilon} = \\chi_{\\epsilon}$,\n\n$$\nr_{k,t} := 1 + 4(1 + \\epsilon) \\eta_{t-1}k_{t-1}(M + \\lambda_1) + 4(1 + c)^2 \\eta^2_{t-1}k^2_{t-1}(M + \\lambda_1)^2 \\leq 1 + 4\\epsilon(1 + \\epsilon) \\quad (S.53)\n$$\n\nwe have,\n\n$$\n\\alpha_{t-1} \\leq r_{k,t} \\exp \\left(2\\lambda_1 t-k_{t-1} \\eta_t + t-k_{t-1} \\eta^2\\right) \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} \\cdot \\frac{1 + \\xi_{k,i}}{1 - |\\lambda^2 (P)|}\n$$\n\nNow, we note the definition of $V_{k,t}$ and $V'$ as mentioned in S.49 -\n\n$$\nV_{k,t} := \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} \\cdot \\frac{1 + \\xi_{k,t}}{1 - |\\lambda^2 (P)|}\n$$\n\nTherefore using S.52,\n\n$$\nV_{k,t} = V' + \\lambda^2 \\cdot \\frac{1 + \\xi_{k,t}}{1 - |\\lambda^2 (P)|}\n$$\n\n$$\n\\beta_t \\leq 1 + 2\\eta_t \\lambda^2 + \\eta^2_{t-k_t}V_{k,t} \\beta_{t-1} + \\eta^2_{t-k_t}r_{k,t}(V' + \\xi_{k,t}) \\exp \\left(2\\lambda_1 t-k_{t-1} \\eta_i + t-k_{t-1} \\eta^2_i\\right) \\quad (S.54)\n$$\n\nRecursing on the above inequality for $u < t \\leq n$ where $u = \\min \\{i : i \\in [n], i - k_i \\geq 0\\}$, we have,\n\n$$\n\\beta_n \\leq \\beta_u \\exp \\left(2 \\sum_{i=u+1}^{n} \\eta_i \\lambda^2 + \\sum_{i=u+1}^{n} V_{k,i} \\eta^2_{i-k_i}\\right) + \\sum_{i=u+1}^{n} r_{k,i}(V' + \\xi_{k,i}) \\eta^2_{i-k_i} \\exp \\left(2 \\sum_{j=i+1}^{n} \\eta_j \\lambda^2 + V_{k,j} \\eta^2_{j-k_j}\\right) \\exp \\left(\\sum_{j=1}^{i} 2\\eta_j \\lambda_1 + V_{k,j} \\eta^2_j\\right)\n$$", "md": "Putting everything together, we have,\n\n$$\nE \\leq Tr \\cdot BtBT \\cdot t V^{\\perp}V T^{\\perp} \\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + 2(1 + \\epsilon) \\eta_t \\eta_{t-k_t} \\cdot 2|V| \\cdot |\\lambda^2 (P)| \\cdot \\frac{2(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2}{k^2} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t}) \\cdot \\frac{1 - |\\lambda^2 (P)| + \\eta_{t-k_t}M}{t(M + \\lambda_1)^2} + \\eta^2_t V + 2\\epsilon \\eta_t M + 2\\eta_{t-k_t+1}M \\cdot (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon))k^2 \\cdot \\frac{t(M + \\lambda_1)^2}{(\\alpha_t - k_t + \\beta_{t-k_t})}\n$$\n\n$$\n\\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + 2(1 + \\epsilon) \\eta^2_{t-k_t} \\cdot 2|V| \\cdot |\\lambda^2 (P)| \\cdot \\frac{2(1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2}{k^2} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t}) \\cdot \\frac{1 - |\\lambda^2 (P)| + \\eta_{t-k_t}M}{t(M + \\lambda_1)^2} + \\eta^2_{t-k_t} V + 2\\epsilon \\eta_t M + 2\\eta_{t-k_t+1}M \\cdot (1 + \\epsilon)(1 + \\epsilon(1 + \\epsilon))k^2 \\cdot \\frac{t(M + \\lambda_1)^2}{(\\alpha_t - k_t + \\beta_{t-k_t})}\n$$\n\n$$\n\\leq (1 + \\eta_t \\lambda^2)^2 \\beta_{t-1} + \\eta^2 \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\xi_{k,t}} \\cdot (\\alpha_t - k_t + \\beta_{t-k_t})\n$$\n\nwhere $\\xi_{k,t}$ is as defined in S.49. Therefore using Lemma S.4,\n\n$$\nE \\leq Tr \\cdot BtBT \\cdot t V^{\\perp}V T^{\\perp} \\leq 1 + 2\\eta_t \\lambda^2 + \\eta^2_{t-k_t} \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} + \\xi_{k,t} \\cdot \\beta_{t-1} \\cdot \\frac{1 - |\\lambda^2 (P)|}{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|} + \\eta^2 \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\xi_{k,t}} \\cdot \\alpha_{t-1} \\quad (S.52)\n$$\n\nLet $\\chi_{\\epsilon} := \\frac{1 + 4\\epsilon(1 + \\epsilon)}{1 + \\epsilon + \\epsilon^2} \\leq 1.05$. From Theorem 2 denoting $\\chi_{\\epsilon} = \\chi_{\\epsilon}$,\n\n$$\nr_{k,t} := 1 + 4(1 + \\epsilon) \\eta_{t-1}k_{t-1}(M + \\lambda_1) + 4(1 + c)^2 \\eta^2_{t-1}k^2_{t-1}(M + \\lambda_1)^2 \\leq 1 + 4\\epsilon(1 + \\epsilon) \\quad (S.53)\n$$\n\nwe have,\n\n$$\n\\alpha_{t-1} \\leq r_{k,t} \\exp \\left(2\\lambda_1 t-k_{t-1} \\eta_t + t-k_{t-1} \\eta^2\\right) \\cdot \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} \\cdot \\frac{1 + \\xi_{k,i}}{1 - |\\lambda^2 (P)|}\n$$\n\nNow, we note the definition of $V_{k,t}$ and $V'$ as mentioned in S.49 -\n\n$$\nV_{k,t} := \\frac{1 + (3 + 4\\epsilon)|\\lambda^2 (P)|}{V + \\lambda^2} \\cdot \\frac{1 + \\xi_{k,t}}{1 - |\\lambda^2 (P)|}\n$$\n\nTherefore using S.52,\n\n$$\nV_{k,t} = V' + \\lambda^2 \\cdot \\frac{1 + \\xi_{k,t}}{1 - |\\lambda^2 (P)|}\n$$\n\n$$\n\\beta_t \\leq 1 + 2\\eta_t \\lambda^2 + \\eta^2_{t-k_t}V_{k,t} \\beta_{t-1} + \\eta^2_{t-k_t}r_{k,t}(V' + \\xi_{k,t}) \\exp \\left(2\\lambda_1 t-k_{t-1} \\eta_i + t-k_{t-1} \\eta^2_i\\right) \\quad (S.54)\n$$\n\nRecursing on the above inequality for $u < t \\leq n$ where $u = \\min \\{i : i \\in [n], i - k_i \\geq 0\\}$, we have,\n\n$$\n\\beta_n \\leq \\beta_u \\exp \\left(2 \\sum_{i=u+1}^{n} \\eta_i \\lambda^2 + \\sum_{i=u+1}^{n} V_{k,i} \\eta^2_{i-k_i}\\right) + \\sum_{i=u+1}^{n} r_{k,i}(V' + \\xi_{k,i}) \\eta^2_{i-k_i} \\exp \\left(2 \\sum_{j=i+1}^{n} \\eta_j \\lambda^2 + V_{k,j} \\eta^2_{j-k_j}\\right) \\exp \\left(\\sum_{j=1}^{i} 2\\eta_j \\lambda_1 + V_{k,j} \\eta^2_j\\right)\n$$"}]}, {"page": 36, "text": "Now, since ki, kj \u2265          ku = u, therefore, we have\n\u03b2n \u2264    exp          n    2\u03b7i\u03bb2 + Vk,i\u03b72      i\u2212ki      \u00d7\n                 i=u+1\n           \uf8eb              n                                         \uf8ebi\u2212u                                      i                               \uf8f6\uf8f6\n           \uf8ed\u03b2u +      i=u+1     rk,i (V\u2032 + \u03bek,i) \u03b72    i\u2212ki exp     \uf8ed  j=1  (2\u03b7j\u03bb1 + Vk,j\u03b72       j ) \u2212   j=u+1      2\u03b7j\u03bb2 + Vk,j\u03b72      j\u2212u  \uf8f8\uf8f8\nRecall that C\u2032    k,i := exp       2\u03bb1    u  j=1 (\u03b7j \u2212      \u03b7i\u2212u+j) +  i\u2212u     j=1 \u03b72  j   Vk,j \u2212    Vk,j+u         as defined in S.49.\nTherefore,\n         \u03b2n \u2264     exp         n     2\u03b7i\u03bb2 + Vk,i\u03b72      i\u2212ki     \u00d7\n                           i=u+1\n                              \uf8eb             n                                               \uf8eb      i                         \uf8f6\uf8f6\n                              \uf8ed\u03b2u +      i=u+1    rk,i (V\u2032 + \u03bek,i) C\u2032     k,i\u03b72 i\u2212ki exp    \uf8ed  j=u+1     2\u03b7j (\u03bb1 \u2212      \u03bb2)  \uf8f8\uf8f8\n     Let Bu = I + R\u2032 with \u2225R\u2032\u2225\u2264                  r\u2032 a.s. Using Lemma 2 we have\n                             r\u2032 \u2264    (1 + \u03f5) ku\u03b71 (M + \u03bb1)\n                                 \u2264   (1 + \u03f5) ku\u03b70 (M + \u03bb1)\n                                 \u2264   2 (1 + \u03f5) ku\u03b7u (M + \u03bb1) since \u03b70 = \u03b7u\u2212ku \u2264                          2\u03b7u\n                                 < 2\u03f5 (1 + \u03f5)\nTherefore,\n              \u03b2u = E       Tr    V T         u V\u22a5\n                                   \u22a5  BuBT\n                   = E     Tr    V T          + E     Tr    V T                           + E     Tr    V T\n                                   \u22a5  V\u22a5                      \u22a5  (R\u2032 + R\u2032T )V\u22a5                            \u22a5  R\u2032R\u2032T V\u22a5\n                   \u2264   d   1 + 2r\u2032 + r\u20322\n                   \u2264   d   1 + 4\u03f5 (1 + \u03f5) + 4\u03f52 (1 + \u03f5)2\n                   = d     1 + 4\u03f5 (1 + \u03f5)        1 + \u03f5 + \u03f52\n                   = \u03c7\u03f5d\nThe proof follows by noting that rk,t \u2264                  \u03c7\u03f5 as shown in S.53.\nTheorem 4. (General Version) Under Assumptions 1, 2 and 3, for all n > kn, any decaying step-size\n\u03b7i satisfying C.1 and C.2, we have:\n                         E   vT1 Bn,1BT    n,1v1     \u2265  (1 \u2212    t) exp    n\u2212kn      2\u03b7i\u03bb1 \u2212     n\u2212kn    4\u03b72 i \u03bb2\n                                                                                                               1\n                                                                             i=1                  i=1\nwhere t := 2r + s, s := 3 (1 + r)2 exp                   2\u03bb2 1  n         i    n\u2212kn      Wk,t\u03b72   t exp      n\u2212kn          i   , Wk,t :=\n                                                                   i=1 \u03b72         t=1                           i=t+1 \u03b72\nV\u2032 + \u03bek,t and Bj,i has been defined in 7.\nProof. We will start will expanding the quantity of interest using Eq S.50.\n            \u03b1n,t = E       vT1 Bn,tBT   n,tv1     \u2265   E   vT1 Bn,t+1 (I + \u03b7t\u03a3)2 BT          n,t+1v1 + 2\u03b7tPn,t                       (S.54)\nwhere Pn,t has been defined in Theorem 2. Let\u2019s define\n                                              1                   t\n                                   St :=    i=t  (I + \u03b7i\u03a3)      i=1  (I + \u03b7i\u03a3) , S0 = I and\n                                   \u03b4n,t := E       vT\n                                                    1 Bn,t+1StBT       n,t+1v1\n                                                                    36", "md": "Now, since \\(k_i, k_j \\geq k_u = u\\), therefore, we have\n\n$$\n\\beta_n \\leq \\exp\\left( n \\frac{2\\eta_i\\lambda^2 + V_{k,i}\\eta^2}{i-k_i} \\right) \\times \\left( \\frac{\\beta_u + \\sum_{i=u+1}^{n} r_{k,i} (V' + \\xi_{k,i}) \\eta^2}{i-k_i \\exp\\left( \\sum_{j=1}^{i-u} (2\\eta_j\\lambda_1 + V_{k,j}\\eta^2) - \\sum_{j=u+1}^{i} 2\\eta_j\\lambda^2 + V_{k,j}\\eta^2 \\right)} \\right)\n$$\nRecall that \\(C'_{k,i} := \\exp\\left( 2\\lambda_1 \\sum_{u}^{j=1} (\\eta_j - \\eta_{i-u+j}) + \\sum_{j=1}^{i-u} \\eta^2_j V_{k,j} - V_{k,j+u} \\right)\\) as defined in S.49.\n\nTherefore,\n\n$$\n\\beta_n \\leq \\exp\\left( n \\frac{2\\eta_i\\lambda^2 + V_{k,i}\\eta^2}{i-k_i} \\right) \\times \\left( \\frac{\\beta_u + \\sum_{i=u+1}^{n} r_{k,i} (V' + \\xi_{k,i}) C'_{k,i}\\eta^2}{i-k_i \\exp\\left( \\sum_{j=u+1}^{i} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right)} \\right)\n$$\nLet \\(B_u = I + R'\\) with \\(\\|R'\\| \\leq r'\\) a.s. Using Lemma 2 we have\n\n$$\nr' \\leq (1 + \\epsilon) k_u\\eta_1 (M + \\lambda_1) \\leq (1 + \\epsilon) k_u\\eta_0 (M + \\lambda_1) \\leq 2 (1 + \\epsilon) k_u\\eta_{u} (M + \\lambda_1) \\text{ since } \\eta_0 = \\eta_{u-k_u} \\leq 2\\eta_u < 2\\epsilon (1 + \\epsilon)\n$$\nTherefore,\n\n$$\n\\beta_u = E\\left[ \\text{Tr} \\left( V^T_u V_{\\perp} \\right) B_u B^T_u \\right] = E\\left[ \\text{Tr} \\left( V^T_{\\perp} \\right) + E\\left[ \\text{Tr} \\left( V^T_{\\perp} \\right) (R' + R'^T) V_{\\perp} \\right] \\right] \\leq d(1 + 2r' + r'^2) \\leq d(1 + 4\\epsilon (1 + \\epsilon) + 4\\epsilon^2 (1 + \\epsilon)^2) = d(1 + 4\\epsilon (1 + \\epsilon))^{1 + \\epsilon + \\epsilon^2} = \\chi\\epsilon d\n$$\nThe proof follows by noting that \\(r_{k,t} \\leq \\chi\\epsilon\\) as shown in S.53.\n\nTheorem 4. (General Version) Under Assumptions 1, 2 and 3, for all \\(n > k_n\\), any decaying step-size \\(\\eta_i\\) satisfying C.1 and C.2, we have:\n\n$$\nE\\left[ v^T_1 B_{n,1} B^T_{n,1} v_1 \\right] \\geq (1 - t) \\exp\\left( n-k_n \\frac{2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda_2}{i} \\right) \\quad \\text{where } t := 2r + s, s := 3(1 + r)^2 \\exp\\left( 2\\lambda^2_1 n \\sum_{i=1}^{n-k_n} W_{k,t}\\eta^2_t \\exp\\left( n-k_n \\sum_{i=1}^{t} \\eta^2_i \\right), W_{k,t} := V' + \\xi_{k,t} \\text{ and } B_{j,i} \\text{ has been defined in 7.}\n\nProof. We will start will expanding the quantity of interest using Eq S.50.\n\n$$\n\\alpha_{n,t} = E\\left[ v^T_1 B_{n,t} B^T_{n,t} v_1 \\right] \\geq E\\left[ v^T_1 B_{n,t+1} (I + \\eta_t\\Sigma)^2 B^T_{n,t+1} v_1 + 2\\eta_t P_{n,t} \\right] \\quad \\text{where } P_{n,t} \\text{ has been defined in Theorem 2.}\n$$\nLet\u2019s define\n\n$$\nS_t := \\prod_{i=t}^{1} (I + \\eta_i\\Sigma), S_0 = I \\quad \\text{and} \\quad \\delta_{n,t} := E\\left[ v^T_1 B_{n,t+1} S_t B^T_{n,t+1} v_1 \\right]\n$$", "images": [], "items": [{"type": "text", "value": "Now, since \\(k_i, k_j \\geq k_u = u\\), therefore, we have\n\n$$\n\\beta_n \\leq \\exp\\left( n \\frac{2\\eta_i\\lambda^2 + V_{k,i}\\eta^2}{i-k_i} \\right) \\times \\left( \\frac{\\beta_u + \\sum_{i=u+1}^{n} r_{k,i} (V' + \\xi_{k,i}) \\eta^2}{i-k_i \\exp\\left( \\sum_{j=1}^{i-u} (2\\eta_j\\lambda_1 + V_{k,j}\\eta^2) - \\sum_{j=u+1}^{i} 2\\eta_j\\lambda^2 + V_{k,j}\\eta^2 \\right)} \\right)\n$$\nRecall that \\(C'_{k,i} := \\exp\\left( 2\\lambda_1 \\sum_{u}^{j=1} (\\eta_j - \\eta_{i-u+j}) + \\sum_{j=1}^{i-u} \\eta^2_j V_{k,j} - V_{k,j+u} \\right)\\) as defined in S.49.\n\nTherefore,\n\n$$\n\\beta_n \\leq \\exp\\left( n \\frac{2\\eta_i\\lambda^2 + V_{k,i}\\eta^2}{i-k_i} \\right) \\times \\left( \\frac{\\beta_u + \\sum_{i=u+1}^{n} r_{k,i} (V' + \\xi_{k,i}) C'_{k,i}\\eta^2}{i-k_i \\exp\\left( \\sum_{j=u+1}^{i} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right)} \\right)\n$$\nLet \\(B_u = I + R'\\) with \\(\\|R'\\| \\leq r'\\) a.s. Using Lemma 2 we have\n\n$$\nr' \\leq (1 + \\epsilon) k_u\\eta_1 (M + \\lambda_1) \\leq (1 + \\epsilon) k_u\\eta_0 (M + \\lambda_1) \\leq 2 (1 + \\epsilon) k_u\\eta_{u} (M + \\lambda_1) \\text{ since } \\eta_0 = \\eta_{u-k_u} \\leq 2\\eta_u < 2\\epsilon (1 + \\epsilon)\n$$\nTherefore,\n\n$$\n\\beta_u = E\\left[ \\text{Tr} \\left( V^T_u V_{\\perp} \\right) B_u B^T_u \\right] = E\\left[ \\text{Tr} \\left( V^T_{\\perp} \\right) + E\\left[ \\text{Tr} \\left( V^T_{\\perp} \\right) (R' + R'^T) V_{\\perp} \\right] \\right] \\leq d(1 + 2r' + r'^2) \\leq d(1 + 4\\epsilon (1 + \\epsilon) + 4\\epsilon^2 (1 + \\epsilon)^2) = d(1 + 4\\epsilon (1 + \\epsilon))^{1 + \\epsilon + \\epsilon^2} = \\chi\\epsilon d\n$$\nThe proof follows by noting that \\(r_{k,t} \\leq \\chi\\epsilon\\) as shown in S.53.\n\nTheorem 4. (General Version) Under Assumptions 1, 2 and 3, for all \\(n > k_n\\), any decaying step-size \\(\\eta_i\\) satisfying C.1 and C.2, we have:\n\n$$\nE\\left[ v^T_1 B_{n,1} B^T_{n,1} v_1 \\right] \\geq (1 - t) \\exp\\left( n-k_n \\frac{2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda_2}{i} \\right) \\quad \\text{where } t := 2r + s, s := 3(1 + r)^2 \\exp\\left( 2\\lambda^2_1 n \\sum_{i=1}^{n-k_n} W_{k,t}\\eta^2_t \\exp\\left( n-k_n \\sum_{i=1}^{t} \\eta^2_i \\right), W_{k,t} := V' + \\xi_{k,t} \\text{ and } B_{j,i} \\text{ has been defined in 7.}\n\nProof. We will start will expanding the quantity of interest using Eq S.50.\n\n$$\n\\alpha_{n,t} = E\\left[ v^T_1 B_{n,t} B^T_{n,t} v_1 \\right] \\geq E\\left[ v^T_1 B_{n,t+1} (I + \\eta_t\\Sigma)^2 B^T_{n,t+1} v_1 + 2\\eta_t P_{n,t} \\right] \\quad \\text{where } P_{n,t} \\text{ has been defined in Theorem 2.}\n$$\nLet\u2019s define\n\n$$\nS_t := \\prod_{i=t}^{1} (I + \\eta_i\\Sigma), S_0 = I \\quad \\text{and} \\quad \\delta_{n,t} := E\\left[ v^T_1 B_{n,t+1} S_t B^T_{n,t+1} v_1 \\right]\n$$", "md": "Now, since \\(k_i, k_j \\geq k_u = u\\), therefore, we have\n\n$$\n\\beta_n \\leq \\exp\\left( n \\frac{2\\eta_i\\lambda^2 + V_{k,i}\\eta^2}{i-k_i} \\right) \\times \\left( \\frac{\\beta_u + \\sum_{i=u+1}^{n} r_{k,i} (V' + \\xi_{k,i}) \\eta^2}{i-k_i \\exp\\left( \\sum_{j=1}^{i-u} (2\\eta_j\\lambda_1 + V_{k,j}\\eta^2) - \\sum_{j=u+1}^{i} 2\\eta_j\\lambda^2 + V_{k,j}\\eta^2 \\right)} \\right)\n$$\nRecall that \\(C'_{k,i} := \\exp\\left( 2\\lambda_1 \\sum_{u}^{j=1} (\\eta_j - \\eta_{i-u+j}) + \\sum_{j=1}^{i-u} \\eta^2_j V_{k,j} - V_{k,j+u} \\right)\\) as defined in S.49.\n\nTherefore,\n\n$$\n\\beta_n \\leq \\exp\\left( n \\frac{2\\eta_i\\lambda^2 + V_{k,i}\\eta^2}{i-k_i} \\right) \\times \\left( \\frac{\\beta_u + \\sum_{i=u+1}^{n} r_{k,i} (V' + \\xi_{k,i}) C'_{k,i}\\eta^2}{i-k_i \\exp\\left( \\sum_{j=u+1}^{i} 2\\eta_j (\\lambda_1 - \\lambda_2) \\right)} \\right)\n$$\nLet \\(B_u = I + R'\\) with \\(\\|R'\\| \\leq r'\\) a.s. Using Lemma 2 we have\n\n$$\nr' \\leq (1 + \\epsilon) k_u\\eta_1 (M + \\lambda_1) \\leq (1 + \\epsilon) k_u\\eta_0 (M + \\lambda_1) \\leq 2 (1 + \\epsilon) k_u\\eta_{u} (M + \\lambda_1) \\text{ since } \\eta_0 = \\eta_{u-k_u} \\leq 2\\eta_u < 2\\epsilon (1 + \\epsilon)\n$$\nTherefore,\n\n$$\n\\beta_u = E\\left[ \\text{Tr} \\left( V^T_u V_{\\perp} \\right) B_u B^T_u \\right] = E\\left[ \\text{Tr} \\left( V^T_{\\perp} \\right) + E\\left[ \\text{Tr} \\left( V^T_{\\perp} \\right) (R' + R'^T) V_{\\perp} \\right] \\right] \\leq d(1 + 2r' + r'^2) \\leq d(1 + 4\\epsilon (1 + \\epsilon) + 4\\epsilon^2 (1 + \\epsilon)^2) = d(1 + 4\\epsilon (1 + \\epsilon))^{1 + \\epsilon + \\epsilon^2} = \\chi\\epsilon d\n$$\nThe proof follows by noting that \\(r_{k,t} \\leq \\chi\\epsilon\\) as shown in S.53.\n\nTheorem 4. (General Version) Under Assumptions 1, 2 and 3, for all \\(n > k_n\\), any decaying step-size \\(\\eta_i\\) satisfying C.1 and C.2, we have:\n\n$$\nE\\left[ v^T_1 B_{n,1} B^T_{n,1} v_1 \\right] \\geq (1 - t) \\exp\\left( n-k_n \\frac{2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda_2}{i} \\right) \\quad \\text{where } t := 2r + s, s := 3(1 + r)^2 \\exp\\left( 2\\lambda^2_1 n \\sum_{i=1}^{n-k_n} W_{k,t}\\eta^2_t \\exp\\left( n-k_n \\sum_{i=1}^{t} \\eta^2_i \\right), W_{k,t} := V' + \\xi_{k,t} \\text{ and } B_{j,i} \\text{ has been defined in 7.}\n\nProof. We will start will expanding the quantity of interest using Eq S.50.\n\n$$\n\\alpha_{n,t} = E\\left[ v^T_1 B_{n,t} B^T_{n,t} v_1 \\right] \\geq E\\left[ v^T_1 B_{n,t+1} (I + \\eta_t\\Sigma)^2 B^T_{n,t+1} v_1 + 2\\eta_t P_{n,t} \\right] \\quad \\text{where } P_{n,t} \\text{ has been defined in Theorem 2.}\n$$\nLet\u2019s define\n\n$$\nS_t := \\prod_{i=t}^{1} (I + \\eta_i\\Sigma), S_0 = I \\quad \\text{and} \\quad \\delta_{n,t} := E\\left[ v^T_1 B_{n,t+1} S_t B^T_{n,t+1} v_1 \\right]\n$$"}]}, {"page": 37, "text": "Note that \u03b4n,0 = \u03b1n,1. First we bound \u03b4n,n\u2212kn. Let Bn,n\u2212kn = I + R\u2032. By Lemma 2 along with\nthe slow-decay assumption on the step-sizes, we know that \u2225R\u2032\u22252\u2264                                 r := 2 (1 + \u03f5) \u03b7nkn (M + \u03bb1)\na.s. Then,\n             \u03b4n,n\u2212kn \u2212       n\u2212kn    (1 + \u03b7i\u03bb1)2 \u2265         \u22122    E[vT 1 R\u2032Sn\u2212knv1]          \u2265   \u22122r    n\u2212kn   (1 + \u03b7i\u03bb1)2\n                              i=1                                                                       i=1\nTherefore,\n                                           \u03b4n,n\u2212kn \u2265       n\u2212kn    (1 + \u03b7i\u03bb1)2 (1 \u2212         2r)\n                                                             i=1\nNow using S.54, we have                            \uf8ee    = (1 \u2212     2r) \u2225Sn\u2212kn\u22252                                       \uf8f9             (S.55)\n                                                   \uf8ef                                                                  \uf8fa\n                    \u03b4n,t\u22121 \u2265     \u03b4n,t + 2\u03b7tE       \uf8f0vT 1 Bn,t+1 (I + \u03b7t\u03a3) St\u22121 (At \u2212Ut              \u03a3) BT  n,t+1v1    \uf8fb\nFirst, observe that St\u22121 = U\u039bU T , where U denotes a matrix of eigenvectors of \u03a3, and \u039b is a PSD\ndiagonal matrix. Since I + \u03b7t\u03a3 = U\u039b\u2032U T for some other PSD diagonal matrix \u039b\u2032, the product will\nalso be PSD.\nBy using Lemma S.8 with U = v1, G = (I + \u03b7t\u03a3) St\u22121, \u03b3 = 1 and noting that E\u03c0 [At \u2212                                                \u03a3] = 0,\nwe have\n                                                                     2V |\u03bb2 (P)|\n              |E [Ut]| \u2264     (1 + \u03b7t\u03bb1) \u03b7t+1 \u2225St\u22121\u22252                1 \u2212   |\u03bb  2 (P)|\n                          +\u03b7t+1M          2 (1 + 8\u03f5) +         2 + (1 + \u03f5)2        k2t+1 (M + \u03bb1)2              \u03b1n,t+kt+1\n                          \u2264  (1 + \u03f5) \u03b7t+1 \u2225St\u22121\u22252 Wk,t\u03b1n,t+1\nwhere Wk,t = V\u2032 + \u03bek,t. Therefore,\nLet                     \u03b4n,t\u22121 \u2265      \u03b4n,t \u2212   2 (1 + \u03f5) Wk,t\u03b72      t \u03b1n,t+1 \u2225St\u22121\u22252 for t \u2264             n \u2212   kn\n                                             V\u2032 :=      1 + (3 + 4\u03f5) |\u03bb2 (P) |              V\n                                                                1 \u2212   |\u03bb2 (P)|\nas defined in S.49. Unwinding the recursion for t \u2264                       n \u2212   kn, we have,\n \u03b4n,0 \u2265    \u03b4n,n\u2212kn \u2212       2 (1 + \u03f5)    n\u2212kn    Wk,t\u03b72   t \u03b1n,t+1 \u2225St\u22121\u22252\n                                          t=1\n        \u2265  (1 \u2212    2r) \u2225Sn\u2212kn\u22252\n       \u2212   2 (1 + \u03f5) (1 + r)2        n\u2212kn    Wk,t\u03b72              2\u03bb1   n\u2212kn    \u03b7i +    n\u2212kn    \u03b72   V\u2032 + \u03bb2                    \u2225St\u22121\u22252\n                                                     t exp                                      i             1 + Ck,i\n                                      t=1                              i=t+1          i=t+1\nwhere second step followed from Theorem 2 and S.55.\nUsing the inequalities \u2200x \u2208                R, 1 + x \u2264           ex and \u2200x \u2208          R, x \u2265        0, 1 + x \u2265        ex\u2212x2,         \u2200t we\n                                                                    37", "md": "Note that $$\\delta_{n,0} = \\alpha_{n,1}$$. First we bound $$\\delta_{n,n-kn}$$. Let $$B_{n,n-kn} = I + R'$$. By Lemma 2 along with the slow-decay assumption on the step-sizes, we know that $$\\|R'\\|_2 \\leq r := 2 (1 + \\epsilon) \\eta_n kn (M + \\lambda_1)$$ a.s. Then,\n$$\n\\begin{aligned}\n\\delta_{n,n-kn} - \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2 &\\geq -2 E[v_1^T R' S_{n-kn} v_1] \\\\\n&\\geq -2r \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2\n\\end{aligned}\n$$\nTherefore,\n$$\n\\delta_{n,n-kn} \\geq \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2 (1 - 2r)\n$$\nNow using S.54, we have\n$$\n\\begin{aligned}\n&= (1 - 2r) \\|S_{n-kn}\\|_2 \\quad \\text{(S.55)}\n\\end{aligned}\n$$\n$$\n\\delta_{n,t-1} \\geq \\delta_{n,t} + 2\\eta_t E[v_1 B_{n,t+1} (I + \\eta_t \\Sigma) S_{t-1} (A_t - U_t \\Sigma) B_{n,t+1} v_1]\n$$\nFirst, observe that $$S_{t-1} = U\\Lambda U^T$$, where U denotes a matrix of eigenvectors of $$\\Sigma$$, and $$\\Lambda$$ is a PSD diagonal matrix. Since $$I + \\eta_t \\Sigma = U\\Lambda'U^T$$ for some other PSD diagonal matrix $$\\Lambda'$$, the product will also be PSD.\nBy using Lemma S.8 with $$U = v_1$$, $$G = (I + \\eta_t \\Sigma) S_{t-1}$$, $$\\gamma = 1$$ and noting that $$E_{\\pi}[A_t - \\Sigma] = 0$$, we have\n$$\n\\begin{aligned}\n|E [U_t]| &\\leq (1 + \\eta_t \\lambda_1) \\eta_{t+1} \\|S_{t-1}\\|_2 \\\\\n&\\quad \\times \\left(1 - |\\lambda_2 (P)|\\right) + \\eta_{t+1} M \\\\\n&\\quad \\times 2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k_{t+1} (M + \\lambda_1)^2 \\alpha_{n,t+kt+1} \\\\\n&\\leq (1 + \\epsilon) \\eta_{t+1} \\|S_{t-1}\\|_2 W_{k,t} \\alpha_{n,t+1}\n\\end{aligned}\n$$\nwhere $$W_{k,t} = V' + \\xi_{k,t}$$. Therefore,\nLet $$\\delta_{n,t-1} \\geq \\delta_{n,t} - 2 (1 + \\epsilon) W_{k,t} \\eta_t^2 \\alpha_{n,t+1} \\|S_{t-1}\\|_2$$ for $$t \\leq n - kn$$\n$$\nV' := \\frac{1 + (3 + 4\\epsilon) |\\lambda_2 (P)|}{1 - |\\lambda_2 (P)|}\n$$\nas defined in S.49. Unwinding the recursion for $$t \\leq n - kn$$, we have,\n$$\n\\begin{aligned}\n\\delta_{n,0} &\\geq \\delta_{n,n-kn} - 2 (1 + \\epsilon) \\sum_{t=1}^{n-kn} W_{k,t} \\eta_t^2 \\alpha_{n,t+1} \\|S_{t-1}\\|_2 \\\\\n&\\geq (1 - 2r) \\|S_{n-kn}\\|_2 \\\\\n&\\quad - 2 (1 + \\epsilon) (1 + r)^2 \\sum_{t=1}^{n-kn} W_{k,t} \\eta_t^2 \\\\\n&\\quad \\times 2\\lambda_1 \\sum_{i=1}^{n-kn} \\eta_i + \\sum_{i=1}^{n-kn} \\eta^2 V' + \\lambda_2 \\|S_{t-1}\\|_2 \\\\\n&\\quad \\times \\exp \\left(1 + Ck,i\\right)\n\\end{aligned}\n$$\nwhere the second step followed from Theorem 2 and S.55.\nUsing the inequalities $$\\forall x \\in \\mathbb{R}, 1 + x \\leq e^x$$ and $$\\forall x \\in \\mathbb{R}, x \\geq 0, 1 + x \\geq e^x - x^2$$, $$\\forall t$$ we have\n$$\n37\n$$", "images": [], "items": [{"type": "text", "value": "Note that $$\\delta_{n,0} = \\alpha_{n,1}$$. First we bound $$\\delta_{n,n-kn}$$. Let $$B_{n,n-kn} = I + R'$$. By Lemma 2 along with the slow-decay assumption on the step-sizes, we know that $$\\|R'\\|_2 \\leq r := 2 (1 + \\epsilon) \\eta_n kn (M + \\lambda_1)$$ a.s. Then,\n$$\n\\begin{aligned}\n\\delta_{n,n-kn} - \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2 &\\geq -2 E[v_1^T R' S_{n-kn} v_1] \\\\\n&\\geq -2r \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2\n\\end{aligned}\n$$\nTherefore,\n$$\n\\delta_{n,n-kn} \\geq \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2 (1 - 2r)\n$$\nNow using S.54, we have\n$$\n\\begin{aligned}\n&= (1 - 2r) \\|S_{n-kn}\\|_2 \\quad \\text{(S.55)}\n\\end{aligned}\n$$\n$$\n\\delta_{n,t-1} \\geq \\delta_{n,t} + 2\\eta_t E[v_1 B_{n,t+1} (I + \\eta_t \\Sigma) S_{t-1} (A_t - U_t \\Sigma) B_{n,t+1} v_1]\n$$\nFirst, observe that $$S_{t-1} = U\\Lambda U^T$$, where U denotes a matrix of eigenvectors of $$\\Sigma$$, and $$\\Lambda$$ is a PSD diagonal matrix. Since $$I + \\eta_t \\Sigma = U\\Lambda'U^T$$ for some other PSD diagonal matrix $$\\Lambda'$$, the product will also be PSD.\nBy using Lemma S.8 with $$U = v_1$$, $$G = (I + \\eta_t \\Sigma) S_{t-1}$$, $$\\gamma = 1$$ and noting that $$E_{\\pi}[A_t - \\Sigma] = 0$$, we have\n$$\n\\begin{aligned}", "md": "Note that $$\\delta_{n,0} = \\alpha_{n,1}$$. First we bound $$\\delta_{n,n-kn}$$. Let $$B_{n,n-kn} = I + R'$$. By Lemma 2 along with the slow-decay assumption on the step-sizes, we know that $$\\|R'\\|_2 \\leq r := 2 (1 + \\epsilon) \\eta_n kn (M + \\lambda_1)$$ a.s. Then,\n$$\n\\begin{aligned}\n\\delta_{n,n-kn} - \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2 &\\geq -2 E[v_1^T R' S_{n-kn} v_1] \\\\\n&\\geq -2r \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2\n\\end{aligned}\n$$\nTherefore,\n$$\n\\delta_{n,n-kn} \\geq \\sum_{i=1}^{n-kn} (1 + \\eta_i \\lambda_1)^2 (1 - 2r)\n$$\nNow using S.54, we have\n$$\n\\begin{aligned}\n&= (1 - 2r) \\|S_{n-kn}\\|_2 \\quad \\text{(S.55)}\n\\end{aligned}\n$$\n$$\n\\delta_{n,t-1} \\geq \\delta_{n,t} + 2\\eta_t E[v_1 B_{n,t+1} (I + \\eta_t \\Sigma) S_{t-1} (A_t - U_t \\Sigma) B_{n,t+1} v_1]\n$$\nFirst, observe that $$S_{t-1} = U\\Lambda U^T$$, where U denotes a matrix of eigenvectors of $$\\Sigma$$, and $$\\Lambda$$ is a PSD diagonal matrix. Since $$I + \\eta_t \\Sigma = U\\Lambda'U^T$$ for some other PSD diagonal matrix $$\\Lambda'$$, the product will also be PSD.\nBy using Lemma S.8 with $$U = v_1$$, $$G = (I + \\eta_t \\Sigma) S_{t-1}$$, $$\\gamma = 1$$ and noting that $$E_{\\pi}[A_t - \\Sigma] = 0$$, we have\n$$\n\\begin{aligned}"}, {"type": "table", "rows": [["E [U_t]", "&\\leq (1 + \\eta_t \\lambda_1) \\eta_{t+1} \\", "S_{t-1}\\"]], "md": "|E [U_t]| &\\leq (1 + \\eta_t \\lambda_1) \\eta_{t+1} \\|S_{t-1}\\|_2 \\\\", "isPerfectTable": true, "csv": "\"E [U_t]\",\"&\\leq (1 + \\eta_t \\lambda_1) \\eta_{t+1} \\\",\"S_{t-1}\\\""}, {"type": "text", "value": "&\\quad \\times \\left(1 - |\\lambda_2 (P)|\\right) + \\eta_{t+1} M \\\\\n&\\quad \\times 2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k_{t+1} (M + \\lambda_1)^2 \\alpha_{n,t+kt+1} \\\\\n&\\leq (1 + \\epsilon) \\eta_{t+1} \\|S_{t-1}\\|_2 W_{k,t} \\alpha_{n,t+1}\n\\end{aligned}\n$$\nwhere $$W_{k,t} = V' + \\xi_{k,t}$$. Therefore,\nLet $$\\delta_{n,t-1} \\geq \\delta_{n,t} - 2 (1 + \\epsilon) W_{k,t} \\eta_t^2 \\alpha_{n,t+1} \\|S_{t-1}\\|_2$$ for $$t \\leq n - kn$$\n$$\nV' := \\frac{1 + (3 + 4\\epsilon) |\\lambda_2 (P)|}{1 - |\\lambda_2 (P)|}\n$$\nas defined in S.49. Unwinding the recursion for $$t \\leq n - kn$$, we have,\n$$\n\\begin{aligned}\n\\delta_{n,0} &\\geq \\delta_{n,n-kn} - 2 (1 + \\epsilon) \\sum_{t=1}^{n-kn} W_{k,t} \\eta_t^2 \\alpha_{n,t+1} \\|S_{t-1}\\|_2 \\\\\n&\\geq (1 - 2r) \\|S_{n-kn}\\|_2 \\\\\n&\\quad - 2 (1 + \\epsilon) (1 + r)^2 \\sum_{t=1}^{n-kn} W_{k,t} \\eta_t^2 \\\\\n&\\quad \\times 2\\lambda_1 \\sum_{i=1}^{n-kn} \\eta_i + \\sum_{i=1}^{n-kn} \\eta^2 V' + \\lambda_2 \\|S_{t-1}\\|_2 \\\\\n&\\quad \\times \\exp \\left(1 + Ck,i\\right)\n\\end{aligned}\n$$\nwhere the second step followed from Theorem 2 and S.55.\nUsing the inequalities $$\\forall x \\in \\mathbb{R}, 1 + x \\leq e^x$$ and $$\\forall x \\in \\mathbb{R}, x \\geq 0, 1 + x \\geq e^x - x^2$$, $$\\forall t$$ we have\n$$\n37\n$$", "md": "&\\quad \\times \\left(1 - |\\lambda_2 (P)|\\right) + \\eta_{t+1} M \\\\\n&\\quad \\times 2 (1 + 8\\epsilon) + 2 + (1 + \\epsilon)^2 k_{t+1} (M + \\lambda_1)^2 \\alpha_{n,t+kt+1} \\\\\n&\\leq (1 + \\epsilon) \\eta_{t+1} \\|S_{t-1}\\|_2 W_{k,t} \\alpha_{n,t+1}\n\\end{aligned}\n$$\nwhere $$W_{k,t} = V' + \\xi_{k,t}$$. Therefore,\nLet $$\\delta_{n,t-1} \\geq \\delta_{n,t} - 2 (1 + \\epsilon) W_{k,t} \\eta_t^2 \\alpha_{n,t+1} \\|S_{t-1}\\|_2$$ for $$t \\leq n - kn$$\n$$\nV' := \\frac{1 + (3 + 4\\epsilon) |\\lambda_2 (P)|}{1 - |\\lambda_2 (P)|}\n$$\nas defined in S.49. Unwinding the recursion for $$t \\leq n - kn$$, we have,\n$$\n\\begin{aligned}\n\\delta_{n,0} &\\geq \\delta_{n,n-kn} - 2 (1 + \\epsilon) \\sum_{t=1}^{n-kn} W_{k,t} \\eta_t^2 \\alpha_{n,t+1} \\|S_{t-1}\\|_2 \\\\\n&\\geq (1 - 2r) \\|S_{n-kn}\\|_2 \\\\\n&\\quad - 2 (1 + \\epsilon) (1 + r)^2 \\sum_{t=1}^{n-kn} W_{k,t} \\eta_t^2 \\\\\n&\\quad \\times 2\\lambda_1 \\sum_{i=1}^{n-kn} \\eta_i + \\sum_{i=1}^{n-kn} \\eta^2 V' + \\lambda_2 \\|S_{t-1}\\|_2 \\\\\n&\\quad \\times \\exp \\left(1 + Ck,i\\right)\n\\end{aligned}\n$$\nwhere the second step followed from Theorem 2 and S.55.\nUsing the inequalities $$\\forall x \\in \\mathbb{R}, 1 + x \\leq e^x$$ and $$\\forall x \\in \\mathbb{R}, x \\geq 0, 1 + x \\geq e^x - x^2$$, $$\\forall t$$ we have\n$$\n37\n$$"}]}, {"page": 38, "text": "have,\n                            \u2225St\u22252 =        t  (1 + \u03b7i\u03bb1)2 \u2264        exp      2\u03bb1     t   \u03b7i    , and\n                                         i=1                                      i=1\n                            \u2225St\u22252 =        t  (1 + \u03b7i\u03bb1)2 \u2265        exp      2\u03bb1     t   \u03b7i \u2212   4\u03bb2 1   t   \u03b72i\n                                         i=1                                      i=1                i=1\nTherefore denoting \u03b8\u03f5 := 2 (1 + \u03f5) exp                   2\u03bb2 1  n  i=1 \u03b72 i   , we have\n\u03b4n,0\n \u2265   exp     2\u03bb1   n\u2212kn    \u03b7i \u2212   4\u03bb2 1 n\u2212kn    \u03b72i      (1 \u2212    2r) \u2212    \u03b8\u03f5 (1 + r)2     n\u2212kn    Wk,t\u03b72   t exp    n\u2212kn      \u03b72i   V\u2032 + \u03bb2  1 + Ck,i\n                     i=1                  i=1                                              t=1                        i=t+1\n \u2265   exp     2\u03bb1   n\u2212kn    \u03b7i \u2212   4\u03bb2 1 n\u2212kn    \u03b72i      (1 \u2212    2r) \u2212    \u03b8\u03f5 (1 + r)2     n\u2212kn    Wk,t\u03b72   t exp    n\u2212kn      \u03b72i   V\u2032 + \u03bb2  1 + Ck,i\n                     i=1                  i=1                                              t=1                        i=t+1\n \u2265   exp     2\u03bb1   n\u2212kn    \u03b7i \u2212   4\u03bb2 1 n\u2212kn    \u03b72i      1 \u2212      2r + \u03b8\u03f5 (1 + r)2        n\u2212kn    Wk,t\u03b72   t exp    n\u2212kn      \u03b72i Vk,i\n                     i=1                  i=1                                              t=1                        i=t+1\nwhere Vk,i is defined in S.49. Hence proved.\nTheorem 5. (General Version) Under Assumptions 1, 2 and 3, for all n > kn, and decaying step-size\n\u03b7i satisfying C.1 and C.2, we have:\n                     E     vT                    2    \u2264   (1 + r)4 exp        n\u2212kn     4\u03b7i\u03bb1 +      n\u2212kn    \u03b72\n                             1 Bn,1BT    n,1v1                                                               i \u03b6k,i\n                                                                                 i=1                 i=1\nwhere Bj,i has been defined in 7.\nProof. Define Qn,t := vT             1 Bn,t+1(At \u2212          \u03a3)2BT   n,t+1v1, and Pn,t := vT             1 Bn,t+1(I + \u03b7t\u03a3)(At \u2212\n\u03a3)BT  n,t+1v1. Using S.50, we have, for n \u2265                   t \u2265   1,\n  0 \u2264    vT1 Bn,tBT   n,tv1 = vT    1 Bn,t+1(I + \u03b7t\u03a3)2BT           n,t+1v1 + \u03b72     t Qn,t + 2\u03b7tPn,t\n                              \u2264   vT1 Bj,t+1BT    j,t+1v1(1 + \u03b7t\u03bb1)2 + \u03b72           t M2     vT1 Bn,t+1BT     n,t+1v1      + 2\u03b7tPn,t\n                              \u2264   vT1 Bj,t+1BT    j,t+1v1     (1 + \u03b7t\u03bb1)2 + \u03b72        t M2      +2\u03b7tPn,t\n                                                                             ct\nThus, we have -\n  \u03ban,t := E       (vT1 Bn,tBT    n,tv1)2     \u2264   E     ctvT 1 Bn,t+1BT     n,t+1v1 + 2\u03b7tPn,t          2\n                                             \u2264   c2                        P 2      + 4ct\u03b7tE         vT                            Pn,t\n                                                  t \u03ban,t+1 + 4\u03b72     t E      n,t                      1 Bn,t+1BT     n,t+1v1       (S.56)\nNote that,\n                            E    P 2     \u2264   E     vT                                                       2\n                                   n,t               1 Bn,t+1(I + \u03b7t\u03a3)(At \u2212               \u03a3)BT  n,t+1v1\n                                         \u2264   (1 + \u03b7t\u03bb1)2M2E               vT1 Bn,t+1BT     n,t+1v1     2\n                                         = (1 + \u03b7t\u03bb1)2M2\u03ban,t+1\nNow we work on the cross-term. For the convenience of notation, let\u2019s denote k := kt+1 unless\notherwise specified. Let Bn,t+1 = Bn,t+k (I + R) with,\n                                   \u2225R\u22252\u2264      (1 + c)\u03b7t+1k(M + \u03bb1) =: rt \u2264                   \u03f5 (1 + \u03f5)\n                                                                    38", "md": "have,\n\n$$\n\\|St\\|_2 = t(1 + \\eta_i\\lambda_1)^2 \\leq \\exp(2\\lambda_1 t \\eta_i), \\text{ and} \\\\\n\\|St\\|_2 = t(1 + \\eta_i\\lambda_1)^2 \\geq \\exp(2\\lambda_1 t \\eta_i - 4\\lambda_1^2 t \\eta_{2i})\n$$\nTherefore denoting $ \\theta_{\\epsilon} := 2(1 + \\epsilon) \\exp(2\\lambda_1 n \\sum_{i=1}^{n} \\eta_{2i}) $, we have\n\n$$\n\\delta_{n,0} \\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r) - \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V' + \\lambda_1^2 + C_{k,i}) \\\\\n\\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r) - \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V' + \\lambda_1^2 + C_{k,i}) \\\\\n\\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r + \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V_{k,i}) \\\\\n\\text{where } V_{k,i} \\text{ is defined in S.49. Hence proved.}\n$$\nTheorem 5. (General Version) Under Assumptions 1, 2 and 3, for all $n > kn$, and decaying step-size $\\eta_i$ satisfying C.1 and C.2, we have:\n\n$$\nE(v^T_1 B_{n,1}B^T_{n,1}v_1^{}) \\leq (1 + r)^4 \\exp(n - kn 4\\eta_i\\lambda_1 + \\sum_{i=1}^{n} \\eta_{2i} \\zeta_{k,i})\n$$\nwhere $B_{j,i}$ has been defined in 7.\n\nProof. Define $Q_{n,t} := v^T_1 B_{n,t+1}(A_t - \\Sigma)^2B^T_{n,t+1}v_1$, and $P_{n,t} := v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1$. Using S.50, we have, for $n \\geq t \\geq 1$,\n\n$$\n\\begin{align*}\n0 &\\leq v^T_1 B_{n,t}B^T_{n,t}v_1 \\\\\n&= v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)^2B^T_{n,t+1}v_1 + \\eta^2_t Q_{n,t} + 2\\eta_t P_{n,t} \\\\\n&\\leq v^T_1 B_{j,t+1}B^T_{j,t+1}v_1(1 + \\eta_t\\lambda_1)^2 + \\eta^2_t M^2 v^T_1 B_{n,t+1}B^T_{n,t+1}v_1 + 2\\eta_t P_{n,t} \\\\\n&\\leq v^T_1 B_{j,t+1}B^T_{j,t+1}v_1(1 + \\eta_t\\lambda_1)^2 + \\eta^2_t M^2 + 2\\eta_t P_{n,t}\n\\end{align*}\n$$\nThus, we have -\n\n$$\n\\kappa_{n,t} := E((v^T_1 B_{n,t}B^T_{n,t}v_1)^2) \\leq E(c_tv^T_1 B_{n,t+1}B^T_{n,t+1}v_1 + 2\\eta_t P_{n,t})^2 \\\\\n\\leq c^2P^2 + 4c_t\\eta_tE(v^T_1 P_{n,t})^2 + 4\\eta^2_t E(n,t_1 B_{n,t+1}B^T_{n,t+1}v_1) \\text{(S.56)}\n$$\nNote that,\n\n$$\nE(P^2) \\leq E(v^T_1 B_{n,t}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1) \\\\\n\\leq (1 + \\eta_t\\lambda_1)^2M^2E(v^T_1 B_{n,t+1}B^T_{n,t+1}v_1)^2 \\\\\n= (1 + \\eta_t\\lambda_1)^2M^2\\kappa_{n,t+1}\n$$\nNow we work on the cross-term. For the convenience of notation, let\u2019s denote $k := kt+1$ unless otherwise specified. Let $B_{n,t+1} = B_{n,t+k}(I + R)$ with,\n\n$$\n\\|R\\|_2 \\leq (1 + c)\\eta_{t+1}k(M + \\lambda_1) =: r_t \\leq \\epsilon(1 + \\epsilon)\n$$", "images": [], "items": [{"type": "text", "value": "have,\n\n$$\n\\|St\\|_2 = t(1 + \\eta_i\\lambda_1)^2 \\leq \\exp(2\\lambda_1 t \\eta_i), \\text{ and} \\\\\n\\|St\\|_2 = t(1 + \\eta_i\\lambda_1)^2 \\geq \\exp(2\\lambda_1 t \\eta_i - 4\\lambda_1^2 t \\eta_{2i})\n$$\nTherefore denoting $ \\theta_{\\epsilon} := 2(1 + \\epsilon) \\exp(2\\lambda_1 n \\sum_{i=1}^{n} \\eta_{2i}) $, we have\n\n$$\n\\delta_{n,0} \\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r) - \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V' + \\lambda_1^2 + C_{k,i}) \\\\\n\\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r) - \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V' + \\lambda_1^2 + C_{k,i}) \\\\\n\\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r + \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V_{k,i}) \\\\\n\\text{where } V_{k,i} \\text{ is defined in S.49. Hence proved.}\n$$\nTheorem 5. (General Version) Under Assumptions 1, 2 and 3, for all $n > kn$, and decaying step-size $\\eta_i$ satisfying C.1 and C.2, we have:\n\n$$\nE(v^T_1 B_{n,1}B^T_{n,1}v_1^{}) \\leq (1 + r)^4 \\exp(n - kn 4\\eta_i\\lambda_1 + \\sum_{i=1}^{n} \\eta_{2i} \\zeta_{k,i})\n$$\nwhere $B_{j,i}$ has been defined in 7.\n\nProof. Define $Q_{n,t} := v^T_1 B_{n,t+1}(A_t - \\Sigma)^2B^T_{n,t+1}v_1$, and $P_{n,t} := v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1$. Using S.50, we have, for $n \\geq t \\geq 1$,\n\n$$\n\\begin{align*}\n0 &\\leq v^T_1 B_{n,t}B^T_{n,t}v_1 \\\\\n&= v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)^2B^T_{n,t+1}v_1 + \\eta^2_t Q_{n,t} + 2\\eta_t P_{n,t} \\\\\n&\\leq v^T_1 B_{j,t+1}B^T_{j,t+1}v_1(1 + \\eta_t\\lambda_1)^2 + \\eta^2_t M^2 v^T_1 B_{n,t+1}B^T_{n,t+1}v_1 + 2\\eta_t P_{n,t} \\\\\n&\\leq v^T_1 B_{j,t+1}B^T_{j,t+1}v_1(1 + \\eta_t\\lambda_1)^2 + \\eta^2_t M^2 + 2\\eta_t P_{n,t}\n\\end{align*}\n$$\nThus, we have -\n\n$$\n\\kappa_{n,t} := E((v^T_1 B_{n,t}B^T_{n,t}v_1)^2) \\leq E(c_tv^T_1 B_{n,t+1}B^T_{n,t+1}v_1 + 2\\eta_t P_{n,t})^2 \\\\\n\\leq c^2P^2 + 4c_t\\eta_tE(v^T_1 P_{n,t})^2 + 4\\eta^2_t E(n,t_1 B_{n,t+1}B^T_{n,t+1}v_1) \\text{(S.56)}\n$$\nNote that,\n\n$$\nE(P^2) \\leq E(v^T_1 B_{n,t}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1) \\\\\n\\leq (1 + \\eta_t\\lambda_1)^2M^2E(v^T_1 B_{n,t+1}B^T_{n,t+1}v_1)^2 \\\\\n= (1 + \\eta_t\\lambda_1)^2M^2\\kappa_{n,t+1}\n$$\nNow we work on the cross-term. For the convenience of notation, let\u2019s denote $k := kt+1$ unless otherwise specified. Let $B_{n,t+1} = B_{n,t+k}(I + R)$ with,\n\n$$\n\\|R\\|_2 \\leq (1 + c)\\eta_{t+1}k(M + \\lambda_1) =: r_t \\leq \\epsilon(1 + \\epsilon)\n$$", "md": "have,\n\n$$\n\\|St\\|_2 = t(1 + \\eta_i\\lambda_1)^2 \\leq \\exp(2\\lambda_1 t \\eta_i), \\text{ and} \\\\\n\\|St\\|_2 = t(1 + \\eta_i\\lambda_1)^2 \\geq \\exp(2\\lambda_1 t \\eta_i - 4\\lambda_1^2 t \\eta_{2i})\n$$\nTherefore denoting $ \\theta_{\\epsilon} := 2(1 + \\epsilon) \\exp(2\\lambda_1 n \\sum_{i=1}^{n} \\eta_{2i}) $, we have\n\n$$\n\\delta_{n,0} \\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r) - \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V' + \\lambda_1^2 + C_{k,i}) \\\\\n\\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r) - \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V' + \\lambda_1^2 + C_{k,i}) \\\\\n\\geq \\exp(2\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_i - 4\\lambda_1 n - kn \\sum_{i=1}^{n} \\eta_{2i} (1 - 2r + \\theta_{\\epsilon} (1 + r)^2 n - kn \\sum_{t=1}^{n} W_{k,t}\\eta_{2t} \\exp(n - kn \\sum_{i=1}^{n} \\eta_{2i}) V_{k,i}) \\\\\n\\text{where } V_{k,i} \\text{ is defined in S.49. Hence proved.}\n$$\nTheorem 5. (General Version) Under Assumptions 1, 2 and 3, for all $n > kn$, and decaying step-size $\\eta_i$ satisfying C.1 and C.2, we have:\n\n$$\nE(v^T_1 B_{n,1}B^T_{n,1}v_1^{}) \\leq (1 + r)^4 \\exp(n - kn 4\\eta_i\\lambda_1 + \\sum_{i=1}^{n} \\eta_{2i} \\zeta_{k,i})\n$$\nwhere $B_{j,i}$ has been defined in 7.\n\nProof. Define $Q_{n,t} := v^T_1 B_{n,t+1}(A_t - \\Sigma)^2B^T_{n,t+1}v_1$, and $P_{n,t} := v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1$. Using S.50, we have, for $n \\geq t \\geq 1$,\n\n$$\n\\begin{align*}\n0 &\\leq v^T_1 B_{n,t}B^T_{n,t}v_1 \\\\\n&= v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)^2B^T_{n,t+1}v_1 + \\eta^2_t Q_{n,t} + 2\\eta_t P_{n,t} \\\\\n&\\leq v^T_1 B_{j,t+1}B^T_{j,t+1}v_1(1 + \\eta_t\\lambda_1)^2 + \\eta^2_t M^2 v^T_1 B_{n,t+1}B^T_{n,t+1}v_1 + 2\\eta_t P_{n,t} \\\\\n&\\leq v^T_1 B_{j,t+1}B^T_{j,t+1}v_1(1 + \\eta_t\\lambda_1)^2 + \\eta^2_t M^2 + 2\\eta_t P_{n,t}\n\\end{align*}\n$$\nThus, we have -\n\n$$\n\\kappa_{n,t} := E((v^T_1 B_{n,t}B^T_{n,t}v_1)^2) \\leq E(c_tv^T_1 B_{n,t+1}B^T_{n,t+1}v_1 + 2\\eta_t P_{n,t})^2 \\\\\n\\leq c^2P^2 + 4c_t\\eta_tE(v^T_1 P_{n,t})^2 + 4\\eta^2_t E(n,t_1 B_{n,t+1}B^T_{n,t+1}v_1) \\text{(S.56)}\n$$\nNote that,\n\n$$\nE(P^2) \\leq E(v^T_1 B_{n,t}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1) \\\\\n\\leq (1 + \\eta_t\\lambda_1)^2M^2E(v^T_1 B_{n,t+1}B^T_{n,t+1}v_1)^2 \\\\\n= (1 + \\eta_t\\lambda_1)^2M^2\\kappa_{n,t+1}\n$$\nNow we work on the cross-term. For the convenience of notation, let\u2019s denote $k := kt+1$ unless otherwise specified. Let $B_{n,t+1} = B_{n,t+k}(I + R)$ with,\n\n$$\n\\|R\\|_2 \\leq (1 + c)\\eta_{t+1}k(M + \\lambda_1) =: r_t \\leq \\epsilon(1 + \\epsilon)\n$$"}]}, {"page": 39, "text": " Using Lemma 2, we have\n          |vT                                                           | = |vT\n            1 Bn,t+1BT      n,t+1v1 \u2212   Y1 vT1 Bn,t+kBT      n,t+kv1      \u2264  |vT1 Bn,t+k(R + RT + RRT )BT     2rt + r2     n,t+kv1|    (S.57)\nWe will also bound                                                              1 Bn,t+kBT      n,t+kv1|                t\n |vT                                                                                                                    |\n   1 Bn,t+1(I + \u03b7t\u03a3)(At \u2212                \u03a3)BT  n,t+1v1 \u2212   Y2  vT1 Bn,t+k(I + \u03b7t\u03a3)(At \u2212               \u03a3)BT   n,t+kv1\n      = |vT 1 Bn,t+kR(I + \u03b7t\u03a3)(At \u2212                 \u03a3)(I + RT )BT        n,t+kv1 + vT      1 Bn,t+k(I + \u03b7t\u03a3)(At \u2212               \u03a3)RT BT     n,t+kv1|\n      \u2264  (2rt + r2    t ) (1 + \u03b7t\u03bb1) M|vT        1 Bn,t+kBT     n,t+kv1|                                                               (S.58)\n So, now we have:\n       E     vT1 Bn,t+1BT      n,t+1v1Pn,t\n        = E      (vT1 Bn,t+1BT     n,t+1v1)(vT     1 Bn,t+1(I + \u03b7t\u03a3)(At \u2212               \u03a3)BT   n,t+1v1)\n        = E      (Y1 + vT   1 Bn,t+kBT     n,t+kv1)(Y2 + vT        1 Bn,t+k(I + \u03b7t\u03a3)(At \u2212                \u03a3)BT  n,t+kv1)\n        = E [Y1Y2]       + E     Y1vT  1 Bn,t+k(I + \u03b7t\u03a3)(At \u2212               \u03a3)BT   n,t+kv1      + E     Y2vT 1 Bn,t+kBT      n,t+kv1\n                 T1                                         T 2                                                      T3\n          + E     (vT1 Bn,t+kBT      n,t+kv1)(vT     1 Bn,t+k(I + \u03b7t\u03a3)(At \u2212               \u03a3)BT   n,t+kv1)\n                                                             T4\n Lets start with the last term, T4. Using Lemma S.3 we have,\n           |T4| \u2264     E    (vT1 Bn,t+kBT      n,t+kv1)(vT    1 Bn,t+k(I + \u03b7t\u03a3)E [(At \u2212                 \u03a3)|st+k] BT     n,t+kv1)\n                 \u2264   2(1 + \u03b7t\u03bb1)Mdmix (k) \u03ban,t+k\n                 \u2264   2\u03b72 t+1(1 + \u03b7t\u03bb1)M\u03ban,t+k\n                 \u2264   2\u03b72 t+1(1 + \u03b7t\u03bb1)M\u03ban,t+1\n Using Eqs S.57 and S.58 the first three terms can be bounded as:\n       |T1|\u2264     E [|Y1Y2|] \u2264         2rt + r2   t 2 (1 + \u03b7t\u03bb1) M\u03ban,t+k\n                                \u2264     2rt + r2   t 2 (1 + \u03b7t\u03bb1) M\u03ban,t+1 using Lemma S.3\n                                = (2 + rt)2 r2      t (1 + \u03b7t\u03bb1) M\u03ban,t+1\n                                \u2264   (1 + \u03f5)2 (2 + \u03f5 (1 + \u03f5))2 (1 + \u03b7t\u03bb1) \u03b72                t+1k2  t+1M (M + \u03bb1)2 \u03ban,t+1\n                                \u2264   (1 + \u03f5)3  2 + \u03f5 + \u03f52 2 \u03b72          t+1k2  t+1M (M + \u03bb1)2 \u03ban,t+1 since \u03b7t\u03bb1 \u2264                        \u03f5\n                      |T2| \u2264    E    |Y1vT 1 Bn,t+k(I + \u03b7t\u03a3)(At \u2212                \u03a3)BT   n,t+kv1|\n                             \u2264  (2 + rt) rt (1 + \u03b7t\u03bb1) M\u03ban,t+k\n                             \u2264  (2 + rt) rt (1 + \u03b7t\u03bb1) M\u03ban,t+1 using Lemma S.3\n                             \u2264    2 + \u03f5 + \u03f52       (1 + \u03f5) (1 + \u03b7t\u03bb1) \u03b7t+1kt+1 (M + \u03bb1) M\u03ban,t+1\n                             \u2264  (1 + \u03f5)2  2 + \u03f5 + \u03f52            \u03b7t+1kt+1M (M + \u03bb1) \u03ban,t+1\n                                                                       39", "md": "Using Lemma 2, we have\n\n$$\n\\left| v^T \\right| = \\left| v^T_1 B_{n,t+1} B^T_{n,t+1} v_1 - Y_1 v^T_1 B_{n,t+k} B^T_{n,t+k} v_1 \\right| \\leq \\left| v^T_1 B_{n,t+k}(R + R^T + R^TR)B^T_{n,t+k}v_1 \\right| (2r_t + r^2_{n,t+k}v_1| \\quad (S.57)\n$$\nWe will also bound\n\n$$\n\\left| v^T_1 B_{n,t+k}B^T_{n,t+k}v_1 \\right| \\leq \\left| v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1 - Y_2 v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1 \\right| \\leq (2r_t + r^2_t) (1 + \\eta_t\\lambda_1) M \\left| v^T_1 B_{n,t+k}B^T_{n,t+k}v_1 \\right| \\quad (S.58)\n$$\nSo, now we have:\n\n$$\n\\begin{align*}\nE \\left[ v^T_1 B_{n,t+1}B^T_{n,t+1}v_1 P_{n,t} \\right] & = E \\left[ (v^T_1 B_{n,t+1}B^T_{n,t+1}v_1)(v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1) \\right] \\\\\n& = E \\left[ (Y_1 + v^T_1 B_{n,t+k}B^T_{n,t+k}v_1)(Y_2 + v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1) \\right] \\\\\n& = E[Y_1Y_2] + E[Y_1v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1] + E[Y_2v^T_1 B_{n,t+k}B^T_{n,t+k}v_1]\n\\end{align*}\n$$\nLet's start with the last term, T4. Using Lemma S.3 we have,\n\n$$\n\\left| T_4 \\right| \\leq E \\left[ (v^T_1 B_{n,t+k}B^T_{n,t+k}v_1)(v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)E[(A_t - \\Sigma)|s_{t+k}]B^T_{n,t+k}v_1) \\right] \\\\\n\\leq 2(1 + \\eta_t\\lambda_1)Mdmix(k)\\kappa_{n,t+k} \\\\\n\\leq 2\\eta^2_{t+1}(1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n\\leq 2\\eta^2_{t+1}(1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1}\n$$\nUsing Eqs S.57 and S.58 the first three terms can be bounded as:\n\n$$\n\\begin{align*}\n\\left| T_1 \\right| & \\leq E \\left[ |Y_1Y_2| \\right] \\leq 2r_t + r^2_t 2 (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n& \\leq 2r_t + r^2_t 2 (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\text{ using Lemma S.3} \\\\\n& = (2 + r_t)^2 r^2_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^2 (2 + \\epsilon(1 + \\epsilon))^2 (1 + \\eta_t\\lambda_1) \\eta^2_{t+1}k^2_{t+1}M(M + \\lambda_1)^2\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^3 2 + \\epsilon + \\epsilon^2 2 \\eta^2_{t+1}k^2_{t+1}M(M + \\lambda_1)^2\\kappa_{n,t+1} \\text{ since } \\eta_t\\lambda_1 \\leq \\epsilon\n\\end{align*}\n$$\n$$\n\\begin{align*}\n\\left| T_2 \\right| & \\leq E \\left[ |Y_1v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1| \\right] \\\\\n& \\leq (2 + r_t) r_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n& \\leq (2 + r_t) r_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\text{ using Lemma S.3} \\\\\n& \\leq 2 + \\epsilon + \\epsilon^2 (1 + \\epsilon) (1 + \\eta_t\\lambda_1) \\eta_{t+1}k_{t+1} (M + \\lambda_1) M\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^2 2 + \\epsilon + \\epsilon^2 \\eta_{t+1}k_{t+1}M(M + \\lambda_1) \\kappa_{n,t+1}\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Using Lemma 2, we have\n\n$$\n\\left| v^T \\right| = \\left| v^T_1 B_{n,t+1} B^T_{n,t+1} v_1 - Y_1 v^T_1 B_{n,t+k} B^T_{n,t+k} v_1 \\right| \\leq \\left| v^T_1 B_{n,t+k}(R + R^T + R^TR)B^T_{n,t+k}v_1 \\right| (2r_t + r^2_{n,t+k}v_1| \\quad (S.57)\n$$\nWe will also bound\n\n$$\n\\left| v^T_1 B_{n,t+k}B^T_{n,t+k}v_1 \\right| \\leq \\left| v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1 - Y_2 v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1 \\right| \\leq (2r_t + r^2_t) (1 + \\eta_t\\lambda_1) M \\left| v^T_1 B_{n,t+k}B^T_{n,t+k}v_1 \\right| \\quad (S.58)\n$$\nSo, now we have:\n\n$$\n\\begin{align*}\nE \\left[ v^T_1 B_{n,t+1}B^T_{n,t+1}v_1 P_{n,t} \\right] & = E \\left[ (v^T_1 B_{n,t+1}B^T_{n,t+1}v_1)(v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1) \\right] \\\\\n& = E \\left[ (Y_1 + v^T_1 B_{n,t+k}B^T_{n,t+k}v_1)(Y_2 + v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1) \\right] \\\\\n& = E[Y_1Y_2] + E[Y_1v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1] + E[Y_2v^T_1 B_{n,t+k}B^T_{n,t+k}v_1]\n\\end{align*}\n$$\nLet's start with the last term, T4. Using Lemma S.3 we have,\n\n$$\n\\left| T_4 \\right| \\leq E \\left[ (v^T_1 B_{n,t+k}B^T_{n,t+k}v_1)(v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)E[(A_t - \\Sigma)|s_{t+k}]B^T_{n,t+k}v_1) \\right] \\\\\n\\leq 2(1 + \\eta_t\\lambda_1)Mdmix(k)\\kappa_{n,t+k} \\\\\n\\leq 2\\eta^2_{t+1}(1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n\\leq 2\\eta^2_{t+1}(1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1}\n$$\nUsing Eqs S.57 and S.58 the first three terms can be bounded as:\n\n$$\n\\begin{align*}\n\\left| T_1 \\right| & \\leq E \\left[ |Y_1Y_2| \\right] \\leq 2r_t + r^2_t 2 (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n& \\leq 2r_t + r^2_t 2 (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\text{ using Lemma S.3} \\\\\n& = (2 + r_t)^2 r^2_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^2 (2 + \\epsilon(1 + \\epsilon))^2 (1 + \\eta_t\\lambda_1) \\eta^2_{t+1}k^2_{t+1}M(M + \\lambda_1)^2\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^3 2 + \\epsilon + \\epsilon^2 2 \\eta^2_{t+1}k^2_{t+1}M(M + \\lambda_1)^2\\kappa_{n,t+1} \\text{ since } \\eta_t\\lambda_1 \\leq \\epsilon\n\\end{align*}\n$$\n$$\n\\begin{align*}\n\\left| T_2 \\right| & \\leq E \\left[ |Y_1v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1| \\right] \\\\\n& \\leq (2 + r_t) r_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n& \\leq (2 + r_t) r_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\text{ using Lemma S.3} \\\\\n& \\leq 2 + \\epsilon + \\epsilon^2 (1 + \\epsilon) (1 + \\eta_t\\lambda_1) \\eta_{t+1}k_{t+1} (M + \\lambda_1) M\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^2 2 + \\epsilon + \\epsilon^2 \\eta_{t+1}k_{t+1}M(M + \\lambda_1) \\kappa_{n,t+1}\n\\end{align*}\n$$", "md": "Using Lemma 2, we have\n\n$$\n\\left| v^T \\right| = \\left| v^T_1 B_{n,t+1} B^T_{n,t+1} v_1 - Y_1 v^T_1 B_{n,t+k} B^T_{n,t+k} v_1 \\right| \\leq \\left| v^T_1 B_{n,t+k}(R + R^T + R^TR)B^T_{n,t+k}v_1 \\right| (2r_t + r^2_{n,t+k}v_1| \\quad (S.57)\n$$\nWe will also bound\n\n$$\n\\left| v^T_1 B_{n,t+k}B^T_{n,t+k}v_1 \\right| \\leq \\left| v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1 - Y_2 v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1 \\right| \\leq (2r_t + r^2_t) (1 + \\eta_t\\lambda_1) M \\left| v^T_1 B_{n,t+k}B^T_{n,t+k}v_1 \\right| \\quad (S.58)\n$$\nSo, now we have:\n\n$$\n\\begin{align*}\nE \\left[ v^T_1 B_{n,t+1}B^T_{n,t+1}v_1 P_{n,t} \\right] & = E \\left[ (v^T_1 B_{n,t+1}B^T_{n,t+1}v_1)(v^T_1 B_{n,t+1}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+1}v_1) \\right] \\\\\n& = E \\left[ (Y_1 + v^T_1 B_{n,t+k}B^T_{n,t+k}v_1)(Y_2 + v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1) \\right] \\\\\n& = E[Y_1Y_2] + E[Y_1v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1] + E[Y_2v^T_1 B_{n,t+k}B^T_{n,t+k}v_1]\n\\end{align*}\n$$\nLet's start with the last term, T4. Using Lemma S.3 we have,\n\n$$\n\\left| T_4 \\right| \\leq E \\left[ (v^T_1 B_{n,t+k}B^T_{n,t+k}v_1)(v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)E[(A_t - \\Sigma)|s_{t+k}]B^T_{n,t+k}v_1) \\right] \\\\\n\\leq 2(1 + \\eta_t\\lambda_1)Mdmix(k)\\kappa_{n,t+k} \\\\\n\\leq 2\\eta^2_{t+1}(1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n\\leq 2\\eta^2_{t+1}(1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1}\n$$\nUsing Eqs S.57 and S.58 the first three terms can be bounded as:\n\n$$\n\\begin{align*}\n\\left| T_1 \\right| & \\leq E \\left[ |Y_1Y_2| \\right] \\leq 2r_t + r^2_t 2 (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n& \\leq 2r_t + r^2_t 2 (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\text{ using Lemma S.3} \\\\\n& = (2 + r_t)^2 r^2_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^2 (2 + \\epsilon(1 + \\epsilon))^2 (1 + \\eta_t\\lambda_1) \\eta^2_{t+1}k^2_{t+1}M(M + \\lambda_1)^2\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^3 2 + \\epsilon + \\epsilon^2 2 \\eta^2_{t+1}k^2_{t+1}M(M + \\lambda_1)^2\\kappa_{n,t+1} \\text{ since } \\eta_t\\lambda_1 \\leq \\epsilon\n\\end{align*}\n$$\n$$\n\\begin{align*}\n\\left| T_2 \\right| & \\leq E \\left[ |Y_1v^T_1 B_{n,t+k}(I + \\eta_t\\Sigma)(A_t - \\Sigma)B^T_{n,t+k}v_1| \\right] \\\\\n& \\leq (2 + r_t) r_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+k} \\\\\n& \\leq (2 + r_t) r_t (1 + \\eta_t\\lambda_1)M\\kappa_{n,t+1} \\text{ using Lemma S.3} \\\\\n& \\leq 2 + \\epsilon + \\epsilon^2 (1 + \\epsilon) (1 + \\eta_t\\lambda_1) \\eta_{t+1}k_{t+1} (M + \\lambda_1) M\\kappa_{n,t+1} \\\\\n& \\leq (1 + \\epsilon)^2 2 + \\epsilon + \\epsilon^2 \\eta_{t+1}k_{t+1}M(M + \\lambda_1) \\kappa_{n,t+1}\n\\end{align*}\n$$"}]}, {"page": 40, "text": "and similarly,\n              |T3| \u2264    E   Y2vT 1 Bn,t+kBT     n,t+kv1\n                     \u2264  rt (2 + rt) (1 + \u03b7t\u03bb1) M\u03ban,t+k\n                     \u2264  (1 + \u03f5)     2 + \u03f5 + \u03f52      (1 + \u03b7t\u03bb1) \u03b7t+1kt+1M (M + \u03bb1) \u03ban,t+k\n                     \u2264  (1 + \u03f5)2  2 + \u03f5 + \u03f52         \u03b7t+1kt+1M (M + \u03bb1) \u03ban,t+k\nNote that            \u2264  (1 + \u03f5)2  2 + \u03f5 + \u03f52         \u03b7t+1kt+1M (M + \u03bb1) \u03ban,t+1 using Lemma S.3\n          ct := (1 + \u03b7t\u03bb1)2 + \u03b72        t M2 \u2264     1 + 2\u03f5 + 2\u03f52, and\n          c2       1 + 2\u03b7t\u03bb1 + \u03b72         M2 + \u03bb2        2\n           t =                        t              1\n              = 1 + 4\u03b72   t \u03bb21 + \u03b74 t   M2 + \u03bb2    1 2 + 4\u03b7t\u03bb1 + 4\u03b73       t \u03bb1   M2 + \u03bb2    1   + 2\u03b72  t  M2 + \u03bb2     1\n              \u2264  1 + 4\u03b7t\u03bb1 + \u03b72     t   2M2 + 6\u03bb2      1 + \u03f5M + \u03f5\u03bb1          + 4\u03b73 t \u03bb1    M2 + \u03bb2    1\nDefine        \u2264  1 + 4\u03b7t\u03bb1 + 6\u03b72      t (M + \u03bb1)2 + 4\u03b73        t \u03bb1 (M + \u03bb1)2\n                           \u03d5\u03f5 := (1 + \u03f5)       2 + \u03f5 + \u03f52\n                           \u03c9\u03f5 := 1 + 2\u03f5 + 2\u03f52\n                         \u03b6k,t := (10 + 8 (1 + \u03f5) + 4 (1 + 2\u03f5) \u03d5\u03f5) \u03d5\u03f5ctkt+1 (M + \u03bb1)2\nPutting everything together in Eq S.56, for t \u2264                n \u2212   kt+1 we have,\n   \u03ban,t\n \u03ban,t+1                                                             2\u03d5\u03f5\u03b7t+1kt+1 (M + \u03bb1) +                2 + \u03d52                            \u03b72\n \u2264   c2        t (1 + \u03b7t\u03bb1)2 M2 + 4 (1 + \u03f5) ct\u03b7tM                                                                \u03f5k2 t+1 (M + \u03bb1)2           t+1\n      t + 4\u03b72\n \u2264   c2        t (1 + \u03b7t\u03bb1)2 M2 + 4 (1 + \u03f5) ct\u03b7tM                   2\u03d5\u03f5\u03b7tkt+1 (M + \u03bb1) +              2 + \u03d52  \u03f5k2t+1 (M + \u03bb1)2          \u03b72t\n      t + 4\u03b72\n = c2 t + 4\u03b72  t   M2 + 2\u03d5\u03f5 (1 + \u03f5) ctM (M + \u03bb1) kt+1                     + 4\u03b73  t  (1 + 2\u03f5) \u03bb1 + (1 + \u03f5) ctM              2 + \u03d52  \u03f5k2t+1 (M + \u03bb1)2\n \u2264   c2        t [2 + 2\u03d5\u03f5 (1 + \u03f5) ctkt+1] M (M + \u03bb1) + 4 (1 + 2\u03f5) \u03b73                    t   \u03bb1 + ctM        2 + \u03d52  \u03f5k2t+1 (M + \u03bb1)2\n      t + 4\u03b72\n \u2264   1 + 4\u03b7t\u03bb1 + \u03b72     t [10 + 8\u03d5\u03f5 (2 + \u03f5) ctkt+1] (M + \u03bb1)2 + 4 (1 + 2\u03f5) \u03b73                   t  \u03bb1 + 2ctM + ct\u03d52         \u03f5k2t+1 (M + \u03bb1)3\n \u2264   exp    4\u03b7t\u03bb1 + \u03b72    t (10 + 8\u03d5\u03f5 (1 + \u03f5) ctkt+1) (M + \u03bb1)2 + 4 (1 + 2\u03f5) \u03b73                   t    \u03bb1 + ctM + 2ct\u03d52        \u03f5k2t+1 (M + \u03bb1)3\n \u2264   exp    4\u03b7t\u03bb1 + \u03b72    t (10 + 8\u03d5\u03f5 (1 + \u03f5) ctkt+1) (M + \u03bb1)2 + 4\u03f5 (1 + 2\u03f5) \u03b72                    t   2ct + ct\u03d52   \u03f5kt+1 (M + \u03bb1)2\n \u2264   exp    4\u03b7t\u03bb1 + \u03b72    t   8\u03f5 (1 + 2\u03f5) \u03c9\u03f5 + (10 + (8 (1 + \u03f5) + 4\u03f5 (1 + 2\u03f5) \u03d5\u03f5) \u03d5\u03f5\u03c9\u03f5kt+1) (M + \u03bb1)2\n \u2264   exp    4\u03b7t\u03bb1 + \u03b72    t   1 + (10 + 20kt+1) (M + \u03bb1)2\n \u2264   exp    4\u03b7t\u03bb1 + \u03b72    t   1 + (10 + 20kt+1) (M + \u03bb1)2\n \u2264   exp    4\u03b7t\u03bb1 + 40\u03b72     t kt+1 (M + \u03bb1)2            since (M + \u03bb1) , kt+1 \u2265            1\nRecall our definition of k := kt+1. We can use the above recursion for 1 \u2264                           t \u2264   n \u2212   kt+1. We note\nthat t = n \u2212      kn satisfies the conditions. Therefore,\n                               \u03ban,1 \u2264    exp    n\u2212kn     4\u03b7i\u03bb1 +     n\u2212kn   \u03b72i \u03b6k,i   \u03ban,n\u2212kn+1\n                                                   i=1                i=1\n                                                                 40", "md": "# Math Equations\n\nand similarly,\n\n$$\n\\begin{align*}\n|T3| &amp; \\leq E Y^2vT1Bn,t+kBTn,t+kv1 \\\\\n&amp; \\leq rt(2 + rt)(1 + \\eta t \\lambda 1) M\\kappa n,t+k \\\\\n&amp; \\leq (1 + \\epsilon) 2 + \\epsilon + \\epsilon^2 (1 + \\eta t \\lambda 1) \\eta t+1 k t+1 M (M + \\lambda 1) \\kappa n,t+k \\\\\n&amp; \\leq (1 + \\epsilon)^2 2 + \\epsilon + \\epsilon^2 \\eta t+1 k t+1 M (M + \\lambda 1) \\kappa n,t+k \\\\\n\\end{align*}\n$$\n\nNote that\n\n$$\n\\begin{align*}\nct &amp; := (1 + \\eta t \\lambda 1)^2 + \\eta^2 t M^2 \\leq 1 + 2\\epsilon + 2\\epsilon^2, \\text{and} \\\\\nc2 &amp; 1 + 2\\eta t \\lambda 1 + \\eta^2 t M^2 + \\lambda^2 2 \\\\\nt &amp; = 1 + 4\\eta^2 t \\lambda^2 1 + \\eta^4 t M^2 + \\lambda^2 1 2 + 4\\eta t \\lambda 1 + 4\\eta^3 t \\lambda 1 M^2 + \\lambda^2 1 + 2\\eta^2 t M^2 + \\lambda^2 1 \\\\\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nDefine\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nPutting everything together in Eq S.56, for \\(t \\leq n - kt+1\\) we have,\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nRecall our definition of \\(k := kt+1\\). We can use the above recursion for \\(1 \\leq t \\leq n - kt+1\\). We note that \\(t = n - kn\\) satisfies the conditions. Therefore,\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "and similarly,\n\n$$\n\\begin{align*}", "md": "and similarly,\n\n$$\n\\begin{align*}"}, {"type": "table", "rows": [["T3"]], "md": "|T3| &amp; \\leq E Y^2vT1Bn,t+kBTn,t+kv1 \\\\", "isPerfectTable": true, "csv": "\"T3\""}, {"type": "text", "value": "&amp; \\leq rt(2 + rt)(1 + \\eta t \\lambda 1) M\\kappa n,t+k \\\\\n&amp; \\leq (1 + \\epsilon) 2 + \\epsilon + \\epsilon^2 (1 + \\eta t \\lambda 1) \\eta t+1 k t+1 M (M + \\lambda 1) \\kappa n,t+k \\\\\n&amp; \\leq (1 + \\epsilon)^2 2 + \\epsilon + \\epsilon^2 \\eta t+1 k t+1 M (M + \\lambda 1) \\kappa n,t+k \\\\\n\\end{align*}\n$$\n\nNote that\n\n$$\n\\begin{align*}\nct &amp; := (1 + \\eta t \\lambda 1)^2 + \\eta^2 t M^2 \\leq 1 + 2\\epsilon + 2\\epsilon^2, \\text{and} \\\\\nc2 &amp; 1 + 2\\eta t \\lambda 1 + \\eta^2 t M^2 + \\lambda^2 2 \\\\\nt &amp; = 1 + 4\\eta^2 t \\lambda^2 1 + \\eta^4 t M^2 + \\lambda^2 1 2 + 4\\eta t \\lambda 1 + 4\\eta^3 t \\lambda 1 M^2 + \\lambda^2 1 + 2\\eta^2 t M^2 + \\lambda^2 1 \\\\\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nDefine\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nPutting everything together in Eq S.56, for \\(t \\leq n - kt+1\\) we have,\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nRecall our definition of \\(k := kt+1\\). We can use the above recursion for \\(1 \\leq t \\leq n - kt+1\\). We note that \\(t = n - kn\\) satisfies the conditions. Therefore,\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$", "md": "&amp; \\leq rt(2 + rt)(1 + \\eta t \\lambda 1) M\\kappa n,t+k \\\\\n&amp; \\leq (1 + \\epsilon) 2 + \\epsilon + \\epsilon^2 (1 + \\eta t \\lambda 1) \\eta t+1 k t+1 M (M + \\lambda 1) \\kappa n,t+k \\\\\n&amp; \\leq (1 + \\epsilon)^2 2 + \\epsilon + \\epsilon^2 \\eta t+1 k t+1 M (M + \\lambda 1) \\kappa n,t+k \\\\\n\\end{align*}\n$$\n\nNote that\n\n$$\n\\begin{align*}\nct &amp; := (1 + \\eta t \\lambda 1)^2 + \\eta^2 t M^2 \\leq 1 + 2\\epsilon + 2\\epsilon^2, \\text{and} \\\\\nc2 &amp; 1 + 2\\eta t \\lambda 1 + \\eta^2 t M^2 + \\lambda^2 2 \\\\\nt &amp; = 1 + 4\\eta^2 t \\lambda^2 1 + \\eta^4 t M^2 + \\lambda^2 1 2 + 4\\eta t \\lambda 1 + 4\\eta^3 t \\lambda 1 M^2 + \\lambda^2 1 + 2\\eta^2 t M^2 + \\lambda^2 1 \\\\\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nDefine\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nPutting everything together in Eq S.56, for \\(t \\leq n - kt+1\\) we have,\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$\n\nRecall our definition of \\(k := kt+1\\). We can use the above recursion for \\(1 \\leq t \\leq n - kt+1\\). We note that \\(t = n - kn\\) satisfies the conditions. Therefore,\n\n$$\n\\begin{align*}\n\\ldots\n\\end{align*}\n$$"}]}, {"page": 41, "text": "Let Bn,n\u2212kn+1 = I + R\u2032, with \u2225R\u2032\u22252\u2264                       r a.s.\n                          \u03ban,n\u2212kn+1 = E              vT                                      2\n                                                      1 Bn,n\u2212kn+1BT         n,n\u2212kn+1v1\n                                           = E       vT                                                    2\n                                                      1 v1 + vT   1 (R\u2032 + R\u2032T )v1 + vT        1 R\u2032R\u2032T v1\nUsing Lemma 2, we have                     \u2264    1 + 2r + r2 2 E            vT1 v1   2\n                                  r \u2264   (1 + \u03f5) kn+1\u03b7n\u2212kn+1 (M + \u03bb1)\n                                    \u2264   (1 + \u03f5) kn\u03b7n\u2212kn (M + \u03bb1)\nwhich completes our proof.          \u2264   2 (1 + \u03f5) kn\u03b7n (M + \u03bb1) since \u03b7n\u2212kn \u2264                        2\u03b7n\nS.5         Main Results : Details and Proofs\nS.5.1        Proof of Theorem 1\nLemma S.13. This lemma proves conditions required later in the proof. Let the step-sizes be set\naccording to Lemma S.12 and m := 200. Define\n                                r := 2 (1 + \u03f5) \u03b7nkn (M + \u03bb1) ,\n                                s := 3(1 + r)2        n\u2212kn\u22121     W  k,t\u03b72 t exp    n\u2212kn\u22121       Vk,i\u03b72  i\n                                                         t=1                          i=t+1\nwhere Wk,t is defined in Theorem 4, Vk,i is defined in S.49 and \u03b1, \u03b2, f (.) , \u03b4 are defined in Lemma\nS.12. Then for sufficiently large number of samples n, such that\n                                                           n                    \u03b2\n                                                    log (f (n)) >        log (f (0))\nwe have\n    1. 2r + s \u2264        1\n                       2 (S.63)\n                                                         1    \u03b4/m\n    2. r = 2 (1 + \u03f5) \u03b7nkn (M + \u03bb1) <                    50  1+\u03b4/m (S.66)\nProof. For (1), using Lemma S.12-(3), we note that\n                                s \u2264    3(1 + r)2     n\u2212kn\u22121      Wk,t\u03b72  t exp     n\u2212kn\u22121       Vk,i\u03b72 i\n                                                        t=1                           i=t+1\n                                   \u2264   3(1 + r)2     n\u2212kn\u22121      Wk,t\u03b72  t    1 + \u03b4  m\n                                                        t=1\n                                   \u2264   3(1 + r)2         1 + \u03b4        log     1 + \u03b4                           (S.59)\n                                           100                 m                    m\n                                   \u2264   3(1 + r)2 log (2)         since \u03b4\n                                                 50                      m < 1\nTherefore,\n                                                  2r + s \u2264      2r + 3 (1 + r)2\n                                                                             25\n                                                            = 3 25 + 56  25r + 3   25r2                       (S.60)\n                                                                    41", "md": "# Math Equations\n\nLet $$B_{n,n-kn+1} = I + R',$$ with $$\\|R'\\|_2 \\leq r$$ a.s.\n\n$$\\kappa_{n,n-kn+1} = E[v^T_1 B_{n,n-kn+1} B^T_{n,n-kn+1}v_1]$$\n\n$$= E[v^T_1 v_1 + v^T_1 (R' + R'^T)v_1 + v^T_1 R'R'^Tv_1]$$\n\nUsing Lemma 2, we have\n\n$$r \\leq (1 + \\epsilon)kn+1\\eta_{n-kn+1}(M + \\lambda_1)$$\n\n$$\\leq (1 + \\epsilon)kn\\eta_{n-kn}(M + \\lambda_1)$$\n\nwhich completes our proof.\n\n$$\\leq 2(1 + \\epsilon)kn\\eta_n(M + \\lambda_1)$$ since $$\\eta_{n-kn} \\leq 2\\eta_n$$\n\n## Main Results: Details and Proofs\n\n### Proof of Theorem 1\n\nLemma S.13. This lemma proves conditions required later in the proof. Let the step-sizes be set according to Lemma S.12 and $$m := 200$$. Define\n\n$$r := 2(1 + \\epsilon)\\eta_nkn(M + \\lambda_1),$$\n\n$$s := 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn-1}V_{k,i}\\eta^2_i\\right)$$\n\nwhere $$W_{k,t}$$ is defined in Theorem 4, $$V_{k,i}$$ is defined in S.49 and $$\\alpha, \\beta, f(.), \\delta$$ are defined in Lemma S.12. Then for a sufficiently large number of samples $$n$$, such that\n\n$$\\log(f(n)) > \\frac{\\log(f(0))}{\\beta}$$\n\nwe have\n\n1. $2r + s \\leq \\frac{1}{2}$ (S.63)\n2. $r = 2(1 + \\epsilon)\\eta_nkn(M + \\lambda_1) < \\frac{50}{1+\\delta/m}$ (S.66)\n\nProof. For (1), using Lemma S.12-(3), we note that\n\n$$s \\leq 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn-1}V_{k,i}\\eta^2_i\\right)$$\n\n$$\\leq 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t(1 + \\delta/m)$$\n\n$$\\leq 3(1 + r)^2\\frac{1 + \\delta}{100}\\log\\left(1 + \\frac{\\delta}{m}\\right)$$ (S.59)\n\n$$\\leq 3(1 + r)^2\\log(2)$$ since $$\\delta < \\frac{50}{m} < 1$$\n\nTherefore,\n\n$$2r + s \\leq 2r + 3(1 + r)^2/25$$\n\n$$= \\frac{3}{25} + \\frac{56}{25}r + \\frac{3}{25}r^2$$ (S.60)\n\n$$\\frac{41}{25}$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations", "md": "# Math Equations"}, {"type": "text", "value": "Let $$B_{n,n-kn+1} = I + R',$$ with $$\\|R'\\|_2 \\leq r$$ a.s.\n\n$$\\kappa_{n,n-kn+1} = E[v^T_1 B_{n,n-kn+1} B^T_{n,n-kn+1}v_1]$$\n\n$$= E[v^T_1 v_1 + v^T_1 (R' + R'^T)v_1 + v^T_1 R'R'^Tv_1]$$\n\nUsing Lemma 2, we have\n\n$$r \\leq (1 + \\epsilon)kn+1\\eta_{n-kn+1}(M + \\lambda_1)$$\n\n$$\\leq (1 + \\epsilon)kn\\eta_{n-kn}(M + \\lambda_1)$$\n\nwhich completes our proof.\n\n$$\\leq 2(1 + \\epsilon)kn\\eta_n(M + \\lambda_1)$$ since $$\\eta_{n-kn} \\leq 2\\eta_n$$", "md": "Let $$B_{n,n-kn+1} = I + R',$$ with $$\\|R'\\|_2 \\leq r$$ a.s.\n\n$$\\kappa_{n,n-kn+1} = E[v^T_1 B_{n,n-kn+1} B^T_{n,n-kn+1}v_1]$$\n\n$$= E[v^T_1 v_1 + v^T_1 (R' + R'^T)v_1 + v^T_1 R'R'^Tv_1]$$\n\nUsing Lemma 2, we have\n\n$$r \\leq (1 + \\epsilon)kn+1\\eta_{n-kn+1}(M + \\lambda_1)$$\n\n$$\\leq (1 + \\epsilon)kn\\eta_{n-kn}(M + \\lambda_1)$$\n\nwhich completes our proof.\n\n$$\\leq 2(1 + \\epsilon)kn\\eta_n(M + \\lambda_1)$$ since $$\\eta_{n-kn} \\leq 2\\eta_n$$"}, {"type": "heading", "lvl": 2, "value": "Main Results: Details and Proofs", "md": "## Main Results: Details and Proofs"}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 1", "md": "### Proof of Theorem 1"}, {"type": "text", "value": "Lemma S.13. This lemma proves conditions required later in the proof. Let the step-sizes be set according to Lemma S.12 and $$m := 200$$. Define\n\n$$r := 2(1 + \\epsilon)\\eta_nkn(M + \\lambda_1),$$\n\n$$s := 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn-1}V_{k,i}\\eta^2_i\\right)$$\n\nwhere $$W_{k,t}$$ is defined in Theorem 4, $$V_{k,i}$$ is defined in S.49 and $$\\alpha, \\beta, f(.), \\delta$$ are defined in Lemma S.12. Then for a sufficiently large number of samples $$n$$, such that\n\n$$\\log(f(n)) > \\frac{\\log(f(0))}{\\beta}$$\n\nwe have\n\n1. $2r + s \\leq \\frac{1}{2}$ (S.63)\n2. $r = 2(1 + \\epsilon)\\eta_nkn(M + \\lambda_1) < \\frac{50}{1+\\delta/m}$ (S.66)\n\nProof. For (1), using Lemma S.12-(3), we note that\n\n$$s \\leq 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn-1}V_{k,i}\\eta^2_i\\right)$$\n\n$$\\leq 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t(1 + \\delta/m)$$\n\n$$\\leq 3(1 + r)^2\\frac{1 + \\delta}{100}\\log\\left(1 + \\frac{\\delta}{m}\\right)$$ (S.59)\n\n$$\\leq 3(1 + r)^2\\log(2)$$ since $$\\delta < \\frac{50}{m} < 1$$\n\nTherefore,\n\n$$2r + s \\leq 2r + 3(1 + r)^2/25$$\n\n$$= \\frac{3}{25} + \\frac{56}{25}r + \\frac{3}{25}r^2$$ (S.60)\n\n$$\\frac{41}{25}$$", "md": "Lemma S.13. This lemma proves conditions required later in the proof. Let the step-sizes be set according to Lemma S.12 and $$m := 200$$. Define\n\n$$r := 2(1 + \\epsilon)\\eta_nkn(M + \\lambda_1),$$\n\n$$s := 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn-1}V_{k,i}\\eta^2_i\\right)$$\n\nwhere $$W_{k,t}$$ is defined in Theorem 4, $$V_{k,i}$$ is defined in S.49 and $$\\alpha, \\beta, f(.), \\delta$$ are defined in Lemma S.12. Then for a sufficiently large number of samples $$n$$, such that\n\n$$\\log(f(n)) > \\frac{\\log(f(0))}{\\beta}$$\n\nwe have\n\n1. $2r + s \\leq \\frac{1}{2}$ (S.63)\n2. $r = 2(1 + \\epsilon)\\eta_nkn(M + \\lambda_1) < \\frac{50}{1+\\delta/m}$ (S.66)\n\nProof. For (1), using Lemma S.12-(3), we note that\n\n$$s \\leq 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn-1}V_{k,i}\\eta^2_i\\right)$$\n\n$$\\leq 3(1 + r)^2\\sum_{t=1}^{n-kn-1}W_{k,t}\\eta^2_t(1 + \\delta/m)$$\n\n$$\\leq 3(1 + r)^2\\frac{1 + \\delta}{100}\\log\\left(1 + \\frac{\\delta}{m}\\right)$$ (S.59)\n\n$$\\leq 3(1 + r)^2\\log(2)$$ since $$\\delta < \\frac{50}{m} < 1$$\n\nTherefore,\n\n$$2r + s \\leq 2r + 3(1 + r)^2/25$$\n\n$$= \\frac{3}{25} + \\frac{56}{25}r + \\frac{3}{25}r^2$$ (S.60)\n\n$$\\frac{41}{25}$$"}]}, {"page": 42, "text": "             3\nSetting      25 + 56  25r + 3    25r2 \u2264       1\n                                              2, we have,\n                                                                  3\n                                                                 25 + 56   25r + 3     25r2 \u2264        1\n                                                                                                     2\nwhich holds for r \u2208               0, 1 10   .             =\u21d2       6r2 + 112r \u2212           19 \u2264     0\nFor (2), using Lemma S.12 and substituting the value of ki := \u03c4mix                                                  \u03b72     \u2264     2\u03c4mix              1     for\n                                                                                                                      i          log(2) log        \u03b72\n                                                                                                                                                    i\n\u03b7i < 1, we note that\n                   r \u2264     8 (1 + \u03f5) \u03c4mix (M + \u03bb1)                              \u03b1                       (\u03bb1 \u2212        \u03bb2) (\u03b2 + n)\n                                         log (2)                  (\u03bb1 \u2212      \u03bb2) (\u03b2 + n) log                            \u03b1\n                                                                  log      (\u03bb1\u2212\u03bb2)(\u03b2+n)\n                      = 8 (1 + \u03f5) \u03c4mix (M + \u03bb1)                                     \u03b1\n                                         log (2)                        (\u03bb1\u2212\u03bb2)(\u03b2+n)\n                                                                                  \u03b1\n                      = 8 (1 + \u03f5) \u03c4mix (M + \u03bb1)                   log (f (n))\n                                         log (2)                       f (n)\nTherefore (2) holds for sufficiently large n, i.e,\n                                                 f (n)                      1 + \u03b4   m     (1 + \u03f5) \u03c4     mix (M + \u03bb1)\nThis is satisfied if                        log (f (n)) \u2265           400                  log (2) \u03b4   m\n                                            n                              1 + \u03b4   m     (1 + \u03f5)     (M + \u03bb1) \u03b1                                      (S.61)\nFrom Lemma S.12, we have             log (f (n)) \u2265           400\u03c4mix       log (2)                   (\u03bb1 \u2212      \u03bb2) \u03b4  m\n               \u03b2                                                                     (i)                 1 + \u03b4  m     (1 + \u03f5)      (M + \u03bb1) \u03b1\n       log (f (0)) \u2265           600\u03c4mix (1 + 2\u03f5)2 (M + \u03bb1)2 \u03b12                        \u2265    400\u03c4mix        log (2)                   (\u03bb  1 \u2212   \u03bb2) \u03b4\nwhere (i) follows since M+\u03bb1          (\u03bb1 \u2212     \u03bb2)2 log        1 + \u03b4  m                                                            n               m  \u03b2\n                                      \u03bb1\u2212\u03bb2 > 1, \u03b1 > 2 and log (1 + x) \u2264                          x \u2200x. Therefore,            log(f(n)) >         log(f(0))\nsuffices. Further, we note that (2) implies (1) for m = 200, \u03b4 \u2264                                       1. Therefore, the condition on n is\nsufficient for both results. Hence proved.\nLemma S.14. Let\nwhere ki is defined in Lemma S.12. Then,            u := min {i : i \u2208            [n], i \u2212     ki \u2265     0}\nProof. Using the definition of ki mentioned in Lemma S.12, we have   u \u2264    \u230a\u03b2\u230b    \u2264    \u03b2\n                                     ki := \u03c4mix         \u03b72i    \u2264     2\u03c4mix                1\n                                                                    log (2) log          \u03b72i\n                                                               = 4\u03c4mix                 (\u03bb1 \u2212        \u03bb2) (\u03b2 + i)\n                                                                    log (2) log                       \u03b1\n                                                                             42", "md": "$$\n\\begin{align*}\n&\\text{Setting} \\quad 25 + 56 \\cdot 25r + 3 \\cdot 25r^2 \\leq 1 \\\\\n&\\text{which holds for } r \\in \\left[0, \\frac{1}{10}\\right] \\Rightarrow 6r^2 + 112r - 19 \\leq 0\n\\end{align*}\n$$\n\nFor (2), using Lemma S.12 and substituting the value of \\(k_i := \\tau_{\\text{mix}} \\frac{\\eta^2}{\\log(2) \\log \\eta^2}\\) for \\(\\eta_i < 1\\), we note that\n\n$$\n\\begin{align*}\nr &\\leq 8(1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\alpha}{\\log(2)} \\frac{(\\lambda_1 - \\lambda_2)(\\beta + n)}{\\log(\\alpha)} \\\\\n&= 8(1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\log(f(n))}{\\log(2) f(n)}\n\\end{align*}\n$$\n\nTherefore (2) holds for sufficiently large \\(n\\), i.e,\n\n$$\nf(n) \\geq \\frac{1 + \\delta}{m} (1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\alpha}{\\log(2)} \\quad \\text{(S.61)}\n$$\n\nFrom Lemma S.12, we have\n\n$$\n\\begin{align*}\n\\log(f(n)) &\\geq 400\\tau_{\\text{mix}} \\log(2) \\frac{(\\lambda_1 - \\lambda_2)\\delta}{1 + \\delta} \\frac{m}{(1 + \\epsilon)(M + \\lambda_1) \\alpha} \\\\\n\\log(f(0)) &\\geq 600\\tau_{\\text{mix}}(1 + 2\\epsilon)^2(M + \\lambda_1)^2 \\alpha^2 \\geq 400\\tau_{\\text{mix}} \\log(2) \\frac{(\\lambda_1 - \\lambda_2)\\delta}{1 + \\delta} \\frac{m}{\\beta}\n\\end{align*}\n$$\n\nwhere \\(\\lambda_1 - \\lambda_2 > 1\\), \\(\\alpha > 2\\), and \\(\\log(1 + x) \\leq x \\, \\forall x\\). Therefore, \\(\\log(f(n)) > \\log(f(0))\\) suffices. Further, we note that (2) implies (1) for \\(m = 200\\), \\(\\delta \\leq 1\\). Therefore, the condition on \\(n\\) is sufficient for both results. Hence proved.\n\nLemma S.14. Let\n\nwhere \\(k_i\\) is defined in Lemma S.12. Then, \\(u := \\min \\{i : i \\in [n], i - k_i \\geq 0\\}\\)\n\nProof. Using the definition of \\(k_i\\) mentioned in Lemma S.12, we have \\(u \\leq \\lfloor \\beta \\rfloor \\leq \\beta\\)\n\n$$\n\\begin{align*}\nk_i &:= \\tau_{\\text{mix}} \\frac{\\eta^2_i}{\\log(2) \\log \\eta^2_i} \\\\\n&= 4\\tau_{\\text{mix}} \\frac{(\\lambda_1 - \\lambda_2)(\\beta + i)}{\\log(2) \\log \\alpha}\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "$$\n\\begin{align*}\n&\\text{Setting} \\quad 25 + 56 \\cdot 25r + 3 \\cdot 25r^2 \\leq 1 \\\\\n&\\text{which holds for } r \\in \\left[0, \\frac{1}{10}\\right] \\Rightarrow 6r^2 + 112r - 19 \\leq 0\n\\end{align*}\n$$\n\nFor (2), using Lemma S.12 and substituting the value of \\(k_i := \\tau_{\\text{mix}} \\frac{\\eta^2}{\\log(2) \\log \\eta^2}\\) for \\(\\eta_i < 1\\), we note that\n\n$$\n\\begin{align*}\nr &\\leq 8(1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\alpha}{\\log(2)} \\frac{(\\lambda_1 - \\lambda_2)(\\beta + n)}{\\log(\\alpha)} \\\\\n&= 8(1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\log(f(n))}{\\log(2) f(n)}\n\\end{align*}\n$$\n\nTherefore (2) holds for sufficiently large \\(n\\), i.e,\n\n$$\nf(n) \\geq \\frac{1 + \\delta}{m} (1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\alpha}{\\log(2)} \\quad \\text{(S.61)}\n$$\n\nFrom Lemma S.12, we have\n\n$$\n\\begin{align*}\n\\log(f(n)) &\\geq 400\\tau_{\\text{mix}} \\log(2) \\frac{(\\lambda_1 - \\lambda_2)\\delta}{1 + \\delta} \\frac{m}{(1 + \\epsilon)(M + \\lambda_1) \\alpha} \\\\\n\\log(f(0)) &\\geq 600\\tau_{\\text{mix}}(1 + 2\\epsilon)^2(M + \\lambda_1)^2 \\alpha^2 \\geq 400\\tau_{\\text{mix}} \\log(2) \\frac{(\\lambda_1 - \\lambda_2)\\delta}{1 + \\delta} \\frac{m}{\\beta}\n\\end{align*}\n$$\n\nwhere \\(\\lambda_1 - \\lambda_2 > 1\\), \\(\\alpha > 2\\), and \\(\\log(1 + x) \\leq x \\, \\forall x\\). Therefore, \\(\\log(f(n)) > \\log(f(0))\\) suffices. Further, we note that (2) implies (1) for \\(m = 200\\), \\(\\delta \\leq 1\\). Therefore, the condition on \\(n\\) is sufficient for both results. Hence proved.\n\nLemma S.14. Let\n\nwhere \\(k_i\\) is defined in Lemma S.12. Then, \\(u := \\min \\{i : i \\in [n], i - k_i \\geq 0\\}\\)\n\nProof. Using the definition of \\(k_i\\) mentioned in Lemma S.12, we have \\(u \\leq \\lfloor \\beta \\rfloor \\leq \\beta\\)\n\n$$\n\\begin{align*}\nk_i &:= \\tau_{\\text{mix}} \\frac{\\eta^2_i}{\\log(2) \\log \\eta^2_i} \\\\\n&= 4\\tau_{\\text{mix}} \\frac{(\\lambda_1 - \\lambda_2)(\\beta + i)}{\\log(2) \\log \\alpha}\n\\end{align*}\n$$", "md": "$$\n\\begin{align*}\n&\\text{Setting} \\quad 25 + 56 \\cdot 25r + 3 \\cdot 25r^2 \\leq 1 \\\\\n&\\text{which holds for } r \\in \\left[0, \\frac{1}{10}\\right] \\Rightarrow 6r^2 + 112r - 19 \\leq 0\n\\end{align*}\n$$\n\nFor (2), using Lemma S.12 and substituting the value of \\(k_i := \\tau_{\\text{mix}} \\frac{\\eta^2}{\\log(2) \\log \\eta^2}\\) for \\(\\eta_i < 1\\), we note that\n\n$$\n\\begin{align*}\nr &\\leq 8(1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\alpha}{\\log(2)} \\frac{(\\lambda_1 - \\lambda_2)(\\beta + n)}{\\log(\\alpha)} \\\\\n&= 8(1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\log(f(n))}{\\log(2) f(n)}\n\\end{align*}\n$$\n\nTherefore (2) holds for sufficiently large \\(n\\), i.e,\n\n$$\nf(n) \\geq \\frac{1 + \\delta}{m} (1 + \\epsilon) \\tau_{\\text{mix}}(M + \\lambda_1) \\frac{\\alpha}{\\log(2)} \\quad \\text{(S.61)}\n$$\n\nFrom Lemma S.12, we have\n\n$$\n\\begin{align*}\n\\log(f(n)) &\\geq 400\\tau_{\\text{mix}} \\log(2) \\frac{(\\lambda_1 - \\lambda_2)\\delta}{1 + \\delta} \\frac{m}{(1 + \\epsilon)(M + \\lambda_1) \\alpha} \\\\\n\\log(f(0)) &\\geq 600\\tau_{\\text{mix}}(1 + 2\\epsilon)^2(M + \\lambda_1)^2 \\alpha^2 \\geq 400\\tau_{\\text{mix}} \\log(2) \\frac{(\\lambda_1 - \\lambda_2)\\delta}{1 + \\delta} \\frac{m}{\\beta}\n\\end{align*}\n$$\n\nwhere \\(\\lambda_1 - \\lambda_2 > 1\\), \\(\\alpha > 2\\), and \\(\\log(1 + x) \\leq x \\, \\forall x\\). Therefore, \\(\\log(f(n)) > \\log(f(0))\\) suffices. Further, we note that (2) implies (1) for \\(m = 200\\), \\(\\delta \\leq 1\\). Therefore, the condition on \\(n\\) is sufficient for both results. Hence proved.\n\nLemma S.14. Let\n\nwhere \\(k_i\\) is defined in Lemma S.12. Then, \\(u := \\min \\{i : i \\in [n], i - k_i \\geq 0\\}\\)\n\nProof. Using the definition of \\(k_i\\) mentioned in Lemma S.12, we have \\(u \\leq \\lfloor \\beta \\rfloor \\leq \\beta\\)\n\n$$\n\\begin{align*}\nk_i &:= \\tau_{\\text{mix}} \\frac{\\eta^2_i}{\\log(2) \\log \\eta^2_i} \\\\\n&= 4\\tau_{\\text{mix}} \\frac{(\\lambda_1 - \\lambda_2)(\\beta + i)}{\\log(2) \\log \\alpha}\n\\end{align*}\n$$"}]}, {"page": 43, "text": "Therefore,\n          \u230a\u03b2\u230b   \u2212   k\u230a\u03b2\u230b   \u2265   \u230a\u03b2\u230b   \u2212    4\u03c4mix           \u03b2 + \u230a\u03b2\u230b\u03b1\n                                         log (2) log          \u03bb1\u2212\u03bb2\n                           \u2265    \u03b2                            2\u03b2         since \u03b2 > 1\n                                2 \u2212     4\u03c4mix                 \u03b1\n                                       log (2) log        \u03bb1\u2212\u03bb2\n                           = \u03b2     12 \u2212     4\u03c4mix     log (2f (0))       , where f (.) is defined in Lemma S.12\n                                           log (2)           \u03b2\nNow, from Lemma S.12, we know that f (0) > e. Therefore, log (2f (0)) \u2264                                     2 log (f (0)). Then,\n                                        \u230a\u03b2\u230b   \u2212  k\u230a\u03b2\u230b    \u2265  \u03b2   1 2 \u2212     8\u03c4mix    log (f (0))\n                                                                        log (2)          \u03b2\nAgain, from the conditions in Lemma S.12, we know that\n            log (f (0))      \u2264      \u03f5       \u03bb1 \u2212    \u03bb2               1       since \u03b1 > 2, \u03bb1 \u2212          \u03bb2   \u2264   1, \u03f5 \u2264      1\nTherefore,         \u03b2             6\u03c4mix   (M + \u03bb1) \u03b1 \u2264           120\u03c4mix                         M + \u03bb1                     100\nHence proved.                            \u230a\u03b2\u230b   \u2212  k\u230a\u03b2\u230b    \u2265   \u03b2  1 2 \u2212    120 log8(2)         \u2265   0\nS.5.1.1      Numerator\nUsing Theorem 3 and Markov\u2019s Inequality, we have with probability atleast (1 \u2212                                     \u03b4)\nTr    V T          n V\u22a5     \u2264\n        \u22a5  BnBT   n                                i\u2212ki     \uf8eb           n                                          \uf8eb      i                      \uf8f6\uf8f6\n    1.05exp           i=u+1 2\u03b7i\u03bb2 + Vk,i\u03b72                  \uf8edd +              (V\u2032 + \u03bek,i) C\u2032     k,i\u03b72i\u2212ki exp     \uf8ed           2\u03b7j (\u03bb1 \u2212    \u03bb2)  \uf8f8\uf8f8\n                                  \u03b4                                  i=u+1                                            j=u+1\nS.5.1.2      Denominator\nUsing Chebyshev\u2019s Inequality we have, with probability atleast (1 \u2212                               \u03b4)\n                                                                \uf8eb                         vT                2         \uf8f6\n                                                                \uf8ec            1              1 BnBT   n v1             \uf8f7\n                   vT1 BnBT   n v1 \u2265     E   vT1 BnBT   n v1    \uf8ec               E                           2    \u2212   1\uf8f7             (S.62)\n                                                       1        \uf8ed1 \u2212         \u03b4       E    vT1 BnBT   n v1             \uf8f8\nLet r := 2 (1 + \u03f5) \u03b7nkn (M + \u03bb1) \u2264                    10. Using Theorem 3, we have\n                       E     vT1 BnBT          2    \u2264   (1 + r)4 exp        n\u2212kn     4\u03b7i\u03bb1 +      n\u2212kn    \u03b72\n                                         n v1                                                              i \u03b6k,t\n                                                                               i=1                 i=1\nUsing Theorem 4, we have\nE   vT1 Bn,1BT    n,1v1     \u2265   exp     2\u03bb1   n\u2212kn    \u03b7i \u2212   4\u03bb2 1 n\u2212kn    \u03b72i      1 \u2212      2r + 3 (1 + r)2        n\u2212kn    Wk,t\u03b72  t exp   n\u2212kn     \u03b72i Vk,i\n                                                i=1                  i=1                                             t=1                     i=t+1\n                                                                    43", "md": "Therefore,\n\n$$\n\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\left\\lfloor \\beta \\right\\rfloor - 4\\tau_{\\text{mix}} \\frac{\\beta + \\left\\lfloor \\beta \\right\\rfloor \\alpha}{\\log(2) \\log \\frac{\\lambda_1}{\\lambda_2}} \\geq \\beta \\frac{2 - 4\\tau_{\\text{mix}}}{\\alpha \\log(2) \\log \\frac{\\lambda_1}{\\lambda_2}} = \\beta \\frac{12 - 4\\tau_{\\text{mix}} \\log(2f(0))}{\\log(2) \\beta}\n$$\nwhere f(.) is defined in Lemma S.12\n\nNow, from Lemma S.12, we know that f(0) &gt; e. Therefore, $\\log(2f(0)) \\leq 2\\log(f(0))$. Then,\n\n$$\n\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\beta \\frac{1}{2} - 8\\tau_{\\text{mix}} \\frac{\\log(f(0))}{\\log(2) \\beta}\n$$\nAgain, from the conditions in Lemma S.12, we know that $\\log(f(0)) \\leq \\epsilon \\frac{\\lambda_1 - \\lambda_2}{1}$ since $\\alpha &gt; 2$, $\\lambda_1 - \\lambda_2 \\leq 1$, $\\epsilon \\leq 1$\n\nTherefore, $\\beta \\frac{6\\tau_{\\text{mix}}(M + \\lambda_1)\\alpha}{120\\tau_{\\text{mix}}(M + \\lambda_1)100}$\n\nHence proved. $\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\beta \\frac{1}{2} - 120\\log_8(2) \\geq 0$\n\nS.5.1.1 Numerator\n\nUsing Theorem 3 and Markov\u2019s Inequality, we have with probability at least $(1 - \\delta)$\n\n$$\n\\text{Tr} \\left( V^T \\right)_{\\perp} B_n B^T_n \\leq \\frac{n}{i-k_i} \\left( \\begin{array}{c} n \\\\ i \\end{array} \\right) \\left( \\begin{array}{c} i \\\\ u+1 \\end{array} \\right) 2\\eta_i\\lambda_2 + V_{k,i}\\eta^2 \\left( d + \\frac{(V' + \\xi_{k,i})C'}{k_i\\eta^2i-k_i} \\exp \\left( \\begin{array}{c} 2\\eta_j(\\lambda_1 - \\lambda_2) \\\\ j=u+1 \\end{array} \\right) \\right)\n$$\nwith probability at least $(1 - \\delta)$\n\nS.5.1.2 Denominator\n\nUsing Chebyshev\u2019s Inequality we have, with probability at least $(1 - \\delta)$\n\n$$\n\\frac{v^T_1 B_n B^T_n v_1}{E(v^T_1 B_n B^T_n v_1)} \\geq \\frac{1}{1 - \\delta} \\left( \\begin{array}{cc} \\frac{v^T}{v_1} & 2 \\\\ 1 & 1 - B_n B^T_n v_1 \\end{array} \\right) \\text{(S.62)}\n$$\nLet $r := 2(1 + \\epsilon)\\eta n k_n (M + \\lambda_1) \\leq 10$. Using Theorem 3, we have\n\n$$\nE(v^T_1 B_n B^T_n)^2 \\leq (1 + r)^4 \\exp \\left( n-k_n 4\\eta_i\\lambda_1 + n-k_n \\eta^2 \\right)\n$$\nUsing Theorem 4, we have\n\n$$\nE(v^T_1 B_{n,1} B^T_{n,1} v_1) \\geq \\exp \\left( 2\\lambda_1 n-k_n \\eta_i - 4\\lambda_2 1 n-k_n \\eta^2 \\right) 1 - 2r + 3(1 + r)^2 n-k_n W_{k,t}\\eta^2_t \\exp \\left( n-k_n \\eta^2_i V_{k,i} \\right)\n$$\nwhere $i=1$, $t=1$, $i=t+1$\n\n43", "images": [], "items": [{"type": "text", "value": "Therefore,\n\n$$\n\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\left\\lfloor \\beta \\right\\rfloor - 4\\tau_{\\text{mix}} \\frac{\\beta + \\left\\lfloor \\beta \\right\\rfloor \\alpha}{\\log(2) \\log \\frac{\\lambda_1}{\\lambda_2}} \\geq \\beta \\frac{2 - 4\\tau_{\\text{mix}}}{\\alpha \\log(2) \\log \\frac{\\lambda_1}{\\lambda_2}} = \\beta \\frac{12 - 4\\tau_{\\text{mix}} \\log(2f(0))}{\\log(2) \\beta}\n$$\nwhere f(.) is defined in Lemma S.12\n\nNow, from Lemma S.12, we know that f(0) &gt; e. Therefore, $\\log(2f(0)) \\leq 2\\log(f(0))$. Then,\n\n$$\n\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\beta \\frac{1}{2} - 8\\tau_{\\text{mix}} \\frac{\\log(f(0))}{\\log(2) \\beta}\n$$\nAgain, from the conditions in Lemma S.12, we know that $\\log(f(0)) \\leq \\epsilon \\frac{\\lambda_1 - \\lambda_2}{1}$ since $\\alpha &gt; 2$, $\\lambda_1 - \\lambda_2 \\leq 1$, $\\epsilon \\leq 1$\n\nTherefore, $\\beta \\frac{6\\tau_{\\text{mix}}(M + \\lambda_1)\\alpha}{120\\tau_{\\text{mix}}(M + \\lambda_1)100}$\n\nHence proved. $\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\beta \\frac{1}{2} - 120\\log_8(2) \\geq 0$\n\nS.5.1.1 Numerator\n\nUsing Theorem 3 and Markov\u2019s Inequality, we have with probability at least $(1 - \\delta)$\n\n$$\n\\text{Tr} \\left( V^T \\right)_{\\perp} B_n B^T_n \\leq \\frac{n}{i-k_i} \\left( \\begin{array}{c} n \\\\ i \\end{array} \\right) \\left( \\begin{array}{c} i \\\\ u+1 \\end{array} \\right) 2\\eta_i\\lambda_2 + V_{k,i}\\eta^2 \\left( d + \\frac{(V' + \\xi_{k,i})C'}{k_i\\eta^2i-k_i} \\exp \\left( \\begin{array}{c} 2\\eta_j(\\lambda_1 - \\lambda_2) \\\\ j=u+1 \\end{array} \\right) \\right)\n$$\nwith probability at least $(1 - \\delta)$\n\nS.5.1.2 Denominator\n\nUsing Chebyshev\u2019s Inequality we have, with probability at least $(1 - \\delta)$\n\n$$\n\\frac{v^T_1 B_n B^T_n v_1}{E(v^T_1 B_n B^T_n v_1)} \\geq \\frac{1}{1 - \\delta} \\left( \\begin{array}{cc} \\frac{v^T}{v_1} & 2 \\\\ 1 & 1 - B_n B^T_n v_1 \\end{array} \\right) \\text{(S.62)}\n$$\nLet $r := 2(1 + \\epsilon)\\eta n k_n (M + \\lambda_1) \\leq 10$. Using Theorem 3, we have\n\n$$\nE(v^T_1 B_n B^T_n)^2 \\leq (1 + r)^4 \\exp \\left( n-k_n 4\\eta_i\\lambda_1 + n-k_n \\eta^2 \\right)\n$$\nUsing Theorem 4, we have\n\n$$\nE(v^T_1 B_{n,1} B^T_{n,1} v_1) \\geq \\exp \\left( 2\\lambda_1 n-k_n \\eta_i - 4\\lambda_2 1 n-k_n \\eta^2 \\right) 1 - 2r + 3(1 + r)^2 n-k_n W_{k,t}\\eta^2_t \\exp \\left( n-k_n \\eta^2_i V_{k,i} \\right)\n$$\nwhere $i=1$, $t=1$, $i=t+1$\n\n43", "md": "Therefore,\n\n$$\n\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\left\\lfloor \\beta \\right\\rfloor - 4\\tau_{\\text{mix}} \\frac{\\beta + \\left\\lfloor \\beta \\right\\rfloor \\alpha}{\\log(2) \\log \\frac{\\lambda_1}{\\lambda_2}} \\geq \\beta \\frac{2 - 4\\tau_{\\text{mix}}}{\\alpha \\log(2) \\log \\frac{\\lambda_1}{\\lambda_2}} = \\beta \\frac{12 - 4\\tau_{\\text{mix}} \\log(2f(0))}{\\log(2) \\beta}\n$$\nwhere f(.) is defined in Lemma S.12\n\nNow, from Lemma S.12, we know that f(0) &gt; e. Therefore, $\\log(2f(0)) \\leq 2\\log(f(0))$. Then,\n\n$$\n\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\beta \\frac{1}{2} - 8\\tau_{\\text{mix}} \\frac{\\log(f(0))}{\\log(2) \\beta}\n$$\nAgain, from the conditions in Lemma S.12, we know that $\\log(f(0)) \\leq \\epsilon \\frac{\\lambda_1 - \\lambda_2}{1}$ since $\\alpha &gt; 2$, $\\lambda_1 - \\lambda_2 \\leq 1$, $\\epsilon \\leq 1$\n\nTherefore, $\\beta \\frac{6\\tau_{\\text{mix}}(M + \\lambda_1)\\alpha}{120\\tau_{\\text{mix}}(M + \\lambda_1)100}$\n\nHence proved. $\\left\\lfloor \\beta \\right\\rfloor - k\\left\\lfloor \\beta \\right\\rfloor \\geq \\beta \\frac{1}{2} - 120\\log_8(2) \\geq 0$\n\nS.5.1.1 Numerator\n\nUsing Theorem 3 and Markov\u2019s Inequality, we have with probability at least $(1 - \\delta)$\n\n$$\n\\text{Tr} \\left( V^T \\right)_{\\perp} B_n B^T_n \\leq \\frac{n}{i-k_i} \\left( \\begin{array}{c} n \\\\ i \\end{array} \\right) \\left( \\begin{array}{c} i \\\\ u+1 \\end{array} \\right) 2\\eta_i\\lambda_2 + V_{k,i}\\eta^2 \\left( d + \\frac{(V' + \\xi_{k,i})C'}{k_i\\eta^2i-k_i} \\exp \\left( \\begin{array}{c} 2\\eta_j(\\lambda_1 - \\lambda_2) \\\\ j=u+1 \\end{array} \\right) \\right)\n$$\nwith probability at least $(1 - \\delta)$\n\nS.5.1.2 Denominator\n\nUsing Chebyshev\u2019s Inequality we have, with probability at least $(1 - \\delta)$\n\n$$\n\\frac{v^T_1 B_n B^T_n v_1}{E(v^T_1 B_n B^T_n v_1)} \\geq \\frac{1}{1 - \\delta} \\left( \\begin{array}{cc} \\frac{v^T}{v_1} & 2 \\\\ 1 & 1 - B_n B^T_n v_1 \\end{array} \\right) \\text{(S.62)}\n$$\nLet $r := 2(1 + \\epsilon)\\eta n k_n (M + \\lambda_1) \\leq 10$. Using Theorem 3, we have\n\n$$\nE(v^T_1 B_n B^T_n)^2 \\leq (1 + r)^4 \\exp \\left( n-k_n 4\\eta_i\\lambda_1 + n-k_n \\eta^2 \\right)\n$$\nUsing Theorem 4, we have\n\n$$\nE(v^T_1 B_{n,1} B^T_{n,1} v_1) \\geq \\exp \\left( 2\\lambda_1 n-k_n \\eta_i - 4\\lambda_2 1 n-k_n \\eta^2 \\right) 1 - 2r + 3(1 + r)^2 n-k_n W_{k,t}\\eta^2_t \\exp \\left( n-k_n \\eta^2_i V_{k,i} \\right)\n$$\nwhere $i=1$, $t=1$, $i=t+1$\n\n43"}]}, {"page": 44, "text": "Let\n                                   s := 3 (1 + r)2       n\u2212kn    W  k,t\u03b72 t exp    n\u2212kn      \u03b72i Vk,i\n                                                           t=1                       i=t+1\nThen,\n                      E     vT1 BnBT    n v1   2    \u2264       (1 + r)4                n\u2212kn     \u03b72    \u03b6k,i + 4\u03bb2\n                        E   vT1 BnBT    n v1   2        (1 \u2212   2r \u2212    s)2 exp         i=1     i                1\nBy Lemma S.13, we have that\n                                                             2r + s \u2264       1                                                    (S.63)\n                                                                            2.\nThen, using\nwe have,        (1 \u2212 1 x)2 \u2264     1 + 6x for x \u2208          0, 12     and, (1 + x)4 \u2264         1 + 5x for x \u2208          0, 110\n          E      vT         n v1   2                                                  n\u2212kn\n                  1 BnBT\n            E    vT         n v1   2    \u2264   (1 + 5r) (1 + 12r + 6s) exp                  i=1   \u03b72i   \u03b6k,i + 4\u03bb2   1\n                  1 BnBT                \u2264    1 + 17r + 6s + 60r2 + 30rs                  exp    n\u2212kn     \u03b72i   \u03b6k,i + 4\u03bb2  1\n                                                                                                   i=1\n                                        \u2264   (1 + 22r + 12s) exp            n\u2212kn     \u03b72i   \u03b6k,i + 4\u03bb2   1       since r \u2264      1\n                                                                              i=1                                            10\nBy Lemma S.12-(3), we have that\n                                          exp    n\u2212kn      \u03b72i   \u03b6k,i + 4\u03bb2   1      \u2264   1 + \u03b4 m                                 (S.64)\n                                                    i=1\nBy S.59, we have that\n                                  12s \u2264      48(1 + r)2         1 + \u03b4      2   log     1 + \u03b4\n                                                  100                  m                     m\n                                         \u2264   3    1 + \u03b4      2   log     1 + \u03b4         since r \u2264       1                         (S.65)\n                                             5           m                     m                      10\nBy Lemma S.13, we have that\n                                     r = 2 (1 + \u03f5) \u03b7nkn (M + \u03bb1) < 1                         \u03b4/m                                 (S.66)\n                                                                                     50   1 + \u03b4/m\n                                                                    44", "md": "Let $$s := 3(1 + r)^2 \\sum_{t=1}^{n-kn} W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn} \\eta^2_i V_{k,i}\\right)$$\n\nThen,\n\n$$\n\\begin{align*}\n&E[v^T_1 B_n B^T_n v_1]^2 \\\\\n&\\leq (1 + r)^4 \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2 \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&E[v^T_1 B_n B^T_n v_1]^2 \\leq (1 - 2r - s)^2 \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2 \\zeta_{k,i} + 4\\lambda^2\\right)\n\\end{align*}\n$$\n\nBy Lemma S.13, we have that $$2r + s \\leq 1$$ (S.63)\n\nThen, using\n\n$$\n\\begin{align*}\n&(1 - \\frac{1}{x})^2 \\leq 1 + 6x \\text{ for } x \\in [0, \\frac{1}{2}] \\\\\n&(1 + x)^4 \\leq 1 + 5x \\text{ for } x \\in [0, \\frac{1}{10}]\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n&E[v^T_1 B_n B^T_n v_1]^2 \\leq (1 + 5r)(1 + 12r + 6s) \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&\\leq 1 + 17r + 6s + 60r^2 + 30rs \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&\\leq (1 + 22r + 12s) \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\text{ since } r \\leq \\frac{1}{10}\n\\end{align*}\n$$\n\nBy Lemma S.12-(3), we have that $$\\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\leq 1 + \\delta m$$ (S.64)\n\nBy S.59, we have that\n\n$$\n\\begin{align*}\n&12s \\leq 48(1 + r)^2 \\frac{1 + \\delta}{100m} \\log\\left(1 + \\delta\\right) \\\\\n&\\leq 3\\frac{1 + \\delta}{5m} \\log\\left(1 + \\delta\\right) \\text{ since } r \\leq \\frac{1}{10} \\text{ (S.65)}\n\\end{align*}\n$$\n\nBy Lemma S.13, we have that $$r = 2(1 + \\epsilon) \\eta nkn (M + \\lambda_1) < \\frac{1}{\\delta/m}$$ (S.66)\n\n$$\\frac{50}{1 + \\delta/m}$$", "images": [], "items": [{"type": "text", "value": "Let $$s := 3(1 + r)^2 \\sum_{t=1}^{n-kn} W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn} \\eta^2_i V_{k,i}\\right)$$\n\nThen,\n\n$$\n\\begin{align*}\n&E[v^T_1 B_n B^T_n v_1]^2 \\\\\n&\\leq (1 + r)^4 \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2 \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&E[v^T_1 B_n B^T_n v_1]^2 \\leq (1 - 2r - s)^2 \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2 \\zeta_{k,i} + 4\\lambda^2\\right)\n\\end{align*}\n$$\n\nBy Lemma S.13, we have that $$2r + s \\leq 1$$ (S.63)\n\nThen, using\n\n$$\n\\begin{align*}\n&(1 - \\frac{1}{x})^2 \\leq 1 + 6x \\text{ for } x \\in [0, \\frac{1}{2}] \\\\\n&(1 + x)^4 \\leq 1 + 5x \\text{ for } x \\in [0, \\frac{1}{10}]\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n&E[v^T_1 B_n B^T_n v_1]^2 \\leq (1 + 5r)(1 + 12r + 6s) \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&\\leq 1 + 17r + 6s + 60r^2 + 30rs \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&\\leq (1 + 22r + 12s) \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\text{ since } r \\leq \\frac{1}{10}\n\\end{align*}\n$$\n\nBy Lemma S.12-(3), we have that $$\\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\leq 1 + \\delta m$$ (S.64)\n\nBy S.59, we have that\n\n$$\n\\begin{align*}\n&12s \\leq 48(1 + r)^2 \\frac{1 + \\delta}{100m} \\log\\left(1 + \\delta\\right) \\\\\n&\\leq 3\\frac{1 + \\delta}{5m} \\log\\left(1 + \\delta\\right) \\text{ since } r \\leq \\frac{1}{10} \\text{ (S.65)}\n\\end{align*}\n$$\n\nBy Lemma S.13, we have that $$r = 2(1 + \\epsilon) \\eta nkn (M + \\lambda_1) < \\frac{1}{\\delta/m}$$ (S.66)\n\n$$\\frac{50}{1 + \\delta/m}$$", "md": "Let $$s := 3(1 + r)^2 \\sum_{t=1}^{n-kn} W_{k,t}\\eta^2_t \\exp\\left(\\sum_{i=t+1}^{n-kn} \\eta^2_i V_{k,i}\\right)$$\n\nThen,\n\n$$\n\\begin{align*}\n&E[v^T_1 B_n B^T_n v_1]^2 \\\\\n&\\leq (1 + r)^4 \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2 \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&E[v^T_1 B_n B^T_n v_1]^2 \\leq (1 - 2r - s)^2 \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2 \\zeta_{k,i} + 4\\lambda^2\\right)\n\\end{align*}\n$$\n\nBy Lemma S.13, we have that $$2r + s \\leq 1$$ (S.63)\n\nThen, using\n\n$$\n\\begin{align*}\n&(1 - \\frac{1}{x})^2 \\leq 1 + 6x \\text{ for } x \\in [0, \\frac{1}{2}] \\\\\n&(1 + x)^4 \\leq 1 + 5x \\text{ for } x \\in [0, \\frac{1}{10}]\n\\end{align*}\n$$\n\n$$\n\\begin{align*}\n&E[v^T_1 B_n B^T_n v_1]^2 \\leq (1 + 5r)(1 + 12r + 6s) \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&\\leq 1 + 17r + 6s + 60r^2 + 30rs \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\\\\n&\\leq (1 + 22r + 12s) \\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\text{ since } r \\leq \\frac{1}{10}\n\\end{align*}\n$$\n\nBy Lemma S.12-(3), we have that $$\\exp\\left(\\sum_{i=1}^{n-kn} \\eta^2_i \\zeta_{k,i} + 4\\lambda^2\\right) \\leq 1 + \\delta m$$ (S.64)\n\nBy S.59, we have that\n\n$$\n\\begin{align*}\n&12s \\leq 48(1 + r)^2 \\frac{1 + \\delta}{100m} \\log\\left(1 + \\delta\\right) \\\\\n&\\leq 3\\frac{1 + \\delta}{5m} \\log\\left(1 + \\delta\\right) \\text{ since } r \\leq \\frac{1}{10} \\text{ (S.65)}\n\\end{align*}\n$$\n\nBy Lemma S.13, we have that $$r = 2(1 + \\epsilon) \\eta nkn (M + \\lambda_1) < \\frac{1}{\\delta/m}$$ (S.66)\n\n$$\\frac{50}{1 + \\delta/m}$$"}]}, {"page": 45, "text": "Then,\n           E     vT1 BnBT    n v1   2    \u2264   (1 + 22r + 12s)           1 + \u03b4\n             E   vT1 BnBT    n v1   2    = 1 + \u03b4   m + 22r         1 + \u03b4  m   m  + 12s       1 + \u03b4 m\n                                         \u2264   1 + \u03b4 m + 22       \u03b4            1 + \u03b4      3   log     1 + \u03b4\n                                                           50   m + 3   5           m                     m\n                                         \u2264   1 + \u03b4 m + 22       \u03b4                   1 + \u03b4         since \u03b4 \u2264      1, m = 200\n                                                           50   m + 7   10 log             m\n                                         \u2264   1 + \u03b4 m + 22       \u03b4            \u03b4\nThen setting m = 200, from S.62 we have  \u2264   1 + 3 \u03b4 m     50   m + 7   10  m since \u2200x, log (1 + x) \u2264               x\nvT1 BnBT   n v1 \u2265     exp    n\u2212kn     2\u03b7i\u03bb1 \u2212      4\u03b72i \u03bb21    (1 \u2212    2r \u2212   s)     1 \u2212       1     3\u03b4\n                                i=1                                                            \u03b4     m\n                  \u2265   exp    n\u2212kn     2\u03b7i\u03bb1 \u2212      4\u03b72i \u03bb21       1 \u2212    1      \u03b4/m                   1 + \u03b4      2   log    1 + \u03b4     1 \u2212   3\n                                                                        25   1 + \u03b4/m \u2212          1\n                                i=1                                                            20           m                      m       m\n                  \u2265   5         n\u2212kn     2\u03b7i\u03bb1 \u2212      4\u03b72    1     since \u03b4 \u2264      1 and m = 200\n                      6 exp        i=1                    i \u03bb2\nThe second inequality uses Eqs S.65, S.66.\nS.5.1.3      Fraction\nNow that we have established this result let\u2019s calculate the fraction. Let the step-sizes be set according\nto Lemma S.12. Define\n                    S := exp             n    Vk,i\u03b72  i\u2212ki +    n\u2212kn    4\u03bb2 1\u03b72i\n                                     i=u+1                        i=1\n                                     \uf8eb        \uf8eb    u                 n          \uf8f6\uf8f6\n                    Qu := exp        \uf8ed2\u03bb1     \uf8ed  j=1   \u03b7j \u2212   j=n\u2212kn+1       \u03b7j \uf8f8\uf8f8\n                    Rk,t :=      exp     t\u2212u j=1 \u03b72  j  Vk,j \u2212     Vk,j+uu       exp     2\u03bb1    n  j=n\u2212kn+1 \u03b7j\nThen, recall that                                         exp     2\u03bb1       j=1 \u03b7t\u2212u+j\n                   u := min {i : i \u2208         [n], i \u2212   ki \u2265    0}\n                \u03bek,t := 6\u03b7t\u2212ktM            1 + 3k2   t+1 (M + \u03bb1)2\n                  V\u2032 :=     1 + (3 + 4\u03f5) |\u03bb2 (P) |               V\n                                     1 \u2212   |\u03bb2 (P)|\n               Vk,t := V\u2032 + \u03bb2       1 + \u03bek,t\n                                 \uf8eb          u                            t\u2212u                              \uf8f6\n               C\u2032 k,t := exp     \uf8ed2\u03bb1     j=1   (\u03b7j \u2212    \u03b7t\u2212u+j) +  45   j=1  \u03b72j   Vk,j \u2212     Vk,j+u     \uf8f8   = QuRk,t", "md": "Then,\n\n$$E[vT1 BnBT n v1^2] \\leq (1 + 22r + 12s) \\frac{1 + \\delta}{1 + \\delta}$$\n\n$$E[vT1 BnBT n v1^2] = 1 + \\delta m + 22r \\frac{1 + \\delta m}{1 + \\delta m} + 12s \\frac{1 + \\delta m}{1 + \\delta m}$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 3m^5} \\log(1 + \\delta)$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 7m^{10} \\log m}$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 7m^{10} \\log m} \\leq 1 + 3 \\delta m \\frac{50m + 7m^{10}}{m} \\text{ since } \\forall x, \\log(1 + x) \\leq x$$\n\n$$vT1 BnBT n v1 \\geq \\exp\\left(n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda^2_1 (1 - 2r - s) \\frac{1 - 1}{3\\delta}\\right)$$\n\n$$\\geq \\exp\\left(n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda^2_1 \\frac{1 - 1}{\\delta m} \\frac{1 + \\delta}{2 \\log(1 + \\delta) 1 - 3}\\frac{25}{1 + \\delta/m - 1}\\right)$$\n\n$$\\geq 5^{n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i 1} \\text{ since } \\delta \\leq 1 \\text{ and } m = 200$$\n\nThe second inequality uses Eqs S.65, S.66.\n\nS.5.1.3 Fraction\n\nNow that we have established this result let\u2019s calculate the fraction. Let the step-sizes be set according to Lemma S.12. Define\n\n$$S := \\exp\\left(n Vk,i\\eta^2_i - ki + n-kn 4\\lambda^2_1\\eta^2_i\\right)_{i=u+1}^{j=1}$$\n\n$$Qu := \\exp\\left(2\\lambda^1 u \\left(\\sum_{j=1}^{u} \\eta_j - \\sum_{j=n-kn+1}^{n} \\eta_j\\right)\\right)$$\n\n$$Rk,t := \\exp\\left(t-u \\sum_{j=1}^{t} \\eta^2_j Vk,j - Vk,j+u \\exp\\left(2\\lambda^1 n \\sum_{j=n-kn+1}^{n} \\eta_j\\right)\\right)$$\n\nThen, recall that\n\n$$u := \\min \\{i : i \\in [n], i - ki \\geq 0\\}$$\n\n$$\\xi_{k,t} := 6\\eta_{t-kt}M \\frac{1 + 3k^2 t+1 (M + \\lambda^1)^2}{1 - |\u03bb2 (P)|}$$\n\n$$V' := 1 + (3 + 4\\epsilon) |\\lambda^2 (P)| \\frac{1}{1 - |\u03bb2 (P)|}$$\n\n$$Vk,t := V' + \\lambda^2 \\frac{1 + \\xi_{k,t}}{1 + \\xi_{k,t}}$$\n\n$$C'_{k,t} := \\exp\\left(2\\lambda^1 \\sum_{j=1}^{u} (\\eta_j - \\eta_{t-u+j}) + 45 \\sum_{j=1}^{t} \\eta^2_j Vk,j - Vk,j+u\\right) = QuRk,t$$", "images": [], "items": [{"type": "text", "value": "Then,\n\n$$E[vT1 BnBT n v1^2] \\leq (1 + 22r + 12s) \\frac{1 + \\delta}{1 + \\delta}$$\n\n$$E[vT1 BnBT n v1^2] = 1 + \\delta m + 22r \\frac{1 + \\delta m}{1 + \\delta m} + 12s \\frac{1 + \\delta m}{1 + \\delta m}$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 3m^5} \\log(1 + \\delta)$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 7m^{10} \\log m}$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 7m^{10} \\log m} \\leq 1 + 3 \\delta m \\frac{50m + 7m^{10}}{m} \\text{ since } \\forall x, \\log(1 + x) \\leq x$$\n\n$$vT1 BnBT n v1 \\geq \\exp\\left(n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda^2_1 (1 - 2r - s) \\frac{1 - 1}{3\\delta}\\right)$$\n\n$$\\geq \\exp\\left(n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda^2_1 \\frac{1 - 1}{\\delta m} \\frac{1 + \\delta}{2 \\log(1 + \\delta) 1 - 3}\\frac{25}{1 + \\delta/m - 1}\\right)$$\n\n$$\\geq 5^{n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i 1} \\text{ since } \\delta \\leq 1 \\text{ and } m = 200$$\n\nThe second inequality uses Eqs S.65, S.66.\n\nS.5.1.3 Fraction\n\nNow that we have established this result let\u2019s calculate the fraction. Let the step-sizes be set according to Lemma S.12. Define\n\n$$S := \\exp\\left(n Vk,i\\eta^2_i - ki + n-kn 4\\lambda^2_1\\eta^2_i\\right)_{i=u+1}^{j=1}$$\n\n$$Qu := \\exp\\left(2\\lambda^1 u \\left(\\sum_{j=1}^{u} \\eta_j - \\sum_{j=n-kn+1}^{n} \\eta_j\\right)\\right)$$\n\n$$Rk,t := \\exp\\left(t-u \\sum_{j=1}^{t} \\eta^2_j Vk,j - Vk,j+u \\exp\\left(2\\lambda^1 n \\sum_{j=n-kn+1}^{n} \\eta_j\\right)\\right)$$\n\nThen, recall that\n\n$$u := \\min \\{i : i \\in [n], i - ki \\geq 0\\}$$\n\n$$\\xi_{k,t} := 6\\eta_{t-kt}M \\frac{1 + 3k^2 t+1 (M + \\lambda^1)^2}{1 - |\u03bb2 (P)|}$$\n\n$$V' := 1 + (3 + 4\\epsilon) |\\lambda^2 (P)| \\frac{1}{1 - |\u03bb2 (P)|}$$\n\n$$Vk,t := V' + \\lambda^2 \\frac{1 + \\xi_{k,t}}{1 + \\xi_{k,t}}$$\n\n$$C'_{k,t} := \\exp\\left(2\\lambda^1 \\sum_{j=1}^{u} (\\eta_j - \\eta_{t-u+j}) + 45 \\sum_{j=1}^{t} \\eta^2_j Vk,j - Vk,j+u\\right) = QuRk,t$$", "md": "Then,\n\n$$E[vT1 BnBT n v1^2] \\leq (1 + 22r + 12s) \\frac{1 + \\delta}{1 + \\delta}$$\n\n$$E[vT1 BnBT n v1^2] = 1 + \\delta m + 22r \\frac{1 + \\delta m}{1 + \\delta m} + 12s \\frac{1 + \\delta m}{1 + \\delta m}$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 3m^5} \\log(1 + \\delta)$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 7m^{10} \\log m}$$\n\n$$\\leq 1 + \\delta m + 22 \\delta \\frac{1 + \\delta}{50m + 7m^{10} \\log m} \\leq 1 + 3 \\delta m \\frac{50m + 7m^{10}}{m} \\text{ since } \\forall x, \\log(1 + x) \\leq x$$\n\n$$vT1 BnBT n v1 \\geq \\exp\\left(n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda^2_1 (1 - 2r - s) \\frac{1 - 1}{3\\delta}\\right)$$\n\n$$\\geq \\exp\\left(n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i \\lambda^2_1 \\frac{1 - 1}{\\delta m} \\frac{1 + \\delta}{2 \\log(1 + \\delta) 1 - 3}\\frac{25}{1 + \\delta/m - 1}\\right)$$\n\n$$\\geq 5^{n-kn 2\\eta_i\\lambda_1 - 4\\eta^2_i 1} \\text{ since } \\delta \\leq 1 \\text{ and } m = 200$$\n\nThe second inequality uses Eqs S.65, S.66.\n\nS.5.1.3 Fraction\n\nNow that we have established this result let\u2019s calculate the fraction. Let the step-sizes be set according to Lemma S.12. Define\n\n$$S := \\exp\\left(n Vk,i\\eta^2_i - ki + n-kn 4\\lambda^2_1\\eta^2_i\\right)_{i=u+1}^{j=1}$$\n\n$$Qu := \\exp\\left(2\\lambda^1 u \\left(\\sum_{j=1}^{u} \\eta_j - \\sum_{j=n-kn+1}^{n} \\eta_j\\right)\\right)$$\n\n$$Rk,t := \\exp\\left(t-u \\sum_{j=1}^{t} \\eta^2_j Vk,j - Vk,j+u \\exp\\left(2\\lambda^1 n \\sum_{j=n-kn+1}^{n} \\eta_j\\right)\\right)$$\n\nThen, recall that\n\n$$u := \\min \\{i : i \\in [n], i - ki \\geq 0\\}$$\n\n$$\\xi_{k,t} := 6\\eta_{t-kt}M \\frac{1 + 3k^2 t+1 (M + \\lambda^1)^2}{1 - |\u03bb2 (P)|}$$\n\n$$V' := 1 + (3 + 4\\epsilon) |\\lambda^2 (P)| \\frac{1}{1 - |\u03bb2 (P)|}$$\n\n$$Vk,t := V' + \\lambda^2 \\frac{1 + \\xi_{k,t}}{1 + \\xi_{k,t}}$$\n\n$$C'_{k,t} := \\exp\\left(2\\lambda^1 \\sum_{j=1}^{u} (\\eta_j - \\eta_{t-u+j}) + 45 \\sum_{j=1}^{t} \\eta^2_j Vk,j - Vk,j+u\\right) = QuRk,t$$"}]}, {"page": 46, "text": "Therefore,\nTr    V T         n V\u22a5\n        \u22a5  BnBT\n     vT1 BnBT   n v1\n          exp     n                                i\u2212ki     \uf8eb           n                                          \uf8eb      i                        \uf8f6\uf8f6\n                      i=u+1 2\u03b7i\u03bb2 + Vk,i\u03b72                  \uf8edd +                                                   \uf8ed                               \uf8f8\uf8f8\n \u2264   1.3             n\u2212kn                                                     (V\u2032 + \u03bek,i) C\u2032     k,i\u03b72i\u2212ki exp                 2\u03b7j (\u03bb1 \u2212      \u03bb2)\n      \u03b4      exp         i=1     2\u03b7i\u03bb1 \u2212     4\u03b72 i \u03bb21               i=u+1                                            j=u+1\n                            n                            \uf8eb           n                                          \uf8eb      i                        \uf8f6\uf8f6\n \u2264   1.3   S    exp               2\u03b7i (\u03bb2 \u2212     \u03bb1)      \uf8edd +              (V\u2032 + \u03bek,i) C\u2032     k,i\u03b72i\u2212ki exp     \uf8ed           2\u03b7j (\u03bb1 \u2212      \u03bb2)  \uf8f8\uf8f8\n      \u03b4   Qu             i=u+1                                    i=u+1                                            j=u+1\n             \uf8eb                                                                                                 \uf8eb                                  \uf8f6\uf8f7 \uf8f6\n             \uf8ec             n                                        n                                                    n                           \uf8f7\n             \uf8ec  d exp                                                                                                                                \uf8f7\n \u2264   1.3     \uf8ec                i=u+1 2\u03b7i (\u03bb2 \u2212         \u03bb1)     +           (V\u2032 + \u03bek,i) Rk,i\u03b72                   \uf8ed\u2212             2\u03b7j (\u03bb1 \u2212      \u03bb2)  \uf8f8  \uf8f7\n             \uf8ec                                                                                     i\u2212ki exp                                          \uf8f7\n      \u03b4 S    \uf8ec                      Qu                           i=u+1                                                j=i+1                          \uf8f8\n             \uf8ed                       X1                                                                  X2                         (S.67)\nFor X1, we have\n                       X1 \u2264     d exp      n  i=u+1 2\u03b7i (\u03bb2 \u2212         \u03bb1)\n                            =           d exp   u n Qui=u+1 2\u03b7i (\u03bb2 \u2212        \u03bb1)\n                                exp     2\u03bb1n        j=1 \u03b7j \u2212      n  j=n\u2212kn+1 \u03b7j\n                                               i=u+1 2\u03b7i (\u03bb2 \u2212        \u03bb1)\n                            \u2264    d exp             n\n                                exp     \u22122\u03bb1  n        j=n\u2212kn+1 \u03b7j               \uf8eb        \uf8eb         n          \uf8f6\uf8f6\nNote that                   \u2264   d exp      i=u+1    2\u03b7i (\u03bb2 \u2212      \u03bb1)     exp   \uf8ed2\u03bb1     \uf8ed  j=n\u2212kn+1       \u03b7j \uf8f8\uf8f8\n      \uf8eb              n           \uf8f6\nexp   \uf8ed2\u03bb1     j=n\u2212kn+1       \u03b7j \uf8f8   \u2264   exp (2 (1 + 2\u03f5) \u03bb1kn\u03b7n\u2212kn+1) using monotonicity of \u03b7i\n                                     \u2264   exp (4 (1 + 2\u03f5) \u03bb1kn\u03b7n) using slow-decay of \u03b7i\nTherefore, using S.31                \u2264   1 + 2 \u03b4 m using Lemma S.13 along with ex \u2264                      1 + x + x2 for x \u2208           (0, 1)\n     Next, for X2, we first have               X1 \u2264     d    1 + 2\u03b4 m       \u03b2 + un2\u03b1\n       Rk,t :=      exp     t\u2212uj=1 \u03b72  j   Vk,j \u2212    Vk,j+u u      exp      2\u03bb1    n j=n\u2212kn+1 \u03b7j\n                                             exp     2\u03bb1      j=1 \u03b7t\u2212u+j\n                        \uf8eb  t\u2212u            \uf8f6         \uf8eb               n          \uf8f6\n              \u2264   exp   \uf8ed  j=1   \u03b72j Vk,j \uf8f8   exp   \uf8ed2\u03bb1     j=n\u2212kn+1       \u03b7j \uf8f8\n              \u2264      1 + 2\u03b4m    2    using Lemmas S.12 \u2212             (3), S.13 and ex \u2264          1 + x + x2 for x \u2208           (0, 1)\n                                                                    46", "md": "Therefore,\n\n$$\n\\begin{array}{ll}\n\\text{Tr} & V^T n V^\\perp \\\\\n& V^\\perp B_n B^T \\\\\n& v^T_1 B_n B^T n v_1 \\\\\n& \\exp \\left( \\sum_{i=u+1}^{n} \\frac{2\\eta_i\\lambda_2 + V_{k,i}\\eta^2}{d + \\sum_{i}^{n} \\frac{i-k_i}{\\left( V' + \\xi_{k,i} \\right) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2)}} \\right) \\\\\n& \\leq 1.3^{n-k_n} (V' + \\xi_{k,i}) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2) \\\\\n& \\leq 1.3^\\delta \\exp \\left( \\sum_{i=1}^{n} \\frac{2\\eta_i(\\lambda_2 - \\lambda_1)}{d + (V' + \\xi_{k,i}) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2)} \\right) \\\\\n& \\leq 1.3^\\delta S \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{i=u+1} \\right) \\\\\n& \\leq 1.3^\\delta S \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1) + (V' + \\xi_{k,i}) R_{k,i}\\eta^2}{-2\\eta_j (\\lambda_1 - \\lambda_2)} \\right)\n\\end{array}\n$$\n\nFor X1, we have\n\n$$\n\\begin{array}{ll}\nX1 & \\leq d \\exp \\left( \\sum_{i=u+1}^{n} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1n} \\right) \\\\\n& = d \\exp \\left( u \\sum_{i=u+1}^{n} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1n} \\right) \\\\\n& \\leq d \\exp \\left( -2\\lambda_1n \\sum_{j=1}^{n} \\eta_j - n \\sum_{j=n-k_n+1}^{n} \\eta_j \\right) \\\\\n& \\leq d \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1} \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\right) \\\\\n& \\leq \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\leq \\exp \\left( 2(1 + 2\\epsilon) \\lambda_1kn\\eta_{n-k_n+1} \\right) \\text{ using monotonicity of } \\eta_i \\\\\n& \\leq \\exp \\left( 4(1 + 2\\epsilon) \\lambda_1kn\\eta_n \\right) \\text{ using slow-decay of } \\eta_i \\\\\n& \\leq 1 + 2\\delta m \\text{ using Lemma S.13 along with } \\exp \\leq 1 + x + x^2 \\text{ for } x \\in (0, 1)\n\\end{array}\n$$\n\nNext, for X2, we first have\n\n$$\n\\begin{array}{ll}\nX1 & \\leq d (1 + 2\\delta m) \\beta + un^2\\alpha \\\\\nR_{k,t} & := \\exp \\left( t-u \\sum_{j=1}^{t} \\eta^2_j V_{k,j} - V_{k,j+u}^u \\exp 2\\lambda_1n \\sum_{j=n-k_n+1} \\eta_j \\right) \\\\\n& \\leq \\exp \\left( \\sum_{j=1}^{t-u} \\eta^2_j V_{k,j} \\right) \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\\\\n& \\leq 1 + 2\\delta m^2 \\text{ using Lemmas S.12 - (3), S.13 and } \\exp \\leq 1 + x + x^2 \\text{ for } x \\in (0, 1)\n\\end{array}\n$$", "images": [], "items": [{"type": "text", "value": "Therefore,\n\n$$\n\\begin{array}{ll}\n\\text{Tr} & V^T n V^\\perp \\\\\n& V^\\perp B_n B^T \\\\\n& v^T_1 B_n B^T n v_1 \\\\\n& \\exp \\left( \\sum_{i=u+1}^{n} \\frac{2\\eta_i\\lambda_2 + V_{k,i}\\eta^2}{d + \\sum_{i}^{n} \\frac{i-k_i}{\\left( V' + \\xi_{k,i} \\right) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2)}} \\right) \\\\\n& \\leq 1.3^{n-k_n} (V' + \\xi_{k,i}) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2) \\\\\n& \\leq 1.3^\\delta \\exp \\left( \\sum_{i=1}^{n} \\frac{2\\eta_i(\\lambda_2 - \\lambda_1)}{d + (V' + \\xi_{k,i}) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2)} \\right) \\\\\n& \\leq 1.3^\\delta S \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{i=u+1} \\right) \\\\\n& \\leq 1.3^\\delta S \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1) + (V' + \\xi_{k,i}) R_{k,i}\\eta^2}{-2\\eta_j (\\lambda_1 - \\lambda_2)} \\right)\n\\end{array}\n$$\n\nFor X1, we have\n\n$$\n\\begin{array}{ll}\nX1 & \\leq d \\exp \\left( \\sum_{i=u+1}^{n} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1n} \\right) \\\\\n& = d \\exp \\left( u \\sum_{i=u+1}^{n} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1n} \\right) \\\\\n& \\leq d \\exp \\left( -2\\lambda_1n \\sum_{j=1}^{n} \\eta_j - n \\sum_{j=n-k_n+1}^{n} \\eta_j \\right) \\\\\n& \\leq d \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1} \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\right) \\\\\n& \\leq \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\leq \\exp \\left( 2(1 + 2\\epsilon) \\lambda_1kn\\eta_{n-k_n+1} \\right) \\text{ using monotonicity of } \\eta_i \\\\\n& \\leq \\exp \\left( 4(1 + 2\\epsilon) \\lambda_1kn\\eta_n \\right) \\text{ using slow-decay of } \\eta_i \\\\\n& \\leq 1 + 2\\delta m \\text{ using Lemma S.13 along with } \\exp \\leq 1 + x + x^2 \\text{ for } x \\in (0, 1)\n\\end{array}\n$$\n\nNext, for X2, we first have\n\n$$\n\\begin{array}{ll}\nX1 & \\leq d (1 + 2\\delta m) \\beta + un^2\\alpha \\\\\nR_{k,t} & := \\exp \\left( t-u \\sum_{j=1}^{t} \\eta^2_j V_{k,j} - V_{k,j+u}^u \\exp 2\\lambda_1n \\sum_{j=n-k_n+1} \\eta_j \\right) \\\\\n& \\leq \\exp \\left( \\sum_{j=1}^{t-u} \\eta^2_j V_{k,j} \\right) \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\\\\n& \\leq 1 + 2\\delta m^2 \\text{ using Lemmas S.12 - (3), S.13 and } \\exp \\leq 1 + x + x^2 \\text{ for } x \\in (0, 1)\n\\end{array}\n$$", "md": "Therefore,\n\n$$\n\\begin{array}{ll}\n\\text{Tr} & V^T n V^\\perp \\\\\n& V^\\perp B_n B^T \\\\\n& v^T_1 B_n B^T n v_1 \\\\\n& \\exp \\left( \\sum_{i=u+1}^{n} \\frac{2\\eta_i\\lambda_2 + V_{k,i}\\eta^2}{d + \\sum_{i}^{n} \\frac{i-k_i}{\\left( V' + \\xi_{k,i} \\right) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2)}} \\right) \\\\\n& \\leq 1.3^{n-k_n} (V' + \\xi_{k,i}) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2) \\\\\n& \\leq 1.3^\\delta \\exp \\left( \\sum_{i=1}^{n} \\frac{2\\eta_i(\\lambda_2 - \\lambda_1)}{d + (V' + \\xi_{k,i}) C'_{k,i}\\eta^2_i-k_i \\exp 2\\eta_j (\\lambda_1 - \\lambda_2)} \\right) \\\\\n& \\leq 1.3^\\delta S \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{i=u+1} \\right) \\\\\n& \\leq 1.3^\\delta S \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1) + (V' + \\xi_{k,i}) R_{k,i}\\eta^2}{-2\\eta_j (\\lambda_1 - \\lambda_2)} \\right)\n\\end{array}\n$$\n\nFor X1, we have\n\n$$\n\\begin{array}{ll}\nX1 & \\leq d \\exp \\left( \\sum_{i=u+1}^{n} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1n} \\right) \\\\\n& = d \\exp \\left( u \\sum_{i=u+1}^{n} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1n} \\right) \\\\\n& \\leq d \\exp \\left( -2\\lambda_1n \\sum_{j=1}^{n} \\eta_j - n \\sum_{j=n-k_n+1}^{n} \\eta_j \\right) \\\\\n& \\leq d \\exp \\left( \\sum_{i=u+1} \\frac{2\\eta_i (\\lambda_2 - \\lambda_1)}{2\\lambda_1} \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\right) \\\\\n& \\leq \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\leq \\exp \\left( 2(1 + 2\\epsilon) \\lambda_1kn\\eta_{n-k_n+1} \\right) \\text{ using monotonicity of } \\eta_i \\\\\n& \\leq \\exp \\left( 4(1 + 2\\epsilon) \\lambda_1kn\\eta_n \\right) \\text{ using slow-decay of } \\eta_i \\\\\n& \\leq 1 + 2\\delta m \\text{ using Lemma S.13 along with } \\exp \\leq 1 + x + x^2 \\text{ for } x \\in (0, 1)\n\\end{array}\n$$\n\nNext, for X2, we first have\n\n$$\n\\begin{array}{ll}\nX1 & \\leq d (1 + 2\\delta m) \\beta + un^2\\alpha \\\\\nR_{k,t} & := \\exp \\left( t-u \\sum_{j=1}^{t} \\eta^2_j V_{k,j} - V_{k,j+u}^u \\exp 2\\lambda_1n \\sum_{j=n-k_n+1} \\eta_j \\right) \\\\\n& \\leq \\exp \\left( \\sum_{j=1}^{t-u} \\eta^2_j V_{k,j} \\right) \\exp \\left( 2\\lambda_1 \\sum_{j=n-k_n+1} \\eta_j \\right) \\\\\n& \\leq 1 + 2\\delta m^2 \\text{ using Lemmas S.12 - (3), S.13 and } \\exp \\leq 1 + x + x^2 \\text{ for } x \\in (0, 1)\n\\end{array}\n$$"}]}, {"page": 47, "text": "Now, using S.12-(4) we have,\n  n                        \uf8eb        n                         \uf8f6\n i=1  Vk,i\u03b72  i\u2212ki exp     \uf8ed\u2212    j=i+1    2\u03b7j (\u03bb1 \u2212      \u03bb2)  \uf8f8    \u2264                                                         (\u03b2+n)(\u03bb1\u2212\u03bb2)\n   2 (1 + 10\u03f5) \u03b12                    V\u2032         1       800 (1 + 10\u03f5) \u03b13               M (M + \u03bb1)2 \u03c4 2         mix  log2             \u03b1\nThen,      2\u03b1 \u2212    1           (\u03bb1 \u2212    \u03bb2)2    n +             (\u03b1 \u2212     1)                 (\u03bb1 \u2212     \u03bb2)3                       n2\n                            \uf8ee  2 (1 + 10\u03f5) \u03b12                                                                                               \uf8f9\n                        2   \uf8ef                                    V\u2032         1       24 (1 + 10\u03f5) \u03b13              M (M + \u03bb1)2            k2  \uf8fa\n                            \uf8ef                                                                                                             n \uf8fa\nX2 \u2264         1 + 2\u03b4         \uf8ef                                                                                                               \uf8fa\n                   m        \uf8f0          2\u03b1 \u2212    1           (\u03bb1 \u2212    \u03bb2)2    n +             (\u03b1 \u2212    1)              (\u03bb1 \u2212    \u03bb2)3       n2  \uf8fb\n                                          C1                                                    C2\nTherefore substituting in S.67,\nTr    V T          n V\u22a5                                2         \u03b2 + u 2\u03b1                  C1V\u2032         1                                  k2\n        \u22a5  BnBT              \u2264   1.3S       1 + 2\u03b4           d                     +                                                        n\n     vT                             \u03b4              m                  n                                 n + C2M (M + \u03bb1)2                  n2\n       1 BnBT    n v1                                                                  (\u03bb1 \u2212     \u03bb2)2                (\u03bb1 \u2212    \u03bb2)3    (S.68)\nProof of Theorem 1. To complete our proof, we bound S to simplify S.68. We note that under the\nlearning rate schedule presented in Lemma S.12-(3),\nTherefore,                                                  S \u2264       1 + \u03b4  m\nTr    V T          n V\u22a5                              3         \u03b2 + u 2\u03b1                  C1V\u2032         1                                 k2\n        \u22a5   BnBT             \u2264   1.3      1 + 2\u03b4           d                     +                                                        n\n     vT                            \u03b4            m                   n                                 n + C2M (M + \u03bb1)2                 n2\n       1 BnBT    n v1                                                                (\u03bb1 \u2212    \u03bb2)2                (\u03bb1 \u2212     \u03bb2)3\n                             \u2264   1.4     d   \u03b2 + u 2\u03b1          +       C1V\u2032         1                                 k2n\n                                   \u03b4              n                                 n + C2M (M + \u03bb1)2                 n2\n                                                                   (\u03bb1 \u2212    \u03bb2)2                (\u03bb1 \u2212     \u03bb2)3\nUsing lemma S.14, we have that u \u2264                   \u03b2. Then, using Lemma 3.1 from [15] completes our proof.\nS.5.2        Proof of Corollary 1\nProof of Corollary 1. We note that the downsampled data stream can be considered to be drawn from\na Markov chain with transition kernel P k (., .) since each data-point is k steps away from the previous\none. We will denote the parameters of this transformed chain by \u02dc                          y when the corresponding parameter\nis y under the original chain. For example, \u02dc                 \u03c4mix is the mixing time of the new chain.\n     Note that this modified transition matrix has the same stationary distribution \u03c0. It is also\nreversible. This can be seen by considering the diagonal matrix of stationary distribution probabilities\n\u03a0, where \u03a0ii = \u03c0i. For a reversible Markov Chain, we have \u03a0P = P                                             \u03a0. However, that also\nimplies \u03a0P 2 = (\u03a0P            )P = (P      \u03a0)P = P        (\u03a0P    ) = P 2\u03a0. This same technique works for P k yielding\n\u03a0P k = P k\u03a0.\n     Using standard results on Markov chains [19],\n                           |\u03bb2 (P    ) |             1      \u2264  \u03c4mix(\u03f5) \u2264               1                      1       ,               (S.69)\n                        1 \u2212   |\u03bb2 (P    ) | log     2\u03f5                         1 \u2212   |\u03bb2 (P   ) | log      \u03f5\u03c0min\nwhere \u03c0min := mini \u03c0i. Therefore, as noted in the theorem statement, we substitute the modified\nparameters in the bound we have proven for Theorem 1.\n                                                                      47", "md": "Now, using $$S.12-(4)$$ we have,\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{n} V_{k,i}\\eta^2_{i-k} \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\le (\\beta+n)(\\lambda_1-\\lambda_2) \\\\\n&\\frac{2(1 + 10\\epsilon)\\alpha^2}{V'} \\le \\frac{1}{800}(1 + 10\\epsilon)\\alpha^3 \\frac{M(M + \\lambda_1)^2\\tau^2}{\\text{mix}\\log^2\\alpha}\n\\end{align*}\n$$\n\nThen,\n\n$$\n\\begin{align*}\nX^2 &\\le 1 + 2\\delta \\frac{2\\alpha - 1}{(\\lambda_1 - \\lambda_2)^2 n + (\\alpha - 1)(\\lambda_1 - \\lambda_2)^3 n^2} \\\\\n&\\le \\frac{1 + 2\\delta m}{C_1V' + \\frac{1}{24}(1 + 10\\epsilon)\\alpha^3 \\frac{M(M + \\lambda_1)^2}{k^2}n}\n\\end{align*}\n$$\n\nTherefore substituting in $$S.67$$,\n\n$$\n\\begin{align*}\n\\text{Tr}\\left(V^TnV^\\perp BnBT\\right) &\\le 1.3S\\frac{1 + 2\\delta d}{\\delta m n + C_2M(M + \\lambda_1)^2 n^2} \\\\\n&\\le \\frac{1.3}{1 + 2\\delta}\\frac{2\\alpha}{\\beta + u}\\frac{C_1V'}{k^2}n\n\\end{align*}\n$$\n\nProof of Theorem 1. To complete our proof, we bound S to simplify $$S.68$$. We note that under the learning rate schedule presented in Lemma $$S.12-(3)$$,\n\nTherefore,\n\n$$\nS \\le 1 + \\delta \\frac{3\\beta + u}{2\\alpha}\\frac{C_1V'}{k^2n}\n$$\n\nUsing lemma $$S.14$$, we have that $$u \\le \\beta$$. Then, using Lemma 3.1 from [15] completes our proof.\n\n$$S.5.2$$ Proof of Corollary 1\n\nProof of Corollary 1. We note that the downsampled data stream can be considered to be drawn from a Markov chain with transition kernel $$P^k(., .)$$ since each data-point is $$k$$ steps away from the previous one. We will denote the parameters of this transformed chain by $$\\tilde{y}$$ when the corresponding parameter is $$y$$ under the original chain. For example, $$\\tilde{\\tau}_{\\text{mix}}$$ is the mixing time of the new chain.\n\nNote that this modified transition matrix has the same stationary distribution $$\\pi$$. It is also reversible. This can be seen by considering the diagonal matrix of stationary distribution probabilities $$\\Pi$$, where $$\\Pi_{ii} = \\pi_i$$. For a reversible Markov Chain, we have $$\\Pi P = P \\Pi$$. However, that also implies $$\\Pi P^2 = (\\Pi P)P = (P \\Pi)P = P (\\Pi P) = P^2 \\Pi$$. This same technique works for $$P^k$$ yielding $$\\Pi P^k = P^k \\Pi$$.\n\nUsing standard results on Markov chains [19],\n\n$$\n\\left|\\lambda_2(P)\\right| \\le \\frac{1}{1 - \\left|\\lambda_2(P)\\right| \\log_2\\epsilon} \\le \\tau_{\\text{mix}(\\epsilon)} \\le \\frac{1}{1 - \\left|\\lambda_2(P)\\right| \\log\\epsilon\\pi_{\\text{min}}}\n$$\n\nwhere $$\\pi_{\\text{min}} := \\min_i \\pi_i$$. Therefore, as noted in the theorem statement, we substitute the modified parameters in the bound we have proven for Theorem 1.\n\n47", "images": [], "items": [{"type": "text", "value": "Now, using $$S.12-(4)$$ we have,\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{n} V_{k,i}\\eta^2_{i-k} \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\le (\\beta+n)(\\lambda_1-\\lambda_2) \\\\\n&\\frac{2(1 + 10\\epsilon)\\alpha^2}{V'} \\le \\frac{1}{800}(1 + 10\\epsilon)\\alpha^3 \\frac{M(M + \\lambda_1)^2\\tau^2}{\\text{mix}\\log^2\\alpha}\n\\end{align*}\n$$\n\nThen,\n\n$$\n\\begin{align*}\nX^2 &\\le 1 + 2\\delta \\frac{2\\alpha - 1}{(\\lambda_1 - \\lambda_2)^2 n + (\\alpha - 1)(\\lambda_1 - \\lambda_2)^3 n^2} \\\\\n&\\le \\frac{1 + 2\\delta m}{C_1V' + \\frac{1}{24}(1 + 10\\epsilon)\\alpha^3 \\frac{M(M + \\lambda_1)^2}{k^2}n}\n\\end{align*}\n$$\n\nTherefore substituting in $$S.67$$,\n\n$$\n\\begin{align*}\n\\text{Tr}\\left(V^TnV^\\perp BnBT\\right) &\\le 1.3S\\frac{1 + 2\\delta d}{\\delta m n + C_2M(M + \\lambda_1)^2 n^2} \\\\\n&\\le \\frac{1.3}{1 + 2\\delta}\\frac{2\\alpha}{\\beta + u}\\frac{C_1V'}{k^2}n\n\\end{align*}\n$$\n\nProof of Theorem 1. To complete our proof, we bound S to simplify $$S.68$$. We note that under the learning rate schedule presented in Lemma $$S.12-(3)$$,\n\nTherefore,\n\n$$\nS \\le 1 + \\delta \\frac{3\\beta + u}{2\\alpha}\\frac{C_1V'}{k^2n}\n$$\n\nUsing lemma $$S.14$$, we have that $$u \\le \\beta$$. Then, using Lemma 3.1 from [15] completes our proof.\n\n$$S.5.2$$ Proof of Corollary 1\n\nProof of Corollary 1. We note that the downsampled data stream can be considered to be drawn from a Markov chain with transition kernel $$P^k(., .)$$ since each data-point is $$k$$ steps away from the previous one. We will denote the parameters of this transformed chain by $$\\tilde{y}$$ when the corresponding parameter is $$y$$ under the original chain. For example, $$\\tilde{\\tau}_{\\text{mix}}$$ is the mixing time of the new chain.\n\nNote that this modified transition matrix has the same stationary distribution $$\\pi$$. It is also reversible. This can be seen by considering the diagonal matrix of stationary distribution probabilities $$\\Pi$$, where $$\\Pi_{ii} = \\pi_i$$. For a reversible Markov Chain, we have $$\\Pi P = P \\Pi$$. However, that also implies $$\\Pi P^2 = (\\Pi P)P = (P \\Pi)P = P (\\Pi P) = P^2 \\Pi$$. This same technique works for $$P^k$$ yielding $$\\Pi P^k = P^k \\Pi$$.\n\nUsing standard results on Markov chains [19],\n\n$$\n\\left|\\lambda_2(P)\\right| \\le \\frac{1}{1 - \\left|\\lambda_2(P)\\right| \\log_2\\epsilon} \\le \\tau_{\\text{mix}(\\epsilon)} \\le \\frac{1}{1 - \\left|\\lambda_2(P)\\right| \\log\\epsilon\\pi_{\\text{min}}}\n$$\n\nwhere $$\\pi_{\\text{min}} := \\min_i \\pi_i$$. Therefore, as noted in the theorem statement, we substitute the modified parameters in the bound we have proven for Theorem 1.\n\n47", "md": "Now, using $$S.12-(4)$$ we have,\n\n$$\n\\begin{align*}\n&\\sum_{i=1}^{n} V_{k,i}\\eta^2_{i-k} \\exp\\left(-\\sum_{j=i+1}^{n} 2\\eta_j (\\lambda_1 - \\lambda_2)\\right) \\le (\\beta+n)(\\lambda_1-\\lambda_2) \\\\\n&\\frac{2(1 + 10\\epsilon)\\alpha^2}{V'} \\le \\frac{1}{800}(1 + 10\\epsilon)\\alpha^3 \\frac{M(M + \\lambda_1)^2\\tau^2}{\\text{mix}\\log^2\\alpha}\n\\end{align*}\n$$\n\nThen,\n\n$$\n\\begin{align*}\nX^2 &\\le 1 + 2\\delta \\frac{2\\alpha - 1}{(\\lambda_1 - \\lambda_2)^2 n + (\\alpha - 1)(\\lambda_1 - \\lambda_2)^3 n^2} \\\\\n&\\le \\frac{1 + 2\\delta m}{C_1V' + \\frac{1}{24}(1 + 10\\epsilon)\\alpha^3 \\frac{M(M + \\lambda_1)^2}{k^2}n}\n\\end{align*}\n$$\n\nTherefore substituting in $$S.67$$,\n\n$$\n\\begin{align*}\n\\text{Tr}\\left(V^TnV^\\perp BnBT\\right) &\\le 1.3S\\frac{1 + 2\\delta d}{\\delta m n + C_2M(M + \\lambda_1)^2 n^2} \\\\\n&\\le \\frac{1.3}{1 + 2\\delta}\\frac{2\\alpha}{\\beta + u}\\frac{C_1V'}{k^2}n\n\\end{align*}\n$$\n\nProof of Theorem 1. To complete our proof, we bound S to simplify $$S.68$$. We note that under the learning rate schedule presented in Lemma $$S.12-(3)$$,\n\nTherefore,\n\n$$\nS \\le 1 + \\delta \\frac{3\\beta + u}{2\\alpha}\\frac{C_1V'}{k^2n}\n$$\n\nUsing lemma $$S.14$$, we have that $$u \\le \\beta$$. Then, using Lemma 3.1 from [15] completes our proof.\n\n$$S.5.2$$ Proof of Corollary 1\n\nProof of Corollary 1. We note that the downsampled data stream can be considered to be drawn from a Markov chain with transition kernel $$P^k(., .)$$ since each data-point is $$k$$ steps away from the previous one. We will denote the parameters of this transformed chain by $$\\tilde{y}$$ when the corresponding parameter is $$y$$ under the original chain. For example, $$\\tilde{\\tau}_{\\text{mix}}$$ is the mixing time of the new chain.\n\nNote that this modified transition matrix has the same stationary distribution $$\\pi$$. It is also reversible. This can be seen by considering the diagonal matrix of stationary distribution probabilities $$\\Pi$$, where $$\\Pi_{ii} = \\pi_i$$. For a reversible Markov Chain, we have $$\\Pi P = P \\Pi$$. However, that also implies $$\\Pi P^2 = (\\Pi P)P = (P \\Pi)P = P (\\Pi P) = P^2 \\Pi$$. This same technique works for $$P^k$$ yielding $$\\Pi P^k = P^k \\Pi$$.\n\nUsing standard results on Markov chains [19],\n\n$$\n\\left|\\lambda_2(P)\\right| \\le \\frac{1}{1 - \\left|\\lambda_2(P)\\right| \\log_2\\epsilon} \\le \\tau_{\\text{mix}(\\epsilon)} \\le \\frac{1}{1 - \\left|\\lambda_2(P)\\right| \\log\\epsilon\\pi_{\\text{min}}}\n$$\n\nwhere $$\\pi_{\\text{min}} := \\min_i \\pi_i$$. Therefore, as noted in the theorem statement, we substitute the modified parameters in the bound we have proven for Theorem 1.\n\n47"}]}, {"page": 48, "text": "       First, we will show that the mixing time for this new chain is \u0398(1). We will use k := \u03c4mix                                                       \u03b72n   .\n So by definition the dmix(k) \u2264                   \u03b72n using the definition of dmix in Section 2.1. Hence dmix(k) \u2264                                       1/4\n using conditions on the learning rate schedule imposed in Theorem 1. Therefore, in the transformed\n chain, the \u201cnew\u201d \u02dc         \u03c4mix is \u0398(1).\n      We also have:\n                              k \u2264       2\u03c4mix                1      \u2264    2 log(4/\u03c0min)                   1\n                                      log (2) log           \u03b72n                 log 2           1 \u2212    |\u03bb2 (P     )| log (n)\n                                                                                  C\nWe see that C > 1. Next, we note that for the transition kernel P k (., .), the second-largest absolute     1\n eigenvalue is given as |\u03bb2 (P                 )|k. Consider the function f (x) := x                      1\u2212x for x \u2208         (0, 1). Then,\n                                              f \u2032 (x) = f (x)             1 \u2212    x \u2212    x log (x)          > 0\nTherefore, f (x) < limx\u21921 f (x) = 1                                            x (1 \u2212      x)2                  \u2264     1 C log(n) < 1\n follows if C > 1, n > 3, which is true. Therefore,         e < 1. which implies |\u03bb2 (P                   )|k (i)      e                     e . Here (i)\n                                              V\u2032 :=         1 + (3 + 4\u03f5) |\u03bb2 (P            ) |k      V \u2264     5V\n                                                                   1 \u2212    |\u03bb2 (P    )|k\nThis also implies that the mixing time for the new Markov chain for sub-sampled data is \u0398 (1). The\n bound then follows by substituting n to be n                          k = nk = \u0398               C\u03c4  mixnlog(n)       and setting the \u03c4mix in the\n original expression of Theorem 1 to a constant.\n S.6          Additional Experiments\n In this section, we provide additional experiments to support the results established in Section 3 of the\n manuscript. We present experiments with distributions that have nonzero mean vectors at each state,\n but zero mean with respect to the stationary distribution. This means that the Zi\u2019s are not necessarily\n zero-mean with respect to each state distribution D (s). To normalize the data-points, we estimate the\n mean \u00b5 and covariance matrix \u03a3 empirically from a much larger independently generated dataset.\n       We experiment with two different settings here - Figure S.2 contains the results for each state\n distribution being D(s) := Bernoulli(ps) with ps \u223c                                 U (0, 0.05) being fixed for each dataset. Figure\n S.3 provides results for each state distribution being D (s) := U (0, \u2113s) with \u2113s \u223c                                                  U (0, 10) being\n selected at the start of each random run. We observe that these experiments depict similar trends to\n those shown in the main manuscript, which validates our results for the case of non-zero state means.\n Furthermore, the Bernoulli data, being sparse compared to the Uniform one, seems to exhibit a clearer\n difference between data downsampling and the traditional Oja\u2019s algorithm. To provide clear plots\n demonstrating the relative behavior of the algorithms considered in this paper, we have shown the\n averaged sin2 errors in Figures S.2 and S.3. In Figure S.4 we show six random runs where we fixed\n the ps, s \u2208       \u2126   for each state for all runs. These figures clearly show that in general, Downsampled\n Oja has a worse performance than Oja\u2019s algorithm, which has a similar performance as the offl                                                            ine\n algorithm. It also shows that the Downsampled algorithm has the most variability, whereas Oja\u2019s\n algorithm on the whole dataset has much less variability, and finally, and not surprisingly, the offline\n algorithm has the least variability. Similar qualitative trends can be observed for the other settings.\n                                                                              48", "md": "# Math Equations and Text\n\nFirst, we will show that the mixing time for this new chain is \u0398(1). We will use $$k := \\tau_{\\text{mix}} \\frac{\\eta^2 n}{2}$$. So by definition the $$d_{\\text{mix}}(k) \\leq \\frac{\\eta^2 n}{2}$$ using the definition of $$d_{\\text{mix}}$$ in Section 2.1. Hence $$d_{\\text{mix}}(k) \\leq \\frac{1}{4}$$ using conditions on the learning rate schedule imposed in Theorem 1. Therefore, in the transformed chain, the \"new\" $$\\tilde{\\tau}_{\\text{mix}}$$ is \u0398(1).\n\nWe also have:\n\n$$\nk \\leq 2\\tau_{\\text{mix}} \\frac{1}{\\log(4/\\pi_{\\text{min}})} \\frac{1}{\\log(2) \\log \\left( \\frac{\\eta^2 n}{\\log 2} \\right) \\frac{1 - |\\lambda_2(P)| \\log(n)}{C}}\n$$\n\nWe see that C > 1. Next, we note that for the transition kernel $$P^k(., .)$$, the second-largest absolute eigenvalue is given as $$|\\lambda_2(P)|^k$$. Consider the function $$f(x) := x^{1-x}$$ for $$x \\in (0, 1)$$. Then,\n\n$$\nf'(x) = f(x) \\left(1 - x - x \\log(x)\\right) > 0\n$$\nTherefore, $$f(x) < \\lim_{x \\to 1} f(x) = 1 \\frac{x}{(1 - x)^2} \\leq \\frac{1}{C \\log(n)} < 1$$ follows if C > 1, n > 3, which is true. Therefore, $$e < 1$$, which implies $$|\\lambda_2(P)|^k (i) < e^e$$. Here (i)\n\n$$\nV' := \\frac{1 + (3 + 4\\epsilon) |\\lambda_2(P)|^k}{1 - |\\lambda_2(P)|^k} V \\leq 5V\n$$\n\nThis also implies that the mixing time for the new Markov chain for sub-sampled data is \u0398(1). The bound then follows by substituting n to be $$n_k = nk = \\Theta(C\\tau_{\\text{mix}} n \\log(n))$$ and setting the $$\\tau_{\\text{mix}}$$ in the original expression of Theorem 1 to a constant.\n\nS.6 Additional Experiments\n\nIn this section, we provide additional experiments to support the results established in Section 3 of the manuscript. We present experiments with distributions that have nonzero mean vectors at each state, but zero mean with respect to the stationary distribution. This means that the Zi\u2019s are not necessarily zero-mean with respect to each state distribution D(s). To normalize the data-points, we estimate the mean $$\\mu$$ and covariance matrix $$\\Sigma$$ empirically from a much larger independently generated dataset.\n\nWe experiment with two different settings here - Figure S.2 contains the results for each state distribution being $$D(s) := \\text{Bernoulli}(p_s)$$ with $$p_s \\sim U(0, 0.05)$$ being fixed for each dataset. Figure S.3 provides results for each state distribution being $$D(s) := U(0, \\ell_s)$$ with $$\\ell_s \\sim U(0, 10)$$ being selected at the start of each random run. We observe that these experiments depict similar trends to those shown in the main manuscript, which validates our results for the case of non-zero state means.\n\nFurthermore, the Bernoulli data, being sparse compared to the Uniform one, seems to exhibit a clearer difference between data downsampling and the traditional Oja\u2019s algorithm. To provide clear plots demonstrating the relative behavior of the algorithms considered in this paper, we have shown the averaged sin^2 errors in Figures S.2 and S.3. In Figure S.4 we show six random runs where we fixed the $$p_s, s \\in \\Omega$$ for each state for all runs. These figures clearly show that in general, Downsampled Oja has a worse performance than Oja\u2019s algorithm, which has a similar performance as the offline algorithm. It also shows that the Downsampled algorithm has the most variability, whereas Oja\u2019s algorithm on the whole dataset has much less variability, and finally, and not surprisingly, the offline algorithm has the least variability. Similar qualitative trends can be observed for the other settings.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "First, we will show that the mixing time for this new chain is \u0398(1). We will use $$k := \\tau_{\\text{mix}} \\frac{\\eta^2 n}{2}$$. So by definition the $$d_{\\text{mix}}(k) \\leq \\frac{\\eta^2 n}{2}$$ using the definition of $$d_{\\text{mix}}$$ in Section 2.1. Hence $$d_{\\text{mix}}(k) \\leq \\frac{1}{4}$$ using conditions on the learning rate schedule imposed in Theorem 1. Therefore, in the transformed chain, the \"new\" $$\\tilde{\\tau}_{\\text{mix}}$$ is \u0398(1).\n\nWe also have:\n\n$$\nk \\leq 2\\tau_{\\text{mix}} \\frac{1}{\\log(4/\\pi_{\\text{min}})} \\frac{1}{\\log(2) \\log \\left( \\frac{\\eta^2 n}{\\log 2} \\right) \\frac{1 - |\\lambda_2(P)| \\log(n)}{C}}\n$$\n\nWe see that C > 1. Next, we note that for the transition kernel $$P^k(., .)$$, the second-largest absolute eigenvalue is given as $$|\\lambda_2(P)|^k$$. Consider the function $$f(x) := x^{1-x}$$ for $$x \\in (0, 1)$$. Then,\n\n$$\nf'(x) = f(x) \\left(1 - x - x \\log(x)\\right) > 0\n$$\nTherefore, $$f(x) < \\lim_{x \\to 1} f(x) = 1 \\frac{x}{(1 - x)^2} \\leq \\frac{1}{C \\log(n)} < 1$$ follows if C > 1, n > 3, which is true. Therefore, $$e < 1$$, which implies $$|\\lambda_2(P)|^k (i) < e^e$$. Here (i)\n\n$$\nV' := \\frac{1 + (3 + 4\\epsilon) |\\lambda_2(P)|^k}{1 - |\\lambda_2(P)|^k} V \\leq 5V\n$$\n\nThis also implies that the mixing time for the new Markov chain for sub-sampled data is \u0398(1). The bound then follows by substituting n to be $$n_k = nk = \\Theta(C\\tau_{\\text{mix}} n \\log(n))$$ and setting the $$\\tau_{\\text{mix}}$$ in the original expression of Theorem 1 to a constant.\n\nS.6 Additional Experiments\n\nIn this section, we provide additional experiments to support the results established in Section 3 of the manuscript. We present experiments with distributions that have nonzero mean vectors at each state, but zero mean with respect to the stationary distribution. This means that the Zi\u2019s are not necessarily zero-mean with respect to each state distribution D(s). To normalize the data-points, we estimate the mean $$\\mu$$ and covariance matrix $$\\Sigma$$ empirically from a much larger independently generated dataset.\n\nWe experiment with two different settings here - Figure S.2 contains the results for each state distribution being $$D(s) := \\text{Bernoulli}(p_s)$$ with $$p_s \\sim U(0, 0.05)$$ being fixed for each dataset. Figure S.3 provides results for each state distribution being $$D(s) := U(0, \\ell_s)$$ with $$\\ell_s \\sim U(0, 10)$$ being selected at the start of each random run. We observe that these experiments depict similar trends to those shown in the main manuscript, which validates our results for the case of non-zero state means.\n\nFurthermore, the Bernoulli data, being sparse compared to the Uniform one, seems to exhibit a clearer difference between data downsampling and the traditional Oja\u2019s algorithm. To provide clear plots demonstrating the relative behavior of the algorithms considered in this paper, we have shown the averaged sin^2 errors in Figures S.2 and S.3. In Figure S.4 we show six random runs where we fixed the $$p_s, s \\in \\Omega$$ for each state for all runs. These figures clearly show that in general, Downsampled Oja has a worse performance than Oja\u2019s algorithm, which has a similar performance as the offline algorithm. It also shows that the Downsampled algorithm has the most variability, whereas Oja\u2019s algorithm on the whole dataset has much less variability, and finally, and not surprisingly, the offline algorithm has the least variability. Similar qualitative trends can be observed for the other settings.", "md": "First, we will show that the mixing time for this new chain is \u0398(1). We will use $$k := \\tau_{\\text{mix}} \\frac{\\eta^2 n}{2}$$. So by definition the $$d_{\\text{mix}}(k) \\leq \\frac{\\eta^2 n}{2}$$ using the definition of $$d_{\\text{mix}}$$ in Section 2.1. Hence $$d_{\\text{mix}}(k) \\leq \\frac{1}{4}$$ using conditions on the learning rate schedule imposed in Theorem 1. Therefore, in the transformed chain, the \"new\" $$\\tilde{\\tau}_{\\text{mix}}$$ is \u0398(1).\n\nWe also have:\n\n$$\nk \\leq 2\\tau_{\\text{mix}} \\frac{1}{\\log(4/\\pi_{\\text{min}})} \\frac{1}{\\log(2) \\log \\left( \\frac{\\eta^2 n}{\\log 2} \\right) \\frac{1 - |\\lambda_2(P)| \\log(n)}{C}}\n$$\n\nWe see that C > 1. Next, we note that for the transition kernel $$P^k(., .)$$, the second-largest absolute eigenvalue is given as $$|\\lambda_2(P)|^k$$. Consider the function $$f(x) := x^{1-x}$$ for $$x \\in (0, 1)$$. Then,\n\n$$\nf'(x) = f(x) \\left(1 - x - x \\log(x)\\right) > 0\n$$\nTherefore, $$f(x) < \\lim_{x \\to 1} f(x) = 1 \\frac{x}{(1 - x)^2} \\leq \\frac{1}{C \\log(n)} < 1$$ follows if C > 1, n > 3, which is true. Therefore, $$e < 1$$, which implies $$|\\lambda_2(P)|^k (i) < e^e$$. Here (i)\n\n$$\nV' := \\frac{1 + (3 + 4\\epsilon) |\\lambda_2(P)|^k}{1 - |\\lambda_2(P)|^k} V \\leq 5V\n$$\n\nThis also implies that the mixing time for the new Markov chain for sub-sampled data is \u0398(1). The bound then follows by substituting n to be $$n_k = nk = \\Theta(C\\tau_{\\text{mix}} n \\log(n))$$ and setting the $$\\tau_{\\text{mix}}$$ in the original expression of Theorem 1 to a constant.\n\nS.6 Additional Experiments\n\nIn this section, we provide additional experiments to support the results established in Section 3 of the manuscript. We present experiments with distributions that have nonzero mean vectors at each state, but zero mean with respect to the stationary distribution. This means that the Zi\u2019s are not necessarily zero-mean with respect to each state distribution D(s). To normalize the data-points, we estimate the mean $$\\mu$$ and covariance matrix $$\\Sigma$$ empirically from a much larger independently generated dataset.\n\nWe experiment with two different settings here - Figure S.2 contains the results for each state distribution being $$D(s) := \\text{Bernoulli}(p_s)$$ with $$p_s \\sim U(0, 0.05)$$ being fixed for each dataset. Figure S.3 provides results for each state distribution being $$D(s) := U(0, \\ell_s)$$ with $$\\ell_s \\sim U(0, 10)$$ being selected at the start of each random run. We observe that these experiments depict similar trends to those shown in the main manuscript, which validates our results for the case of non-zero state means.\n\nFurthermore, the Bernoulli data, being sparse compared to the Uniform one, seems to exhibit a clearer difference between data downsampling and the traditional Oja\u2019s algorithm. To provide clear plots demonstrating the relative behavior of the algorithms considered in this paper, we have shown the averaged sin^2 errors in Figures S.2 and S.3. In Figure S.4 we show six random runs where we fixed the $$p_s, s \\in \\Omega$$ for each state for all runs. These figures clearly show that in general, Downsampled Oja has a worse performance than Oja\u2019s algorithm, which has a similar performance as the offline algorithm. It also shows that the Downsampled algorithm has the most variability, whereas Oja\u2019s algorithm on the whole dataset has much less variability, and finally, and not surprisingly, the offline algorithm has the least variability. Similar qualitative trends can be observed for the other settings."}]}, {"page": 49, "text": "                                        04\n                           (a)                                           (b)                                     (c)\nFigure S.2: Experiments with Bernoulli data. S.2a compares the three different algorithms, S.2b shows effect of\nchanging the eigengap of the transition_matrix and S.2c records the variation in performance on changing the\neigengap of the data covariance matrix.\n                           (a)                                           (b)                                     (c)\nFigure S.3: Experiments with Uniform data. S.3a compares the three different algorithms, S.3b shows effect of\nchanging the eigengap of the transition matrix and S.3c records the variation in performance on changing the\neigengap of the data covariance matrix.\n                           (a)                                           (b)                                     (c)\n                           (d)                                           (e)                                     (f)\n                                    Figure S.4: Randomly chosen runs for the Bernoulli case\n                                                                         49", "md": "(a)\n(b)\n(c)\n\n(a)\n(b)\n(c)\n\n49", "images": [{"name": "page-49-1.jpg", "height": 97, "width": 119, "x": 246, "y": 72}, {"name": "page-49-0.jpg", "height": 96, "width": 119, "x": 125, "y": 73}, {"name": "page-49-4.jpg", "height": 96, "width": 119, "x": 246, "y": 229}, {"name": "page-49-3.jpg", "height": 96, "width": 119, "x": 125, "y": 229}, {"name": "page-49-2.jpg", "height": 96, "width": 119, "x": 367, "y": 73}, {"name": "page-49-5.jpg", "height": 97, "width": 119, "x": 367, "y": 228}, {"name": "page-49-6.jpg", "height": 96, "width": 119, "x": 125, "y": 393}, {"name": "page-49-7.jpg", "height": 97, "width": 119, "x": 246, "y": 393}, {"name": "page-49-9.jpg", "height": 96, "width": 119, "x": 125, "y": 507}, {"name": "page-49-11.jpg", "height": 96, "width": 119, "x": 367, "y": 507}, {"name": "page-49-8.jpg", "height": 96, "width": 119, "x": 367, "y": 393}, {"name": "page-49-10.jpg", "height": 97, "width": 119, "x": 246, "y": 506}], "items": [{"type": "text", "value": "(a)\n(b)\n(c)\n\n(a)\n(b)\n(c)\n\n49", "md": "(a)\n(b)\n(c)\n\n(a)\n(b)\n(c)\n\n49"}]}], "job_id": "36628a36-1b1a-4a2d-ba2e-1d33d7a90a58", "file_path": "./corpus/2305.02456.pdf"}