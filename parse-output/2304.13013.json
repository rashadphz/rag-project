{"pages": [{"page": 1, "text": "                Stable and low-precision training for large-scale vision-language models\n                 Mitchell Wortsman            \u22171        Tim Dettmers         \u22171         Luke Zettlemoyer          12        Ari Morcos       \u20202\n                                                 Ali Farhadi      \u20201              Ludwig Schmidt          \u2020134\n           Abstract                                                              such as GPT-3 [5] and PaLM [11]. While many com-\n                                                                                 ponents are necessary for successful large-scale train-\n           We introduce new methods for 1) accelerating and 2)                   ing, two critical elements are training speed and sta-\n            stabilizing training for large language-vision models.               bility.   To enable further progress, we must ensure\n           1) For acceleration, we introduce SwitchBack, a lin-                  that 1) training is fast\u2014the model should be able\n            ear layer for int8 quantized training which provides a               to see a lot of data even if it is large, and 2) train-\n            speed-up of 13-25% while matching the performance of                 ing is stable\u2014large models should not suffer from\n            bfloat16 training within 0.1 percentage points for the               loss spikes which degrade performance.                 We study\n           1B parameter CLIP ViT-Huge\u2014the largest int8 train-                    these two directions in the context of contrastive\n            ing to date. Our main focus is int8 as GPU support                   language-image pre-training (CLIP) [46]. We exam-\n            for float8 is rare, though we also analyze float8 train-             ine CLIP-style models because of their importance in\n            ing through simulation.          While SwitchBack proves             computer vision: CLIP-style models reach state-of-\n            effective for float8, we show that standard techniques               the-art performance on a wide range of image classi-\n            are also successful if the network is trained and initial-           fication tasks [46,     65,  44,  7] and underlie image gen-\n            ized so that large feature magnitudes are discouraged,               eration methods such as DALLE\u00b72 [49] and Stable\n           which we accomplish via layer-scale initialized with                  Diffusion [51]. Our contributions towards fast train-\n            zeros. 2) For stabilization, we analyze loss spikes and              ing and stable training are as follows.\n            find they consistently occur 1-8 iterations after the\n            squared gradients become under-estimated by their                    Towards fast training, we introduce                SwitchBack,\n           AdamW second moment estimator. As a result, we rec-                   a linear layer for quantized training with int8 precision\n            ommend an AdamW-Adafactor hybrid which avoids                        which matches the performance of the bfloat16 [63]\n            loss spikes when training a CLIP ViT-Huge model                      baseline within 0.1 percentage points for CLIP ViT-\n            and outperforms gradient clipping at the scales we                   Huge\u2014a larger model than considered in the original\n            test.                                                                CLIP paper. Linear layers account for the majority of\n                                                                                 the compute in standard transformer models, usually\n            1      Introduction                                                  more than 90%, comprising the key, query, value, and\n                                                                                 out projection of the attention blocks as well as the\n            Large models trained on large datasets have recently                 multilayer perceptron. We perform all linear layers in\n            led to multiple breakthroughs in machine learning                    low-precision (int8) while retaining other layers, such\n               1University of Washington.   2 Meta AI Research, FAIR Team.       as layer norms, in higher precision. With this setup,\n           3Allen Institute for AI.  4LAION.   \u2217 Equal contribution.   \u2020Equal    we observe end-to-end speedups between 13 and 25%\n            senior contribution.                                                 for CLIP ViT-Huge training: 25% compared to a stan-\n                                                                                   60\narXiv:2304.13013v2 [cs.LG] 17 Oct 202355                                           50\n                      50                                                           40\n                                                                                   30\n                      45                                   bfloat16 baseline       20                              bfloat16 baseline\n                                                           LLM.int8() baseline                                     fp8 tensor-wise baseline\n                      40                                   SwitchBack int8         100                             SwitchBack fp8\n                     Zero-shot ImageNet accuracy                                  Zero-shot ImageNet accuracy\n                        ViT-Base               ViT-Large              ViT-Huge        ViT-Base              ViT-Large               ViT-Huge\n                                             Model size                                                    Model size\n            Figure 1: We introduce SwitchBack, a linear layer for low-precision training.                (Left)    SwitchBack for int8 training\n            matches the zero-shot ImageNet [15] accuracy of standard bfloat16 training within 0.1 percentage point for CLIP\n           ViT-Huge [46,      20] and outperforms LLM.int8() [17].          (Right)    For float8 (fp8) training [40], a baseline which uses\n            tensor-wise quantization diverges for large models while SwitchBack matches the baseline.                       In these large-model,\n            small-data experiments, our focus is on comparing methods and not final model accuracy, so we use short runs which\n            makes it feasible to run many experiments.", "md": "# Stable and low-precision training for large-scale vision-language models\n\n## Stable and low-precision training for large-scale vision-language models\n\nMitchell Wortsman \u22171, Tim Dettmers \u22171, Luke Zettlemoyer 12, Ari Morcos \u20202, Ali Farhadi \u20201, Ludwig Schmidt \u2020134\n\n### Abstract\n\nWe introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models.\n\n1. For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge\u2014the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros.\n2. For stabilization, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.\n\n### Introduction\n\nLarge models trained on large datasets have recently led to multiple breakthroughs in machine learning.\n\n1University of Washington. 2 Meta AI Research, FAIR Team. 3Allen Institute for AI. 4LAION. \u2217 Equal contribution. \u2020Equal senior contribution.\n\n### Figure 1:\n\nWe introduce SwitchBack, a linear layer for low-precision training. (Left) SwitchBack for int8 training matches the zero-shot ImageNet accuracy of standard bfloat16 training within 0.1 percentage point for CLIP ViT-Huge and outperforms LLM.int8(). (Right) For float8 (fp8) training, a baseline which uses tensor-wise quantization diverges for large models while SwitchBack matches the baseline. In these large-model, small-data experiments, our focus is on comparing methods and not final model accuracy, so we use short runs which makes it feasible to run many experiments.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Stable and low-precision training for large-scale vision-language models", "md": "# Stable and low-precision training for large-scale vision-language models"}, {"type": "heading", "lvl": 2, "value": "Stable and low-precision training for large-scale vision-language models", "md": "## Stable and low-precision training for large-scale vision-language models"}, {"type": "text", "value": "Mitchell Wortsman \u22171, Tim Dettmers \u22171, Luke Zettlemoyer 12, Ari Morcos \u20202, Ali Farhadi \u20201, Ludwig Schmidt \u2020134", "md": "Mitchell Wortsman \u22171, Tim Dettmers \u22171, Luke Zettlemoyer 12, Ari Morcos \u20202, Ali Farhadi \u20201, Ludwig Schmidt \u2020134"}, {"type": "heading", "lvl": 3, "value": "Abstract", "md": "### Abstract"}, {"type": "text", "value": "We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models.\n\n1. For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge\u2014the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros.\n2. For stabilization, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.", "md": "We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models.\n\n1. For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge\u2014the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros.\n2. For stabilization, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test."}, {"type": "heading", "lvl": 3, "value": "Introduction", "md": "### Introduction"}, {"type": "text", "value": "Large models trained on large datasets have recently led to multiple breakthroughs in machine learning.\n\n1University of Washington. 2 Meta AI Research, FAIR Team. 3Allen Institute for AI. 4LAION. \u2217 Equal contribution. \u2020Equal senior contribution.", "md": "Large models trained on large datasets have recently led to multiple breakthroughs in machine learning.\n\n1University of Washington. 2 Meta AI Research, FAIR Team. 3Allen Institute for AI. 4LAION. \u2217 Equal contribution. \u2020Equal senior contribution."}, {"type": "heading", "lvl": 3, "value": "Figure 1:", "md": "### Figure 1:"}, {"type": "text", "value": "We introduce SwitchBack, a linear layer for low-precision training. (Left) SwitchBack for int8 training matches the zero-shot ImageNet accuracy of standard bfloat16 training within 0.1 percentage point for CLIP ViT-Huge and outperforms LLM.int8(). (Right) For float8 (fp8) training, a baseline which uses tensor-wise quantization diverges for large models while SwitchBack matches the baseline. In these large-model, small-data experiments, our focus is on comparing methods and not final model accuracy, so we use short runs which makes it feasible to run many experiments.", "md": "We introduce SwitchBack, a linear layer for low-precision training. (Left) SwitchBack for int8 training matches the zero-shot ImageNet accuracy of standard bfloat16 training within 0.1 percentage point for CLIP ViT-Huge and outperforms LLM.int8(). (Right) For float8 (fp8) training, a baseline which uses tensor-wise quantization diverges for large models while SwitchBack matches the baseline. In these large-model, small-data experiments, our focus is on comparing methods and not final model accuracy, so we use short runs which makes it feasible to run many experiments."}]}, {"page": 2, "text": "dard linear layer implemented using the PyTorch [43]                   Section 2 focuses on low-precision training while Sec-\nautograd python module and 13% compared to the                         tion 3 stabilizes training by reducing loss spikes.\nstandard PyTorch layer which includes CUDA and\nC++ optimizations that happen in the background\nand which are difficult to replicate for custom layers.                2       8-bit training\nSwitchBack starts from the observation that quan-                      This section develops and compares methods for eight-\ntization noise grows with the inner dimension in a                     bit training of languge-vision transformer models.\nmatrix multiplication. For CLIP training, the weight                   First, Section 2.1 discusses preliminaries and related\ngradient computation involves a large inner dimension                  work. Next, Section 2.2 introduces and tests Switch-\nbecause CLIP training requires a large batch size [44].                Back, a linear layer for int8 and float8 training. Fi-\nHence SwitchBack uses 16 bit precision matrix multi-                   nally, Section 2.3 develops alternatives to SwitchBack\nplication for the weight gradient computation while                    which can be used for float8.\nusing int8 multiplications for the forward pass and\nlayer input gradient computations.               This approach         2.1      Preliminaries and related work\nleads to large accuracy improvements compared to\nLLM.int8() [17] (Figure 1). We provide open-source                     Neural networks today typically use 16-bit operations\nTriton [57] kernels for Switchback to enable future                    for training [39] in either the float16 or bfloat16 for-\nwork on efficient quantization schemes.                                mat [63]. Floating point formats use a subset of bits\n                                                                       to represent the exponent while the remainder spec-\nBesides int8 training, we also study large-scale 8-bit                 ifies the fraction (often referred to as the mantissa).\nfloat (fp8) [40] training.        We do not have access to             The float16 format uses 5 bits for the exponent while\nhardware that supports fp8 data types, which is cur-                   bfloat16 uses 8 and therefore covers a larger range\u2014\nrently more rare than int8, so we use an accurate                      float16 has a range of (5.96\u00b710      \u22128  ,65504) while bfloat16\nsimulation of fp8 computation. SwitchBack also out-                    has a range of (10      \u221238  ,3 \u00b7 10 38 ). Most floating point\nperforms straightforward 8-bit float (fp8) baselines                   formats also have denormalized numbers which allow\nbecause tensor-wise quantized baselines diverge at                     for a \u201csoft underflow\u201d which gets exponentially closer\n>420M scale (Figure 1). However, we demonstrate                        to 0.0f for each additional bit in the mantissa. To pre-\nthat these methods can achieve high accuracy if the                    vent underflows float16 mixed precision training [39]\nnetwork is trained while keeping feature magnitudes                    has been developed which works as follows. The loss\nsmall, which we accomplish via layer-scale [58] initial-               of a mini-batch is multiplied by a loss scalar to scale\nized with zeros.                                                       the loss and following backpropagation gradients into\n                                                                       the representable range of fp16. This loss scaling is\nTowards stable training, we find that loss spikes                      undone by rescaling the weight gradients before the\noccur in CLIP training when the AdamW [37] sec-                        optimizer updates fp32 main weights with the fp16\nond moment estimator becomes out-of-date in the                        gradients. In PyTorch [43], the loss scalar is initialized\npatch embedding [20] layer. In particular, the learn-                  to 65536. Everytime an Inf/NaN is encountered, the\ning signal changes so that the moving averages of                      update is skipped and the loss scalar is halved. If no\nsquared gradients underestimates their true magni-                     Inf/NaN are encountered for 2k iterations, the scalar\ntude.    Indeed, in the absence of stability interven-                 is doubled.\ntions, we show that loss spikes can be predicted by\nexamining this ratio of the squared gradients to their                 When the loss scalar becomes too low in float16\nmoving average. We therefore recommend a AdamW-                        training the loss slowly diverges. This was observed\nAdaFactor [54] hybrid, which we refer to as                    Sta-    by Cherti et al.       [9] when training ViT-Huge CLIP\nbleAdamW          as it removes instabilities at the scales            models and remedied by switching to bfloat16. An-\nwe consider and outperforms gradient clipping. Con-                    other instance of float16 creating issues at scale was\ncretely, StableAdamW is AdamW with the update                          the training of OPT [74] and BLOOM models [52].\nclipping technique introduced in AdaFactor. Update                     Indeed, many obstacles faced during the OPT project\nclipping tracks the average ratio of the gradient square               could have been alleviated by using bfloat16 [73]. Sim-\nto the second moment estimator and lowers the learn-                   ilarly, all float16 training runs for BLOOM ended in\ning rate when the ratio is large.                                      divergence, only after using bfloat16 was the training\n                                                                       stable. However, fast bfloat16 support is only avail-\nThe remainder of this paper is organized as follows:                 2 able on TPUs, or GPUs developed with or after the", "md": "dard linear layer implemented using the PyTorch [43] Section 2 focuses on low-precision training while Sec- autograd python module and 13% compared to the standard PyTorch layer which includes CUDA and C++ optimizations that happen in the background and which are difficult to replicate for custom layers. 2 8-bit training This section develops and compares methods for eight- bit training of languge-vision transformer models. First, Section 2.1 discusses preliminaries and related work. Next, Section 2.2 introduces and tests Switch- Back, a linear layer for int8 and float8 training. Fi- nally, Section 2.3 develops alternatives to SwitchBack which can be used for float8. Neural networks today typically use 16-bit operations for training [39] in either the float16 or bfloat16 for- mat [63]. Floating point formats use a subset of bits to represent the exponent while the remainder spec- ifies the fraction (often referred to as the mantissa). The float16 format uses 5 bits for the exponent while bfloat16 uses 8 and therefore covers a larger range\u2014 float16 has a range of $$(5.96 \\times 10^{-8}, 65504)$$ while bfloat16 has a range of $$(10^{-38}, 3 \\times 10^{38})$$. Most floating point formats also have denormalized numbers which allow for a \u201csoft underflow\u201d which gets exponentially closer to 0.0f for each additional bit in the mantissa. To pre- vent underflows float16 mixed precision training [39] has been developed which works as follows. The loss of a mini-batch is multiplied by a loss scalar to scale the loss and following backpropagation gradients into the representable range of fp16. This loss scaling is undone by rescaling the weight gradients before the optimizer updates fp32 main weights with the fp16 gradients. In PyTorch [43], the loss scalar is initialized to 65536. Everytime an Inf/NaN is encountered, the update is skipped and the loss scalar is halved. If no Inf/NaN are encountered for 2k iterations, the scalar is doubled. When the loss scalar becomes too low in float16 training the loss slowly diverges. This was observed by Cherti et al. [9] when training ViT-Huge CLIP models and remedied by switching to bfloat16. An- other instance of float16 creating issues at scale was the training of OPT [74] and BLOOM models [52]. Indeed, many obstacles faced during the OPT project could have been alleviated by using bfloat16 [73]. Sim- ilarly, all float16 training runs for BLOOM ended in divergence, only after using bfloat16 was the training stable. However, fast bfloat16 support is only avail- able on TPUs, or GPUs developed with or after the\n\n|Section|Title|\n|---|---|\n|2|8-bit training|\n|2.1|Preliminaries and related work|\n\nstandard PyTorch layer which includes CUDA and C++ optimizations that happen in the background and which are difficult to replicate for custom layers. SwitchBack starts from the observation that quan- tization noise grows with the inner dimension in a matrix multiplication. For CLIP training, the weight gradient computation involves a large inner dimension because CLIP training requires a large batch size [44]. Hence SwitchBack uses 16 bit precision matrix multi- plication for the weight gradient computation while using int8 multiplications for the forward pass and layer input gradient computations. This approach leads to large accuracy improvements compared to LLM.int8() [17] (Figure 1). We provide open-source Triton [57] kernels for Switchback to enable future work on efficient quantization schemes. Besides int8 training, we also study large-scale 8-bit float (fp8) [40] training. We do not have access to hardware that supports fp8 data types, which is cur- rently more rare than int8, so we use an accurate simulation of fp8 computation. SwitchBack also out- performs straightforward 8-bit float (fp8) baselines because tensor-wise quantized baselines diverge at >420M scale (Figure 1). However, we demonstrate that these methods can achieve high accuracy if the network is trained while keeping feature magnitudes small, which we accomplish via layer-scale [58] initial- ized with zeros. Towards stable training, we find that loss spikes occur in CLIP training when the AdamW [37] sec- ond moment estimator becomes out-of-date in the patch embedding [20] layer. In particular, the learn- ing signal changes so that the moving averages of squared gradients underestimates their true magni- tude. Indeed, in the absence of stability interven- tions, we show that loss spikes can be predicted by examining this ratio of the squared gradients to their moving average. We therefore recommend a AdamW- AdaFactor [54] hybrid, which we refer to as StableAdamW as it removes instabilities at the scales we consider and outperforms gradient clipping. Con- cretely, StableAdamW is AdamW with the update clipping technique introduced in AdaFactor. Update clipping tracks the average ratio of the gradient square to the second moment estimator and lowers the learn- ing rate when the ratio is large. The remainder of this paper is organized as follows:", "images": [], "items": [{"type": "text", "value": "dard linear layer implemented using the PyTorch [43] Section 2 focuses on low-precision training while Sec- autograd python module and 13% compared to the standard PyTorch layer which includes CUDA and C++ optimizations that happen in the background and which are difficult to replicate for custom layers. 2 8-bit training This section develops and compares methods for eight- bit training of languge-vision transformer models. First, Section 2.1 discusses preliminaries and related work. Next, Section 2.2 introduces and tests Switch- Back, a linear layer for int8 and float8 training. Fi- nally, Section 2.3 develops alternatives to SwitchBack which can be used for float8. Neural networks today typically use 16-bit operations for training [39] in either the float16 or bfloat16 for- mat [63]. Floating point formats use a subset of bits to represent the exponent while the remainder spec- ifies the fraction (often referred to as the mantissa). The float16 format uses 5 bits for the exponent while bfloat16 uses 8 and therefore covers a larger range\u2014 float16 has a range of $$(5.96 \\times 10^{-8}, 65504)$$ while bfloat16 has a range of $$(10^{-38}, 3 \\times 10^{38})$$. Most floating point formats also have denormalized numbers which allow for a \u201csoft underflow\u201d which gets exponentially closer to 0.0f for each additional bit in the mantissa. To pre- vent underflows float16 mixed precision training [39] has been developed which works as follows. The loss of a mini-batch is multiplied by a loss scalar to scale the loss and following backpropagation gradients into the representable range of fp16. This loss scaling is undone by rescaling the weight gradients before the optimizer updates fp32 main weights with the fp16 gradients. In PyTorch [43], the loss scalar is initialized to 65536. Everytime an Inf/NaN is encountered, the update is skipped and the loss scalar is halved. If no Inf/NaN are encountered for 2k iterations, the scalar is doubled. When the loss scalar becomes too low in float16 training the loss slowly diverges. This was observed by Cherti et al. [9] when training ViT-Huge CLIP models and remedied by switching to bfloat16. An- other instance of float16 creating issues at scale was the training of OPT [74] and BLOOM models [52]. Indeed, many obstacles faced during the OPT project could have been alleviated by using bfloat16 [73]. Sim- ilarly, all float16 training runs for BLOOM ended in divergence, only after using bfloat16 was the training stable. However, fast bfloat16 support is only avail- able on TPUs, or GPUs developed with or after the", "md": "dard linear layer implemented using the PyTorch [43] Section 2 focuses on low-precision training while Sec- autograd python module and 13% compared to the standard PyTorch layer which includes CUDA and C++ optimizations that happen in the background and which are difficult to replicate for custom layers. 2 8-bit training This section develops and compares methods for eight- bit training of languge-vision transformer models. First, Section 2.1 discusses preliminaries and related work. Next, Section 2.2 introduces and tests Switch- Back, a linear layer for int8 and float8 training. Fi- nally, Section 2.3 develops alternatives to SwitchBack which can be used for float8. Neural networks today typically use 16-bit operations for training [39] in either the float16 or bfloat16 for- mat [63]. Floating point formats use a subset of bits to represent the exponent while the remainder spec- ifies the fraction (often referred to as the mantissa). The float16 format uses 5 bits for the exponent while bfloat16 uses 8 and therefore covers a larger range\u2014 float16 has a range of $$(5.96 \\times 10^{-8}, 65504)$$ while bfloat16 has a range of $$(10^{-38}, 3 \\times 10^{38})$$. Most floating point formats also have denormalized numbers which allow for a \u201csoft underflow\u201d which gets exponentially closer to 0.0f for each additional bit in the mantissa. To pre- vent underflows float16 mixed precision training [39] has been developed which works as follows. The loss of a mini-batch is multiplied by a loss scalar to scale the loss and following backpropagation gradients into the representable range of fp16. This loss scaling is undone by rescaling the weight gradients before the optimizer updates fp32 main weights with the fp16 gradients. In PyTorch [43], the loss scalar is initialized to 65536. Everytime an Inf/NaN is encountered, the update is skipped and the loss scalar is halved. If no Inf/NaN are encountered for 2k iterations, the scalar is doubled. When the loss scalar becomes too low in float16 training the loss slowly diverges. This was observed by Cherti et al. [9] when training ViT-Huge CLIP models and remedied by switching to bfloat16. An- other instance of float16 creating issues at scale was the training of OPT [74] and BLOOM models [52]. Indeed, many obstacles faced during the OPT project could have been alleviated by using bfloat16 [73]. Sim- ilarly, all float16 training runs for BLOOM ended in divergence, only after using bfloat16 was the training stable. However, fast bfloat16 support is only avail- able on TPUs, or GPUs developed with or after the"}, {"type": "table", "rows": [["Section", "Title"], ["2", "8-bit training"], ["2.1", "Preliminaries and related work"]], "md": "|Section|Title|\n|---|---|\n|2|8-bit training|\n|2.1|Preliminaries and related work|", "isPerfectTable": true, "csv": "\"Section\",\"Title\"\n\"2\",\"8-bit training\"\n\"2.1\",\"Preliminaries and related work\""}, {"type": "text", "value": "standard PyTorch layer which includes CUDA and C++ optimizations that happen in the background and which are difficult to replicate for custom layers. SwitchBack starts from the observation that quan- tization noise grows with the inner dimension in a matrix multiplication. For CLIP training, the weight gradient computation involves a large inner dimension because CLIP training requires a large batch size [44]. Hence SwitchBack uses 16 bit precision matrix multi- plication for the weight gradient computation while using int8 multiplications for the forward pass and layer input gradient computations. This approach leads to large accuracy improvements compared to LLM.int8() [17] (Figure 1). We provide open-source Triton [57] kernels for Switchback to enable future work on efficient quantization schemes. Besides int8 training, we also study large-scale 8-bit float (fp8) [40] training. We do not have access to hardware that supports fp8 data types, which is cur- rently more rare than int8, so we use an accurate simulation of fp8 computation. SwitchBack also out- performs straightforward 8-bit float (fp8) baselines because tensor-wise quantized baselines diverge at >420M scale (Figure 1). However, we demonstrate that these methods can achieve high accuracy if the network is trained while keeping feature magnitudes small, which we accomplish via layer-scale [58] initial- ized with zeros. Towards stable training, we find that loss spikes occur in CLIP training when the AdamW [37] sec- ond moment estimator becomes out-of-date in the patch embedding [20] layer. In particular, the learn- ing signal changes so that the moving averages of squared gradients underestimates their true magni- tude. Indeed, in the absence of stability interven- tions, we show that loss spikes can be predicted by examining this ratio of the squared gradients to their moving average. We therefore recommend a AdamW- AdaFactor [54] hybrid, which we refer to as StableAdamW as it removes instabilities at the scales we consider and outperforms gradient clipping. Con- cretely, StableAdamW is AdamW with the update clipping technique introduced in AdaFactor. Update clipping tracks the average ratio of the gradient square to the second moment estimator and lowers the learn- ing rate when the ratio is large. The remainder of this paper is organized as follows:", "md": "standard PyTorch layer which includes CUDA and C++ optimizations that happen in the background and which are difficult to replicate for custom layers. SwitchBack starts from the observation that quan- tization noise grows with the inner dimension in a matrix multiplication. For CLIP training, the weight gradient computation involves a large inner dimension because CLIP training requires a large batch size [44]. Hence SwitchBack uses 16 bit precision matrix multi- plication for the weight gradient computation while using int8 multiplications for the forward pass and layer input gradient computations. This approach leads to large accuracy improvements compared to LLM.int8() [17] (Figure 1). We provide open-source Triton [57] kernels for Switchback to enable future work on efficient quantization schemes. Besides int8 training, we also study large-scale 8-bit float (fp8) [40] training. We do not have access to hardware that supports fp8 data types, which is cur- rently more rare than int8, so we use an accurate simulation of fp8 computation. SwitchBack also out- performs straightforward 8-bit float (fp8) baselines because tensor-wise quantized baselines diverge at >420M scale (Figure 1). However, we demonstrate that these methods can achieve high accuracy if the network is trained while keeping feature magnitudes small, which we accomplish via layer-scale [58] initial- ized with zeros. Towards stable training, we find that loss spikes occur in CLIP training when the AdamW [37] sec- ond moment estimator becomes out-of-date in the patch embedding [20] layer. In particular, the learn- ing signal changes so that the moving averages of squared gradients underestimates their true magni- tude. Indeed, in the absence of stability interven- tions, we show that loss spikes can be predicted by examining this ratio of the squared gradients to their moving average. We therefore recommend a AdamW- AdaFactor [54] hybrid, which we refer to as StableAdamW as it removes instabilities at the scales we consider and outperforms gradient clipping. Con- cretely, StableAdamW is AdamW with the update clipping technique introduced in AdaFactor. Update clipping tracks the average ratio of the gradient square to the second moment estimator and lowers the learn- ing rate when the ratio is large. The remainder of this paper is organized as follows:"}]}, {"page": 3, "text": "Algorithm 1         PyTorch pseudo-code for          SwitchBack          outputs and two in the backwards pass to compute\nclass SwitchBackMatmul(autograd.Function):                               gradients for the input and weights. Our SwitchBack\n    @staticmethod                                                        layer uses 8 bit precision for the first two matrix\n    def forward(ctx, X, W):                                              multiplies but switches back to higher precision for\n        # X [b, n] inputs\n        # W [n, m] weights                                               the weight gradient.\n        # save tensors in ctx\n        ctx.save_for_backward = X, W                                     We compute the weight gradient in higher precision\n        X_int8, state_X = row-wise_quantize(X)                           because this matrix multiplication involves dot prod-\n        W_int8, state_W = tensor-wise_quantize(W)                        ucts between vectors which have a length of batch\n        # Return output                                                  size times sequence length. As CLIP training requires\n        return matmul_int8_and_dequanitze(                               large batch sizes [46,     44], this inner dimension of batch\n        )    X_int8, W_int8.t(), state_X, state_W                        size times sequence length is much larger than for the\n    @staticmethod                                                        other matrix multiplies. As we show in Appendix C,\n    def backward(ctx, G):                                                variance due to quantization increases with the inner\n        # G [b, m] gradient to output\n        # Recover tensors from ctx                                       dimension of the matrix multiply. This modification\n        X, W = ctx.save_for_backward                                     is what differentiates SwitchBack from LLM.int8(),\n        G_rowwise = rowwise_quantize(G)                                  allowing SwitchBack to match the bfloat16 baseline\n        W_int8, state_W = tensor-wise_quantize_transpose(W)              (Figure 1).\n        # Use 8bit matmul only for X_gradient\n        X_gradient = matmul_int8_and_dequanitze(                         Notation.        A standard linear layer is comprised of\n             G_int8, W_int8.t(), state_X, state_W                        inputs    X   \u2208  R b\u00d7n , weights    W    \u2208  R m\u00d7n   , and outputs\n        )                                                                         b\u00d7m\n        W_gradient = matmul_fp16(G.t(), X)                               Y   \u2208  R      .   In the forward pass, outputs are com-\n        return X_gradient, W_gradient                                    puted as    Y   =  XW    \u22a4 . In the backwards pass the layer\n                                                                         receives gradients of the loss with respect to            Y  , which\nclass SwitchBackLinear(nn.Linear):                                       we denote      Y\u02d9. Then, gradients to inputs           X\u02d9  are com-\n    def forward(self, X):\n        return SwitchBackMatmul.apply(X, self.weight)                    puted via     X\u02d9 =  Y\u02d9W    while gradients to the weights          W\u02d9\n                                                                                                   \u02d9      \u02d9 \u22a4\n                                                                         are computed via         W   =   Y   X. For linear layers in a\n                                                                         transformer [60],      b is batch size times sequence length,\nNVIDIA Ampere series (2021 or later).                                    while   n  and   m   are small multiples of the embedding\nWhile 16 bit training is the standard today, hardware                    dimension.\nsupport for 8 bit operations are becoming more com-                      Quantization.           For the matrix multiplies in 8 bit\nmon.     Hopper GPUs support float8 (fp8) [40] and                       precision we use quantization. There are a multiple\nAmpere GPUs support int8. However, it is currently                       quantization techniques to choose from and we re-\n(2023) very difficult to attain Hopper GPUs. Moreover,                   lease code for all these alternatives. However, we find\nwhile int8 and int4 are used for inference [17,              66,  16],   the best trade-off of simplicity and performance is\nand there is earlier work exploring 8 bit training for                   from using i) row-wise quantization [31] for the in-\nconvnets [61,     79,  10], these formats are not commonly               puts and gradients and ii) tensor-wise quantization\nused for training transformer models at scale. The                       for the weights.        Additional information on quanti-\nCLIP ViT-Huge models we train have 1B parame-                            zation methods is provided by Dettmers et al.                    [17]\nters including the image and text towers which is 40x                    but we summarize below. Using int8 as an example,\nlarger than a standard ResNet-50 (23M) [28], and                         which can represent integers from               \u2212127 to 127, we\nquantization is more challenging for large tensors [17].                 now define row-wise and tensor wise quantization. For\n                                                                         a matrix    X   with rows    x  , ..., x, row-wise quantization\nAdditional related work on quantization of large scale                                                  1       b\nmodels (larger than BERT-large) and low-precision                        Q  row  is given by\ntraining and be found in Appendix A.                                                \uf8eb\uf8ee   x  \uf8f9\uf8f6               \uf8eb\uf8ee       127      \u00b7x 1\uf8f9\uf8f6\n                                                                                           1                      absmax(x  )\n                                                                                    \uf8ec\uf8ef    . \uf8fa\uf8f7               \uf8ec\uf8ef           . 1      \uf8fa\uf8f7\n                                                                             Q      \uf8ed\uf8f0    . \uf8fb\uf8f8    =  round   \uf8ed\uf8f0           .        \uf8fb\uf8f8       (1)\n2.2       SwitchBack                                                            row      x.n                          127 .    \u00b7x b\n                                                                                                                  absmax(x  b)\n2.2.1      Method                                                        while tensor-wise quantization           Q  tensor is given by\nOverview.         A linear layer consists of three matrix                         Q tensor (X) =    round   \u0012       127        \u00b7X  \u0013  ,     (2)\nmultiplications\u2014one in the forward pass to compute                     3                                       absmax    (X)", "md": "Algorithm 1 PyTorch pseudo-code for SwitchBack outputs and two in the backwards pass to compute\n\nclass SwitchBackMatmul(autograd.Function):\n\n$$\n\\begin{align*}\n&\\text{def forward(ctx, X, W):} \\\\\n&\\# X [b, n] inputs \\\\\n&\\# W [n, m] weights \\\\\n&\\text{ctx.save\\_for\\_backward = X, W} \\\\\n&X\\_int8, state\\_X = row-wise\\_quantize(X) \\\\\n&W\\_int8, state\\_W = tensor-wise\\_quantize(W) \\\\\n&\\text{return matmul\\_int8\\_and\\_dequanitze(X\\_int8, W\\_int8.t(), state\\_X, state\\_W)} \\\\\n\\end{align*}\n$$\n\n...\n\nclass SwitchBackLinear(nn.Linear):\n\n$$\n\\begin{align*}\n&\\text{def forward(self, X):} \\\\\n&\\text{return SwitchBackMatmul.apply(X, self.weight)} \\\\\n\\end{align*}\n$$\n\nNVIDIA Ampere series (2021 or later).\n\nWhile 16 bit training is the standard today, hardware support for 8 bit operations are becoming more common. Hopper GPUs support float8 (fp8) [40] and Ampere GPUs support int8. However, it is currently (2023) very difficult to attain Hopper GPUs. Moreover, while int8 and int4 are used for inference [17, 66, 16], and there is earlier work exploring 8 bit training for convnets [61, 79, 10], these formats are not commonly used for training transformer models at scale. The CLIP ViT-Huge models we train have 1B parameters including the image and text towers which is 40x larger than a standard ResNet-50 (23M) [28], and quantization is more challenging for large tensors [17].\n\nAdditional related work on quantization of large scale models (larger than BERT-large) and low-precision training and be found in Appendix A.\n\n2.2 SwitchBack\n\n2.2.1 Method\n\nOverview. A linear layer consists of three matrix multiplications\u2014one in the forward pass to compute gradients for the input and weights. Our SwitchBack layer uses 8 bit precision for the first two matrix multiplies but switches back to higher precision for the weight gradient.\n\nWe compute the weight gradient in higher precision because this matrix multiplication involves dot products between vectors which have a length of batch size times sequence length. As CLIP training requires large batch sizes [46, 44], this inner dimension of batch size times sequence length is much larger than for the other matrix multiplies. As we show in Appendix C, variance due to quantization increases with the inner dimension of the matrix multiply. This modification is what differentiates SwitchBack from LLM.int8(), allowing SwitchBack to match the bfloat16 baseline (Figure 1).\n\nNotation. A standard linear layer is comprised of inputs \\(X \\in \\mathbb{R}^{b \\times n}\\), weights \\(W \\in \\mathbb{R}^{m \\times n}\\), and outputs \\(Y \\in \\mathbb{R}^{b \\times m}\\). In the forward pass, outputs are computed as \\(Y = XW^T\\). In the backwards pass the layer receives gradients of the loss with respect to \\(Y\\), which we denote \\(\\dot{Y}\\). Then, gradients to inputs \\(\\dot{X}\\) are computed via \\(\\dot{X} = \\dot{Y}W\\) while gradients to the weights \\(\\dot{W}\\) are computed via \\(\\dot{W} = \\dot{Y}X\\). For linear layers in a transformer [60], \\(b\\) is batch size times sequence length, while \\(n\\) and \\(m\\) are small multiples of the embedding dimension.\n\nQuantization. For the matrix multiplies in 8 bit precision we use quantization. There are multiple quantization techniques to choose from and we release code for all these alternatives. However, we find the best trade-off of simplicity and performance is from using i) row-wise quantization [31] for the inputs and gradients and ii) tensor-wise quantization for the weights. Additional information on quantization methods is provided by Dettmers et al. [17] but we summarize below. Using int8 as an example, which can represent integers from -127 to 127, we now define row-wise and tensor wise quantization. For a matrix \\(X\\) with rows \\(x_1, ..., x_b\\), row-wise quantization \\(Q_{\\text{row}}\\) is given by\n\n$$\nQ_{\\text{row}}(x) = \\text{round}\\left(\\frac{127 \\cdot x}{\\text{absmax}(x)}\\right)\n$$\nwhile tensor-wise quantization \\(Q_{\\text{tensor}}\\) is given by\n\n$$\nQ_{\\text{tensor}}(X) = \\text{round}\\left(\\frac{127 \\cdot X}{\\text{absmax}(X)}\\right)\n$$", "images": [], "items": [{"type": "text", "value": "Algorithm 1 PyTorch pseudo-code for SwitchBack outputs and two in the backwards pass to compute\n\nclass SwitchBackMatmul(autograd.Function):\n\n$$\n\\begin{align*}\n&\\text{def forward(ctx, X, W):} \\\\\n&\\# X [b, n] inputs \\\\\n&\\# W [n, m] weights \\\\\n&\\text{ctx.save\\_for\\_backward = X, W} \\\\\n&X\\_int8, state\\_X = row-wise\\_quantize(X) \\\\\n&W\\_int8, state\\_W = tensor-wise\\_quantize(W) \\\\\n&\\text{return matmul\\_int8\\_and\\_dequanitze(X\\_int8, W\\_int8.t(), state\\_X, state\\_W)} \\\\\n\\end{align*}\n$$\n\n...\n\nclass SwitchBackLinear(nn.Linear):\n\n$$\n\\begin{align*}\n&\\text{def forward(self, X):} \\\\\n&\\text{return SwitchBackMatmul.apply(X, self.weight)} \\\\\n\\end{align*}\n$$\n\nNVIDIA Ampere series (2021 or later).\n\nWhile 16 bit training is the standard today, hardware support for 8 bit operations are becoming more common. Hopper GPUs support float8 (fp8) [40] and Ampere GPUs support int8. However, it is currently (2023) very difficult to attain Hopper GPUs. Moreover, while int8 and int4 are used for inference [17, 66, 16], and there is earlier work exploring 8 bit training for convnets [61, 79, 10], these formats are not commonly used for training transformer models at scale. The CLIP ViT-Huge models we train have 1B parameters including the image and text towers which is 40x larger than a standard ResNet-50 (23M) [28], and quantization is more challenging for large tensors [17].\n\nAdditional related work on quantization of large scale models (larger than BERT-large) and low-precision training and be found in Appendix A.\n\n2.2 SwitchBack\n\n2.2.1 Method\n\nOverview. A linear layer consists of three matrix multiplications\u2014one in the forward pass to compute gradients for the input and weights. Our SwitchBack layer uses 8 bit precision for the first two matrix multiplies but switches back to higher precision for the weight gradient.\n\nWe compute the weight gradient in higher precision because this matrix multiplication involves dot products between vectors which have a length of batch size times sequence length. As CLIP training requires large batch sizes [46, 44], this inner dimension of batch size times sequence length is much larger than for the other matrix multiplies. As we show in Appendix C, variance due to quantization increases with the inner dimension of the matrix multiply. This modification is what differentiates SwitchBack from LLM.int8(), allowing SwitchBack to match the bfloat16 baseline (Figure 1).\n\nNotation. A standard linear layer is comprised of inputs \\(X \\in \\mathbb{R}^{b \\times n}\\), weights \\(W \\in \\mathbb{R}^{m \\times n}\\), and outputs \\(Y \\in \\mathbb{R}^{b \\times m}\\). In the forward pass, outputs are computed as \\(Y = XW^T\\). In the backwards pass the layer receives gradients of the loss with respect to \\(Y\\), which we denote \\(\\dot{Y}\\). Then, gradients to inputs \\(\\dot{X}\\) are computed via \\(\\dot{X} = \\dot{Y}W\\) while gradients to the weights \\(\\dot{W}\\) are computed via \\(\\dot{W} = \\dot{Y}X\\). For linear layers in a transformer [60], \\(b\\) is batch size times sequence length, while \\(n\\) and \\(m\\) are small multiples of the embedding dimension.\n\nQuantization. For the matrix multiplies in 8 bit precision we use quantization. There are multiple quantization techniques to choose from and we release code for all these alternatives. However, we find the best trade-off of simplicity and performance is from using i) row-wise quantization [31] for the inputs and gradients and ii) tensor-wise quantization for the weights. Additional information on quantization methods is provided by Dettmers et al. [17] but we summarize below. Using int8 as an example, which can represent integers from -127 to 127, we now define row-wise and tensor wise quantization. For a matrix \\(X\\) with rows \\(x_1, ..., x_b\\), row-wise quantization \\(Q_{\\text{row}}\\) is given by\n\n$$\nQ_{\\text{row}}(x) = \\text{round}\\left(\\frac{127 \\cdot x}{\\text{absmax}(x)}\\right)\n$$\nwhile tensor-wise quantization \\(Q_{\\text{tensor}}\\) is given by\n\n$$\nQ_{\\text{tensor}}(X) = \\text{round}\\left(\\frac{127 \\cdot X}{\\text{absmax}(X)}\\right)\n$$", "md": "Algorithm 1 PyTorch pseudo-code for SwitchBack outputs and two in the backwards pass to compute\n\nclass SwitchBackMatmul(autograd.Function):\n\n$$\n\\begin{align*}\n&\\text{def forward(ctx, X, W):} \\\\\n&\\# X [b, n] inputs \\\\\n&\\# W [n, m] weights \\\\\n&\\text{ctx.save\\_for\\_backward = X, W} \\\\\n&X\\_int8, state\\_X = row-wise\\_quantize(X) \\\\\n&W\\_int8, state\\_W = tensor-wise\\_quantize(W) \\\\\n&\\text{return matmul\\_int8\\_and\\_dequanitze(X\\_int8, W\\_int8.t(), state\\_X, state\\_W)} \\\\\n\\end{align*}\n$$\n\n...\n\nclass SwitchBackLinear(nn.Linear):\n\n$$\n\\begin{align*}\n&\\text{def forward(self, X):} \\\\\n&\\text{return SwitchBackMatmul.apply(X, self.weight)} \\\\\n\\end{align*}\n$$\n\nNVIDIA Ampere series (2021 or later).\n\nWhile 16 bit training is the standard today, hardware support for 8 bit operations are becoming more common. Hopper GPUs support float8 (fp8) [40] and Ampere GPUs support int8. However, it is currently (2023) very difficult to attain Hopper GPUs. Moreover, while int8 and int4 are used for inference [17, 66, 16], and there is earlier work exploring 8 bit training for convnets [61, 79, 10], these formats are not commonly used for training transformer models at scale. The CLIP ViT-Huge models we train have 1B parameters including the image and text towers which is 40x larger than a standard ResNet-50 (23M) [28], and quantization is more challenging for large tensors [17].\n\nAdditional related work on quantization of large scale models (larger than BERT-large) and low-precision training and be found in Appendix A.\n\n2.2 SwitchBack\n\n2.2.1 Method\n\nOverview. A linear layer consists of three matrix multiplications\u2014one in the forward pass to compute gradients for the input and weights. Our SwitchBack layer uses 8 bit precision for the first two matrix multiplies but switches back to higher precision for the weight gradient.\n\nWe compute the weight gradient in higher precision because this matrix multiplication involves dot products between vectors which have a length of batch size times sequence length. As CLIP training requires large batch sizes [46, 44], this inner dimension of batch size times sequence length is much larger than for the other matrix multiplies. As we show in Appendix C, variance due to quantization increases with the inner dimension of the matrix multiply. This modification is what differentiates SwitchBack from LLM.int8(), allowing SwitchBack to match the bfloat16 baseline (Figure 1).\n\nNotation. A standard linear layer is comprised of inputs \\(X \\in \\mathbb{R}^{b \\times n}\\), weights \\(W \\in \\mathbb{R}^{m \\times n}\\), and outputs \\(Y \\in \\mathbb{R}^{b \\times m}\\). In the forward pass, outputs are computed as \\(Y = XW^T\\). In the backwards pass the layer receives gradients of the loss with respect to \\(Y\\), which we denote \\(\\dot{Y}\\). Then, gradients to inputs \\(\\dot{X}\\) are computed via \\(\\dot{X} = \\dot{Y}W\\) while gradients to the weights \\(\\dot{W}\\) are computed via \\(\\dot{W} = \\dot{Y}X\\). For linear layers in a transformer [60], \\(b\\) is batch size times sequence length, while \\(n\\) and \\(m\\) are small multiples of the embedding dimension.\n\nQuantization. For the matrix multiplies in 8 bit precision we use quantization. There are multiple quantization techniques to choose from and we release code for all these alternatives. However, we find the best trade-off of simplicity and performance is from using i) row-wise quantization [31] for the inputs and gradients and ii) tensor-wise quantization for the weights. Additional information on quantization methods is provided by Dettmers et al. [17] but we summarize below. Using int8 as an example, which can represent integers from -127 to 127, we now define row-wise and tensor wise quantization. For a matrix \\(X\\) with rows \\(x_1, ..., x_b\\), row-wise quantization \\(Q_{\\text{row}}\\) is given by\n\n$$\nQ_{\\text{row}}(x) = \\text{round}\\left(\\frac{127 \\cdot x}{\\text{absmax}(x)}\\right)\n$$\nwhile tensor-wise quantization \\(Q_{\\text{tensor}}\\) is given by\n\n$$\nQ_{\\text{tensor}}(X) = \\text{round}\\left(\\frac{127 \\cdot X}{\\text{absmax}(X)}\\right)\n$$"}]}, {"page": 4, "text": "    6       ViT-Base model             6       ViT-Huge model             6       ViT-Base model             6       ViT-Huge model\n    5                                  5                                  5                                  5\n    4                                  4                                  4                                  4\n                                       3                                                                     3\n   Loss                               Loss                               Loss                               Loss\n    3                                  2                                  3                                  2\n    2                                  1                                  2                                  1\n    1  0   5000  100001500020000       0  0   5000  100001500020000       1 0    5000 1000015000   20000     0 0    5000 100001500020000\n              Iteration                          Iteration                           Iteration                          Iteration\n                   bfloat16 baseline        SwitchBack int8                           bfloat16 baseline             SwitchBack fp8\n                   LLM.int8() baseline                                                fp8 tensor-wise baseline\nFigure 2: Loss curves for the CLIP ViT-Base and CLIP ViT-Huge models evaluated in Figure 1. The left two plots\ndisplay results for int8 training while the right two plots display results for float8 (fp8) training.\nwhere   absmax     is the maximum of the absolute value.               and quantization we fuse both operations, meaning\n                                                                       we load the required data once from slow DRAM\nImportantly, when applying             Q row  we also save the         into fast SRAM/shared memory and then perform\nrow-wise absolute maximums so that we can use                          both operation in this cached memory \u2013 this is crit-\nthem later for dequantization.                We refer to this         ical for achieving speedups.           We call this operation\nas the quantization state,           or   state,   for short,     so   tensor-wise quantize transpose, which is a fused\nstate row (X) =     [absmax(x    1), ...,absmax(x   b )]\u22a4  \u2208  R b\u00d71 .  tensor-wise quantize and transpose operation.\nEquivalently, for tensor-wise quantization we only\nneed to store the tensor-wise absolute maximum so                      Putting the pieces together, the result is Algorithm 1.\nstate tensor (X) =    absmax(X)      \u2208  R.                             Variants.      While Algorithm 1 is the most straightfor-\nSince only the matrix multiply occurs in int8 precision                ward version of SwitchBack, we also present two al-\nwe need to dequantize the outputs back to the original                 ternative versions\u2014SwitchBackM and SwitchBackQ\u2014\nfloating point precision. Consequently, the forward                    and release triton [57] implementations for all three\npass with quantization and dequantization becomes                      via the bitsandbytes library [18]. Appendix B contains\n                                                                       pseudocode. SwitchBackM (Algorithm 3) is a memory\n state tensor(W)    \u00b7state row (X)   \u2217 Q row  (X  )Q  tensor (W)  \u22a4    efficient version of SwitchBack which only saves 8 bit\n       127 2                           |            {z             }   tensors for the backwards pass\u2014we recommend its\n                                               int8 matmul             use when memory is limited.               The small downside\n                                                                 (3)   of SwitchBackM is that it requires an additional de-\n                                                                       quantize operation during the backwards pass which\nwhere   \u2217  denotes elementwise-multiplication, which in                increases the runtime overhead. For CLIP ViT-Huge\nthis case is broadcasted so\u22a4that row             i of the matrix       we observed only a negligible accuracy differences be-\nQ row  (X)  Q tensor  (W)     is multiplied by element          i of   tween SwitchBack and SwitchBackM. In addition, we\nstate row (X).                                                         present SwitchBackQ (Algorithm 4) which uses row-\nAs mentioned previously, we use row-wise quantization                  wise and column-wise quantization for the weights\nfor the inputs and gradients and tensor-wise quanti-                   instead of tensor-wise. While we did not observe this\nzation for the weights. We find that using row-wise                    to improve accuracy at the scales we consider, it\u2019s\nquantization for both matrices increases complexity                    possible that it will perform better than SwitchBack\nat a negligible or no performance increase. As such,                   at larger scale. For SwitchBackQ, the forward pass is\nwe use this simpler approach.                                          given by\nThe last detail in our algorithm is hardware specific.                    1 2 state row (X)state   row (W)  \u22a4  \u2217 Q  row (X)Q    row (W)   \u22a4\nNVIDIA GPUs, which we use in this work, do not                          127                                      |           {z            }\nimplement the int8/float8 operation            AB    for matrices                                                       int8 matmul      (4)\nA  and  B   and only    AB  T  is implemented. As such, it is\nnecessary to transpose the weight matrix in the back-                  where    \u2217 is an elementwise product. Again, we append\nward pass. To reduce the overhead of transposition                   4   transpose     to a function in Algorithm 4 to mean that", "md": "|ViT-Base model|ViT-Huge model|ViT-Base model|ViT-Huge model|\n|---|---|---|---|\n|5|5|5|5|\n|4|4|4|4|\n|3| |3| |\n|Loss|Loss|Loss|Loss|\n|2|1|2|1|\n|1|0|1|0|\n\nFigure 2: Loss curves for the CLIP ViT-Base and CLIP ViT-Huge models evaluated in Figure 1. The left two plots display results for int8 training while the right two plots display results for float8 (fp8) training.\n\n$$\n\\text{state row}(X) = [absmax(x_1), ..., absmax(x_b)]^T \\in \\mathbb{R}^{b \\times 1}\n$$\n\nEquivalently, for tensor-wise quantization we only need to store the tensor-wise absolute maximum so\n\n$$\n\\text{state tensor}(X) = absmax(X) \\in \\mathbb{R}\n$$\n\nThe forward pass with quantization and dequantization becomes\n\n$$\n\\text{state tensor}(W) \\cdot \\text{state row}(X) * Q_{\\text{row}}(X)Q_{\\text{tensor}}(W)^T\n$$\n\nwhere $*$ denotes elementwise-multiplication, which in this case is broadcasted so that row $i$ of the matrix $Q_{\\text{row}}(X)Q_{\\text{tensor}}(W)$ is multiplied by element $i$ of state row $(X)$.\n\nAs mentioned previously, we use row-wise quantization for the inputs and gradients and tensor-wise quantization for the weights. We find that using row-wise quantization for both matrices increases complexity at a negligible or no performance increase. As such, we use this simpler approach.\n\nThe last detail in our algorithm is hardware specific. NVIDIA GPUs, which we use in this work, do not implement the int8/float8 operation $AB$ for matrices $A$ and $B$ and only $AB^T$ is implemented. As such, it is necessary to transpose the weight matrix in the backward pass. To reduce the overhead of transposition, we fuse both operations, meaning we load the required data once from slow DRAM into fast SRAM/shared memory and then perform both operations in this cached memory \u2013 this is critical for achieving speedups. We call this operation tensor-wise quantize transpose, which is a fused tensor-wise quantize and transpose operation.\n\nPutting the pieces together, the result is Algorithm 1. Variants. While Algorithm 1 is the most straightforward version of SwitchBack, we also present two alternative versions\u2014SwitchBackM and SwitchBackQ\u2014and release triton implementations for all three via the bitsandbytes library. Appendix B contains pseudocode. SwitchBackM (Algorithm 3) is a memory efficient version of SwitchBack which only saves 8 bit tensors for the backwards pass\u2014we recommend its use when memory is limited. The small downside of SwitchBackM is that it requires an additional dequantize operation during the backwards pass which increases the runtime overhead. For CLIP ViT-Huge we observed only a negligible accuracy differences between SwitchBack and SwitchBackM. In addition, we present SwitchBackQ (Algorithm 4) which uses row-wise and column-wise quantization for the weights instead of tensor-wise. While we did not observe this to improve accuracy at the scales we consider, it\u2019s possible that it will perform better than SwitchBack at larger scale. For SwitchBackQ, the forward pass is given by\n\n$$\n\\text{state row}(X)\\text{state row}(W)^T * Q_{\\text{row}}(X)Q_{\\text{row}}(W)^T\n$$\n\nwhere $*$ is an elementwise product.", "images": [], "items": [{"type": "table", "rows": [["ViT-Base model", "ViT-Huge model", "ViT-Base model", "ViT-Huge model"], ["5", "5", "5", "5"], ["4", "4", "4", "4"], ["3", "", "3", ""], ["Loss", "Loss", "Loss", "Loss"], ["2", "1", "2", "1"], ["1", "0", "1", "0"]], "md": "|ViT-Base model|ViT-Huge model|ViT-Base model|ViT-Huge model|\n|---|---|---|---|\n|5|5|5|5|\n|4|4|4|4|\n|3| |3| |\n|Loss|Loss|Loss|Loss|\n|2|1|2|1|\n|1|0|1|0|", "isPerfectTable": true, "csv": "\"ViT-Base model\",\"ViT-Huge model\",\"ViT-Base model\",\"ViT-Huge model\"\n\"5\",\"5\",\"5\",\"5\"\n\"4\",\"4\",\"4\",\"4\"\n\"3\",\"\",\"3\",\"\"\n\"Loss\",\"Loss\",\"Loss\",\"Loss\"\n\"2\",\"1\",\"2\",\"1\"\n\"1\",\"0\",\"1\",\"0\""}, {"type": "text", "value": "Figure 2: Loss curves for the CLIP ViT-Base and CLIP ViT-Huge models evaluated in Figure 1. The left two plots display results for int8 training while the right two plots display results for float8 (fp8) training.\n\n$$\n\\text{state row}(X) = [absmax(x_1), ..., absmax(x_b)]^T \\in \\mathbb{R}^{b \\times 1}\n$$\n\nEquivalently, for tensor-wise quantization we only need to store the tensor-wise absolute maximum so\n\n$$\n\\text{state tensor}(X) = absmax(X) \\in \\mathbb{R}\n$$\n\nThe forward pass with quantization and dequantization becomes\n\n$$\n\\text{state tensor}(W) \\cdot \\text{state row}(X) * Q_{\\text{row}}(X)Q_{\\text{tensor}}(W)^T\n$$\n\nwhere $*$ denotes elementwise-multiplication, which in this case is broadcasted so that row $i$ of the matrix $Q_{\\text{row}}(X)Q_{\\text{tensor}}(W)$ is multiplied by element $i$ of state row $(X)$.\n\nAs mentioned previously, we use row-wise quantization for the inputs and gradients and tensor-wise quantization for the weights. We find that using row-wise quantization for both matrices increases complexity at a negligible or no performance increase. As such, we use this simpler approach.\n\nThe last detail in our algorithm is hardware specific. NVIDIA GPUs, which we use in this work, do not implement the int8/float8 operation $AB$ for matrices $A$ and $B$ and only $AB^T$ is implemented. As such, it is necessary to transpose the weight matrix in the backward pass. To reduce the overhead of transposition, we fuse both operations, meaning we load the required data once from slow DRAM into fast SRAM/shared memory and then perform both operations in this cached memory \u2013 this is critical for achieving speedups. We call this operation tensor-wise quantize transpose, which is a fused tensor-wise quantize and transpose operation.\n\nPutting the pieces together, the result is Algorithm 1. Variants. While Algorithm 1 is the most straightforward version of SwitchBack, we also present two alternative versions\u2014SwitchBackM and SwitchBackQ\u2014and release triton implementations for all three via the bitsandbytes library. Appendix B contains pseudocode. SwitchBackM (Algorithm 3) is a memory efficient version of SwitchBack which only saves 8 bit tensors for the backwards pass\u2014we recommend its use when memory is limited. The small downside of SwitchBackM is that it requires an additional dequantize operation during the backwards pass which increases the runtime overhead. For CLIP ViT-Huge we observed only a negligible accuracy differences between SwitchBack and SwitchBackM. In addition, we present SwitchBackQ (Algorithm 4) which uses row-wise and column-wise quantization for the weights instead of tensor-wise. While we did not observe this to improve accuracy at the scales we consider, it\u2019s possible that it will perform better than SwitchBack at larger scale. For SwitchBackQ, the forward pass is given by\n\n$$\n\\text{state row}(X)\\text{state row}(W)^T * Q_{\\text{row}}(X)Q_{\\text{row}}(W)^T\n$$\n\nwhere $*$ is an elementwise product.", "md": "Figure 2: Loss curves for the CLIP ViT-Base and CLIP ViT-Huge models evaluated in Figure 1. The left two plots display results for int8 training while the right two plots display results for float8 (fp8) training.\n\n$$\n\\text{state row}(X) = [absmax(x_1), ..., absmax(x_b)]^T \\in \\mathbb{R}^{b \\times 1}\n$$\n\nEquivalently, for tensor-wise quantization we only need to store the tensor-wise absolute maximum so\n\n$$\n\\text{state tensor}(X) = absmax(X) \\in \\mathbb{R}\n$$\n\nThe forward pass with quantization and dequantization becomes\n\n$$\n\\text{state tensor}(W) \\cdot \\text{state row}(X) * Q_{\\text{row}}(X)Q_{\\text{tensor}}(W)^T\n$$\n\nwhere $*$ denotes elementwise-multiplication, which in this case is broadcasted so that row $i$ of the matrix $Q_{\\text{row}}(X)Q_{\\text{tensor}}(W)$ is multiplied by element $i$ of state row $(X)$.\n\nAs mentioned previously, we use row-wise quantization for the inputs and gradients and tensor-wise quantization for the weights. We find that using row-wise quantization for both matrices increases complexity at a negligible or no performance increase. As such, we use this simpler approach.\n\nThe last detail in our algorithm is hardware specific. NVIDIA GPUs, which we use in this work, do not implement the int8/float8 operation $AB$ for matrices $A$ and $B$ and only $AB^T$ is implemented. As such, it is necessary to transpose the weight matrix in the backward pass. To reduce the overhead of transposition, we fuse both operations, meaning we load the required data once from slow DRAM into fast SRAM/shared memory and then perform both operations in this cached memory \u2013 this is critical for achieving speedups. We call this operation tensor-wise quantize transpose, which is a fused tensor-wise quantize and transpose operation.\n\nPutting the pieces together, the result is Algorithm 1. Variants. While Algorithm 1 is the most straightforward version of SwitchBack, we also present two alternative versions\u2014SwitchBackM and SwitchBackQ\u2014and release triton implementations for all three via the bitsandbytes library. Appendix B contains pseudocode. SwitchBackM (Algorithm 3) is a memory efficient version of SwitchBack which only saves 8 bit tensors for the backwards pass\u2014we recommend its use when memory is limited. The small downside of SwitchBackM is that it requires an additional dequantize operation during the backwards pass which increases the runtime overhead. For CLIP ViT-Huge we observed only a negligible accuracy differences between SwitchBack and SwitchBackM. In addition, we present SwitchBackQ (Algorithm 4) which uses row-wise and column-wise quantization for the weights instead of tensor-wise. While we did not observe this to improve accuracy at the scales we consider, it\u2019s possible that it will perform better than SwitchBack at larger scale. For SwitchBackQ, the forward pass is given by\n\n$$\n\\text{state row}(X)\\text{state row}(W)^T * Q_{\\text{row}}(X)Q_{\\text{row}}(W)^T\n$$\n\nwhere $*$ is an elementwise product."}]}, {"page": 5, "text": "       Standard fp16 (sum of parts)             Linear layer, batch * sequence length = 32k   35   Linear layer summary, varying dimensions\n       SwitchBack int8 (sum of parts)      50\n       Average fp16 matmul                                                                    30\n       Average int8 matmul                 40\n       Average quantize operation                                                             25\n                                           30                                                 20\n                                           20\n                                          time (ms)                                           15\n                                                                                             % speedup      batch * sequence length = 16384\n                                           10                                                 10            batch * sequence length = 32768\n                                                                                                            batch * sequence length = 65536\n                                             0                                                  5           batch * sequence length = 131072\n                                               1024               2048               4096         1024               2048               4096\n                                                                  dim                                                dim\nFigure 3:   (Left)   Individually profiling operations which constitute a forward and backward pass in a linear layer for i)\nSwitchBack using triton kernels and ii) an fp16 baseline using               torch.matmul. Times are averaged over a linear layer\nfrom   dim  to 4  \u00b7dim  and a linear layer from 4       \u00b7dim  to  dim\u2014representative of the linear layers in a transformer MLP.\n(Right)    The % speedup of SwitchBack over a standard fp16 linear layer when all operations in Figure 3 (left) are\nsummed.\nthe operation is fused with a transpose.                                 or approximately 2.9e20 FLOPs per training run.After\n                                                                         training on LAION-2B we evaluate the models zero-\nfloat8.     While the explanation so far has used int8                   shot on ImageNet [15] using the 80 prompt templates\nas an example, the code for SwitchBack and float8                        from CLIP [46].\n(fp8) is nearly identical.          The only modification is\nthat operations such as          round(127x/absmax(x)) are               We use batch size 16384 (per-gpu batch size of 256)\nreplaced by      float8cast(x/absmax(x)) where we simu-                  and train for a total of 20k iterations. The first 5k iter-\nlate  float8cast    through bitsandbytes by rounding to                  ations are linear warmup while the remaining 15k are\nthe exact values of the float8 data type. This simu-                     cosine decay. Training and evaluation are conducted\nlation improves on the simulation of [40] which only                     with the OpenCLIP library [29] with learning rate\nclips the input tensors into the representable range                     2e-3, weight decay 0.2, and batch-size 16384 using the\nof the float8 data type, but not the exact values of                     optimizer described in Section 3.5.\nthe float8 data type.          This simulation theoretically\nmatches float8 training, but we are unable to per-                       2.2.3     Results\nform real float8 training because we lack the hardware\nthat supports float8 arithmetic. As such, we perform                     We test two main questions: (1) can we replicate 16-\narithmetic in 16-bit with exact float8 values. For our                   bit performance with SwitchBack and (2) can we get\nint8 experiments we conduct the multiplications in                       speedups.      To test (1) we train CLIP models with\nint8 using A100 GPUs\u2014we perform real int8 training                       SwitchBack across multiple scales with both int8 and\nwithout any simulation.                                                  float8 precision (Figures 1 and 2).              To test (2) we\n                                                                         profile operations in an individual linear layer and\n2.2.2      Experimental setup                                            also measure end-to-end training speed.\n                                                                         Accuracy.      We find that SwitchBack can match stan-\nTo evaluate SwitchBack we train CLIP [46] visual                         dard 16-bit training performance and outperform base-\ntransformer [20] models on LAION-2B [53].                      Typi-     lines for both a) int8 precision and b) float8 precision.\ncally CLIP training, especially at ViT-Huge scale, is\nprohibitively expensive.          Our goal is not high final             For our int8 experiments (Figures 1 and 2 left), we\naccuracy but rather to contrast different methods for                    contrast the performance of i) the standard baseline\nlow-precision training.          To enable running multiple              which uses mixed-precision bfloat16, ii) the matrix\nexperiments, we therefore only train for a small num-                    multiplication kernels from LLM.int8() [17], which\nber of samples seen\u2014380 million images\u2014and use                           is equivalent to SwitchBackQ (Algorithm 4) if the\npatch-dropout 0.5 [35]. We note that the experiment                      weight gradient multiplication was also performed in\nis still very expensive, corresponding to roughly 300                    int8 using row- and column-wise quantization, and\nepochs of ImageNet training in terms of samples seen,                 5  iii) SwitchBack. SwitchBack has a negligible accuracy", "md": "Standard fp16 (sum of parts) Linear layer, batch * sequence length = 32k 35 Linear layer summary, varying dimensions\n\nSwitchBack int8 (sum of parts) 50\n\nAverage fp16 matmul 30\n\nAverage int8 matmul 40\n\nAverage quantize operation 25\n\n| |30|20|\n|---|---|---|\n|time (ms)|15| |\n| |10|10|\n| |5| |\n| |1024|2048|4096|\n| |dim| | | |\n\nFigure 3: (Left) Individually profiling operations which constitute a forward and backward pass in a linear layer for i) SwitchBack using triton kernels and ii) an fp16 baseline using $$\\text{torch.matmul}$$. Times are averaged over a linear layer from dim to 4 \u00b7 dim and a linear layer from 4 \u00b7 dim to dim\u2014representative of the linear layers in a transformer MLP. (Right) The % speedup of SwitchBack over a standard fp16 linear layer when all operations in Figure 3 (left) are summed. the operation is fused with a transpose. or approximately 2.9e20 FLOPs per training run.After training on LAION-2B we evaluate the models zero-shot on ImageNet [15] using the 80 prompt templates from CLIP [46].\n\nWe use batch size 16384 (per-gpu batch size of 256) and train for a total of 20k iterations. The first 5k iterations are linear warmup while the remaining 15k are cosine decay. Training and evaluation are conducted with the OpenCLIP library [29] with learning rate 2e-3, weight decay 0.2, and batch-size 16384 using the optimizer described in Section 3.5.\n\n2.2.3 Results\n\nWe test two main questions: (1) can we replicate 16-bit performance with SwitchBack and (2) can we get speedups. To test (1) we train CLIP models with SwitchBack across multiple scales with both int8 and float8 precision (Figures 1 and 2). To test (2) we profile operations in an individual linear layer and also measure end-to-end training speed. Accuracy. We find that SwitchBack can match standard 16-bit training performance and outperform baselines for both a) int8 precision and b) float8 precision. For our int8 experiments (Figures 1 and 2 left), we contrast the performance of i) the standard baseline which uses mixed-precision bfloat16, ii) the matrix multiplication kernels from LLM.int8() [17], which is equivalent to SwitchBackQ (Algorithm 4) if the weight gradient multiplication was also performed in int8 using row- and column-wise quantization, and iii) SwitchBack. SwitchBack has a negligible accuracy\n\nfloat8. While the explanation so far has used int8 as an example, the code for SwitchBack and float8 (fp8) is nearly identical. The only modification is that operations such as round(127x/absmax(x)) are replaced by float8cast(x/absmax(x)) where we simulate float8cast through bitsandbytes by rounding to the exact values of the float8 data type. This simulation improves on the simulation of [40] which only clips the input tensors into the representable range of the float8 data type, but not the exact values of the float8 data type. This simulation theoretically matches float8 training, but we are unable to perform real float8 training because we lack the hardware that supports float8 arithmetic. As such, we perform arithmetic in 16-bit with exact float8 values. For our int8 experiments we conduct the multiplications in int8 using A100 GPUs\u2014we perform real int8 training without any simulation.\n\n2.2.2 Experimental setup\n\nTo evaluate SwitchBack we train CLIP [46] visual transformer [20] models on LAION-2B [53]. Typically CLIP training, especially at ViT-Huge scale, is prohibitively expensive. Our goal is not high final accuracy but rather to contrast different methods for low-precision training. To enable running multiple experiments, we therefore only train for a small number of samples seen\u2014380 million images\u2014and use patch-dropout 0.5 [35]. We note that the experiment is still very expensive, corresponding to roughly 300 epochs of ImageNet training in terms of samples seen.", "images": [], "items": [{"type": "text", "value": "Standard fp16 (sum of parts) Linear layer, batch * sequence length = 32k 35 Linear layer summary, varying dimensions\n\nSwitchBack int8 (sum of parts) 50\n\nAverage fp16 matmul 30\n\nAverage int8 matmul 40\n\nAverage quantize operation 25", "md": "Standard fp16 (sum of parts) Linear layer, batch * sequence length = 32k 35 Linear layer summary, varying dimensions\n\nSwitchBack int8 (sum of parts) 50\n\nAverage fp16 matmul 30\n\nAverage int8 matmul 40\n\nAverage quantize operation 25"}, {"type": "table", "rows": [["", "30", "20"], ["time (ms)", "15", ""], ["", "10", "10"], ["", "5", ""], ["", "1024", "2048", "4096"], ["", "dim", "", "", ""]], "md": "| |30|20|\n|---|---|---|\n|time (ms)|15| |\n| |10|10|\n| |5| |\n| |1024|2048|4096|\n| |dim| | | |", "isPerfectTable": false, "csv": "\"\",\"30\",\"20\"\n\"time (ms)\",\"15\",\"\"\n\"\",\"10\",\"10\"\n\"\",\"5\",\"\"\n\"\",\"1024\",\"2048\",\"4096\"\n\"\",\"dim\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 3: (Left) Individually profiling operations which constitute a forward and backward pass in a linear layer for i) SwitchBack using triton kernels and ii) an fp16 baseline using $$\\text{torch.matmul}$$. Times are averaged over a linear layer from dim to 4 \u00b7 dim and a linear layer from 4 \u00b7 dim to dim\u2014representative of the linear layers in a transformer MLP. (Right) The % speedup of SwitchBack over a standard fp16 linear layer when all operations in Figure 3 (left) are summed. the operation is fused with a transpose. or approximately 2.9e20 FLOPs per training run.After training on LAION-2B we evaluate the models zero-shot on ImageNet [15] using the 80 prompt templates from CLIP [46].\n\nWe use batch size 16384 (per-gpu batch size of 256) and train for a total of 20k iterations. The first 5k iterations are linear warmup while the remaining 15k are cosine decay. Training and evaluation are conducted with the OpenCLIP library [29] with learning rate 2e-3, weight decay 0.2, and batch-size 16384 using the optimizer described in Section 3.5.\n\n2.2.3 Results\n\nWe test two main questions: (1) can we replicate 16-bit performance with SwitchBack and (2) can we get speedups. To test (1) we train CLIP models with SwitchBack across multiple scales with both int8 and float8 precision (Figures 1 and 2). To test (2) we profile operations in an individual linear layer and also measure end-to-end training speed. Accuracy. We find that SwitchBack can match standard 16-bit training performance and outperform baselines for both a) int8 precision and b) float8 precision. For our int8 experiments (Figures 1 and 2 left), we contrast the performance of i) the standard baseline which uses mixed-precision bfloat16, ii) the matrix multiplication kernels from LLM.int8() [17], which is equivalent to SwitchBackQ (Algorithm 4) if the weight gradient multiplication was also performed in int8 using row- and column-wise quantization, and iii) SwitchBack. SwitchBack has a negligible accuracy\n\nfloat8. While the explanation so far has used int8 as an example, the code for SwitchBack and float8 (fp8) is nearly identical. The only modification is that operations such as round(127x/absmax(x)) are replaced by float8cast(x/absmax(x)) where we simulate float8cast through bitsandbytes by rounding to the exact values of the float8 data type. This simulation improves on the simulation of [40] which only clips the input tensors into the representable range of the float8 data type, but not the exact values of the float8 data type. This simulation theoretically matches float8 training, but we are unable to perform real float8 training because we lack the hardware that supports float8 arithmetic. As such, we perform arithmetic in 16-bit with exact float8 values. For our int8 experiments we conduct the multiplications in int8 using A100 GPUs\u2014we perform real int8 training without any simulation.\n\n2.2.2 Experimental setup\n\nTo evaluate SwitchBack we train CLIP [46] visual transformer [20] models on LAION-2B [53]. Typically CLIP training, especially at ViT-Huge scale, is prohibitively expensive. Our goal is not high final accuracy but rather to contrast different methods for low-precision training. To enable running multiple experiments, we therefore only train for a small number of samples seen\u2014380 million images\u2014and use patch-dropout 0.5 [35]. We note that the experiment is still very expensive, corresponding to roughly 300 epochs of ImageNet training in terms of samples seen.", "md": "Figure 3: (Left) Individually profiling operations which constitute a forward and backward pass in a linear layer for i) SwitchBack using triton kernels and ii) an fp16 baseline using $$\\text{torch.matmul}$$. Times are averaged over a linear layer from dim to 4 \u00b7 dim and a linear layer from 4 \u00b7 dim to dim\u2014representative of the linear layers in a transformer MLP. (Right) The % speedup of SwitchBack over a standard fp16 linear layer when all operations in Figure 3 (left) are summed. the operation is fused with a transpose. or approximately 2.9e20 FLOPs per training run.After training on LAION-2B we evaluate the models zero-shot on ImageNet [15] using the 80 prompt templates from CLIP [46].\n\nWe use batch size 16384 (per-gpu batch size of 256) and train for a total of 20k iterations. The first 5k iterations are linear warmup while the remaining 15k are cosine decay. Training and evaluation are conducted with the OpenCLIP library [29] with learning rate 2e-3, weight decay 0.2, and batch-size 16384 using the optimizer described in Section 3.5.\n\n2.2.3 Results\n\nWe test two main questions: (1) can we replicate 16-bit performance with SwitchBack and (2) can we get speedups. To test (1) we train CLIP models with SwitchBack across multiple scales with both int8 and float8 precision (Figures 1 and 2). To test (2) we profile operations in an individual linear layer and also measure end-to-end training speed. Accuracy. We find that SwitchBack can match standard 16-bit training performance and outperform baselines for both a) int8 precision and b) float8 precision. For our int8 experiments (Figures 1 and 2 left), we contrast the performance of i) the standard baseline which uses mixed-precision bfloat16, ii) the matrix multiplication kernels from LLM.int8() [17], which is equivalent to SwitchBackQ (Algorithm 4) if the weight gradient multiplication was also performed in int8 using row- and column-wise quantization, and iii) SwitchBack. SwitchBack has a negligible accuracy\n\nfloat8. While the explanation so far has used int8 as an example, the code for SwitchBack and float8 (fp8) is nearly identical. The only modification is that operations such as round(127x/absmax(x)) are replaced by float8cast(x/absmax(x)) where we simulate float8cast through bitsandbytes by rounding to the exact values of the float8 data type. This simulation improves on the simulation of [40] which only clips the input tensors into the representable range of the float8 data type, but not the exact values of the float8 data type. This simulation theoretically matches float8 training, but we are unable to perform real float8 training because we lack the hardware that supports float8 arithmetic. As such, we perform arithmetic in 16-bit with exact float8 values. For our int8 experiments we conduct the multiplications in int8 using A100 GPUs\u2014we perform real int8 training without any simulation.\n\n2.2.2 Experimental setup\n\nTo evaluate SwitchBack we train CLIP [46] visual transformer [20] models on LAION-2B [53]. Typically CLIP training, especially at ViT-Huge scale, is prohibitively expensive. Our goal is not high final accuracy but rather to contrast different methods for low-precision training. To enable running multiple experiments, we therefore only train for a small number of samples seen\u2014380 million images\u2014and use patch-dropout 0.5 [35]. We note that the experiment is still very expensive, corresponding to roughly 300 epochs of ImageNet training in terms of samples seen."}]}, {"page": 6, "text": "       25                             batch * sequence length = 16384        25      speedup over baseline (torch.autograd Linear)\n                                      batch * sequence length = 32768                speedup over pytorch optimized linear\n                                      batch * sequence length = 65536        20\n       20                             batch * sequence length = 131072       15\n       15                                                                    10\n                                                                            % speedup\n       10                                                                     5\n      Quantization overhead\n          1024                      2048                     4096             0 ViT-Base                ViT-Large                 ViT-Huge\n                                    dim\nFigure 4:   (Left)    Measuring the % of time occupied by quantize operations for a SwitchBack linear layer, which is\nusually less than 20% and decreases with            dim.  (Right)    Benchmarking speedups for end-to-end CLIP training on a\nsingle node (with 4 A100 GPUs, per-GPU batch size 256, and gradient checkpointing) for various model sizes when\nreplacing all linear operations in the transformer with SwitchBack (i.e., key, query, value, and out projections as well\nas the MLP). speedups reported over i) a custom linear layer implemented with                     torch.autograd    (Algorithm 5), which\nmatches our implementation of SwitchBack that uses                 torch.autograd, and ii) using the standard PyTorch              nn.Linear\nwhich includes additional background C++/CUDA optimizations which we do not replicate. LLM.int8() [17] does not\nprovide speed-ups over the        torch.autograd    or  nn.Linear  baseline at this scale\u2014we compare the speed of SwitchBack\nand LLM.int8() in Figure 13.\ndrop of 0.1 percentage points compared to the bfloat16                  Figure 3 (right) displays the % speedup of Switch-\nbaseline for CLIP ViT-Huge.               In contrast, there is         Back over a standard fp16 layer when all opera-\na drop of 5.9 percentage points when training with                      tions in Figure 3 (left) are summed.                 Overall, the\nLLM.int8(). Section C details our hypothesis for why                    advantage of SwitchBack is greater for larger                    dim\nLLM.int8() fails to replicate 16-bit performance for                    and   batch size\u2217    sequence length. Overall, the speedup\nCLIP training.                                                          ranges from 5% to 35%. We see a bump at                dim   = 1280\n                                                                        because standard PyTorch matmuls do not have op-\nFor our simulated float8 training experiments (Fig-                     timized kernels for matrices of this size while we use\nures 1 and 2 right), we contrast the performance of                     triton\u2019s autotune feature which provides fine-grained\ni) the standard baseline which uses mixed-precision                     optimized kernels for matrices of any size. Our kernels\nbfloat16, ii) a baseline which uses tensor-wise quan-                   are easy to modify as they are written in Triton [57],\ntization for all matrices, that is the weights, inputs,                 and the code to run the benchmarks and produce\nand gradients, and iii) SwitchBack. SwitchBack has                      Figure 3 is open sourced. In doing so, we invite the\na negligible accuracy drop of 0.1 percentage points                     community to further improve the kernels and provide\nfrom the bfloat16 baseline for CLIP ViT-Huge.                      In   a benchmark for measuring this progress. Due to com-\ncontrast, training diverges for the baseline that uses                  putational constraints we have not tested              dim   >  4096\ntensor-wise quantization for all matrices.                              and it\u2019s possible the kernels require additional tuning\nSpeed.     We now test the speedups offered by Switch-                  to perform well at that scale.\nBack by first examining individual operations and\nthen end-to-end training.                                               One downside of SwitchBack is that it requires quan-\n                                                                        tize operations. However, it is already evident from\nWe profile all of the operations which constitute a                     Figure 3 that quantize operations occupy a small\nforward and backward pass for a single linear layer in                  amount of time compared to matmuls. This is high-\nFigure 3 (left) for both SwitchBack and the baseline.                   lighted by Figure 4 (left) which displays the fraction\nFor SwitchBack we profile our custom triton kernels                     of time occupied by quantize operations relative to\nand for the baseline we profile          torch.matmul. Overall,         matmuls for SwitchBack linear layers. Quantize oper-\nwe observe that int8 multiplies occupy just over half                   ations occupy at most 25% of the time, this fraction\nthe time as standard fp16 matmuls, and that quantize                    decreases to around 10% or below for large                dim.\noperations are roughly an order of magnitude less\ntime than a matmul. Note that our int8 matmuls are                      We now conduct end-to-end speed tests for CLIP\nfused with the dequantize operation.                                  6 training on a single node with 4x A100 GPUs (Fig-", "md": "$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Quantization overhead} & \\text{ViT-Base} & \\text{ViT-Large} & \\text{ViT-Huge} \\\\\n\\hline\n1024 & & & \\\\\n2048 & & & \\\\\n4096 & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 4: (Left) Measuring the % of time occupied by quantize operations for a SwitchBack linear layer, which is usually less than 20% and decreases with dim. (Right) Benchmarking speedups for end-to-end CLIP training on a single node (with 4 A100 GPUs, per-GPU batch size 256, and gradient checkpointing) for various model sizes when replacing all linear operations in the transformer with SwitchBack (i.e., key, query, value, and out projections as well as the MLP). Speedups reported over i) a custom linear layer implemented with torch.autograd (Algorithm 5), which matches our implementation of SwitchBack that uses torch.autograd, and ii) using the standard PyTorch nn.Linear which includes additional background C++/CUDA optimizations which we do not replicate. LLM.int8() [17] does not provide speed-ups over the torch.autograd or nn.Linear baseline at this scale\u2014we compare the speed of SwitchBack and LLM.int8() in Figure 13.\n\nDrop of 0.1 percentage points compared to the bfloat16 baseline for CLIP ViT-Huge. In contrast, there is a drop of 5.9 percentage points when training with LLM.int8(). Section C details our hypothesis for why LLM.int8() fails to replicate 16-bit performance for CLIP training.\n\nFor our simulated float8 training experiments (Figures 1 and 2 right), we contrast the performance of i) the standard baseline which uses mixed-precision bfloat16, ii) a baseline which uses tensor-wise quantization for all matrices, that is the weights, inputs, and gradients, and iii) SwitchBack. SwitchBack has a negligible accuracy drop of 0.1 percentage points from the bfloat16 baseline for CLIP ViT-Huge. In contrast, training diverges for the baseline that uses tensor-wise quantization for all matrices.\n\nSpeed. We now test the speedups offered by SwitchBack by first examining individual operations and then end-to-end training.\n\nWe profile all of the operations which constitute a forward and backward pass for a single linear layer in Figure 3 (left) for both SwitchBack and the baseline. For SwitchBack we profile our custom triton kernels and for the baseline we profile torch.matmul. Overall, we observe that int8 multiplies occupy just over half the time as standard fp16 matmuls, and that quantize operations are roughly an order of magnitude less time than a matmul. Note that our int8 matmuls are fused with the dequantize operation.\n\nWe now conduct end-to-end speed tests for CLIP training on a single node with 4x A100 GPUs (Fig-6).", "images": [], "items": [{"type": "text", "value": "$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Quantization overhead} & \\text{ViT-Base} & \\text{ViT-Large} & \\text{ViT-Huge} \\\\\n\\hline\n1024 & & & \\\\\n2048 & & & \\\\\n4096 & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 4: (Left) Measuring the % of time occupied by quantize operations for a SwitchBack linear layer, which is usually less than 20% and decreases with dim. (Right) Benchmarking speedups for end-to-end CLIP training on a single node (with 4 A100 GPUs, per-GPU batch size 256, and gradient checkpointing) for various model sizes when replacing all linear operations in the transformer with SwitchBack (i.e., key, query, value, and out projections as well as the MLP). Speedups reported over i) a custom linear layer implemented with torch.autograd (Algorithm 5), which matches our implementation of SwitchBack that uses torch.autograd, and ii) using the standard PyTorch nn.Linear which includes additional background C++/CUDA optimizations which we do not replicate. LLM.int8() [17] does not provide speed-ups over the torch.autograd or nn.Linear baseline at this scale\u2014we compare the speed of SwitchBack and LLM.int8() in Figure 13.\n\nDrop of 0.1 percentage points compared to the bfloat16 baseline for CLIP ViT-Huge. In contrast, there is a drop of 5.9 percentage points when training with LLM.int8(). Section C details our hypothesis for why LLM.int8() fails to replicate 16-bit performance for CLIP training.\n\nFor our simulated float8 training experiments (Figures 1 and 2 right), we contrast the performance of i) the standard baseline which uses mixed-precision bfloat16, ii) a baseline which uses tensor-wise quantization for all matrices, that is the weights, inputs, and gradients, and iii) SwitchBack. SwitchBack has a negligible accuracy drop of 0.1 percentage points from the bfloat16 baseline for CLIP ViT-Huge. In contrast, training diverges for the baseline that uses tensor-wise quantization for all matrices.\n\nSpeed. We now test the speedups offered by SwitchBack by first examining individual operations and then end-to-end training.\n\nWe profile all of the operations which constitute a forward and backward pass for a single linear layer in Figure 3 (left) for both SwitchBack and the baseline. For SwitchBack we profile our custom triton kernels and for the baseline we profile torch.matmul. Overall, we observe that int8 multiplies occupy just over half the time as standard fp16 matmuls, and that quantize operations are roughly an order of magnitude less time than a matmul. Note that our int8 matmuls are fused with the dequantize operation.\n\nWe now conduct end-to-end speed tests for CLIP training on a single node with 4x A100 GPUs (Fig-6).", "md": "$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Quantization overhead} & \\text{ViT-Base} & \\text{ViT-Large} & \\text{ViT-Huge} \\\\\n\\hline\n1024 & & & \\\\\n2048 & & & \\\\\n4096 & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 4: (Left) Measuring the % of time occupied by quantize operations for a SwitchBack linear layer, which is usually less than 20% and decreases with dim. (Right) Benchmarking speedups for end-to-end CLIP training on a single node (with 4 A100 GPUs, per-GPU batch size 256, and gradient checkpointing) for various model sizes when replacing all linear operations in the transformer with SwitchBack (i.e., key, query, value, and out projections as well as the MLP). Speedups reported over i) a custom linear layer implemented with torch.autograd (Algorithm 5), which matches our implementation of SwitchBack that uses torch.autograd, and ii) using the standard PyTorch nn.Linear which includes additional background C++/CUDA optimizations which we do not replicate. LLM.int8() [17] does not provide speed-ups over the torch.autograd or nn.Linear baseline at this scale\u2014we compare the speed of SwitchBack and LLM.int8() in Figure 13.\n\nDrop of 0.1 percentage points compared to the bfloat16 baseline for CLIP ViT-Huge. In contrast, there is a drop of 5.9 percentage points when training with LLM.int8(). Section C details our hypothesis for why LLM.int8() fails to replicate 16-bit performance for CLIP training.\n\nFor our simulated float8 training experiments (Figures 1 and 2 right), we contrast the performance of i) the standard baseline which uses mixed-precision bfloat16, ii) a baseline which uses tensor-wise quantization for all matrices, that is the weights, inputs, and gradients, and iii) SwitchBack. SwitchBack has a negligible accuracy drop of 0.1 percentage points from the bfloat16 baseline for CLIP ViT-Huge. In contrast, training diverges for the baseline that uses tensor-wise quantization for all matrices.\n\nSpeed. We now test the speedups offered by SwitchBack by first examining individual operations and then end-to-end training.\n\nWe profile all of the operations which constitute a forward and backward pass for a single linear layer in Figure 3 (left) for both SwitchBack and the baseline. For SwitchBack we profile our custom triton kernels and for the baseline we profile torch.matmul. Overall, we observe that int8 multiplies occupy just over half the time as standard fp16 matmuls, and that quantize operations are roughly an order of magnitude less time than a matmul. Note that our int8 matmuls are fused with the dequantize operation.\n\nWe now conduct end-to-end speed tests for CLIP training on a single node with 4x A100 GPUs (Fig-6)."}]}, {"page": 7, "text": "                              ViT-Large model\n        10                               1.0                                   5        standard at init\n         8                               0.5                                            standard final\n                                                                               4        zero-init layer scale at init\n         6                                                                     3        zero-init layer scale final\n       Loss4\n         2                                                                     2\n             0   2500   5000  7500 10000 12500  15000 17500 20000              1\n                                Iterations                                    Average feature magnitude\n          i) bfloat16 baseline      iv) fp8 tensor + KQ Layernorm              0    0               10                20               30\n          ii) fp8 tensor quantize   v) fp8 tensor + zero-init layerscale                         Transformer block index\n          iii) fp8 tensor + grad clip\n Figure 5:   (Left)  Training CLIP ViT-Large models with simulated fp8 precision using tensor-wise quantization for the\n inputs, weights, and gradients. All methods we try diverge except for using                  zero-init layerscale    [58], which multiplies\n the output of each self-attention or mlp block with a learnable vector initialized to zero.                  (Right)     Examining feature\n magnitudes (i.e., the average absolute value of the output for transformer block                k) for CLIP ViT-Huge at the beginning\n(init) and end of training. This suggest why zero-init layer scale enables float8 training\u2014zero-init layer scale prevents\n high feature magnitudes which may cause issues for low precision training [17]. Without the intervention, the average\n feature magnitude becomes large for later blocks.\n ure 4, right).     This is in contrast with the speedup                  discourages large feature magnitudes. We accomplish\n measurements so far in this which have measured indi-                    via layer-scale [58] initialized to zero.\nvidual layers independently. We benchmark speedups\n relative to using i) a baseline linear layer which we                    We use the bitsandbytes library [18] to simulate float8\n implement in PyTorch with            torch.autograd.linear       (Al-    training using the fp8 types from Micikevicius et al.\n gorithm 5) and ii) the PyTorch optimized linear layer                    [40]. We use tensor-wise quantization for the inputs,\n nn.Linear. In both cases the speedups increase when                      weights, and gradients, so that all operations occur\n going from CLIP ViT-Base to CLIP ViT-Huge. How-                          in simulated float8. In our simulation, we represent\n ever, there is an additional           \u223c12.5% speedup when               each value only with the exact values representable\n comparing SwitchBack to the baseline linear layer                        by float8, but we perform computations in float16\nwhich uses     torch.autograd. We believe this comparison                 precision. We believe that tensor-wise quantization\n is fair because SwitchBack is also implemented using                     approximates the removal of quantize operations en-\n torch.autograd, while the standard PyTorch                nn.Linear      tirely. This is because, as we show in Appendix B.2\n layer has additional C++ and CUDA optimizations                         (Figure 14), the maximum of these tensors tends to\n that we do not implement.                We hope to collabo-             evolve smoothly. Consequently, using a moving aver-\n rate with the PyTorch team to realize the additional                     age for a maximum which is divided directly in the\n \u223c12.5% speedup. Finally, we note that the kernels                        matmul is similar to tensor-wise quantization.\n from LLM.int8() [17] do not provide speedups over                        Layer-scale, introduced by Touvron et al.               [58], scales\n fp16 at the scale we consider.                                           each self-attention and MLP block output hidden\n                                                                          state by a learnable vector of shape               embed dim.       A\n                                                                          pre-norm transformer block with layer-scale tensors\n 2.3      Float8 training by reducing feature                             \u03b31  and   \u03b32  is defined as\n          magnitude                                                                 x \u2032 =  x k +  \u03b3 1 \u2217 self attention(norm      1(x k ))    (5)\nWe find that SwitchBack is necessary for high accu-                              x k+1k =  x \u2032 +  \u03b3 2 \u2217 mlp(norm     2(x \u2032)),                (6)\n racy int8 training.        However, this section develops                                   k                           k\n other interventions which enable float8 training with-                   where    \u2217 is broadcasted elementwise multiplication.\n out SwitchBack. We show that high accuracy can be\n achieved via float8 training with tensor-wise quantiza-                  Typically, layers are initialized so that they approx-\n tion for the inputs, weights, and gradients, so long as                  imately preserve the variance of their inputs, and\n the network is initialized and trained in a way which                    inputs have approximately unit variance [26,              27]. How-\n                                                                       7", "md": "# ViT-Large model\n\n## ViT-Large model\n\n$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Iterations} & \\text{Average feature magnitude} \\\\\n\\hline\n\\text{i) bfloat16 baseline} & 0, 0, 10, 20, 30 \\\\\n\\text{ii) fp8 tensor quantize} & \\\\\n\\text{iii) fp8 tensor + grad clip} & \\\\\n\\text{iv) fp8 tensor + KQ Layernorm} & \\\\\n\\text{v) fp8 tensor + zero-init layerscale} & \\text{Transformer block index} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 5: (Left) Training CLIP ViT-Large models with simulated fp8 precision using tensor-wise quantization for the inputs, weights, and gradients. All methods we try diverge except for using zero-init layerscale [58], which multiplies the output of each self-attention or mlp block with a learnable vector initialized to zero. (Right) Examining feature magnitudes (i.e., the average absolute value of the output for transformer block k) for CLIP ViT-Huge at the beginning (init) and end of training. This suggests why zero-init layer scale enables float8 training\u2014zero-init layer scale prevents high feature magnitudes which may cause issues for low precision training [17]. Without the intervention, the average feature magnitude becomes large for later blocks.\n\nWe use the bitsandbytes library [18] to simulate float8 training using the fp8 types from Micikevicius et al. [40]. We use tensor-wise quantization for the inputs, weights, and gradients, so that all operations occur in simulated float8. In our simulation, we represent each value only with the exact values representable by float8, but we perform computations in float16 precision. We believe that tensor-wise quantization approximates the removal of quantize operations entirely. This is because, as we show in Appendix B.2 (Figure 14), the maximum of these tensors tends to evolve smoothly. Consequently, using a moving average for a maximum which is divided directly in the matmul is similar to tensor-wise quantization.\n\nLayer-scale, introduced by Touvron et al. [58], scales each self-attention and MLP block output hidden state by a learnable vector of shape embed dim. A pre-norm transformer block with layer-scale tensors \u03b31 and \u03b32 is defined as\n\n$$\n\\begin{align*}\nx' & = x_k + \\gamma_1 * \\text{self attention(norm}_1(x_k)) \\quad (5) \\\\\nx_{k+1} & = x' + \\gamma_2 * \\text{mlp(norm}_2(x')) \\quad (6)\n\\end{align*}\n$$\n\nTypically, layers are initialized so that they approximately preserve the variance of their inputs, and inputs have approximately unit variance [26, 27].", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "ViT-Large model", "md": "# ViT-Large model"}, {"type": "heading", "lvl": 2, "value": "ViT-Large model", "md": "## ViT-Large model"}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Iterations} & \\text{Average feature magnitude} \\\\\n\\hline\n\\text{i) bfloat16 baseline} & 0, 0, 10, 20, 30 \\\\\n\\text{ii) fp8 tensor quantize} & \\\\\n\\text{iii) fp8 tensor + grad clip} & \\\\\n\\text{iv) fp8 tensor + KQ Layernorm} & \\\\\n\\text{v) fp8 tensor + zero-init layerscale} & \\text{Transformer block index} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 5: (Left) Training CLIP ViT-Large models with simulated fp8 precision using tensor-wise quantization for the inputs, weights, and gradients. All methods we try diverge except for using zero-init layerscale [58], which multiplies the output of each self-attention or mlp block with a learnable vector initialized to zero. (Right) Examining feature magnitudes (i.e., the average absolute value of the output for transformer block k) for CLIP ViT-Huge at the beginning (init) and end of training. This suggests why zero-init layer scale enables float8 training\u2014zero-init layer scale prevents high feature magnitudes which may cause issues for low precision training [17]. Without the intervention, the average feature magnitude becomes large for later blocks.\n\nWe use the bitsandbytes library [18] to simulate float8 training using the fp8 types from Micikevicius et al. [40]. We use tensor-wise quantization for the inputs, weights, and gradients, so that all operations occur in simulated float8. In our simulation, we represent each value only with the exact values representable by float8, but we perform computations in float16 precision. We believe that tensor-wise quantization approximates the removal of quantize operations entirely. This is because, as we show in Appendix B.2 (Figure 14), the maximum of these tensors tends to evolve smoothly. Consequently, using a moving average for a maximum which is divided directly in the matmul is similar to tensor-wise quantization.\n\nLayer-scale, introduced by Touvron et al. [58], scales each self-attention and MLP block output hidden state by a learnable vector of shape embed dim. A pre-norm transformer block with layer-scale tensors \u03b31 and \u03b32 is defined as\n\n$$\n\\begin{align*}\nx' & = x_k + \\gamma_1 * \\text{self attention(norm}_1(x_k)) \\quad (5) \\\\\nx_{k+1} & = x' + \\gamma_2 * \\text{mlp(norm}_2(x')) \\quad (6)\n\\end{align*}\n$$\n\nTypically, layers are initialized so that they approximately preserve the variance of their inputs, and inputs have approximately unit variance [26, 27].", "md": "$$\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Iterations} & \\text{Average feature magnitude} \\\\\n\\hline\n\\text{i) bfloat16 baseline} & 0, 0, 10, 20, 30 \\\\\n\\text{ii) fp8 tensor quantize} & \\\\\n\\text{iii) fp8 tensor + grad clip} & \\\\\n\\text{iv) fp8 tensor + KQ Layernorm} & \\\\\n\\text{v) fp8 tensor + zero-init layerscale} & \\text{Transformer block index} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 5: (Left) Training CLIP ViT-Large models with simulated fp8 precision using tensor-wise quantization for the inputs, weights, and gradients. All methods we try diverge except for using zero-init layerscale [58], which multiplies the output of each self-attention or mlp block with a learnable vector initialized to zero. (Right) Examining feature magnitudes (i.e., the average absolute value of the output for transformer block k) for CLIP ViT-Huge at the beginning (init) and end of training. This suggests why zero-init layer scale enables float8 training\u2014zero-init layer scale prevents high feature magnitudes which may cause issues for low precision training [17]. Without the intervention, the average feature magnitude becomes large for later blocks.\n\nWe use the bitsandbytes library [18] to simulate float8 training using the fp8 types from Micikevicius et al. [40]. We use tensor-wise quantization for the inputs, weights, and gradients, so that all operations occur in simulated float8. In our simulation, we represent each value only with the exact values representable by float8, but we perform computations in float16 precision. We believe that tensor-wise quantization approximates the removal of quantize operations entirely. This is because, as we show in Appendix B.2 (Figure 14), the maximum of these tensors tends to evolve smoothly. Consequently, using a moving average for a maximum which is divided directly in the matmul is similar to tensor-wise quantization.\n\nLayer-scale, introduced by Touvron et al. [58], scales each self-attention and MLP block output hidden state by a learnable vector of shape embed dim. A pre-norm transformer block with layer-scale tensors \u03b31 and \u03b32 is defined as\n\n$$\n\\begin{align*}\nx' & = x_k + \\gamma_1 * \\text{self attention(norm}_1(x_k)) \\quad (5) \\\\\nx_{k+1} & = x' + \\gamma_2 * \\text{mlp(norm}_2(x')) \\quad (6)\n\\end{align*}\n$$\n\nTypically, layers are initialized so that they approximately preserve the variance of their inputs, and inputs have approximately unit variance [26, 27]."}]}, {"page": 8, "text": "ever, when combined with residual connections this                      3.1      Preliminaries and related work\ncan lead to higher norms in deeper networks.                            Loss spikes can emerge when scaling up models [8,                25,\n                                                                        14,  69, 71,  54,  74]. These instabilities may slow learn-\nConsequently, researchers have proposed initialization                  ing, or even destabilize training completely. Various\nand scaling schemes which remedy this issue [1,                  72,    solutions have been proposed, including freezing the\n4, 19]. Layer-scale with initialization 0 is an example                 embedding layer [8], adding additional layer normal-\nof one such scheme\u2014at initialization the transformer                    ization [14, 25], or reparametrizing the weights [69].\nis an identity function.          While    \u03b3 1, \u03b32  are typically\ninitialized as vectors of 10        \u22124  or 10  \u22126 , we use 0 for        In our work we investigate instabilities which arise\nsimplicity.                                                             during CLIP training.           Unlike the instabilities ob-\n                                                                        served in [14,    69] which lead to a slow divergence, we\nFigure 5 (right) demonstrates that the layer-scale                      study fast loss spikes. Our results indicate that these\nintervention is successful at controlling the average                   spikes arise when the second moment estimator is out\nmagnitude output. Without the intervention, the av-                     of date for early layers.\nerage feature magnitude         E[abs(x    k)] becomes high for\nlater blocks. Previous work [17] has shown that large                   While     our    analysis    and    methods       build    directly\nfeature magnitudes result in issues for low precision                   on Shazeer and Stern           [54]   (AdaFactor), there are\ntraining.                                                               important differences.            In contrast with Shazeer\n                                                                        and Stern     [54], who only observe instabilities with-\nResults for simulated fp8 training are shown in Fig-                    out warmup, we observe instabilities despite a long\nure 5 (left) for ViT-Large. We find that all fp8 runs                   warmup period. Moreover, in contrast with Shazeer\ndiverge except for when we use layer-scale initialized                  and Stern     [54] we find that an out-of-date second mo-\nto zero. Concretely, Figure 5 compares i) the base-                     ment estimator is primarily an issue for the (patch)\nline which uses bfloat16 training, ii) using fp8 with                   embedding layer, and measure how well loss spikes\ntensor-wise quantization and no further modifications,                  are predicted by this event.            Finally, we note that\nwhich slowly diverges, iii) adding gradient clipping to                 researchers have moved away from AdaFactor in its\nii), which also diverges, iv) adding KQ layernorm [14]                  original formulation for large-scale training [47,          11, 70],\nto ii), which also diverges, and v) using           zero-init lay-      finding AdaFactor to under-perform AdamW [47]. We\nerscale, which trains without diverging. While there                    believe this is due to the factored second moment or\nis a difference still between fp8 and bfloat16 training,                absence of first moment.            This is why our focus is\nthis is primarily because of layerscale. Moreover, we                   AdamW [37] which is the de facto standard optimizer\nbelieve that with hyperparameter tuning layerscale                      for transformers.\nwould match standard training in terms of accuracy.                     After the initial version of this paper we became aware\n                                                                        of Cohen et al.     [12]  which offers a general and princi-\n                                                                        pled treatment of fast loss spikes, and which we rec-\n                                                                        ommend to readers. Moreover, we direct the reader\u2019s\n3      Stability                                                        attention to the concurrent work of [41].\nWe now switch focus from accelerating learning by\nreducing precision to addressing instabilities which                    3.2      Experimental setup\ncan arise during training.           Section 3.1 reviews pre-\nliminaries and related work while Section 3.2 details                   As in Section 2,         we train ViT CLIP models on\nthe experimental setup. Next, Section 3.3 examines                      LAION [53] using OpenCLIP [29] and evaluate them\ntrends for training instability, finding loss spikes to                 zero-shot on ImageNet. Since we are not interested\nincrease with model scale but decrease with lower                       in final performance and instead interested in study-\nAdamW       \u03b2 2. Then, Section 3.4 finds that loss spikes               ing instability\u2014even for very large models\u2014we use\narise in our setting due to an out-of-date AdamW                        a short run which allows us to conduct multiple ex-\nsecond moment estimator leading Section 3.5 to adopt                    periments. Concretly, we use patch-dropout 0.5 [35]\nand tests a fix developed in the context of AdaFac-                     and 20k iterations. The first 5k iterations are linear\ntor [54]. Finally, Section 3.6 connects loss spikes to                  warmup while the remainder are cosine decay [36]. We\nlow precision training.                                               8 follow the CLIP paper [46] in that i) we do not use", "md": "ever, when combined with residual connections this $$3.1$$ Preliminaries and related work\ncan lead to higher norms in deeper networks. Loss spikes can emerge when scaling up models [8, 25, 14, 69, 71, 54, 74]. These instabilities may slow learning, or even destabilize training completely. Various solutions have been proposed, including freezing the embedding layer [8], adding additional layer normalization [14, 25], or reparametrizing the weights [69].\n\nIn our work we investigate instabilities which arise during CLIP training. Unlike the instabilities observed in [14, 69] which lead to a slow divergence, we study fast loss spikes. Our results indicate that these spikes arise when the second moment estimator is out of date for early layers.\n\nWhile our analysis and methods build directly on Shazeer and Stern [54] (AdaFactor), there are important differences. In contrast with Shazeer and Stern [54], who only observe instabilities without warmup, we observe instabilities despite a long warmup period. Moreover, in contrast with Shazeer and Stern [54] we find that an out-of-date second moment estimator is primarily an issue for the (patch) embedding layer, and measure how well loss spikes are predicted by this event. Finally, we note that researchers have moved away from AdaFactor in its original formulation for large-scale training [47, 11, 70], finding AdaFactor to under-perform AdamW [47]. We believe this is due to the factored second moment or absence of first moment. This is why our focus is AdamW [37] which is the de facto standard optimizer for transformers.\n\nAfter the initial version of this paper we became aware of Cohen et al. [12] which offers a general and principled treatment of fast loss spikes, and which we recommend to readers. Moreover, we direct the reader\u2019s attention to the concurrent work of [41].\n\n### 3 Stability\n\nWe now switch focus from accelerating learning by reducing precision to addressing instabilities which can arise during training. Section 3.1 reviews preliminaries and related work while Section 3.2 details the experimental setup. Next, Section 3.3 examines trends for training instability, finding loss spikes to increase with model scale but decrease with lower AdamW $$\\beta_2$$. Then, Section 3.4 finds that loss spikes arise in our setting due to an out-of-date AdamW second moment estimator leading Section 3.5 to adopt and tests a fix developed in the context of AdaFactor [54]. Finally, Section 3.6 connects loss spikes to low precision training.\n\n### 3.2 Experimental setup\n\nAs in Section 2, we train ViT CLIP models on LAION [53] using OpenCLIP [29] and evaluate them zero-shot on ImageNet. Since we are not interested in final performance and instead interested in studying instability\u2014even for very large models\u2014we use a short run which allows us to conduct multiple experiments. Concretely, we use patch-dropout 0.5 [35] and 20k iterations. The first 5k iterations are linear warmup while the remainder are cosine decay [36]. We follow the CLIP paper [46] in that i) we do not use", "images": [], "items": [{"type": "text", "value": "ever, when combined with residual connections this $$3.1$$ Preliminaries and related work\ncan lead to higher norms in deeper networks. Loss spikes can emerge when scaling up models [8, 25, 14, 69, 71, 54, 74]. These instabilities may slow learning, or even destabilize training completely. Various solutions have been proposed, including freezing the embedding layer [8], adding additional layer normalization [14, 25], or reparametrizing the weights [69].\n\nIn our work we investigate instabilities which arise during CLIP training. Unlike the instabilities observed in [14, 69] which lead to a slow divergence, we study fast loss spikes. Our results indicate that these spikes arise when the second moment estimator is out of date for early layers.\n\nWhile our analysis and methods build directly on Shazeer and Stern [54] (AdaFactor), there are important differences. In contrast with Shazeer and Stern [54], who only observe instabilities without warmup, we observe instabilities despite a long warmup period. Moreover, in contrast with Shazeer and Stern [54] we find that an out-of-date second moment estimator is primarily an issue for the (patch) embedding layer, and measure how well loss spikes are predicted by this event. Finally, we note that researchers have moved away from AdaFactor in its original formulation for large-scale training [47, 11, 70], finding AdaFactor to under-perform AdamW [47]. We believe this is due to the factored second moment or absence of first moment. This is why our focus is AdamW [37] which is the de facto standard optimizer for transformers.\n\nAfter the initial version of this paper we became aware of Cohen et al. [12] which offers a general and principled treatment of fast loss spikes, and which we recommend to readers. Moreover, we direct the reader\u2019s attention to the concurrent work of [41].", "md": "ever, when combined with residual connections this $$3.1$$ Preliminaries and related work\ncan lead to higher norms in deeper networks. Loss spikes can emerge when scaling up models [8, 25, 14, 69, 71, 54, 74]. These instabilities may slow learning, or even destabilize training completely. Various solutions have been proposed, including freezing the embedding layer [8], adding additional layer normalization [14, 25], or reparametrizing the weights [69].\n\nIn our work we investigate instabilities which arise during CLIP training. Unlike the instabilities observed in [14, 69] which lead to a slow divergence, we study fast loss spikes. Our results indicate that these spikes arise when the second moment estimator is out of date for early layers.\n\nWhile our analysis and methods build directly on Shazeer and Stern [54] (AdaFactor), there are important differences. In contrast with Shazeer and Stern [54], who only observe instabilities without warmup, we observe instabilities despite a long warmup period. Moreover, in contrast with Shazeer and Stern [54] we find that an out-of-date second moment estimator is primarily an issue for the (patch) embedding layer, and measure how well loss spikes are predicted by this event. Finally, we note that researchers have moved away from AdaFactor in its original formulation for large-scale training [47, 11, 70], finding AdaFactor to under-perform AdamW [47]. We believe this is due to the factored second moment or absence of first moment. This is why our focus is AdamW [37] which is the de facto standard optimizer for transformers.\n\nAfter the initial version of this paper we became aware of Cohen et al. [12] which offers a general and principled treatment of fast loss spikes, and which we recommend to readers. Moreover, we direct the reader\u2019s attention to the concurrent work of [41]."}, {"type": "heading", "lvl": 3, "value": "3 Stability", "md": "### 3 Stability"}, {"type": "text", "value": "We now switch focus from accelerating learning by reducing precision to addressing instabilities which can arise during training. Section 3.1 reviews preliminaries and related work while Section 3.2 details the experimental setup. Next, Section 3.3 examines trends for training instability, finding loss spikes to increase with model scale but decrease with lower AdamW $$\\beta_2$$. Then, Section 3.4 finds that loss spikes arise in our setting due to an out-of-date AdamW second moment estimator leading Section 3.5 to adopt and tests a fix developed in the context of AdaFactor [54]. Finally, Section 3.6 connects loss spikes to low precision training.", "md": "We now switch focus from accelerating learning by reducing precision to addressing instabilities which can arise during training. Section 3.1 reviews preliminaries and related work while Section 3.2 details the experimental setup. Next, Section 3.3 examines trends for training instability, finding loss spikes to increase with model scale but decrease with lower AdamW $$\\beta_2$$. Then, Section 3.4 finds that loss spikes arise in our setting due to an out-of-date AdamW second moment estimator leading Section 3.5 to adopt and tests a fix developed in the context of AdaFactor [54]. Finally, Section 3.6 connects loss spikes to low precision training."}, {"type": "heading", "lvl": 3, "value": "3.2 Experimental setup", "md": "### 3.2 Experimental setup"}, {"type": "text", "value": "As in Section 2, we train ViT CLIP models on LAION [53] using OpenCLIP [29] and evaluate them zero-shot on ImageNet. Since we are not interested in final performance and instead interested in studying instability\u2014even for very large models\u2014we use a short run which allows us to conduct multiple experiments. Concretely, we use patch-dropout 0.5 [35] and 20k iterations. The first 5k iterations are linear warmup while the remainder are cosine decay [36]. We follow the CLIP paper [46] in that i) we do not use", "md": "As in Section 2, we train ViT CLIP models on LAION [53] using OpenCLIP [29] and evaluate them zero-shot on ImageNet. Since we are not interested in final performance and instead interested in studying instability\u2014even for very large models\u2014we use a short run which allows us to conduct multiple experiments. Concretely, we use patch-dropout 0.5 [35] and 20k iterations. The first 5k iterations are linear warmup while the remainder are cosine decay [36]. We follow the CLIP paper [46] in that i) we do not use"}]}, {"page": 9, "text": "          10           ViT-Base model               10           ViT-Large model               10          ViT-Huge model\n           8                                         8                                          8\n           6                                         6                                          6\n         Loss                                      Loss                                       Loss\n           4                                         4                                          4\n           2                                         2                                          2\n           0  0     5000  10000   15000  20000       0  0     5000   10000  15000   20000       0  0     5000  10000   15000  20000\n                          Iteration                                 Iteration                                 Iteration\n                                    beta2 = 0.99    beta2 = 0.98     beta2 = 0.95     beta2 = 0.9     beta2 = 0.5\nFigure 6: Loss spikes increase with       model size     for fixed learning rate and batch size. Reducing AdamW               \u03b2 2 from its\ndefault in PyTorch of 0.999 mitigates loss spikes. Reducing             \u03b22 too much slows training.\n          10    Base model, batch size = 4096       10    Base model, batch size = 16384       10   Base model, batch size = 65536\n           8                                         8                                          8\n           6                                         6                                          6\n         Loss                                      Loss                                       Loss\n           4                                         4                                          4\n           2  0     5000  10000   15000  20000       2  0     5000   10000  15000   20000       2  0     5000  10000   15000  20000\n                          Iteration                                 Iteration                                 Iteration\n                                    beta2 = 0.99    beta2 = 0.98     beta2 = 0.95     beta2 = 0.9     beta2 = 0.5\nFigure 7: Loss spikes increase with       batch size     for fixed learning rate and model size. Reducing AdamW               \u03b2 2 from its\ndefault in PyTorch of 0.999 mitigates loss spikes. Reducing             \u03b22 too much slows training.\n   10         Base model, LR = 5e-4                10        Base model, LR = 2e-3                10        Base model, LR = 5e-3\n    8                                               8                                              8\n    6                                               6                                              6\n  Loss                                            Loss                                           Loss\n    4                                               4                                              4\n    2   0     5000    10000   15000   20000         2  0     5000    10000   15000   20000         2  0      5000   10000   15000   20000\n                    Iteration                                       Iteration                                      Iteration\n                                beta2 = 0.99      beta2 = 0.98       beta2 = 0.95       beta2 = 0.9      beta2 = 0.5\nFigure 8: Loss spikes increase with       learning rate     for fixed batch size and model size. Reducing AdamW               \u03b2 2 from its\ndefault in PyTorch of 0.999 mitigates loss spikes. Reducing          9  \u03b22 too much slows training.", "md": "| |ViT-Base model|ViT-Large model|ViT-Huge model|\n|---|---|---|---|\n|8|8|8|8|\n|6|6|6|6|\n|Loss|Loss|Loss|Loss|\n|4|4|4|4|\n|2|2|2|2|\n|0|0|0|0|\n|Iteration|Iteration|Iteration|Iteration|\n\n$$\\text{Figure 6: Loss spikes increase with model size for fixed learning rate and batch size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$\n\n| |Base model, batch size = 4096|Base model, batch size = 16384|Base model, batch size = 65536|\n|---|---|---|---|\n|8|8|8|8|\n|6|6|6|6|\n|Loss|Loss|Loss|Loss|\n|4|4|4|4|\n|2|2|2|2|\n|0|0|0|0|\n|Iteration|Iteration|Iteration|Iteration|\n\n$$\\text{Figure 7: Loss spikes increase with batch size for fixed learning rate and model size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$\n\n| |Base model, LR = 5e-4|Base model, LR = 2e-3|Base model, LR = 5e-3|\n|---|---|---|---|\n|8|8|8|8|\n|6|6|6|6|\n|Loss|Loss|Loss|Loss|\n|4|4|4|4|\n|2|2|2|2|\n|0|0|0|0|\n|Iteration|Iteration|Iteration|Iteration|\n\n$$\\text{Figure 8: Loss spikes increase with learning rate for fixed batch size and model size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$", "images": [], "items": [{"type": "table", "rows": [["", "ViT-Base model", "ViT-Large model", "ViT-Huge model"], ["8", "8", "8", "8"], ["6", "6", "6", "6"], ["Loss", "Loss", "Loss", "Loss"], ["4", "4", "4", "4"], ["2", "2", "2", "2"], ["0", "0", "0", "0"], ["Iteration", "Iteration", "Iteration", "Iteration"]], "md": "| |ViT-Base model|ViT-Large model|ViT-Huge model|\n|---|---|---|---|\n|8|8|8|8|\n|6|6|6|6|\n|Loss|Loss|Loss|Loss|\n|4|4|4|4|\n|2|2|2|2|\n|0|0|0|0|\n|Iteration|Iteration|Iteration|Iteration|", "isPerfectTable": true, "csv": "\"\",\"ViT-Base model\",\"ViT-Large model\",\"ViT-Huge model\"\n\"8\",\"8\",\"8\",\"8\"\n\"6\",\"6\",\"6\",\"6\"\n\"Loss\",\"Loss\",\"Loss\",\"Loss\"\n\"4\",\"4\",\"4\",\"4\"\n\"2\",\"2\",\"2\",\"2\"\n\"0\",\"0\",\"0\",\"0\"\n\"Iteration\",\"Iteration\",\"Iteration\",\"Iteration\""}, {"type": "text", "value": "$$\\text{Figure 6: Loss spikes increase with model size for fixed learning rate and batch size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$", "md": "$$\\text{Figure 6: Loss spikes increase with model size for fixed learning rate and batch size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$"}, {"type": "table", "rows": [["", "Base model, batch size = 4096", "Base model, batch size = 16384", "Base model, batch size = 65536"], ["8", "8", "8", "8"], ["6", "6", "6", "6"], ["Loss", "Loss", "Loss", "Loss"], ["4", "4", "4", "4"], ["2", "2", "2", "2"], ["0", "0", "0", "0"], ["Iteration", "Iteration", "Iteration", "Iteration"]], "md": "| |Base model, batch size = 4096|Base model, batch size = 16384|Base model, batch size = 65536|\n|---|---|---|---|\n|8|8|8|8|\n|6|6|6|6|\n|Loss|Loss|Loss|Loss|\n|4|4|4|4|\n|2|2|2|2|\n|0|0|0|0|\n|Iteration|Iteration|Iteration|Iteration|", "isPerfectTable": true, "csv": "\"\",\"Base model, batch size = 4096\",\"Base model, batch size = 16384\",\"Base model, batch size = 65536\"\n\"8\",\"8\",\"8\",\"8\"\n\"6\",\"6\",\"6\",\"6\"\n\"Loss\",\"Loss\",\"Loss\",\"Loss\"\n\"4\",\"4\",\"4\",\"4\"\n\"2\",\"2\",\"2\",\"2\"\n\"0\",\"0\",\"0\",\"0\"\n\"Iteration\",\"Iteration\",\"Iteration\",\"Iteration\""}, {"type": "text", "value": "$$\\text{Figure 7: Loss spikes increase with batch size for fixed learning rate and model size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$", "md": "$$\\text{Figure 7: Loss spikes increase with batch size for fixed learning rate and model size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$"}, {"type": "table", "rows": [["", "Base model, LR = 5e-4", "Base model, LR = 2e-3", "Base model, LR = 5e-3"], ["8", "8", "8", "8"], ["6", "6", "6", "6"], ["Loss", "Loss", "Loss", "Loss"], ["4", "4", "4", "4"], ["2", "2", "2", "2"], ["0", "0", "0", "0"], ["Iteration", "Iteration", "Iteration", "Iteration"]], "md": "| |Base model, LR = 5e-4|Base model, LR = 2e-3|Base model, LR = 5e-3|\n|---|---|---|---|\n|8|8|8|8|\n|6|6|6|6|\n|Loss|Loss|Loss|Loss|\n|4|4|4|4|\n|2|2|2|2|\n|0|0|0|0|\n|Iteration|Iteration|Iteration|Iteration|", "isPerfectTable": true, "csv": "\"\",\"Base model, LR = 5e-4\",\"Base model, LR = 2e-3\",\"Base model, LR = 5e-3\"\n\"8\",\"8\",\"8\",\"8\"\n\"6\",\"6\",\"6\",\"6\"\n\"Loss\",\"Loss\",\"Loss\",\"Loss\"\n\"4\",\"4\",\"4\",\"4\"\n\"2\",\"2\",\"2\",\"2\"\n\"0\",\"0\",\"0\",\"0\"\n\"Iteration\",\"Iteration\",\"Iteration\",\"Iteration\""}, {"type": "text", "value": "$$\\text{Figure 8: Loss spikes increase with learning rate for fixed batch size and model size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$", "md": "$$\\text{Figure 8: Loss spikes increase with learning rate for fixed batch size and model size. Reducing AdamW } \\beta_2 \\text{ from its default in PyTorch of 0.999 mitigates loss spikes. Reducing } \\beta_2 \\text{ too much slows training.}$$"}]}, {"page": 10, "text": "gradient clipping unless otherwise mentioned             1, though      learning signal changes. Concretely, exponential mov-\nwe do clip the      logit scale   parameter, and ii) we add             ing averages can become out of date causing updates\na layer-norm after the patch embedding and before                       to be scaled by a value that is too large. This issue is\nthe main transformer. Unless otherwise mentioned,                       discussed in Section 5 of Shazeer and Stern               [54], and\nexperiments use batch size 16384 (per-gpu batch size                    we summarize below.\nof 256), learning rate 2e-3 and weight decay 0.2. We                                                          \u25af      T               \u0001\ninitially tried adding a layer-norm before the patch                    Algorithm 2         StableAdamW        {\u03b1  t}t=0  , \u03b21, \u03b22, \u03f5\nembedding as in [34], but removed this as we found it                      v0 , u0 =  0\nto hurt performance at CLIP ViT-Huge scale.                                for  t = 1   to  T  do\n                                                                              g  =   \u2207f(\u03b8   )\n                                                                               t            t\n                                                                              // apply correction term to debias moving avg.\n3.3      Loss spikes increase with model                                       \u02c6           1\u2212\u03b21t\u22121\n         size, batch size, and learning rate                                  \u03b2 1 =  \u03b2 1 \u00b7  1\u2212\u03b21 t\n                                                                                           1\u2212\u03b2 t\u22121\nWe begin our studying of loss spikes by observing                             \u03b2\u02c62 =  \u03b2 2 \u00b7     2 t\n                                                                                            1\u2212\u03b22\nhow their presence varies when changing model size,                           // update moving averages\nbatch size, and learning rate. The following sections                         vt  = \u03b2\u02c61 vt\u22121  + (1  \u2212  \u03b2\u02c61)g t\nbuild on these observations\u2014in particular the finding                                \u02c6                  \u02c6     2\n                                                                              u t = \u03b2  2u t\u22121  + (1  \u2212  \u03b22 )gt\nthat lowering the AdamW           \u03b2 2 hyperparameter removes                  // for implementation convenience, the steps\nspikes entirely.                                                              // below occur independently for each tensorp\nWe find that loss spikes increase when increasing                             RMS   t =     E  [g2/u  t]\n                                                                                                 t\nmodel size (Figure 6), batch size (Figure 7), or learning                     // update parameters\nrate (Figure 3). However, we also find that loss spikes                       \u03b7t  =  \u03b1 t/max   (1,RMS    t)       \u25af \u221a         \u0001\ncan be avoided by reducing the           \u03b22  hyperparameter for               \u03b8t =   \u03b8t\u22121  \u2212  \u03b7t\u03bb\u03b8 t\u22121  \u2212  \u03b7 tvt/     u t +  \u03f5\nin AdamW. On the other hand, if               \u03b2 2 is reduced too\nmuch then learning is slowed which results in worse                     As in Algorithm 2, let           u   =   {u    } n     denote the\nperformance [50].                                                                                          t         t,j j=1\n                                                                        exponential moving average (EMA) of squared gra-\n                                                                        dients   g 2 =  {g 2  }n     for neural network parameters\n                                                                                   t       t,j j=1\n3.4      On     \u03b2 2  and an out-of-date second                          \u03b8  \u2208  Rn . Ignoring the bias correction term            2 , at each\n         moment estimator                                               iteration   t, u  is updated as     \u03b2  u     + (1\u2212\u03b2      )g2  where\n                                                                                         t                    2  t\u22121           2   t\n                                                                        \u03b2   is referred to as the        decay    for the EMA. Then,\nBased on the observation in the previous section that                     2                                  \u25af \u221a         \u0001\nlowering     \u03b22  reduces spikes, this section traces the                the update is scaled by 1/               u t +  \u03f5 , where     \u03f5 is a\n                                                                        small value added numerical stability. Often the ratio\u25af\u0001\ncause of loss spikes to an out-of-date second moment                          \u221a\nestimator in the patch embedding layer.                                 vt /    u t +  \u03f5   is thought of as signal-to-noise ratio of\nOverview.           Adaptive      optimizers      such    as   Ada-     the gradient over time.\nGrad [22], Adam [33], or AdaFactor [54] scale the                       However, this method can break down when the learn-\n                                                                        ing signal changes and       u t ceases to be a good estimator\nupdate differently for each individual parameter. This                  for the running average of        g2. Consider the case where\nis often conceptualized a per-parameter learning rate.                                                     t\n                                                                        the gradient magnitudes have been historically very\nFor instance, in Adam/AdamW, per-parameter up-                                                                       \u25af\u221a          \u0001\ndates are scaled by the inverse root of the exponential                 small for some parameters so 1/                  u t+  \u03f5   is large\nmoving average of squared gradients (see the code                       for those parameters.          If, then, at iteration       t those\nfor AdamW in Algorithm 2, ignoring for now the                          parameters suddenly receive a larger gradient signal\nmodifications in pink which we discuss in Section 3.5).                 the update can be catastrophically big. We refer to\nThis adaptivity can be a very useful tool for accel-                    the scenario as the       stuck-in-the-past         scenario.\nerating training, but can also cause issues when the                    Overall, if    \u03b22  is too small then convergence may be\n    1It is possible that CLIP is trained with gradient clipping         slowed [50].      If \u03b2 2 is too large then       u t can become\ndespite not mentioning it in the paper. However, this baseline              2In practice, the EMA is debiased with a correction term.\nfollows the OpenCLIP library [29], which does not use gradient          Algorithm 2 follows AdaFactor section 7.1 in applying the\nclipping by default since it follows what is mentioned in Radford       correction term to    \u03b2 1, \u03b22 . Adam is often written with the\net al. [46].                                                         10 correction term applied to    vt ,u tbut they are equivalent [54].", "md": "```markdown\ngradient clipping unless otherwise mentioned             1, though      learning signal changes. Concretely, exponential mov-\nwe do clip the      logit scale   parameter, and ii) we add             ing averages can become out of date causing updates\na layer-norm after the patch embedding and before                       to be scaled by a value that is too large. This issue is\nthe main transformer. Unless otherwise mentioned,                       discussed in Section 5 of Shazeer and Stern               [54], and\nexperiments use batch size 16384 (per-gpu batch size                    of 256), learning rate 2e-3 and weight decay 0.2. We\ninitially tried adding a layer-norm before the patch                    embedding as in [34], but removed this as we found it\nto hurt performance at CLIP ViT-Huge scale.\n```\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm 2 StableAdamW} & \\{ \\alpha_t \\}_{t=0}, \\beta_1, \\beta_2, \\epsilon \\\\\nv_0, u_0 = 0 & \\\\\n\\text{for } t = 1 \\text{ to } T \\text{ do} & \\\\\ng = \\nabla f(\\theta_t) & \\\\\n\\text{// apply correction term to debias moving avg.} & \\\\\n\\hat{\\beta}_1 = \\beta_1 \\cdot \\frac{1 - \\beta_1^{t-1}}{1 - \\beta_1^t} & \\\\\n\\hat{\\beta}_2 = \\beta_2 \\cdot \\frac{2}{1 - \\beta_2} & \\\\\n\\text{// update moving averages} & \\\\\nv_t = \\hat{\\beta}_1 v_{t-1} + (1 - \\hat{\\beta}_1) g_t & \\\\\nu_t = \\hat{\\beta}_2 u_{t-1} + (1 - \\hat{\\beta}_2) g_t & \\\\\n\\text{// for implementation convenience, the steps} & \\\\\n\\text{// below occur independently for each tensor} & \\\\\nRMS_t = \\sqrt{\\mathbb{E} \\left[ \\frac{g^2}{u_t} \\right]} & \\\\\n\\text{// update parameters} & \\\\\n\\eta_t = \\alpha_t / \\max \\left(1, RMS_t\\right) & \\\\\n\\theta_t = \\theta_{t-1} - \\eta_t \\lambda \\theta_{t-1} - \\eta_t \\frac{v_t}{u_t + \\epsilon} & \\\\\n\\end{array}\n$$\n\n```markdown\nLoss spikes increase with model size, batch size, and learning rate\n\nWe begin our studying of loss spikes by observing how their presence varies when changing model size, batch size, and learning rate. The following sections build on these observations\u2014in particular the finding that lowering the AdamW $\\beta_2$ hyperparameter removes spikes entirely.\n\nWe find that loss spikes increase when increasing model size (Figure 6), batch size (Figure 7), or learning rate (Figure 3). However, we also find that loss spikes can be avoided by reducing the $\\beta_2$ hyperparameter for in AdamW. On the other hand, if $\\beta_2$ is reduced too much then learning is slowed which results in worse performance [50].\n```\n\n```markdown\nOn $\\beta_2$ and an out-of-date second moment estimator\n\nBased on the observation in the previous section that lowering $\\beta_2$ reduces spikes, this section traces the cause of loss spikes to an out-of-date second moment estimator in the patch embedding layer.\n\nOverview. Adaptive optimizers such as AdaGrad, Adam, or AdaFactor scale the update differently for each individual parameter. This is often conceptualized as per-parameter learning rate.\n\nFor instance, in Adam/AdamW, per-parameter updates are scaled by the inverse root of the exponential moving average of squared gradients (see the code for AdamW in Algorithm 2, ignoring for now the modifications in pink which we discuss in Section 3.5).\n\nThis adaptivity can be a very useful tool for accelerating training, but can also cause issues when the update differently for each individual parameter. This is often conceptualized as per-parameter learning rate.\n\nFor instance, in Adam/AdamW, per-parameter updates are scaled by the inverse root of the exponential moving average of squared gradients (see the code for AdamW in Algorithm 2, ignoring for now the modifications in pink which we discuss in Section 3.5).\n\nThis adaptivity can be a very useful tool for accelerating training, but can also cause issues when the\n```\n\n```markdown\nlearning signal changes. Overall, if $\\beta_2$ is too small then convergence may be slowed [50]. If $\\beta_2$ is too large then $u_t$ can become\n\nIn practice, the EMA is debiased with a correction term. Algorithm 2 follows AdaFactor section 7.1 in applying the correction term to $\\beta_1, \\beta_2$. Adam is often written with the correction term applied to $v_t, u_t$ but they are equivalent [54].\n```", "images": [], "items": [{"type": "text", "value": "```markdown\ngradient clipping unless otherwise mentioned             1, though      learning signal changes. Concretely, exponential mov-\nwe do clip the      logit scale   parameter, and ii) we add             ing averages can become out of date causing updates\na layer-norm after the patch embedding and before                       to be scaled by a value that is too large. This issue is\nthe main transformer. Unless otherwise mentioned,                       discussed in Section 5 of Shazeer and Stern               [54], and\nexperiments use batch size 16384 (per-gpu batch size                    of 256), learning rate 2e-3 and weight decay 0.2. We\ninitially tried adding a layer-norm before the patch                    embedding as in [34], but removed this as we found it\nto hurt performance at CLIP ViT-Huge scale.\n```\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm 2 StableAdamW} & \\{ \\alpha_t \\}_{t=0}, \\beta_1, \\beta_2, \\epsilon \\\\\nv_0, u_0 = 0 & \\\\\n\\text{for } t = 1 \\text{ to } T \\text{ do} & \\\\\ng = \\nabla f(\\theta_t) & \\\\\n\\text{// apply correction term to debias moving avg.} & \\\\\n\\hat{\\beta}_1 = \\beta_1 \\cdot \\frac{1 - \\beta_1^{t-1}}{1 - \\beta_1^t} & \\\\\n\\hat{\\beta}_2 = \\beta_2 \\cdot \\frac{2}{1 - \\beta_2} & \\\\\n\\text{// update moving averages} & \\\\\nv_t = \\hat{\\beta}_1 v_{t-1} + (1 - \\hat{\\beta}_1) g_t & \\\\\nu_t = \\hat{\\beta}_2 u_{t-1} + (1 - \\hat{\\beta}_2) g_t & \\\\\n\\text{// for implementation convenience, the steps} & \\\\\n\\text{// below occur independently for each tensor} & \\\\\nRMS_t = \\sqrt{\\mathbb{E} \\left[ \\frac{g^2}{u_t} \\right]} & \\\\\n\\text{// update parameters} & \\\\\n\\eta_t = \\alpha_t / \\max \\left(1, RMS_t\\right) & \\\\\n\\theta_t = \\theta_{t-1} - \\eta_t \\lambda \\theta_{t-1} - \\eta_t \\frac{v_t}{u_t + \\epsilon} & \\\\\n\\end{array}\n$$\n\n```markdown\nLoss spikes increase with model size, batch size, and learning rate\n\nWe begin our studying of loss spikes by observing how their presence varies when changing model size, batch size, and learning rate. The following sections build on these observations\u2014in particular the finding that lowering the AdamW $\\beta_2$ hyperparameter removes spikes entirely.\n\nWe find that loss spikes increase when increasing model size (Figure 6), batch size (Figure 7), or learning rate (Figure 3). However, we also find that loss spikes can be avoided by reducing the $\\beta_2$ hyperparameter for in AdamW. On the other hand, if $\\beta_2$ is reduced too much then learning is slowed which results in worse performance [50].\n```\n\n```markdown\nOn $\\beta_2$ and an out-of-date second moment estimator\n\nBased on the observation in the previous section that lowering $\\beta_2$ reduces spikes, this section traces the cause of loss spikes to an out-of-date second moment estimator in the patch embedding layer.\n\nOverview. Adaptive optimizers such as AdaGrad, Adam, or AdaFactor scale the update differently for each individual parameter. This is often conceptualized as per-parameter learning rate.\n\nFor instance, in Adam/AdamW, per-parameter updates are scaled by the inverse root of the exponential moving average of squared gradients (see the code for AdamW in Algorithm 2, ignoring for now the modifications in pink which we discuss in Section 3.5).\n\nThis adaptivity can be a very useful tool for accelerating training, but can also cause issues when the update differently for each individual parameter. This is often conceptualized as per-parameter learning rate.\n\nFor instance, in Adam/AdamW, per-parameter updates are scaled by the inverse root of the exponential moving average of squared gradients (see the code for AdamW in Algorithm 2, ignoring for now the modifications in pink which we discuss in Section 3.5).\n\nThis adaptivity can be a very useful tool for accelerating training, but can also cause issues when the\n```\n\n```markdown\nlearning signal changes. Overall, if $\\beta_2$ is too small then convergence may be slowed [50]. If $\\beta_2$ is too large then $u_t$ can become\n\nIn practice, the EMA is debiased with a correction term. Algorithm 2 follows AdaFactor section 7.1 in applying the correction term to $\\beta_1, \\beta_2$. Adam is often written with the correction term applied to $v_t, u_t$ but they are equivalent [54].\n```", "md": "```markdown\ngradient clipping unless otherwise mentioned             1, though      learning signal changes. Concretely, exponential mov-\nwe do clip the      logit scale   parameter, and ii) we add             ing averages can become out of date causing updates\na layer-norm after the patch embedding and before                       to be scaled by a value that is too large. This issue is\nthe main transformer. Unless otherwise mentioned,                       discussed in Section 5 of Shazeer and Stern               [54], and\nexperiments use batch size 16384 (per-gpu batch size                    of 256), learning rate 2e-3 and weight decay 0.2. We\ninitially tried adding a layer-norm before the patch                    embedding as in [34], but removed this as we found it\nto hurt performance at CLIP ViT-Huge scale.\n```\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Algorithm 2 StableAdamW} & \\{ \\alpha_t \\}_{t=0}, \\beta_1, \\beta_2, \\epsilon \\\\\nv_0, u_0 = 0 & \\\\\n\\text{for } t = 1 \\text{ to } T \\text{ do} & \\\\\ng = \\nabla f(\\theta_t) & \\\\\n\\text{// apply correction term to debias moving avg.} & \\\\\n\\hat{\\beta}_1 = \\beta_1 \\cdot \\frac{1 - \\beta_1^{t-1}}{1 - \\beta_1^t} & \\\\\n\\hat{\\beta}_2 = \\beta_2 \\cdot \\frac{2}{1 - \\beta_2} & \\\\\n\\text{// update moving averages} & \\\\\nv_t = \\hat{\\beta}_1 v_{t-1} + (1 - \\hat{\\beta}_1) g_t & \\\\\nu_t = \\hat{\\beta}_2 u_{t-1} + (1 - \\hat{\\beta}_2) g_t & \\\\\n\\text{// for implementation convenience, the steps} & \\\\\n\\text{// below occur independently for each tensor} & \\\\\nRMS_t = \\sqrt{\\mathbb{E} \\left[ \\frac{g^2}{u_t} \\right]} & \\\\\n\\text{// update parameters} & \\\\\n\\eta_t = \\alpha_t / \\max \\left(1, RMS_t\\right) & \\\\\n\\theta_t = \\theta_{t-1} - \\eta_t \\lambda \\theta_{t-1} - \\eta_t \\frac{v_t}{u_t + \\epsilon} & \\\\\n\\end{array}\n$$\n\n```markdown\nLoss spikes increase with model size, batch size, and learning rate\n\nWe begin our studying of loss spikes by observing how their presence varies when changing model size, batch size, and learning rate. The following sections build on these observations\u2014in particular the finding that lowering the AdamW $\\beta_2$ hyperparameter removes spikes entirely.\n\nWe find that loss spikes increase when increasing model size (Figure 6), batch size (Figure 7), or learning rate (Figure 3). However, we also find that loss spikes can be avoided by reducing the $\\beta_2$ hyperparameter for in AdamW. On the other hand, if $\\beta_2$ is reduced too much then learning is slowed which results in worse performance [50].\n```\n\n```markdown\nOn $\\beta_2$ and an out-of-date second moment estimator\n\nBased on the observation in the previous section that lowering $\\beta_2$ reduces spikes, this section traces the cause of loss spikes to an out-of-date second moment estimator in the patch embedding layer.\n\nOverview. Adaptive optimizers such as AdaGrad, Adam, or AdaFactor scale the update differently for each individual parameter. This is often conceptualized as per-parameter learning rate.\n\nFor instance, in Adam/AdamW, per-parameter updates are scaled by the inverse root of the exponential moving average of squared gradients (see the code for AdamW in Algorithm 2, ignoring for now the modifications in pink which we discuss in Section 3.5).\n\nThis adaptivity can be a very useful tool for accelerating training, but can also cause issues when the update differently for each individual parameter. This is often conceptualized as per-parameter learning rate.\n\nFor instance, in Adam/AdamW, per-parameter updates are scaled by the inverse root of the exponential moving average of squared gradients (see the code for AdamW in Algorithm 2, ignoring for now the modifications in pink which we discuss in Section 3.5).\n\nThis adaptivity can be a very useful tool for accelerating training, but can also cause issues when the\n```\n\n```markdown\nlearning signal changes. Overall, if $\\beta_2$ is too small then convergence may be slowed [50]. If $\\beta_2$ is too large then $u_t$ can become\n\nIn practice, the EMA is debiased with a correction term. Algorithm 2 follows AdaFactor section 7.1 in applying the correction term to $\\beta_1, \\beta_2$. Adam is often written with the correction term applied to $v_t, u_t$ but they are equivalent [54].\n```"}]}, {"page": 11, "text": "   7.5                                                         RMS spike preceeding loss spike     beta2 = 0.98       beta2 = 0.9\n  Loss5.0\n                2500                  2600                  2700                  2800                  2900                  3000\n     4   visual.conv1.weight\n     3\n     2\n    RMS\n     1          2500                  2600                  2700 Iteration        2800                  2900                  3000\nFigure 9:   The learning signal can change so that the AdamW second moment estimator                       u   is out-of-date and\nunderestimates the squared gradients        g2. This can be detected if the aggregate quantity         RMS  tt= p  E  [g2/ut] is far\n                                             t                                                                          t\nfrom 1. This figure observes a predictive relationship between the event of an RMS spike and a loss spike\u2014 we observe\na spike in RMS   t 1-8 iterations before a loss spike. For lower    \u03b2 2, RMS  t does not deviate far from 1. This result looks\nat RMS  t for the patch embedding layer only. This predictive relationship is further examined in Figures 16 to 21 of\nAppendix D.\nout-of-date and no longer a good estimator for               g 2,  the embedding layer in Section D through Figures\nresulting in per-parameter scaling that is too large.          t   16, 17, 18, 19, 20, and 21.          For analysis purposes,\n                                                                   we define a heuristic to characterize loss and RMS\nMeasurement.          We now discuss measurement of                spikes in   visual.conv1.weight. We then show that 28\nthe aforementioned      stuck-in-the-past        scenario and      out of 30 detected loss spikes follow an RMS spike by\nsearch for a predictive relationship between this event            1-8 iterations, while the probability that a loss spike\nand a loss spike.     We follow Shazeer and Stern           [54]   follows an RMS spike by chance is only 1%. Moreover,\nand measure the following root-mean-square quantity,p              we find that the same predictive relationship does not\nRMS  t  =     E[g  2/u t].  If u t is a good estimator for\n 2                 t                                               exist for the RMS in other transformer layers.\ngt then the aggregate quantity         RMS  t will be around\n1. The   stuck-in-the-past        scenario described above\ncorresponds to an     RMS   t \u226b   1.                               3.5      StableAdamW: AdamW with up-\nAs illustrated in Figures 6-8, we observe instability                       date clipping from AdaFactor\nfor high  \u03b2 2 in our experiments even though we have                This Section develops and tests StableAdamW (Al-\n5k iterations of warm-up. While Shazeer and Stern                  gorithm 2), an AdamW-Adafactor hybrid.\n[54] first recognize the out-of-date second moment                 To stabilize training, the AdaFactor optimizer divides\nestimator issue, in their experimental setting they                the learning rate for iteration      t by 1/  max(RMS     t,1). 3\nonly observe instability without warm-up.                          They refer to this as     update clipping. The effect is to\nWe now aim to establish a predictive relationship be-              slow training when       ut is no longer a good estimator\n                                                                   for  g2.\ntween the   stuck-in-the-past       scenario and loss spikes.            t\nWe present initial results in Figure 9, where we ex-               As discussed in Section 3.4, our stability issues can be\namine    RMS   t for the the visual transformer patch              traced to an out-of-date      u t which is what led Shazeer\nembedding layer,       visual.conv1.weight.       This means       and Stern    [54]  to update clipping, even though their\nthat the expectation is computed over parameters                   stability issues are also solved with warm-up. There-\nin visual.conv1.weight     only.   This figure illustrates a       fore, we port update clipping to the standard AdamW\nfew important findings: i) loss spikes tend to follow              optimizer with       d  =   1 and refer to the resulting\n1-8 iterations after an RMS spike, ii) loss spikes slow            AdamW-Adafactor hybrid as StableAdamW (Algo-\nlearning as recovery time is required, and iii),         RMSt      rithm 2).     A modification we make is to compute\nstays around 1 for lower      \u03b2 2.                                    3They   actually  introduce  a  hyperparameter    d  and  use\nAs this is just one example, we further elaborate on               1/max(RMS    t/d,1), but recommend setting     d  = 1 which we\nthe predictive relationship between an RMS spike in             11 follow.", "md": "7.5 RMS spike preceeding loss spike beta2 = 0.98 beta2 = 0.9 Loss5.0\n\n|2500|2600|2700|2800|2900|3000|\n|---|---|---|---|---|---|\n|4 visual.conv1.weight| | | | | |\n| | | | | |3|\n| | | | |2| |\n| | |RMS| | | |\n|1 2500 2600 2700 Iteration 2800 2900 3000| | | | | |\n\nFigure 9: The learning signal can change so that the AdamW second moment estimator u is out-of-date and underestimates the squared gradients g^2. This can be detected if the aggregate quantity $$\\text{RMS}_t = \\sqrt{E\\left[\\frac{g^2}{u_t}\\right]}$$ is far from 1. This figure observes a predictive relationship between the event of an RMS spike and a loss spike\u2014 we observe a spike in RMS 1-8 iterations before a loss spike. For lower \u03b22, RMS_t does not deviate far from 1. This result looks at RMS_t for the patch embedding layer only. This predictive relationship is further examined in Figures 16 to 21 of Appendix D.\n\nout-of-date and no longer a good estimator for g^2, the embedding layer in Section D through Figures resulting in per-parameter scaling that is too large. For analysis purposes, we define a heuristic to characterize loss and RMS Measurement. We now discuss measurement of the aforementioned stuck-in-the-past scenario and search for a predictive relationship between this event and a loss spike. We follow Shazeer and Stern [54] and measure the following root-mean-square quantity, $$\\text{RMS}_t = E\\left[\\frac{g^2}{u_t}\\right]$$. If u_t is a good estimator for g^2 then the aggregate quantity RMS_t will be around 1. The stuck-in-the-past scenario described above corresponds to an RMS_t \u226b 1.\n\nAs illustrated in Figures 6-8, we observe instability for high \u03b22 in our experiments even though we have 5k iterations of warm-up. While Shazeer and Stern [54] first recognize the out-of-date second moment estimator issue, in their experimental setting they only observe instability without warm-up. We now aim to establish a predictive relationship between the stuck-in-the-past scenario and loss spikes. We present initial results in Figure 9, where we examine RMS_t for the visual transformer patch embedding layer, visual.conv1.weight. This means that the expectation is computed over parameters in visual.conv1.weight only. This figure illustrates a few important findings: i) loss spikes tend to follow 1-8 iterations after an RMS spike, ii) loss spikes slow learning as recovery time is required, and iii), RMS_t stays around 1 for lower \u03b22.\n\nAs this is just one example, we further elaborate on the predictive relationship between an RMS spike in RMSt the visual transformer patch embedding layer, visual.conv1.weight. This means that the expectation is computed over parameters in visual.conv1.weight only. This figure illustrates a few important findings: i) loss spikes tend to follow 1-8 iterations after an RMS spike, ii) loss spikes slow learning as recovery time is required, and iii), RMSt stays around 1 for lower \u03b22.\n\n3.5 StableAdamW: AdamW with update clipping from AdaFactor This Section develops and tests StableAdamW (Algorithm 2), an AdamW-Adafactor hybrid. To stabilize training, the AdaFactor optimizer divides the learning rate for iteration t by 1/max(RMS_t,1). They refer to this as update clipping. The effect is to slow training when u_t is no longer a good estimator for g^2.\n\nAs discussed in Section 3.4, our stability issues can be traced to an out-of-date u_t which is what led Shazeer and Stern [54] to update clipping, even though their stability issues are also solved with warm-up. Therefore, we port update clipping to the standard AdamW optimizer with d = 1 and refer to the resulting AdamW-Adafactor hybrid as StableAdamW (Algorithm 2). A modification we make is to compute $$\\frac{1}{\\max\\left(\\text{RMS}_t/d,1\\right)}$$, but recommend setting d = 1 which we follow.", "images": [], "items": [{"type": "text", "value": "7.5 RMS spike preceeding loss spike beta2 = 0.98 beta2 = 0.9 Loss5.0", "md": "7.5 RMS spike preceeding loss spike beta2 = 0.98 beta2 = 0.9 Loss5.0"}, {"type": "table", "rows": [["2500", "2600", "2700", "2800", "2900", "3000"], ["4 visual.conv1.weight", "", "", "", "", ""], ["", "", "", "", "", "3"], ["", "", "", "", "2", ""], ["", "", "RMS", "", "", ""], ["1 2500 2600 2700 Iteration 2800 2900 3000", "", "", "", "", ""]], "md": "|2500|2600|2700|2800|2900|3000|\n|---|---|---|---|---|---|\n|4 visual.conv1.weight| | | | | |\n| | | | | |3|\n| | | | |2| |\n| | |RMS| | | |\n|1 2500 2600 2700 Iteration 2800 2900 3000| | | | | |", "isPerfectTable": true, "csv": "\"2500\",\"2600\",\"2700\",\"2800\",\"2900\",\"3000\"\n\"4 visual.conv1.weight\",\"\",\"\",\"\",\"\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"3\"\n\"\",\"\",\"\",\"\",\"2\",\"\"\n\"\",\"\",\"RMS\",\"\",\"\",\"\"\n\"1 2500 2600 2700 Iteration 2800 2900 3000\",\"\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 9: The learning signal can change so that the AdamW second moment estimator u is out-of-date and underestimates the squared gradients g^2. This can be detected if the aggregate quantity $$\\text{RMS}_t = \\sqrt{E\\left[\\frac{g^2}{u_t}\\right]}$$ is far from 1. This figure observes a predictive relationship between the event of an RMS spike and a loss spike\u2014 we observe a spike in RMS 1-8 iterations before a loss spike. For lower \u03b22, RMS_t does not deviate far from 1. This result looks at RMS_t for the patch embedding layer only. This predictive relationship is further examined in Figures 16 to 21 of Appendix D.\n\nout-of-date and no longer a good estimator for g^2, the embedding layer in Section D through Figures resulting in per-parameter scaling that is too large. For analysis purposes, we define a heuristic to characterize loss and RMS Measurement. We now discuss measurement of the aforementioned stuck-in-the-past scenario and search for a predictive relationship between this event and a loss spike. We follow Shazeer and Stern [54] and measure the following root-mean-square quantity, $$\\text{RMS}_t = E\\left[\\frac{g^2}{u_t}\\right]$$. If u_t is a good estimator for g^2 then the aggregate quantity RMS_t will be around 1. The stuck-in-the-past scenario described above corresponds to an RMS_t \u226b 1.\n\nAs illustrated in Figures 6-8, we observe instability for high \u03b22 in our experiments even though we have 5k iterations of warm-up. While Shazeer and Stern [54] first recognize the out-of-date second moment estimator issue, in their experimental setting they only observe instability without warm-up. We now aim to establish a predictive relationship between the stuck-in-the-past scenario and loss spikes. We present initial results in Figure 9, where we examine RMS_t for the visual transformer patch embedding layer, visual.conv1.weight. This means that the expectation is computed over parameters in visual.conv1.weight only. This figure illustrates a few important findings: i) loss spikes tend to follow 1-8 iterations after an RMS spike, ii) loss spikes slow learning as recovery time is required, and iii), RMS_t stays around 1 for lower \u03b22.\n\nAs this is just one example, we further elaborate on the predictive relationship between an RMS spike in RMSt the visual transformer patch embedding layer, visual.conv1.weight. This means that the expectation is computed over parameters in visual.conv1.weight only. This figure illustrates a few important findings: i) loss spikes tend to follow 1-8 iterations after an RMS spike, ii) loss spikes slow learning as recovery time is required, and iii), RMSt stays around 1 for lower \u03b22.\n\n3.5 StableAdamW: AdamW with update clipping from AdaFactor This Section develops and tests StableAdamW (Algorithm 2), an AdamW-Adafactor hybrid. To stabilize training, the AdaFactor optimizer divides the learning rate for iteration t by 1/max(RMS_t,1). They refer to this as update clipping. The effect is to slow training when u_t is no longer a good estimator for g^2.\n\nAs discussed in Section 3.4, our stability issues can be traced to an out-of-date u_t which is what led Shazeer and Stern [54] to update clipping, even though their stability issues are also solved with warm-up. Therefore, we port update clipping to the standard AdamW optimizer with d = 1 and refer to the resulting AdamW-Adafactor hybrid as StableAdamW (Algorithm 2). A modification we make is to compute $$\\frac{1}{\\max\\left(\\text{RMS}_t/d,1\\right)}$$, but recommend setting d = 1 which we follow.", "md": "Figure 9: The learning signal can change so that the AdamW second moment estimator u is out-of-date and underestimates the squared gradients g^2. This can be detected if the aggregate quantity $$\\text{RMS}_t = \\sqrt{E\\left[\\frac{g^2}{u_t}\\right]}$$ is far from 1. This figure observes a predictive relationship between the event of an RMS spike and a loss spike\u2014 we observe a spike in RMS 1-8 iterations before a loss spike. For lower \u03b22, RMS_t does not deviate far from 1. This result looks at RMS_t for the patch embedding layer only. This predictive relationship is further examined in Figures 16 to 21 of Appendix D.\n\nout-of-date and no longer a good estimator for g^2, the embedding layer in Section D through Figures resulting in per-parameter scaling that is too large. For analysis purposes, we define a heuristic to characterize loss and RMS Measurement. We now discuss measurement of the aforementioned stuck-in-the-past scenario and search for a predictive relationship between this event and a loss spike. We follow Shazeer and Stern [54] and measure the following root-mean-square quantity, $$\\text{RMS}_t = E\\left[\\frac{g^2}{u_t}\\right]$$. If u_t is a good estimator for g^2 then the aggregate quantity RMS_t will be around 1. The stuck-in-the-past scenario described above corresponds to an RMS_t \u226b 1.\n\nAs illustrated in Figures 6-8, we observe instability for high \u03b22 in our experiments even though we have 5k iterations of warm-up. While Shazeer and Stern [54] first recognize the out-of-date second moment estimator issue, in their experimental setting they only observe instability without warm-up. We now aim to establish a predictive relationship between the stuck-in-the-past scenario and loss spikes. We present initial results in Figure 9, where we examine RMS_t for the visual transformer patch embedding layer, visual.conv1.weight. This means that the expectation is computed over parameters in visual.conv1.weight only. This figure illustrates a few important findings: i) loss spikes tend to follow 1-8 iterations after an RMS spike, ii) loss spikes slow learning as recovery time is required, and iii), RMS_t stays around 1 for lower \u03b22.\n\nAs this is just one example, we further elaborate on the predictive relationship between an RMS spike in RMSt the visual transformer patch embedding layer, visual.conv1.weight. This means that the expectation is computed over parameters in visual.conv1.weight only. This figure illustrates a few important findings: i) loss spikes tend to follow 1-8 iterations after an RMS spike, ii) loss spikes slow learning as recovery time is required, and iii), RMSt stays around 1 for lower \u03b22.\n\n3.5 StableAdamW: AdamW with update clipping from AdaFactor This Section develops and tests StableAdamW (Algorithm 2), an AdamW-Adafactor hybrid. To stabilize training, the AdaFactor optimizer divides the learning rate for iteration t by 1/max(RMS_t,1). They refer to this as update clipping. The effect is to slow training when u_t is no longer a good estimator for g^2.\n\nAs discussed in Section 3.4, our stability issues can be traced to an out-of-date u_t which is what led Shazeer and Stern [54] to update clipping, even though their stability issues are also solved with warm-up. Therefore, we port update clipping to the standard AdamW optimizer with d = 1 and refer to the resulting AdamW-Adafactor hybrid as StableAdamW (Algorithm 2). A modification we make is to compute $$\\frac{1}{\\max\\left(\\text{RMS}_t/d,1\\right)}$$, but recommend setting d = 1 which we follow."}]}, {"page": 12, "text": "        10                  ViT-Huge, Beta2 = 0.99                                                          ViT-Huge\n                                default                                        58.0\n         8                      + grad clipping\n                                + update clipping (= StableAdamW)              57.5\n         6                                                                     57.0\n       Loss                                                                    56.5\n         4\n         2                                                                     56.0                       default\n                                                                               55.5                       + grad clipping\n         0   0    2500  5000  7500  1000012500150001750020000                  55.0                       + update clipping (= StableAdamW)\n                                                                                      0.5     0.8     0.9     0.95    0.98    0.99    0.995\n                                   Iteration                                   Zero-shot ImageNet accuracy   Beta2\n Figure 10: Adding update clipping to AdamW mitigates loss spikes and outperforms other interventions such as\n gradient clipping with norm 1. Code for the AdamW-AdaFactor hybrid we recommend of AdamW + update clipping\n is in Algorithm 2. The left plot shows loss curves for            \u03b2 2 = 0.99 while the right displays accuracy ablating over             \u03b2 2.\n and divide learning rate by            max(RMS      t, 1) indepen-       with spikes in the activations (i.e., features) and gradi-\n dently for each tensor, which is for implementation                      ents. As we\u2019ve previously seen (Figure 5), high feature\n convenience. This means that the expectation will be                     magnitudes can pose challenges for low-precision train-\n computed independently for each layer to produce a                       ing. Moreover, the spikes in the gradient are so large\n different   RMS   t.                                                     that Inf/NaN values occur, which results in the loss\n                                                                          scalar [40] dropping many times.                There are a few\nWe now test how StableAdamW compares with other                   4       takeaways from this observation.First, reducing loss\n stability interventions such as gradient clipping                  or    spikes is an important step to enabling low-precision\n lowering   \u03b2 2. These results, presented in Figure 10 find               training. Second, spikes in gradient magnitude can\n that StableAdamW (i.e., AdamW + update clipping)                         be transient and therefore we may be adjusting the\n outperforms these aforementioned interventions for                       loss scalar too often\u2014if using the PyTorch default\n CLIP ViT-Huge. While gradient clipping and update                        loss scalar, thousands of iterations would be required\n clipping both remove instability, update clipping per-                   before the loss scalar recovered to its value before this\n forms better in terms of zero-shot ImageNet accuracy.                    event.    Finally, the layers highlighted in this figure\nWith update or gradient clipping, higher                 \u03b2 2 such as      are the main layers where Inf/NaN are encountered.\n 0.99 tends to perform better.                                            Concretely, while we only track every tenth block, we\nAppendix E provides further commentary and imple-                         never observe any Inf/NaN for any transformer block\n mentation considerations for StableAdamW.                                greater than 0. However, with the PyTorch default\n                                                                          loss scalar an Inf/NaN in a single layer will skip the\n 3.6      Loss spikes and the loss scalar                                 update for the whole network.\n                                                                          This motivates the loss scalar that we use in our exper-\n                                                                          iments when one is required (except for in Figure 11).\nThis final Section ties the low precision training re-                    We use a loss scalar which i) checks for Inf/NaN at\n sults 2 with our investigation into stability. Overall                   the individual tensor level and skips the update at\nwe find that loss spikes can co-occur with large activa-                  the tensor level\u2014not globally, and ii) remains fixed\n tions and gradients. Large activations and gradients                     at its initial value.\n may cause issues during low precision training due to\n a more limited representible range. Therefore, reduc-                    This scalar allows fp16 mixed precision training for\n ing loss spikes is an important step for successful low                  CLIP models at ViT-Huge scale where previously the\n precision training.                                                      scalar became too low and training diverged [9]. We\n Supporting data is illustrated by Figure 11, in which                    also believe an adaptive block-wise scalar as in Ramesh\n an RMS spike precedes a loss spikes which coincides                      et al.  [48]  would remedy this issue. One interesting\n    4We clip at global norm 1. We observed instability when               remark is that often when we observe an Inf/NaN, it\n trying 2 instead of 1. We did not tune this further, but note            is in the patch embedding layer.               Therefore, in the\n that 1.0 is standard in, e.g., PaLM [11], and Scaling Vision             case where Inf/NaN\u2019s happen frequently it recovers\nTransformers [70].                                                     12 the stability solution of Chen et al.              [8] which is to", "md": "```markdown\n| |ViT-Huge, Beta2 = 0.99|ViT-Huge|\n|---|---|---|\n|default| |58.0|\n|+ grad clipping|+ update clipping (= StableAdamW)|57.5|\n| | |57.0|\n|Loss| |56.5|\n| |56.0|default|\n| |55.5|+ grad clipping|\n|0|0|2500|5000|7500|10000|12500|15000|17500|20000|55.0|default|\n| | | | | | | | | | | |55.5|+ grad clipping|\n| | | | | | | | | | | |55.0|+ update clipping (= StableAdamW)|\n| | | | | | | | | | | | |0.5|0.8|0.9|0.95|0.98|0.99|0.995|\n\n$$\n\\text{Figure 10: Adding update clipping to AdamW mitigates loss spikes and outperforms other interventions such as gradient clipping with norm 1. Code for the AdamW-AdaFactor hybrid we recommend of AdamW + update clipping is in Algorithm 2. The left plot shows loss curves for } \\beta_2 = 0.99 \\text{ while the right displays accuracy ablating over } \\beta_2.\n$$\n\nThis final Section ties the low precision training results with our investigation into stability. Overall we find that loss spikes can co-occur with large activations and gradients. Large activations and gradients may cause issues during low precision training due to a more limited representible range. Therefore, reducing loss spikes is an important step for successful low precision training.\n\n$$\n\\text{Supporting data is illustrated by Figure 11, in which an RMS spike precedes a loss spikes which coincides}\n$$\n```", "images": [], "items": [{"type": "text", "value": "```markdown", "md": "```markdown"}, {"type": "table", "rows": [["", "ViT-Huge, Beta2 = 0.99", "ViT-Huge"], ["default", "", "58.0"], ["+ grad clipping", "+ update clipping (= StableAdamW)", "57.5"], ["", "", "57.0"], ["Loss", "", "56.5"], ["", "56.0", "default"], ["", "55.5", "+ grad clipping"], ["0", "0", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000", "55.0", "default"], ["", "", "", "", "", "", "", "", "", "", "", "55.5", "+ grad clipping"], ["", "", "", "", "", "", "", "", "", "", "", "55.0", "+ update clipping (= StableAdamW)"], ["", "", "", "", "", "", "", "", "", "", "", "", "0.5", "0.8", "0.9", "0.95", "0.98", "0.99", "0.995"]], "md": "| |ViT-Huge, Beta2 = 0.99|ViT-Huge|\n|---|---|---|\n|default| |58.0|\n|+ grad clipping|+ update clipping (= StableAdamW)|57.5|\n| | |57.0|\n|Loss| |56.5|\n| |56.0|default|\n| |55.5|+ grad clipping|\n|0|0|2500|5000|7500|10000|12500|15000|17500|20000|55.0|default|\n| | | | | | | | | | | |55.5|+ grad clipping|\n| | | | | | | | | | | |55.0|+ update clipping (= StableAdamW)|\n| | | | | | | | | | | | |0.5|0.8|0.9|0.95|0.98|0.99|0.995|", "isPerfectTable": false, "csv": "\"\",\"ViT-Huge, Beta2 = 0.99\",\"ViT-Huge\"\n\"default\",\"\",\"58.0\"\n\"+ grad clipping\",\"+ update clipping (= StableAdamW)\",\"57.5\"\n\"\",\"\",\"57.0\"\n\"Loss\",\"\",\"56.5\"\n\"\",\"56.0\",\"default\"\n\"\",\"55.5\",\"+ grad clipping\"\n\"0\",\"0\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\",\"55.0\",\"default\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"55.5\",\"+ grad clipping\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"55.0\",\"+ update clipping (= StableAdamW)\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"0.5\",\"0.8\",\"0.9\",\"0.95\",\"0.98\",\"0.99\",\"0.995\""}, {"type": "text", "value": "$$\n\\text{Figure 10: Adding update clipping to AdamW mitigates loss spikes and outperforms other interventions such as gradient clipping with norm 1. Code for the AdamW-AdaFactor hybrid we recommend of AdamW + update clipping is in Algorithm 2. The left plot shows loss curves for } \\beta_2 = 0.99 \\text{ while the right displays accuracy ablating over } \\beta_2.\n$$\n\nThis final Section ties the low precision training results with our investigation into stability. Overall we find that loss spikes can co-occur with large activations and gradients. Large activations and gradients may cause issues during low precision training due to a more limited representible range. Therefore, reducing loss spikes is an important step for successful low precision training.\n\n$$\n\\text{Supporting data is illustrated by Figure 11, in which an RMS spike precedes a loss spikes which coincides}\n$$\n```", "md": "$$\n\\text{Figure 10: Adding update clipping to AdamW mitigates loss spikes and outperforms other interventions such as gradient clipping with norm 1. Code for the AdamW-AdaFactor hybrid we recommend of AdamW + update clipping is in Algorithm 2. The left plot shows loss curves for } \\beta_2 = 0.99 \\text{ while the right displays accuracy ablating over } \\beta_2.\n$$\n\nThis final Section ties the low precision training results with our investigation into stability. Overall we find that loss spikes can co-occur with large activations and gradients. Large activations and gradients may cause issues during low precision training due to a more limited representible range. Therefore, reducing loss spikes is an important step for successful low precision training.\n\n$$\n\\text{Supporting data is illustrated by Figure 11, in which an RMS spike precedes a loss spikes which coincides}\n$$\n```"}]}, {"page": 13, "text": "   2.6                                                                                                 35  visual.transformer.resblocks.20\n   2.4                                                28                                               30\n   2.2                                                                                                 25\n  Loss                                                                                                 20\n   2.0                                               Loss scaler                                      Feature max\n   1.8     2900  2925   2950  2975   3000             27    2900   2925   2950  2975   3000            15     2900   2925  2950   2975   3000\n                      Iteration                                         Iteration                                         Iteration\n     6 visual.conv1.weight                           100 visual.class_embedding                       10 0 visual.transformer.resblocks.0.attn.in_proj_weight\n     4                                                                                                10 1\n    RMS                                             10 1\n     2                                                                                                10 2\n                                                   Grad2absmax                                       Grad absmax\n     0     2900  2925   2950  2975   3000           10      2900   2925   2950  2975   3000                   2900   2925  2950   2975   3000\n                      Iteration                                         Iteration                                         Iteration\n                               RMS spike in embedding layer which preceeds loss spike   Inf gradient causing grad scaler decrease\nFigure 11: Avoiding loss spikes is helpful for low precision training. As shown in this figure, loss spikes can coincide\nwith with activation spikes and gradient spikes. Large activations/gradients can cause issues during low precision\ntraining due to a more limited representible range [17].\nfreeze the embedding layer. As a final remark, we note                    Acknowledgements\nthat loss spikes do not always cause the loss scalar                      For insightful discussions we thank Romain Beaumont,\nto drop, and emphasize the loss scalar can drop for                       Yair Carmon, Mehdi Cherti, Brian Cheung, Alex Fang,\nvarious other reasons than spikes. Figure 11 is just                      Gabriel     Ilharco,   Jenia    Jitsev,    LAION,      Sarah    Pratt,\nan existence example that loss spikes can result in                       Christoph Schuhmann, Ross Whightman, and Sho Yaida.\nactivation spikes and Inf/NaN gradients.                                  We thank Emad Mostaque and stability.ai for compute\n                                                                          resources.\n                                                                          This work is in part supported by NSF IIS 1652052, IIS\n                                                                          17303166, DARPA N66001-19-2-4031, DARPA W911NF-\n                                                                          15-1-0543 and gifts from Allen Institute for Artificial In-\n4      Conclusion                                                         telligence.\n                                                                          References\nIn summary, we have shared experiments in accelerat-                        [1] Thomas Bachlechner, Bodhisattwa Prasad Majumder,\ning and stabilizing large multi-modal model training                            Henry Mao,        Gary Cottrell,      and Julian McAuley.\nwhich we believe will be useful to the community.                               Rezero is all you need:         Fast convergence at large\nMoreover, we have shared resources such as triton                               depth. In    Uncertainty in Artificial Intelligence, pages\nkernels to enable building and improving on our work.                           1352\u20131361. PMLR, 2021.\nWe believe the main limitation of our work is that\nit is non-exhaustive. For instance, we only simulate                        [2] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin,\n                                                                                Xin Jiang, Qun Liu, Michael R. Lyu, and Irwin King.\nfloat8 training. Second, we do not examine the im-                              Binarybert: Pushing the limit of bert quantization.\npact on stability of width scalings for initialization                          ArXiv, abs/2012.15701, 2021.\nand training hyperparameters such as those examined\nby [19]. Finally, a limitation is that the checkpoints                      [3] Charlie Blake, Douglas Orr, and Carlo Luschi. Unit\nwe produce have low accuracy.                This is due to the                 scaling: Out-of-the-box low-precision training.            arXiv\nlimited compute budget that we use \u2013 our aim was                                preprint arXiv:2303.11257, 2023.\nto study loss spikes across multiple trials at scale and                    [4] Andy     Brock,    Soham      De,   Samuel     L   Smith,    and\nnot attain competitive model performance. A redeem-                             Karen Simonyan. High-performance large-scale image\ning aspect is that our early exploration informed the                           recognition without normalization. In            International\ntraining hyperparameters which produced the highest                             Conference on Machine Learning, pages 1059\u20131071.\naccuracy open-source CLIP model so far [64].                           13       PMLR, 2021.", "md": "## 2.6\n\n35\nvisual.transformer.resblocks.20\n\n## 2.4\n\n28\n\n30\n\n## 2.2\n\n25\n\n## Loss\n\n20\n\n## 2.0\n\nLoss scaler\n\nFeature max\n\n## 1.8\n\n2900\n2925\n2950\n2975\n3000\n27\n2900\n2925\n2950\n2975\n3000\n15\n2900\n2925\n2950\n2975\n3000\n\nIteration\n\nIteration\n\nIteration\n\n|6 visual.conv1.weight|100 visual.class_embedding|10 0 visual.transformer.resblocks.0.attn.in_proj_weight|\n|---|---|---|\n|4| |10 1|\n|RMS|10 1| |\n|2| |10 2|\n| |Grad2absmax| |Grad absmax|\n|0|2900|2925|2950|2975|3000|10|2900|2925|2950|2975|3000|2900|2925|2950|2975|3000|\n\nIteration\n\nIteration\n\nIteration\n\nRMS spike in embedding layer which preceeds loss spike Inf gradient causing grad scaler decrease\n\nFigure 11: Avoiding loss spikes is helpful for low precision training. As shown in this figure, loss spikes can coincide with activation spikes and gradient spikes. Large activations/gradients can cause issues during low precision training due to a more limited representible range [17]. freeze the embedding layer. As a final remark, we note that loss spikes do not always cause the loss scalar to drop, and emphasize the loss scalar can drop for various other reasons than spikes. Figure 11 is just an existence example that loss spikes can result in activation spikes and Inf/NaN gradients. We thank Emad Mostaque and stability.ai for compute resources. This work is in part supported by NSF IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543 and gifts from Allen Institute for Artificial Intelligence.\n\n#### Acknowledgements\n\nFor insightful discussions we thank Romain Beaumont, Yair Carmon, Mehdi Cherti, Brian Cheung, Alex Fang, Gabriel Ilharco, Jenia Jitsev, LAION, Sarah Pratt, Christoph Schuhmann, Ross Whightman, and Sho Yaida.\n\n#### References\n\n1. Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pages 1352\u20131361. PMLR, 2021.\n2. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael R. Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. ArXiv, abs/2012.15701, 2021.\n3. Charlie Blake, Douglas Orr, and Carlo Luschi. Unit scaling: Out-of-the-box low-precision training. arXiv preprint arXiv:2303.11257, 2023.\n4. Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, pages 1059\u20131071. PMLR, 2021.\n\n## Conclusion\n\nIn summary, we have shared experiments in accelerating and stabilizing large multi-modal model training which we believe will be useful to the community. Moreover, we have shared resources such as triton kernels to enable building and improving on our work. We believe the main limitation of our work is that it is non-exhaustive. For instance, we only simulate float8 training. Second, we do not examine the impact on stability of width scalings for initialization and training hyperparameters such as those examined by [19]. Finally, a limitation is that the checkpoints we produce have low accuracy. This is due to the limited compute budget that we use \u2013 our aim was to study loss spikes across multiple trials at scale and not attain competitive model performance. A redeeming aspect is that our early exploration informed the training hyperparameters which produced the highest accuracy open-source CLIP model so far [64].", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "2.6", "md": "## 2.6"}, {"type": "text", "value": "35\nvisual.transformer.resblocks.20", "md": "35\nvisual.transformer.resblocks.20"}, {"type": "heading", "lvl": 2, "value": "2.4", "md": "## 2.4"}, {"type": "text", "value": "28\n\n30", "md": "28\n\n30"}, {"type": "heading", "lvl": 2, "value": "2.2", "md": "## 2.2"}, {"type": "text", "value": "25", "md": "25"}, {"type": "heading", "lvl": 2, "value": "Loss", "md": "## Loss"}, {"type": "text", "value": "20", "md": "20"}, {"type": "heading", "lvl": 2, "value": "2.0", "md": "## 2.0"}, {"type": "text", "value": "Loss scaler\n\nFeature max", "md": "Loss scaler\n\nFeature max"}, {"type": "heading", "lvl": 2, "value": "1.8", "md": "## 1.8"}, {"type": "text", "value": "2900\n2925\n2950\n2975\n3000\n27\n2900\n2925\n2950\n2975\n3000\n15\n2900\n2925\n2950\n2975\n3000\n\nIteration\n\nIteration\n\nIteration", "md": "2900\n2925\n2950\n2975\n3000\n27\n2900\n2925\n2950\n2975\n3000\n15\n2900\n2925\n2950\n2975\n3000\n\nIteration\n\nIteration\n\nIteration"}, {"type": "table", "rows": [["6 visual.conv1.weight", "100 visual.class_embedding", "10 0 visual.transformer.resblocks.0.attn.in_proj_weight"], ["4", "", "10 1"], ["RMS", "10 1", ""], ["2", "", "10 2"], ["", "Grad2absmax", "", "Grad absmax"], ["0", "2900", "2925", "2950", "2975", "3000", "10", "2900", "2925", "2950", "2975", "3000", "2900", "2925", "2950", "2975", "3000"]], "md": "|6 visual.conv1.weight|100 visual.class_embedding|10 0 visual.transformer.resblocks.0.attn.in_proj_weight|\n|---|---|---|\n|4| |10 1|\n|RMS|10 1| |\n|2| |10 2|\n| |Grad2absmax| |Grad absmax|\n|0|2900|2925|2950|2975|3000|10|2900|2925|2950|2975|3000|2900|2925|2950|2975|3000|", "isPerfectTable": false, "csv": "\"6 visual.conv1.weight\",\"100 visual.class_embedding\",\"10 0 visual.transformer.resblocks.0.attn.in_proj_weight\"\n\"4\",\"\",\"10 1\"\n\"RMS\",\"10 1\",\"\"\n\"2\",\"\",\"10 2\"\n\"\",\"Grad2absmax\",\"\",\"Grad absmax\"\n\"0\",\"2900\",\"2925\",\"2950\",\"2975\",\"3000\",\"10\",\"2900\",\"2925\",\"2950\",\"2975\",\"3000\",\"2900\",\"2925\",\"2950\",\"2975\",\"3000\""}, {"type": "text", "value": "Iteration\n\nIteration\n\nIteration\n\nRMS spike in embedding layer which preceeds loss spike Inf gradient causing grad scaler decrease\n\nFigure 11: Avoiding loss spikes is helpful for low precision training. As shown in this figure, loss spikes can coincide with activation spikes and gradient spikes. Large activations/gradients can cause issues during low precision training due to a more limited representible range [17]. freeze the embedding layer. As a final remark, we note that loss spikes do not always cause the loss scalar to drop, and emphasize the loss scalar can drop for various other reasons than spikes. Figure 11 is just an existence example that loss spikes can result in activation spikes and Inf/NaN gradients. We thank Emad Mostaque and stability.ai for compute resources. This work is in part supported by NSF IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543 and gifts from Allen Institute for Artificial Intelligence.", "md": "Iteration\n\nIteration\n\nIteration\n\nRMS spike in embedding layer which preceeds loss spike Inf gradient causing grad scaler decrease\n\nFigure 11: Avoiding loss spikes is helpful for low precision training. As shown in this figure, loss spikes can coincide with activation spikes and gradient spikes. Large activations/gradients can cause issues during low precision training due to a more limited representible range [17]. freeze the embedding layer. As a final remark, we note that loss spikes do not always cause the loss scalar to drop, and emphasize the loss scalar can drop for various other reasons than spikes. Figure 11 is just an existence example that loss spikes can result in activation spikes and Inf/NaN gradients. We thank Emad Mostaque and stability.ai for compute resources. This work is in part supported by NSF IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA W911NF-15-1-0543 and gifts from Allen Institute for Artificial Intelligence."}, {"type": "heading", "lvl": 4, "value": "Acknowledgements", "md": "#### Acknowledgements"}, {"type": "text", "value": "For insightful discussions we thank Romain Beaumont, Yair Carmon, Mehdi Cherti, Brian Cheung, Alex Fang, Gabriel Ilharco, Jenia Jitsev, LAION, Sarah Pratt, Christoph Schuhmann, Ross Whightman, and Sho Yaida.", "md": "For insightful discussions we thank Romain Beaumont, Yair Carmon, Mehdi Cherti, Brian Cheung, Alex Fang, Gabriel Ilharco, Jenia Jitsev, LAION, Sarah Pratt, Christoph Schuhmann, Ross Whightman, and Sho Yaida."}, {"type": "heading", "lvl": 4, "value": "References", "md": "#### References"}, {"type": "text", "value": "1. Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pages 1352\u20131361. PMLR, 2021.\n2. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael R. Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. ArXiv, abs/2012.15701, 2021.\n3. Charlie Blake, Douglas Orr, and Carlo Luschi. Unit scaling: Out-of-the-box low-precision training. arXiv preprint arXiv:2303.11257, 2023.\n4. Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, pages 1059\u20131071. PMLR, 2021.", "md": "1. Thomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth. In Uncertainty in Artificial Intelligence, pages 1352\u20131361. PMLR, 2021.\n2. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael R. Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. ArXiv, abs/2012.15701, 2021.\n3. Charlie Blake, Douglas Orr, and Carlo Luschi. Unit scaling: Out-of-the-box low-precision training. arXiv preprint arXiv:2303.11257, 2023.\n4. Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, pages 1059\u20131071. PMLR, 2021."}, {"type": "heading", "lvl": 2, "value": "Conclusion", "md": "## Conclusion"}, {"type": "text", "value": "In summary, we have shared experiments in accelerating and stabilizing large multi-modal model training which we believe will be useful to the community. Moreover, we have shared resources such as triton kernels to enable building and improving on our work. We believe the main limitation of our work is that it is non-exhaustive. For instance, we only simulate float8 training. Second, we do not examine the impact on stability of width scalings for initialization and training hyperparameters such as those examined by [19]. Finally, a limitation is that the checkpoints we produce have low accuracy. This is due to the limited compute budget that we use \u2013 our aim was to study loss spikes across multiple trials at scale and not attain competitive model performance. A redeeming aspect is that our early exploration informed the training hyperparameters which produced the highest accuracy open-source CLIP model so far [64].", "md": "In summary, we have shared experiments in accelerating and stabilizing large multi-modal model training which we believe will be useful to the community. Moreover, we have shared resources such as triton kernels to enable building and improving on our work. We believe the main limitation of our work is that it is non-exhaustive. For instance, we only simulate float8 training. Second, we do not examine the impact on stability of width scalings for initialization and training hyperparameters such as those examined by [19]. Finally, a limitation is that the checkpoints we produce have low accuracy. This is due to the limited compute budget that we use \u2013 our aim was to study loss spikes across multiple trials at scale and not attain competitive model performance. A redeeming aspect is that our early exploration informed the training hyperparameters which produced the highest accuracy open-source CLIP model so far [64]."}]}, {"page": 14, "text": " [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie                             Masashi Sugiyama, and Roman Garnett, editors,               Ad-\n     Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind                       vances in Neural Information Processing Systems 28:\n     Neelakantan, Pranav Shyam, Girish Sastry, Amanda                         Annual Conference on Neural Information Processing\n     Askell, Sandhini Agarwal, Ariel Herbert-Voss, et al.                     Systems      2015,   December       7-12,   2015,    Montreal,\n     Language models are few-shot learners. In            Advances            Quebec, Canada, pages 3123\u20133131, 2015. URL               https:\n     in Neural Information Processing Systems (NeurIPS),                      //proceedings.neurips.cc/paper/2015/hash/\n     2020.   https://arxiv.org/abs/2005.14165.                                 3e15cc11f979ed25912dff5b0669f2cd-Abstract.\n [6] L\u00b4eopold    Cambier,     Anahita     Bhiwandiwalla,        Ting           html.\n     Gong, Oguz H. Elibol, Mehran Nekuii, and Han-                      [14]   Mostafa Dehghani, Josip Djolonga, Basil Mustafa,\n     lin Tang. Shifted and squeezed 8-bit floating point                       Piotr   Padlewski,     Jonathan     Heek,    Justin    Gilmer,\n     format for low-precision training of deep neural net-                    Andreas Steiner, Mathilde Caron, Robert Geirhos,\n     works. In    8th International Conference on Learning                     Ibrahim Alabdulmohsin, et al. Scaling vision trans-\n     Representations, ICLR 2020, Addis Ababa, Ethiopia,                        formers to 22 billion parameters.            arXiv preprint\n     April 26-30,      2020. OpenReview.net,          2020.     URL           arXiv:2302.05442, 2023.\n     https://openreview.net/forum?id=Bkxe2AVtPS.                        [15]   Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\n [7] Xiangning Chen, Chen Liang, Da Huang, Esteban                             and Li Fei-Fei. Imagenet: A large-scale hierarchical\n     Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi                            image database. In      Conference on Computer Vision\n     Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic                        and Pattern Recognition, 2009.        https://ieeexplore.\n     discovery of optimization algorithms.          arXiv preprint             ieee.org/document/5206848.\n     arXiv:2302.06675, 2023.                                            [16]   Tim Dettmers and Luke Zettlemoyer. The case for\n [8] Xinlei Chen, Saining Xie, and Kaiming He. An empir-                      4-bit precision: k-bit inference scaling laws.            arXiv\n     ical study of training self-supervised vision transform-                  preprint arXiv:2212.09720, 2022.\n     ers. In   Proceedings of the IEEE/CVF International\n     Conference on Computer Vision, pages 9640\u20139649,                    [17]   Tim Dettmers, Mike Lewis, Younes Belkada, and\n     2021.                                                                     Luke Zettlemoyer.       Llm. int8 (): 8-bit matrix mul-\n                                                                               tiplication for transformers at scale.        arXiv preprint\n [9] Mehdi Cherti, Romain Beaumont, Ross Wightman,                            arXiv:2208.07339, 2022.\n     Mitchell    Wortsman,       Gabriel    Ilharco,    Cade    Gor-\n     don, Christoph Schuhmann, Ludwig Schmidt, and                      [18]   Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke\n     Jenia Jitsev.       Reproducible scaling laws for con-                    Zettlemoyer.     8-bit optimizers via block-wise quan-\n     trastive language-image learning.             arXiv preprint              tization.  9th International Conference on Learning\n     arXiv:2212.07143, 2022.                                                  Representations, ICLR, 2022.\n[10] Minsik Cho, Keivan A Vahid, Saurabh Adya, and                      [19]   Emily Dinan, Sho Yaida, and Susan Zhang. Effective\n     Mohammad Rastegari. Dkm: Differentiable k-means                           theory of transformers at initialization.      arXiv preprint\n     clustering layer for neural network compression.          arXiv          arXiv:2304.02034, 2023.\n     preprint arXiv:2108.12659, 2021.                                   [20]   Alexey     Dosovitskiy,       Lucas     Beyer,     Alexander\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob De-                             Kolesnikov,      Dirk    Weissenborn,        Xiaohua      Zhai,\n     vlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,                        Thomas Unterthiner, Mostafa Dehghani, Matthias\n     Paul Barham,        Hyung Won Chung,             Charles Sut-             Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\n     ton, Sebastian Gehrmann, et al.               Palm:     Scaling           reit, and Neil Houlsby.        An image is worth 16x16\n     language modeling with pathways.              arXiv preprint             words: Transformers for image recognition at scale.\n     arXiv:2204.02311, 2022.                                                   In International Conference on Learning Representa-\n                                                                              tions (ICLR), 2021.        https://arxiv.org/abs/2010.\n[12] Jeremy M Cohen, Behrooz Ghorbani, Shankar Kr-                             11929.\n     ishnan, Naman Agarwal, Sourabh Medapati, Michal\n     Badura, Daniel Suo, David Cardoze, Zachary Nado,                   [21]   Mario Drumond, Tao Lin, Martin Jaggi, and Babak\n     George E Dahl, et al. Adaptive gradient methods at                        Falsafi.   Training dnns with hybrid block floating\n     the edge of stability.   arXiv preprint arXiv:2207.14484,                 point.   In Samy Bengio, Hanna M. Wallach, Hugo\n     2022.                                                                     Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi,\n                                                                               and   Roman      Garnett,    editors,    Advances     in  Neu-\n[13] Matthieu Courbariaux, Yoshua Bengio, and Jean-                           ral  Information      Processing     Systems     31:    Annual\n     Pierre David. Binaryconnect: Training deep neural                        Conference       on    Neural     Information       Processing\n     networks with binary weights during propagations.                        Systems 2018, NeurIPS 2018, December 3-8, 2018,\n     In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee,              14      Montr\u00b4eal, Canada, pages 451\u2013461, 2018. URL              https:", "md": "```markdown\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3123\u20133131, 2015. URL [https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract](https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract).\n\n[6] L\u00b4eopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Oguz H. Elibol, Mehran Nekuii, and Han-lin Tang. Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL [https://openreview.net/forum?id=Bkxe2AVtPS](https://openreview.net/forum?id=Bkxe2AVtPS).\n\n[7] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675, 2023.\n\n[8] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9640\u20139649, 2021.\n\n[9] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.\n\n[10] Minsik Cho, Keivan A Vahid, Saurabh Adya, and Mohammad Rastegari. Dkm: Differentiable k-means clustering layer for neural network compression. arXiv preprint arXiv:2108.12659, 2021.\n\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\n[12] Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.\n\n[13] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 451\u2013461, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/).\n\n[14] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023.\n\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009. URL [https://ieeexplore.ieee.org/document/5206848](https://ieeexplore.ieee.org/document/5206848).\n\n[16] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.\n\n[17] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\n[18] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR, 2022.\n\n[19] Emily Dinan, Sho Yaida, and Susan Zhang. Effective theory of transformers at initialization. arXiv preprint arXiv:2304.02034, 2023.\n\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. URL [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929).\n\n[21] Mario Drumond, Tao Lin, Martin Jaggi, and Babak Falsafi. Training dnns with hybrid block floating point. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 451\u2013461, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/).\n```", "images": [], "items": [{"type": "text", "value": "```markdown\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3123\u20133131, 2015. URL [https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract](https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract).\n\n[6] L\u00b4eopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Oguz H. Elibol, Mehran Nekuii, and Han-lin Tang. Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL [https://openreview.net/forum?id=Bkxe2AVtPS](https://openreview.net/forum?id=Bkxe2AVtPS).\n\n[7] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675, 2023.\n\n[8] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9640\u20139649, 2021.\n\n[9] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.\n\n[10] Minsik Cho, Keivan A Vahid, Saurabh Adya, and Mohammad Rastegari. Dkm: Differentiable k-means clustering layer for neural network compression. arXiv preprint arXiv:2108.12659, 2021.\n\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\n[12] Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.\n\n[13] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 451\u2013461, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/).\n\n[14] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023.\n\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009. URL [https://ieeexplore.ieee.org/document/5206848](https://ieeexplore.ieee.org/document/5206848).\n\n[16] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.\n\n[17] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\n[18] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR, 2022.\n\n[19] Emily Dinan, Sho Yaida, and Susan Zhang. Effective theory of transformers at initialization. arXiv preprint arXiv:2304.02034, 2023.\n\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. URL [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929).\n\n[21] Mario Drumond, Tao Lin, Martin Jaggi, and Babak Falsafi. Training dnns with hybrid block floating point. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 451\u2013461, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/).\n```", "md": "```markdown\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Masashi Sugiyama, and Roman Garnett, editors, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 3123\u20133131, 2015. URL [https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract](https://proceedings.neurips.cc/paper/2015/hash/3e15cc11f979ed25912dff5b0669f2cd-Abstract).\n\n[6] L\u00b4eopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Oguz H. Elibol, Mehran Nekuii, and Han-lin Tang. Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL [https://openreview.net/forum?id=Bkxe2AVtPS](https://openreview.net/forum?id=Bkxe2AVtPS).\n\n[7] Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. Symbolic discovery of optimization algorithms. arXiv preprint arXiv:2302.06675, 2023.\n\n[8] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9640\u20139649, 2021.\n\n[9] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.\n\n[10] Minsik Cho, Keivan A Vahid, Saurabh Adya, and Mohammad Rastegari. Dkm: Differentiable k-means clustering layer for neural network compression. arXiv preprint arXiv:2108.12659, 2021.\n\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\n[12] Jeremy M Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E Dahl, et al. Adaptive gradient methods at the edge of stability. arXiv preprint arXiv:2207.14484, 2022.\n\n[13] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural networks with binary weights during propagations. In Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 451\u2013461, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/).\n\n[14] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023.\n\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009. URL [https://ieeexplore.ieee.org/document/5206848](https://ieeexplore.ieee.org/document/5206848).\n\n[16] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.\n\n[17] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\n\n[18] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR, 2022.\n\n[19] Emily Dinan, Sho Yaida, and Susan Zhang. Effective theory of transformers at initialization. arXiv preprint arXiv:2304.02034, 2023.\n\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. URL [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929).\n\n[21] Mario Drumond, Tao Lin, Martin Jaggi, and Babak Falsafi. Training dnns with hybrid block floating point. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 451\u2013461, 2018. URL [https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/).\n```"}]}, {"page": 15, "text": "     //proceedings.neurips.cc/paper/2018/hash/                           [31]   Daya Khudia, Jianyu Huang, Protonu Basu, Sum-\n      6a9aeddfc689c1d0e3b9ccc3ab651bc5-Abstract.                                mer Deng, Haixin Liu, Jongsoo Park, and Mikhail\n      html.                                                                     Smelyanskiy. Fbgemm: Enabling high-performance\n                                                                                low-precision deep learning inference.         arXiv preprint\n[22]  John Duchi, Elad Hazan, and Yoram Singer. Adaptive                       arXiv:2101.05615, 2021.\n      subgradient methods for online learning and stochas-\n      tic optimization.      Journal of machine learning re-             [32]   Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W\n     search, 12(7), 2011.                                                       Mahoney, and Kurt Keutzer. I-bert: Integer-only bert\n                                                                                quantization. In    International conference on machine\n[23]  Angela     Fan,    Pierre    Stock,    Benjamin       Graham,            learning, pages 5506\u20135518. PMLR, 2021.\n      Edouard Grave, R\u00b4emi Gribonval, Herve Jegou, and\n     Armand Joulin.         Training with quantization noise             [33]   Diederik P Kingma and Jimmy Ba. Adam: A method\n      for extreme model compression.               arXiv preprint               for stochastic optimization.         In  International Con-\n     arXiv:2004.07320, 2020.                                                    ference on Learning Representations (ICLR), 2014.\n[24]  Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and                       https://arxiv.org/abs/1412.6980.\n      Dan Alistarh. Gptq: Accurate post-training quanti-                 [34]   Manoj Kumar, Mostafa Dehghani, and Neil Houlsby.\n      zation for generative pre-trained transformers.           arXiv           Dual patchnorm.        arXiv preprint arXiv:2302.01327,\n      preprint arXiv:2210.17323, 2022.                                          2023.\n[25]  Justin Gilmer, Andrea Schioppa, and Jeremy Co-                     [35]   Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph\n      hen.   Intriguing properties of transformer training                      Feichtenhofer, and Kaiming He.             Scaling language-\n      instabilities. To appear.                                                 image pre-training via masking.              arXiv preprint\n                                                                               arXiv:2212.00794, 2022.\n[26]  Xavier Glorot and Yoshua Bengio. Understanding the                 [36]   Ilya Loshchilov and Frank Hutter.             Sgdr:   Stochas-\n      difficulty of training deep feedforward neural networks.                  tic gradient descent with warm restarts.                 In  In-\n      In Proceedings of the thirteenth international confer-                   ternational Conference on Learning Representations\n     ence on artificial intelligence and statistics, pages                     (ICLR), 2016.      https://arxiv.org/abs/1608.03983.\n      249\u2013256. JMLR Workshop and Conference Proceed-\n      ings, 2010.                                                        [37]   Ilya Loshchilov and Frank Hutter. Decoupled weight\n                                                                                decay regularization.       In  International Conference\n[27]  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and                             on Learning Representations (ICLR), 2019.                 https:\n      Jian Sun.     Delving deep into rectifiers: Surpassing                   //openreview.net/forum?id=Bkg6RiCqY7.\n      human-level performance on imagenet classification.\n      In Proceedings of the IEEE international conference                [38]   Naveen Mellempudi, Sudarshan Srinivasan, Dipankar\n     on computer vision, pages 1026\u20131034, 2015.                                 Das, and Bharat Kaul. Mixed precision training with\n                                                                                8-bit floating point.      CoRR, abs/1905.12334, 2019.\n[28]  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian                         URL   http://arxiv.org/abs/1905.12334.\n      Sun. Deep residual learning for image recognition. In\n     Conference on Computer Vision and Pattern Recogni-                  [39]   Paulius    Micikevicius,     Sharan     Narang,     Jonah    Al-\n     tion (CVPR), 2016.         https://arxiv.org/abs/1512.                     ben, Gregory Diamos, Erich Elsen, David Garcia,\n      03385.                                                                    Boris Ginsburg, Michael Houston, Oleksii Kuchaiev,\n                                                                                Ganesh Venkatesh, et al. Mixed precision training.\n[29]  Gabriel Ilharco, Mitchell Wortsman, Ross Wight-                          arXiv preprint arXiv:1710.03740, 2017.\n      man, Cade Gordon, Nicholas Carlini, Rohan Taori,\n     Achal Dave, Vaishaal Shankar, Hongseok Namkoong,                    [40]   Paulius Micikevicius, Dusan Stosic, Neil Burgess,\n      John Miller, Hannaneh Hajishirzi, Ali Farhadi, and                        Marius Cornea, Pradeep Dubey, Richard Grisenth-\n      Ludwig Schmidt. Openclip, July 2021. URL                 https:          waite, Sangwon Ha, Alexander Heinecke, Patrick\n     //doi.org/10.5281/zenodo.5143773. If you use this                          Judd, John Kamalu, et al.            Fp8 formats for deep\n      software, please cite it as below.                                        learning.   arXiv preprint arXiv:2209.05433, 2022.\n[30]  Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong                 [41]   Igor Molybog, Peter Albert, Moya Chen, Zachary\n      Zhu, Matthew Tang, Andrew Howard, Hartwig Adam,                           DeVito, David Esiobu, Naman Goyal, Punit Singh\n      and Dmitry Kalenichenko. Quantization and training                        Koura, Sharan Narang, Andrew Poulton, Ruan Silva,\n      of neural networks for efficient integer-arithmetic-                      et al.  A theory on adam instability in large-scale\n      only inference. arxiv e-prints, art.          arXiv preprint              machine learning.      arXiv preprint arXiv:2304.09871,\n     arXiv:1712.05877, 2017.                                           15       2023.", "md": "# References\n\n# References\n\n[22] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.\n\n[23] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020.\n\n[24] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\n[25] Justin Gilmer, Andrea Schioppa, and Jeremy Cohen. Intriguing properties of transformer training instabilities. To appear.\n\n[26] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.\n\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.\n\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\n[29] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as below.\n\n[30] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. arXiv preprint arXiv:1712.05877, 2017.\n\n[31] Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu, Jongsoo Park, and Mikhail Smelyanskiy. Fbgemm: Enabling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615, 2021.\n\n[32] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 5506\u20135518. PMLR, 2021.\n\n[33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2014. https://arxiv.org/abs/1412.6980.\n\n[34] Manoj Kumar, Mostafa Dehghani, and Neil Houlsby. Dual patchnorm. arXiv preprint arXiv:2302.01327, 2023.\n\n[35] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. arXiv preprint arXiv:2212.00794, 2022.\n\n[36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR), 2016. https://arxiv.org/abs/1608.03983.\n\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. https://openreview.net/forum?id=Bkg6RiCqY7.\n\n[38] Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision training with 8-bit floating point. CoRR, abs/1905.12334, 2019. URL http://arxiv.org/abs/1905.12334.\n\n[39] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.\n\n[40] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.\n\n[41] Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, et al. A theory on adam instability in large-scale machine learning. arXiv preprint arXiv:2304.09871, 2023.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "text", "value": "[22] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.\n\n[23] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020.\n\n[24] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\n[25] Justin Gilmer, Andrea Schioppa, and Jeremy Cohen. Intriguing properties of transformer training instabilities. To appear.\n\n[26] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.\n\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.\n\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\n[29] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as below.\n\n[30] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. arXiv preprint arXiv:1712.05877, 2017.\n\n[31] Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu, Jongsoo Park, and Mikhail Smelyanskiy. Fbgemm: Enabling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615, 2021.\n\n[32] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 5506\u20135518. PMLR, 2021.\n\n[33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2014. https://arxiv.org/abs/1412.6980.\n\n[34] Manoj Kumar, Mostafa Dehghani, and Neil Houlsby. Dual patchnorm. arXiv preprint arXiv:2302.01327, 2023.\n\n[35] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. arXiv preprint arXiv:2212.00794, 2022.\n\n[36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR), 2016. https://arxiv.org/abs/1608.03983.\n\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. https://openreview.net/forum?id=Bkg6RiCqY7.\n\n[38] Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision training with 8-bit floating point. CoRR, abs/1905.12334, 2019. URL http://arxiv.org/abs/1905.12334.\n\n[39] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.\n\n[40] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.\n\n[41] Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, et al. A theory on adam instability in large-scale machine learning. arXiv preprint arXiv:2304.09871, 2023.", "md": "[22] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.\n\n[23] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R\u00e9mi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme model compression. arXiv preprint arXiv:2004.07320, 2020.\n\n[24] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\n\n[25] Justin Gilmer, Andrea Schioppa, and Jeremy Cohen. Intriguing properties of transformer training instabilities. To appear.\n\n[26] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010.\n\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034, 2015.\n\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\n\n[29] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as below.\n\n[30] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. arXiv preprint arXiv:1712.05877, 2017.\n\n[31] Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu, Jongsoo Park, and Mikhail Smelyanskiy. Fbgemm: Enabling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615, 2021.\n\n[32] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert quantization. In International conference on machine learning, pages 5506\u20135518. PMLR, 2021.\n\n[33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2014. https://arxiv.org/abs/1412.6980.\n\n[34] Manoj Kumar, Mostafa Dehghani, and Neil Houlsby. Dual patchnorm. arXiv preprint arXiv:2302.01327, 2023.\n\n[35] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. arXiv preprint arXiv:2212.00794, 2022.\n\n[36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations (ICLR), 2016. https://arxiv.org/abs/1608.03983.\n\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. https://openreview.net/forum?id=Bkg6RiCqY7.\n\n[38] Naveen Mellempudi, Sudarshan Srinivasan, Dipankar Das, and Bharat Kaul. Mixed precision training with 8-bit floating point. CoRR, abs/1905.12334, 2019. URL http://arxiv.org/abs/1905.12334.\n\n[39] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.\n\n[40] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.\n\n[41] Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, et al. A theory on adam instability in large-scale machine learning. arXiv preprint arXiv:2304.09871, 2023."}]}, {"page": 16, "text": "[42]  Gunho Park, Baeseong Park, Se Jung Kwon, Byeong-                         on Computer Vision and Pattern Recognition, pages\n      wook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm:                          10684\u201310695, 2022.\n      Quantized matmul for efficient inference of large-\n      scale generative language models.            arXiv preprint        [52]  Teven Le Scao, Angela Fan, Christopher Akiki, El-\n      arXiv:2206.09557, 2022.                                                  lie Pavlick,    Suzana Ili\u00b4c,    Daniel Hesslow,       Roman\n                                                                               Castagn\u00b4e, Alexandra Sasha Luccioni, Fran\u00b8cois Yvon,\n[43]  Adam Paszke, Sam Gross, Francisco Massa, Adam                            Matthias Gall\u00b4e, et al. Bloom: A 176b-parameter open-\n      Lerer, James Bradbury, Gregory Chanan, Trevor                            access multilingual language model.           arXiv preprint\n      Killeen,    Zeming     Lin,   Natalia    Gimelshein,      Luca           arXiv:2211.05100, 2022.\n      Antiga, et al.     Pytorch: An imperative style, high-\n      performance deep learning library.           In  Advances in       [53]  Christoph Schuhmann, Romain Beaumont, Richard\n      Neural Information Processing Systems (NeurIPS),                         Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\n      2019.   https://arxiv.org/abs/1912.01703.                                Theo    Coombes,      Aarush     Katta,     Clayton    Mullis,\n[44]  Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao                            Mitchell Wortsman, et al. Laion-5b: An open large-\n      Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing                            scale dataset for training next generation image-text\n      Tan, and Quoc V. Le.          Combined scaling for zero-                 models.   arXiv preprint arXiv:2210.08402, 2022.\n      shot transfer learning, 2021.         https://arxiv.org/           [54]  Noam Shazeer and Mitchell Stern. Adafactor: Adap-\n      abs/2111.10050.                                                          tive learning rates with sublinear memory cost. In\n[45]  Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao                            International Conference on Machine Learning, pages\n      Bai, Jingkuan Song, and Nicu Sebe. Binary neural                         4596\u20134604. PMLR, 2018.\n      networks: A survey.        CoRR, abs/2004.03333, 2020.\n      URL    https://arxiv.org/abs/2004.03333.                           [55]  Sheng Shen,      Zhen Dong,       Jiayu Ye,     Linjian Ma,\n                                                                               Zhewei Yao, Amir Gholami, Michael W Mahoney,\n[46]  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya                       and Kurt Keutzer. Q-bert: Hessian based ultra low\n      Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-                       precision quantization of bert.          In  Proceedings of\n      try, Amanda Askell, Pamela Mishkin, Jack Clark,                          the AAAI Conference on Artificial Intelligence, vol-\n      Gretchen Krueger, and Ilya Sutskever.                Learning            ume 34, pages 8815\u20138821, 2020.\n      transferable visual models from natural language su-\n      pervision. In    International Conference on Machine               [56]  Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang\n      Learning (ICML), 2021.           https://arxiv.org/abs/                  Wang,     Swagath       Venkataramani,        Vijayalakshmi\n      2103.00020.                                                              Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash\n                                                                               Gopalakrishnan. Hybrid 8-bit floating point (HFP8)\n[47]  Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie                        training   and   inference    for  deep    neural   networks.\n      Millican,    Jordan    Hoffmann,      Francis    Song,    John           In  Hanna     M.    Wallach,     Hugo    Larochelle,     Alina\n      Aslanides, Sarah Henderson, Roman Ring, Susannah                         Beygelzimer, Florence d\u2019Alch\u00b4e-Buc, Emily B. Fox,\n      Young, et al. Scaling language models: Methods, anal-                    and Roman Garnett, editors,            Advances in Neural\n      ysis & insights from training gopher.         arXiv preprint             Information Processing Systems 32: Annual Confer-\n      arXiv:2112.11446, 2021.                                                  ence on Neural Information Processing Systems 2019,\n[48]  Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott                        NeurIPS 2019,       December 8-14,        2019,   Vancouver,\n      Gray, Chelsea Voss, Alec Radford, Mark Chen, and                         BC, Canada, pages 4901\u20134910, 2019.              URL     https:\n      Ilya Sutskever. Zero-shot text-to-image generation.                      //proceedings.neurips.cc/paper/2019/hash/\n      In  International Conference on Machine Learning,                        65fc9fb4897a89789352e211ca2d398f-Abstract.\n      pages 8821\u20138831. PMLR, 2021.                                             html.\n[49]  Aditya Ramesh,         Prafulla Dhariwal,       Alex Nichol,       [57]  Philippe Tillet, Hsiang-Tsung Kung, and David Cox.\n      Casey Chu,       and Mark Chen.           Hierarchical text-             Triton: an intermediate language and compiler for\n      conditional image generation with clip latents.          arXiv           tiled neural network computations.           In  Proceedings\n      preprint arXiv:2204.06125, 2022.                                         of the 3rd ACM SIGPLAN International Workshop\n[50]  Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On                       on Machine Learning and Programming Languages,\n      the convergence of adam and beyond.            arXiv preprint            pages 10\u201319, 2019.\n      arXiv:1904.09237, 2019.                                            [58]  Hugo Touvron, Matthieu Cord, Alexandre Sablay-\n[51]  Robin     Rombach,      Andreas      Blattmann,      Dominik             rolles, Gabriel Synnaeve, and Herv\u00b4e J\u00b4egou.            Going\n      Lorenz, Patrick Esser, and Bj\u00a8orn Ommer.                 High-           deeper with image transformers. In        Proceedings of the\n      resolution image synthesis with latent diffusion mod-                    IEEE/CVF International Conference on Computer\n      els.  In  Proceedings of the IEEE/CVF Conference                16      Vision, pages 32\u201342, 2021.", "md": "```markdown\n[42] Gunho Park, Baeseong Park, Se Jung Kwon, Byeong-wook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [Link](https://arxiv.org/abs/1912.01703).\n[44] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V. Le. Combined scaling for zero-shot transfer learning, 2021. [Link](https://arxiv.org/abs/2111.10050).\n[45] Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks: A survey. CoRR, abs/2004.03333, 2020. [Link](https://arxiv.org/abs/2004.03333).\n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. [Link](https://arxiv.org/abs/2103.00020).\n[47] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n[48] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[50] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237, 2019.\n[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference 16, 2021.\n[52] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n[54] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596\u20134604. PMLR, 2018.\n[55] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815\u20138821, 2020.\n[56] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.\n[57] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10\u201319, 2019.\n[58] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32\u201342, 2021.\n```", "images": [], "items": [{"type": "text", "value": "```markdown\n[42] Gunho Park, Baeseong Park, Se Jung Kwon, Byeong-wook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [Link](https://arxiv.org/abs/1912.01703).\n[44] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V. Le. Combined scaling for zero-shot transfer learning, 2021. [Link](https://arxiv.org/abs/2111.10050).\n[45] Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks: A survey. CoRR, abs/2004.03333, 2020. [Link](https://arxiv.org/abs/2004.03333).\n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. [Link](https://arxiv.org/abs/2103.00020).\n[47] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n[48] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[50] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237, 2019.\n[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference 16, 2021.\n[52] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n[54] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596\u20134604. PMLR, 2018.\n[55] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815\u20138821, 2020.\n[56] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.\n[57] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10\u201319, 2019.\n[58] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32\u201342, 2021.\n```", "md": "```markdown\n[42] Gunho Park, Baeseong Park, Se Jung Kwon, Byeong-wook Kim, Youngjoo Lee, and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale generative language models. arXiv preprint arXiv:2206.09557, 2022.\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [Link](https://arxiv.org/abs/1912.01703).\n[44] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V. Le. Combined scaling for zero-shot transfer learning, 2021. [Link](https://arxiv.org/abs/2111.10050).\n[45] Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, and Nicu Sebe. Binary neural networks: A survey. CoRR, abs/2004.03333, 2020. [Link](https://arxiv.org/abs/2004.03333).\n[46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. [Link](https://arxiv.org/abs/2103.00020).\n[47] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\n[48] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n[49] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[50] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237, 2019.\n[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference 16, 2021.\n[52] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n[53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n[54] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596\u20134604. PMLR, 2018.\n[55] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815\u20138821, 2020.\n[56] Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks.\n[57] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10\u201319, 2019.\n[58] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herv\u00e9 J\u00e9gou. Going deeper with image transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 32\u201342, 2021.\n```"}]}, {"page": 17, "text": "[59]  Hugo Touvron,        Thibaut Lavril,      Gautier Izacard,             efficient post-training quantization for large language\n     Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee                          models.   arXiv preprint arXiv:2211.10438, 2022.\n      Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Ham-\n      bro, Faisal Azhar, et al.        Llama:     Open and effi-       [67]  Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,\n      cient foundation language models.            arXiv preprint            Xiaoxia Wu, Conglong Li, and Yuxiong He.                 Zero-\n     arXiv:2302.13971, 2023.                                                 quant: Efficient and affordable post-training quan-\n                                                                             tization for large-scale transformers.       arXiv preprint\n[60]  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob                       arXiv:2206.01861, 2022.\n      Uszkoreit,   Llion Jones,     Aidan N Gomez,          Lukasz\n      Kaiser, and Illia Polosukhin.         Attention is all you       [68]  Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\n      need.  Advances in neural information processing sys-                  Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\n     tems, 30, 2017.                                                         Wendi Zheng,       Xiao Xia,      et al.    Glm-130b:      An\n                                                                             open bilingual pre-trained model.            arXiv preprint\n[61]  Naigang     Wang,     Jungwook      Choi,    Daniel   Brand,           arXiv:2210.02414, 2022.\n      Chia-Yu      Chen,     and     Kailash     Gopalakrishnan.\n     Training     deep   neural    networks     with   8-bit  float-   [69]  Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin,\n      ing  point   numbers.       In  S.  Bengio,     H.  Wallach,           Dan Busbridge, Jason Ramapuram, Yizhe Zhang,\n      H. Larochelle, K. Grauman, N. Cesa-Bianchi, and                        Jiatao Gu, and Josh Susskind.             Stabilizing trans-\n      R.  Garnett,     editors,   Advances     in  Neural     Infor-         former training by preventing attention entropy col-\n     mation     Processing    Systems,     volume     31.  Curran            lapse.  arXiv preprint arXiv:2303.06296, 2023.\n     Associates, Inc., 2018. URL         https://proceedings.\n      neurips.cc/paper_files/paper/2018/file/                          [70]  Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby,\n      335d3d1cd7ef05ec77714a215134914c-Paper.pdf.                            and Lucas Beyer. Scaling vision transformers, 2021.\n[62]  Naigang     Wang,     Jungwook      Choi,    Daniel   Brand,           https://arxiv.org/abs/2106.04560.\n      Chia-Yu Chen, and Kailash Gopalakrishnan. Train-                 [71]  Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,\n      ing deep neural networks with 8-bit floating point                     and Lucas Beyer. Sigmoid loss for language image\n      numbers. In Samy Bengio, Hanna M. Wallach, Hugo                        pre-training.   arXiv preprint arXiv:2303.15343, 2023.\n      Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi,\n      and Roman Garnett, editors,           Advances in Neural         [72]  Hongyi Zhang, Yann N Dauphin, and Tengyu Ma.\n     Information Processing Systems 31:             Annual Con-              Fixup initialization: Residual learning without nor-\n      ference on Neural Information Processing Systems                       malization.   arXiv preprint arXiv:1901.09321, 2019.\n     2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal,\n     Canada,      pages   7686\u20137695,      2018.      URL     https:    [73]  Susan Zhang.        Open pretrained transformers lec-\n     //proceedings.neurips.cc/paper/2018/hash/                               ture,   2023.     https://www.youtube.com/watch?v=\n      335d3d1cd7ef05ec77714a215134914c-Abstract.                             p9IxoSkvZ-M.\n      html.                                                            [74]  Susan Zhang, Stephen Roller, Naman Goyal, Mikel\n[63]  Shibo    Wang     and    Pankaj     Kanwar.         Bfloat16:          Artetxe, Moya Chen, Shuohui Chen, Christopher\n     The     secret     to   high    performance        on    cloud          Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n      tpus.           2019.             https://cloud.google.                Opt: Open pre-trained transformer language models.\n      com/blog/products/ai-machine-learning/                                 arXiv preprint arXiv:2205.01068, 2022.\n      bfloat16-the-secret-to-high-performance-on-cloud-tpus.           [75]  Wei Zhang,      Lu Hou,      Yichun Yin,      Lifeng Shang,\n[64]  Mitchell    Wortsman.          Reaching     80%     accuracy           Xiao Chen, Xin Jiang, and Qun Liu.              Ternarybert:\n     with    openclip,     2023.      https://laion.ai/blog/                 Distillation-aware ultra-low bit bert.          In  EMNLP,\n      giant-openclip/.                                                       2020.\n[65]  Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre,\n      Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-               [76]  Changsheng       Zhao,   Ting    Hua,   Yilin   Shen,    Qian\n      cos, Hongseok Namkoong, Ali Farhadi, Yair Car-                         Lou, and Hongxia Jin. Automatic mixed-precision\n      mon, Simon Kornblith, et al. Model soups: averag-                      quantization      search    of  bert.       arXiv    preprint\n      ing weights of multiple fine-tuned models improves                     arXiv:2112.14938, 2021.\n      accuracy without increasing inference time.            In  In-\n     ternational Conference on Machine Learning, pages                 [77]  Kang Zhao,       Sida Huang,      Pan Pan,      Yinghan Li,\n      23965\u201323998. PMLR, 2022.                                               Yingya Zhang, Zhenyu Gu, and Yinghui Xu.                  Dis-\n                                                                             tribution adaptive int8 quantization for training cnns.\n[66]  Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien De-                     In Proceedings of the Thirty-Fifth AAAI Conference\n      mouth, and Song Han. Smoothquant: Accurate and                17       on Artificial Intelligence, 2021.", "md": "# Research Papers\n\n# Research Papers\n\n[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n[61] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL Link.\n\n[62] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 7686\u20137695, 2018. URL Link.\n\n[63] Shibo Wang and Pankaj Kanwar. Bfloat16: The secret to high performance on cloud tpus. 2019. Link.\n\n[64] Mitchell Wortsman. Reaching 80% accuracy with openclip, 2023. Link.\n\n[65] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965\u201323998. PMLR, 2022.\n\n[66] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\n[67] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zero-quant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\n[68] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\n\n[69] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Josh Susskind. Stabilizing transformer training by preventing attention entropy collapse. arXiv preprint arXiv:2303.06296, 2023.\n\n[70] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers, 2021. Link.\n\n[71] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. arXiv preprint arXiv:2303.15343, 2023.\n\n[72] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019.\n\n[73] Susan Zhang. Open pretrained transformers lecture, 2023. Link.\n\n[74] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\n[75] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. In EMNLP, 2020.\n\n[76] Changsheng Zhao, Ting Hua, Yilin Shen, Qian Lou, and Hongxia Jin. Automatic mixed-precision quantization search of bert. arXiv preprint arXiv:2112.14938, 2021.\n\n[77] Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, and Yinghui Xu. Distribution adaptive int8 quantization for training cnns. In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Research Papers", "md": "# Research Papers"}, {"type": "heading", "lvl": 1, "value": "Research Papers", "md": "# Research Papers"}, {"type": "text", "value": "[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n[61] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL Link.\n\n[62] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 7686\u20137695, 2018. URL Link.\n\n[63] Shibo Wang and Pankaj Kanwar. Bfloat16: The secret to high performance on cloud tpus. 2019. Link.\n\n[64] Mitchell Wortsman. Reaching 80% accuracy with openclip, 2023. Link.\n\n[65] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965\u201323998. PMLR, 2022.\n\n[66] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\n[67] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zero-quant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\n[68] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\n\n[69] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Josh Susskind. Stabilizing transformer training by preventing attention entropy collapse. arXiv preprint arXiv:2303.06296, 2023.\n\n[70] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers, 2021. Link.\n\n[71] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. arXiv preprint arXiv:2303.15343, 2023.\n\n[72] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019.\n\n[73] Susan Zhang. Open pretrained transformers lecture, 2023. Link.\n\n[74] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\n[75] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. In EMNLP, 2020.\n\n[76] Changsheng Zhao, Ting Hua, Yilin Shen, Qian Lou, and Hongxia Jin. Automatic mixed-precision quantization search of bert. arXiv preprint arXiv:2112.14938, 2021.\n\n[77] Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, and Yinghui Xu. Distribution adaptive int8 quantization for training cnns. In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021.", "md": "[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n[61] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL Link.\n\n[62] Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Training deep neural networks with 8-bit floating point numbers. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00b4eal, Canada, pages 7686\u20137695, 2018. URL Link.\n\n[63] Shibo Wang and Pankaj Kanwar. Bfloat16: The secret to high performance on cloud tpus. 2019. Link.\n\n[64] Mitchell Wortsman. Reaching 80% accuracy with openclip, 2023. Link.\n\n[65] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, pages 23965\u201323998. PMLR, 2022.\n\n[66] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022.\n\n[67] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zero-quant: Efficient and affordable post-training quantization for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.\n\n[68] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414, 2022.\n\n[69] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Josh Susskind. Stabilizing transformer training by preventing attention entropy collapse. arXiv preprint arXiv:2303.06296, 2023.\n\n[70] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers, 2021. Link.\n\n[71] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. arXiv preprint arXiv:2303.15343, 2023.\n\n[72] Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. arXiv preprint arXiv:1901.09321, 2019.\n\n[73] Susan Zhang. Open pretrained transformers lecture, 2023. Link.\n\n[74] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.\n\n[75] Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. Ternarybert: Distillation-aware ultra-low bit bert. In EMNLP, 2020.\n\n[76] Changsheng Zhao, Ting Hua, Yilin Shen, Qian Lou, and Hongxia Jin. Automatic mixed-precision quantization search of bert. arXiv preprint arXiv:2112.14938, 2021.\n\n[77] Kang Zhao, Sida Huang, Pan Pan, Yinghan Li, Yingya Zhang, Zhenyu Gu, and Yinghui Xu. Distribution adaptive int8 quantization for training cnns. In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence, 2021."}]}, {"page": 18, "text": "[78]  Chenzhuo Zhu, Song Han, Huizi Mao, and William J.\n      Dally. Trained ternary quantization. In            5th Interna-\n      tional Conference on Learning Representations, ICLR\n      2017, Toulon, France, April 24-26, 2017, Confer-\n      ence Track Proceedings. OpenReview.net, 2017. URL\n      https://openreview.net/forum?id=S1_pAu9xl.\n[79]  Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu,\n      Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan.\n      Towards unified int8 training for convolutional neural\n      network. In    Proceedings of the IEEE/CVF Conference\n      on Computer Vision and Pattern Recognition, pages\n      1969\u20131979, 2020.                                                 18", "md": "[78] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=S1_pAu9xl.\n\n[79] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1969\u20131979, 2020.", "images": [], "items": [{"type": "text", "value": "[78] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=S1_pAu9xl.\n\n[79] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1969\u20131979, 2020.", "md": "[78] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=S1_pAu9xl.\n\n[79] Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li, Xiuqi Yang, and Junjie Yan. Towards unified int8 training for convolutional neural network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1969\u20131979, 2020."}]}, {"page": 19, "text": "A       Additional Related Work on                                    \u2022  Algorithm 3 is the memory effecient variant of\n        Quantization                                                     SwitchBack.\n                                                                      \u2022  Algorithm 4 is the variant of SwitchBack which\nThe literature on training neural networks in low-bit                    uses row- and column-wise quantization for the\nprecision is vast. The main differentiating factor of                    weights.\nour work is that we train relatively large models \u2013 in\nfact, we train the largest 8-bit vision transformers to               \u2022  Algorithm 5 is a standard linear layer imple-\ndate.                                                                    mented with      torch.autograd.\nThe literature agrees that quantization of very large              These     implementations        can   be   found    at   https:\nnetworks is more difficult than for smaller networks               //github.com/TimDettmers/bitsandbytes/\n[18, 17,  66,  24]. As such, we divide our related work            blob/main/bitsandbytes/nn/triton_based_\ninto three parts:      (1) large-scale low-precision neu-          modules.py.           To    use    in   OpenCLIP        training\nral network (larger than BERT-large), and (2) low-                 (https://github.com/mlfoundations/open_clip),\nprecision training of smaller networks.                            add the argument:\n                                                                      \u2022  --use-bnb-linear SwitchBackLinearGlobal\nLarge-scale Low-precision Neural Networks.                               for Algorithm 1.\nOur work is currently the only work that does low-                    \u2022  --use-bnb-linear SwitchBackLinearGlobalMemEfficient\nprecision (8-bit and below) training of very large net-                  for Algorithm 3.\nworks with more than 230M parameters. Other re-\nlated work studies inference at scale. SmoothQuant                    \u2022  --use-bnb-linear SwitchBackLinearVectorwise\n[66], ZeroQuant [67], NuQmm [42], and LLM.int8()                         for Algorithm 4.\n[17] study inference with Int8 matrix multiplication.                 \u2022  --use-bnb-linear StandardLinear                  for  Algo-\nAnother line of work studies large models inference                      rithm 5.\nwith more than 250M parameters by considering 16-\nbit inputs and k-bit weights [16, 24, 68].                         B.2       Additional Figures\nSmall Scale Low-precision Training                     Training    This section presents additional figures.\nof small-scale low-precision neural networks can take\nmany shapes and forms, such as quantization for in-                   \u2022  Figure 12 presents a more fine-grained version of\nteger only devices, quantization for mobile device, or                   Figure 3.\nquantization to accelerate training. One way to break                 \u2022  Figure 13 compares the speed-up of SwitchBack\nup these directions is through the data type used                        compared to LLM.int8().\nand the neural network trained. One major direction\nis to quantize convolutional neural networks often                    \u2022  Figure 14 shows the mean and max for the gra-\nfor fast and memory efficient usage on edge devices                      dient and activation (i.e., feature) throughout\n[78,  6, 13, 79,  23,  77, 30]. Further work in this area                training.\nis discussed in the survey by [45].           Another line of         \u2022  Figure 15 shows that using a schedule for         \u03b22 of the\nwork is centered around 8-bit float data types which                     form 1   \u2212  iteration \u2212\u03bb  does not improve accuracy.\ncan be used to accelerate training of neural networks\n[21,  56, 62,  38,  3]. Lastly, a common application is\nto finetune (similar to training) BERT models to par-              C       Analysis\nticular datasets. This not only decreases the model\nfootprint and increases inference speed but adjusts                Consider a matrix multiplication            U V  for  U  \u2208  Rn\u00d7k\nthe model to new data [2, 32, 75, 55, 76].                         and   V  \u2208  R k\u00d7m  . This matmul consists of computing\n                                                                   inner products between vectors of length            k.\n                                                                   This section shows that error due to quantization\nB       Additional code and figures                                increases with     k. This suggests why SwitchBack may\n                                                                   achieve high accuracy, as we avoid quantizing matmuls\nB.1       Additional Code                                          for which     k  is very large.     For the weight gradient\nThis Section provides additional pseudocode:                     19computation, which we leave in high precision,                k is", "md": "# Additional Related Work on Quantization\n\n## Additional Related Work on Quantization\n\n- Algorithm 3 is the memory efficient variant of SwitchBack.\n- Algorithm 4 is the variant of SwitchBack which uses row- and column-wise quantization for the weights.\n- Algorithm 5 is a standard linear layer implemented with torch.autograd.\n\nThese implementations can be found at https://github.com/TimDettmers/bitsandbytes/nn/triton_based_modules.py. To use in OpenCLIP training (https://github.com/mlfoundations/open_clip), add the argument:\n\n- --use-bnb-linear SwitchBackLinearGlobal for Algorithm 1.\n- --use-bnb-linear SwitchBackLinearGlobalMemEfficient for Algorithm 3.\n- --use-bnb-linear SwitchBackLinearVectorwise for Algorithm 4.\n- --use-bnb-linear StandardLinear for Algorithm 5.\n\n### Additional Figures\n\n- Figure 12 presents a more fine-grained version of Figure 3.\n- Figure 13 compares the speed-up of SwitchBack compared to LLM.int8().\n- Figure 14 shows the mean and max for the gradient and activation throughout training.\n- Figure 15 shows that using a schedule for $\\beta^2$ of the form $1 - \\text{iteration}^{-\\lambda}$ does not improve accuracy.\n\n## Analysis\n\nConsider a matrix multiplication $$UV$$ for $$U \\in \\mathbb{R}^{n \\times k}$$ and $$V \\in \\mathbb{R}^{k \\times m}$$. This matmul consists of computing inner products between vectors of length $$k$$.\n\nThis section shows that error due to quantization increases with $$k$$. This suggests why SwitchBack may achieve high accuracy, as we avoid quantizing matmuls for which $$k$$ is very large. For the weight gradient computation, which we leave in high precision, $$k$$ is...", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Additional Related Work on Quantization", "md": "# Additional Related Work on Quantization"}, {"type": "heading", "lvl": 2, "value": "Additional Related Work on Quantization", "md": "## Additional Related Work on Quantization"}, {"type": "text", "value": "- Algorithm 3 is the memory efficient variant of SwitchBack.\n- Algorithm 4 is the variant of SwitchBack which uses row- and column-wise quantization for the weights.\n- Algorithm 5 is a standard linear layer implemented with torch.autograd.\n\nThese implementations can be found at https://github.com/TimDettmers/bitsandbytes/nn/triton_based_modules.py. To use in OpenCLIP training (https://github.com/mlfoundations/open_clip), add the argument:\n\n- --use-bnb-linear SwitchBackLinearGlobal for Algorithm 1.\n- --use-bnb-linear SwitchBackLinearGlobalMemEfficient for Algorithm 3.\n- --use-bnb-linear SwitchBackLinearVectorwise for Algorithm 4.\n- --use-bnb-linear StandardLinear for Algorithm 5.", "md": "- Algorithm 3 is the memory efficient variant of SwitchBack.\n- Algorithm 4 is the variant of SwitchBack which uses row- and column-wise quantization for the weights.\n- Algorithm 5 is a standard linear layer implemented with torch.autograd.\n\nThese implementations can be found at https://github.com/TimDettmers/bitsandbytes/nn/triton_based_modules.py. To use in OpenCLIP training (https://github.com/mlfoundations/open_clip), add the argument:\n\n- --use-bnb-linear SwitchBackLinearGlobal for Algorithm 1.\n- --use-bnb-linear SwitchBackLinearGlobalMemEfficient for Algorithm 3.\n- --use-bnb-linear SwitchBackLinearVectorwise for Algorithm 4.\n- --use-bnb-linear StandardLinear for Algorithm 5."}, {"type": "heading", "lvl": 3, "value": "Additional Figures", "md": "### Additional Figures"}, {"type": "text", "value": "- Figure 12 presents a more fine-grained version of Figure 3.\n- Figure 13 compares the speed-up of SwitchBack compared to LLM.int8().\n- Figure 14 shows the mean and max for the gradient and activation throughout training.\n- Figure 15 shows that using a schedule for $\\beta^2$ of the form $1 - \\text{iteration}^{-\\lambda}$ does not improve accuracy.", "md": "- Figure 12 presents a more fine-grained version of Figure 3.\n- Figure 13 compares the speed-up of SwitchBack compared to LLM.int8().\n- Figure 14 shows the mean and max for the gradient and activation throughout training.\n- Figure 15 shows that using a schedule for $\\beta^2$ of the form $1 - \\text{iteration}^{-\\lambda}$ does not improve accuracy."}, {"type": "heading", "lvl": 2, "value": "Analysis", "md": "## Analysis"}, {"type": "text", "value": "Consider a matrix multiplication $$UV$$ for $$U \\in \\mathbb{R}^{n \\times k}$$ and $$V \\in \\mathbb{R}^{k \\times m}$$. This matmul consists of computing inner products between vectors of length $$k$$.\n\nThis section shows that error due to quantization increases with $$k$$. This suggests why SwitchBack may achieve high accuracy, as we avoid quantizing matmuls for which $$k$$ is very large. For the weight gradient computation, which we leave in high precision, $$k$$ is...", "md": "Consider a matrix multiplication $$UV$$ for $$U \\in \\mathbb{R}^{n \\times k}$$ and $$V \\in \\mathbb{R}^{k \\times m}$$. This matmul consists of computing inner products between vectors of length $$k$$.\n\nThis section shows that error due to quantization increases with $$k$$. This suggests why SwitchBack may achieve high accuracy, as we avoid quantizing matmuls for which $$k$$ is very large. For the weight gradient computation, which we leave in high precision, $$k$$ is..."}]}, {"page": 20, "text": "       Standard fp16 (sum of parts)           50   Linear layer, batch * sequence length = 32k      35   Linear layer summary, varying dimensions\n       SwitchBack int8 (sum of parts)\n       Matmul XW (standard)                                                                         30\n       Matmul GW (standard)                   40\n       Matmul G^TX (both)                                                                           25\n       Int8 Matmul XW (switchback)            30\n       Int8 Matmul GW (switchback)                                                                  20\n       Row-wise quantize X (switchback)       20\n       Row-wise quantize G (switchback)      time (ms)                                              15\n                                                                                                   % speedup       batch * sequence length = 16384\n       Tensor quantize W (switchback)         10                                                                   batch * sequence length = 32768\n       Tensor quantize and                                                                          10             batch * sequence length = 65536\n       transpose W (switchback)                0                                                      5            batch * sequence length = 131072\n                                                 1024                 2048                4096          1024                2048                 4096\n                                                                      dim                                                    dim\nFigure 12: A more fine-grained version of Figure 3. To reproduce this figure see                        https://github.com/TimDettmers/\nbitsandbytes/tree/main/benchmarking/switchback.\n                                         54.8                                            speedup over LLM.int8()\n                                         54.7\n                                         54.6\n                                         54.5\n                                         54.4\n                                         54.3\n                                        % speedup\n                                         54.2\n                                         54.1\n                                         54.0 ViT-Base                     ViT-Large                    ViT-Huge\nFigure 13: Benchmarking speedups of SwitchBack compared to LLM.int8() [17] for end-to-end CLIP training on a\nsingle node (with 4 A100 GPUs, per-GPU batch size 128, and gradient checkpointing) for various model sizes when\nreplacing all linear operations in the transformer (i.e., key, query, value, and out projections as well as the MLP).\n                                                                          20", "md": "|Operation|Time (ms)|\n|---|---|\n|Standard fp16 (sum of parts)|50|\n|Linear layer, batch * sequence length = 32k|35|\n|Linear layer summary, varying dimensions| |\n|SwitchBack int8 (sum of parts)| |\n|Matmul XW (standard)|30|\n|Matmul GW (standard)|40|\n|Matmul G^TX (both)|25|\n|Int8 Matmul XW (switchback)|30|\n|Int8 Matmul GW (switchback)|20|\n|Row-wise quantize X (switchback)|20|\n|Row-wise quantize G (switchback)|time (ms)|\n|Tensor quantize W (switchback)|10|\n|Tensor quantize and transpose W (switchback)|0|\n\n$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n& 1024 & 2048 & 4096 \\\\\n\\hline\n\\text{dim} & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 12: A more fine-grained version of Figure 3. To reproduce this figure see here.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Operation} & \\text{Speedup over LLM.int8()} \\\\\n\\hline\n54.8 & \\\\\n54.7 & \\\\\n54.6 & \\\\\n54.5 & \\\\\n54.4 & \\\\\n54.3 & \\\\\n\\% \\text{ speedup} & \\\\\n54.2 & \\\\\n54.1 & \\\\\n54.0 & \\text{ViT-Base} \\\\\n& \\text{ViT-Large} \\\\\n& \\text{ViT-Huge} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 13: Benchmarking speedups of SwitchBack compared to LLM.int8() [17] for end-to-end CLIP training on a single node (with 4 A100 GPUs, per-GPU batch size 128, and gradient checkpointing) for various model sizes when replacing all linear operations in the transformer (i.e., key, query, value, and out projections as well as the MLP).", "images": [], "items": [{"type": "table", "rows": [["Operation", "Time (ms)"], ["Standard fp16 (sum of parts)", "50"], ["Linear layer, batch * sequence length = 32k", "35"], ["Linear layer summary, varying dimensions", ""], ["SwitchBack int8 (sum of parts)", ""], ["Matmul XW (standard)", "30"], ["Matmul GW (standard)", "40"], ["Matmul G^TX (both)", "25"], ["Int8 Matmul XW (switchback)", "30"], ["Int8 Matmul GW (switchback)", "20"], ["Row-wise quantize X (switchback)", "20"], ["Row-wise quantize G (switchback)", "time (ms)"], ["Tensor quantize W (switchback)", "10"], ["Tensor quantize and transpose W (switchback)", "0"]], "md": "|Operation|Time (ms)|\n|---|---|\n|Standard fp16 (sum of parts)|50|\n|Linear layer, batch * sequence length = 32k|35|\n|Linear layer summary, varying dimensions| |\n|SwitchBack int8 (sum of parts)| |\n|Matmul XW (standard)|30|\n|Matmul GW (standard)|40|\n|Matmul G^TX (both)|25|\n|Int8 Matmul XW (switchback)|30|\n|Int8 Matmul GW (switchback)|20|\n|Row-wise quantize X (switchback)|20|\n|Row-wise quantize G (switchback)|time (ms)|\n|Tensor quantize W (switchback)|10|\n|Tensor quantize and transpose W (switchback)|0|", "isPerfectTable": true, "csv": "\"Operation\",\"Time (ms)\"\n\"Standard fp16 (sum of parts)\",\"50\"\n\"Linear layer, batch * sequence length = 32k\",\"35\"\n\"Linear layer summary, varying dimensions\",\"\"\n\"SwitchBack int8 (sum of parts)\",\"\"\n\"Matmul XW (standard)\",\"30\"\n\"Matmul GW (standard)\",\"40\"\n\"Matmul G^TX (both)\",\"25\"\n\"Int8 Matmul XW (switchback)\",\"30\"\n\"Int8 Matmul GW (switchback)\",\"20\"\n\"Row-wise quantize X (switchback)\",\"20\"\n\"Row-wise quantize G (switchback)\",\"time (ms)\"\n\"Tensor quantize W (switchback)\",\"10\"\n\"Tensor quantize and transpose W (switchback)\",\"0\""}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n& 1024 & 2048 & 4096 \\\\\n\\hline\n\\text{dim} & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 12: A more fine-grained version of Figure 3. To reproduce this figure see here.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Operation} & \\text{Speedup over LLM.int8()} \\\\\n\\hline\n54.8 & \\\\\n54.7 & \\\\\n54.6 & \\\\\n54.5 & \\\\\n54.4 & \\\\\n54.3 & \\\\\n\\% \\text{ speedup} & \\\\\n54.2 & \\\\\n54.1 & \\\\\n54.0 & \\text{ViT-Base} \\\\\n& \\text{ViT-Large} \\\\\n& \\text{ViT-Huge} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 13: Benchmarking speedups of SwitchBack compared to LLM.int8() [17] for end-to-end CLIP training on a single node (with 4 A100 GPUs, per-GPU batch size 128, and gradient checkpointing) for various model sizes when replacing all linear operations in the transformer (i.e., key, query, value, and out projections as well as the MLP).", "md": "$$\n\\begin{array}{|c|c|c|c|}\n\\hline\n& 1024 & 2048 & 4096 \\\\\n\\hline\n\\text{dim} & & & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 12: A more fine-grained version of Figure 3. To reproduce this figure see here.\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Operation} & \\text{Speedup over LLM.int8()} \\\\\n\\hline\n54.8 & \\\\\n54.7 & \\\\\n54.6 & \\\\\n54.5 & \\\\\n54.4 & \\\\\n54.3 & \\\\\n\\% \\text{ speedup} & \\\\\n54.2 & \\\\\n54.1 & \\\\\n54.0 & \\text{ViT-Base} \\\\\n& \\text{ViT-Large} \\\\\n& \\text{ViT-Huge} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 13: Benchmarking speedups of SwitchBack compared to LLM.int8() [17] for end-to-end CLIP training on a single node (with 4 A100 GPUs, per-GPU batch size 128, and gradient checkpointing) for various model sizes when replacing all linear operations in the transformer (i.e., key, query, value, and out projections as well as the MLP)."}]}, {"page": 21, "text": "   10 3                    ViT-Huge, no layer scale                         103                   ViT-Huge, no layer scale\n                                                                            102\n   10 5                                                                     101\n   10 7                                                                     100\n  MLP weight gradient                                                      10 1\n   10 9                                                                   Transformer block output\n           0    2500   5000  7500  10000  12500  15000  17500  20000              0    2500   5000   7500  10000  12500  15000 17500  20000\n                                  Iteration                                                              Iteration\n   10 3                   ViT-Huge, layer scale init 0                      103                  ViT-Huge, layer scale init 0\n                                                                            102\n   10 5                                                                     101\n   10 7                                                                     100\n  MLP weight gradient                                                      10 1\n   10 9                                                                   Transformer block output\n           0    2500   5000  7500  10000  12500  15000  17500  20000              0    2500   5000   7500  10000  12500  15000 17500  20000\n                                  Iteration                                                              Iteration\n   10 3                   ViT-Large, no layer scale                         103                   ViT-Large, no layer scale\n                                                                            102\n   10 5                                                                     101\n   10 7                                                                     100\n  MLP weight gradient                                                      10 1\n   10 9                                                                   Transformer block output\n           0    2500   5000  7500  10000  12500  15000  17500  20000              0    2500   5000   7500  10000  12500  15000 17500  20000\n                                  Iteration                                                              Iteration\n   10 3                  ViT-Large, layer scale init 0                      103                  ViT-Large, layer scale init 0\n                                                                            102\n   10 5                                                                     101\n   10 7                                                                     100\n  MLP weight gradient                                                      10 1\n   10 9                                                                   Transformer block output\n           0    2500   5000  7500  10000  12500  15000  17500  20000              0    2500   5000   7500  10000  12500  15000 17500  20000\n                                  Iteration                                                              Iteration\n                                      Block .0 max       Block 10 max        Block 20 max         Block 30 max\n                                      Block .0 mean      Block 10 mean       Block 20 mean        Block 30 mean\nFigure 14: The mean and max for (left) the gradient to the MLP weight and (right) the output of a transformer block\nthroughout training. Different rows correspond to different choice of model size and layer scale.\n                                                                     21", "md": "# OCR Text\n\n10 3 $$ViT-Huge, no layer scale$$ 103 $$ViT-Huge, no layer scale$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Huge, layer scale init 0$$ 103 $$ViT-Huge, layer scale init 0$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Large, no layer scale$$ 103 $$ViT-Large, no layer scale$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Large, layer scale init 0$$ 103 $$ViT-Large, layer scale init 0$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\nBlock .0 max Block 10 max Block 20 max Block 30 max\n\nBlock .0 mean Block 10 mean Block 20 mean Block 30 mean\n\nFigure 14: The mean and max for (left) the gradient to the MLP weight and (right) the output of a transformer block throughout training. Different rows correspond to different choice of model size and layer scale.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "text", "value": "10 3 $$ViT-Huge, no layer scale$$ 103 $$ViT-Huge, no layer scale$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Huge, layer scale init 0$$ 103 $$ViT-Huge, layer scale init 0$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Large, no layer scale$$ 103 $$ViT-Large, no layer scale$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Large, layer scale init 0$$ 103 $$ViT-Large, layer scale init 0$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\nBlock .0 max Block 10 max Block 20 max Block 30 max\n\nBlock .0 mean Block 10 mean Block 20 mean Block 30 mean\n\nFigure 14: The mean and max for (left) the gradient to the MLP weight and (right) the output of a transformer block throughout training. Different rows correspond to different choice of model size and layer scale.", "md": "10 3 $$ViT-Huge, no layer scale$$ 103 $$ViT-Huge, no layer scale$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Huge, layer scale init 0$$ 103 $$ViT-Huge, layer scale init 0$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Large, no layer scale$$ 103 $$ViT-Large, no layer scale$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\n10 3 $$ViT-Large, layer scale init 0$$ 103 $$ViT-Large, layer scale init 0$$\n\n102\n\n10 5\n\n10 7\n\nMLP weight gradient\n\n10 1\n\n10 9\n\n0 2500 5000 7500 10000 12500 15000 17500 20000 0 2500 5000 7500 10000 12500 15000 17500 20000\n\nIteration Iteration\n\nBlock .0 max Block 10 max Block 20 max Block 30 max\n\nBlock .0 mean Block 10 mean Block 20 mean Block 30 mean\n\nFigure 14: The mean and max for (left) the gradient to the MLP weight and (right) the output of a transformer block throughout training. Different rows correspond to different choice of model size and layer scale."}]}, {"page": 22, "text": "Algorithm 3          Memory efficient         SwitchBackM                    Algorithm 4          SwitchBack with row-wise and column-\n class SwitchBackMMatmul(autograd.Function):                                 wise quantization for the weights               SwitchBackQ\n     @staticmethod                                                           class SwitchBackQMatmul(autograd.Function):\n     def forward(ctx, X, W):\n         # X [b, n] inputs                                                        @staticmethod\n         # W [n, m] weights                                                       def forward(ctx, X, W):\n                                                                                      # X [b, n] inputs\n         X_int8, state_X = row-wise_quantize(X)                                       # W [n, m] weights\n         del X\n         W_int8, state_W = tensor-wise_quantize(W)                                    # save tensors in ctx\n         # save tensors in ctx                                                        ctx.save_for_backward = X, W\n         ctx.save = X_int8, state_X, W_int8, state_W                                  X_int8, state_X = row-wise_quantize(X)\n         # Return output                                                              W_int8, state_W = row-wise_quantize(W)\n         return matmul_int8_and_dequanitze(                                           # Return output\n              X_int8, W_int8.t(), state_X, state_W                                    return matmul_int8_and_dequanitze(\n         )                                                                            )   X_int8, W_int8.t(), state_X, state_W\n     @staticmethod                                                                @staticmethod\n     def backward(ctx, G):\n         # G [b, m] gradient to output                                            def backward(ctx, G):\n         # Recover tensors from ctx                                                   # G [b, m] gradient to output\n                                                                                      # Recover tensors from ctx\n         X_int8, state_X, W_int8, state_W = ctx.save                                  X, W = ctx.save_for_backward\n         X = dequantize_row-wise(X_int8, state_X)\n         del X_int8                                                                   G_rowwise = rowwise_quantize(G)\n         W_gradient = matmul_fp16(G.t(), X)                                           W_int8, state_W = column-wise_quantize_transpose(W)\n         del X                                                                        # Use 8bit matmul only for X_gradient\n         G_int8 = row-wise_quantize(G)                                                X_gradient = matmul_int8_and_dequanitze(\n         del G                                                                            G_int8, W_int8.t(), state_X, state_W\n         W_int8 = W_int8.t().contiguous()                                             )\n         # Use 8bit matmul only for X_gradient                                        W_gradient = matmul_fp16(G.t(), X)\n         X_gradient = matmul_int8_and_dequanitze(                                     return X_gradient, W_gradient\n         )    G_int8, W_int8.t(), state_X, state_W                           class SwitchBackQLinear(nn.Linear):\n                                                                                  def forward(self, X):\n         return X_gradient, W_gradient                                                return SwitchBackQMatmul.apply(X, self.weight)\n class SwitchBackMLinear(nn.Linear):\n     def forward(self, X):\n         return SwitchBackMMatmul.apply(X, self.weight)\n                                                                             vative because we do not assume the variance of                     \u03f5i, \u03bei\n batch size times sequence length, which is often                       \u2248    increase with        k, though in practice we believe they\n32000 in our experiments. For the other operations                           would as the absmax of            u  and    v increases with       k.\nwhich comprise a matmul,             k  is less than 4\u00b7embed dim\nwhich is    \u2264  8000 in our experiments. These dimensions\n are standard for CLIP training experiments [46, 9].                         We first examine the variance of               \u02c6\n                                                                                                                            u  \u02c6\n                                                                                                                              ivi. By using that\n                                                                             all random variable are mean centered, this variance\n C.1       Analyzing variance due to quanti-                                 is given by,\n           zation for inner products\nThis section measures the variance due to quantization                                                \u0002         2]\n for the inner product between               u  and   v. Let    u,  v  be         Var(\u02c6u   \u02c6\n                                                                                           v  ) =  E    (\u02c6u \u02c6\n                                                                                          i  i             ivi)                                    (7)\nvectors of length        k  vectors with each element drawn                                     =  E  \u0002 ((u i +  \u03f5i )\u00b7 (v i +  \u03bei)) 2]             (8)\n i.i.d. from a distribution with mean 0. Let                    u i have                              \u0002                                    2]\nvariance     \u03c3 2  and   vi have variance       \u03c3 2 .                                            =  E    (u ivi +   \u03f5ivi +  \u03be iu i+   \u03f5i\u03bei)         (9)\n               u                                 v                                              =  E  \u0002 u 2v 2 +  \u03f52 v2  +  \u03be 2u 2 +  \u03f52 \u03be2 ]     (10)\n Next, let     \u02c6                                                                                          i i      i  i      i   i     i  i\n               u   and   v\u02c6  be the quantized versions of               u                       = Var(u      v ) +   \u03c32 (\u03c3 2 +  \u03c3 2  +  \u03c3 2).     (11)\n and   v, respectively. We model quantization error as                                                     i  i       q    u      v       q\n \u02c6\n ui  =   u i +  \u03f5i  and   v\u02c6i  =  v i +   \u03bei where     \u03f5i, \u03bei  are i.i.d.\n mean centered random variables with variance                      \u03c3 2.\nThe aim of this section is to show that variance due                 q       Next, we use linearity of variance for independent\n to quantization grows with            k. Our analysis is conser-            random variables to calculate                Var (\u27e8\u02c6 u,  \u02c6\n                                                                          22                                                         v\u27e9).    This is", "md": "Algorithm 3          Memory efficient         SwitchBackM                    Algorithm 4          SwitchBack with row-wise and column-\nclass SwitchBackMMatmul(autograd.Function):                                 wise quantization for the weights               SwitchBackQ\n@staticmethod                                                           class SwitchBackQMatmul(autograd.Function):\ndef forward(ctx, X, W):\n# X [b, n] inputs                                                        @staticmethod\n# W [n, m] weights                                                       def forward(ctx, X, W):\n# X [b, n] inputs\nX_int8, state_X = row-wise_quantize(X)                                       # W [n, m] weights\ndel X\nW_int8, state_W = tensor-wise_quantize(W)                                    # save tensors in ctx\n# save tensors in ctx                                                        ctx.save_for_backward = X, W\nctx.save = X_int8, state_X, W_int8, state_W                                  X_int8, state_X = row-wise_quantize(X)\n# Return output                                                              W_int8, state_W = row-wise_quantize(W)\nreturn matmul_int8_and_dequanitze(                                           # Return output\nX_int8, W_int8.t(), state_X, state_W                                    return matmul_int8_and_dequanitze(\n)                                                                            )   X_int8, W_int8.t(), state_X, state_W\n@staticmethod                                                                @staticmethod\ndef backward(ctx, G):\n# G [b, m] gradient to output                                            def backward(ctx, G):\n# Recover tensors from ctx                                                   # G [b, m] gradient to output\n# Recover tensors from ctx\nX_int8, state_X, W_int8, state_W = ctx.save                                  X, W = ctx.save_for_backward\nX = dequantize_row-wise(X_int8, state_X)\ndel X_int8                                                                   G_rowwise = rowwise_quantize(G)\nW_gradient = matmul_fp16(G.t(), X)                                           W_int8, state_W = column-wise_quantize_transpose(W)\ndel X                                                                        # Use 8bit matmul only for X_gradient\nG_int8 = row-wise_quantize(G)                                                X_gradient = matmul_int8_and_dequanitze(\ndel G                                                                            G_int8, W_int8.t(), state_X, state_W\nW_int8 = W_int8.t().contiguous()                                             )\n# Use 8bit matmul only for X_gradient                                        W_gradient = matmul_fp16(G.t(), X)\nX_gradient = matmul_int8_and_dequanitze(                                     return X_gradient, W_gradient\n)    G_int8, W_int8.t(), state_X, state_W                           class SwitchBackQLinear(nn.Linear):\ndef forward(self, X):\nreturn X_gradient, W_gradient                                                return SwitchBackQMatmul.apply(X, self.weight)\nclass SwitchBackMLinear(nn.Linear):\ndef forward(self, X):\nreturn SwitchBackMMatmul.apply(X, self.weight)\nvative because we do not assume the variance of                     $$\\epsilon_i, \\xi_i$$\nbatch size times sequence length, which is often                       $$\\approx$$    increase with        $$k$$, though in practice we believe they\n32000 in our experiments. For the other operations                           would as the absmax of            $$u$$  and    $$v$$ increases with       $$k$$.\nwhich comprise a matmul,             $$k$$  is less than 4\u00b7embed dim\nwhich is    $$\\leq$$  8000 in our experiments. These dimensions\nare standard for CLIP training experiments [46, 9].                         We first examine the variance of               $$\\hat{u} \\hat{v}$$. By using that\nall random variable are mean centered, this variance\nis given by,\n$$\n\\text{Var}(\\hat{u} \\hat{v}) = \\mathbb{E}(\\hat{u} \\hat{v}) = \\mathbb{E} \\left( (u_i +  \\epsilon_i) \\cdot (v_i +  \\xi_i) \\right) = \\mathbb{E} \\left( u_i v_i +  \\epsilon^2 v_i^2  +  \\xi^2 u_i^2 +  \\epsilon^2 \\xi^2 \\right)\n$$\n$$\n= \\text{Var}(u v) +  \\sigma^2 (\\sigma^2 +  \\sigma^2  +  \\sigma^2).\n$$\nNext, we use linearity of variance for independent\nrandom variables to calculate                $$\\text{Var} \\left( \\langle \\hat{u},  \\hat{v} \\rangle \\right)$$. This is\n\nC.1       Analyzing variance due to quanti-\nzation for inner products\nThis section measures the variance due to quantization                                                for the inner product between               $$u$$  and   $$v$$. Let    $$u,  v$$  be         vectors of length        $$k$$  vectors with each element drawn                                     i.i.d. from a distribution with mean 0. Let                    $$u_i$$ have                              variance     $$\\sigma^2$$  and   $$v_i$$ have variance       $$\\sigma^2$$.\n$$\\hat{u}$$   and   $$\\hat{v}$$  be the quantized versions of               $$u$$                       and   $$v$$, respectively. We model quantization error as                                                     $$\\hat{u}_i  =   u_i +  \\epsilon_i$$  and   $$\\hat{v}_i  =  v_i +   \\xi_i$$ where     $$\\epsilon_i, \\xi_i$$  are i.i.d.\nmean centered random variables with variance                      $$\\sigma^2$$.\nThe aim of this section is to show that variance due                 to quantization grows with            $$k$$. Our analysis is conser-            q       Next, we use linearity of variance for independent\nrandom variables to calculate                $$\\text{Var} \\left( \\langle \\hat{u},  \\hat{v} \\rangle \\right)$$. This is", "images": [], "items": [{"type": "text", "value": "Algorithm 3          Memory efficient         SwitchBackM                    Algorithm 4          SwitchBack with row-wise and column-\nclass SwitchBackMMatmul(autograd.Function):                                 wise quantization for the weights               SwitchBackQ\n@staticmethod                                                           class SwitchBackQMatmul(autograd.Function):\ndef forward(ctx, X, W):", "md": "Algorithm 3          Memory efficient         SwitchBackM                    Algorithm 4          SwitchBack with row-wise and column-\nclass SwitchBackMMatmul(autograd.Function):                                 wise quantization for the weights               SwitchBackQ\n@staticmethod                                                           class SwitchBackQMatmul(autograd.Function):\ndef forward(ctx, X, W):"}, {"type": "heading", "lvl": 1, "value": "X [b, n] inputs                                                        @staticmethod", "md": "# X [b, n] inputs                                                        @staticmethod"}, {"type": "heading", "lvl": 1, "value": "W [n, m] weights                                                       def forward(ctx, X, W):", "md": "# W [n, m] weights                                                       def forward(ctx, X, W):"}, {"type": "heading", "lvl": 1, "value": "X [b, n] inputs", "md": "# X [b, n] inputs"}, {"type": "text", "value": "X_int8, state_X = row-wise_quantize(X)                                       # W [n, m] weights\ndel X\nW_int8, state_W = tensor-wise_quantize(W)                                    # save tensors in ctx", "md": "X_int8, state_X = row-wise_quantize(X)                                       # W [n, m] weights\ndel X\nW_int8, state_W = tensor-wise_quantize(W)                                    # save tensors in ctx"}, {"type": "heading", "lvl": 1, "value": "save tensors in ctx                                                        ctx.save_for_backward = X, W", "md": "# save tensors in ctx                                                        ctx.save_for_backward = X, W"}, {"type": "text", "value": "ctx.save = X_int8, state_X, W_int8, state_W                                  X_int8, state_X = row-wise_quantize(X)", "md": "ctx.save = X_int8, state_X, W_int8, state_W                                  X_int8, state_X = row-wise_quantize(X)"}, {"type": "heading", "lvl": 1, "value": "Return output                                                              W_int8, state_W = row-wise_quantize(W)", "md": "# Return output                                                              W_int8, state_W = row-wise_quantize(W)"}, {"type": "text", "value": "return matmul_int8_and_dequanitze(                                           # Return output\nX_int8, W_int8.t(), state_X, state_W                                    return matmul_int8_and_dequanitze(\n)                                                                            )   X_int8, W_int8.t(), state_X, state_W\n@staticmethod                                                                @staticmethod\ndef backward(ctx, G):", "md": "return matmul_int8_and_dequanitze(                                           # Return output\nX_int8, W_int8.t(), state_X, state_W                                    return matmul_int8_and_dequanitze(\n)                                                                            )   X_int8, W_int8.t(), state_X, state_W\n@staticmethod                                                                @staticmethod\ndef backward(ctx, G):"}, {"type": "heading", "lvl": 1, "value": "G [b, m] gradient to output                                            def backward(ctx, G):", "md": "# G [b, m] gradient to output                                            def backward(ctx, G):"}, {"type": "heading", "lvl": 1, "value": "Recover tensors from ctx                                                    G [b, m] gradient to output", "md": "# Recover tensors from ctx                                                   # G [b, m] gradient to output"}, {"type": "heading", "lvl": 1, "value": "Recover tensors from ctx", "md": "# Recover tensors from ctx"}, {"type": "text", "value": "X_int8, state_X, W_int8, state_W = ctx.save                                  X, W = ctx.save_for_backward\nX = dequantize_row-wise(X_int8, state_X)\ndel X_int8                                                                   G_rowwise = rowwise_quantize(G)\nW_gradient = matmul_fp16(G.t(), X)                                           W_int8, state_W = column-wise_quantize_transpose(W)\ndel X                                                                        # Use 8bit matmul only for X_gradient\nG_int8 = row-wise_quantize(G)                                                X_gradient = matmul_int8_and_dequanitze(\ndel G                                                                            G_int8, W_int8.t(), state_X, state_W\nW_int8 = W_int8.t().contiguous()                                             )", "md": "X_int8, state_X, W_int8, state_W = ctx.save                                  X, W = ctx.save_for_backward\nX = dequantize_row-wise(X_int8, state_X)\ndel X_int8                                                                   G_rowwise = rowwise_quantize(G)\nW_gradient = matmul_fp16(G.t(), X)                                           W_int8, state_W = column-wise_quantize_transpose(W)\ndel X                                                                        # Use 8bit matmul only for X_gradient\nG_int8 = row-wise_quantize(G)                                                X_gradient = matmul_int8_and_dequanitze(\ndel G                                                                            G_int8, W_int8.t(), state_X, state_W\nW_int8 = W_int8.t().contiguous()                                             )"}, {"type": "heading", "lvl": 1, "value": "Use 8bit matmul only for X_gradient                                        W_gradient = matmul_fp16(G.t(), X)", "md": "# Use 8bit matmul only for X_gradient                                        W_gradient = matmul_fp16(G.t(), X)"}, {"type": "text", "value": "X_gradient = matmul_int8_and_dequanitze(                                     return X_gradient, W_gradient\n)    G_int8, W_int8.t(), state_X, state_W                           class SwitchBackQLinear(nn.Linear):\ndef forward(self, X):\nreturn X_gradient, W_gradient                                                return SwitchBackQMatmul.apply(X, self.weight)\nclass SwitchBackMLinear(nn.Linear):\ndef forward(self, X):\nreturn SwitchBackMMatmul.apply(X, self.weight)\nvative because we do not assume the variance of                     $$\\epsilon_i, \\xi_i$$\nbatch size times sequence length, which is often                       $$\\approx$$    increase with        $$k$$, though in practice we believe they\n32000 in our experiments. For the other operations                           would as the absmax of            $$u$$  and    $$v$$ increases with       $$k$$.\nwhich comprise a matmul,             $$k$$  is less than 4\u00b7embed dim\nwhich is    $$\\leq$$  8000 in our experiments. These dimensions\nare standard for CLIP training experiments [46, 9].                         We first examine the variance of               $$\\hat{u} \\hat{v}$$. By using that\nall random variable are mean centered, this variance\nis given by,\n$$\n\\text{Var}(\\hat{u} \\hat{v}) = \\mathbb{E}(\\hat{u} \\hat{v}) = \\mathbb{E} \\left( (u_i +  \\epsilon_i) \\cdot (v_i +  \\xi_i) \\right) = \\mathbb{E} \\left( u_i v_i +  \\epsilon^2 v_i^2  +  \\xi^2 u_i^2 +  \\epsilon^2 \\xi^2 \\right)\n$$\n$$\n= \\text{Var}(u v) +  \\sigma^2 (\\sigma^2 +  \\sigma^2  +  \\sigma^2).\n$$\nNext, we use linearity of variance for independent\nrandom variables to calculate                $$\\text{Var} \\left( \\langle \\hat{u},  \\hat{v} \\rangle \\right)$$. This is\n\nC.1       Analyzing variance due to quanti-\nzation for inner products\nThis section measures the variance due to quantization                                                for the inner product between               $$u$$  and   $$v$$. Let    $$u,  v$$  be         vectors of length        $$k$$  vectors with each element drawn                                     i.i.d. from a distribution with mean 0. Let                    $$u_i$$ have                              variance     $$\\sigma^2$$  and   $$v_i$$ have variance       $$\\sigma^2$$.\n$$\\hat{u}$$   and   $$\\hat{v}$$  be the quantized versions of               $$u$$                       and   $$v$$, respectively. We model quantization error as                                                     $$\\hat{u}_i  =   u_i +  \\epsilon_i$$  and   $$\\hat{v}_i  =  v_i +   \\xi_i$$ where     $$\\epsilon_i, \\xi_i$$  are i.i.d.\nmean centered random variables with variance                      $$\\sigma^2$$.\nThe aim of this section is to show that variance due                 to quantization grows with            $$k$$. Our analysis is conser-            q       Next, we use linearity of variance for independent\nrandom variables to calculate                $$\\text{Var} \\left( \\langle \\hat{u},  \\hat{v} \\rangle \\right)$$. This is", "md": "X_gradient = matmul_int8_and_dequanitze(                                     return X_gradient, W_gradient\n)    G_int8, W_int8.t(), state_X, state_W                           class SwitchBackQLinear(nn.Linear):\ndef forward(self, X):\nreturn X_gradient, W_gradient                                                return SwitchBackQMatmul.apply(X, self.weight)\nclass SwitchBackMLinear(nn.Linear):\ndef forward(self, X):\nreturn SwitchBackMMatmul.apply(X, self.weight)\nvative because we do not assume the variance of                     $$\\epsilon_i, \\xi_i$$\nbatch size times sequence length, which is often                       $$\\approx$$    increase with        $$k$$, though in practice we believe they\n32000 in our experiments. For the other operations                           would as the absmax of            $$u$$  and    $$v$$ increases with       $$k$$.\nwhich comprise a matmul,             $$k$$  is less than 4\u00b7embed dim\nwhich is    $$\\leq$$  8000 in our experiments. These dimensions\nare standard for CLIP training experiments [46, 9].                         We first examine the variance of               $$\\hat{u} \\hat{v}$$. By using that\nall random variable are mean centered, this variance\nis given by,\n$$\n\\text{Var}(\\hat{u} \\hat{v}) = \\mathbb{E}(\\hat{u} \\hat{v}) = \\mathbb{E} \\left( (u_i +  \\epsilon_i) \\cdot (v_i +  \\xi_i) \\right) = \\mathbb{E} \\left( u_i v_i +  \\epsilon^2 v_i^2  +  \\xi^2 u_i^2 +  \\epsilon^2 \\xi^2 \\right)\n$$\n$$\n= \\text{Var}(u v) +  \\sigma^2 (\\sigma^2 +  \\sigma^2  +  \\sigma^2).\n$$\nNext, we use linearity of variance for independent\nrandom variables to calculate                $$\\text{Var} \\left( \\langle \\hat{u},  \\hat{v} \\rangle \\right)$$. This is\n\nC.1       Analyzing variance due to quanti-\nzation for inner products\nThis section measures the variance due to quantization                                                for the inner product between               $$u$$  and   $$v$$. Let    $$u,  v$$  be         vectors of length        $$k$$  vectors with each element drawn                                     i.i.d. from a distribution with mean 0. Let                    $$u_i$$ have                              variance     $$\\sigma^2$$  and   $$v_i$$ have variance       $$\\sigma^2$$.\n$$\\hat{u}$$   and   $$\\hat{v}$$  be the quantized versions of               $$u$$                       and   $$v$$, respectively. We model quantization error as                                                     $$\\hat{u}_i  =   u_i +  \\epsilon_i$$  and   $$\\hat{v}_i  =  v_i +   \\xi_i$$ where     $$\\epsilon_i, \\xi_i$$  are i.i.d.\nmean centered random variables with variance                      $$\\sigma^2$$.\nThe aim of this section is to show that variance due                 to quantization grows with            $$k$$. Our analysis is conser-            q       Next, we use linearity of variance for independent\nrandom variables to calculate                $$\\text{Var} \\left( \\langle \\hat{u},  \\hat{v} \\rangle \\right)$$. This is"}]}, {"page": 23, "text": "Algorithm 5         A standard linear layer implemented                      58.0                          ViT-Huge\nwith   torch.autograd                                                        57.5\nclass StandardLinearMatmul(autograd.Function):\n    @staticmethod                                                            57.0\n    def forward(ctx, X, W):\n        # X [b, n] inputs                                                    56.5\n        # W [n, m] weights                                                   56.0\n        # save tensors in ctx                                                55.5         + update clipping\n        ctx.save_for_backward = X, W                                                      + update clipping + beta2 warmup\n        # Return output                                                      55.0  0.98          0.985          0.99         0.995\n        return torch.matmul(X, W.t())                                       Zero-shot ImageNet accuracy     Beta2\n    @staticmethod\n    def backward(ctx, G):\n        # G [b, m] gradient to output                                   Figure 15:     We try a schedule for         \u03b22  which is used in\n        # Recover tensors from ctx                                      AdaFactor [54] and PaLM [11] and refer to the experiment\n        X, W = ctx.save_for_backward                                    as  \u03b22  warmup. This means that          \u03b2 2 at iteration   k  is 1\u2212\n        X_gradient = torch.matmul(G, W)                                 iteration \u2212\u03bb . In this Figure we try       \u03bb = 0.45,0.5,   0.65 and\n        W_gradient = torch.matmul(G.t(), X)                             show on the    x-axis  \u03b2 2 at the final iteration. This    \u03b2 2 warm-\n        return X_gradient, W_gradient                                   up does not improve accuracy in our setting.\nclass StandardLinear(nn.Linear):\n    def forward(self, X):\n        return StandardLinearMatmul.apply(X, self.weight)               From our analysis we have shown that the variance in\n                                                                        the output features increases with the size of the inner\n                                                                        products of a quantized matrix multiplication com-\ngiven by,                                                               pared to the full precision matrix multiplication. As\n                                                                        such, we may have different failure modes for trans-\n                    Xk                                                  formers pretrained on text, such as GPT-3 [5] or\n Var (\u27e8\u02c6 u,\u02c6v\u27e9) =   i=1  Var(\u02c6u iv\u02c6i)                           (12)    LLaMA [59], compared to CLIP models [46].\n                 =  Xk   Var(u   v  ) +  Xk   \u03c32 (\u03c3 2 +  \u03c3 2 +  \u03c3 2)    Pretrained large language models (LLMs) tend to have\n                                i  i           q    u      v      q     larger weight matrices relative to their batch sizes\n                    i=1                  i=1                            when compared to CLIP models. CLIP models per-\n                                                                (13)    form best when the batch size is large [46,            44,  9]. As a\n                 = Var (\u27e8u, v\u27e9) +     k  \u00b7\u03c3 2(\u03c3 2 +  \u03c3 2 +  \u03c3 2 ).      consequence, LLMs and CLIP models have their most\n                                            q   u      v      q (14)    noisy operations for different matrix multiplications.        T\n                                                                        LLMs are most noisy in the forward pass                 XW       and\n                                                                        during layer-to-layer back propagation             Y\u02d9kW  k  =  X\u02d9k\u22121\nC.2       Takeaways                                                     where inner product dimension are large, for example,\nWe have shown that for inner products with length                   k   they are 32768 and 8192 for the output projection of\nvectors, variance due to quantization increases with               k.   LLaMA 65B, 32768 and 8192. While the weight gradi-\nThis means the variance of output units/features due                    ent inner product size is determined by the per-GPU\nto quantization increases with            k which can thought           batch size, which is 2048 for LLaMA [59] (4M tokens\nof making the outputs more noisy. Noise compounds                       per full batch distributed across 2048 GPUs).                     As\nthroughout the network and will eventually drown                        such, if the quantization produces the same variance\nout useful signal\u2014for large         k  the network features or          in quantization errors, then the weight gradient in\ngradient will no longer lead to effective learning.                     LLM int8 training is between 4x and 16x less noisy\n                                                                        if the analysis in Section C.1 is a good model for\nC.3       Why LLM.int8() fails:                       LLMs vs           training.\n                                                                        For CLIP training with ViT-Huge, we have a batch\n          CLIP models                                                   size of 65536 per GPU (256x images of size 224x224\nThis Section details our hypothesis for why Switch-                     inputs with patch size 14x14, leading to 16x16 patches\nBack    outperforms        LLM.int8()       for  CLIP     training,     for each images, resulting in 65536 patches per GPU).\nwhich is conditioned on the analysis in Section C.1                     The dimensions for the weight matrices are 1280\u00d75120.\nbeing a good model for training.                                      23As such, analogous to above for the LLaMA LLM, the", "md": "# Algorithm 5: A standard linear layer implemented with torch.autograd\n\nAlgorithm 5: A standard linear layer implemented with torch.autograd\n\n|Line|Explanation|Accuracy|\n|---|---|---|\n|58.0|ViT-Huge| |\n|57.5| | |\n|57.0| | |\n|56.5|X [b, n] inputs| |\n|56.0|W [n, m] weights| |\n|55.5|save tensors in ctx + update clipping| |\n|55.0|Return output| |\n\n$$\n\\begin{align*}\n\\text{Var}(\\langle\\hat{u},\\hat{v}\\rangle) & = \\sum_{i=1}^{k} \\text{Var}(\\hat{u}_i\\hat{v}_i) \\tag{12} \\\\\n& = X_k \\text{Var}(u_i v_i) + X_k \\sigma^2(\\sigma^2 + \\sigma^2 + \\sigma^2) \\tag{13} \\\\\n& = \\text{Var}(\\langle u, v\\rangle) + k \\cdot \\sigma^2(\\sigma^2 + \\sigma^2 + \\sigma^2). \\tag{14}\n\\end{align*}\n$$\nFrom our analysis we have shown that the variance in the output features increases with the size of the inner products of a quantized matrix multiplication compared to the full precision matrix multiplication. As such, we may have different failure modes for transformers pretrained on text, such as GPT-3 [5] or LLaMA [59], compared to CLIP models [46].\n\nPretrained large language models (LLMs) tend to have larger weight matrices relative to their batch sizes when compared to CLIP models. CLIP models perform best when the batch size is large [46, 44, 9]. As a consequence, LLMs and CLIP models have their most noisy operations for different matrix multiplications. LLMs are most noisy in the forward pass $XW$ and during layer-to-layer back propagation $Y\\dot{k}W_k = X\\dot{k-1}$ where inner product dimensions are large, for example, they are 32768 and 8192 for the output projection of LLaMA 65B, 32768 and 8192. While the weight gradient inner product size is determined by the per-GPU batch size, which is 2048 for LLaMA [59] (4M tokens per full batch distributed across 2048 GPUs). As such, if the quantization produces the same variance in quantization errors, then the weight gradient in LLM int8 training is between 4x and 16x less noisy if the analysis in Section C.1 is a good model for training.\n\nWe have shown that for inner products with length vectors, variance due to quantization increases with $k$. This means the variance of output units/features due to quantization increases with $k$ which can be thought of making the outputs more noisy. Noise compounds throughout the network and will eventually drown out useful signal\u2014for large $k$ the network features or gradient will no longer lead to effective learning.\n\nTakeaways\n\nThis Section details our hypothesis for why Switch-Back outperforms LLM.int8() for CLIP training, which is conditioned on the analysis in Section C.1 being a good model for training.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Algorithm 5: A standard linear layer implemented with torch.autograd", "md": "# Algorithm 5: A standard linear layer implemented with torch.autograd"}, {"type": "text", "value": "Algorithm 5: A standard linear layer implemented with torch.autograd", "md": "Algorithm 5: A standard linear layer implemented with torch.autograd"}, {"type": "table", "rows": [["Line", "Explanation", "Accuracy"], ["58.0", "ViT-Huge", ""], ["57.5", "", ""], ["57.0", "", ""], ["56.5", "X [b, n] inputs", ""], ["56.0", "W [n, m] weights", ""], ["55.5", "save tensors in ctx + update clipping", ""], ["55.0", "Return output", ""]], "md": "|Line|Explanation|Accuracy|\n|---|---|---|\n|58.0|ViT-Huge| |\n|57.5| | |\n|57.0| | |\n|56.5|X [b, n] inputs| |\n|56.0|W [n, m] weights| |\n|55.5|save tensors in ctx + update clipping| |\n|55.0|Return output| |", "isPerfectTable": true, "csv": "\"Line\",\"Explanation\",\"Accuracy\"\n\"58.0\",\"ViT-Huge\",\"\"\n\"57.5\",\"\",\"\"\n\"57.0\",\"\",\"\"\n\"56.5\",\"X [b, n] inputs\",\"\"\n\"56.0\",\"W [n, m] weights\",\"\"\n\"55.5\",\"save tensors in ctx + update clipping\",\"\"\n\"55.0\",\"Return output\",\"\""}, {"type": "text", "value": "$$\n\\begin{align*}\n\\text{Var}(\\langle\\hat{u},\\hat{v}\\rangle) & = \\sum_{i=1}^{k} \\text{Var}(\\hat{u}_i\\hat{v}_i) \\tag{12} \\\\\n& = X_k \\text{Var}(u_i v_i) + X_k \\sigma^2(\\sigma^2 + \\sigma^2 + \\sigma^2) \\tag{13} \\\\\n& = \\text{Var}(\\langle u, v\\rangle) + k \\cdot \\sigma^2(\\sigma^2 + \\sigma^2 + \\sigma^2). \\tag{14}\n\\end{align*}\n$$\nFrom our analysis we have shown that the variance in the output features increases with the size of the inner products of a quantized matrix multiplication compared to the full precision matrix multiplication. As such, we may have different failure modes for transformers pretrained on text, such as GPT-3 [5] or LLaMA [59], compared to CLIP models [46].\n\nPretrained large language models (LLMs) tend to have larger weight matrices relative to their batch sizes when compared to CLIP models. CLIP models perform best when the batch size is large [46, 44, 9]. As a consequence, LLMs and CLIP models have their most noisy operations for different matrix multiplications. LLMs are most noisy in the forward pass $XW$ and during layer-to-layer back propagation $Y\\dot{k}W_k = X\\dot{k-1}$ where inner product dimensions are large, for example, they are 32768 and 8192 for the output projection of LLaMA 65B, 32768 and 8192. While the weight gradient inner product size is determined by the per-GPU batch size, which is 2048 for LLaMA [59] (4M tokens per full batch distributed across 2048 GPUs). As such, if the quantization produces the same variance in quantization errors, then the weight gradient in LLM int8 training is between 4x and 16x less noisy if the analysis in Section C.1 is a good model for training.\n\nWe have shown that for inner products with length vectors, variance due to quantization increases with $k$. This means the variance of output units/features due to quantization increases with $k$ which can be thought of making the outputs more noisy. Noise compounds throughout the network and will eventually drown out useful signal\u2014for large $k$ the network features or gradient will no longer lead to effective learning.\n\nTakeaways\n\nThis Section details our hypothesis for why Switch-Back outperforms LLM.int8() for CLIP training, which is conditioned on the analysis in Section C.1 being a good model for training.", "md": "$$\n\\begin{align*}\n\\text{Var}(\\langle\\hat{u},\\hat{v}\\rangle) & = \\sum_{i=1}^{k} \\text{Var}(\\hat{u}_i\\hat{v}_i) \\tag{12} \\\\\n& = X_k \\text{Var}(u_i v_i) + X_k \\sigma^2(\\sigma^2 + \\sigma^2 + \\sigma^2) \\tag{13} \\\\\n& = \\text{Var}(\\langle u, v\\rangle) + k \\cdot \\sigma^2(\\sigma^2 + \\sigma^2 + \\sigma^2). \\tag{14}\n\\end{align*}\n$$\nFrom our analysis we have shown that the variance in the output features increases with the size of the inner products of a quantized matrix multiplication compared to the full precision matrix multiplication. As such, we may have different failure modes for transformers pretrained on text, such as GPT-3 [5] or LLaMA [59], compared to CLIP models [46].\n\nPretrained large language models (LLMs) tend to have larger weight matrices relative to their batch sizes when compared to CLIP models. CLIP models perform best when the batch size is large [46, 44, 9]. As a consequence, LLMs and CLIP models have their most noisy operations for different matrix multiplications. LLMs are most noisy in the forward pass $XW$ and during layer-to-layer back propagation $Y\\dot{k}W_k = X\\dot{k-1}$ where inner product dimensions are large, for example, they are 32768 and 8192 for the output projection of LLaMA 65B, 32768 and 8192. While the weight gradient inner product size is determined by the per-GPU batch size, which is 2048 for LLaMA [59] (4M tokens per full batch distributed across 2048 GPUs). As such, if the quantization produces the same variance in quantization errors, then the weight gradient in LLM int8 training is between 4x and 16x less noisy if the analysis in Section C.1 is a good model for training.\n\nWe have shown that for inner products with length vectors, variance due to quantization increases with $k$. This means the variance of output units/features due to quantization increases with $k$ which can be thought of making the outputs more noisy. Noise compounds throughout the network and will eventually drown out useful signal\u2014for large $k$ the network features or gradient will no longer lead to effective learning.\n\nTakeaways\n\nThis Section details our hypothesis for why Switch-Back outperforms LLM.int8() for CLIP training, which is conditioned on the analysis in Section C.1 being a good model for training."}]}, {"page": 24, "text": "weight gradient in CLIP models is between 51.2x to                           With only 76 total RMS spike events, the proba-\n12.8x more noisy compared to the forward and layer-                          bility that a loss spike follows 1-8 iterations after\n to-layer backpropagation operations if the analysis in                      an RMS spike by chance is           <  1%.\n Section C.1 is a good model for training. Notice that\n the CLIP weight gradient is twice as noisy compared                      \u2022  Figure 17 repeats this analysis for ViT-Large,\n to the most noisy LLaMA 65B operations if we assume                         wherein 13 out of 15 loss spikes follow an RMS\n that all quantization operations have the same error                        spike by 1-8 iterations. The probability that a\nvariance.                                                                    loss spike follows an RMS spike by chance is 1.0%.\n                                                                          \u2022  Figure 18 zooms in on Figure 16 to show addi-\nAs such, low-precision LLM training and CLIP re-                             tional detail.\n quires high-precision quantization routines for differ-\n ent parts of the training.                                               \u2022  Figures 19 and 20 examine the cases where loss\n                                                                             spikes fail to be detected in Figures 16 and 17,\nThis also gives the reason why we believe LLM.int8()                         finding them to mainly be issues with the heuris-\n fails despite replicating inference performance \u2013 the                       tic identifying loss spikes, i.e., false positive loss\nweight gradient in CLIP training is a highly noisy                           spikes.\n operation which might not give enough signal to SGD\n to converge to a local minimum.                                          \u2022  Finally, Figure 21 repeats Figure 16 but examines\n                                                                             the  RMS    of a random layer in the middle of the\n                                                                             transformer\u2014not the patch embedding layer. In\n                                                                             this case,    none    of the loss spikes follow RMS\n D       RMS           Spikes           precede            Loss              spikes.\n         Spikes\nThis section further elaborate on the predictive re-                   E       StableAdamW continued\n lationship between an RMS spike in the embedding\n layer and a loss spike as in Figure 9.                                E.1       Q&A\nWe define a heuristic to characterize loss and RMS                     This Section asks and answers a series of questions\n spikes which we use for analysis. We determined these                 the reader may have concerning Section 3.5.\n heuristics by checking if they qualitatively coincided\nwith what appeared to be a loss spike. We display                         \u2022  First, why not just use AdaFactor? The answer\n results in this Section so that the reader can also                         is that the community has moved away from\n evaluate if these heuristics appear reasonable.                             AdaFactor [11] as they find that AdaFactor under-\n                                                                             performs AdamW at scale [47]. We believe this\nWe define RMS spikes events as               {t  : RMS   t \u2265   2.3}          is likely due to the factored moments, and not\nwhile loss spike events are defined as the set of                  t         other features such as update-clipping. The goal\nwhere loss at time       t exceeds the running mean by 3.2                   of this work is to advocate using a hybrid. We\n times the running standard deviation.                Finally, we            tried porting other features from AdaFactor to\n ignore the first 1000 iterations when learning rate is                      AdamW such as the         \u03b2 2 schedule but did not find\n low.                                                                        them to help (Figure 15). Moreover, while PaLM\n                                                                             uses an AdaFactor-AdamW hybrid, we believe\nWe also deduplicate the RMS and loss spikes iterations                       they don\u2019t use update clipping.\n as follows: multiple spikes over a short time interval of\n10 iterations are only counted as one spike and start                     \u2022  Another question is, why not use an optimizer\n at the earliest time. Moreover, we only count a loss                        such as Lion [7] which does not divide updates\n spike if there are multiple deviations in an interval of                    by any value, and is therefore immune to the\n10, which indicates that loss has meaningfully spiked.                       stuck-in-the-past scenario. We believe this may\n                                                                             be a promising path forward. However, while we\n Our results are as follows:                                                 observe that Lion outperforms AdamW at small\n   \u2022  Figure 16 observes that out of 15 total loss                           scale, Lion still slightly under-performs AdamW           5\n      spikes for ViT-Huge across different             \u03b2 2, 14 out           for CLIP ViT-Huge scale in our experiments.\n      of 15 come 1-8 iterations after an RMS spike in                      5This may be out of date, track the latest    https://github.\n      the patch embedding layer (module.conv1.weight).              24 com/mlfoundations/open_clip/pull/432.", "md": "Weight gradient in CLIP models is between 51.2x to 12.8x more noisy compared to the forward and layer-to-layer backpropagation operations if the analysis in Section C.1 is a good model for training. Notice that the CLIP weight gradient is twice as noisy compared to the most noisy LLaMA 65B operations if we assume that all quantization operations have the same error variance.\n\nAs such, low-precision LLM training and CLIP requires high-precision quantization routines for different parts of the training.\n\nThis also gives the reason why we believe LLM.int8() fails despite replicating inference performance \u2013 the weight gradient in CLIP training is a highly noisy operation which might not give enough signal to SGD to converge to a local minimum.\n\nD RMS Spikes precede Loss Spikes\n\nThis section further elaborates on the predictive relationship between an RMS spike in the embedding layer and a loss spike as in Figure 9.\n\nWe define a heuristic to characterize loss and RMS spikes which we use for analysis. We determined these heuristics by checking if they qualitatively coincided with what appeared to be a loss spike. We display results in this section so that the reader can also evaluate if these heuristics appear reasonable.\n\nWe define RMS spikes events as $$\\{t : \\text{RMS}_t \\geq 2.3\\}$$ while loss spike events are defined as the set of $$\\{t : \\text{loss}_t \\text{ exceeds the running mean by } 3.2 \\times \\text{running standard deviation}\\}$$. Finally, we ignore the first 1000 iterations when learning rate is low.\n\nWe also deduplicate the RMS and loss spikes iterations as follows: multiple spikes over a short time interval of 10 iterations are only counted as one spike and start at the earliest time. Moreover, we only count a loss spike if there are multiple deviations in an interval of 10, which indicates that loss has meaningfully spiked.\n\nOur results are as follows:\n\n- Figure 16 observes that out of 15 total loss spikes for ViT-Huge across different $\\beta_2$, 14 out of 15 come 1-8 iterations after an RMS spike in the patch embedding layer (module.conv1.weight).\n\nFigure 17 repeats this analysis for ViT-Large, wherein 13 out of 15 loss spikes follow an RMS spike by 1-8 iterations. The probability that a loss spike follows an RMS spike by chance is < 1%.\n\nFigure 18 zooms in on Figure 16 to show additional detail.\n\nFigures 19 and 20 examine the cases where loss spikes fail to be detected in Figures 16 and 17, finding them to mainly be issues with the heuristic identifying loss spikes, i.e., false positive loss spikes.\n\nFinally, Figure 21 repeats Figure 16 but examines the RMS of a random layer in the middle of the transformer\u2014not the patch embedding layer. In this case, none of the loss spikes follow RMS spikes.\n\nE StableAdamW continued\n\nE.1 Q&A\n\nThis section asks and answers a series of questions the reader may have concerning Section 3.5.\n\n- First, why not just use AdaFactor? The answer is that the community has moved away from AdaFactor [11] as they find that AdaFactor underperforms AdamW at scale [47]. We believe this is likely due to the factored moments, and not other features such as update-clipping. The goal of this work is to advocate using a hybrid. We tried porting other features from AdaFactor to AdamW such as the $\\beta_2$ schedule but did not find them to help (Figure 15). Moreover, while PaLM uses an AdaFactor-AdamW hybrid, we believe they don\u2019t use update clipping.\n- Another question is, why not use an optimizer such as Lion [7] which does not divide updates by any value, and is therefore immune to the stuck-in-the-past scenario. We believe this may be a promising path forward. However, while we observe that Lion outperforms AdamW at small scale, Lion still slightly under-performs AdamW for CLIP ViT-Huge scale in our experiments. This may be out of date, track the latest here.", "images": [], "items": [{"type": "text", "value": "Weight gradient in CLIP models is between 51.2x to 12.8x more noisy compared to the forward and layer-to-layer backpropagation operations if the analysis in Section C.1 is a good model for training. Notice that the CLIP weight gradient is twice as noisy compared to the most noisy LLaMA 65B operations if we assume that all quantization operations have the same error variance.\n\nAs such, low-precision LLM training and CLIP requires high-precision quantization routines for different parts of the training.\n\nThis also gives the reason why we believe LLM.int8() fails despite replicating inference performance \u2013 the weight gradient in CLIP training is a highly noisy operation which might not give enough signal to SGD to converge to a local minimum.\n\nD RMS Spikes precede Loss Spikes\n\nThis section further elaborates on the predictive relationship between an RMS spike in the embedding layer and a loss spike as in Figure 9.\n\nWe define a heuristic to characterize loss and RMS spikes which we use for analysis. We determined these heuristics by checking if they qualitatively coincided with what appeared to be a loss spike. We display results in this section so that the reader can also evaluate if these heuristics appear reasonable.\n\nWe define RMS spikes events as $$\\{t : \\text{RMS}_t \\geq 2.3\\}$$ while loss spike events are defined as the set of $$\\{t : \\text{loss}_t \\text{ exceeds the running mean by } 3.2 \\times \\text{running standard deviation}\\}$$. Finally, we ignore the first 1000 iterations when learning rate is low.\n\nWe also deduplicate the RMS and loss spikes iterations as follows: multiple spikes over a short time interval of 10 iterations are only counted as one spike and start at the earliest time. Moreover, we only count a loss spike if there are multiple deviations in an interval of 10, which indicates that loss has meaningfully spiked.\n\nOur results are as follows:\n\n- Figure 16 observes that out of 15 total loss spikes for ViT-Huge across different $\\beta_2$, 14 out of 15 come 1-8 iterations after an RMS spike in the patch embedding layer (module.conv1.weight).\n\nFigure 17 repeats this analysis for ViT-Large, wherein 13 out of 15 loss spikes follow an RMS spike by 1-8 iterations. The probability that a loss spike follows an RMS spike by chance is < 1%.\n\nFigure 18 zooms in on Figure 16 to show additional detail.\n\nFigures 19 and 20 examine the cases where loss spikes fail to be detected in Figures 16 and 17, finding them to mainly be issues with the heuristic identifying loss spikes, i.e., false positive loss spikes.\n\nFinally, Figure 21 repeats Figure 16 but examines the RMS of a random layer in the middle of the transformer\u2014not the patch embedding layer. In this case, none of the loss spikes follow RMS spikes.\n\nE StableAdamW continued\n\nE.1 Q&A\n\nThis section asks and answers a series of questions the reader may have concerning Section 3.5.\n\n- First, why not just use AdaFactor? The answer is that the community has moved away from AdaFactor [11] as they find that AdaFactor underperforms AdamW at scale [47]. We believe this is likely due to the factored moments, and not other features such as update-clipping. The goal of this work is to advocate using a hybrid. We tried porting other features from AdaFactor to AdamW such as the $\\beta_2$ schedule but did not find them to help (Figure 15). Moreover, while PaLM uses an AdaFactor-AdamW hybrid, we believe they don\u2019t use update clipping.\n- Another question is, why not use an optimizer such as Lion [7] which does not divide updates by any value, and is therefore immune to the stuck-in-the-past scenario. We believe this may be a promising path forward. However, while we observe that Lion outperforms AdamW at small scale, Lion still slightly under-performs AdamW for CLIP ViT-Huge scale in our experiments. This may be out of date, track the latest here.", "md": "Weight gradient in CLIP models is between 51.2x to 12.8x more noisy compared to the forward and layer-to-layer backpropagation operations if the analysis in Section C.1 is a good model for training. Notice that the CLIP weight gradient is twice as noisy compared to the most noisy LLaMA 65B operations if we assume that all quantization operations have the same error variance.\n\nAs such, low-precision LLM training and CLIP requires high-precision quantization routines for different parts of the training.\n\nThis also gives the reason why we believe LLM.int8() fails despite replicating inference performance \u2013 the weight gradient in CLIP training is a highly noisy operation which might not give enough signal to SGD to converge to a local minimum.\n\nD RMS Spikes precede Loss Spikes\n\nThis section further elaborates on the predictive relationship between an RMS spike in the embedding layer and a loss spike as in Figure 9.\n\nWe define a heuristic to characterize loss and RMS spikes which we use for analysis. We determined these heuristics by checking if they qualitatively coincided with what appeared to be a loss spike. We display results in this section so that the reader can also evaluate if these heuristics appear reasonable.\n\nWe define RMS spikes events as $$\\{t : \\text{RMS}_t \\geq 2.3\\}$$ while loss spike events are defined as the set of $$\\{t : \\text{loss}_t \\text{ exceeds the running mean by } 3.2 \\times \\text{running standard deviation}\\}$$. Finally, we ignore the first 1000 iterations when learning rate is low.\n\nWe also deduplicate the RMS and loss spikes iterations as follows: multiple spikes over a short time interval of 10 iterations are only counted as one spike and start at the earliest time. Moreover, we only count a loss spike if there are multiple deviations in an interval of 10, which indicates that loss has meaningfully spiked.\n\nOur results are as follows:\n\n- Figure 16 observes that out of 15 total loss spikes for ViT-Huge across different $\\beta_2$, 14 out of 15 come 1-8 iterations after an RMS spike in the patch embedding layer (module.conv1.weight).\n\nFigure 17 repeats this analysis for ViT-Large, wherein 13 out of 15 loss spikes follow an RMS spike by 1-8 iterations. The probability that a loss spike follows an RMS spike by chance is < 1%.\n\nFigure 18 zooms in on Figure 16 to show additional detail.\n\nFigures 19 and 20 examine the cases where loss spikes fail to be detected in Figures 16 and 17, finding them to mainly be issues with the heuristic identifying loss spikes, i.e., false positive loss spikes.\n\nFinally, Figure 21 repeats Figure 16 but examines the RMS of a random layer in the middle of the transformer\u2014not the patch embedding layer. In this case, none of the loss spikes follow RMS spikes.\n\nE StableAdamW continued\n\nE.1 Q&A\n\nThis section asks and answers a series of questions the reader may have concerning Section 3.5.\n\n- First, why not just use AdaFactor? The answer is that the community has moved away from AdaFactor [11] as they find that AdaFactor underperforms AdamW at scale [47]. We believe this is likely due to the factored moments, and not other features such as update-clipping. The goal of this work is to advocate using a hybrid. We tried porting other features from AdaFactor to AdamW such as the $\\beta_2$ schedule but did not find them to help (Figure 15). Moreover, while PaLM uses an AdaFactor-AdamW hybrid, we believe they don\u2019t use update clipping.\n- Another question is, why not use an optimizer such as Lion [7] which does not divide updates by any value, and is therefore immune to the stuck-in-the-past scenario. We believe this may be a promising path forward. However, while we observe that Lion outperforms AdamW at small scale, Lion still slightly under-performs AdamW for CLIP ViT-Huge scale in our experiments. This may be out of date, track the latest here."}]}, {"page": 25, "text": "   \u2022  A final question is, why consider      g 2 in the numer-\n      ator for computing       RMS    and not  t v2 ? We also\n                                    t             t\n      tried  v2 and found the performance worse.\n              t\nE.2       Implementation considerations\nTo prevent divide by 0 issues when computing             RMSt\n                          p      2                   2\nwe compute     RMS   t =     E[g t /maximum(u     t, \u03f5 )]where\n \u03f5 is the AdamW hyperparamer for which we use 1e-6\n and  maximum     is an elementwise maximum. This isp\n instead of  RMS   t =     E[g 2/u t].\n                               t\n                                                                25", "md": "# Math Equations and Implementation Considerations\n\nA final question is, why consider $$g^2$$ in the numerator for computing RMS and not $$\\frac{t}{v^2}$$? We also tried $$\\frac{v^2}{t}$$ and found the performance worse.\n\n## Implementation considerations\n\nTo prevent divide by 0 issues when computing $$RMSt$$, we compute $$RMS_t = E\\left[\\frac{g_t}{\\max(u_t, \\epsilon)}\\right]$$ where $$\\epsilon$$ is the AdamW hyperparameter for which we use 1e-6 and $$\\max$$ is an elementwise maximum. This is instead of $$RMS_t = E\\left[\\frac{g^2}{u_t}\\right].$$", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Implementation Considerations", "md": "# Math Equations and Implementation Considerations"}, {"type": "text", "value": "A final question is, why consider $$g^2$$ in the numerator for computing RMS and not $$\\frac{t}{v^2}$$? We also tried $$\\frac{v^2}{t}$$ and found the performance worse.", "md": "A final question is, why consider $$g^2$$ in the numerator for computing RMS and not $$\\frac{t}{v^2}$$? We also tried $$\\frac{v^2}{t}$$ and found the performance worse."}, {"type": "heading", "lvl": 2, "value": "Implementation considerations", "md": "## Implementation considerations"}, {"type": "text", "value": "To prevent divide by 0 issues when computing $$RMSt$$, we compute $$RMS_t = E\\left[\\frac{g_t}{\\max(u_t, \\epsilon)}\\right]$$ where $$\\epsilon$$ is the AdamW hyperparameter for which we use 1e-6 and $$\\max$$ is an elementwise maximum. This is instead of $$RMS_t = E\\left[\\frac{g^2}{u_t}\\right].$$", "md": "To prevent divide by 0 issues when computing $$RMSt$$, we compute $$RMS_t = E\\left[\\frac{g_t}{\\max(u_t, \\epsilon)}\\right]$$ where $$\\epsilon$$ is the AdamW hyperparameter for which we use 1e-6 and $$\\max$$ is an elementwise maximum. This is instead of $$RMS_t = E\\left[\\frac{g^2}{u_t}\\right].$$"}]}, {"page": 26, "text": "   10.0                                            ViT-Huge, Beta2 = 0.99                 Loss\n    7.5                                                                                   Loss spike 1-8 iterations\n    5.0                                                                                   after RMS spike (7 out of 7)\n  Loss                                                                                    Loss spike not 1-8 iterations\n    2.5                                                                                   after RMS spike(0 out of 7)\n    0.00           2500         5000         7500        10000        12500        15000         17500        20000\n      4                                            ViT-Huge, Beta2 = 0.99                  RMS (visual.conv1.weight)\n      2                                                                                    RMS spike (total 25)\n     RMS\n      00           2500         5000         7500        10000        12500        15000         17500        20000\n   10.0                                            ViT-Huge, Beta2 = 0.98\n    7.5                                                                                   Loss\n                                                                                          Loss spike 1-8 iterations\n    5.0                                                                                   after RMS spike (7 out of 8)\n  Loss                                                                                    Loss spike not 1-8 iterations\n    2.5                                                                                   after RMS spike(1 out of 8)\n    0.00           2500         5000         7500        10000        12500        15000         17500        20000\n      4                                            ViT-Huge, Beta2 = 0.98                  RMS (visual.conv1.weight)\n      2                                                                                    RMS spike (total 41)\n     RMS\n      00           2500         5000         7500        10000        12500        15000         17500        20000\n   10.0                                            ViT-Huge, Beta2 = 0.95                 Loss\n    7.5                                                                                   Loss spike 1-8 iterations\n    5.0                                                                                   after RMS spike (0 out of 0)\n  Loss                                                                                    Loss spike not 1-8 iterations\n    2.5                                                                                   after RMS spike(0 out of 0)\n    0.00           2500         5000         7500        10000        12500        15000         17500        20000\n      4                                            ViT-Huge, Beta2 = 0.95                  RMS (visual.conv1.weight)\n      2                                                                                    RMS spike (total 10)\n     RMS\n      00           2500         5000         7500        10000        12500        15000         17500        20000\n   10.0                                            ViT-Huge, Beta2 = 0.9                  Loss\n    7.5                                                                                   Loss spike 1-8 iterations\n    5.0                                                                                   after RMS spike (0 out of 0)\n  Loss                                                                                    Loss spike not 1-8 iterations\n    2.5                                                                                   after RMS spike(0 out of 0)\n    0.00           2500         5000         7500        10000        12500        15000         17500        20000\n      4                                            ViT-Huge, Beta2 = 0.9                   RMS (visual.conv1.weight)\n      2                                                                                    RMS spike (total 0)\n     RMS\n      00           2500         5000         7500        10000        12500        15000         17500        20000\n                                                        Iteration\nFigure 16: Observing a predictive relation between RMS spikes and loss spikes. For CLIP ViT-Huge and multiple    \u03b22\nvalues, we use heuristics (Appendix D) to automatically identify loss spikes which we use for analysis. Out of 15 total\nloss spikes, 14 follow an RMS spike in the patch embedding layer (RMS  >  2.3) by 1-8 iterations. We show loss spikes\nwhich are identified by our heuristic. We use green if they follow an RMS spike and otherwise use red. An RMS Spike\nindicates that the second moment estimator is out of date (see Section 3.4 and Shazeer and Stern  [54]). The chance\nthat a loss spike follows 1-8 iterations after an RMS spike by chance is<1%.\n                                                         26", "md": "# Observing a predictive relation between RMS spikes and loss spikes\n\n## Observing a predictive relation between RMS spikes and loss spikes\n\nFor CLIP ViT-Huge and multiple $$\\beta2$$ values, we use heuristics (Appendix D) to automatically identify loss spikes which we use for analysis. Out of 15 total loss spikes, 14 follow an RMS spike in the patch embedding layer (RMS $$>$$ 2.3) by 1-8 iterations. We show loss spikes which are identified by our heuristic. We use green if they follow an RMS spike and otherwise use red. An RMS Spike indicates that the second moment estimator is out of date (see Section 3.4 and Shazeer and Stern [54]). The chance that a loss spike follows 1-8 iterations after an RMS spike by chance is $$<1\\%$$.\n\nFigure 16: Observing a predictive relation between RMS spikes and loss spikes.\n\n| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|10.0| | | |ViT-Huge, Beta2 = 0.99| | |Loss| |\n|7.5| | | | | | |Loss spike 1-8 iterations| |\n|5.0| | | | | | |after RMS spike (7 out of 7)| |\n|Loss| | | | | | |Loss spike not 1-8 iterations| |\n|2.5| | | | | | |after RMS spike(0 out of 7)| |\n|0.00| | | | | | | | |\n\n| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|4| | | |ViT-Huge, Beta2 = 0.99| | |RMS (visual.conv1.weight)| |\n|2| | | | | | |RMS spike (total 25)| |\n| | | | | | | |RMS| |\n\n| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|10.0|ViT-Huge, Beta2 = 0.98| | | | | |Loss| |\n|7.5| | | | | | |Loss spike 1-8 iterations| |\n|5.0| | | | | | |after RMS spike (7 out of 8)| |\n|Loss| | | | | | |Loss spike not 1-8 iterations| |\n|2.5| | | | | | |after RMS spike(1 out of 8)| |\n|0.00| | | | | | | | |\n\n| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|4| | | |ViT-Huge, Beta2 = 0.98| | |RMS (visual.conv1.weight)| |\n|2| | | | | | |RMS spike (total 41)| |\n| | | | | | | |RMS| |\n\n| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|10.0| | | |ViT-Huge, Beta2 = 0.95| | |Loss| |\n|7.5| | | | | | |Loss spike 1-8 iterations| |\n|5.0| | | | | | |after RMS spike (0 out of 0)| |\n|Loss| | | | | | |Loss spike not 1-8 iterations| |\n|2.5| | | | | | |after RMS spike(0 out of 0)| |\n|0.00| | | | | | | | |\n\n| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|4| | | |ViT-Huge, Beta2 = 0.95| | |RMS (visual.conv1.weight)| |\n|2| | | | | | |RMS spike (total 10)| |\n| | | | | | | |RMS| |\n\n| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|10.0| | | |ViT-Huge, Beta2 = 0.9| | |Loss| |\n|7.5| | | | | | |Loss spike 1-8 iterations| |\n|5.0| | | | | | |after RMS spike (0 out of 0)| |\n|Loss| | | | | | |Loss spike not 1-8 iterations| |\n|2.5| | | | | | |after RMS spike(0 out of 0)| |\n|0.00| | | | | | | | |\n\n| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|4| | | |ViT-Huge, Beta2 = 0.9| | |RMS (visual.conv1.weight)| |\n|2| | | | | | |RMS spike (total 0)| |\n| | | | | | | |RMS| |\n\nIteration", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Observing a predictive relation between RMS spikes and loss spikes", "md": "# Observing a predictive relation between RMS spikes and loss spikes"}, {"type": "heading", "lvl": 2, "value": "Observing a predictive relation between RMS spikes and loss spikes", "md": "## Observing a predictive relation between RMS spikes and loss spikes"}, {"type": "text", "value": "For CLIP ViT-Huge and multiple $$\\beta2$$ values, we use heuristics (Appendix D) to automatically identify loss spikes which we use for analysis. Out of 15 total loss spikes, 14 follow an RMS spike in the patch embedding layer (RMS $$>$$ 2.3) by 1-8 iterations. We show loss spikes which are identified by our heuristic. We use green if they follow an RMS spike and otherwise use red. An RMS Spike indicates that the second moment estimator is out of date (see Section 3.4 and Shazeer and Stern [54]). The chance that a loss spike follows 1-8 iterations after an RMS spike by chance is $$<1\\%$$.\n\nFigure 16: Observing a predictive relation between RMS spikes and loss spikes.", "md": "For CLIP ViT-Huge and multiple $$\\beta2$$ values, we use heuristics (Appendix D) to automatically identify loss spikes which we use for analysis. Out of 15 total loss spikes, 14 follow an RMS spike in the patch embedding layer (RMS $$>$$ 2.3) by 1-8 iterations. We show loss spikes which are identified by our heuristic. We use green if they follow an RMS spike and otherwise use red. An RMS Spike indicates that the second moment estimator is out of date (see Section 3.4 and Shazeer and Stern [54]). The chance that a loss spike follows 1-8 iterations after an RMS spike by chance is $$<1\\%$$.\n\nFigure 16: Observing a predictive relation between RMS spikes and loss spikes."}, {"type": "table", "rows": [["", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000"], ["10.0", "", "", "", "ViT-Huge, Beta2 = 0.99", "", "", "Loss", ""], ["7.5", "", "", "", "", "", "", "Loss spike 1-8 iterations", ""], ["5.0", "", "", "", "", "", "", "after RMS spike (7 out of 7)", ""], ["Loss", "", "", "", "", "", "", "Loss spike not 1-8 iterations", ""], ["2.5", "", "", "", "", "", "", "after RMS spike(0 out of 7)", ""], ["0.00", "", "", "", "", "", "", "", ""]], "md": "| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|10.0| | | |ViT-Huge, Beta2 = 0.99| | |Loss| |\n|7.5| | | | | | |Loss spike 1-8 iterations| |\n|5.0| | | | | | |after RMS spike (7 out of 7)| |\n|Loss| | | | | | |Loss spike not 1-8 iterations| |\n|2.5| | | | | | |after RMS spike(0 out of 7)| |\n|0.00| | | | | | | | |", "isPerfectTable": true, "csv": "\"\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\"\n\"10.0\",\"\",\"\",\"\",\"ViT-Huge, Beta2 = 0.99\",\"\",\"\",\"Loss\",\"\"\n\"7.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"Loss spike 1-8 iterations\",\"\"\n\"5.0\",\"\",\"\",\"\",\"\",\"\",\"\",\"after RMS spike (7 out of 7)\",\"\"\n\"Loss\",\"\",\"\",\"\",\"\",\"\",\"\",\"Loss spike not 1-8 iterations\",\"\"\n\"2.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"after RMS spike(0 out of 7)\",\"\"\n\"0.00\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\""}, {"type": "table", "rows": [["", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000"], ["4", "", "", "", "ViT-Huge, Beta2 = 0.99", "", "", "RMS (visual.conv1.weight)", ""], ["2", "", "", "", "", "", "", "RMS spike (total 25)", ""], ["", "", "", "", "", "", "", "RMS", ""]], "md": "| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|4| | | |ViT-Huge, Beta2 = 0.99| | |RMS (visual.conv1.weight)| |\n|2| | | | | | |RMS spike (total 25)| |\n| | | | | | | |RMS| |", "isPerfectTable": true, "csv": "\"\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\"\n\"4\",\"\",\"\",\"\",\"ViT-Huge, Beta2 = 0.99\",\"\",\"\",\"RMS (visual.conv1.weight)\",\"\"\n\"2\",\"\",\"\",\"\",\"\",\"\",\"\",\"RMS spike (total 25)\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"RMS\",\"\""}, {"type": "table", "rows": [["", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000"], ["10.0", "ViT-Huge, Beta2 = 0.98", "", "", "", "", "", "Loss", ""], ["7.5", "", "", "", "", "", "", "Loss spike 1-8 iterations", ""], ["5.0", "", "", "", "", "", "", "after RMS spike (7 out of 8)", ""], ["Loss", "", "", "", "", "", "", "Loss spike not 1-8 iterations", ""], ["2.5", "", "", "", "", "", "", "after RMS spike(1 out of 8)", ""], ["0.00", "", "", "", "", "", "", "", ""]], "md": "| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|10.0|ViT-Huge, Beta2 = 0.98| | | | | |Loss| |\n|7.5| | | | | | |Loss spike 1-8 iterations| |\n|5.0| | | | | | |after RMS spike (7 out of 8)| |\n|Loss| | | | | | |Loss spike not 1-8 iterations| |\n|2.5| | | | | | |after RMS spike(1 out of 8)| |\n|0.00| | | | | | | | |", "isPerfectTable": true, "csv": "\"\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\"\n\"10.0\",\"ViT-Huge, Beta2 = 0.98\",\"\",\"\",\"\",\"\",\"\",\"Loss\",\"\"\n\"7.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"Loss spike 1-8 iterations\",\"\"\n\"5.0\",\"\",\"\",\"\",\"\",\"\",\"\",\"after RMS spike (7 out of 8)\",\"\"\n\"Loss\",\"\",\"\",\"\",\"\",\"\",\"\",\"Loss spike not 1-8 iterations\",\"\"\n\"2.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"after RMS spike(1 out of 8)\",\"\"\n\"0.00\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\""}, {"type": "table", "rows": [["", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000"], ["4", "", "", "", "ViT-Huge, Beta2 = 0.98", "", "", "RMS (visual.conv1.weight)", ""], ["2", "", "", "", "", "", "", "RMS spike (total 41)", ""], ["", "", "", "", "", "", "", "RMS", ""]], "md": "| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|4| | | |ViT-Huge, Beta2 = 0.98| | |RMS (visual.conv1.weight)| |\n|2| | | | | | |RMS spike (total 41)| |\n| | | | | | | |RMS| |", "isPerfectTable": true, "csv": "\"\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\"\n\"4\",\"\",\"\",\"\",\"ViT-Huge, Beta2 = 0.98\",\"\",\"\",\"RMS (visual.conv1.weight)\",\"\"\n\"2\",\"\",\"\",\"\",\"\",\"\",\"\",\"RMS spike (total 41)\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"RMS\",\"\""}, {"type": "table", "rows": [["", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000"], ["10.0", "", "", "", "ViT-Huge, Beta2 = 0.95", "", "", "Loss", ""], ["7.5", "", "", "", "", "", "", "Loss spike 1-8 iterations", ""], ["5.0", "", "", "", "", "", "", "after RMS spike (0 out of 0)", ""], ["Loss", "", "", "", "", "", "", "Loss spike not 1-8 iterations", ""], ["2.5", "", "", "", "", "", "", "after RMS spike(0 out of 0)", ""], ["0.00", "", "", "", "", "", "", "", ""]], "md": "| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|10.0| | | |ViT-Huge, Beta2 = 0.95| | |Loss| |\n|7.5| | | | | | |Loss spike 1-8 iterations| |\n|5.0| | | | | | |after RMS spike (0 out of 0)| |\n|Loss| | | | | | |Loss spike not 1-8 iterations| |\n|2.5| | | | | | |after RMS spike(0 out of 0)| |\n|0.00| | | | | | | | |", "isPerfectTable": true, "csv": "\"\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\"\n\"10.0\",\"\",\"\",\"\",\"ViT-Huge, Beta2 = 0.95\",\"\",\"\",\"Loss\",\"\"\n\"7.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"Loss spike 1-8 iterations\",\"\"\n\"5.0\",\"\",\"\",\"\",\"\",\"\",\"\",\"after RMS spike (0 out of 0)\",\"\"\n\"Loss\",\"\",\"\",\"\",\"\",\"\",\"\",\"Loss spike not 1-8 iterations\",\"\"\n\"2.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"after RMS spike(0 out of 0)\",\"\"\n\"0.00\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\""}, {"type": "table", "rows": [["", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000"], ["4", "", "", "", "ViT-Huge, Beta2 = 0.95", "", "", "RMS (visual.conv1.weight)", ""], ["2", "", "", "", "", "", "", "RMS spike (total 10)", ""], ["", "", "", "", "", "", "", "RMS", ""]], "md": "| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|4| | | |ViT-Huge, Beta2 = 0.95| | |RMS (visual.conv1.weight)| |\n|2| | | | | | |RMS spike (total 10)| |\n| | | | | | | |RMS| |", "isPerfectTable": true, "csv": "\"\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\"\n\"4\",\"\",\"\",\"\",\"ViT-Huge, Beta2 = 0.95\",\"\",\"\",\"RMS (visual.conv1.weight)\",\"\"\n\"2\",\"\",\"\",\"\",\"\",\"\",\"\",\"RMS spike (total 10)\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"RMS\",\"\""}, {"type": "table", "rows": [["", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000"], ["10.0", "", "", "", "ViT-Huge, Beta2 = 0.9", "", "", "Loss", ""], ["7.5", "", "", "", "", "", "", "Loss spike 1-8 iterations", ""], ["5.0", "", "", "", "", "", "", "after RMS spike (0 out of 0)", ""], ["Loss", "", "", "", "", "", "", "Loss spike not 1-8 iterations", ""], ["2.5", "", "", "", "", "", "", "after RMS spike(0 out of 0)", ""], ["0.00", "", "", "", "", "", "", "", ""]], "md": "| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|10.0| | | |ViT-Huge, Beta2 = 0.9| | |Loss| |\n|7.5| | | | | | |Loss spike 1-8 iterations| |\n|5.0| | | | | | |after RMS spike (0 out of 0)| |\n|Loss| | | | | | |Loss spike not 1-8 iterations| |\n|2.5| | | | | | |after RMS spike(0 out of 0)| |\n|0.00| | | | | | | | |", "isPerfectTable": true, "csv": "\"\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\"\n\"10.0\",\"\",\"\",\"\",\"ViT-Huge, Beta2 = 0.9\",\"\",\"\",\"Loss\",\"\"\n\"7.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"Loss spike 1-8 iterations\",\"\"\n\"5.0\",\"\",\"\",\"\",\"\",\"\",\"\",\"after RMS spike (0 out of 0)\",\"\"\n\"Loss\",\"\",\"\",\"\",\"\",\"\",\"\",\"Loss spike not 1-8 iterations\",\"\"\n\"2.5\",\"\",\"\",\"\",\"\",\"\",\"\",\"after RMS spike(0 out of 0)\",\"\"\n\"0.00\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\""}, {"type": "table", "rows": [["", "2500", "5000", "7500", "10000", "12500", "15000", "17500", "20000"], ["4", "", "", "", "ViT-Huge, Beta2 = 0.9", "", "", "RMS (visual.conv1.weight)", ""], ["2", "", "", "", "", "", "", "RMS spike (total 0)", ""], ["", "", "", "", "", "", "", "RMS", ""]], "md": "| |2500|5000|7500|10000|12500|15000|17500|20000|\n|---|---|---|---|---|---|---|---|---|\n|4| | | |ViT-Huge, Beta2 = 0.9| | |RMS (visual.conv1.weight)| |\n|2| | | | | | |RMS spike (total 0)| |\n| | | | | | | |RMS| |", "isPerfectTable": true, "csv": "\"\",\"2500\",\"5000\",\"7500\",\"10000\",\"12500\",\"15000\",\"17500\",\"20000\"\n\"4\",\"\",\"\",\"\",\"ViT-Huge, Beta2 = 0.9\",\"\",\"\",\"RMS (visual.conv1.weight)\",\"\"\n\"2\",\"\",\"\",\"\",\"\",\"\",\"\",\"RMS spike (total 0)\",\"\"\n\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"RMS\",\"\""}, {"type": "text", "value": "Iteration", "md": "Iteration"}]}, {"page": 27, "text": "   10.0                                             ViT-Large, Beta2 = 0.99               Loss\n    7.5                                                                                   Loss spike 1-8 iterations\n    5.0                                                                                   after RMS spike (9 out of 10)\n   Loss                                                                                   Loss spike not 1-8 iterations\n    2.5                                                                                   after RMS spike(1 out of 10)\n    0.00           2500         5000         7500         10000        12500        15000        17500        20000\n      4                                             ViT-Large, Beta2 = 0.99                 RMS (visual.conv1.weight)\n      2                                                                                     RMS spike (total 52)\n     RMS\n      00           2500         5000         7500         10000        12500        15000        17500        20000\n   10.0                                             ViT-Large, Beta2 = 0.98\n    7.5                                                                                    Loss\n                                                                                           Loss spike 1-8 iterations\n    5.0                                                                                    after RMS spike (4 out of 4)\n   Loss                                                                                    Loss spike not 1-8 iterations\n    2.5                                                                                    after RMS spike(0 out of 4)\n    0.00           2500         5000         7500         10000        12500        15000        17500        20000\n      4                                             ViT-Large, Beta2 = 0.98                 RMS (visual.conv1.weight)\n      2                                                                                     RMS spike (total 29)\n     RMS\n      00           2500         5000         7500         10000        12500        15000        17500        20000\n   10.0                                             ViT-Large, Beta2 = 0.95                Loss\n    7.5                                                                                    Loss spike 1-8 iterations\n    5.0                                                                                    after RMS spike (0 out of 1)\n   Loss                                                                                    Loss spike not 1-8 iterations\n    2.5                                                                                    after RMS spike(1 out of 1)\n    0.00           2500         5000         7500         10000        12500        15000        17500        20000\n      4                                             ViT-Large, Beta2 = 0.95                 RMS (visual.conv1.weight)\n      2                                                                                     RMS spike (total 15)\n     RMS\n      00           2500         5000         7500         10000        12500        15000        17500        20000\n   10.0                                             ViT-Large, Beta2 = 0.9                 Loss\n    7.5                                                                                    Loss spike 1-8 iterations\n    5.0                                                                                    after RMS spike (0 out of 0)\n   Loss                                                                                    Loss spike not 1-8 iterations\n    2.5                                                                                    after RMS spike(0 out of 0)\n    0.00           2500         5000         7500         10000        12500        15000        17500        20000\n      4                                             ViT-Large, Beta2 = 0.9                  RMS (visual.conv1.weight)\n      2                                                                                     RMS spike (total 1)\n     RMS\n      00           2500         5000         7500         10000        12500        15000        17500        20000\n                                                         Iteration\nFigure 17: Observing a predictive relation between RMS spikes and loss spikes. For CLIP ViT-Large and multiple    \u03b22\nvalues, we use heuristics (Appendix D) to automatically identify loss spikes which we use for analysis. Out of 15 total\nloss spikes, 13 follow an RMS spike in the patch embedding layer (RMS   > 2.3) by 1-8 iterations. We show loss spikes\nwhich are identified by our heuristic. We use green if they follow an RMS spike and otherwise use red. An RMS Spike\nindicates that the second moment estimator is out of date (see Section 3.4 and Shazeer and Stern   [54]). The chance\nthat a loss spike follows 1-8 iterations after an RMS spike by chance is 1.0%.\n                                                         27", "md": "## 10.0 ViT-Large, Beta2 = 0.99 Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (9 out of 10) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(1 out of 10)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n## 4 ViT-Large, Beta2 = 0.99 RMS (visual.conv1.weight)\n\n2 RMS spike (total 52)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n## 10.0 ViT-Large, Beta2 = 0.98\n\n7.5 Loss Loss spike 1-8 iterations\n\n5.0 after RMS spike (4 out of 4) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(0 out of 4)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n## 4 ViT-Large, Beta2 = 0.98 RMS (visual.conv1.weight)\n\n2 RMS spike (total 29)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n## 10.0 ViT-Large, Beta2 = 0.95 Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 1) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(1 out of 1)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n## 4 ViT-Large, Beta2 = 0.95 RMS (visual.conv1.weight)\n\n2 RMS spike (total 15)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n## 10.0 ViT-Large, Beta2 = 0.9 Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 0) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(0 out of 0)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n## 4 ViT-Large, Beta2 = 0.9 RMS (visual.conv1.weight)\n\n2 RMS spike (total 1)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nIteration\n\n## Figure 17: Observing a predictive relation between RMS spikes and loss spikes. For CLIP ViT-Large and multiple \u03b22 values, we use heuristics (Appendix D) to automatically identify loss spikes which we use for analysis. Out of 15 total loss spikes, 13 follow an RMS spike in the patch embedding layer (RMS > 2.3) by 1-8 iterations. We show loss spikes which are identified by our heuristic. We use green if they follow an RMS spike and otherwise use red. An RMS Spike indicates that the second moment estimator is out of date (see Section 3.4 and Shazeer and Stern [54]). The chance that a loss spike follows 1-8 iterations after an RMS spike by chance is 1.0%.\n\n27", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "10.0 ViT-Large, Beta2 = 0.99 Loss", "md": "## 10.0 ViT-Large, Beta2 = 0.99 Loss"}, {"type": "text", "value": "7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (9 out of 10) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(1 out of 10)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000", "md": "7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (9 out of 10) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(1 out of 10)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000"}, {"type": "heading", "lvl": 2, "value": "4 ViT-Large, Beta2 = 0.99 RMS (visual.conv1.weight)", "md": "## 4 ViT-Large, Beta2 = 0.99 RMS (visual.conv1.weight)"}, {"type": "text", "value": "2 RMS spike (total 52)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000", "md": "2 RMS spike (total 52)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000"}, {"type": "heading", "lvl": 2, "value": "10.0 ViT-Large, Beta2 = 0.98", "md": "## 10.0 ViT-Large, Beta2 = 0.98"}, {"type": "text", "value": "7.5 Loss Loss spike 1-8 iterations\n\n5.0 after RMS spike (4 out of 4) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(0 out of 4)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000", "md": "7.5 Loss Loss spike 1-8 iterations\n\n5.0 after RMS spike (4 out of 4) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(0 out of 4)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000"}, {"type": "heading", "lvl": 2, "value": "4 ViT-Large, Beta2 = 0.98 RMS (visual.conv1.weight)", "md": "## 4 ViT-Large, Beta2 = 0.98 RMS (visual.conv1.weight)"}, {"type": "text", "value": "2 RMS spike (total 29)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000", "md": "2 RMS spike (total 29)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000"}, {"type": "heading", "lvl": 2, "value": "10.0 ViT-Large, Beta2 = 0.95 Loss", "md": "## 10.0 ViT-Large, Beta2 = 0.95 Loss"}, {"type": "text", "value": "7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 1) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(1 out of 1)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000", "md": "7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 1) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(1 out of 1)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000"}, {"type": "heading", "lvl": 2, "value": "4 ViT-Large, Beta2 = 0.95 RMS (visual.conv1.weight)", "md": "## 4 ViT-Large, Beta2 = 0.95 RMS (visual.conv1.weight)"}, {"type": "text", "value": "2 RMS spike (total 15)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000", "md": "2 RMS spike (total 15)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000"}, {"type": "heading", "lvl": 2, "value": "10.0 ViT-Large, Beta2 = 0.9 Loss", "md": "## 10.0 ViT-Large, Beta2 = 0.9 Loss"}, {"type": "text", "value": "7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 0) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(0 out of 0)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000", "md": "7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 0) Loss spike not 1-8 iterations\n\n2.5 after RMS spike(0 out of 0)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000"}, {"type": "heading", "lvl": 2, "value": "4 ViT-Large, Beta2 = 0.9 RMS (visual.conv1.weight)", "md": "## 4 ViT-Large, Beta2 = 0.9 RMS (visual.conv1.weight)"}, {"type": "text", "value": "2 RMS spike (total 1)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nIteration", "md": "2 RMS spike (total 1)\n\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\nIteration"}, {"type": "heading", "lvl": 2, "value": "Figure 17: Observing a predictive relation between RMS spikes and loss spikes. For CLIP ViT-Large and multiple \u03b22 values, we use heuristics (Appendix D) to automatically identify loss spikes which we use for analysis. Out of 15 total loss spikes, 13 follow an RMS spike in the patch embedding layer (RMS > 2.3) by 1-8 iterations. We show loss spikes which are identified by our heuristic. We use green if they follow an RMS spike and otherwise use red. An RMS Spike indicates that the second moment estimator is out of date (see Section 3.4 and Shazeer and Stern [54]). The chance that a loss spike follows 1-8 iterations after an RMS spike by chance is 1.0%.", "md": "## Figure 17: Observing a predictive relation between RMS spikes and loss spikes. For CLIP ViT-Large and multiple \u03b22 values, we use heuristics (Appendix D) to automatically identify loss spikes which we use for analysis. Out of 15 total loss spikes, 13 follow an RMS spike in the patch embedding layer (RMS > 2.3) by 1-8 iterations. We show loss spikes which are identified by our heuristic. We use green if they follow an RMS spike and otherwise use red. An RMS Spike indicates that the second moment estimator is out of date (see Section 3.4 and Shazeer and Stern [54]). The chance that a loss spike follows 1-8 iterations after an RMS spike by chance is 1.0%."}, {"type": "text", "value": "27", "md": "27"}]}, {"page": 28, "text": "   10.0                                           ViT-Huge, Beta2 = 0.99                Loss\n    7.5                                                                                 Loss spike 1-8 iterations\n    5.0                                                                                 after RMS spike\n  Loss                                                                                  Loss spike not 1-8 iterations\n    2.5                                                                                 after RMS spike\n    0.0\n     2800         3000         3200          3400         3600         3800          4000         4200\n     4                                            ViT-Huge, Beta2 = 0.99                 RMS (visual.conv1.weight)\n     2                                                                                   RMS spike\n    RMS\n     0\n     2800         3000         3200          3400         3600         3800          4000         4200\n                                                       Iteration\n                                Figure 18: Zooming in on a section of Figure 16.\n   10.0                                           ViT-Huge, Beta2 = 0.98\n    7.5                                                                                 Loss\n                                                                                        Loss spike 1-8 iterations\n    5.0                                                                                 after RMS spike\n  Loss                                                                                  Loss spike not 1-8 iterations\n    2.5                                                                                 after RMS spike\n    0.0\n     4000          4050         4100          4150         4200          4250         4300          4350\n     4                                            ViT-Huge, Beta2 = 0.98                 RMS (visual.conv1.weight)\n     2                                                                                   RMS spike\n    RMS\n     0\n     4000          4050         4100          4150         4200          4250         4300          4350\n                                                       Iteration\nFigure 19: Examining the \u201cfailure\u201d case in Figure 16. We believe this is not really a failure as the non-predicted red\nloss spike does not really appear to be a spike at all. However, adjusting our heuristic led to the issue of true spikes\nnot being identified.                                 28", "md": "|10.0|ViT-Huge, Beta2 = 0.99|Loss|\n|---|---|---|\n|7.5| |Loss spike 1-8 iterations|\n|5.0| |after RMS spike|\n|Loss| |Loss spike not 1-8 iterations|\n|2.5| |after RMS spike|\n|0.0| | |\n\n$$\n\\begin{array}{cccccccccc}\n2800 & 3000 & 3200 & 3400 & 3600 & 3800 & 4000 & 4200 \\\\\n4 & ViT-Huge, Beta2 = 0.99 & RMS (visual.conv1.weight) \\\\\n2 & & RMS spike \\\\\nRMS & & \\\\\n0 & & \\\\\n2800 & 3000 & 3200 & 3400 & 3600 & 3800 & 4000 & 4200 \\\\\n\\end{array}\n$$\n\n|10.0|ViT-Huge, Beta2 = 0.98|\n|---|---|\n|7.5|Loss|\n| |Loss spike 1-8 iterations|\n|5.0|after RMS spike|\n|Loss|Loss spike not 1-8 iterations|\n|2.5|after RMS spike|\n|0.0| |\n\n$$\n\\begin{array}{cccccccccc}\n4000 & 4050 & 4100 & 4150 & 4200 & 4250 & 4300 & 4350 \\\\\n4 & ViT-Huge, Beta2 = 0.98 & RMS (visual.conv1.weight) \\\\\n2 & & RMS spike \\\\\nRMS & & \\\\\n0 & & \\\\\n4000 & 4050 & 4100 & 4150 & 4200 & 4250 & 4300 & 4350 \\\\\n\\end{array}\n$$\n\nFigure 19: Examining the \u201cfailure\u201d case in Figure 16. We believe this is not really a failure as the non-predicted red loss spike does not really appear to be a spike at all. However, adjusting our heuristic led to the issue of true spikes not being identified.", "images": [], "items": [{"type": "table", "rows": [["10.0", "ViT-Huge, Beta2 = 0.99", "Loss"], ["7.5", "", "Loss spike 1-8 iterations"], ["5.0", "", "after RMS spike"], ["Loss", "", "Loss spike not 1-8 iterations"], ["2.5", "", "after RMS spike"], ["0.0", "", ""]], "md": "|10.0|ViT-Huge, Beta2 = 0.99|Loss|\n|---|---|---|\n|7.5| |Loss spike 1-8 iterations|\n|5.0| |after RMS spike|\n|Loss| |Loss spike not 1-8 iterations|\n|2.5| |after RMS spike|\n|0.0| | |", "isPerfectTable": true, "csv": "\"10.0\",\"ViT-Huge, Beta2 = 0.99\",\"Loss\"\n\"7.5\",\"\",\"Loss spike 1-8 iterations\"\n\"5.0\",\"\",\"after RMS spike\"\n\"Loss\",\"\",\"Loss spike not 1-8 iterations\"\n\"2.5\",\"\",\"after RMS spike\"\n\"0.0\",\"\",\"\""}, {"type": "text", "value": "$$\n\\begin{array}{cccccccccc}\n2800 & 3000 & 3200 & 3400 & 3600 & 3800 & 4000 & 4200 \\\\\n4 & ViT-Huge, Beta2 = 0.99 & RMS (visual.conv1.weight) \\\\\n2 & & RMS spike \\\\\nRMS & & \\\\\n0 & & \\\\\n2800 & 3000 & 3200 & 3400 & 3600 & 3800 & 4000 & 4200 \\\\\n\\end{array}\n$$", "md": "$$\n\\begin{array}{cccccccccc}\n2800 & 3000 & 3200 & 3400 & 3600 & 3800 & 4000 & 4200 \\\\\n4 & ViT-Huge, Beta2 = 0.99 & RMS (visual.conv1.weight) \\\\\n2 & & RMS spike \\\\\nRMS & & \\\\\n0 & & \\\\\n2800 & 3000 & 3200 & 3400 & 3600 & 3800 & 4000 & 4200 \\\\\n\\end{array}\n$$"}, {"type": "table", "rows": [["10.0", "ViT-Huge, Beta2 = 0.98"], ["7.5", "Loss"], ["", "Loss spike 1-8 iterations"], ["5.0", "after RMS spike"], ["Loss", "Loss spike not 1-8 iterations"], ["2.5", "after RMS spike"], ["0.0", ""]], "md": "|10.0|ViT-Huge, Beta2 = 0.98|\n|---|---|\n|7.5|Loss|\n| |Loss spike 1-8 iterations|\n|5.0|after RMS spike|\n|Loss|Loss spike not 1-8 iterations|\n|2.5|after RMS spike|\n|0.0| |", "isPerfectTable": true, "csv": "\"10.0\",\"ViT-Huge, Beta2 = 0.98\"\n\"7.5\",\"Loss\"\n\"\",\"Loss spike 1-8 iterations\"\n\"5.0\",\"after RMS spike\"\n\"Loss\",\"Loss spike not 1-8 iterations\"\n\"2.5\",\"after RMS spike\"\n\"0.0\",\"\""}, {"type": "text", "value": "$$\n\\begin{array}{cccccccccc}\n4000 & 4050 & 4100 & 4150 & 4200 & 4250 & 4300 & 4350 \\\\\n4 & ViT-Huge, Beta2 = 0.98 & RMS (visual.conv1.weight) \\\\\n2 & & RMS spike \\\\\nRMS & & \\\\\n0 & & \\\\\n4000 & 4050 & 4100 & 4150 & 4200 & 4250 & 4300 & 4350 \\\\\n\\end{array}\n$$\n\nFigure 19: Examining the \u201cfailure\u201d case in Figure 16. We believe this is not really a failure as the non-predicted red loss spike does not really appear to be a spike at all. However, adjusting our heuristic led to the issue of true spikes not being identified.", "md": "$$\n\\begin{array}{cccccccccc}\n4000 & 4050 & 4100 & 4150 & 4200 & 4250 & 4300 & 4350 \\\\\n4 & ViT-Huge, Beta2 = 0.98 & RMS (visual.conv1.weight) \\\\\n2 & & RMS spike \\\\\nRMS & & \\\\\n0 & & \\\\\n4000 & 4050 & 4100 & 4150 & 4200 & 4250 & 4300 & 4350 \\\\\n\\end{array}\n$$\n\nFigure 19: Examining the \u201cfailure\u201d case in Figure 16. We believe this is not really a failure as the non-predicted red loss spike does not really appear to be a spike at all. However, adjusting our heuristic led to the issue of true spikes not being identified."}]}, {"page": 29, "text": "   10.0                                         ViT-Large, Beta2 = 0.99             Loss\n    7.5                                                                             Loss spike 1-8 iterations\n    5.0                                                                             after RMS spike\n  Loss                                                                              Loss spike not 1-8 iterations\n    2.5                                                                             after RMS spike\n    0.0\n     5600        5650        5700         5750        5800        5850        5900        5950        6000\n     4                                          ViT-Large, Beta2 = 0.99              RMS (visual.conv1.weight)\n     2                                                                               RMS spike\n    RMS\n     0\n     5600        5650        5700         5750        5800        5850        5900        5950        6000\n   10.0                                         ViT-Large, Beta2 = 0.95             Loss\n    7.5                                                                             Loss spike 1-8 iterations\n    5.0                                                                             after RMS spike\n  Loss                                                                              Loss spike not 1-8 iterations\n    2.5                                                                             after RMS spike\n    0.0\n     4500              4550              4600              4650             4700              4750\n     4                                          ViT-Large, Beta2 = 0.95              RMS (visual.conv1.weight)\n     2                                                                               RMS spike\n    RMS\n     0\n     4500              4550              4600              4650             4700              4750\n                                                    Iteration\nFigure 20: Examining the \u201cfailure\u201d cases in Figure 17. We believe these to primarily issues with our heuristic, but\nadjusting our heuristic led to other issues such as true spikes not being identified.\n   10.0                                        ViT-Huge, Beta2 = 0.99              Loss\n    7.5                                                                            Loss spike 1-8 iterations\n    5.0                                                                            after RMS spike (0 out of 7)\n  Loss                                                                             Loss spike not 1-8 iterations\n    2.5                                                                            after RMS spike(7 out of 7)\n    0.0\n      0          2500        5000        7500        10000       12500       15000       17500       20000\n     4                                         ViT-Huge, Beta2 = 0.99               RMS (block.20.attn.in_proj)\n     2                                                                              RMS spike (total 7)\n    RMS\n     00          2500        5000        7500        10000       12500       15000       17500       20000\n   10.0                                        ViT-Huge, Beta2 = 0.98              Loss\n    7.5                                                                            Loss spike 1-8 iterations\n    5.0                                                                            after RMS spike (0 out of 8)\n  Loss                                                                             Loss spike not 1-8 iterations\n    2.5                                                                            after RMS spike(8 out of 8)\n    0.0\n      0          2500        5000        7500        10000       12500       15000       17500       20000\n     4                                         ViT-Huge, Beta2 = 0.98               RMS (block.20.attn.in_proj)\n     2                                                                              RMS spike (total 4)\n    RMS\n     00          2500        5000        7500        10000       12500       15000       17500       20000\n                                                    Iteration\nFigure 21: This figure repeats part of Figure 16 but examines theRMS  of a random layer in the middle of the\ntransformer\u2014blocks.20.attn.in proj\u2014not the patch embedding layer. RMS spikes no longer precede loss spikes.\n                                                    29", "md": "# OCR Text\n\n10.0 $$\\text{ViT-Large, Beta2 = 0.99}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike\n\n0.0\n\n5600\n5650\n5700\n5750\n5800\n5850\n5900\n5950\n6000\n10.0 $$\\text{ViT-Large, Beta2 = 0.99}$$ RMS (visual.conv1.weight)\n\n2 RMS spike\n\nRMS\n\n0\n\n5600\n5650\n5700\n5750\n5800\n5850\n5900\n5950\n6000\n10.0 $$\\text{ViT-Huge, Beta2 = 0.99}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 7)\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike (7 out of 7)\n\n0.0\n\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n10.0 $$\\text{ViT-Huge, Beta2 = 0.98}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 8)\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike (8 out of 8)\n\n0.0\n\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n\nFigure 20: Examining the \u201cfailure\u201d cases in Figure 17. We believe these to primarily issues with our heuristic, but adjusting our heuristic led to other issues such as true spikes not being identified.\n\nFigure 21: This figure repeats part of Figure 16 but examines the RMS of a random layer in the middle of the transformer\u2014blocks.20.attn.in proj\u2014not the patch embedding layer. RMS spikes no longer precede loss spikes.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "text", "value": "10.0 $$\\text{ViT-Large, Beta2 = 0.99}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike\n\n0.0\n\n5600\n5650\n5700\n5750\n5800\n5850\n5900\n5950\n6000\n10.0 $$\\text{ViT-Large, Beta2 = 0.99}$$ RMS (visual.conv1.weight)\n\n2 RMS spike\n\nRMS\n\n0\n\n5600\n5650\n5700\n5750\n5800\n5850\n5900\n5950\n6000\n10.0 $$\\text{ViT-Huge, Beta2 = 0.99}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 7)\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike (7 out of 7)\n\n0.0\n\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n10.0 $$\\text{ViT-Huge, Beta2 = 0.98}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 8)\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike (8 out of 8)\n\n0.0\n\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n\nFigure 20: Examining the \u201cfailure\u201d cases in Figure 17. We believe these to primarily issues with our heuristic, but adjusting our heuristic led to other issues such as true spikes not being identified.\n\nFigure 21: This figure repeats part of Figure 16 but examines the RMS of a random layer in the middle of the transformer\u2014blocks.20.attn.in proj\u2014not the patch embedding layer. RMS spikes no longer precede loss spikes.", "md": "10.0 $$\\text{ViT-Large, Beta2 = 0.99}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike\n\n0.0\n\n5600\n5650\n5700\n5750\n5800\n5850\n5900\n5950\n6000\n10.0 $$\\text{ViT-Large, Beta2 = 0.99}$$ RMS (visual.conv1.weight)\n\n2 RMS spike\n\nRMS\n\n0\n\n5600\n5650\n5700\n5750\n5800\n5850\n5900\n5950\n6000\n10.0 $$\\text{ViT-Huge, Beta2 = 0.99}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 7)\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike (7 out of 7)\n\n0.0\n\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n10.0 $$\\text{ViT-Huge, Beta2 = 0.98}$$ Loss\n\n7.5 Loss spike 1-8 iterations\n\n5.0 after RMS spike (0 out of 8)\n\nLoss Loss spike not 1-8 iterations\n\n2.5 after RMS spike (8 out of 8)\n\n0.0\n\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n\nFigure 20: Examining the \u201cfailure\u201d cases in Figure 17. We believe these to primarily issues with our heuristic, but adjusting our heuristic led to other issues such as true spikes not being identified.\n\nFigure 21: This figure repeats part of Figure 16 but examines the RMS of a random layer in the middle of the transformer\u2014blocks.20.attn.in proj\u2014not the patch embedding layer. RMS spikes no longer precede loss spikes."}]}], "job_id": "2be27631-4fd5-443c-a650-031fed254bab", "file_path": "./corpus/2304.13013.pdf"}