{"pages": [{"page": 1, "text": "                           GENEVAL: An Object-Focused Framework for\n                                     Evaluating Text-to-Image Alignment\n                               Dhruba Ghosh1                 Hannaneh Hajishirzi1,2 \u2217           Ludwig Schmidt1,2,3 \u2217\n                         1 University of Washington            2Allen Institute for AI                  3LAION\narXiv:2310.11513v1  [cs.CV]  17 Oct 2023                     \u2217 denotes equal contribution\n                                                                   Abstract\n                              Recent breakthroughs in diffusion models, multimodal pretraining, and effi       cient\n                              finetuning have led to an explosion of text-to-image generative models. Given\n                              human evaluation is expensive and difficult to scale, automated methods are critical\n                              for evaluating the increasingly large number of new models. However, most current\n                              automated evaluation metrics like FID or CLIPScore only offer a holistic measure of\n                              image quality or image-text alignment, and are unsuited for fine-grained or instance-\n                              level analysis. In this paper, we introduce GENEVAL, an object-focused framework\n                              to evaluate compositional image properties such as object co-occurrence, position,\n                              count, and color. We show that current object detection models can be leveraged\n                              to evaluate text-to-image models on a variety of generation tasks with strong\n                              human agreement, and that other discriminative vision models can be linked to this\n                              pipeline to further verify properties like object color. We then evaluate several open-\n                              source text-to-image models and analyze their relative generative capabilities on\n                              our benchmark. We find that recent models demonstrate significant improvement\n                              on these tasks, though they are still lacking in complex capabilities such as spatial\n                              relations and attribute binding. Finally, we demonstrate how GENEVAL might be\n                              used to help discover existing failure modes, in order to inform development of the\n                              next generation of text-to-image models. Our code to run the GENEVAL framework\n                              is publicly available at https://github.com/djghosh13/geneval.\n                    1    Introduction\n                    Text-to-image (T2I) models have exploded in popularity in recent years. After the introduction of\n                    DALL-E [33], breakthroughs in diffusion models quickly led to the development of more capable T2I\n                    models like DALL-E 2 [1] and Stable Diffusion [36]. Since then, T2I models have found varied use\n                    cases in creative matters like art and research applications like training data generation. Research into\n                    parameter-efficient finetuning methods has also led to a large number of new models and finetuned\n                    checkpoints of popular models. Currently, the Huggingface Hub1 hosts over 4,000 T2I-related models\n                    and repositories.\n                    The current gold standard for evaluating T2I models is typically human preference comparison\n                    between models, which is costly to scale up. Thus, the increase in number of T2I models makes\n                    manual evaluation inadequate. This raises the need for reliable, automated evaluation methods.\n                    However, traditional automated evaluation methods cannot analyze compositional capabilities and\n                    lack fine-grained reporting. Metrics such as the Frechet Inception Distance [16] solely evaluate image\n                    quality without taking the prompt into account. Other common metrics such as CLIPScore [15] rely\n                        1https://huggingface.co/models", "md": "# Text-to-Image Alignment Evaluation\n\n# GENEVAL: An Object-Focused Framework for Evaluating Text-to-Image Alignment\n\nDhruba Ghosh1, Hannaneh Hajishirzi1,2 \u2217, Ludwig Schmidt1,2,3 \u2217\n\n1University of Washington, 2Allen Institute for AI, 3LAION\n\narXiv:2310.11513v1 [cs.CV] 17 Oct 2023\n\n\u2217 denotes equal contribution\n\n## Abstract\n\nRecent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. In this paper, we introduce GENEVAL, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open-source text-to-image models and analyze their relative generative capabilities on our benchmark. We find that recent models demonstrate significant improvement on these tasks, though they are still lacking in complex capabilities such as spatial relations and attribute binding. Finally, we demonstrate how GENEVAL might be used to help discover existing failure modes, in order to inform development of the next generation of text-to-image models. Our code to run the GENEVAL framework is publicly available at https://github.com/djghosh13/geneval.\n\n## Introduction\n\nText-to-image (T2I) models have exploded in popularity in recent years. After the introduction of DALL-E [33], breakthroughs in diffusion models quickly led to the development of more capable T2I models like DALL-E 2 [1] and Stable Diffusion [36]. Since then, T2I models have found varied use cases in creative matters like art and research applications like training data generation. Research into parameter-efficient finetuning methods has also led to a large number of new models and finetuned checkpoints of popular models. Currently, the Huggingface Hub1 hosts over 4,000 T2I-related models and repositories.\n\nThe current gold standard for evaluating T2I models is typically human preference comparison between models, which is costly to scale up. Thus, the increase in the number of T2I models makes manual evaluation inadequate. This raises the need for reliable, automated evaluation methods. However, traditional automated evaluation methods cannot analyze compositional capabilities and lack fine-grained reporting. Metrics such as the Frechet Inception Distance [16] solely evaluate image quality without taking the prompt into account. Other common metrics such as CLIPScore [15] rely", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Text-to-Image Alignment Evaluation", "md": "# Text-to-Image Alignment Evaluation"}, {"type": "heading", "lvl": 1, "value": "GENEVAL: An Object-Focused Framework for Evaluating Text-to-Image Alignment", "md": "# GENEVAL: An Object-Focused Framework for Evaluating Text-to-Image Alignment"}, {"type": "text", "value": "Dhruba Ghosh1, Hannaneh Hajishirzi1,2 \u2217, Ludwig Schmidt1,2,3 \u2217\n\n1University of Washington, 2Allen Institute for AI, 3LAION\n\narXiv:2310.11513v1 [cs.CV] 17 Oct 2023\n\n\u2217 denotes equal contribution", "md": "Dhruba Ghosh1, Hannaneh Hajishirzi1,2 \u2217, Ludwig Schmidt1,2,3 \u2217\n\n1University of Washington, 2Allen Institute for AI, 3LAION\n\narXiv:2310.11513v1 [cs.CV] 17 Oct 2023\n\n\u2217 denotes equal contribution"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. In this paper, we introduce GENEVAL, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open-source text-to-image models and analyze their relative generative capabilities on our benchmark. We find that recent models demonstrate significant improvement on these tasks, though they are still lacking in complex capabilities such as spatial relations and attribute binding. Finally, we demonstrate how GENEVAL might be used to help discover existing failure modes, in order to inform development of the next generation of text-to-image models. Our code to run the GENEVAL framework is publicly available at https://github.com/djghosh13/geneval.", "md": "Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. However, most current automated evaluation metrics like FID or CLIPScore only offer a holistic measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. In this paper, we introduce GENEVAL, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. We then evaluate several open-source text-to-image models and analyze their relative generative capabilities on our benchmark. We find that recent models demonstrate significant improvement on these tasks, though they are still lacking in complex capabilities such as spatial relations and attribute binding. Finally, we demonstrate how GENEVAL might be used to help discover existing failure modes, in order to inform development of the next generation of text-to-image models. Our code to run the GENEVAL framework is publicly available at https://github.com/djghosh13/geneval."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "Text-to-image (T2I) models have exploded in popularity in recent years. After the introduction of DALL-E [33], breakthroughs in diffusion models quickly led to the development of more capable T2I models like DALL-E 2 [1] and Stable Diffusion [36]. Since then, T2I models have found varied use cases in creative matters like art and research applications like training data generation. Research into parameter-efficient finetuning methods has also led to a large number of new models and finetuned checkpoints of popular models. Currently, the Huggingface Hub1 hosts over 4,000 T2I-related models and repositories.\n\nThe current gold standard for evaluating T2I models is typically human preference comparison between models, which is costly to scale up. Thus, the increase in the number of T2I models makes manual evaluation inadequate. This raises the need for reliable, automated evaluation methods. However, traditional automated evaluation methods cannot analyze compositional capabilities and lack fine-grained reporting. Metrics such as the Frechet Inception Distance [16] solely evaluate image quality without taking the prompt into account. Other common metrics such as CLIPScore [15] rely", "md": "Text-to-image (T2I) models have exploded in popularity in recent years. After the introduction of DALL-E [33], breakthroughs in diffusion models quickly led to the development of more capable T2I models like DALL-E 2 [1] and Stable Diffusion [36]. Since then, T2I models have found varied use cases in creative matters like art and research applications like training data generation. Research into parameter-efficient finetuning methods has also led to a large number of new models and finetuned checkpoints of popular models. Currently, the Huggingface Hub1 hosts over 4,000 T2I-related models and repositories.\n\nThe current gold standard for evaluating T2I models is typically human preference comparison between models, which is costly to scale up. Thus, the increase in the number of T2I models makes manual evaluation inadequate. This raises the need for reliable, automated evaluation methods. However, traditional automated evaluation methods cannot analyze compositional capabilities and lack fine-grained reporting. Metrics such as the Frechet Inception Distance [16] solely evaluate image quality without taking the prompt into account. Other common metrics such as CLIPScore [15] rely"}]}, {"page": 2, "text": "                    a photo of a                                               a photo of a                                a photo of a\n           [purple      ] [backpack      ]                            [purple      ] [backpack      ]             [purple      ] [backpack    ]\n         and a [white      ] [umbrella      ]                       and a [white      ] [umbrella      ]       and a [white      ] [umbrella    ]\n                                                                                            umbrella                              purple\n                                                                                            umbrella                              yellow\n                                                                                           backpack                               purple\n                  Text-to-image                                        Object detection                            Color classification\n Figure 1: Visualization of GENEVAL. Modern object detection models can be used to automatically\n verify text-to-image generations. The detected bounding boxes and segmentation masks can be used\n to verify object presence, count, and position, and then passed to downstream discriminative vision\n models to verify fine-grained object properties such as color. The example image was generated by\n Stable Diffusion v2.1.\n on alignment of image and text embeddings, which are not strongly correlated with human judgment\n on complex tasks, as our experiments show.\n In light of this, we propose GENEVAL, an automated object-focused framework for evaluating T2I\n model capabilities on structured tasks (Figure 1). GENEVAL centers around the use of an object\n detection model, which verifies that the generated image contains the objects specified in the text\n prompt. The bounding box information and segmentation masks returned by the model are used to\n verify properties specified in the prompt, such as counting and relative positioning between objects.\n This metadata is then also passed to other vision models to evaluate other properties, in our case,\n object color classification. Overall, this results in an interpretable and modular framework which\n provides fine-grained information about T2I model capabilities.\nWe verify that our evaluation framework aligns with human judgment through a human evaluation\n study of 6,000 fine-grained annotations over 1,200 images. Overall, GENEVAL attains 83% agreement\n with annotators about the correctness of generated images, compared to the interannotator agreement\n of 88%. This agreement rate is boosted to 91% on images that annotators unanimously agree on,\n showing that our evaluation framework is highly reliable and aligns with human judgment. We also\n find that GENEVAL obtains significantly greater human agreement than the CLIPScore image-text\n alignment metric on complex tasks that involve more compositional reasoning.\nWe then evaluate several popular open-source T2I image models using our GENEVAL framework.\n Our experiments show that the new IF model [11], with a larger text encoder and pixel-space diffusion,\n outperforms prior Stable Diffusion models [36], with IF-XL correctly generating 61% of images over\n 50% from Stable Diffusion v2.1. The most recent Stable Diffusion XL [30], with various architecture\n and training changes, beats v2.1 on certain tasks like depicting multiple objects but fails to improve at\n counting. Increasing model size leads to better performance on certain tasks, but increased pretraining\n time does not necessarily improve performance.\n Moreover, all the tested models perform poorly on complex tasks such as relative positioning\n and attribute binding\u2014with the best model generating only 15% and 35% of images correctly,\n respectively\u2014showing there is still much progress to be made in text-guided image generation. This\n supports prior claims that spatial reasoning and attribute binding are difficult for T2I models [9, 13, 14].\n However, we demonstrate how the fine-grained nature of GENEVAL can inform development of new\n models by uncovering failure modes and patterns in current image generation.\n In summary, our contributions are as follows: (1) We introduce GENEVAL, an automated framework\n for evaluating T2I models using existing discriminative vision models; (2) we show that current\n object detection models are strongly aligned with human judgment and can be used to evaluate a\n variety of compositional capabilities; and (3) we evaluate several popular open-source T2I models\n                                                                                    2", "md": "# Text-to-Image Evaluation Framework\n\n# Text-to-Image Evaluation Framework\n\nA photo of a purple backpack and a white umbrella.\n\nObject detection:\n\n|Object|Color|\n|---|---|\n|Umbrella|White|\n|Umbrella|Yellow|\n|Backpack|Purple|\n\nText-to-image alignment of image and text embeddings, which are not strongly correlated with human judgment on complex tasks, as our experiments show.\n\nIn light of this, we propose GENEVAL, an automated object-focused framework for evaluating T2I model capabilities on structured tasks (Figure 1). GENEVAL centers around the use of an object detection model, which verifies that the generated image contains the objects specified in the text prompt. The bounding box information and segmentation masks returned by the model are used to verify properties specified in the prompt, such as counting and relative positioning between objects. This metadata is then also passed to other vision models to evaluate other properties, in our case, object color classification. Overall, this results in an interpretable and modular framework which provides fine-grained information about T2I model capabilities.\n\nWe verify that our evaluation framework aligns with human judgment through a human evaluation study of 6,000 fine-grained annotations over 1,200 images. Overall, GENEVAL attains 83% agreement with annotators about the correctness of generated images, compared to the interannotator agreement of 88%. This agreement rate is boosted to 91% on images that annotators unanimously agree on, showing that our evaluation framework is highly reliable and aligns with human judgment. We also find that GENEVAL obtains significantly greater human agreement than the CLIPScore image-text alignment metric on complex tasks that involve more compositional reasoning.\n\nWe then evaluate several popular open-source T2I image models using our GENEVAL framework. Our experiments show that the new IF model [11], with a larger text encoder and pixel-space diffusion, outperforms prior Stable Diffusion models [36], with IF-XL correctly generating 61% of images over 50% from Stable Diffusion v2.1. The most recent Stable Diffusion XL [30], with various architecture and training changes, beats v2.1 on certain tasks like depicting multiple objects but fails to improve at counting. Increasing model size leads to better performance on certain tasks, but increased pretraining time does not necessarily improve performance.\n\nMoreover, all the tested models perform poorly on complex tasks such as relative positioning and attribute binding\u2014with the best model generating only 15% and 35% of images correctly, respectively\u2014showing there is still much progress to be made in text-guided image generation. This supports prior claims that spatial reasoning and attribute binding are difficult for T2I models [9, 13, 14]. However, we demonstrate how the fine-grained nature of GENEVAL can inform development of new models by uncovering failure modes and patterns in current image generation.\n\nIn summary, our contributions are as follows: (1) We introduce GENEVAL, an automated framework for evaluating T2I models using existing discriminative vision models; (2) we show that current object detection models are strongly aligned with human judgment and can be used to evaluate a variety of compositional capabilities; and (3) we evaluate several popular open-source T2I models.\n\n$$\\text{Figure 1: Visualization of GENEVAL. Modern object detection models can be used to automatically verify text-to-image generations. The detected bounding boxes and segmentation masks can be used to verify object presence, count, and position, and then passed to downstream discriminative vision models to verify fine-grained object properties such as color. The example image was generated by Stable Diffusion v2.1.}$$", "images": [{"name": "page-2-0.jpg", "height": 82, "width": 82, "x": 128, "y": 114}, {"name": "page-2-1.jpg", "height": 73, "width": 73, "x": 229, "y": 119}, {"name": "page-2-3.jpg", "height": 14, "width": 33, "x": 369, "y": 133}, {"name": "page-2-2.jpg", "height": 43, "width": 35, "x": 368, "y": 152}, {"name": "page-2-4.jpg", "height": 15, "width": 43, "x": 364, "y": 112}], "items": [{"type": "heading", "lvl": 1, "value": "Text-to-Image Evaluation Framework", "md": "# Text-to-Image Evaluation Framework"}, {"type": "heading", "lvl": 1, "value": "Text-to-Image Evaluation Framework", "md": "# Text-to-Image Evaluation Framework"}, {"type": "text", "value": "A photo of a purple backpack and a white umbrella.\n\nObject detection:", "md": "A photo of a purple backpack and a white umbrella.\n\nObject detection:"}, {"type": "table", "rows": [["Object", "Color"], ["Umbrella", "White"], ["Umbrella", "Yellow"], ["Backpack", "Purple"]], "md": "|Object|Color|\n|---|---|\n|Umbrella|White|\n|Umbrella|Yellow|\n|Backpack|Purple|", "isPerfectTable": true, "csv": "\"Object\",\"Color\"\n\"Umbrella\",\"White\"\n\"Umbrella\",\"Yellow\"\n\"Backpack\",\"Purple\""}, {"type": "text", "value": "Text-to-image alignment of image and text embeddings, which are not strongly correlated with human judgment on complex tasks, as our experiments show.\n\nIn light of this, we propose GENEVAL, an automated object-focused framework for evaluating T2I model capabilities on structured tasks (Figure 1). GENEVAL centers around the use of an object detection model, which verifies that the generated image contains the objects specified in the text prompt. The bounding box information and segmentation masks returned by the model are used to verify properties specified in the prompt, such as counting and relative positioning between objects. This metadata is then also passed to other vision models to evaluate other properties, in our case, object color classification. Overall, this results in an interpretable and modular framework which provides fine-grained information about T2I model capabilities.\n\nWe verify that our evaluation framework aligns with human judgment through a human evaluation study of 6,000 fine-grained annotations over 1,200 images. Overall, GENEVAL attains 83% agreement with annotators about the correctness of generated images, compared to the interannotator agreement of 88%. This agreement rate is boosted to 91% on images that annotators unanimously agree on, showing that our evaluation framework is highly reliable and aligns with human judgment. We also find that GENEVAL obtains significantly greater human agreement than the CLIPScore image-text alignment metric on complex tasks that involve more compositional reasoning.\n\nWe then evaluate several popular open-source T2I image models using our GENEVAL framework. Our experiments show that the new IF model [11], with a larger text encoder and pixel-space diffusion, outperforms prior Stable Diffusion models [36], with IF-XL correctly generating 61% of images over 50% from Stable Diffusion v2.1. The most recent Stable Diffusion XL [30], with various architecture and training changes, beats v2.1 on certain tasks like depicting multiple objects but fails to improve at counting. Increasing model size leads to better performance on certain tasks, but increased pretraining time does not necessarily improve performance.\n\nMoreover, all the tested models perform poorly on complex tasks such as relative positioning and attribute binding\u2014with the best model generating only 15% and 35% of images correctly, respectively\u2014showing there is still much progress to be made in text-guided image generation. This supports prior claims that spatial reasoning and attribute binding are difficult for T2I models [9, 13, 14]. However, we demonstrate how the fine-grained nature of GENEVAL can inform development of new models by uncovering failure modes and patterns in current image generation.\n\nIn summary, our contributions are as follows: (1) We introduce GENEVAL, an automated framework for evaluating T2I models using existing discriminative vision models; (2) we show that current object detection models are strongly aligned with human judgment and can be used to evaluate a variety of compositional capabilities; and (3) we evaluate several popular open-source T2I models.\n\n$$\\text{Figure 1: Visualization of GENEVAL. Modern object detection models can be used to automatically verify text-to-image generations. The detected bounding boxes and segmentation masks can be used to verify object presence, count, and position, and then passed to downstream discriminative vision models to verify fine-grained object properties such as color. The example image was generated by Stable Diffusion v2.1.}$$", "md": "Text-to-image alignment of image and text embeddings, which are not strongly correlated with human judgment on complex tasks, as our experiments show.\n\nIn light of this, we propose GENEVAL, an automated object-focused framework for evaluating T2I model capabilities on structured tasks (Figure 1). GENEVAL centers around the use of an object detection model, which verifies that the generated image contains the objects specified in the text prompt. The bounding box information and segmentation masks returned by the model are used to verify properties specified in the prompt, such as counting and relative positioning between objects. This metadata is then also passed to other vision models to evaluate other properties, in our case, object color classification. Overall, this results in an interpretable and modular framework which provides fine-grained information about T2I model capabilities.\n\nWe verify that our evaluation framework aligns with human judgment through a human evaluation study of 6,000 fine-grained annotations over 1,200 images. Overall, GENEVAL attains 83% agreement with annotators about the correctness of generated images, compared to the interannotator agreement of 88%. This agreement rate is boosted to 91% on images that annotators unanimously agree on, showing that our evaluation framework is highly reliable and aligns with human judgment. We also find that GENEVAL obtains significantly greater human agreement than the CLIPScore image-text alignment metric on complex tasks that involve more compositional reasoning.\n\nWe then evaluate several popular open-source T2I image models using our GENEVAL framework. Our experiments show that the new IF model [11], with a larger text encoder and pixel-space diffusion, outperforms prior Stable Diffusion models [36], with IF-XL correctly generating 61% of images over 50% from Stable Diffusion v2.1. The most recent Stable Diffusion XL [30], with various architecture and training changes, beats v2.1 on certain tasks like depicting multiple objects but fails to improve at counting. Increasing model size leads to better performance on certain tasks, but increased pretraining time does not necessarily improve performance.\n\nMoreover, all the tested models perform poorly on complex tasks such as relative positioning and attribute binding\u2014with the best model generating only 15% and 35% of images correctly, respectively\u2014showing there is still much progress to be made in text-guided image generation. This supports prior claims that spatial reasoning and attribute binding are difficult for T2I models [9, 13, 14]. However, we demonstrate how the fine-grained nature of GENEVAL can inform development of new models by uncovering failure modes and patterns in current image generation.\n\nIn summary, our contributions are as follows: (1) We introduce GENEVAL, an automated framework for evaluating T2I models using existing discriminative vision models; (2) we show that current object detection models are strongly aligned with human judgment and can be used to evaluate a variety of compositional capabilities; and (3) we evaluate several popular open-source T2I models.\n\n$$\\text{Figure 1: Visualization of GENEVAL. Modern object detection models can be used to automatically verify text-to-image generations. The detected bounding boxes and segmentation masks can be used to verify object presence, count, and position, and then passed to downstream discriminative vision models to verify fine-grained object properties such as color. The example image was generated by Stable Diffusion v2.1.}$$"}]}, {"page": 3, "text": "                                                                          a photo of a wine glass\n                                                                             right of a hot dog\n                                                                            Human gold label\n                                                                                CLIPScore\n                                                                          33.8     <      36.7\n                                                                              GenEval (ours)\n                                                                    Found hot dog          Found hot dog\n                                                                 Found wine glass          Found wine glass\n                                                  Found wine glass right of hot dog        Expected wine glass right of hot dog, found above hot dog\n                                                                        \u21d2 Correct          \u21d2 Incorrect\nFigure 2: Comparison between GENEVAL and CLIPScore. CLIPScore returns a scalar value\nindicating image-text alignment, whereas GENEVAL breaks the prompt down into correct and\nincorrect elements before producing a final binary score. Compared to CLIPScore, GENEVAL obtains\nhigher agreement with human judgment on complex compositional tasks.\nand find significant performance improvement in recent models, though there is still much room to\nimprove on complex compositional tasks.\n2       Related work\nText-to-image models.                         While T2I models have been around since 2016 [25, 35], the field saw a\nsurge in popularity starting with the autoregressive DALL-E in 2021 [33], followed by diffusion\nmodels like DALL-E 2 [34], Midjourney [2], GLIDE [27], and Stable Diffusion [36]. Subsequent\nimprovements scaled the text encoder and made other architectural changes, such as in Imagen [37],\nIF [11], and Stable Diffusion XL [30]. Meanwhile, new GAN and autoregressive models are still\nbeing developed [20, 44]. However, most popularly used models and finetuned checkpoints for T2I\nare still based off of the Stable Diffusion architecture.\nAutomated evaluation.                          Automated T2I evaluation techniques primarily measure either image\nquality or image-text alignment. The Inception Score (IS) [38] and Frechet Inception Distance (FID)\n[16] metrics measure image quality independent of text. FID score estimates the distance between\nthe distribution of generated images and a reference distribution of real-world images, and serves as a\nheuristic for how \u201crealistic\u201d the generated images are.\nOther metrics evaluate image-text alignment. Many of these are reference-based: BLEU [28] and\nCIDEr [40] can be used to evaluate generations by performing captioning and comparing to a set of\nreference captions [17, 18], while R-Precision [29, 43] measures text recall scores from a reference\ndataset of captions. CLIPScore [15], on the other hand, is reference-free, and produces an alignment\nmeasure based on the cosine similarity of the prompt and generated image CLIP [31] embeddings.\nThis is shown to be more strongly correlated with human judgment than previous reference-based\nmetrics that compare generated captions with a set of potential captions [15].\nHuman preference-based evaluation.                                     Inspired by past work in NLP, several new methods evaluate\ngenerated images using models trained on direct human preference data [21, 41, 42]. This involves\nmanual annotation of a large dataset of images \u2014 ranking groups of two or more images generated\nfrom the same prompt. A model is then trained on this dataset to predict a scalar score which\ncorrelates with human preference, offering a holistic evaluation of generated images. This serves to\ndirectly measure the endgoal of human preference, but combines multiple aspects of image generation,\ne.g., aesthetics, realism, and image-text alignment, into one number, whereas we aim to provide a\nbreakdown of exact errors in each generated image.\nObject detection-based evaluation.                                  A couple prior works also propose the use of object detection\nto evaluate T2I generation capabilities. Dall-Eval [9] trains a task-specific object detector for\nmeasuring each of several compositional reasoning tasks. They show that finetuning T2I models\n                                                                                      3", "md": "# Document\n\na photo of a wine glass\n\nright of a hot dog\n\nHuman gold label\n\nCLIPScore\n\n$$33.8 < 36.7$$\n\nGenEval (ours)\n\nFound hot dog &emsp; Found hot dog\n\nFound wine glass &emsp; Found wine glass\n\nFound wine glass right of hot dog &emsp; Expected wine glass right of hot dog, found above hot dog\n\n\u21d2 Correct &emsp; \u21d2 Incorrect\n\nFigure 2: Comparison between GENEVAL and CLIPScore. CLIPScore returns a scalar value indicating image-text alignment, whereas GENEVAL breaks the prompt down into correct and incorrect elements before producing a final binary score. Compared to CLIPScore, GENEVAL obtains higher agreement with human judgment on complex compositional tasks and find significant performance improvement in recent models, though there is still much room to improve on complex compositional tasks.\n\n## Related work\n\nText-to-image models. While T2I models have been around since 2016 [25, 35], the field saw a surge in popularity starting with the autoregressive DALL-E in 2021 [33], followed by diffusion models like DALL-E 2 [34], Midjourney [2], GLIDE [27], and Stable Diffusion [36]. Subsequent improvements scaled the text encoder and made other architectural changes, such as in Imagen [37], IF [11], and Stable Diffusion XL [30]. Meanwhile, new GAN and autoregressive models are still being developed [20, 44]. However, most popularly used models and finetuned checkpoints for T2I are still based off of the Stable Diffusion architecture.\n\nAutomated evaluation. Automated T2I evaluation techniques primarily measure either image quality or image-text alignment. The Inception Score (IS) [38] and Frechet Inception Distance (FID) [16] metrics measure image quality independent of text. FID score estimates the distance between the distribution of generated images and a reference distribution of real-world images, and serves as a heuristic for how \u201crealistic\u201d the generated images are.\n\nOther metrics evaluate image-text alignment. Many of these are reference-based: BLEU [28] and CIDEr [40] can be used to evaluate generations by performing captioning and comparing to a set of reference captions [17, 18], while R-Precision [29, 43] measures text recall scores from a reference dataset of captions. CLIPScore [15], on the other hand, is reference-free, and produces an alignment measure based on the cosine similarity of the prompt and generated image CLIP [31] embeddings. This is shown to be more strongly correlated with human judgment than previous reference-based metrics that compare generated captions with a set of potential captions [15].\n\nHuman preference-based evaluation. Inspired by past work in NLP, several new methods evaluate generated images using models trained on direct human preference data [21, 41, 42]. This involves manual annotation of a large dataset of images \u2014 ranking groups of two or more images generated from the same prompt. A model is then trained on this dataset to predict a scalar score which correlates with human preference, offering a holistic evaluation of generated images. This serves to directly measure the endgoal of human preference, but combines multiple aspects of image generation, e.g., aesthetics, realism, and image-text alignment, into one number, whereas we aim to provide a breakdown of exact errors in each generated image.\n\nObject detection-based evaluation. A couple prior works also propose the use of object detection to evaluate T2I generation capabilities. Dall-Eval [9] trains a task-specific object detector for measuring each of several compositional reasoning tasks. They show that finetuning T2I models", "images": [{"name": "page-3-1.jpg", "height": 15, "width": 15, "x": 246, "y": 133}, {"name": "page-3-3.jpg", "height": 15, "width": 15, "x": 350, "y": 105}, {"name": "page-3-0.jpg", "height": 15, "width": 15, "x": 350, "y": 133}, {"name": "page-3-6.jpg", "height": 15, "width": 15, "x": 246, "y": 161}, {"name": "page-3-7.jpg", "height": 15, "width": 15, "x": 350, "y": 161}, {"name": "page-3-4.jpg", "height": 93, "width": 93, "x": 142, "y": 98}, {"name": "page-3-2.jpg", "height": 15, "width": 15, "x": 246, "y": 105}, {"name": "page-3-5.jpg", "height": 93, "width": 93, "x": 377, "y": 98}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "a photo of a wine glass\n\nright of a hot dog\n\nHuman gold label\n\nCLIPScore\n\n$$33.8 < 36.7$$\n\nGenEval (ours)\n\nFound hot dog &emsp; Found hot dog\n\nFound wine glass &emsp; Found wine glass\n\nFound wine glass right of hot dog &emsp; Expected wine glass right of hot dog, found above hot dog\n\n\u21d2 Correct &emsp; \u21d2 Incorrect\n\nFigure 2: Comparison between GENEVAL and CLIPScore. CLIPScore returns a scalar value indicating image-text alignment, whereas GENEVAL breaks the prompt down into correct and incorrect elements before producing a final binary score. Compared to CLIPScore, GENEVAL obtains higher agreement with human judgment on complex compositional tasks and find significant performance improvement in recent models, though there is still much room to improve on complex compositional tasks.", "md": "a photo of a wine glass\n\nright of a hot dog\n\nHuman gold label\n\nCLIPScore\n\n$$33.8 < 36.7$$\n\nGenEval (ours)\n\nFound hot dog &emsp; Found hot dog\n\nFound wine glass &emsp; Found wine glass\n\nFound wine glass right of hot dog &emsp; Expected wine glass right of hot dog, found above hot dog\n\n\u21d2 Correct &emsp; \u21d2 Incorrect\n\nFigure 2: Comparison between GENEVAL and CLIPScore. CLIPScore returns a scalar value indicating image-text alignment, whereas GENEVAL breaks the prompt down into correct and incorrect elements before producing a final binary score. Compared to CLIPScore, GENEVAL obtains higher agreement with human judgment on complex compositional tasks and find significant performance improvement in recent models, though there is still much room to improve on complex compositional tasks."}, {"type": "heading", "lvl": 2, "value": "Related work", "md": "## Related work"}, {"type": "text", "value": "Text-to-image models. While T2I models have been around since 2016 [25, 35], the field saw a surge in popularity starting with the autoregressive DALL-E in 2021 [33], followed by diffusion models like DALL-E 2 [34], Midjourney [2], GLIDE [27], and Stable Diffusion [36]. Subsequent improvements scaled the text encoder and made other architectural changes, such as in Imagen [37], IF [11], and Stable Diffusion XL [30]. Meanwhile, new GAN and autoregressive models are still being developed [20, 44]. However, most popularly used models and finetuned checkpoints for T2I are still based off of the Stable Diffusion architecture.\n\nAutomated evaluation. Automated T2I evaluation techniques primarily measure either image quality or image-text alignment. The Inception Score (IS) [38] and Frechet Inception Distance (FID) [16] metrics measure image quality independent of text. FID score estimates the distance between the distribution of generated images and a reference distribution of real-world images, and serves as a heuristic for how \u201crealistic\u201d the generated images are.\n\nOther metrics evaluate image-text alignment. Many of these are reference-based: BLEU [28] and CIDEr [40] can be used to evaluate generations by performing captioning and comparing to a set of reference captions [17, 18], while R-Precision [29, 43] measures text recall scores from a reference dataset of captions. CLIPScore [15], on the other hand, is reference-free, and produces an alignment measure based on the cosine similarity of the prompt and generated image CLIP [31] embeddings. This is shown to be more strongly correlated with human judgment than previous reference-based metrics that compare generated captions with a set of potential captions [15].\n\nHuman preference-based evaluation. Inspired by past work in NLP, several new methods evaluate generated images using models trained on direct human preference data [21, 41, 42]. This involves manual annotation of a large dataset of images \u2014 ranking groups of two or more images generated from the same prompt. A model is then trained on this dataset to predict a scalar score which correlates with human preference, offering a holistic evaluation of generated images. This serves to directly measure the endgoal of human preference, but combines multiple aspects of image generation, e.g., aesthetics, realism, and image-text alignment, into one number, whereas we aim to provide a breakdown of exact errors in each generated image.\n\nObject detection-based evaluation. A couple prior works also propose the use of object detection to evaluate T2I generation capabilities. Dall-Eval [9] trains a task-specific object detector for measuring each of several compositional reasoning tasks. They show that finetuning T2I models", "md": "Text-to-image models. While T2I models have been around since 2016 [25, 35], the field saw a surge in popularity starting with the autoregressive DALL-E in 2021 [33], followed by diffusion models like DALL-E 2 [34], Midjourney [2], GLIDE [27], and Stable Diffusion [36]. Subsequent improvements scaled the text encoder and made other architectural changes, such as in Imagen [37], IF [11], and Stable Diffusion XL [30]. Meanwhile, new GAN and autoregressive models are still being developed [20, 44]. However, most popularly used models and finetuned checkpoints for T2I are still based off of the Stable Diffusion architecture.\n\nAutomated evaluation. Automated T2I evaluation techniques primarily measure either image quality or image-text alignment. The Inception Score (IS) [38] and Frechet Inception Distance (FID) [16] metrics measure image quality independent of text. FID score estimates the distance between the distribution of generated images and a reference distribution of real-world images, and serves as a heuristic for how \u201crealistic\u201d the generated images are.\n\nOther metrics evaluate image-text alignment. Many of these are reference-based: BLEU [28] and CIDEr [40] can be used to evaluate generations by performing captioning and comparing to a set of reference captions [17, 18], while R-Precision [29, 43] measures text recall scores from a reference dataset of captions. CLIPScore [15], on the other hand, is reference-free, and produces an alignment measure based on the cosine similarity of the prompt and generated image CLIP [31] embeddings. This is shown to be more strongly correlated with human judgment than previous reference-based metrics that compare generated captions with a set of potential captions [15].\n\nHuman preference-based evaluation. Inspired by past work in NLP, several new methods evaluate generated images using models trained on direct human preference data [21, 41, 42]. This involves manual annotation of a large dataset of images \u2014 ranking groups of two or more images generated from the same prompt. A model is then trained on this dataset to predict a scalar score which correlates with human preference, offering a holistic evaluation of generated images. This serves to directly measure the endgoal of human preference, but combines multiple aspects of image generation, e.g., aesthetics, realism, and image-text alignment, into one number, whereas we aim to provide a breakdown of exact errors in each generated image.\n\nObject detection-based evaluation. A couple prior works also propose the use of object detection to evaluate T2I generation capabilities. Dall-Eval [9] trains a task-specific object detector for measuring each of several compositional reasoning tasks. They show that finetuning T2I models"}]}, {"page": 4, "text": "on images generated from a 3D simulator can increase model performance on these tasks to some\ndegree. In contrast, we find that with modern object detection models, training task-specific detectors\nis not necessary to obtain strong human agreement. This allows us to improve GENEVAL as better\nstate-of-the-art vision models become available without need for further finetuning or reliance on\ntraining sets of synthetic 3D-rendered images.\nMeanwhile, VISOR [14] is a different metric which allows a thorough evaluation of relative position\n(spatial reasoning) capabilities of T2I models. They perform a comprehensive evaluation over all\ntriplets of object pairs and spatial relations to provide in-depth analysis of models\u2019 spatial reasoning\ncapabilities. In contrast, we aim to cover a greater diversity of tasks, and show that object detector\noutputs can also be passed to downstream models that predict individual object properties such as\ncolor, expanding the scope of evaluation.\nVQA-based evaluation.          A concurrent work, TIFA [19], demonstrates the usefulness of large\nlanguage models (LLM) combined with visual question answering (VQA) models to perform fine-\ngrained T2I evaluation. They use an LLM to generate atomic verifi           cation questions from the text\nprompts, and apply a VQA model to answer the question given the generated image. This is a flexible\napproach which can cover a diversity of prompt types, depending on the training distribution of the\nLLM and VQA models. Similar recent works use LLMs to evaluate generated images, either by\npassing in a visual description [24] or using a multimodal vision-LLM (VLLM) to directly answer\nquestions about the image [6]. In comparison, we find that GENEVAL outputs and failure modes are\nmore easily interpretable, as the object detector and discriminative models produce detailed bounding\nbox and confidence scores on a per-object basis. Furthermore, each component of our framework can\nbe independently upgraded as better models are developed.\n3    GENEVAL: Our object-focused evaluation framework\n3.1    Setup\nIn order to produce a fine-grained verification of how well a generated image matches the description\nprovided in the text prompt, we break the prompt down into the types of objects, their properties\n(such as color and number), and their relations to other objects (such as relative position).\nText-to-image tasks.       We focus on 6 different tasks of varying difficulty requiring various compo-\nsitional skills, enumerated along with their prompts in Table 1. Here, we briefly summarize each\ntask.\n        \u2022 single object: rendering the specified type of object. This is the simplest task, and is\n          trivial for the modern T2I models we test.\n        \u2022 two object: rendering two different objects. This can be challenging in and of itself, as\n          our benchmark results show (Table 2), and also serves as a base for more complex tasks like\n          position and attribute binding.\n        \u2022 counting: rendering a specific number of one type of object. T2I models may struggle\n          with this task, even for a small number of objects.\n        \u2022 colors: rendering an object with a specific color. We show that this can be verified reliably\n          with a zero-shot image classifier by masking out the background.\n        \u2022 position: rendering two objects with specified positions relative to each other. Spatial\n          understanding has been found by prior works to be challenging for T2I models [14, 9]; our\n          findings show this is still true.\n        \u2022 attribute binding: rendering two different objects with two different colors. Prior\n          works qualitatively observe that binding attributes like color to their respective objects is\n          difficult for T2I models, often resulting in swapping (when the colors of two objects are\n          swapped) or leakage (when the color instead appears on background objects) [13].\nPrompt generation.        All our text prompts are generated from task-specific templates filled in with\nrandomly sampled object names, colors, numbers, and relative positions. Object names are drawn\nfrom the 80 MS COCO [23] class names, with some classes renamed to remove ambiguity, e.g.,\n                                                      4", "md": "# Object-focused Evaluation Framework\n\n## Object-focused Evaluation Framework\n\n### Setup\n\nIn order to produce a fine-grained verification of how well a generated image matches the description provided in the text prompt, we break the prompt down into the types of objects, their properties (such as color and number), and their relations to other objects (such as relative position).\n\n### Text-to-image tasks\n\nWe focus on 6 different tasks of varying difficulty requiring various compositional skills, enumerated along with their prompts in Table 1. Here, we briefly summarize each task.\n\n|Task|Prompt|\n|---|---|\n|Single object|Rendering the specified type of object. This is the simplest task, and is trivial for the modern T2I models we test.|\n|Two object|Rendering two different objects. This can be challenging in and of itself, as our benchmark results show (Table 2), and also serves as a base for more complex tasks like position and attribute binding.|\n|Counting|Rendering a specific number of one type of object. T2I models may struggle with this task, even for a small number of objects.|\n|Colors|Rendering an object with a specific color. We show that this can be verified reliably with a zero-shot image classifier by masking out the background.|\n|Position|Rendering two objects with specified positions relative to each other. Spatial understanding has been found by prior works to be challenging for T2I models [14, 9]; our findings show this is still true.|\n|Attribute binding|Rendering two different objects with two different colors. Prior works qualitatively observe that binding attributes like color to their respective objects is difficult for T2I models, often resulting in swapping (when the colors of two objects are swapped) or leakage (when the color instead appears on background objects) [13].|\n\nPrompt generation. All our text prompts are generated from task-specific templates filled in with randomly sampled object names, colors, numbers, and relative positions. Object names are drawn from the 80 MS COCO [23] class names, with some classes renamed to remove ambiguity, e.g., 4.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Object-focused Evaluation Framework", "md": "# Object-focused Evaluation Framework"}, {"type": "heading", "lvl": 2, "value": "Object-focused Evaluation Framework", "md": "## Object-focused Evaluation Framework"}, {"type": "heading", "lvl": 3, "value": "Setup", "md": "### Setup"}, {"type": "text", "value": "In order to produce a fine-grained verification of how well a generated image matches the description provided in the text prompt, we break the prompt down into the types of objects, their properties (such as color and number), and their relations to other objects (such as relative position).", "md": "In order to produce a fine-grained verification of how well a generated image matches the description provided in the text prompt, we break the prompt down into the types of objects, their properties (such as color and number), and their relations to other objects (such as relative position)."}, {"type": "heading", "lvl": 3, "value": "Text-to-image tasks", "md": "### Text-to-image tasks"}, {"type": "text", "value": "We focus on 6 different tasks of varying difficulty requiring various compositional skills, enumerated along with their prompts in Table 1. Here, we briefly summarize each task.", "md": "We focus on 6 different tasks of varying difficulty requiring various compositional skills, enumerated along with their prompts in Table 1. Here, we briefly summarize each task."}, {"type": "table", "rows": [["Task", "Prompt"], ["Single object", "Rendering the specified type of object. This is the simplest task, and is trivial for the modern T2I models we test."], ["Two object", "Rendering two different objects. This can be challenging in and of itself, as our benchmark results show (Table 2), and also serves as a base for more complex tasks like position and attribute binding."], ["Counting", "Rendering a specific number of one type of object. T2I models may struggle with this task, even for a small number of objects."], ["Colors", "Rendering an object with a specific color. We show that this can be verified reliably with a zero-shot image classifier by masking out the background."], ["Position", "Rendering two objects with specified positions relative to each other. Spatial understanding has been found by prior works to be challenging for T2I models [14, 9]; our findings show this is still true."], ["Attribute binding", "Rendering two different objects with two different colors. Prior works qualitatively observe that binding attributes like color to their respective objects is difficult for T2I models, often resulting in swapping (when the colors of two objects are swapped) or leakage (when the color instead appears on background objects) [13]."]], "md": "|Task|Prompt|\n|---|---|\n|Single object|Rendering the specified type of object. This is the simplest task, and is trivial for the modern T2I models we test.|\n|Two object|Rendering two different objects. This can be challenging in and of itself, as our benchmark results show (Table 2), and also serves as a base for more complex tasks like position and attribute binding.|\n|Counting|Rendering a specific number of one type of object. T2I models may struggle with this task, even for a small number of objects.|\n|Colors|Rendering an object with a specific color. We show that this can be verified reliably with a zero-shot image classifier by masking out the background.|\n|Position|Rendering two objects with specified positions relative to each other. Spatial understanding has been found by prior works to be challenging for T2I models [14, 9]; our findings show this is still true.|\n|Attribute binding|Rendering two different objects with two different colors. Prior works qualitatively observe that binding attributes like color to their respective objects is difficult for T2I models, often resulting in swapping (when the colors of two objects are swapped) or leakage (when the color instead appears on background objects) [13].|", "isPerfectTable": true, "csv": "\"Task\",\"Prompt\"\n\"Single object\",\"Rendering the specified type of object. This is the simplest task, and is trivial for the modern T2I models we test.\"\n\"Two object\",\"Rendering two different objects. This can be challenging in and of itself, as our benchmark results show (Table 2), and also serves as a base for more complex tasks like position and attribute binding.\"\n\"Counting\",\"Rendering a specific number of one type of object. T2I models may struggle with this task, even for a small number of objects.\"\n\"Colors\",\"Rendering an object with a specific color. We show that this can be verified reliably with a zero-shot image classifier by masking out the background.\"\n\"Position\",\"Rendering two objects with specified positions relative to each other. Spatial understanding has been found by prior works to be challenging for T2I models [14, 9]; our findings show this is still true.\"\n\"Attribute binding\",\"Rendering two different objects with two different colors. Prior works qualitatively observe that binding attributes like color to their respective objects is difficult for T2I models, often resulting in swapping (when the colors of two objects are swapped) or leakage (when the color instead appears on background objects) [13].\""}, {"type": "text", "value": "Prompt generation. All our text prompts are generated from task-specific templates filled in with randomly sampled object names, colors, numbers, and relative positions. Object names are drawn from the 80 MS COCO [23] class names, with some classes renamed to remove ambiguity, e.g., 4.", "md": "Prompt generation. All our text prompts are generated from task-specific templates filled in with randomly sampled object names, colors, numbers, and relative positions. Object names are drawn from the 80 MS COCO [23] class names, with some classes renamed to remove ambiguity, e.g., 4."}]}, {"page": 5, "text": "  Task                  Description                      # Prompts     Template\n  Single object         One object                           80        \u201ca photo of a/an [OBJECT]\u201d\n  Two object            Two different objects                99        \u201ca photo of a/an [OBJECT A] and a/an\n                                                                       [OBJECT B]\u201d\n  Counting              Specified number of an               80        \u201ca photo of [NUMBER] [OBJECT]s\u201d\n                        object\n  Colors                One object with a specified          94        \u201ca photo of a/an [COLOR] [OBJECT]\u201d\n                        color\n  Position              Two objects with specified          100        \u201ca photo of a/an [OBJECT A] [RELATIVE\n                        relative position                              POSITION] a/an [OBJECT B]\u201d\n  Attribute binding     Two objects with different          100        \u201ca photo of a/an [COLOR A] [OBJECT A]\n                        specified colors                               and a/an [COLOR B] [OBJECT B]\u201d\nTable 1: List of GENEVAL tasks. \u201cA/an\u201d in the templates are decided based on whether the following\nwords starts with a vowel.\n\u201cmouse\u201d to \u201ccomputer mouse\u201d. This choice is driven by the fact that most state-of-the-art object\ndetection models are trained on the MS COCO set of objects. Colors are taken from a list of 11 basic\ncolor terms from Berlin-Kay basic color theory [4]. For the counting task, the number is chosen to\nbe either 2, 3, or 4. For the position task, the relative position is one of \u201cabove\u201d, \u201cbelow\u201d, \u201cto the\nleft of\u201d, or \u201cto the right of\u201d.\n3.2    Evaluation framework\nObject detection.        For object detection and instance segmentation, we choose the best instance\nsegmentation model available from the MMDetection toolbox [5], a Mask2Former [7] trained on\nMS COCO. For each image, we extract all detected objects above the default confidence threshold\nof 0.3. We find that when multiple objects of the same type are in an image (specifically, for the\ncounting task), the detector tends to detect an excessive number of low confidence bounding boxes.\nTo alleviate this, we raise the minimum confidence threshold to 0.9 for the counting task only. We\nalso confirm that further pre-processing such as non-maximal suppression does not improve object\ndetector performance in these cases. Specific details on hyperparameter decisions are provided in\nAppendix C.\nFor all tasks, we generate an intermediate score for object presence, marking whether the desired\nobject types are present in the image. For the single object and two object task, this is also\nthe final score, since the prompt only asks for the specified objects to exist. For the counting task,\nwe further verify that the number of detected objects matches the number specified in the prompt. For\nthe position task, we use the 2D coordinates of the detected bounding box centroids to compute\nrelative position. We find that T2I models often generate overlapping objects; thus, we set a minimum\ndistance along each axis (proportional to the bounding box dimensions) after which the objects will\nbe classified as \u201cleft\u201d, \u201cright\u201d, \u201cabove\u201d, or \u201cbelow\u201d one another.\nColor classification.       For the colors and attribute binding tasks, we use the CLIP ViT-L/14\nmodel [31] to classify object color. For each object, using the information from the object detector,\nthe image is cropped to the bounding box of the object. The segmentation mask is used to replace the\nbackground pixels with a solid gray background. This cropped and processed image is then passed to\nthe CLIP model, which performs zero-shot classification between prompts of the form \u201ca photo of a\n[COLOR] [OBJECT]\u201d with all of the different candidate colors. We find that image cropping and\nbackground masking greatly improve the performance of the color classification model.\nScoring.      For each image, the GENEVAL score is a binary classification of image correctness:\nwhether all elements specified in the prompt were correctly rendered in the image. This score is\naveraged across all images generated for each task to obtain task-specific scores, and then averaged\nacross the six tasks to produce an overall score for a given T2I model. If an image is incorrect, the\nframework also produces a description of how the image deviates from the expectation: whether\nrequired objects are missing, or how the computed count, position, or color of objects differs from the\n                                                          5", "md": "|Task|Description|# Prompts|Template|\n|---|---|---|---|\n|Single object|One object|80|\u201ca photo of a/an [OBJECT]\u201d|\n|Two object|Two different objects|99|\u201ca photo of a/an [OBJECT A] and a/an [OBJECT B]\u201d|\n|Counting|Specified number of an object|80|\u201ca photo of [NUMBER] [OBJECT]s\u201d|\n|Colors|One object with a specified color|94|\u201ca photo of a/an [COLOR] [OBJECT]\u201d|\n|Position|Two objects with specified relative position|100|\u201ca photo of a/an [OBJECT A] [RELATIVE POSITION] a/an [OBJECT B]\u201d|\n|Attribute binding|Two objects with different specified colors|100|\u201ca photo of a/an [COLOR A] [OBJECT A] and a/an [COLOR B] [OBJECT B]\u201d|\n\nTable 1: List of GENEVAL tasks. \u201cA/an\u201d in the templates are decided based on whether the following words start with a vowel.\n\n\u201cmouse\u201d to \u201ccomputer mouse\u201d. This choice is driven by the fact that most state-of-the-art object detection models are trained on the MS COCO set of objects. Colors are taken from a list of 11 basic color terms from Berlin-Kay basic color theory [4]. For the counting task, the number is chosen to be either 2, 3, or 4. For the position task, the relative position is one of \u201cabove\u201d, \u201cbelow\u201d, \u201cto the left of\u201d, or \u201cto the right of\u201d.\n\n### Evaluation framework\n\nObject detection. For object detection and instance segmentation, we choose the best instance segmentation model available from the MMDetection toolbox [5], a Mask2Former [7] trained on MS COCO. For each image, we extract all detected objects above the default confidence threshold of 0.3. We find that when multiple objects of the same type are in an image (specifically, for the counting task), the detector tends to detect an excessive number of low confidence bounding boxes. To alleviate this, we raise the minimum confidence threshold to 0.9 for the counting task only. We also confirm that further pre-processing such as non-maximal suppression does not improve object detector performance in these cases. Specific details on hyperparameter decisions are provided in Appendix C.\n\nFor all tasks, we generate an intermediate score for object presence, marking whether the desired object types are present in the image. For the single object and two object task, this is also the final score, since the prompt only asks for the specified objects to exist. For the counting task, we further verify that the number of detected objects matches the number specified in the prompt. For the position task, we use the 2D coordinates of the detected bounding box centroids to compute relative position. We find that T2I models often generate overlapping objects; thus, we set a minimum distance along each axis (proportional to the bounding box dimensions) after which the objects will be classified as \u201cleft\u201d, \u201cright\u201d, \u201cabove\u201d, or \u201cbelow\u201d one another.\n\nColor classification. For the colors and attribute binding tasks, we use the CLIP ViT-L/14 model [31] to classify object color. For each object, using the information from the object detector, the image is cropped to the bounding box of the object. The segmentation mask is used to replace the background pixels with a solid gray background. This cropped and processed image is then passed to the CLIP model, which performs zero-shot classification between prompts of the form \u201ca photo of a [COLOR] [OBJECT]\u201d with all of the different candidate colors. We find that image cropping and background masking greatly improve the performance of the color classification model.\n\nScoring. For each image, the GENEVAL score is a binary classification of image correctness: whether all elements specified in the prompt were correctly rendered in the image. This score is averaged across all images generated for each task to obtain task-specific scores, and then averaged across the six tasks to produce an overall score for a given T2I model. If an image is incorrect, the framework also produces a description of how the image deviates from the expectation: whether required objects are missing, or how the computed count, position, or color of objects differs from the expectation.", "images": [], "items": [{"type": "table", "rows": [["Task", "Description", "# Prompts", "Template"], ["Single object", "One object", "80", "\u201ca photo of a/an [OBJECT]\u201d"], ["Two object", "Two different objects", "99", "\u201ca photo of a/an [OBJECT A] and a/an [OBJECT B]\u201d"], ["Counting", "Specified number of an object", "80", "\u201ca photo of [NUMBER] [OBJECT]s\u201d"], ["Colors", "One object with a specified color", "94", "\u201ca photo of a/an [COLOR] [OBJECT]\u201d"], ["Position", "Two objects with specified relative position", "100", "\u201ca photo of a/an [OBJECT A] [RELATIVE POSITION] a/an [OBJECT B]\u201d"], ["Attribute binding", "Two objects with different specified colors", "100", "\u201ca photo of a/an [COLOR A] [OBJECT A] and a/an [COLOR B] [OBJECT B]\u201d"]], "md": "|Task|Description|# Prompts|Template|\n|---|---|---|---|\n|Single object|One object|80|\u201ca photo of a/an [OBJECT]\u201d|\n|Two object|Two different objects|99|\u201ca photo of a/an [OBJECT A] and a/an [OBJECT B]\u201d|\n|Counting|Specified number of an object|80|\u201ca photo of [NUMBER] [OBJECT]s\u201d|\n|Colors|One object with a specified color|94|\u201ca photo of a/an [COLOR] [OBJECT]\u201d|\n|Position|Two objects with specified relative position|100|\u201ca photo of a/an [OBJECT A] [RELATIVE POSITION] a/an [OBJECT B]\u201d|\n|Attribute binding|Two objects with different specified colors|100|\u201ca photo of a/an [COLOR A] [OBJECT A] and a/an [COLOR B] [OBJECT B]\u201d|", "isPerfectTable": true, "csv": "\"Task\",\"Description\",\"# Prompts\",\"Template\"\n\"Single object\",\"One object\",\"80\",\"\u201ca photo of a/an [OBJECT]\u201d\"\n\"Two object\",\"Two different objects\",\"99\",\"\u201ca photo of a/an [OBJECT A] and a/an [OBJECT B]\u201d\"\n\"Counting\",\"Specified number of an object\",\"80\",\"\u201ca photo of [NUMBER] [OBJECT]s\u201d\"\n\"Colors\",\"One object with a specified color\",\"94\",\"\u201ca photo of a/an [COLOR] [OBJECT]\u201d\"\n\"Position\",\"Two objects with specified relative position\",\"100\",\"\u201ca photo of a/an [OBJECT A] [RELATIVE POSITION] a/an [OBJECT B]\u201d\"\n\"Attribute binding\",\"Two objects with different specified colors\",\"100\",\"\u201ca photo of a/an [COLOR A] [OBJECT A] and a/an [COLOR B] [OBJECT B]\u201d\""}, {"type": "text", "value": "Table 1: List of GENEVAL tasks. \u201cA/an\u201d in the templates are decided based on whether the following words start with a vowel.\n\n\u201cmouse\u201d to \u201ccomputer mouse\u201d. This choice is driven by the fact that most state-of-the-art object detection models are trained on the MS COCO set of objects. Colors are taken from a list of 11 basic color terms from Berlin-Kay basic color theory [4]. For the counting task, the number is chosen to be either 2, 3, or 4. For the position task, the relative position is one of \u201cabove\u201d, \u201cbelow\u201d, \u201cto the left of\u201d, or \u201cto the right of\u201d.", "md": "Table 1: List of GENEVAL tasks. \u201cA/an\u201d in the templates are decided based on whether the following words start with a vowel.\n\n\u201cmouse\u201d to \u201ccomputer mouse\u201d. This choice is driven by the fact that most state-of-the-art object detection models are trained on the MS COCO set of objects. Colors are taken from a list of 11 basic color terms from Berlin-Kay basic color theory [4]. For the counting task, the number is chosen to be either 2, 3, or 4. For the position task, the relative position is one of \u201cabove\u201d, \u201cbelow\u201d, \u201cto the left of\u201d, or \u201cto the right of\u201d."}, {"type": "heading", "lvl": 3, "value": "Evaluation framework", "md": "### Evaluation framework"}, {"type": "text", "value": "Object detection. For object detection and instance segmentation, we choose the best instance segmentation model available from the MMDetection toolbox [5], a Mask2Former [7] trained on MS COCO. For each image, we extract all detected objects above the default confidence threshold of 0.3. We find that when multiple objects of the same type are in an image (specifically, for the counting task), the detector tends to detect an excessive number of low confidence bounding boxes. To alleviate this, we raise the minimum confidence threshold to 0.9 for the counting task only. We also confirm that further pre-processing such as non-maximal suppression does not improve object detector performance in these cases. Specific details on hyperparameter decisions are provided in Appendix C.\n\nFor all tasks, we generate an intermediate score for object presence, marking whether the desired object types are present in the image. For the single object and two object task, this is also the final score, since the prompt only asks for the specified objects to exist. For the counting task, we further verify that the number of detected objects matches the number specified in the prompt. For the position task, we use the 2D coordinates of the detected bounding box centroids to compute relative position. We find that T2I models often generate overlapping objects; thus, we set a minimum distance along each axis (proportional to the bounding box dimensions) after which the objects will be classified as \u201cleft\u201d, \u201cright\u201d, \u201cabove\u201d, or \u201cbelow\u201d one another.\n\nColor classification. For the colors and attribute binding tasks, we use the CLIP ViT-L/14 model [31] to classify object color. For each object, using the information from the object detector, the image is cropped to the bounding box of the object. The segmentation mask is used to replace the background pixels with a solid gray background. This cropped and processed image is then passed to the CLIP model, which performs zero-shot classification between prompts of the form \u201ca photo of a [COLOR] [OBJECT]\u201d with all of the different candidate colors. We find that image cropping and background masking greatly improve the performance of the color classification model.\n\nScoring. For each image, the GENEVAL score is a binary classification of image correctness: whether all elements specified in the prompt were correctly rendered in the image. This score is averaged across all images generated for each task to obtain task-specific scores, and then averaged across the six tasks to produce an overall score for a given T2I model. If an image is incorrect, the framework also produces a description of how the image deviates from the expectation: whether required objects are missing, or how the computed count, position, or color of objects differs from the expectation.", "md": "Object detection. For object detection and instance segmentation, we choose the best instance segmentation model available from the MMDetection toolbox [5], a Mask2Former [7] trained on MS COCO. For each image, we extract all detected objects above the default confidence threshold of 0.3. We find that when multiple objects of the same type are in an image (specifically, for the counting task), the detector tends to detect an excessive number of low confidence bounding boxes. To alleviate this, we raise the minimum confidence threshold to 0.9 for the counting task only. We also confirm that further pre-processing such as non-maximal suppression does not improve object detector performance in these cases. Specific details on hyperparameter decisions are provided in Appendix C.\n\nFor all tasks, we generate an intermediate score for object presence, marking whether the desired object types are present in the image. For the single object and two object task, this is also the final score, since the prompt only asks for the specified objects to exist. For the counting task, we further verify that the number of detected objects matches the number specified in the prompt. For the position task, we use the 2D coordinates of the detected bounding box centroids to compute relative position. We find that T2I models often generate overlapping objects; thus, we set a minimum distance along each axis (proportional to the bounding box dimensions) after which the objects will be classified as \u201cleft\u201d, \u201cright\u201d, \u201cabove\u201d, or \u201cbelow\u201d one another.\n\nColor classification. For the colors and attribute binding tasks, we use the CLIP ViT-L/14 model [31] to classify object color. For each object, using the information from the object detector, the image is cropped to the bounding box of the object. The segmentation mask is used to replace the background pixels with a solid gray background. This cropped and processed image is then passed to the CLIP model, which performs zero-shot classification between prompts of the form \u201ca photo of a [COLOR] [OBJECT]\u201d with all of the different candidate colors. We find that image cropping and background masking greatly improve the performance of the color classification model.\n\nScoring. For each image, the GENEVAL score is a binary classification of image correctness: whether all elements specified in the prompt were correctly rendered in the image. This score is averaged across all images generated for each task to obtain task-specific scores, and then averaged across the six tasks to produce an overall score for a given T2I model. If an image is incorrect, the framework also produces a description of how the image deviates from the expectation: whether required objects are missing, or how the computed count, position, or color of objects differs from the expectation."}]}, {"page": 6, "text": "                                                          Annotator agreement\n                  1.0                                                                                  CLIPScore\n                                                                                                       GenEval\n                  0.9                                                                                  Interannotator\n                 Agreement\n                  0.8\n                  0.7\n                  0.6\n                  0.5       Overall       Single        Two        Counting      Colors       Position    Attribute\n                                          object       object                                              binding\nFigure 3: Human study agreement results. GENEVAL obtains higher agreement with human annota-\ntors on the more complex tasks (counting, position, and attribute binding) than thresholded\nCLIPScore, even when the CLIPScore threshold is tuned separately for each task. The difference is\nespecially significant for the position task.\nprompt. This can be helpful in understanding failures of both the generative T2I models (Figure 6)\nand the discriminative models used for evaluation (Figure 4).\n4     Measuring alignment with human judgment\nWe perform a human study to verify that the object detection-based GENEVAL aligns with human\nperception on machine-generated images. While the Mask2Former model attains high box AP\nand mask AP scores on the MS COCO validation set of real-world images [5], this needs to be\nconfirmed for AI-generated images on our distribution of prompts. We compare against interannotator\nagreement as well as the CLIPScore [15] evaluation metric, as it is also a reference-free evaluation\nmethod for judging image-text alignment. Since GENEVAL returns a binary correctness classification\nand CLIPScore returns a scalar cosine similarity score, we compare against human annotations by\nthresholding CLIPScore. For a fair comparison, we choose the best CLIPScore threshold for each\ntask separately.\nThe original CLIPScore metric from [15] uses the CLIP ViT-B/32 model from [31] to compute image\nand text embeddings. Since then, many improved CLIP models have been released from various\nsources. We test several of these models and opt to compare GENEVAL against CLIPScore with\nOpenCLIP ViT-H/14 [8], which shows the highest human agreement on our annotated samples. The\ncomparison of our tested CLIP models is enumerated in Table 5.\nFormat.         We conduct the study through Amazon Mechanical Turk, and gather 6,000 annotations\non a total of 1,200 images, with 400 images collected from each of Stable Diffusion v2.1, IF-XL,\nand LAION-5B with CLIP retrieval. All models and the CLIP baseline are listed in Section 5.1. For\neach image, annotators are asked to list all objects they can recognize. This helps ground following\nresponses, especially since AI-generated images may be diffi                         cult to parse. Then, for each type of\nobject in the prompt for that image, they are asked to mark how many of that object, what primary\ncolor(s), and how realistic the objects are in the image. If there are two objects in the prompt, they\nare also asked about the relative position between the two objects, both horizontally and vertically.\nFinally, they give an overall score (on a scale of 1\u20134) for how well the image matches the text prompt.\nA screenshot of the annotation interface and further details are provided in Appendix D.\nAnalysis.        The results of our human study are presented in Figure 3. Overall, we find that the object\ndetector and color classifier strongly correlate with human perception of the images. Across all\nimages, GENEVAL obtains 83% agreement with human annotators, where pairwise interannotator\nagreement is 88%. By comparison, CLIPScore obtains 80% overall agreement with human annotators.\nWhile a threshold-tuned CLIPScore has slightly higher agreement on the simpler single object\nand colors tasks, GENEVAL shows higher agreement on the other four, more complex tasks. This\n                                                                   6", "md": "# Annotator Agreement\n\n## Annotator agreement\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{1.0} & \\text{CLIPScore} \\\\\n\\text{0.9} & \\text{GenEval} \\\\\n\\text{0.8} & \\text{Interannotator Agreement} \\\\\n\\text{0.7} \\\\\n\\text{0.6} \\\\\n\\text{0.5} & \\text{Overall Single object Two object Counting Colors Position Attribute binding} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 3: Human study agreement results. GenEval obtains higher agreement with human annotators on the more complex tasks (counting, position, and attribute binding) than thresholded CLIPScore, even when the CLIPScore threshold is tuned separately for each task. The difference is especially significant for the position task.\n\n### Measuring alignment with human judgment\n\nWe perform a human study to verify that the object detection-based GenEval aligns with human perception on machine-generated images. While the Mask2Former model attains high box AP and mask AP scores on the MS COCO validation set of real-world images, this needs to be confirmed for AI-generated images on our distribution of prompts. We compare against interannotator agreement as well as the CLIPScore evaluation metric, as it is also a reference-free evaluation method for judging image-text alignment. Since GenEval returns a binary correctness classification and CLIPScore returns a scalar cosine similarity score, we compare against human annotations by thresholding CLIPScore. For a fair comparison, we choose the best CLIPScore threshold for each task separately.\n\nThe original CLIPScore metric uses the CLIP ViT-B/32 model to compute image and text embeddings. We test several improved CLIP models and opt to compare GenEval against CLIPScore with OpenCLIP ViT-H/14, which shows the highest human agreement on our annotated samples. The comparison of tested CLIP models is enumerated in Table 5.\n\n#### Format\n\nWe conduct the study through Amazon Mechanical Turk, and gather 6,000 annotations on a total of 1,200 images, with 400 images collected from each of Stable Diffusion v2.1, IF-XL, and LAION-5B with CLIP retrieval. All models and the CLIP baseline are listed in Section 5.1. For each image, annotators are asked to list all objects they can recognize. Then, for each type of object in the prompt for that image, they are asked to mark how many of that object, what primary color(s), and how realistic the objects are in the image. If there are two objects in the prompt, they are also asked about the relative position between the two objects, both horizontally and vertically. Finally, they give an overall score (on a scale of 1\u20134) for how well the image matches the text prompt.\n\nA screenshot of the annotation interface and further details are provided in Appendix D.\n\n#### Analysis\n\nThe results of our human study are presented in Figure 3. Overall, we find that the object detector and color classifier strongly correlate with human perception of the images. Across all images, GenEval obtains 83% agreement with human annotators, where pairwise interannotator agreement is 88%. By comparison, CLIPScore obtains 80% overall agreement with human annotators. While a threshold-tuned CLIPScore has slightly higher agreement on the simpler single object and colors tasks, GenEval shows higher agreement on the other four, more complex tasks.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Annotator Agreement", "md": "# Annotator Agreement"}, {"type": "heading", "lvl": 2, "value": "Annotator agreement", "md": "## Annotator agreement"}, {"type": "text", "value": "$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{1.0} & \\text{CLIPScore} \\\\\n\\text{0.9} & \\text{GenEval} \\\\\n\\text{0.8} & \\text{Interannotator Agreement} \\\\\n\\text{0.7} \\\\\n\\text{0.6} \\\\\n\\text{0.5} & \\text{Overall Single object Two object Counting Colors Position Attribute binding} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 3: Human study agreement results. GenEval obtains higher agreement with human annotators on the more complex tasks (counting, position, and attribute binding) than thresholded CLIPScore, even when the CLIPScore threshold is tuned separately for each task. The difference is especially significant for the position task.", "md": "$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{1.0} & \\text{CLIPScore} \\\\\n\\text{0.9} & \\text{GenEval} \\\\\n\\text{0.8} & \\text{Interannotator Agreement} \\\\\n\\text{0.7} \\\\\n\\text{0.6} \\\\\n\\text{0.5} & \\text{Overall Single object Two object Counting Colors Position Attribute binding} \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 3: Human study agreement results. GenEval obtains higher agreement with human annotators on the more complex tasks (counting, position, and attribute binding) than thresholded CLIPScore, even when the CLIPScore threshold is tuned separately for each task. The difference is especially significant for the position task."}, {"type": "heading", "lvl": 3, "value": "Measuring alignment with human judgment", "md": "### Measuring alignment with human judgment"}, {"type": "text", "value": "We perform a human study to verify that the object detection-based GenEval aligns with human perception on machine-generated images. While the Mask2Former model attains high box AP and mask AP scores on the MS COCO validation set of real-world images, this needs to be confirmed for AI-generated images on our distribution of prompts. We compare against interannotator agreement as well as the CLIPScore evaluation metric, as it is also a reference-free evaluation method for judging image-text alignment. Since GenEval returns a binary correctness classification and CLIPScore returns a scalar cosine similarity score, we compare against human annotations by thresholding CLIPScore. For a fair comparison, we choose the best CLIPScore threshold for each task separately.\n\nThe original CLIPScore metric uses the CLIP ViT-B/32 model to compute image and text embeddings. We test several improved CLIP models and opt to compare GenEval against CLIPScore with OpenCLIP ViT-H/14, which shows the highest human agreement on our annotated samples. The comparison of tested CLIP models is enumerated in Table 5.", "md": "We perform a human study to verify that the object detection-based GenEval aligns with human perception on machine-generated images. While the Mask2Former model attains high box AP and mask AP scores on the MS COCO validation set of real-world images, this needs to be confirmed for AI-generated images on our distribution of prompts. We compare against interannotator agreement as well as the CLIPScore evaluation metric, as it is also a reference-free evaluation method for judging image-text alignment. Since GenEval returns a binary correctness classification and CLIPScore returns a scalar cosine similarity score, we compare against human annotations by thresholding CLIPScore. For a fair comparison, we choose the best CLIPScore threshold for each task separately.\n\nThe original CLIPScore metric uses the CLIP ViT-B/32 model to compute image and text embeddings. We test several improved CLIP models and opt to compare GenEval against CLIPScore with OpenCLIP ViT-H/14, which shows the highest human agreement on our annotated samples. The comparison of tested CLIP models is enumerated in Table 5."}, {"type": "heading", "lvl": 4, "value": "Format", "md": "#### Format"}, {"type": "text", "value": "We conduct the study through Amazon Mechanical Turk, and gather 6,000 annotations on a total of 1,200 images, with 400 images collected from each of Stable Diffusion v2.1, IF-XL, and LAION-5B with CLIP retrieval. All models and the CLIP baseline are listed in Section 5.1. For each image, annotators are asked to list all objects they can recognize. Then, for each type of object in the prompt for that image, they are asked to mark how many of that object, what primary color(s), and how realistic the objects are in the image. If there are two objects in the prompt, they are also asked about the relative position between the two objects, both horizontally and vertically. Finally, they give an overall score (on a scale of 1\u20134) for how well the image matches the text prompt.\n\nA screenshot of the annotation interface and further details are provided in Appendix D.", "md": "We conduct the study through Amazon Mechanical Turk, and gather 6,000 annotations on a total of 1,200 images, with 400 images collected from each of Stable Diffusion v2.1, IF-XL, and LAION-5B with CLIP retrieval. All models and the CLIP baseline are listed in Section 5.1. For each image, annotators are asked to list all objects they can recognize. Then, for each type of object in the prompt for that image, they are asked to mark how many of that object, what primary color(s), and how realistic the objects are in the image. If there are two objects in the prompt, they are also asked about the relative position between the two objects, both horizontally and vertically. Finally, they give an overall score (on a scale of 1\u20134) for how well the image matches the text prompt.\n\nA screenshot of the annotation interface and further details are provided in Appendix D."}, {"type": "heading", "lvl": 4, "value": "Analysis", "md": "#### Analysis"}, {"type": "text", "value": "The results of our human study are presented in Figure 3. Overall, we find that the object detector and color classifier strongly correlate with human perception of the images. Across all images, GenEval obtains 83% agreement with human annotators, where pairwise interannotator agreement is 88%. By comparison, CLIPScore obtains 80% overall agreement with human annotators. While a threshold-tuned CLIPScore has slightly higher agreement on the simpler single object and colors tasks, GenEval shows higher agreement on the other four, more complex tasks.", "md": "The results of our human study are presented in Figure 3. Overall, we find that the object detector and color classifier strongly correlate with human perception of the images. Across all images, GenEval obtains 83% agreement with human annotators, where pairwise interannotator agreement is 88%. By comparison, CLIPScore obtains 80% overall agreement with human annotators. While a threshold-tuned CLIPScore has slightly higher agreement on the simpler single object and colors tasks, GenEval shows higher agreement on the other four, more complex tasks."}]}, {"page": 7, "text": "       Confusing background color                                   Merging objects                        Artistic renders\n          a photo of a red cake and a                             a photo of two bananas                  a photo of a blue cow\n                  purple chair\n             Stable Diffusion v2.1                                         IF-XL                              CLIP retrieval\n                Found red cake                                        Found banana                           Found no cows\n              Found purple chair                                      Found 1 banana                          \u21d2 Incorrect\n                   \u21d2 Correct                                           \u21d2 Incorrect\n Figure 4: Failure modes of GENEVAL. (Left) Holes in the object which are incorporated into the\n segmentation mask can mislead downstream color classification. (Center) Images with overlapping\n objects of the same type are difficult for object detectors. (Right) Simpler artistic renderings are\n out-of-distribution for the detector, which reduces classification accuracy.\n difference is especially pronounced in the counting task, where GENEVAL shows a 22 point\n improvement in human agreement over CLIPScore.\nWe also perform a qualitative analysis of the successes and failure modes of GENEVAL. An example\n comparison between GENEVAL and CLIPScore is shown in Figure 2. When comparing images\n generated by two different models from the same prompt, CLIPScore may assign significantly\n different scores to the images with no simple way to explain the scores. In contrast, GENEVAL\n outputs a sequence of verifications that explain why an image was marked correct or incorrect.\nThis also makes it easier to debug cases where GENEVAL does not match human judgment (Figure 4).\nThe color classifier can be confused by objects with holes, as the segmentation masks generated by\n our object detector may erroneously include these holes as part of the object. Other hard cases for the\n object detector include multiple overlapping objects of the same type, which may result in merged\n bounding boxes, and out-of-distribution images such as clip art. Despite these occasional failure\n modes, we find that GENEVAL aligns with human judgment significantly more than the CLIPScore\n baseline: on the 860 examples where human scores (across 5 annotators) are unanimous, GENEVAL\n obtains 91% overall agreement, while CLIPScore obtains 87% overall agreement.\n 5      Benchmarking progress in recent T2I models\n 5.1     Experiments\n Models. We evaluate a variety of open-source text-to-image models. This includes all versions\n of Stable Diffusion (SD) v1 and v2 [36], the recently released SD XL model, the IF pixel-space\n diffusion models from DeepFloyd [11], and the older model minDALL-E [22, 10], inspired by the\n original DALL-E model. In our primary results, we display only the latest or largest variant of each\n model, namely, IF-XL, SDv2.1, SDv1.5, and SD-XL 1.0. We also compare these models against\n a baseline of real images from LAION-5B [39], selected using CLIP ViT-L/14 image retrieval [3].\n Further details are provided in Appendix C.\n Image generation. We evaluate the models on a set of 553 prompts spanning all six tasks enumerated\n in Table 1. For each prompt, we generate 4 images, and the GENEVAL score is averaged over\n all generated images. This choice is motivated by current text-to-image APIs like DALL-E 2 [1]\n and Midjourney [2], which generate 4 images for a prompt to allow the user more choice. Other\n parameters, such as image resolution, sampling steps, and sampling method, are left at their default\nvalues for each model.\n                                                                             7", "md": "# Document\n\n## Confusing background color\n\nMerging objects\n\n- a photo of a red cake and a purple chair\n- a photo of two bananas\n- a photo of a blue cow\n\n## Stable Diffusion v2.1\n\nIF-XL\n\n- Found red cake\n- Found banana\n- Found purple chair\n\nCLIP retrieval\n\n- Found no cows\n- Found 1 banana\n\n\u21d2 Correct\n\n\u21d2 Incorrect\n\nFigure 4: Failure modes of GENEVAL. (Left) Holes in the object which are incorporated into the segmentation mask can mislead downstream color classification. (Center) Images with overlapping objects of the same type are difficult for object detectors. (Right) Simpler artistic renderings are out-of-distribution for the detector, which reduces classification accuracy.\n\nThe difference is especially pronounced in the counting task, where GENEVAL shows a 22 point improvement in human agreement over CLIPScore.\n\nWe also perform a qualitative analysis of the successes and failure modes of GENEVAL. An example comparison between GENEVAL and CLIPScore is shown in Figure 2. When comparing images generated by two different models from the same prompt, CLIPScore may assign significantly different scores to the images with no simple way to explain the scores. In contrast, GENEVAL outputs a sequence of verifications that explain why an image was marked correct or incorrect. This also makes it easier to debug cases where GENEVAL does not match human judgment (Figure 4).\n\nThe color classifier can be confused by objects with holes, as the segmentation masks generated by our object detector may erroneously include these holes as part of the object. Other hard cases for the object detector include multiple overlapping objects of the same type, which may result in merged bounding boxes, and out-of-distribution images such as clip art. Despite these occasional failure modes, we find that GENEVAL aligns with human judgment significantly more than the CLIPScore baseline: on the 860 examples where human scores (across 5 annotators) are unanimous, GENEVAL obtains 91% overall agreement, while CLIPScore obtains 87% overall agreement.\n\n### Benchmarking progress in recent T2I models\n\n#### Experiments\n\nModels. We evaluate a variety of open-source text-to-image models. This includes all versions of Stable Diffusion (SD) v1 and v2 [36], the recently released SD XL model, the IF pixel-space diffusion models from DeepFloyd [11], and the older model minDALL-E [22, 10], inspired by the original DALL-E model. In our primary results, we display only the latest or largest variant of each model, namely, IF-XL, SDv2.1, SDv1.5, and SD-XL 1.0. We also compare these models against a baseline of real images from LAION-5B [39], selected using CLIP ViT-L/14 image retrieval [3]. Further details are provided in Appendix C.\n\nImage generation. We evaluate the models on a set of 553 prompts spanning all six tasks enumerated in Table 1. For each prompt, we generate 4 images, and the GENEVAL score is averaged over all generated images. This choice is motivated by current text-to-image APIs like DALL-E 2 [1] and Midjourney [2], which generate 4 images for a prompt to allow the user more choice. Other parameters, such as image resolution, sampling steps, and sampling method, are left at their default values for each model.", "images": [{"name": "page-7-0.jpg", "height": 91, "width": 91, "x": 124, "y": 129}, {"name": "page-7-2.jpg", "height": 82, "width": 122, "x": 379, "y": 132}, {"name": "page-7-1.jpg", "height": 91, "width": 91, "x": 261, "y": 129}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Confusing background color", "md": "## Confusing background color"}, {"type": "text", "value": "Merging objects\n\n- a photo of a red cake and a purple chair\n- a photo of two bananas\n- a photo of a blue cow", "md": "Merging objects\n\n- a photo of a red cake and a purple chair\n- a photo of two bananas\n- a photo of a blue cow"}, {"type": "heading", "lvl": 2, "value": "Stable Diffusion v2.1", "md": "## Stable Diffusion v2.1"}, {"type": "text", "value": "IF-XL\n\n- Found red cake\n- Found banana\n- Found purple chair\n\nCLIP retrieval\n\n- Found no cows\n- Found 1 banana\n\n\u21d2 Correct\n\n\u21d2 Incorrect\n\nFigure 4: Failure modes of GENEVAL. (Left) Holes in the object which are incorporated into the segmentation mask can mislead downstream color classification. (Center) Images with overlapping objects of the same type are difficult for object detectors. (Right) Simpler artistic renderings are out-of-distribution for the detector, which reduces classification accuracy.\n\nThe difference is especially pronounced in the counting task, where GENEVAL shows a 22 point improvement in human agreement over CLIPScore.\n\nWe also perform a qualitative analysis of the successes and failure modes of GENEVAL. An example comparison between GENEVAL and CLIPScore is shown in Figure 2. When comparing images generated by two different models from the same prompt, CLIPScore may assign significantly different scores to the images with no simple way to explain the scores. In contrast, GENEVAL outputs a sequence of verifications that explain why an image was marked correct or incorrect. This also makes it easier to debug cases where GENEVAL does not match human judgment (Figure 4).\n\nThe color classifier can be confused by objects with holes, as the segmentation masks generated by our object detector may erroneously include these holes as part of the object. Other hard cases for the object detector include multiple overlapping objects of the same type, which may result in merged bounding boxes, and out-of-distribution images such as clip art. Despite these occasional failure modes, we find that GENEVAL aligns with human judgment significantly more than the CLIPScore baseline: on the 860 examples where human scores (across 5 annotators) are unanimous, GENEVAL obtains 91% overall agreement, while CLIPScore obtains 87% overall agreement.", "md": "IF-XL\n\n- Found red cake\n- Found banana\n- Found purple chair\n\nCLIP retrieval\n\n- Found no cows\n- Found 1 banana\n\n\u21d2 Correct\n\n\u21d2 Incorrect\n\nFigure 4: Failure modes of GENEVAL. (Left) Holes in the object which are incorporated into the segmentation mask can mislead downstream color classification. (Center) Images with overlapping objects of the same type are difficult for object detectors. (Right) Simpler artistic renderings are out-of-distribution for the detector, which reduces classification accuracy.\n\nThe difference is especially pronounced in the counting task, where GENEVAL shows a 22 point improvement in human agreement over CLIPScore.\n\nWe also perform a qualitative analysis of the successes and failure modes of GENEVAL. An example comparison between GENEVAL and CLIPScore is shown in Figure 2. When comparing images generated by two different models from the same prompt, CLIPScore may assign significantly different scores to the images with no simple way to explain the scores. In contrast, GENEVAL outputs a sequence of verifications that explain why an image was marked correct or incorrect. This also makes it easier to debug cases where GENEVAL does not match human judgment (Figure 4).\n\nThe color classifier can be confused by objects with holes, as the segmentation masks generated by our object detector may erroneously include these holes as part of the object. Other hard cases for the object detector include multiple overlapping objects of the same type, which may result in merged bounding boxes, and out-of-distribution images such as clip art. Despite these occasional failure modes, we find that GENEVAL aligns with human judgment significantly more than the CLIPScore baseline: on the 860 examples where human scores (across 5 annotators) are unanimous, GENEVAL obtains 91% overall agreement, while CLIPScore obtains 87% overall agreement."}, {"type": "heading", "lvl": 3, "value": "Benchmarking progress in recent T2I models", "md": "### Benchmarking progress in recent T2I models"}, {"type": "heading", "lvl": 4, "value": "Experiments", "md": "#### Experiments"}, {"type": "text", "value": "Models. We evaluate a variety of open-source text-to-image models. This includes all versions of Stable Diffusion (SD) v1 and v2 [36], the recently released SD XL model, the IF pixel-space diffusion models from DeepFloyd [11], and the older model minDALL-E [22, 10], inspired by the original DALL-E model. In our primary results, we display only the latest or largest variant of each model, namely, IF-XL, SDv2.1, SDv1.5, and SD-XL 1.0. We also compare these models against a baseline of real images from LAION-5B [39], selected using CLIP ViT-L/14 image retrieval [3]. Further details are provided in Appendix C.\n\nImage generation. We evaluate the models on a set of 553 prompts spanning all six tasks enumerated in Table 1. For each prompt, we generate 4 images, and the GENEVAL score is averaged over all generated images. This choice is motivated by current text-to-image APIs like DALL-E 2 [1] and Midjourney [2], which generate 4 images for a prompt to allow the user more choice. Other parameters, such as image resolution, sampling steps, and sampling method, are left at their default values for each model.", "md": "Models. We evaluate a variety of open-source text-to-image models. This includes all versions of Stable Diffusion (SD) v1 and v2 [36], the recently released SD XL model, the IF pixel-space diffusion models from DeepFloyd [11], and the older model minDALL-E [22, 10], inspired by the original DALL-E model. In our primary results, we display only the latest or largest variant of each model, namely, IF-XL, SDv2.1, SDv1.5, and SD-XL 1.0. We also compare these models against a baseline of real images from LAION-5B [39], selected using CLIP ViT-L/14 image retrieval [3]. Further details are provided in Appendix C.\n\nImage generation. We evaluate the models on a set of 553 prompts spanning all six tasks enumerated in Table 1. For each prompt, we generate 4 images, and the GENEVAL score is averaged over all generated images. This choice is motivated by current text-to-image APIs like DALL-E 2 [1] and Midjourney [2], which generate 4 images for a prompt to allow the user more choice. Other parameters, such as image resolution, sampling steps, and sampling method, are left at their default values for each model."}]}, {"page": 8, "text": "                                 Single        Two                                                      Attribute\n          Model                  object       object       Counting         Colors       Position        binding         Overall        CLIPScore       Human\n          CLIP retrieval          0.89         0.22           0.37           0.62          0.03            0.00            0.35             27.8          0.42\n          minDALL-E               0.73         0.11           0.12           0.37          0.02            0.01            0.23             27.3           \u2014\n          SDv1.5                  0.97         0.38           0.35           0.76          0.04            0.06            0.43             33.5           \u2014\n          SDv2.1                  0.98         0.51           0.44           0.85          0.07            0.17            0.50             36.2          0.57\n          SD-XL                   0.98         0.74           0.39           0.85          0.15            0.23            0.55             36.7           \u2014\n          IF-XL                   0.97         0.74           0.66           0.81          0.13            0.35            0.61             36.5          0.72\nTable 2: GENEVAL scores over the main T2I models and baseline. The recent models, IF-XL and\nSD-XL, display a significant improvement over previous models on challenging tasks. However,\ntasks like counting, position, and attribute binding still show much room for improvement.\nOverall scores vary by 0.01\u20130.02 across random seeds. The relative ordering of the evaluated T2I\nmodels is consistent with CLIPScore and human annotators, except with SD-XL and IF-XL, which\nCLIPScore cannot distinguish between.\n5.2       Results\nOverall rankings. Table 2 reports the results of evaluating the T2I models and CLIP baseline on the\nsix GENEVAL tasks. Of these models, IF-XL has the best overall performance, followed by SD-XL.\nWhile the single object and colors tasks are relatively easy for all models (as expected from\nthe complexity of the task), there is a significant performance gap between models on the other four\ntasks. In particular, two object performance has risen in the recent IF-XL and SD-XL models,\nwhile position and attribute binding tasks remain difficult overall. Unsurprisingly, the oldest\nmodel, minDALL-E, performs worst overall. Interestingly, the CLIP retrieval baseline surpasses\nminDALL-E and even SDv1.5 on the counting task, but otherwise performs poorly. This is to be\nexpected, as many object pairings and colorings, e.g. \u201ca photo of a blue cow\u201d, would rarely appear in\na real-world dataset like LAION-5B.\nEffects of model scale.                        Figure 5 compares the GENEVAL performance across the three different\nscales of the DeepFloyd IF models: IF-M, IF-L, and IF-XL. All three models utilize the same\nT5-XXL text encoder [32], with increasingly sized text-to-image and image upscaling modules. We\nobserve that on the two object, counting, and especially the attribute binding tasks, we see\nconsistent improvement as model size increases. However, we do not see such an improvement on\nthe position task. This suggests that certain T2I capabilities may not be improved by vision model\nscale only; different training data or a better text encoder may be required.\nEffects of increased pretraining.                                 SDv1 is available in fi                  ve checkpoints, corresponding to con-\ntinued training on increasing amounts of data from the LAION-5B dataset. We evaluate all fi                                                                     ve\nmodel checkpoints and report the results in Figure 5. Surprisingly, there is little increase in model\n                    1.0             Performance across model scale                                       0.7     Stable Diffusion performance\n                                                                                        IF-M                           SDv1\n                    0.8                                                                 IF-L             0.6           SDv2\n                                                                                        IF-XL          Overall Score\n                    0.6                                                                                  0.5\n                  Score\n                    0.4                                                                                  0.4\n                    0.2                                                                                  0.3\n                    0.0      Overall   Single     Two   Counting   Colors   Position Attribute           0.2   1.1    1.2    1.3     1.4    1.5    2.0  2.1\n                                       object   object                                binding                           Stable Diffusion version\nFigure 5: (Left) Change in model performance IF model scales. Overall, GENEVAL score increases\nwith model size, though this does not appear to be the case for the position task. (Right) Change in\nmodel performance over successive iterations of Stable Diffusion. Despite each subsequent version\nhaving been trained for more iterations, there is no consistent performance increase from v1.1 to v1.5.\nIn contrast, v2, which uses a different text encoder, significantly improves performance.\n                                                                                      8", "md": "# Results\n\n## Table 2: GENEVAL scores over the main T2I models and baseline\n\n|Model|Single object|Two object|Counting|Colors|Position|Attribute binding|Overall|CLIPScore|Human|\n|---|---|---|---|---|---|---|---|---|---|\n|CLIP retrieval|0.89|0.22|0.37|0.62|0.03|0.00|0.35|27.8|0.42|\n|minDALL-E|0.73|0.11|0.12|0.37|0.02|0.01|0.23|27.3|\u2014|\n|SDv1.5|0.97|0.38|0.35|0.76|0.04|0.06|0.43|33.5|\u2014|\n|SDv2.1|0.98|0.51|0.44|0.85|0.07|0.17|0.50|36.2|0.57|\n|SD-XL|0.98|0.74|0.39|0.85|0.15|0.23|0.55|36.7|\u2014|\n|IF-XL|0.97|0.74|0.66|0.81|0.13|0.35|0.61|36.5|0.72|\n\n## Effects of model scale\n\nFigure 5 compares the GENEVAL performance across the three different scales of the DeepFloyd IF models: IF-M, IF-L, and IF-XL. All three models utilize the same T5-XXL text encoder [32], with increasingly sized text-to-image and image upscaling modules. We observe that on the two object, counting, and especially the attribute binding tasks, we see consistent improvement as model size increases. However, we do not see such an improvement on the position task. This suggests that certain T2I capabilities may not be improved by vision model scale only; different training data or a better text encoder may be required.\n\n## Effects of increased pretraining\n\nSDv1 is available in five checkpoints, corresponding to continued training on increasing amounts of data from the LAION-5B dataset. We evaluate all five model checkpoints and report the results in Figure 5. Surprisingly, there is little increase in model performance across model scale. Overall, GENEVAL score increases with model size, though this does not appear to be the case for the position task. Despite each subsequent version having been trained for more iterations, there is no consistent performance increase from v1.1 to v1.5. In contrast, v2, which uses a different text encoder, significantly improves performance.\n\n## Figure 5", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Results", "md": "# Results"}, {"type": "heading", "lvl": 2, "value": "Table 2: GENEVAL scores over the main T2I models and baseline", "md": "## Table 2: GENEVAL scores over the main T2I models and baseline"}, {"type": "table", "rows": [["Model", "Single object", "Two object", "Counting", "Colors", "Position", "Attribute binding", "Overall", "CLIPScore", "Human"], ["CLIP retrieval", "0.89", "0.22", "0.37", "0.62", "0.03", "0.00", "0.35", "27.8", "0.42"], ["minDALL-E", "0.73", "0.11", "0.12", "0.37", "0.02", "0.01", "0.23", "27.3", "\u2014"], ["SDv1.5", "0.97", "0.38", "0.35", "0.76", "0.04", "0.06", "0.43", "33.5", "\u2014"], ["SDv2.1", "0.98", "0.51", "0.44", "0.85", "0.07", "0.17", "0.50", "36.2", "0.57"], ["SD-XL", "0.98", "0.74", "0.39", "0.85", "0.15", "0.23", "0.55", "36.7", "\u2014"], ["IF-XL", "0.97", "0.74", "0.66", "0.81", "0.13", "0.35", "0.61", "36.5", "0.72"]], "md": "|Model|Single object|Two object|Counting|Colors|Position|Attribute binding|Overall|CLIPScore|Human|\n|---|---|---|---|---|---|---|---|---|---|\n|CLIP retrieval|0.89|0.22|0.37|0.62|0.03|0.00|0.35|27.8|0.42|\n|minDALL-E|0.73|0.11|0.12|0.37|0.02|0.01|0.23|27.3|\u2014|\n|SDv1.5|0.97|0.38|0.35|0.76|0.04|0.06|0.43|33.5|\u2014|\n|SDv2.1|0.98|0.51|0.44|0.85|0.07|0.17|0.50|36.2|0.57|\n|SD-XL|0.98|0.74|0.39|0.85|0.15|0.23|0.55|36.7|\u2014|\n|IF-XL|0.97|0.74|0.66|0.81|0.13|0.35|0.61|36.5|0.72|", "isPerfectTable": true, "csv": "\"Model\",\"Single object\",\"Two object\",\"Counting\",\"Colors\",\"Position\",\"Attribute binding\",\"Overall\",\"CLIPScore\",\"Human\"\n\"CLIP retrieval\",\"0.89\",\"0.22\",\"0.37\",\"0.62\",\"0.03\",\"0.00\",\"0.35\",\"27.8\",\"0.42\"\n\"minDALL-E\",\"0.73\",\"0.11\",\"0.12\",\"0.37\",\"0.02\",\"0.01\",\"0.23\",\"27.3\",\"\u2014\"\n\"SDv1.5\",\"0.97\",\"0.38\",\"0.35\",\"0.76\",\"0.04\",\"0.06\",\"0.43\",\"33.5\",\"\u2014\"\n\"SDv2.1\",\"0.98\",\"0.51\",\"0.44\",\"0.85\",\"0.07\",\"0.17\",\"0.50\",\"36.2\",\"0.57\"\n\"SD-XL\",\"0.98\",\"0.74\",\"0.39\",\"0.85\",\"0.15\",\"0.23\",\"0.55\",\"36.7\",\"\u2014\"\n\"IF-XL\",\"0.97\",\"0.74\",\"0.66\",\"0.81\",\"0.13\",\"0.35\",\"0.61\",\"36.5\",\"0.72\""}, {"type": "heading", "lvl": 2, "value": "Effects of model scale", "md": "## Effects of model scale"}, {"type": "text", "value": "Figure 5 compares the GENEVAL performance across the three different scales of the DeepFloyd IF models: IF-M, IF-L, and IF-XL. All three models utilize the same T5-XXL text encoder [32], with increasingly sized text-to-image and image upscaling modules. We observe that on the two object, counting, and especially the attribute binding tasks, we see consistent improvement as model size increases. However, we do not see such an improvement on the position task. This suggests that certain T2I capabilities may not be improved by vision model scale only; different training data or a better text encoder may be required.", "md": "Figure 5 compares the GENEVAL performance across the three different scales of the DeepFloyd IF models: IF-M, IF-L, and IF-XL. All three models utilize the same T5-XXL text encoder [32], with increasingly sized text-to-image and image upscaling modules. We observe that on the two object, counting, and especially the attribute binding tasks, we see consistent improvement as model size increases. However, we do not see such an improvement on the position task. This suggests that certain T2I capabilities may not be improved by vision model scale only; different training data or a better text encoder may be required."}, {"type": "heading", "lvl": 2, "value": "Effects of increased pretraining", "md": "## Effects of increased pretraining"}, {"type": "text", "value": "SDv1 is available in five checkpoints, corresponding to continued training on increasing amounts of data from the LAION-5B dataset. We evaluate all five model checkpoints and report the results in Figure 5. Surprisingly, there is little increase in model performance across model scale. Overall, GENEVAL score increases with model size, though this does not appear to be the case for the position task. Despite each subsequent version having been trained for more iterations, there is no consistent performance increase from v1.1 to v1.5. In contrast, v2, which uses a different text encoder, significantly improves performance.", "md": "SDv1 is available in five checkpoints, corresponding to continued training on increasing amounts of data from the LAION-5B dataset. We evaluate all five model checkpoints and report the results in Figure 5. Surprisingly, there is little increase in model performance across model scale. Overall, GENEVAL score increases with model size, though this does not appear to be the case for the position task. Despite each subsequent version having been trained for more iterations, there is no consistent performance increase from v1.1 to v1.5. In contrast, v2, which uses a different text encoder, significantly improves performance."}, {"type": "heading", "lvl": 2, "value": "Figure 5", "md": "## Figure 5"}]}, {"page": 9, "text": "                       0.3              IF-XL Position outcomes                                  0.5           Attribute binding outcomes  SDv2.1\n                                                                                                 0.4                                       IF-XL\n                      Frequency                                                                 Frequency\n                       0.2                                                                       0.3\n                       0.1                                                                       0.2\n                                                                                                 0.1\n                       0.0       Left           Right           Above           Below            0.0         Correct            Swapped    Other\n                                                     Direction                                                                   Colors\nFigure 6: Failure modes of T2I models. (Left) IF-XL displays a position bias where the first\nmentioned object is more likely to be on the left than the right of the second object. (Right) SDv2.1 is\nprone to swap the colors of two objects, failing to correctly bind attributes to their respective objects.\nperformance with increased pretraining steps. While there appears to be a noticeable step up from\nv1.1 to v1.2, there is no further increase in overall scores after that. However, SDv2 does show\nsignificant improvement over SDv1. This may be explained by the change in text encoder: SDv2\nuses OpenCLIP ViT-H/14 [8], which is larger and trained on different data compared to the CLIP\nViT-L/14 [31] used by SDv1. SD-XL also increases scores over SDv2, though there are numerous\nqualitative differences in architecture and training that may contribute to this improvement.\nUnderstanding T2I model failure cases.                                           Our GENEVAL framework produces fine-grained output\nwhich facilitates quantitative analysis of when and how T2I models fail. We describe two interesting\npatterns in Figure 6. On the position task, it appears that when IF-XL is able to generate both\nobjects, it is biased towards placing the first object to the left of the second object, and biased against\nplacing it to the right, even when the prompts are evenly distributed among all four directions. On\nthe attribute binding task, we find that Stable Diffusion v2.1 is significantly more susceptible\nto color swapping (applying one of the specified colors to the wrong object) as compared to IF-XL.\nThese examples showcase how GENEVAL can be used to analyze flaws and biases in T2I generations,\nsuggesting avenues for future improvement in image generation. Qualitative examples of these\nfindings are shown in Appendix A.\n6       Limitations\nGENEVAL is primarily limited by the performance of the object detector used and the availability of\ndiscriminative vision models. We note that the best object detection algorithms are still trained or\nfinetuned on the MS-COCO dataset, which has a limited number of classes defined at a particular\nlevel of granularity. This means that while, for example, GENEVAL can verify the number of people\npresent in a generated image, it cannot verify the number of fingers on each person\u2019s hands. Similarly,\nas noted in Figure 4, object detectors trained primarily on photos do not generalize well to visually\ndistinct art. These constraints may be removed in the future with the development of more powerful\nopen-vocabulary object detectors trained on a wider distribution of images [26].\n7       Conclusion\nWe introduce GENEVAL, a new object-focused framework for automated evaluation of text-to-image\nmodels. GENEVAL evaluates T2I model capabilities across a suite of compositional reasoning tasks\nusing object detection and color classification to verify fine-grained object properties. We perform\na human study and find that GENEVAL scores align strongly with human judgment on an instance\nlevel, beating out prior approaches measuring overall image-text alignment. We then benchmark\nopen-source T2I models and find that while complex compositional tasks like relative position and\nattribute binding are difficult for current T2I models, GENEVAL can help identify failure modes to\nfacilitate future improvement. We hope to expand GENEVAL in future work to take advantage of the\nwide variety of discriminative vision models that have been developed, to produce an even broader\nhighly interpretable evaluation framework for text-to-image generation.\n                                                                                          9", "md": "# Text-to-Image Evaluation Framework\n\n## Failure modes of T2I models\n\n(Left) IF-XL displays a position bias where the first mentioned object is more likely to be on the left than the right of the second object. (Right) SDv2.1 is prone to swap the colors of two objects, failing to correctly bind attributes to their respective objects.\n\n### Performance Comparison\n\nThere is a noticeable step up in performance with increased pretraining steps from v1.1 to v1.2. However, there is no further increase in overall scores after that. SDv2 shows significant improvement over SDv1, possibly due to the change in text encoder.\n\n### Understanding T2I model failure cases\n\nOur GENEVAL framework produces fine-grained output which facilitates quantitative analysis of when and how T2I models fail. Two interesting patterns are described in Figure 6.\n\n#### Position Task\n\nIF-XL is biased towards placing the first object to the left of the second object, even when prompts are evenly distributed among all four directions.\n\n#### Attribute Binding Task\n\nStable Diffusion v2.1 is significantly more susceptible to color swapping compared to IF-XL, applying one of the specified colors to the wrong object.\n\n### Limitations\n\nGENEVAL is primarily limited by the performance of the object detector used and the availability of discriminative vision models. Object detectors trained on limited datasets may not generalize well to all scenarios.\n\n### Conclusion\n\nGENEVAL is a new object-focused framework for automated evaluation of text-to-image models. It evaluates T2I model capabilities across compositional reasoning tasks using object detection and color classification. GENEVAL scores align strongly with human judgment, providing insights into failure modes for future improvement.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Text-to-Image Evaluation Framework", "md": "# Text-to-Image Evaluation Framework"}, {"type": "heading", "lvl": 2, "value": "Failure modes of T2I models", "md": "## Failure modes of T2I models"}, {"type": "text", "value": "(Left) IF-XL displays a position bias where the first mentioned object is more likely to be on the left than the right of the second object. (Right) SDv2.1 is prone to swap the colors of two objects, failing to correctly bind attributes to their respective objects.", "md": "(Left) IF-XL displays a position bias where the first mentioned object is more likely to be on the left than the right of the second object. (Right) SDv2.1 is prone to swap the colors of two objects, failing to correctly bind attributes to their respective objects."}, {"type": "heading", "lvl": 3, "value": "Performance Comparison", "md": "### Performance Comparison"}, {"type": "text", "value": "There is a noticeable step up in performance with increased pretraining steps from v1.1 to v1.2. However, there is no further increase in overall scores after that. SDv2 shows significant improvement over SDv1, possibly due to the change in text encoder.", "md": "There is a noticeable step up in performance with increased pretraining steps from v1.1 to v1.2. However, there is no further increase in overall scores after that. SDv2 shows significant improvement over SDv1, possibly due to the change in text encoder."}, {"type": "heading", "lvl": 3, "value": "Understanding T2I model failure cases", "md": "### Understanding T2I model failure cases"}, {"type": "text", "value": "Our GENEVAL framework produces fine-grained output which facilitates quantitative analysis of when and how T2I models fail. Two interesting patterns are described in Figure 6.", "md": "Our GENEVAL framework produces fine-grained output which facilitates quantitative analysis of when and how T2I models fail. Two interesting patterns are described in Figure 6."}, {"type": "heading", "lvl": 4, "value": "Position Task", "md": "#### Position Task"}, {"type": "text", "value": "IF-XL is biased towards placing the first object to the left of the second object, even when prompts are evenly distributed among all four directions.", "md": "IF-XL is biased towards placing the first object to the left of the second object, even when prompts are evenly distributed among all four directions."}, {"type": "heading", "lvl": 4, "value": "Attribute Binding Task", "md": "#### Attribute Binding Task"}, {"type": "text", "value": "Stable Diffusion v2.1 is significantly more susceptible to color swapping compared to IF-XL, applying one of the specified colors to the wrong object.", "md": "Stable Diffusion v2.1 is significantly more susceptible to color swapping compared to IF-XL, applying one of the specified colors to the wrong object."}, {"type": "heading", "lvl": 3, "value": "Limitations", "md": "### Limitations"}, {"type": "text", "value": "GENEVAL is primarily limited by the performance of the object detector used and the availability of discriminative vision models. Object detectors trained on limited datasets may not generalize well to all scenarios.", "md": "GENEVAL is primarily limited by the performance of the object detector used and the availability of discriminative vision models. Object detectors trained on limited datasets may not generalize well to all scenarios."}, {"type": "heading", "lvl": 3, "value": "Conclusion", "md": "### Conclusion"}, {"type": "text", "value": "GENEVAL is a new object-focused framework for automated evaluation of text-to-image models. It evaluates T2I model capabilities across compositional reasoning tasks using object detection and color classification. GENEVAL scores align strongly with human judgment, providing insights into failure modes for future improvement.", "md": "GENEVAL is a new object-focused framework for automated evaluation of text-to-image models. It evaluates T2I model capabilities across compositional reasoning tasks using object detection and color classification. GENEVAL scores align strongly with human judgment, providing insights into failure modes for future improvement."}]}, {"page": 10, "text": "Acknowledgments and Disclosure of Funding\nWe would like to thank Achal Dave, Vivek Ramanujan, Ellen Wu, Yushi Hu, Joongwon Kim,\nTaylor Sorensen, Jeffrey Li, Sebastin Santy, and many other members of UW and UW NLP for\ntheir discussion and constructive feedback. We would also like to thank Hyak computing cluster\nat the University of Washington for providing access to computational resources for running our\nexperiments. This work was funded in part by the DARPA MCS program through NIWC Pacific\n(N66001-19-2-4031), NSF IIS-2044660, Open Philanthropy, the Allen Institute for AI, and NSF\ngrants DMS-2134012 and CCF-2019844 as a part of NSF Institute for Foundations of Machine\nLearning (IFML).\nReferences\n  [1] URL https://openai.com/dall-e-2.\n  [2] URL https://www.midjourney.com/.\n  [3] Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with\n      them. https://github.com/rom1504/clip-retrieval, 2022.\n  [4] Brent Berlin and Paul Kay. Basic color terms: Their universality and evolution. CLSI Publ., 2000.\n  [5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\n      Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu\n      Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change\n      Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint\n      arXiv:1906.07155, 2019.\n  [6] Yixiong Chen, Li Liu, and Chris Ding. X-iqe: explainable image quality evaluation for text-to-image\n      generation with visual large language models, 2023.\n  [7] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\n      attention mask transformer for universal image segmentation. 2022.\n  [8] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,\n      Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive\n      language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n      Recognition, pp. 2818\u20132829, 2023.\n  [9] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of\n      text-to-image generative transformers. 2022.\n[10] Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phuc Le Khac, Luke Melas,\n      and Ritobrata Ghosh. Dall\u00b7e mini, 7 2021. URL https://github.com/borisdayma/dalle-mini.\n[11] Deep-Floyd. Deep-floyd/if. URL https://github.com/deep-floyd/IF.\n[12] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual\n      representation for neon genesis. arXiv preprint arXiv:2303.11331, 2023.\n[13] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato\n      Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for composi-\n      tional text-to-image synthesis. In The Eleventh International Conference on Learning Representations,\n      2023. URL https://openreview.net/forum?id=PUIqjT4rzq7.\n[14] Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and\n      Yezhou Yang. Benchmarking spatial relationships in text-to-image generation, 2022.\n[15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a reference-free\n      evaluation metric for image captioning. In EMNLP, 2021.\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\n      trained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the 31st\n      International Conference on Neural Information Processing Systems, NIPS\u201917, pp. 6629\u20136640, Red Hook,\n      NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.\n                                                      10", "md": "# Acknowledgments and Disclosure of Funding\n\n## Acknowledgments and Disclosure of Funding\n\nWe would like to thank Achal Dave, Vivek Ramanujan, Ellen Wu, Yushi Hu, Joongwon Kim,\nTaylor Sorensen, Jeffrey Li, Sebastin Santy, and many other members of UW and UW NLP for\ntheir discussion and constructive feedback. We would also like to thank Hyak computing cluster\nat the University of Washington for providing access to computational resources for running our\nexperiments. This work was funded in part by the DARPA MCS program through NIWC Pacific\n(N66001-19-2-4031), NSF IIS-2044660, Open Philanthropy, the Allen Institute for AI, and NSF\ngrants DMS-2134012 and CCF-2019844 as a part of NSF Institute for Foundations of Machine\nLearning (IFML).\n\n## References\n\n1. https://openai.com/dall-e-2\n2. https://www.midjourney.com/\n3. Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with\nthem. https://github.com/rom1504/clip-retrieval, 2022.\n4. Brent Berlin and Paul Kay. Basic color terms: Their universality and evolution. CLSI Publ., 2000.\n5. Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu\nLi, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change\nLoy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019.\n6. Yixiong Chen, Li Liu, and Chris Ding. X-iqe: explainable image quality evaluation for text-to-image\ngeneration with visual large language models, 2023.\n7. Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\nattention mask transformer for universal image segmentation. 2022.\n8. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,\nChristoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive\nlanguage-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 2818\u20132829, 2023.\n9. Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of\ntext-to-image generative transformers. 2022.\n10. Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phuc Le Khac, Luke Melas,\nand Ritobrata Ghosh. Dall\u00b7e mini, 7 2021. https://github.com/borisdayma/dalle-mini.\n11. Deep-Floyd. Deep-floyd/if. https://github.com/deep-floyd/IF.\n12. Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual\nrepresentation for neon genesis. arXiv preprint arXiv:2303.11331, 2023.\n13. Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato\nBasu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for composi-\ntional text-to-image synthesis. In The Eleventh International Conference on Learning Representations,\n2023. https://openreview.net/forum?id=PUIqjT4rzq7.\n14. Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and\nYezhou Yang. Benchmarking spatial relationships in text-to-image generation, 2022.\n15. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a reference-free\nevaluation metric for image captioning. In EMNLP, 2021.\n16. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, NIPS\u201917, pp. 6629\u20136640, Red Hook,\nNY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Acknowledgments and Disclosure of Funding", "md": "# Acknowledgments and Disclosure of Funding"}, {"type": "heading", "lvl": 2, "value": "Acknowledgments and Disclosure of Funding", "md": "## Acknowledgments and Disclosure of Funding"}, {"type": "text", "value": "We would like to thank Achal Dave, Vivek Ramanujan, Ellen Wu, Yushi Hu, Joongwon Kim,\nTaylor Sorensen, Jeffrey Li, Sebastin Santy, and many other members of UW and UW NLP for\ntheir discussion and constructive feedback. We would also like to thank Hyak computing cluster\nat the University of Washington for providing access to computational resources for running our\nexperiments. This work was funded in part by the DARPA MCS program through NIWC Pacific\n(N66001-19-2-4031), NSF IIS-2044660, Open Philanthropy, the Allen Institute for AI, and NSF\ngrants DMS-2134012 and CCF-2019844 as a part of NSF Institute for Foundations of Machine\nLearning (IFML).", "md": "We would like to thank Achal Dave, Vivek Ramanujan, Ellen Wu, Yushi Hu, Joongwon Kim,\nTaylor Sorensen, Jeffrey Li, Sebastin Santy, and many other members of UW and UW NLP for\ntheir discussion and constructive feedback. We would also like to thank Hyak computing cluster\nat the University of Washington for providing access to computational resources for running our\nexperiments. This work was funded in part by the DARPA MCS program through NIWC Pacific\n(N66001-19-2-4031), NSF IIS-2044660, Open Philanthropy, the Allen Institute for AI, and NSF\ngrants DMS-2134012 and CCF-2019844 as a part of NSF Institute for Foundations of Machine\nLearning (IFML)."}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "1. https://openai.com/dall-e-2\n2. https://www.midjourney.com/\n3. Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with\nthem. https://github.com/rom1504/clip-retrieval, 2022.\n4. Brent Berlin and Paul Kay. Basic color terms: Their universality and evolution. CLSI Publ., 2000.\n5. Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu\nLi, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change\nLoy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019.\n6. Yixiong Chen, Li Liu, and Chris Ding. X-iqe: explainable image quality evaluation for text-to-image\ngeneration with visual large language models, 2023.\n7. Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\nattention mask transformer for universal image segmentation. 2022.\n8. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,\nChristoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive\nlanguage-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 2818\u20132829, 2023.\n9. Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of\ntext-to-image generative transformers. 2022.\n10. Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phuc Le Khac, Luke Melas,\nand Ritobrata Ghosh. Dall\u00b7e mini, 7 2021. https://github.com/borisdayma/dalle-mini.\n11. Deep-Floyd. Deep-floyd/if. https://github.com/deep-floyd/IF.\n12. Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual\nrepresentation for neon genesis. arXiv preprint arXiv:2303.11331, 2023.\n13. Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato\nBasu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for composi-\ntional text-to-image synthesis. In The Eleventh International Conference on Learning Representations,\n2023. https://openreview.net/forum?id=PUIqjT4rzq7.\n14. Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and\nYezhou Yang. Benchmarking spatial relationships in text-to-image generation, 2022.\n15. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a reference-free\nevaluation metric for image captioning. In EMNLP, 2021.\n16. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, NIPS\u201917, pp. 6629\u20136640, Red Hook,\nNY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.", "md": "1. https://openai.com/dall-e-2\n2. https://www.midjourney.com/\n3. Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with\nthem. https://github.com/rom1504/clip-retrieval, 2022.\n4. Brent Berlin and Paul Kay. Basic color terms: Their universality and evolution. CLSI Publ., 2000.\n5. Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu\nLi, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change\nLoy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019.\n6. Yixiong Chen, Li Liu, and Chris Ding. X-iqe: explainable image quality evaluation for text-to-image\ngeneration with visual large language models, 2023.\n7. Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\nattention mask transformer for universal image segmentation. 2022.\n8. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,\nChristoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive\nlanguage-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 2818\u20132829, 2023.\n9. Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Probing the reasoning skills and social biases of\ntext-to-image generative transformers. 2022.\n10. Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phuc Le Khac, Luke Melas,\nand Ritobrata Ghosh. Dall\u00b7e mini, 7 2021. https://github.com/borisdayma/dalle-mini.\n11. Deep-Floyd. Deep-floyd/if. https://github.com/deep-floyd/IF.\n12. Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual\nrepresentation for neon genesis. arXiv preprint arXiv:2303.11331, 2023.\n13. Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato\nBasu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for composi-\ntional text-to-image synthesis. In The Eleventh International Conference on Learning Representations,\n2023. https://openreview.net/forum?id=PUIqjT4rzq7.\n14. Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and\nYezhou Yang. Benchmarking spatial relationships in text-to-image generation, 2022.\n15. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a reference-free\nevaluation metric for image captioning. In EMNLP, 2021.\n16. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, NIPS\u201917, pp. 6629\u20136640, Red Hook,\nNY, USA, 2017. Curran Associates Inc. ISBN 9781510860964."}]}, {"page": 11, "text": "[17] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Semantic object accuracy for generative text-to-image\n      synthesis. IEEE transactions on pattern analysis and machine intelligence, 44(3):1552\u20131565, 2020.\n[18] Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for\n      hierarchical text-to-image synthesis. In Proceedings of the IEEE conference on computer vision and\n      pattern recognition, pp. 7986\u20137994, 2018.\n[19] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith.\n      Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv\n      preprint arXiv:2303.11897, 2023.\n[20] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.\n      Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision\n      and Pattern Recognition (CVPR), 2023.\n[21] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic:\n      An open dataset of user preferences for text-to-image generation, 2023.\n[22] Kuprel. Kuprel/min-dalle: Min(dall\u00b7e) is a fast, minimal port of dall\u00b7e mini to pytorch. URL https:\n      //github.com/kuprel/min-dalle.\n[23] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays,\n      Pietro Perona, Deva Ramanan, Piotr Doll\u2019a r, and C. Lawrence Zitnick. Microsoft COCO: common objects\n      in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312.\n[24] Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the\n      power of large language models in text-to-image synthesis evaluation, 2023.\n[25] Elman Mansimov, Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Generating images from\n      captions with attention. In ICLR, 2016.\n[26] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Doso-\n      vitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-\n      vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022.\n[27] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\n      Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\n      diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[28] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation\n      of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational\n      Linguistics, ACL \u201902, pp. 311\u2013318, USA, 2002. Association for Computational Linguistics. doi: 10.3115/\n      1073083.1073135. URL https://doi.org/10.3115/1073083.1073135.\n[29] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach.                 Benchmark for\n      compositional text-to-image synthesis. In Thirty-fifth Conference on Neural Information Processing\n      Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?\n      id=bKBhQhPeKaF.\n[30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,\n      and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\n      Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\n      natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR,\n      2021.\n[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\n      Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\n      Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/\n      20-074.html.\n[33] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\n      Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of\n      the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning\n      Research, pp. 8821\u20138831. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/\n      ramesh21a.html.\n                                                       11", "md": "1. Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Semantic object accuracy for generative text-to-image synthesis. IEEE transactions on pattern analysis and machine intelligence, 44(3):1552\u20131565, 2020.\n2. Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for hierarchical text-to-image synthesis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7986\u20137994, 2018.\n3. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897, 2023.\n4. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n5. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023.\n6. Kuprel. Kuprel/min-dalle: Min(dall\u00b7e) is a fast, minimal port of dall\u00b7e mini to pytorch. URL https://github.com/kuprel/min-dalle.\n7. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u2019a r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312.\n8. Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation, 2023.\n9. Elman Mansimov, Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Generating images from captions with attention. In ICLR, 2016.\n10. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022.\n11. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n12. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pp. 311\u2013318, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10.3115/1073083.1073135.\n13. Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for compositional text-to-image synthesis. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id=bKBhQhPeKaF.\n14. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n15. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR, 2021.\n16. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n17. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8821\u20138831. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/ramesh21a.html.", "images": [], "items": [{"type": "text", "value": "1. Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Semantic object accuracy for generative text-to-image synthesis. IEEE transactions on pattern analysis and machine intelligence, 44(3):1552\u20131565, 2020.\n2. Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for hierarchical text-to-image synthesis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7986\u20137994, 2018.\n3. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897, 2023.\n4. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n5. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023.\n6. Kuprel. Kuprel/min-dalle: Min(dall\u00b7e) is a fast, minimal port of dall\u00b7e mini to pytorch. URL https://github.com/kuprel/min-dalle.\n7. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u2019a r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312.\n8. Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation, 2023.\n9. Elman Mansimov, Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Generating images from captions with attention. In ICLR, 2016.\n10. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022.\n11. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n12. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pp. 311\u2013318, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10.3115/1073083.1073135.\n13. Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for compositional text-to-image synthesis. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id=bKBhQhPeKaF.\n14. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n15. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR, 2021.\n16. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n17. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8821\u20138831. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/ramesh21a.html.", "md": "1. Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Semantic object accuracy for generative text-to-image synthesis. IEEE transactions on pattern analysis and machine intelligence, 44(3):1552\u20131565, 2020.\n2. Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic layout for hierarchical text-to-image synthesis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7986\u20137994, 2018.\n3. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897, 2023.\n4. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n5. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023.\n6. Kuprel. Kuprel/min-dalle: Min(dall\u00b7e) is a fast, minimal port of dall\u00b7e mini to pytorch. URL https://github.com/kuprel/min-dalle.\n7. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u2019a r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. URL http://arxiv.org/abs/1405.0312.\n8. Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, and William Yang Wang. Llmscore: Unveiling the power of large language models in text-to-image synthesis evaluation, 2023.\n9. Elman Mansimov, Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Generating images from captions with attention. In ICLR, 2016.\n10. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection with vision transformers. arXiv preprint arXiv:2205.06230, 2022.\n11. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n12. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pp. 311\u2013318, USA, 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://doi.org/10.3115/1073083.1073135.\n13. Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for compositional text-to-image synthesis. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum?id=bKBhQhPeKaF.\n14. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023.\n15. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748\u20138763. PMLR, 2021.\n16. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL http://jmlr.org/papers/v21/20-074.html.\n17. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 8821\u20138831. PMLR, 18\u201324 Jul 2021. URL https://proceedings.mlr.press/v139/ramesh21a.html."}]}, {"page": 12, "text": "[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\n     image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[35] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\n     Generative adversarial text to image synthesis. CoRR, abs/1605.05396, 2016. URL http://arxiv.org/\n     abs/1605.05396.\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\n     image synthesis with latent diffusion models, 2021.\n[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\n     Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,\n     David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language\n     understanding, 2022.\n[38] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\n     techniques for training gans. In Proceedings of the 30th International Conference on Neural Information\n     Processing Systems, NIPS\u201916, pp. 2234\u20132242, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN\n     9781510838819.\n[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi\n     Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R\n     Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b:\n     An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference\n     on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://\n     openreview.net/forum?id=M3Y74vmsMcY.\n[40] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\n     evaluation. CoRR, abs/1411.5726, 2014. URL http://arxiv.org/abs/1411.5726.\n[41] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human\n     preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis, 2023.\n[42] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.\n     Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023.\n[43] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.\n     Attngan: Fine-grained text to image generation with attentional generative adversarial networks. CoRR,\n     abs/1711.10485, 2017. URL http://arxiv.org/abs/1711.10485.\n[44] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\n     Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li,\n     Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-\n     to-image generation. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL\n     https://openreview.net/forum?id=AFDcYJKhND. Featured Certification.\n                                                     12", "md": "# References\n\n## List of References\n\n1. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n2. Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. CoRR, abs/1605.05396, 2016. URL http://arxiv.org/abs/1605.05396.\n3. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models, 2021.\n4. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.\n5. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, pp. 2234\u20132242, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.\n6. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=M3Y74vmsMcY.\n7. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. CoRR, abs/1411.5726, 2014. URL http://arxiv.org/abs/1411.5726.\n8. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis, 2023.\n9. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023.\n10. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. CoRR, abs/1711.10485, 2017. URL http://arxiv.org/abs/1711.10485.\n11. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=AFDcYJKhND. Featured Certification.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "List of References", "md": "## List of References"}, {"type": "text", "value": "1. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n2. Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. CoRR, abs/1605.05396, 2016. URL http://arxiv.org/abs/1605.05396.\n3. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models, 2021.\n4. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.\n5. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, pp. 2234\u20132242, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.\n6. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=M3Y74vmsMcY.\n7. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. CoRR, abs/1411.5726, 2014. URL http://arxiv.org/abs/1411.5726.\n8. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis, 2023.\n9. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023.\n10. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. CoRR, abs/1711.10485, 2017. URL http://arxiv.org/abs/1711.10485.\n11. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=AFDcYJKhND. Featured Certification.", "md": "1. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n2. Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. CoRR, abs/1605.05396, 2016. URL http://arxiv.org/abs/1605.05396.\n3. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models, 2021.\n4. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.\n5. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS\u201916, pp. 2234\u20132242, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819.\n6. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum?id=M3Y74vmsMcY.\n7. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. CoRR, abs/1411.5726, 2014. URL http://arxiv.org/abs/1411.5726.\n8. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis, 2023.\n9. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023.\n10. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. CoRR, abs/1711.10485, 2017. URL http://arxiv.org/abs/1711.10485.\n11. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=AFDcYJKhND. Featured Certification."}]}, {"page": 13, "text": "                                               Single       Two                                                 Attribute\n            Model                Overall       object      object      Counting       Colors      Position       binding\n            CLIP retrieval         0.35         0.89        0.22          0.37         0.62          0.03          0.00\n            minDALL-E              0.23         0.73        0.11          0.12         0.37          0.02          0.01\n            SD v1.1                0.41         0.98        0.31          0.33         0.77          0.02          0.05\n            SD v1.2                0.44         0.97        0.41          0.37         0.76          0.03          0.10\n            SD v1.3                0.43         0.97        0.38          0.35         0.77          0.03          0.05\n            SD v1.4                0.42         0.98        0.36          0.35         0.73          0.01          0.07\n            SD v1.5                0.43         0.97        0.38          0.35         0.76          0.04          0.06\n            SD v2.0                0.51         0.98        0.50          0.48         0.86          0.06          0.15\n            SD v2.1                0.50         0.98        0.51          0.44         0.85          0.07          0.17\n            SD-XL                  0.55         0.98        0.74          0.39         0.85          0.15          0.23\n            IF-M                   0.52         0.97        0.53          0.53         0.84          0.10          0.14\n            IF-L                   0.54         0.98        0.60          0.57         0.80          0.15          0.17\n            IF-XL                  0.61         0.97        0.74          0.66         0.81          0.13          0.35\nTable 3: GENEVAL scores for all models evaluated, including different versions of each model.\nOverall scores have a standard deviation of about 0.01 across random seeds.\nA      Further experiments\nTable 3 shows per-task GENEVAL scores for all the models we evaluate. Here, we cover additional\nablations and analysis.\nA.1      Alignment with human judgment\nFigure 3 shows percent agreement with human judgment across tasks. The agreement rates vary\nacross tasks partly due to differing scores across tasks, i.e., easier tasks will have higher baseline\nagreement. Thus, Figure 7 compares Cohen\u2019s kappa scores, which range from \u22121 (complete\ndisagreement) to 1 (complete agreement) and take random agreement chances into account. single\nobject kappa for GENEVAL is approximately 0, mainly because almost all images for that task are\ncorrect, which penalizes any disagreements greatly.\nA.2      Evaluation parameters\nColor classification.           As explained in Section 3.2, our color classification involves two preprocessing\nsteps: cropping the image to the bounding box, and masking out the background to replace it with\ngray. Table 4 shows the individual effects of these steps on the two color-related tasks. Both cropping\n                                                          Annotator kappa score\n                  0.8          CLIPScore\n                  0.7          GenEval\n                               Interannotator\n                 Cohen's kappa ( )\n                  0.6\n                  0.5\n                  0.4\n                  0.3\n                  0.2\n                  0.1\n                  0.0        Overall      Single         Two       Counting       Colors      Position     Attribute\n                                          object        object                                             binding\nFigure 7: Human study Cohen\u2019s kappa results. CLIPscore shows strong correlation with human\njudgment on the single object and two object tasks, but is beaten out by GENEVAL for the other tasks.\nGENEVAL obtains a kappa score of 0 on the single object task because almost all annotated images\nwere correct, penalizing disagreements more strongly.\n                                                                  13", "md": "|Model|Overall|Single object|Two object|Counting|Colors|Position|Attribute|\n|---|---|---|---|---|---|---|---|\n|CLIP retrieval|0.35|0.89|0.22|0.37|0.62|0.03|0.00|\n|minDALL-E|0.23|0.73|0.11|0.12|0.37|0.02|0.01|\n|SD v1.1|0.41|0.98|0.31|0.33|0.77|0.02|0.05|\n|SD v1.2|0.44|0.97|0.41|0.37|0.76|0.03|0.10|\n|SD v1.3|0.43|0.97|0.38|0.35|0.77|0.03|0.05|\n|SD v1.4|0.42|0.98|0.36|0.35|0.73|0.01|0.07|\n|SD v1.5|0.43|0.97|0.38|0.35|0.76|0.04|0.06|\n|SD v2.0|0.51|0.98|0.50|0.48|0.86|0.06|0.15|\n|SD v2.1|0.50|0.98|0.51|0.44|0.85|0.07|0.17|\n|SD-XL|0.55|0.98|0.74|0.39|0.85|0.15|0.23|\n|IF-M|0.52|0.97|0.53|0.53|0.84|0.10|0.14|\n|IF-L|0.54|0.98|0.60|0.57|0.80|0.15|0.17|\n|IF-XL|0.61|0.97|0.74|0.66|0.81|0.13|0.35|\n\nTable 3: GENEVAL scores for all models evaluated, including different versions of each model. Overall scores have a standard deviation of about 0.01 across random seeds.\n\n## Further experiments\n\nTable 3 shows per-task GENEVAL scores for all the models we evaluate. Here, we cover additional ablations and analysis.\n\n### Alignment with human judgment\n\nFigure 3 shows percent agreement with human judgment across tasks. The agreement rates vary across tasks partly due to differing scores across tasks, i.e., easier tasks will have higher baseline agreement. Thus, Figure 7 compares Cohen\u2019s kappa scores, which range from -1 (complete disagreement) to 1 (complete agreement) and take random agreement chances into account. Single object kappa for GENEVAL is approximately 0, mainly because almost all images for that task are correct, which penalizes any disagreements greatly.\n\n### Evaluation parameters\n\nColor classification. As explained in Section 3.2, our color classification involves two preprocessing steps: cropping the image to the bounding box, and masking out the background to replace it with gray. Table 4 shows the individual effects of these steps on the two color-related tasks. Both cropping\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Annotator kappa score} & 0.8 \\\\\n\\hline\n\\text{CLIPScore} & 0.7 \\\\\n\\hline\n\\text{GenEval} & 0.6 \\\\\n\\hline\n\\text{Interannotator Cohen's kappa} & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 7: Human study Cohen\u2019s kappa results. CLIPscore shows strong correlation with human judgment on the single object and two object tasks, but is beaten out by GENEVAL for the other tasks. GENEVAL obtains a kappa score of 0 on the single object task because almost all annotated images were correct, penalizing disagreements more strongly.", "images": [], "items": [{"type": "table", "rows": [["Model", "Overall", "Single object", "Two object", "Counting", "Colors", "Position", "Attribute"], ["CLIP retrieval", "0.35", "0.89", "0.22", "0.37", "0.62", "0.03", "0.00"], ["minDALL-E", "0.23", "0.73", "0.11", "0.12", "0.37", "0.02", "0.01"], ["SD v1.1", "0.41", "0.98", "0.31", "0.33", "0.77", "0.02", "0.05"], ["SD v1.2", "0.44", "0.97", "0.41", "0.37", "0.76", "0.03", "0.10"], ["SD v1.3", "0.43", "0.97", "0.38", "0.35", "0.77", "0.03", "0.05"], ["SD v1.4", "0.42", "0.98", "0.36", "0.35", "0.73", "0.01", "0.07"], ["SD v1.5", "0.43", "0.97", "0.38", "0.35", "0.76", "0.04", "0.06"], ["SD v2.0", "0.51", "0.98", "0.50", "0.48", "0.86", "0.06", "0.15"], ["SD v2.1", "0.50", "0.98", "0.51", "0.44", "0.85", "0.07", "0.17"], ["SD-XL", "0.55", "0.98", "0.74", "0.39", "0.85", "0.15", "0.23"], ["IF-M", "0.52", "0.97", "0.53", "0.53", "0.84", "0.10", "0.14"], ["IF-L", "0.54", "0.98", "0.60", "0.57", "0.80", "0.15", "0.17"], ["IF-XL", "0.61", "0.97", "0.74", "0.66", "0.81", "0.13", "0.35"]], "md": "|Model|Overall|Single object|Two object|Counting|Colors|Position|Attribute|\n|---|---|---|---|---|---|---|---|\n|CLIP retrieval|0.35|0.89|0.22|0.37|0.62|0.03|0.00|\n|minDALL-E|0.23|0.73|0.11|0.12|0.37|0.02|0.01|\n|SD v1.1|0.41|0.98|0.31|0.33|0.77|0.02|0.05|\n|SD v1.2|0.44|0.97|0.41|0.37|0.76|0.03|0.10|\n|SD v1.3|0.43|0.97|0.38|0.35|0.77|0.03|0.05|\n|SD v1.4|0.42|0.98|0.36|0.35|0.73|0.01|0.07|\n|SD v1.5|0.43|0.97|0.38|0.35|0.76|0.04|0.06|\n|SD v2.0|0.51|0.98|0.50|0.48|0.86|0.06|0.15|\n|SD v2.1|0.50|0.98|0.51|0.44|0.85|0.07|0.17|\n|SD-XL|0.55|0.98|0.74|0.39|0.85|0.15|0.23|\n|IF-M|0.52|0.97|0.53|0.53|0.84|0.10|0.14|\n|IF-L|0.54|0.98|0.60|0.57|0.80|0.15|0.17|\n|IF-XL|0.61|0.97|0.74|0.66|0.81|0.13|0.35|", "isPerfectTable": true, "csv": "\"Model\",\"Overall\",\"Single object\",\"Two object\",\"Counting\",\"Colors\",\"Position\",\"Attribute\"\n\"CLIP retrieval\",\"0.35\",\"0.89\",\"0.22\",\"0.37\",\"0.62\",\"0.03\",\"0.00\"\n\"minDALL-E\",\"0.23\",\"0.73\",\"0.11\",\"0.12\",\"0.37\",\"0.02\",\"0.01\"\n\"SD v1.1\",\"0.41\",\"0.98\",\"0.31\",\"0.33\",\"0.77\",\"0.02\",\"0.05\"\n\"SD v1.2\",\"0.44\",\"0.97\",\"0.41\",\"0.37\",\"0.76\",\"0.03\",\"0.10\"\n\"SD v1.3\",\"0.43\",\"0.97\",\"0.38\",\"0.35\",\"0.77\",\"0.03\",\"0.05\"\n\"SD v1.4\",\"0.42\",\"0.98\",\"0.36\",\"0.35\",\"0.73\",\"0.01\",\"0.07\"\n\"SD v1.5\",\"0.43\",\"0.97\",\"0.38\",\"0.35\",\"0.76\",\"0.04\",\"0.06\"\n\"SD v2.0\",\"0.51\",\"0.98\",\"0.50\",\"0.48\",\"0.86\",\"0.06\",\"0.15\"\n\"SD v2.1\",\"0.50\",\"0.98\",\"0.51\",\"0.44\",\"0.85\",\"0.07\",\"0.17\"\n\"SD-XL\",\"0.55\",\"0.98\",\"0.74\",\"0.39\",\"0.85\",\"0.15\",\"0.23\"\n\"IF-M\",\"0.52\",\"0.97\",\"0.53\",\"0.53\",\"0.84\",\"0.10\",\"0.14\"\n\"IF-L\",\"0.54\",\"0.98\",\"0.60\",\"0.57\",\"0.80\",\"0.15\",\"0.17\"\n\"IF-XL\",\"0.61\",\"0.97\",\"0.74\",\"0.66\",\"0.81\",\"0.13\",\"0.35\""}, {"type": "text", "value": "Table 3: GENEVAL scores for all models evaluated, including different versions of each model. Overall scores have a standard deviation of about 0.01 across random seeds.", "md": "Table 3: GENEVAL scores for all models evaluated, including different versions of each model. Overall scores have a standard deviation of about 0.01 across random seeds."}, {"type": "heading", "lvl": 2, "value": "Further experiments", "md": "## Further experiments"}, {"type": "text", "value": "Table 3 shows per-task GENEVAL scores for all the models we evaluate. Here, we cover additional ablations and analysis.", "md": "Table 3 shows per-task GENEVAL scores for all the models we evaluate. Here, we cover additional ablations and analysis."}, {"type": "heading", "lvl": 3, "value": "Alignment with human judgment", "md": "### Alignment with human judgment"}, {"type": "text", "value": "Figure 3 shows percent agreement with human judgment across tasks. The agreement rates vary across tasks partly due to differing scores across tasks, i.e., easier tasks will have higher baseline agreement. Thus, Figure 7 compares Cohen\u2019s kappa scores, which range from -1 (complete disagreement) to 1 (complete agreement) and take random agreement chances into account. Single object kappa for GENEVAL is approximately 0, mainly because almost all images for that task are correct, which penalizes any disagreements greatly.", "md": "Figure 3 shows percent agreement with human judgment across tasks. The agreement rates vary across tasks partly due to differing scores across tasks, i.e., easier tasks will have higher baseline agreement. Thus, Figure 7 compares Cohen\u2019s kappa scores, which range from -1 (complete disagreement) to 1 (complete agreement) and take random agreement chances into account. Single object kappa for GENEVAL is approximately 0, mainly because almost all images for that task are correct, which penalizes any disagreements greatly."}, {"type": "heading", "lvl": 3, "value": "Evaluation parameters", "md": "### Evaluation parameters"}, {"type": "text", "value": "Color classification. As explained in Section 3.2, our color classification involves two preprocessing steps: cropping the image to the bounding box, and masking out the background to replace it with gray. Table 4 shows the individual effects of these steps on the two color-related tasks. Both cropping\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Annotator kappa score} & 0.8 \\\\\n\\hline\n\\text{CLIPScore} & 0.7 \\\\\n\\hline\n\\text{GenEval} & 0.6 \\\\\n\\hline\n\\text{Interannotator Cohen's kappa} & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 7: Human study Cohen\u2019s kappa results. CLIPscore shows strong correlation with human judgment on the single object and two object tasks, but is beaten out by GENEVAL for the other tasks. GENEVAL obtains a kappa score of 0 on the single object task because almost all annotated images were correct, penalizing disagreements more strongly.", "md": "Color classification. As explained in Section 3.2, our color classification involves two preprocessing steps: cropping the image to the bounding box, and masking out the background to replace it with gray. Table 4 shows the individual effects of these steps on the two color-related tasks. Both cropping\n\n$$\n\\begin{array}{|c|c|}\n\\hline\n\\text{Annotator kappa score} & 0.8 \\\\\n\\hline\n\\text{CLIPScore} & 0.7 \\\\\n\\hline\n\\text{GenEval} & 0.6 \\\\\n\\hline\n\\text{Interannotator Cohen's kappa} & \\\\\n\\hline\n\\end{array}\n$$\n\nFigure 7: Human study Cohen\u2019s kappa results. CLIPscore shows strong correlation with human judgment on the single object and two object tasks, but is beaten out by GENEVAL for the other tasks. GENEVAL obtains a kappa score of 0 on the single object task because almost all annotated images were correct, penalizing disagreements more strongly."}]}, {"page": 14, "text": "                                                                                      Attribute\n                              Method                Crop      Mask        Colors       binding\n                                                      \u2717          \u2717         0.32          0.01\n                              GENEVAL                 \u2713          \u2717         0.37          0.33\n                                                      \u2717          \u2713         0.43          0.47\n                                                      \u2713          \u2713         0.45          0.49\n                              CLIPScore               \u2014         \u2014          0.23          0.47\n                              Interannotator          \u2014         \u2014          0.55          0.65\nTable 4: Cohen\u2019s kappa agreement with human annotators for different color classification methods.\nBounding box cropping and background masking both increase alignment with human judgment,\nespecially for the attribute binding task where the presence of other objects may otherwise\nconfuse the classifier. Combining cropping with masking provides a small gain over just masking.\nand masking are individually useful at removing distractions (i.e. other objects in the image), which\nmanifests most often in the attribute binding task. Combining the two gives the best human\nagreement.\nCounting task threshold.              When images contain multiple instances of the same type of object,\nthe object detector tends to predict disproportionately more bounding boxes. To remedy this, we\nincrease the minimum confidence threshold for only the counting task (Figure 8). The optimal\nthreshold is much higher, around 0.9, and increases Cohen\u2019s kappa agreement greatly, from 0.37 to\n0.65. Using K-fold cross-validation (k = 5), we confirm that this is the optimal threshold across all\nsplits, obtaining 0.823 \u00b1 0.013 agreement with human annotations on validation splits.\nPosition task minimum distance.                  As described in Appendix C.3, there are rare cases when two\nobjects may be generated too close together to determine their relative position. In these cases,\nhuman annotators mark the objects as neutrally/not offset from one another. We find this has an\ninsignificant effect on overall scores, but setting a small minimum distance between objects before\nthey are classified as to the left/right/above/below one another does improve alignment with human\njudgment (Figure 8). Using K-fold cross-validation (k = 5), we confirm that our choice of threshold\nis not overfit: we obtain 0.822 \u00b1 0.013 agreement with human annotations on validation splits.\n         0.8             Counting task kappa score                        0.5        Position task kappa score\n                                                                          0.4\n        Cohen's kappa ( )                                                Cohen's kappa ( )\n         0.6\n                                                                          0.3\n         0.4                                                              0.2\n         0.2                                        GenEval               0.1         GenEval\n                                                    Interannotator                    Interannotator\n                                                    CLIPScore                         CLIPScore\n         0.0   0.3    0.4     0.5    0.6     0.7    0.8     0.9           0.0 0.00    0.05    0.10    0.15    0.20  0.25\n                              Confidence threshold                                    Minimum distance threshold\nFigure 8: Cohen\u2019s kappa agreement with human annotators for varying evaluation hyperparameters.\n(Left) counting task confidence threshold. The default threshold of 0.3 is too low when multiple\ninstance of the same object are in the image, resulting in superfluous detected objects. (Right)\nposition task minimum distance threshold. In rare cases when objects are merged or very close\ntogether, they should be classified as neutral/not offset in the given direction to match human\njudgment.\n                                                               14", "md": "# Document\n\n## Attribute\n\n|Method|Crop|Mask|Colors|Binding|\n|---|---|---|---|---|\n|GENEVAL|\u2713|\u2717|0.37|0.33|\n| |\u2717|\u2713|0.43|0.47|\n| |\u2713|\u2713|0.45|0.49|\n|CLIPScore|\u2014|\u2014|0.23|0.47|\n|Interannotator|\u2014|\u2014|0.55|0.65|\n\nTable 4: Cohen\u2019s kappa agreement with human annotators for different color classification methods. Bounding box cropping and background masking both increase alignment with human judgment, especially for the attribute binding task where the presence of other objects may otherwise confuse the classifier. Combining cropping with masking provides a small gain over just masking. and masking are individually useful at removing distractions (i.e. other objects in the image), which manifests most often in the attribute binding task. Combining the two gives the best human agreement.\n\nCounting task threshold. When images contain multiple instances of the same type of object, the object detector tends to predict disproportionately more bounding boxes. To remedy this, we increase the minimum confidence threshold for only the counting task (Figure 8). The optimal threshold is much higher, around 0.9, and increases Cohen\u2019s kappa agreement greatly, from 0.37 to 0.65. Using K-fold cross-validation (k = 5), we confirm that this is the optimal threshold across all splits, obtaining 0.823 \u00b1 0.013 agreement with human annotations on validation splits.\n\nPosition task minimum distance. As described in Appendix C.3, there are rare cases when two objects may be generated too close together to determine their relative position. In these cases, human annotators mark the objects as neutrally/not offset from one another. We find this has an insignificant effect on overall scores, but setting a small minimum distance between objects before they are classified as to the left/right/above/below one another does improve alignment with human judgment (Figure 8). Using K-fold cross-validation (k = 5), we confirm that our choice of threshold is not overfit: we obtain 0.822 \u00b1 0.013 agreement with human annotations on validation splits.\n\n$$0.8 \\text{ Counting task kappa score} \\quad 0.5 \\text{ Position task kappa score}$$\n\n$$\\text{Cohen's kappa (Counting task)} = 0.6 \\quad \\text{Cohen's kappa (Position task)} = 0.4$$\n\n$$\\begin{array}{|c|c|c|c|c|c|c|c|c|} \\hline Confidence \\ threshold & 0.0 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\\\ \\hline Cohen's \\ kappa & & & & & & & & \\\\ \\hline \\end{array} \\quad \\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline Minimum \\ distance \\ threshold & 0.0 & 0.00 & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 \\\\ \\hline Cohen's \\ kappa & & & & & & & \\\\ \\hline \\end{array}$$\n\nFigure 8: Cohen\u2019s kappa agreement with human annotators for varying evaluation hyperparameters. (Left) counting task confidence threshold. The default threshold of 0.3 is too low when multiple instance of the same object are in the image, resulting in superfluous detected objects. (Right) position task minimum distance threshold. In rare cases when objects are merged or very close together, they should be classified as neutral/not offset in the given direction to match human judgment.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Attribute", "md": "## Attribute"}, {"type": "table", "rows": [["Method", "Crop", "Mask", "Colors", "Binding"], ["GENEVAL", "\u2713", "\u2717", "0.37", "0.33"], ["", "\u2717", "\u2713", "0.43", "0.47"], ["", "\u2713", "\u2713", "0.45", "0.49"], ["CLIPScore", "\u2014", "\u2014", "0.23", "0.47"], ["Interannotator", "\u2014", "\u2014", "0.55", "0.65"]], "md": "|Method|Crop|Mask|Colors|Binding|\n|---|---|---|---|---|\n|GENEVAL|\u2713|\u2717|0.37|0.33|\n| |\u2717|\u2713|0.43|0.47|\n| |\u2713|\u2713|0.45|0.49|\n|CLIPScore|\u2014|\u2014|0.23|0.47|\n|Interannotator|\u2014|\u2014|0.55|0.65|", "isPerfectTable": true, "csv": "\"Method\",\"Crop\",\"Mask\",\"Colors\",\"Binding\"\n\"GENEVAL\",\"\u2713\",\"\u2717\",\"0.37\",\"0.33\"\n\"\",\"\u2717\",\"\u2713\",\"0.43\",\"0.47\"\n\"\",\"\u2713\",\"\u2713\",\"0.45\",\"0.49\"\n\"CLIPScore\",\"\u2014\",\"\u2014\",\"0.23\",\"0.47\"\n\"Interannotator\",\"\u2014\",\"\u2014\",\"0.55\",\"0.65\""}, {"type": "text", "value": "Table 4: Cohen\u2019s kappa agreement with human annotators for different color classification methods. Bounding box cropping and background masking both increase alignment with human judgment, especially for the attribute binding task where the presence of other objects may otherwise confuse the classifier. Combining cropping with masking provides a small gain over just masking. and masking are individually useful at removing distractions (i.e. other objects in the image), which manifests most often in the attribute binding task. Combining the two gives the best human agreement.\n\nCounting task threshold. When images contain multiple instances of the same type of object, the object detector tends to predict disproportionately more bounding boxes. To remedy this, we increase the minimum confidence threshold for only the counting task (Figure 8). The optimal threshold is much higher, around 0.9, and increases Cohen\u2019s kappa agreement greatly, from 0.37 to 0.65. Using K-fold cross-validation (k = 5), we confirm that this is the optimal threshold across all splits, obtaining 0.823 \u00b1 0.013 agreement with human annotations on validation splits.\n\nPosition task minimum distance. As described in Appendix C.3, there are rare cases when two objects may be generated too close together to determine their relative position. In these cases, human annotators mark the objects as neutrally/not offset from one another. We find this has an insignificant effect on overall scores, but setting a small minimum distance between objects before they are classified as to the left/right/above/below one another does improve alignment with human judgment (Figure 8). Using K-fold cross-validation (k = 5), we confirm that our choice of threshold is not overfit: we obtain 0.822 \u00b1 0.013 agreement with human annotations on validation splits.\n\n$$0.8 \\text{ Counting task kappa score} \\quad 0.5 \\text{ Position task kappa score}$$\n\n$$\\text{Cohen's kappa (Counting task)} = 0.6 \\quad \\text{Cohen's kappa (Position task)} = 0.4$$\n\n$$\\begin{array}{|c|c|c|c|c|c|c|c|c|} \\hline Confidence \\ threshold & 0.0 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\\\ \\hline Cohen's \\ kappa & & & & & & & & \\\\ \\hline \\end{array} \\quad \\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline Minimum \\ distance \\ threshold & 0.0 & 0.00 & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 \\\\ \\hline Cohen's \\ kappa & & & & & & & \\\\ \\hline \\end{array}$$\n\nFigure 8: Cohen\u2019s kappa agreement with human annotators for varying evaluation hyperparameters. (Left) counting task confidence threshold. The default threshold of 0.3 is too low when multiple instance of the same object are in the image, resulting in superfluous detected objects. (Right) position task minimum distance threshold. In rare cases when objects are merged or very close together, they should be classified as neutral/not offset in the given direction to match human judgment.", "md": "Table 4: Cohen\u2019s kappa agreement with human annotators for different color classification methods. Bounding box cropping and background masking both increase alignment with human judgment, especially for the attribute binding task where the presence of other objects may otherwise confuse the classifier. Combining cropping with masking provides a small gain over just masking. and masking are individually useful at removing distractions (i.e. other objects in the image), which manifests most often in the attribute binding task. Combining the two gives the best human agreement.\n\nCounting task threshold. When images contain multiple instances of the same type of object, the object detector tends to predict disproportionately more bounding boxes. To remedy this, we increase the minimum confidence threshold for only the counting task (Figure 8). The optimal threshold is much higher, around 0.9, and increases Cohen\u2019s kappa agreement greatly, from 0.37 to 0.65. Using K-fold cross-validation (k = 5), we confirm that this is the optimal threshold across all splits, obtaining 0.823 \u00b1 0.013 agreement with human annotations on validation splits.\n\nPosition task minimum distance. As described in Appendix C.3, there are rare cases when two objects may be generated too close together to determine their relative position. In these cases, human annotators mark the objects as neutrally/not offset from one another. We find this has an insignificant effect on overall scores, but setting a small minimum distance between objects before they are classified as to the left/right/above/below one another does improve alignment with human judgment (Figure 8). Using K-fold cross-validation (k = 5), we confirm that our choice of threshold is not overfit: we obtain 0.822 \u00b1 0.013 agreement with human annotations on validation splits.\n\n$$0.8 \\text{ Counting task kappa score} \\quad 0.5 \\text{ Position task kappa score}$$\n\n$$\\text{Cohen's kappa (Counting task)} = 0.6 \\quad \\text{Cohen's kappa (Position task)} = 0.4$$\n\n$$\\begin{array}{|c|c|c|c|c|c|c|c|c|} \\hline Confidence \\ threshold & 0.0 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9 \\\\ \\hline Cohen's \\ kappa & & & & & & & & \\\\ \\hline \\end{array} \\quad \\begin{array}{|c|c|c|c|c|c|c|c|c|c|c|} \\hline Minimum \\ distance \\ threshold & 0.0 & 0.00 & 0.05 & 0.10 & 0.15 & 0.20 & 0.25 \\\\ \\hline Cohen's \\ kappa & & & & & & & \\\\ \\hline \\end{array}$$\n\nFigure 8: Cohen\u2019s kappa agreement with human annotators for varying evaluation hyperparameters. (Left) counting task confidence threshold. The default threshold of 0.3 is too low when multiple instance of the same object are in the image, resulting in superfluous detected objects. (Right) position task minimum distance threshold. In rare cases when objects are merged or very close together, they should be classified as neutral/not offset in the given direction to match human judgment."}]}, {"page": 15, "text": "                          minDALL-E               SDv1.5               SDv2.1               IF-XL         CLIP retr.\n        Single object\n   a photo of a carrot\n          Two object\n     a photo of a fork\n          and a book\n            Counting\n     a photo of three\n                 cups\n               Colors\n  a photo of a purple\n              scissors\n             Position\n      a photo of a tie\n         above a sink\n  Attribute binding\n   a photo of a white\n      dog and a blue\n         potted plant\n                      Figure 9: Random examples of images generated by each model.\nA.3     Qualitative examples\nFigure 9 displays sample generated images for each model for an example prompt from each task. In\ngeneral, all of the T2I models struggle with objects that have more complicated structure; items like\nforks (second row) and scissors (fourth row) are difficult even for Stable Diffusion and IF. Certain\nobject combinations are also surprisingly difficult for the models to generate at all (fifth row), while\nothers are easier, perhaps due to co-occurrence in the training data. Note, however, that the T2I\nmodels often do succeed in generating unseen or rarely seen object pairs: for example, the CLIP\nretrieval from LAION-5B in row 6 does not find an image of a dog and a potted plant, whereas Stable\nDiffusion (primarily trained on LAION) succeeds in doing so.\n                                                             15", "md": "# Generated Images by Different Models\n\n## Generated Images by Different Models\n\nFigure 9 displays sample generated images for each model for an example prompt from each task. In general, all of the T2I models struggle with objects that have more complicated structure; items like forks (second row) and scissors (fourth row) are difficult even for Stable Diffusion and IF. Certain object combinations are also surprisingly difficult for the models to generate at all (fifth row), while others are easier, perhaps due to co-occurrence in the training data. Note, however, that the T2I models often do succeed in generating unseen or rarely seen object pairs: for example, the CLIP retrieval from LAION-5B in row 6 does not find an image of a dog and a potted plant, whereas Stable Diffusion (primarily trained on LAION) succeeds in doing so.\n\n### Random Examples of Images Generated by Each Model\n\n|minDALL-E|SDv1.5|SDv2.1|IF-XL|CLIP retr.|\n|---|---|---|---|---|\n|Single object a photo of a carrot| | | | |\n|Two objects a photo of a fork and a book| | | | |\n|Counting a photo of three cups| | | | |\n|Colors a photo of a purple scissors| | | | |\n|Position a photo of a tie above a sink| | | | |\n|Attribute binding a photo of a white dog and a blue potted plant| | | | |\n\nFigure 9: Random examples of images generated by each model.", "images": [{"name": "page-15-1.jpg", "height": 57, "width": 57, "x": 184, "y": 261}, {"name": "page-15-0.jpg", "height": 57, "width": 57, "x": 184, "y": 333}, {"name": "page-15-2.jpg", "height": 57, "width": 57, "x": 184, "y": 405}, {"name": "page-15-5.jpg", "height": 57, "width": 57, "x": 184, "y": 477}, {"name": "page-15-6.jpg", "height": 57, "width": 57, "x": 249, "y": 477}, {"name": "page-15-3.jpg", "height": 57, "width": 57, "x": 184, "y": 118}, {"name": "page-15-9.jpg", "height": 57, "width": 57, "x": 249, "y": 405}, {"name": "page-15-4.jpg", "height": 57, "width": 57, "x": 184, "y": 189}, {"name": "page-15-11.jpg", "height": 57, "width": 57, "x": 249, "y": 189}, {"name": "page-15-7.jpg", "height": 57, "width": 57, "x": 249, "y": 333}, {"name": "page-15-13.jpg", "height": 57, "width": 57, "x": 314, "y": 333}, {"name": "page-15-8.jpg", "height": 57, "width": 57, "x": 249, "y": 261}, {"name": "page-15-10.jpg", "height": 57, "width": 57, "x": 249, "y": 118}, {"name": "page-15-14.jpg", "height": 57, "width": 57, "x": 314, "y": 261}, {"name": "page-15-12.jpg", "height": 57, "width": 57, "x": 314, "y": 477}, {"name": "page-15-18.jpg", "height": 57, "width": 57, "x": 379, "y": 477}, {"name": "page-15-17.jpg", "height": 57, "width": 57, "x": 314, "y": 189}, {"name": "page-15-15.jpg", "height": 57, "width": 57, "x": 314, "y": 405}, {"name": "page-15-16.jpg", "height": 57, "width": 57, "x": 314, "y": 118}, {"name": "page-15-20.jpg", "height": 57, "width": 57, "x": 379, "y": 261}, {"name": "page-15-21.jpg", "height": 57, "width": 57, "x": 379, "y": 405}, {"name": "page-15-19.jpg", "height": 57, "width": 57, "x": 379, "y": 333}, {"name": "page-15-22.jpg", "height": 57, "width": 57, "x": 378, "y": 118}, {"name": "page-15-23.jpg", "height": 57, "width": 57, "x": 379, "y": 189}, {"name": "page-15-24.jpg", "height": 42, "width": 57, "x": 444, "y": 484}, {"name": "page-15-25.jpg", "height": 57, "width": 38, "x": 453, "y": 333}, {"name": "page-15-29.jpg", "height": 38, "width": 57, "x": 444, "y": 199}, {"name": "page-15-27.jpg", "height": 38, "width": 57, "x": 444, "y": 414}, {"name": "page-15-26.jpg", "height": 57, "width": 41, "x": 452, "y": 261}, {"name": "page-15-28.jpg", "height": 26, "width": 58, "x": 443, "y": 133}], "items": [{"type": "heading", "lvl": 1, "value": "Generated Images by Different Models", "md": "# Generated Images by Different Models"}, {"type": "heading", "lvl": 2, "value": "Generated Images by Different Models", "md": "## Generated Images by Different Models"}, {"type": "text", "value": "Figure 9 displays sample generated images for each model for an example prompt from each task. In general, all of the T2I models struggle with objects that have more complicated structure; items like forks (second row) and scissors (fourth row) are difficult even for Stable Diffusion and IF. Certain object combinations are also surprisingly difficult for the models to generate at all (fifth row), while others are easier, perhaps due to co-occurrence in the training data. Note, however, that the T2I models often do succeed in generating unseen or rarely seen object pairs: for example, the CLIP retrieval from LAION-5B in row 6 does not find an image of a dog and a potted plant, whereas Stable Diffusion (primarily trained on LAION) succeeds in doing so.", "md": "Figure 9 displays sample generated images for each model for an example prompt from each task. In general, all of the T2I models struggle with objects that have more complicated structure; items like forks (second row) and scissors (fourth row) are difficult even for Stable Diffusion and IF. Certain object combinations are also surprisingly difficult for the models to generate at all (fifth row), while others are easier, perhaps due to co-occurrence in the training data. Note, however, that the T2I models often do succeed in generating unseen or rarely seen object pairs: for example, the CLIP retrieval from LAION-5B in row 6 does not find an image of a dog and a potted plant, whereas Stable Diffusion (primarily trained on LAION) succeeds in doing so."}, {"type": "heading", "lvl": 3, "value": "Random Examples of Images Generated by Each Model", "md": "### Random Examples of Images Generated by Each Model"}, {"type": "table", "rows": [["minDALL-E", "SDv1.5", "SDv2.1", "IF-XL", "CLIP retr."], ["Single object a photo of a carrot", "", "", "", ""], ["Two objects a photo of a fork and a book", "", "", "", ""], ["Counting a photo of three cups", "", "", "", ""], ["Colors a photo of a purple scissors", "", "", "", ""], ["Position a photo of a tie above a sink", "", "", "", ""], ["Attribute binding a photo of a white dog and a blue potted plant", "", "", "", ""]], "md": "|minDALL-E|SDv1.5|SDv2.1|IF-XL|CLIP retr.|\n|---|---|---|---|---|\n|Single object a photo of a carrot| | | | |\n|Two objects a photo of a fork and a book| | | | |\n|Counting a photo of three cups| | | | |\n|Colors a photo of a purple scissors| | | | |\n|Position a photo of a tie above a sink| | | | |\n|Attribute binding a photo of a white dog and a blue potted plant| | | | |", "isPerfectTable": true, "csv": "\"minDALL-E\",\"SDv1.5\",\"SDv2.1\",\"IF-XL\",\"CLIP retr.\"\n\"Single object a photo of a carrot\",\"\",\"\",\"\",\"\"\n\"Two objects a photo of a fork and a book\",\"\",\"\",\"\",\"\"\n\"Counting a photo of three cups\",\"\",\"\",\"\",\"\"\n\"Colors a photo of a purple scissors\",\"\",\"\",\"\",\"\"\n\"Position a photo of a tie above a sink\",\"\",\"\",\"\",\"\"\n\"Attribute binding a photo of a white dog and a blue potted plant\",\"\",\"\",\"\",\"\""}, {"type": "text", "value": "Figure 9: Random examples of images generated by each model.", "md": "Figure 9: Random examples of images generated by each model."}]}, {"page": 16, "text": "                           a photo of a dog                                           a photo of a white handbag\n                         right of a teddy bear                                              and a purple bed\nFigure 10: T2I failure modes discovered through GENEVAL. (Left) IF-XL exhibits a position bias\nwhere the first object is more likely to be on the left of the second object. (Right) Stable Diffusion\nv2.1 has a tendency to swap the specified colors in the attribute binding task, or sometimes to\nleak the specified color into the background instead, as in the top right example.\nFigure 10 shows some examples of T2I failure modes as described in Section 5.2. The position bias\nin IF-XL and the color swapping tendency of Stable Diffusion can be discovered through quantitative\nanalysis, and confirmed through qualitative observation of the generated images. These failure modes\nsuggest avenues of research for future targeted improvement of T2I capabilities.\nB      Ethics statement\nThe primary purpose of this work is to facilitate the development of better text-to-image generative\nmodels. While we hope that such models are used for benign uses such as generating art, drafting\ncreative ideas, or producing synthetic training data for vision models, we acknowledge that text-to-\nimage technology can be used for harm. This may manifest through spreading misinformation with\ndeepfakes to cause individual or widespread harm. This is a potential impact down the line for any\nresearch into developing text-to-image models, and careful consideration will be required to mitigate\nthis harm.\nAnother potential issue is amplification of biases present in the evaluation method. For example, the\nobject detector classes implicitly place importance on certain types of objects, and the detector and\nthe color classifier encode biases present in their respective training data. However, we hope that our\nevaluation framework will be expanded and improved with better models, so that mitigating bias\nupstream, e.g., by diversifying training data and expanding the number of object classes, will also\nlead to a more fair evaluation.\nC      Technical details\nC.1      Prompt generation\nFor each task, we generate 100 random prompts and then remove duplicates, sampling uniformly\n(with specific exceptions) from:\n          \u2022 Objects: 80 MS-COCO object names\n          \u2022 Numbers: \u201ctwo\u201d, \u201cthree\u201d, or \u201cfour\u201d\n          \u2022 Positions: \u201cabove\u201d, \u201cbelow\u201d, \u201cleft of\u201d, \u201cright of\u201d\n          \u2022 Colors: \u201cred\u201d, \u201corange\u201d, \u201cyellow\u201d, \u201cgreen\u201d, \u201cblue\u201d, \u201cpurple\u201d, \u201cpink\u201d, \u201cbrown\u201d, \u201cblack\u201d,\n            \u201cwhite\u201d\n                                                                  16", "md": "# Document\n\na photo of a dog &emsp; a photo of a white handbag\n\nright of a teddy bear &emsp; and a purple bed\n\nFigure 10: T2I failure modes discovered through GENEVAL. (Left) IF-XL exhibits a position bias\nwhere the first object is more likely to be on the left of the second object. (Right) Stable Diffusion\nv2.1 has a tendency to swap the specified colors in the attribute binding task, or sometimes to\nleak the specified color into the background instead, as in the top right example.\n\nFigure 10 shows some examples of T2I failure modes as described in Section 5.2. The position bias\nin IF-XL and the color swapping tendency of Stable Diffusion can be discovered through quantitative\nanalysis, and confirmed through qualitative observation of the generated images. These failure modes\nsuggest avenues of research for future targeted improvement of T2I capabilities.\n\n##  Ethics statement\n\nThe primary purpose of this work is to facilitate the development of better text-to-image generative\nmodels. While we hope that such models are used for benign uses such as generating art, drafting\ncreative ideas, or producing synthetic training data for vision models, we acknowledge that text-to-\nimage technology can be used for harm. This may manifest through spreading misinformation with\ndeepfakes to cause individual or widespread harm. This is a potential impact down the line for any\nresearch into developing text-to-image models, and careful consideration will be required to mitigate\nthis harm.\n\nAnother potential issue is amplification of biases present in the evaluation method. For example, the\nobject detector classes implicitly place importance on certain types of objects, and the detector and\nthe color classifier encode biases present in their respective training data. However, we hope that our\nevaluation framework will be expanded and improved with better models, so that mitigating bias\nupstream, e.g., by diversifying training data and expanding the number of object classes, will also\nlead to a more fair evaluation.\n\n##  Technical details\n\n###  Prompt generation\n\nFor each task, we generate 100 random prompts and then remove duplicates, sampling uniformly\n(with specific exceptions) from:\n\n- Objects: 80 MS-COCO object names\n- Numbers: \"two\", \"three\", or \"four\"\n- Positions: \"above\", \"below\", \"left of\", \"right of\"\n- Colors: \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\", \"pink\", \"brown\", \"black\", \"white\"", "images": [{"name": "page-16-0.jpg", "height": 74, "width": 74, "x": 134, "y": 101}, {"name": "page-16-1.jpg", "height": 74, "width": 74, "x": 212, "y": 101}, {"name": "page-16-6.jpg", "height": 74, "width": 74, "x": 403, "y": 100}, {"name": "page-16-2.jpg", "height": 74, "width": 74, "x": 134, "y": 179}, {"name": "page-16-3.jpg", "height": 74, "width": 74, "x": 212, "y": 179}, {"name": "page-16-4.jpg", "height": 74, "width": 74, "x": 403, "y": 179}, {"name": "page-16-5.jpg", "height": 74, "width": 74, "x": 325, "y": 179}, {"name": "page-16-7.jpg", "height": 74, "width": 74, "x": 325, "y": 100}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "a photo of a dog &emsp; a photo of a white handbag\n\nright of a teddy bear &emsp; and a purple bed\n\nFigure 10: T2I failure modes discovered through GENEVAL. (Left) IF-XL exhibits a position bias\nwhere the first object is more likely to be on the left of the second object. (Right) Stable Diffusion\nv2.1 has a tendency to swap the specified colors in the attribute binding task, or sometimes to\nleak the specified color into the background instead, as in the top right example.\n\nFigure 10 shows some examples of T2I failure modes as described in Section 5.2. The position bias\nin IF-XL and the color swapping tendency of Stable Diffusion can be discovered through quantitative\nanalysis, and confirmed through qualitative observation of the generated images. These failure modes\nsuggest avenues of research for future targeted improvement of T2I capabilities.", "md": "a photo of a dog &emsp; a photo of a white handbag\n\nright of a teddy bear &emsp; and a purple bed\n\nFigure 10: T2I failure modes discovered through GENEVAL. (Left) IF-XL exhibits a position bias\nwhere the first object is more likely to be on the left of the second object. (Right) Stable Diffusion\nv2.1 has a tendency to swap the specified colors in the attribute binding task, or sometimes to\nleak the specified color into the background instead, as in the top right example.\n\nFigure 10 shows some examples of T2I failure modes as described in Section 5.2. The position bias\nin IF-XL and the color swapping tendency of Stable Diffusion can be discovered through quantitative\nanalysis, and confirmed through qualitative observation of the generated images. These failure modes\nsuggest avenues of research for future targeted improvement of T2I capabilities."}, {"type": "heading", "lvl": 2, "value": "Ethics statement", "md": "##  Ethics statement"}, {"type": "text", "value": "The primary purpose of this work is to facilitate the development of better text-to-image generative\nmodels. While we hope that such models are used for benign uses such as generating art, drafting\ncreative ideas, or producing synthetic training data for vision models, we acknowledge that text-to-\nimage technology can be used for harm. This may manifest through spreading misinformation with\ndeepfakes to cause individual or widespread harm. This is a potential impact down the line for any\nresearch into developing text-to-image models, and careful consideration will be required to mitigate\nthis harm.\n\nAnother potential issue is amplification of biases present in the evaluation method. For example, the\nobject detector classes implicitly place importance on certain types of objects, and the detector and\nthe color classifier encode biases present in their respective training data. However, we hope that our\nevaluation framework will be expanded and improved with better models, so that mitigating bias\nupstream, e.g., by diversifying training data and expanding the number of object classes, will also\nlead to a more fair evaluation.", "md": "The primary purpose of this work is to facilitate the development of better text-to-image generative\nmodels. While we hope that such models are used for benign uses such as generating art, drafting\ncreative ideas, or producing synthetic training data for vision models, we acknowledge that text-to-\nimage technology can be used for harm. This may manifest through spreading misinformation with\ndeepfakes to cause individual or widespread harm. This is a potential impact down the line for any\nresearch into developing text-to-image models, and careful consideration will be required to mitigate\nthis harm.\n\nAnother potential issue is amplification of biases present in the evaluation method. For example, the\nobject detector classes implicitly place importance on certain types of objects, and the detector and\nthe color classifier encode biases present in their respective training data. However, we hope that our\nevaluation framework will be expanded and improved with better models, so that mitigating bias\nupstream, e.g., by diversifying training data and expanding the number of object classes, will also\nlead to a more fair evaluation."}, {"type": "heading", "lvl": 2, "value": "Technical details", "md": "##  Technical details"}, {"type": "heading", "lvl": 3, "value": "Prompt generation", "md": "###  Prompt generation"}, {"type": "text", "value": "For each task, we generate 100 random prompts and then remove duplicates, sampling uniformly\n(with specific exceptions) from:\n\n- Objects: 80 MS-COCO object names\n- Numbers: \"two\", \"three\", or \"four\"\n- Positions: \"above\", \"below\", \"left of\", \"right of\"\n- Colors: \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\", \"pink\", \"brown\", \"black\", \"white\"", "md": "For each task, we generate 100 random prompts and then remove duplicates, sampling uniformly\n(with specific exceptions) from:\n\n- Objects: 80 MS-COCO object names\n- Numbers: \"two\", \"three\", or \"four\"\n- Positions: \"above\", \"below\", \"left of\", \"right of\"\n- Colors: \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\", \"pink\", \"brown\", \"black\", \"white\""}]}, {"page": 17, "text": "Note that \u201cgray\u201d is excluded from the generation colors, because we find that gray objects tend\nto be close to black or white, which makes those prompts ambiguous. This also enables us to fill\nin the background of cropped images with gray when passed to the color classifier, since we can\nexclude gray as a potential classification option. Furthermore, for the color-related tasks (colors\nand attribute binding) we exclude the \u201cperson\u201d class from the list of candidate objects.\nThe prompt text is generated from the templates listed in Table 1. As stated, the choice between \u201ca\u201d\nand \u201can\u201d is decided based on whether the following letter is a vowel or not. Plurals are not necessarily\ngrammatical; an \u201cs\u201d is simply appended to the object name. Metadata about each prompt, required\nfor evaluation, is stored in JSON format. For example:\n{\n   \"tag\": \"colors\",\n   \"include\": [\n     {\"class\": \"bicycle\", \"count\": 1, \"color\": \"red\"}\n   ],\n   \"prompt\": \"a photo of a red bicycle\"\n}\nC.2   Image generation\nText-to-image models and baseline.        We evaluate the following models using our framework:\n       \u2022 minDALL-E [22], a port of DALL-E Mini [10], inspired by the original DALL-E [33].\n         minDALL-E is similarly autoregressive but replaces the discrete VAE with a VQGAN. The\n         generated image resolution is 256x256.\n       \u2022 Stable Diffusion v1 [36], with fi   ve iterations from v1.1 to v1.5. SDv1 is a latent space\n         diffusion model using the pretrained CLIP ViT-L/14 text encoder. The default image\n         resolution is 512x512.\n       \u2022 Stable Diffusion v2 [36], with two iterations, v2.0 and v2.1. SDv2 shares a similar\n         architecture to SDv1, but notably, uses a larger OpenCLIP ViT-H/14 text encoder trained on\n         LAION-5B. It is also trained on larger images, so the default image resolution is 768x768.\n       \u2022 IF [11], with three sizes: IF-M, IF-L, IF-XL. IF is a three-stage pixel space diffusion model.\n         We exclude the third stage upscaler in our evaluation because at this time, they recommend\n         using the Stable Diffusion 4x upscaler and the IF upscaler is not available on Huggingface.\n         There are also only two second stage sizes, M and L, so IF-XL uses the L-size stage 2 model.\n         The output resolution of the second stage is 256x256.\n       \u2022 Stable Diffusion XL [30], which massively increases the UNet backbone size from Stable\n         Diffusion v2 and incorporates two text encoders. There is a second refinement model, which\n         we do not use as it does not affect the composition of the image.\nAs well as the following real image baseline:\n       \u2022 CLIP retrieval [3] from LAION-5B. The LAION-5B dataset is indexed with CLIP ViT-\n         L/14, and for each prompt, we select the images with the highest image-text embedding\n         alignment. These images are not fixed to any specific resolution or aspect ratio.\nGeneration.     For each prompt, we generate 4 images (in the case of CLIP retrieval, we select\nthe top 4 matches). In typical usage of T2I models, there are a wide variety of parameters that\nmight be modified to improve generations, such as sampling method, number of diffusion steps,\nprompt prefixes/suffi xes, and negative prompts. We do not tune these hyperparameters due to the\ncompute cost of image generation, as well as the open-ended nature of text prompts. However,\nsome preliminary tests, such as removing the \u201ca photo of\u201d prefix or adding a \u201cuniversal\u201d (general\npurpose) negative prompt, do not seem to affect GENEVAL scores positively. We expect that careful\nexperimentation with individual models may lead to small performance gains.\nC.3   Evaluation\nObject detection.     We use the Mask2Former with Swin-S backbone trained for instance segmen-\ntation available from the MMDetection toolbox from OpenMMLab, released under an Apache 2.0\n                                                  17", "md": "# Document\n\nNote that \"gray\" is excluded from the generation colors, because we find that gray objects tend\nto be close to black or white, which makes those prompts ambiguous. This also enables us to fill\nin the background of cropped images with gray when passed to the color classifier, since we can\nexclude gray as a potential classification option. Furthermore, for the color-related tasks (colors\nand attribute binding) we exclude the \"person\" class from the list of candidate objects.\n\nThe prompt text is generated from the templates listed in Table 1. As stated, the choice between \"a\"\nand \"an\" is decided based on whether the following letter is a vowel or not. Plurals are not necessarily\ngrammatical; an \"s\" is simply appended to the object name. Metadata about each prompt, required\nfor evaluation, is stored in JSON format. For example:\n\n{\n\"tag\": \"colors\",\n\"include\": [\n{\"class\": \"bicycle\", \"count\": 1, \"color\": \"red\"}\n],\n\"prompt\": \"a photo of a red bicycle\"\n}\n\n## Image generation\n\nText-to-image models and baseline. We evaluate the following models using our framework:\n\n- minDALL-E [22], a port of DALL-E Mini [10], inspired by the original DALL-E [33]. minDALL-E is similarly autoregressive but replaces the discrete VAE with a VQGAN. The generated image resolution is 256x256.\n- Stable Diffusion v1 [36], with five iterations from v1.1 to v1.5. SDv1 is a latent space diffusion model using the pretrained CLIP ViT-L/14 text encoder. The default image resolution is 512x512.\n- Stable Diffusion v2 [36], with two iterations, v2.0 and v2.1. SDv2 shares a similar architecture to SDv1, but notably, uses a larger OpenCLIP ViT-H/14 text encoder trained on LAION-5B. It is also trained on larger images, so the default image resolution is 768x768.\n- IF [11], with three sizes: IF-M, IF-L, IF-XL. IF is a three-stage pixel space diffusion model. We exclude the third stage upscaler in our evaluation because at this time, they recommend using the Stable Diffusion 4x upscaler and the IF upscaler is not available on Huggingface. There are also only two second stage sizes, M and L, so IF-XL uses the L-size stage 2 model. The output resolution of the second stage is 256x256.\n- Stable Diffusion XL [30], which massively increases the UNet backbone size from Stable Diffusion v2 and incorporates two text encoders. There is a second refinement model, which we do not use as it does not affect the composition of the image.\n\nAs well as the following real image baseline:\n\n- CLIP retrieval [3] from LAION-5B. The LAION-5B dataset is indexed with CLIP ViT-L/14, and for each prompt, we select the images with the highest image-text embedding alignment. These images are not fixed to any specific resolution or aspect ratio.\n\nGeneration. For each prompt, we generate 4 images (in the case of CLIP retrieval, we select the top 4 matches). In typical usage of T2I models, there are a wide variety of parameters that might be modified to improve generations, such as sampling method, number of diffusion steps, prompt prefixes/suffixes, and negative prompts. We do not tune these hyperparameters due to the compute cost of image generation, as well as the open-ended nature of text prompts. However, some preliminary tests, such as removing the \"a photo of\" prefix or adding a \"universal\" (general purpose) negative prompt, do not seem to affect GENEVAL scores positively. We expect that careful experimentation with individual models may lead to small performance gains.\n\n## Evaluation\n\nObject detection. We use the Mask2Former with Swin-S backbone trained for instance segmentation available from the MMDetection toolbox from OpenMMLab, released under an Apache 2.0", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Note that \"gray\" is excluded from the generation colors, because we find that gray objects tend\nto be close to black or white, which makes those prompts ambiguous. This also enables us to fill\nin the background of cropped images with gray when passed to the color classifier, since we can\nexclude gray as a potential classification option. Furthermore, for the color-related tasks (colors\nand attribute binding) we exclude the \"person\" class from the list of candidate objects.\n\nThe prompt text is generated from the templates listed in Table 1. As stated, the choice between \"a\"\nand \"an\" is decided based on whether the following letter is a vowel or not. Plurals are not necessarily\ngrammatical; an \"s\" is simply appended to the object name. Metadata about each prompt, required\nfor evaluation, is stored in JSON format. For example:\n\n{\n\"tag\": \"colors\",\n\"include\": [\n{\"class\": \"bicycle\", \"count\": 1, \"color\": \"red\"}\n],\n\"prompt\": \"a photo of a red bicycle\"\n}", "md": "Note that \"gray\" is excluded from the generation colors, because we find that gray objects tend\nto be close to black or white, which makes those prompts ambiguous. This also enables us to fill\nin the background of cropped images with gray when passed to the color classifier, since we can\nexclude gray as a potential classification option. Furthermore, for the color-related tasks (colors\nand attribute binding) we exclude the \"person\" class from the list of candidate objects.\n\nThe prompt text is generated from the templates listed in Table 1. As stated, the choice between \"a\"\nand \"an\" is decided based on whether the following letter is a vowel or not. Plurals are not necessarily\ngrammatical; an \"s\" is simply appended to the object name. Metadata about each prompt, required\nfor evaluation, is stored in JSON format. For example:\n\n{\n\"tag\": \"colors\",\n\"include\": [\n{\"class\": \"bicycle\", \"count\": 1, \"color\": \"red\"}\n],\n\"prompt\": \"a photo of a red bicycle\"\n}"}, {"type": "heading", "lvl": 2, "value": "Image generation", "md": "## Image generation"}, {"type": "text", "value": "Text-to-image models and baseline. We evaluate the following models using our framework:\n\n- minDALL-E [22], a port of DALL-E Mini [10], inspired by the original DALL-E [33]. minDALL-E is similarly autoregressive but replaces the discrete VAE with a VQGAN. The generated image resolution is 256x256.\n- Stable Diffusion v1 [36], with five iterations from v1.1 to v1.5. SDv1 is a latent space diffusion model using the pretrained CLIP ViT-L/14 text encoder. The default image resolution is 512x512.\n- Stable Diffusion v2 [36], with two iterations, v2.0 and v2.1. SDv2 shares a similar architecture to SDv1, but notably, uses a larger OpenCLIP ViT-H/14 text encoder trained on LAION-5B. It is also trained on larger images, so the default image resolution is 768x768.\n- IF [11], with three sizes: IF-M, IF-L, IF-XL. IF is a three-stage pixel space diffusion model. We exclude the third stage upscaler in our evaluation because at this time, they recommend using the Stable Diffusion 4x upscaler and the IF upscaler is not available on Huggingface. There are also only two second stage sizes, M and L, so IF-XL uses the L-size stage 2 model. The output resolution of the second stage is 256x256.\n- Stable Diffusion XL [30], which massively increases the UNet backbone size from Stable Diffusion v2 and incorporates two text encoders. There is a second refinement model, which we do not use as it does not affect the composition of the image.\n\nAs well as the following real image baseline:\n\n- CLIP retrieval [3] from LAION-5B. The LAION-5B dataset is indexed with CLIP ViT-L/14, and for each prompt, we select the images with the highest image-text embedding alignment. These images are not fixed to any specific resolution or aspect ratio.\n\nGeneration. For each prompt, we generate 4 images (in the case of CLIP retrieval, we select the top 4 matches). In typical usage of T2I models, there are a wide variety of parameters that might be modified to improve generations, such as sampling method, number of diffusion steps, prompt prefixes/suffixes, and negative prompts. We do not tune these hyperparameters due to the compute cost of image generation, as well as the open-ended nature of text prompts. However, some preliminary tests, such as removing the \"a photo of\" prefix or adding a \"universal\" (general purpose) negative prompt, do not seem to affect GENEVAL scores positively. We expect that careful experimentation with individual models may lead to small performance gains.", "md": "Text-to-image models and baseline. We evaluate the following models using our framework:\n\n- minDALL-E [22], a port of DALL-E Mini [10], inspired by the original DALL-E [33]. minDALL-E is similarly autoregressive but replaces the discrete VAE with a VQGAN. The generated image resolution is 256x256.\n- Stable Diffusion v1 [36], with five iterations from v1.1 to v1.5. SDv1 is a latent space diffusion model using the pretrained CLIP ViT-L/14 text encoder. The default image resolution is 512x512.\n- Stable Diffusion v2 [36], with two iterations, v2.0 and v2.1. SDv2 shares a similar architecture to SDv1, but notably, uses a larger OpenCLIP ViT-H/14 text encoder trained on LAION-5B. It is also trained on larger images, so the default image resolution is 768x768.\n- IF [11], with three sizes: IF-M, IF-L, IF-XL. IF is a three-stage pixel space diffusion model. We exclude the third stage upscaler in our evaluation because at this time, they recommend using the Stable Diffusion 4x upscaler and the IF upscaler is not available on Huggingface. There are also only two second stage sizes, M and L, so IF-XL uses the L-size stage 2 model. The output resolution of the second stage is 256x256.\n- Stable Diffusion XL [30], which massively increases the UNet backbone size from Stable Diffusion v2 and incorporates two text encoders. There is a second refinement model, which we do not use as it does not affect the composition of the image.\n\nAs well as the following real image baseline:\n\n- CLIP retrieval [3] from LAION-5B. The LAION-5B dataset is indexed with CLIP ViT-L/14, and for each prompt, we select the images with the highest image-text embedding alignment. These images are not fixed to any specific resolution or aspect ratio.\n\nGeneration. For each prompt, we generate 4 images (in the case of CLIP retrieval, we select the top 4 matches). In typical usage of T2I models, there are a wide variety of parameters that might be modified to improve generations, such as sampling method, number of diffusion steps, prompt prefixes/suffixes, and negative prompts. We do not tune these hyperparameters due to the compute cost of image generation, as well as the open-ended nature of text prompts. However, some preliminary tests, such as removing the \"a photo of\" prefix or adding a \"universal\" (general purpose) negative prompt, do not seem to affect GENEVAL scores positively. We expect that careful experimentation with individual models may lead to small performance gains."}, {"type": "heading", "lvl": 2, "value": "Evaluation", "md": "## Evaluation"}, {"type": "text", "value": "Object detection. We use the Mask2Former with Swin-S backbone trained for instance segmentation available from the MMDetection toolbox from OpenMMLab, released under an Apache 2.0", "md": "Object detection. We use the Mask2Former with Swin-S backbone trained for instance segmentation available from the MMDetection toolbox from OpenMMLab, released under an Apache 2.0"}]}, {"page": 18, "text": "license. For all tasks except for counting, we take objects with a confidence above the default\nthreshold of 0.3. For counting, we find that a higher threshold of 0.9 gives the highest human agree-\nment results (Figure 8), particularly due to multiple bounding boxes being generated for overlapping\nobjects of the same type. We also experiment with non-maximal suppression (NMS) within each\nclass but find that this does not raise human agreement. For all but the counting task, there is also no\nupper limit on the number of objects. For example, for the prompt \u201ca photo of a book and a laptop\u201d,\nthe image is counted as correct even if there are three books and two laptops.\nRelative position.    The relative position between objects for the position task is evaluated from\nthe bounding box coordinates. Prior works [9, 14] use a simple heuristic where the relative position\nis determined by the difference between bounding box centroids: if object A is centered at (xA, yA)\nand object B is centered at (xB, yB), then:\n                                    xB > xA =\u21d2       B is right of A\n                                    xB < xA =\u21d2       B is left of A\n                                    yB > yA =\u21d2       B is below A\n                                    yB < yA =\u21d2       B is above A.\nHowever, qualitatively, some images generate with both objects overlapping or merged. In these\ncases, human annotators do not perceive the objects as being offset from one another (whereas with\nthis heuristic, that is only the case if the objects are aligned pixel-perfectly). Thus, we add another\nterm denoting the minimum threshold before two objects are considered \u201cvisibly offset\u201d in a direction.\nIf object A has dimensions wA \u00d7 hA and object B has dimensions wB \u00d7 hB, then with a distance\nthreshold of c we have:\n                           xB > xA + c(wA + wB) =\u21d2           B is right of A\n                           xB < xA \u2212     c(wA + wB) =\u21d2       B is left of A\n                             yB > yA + c(hA + hB) =\u21d2         B is below A\n                             yB > yA \u2212   c(hA + hB) =\u21d2       B is above A.\nWe go with this method for evaluating relative position, and find that c = 0.1 optimizes human\nagreement.\nColor classification.   We use the CLIP ViT-L/14 model from OpenAI [31], released under an MIT\nlicense, as a zeroshot color classifier. We also test this with the ViT-B/32 model, but find it shows\nlower human agreement. For each candidate object, the image is cropped down to the bounding\nbox of the object, and the segmentation mask is used to replace all background pixels with gray.\nWe find that this shows higher agreement with human judgment than using the whole image or not\nreplacing the background (Table 4). For the classification, we classify between the 10 candidate\ncolors described above in C.1, with the following class templates:\n       \u2022 \u201ca photo of a [COLOR] [OBJECT]\u201d\n       \u2022 \u201ca photo of a [COLOR]-colored [OBJECT]\u201d\n       \u2022 \u201ca photo of a [COLOR] object\u201d\nwhere [COLOR] is replaced with the color name and [OBJECT] is replaced with the detected object\u2019s\nname. The normalized prompt embeddings for each color are averaged, and the prompt embedding\nwith the highest cosine similarity to the cropped image determines the predicted color.\nCLIPScore baseline.      We compare our evaluation framework against CLIPScore [15]. This boils\ndown to the cosine similarity between the prompt and image CLIP embeddings, clipped to be at\nleast 0. The original paper multiplies the score by 2.5 to span a wider natural range; since this is\nhas no real impact on comparing CLIPScore values, we simply scale the similarity to be between 0\n(orthogonal) and 100 (perfectly aligned).\n[15] used the CLIP ViT-B/32 model from [31] available at the time; since newer improved CLIP\nmodels have been developed more recently, we test several different potential models and choose the\nmodel most aligned with human judgment as a baseline. We compare the ViT-B/32 model against\nthe larger ViT-L/14 model [31], an even larger ViT-H/14 model from OpenCLIP [8], and the EVA-\n02-CLIP model from [12]. Table 5 displays the results of this comparison. Overall, the OpenCLIP\n                                                   18", "md": "# Document\n\nlicense. For all tasks except for counting, we take objects with a confidence above the default threshold of 0.3. For counting, we find that a higher threshold of 0.9 gives the highest human agreement results (Figure 8), particularly due to multiple bounding boxes being generated for overlapping objects of the same type. We also experiment with non-maximal suppression (NMS) within each class but find that this does not raise human agreement. For all but the counting task, there is also no upper limit on the number of objects. For example, for the prompt \u201ca photo of a book and a laptop\u201d, the image is counted as correct even if there are three books and two laptops.\n\nRelative position. The relative position between objects for the position task is evaluated from the bounding box coordinates. Prior works [9, 14] use a simple heuristic where the relative position is determined by the difference between bounding box centroids: if object A is centered at (xA, yA) and object B is centered at (xB, yB), then:\n\n$$\n\\begin{align*}\n&xB > xA \\Rightarrow \\text{B is right of A} \\\\\n&xB < xA \\Rightarrow \\text{B is left of A} \\\\\n&yB > yA \\Rightarrow \\text{B is below A} \\\\\n&yB < yA \\Rightarrow \\text{B is above A}\n\\end{align*}\n$$\nHowever, qualitatively, some images generate with both objects overlapping or merged. In these cases, human annotators do not perceive the objects as being offset from one another (whereas with this heuristic, that is only the case if the objects are aligned pixel-perfectly). Thus, we add another term denoting the minimum threshold before two objects are considered \u201cvisibly offset\u201d in a direction. If object A has dimensions wA \u00d7 hA and object B has dimensions wB \u00d7 hB, then with a distance threshold of c we have:\n\n$$\n\\begin{align*}\n&xB > xA + c(wA + wB) \\Rightarrow \\text{B is right of A} \\\\\n&xB < xA - c(wA + wB) \\Rightarrow \\text{B is left of A} \\\\\n&yB > yA + c(hA + hB) \\Rightarrow \\text{B is below A} \\\\\n&yB > yA - c(hA + hB) \\Rightarrow \\text{B is above A}\n\\end{align*}\n$$\nWe go with this method for evaluating relative position, and find that c = 0.1 optimizes human agreement.\n\nColor classification. We use the CLIP ViT-L/14 model from OpenAI [31], released under an MIT license, as a zeroshot color classifier. We also test this with the ViT-B/32 model, but find it shows lower human agreement. For each candidate object, the image is cropped down to the bounding box of the object, and the segmentation mask is used to replace all background pixels with gray. We find that this shows higher agreement with human judgment than using the whole image or not replacing the background (Table 4). For the classification, we classify between the 10 candidate colors described above in C.1, with the following class templates:\n\n- \u201ca photo of a [COLOR] [OBJECT]\u201d\n- \u201ca photo of a [COLOR]-colored [OBJECT]\u201d\n- \u201ca photo of a [COLOR] object\u201d\n\nwhere [COLOR] is replaced with the color name and [OBJECT] is replaced with the detected object\u2019s name. The normalized prompt embeddings for each color are averaged, and the prompt embedding with the highest cosine similarity to the cropped image determines the predicted color.\n\nCLIPScore baseline. We compare our evaluation framework against CLIPScore [15]. This boils down to the cosine similarity between the prompt and image CLIP embeddings, clipped to be at least 0. The original paper multiplies the score by 2.5 to span a wider natural range; since this is has no real impact on comparing CLIPScore values, we simply scale the similarity to be between 0 (orthogonal) and 100 (perfectly aligned).\n\n[15] used the CLIP ViT-B/32 model from [31] available at the time; since newer improved CLIP models have been developed more recently, we test several different potential models and choose the model most aligned with human judgment as a baseline. We compare the ViT-B/32 model against the larger ViT-L/14 model [31], an even larger ViT-H/14 model from OpenCLIP [8], and the EVA-02-CLIP model from [12]. Table 5 displays the results of this comparison. Overall, the OpenCLIP", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "license. For all tasks except for counting, we take objects with a confidence above the default threshold of 0.3. For counting, we find that a higher threshold of 0.9 gives the highest human agreement results (Figure 8), particularly due to multiple bounding boxes being generated for overlapping objects of the same type. We also experiment with non-maximal suppression (NMS) within each class but find that this does not raise human agreement. For all but the counting task, there is also no upper limit on the number of objects. For example, for the prompt \u201ca photo of a book and a laptop\u201d, the image is counted as correct even if there are three books and two laptops.\n\nRelative position. The relative position between objects for the position task is evaluated from the bounding box coordinates. Prior works [9, 14] use a simple heuristic where the relative position is determined by the difference between bounding box centroids: if object A is centered at (xA, yA) and object B is centered at (xB, yB), then:\n\n$$\n\\begin{align*}\n&xB > xA \\Rightarrow \\text{B is right of A} \\\\\n&xB < xA \\Rightarrow \\text{B is left of A} \\\\\n&yB > yA \\Rightarrow \\text{B is below A} \\\\\n&yB < yA \\Rightarrow \\text{B is above A}\n\\end{align*}\n$$\nHowever, qualitatively, some images generate with both objects overlapping or merged. In these cases, human annotators do not perceive the objects as being offset from one another (whereas with this heuristic, that is only the case if the objects are aligned pixel-perfectly). Thus, we add another term denoting the minimum threshold before two objects are considered \u201cvisibly offset\u201d in a direction. If object A has dimensions wA \u00d7 hA and object B has dimensions wB \u00d7 hB, then with a distance threshold of c we have:\n\n$$\n\\begin{align*}\n&xB > xA + c(wA + wB) \\Rightarrow \\text{B is right of A} \\\\\n&xB < xA - c(wA + wB) \\Rightarrow \\text{B is left of A} \\\\\n&yB > yA + c(hA + hB) \\Rightarrow \\text{B is below A} \\\\\n&yB > yA - c(hA + hB) \\Rightarrow \\text{B is above A}\n\\end{align*}\n$$\nWe go with this method for evaluating relative position, and find that c = 0.1 optimizes human agreement.\n\nColor classification. We use the CLIP ViT-L/14 model from OpenAI [31], released under an MIT license, as a zeroshot color classifier. We also test this with the ViT-B/32 model, but find it shows lower human agreement. For each candidate object, the image is cropped down to the bounding box of the object, and the segmentation mask is used to replace all background pixels with gray. We find that this shows higher agreement with human judgment than using the whole image or not replacing the background (Table 4). For the classification, we classify between the 10 candidate colors described above in C.1, with the following class templates:\n\n- \u201ca photo of a [COLOR] [OBJECT]\u201d\n- \u201ca photo of a [COLOR]-colored [OBJECT]\u201d\n- \u201ca photo of a [COLOR] object\u201d\n\nwhere [COLOR] is replaced with the color name and [OBJECT] is replaced with the detected object\u2019s name. The normalized prompt embeddings for each color are averaged, and the prompt embedding with the highest cosine similarity to the cropped image determines the predicted color.\n\nCLIPScore baseline. We compare our evaluation framework against CLIPScore [15]. This boils down to the cosine similarity between the prompt and image CLIP embeddings, clipped to be at least 0. The original paper multiplies the score by 2.5 to span a wider natural range; since this is has no real impact on comparing CLIPScore values, we simply scale the similarity to be between 0 (orthogonal) and 100 (perfectly aligned).\n\n[15] used the CLIP ViT-B/32 model from [31] available at the time; since newer improved CLIP models have been developed more recently, we test several different potential models and choose the model most aligned with human judgment as a baseline. We compare the ViT-B/32 model against the larger ViT-L/14 model [31], an even larger ViT-H/14 model from OpenCLIP [8], and the EVA-02-CLIP model from [12]. Table 5 displays the results of this comparison. Overall, the OpenCLIP", "md": "license. For all tasks except for counting, we take objects with a confidence above the default threshold of 0.3. For counting, we find that a higher threshold of 0.9 gives the highest human agreement results (Figure 8), particularly due to multiple bounding boxes being generated for overlapping objects of the same type. We also experiment with non-maximal suppression (NMS) within each class but find that this does not raise human agreement. For all but the counting task, there is also no upper limit on the number of objects. For example, for the prompt \u201ca photo of a book and a laptop\u201d, the image is counted as correct even if there are three books and two laptops.\n\nRelative position. The relative position between objects for the position task is evaluated from the bounding box coordinates. Prior works [9, 14] use a simple heuristic where the relative position is determined by the difference between bounding box centroids: if object A is centered at (xA, yA) and object B is centered at (xB, yB), then:\n\n$$\n\\begin{align*}\n&xB > xA \\Rightarrow \\text{B is right of A} \\\\\n&xB < xA \\Rightarrow \\text{B is left of A} \\\\\n&yB > yA \\Rightarrow \\text{B is below A} \\\\\n&yB < yA \\Rightarrow \\text{B is above A}\n\\end{align*}\n$$\nHowever, qualitatively, some images generate with both objects overlapping or merged. In these cases, human annotators do not perceive the objects as being offset from one another (whereas with this heuristic, that is only the case if the objects are aligned pixel-perfectly). Thus, we add another term denoting the minimum threshold before two objects are considered \u201cvisibly offset\u201d in a direction. If object A has dimensions wA \u00d7 hA and object B has dimensions wB \u00d7 hB, then with a distance threshold of c we have:\n\n$$\n\\begin{align*}\n&xB > xA + c(wA + wB) \\Rightarrow \\text{B is right of A} \\\\\n&xB < xA - c(wA + wB) \\Rightarrow \\text{B is left of A} \\\\\n&yB > yA + c(hA + hB) \\Rightarrow \\text{B is below A} \\\\\n&yB > yA - c(hA + hB) \\Rightarrow \\text{B is above A}\n\\end{align*}\n$$\nWe go with this method for evaluating relative position, and find that c = 0.1 optimizes human agreement.\n\nColor classification. We use the CLIP ViT-L/14 model from OpenAI [31], released under an MIT license, as a zeroshot color classifier. We also test this with the ViT-B/32 model, but find it shows lower human agreement. For each candidate object, the image is cropped down to the bounding box of the object, and the segmentation mask is used to replace all background pixels with gray. We find that this shows higher agreement with human judgment than using the whole image or not replacing the background (Table 4). For the classification, we classify between the 10 candidate colors described above in C.1, with the following class templates:\n\n- \u201ca photo of a [COLOR] [OBJECT]\u201d\n- \u201ca photo of a [COLOR]-colored [OBJECT]\u201d\n- \u201ca photo of a [COLOR] object\u201d\n\nwhere [COLOR] is replaced with the color name and [OBJECT] is replaced with the detected object\u2019s name. The normalized prompt embeddings for each color are averaged, and the prompt embedding with the highest cosine similarity to the cropped image determines the predicted color.\n\nCLIPScore baseline. We compare our evaluation framework against CLIPScore [15]. This boils down to the cosine similarity between the prompt and image CLIP embeddings, clipped to be at least 0. The original paper multiplies the score by 2.5 to span a wider natural range; since this is has no real impact on comparing CLIPScore values, we simply scale the similarity to be between 0 (orthogonal) and 100 (perfectly aligned).\n\n[15] used the CLIP ViT-B/32 model from [31] available at the time; since newer improved CLIP models have been developed more recently, we test several different potential models and choose the model most aligned with human judgment as a baseline. We compare the ViT-B/32 model against the larger ViT-L/14 model [31], an even larger ViT-H/14 model from OpenCLIP [8], and the EVA-02-CLIP model from [12]. Table 5 displays the results of this comparison. Overall, the OpenCLIP"}]}, {"page": 19, "text": "          CLIP                                 Single      Two                                           Attribute  ImageNet-1k\n          Model                    Overall     object     object    Counting      Colors     Position     binding     zeroshot\n          ViT-B/32 [31]             0.773      0.991      0.720       0.604       0.811       0.790        0.723       0.632\n          ViT-L/14 [31]             0.760      0.985      0.692       0.583       0.796       0.786        0.719       0.753\n          ViT-H/14 [8]              0.798      0.986      0.828       0.734       0.827       0.682        0.732       0.780\n          EVA-02-E/14+ [12]         0.600      0.992      0.628       0.612       0.804       0.240        0.322       0.820\n          GENEVAL                   0.834      0.953      0.799       0.823       0.808       0.823        0.799\n          Interannotator            0.878      0.990      0.872       0.901       0.854       0.797        0.853\nTable 5: Human study agreement with CLIPScore using various CLIP models. While the OpenCLIP\nViT-H/14 obtains the best overall agreement with human judgment, it is significantly worse at\nevaluating position, suggesting a qualitative difference in training data compared to [31]. In\naddition while EVA-02-CLIP has the highest accuracy on ImageNet, this does not appear to transfer\nto tasks aside from single object recognition.\nViT-H/14 agrees most with human annotators \u2014 however, none of the models are consistently better\nor worse for CLIPScore across all our tasks.\nD       Human study details\nThe human study was conducted through Amazon Mechanical Turk. Annotators were asked to\nanswer a series of questions about a displayed image, as enumerated below and shown in Figure 11.\nThere were very limited potential participant risks, if they were to be exposed to an image that was\ndisturbing or not safe for work (NSFW). However, the images we used were sampled from templated\nprompts which were not in themselves offensive. Furthermore, all images generated by our models or\nretrieved by CLIP were passed through NSFW filters which would black out any potentially unsafe\nimages.\nWe annotated 400 images generated from 100 randomly chosen prompts for each of Stable Diffusion\nv2.1, IF-XL, and CLIP retrieval from LAION-5B, with 5 annotations for each image. Depending\non the prompt (which affected the questions asked), each annotation took about 1\u20132 minutes. Each\ncrowdworker was paid an estimated $15 per hour. In total, across all 6,000 annotations and including\nMechanical Turk fees, $3,600 were spent on our human study.\nD.1      Full text\nThe task consists of general instructions and a set of questions specific to each image. For each\ninstance of the task, the \u201c[OBJECT]\u201d in each question is replaced with the names of the objects from\nthe prompt. If the prompt consists of two types of objects (e.g., \u201ca photo of a bird and a skateboard\u201d),\nquestions 2 and 3 refer to each type respectively. If the prompt only has one type of object (e.g., \u201ca\nphoto of four handbags\u201d), question 3 and question 4 are skipped.\nInstructions.          Thanks for participating in this HIT! You will view an image, potentially generated\nby an AI image generator. Please answer all of the following questions to the best of your abilities.\nFor each image you may be asked about:\n          \u2022 List of objects: What objects are you able to discern in the image?\n          \u2022 Number of objects: How many objects of a particular type are visible, either fully or\n             partially?\n          \u2022 Color of objects: What color(s) would you consider the object(s) to be?\n          \u2022 Realism: How realistic or structurally correct are the objects?\n          \u2022 Relative position: What is the relative position in the image of one object to another?\n          \u2022 Caption fit: How well does the image correspond to a provided caption?\nA few notes:\n          \u2022 Take a look at the examples if any of the questions are unclear to you.\n                                                                      19", "md": "|Model|Overall|Single object|Two object|Counting|Colors|Position|Binding|Zeroshot|\n|---|---|---|---|---|---|---|---|---|\n|ViT-B/32 [31]|0.773|0.991|0.720|0.604|0.811|0.790|0.723|0.632|\n|ViT-L/14 [31]|0.760|0.985|0.692|0.583|0.796|0.786|0.719|0.753|\n|ViT-H/14 [8]|0.798|0.986|0.828|0.734|0.827|0.682|0.732|0.780|\n|EVA-02-E/14+ [12]|0.600|0.992|0.628|0.612|0.804|0.240|0.322|0.820|\n|GENEVAL|0.834|0.953|0.799|0.823|0.808|0.823|0.799| |\n|Interannotator|0.878|0.990|0.872|0.901|0.854|0.797|0.853| |\n\nTable 5: Human study agreement with CLIPScore using various CLIP models. While the OpenCLIP ViT-H/14 obtains the best overall agreement with human judgment, it is significantly worse at evaluating position, suggesting a qualitative difference in training data compared to [31]. In addition while EVA-02-CLIP has the highest accuracy on ImageNet, this does not appear to transfer to tasks aside from single object recognition.\n\nViT-H/14 agrees most with human annotators \u2014 however, none of the models are consistently better or worse for CLIPScore across all our tasks.\n\n## Human study details\n\nThe human study was conducted through Amazon Mechanical Turk. Annotators were asked to answer a series of questions about a displayed image, as enumerated below and shown in Figure 11. There were very limited potential participant risks, if they were to be exposed to an image that was disturbing or not safe for work (NSFW). However, the images we used were sampled from templated prompts which were not in themselves offensive. Furthermore, all images generated by our models or retrieved by CLIP were passed through NSFW filters which would black out any potentially unsafe images.\n\nWe annotated 400 images generated from 100 randomly chosen prompts for each of Stable Diffusion v2.1, IF-XL, and CLIP retrieval from LAION-5B, with 5 annotations for each image. Depending on the prompt (which affected the questions asked), each annotation took about 1\u20132 minutes. Each crowdworker was paid an estimated $15 per hour. In total, across all 6,000 annotations and including Mechanical Turk fees, $3,600 were spent on our human study.\n\n### Full text\n\nThe task consists of general instructions and a set of questions specific to each image. For each instance of the task, the \u201c[OBJECT]\u201d in each question is replaced with the names of the objects from the prompt. If the prompt consists of two types of objects (e.g., \u201ca photo of a bird and a skateboard\u201d), questions 2 and 3 refer to each type respectively. If the prompt only has one type of object (e.g., \u201ca photo of four handbags\u201d), question 3 and question 4 are skipped.\n\nInstructions. Thanks for participating in this HIT! You will view an image, potentially generated by an AI image generator. Please answer all of the following questions to the best of your abilities. For each image you may be asked about:\n\n- List of objects: What objects are you able to discern in the image?\n- Number of objects: How many objects of a particular type are visible, either fully or partially?\n- Color of objects: What color(s) would you consider the object(s) to be?\n- Realism: How realistic or structurally correct are the objects?\n- Relative position: What is the relative position in the image of one object to another?\n- Caption fit: How well does the image correspond to a provided caption?\n\nA few notes:\n\n- Take a look at the examples if any of the questions are unclear to you.", "images": [], "items": [{"type": "table", "rows": [["Model", "Overall", "Single object", "Two object", "Counting", "Colors", "Position", "Binding", "Zeroshot"], ["ViT-B/32 [31]", "0.773", "0.991", "0.720", "0.604", "0.811", "0.790", "0.723", "0.632"], ["ViT-L/14 [31]", "0.760", "0.985", "0.692", "0.583", "0.796", "0.786", "0.719", "0.753"], ["ViT-H/14 [8]", "0.798", "0.986", "0.828", "0.734", "0.827", "0.682", "0.732", "0.780"], ["EVA-02-E/14+ [12]", "0.600", "0.992", "0.628", "0.612", "0.804", "0.240", "0.322", "0.820"], ["GENEVAL", "0.834", "0.953", "0.799", "0.823", "0.808", "0.823", "0.799", ""], ["Interannotator", "0.878", "0.990", "0.872", "0.901", "0.854", "0.797", "0.853", ""]], "md": "|Model|Overall|Single object|Two object|Counting|Colors|Position|Binding|Zeroshot|\n|---|---|---|---|---|---|---|---|---|\n|ViT-B/32 [31]|0.773|0.991|0.720|0.604|0.811|0.790|0.723|0.632|\n|ViT-L/14 [31]|0.760|0.985|0.692|0.583|0.796|0.786|0.719|0.753|\n|ViT-H/14 [8]|0.798|0.986|0.828|0.734|0.827|0.682|0.732|0.780|\n|EVA-02-E/14+ [12]|0.600|0.992|0.628|0.612|0.804|0.240|0.322|0.820|\n|GENEVAL|0.834|0.953|0.799|0.823|0.808|0.823|0.799| |\n|Interannotator|0.878|0.990|0.872|0.901|0.854|0.797|0.853| |", "isPerfectTable": true, "csv": "\"Model\",\"Overall\",\"Single object\",\"Two object\",\"Counting\",\"Colors\",\"Position\",\"Binding\",\"Zeroshot\"\n\"ViT-B/32 [31]\",\"0.773\",\"0.991\",\"0.720\",\"0.604\",\"0.811\",\"0.790\",\"0.723\",\"0.632\"\n\"ViT-L/14 [31]\",\"0.760\",\"0.985\",\"0.692\",\"0.583\",\"0.796\",\"0.786\",\"0.719\",\"0.753\"\n\"ViT-H/14 [8]\",\"0.798\",\"0.986\",\"0.828\",\"0.734\",\"0.827\",\"0.682\",\"0.732\",\"0.780\"\n\"EVA-02-E/14+ [12]\",\"0.600\",\"0.992\",\"0.628\",\"0.612\",\"0.804\",\"0.240\",\"0.322\",\"0.820\"\n\"GENEVAL\",\"0.834\",\"0.953\",\"0.799\",\"0.823\",\"0.808\",\"0.823\",\"0.799\",\"\"\n\"Interannotator\",\"0.878\",\"0.990\",\"0.872\",\"0.901\",\"0.854\",\"0.797\",\"0.853\",\"\""}, {"type": "text", "value": "Table 5: Human study agreement with CLIPScore using various CLIP models. While the OpenCLIP ViT-H/14 obtains the best overall agreement with human judgment, it is significantly worse at evaluating position, suggesting a qualitative difference in training data compared to [31]. In addition while EVA-02-CLIP has the highest accuracy on ImageNet, this does not appear to transfer to tasks aside from single object recognition.\n\nViT-H/14 agrees most with human annotators \u2014 however, none of the models are consistently better or worse for CLIPScore across all our tasks.", "md": "Table 5: Human study agreement with CLIPScore using various CLIP models. While the OpenCLIP ViT-H/14 obtains the best overall agreement with human judgment, it is significantly worse at evaluating position, suggesting a qualitative difference in training data compared to [31]. In addition while EVA-02-CLIP has the highest accuracy on ImageNet, this does not appear to transfer to tasks aside from single object recognition.\n\nViT-H/14 agrees most with human annotators \u2014 however, none of the models are consistently better or worse for CLIPScore across all our tasks."}, {"type": "heading", "lvl": 2, "value": "Human study details", "md": "## Human study details"}, {"type": "text", "value": "The human study was conducted through Amazon Mechanical Turk. Annotators were asked to answer a series of questions about a displayed image, as enumerated below and shown in Figure 11. There were very limited potential participant risks, if they were to be exposed to an image that was disturbing or not safe for work (NSFW). However, the images we used were sampled from templated prompts which were not in themselves offensive. Furthermore, all images generated by our models or retrieved by CLIP were passed through NSFW filters which would black out any potentially unsafe images.\n\nWe annotated 400 images generated from 100 randomly chosen prompts for each of Stable Diffusion v2.1, IF-XL, and CLIP retrieval from LAION-5B, with 5 annotations for each image. Depending on the prompt (which affected the questions asked), each annotation took about 1\u20132 minutes. Each crowdworker was paid an estimated $15 per hour. In total, across all 6,000 annotations and including Mechanical Turk fees, $3,600 were spent on our human study.", "md": "The human study was conducted through Amazon Mechanical Turk. Annotators were asked to answer a series of questions about a displayed image, as enumerated below and shown in Figure 11. There were very limited potential participant risks, if they were to be exposed to an image that was disturbing or not safe for work (NSFW). However, the images we used were sampled from templated prompts which were not in themselves offensive. Furthermore, all images generated by our models or retrieved by CLIP were passed through NSFW filters which would black out any potentially unsafe images.\n\nWe annotated 400 images generated from 100 randomly chosen prompts for each of Stable Diffusion v2.1, IF-XL, and CLIP retrieval from LAION-5B, with 5 annotations for each image. Depending on the prompt (which affected the questions asked), each annotation took about 1\u20132 minutes. Each crowdworker was paid an estimated $15 per hour. In total, across all 6,000 annotations and including Mechanical Turk fees, $3,600 were spent on our human study."}, {"type": "heading", "lvl": 3, "value": "Full text", "md": "### Full text"}, {"type": "text", "value": "The task consists of general instructions and a set of questions specific to each image. For each instance of the task, the \u201c[OBJECT]\u201d in each question is replaced with the names of the objects from the prompt. If the prompt consists of two types of objects (e.g., \u201ca photo of a bird and a skateboard\u201d), questions 2 and 3 refer to each type respectively. If the prompt only has one type of object (e.g., \u201ca photo of four handbags\u201d), question 3 and question 4 are skipped.\n\nInstructions. Thanks for participating in this HIT! You will view an image, potentially generated by an AI image generator. Please answer all of the following questions to the best of your abilities. For each image you may be asked about:\n\n- List of objects: What objects are you able to discern in the image?\n- Number of objects: How many objects of a particular type are visible, either fully or partially?\n- Color of objects: What color(s) would you consider the object(s) to be?\n- Realism: How realistic or structurally correct are the objects?\n- Relative position: What is the relative position in the image of one object to another?\n- Caption fit: How well does the image correspond to a provided caption?\n\nA few notes:\n\n- Take a look at the examples if any of the questions are unclear to you.", "md": "The task consists of general instructions and a set of questions specific to each image. For each instance of the task, the \u201c[OBJECT]\u201d in each question is replaced with the names of the objects from the prompt. If the prompt consists of two types of objects (e.g., \u201ca photo of a bird and a skateboard\u201d), questions 2 and 3 refer to each type respectively. If the prompt only has one type of object (e.g., \u201ca photo of four handbags\u201d), question 3 and question 4 are skipped.\n\nInstructions. Thanks for participating in this HIT! You will view an image, potentially generated by an AI image generator. Please answer all of the following questions to the best of your abilities. For each image you may be asked about:\n\n- List of objects: What objects are you able to discern in the image?\n- Number of objects: How many objects of a particular type are visible, either fully or partially?\n- Color of objects: What color(s) would you consider the object(s) to be?\n- Realism: How realistic or structurally correct are the objects?\n- Relative position: What is the relative position in the image of one object to another?\n- Caption fit: How well does the image correspond to a provided caption?\n\nA few notes:\n\n- Take a look at the examples if any of the questions are unclear to you."}]}, {"page": 20, "text": "        \u2022 Complete question 1 before moving on to other questions. If you are unable to recognize a\n          particular object, guess what it is or use a brief description.\n        \u2022 Additional questions may appear based on your previous answers. Make sure to answer all\n          questions shown.\n        \u2022 When asked about color, select at least one color that best matches the true color of the\n          object(s).\n        \u2022 Do not consider color when deciding how realistic an object is; focus on the shape and form\n          compared to real life.\n        \u2022 When asked about position, an object can be both above/below and to the left/right of another\n          object. Mark position from your perspective looking at the 2D image, not from the camera\n          perspective.\n        \u2022 The image may be just a black square, in which case, you should say there are no objects.\n        \u2022 Please answer with care: Some HITs will be checked by hand, and work may be rejected if\n          there are too many errors.\nQuestion 1.     Briefly list all the objects visible in the image, separated by commas. If there are no\nobjects in the image, write \u201cnone\u201d.\nQuestion 2a/3a.      How many [OBJECT]s are in the image?\nQuestion 2b/3b.      What color of [OBJECT]s are in the image? Choose all that apply.\nQuestion 2c/3c.     How realistic are the [OBJECT]s in the image? (1\u20133) Ignore the color of the object,\nand focus on any visible defects in the shape or structure.\nQuestion 4a.     Is the [OBJECT B] to the left or right of the [OBJECT A]? Provide your answer as a\nviewer of the image, not from the \u201ccamera\u2019s perspective\u201d.\nQuestion 4b.      Is the [OBJECT B] to above or below the [OBJECT A]? Provide your answer as a\nviewer of the image, not from the \u201ccamera\u2019s perspective\u201d.\n                                                    20", "md": "- Complete question 1 before moving on to other questions. If you are unable to recognize a particular object, guess what it is or use a brief description.\n- Additional questions may appear based on your previous answers. Make sure to answer all questions shown.\n- When asked about color, select at least one color that best matches the true color of the object(s).\n- Do not consider color when deciding how realistic an object is; focus on the shape and form compared to real life.\n- When asked about position, an object can be both above/below and to the left/right of another object. Mark position from your perspective looking at the 2D image, not from the camera perspective.\n- The image may be just a black square, in which case, you should say there are no objects.\n- Please answer with care: Some HITs will be checked by hand, and work may be rejected if there are too many errors.\n\nQuestion 1. Briefly list all the objects visible in the image, separated by commas. If there are no objects in the image, write \u201cnone\u201d.\n\nQuestion 2a/3a. How many $$\\text{[OBJECT]}$$s are in the image?\n\nQuestion 2b/3b. What color of $$\\text{[OBJECT]}$$s are in the image? Choose all that apply.\n\nQuestion 2c/3c. How realistic are the $$\\text{[OBJECT]}$$s in the image? (1\u20133) Ignore the color of the object, and focus on any visible defects in the shape or structure.\n\nQuestion 4a. Is the $$\\text{[OBJECT B]}$$ to the left or right of the $$\\text{[OBJECT A]}$$? Provide your answer as a viewer of the image, not from the \u201ccamera\u2019s perspective\u201d.\n\nQuestion 4b. Is the $$\\text{[OBJECT B]}$$ to above or below the $$\\text{[OBJECT A]}$$? Provide your answer as a viewer of the image, not from the \u201ccamera\u2019s perspective\u201d.\n\n20", "images": [], "items": [{"type": "text", "value": "- Complete question 1 before moving on to other questions. If you are unable to recognize a particular object, guess what it is or use a brief description.\n- Additional questions may appear based on your previous answers. Make sure to answer all questions shown.\n- When asked about color, select at least one color that best matches the true color of the object(s).\n- Do not consider color when deciding how realistic an object is; focus on the shape and form compared to real life.\n- When asked about position, an object can be both above/below and to the left/right of another object. Mark position from your perspective looking at the 2D image, not from the camera perspective.\n- The image may be just a black square, in which case, you should say there are no objects.\n- Please answer with care: Some HITs will be checked by hand, and work may be rejected if there are too many errors.\n\nQuestion 1. Briefly list all the objects visible in the image, separated by commas. If there are no objects in the image, write \u201cnone\u201d.\n\nQuestion 2a/3a. How many $$\\text{[OBJECT]}$$s are in the image?\n\nQuestion 2b/3b. What color of $$\\text{[OBJECT]}$$s are in the image? Choose all that apply.\n\nQuestion 2c/3c. How realistic are the $$\\text{[OBJECT]}$$s in the image? (1\u20133) Ignore the color of the object, and focus on any visible defects in the shape or structure.\n\nQuestion 4a. Is the $$\\text{[OBJECT B]}$$ to the left or right of the $$\\text{[OBJECT A]}$$? Provide your answer as a viewer of the image, not from the \u201ccamera\u2019s perspective\u201d.\n\nQuestion 4b. Is the $$\\text{[OBJECT B]}$$ to above or below the $$\\text{[OBJECT A]}$$? Provide your answer as a viewer of the image, not from the \u201ccamera\u2019s perspective\u201d.\n\n20", "md": "- Complete question 1 before moving on to other questions. If you are unable to recognize a particular object, guess what it is or use a brief description.\n- Additional questions may appear based on your previous answers. Make sure to answer all questions shown.\n- When asked about color, select at least one color that best matches the true color of the object(s).\n- Do not consider color when deciding how realistic an object is; focus on the shape and form compared to real life.\n- When asked about position, an object can be both above/below and to the left/right of another object. Mark position from your perspective looking at the 2D image, not from the camera perspective.\n- The image may be just a black square, in which case, you should say there are no objects.\n- Please answer with care: Some HITs will be checked by hand, and work may be rejected if there are too many errors.\n\nQuestion 1. Briefly list all the objects visible in the image, separated by commas. If there are no objects in the image, write \u201cnone\u201d.\n\nQuestion 2a/3a. How many $$\\text{[OBJECT]}$$s are in the image?\n\nQuestion 2b/3b. What color of $$\\text{[OBJECT]}$$s are in the image? Choose all that apply.\n\nQuestion 2c/3c. How realistic are the $$\\text{[OBJECT]}$$s in the image? (1\u20133) Ignore the color of the object, and focus on any visible defects in the shape or structure.\n\nQuestion 4a. Is the $$\\text{[OBJECT B]}$$ to the left or right of the $$\\text{[OBJECT A]}$$? Provide your answer as a viewer of the image, not from the \u201ccamera\u2019s perspective\u201d.\n\nQuestion 4b. Is the $$\\text{[OBJECT B]}$$ to above or below the $$\\text{[OBJECT A]}$$? Provide your answer as a viewer of the image, not from the \u201ccamera\u2019s perspective\u201d.\n\n20"}]}, {"page": 21, "text": "Annotationtask\n                                                                             'magc\n                                    Question Brletly Ilst all the objects visible In the\n                                   separated                     objects Lhie imnage,\n                                   rite none: Commas  Mchere Areno\n                                     eddv heat\n                                                                       the image?\n                                    Question Za: How mony teddybears are\n                                      None\n                                       moro\n                                    Question ZbWhat       teddy bears_ in the image?\n                                            that   color 0f\n                                    Choose all apply:\n                                            Orange   Yellov\n                                     Green           Purolz\n                                                     Black\n                                                   ealistic are the teddy bears\n                                    Question Zc: How                           image?\n                                    (1-3) Iznore the rolaroftne object, and focuis On any vsible\n                                   defects    emjpc   crructure\n                                      Reallstc\n                                      comnletck unrealkstic\n                                    Question How well does the Image fit the caption below\n                                    (1-4) 'Elements    indivicua       atrinutes\n                                                 Terer         odiecls\n                                    specitied by the text\n                                         phota            ceddy EAAE\n                                   The imnage\n                                     Mosly captures the meaning but missc} Orc ci\n                                    rany emnttls Lflc Caplidm    the capiion:\n                                                 Most nfthe eletert\n                                                        the caFtion:\n                                       complezely unrelated\n                Figure 11: Mechanical Turk annotation example.\n                                     21", "md": "Annotationtask\n\nQuestion A: Briefly list all the objects visible in the separated objects line image, write none: Commas where are in the image?\n\nQuestion Za: How many teddy bears are\n\nQuestion Zb: What teddy bears in the image?\n\nChoose all apply:\n\n$$\n\\begin{array}{llll}\n\\text{Orange} & \\text{Yellow} & \\text{Green} & \\text{Purple} \\\\\n\\text{Black} & & &\n\\end{array}\n$$\nQuestion Zc: How image?\n\n(1-3) Ignore the color of the object, and focus on any visible defects structure\n\nRealistic\n\ncompletely unrealistic\n\nQuestion: How well does the image fit the caption below\n\n(1-4) 'Elements individual attributes\n\nspecified by the text\n\nphoto teddy bear\n\nThe image\n\nMostly captures the meaning but misses or any elements the caption:\n\nMost of the elements\n\nthe caption:\n\ncompletely unrelated\n\nFigure 11: Mechanical Turk annotation example.\n\n21", "images": [{"name": "page-21-0.jpg", "height": 553, "width": 397, "x": 108, "y": 109}], "items": [{"type": "text", "value": "Annotationtask\n\nQuestion A: Briefly list all the objects visible in the separated objects line image, write none: Commas where are in the image?\n\nQuestion Za: How many teddy bears are\n\nQuestion Zb: What teddy bears in the image?\n\nChoose all apply:\n\n$$\n\\begin{array}{llll}\n\\text{Orange} & \\text{Yellow} & \\text{Green} & \\text{Purple} \\\\\n\\text{Black} & & &\n\\end{array}\n$$\nQuestion Zc: How image?\n\n(1-3) Ignore the color of the object, and focus on any visible defects structure\n\nRealistic\n\ncompletely unrealistic\n\nQuestion: How well does the image fit the caption below\n\n(1-4) 'Elements individual attributes\n\nspecified by the text\n\nphoto teddy bear\n\nThe image\n\nMostly captures the meaning but misses or any elements the caption:\n\nMost of the elements\n\nthe caption:\n\ncompletely unrelated\n\nFigure 11: Mechanical Turk annotation example.\n\n21", "md": "Annotationtask\n\nQuestion A: Briefly list all the objects visible in the separated objects line image, write none: Commas where are in the image?\n\nQuestion Za: How many teddy bears are\n\nQuestion Zb: What teddy bears in the image?\n\nChoose all apply:\n\n$$\n\\begin{array}{llll}\n\\text{Orange} & \\text{Yellow} & \\text{Green} & \\text{Purple} \\\\\n\\text{Black} & & &\n\\end{array}\n$$\nQuestion Zc: How image?\n\n(1-3) Ignore the color of the object, and focus on any visible defects structure\n\nRealistic\n\ncompletely unrealistic\n\nQuestion: How well does the image fit the caption below\n\n(1-4) 'Elements individual attributes\n\nspecified by the text\n\nphoto teddy bear\n\nThe image\n\nMostly captures the meaning but misses or any elements the caption:\n\nMost of the elements\n\nthe caption:\n\ncompletely unrelated\n\nFigure 11: Mechanical Turk annotation example.\n\n21"}]}], "job_id": "e4a4685d-187b-4cc1-9cec-dc74411439bf", "file_path": "./corpus/2310.11513v1.pdf"}