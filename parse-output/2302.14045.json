{"pages": [{"page": 1, "text": "                             Language Is Not All You Need: Aligning Perception\n                                                                  with Language Models\n                                   Shaohan Huang\u2217       , Li Dong\u2217    , Wenhui Wang\u2217        , Yaru Hao\u2217     , Saksham Singhal\u2217        , Shuming Ma\u2217\n                               Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal\n                                    Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei\u2020\n                                                                                        Microsoft\narXiv:2302.14045v2  [cs.CL]  1 Mar 2023                        https://github.com/microsoft/unilm\n                                                                                                                                                   output\n                                                        Multimodal Large Language Model (MLLM)\n                              Kosmos-1 can perceive both language and                                    , learn in context , reason, and generate\n                                                                                         Embedding\n                                                                                                          31 8 \u2026 70 2\n                                                                                                          Wp\n                                                                                     Vision                Audition\n                                                                                  Description of three toed\n                                                                                  woodpecker: It has black                     Here are eight images:\n                                                                                  and white stripes\n                                                                                  throughout the body and a\n                                                                                  yellow crown.\n                                                                                  Description of downy\n                                         What's in this picture?                  woodpecker: It has white\n                                                                                  spots on its black wings\n                                                                                  and some red on its crown.\n                                   Looks like a duck.\n                                         That\u2019s not a duck. Then\n                                         what\u2019s it?                                                                            The following image is:\n                                   Looks more like a                                                                         A           B          C\n                                   bunny.                                            Question: what is the\n                                                                                     name of the\n                                                           Why?                      woodpecker in the\n                                                                                     picture?                                 D          E          F\n                                   It has bunny ears.                          Downy\n                           Figure 1: KOSMOS-1 is a multimodal large language model (MLLM) that is capable of perceiving\n                           multimodal input, following instructions, and performing in-context learning for not only language\n                           tasks but also multimodal tasks. In this work, we align vision with large language models (LLMs),\n                           advancing the trend of going from LLMs to MLLMs.\n                               \u2217 Equal contribution. \u2020 Corresponding author.", "md": "# Document\n\nLanguage Is Not All You Need: Aligning Perception with Language Models\n\nShaohan Huang\u2217 , Li Dong\u2217 , Wenhui Wang\u2217 , Yaru Hao\u2217 , Saksham Singhal\u2217 , Shuming Ma\u2217 Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei\u2020 Microsoft\n\narXiv:2302.14045v2 [cs.CL] 1 Mar 2023 https://github.com/microsoft/unilm\n\nMultimodal Large Language Model (MLLM) Kosmos-1 can perceive both language and , learn in context , reason, and generate Embedding 31 8 \u2026 70 2 Wp Vision Audition Description of three toed woodpecker: It has black and white stripes throughout the body and a yellow crown. Description of downy woodpecker: It has white spots on its black wings and some red on its crown. Looks like a duck. That\u2019s not a duck. Then what\u2019s it? Looks more like a bunny. Why? It has bunny ears.\n\nFigure 1: KOSMOS-1 is a multimodal large language model (MLLM) that is capable of perceiving multimodal input, following instructions, and performing in-context learning for not only language tasks but also multimodal tasks. In this work, we align vision with large language models (LLMs), advancing the trend of going from LLMs to MLLMs. \u2217 Equal contribution. \u2020 Corresponding author.", "images": [{"name": "page-1-5.jpg", "height": 29, "width": 36, "x": 341, "y": 368}, {"name": "page-1-4.jpg", "height": 18, "width": 18, "x": 172, "y": 272}, {"name": "page-1-6.jpg", "height": 38, "width": 41, "x": 291, "y": 368}, {"name": "page-1-3.jpg", "height": 35, "width": 51, "x": 236, "y": 369}, {"name": "page-1-15.jpg", "height": 21, "width": 21, "x": 404, "y": 619}, {"name": "page-1-1.jpg", "height": 89, "width": 80, "x": 399, "y": 463}, {"name": "page-1-17.jpg", "height": 22, "width": 23, "x": 434, "y": 576}, {"name": "page-1-19.jpg", "height": 22, "width": 23, "x": 435, "y": 619}, {"name": "page-1-18.jpg", "height": 23, "width": 26, "x": 461, "y": 576}, {"name": "page-1-2.jpg", "height": 34, "width": 45, "x": 180, "y": 445}, {"name": "page-1-0.jpg", "height": 43, "width": 64, "x": 290, "y": 531}, {"name": "page-1-16.jpg", "height": 23, "width": 23, "x": 403, "y": 576}, {"name": "page-1-20.jpg", "height": 19, "width": 19, "x": 466, "y": 620}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "Language Is Not All You Need: Aligning Perception with Language Models\n\nShaohan Huang\u2217 , Li Dong\u2217 , Wenhui Wang\u2217 , Yaru Hao\u2217 , Saksham Singhal\u2217 , Shuming Ma\u2217 Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei\u2020 Microsoft\n\narXiv:2302.14045v2 [cs.CL] 1 Mar 2023 https://github.com/microsoft/unilm\n\nMultimodal Large Language Model (MLLM) Kosmos-1 can perceive both language and , learn in context , reason, and generate Embedding 31 8 \u2026 70 2 Wp Vision Audition Description of three toed woodpecker: It has black and white stripes throughout the body and a yellow crown. Description of downy woodpecker: It has white spots on its black wings and some red on its crown. Looks like a duck. That\u2019s not a duck. Then what\u2019s it? Looks more like a bunny. Why? It has bunny ears.\n\nFigure 1: KOSMOS-1 is a multimodal large language model (MLLM) that is capable of perceiving multimodal input, following instructions, and performing in-context learning for not only language tasks but also multimodal tasks. In this work, we align vision with large language models (LLMs), advancing the trend of going from LLMs to MLLMs. \u2217 Equal contribution. \u2020 Corresponding author.", "md": "Language Is Not All You Need: Aligning Perception with Language Models\n\nShaohan Huang\u2217 , Li Dong\u2217 , Wenhui Wang\u2217 , Yaru Hao\u2217 , Saksham Singhal\u2217 , Shuming Ma\u2217 Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei\u2020 Microsoft\n\narXiv:2302.14045v2 [cs.CL] 1 Mar 2023 https://github.com/microsoft/unilm\n\nMultimodal Large Language Model (MLLM) Kosmos-1 can perceive both language and , learn in context , reason, and generate Embedding 31 8 \u2026 70 2 Wp Vision Audition Description of three toed woodpecker: It has black and white stripes throughout the body and a yellow crown. Description of downy woodpecker: It has white spots on its black wings and some red on its crown. Looks like a duck. That\u2019s not a duck. Then what\u2019s it? Looks more like a bunny. Why? It has bunny ears.\n\nFigure 1: KOSMOS-1 is a multimodal large language model (MLLM) that is capable of perceiving multimodal input, following instructions, and performing in-context learning for not only language tasks but also multimodal tasks. In this work, we align vision with large language models (LLMs), advancing the trend of going from LLMs to MLLMs. \u2217 Equal contribution. \u2020 Corresponding author."}]}, {"page": 2, "text": "                                   The limits of my language means the limits of my world.\n                                                                                                                                                                                                                                                Ludwig Wittgenstein\n    \u201c                              A big convergence of language, multimodal perception, action, and world model-                                                Abstract                                                                                                                                   \u201d\n                                   ing is a key step toward artificial general intelligence. In this work, we introduce\n                                    KO            SM          O      S    -   12, a Multimodal Large Language Model (MLLM) that can perceive\n                                   general modalities, learn in context (i.e., few-shot), and follow instructions (i.e.,\n                                   zero-shot). Specifically, we train KO                                                                                                   SM       OS  -   1 from scratch on web-scale multi-\n                                   modal corpora, including arbitrarily interleaved text and images, image-caption\n                                   pairs, and text data. We evaluate various settings, including zero-shot, few-shot,\n                                   and multimodal chain-of-thought prompting, on a wide range of tasks without\n                                   any gradient updates or finetuning. Experimental results show that KO                                                                                                                                                                           SM           O  S  -  1\n                                   achieves impressive performance on (i) language understanding, generation, and\n                                   even OCR-free NLP (directly fed with document images), (ii) perception-language\n                                   tasks, including multimodal dialogue, image captioning, visual question answering,\n                                   and (iii) vision tasks, such as image recognition with descriptions (specifying\n                                   classification via text instructions). We also show that MLLMs can benefit from\n                                   cross-modal transfer, i.e., transfer knowledge from language to multimodal, and\n                                   from multimodal to language. In addition, we introduce a dataset of Raven IQ test,\n                                   which diagnoses the nonverbal reasoning capability of MLLMs.\n                I\n                n\n                p\n                u\n                t\n                P\n                r\n                o\n                m\n                p\n                t\n                                   Question: Explain why                                                                    Question: Why did the                                       Question: What is the                                                        Question: When will the\n                                   this photo is funny?                                                                     little boy cry? Answer:                                     hairstyle of the blond                                                       movie be released?\n                                   Answer:                                                                                                                                              called? Answer:                                                              Answer:\n                                   The cat is wearing a                                                                      Because his scooter                                                             pony tail                                                           On June 27\n                                   mask that gives the                                                                       broke.\n              C                    cat a smile.\n              o\n              m\n              p\n              l\n              e\n              t\n              i\n              o\n              n\n                                                        (1)                                                                                         (2)                                                           (3)                                                                        (4)\n               I                                                                                                                  5+4\n               n\n               p\n               u\n               t\n               P\n               r\n               o\n               m\n               p\n               t\n                                 What is TorchScale?                                                                         Question: The result\n                                                                                                                             is? Answer:                                                  Question: What is the heart                                                   The time now is\n                                                                                                                                                                                          rate in this picture? Answer:\n                       A library that allows                                                                                                     5 + 4 = 9                                                       57 bpm                                              10:10 on a large clock\n                       transformers to work efficiently\n             C         and effectively at any scale.\n             o\n             m\n             p\n             l\n             e\n             t\n             i\n             o\n             n\n                                                        (5)                                                                                         (6)                                                           (7)                                                                        (8)\nFigure 2: Selected examples generated from KO                                                                                                                                     SM    OS         -   1. Blue boxes are input prompt and pink\nboxes are KO                                              SM          O      S    -   1 output. The examples include (1)-(2) visual explanation, (3)-(4) visual\nquestion answering, (5) web page question answering, (6) simple math equation, and (7)-(8) number\nrecognition.\n            2   KOSMOS is pronounced as and means \u201cCosmos\u201d.\n                                                                                                                                                                               2", "md": "# Document\n\nThe limits of my language means the limits of my world.\n\nLudwig Wittgenstein\n\n\"A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce KOSMOS-12, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train KOSMOS-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.\n\nInput Prompt\n\nQuestion: Explain why this photo is funny?\n\nAnswer: The cat is wearing a mask that gives the cat a smile.\n\nQuestion: Why did the little boy cry? Answer: Because his scooter broke.\n\nQuestion: What is the hairstyle of the blond called? Answer: ponytail\n\nQuestion: When will the movie be released? Answer: On June 27\n\nInput Prompt\n\nQuestion: What is TorchScale?\n\nAnswer: A library that allows transformers to work efficiently and effectively at any scale.\n\nQuestion: The result is? Answer: 5 + 4 = 9\n\nQuestion: What is the heart rate in this picture? Answer: 57 bpm\n\nThe time now is 10:10 on a large clock\n\nFigure 2: Selected examples generated from KOSMOS-1. Blue boxes are input prompt and pink boxes are KOSMOS-1 output. The examples include (1)-(2) visual explanation, (3)-(4) visual question answering, (5) web page question answering, (6) simple math equation, and (7)-(8) number recognition.\n\nKOSMOS is pronounced as and means \"Cosmos\".", "images": [{"name": "page-2-2.jpg", "height": 54, "width": 39, "x": 435, "y": 378}, {"name": "page-2-1.jpg", "height": 44, "width": 69, "x": 234, "y": 383}, {"name": "page-2-3.jpg", "height": 57, "width": 61, "x": 428, "y": 515}, {"name": "page-2-4.jpg", "height": 36, "width": 89, "x": 138, "y": 522}, {"name": "page-2-5.jpg", "height": 51, "width": 53, "x": 331, "y": 515}, {"name": "page-2-0.jpg", "height": 44, "width": 63, "x": 144, "y": 384}, {"name": "page-2-6.jpg", "height": 30, "width": 59, "x": 235, "y": 524}, {"name": "page-2-7.jpg", "height": 44, "width": 58, "x": 328, "y": 384}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "The limits of my language means the limits of my world.\n\nLudwig Wittgenstein\n\n\"A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce KOSMOS-12, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train KOSMOS-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.\n\nInput Prompt\n\nQuestion: Explain why this photo is funny?\n\nAnswer: The cat is wearing a mask that gives the cat a smile.\n\nQuestion: Why did the little boy cry? Answer: Because his scooter broke.\n\nQuestion: What is the hairstyle of the blond called? Answer: ponytail\n\nQuestion: When will the movie be released? Answer: On June 27\n\nInput Prompt\n\nQuestion: What is TorchScale?\n\nAnswer: A library that allows transformers to work efficiently and effectively at any scale.\n\nQuestion: The result is? Answer: 5 + 4 = 9\n\nQuestion: What is the heart rate in this picture? Answer: 57 bpm\n\nThe time now is 10:10 on a large clock\n\nFigure 2: Selected examples generated from KOSMOS-1. Blue boxes are input prompt and pink boxes are KOSMOS-1 output. The examples include (1)-(2) visual explanation, (3)-(4) visual question answering, (5) web page question answering, (6) simple math equation, and (7)-(8) number recognition.\n\nKOSMOS is pronounced as and means \"Cosmos\".", "md": "The limits of my language means the limits of my world.\n\nLudwig Wittgenstein\n\n\"A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce KOSMOS-12, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train KOSMOS-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.\n\nInput Prompt\n\nQuestion: Explain why this photo is funny?\n\nAnswer: The cat is wearing a mask that gives the cat a smile.\n\nQuestion: Why did the little boy cry? Answer: Because his scooter broke.\n\nQuestion: What is the hairstyle of the blond called? Answer: ponytail\n\nQuestion: When will the movie be released? Answer: On June 27\n\nInput Prompt\n\nQuestion: What is TorchScale?\n\nAnswer: A library that allows transformers to work efficiently and effectively at any scale.\n\nQuestion: The result is? Answer: 5 + 4 = 9\n\nQuestion: What is the heart rate in this picture? Answer: 57 bpm\n\nThe time now is 10:10 on a large clock\n\nFigure 2: Selected examples generated from KOSMOS-1. Blue boxes are input prompt and pink boxes are KOSMOS-1 output. The examples include (1)-(2) visual explanation, (3)-(4) visual question answering, (5) web page question answering, (6) simple math equation, and (7)-(8) number recognition.\n\nKOSMOS is pronounced as and means \"Cosmos\"."}]}, {"page": 3, "text": "            I\n            n\n            p\n            u\n            t\n            P\n            r\n            o\n            m\n            p\n            t\n                                            An image of                                                            An image of                                                        Question: What coffee                                                            Question: What is inside\n                                                                                                                                                                                      shop is in the background?                                                       this canned good? Answer:\n                                                                                                                                                                                      Answer:\n                            a girl blowing out a                                                   a group of people                                                                                             Starbucks                                                                                   Corn\n                            candle on her birthday                                                 posing for a wedding\n           C                cake.                                                                  photo.\n           o\n           m\n           p\n           l\n           e\n           t\n           i\n           o\n           n\n                                                         (1)                                                                    (2)                                                                                        (3)                                                                                  (4)\n            I                                                                                                                SchO                                                                       WAITING?                                                  Ayc\n            n\n            p\n            u\n            t\n            P\n            r\n            o\n            m\n            p\n            t\n                               Question: What is on the                                            Question: What is the title                                                                              An image of                                                                          An image of\n                               pillowcase? Answer:                                                 of this book? Answer:\n                               a Texas Longhorns                                                          A Fine, Fine School                                                              a parking sign that says                                                          Hope you have done it\n                               pillow case.                                                                                                                                                ``Waiting? Please turn off\n           C                                                                                                                                                                               your engine.''\n           o\n           m\n           p\n           l\n           e\n           t\n           i\n           o\n           n\n                                                     (5)                                                                    (6)                                                                                         (7)                                                                                 (8)\n                                                                                                                                                                                                                                                                                                Window510\n                                     What's in this picture?                                                                                           What kind of animal is                                                                                           What's in this picture?\n                                                                                                                                                       in the picture?\n                  sausage roll.                                                                                                     It's a zebra.                                                                                                     A screenshot of the Windows\n                                                                                                                                                                                                                                                      10 shutdown window\n                                     How to cook it?                                                                                                   Where do they live in?                                                                                                I would like to restart\n                                                                                                                                                                                                                                                                             my computer. Which\n                                                                                                                                                                                                                                                                             button should I click?\n                   Soak sausage roll in\n                   ketchup. Bake in oven for                                                                                        In Africa.                                                                                                        Press OK.\n                   15 min. Cut and enjoy.\n                                                                                                                                                        Tell me a joke about\n                                Can I put cheese in the                                                                                                 zebras.                                                                                                           Now I would not like to\n                                dish?                                                                                                                                                                                                                                     restart. What can I do?\n                 Sure. But make sure it is                                                                                           Zebras get in trouble for\n                 melted.                                                                                                             crossing the road.                                                                                               Click Cancel.\n                                                        (9)                                                                                                                     (10)                                                                                                             (11)\nFigure 3: Selected examples generated from KO                                                                                                                                    SM           OS         -   1. Blue boxes are input prompt and pink\nboxes are KOSMOS-1 output. The examples include (1)-(2) image captioning, (3)-(6) visual question\nanswering, (7)-(8) OCR, and (9)-(11) visual dialogue.\n                                                                                                                                                                             3", "md": "# OCR Text\n\nI\n\nn\n\np\n\nu\n\nt\n\nP\n\nr\n\no\n\nm\n\np\n\nt\n\nAn image of $$\\text{a girl blowing out a candle on her birthday cake.}$$\n\nAn image of $$\\text{a group of people posing for a wedding photo.}$$\n\nQuestion: What coffee shop is in the background? Answer: Starbucks\n\nQuestion: What is inside this canned good? Answer: Corn\n\nC o m p l e t i o n\n(1) What's in this picture? Answer: sausage roll. How to cook it? Soak sausage roll in ketchup. Bake in oven for 15 min. Cut and enjoy. Can I put cheese in the dish? Sure. But make sure it is melted.\n(2) What kind of animal is in the picture? It's a zebra. Where do they live in? In Africa. Tell me a joke about zebras. Zebras get in trouble for crossing the road.\n(3) An image of a parking sign that says \"Waiting? Please turn off your engine.\" (4) Hope you have done it\n\nI\n\nn\n\np\n\nu\n\nt\n\nP\n\nr\n\no\n\nm\n\np\n\nt\n\nQuestion: What is on the pillowcase? Answer: a Texas Longhorns pillow case.\n\nQuestion: What is the title of this book? Answer: A Fine, Fine School\n\nC o m p l e t i o n\n(5) What's in this picture? Answer: A screenshot of the Windows 10 shutdown window (6) I would like to restart my computer. Which button should I click? Press OK.\n(7) What kind of animal is in the picture? It's a zebra. (8) Now I would not like to restart. What can I do? Click Cancel.\n\nFigure 3: Selected examples generated from KO boxes are KOSMOS-1 output. The examples include (1)-(2) image captioning, (3)-(6) visual question answering, (7)-(8) OCR, and (9)-(11) visual dialogue.", "images": [{"name": "page-3-0.jpg", "height": 45, "width": 15, "x": 112, "y": 95}, {"name": "page-3-2.jpg", "height": 51, "width": 39, "x": 246, "y": 211}, {"name": "page-3-3.jpg", "height": 45, "width": 15, "x": 111, "y": 223}, {"name": "page-3-1.jpg", "height": 36, "width": 14, "x": 111, "y": 159}, {"name": "page-3-7.jpg", "height": 42, "width": 64, "x": 141, "y": 87}, {"name": "page-3-5.jpg", "height": 46, "width": 62, "x": 323, "y": 212}, {"name": "page-3-4.jpg", "height": 47, "width": 93, "x": 404, "y": 217}, {"name": "page-3-6.jpg", "height": 51, "width": 38, "x": 153, "y": 210}, {"name": "page-3-9.jpg", "height": 42, "width": 64, "x": 325, "y": 87}, {"name": "page-3-14.jpg", "height": 56, "width": 42, "x": 430, "y": 76}, {"name": "page-3-8.jpg", "height": 42, "width": 63, "x": 234, "y": 88}, {"name": "page-3-10.jpg", "height": 36, "width": 14, "x": 111, "y": 284}, {"name": "page-3-11.jpg", "height": 43, "width": 54, "x": 167, "y": 345}, {"name": "page-3-12.jpg", "height": 43, "width": 77, "x": 280, "y": 345}, {"name": "page-3-13.jpg", "height": 45, "width": 78, "x": 408, "y": 344}], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text", "md": "# OCR Text"}, {"type": "text", "value": "I\n\nn\n\np\n\nu\n\nt\n\nP\n\nr\n\no\n\nm\n\np\n\nt\n\nAn image of $$\\text{a girl blowing out a candle on her birthday cake.}$$\n\nAn image of $$\\text{a group of people posing for a wedding photo.}$$\n\nQuestion: What coffee shop is in the background? Answer: Starbucks\n\nQuestion: What is inside this canned good? Answer: Corn\n\nC o m p l e t i o n\n(1) What's in this picture? Answer: sausage roll. How to cook it? Soak sausage roll in ketchup. Bake in oven for 15 min. Cut and enjoy. Can I put cheese in the dish? Sure. But make sure it is melted.\n(2) What kind of animal is in the picture? It's a zebra. Where do they live in? In Africa. Tell me a joke about zebras. Zebras get in trouble for crossing the road.\n(3) An image of a parking sign that says \"Waiting? Please turn off your engine.\" (4) Hope you have done it\n\nI\n\nn\n\np\n\nu\n\nt\n\nP\n\nr\n\no\n\nm\n\np\n\nt\n\nQuestion: What is on the pillowcase? Answer: a Texas Longhorns pillow case.\n\nQuestion: What is the title of this book? Answer: A Fine, Fine School\n\nC o m p l e t i o n\n(5) What's in this picture? Answer: A screenshot of the Windows 10 shutdown window (6) I would like to restart my computer. Which button should I click? Press OK.\n(7) What kind of animal is in the picture? It's a zebra. (8) Now I would not like to restart. What can I do? Click Cancel.\n\nFigure 3: Selected examples generated from KO boxes are KOSMOS-1 output. The examples include (1)-(2) image captioning, (3)-(6) visual question answering, (7)-(8) OCR, and (9)-(11) visual dialogue.", "md": "I\n\nn\n\np\n\nu\n\nt\n\nP\n\nr\n\no\n\nm\n\np\n\nt\n\nAn image of $$\\text{a girl blowing out a candle on her birthday cake.}$$\n\nAn image of $$\\text{a group of people posing for a wedding photo.}$$\n\nQuestion: What coffee shop is in the background? Answer: Starbucks\n\nQuestion: What is inside this canned good? Answer: Corn\n\nC o m p l e t i o n\n(1) What's in this picture? Answer: sausage roll. How to cook it? Soak sausage roll in ketchup. Bake in oven for 15 min. Cut and enjoy. Can I put cheese in the dish? Sure. But make sure it is melted.\n(2) What kind of animal is in the picture? It's a zebra. Where do they live in? In Africa. Tell me a joke about zebras. Zebras get in trouble for crossing the road.\n(3) An image of a parking sign that says \"Waiting? Please turn off your engine.\" (4) Hope you have done it\n\nI\n\nn\n\np\n\nu\n\nt\n\nP\n\nr\n\no\n\nm\n\np\n\nt\n\nQuestion: What is on the pillowcase? Answer: a Texas Longhorns pillow case.\n\nQuestion: What is the title of this book? Answer: A Fine, Fine School\n\nC o m p l e t i o n\n(5) What's in this picture? Answer: A screenshot of the Windows 10 shutdown window (6) I would like to restart my computer. Which button should I click? Press OK.\n(7) What kind of animal is in the picture? It's a zebra. (8) Now I would not like to restart. What can I do? Click Cancel.\n\nFigure 3: Selected examples generated from KO boxes are KOSMOS-1 output. The examples include (1)-(2) image captioning, (3)-(6) visual question answering, (7)-(8) OCR, and (9)-(11) visual dialogue."}]}, {"page": 4, "text": " Dataset                        Task description                                    Metric        Zero-shot  Few-shot\n   Language tasks\n StoryCloze [MRL+17]            Commonsense reasoning                               Accuracy           \u0013         \u0013\n HellaSwag [ZHB+19]             Commonsense NLI                                     Accuracy           \u0013         \u0013\n Winograd [LDM12a]              Word ambiguity                                      Accuracy           \u0013         \u0013\n Winogrande [SBBC20]            Word ambiguity                                      Accuracy           \u0013         \u0013\n PIQA [BZB+20]                  Physical commonsense                                Accuracy           \u0013         \u0013\n BoolQ [CLC+19]                 Question answering                                  Accuracy           \u0013         \u0013\n CB [dMST19]                    Textual entailment                                  Accuracy           \u0013         \u0013\n COPA [RBG11]                   Causal reasoning                                    Accuracy           \u0013         \u0013\n Rendered SST-2 [RKH+21]        OCR-free sentiment classification                   Accuracy           \u0013\n HatefulMemes [KFM+20]          OCR-free meme classification                        ROC AUC            \u0013\n   Cross-modal transfer\n RelativeSize [BHCF16]          Commonsense reasoning (object size)                 Accuracy           \u0013\n MemoryColor [NHJ21]            Commonsense reasoning (object color)                Accuracy           \u0013\n ColorTerms [BBBT12]            Commonsense reasoning (object color)                Accuracy           \u0013\n   Nonverbal reasoning tasks\n IQ Test                        Raven\u2019s Progressive Matrices                        Accuracy           \u0013\n   Perception-language tasks\n COCO Caption [LMB+14]          Image captioning                                    CIDEr, etc.        \u0013         \u0013\n Flicker30k [YLHH14]            Image captioning                                    CIDEr, etc.        \u0013         \u0013\n VQAv2 [GKSS+17]                Visual question answering                           VQA acc.           \u0013         \u0013\n VizWiz [GLS+18]                Visual question answering                           VQA acc.           \u0013         \u0013\n WebSRC [CZC+21]                Web page question answering                         F1 score           \u0013\n   Vision tasks\n ImageNet [DDS+09]              Zero-shot image classification                      Top-1 acc.         \u0013\n CUB [WBW+11]                   Zero-shot image classification with descriptions    Accuracy           \u0013\nTable 1: We evaluate the capabilities of KOSMOS-1 on language, perception-language, and vision\ntasks under both zero- and few-shot learning settings.\n1    Introduction: From LLMs to MLLMs\nLarge language models (LLMs) have successfully served as a general-purpose interface across various\nnatural language tasks [BMR+20]. The LLM-based interface can be adapted to a task as long as we\nare able to transform the input and output into texts. For example, the input of the summarization task\nis a document and the output is its summary. So we can feed the input document into the language\nmodel and then produce the generated summary.\nDespite the successful applications in natural language processing, it is still struggling to natively use\nLLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal\nperception is a necessity to achieve artificial general intelligence, in terms of knowledge acquisition\nand grounding to the real world. More importantly, unlocking multimodal input [TMC+21, HSD+22,\nWBD+22, ADL+22, AHR+22, LLSH23] greatly widens the applications of language models to\nmore high-value areas, such as multimodal machine learning, document intelligence, and robotics.\nIn this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can\nperceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e.,\nfew-shot learning). The goal is to align perception with LLMs, so that the models are able to see and\ntalk. To be specific, we follow METALM [HSD+22] to train the KOSMOS-1 model from scratch.\nAs shown in Figure 1, a Transformer-based language model is regarded as the general-purpose\ninterface, and perception modules are docked with the language model. We train the model on web-\nscale multimodal corpora, i.e., text data, arbitrarily interleaved images and texts, and image-caption\npairs. In addition, we calibrate the instruction-following capability across modalities by transferring\nlanguage-only data.\nAs shown in Table 1, the KOSMOS-1 model natively supports language, perception-language, and\nvision tasks. We also present some generated examples in Figure 2 and 3. In addition to various\nnatural language tasks, the KOSMOS-1 models natively handle a wide range of perception-intensive\ntasks, spanning visual dialogue, visual explanation, visual question answering, image captioning,\nsimple math equation, OCR, and zero-shot image classification with descriptions. We also build\n                                                            4", "md": "# KOSMOS-1 Capabilities\n\n# Dataset\n\n|Dataset|Task description|Metric|Zero-shot|Few-shot|\n|---|---|---|---|---|\n|StoryCloze [MRL+17]|Commonsense reasoning|Accuracy|\u0013|\u0013|\n|HellaSwag [ZHB+19]|Commonsense NLI|Accuracy|\u0013|\u0013|\n|Winograd [LDM12a]|Word ambiguity|Accuracy|\u0013|\u0013|\n|Winogrande [SBBC20]|Word ambiguity|Accuracy|\u0013|\u0013|\n|PIQA [BZB+20]|Physical commonsense|Accuracy|\u0013|\u0013|\n|BoolQ [CLC+19]|Question answering|Accuracy|\u0013|\u0013|\n|CB [dMST19]|Textual entailment|Accuracy|\u0013|\u0013|\n|COPA [RBG11]|Causal reasoning|Accuracy|\u0013|\u0013|\n|Rendered SST-2 [RKH+21]|OCR-free sentiment classification|Accuracy|\u0013| |\n|HatefulMemes [KFM+20]|OCR-free meme classification|ROC AUC|\u0013| |\n\n# Introduction: From LLMs to MLLMs\n\nLarge language models (LLMs) have successfully served as a general-purpose interface across various natural language tasks [BMR+20]. The LLM-based interface can be adapted to a task as long as we are able to transform the input and output into texts. For example, the input of the summarization task is a document and the output is its summary. So we can feed the input document into the language model and then produce the generated summary.\n\nDespite the successful applications in natural language processing, it is still struggling to natively use LLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal perception is a necessity to achieve artificial general intelligence, in terms of knowledge acquisition and grounding to the real world. More importantly, unlocking multimodal input [TMC+21, HSD+22, WBD+22, ADL+22, AHR+22, LLSH23] greatly widens the applications of language models to more high-value areas, such as multimodal machine learning, document intelligence, and robotics.\n\nIn this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e., few-shot learning). The goal is to align perception with LLMs, so that the models are able to see and talk. To be specific, we follow METALM [HSD+22] to train the KOSMOS-1 model from scratch.\n\nAs shown in Figure 1, a Transformer-based language model is regarded as the general-purpose interface, and perception modules are docked with the language model. We train the model on web-scale multimodal corpora, i.e., text data, arbitrarily interleaved images and texts, and image-caption pairs. In addition, we calibrate the instruction-following capability across modalities by transferring language-only data.\n\n# KOSMOS-1 Capabilities\n\nAs shown in Table 1, the KOSMOS-1 model natively supports language, perception-language, and vision tasks. We also present some generated examples in Figure 2 and 3. In addition to various natural language tasks, the KOSMOS-1 models natively handle a wide range of perception-intensive tasks, spanning visual dialogue, visual explanation, visual question answering, image captioning, simple math equation, OCR, and zero-shot image classification with descriptions. We also build", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "KOSMOS-1 Capabilities", "md": "# KOSMOS-1 Capabilities"}, {"type": "heading", "lvl": 1, "value": "Dataset", "md": "# Dataset"}, {"type": "table", "rows": [["Dataset", "Task description", "Metric", "Zero-shot", "Few-shot"], ["StoryCloze [MRL+17]", "Commonsense reasoning", "Accuracy", "\u0013", "\u0013"], ["HellaSwag [ZHB+19]", "Commonsense NLI", "Accuracy", "\u0013", "\u0013"], ["Winograd [LDM12a]", "Word ambiguity", "Accuracy", "\u0013", "\u0013"], ["Winogrande [SBBC20]", "Word ambiguity", "Accuracy", "\u0013", "\u0013"], ["PIQA [BZB+20]", "Physical commonsense", "Accuracy", "\u0013", "\u0013"], ["BoolQ [CLC+19]", "Question answering", "Accuracy", "\u0013", "\u0013"], ["CB [dMST19]", "Textual entailment", "Accuracy", "\u0013", "\u0013"], ["COPA [RBG11]", "Causal reasoning", "Accuracy", "\u0013", "\u0013"], ["Rendered SST-2 [RKH+21]", "OCR-free sentiment classification", "Accuracy", "\u0013", ""], ["HatefulMemes [KFM+20]", "OCR-free meme classification", "ROC AUC", "\u0013", ""]], "md": "|Dataset|Task description|Metric|Zero-shot|Few-shot|\n|---|---|---|---|---|\n|StoryCloze [MRL+17]|Commonsense reasoning|Accuracy|\u0013|\u0013|\n|HellaSwag [ZHB+19]|Commonsense NLI|Accuracy|\u0013|\u0013|\n|Winograd [LDM12a]|Word ambiguity|Accuracy|\u0013|\u0013|\n|Winogrande [SBBC20]|Word ambiguity|Accuracy|\u0013|\u0013|\n|PIQA [BZB+20]|Physical commonsense|Accuracy|\u0013|\u0013|\n|BoolQ [CLC+19]|Question answering|Accuracy|\u0013|\u0013|\n|CB [dMST19]|Textual entailment|Accuracy|\u0013|\u0013|\n|COPA [RBG11]|Causal reasoning|Accuracy|\u0013|\u0013|\n|Rendered SST-2 [RKH+21]|OCR-free sentiment classification|Accuracy|\u0013| |\n|HatefulMemes [KFM+20]|OCR-free meme classification|ROC AUC|\u0013| |", "isPerfectTable": true, "csv": "\"Dataset\",\"Task description\",\"Metric\",\"Zero-shot\",\"Few-shot\"\n\"StoryCloze [MRL+17]\",\"Commonsense reasoning\",\"Accuracy\",\"\u0013\",\"\u0013\"\n\"HellaSwag [ZHB+19]\",\"Commonsense NLI\",\"Accuracy\",\"\u0013\",\"\u0013\"\n\"Winograd [LDM12a]\",\"Word ambiguity\",\"Accuracy\",\"\u0013\",\"\u0013\"\n\"Winogrande [SBBC20]\",\"Word ambiguity\",\"Accuracy\",\"\u0013\",\"\u0013\"\n\"PIQA [BZB+20]\",\"Physical commonsense\",\"Accuracy\",\"\u0013\",\"\u0013\"\n\"BoolQ [CLC+19]\",\"Question answering\",\"Accuracy\",\"\u0013\",\"\u0013\"\n\"CB [dMST19]\",\"Textual entailment\",\"Accuracy\",\"\u0013\",\"\u0013\"\n\"COPA [RBG11]\",\"Causal reasoning\",\"Accuracy\",\"\u0013\",\"\u0013\"\n\"Rendered SST-2 [RKH+21]\",\"OCR-free sentiment classification\",\"Accuracy\",\"\u0013\",\"\"\n\"HatefulMemes [KFM+20]\",\"OCR-free meme classification\",\"ROC AUC\",\"\u0013\",\"\""}, {"type": "heading", "lvl": 1, "value": "Introduction: From LLMs to MLLMs", "md": "# Introduction: From LLMs to MLLMs"}, {"type": "text", "value": "Large language models (LLMs) have successfully served as a general-purpose interface across various natural language tasks [BMR+20]. The LLM-based interface can be adapted to a task as long as we are able to transform the input and output into texts. For example, the input of the summarization task is a document and the output is its summary. So we can feed the input document into the language model and then produce the generated summary.\n\nDespite the successful applications in natural language processing, it is still struggling to natively use LLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal perception is a necessity to achieve artificial general intelligence, in terms of knowledge acquisition and grounding to the real world. More importantly, unlocking multimodal input [TMC+21, HSD+22, WBD+22, ADL+22, AHR+22, LLSH23] greatly widens the applications of language models to more high-value areas, such as multimodal machine learning, document intelligence, and robotics.\n\nIn this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e., few-shot learning). The goal is to align perception with LLMs, so that the models are able to see and talk. To be specific, we follow METALM [HSD+22] to train the KOSMOS-1 model from scratch.\n\nAs shown in Figure 1, a Transformer-based language model is regarded as the general-purpose interface, and perception modules are docked with the language model. We train the model on web-scale multimodal corpora, i.e., text data, arbitrarily interleaved images and texts, and image-caption pairs. In addition, we calibrate the instruction-following capability across modalities by transferring language-only data.", "md": "Large language models (LLMs) have successfully served as a general-purpose interface across various natural language tasks [BMR+20]. The LLM-based interface can be adapted to a task as long as we are able to transform the input and output into texts. For example, the input of the summarization task is a document and the output is its summary. So we can feed the input document into the language model and then produce the generated summary.\n\nDespite the successful applications in natural language processing, it is still struggling to natively use LLMs for multimodal data, such as image, and audio. Being a basic part of intelligence, multimodal perception is a necessity to achieve artificial general intelligence, in terms of knowledge acquisition and grounding to the real world. More importantly, unlocking multimodal input [TMC+21, HSD+22, WBD+22, ADL+22, AHR+22, LLSH23] greatly widens the applications of language models to more high-value areas, such as multimodal machine learning, document intelligence, and robotics.\n\nIn this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, follow instructions (i.e., zero-shot learning), and learn in context (i.e., few-shot learning). The goal is to align perception with LLMs, so that the models are able to see and talk. To be specific, we follow METALM [HSD+22] to train the KOSMOS-1 model from scratch.\n\nAs shown in Figure 1, a Transformer-based language model is regarded as the general-purpose interface, and perception modules are docked with the language model. We train the model on web-scale multimodal corpora, i.e., text data, arbitrarily interleaved images and texts, and image-caption pairs. In addition, we calibrate the instruction-following capability across modalities by transferring language-only data."}, {"type": "heading", "lvl": 1, "value": "KOSMOS-1 Capabilities", "md": "# KOSMOS-1 Capabilities"}, {"type": "text", "value": "As shown in Table 1, the KOSMOS-1 model natively supports language, perception-language, and vision tasks. We also present some generated examples in Figure 2 and 3. In addition to various natural language tasks, the KOSMOS-1 models natively handle a wide range of perception-intensive tasks, spanning visual dialogue, visual explanation, visual question answering, image captioning, simple math equation, OCR, and zero-shot image classification with descriptions. We also build", "md": "As shown in Table 1, the KOSMOS-1 model natively supports language, perception-language, and vision tasks. We also present some generated examples in Figure 2 and 3. In addition to various natural language tasks, the KOSMOS-1 models natively handle a wide range of perception-intensive tasks, spanning visual dialogue, visual explanation, visual question answering, image captioning, simple math equation, OCR, and zero-shot image classification with descriptions. We also build"}]}, {"page": 5, "text": "an IQ test benchmark following Raven\u2019s Progressive Matrices [JR03, CJS90], which evaluates\nthe capability of nonverbal reasoning for MLLMs. The examples show that the native support of\nmultimodal perception enables new opportunities to apply LLMs to new tasks. Moreover, we show\nthat MLLMs achieve better commonsense reasoning performance compared with LLMs, which\nindicates cross-modal transfer helps knowledge acquisition.\nThe key takeaways are as follows:\nFrom LLMs to MLLMs.           Properly handling perception is a necessary step toward artificial general\nintelligence. The capability of perceiving multimodal input is critical to LLMs. First, multimodal\nperception enables LLMs to acquire commonsense knowledge beyond text descriptions. Second,\naligning perception with LLMs opens the door to new tasks, such as robotics, and document intelli-\ngence. Third, the capability of perception unifies various APIs, as graphical user interfaces are the\nmost natural and unified way to interact with. For example, MLLMs can directly read the screen or\nextract numbers from receipts. We train the KOSMOS-1 models on web-scale multimodal corpora,\nwhich ensures that the model robustly learns from diverse sources. We not only use a large-scale\ntext corpus but also mine high-quality image-caption pairs and arbitrarily interleaved image and text\ndocuments from the web.\nLanguage models as general-purpose interfaces.              Following the philosophy proposed in\nMETALM [HSD+22], we regard language models as a universal task layer. Because of the open-\nended output space, we are able to unify various task predictions as texts. Moreover, natural-language\ninstructions and action sequences (such as programming language) can be well handled by language\nmodels. LLMs also serve as basic reasoners [WWS+22], which is complementary to perception\nmodules on complex tasks. So it is natural to align world, action, and multimodal perception with the\ngeneral-purpose interface, i.e., language models.\nNew capabilities of MLLMs.        As shown in Table 1, apart from the capabilities found in previous\nLLMs [BMR+20, CND+22], MLLMs enable new usages and possibilities. First, we can conduct\nzero- and few-shot multimodal learning by using natural language instructions and demonstration\nexamples. Second, we observe promising signals of nonverbal reasoning by evaluating the Raven\nIQ test, which measures the fluid reasoning ability of humans. Third, MLLMs naturally support\nmulti-turn interactions for general modalities, such as multimodal dialogue.\n2    KOSMOS-1: A Multimodal Large Language Model\nAs shown in Figure 1, KOSMOS-1 is a multimodal language model that can perceive general\nmodalities, follow instructions, learn in context, and generate outputs. Given the previous context, the\nmodel learns to generate texts in an auto-regressive manner. Specifically, the backbone of KOSMOS-1\nis a Transformer-based causal language model. Apart from text, other modalities are embedded\nand fed into the language model. The Transformer decoder serves as a general-purpose interface to\nmultimodal input. We train KOSMOS-1 on multimodal corpora, including monomodal data, cross-\nmodal paired data, and interleaved multimodal data. Once the models are trained, we can directly\nevaluate the models in zero-shot and few-shot settings on both language tasks and multimodal tasks.\n2.1   Input Representation\nThe Transformer decoder perceives general modalities in a unified way. For input format, we flatten\ninput as a sequence decorated with special tokens. Specifically, we use <s> and </s> to denote start-\nand end-of-sequence. The special tokens <image> and </image> indicate the beginning and end of\nencoded image embeddings. For example, \u201c<s> document </s>\u201d is a text input, and \u201c<s> paragraph\n<image> Image Embedding </image> paragraph </s>\u201d is an interleaved image-text input. Table 21\nin Appendix shows some examples of input format.\nAn embedding module is used to encode both text tokens and other input modalities into vectors. Then\nthe embeddings are fed into the decoder. For input tokens, we use a lookup table to map them into\nembeddings. For the modalities of continuous signals (e.g., image, and audio), it is also feasible to\nrepresent inputs as discrete code and then regard them as \u201cforeign languages\u201d [WBD+22, WCW+23].\nIn this work, following [HSD+22], we employ a vision encoder as the embedding module for input\n                                                   5", "md": "# Document\n\nAn IQ test benchmark following Raven\u2019s Progressive Matrices [JR03, CJS90], which evaluates\nthe capability of nonverbal reasoning for MLLMs. The examples show that the native support of\nmultimodal perception enables new opportunities to apply LLMs to new tasks. Moreover, we show\nthat MLLMs achieve better commonsense reasoning performance compared with LLMs, which\nindicates cross-modal transfer helps knowledge acquisition.\n\nThe key takeaways are as follows:\n\nFrom LLMs to MLLMs. Properly handling perception is a necessary step toward artificial general\nintelligence. The capability of perceiving multimodal input is critical to LLMs. First, multimodal\nperception enables LLMs to acquire commonsense knowledge beyond text descriptions. Second,\naligning perception with LLMs opens the door to new tasks, such as robotics, and document intelligence.\nThird, the capability of perception unifies various APIs, as graphical user interfaces are the\nmost natural and unified way to interact with. For example, MLLMs can directly read the screen or\nextract numbers from receipts. We train the KOSMOS-1 models on web-scale multimodal corpora,\nwhich ensures that the model robustly learns from diverse sources. We not only use a large-scale\ntext corpus but also mine high-quality image-caption pairs and arbitrarily interleaved image and text\ndocuments from the web.\n\nLanguage models as general-purpose interfaces. Following the philosophy proposed in\nMETALM [HSD+22], we regard language models as a universal task layer. Because of the open-ended\noutput space, we are able to unify various task predictions as texts. Moreover, natural-language\ninstructions and action sequences (such as programming language) can be well handled by language\nmodels. LLMs also serve as basic reasoners [WWS+22], which is complementary to perception\nmodules on complex tasks. So it is natural to align world, action, and multimodal perception with the\ngeneral-purpose interface, i.e., language models.\n\nNew capabilities of MLLMs. As shown in Table 1, apart from the capabilities found in previous\nLLMs [BMR+20, CND+22], MLLMs enable new usages and possibilities. First, we can conduct\nzero- and few-shot multimodal learning by using natural language instructions and demonstration\nexamples. Second, we observe promising signals of nonverbal reasoning by evaluating the Raven\nIQ test, which measures the fluid reasoning ability of humans. Third, MLLMs naturally support\nmulti-turn interactions for general modalities, such as multimodal dialogue.\n\n## KOSMOS-1: A Multimodal Large Language Model\n\nAs shown in Figure 1, KOSMOS-1 is a multimodal language model that can perceive general\nmodalities, follow instructions, learn in context, and generate outputs. Given the previous context, the\nmodel learns to generate texts in an auto-regressive manner. Specifically, the backbone of KOSMOS-1\nis a Transformer-based causal language model. Apart from text, other modalities are embedded\nand fed into the language model. The Transformer decoder serves as a general-purpose interface to\nmultimodal input. We train KOSMOS-1 on multimodal corpora, including monomodal data, cross-\nmodal paired data, and interleaved multimodal data. Once the models are trained, we can directly\nevaluate the models in zero-shot and few-shot settings on both language tasks and multimodal tasks.\n\n### Input Representation\n\nThe Transformer decoder perceives general modalities in a unified way. For input format, we flatten\ninput as a sequence decorated with special tokens. Specifically, we use &lt;s&gt; and &lt;/s&gt; to denote start-\nand end-of-sequence. The special tokens &lt;image&gt; and &lt;/image&gt; indicate the beginning and end of\nencoded image embeddings. For example, \u201c&lt;s&gt; document &lt;/s&gt;\u201d is a text input, and \u201c&lt;s&gt; paragraph\n&lt;image&gt; Image Embedding &lt;/image&gt; paragraph &lt;/s&gt;\u201d is an interleaved image-text input. Table 21\nin Appendix shows some examples of input format.\n\nAn embedding module is used to encode both text tokens and other input modalities into vectors. Then\nthe embeddings are fed into the decoder. For input tokens, we use a lookup table to map them into\nembeddings. For the modalities of continuous signals (e.g., image, and audio), it is also feasible to\nrepresent inputs as discrete code and then regard them as \u201cforeign languages\u201d [WBD+22, WCW+23].\nIn this work, following [HSD+22], we employ a vision encoder as the embedding module for input.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "An IQ test benchmark following Raven\u2019s Progressive Matrices [JR03, CJS90], which evaluates\nthe capability of nonverbal reasoning for MLLMs. The examples show that the native support of\nmultimodal perception enables new opportunities to apply LLMs to new tasks. Moreover, we show\nthat MLLMs achieve better commonsense reasoning performance compared with LLMs, which\nindicates cross-modal transfer helps knowledge acquisition.\n\nThe key takeaways are as follows:\n\nFrom LLMs to MLLMs. Properly handling perception is a necessary step toward artificial general\nintelligence. The capability of perceiving multimodal input is critical to LLMs. First, multimodal\nperception enables LLMs to acquire commonsense knowledge beyond text descriptions. Second,\naligning perception with LLMs opens the door to new tasks, such as robotics, and document intelligence.\nThird, the capability of perception unifies various APIs, as graphical user interfaces are the\nmost natural and unified way to interact with. For example, MLLMs can directly read the screen or\nextract numbers from receipts. We train the KOSMOS-1 models on web-scale multimodal corpora,\nwhich ensures that the model robustly learns from diverse sources. We not only use a large-scale\ntext corpus but also mine high-quality image-caption pairs and arbitrarily interleaved image and text\ndocuments from the web.\n\nLanguage models as general-purpose interfaces. Following the philosophy proposed in\nMETALM [HSD+22], we regard language models as a universal task layer. Because of the open-ended\noutput space, we are able to unify various task predictions as texts. Moreover, natural-language\ninstructions and action sequences (such as programming language) can be well handled by language\nmodels. LLMs also serve as basic reasoners [WWS+22], which is complementary to perception\nmodules on complex tasks. So it is natural to align world, action, and multimodal perception with the\ngeneral-purpose interface, i.e., language models.\n\nNew capabilities of MLLMs. As shown in Table 1, apart from the capabilities found in previous\nLLMs [BMR+20, CND+22], MLLMs enable new usages and possibilities. First, we can conduct\nzero- and few-shot multimodal learning by using natural language instructions and demonstration\nexamples. Second, we observe promising signals of nonverbal reasoning by evaluating the Raven\nIQ test, which measures the fluid reasoning ability of humans. Third, MLLMs naturally support\nmulti-turn interactions for general modalities, such as multimodal dialogue.", "md": "An IQ test benchmark following Raven\u2019s Progressive Matrices [JR03, CJS90], which evaluates\nthe capability of nonverbal reasoning for MLLMs. The examples show that the native support of\nmultimodal perception enables new opportunities to apply LLMs to new tasks. Moreover, we show\nthat MLLMs achieve better commonsense reasoning performance compared with LLMs, which\nindicates cross-modal transfer helps knowledge acquisition.\n\nThe key takeaways are as follows:\n\nFrom LLMs to MLLMs. Properly handling perception is a necessary step toward artificial general\nintelligence. The capability of perceiving multimodal input is critical to LLMs. First, multimodal\nperception enables LLMs to acquire commonsense knowledge beyond text descriptions. Second,\naligning perception with LLMs opens the door to new tasks, such as robotics, and document intelligence.\nThird, the capability of perception unifies various APIs, as graphical user interfaces are the\nmost natural and unified way to interact with. For example, MLLMs can directly read the screen or\nextract numbers from receipts. We train the KOSMOS-1 models on web-scale multimodal corpora,\nwhich ensures that the model robustly learns from diverse sources. We not only use a large-scale\ntext corpus but also mine high-quality image-caption pairs and arbitrarily interleaved image and text\ndocuments from the web.\n\nLanguage models as general-purpose interfaces. Following the philosophy proposed in\nMETALM [HSD+22], we regard language models as a universal task layer. Because of the open-ended\noutput space, we are able to unify various task predictions as texts. Moreover, natural-language\ninstructions and action sequences (such as programming language) can be well handled by language\nmodels. LLMs also serve as basic reasoners [WWS+22], which is complementary to perception\nmodules on complex tasks. So it is natural to align world, action, and multimodal perception with the\ngeneral-purpose interface, i.e., language models.\n\nNew capabilities of MLLMs. As shown in Table 1, apart from the capabilities found in previous\nLLMs [BMR+20, CND+22], MLLMs enable new usages and possibilities. First, we can conduct\nzero- and few-shot multimodal learning by using natural language instructions and demonstration\nexamples. Second, we observe promising signals of nonverbal reasoning by evaluating the Raven\nIQ test, which measures the fluid reasoning ability of humans. Third, MLLMs naturally support\nmulti-turn interactions for general modalities, such as multimodal dialogue."}, {"type": "heading", "lvl": 2, "value": "KOSMOS-1: A Multimodal Large Language Model", "md": "## KOSMOS-1: A Multimodal Large Language Model"}, {"type": "text", "value": "As shown in Figure 1, KOSMOS-1 is a multimodal language model that can perceive general\nmodalities, follow instructions, learn in context, and generate outputs. Given the previous context, the\nmodel learns to generate texts in an auto-regressive manner. Specifically, the backbone of KOSMOS-1\nis a Transformer-based causal language model. Apart from text, other modalities are embedded\nand fed into the language model. The Transformer decoder serves as a general-purpose interface to\nmultimodal input. We train KOSMOS-1 on multimodal corpora, including monomodal data, cross-\nmodal paired data, and interleaved multimodal data. Once the models are trained, we can directly\nevaluate the models in zero-shot and few-shot settings on both language tasks and multimodal tasks.", "md": "As shown in Figure 1, KOSMOS-1 is a multimodal language model that can perceive general\nmodalities, follow instructions, learn in context, and generate outputs. Given the previous context, the\nmodel learns to generate texts in an auto-regressive manner. Specifically, the backbone of KOSMOS-1\nis a Transformer-based causal language model. Apart from text, other modalities are embedded\nand fed into the language model. The Transformer decoder serves as a general-purpose interface to\nmultimodal input. We train KOSMOS-1 on multimodal corpora, including monomodal data, cross-\nmodal paired data, and interleaved multimodal data. Once the models are trained, we can directly\nevaluate the models in zero-shot and few-shot settings on both language tasks and multimodal tasks."}, {"type": "heading", "lvl": 3, "value": "Input Representation", "md": "### Input Representation"}, {"type": "text", "value": "The Transformer decoder perceives general modalities in a unified way. For input format, we flatten\ninput as a sequence decorated with special tokens. Specifically, we use &lt;s&gt; and &lt;/s&gt; to denote start-\nand end-of-sequence. The special tokens &lt;image&gt; and &lt;/image&gt; indicate the beginning and end of\nencoded image embeddings. For example, \u201c&lt;s&gt; document &lt;/s&gt;\u201d is a text input, and \u201c&lt;s&gt; paragraph\n&lt;image&gt; Image Embedding &lt;/image&gt; paragraph &lt;/s&gt;\u201d is an interleaved image-text input. Table 21\nin Appendix shows some examples of input format.\n\nAn embedding module is used to encode both text tokens and other input modalities into vectors. Then\nthe embeddings are fed into the decoder. For input tokens, we use a lookup table to map them into\nembeddings. For the modalities of continuous signals (e.g., image, and audio), it is also feasible to\nrepresent inputs as discrete code and then regard them as \u201cforeign languages\u201d [WBD+22, WCW+23].\nIn this work, following [HSD+22], we employ a vision encoder as the embedding module for input.", "md": "The Transformer decoder perceives general modalities in a unified way. For input format, we flatten\ninput as a sequence decorated with special tokens. Specifically, we use &lt;s&gt; and &lt;/s&gt; to denote start-\nand end-of-sequence. The special tokens &lt;image&gt; and &lt;/image&gt; indicate the beginning and end of\nencoded image embeddings. For example, \u201c&lt;s&gt; document &lt;/s&gt;\u201d is a text input, and \u201c&lt;s&gt; paragraph\n&lt;image&gt; Image Embedding &lt;/image&gt; paragraph &lt;/s&gt;\u201d is an interleaved image-text input. Table 21\nin Appendix shows some examples of input format.\n\nAn embedding module is used to encode both text tokens and other input modalities into vectors. Then\nthe embeddings are fed into the decoder. For input tokens, we use a lookup table to map them into\nembeddings. For the modalities of continuous signals (e.g., image, and audio), it is also feasible to\nrepresent inputs as discrete code and then regard them as \u201cforeign languages\u201d [WBD+22, WCW+23].\nIn this work, following [HSD+22], we employ a vision encoder as the embedding module for input."}]}, {"page": 6, "text": "images. In addition, Resampler [ADL+22] is used as an attentive pooling mechanism to reduce the\nnumber of image embeddings.\n2.2   Multimodal Large Language Models (MLLMs)\nAfter obtaining the embeddings of an input sequence, we feed them into the Transformer-based\ndecoder. The left-to-right causal model processes the sequence in an auto-regressive manner, which\nproduces the next token by conditioning on past timesteps. The causal masking is used to mask\nout future information. A softmax classifier upon Transformer is used to generate tokens over the\nvocabulary.\nMLLMs serve as general-purpose interfaces [HSD+22] that can perform interactions with both\nnatural language and multimodal input. The framework is flexible to handle various data types,\nas long as we can represent input as vectors. MLLMs combine the best of two worlds. First, the\nlanguage models naturally inherit the capabilities of in-context learning and instruction following.\nSecond, perception is aligned with language models by training on multimodal corpora.\nThe implementation is based on the library TorchScale3 [MWH+22], which is designed for large-\nscale model training. Compared with the standard Transformer architecture, we include the following\nmodifications:\nMAGNETO        We use MAGNETO [WMH+22], a Transformer variant, as the backbone architecture.\nMAGNETO has better training stability and superior performance across modalities. It introduces an\nextra LayerNorm to each sublayer (i.e., multi-head self-attention, and feed-forward network). The\nmethod has a theoretically derived initialization method [WMD+22] to improve the optimization\nfundamentally, which allows us to effectively scale up the models without pain.\nXPOS     We employ XPOS [SDP+22] relative position encoding for better long-context modeling.\nThe method can better generalize to different lengths, i.e., training on short while testing on longer\nsequences. Moreover, XPOS optimizes attention resolution so that the position information can\nbe captured more precisely. The method XPOS is efficient and effective in both interpolation and\nextrapolation settings.\n2.3   Training Objective\nThe KOSMOS-1 training is conducted on web-scale multimodal corpora, including monomodal data\n(e.g., text corpus), cross-modal paired data (e.g., image-caption pairs), and interleaved multimodal\ndata (e.g., documents of arbitrarily interleaved images and texts). To be specific, we use monomodal\ndata for representation learning. For example, language modeling with text data pretrains instruction\nfollowing, in-context learning, and various language tasks. Moreover, cross-modal pairs and inter-\nleaved data learn to align the perception of general modalities with language models. Interleaved data\nalso naturally fit in the multimodal language modeling task. We present more details of training data\ncollection in Section 3.1.\nThe models are trained with the next-token prediction task, i.e., learning to generate the next token\ndepending on the previous context. The training objective is to maximize the log-likelihood of tokens\nin examples. Notice that only discrete tokens, such as text tokens, are accounted for in the training\nloss. Multimodal language modeling is a scalable way to train the models. More importantly, the\nemergence of various capabilities makes the training task favorable for downstream applications.\n3   Model Training\n3.1   Multimodal Training Data\nThe models are trained on web-scale multimodal corpora. The training datasets consist of text corpora,\nimage-caption pairs, and interleaved data of images and texts.\n   3https://github.com/microsoft/torchscale\n                                                   6", "md": "# Document\n\n## 2.2 Multimodal Large Language Models (MLLMs)\n\nAfter obtaining the embeddings of an input sequence, we feed them into the Transformer-based decoder. The left-to-right causal model processes the sequence in an auto-regressive manner, which produces the next token by conditioning on past timesteps. The causal masking is used to mask out future information. A softmax classifier upon Transformer is used to generate tokens over the vocabulary.\n\nMLLMs serve as general-purpose interfaces [HSD+22] that can perform interactions with both natural language and multimodal input. The framework is flexible to handle various data types, as long as we can represent input as vectors. MLLMs combine the best of two worlds. First, the language models naturally inherit the capabilities of in-context learning and instruction following. Second, perception is aligned with language models by training on multimodal corpora.\n\nThe implementation is based on the library TorchScale3 [MWH+22], which is designed for large-scale model training. Compared with the standard Transformer architecture, we include the following modifications:\n\n- MAGNETO: We use MAGNETO [WMH+22], a Transformer variant, as the backbone architecture. MAGNETO has better training stability and superior performance across modalities. It introduces an extra LayerNorm to each sublayer (i.e., multi-head self-attention, and feed-forward network). The method has a theoretically derived initialization method [WMD+22] to improve the optimization fundamentally, which allows us to effectively scale up the models without pain.\n- XPOS: We employ XPOS [SDP+22] relative position encoding for better long-context modeling. The method can better generalize to different lengths, i.e., training on short while testing on longer sequences. Moreover, XPOS optimizes attention resolution so that the position information can be captured more precisely. The method XPOS is efficient and effective in both interpolation and extrapolation settings.\n\n## 2.3 Training Objective\n\nThe KOSMOS-1 training is conducted on web-scale multimodal corpora, including monomodal data (e.g., text corpus), cross-modal paired data (e.g., image-caption pairs), and interleaved multimodal data (e.g., documents of arbitrarily interleaved images and texts). To be specific, we use monomodal data for representation learning. For example, language modeling with text data pretrains instruction following, in-context learning, and various language tasks. Moreover, cross-modal pairs and interleaved data learn to align the perception of general modalities with language models. Interleaved data also naturally fit in the multimodal language modeling task. We present more details of training data collection in Section 3.1.\n\nThe models are trained with the next-token prediction task, i.e., learning to generate the next token depending on the previous context. The training objective is to maximize the log-likelihood of tokens in examples. Notice that only discrete tokens, such as text tokens, are accounted for in the training loss. Multimodal language modeling is a scalable way to train the models. More importantly, the emergence of various capabilities makes the training task favorable for downstream applications.\n\n## 3 Model Training\n\n### 3.1 Multimodal Training Data\n\nThe models are trained on web-scale multimodal corpora. The training datasets consist of text corpora, image-caption pairs, and interleaved data of images and texts.\n\nTorchScale", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "2.2 Multimodal Large Language Models (MLLMs)", "md": "## 2.2 Multimodal Large Language Models (MLLMs)"}, {"type": "text", "value": "After obtaining the embeddings of an input sequence, we feed them into the Transformer-based decoder. The left-to-right causal model processes the sequence in an auto-regressive manner, which produces the next token by conditioning on past timesteps. The causal masking is used to mask out future information. A softmax classifier upon Transformer is used to generate tokens over the vocabulary.\n\nMLLMs serve as general-purpose interfaces [HSD+22] that can perform interactions with both natural language and multimodal input. The framework is flexible to handle various data types, as long as we can represent input as vectors. MLLMs combine the best of two worlds. First, the language models naturally inherit the capabilities of in-context learning and instruction following. Second, perception is aligned with language models by training on multimodal corpora.\n\nThe implementation is based on the library TorchScale3 [MWH+22], which is designed for large-scale model training. Compared with the standard Transformer architecture, we include the following modifications:\n\n- MAGNETO: We use MAGNETO [WMH+22], a Transformer variant, as the backbone architecture. MAGNETO has better training stability and superior performance across modalities. It introduces an extra LayerNorm to each sublayer (i.e., multi-head self-attention, and feed-forward network). The method has a theoretically derived initialization method [WMD+22] to improve the optimization fundamentally, which allows us to effectively scale up the models without pain.\n- XPOS: We employ XPOS [SDP+22] relative position encoding for better long-context modeling. The method can better generalize to different lengths, i.e., training on short while testing on longer sequences. Moreover, XPOS optimizes attention resolution so that the position information can be captured more precisely. The method XPOS is efficient and effective in both interpolation and extrapolation settings.", "md": "After obtaining the embeddings of an input sequence, we feed them into the Transformer-based decoder. The left-to-right causal model processes the sequence in an auto-regressive manner, which produces the next token by conditioning on past timesteps. The causal masking is used to mask out future information. A softmax classifier upon Transformer is used to generate tokens over the vocabulary.\n\nMLLMs serve as general-purpose interfaces [HSD+22] that can perform interactions with both natural language and multimodal input. The framework is flexible to handle various data types, as long as we can represent input as vectors. MLLMs combine the best of two worlds. First, the language models naturally inherit the capabilities of in-context learning and instruction following. Second, perception is aligned with language models by training on multimodal corpora.\n\nThe implementation is based on the library TorchScale3 [MWH+22], which is designed for large-scale model training. Compared with the standard Transformer architecture, we include the following modifications:\n\n- MAGNETO: We use MAGNETO [WMH+22], a Transformer variant, as the backbone architecture. MAGNETO has better training stability and superior performance across modalities. It introduces an extra LayerNorm to each sublayer (i.e., multi-head self-attention, and feed-forward network). The method has a theoretically derived initialization method [WMD+22] to improve the optimization fundamentally, which allows us to effectively scale up the models without pain.\n- XPOS: We employ XPOS [SDP+22] relative position encoding for better long-context modeling. The method can better generalize to different lengths, i.e., training on short while testing on longer sequences. Moreover, XPOS optimizes attention resolution so that the position information can be captured more precisely. The method XPOS is efficient and effective in both interpolation and extrapolation settings."}, {"type": "heading", "lvl": 2, "value": "2.3 Training Objective", "md": "## 2.3 Training Objective"}, {"type": "text", "value": "The KOSMOS-1 training is conducted on web-scale multimodal corpora, including monomodal data (e.g., text corpus), cross-modal paired data (e.g., image-caption pairs), and interleaved multimodal data (e.g., documents of arbitrarily interleaved images and texts). To be specific, we use monomodal data for representation learning. For example, language modeling with text data pretrains instruction following, in-context learning, and various language tasks. Moreover, cross-modal pairs and interleaved data learn to align the perception of general modalities with language models. Interleaved data also naturally fit in the multimodal language modeling task. We present more details of training data collection in Section 3.1.\n\nThe models are trained with the next-token prediction task, i.e., learning to generate the next token depending on the previous context. The training objective is to maximize the log-likelihood of tokens in examples. Notice that only discrete tokens, such as text tokens, are accounted for in the training loss. Multimodal language modeling is a scalable way to train the models. More importantly, the emergence of various capabilities makes the training task favorable for downstream applications.", "md": "The KOSMOS-1 training is conducted on web-scale multimodal corpora, including monomodal data (e.g., text corpus), cross-modal paired data (e.g., image-caption pairs), and interleaved multimodal data (e.g., documents of arbitrarily interleaved images and texts). To be specific, we use monomodal data for representation learning. For example, language modeling with text data pretrains instruction following, in-context learning, and various language tasks. Moreover, cross-modal pairs and interleaved data learn to align the perception of general modalities with language models. Interleaved data also naturally fit in the multimodal language modeling task. We present more details of training data collection in Section 3.1.\n\nThe models are trained with the next-token prediction task, i.e., learning to generate the next token depending on the previous context. The training objective is to maximize the log-likelihood of tokens in examples. Notice that only discrete tokens, such as text tokens, are accounted for in the training loss. Multimodal language modeling is a scalable way to train the models. More importantly, the emergence of various capabilities makes the training task favorable for downstream applications."}, {"type": "heading", "lvl": 2, "value": "3 Model Training", "md": "## 3 Model Training"}, {"type": "heading", "lvl": 3, "value": "3.1 Multimodal Training Data", "md": "### 3.1 Multimodal Training Data"}, {"type": "text", "value": "The models are trained on web-scale multimodal corpora. The training datasets consist of text corpora, image-caption pairs, and interleaved data of images and texts.\n\nTorchScale", "md": "The models are trained on web-scale multimodal corpora. The training datasets consist of text corpora, image-caption pairs, and interleaved data of images and texts.\n\nTorchScale"}]}, {"page": 7, "text": "Text Corpora      We train our model with The Pile [GBB+20] and Common Crawl (CC). The Pile\nis a massive English text dataset built for training large-scale language models, which is produced\nfrom a variety of data sources. We exclude data splits from GitHub, arXiv, Stack Exchange, and\nPubMed Central. We also include the Common Crawl snapshots (2020-50 and 2021-04) datasets,\nCC-Stories, and RealNews datasets [SPP+19, SPN+22]. The entire datasets have been purged of\nduplicate and near-duplicate documents, as well as filtered to exclude downstream task data. Refer to\nAppendix B.1.1 for detailed descriptions of training text corpora.\nImage-Caption Pairs       The image-caption pairs are constructed from several datasets, including\nEnglish LAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22], and Conceptual\nCaptions [SDGS18, CSDS21]. English LAION-2B, LAION-400M, and COYO-700M are collected\nfrom web pages of the Common Crawl web data by extracting image sources and the corresponding\nalt-text. Conceptual Captions are also from internet web pages. More details can be found in\nAppendix B.1.2.\nInterleaved Image-Text Data        We collect interleaved multimodal data from the Common Crawl\nsnapshot, which is a publicly available archive of web pages. We use a filtering process to select\nabout 71M web pages from the original 2B web pages in the snapshot. We then extract the text and\nimages from the HTML of each selected web page. For each document, we limit the number of\nimages to five to reduce noise and redundancy. We also randomly discard half of the documents that\nonly have one image to increase the diversity. We provide more details about the data collection\nprocess in Appendix B.1.3. By using this corpus, we enable KOSMOS-1 to handle interleaved text\nand image and improve its few-shot ability.\n3.2   Training Setup\nThe MLLM component has 24 layers with 2,048 hidden dimensions, 8,192 FFN intermediate size,\nand 32 attention heads, resulting in about 1.3B parameters. We use Magneto\u2019s initialization for\noptimization stability. For faster convergence, the image representation is obtained from a pretrained\nCLIP ViT-L/14 model with 1,024 feature dimensions. The images are preprocessed into 224\u00d7224\nresolution during training. We freeze the parameters of the CLIP model except for the last layer\nduring training. The total number of parameters of KOSMOS-1 is about 1.6B. More details about\nhyperparameters can be found in Appendix A.\nWe use a batch size of 1.2 million tokens (0.5 million tokens from text corpora, 0.5 million tokens\nfrom image-caption pairs, and 0.2 million tokens from interleaved data) and train KOSMOS-1 for\n300k steps, corresponding to about 360 billion tokens. We adopt the AdamW optimizer with\n\u03b2 = (0.9, 0.98). We set the weight decay to 0.01 and the dropout rate to 0.1. The learning rate\nincreases to 2e-4 for the first 375 warming-up steps and decays linearly to 0 for the rest of the\ntraining steps. We use SentencePiece [KR18] to tokenize the text. We preprocess the data in the\n\u201cfull-sentence\u201d format [LOG+19], which packs each input sequence with full sentences that are\nsampled continuously from one or more documents.\n3.3   Language-Only Instruction Tuning\nIn order to better align KOSMOS-1 with human instructions, we perform language-only instruction\ntuning [LHV+23, HSLS22]. Specifically, we continue-train the model with the instruction data in\nthe format of (instructions, inputs, and outputs). The instruction data is language-only, which is\nmixed with training corpora. The tuning process is conducted as language modeling. Notice that\ninstructions and inputs are not accounted for in the loss. Section 4.9.1 shows that the improvements\nin the instruction-following capability can transfer across modalities.\nWe combine Unnatural Instructions [HSLS22] and FLANv2 [LHV+23] as our instruction dataset.\nUnnatural Instructions is a dataset that was created by using a large language model to generate\ninstructions for various natural language processing tasks. It has 68,478 instruction-input-output\ntriplets in its core dataset. FLANv2 is a collection of datasets that cover diverse types of language\nunderstanding tasks, such as reading comprehension, commonsense reasoning, and closed-book\nquestion answering. We randomly select 54k examples of instructions from FLANv2 to augment our\ninstruction dataset. Details of the training hyperparameter settings are described in Appendix A.2.\n                                                   7", "md": "# Document\n\n## Text Corpora\n\nWe train our model with The Pile [GBB+20] and Common Crawl (CC). The Pile is a massive English text dataset built for training large-scale language models, which is produced from a variety of data sources. We exclude data splits from GitHub, arXiv, Stack Exchange, and PubMed Central. We also include the Common Crawl snapshots (2020-50 and 2021-04) datasets, CC-Stories, and RealNews datasets [SPP+19, SPN+22]. The entire datasets have been purged of duplicate and near-duplicate documents, as well as filtered to exclude downstream task data. Refer to Appendix B.1.1 for detailed descriptions of training text corpora.\n\n## Image-Caption Pairs\n\nThe image-caption pairs are constructed from several datasets, including English LAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22], and Conceptual Captions [SDGS18, CSDS21]. English LAION-2B, LAION-400M, and COYO-700M are collected from web pages of the Common Crawl web data by extracting image sources and the corresponding alt-text. Conceptual Captions are also from internet web pages. More details can be found in Appendix B.1.2.\n\n## Interleaved Image-Text Data\n\nWe collect interleaved multimodal data from the Common Crawl snapshot, which is a publicly available archive of web pages. We use a filtering process to select about 71M web pages from the original 2B web pages in the snapshot. We then extract the text and images from the HTML of each selected web page. For each document, we limit the number of images to five to reduce noise and redundancy. We also randomly discard half of the documents that only have one image to increase the diversity. We provide more details about the data collection process in Appendix B.1.3. By using this corpus, we enable KOSMOS-1 to handle interleaved text and image and improve its few-shot ability.\n\n### Training Setup\n\nThe MLLM component has 24 layers with 2,048 hidden dimensions, 8,192 FFN intermediate size, and 32 attention heads, resulting in about 1.3B parameters. We use Magneto\u2019s initialization for optimization stability. For faster convergence, the image representation is obtained from a pretrained CLIP ViT-L/14 model with 1,024 feature dimensions. The images are preprocessed into 224x224 resolution during training. We freeze the parameters of the CLIP model except for the last layer during training. The total number of parameters of KOSMOS-1 is about 1.6B. More details about hyperparameters can be found in Appendix A.\n\n### Language-Only Instruction Tuning\n\nIn order to better align KOSMOS-1 with human instructions, we perform language-only instruction tuning [LHV+23, HSLS22]. Specifically, we continue-train the model with the instruction data in the format of (instructions, inputs, and outputs). The instruction data is language-only, which is mixed with training corpora. The tuning process is conducted as language modeling. Notice that instructions and inputs are not accounted for in the loss. Section 4.9.1 shows that the improvements in the instruction-following capability can transfer across modalities.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Text Corpora", "md": "## Text Corpora"}, {"type": "text", "value": "We train our model with The Pile [GBB+20] and Common Crawl (CC). The Pile is a massive English text dataset built for training large-scale language models, which is produced from a variety of data sources. We exclude data splits from GitHub, arXiv, Stack Exchange, and PubMed Central. We also include the Common Crawl snapshots (2020-50 and 2021-04) datasets, CC-Stories, and RealNews datasets [SPP+19, SPN+22]. The entire datasets have been purged of duplicate and near-duplicate documents, as well as filtered to exclude downstream task data. Refer to Appendix B.1.1 for detailed descriptions of training text corpora.", "md": "We train our model with The Pile [GBB+20] and Common Crawl (CC). The Pile is a massive English text dataset built for training large-scale language models, which is produced from a variety of data sources. We exclude data splits from GitHub, arXiv, Stack Exchange, and PubMed Central. We also include the Common Crawl snapshots (2020-50 and 2021-04) datasets, CC-Stories, and RealNews datasets [SPP+19, SPN+22]. The entire datasets have been purged of duplicate and near-duplicate documents, as well as filtered to exclude downstream task data. Refer to Appendix B.1.1 for detailed descriptions of training text corpora."}, {"type": "heading", "lvl": 2, "value": "Image-Caption Pairs", "md": "## Image-Caption Pairs"}, {"type": "text", "value": "The image-caption pairs are constructed from several datasets, including English LAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22], and Conceptual Captions [SDGS18, CSDS21]. English LAION-2B, LAION-400M, and COYO-700M are collected from web pages of the Common Crawl web data by extracting image sources and the corresponding alt-text. Conceptual Captions are also from internet web pages. More details can be found in Appendix B.1.2.", "md": "The image-caption pairs are constructed from several datasets, including English LAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22], and Conceptual Captions [SDGS18, CSDS21]. English LAION-2B, LAION-400M, and COYO-700M are collected from web pages of the Common Crawl web data by extracting image sources and the corresponding alt-text. Conceptual Captions are also from internet web pages. More details can be found in Appendix B.1.2."}, {"type": "heading", "lvl": 2, "value": "Interleaved Image-Text Data", "md": "## Interleaved Image-Text Data"}, {"type": "text", "value": "We collect interleaved multimodal data from the Common Crawl snapshot, which is a publicly available archive of web pages. We use a filtering process to select about 71M web pages from the original 2B web pages in the snapshot. We then extract the text and images from the HTML of each selected web page. For each document, we limit the number of images to five to reduce noise and redundancy. We also randomly discard half of the documents that only have one image to increase the diversity. We provide more details about the data collection process in Appendix B.1.3. By using this corpus, we enable KOSMOS-1 to handle interleaved text and image and improve its few-shot ability.", "md": "We collect interleaved multimodal data from the Common Crawl snapshot, which is a publicly available archive of web pages. We use a filtering process to select about 71M web pages from the original 2B web pages in the snapshot. We then extract the text and images from the HTML of each selected web page. For each document, we limit the number of images to five to reduce noise and redundancy. We also randomly discard half of the documents that only have one image to increase the diversity. We provide more details about the data collection process in Appendix B.1.3. By using this corpus, we enable KOSMOS-1 to handle interleaved text and image and improve its few-shot ability."}, {"type": "heading", "lvl": 3, "value": "Training Setup", "md": "### Training Setup"}, {"type": "text", "value": "The MLLM component has 24 layers with 2,048 hidden dimensions, 8,192 FFN intermediate size, and 32 attention heads, resulting in about 1.3B parameters. We use Magneto\u2019s initialization for optimization stability. For faster convergence, the image representation is obtained from a pretrained CLIP ViT-L/14 model with 1,024 feature dimensions. The images are preprocessed into 224x224 resolution during training. We freeze the parameters of the CLIP model except for the last layer during training. The total number of parameters of KOSMOS-1 is about 1.6B. More details about hyperparameters can be found in Appendix A.", "md": "The MLLM component has 24 layers with 2,048 hidden dimensions, 8,192 FFN intermediate size, and 32 attention heads, resulting in about 1.3B parameters. We use Magneto\u2019s initialization for optimization stability. For faster convergence, the image representation is obtained from a pretrained CLIP ViT-L/14 model with 1,024 feature dimensions. The images are preprocessed into 224x224 resolution during training. We freeze the parameters of the CLIP model except for the last layer during training. The total number of parameters of KOSMOS-1 is about 1.6B. More details about hyperparameters can be found in Appendix A."}, {"type": "heading", "lvl": 3, "value": "Language-Only Instruction Tuning", "md": "### Language-Only Instruction Tuning"}, {"type": "text", "value": "In order to better align KOSMOS-1 with human instructions, we perform language-only instruction tuning [LHV+23, HSLS22]. Specifically, we continue-train the model with the instruction data in the format of (instructions, inputs, and outputs). The instruction data is language-only, which is mixed with training corpora. The tuning process is conducted as language modeling. Notice that instructions and inputs are not accounted for in the loss. Section 4.9.1 shows that the improvements in the instruction-following capability can transfer across modalities.", "md": "In order to better align KOSMOS-1 with human instructions, we perform language-only instruction tuning [LHV+23, HSLS22]. Specifically, we continue-train the model with the instruction data in the format of (instructions, inputs, and outputs). The instruction data is language-only, which is mixed with training corpora. The tuning process is conducted as language modeling. Notice that instructions and inputs are not accounted for in the loss. Section 4.9.1 shows that the improvements in the instruction-following capability can transfer across modalities."}]}, {"page": 8, "text": "4    Evaluation\nMLLMs can handle both language tasks and perception-intensive tasks. We evaluate KOSMOS-1 on\nvarious types of tasks as follows:\n        \u2022 Language tasks\n            \u2013 Language understanding\n            \u2013 Language generation\n            \u2013 OCR-free text classification\n        \u2022 Cross-modal transfer\n            \u2013 Commonsense reasoning\n        \u2022 Nonverbal reasoning\n            \u2013 IQ Test (Raven\u2019s Progressive Matrices)\n        \u2022 Perception-language tasks\n            \u2013 Image captioning\n            \u2013 Visual question answering\n            \u2013 Web page question answering\n        \u2022 Vision tasks\n            \u2013 Zero-shot image classification\n            \u2013 Zero-shot image classification with descriptions\n4.1   Perception-Language Tasks\nWe evaluate the perception-language capability of KOSMOS-1 under vision-language settings. Specif-\nically, we conduct zero-shot and few-shot experiments on two widely used tasks, including image\ncaptioning and visual question answering. Image captioning involves generating a natural language\ndescription of an image, while visual question answering aims to answer a natural language question\nwith respect to an image.\n4.1.1   Evaluation Setup\nWe evaluate the caption generation on MS COCO Caption [LMB+14], and Flickr30k [YLHH14].\nWe use the test set of COCO Karpathy split [KFF17], which re-partitions the train2014 and val2014\nimages [LMB+14] into 113,287, 5,000, and 5,000 for the training set, validation set, and test set,\nrespectively. We conduct an evaluation on Flickr30k\u2019s Karpathy split test set. The image resolution\nis 224\u00d7224. We use beam search to generate the captions, and the beam size is 5. In the few-shot\nsettings, we randomly sample demonstrations from the training set. We use COCOEvalCap4 to\ncompute CIDEr [VLZP15] and SPICE [AFJG16] scores as the evaluation metrics. We prompt\nKOSMOS-1 with \u201cAn image of\u201d for zero-shot and few-shot caption generation experiments.\nFor visual question-answering tasks, we evaluate zero-shot and few-shot results on test-dev set of\nVQAv2 [GKSS+17] and test-dev set of VizWiz [GLS+18], respectively. The resolution of images is\n224\u00d7224. We use greedy search for the decoding. We follow the normalization rules of the VQAv2\nevaluation code5 when computing the VQA accuracy. We evaluate the performance of VQA in an\nopen-ended setting that KOSMOS-1 generates answers and stops at the </s> (\u201cend of sequence\u201d)\ntoken. The prompt is \u201cQuestion: {question} Answer: {answer}\u201d for visual question answering tasks.\n4.1.2   Results\nImage Captioning       Table 2 shows the zero-shot captioning performance on COCO Karpathy test\nsplit and Flickr30k test set. KOSMOS-1 achieves remarkable results in zero-shot setting on two image\ncaptioning datasets. Specifically, our model achieves a CIDEr score of 67.1 on the Flickr30k dataset,\ncompared to 60.6 and 61.5 for the Flamingo-3B and Flamingo-9B models, respectively. Notably, our\nmodel is able to accomplish this feat with a smaller size of 1.6B, compared to Flamingo models. This\ndemonstrates our model\u2019s superiority in zero-shot image captioning.\n    4https://github.com/salaniz/pycocoevalcap\n    5https://github.com/GT-Vision-Lab/VQA\n                                                   8", "md": "# Evaluation\n\n#### Evaluation\n\nMLLMs can handle both language tasks and perception-intensive tasks. We evaluate KOSMOS-1 on various types of tasks as follows:\n\n- Language tasks\n- Language understanding\n- Language generation\n- OCR-free text classification\n- Cross-modal transfer\n- Commonsense reasoning\n- Nonverbal reasoning\n- IQ Test (Raven\u2019s Progressive Matrices)\n- Perception-language tasks\n- Image captioning\n- Visual question answering\n- Web page question answering\n- Vision tasks\n- Zero-shot image classification\n- Zero-shot image classification with descriptions\n\n#### Perception-Language Tasks\n\nWe evaluate the perception-language capability of KOSMOS-1 under vision-language settings. Specifically, we conduct zero-shot and few-shot experiments on two widely used tasks, including image captioning and visual question answering. Image captioning involves generating a natural language description of an image, while visual question answering aims to answer a natural language question with respect to an image.\n\n##### Evaluation Setup\n\nWe evaluate the caption generation on MS COCO Caption [LMB+14], and Flickr30k [YLHH14].\n\nWe use the test set of COCO Karpathy split [KFF17], which re-partitions the train2014 and val2014 images [LMB+14] into 113,287, 5,000, and 5,000 for the training set, validation set, and test set, respectively. We conduct an evaluation on Flickr30k\u2019s Karpathy split test set. The image resolution is 224x224. We use beam search to generate the captions, and the beam size is 5. In the few-shot settings, we randomly sample demonstrations from the training set. We use COCOEvalCap4 to compute CIDEr [VLZP15] and SPICE [AFJG16] scores as the evaluation metrics. We prompt KOSMOS-1 with \"An image of\" for zero-shot and few-shot caption generation experiments.\n\nFor visual question-answering tasks, we evaluate zero-shot and few-shot results on test-dev set of VQAv2 [GKSS+17] and test-dev set of VizWiz [GLS+18], respectively. The resolution of images is 224x224. We use greedy search for the decoding. We follow the normalization rules of the VQAv2 evaluation code5 when computing the VQA accuracy. We evaluate the performance of VQA in an open-ended setting that KOSMOS-1 generates answers and stops at the &lt;/s&gt; (\"end of sequence\") token. The prompt is \"Question: {question} Answer: {answer}\" for visual question answering tasks.\n\n##### Results\n\nImage Captioning\n\nTable 2 shows the zero-shot captioning performance on COCO Karpathy test split and Flickr30k test set. KOSMOS-1 achieves remarkable results in zero-shot setting on two image captioning datasets. Specifically, our model achieves a CIDEr score of 67.1 on the Flickr30k dataset, compared to 60.6 and 61.5 for the Flamingo-3B and Flamingo-9B models, respectively. Notably, our model is able to accomplish this feat with a smaller size of 1.6B, compared to Flamingo models. This demonstrates our model\u2019s superiority in zero-shot image captioning.\n\n4https://github.com/salaniz/pycocoevalcap\n\n5https://github.com/GT-Vision-Lab/VQA", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Evaluation", "md": "# Evaluation"}, {"type": "heading", "lvl": 4, "value": "Evaluation", "md": "#### Evaluation"}, {"type": "text", "value": "MLLMs can handle both language tasks and perception-intensive tasks. We evaluate KOSMOS-1 on various types of tasks as follows:\n\n- Language tasks\n- Language understanding\n- Language generation\n- OCR-free text classification\n- Cross-modal transfer\n- Commonsense reasoning\n- Nonverbal reasoning\n- IQ Test (Raven\u2019s Progressive Matrices)\n- Perception-language tasks\n- Image captioning\n- Visual question answering\n- Web page question answering\n- Vision tasks\n- Zero-shot image classification\n- Zero-shot image classification with descriptions", "md": "MLLMs can handle both language tasks and perception-intensive tasks. We evaluate KOSMOS-1 on various types of tasks as follows:\n\n- Language tasks\n- Language understanding\n- Language generation\n- OCR-free text classification\n- Cross-modal transfer\n- Commonsense reasoning\n- Nonverbal reasoning\n- IQ Test (Raven\u2019s Progressive Matrices)\n- Perception-language tasks\n- Image captioning\n- Visual question answering\n- Web page question answering\n- Vision tasks\n- Zero-shot image classification\n- Zero-shot image classification with descriptions"}, {"type": "heading", "lvl": 4, "value": "Perception-Language Tasks", "md": "#### Perception-Language Tasks"}, {"type": "text", "value": "We evaluate the perception-language capability of KOSMOS-1 under vision-language settings. Specifically, we conduct zero-shot and few-shot experiments on two widely used tasks, including image captioning and visual question answering. Image captioning involves generating a natural language description of an image, while visual question answering aims to answer a natural language question with respect to an image.", "md": "We evaluate the perception-language capability of KOSMOS-1 under vision-language settings. Specifically, we conduct zero-shot and few-shot experiments on two widely used tasks, including image captioning and visual question answering. Image captioning involves generating a natural language description of an image, while visual question answering aims to answer a natural language question with respect to an image."}, {"type": "heading", "lvl": 5, "value": "Evaluation Setup", "md": "##### Evaluation Setup"}, {"type": "text", "value": "We evaluate the caption generation on MS COCO Caption [LMB+14], and Flickr30k [YLHH14].\n\nWe use the test set of COCO Karpathy split [KFF17], which re-partitions the train2014 and val2014 images [LMB+14] into 113,287, 5,000, and 5,000 for the training set, validation set, and test set, respectively. We conduct an evaluation on Flickr30k\u2019s Karpathy split test set. The image resolution is 224x224. We use beam search to generate the captions, and the beam size is 5. In the few-shot settings, we randomly sample demonstrations from the training set. We use COCOEvalCap4 to compute CIDEr [VLZP15] and SPICE [AFJG16] scores as the evaluation metrics. We prompt KOSMOS-1 with \"An image of\" for zero-shot and few-shot caption generation experiments.\n\nFor visual question-answering tasks, we evaluate zero-shot and few-shot results on test-dev set of VQAv2 [GKSS+17] and test-dev set of VizWiz [GLS+18], respectively. The resolution of images is 224x224. We use greedy search for the decoding. We follow the normalization rules of the VQAv2 evaluation code5 when computing the VQA accuracy. We evaluate the performance of VQA in an open-ended setting that KOSMOS-1 generates answers and stops at the &lt;/s&gt; (\"end of sequence\") token. The prompt is \"Question: {question} Answer: {answer}\" for visual question answering tasks.", "md": "We evaluate the caption generation on MS COCO Caption [LMB+14], and Flickr30k [YLHH14].\n\nWe use the test set of COCO Karpathy split [KFF17], which re-partitions the train2014 and val2014 images [LMB+14] into 113,287, 5,000, and 5,000 for the training set, validation set, and test set, respectively. We conduct an evaluation on Flickr30k\u2019s Karpathy split test set. The image resolution is 224x224. We use beam search to generate the captions, and the beam size is 5. In the few-shot settings, we randomly sample demonstrations from the training set. We use COCOEvalCap4 to compute CIDEr [VLZP15] and SPICE [AFJG16] scores as the evaluation metrics. We prompt KOSMOS-1 with \"An image of\" for zero-shot and few-shot caption generation experiments.\n\nFor visual question-answering tasks, we evaluate zero-shot and few-shot results on test-dev set of VQAv2 [GKSS+17] and test-dev set of VizWiz [GLS+18], respectively. The resolution of images is 224x224. We use greedy search for the decoding. We follow the normalization rules of the VQAv2 evaluation code5 when computing the VQA accuracy. We evaluate the performance of VQA in an open-ended setting that KOSMOS-1 generates answers and stops at the &lt;/s&gt; (\"end of sequence\") token. The prompt is \"Question: {question} Answer: {answer}\" for visual question answering tasks."}, {"type": "heading", "lvl": 5, "value": "Results", "md": "##### Results"}, {"type": "text", "value": "Image Captioning\n\nTable 2 shows the zero-shot captioning performance on COCO Karpathy test split and Flickr30k test set. KOSMOS-1 achieves remarkable results in zero-shot setting on two image captioning datasets. Specifically, our model achieves a CIDEr score of 67.1 on the Flickr30k dataset, compared to 60.6 and 61.5 for the Flamingo-3B and Flamingo-9B models, respectively. Notably, our model is able to accomplish this feat with a smaller size of 1.6B, compared to Flamingo models. This demonstrates our model\u2019s superiority in zero-shot image captioning.\n\n4https://github.com/salaniz/pycocoevalcap\n\n5https://github.com/GT-Vision-Lab/VQA", "md": "Image Captioning\n\nTable 2 shows the zero-shot captioning performance on COCO Karpathy test split and Flickr30k test set. KOSMOS-1 achieves remarkable results in zero-shot setting on two image captioning datasets. Specifically, our model achieves a CIDEr score of 67.1 on the Flickr30k dataset, compared to 60.6 and 61.5 for the Flamingo-3B and Flamingo-9B models, respectively. Notably, our model is able to accomplish this feat with a smaller size of 1.6B, compared to Flamingo models. This demonstrates our model\u2019s superiority in zero-shot image captioning.\n\n4https://github.com/salaniz/pycocoevalcap\n\n5https://github.com/GT-Vision-Lab/VQA"}]}, {"page": 9, "text": "                        Model                            COCO                 Flickr30k\n                                                   CIDEr       SPICE       CIDEr      SPICE\n                        ZeroCap                      14.6        5.5          -           -\n                        VLKD                         58.3       13.4          -           -\n                        FewVLM                         -           -        31.0        10.0\n                        METALM                       82.2       15.7        43.4        11.7\n                        Flamingo-3B\u2217                 73.0          -        60.6          -\n                        Flamingo-9B\u2217                 79.4          -        61.5          -\n                        KOSMOS-1 (1.6B)              84.7       16.8        67.1        14.5\nTable 2: Zero-shot image captioning results on COCO caption Karpathy test and Flickr30k test.\n\u2217 Flamingo [ADL+22] prompts with two examples from the downstream tasks while removing their\ncorresponding images (i.e., similar to few-shot text prompts). The other models do not include any\nexamples in the prompt.\nTable 3 reports the results of the few-shot (k = 2, 4, 8) settings. The overall performance improves as\nthe number of shots increases from two to four. The trends are consistent across the two datasets.\nMoreover, the few-shot results outperform zero-shot captioning in Table 2.\n               Model                                COCO                           Flickr30k\n                                          k = 2      k = 4      k = 8      k = 2     k = 4      k = 8\n               Flamingo-3B                   -        85.0       90.6         -       72.0       71.7\n               Flamingo-9B                   -        93.1       99.0         -       72.6       73.4\n               KOSMOS-1 (1.6B)             99.6      101.7       96.7       70.0      75.3       68.0\nTable 3: Few-shot image captioning results on COCO caption Karpathy test and Flickr30k test.\nCIDEr scores are reported.\nVisual Question Answering             Table 4 reports the zero-shot visual question answering results on\nVQAv2 and VizWiz. We show that KOSMOS-1 can better handle the diversity and complexity of\nthe VizWiz dataset. KOSMOS-1 achieves higher accuracy and robustness than Flamingo-3B and\nFlamingo-9B models. In addition, our model is competitive with Flamingo on the VQAv2 dataset.\n                                   Model                      VQAv2        VizWiz\n                                   Frozen                       29.5           -\n                                   VLKDViT-B/16                 38.6           -\n                                   METALM                       41.1           -\n                                   Flamingo-3B\u2217                 49.2         28.9\n                                   Flamingo-9B\u2217                 51.8         28.8\n                                   KOSMOS-1 (1.6B)              51.0         29.2\nTable 4: Zero-shot visual question answering results on VQAv2 and VizWiz. We present VQA\naccuracy scores. \u201c\u2217\u201d: Flamingo [ADL+22] builds the zero-shot prompt with two examples from\nthe downstream tasks where their corresponding images are removed (i.e., similar to few-shot text\nprompts) while the others evaluate true zero-shot learning.\nTable 5 shows the few-shot performance on visual question answering tasks. KOSMOS-1 outperforms\nother models in few-shot (k = 2, 4) settings on the VizWiz dataset. We also observe a positive\ncorrelation between the number of shots and the quality of the results on the VizWiz dataset. Moreover,\nthe few-shot results are better than the zero-shot numbers as reported in Table 4.\n                                                           9", "md": "|Model|COCO|Flickr30k|\n|---|---|---|\n|ZeroCap|14.6 (CIDEr) 5.5 (SPICE)|-|\n|VLKD|58.3 (CIDEr) 13.4 (SPICE)|-|\n|FewVLM|-|31.0 (CIDEr) 10.0 (SPICE)|\n|METALM|82.2 (CIDEr) 15.7 (SPICE)|43.4 (CIDEr) 11.7 (SPICE)|\n|Flamingo-3B*|73.0 (CIDEr)|60.6 (CIDEr)|\n|Flamingo-9B*|79.4 (CIDEr)|61.5 (CIDEr)|\n|KOSMOS-1 (1.6B)|84.7 (CIDEr) 16.8 (SPICE)|67.1 (CIDEr) 14.5 (SPICE)|\n\nTable 2: Zero-shot image captioning results on COCO caption Karpathy test and Flickr30k test.\n\n* Flamingo [ADL+22] prompts with two examples from the downstream tasks while removing their corresponding images (i.e., similar to few-shot text prompts). The other models do not include any examples in the prompt.\n\n|Model|COCO|Flickr30k|\n|---|---|---|\n|Flamingo-3B|-|85.0 (k=4) 90.6 (k=8)|\n|Flamingo-9B|-|93.1 (k=4) 99.0 (k=8)|\n|KOSMOS-1 (1.6B)|99.6 (k=2) 101.7 (k=4) 96.7 (k=8)|70.0 (k=2) 75.3 (k=4) 68.0 (k=8)|\n\nTable 3: Few-shot image captioning results on COCO caption Karpathy test and Flickr30k test. CIDEr scores are reported.\n\nVisual Question Answering\n\nTable 4 reports the zero-shot visual question answering results on VQAv2 and VizWiz. We show that KOSMOS-1 can better handle the diversity and complexity of the VizWiz dataset. KOSMOS-1 achieves higher accuracy and robustness than Flamingo-3B and Flamingo-9B models. In addition, our model is competitive with Flamingo on the VQAv2 dataset.\n\n|Model|VQAv2|VizWiz|\n|---|---|---|\n|Frozen|29.5|-|\n|VLKDViT-B/16|38.6|-|\n|METALM|41.1|-|\n|Flamingo-3B*|49.2|28.9|\n|Flamingo-9B*|51.8|28.8|\n|KOSMOS-1 (1.6B)|51.0|29.2|\n\nTable 4: Zero-shot visual question answering results on VQAv2 and VizWiz. We present VQA accuracy scores. \"*\": Flamingo [ADL+22] builds the zero-shot prompt with two examples from the downstream tasks where their corresponding images are removed (i.e., similar to few-shot text prompts) while the others evaluate true zero-shot learning.\n\nTable 5 shows the few-shot performance on visual question answering tasks. KOSMOS-1 outperforms other models in few-shot (k = 2, 4) settings on the VizWiz dataset. We also observe a positive correlation between the number of shots and the quality of the results on the VizWiz dataset. Moreover, the few-shot results are better than the zero-shot numbers as reported in Table 4.", "images": [], "items": [{"type": "table", "rows": [["Model", "COCO", "Flickr30k"], ["ZeroCap", "14.6 (CIDEr) 5.5 (SPICE)", "-"], ["VLKD", "58.3 (CIDEr) 13.4 (SPICE)", "-"], ["FewVLM", "-", "31.0 (CIDEr) 10.0 (SPICE)"], ["METALM", "82.2 (CIDEr) 15.7 (SPICE)", "43.4 (CIDEr) 11.7 (SPICE)"], ["Flamingo-3B*", "73.0 (CIDEr)", "60.6 (CIDEr)"], ["Flamingo-9B*", "79.4 (CIDEr)", "61.5 (CIDEr)"], ["KOSMOS-1 (1.6B)", "84.7 (CIDEr) 16.8 (SPICE)", "67.1 (CIDEr) 14.5 (SPICE)"]], "md": "|Model|COCO|Flickr30k|\n|---|---|---|\n|ZeroCap|14.6 (CIDEr) 5.5 (SPICE)|-|\n|VLKD|58.3 (CIDEr) 13.4 (SPICE)|-|\n|FewVLM|-|31.0 (CIDEr) 10.0 (SPICE)|\n|METALM|82.2 (CIDEr) 15.7 (SPICE)|43.4 (CIDEr) 11.7 (SPICE)|\n|Flamingo-3B*|73.0 (CIDEr)|60.6 (CIDEr)|\n|Flamingo-9B*|79.4 (CIDEr)|61.5 (CIDEr)|\n|KOSMOS-1 (1.6B)|84.7 (CIDEr) 16.8 (SPICE)|67.1 (CIDEr) 14.5 (SPICE)|", "isPerfectTable": true, "csv": "\"Model\",\"COCO\",\"Flickr30k\"\n\"ZeroCap\",\"14.6 (CIDEr) 5.5 (SPICE)\",\"-\"\n\"VLKD\",\"58.3 (CIDEr) 13.4 (SPICE)\",\"-\"\n\"FewVLM\",\"-\",\"31.0 (CIDEr) 10.0 (SPICE)\"\n\"METALM\",\"82.2 (CIDEr) 15.7 (SPICE)\",\"43.4 (CIDEr) 11.7 (SPICE)\"\n\"Flamingo-3B*\",\"73.0 (CIDEr)\",\"60.6 (CIDEr)\"\n\"Flamingo-9B*\",\"79.4 (CIDEr)\",\"61.5 (CIDEr)\"\n\"KOSMOS-1 (1.6B)\",\"84.7 (CIDEr) 16.8 (SPICE)\",\"67.1 (CIDEr) 14.5 (SPICE)\""}, {"type": "text", "value": "Table 2: Zero-shot image captioning results on COCO caption Karpathy test and Flickr30k test.\n\n* Flamingo [ADL+22] prompts with two examples from the downstream tasks while removing their corresponding images (i.e., similar to few-shot text prompts). The other models do not include any examples in the prompt.", "md": "Table 2: Zero-shot image captioning results on COCO caption Karpathy test and Flickr30k test.\n\n* Flamingo [ADL+22] prompts with two examples from the downstream tasks while removing their corresponding images (i.e., similar to few-shot text prompts). The other models do not include any examples in the prompt."}, {"type": "table", "rows": [["Model", "COCO", "Flickr30k"], ["Flamingo-3B", "-", "85.0 (k=4) 90.6 (k=8)"], ["Flamingo-9B", "-", "93.1 (k=4) 99.0 (k=8)"], ["KOSMOS-1 (1.6B)", "99.6 (k=2) 101.7 (k=4) 96.7 (k=8)", "70.0 (k=2) 75.3 (k=4) 68.0 (k=8)"]], "md": "|Model|COCO|Flickr30k|\n|---|---|---|\n|Flamingo-3B|-|85.0 (k=4) 90.6 (k=8)|\n|Flamingo-9B|-|93.1 (k=4) 99.0 (k=8)|\n|KOSMOS-1 (1.6B)|99.6 (k=2) 101.7 (k=4) 96.7 (k=8)|70.0 (k=2) 75.3 (k=4) 68.0 (k=8)|", "isPerfectTable": true, "csv": "\"Model\",\"COCO\",\"Flickr30k\"\n\"Flamingo-3B\",\"-\",\"85.0 (k=4) 90.6 (k=8)\"\n\"Flamingo-9B\",\"-\",\"93.1 (k=4) 99.0 (k=8)\"\n\"KOSMOS-1 (1.6B)\",\"99.6 (k=2) 101.7 (k=4) 96.7 (k=8)\",\"70.0 (k=2) 75.3 (k=4) 68.0 (k=8)\""}, {"type": "text", "value": "Table 3: Few-shot image captioning results on COCO caption Karpathy test and Flickr30k test. CIDEr scores are reported.\n\nVisual Question Answering\n\nTable 4 reports the zero-shot visual question answering results on VQAv2 and VizWiz. We show that KOSMOS-1 can better handle the diversity and complexity of the VizWiz dataset. KOSMOS-1 achieves higher accuracy and robustness than Flamingo-3B and Flamingo-9B models. In addition, our model is competitive with Flamingo on the VQAv2 dataset.", "md": "Table 3: Few-shot image captioning results on COCO caption Karpathy test and Flickr30k test. CIDEr scores are reported.\n\nVisual Question Answering\n\nTable 4 reports the zero-shot visual question answering results on VQAv2 and VizWiz. We show that KOSMOS-1 can better handle the diversity and complexity of the VizWiz dataset. KOSMOS-1 achieves higher accuracy and robustness than Flamingo-3B and Flamingo-9B models. In addition, our model is competitive with Flamingo on the VQAv2 dataset."}, {"type": "table", "rows": [["Model", "VQAv2", "VizWiz"], ["Frozen", "29.5", "-"], ["VLKDViT-B/16", "38.6", "-"], ["METALM", "41.1", "-"], ["Flamingo-3B*", "49.2", "28.9"], ["Flamingo-9B*", "51.8", "28.8"], ["KOSMOS-1 (1.6B)", "51.0", "29.2"]], "md": "|Model|VQAv2|VizWiz|\n|---|---|---|\n|Frozen|29.5|-|\n|VLKDViT-B/16|38.6|-|\n|METALM|41.1|-|\n|Flamingo-3B*|49.2|28.9|\n|Flamingo-9B*|51.8|28.8|\n|KOSMOS-1 (1.6B)|51.0|29.2|", "isPerfectTable": true, "csv": "\"Model\",\"VQAv2\",\"VizWiz\"\n\"Frozen\",\"29.5\",\"-\"\n\"VLKDViT-B/16\",\"38.6\",\"-\"\n\"METALM\",\"41.1\",\"-\"\n\"Flamingo-3B*\",\"49.2\",\"28.9\"\n\"Flamingo-9B*\",\"51.8\",\"28.8\"\n\"KOSMOS-1 (1.6B)\",\"51.0\",\"29.2\""}, {"type": "text", "value": "Table 4: Zero-shot visual question answering results on VQAv2 and VizWiz. We present VQA accuracy scores. \"*\": Flamingo [ADL+22] builds the zero-shot prompt with two examples from the downstream tasks where their corresponding images are removed (i.e., similar to few-shot text prompts) while the others evaluate true zero-shot learning.\n\nTable 5 shows the few-shot performance on visual question answering tasks. KOSMOS-1 outperforms other models in few-shot (k = 2, 4) settings on the VizWiz dataset. We also observe a positive correlation between the number of shots and the quality of the results on the VizWiz dataset. Moreover, the few-shot results are better than the zero-shot numbers as reported in Table 4.", "md": "Table 4: Zero-shot visual question answering results on VQAv2 and VizWiz. We present VQA accuracy scores. \"*\": Flamingo [ADL+22] builds the zero-shot prompt with two examples from the downstream tasks where their corresponding images are removed (i.e., similar to few-shot text prompts) while the others evaluate true zero-shot learning.\n\nTable 5 shows the few-shot performance on visual question answering tasks. KOSMOS-1 outperforms other models in few-shot (k = 2, 4) settings on the VizWiz dataset. We also observe a positive correlation between the number of shots and the quality of the results on the VizWiz dataset. Moreover, the few-shot results are better than the zero-shot numbers as reported in Table 4."}]}, {"page": 10, "text": "                       Model                                                   VQAv2                                         VizWiz\n                                                                k = 2           k = 4           k = 8            k = 2         k = 4           k = 8\n                       Frozen                                        -            38.2               -               -             -                -\n                        METALM                                       -            45.3               -               -             -                -\n                       Flamingo-3B                                   -            53.2            55.4               -          34.4            38.4\n                       Flamingo-9B                                   -            56.3            58.0               -          34.9            39.4\n                        KOSMOS-1 (1.6B)                           51.4            51.8            51.4            31.4          35.3            39.0\nTable 5: Few-shot visual question answering results on VQAv2 and VizWiz.VQA accuracy scores\nare reported.\n                                                                                                                                    Example ofiQ Test\n                                                                                                                                     Example of IQ Test\n                                                         Which option can complete the matrix?\n                                                         A                    B                C              D                E                  F\n                                     ?\n             Input Prompt\n              Input Prompt\n                Here are                                                                                                                      The following\n                eight images:                                                                                                                 image is:\n          Is it correct?             Is it correct?              Is it correct?               Is it correct?            Is it correct?              Is it correct?\n                   Yes                        Yes                         Yes                         Yes                       Yes                          Yes\n              P( Kes      Input Prompt |\n              \ud835\udc43( Yes | Input Prompt )\n                      A                         B                           C                          D                         E                            F\nFigure 4: Top: An example of Raven IQ test. Bottom: Evaluate KOSMOS-1 on Raven IQ test.\nThe input prompt consists of the flattened image matrix and verbal instruction. We append each\ncandidate image to the prompt separately and query the model if it is correct. The final prediction is\nthe candidate that motivates the model to yield the highest probability of \u201cYes\u201d.\n4.2        IQ Test: Nonverbal Reasoning\nRaven\u2019s Progressive Matrices [CJS90, JR03] is one of the most common tests to evaluate nonverbal\nreasoning. The capability of nonverbal reasoning is typically a reflection of an individual\u2019s intelligence\nquotient (IQ). Figure 4 shows an example. Given eight images presented in a 3 \u00d7 3 matrix, the task is\nto identify the following element from six similar candidates.\nThe models need to conduct zero-shot nonverbal reasoning without explicitly fine-tuning. The Raven\nIQ test is analogous to in-context learning of language models, where the difference is whether the\ncontext is nonverbal or verbal. In order to infer the answers, the models have to recognize abstract\nconcepts and identify the underlying patterns of given images. So the IQ task is a good testbed to\nbenchmark the nonverbal in-context learning capability.\n                                                                                        10", "md": "# IQ Test Results\n\n## Table 5: Few-shot visual question answering results on VQAv2 and VizWiz\n\n|Model|VQAv2| | |VizWiz|\n|---|---|---|---|---|\n| |k = 2|k = 4|k = 8|k = 2|k = 4|k = 8|\n|Frozen|-|38.2|-|-|-|-|\n|METALM|-|45.3|-|-|-|-|\n|Flamingo-3B|-|53.2|55.4|-|34.4|38.4|\n|Flamingo-9B|-|56.3|58.0|-|34.9|39.4|\n|KOSMOS-1 (1.6B)|51.4|51.8|51.4|31.4|35.3|39.0|\n\n## Figure 4: Top: An example of Raven IQ test. Bottom: Evaluate KOSMOS-1 on Raven IQ test.\n\nInput Prompt: Here are eight images. Is it correct?\n\nInput Prompt: The following image is:\n\nWhich option can complete the matrix?\n\n| |A|B|C|D|E|F|\n|---|---|---|---|---|---|---|\n| | | |?| | | |\n\nInput Prompt: P( Kes | Input Prompt )\n\nInput Prompt: P( Yes | Input Prompt )\n\n| |A|B|C|D|E|F|\n|---|---|---|---|---|---|---|\n|Yes|Yes|Yes|Yes|Yes|Yes|Yes|\n\n### IQ Test: Nonverbal Reasoning\n\nRaven\u2019s Progressive Matrices is one of the most common tests to evaluate nonverbal reasoning.\n\nThe models need to conduct zero-shot nonverbal reasoning without explicitly fine-tuning.\n\nThe Raven IQ test is analogous to in-context learning of language models.\n\nThe IQ task is a good testbed to benchmark the nonverbal in-context learning capability.", "images": [{"name": "page-10-0.jpg", "height": 27, "width": 27, "x": 359, "y": 287}, {"name": "page-10-4.jpg", "height": 29, "width": 32, "x": 130, "y": 267}, {"name": "page-10-5.jpg", "height": 29, "width": 29, "x": 243, "y": 286}, {"name": "page-10-8.jpg", "height": 28, "width": 29, "x": 400, "y": 287}, {"name": "page-10-1.jpg", "height": 25, "width": 51, "x": 130, "y": 241}, {"name": "page-10-2.jpg", "height": 27, "width": 29, "x": 182, "y": 240}, {"name": "page-10-3.jpg", "height": 57, "width": 33, "x": 128, "y": 267}, {"name": "page-10-6.jpg", "height": 27, "width": 29, "x": 285, "y": 287}, {"name": "page-10-9.jpg", "height": 24, "width": 24, "x": 439, "y": 289}, {"name": "page-10-10.jpg", "height": 19, "width": 74, "x": 398, "y": 232}, {"name": "page-10-14.jpg", "height": 25, "width": 51, "x": 204, "y": 348}, {"name": "page-10-11.jpg", "height": 19, "width": 89, "x": 132, "y": 439}, {"name": "page-10-15.jpg", "height": 27, "width": 29, "x": 255, "y": 347}, {"name": "page-10-12.jpg", "height": 29, "width": 32, "x": 286, "y": 347}, {"name": "page-10-17.jpg", "height": 29, "width": 29, "x": 137, "y": 379}, {"name": "page-10-7.jpg", "height": 29, "width": 32, "x": 318, "y": 286}, {"name": "page-10-18.jpg", "height": 19, "width": 23, "x": 143, "y": 420}, {"name": "page-10-13.jpg", "height": 30, "width": 34, "x": 365, "y": 344}, {"name": "page-10-19.jpg", "height": 19, "width": 23, "x": 203, "y": 420}, {"name": "page-10-21.jpg", "height": 19, "width": 23, "x": 327, "y": 420}, {"name": "page-10-23.jpg", "height": 19, "width": 23, "x": 453, "y": 420}, {"name": "page-10-25.jpg", "height": 29, "width": 32, "x": 257, "y": 380}, {"name": "page-10-20.jpg", "height": 19, "width": 23, "x": 264, "y": 420}, {"name": "page-10-16.jpg", "height": 19, "width": 57, "x": 131, "y": 329}, {"name": "page-10-24.jpg", "height": 27, "width": 29, "x": 200, "y": 380}, {"name": "page-10-22.jpg", "height": 19, "width": 23, "x": 389, "y": 420}, {"name": "page-10-26.jpg", "height": 27, "width": 27, "x": 324, "y": 381}, {"name": "page-10-28.jpg", "height": 24, "width": 24, "x": 449, "y": 382}, {"name": "page-10-27.jpg", "height": 28, "width": 29, "x": 384, "y": 381}], "items": [{"type": "heading", "lvl": 1, "value": "IQ Test Results", "md": "# IQ Test Results"}, {"type": "heading", "lvl": 2, "value": "Table 5: Few-shot visual question answering results on VQAv2 and VizWiz", "md": "## Table 5: Few-shot visual question answering results on VQAv2 and VizWiz"}, {"type": "table", "rows": [["Model", "VQAv2", "", "", "VizWiz"], ["", "k = 2", "k = 4", "k = 8", "k = 2", "k = 4", "k = 8"], ["Frozen", "-", "38.2", "-", "-", "-", "-"], ["METALM", "-", "45.3", "-", "-", "-", "-"], ["Flamingo-3B", "-", "53.2", "55.4", "-", "34.4", "38.4"], ["Flamingo-9B", "-", "56.3", "58.0", "-", "34.9", "39.4"], ["KOSMOS-1 (1.6B)", "51.4", "51.8", "51.4", "31.4", "35.3", "39.0"]], "md": "|Model|VQAv2| | |VizWiz|\n|---|---|---|---|---|\n| |k = 2|k = 4|k = 8|k = 2|k = 4|k = 8|\n|Frozen|-|38.2|-|-|-|-|\n|METALM|-|45.3|-|-|-|-|\n|Flamingo-3B|-|53.2|55.4|-|34.4|38.4|\n|Flamingo-9B|-|56.3|58.0|-|34.9|39.4|\n|KOSMOS-1 (1.6B)|51.4|51.8|51.4|31.4|35.3|39.0|", "isPerfectTable": false, "csv": "\"Model\",\"VQAv2\",\"\",\"\",\"VizWiz\"\n\"\",\"k = 2\",\"k = 4\",\"k = 8\",\"k = 2\",\"k = 4\",\"k = 8\"\n\"Frozen\",\"-\",\"38.2\",\"-\",\"-\",\"-\",\"-\"\n\"METALM\",\"-\",\"45.3\",\"-\",\"-\",\"-\",\"-\"\n\"Flamingo-3B\",\"-\",\"53.2\",\"55.4\",\"-\",\"34.4\",\"38.4\"\n\"Flamingo-9B\",\"-\",\"56.3\",\"58.0\",\"-\",\"34.9\",\"39.4\"\n\"KOSMOS-1 (1.6B)\",\"51.4\",\"51.8\",\"51.4\",\"31.4\",\"35.3\",\"39.0\""}, {"type": "heading", "lvl": 2, "value": "Figure 4: Top: An example of Raven IQ test. Bottom: Evaluate KOSMOS-1 on Raven IQ test.", "md": "## Figure 4: Top: An example of Raven IQ test. Bottom: Evaluate KOSMOS-1 on Raven IQ test."}, {"type": "text", "value": "Input Prompt: Here are eight images. Is it correct?\n\nInput Prompt: The following image is:\n\nWhich option can complete the matrix?", "md": "Input Prompt: Here are eight images. Is it correct?\n\nInput Prompt: The following image is:\n\nWhich option can complete the matrix?"}, {"type": "table", "rows": [["", "A", "B", "C", "D", "E", "F"], ["", "", "", "?", "", "", ""]], "md": "| |A|B|C|D|E|F|\n|---|---|---|---|---|---|---|\n| | | |?| | | |", "isPerfectTable": true, "csv": "\"\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"\n\"\",\"\",\"\",\"?\",\"\",\"\",\"\""}, {"type": "text", "value": "Input Prompt: P( Kes | Input Prompt )\n\nInput Prompt: P( Yes | Input Prompt )", "md": "Input Prompt: P( Kes | Input Prompt )\n\nInput Prompt: P( Yes | Input Prompt )"}, {"type": "table", "rows": [["", "A", "B", "C", "D", "E", "F"], ["Yes", "Yes", "Yes", "Yes", "Yes", "Yes", "Yes"]], "md": "| |A|B|C|D|E|F|\n|---|---|---|---|---|---|---|\n|Yes|Yes|Yes|Yes|Yes|Yes|Yes|", "isPerfectTable": true, "csv": "\"\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"\n\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\",\"Yes\""}, {"type": "heading", "lvl": 3, "value": "IQ Test: Nonverbal Reasoning", "md": "### IQ Test: Nonverbal Reasoning"}, {"type": "text", "value": "Raven\u2019s Progressive Matrices is one of the most common tests to evaluate nonverbal reasoning.\n\nThe models need to conduct zero-shot nonverbal reasoning without explicitly fine-tuning.\n\nThe Raven IQ test is analogous to in-context learning of language models.\n\nThe IQ task is a good testbed to benchmark the nonverbal in-context learning capability.", "md": "Raven\u2019s Progressive Matrices is one of the most common tests to evaluate nonverbal reasoning.\n\nThe models need to conduct zero-shot nonverbal reasoning without explicitly fine-tuning.\n\nThe Raven IQ test is analogous to in-context learning of language models.\n\nThe IQ task is a good testbed to benchmark the nonverbal in-context learning capability."}]}, {"page": 11, "text": "4.2.1   Evaluation Setup\nTo evaluate the KOSMOS-1 on zero-shot nonverbal reasoning, we construct a dataset of the Raven IQ\ntest. It consists of 50 examples collected from different websites6789. Each example has three (i.e.,\n2\u00d72 matrix), four, or eight (i.e., 3\u00d73 matrix) given images. The goal is to predict the next one. Each\ninstance has six candidate images with a unique correct completion. We measure accuracy scores to\nevaluate the models. The evaluation dataset is available at https://aka.ms/kosmos-iq50.\nFigure 4 illustrates how to evaluate KOSMOS-1 on the Raven IQ test. The matrix-style images are\nflattened and fed into the models one-by-one. To enable the model to better understand the desired\ntask, we also use a textual instruction \u201cHere are three/four/eight images:\u201d, \u201cThe following image is:\u201d,\nand \u201cIs it correct?\u201d for conditioning. We append each possible candidate to the context separately\nand compare the probability that the model outputs \u201cYes\u201d in a close-ended setting. The candidate that\nyields the largest probability is regarded as the prediction.\n4.2.2   Results\nTable 6 shows the evaluation results on the IQ test dataset. Both KOSMOS-1 with and without\nlanguage-only instruction tuning achieve 5.3% and 9.3% improvement respectively over the random\nbaseline. The results indicate that KOSMOS-1 is able to perceive abstract conceptual patterns in a\nnonverbal context, and then deduce the following element across multiple choices. To the best of\nour knowledge, it is the first time that a model can perform such zero-shot Raven IQ tests. Although\nthere is still a large performance gap between the current model and the average level of adults,\nKOSMOS-1 demonstrates the potential of MLLMs to perform zero-shot nonverbal reasoning by\naligning perception with language models.\n                         Method                                     Accuracy\n                         Random Choice                                 17%\n                         KOSMOS-1                                      22%\n                          w/o language-only instruction tuning         26%\n                         Table 6: Zero-shot generalization on Raven IQ test.\n4.3   OCR-Free Language Understanding\nOCR-free language understanding is a task that focuses on understanding text and images without\nrelying on Optical Character Recognition (OCR). For example, during the Rendered SST-2 task,\nsentences from the Stanford Sentiment Treebank [SPW+13] dataset are rendered as images. The\nmodel is asked to predict the sentiment of the text within the images. The task evaluates a model\u2019s\nability to read and comprehend the meaning of words and sentences directly from the images.\n4.3.1   Evaluation Setup\nWe evaluate OCR-free language understanding on the Rendered SST-2 [RKH+21] test set and\nHatefulMemes [KFM+20] validation set. We use accuracy as the metric for the Rendered SST-2 and\nreport ROC AUC for the HatefulMemes dataset. We use the prompt \u201cQuestion: what is the sentiment\nof the opinion? Answer: {answer}\u201d, where the answer is either positive or negative for the Rendered\nSST-2. For the HatefulMemes task, the prompt is \u201cQuestion: does this picture contain real hate\nspeech? Answer: {answer}\u201d, where the answer is either yes or no.\n4.3.2   Results\nAs shown in Table 7, KOSMOS-1 achieves a ROC AUC of 63.9% for the HatefulMemes validation\nset and a test accuracy of 67.1% for the Rendered SST-2 test set. It outperforms CLIP ViT-L\n   6https://en.testometrika.com/intellectual/iq-test/\n   7https://en.testometrika.com/intellectual/iq-test-for-kids-7-to-16-year-old/\n   8https://iqpro.org/\n   9https://iqhaven.com/matrix-g\n                                                   11", "md": "## 4.2.1 Evaluation Setup\n\nTo evaluate the KOSMOS-1 on zero-shot nonverbal reasoning, we construct a dataset of the Raven IQ test. It consists of 50 examples collected from different websites6789. Each example has three (i.e., 2x2 matrix), four, or eight (i.e., 3x3 matrix) given images. The goal is to predict the next one. Each instance has six candidate images with a unique correct completion. We measure accuracy scores to evaluate the models. The evaluation dataset is available at https://aka.ms/kosmos-iq50.\n\nFigure 4 illustrates how to evaluate KOSMOS-1 on the Raven IQ test. The matrix-style images are flattened and fed into the models one-by-one. To enable the model to better understand the desired task, we also use a textual instruction \"Here are three/four/eight images:\", \"The following image is:\", and \"Is it correct?\" for conditioning. We append each possible candidate to the context separately and compare the probability that the model outputs \"Yes\" in a close-ended setting. The candidate that yields the largest probability is regarded as the prediction.\n\n## 4.2.2 Results\n\n|Method|Accuracy|\n|---|---|\n|Random Choice|17%|\n|KOSMOS-1|22%|\n|w/o language-only instruction tuning|26%|\n\nTable 6: Zero-shot generalization on Raven IQ test.\n\nTable 6 shows the evaluation results on the IQ test dataset. Both KOSMOS-1 with and without language-only instruction tuning achieve 5.3% and 9.3% improvement respectively over the random baseline. The results indicate that KOSMOS-1 is able to perceive abstract conceptual patterns in a nonverbal context, and then deduce the following element across multiple choices. To the best of our knowledge, it is the first time that a model can perform such zero-shot Raven IQ tests. Although there is still a large performance gap between the current model and the average level of adults, KOSMOS-1 demonstrates the potential of MLLMs to perform zero-shot nonverbal reasoning by aligning perception with language models.\n\n## 4.3 OCR-Free Language Understanding\n\nOCR-free language understanding is a task that focuses on understanding text and images without relying on Optical Character Recognition (OCR). For example, during the Rendered SST-2 task, sentences from the Stanford Sentiment Treebank [SPW+13] dataset are rendered as images. The model is asked to predict the sentiment of the text within the images. The task evaluates a model's ability to read and comprehend the meaning of words and sentences directly from the images.\n\n## 4.3.1 Evaluation Setup\n\nWe evaluate OCR-free language understanding on the Rendered SST-2 [RKH+21] test set and HatefulMemes [KFM+20] validation set. We use accuracy as the metric for the Rendered SST-2 and report ROC AUC for the HatefulMemes dataset. We use the prompt \"Question: what is the sentiment of the opinion? Answer: {answer}\", where the answer is either positive or negative for the Rendered SST-2. For the HatefulMemes task, the prompt is \"Question: does this picture contain real hate speech? Answer: {answer}\", where the answer is either yes or no.\n\n## 4.3.2 Results\n\n|Dataset|Performance Metric|Score|\n|---|---|---|\n|Rendered SST-2 test set|Accuracy|67.1%|\n|HatefulMemes validation set|ROC AUC|63.9%|\n\nKOSMOS-1 outperforms CLIP ViT-L.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "4.2.1 Evaluation Setup", "md": "## 4.2.1 Evaluation Setup"}, {"type": "text", "value": "To evaluate the KOSMOS-1 on zero-shot nonverbal reasoning, we construct a dataset of the Raven IQ test. It consists of 50 examples collected from different websites6789. Each example has three (i.e., 2x2 matrix), four, or eight (i.e., 3x3 matrix) given images. The goal is to predict the next one. Each instance has six candidate images with a unique correct completion. We measure accuracy scores to evaluate the models. The evaluation dataset is available at https://aka.ms/kosmos-iq50.\n\nFigure 4 illustrates how to evaluate KOSMOS-1 on the Raven IQ test. The matrix-style images are flattened and fed into the models one-by-one. To enable the model to better understand the desired task, we also use a textual instruction \"Here are three/four/eight images:\", \"The following image is:\", and \"Is it correct?\" for conditioning. We append each possible candidate to the context separately and compare the probability that the model outputs \"Yes\" in a close-ended setting. The candidate that yields the largest probability is regarded as the prediction.", "md": "To evaluate the KOSMOS-1 on zero-shot nonverbal reasoning, we construct a dataset of the Raven IQ test. It consists of 50 examples collected from different websites6789. Each example has three (i.e., 2x2 matrix), four, or eight (i.e., 3x3 matrix) given images. The goal is to predict the next one. Each instance has six candidate images with a unique correct completion. We measure accuracy scores to evaluate the models. The evaluation dataset is available at https://aka.ms/kosmos-iq50.\n\nFigure 4 illustrates how to evaluate KOSMOS-1 on the Raven IQ test. The matrix-style images are flattened and fed into the models one-by-one. To enable the model to better understand the desired task, we also use a textual instruction \"Here are three/four/eight images:\", \"The following image is:\", and \"Is it correct?\" for conditioning. We append each possible candidate to the context separately and compare the probability that the model outputs \"Yes\" in a close-ended setting. The candidate that yields the largest probability is regarded as the prediction."}, {"type": "heading", "lvl": 2, "value": "4.2.2 Results", "md": "## 4.2.2 Results"}, {"type": "table", "rows": [["Method", "Accuracy"], ["Random Choice", "17%"], ["KOSMOS-1", "22%"], ["w/o language-only instruction tuning", "26%"]], "md": "|Method|Accuracy|\n|---|---|\n|Random Choice|17%|\n|KOSMOS-1|22%|\n|w/o language-only instruction tuning|26%|", "isPerfectTable": true, "csv": "\"Method\",\"Accuracy\"\n\"Random Choice\",\"17%\"\n\"KOSMOS-1\",\"22%\"\n\"w/o language-only instruction tuning\",\"26%\""}, {"type": "text", "value": "Table 6: Zero-shot generalization on Raven IQ test.\n\nTable 6 shows the evaluation results on the IQ test dataset. Both KOSMOS-1 with and without language-only instruction tuning achieve 5.3% and 9.3% improvement respectively over the random baseline. The results indicate that KOSMOS-1 is able to perceive abstract conceptual patterns in a nonverbal context, and then deduce the following element across multiple choices. To the best of our knowledge, it is the first time that a model can perform such zero-shot Raven IQ tests. Although there is still a large performance gap between the current model and the average level of adults, KOSMOS-1 demonstrates the potential of MLLMs to perform zero-shot nonverbal reasoning by aligning perception with language models.", "md": "Table 6: Zero-shot generalization on Raven IQ test.\n\nTable 6 shows the evaluation results on the IQ test dataset. Both KOSMOS-1 with and without language-only instruction tuning achieve 5.3% and 9.3% improvement respectively over the random baseline. The results indicate that KOSMOS-1 is able to perceive abstract conceptual patterns in a nonverbal context, and then deduce the following element across multiple choices. To the best of our knowledge, it is the first time that a model can perform such zero-shot Raven IQ tests. Although there is still a large performance gap between the current model and the average level of adults, KOSMOS-1 demonstrates the potential of MLLMs to perform zero-shot nonverbal reasoning by aligning perception with language models."}, {"type": "heading", "lvl": 2, "value": "4.3 OCR-Free Language Understanding", "md": "## 4.3 OCR-Free Language Understanding"}, {"type": "text", "value": "OCR-free language understanding is a task that focuses on understanding text and images without relying on Optical Character Recognition (OCR). For example, during the Rendered SST-2 task, sentences from the Stanford Sentiment Treebank [SPW+13] dataset are rendered as images. The model is asked to predict the sentiment of the text within the images. The task evaluates a model's ability to read and comprehend the meaning of words and sentences directly from the images.", "md": "OCR-free language understanding is a task that focuses on understanding text and images without relying on Optical Character Recognition (OCR). For example, during the Rendered SST-2 task, sentences from the Stanford Sentiment Treebank [SPW+13] dataset are rendered as images. The model is asked to predict the sentiment of the text within the images. The task evaluates a model's ability to read and comprehend the meaning of words and sentences directly from the images."}, {"type": "heading", "lvl": 2, "value": "4.3.1 Evaluation Setup", "md": "## 4.3.1 Evaluation Setup"}, {"type": "text", "value": "We evaluate OCR-free language understanding on the Rendered SST-2 [RKH+21] test set and HatefulMemes [KFM+20] validation set. We use accuracy as the metric for the Rendered SST-2 and report ROC AUC for the HatefulMemes dataset. We use the prompt \"Question: what is the sentiment of the opinion? Answer: {answer}\", where the answer is either positive or negative for the Rendered SST-2. For the HatefulMemes task, the prompt is \"Question: does this picture contain real hate speech? Answer: {answer}\", where the answer is either yes or no.", "md": "We evaluate OCR-free language understanding on the Rendered SST-2 [RKH+21] test set and HatefulMemes [KFM+20] validation set. We use accuracy as the metric for the Rendered SST-2 and report ROC AUC for the HatefulMemes dataset. We use the prompt \"Question: what is the sentiment of the opinion? Answer: {answer}\", where the answer is either positive or negative for the Rendered SST-2. For the HatefulMemes task, the prompt is \"Question: does this picture contain real hate speech? Answer: {answer}\", where the answer is either yes or no."}, {"type": "heading", "lvl": 2, "value": "4.3.2 Results", "md": "## 4.3.2 Results"}, {"type": "table", "rows": [["Dataset", "Performance Metric", "Score"], ["Rendered SST-2 test set", "Accuracy", "67.1%"], ["HatefulMemes validation set", "ROC AUC", "63.9%"]], "md": "|Dataset|Performance Metric|Score|\n|---|---|---|\n|Rendered SST-2 test set|Accuracy|67.1%|\n|HatefulMemes validation set|ROC AUC|63.9%|", "isPerfectTable": true, "csv": "\"Dataset\",\"Performance Metric\",\"Score\"\n\"Rendered SST-2 test set\",\"Accuracy\",\"67.1%\"\n\"HatefulMemes validation set\",\"ROC AUC\",\"63.9%\""}, {"type": "text", "value": "KOSMOS-1 outperforms CLIP ViT-L.", "md": "KOSMOS-1 outperforms CLIP ViT-L."}]}, {"page": 12, "text": "and Flamingo-9B, which achieve AUCs of 63.3% and 57.0% on the HatefulMemes task. Note\nthat Flamingo explicitly provides OCR text into the prompt, while KOSMOS-1 does not access\nany external tools or resources. This indicates that KOSMOS-1 has built-in abilities to read and\ncomprehend the text in the rendered images.\n                        Model                      HatefulMemes           Rendered SST-2\n                        CLIP ViT-B/32                     57.6                   59.6\n                        CLIP ViT-B/16                     61.7                   59.8\n                        CLIP ViT-L/14                     63.3                   64.0\n                        Flamingo-3B                       53.7                      -\n                        Flamingo-9B                       57.0                      -\n                        KOSMOS-1 (1.6B)                   63.9                   67.1\nTable 7: Zero-shot generalization on OCR-free language understanding. We report accuracy scores.\n4.4    Web Page Question Answering\nWeb page question answering aims at finding answers to questions from web pages. It requires the\nmodel to comprehend both the semantics and the structure of texts. The structure of the web page\n(such as tables, lists, and HTML layout) plays a key role in how the information is arranged and\ndisplayed. The task can help us evaluate our model\u2019s ability to understand the semantics and the\nstructure of web pages.\n4.4.1    Evaluation Setup\nWe compare the performance on the Web-based Structural Reading Comprehension (WebSRC)\ndataset [CZC+21]. For comparisons, we train a language model (LLM) on the same text corpora\nwith the same training setup as in KOSMOS-1. The LLM takes the text extracted from the web\npage as input. Its template of the prompt is \u201cGiven the context below from web page, extract the\nanswer from the given text like this: Qusestion: Who is the publisher of this book? Answer: Penguin\nBooks Ltd. Context: {WebText} Q: {question} A: {answer} \u201d, where the {WebText} presents the text\nextracted from the web page. Besides using the same prompt, KOSMOS-1 prepends the image before\nthe prompt. Two example images from WebSRC are shown in Appendix C.3. Following the original\npaper [CZC+21], we use exact match (EM) and F1 scores as our evaluation metrics.\n4.4.2    Results\nThe experimental results are summarized in Table 8. We observe that KOSMOS-1 outperforms the\nLLM, indicating that KOSMOS-1 can benefit from the layout and style information of web pages in\nimages. In addition, we evaluate the performance of KOSMOS-1 without the extracted text in the\nprompt. It shows that extracted text has a contribution of +12.0/20.7 EM/F1 to KOSMOS-1, indicating\nthat the benefit from modeling images does not sacrifice its language abilities.\n                                     Models                  EM              F1\n                                       Using extracted text\n                                      LLM                     7.6           17.9\n                                      KOSMOS-1               15.8           31.3\n                                       Without using extracted text\n                                      KOSMOS-1                3.8           10.6\n   Table 8: Zero-shot performance on WebSRC task. We report exact match (EM) and F1 scores.\n4.5    Multimodal Chain-of-Thought Prompting\nChain-of-thought prompting [WWS+22] allows large language models to generate a series of rea-\nsoning steps and decompose a multi-step problem into intermediate steps, which can significantly\n                                                          12", "md": "## Zero-shot generalization on OCR-free language understanding\n\nWe report accuracy scores for different models on the HatefulMemes and Rendered SST-2 tasks:\n\n| Model          | HatefulMemes | Rendered SST-2 |\n|----------------|--------------|----------------|\n| CLIP ViT-B/32  | 57.6         | 59.6           |\n| CLIP ViT-B/16  | 61.7         | 59.8           |\n| CLIP ViT-L/14  | 63.3         | 64.0           |\n| Flamingo-3B    | 53.7         | -              |\n| Flamingo-9B    | 57.0         | -              |\n| KOSMOS-1 (1.6B)| 63.9         | 67.1           |\n\n## Web Page Question Answering\n\nWeb page question answering aims at finding answers to questions from web pages. It requires the model to comprehend both the semantics and the structure of texts. The structure of the web page (such as tables, lists, and HTML layout) plays a key role in how the information is arranged and displayed. The task can help us evaluate our model\u2019s ability to understand the semantics and the structure of web pages.\n\n### Evaluation Setup\n\nWe compare the performance on the Web-based Structural Reading Comprehension (WebSRC) dataset [CZC+21]. For comparisons, we train a language model (LLM) on the same text corpora with the same training setup as in KOSMOS-1. The LLM takes the text extracted from the web page as input. Its template of the prompt is:\n\n$$\"Given the context below from web page, extract the answer from the given text like this:$$\n\n$$Question: Who is the publisher of this book?$$\n\n$$Answer: Penguin Books Ltd.$$\n\n$$Context: \\{WebText\\} Q: \\{question\\} A: \\{answer\\}\"$$\n\nwhere the {WebText} presents the text extracted from the web page. Besides using the same prompt, KOSMOS-1 prepends the image before the prompt.\n\n### Results\n\nThe experimental results are summarized in Table 8. We observe that KOSMOS-1 outperforms the LLM, indicating that KOSMOS-1 can benefit from the layout and style information of web pages in images. In addition, we evaluate the performance of KOSMOS-1 without the extracted text in the prompt. It shows that extracted text has a contribution of +12.0/20.7 EM/F1 to KOSMOS-1, indicating that the benefit from modeling images does not sacrifice its language abilities.\n\n| Models         | EM   | F1   |\n|----------------|------|------|\n| Using extracted text |\n| LLM            | 7.6  | 17.9 |\n| KOSMOS-1       | 15.8 | 31.3 |\n| Without using extracted text |\n| KOSMOS-1       | 3.8  | 10.6 |\n\n## Multimodal Chain-of-Thought Prompting\n\nChain-of-thought prompting allows large language models to generate a series of reasoning steps and decompose a multi-step problem into intermediate steps, which can significantly improve the model's performance.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Zero-shot generalization on OCR-free language understanding", "md": "## Zero-shot generalization on OCR-free language understanding"}, {"type": "text", "value": "We report accuracy scores for different models on the HatefulMemes and Rendered SST-2 tasks:", "md": "We report accuracy scores for different models on the HatefulMemes and Rendered SST-2 tasks:"}, {"type": "table", "rows": [["Model", "HatefulMemes", "Rendered SST-2"], ["CLIP ViT-B/32", "57.6", "59.6"], ["CLIP ViT-B/16", "61.7", "59.8"], ["CLIP ViT-L/14", "63.3", "64.0"], ["Flamingo-3B", "53.7", "-"], ["Flamingo-9B", "57.0", "-"], ["KOSMOS-1 (1.6B)", "63.9", "67.1"]], "md": "| Model          | HatefulMemes | Rendered SST-2 |\n|----------------|--------------|----------------|\n| CLIP ViT-B/32  | 57.6         | 59.6           |\n| CLIP ViT-B/16  | 61.7         | 59.8           |\n| CLIP ViT-L/14  | 63.3         | 64.0           |\n| Flamingo-3B    | 53.7         | -              |\n| Flamingo-9B    | 57.0         | -              |\n| KOSMOS-1 (1.6B)| 63.9         | 67.1           |", "isPerfectTable": true, "csv": "\"Model\",\"HatefulMemes\",\"Rendered SST-2\"\n\"CLIP ViT-B/32\",\"57.6\",\"59.6\"\n\"CLIP ViT-B/16\",\"61.7\",\"59.8\"\n\"CLIP ViT-L/14\",\"63.3\",\"64.0\"\n\"Flamingo-3B\",\"53.7\",\"-\"\n\"Flamingo-9B\",\"57.0\",\"-\"\n\"KOSMOS-1 (1.6B)\",\"63.9\",\"67.1\""}, {"type": "heading", "lvl": 2, "value": "Web Page Question Answering", "md": "## Web Page Question Answering"}, {"type": "text", "value": "Web page question answering aims at finding answers to questions from web pages. It requires the model to comprehend both the semantics and the structure of texts. The structure of the web page (such as tables, lists, and HTML layout) plays a key role in how the information is arranged and displayed. The task can help us evaluate our model\u2019s ability to understand the semantics and the structure of web pages.", "md": "Web page question answering aims at finding answers to questions from web pages. It requires the model to comprehend both the semantics and the structure of texts. The structure of the web page (such as tables, lists, and HTML layout) plays a key role in how the information is arranged and displayed. The task can help us evaluate our model\u2019s ability to understand the semantics and the structure of web pages."}, {"type": "heading", "lvl": 3, "value": "Evaluation Setup", "md": "### Evaluation Setup"}, {"type": "text", "value": "We compare the performance on the Web-based Structural Reading Comprehension (WebSRC) dataset [CZC+21]. For comparisons, we train a language model (LLM) on the same text corpora with the same training setup as in KOSMOS-1. The LLM takes the text extracted from the web page as input. Its template of the prompt is:\n\n$$\"Given the context below from web page, extract the answer from the given text like this:$$\n\n$$Question: Who is the publisher of this book?$$\n\n$$Answer: Penguin Books Ltd.$$\n\n$$Context: \\{WebText\\} Q: \\{question\\} A: \\{answer\\}\"$$\n\nwhere the {WebText} presents the text extracted from the web page. Besides using the same prompt, KOSMOS-1 prepends the image before the prompt.", "md": "We compare the performance on the Web-based Structural Reading Comprehension (WebSRC) dataset [CZC+21]. For comparisons, we train a language model (LLM) on the same text corpora with the same training setup as in KOSMOS-1. The LLM takes the text extracted from the web page as input. Its template of the prompt is:\n\n$$\"Given the context below from web page, extract the answer from the given text like this:$$\n\n$$Question: Who is the publisher of this book?$$\n\n$$Answer: Penguin Books Ltd.$$\n\n$$Context: \\{WebText\\} Q: \\{question\\} A: \\{answer\\}\"$$\n\nwhere the {WebText} presents the text extracted from the web page. Besides using the same prompt, KOSMOS-1 prepends the image before the prompt."}, {"type": "heading", "lvl": 3, "value": "Results", "md": "### Results"}, {"type": "text", "value": "The experimental results are summarized in Table 8. We observe that KOSMOS-1 outperforms the LLM, indicating that KOSMOS-1 can benefit from the layout and style information of web pages in images. In addition, we evaluate the performance of KOSMOS-1 without the extracted text in the prompt. It shows that extracted text has a contribution of +12.0/20.7 EM/F1 to KOSMOS-1, indicating that the benefit from modeling images does not sacrifice its language abilities.", "md": "The experimental results are summarized in Table 8. We observe that KOSMOS-1 outperforms the LLM, indicating that KOSMOS-1 can benefit from the layout and style information of web pages in images. In addition, we evaluate the performance of KOSMOS-1 without the extracted text in the prompt. It shows that extracted text has a contribution of +12.0/20.7 EM/F1 to KOSMOS-1, indicating that the benefit from modeling images does not sacrifice its language abilities."}, {"type": "table", "rows": [["Models", "EM", "F1"], ["Using extracted text"], ["LLM", "7.6", "17.9"], ["KOSMOS-1", "15.8", "31.3"], ["Without using extracted text"], ["KOSMOS-1", "3.8", "10.6"]], "md": "| Models         | EM   | F1   |\n|----------------|------|------|\n| Using extracted text |\n| LLM            | 7.6  | 17.9 |\n| KOSMOS-1       | 15.8 | 31.3 |\n| Without using extracted text |\n| KOSMOS-1       | 3.8  | 10.6 |", "isPerfectTable": false, "csv": "\"Models\",\"EM\",\"F1\"\n\"Using extracted text\"\n\"LLM\",\"7.6\",\"17.9\"\n\"KOSMOS-1\",\"15.8\",\"31.3\"\n\"Without using extracted text\"\n\"KOSMOS-1\",\"3.8\",\"10.6\""}, {"type": "heading", "lvl": 2, "value": "Multimodal Chain-of-Thought Prompting", "md": "## Multimodal Chain-of-Thought Prompting"}, {"type": "text", "value": "Chain-of-thought prompting allows large language models to generate a series of reasoning steps and decompose a multi-step problem into intermediate steps, which can significantly improve the model's performance.", "md": "Chain-of-thought prompting allows large language models to generate a series of reasoning steps and decompose a multi-step problem into intermediate steps, which can significantly improve the model's performance."}]}, {"page": 13, "text": "             Input Prompt\n                            Question: which studio created the movie characters in the image?\n                            Answer:\n              Comulevon\n              Completion\n              Disney                  X               Standard Prompting\n            Inpui Promal\n             Input Prompt\n                D           Introduce this picture in detail:\n             ComplenOn\n             Completion\n            It is a robot that is holding up a cube that is the same color as the wall. The image is from the film,\n            WALL-E, by Pixar Animation Studios. It is the story of a small robot named WALL-E and his quest to\n            build the ultimate life form. WALL-E is created by a group\u2026\n            Indut Protin\n             Input Prompt\n                             It is a robot that is holding up a cube that is the same color as the wall. The image is\n                             from the film, WALL-E, by Pixar Animation Studios. It is the story\u2026\n                             Question: which studio created the movie characters in the image?\n                             Answer:\n              Completion\n            Pixar Animation Studios\n                                          Multimodal Chain-of-Thought Prompting\nFigure 5: Multimodal Chain-of-Thought prompting enables KOSMOS-1 to generate a rationale first,\nthen to tackle complex question-answering and reasoning tasks.\nimprove the performance in complex tasks. Motivated by chain-of-thought prompting, we investigate\na multimodal chain-of-thought prompting using KOSMOS-1. As illustrated in Figure 5, we break\ndown perception-language tasks into two steps. In the first stage, given an image, we use a prompt to\nguide the model to generate a rationale. The model is then fed the rationale and a task-aware prompt\nto produce the final results.\n4.5.1     Evaluation Setup\nWe evaluate the ability of multimodal chain-of-thought prompting on the Rendered SST-2. We use the\nprompt \u201cIntroduce this picture in detail:\u201d to generate the content in the picture as the rationale. Then,\nwe use the prompt \u201c{rationale} Question: what is the sentiment of the opinion? Answer: {answer}\u201d\nto predict the sentiment, where the answer is either positive or negative.\n4.5.2     Results\nWe conduct experiments to evaluate the performance of the multimodal chain-of-thought prompting.\nTable 9 shows that multimodal chain-of-thought prompting achieves a score of 72.9, which is 5.8\npoints higher than the standard prompting. By generating intermediate content, the model can\nrecognize the text in the images and infer the sentiment of the sentences more correctly.\n4.6     Zero-Shot Image Classification\nWe report the zero-shot image classification performance on ImageNet [DDS+09]. Image classifica-\ntion comprehends an entire image as a whole and aims to assign a label to the image. We map each\nlabel to its category name in natural language. The model is prompted to predict the category name\nto perform zero-shot image classification.\n                                                                 13", "md": "# Document\n\n## Input Prompt\n\nQuestion: which studio created the movie characters in the image?\n\nAnswer: Comulevon\n\n## Completion\n\nDisney\nX\nStandard Prompting\n\n## Input Prompt\n\nIntroduce this picture in detail:\n\n## Completion\n\nIt is a robot that is holding up a cube that is the same color as the wall. The image is from the film, WALL-E, by Pixar Animation Studios. It is the story of a small robot named WALL-E and his quest to build the ultimate life form. WALL-E is created by a group...\n\n## Input Prompt\n\nIt is a robot that is holding up a cube that is the same color as the wall. The image is from the film, WALL-E, by Pixar Animation Studios. It is the story...\n\nQuestion: which studio created the movie characters in the image?\n\nAnswer: Pixar Animation Studios\n\nMultimodal Chain-of-Thought Prompting\n\nFigure 5: Multimodal Chain-of-Thought prompting enables KOSMOS-1 to generate a rationale first, then to tackle complex question-answering and reasoning tasks.\n\nImprove the performance in complex tasks. Motivated by chain-of-thought prompting, we investigate a multimodal chain-of-thought prompting using KOSMOS-1. As illustrated in Figure 5, we break down perception-language tasks into two steps. In the first stage, given an image, we use a prompt to guide the model to generate a rationale. The model is then fed the rationale and a task-aware prompt to produce the final results.\n\n### Evaluation Setup\n\nWe evaluate the ability of multimodal chain-of-thought prompting on the Rendered SST-2. We use the prompt \"Introduce this picture in detail:\" to generate the content in the picture as the rationale. Then, we use the prompt \"{rationale} Question: what is the sentiment of the opinion? Answer: {answer}\" to predict the sentiment, where the answer is either positive or negative.\n\n### Results\n\nWe conduct experiments to evaluate the performance of the multimodal chain-of-thought prompting.\n\nTable 9\n72.9\n5.8 points higher than the standard prompting\n\n### Zero-Shot Image Classification\n\nWe report the zero-shot image classification performance on ImageNet [DDS+09]. Image classification comprehends an entire image as a whole and aims to assign a label to the image. We map each label to its category name in natural language. The model is prompted to predict the category name to perform zero-shot image classification.", "images": [{"name": "page-13-1.jpg", "height": 32, "width": 41, "x": 145, "y": 89}, {"name": "page-13-7.jpg", "height": 32, "width": 41, "x": 144, "y": 294}, {"name": "page-13-3.jpg", "height": 16, "width": 50, "x": 140, "y": 73}, {"name": "page-13-4.jpg", "height": 16, "width": 43, "x": 143, "y": 127}, {"name": "page-13-0.jpg", "height": 20, "width": 28, "x": 222, "y": 347}, {"name": "page-13-6.jpg", "height": 16, "width": 43, "x": 143, "y": 225}, {"name": "page-13-2.jpg", "height": 18, "width": 20, "x": 220, "y": 140}, {"name": "page-13-9.jpg", "height": 16, "width": 43, "x": 143, "y": 335}, {"name": "page-13-5.jpg", "height": 32, "width": 50, "x": 140, "y": 171}, {"name": "page-13-8.jpg", "height": 16, "width": 50, "x": 140, "y": 274}], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "heading", "lvl": 2, "value": "Input Prompt", "md": "## Input Prompt"}, {"type": "text", "value": "Question: which studio created the movie characters in the image?\n\nAnswer: Comulevon", "md": "Question: which studio created the movie characters in the image?\n\nAnswer: Comulevon"}, {"type": "heading", "lvl": 2, "value": "Completion", "md": "## Completion"}, {"type": "text", "value": "Disney\nX\nStandard Prompting", "md": "Disney\nX\nStandard Prompting"}, {"type": "heading", "lvl": 2, "value": "Input Prompt", "md": "## Input Prompt"}, {"type": "text", "value": "Introduce this picture in detail:", "md": "Introduce this picture in detail:"}, {"type": "heading", "lvl": 2, "value": "Completion", "md": "## Completion"}, {"type": "text", "value": "It is a robot that is holding up a cube that is the same color as the wall. The image is from the film, WALL-E, by Pixar Animation Studios. It is the story of a small robot named WALL-E and his quest to build the ultimate life form. WALL-E is created by a group...", "md": "It is a robot that is holding up a cube that is the same color as the wall. The image is from the film, WALL-E, by Pixar Animation Studios. It is the story of a small robot named WALL-E and his quest to build the ultimate life form. WALL-E is created by a group..."}, {"type": "heading", "lvl": 2, "value": "Input Prompt", "md": "## Input Prompt"}, {"type": "text", "value": "It is a robot that is holding up a cube that is the same color as the wall. The image is from the film, WALL-E, by Pixar Animation Studios. It is the story...\n\nQuestion: which studio created the movie characters in the image?\n\nAnswer: Pixar Animation Studios\n\nMultimodal Chain-of-Thought Prompting\n\nFigure 5: Multimodal Chain-of-Thought prompting enables KOSMOS-1 to generate a rationale first, then to tackle complex question-answering and reasoning tasks.\n\nImprove the performance in complex tasks. Motivated by chain-of-thought prompting, we investigate a multimodal chain-of-thought prompting using KOSMOS-1. As illustrated in Figure 5, we break down perception-language tasks into two steps. In the first stage, given an image, we use a prompt to guide the model to generate a rationale. The model is then fed the rationale and a task-aware prompt to produce the final results.", "md": "It is a robot that is holding up a cube that is the same color as the wall. The image is from the film, WALL-E, by Pixar Animation Studios. It is the story...\n\nQuestion: which studio created the movie characters in the image?\n\nAnswer: Pixar Animation Studios\n\nMultimodal Chain-of-Thought Prompting\n\nFigure 5: Multimodal Chain-of-Thought prompting enables KOSMOS-1 to generate a rationale first, then to tackle complex question-answering and reasoning tasks.\n\nImprove the performance in complex tasks. Motivated by chain-of-thought prompting, we investigate a multimodal chain-of-thought prompting using KOSMOS-1. As illustrated in Figure 5, we break down perception-language tasks into two steps. In the first stage, given an image, we use a prompt to guide the model to generate a rationale. The model is then fed the rationale and a task-aware prompt to produce the final results."}, {"type": "heading", "lvl": 3, "value": "Evaluation Setup", "md": "### Evaluation Setup"}, {"type": "text", "value": "We evaluate the ability of multimodal chain-of-thought prompting on the Rendered SST-2. We use the prompt \"Introduce this picture in detail:\" to generate the content in the picture as the rationale. Then, we use the prompt \"{rationale} Question: what is the sentiment of the opinion? Answer: {answer}\" to predict the sentiment, where the answer is either positive or negative.", "md": "We evaluate the ability of multimodal chain-of-thought prompting on the Rendered SST-2. We use the prompt \"Introduce this picture in detail:\" to generate the content in the picture as the rationale. Then, we use the prompt \"{rationale} Question: what is the sentiment of the opinion? Answer: {answer}\" to predict the sentiment, where the answer is either positive or negative."}, {"type": "heading", "lvl": 3, "value": "Results", "md": "### Results"}, {"type": "text", "value": "We conduct experiments to evaluate the performance of the multimodal chain-of-thought prompting.\n\nTable 9\n72.9\n5.8 points higher than the standard prompting", "md": "We conduct experiments to evaluate the performance of the multimodal chain-of-thought prompting.\n\nTable 9\n72.9\n5.8 points higher than the standard prompting"}, {"type": "heading", "lvl": 3, "value": "Zero-Shot Image Classification", "md": "### Zero-Shot Image Classification"}, {"type": "text", "value": "We report the zero-shot image classification performance on ImageNet [DDS+09]. Image classification comprehends an entire image as a whole and aims to assign a label to the image. We map each label to its category name in natural language. The model is prompted to predict the category name to perform zero-shot image classification.", "md": "We report the zero-shot image classification performance on ImageNet [DDS+09]. Image classification comprehends an entire image as a whole and aims to assign a label to the image. We map each label to its category name in natural language. The model is prompted to predict the category name to perform zero-shot image classification."}]}, {"page": 14, "text": "                                  Models                                         Accuracy\n                                  CLIP ViT-B/32                                      59.6\n                                  CLIP ViT-B/16                                      59.8\n                                  CLIP ViT-L/14                                      64.0\n                                  KOSMOS-1                                           67.1\n                                     w/ multimodal CoT prompting                     72.9\n           Table 9: Multimodal chain-of-thought (CoT) prompting on Rendered SST-2 task.\n     Innut Pramnt                                Innt Promat\n     Input Prompt                                Input Prompt\n                     Question:what is         Description of three toed woodpecker: It                       Question:what\n                     the name of the          has black and white stripes throughout                         is the name of\n                     woodpecker in the        the body and a yellow crown.                                   the woodpecker\n                     picture, three toed      Description of downy woodpecker: It                            in the picture?\n                     or downy?                has white spots on its black wings and                         Answer:\n                     Answer:                  some red on its crown.\n      Complelion                                  Complelion\n      Completion                                  Completion\n       downy woodpecker                            three toe woodpecker\n         Zero-Shot Classification                                Zero-Shot Classification with Descriptions\n  Figure 6: In-context verbal descriptions can help KOSMOS-1 recognize visual categories better.\n4.6.1    Evaluation Setup\nGiven an input image, we concatenate the image with the prompt \u201cThe photo of the\u201d. The input\nis then fed into the model to obtain the category name of the image. We evaluate the model on\nImageNet [DDS+09], which contains 1.28M training images and 50k validation images in 1k object\ncategories. The prediction is classified as correct if it is exactly the same as the ground-truth category\nname. The image resolution used for evaluation is 224\u00d7224. We use beam search to generate the\ncategory names and the beam size is 2.\n4.6.2    Results\nAs shown in Table 10, we report zero-shot results in both constrained and unconstrained settings.\nThe difference between the two settings is whether we use the 1k object category names to constrain\nthe decoding. KOSMOS-1 significantly outperforms GIT [WYH+22] by 4.6% under the constrained\nsetting and 2.1% under the unconstrained setting.\n                       Model                    Without Constraints              With Constraints\n                       GIT [WYH+22]                          1.9                          33.5\n                       KOSMOS-1                              4.0                          38.1\nTable 10: Zero-shot image classification on ImageNet. For the results with constraints, we use the 1k\nImageNet object category names for constrained decoding. We report top-1 accuracy scores.\n4.7    Zero-Shot Image Classification with Descriptions\nThe standard approach of image classification as above is to prompt the model for the specific name\nof the object depicted in the image. However, there are also some classification rules customized for\ndifferent users and scenarios, such as the refined classification of complex animal subspecies. We\ncan utilize natural language descriptions to guide KOSMOS-1 to distinguish images in the zero-shot\nsetting, which makes the decision process more interpretable.\n                                                               14", "md": "# OCR Text Analysis\n\n## Models Accuracy\n\n|Models|Accuracy|\n|---|---|\n|CLIP ViT-B/32|59.6|\n|CLIP ViT-B/16|59.8|\n|CLIP ViT-L/14|64.0|\n|KOSMOS-1|67.1|\n|KOSMOS-1 w/ multimodal CoT prompting|72.9|\n\nTable 9: Multimodal chain-of-thought (CoT) prompting on Rendered SST-2 task.\n\n### Innut Pramnt\n\nInnt Promat\n\n- Question: what is the name of the woodpecker in the picture, three toed or downy?\n- Answer: downy woodpecker\n\n### Completion\n\nCompletion\n\n- Description of three toed woodpecker: It has black and white stripes throughout the body and a yellow crown.\n- Description of downy woodpecker: It has white spots on its black wings and some red on its crown.\n\nFigure 6: In-context verbal descriptions can help KOSMOS-1 recognize visual categories better.\n\n### Evaluation Setup\n\nGiven an input image, we concatenate the image with the prompt \"The photo of the\". The input is then fed into the model to obtain the category name of the image. We evaluate the model on ImageNet, which contains 1.28M training images and 50k validation images in 1k object categories. The prediction is classified as correct if it is exactly the same as the ground-truth category name. The image resolution used for evaluation is 224x224. We use beam search to generate the category names and the beam size is 2.\n\n### Results\n\nAs shown in Table 10, we report zero-shot results in both constrained and unconstrained settings. The difference between the two settings is whether we use the 1k object category names to constrain the decoding. KOSMOS-1 significantly outperforms GIT [WYH+22] by 4.6% under the constrained setting and 2.1% under the unconstrained setting.\n\n|Model|Without Constraints|With Constraints|\n|---|---|---|\n|GIT [WYH+22]|1.9|33.5|\n|KOSMOS-1|4.0|38.1|\n\nTable 10: Zero-shot image classification on ImageNet. For the results with constraints, we use the 1k ImageNet object category names for constrained decoding. We report top-1 accuracy scores.\n\n### Zero-Shot Image Classification with Descriptions\n\nThe standard approach of image classification as above is to prompt the model for the specific name of the object depicted in the image. However, there are also some classification rules customized for different users and scenarios, such as the refined classification of complex animal subspecies. We can utilize natural language descriptions to guide KOSMOS-1 to distinguish images in the zero-shot setting, which makes the decision process more interpretable.", "images": [{"name": "page-14-1.jpg", "height": 17, "width": 50, "x": 120, "y": 189}, {"name": "page-14-4.jpg", "height": 17, "width": 43, "x": 257, "y": 265}, {"name": "page-14-0.jpg", "height": 14, "width": 19, "x": 374, "y": 276}, {"name": "page-14-2.jpg", "height": 17, "width": 43, "x": 122, "y": 265}, {"name": "page-14-3.jpg", "height": 17, "width": 50, "x": 254, "y": 189}, {"name": "page-14-6.jpg", "height": 44, "width": 50, "x": 117, "y": 209}, {"name": "page-14-7.jpg", "height": 44, "width": 50, "x": 386, "y": 209}], "items": [{"type": "heading", "lvl": 1, "value": "OCR Text Analysis", "md": "# OCR Text Analysis"}, {"type": "heading", "lvl": 2, "value": "Models Accuracy", "md": "## Models Accuracy"}, {"type": "table", "rows": [["Models", "Accuracy"], ["CLIP ViT-B/32", "59.6"], ["CLIP ViT-B/16", "59.8"], ["CLIP ViT-L/14", "64.0"], ["KOSMOS-1", "67.1"], ["KOSMOS-1 w/ multimodal CoT prompting", "72.9"]], "md": "|Models|Accuracy|\n|---|---|\n|CLIP ViT-B/32|59.6|\n|CLIP ViT-B/16|59.8|\n|CLIP ViT-L/14|64.0|\n|KOSMOS-1|67.1|\n|KOSMOS-1 w/ multimodal CoT prompting|72.9|", "isPerfectTable": true, "csv": "\"Models\",\"Accuracy\"\n\"CLIP ViT-B/32\",\"59.6\"\n\"CLIP ViT-B/16\",\"59.8\"\n\"CLIP ViT-L/14\",\"64.0\"\n\"KOSMOS-1\",\"67.1\"\n\"KOSMOS-1 w/ multimodal CoT prompting\",\"72.9\""}, {"type": "text", "value": "Table 9: Multimodal chain-of-thought (CoT) prompting on Rendered SST-2 task.", "md": "Table 9: Multimodal chain-of-thought (CoT) prompting on Rendered SST-2 task."}, {"type": "heading", "lvl": 3, "value": "Innut Pramnt", "md": "### Innut Pramnt"}, {"type": "text", "value": "Innt Promat\n\n- Question: what is the name of the woodpecker in the picture, three toed or downy?\n- Answer: downy woodpecker", "md": "Innt Promat\n\n- Question: what is the name of the woodpecker in the picture, three toed or downy?\n- Answer: downy woodpecker"}, {"type": "heading", "lvl": 3, "value": "Completion", "md": "### Completion"}, {"type": "text", "value": "Completion\n\n- Description of three toed woodpecker: It has black and white stripes throughout the body and a yellow crown.\n- Description of downy woodpecker: It has white spots on its black wings and some red on its crown.\n\nFigure 6: In-context verbal descriptions can help KOSMOS-1 recognize visual categories better.", "md": "Completion\n\n- Description of three toed woodpecker: It has black and white stripes throughout the body and a yellow crown.\n- Description of downy woodpecker: It has white spots on its black wings and some red on its crown.\n\nFigure 6: In-context verbal descriptions can help KOSMOS-1 recognize visual categories better."}, {"type": "heading", "lvl": 3, "value": "Evaluation Setup", "md": "### Evaluation Setup"}, {"type": "text", "value": "Given an input image, we concatenate the image with the prompt \"The photo of the\". The input is then fed into the model to obtain the category name of the image. We evaluate the model on ImageNet, which contains 1.28M training images and 50k validation images in 1k object categories. The prediction is classified as correct if it is exactly the same as the ground-truth category name. The image resolution used for evaluation is 224x224. We use beam search to generate the category names and the beam size is 2.", "md": "Given an input image, we concatenate the image with the prompt \"The photo of the\". The input is then fed into the model to obtain the category name of the image. We evaluate the model on ImageNet, which contains 1.28M training images and 50k validation images in 1k object categories. The prediction is classified as correct if it is exactly the same as the ground-truth category name. The image resolution used for evaluation is 224x224. We use beam search to generate the category names and the beam size is 2."}, {"type": "heading", "lvl": 3, "value": "Results", "md": "### Results"}, {"type": "text", "value": "As shown in Table 10, we report zero-shot results in both constrained and unconstrained settings. The difference between the two settings is whether we use the 1k object category names to constrain the decoding. KOSMOS-1 significantly outperforms GIT [WYH+22] by 4.6% under the constrained setting and 2.1% under the unconstrained setting.", "md": "As shown in Table 10, we report zero-shot results in both constrained and unconstrained settings. The difference between the two settings is whether we use the 1k object category names to constrain the decoding. KOSMOS-1 significantly outperforms GIT [WYH+22] by 4.6% under the constrained setting and 2.1% under the unconstrained setting."}, {"type": "table", "rows": [["Model", "Without Constraints", "With Constraints"], ["GIT [WYH+22]", "1.9", "33.5"], ["KOSMOS-1", "4.0", "38.1"]], "md": "|Model|Without Constraints|With Constraints|\n|---|---|---|\n|GIT [WYH+22]|1.9|33.5|\n|KOSMOS-1|4.0|38.1|", "isPerfectTable": true, "csv": "\"Model\",\"Without Constraints\",\"With Constraints\"\n\"GIT [WYH+22]\",\"1.9\",\"33.5\"\n\"KOSMOS-1\",\"4.0\",\"38.1\""}, {"type": "text", "value": "Table 10: Zero-shot image classification on ImageNet. For the results with constraints, we use the 1k ImageNet object category names for constrained decoding. We report top-1 accuracy scores.", "md": "Table 10: Zero-shot image classification on ImageNet. For the results with constraints, we use the 1k ImageNet object category names for constrained decoding. We report top-1 accuracy scores."}, {"type": "heading", "lvl": 3, "value": "Zero-Shot Image Classification with Descriptions", "md": "### Zero-Shot Image Classification with Descriptions"}, {"type": "text", "value": "The standard approach of image classification as above is to prompt the model for the specific name of the object depicted in the image. However, there are also some classification rules customized for different users and scenarios, such as the refined classification of complex animal subspecies. We can utilize natural language descriptions to guide KOSMOS-1 to distinguish images in the zero-shot setting, which makes the decision process more interpretable.", "md": "The standard approach of image classification as above is to prompt the model for the specific name of the object depicted in the image. However, there are also some classification rules customized for different users and scenarios, such as the refined classification of complex animal subspecies. We can utilize natural language descriptions to guide KOSMOS-1 to distinguish images in the zero-shot setting, which makes the decision process more interpretable."}]}, {"page": 15, "text": "                     Category 1                                         Category 2\n                three toed woodpecker                               downy woodpecker\n               It has black and white stripes                    It has white spots on its black wings\n               throughout the body and a yellow                  and some red on its crown.\n               crown.\n                   Gentoo penguin                                      royal penguin\n               It has a black head and white patch               It has a white face and a yellow\n               above its eyes.                                   crown.\n                black throated sparrow                                  fox sparrow\n               It has white underparts and a dis-                It has a reddish-brown plumage and\n               tinctive black bib on the throat.                 a streaked breast.\n    Table 11: The detailed descriptions of different categories for in-context image classification.\n4.7.1   Evaluation Setup\nFollowing CUB [WBW+11], we construct a bird classification dataset that contains images and\nnatural-language descriptions of categories. The dataset has three groups of binary image classifica-\ntion. Each group contains two animal categories with similar appearances. Our goal is to classify\nimages given the categories\u2019 descriptions. Table 11 presents the data samples. The first group is\nfrom [WBW+11], while the other two groups are collected from the website. Each category contains\ntwenty images.\nThe evaluation procedure is illustrated in Figure 6. For the zero-shot setting, we provide detailed\ndescriptions of two specific categories and use the template \u201cQuestion:what is the name of {general\ncategory} in the picture? Answer:\u201d to prompt the model for the specific category name in an open-\nended manner. To evaluate the effect of providing verbal descriptions in context, we also implement\na zero-shot baseline without prompting descriptions. Instead, we provide the corresponding specific\nnames in the prompt.\n4.7.2   Results\nThe evaluation results are shown in Table 12. We observe that providing descriptions in context can\nsignificantly improve the accuracy of image classification. The consistent improvements indicate\nthat KOSMOS-1 can perceive the intentions of instructions and well align the concepts in language\nmodality with visual features in vision modality.\n                                   Settings                 Accuracy\n                                   Without Descriptions        61.7\n                                   With Descriptions           90.0\n      Table 12: Results of zero-shot image classification without and with verbal descriptions.\n4.8   Language Tasks\nThe models are evaluated on the language tasks given task instructions (i.e., zero-shot) or several\ndemonstration examples (i.e., few-shot). Text inputs are directly fed into the models as in vanilla\nlanguage models.\n                                                    15", "md": "## Category 1\n\n|three toed woodpecker|downy woodpecker|\n|---|---|\n|It has black and white stripes throughout the body and a yellow crown.|It has white spots on its black wings and some red on its crown.|\n\n## Category 2\n\n|Gentoo penguin|royal penguin|\n|---|---|\n|It has a black head and white patch above its eyes.|It has a white face and a yellow crown.|\n\n|black throated sparrow|fox sparrow|\n|---|---|\n|It has white underparts and a distinctive black bib on the throat.|It has a reddish-brown plumage and a streaked breast.|\n\n$$\\text{Table 11: The detailed descriptions of different categories for in-context image classification.}$$\n\n### 4.7.1 Evaluation Setup\n\nFollowing CUB [WBW+11], we construct a bird classification dataset that contains images and natural-language descriptions of categories. The dataset has three groups of binary image classification. Each group contains two animal categories with similar appearances. Our goal is to classify images given the categories\u2019 descriptions. Table 11 presents the data samples. The first group is from [WBW+11], while the other two groups are collected from the website. Each category contains twenty images.\n\nThe evaluation procedure is illustrated in Figure 6. For the zero-shot setting, we provide detailed descriptions of two specific categories and use the template \u201cQuestion: what is the name of {general category} in the picture? Answer:\u201d to prompt the model for the specific category name in an open-ended manner. To evaluate the effect of providing verbal descriptions in context, we also implement a zero-shot baseline without prompting descriptions. Instead, we provide the corresponding specific names in the prompt.\n\n### 4.7.2 Results\n\nThe evaluation results are shown in Table 12. We observe that providing descriptions in context can significantly improve the accuracy of image classification. The consistent improvements indicate that KOSMOS-1 can perceive the intentions of instructions and well align the concepts in language modality with visual features in vision modality.\n\n|Settings|Accuracy|\n|---|---|\n|Without Descriptions|61.7|\n|With Descriptions|90.0|\n\n$$\\text{Table 12: Results of zero-shot image classification without and with verbal descriptions.}$$\n\n### 4.8 Language Tasks\n\nThe models are evaluated on the language tasks given task instructions (i.e., zero-shot) or several demonstration examples (i.e., few-shot). Text inputs are directly fed into the models as in vanilla language models.", "images": [{"name": "page-15-2.jpg", "height": 40, "width": 40, "x": 119, "y": 172}, {"name": "page-15-3.jpg", "height": 40, "width": 40, "x": 307, "y": 172}, {"name": "page-15-0.jpg", "height": 40, "width": 40, "x": 119, "y": 107}, {"name": "page-15-4.jpg", "height": 40, "width": 40, "x": 119, "y": 236}, {"name": "page-15-5.jpg", "height": 40, "width": 40, "x": 307, "y": 236}, {"name": "page-15-1.jpg", "height": 40, "width": 40, "x": 307, "y": 107}], "items": [{"type": "heading", "lvl": 2, "value": "Category 1", "md": "## Category 1"}, {"type": "table", "rows": [["three toed woodpecker", "downy woodpecker"], ["It has black and white stripes throughout the body and a yellow crown.", "It has white spots on its black wings and some red on its crown."]], "md": "|three toed woodpecker|downy woodpecker|\n|---|---|\n|It has black and white stripes throughout the body and a yellow crown.|It has white spots on its black wings and some red on its crown.|", "isPerfectTable": true, "csv": "\"three toed woodpecker\",\"downy woodpecker\"\n\"It has black and white stripes throughout the body and a yellow crown.\",\"It has white spots on its black wings and some red on its crown.\""}, {"type": "heading", "lvl": 2, "value": "Category 2", "md": "## Category 2"}, {"type": "table", "rows": [["Gentoo penguin", "royal penguin"], ["It has a black head and white patch above its eyes.", "It has a white face and a yellow crown."]], "md": "|Gentoo penguin|royal penguin|\n|---|---|\n|It has a black head and white patch above its eyes.|It has a white face and a yellow crown.|", "isPerfectTable": true, "csv": "\"Gentoo penguin\",\"royal penguin\"\n\"It has a black head and white patch above its eyes.\",\"It has a white face and a yellow crown.\""}, {"type": "table", "rows": [["black throated sparrow", "fox sparrow"], ["It has white underparts and a distinctive black bib on the throat.", "It has a reddish-brown plumage and a streaked breast."]], "md": "|black throated sparrow|fox sparrow|\n|---|---|\n|It has white underparts and a distinctive black bib on the throat.|It has a reddish-brown plumage and a streaked breast.|", "isPerfectTable": true, "csv": "\"black throated sparrow\",\"fox sparrow\"\n\"It has white underparts and a distinctive black bib on the throat.\",\"It has a reddish-brown plumage and a streaked breast.\""}, {"type": "text", "value": "$$\\text{Table 11: The detailed descriptions of different categories for in-context image classification.}$$", "md": "$$\\text{Table 11: The detailed descriptions of different categories for in-context image classification.}$$"}, {"type": "heading", "lvl": 3, "value": "4.7.1 Evaluation Setup", "md": "### 4.7.1 Evaluation Setup"}, {"type": "text", "value": "Following CUB [WBW+11], we construct a bird classification dataset that contains images and natural-language descriptions of categories. The dataset has three groups of binary image classification. Each group contains two animal categories with similar appearances. Our goal is to classify images given the categories\u2019 descriptions. Table 11 presents the data samples. The first group is from [WBW+11], while the other two groups are collected from the website. Each category contains twenty images.\n\nThe evaluation procedure is illustrated in Figure 6. For the zero-shot setting, we provide detailed descriptions of two specific categories and use the template \u201cQuestion: what is the name of {general category} in the picture? Answer:\u201d to prompt the model for the specific category name in an open-ended manner. To evaluate the effect of providing verbal descriptions in context, we also implement a zero-shot baseline without prompting descriptions. Instead, we provide the corresponding specific names in the prompt.", "md": "Following CUB [WBW+11], we construct a bird classification dataset that contains images and natural-language descriptions of categories. The dataset has three groups of binary image classification. Each group contains two animal categories with similar appearances. Our goal is to classify images given the categories\u2019 descriptions. Table 11 presents the data samples. The first group is from [WBW+11], while the other two groups are collected from the website. Each category contains twenty images.\n\nThe evaluation procedure is illustrated in Figure 6. For the zero-shot setting, we provide detailed descriptions of two specific categories and use the template \u201cQuestion: what is the name of {general category} in the picture? Answer:\u201d to prompt the model for the specific category name in an open-ended manner. To evaluate the effect of providing verbal descriptions in context, we also implement a zero-shot baseline without prompting descriptions. Instead, we provide the corresponding specific names in the prompt."}, {"type": "heading", "lvl": 3, "value": "4.7.2 Results", "md": "### 4.7.2 Results"}, {"type": "text", "value": "The evaluation results are shown in Table 12. We observe that providing descriptions in context can significantly improve the accuracy of image classification. The consistent improvements indicate that KOSMOS-1 can perceive the intentions of instructions and well align the concepts in language modality with visual features in vision modality.", "md": "The evaluation results are shown in Table 12. We observe that providing descriptions in context can significantly improve the accuracy of image classification. The consistent improvements indicate that KOSMOS-1 can perceive the intentions of instructions and well align the concepts in language modality with visual features in vision modality."}, {"type": "table", "rows": [["Settings", "Accuracy"], ["Without Descriptions", "61.7"], ["With Descriptions", "90.0"]], "md": "|Settings|Accuracy|\n|---|---|\n|Without Descriptions|61.7|\n|With Descriptions|90.0|", "isPerfectTable": true, "csv": "\"Settings\",\"Accuracy\"\n\"Without Descriptions\",\"61.7\"\n\"With Descriptions\",\"90.0\""}, {"type": "text", "value": "$$\\text{Table 12: Results of zero-shot image classification without and with verbal descriptions.}$$", "md": "$$\\text{Table 12: Results of zero-shot image classification without and with verbal descriptions.}$$"}, {"type": "heading", "lvl": 3, "value": "4.8 Language Tasks", "md": "### 4.8 Language Tasks"}, {"type": "text", "value": "The models are evaluated on the language tasks given task instructions (i.e., zero-shot) or several demonstration examples (i.e., few-shot). Text inputs are directly fed into the models as in vanilla language models.", "md": "The models are evaluated on the language tasks given task instructions (i.e., zero-shot) or several demonstration examples (i.e., few-shot). Text inputs are directly fed into the models as in vanilla language models."}]}, {"page": 16, "text": " 4.8.1  Evaluation Setup\nWe train a language model (LLM) baseline with the same text corpora and training setup. We evaluate\n KOSMOS-1 and the LLM baseline on eight language tasks, including cloze and completion tasks (i.e,\n StoryCloze, HellaSwag), Winograd-style tasks (i.e, Winograd, Winogrande), commonsense reasoning\n(i.e, PIQA), and three datasets BoolQ, CB, and COPA from the SuperGLUE benchmark [WPN+19].\nThe detailed descriptions of these datasets are provided in Appendix C.2. We conduct experiments\n under zero-shot and few-shot settings. We evaluate each test example by randomly sampling examples\n from the training set as demonstrations. We set the number of shots to 0, 1, and 4 in our experiments.\n 4.8.2  Results\nTable 13 presents the in-context learning performance of language tasks. KOSMOS-1 achieves\n comparable or even better performance in cloze completion and commonsense reasoning tasks\nwhen compared to LLM. In terms of the average result across all these datasets, LLM performs\n better in zero-shot and one-shot settings, whereas our model performs better in few-shot (k = 4)\n settings. The results indicate that KOSMOS-1 also handles language-only tasks well and achieves\n favorable performance across datasets. In addition, Section 4.9.2 shows that MLLMs learn better\n visual commonsense knowledge compared with LLMs.\n          Task                Zero-shot                One-shot           Few-shot (k = 4)\n                         LLM      KOSMOS-1       LLM      KOSMOS-1       LLM      KOSMOS-1\n          StoryCloze      72.9        72.1        72.9        72.2        73.1       72.3\n          HellaSwag       50.4        50.0        50.2        50.0        50.4       50.3\n          Winograd        71.6        69.8        71.2        68.4        70.9       69.8\n          Winogrande      56.7        54.8        56.7        54.5        57.0       55.7\n          PIQA            73.2        72.9        73.0        72.5        72.6       72.3\n          BoolQ           56.4        56.4        55.1        57.2        58.7       59.2\n          CB              39.3        44.6        41.1        48.2        42.9       53.6\n          COPA            68.0        63.0        69.0        64.0        69.0       64.0\n          Average         61.1        60.5        61.2        60.9        61.8       62.2\nTable 13: Performance comparisons of language tasks between KOSMOS-1 and LLM. We use the\n same textual data and training setup to reimplement a language model. Both models do not use\n instruction tuning for fair comparisons.\n 4.9  Cross-modal Transfer\n Cross-modal transferability allows a model to learn from one modality (such as text, image, audio,\n etc.) and transfer the knowledge to the other modalities. This skill can enable a model to perform\n various tasks across different modalities. In this part, we evaluate the cross-model transferability of\n KOSMOS-1 on several benchmarks.\n 4.9.1  Transfer from Language to Multimodal: Language-Only Instruction Tuning\nTo evaluate the effect of language-only instruction tuning, we conduct an ablation study using four\n datasets: COCO, Flickr30k, VQAv2, and VizWiz. These datasets consist of image captioning and\n visual questions anwsering. The evaluation metrics are: CIDEr scores for COCO/Flickr30k and VQA\n accuracy for VQAv2/VizWiz.\nTable 14 shows the experimental results. Language-only instruction tuning boosts our model\u2019s\n performance by 1.9 points on Flickr30k, 4.3 points on VQAv2, and 1.3 points on VizWiz. Our experi-\n ments show that language-only instruction tuning can significantly improve the model\u2019s instruction-\n following capabilities across modalities. The results also indicate that our model can transfer the\n instruction-following capability from language to other modalities.\n                                                   16", "md": "## 4.8.1 Evaluation Setup\n\nWe train a language model (LLM) baseline with the same text corpora and training setup. We evaluate KOSMOS-1 and the LLM baseline on eight language tasks, including cloze and completion tasks (i.e., StoryCloze, HellaSwag), Winograd-style tasks (i.e., Winograd, Winogrande), commonsense reasoning (i.e., PIQA), and three datasets BoolQ, CB, and COPA from the SuperGLUE benchmark [WPN+19]. The detailed descriptions of these datasets are provided in Appendix C.2. We conduct experiments under zero-shot and few-shot settings. We evaluate each test example by randomly sampling examples from the training set as demonstrations. We set the number of shots to 0, 1, and 4 in our experiments.\n\n## 4.8.2 Results\n\nTable 13 presents the in-context learning performance of language tasks. KOSMOS-1 achieves comparable or even better performance in cloze completion and commonsense reasoning tasks when compared to LLM. In terms of the average result across all these datasets, LLM performs better in zero-shot and one-shot settings, whereas our model performs better in few-shot (k = 4) settings. The results indicate that KOSMOS-1 also handles language-only tasks well and achieves favorable performance across datasets. In addition, Section 4.9.2 shows that MLLMs learn better visual commonsense knowledge compared with LLMs.\n\n|Task|Zero-shot|One-shot|Few-shot (k = 4)|\n|---|---|---|---|\n|StoryCloze|LLM 72.9 KOSMOS-1 72.1|LLM 72.9 KOSMOS-1 72.2|LLM 73.1 KOSMOS-1 72.3|\n|HellaSwag|LLM 50.4 KOSMOS-1 50.0|LLM 50.2 KOSMOS-1 50.0|LLM 50.4 KOSMOS-1 50.3|\n|Winograd|LLM 71.6 KOSMOS-1 69.8|LLM 71.2 KOSMOS-1 68.4|LLM 70.9 KOSMOS-1 69.8|\n|Winogrande|LLM 56.7 KOSMOS-1 54.8|LLM 56.7 KOSMOS-1 54.5|LLM 57.0 KOSMOS-1 55.7|\n|PIQA|LLM 73.2 KOSMOS-1 72.9|LLM 73.0 KOSMOS-1 72.5|LLM 72.6 KOSMOS-1 72.3|\n|BoolQ|LLM 56.4 KOSMOS-1 56.4|LLM 55.1 KOSMOS-1 57.2|LLM 58.7 KOSMOS-1 59.2|\n|CB|LLM 39.3 KOSMOS-1 44.6|LLM 41.1 KOSMOS-1 48.2|LLM 42.9 KOSMOS-1 53.6|\n|COPA|LLM 68.0 KOSMOS-1 63.0|LLM 69.0 KOSMOS-1 64.0|LLM 69.0 KOSMOS-1 64.0|\n|Average|LLM 61.1 KOSMOS-1 60.5|LLM 61.2 KOSMOS-1 60.9|LLM 61.8 KOSMOS-1 62.2|\n\nTable 13: Performance comparisons of language tasks between KOSMOS-1 and LLM. We use the same textual data and training setup to reimplement a language model. Both models do not use instruction tuning for fair comparisons.\n\n## 4.9 Cross-modal Transfer\n\nCross-modal transferability allows a model to learn from one modality (such as text, image, audio, etc.) and transfer the knowledge to the other modalities. This skill can enable a model to perform various tasks across different modalities. In this part, we evaluate the cross-model transferability of KOSMOS-1 on several benchmarks.\n\n## 4.9.1 Transfer from Language to Multimodal: Language-Only Instruction Tuning\n\nTo evaluate the effect of language-only instruction tuning, we conduct an ablation study using four datasets: COCO, Flickr30k, VQAv2, and VizWiz. These datasets consist of image captioning and visual questions answering. The evaluation metrics are: CIDEr scores for COCO/Flickr30k and VQA accuracy for VQAv2/VizWiz.\n\nTable 14 shows the experimental results. Language-only instruction tuning boosts our model\u2019s performance by 1.9 points on Flickr30k, 4.3 points on VQAv2, and 1.3 points on VizWiz. Our experiments show that language-only instruction tuning can significantly improve the model\u2019s instruction-following capabilities across modalities. The results also indicate that our model can transfer the instruction-following capability from language to other modalities.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "4.8.1 Evaluation Setup", "md": "## 4.8.1 Evaluation Setup"}, {"type": "text", "value": "We train a language model (LLM) baseline with the same text corpora and training setup. We evaluate KOSMOS-1 and the LLM baseline on eight language tasks, including cloze and completion tasks (i.e., StoryCloze, HellaSwag), Winograd-style tasks (i.e., Winograd, Winogrande), commonsense reasoning (i.e., PIQA), and three datasets BoolQ, CB, and COPA from the SuperGLUE benchmark [WPN+19]. The detailed descriptions of these datasets are provided in Appendix C.2. We conduct experiments under zero-shot and few-shot settings. We evaluate each test example by randomly sampling examples from the training set as demonstrations. We set the number of shots to 0, 1, and 4 in our experiments.", "md": "We train a language model (LLM) baseline with the same text corpora and training setup. We evaluate KOSMOS-1 and the LLM baseline on eight language tasks, including cloze and completion tasks (i.e., StoryCloze, HellaSwag), Winograd-style tasks (i.e., Winograd, Winogrande), commonsense reasoning (i.e., PIQA), and three datasets BoolQ, CB, and COPA from the SuperGLUE benchmark [WPN+19]. The detailed descriptions of these datasets are provided in Appendix C.2. We conduct experiments under zero-shot and few-shot settings. We evaluate each test example by randomly sampling examples from the training set as demonstrations. We set the number of shots to 0, 1, and 4 in our experiments."}, {"type": "heading", "lvl": 2, "value": "4.8.2 Results", "md": "## 4.8.2 Results"}, {"type": "text", "value": "Table 13 presents the in-context learning performance of language tasks. KOSMOS-1 achieves comparable or even better performance in cloze completion and commonsense reasoning tasks when compared to LLM. In terms of the average result across all these datasets, LLM performs better in zero-shot and one-shot settings, whereas our model performs better in few-shot (k = 4) settings. The results indicate that KOSMOS-1 also handles language-only tasks well and achieves favorable performance across datasets. In addition, Section 4.9.2 shows that MLLMs learn better visual commonsense knowledge compared with LLMs.", "md": "Table 13 presents the in-context learning performance of language tasks. KOSMOS-1 achieves comparable or even better performance in cloze completion and commonsense reasoning tasks when compared to LLM. In terms of the average result across all these datasets, LLM performs better in zero-shot and one-shot settings, whereas our model performs better in few-shot (k = 4) settings. The results indicate that KOSMOS-1 also handles language-only tasks well and achieves favorable performance across datasets. In addition, Section 4.9.2 shows that MLLMs learn better visual commonsense knowledge compared with LLMs."}, {"type": "table", "rows": [["Task", "Zero-shot", "One-shot", "Few-shot (k = 4)"], ["StoryCloze", "LLM 72.9 KOSMOS-1 72.1", "LLM 72.9 KOSMOS-1 72.2", "LLM 73.1 KOSMOS-1 72.3"], ["HellaSwag", "LLM 50.4 KOSMOS-1 50.0", "LLM 50.2 KOSMOS-1 50.0", "LLM 50.4 KOSMOS-1 50.3"], ["Winograd", "LLM 71.6 KOSMOS-1 69.8", "LLM 71.2 KOSMOS-1 68.4", "LLM 70.9 KOSMOS-1 69.8"], ["Winogrande", "LLM 56.7 KOSMOS-1 54.8", "LLM 56.7 KOSMOS-1 54.5", "LLM 57.0 KOSMOS-1 55.7"], ["PIQA", "LLM 73.2 KOSMOS-1 72.9", "LLM 73.0 KOSMOS-1 72.5", "LLM 72.6 KOSMOS-1 72.3"], ["BoolQ", "LLM 56.4 KOSMOS-1 56.4", "LLM 55.1 KOSMOS-1 57.2", "LLM 58.7 KOSMOS-1 59.2"], ["CB", "LLM 39.3 KOSMOS-1 44.6", "LLM 41.1 KOSMOS-1 48.2", "LLM 42.9 KOSMOS-1 53.6"], ["COPA", "LLM 68.0 KOSMOS-1 63.0", "LLM 69.0 KOSMOS-1 64.0", "LLM 69.0 KOSMOS-1 64.0"], ["Average", "LLM 61.1 KOSMOS-1 60.5", "LLM 61.2 KOSMOS-1 60.9", "LLM 61.8 KOSMOS-1 62.2"]], "md": "|Task|Zero-shot|One-shot|Few-shot (k = 4)|\n|---|---|---|---|\n|StoryCloze|LLM 72.9 KOSMOS-1 72.1|LLM 72.9 KOSMOS-1 72.2|LLM 73.1 KOSMOS-1 72.3|\n|HellaSwag|LLM 50.4 KOSMOS-1 50.0|LLM 50.2 KOSMOS-1 50.0|LLM 50.4 KOSMOS-1 50.3|\n|Winograd|LLM 71.6 KOSMOS-1 69.8|LLM 71.2 KOSMOS-1 68.4|LLM 70.9 KOSMOS-1 69.8|\n|Winogrande|LLM 56.7 KOSMOS-1 54.8|LLM 56.7 KOSMOS-1 54.5|LLM 57.0 KOSMOS-1 55.7|\n|PIQA|LLM 73.2 KOSMOS-1 72.9|LLM 73.0 KOSMOS-1 72.5|LLM 72.6 KOSMOS-1 72.3|\n|BoolQ|LLM 56.4 KOSMOS-1 56.4|LLM 55.1 KOSMOS-1 57.2|LLM 58.7 KOSMOS-1 59.2|\n|CB|LLM 39.3 KOSMOS-1 44.6|LLM 41.1 KOSMOS-1 48.2|LLM 42.9 KOSMOS-1 53.6|\n|COPA|LLM 68.0 KOSMOS-1 63.0|LLM 69.0 KOSMOS-1 64.0|LLM 69.0 KOSMOS-1 64.0|\n|Average|LLM 61.1 KOSMOS-1 60.5|LLM 61.2 KOSMOS-1 60.9|LLM 61.8 KOSMOS-1 62.2|", "isPerfectTable": true, "csv": "\"Task\",\"Zero-shot\",\"One-shot\",\"Few-shot (k = 4)\"\n\"StoryCloze\",\"LLM 72.9 KOSMOS-1 72.1\",\"LLM 72.9 KOSMOS-1 72.2\",\"LLM 73.1 KOSMOS-1 72.3\"\n\"HellaSwag\",\"LLM 50.4 KOSMOS-1 50.0\",\"LLM 50.2 KOSMOS-1 50.0\",\"LLM 50.4 KOSMOS-1 50.3\"\n\"Winograd\",\"LLM 71.6 KOSMOS-1 69.8\",\"LLM 71.2 KOSMOS-1 68.4\",\"LLM 70.9 KOSMOS-1 69.8\"\n\"Winogrande\",\"LLM 56.7 KOSMOS-1 54.8\",\"LLM 56.7 KOSMOS-1 54.5\",\"LLM 57.0 KOSMOS-1 55.7\"\n\"PIQA\",\"LLM 73.2 KOSMOS-1 72.9\",\"LLM 73.0 KOSMOS-1 72.5\",\"LLM 72.6 KOSMOS-1 72.3\"\n\"BoolQ\",\"LLM 56.4 KOSMOS-1 56.4\",\"LLM 55.1 KOSMOS-1 57.2\",\"LLM 58.7 KOSMOS-1 59.2\"\n\"CB\",\"LLM 39.3 KOSMOS-1 44.6\",\"LLM 41.1 KOSMOS-1 48.2\",\"LLM 42.9 KOSMOS-1 53.6\"\n\"COPA\",\"LLM 68.0 KOSMOS-1 63.0\",\"LLM 69.0 KOSMOS-1 64.0\",\"LLM 69.0 KOSMOS-1 64.0\"\n\"Average\",\"LLM 61.1 KOSMOS-1 60.5\",\"LLM 61.2 KOSMOS-1 60.9\",\"LLM 61.8 KOSMOS-1 62.2\""}, {"type": "text", "value": "Table 13: Performance comparisons of language tasks between KOSMOS-1 and LLM. We use the same textual data and training setup to reimplement a language model. Both models do not use instruction tuning for fair comparisons.", "md": "Table 13: Performance comparisons of language tasks between KOSMOS-1 and LLM. We use the same textual data and training setup to reimplement a language model. Both models do not use instruction tuning for fair comparisons."}, {"type": "heading", "lvl": 2, "value": "4.9 Cross-modal Transfer", "md": "## 4.9 Cross-modal Transfer"}, {"type": "text", "value": "Cross-modal transferability allows a model to learn from one modality (such as text, image, audio, etc.) and transfer the knowledge to the other modalities. This skill can enable a model to perform various tasks across different modalities. In this part, we evaluate the cross-model transferability of KOSMOS-1 on several benchmarks.", "md": "Cross-modal transferability allows a model to learn from one modality (such as text, image, audio, etc.) and transfer the knowledge to the other modalities. This skill can enable a model to perform various tasks across different modalities. In this part, we evaluate the cross-model transferability of KOSMOS-1 on several benchmarks."}, {"type": "heading", "lvl": 2, "value": "4.9.1 Transfer from Language to Multimodal: Language-Only Instruction Tuning", "md": "## 4.9.1 Transfer from Language to Multimodal: Language-Only Instruction Tuning"}, {"type": "text", "value": "To evaluate the effect of language-only instruction tuning, we conduct an ablation study using four datasets: COCO, Flickr30k, VQAv2, and VizWiz. These datasets consist of image captioning and visual questions answering. The evaluation metrics are: CIDEr scores for COCO/Flickr30k and VQA accuracy for VQAv2/VizWiz.\n\nTable 14 shows the experimental results. Language-only instruction tuning boosts our model\u2019s performance by 1.9 points on Flickr30k, 4.3 points on VQAv2, and 1.3 points on VizWiz. Our experiments show that language-only instruction tuning can significantly improve the model\u2019s instruction-following capabilities across modalities. The results also indicate that our model can transfer the instruction-following capability from language to other modalities.", "md": "To evaluate the effect of language-only instruction tuning, we conduct an ablation study using four datasets: COCO, Flickr30k, VQAv2, and VizWiz. These datasets consist of image captioning and visual questions answering. The evaluation metrics are: CIDEr scores for COCO/Flickr30k and VQA accuracy for VQAv2/VizWiz.\n\nTable 14 shows the experimental results. Language-only instruction tuning boosts our model\u2019s performance by 1.9 points on Flickr30k, 4.3 points on VQAv2, and 1.3 points on VizWiz. Our experiments show that language-only instruction tuning can significantly improve the model\u2019s instruction-following capabilities across modalities. The results also indicate that our model can transfer the instruction-following capability from language to other modalities."}]}, {"page": 17, "text": "         Model                                             COCO        Flickr30k       VQAv2        VizWiz\n         KOSMOS-1                                           84.7           67.1          51.0         29.2\n           w/o language-only instruction tuning             87.6           65.2          46.7         27.9\nTable 14: Ablation study on language-only instruction tuning. We report CIDEr scores for COCO\nand Flickr30k, and VQA accuracy scores for VQAv2 and VizWiz.\n4.9.2    Transfer from Multimodal to Language: Visual Commonsense Reasoning\nVisual commonsense reasoning tasks require an understanding of the properties of everyday objects\nin the real world, such as color, size, and shape. These tasks are challenging for language models\nbecause they may require more information about object properties than what is available in texts.\nTo investigate the visual commonsense capabilities, we compare the zero-shot performance of\nKOSMOS-1 and LLM on visual commonsense reasoning tasks.\nEvaluation Setup          We compare KOSMOS-1 and the LLM baseline on three object com-\nmonsense reasoning datasets, RELATIVESIZE [BHCF16], MEMORYCOLOR [NHJ21] and COL-\nORTERMS [BBBT12] datasets. Table 15 shows some examples of object size and color reasoning\ntasks. RELATIVESIZE contains 486 object pairs from 41 physical objects. The model is required to\npredict the size relation between two objects in a binary question-answering format with \u201cYes\u201d/\u201cNo\u201d\nanswers. MEMORYCOLOR and COLORTERMS require the model to predict the color of objects from\na set of 11 color labels in a multiple-choice format. We use only text as our input and do not include\nany images. We measure the accuracy of our model on these three datasets.\n      Task                         Example Prompt                                    Object / Pair     Answer\n      Object Size Reasoning        Is {Item1} larger than {Item2}? {Answer}          (sofa, cat)       Yes\n      Object Color Reasoning       The color of {Object} is? {Answer}                the sky           blue\n                    Table 15: Evaluation examples of object size and color reasoning.\nResults     Table 16 presents the zero-shot performance of KOSMOS-1 and LLM on visual common-\nsense reasoning tasks. KOSMOS-1 significantly outperforms LLM by 1.5% on RELATIVESIZE,\n14.7% on MEMORYCOLOR, and 9.7% on COLORTERMS dataset. The consistent improvements\nindicate that KOSMOS-1 benefits from the visual knowledge to complete the corresponding visual\ncommonsense reasoning. The reason for KOSMOS-1\u2019s superior performance is that it has modality\ntransferability, which enables the model to transfer visual knowledge to language tasks. On the\ncontrary, LLM has to rely on textual knowledge and clues to answer visual commonsense questions,\nwhich limits its ability to reason about object properties.\n             Model                      Size Reasoning                   Color Reasoning\n                                        RELATIVESIZE           MEMORYCOLOR              COLORTERMS\n               Using retrieved images\n             VALM [WDC+23]                     85.0                    58.6                   52.7\n               Language-only zero-shot evaluation\n             LLM                               92.7                    61.4                   63.4\n             KOSMOS-1                          94.2                    76.1                   73.1\nTable 16: Zero-shot visual commonsense reasoning on RELATIVESIZE, MEMORYCOLOR, and\nCOLORTERMS datasets. Accuracy scores are reported.\n5    Conclusion\nIn this work, we introduce KOSMOS-1, a multimodal large language model that can perceive general\nmodalities, follow instructions, and perform in-context learning. The models trained on web-scale\n                                                          17", "md": "|Model|COCO|Flickr30k|VQAv2|VizWiz|\n|---|---|---|---|---|\n|KOSMOS-1|84.7|67.1|51.0|29.2|\n|w/o language-only instruction tuning|87.6|65.2|46.7|27.9|\n\nTable 14: Ablation study on language-only instruction tuning. We report CIDEr scores for COCO and Flickr30k, and VQA accuracy scores for VQAv2 and VizWiz.\n\n### Transfer from Multimodal to Language: Visual Commonsense Reasoning\n\nVisual commonsense reasoning tasks require an understanding of the properties of everyday objects in the real world, such as color, size, and shape. These tasks are challenging for language models because they may require more information about object properties than what is available in texts. To investigate the visual commonsense capabilities, we compare the zero-shot performance of KOSMOS-1 and LLM on visual commonsense reasoning tasks.\n\n#### Evaluation Setup\n\nWe compare KOSMOS-1 and the LLM baseline on three object commonsense reasoning datasets, RELATIVESIZE [BHCF16], MEMORYCOLOR [NHJ21], and COLORTERMS [BBBT12] datasets. Table 15 shows some examples of object size and color reasoning tasks. RELATIVESIZE contains 486 object pairs from 41 physical objects. The model is required to predict the size relation between two objects in a binary question-answering format with \u201cYes\u201d/\u201cNo\u201d answers. MEMORYCOLOR and COLORTERMS require the model to predict the color of objects from a set of 11 color labels in a multiple-choice format. We use only text as our input and do not include any images. We measure the accuracy of our model on these three datasets.\n\n|Task|Example Prompt|Object / Pair|Answer|\n|---|---|---|---|\n|Object Size Reasoning|Is {Item1} larger than {Item2}? {Answer}|(sofa, cat)|Yes|\n|Object Color Reasoning|The color of {Object} is? {Answer}|the sky|blue|\n\nTable 15: Evaluation examples of object size and color reasoning.\n\n#### Results\n\nTable 16 presents the zero-shot performance of KOSMOS-1 and LLM on visual commonsense reasoning tasks. KOSMOS-1 significantly outperforms LLM by 1.5% on RELATIVESIZE, 14.7% on MEMORYCOLOR, and 9.7% on COLORTERMS dataset. The consistent improvements indicate that KOSMOS-1 benefits from the visual knowledge to complete the corresponding visual commonsense reasoning. The reason for KOSMOS-1\u2019s superior performance is that it has modality transferability, which enables the model to transfer visual knowledge to language tasks. On the contrary, LLM has to rely on textual knowledge and clues to answer visual commonsense questions, which limits its ability to reason about object properties.\n\n|Model|Size Reasoning|Color Reasoning|\n|---|---|---|\n|Using retrieved images| | | |\n|VALM [WDC+23]|85.0|58.6|52.7|\n|Language-only zero-shot evaluation| | | |\n|LLM|92.7|61.4|63.4|\n|KOSMOS-1|94.2|76.1|73.1|\n\nTable 16: Zero-shot visual commonsense reasoning on RELATIVESIZE, MEMORYCOLOR, and COLORTERMS datasets. Accuracy scores are reported.\n\n## Conclusion\n\nIn this work, we introduce KOSMOS-1, a multimodal large language model that can perceive general modalities, follow instructions, and perform in-context learning. The models trained on web-scale", "images": [], "items": [{"type": "table", "rows": [["Model", "COCO", "Flickr30k", "VQAv2", "VizWiz"], ["KOSMOS-1", "84.7", "67.1", "51.0", "29.2"], ["w/o language-only instruction tuning", "87.6", "65.2", "46.7", "27.9"]], "md": "|Model|COCO|Flickr30k|VQAv2|VizWiz|\n|---|---|---|---|---|\n|KOSMOS-1|84.7|67.1|51.0|29.2|\n|w/o language-only instruction tuning|87.6|65.2|46.7|27.9|", "isPerfectTable": true, "csv": "\"Model\",\"COCO\",\"Flickr30k\",\"VQAv2\",\"VizWiz\"\n\"KOSMOS-1\",\"84.7\",\"67.1\",\"51.0\",\"29.2\"\n\"w/o language-only instruction tuning\",\"87.6\",\"65.2\",\"46.7\",\"27.9\""}, {"type": "text", "value": "Table 14: Ablation study on language-only instruction tuning. We report CIDEr scores for COCO and Flickr30k, and VQA accuracy scores for VQAv2 and VizWiz.", "md": "Table 14: Ablation study on language-only instruction tuning. We report CIDEr scores for COCO and Flickr30k, and VQA accuracy scores for VQAv2 and VizWiz."}, {"type": "heading", "lvl": 3, "value": "Transfer from Multimodal to Language: Visual Commonsense Reasoning", "md": "### Transfer from Multimodal to Language: Visual Commonsense Reasoning"}, {"type": "text", "value": "Visual commonsense reasoning tasks require an understanding of the properties of everyday objects in the real world, such as color, size, and shape. These tasks are challenging for language models because they may require more information about object properties than what is available in texts. To investigate the visual commonsense capabilities, we compare the zero-shot performance of KOSMOS-1 and LLM on visual commonsense reasoning tasks.", "md": "Visual commonsense reasoning tasks require an understanding of the properties of everyday objects in the real world, such as color, size, and shape. These tasks are challenging for language models because they may require more information about object properties than what is available in texts. To investigate the visual commonsense capabilities, we compare the zero-shot performance of KOSMOS-1 and LLM on visual commonsense reasoning tasks."}, {"type": "heading", "lvl": 4, "value": "Evaluation Setup", "md": "#### Evaluation Setup"}, {"type": "text", "value": "We compare KOSMOS-1 and the LLM baseline on three object commonsense reasoning datasets, RELATIVESIZE [BHCF16], MEMORYCOLOR [NHJ21], and COLORTERMS [BBBT12] datasets. Table 15 shows some examples of object size and color reasoning tasks. RELATIVESIZE contains 486 object pairs from 41 physical objects. The model is required to predict the size relation between two objects in a binary question-answering format with \u201cYes\u201d/\u201cNo\u201d answers. MEMORYCOLOR and COLORTERMS require the model to predict the color of objects from a set of 11 color labels in a multiple-choice format. We use only text as our input and do not include any images. We measure the accuracy of our model on these three datasets.", "md": "We compare KOSMOS-1 and the LLM baseline on three object commonsense reasoning datasets, RELATIVESIZE [BHCF16], MEMORYCOLOR [NHJ21], and COLORTERMS [BBBT12] datasets. Table 15 shows some examples of object size and color reasoning tasks. RELATIVESIZE contains 486 object pairs from 41 physical objects. The model is required to predict the size relation between two objects in a binary question-answering format with \u201cYes\u201d/\u201cNo\u201d answers. MEMORYCOLOR and COLORTERMS require the model to predict the color of objects from a set of 11 color labels in a multiple-choice format. We use only text as our input and do not include any images. We measure the accuracy of our model on these three datasets."}, {"type": "table", "rows": [["Task", "Example Prompt", "Object / Pair", "Answer"], ["Object Size Reasoning", "Is {Item1} larger than {Item2}? {Answer}", "(sofa, cat)", "Yes"], ["Object Color Reasoning", "The color of {Object} is? {Answer}", "the sky", "blue"]], "md": "|Task|Example Prompt|Object / Pair|Answer|\n|---|---|---|---|\n|Object Size Reasoning|Is {Item1} larger than {Item2}? {Answer}|(sofa, cat)|Yes|\n|Object Color Reasoning|The color of {Object} is? {Answer}|the sky|blue|", "isPerfectTable": true, "csv": "\"Task\",\"Example Prompt\",\"Object / Pair\",\"Answer\"\n\"Object Size Reasoning\",\"Is {Item1} larger than {Item2}? {Answer}\",\"(sofa, cat)\",\"Yes\"\n\"Object Color Reasoning\",\"The color of {Object} is? {Answer}\",\"the sky\",\"blue\""}, {"type": "text", "value": "Table 15: Evaluation examples of object size and color reasoning.", "md": "Table 15: Evaluation examples of object size and color reasoning."}, {"type": "heading", "lvl": 4, "value": "Results", "md": "#### Results"}, {"type": "text", "value": "Table 16 presents the zero-shot performance of KOSMOS-1 and LLM on visual commonsense reasoning tasks. KOSMOS-1 significantly outperforms LLM by 1.5% on RELATIVESIZE, 14.7% on MEMORYCOLOR, and 9.7% on COLORTERMS dataset. The consistent improvements indicate that KOSMOS-1 benefits from the visual knowledge to complete the corresponding visual commonsense reasoning. The reason for KOSMOS-1\u2019s superior performance is that it has modality transferability, which enables the model to transfer visual knowledge to language tasks. On the contrary, LLM has to rely on textual knowledge and clues to answer visual commonsense questions, which limits its ability to reason about object properties.", "md": "Table 16 presents the zero-shot performance of KOSMOS-1 and LLM on visual commonsense reasoning tasks. KOSMOS-1 significantly outperforms LLM by 1.5% on RELATIVESIZE, 14.7% on MEMORYCOLOR, and 9.7% on COLORTERMS dataset. The consistent improvements indicate that KOSMOS-1 benefits from the visual knowledge to complete the corresponding visual commonsense reasoning. The reason for KOSMOS-1\u2019s superior performance is that it has modality transferability, which enables the model to transfer visual knowledge to language tasks. On the contrary, LLM has to rely on textual knowledge and clues to answer visual commonsense questions, which limits its ability to reason about object properties."}, {"type": "table", "rows": [["Model", "Size Reasoning", "Color Reasoning"], ["Using retrieved images", "", "", ""], ["VALM [WDC+23]", "85.0", "58.6", "52.7"], ["Language-only zero-shot evaluation", "", "", ""], ["LLM", "92.7", "61.4", "63.4"], ["KOSMOS-1", "94.2", "76.1", "73.1"]], "md": "|Model|Size Reasoning|Color Reasoning|\n|---|---|---|\n|Using retrieved images| | | |\n|VALM [WDC+23]|85.0|58.6|52.7|\n|Language-only zero-shot evaluation| | | |\n|LLM|92.7|61.4|63.4|\n|KOSMOS-1|94.2|76.1|73.1|", "isPerfectTable": false, "csv": "\"Model\",\"Size Reasoning\",\"Color Reasoning\"\n\"Using retrieved images\",\"\",\"\",\"\"\n\"VALM [WDC+23]\",\"85.0\",\"58.6\",\"52.7\"\n\"Language-only zero-shot evaluation\",\"\",\"\",\"\"\n\"LLM\",\"92.7\",\"61.4\",\"63.4\"\n\"KOSMOS-1\",\"94.2\",\"76.1\",\"73.1\""}, {"type": "text", "value": "Table 16: Zero-shot visual commonsense reasoning on RELATIVESIZE, MEMORYCOLOR, and COLORTERMS datasets. Accuracy scores are reported.", "md": "Table 16: Zero-shot visual commonsense reasoning on RELATIVESIZE, MEMORYCOLOR, and COLORTERMS datasets. Accuracy scores are reported."}, {"type": "heading", "lvl": 2, "value": "Conclusion", "md": "## Conclusion"}, {"type": "text", "value": "In this work, we introduce KOSMOS-1, a multimodal large language model that can perceive general modalities, follow instructions, and perform in-context learning. The models trained on web-scale", "md": "In this work, we introduce KOSMOS-1, a multimodal large language model that can perceive general modalities, follow instructions, and perform in-context learning. The models trained on web-scale"}]}, {"page": 18, "text": "multimodal corpora achieve promising results across a wide range of language tasks and multimodal\ntasks. We show that going from LLMs to MLLMs enables new capabilities and opportunities. In\nthe future, we would like to scale up KOSMOS-1 in terms of model size [MWH+22, WMH+22,\nCDH+22], and integrate the speech [WCW+23] capability into KOSMOS-1. In addition, KOSMOS-1\ncan be used as a unified interface for multimodal learning, e.g., enabling using instructions and\nexamples to control text-to-image generation.\nReferences\n  [ADL+22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana\n              Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman\n              Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Mar-\n              ianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh,\n              Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew\n              Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot\n              learning. In Advances in Neural Information Processing Systems, 2022.\n   [AFJG16] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic\n              propositional image caption evaluation. In ECCV, pages 382\u2013398, 2016.\n  [AHR+22] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Na-\n              man Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke\n              Zettlemoyer.    CM3: A causal masked multimodal model of the Internet.            ArXiv,\n              abs/2201.07520, 2022.\n  [BBBT12] Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. Distributional\n              semantics in technicolor. In ACL, 2012.\n  [BHCF16] Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are ele-\n              phants bigger than butterflies? reasoning about sizes of objects. ArXiv, abs/1602.00753,\n              2016.\n [BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\n              Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\n              hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\n              Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,\n              Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher\n              Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\n              models are few-shot learners. In Advances in Neural Information Processing Systems,\n              volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\n  [BPK+22] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and\n              Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022.\n  [BZB+20] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa:\n              Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI\n              Conference on Artificial Intelligence, 2020.\n  [CDH+22] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham\n              Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the\n              representation collapse of sparse mixture of experts. In Advances in Neural Information\n              Processing Systems, 2022.\n     [CJS90] Patricia A Carpenter, Marcel A Just, and Peter Shell. What one intelligence test\n              measures: a theoretical account of the processing in the raven progressive matrices test.\n              Psychological review, 97(3):404, 1990.\n  [CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,\n              and Kristina Toutanova. BoolQ: Exploring the surprising diffi    culty of natural yes/no\n              questions. In Proceedings of the 2019 Conference of the North American Chapter of the\n              Association for Computational Linguistics: Human Language Technologies, Volume\n              1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota, June 2019.\n              Association for Computational Linguistics.\n                                                  18", "md": "# References\n\n## References\n\n|[ADL+22]|Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems, 2022.|\n|---|---|\n|[AFJG16]|Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, pages 382\u2013398, 2016.|\n|[AHR+22]|Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A causal masked multimodal model of the Internet. ArXiv, abs/2201.07520, 2022.|\n|[BBBT12]|Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. Distributional semantics in technicolor. In ACL, 2012.|\n|[BHCF16]|Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants bigger than butterflies? reasoning about sizes of objects. ArXiv, abs/1602.00753, 2016.|\n|[BMR+20]|Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.|\n|[BPK+22]|Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022.|\n|[BZB+20]|Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.|\n|[CDH+22]|Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture of experts. In Advances in Neural Information Processing Systems, 2022.|\n|[CJS90]|Patricia A Carpenter, Marcel A Just, and Peter Shell. What one intelligence test measures: a theoretical account of the processing in the raven progressive matrices test. Psychological review, 97(3):404, 1990.|\n|[CLC+19]|Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "table", "rows": [["[ADL+22]", "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems, 2022."], ["[AFJG16]", "Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, pages 382\u2013398, 2016."], ["[AHR+22]", "Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A causal masked multimodal model of the Internet. ArXiv, abs/2201.07520, 2022."], ["[BBBT12]", "Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. Distributional semantics in technicolor. In ACL, 2012."], ["[BHCF16]", "Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants bigger than butterflies? reasoning about sizes of objects. ArXiv, abs/1602.00753, 2016."], ["[BMR+20]", "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020."], ["[BPK+22]", "Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022."], ["[BZB+20]", "Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020."], ["[CDH+22]", "Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture of experts. In Advances in Neural Information Processing Systems, 2022."], ["[CJS90]", "Patricia A Carpenter, Marcel A Just, and Peter Shell. What one intelligence test measures: a theoretical account of the processing in the raven progressive matrices test. Psychological review, 97(3):404, 1990."], ["[CLC+19]", "Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics."]], "md": "|[ADL+22]|Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems, 2022.|\n|---|---|\n|[AFJG16]|Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, pages 382\u2013398, 2016.|\n|[AHR+22]|Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A causal masked multimodal model of the Internet. ArXiv, abs/2201.07520, 2022.|\n|[BBBT12]|Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. Distributional semantics in technicolor. In ACL, 2012.|\n|[BHCF16]|Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants bigger than butterflies? reasoning about sizes of objects. ArXiv, abs/1602.00753, 2016.|\n|[BMR+20]|Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.|\n|[BPK+22]|Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022.|\n|[BZB+20]|Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.|\n|[CDH+22]|Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture of experts. In Advances in Neural Information Processing Systems, 2022.|\n|[CJS90]|Patricia A Carpenter, Marcel A Just, and Peter Shell. What one intelligence test measures: a theoretical account of the processing in the raven progressive matrices test. Psychological review, 97(3):404, 1990.|\n|[CLC+19]|Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.|", "isPerfectTable": true, "csv": "\"[ADL+22]\",\"Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. In Advances in Neural Information Processing Systems, 2022.\"\n\"[AFJG16]\",\"Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, pages 382\u2013398, 2016.\"\n\"[AHR+22]\",\"Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A causal masked multimodal model of the Internet. ArXiv, abs/2201.07520, 2022.\"\n\"[BBBT12]\",\"Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. Distributional semantics in technicolor. In ACL, 2012.\"\n\"[BHCF16]\",\"Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants bigger than butterflies? reasoning about sizes of objects. ArXiv, abs/1602.00753, 2016.\"\n\"[BMR+20]\",\"Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\"\n\"[BPK+22]\",\"Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset, 2022.\"\n\"[BZB+20]\",\"Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.\"\n\"[CDH+22]\",\"Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture of experts. In Advances in Neural Information Processing Systems, 2022.\"\n\"[CJS90]\",\"Patricia A Carpenter, Marcel A Just, and Peter Shell. What one intelligence test measures: a theoretical account of the processing in the raven progressive matrices test. Psychological review, 97(3):404, 1990.\"\n\"[CLC+19]\",\"Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\""}]}, {"page": 19, "text": " [CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\n             Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\n             Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao,\n             Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan\n             Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\n             Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\n             Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson,\n             Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\n             Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\n             Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,\n             Erica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\n             Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason\n             Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah\n             Fiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311,\n             2022.\n  [CSDS21] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m:\n             Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In\n             Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n             pages 3558\u20133568, 2021.\n  [CZC+21] Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong,\n             and Kai Yu. WebSRC: A dataset for web-based structural reading comprehension.\n             In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\n             Processing, pages 4173\u20134185, Online and Punta Cana, Dominican Republic, November\n             2021. Association for Computational Linguistics.\n  [DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\n             large-scale hierarchical image database. In 2009 IEEE Computer Society Conference\n             on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami,\n             Florida, USA, pages 248\u2013255. IEEE Computer Society, 2009.\n  [dMST19] Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The Commitment-\n             Bank: Investigating projection in naturally occurring discourse. Proceedings of Sinn\n             und Bedeutung, 23(2):107\u2013124, Jul. 2019.\n [GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\n             Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb\n             dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n[GKSS+17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\n             the v in vqa matter: Elevating the role of image understanding in visual question\n             answering. In CVPR, pages 6325\u20136334, 2017.\n  [GLS+18] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,\n             Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions\n             from blind people. In Proceedings of the IEEE conference on computer vision and\n             pattern recognition, pages 3608\u20133617, 2018.\n     [HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\n             arXiv:1606.08415, 2016.\n  [HSD+22] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shum-\n             ing Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv,\n             abs/2206.06336, 2022.\n  [HSLS22] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions:\n             Tuning language models with (almost) no human labor, 2022.\n      [JR03] John and Jean Raven. Raven Progressive Matrices, pages 223\u2013237. Springer US,\n             Boston, MA, 2003.\n                                                19", "md": "# References\n\n## References\n\n- [CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan\nDu, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson,\nLiam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah\nFiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311,\n2022.\n- [CSDS21] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m:\nPushing web-scale image-text pre-training to recognize long-tail visual concepts. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 3558\u20133568, 2021.\n- [CZC+21] Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong,\nand Kai Yu. WebSRC: A dataset for web-based structural reading comprehension.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 4173\u20134185, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics.\n- [DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In 2009 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami,\nFlorida, USA, pages 248\u2013255. IEEE Computer Society, 2009.\n- [dMST19] Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The Commitment-\nBank: Investigating projection in naturally occurring discourse. Proceedings of Sinn\nund Bedeutung, 23(2):107\u2013124, Jul. 2019.\n- [GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb\ndataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n- [GKSS+17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe v in vqa matter: Elevating the role of image understanding in visual question\nanswering. In CVPR, pages 6325\u20136334, 2017.\n- [GLS+18] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,\nJiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions\nfrom blind people. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3608\u20133617, 2018.\n- [HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\narXiv:1606.08415, 2016.\n- [HSD+22] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shum-\ning Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv,\nabs/2206.06336, 2022.\n- [HSLS22] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions:\nTuning language models with (almost) no human labor, 2022.\n- [JR03] John and Jean Raven. Raven Progressive Matrices, pages 223\u2013237. Springer US,\nBoston, MA, 2003.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 2, "value": "References", "md": "## References"}, {"type": "text", "value": "- [CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan\nDu, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson,\nLiam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah\nFiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311,\n2022.\n- [CSDS21] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m:\nPushing web-scale image-text pre-training to recognize long-tail visual concepts. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 3558\u20133568, 2021.\n- [CZC+21] Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong,\nand Kai Yu. WebSRC: A dataset for web-based structural reading comprehension.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 4173\u20134185, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics.\n- [DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In 2009 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami,\nFlorida, USA, pages 248\u2013255. IEEE Computer Society, 2009.\n- [dMST19] Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The Commitment-\nBank: Investigating projection in naturally occurring discourse. Proceedings of Sinn\nund Bedeutung, 23(2):107\u2013124, Jul. 2019.\n- [GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb\ndataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n- [GKSS+17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe v in vqa matter: Elevating the role of image understanding in visual question\nanswering. In CVPR, pages 6325\u20136334, 2017.\n- [GLS+18] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,\nJiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions\nfrom blind people. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3608\u20133617, 2018.\n- [HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\narXiv:1606.08415, 2016.\n- [HSD+22] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shum-\ning Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv,\nabs/2206.06336, 2022.\n- [HSLS22] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions:\nTuning language models with (almost) no human labor, 2022.\n- [JR03] John and Jean Raven. Raven Progressive Matrices, pages 223\u2013237. Springer US,\nBoston, MA, 2003.", "md": "- [CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek B Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan\nDu, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra, Kevin Robinson,\nLiam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,\nAlexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Oliveira Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah\nFiedel. PaLM: Scaling language modeling with pathways. ArXiv, abs/2204.02311,\n2022.\n- [CSDS21] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m:\nPushing web-scale image-text pre-training to recognize long-tail visual concepts. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 3558\u20133568, 2021.\n- [CZC+21] Xingyu Chen, Zihan Zhao, Lu Chen, JiaBao Ji, Danyang Zhang, Ao Luo, Yuxuan Xiong,\nand Kai Yu. WebSRC: A dataset for web-based structural reading comprehension.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pages 4173\u20134185, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics.\n- [DDS+09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In 2009 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami,\nFlorida, USA, pages 248\u2013255. IEEE Computer Society, 2009.\n- [dMST19] Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. The Commitment-\nBank: Investigating projection in naturally occurring discourse. Proceedings of Sinn\nund Bedeutung, 23(2):107\u2013124, Jul. 2019.\n- [GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb\ndataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n- [GKSS+17] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making\nthe v in vqa matter: Elevating the role of image understanding in visual question\nanswering. In CVPR, pages 6325\u20136334, 2017.\n- [GLS+18] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,\nJiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions\nfrom blind people. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 3608\u20133617, 2018.\n- [HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint\narXiv:1606.08415, 2016.\n- [HSD+22] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shum-\ning Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv,\nabs/2206.06336, 2022.\n- [HSLS22] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions:\nTuning language models with (almost) no human labor, 2022.\n- [JR03] John and Jean Raven. Raven Progressive Matrices, pages 223\u2013237. Springer US,\nBoston, MA, 2003."}]}, {"page": 20, "text": "    [KFF17] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating\n             image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n             39(4):664\u2013676, 2017.\n [KFM+20] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh,\n             Pratik Ringshia, and Davide Testuggine. The hateful memes challenge: Detecting hate\n             speech in multimodal memes. In Advances in Neural Information Processing Systems,\n             volume 33, pages 2611\u20132624, 2020.\n     [KR18] Taku Kudo and John Richardson. SentencePiece: A simple and language independent\n             subword tokenizer and detokenizer for neural text processing. In EMNLP, pages 66\u201371,\n             2018.\n    [KSF23] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to\n             images for multimodal generation. arXiv preprint arXiv:2301.13823, 2023.\n  [LDM12a] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema chal-\n             lenge. In Thirteenth International Conference on the Principles of Knowledge Repre-\n             sentation and Reasoning, 2012.\n [LDM12b] Hector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema\n             challenge. In Principles of Knowledge Representation and Reasoning, 2012.\n [LHV+23] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny\n             Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data\n             and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023.\n  [LLSH23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping\n             language-image pre-training with frozen image encoders and large language models.\n             ArXiv, abs/2301.12597, 2023.\n [LMB+14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\n             Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in\n             context. In ECCV, pages 740\u2013755, 2014.\n [LOG+19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n             Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly\n             optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\n [MRL+17] Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James\n             Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd\n             Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics,\n             pages 46\u201351, 2017.\n[MWH+22] Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong,\n             Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. TorchScale:\n             Transformers at scale. CoRR, abs/2211.13184, 2022.\n    [NHJ21] Tobias Norlund, Lovisa Hagstr\u00f6m, and Richard Johansson.           Transferring knowl-\n             edge from vision to language: How to achieve it and how to measure it?         ArXiv,\n             abs/2109.11321, 2021.\n   [RBG11] Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. Choice of plau-\n             sible alternatives: An evaluation of commonsense causal reasoning. In AAAI Spring\n             Symposium, 2011.\n [RKH+21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\n             Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\n             ing transferable visual models from natural language supervision. In International\n             conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n  [RPJ+20] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P.\n             Lillicrap. Compressive transformers for long-range sequence modelling. In ICLR, 2020.\n                                                20", "md": "# References\n\n# List of References\n\n|Reference|Authors|Title|Publication Details|\n|---|---|---|---|\n|[KFF17]|Andrej Karpathy and Li Fei-Fei|Deep visual-semantic alignments for generating image descriptions|IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):664\u2013676, 2017|\n|[KFM+20]|Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine|The hateful memes challenge: Detecting hate speech in multimodal memes|In Advances in Neural Information Processing Systems, volume 33, pages 2611\u20132624, 2020|\n|[KR18]|Taku Kudo and John Richardson|SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing|In EMNLP, pages 66\u201371, 2018|\n|[KSF23]|Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried|Grounding language models to images for multimodal generation|arXiv preprint arXiv:2301.13823, 2023|\n|[LDM12a]|Hector Levesque, Ernest Davis, and Leora Morgenstern|The winograd schema challenge|In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012|\n|[LDM12b]|Hector J. Levesque, Ernest Davis, and Leora Morgenstern|The winograd schema challenge|In Principles of Knowledge Representation and Reasoning, 2012|\n|[LHV+23]|Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al.|The flan collection: Designing data and methods for effective instruction tuning|arXiv preprint arXiv:2301.13688, 2023|\n|[LLSH23]|Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi|BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models|ArXiv, abs/2301.12597, 2023|\n|[LMB+14]|Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick|Microsoft coco: Common objects in context|In ECCV, pages 740\u2013755, 2014|\n|[LOG+19]|Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov|RoBERTa: A robustly optimized bert pretraining approach|arXiv preprint arXiv:1907.11692, 2019|\n|[MRL+17]|Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen|Lsdsem 2017 shared task: The story cloze test|In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46\u201351, 2017|\n|[MWH+22]|Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei|TorchScale: Transformers at scale|CoRR, abs/2211.13184, 2022|\n|[NHJ21]|Tobias Norlund, Lovisa Hagstr\u00f6m, and Richard Johansson|Transferring knowledge from vision to language: How to achieve it and how to measure it?|ArXiv, abs/2109.11321, 2021|\n|[RBG11]|Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon|Choice of plausible alternatives: An evaluation of commonsense causal reasoning|In AAAI Spring Symposium, 2011|\n|[RKH+21]|Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.|Learning transferable visual models from natural language supervision|In International conference on machine learning, pages 8748\u20138763. PMLR, 2021|\n|[RPJ+20]|Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap|Compressive transformers for long-range sequence modelling|In ICLR, 2020|", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["Reference", "Authors", "Title", "Publication Details"], ["[KFF17]", "Andrej Karpathy and Li Fei-Fei", "Deep visual-semantic alignments for generating image descriptions", "IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):664\u2013676, 2017"], ["[KFM+20]", "Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine", "The hateful memes challenge: Detecting hate speech in multimodal memes", "In Advances in Neural Information Processing Systems, volume 33, pages 2611\u20132624, 2020"], ["[KR18]", "Taku Kudo and John Richardson", "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "In EMNLP, pages 66\u201371, 2018"], ["[KSF23]", "Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried", "Grounding language models to images for multimodal generation", "arXiv preprint arXiv:2301.13823, 2023"], ["[LDM12a]", "Hector Levesque, Ernest Davis, and Leora Morgenstern", "The winograd schema challenge", "In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012"], ["[LDM12b]", "Hector J. Levesque, Ernest Davis, and Leora Morgenstern", "The winograd schema challenge", "In Principles of Knowledge Representation and Reasoning, 2012"], ["[LHV+23]", "Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al.", "The flan collection: Designing data and methods for effective instruction tuning", "arXiv preprint arXiv:2301.13688, 2023"], ["[LLSH23]", "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi", "BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "ArXiv, abs/2301.12597, 2023"], ["[LMB+14]", "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick", "Microsoft coco: Common objects in context", "In ECCV, pages 740\u2013755, 2014"], ["[LOG+19]", "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov", "RoBERTa: A robustly optimized bert pretraining approach", "arXiv preprint arXiv:1907.11692, 2019"], ["[MRL+17]", "Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen", "Lsdsem 2017 shared task: The story cloze test", "In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46\u201351, 2017"], ["[MWH+22]", "Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei", "TorchScale: Transformers at scale", "CoRR, abs/2211.13184, 2022"], ["[NHJ21]", "Tobias Norlund, Lovisa Hagstr\u00f6m, and Richard Johansson", "Transferring knowledge from vision to language: How to achieve it and how to measure it?", "ArXiv, abs/2109.11321, 2021"], ["[RBG11]", "Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon", "Choice of plausible alternatives: An evaluation of commonsense causal reasoning", "In AAAI Spring Symposium, 2011"], ["[RKH+21]", "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.", "Learning transferable visual models from natural language supervision", "In International conference on machine learning, pages 8748\u20138763. PMLR, 2021"], ["[RPJ+20]", "Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap", "Compressive transformers for long-range sequence modelling", "In ICLR, 2020"]], "md": "|Reference|Authors|Title|Publication Details|\n|---|---|---|---|\n|[KFF17]|Andrej Karpathy and Li Fei-Fei|Deep visual-semantic alignments for generating image descriptions|IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):664\u2013676, 2017|\n|[KFM+20]|Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine|The hateful memes challenge: Detecting hate speech in multimodal memes|In Advances in Neural Information Processing Systems, volume 33, pages 2611\u20132624, 2020|\n|[KR18]|Taku Kudo and John Richardson|SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing|In EMNLP, pages 66\u201371, 2018|\n|[KSF23]|Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried|Grounding language models to images for multimodal generation|arXiv preprint arXiv:2301.13823, 2023|\n|[LDM12a]|Hector Levesque, Ernest Davis, and Leora Morgenstern|The winograd schema challenge|In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012|\n|[LDM12b]|Hector J. Levesque, Ernest Davis, and Leora Morgenstern|The winograd schema challenge|In Principles of Knowledge Representation and Reasoning, 2012|\n|[LHV+23]|Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al.|The flan collection: Designing data and methods for effective instruction tuning|arXiv preprint arXiv:2301.13688, 2023|\n|[LLSH23]|Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi|BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models|ArXiv, abs/2301.12597, 2023|\n|[LMB+14]|Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick|Microsoft coco: Common objects in context|In ECCV, pages 740\u2013755, 2014|\n|[LOG+19]|Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov|RoBERTa: A robustly optimized bert pretraining approach|arXiv preprint arXiv:1907.11692, 2019|\n|[MRL+17]|Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen|Lsdsem 2017 shared task: The story cloze test|In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46\u201351, 2017|\n|[MWH+22]|Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei|TorchScale: Transformers at scale|CoRR, abs/2211.13184, 2022|\n|[NHJ21]|Tobias Norlund, Lovisa Hagstr\u00f6m, and Richard Johansson|Transferring knowledge from vision to language: How to achieve it and how to measure it?|ArXiv, abs/2109.11321, 2021|\n|[RBG11]|Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon|Choice of plausible alternatives: An evaluation of commonsense causal reasoning|In AAAI Spring Symposium, 2011|\n|[RKH+21]|Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.|Learning transferable visual models from natural language supervision|In International conference on machine learning, pages 8748\u20138763. PMLR, 2021|\n|[RPJ+20]|Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap|Compressive transformers for long-range sequence modelling|In ICLR, 2020|", "isPerfectTable": true, "csv": "\"Reference\",\"Authors\",\"Title\",\"Publication Details\"\n\"[KFF17]\",\"Andrej Karpathy and Li Fei-Fei\",\"Deep visual-semantic alignments for generating image descriptions\",\"IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4):664\u2013676, 2017\"\n\"[KFM+20]\",\"Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine\",\"The hateful memes challenge: Detecting hate speech in multimodal memes\",\"In Advances in Neural Information Processing Systems, volume 33, pages 2611\u20132624, 2020\"\n\"[KR18]\",\"Taku Kudo and John Richardson\",\"SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing\",\"In EMNLP, pages 66\u201371, 2018\"\n\"[KSF23]\",\"Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried\",\"Grounding language models to images for multimodal generation\",\"arXiv preprint arXiv:2301.13823, 2023\"\n\"[LDM12a]\",\"Hector Levesque, Ernest Davis, and Leora Morgenstern\",\"The winograd schema challenge\",\"In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning, 2012\"\n\"[LDM12b]\",\"Hector J. Levesque, Ernest Davis, and Leora Morgenstern\",\"The winograd schema challenge\",\"In Principles of Knowledge Representation and Reasoning, 2012\"\n\"[LHV+23]\",\"Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al.\",\"The flan collection: Designing data and methods for effective instruction tuning\",\"arXiv preprint arXiv:2301.13688, 2023\"\n\"[LLSH23]\",\"Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi\",\"BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\",\"ArXiv, abs/2301.12597, 2023\"\n\"[LMB+14]\",\"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick\",\"Microsoft coco: Common objects in context\",\"In ECCV, pages 740\u2013755, 2014\"\n\"[LOG+19]\",\"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov\",\"RoBERTa: A robustly optimized bert pretraining approach\",\"arXiv preprint arXiv:1907.11692, 2019\"\n\"[MRL+17]\",\"Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen\",\"Lsdsem 2017 shared task: The story cloze test\",\"In Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 46\u201351, 2017\"\n\"[MWH+22]\",\"Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei\",\"TorchScale: Transformers at scale\",\"CoRR, abs/2211.13184, 2022\"\n\"[NHJ21]\",\"Tobias Norlund, Lovisa Hagstr\u00f6m, and Richard Johansson\",\"Transferring knowledge from vision to language: How to achieve it and how to measure it?\",\"ArXiv, abs/2109.11321, 2021\"\n\"[RBG11]\",\"Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon\",\"Choice of plausible alternatives: An evaluation of commonsense causal reasoning\",\"In AAAI Spring Symposium, 2011\"\n\"[RKH+21]\",\"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.\",\"Learning transferable visual models from natural language supervision\",\"In International conference on machine learning, pages 8748\u20138763. PMLR, 2021\"\n\"[RPJ+20]\",\"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap\",\"Compressive transformers for long-range sequence modelling\",\"In ICLR, 2020\""}]}, {"page": 21, "text": "  [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande:\n              An adversarial winograd schema challenge at scale. In AAAI, pages 8732\u20138740, 2020.\n  [SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\n              man, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman,\n              et al. Laion-5b: An open large-scale dataset for training next generation image-text\n              models. arXiv preprint arXiv:2210.08402, 2022.\n  [SDGS18] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions:\n              A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Pro-\n              ceedings of the 56th Annual Meeting of the Association for Computational Linguistics,\n              ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages\n              2556\u20132565. Association for Computational Linguistics, 2018.\n  [SDP+22] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,\n              Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer.\n              arXiv preprint arXiv:2212.10554, 2022.\n  [SPN+22] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhan-\n              dari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti,\n              Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mo-\n              hammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.\n              Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale\n              generative language model, 2022.\n  [SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\n              and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models\n              using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n [SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning,\n              Andrew Ng, and Christopher Potts. Recursive deep models for semantic composition-\n              ality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical\n              Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA,\n              October 2013. Association for Computational Linguistics.\n  [SVB+21] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clay-\n              ton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-\n              400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint\n              arXiv:2111.02114, 2021.\n [TMC+21] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and\n              Felix Hill. Multimodal few-shot learning with frozen language models. In Neural\n              Information Processing Systems, 2021.\n  [VLZP15] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based\n              image description evaluation. In CVPR, pages 4566\u20134575, 2015.\n [WBD+22] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti\n              Aggarwal, Owais Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image\n              as a foreign language: BEiT pretraining for all vision and vision-language tasks. ArXiv,\n              abs/2208.10442, 2022.\n[WBW+11] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge J. Belongie.\n              The caltech-ucsd birds-200-2011 dataset. 2011.\n[WCW+23] Chengyi Wang, Sanyuan Chen, Yu Wu, Zi-Hua Zhang, Long Zhou, Shujie Liu, Zhuo\n              Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei.\n              Neural codec language models are zero-shot text to speech synthesizers.          ArXiv,\n              abs/2301.02111, 2023.\n [WDC+23] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jian-\n              feng Gao, and Furu Wei. Visually-augmented language modeling. In International\n              Conference on Learning Representations, 2023.\n                                                  21", "md": "# List of References\n\n# List of References\n\n- [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. In AAAI, pages 8732\u20138740, 2020.\n- [SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n- [SDGS18] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2556\u20132565. Association for Computational Linguistics, 2018.\n- [SDP+22] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022.\n- [SPN+22] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model, 2022.\n- [SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n- [SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\n- [SVB+21] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n- [TMC+21] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. In Neural Information Processing Systems, 2021.\n- [VLZP15] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, pages 4566\u20134575, 2015.\n- [WBD+22] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: BEiT pretraining for all vision and vision-language tasks. ArXiv, abs/2208.10442, 2022.\n- [WBW+11] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge J. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n- [WCW+23] Chengyi Wang, Sanyuan Chen, Yu Wu, Zi-Hua Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language models are zero-shot text to speech synthesizers. ArXiv, abs/2301.02111, 2023.\n- [WDC+23] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Visually-augmented language modeling. In International Conference on Learning Representations, 2023.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "- [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. In AAAI, pages 8732\u20138740, 2020.\n- [SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n- [SDGS18] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2556\u20132565. Association for Computational Linguistics, 2018.\n- [SDP+22] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022.\n- [SPN+22] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model, 2022.\n- [SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n- [SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\n- [SVB+21] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n- [TMC+21] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. In Neural Information Processing Systems, 2021.\n- [VLZP15] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, pages 4566\u20134575, 2015.\n- [WBD+22] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: BEiT pretraining for all vision and vision-language tasks. ArXiv, abs/2208.10442, 2022.\n- [WBW+11] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge J. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n- [WCW+23] Chengyi Wang, Sanyuan Chen, Yu Wu, Zi-Hua Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language models are zero-shot text to speech synthesizers. ArXiv, abs/2301.02111, 2023.\n- [WDC+23] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Visually-augmented language modeling. In International Conference on Learning Representations, 2023.", "md": "- [SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial winograd schema challenge at scale. In AAAI, pages 8732\u20138740, 2020.\n- [SBV+22] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\n- [SDGS18] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2556\u20132565. Association for Computational Linguistics, 2018.\n- [SDP+22] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022.\n- [SPN+22] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative language model, 2022.\n- [SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.\n- [SPW+13] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\n- [SVB+21] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n- [TMC+21] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. Multimodal few-shot learning with frozen language models. In Neural Information Processing Systems, 2021.\n- [VLZP15] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, pages 4566\u20134575, 2015.\n- [WBD+22] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: BEiT pretraining for all vision and vision-language tasks. ArXiv, abs/2208.10442, 2022.\n- [WBW+11] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge J. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.\n- [WCW+23] Chengyi Wang, Sanyuan Chen, Yu Wu, Zi-Hua Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, and Furu Wei. Neural codec language models are zero-shot text to speech synthesizers. ArXiv, abs/2301.02111, 2023.\n- [WDC+23] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Visually-augmented language modeling. In International Conference on Learning Representations, 2023."}]}, {"page": 22, "text": "[WMD+22] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu\n              Wei. DeepNet: Scaling Transformers to 1,000 layers. CoRR, abs/2203.00555, 2022.\n[WMH+22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,\n              Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav\n              Chaudhary, Xia Song, and Furu Wei. Foundation transformers. CoRR, abs/2210.06423,\n              2022.\n [WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,\n              Felix Hill, Omer Levy, and Samuel R Bowman. SuperGLUE: A stickier benchmark for\n              general-purpose language understanding systems. arXiv preprint arXiv:1905.00537,\n              2019.\n [WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and\n              Denny Zhou. Chain of thought prompting elicits reasoning in large language models.\n              arXiv preprint arXiv:2201.11903, 2022.\n [WYH+22] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng\n              Liu, Ce Liu, and Lijuan Wang. GIT: A generative image-to-text transformer for vision\n              and language. CoRR, abs/2205.14100, 2022.\n  [YAS+22] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec,\n              Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Retrieval-augmented\n              multimodal language modeling. ArXiv, abs/2211.12561, 2022.\n [YLHH14] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descrip-\n              tions to visual denotations: New similarity metrics for semantic inference over event\n              descriptions. TACL, 2:67\u201378, 2014.\n  [ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag:\n              Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting\n              of the Association for Computational Linguistics, 2019.\n                                                22", "md": "# References\n\n# List of References\n\n## [WMD+22] DeepNet: Scaling Transformers to 1,000 layers\n\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. CoRR, abs/2203.00555, 2022.\n\n## [WMH+22] Foundation transformers\n\nHongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, and Furu Wei. CoRR, abs/2210.06423, 2022.\n\n## [WPN+19] SuperGLUE: A stickier benchmark for general-purpose language understanding systems\n\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. arXiv preprint arXiv:1905.00537, 2019.\n\n## [WWS+22] Chain of thought prompting elicits reasoning in large language models\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. arXiv preprint arXiv:2201.11903, 2022.\n\n## [WYH+22] GIT: A generative image-to-text transformer for vision and language\n\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. CoRR, abs/2205.14100, 2022.\n\n## [YAS+22] Retrieval-augmented multimodal language modeling\n\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. ArXiv, abs/2211.12561, 2022.\n\n## [YLHH14] From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\n\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. TACL, 2:67\u201378, 2014.\n\n## [ZHB+19] Hellaswag: Can a machine really finish your sentence?\n\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "heading", "lvl": 2, "value": "[WMD+22] DeepNet: Scaling Transformers to 1,000 layers", "md": "## [WMD+22] DeepNet: Scaling Transformers to 1,000 layers"}, {"type": "text", "value": "Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. CoRR, abs/2203.00555, 2022.", "md": "Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. CoRR, abs/2203.00555, 2022."}, {"type": "heading", "lvl": 2, "value": "[WMH+22] Foundation transformers", "md": "## [WMH+22] Foundation transformers"}, {"type": "text", "value": "Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, and Furu Wei. CoRR, abs/2210.06423, 2022.", "md": "Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song, and Furu Wei. CoRR, abs/2210.06423, 2022."}, {"type": "heading", "lvl": 2, "value": "[WPN+19] SuperGLUE: A stickier benchmark for general-purpose language understanding systems", "md": "## [WPN+19] SuperGLUE: A stickier benchmark for general-purpose language understanding systems"}, {"type": "text", "value": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. arXiv preprint arXiv:1905.00537, 2019.", "md": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. arXiv preprint arXiv:1905.00537, 2019."}, {"type": "heading", "lvl": 2, "value": "[WWS+22] Chain of thought prompting elicits reasoning in large language models", "md": "## [WWS+22] Chain of thought prompting elicits reasoning in large language models"}, {"type": "text", "value": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. arXiv preprint arXiv:2201.11903, 2022.", "md": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. arXiv preprint arXiv:2201.11903, 2022."}, {"type": "heading", "lvl": 2, "value": "[WYH+22] GIT: A generative image-to-text transformer for vision and language", "md": "## [WYH+22] GIT: A generative image-to-text transformer for vision and language"}, {"type": "text", "value": "Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. CoRR, abs/2205.14100, 2022.", "md": "Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. CoRR, abs/2205.14100, 2022."}, {"type": "heading", "lvl": 2, "value": "[YAS+22] Retrieval-augmented multimodal language modeling", "md": "## [YAS+22] Retrieval-augmented multimodal language modeling"}, {"type": "text", "value": "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. ArXiv, abs/2211.12561, 2022.", "md": "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. ArXiv, abs/2211.12561, 2022."}, {"type": "heading", "lvl": 2, "value": "[YLHH14] From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "md": "## [YLHH14] From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"}, {"type": "text", "value": "Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. TACL, 2:67\u201378, 2014.", "md": "Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. TACL, 2:67\u201378, 2014."}, {"type": "heading", "lvl": 2, "value": "[ZHB+19] Hellaswag: Can a machine really finish your sentence?", "md": "## [ZHB+19] Hellaswag: Can a machine really finish your sentence?"}, {"type": "text", "value": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019.", "md": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019."}]}, {"page": 23, "text": "A     Hyperparameters\nA.1    Training\nWe report the detailed model hyperparameter settings of KOSMOS-1 in Table 17 and training\nhyperparameters in Table 18.\n                         Hyperparameters\n                         Number of layers                             24\n                         Hidden size                                 2,048\n                         FFN inner hidden size                       8,192\n                         Attention heads                              32\n                         Dropout                                      0.1\n                         Attention dropout                            0.1\n                         Activation function                    GeLU [HG16]\n                         Vocabulary size                            64,007\n                         Soft tokens V size                           64\n                         Max length                                  2,048\n                         Relative position embedding           xPos [SDP+22]\n                         Initialization                     Magneto [WMH+22]\n                 Table 17: Hyperparameters of causal language model of KOSMOS-1\n                             Hyperparameters\n                             Training steps                           300,000\n                             Warmup steps                               375\n                             Batch size of text corpora                 256\n                             Max length of text corpora                2,048\n                             Batch size of image-caption pairs         6,144\n                             Batch size of interleaved data             128\n                             Optimizer                                 Adam\n                             Learning rate                              2e-4\n                             Learning Rate Decay                       Linear\n                             Adam \u03f5                                     1e-6\n                             Adam \u03b2                                  (0.9, 0.98)\n                             Weight decay                               0.01\n                           Table 18: Training hyperparameters of KOSMOS-1\nA.2    Language-Only Instruction Tuning\nThe detailed instruction tuning hyperparameters are listed in Table 19.\n                               Hyperparameters\n                               Training steps                          10,000\n                               Warmup steps                             375\n                               Batch size of instruction data           256\n                               Batch size of text corpora                32\n                               Batch size of image-caption pairs        768\n                               Batch size of interleaved data            16\n                               Learning rate                            2e-5\n                      Table 19: Instruction tuning hyperparameters of KOSMOS-1\n                                                     23", "md": "# Hyperparameters\n\n## A.1 Training\n\n|Hyperparameters| |\n|---|---|\n|Number of layers|24|\n|Hidden size|2,048|\n|FFN inner hidden size|8,192|\n|Attention heads|32|\n|Dropout|0.1|\n|Attention dropout|0.1|\n|Activation function|GeLU [HG16]|\n|Vocabulary size|64,007|\n|Soft tokens V size|64|\n|Max length|2,048|\n|Relative position embedding|xPos [SDP+22]|\n|Initialization|Magneto [WMH+22]|\n\nTable 17: Hyperparameters of causal language model of KOSMOS-1\n\n|Hyperparameters| |\n|---|---|\n|Training steps|300,000|\n|Warmup steps|375|\n|Batch size of text corpora|256|\n|Max length of text corpora|2,048|\n|Batch size of image-caption pairs|6,144|\n|Batch size of interleaved data|128|\n|Optimizer|Adam|\n|Learning rate|2e-4|\n|Learning Rate Decay|Linear|\n|Adam \u03f5|1e-6|\n|Adam \u03b2|(0.9, 0.98)|\n|Weight decay|0.01|\n\nTable 18: Training hyperparameters of KOSMOS-1\n\n## A.2 Language-Only Instruction Tuning\n\n|Hyperparameters| |\n|---|---|\n|Training steps|10,000|\n|Warmup steps|375|\n|Batch size of instruction data|256|\n|Batch size of text corpora|32|\n|Batch size of image-caption pairs|768|\n|Batch size of interleaved data|16|\n|Learning rate|2e-5|\n\nTable 19: Instruction tuning hyperparameters of KOSMOS-1", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Hyperparameters", "md": "# Hyperparameters"}, {"type": "heading", "lvl": 2, "value": "A.1 Training", "md": "## A.1 Training"}, {"type": "table", "rows": [["Hyperparameters", ""], ["Number of layers", "24"], ["Hidden size", "2,048"], ["FFN inner hidden size", "8,192"], ["Attention heads", "32"], ["Dropout", "0.1"], ["Attention dropout", "0.1"], ["Activation function", "GeLU [HG16]"], ["Vocabulary size", "64,007"], ["Soft tokens V size", "64"], ["Max length", "2,048"], ["Relative position embedding", "xPos [SDP+22]"], ["Initialization", "Magneto [WMH+22]"]], "md": "|Hyperparameters| |\n|---|---|\n|Number of layers|24|\n|Hidden size|2,048|\n|FFN inner hidden size|8,192|\n|Attention heads|32|\n|Dropout|0.1|\n|Attention dropout|0.1|\n|Activation function|GeLU [HG16]|\n|Vocabulary size|64,007|\n|Soft tokens V size|64|\n|Max length|2,048|\n|Relative position embedding|xPos [SDP+22]|\n|Initialization|Magneto [WMH+22]|", "isPerfectTable": true, "csv": "\"Hyperparameters\",\"\"\n\"Number of layers\",\"24\"\n\"Hidden size\",\"2,048\"\n\"FFN inner hidden size\",\"8,192\"\n\"Attention heads\",\"32\"\n\"Dropout\",\"0.1\"\n\"Attention dropout\",\"0.1\"\n\"Activation function\",\"GeLU [HG16]\"\n\"Vocabulary size\",\"64,007\"\n\"Soft tokens V size\",\"64\"\n\"Max length\",\"2,048\"\n\"Relative position embedding\",\"xPos [SDP+22]\"\n\"Initialization\",\"Magneto [WMH+22]\""}, {"type": "text", "value": "Table 17: Hyperparameters of causal language model of KOSMOS-1", "md": "Table 17: Hyperparameters of causal language model of KOSMOS-1"}, {"type": "table", "rows": [["Hyperparameters", ""], ["Training steps", "300,000"], ["Warmup steps", "375"], ["Batch size of text corpora", "256"], ["Max length of text corpora", "2,048"], ["Batch size of image-caption pairs", "6,144"], ["Batch size of interleaved data", "128"], ["Optimizer", "Adam"], ["Learning rate", "2e-4"], ["Learning Rate Decay", "Linear"], ["Adam \u03f5", "1e-6"], ["Adam \u03b2", "(0.9, 0.98)"], ["Weight decay", "0.01"]], "md": "|Hyperparameters| |\n|---|---|\n|Training steps|300,000|\n|Warmup steps|375|\n|Batch size of text corpora|256|\n|Max length of text corpora|2,048|\n|Batch size of image-caption pairs|6,144|\n|Batch size of interleaved data|128|\n|Optimizer|Adam|\n|Learning rate|2e-4|\n|Learning Rate Decay|Linear|\n|Adam \u03f5|1e-6|\n|Adam \u03b2|(0.9, 0.98)|\n|Weight decay|0.01|", "isPerfectTable": true, "csv": "\"Hyperparameters\",\"\"\n\"Training steps\",\"300,000\"\n\"Warmup steps\",\"375\"\n\"Batch size of text corpora\",\"256\"\n\"Max length of text corpora\",\"2,048\"\n\"Batch size of image-caption pairs\",\"6,144\"\n\"Batch size of interleaved data\",\"128\"\n\"Optimizer\",\"Adam\"\n\"Learning rate\",\"2e-4\"\n\"Learning Rate Decay\",\"Linear\"\n\"Adam \u03f5\",\"1e-6\"\n\"Adam \u03b2\",\"(0.9, 0.98)\"\n\"Weight decay\",\"0.01\""}, {"type": "text", "value": "Table 18: Training hyperparameters of KOSMOS-1", "md": "Table 18: Training hyperparameters of KOSMOS-1"}, {"type": "heading", "lvl": 2, "value": "A.2 Language-Only Instruction Tuning", "md": "## A.2 Language-Only Instruction Tuning"}, {"type": "table", "rows": [["Hyperparameters", ""], ["Training steps", "10,000"], ["Warmup steps", "375"], ["Batch size of instruction data", "256"], ["Batch size of text corpora", "32"], ["Batch size of image-caption pairs", "768"], ["Batch size of interleaved data", "16"], ["Learning rate", "2e-5"]], "md": "|Hyperparameters| |\n|---|---|\n|Training steps|10,000|\n|Warmup steps|375|\n|Batch size of instruction data|256|\n|Batch size of text corpora|32|\n|Batch size of image-caption pairs|768|\n|Batch size of interleaved data|16|\n|Learning rate|2e-5|", "isPerfectTable": true, "csv": "\"Hyperparameters\",\"\"\n\"Training steps\",\"10,000\"\n\"Warmup steps\",\"375\"\n\"Batch size of instruction data\",\"256\"\n\"Batch size of text corpora\",\"32\"\n\"Batch size of image-caption pairs\",\"768\"\n\"Batch size of interleaved data\",\"16\"\n\"Learning rate\",\"2e-5\""}, {"type": "text", "value": "Table 19: Instruction tuning hyperparameters of KOSMOS-1", "md": "Table 19: Instruction tuning hyperparameters of KOSMOS-1"}]}, {"page": 24, "text": "B    Datasets\nB.1   Pretraning\nB.1.1   Text Corpora\nKOSMOS-1 is trained on The Pile [GBB+20] and Common Crawl. The Pile is an 800 GB English\ntext corpus combining 22 diverse sources. We select a subset with seven sources from The Pile.\nCommon Crawl is also included in training corpora. Common Crawl takes snapshots of the web,\nwhich contains massive amounts of language data. Table 20 provides a full overview of the language\ndatasets that were used in the training of KOSMOS-1 model. These data sources can be divided into\nthe following three categories:\n\u2022 Academic: NIH Exporter\n\u2022 Internet: Pile-CC, OpenWebText2, Wikipedia (English), CC-2020-50, CC-2021-04, Realnews\n\u2022 Prose: BookCorpus2, Books3, Gutenberg [RPJ+20], CC-Stories\n                 Datasets               Tokens (billion)    Weight (%)      Epochs\n                 OpenWebText2                 14.8             21.8%          1.47\n                 CC-2021-04                   82.6             17.7%          0.21\n                 Books3                       25.7             16.2%          0.63\n                 CC-2020-50                   68.7             14.7%          0.21\n                 Pile-CC                      49.8             10.6%          0.21\n                 Realnews                     21.9             10.2%          0.46\n                 Wikipedia                     4.2              5.4%          1.29\n                 BookCorpus2                   1.5              1.1%          0.75\n                 Gutenberg (PG-19)             2.7              1.0%          0.38\n                 CC-Stories                    5.3              1.0%          0.19\n                 NIH ExPorter                  0.3              0.2%          0.75\n                  Table 20: Language datasets used to train the KOSMOS-1 model.\nB.1.2   Image-Caption Pairs\nKOSMOS-1 is trained on image-caption pairs constructed from several datasets, including English\nLAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22] and Conceptual Cap-\ntions [SDGS18, CSDS21]. LAION-2B, LAION-400M, and COYO-700M datasets are extracted by\nparsing out image URLs and alt-texts of web pages from the Common Crawl web data. LAION-2B\ncontains about 2B English image-caption pairs, LAION-400M consists of 400M English image-\ncaption pairs, and COYO-700M has 700M English image-caption pairs. Conceptual Captions contains\n15M English image-caption pairs and consists of two datasets: CC3M and CC12M, which are also\ncollected from internet webpages using a Flume pipeline. For Conceptual Captions, we discard pairs\nwhose captions contain special tags such as \u201c<PERSON>\u201d.\nB.1.3   Interleaved Data\nWe collect a large corpus of 2 billion web pages from the snapshots of common crawls. To ensure\nquality and relevance, we apply several filtering criteria. First, we discard any pages that are not\nwritten in English. Second, we discard any pages that do not have images interspersed in the text.\nThird, we discard any images that have a resolution lower than 64 by 64 pixels or that are single-\ncolored. Fourth, we discard any text that is not meaningful or coherent, such as spam or gibberish.\nWe use some heuristics to identify and remove gibberish text containing emoji symbols, hashtags,\nand URL links. After applying these filters, we end up with about 71 million documents for training.\nB.2   Data Format\nThe training data is organized in the format as follows:\n                                                  24", "md": "```markdown\nB    Datasets\nB.1   Pretraining\nB.1.1   Text Corpora\nKOSMOS-1 is trained on The Pile [GBB+20] and Common Crawl. The Pile is an 800 GB English\ntext corpus combining 22 diverse sources. We select a subset with seven sources from The Pile.\nCommon Crawl is also included in training corpora. Common Crawl takes snapshots of the web,\nwhich contains massive amounts of language data. Table 20 provides a full overview of the language\ndatasets that were used in the training of KOSMOS-1 model. These data sources can be divided into\nthe following three categories:\n\u2022 Academic: NIH Exporter\n\u2022 Internet: Pile-CC, OpenWebText2, Wikipedia (English), CC-2020-50, CC-2021-04, Realnews\n\u2022 Prose: BookCorpus2, Books3, Gutenberg [RPJ+20], CC-Stories\n\n| Datasets       | Tokens (billion) | Weight (%) | Epochs |\n|----------------|------------------|------------|--------|\n| OpenWebText2   | 14.8             | 21.8%      | 1.47   |\n| CC-2021-04     | 82.6             | 17.7%      | 0.21   |\n| Books3         | 25.7             | 16.2%      | 0.63   |\n| CC-2020-50     | 68.7             | 14.7%      | 0.21   |\n| Pile-CC        | 49.8             | 10.6%      | 0.21   |\n| Realnews       | 21.9             | 10.2%      | 0.46   |\n| Wikipedia      | 4.2              | 5.4%       | 1.29   |\n| BookCorpus2    | 1.5              | 1.1%       | 0.75   |\n| Gutenberg (PG-19) | 2.7           | 1.0%       | 0.38   |\n| CC-Stories     | 5.3              | 1.0%       | 0.19   |\n| NIH ExPorter   | 0.3              | 0.2%       | 0.75   |\n\nTable 20: Language datasets used to train the KOSMOS-1 model.\n\nB.1.2   Image-Caption Pairs\nKOSMOS-1 is trained on image-caption pairs constructed from several datasets, including English\nLAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22] and Conceptual Captions [SDGS18, CSDS21]. LAION-2B, LAION-400M, and COYO-700M datasets are extracted by\nparsing out image URLs and alt-texts of web pages from the Common Crawl web data. LAION-2B\ncontains about 2B English image-caption pairs, LAION-400M consists of 400M English image-\ncaption pairs, and COYO-700M has 700M English image-caption pairs. Conceptual Captions contains\n15M English image-caption pairs and consists of two datasets: CC3M and CC12M, which are also\ncollected from internet webpages using a Flume pipeline. For Conceptual Captions, we discard pairs\nwhose captions contain special tags such as \u201c<PERSON>\u201d.\n\nB.1.3   Interleaved Data\nWe collect a large corpus of 2 billion web pages from the snapshots of common crawls. To ensure\nquality and relevance, we apply several filtering criteria. First, we discard any pages that are not\nwritten in English. Second, we discard any pages that do not have images interspersed in the text.\nThird, we discard any images that have a resolution lower than 64 by 64 pixels or that are single-\ncolored. Fourth, we discard any text that is not meaningful or coherent, such as spam or gibberish.\nWe use some heuristics to identify and remove gibberish text containing emoji symbols, hashtags,\nand URL links. After applying these filters, we end up with about 71 million documents for training.\n\nB.2   Data Format\nThe training data is organized in the format as follows:\n$$\n24\n$$\n```", "images": [], "items": [{"type": "text", "value": "```markdown\nB    Datasets\nB.1   Pretraining\nB.1.1   Text Corpora\nKOSMOS-1 is trained on The Pile [GBB+20] and Common Crawl. The Pile is an 800 GB English\ntext corpus combining 22 diverse sources. We select a subset with seven sources from The Pile.\nCommon Crawl is also included in training corpora. Common Crawl takes snapshots of the web,\nwhich contains massive amounts of language data. Table 20 provides a full overview of the language\ndatasets that were used in the training of KOSMOS-1 model. These data sources can be divided into\nthe following three categories:\n\u2022 Academic: NIH Exporter\n\u2022 Internet: Pile-CC, OpenWebText2, Wikipedia (English), CC-2020-50, CC-2021-04, Realnews\n\u2022 Prose: BookCorpus2, Books3, Gutenberg [RPJ+20], CC-Stories", "md": "```markdown\nB    Datasets\nB.1   Pretraining\nB.1.1   Text Corpora\nKOSMOS-1 is trained on The Pile [GBB+20] and Common Crawl. The Pile is an 800 GB English\ntext corpus combining 22 diverse sources. We select a subset with seven sources from The Pile.\nCommon Crawl is also included in training corpora. Common Crawl takes snapshots of the web,\nwhich contains massive amounts of language data. Table 20 provides a full overview of the language\ndatasets that were used in the training of KOSMOS-1 model. These data sources can be divided into\nthe following three categories:\n\u2022 Academic: NIH Exporter\n\u2022 Internet: Pile-CC, OpenWebText2, Wikipedia (English), CC-2020-50, CC-2021-04, Realnews\n\u2022 Prose: BookCorpus2, Books3, Gutenberg [RPJ+20], CC-Stories"}, {"type": "table", "rows": [["Datasets", "Tokens (billion)", "Weight (%)", "Epochs"], ["OpenWebText2", "14.8", "21.8%", "1.47"], ["CC-2021-04", "82.6", "17.7%", "0.21"], ["Books3", "25.7", "16.2%", "0.63"], ["CC-2020-50", "68.7", "14.7%", "0.21"], ["Pile-CC", "49.8", "10.6%", "0.21"], ["Realnews", "21.9", "10.2%", "0.46"], ["Wikipedia", "4.2", "5.4%", "1.29"], ["BookCorpus2", "1.5", "1.1%", "0.75"], ["Gutenberg (PG-19)", "2.7", "1.0%", "0.38"], ["CC-Stories", "5.3", "1.0%", "0.19"], ["NIH ExPorter", "0.3", "0.2%", "0.75"]], "md": "| Datasets       | Tokens (billion) | Weight (%) | Epochs |\n|----------------|------------------|------------|--------|\n| OpenWebText2   | 14.8             | 21.8%      | 1.47   |\n| CC-2021-04     | 82.6             | 17.7%      | 0.21   |\n| Books3         | 25.7             | 16.2%      | 0.63   |\n| CC-2020-50     | 68.7             | 14.7%      | 0.21   |\n| Pile-CC        | 49.8             | 10.6%      | 0.21   |\n| Realnews       | 21.9             | 10.2%      | 0.46   |\n| Wikipedia      | 4.2              | 5.4%       | 1.29   |\n| BookCorpus2    | 1.5              | 1.1%       | 0.75   |\n| Gutenberg (PG-19) | 2.7           | 1.0%       | 0.38   |\n| CC-Stories     | 5.3              | 1.0%       | 0.19   |\n| NIH ExPorter   | 0.3              | 0.2%       | 0.75   |", "isPerfectTable": true, "csv": "\"Datasets\",\"Tokens (billion)\",\"Weight (%)\",\"Epochs\"\n\"OpenWebText2\",\"14.8\",\"21.8%\",\"1.47\"\n\"CC-2021-04\",\"82.6\",\"17.7%\",\"0.21\"\n\"Books3\",\"25.7\",\"16.2%\",\"0.63\"\n\"CC-2020-50\",\"68.7\",\"14.7%\",\"0.21\"\n\"Pile-CC\",\"49.8\",\"10.6%\",\"0.21\"\n\"Realnews\",\"21.9\",\"10.2%\",\"0.46\"\n\"Wikipedia\",\"4.2\",\"5.4%\",\"1.29\"\n\"BookCorpus2\",\"1.5\",\"1.1%\",\"0.75\"\n\"Gutenberg (PG-19)\",\"2.7\",\"1.0%\",\"0.38\"\n\"CC-Stories\",\"5.3\",\"1.0%\",\"0.19\"\n\"NIH ExPorter\",\"0.3\",\"0.2%\",\"0.75\""}, {"type": "text", "value": "Table 20: Language datasets used to train the KOSMOS-1 model.\n\nB.1.2   Image-Caption Pairs\nKOSMOS-1 is trained on image-caption pairs constructed from several datasets, including English\nLAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22] and Conceptual Captions [SDGS18, CSDS21]. LAION-2B, LAION-400M, and COYO-700M datasets are extracted by\nparsing out image URLs and alt-texts of web pages from the Common Crawl web data. LAION-2B\ncontains about 2B English image-caption pairs, LAION-400M consists of 400M English image-\ncaption pairs, and COYO-700M has 700M English image-caption pairs. Conceptual Captions contains\n15M English image-caption pairs and consists of two datasets: CC3M and CC12M, which are also\ncollected from internet webpages using a Flume pipeline. For Conceptual Captions, we discard pairs\nwhose captions contain special tags such as \u201c<PERSON>\u201d.\n\nB.1.3   Interleaved Data\nWe collect a large corpus of 2 billion web pages from the snapshots of common crawls. To ensure\nquality and relevance, we apply several filtering criteria. First, we discard any pages that are not\nwritten in English. Second, we discard any pages that do not have images interspersed in the text.\nThird, we discard any images that have a resolution lower than 64 by 64 pixels or that are single-\ncolored. Fourth, we discard any text that is not meaningful or coherent, such as spam or gibberish.\nWe use some heuristics to identify and remove gibberish text containing emoji symbols, hashtags,\nand URL links. After applying these filters, we end up with about 71 million documents for training.\n\nB.2   Data Format\nThe training data is organized in the format as follows:\n$$\n24\n$$\n```", "md": "Table 20: Language datasets used to train the KOSMOS-1 model.\n\nB.1.2   Image-Caption Pairs\nKOSMOS-1 is trained on image-caption pairs constructed from several datasets, including English\nLAION-2B [SBV+22], LAION-400M [SVB+21], COYO-700M [BPK+22] and Conceptual Captions [SDGS18, CSDS21]. LAION-2B, LAION-400M, and COYO-700M datasets are extracted by\nparsing out image URLs and alt-texts of web pages from the Common Crawl web data. LAION-2B\ncontains about 2B English image-caption pairs, LAION-400M consists of 400M English image-\ncaption pairs, and COYO-700M has 700M English image-caption pairs. Conceptual Captions contains\n15M English image-caption pairs and consists of two datasets: CC3M and CC12M, which are also\ncollected from internet webpages using a Flume pipeline. For Conceptual Captions, we discard pairs\nwhose captions contain special tags such as \u201c<PERSON>\u201d.\n\nB.1.3   Interleaved Data\nWe collect a large corpus of 2 billion web pages from the snapshots of common crawls. To ensure\nquality and relevance, we apply several filtering criteria. First, we discard any pages that are not\nwritten in English. Second, we discard any pages that do not have images interspersed in the text.\nThird, we discard any images that have a resolution lower than 64 by 64 pixels or that are single-\ncolored. Fourth, we discard any text that is not meaningful or coherent, such as spam or gibberish.\nWe use some heuristics to identify and remove gibberish text containing emoji symbols, hashtags,\nand URL links. After applying these filters, we end up with about 71 million documents for training.\n\nB.2   Data Format\nThe training data is organized in the format as follows:\n$$\n24\n$$\n```"}]}, {"page": 25, "text": "    Datasets                     Format Examples\n    Text                         <s> KOSMOS-1 can perceive multimodal input, learn in context, and gener-\n                                 ate output. </s>\n    Image-Caption                <s> <image> Image Embedding </image> WALL-E giving potted plant to\n                                 EVE. </s>\n    Multimodal                   <s> <image> Image Embedding </image> This is WALL-E. <image>\n                                 Image Embedding </image> This is EVE. </s>\n                    Table 21: The examples of the data format to train the KOSMOS-1 model.\nC       Evaluation\nC.1       Input Format Used for Perception-Language Tasks\nFigure 7 shows how we conduct zero-shot and few-shot evaluations on perception-language tasks.\n                                                          WALL-E          giving          potted        plant     to            EVE\n                                               Multimodal Large Language Model (MLLM)\n                                               An   image    of           WALL-E        giving        potted   plant           to\n                     Embedding\n                         Image\n                                                           (a) Zero-shot learning\n                                                                                                                     Rubik's Cube\n                                               Multimodal Large Language Model (MLLM)\n                                                  Question: what did                                            Question: What's\n                                                  WALL-E give EVE?                                              in WALL-E\u2019s\n                      Embedding                  Answer: potted plant                     Embedding             hand? Answer:\n                         Image                                                                Image\n                                                           (b) Few-shot learning\nFigure 7: We evaluate KOSMOS-1 on the perception-language tasks in zero- and few-shot settings. (a)\nZero-shot learning, e.g., zero-shot image captioning with language prompts. (b) Few-shot learning,\ne.g., visual question answering with in-context learning.\n                                                                           25", "md": "# Evaluation\n\n## Datasets\n\n|Format|Examples|\n|---|---|\n|Text| KOSMOS-1 can perceive multimodal input, learn in context, and generate output. |\n|Image-Caption| <image> Image Embedding </image> WALL-E giving potted plant to EVE. |\n|Multimodal| <image> Image Embedding </image> This is WALL-E. <image> Image Embedding </image> This is EVE. |\n\nTable 21: The examples of the data format to train the KOSMOS-1 model.\n\n## Evaluation\n\n### Input Format Used for Perception-Language Tasks\n\nFigure 7 shows how we conduct zero-shot and few-shot evaluations on perception-language tasks.\n\n#### WALL-E giving potted plant to EVE\n\nMultimodal Large Language Model (MLLM)\n\n- An image of WALL-E giving potted plant to EVE\n- Embedding Image\n\n(a) Zero-shot learning\n\nMultimodal Large Language Model (MLLM)\n\n- Question: what did WALL-E give EVE?\n- Answer: potted plant\n- Embedding Image\n\n(b) Few-shot learning\n\nMultimodal Large Language Model (MLLM)\n\n- Question: What's in WALL-E\u2019s hand?\n- Answer: Rubik's Cube\n- Embedding Image\n\nFigure 7: We evaluate KOSMOS-1 on the perception-language tasks in zero- and few-shot settings. (a) Zero-shot learning, e.g., zero-shot image captioning with language prompts. (b) Few-shot learning, e.g., visual question answering with in-context learning.\n\n25", "images": [{"name": "page-25-0.jpg", "height": 22, "width": 22, "x": 207, "y": 322}, {"name": "page-25-4.jpg", "height": 47, "width": 60, "x": 330, "y": 585}, {"name": "page-25-1.jpg", "height": 47, "width": 69, "x": 146, "y": 403}, {"name": "page-25-3.jpg", "height": 47, "width": 69, "x": 147, "y": 587}, {"name": "page-25-2.jpg", "height": 22, "width": 22, "x": 207, "y": 506}], "items": [{"type": "heading", "lvl": 1, "value": "Evaluation", "md": "# Evaluation"}, {"type": "heading", "lvl": 2, "value": "Datasets", "md": "## Datasets"}, {"type": "table", "rows": [["Format", "Examples"], ["Text", "KOSMOS-1 can perceive multimodal input, learn in context, and generate output."], ["Image-Caption", "<image> Image Embedding </image> WALL-E giving potted plant to EVE."], ["Multimodal", "<image> Image Embedding </image> This is WALL-E. <image> Image Embedding </image> This is EVE."]], "md": "|Format|Examples|\n|---|---|\n|Text| KOSMOS-1 can perceive multimodal input, learn in context, and generate output. |\n|Image-Caption| <image> Image Embedding </image> WALL-E giving potted plant to EVE. |\n|Multimodal| <image> Image Embedding </image> This is WALL-E. <image> Image Embedding </image> This is EVE. |", "isPerfectTable": true, "csv": "\"Format\",\"Examples\"\n\"Text\",\"KOSMOS-1 can perceive multimodal input, learn in context, and generate output.\"\n\"Image-Caption\",\"<image> Image Embedding </image> WALL-E giving potted plant to EVE.\"\n\"Multimodal\",\"<image> Image Embedding </image> This is WALL-E. <image> Image Embedding </image> This is EVE.\""}, {"type": "text", "value": "Table 21: The examples of the data format to train the KOSMOS-1 model.", "md": "Table 21: The examples of the data format to train the KOSMOS-1 model."}, {"type": "heading", "lvl": 2, "value": "Evaluation", "md": "## Evaluation"}, {"type": "heading", "lvl": 3, "value": "Input Format Used for Perception-Language Tasks", "md": "### Input Format Used for Perception-Language Tasks"}, {"type": "text", "value": "Figure 7 shows how we conduct zero-shot and few-shot evaluations on perception-language tasks.", "md": "Figure 7 shows how we conduct zero-shot and few-shot evaluations on perception-language tasks."}, {"type": "heading", "lvl": 4, "value": "WALL-E giving potted plant to EVE", "md": "#### WALL-E giving potted plant to EVE"}, {"type": "text", "value": "Multimodal Large Language Model (MLLM)\n\n- An image of WALL-E giving potted plant to EVE\n- Embedding Image\n\n(a) Zero-shot learning\n\nMultimodal Large Language Model (MLLM)\n\n- Question: what did WALL-E give EVE?\n- Answer: potted plant\n- Embedding Image\n\n(b) Few-shot learning\n\nMultimodal Large Language Model (MLLM)\n\n- Question: What's in WALL-E\u2019s hand?\n- Answer: Rubik's Cube\n- Embedding Image\n\nFigure 7: We evaluate KOSMOS-1 on the perception-language tasks in zero- and few-shot settings. (a) Zero-shot learning, e.g., zero-shot image captioning with language prompts. (b) Few-shot learning, e.g., visual question answering with in-context learning.\n\n25", "md": "Multimodal Large Language Model (MLLM)\n\n- An image of WALL-E giving potted plant to EVE\n- Embedding Image\n\n(a) Zero-shot learning\n\nMultimodal Large Language Model (MLLM)\n\n- Question: what did WALL-E give EVE?\n- Answer: potted plant\n- Embedding Image\n\n(b) Few-shot learning\n\nMultimodal Large Language Model (MLLM)\n\n- Question: What's in WALL-E\u2019s hand?\n- Answer: Rubik's Cube\n- Embedding Image\n\nFigure 7: We evaluate KOSMOS-1 on the perception-language tasks in zero- and few-shot settings. (a) Zero-shot learning, e.g., zero-shot image captioning with language prompts. (b) Few-shot learning, e.g., visual question answering with in-context learning.\n\n25"}]}, {"page": 26, "text": "C.2    Language Tasks\nWe conduct experiments on language tasks in four categories:\n\u2022 Cloze and completion tasks: StoryCloze [MRL+17], HellaSwag [ZHB+19]\n\u2022 Winograd-style tasks: Winograd [LDM12b], Winogrande [SBBC20]\n\u2022 Commonsense reasoning: PIQA [BZB+20]\n\u2022 Three datasets from SuperGLUE benchmark [WPN+19]: BoolQ [CLC+19], CB [dMST19],\n   COPA [RBG11]\nC.3    WebSRC Task Examples\n                                              NEY, RELEA 5ES\n(a) Question is \u201cWhat is the type\nof this drive?\u201d                                       (b) Question is \u201cWho is the author of \"Cicada\"?\u201d\n                               Figure 8: Examples form WebSRC [CZC+21].\n                                                         26", "md": "## C.2 Language Tasks\n\nWe conduct experiments on language tasks in four categories:\n\n- Cloze and completion tasks: StoryCloze [MRL+17], HellaSwag [ZHB+19]\n- Winograd-style tasks: Winograd [LDM12b], Winogrande [SBBC20]\n- Commonsense reasoning: PIQA [BZB+20]\n- Three datasets from SuperGLUE benchmark [WPN+19]: BoolQ [CLC+19], CB [dMST19], COPA [RBG11]\n\n## C.3 WebSRC Task Examples\n\n(a) Question is \u201cWhat is the type of this drive?\u201d\n(b) Question is \u201cWho is the author of \"Cicada\"?\u201d\n\nFigure 8: Examples from WebSRC [CZC+21].", "images": [{"name": "page-26-0.jpg", "height": 93, "width": 119, "x": 108, "y": 219}, {"name": "page-26-1.jpg", "height": 89, "width": 238, "x": 263, "y": 234}], "items": [{"type": "heading", "lvl": 2, "value": "C.2 Language Tasks", "md": "## C.2 Language Tasks"}, {"type": "text", "value": "We conduct experiments on language tasks in four categories:\n\n- Cloze and completion tasks: StoryCloze [MRL+17], HellaSwag [ZHB+19]\n- Winograd-style tasks: Winograd [LDM12b], Winogrande [SBBC20]\n- Commonsense reasoning: PIQA [BZB+20]\n- Three datasets from SuperGLUE benchmark [WPN+19]: BoolQ [CLC+19], CB [dMST19], COPA [RBG11]", "md": "We conduct experiments on language tasks in four categories:\n\n- Cloze and completion tasks: StoryCloze [MRL+17], HellaSwag [ZHB+19]\n- Winograd-style tasks: Winograd [LDM12b], Winogrande [SBBC20]\n- Commonsense reasoning: PIQA [BZB+20]\n- Three datasets from SuperGLUE benchmark [WPN+19]: BoolQ [CLC+19], CB [dMST19], COPA [RBG11]"}, {"type": "heading", "lvl": 2, "value": "C.3 WebSRC Task Examples", "md": "## C.3 WebSRC Task Examples"}, {"type": "text", "value": "(a) Question is \u201cWhat is the type of this drive?\u201d\n(b) Question is \u201cWho is the author of \"Cicada\"?\u201d\n\nFigure 8: Examples from WebSRC [CZC+21].", "md": "(a) Question is \u201cWhat is the type of this drive?\u201d\n(b) Question is \u201cWho is the author of \"Cicada\"?\u201d\n\nFigure 8: Examples from WebSRC [CZC+21]."}]}], "job_id": "cfa82136-4452-4105-ae51-b4bf9836173a", "file_path": "./corpus/2302.14045.pdf"}