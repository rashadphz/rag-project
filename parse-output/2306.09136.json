{"pages": [{"page": 1, "text": "                    Finite-Time Logarithmic Bayes Regret Upper Bounds\n                                  Alexia Atsidakou                  Branislav Kveton             Sumeet Katariya\n                             University of Texas, Austin              AWS AI Labs\u2217                     Amazon\n                                 Constantine Caramanis                               Sujay Sanghavi\n                                University of Texas, Austin               University of Texas, Austin / Amazon\narXiv:2306.09136v3  [cs.LG]  22 Jan 2024\n                                                                   Abstract\n                              We derive the first finite-time logarithmic Bayes regret upper bounds for Bayesian\n                              bandits. In a multi-armed bandit, we obtain O(c\u2206     log n) and O(ch log2 n) upper\n                              bounds for an upper confidence bound algorithm, where ch and c\u2206         are constants\n                              depending on the prior distribution and the gaps of bandit instances sampled from\n                              it, respectively. The latter bound asymptotically matches the lower bound of Lai\n                              (1987). Our proofs are a major technical departure from prior works, while being\n                              simple and general. To show the generality of our techniques, we apply them to\n                              linear bandits. Our results provide insights on the value of prior in the Bayesian\n                              setting, both in the objective and as a side information given to the learner. They\n                              significantly improve upon existing \u02dc O(\u221an) bounds, which have become standard\n                              in the literature despite the logarithmic lower bound of Lai (1987).\n                    1    Introduction\n                    A stochastic multi-armed bandit [Lai and Robbins, 1985, Auer et al., 2002, Lattimore and Szepesvari,\n                    2019] is an online learning problem where a learner sequentially interacts with an environment over\n                    n rounds. In each round, the learner takes an action and receives its stochastic reward. The goal of\n                    the learner is to maximize its expected cumulative reward over n rounds. The mean rewards of the\n                    actions are unknown a priori but can be learned by taking the actions. Therefore, the learner faces\n                    the exploration-exploitation dilemma: explore, and learn more about the actions; or exploit, and take\n                    the action with the highest estimated reward. Bandits have been successfully applied to problems\n                    where uncertainty modeling and subsequent adaptation are beneficial. One example are recommender\n                    systems [Li et al., 2010, Zhao et al., 2013, Kawale et al., 2015, Li et al., 2016], where the actions are\n                    recommended items and their rewards are clicks. Another example is hyper-parameter optimization\n                    [Li et al., 2018], where the actions are values of the optimized parameters and their reward is the\n                    optimized metric.\n                    Cumulative regret minimization in stochastic bandits has been traditionally studied in two settings:\n                    frequentist [Lai and Robbins, 1985, Auer et al., 2002, Abbasi-Yadkori et al., 2011] and Bayesian\n                    [Gittins, 1979, Tsitsiklis, 1994, Lai, 1987, Russo and Van Roy, 2014, Russo et al., 2018]. In the\n                    frequentist setting, the learner minimizes the regret with respect to a fixed unknown bandit instance.\n                    In the Bayesian setting, the learner minimizes the average regret with respect to bandit instances\n                    drawn from a prior distribution. The instance is unknown but the learner knows its prior distribution.\n                    The Bayesian setting allows surprisingly simple and insightful analyses of Thompson sampling. One\n                    fundamental result in this setting is that linear Thompson sampling [Russo and Van Roy, 2014] has a\n                    comparable regret bound to LinUCB in the frequentist setting [Abbasi-Yadkori et al., 2011, Agrawal\n                    and Goyal, 2013, Abeille and Lazaric, 2017]. Moreover, many recent meta- and multi-task bandit\n                    works [Bastani et al., 2019, Kveton et al., 2021, Basu et al., 2021, Simchowitz et al., 2021, Wang\n                    et al., 2021, Hong et al., 2022, Aouali et al., 2023] adopt the Bayes regret to analyze the stochastic\n                    37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "# Finite-Time Logarithmic Bayes Regret Upper Bounds\n\n# Finite-Time Logarithmic Bayes Regret Upper Bounds\n\nAlexia Atsidakou, University of Texas, Austin\n\nBranislav Kveton, AWS AI Labs*\n\nSumeet Katariya, Amazon\n\nConstantine Caramanis, University of Texas, Austin\n\nSujay Sanghavi, University of Texas, Austin / Amazon\n\narXiv:2306.09136v3 [cs.LG] 22 Jan 2024\n\n## Abstract\n\nWe derive the first finite-time logarithmic Bayes regret upper bounds for Bayesian bandits. In a multi-armed bandit, we obtain $$O(c\\Delta \\log n)$$ and $$O(ch \\log^2 n)$$ upper bounds for an upper confidence bound algorithm, where $$ch$$ and $$c\\Delta$$ are constants depending on the prior distribution and the gaps of bandit instances sampled from it, respectively. The latter bound asymptotically matches the lower bound of Lai (1987). Our proofs are a major technical departure from prior works, while being simple and general. To show the generality of our techniques, we apply them to linear bandits. Our results provide insights on the value of prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve upon existing $$\\tilde{O}(\\sqrt{n})$$ bounds, which have become standard in the literature despite the logarithmic lower bound of Lai (1987).\n\n## Introduction\n\nA stochastic multi-armed bandit [Lai and Robbins, 1985, Auer et al., 2002, Lattimore and Szepesvari, 2019] is an online learning problem where a learner sequentially interacts with an environment over n rounds. In each round, the learner takes an action and receives its stochastic reward. The goal of the learner is to maximize its expected cumulative reward over n rounds. The mean rewards of the actions are unknown a priori but can be learned by taking the actions. Therefore, the learner faces the exploration-exploitation dilemma: explore, and learn more about the actions; or exploit, and take the action with the highest estimated reward. Bandits have been successfully applied to problems where uncertainty modeling and subsequent adaptation are beneficial. One example are recommender systems [Li et al., 2010, Zhao et al., 2013, Kawale et al., 2015, Li et al., 2016], where the actions are recommended items and their rewards are clicks. Another example is hyper-parameter optimization [Li et al., 2018], where the actions are values of the optimized parameters and their reward is the optimized metric.\n\nCumulative regret minimization in stochastic bandits has been traditionally studied in two settings: frequentist [Lai and Robbins, 1985, Auer et al., 2002, Abbasi-Yadkori et al., 2011] and Bayesian [Gittins, 1979, Tsitsiklis, 1994, Lai, 1987, Russo and Van Roy, 2014, Russo et al., 2018]. In the frequentist setting, the learner minimizes the regret with respect to a fixed unknown bandit instance. In the Bayesian setting, the learner minimizes the average regret with respect to bandit instances drawn from a prior distribution. The instance is unknown but the learner knows its prior distribution. The Bayesian setting allows surprisingly simple and insightful analyses of Thompson sampling. One fundamental result in this setting is that linear Thompson sampling [Russo and Van Roy, 2014] has a comparable regret bound to LinUCB in the frequentist setting [Abbasi-Yadkori et al., 2011, Agrawal and Goyal, 2013, Abeille and Lazaric, 2017]. Moreover, many recent meta- and multi-task bandit works [Bastani et al., 2019, Kveton et al., 2021, Basu et al., 2021, Simchowitz et al., 2021, Wang et al., 2021, Hong et al., 2022, Aouali et al., 2023] adopt the Bayes regret to analyze the stochastic 37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Finite-Time Logarithmic Bayes Regret Upper Bounds", "md": "# Finite-Time Logarithmic Bayes Regret Upper Bounds"}, {"type": "heading", "lvl": 1, "value": "Finite-Time Logarithmic Bayes Regret Upper Bounds", "md": "# Finite-Time Logarithmic Bayes Regret Upper Bounds"}, {"type": "text", "value": "Alexia Atsidakou, University of Texas, Austin\n\nBranislav Kveton, AWS AI Labs*\n\nSumeet Katariya, Amazon\n\nConstantine Caramanis, University of Texas, Austin\n\nSujay Sanghavi, University of Texas, Austin / Amazon\n\narXiv:2306.09136v3 [cs.LG] 22 Jan 2024", "md": "Alexia Atsidakou, University of Texas, Austin\n\nBranislav Kveton, AWS AI Labs*\n\nSumeet Katariya, Amazon\n\nConstantine Caramanis, University of Texas, Austin\n\nSujay Sanghavi, University of Texas, Austin / Amazon\n\narXiv:2306.09136v3 [cs.LG] 22 Jan 2024"}, {"type": "heading", "lvl": 2, "value": "Abstract", "md": "## Abstract"}, {"type": "text", "value": "We derive the first finite-time logarithmic Bayes regret upper bounds for Bayesian bandits. In a multi-armed bandit, we obtain $$O(c\\Delta \\log n)$$ and $$O(ch \\log^2 n)$$ upper bounds for an upper confidence bound algorithm, where $$ch$$ and $$c\\Delta$$ are constants depending on the prior distribution and the gaps of bandit instances sampled from it, respectively. The latter bound asymptotically matches the lower bound of Lai (1987). Our proofs are a major technical departure from prior works, while being simple and general. To show the generality of our techniques, we apply them to linear bandits. Our results provide insights on the value of prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve upon existing $$\\tilde{O}(\\sqrt{n})$$ bounds, which have become standard in the literature despite the logarithmic lower bound of Lai (1987).", "md": "We derive the first finite-time logarithmic Bayes regret upper bounds for Bayesian bandits. In a multi-armed bandit, we obtain $$O(c\\Delta \\log n)$$ and $$O(ch \\log^2 n)$$ upper bounds for an upper confidence bound algorithm, where $$ch$$ and $$c\\Delta$$ are constants depending on the prior distribution and the gaps of bandit instances sampled from it, respectively. The latter bound asymptotically matches the lower bound of Lai (1987). Our proofs are a major technical departure from prior works, while being simple and general. To show the generality of our techniques, we apply them to linear bandits. Our results provide insights on the value of prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve upon existing $$\\tilde{O}(\\sqrt{n})$$ bounds, which have become standard in the literature despite the logarithmic lower bound of Lai (1987)."}, {"type": "heading", "lvl": 2, "value": "Introduction", "md": "## Introduction"}, {"type": "text", "value": "A stochastic multi-armed bandit [Lai and Robbins, 1985, Auer et al., 2002, Lattimore and Szepesvari, 2019] is an online learning problem where a learner sequentially interacts with an environment over n rounds. In each round, the learner takes an action and receives its stochastic reward. The goal of the learner is to maximize its expected cumulative reward over n rounds. The mean rewards of the actions are unknown a priori but can be learned by taking the actions. Therefore, the learner faces the exploration-exploitation dilemma: explore, and learn more about the actions; or exploit, and take the action with the highest estimated reward. Bandits have been successfully applied to problems where uncertainty modeling and subsequent adaptation are beneficial. One example are recommender systems [Li et al., 2010, Zhao et al., 2013, Kawale et al., 2015, Li et al., 2016], where the actions are recommended items and their rewards are clicks. Another example is hyper-parameter optimization [Li et al., 2018], where the actions are values of the optimized parameters and their reward is the optimized metric.\n\nCumulative regret minimization in stochastic bandits has been traditionally studied in two settings: frequentist [Lai and Robbins, 1985, Auer et al., 2002, Abbasi-Yadkori et al., 2011] and Bayesian [Gittins, 1979, Tsitsiklis, 1994, Lai, 1987, Russo and Van Roy, 2014, Russo et al., 2018]. In the frequentist setting, the learner minimizes the regret with respect to a fixed unknown bandit instance. In the Bayesian setting, the learner minimizes the average regret with respect to bandit instances drawn from a prior distribution. The instance is unknown but the learner knows its prior distribution. The Bayesian setting allows surprisingly simple and insightful analyses of Thompson sampling. One fundamental result in this setting is that linear Thompson sampling [Russo and Van Roy, 2014] has a comparable regret bound to LinUCB in the frequentist setting [Abbasi-Yadkori et al., 2011, Agrawal and Goyal, 2013, Abeille and Lazaric, 2017]. Moreover, many recent meta- and multi-task bandit works [Bastani et al., 2019, Kveton et al., 2021, Basu et al., 2021, Simchowitz et al., 2021, Wang et al., 2021, Hong et al., 2022, Aouali et al., 2023] adopt the Bayes regret to analyze the stochastic 37th Conference on Neural Information Processing Systems (NeurIPS 2023).", "md": "A stochastic multi-armed bandit [Lai and Robbins, 1985, Auer et al., 2002, Lattimore and Szepesvari, 2019] is an online learning problem where a learner sequentially interacts with an environment over n rounds. In each round, the learner takes an action and receives its stochastic reward. The goal of the learner is to maximize its expected cumulative reward over n rounds. The mean rewards of the actions are unknown a priori but can be learned by taking the actions. Therefore, the learner faces the exploration-exploitation dilemma: explore, and learn more about the actions; or exploit, and take the action with the highest estimated reward. Bandits have been successfully applied to problems where uncertainty modeling and subsequent adaptation are beneficial. One example are recommender systems [Li et al., 2010, Zhao et al., 2013, Kawale et al., 2015, Li et al., 2016], where the actions are recommended items and their rewards are clicks. Another example is hyper-parameter optimization [Li et al., 2018], where the actions are values of the optimized parameters and their reward is the optimized metric.\n\nCumulative regret minimization in stochastic bandits has been traditionally studied in two settings: frequentist [Lai and Robbins, 1985, Auer et al., 2002, Abbasi-Yadkori et al., 2011] and Bayesian [Gittins, 1979, Tsitsiklis, 1994, Lai, 1987, Russo and Van Roy, 2014, Russo et al., 2018]. In the frequentist setting, the learner minimizes the regret with respect to a fixed unknown bandit instance. In the Bayesian setting, the learner minimizes the average regret with respect to bandit instances drawn from a prior distribution. The instance is unknown but the learner knows its prior distribution. The Bayesian setting allows surprisingly simple and insightful analyses of Thompson sampling. One fundamental result in this setting is that linear Thompson sampling [Russo and Van Roy, 2014] has a comparable regret bound to LinUCB in the frequentist setting [Abbasi-Yadkori et al., 2011, Agrawal and Goyal, 2013, Abeille and Lazaric, 2017]. Moreover, many recent meta- and multi-task bandit works [Bastani et al., 2019, Kveton et al., 2021, Basu et al., 2021, Simchowitz et al., 2021, Wang et al., 2021, Hong et al., 2022, Aouali et al., 2023] adopt the Bayes regret to analyze the stochastic 37th Conference on Neural Information Processing Systems (NeurIPS 2023)."}]}, {"page": 2, "text": "structure of their problems, that the bandit tasks are similar because their parameters are sampled\ni.i.d. from a task distribution.\nMany bandit algorithms have frequentist regret bounds that match a lower bound. As an example,\nin a K-armed bandit with the minimum gap \u2206           and horizon n, the gap-dependent O(K\u2206\u22121 log n)\nregret bound of UCB1 [Auer et al., 2002] matches the gap-dependent \u2126(K\u2206\u22121 log n) lower bound\nof Lai and Robbins [1985]. Moreover, the gap-free \u02dc       O( \u221a Kn) regret bound of UCB1 matches, up to\nlogarithmic factors, the gap-free \u2126(   \u221a  Kn) lower bound of Auer et al. [1995]. The extra logarithmic\nfactor in the \u02dc  \u221a\n              O(   Kn) bound can be eliminated by modifying UCB1 [Audibert and Bubeck, 2009]. In\ncontrast, and despite the popularity of the model, matching upper and lower bounds mostly do not\nexist in the Bayesian setting. Specifically, Lai [1987] proved asymptotic ch log2 n upper and lower\nbounds, where ch is a prior-dependent constant. However, all recent Bayes regret bounds are \u02dc         O(\u221an)\n[Russo and Van Roy, 2014, 2016, Lu and Van Roy, 2019, Hong et al., 2020, Kveton et al., 2021].\nThis leaves open the question of finite-time logarithmic regret bounds in the Bayesian setting.\nIn this work, we answer this question positively and make the following contributions:\n       1. We derive the first finite-time logarithmic Bayes regret upper bounds for a Bayesian upper\n          confidence bound (UCB) algorithm. The bounds are O(c\u2206            log n) and O(ch log2 n), where\n          ch and c\u2206   are constants depending on the prior distribution h and the gaps of random bandit\n          instances sampled from h, respectively. The latter matches the lower bound of Lai [1987]\n          asymptotically. When compared to prior \u02dc     O(\u221an) bounds, we better characterize low-regret\n          regimes, where the random gaps are large.\n       2. To show the value of prior as a side information, we also derive a finite-time logarithmic\n          Bayes regret upper bound for a frequentist UCB algorithm. The bound changes only little\n          as the prior becomes more informative, while the regret bound for the Bayesian algorithm\n          eventually goes to zero. The bounds match asymptotically when n \u2192              \u221e   and the prior is\n          overtaken by data.\n       3. To show the generality of our approach, we prove a O(d c\u2206         log2 n) Bayes regret bound for\n          a Bayesian linear bandit algorithm, where d denotes the number of dimensions and c\u2206             is a\n          constant depending on random gaps. This bound also improves with a better prior.\n       4. Our analyses are a major departure from all recent Bayesian bandit analyses, starting with\n          Russo and Van Roy [2014]. Roughly speaking, we first bound the regret in a fixed bandit\n          instance, similarly to frequentist analyses, and then integrate out the random gap.\n       5. We show the tightness of our bounds empirically and compare them to prior bounds.\nThis paper is organized as follows. In Section 2, we introduce the setting of Bayesian bandits. In\nSection 3, we present a Bayesian upper confidence bound algorithm called BayesUCB [Kaufmann\net al., 2012]. In Section 4, we derive finite-time logarithmic Bayes regret bounds for BayesUCB, in\nboth multi-armed and linear bandits. These are the first such bounds ever derived. In Section 5, we\ncompare our bounds to prior works and show that one matches an existing lower bound [Lai, 1987]\nasymptotically. In Section 6, we evaluate the bounds empirically. We conclude in Section 7.\n2    Setting\nWe start with introducing our notation. Random variables are capitalized, except for Greek letters\nlike \u03b8. For any positive integer n, we define [n] = {1, . . . , n}. The indicator function is 1{\u00b7}. The\ni-th entry of vector v is vi. If the vector is already indexed, such as vj, we write vj,i. We denote the\nmaximum and minimum eigenvalues of matrix M \u2208              Rd\u00d7d by \u03bb1(M) and \u03bbd(M), respectively.\nOur setting is defined as follows. We have a multi-armed bandit [Lai and Robbins, 1985, Lai, 1987,\nAuer et al., 2002, Abbasi-Yadkori et al., 2011] with an action set A. Each action a \u2208        A is associated\nwith a reward distribution pa(\u00b7; \u03b8), which is parameterized by an unknown model parameter \u03b8 shared\nby all actions. The learner interacts with the bandit instance for n rounds indexed by t \u2208       [n]. In each\nround t, it takes an action At \u2208    A and observes its stochastic reward Yt \u223c       pAt(\u00b7; \u03b8). The rewards\nare sampled independently across the rounds. We denote the mean of pa(\u00b7; \u03b8) by \u00b5a(\u03b8) and call it\n    *The work started at Amazon Search.\n                                                      2", "md": "# Bayesian Bandits\n\n# Structure of Bandit Tasks\n\nWe consider bandit tasks where the parameters are sampled i.i.d. from a task distribution.\n\n## Regret Bounds\n\nMany bandit algorithms have frequentist regret bounds that match lower bounds. For example, in a K-armed bandit with a minimum gap $$\\Delta$$ and horizon n:\n\n- The gap-dependent regret bound of UCB1 matches the lower bound of Lai and Robbins.\n- The gap-free regret bound of UCB1 matches the lower bound of Auer et al.\n\nThe extra logarithmic factor in the bound can be eliminated by modifying UCB1. However, in the Bayesian setting, matching upper and lower bounds mostly do not exist.\n\n## Contributions\n\n1. We derive finite-time logarithmic Bayes regret upper bounds for a Bayesian UCB algorithm.\n2. We show the value of prior information by deriving a regret bound for a frequentist UCB algorithm.\n3. We prove a Bayes regret bound for a Bayesian linear bandit algorithm.\n4. Our analyses depart from recent Bayesian bandit analyses by integrating out the random gap.\n5. We empirically show the tightness of our bounds and compare them to prior works.\n\n## Organization of the Paper\n\nThe paper is organized as follows:\n\n1. Introduction to Bayesian bandits\n2. Bayesian upper confidence bound algorithm - BayesUCB\n3. Derivation of finite-time logarithmic Bayes regret bounds for BayesUCB\n4. Comparison of bounds with prior works\n5. Evaluation of bounds empirically\n6. Conclusion\n\n## Setting\n\nNotation:\n\n- Random variables are capitalized, except for Greek letters like $\\theta$.\n- For any positive integer n, [n] = {1, ..., n}.\n- The indicator function is 1{\u00b7}.\n- The i-th entry of vector v is vi.\n\nOur setting involves a multi-armed bandit with an action set A. Each action a is associated with a reward distribution pa(\u00b7; $$\\theta$$). The learner interacts with the bandit instance for n rounds, taking actions and observing rewards.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Bayesian Bandits", "md": "# Bayesian Bandits"}, {"type": "heading", "lvl": 1, "value": "Structure of Bandit Tasks", "md": "# Structure of Bandit Tasks"}, {"type": "text", "value": "We consider bandit tasks where the parameters are sampled i.i.d. from a task distribution.", "md": "We consider bandit tasks where the parameters are sampled i.i.d. from a task distribution."}, {"type": "heading", "lvl": 2, "value": "Regret Bounds", "md": "## Regret Bounds"}, {"type": "text", "value": "Many bandit algorithms have frequentist regret bounds that match lower bounds. For example, in a K-armed bandit with a minimum gap $$\\Delta$$ and horizon n:\n\n- The gap-dependent regret bound of UCB1 matches the lower bound of Lai and Robbins.\n- The gap-free regret bound of UCB1 matches the lower bound of Auer et al.\n\nThe extra logarithmic factor in the bound can be eliminated by modifying UCB1. However, in the Bayesian setting, matching upper and lower bounds mostly do not exist.", "md": "Many bandit algorithms have frequentist regret bounds that match lower bounds. For example, in a K-armed bandit with a minimum gap $$\\Delta$$ and horizon n:\n\n- The gap-dependent regret bound of UCB1 matches the lower bound of Lai and Robbins.\n- The gap-free regret bound of UCB1 matches the lower bound of Auer et al.\n\nThe extra logarithmic factor in the bound can be eliminated by modifying UCB1. However, in the Bayesian setting, matching upper and lower bounds mostly do not exist."}, {"type": "heading", "lvl": 2, "value": "Contributions", "md": "## Contributions"}, {"type": "text", "value": "1. We derive finite-time logarithmic Bayes regret upper bounds for a Bayesian UCB algorithm.\n2. We show the value of prior information by deriving a regret bound for a frequentist UCB algorithm.\n3. We prove a Bayes regret bound for a Bayesian linear bandit algorithm.\n4. Our analyses depart from recent Bayesian bandit analyses by integrating out the random gap.\n5. We empirically show the tightness of our bounds and compare them to prior works.", "md": "1. We derive finite-time logarithmic Bayes regret upper bounds for a Bayesian UCB algorithm.\n2. We show the value of prior information by deriving a regret bound for a frequentist UCB algorithm.\n3. We prove a Bayes regret bound for a Bayesian linear bandit algorithm.\n4. Our analyses depart from recent Bayesian bandit analyses by integrating out the random gap.\n5. We empirically show the tightness of our bounds and compare them to prior works."}, {"type": "heading", "lvl": 2, "value": "Organization of the Paper", "md": "## Organization of the Paper"}, {"type": "text", "value": "The paper is organized as follows:\n\n1. Introduction to Bayesian bandits\n2. Bayesian upper confidence bound algorithm - BayesUCB\n3. Derivation of finite-time logarithmic Bayes regret bounds for BayesUCB\n4. Comparison of bounds with prior works\n5. Evaluation of bounds empirically\n6. Conclusion", "md": "The paper is organized as follows:\n\n1. Introduction to Bayesian bandits\n2. Bayesian upper confidence bound algorithm - BayesUCB\n3. Derivation of finite-time logarithmic Bayes regret bounds for BayesUCB\n4. Comparison of bounds with prior works\n5. Evaluation of bounds empirically\n6. Conclusion"}, {"type": "heading", "lvl": 2, "value": "Setting", "md": "## Setting"}, {"type": "text", "value": "Notation:\n\n- Random variables are capitalized, except for Greek letters like $\\theta$.\n- For any positive integer n, [n] = {1, ..., n}.\n- The indicator function is 1{\u00b7}.\n- The i-th entry of vector v is vi.\n\nOur setting involves a multi-armed bandit with an action set A. Each action a is associated with a reward distribution pa(\u00b7; $$\\theta$$). The learner interacts with the bandit instance for n rounds, taking actions and observing rewards.", "md": "Notation:\n\n- Random variables are capitalized, except for Greek letters like $\\theta$.\n- For any positive integer n, [n] = {1, ..., n}.\n- The indicator function is 1{\u00b7}.\n- The i-th entry of vector v is vi.\n\nOur setting involves a multi-armed bandit with an action set A. Each action a is associated with a reward distribution pa(\u00b7; $$\\theta$$). The learner interacts with the bandit instance for n rounds, taking actions and observing rewards."}]}, {"page": 3, "text": " Algorithm 1 BayesUCB\n  1: for t = 1, . . . , n do\n  2:        Compute the posterior distribution of \u03b8 using prior h and history Ht\n  3:        for each action a \u2208          A do\n  4:              Compute Ut,a according to (1)\n  5:        Take action At \u2190           arg max a\u2208A Ut,a and observe its reward Yt\n the mean reward of action a. The optimal action is A\u2217                         = arg max a\u2208A \u00b5a(\u03b8) and its mean reward is\n \u00b5\u2217(\u03b8) = \u00b5A\u2217(\u03b8). For a fixed model parameter \u03b8, the n-round regret of a policy is defined as\n                                           R(n; \u03b8) = E            n   \u00b5\u2217(\u03b8) \u2212      \u00b5At(\u03b8)       \u03b8    ,\n                                                                t=1\n where the expectation is taken over both random observations Yt and actions At. The suboptimality\n gap of action a is \u2206a = \u00b5\u2217(\u03b8) \u2212                 \u00b5a(\u03b8) and the minimum gap is \u2206min = mina\u2208A\\{A\u2217} \u2206a.\n Two settings are common in stochastic bandits. In the frequentist setting [Lai and Robbins, 1985,\n Auer et al., 2002, Abbasi-Yadkori et al., 2011], the learner has no additional information about \u03b8 and\n its objective is to minimize the worst-case regret for any bounded \u03b8. We study the Bayesian setting\n [Gittins, 1979, Lai, 1987, Russo and Van Roy, 2014, Russo et al., 2018], where the model parameter\n \u03b8 is drawn from a prior distribution h that is given to the learner as a side information. The goal of\n the learner is to minimize the n-round Bayes regret R(n) = E [R(n; \u03b8)], where the expectation is\n taken over the random model parameter \u03b8 \u223c                          h. Note that A\u2217, \u2206a, and \u2206min are random because\n they depend on the random instance \u03b8.\n 3     Algorithm\nWe study a Bayesian upper confidence bound algorithm called BayesUCB [Kaufmann et al., 2012].\n The algorithm was analyzed in the Bayesian setting by Russo and Van Roy [2014]. The key idea\n in BayesUCB is to take the action with the highest UCB with respect to the posterior distribution\n of model parameter \u03b8. This differentiates it from frequentist algorithms, such as UCB1 [Auer et al.,\n 2002] and LinUCB [Abbasi-Yadkori et al., 2011], where the UCBs are computed using a frequentist\n maximum likelihood estimate (MLE) of the model parameter.\n Let Ht = (A\u2113, Y\u2113)\u2113\u2208[t\u22121] be the history of taken actions and their observed rewards up to round t.\n The Bayesian UCB for the mean reward of action a at round t is\n                                                        Ut,a = \u00b5a(\u02c6     \u03b8t) + Ct,a ,                                                        (1)\n where \u02c6  \u03b8t is the posterior mean estimate of \u03b8 at round t and Ct,a is a confidence interval width for\n action a at round t. The posterior distribution of model parameter \u03b8 is computed from a prior h and\n history Ht using Bayes\u2019 rule. The width is chosen so that |\u00b5a(\u02c6                         \u03b8t) \u2212    \u00b5a(\u03b8)| \u2264      Ct,a holds with a high\n probability conditioned on any history Ht. Technically speaking, Ct,a is a half-width but we call it a\n width to simplify terminology.\n Our algorithm is presented in Algorithm 1. We instantiate it in a Gaussian bandit in Section 3.1, in a\n Bernoulli bandit in Section 3.2, and in a linear bandit with Gaussian rewards in Section 3.3. These\n settings are of practical interest because they lead to computationally-efficient implementations that\n can be analyzed due to closed-form posteriors [Lu and Van Roy, 2019, Kveton et al., 2021, Basu\n et al., 2021, Wang et al., 2021, Hong et al., 2022]. While we focus on deriving logarithmic Bayes\n regret bounds for BayesUCB, we believe that similar analyses can be done for Thompson sampling\n [Thompson, 1933, Chapelle and Li, 2012, Agrawal and Goyal, 2012, 2013, Russo and Van Roy, 2014,\n Russo et al., 2018]. This extension is non-trivial because a key step in our analysis is that the action\n with the highest UCB is taken (Section 5.3).\n 3.1     Gaussian Bandit\n In a K-armed Gaussian bandit, the action set is A = [K] and the model parameter is \u03b8 \u2208                                           RK. Each\n action a \u2208      A has a Gaussian reward distribution, pa(\u00b7; \u03b8) = N                         (\u00b7; \u03b8a, \u03c32), where \u03b8a is its mean and\n                                                                       3", "md": "# BayesUCB Algorithm\n\n## Algorithm 1 BayesUCB\n\n1. for t = 1, . . . , n do\n2. Compute the posterior distribution of \u03b8 using prior h and history Ht\n3. for each action a \u2208 A do\n4. Compute $U_{t,a}$ according to (1)\n5. Take action $A_{t} \\leftarrow \\text{arg max}_{a \\in A} U_{t,a}$ and observe its reward Yt\n\nThe mean reward of action a. The optimal action is $$A^* = \\text{arg max}_{a \\in A} \\mu_{a}(\\theta)$$ and its mean reward is $$\\mu^*(\\theta) = \\mu_{A^*}(\\theta)$$. For a fixed model parameter \u03b8, the n-round regret of a policy is defined as\n\n$$R(n; \\theta) = E \\left[ \\sum_{t=1}^{n} \\mu^*(\\theta) - \\mu_{A_t}(\\theta) \\right],$$\nwhere the expectation is taken over both random observations Yt and actions At. The suboptimality gap of action a is $$\\Delta_a = \\mu^*(\\theta) - \\mu_a(\\theta)$$ and the minimum gap is $$\\Delta_{\\text{min}} = \\min_{a \\in A\\backslash\\{A^*\\}} \\Delta_a$$.\n\nTwo settings are common in stochastic bandits. In the frequentist setting [Lai and Robbins, 1985, Auer et al., 2002, Abbasi-Yadkori et al., 2011], the learner has no additional information about \u03b8 and its objective is to minimize the worst-case regret for any bounded \u03b8. We study the Bayesian setting [Gittins, 1979, Lai, 1987, Russo and Van Roy, 2014, Russo et al., 2018], where the model parameter \u03b8 is drawn from a prior distribution h that is given to the learner as a side information. The goal of the learner is to minimize the n-round Bayes regret $$R(n) = E[R(n; \\theta)]$$, where the expectation is taken over the random model parameter $$\\theta \\sim h$$. Note that $$A^*, \\Delta_a$$, and $$\\Delta_{\\text{min}}$$ are random because they depend on the random instance $$\\theta$$.\n\n### Algorithm\n\nWe study a Bayesian upper confidence bound algorithm called BayesUCB [Kaufmann et al., 2012]. The algorithm was analyzed in the Bayesian setting by Russo and Van Roy [2014]. The key idea in BayesUCB is to take the action with the highest UCB with respect to the posterior distribution of model parameter $$\\theta$$. This differentiates it from frequentist algorithms, such as UCB1 [Auer et al., 2002] and LinUCB [Abbasi-Yadkori et al., 2011], where the UCBs are computed using a frequentist maximum likelihood estimate (MLE) of the model parameter.\n\nLet $$H_t = \\{(A_{\\ell}, Y_{\\ell})\\}_{\\ell \\in [t-1]}$$ be the history of taken actions and their observed rewards up to round t. The Bayesian UCB for the mean reward of action a at round t is\n\n$$U_{t,a} = \\mu_a(\\hat{\\theta}_t) + C_{t,a},$$\nwhere $$\\hat{\\theta}_t$$ is the posterior mean estimate of $$\\theta$$ at round t and $$C_{t,a}$$ is a confidence interval width for action a at round t. The posterior distribution of model parameter $$\\theta$$ is computed from a prior h and history $$H_t$$ using Bayes\u2019 rule. The width is chosen so that $$|\\mu_a(\\hat{\\theta}_t) - \\mu_a(\\theta)| \\leq C_{t,a}$$ holds with a high probability conditioned on any history $$H_t$$. Technically speaking, $$C_{t,a}$$ is a half-width but we call it a width to simplify terminology.\n\nOur algorithm is presented in Algorithm 1. We instantiate it in a Gaussian bandit in Section 3.1, in a Bernoulli bandit in Section 3.2, and in a linear bandit with Gaussian rewards in Section 3.3. These settings are of practical interest because they lead to computationally-efficient implementations that can be analyzed due to closed-form posteriors [Lu and Van Roy, 2019, Kveton et al., 2021, Basu et al., 2021, Wang et al., 2021, Hong et al., 2022]. While we focus on deriving logarithmic Bayes regret bounds for BayesUCB, we believe that similar analyses can be done for Thompson sampling [Thompson, 1933, Chapelle and Li, 2012, Agrawal and Goyal, 2012, 2013, Russo and Van Roy, 2014, Russo et al., 2018]. This extension is non-trivial because a key step in our analysis is that the action with the highest UCB is taken (Section 5.3).\n\n#### 3.1 Gaussian Bandit\n\nIn a K-armed Gaussian bandit, the action set is $$A = [K]$$ and the model parameter is $$\\theta \\in \\mathbb{R}^K$$. Each action $$a \\in A$$ has a Gaussian reward distribution, $$p_a(\\cdot; \\theta) = \\mathcal{N}(\\cdot; \\theta_a, \\sigma^2)$$, where $$\\theta_a$$ is its mean.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "BayesUCB Algorithm", "md": "# BayesUCB Algorithm"}, {"type": "heading", "lvl": 2, "value": "Algorithm 1 BayesUCB", "md": "## Algorithm 1 BayesUCB"}, {"type": "text", "value": "1. for t = 1, . . . , n do\n2. Compute the posterior distribution of \u03b8 using prior h and history Ht\n3. for each action a \u2208 A do\n4. Compute $U_{t,a}$ according to (1)\n5. Take action $A_{t} \\leftarrow \\text{arg max}_{a \\in A} U_{t,a}$ and observe its reward Yt\n\nThe mean reward of action a. The optimal action is $$A^* = \\text{arg max}_{a \\in A} \\mu_{a}(\\theta)$$ and its mean reward is $$\\mu^*(\\theta) = \\mu_{A^*}(\\theta)$$. For a fixed model parameter \u03b8, the n-round regret of a policy is defined as\n\n$$R(n; \\theta) = E \\left[ \\sum_{t=1}^{n} \\mu^*(\\theta) - \\mu_{A_t}(\\theta) \\right],$$\nwhere the expectation is taken over both random observations Yt and actions At. The suboptimality gap of action a is $$\\Delta_a = \\mu^*(\\theta) - \\mu_a(\\theta)$$ and the minimum gap is $$\\Delta_{\\text{min}} = \\min_{a \\in A\\backslash\\{A^*\\}} \\Delta_a$$.\n\nTwo settings are common in stochastic bandits. In the frequentist setting [Lai and Robbins, 1985, Auer et al., 2002, Abbasi-Yadkori et al., 2011], the learner has no additional information about \u03b8 and its objective is to minimize the worst-case regret for any bounded \u03b8. We study the Bayesian setting [Gittins, 1979, Lai, 1987, Russo and Van Roy, 2014, Russo et al., 2018], where the model parameter \u03b8 is drawn from a prior distribution h that is given to the learner as a side information. The goal of the learner is to minimize the n-round Bayes regret $$R(n) = E[R(n; \\theta)]$$, where the expectation is taken over the random model parameter $$\\theta \\sim h$$. Note that $$A^*, \\Delta_a$$, and $$\\Delta_{\\text{min}}$$ are random because they depend on the random instance $$\\theta$$.", "md": "1. for t = 1, . . . , n do\n2. Compute the posterior distribution of \u03b8 using prior h and history Ht\n3. for each action a \u2208 A do\n4. Compute $U_{t,a}$ according to (1)\n5. Take action $A_{t} \\leftarrow \\text{arg max}_{a \\in A} U_{t,a}$ and observe its reward Yt\n\nThe mean reward of action a. The optimal action is $$A^* = \\text{arg max}_{a \\in A} \\mu_{a}(\\theta)$$ and its mean reward is $$\\mu^*(\\theta) = \\mu_{A^*}(\\theta)$$. For a fixed model parameter \u03b8, the n-round regret of a policy is defined as\n\n$$R(n; \\theta) = E \\left[ \\sum_{t=1}^{n} \\mu^*(\\theta) - \\mu_{A_t}(\\theta) \\right],$$\nwhere the expectation is taken over both random observations Yt and actions At. The suboptimality gap of action a is $$\\Delta_a = \\mu^*(\\theta) - \\mu_a(\\theta)$$ and the minimum gap is $$\\Delta_{\\text{min}} = \\min_{a \\in A\\backslash\\{A^*\\}} \\Delta_a$$.\n\nTwo settings are common in stochastic bandits. In the frequentist setting [Lai and Robbins, 1985, Auer et al., 2002, Abbasi-Yadkori et al., 2011], the learner has no additional information about \u03b8 and its objective is to minimize the worst-case regret for any bounded \u03b8. We study the Bayesian setting [Gittins, 1979, Lai, 1987, Russo and Van Roy, 2014, Russo et al., 2018], where the model parameter \u03b8 is drawn from a prior distribution h that is given to the learner as a side information. The goal of the learner is to minimize the n-round Bayes regret $$R(n) = E[R(n; \\theta)]$$, where the expectation is taken over the random model parameter $$\\theta \\sim h$$. Note that $$A^*, \\Delta_a$$, and $$\\Delta_{\\text{min}}$$ are random because they depend on the random instance $$\\theta$$."}, {"type": "heading", "lvl": 3, "value": "Algorithm", "md": "### Algorithm"}, {"type": "text", "value": "We study a Bayesian upper confidence bound algorithm called BayesUCB [Kaufmann et al., 2012]. The algorithm was analyzed in the Bayesian setting by Russo and Van Roy [2014]. The key idea in BayesUCB is to take the action with the highest UCB with respect to the posterior distribution of model parameter $$\\theta$$. This differentiates it from frequentist algorithms, such as UCB1 [Auer et al., 2002] and LinUCB [Abbasi-Yadkori et al., 2011], where the UCBs are computed using a frequentist maximum likelihood estimate (MLE) of the model parameter.\n\nLet $$H_t = \\{(A_{\\ell}, Y_{\\ell})\\}_{\\ell \\in [t-1]}$$ be the history of taken actions and their observed rewards up to round t. The Bayesian UCB for the mean reward of action a at round t is\n\n$$U_{t,a} = \\mu_a(\\hat{\\theta}_t) + C_{t,a},$$\nwhere $$\\hat{\\theta}_t$$ is the posterior mean estimate of $$\\theta$$ at round t and $$C_{t,a}$$ is a confidence interval width for action a at round t. The posterior distribution of model parameter $$\\theta$$ is computed from a prior h and history $$H_t$$ using Bayes\u2019 rule. The width is chosen so that $$|\\mu_a(\\hat{\\theta}_t) - \\mu_a(\\theta)| \\leq C_{t,a}$$ holds with a high probability conditioned on any history $$H_t$$. Technically speaking, $$C_{t,a}$$ is a half-width but we call it a width to simplify terminology.\n\nOur algorithm is presented in Algorithm 1. We instantiate it in a Gaussian bandit in Section 3.1, in a Bernoulli bandit in Section 3.2, and in a linear bandit with Gaussian rewards in Section 3.3. These settings are of practical interest because they lead to computationally-efficient implementations that can be analyzed due to closed-form posteriors [Lu and Van Roy, 2019, Kveton et al., 2021, Basu et al., 2021, Wang et al., 2021, Hong et al., 2022]. While we focus on deriving logarithmic Bayes regret bounds for BayesUCB, we believe that similar analyses can be done for Thompson sampling [Thompson, 1933, Chapelle and Li, 2012, Agrawal and Goyal, 2012, 2013, Russo and Van Roy, 2014, Russo et al., 2018]. This extension is non-trivial because a key step in our analysis is that the action with the highest UCB is taken (Section 5.3).", "md": "We study a Bayesian upper confidence bound algorithm called BayesUCB [Kaufmann et al., 2012]. The algorithm was analyzed in the Bayesian setting by Russo and Van Roy [2014]. The key idea in BayesUCB is to take the action with the highest UCB with respect to the posterior distribution of model parameter $$\\theta$$. This differentiates it from frequentist algorithms, such as UCB1 [Auer et al., 2002] and LinUCB [Abbasi-Yadkori et al., 2011], where the UCBs are computed using a frequentist maximum likelihood estimate (MLE) of the model parameter.\n\nLet $$H_t = \\{(A_{\\ell}, Y_{\\ell})\\}_{\\ell \\in [t-1]}$$ be the history of taken actions and their observed rewards up to round t. The Bayesian UCB for the mean reward of action a at round t is\n\n$$U_{t,a} = \\mu_a(\\hat{\\theta}_t) + C_{t,a},$$\nwhere $$\\hat{\\theta}_t$$ is the posterior mean estimate of $$\\theta$$ at round t and $$C_{t,a}$$ is a confidence interval width for action a at round t. The posterior distribution of model parameter $$\\theta$$ is computed from a prior h and history $$H_t$$ using Bayes\u2019 rule. The width is chosen so that $$|\\mu_a(\\hat{\\theta}_t) - \\mu_a(\\theta)| \\leq C_{t,a}$$ holds with a high probability conditioned on any history $$H_t$$. Technically speaking, $$C_{t,a}$$ is a half-width but we call it a width to simplify terminology.\n\nOur algorithm is presented in Algorithm 1. We instantiate it in a Gaussian bandit in Section 3.1, in a Bernoulli bandit in Section 3.2, and in a linear bandit with Gaussian rewards in Section 3.3. These settings are of practical interest because they lead to computationally-efficient implementations that can be analyzed due to closed-form posteriors [Lu and Van Roy, 2019, Kveton et al., 2021, Basu et al., 2021, Wang et al., 2021, Hong et al., 2022]. While we focus on deriving logarithmic Bayes regret bounds for BayesUCB, we believe that similar analyses can be done for Thompson sampling [Thompson, 1933, Chapelle and Li, 2012, Agrawal and Goyal, 2012, 2013, Russo and Van Roy, 2014, Russo et al., 2018]. This extension is non-trivial because a key step in our analysis is that the action with the highest UCB is taken (Section 5.3)."}, {"type": "heading", "lvl": 4, "value": "3.1 Gaussian Bandit", "md": "#### 3.1 Gaussian Bandit"}, {"type": "text", "value": "In a K-armed Gaussian bandit, the action set is $$A = [K]$$ and the model parameter is $$\\theta \\in \\mathbb{R}^K$$. Each action $$a \\in A$$ has a Gaussian reward distribution, $$p_a(\\cdot; \\theta) = \\mathcal{N}(\\cdot; \\theta_a, \\sigma^2)$$, where $$\\theta_a$$ is its mean.", "md": "In a K-armed Gaussian bandit, the action set is $$A = [K]$$ and the model parameter is $$\\theta \\in \\mathbb{R}^K$$. Each action $$a \\in A$$ has a Gaussian reward distribution, $$p_a(\\cdot; \\theta) = \\mathcal{N}(\\cdot; \\theta_a, \\sigma^2)$$, where $$\\theta_a$$ is its mean."}]}, {"page": 4, "text": " \u03c3 > 0 is a known reward noise. Thus \u00b5a(\u03b8) = \u03b8a. The model parameter \u03b8 is drawn from a known\n Gaussian prior h(\u00b7) = N              (\u00b7; \u00b50, \u03c32 0IK), where \u00b50 \u2208            RK is a vector of prior means and \u03c30 > 0 is a\n prior width.\n The posterior distribution of the mean reward of action a at round t is N                               (\u00b7; \u02c6\n                                                                                                             \u03b8t,a, \u02c6\u03c32t,a), where\n                                                    \u02c6\n                                                    \u03c32t,a = (\u03c3\u22122  0    + \u03c3\u22122Nt,a)\u22121\n is the posterior variance, Nt,a =  t\u22121              \u2113=1 1{A\u2113      = a} is the number of observations of action a up to\n round t, and\n                                     \u02c6                                         t\u22121\n                                    \u03b8t,a = \u02c6  \u03c32        \u03c3\u22122                          1{A\u2113     = a} Y\u2113\n                                                 t,a      0 \u00b50,a + \u03c3\u22122         \u2113=1\n is the posterior mean. This follows from a classic result, that the posterior distribution of the mean\n of a Gaussian random variable with a Gaussian prior is a Gaussian [Bishop, 2006]. The Bayesian\n UCB of action a at round t is Ut,a = \u02c6                \u03b8t,a + Ct,a, where Ct,a =                  2\u02c6\n                                                                                                   \u03c32 t,a log(1/\u03b4) is the confidence\n interval width and \u03b4 \u2208           (0, 1) is a failure probability of the confidence interval.\n 3.2     Bernoulli Bandit\n In a K-armed Bernoulli bandit, the action set is A = [K] and the model parameter is \u03b8 \u2208                                          RK. Each\n action a \u2208      A has a Bernoulli reward distribution, pa(\u00b7; \u03b8) = Ber(\u00b7; \u03b8a), where \u03b8a is its mean. Hence\n \u00b5a(\u03b8) = \u03b8a. Each parameter \u03b8a is drawn from a known prior Beta(\u00b7; \u03b1a, \u03b2a), where \u03b1a > 0 and\n \u03b2a > 0 are positive and negative prior pseudo-counts, respectively.\n The posterior distribution of the mean reward of action a at round t is Beta(\u00b7; \u03b1t,a, \u03b2t,a), where\n                   \u03b1t,a = \u03b1a +        t\u22121   1{A\u2113     = a} Y\u2113     ,    \u03b2t,a = \u03b2a +        t\u22121   1{A\u2113     = a} (1 \u2212       Y\u2113) .\n                                      \u2113=1                                                \u2113=1\n This follows from a classic result, that the posterior distribution of the mean of a Bernoulli random\n variable with a beta prior is a beta distribution [Bishop, 2006]. The corresponding Bayesian UCB is\n Ut,a = \u02c6   \u03b8t,a + Ct,a, where\n            \u02c6               \u03b1t,a        ,    Ct,a =                log(1/\u03b4)                                log(1/\u03b4)\n            \u03b8t,a =     \u03b1t,a + \u03b2t,a                          2(\u03b1t,a + \u03b2t,a + 1) =                 2(\u03b1a + \u03b2a + Nt,a + 1) ,\n denote the posterior mean and confidence interval width, respectively, of action a at round t; and\n \u03b4 \u2208   (0, 1) is a failure probability of the confidence interval. The confidence interval is derived using                        1\n the fact that Beta(\u00b7; \u03b1t,a, \u03b2t,a) is a sub-Gaussian distribution with variance proxy                                   4(\u03b1a+\u03b2a+Nt,a+1)\n [Marchal and Arbel, 2017].\n 3.3     Linear Bandit with Gaussian Rewards\nWe also study linear bandits [Dani et al., 2008, Abbasi-Yadkori et al., 2011] with a finite number of\n actions A \u2286       Rd in d dimensions. The model parameter is \u03b8 \u2208                         Rd. All actions a \u2208           A have Gaussian\n reward distributions, pa(\u00b7; \u03b8) = N                 (\u00b7; a\u22a4\u03b8, \u03c32), where \u03c3 > 0 is a known reward noise. Therefore,\n the mean reward of action a is \u00b5a(\u03b8) = a\u22a4\u03b8. The parameter \u03b8 is drawn from a known multivariate\n Gaussian prior h(\u00b7) = N              (\u00b7; \u03b80, \u03a30), where \u03b80 \u2208            Rd is its mean and \u03a30 \u2208               Rd\u00d7d is its covariance,\n represented by a positive semi-definite (PSD) matrix.\n The posterior distribution of \u03b8 at round t is N                   (\u00b7; \u02c6\n                                                                       \u03b8t, \u02c6\n                                                                           \u03a3t), where\n         \u02c6                                   t\u22121                    \u02c6                                                  t\u22121   A\u2113A\u22a4\n         \u03b8t = \u02c6  \u03a3t     \u03a3\u221210 \u03b80 + \u03c3\u22122        \u2113=1   A\u2113Y\u2113       ,    \u03a3t = (\u03a3\u22121    0    + Gt)\u22121 ,          Gt = \u03c3\u22122       \u2113=1          \u2113  .\n Here \u02c6  \u03b8t and \u02c6  \u03a3t are the posterior mean and covariance of \u03b8, respectively, and Gt is the outer product\n of the feature vectors of the taken actions up to round t. These formulas follow from a classic result,\n                                                                       4", "md": "$$\\sigma > 0$$ is a known reward noise. Thus $$\\mu_a(\\theta) = \\theta_a$$. The model parameter $$\\theta$$ is drawn from a known Gaussian prior $$h(\\cdot) = \\mathcal{N}(\\cdot; \\mu_0, \\sigma^2_0I_K)$$, where $$\\mu_0 \\in \\mathbb{R}^K$$ is a vector of prior means and $$\\sigma_0 > 0$$ is a prior width.\n\nThe posterior distribution of the mean reward of action $$a$$ at round $$t$$ is $$\\mathcal{N}(\\cdot; \\hat{\\theta}_{t,a}, \\hat{\\sigma}^2_{t,a})$$, where\n\n$$\n\\hat{\\sigma}^2_{t,a} = (\\sigma^{-2}_0 + \\sigma^{-2}N_{t,a})^{-1}\n$$\n\nis the posterior variance, $$N_{t,a} = \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\}$$ is the number of observations of action $$a$$ up to round $$t$$, and\n\n$$\n\\hat{\\theta}_{t,a} = \\hat{\\sigma}^2_{t,a} \\sigma^{-2}_0 \\mu_{0,a} + \\sigma^{-2} \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} Y_{\\ell}\n$$\n\nis the posterior mean. This follows from a classic result, that the posterior distribution of the mean of a Gaussian random variable with a Gaussian prior is a Gaussian [Bishop, 2006]. The Bayesian UCB of action $$a$$ at round $$t$$ is $$U_{t,a} = \\hat{\\theta}_{t,a} + C_{t,a}$$, where $$C_{t,a} = 2\\hat{\\sigma}^2_{t,a} \\log(1/\\delta)$$ is the confidence interval width and $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval.\n\n### 3.2 Bernoulli Bandit\n\nIn a K-armed Bernoulli bandit, the action set is $$A = [K]$$ and the model parameter is $$\\theta \\in \\mathbb{R}^K$$. Each action $$a \\in A$$ has a Bernoulli reward distribution, $$p_a(\\cdot; \\theta) = \\text{Ber}(\\cdot; \\theta_a)$$, where $$\\theta_a$$ is its mean. Hence $$\\mu_a(\\theta) = \\theta_a$$. Each parameter $$\\theta_a$$ is drawn from a known prior $$\\text{Beta}(\\cdot; \\alpha_a, \\beta_a)$$, where $$\\alpha_a > 0$$ and $$\\beta_a > 0$$ are positive and negative prior pseudo-counts, respectively.\n\nThe posterior distribution of the mean reward of action $$a$$ at round $$t$$ is $$\\text{Beta}(\\cdot; \\alpha_{t,a}, \\beta_{t,a})$$, where\n\n$$\n\\alpha_{t,a} = \\alpha_a + \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} Y_{\\ell}, \\quad \\beta_{t,a} = \\beta_a + \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} (1 - Y_{\\ell}).\n$$\n\nThis follows from a classic result, that the posterior distribution of the mean of a Bernoulli random variable with a beta prior is a beta distribution [Bishop, 2006]. The corresponding Bayesian UCB is $$U_{t,a} = \\hat{\\theta}_{t,a} + C_{t,a}$$, where\n\n$$\n\\hat{\\theta}_{t,a} = \\frac{\\alpha_{t,a}}{\\alpha_{t,a} + \\beta_{t,a}}, \\quad C_{t,a} = 2\\log(1/\\delta) \\sqrt{\\frac{1}{4(\\alpha_a + \\beta_a + N_{t,a} + 1)}}\n$$\n\ndenote the posterior mean and confidence interval width, respectively, of action $$a$$ at round $$t$$; and $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval. The confidence interval is derived using the fact that $$\\text{Beta}(\\cdot; \\alpha_{t,a}, \\beta_{t,a})$$ is a sub-Gaussian distribution with variance proxy $$4(\\alpha_a+\\beta_a+N_{t,a}+1)$$ [Marchal and Arbel, 2017].\n\n### 3.3 Linear Bandit with Gaussian Rewards\n\nWe also study linear bandits [Dani et al., 2008, Abbasi-Yadkori et al., 2011] with a finite number of actions $$A \\subseteq \\mathbb{R}^d$$ in $$d$$ dimensions. The model parameter is $$\\theta \\in \\mathbb{R}^d$$. All actions $$a \\in A$$ have Gaussian reward distributions, $$p_a(\\cdot; \\theta) = \\mathcal{N}(\\cdot; a^T\\theta, \\sigma^2)$$, where $$\\sigma > 0$$ is a known reward noise. Therefore, the mean reward of action $$a$$ is $$\\mu_a(\\theta) = a^T\\theta$$. The parameter $$\\theta$$ is drawn from a known multivariate Gaussian prior $$h(\\cdot) = \\mathcal{N}(\\cdot; \\theta_0, \\Sigma_0)$$, where $$\\theta_0 \\in \\mathbb{R}^d$$ is its mean and $$\\Sigma_0 \\in \\mathbb{R}^{d \\times d}$$ is its covariance, represented by a positive semi-definite (PSD) matrix.\n\nThe posterior distribution of $$\\theta$$ at round $$t$$ is $$\\mathcal{N}(\\cdot; \\hat{\\theta}_t, \\hat{\\Sigma}_t)$$, where\n\n$$\n\\hat{\\theta}_t = \\hat{\\Sigma}_t \\Sigma^{-1}_0 \\theta_0 + \\sigma^{-2} \\sum_{\\ell=1}^{t-1} A_{\\ell}Y_{\\ell}, \\quad \\hat{\\Sigma}_t = (\\Sigma^{-1}_0 + G_t)^{-1}, \\quad G_t = \\sigma^{-2} \\sum_{\\ell=1}^{t-1} A_{\\ell}A_{\\ell}^T.\n$$\n\nHere $$\\hat{\\theta}_t$$ and $$\\hat{\\Sigma}_t$$ are the posterior mean and covariance of $$\\theta$$, respectively, and $$G_t$$ is the outer product of the feature vectors of the taken actions up to round $$t$$. These formulas follow from a classic result.", "images": [], "items": [{"type": "text", "value": "$$\\sigma > 0$$ is a known reward noise. Thus $$\\mu_a(\\theta) = \\theta_a$$. The model parameter $$\\theta$$ is drawn from a known Gaussian prior $$h(\\cdot) = \\mathcal{N}(\\cdot; \\mu_0, \\sigma^2_0I_K)$$, where $$\\mu_0 \\in \\mathbb{R}^K$$ is a vector of prior means and $$\\sigma_0 > 0$$ is a prior width.\n\nThe posterior distribution of the mean reward of action $$a$$ at round $$t$$ is $$\\mathcal{N}(\\cdot; \\hat{\\theta}_{t,a}, \\hat{\\sigma}^2_{t,a})$$, where\n\n$$\n\\hat{\\sigma}^2_{t,a} = (\\sigma^{-2}_0 + \\sigma^{-2}N_{t,a})^{-1}\n$$\n\nis the posterior variance, $$N_{t,a} = \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\}$$ is the number of observations of action $$a$$ up to round $$t$$, and\n\n$$\n\\hat{\\theta}_{t,a} = \\hat{\\sigma}^2_{t,a} \\sigma^{-2}_0 \\mu_{0,a} + \\sigma^{-2} \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} Y_{\\ell}\n$$\n\nis the posterior mean. This follows from a classic result, that the posterior distribution of the mean of a Gaussian random variable with a Gaussian prior is a Gaussian [Bishop, 2006]. The Bayesian UCB of action $$a$$ at round $$t$$ is $$U_{t,a} = \\hat{\\theta}_{t,a} + C_{t,a}$$, where $$C_{t,a} = 2\\hat{\\sigma}^2_{t,a} \\log(1/\\delta)$$ is the confidence interval width and $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval.", "md": "$$\\sigma > 0$$ is a known reward noise. Thus $$\\mu_a(\\theta) = \\theta_a$$. The model parameter $$\\theta$$ is drawn from a known Gaussian prior $$h(\\cdot) = \\mathcal{N}(\\cdot; \\mu_0, \\sigma^2_0I_K)$$, where $$\\mu_0 \\in \\mathbb{R}^K$$ is a vector of prior means and $$\\sigma_0 > 0$$ is a prior width.\n\nThe posterior distribution of the mean reward of action $$a$$ at round $$t$$ is $$\\mathcal{N}(\\cdot; \\hat{\\theta}_{t,a}, \\hat{\\sigma}^2_{t,a})$$, where\n\n$$\n\\hat{\\sigma}^2_{t,a} = (\\sigma^{-2}_0 + \\sigma^{-2}N_{t,a})^{-1}\n$$\n\nis the posterior variance, $$N_{t,a} = \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\}$$ is the number of observations of action $$a$$ up to round $$t$$, and\n\n$$\n\\hat{\\theta}_{t,a} = \\hat{\\sigma}^2_{t,a} \\sigma^{-2}_0 \\mu_{0,a} + \\sigma^{-2} \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} Y_{\\ell}\n$$\n\nis the posterior mean. This follows from a classic result, that the posterior distribution of the mean of a Gaussian random variable with a Gaussian prior is a Gaussian [Bishop, 2006]. The Bayesian UCB of action $$a$$ at round $$t$$ is $$U_{t,a} = \\hat{\\theta}_{t,a} + C_{t,a}$$, where $$C_{t,a} = 2\\hat{\\sigma}^2_{t,a} \\log(1/\\delta)$$ is the confidence interval width and $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval."}, {"type": "heading", "lvl": 3, "value": "3.2 Bernoulli Bandit", "md": "### 3.2 Bernoulli Bandit"}, {"type": "text", "value": "In a K-armed Bernoulli bandit, the action set is $$A = [K]$$ and the model parameter is $$\\theta \\in \\mathbb{R}^K$$. Each action $$a \\in A$$ has a Bernoulli reward distribution, $$p_a(\\cdot; \\theta) = \\text{Ber}(\\cdot; \\theta_a)$$, where $$\\theta_a$$ is its mean. Hence $$\\mu_a(\\theta) = \\theta_a$$. Each parameter $$\\theta_a$$ is drawn from a known prior $$\\text{Beta}(\\cdot; \\alpha_a, \\beta_a)$$, where $$\\alpha_a > 0$$ and $$\\beta_a > 0$$ are positive and negative prior pseudo-counts, respectively.\n\nThe posterior distribution of the mean reward of action $$a$$ at round $$t$$ is $$\\text{Beta}(\\cdot; \\alpha_{t,a}, \\beta_{t,a})$$, where\n\n$$\n\\alpha_{t,a} = \\alpha_a + \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} Y_{\\ell}, \\quad \\beta_{t,a} = \\beta_a + \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} (1 - Y_{\\ell}).\n$$\n\nThis follows from a classic result, that the posterior distribution of the mean of a Bernoulli random variable with a beta prior is a beta distribution [Bishop, 2006]. The corresponding Bayesian UCB is $$U_{t,a} = \\hat{\\theta}_{t,a} + C_{t,a}$$, where\n\n$$\n\\hat{\\theta}_{t,a} = \\frac{\\alpha_{t,a}}{\\alpha_{t,a} + \\beta_{t,a}}, \\quad C_{t,a} = 2\\log(1/\\delta) \\sqrt{\\frac{1}{4(\\alpha_a + \\beta_a + N_{t,a} + 1)}}\n$$\n\ndenote the posterior mean and confidence interval width, respectively, of action $$a$$ at round $$t$$; and $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval. The confidence interval is derived using the fact that $$\\text{Beta}(\\cdot; \\alpha_{t,a}, \\beta_{t,a})$$ is a sub-Gaussian distribution with variance proxy $$4(\\alpha_a+\\beta_a+N_{t,a}+1)$$ [Marchal and Arbel, 2017].", "md": "In a K-armed Bernoulli bandit, the action set is $$A = [K]$$ and the model parameter is $$\\theta \\in \\mathbb{R}^K$$. Each action $$a \\in A$$ has a Bernoulli reward distribution, $$p_a(\\cdot; \\theta) = \\text{Ber}(\\cdot; \\theta_a)$$, where $$\\theta_a$$ is its mean. Hence $$\\mu_a(\\theta) = \\theta_a$$. Each parameter $$\\theta_a$$ is drawn from a known prior $$\\text{Beta}(\\cdot; \\alpha_a, \\beta_a)$$, where $$\\alpha_a > 0$$ and $$\\beta_a > 0$$ are positive and negative prior pseudo-counts, respectively.\n\nThe posterior distribution of the mean reward of action $$a$$ at round $$t$$ is $$\\text{Beta}(\\cdot; \\alpha_{t,a}, \\beta_{t,a})$$, where\n\n$$\n\\alpha_{t,a} = \\alpha_a + \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} Y_{\\ell}, \\quad \\beta_{t,a} = \\beta_a + \\sum_{\\ell=1}^{t-1} 1\\{A_{\\ell} = a\\} (1 - Y_{\\ell}).\n$$\n\nThis follows from a classic result, that the posterior distribution of the mean of a Bernoulli random variable with a beta prior is a beta distribution [Bishop, 2006]. The corresponding Bayesian UCB is $$U_{t,a} = \\hat{\\theta}_{t,a} + C_{t,a}$$, where\n\n$$\n\\hat{\\theta}_{t,a} = \\frac{\\alpha_{t,a}}{\\alpha_{t,a} + \\beta_{t,a}}, \\quad C_{t,a} = 2\\log(1/\\delta) \\sqrt{\\frac{1}{4(\\alpha_a + \\beta_a + N_{t,a} + 1)}}\n$$\n\ndenote the posterior mean and confidence interval width, respectively, of action $$a$$ at round $$t$$; and $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval. The confidence interval is derived using the fact that $$\\text{Beta}(\\cdot; \\alpha_{t,a}, \\beta_{t,a})$$ is a sub-Gaussian distribution with variance proxy $$4(\\alpha_a+\\beta_a+N_{t,a}+1)$$ [Marchal and Arbel, 2017]."}, {"type": "heading", "lvl": 3, "value": "3.3 Linear Bandit with Gaussian Rewards", "md": "### 3.3 Linear Bandit with Gaussian Rewards"}, {"type": "text", "value": "We also study linear bandits [Dani et al., 2008, Abbasi-Yadkori et al., 2011] with a finite number of actions $$A \\subseteq \\mathbb{R}^d$$ in $$d$$ dimensions. The model parameter is $$\\theta \\in \\mathbb{R}^d$$. All actions $$a \\in A$$ have Gaussian reward distributions, $$p_a(\\cdot; \\theta) = \\mathcal{N}(\\cdot; a^T\\theta, \\sigma^2)$$, where $$\\sigma > 0$$ is a known reward noise. Therefore, the mean reward of action $$a$$ is $$\\mu_a(\\theta) = a^T\\theta$$. The parameter $$\\theta$$ is drawn from a known multivariate Gaussian prior $$h(\\cdot) = \\mathcal{N}(\\cdot; \\theta_0, \\Sigma_0)$$, where $$\\theta_0 \\in \\mathbb{R}^d$$ is its mean and $$\\Sigma_0 \\in \\mathbb{R}^{d \\times d}$$ is its covariance, represented by a positive semi-definite (PSD) matrix.\n\nThe posterior distribution of $$\\theta$$ at round $$t$$ is $$\\mathcal{N}(\\cdot; \\hat{\\theta}_t, \\hat{\\Sigma}_t)$$, where\n\n$$\n\\hat{\\theta}_t = \\hat{\\Sigma}_t \\Sigma^{-1}_0 \\theta_0 + \\sigma^{-2} \\sum_{\\ell=1}^{t-1} A_{\\ell}Y_{\\ell}, \\quad \\hat{\\Sigma}_t = (\\Sigma^{-1}_0 + G_t)^{-1}, \\quad G_t = \\sigma^{-2} \\sum_{\\ell=1}^{t-1} A_{\\ell}A_{\\ell}^T.\n$$\n\nHere $$\\hat{\\theta}_t$$ and $$\\hat{\\Sigma}_t$$ are the posterior mean and covariance of $$\\theta$$, respectively, and $$G_t$$ is the outer product of the feature vectors of the taken actions up to round $$t$$. These formulas follow from a classic result.", "md": "We also study linear bandits [Dani et al., 2008, Abbasi-Yadkori et al., 2011] with a finite number of actions $$A \\subseteq \\mathbb{R}^d$$ in $$d$$ dimensions. The model parameter is $$\\theta \\in \\mathbb{R}^d$$. All actions $$a \\in A$$ have Gaussian reward distributions, $$p_a(\\cdot; \\theta) = \\mathcal{N}(\\cdot; a^T\\theta, \\sigma^2)$$, where $$\\sigma > 0$$ is a known reward noise. Therefore, the mean reward of action $$a$$ is $$\\mu_a(\\theta) = a^T\\theta$$. The parameter $$\\theta$$ is drawn from a known multivariate Gaussian prior $$h(\\cdot) = \\mathcal{N}(\\cdot; \\theta_0, \\Sigma_0)$$, where $$\\theta_0 \\in \\mathbb{R}^d$$ is its mean and $$\\Sigma_0 \\in \\mathbb{R}^{d \\times d}$$ is its covariance, represented by a positive semi-definite (PSD) matrix.\n\nThe posterior distribution of $$\\theta$$ at round $$t$$ is $$\\mathcal{N}(\\cdot; \\hat{\\theta}_t, \\hat{\\Sigma}_t)$$, where\n\n$$\n\\hat{\\theta}_t = \\hat{\\Sigma}_t \\Sigma^{-1}_0 \\theta_0 + \\sigma^{-2} \\sum_{\\ell=1}^{t-1} A_{\\ell}Y_{\\ell}, \\quad \\hat{\\Sigma}_t = (\\Sigma^{-1}_0 + G_t)^{-1}, \\quad G_t = \\sigma^{-2} \\sum_{\\ell=1}^{t-1} A_{\\ell}A_{\\ell}^T.\n$$\n\nHere $$\\hat{\\theta}_t$$ and $$\\hat{\\Sigma}_t$$ are the posterior mean and covariance of $$\\theta$$, respectively, and $$G_t$$ is the outer product of the feature vectors of the taken actions up to round $$t$$. These formulas follow from a classic result."}]}, {"page": 5, "text": "that the posterior distribution of a linear model parameter with a Gaussian prior and observations is a\nGaussian [Bishop, 2006]. The Bayesian UCB of action a at round t is Ut,a = a\u22a4\u02c6                   \u03b8t + Ct,a, where\nCt,a =      2 log(1/\u03b4)\u2225a\u2225\u02c6   \u03a3t is the confidence interval width, \u03b4 \u2208       (0, 1) is a failure probability of the\nconfidence interval, and \u2225a\u2225M =         \u221a  a\u22a4Ma.\n4     Logarithmic Bayes Regret Upper Bounds\nIn this section, we present finite-time logarithmic Bayes regret bounds for BayesUCB. We derive\nthem for both K-armed and linear bandits. One bound matches an existing lower bound of Lai [1987]\nasymptotically and all improve upon prior \u02dc        O(\u221an) bounds. We discuss this in detail in Section 5.\n4.1    BayesUCB in Gaussian Bandit\nOur first regret bound is for BayesUCB in a K-armed Gaussian bandit. It depends on random gaps.\nTo control the gaps, we clip them as \u2206\u03b5       a = max {\u2206a, \u03b5}. The bound is stated below.\nTheorem 1. For any \u03b5 > 0 and \u03b4 \u2208             (0, 1), the n-round Bayes regret of BayesUCB in a K-armed\nGaussian bandit is bounded as              \uf8ee       8\u03c32 log(1/\u03b4)              a \uf8f9\n                              R(n) \u2264    E  \uf8f0                        \u2212  \u03c32\u2206\u03b5    \uf8fb + C ,\n                                             a\u0338=A\u2217       \u2206\u03b5 a            \u03c320\nwhere C = \u03b5n + 2(        2 log(1/\u03b4) + 2K)\u03c30Kn\u03b4 is a low-order term.\nThe proof is in Appendix A.1. For \u03b5 = 1/n and \u03b4 = 1/n, the bound is O(c\u2206                    log n), where c\u2206     is a\nconstant depending on the gaps of random bandit instances. The dependence on \u03c30 in the low-order\nterm C can be reduced to min {\u03c30, \u03c3} by a more elaborate analysis, where the regret of taking each\naction for the first time is bounded separately. This also applies to Corollary 2.\nNow we derive an upper bound on Theorem 1 that eliminates the dependence on random gaps. To\nstate it, we need to introduce additional notation. For any action a, we denote all action parameters\nexcept for a by \u03b8\u2212a = (\u03b81, . . . , \u03b8a\u22121, \u03b8a+1, . . . , \u03b8K) and the corresponding optimal action in \u03b8\u2212a by\n\u03b8\u2217a = maxj\u2208A\\{a} \u03b8j. We denote by ha the prior density of \u03b8a and by h\u2212a the prior density of \u03b8\u2212a.\nSince the prior is factored (Section 3.1), note that h(\u03b8) = ha(\u03b8a)h\u2212a(\u03b8\u2212a) for any \u03b8 and action a.\nTo keep the result clean, we state it for a \u201csufficiently\u201d large prior variance. A complete statement for\nall prior variances is given in Appendix B. We note that the setting of small prior variances favors\nBayesian algorithms since their regret decreases with a more informative prior. In fact, we show in\nAppendix B that the regret of BayesUCB is O(1) for a sufficiently small \u03c30.\n                                        1                                                          1  \u221a  1\nCorollary 2. Let \u03c32     0 \u2265   8 log(1/\u03b4) n2 log log n. Then there exist functions \u03bea : R \u2192         n,   log n   such\nthat the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n    R(n) \u2264      8\u03c32 log(1/\u03b4) log n \u2212      2\u03c32 \u03c32               \u03b8\u2212a ha(\u03b8\u2217 a \u2212  \u03bea(\u03b8\u2217a)) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a + C ,\n                                             0 log n    a\u2208A\nwhere C = 8\u03c32K log(1/\u03b4)\u221alog n + 2(                2 log(1/\u03b4) + 2K)\u03c30Kn\u03b4 + 1 is a low-order term.\nThe proof is in Appendix A.2. For \u03b4 = 1/n, the bound is O(ch log2 n), where ch depends on prior h\nbut not on the gaps of random bandit instances. This bound is motivated by Lai [1987]. The terms \u03bea\narise due to the intermediate value theorem for function ha. Similar terms appear in Lai [1987] but\nvanish in their final asymptotic claims. The rate 1/\u221alog n in the definition of \u03bea cannot be reduced\nto 1/polylog n without increasing dependence on n in other parts of the bound.\nThe complexity term          a\u2208A    \u03b8\u2212a ha(\u03b8\u2217 a \u2212  \u03bea(\u03b8\u2217 a)) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a in Corollary 2 is the same as in\nLai [1987] and can be interpreted as follows. Consider the asymptotic regime of n \u2192                \u221e. Then, since\n                       1     1\nthe range of \u03bea is     n,  \u221a log n , the term simplifies to       a\u2208A   \u03b8\u2212a ha(\u03b8\u2217  a)h\u2212a(\u03b8\u2212a) d\u03b8\u2212a and can be\nviewed as the distance between prior means. In a Gaussian bandit with K = 2 actions, it has a closed\nform of   \u221a  1          \u2212(\u00b50,1\u2212\u00b50,2)2     . A general upper bound for K > 2 actions is given below.\n                                4\u03c32\n             \u03c0\u03c320 exp             0\n                                                          5", "md": "# Math Equations and Text\n\nthat the posterior distribution of a linear model parameter with a Gaussian prior and observations is a Gaussian [Bishop, 2006]. The Bayesian UCB of action a at round t is $$U_{t,a} = a^T \\hat{\\theta}_t + C_{t,a}$$, where $$C_{t,a} = 2 \\log(1/\\delta) \\|a\\|^{\\Sigma_t}$$ is the confidence interval width, $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval, and $$\\|a\\|^M = \\sqrt{a^T Ma}$$.\n\n#### Logarithmic Bayes Regret Upper Bounds\n\nIn this section, we present finite-time logarithmic Bayes regret bounds for BayesUCB. We derive them for both K-armed and linear bandits. One bound matches an existing lower bound of Lai [1987] asymptotically and all improve upon prior $$\\tilde{O}(\\sqrt{n})$$ bounds. We discuss this in detail in Section 5.\n\n#### BayesUCB in Gaussian Bandit\n\nOur first regret bound is for BayesUCB in a K-armed Gaussian bandit. It depends on random gaps. To control the gaps, we clip them as $$\\Delta_{\\epsilon}^a = \\max \\{\\Delta_a, \\epsilon\\}$$. The bound is stated below.\n\nTheorem 1. For any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$R(n) \\leq E \\left[8\\sigma^2 \\log(1/\\delta) \\sum_{a \\neq A^*} \\frac{\\Delta_{\\epsilon}^a}{\\sigma^2} \\right] + C$,\n\nwhere $$C = \\epsilon n + 2(2 \\log(1/\\delta) + 2K)\\sigma_0Kn\\delta$$ is a low-order term. The proof is in Appendix A.1. For $$\\epsilon = 1/n$$ and $$\\delta = 1/n$$, the bound is $$O(c\\Delta \\log n)$$, where $$c\\Delta$$ is a constant depending on the gaps of random bandit instances. The dependence on $$\\sigma_0$$ in the low-order term C can be reduced to $$\\min \\{\\sigma_0, \\sigma\\}$$ by a more elaborate analysis, where the regret of taking each action for the first time is bounded separately. This also applies to Corollary 2.\n\nNow we derive an upper bound on Theorem 1 that eliminates the dependence on random gaps. To state it, we need to introduce additional notation. For any action a, we denote all action parameters except for a by $$\\theta_{-a} = (\\theta_1, ..., \\theta_{a-1}, \\theta_{a+1}, ..., \\theta_K)$$ and the corresponding optimal action in $$\\theta_{-a}$$ by $$\\theta^*_a = \\max_{j \\in A\\{a}} \\theta_j$$. We denote by $$h_a$$ the prior density of $$\\theta_a$$ and by $$h_{-a}$$ the prior density of $$\\theta_{-a}$$. Since the prior is factored (Section 3.1), note that $$h(\\theta) = h_a(\\theta_a)h_{-a}(\\theta_{-a})$$ for any $$\\theta$$ and action a.\n\nTo keep the result clean, we state it for a \u201csufficiently\u201d large prior variance. A complete statement for all prior variances is given in Appendix B. We note that the setting of small prior variances favors Bayesian algorithms since their regret decreases with a more informative prior. In fact, we show in Appendix B that the regret of BayesUCB is $$O(1)$$ for a sufficiently small $$\\sigma_0$$.\n\nCorollary 2. Let $$\\sigma_0^2 \\geq 8 \\log(1/\\delta) n^2 \\log \\log n$$. Then there exist functions $$\\xi_a : \\mathbb{R} \\rightarrow n, \\log n$$ such that the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$R(n) \\leq 8\\sigma^2 \\log(1/\\delta) \\log n - 2\\sigma^2 \\sigma^2 \\int_{a \\in A} \\peta_{-a} h_a(\\peta^*_a - \\xi_a(\\peta^*_a)) h_{-a}(\\peta_{-a}) d\\peta_{-a} + C$,\n\nwhere $$C = 8\\sigma^2 K \\log(1/\\delta) \\sqrt{\\log n} + 2(2 \\log(1/\\delta) + 2K)\\sigma_0Kn\\delta + 1$$ is a low-order term. The proof is in Appendix A.2. For $$\\delta = 1/n$$, the bound is $$O(ch \\log^2 n)$$, where ch depends on prior h but not on the gaps of random bandit instances. This bound is motivated by Lai [1987]. The terms $$\\xi_a$$ arise due to the intermediate value theorem for function $$h_a$$. Similar terms appear in Lai [1987] but vanish in their final asymptotic claims. The rate $$1/\\sqrt{\\log n}$$ in the definition of $$\\xi_a$$ cannot be reduced to $$1/\\text{polylog} n$$ without increasing dependence on n in other parts of the bound.\n\nThe complexity term $$\\int_{a \\in A} \\theta_{-a} h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a}$$ in Corollary 2 is the same as in Lai [1987] and can be interpreted as follows. Consider the asymptotic regime of $$n \\rightarrow \\infty$$. Then, since the range of $$\\xi_a$$ is $$n, \\sqrt{\\log n}$$, the term simplifies to $$\\int_{a \\in A} \\theta_{-a} h_a(\\theta^*_a)h_{-a}(\\theta_{-a}) d\\theta_{-a}$$ and can be viewed as the distance between prior means. In a Gaussian bandit with K = 2 actions, it has a closed form of $$\\frac{\\sqrt{1}}{4\\sigma^2} \\pi\\sigma_0^2 \\exp\\left(-\\frac{(\u00b5_{0,1}-\u00b5_{0,2})^2}{4\\sigma^2}\\right)$$. A general upper bound for K > 2 actions is given below.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "that the posterior distribution of a linear model parameter with a Gaussian prior and observations is a Gaussian [Bishop, 2006]. The Bayesian UCB of action a at round t is $$U_{t,a} = a^T \\hat{\\theta}_t + C_{t,a}$$, where $$C_{t,a} = 2 \\log(1/\\delta) \\|a\\|^{\\Sigma_t}$$ is the confidence interval width, $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval, and $$\\|a\\|^M = \\sqrt{a^T Ma}$$.", "md": "that the posterior distribution of a linear model parameter with a Gaussian prior and observations is a Gaussian [Bishop, 2006]. The Bayesian UCB of action a at round t is $$U_{t,a} = a^T \\hat{\\theta}_t + C_{t,a}$$, where $$C_{t,a} = 2 \\log(1/\\delta) \\|a\\|^{\\Sigma_t}$$ is the confidence interval width, $$\\delta \\in (0, 1)$$ is a failure probability of the confidence interval, and $$\\|a\\|^M = \\sqrt{a^T Ma}$$."}, {"type": "heading", "lvl": 4, "value": "Logarithmic Bayes Regret Upper Bounds", "md": "#### Logarithmic Bayes Regret Upper Bounds"}, {"type": "text", "value": "In this section, we present finite-time logarithmic Bayes regret bounds for BayesUCB. We derive them for both K-armed and linear bandits. One bound matches an existing lower bound of Lai [1987] asymptotically and all improve upon prior $$\\tilde{O}(\\sqrt{n})$$ bounds. We discuss this in detail in Section 5.", "md": "In this section, we present finite-time logarithmic Bayes regret bounds for BayesUCB. We derive them for both K-armed and linear bandits. One bound matches an existing lower bound of Lai [1987] asymptotically and all improve upon prior $$\\tilde{O}(\\sqrt{n})$$ bounds. We discuss this in detail in Section 5."}, {"type": "heading", "lvl": 4, "value": "BayesUCB in Gaussian Bandit", "md": "#### BayesUCB in Gaussian Bandit"}, {"type": "text", "value": "Our first regret bound is for BayesUCB in a K-armed Gaussian bandit. It depends on random gaps. To control the gaps, we clip them as $$\\Delta_{\\epsilon}^a = \\max \\{\\Delta_a, \\epsilon\\}$$. The bound is stated below.\n\nTheorem 1. For any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$R(n) \\leq E \\left[8\\sigma^2 \\log(1/\\delta) \\sum_{a \\neq A^*} \\frac{\\Delta_{\\epsilon}^a}{\\sigma^2} \\right] + C$,\n\nwhere $$C = \\epsilon n + 2(2 \\log(1/\\delta) + 2K)\\sigma_0Kn\\delta$$ is a low-order term. The proof is in Appendix A.1. For $$\\epsilon = 1/n$$ and $$\\delta = 1/n$$, the bound is $$O(c\\Delta \\log n)$$, where $$c\\Delta$$ is a constant depending on the gaps of random bandit instances. The dependence on $$\\sigma_0$$ in the low-order term C can be reduced to $$\\min \\{\\sigma_0, \\sigma\\}$$ by a more elaborate analysis, where the regret of taking each action for the first time is bounded separately. This also applies to Corollary 2.\n\nNow we derive an upper bound on Theorem 1 that eliminates the dependence on random gaps. To state it, we need to introduce additional notation. For any action a, we denote all action parameters except for a by $$\\theta_{-a} = (\\theta_1, ..., \\theta_{a-1}, \\theta_{a+1}, ..., \\theta_K)$$ and the corresponding optimal action in $$\\theta_{-a}$$ by $$\\theta^*_a = \\max_{j \\in A\\{a}} \\theta_j$$. We denote by $$h_a$$ the prior density of $$\\theta_a$$ and by $$h_{-a}$$ the prior density of $$\\theta_{-a}$$. Since the prior is factored (Section 3.1), note that $$h(\\theta) = h_a(\\theta_a)h_{-a}(\\theta_{-a})$$ for any $$\\theta$$ and action a.\n\nTo keep the result clean, we state it for a \u201csufficiently\u201d large prior variance. A complete statement for all prior variances is given in Appendix B. We note that the setting of small prior variances favors Bayesian algorithms since their regret decreases with a more informative prior. In fact, we show in Appendix B that the regret of BayesUCB is $$O(1)$$ for a sufficiently small $$\\sigma_0$$.\n\nCorollary 2. Let $$\\sigma_0^2 \\geq 8 \\log(1/\\delta) n^2 \\log \\log n$$. Then there exist functions $$\\xi_a : \\mathbb{R} \\rightarrow n, \\log n$$ such that the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$R(n) \\leq 8\\sigma^2 \\log(1/\\delta) \\log n - 2\\sigma^2 \\sigma^2 \\int_{a \\in A} \\peta_{-a} h_a(\\peta^*_a - \\xi_a(\\peta^*_a)) h_{-a}(\\peta_{-a}) d\\peta_{-a} + C$,\n\nwhere $$C = 8\\sigma^2 K \\log(1/\\delta) \\sqrt{\\log n} + 2(2 \\log(1/\\delta) + 2K)\\sigma_0Kn\\delta + 1$$ is a low-order term. The proof is in Appendix A.2. For $$\\delta = 1/n$$, the bound is $$O(ch \\log^2 n)$$, where ch depends on prior h but not on the gaps of random bandit instances. This bound is motivated by Lai [1987]. The terms $$\\xi_a$$ arise due to the intermediate value theorem for function $$h_a$$. Similar terms appear in Lai [1987] but vanish in their final asymptotic claims. The rate $$1/\\sqrt{\\log n}$$ in the definition of $$\\xi_a$$ cannot be reduced to $$1/\\text{polylog} n$$ without increasing dependence on n in other parts of the bound.\n\nThe complexity term $$\\int_{a \\in A} \\theta_{-a} h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a}$$ in Corollary 2 is the same as in Lai [1987] and can be interpreted as follows. Consider the asymptotic regime of $$n \\rightarrow \\infty$$. Then, since the range of $$\\xi_a$$ is $$n, \\sqrt{\\log n}$$, the term simplifies to $$\\int_{a \\in A} \\theta_{-a} h_a(\\theta^*_a)h_{-a}(\\theta_{-a}) d\\theta_{-a}$$ and can be viewed as the distance between prior means. In a Gaussian bandit with K = 2 actions, it has a closed form of $$\\frac{\\sqrt{1}}{4\\sigma^2} \\pi\\sigma_0^2 \\exp\\left(-\\frac{(\u00b5_{0,1}-\u00b5_{0,2})^2}{4\\sigma^2}\\right)$$. A general upper bound for K > 2 actions is given below.", "md": "Our first regret bound is for BayesUCB in a K-armed Gaussian bandit. It depends on random gaps. To control the gaps, we clip them as $$\\Delta_{\\epsilon}^a = \\max \\{\\Delta_a, \\epsilon\\}$$. The bound is stated below.\n\nTheorem 1. For any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$R(n) \\leq E \\left[8\\sigma^2 \\log(1/\\delta) \\sum_{a \\neq A^*} \\frac{\\Delta_{\\epsilon}^a}{\\sigma^2} \\right] + C$,\n\nwhere $$C = \\epsilon n + 2(2 \\log(1/\\delta) + 2K)\\sigma_0Kn\\delta$$ is a low-order term. The proof is in Appendix A.1. For $$\\epsilon = 1/n$$ and $$\\delta = 1/n$$, the bound is $$O(c\\Delta \\log n)$$, where $$c\\Delta$$ is a constant depending on the gaps of random bandit instances. The dependence on $$\\sigma_0$$ in the low-order term C can be reduced to $$\\min \\{\\sigma_0, \\sigma\\}$$ by a more elaborate analysis, where the regret of taking each action for the first time is bounded separately. This also applies to Corollary 2.\n\nNow we derive an upper bound on Theorem 1 that eliminates the dependence on random gaps. To state it, we need to introduce additional notation. For any action a, we denote all action parameters except for a by $$\\theta_{-a} = (\\theta_1, ..., \\theta_{a-1}, \\theta_{a+1}, ..., \\theta_K)$$ and the corresponding optimal action in $$\\theta_{-a}$$ by $$\\theta^*_a = \\max_{j \\in A\\{a}} \\theta_j$$. We denote by $$h_a$$ the prior density of $$\\theta_a$$ and by $$h_{-a}$$ the prior density of $$\\theta_{-a}$$. Since the prior is factored (Section 3.1), note that $$h(\\theta) = h_a(\\theta_a)h_{-a}(\\theta_{-a})$$ for any $$\\theta$$ and action a.\n\nTo keep the result clean, we state it for a \u201csufficiently\u201d large prior variance. A complete statement for all prior variances is given in Appendix B. We note that the setting of small prior variances favors Bayesian algorithms since their regret decreases with a more informative prior. In fact, we show in Appendix B that the regret of BayesUCB is $$O(1)$$ for a sufficiently small $$\\sigma_0$$.\n\nCorollary 2. Let $$\\sigma_0^2 \\geq 8 \\log(1/\\delta) n^2 \\log \\log n$$. Then there exist functions $$\\xi_a : \\mathbb{R} \\rightarrow n, \\log n$$ such that the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$R(n) \\leq 8\\sigma^2 \\log(1/\\delta) \\log n - 2\\sigma^2 \\sigma^2 \\int_{a \\in A} \\peta_{-a} h_a(\\peta^*_a - \\xi_a(\\peta^*_a)) h_{-a}(\\peta_{-a}) d\\peta_{-a} + C$,\n\nwhere $$C = 8\\sigma^2 K \\log(1/\\delta) \\sqrt{\\log n} + 2(2 \\log(1/\\delta) + 2K)\\sigma_0Kn\\delta + 1$$ is a low-order term. The proof is in Appendix A.2. For $$\\delta = 1/n$$, the bound is $$O(ch \\log^2 n)$$, where ch depends on prior h but not on the gaps of random bandit instances. This bound is motivated by Lai [1987]. The terms $$\\xi_a$$ arise due to the intermediate value theorem for function $$h_a$$. Similar terms appear in Lai [1987] but vanish in their final asymptotic claims. The rate $$1/\\sqrt{\\log n}$$ in the definition of $$\\xi_a$$ cannot be reduced to $$1/\\text{polylog} n$$ without increasing dependence on n in other parts of the bound.\n\nThe complexity term $$\\int_{a \\in A} \\theta_{-a} h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a}$$ in Corollary 2 is the same as in Lai [1987] and can be interpreted as follows. Consider the asymptotic regime of $$n \\rightarrow \\infty$$. Then, since the range of $$\\xi_a$$ is $$n, \\sqrt{\\log n}$$, the term simplifies to $$\\int_{a \\in A} \\theta_{-a} h_a(\\theta^*_a)h_{-a}(\\theta_{-a}) d\\theta_{-a}$$ and can be viewed as the distance between prior means. In a Gaussian bandit with K = 2 actions, it has a closed form of $$\\frac{\\sqrt{1}}{4\\sigma^2} \\pi\\sigma_0^2 \\exp\\left(-\\frac{(\u00b5_{0,1}-\u00b5_{0,2})^2}{4\\sigma^2}\\right)$$. A general upper bound for K > 2 actions is given below."}]}, {"page": 6, "text": " Lemma 3. In a K-armed Gaussian bandit with prior h(\u00b7) = N                                   (\u00b7; \u00b50, \u03c32  0IK), we have\n                            ha(\u03b8\u2217  a) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a \u2264                   1                    exp     \u2212(\u00b50,a \u2212   4\u03c32 \u00b50,a\u2032)2       .\n              a\u2208A     \u03b8\u2212a                                          2    \u03c0\u03c32  0 a\u2208A    a\u2032\u0338=a                         0\n The bound is proved in Appendix A.3 and has several interesting properties that capture low-regret\n regimes. First, as the prior becomes more informative and concentrated, \u03c30 \u2192                                      0, the bound goes to\n zero. Second, when the gaps of bandit instances sampled from the prior are large, low regret is also\n expected. This can happen when the prior means become more separated, |\u00b50,a \u2212                                            \u00b50,a\u2032| \u2192      \u221e, or\n the prior becomes wider, \u03c30 \u2192                \u221e. Our bound goes to zero in both of these cases. This also implies\n that Bayes regret bounds are not necessarily monotone in prior parameters, such as \u03c30.\n 4.2     UCB1 in Gaussian Bandit\n Using a similar approach, we prove a Bayes regret bound for UCB1 [Auer et al., 2002]. We view it as\n BayesUCB (Section 3.1) where \u03c30 = \u221e                       and each action a \u2208          A is initially taken once at round t = a.\n This generalizes classic UCB1 to \u03c32-sub-Gaussian noise. An asymptotic Bayes regret bound for UCB1\n was proved by Lai [1987] (claim (i) in their Theorem 3). We derive a finite-time prior-dependent\n Bayes regret bound below.                                         1   \u221a   1\n Theorem 4. There exist functions \u03bea : R \u2192                         n,     log n    such that the n-round Bayes regret of UCB1\n in a K-armed Gaussian bandit is bounded as\n                 R(n) \u2264       8\u03c32 log(1/\u03b4) log n         a\u2208A      \u03b8\u2212a  ha(\u03b8\u2217  a \u2212   \u03bea(\u03b8\u2217  a)) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a + C ,\n where C = 8\u03c32K log(1/\u03b4)\u221alog n + 2(                           2 log(1/\u03b4) + 2K)\u03c3Kn\u03b4 +  a\u2208A E [\u2206a] + 1.\n The proof is in Appendix A.4. For \u03b4 = 1/n, the bound is O(ch log2 n) and similar to Corollary 2.\n                                                                         \u03c32\n The main difference is in the additional factor                     2\u03c320 log n in Corollary 2, which decreases the bound.\n This means that the regret of BayesUCB improves as \u03c30 decreases while that of UCB1 may not change\n much. In fact, the regret bound of BayesUCB is O(1) as \u03c30 \u2192                                 0 (Appendix B) while that of UCB1\n remains logarithmic. This is expected because BayesUCB has more information about the random\n instance \u03b8 as \u03c30 decreases, while the frequentist algorithm is oblivious to the prior.\n 4.3     BayesUCB in Bernoulli Bandit\n Theorem 1 and Corollary 2 can be straightforwardly extended to Bernoulli bandits because\n                                                P    |\u03b8a \u2212    \u02c6\n                                                              \u03b8t,a| \u2265    Ct,a     Ht     \u2264   2\u03b4\n holds for any action a and history Ht (Section 3.2). We state the extension below and prove it in\n Appendix A.5.\n Theorem 5. For any \u03b5 > 0 and \u03b4 \u2208                       (0, 1), the n-round Bayes regret of BayesUCB in a K-armed\n Bernoulli bandit is bounded as\n                                              \uf8ee          2 log(1/\u03b4)                                    \uf8f9\n                              R(n) \u2264       E  \uf8f0 a\u0338=A\u2217         \u2206\u03b5 a       \u2212   (\u03b1a + \u03b2a + 1)\u2206\u03b5         a \uf8fb  + C ,\n where C = \u03b5n + 2Kn\u03b4 is a low-order term.\nMoreover, let \u03bb = mina\u2208A \u03b1a + \u03b2a + 1 and \u03bb \u2264                             2 log(1/\u03b4) n2 log log n. Then\n         R(n) \u2264         2 log(1/\u03b4) log n \u2212              \u03bb                      ha(\u03b8\u2217  a \u2212   \u03bea(\u03b8\u2217 a)) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a + C ,\n                                                    2 log n      a\u2208A     \u03b8\u2212a\n where C = 2K log(1/\u03b4)\u221alog n + 2Kn\u03b4 + 1 is a low-order term.\n                                                                       6", "md": "Lemma 3. In a K-armed Gaussian bandit with prior \\(h(\\cdot) = \\mathcal{N}(\\cdot; \\mu_0, \\sigma^2_0I_K)\\), we have\n\n$$\n\\begin{align*}\n&\\frac{h_a(\\theta^*_a)}{h_{-a}(\\theta_{-a})} \\int_{\\theta_{-a}} \\leq \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} \\exp\\left(-(\\mu_{0,a} - 4\\sigma^2\\mu_{0,a'})^2\\right) \\\\\n&a\\in A, \\theta_{-a} \\quad \\quad a\\in A, a'\\neq a\n\\end{align*}\n$$\nThe bound is proved in Appendix A.3 and has several interesting properties that capture low-regret regimes. First, as the prior becomes more informative and concentrated, \\(\\sigma_0 \\rightarrow 0\\), the bound goes to zero. Second, when the gaps of bandit instances sampled from the prior are large, low regret is also expected. This can happen when the prior means become more separated, \\(|\\mu_{0,a} - \\mu_{0,a'}| \\rightarrow \\infty\\), or the prior becomes wider, \\(\\sigma_0 \\rightarrow \\infty\\). Our bound goes to zero in both of these cases. This also implies that Bayes regret bounds are not necessarily monotone in prior parameters, such as \\(\\sigma_0\\).\n\n4.2 UCB1 in Gaussian Bandit\n\nUsing a similar approach, we prove a Bayes regret bound for UCB1 [Auer et al., 2002]. We view it as BayesUCB (Section 3.1) where \\(\\sigma_0 = \\infty\\) and each action \\(a \\in A\\) is initially taken once at round \\(t = a\\). This generalizes classic UCB1 to \\(\\sigma^2\\)-sub-Gaussian noise. An asymptotic Bayes regret bound for UCB1 was proved by Lai [1987] (claim (i) in their Theorem 3). We derive a finite-time prior-dependent Bayes regret bound below.\n\n$$\n\\begin{align*}\nR(n) &\\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\int_{\\theta_{-a}} h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a} + C, \\\\\n&\\text{where } C = 8\\sigma^2K \\log\\left(\\frac{1}{\\delta}\\right) \\sqrt{\\log n} + 2\\left(2\\log\\left(\\frac{1}{\\delta}\\right) + 2K\\right)\\sigma Kn\\delta + \\int_{a\\in A} E[\\Delta_a] + 1.\n\\end{align*}\n$$\nThe proof is in Appendix A.4. For \\(\\delta = 1/n\\), the bound is \\(O(c h \\log^2 n)\\) and similar to Corollary 2. The main difference is in the additional factor \\(2\\sigma^2_0 \\log n\\) in Corollary 2, which decreases the bound. This means that the regret of BayesUCB improves as \\(\\sigma_0\\) decreases while that of UCB1 may not change much. In fact, the regret bound of BayesUCB is \\(O(1)\\) as \\(\\sigma_0 \\rightarrow 0\\) (Appendix B) while that of UCB1 remains logarithmic. This is expected because BayesUCB has more information about the random instance \\(\\theta\\) as \\(\\sigma_0\\) decreases, while the frequentist algorithm is oblivious to the prior.\n\n4.3 BayesUCB in Bernoulli Bandit\n\nTheorem 1 and Corollary 2 can be straightforwardly extended to Bernoulli bandits because \\(P \\left(|\\theta_a - \\hat{\\theta}_{t,a}| \\geq C_{t,a}\\right) \\leq 2\\delta\\) holds for any action \\(a\\) and history \\(H_t\\) (Section 3.2). We state the extension below and prove it in Appendix A.5.\n\n$$\n\\begin{align*}\nR(n) &\\leq E\\left[\\sum_{a\\neq A^*} 2\\log\\left(\\frac{1}{\\delta}\\right) \\Delta_{\\epsilon_a} - (\\alpha_a + \\beta_a + 1)\\Delta_{\\epsilon_a}\\right] + C, \\\\\n&\\text{where } C = \\epsilon n + 2Kn\\delta \\text{ is a low-order term.}\n\\end{align*}\n$$\nMoreover, let \\(\\lambda = \\min_{a\\in A} (\\alpha_a + \\beta_a + 1)\\) and \\(\\lambda \\leq 2\\log\\left(\\frac{1}{\\delta}\\right) n^2 \\log\\log n\\). Then\n\n$$\n\\begin{align*}\nR(n) &\\leq 2\\log\\left(\\frac{1}{\\delta}\\right) \\log n - \\int_{\\theta_{-a}} \\lambda h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a} + C, \\\\\n&\\text{where } C = 2K \\log\\left(\\frac{1}{\\delta}\\right) \\sqrt{\\log n} + 2Kn\\delta + 1 \\text{ is a low-order term.}\n\\end{align*}\n$$", "images": [], "items": [{"type": "text", "value": "Lemma 3. In a K-armed Gaussian bandit with prior \\(h(\\cdot) = \\mathcal{N}(\\cdot; \\mu_0, \\sigma^2_0I_K)\\), we have\n\n$$\n\\begin{align*}\n&\\frac{h_a(\\theta^*_a)}{h_{-a}(\\theta_{-a})} \\int_{\\theta_{-a}} \\leq \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} \\exp\\left(-(\\mu_{0,a} - 4\\sigma^2\\mu_{0,a'})^2\\right) \\\\\n&a\\in A, \\theta_{-a} \\quad \\quad a\\in A, a'\\neq a\n\\end{align*}\n$$\nThe bound is proved in Appendix A.3 and has several interesting properties that capture low-regret regimes. First, as the prior becomes more informative and concentrated, \\(\\sigma_0 \\rightarrow 0\\), the bound goes to zero. Second, when the gaps of bandit instances sampled from the prior are large, low regret is also expected. This can happen when the prior means become more separated, \\(|\\mu_{0,a} - \\mu_{0,a'}| \\rightarrow \\infty\\), or the prior becomes wider, \\(\\sigma_0 \\rightarrow \\infty\\). Our bound goes to zero in both of these cases. This also implies that Bayes regret bounds are not necessarily monotone in prior parameters, such as \\(\\sigma_0\\).\n\n4.2 UCB1 in Gaussian Bandit\n\nUsing a similar approach, we prove a Bayes regret bound for UCB1 [Auer et al., 2002]. We view it as BayesUCB (Section 3.1) where \\(\\sigma_0 = \\infty\\) and each action \\(a \\in A\\) is initially taken once at round \\(t = a\\). This generalizes classic UCB1 to \\(\\sigma^2\\)-sub-Gaussian noise. An asymptotic Bayes regret bound for UCB1 was proved by Lai [1987] (claim (i) in their Theorem 3). We derive a finite-time prior-dependent Bayes regret bound below.\n\n$$\n\\begin{align*}\nR(n) &\\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\int_{\\theta_{-a}} h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a} + C, \\\\\n&\\text{where } C = 8\\sigma^2K \\log\\left(\\frac{1}{\\delta}\\right) \\sqrt{\\log n} + 2\\left(2\\log\\left(\\frac{1}{\\delta}\\right) + 2K\\right)\\sigma Kn\\delta + \\int_{a\\in A} E[\\Delta_a] + 1.\n\\end{align*}\n$$\nThe proof is in Appendix A.4. For \\(\\delta = 1/n\\), the bound is \\(O(c h \\log^2 n)\\) and similar to Corollary 2. The main difference is in the additional factor \\(2\\sigma^2_0 \\log n\\) in Corollary 2, which decreases the bound. This means that the regret of BayesUCB improves as \\(\\sigma_0\\) decreases while that of UCB1 may not change much. In fact, the regret bound of BayesUCB is \\(O(1)\\) as \\(\\sigma_0 \\rightarrow 0\\) (Appendix B) while that of UCB1 remains logarithmic. This is expected because BayesUCB has more information about the random instance \\(\\theta\\) as \\(\\sigma_0\\) decreases, while the frequentist algorithm is oblivious to the prior.\n\n4.3 BayesUCB in Bernoulli Bandit\n\nTheorem 1 and Corollary 2 can be straightforwardly extended to Bernoulli bandits because \\(P \\left(|\\theta_a - \\hat{\\theta}_{t,a}| \\geq C_{t,a}\\right) \\leq 2\\delta\\) holds for any action \\(a\\) and history \\(H_t\\) (Section 3.2). We state the extension below and prove it in Appendix A.5.\n\n$$\n\\begin{align*}\nR(n) &\\leq E\\left[\\sum_{a\\neq A^*} 2\\log\\left(\\frac{1}{\\delta}\\right) \\Delta_{\\epsilon_a} - (\\alpha_a + \\beta_a + 1)\\Delta_{\\epsilon_a}\\right] + C, \\\\\n&\\text{where } C = \\epsilon n + 2Kn\\delta \\text{ is a low-order term.}\n\\end{align*}\n$$\nMoreover, let \\(\\lambda = \\min_{a\\in A} (\\alpha_a + \\beta_a + 1)\\) and \\(\\lambda \\leq 2\\log\\left(\\frac{1}{\\delta}\\right) n^2 \\log\\log n\\). Then\n\n$$\n\\begin{align*}\nR(n) &\\leq 2\\log\\left(\\frac{1}{\\delta}\\right) \\log n - \\int_{\\theta_{-a}} \\lambda h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a} + C, \\\\\n&\\text{where } C = 2K \\log\\left(\\frac{1}{\\delta}\\right) \\sqrt{\\log n} + 2Kn\\delta + 1 \\text{ is a low-order term.}\n\\end{align*}\n$$", "md": "Lemma 3. In a K-armed Gaussian bandit with prior \\(h(\\cdot) = \\mathcal{N}(\\cdot; \\mu_0, \\sigma^2_0I_K)\\), we have\n\n$$\n\\begin{align*}\n&\\frac{h_a(\\theta^*_a)}{h_{-a}(\\theta_{-a})} \\int_{\\theta_{-a}} \\leq \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} \\exp\\left(-(\\mu_{0,a} - 4\\sigma^2\\mu_{0,a'})^2\\right) \\\\\n&a\\in A, \\theta_{-a} \\quad \\quad a\\in A, a'\\neq a\n\\end{align*}\n$$\nThe bound is proved in Appendix A.3 and has several interesting properties that capture low-regret regimes. First, as the prior becomes more informative and concentrated, \\(\\sigma_0 \\rightarrow 0\\), the bound goes to zero. Second, when the gaps of bandit instances sampled from the prior are large, low regret is also expected. This can happen when the prior means become more separated, \\(|\\mu_{0,a} - \\mu_{0,a'}| \\rightarrow \\infty\\), or the prior becomes wider, \\(\\sigma_0 \\rightarrow \\infty\\). Our bound goes to zero in both of these cases. This also implies that Bayes regret bounds are not necessarily monotone in prior parameters, such as \\(\\sigma_0\\).\n\n4.2 UCB1 in Gaussian Bandit\n\nUsing a similar approach, we prove a Bayes regret bound for UCB1 [Auer et al., 2002]. We view it as BayesUCB (Section 3.1) where \\(\\sigma_0 = \\infty\\) and each action \\(a \\in A\\) is initially taken once at round \\(t = a\\). This generalizes classic UCB1 to \\(\\sigma^2\\)-sub-Gaussian noise. An asymptotic Bayes regret bound for UCB1 was proved by Lai [1987] (claim (i) in their Theorem 3). We derive a finite-time prior-dependent Bayes regret bound below.\n\n$$\n\\begin{align*}\nR(n) &\\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\int_{\\theta_{-a}} h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a} + C, \\\\\n&\\text{where } C = 8\\sigma^2K \\log\\left(\\frac{1}{\\delta}\\right) \\sqrt{\\log n} + 2\\left(2\\log\\left(\\frac{1}{\\delta}\\right) + 2K\\right)\\sigma Kn\\delta + \\int_{a\\in A} E[\\Delta_a] + 1.\n\\end{align*}\n$$\nThe proof is in Appendix A.4. For \\(\\delta = 1/n\\), the bound is \\(O(c h \\log^2 n)\\) and similar to Corollary 2. The main difference is in the additional factor \\(2\\sigma^2_0 \\log n\\) in Corollary 2, which decreases the bound. This means that the regret of BayesUCB improves as \\(\\sigma_0\\) decreases while that of UCB1 may not change much. In fact, the regret bound of BayesUCB is \\(O(1)\\) as \\(\\sigma_0 \\rightarrow 0\\) (Appendix B) while that of UCB1 remains logarithmic. This is expected because BayesUCB has more information about the random instance \\(\\theta\\) as \\(\\sigma_0\\) decreases, while the frequentist algorithm is oblivious to the prior.\n\n4.3 BayesUCB in Bernoulli Bandit\n\nTheorem 1 and Corollary 2 can be straightforwardly extended to Bernoulli bandits because \\(P \\left(|\\theta_a - \\hat{\\theta}_{t,a}| \\geq C_{t,a}\\right) \\leq 2\\delta\\) holds for any action \\(a\\) and history \\(H_t\\) (Section 3.2). We state the extension below and prove it in Appendix A.5.\n\n$$\n\\begin{align*}\nR(n) &\\leq E\\left[\\sum_{a\\neq A^*} 2\\log\\left(\\frac{1}{\\delta}\\right) \\Delta_{\\epsilon_a} - (\\alpha_a + \\beta_a + 1)\\Delta_{\\epsilon_a}\\right] + C, \\\\\n&\\text{where } C = \\epsilon n + 2Kn\\delta \\text{ is a low-order term.}\n\\end{align*}\n$$\nMoreover, let \\(\\lambda = \\min_{a\\in A} (\\alpha_a + \\beta_a + 1)\\) and \\(\\lambda \\leq 2\\log\\left(\\frac{1}{\\delta}\\right) n^2 \\log\\log n\\). Then\n\n$$\n\\begin{align*}\nR(n) &\\leq 2\\log\\left(\\frac{1}{\\delta}\\right) \\log n - \\int_{\\theta_{-a}} \\lambda h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta_{-a}) d\\theta_{-a} + C, \\\\\n&\\text{where } C = 2K \\log\\left(\\frac{1}{\\delta}\\right) \\sqrt{\\log n} + 2Kn\\delta + 1 \\text{ is a low-order term.}\n\\end{align*}\n$$"}]}, {"page": 7, "text": " 4.4     BayesUCB in Linear Bandit\n Now we present a gap-dependent Bayes regret bound for BayesUCB in a linear bandit with a finite\n number of actions. The bound depends on a random minimum gap. To control the gap, we clip it as\n\u2206\u03b5 min = max {\u2206min, \u03b5}.\n Theorem 6. Suppose that \u2225\u03b8\u22252 \u2264                     L\u2217   holds with probability at least 1 \u2212               \u03b4\u2217. Let \u2225a\u22252 \u2264         L for any\n action a \u2208      A. Then for any \u03b5 > 0 and \u03b4 \u2208                  (0, 1), the n-round Bayes regret of linear BayesUCB is\n bounded as\n        R(n) \u2264      8E         1              \u03c320,maxd           log     1 + \u03c32   0,maxn        log(1/\u03b4) + \u03b5n + 4LL\u2217Kn\u03b4\n                            \u2206\u03b5                       \u03c320,max                       \u03c32d\n                               min     log    1 +       \u03c32\n with probability at least 1 \u2212            \u03b4\u2217, where \u03c30,max =               \u03bb1(\u03a30)L.\n The proof is in Appendix A.6. For \u03b5 = 1/n and \u03b4 = 1/n, the bound is O(d c\u2206                                        log2 n), where c\u2206          is\n a constant depending on the gaps of random bandit instances. The bound is remarkably similar to the\n frequentist O(d \u2206\u22121       min log2 n) bound in Theorem 5 of Abbasi-Yadkori et al. [2011], where \u2206min is\n the minimum gap. There are two differences. First, we integrate \u2206\u22121                               min over the prior. Second, our\n bound decreases as the prior becomes more informative, \u03c30,max \u2192                                  0.\n In a Gaussian bandit, the bound becomes O(KE [1/\u2206\u03b5                              min] log2 n). Therefore, it is comparable to\n Theorem 1 up to an additional logarithmic factor in n. This is due to a more general proof technique,\n which allows for dependencies between the mean rewards of actions. We also note that Theorem 6\n does not assume that the prior is factored, unlike Theorem 1.\n 5     Comparison to Prior Works\n This section is organized as follows. In Section 5.1, we show that the bound in Theorem 5 matches\n an existing lower bound of Lai [1987] asymptotically. In Section 5.2, we compare our logarithmic\n bounds to prior \u02dc      O(\u221an) bounds. Finally, in Section 5.3, we outline the key steps in our analyses and\n how they differ from prior works.\n 5.1     Matching Lower Bound\n In frequentist bandit analyses, it is standard to compare asymptotic lower bounds to finite-time upper\n bounds because finite-time logarithmic lower bounds do not exist [Lattimore and Szepesvari, 2019].\nWe follow the same approach when arguing that our finite-time upper bounds are order optimal.\n The results in Lai [1987] are for single-parameter exponential-family reward distributions, which\n excludes Gaussian rewards. Therefore, we argue about the tightness of Theorem 5 only. Specifically,                         \u03bb\n we take the second bound in Theorem 5, set \u03b4 = 1/n, and let n \u2192                                  \u221e. In this case,        2 log n \u2192     0 and\n \u03bea(\u00b7) \u2192     0, and the bound matches up to constant factors the lower bound in Lai [1987] (claim (ii) in\n their Theorem 3), which is\n                                      \u2126     log2 n    a\u2208A     \u03b8\u2212a   ha(\u03b8\u2217  a) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a               .                                 (2)\n Lai [1987] also matched this lower bound with an asymptotic upper bound for a frequentist policy.\n Our finite-time upper bounds also reveal an interesting difference from the asymptotic lower bound\n in (2), which may deserve more future attention. More specifically, the regret bound of BayesUCB\n (Corollary 2) improves with prior information while that of UCB1 (Theorem 4) does not. We observe\n these improvements empirically as well (Section 6 and Appendix D). However, both bounds are the\n same asymptotically. This is because the benefit of knowing the prior vanishes in asymptotic analyses,\n              \u03c32\n since    2\u03c320 log n \u2192     0 in Corollary 2 as n \u2192              \u221e. This motivates the need for finite-time logarithmic\n Bayes regret lower bounds, which do not exist.\n                                                                       7", "md": "## 4.4 BayesUCB in Linear Bandit\n\nNow we present a gap-dependent Bayes regret bound for BayesUCB in a linear bandit with a finite number of actions. The bound depends on a random minimum gap. To control the gap, we clip it as $$\\Delta_{\\epsilon}^{min} = \\max\\{\\Delta_{min}, \\epsilon\\}$$.\n\nTheorem 6. Suppose that $$\\|\\theta\\|_{2} \\leq L^{*}$$ holds with probability at least $$1 - \\delta^{*}$$. Let $$\\|a\\|_{2} \\leq L$$ for any action $$a \\in A$$. Then for any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, the n-round Bayes regret of linear BayesUCB is bounded as\n\n$$\nR(n) \\leq 8E\\left[\\frac{1}{\\Delta_{\\epsilon}^{min}} \\log\\left(1 + \\frac{\\sigma_{0}^{2}d}{\\sigma_{0,max}}\\right) \\log\\left(\\frac{1}{\\delta}\\right) + \\epsilon n + 4LL^{*}Kn\\delta\\right]\n$$\n\nwith probability at least $$1 - \\delta^{*}$$, where $$\\sigma_{0,max} = \\lambda_{1}(\\Sigma_{0})L$$. The proof is in Appendix A.6. For $$\\epsilon = 1/n$$ and $$\\delta = 1/n$$, the bound is $$O(d c\\Delta \\log^{2} n)$$, where $$c\\Delta$$ is a constant depending on the gaps of random bandit instances. The bound is remarkably similar to the frequentist $$O(d \\Delta^{-1} \\log^{2} n)$$ bound in Theorem 5 of Abbasi-Yadkori et al. [2011], where $$\\Delta_{min}$$ is the minimum gap. There are two differences. First, we integrate $$\\Delta^{-1}_{min}$$ over the prior. Second, our bound decreases as the prior becomes more informative, $$\\sigma_{0,max} \\rightarrow 0$$.\n\nIn a Gaussian bandit, the bound becomes $$O(KE[1/\\Delta_{\\epsilon}^{min}] \\log^{2} n)$$. Therefore, it is comparable to Theorem 1 up to an additional logarithmic factor in n. This is due to a more general proof technique, which allows for dependencies between the mean rewards of actions. We also note that Theorem 6 does not assume that the prior is factored, unlike Theorem 1.\n\n## 5 Comparison to Prior Works\n\nThis section is organized as follows. In Section 5.1, we show that the bound in Theorem 5 matches an existing lower bound of Lai [1987] asymptotically. In Section 5.2, we compare our logarithmic bounds to prior $$\\tilde{O}(\\sqrt{n})$$ bounds. Finally, in Section 5.3, we outline the key steps in our analyses and how they differ from prior works.\n\n### 5.1 Matching Lower Bound\n\nIn frequentist bandit analyses, it is standard to compare asymptotic lower bounds to finite-time upper bounds because finite-time logarithmic lower bounds do not exist [Lattimore and Szepesvari, 2019]. We follow the same approach when arguing that our finite-time upper bounds are order optimal.\n\nThe results in Lai [1987] are for single-parameter exponential-family reward distributions, which excludes Gaussian rewards. Therefore, we argue about the tightness of Theorem 5 only. Specifically, we take the second bound in Theorem 5, set $$\\delta = 1/n$$, and let $$n \\rightarrow \\infty$$. In this case, $$2 \\log n \\rightarrow 0$$ and $$\\xi_{a}(\\cdot) \\rightarrow 0$$, and the bound matches up to constant factors the lower bound in Lai [1987] (claim (ii) in their Theorem 3), which is\n\n$$\n\\Omega \\log^{2} n \\sum_{a \\in A} \\theta_{-a} h_{a}(\\theta^{*}_{a}) h_{-a}(\\theta_{-a}) d\\theta_{-a}.\n$$\n\nLai [1987] also matched this lower bound with an asymptotic upper bound for a frequentist policy. Our finite-time upper bounds also reveal an interesting difference from the asymptotic lower bound in (2), which may deserve more future attention. More specifically, the regret bound of BayesUCB (Corollary 2) improves with prior information while that of UCB1 (Theorem 4) does not. We observe these improvements empirically as well (Section 6 and Appendix D). However, both bounds are the same asymptotically. This is because the benefit of knowing the prior vanishes in asymptotic analyses, since $$2\\sigma_{0}^{2} \\log n \\rightarrow 0$$ in Corollary 2 as $$n \\rightarrow \\infty$$. This motivates the need for finite-time logarithmic Bayes regret lower bounds, which do not exist.", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "4.4 BayesUCB in Linear Bandit", "md": "## 4.4 BayesUCB in Linear Bandit"}, {"type": "text", "value": "Now we present a gap-dependent Bayes regret bound for BayesUCB in a linear bandit with a finite number of actions. The bound depends on a random minimum gap. To control the gap, we clip it as $$\\Delta_{\\epsilon}^{min} = \\max\\{\\Delta_{min}, \\epsilon\\}$$.\n\nTheorem 6. Suppose that $$\\|\\theta\\|_{2} \\leq L^{*}$$ holds with probability at least $$1 - \\delta^{*}$$. Let $$\\|a\\|_{2} \\leq L$$ for any action $$a \\in A$$. Then for any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, the n-round Bayes regret of linear BayesUCB is bounded as\n\n$$\nR(n) \\leq 8E\\left[\\frac{1}{\\Delta_{\\epsilon}^{min}} \\log\\left(1 + \\frac{\\sigma_{0}^{2}d}{\\sigma_{0,max}}\\right) \\log\\left(\\frac{1}{\\delta}\\right) + \\epsilon n + 4LL^{*}Kn\\delta\\right]\n$$\n\nwith probability at least $$1 - \\delta^{*}$$, where $$\\sigma_{0,max} = \\lambda_{1}(\\Sigma_{0})L$$. The proof is in Appendix A.6. For $$\\epsilon = 1/n$$ and $$\\delta = 1/n$$, the bound is $$O(d c\\Delta \\log^{2} n)$$, where $$c\\Delta$$ is a constant depending on the gaps of random bandit instances. The bound is remarkably similar to the frequentist $$O(d \\Delta^{-1} \\log^{2} n)$$ bound in Theorem 5 of Abbasi-Yadkori et al. [2011], where $$\\Delta_{min}$$ is the minimum gap. There are two differences. First, we integrate $$\\Delta^{-1}_{min}$$ over the prior. Second, our bound decreases as the prior becomes more informative, $$\\sigma_{0,max} \\rightarrow 0$$.\n\nIn a Gaussian bandit, the bound becomes $$O(KE[1/\\Delta_{\\epsilon}^{min}] \\log^{2} n)$$. Therefore, it is comparable to Theorem 1 up to an additional logarithmic factor in n. This is due to a more general proof technique, which allows for dependencies between the mean rewards of actions. We also note that Theorem 6 does not assume that the prior is factored, unlike Theorem 1.", "md": "Now we present a gap-dependent Bayes regret bound for BayesUCB in a linear bandit with a finite number of actions. The bound depends on a random minimum gap. To control the gap, we clip it as $$\\Delta_{\\epsilon}^{min} = \\max\\{\\Delta_{min}, \\epsilon\\}$$.\n\nTheorem 6. Suppose that $$\\|\\theta\\|_{2} \\leq L^{*}$$ holds with probability at least $$1 - \\delta^{*}$$. Let $$\\|a\\|_{2} \\leq L$$ for any action $$a \\in A$$. Then for any $$\\epsilon > 0$$ and $$\\delta \\in (0, 1)$$, the n-round Bayes regret of linear BayesUCB is bounded as\n\n$$\nR(n) \\leq 8E\\left[\\frac{1}{\\Delta_{\\epsilon}^{min}} \\log\\left(1 + \\frac{\\sigma_{0}^{2}d}{\\sigma_{0,max}}\\right) \\log\\left(\\frac{1}{\\delta}\\right) + \\epsilon n + 4LL^{*}Kn\\delta\\right]\n$$\n\nwith probability at least $$1 - \\delta^{*}$$, where $$\\sigma_{0,max} = \\lambda_{1}(\\Sigma_{0})L$$. The proof is in Appendix A.6. For $$\\epsilon = 1/n$$ and $$\\delta = 1/n$$, the bound is $$O(d c\\Delta \\log^{2} n)$$, where $$c\\Delta$$ is a constant depending on the gaps of random bandit instances. The bound is remarkably similar to the frequentist $$O(d \\Delta^{-1} \\log^{2} n)$$ bound in Theorem 5 of Abbasi-Yadkori et al. [2011], where $$\\Delta_{min}$$ is the minimum gap. There are two differences. First, we integrate $$\\Delta^{-1}_{min}$$ over the prior. Second, our bound decreases as the prior becomes more informative, $$\\sigma_{0,max} \\rightarrow 0$$.\n\nIn a Gaussian bandit, the bound becomes $$O(KE[1/\\Delta_{\\epsilon}^{min}] \\log^{2} n)$$. Therefore, it is comparable to Theorem 1 up to an additional logarithmic factor in n. This is due to a more general proof technique, which allows for dependencies between the mean rewards of actions. We also note that Theorem 6 does not assume that the prior is factored, unlike Theorem 1."}, {"type": "heading", "lvl": 2, "value": "5 Comparison to Prior Works", "md": "## 5 Comparison to Prior Works"}, {"type": "text", "value": "This section is organized as follows. In Section 5.1, we show that the bound in Theorem 5 matches an existing lower bound of Lai [1987] asymptotically. In Section 5.2, we compare our logarithmic bounds to prior $$\\tilde{O}(\\sqrt{n})$$ bounds. Finally, in Section 5.3, we outline the key steps in our analyses and how they differ from prior works.", "md": "This section is organized as follows. In Section 5.1, we show that the bound in Theorem 5 matches an existing lower bound of Lai [1987] asymptotically. In Section 5.2, we compare our logarithmic bounds to prior $$\\tilde{O}(\\sqrt{n})$$ bounds. Finally, in Section 5.3, we outline the key steps in our analyses and how they differ from prior works."}, {"type": "heading", "lvl": 3, "value": "5.1 Matching Lower Bound", "md": "### 5.1 Matching Lower Bound"}, {"type": "text", "value": "In frequentist bandit analyses, it is standard to compare asymptotic lower bounds to finite-time upper bounds because finite-time logarithmic lower bounds do not exist [Lattimore and Szepesvari, 2019]. We follow the same approach when arguing that our finite-time upper bounds are order optimal.\n\nThe results in Lai [1987] are for single-parameter exponential-family reward distributions, which excludes Gaussian rewards. Therefore, we argue about the tightness of Theorem 5 only. Specifically, we take the second bound in Theorem 5, set $$\\delta = 1/n$$, and let $$n \\rightarrow \\infty$$. In this case, $$2 \\log n \\rightarrow 0$$ and $$\\xi_{a}(\\cdot) \\rightarrow 0$$, and the bound matches up to constant factors the lower bound in Lai [1987] (claim (ii) in their Theorem 3), which is\n\n$$\n\\Omega \\log^{2} n \\sum_{a \\in A} \\theta_{-a} h_{a}(\\theta^{*}_{a}) h_{-a}(\\theta_{-a}) d\\theta_{-a}.\n$$\n\nLai [1987] also matched this lower bound with an asymptotic upper bound for a frequentist policy. Our finite-time upper bounds also reveal an interesting difference from the asymptotic lower bound in (2), which may deserve more future attention. More specifically, the regret bound of BayesUCB (Corollary 2) improves with prior information while that of UCB1 (Theorem 4) does not. We observe these improvements empirically as well (Section 6 and Appendix D). However, both bounds are the same asymptotically. This is because the benefit of knowing the prior vanishes in asymptotic analyses, since $$2\\sigma_{0}^{2} \\log n \\rightarrow 0$$ in Corollary 2 as $$n \\rightarrow \\infty$$. This motivates the need for finite-time logarithmic Bayes regret lower bounds, which do not exist.", "md": "In frequentist bandit analyses, it is standard to compare asymptotic lower bounds to finite-time upper bounds because finite-time logarithmic lower bounds do not exist [Lattimore and Szepesvari, 2019]. We follow the same approach when arguing that our finite-time upper bounds are order optimal.\n\nThe results in Lai [1987] are for single-parameter exponential-family reward distributions, which excludes Gaussian rewards. Therefore, we argue about the tightness of Theorem 5 only. Specifically, we take the second bound in Theorem 5, set $$\\delta = 1/n$$, and let $$n \\rightarrow \\infty$$. In this case, $$2 \\log n \\rightarrow 0$$ and $$\\xi_{a}(\\cdot) \\rightarrow 0$$, and the bound matches up to constant factors the lower bound in Lai [1987] (claim (ii) in their Theorem 3), which is\n\n$$\n\\Omega \\log^{2} n \\sum_{a \\in A} \\theta_{-a} h_{a}(\\theta^{*}_{a}) h_{-a}(\\theta_{-a}) d\\theta_{-a}.\n$$\n\nLai [1987] also matched this lower bound with an asymptotic upper bound for a frequentist policy. Our finite-time upper bounds also reveal an interesting difference from the asymptotic lower bound in (2), which may deserve more future attention. More specifically, the regret bound of BayesUCB (Corollary 2) improves with prior information while that of UCB1 (Theorem 4) does not. We observe these improvements empirically as well (Section 6 and Appendix D). However, both bounds are the same asymptotically. This is because the benefit of knowing the prior vanishes in asymptotic analyses, since $$2\\sigma_{0}^{2} \\log n \\rightarrow 0$$ in Corollary 2 as $$n \\rightarrow \\infty$$. This motivates the need for finite-time logarithmic Bayes regret lower bounds, which do not exist."}]}, {"page": 8, "text": " 5.2     Prior Bayes Regret Upper Bounds\n Theorem 1 and Corollary 2 are major improvements upon existing \u02dc                              O(\u221an) bounds. For instance, take\n a prior-dependent bound in Lemma 4 of Kveton et al. [2021], which holds for both BayesUCB and\n Thompson sampling due to a well-known equivalence of their analyses [Russo and Van Roy, 2014,\n Hong et al., 2020]. For \u03b4 = 1/n, their leading term becomes\n                                   4    2\u03c32K log n              n + \u03c32\u03c3\u22122    0 K \u2212          \u03c32\u03c3\u22122 0 K        .                              (3)\n Similarly to Theorem 1 and Corollary 2, (3) decreases as the prior concentrates and becomes more\n informative, \u03c30 \u2192           0. However, the bound is \u02dc           O(\u221an). Moreover, it does not depend on prior means\n \u00b50 or the gaps of random bandit instances. Therefore, it cannot capture low-regret regimes due to\n large random gaps \u2206\u03b5          a in Theorem 1 or a small complexity term in Corollary 2. We demonstrate it\n empirically in Section 6.\nWhen the random gaps \u2206\u03b5              a in Theorem 1 are small or the complexity term in Corollary 2 is large, our\n bounds can be worse than \u02dc             O(\u221an) bounds. This is analogous to the relation of the gap-dependent\n and gap-free frequentist bounds [Lattimore and Szepesvari, 2019]. Specifically, a gap-dependent\n bound of UCB1 in a K-armed bandit with 1-sub-Gaussian rewards (Theorem 7.1) is O(K\u2206\u22121                                           min log n),\n where \u2206min is the minimum gap. A corresponding gap-free bound (Theorem 7.2) is O(\u221aKn log n).\n The latter is smaller when the gap is small, \u2206min = o(                             (K log n)/n). To get the best bound, the\n minimum of the two should be taken, and the same is true in our Bayesian setting.\n No prior-dependent Bayes regret lower bound exists in linear bandits. Thus we treat E [1/\u2206\u03b5                                          min] in\n Theorem 6 as the complexity term and do not further bound it as in Corollary 2. To compare our\n bound fairly to existing \u02dc        O(\u221an) bounds, we derive an \u02dc                O(\u221an) bound in Appendix C, by a relatively\n minor change in the proof of Theorem 6. A similar bound can be obtained by adapting the proofs of\n Lu and Van Roy [2019] and Hong et al. [2022] to a linear bandit with a finite number of actions. The\n leading term of the bound is\n                                  2         2\u03c32 0,maxdn           log     1 + \u03c32   0,maxn        log(1/\u03b4) .                                 (4)\n                                        log    1 +    \u03c320,max                       \u03c32d\n                                                        \u03c32\n Similarly to Theorem 6, (4) decreases as the prior becomes more informative, \u03c30,max \u2192                                         0. However,\n the bound is \u02dc    O(\u221an) and does not depend on the gaps of random bandit instances. Hence it cannot\n capture low-regret regimes due to a large random minimum gap \u2206min in Theorem 6. We validate it\n empirically in Section 6.\n 5.3     Technical Novelty\n All modern Bayesian analyses follow Russo and Van Roy [2014], who derived the first finite-time\n \u02dc\n O(\u221an) Bayes regret bounds for BayesUCB and Thompson sampling. The key idea in their analyses\n is that conditioned on history, the optimal and taken actions are identically distributed, and that the\n upper confidence bounds are deterministic functions of the history. This is where the randomness of\n instances in Bayesian bandits is used. Using this, the regret at round t is bounded by the confidence\n interval width of the taken action, and the usual \u02dc                  O(\u221an) bounds can be obtained by summing up the\n confidence interval widths over n rounds.\n The main difference in our work is that we first bound the regret in a fixed bandit instance, similarly\n to frequentist analyses. The bound involves \u2206\u22121                    a    and is derived using biased Bayesian confidence\n intervals. The rest of our analysis is Bayesian in two parts: we prove that the confidence intervals\n fail with a low probability and bound random \u2206\u22121                    a , following a similar technique to Lai [1987]. The\n resulting logarithmic Bayes regret bounds cannot be derived using the techniques of Russo and Van\n Roy [2014], as these become loose when the confidence interval widths are introduced.\n Asymptotic logarithmic Bayes regret bounds were derived in Lai [1987]. From this analysis, we use\n only the technique for bounding \u2206\u22121               a    when proving Corollary 2 and Theorem 5. The central part of\n our proof is a finite-time per-instance bound on the number of times that a suboptimal action is taken.\n This quantity is bounded based on the assumption that the action with the highest UCB is taken. A\n                                                                       8", "md": "# Bayes Regret Upper Bounds\n\n## 5.2 Prior Bayes Regret Upper Bounds\n\nTheorem 1 and Corollary 2 are major improvements upon existing \u02dc O(\u221an) bounds. For instance, take a prior-dependent bound in Lemma 4 of Kveton et al. [2021], which holds for both BayesUCB and Thompson sampling due to a well-known equivalence of their analyses [Russo and Van Roy, 2014, Hong et al., 2020]. For \u03b4 = 1/n, their leading term becomes\n\n$$4\\left(2\\sigma^2K\\log n\\right)\\left(n + \\sigma^2\\sigma^{-2}_0K - \\sigma^2\\sigma^{-2}_0K\\,.$$ (3)\n\nSimilarly to Theorem 1 and Corollary 2, (3) decreases as the prior concentrates and becomes more informative, \u03c30 \u2192 0. However, the bound is \u02dc O(\u221an). Moreover, it does not depend on prior means \u00b50 or the gaps of random bandit instances. Therefore, it cannot capture low-regret regimes due to large random gaps \u2206\u03b5 a in Theorem 1 or a small complexity term in Corollary 2. We demonstrate it empirically in Section 6.\n\nWhen the random gaps \u2206\u03b5 a in Theorem 1 are small or the complexity term in Corollary 2 is large, our bounds can be worse than \u02dc O(\u221an) bounds. This is analogous to the relation of the gap-dependent and gap-free frequentist bounds [Lattimore and Szepesvari, 2019]. Specifically, a gap-dependent bound of UCB1 in a K-armed bandit with 1-sub-Gaussian rewards (Theorem 7.1) is O(K\u2206-1minlog n), where \u2206min is the minimum gap. A corresponding gap-free bound (Theorem 7.2) is O(\u221aKn log n). The latter is smaller when the gap is small, \u2206min = o((K log n)/n). To get the best bound, the minimum of the two should be taken, and the same is true in our Bayesian setting.\n\nNo prior-dependent Bayes regret lower bound exists in linear bandits. Thus we treat E [1/\u2206min] in Theorem 6 as the complexity term and do not further bound it as in Corollary 2. To compare our bound fairly to existing \u02dc O(\u221an) bounds, we derive an \u02dc O(\u221an) bound in Appendix C, by a relatively minor change in the proof of Theorem 6. A similar bound can be obtained by adapting the proofs of Lu and Van Roy [2019] and Hong et al. [2022] to a linear bandit with a finite number of actions. The leading term of the bound is\n\n$$2\\left(2\\sigma^2_{0,\\text{max}}dn\\log\\left(1 + \\frac{\\sigma^2_{0,\\text{max}}n}{\\log(1/\\delta)}\\right)\\right)\\,.$$ (4)\n\nSimilarly to Theorem 6, (4) decreases as the prior becomes more informative, \u03c30,max \u2192 0. However, the bound is \u02dc O(\u221an) and does not depend on the gaps of random bandit instances. Hence it cannot capture low-regret regimes due to a large random minimum gap \u2206min in Theorem 6. We validate it empirically in Section 6.\n\n## 5.3 Technical Novelty\n\nAll modern Bayesian analyses follow Russo and Van Roy [2014], who derived the first finite-time \u02dc O(\u221an) Bayes regret bounds for BayesUCB and Thompson sampling. The key idea in their analyses is that conditioned on history, the optimal and taken actions are identically distributed, and that the upper confidence bounds are deterministic functions of the history. This is where the randomness of instances in Bayesian bandits is used. Using this, the regret at round t is bounded by the confidence interval width of the taken action, and the usual \u02dc O(\u221an) bounds can be obtained by summing up the confidence interval widths over n rounds.\n\nThe main difference in our work is that we first bound the regret in a fixed bandit instance, similarly to frequentist analyses. The bound involves \u2206-1a and is derived using biased Bayesian confidence intervals. The rest of our analysis is Bayesian in two parts: we prove that the confidence intervals fail with a low probability and bound random \u2206-1a, following a similar technique to Lai [1987]. The resulting logarithmic Bayes regret bounds cannot be derived using the techniques of Russo and Van Roy [2014], as these become loose when the confidence interval widths are introduced.\n\nAsymptotic logarithmic Bayes regret bounds were derived in Lai [1987]. From this analysis, we use only the technique for bounding \u2206-1a when proving Corollary 2 and Theorem 5. The central part of our proof is a finite-time per-instance bound on the number of times that a suboptimal action is taken. This quantity is bounded based on the assumption that the action with the highest UCB is taken.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Bayes Regret Upper Bounds", "md": "# Bayes Regret Upper Bounds"}, {"type": "heading", "lvl": 2, "value": "5.2 Prior Bayes Regret Upper Bounds", "md": "## 5.2 Prior Bayes Regret Upper Bounds"}, {"type": "text", "value": "Theorem 1 and Corollary 2 are major improvements upon existing \u02dc O(\u221an) bounds. For instance, take a prior-dependent bound in Lemma 4 of Kveton et al. [2021], which holds for both BayesUCB and Thompson sampling due to a well-known equivalence of their analyses [Russo and Van Roy, 2014, Hong et al., 2020]. For \u03b4 = 1/n, their leading term becomes\n\n$$4\\left(2\\sigma^2K\\log n\\right)\\left(n + \\sigma^2\\sigma^{-2}_0K - \\sigma^2\\sigma^{-2}_0K\\,.$$ (3)\n\nSimilarly to Theorem 1 and Corollary 2, (3) decreases as the prior concentrates and becomes more informative, \u03c30 \u2192 0. However, the bound is \u02dc O(\u221an). Moreover, it does not depend on prior means \u00b50 or the gaps of random bandit instances. Therefore, it cannot capture low-regret regimes due to large random gaps \u2206\u03b5 a in Theorem 1 or a small complexity term in Corollary 2. We demonstrate it empirically in Section 6.\n\nWhen the random gaps \u2206\u03b5 a in Theorem 1 are small or the complexity term in Corollary 2 is large, our bounds can be worse than \u02dc O(\u221an) bounds. This is analogous to the relation of the gap-dependent and gap-free frequentist bounds [Lattimore and Szepesvari, 2019]. Specifically, a gap-dependent bound of UCB1 in a K-armed bandit with 1-sub-Gaussian rewards (Theorem 7.1) is O(K\u2206-1minlog n), where \u2206min is the minimum gap. A corresponding gap-free bound (Theorem 7.2) is O(\u221aKn log n). The latter is smaller when the gap is small, \u2206min = o((K log n)/n). To get the best bound, the minimum of the two should be taken, and the same is true in our Bayesian setting.\n\nNo prior-dependent Bayes regret lower bound exists in linear bandits. Thus we treat E [1/\u2206min] in Theorem 6 as the complexity term and do not further bound it as in Corollary 2. To compare our bound fairly to existing \u02dc O(\u221an) bounds, we derive an \u02dc O(\u221an) bound in Appendix C, by a relatively minor change in the proof of Theorem 6. A similar bound can be obtained by adapting the proofs of Lu and Van Roy [2019] and Hong et al. [2022] to a linear bandit with a finite number of actions. The leading term of the bound is\n\n$$2\\left(2\\sigma^2_{0,\\text{max}}dn\\log\\left(1 + \\frac{\\sigma^2_{0,\\text{max}}n}{\\log(1/\\delta)}\\right)\\right)\\,.$$ (4)\n\nSimilarly to Theorem 6, (4) decreases as the prior becomes more informative, \u03c30,max \u2192 0. However, the bound is \u02dc O(\u221an) and does not depend on the gaps of random bandit instances. Hence it cannot capture low-regret regimes due to a large random minimum gap \u2206min in Theorem 6. We validate it empirically in Section 6.", "md": "Theorem 1 and Corollary 2 are major improvements upon existing \u02dc O(\u221an) bounds. For instance, take a prior-dependent bound in Lemma 4 of Kveton et al. [2021], which holds for both BayesUCB and Thompson sampling due to a well-known equivalence of their analyses [Russo and Van Roy, 2014, Hong et al., 2020]. For \u03b4 = 1/n, their leading term becomes\n\n$$4\\left(2\\sigma^2K\\log n\\right)\\left(n + \\sigma^2\\sigma^{-2}_0K - \\sigma^2\\sigma^{-2}_0K\\,.$$ (3)\n\nSimilarly to Theorem 1 and Corollary 2, (3) decreases as the prior concentrates and becomes more informative, \u03c30 \u2192 0. However, the bound is \u02dc O(\u221an). Moreover, it does not depend on prior means \u00b50 or the gaps of random bandit instances. Therefore, it cannot capture low-regret regimes due to large random gaps \u2206\u03b5 a in Theorem 1 or a small complexity term in Corollary 2. We demonstrate it empirically in Section 6.\n\nWhen the random gaps \u2206\u03b5 a in Theorem 1 are small or the complexity term in Corollary 2 is large, our bounds can be worse than \u02dc O(\u221an) bounds. This is analogous to the relation of the gap-dependent and gap-free frequentist bounds [Lattimore and Szepesvari, 2019]. Specifically, a gap-dependent bound of UCB1 in a K-armed bandit with 1-sub-Gaussian rewards (Theorem 7.1) is O(K\u2206-1minlog n), where \u2206min is the minimum gap. A corresponding gap-free bound (Theorem 7.2) is O(\u221aKn log n). The latter is smaller when the gap is small, \u2206min = o((K log n)/n). To get the best bound, the minimum of the two should be taken, and the same is true in our Bayesian setting.\n\nNo prior-dependent Bayes regret lower bound exists in linear bandits. Thus we treat E [1/\u2206min] in Theorem 6 as the complexity term and do not further bound it as in Corollary 2. To compare our bound fairly to existing \u02dc O(\u221an) bounds, we derive an \u02dc O(\u221an) bound in Appendix C, by a relatively minor change in the proof of Theorem 6. A similar bound can be obtained by adapting the proofs of Lu and Van Roy [2019] and Hong et al. [2022] to a linear bandit with a finite number of actions. The leading term of the bound is\n\n$$2\\left(2\\sigma^2_{0,\\text{max}}dn\\log\\left(1 + \\frac{\\sigma^2_{0,\\text{max}}n}{\\log(1/\\delta)}\\right)\\right)\\,.$$ (4)\n\nSimilarly to Theorem 6, (4) decreases as the prior becomes more informative, \u03c30,max \u2192 0. However, the bound is \u02dc O(\u221an) and does not depend on the gaps of random bandit instances. Hence it cannot capture low-regret regimes due to a large random minimum gap \u2206min in Theorem 6. We validate it empirically in Section 6."}, {"type": "heading", "lvl": 2, "value": "5.3 Technical Novelty", "md": "## 5.3 Technical Novelty"}, {"type": "text", "value": "All modern Bayesian analyses follow Russo and Van Roy [2014], who derived the first finite-time \u02dc O(\u221an) Bayes regret bounds for BayesUCB and Thompson sampling. The key idea in their analyses is that conditioned on history, the optimal and taken actions are identically distributed, and that the upper confidence bounds are deterministic functions of the history. This is where the randomness of instances in Bayesian bandits is used. Using this, the regret at round t is bounded by the confidence interval width of the taken action, and the usual \u02dc O(\u221an) bounds can be obtained by summing up the confidence interval widths over n rounds.\n\nThe main difference in our work is that we first bound the regret in a fixed bandit instance, similarly to frequentist analyses. The bound involves \u2206-1a and is derived using biased Bayesian confidence intervals. The rest of our analysis is Bayesian in two parts: we prove that the confidence intervals fail with a low probability and bound random \u2206-1a, following a similar technique to Lai [1987]. The resulting logarithmic Bayes regret bounds cannot be derived using the techniques of Russo and Van Roy [2014], as these become loose when the confidence interval widths are introduced.\n\nAsymptotic logarithmic Bayes regret bounds were derived in Lai [1987]. From this analysis, we use only the technique for bounding \u2206-1a when proving Corollary 2 and Theorem 5. The central part of our proof is a finite-time per-instance bound on the number of times that a suboptimal action is taken. This quantity is bounded based on the assumption that the action with the highest UCB is taken.", "md": "All modern Bayesian analyses follow Russo and Van Roy [2014], who derived the first finite-time \u02dc O(\u221an) Bayes regret bounds for BayesUCB and Thompson sampling. The key idea in their analyses is that conditioned on history, the optimal and taken actions are identically distributed, and that the upper confidence bounds are deterministic functions of the history. This is where the randomness of instances in Bayesian bandits is used. Using this, the regret at round t is bounded by the confidence interval width of the taken action, and the usual \u02dc O(\u221an) bounds can be obtained by summing up the confidence interval widths over n rounds.\n\nThe main difference in our work is that we first bound the regret in a fixed bandit instance, similarly to frequentist analyses. The bound involves \u2206-1a and is derived using biased Bayesian confidence intervals. The rest of our analysis is Bayesian in two parts: we prove that the confidence intervals fail with a low probability and bound random \u2206-1a, following a similar technique to Lai [1987]. The resulting logarithmic Bayes regret bounds cannot be derived using the techniques of Russo and Van Roy [2014], as these become loose when the confidence interval widths are introduced.\n\nAsymptotic logarithmic Bayes regret bounds were derived in Lai [1987]. From this analysis, we use only the technique for bounding \u2206-1a when proving Corollary 2 and Theorem 5. The central part of our proof is a finite-time per-instance bound on the number of times that a suboptimal action is taken. This quantity is bounded based on the assumption that the action with the highest UCB is taken."}]}, {"page": 9, "text": "                             (a) Decreasing prior width                                                 (b) Increasing prior gap\n                                                                                                                            BayesUCB\n      103                                                                      103                                          UCB1\n                                                                                                                            BayesUCB log bound\n                                                                                                                            UCB1 log bound\n    Regret                                                                                                                  O( pn) bound\n      102                                                                      102\n      101                                                                      101\n         0.0          0.2          0.4         0.6          0.8          1.0      1.0     1.5     2.0     2.5     3.0     3.5     4.0     4.5   5.0\n                                    Prior width \u00be0                                                            Prior gap \u00a20\n            Figure 1: Gaussian bandit as (a) the prior width \u03c30 and (b) the prior gap \u22060 change.\ncomparable argument in Theorem 2 of Lai [1987] is asymptotic and on average over random bandit\ninstances.\n6      Experiments\nWe experiment with UCB algorithms in two environments: Gaussian bandits (Section 3.1) and linear\nbandits with Gaussian rewards (Section 3.3). In both experiments, the horizon is n = 1 000 rounds.\nAll results are averaged over 10 000 random runs. Shaded regions in the plots are standard errors of\nthe estimates. They are generally small because the number of runs is high.\n6.1      Gaussian Bandit\nThe first problem is a K-armed bandit with K = 10 actions (Section 3.1). The prior width is \u03c30 = 1.\nThe prior mean is \u00b50, where \u00b50,1 = \u22060 and \u00b50,a = 0 for a > 1. We set \u22060 = 1 and call it the prior\ngap. We vary \u03c30 and \u22060 in our experiments, and observe how the regret and its upper bounds change\nas the problem hardness varies (Sections 4.1 and 5.2).\nWe plot fi    ve trends: (a) Bayes regret of BayesUCB. (b) Bayes regret of UCB1 (Section 4.2), which is a\ncomparable frequentist algorithm to BayesUCB. (c) A leading term of the BayesUCB regret bound in\nTheorem 1, where \u03b5 = 1/n and \u03b4 = 1/n. (d) A leading term of the UCB1 regret bound: This is the\nsame as (c) with \u03c30 = \u221e. (e) An existing \u02dc                       O(\u221an) regret bound in (3).\nOur results are reported in Figure 1. We observe three major trends. First, the regret of BayesUCB\ndecreases as the problem becomes easier, either \u03c30 \u2192                                  0 or \u22060 \u2192         \u221e. It is also lower than that of\nUCB1, which does not leverage the prior. Second, the regret bound of BayesUCB is tighter than that\nof UCB1, due to capturing the benefit of the prior. Finally, the logarithmic regret bounds are tighter\nthan the \u02dc   O(\u221an) bound. In addition, the \u02dc                O(\u221an) bound depends on the prior only through \u03c30 and thus\nremains constant as the prior gap \u22060 changes.\nIn Appendix D, we compare BayesUCB to UCB1 more comprehensively for various K, \u03c3, \u22060, and\n\u03c30. In all experiments, BayesUCB has a lower regret than UCB1. This also happens when the noise is\nnot Gaussian, which a testament to the robustness of Bayesian methods to model misspecification.\n6.2      Linear Bandit with Gaussian Rewards\nThe second problem is a linear bandit with K = 30 actions in d = 10 dimensions (Section 3.3). The\nprior covariance is \u03a30 = \u03c32              0Id. The prior mean is \u03b80, where \u03b80,1 = \u22060 and \u03b80,i = \u22121 for i > 1. As\nin Section 6.1, we set \u22060 = 1 and call it the prior gap. The action set A is generated as follows. The\nfirst d actions are the canonical basis in Rd. The remaining K \u2212                                      d actions are sampled uniformly\nat random from the positive orthant and scaled to unit length. This ensures that the first action has\nthe highest mean reward, of \u22060, under the prior mean \u03b80. We vary \u03c30 and \u22060, and observe how the\nregret and its upper bounds change as the problem hardness varies (Sections 4.4 and 5.2). We plot\nthree trends: (a) Bayes regret of BayesUCB. (b) A leading term of the BayesUCB regret bound in\nTheorem 6, where \u03c30,max = \u03c30, \u03b5 = 1/n, and \u03b4 = 1/n. (c) An existing \u02dc                                          O(\u221an) regret bound in (4).\n                                                                           9", "md": "## Decreasing prior width\n\n## Increasing prior gap\n\n## BayesUCB\n\n## UCB1\n\n## BayesUCB log bound\n\n## UCB1 log bound\n\n### Regret\n\n|102|102|\n|---|---|\n|101|101|\n\n0.0 0.2 0.4 0.6 0.8 1.0 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nPrior width \u00be0 Prior gap \u00a20\n\nFigure 1: Gaussian bandit as (a) the prior width \u03c30 and (b) the prior gap \u22060 change.\n\ncomparable argument in Theorem 2 of Lai [1987] is asymptotic and on average over random bandit instances.\n\n## Experiments\n\nWe experiment with UCB algorithms in two environments: Gaussian bandits (Section 3.1) and linear bandits with Gaussian rewards (Section 3.3). In both experiments, the horizon is n = 1 000 rounds. All results are averaged over 10 000 random runs. Shaded regions in the plots are standard errors of the estimates. They are generally small because the number of runs is high.\n\n### Gaussian Bandit\n\nThe first problem is a K-armed bandit with K = 10 actions (Section 3.1). The prior width is \u03c30 = 1. The prior mean is \u00b50, where \u00b50,1 = \u22060 and \u00b50,a = 0 for a > 1. We set \u22060 = 1 and call it the prior gap. We vary \u03c30 and \u22060 in our experiments, and observe how the regret and its upper bounds change as the problem hardness varies (Sections 4.1 and 5.2).\n\nWe plot fi ve trends: (a) Bayes regret of BayesUCB. (b) Bayes regret of UCB1 (Section 4.2), which is a comparable frequentist algorithm to BayesUCB. (c) A leading term of the BayesUCB regret bound in Theorem 1, where \u03b5 = 1/n and \u03b4 = 1/n. (d) A leading term of the UCB1 regret bound: This is the same as (c) with \u03c30 = \u221e. (e) An existing \u02dc O(\u221an) regret bound in (3).\n\nOur results are reported in Figure 1. We observe three major trends. First, the regret of BayesUCB decreases as the problem becomes easier, either \u03c30 \u2192 0 or \u22060 \u2192 \u221e. It is also lower than that of UCB1, which does not leverage the prior. Second, the regret bound of BayesUCB is tighter than that of UCB1, due to capturing the benefit of the prior. Finally, the logarithmic regret bounds are tighter than the \u02dc O(\u221an) bound. In addition, the \u02dc O(\u221an) bound depends on the prior only through \u03c30 and thus remains constant as the prior gap \u22060 changes.\n\nIn Appendix D, we compare BayesUCB to UCB1 more comprehensively for various K, \u03c3, \u22060, and \u03c30. In all experiments, BayesUCB has a lower regret than UCB1. This also happens when the noise is not Gaussian, which a testament to the robustness of Bayesian methods to model misspecification.\n\n### Linear Bandit with Gaussian Rewards\n\nThe second problem is a linear bandit with K = 30 actions in d = 10 dimensions (Section 3.3). The prior covariance is \u03a30 = \u03c32 0Id. The prior mean is \u03b80, where \u03b80,1 = \u22060 and \u03b80,i = \u22121 for i > 1. As in Section 6.1, we set \u22060 = 1 and call it the prior gap. The action set A is generated as follows. The first d actions are the canonical basis in Rd. The remaining K \u2212 d actions are sampled uniformly at random from the positive orthant and scaled to unit length. This ensures that the first action has the highest mean reward, of \u22060, under the prior mean \u03b80. We vary \u03c30 and \u22060, and observe how the regret and its upper bounds change as the problem hardness varies (Sections 4.4 and 5.2). We plot three trends: (a) Bayes regret of BayesUCB. (b) A leading term of the BayesUCB regret bound in Theorem 6, where \u03c30,max = \u03c30, \u03b5 = 1/n, and \u03b4 = 1/n. (c) An existing \u02dc O(\u221an) regret bound in (4).", "images": [], "items": [{"type": "heading", "lvl": 2, "value": "Decreasing prior width", "md": "## Decreasing prior width"}, {"type": "heading", "lvl": 2, "value": "Increasing prior gap", "md": "## Increasing prior gap"}, {"type": "heading", "lvl": 2, "value": "BayesUCB", "md": "## BayesUCB"}, {"type": "heading", "lvl": 2, "value": "UCB1", "md": "## UCB1"}, {"type": "heading", "lvl": 2, "value": "BayesUCB log bound", "md": "## BayesUCB log bound"}, {"type": "heading", "lvl": 2, "value": "UCB1 log bound", "md": "## UCB1 log bound"}, {"type": "heading", "lvl": 3, "value": "Regret", "md": "### Regret"}, {"type": "table", "rows": [["102", "102"], ["101", "101"]], "md": "|102|102|\n|---|---|\n|101|101|", "isPerfectTable": true, "csv": "\"102\",\"102\"\n\"101\",\"101\""}, {"type": "text", "value": "0.0 0.2 0.4 0.6 0.8 1.0 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nPrior width \u00be0 Prior gap \u00a20\n\nFigure 1: Gaussian bandit as (a) the prior width \u03c30 and (b) the prior gap \u22060 change.\n\ncomparable argument in Theorem 2 of Lai [1987] is asymptotic and on average over random bandit instances.", "md": "0.0 0.2 0.4 0.6 0.8 1.0 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\nPrior width \u00be0 Prior gap \u00a20\n\nFigure 1: Gaussian bandit as (a) the prior width \u03c30 and (b) the prior gap \u22060 change.\n\ncomparable argument in Theorem 2 of Lai [1987] is asymptotic and on average over random bandit instances."}, {"type": "heading", "lvl": 2, "value": "Experiments", "md": "## Experiments"}, {"type": "text", "value": "We experiment with UCB algorithms in two environments: Gaussian bandits (Section 3.1) and linear bandits with Gaussian rewards (Section 3.3). In both experiments, the horizon is n = 1 000 rounds. All results are averaged over 10 000 random runs. Shaded regions in the plots are standard errors of the estimates. They are generally small because the number of runs is high.", "md": "We experiment with UCB algorithms in two environments: Gaussian bandits (Section 3.1) and linear bandits with Gaussian rewards (Section 3.3). In both experiments, the horizon is n = 1 000 rounds. All results are averaged over 10 000 random runs. Shaded regions in the plots are standard errors of the estimates. They are generally small because the number of runs is high."}, {"type": "heading", "lvl": 3, "value": "Gaussian Bandit", "md": "### Gaussian Bandit"}, {"type": "text", "value": "The first problem is a K-armed bandit with K = 10 actions (Section 3.1). The prior width is \u03c30 = 1. The prior mean is \u00b50, where \u00b50,1 = \u22060 and \u00b50,a = 0 for a > 1. We set \u22060 = 1 and call it the prior gap. We vary \u03c30 and \u22060 in our experiments, and observe how the regret and its upper bounds change as the problem hardness varies (Sections 4.1 and 5.2).\n\nWe plot fi ve trends: (a) Bayes regret of BayesUCB. (b) Bayes regret of UCB1 (Section 4.2), which is a comparable frequentist algorithm to BayesUCB. (c) A leading term of the BayesUCB regret bound in Theorem 1, where \u03b5 = 1/n and \u03b4 = 1/n. (d) A leading term of the UCB1 regret bound: This is the same as (c) with \u03c30 = \u221e. (e) An existing \u02dc O(\u221an) regret bound in (3).\n\nOur results are reported in Figure 1. We observe three major trends. First, the regret of BayesUCB decreases as the problem becomes easier, either \u03c30 \u2192 0 or \u22060 \u2192 \u221e. It is also lower than that of UCB1, which does not leverage the prior. Second, the regret bound of BayesUCB is tighter than that of UCB1, due to capturing the benefit of the prior. Finally, the logarithmic regret bounds are tighter than the \u02dc O(\u221an) bound. In addition, the \u02dc O(\u221an) bound depends on the prior only through \u03c30 and thus remains constant as the prior gap \u22060 changes.\n\nIn Appendix D, we compare BayesUCB to UCB1 more comprehensively for various K, \u03c3, \u22060, and \u03c30. In all experiments, BayesUCB has a lower regret than UCB1. This also happens when the noise is not Gaussian, which a testament to the robustness of Bayesian methods to model misspecification.", "md": "The first problem is a K-armed bandit with K = 10 actions (Section 3.1). The prior width is \u03c30 = 1. The prior mean is \u00b50, where \u00b50,1 = \u22060 and \u00b50,a = 0 for a > 1. We set \u22060 = 1 and call it the prior gap. We vary \u03c30 and \u22060 in our experiments, and observe how the regret and its upper bounds change as the problem hardness varies (Sections 4.1 and 5.2).\n\nWe plot fi ve trends: (a) Bayes regret of BayesUCB. (b) Bayes regret of UCB1 (Section 4.2), which is a comparable frequentist algorithm to BayesUCB. (c) A leading term of the BayesUCB regret bound in Theorem 1, where \u03b5 = 1/n and \u03b4 = 1/n. (d) A leading term of the UCB1 regret bound: This is the same as (c) with \u03c30 = \u221e. (e) An existing \u02dc O(\u221an) regret bound in (3).\n\nOur results are reported in Figure 1. We observe three major trends. First, the regret of BayesUCB decreases as the problem becomes easier, either \u03c30 \u2192 0 or \u22060 \u2192 \u221e. It is also lower than that of UCB1, which does not leverage the prior. Second, the regret bound of BayesUCB is tighter than that of UCB1, due to capturing the benefit of the prior. Finally, the logarithmic regret bounds are tighter than the \u02dc O(\u221an) bound. In addition, the \u02dc O(\u221an) bound depends on the prior only through \u03c30 and thus remains constant as the prior gap \u22060 changes.\n\nIn Appendix D, we compare BayesUCB to UCB1 more comprehensively for various K, \u03c3, \u22060, and \u03c30. In all experiments, BayesUCB has a lower regret than UCB1. This also happens when the noise is not Gaussian, which a testament to the robustness of Bayesian methods to model misspecification."}, {"type": "heading", "lvl": 3, "value": "Linear Bandit with Gaussian Rewards", "md": "### Linear Bandit with Gaussian Rewards"}, {"type": "text", "value": "The second problem is a linear bandit with K = 30 actions in d = 10 dimensions (Section 3.3). The prior covariance is \u03a30 = \u03c32 0Id. The prior mean is \u03b80, where \u03b80,1 = \u22060 and \u03b80,i = \u22121 for i > 1. As in Section 6.1, we set \u22060 = 1 and call it the prior gap. The action set A is generated as follows. The first d actions are the canonical basis in Rd. The remaining K \u2212 d actions are sampled uniformly at random from the positive orthant and scaled to unit length. This ensures that the first action has the highest mean reward, of \u22060, under the prior mean \u03b80. We vary \u03c30 and \u22060, and observe how the regret and its upper bounds change as the problem hardness varies (Sections 4.4 and 5.2). We plot three trends: (a) Bayes regret of BayesUCB. (b) A leading term of the BayesUCB regret bound in Theorem 6, where \u03c30,max = \u03c30, \u03b5 = 1/n, and \u03b4 = 1/n. (c) An existing \u02dc O(\u221an) regret bound in (4).", "md": "The second problem is a linear bandit with K = 30 actions in d = 10 dimensions (Section 3.3). The prior covariance is \u03a30 = \u03c32 0Id. The prior mean is \u03b80, where \u03b80,1 = \u22060 and \u03b80,i = \u22121 for i > 1. As in Section 6.1, we set \u22060 = 1 and call it the prior gap. The action set A is generated as follows. The first d actions are the canonical basis in Rd. The remaining K \u2212 d actions are sampled uniformly at random from the positive orthant and scaled to unit length. This ensures that the first action has the highest mean reward, of \u22060, under the prior mean \u03b80. We vary \u03c30 and \u22060, and observe how the regret and its upper bounds change as the problem hardness varies (Sections 4.4 and 5.2). We plot three trends: (a) Bayes regret of BayesUCB. (b) A leading term of the BayesUCB regret bound in Theorem 6, where \u03c30,max = \u03c30, \u03b5 = 1/n, and \u03b4 = 1/n. (c) An existing \u02dc O(\u221an) regret bound in (4)."}]}, {"page": 10, "text": "                             (a) Decreasing prior width                                                 (b) Increasing prior gap\n      104                                                                      104                                          BayesUCB\n                                                                                                                            BayesUCB log bound\n    Regret                                                                                                                  O( pn) bound\n      103                                                                      103\n      102                                                                      102\n      101                                                                      101\n         0.0          0.2          0.4          0.6         0.8          1.0        1      2      3      4      5      6      7      8       9     10\n                                    Prior width \u00be0                                                            Prior gap \u00a20\n              Figure 2: Linear bandit as (a) the prior width \u03c30 and (b) the prior gap \u22060 change.\nOur results are reported in Figure 2 and we observe three major trends. First, the regret of BayesUCB\ndecreases as the problem becomes easier, either \u03c30 \u2192                                 0 or \u22060 \u2192         \u221e. Second, the regret bound of\nBayesUCB decreases as the problem becomes easier. Finally, our logarithmic regret bound can also\nbe tighter than the \u02dc       O(\u221an) bound. In particular, the \u02dc                 O(\u221an) bound depends on the prior only through\n\u03c30, and thus remains constant as the prior gap \u22060 changes. We discuss when our bounds could be\nlooser than \u02dc     O(\u221an) bounds in Section 5.2.\n7      Conclusions\nFinite-time logarithmic frequentist regret bounds are the standard way of analyzing K-armed bandits\n[Auer et al., 2002, Garivier and Cappe, 2011, Agrawal and Goyal, 2012]. In our work, we prove the\nfirst comparable finite-time bounds, logarithmic in n, in the Bayesian setting. This is a major step\nin theory and a significant improvement upon prior \u02dc                            O(\u221an) Bayes regret bounds that have become\nstandard in the literature. Comparing to frequentist regret bounds, our bounds capture the value of\nprior information given to the learner. Our proof technique is general and we also apply it to linear\nbandits.\nThis work can be extended in many directions. First, our analyses only need closed-form posteriors,\nwhich are available for other reward distributions than Gaussian and Bernoulli. Second, our linear\nbandit analysis (Section 4.4) seems preliminary when compared to our multi-armed bandit analyses.\nAs an example, the complexity term E [1/\u2206\u03b5                          min] in Theorem 6 could be bounded as in Corollary 2\nand Theorem 5. We do not do this because the main reason for deriving the O(ch log2 n) bound in\nTheorem 5, an upper bound on the corresponding O(c\u2206                                     log n) bound, is that it matches the lower\nbound in (2). No such instance-dependent lower bound exists in Bayesian linear bandits. Third, we\nbelieve that our approach can be extended to information-theory bounds [Russo and Van Roy, 2016]\nand discuss it in Appendix E. Fourth, although we only analyze BayesUCB, we believe that similar\nguarantees can be obtained for Thompson sampling. Finally, we would like to extend our results to\nreinforcement learning, for instance by building on the work of Lu and Van Roy [2019].\nThere have been recent attempts in theory [Wagenmaker and Foster, 2023] to design general adaptive\nalgorithms with finite-time instance-dependent bounds based on optimal allocations. The promise of\nthese methods is a higher statistical efficiency than exploring by optimism, which we adopt in this\nwork. One of their shortcomings is that they are not guaranteed to be computationally effi                                                 cient, as\ndiscussed in Section 2.2 of Wagenmaker and Foster [2023]. This work is also frequentist.\nReferences\nYasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic\n    bandits. In Advances in Neural Information Processing Systems 24, pages 2312\u20132320, 2011.\nMarc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the\n    20th International Conference on Artificial Intelligence and Statistics, 2017.\n                                                                          10", "md": "# Bandit Analysis\n\n## Figure 2: Linear bandit as (a) the prior width \u03c30 and (b) the prior gap \u22060 change.\n\nOur results are reported in Figure 2 and we observe three major trends. First, the regret of BayesUCB decreases as the problem becomes easier, either \u03c30 \u2192 0 or \u22060 \u2192 \u221e. Second, the regret bound of BayesUCB decreases as the problem becomes easier. Finally, our logarithmic regret bound can also be tighter than the $$\\tilde{O}(\\sqrt{n})$$ bound. In particular, the $$\\tilde{O}(\\sqrt{n})$$ bound depends on the prior only through \u03c30, and thus remains constant as the prior gap \u22060 changes. We discuss when our bounds could be looser than $$\\tilde{O}(\\sqrt{n})$$ bounds in Section 5.2.\n\n### Conclusions\n\nFinite-time logarithmic frequentist regret bounds are the standard way of analyzing K-armed bandits [Auer et al., 2002, Garivier and Cappe, 2011, Agrawal and Goyal, 2012]. In our work, we prove the first comparable finite-time bounds, logarithmic in n, in the Bayesian setting. This is a major step in theory and a significant improvement upon prior $$\\tilde{O}(\\sqrt{n})$$ Bayes regret bounds that have become standard in the literature. Comparing to frequentist regret bounds, our bounds capture the value of prior information given to the learner. Our proof technique is general and we also apply it to linear bandits.\n\nThis work can be extended in many directions. First, our analyses only need closed-form posteriors, which are available for other reward distributions than Gaussian and Bernoulli. Second, our linear bandit analysis (Section 4.4) seems preliminary when compared to our multi-armed bandit analyses. As an example, the complexity term E [1/\u2206\u03b5min] in Theorem 6 could be bounded as in Corollary 2 and Theorem 5. We do not do this because the main reason for deriving the $$O(ch \\log^2 n)$$ bound in Theorem 5, an upper bound on the corresponding $$O(c\\Delta \\log n)$$ bound, is that it matches the lower bound in (2). No such instance-dependent lower bound exists in Bayesian linear bandits. Third, we believe that our approach can be extended to information-theory bounds [Russo and Van Roy, 2016] and discuss it in Appendix E. Fourth, although we only analyze BayesUCB, we believe that similar guarantees can be obtained for Thompson sampling. Finally, we would like to extend our results to reinforcement learning, for instance by building on the work of Lu and Van Roy [2019].\n\nThere have been recent attempts in theory [Wagenmaker and Foster, 2023] to design general adaptive algorithms with finite-time instance-dependent bounds based on optimal allocations. The promise of these methods is a higher statistical efficiency than exploring by optimism, which we adopt in this work. One of their shortcomings is that they are not guaranteed to be computationally efficient, as discussed in Section 2.2 of Wagenmaker and Foster [2023]. This work is also frequentist.\n\n### References\n\n1. Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312\u20132320, 2011.\n2. Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Bandit Analysis", "md": "# Bandit Analysis"}, {"type": "heading", "lvl": 2, "value": "Figure 2: Linear bandit as (a) the prior width \u03c30 and (b) the prior gap \u22060 change.", "md": "## Figure 2: Linear bandit as (a) the prior width \u03c30 and (b) the prior gap \u22060 change."}, {"type": "text", "value": "Our results are reported in Figure 2 and we observe three major trends. First, the regret of BayesUCB decreases as the problem becomes easier, either \u03c30 \u2192 0 or \u22060 \u2192 \u221e. Second, the regret bound of BayesUCB decreases as the problem becomes easier. Finally, our logarithmic regret bound can also be tighter than the $$\\tilde{O}(\\sqrt{n})$$ bound. In particular, the $$\\tilde{O}(\\sqrt{n})$$ bound depends on the prior only through \u03c30, and thus remains constant as the prior gap \u22060 changes. We discuss when our bounds could be looser than $$\\tilde{O}(\\sqrt{n})$$ bounds in Section 5.2.", "md": "Our results are reported in Figure 2 and we observe three major trends. First, the regret of BayesUCB decreases as the problem becomes easier, either \u03c30 \u2192 0 or \u22060 \u2192 \u221e. Second, the regret bound of BayesUCB decreases as the problem becomes easier. Finally, our logarithmic regret bound can also be tighter than the $$\\tilde{O}(\\sqrt{n})$$ bound. In particular, the $$\\tilde{O}(\\sqrt{n})$$ bound depends on the prior only through \u03c30, and thus remains constant as the prior gap \u22060 changes. We discuss when our bounds could be looser than $$\\tilde{O}(\\sqrt{n})$$ bounds in Section 5.2."}, {"type": "heading", "lvl": 3, "value": "Conclusions", "md": "### Conclusions"}, {"type": "text", "value": "Finite-time logarithmic frequentist regret bounds are the standard way of analyzing K-armed bandits [Auer et al., 2002, Garivier and Cappe, 2011, Agrawal and Goyal, 2012]. In our work, we prove the first comparable finite-time bounds, logarithmic in n, in the Bayesian setting. This is a major step in theory and a significant improvement upon prior $$\\tilde{O}(\\sqrt{n})$$ Bayes regret bounds that have become standard in the literature. Comparing to frequentist regret bounds, our bounds capture the value of prior information given to the learner. Our proof technique is general and we also apply it to linear bandits.\n\nThis work can be extended in many directions. First, our analyses only need closed-form posteriors, which are available for other reward distributions than Gaussian and Bernoulli. Second, our linear bandit analysis (Section 4.4) seems preliminary when compared to our multi-armed bandit analyses. As an example, the complexity term E [1/\u2206\u03b5min] in Theorem 6 could be bounded as in Corollary 2 and Theorem 5. We do not do this because the main reason for deriving the $$O(ch \\log^2 n)$$ bound in Theorem 5, an upper bound on the corresponding $$O(c\\Delta \\log n)$$ bound, is that it matches the lower bound in (2). No such instance-dependent lower bound exists in Bayesian linear bandits. Third, we believe that our approach can be extended to information-theory bounds [Russo and Van Roy, 2016] and discuss it in Appendix E. Fourth, although we only analyze BayesUCB, we believe that similar guarantees can be obtained for Thompson sampling. Finally, we would like to extend our results to reinforcement learning, for instance by building on the work of Lu and Van Roy [2019].\n\nThere have been recent attempts in theory [Wagenmaker and Foster, 2023] to design general adaptive algorithms with finite-time instance-dependent bounds based on optimal allocations. The promise of these methods is a higher statistical efficiency than exploring by optimism, which we adopt in this work. One of their shortcomings is that they are not guaranteed to be computationally efficient, as discussed in Section 2.2 of Wagenmaker and Foster [2023]. This work is also frequentist.", "md": "Finite-time logarithmic frequentist regret bounds are the standard way of analyzing K-armed bandits [Auer et al., 2002, Garivier and Cappe, 2011, Agrawal and Goyal, 2012]. In our work, we prove the first comparable finite-time bounds, logarithmic in n, in the Bayesian setting. This is a major step in theory and a significant improvement upon prior $$\\tilde{O}(\\sqrt{n})$$ Bayes regret bounds that have become standard in the literature. Comparing to frequentist regret bounds, our bounds capture the value of prior information given to the learner. Our proof technique is general and we also apply it to linear bandits.\n\nThis work can be extended in many directions. First, our analyses only need closed-form posteriors, which are available for other reward distributions than Gaussian and Bernoulli. Second, our linear bandit analysis (Section 4.4) seems preliminary when compared to our multi-armed bandit analyses. As an example, the complexity term E [1/\u2206\u03b5min] in Theorem 6 could be bounded as in Corollary 2 and Theorem 5. We do not do this because the main reason for deriving the $$O(ch \\log^2 n)$$ bound in Theorem 5, an upper bound on the corresponding $$O(c\\Delta \\log n)$$ bound, is that it matches the lower bound in (2). No such instance-dependent lower bound exists in Bayesian linear bandits. Third, we believe that our approach can be extended to information-theory bounds [Russo and Van Roy, 2016] and discuss it in Appendix E. Fourth, although we only analyze BayesUCB, we believe that similar guarantees can be obtained for Thompson sampling. Finally, we would like to extend our results to reinforcement learning, for instance by building on the work of Lu and Van Roy [2019].\n\nThere have been recent attempts in theory [Wagenmaker and Foster, 2023] to design general adaptive algorithms with finite-time instance-dependent bounds based on optimal allocations. The promise of these methods is a higher statistical efficiency than exploring by optimism, which we adopt in this work. One of their shortcomings is that they are not guaranteed to be computationally efficient, as discussed in Section 2.2 of Wagenmaker and Foster [2023]. This work is also frequentist."}, {"type": "heading", "lvl": 3, "value": "References", "md": "### References"}, {"type": "text", "value": "1. Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312\u20132320, 2011.\n2. Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017.", "md": "1. Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In Advances in Neural Information Processing Systems 24, pages 2312\u20132320, 2011.\n2. Marc Abeille and Alessandro Lazaric. Linear Thompson sampling revisited. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 2017."}]}, {"page": 11, "text": "Shipra Agrawal and Navin Goyal. Analysis of Thompson sampling for the multi-armed bandit\n   problem. In Proceedings of the 25th Annual Conference on Learning Theory, pages 39.1\u201339.26,\n   2012.\nShipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In\n   Proceedings of the 30th International Conference on Machine Learning, pages 127\u2013135, 2013.\nImad Aouali, Branislav Kveton, and Sumeet Katariya. Mixed-effect Thompson sampling. In\n   Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, 2023.\nJean-Yves Audibert and Sebastien Bubeck. Minimax policies for adversarial and stochastic bandits.\n   In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.\nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert Schapire. Gambling in a rigged casino:\n   The adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on\n   Foundations of Computer Science, pages 322\u2013331, 1995.\nPeter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit\n   problem. Machine Learning, 47:235\u2013256, 2002.\nHamsa Bastani, David Simchi-Levi, and Ruihao Zhu. Meta dynamic pricing: Transfer learning across\n   experiments. CoRR, abs/1902.10918, 2019. URL https://arxiv.org/abs/1902.10918.\nSoumya Basu, Branislav Kveton, Manzil Zaheer, and Csaba Szepesvari. No regrets for learning the\n   prior in bandits. In Advances in Neural Information Processing Systems 34, 2021.\nChristopher Bishop. Pattern Recognition and Machine Learning. Springer, New York, NY, 2006.\nOlivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In Advances in\n   Neural Information Processing Systems 24, pages 2249\u20132257, 2012.\nVarsha Dani, Thomas Hayes, and Sham Kakade. Stochastic linear optimization under bandit feedback.\n   In Proceedings of the 21st Annual Conference on Learning Theory, pages 355\u2013366, 2008.\nAurelien Garivier and Olivier Cappe. The KL-UCB algorithm for bounded stochastic bandits and\n   beyond. In Proceedings of the 24th Annual Conference on Learning Theory, pages 359\u2013376, 2011.\nJohn Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical\n   Society. Series B (Methodological), 41:148\u2013177, 1979.\nJoey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, and Craig Boutilier. Latent\n   bandits revisited. In Advances in Neural Information Processing Systems 33, 2020.\nJoey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical Bayesian\n   bandits. In Proceedings of the 25th International Conference on Artificial Intelligence and Statistics,\n   2022.\nEmilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On Bayesian upper confidence bounds for\n   bandit problems. In Proceedings of the 15th International Conference on Artificial Intelligence\n   and Statistics, pages 592\u2013600, 2012.\nJaya Kawale, Hung Bui, Branislav Kveton, Long Tran-Thanh, and Sanjay Chawla. Efficient Thomp-\n   son sampling for online matrix-factorization recommendation. In Advances in Neural Information\n   Processing Systems 28, pages 1297\u20131305, 2015.\nBranislav Kveton, Mikhail Konobeev, Manzil Zaheer, Chih-Wei Hsu, Martin Mladenov, Craig\n   Boutilier, and Csaba Szepesvari. Meta-Thompson sampling. In Proceedings of the 38th Interna-\n   tional Conference on Machine Learning, 2021.\nTze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of\n   Statistics, 15(3):1091\u20131114, 1987.\nTze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in\n   Applied Mathematics, 6(1):4\u201322, 1985.\n                                                    11", "md": "# References\n\n# List of References\n\n|Shipra Agrawal and Navin Goyal.|Analysis of Thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory, pages 39.1\u201339.26, 2012.|\n|---|---|\n|Shipra Agrawal and Navin Goyal.|Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Machine Learning, pages 127\u2013135, 2013.|\n|Imad Aouali, Branislav Kveton, and Sumeet Katariya.|Mixed-effect Thompson sampling. In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, 2023.|\n|Jean-Yves Audibert and Sebastien Bubeck.|Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.|\n|Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert Schapire.|Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, pages 322\u2013331, 1995.|\n\nAnd many more...", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "table", "rows": [["Shipra Agrawal and Navin Goyal.", "Analysis of Thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory, pages 39.1\u201339.26, 2012."], ["Shipra Agrawal and Navin Goyal.", "Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Machine Learning, pages 127\u2013135, 2013."], ["Imad Aouali, Branislav Kveton, and Sumeet Katariya.", "Mixed-effect Thompson sampling. In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, 2023."], ["Jean-Yves Audibert and Sebastien Bubeck.", "Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009."], ["Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert Schapire.", "Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, pages 322\u2013331, 1995."]], "md": "|Shipra Agrawal and Navin Goyal.|Analysis of Thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory, pages 39.1\u201339.26, 2012.|\n|---|---|\n|Shipra Agrawal and Navin Goyal.|Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Machine Learning, pages 127\u2013135, 2013.|\n|Imad Aouali, Branislav Kveton, and Sumeet Katariya.|Mixed-effect Thompson sampling. In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, 2023.|\n|Jean-Yves Audibert and Sebastien Bubeck.|Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.|\n|Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert Schapire.|Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, pages 322\u2013331, 1995.|", "isPerfectTable": true, "csv": "\"Shipra Agrawal and Navin Goyal.\",\"Analysis of Thompson sampling for the multi-armed bandit problem. In Proceedings of the 25th Annual Conference on Learning Theory, pages 39.1\u201339.26, 2012.\"\n\"Shipra Agrawal and Navin Goyal.\",\"Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Machine Learning, pages 127\u2013135, 2013.\"\n\"Imad Aouali, Branislav Kveton, and Sumeet Katariya.\",\"Mixed-effect Thompson sampling. In Proceedings of the 26th International Conference on Artificial Intelligence and Statistics, 2023.\"\n\"Jean-Yves Audibert and Sebastien Bubeck.\",\"Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory, 2009.\"\n\"Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert Schapire.\",\"Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, pages 322\u2013331, 1995.\""}, {"type": "text", "value": "And many more...", "md": "And many more..."}]}, {"page": 12, "text": "Tor Lattimore and Csaba Szepesvari. Bandit Algorithms. Cambridge University Press, 2019.\nLihong Li, Wei Chu, John Langford, and Robert Schapire. A contextual-bandit approach to personal-\n   ized news article recommendation. In Proceedings of the 19th International Conference on World\n   Wide Web, 2010.\nLisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband:\n   A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning\n   Research, 18(185):1\u201352, 2018.\nShuai Li, Alexandros Karatzoglou, and Claudio Gentile. Collaborative filtering bandits. In Proceed-\n   ings of the 39th Annual International ACM SIGIR Conference, 2016.\nXiuyuan Lu and Benjamin Van Roy. Information-theoretic confidence bounds for reinforcement\n   learning. In Advances in Neural Information Processing Systems 32, 2019.\nOlivier Marchal and Julyan Arbel. On the sub-Gaussianity of the beta and Dirichlet distributions.\n   Electronic Communications in Probability, 22:1\u201314, 2017.\nDaniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of\n   Operations Research, 39(4):1221\u20131243, 2014.\nDaniel Russo and Benjamin Van Roy. An information-theoretic analysis of Thompson sampling.\n   Journal of Machine Learning Research, 17(68):1\u201330, 2016.\nDaniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on\n   Thompson sampling. Foundations and Trends in Machine Learning, 11(1):1\u201396, 2018.\nMax Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro\n   Dudik, and Robert Schapire. Bayesian decision-making under misspecified priors with applications\n   to meta-learning. In Advances in Neural Information Processing Systems 34, 2021.\nWilliam R. Thompson. On the likelihood that one unknown probability exceeds another in view of\n   the evidence of two samples. Biometrika, 25(3-4):285\u2013294, 1933.\nJohn Tsitsiklis. A short proof of the gittins index theorem. Neural Computation, 4(1):194\u2013199, 1994.\nAndrew Wagenmaker and Dylan Foster. Instance-optimality in interactive decision making: Toward a\n   non-asymptotic theory. In Proceedings of the 36th Annual Conference on Learning Theory, 2023.\nZhi Wang, Chicheng Zhang, Manish Kumar Singh, Laurel Riek, and Kamalika Chaudhuri. Mul-\n   titask bandit learning through heterogeneous feedback aggregation. In Proceedings of the 24th\n   International Conference on Artificial Intelligence and Statistics, 2021.\nXiaoxue Zhao, Weinan Zhang, and Jun Wang. Interactive collaborative filtering. In Proceedings\n   of the 22nd ACM International Conference on Information and Knowledge Management, pages\n   1411\u20131420, 2013.\n                                                  12", "md": "# References\n\n# List of References\n\n- Tor Lattimore and Csaba Szepesvari. *Bandit Algorithms*. Cambridge University Press, 2019.\n- Lihong Li, Wei Chu, John Langford, and Robert Schapire. A contextual-bandit approach to personalized news article recommendation. In *Proceedings of the 19th International Conference on World Wide Web*, 2010.\n- Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. *Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research*, 18(185):1\u201352, 2018.\n- Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. *Collaborative filtering bandits. In Proceedings of the 39th Annual International ACM SIGIR Conference*, 2016.\n- Xiuyuan Lu and Benjamin Van Roy. *Information-theoretic confidence bounds for reinforcement learning. In Advances in Neural Information Processing Systems 32*, 2019.\n- Olivier Marchal and Julyan Arbel. *On the sub-Gaussianity of the beta and Dirichlet distributions. Electronic Communications in Probability*, 22:1\u201314, 2017.\n- Daniel Russo and Benjamin Van Roy. *Learning to optimize via posterior sampling. Mathematics of Operations Research*, 39(4):1221\u20131243, 2014.\n- Daniel Russo and Benjamin Van Roy. *An information-theoretic analysis of Thompson sampling. Journal of Machine Learning Research*, 17(68):1\u201330, 2016.\n- Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. *A tutorial on Thompson sampling. Foundations and Trends in Machine Learning*, 11(1):1\u201396, 2018.\n- Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro Dudik, and Robert Schapire. *Bayesian decision-making under misspecified priors with applications to meta-learning. In Advances in Neural Information Processing Systems 34*, 2021.\n- William R. Thompson. *On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika*, 25(3-4):285\u2013294, 1933.\n- John Tsitsiklis. *A short proof of the gittins index theorem. Neural Computation*, 4(1):194\u2013199, 1994.\n- Andrew Wagenmaker and Dylan Foster. *Instance-optimality in interactive decision making: Toward a non-asymptotic theory. In Proceedings of the 36th Annual Conference on Learning Theory*, 2023.\n- Zhi Wang, Chicheng Zhang, Manish Kumar Singh, Laurel Riek, and Kamalika Chaudhuri. *Multitask bandit learning through heterogeneous feedback aggregation. In Proceedings of the 24th International Conference on Artificial Intelligence and Statistics*, 2021.\n- Xiaoxue Zhao, Weinan Zhang, and Jun Wang. *Interactive collaborative filtering. In Proceedings of the 22nd ACM International Conference on Information and Knowledge Management*, pages 1411\u20131420, 2013.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "References", "md": "# References"}, {"type": "heading", "lvl": 1, "value": "List of References", "md": "# List of References"}, {"type": "text", "value": "- Tor Lattimore and Csaba Szepesvari. *Bandit Algorithms*. Cambridge University Press, 2019.\n- Lihong Li, Wei Chu, John Langford, and Robert Schapire. A contextual-bandit approach to personalized news article recommendation. In *Proceedings of the 19th International Conference on World Wide Web*, 2010.\n- Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. *Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research*, 18(185):1\u201352, 2018.\n- Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. *Collaborative filtering bandits. In Proceedings of the 39th Annual International ACM SIGIR Conference*, 2016.\n- Xiuyuan Lu and Benjamin Van Roy. *Information-theoretic confidence bounds for reinforcement learning. In Advances in Neural Information Processing Systems 32*, 2019.\n- Olivier Marchal and Julyan Arbel. *On the sub-Gaussianity of the beta and Dirichlet distributions. Electronic Communications in Probability*, 22:1\u201314, 2017.\n- Daniel Russo and Benjamin Van Roy. *Learning to optimize via posterior sampling. Mathematics of Operations Research*, 39(4):1221\u20131243, 2014.\n- Daniel Russo and Benjamin Van Roy. *An information-theoretic analysis of Thompson sampling. Journal of Machine Learning Research*, 17(68):1\u201330, 2016.\n- Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. *A tutorial on Thompson sampling. Foundations and Trends in Machine Learning*, 11(1):1\u201396, 2018.\n- Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro Dudik, and Robert Schapire. *Bayesian decision-making under misspecified priors with applications to meta-learning. In Advances in Neural Information Processing Systems 34*, 2021.\n- William R. Thompson. *On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika*, 25(3-4):285\u2013294, 1933.\n- John Tsitsiklis. *A short proof of the gittins index theorem. Neural Computation*, 4(1):194\u2013199, 1994.\n- Andrew Wagenmaker and Dylan Foster. *Instance-optimality in interactive decision making: Toward a non-asymptotic theory. In Proceedings of the 36th Annual Conference on Learning Theory*, 2023.\n- Zhi Wang, Chicheng Zhang, Manish Kumar Singh, Laurel Riek, and Kamalika Chaudhuri. *Multitask bandit learning through heterogeneous feedback aggregation. In Proceedings of the 24th International Conference on Artificial Intelligence and Statistics*, 2021.\n- Xiaoxue Zhao, Weinan Zhang, and Jun Wang. *Interactive collaborative filtering. In Proceedings of the 22nd ACM International Conference on Information and Knowledge Management*, pages 1411\u20131420, 2013.", "md": "- Tor Lattimore and Csaba Szepesvari. *Bandit Algorithms*. Cambridge University Press, 2019.\n- Lihong Li, Wei Chu, John Langford, and Robert Schapire. A contextual-bandit approach to personalized news article recommendation. In *Proceedings of the 19th International Conference on World Wide Web*, 2010.\n- Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. *Hyperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research*, 18(185):1\u201352, 2018.\n- Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. *Collaborative filtering bandits. In Proceedings of the 39th Annual International ACM SIGIR Conference*, 2016.\n- Xiuyuan Lu and Benjamin Van Roy. *Information-theoretic confidence bounds for reinforcement learning. In Advances in Neural Information Processing Systems 32*, 2019.\n- Olivier Marchal and Julyan Arbel. *On the sub-Gaussianity of the beta and Dirichlet distributions. Electronic Communications in Probability*, 22:1\u201314, 2017.\n- Daniel Russo and Benjamin Van Roy. *Learning to optimize via posterior sampling. Mathematics of Operations Research*, 39(4):1221\u20131243, 2014.\n- Daniel Russo and Benjamin Van Roy. *An information-theoretic analysis of Thompson sampling. Journal of Machine Learning Research*, 17(68):1\u201330, 2016.\n- Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. *A tutorial on Thompson sampling. Foundations and Trends in Machine Learning*, 11(1):1\u201396, 2018.\n- Max Simchowitz, Christopher Tosh, Akshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro Dudik, and Robert Schapire. *Bayesian decision-making under misspecified priors with applications to meta-learning. In Advances in Neural Information Processing Systems 34*, 2021.\n- William R. Thompson. *On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika*, 25(3-4):285\u2013294, 1933.\n- John Tsitsiklis. *A short proof of the gittins index theorem. Neural Computation*, 4(1):194\u2013199, 1994.\n- Andrew Wagenmaker and Dylan Foster. *Instance-optimality in interactive decision making: Toward a non-asymptotic theory. In Proceedings of the 36th Annual Conference on Learning Theory*, 2023.\n- Zhi Wang, Chicheng Zhang, Manish Kumar Singh, Laurel Riek, and Kamalika Chaudhuri. *Multitask bandit learning through heterogeneous feedback aggregation. In Proceedings of the 24th International Conference on Artificial Intelligence and Statistics*, 2021.\n- Xiaoxue Zhao, Weinan Zhang, and Jun Wang. *Interactive collaborative filtering. In Proceedings of the 22nd ACM International Conference on Information and Knowledge Management*, pages 1411\u20131420, 2013."}]}, {"page": 13, "text": " A      Proofs\nWe present a general approach for deriving finite-time logarithmic Bayes regret bounds. We start\n with K-armed bandits and then extend it to linear bandits.\n A.1      Proof of Theorem 1\n Let Et =         \u2200a \u2208    A : |\u03b8a \u2212      \u02c6\n                                         \u03b8t,a| \u2264    Ct,a      be the event that all confidence intervals at round t hold.\n Fix \u03b5 > 0. We start with decomposing the n-round regret as\n                   n                      n                                             n\n                  t=1  E [\u2206At] \u2264         t=1  E [\u2206At1{\u2206At \u2265             \u03b5, Et}] +      t=1  E [\u2206At1{\u2206At < \u03b5}] +                           (5)\n                                          n                   \u00af\n                                         t=1  E    \u2206At1       Et      .\nWe bound the first term using the design of BayesUCB and its closed-form posteriors.\n Case 1: Event Et occurs and the gap is large, \u2206A                            t \u2265   \u03b5. Since Et occurs and the action with the\n highest UCB is taken, the regret at round t can be bounded as\n                 \u2206A   t = \u03b8A\u2217      \u2212   \u03b8At \u2264     \u03b8A\u2217   \u2212   Ut,A\u2217    + Ut,At \u2212       \u03b8At \u2264     Ut,At \u2212     \u03b8At \u2264     2Ct,At .\n In the second inequality, we use that \u03b8A                \u2217  \u2264   Ut,A\u2217    on event Et. This implies that on event Et, action\n a can be taken only if\n                              \u2206a \u2264      2Ct,a = 2         2\u02c6\u03c32t,a log(1/\u03b4) = 2                2 log(1/\u03b4)          ,\n                                                                                          \u03c3\u22122   + \u03c3\u22122Nt,a\n                                                                                            0\n which can be rearranged as\n                                                 Nt,a \u2264      8\u03c32 log(1/\u03b4)        \u2212   \u03c32\u03c3\u221220    .\n                                                                    \u22062 a\n Therefore, the number of times that action a is taken in n rounds while all confidence intervals hold,\n Na =  n      t=1 1{At = a, Et}, is bounded as    Na \u2264      8\u03c32 log(1/\u03b4)        \u2212   \u03c32\u03c3\u221220    .                                           (6)\n                                                                   \u22062 a\n Now we apply this inequality to bound the first term in (5) as\n           n                                               \uf8ee          8\u03c32 log(1/\u03b4)                                                 \uf8f9\n         t=1   E [\u2206At1{\u2206At \u2265             \u03b5, Et}] \u2264      E  \uf8f0 a\u0338=A\u2217             \u2206a            \u2212  \u03c32\u03c3\u22122 0 \u2206a        1{\u2206a \u2265        \u03b5} \uf8fb   .  (7)\n Case 2: The gap is small, \u2206A                t < \u03b5. Then naively  n           t=1 E [\u2206At1{\u2206At < \u03b5}] < \u03b5n.\n Case 3: Event Et does not occur. The last term in (5) can be bounded as\n                  \u00af\n  E    \u2206At1       Et                   \u00af       Ht                                                                                         (8)\n   = E      E   (\u03b8A\u2217    \u2212  \u03b8At)1      Et  \u00af       Ht                                      \u00af       Ht\n   \u2264   E    E   (\u03b8A\u2217    \u2212  Ut,A\u2217)1       Et             + E      (Ut,At \u2212     \u03b8At)1      Et\n   \u2264   E    E    (\u03b8A\u2217   \u2212   \u02c6             \u00af                                              \u00af                                   \u00af       Ht\n                            \u03b8t,A\u2217)1      Et             + E      (\u02c6\n                                                  Ht              \u03b8t,At \u2212     \u03b8At)1      Et      Ht     + E     Ct,At1      Et             .\n To bound the resulting terms, we use that \u03b8a \u2212                      \u02c6\n                                                                     \u03b8t,a | Ht \u223c      N   (0, \u02c6\u03c32t,a).\n Lemma 7. For any action a \u2208                  A, round t \u2208        [n], and history Ht,\n                                            E    (\u03b8a \u2212    \u02c6           \u00af\n                                                          \u03b8t,a)1     Et13     Ht     \u2264  2\u03c30K\u03b4 .", "md": "Proofs\n\nWe present a general approach for deriving finite-time logarithmic Bayes regret bounds. We start with K-armed bandits and then extend it to linear bandits.\n\nProof of Theorem 1\n\nLet \\( E_t = \\forall a \\in A : |\\theta_a - \\hat{\\theta}_{t,a}| \\leq C_{t,a} \\) be the event that all confidence intervals at round t hold.\n\nFix \\( \\epsilon > 0 \\). We start with decomposing the n-round regret as\n\n$$\n\\begin{aligned}\n&\\sum_{t=1}^{n} E[\\Delta A_t] \\leq \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t \\geq \\epsilon, E_t\\}] + \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t < \\epsilon}] + \\sum_{t=1}^{n} E[\\Delta A_t 1\\overline{E_t}].\n\\end{aligned}\n$$\nWe bound the first term using the design of BayesUCB and its closed-form posteriors.\n\nCase 1: Event \\( E_t \\) occurs and the gap is large, \\( \\Delta A_t \\geq \\epsilon \\). Since \\( E_t \\) occurs and the action with the highest UCB is taken, the regret at round t can be bounded as\n\n$$\n\\begin{aligned}\n\\Delta A_t &= \\theta_{A^*} - \\theta_{A_t} \\leq \\theta_{A^*} - U_{t,A^*} + U_{t,A_t} - \\theta_{A_t} \\leq U_{t,A_t} - \\theta_{A_t} \\leq 2C_{t,A_t}.\n\\end{aligned}\n$$\nIn the second inequality, we use that \\( \\theta_{A^*} \\leq U_{t,A^*} \\) on event \\( E_t \\). This implies that on event \\( E_t \\), action a can be taken only if\n\n$$\n\\begin{aligned}\n\\Delta a \\leq 2C_{t,a} &= 2\\sqrt{2}\\sigma_{t,a} \\log(1/\\delta) = 2\\sqrt{2} \\log(1/\\delta) \\left( \\sigma^{-2} + \\sigma^{-2}N_{t,a} \\right),\n\\end{aligned}\n$$\nwhich can be rearranged as\n\n$$\n\\begin{aligned}\nN_{t,a} \\leq 8\\sigma^2 \\log(1/\\delta) - \\sigma^2\\sigma^{-2}_0.\n\\end{aligned}\n$$\nTherefore, the number of times that action a is taken in n rounds while all confidence intervals hold, \\( N_a = \\sum_{t=1}^{n} 1\\{A_t = a, E_t\\} \\), is bounded as\n\n$$\n\\begin{aligned}\nN_a \\leq 8\\sigma^2 \\log(1/\\delta) - \\sigma^2\\sigma^{-2}_0.\n\\end{aligned}\n$$\nNow we apply this inequality to bound the first term in (5) as\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t \\geq \\epsilon, E_t\\}] \\leq E \\left[ \\sum_{a \\neq A^*} \\Delta a - \\sigma^2\\sigma^{-2}_0 \\Delta a 1\\{\\Delta a \\geq \\epsilon\\} \\right].\n\\end{aligned}\n$$\nCase 2: The gap is small, \\( \\Delta A_t < \\epsilon \\). Then naively \\( \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t < \\epsilon\\}] < \\epsilon n \\).\n\nCase 3: Event \\( E_t \\) does not occur. The last term in (5) can be bounded as\n\n$$\n\\begin{aligned}\nE[\\Delta A_t 1\\overline{E_t}] \\leq E\\left[ E(\\theta_{A^*} - \\theta_{A_t})1\\overline{E_t} \\right] \\leq E\\left[ E(\\theta_{A^*} - U_{t,A^*})1\\overline{E_t} \\right] + E\\left[ (U_{t,A_t} - \\theta_{A_t})1\\overline{E_t} \\right] + E\\left[ C_{t,A_t}1\\overline{E_t} \\right].\n\\end{aligned}\n$$\nTo bound the resulting terms, we use that \\( \\theta_a - \\hat{\\theta}_{t,a} | H_t \\sim N(0, \\hat{\\sigma}^2_{t,a}) \\).\n\nLemma 7. For any action a in A, round t in [n], and history \\( H_t \\),\n\n$$\n\\begin{aligned}\nE(\\theta_a - \\hat{\\theta}_{t,a})1_{E_t}^3 H_t \\leq 2\\sigma_0 K \\delta.\n\\end{aligned}\n$$", "images": [], "items": [{"type": "text", "value": "Proofs\n\nWe present a general approach for deriving finite-time logarithmic Bayes regret bounds. We start with K-armed bandits and then extend it to linear bandits.\n\nProof of Theorem 1\n\nLet \\( E_t = \\forall a \\in A : |\\theta_a - \\hat{\\theta}_{t,a}| \\leq C_{t,a} \\) be the event that all confidence intervals at round t hold.\n\nFix \\( \\epsilon > 0 \\). We start with decomposing the n-round regret as\n\n$$\n\\begin{aligned}\n&\\sum_{t=1}^{n} E[\\Delta A_t] \\leq \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t \\geq \\epsilon, E_t\\}] + \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t < \\epsilon}] + \\sum_{t=1}^{n} E[\\Delta A_t 1\\overline{E_t}].\n\\end{aligned}\n$$\nWe bound the first term using the design of BayesUCB and its closed-form posteriors.\n\nCase 1: Event \\( E_t \\) occurs and the gap is large, \\( \\Delta A_t \\geq \\epsilon \\). Since \\( E_t \\) occurs and the action with the highest UCB is taken, the regret at round t can be bounded as\n\n$$\n\\begin{aligned}\n\\Delta A_t &= \\theta_{A^*} - \\theta_{A_t} \\leq \\theta_{A^*} - U_{t,A^*} + U_{t,A_t} - \\theta_{A_t} \\leq U_{t,A_t} - \\theta_{A_t} \\leq 2C_{t,A_t}.\n\\end{aligned}\n$$\nIn the second inequality, we use that \\( \\theta_{A^*} \\leq U_{t,A^*} \\) on event \\( E_t \\). This implies that on event \\( E_t \\), action a can be taken only if\n\n$$\n\\begin{aligned}\n\\Delta a \\leq 2C_{t,a} &= 2\\sqrt{2}\\sigma_{t,a} \\log(1/\\delta) = 2\\sqrt{2} \\log(1/\\delta) \\left( \\sigma^{-2} + \\sigma^{-2}N_{t,a} \\right),\n\\end{aligned}\n$$\nwhich can be rearranged as\n\n$$\n\\begin{aligned}\nN_{t,a} \\leq 8\\sigma^2 \\log(1/\\delta) - \\sigma^2\\sigma^{-2}_0.\n\\end{aligned}\n$$\nTherefore, the number of times that action a is taken in n rounds while all confidence intervals hold, \\( N_a = \\sum_{t=1}^{n} 1\\{A_t = a, E_t\\} \\), is bounded as\n\n$$\n\\begin{aligned}\nN_a \\leq 8\\sigma^2 \\log(1/\\delta) - \\sigma^2\\sigma^{-2}_0.\n\\end{aligned}\n$$\nNow we apply this inequality to bound the first term in (5) as\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t \\geq \\epsilon, E_t\\}] \\leq E \\left[ \\sum_{a \\neq A^*} \\Delta a - \\sigma^2\\sigma^{-2}_0 \\Delta a 1\\{\\Delta a \\geq \\epsilon\\} \\right].\n\\end{aligned}\n$$\nCase 2: The gap is small, \\( \\Delta A_t < \\epsilon \\). Then naively \\( \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t < \\epsilon\\}] < \\epsilon n \\).\n\nCase 3: Event \\( E_t \\) does not occur. The last term in (5) can be bounded as\n\n$$\n\\begin{aligned}\nE[\\Delta A_t 1\\overline{E_t}] \\leq E\\left[ E(\\theta_{A^*} - \\theta_{A_t})1\\overline{E_t} \\right] \\leq E\\left[ E(\\theta_{A^*} - U_{t,A^*})1\\overline{E_t} \\right] + E\\left[ (U_{t,A_t} - \\theta_{A_t})1\\overline{E_t} \\right] + E\\left[ C_{t,A_t}1\\overline{E_t} \\right].\n\\end{aligned}\n$$\nTo bound the resulting terms, we use that \\( \\theta_a - \\hat{\\theta}_{t,a} | H_t \\sim N(0, \\hat{\\sigma}^2_{t,a}) \\).\n\nLemma 7. For any action a in A, round t in [n], and history \\( H_t \\),\n\n$$\n\\begin{aligned}\nE(\\theta_a - \\hat{\\theta}_{t,a})1_{E_t}^3 H_t \\leq 2\\sigma_0 K \\delta.\n\\end{aligned}\n$$", "md": "Proofs\n\nWe present a general approach for deriving finite-time logarithmic Bayes regret bounds. We start with K-armed bandits and then extend it to linear bandits.\n\nProof of Theorem 1\n\nLet \\( E_t = \\forall a \\in A : |\\theta_a - \\hat{\\theta}_{t,a}| \\leq C_{t,a} \\) be the event that all confidence intervals at round t hold.\n\nFix \\( \\epsilon > 0 \\). We start with decomposing the n-round regret as\n\n$$\n\\begin{aligned}\n&\\sum_{t=1}^{n} E[\\Delta A_t] \\leq \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t \\geq \\epsilon, E_t\\}] + \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t < \\epsilon}] + \\sum_{t=1}^{n} E[\\Delta A_t 1\\overline{E_t}].\n\\end{aligned}\n$$\nWe bound the first term using the design of BayesUCB and its closed-form posteriors.\n\nCase 1: Event \\( E_t \\) occurs and the gap is large, \\( \\Delta A_t \\geq \\epsilon \\). Since \\( E_t \\) occurs and the action with the highest UCB is taken, the regret at round t can be bounded as\n\n$$\n\\begin{aligned}\n\\Delta A_t &= \\theta_{A^*} - \\theta_{A_t} \\leq \\theta_{A^*} - U_{t,A^*} + U_{t,A_t} - \\theta_{A_t} \\leq U_{t,A_t} - \\theta_{A_t} \\leq 2C_{t,A_t}.\n\\end{aligned}\n$$\nIn the second inequality, we use that \\( \\theta_{A^*} \\leq U_{t,A^*} \\) on event \\( E_t \\). This implies that on event \\( E_t \\), action a can be taken only if\n\n$$\n\\begin{aligned}\n\\Delta a \\leq 2C_{t,a} &= 2\\sqrt{2}\\sigma_{t,a} \\log(1/\\delta) = 2\\sqrt{2} \\log(1/\\delta) \\left( \\sigma^{-2} + \\sigma^{-2}N_{t,a} \\right),\n\\end{aligned}\n$$\nwhich can be rearranged as\n\n$$\n\\begin{aligned}\nN_{t,a} \\leq 8\\sigma^2 \\log(1/\\delta) - \\sigma^2\\sigma^{-2}_0.\n\\end{aligned}\n$$\nTherefore, the number of times that action a is taken in n rounds while all confidence intervals hold, \\( N_a = \\sum_{t=1}^{n} 1\\{A_t = a, E_t\\} \\), is bounded as\n\n$$\n\\begin{aligned}\nN_a \\leq 8\\sigma^2 \\log(1/\\delta) - \\sigma^2\\sigma^{-2}_0.\n\\end{aligned}\n$$\nNow we apply this inequality to bound the first term in (5) as\n\n$$\n\\begin{aligned}\n\\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t \\geq \\epsilon, E_t\\}] \\leq E \\left[ \\sum_{a \\neq A^*} \\Delta a - \\sigma^2\\sigma^{-2}_0 \\Delta a 1\\{\\Delta a \\geq \\epsilon\\} \\right].\n\\end{aligned}\n$$\nCase 2: The gap is small, \\( \\Delta A_t < \\epsilon \\). Then naively \\( \\sum_{t=1}^{n} E[\\Delta A_t 1\\{\\Delta A_t < \\epsilon\\}] < \\epsilon n \\).\n\nCase 3: Event \\( E_t \\) does not occur. The last term in (5) can be bounded as\n\n$$\n\\begin{aligned}\nE[\\Delta A_t 1\\overline{E_t}] \\leq E\\left[ E(\\theta_{A^*} - \\theta_{A_t})1\\overline{E_t} \\right] \\leq E\\left[ E(\\theta_{A^*} - U_{t,A^*})1\\overline{E_t} \\right] + E\\left[ (U_{t,A_t} - \\theta_{A_t})1\\overline{E_t} \\right] + E\\left[ C_{t,A_t}1\\overline{E_t} \\right].\n\\end{aligned}\n$$\nTo bound the resulting terms, we use that \\( \\theta_a - \\hat{\\theta}_{t,a} | H_t \\sim N(0, \\hat{\\sigma}^2_{t,a}) \\).\n\nLemma 7. For any action a in A, round t in [n], and history \\( H_t \\),\n\n$$\n\\begin{aligned}\nE(\\theta_a - \\hat{\\theta}_{t,a})1_{E_t}^3 H_t \\leq 2\\sigma_0 K \\delta.\n\\end{aligned}\n$$"}]}, {"page": 14, "text": "Proof. Let Et,a =             |\u03b8a \u2212    \u02c6\n                                       \u03b8t,a| \u2264    Ct,a    . We start with decomposing \u00af              Et into individual \u00af      Et,a as\n E    (\u03b8a \u2212    \u02c6           \u00af                                          \u00af                                                    \u00af\n               \u03b8t,a)1     Et             \u2264   E    |\u03b8a \u2212    \u02c6\n                                                           \u03b8t,a|1    Et,a              +          E   |\u03b8a \u2212    \u02c6\n                                  Ht                                            Ht         a\u2032\u0338=a               \u03b8t,a|1     Et,a\u2032       Ht  .\nTo bound the first term, we use that \u03b8a \u2212                   \u02c6\n                                                           \u03b8t,a | Ht \u223c       N  (0, \u02c6\u03c32t,a). Thus\n                E    |\u03b8a \u2212    \u02c6          \u00af                          2            \u221e       x exp      \u2212    x2       dx\n                              \u03b8t,a|1    Et,a       Ht     \u2264       2\u03c0\u02c6 \u03c32t,a    x=Ct,a                  2\u02c6\u03c32t,a\n                                                          = \u2212        2\u02c6\u03c32t,a     \u221e         \u2202      exp     \u2212    x2         dx\n                                                                        \u03c0       x=Ct,a    \u2202x                 2\u02c6\u03c32t,a\n                                                          =    2\u02c6   \u03c32t,a \u03b4 \u2264    \u03c30\u03b4 .\n                                                                     \u03c0\nTo bound the second term, we use the independence of the distributions for a and a\u2032,\n                           |\u03b8a \u2212    \u02c6          \u00af                                                       \u00af        Ht     .\n                                    \u03b8t,a|1     Et,a\u2032             = E      |\u03b8a \u2212    \u02c6\n                      E                                   Ht                       \u03b8t,a|    Ht    P    Et,a\u2032\nThe probability is at most 2\u03b4 and the expectation can be bounded as\n       E    |\u03b8a \u2212    \u02c6               = E\n                     \u03b8t,a|                       (\u03b8a \u2212     \u02c6                                                        = \u02c6\u03c3t,a \u2264     \u03c30 .\n                                                          \u03b8t,a)2            \u2264       E    (\u03b8a \u2212    \u02c6\nThis completes the proof.     Ht                                     Ht                           \u03b8t,a)2     Ht\nThe first two terms in (8) can be bounded using a union bound over a \u2208                                A and Lemma 7. For the last\n                                                                                                 \u00af    Ht\nterm, we use that Ct,a \u2264                2\u03c32 0 log(1/\u03b4) and a union bound in P                   Et           to get\n                                  \u00af      Ht     \u2264                               \u00af    Ht     \u2264   2\n                E   Ct,At1       Et                    2\u03c32 0 log(1/\u03b4)P         Et                    2 log(1/\u03b4)\u03c30K\u03b4 .\nFinally, we sum up the upper bounds on (8) over all rounds t \u2208                              [n].\nA.2      Proof of Corollary 2\nThe key idea in the proof is to integrate out the random gap in (7). Fix action a \u2208                                   A and thresholds\n\u03b52 > \u03b5 > 0. We consider two cases.\nCase 1: Diminishing gaps \u03b5 < \u2206a \u2264                       \u03b52. Let\n                                                 \u03bea(\u03b8\u2217  a) = arg max         ha(\u03b8\u2217  a \u2212   x)\nand Na be defined as in Appendix A.1. Then                \u03b8\u2217     x\u2208[\u03b5,\u03b52]\n                                                           a\u2212\u03b5\n  E [\u2206aNa1{\u03b5 < \u2206a < \u03b52}] =                      \u03b8\u2212a     \u03b8a=\u03b8\u2217  a\u2212\u03b52   \u2206aNaha(\u03b8a) d\u03b8a h\u2212a(\u03b8\u2212a) d\u03b8\u2212a\n                                          \u2264                 \u03b8\u2217a\u2212\u03b5        \u2206aNa d\u03b8a           ha(\u03b8\u2217  a \u2212   \u03bea(\u03b8\u2217 a)) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a ,\n                                                \u03b8\u2212a        \u03b8a=\u03b8\u2217 a\u2212\u03b52\nwhere the inequality is by the definition of \u03bea. Now the inner integral is independent of ha and thus\ncan be easily bounded. Specifically, the upper bound in (6) and simple integration yield\n                   \u03b8\u2217                                  \u03b8\u2217            8\u03c32 log(1/\u03b4)\n                    a\u2212\u03b5        \u2206aNa d\u03b8a \u2264                a\u2212\u03b5               \u03b8\u2217              \u2212   \u03c32\u03c3\u221220 (\u03b8\u2217   a \u2212   \u03b8a)     d\u03b8a\n                 \u03b8a=\u03b8\u2217  a\u2212\u03b52                          \u03b8a=\u03b8\u2217 a\u2212\u03b52            a \u2212   \u03b8a\n                                                = 8\u03c32 log(1/\u03b4)(log \u03b52 \u2212              log \u03b5) \u2212     \u03c32(\u03b52  2 \u2212   \u03b52)   .\n                                                                                                        2\u03c32 0\n                                                                      14", "md": "Proof. Let $$E_{t,a} = |\\theta_{a} - \\hat{\\theta}_{t,a}| \\leq C_{t,a}$$. We start with decomposing $$\\overline{E}_{t}$$ into individual $$\\overline{E}_{t,a}$$ as\n$$E(\\theta_{a} - \\hat{\\theta}_{t,a})1_{E_{t}} \\leq E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} + E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t} \\cap a' \\neq a}$$.\n\nTo bound the first term, we use that $$\\theta_{a} - \\hat{\\theta}_{t,a} | H_{t} \\sim N(0, \\hat{\\sigma}^2_{t,a})$$. Thus\n$$E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} H_{t} \\leq \\frac{1}{2\\pi \\hat{\\sigma}^2_{t,a}} \\int_{C_{t,a}}^{\\infty} x \\exp\\left(-\\frac{x^2}{2\\hat{\\sigma}^2_{t,a}}\\right) dx = -\\frac{1}{\\pi} \\frac{2\\hat{\\sigma}^2_{t,a}}{\\infty} \\frac{\\partial}{\\partial x} \\exp\\left(-\\frac{x^2}{2\\hat{\\sigma}^2_{t,a}}\\right) dx = \\frac{2\\hat{\\sigma}^2_{t,a}}{\\pi} \\delta \\leq \\sigma_{0}\\delta$$.\n\nTo bound the second term, we use the independence of the distributions for $$a$$ and $$a'$$,\n$$|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}} = E|(\\theta_{a} - \\hat{\\theta}_{t,a}) H_{t} \\theta_{t,a}| H_{t} P(E_{t,a'})$$.\n\nThe probability is at most $$2\\delta$$ and the expectation can be bounded as\n$$E|\\theta_{a} - \\hat{\\theta}_{t,a}| = E(\\theta_{a} - \\hat{\\theta}_{t,a})^2 \\leq E(\\theta_{a} - \\hat{\\theta}_{t,a})^2 H_{t} = \\hat{\\sigma}^2_{t,a} \\leq \\sigma_{0}$$.\n\nThis completes the proof. The first two terms in (8) can be bounded using a union bound over $$a \\in A$$ and Lemma 7. For the last term, we use that $$C_{t,a} \\leq 2\\sigma_{0}^2 \\log(1/\\delta)$$ and a union bound in $$P(E_{t})$$ to get\n$$\\overline{E}_{t} \\leq E[C_{t,A}1_{E_{t}}] \\leq 2\\sigma_{0}^2 \\log(1/\\delta)P(E_{t}) \\leq 2\\log(1/\\delta)\\sigma_{0}K\\delta$$.\n\nFinally, we sum up the upper bounds on (8) over all rounds $$t \\in [n]$$.\n\n**Proof of Corollary 2**\n\nThe key idea in the proof is to integrate out the random gap in (7). Fix action $$a \\in A$$ and thresholds $$\\epsilon^2 > \\epsilon > 0$$. We consider two cases.\n\n**Case 1:** Diminishing gaps $$\\epsilon < \\Delta_{a} \\leq \\epsilon^2$$. Let\n$$\\xi_{a}(\\theta^{*}_{a}) = \\arg \\max_{x \\in [\\epsilon, \\epsilon^2]} h_{a}(\\theta^{*}_{a} - x)$$\nand $$N_{a}$$ be defined as in Appendix A.1. Then\n$$E[\\Delta_{a}N_{a}1\\{\\epsilon < \\Delta_{a} < \\epsilon^2\\}] = \\int_{\\theta^{-a}} \\int_{\\theta_{a} = \\theta^{*}_{a} - \\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a}h_{a}(\\theta_{a}) d\\theta_{a} h^{-a}(\\theta^{-a}) d\\theta^{-a}$$\n$$\\leq \\int_{\\theta^{-a}} \\int_{\\theta_{a} = \\theta^{*}_{a} - \\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a} d\\theta_{a} h_{a}(\\theta^{*}_{a} - \\xi_{a}(\\theta^{*}_{a})) h^{-a}(\\theta^{-a}) d\\theta^{-a}$$,\n\nwhere the inequality is by the definition of $$\\xi_{a}$$. Now the inner integral is independent of $$h_{a}$$ and thus can be easily bounded. Specifically, the upper bound in (6) and simple integration yield\n$$\\int_{\\theta^{*}_{a}-\\epsilon}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a} d\\theta_{a} \\leq \\int_{\\theta^{*}_{a}-\\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\sigma^2 \\log(1/\\delta)(\\log \\epsilon^2 - \\log \\epsilon) - \\sigma^2(\\epsilon^2/2 - \\epsilon^2)/2\\sigma_{0}$$.\n\nThis completes the proof.", "images": [], "items": [{"type": "text", "value": "Proof. Let $$E_{t,a} = |\\theta_{a} - \\hat{\\theta}_{t,a}| \\leq C_{t,a}$$. We start with decomposing $$\\overline{E}_{t}$$ into individual $$\\overline{E}_{t,a}$$ as\n$$E(\\theta_{a} - \\hat{\\theta}_{t,a})1_{E_{t}} \\leq E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} + E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t} \\cap a' \\neq a}$$.\n\nTo bound the first term, we use that $$\\theta_{a} - \\hat{\\theta}_{t,a} | H_{t} \\sim N(0, \\hat{\\sigma}^2_{t,a})$$. Thus\n$$E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} H_{t} \\leq \\frac{1}{2\\pi \\hat{\\sigma}^2_{t,a}} \\int_{C_{t,a}}^{\\infty} x \\exp\\left(-\\frac{x^2}{2\\hat{\\sigma}^2_{t,a}}\\right) dx = -\\frac{1}{\\pi} \\frac{2\\hat{\\sigma}^2_{t,a}}{\\infty} \\frac{\\partial}{\\partial x} \\exp\\left(-\\frac{x^2}{2\\hat{\\sigma}^2_{t,a}}\\right) dx = \\frac{2\\hat{\\sigma}^2_{t,a}}{\\pi} \\delta \\leq \\sigma_{0}\\delta$$.\n\nTo bound the second term, we use the independence of the distributions for $$a$$ and $$a'$$,\n$$|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}} = E|(\\theta_{a} - \\hat{\\theta}_{t,a}) H_{t} \\theta_{t,a}| H_{t} P(E_{t,a'})$$.\n\nThe probability is at most $$2\\delta$$ and the expectation can be bounded as\n$$E|\\theta_{a} - \\hat{\\theta}_{t,a}| = E(\\theta_{a} - \\hat{\\theta}_{t,a})^2 \\leq E(\\theta_{a} - \\hat{\\theta}_{t,a})^2 H_{t} = \\hat{\\sigma}^2_{t,a} \\leq \\sigma_{0}$$.\n\nThis completes the proof. The first two terms in (8) can be bounded using a union bound over $$a \\in A$$ and Lemma 7. For the last term, we use that $$C_{t,a} \\leq 2\\sigma_{0}^2 \\log(1/\\delta)$$ and a union bound in $$P(E_{t})$$ to get\n$$\\overline{E}_{t} \\leq E[C_{t,A}1_{E_{t}}] \\leq 2\\sigma_{0}^2 \\log(1/\\delta)P(E_{t}) \\leq 2\\log(1/\\delta)\\sigma_{0}K\\delta$$.\n\nFinally, we sum up the upper bounds on (8) over all rounds $$t \\in [n]$$.\n\n**Proof of Corollary 2**\n\nThe key idea in the proof is to integrate out the random gap in (7). Fix action $$a \\in A$$ and thresholds $$\\epsilon^2 > \\epsilon > 0$$. We consider two cases.\n\n**Case 1:** Diminishing gaps $$\\epsilon < \\Delta_{a} \\leq \\epsilon^2$$. Let\n$$\\xi_{a}(\\theta^{*}_{a}) = \\arg \\max_{x \\in [\\epsilon, \\epsilon^2]} h_{a}(\\theta^{*}_{a} - x)$$\nand $$N_{a}$$ be defined as in Appendix A.1. Then\n$$E[\\Delta_{a}N_{a}1\\{\\epsilon < \\Delta_{a} < \\epsilon^2\\}] = \\int_{\\theta^{-a}} \\int_{\\theta_{a} = \\theta^{*}_{a} - \\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a}h_{a}(\\theta_{a}) d\\theta_{a} h^{-a}(\\theta^{-a}) d\\theta^{-a}$$\n$$\\leq \\int_{\\theta^{-a}} \\int_{\\theta_{a} = \\theta^{*}_{a} - \\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a} d\\theta_{a} h_{a}(\\theta^{*}_{a} - \\xi_{a}(\\theta^{*}_{a})) h^{-a}(\\theta^{-a}) d\\theta^{-a}$$,\n\nwhere the inequality is by the definition of $$\\xi_{a}$$. Now the inner integral is independent of $$h_{a}$$ and thus can be easily bounded. Specifically, the upper bound in (6) and simple integration yield\n$$\\int_{\\theta^{*}_{a}-\\epsilon}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a} d\\theta_{a} \\leq \\int_{\\theta^{*}_{a}-\\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\sigma^2 \\log(1/\\delta)(\\log \\epsilon^2 - \\log \\epsilon) - \\sigma^2(\\epsilon^2/2 - \\epsilon^2)/2\\sigma_{0}$$.\n\nThis completes the proof.", "md": "Proof. Let $$E_{t,a} = |\\theta_{a} - \\hat{\\theta}_{t,a}| \\leq C_{t,a}$$. We start with decomposing $$\\overline{E}_{t}$$ into individual $$\\overline{E}_{t,a}$$ as\n$$E(\\theta_{a} - \\hat{\\theta}_{t,a})1_{E_{t}} \\leq E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} + E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t} \\cap a' \\neq a}$$.\n\nTo bound the first term, we use that $$\\theta_{a} - \\hat{\\theta}_{t,a} | H_{t} \\sim N(0, \\hat{\\sigma}^2_{t,a})$$. Thus\n$$E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} H_{t} \\leq \\frac{1}{2\\pi \\hat{\\sigma}^2_{t,a}} \\int_{C_{t,a}}^{\\infty} x \\exp\\left(-\\frac{x^2}{2\\hat{\\sigma}^2_{t,a}}\\right) dx = -\\frac{1}{\\pi} \\frac{2\\hat{\\sigma}^2_{t,a}}{\\infty} \\frac{\\partial}{\\partial x} \\exp\\left(-\\frac{x^2}{2\\hat{\\sigma}^2_{t,a}}\\right) dx = \\frac{2\\hat{\\sigma}^2_{t,a}}{\\pi} \\delta \\leq \\sigma_{0}\\delta$$.\n\nTo bound the second term, we use the independence of the distributions for $$a$$ and $$a'$$,\n$$|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}} = E|(\\theta_{a} - \\hat{\\theta}_{t,a}) H_{t} \\theta_{t,a}| H_{t} P(E_{t,a'})$$.\n\nThe probability is at most $$2\\delta$$ and the expectation can be bounded as\n$$E|\\theta_{a} - \\hat{\\theta}_{t,a}| = E(\\theta_{a} - \\hat{\\theta}_{t,a})^2 \\leq E(\\theta_{a} - \\hat{\\theta}_{t,a})^2 H_{t} = \\hat{\\sigma}^2_{t,a} \\leq \\sigma_{0}$$.\n\nThis completes the proof. The first two terms in (8) can be bounded using a union bound over $$a \\in A$$ and Lemma 7. For the last term, we use that $$C_{t,a} \\leq 2\\sigma_{0}^2 \\log(1/\\delta)$$ and a union bound in $$P(E_{t})$$ to get\n$$\\overline{E}_{t} \\leq E[C_{t,A}1_{E_{t}}] \\leq 2\\sigma_{0}^2 \\log(1/\\delta)P(E_{t}) \\leq 2\\log(1/\\delta)\\sigma_{0}K\\delta$$.\n\nFinally, we sum up the upper bounds on (8) over all rounds $$t \\in [n]$$.\n\n**Proof of Corollary 2**\n\nThe key idea in the proof is to integrate out the random gap in (7). Fix action $$a \\in A$$ and thresholds $$\\epsilon^2 > \\epsilon > 0$$. We consider two cases.\n\n**Case 1:** Diminishing gaps $$\\epsilon < \\Delta_{a} \\leq \\epsilon^2$$. Let\n$$\\xi_{a}(\\theta^{*}_{a}) = \\arg \\max_{x \\in [\\epsilon, \\epsilon^2]} h_{a}(\\theta^{*}_{a} - x)$$\nand $$N_{a}$$ be defined as in Appendix A.1. Then\n$$E[\\Delta_{a}N_{a}1\\{\\epsilon < \\Delta_{a} < \\epsilon^2\\}] = \\int_{\\theta^{-a}} \\int_{\\theta_{a} = \\theta^{*}_{a} - \\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a}h_{a}(\\theta_{a}) d\\theta_{a} h^{-a}(\\theta^{-a}) d\\theta^{-a}$$\n$$\\leq \\int_{\\theta^{-a}} \\int_{\\theta_{a} = \\theta^{*}_{a} - \\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a} d\\theta_{a} h_{a}(\\theta^{*}_{a} - \\xi_{a}(\\theta^{*}_{a})) h^{-a}(\\theta^{-a}) d\\theta^{-a}$$,\n\nwhere the inequality is by the definition of $$\\xi_{a}$$. Now the inner integral is independent of $$h_{a}$$ and thus can be easily bounded. Specifically, the upper bound in (6) and simple integration yield\n$$\\int_{\\theta^{*}_{a}-\\epsilon}^{\\theta^{*}_{a}-\\epsilon} \\Delta_{a}N_{a} d\\theta_{a} \\leq \\int_{\\theta^{*}_{a}-\\epsilon^2}^{\\theta^{*}_{a}-\\epsilon} \\sigma^2 \\log(1/\\delta)(\\log \\epsilon^2 - \\log \\epsilon) - \\sigma^2(\\epsilon^2/2 - \\epsilon^2)/2\\sigma_{0}$$.\n\nThis completes the proof."}]}, {"page": 15, "text": " For \u03b5 = 1/n and \u03b52 = 1/\u221alog n, we get\n       \u03b8\u2217a\u2212\u03b5        \u2206aNa d\u03b8a \u2264          8\u03c32 log(1/\u03b4) log n \u2212                 \u03c32              \u03c32\n      \u03b8a=\u03b8\u2217 a\u2212\u03b52                                                        2\u03c32 0 log n +      2\u03c32 0n2 \u2212     4\u03c32 log(1/\u03b4) log log n (9)\n                                                                             \u03c32\n                                    \u2264   8\u03c32 log(1/\u03b4) log n \u2212            2\u03c32 0 log n .\n The last inequality holds for \u03c32           0 \u2265    8 log(1/\u03b4) n21log log n.\n Case 2: Large gaps \u2206a > \u03b52. Here we use (6) together with \u03b52 = 1/\u221alog n to get\n            E [\u2206aNa1{\u2206a > \u03b52}] \u2264                  E   8\u03c32 log(1/\u03b4)         1{\u2206a > \u03b52}            < 8\u03c32 log(1/\u03b4)            log n .     (10)\n                                                               \u2206a\n Finally, we chain all inequalities.\n A.3      Proof of Lemma 3\nWe have that         a\u2208A      \u03b8\u2212a  ha(\u03b8\u2217  a) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a\n                         \u2264   a\u2208A     \u03b8\u2212a       a\u2032\u0338=a  ha(\u03b8a\u2032)          a\u2032\u0338=a  ha\u2032(\u03b8a\u2032)       d\u03b8\u2212a\n                         =   a\u2208A   a\u2032\u0338=a    \u03b8 a\u2032 ha(\u03b8a\u2032) ha\u2032(\u03b8a\u2032) d\u03b8a\u2032\n                         =      1                         exp     \u2212(\u03b8a\u2032 \u2212       \u00b50,a)2    \u2212   (\u03b8a\u2032 \u2212     \u00b50,a\u2032)2      d\u03b8a\u2032\n                             2\u03c0\u03c32  0  a\u2208A    a\u2032\u0338=a    \u03b8a\u2032                    2\u03c320                     2\u03c32 0\n                         =   2    1\u03c0\u03c32                 exp     \u2212(\u00b50,a \u2212    4\u03c32 \u00b50,a\u2032)2      ,\n                                       0  a\u2208A   a\u2032\u0338=a                          0\n where the last step is by completing the square and integrating out \u03b8a\u2032.\n A.4      Proof of Theorem 4\n The regret bound of UCB1 is proved similarly to Theorem 1 and Corollary 2. This is because UCB1\n can be viewed as BayesUCB where \u03c30 = \u221e                           and each action a \u2208           A is initially taken once at round\n t = a. Since \u03c30 = \u221e, the confidence interval becomes Ct,a =      2\u03c32 log(1/\u03b4)           .\n                                                                            Nt,a\n The proof differs in two steps. First, the regret in the first K rounds is bounded by  a\u2208A E [\u2206a].\n Second, the concentration argument (Case 3 in Appendix A.1) changes because the bandit instance \u03b8\n is fixed and the estimated model parameter \u02c6                  \u03b8t is random. We detail it below.\n Case 3: Event Et does\u00afnot occur. The last term in (5) can be bounded as\n      E   \u2206At1       Et                    \u00af       \u03b8                                                                                   (11)\n       = E     E    (\u03b8A\u2217    \u2212  \u03b8At)1      Et\n       \u2264   E   E    (\u03b8A\u2217    \u2212  Ut,A\u2217)1        \u00af       \u03b8   + E     (Ut,At \u2212      \u03b8At)1      \u00af       \u03b8\n                                             Et                                            Et\n       \u2264   E    E   (\u03b8A\u2217    \u2212   \u02c6             \u00af                                            \u00af                                \u00af       \u03b8\n                                \u03b8t,A\u2217)1      Et           + E     (\u02c6\n                                                     \u03b8             \u03b8t,At \u2212     \u03b8At)1      Et      \u03b8    + E     Ct,At1      Et          .\n To bound the resulting terms, we use that \u03b8a \u2212                      \u02c6\n                                                                     \u03b8t,a | \u03b8 \u223c     N  (0, \u03c32/Nt,a).\n                                                                       15", "md": "# Math Equations and Inequalities\n\n## Math Equations and Inequalities\n\nFor $$\\epsilon = \\frac{1}{n}$$ and $$\\epsilon_2 = \\frac{1}{\\sqrt{\\log n}}$$, we get\n\n$$\n\\theta^*a - \\epsilon \\Delta a N_a d\\theta_a \\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n - \\frac{\\sigma^2}{\\sigma^2} \\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n - 2\\sigma^2 \\log n.\n$$\nThe last inequality holds for $$\\sigma_0^2 \\geq 8 \\log\\left(\\frac{1}{\\delta}\\right) n^2\\log\\log n$$.\n\nCase 2: Large gaps $$\\Delta a > \\epsilon_2$$. Here we use (6) together with $$\\epsilon_2 = \\frac{1}{\\sqrt{\\log n}}$$ to get\n\n$$\nE[\\Delta a N_a 1\\{\\Delta a > \\epsilon_2\\}] \\leq E[8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) 1\\{\\Delta a > \\epsilon_2\\}] < 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n.\n$$\nFinally, we chain all inequalities.\n\n### Proof of Lemma 3\n\nWe have that\n\n$$\n\\begin{align*}\n&\\sum_{a\\in A} \\theta^{-a} h_a(\\theta^*_a) h^{-a}(\\theta^{-a}) d\\theta^{-a} \\\\\n&\\leq \\sum_{a\\in A} \\theta^{-a} \\sum_{a'\\neq a} h_a(\\theta_{a'}) \\sum_{a'\\neq a} h_{a'}(\\theta_{a'}) d\\theta^{-a} \\\\\n&= \\sum_{a\\in A} \\sum_{a'\\neq a} \\theta_{a'} h_a(\\theta_{a'}) h_{a'}(\\theta_{a'}) d\\theta_{a'} \\\\\n&= \\frac{1}{2\\pi\\sigma^2_0} \\sum_{a\\in A} \\sum_{a'\\neq a} \\exp\\left(-(\\theta_{a'} - \\mu_{0,a})^2 - (\\theta_{a'} - \\mu_{0,a'})^2\\right) d\\theta_{a'} \\\\\n&= 2\\cdot\\frac{1}{\\pi\\sigma^2_0} \\sum_{a\\in A} \\exp\\left(-(\\mu_{0,a} - 4\\sigma^2 \\mu_{0,a'})^2\\right),\n\\end{align*}\n$$\nwhere the last step is by completing the square and integrating out $$\\theta_{a'}$$.\n\n### Proof of Theorem 4\n\nThe regret bound of UCB1 is proved similarly to Theorem 1 and Corollary 2. This is because UCB1 can be viewed as BayesUCB where $$\\sigma_0 = \\infty$$ and each action $$a \\in A$$ is initially taken once at round $$t = a$$. Since $$\\sigma_0 = \\infty$$, the confidence interval becomes $$C_{t,a} = \\frac{2\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right)}{N_{t,a}}$$.\n\nThe proof differs in two steps. First, the regret in the first K rounds is bounded by $$\\sum_{a\\in A} E[\\Delta a]$$. Second, the concentration argument (Case 3 in Appendix A.1) changes because the bandit instance $$\\theta$$ is fixed and the estimated model parameter $$\\hat{\\theta}_t$$ is random. We detail it below.\n\nCase 3: Event $$E_t$$ does not occur. The last term in (5) can be bounded as\n\n$$\n\\begin{align*}\n& E[\\Delta A_t 1_{E_t} \\bar{\\theta}] \\\\\n&= E[E(\\theta_{A^*} - \\theta_{A_t}) 1_{E_t}] \\\\\n&\\leq E[E(\\theta_{A^*} - U_{t,A^*}) 1_{\\bar{\\theta}} + E(U_{t,A_t} - \\theta_{A_t}) 1_{\\bar{\\theta}} \\\\\n&\\leq E[E(\\theta_{A^*} - \\hat{\\theta}_{t,A^*}) 1_{E_t} + E(\\hat{\\theta}_{t,A_t} - \\theta_{A_t}) 1_{E_t} + E(C_{t,A_t} 1_{E_t}].\n\\end{align*}\n$$\nTo bound the resulting terms, we use that $$\\theta_a - \\hat{\\theta}_{t,a} | \\theta \\sim N(0, \\sigma^2/N_{t,a})$$.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Inequalities", "md": "# Math Equations and Inequalities"}, {"type": "heading", "lvl": 2, "value": "Math Equations and Inequalities", "md": "## Math Equations and Inequalities"}, {"type": "text", "value": "For $$\\epsilon = \\frac{1}{n}$$ and $$\\epsilon_2 = \\frac{1}{\\sqrt{\\log n}}$$, we get\n\n$$\n\\theta^*a - \\epsilon \\Delta a N_a d\\theta_a \\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n - \\frac{\\sigma^2}{\\sigma^2} \\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n - 2\\sigma^2 \\log n.\n$$\nThe last inequality holds for $$\\sigma_0^2 \\geq 8 \\log\\left(\\frac{1}{\\delta}\\right) n^2\\log\\log n$$.\n\nCase 2: Large gaps $$\\Delta a > \\epsilon_2$$. Here we use (6) together with $$\\epsilon_2 = \\frac{1}{\\sqrt{\\log n}}$$ to get\n\n$$\nE[\\Delta a N_a 1\\{\\Delta a > \\epsilon_2\\}] \\leq E[8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) 1\\{\\Delta a > \\epsilon_2\\}] < 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n.\n$$\nFinally, we chain all inequalities.", "md": "For $$\\epsilon = \\frac{1}{n}$$ and $$\\epsilon_2 = \\frac{1}{\\sqrt{\\log n}}$$, we get\n\n$$\n\\theta^*a - \\epsilon \\Delta a N_a d\\theta_a \\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n - \\frac{\\sigma^2}{\\sigma^2} \\leq 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n - 2\\sigma^2 \\log n.\n$$\nThe last inequality holds for $$\\sigma_0^2 \\geq 8 \\log\\left(\\frac{1}{\\delta}\\right) n^2\\log\\log n$$.\n\nCase 2: Large gaps $$\\Delta a > \\epsilon_2$$. Here we use (6) together with $$\\epsilon_2 = \\frac{1}{\\sqrt{\\log n}}$$ to get\n\n$$\nE[\\Delta a N_a 1\\{\\Delta a > \\epsilon_2\\}] \\leq E[8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) 1\\{\\Delta a > \\epsilon_2\\}] < 8\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right) \\log n.\n$$\nFinally, we chain all inequalities."}, {"type": "heading", "lvl": 3, "value": "Proof of Lemma 3", "md": "### Proof of Lemma 3"}, {"type": "text", "value": "We have that\n\n$$\n\\begin{align*}\n&\\sum_{a\\in A} \\theta^{-a} h_a(\\theta^*_a) h^{-a}(\\theta^{-a}) d\\theta^{-a} \\\\\n&\\leq \\sum_{a\\in A} \\theta^{-a} \\sum_{a'\\neq a} h_a(\\theta_{a'}) \\sum_{a'\\neq a} h_{a'}(\\theta_{a'}) d\\theta^{-a} \\\\\n&= \\sum_{a\\in A} \\sum_{a'\\neq a} \\theta_{a'} h_a(\\theta_{a'}) h_{a'}(\\theta_{a'}) d\\theta_{a'} \\\\\n&= \\frac{1}{2\\pi\\sigma^2_0} \\sum_{a\\in A} \\sum_{a'\\neq a} \\exp\\left(-(\\theta_{a'} - \\mu_{0,a})^2 - (\\theta_{a'} - \\mu_{0,a'})^2\\right) d\\theta_{a'} \\\\\n&= 2\\cdot\\frac{1}{\\pi\\sigma^2_0} \\sum_{a\\in A} \\exp\\left(-(\\mu_{0,a} - 4\\sigma^2 \\mu_{0,a'})^2\\right),\n\\end{align*}\n$$\nwhere the last step is by completing the square and integrating out $$\\theta_{a'}$$.", "md": "We have that\n\n$$\n\\begin{align*}\n&\\sum_{a\\in A} \\theta^{-a} h_a(\\theta^*_a) h^{-a}(\\theta^{-a}) d\\theta^{-a} \\\\\n&\\leq \\sum_{a\\in A} \\theta^{-a} \\sum_{a'\\neq a} h_a(\\theta_{a'}) \\sum_{a'\\neq a} h_{a'}(\\theta_{a'}) d\\theta^{-a} \\\\\n&= \\sum_{a\\in A} \\sum_{a'\\neq a} \\theta_{a'} h_a(\\theta_{a'}) h_{a'}(\\theta_{a'}) d\\theta_{a'} \\\\\n&= \\frac{1}{2\\pi\\sigma^2_0} \\sum_{a\\in A} \\sum_{a'\\neq a} \\exp\\left(-(\\theta_{a'} - \\mu_{0,a})^2 - (\\theta_{a'} - \\mu_{0,a'})^2\\right) d\\theta_{a'} \\\\\n&= 2\\cdot\\frac{1}{\\pi\\sigma^2_0} \\sum_{a\\in A} \\exp\\left(-(\\mu_{0,a} - 4\\sigma^2 \\mu_{0,a'})^2\\right),\n\\end{align*}\n$$\nwhere the last step is by completing the square and integrating out $$\\theta_{a'}$$."}, {"type": "heading", "lvl": 3, "value": "Proof of Theorem 4", "md": "### Proof of Theorem 4"}, {"type": "text", "value": "The regret bound of UCB1 is proved similarly to Theorem 1 and Corollary 2. This is because UCB1 can be viewed as BayesUCB where $$\\sigma_0 = \\infty$$ and each action $$a \\in A$$ is initially taken once at round $$t = a$$. Since $$\\sigma_0 = \\infty$$, the confidence interval becomes $$C_{t,a} = \\frac{2\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right)}{N_{t,a}}$$.\n\nThe proof differs in two steps. First, the regret in the first K rounds is bounded by $$\\sum_{a\\in A} E[\\Delta a]$$. Second, the concentration argument (Case 3 in Appendix A.1) changes because the bandit instance $$\\theta$$ is fixed and the estimated model parameter $$\\hat{\\theta}_t$$ is random. We detail it below.\n\nCase 3: Event $$E_t$$ does not occur. The last term in (5) can be bounded as\n\n$$\n\\begin{align*}\n& E[\\Delta A_t 1_{E_t} \\bar{\\theta}] \\\\\n&= E[E(\\theta_{A^*} - \\theta_{A_t}) 1_{E_t}] \\\\\n&\\leq E[E(\\theta_{A^*} - U_{t,A^*}) 1_{\\bar{\\theta}} + E(U_{t,A_t} - \\theta_{A_t}) 1_{\\bar{\\theta}} \\\\\n&\\leq E[E(\\theta_{A^*} - \\hat{\\theta}_{t,A^*}) 1_{E_t} + E(\\hat{\\theta}_{t,A_t} - \\theta_{A_t}) 1_{E_t} + E(C_{t,A_t} 1_{E_t}].\n\\end{align*}\n$$\nTo bound the resulting terms, we use that $$\\theta_a - \\hat{\\theta}_{t,a} | \\theta \\sim N(0, \\sigma^2/N_{t,a})$$.", "md": "The regret bound of UCB1 is proved similarly to Theorem 1 and Corollary 2. This is because UCB1 can be viewed as BayesUCB where $$\\sigma_0 = \\infty$$ and each action $$a \\in A$$ is initially taken once at round $$t = a$$. Since $$\\sigma_0 = \\infty$$, the confidence interval becomes $$C_{t,a} = \\frac{2\\sigma^2 \\log\\left(\\frac{1}{\\delta}\\right)}{N_{t,a}}$$.\n\nThe proof differs in two steps. First, the regret in the first K rounds is bounded by $$\\sum_{a\\in A} E[\\Delta a]$$. Second, the concentration argument (Case 3 in Appendix A.1) changes because the bandit instance $$\\theta$$ is fixed and the estimated model parameter $$\\hat{\\theta}_t$$ is random. We detail it below.\n\nCase 3: Event $$E_t$$ does not occur. The last term in (5) can be bounded as\n\n$$\n\\begin{align*}\n& E[\\Delta A_t 1_{E_t} \\bar{\\theta}] \\\\\n&= E[E(\\theta_{A^*} - \\theta_{A_t}) 1_{E_t}] \\\\\n&\\leq E[E(\\theta_{A^*} - U_{t,A^*}) 1_{\\bar{\\theta}} + E(U_{t,A_t} - \\theta_{A_t}) 1_{\\bar{\\theta}} \\\\\n&\\leq E[E(\\theta_{A^*} - \\hat{\\theta}_{t,A^*}) 1_{E_t} + E(\\hat{\\theta}_{t,A_t} - \\theta_{A_t}) 1_{E_t} + E(C_{t,A_t} 1_{E_t}].\n\\end{align*}\n$$\nTo bound the resulting terms, we use that $$\\theta_a - \\hat{\\theta}_{t,a} | \\theta \\sim N(0, \\sigma^2/N_{t,a})$$."}]}, {"page": 16, "text": "Lemma 8. For any action a \u2208                  A, round t > K, and Nt,a \u2265                 1,\n                                             E    (\u03b8a \u2212    \u02c6           \u00af\nProof. Let Et,a =             |\u03b8a \u2212    \u02c6                   \u03b8t,a)1     Et       \u03b8   \u2264   2\u03c3K\u03b4 .\n                                       \u03b8t,a| \u2264    Ct,a    . We start with decomposing \u00af              Et into individual \u00af      Et,a as\n     E   (\u03b8a \u2212     \u02c6          \u00af                                         \u00af                                                \u00af\n                   \u03b8t,a)1     Et           \u2264  E    |\u03b8a \u2212    \u02c6\n                                                            \u03b8t,a|1     Et,a           +         E    |\u03b8a \u2212    \u02c6\n                                      \u03b8                                           \u03b8      a\u2032\u0338=a                \u03b8t,a|1     Et,a\u2032       \u03b8   .\nTo bound the first term, we use that \u03b8a \u2212                   \u02c6\n                                                           \u03b8t,a | \u03b8 \u223c     N   (0, \u03c32/Nt,a). Thus\n               E    |\u03b8a \u2212    \u02c6          \u00af                        2               \u221e       x exp      \u2212       x2          dx\n                             \u03b8t,a|1    Et       \u03b8   \u2264       2\u03c0\u03c32/N      t,a \u221e  x=Ct,a                  2\u03c32/Nt,a\n                                                                2\u03c32                   \u2202                      x2\n                                                    = \u2212        \u03c0Nt,a       x=Ct,a    \u2202x      exp     \u2212  2\u03c32/Nt,a            dx\n                                                    =         2\u03c32    \u03b4 \u2264    \u03c3\u03b4 .\n                                                            \u03c0Nt,a\nTo bound the second term, we use the independence of the distributions for a and a\u2032,\n                          E    |\u03b8a \u2212    \u02c6          \u00af                                                  \u00af        \u03b8    .\n                                        \u03b8t,a|1    Et,a\u2032            = E     |\u03b8a \u2212    \u02c6\n                                                              \u03b8                     \u03b8t,a|    \u03b8   P   Et,a\u2032\nThe probability is at most 2\u03b4 and the expectation can be bounded as\n         E    |\u03b8a \u2212    \u02c6                                                                                               \u03c32\n                       \u03b8t,a|        = E         (\u03b8a \u2212     \u02c6\n                                                          \u03b8t,a)2         \u2264       E   (\u03b8a \u2212     \u02c6\nThis completes the proof.       \u03b8                                    \u03b8                         \u03b8t,a)2    \u03b8    =       Nt,a    \u2264   \u03c3 .\nThe first two terms in (11) can be bounded using a union bound over a \u2208                                    A and Lemma 8. For the\n                                                                                                      \u00af     \u03b8   to get\nlast term, we use that Ct,a \u2264                 2\u03c32 log(1/\u03b4) and a union bound in P                     Et\n                                     \u00af      \u03b8                                   \u00af     \u03b8   \u2264   2\n                   E    Ct,At1      Et           \u2264      2\u03c32 log(1/\u03b4)P           Et                  2 log(1/\u03b4)\u03c3K\u03b4 .\nFinally, we sum up the upper bounds on (11) over all rounds t \u2208                              [n].\nA.5      Proof of Theorem 5\nLet Et =         \u2200a \u2208    A : |\u03b8a \u2212      \u02c6\n                                        \u03b8t,a| \u2264    Ct,a      be the event that all confidence intervals at round t hold.\nFix \u03b5 > 0. We decompose the n-round regret as in (5) and then bound each resulting term next.\nCase 1: Event Et occurs and the gap is large, \u2206A                           t \u2265   \u03b5. As in Appendix A.1,\n                \u2206At = \u03b8A\u2217         \u2212   \u03b8At \u2264     \u03b8A\u2217   \u2212   Ut,A\u2217    + Ut,At \u2212       \u03b8At \u2264     Ut,At \u2212     \u03b8At \u2264     2Ct,At .\nIn the second inequality, we use that \u03b8A                \u2217  \u2264   Ut,A\u2217    on event Et. This implies that on event Et, action\na can be taken only if\n                                            Nt,a \u2264      2 log(1/\u03b4)      \u2212   (\u03b1a + \u03b2a + 1) .\n                                                             \u22062 a\nNow we apply this inequality to bound the first term in (5) as\n      n                                               \uf8ee          2 log(1/\u03b4)                                                           \uf8f9\n     t=1  E [\u2206At1{\u2206At \u2265             \u03b5, Et}] \u2264      E  \uf8f0 a\u0338=A\u2217            \u2206a         \u2212   (\u03b1a + \u03b2a + 1)\u2206a              1{\u2206a \u2265       \u03b5}  \uf8fb  .\nCase 2: The gap is small, \u2206A                t < \u03b5. Then naively  n    16     t=1 E [\u2206At1{\u2206At < \u03b5}] < \u03b5n.", "md": "# Math Equations in HTML\n\n## Lemma 8\n\nFor any action \\( a \\in A \\), round \\( t > K \\), and \\( N_{t,a} \\geq 1 \\),\n\n$$\nE(\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a})1_{E_{t}} \\leq 2\\sigma K\\delta .\n$$\nProof. Let \\( E_{t,a} = |\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t}} \\leq C_{t,a} \\). We start with decomposing \\( \\hat{\\bar{E}} \\) into individual \\( \\hat{\\bar{E}}_{t,a} \\) as\n\n$$\nE(\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a})1_{E_{t}} \\leq E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} + E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}\\theta} .\n$$\nTo bound the first term, we use that \\( \\theta_{a} - \\hat{\\theta}_{t,a} | \\theta \\sim N(0, \\sigma^2/N_{t,a}) \\). Thus\n\n$$\nE|\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a}|1_{E_{t}} \\leq \\frac{2\\sigma^2}{N_{t,a}} \\leq \\sigma\\delta .\n$$\nTo bound the second term, we use the independence of the distributions for \\( a \\) and \\( a' \\),\n\n$$\nE|\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a}|1_{E_{t,a'}} = E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}} \\leq \\sigma^2 .\n$$\nThis completes the proof. The first two terms in (11) can be bounded using a union bound over \\( a \\in A \\) and Lemma 8. For the last term, we use that \\( C_{t,a} \\leq 2\\sigma^2 \\log(1/\\delta) \\) and a union bound in \\( P_{E_{t}} \\) to get\n\n$$\nE[C_{t,A}1_{E_{t}}] \\leq 2\\sigma^2 \\log(1/\\delta)P_{E_{t}}2\\log(1/\\delta)\\sigma K\\delta .\n$$\nFinally, we sum up the upper bounds on (11) over all rounds \\( t \\in [n] \\).\n\n## Proof of Theorem 5\n\nLet \\( E_{t} = \\forall a \\in A : |\\theta_{a} - \\hat{\\theta}_{t,a}| \\leq C_{t,a} \\) be the event that all confidence intervals at round \\( t \\) hold.\n\nFix \\( \\epsilon > 0 \\). We decompose the n-round regret as in (5) and then bound each resulting term next.\n\nCase 1: Event \\( E_{t} \\) occurs and the gap is large, \\( \\Delta A_{t} \\geq \\epsilon \\). As in Appendix A.1,\n\n$$\n\\Delta A_{t} = \\theta_{A^*} - \\theta_{A_{t}} \\leq \\theta_{A^*} - U_{t,A^*} + U_{t,A_{t}} - \\theta_{A_{t}} \\leq U_{t,A_{t}} - \\theta_{A_{t}} \\leq 2C_{t,A_{t}} .\n$$\nIn the second inequality, we use that \\( \\theta_{A^*} \\leq U_{t,A^*} \\) on event \\( E_{t} \\). This implies that on event \\( E_{t} \\), action \\( a \\) can be taken only if\n\n$$\nN_{t,a} \\leq 2\\log(1/\\delta) - (\\alpha_{a} + \\beta_{a} + 1) .\n$$\nNow we apply this inequality to bound the first term in (5) as\n\n$$\n\\sum_{t=1}^{n} E[\\Delta A_{t}1\\{\\Delta A_{t} \\geq \\epsilon, E_{t}\\}] \\leq E\\left[\\sum_{a \\neq A^*} \\Delta_{a} - (\\alpha_{a} + \\beta_{a} + 1)\\Delta_{a}1\\{\\Delta_{a} \\geq \\epsilon}\\right] .\n$$\nCase 2: The gap is small, \\( \\Delta A_{t} < \\epsilon \\). Then naively \\( \\sum_{t=1}^{n} E[\\Delta A_{t}1\\{\\Delta A_{t} < \\epsilon\\}] < \\epsilon n .", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations in HTML", "md": "# Math Equations in HTML"}, {"type": "heading", "lvl": 2, "value": "Lemma 8", "md": "## Lemma 8"}, {"type": "text", "value": "For any action \\( a \\in A \\), round \\( t > K \\), and \\( N_{t,a} \\geq 1 \\),\n\n$$\nE(\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a})1_{E_{t}} \\leq 2\\sigma K\\delta .\n$$\nProof. Let \\( E_{t,a} = |\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t}} \\leq C_{t,a} \\). We start with decomposing \\( \\hat{\\bar{E}} \\) into individual \\( \\hat{\\bar{E}}_{t,a} \\) as\n\n$$\nE(\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a})1_{E_{t}} \\leq E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} + E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}\\theta} .\n$$\nTo bound the first term, we use that \\( \\theta_{a} - \\hat{\\theta}_{t,a} | \\theta \\sim N(0, \\sigma^2/N_{t,a}) \\). Thus\n\n$$\nE|\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a}|1_{E_{t}} \\leq \\frac{2\\sigma^2}{N_{t,a}} \\leq \\sigma\\delta .\n$$\nTo bound the second term, we use the independence of the distributions for \\( a \\) and \\( a' \\),\n\n$$\nE|\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a}|1_{E_{t,a'}} = E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}} \\leq \\sigma^2 .\n$$\nThis completes the proof. The first two terms in (11) can be bounded using a union bound over \\( a \\in A \\) and Lemma 8. For the last term, we use that \\( C_{t,a} \\leq 2\\sigma^2 \\log(1/\\delta) \\) and a union bound in \\( P_{E_{t}} \\) to get\n\n$$\nE[C_{t,A}1_{E_{t}}] \\leq 2\\sigma^2 \\log(1/\\delta)P_{E_{t}}2\\log(1/\\delta)\\sigma K\\delta .\n$$\nFinally, we sum up the upper bounds on (11) over all rounds \\( t \\in [n] \\).", "md": "For any action \\( a \\in A \\), round \\( t > K \\), and \\( N_{t,a} \\geq 1 \\),\n\n$$\nE(\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a})1_{E_{t}} \\leq 2\\sigma K\\delta .\n$$\nProof. Let \\( E_{t,a} = |\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t}} \\leq C_{t,a} \\). We start with decomposing \\( \\hat{\\bar{E}} \\) into individual \\( \\hat{\\bar{E}}_{t,a} \\) as\n\n$$\nE(\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a})1_{E_{t}} \\leq E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a}} + E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}\\theta} .\n$$\nTo bound the first term, we use that \\( \\theta_{a} - \\hat{\\theta}_{t,a} | \\theta \\sim N(0, \\sigma^2/N_{t,a}) \\). Thus\n\n$$\nE|\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a}|1_{E_{t}} \\leq \\frac{2\\sigma^2}{N_{t,a}} \\leq \\sigma\\delta .\n$$\nTo bound the second term, we use the independence of the distributions for \\( a \\) and \\( a' \\),\n\n$$\nE|\\theta_{a} - \\hat{\\bar{\\theta}}_{t,a}|1_{E_{t,a'}} = E|\\theta_{a} - \\hat{\\theta}_{t,a}|1_{E_{t,a'}} \\leq \\sigma^2 .\n$$\nThis completes the proof. The first two terms in (11) can be bounded using a union bound over \\( a \\in A \\) and Lemma 8. For the last term, we use that \\( C_{t,a} \\leq 2\\sigma^2 \\log(1/\\delta) \\) and a union bound in \\( P_{E_{t}} \\) to get\n\n$$\nE[C_{t,A}1_{E_{t}}] \\leq 2\\sigma^2 \\log(1/\\delta)P_{E_{t}}2\\log(1/\\delta)\\sigma K\\delta .\n$$\nFinally, we sum up the upper bounds on (11) over all rounds \\( t \\in [n] \\)."}, {"type": "heading", "lvl": 2, "value": "Proof of Theorem 5", "md": "## Proof of Theorem 5"}, {"type": "text", "value": "Let \\( E_{t} = \\forall a \\in A : |\\theta_{a} - \\hat{\\theta}_{t,a}| \\leq C_{t,a} \\) be the event that all confidence intervals at round \\( t \\) hold.\n\nFix \\( \\epsilon > 0 \\). We decompose the n-round regret as in (5) and then bound each resulting term next.\n\nCase 1: Event \\( E_{t} \\) occurs and the gap is large, \\( \\Delta A_{t} \\geq \\epsilon \\). As in Appendix A.1,\n\n$$\n\\Delta A_{t} = \\theta_{A^*} - \\theta_{A_{t}} \\leq \\theta_{A^*} - U_{t,A^*} + U_{t,A_{t}} - \\theta_{A_{t}} \\leq U_{t,A_{t}} - \\theta_{A_{t}} \\leq 2C_{t,A_{t}} .\n$$\nIn the second inequality, we use that \\( \\theta_{A^*} \\leq U_{t,A^*} \\) on event \\( E_{t} \\). This implies that on event \\( E_{t} \\), action \\( a \\) can be taken only if\n\n$$\nN_{t,a} \\leq 2\\log(1/\\delta) - (\\alpha_{a} + \\beta_{a} + 1) .\n$$\nNow we apply this inequality to bound the first term in (5) as\n\n$$\n\\sum_{t=1}^{n} E[\\Delta A_{t}1\\{\\Delta A_{t} \\geq \\epsilon, E_{t}\\}] \\leq E\\left[\\sum_{a \\neq A^*} \\Delta_{a} - (\\alpha_{a} + \\beta_{a} + 1)\\Delta_{a}1\\{\\Delta_{a} \\geq \\epsilon}\\right] .\n$$\nCase 2: The gap is small, \\( \\Delta A_{t} < \\epsilon \\). Then naively \\( \\sum_{t=1}^{n} E[\\Delta A_{t}1\\{\\Delta A_{t} < \\epsilon\\}] < \\epsilon n .", "md": "Let \\( E_{t} = \\forall a \\in A : |\\theta_{a} - \\hat{\\theta}_{t,a}| \\leq C_{t,a} \\) be the event that all confidence intervals at round \\( t \\) hold.\n\nFix \\( \\epsilon > 0 \\). We decompose the n-round regret as in (5) and then bound each resulting term next.\n\nCase 1: Event \\( E_{t} \\) occurs and the gap is large, \\( \\Delta A_{t} \\geq \\epsilon \\). As in Appendix A.1,\n\n$$\n\\Delta A_{t} = \\theta_{A^*} - \\theta_{A_{t}} \\leq \\theta_{A^*} - U_{t,A^*} + U_{t,A_{t}} - \\theta_{A_{t}} \\leq U_{t,A_{t}} - \\theta_{A_{t}} \\leq 2C_{t,A_{t}} .\n$$\nIn the second inequality, we use that \\( \\theta_{A^*} \\leq U_{t,A^*} \\) on event \\( E_{t} \\). This implies that on event \\( E_{t} \\), action \\( a \\) can be taken only if\n\n$$\nN_{t,a} \\leq 2\\log(1/\\delta) - (\\alpha_{a} + \\beta_{a} + 1) .\n$$\nNow we apply this inequality to bound the first term in (5) as\n\n$$\n\\sum_{t=1}^{n} E[\\Delta A_{t}1\\{\\Delta A_{t} \\geq \\epsilon, E_{t}\\}] \\leq E\\left[\\sum_{a \\neq A^*} \\Delta_{a} - (\\alpha_{a} + \\beta_{a} + 1)\\Delta_{a}1\\{\\Delta_{a} \\geq \\epsilon}\\right] .\n$$\nCase 2: The gap is small, \\( \\Delta A_{t} < \\epsilon \\). Then naively \\( \\sum_{t=1}^{n} E[\\Delta A_{t}1\\{\\Delta A_{t} < \\epsilon\\}] < \\epsilon n ."}]}, {"page": 17, "text": "Case 3: Event Et does not occur. Since \u03b8a \u2208                         [0, 1], the last term in (5) can be bounded as\n                                                        \u00af                    \u00af     Ht\n                                       E    \u2206At1       Et      \u2264   E    P   Et             \u2264   2K\u03b4 .\nThis completes the first part of the proof.\nThe second claim is proved as in Appendix A.2 and we only comment on what differs. For \u03b5 = 1/n\nand \u03b52 = 1/\u221alog n, (9) becomes\n                 \u03b8\u2217                                  \u03b8\u2217\n                  a\u2212\u03b5                                  a\u2212\u03b5        2 log(1/\u03b4)\n               \u03b8a=\u03b8\u2217  a\u2212\u03b52   \u2206aNa d\u03b8a \u2264             \u03b8a=\u03b8\u2217 a\u2212\u03b52      \u03b8\u2217a \u2212   \u03b8a    \u2212   \u03bb(\u03b8\u2217 a \u2212   \u03b8a) d\u03b8a\n                                              = 2 log(1/\u03b4)(log \u03b52 \u2212            log \u03b5) \u2212     \u03bb(\u03b52  22\u2212   \u03b52)\n                                                                                  \u03bb            \u03bb\n                                              = 2 log(1/\u03b4) log n \u2212            2 log n +      2n2 \u2212     log(1/\u03b4) log log n\n                                                                                  \u03bb\n                                              \u2264   2 log(1/\u03b4) log n \u2212          2 log n .\nThe last inequality holds for \u03bb \u2264               2 log(1/\u03b4) n2 log log n. Moreover, (10) becomes\n               E [\u2206aNa1{\u2206a > \u03b52}] \u2264                  E   2 log(1/\u03b4)       1{\u2206a > \u03b52}            < 2 log(1/\u03b4)          log n .\n                                                                \u2206a\nThis completes the second part of the proof.\nA.6      Proof of Theorem 6\nLet                             Et =       \u2200a \u2208     A : |a\u22a4(\u03b8 \u2212        \u02c6\n                                                                      \u03b8t)| \u2264        2 log(1/\u03b4)\u2225a\u2225\u02c6       \u03a3t                              (12)\nbe an event that high-probability confidence intervals for mean rewards at round t hold. Our proof\nhas three parts.\nCase 1: Event Et occurs and the gap is large, \u2206A                           t \u2265   \u03b5. Then\n                        1                  1                                    1\n          \u2206A   t =    \u2206A   t \u22062 At \u2264    \u2206\u03b5 min   (A\u22a4 \u2217 \u03b8 \u2212    A\u22a4t \u03b8)2 \u2264      \u2206\u03b5 min  (A\u22a4 \u2217  \u03b8 \u2212   Ut,A\u2217    + Ut,At \u2212       A\u22a4 t \u03b8)2\n                 \u2264       1    (Ut,At \u2212      A\u22a4 t \u03b8)2 \u2264        4    C2 t,At = 8 log(1/\u03b4)         \u2225At\u22252\u02c6  \u03a3t .\n                      \u2206\u03b5 min                               \u2206\u03b5 min                   \u2206\u03b5 min\nThe first inequality follows from definitions of \u2206A                       t and \u2206\u03b5    min; and that the gap is large, \u2206A               t \u2265   \u03b5.\nThe second inequality holds because A\u22a4                   \u2217 \u03b8 \u2212    A\u22a4t \u03b8 \u2265    0 by definition and Ut,At \u2212              Ut,A\u2217     \u2265  0 by the\ndesign of BayesUCB. The third inequality holds because A\u22a4                          \u2217  \u03b8 \u2212   Ut,A\u2217    \u2264   0 on event Et. Specifically,\nfor any action a \u2208         A on event Et,\n                                a\u22a4\u03b8 \u2212      Ut,a = a\u22a4(\u03b8 \u2212          \u02c6\n                                                                  \u03b8t) \u2212   Ct,a \u2264     Ct,a \u2212     Ct,a = 0 .\nThe last inequality follows from the definition of event Et. Specifically, for any action a \u2208                                           A on\nevent Et,\n                             Ut,a \u2212     a\u22a4\u03b8 = a\u22a4(\u02c6      \u03b8t \u2212    \u03b8) + Ct,a \u2264       Ct,a + Ct,a = 2Ct,a .\nCase 2: The gap is small, \u2206A                t \u2264   \u03b5. Then naively \u2206At \u2264              \u03b5.\nCase 3: Event Et does not occur. Then \u2206A                               \u00af                                \u00af                      \u00af\n                                                                t1    Et     \u2264   2\u2225At\u22252\u2225\u03b8\u222521           Et     \u2264   2LL\u22171        Et    , where\n2LL\u2217     is a trivial upper bound on \u2206At. We bound the event in expectation as follows.          \u00af    Ht\nLemma 9. For any round t \u2208                  [n] and history Ht, we have that P                  Et           \u2264   2K\u03b4.\nProof. First, note that for any history Ht,\n                             \u00af     Ht\n                        P   Et            \u2264         P    |a\u22a4(\u03b8 \u2212      \u02c6\n                                             a\u2208A                      \u03b8t)| \u2265        2 log(1/\u03b4)\u2225a\u2225\u02c6      \u03a3t    Ht      .\n                                                                      17", "md": "Case 3: Event Et does not occur. Since \u03b8a \u2208 [0, 1], the last term in (5) can be bounded as\n\n$$\nE[\\Delta A_t1 \\leq E[P(E_t)] \\leq 2K\\delta.\n$$\nThis completes the first part of the proof.\n\nThe second claim is proved as in Appendix A.2 and we only comment on what differs. For \u03b5 = 1/n and \u03b52 = 1/\u221alog n, (9) becomes\n\n$$\n\\begin{align*}\n&\\theta^*_a - \\epsilon \\leq \\theta_a \\leq 2\\log(1/\\delta) \\\\\n&\\theta_a=\\theta^*_a - \\epsilon_2 \\leq \\int_{\\theta_a=\\theta^*_a - \\epsilon_2}^{\\theta^*_a} (\\theta^*_a - \\theta_a - \\lambda(\\theta^*_a - \\theta_a)) d\\theta_a \\\\\n&= 2\\log(1/\\delta)(\\log \\epsilon_2 - \\log \\epsilon) - \\lambda(\\epsilon_2^2 - \\epsilon_2) \\\\\n&= 2\\log(1/\\delta)\\log n - 2\\log n + 2n^2 - \\log(1/\\delta)\\log\\log n \\\\\n&\\leq 2\\log(1/\\delta)\\log n - 2\\log n.\n\\end{align*}\n$$\nThe last inequality holds for \u03bb \u2264 2\\log(1/\\delta)n^2\\log\\log n. Moreover, (10) becomes\n\n$$\nE[\\Delta A_{t}1\\{\\Delta A > \\epsilon_2\\}] \\leq E[2\\log(1/\\delta)1\\{\\Delta A > \\epsilon_2\\}] < 2\\log(1/\\delta)\\log n.\n$$\nThis completes the second part of the proof.\n\nA.6 Proof of Theorem 6\n\nLet $$E_t = \\forall a \\in A : |a^T(\\theta - \\hat{\\theta}_t)| \\leq 2\\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t$$ (12) be an event that high-probability confidence intervals for mean rewards at round t hold. Our proof has three parts.\n\nCase 1: Event Et occurs and the gap is large, \u2206A_t \u2265 \u03b5. Then\n\n$$\n\\begin{align*}\n\\Delta A_t &= \\Delta A_t \\Delta^2 A_t \\leq \\Delta_{\\epsilon} \\min(A^T * \\theta - A^T_t \\theta)^2 \\leq \\Delta_{\\epsilon} \\min(A^T * \\theta - U_{t,A^*} + U_{t,A_t} - A^T_t \\theta)^2 \\\\\n&\\leq \\frac{1}{\\Delta_{\\epsilon}}(U_{t,A_t} - A^T_t \\theta)^2 \\leq 4C^2_{t,A_t} = 8\\log(1/\\delta)\\|A_t\\|^2\\hat{\\Sigma}_t.\n\\end{align*}\n$$\nThe first inequality follows from definitions of \u2206A_t and \u2206\u03b5_min; and that the gap is large, \u2206A_t \u2265 \u03b5. The second inequality holds because A^T * \u03b8 - A^T_t \u03b8 \u2265 0 by definition and U_{t,A_t} - U_{t,A^*} \u2265 0 by the design of BayesUCB. The third inequality holds because A^T * \u03b8 - U_{t,A^*} \u2264 0 on event Et. Specifically, for any action a \u2208 A on event Et,\n\n$$\na^T\\theta - U_{t,a} = a^T(\\theta - \\hat{\\theta}_t) - C_{t,a} \\leq C_{t,a} - C_{t,a} = 0.\n$$\nThe last inequality follows from the definition of event Et. Specifically, for any action a \u2208 A on event Et,\n\n$$\nU_{t,a} - a^T\\theta = a^T(\\hat{\\theta}_t - \\theta) + C_{t,a} \\leq C_{t,a} + C_{t,a} = 2C_{t,a}.\n$$\nCase 2: The gap is small, \u2206A_t \u2264 \u03b5. Then naively \u2206A_t \u2264 \u03b5.\n\nCase 3: Event Et does not occur. Then\n\n$$\n\\Delta A_t1 \\leq 2\\|A_t\\|^2\\|\\theta\\|^2 1_Et \\leq 2LL^*1_Et,\n$$\nwhere 2LL* is a trivial upper bound on \u2206At. We bound the event in expectation as follows.\n\nLemma 9. For any round t \u2208 [n] and history Ht, we have that P(Et) \u2264 2K\u03b4.\n\nProof. First, note that for any history Ht,\n\n$$\nP(Et) \\leq P\\left(\\bigcup_{a\\in A} |a^T(\\theta - \\hat{\\theta}_t)| \\geq 2\\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t | Ht\\right) \\leq 2K\\delta.\n$$", "images": [], "items": [{"type": "text", "value": "Case 3: Event Et does not occur. Since \u03b8a \u2208 [0, 1], the last term in (5) can be bounded as\n\n$$\nE[\\Delta A_t1 \\leq E[P(E_t)] \\leq 2K\\delta.\n$$\nThis completes the first part of the proof.\n\nThe second claim is proved as in Appendix A.2 and we only comment on what differs. For \u03b5 = 1/n and \u03b52 = 1/\u221alog n, (9) becomes\n\n$$\n\\begin{align*}\n&\\theta^*_a - \\epsilon \\leq \\theta_a \\leq 2\\log(1/\\delta) \\\\\n&\\theta_a=\\theta^*_a - \\epsilon_2 \\leq \\int_{\\theta_a=\\theta^*_a - \\epsilon_2}^{\\theta^*_a} (\\theta^*_a - \\theta_a - \\lambda(\\theta^*_a - \\theta_a)) d\\theta_a \\\\\n&= 2\\log(1/\\delta)(\\log \\epsilon_2 - \\log \\epsilon) - \\lambda(\\epsilon_2^2 - \\epsilon_2) \\\\\n&= 2\\log(1/\\delta)\\log n - 2\\log n + 2n^2 - \\log(1/\\delta)\\log\\log n \\\\\n&\\leq 2\\log(1/\\delta)\\log n - 2\\log n.\n\\end{align*}\n$$\nThe last inequality holds for \u03bb \u2264 2\\log(1/\\delta)n^2\\log\\log n. Moreover, (10) becomes\n\n$$\nE[\\Delta A_{t}1\\{\\Delta A > \\epsilon_2\\}] \\leq E[2\\log(1/\\delta)1\\{\\Delta A > \\epsilon_2\\}] < 2\\log(1/\\delta)\\log n.\n$$\nThis completes the second part of the proof.\n\nA.6 Proof of Theorem 6\n\nLet $$E_t = \\forall a \\in A : |a^T(\\theta - \\hat{\\theta}_t)| \\leq 2\\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t$$ (12) be an event that high-probability confidence intervals for mean rewards at round t hold. Our proof has three parts.\n\nCase 1: Event Et occurs and the gap is large, \u2206A_t \u2265 \u03b5. Then\n\n$$\n\\begin{align*}\n\\Delta A_t &= \\Delta A_t \\Delta^2 A_t \\leq \\Delta_{\\epsilon} \\min(A^T * \\theta - A^T_t \\theta)^2 \\leq \\Delta_{\\epsilon} \\min(A^T * \\theta - U_{t,A^*} + U_{t,A_t} - A^T_t \\theta)^2 \\\\\n&\\leq \\frac{1}{\\Delta_{\\epsilon}}(U_{t,A_t} - A^T_t \\theta)^2 \\leq 4C^2_{t,A_t} = 8\\log(1/\\delta)\\|A_t\\|^2\\hat{\\Sigma}_t.\n\\end{align*}\n$$\nThe first inequality follows from definitions of \u2206A_t and \u2206\u03b5_min; and that the gap is large, \u2206A_t \u2265 \u03b5. The second inequality holds because A^T * \u03b8 - A^T_t \u03b8 \u2265 0 by definition and U_{t,A_t} - U_{t,A^*} \u2265 0 by the design of BayesUCB. The third inequality holds because A^T * \u03b8 - U_{t,A^*} \u2264 0 on event Et. Specifically, for any action a \u2208 A on event Et,\n\n$$\na^T\\theta - U_{t,a} = a^T(\\theta - \\hat{\\theta}_t) - C_{t,a} \\leq C_{t,a} - C_{t,a} = 0.\n$$\nThe last inequality follows from the definition of event Et. Specifically, for any action a \u2208 A on event Et,\n\n$$\nU_{t,a} - a^T\\theta = a^T(\\hat{\\theta}_t - \\theta) + C_{t,a} \\leq C_{t,a} + C_{t,a} = 2C_{t,a}.\n$$\nCase 2: The gap is small, \u2206A_t \u2264 \u03b5. Then naively \u2206A_t \u2264 \u03b5.\n\nCase 3: Event Et does not occur. Then\n\n$$\n\\Delta A_t1 \\leq 2\\|A_t\\|^2\\|\\theta\\|^2 1_Et \\leq 2LL^*1_Et,\n$$\nwhere 2LL* is a trivial upper bound on \u2206At. We bound the event in expectation as follows.\n\nLemma 9. For any round t \u2208 [n] and history Ht, we have that P(Et) \u2264 2K\u03b4.\n\nProof. First, note that for any history Ht,\n\n$$\nP(Et) \\leq P\\left(\\bigcup_{a\\in A} |a^T(\\theta - \\hat{\\theta}_t)| \\geq 2\\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t | Ht\\right) \\leq 2K\\delta.\n$$", "md": "Case 3: Event Et does not occur. Since \u03b8a \u2208 [0, 1], the last term in (5) can be bounded as\n\n$$\nE[\\Delta A_t1 \\leq E[P(E_t)] \\leq 2K\\delta.\n$$\nThis completes the first part of the proof.\n\nThe second claim is proved as in Appendix A.2 and we only comment on what differs. For \u03b5 = 1/n and \u03b52 = 1/\u221alog n, (9) becomes\n\n$$\n\\begin{align*}\n&\\theta^*_a - \\epsilon \\leq \\theta_a \\leq 2\\log(1/\\delta) \\\\\n&\\theta_a=\\theta^*_a - \\epsilon_2 \\leq \\int_{\\theta_a=\\theta^*_a - \\epsilon_2}^{\\theta^*_a} (\\theta^*_a - \\theta_a - \\lambda(\\theta^*_a - \\theta_a)) d\\theta_a \\\\\n&= 2\\log(1/\\delta)(\\log \\epsilon_2 - \\log \\epsilon) - \\lambda(\\epsilon_2^2 - \\epsilon_2) \\\\\n&= 2\\log(1/\\delta)\\log n - 2\\log n + 2n^2 - \\log(1/\\delta)\\log\\log n \\\\\n&\\leq 2\\log(1/\\delta)\\log n - 2\\log n.\n\\end{align*}\n$$\nThe last inequality holds for \u03bb \u2264 2\\log(1/\\delta)n^2\\log\\log n. Moreover, (10) becomes\n\n$$\nE[\\Delta A_{t}1\\{\\Delta A > \\epsilon_2\\}] \\leq E[2\\log(1/\\delta)1\\{\\Delta A > \\epsilon_2\\}] < 2\\log(1/\\delta)\\log n.\n$$\nThis completes the second part of the proof.\n\nA.6 Proof of Theorem 6\n\nLet $$E_t = \\forall a \\in A : |a^T(\\theta - \\hat{\\theta}_t)| \\leq 2\\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t$$ (12) be an event that high-probability confidence intervals for mean rewards at round t hold. Our proof has three parts.\n\nCase 1: Event Et occurs and the gap is large, \u2206A_t \u2265 \u03b5. Then\n\n$$\n\\begin{align*}\n\\Delta A_t &= \\Delta A_t \\Delta^2 A_t \\leq \\Delta_{\\epsilon} \\min(A^T * \\theta - A^T_t \\theta)^2 \\leq \\Delta_{\\epsilon} \\min(A^T * \\theta - U_{t,A^*} + U_{t,A_t} - A^T_t \\theta)^2 \\\\\n&\\leq \\frac{1}{\\Delta_{\\epsilon}}(U_{t,A_t} - A^T_t \\theta)^2 \\leq 4C^2_{t,A_t} = 8\\log(1/\\delta)\\|A_t\\|^2\\hat{\\Sigma}_t.\n\\end{align*}\n$$\nThe first inequality follows from definitions of \u2206A_t and \u2206\u03b5_min; and that the gap is large, \u2206A_t \u2265 \u03b5. The second inequality holds because A^T * \u03b8 - A^T_t \u03b8 \u2265 0 by definition and U_{t,A_t} - U_{t,A^*} \u2265 0 by the design of BayesUCB. The third inequality holds because A^T * \u03b8 - U_{t,A^*} \u2264 0 on event Et. Specifically, for any action a \u2208 A on event Et,\n\n$$\na^T\\theta - U_{t,a} = a^T(\\theta - \\hat{\\theta}_t) - C_{t,a} \\leq C_{t,a} - C_{t,a} = 0.\n$$\nThe last inequality follows from the definition of event Et. Specifically, for any action a \u2208 A on event Et,\n\n$$\nU_{t,a} - a^T\\theta = a^T(\\hat{\\theta}_t - \\theta) + C_{t,a} \\leq C_{t,a} + C_{t,a} = 2C_{t,a}.\n$$\nCase 2: The gap is small, \u2206A_t \u2264 \u03b5. Then naively \u2206A_t \u2264 \u03b5.\n\nCase 3: Event Et does not occur. Then\n\n$$\n\\Delta A_t1 \\leq 2\\|A_t\\|^2\\|\\theta\\|^2 1_Et \\leq 2LL^*1_Et,\n$$\nwhere 2LL* is a trivial upper bound on \u2206At. We bound the event in expectation as follows.\n\nLemma 9. For any round t \u2208 [n] and history Ht, we have that P(Et) \u2264 2K\u03b4.\n\nProof. First, note that for any history Ht,\n\n$$\nP(Et) \\leq P\\left(\\bigcup_{a\\in A} |a^T(\\theta - \\hat{\\theta}_t)| \\geq 2\\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t | Ht\\right) \\leq 2K\\delta.\n$$"}]}, {"page": 18, "text": "By definition, \u03b8 \u2212         \u02c6\n                           \u03b8t | Ht \u223c       N  (0d, \u02c6 \u03a3t), and therefore a\u22a4(\u03b8 \u2212               \u02c6\naction a \u2208      A. It immediately follows that                                               \u03b8t)/\u2225a\u2225\u02c6   \u03a3t | Ht \u223c       N   (0, 1) for any\n                                  P    |a\u22a4(\u03b8 \u2212      \u02c6\nThis completes the proof.                           \u03b8t)| \u2265        2 log(1/\u03b4)\u2225a\u2225\u02c6      \u03a3t    Ht      \u2264   2\u03b4 .\nFinally, we chain all inequalities, add them over all rounds, and get\n                          R(n) \u2264      8E      \u2206\u03b5 1      n    \u2225At\u22252\u02c6 \u03a3t    log(1/\u03b4) + \u03b5n + 4LL\u2217Kn\u03b4 .\n                                                 min   t=1\nThe sum can bounded using a worst-case argument below, which yields our claim.\nLemma 10. The sum of posterior variances is bounded as\n                                 n   \u2225At\u22252   \u02c6              \u03c320,maxd           log     1 + \u03c32   0,maxn         .\n                               t=1           \u03a3t \u2264    log     1 +   \u03c320,max                       \u03c32d\n                                                                      \u03c32\nProof. We start with an upper bound on the posterior variance of the mean reward estimate of any\naction. In any round t \u2208           [n], by Weyl\u2019s inequalities, we have\n                 \u03bb1(\u02c6 \u03a3t) = \u03bb1((\u03a3\u22121      0    + Gt)\u22121) = \u03bb\u22121               0    + Gt) \u2264       \u03bb\u22121       0 ) = \u03bb1(\u03a30) .\n                                                                   d (\u03a3\u22121                       d (\u03a3\u22121\nThus, when \u2225a\u22252 \u2264            L for any action a \u2208           A, we have maxa\u2208A \u2225a\u2225\u02c6              \u03a3t \u2264        \u03bb1(\u03a30)L = \u03c30,max.\nNow we bound the sum of posterior variances  n                         t=1 \u2225At\u22252    \u02c6\n                                                                                    \u03a3t. Fix round t and note that\n                             \u03a3tAt                                                                                   1             1\n    \u2225At\u22252   \u02c6              t \u02c6        \u2264   c1 log(1 + \u03c3\u22122A\u22a4\u03a3tAt) = c1 log det(Id + \u03c3\u22122 \u02c6                          \u03a3 t2AtA\u22a4   t \u02c6\u03a3 t2)     (13)\nfor         \u03a3t = \u03c32 A\u22a4       \u03c32                                    t \u02c6\n                                                                      \u03c320,max\n                                                   c1 =     log(1 + \u03c3\u22122\u03c32       0,max) .\nThis upper bound is derived as follows. For any x \u2208                         [0, u],\n     x =           x                                  max             x           log(1 + x) =                u\n             log(1 + x) log(1 + x) \u2264                 x\u2208[0,u]   log(1 + x)                              log(1 + u) log(1 + x) .\nThen we set x = \u03c3\u22122A\u22a4\u03a3tAt and use the definition of \u03c30,max.\n                                  t \u02c6\nThe next step is bounding the logarithmic term in (13), which can be rewritten as\n                                          1             1\n              log det(Id + \u03c3\u22122 \u02c6       \u03a3  2          \u03a3  2                   \u03a3\u22121    + \u03c3\u22122AtA\u22a4                            \u03a3\u22121\n                                          t AtA\u22a4   t \u02c6  t ) = log det(\u02c6t                            t ) \u2212   log det(\u02c6     t ) .\nBecause of that, when we sum over all rounds, we get telescoping and the total contribution of all\nterms is at mostn                                1             1\n                    log det(Id + \u03c3\u22122 \u02c6        \u03a3 t2AtA\u22a4   t \u02c6\u03a3 t2) = log det(\u02c6      \u03a3\u22121                      \u03a3\u22121\n               t=1                                                                   n+1) \u2212      log det(\u02c61 )\n                                                                                      1            1\n                                                                   = log det(\u03a3       02\u02c6\u03a3\u22121       02)\n                                                                                          n+1\u03a3\n                                                                                1           1            1\n                                                                   \u2264   d log                2 \u03a3\u22121        2\n                                                                                  d tr(\u03a3    0 \u02c6n+1\u03a30 )\n                                                                                          1      n          1             1\n                                                                                                            2             2\n                                                                   = d log        1 +   \u03c32d    t=1   tr(\u03a3   0 AtA\u22a4   t \u03a3  0 )\n                                                                   = d log        1 +     1      n   A\u22a4t \u03a30At\n                                                                                        \u03c32d    t=1\n                                                                   \u2264   d log      1 + \u03c32  0,maxn         .\n                                                                                           \u03c32d\nThis completes the proof.\n                                                                      18", "md": "# Math Equations and Text\n\nBy definition, $$\\theta - \\hat{\\theta}_t | H_t \\sim N(0, \\hat{\\Sigma}_t)$$, and therefore $$a^T(\\theta - \\hat{\\theta}_t)/\\|a\\|\\hat{\\Sigma}_t | H_t \\sim N(0, 1)$$ for any action $$a \\in A$$. It immediately follows that $$P(|a^T(\\theta - \\hat{\\theta}_t)| \\geq 2 \\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t | H_t) \\leq 2\\delta$$. This completes the proof.\n\nFinally, we chain all inequalities, add them over all rounds, and get\n\n$$R(n) \\leq 8E \\left[ \\Delta \\epsilon \\frac{1}{n} \\|A_t\\|^2 \\hat{\\Sigma}_t \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta \\right]$$\n\nThe sum can be bounded using a worst-case argument below, which yields our claim.\n\nLemma 10. The sum of posterior variances is bounded as\n\n$$\\sum_{t=1}^{n} \\|A_t\\|^2 \\hat{\\Sigma}_t \\leq \\sigma_{0,max}^d \\log(1 + \\sigma_{0,max}^n).$$\n\nProof. We start with an upper bound on the posterior variance of the mean reward estimate of any action. In any round $$t \\in [n]$$, by Weyl\u2019s inequalities, we have\n\n$$\\lambda_1(\\hat{\\Sigma}_t) = \\lambda_1((\\Sigma_0^{-1} + G_t)^{-1}) = \\lambda_{-1}(\\Sigma_0 + G_t) \\leq \\lambda_{-1}(\\Sigma_0) = \\lambda_1(\\Sigma_0).$$\n\nThus, when $$\\|a\\|^2 \\leq L$$ for any action $$a \\in A$$, we have $$\\max_{a \\in A} \\|a\\|\\hat{\\Sigma}_t \\leq \\lambda_1(\\Sigma_0)L = \\sigma_{0,max}$$. Now we bound the sum of posterior variances $$\\sum_{t=1}^{n} \\|A_t\\|^2 \\hat{\\Sigma}_t$$. Fix round $$t$$ and note that\n\n$$\\|A_t\\|^2 \\hat{\\Sigma}_t \\leq c_1 \\log(1 + \\sigma_{-2}A^T\\hat{\\Sigma}_tA_t) = c_1 \\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T)$$\n\nfor $$\\hat{\\Sigma}_t = \\sigma^2 A^T\\sigma^2$$,\n\n$$c_1 = \\log(1 + \\sigma_{-2}\\sigma^2_{0,max}).$$\n\nThis upper bound is derived as follows. For any $$x \\in [0, u]$$,\n\n$$x = \\max_{x \\in [0,u]} x \\log(1 + x) = u \\log(1 + u) \\leq \\max_{x \\in [0,u]} x \\log(1 + x).$$\n\nThen we set $$x = \\sigma_{-2}A^T\\hat{\\Sigma}_tA_t$$ and use the definition of $$\\sigma_{0,max}$$. The next step is bounding the logarithmic term in the above equation, which can be rewritten as\n\n$$\\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T) = \\log \\det(\\hat{\\Sigma}_t^2A_tA^T) - \\log \\det(\\hat{\\Sigma}_t^2).$$\n\nBecause of that, when we sum over all rounds, we get telescoping and the total contribution of all terms is at most\n\n$$\\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T) = \\log \\det(\\Sigma_{0^2}\\hat{\\Sigma}_{-1^2}) \\leq d \\log(1 + \\sigma^2_d \\sum_{t=1}^{n} \\text{tr}(\\Sigma_0 A_tA^T \\Sigma_0) = d \\log(1 + \\sigma^2_{0,max}n).$$\n\nThis completes the proof.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Math Equations and Text", "md": "# Math Equations and Text"}, {"type": "text", "value": "By definition, $$\\theta - \\hat{\\theta}_t | H_t \\sim N(0, \\hat{\\Sigma}_t)$$, and therefore $$a^T(\\theta - \\hat{\\theta}_t)/\\|a\\|\\hat{\\Sigma}_t | H_t \\sim N(0, 1)$$ for any action $$a \\in A$$. It immediately follows that $$P(|a^T(\\theta - \\hat{\\theta}_t)| \\geq 2 \\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t | H_t) \\leq 2\\delta$$. This completes the proof.\n\nFinally, we chain all inequalities, add them over all rounds, and get\n\n$$R(n) \\leq 8E \\left[ \\Delta \\epsilon \\frac{1}{n} \\|A_t\\|^2 \\hat{\\Sigma}_t \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta \\right]$$\n\nThe sum can be bounded using a worst-case argument below, which yields our claim.\n\nLemma 10. The sum of posterior variances is bounded as\n\n$$\\sum_{t=1}^{n} \\|A_t\\|^2 \\hat{\\Sigma}_t \\leq \\sigma_{0,max}^d \\log(1 + \\sigma_{0,max}^n).$$\n\nProof. We start with an upper bound on the posterior variance of the mean reward estimate of any action. In any round $$t \\in [n]$$, by Weyl\u2019s inequalities, we have\n\n$$\\lambda_1(\\hat{\\Sigma}_t) = \\lambda_1((\\Sigma_0^{-1} + G_t)^{-1}) = \\lambda_{-1}(\\Sigma_0 + G_t) \\leq \\lambda_{-1}(\\Sigma_0) = \\lambda_1(\\Sigma_0).$$\n\nThus, when $$\\|a\\|^2 \\leq L$$ for any action $$a \\in A$$, we have $$\\max_{a \\in A} \\|a\\|\\hat{\\Sigma}_t \\leq \\lambda_1(\\Sigma_0)L = \\sigma_{0,max}$$. Now we bound the sum of posterior variances $$\\sum_{t=1}^{n} \\|A_t\\|^2 \\hat{\\Sigma}_t$$. Fix round $$t$$ and note that\n\n$$\\|A_t\\|^2 \\hat{\\Sigma}_t \\leq c_1 \\log(1 + \\sigma_{-2}A^T\\hat{\\Sigma}_tA_t) = c_1 \\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T)$$\n\nfor $$\\hat{\\Sigma}_t = \\sigma^2 A^T\\sigma^2$$,\n\n$$c_1 = \\log(1 + \\sigma_{-2}\\sigma^2_{0,max}).$$\n\nThis upper bound is derived as follows. For any $$x \\in [0, u]$$,\n\n$$x = \\max_{x \\in [0,u]} x \\log(1 + x) = u \\log(1 + u) \\leq \\max_{x \\in [0,u]} x \\log(1 + x).$$\n\nThen we set $$x = \\sigma_{-2}A^T\\hat{\\Sigma}_tA_t$$ and use the definition of $$\\sigma_{0,max}$$. The next step is bounding the logarithmic term in the above equation, which can be rewritten as\n\n$$\\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T) = \\log \\det(\\hat{\\Sigma}_t^2A_tA^T) - \\log \\det(\\hat{\\Sigma}_t^2).$$\n\nBecause of that, when we sum over all rounds, we get telescoping and the total contribution of all terms is at most\n\n$$\\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T) = \\log \\det(\\Sigma_{0^2}\\hat{\\Sigma}_{-1^2}) \\leq d \\log(1 + \\sigma^2_d \\sum_{t=1}^{n} \\text{tr}(\\Sigma_0 A_tA^T \\Sigma_0) = d \\log(1 + \\sigma^2_{0,max}n).$$\n\nThis completes the proof.", "md": "By definition, $$\\theta - \\hat{\\theta}_t | H_t \\sim N(0, \\hat{\\Sigma}_t)$$, and therefore $$a^T(\\theta - \\hat{\\theta}_t)/\\|a\\|\\hat{\\Sigma}_t | H_t \\sim N(0, 1)$$ for any action $$a \\in A$$. It immediately follows that $$P(|a^T(\\theta - \\hat{\\theta}_t)| \\geq 2 \\log(1/\\delta)\\|a\\|\\hat{\\Sigma}_t | H_t) \\leq 2\\delta$$. This completes the proof.\n\nFinally, we chain all inequalities, add them over all rounds, and get\n\n$$R(n) \\leq 8E \\left[ \\Delta \\epsilon \\frac{1}{n} \\|A_t\\|^2 \\hat{\\Sigma}_t \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta \\right]$$\n\nThe sum can be bounded using a worst-case argument below, which yields our claim.\n\nLemma 10. The sum of posterior variances is bounded as\n\n$$\\sum_{t=1}^{n} \\|A_t\\|^2 \\hat{\\Sigma}_t \\leq \\sigma_{0,max}^d \\log(1 + \\sigma_{0,max}^n).$$\n\nProof. We start with an upper bound on the posterior variance of the mean reward estimate of any action. In any round $$t \\in [n]$$, by Weyl\u2019s inequalities, we have\n\n$$\\lambda_1(\\hat{\\Sigma}_t) = \\lambda_1((\\Sigma_0^{-1} + G_t)^{-1}) = \\lambda_{-1}(\\Sigma_0 + G_t) \\leq \\lambda_{-1}(\\Sigma_0) = \\lambda_1(\\Sigma_0).$$\n\nThus, when $$\\|a\\|^2 \\leq L$$ for any action $$a \\in A$$, we have $$\\max_{a \\in A} \\|a\\|\\hat{\\Sigma}_t \\leq \\lambda_1(\\Sigma_0)L = \\sigma_{0,max}$$. Now we bound the sum of posterior variances $$\\sum_{t=1}^{n} \\|A_t\\|^2 \\hat{\\Sigma}_t$$. Fix round $$t$$ and note that\n\n$$\\|A_t\\|^2 \\hat{\\Sigma}_t \\leq c_1 \\log(1 + \\sigma_{-2}A^T\\hat{\\Sigma}_tA_t) = c_1 \\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T)$$\n\nfor $$\\hat{\\Sigma}_t = \\sigma^2 A^T\\sigma^2$$,\n\n$$c_1 = \\log(1 + \\sigma_{-2}\\sigma^2_{0,max}).$$\n\nThis upper bound is derived as follows. For any $$x \\in [0, u]$$,\n\n$$x = \\max_{x \\in [0,u]} x \\log(1 + x) = u \\log(1 + u) \\leq \\max_{x \\in [0,u]} x \\log(1 + x).$$\n\nThen we set $$x = \\sigma_{-2}A^T\\hat{\\Sigma}_tA_t$$ and use the definition of $$\\sigma_{0,max}$$. The next step is bounding the logarithmic term in the above equation, which can be rewritten as\n\n$$\\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T) = \\log \\det(\\hat{\\Sigma}_t^2A_tA^T) - \\log \\det(\\hat{\\Sigma}_t^2).$$\n\nBecause of that, when we sum over all rounds, we get telescoping and the total contribution of all terms is at most\n\n$$\\log \\det(I_d + \\sigma_{-2}\\hat{\\Sigma}_t^2A_tA^T) = \\log \\det(\\Sigma_{0^2}\\hat{\\Sigma}_{-1^2}) \\leq d \\log(1 + \\sigma^2_d \\sum_{t=1}^{n} \\text{tr}(\\Sigma_0 A_tA^T \\Sigma_0) = d \\log(1 + \\sigma^2_{0,max}n).$$\n\nThis completes the proof."}]}, {"page": 19, "text": "B    Complete Statement of Corollary 2\n                                      1                                                      1   \u221a 1\nTheorem 11. Let \u03c32    0 \u2265   8 log(1/\u03b4) n2 log log n. Then there exist functions \u03bea : R \u2192     n,   log n   such\nthat the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n    R(n) \u2264     8\u03c32 log(1/\u03b4) log n \u2212    2\u03c32 \u03c32              \u03b8\u2212a ha(\u03b8\u2217 a \u2212 \u03bea(\u03b8\u2217a)) h\u2212a(\u03b8\u2212a) d\u03b8\u2212a + C ,\n                                          0 log n   a\u2208A\nwhere C = 8\u03c32K log(1/\u03b4)\u221alog n + (2             2 log(1/\u03b4) + 1)\u03c30Kn\u03b4 + 1 is a low-order term.\nMoreover, when \u03c32   0 <  8 log(1/\u03b4)1n2 log log n, the regret is bounded as\n                                 R(n) \u2264      2   2 log(1/\u03b4) + 1     K\u03b4 + 1 .\n                                             8 log(1/\u03b4) log log n\nProof. The first claim is proved in Appendix A.2. The second claim can be proved as follows. Take\nTheorem 1, set \u03b5 = 0, and consider the three cases in Appendix A.1.\nCase 1: Event Et occurs and the gap is large, \u2206A         t \u2265  \u03b5. On event Et, action a can be taken only if\n                             2 log(1/\u03b4)                                          1\n              \u2206a \u2264   2    \u03c3\u22122  + \u03c3\u22122Nt,a     \u2264 2    2\u03c320 log(1/\u03b4) \u2264   2    4n2 log log n < 1  n .\n                           0\nTherefore, the corresponding n-round regret is bounded by 1.\nCase 2: The gap is small, \u2206A      t < \u03b5. This case cannot happen because \u03b5 = 0.\nCase 3: Event Et does not occur. The n-round regret is bounded by\n                      (2    2 log(1/\u03b4) + 1)\u03c30Kn\u03b4 \u2264          2   2 log(1/\u03b4) + 1     K\u03b4 .\n                                                             8 log(1/\u03b4) log log n\nThis completes the proof.\nC     Gap-Free Regret Bound of BayesUCB in Linear Bandit\nLet Et be the event in (12). Our proof has three parts.\nCase 1: Event Et occurs and the gap is large, \u2206A          t \u2265  \u03b5. Then\n          \u2206A  t = A\u22a4\u2217 \u03b8 \u2212  A\u22a4t \u03b8 \u2264  A\u22a4\u2217 \u03b8 \u2212  Ut,A\u2217  + Ut,At \u2212   A\u22a4t \u03b8 \u2264  Ut,At \u2212   A\u22a4t \u03b8 \u2264  2Ct,At .\nThe first inequality holds because Ut,A     t \u2212 Ut,A\u2217  \u2265  0 by the design of BayesUCB. The second one\nuses that A\u22a4\u2217 \u03b8 \u2212  Ut,A\u2217  \u2264  0. Specifically, for any action a \u2208   A on event Et,\n                         a\u22a4\u03b8 \u2212   Ut,a = a\u22a4(\u03b8 \u2212     \u02c6\n                                                   \u03b8t) \u2212  Ct,a \u2264  Ct,a \u2212  Ct,a = 0 .\nThe last inequality follows from the definition of event Et. Specifically, for any action a \u2208            A on\nevent Et,\n                      Ut,a \u2212   a\u22a4\u03b8 = a\u22a4(\u02c6   \u03b8t \u2212 \u03b8) + Ct,a \u2264    Ct,a + Ct,a = 2Ct,a .\nCases 2 and 3 are bounded as in Appendix A.6. Now we chain all inequalities, add them over all\nrounds, and get    R(n) \u2264    2E     n  \u2225At\u2225\u02c6 \u03a3t     2 log(1/\u03b4) + \u03b5n + 4LL\u2217Kn\u03b4\n                                  t=1\n                          \u2264  2 E       n  \u2225At\u22252 \u02c6      2n log(1/\u03b4) + \u03b5n + 4LL\u2217Kn\u03b4 ,\n                                                \u03a3t\n                                     t=1\nwhere the last inequality uses the Cauchy-Schwarz inequality and the concavity of the square root.\nFinally, the sum  n  t=1 \u2225At\u22252  \u02c6\n                               \u03a3t is bounded using Lemma 10. This completes the proof.\n                                                      19", "md": "Complete Statement of Corollary 2\n\nTheorem 11. Let $$\\sigma_0^2 \\geq 8 \\log(1/\\delta) n^2 \\log \\log n$$. Then there exist functions $$\\xi_a : \\mathbb{R} \\rightarrow [n, \\log n]$$ such that the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$$\nR(n) \\leq 8\\sigma^2 \\log(1/\\delta) \\log n - 2\\sigma^2 \\int_{0}^{\\log n} \\sum_{a \\in A} \\frac{\\theta - a}{h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta - a)} d\\theta - a + C,\n$$\nwhere $$C = 8\\sigma^2 K \\log(1/\\delta) \\sqrt{\\log n} + \\left(2\\frac{2 \\log(1/\\delta) + 1}{\\sigma_0}Kn\\delta + 1\\right)$$ is a low-order term.\n\nMoreover, when $$\\sigma_0^2 < 8 \\log(1/\\delta) \\frac{1}{n^2} \\log \\log n$$, the regret is bounded as\n\n$$\nR(n) \\leq \\frac{2(2 \\log(1/\\delta) + 1)K\\delta + 1}{8 \\log(1/\\delta) \\log \\log n}.\n$$\nProof. The first claim is proved in Appendix A.2. The second claim can be proved as follows. Take Theorem 1, set $$\\epsilon = 0$$, and consider the three cases in Appendix A.1.\n\nCase 1: Event Et occurs and the gap is large, $$\\Delta A_t \\geq \\epsilon$$. On event Et, action a can be taken only if\n\n$$\n\\Delta a \\leq 2 \\sqrt{2} \\sigma_0^{-1} + \\sigma_0^{-1} N_{t,a} \\leq 2 \\sqrt{2} \\sigma_0^2 \\log(1/\\delta) \\leq 2 \\sqrt{4n^2 \\log \\log n} < \\frac{1}{n}.\n$$\nTherefore, the corresponding n-round regret is bounded by 1.\n\nCase 2: The gap is small, $$\\Delta A_t < \\epsilon$$. This case cannot happen because $$\\epsilon = 0$$.\n\nCase 3: Event Et does not occur. The n-round regret is bounded by\n\n$$\n\\left(2 \\frac{2 \\log(1/\\delta) + 1}{\\sigma_0}Kn\\delta\\right) \\leq \\frac{2(2 \\log(1/\\delta) + 1)K\\delta}{8 \\log(1/\\delta) \\log \\log n}.\n$$\nThis completes the proof.\n\nGap-Free Regret Bound of BayesUCB in Linear Bandit\n\nLet Et be the event in (12). Our proof has three parts.\n\nCase 1: Event Et occurs and the gap is large, $$\\Delta A_t \\geq \\epsilon$$. Then\n\n$$\n\\Delta A_t = A^{\\top*} \\theta - A^{\\top}_t \\theta \\leq A^{\\top*} \\theta - U_{t,A^*} + U_{t,A_t} - A^{\\top}_t \\theta \\leq U_{t,A_t} - A^{\\top}_t \\theta \\leq 2C_{t,A_t}.\n$$\nThe first inequality holds because $$U_{t,A_t} - U_{t,A^*} \\geq 0$$ by the design of BayesUCB. The second one uses that $$A^{\\top*} \\theta - U_{t,A^*} \\leq 0$$. Specifically, for any action a in A on event Et,\n\n$$\na^{\\top} \\theta - U_{t,a} = a^{\\top}(\\theta - \\hat{\\theta}_t) - C_{t,a} \\leq C_{t,a} - C_{t,a} = 0.\n$$\nThe last inequality follows from the definition of event Et. Specifically, for any action a in A on event Et,\n\n$$\nU_{t,a} - a^{\\top} \\theta = a^{\\top}(\\hat{\\theta}_t - \\theta) + C_{t,a} \\leq C_{t,a} + C_{t,a} = 2C_{t,a}.\n$$\nCases 2 and 3 are bounded as in Appendix A.6. Now we chain all inequalities, add them over all rounds, and get\n\n$$\nR(n) \\leq 2E \\sum_{t=1}^{n} \\left\\|A_t\\right\\|^{\\hat{\\Sigma}_t} 2 \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta\n$$\n$$\n\\leq 2E \\sum_{t=1}^{n} \\left\\|A_t\\right\\|^2 \\hat{\\Sigma}_t 2n \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta,\n$$\nwhere the last inequality uses the Cauchy-Schwarz inequality and the concavity of the square root. Finally, the sum $$\\sum_{t=1}^{n} \\left\\|A_t\\right\\|^2 \\hat{\\Sigma}_t$$ is bounded using Lemma 10. This completes the proof.\n\n19", "images": [], "items": [{"type": "text", "value": "Complete Statement of Corollary 2\n\nTheorem 11. Let $$\\sigma_0^2 \\geq 8 \\log(1/\\delta) n^2 \\log \\log n$$. Then there exist functions $$\\xi_a : \\mathbb{R} \\rightarrow [n, \\log n]$$ such that the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$$\nR(n) \\leq 8\\sigma^2 \\log(1/\\delta) \\log n - 2\\sigma^2 \\int_{0}^{\\log n} \\sum_{a \\in A} \\frac{\\theta - a}{h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta - a)} d\\theta - a + C,\n$$\nwhere $$C = 8\\sigma^2 K \\log(1/\\delta) \\sqrt{\\log n} + \\left(2\\frac{2 \\log(1/\\delta) + 1}{\\sigma_0}Kn\\delta + 1\\right)$$ is a low-order term.\n\nMoreover, when $$\\sigma_0^2 < 8 \\log(1/\\delta) \\frac{1}{n^2} \\log \\log n$$, the regret is bounded as\n\n$$\nR(n) \\leq \\frac{2(2 \\log(1/\\delta) + 1)K\\delta + 1}{8 \\log(1/\\delta) \\log \\log n}.\n$$\nProof. The first claim is proved in Appendix A.2. The second claim can be proved as follows. Take Theorem 1, set $$\\epsilon = 0$$, and consider the three cases in Appendix A.1.\n\nCase 1: Event Et occurs and the gap is large, $$\\Delta A_t \\geq \\epsilon$$. On event Et, action a can be taken only if\n\n$$\n\\Delta a \\leq 2 \\sqrt{2} \\sigma_0^{-1} + \\sigma_0^{-1} N_{t,a} \\leq 2 \\sqrt{2} \\sigma_0^2 \\log(1/\\delta) \\leq 2 \\sqrt{4n^2 \\log \\log n} < \\frac{1}{n}.\n$$\nTherefore, the corresponding n-round regret is bounded by 1.\n\nCase 2: The gap is small, $$\\Delta A_t < \\epsilon$$. This case cannot happen because $$\\epsilon = 0$$.\n\nCase 3: Event Et does not occur. The n-round regret is bounded by\n\n$$\n\\left(2 \\frac{2 \\log(1/\\delta) + 1}{\\sigma_0}Kn\\delta\\right) \\leq \\frac{2(2 \\log(1/\\delta) + 1)K\\delta}{8 \\log(1/\\delta) \\log \\log n}.\n$$\nThis completes the proof.\n\nGap-Free Regret Bound of BayesUCB in Linear Bandit\n\nLet Et be the event in (12). Our proof has three parts.\n\nCase 1: Event Et occurs and the gap is large, $$\\Delta A_t \\geq \\epsilon$$. Then\n\n$$\n\\Delta A_t = A^{\\top*} \\theta - A^{\\top}_t \\theta \\leq A^{\\top*} \\theta - U_{t,A^*} + U_{t,A_t} - A^{\\top}_t \\theta \\leq U_{t,A_t} - A^{\\top}_t \\theta \\leq 2C_{t,A_t}.\n$$\nThe first inequality holds because $$U_{t,A_t} - U_{t,A^*} \\geq 0$$ by the design of BayesUCB. The second one uses that $$A^{\\top*} \\theta - U_{t,A^*} \\leq 0$$. Specifically, for any action a in A on event Et,\n\n$$\na^{\\top} \\theta - U_{t,a} = a^{\\top}(\\theta - \\hat{\\theta}_t) - C_{t,a} \\leq C_{t,a} - C_{t,a} = 0.\n$$\nThe last inequality follows from the definition of event Et. Specifically, for any action a in A on event Et,\n\n$$\nU_{t,a} - a^{\\top} \\theta = a^{\\top}(\\hat{\\theta}_t - \\theta) + C_{t,a} \\leq C_{t,a} + C_{t,a} = 2C_{t,a}.\n$$\nCases 2 and 3 are bounded as in Appendix A.6. Now we chain all inequalities, add them over all rounds, and get\n\n$$\nR(n) \\leq 2E \\sum_{t=1}^{n} \\left\\|A_t\\right\\|^{\\hat{\\Sigma}_t} 2 \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta\n$$\n$$\n\\leq 2E \\sum_{t=1}^{n} \\left\\|A_t\\right\\|^2 \\hat{\\Sigma}_t 2n \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta,\n$$\nwhere the last inequality uses the Cauchy-Schwarz inequality and the concavity of the square root. Finally, the sum $$\\sum_{t=1}^{n} \\left\\|A_t\\right\\|^2 \\hat{\\Sigma}_t$$ is bounded using Lemma 10. This completes the proof.\n\n19", "md": "Complete Statement of Corollary 2\n\nTheorem 11. Let $$\\sigma_0^2 \\geq 8 \\log(1/\\delta) n^2 \\log \\log n$$. Then there exist functions $$\\xi_a : \\mathbb{R} \\rightarrow [n, \\log n]$$ such that the n-round Bayes regret of BayesUCB in a K-armed Gaussian bandit is bounded as\n\n$$\nR(n) \\leq 8\\sigma^2 \\log(1/\\delta) \\log n - 2\\sigma^2 \\int_{0}^{\\log n} \\sum_{a \\in A} \\frac{\\theta - a}{h_a(\\theta^*_a - \\xi_a(\\theta^*_a)) h_{-a}(\\theta - a)} d\\theta - a + C,\n$$\nwhere $$C = 8\\sigma^2 K \\log(1/\\delta) \\sqrt{\\log n} + \\left(2\\frac{2 \\log(1/\\delta) + 1}{\\sigma_0}Kn\\delta + 1\\right)$$ is a low-order term.\n\nMoreover, when $$\\sigma_0^2 < 8 \\log(1/\\delta) \\frac{1}{n^2} \\log \\log n$$, the regret is bounded as\n\n$$\nR(n) \\leq \\frac{2(2 \\log(1/\\delta) + 1)K\\delta + 1}{8 \\log(1/\\delta) \\log \\log n}.\n$$\nProof. The first claim is proved in Appendix A.2. The second claim can be proved as follows. Take Theorem 1, set $$\\epsilon = 0$$, and consider the three cases in Appendix A.1.\n\nCase 1: Event Et occurs and the gap is large, $$\\Delta A_t \\geq \\epsilon$$. On event Et, action a can be taken only if\n\n$$\n\\Delta a \\leq 2 \\sqrt{2} \\sigma_0^{-1} + \\sigma_0^{-1} N_{t,a} \\leq 2 \\sqrt{2} \\sigma_0^2 \\log(1/\\delta) \\leq 2 \\sqrt{4n^2 \\log \\log n} < \\frac{1}{n}.\n$$\nTherefore, the corresponding n-round regret is bounded by 1.\n\nCase 2: The gap is small, $$\\Delta A_t < \\epsilon$$. This case cannot happen because $$\\epsilon = 0$$.\n\nCase 3: Event Et does not occur. The n-round regret is bounded by\n\n$$\n\\left(2 \\frac{2 \\log(1/\\delta) + 1}{\\sigma_0}Kn\\delta\\right) \\leq \\frac{2(2 \\log(1/\\delta) + 1)K\\delta}{8 \\log(1/\\delta) \\log \\log n}.\n$$\nThis completes the proof.\n\nGap-Free Regret Bound of BayesUCB in Linear Bandit\n\nLet Et be the event in (12). Our proof has three parts.\n\nCase 1: Event Et occurs and the gap is large, $$\\Delta A_t \\geq \\epsilon$$. Then\n\n$$\n\\Delta A_t = A^{\\top*} \\theta - A^{\\top}_t \\theta \\leq A^{\\top*} \\theta - U_{t,A^*} + U_{t,A_t} - A^{\\top}_t \\theta \\leq U_{t,A_t} - A^{\\top}_t \\theta \\leq 2C_{t,A_t}.\n$$\nThe first inequality holds because $$U_{t,A_t} - U_{t,A^*} \\geq 0$$ by the design of BayesUCB. The second one uses that $$A^{\\top*} \\theta - U_{t,A^*} \\leq 0$$. Specifically, for any action a in A on event Et,\n\n$$\na^{\\top} \\theta - U_{t,a} = a^{\\top}(\\theta - \\hat{\\theta}_t) - C_{t,a} \\leq C_{t,a} - C_{t,a} = 0.\n$$\nThe last inequality follows from the definition of event Et. Specifically, for any action a in A on event Et,\n\n$$\nU_{t,a} - a^{\\top} \\theta = a^{\\top}(\\hat{\\theta}_t - \\theta) + C_{t,a} \\leq C_{t,a} + C_{t,a} = 2C_{t,a}.\n$$\nCases 2 and 3 are bounded as in Appendix A.6. Now we chain all inequalities, add them over all rounds, and get\n\n$$\nR(n) \\leq 2E \\sum_{t=1}^{n} \\left\\|A_t\\right\\|^{\\hat{\\Sigma}_t} 2 \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta\n$$\n$$\n\\leq 2E \\sum_{t=1}^{n} \\left\\|A_t\\right\\|^2 \\hat{\\Sigma}_t 2n \\log(1/\\delta) + \\epsilon n + 4LL^*Kn\\delta,\n$$\nwhere the last inequality uses the Cauchy-Schwarz inequality and the concavity of the square root. Finally, the sum $$\\sum_{t=1}^{n} \\left\\|A_t\\right\\|^2 \\hat{\\Sigma}_t$$ is bounded using Lemma 10. This completes the proof.\n\n19"}]}, {"page": 20, "text": "      103                       (a) Gaussian noise                             103                         (b) Box noise\n      102                                                                      102\n   Regret difference                                                        Regret difference\n      101                                                                      101\n      100                                                                      100\n     10-1                                                                     10-1\n          0      10      20     30      40     50      60     70      80           0      10     20      30      40     50      60     70      80\n                                     Instance                                                                 Instance\nFigure 3: The difference in regret of UCB1 and BayesUCB on 81 Bayesian bandit instances, sorted by\nthe difference. In plot (a), the noise is Gaussian N                      (0, \u03c32). In plot (b), the noise is \u03c3 with probability\n0.5 and \u2212\u03c3 otherwise.\nD       Comparison of BayesUCB and UCB1\nWe report the difference in regret of UCB1 and BayesUCB on 81 Bayesian bandit instances. These\ninstances are obtained by all combinations of K \u2208                           {5, 10, 20} actions, reward noise \u03c3 \u2208                   {0.5, 1, 2},\nprior gap \u22060 \u2208          {0.5, 1, 2}, and prior width \u03c30 \u2208                 {0.5, 1, 2}. The horizon is n = 1 000 rounds and\nall results are averaged over 1 000 random runs.\nOur results are reported in Figure 3. In Figure 3a, the noise is Gaussian N                                    (0, \u03c32). In Figure 3b, the\nnoise is \u03c3 with probability 0.5 and \u2212\u03c3 otherwise. Therefore, this noise is \u03c32-sub-Gaussian, of the\nsame magnitude as N              (0, \u03c32) but far from it in terms of the distribution. This tests the robustness of\nBayesUCB to Gaussian posterior updates. UCB1 only needs \u03c32-sub-Gaussian noise. In both plots, and\nin all 81 Bayesian bandit instances, BayesUCB has a lower regret than UCB1. It is also remarkably\nrobust to noise misspecification, although we cannot prove it.\nE      Note on Information-Theory Bounds\nOur approach could be used to derive Bayesian information-theory bounds [Russo and Van Roy,\n2016]. The key step in these bounds, where the information-theory term It,a for action a at round t\narises, is \u2206A      t \u2264   \u0393     It,At, where \u0393 is the highest possible ratio of regret to information gain. As in\nCase 1 in Appendix A.6, the n-round regret can be bounded as\n                            n   \u2206At =         n   \u2206A 1  t \u22062 At \u2264     \u2206\u03b5 1       n   \u22062 At \u2264     \u2206\u03b5\u03932       n   It,A  t .\n                          t=1               t=1                          min   t=1                  min    t=1\nThe term  n       t=1 It,At can be bounded using a worst-case argument by a O(log n) bound.\n                                                                        20", "md": "# Document\n\n$$\n\\begin{array}{cc}\n\\text{103} & \\text{(a) Gaussian noise} & \\text{103} & \\text{(b) Box noise} \\\\\n\\text{102} & & \\text{102} \\\\\n\\text{Regret difference} & & \\text{Regret difference} \\\\\n\\text{101} & & \\text{101} \\\\\n\\text{100} & & \\text{100} \\\\\n10^{-1} & & 10^{-1} \\\\\n& 0 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & & 0 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 \\\\\n& & \\text{Instance} & & & & & & & & & & \\text{Instance}\n\\end{array}\n$$\n\nFigure 3: The difference in regret of UCB1 and BayesUCB on 81 Bayesian bandit instances, sorted by the difference. In plot (a), the noise is Gaussian N(0, \u03c3^2). In plot (b), the noise is \u03c3 with probability 0.5 and -\u03c3 otherwise.\n\nD Comparison of BayesUCB and UCB1\n\nWe report the difference in regret of UCB1 and BayesUCB on 81 Bayesian bandit instances. These instances are obtained by all combinations of K \u2208 {5, 10, 20} actions, reward noise \u03c3 \u2208 {0.5, 1, 2}, prior gap \u22060 \u2208 {0.5, 1, 2}, and prior width \u03c30 \u2208 {0.5, 1, 2}. The horizon is n = 1,000 rounds and all results are averaged over 1,000 random runs.\n\nOur results are reported in Figure 3. In Figure 3a, the noise is Gaussian N(0, \u03c3^2). In Figure 3b, the noise is \u03c3 with probability 0.5 and -\u03c3 otherwise. Therefore, this noise is \u03c3^2-sub-Gaussian, of the same magnitude as N(0, \u03c3^2) but far from it in terms of the distribution. This tests the robustness of BayesUCB to Gaussian posterior updates. UCB1 only needs \u03c3^2-sub-Gaussian noise. In both plots, and in all 81 Bayesian bandit instances, BayesUCB has a lower regret than UCB1. It is also remarkably robust to noise misspecification, although we cannot prove it.\n\nE Note on Information-Theory Bounds\n\nOur approach could be used to derive Bayesian information-theory bounds [Russo and Van Roy, 2016]. The key step in these bounds, where the information-theory term $I_{t,a}$ for action a at round t arises, is $\\Delta A_t \\leq \\Gamma I_{t,A_t}$, where $\\Gamma$ is the highest possible ratio of regret to information gain. As in Case 1 in Appendix A.6, the n-round regret can be bounded as\n\n$$\n\\begin{align*}\n&\\sum_{t=1}^{n} \\Delta A_t = \\sum_{t=1}^{n} \\Delta A_{1,t} \\Delta^2 A_t \\leq \\Delta \\epsilon_1 \\sum_{t=1}^{n} \\Delta^2 A_t \\leq \\Delta \\epsilon \\Gamma^2 \\sum_{t=1}^{n} I_{t,A_t} \\\\\n&\\text{min} \\ t=1 \\ \\text{min} \\ t=1\n\\end{align*}\n$$\n\nThe term $\\sum_{t=1}^{n} I_{t,A_t}$ can be bounded using a worst-case argument by a $O(\\log n)$ bound.", "images": [], "items": [{"type": "heading", "lvl": 1, "value": "Document", "md": "# Document"}, {"type": "text", "value": "$$\n\\begin{array}{cc}\n\\text{103} & \\text{(a) Gaussian noise} & \\text{103} & \\text{(b) Box noise} \\\\\n\\text{102} & & \\text{102} \\\\\n\\text{Regret difference} & & \\text{Regret difference} \\\\\n\\text{101} & & \\text{101} \\\\\n\\text{100} & & \\text{100} \\\\\n10^{-1} & & 10^{-1} \\\\\n& 0 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & & 0 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 \\\\\n& & \\text{Instance} & & & & & & & & & & \\text{Instance}\n\\end{array}\n$$\n\nFigure 3: The difference in regret of UCB1 and BayesUCB on 81 Bayesian bandit instances, sorted by the difference. In plot (a), the noise is Gaussian N(0, \u03c3^2). In plot (b), the noise is \u03c3 with probability 0.5 and -\u03c3 otherwise.\n\nD Comparison of BayesUCB and UCB1\n\nWe report the difference in regret of UCB1 and BayesUCB on 81 Bayesian bandit instances. These instances are obtained by all combinations of K \u2208 {5, 10, 20} actions, reward noise \u03c3 \u2208 {0.5, 1, 2}, prior gap \u22060 \u2208 {0.5, 1, 2}, and prior width \u03c30 \u2208 {0.5, 1, 2}. The horizon is n = 1,000 rounds and all results are averaged over 1,000 random runs.\n\nOur results are reported in Figure 3. In Figure 3a, the noise is Gaussian N(0, \u03c3^2). In Figure 3b, the noise is \u03c3 with probability 0.5 and -\u03c3 otherwise. Therefore, this noise is \u03c3^2-sub-Gaussian, of the same magnitude as N(0, \u03c3^2) but far from it in terms of the distribution. This tests the robustness of BayesUCB to Gaussian posterior updates. UCB1 only needs \u03c3^2-sub-Gaussian noise. In both plots, and in all 81 Bayesian bandit instances, BayesUCB has a lower regret than UCB1. It is also remarkably robust to noise misspecification, although we cannot prove it.\n\nE Note on Information-Theory Bounds\n\nOur approach could be used to derive Bayesian information-theory bounds [Russo and Van Roy, 2016]. The key step in these bounds, where the information-theory term $I_{t,a}$ for action a at round t arises, is $\\Delta A_t \\leq \\Gamma I_{t,A_t}$, where $\\Gamma$ is the highest possible ratio of regret to information gain. As in Case 1 in Appendix A.6, the n-round regret can be bounded as\n\n$$\n\\begin{align*}\n&\\sum_{t=1}^{n} \\Delta A_t = \\sum_{t=1}^{n} \\Delta A_{1,t} \\Delta^2 A_t \\leq \\Delta \\epsilon_1 \\sum_{t=1}^{n} \\Delta^2 A_t \\leq \\Delta \\epsilon \\Gamma^2 \\sum_{t=1}^{n} I_{t,A_t} \\\\\n&\\text{min} \\ t=1 \\ \\text{min} \\ t=1\n\\end{align*}\n$$\n\nThe term $\\sum_{t=1}^{n} I_{t,A_t}$ can be bounded using a worst-case argument by a $O(\\log n)$ bound.", "md": "$$\n\\begin{array}{cc}\n\\text{103} & \\text{(a) Gaussian noise} & \\text{103} & \\text{(b) Box noise} \\\\\n\\text{102} & & \\text{102} \\\\\n\\text{Regret difference} & & \\text{Regret difference} \\\\\n\\text{101} & & \\text{101} \\\\\n\\text{100} & & \\text{100} \\\\\n10^{-1} & & 10^{-1} \\\\\n& 0 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & & 0 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 \\\\\n& & \\text{Instance} & & & & & & & & & & \\text{Instance}\n\\end{array}\n$$\n\nFigure 3: The difference in regret of UCB1 and BayesUCB on 81 Bayesian bandit instances, sorted by the difference. In plot (a), the noise is Gaussian N(0, \u03c3^2). In plot (b), the noise is \u03c3 with probability 0.5 and -\u03c3 otherwise.\n\nD Comparison of BayesUCB and UCB1\n\nWe report the difference in regret of UCB1 and BayesUCB on 81 Bayesian bandit instances. These instances are obtained by all combinations of K \u2208 {5, 10, 20} actions, reward noise \u03c3 \u2208 {0.5, 1, 2}, prior gap \u22060 \u2208 {0.5, 1, 2}, and prior width \u03c30 \u2208 {0.5, 1, 2}. The horizon is n = 1,000 rounds and all results are averaged over 1,000 random runs.\n\nOur results are reported in Figure 3. In Figure 3a, the noise is Gaussian N(0, \u03c3^2). In Figure 3b, the noise is \u03c3 with probability 0.5 and -\u03c3 otherwise. Therefore, this noise is \u03c3^2-sub-Gaussian, of the same magnitude as N(0, \u03c3^2) but far from it in terms of the distribution. This tests the robustness of BayesUCB to Gaussian posterior updates. UCB1 only needs \u03c3^2-sub-Gaussian noise. In both plots, and in all 81 Bayesian bandit instances, BayesUCB has a lower regret than UCB1. It is also remarkably robust to noise misspecification, although we cannot prove it.\n\nE Note on Information-Theory Bounds\n\nOur approach could be used to derive Bayesian information-theory bounds [Russo and Van Roy, 2016]. The key step in these bounds, where the information-theory term $I_{t,a}$ for action a at round t arises, is $\\Delta A_t \\leq \\Gamma I_{t,A_t}$, where $\\Gamma$ is the highest possible ratio of regret to information gain. As in Case 1 in Appendix A.6, the n-round regret can be bounded as\n\n$$\n\\begin{align*}\n&\\sum_{t=1}^{n} \\Delta A_t = \\sum_{t=1}^{n} \\Delta A_{1,t} \\Delta^2 A_t \\leq \\Delta \\epsilon_1 \\sum_{t=1}^{n} \\Delta^2 A_t \\leq \\Delta \\epsilon \\Gamma^2 \\sum_{t=1}^{n} I_{t,A_t} \\\\\n&\\text{min} \\ t=1 \\ \\text{min} \\ t=1\n\\end{align*}\n$$\n\nThe term $\\sum_{t=1}^{n} I_{t,A_t}$ can be bounded using a worst-case argument by a $O(\\log n)$ bound."}]}], "job_id": "4940a2ba-2315-4217-a9a0-5bdd909cc442", "file_path": "./corpus/2306.09136.pdf"}